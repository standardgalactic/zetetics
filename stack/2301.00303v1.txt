Rethinking with Retrieval: Faithful Large Language Model Inference
Hangfeng He†∗
Hongming Zhang‡
Dan Roth§
†University of Rochester
‡Tencent AI Lab, Seattle
§University of Pennsylvania
hanfeng.he@rochester.edu, hongmzhang@global.tencent.com
danroth@seas.upenn.edu
Abstract
Despite the success of large language mod-
els (LLMs) in various natural language pro-
cessing (NLP) tasks, the stored knowledge
in these models may inevitably be incom-
plete, out-of-date, or incorrect.
This mo-
tivates the need to utilize external knowl-
edge to assist LLMs. Unfortunately, current
methods for incorporating external knowl-
edge often require additional training or
ﬁne-tuning, which can be costly and may
not be feasible for LLMs. To address this
issue, we propose a novel post-processing
approach, rethinking with retrieval (RR),
which retrieves relevant external knowledge
based on the decomposed reasoning steps
obtained from the chain-of-thought (CoT)
prompting. This lightweight approach does
not require additional training or ﬁne-tuning
and is not limited by the input length of
LLMs. We evaluate the effectiveness of RR
through extensive experiments with GPT-3
on three complex reasoning tasks: common-
sense reasoning, temporal reasoning, and
tabular reasoning. Our results show that RR
can produce more faithful explanations and
improve the performance of LLMs.1
1
Introduction
Large language models (LLMs) have shown
exceptional performance across various tasks
through in-context learning without task-speciﬁc
training or ﬁne-tuning (Brown et al., 2020;
Chowdhery et al., 2022; Zhang et al., 2022;
Ouyang et al., 2022). Recent progress in prompt-
ing (Wei et al., 2022; Zhou et al., 2022; Kojima
et al., 2022) and decoding (Wang et al., 2022) has
made it feasible for LLMs to tackle tasks that de-
mand complex reasoning.
∗Part of this work was done while the author was at the
University of Pennsylvania.
1Our code is publicly available at https://github.
com/HornHehhf/RR.
Query
Prediction
LLM
Query 
Explanation + Prediction
LLM
Query
Explanation + Prediction
LLM
(a)
(b)
(c)
Knowledge
Chain of thought
Chain of thought
Retrieval
Rethinking
Figure 1: An overview of three approaches for using
LLMs: (a) Standard prompting for generating a pre-
diction in response to a query. (b) Chain-of-thought
prompting for generating both an explanation and a
prediction in response to a query. (c) Rethinking with
retrieval, our proposed approach for using the decom-
posed reasoning steps obtained from chain-of-thought
prompting to retrieve relevant external knowledge for
LLMs, leading to more faithful explanations and im-
proved predictions in response to a query.
However, the knowledge stored in LLMs might
inevitably be incomplete, out-of-date, or incorrect.
As a result, external sources of knowledge, such
as Wikipedia, may be essential for the success-
ful deployment of LLMs for real-world applica-
tions. Previously, people tried to utilize knowl-
edge for smaller language models (LMs), such
as T5 (Raffel et al., 2020), BERT (Devlin et al.,
2019), and RoBERTa (Liu et al., 2019). However,
these methods often require additional training or
ﬁne-tuning, which can be costly and thus imprac-
tical for LLMs.
In this paper, we present a post-processing
approach called rethinking with retrieval (RR)
for utilizing external knowledge in LLMs. Our
method begins by using the chain-of-thought
(CoT) prompting method (Wei et al., 2022) to gen-
erate a diverse set of reasoning paths, as described
in Wang et al. (2022).
We then use each rea-
soning step in those paths to retrieve relevant ex-
ternal knowledge, which enables RR to provide
arXiv:2301.00303v1  [cs.CL]  31 Dec 2022

more faithful explanations and more accurate pre-
dictions, as illustrated in Figure 1.
We evaluate the effectiveness of our proposed
method, RR, on three complex reasoning tasks:
commonsense reasoning, temporal reasoning, and
tabular reasoning, using GPT-3 175B (Brown
et al., 2020) and different external knowledge
sources:
Wikipedia, Wikidata (Vrandeˇci´c and
Krötzsch, 2014), WordNet (Miller, 1995), and
Conceptnet (Speer et al., 2017).
The results
demonstrate that RR consistently outperforms all
baselines on all three tasks without requiring ad-
ditional training or ﬁne-tuning, indicating the su-
periority of our approach in leveraging external
knowledge to enhance the performance of LLMs.
2
Related Work
Enhancing LMs through retrieval.
Retrieval-
enhanced LMs have received signiﬁcant attention
as a means of improving performance through the
incorporation of external knowledge. For exam-
ple, the k-most similar training contexts can be re-
trieved to improve the estimation of the next word
distribution in both the training stage (Borgeaud
et al., 2021) and the inference stage (Khandelwal
et al., 2020). Furthermore, search query genera-
tors have been adopted to generate search queries
for search engines to retrieve relevant documents
(Komeili et al., 2022; Shuster et al., 2022; Thop-
pilan et al., 2022).
Other approaches have uti-
lized retrieved documents as the additional con-
text in generation tasks (Joshi et al., 2020; Guu
et al., 2020; Lewis et al., 2020). Nakano et al.
(2021) instead use human feedback in a text-based
web-browsing environment.
Among these pre-
vious works, Khandelwal et al. (2020) is most
closely related to our approach.
However, they
focus on improving local inference by using the
nearest neighbor datastore constructed from train-
ing data, whereas we focus on conducting faith-
ful inference using external knowledge. In con-
trast to other aforementioned approaches, which
require training or ﬁne-tuning to incorporate re-
trieved knowledge, we propose a post-processing
method for leveraging retrieved knowledge with-
out additional training or ﬁne-tuning.
Incorporating external knowledge into LMs.
Signiﬁcant effort has been devoted to leveraging
external knowledge to improve the reasoning abil-
ity of LMs. Previous work has incorporated exter-
nal knowledge sources such as WordNet (Miller,
1995) and ConceptNet (Speer et al., 2017) to en-
hance LMs for tabular reasoning tasks (Neeraja
et al., 2021; Varun et al., 2022).
Explicit rules
have also been added to inputs to improve rea-
soning ability over implicit knowledge (Talmor
et al., 2020). In addition, explicit knowledge from
Wikidata (Vrandeˇci´c and Krötzsch, 2014) and im-
plicit knowledge in LLMs have been integrated
into a transformer (Vaswani et al., 2017) for vi-
sual question answering (Gui et al., 2021). Nye
et al. (2021) instead introduces a symbolic reason-
ing module to improve coherence and consistency
in LLMs. Among these previous works, Nye et al.
(2021) is the most relevant to our approach. Still,
they focus on incorporating logical constraints to
improve coherence and consistency, whereas we
aim to improve the faithfulness of explanations
through the use of external knowledge. In con-
trast to other aforementioned approaches that in-
corporate external knowledge before generation
and require additional training or ﬁne-tuning, our
proposal leverages external knowledge in a post-
processing manner to enhance LMs without addi-
tional training or ﬁne-tuning.
Uncovering latent Knowledge in LLMs.
There
has been a line of work exploring the knowledge
hidden within LLMs for reasoning. This has in-
cluded the use of careful prompting to encourage
LLMs to generate explanations in the reasoning
process, such as through chain of thought prompt-
ing in few-shot (Wei et al., 2022) or zero-shot
(Kojima et al., 2022) learning, or through the use
of scratchpads for intermediate computation (Nye
et al., 2022). In addition, various methods based
on sampling a diverse set of reasoning paths in
LLMs have been proposed, including training ver-
iﬁers to judge the correctness of model comple-
tions (Cobbe et al., 2021), calibrating model pre-
dictions based on the reliability of the explana-
tions (Ye and Durrett, 2022), and promoting self-
consistency over diverse reasoning paths (Wang
et al., 2022). Zelikman et al. (2022) instead it-
eratively bootstrap the ability of LLMs to gener-
ate high-quality rationales from a few initial ex-
amples. Liu et al. (2022) further propose generat-
ing knowledge from LLMs, which is then used as
additional input to improve commonsense reason-
ing. In contrast to this line of work, our proposal
focuses on leveraging external knowledge to en-
hance LLMs, while they aim to explore the knowl-
edge hidden within LLMs.

3
Rethinking with Retrieval
LLMs have been shown to generate incorrect sup-
porting facts from time to time, even when they ac-
curately capture the perspective needed to answer
a question. This phenomenon highlights intrinsic
issues in the way LLMs store and retrieve knowl-
edge, including (1) the presence of out-of-date,
incorrect, or missing relevant knowledge in the
pre-training corpus; (2) incorrect memorization of
relevant knowledge during pre-training; and (3)
incorrect retrieval of relevant knowledge during
the inference stage. To address these issues, we
propose the use of RR, which leverages external
knowledge through the retrieval of relevant infor-
mation based on decomposed reasoning steps.
Overview.
Given a query Q, we utilize chain-of-
thought prompting to generate a diverse set of rea-
soning paths R1, R2, · · · RN, where each reason-
ing path Ri consists of an explanation Ei followed
by a prediction Pi. After that, we retrieve relevant
knowledge K1, · · · KM from a suitable knowledge
base KB to support the explanation in each reason-
ing path, and select the prediction ˆP that is most
faithful to this knowledge. To better illustrate our
proposal, we use “Did Aristotle use a laptop?” as
a running example in this work.
Chain-of-thought prompting.
In contrast to
standard prompting, CoT prompting (Wei et al.,
2022) includes demonstrations of step-by-step rea-
soning examples in the prompt to produce a series
of short sentences that capture the reasoning pro-
cess. For instance, given the question “Did Aris-
totle use a laptop?”, CoT prompting aims to gen-
erate the complete reasoning path “Aristotle died
in 322 BC. The ﬁrst laptop was invented in 1980.
Thus, Aristotle did not use a laptop. So the answer
is no.” rather than simply outputs “No.” Empirical
results show that CoT prompting signiﬁcantly im-
proves the performance of LLMs on many multi-
step reasoning tasks. Therefore, we adopt CoT
prompting to obtain both explanation E and pre-
diction P for the query Q.
Sampling diverse reasoning paths.
Similar to
Wang et al. (2022), we sample a diverse set of rea-
soning paths R1, R2, · · · RN rather than only con-
sidering the greedy path as in Wei et al. (2022).
For the question “Did Aristotle use a laptop?”, the
potential reasoning paths can be as follows:
(R1) Aristotle died in 2000. The ﬁrst laptop was
invented in 1980. Thus, Aristotle used a lap-
top. So the answer is yes.
(R2) Aristotle died in 322BC. The ﬁrst laptop was
invented in 2000. Thus, Aristotle did not use
a laptop. So the answer is no.
(R3) Aristotle died in 322BC. The ﬁrst laptop was
invented in 1980. Thus, Aristotle did not use
a laptop. So the answer is no.
Knowledge
retrieval.
Different
knowledge
bases can be used to address different tasks. For
example, to address the question “Did Aristotle
use a laptop?”, we can use Wikipedia as the ex-
ternal knowledge base KB. Information retrieval
techniques can be applied to retrieve the relevant
knowledge K1, · · · KM from Wikipedia based
on the decomposed reasoning steps. Ideally, we
would obtain the following two paragraphs from
Wikipedia for this question:
(K1) Aristotle (384–322 BC) was a Greek philoso-
pher and polymath during the Classical pe-
riod in Ancient Greece. ...
(K2) The Epson HX-20, the ﬁrst laptop computer,
was invented in 1980. ...
Faithful inference.
The faithfulness of each rea-
soning path Ri can be estimated using a function
fKB(Ri), which is based on relevant knowledge
K1, · · · , KM retrieved from the knowledge base
KB. The ﬁnal prediction is obtained through the
application of the following inference procedure2:
ˆP =
arg max
Pi∈{P1,··· ,PN}
N
X
i=1
1(Pi = P)fKB(Ri), (1)
where Pi denotes the corresponding prediction in
the reasoning path Ri. This inference procedure
is designed to identify the most faithful prediction
ˆP to the knowledge base among all predictions in
the N reasoning paths. For instance, in the run-
ning example, given reasoning paths R1, R2, R3
and the retrieved knowledge K1, K2, the above in-
ference procedure would output the prediction “So
the answer is no.”, as it is supported by both R2
and R3 and has a higher faithfulness score com-
pared to the prediction “So the answer is yes.”,
which is only supported by R1.
2Note that this is the basic version of faithful inference,
and further variations can be found in Section 5.3.

4
Experiments
In this section, we present the evaluation of our
proposed method, RR, on three complex reason-
ing tasks: commonsense reasoning, temporal rea-
soning, and tabular reasoning.
4.1
Baselines
We compare with the following baselines.
Zero-shot/few-shot prompting.
In our experi-
ments, we consider GPT-3 with standard zero-
shot/few-shot prompting as baselines, following
the approach described in Brown et al. (2020), in
which zero or few in-context exemplars of input-
output pairs are provided in the prompt.
Chain-of-thought prompting.
In addition to
the standard zero-shot/few-shot prompting, we
also consider GPT-3 with the CoT prompting pro-
posed in (Wei et al., 2022) as a baseline in our ex-
periments. This approach involves feeding LLMs
step-by-step reasoning examples instead of stan-
dard input-output examples.
Self-consistency.
In addition, we also consider
self-consistency (Wang et al., 2022) as a baseline
in our experiments. This approach, proposed as an
alternative to the naive greedy decoding used in
CoT prompting (Wei et al., 2022), involves sam-
pling a diverse set of reasoning paths and select-
ing the most consistent answer by marginalizing
the sampled paths.
4.2
Commonsense Reasoning
Dataset description.
For commonsense reason-
ing, we consider the StrategyQA dataset (Geva
et al., 2021), which includes questions that require
implicit reasoning strategies.
For example, the
question “Did Aristotle use a laptop?” requires
implicit decomposition into reasoning steps, while
the question “Was Aristotle alive when the laptop
was invented?” explicitly speciﬁes the reasoning
process. The StrategyQA dataset includes 2, 290
training examples, each consisting of a question
(Q), a yes/no answer (A), a decomposition (D),
evidence paragraphs (E), and supporting facts (F).
On average, each question requires about 2.93 rea-
soning steps and 2.33 evidence paragraphs. In ad-
dition, a development set is constructed by ran-
domly sampling 10% of the training examples
(i.e., 229 examples). The answer distribution is
roughly balanced, with approximately 47% "yes"
questions in both the training and development
sets. Unless otherwise speciﬁed, the models are
evaluated on the development set3 for StrategyQA.
Implementation details.
In this part, we uti-
lize Wikipedia as the external knowledge base
KB. For each sentence in the explanation of ev-
ery reasoning path, we ﬁrst apply BM25 (Robert-
son et al., 2009) to retrieve the top 10 most rele-
vant paragraphs from Wikipedia. In particular, we
use the re-implementation of the sparse retrieval
BM254 in Karpukhin et al. (2020) from Pyserini
(Lin et al., 2021). Subsequently, we use the pre-
trained MPNet model (Song et al., 2020) to se-
lect the most similar paragraph based on the cosine
similarity between the sentence embeddings of the
retrieved paragraph and the sentence.
We then
employ a pre-trained natural language inference
(NLI) model (Nie et al., 2020) to obtain the en-
tailment and contradiction scores for the sentence,
treating the most similar paragraph as the premise.
The faithfulness of each reasoning path is then
calculated using fKB(·) based on the entailment
scores, contradiction scores, and MPNet similari-
ties of all sentences in the explanation of the rea-
soning path. The ﬁnal prediction for each ques-
tion is obtained through faithful inference (Equa-
tion 1). More details about fKB(·) can be found in
Appendix A.2.
4.3
Temporal Reasoning
Dataset description.
In this experiment, we use
the TempQuestions dataset (Jia et al., 2018) to
investigate temporal reasoning. This dataset in-
cludes 1, 271 temporal questions that are divided
into four classes: explicit temporal, implicit tem-
poral, temporal answer, and ordinal constraints.
The questions are paired with their answers from
Freebase (Bollacker et al., 2008). To examine the
most challenging aspect of temporal reasoning, we
focus on the set of implicit temporal questions,
which contain implicit temporal expressions, in-
cluding free-text temporal expressions.
For ex-
ample, the question “who was governor of oregon
when shanghai noon was released?” is an implicit
temporal question. To facilitate our analysis, we
only consider questions with a single answer, re-
sulting in a total of 175 examples. Of these ex-
3As the annotations for the test set are not publicly avail-
able, we use the development set for evaluation. This allows
us to perform a more comprehensive analysis.
4We also experimented with DPR and BM25+DPR, and
found that BM25 outperformed these methods in our experi-
ments. More details can be found in Appendix A.3.

Methods
Commonsense
Temporal
Tabular
GPT-3
Zero-shot prompting
58.08
28.40
82.00
Few-shot prompting
63.32
29.59
83.08
Chain-of-thought prompting
65.94
33.14
83.33
Self-consistency
73.36
37.28
84.00
Rethinking with retrieval
77.73
39.05
84.83
Table 1: Performance of different methods using GPT-3 on three reasoning tasks.
amples, the ﬁrst 6 are used for prompting, and the
remaining 169 are used for evaluation.
Implementation details.
In this part, we utilize
Wikidata (Vrandeˇci´c and Krötzsch, 2014) as the
external knowledge base KB, as it is the largest
publicly available knowledge graph, and the data
from Freebase has been migrated to Wikidata. To
incorporate this knowledge into our system, we
apply an entity linking system5 to each sentence
in the explanation of each reasoning path to iden-
tify the corresponding Wikidata pages for all enti-
ties in the sentence. Next, we extract all temporal
relations from these relevant Wikidata pages and
use templates to convert these temporal relations
into sentences. This step generates a set of rele-
vant knowledge sentences for each sentence in the
explanation of each reasoning path. The ﬁnal pre-
diction is then obtained by applying the procedure
described in Section 4.2, in which the retrieved
paragraphs are replaced with the relevant knowl-
edge sentences from the current part.
4.4
Tabular Reasoning
Dataset
description.
We
consider
the
IN-
FOTABS dataset (Gupta et al., 2020) for tabu-
lar reasoning, which consists of 23, 738 human-
written textual hypotheses based on premises in
the form of tables extracted from 2, 540 unique
Wikipedia info-boxes. We focus on the develop-
ment set, which includes 1, 800 hypotheses based
on 200 tables, and only consider entailed and con-
tradictory hypotheses as it is tricky to write CoT
demonstrations for neutral hypotheses. This re-
sults in a total of 1, 200 hypotheses based on 200
tables for evaluation, with an equal number of en-
tailed and contradictory hypotheses.
Implementation details.
In this part, we utilize
WordNet (Miller, 1995) and ConceptNet (Speer
5We use the spacy entity linker: https://pypi.org/
project/spacy-entity-linker/.
et al., 2017) as external knowledge bases. To con-
vert tables into textual premises, we follow the
same technique as in Varun et al. (2022). For each
premise-hypothesis pair, we follow the procedure
outlined in Varun et al. (2022) to retrieve rele-
vant word relation triples that connect the premise
and hypothesis words, such as “married” RelatedTo
←−−−−→
“spouse”.
These triples are then converted into
sentences using some simple templates. The re-
sulting sentences, along with the textual premises
from the tables, serve as relevant knowledge for
each sentence in the explanation of each reasoning
path. To obtain the ﬁnal prediction, the procedure
described in Section 4.2 is applied, whereby the
retrieved paragraphs in Section 4.2 are replaced
with the relevant knowledge from the current part.
4.5
Evaluation
Experimental settings.
In all experiments, we
utilize GPT-3 text-davinci-002 unless oth-
erwise stated. The maximum number of tokens for
generation during completion is set to 256. For
zero-shot, few-shot, and chain-of-thought prompt-
ing, the temperature is ﬁxed at 0.
For self-
consistency and rethinking with retrieval, we ran-
domly sample 10 outputs6 with temperature 0.7.
Detailed prompts can be found in Appendix A.1.
We evaluate the performance of different methods
on commonsense and tabular reasoning using ac-
curacy, and on temporal reasoning using the exact
match metric as deﬁned in Rajpurkar et al. (2016).
Results.
As shown in Table 1, our proposed
method, rethinking with retrieval, consistently
outperforms all baselines on all three reasoning
tasks without requiring additional training or ﬁne-
tuning. The results highlight the effectiveness of
our approach in leveraging external knowledge to
improve the performance of LLMs.
6For commonsense reasoning, we sample 9 outputs, as we
have found that odd numbers of outputs tend to yield better
voting performance for self-consistency on StrategyQA.

Wrong supporting facts
Wrong inference
Question
Was Lil Jon’s top ranked Billboard song a
collaboration with a member of The Lox?
Would the top of Mount Fuji stick out of
the Sea of Japan?
Answer
No
Yes
Facts
Lil Jon’s highest ranked billboard song
was Yeah; Yeah was a collaboration be-
tween Lil Jon, Usher, and Ludacris; The
Lox is a rap trio consisting of: Styles P,
Sheek Louch, and Jadakiss.
The average depth of the Sea of Japan is
5,748 feet (1,752 metres) and its maxi-
mum depth is 12,276 feet (3,742 metres);
Mount Fuji is 3,776.24 metres (12,389.2
ft) tall.
GPT-3
Lil Jon’s top ranked Billboard song was
"Get Low" with the East Side Boyz. The
Lox is not a member of the East Side
Boyz.
Thus, Lil Jon’s top ranked Bill-
board song was not a collaboration with
a member of The Lox. So the answer is
no.
Mount Fuji is 3,776 meters tall.
The
Sea of Japan is about 3,741 meters deep.
Thus, the top of Mount Fuji would not
stick out of the Sea of Japan. So the an-
swer is no.
Table 2: Examples of incorrect outputs from GPT-3 with CoT prompting.
5
Analysis
In this section, we perform a thorough analysis to
gain a deeper understanding of RR.
5.1
Limitations of LLMs in Reasoning
In this subsection, we present an analysis of GPT-
3 with CoT prompting on the StrategyQA dataset.
Upon closer examination of the outputs of GPT-
3, we observed that it can provide reasonable ex-
planations and correct predictions for a number
of questions. For example, when given the ques-
tion “Will the Albany in Georgia reach a hundred
thousand occupants before the one in New York?”,
GPT-3 produced the following output:
The Albany in New York has a pop-
ulation of about 98,000.
The Albany
in Georgia has a population of about
77,000. Thus, the Albany in New York
is more populous than the Albany in
Georgia. So the answer is no.
The above output consists of three components:
(1) supporting facts (in cyan) that are based on a
particular perspective, (2) chaining arguments (in
orange), and (3) a prediction (in green).
Com-
ponents (1) and (2) contribute to the explanation.
Overall, the output exhibits a high level of quality.
However, we also observed that GPT-3 may occa-
sionally produce incorrect supporting facts for its
explanations or make incorrect inferences for its
Retrieval
Commonsense
Tabular
Query-based
73.36
36.69
Decomposition-based
77.73
39.05
Table
3:
Comparison
of
query-based
and
decomposition-based
retrieval
on
commonsense
and tabular reasoning.
predictions, despite generally being able to iden-
tify suitable perspectives.
Wrong supporting facts.
As shown in Table 2,
GPT-3 provides the incorrect supporting fact for
Lil Jon’s top-ranked Billboard song, stating that
it was “Get Low” instead of the correct answer,
“Yeah”. However, it does have the correct per-
spective on how to answer the question, “Was Lil
Jon’s top ranked Billboard song a collaboration
with a member of The Lox?”.
Wrong inference.
As shown in Table 2, GPT-3
makes an incorrect inference, stating that the top
of Mount Fuji “would not stick out” of the Sea of
Japan, rather than the correct answer, “would stick
out”. However, it does provide correct supporting
facts based on the appropriate perspective for the
question, “Would the top of Mount Fuji stick out of
the Sea of Japan?”.
5.2
Ablation Study
Importance of decomposition-based retrieval.
In our proposed method, we retrieve relevant ex-

Knowledge
Tabular
External
79.92
Background
84.75
Background + External
84.83
Table 4: Performance of RR with different types of
knowledge on tabular reasoning: external only, back-
ground only, and a combination of both.
External
knowledge refers to WordNet and ConceptNet, while
background knowledge refers to the tables.
ternal knowledge based on the decomposed rea-
soning steps rather than the original query. To fur-
ther investigate the impact of this choice, we con-
ducted additional experiments in which we used
the original query for knowledge retrieval while
keeping other aspects of our method unchanged.
As shown in Table 3, the results for these experi-
ments are poor for both commonsense and tempo-
ral reasoning, indicating the importance of using
decomposition-based retrieval in our approach.
The impact of different types of knowledge.
For tabular reasoning, we use both external knowl-
edge (WordNet and ConceptNet) and background
knowledge (tables) in our experiments.
In this
section, we further examine the effect of differ-
ent types of knowledge on the performance of our
proposed method. As shown in Table 4, the addi-
tional improvement gained by incorporating Wiki-
data and ConceptNet in addition to tables is lim-
ited, indicating that GPT-3 already captures many
word-level relations in these external knowledge
sources. In addition, the observed signiﬁcant im-
provement in tabular reasoning from using tables
alone suggests that our proposed method can also
effectively leverage background knowledge.
5.3
Variations of the Proposed Approach
Basic approach: Weighting outputs.
In Sec-
tion 3, we present a basic version of our proposal
for taking advantage of external knowledge. Our
basic approach involves weighting outputs as indi-
vidual units and using a voting mechanism to se-
lect the best-supported prediction. We can also di-
rectly choose the best-supported output, which in-
cludes both an explanation and a prediction, with-
out using voting.
For example, in the running
example of “Did Aristotle use a laptop?”
(see
more in Section 3), the third reasoning path R3 is
the output most supported by the knowledge para-
graphs K1 and K2.
Variant I: Fact selection.
The ﬁrst variant of
our approach involves selecting facts from the out-
puts of LLMs based on external knowledge. For
example, consider the running example of “Did
Aristotle use a laptop?”, where we only have ac-
cess to the ﬁrst two reasoning paths, R1 and R2.
In this case, the ﬁrst sentence in R2 and the sec-
ond sentence in R1 are supported by knowledge
K1 and K2, respectively. Therefore, the ﬁrst vari-
ant would output the ﬁrst sentence in R2 and the
second sentence in R1 as the supporting facts.
Variant II: Fact generation.
The second vari-
ant of our approach involves generating facts
based on both the outputs of LLMs and external
knowledge. For example, consider the running ex-
ample of “Did Aristotle use a laptop?”, where we
only have access to the ﬁrst reasoning path R1.
The second sentence in R1 is supported by the sec-
ond knowledge paragraph K2. However, the ﬁrst
sentence is not supported by any evidence para-
graphs. We can generate questions about the ﬁrst
sentence, such as “When did Aristotle die?” and
use the ﬁrst knowledge paragraph K1 to generate
a new fact: “Aristotle died in 322BC.”. As a result,
the second variant would output the generated fact
“Aristotle died in 322 BC.” and the second sen-
tence in R1 as the supporting facts.
Inference with supporting facts.
For the two
variants of our approach, we only have the sup-
porting facts and need to perform a ﬁnal inference
step to obtain the corresponding prediction. One
option for this inference is to use LLMs, but they
can be costly (Brown et al., 2020) or difﬁcult to
use (Zhang et al., 2022). An alternative is to use an
off-the-shelf model for inference with supporting
facts, such as UniﬁedQA (Khashabi et al., 2020,
2022). As discussed in Appendix A.5, UniﬁedQA
is more robust to noisy supporting facts than GPT-
3. We thus use the second version of UniﬁedQA,
UniﬁedQA-v2 (Khashabi et al., 2022), for the ﬁnal
step of inference.
Experimental settings.
In this part, we focus
on commonsense reasoning and use the evidence
paragraphs provided in StrategyQA as the rele-
vant knowledge, rather than the retrieved para-
graphs discussed in Section 4.2. To evaluate the
quality of the explanations, we adopt the best met-
ric for factual consistency evaluation in Honovich

1.3B
2.7B
6.7B
13B
30B
175B
Model Size
0
20
40
60
80
Accuracy (%)
Chain-of-thought prompting
Rethinking with retrieval
(a) Accuracy of predictions
1.3B
2.7B
6.7B
13B
30B
175B
Model Size
20
25
30
35
40
45
50
55
Factuality (%)
Chain-of-thought prompting
Rethinking with retrieval
(b) Faithfulness of explanations
Figure 2: The effect of LM size on the performance of our proposed method (Variant II) and CoT prompting. We
use various sizes of OPT models, with the exception of the 175B model, which is GPT-3.
Methods
Accuracy (%)
Faithfulness (%)
CoT prompting
65.94
38.73
Basic (w/o voting)
76.86
50.02
Variant I
78.60
54.11
Variant II
78.60
54.54
Table 5: Comparison of various variations of RR and
the CoT prompting baseline on StrategyQA using evi-
dence paragraphs.
et al. (2022). For simplicity, we use the pre-trained
NLI model released by Nie et al. (2020) to com-
pute the NLI-based metric, rather than ﬁne-tuning
T5-11B (Raffel et al., 2020) ourselves. The imple-
mentation details of the two variants can be found
in Appendix A.4.
Results.
Table 5 illustrates that the fact selec-
tion and fact generation variants of our proposal
improve the faithfulness of the supporting facts in
explanations, leading to increased prediction ac-
curacy compared to the basic approach without
voting. Across all variations of our proposal, we
observe signiﬁcant improvements in both predic-
tion accuracy and the faithfulness of explanations
when compared to the CoT prompting baseline.
The incorporation of a voting mechanism leads
to an increased prediction accuracy of 79.91% for
the basic approach. Comparison with the perfor-
mance (i.e., 77.73%) of the same approach us-
ing retrieved paragraphs rather than evidence para-
graphs in Table 1 demonstrates that retrieved para-
graphs are also effective for our proposal, as both
signiﬁcantly outperform the voting baseline, self-
consistency (i.e., 73.36%), as shown in Table 1.
It is noteworthy that UniﬁedQA performs
poorly on StrategyQA, achieving an accuracy of
only 58.95%.
However, when provided with
gold supporting facts in StrategyQA, UniﬁedQA
demonstrates excellent performance with an accu-
racy of 90.83%. This suggests that UniﬁedQA is
suitable for last-step inference, but not effective
for answering questions in StrategyQA.
5.4
Impact of the Size of LMs
In this subsection, we examine the effect of the
size of LMs on the performance of our proposed
method, speciﬁcally in the context of the fact gen-
eration variant. We compare the performance of
our method using various sizes of OPT models
(Zhang et al., 2022) in addition to GPT-3 (175B)
using the same experimental setup as in Sec-
tion 5.3.
As shown in Figure 2, our proposed
method (Variant II) consistently outperforms CoT
prompting in terms of both prediction accuracy
and the faithfulness of explanations, even when
using smaller LMs.
6
Conclusion
In conclusion, the proposed approach is a promis-
ing solution for utilizing external knowledge to as-
sist LLMs. Unlike traditional methods, RR does
not require additional training or ﬁne-tuning, mak-
ing it a lightweight and feasible option for LLMs.
Through extensive experiments on three reason-
ing tasks using GPT-3, we have shown that RR is
able to produce more faithful explanations and im-
prove the performance of LLMs. In the future, we
plan to investigate various variations of RR to en-
hance its effectiveness and efﬁciency in augment-
ing LLMs with external knowledge.

References
Kurt Bollacker, Colin Evans, Praveen Paritosh,
Tim Sturge, and Jamie Taylor. 2008. Freebase:
a collaboratively created graph database for
structuring human knowledge. In Proceedings
of the 2008 ACM SIGMOD international con-
ference on Management of data, pages 1247–
1250.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Mil-
lican, George van den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al.
2021. Improving language models by retriev-
ing from trillions of tokens.
arXiv preprint
arXiv:2112.04426.
Tom Brown,
Benjamin Mann,
Nick Ryder,
Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, et al. 2020.
Language models are few-shot learners.
Ad-
vances in neural information processing sys-
tems, 33:1877–1901.
Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al.
2022. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christo-
pher Hesse, and John Schulman. 2021. Training
veriﬁers to solve math word problems. arXiv
preprint arXiv:2110.14168.
Ido
Dagan,
Oren
Glickman,
and
Bernardo
Magnini. 2005.
The pascal recognising tex-
tual entailment challenge. In Machine learning
challenges workshop, pages 177–190. Springer.
Daniel Deutsch, Tania Bedrax-Weiss, and Dan
Roth. 2021.
Towards question-answering as
an automatic metric for evaluating the content
quality of a summary. Transactions of the Asso-
ciation for Computational Linguistics, 9:774–
789.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding.
In Proceedings of the 2019
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 4171–4186.
Alexander R Fabbri, Chien-Sheng Wu, Wenhao
Liu, and Caiming Xiong. 2021.
Qafacte-
val:
Improved qa-based factual consistency
evaluation for summarization.
arXiv preprint
arXiv:2112.08542.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar
Khot, Dan Roth, and Jonathan Berant. 2021.
Did aristotle use a laptop? a question answer-
ing benchmark with implicit reasoning strate-
gies. Transactions of the Association for Com-
putational Linguistics, 9:346–361.
Liangke Gui, Borui Wang, Qiuyuan Huang, Alex
Hauptmann, Yonatan Bisk, and Jianfeng Gao.
2021.
Kat: A knowledge augmented trans-
former for vision-and-language. arXiv preprint
arXiv:2112.08614.
Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and
Vivek Srikumar. 2020. Infotabs: Inference on
tables as semi-structured data.
In Proceed-
ings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, pages
2309–2324.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong
Pasupat, and Mingwei Chang. 2020. Retrieval
augmented language model pre-training. In In-
ternational Conference on Machine Learning,
pages 3929–3938. PMLR.
Or Honovich, Roee Aharoni, Jonathan Herzig,
Hagai Taitelbaum, Doron Kukliansy, Vered Co-
hen, Thomas Scialom, Idan Szpektor, Avinatan
Hassidim, and Yossi Matias. 2022. True: Re-
evaluating factual consistency evaluation.
In
Proceedings of the Second DialDoc Workshop
on Document-grounded Dialogue and Conver-
sational Question Answering, pages 161–175.
Or Honovich, Leshem Choshen, Roee Aharoni,
Ella Neeman, Idan Szpektor, and Omri Abend.
2021.
Q2::
Evaluating factual consistency
in knowledge-grounded dialogues via question
generation and question answering.
In Pro-
ceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 7856–7870.

Zhen
Jia,
Abdalghani
Abujabal,
Rishiraj
Saha Roy,
Jannik Strötgen,
and Gerhard
Weikum. 2018. Tempquestions: A benchmark
for temporal question answering. In Compan-
ion Proceedings of the The Web Conference
2018, pages 1057–1062.
Mandar Joshi, Kenton Lee, Yi Luan, and Kristina
Toutanova. 2020.
Contextualized representa-
tions using textual encyclopedic knowledge.
arXiv preprint arXiv:2004.12006.
Vladimir Karpukhin, Barlas Oguz, Sewon Min,
Patrick Lewis, Ledell Wu, Sergey Edunov,
Danqi Chen, and Wen-tau Yih. 2020.
Dense
passage retrieval for open-domain question an-
swering. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 6769–6781.
Urvashi Khandelwal, Omer Levy, Dan Juraf-
sky, Luke Zettlemoyer, and Mike Lewis. 2020.
Generalization through memorization: Nearest
neighbor language models.
In International
Conference on Learning Representations.
Daniel Khashabi, Yeganeh Kordi, and Hannaneh
Hajishirzi. 2022. Uniﬁedqa-v2: Stronger gen-
eralization via broader cross-format training.
arXiv preprint arXiv:2202.12359.
Daniel Khashabi,
Sewon Min,
Tushar Khot,
Ashish Sabharwal, Oyvind Tafjord, Peter Clark,
and Hannaneh Hajishirzi. 2020.
Uniﬁedqa:
Crossing format boundaries with a single qa
system. In Findings of the Association for Com-
putational Linguistics:
EMNLP 2020, pages
1896–1907.
Takeshi Kojima, Shixiang Shane Gu, Machel
Reid, Yutaka Matsuo, and Yusuke Iwasawa.
2022. Large language models are zero-shot rea-
soners. arXiv preprint arXiv:2205.11916.
Mojtaba Komeili, Kurt Shuster, and Jason Weston.
2022. Internet-augmented dialogue generation.
In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers), pages 8460–8478.
Patrick Lewis, Ethan Perez, Aleksandra Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
Yih, Tim Rocktäschel, et al. 2020. Retrieval-
augmented generation for knowledge-intensive
nlp tasks. Advances in Neural Information Pro-
cessing Systems, 33:9459–9474.
Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,
Jheng-Hong Yang, Ronak Pradeep, and Rodrigo
Nogueira. 2021. Pyserini: A Python toolkit for
reproducible information retrieval research with
sparse and dense representations. In Proceed-
ings of the 44th Annual International ACM SI-
GIR Conference on Research and Development
in Information Retrieval (SIGIR 2021), pages
2356–2362.
Jiacheng Liu,
Alisa Liu,
Ximing Lu,
Sean
Welleck, Peter West, Ronan Le Bras, Yejin
Choi, and Hannaneh Hajishirzi. 2022.
Gen-
erated knowledge prompting for commonsense
reasoning. In Proceedings of the 60th Annual
Meeting of the Association for Computational
Linguistics (Volume 1:
Long Papers), pages
3154–3169.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. 2019.
Roberta:
A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692.
George A Miller. 1995.
Wordnet:
a lexical
database for english.
Communications of the
ACM, 38(11):39–41.
Reiichiro Nakano, Jacob Hilton, Suchir Bal-
aji, Jeff Wu, Long Ouyang, Christina Kim,
Christopher Hesse,
Shantanu Jain,
Vineet
Kosaraju, William Saunders, et al. 2021. We-
bgpt:
Browser-assisted
question-answering
with
human
feedback.
arXiv
preprint
arXiv:2112.09332.
J Neeraja, Vivek Gupta, and Vivek Srikumar.
2021. Incorporating external knowledge to en-
hance tabular reasoning. In Proceedings of the
2021 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, pages
2799–2809.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020.
Adversarial nli: A new benchmark for natu-
ral language understanding. In Proceedings of
the 58th Annual Meeting of the Association for
Computational Linguistics, pages 4885–4901.

Maxwell Nye, Anders Johan Andreassen, Guy
Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz,
Maarten Bosma, David Luan, et al. 2022. Show
your work: Scratchpads for intermediate com-
putation with language models. In Deep Learn-
ing for Code Workshop.
Maxwell Nye, Michael Tessler, Josh Tenenbaum,
and Brenden M Lake. 2021. Improving coher-
ence and consistency in neural sequence mod-
els with dual-system, neuro-symbolic reason-
ing. Advances in Neural Information Process-
ing Systems, 34:25192–25204.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama,
Alex Ray, et al. 2022. Training language mod-
els to follow instructions with human feedback.
arXiv preprint arXiv:2203.02155.
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine
Lee,
Sharan
Narang,
Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
2020. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. Journal
of Machine Learning Research, 21:1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
rev, and Percy Liang. 2016. Squad: 100,000+
questions for machine comprehension of text.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 2383–2392.
Stephen Robertson, Hugo Zaragoza, et al. 2009.
The probabilistic relevance framework: Bm25
and beyond. Foundations and Trends® in In-
formation Retrieval, 3(4):333–389.
Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,
Stephen Roller, Arthur Szlam, and Jason We-
ston. 2022.
Language models that seek for
knowledge:
Modular search & generation
for dialogue and prompt completion.
arXiv
preprint arXiv:2203.13224.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu,
and Tie-Yan Liu. 2020.
Mpnet: Masked and
permuted pre-training for language understand-
ing. Advances in Neural Information Process-
ing Systems, 33:16857–16867.
Robyn Speer, Joshua Chin, and Catherine Havasi.
2017.
Conceptnet 5.5: An open multilingual
graph of general knowledge.
In Thirty-ﬁrst
AAAI conference on artiﬁcial intelligence.
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav
Goldberg, and Jonathan Berant. 2020.
Leap-
of-thought:
Teaching pre-trained models to
systematically reason over implicit knowledge.
Advances in Neural Information Processing
Systems, 33:20227–20237.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie
Baker, Yu Du, et al. 2022. Lamda: Language
models for dialog applications. arXiv preprint
arXiv:2201.08239.
Yerram Varun, Aayush Sharma, and Vivek Gupta.
2022.
Trans-kblstm: An external knowledge
enhanced transformer bilstm model for tabular
reasoning. In Proceedings of Deep Learning In-
side Out (DeeLIO 2022): The 3rd Workshop on
Knowledge Extraction and Integration for Deep
Learning Architectures, pages 62–78.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. Advances in neural in-
formation processing systems, 30.
Denny Vrandeˇci´c and Markus Krötzsch. 2014.
Wikidata: a free collaborative knowledgebase.
Communications of the ACM, 57(10):78–85.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
Le, Ed Chi, and Denny Zhou. 2022.
Self-
consistency improves chain of thought rea-
soning in language models.
arXiv preprint
arXiv:2203.11171.
Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. 2022. Chain of thought prompting elic-
its reasoning in large language models. arXiv
preprint arXiv:2201.11903.
Thomas Wolf, Lysandre Debut, Victor Sanh,
Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf,
Morgan Funtowicz, et al. 2020. Transformers:
State-of-the-art natural language processing. In

Proceedings of the 2020 conference on empir-
ical methods in natural language processing:
system demonstrations, pages 38–45.
Xi Ye and Greg Durrett. 2022. The unreliability
of explanations in few-shot in-context learning.
arXiv preprint arXiv:2205.03401.
Eric Zelikman, Yuhuai Wu, and Noah D Good-
man. 2022. Star: Bootstrapping reasoning with
reasoning. arXiv preprint arXiv:2203.14465.
Susan Zhang, Stephen Roller, Naman Goyal,
Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li,
Xi Victoria Lin, et al. 2022. Opt: Open pre-
trained transformer language models.
arXiv
preprint arXiv:2205.01068.
Denny Zhou, Nathanael Schärli, Le Hou, Ja-
son Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Olivier Bousquet, Quoc Le, and
Ed Chi. 2022. Least-to-most prompting enables
complex reasoning in large language models.
arXiv preprint arXiv:2205.10625.

A
Appendix
In this section, we provide additional details on
our experimental setup. Further information can
be found in our code.
A.1
Detailed Prompts
We adopt the same CoT prompt for commonsense
reasoning (i.e., StrategyQA) as those presented in
Wei et al. (2022). The CoT prompt for tempo-
ral reasoning is provided in Table 6. For tabular
reasoning, we adopt the method of Brown et al.
(2020) for converting NLI into QA for RTE (Da-
gan et al., 2005), and randomly sample 6 examples
from the training data to construct the prompt, as
shown in Table 8. The few-shot prompt utilizes
the same exemplars as the CoT prompt and does
not involve CoT reasoning processes.
A.2
Description of Faithfulness Functions
For a sentence s, we denote its MPNet similarity,
entailment score, and contradiction score as M(s),
E(s), and C(s), respectively. In our experiments,
the corresponding thresholds for these scores are
Tm = 0.5, Te = 0.6, and Tc = 0.99. Given the
entailment scores, contradiction scores, and MP-
Net similarities of all supporting facts (denoted as
S) in the explanation of a reasoning path R, differ-
ent faithfulness functions fKB(·) can be adopted in
different settings as follows:
(1) fKB(R) = P
s∈S[M(s)×(M(s) >= Tm)+
E(s) × (M(s) < Tm) −C(s)]
(2) fKB(R) = P
s∈S[M(s) + E(s)]
(3) fKB(R) = P
s∈S[E(s) × (E(s) >= Te) −
C(s) × (C(s) >= Tc)]
In Section 4, we employ function (1) for com-
monsense and tabular reasoning. For temporal rea-
soning, we use function (2) as the distinct nature of
sentences converted from temporal relations leads
to unreliable contradiction scores. In Sections 5.3-
5.4, we use function (3) for commonsense reason-
ing with evidence paragraphs, as the high quality
of the relevant knowledge negates the need for the
complementary use of the MPNet similarity to im-
prove the entailment score.
A.3
Comparison of Retrieval Systems
For commonsense reasoning, we utilized different
retrieval systems in Karpukhin et al. (2020) to re-
trieve relevant paragraphs from Wikipedia. The
performance of BM25, DPR, and BM25+DPR
were 77.73%, 58.52%, and 77.29%, respectively,
indicating that BM25 is the best choice in our case.
A.4
Implementation Details for the Two
Variants of RR
Fact selection implementation details.
In this
work, we utilize the information present in the top-
ranked output produced by our basic approach as
a guide. To this end, we apply a greedy clustering
algorithm to group the sentences from all outputs
into distinct topic categories based on the cosine
similarity of their MPNet sentence embeddings.
For each fact in the top-ranked output of our ba-
sic approach, we identify the fact with the highest
faithfulness within the same topic group and re-
place it in the output. The faithfulness of a fact is
calculated using the fKB function by replacing the
supporting facts with a single fact.
Fact generation implementation details.
In
this part, we generate questions for the named en-
tities present in each fact of the top-ranked output
produced by our basic approach, and retrieve the
corresponding answers from the evidence para-
graphs using UniﬁedQA. We employ the ques-
tion generation model described in Deutsch et al.
(2021), which has been shown to be more ex-
tractive compared to other models as demon-
strated in Fabbri et al. (2021).
We adopt the
question ﬁltering approach proposed in Honovich
et al. (2021) using an off-the-shelf extractive
QA model (ktrapeznikov/albert-xlarge-v2-squad-
v2 from Hugging Face (Wolf et al., 2020)). We
then use an off-the-shelf model (MarkS/bart-base-
qa2d from Hugging Face) to convert the generated
QA pairs into declarative sentences.
We apply
simple rules based on the entailment and contra-
diction scores of the selected facts from the fact se-
lection variant and the generated declarative sen-
tences to obtain the ﬁnal generated facts.
A.5
Comparison of Different Inference
Methods with Supporting Facts
In our experiments, we utilize UniﬁedQA for the
ﬁnal step of inference in both variants. However,
it is worth noting that GPT-3 could also be used
for this purpose. As shown in Table 7, we observe
that UniﬁedQA performs better at inference with
generated facts, while GPT-3 with CoT prompt-
ing performs better with empty or gold facts. This
suggests that UniﬁedQA is more robust to noisy

Q: who was governor of minnesota when maathaad maathaadu mallige was released?
A: Maathaad Maathaadu Mallige was released on 24 August 2007. Tim Pawlenty served as the 39th gov-
ernor of Minnesota from 2003 to 2011. Thus, Tim Pawlenty was governor of minnesota when maathaad
maathaadu mallige was released. So the answer is Tim Pawlenty.
Q: who was us president during the costa rican civil war?
A: The Costa Rican civil war was a civil war in Costa Rica from 12 March to 24 April 1948. Harry S.
Truman was the 33rd president of the United States, serving from 1945 to 1953. Thus, Harry S. Truman
was us president during the costa rican civil war. So the answer is Harry S. Truman.
Q: who was governor of oregon when the collector was released?
A: The Collector premiered at the Cannes Film Festival on May 20, 1965. Mark Hatﬁeld served as the
29th governor of Oregon from 1959 to 1967. Thus, Mark Hatﬁeld was governor of oregon when the
collector was released. So the answer is Mark Hatﬁeld.
Q: who was governor of oregon when shanghai noon was released?
A: Shanghai Noon was released on May 26, 2000. John Kitzhaber served as the 35th governor of Oregon
from 1995 to 2003. Thus, John Kitzhaber was governor of oregon when shanghai noon was released. So
the answer is John Kitzhaber.
Q: who was us president when john andrew shulze was a teenager?
A: John Andrew Shulze was born on July 19, 1775. A teenager is someone who is between 13 and 19
years old. George Washington served as the ﬁrst president of the United States from 1789 to 1797. Thus,
George Washington was us president when john andrew shulze was a teenager. So the answer is George
Washington.
Q: who was us president during the seventh coalition?
A: The War of the Seventh Coalition was from 20 March to 8 July 1815. James Madison served as the
fourth president of the United States from 1809 to 1817. Thus, James Madison was us president during
the seventh coalition. So the answer is James Madison.
Table 6: The CoT prompt for temporal reasoning.
Methods
Accuracy (%)
Empty facts
GPT-3 (zero-shot)
58.08
GPT-3 (CoT)
65.94
UniﬁedQA
58.95
Gold facts
GPT-3 (zero-shot)
81.66
GPT-3 (CoT)
91.70
UniﬁedQA
90.83
Generated facts
GPT-3 (zero-shot)
69.87
GPT-3 (CoT)
76.42
UniﬁedQA
78.60
Table 7: Comparison of different inference methods on
empty, gold, and generated facts.
inputs compared to GPT-3.
Additionally, both
UniﬁedQA and GPT-3 with CoT prompting signif-
icantly outperform GPT-3 with zero-shot prompt-
ing, indicating that the CoT prompting is also ben-
eﬁcial for the ﬁnal step of inference.

Charles Sumner Tainter was Born on April 25, 1854 ( 1854-04-25 ) Watertown, Massachusetts, U.S..
Charles Sumner Tainter was Died on April 20, 1940 ( 1940-04-21 ) (aged 85) San Diego, California,
U.S.. The Nationality of Charles Sumner Tainter are American. The Known for of Charles Sumner
Tainter are Photophone, phonograph Father Of The Speaking Machine.
Question: Charles Sumner Tainter never left the state of Massachusetts. True or False?
Answer: Charles Sumner Tainter was died in San Diego, California, U.S.. California is a state. Thus,
Charles Sumner Tainter has left the state of Massachusetts. So the answer is false.
The Region of Curitiba are South. The Elevation of Curitiba are 934.6 m (3,066.3 ft). The Density of
Curitiba are 4,062/km 2 (10,523/sq mi). The Metro density of Curitiba are 210.9/km 2 (546.2/sq mi).
Question: Curitiba is above sea level. True or False?
Answer: The elevation of Curitiba are 934.6 m (3,066.3 ft). Elevation is a hypernym of level. Thus,
Curitiba is above sea level. So the answer is true.
Charles (Prince of Wales) was Born on 14 November 1948 ( 1948-11-14 ) (age 70) Buckingham Palace,
London, England. The Spouse of Charles (Prince of Wales) are Lady Diana Spencer ( m. 1981 ; div.
1996 ) , and Camilla Parker Bowles ( m. 2005 ). The Issue of Charles (Prince of Wales) are Prince
William, Duke of Cambridge , and Prince Harry, Duke of Sussex.
Question: Charles was born in 1948 and has been married twice. True or False?
Answer: Charles (Prince of Wales) was Born on 14 November 1948. The Spouse of Charles (Prince of
Wales) are Lady Diana Spencer ( m. 1981 ; div. 1996 ) , and Camilla Parker Bowles ( m. 2005 ). Married
is related to spouse. Thus, Charles was born in 1948 and has been married twice. So the answer is true.
The Born of Idris Elba are 6 September 1972 (age 46) Hackney, London, England. The Residence of
Idris Elba are London. The Other names of Idris Elba are DJ Big Driis, Big Driis the Londoner, Big
Driis, and 7 Dub. The Occupation of Idris Elba are Actor, producer, director, musician, and DJ.
Question: Idris Elba is an English entertainer. True or False?
Answer: The residence of Idris Elba is London. English is related to London. The occupation of Idris
Elba are actor, producer, director, musician, and DJ. Actor is a hyponym of entertainer. Musician is a
hyponym of entertainer. DJ is an entertainer. Thus, Idris Elba is an English entertainer. So the answer
is true.
The Breed of Jean, the Vitagraph Dog are Scotch Collie. The Sex of Jean, the Vitagraph Dog are Female.
The Born of Jean, the Vitagraph Dog are 1902 Eastport, Maine. The Years active of Jean, the Vitagraph
Dog are 1909 - 1916.
Question: Jean, the Vitagraph Dog was a Golden Retriever which perform in circus. True or False?
Answer: The Breed of Jean, the Vitagraph Dog are Scotch Collie. Collie is a hyponym of dog. Retriever
is a hyponym of dog. Thus, Jean, the Vitagraph Dog was not a Golden Retriever which perform in circus.
So the answer is false.
The Studio of Hydrograd are Sphere Studios, North Hollywood, Los Angeles. The Genre of Hydrograd
are Hard rock. The Label of Hydrograd are Roadrunner. The Producer of Hydrograd are Jay Ruston.
Question: Hydrograd is in the rap genre. True or False?
Answer: The Genre of Hydrograd are Hard rock. Rap is distinct from rock. Thus, Hydrograd is not in
the rap genre. So the answer is false.
Table 8: The CoT prompt for tabular reasoning.

