The Visual Computer
https://doi.org/10.1007/s00371-022-02664-2
ORIGINAL ARTICLE
Neural style transfer based on deep feature synthesis
Dajin Li1 · Wenran Gao2
Accepted: 27 August 2022
© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022
Abstract
Neural Style Transfer makes full use of the high-level features of deep neural networks, so stylized images can represent
content and style features on high-level semantics. But neural networks are end-to-end black box systems. Previous style
transfer models are based on the overall features of the image when constructing the target image, so they cannot effectively
intervene in the content and style representations. This paper presents a locally controllable nonparametric neural style transfer
model. We treat style transfer as a feature matching process independent of neural networks and propose a deep-to-shallow
feature synthesis algorithm. The target feature map is synthesized layer by layer in the deep feature space and then transformed
into the target image. Because the feature synthesis is a local manipulation on feature maps, it is easy to control the local
texture structure, content details and texture distribution. Based on our synthesis algorithm, we propose a multi-exemplar
synthesis method that can make local stroke directions better match content semantics or combine multiple styles into a single
image. Our experiments show that our model can produce more impressive results than previous methods.
Keywords Non-photorealistic rendering · Deep neural network · Texture synthesis · Feature synthesis
1 Introduction
Image-based stylization is an important topic in non-
photorealisticrendering(NPR).Traditionalimagestylization
methods generally simulate speciﬁc painting styles using
some algorithms, such as texture synthesis [1–4], image
segmentation [5, 6] and edge-preserving ﬁltering [7, 8], or
simulating painting techniques from the level of a single
stroke [9, 10] (SBR). Most of these methods generate style
textures on local regions using low-level features (such as
color, edges, etc.) in pixel space, so local strokes can be
explicitly intervened. For example, change the geometry
and stroke distribution according to the local image struc-
ture. In NPR applications, this local control is conducive to
improving rendering results and developing interactive paint-
ing systems. However, low-level features fail to represent
high-level image semantics, so the synthesized style textures
B Dajin Li
ldjwqc@163.com
Wenran Gao
Wenrangao@163.com
1
Communication School of Shandong Normal University,
Jinan 250014, China
2
Physics and Electronic School of Shandong Normal
University, Jinan 250014, China
sometimes fail to match the content semantics well. Although
some higher-level features (e.g. saliency [5, 11]) can be used
to guide brush stroke rendering, the similarity measure is still
in low-level feature space. What’s more, it is hard to ﬁnd a
universal algorithm to simulate different styles.
In recent years, with the success of Deep Convolutional
Neural Networks (DCNNs) in the ﬁeld of computer vision,
many researchers have used DCNNs to study image styliza-
tion, which is called Neural Style Transfer (NST) [12]. Based
ondeepfeatures,NSTmethodscreatetargetimagesthatcom-
bine content semantics and style textures by a feed-forward
network [13–15]) or an iterative optimization strategy [12,
16]. Beneﬁting from the powerful semantic representation
of deep features, NST methods can make the target image
match more closely with the content and style at high-level
semantics.
There are two categories of typical NST models: para-
metric and nonparametric models. The parametric model
synthesizes target images based on the global statistical simi-
larity between style and target image features. It can produce
faithful styles, but may produce unreasonable stroke distri-
bution resulting in inaccurate content representation (e.g. the
unnecessary strokes in the sky in Fig. 1a). The nonparametric
model creates target images by local feature matching. So it
can perfectly preserve content semantics but may lose many
123

D. Li, W. Gao
Content & style exemplars                        (a)                                                    (b)                                                 (c)                                              (d)
Fig. 1 When the style exemplar lacks strokes with appropriate orienta-
tions to express content semantics, the parametric model [12] a can get
the faithful style effect but may produce inexact semantics. The non-
parametric model [16] b gives an accurate content description, while the
stroke structure is broken, leading to unfaithful style characteristics. Our
method c can better preserve the stroke structure and content semantics.
The multi-exemplar version of our method d can get a more appealing
result by changing stroke orientations to match the local image structure
style characteristics because the stroke structure is destroyed
(see Fig. 1b). From Fig. 1, we can also ﬁnd that stroke orienta-
tion is another factor that affects the stylization results. When
the stroke orientations are nearly the same in the exemplar,
the content cannot be exactly conveyed due to the lack of
appropriate stroke orientations in the parametric model (see
Fig. 1a). While in the nonparametric model, strokes are bro-
ken into shorter segments and lose style characteristics (see
Fig. 1b). To address these limitations, an NST model must
be able to implicitly or explicitly control local texture struc-
ture, stroke orientations, and stroke distribution according to
semantics.
However, neural networks are black-box systems. We
cannot get the disentangled representations of stroke char-
acteristics, such as orientation, size and texture structure,
etc. Although additional information (such as image edge
[17], image depth [18] and semantic mask [19]) can be used
to guide the overall distribution of textures implicitly, it is
difﬁcult to provide users a meaningful local control during
image stylization. How to control local style textures remains
a challenge for the NST study.
In this study, our purpose is to separate the style creation
from network parameters to enhance local controllability and
improve the stylization quality. Therefore, we do not study
the NST model from the neural network itself, nor simply
improvethetexturedistributionthroughextralossconstraints
as other authors. The main contributions of the study are as
follows:
We propose a new neural style transfer framework in which
the stylization process is independent of the neural network.
It has good local controllability.
We developed a layer-by-layer feature synthesis algorithm
from deep to shallow layers. The algorithm can better pre-
serve the content semantics and the structural integrity of the
style textures (see Fig. 1 (c)).
We extend the proposed synthesis algorithm to a multi-
exemplar version that the target features are synthesized from
different exemplars. Multi-exemplar synthesis can effec-
tively improve the stylization effects, such as changing local
stroke directions according to image semantics (see Fig. 1
(d)) or mixing different styles in a single image.
2 Related work
2.1 Style transfer based on pixel space
The traditional style transfer is called “texture transfer” [1, 2],
which is essentially a constrained texture synthesis method.
Generally, a nonparametric texture synthesis algorithm is
applied to generate style textures on target images. It takes
a style image as the exemplar and determines the sampling
position by estimating local feature similarities between the
target image and the style exemplar. Ashikhmin [1] and Efros
et al. [2] ﬁrstly developed two texture synthesis algorithms
forstyletransferbasedonpixelsamplingandpatchsampling,
respectively. The follow-up studies improve their algorithms
in different ways, such as guidance sampling [20, 21], stroke
direction control [22, 23], multi-scale patch sampling [4, 24],
etc. Another type of style transfer method is a supervised tex-
ture transfer called “image analogy” [25], which requires an
image pair with the same content (a source image and its
artistic version). During the texture transfer, sample pixels
from the artistic image according to the feature similarity
between the source and the target images. There have been
many studies on image analogy [26–29], and it has been
extended to three-dimensional graphics [29, 30], animation
and video stylization [30, 31]. Traditional nonparametric tex-
ture transfer can locally control style textures but only use
low-level image features.
123

Neural style transfer based on deep feature synthesis
2.2 Neural style transfer
NST is a style transfer method based on deep feature space.
The loss function in an NST model contains at least two
items: content loss and style loss. There are two typical style
loss deﬁnitions: parametric deﬁnition using Gram matrix [12,
17] based on a summary statistical measure, and nonparamet-
ric deﬁnition based on local statistical measures [16, 32]. The
parametric method usually produces faithful style effects, but
often leads to an inexact content description. In contrast, the
nonparametric deﬁnition can correctly convey the content,
but could destroy the texture structures and result in unfaith-
ful style effects.
Gu et al. [33] attempted to balance the virtues of both
parametric and nonparametric methods. Based on the para-
metricmodel,theyreshufﬂedfeaturemapstoimprovefeature
matching locally to constrain the texture distribution. Sim-
ilarly, Kolkin et al. [34] proposed a parametric model with
Earth Movers Distance (EMD) to measure style loss, which
can eliminate the unreasonable texture distribution well.
Some authors strengthen content representation by adding
extra constraints to loss functions. For example, image edges
[17, 35] or image depth [18] are used to improve the image
structure, and histogram constraint [36] is used to eliminate
“ghosting” artifacts. Other studies directly manipulated the
target feature maps to improve content representation, such
as feature mixing [37, 38] and feature warping [39]. All these
methods change the texture distribution based on the entire
image and cannot effectively control the local texture gener-
ation.
To correct local patch matching, Zhao et al. [40, 41]
proposed a semantic-guided model to produce semantically-
consistent textures. Gatys et al. [19] presented a simple and
intuitive method for generating multi-scale strokes. They
merge stroke elements of different sizes into one exemplar so
thattherearerichtexturesizestomatchthecontent.However,
it is not easy to create such exemplars in practical applica-
tions. Jing et al. [42] proposed a network architecture with
multiple branches to encode strokes of various scales. The
output stroke sizes can be controlled by decoding the features
of different branches. All these improved models enable the
control of local texture distribution or sizes to a certain extent,
but cannot control texture structure and stroke directions.
Recently, Reimann et al. [43] presented a user-controllable
network architecture and applied a reversible transformation
(e.g. afﬁne transformation) on style features to rotate strokes.
But the authors did not give the solution of auto-matching
between stroke orientations and content semantics. So far,
how to control the local stroke directions and texture struc-
ture in neural style transfer is still an open challenge.
2.3 Real-time neural image reconstruction
There are two image reconstruction approaches in neural
style transfer. One method is iteratively optimizing the tar-
get image based on a pre-trained feature extraction network
[12, 16, 44], this approach is time-consuming, but there is
no style limit. The other way is to train a feed-forward trans-
formation network that can transform an input image into
an artistic image in real time. The transformation network
can be created based on CNN ([13, 14, 42, 45]) or genera-
tive adversarial networks (GAN) [46–48]. This approach is
efﬁcient, but an issue is how to decouple style information
from network parameters to transfer various styles by one
trained network. Some methods for designing multi-style or
arbitrary-style networks have been proposed, such as adap-
tive instance normalization [15], conditional normalization
[49], multi-style learning model [50, 51], image steganog-
raphy [52], WCT transformation [53] and feature wrap [37,
38]. Real-time style transfer networks are not the topic of
this paper, but the proposed feature synthesis is a process
separated from the image reconstruction, which helps design
a real-time transformation network for arbitrary styles.
3 Method overview
Images can be reconstructed from their classiﬁcation feature
maps by an iterative optimization strategy [12, 54] or decoder
networks [55], that is, if a target feature map that combin-
ing content semantics and style textures is created, it can be
transformed into a stylized image. Thus, we can model the
style transfer as a target feature creation process. Inspired
by texture transfer [1], we developed a feature creation algo-
rithm in deep feature space. The target feature map is created
by sampling style features that match the local content struc-
ture. Since the process is similar to texture synthesis, we call
it “Feature synthesis.” The content and style features can
be extracted by a pre-trained classiﬁcation network. In this
study, we use a pre-trained VGG19 network [56] to extract
these feature maps.
Compared to the traditional texture transfer, feature syn-
thesis uses depth features to measure feature similarities,
which can ensure that style textures better match the content
at a higher semantic level. Not only that, deep feature blend-
ing can yield more meaningful results than pixel blending
[16]. The deeper the network layer is, the stronger the image
representation power of its features, so using deeper fea-
tures can synthesize more meaningful style texture patches
to express content semantics. However, deeper layers lose a
lot of detailed information crucial to reconstructing images,
123

D. Li, W. Gao
Xc
Xs
Feature 
synthesis
Xt
Image 
reconstruction
VGG19
Target 
features
Fig. 2 Flowchart of style transfer framework based on feature synthesis.
Xc is the content image, Xs is the style image, Xt is the target image
so the target feature maps for image reconstruction should be
in shallow or middle layers. Considering these reasons, we
developed a layer-by-layer synthesis algorithm from deep to
shallow (discussed in Sect. 4.2).
Based on the above discussion, we designed a style trans-
fer framework based on feature synthesis. As shown in
Fig. 2, ﬁrstly, feed a content image Xc and a style exem-
plar Xs into the VGG network and get their feature maps
of each layer. Then, synthesize the target feature map by
the layer-by-layer synthesis algorithm. Finally, transform
the target feature map into the target image. In all experi-
ments, we use iterative optimization [12] to reconstruct the
target image. The loss function for optimization is follow-
ing:
Loss 
1
HW N
(x) −(xt)
2 + γ Ltv
where (x) is the feature map of the target image being opti-
mized, (xt) is the synthesized target feature map, and H,
W, and N are the height, width, and depth (channels) of the
feature map. Since max pooling of CNNs may lead to spike-
shaped artifacts in the reconstructed image [54], we follow
the prior works [16, 54] and use the total variation regularizer
Ltv to smooth these “spikes.” In our experiments, when the
weight γ is between 0.001 and 0.1, desired results can be
obtained, we set it to 0.01 in all experiments.
Our feature synthesis based on local feature similarity can
ensure that texture features conform to the content seman-
tics. During the feature synthesis process, we can control the
synthesis locally. For example, changing content weights to
inﬂuence local texture structure or content details (discussed
in Sect. 4), sampling multiple exemplars to change stroke
directions, or merging different styles in a single image (dis-
cussed in Sect. 5).
4 Feature synthesis
Traditional texture synthesis algorithms (such as spatial-
coherence synthesis [57], patch-based synthesis [58, 59],
etc.) provide good references for us. Our feature synthesis
Candidate point
Current synthesizing point
Exemplar
Target  image
Completed  region
Completed point
Fig. 3 In the spatially-coherent texture synthesis [57], the current
point’s candidates are derived from its L-neighborhood according to
the location offsets
algorithm is an extension of spatial-coherence synthesis [57],
so we ﬁrst give a brief introduction to the synthesis algorithm.
4.1 Spatial-coherence texture synthesis
The spatial-coherence texture synthesis [57] is a pixel-by-
pixel synthesis method that searches for the best matching
point in an L-shaped neighborhood. For each pixel to be
synthesized, some candidate pixels are determined from the
already synthesized L-neighborhood. Each neighborhood
pixel generates a candidate pixel whose location is shifted
from its original position in the sample (see Fig. 3). For
instance, if the coordinate of the current synthesizing point
in the target image is (20, 20), one pixel of its neighborhood
is located at (19, 21) and sampled from the exemplar at (10,
15), then the candidate pixel should be located at (11, 14)
in the exemplar. The similarity is estimated using the color
distance between two L-shaped neighborhoods: the current
point’s L-neighborhood in the target image and the candi-
date’s L-neighborhood in the exemplar. The candidate with
the smallest color distance should be taken as the sampling
pixel. This method restricts the search space to a local neigh-
borhood, which enables sampling to have location coherence
and thus forms irregular texture blocks. These texture blocks
can better preserve the stroke structure.
4.2 Feature synthesis algorithm
4.2.1 Synthesis pipeline
As shown in Fig. 4, the implementation process of the layer-
by-layer feature synthesis algorithm is as follows:
(1) Taking a deep layer as the initial layer, a greedy algo-
rithm is applied to search the style feature map for the
features matching the content. For each point to be
synthesized, randomly sample one of the best similar
features.
123

Neural style transfer based on deep feature synthesis
Initial layer
Second layer
Target Layer
Style feature map
Content feature map
Up-sampled feature map
Synthesized map 
Upsam-
pling
Feature 
synthesis
Greedy
search 
Upsam-
pling
Fig. 4 Flow chart of the proposed layer-by-layer feature synthesis method
(2) Transmitting the synthesized feature map to the next
layer by upsampling, and obtaining the up-sampled fea-
ture map of the next layer. Upsampling uses the nearest
neighbor method.
(3) Based on the up-sampled feature map of the current
layer, with the content map as the constraint and the style
map as the exemplar, the spatial-coherence synthesis
algorithm is applied to synthesize the feature map of
the current layer.
(4) Repeating steps (2) and (3) until the target layer and
obtaining the ﬁnal target feature map.
Although the greedy search is an inefﬁcient algorithm,
it does not consume much time because the initial layer is
generally deeper, and the size of its feature map has signiﬁ-
cantly reduced. Moreover, GPU Acceleration can be applied
to the neighborhood similarity evaluation by convolutional
computation [16]. To search for candidate points, we used
an auxiliary 2D index array to record the coordinate grid
of sampling locations. Upsampling only doubles the index
array size and the sampling coordinate values using the near-
est neighbor method. The up-sampled feature map of the
next layer is created by copying the feature vectors from
the style feature map according to the coordinate index
array.
At the subsequent layers, feature synthesis is applied on
the up-sampled feature map to generate irregular feature
patches. Compared with [57], we expand the range of can-
didate points. For each feature point to be synthesized, its
candidates also include itself besides those derived from its
L-neighborhood, which can avoid generating some feature
patches seriously mismatched with the content. As the up-
sampled feature map comes from the result of its previous
layer, the synthesis result of the previous layer affects the
candidate derivation of the next layer, which the candidates
match the content at a more abstract level.
4.2.2 Similarity measure methods
Like [16], we use the cosine distance to measure the simi-
larities of neighborhoods. A square neighborhood is used in
the initial layer to search for the best similar features. Let
ψi(x) represent the i-th neighborhood on the feature map x,
and the superscript NB represents the square neighborhood.
Let xc and xs represent the content and style feature maps,
respectively, and S(xs) be the set of all style feature patches
on xs. Greediness searching for the best matching feature can
be formulated as followings:
NN(i)  arg max
j∈S(xs)
 N B
i
(xc) ·  N B
j
(xs)
| N B
i
(xc)|·| N B
j
(xs)|
At the following layers, two distance terms are used to
guide sampling: style distance Es and content distance Ec.
Es leads to the generation of irregular feature blocks; Ec
makes the sampled feature match the content semantics. The
calculation of Ec uses a square neighborhood, while Es uses
anL-shapedneighborhood.LetXt representthetargetfeature
map, and the superscript NL represent the L-neighborhood.
For the current point p and a candidate point k, the content
distance and the style distance are formulated as followings.
Ek
c 
 N B
p
(xc) ·  N B
k
(xs)
| N B
p
(xc)|·| N B
k
(xs)|
Ek
s 
 N L
p
(xt) ·  N L
k
(xs)
| N L
p
(xt)|·| N L
k
(xs)|
The best matching point is the one with the max total
cosine distance of all the candidate points:
NN(p)  arg max
k∈C

wEk
c + (1−w)Ek
s

where C is the candidate set of point p, w is a weight for bal-
ancingcontentmatchingandstylepreservation.Reducingthe
w means that the sampled points are more spatially coherent
with their neighbors and produce larger feature blocks, which
can better preserve the texture structure. Figure 5 shows the
123

D. Li, W. Gao
Style               content                       style                  content
w=0.3                              w=0.5                               w=0.8
Fig. 5 Effecting of different w values on the results. The target feature
maps are synthesized from the “conv4_1” to “conv3_1” layer. The pic-
tures of the middle row use a 7 × 7 L neighborhood, and the bottom
row uses a 5 × 5 L neighborhood
 
 
 
style&                  3×3                            5×5                                    7×7             
content
Fig. 6 Effect of L-neighborhood size on the results
effects of different w values on the synthesis results.
4.2.3 Neighborhood size
From experiments, we ﬁnd that the L-neighborhood size at
the initial layer has little effect on the synthesis results of
subsequent layers, 3 × 3 or 5 × 5 neighborhood is suf-
ﬁcient to get the desired result. At the following layers, a
larger L-neighborhood size means that the feature similarity
is evaluated in a larger region, which helps generate larger
feature blocks. However, this effect will disappear when the
size increases to a certain extent. As shown in Fig. 6, a larger
neighborhood size can produce more complete stroke tex-
tures, but the result of a 7 × 7 neighborhood is not much
changed compared to the 5 × 5. Additionally, a larger neigh-
borhood size will signiﬁcantly increase the calculation time.
In generally, larger neighborhoods are considered only for
 
style&                conv5_1                      conv4_1                    conv3_1
content
Fig. 7 Stylization results obtained when the target layer is “conv3_1”,
and the initial layers are “conv5_1”, “conv4_1” and “conv3_1”, respec-
tively
those styles with larger strokes (such as the ink painting in
Fig. 5). Our experiments show that it is unnecessary to exceed
9 × 9 for most styles.
4.2.4 Initial layer and target layer
Since the neural activations in the “conv3_x” layers of the
VGG19 had good local image representation and the images
reconstructed at these layers had better quality than other
layers. All experiments in this study use the “conv3_1” as the
target layer. Because the feature matching at the initial layer
provides initial candidate ranges for the feature synthesis
of the subsequent layers, the deeper the initial layer is, the
easier it is to form larger feature blocks at the target layer. So
when the target layer is determined, the depth of the initial
layer is also a factor that affects the structural integrity of
the texture (as shown in Fig. 7). Of cause, the initial and the
target layers can be the same layer. Here, there is no cross-
layer upsampling, and the synthesis is performed directly on
the feature map obtained by the greedy search algorithm.
4.3 Local controlling
As discussed above, three factors affect synthesis results,
content weight w, initial layer depth, and L-neighborhood
size. The initial layer and L-neighborhood size inﬂuence the
global style and content representation. The content weight is
a local-controlling factor that can inﬂuence texture integrity,
and we can stylize an image at different levels of detail in
local image regions. Local-region weights can be speciﬁed
by a weight map in which pixel values represent the content
weights of the corresponding pixel locations. A weight map
can be drawn manually or can be other feature maps such
as a saliency map or edge map. Figure 8 shows the results
obtained using constant weight and weight maps.
5 Result evaluation and analysis
To demonstrate the advantages of our method, we quanti-
tatively and subjectively compared our method with other
123

Neural style transfer based on deep feature synthesis
    
    
    
Style & content                 weight w=0.7           controlling by weight map
Fig. 8 Stylization results using constant weights and weight maps
methods in terms of performance and visual effects, respec-
tively.
5.1 Quantitive evaluation
Our quantitative analysis is based on two metrics, average
loss and execution time. The metric data comes from the
experimental results of more than 50 image pairs containing
artwork of more than 20 styles. The size of all content images
is 512 × 512, and that of style images is 352 × 352. Because
most NST models are based on the loss functions of [12] and
[16], we only compare with these two methods.
Average Loss. To quantitatively analyze content and style
representations, we recorded both content and Gram-style
losses at the “conv4_1” and “conv3_1” layers and calculated
the average losses of all image pairs. When the optimization
begins with a randomized image, the variations of the aver-
age losses with iteration steps are shown in Fig. 9. Obviously,
our average style loss is lower than [12, 16], which indicates
that our method has a more powerful Gram-style representa-
tion. This result interprets why our method can obtain more
faithful style effects than [12] in many cases, as discussed
in Sect. 5.2.1. For content loss, our method is lower than
[12] and higher than [16], which also veriﬁes the subjective
evaluation in Sect. 5.2.1.
Execution Time. Whether it is optimization iteration or
training a transformation network to build the target image,
the updating calculation of the GD algorithm is a vital fac-
tor affecting the execution efﬁciency. Method [12] requires
both the content and style loss calculations at multiple layers
               Style loss                                               Content loss
                             
Li et al.[16]
Gatys et al.[12]
Ours
Li et al.[16]
Gatys et al.[12]
Ours
le9
1.2
1.0
0.8
0.6
0.4
0.2
0         500     1000     1500     2000   2500
Iterations
le5
7.0
6.5
6.0
5.5
5.0
4.5
4.0
0    500       1000        1500        2000
Iterations
Fig. 9 Content and style loss curves of three methods
simultaneously. The [16] method does not need to calculate
the Gram-style loss, but requires extra computation to ﬁnd the
best patches. In our method, since the feature map has been
created, we should only calculate the loss of the target layer.
From the loss curves in Fig. 9, we can see that our method
has the fastest convergence speed. From experiments under
the same conditions, we ﬁnd that our method can obtain the
desired result after 280 to 700 iterations, [12] requires about
1200 to 2000 iterations, and [16] requires at least more than
1700 iterations. For our method, the only extra preprocess
is feature synthesis. If stylizing an image of size 512 × 512
with a 352 × 352 style exemplar, using the synthesis algo-
rithm programmed by the C language, synthesizing the target
feature from “conv4_1” to “conv3_1” layer takes about 4.2 s
on an i7/3.6Ghz CPU. If the initial layer is at “conv5_x”, it
takes about 2.9 s. Table 1 shows the average time consump-
tion and the iterative steps required for the three methods in
our experiments. Our methodology is much faster than [12]
and [16].
5.2 Subjective evaluation
There is no valid objective method to evaluate the artistic
effects produced by NST or other NPR models. Some authors
use image saliency to evaluate content representation [60],
but saliency cannot measure visual aesthetic value. We apply
a subjective method to evaluate the visual quality of the styl-
ized image.
We invited 15 expert painters from the Academy of Fine
Arts of Shandong Normal University to rate the stylized
resultsofourmethodandothermethods,includingﬁveteach-
ers and ten graduate students majoring in painting. They were
asked to rate the results of different methods with scores of 0
to 5 based on the content representation, stylistic characteris-
tics, and overall aesthetic effects. The content score measures
whether the result correctly describes the content seman-
tics, the style score measures whether the result faithfully
preserves the texture style of the exemplar, and the overall
aesthetic score measures the aesthetic value from the per-
spective of artistic creation.
123

D. Li, W. Gao
Table 1 Average time
consumption and required
iterations of three methods
Methods
One iteration
Iterations
Preprocess
Total time
Gatys [12]
141 ms
1650
0
239 s
Li [16]
119 ms
1800
0
226 s
Ours
69 ms
420
4310 ms
51 s
5.2.1 Comparison with other CNN-based models
Figure 10 shows the result comparison with the baseline
CNN-based NST methods [12, 16] under different styles. The
charts under the resulting pictures show the average scores
of each method. We can see that our method and [16] get
signiﬁcantly higher content scores than [12]. This is because
the two methods are based on local feature matching that can
reduce the unreasonable distribution of local textures. How-
ever, [16] easily destroys the basic texture structure to match
the content details, thereby reducing the style characteris-
tics. Our feature synthesis can form irregular feature blocks
that can better preserve the structural integrity of textures and
produce more faithful style characteristics. Thus, our method
achieves higher style scores than [16].
Generally,theGram-basedparametricmodel[12]canpro-
duce appealing visual effects. Therefore, Gatys et al. [12] is
usually regarded as the gold-standard method in the commu-
nity of NST [60]. However, our method can produce more
attractive results than [12] in some cases, such as the Chinese
ink painting style (the second and third column pictures of
Fig. 10) and the abstract painting style (the rightmost column
pictures of Fig. 10). This is because the local-measured con-
tent loss and global-measured style loss jointly constrain the
target features in [12], under the premise of minimizing the
total loss, to match some content details, it cannot guarantee
all the local texture structures similar to the style exemplar.
For example, in the regions of the girl’s eyes and nose in
Fig. 10, the stroke structures are different from those in the
style exemplar. However, in our method, the target features
are copied directly from the style feature maps, although
some feature patches may be small, they can partially pre-
serve texture structures owing to the large receptive ﬁeld of
depth features.
Figure 11 is a result comparison with some state-of-the-
art CNN-based methods given the same styles and contents
as Fig. 10. [17] and [14] are Gram-based parametric meth-
ods, while Gu et al. [33] use a hybrid method. Although
these methods improved the content representation to some
extent, they still produce some undesired texture distribu-
tion, such as the sky in the ﬁrst column of pictures and the
background in the third column of pictures in Fig. 10. Kolkin
et al. [34] is an EMD-based parametric method using self-
similarity descriptors to measure content loss, which can
better eliminate undesired texture distribution. Their con-
tent representation is close to our results. In general, these
improved parametric methods are more suitable for the artis-
tic styles with large-scale texture elements, but for those with
medium-scale or small-scale texture elements, their results
are inferior to ours.
5.2.2 Comparison with GAN-based models
Generative Adversarial Networks (GANs) offer an alterna-
tive image construction approach to neural style transfer.
Unlike CNN-based methods, GAN-based models stylize
images by learning the distribution of artistic styles in a com-
petitive training process between two networks (a generator
network and a discriminative network).
Figure 12 shows a comparison with the baseline GAN-
based model (CycleGAN [46]) and the multi-style GAN
model (GatedGAN [50]). In addition to the four styles used
in [46] and [50], the comparison includes the style of Chinese
landscape ink paintings. Since training the networks of the
two models requires two unpaired sample sets, we specially
created an ink painting dataset. The style set (containing 407
pictures) collects the artworks with similar styles drawn by
Chinese painters, and the content set (containing 4769 pic-
tures) comes partly from the dataset of [46] and partly from
the photos of famous mountains and rivers. Our results are
synthesized using 2 to 3 exemplars from the style dataset.
From the overall artistic scores, our scores are higher than
[46] and [50] except for the Ukiyo-e style, where our score
is slightly lower.
5.3 Limitations
The limitation of feature synthesis lies in that, in some cases,
the feature points obtained by the greedy algorithm at the
initial layer may be concentrated in a portion of the style
feature map, so the synthesized feature map cannot fully
capture the style characteristic of the exemplar. For exam-
ple, in the pictures of trees in Figs. 10 and 11, because a
small range of feature points match the leaves, these points
are sampled many times, resulting in repeated synthesis of
some leaf strokes, which makes the result look monotonous.
This limitation also exists in other nonparametric models.
123

Neural style transfer based on deep feature synthesis
Style images                                                                                      content  images
Gatys et 
al.[12]
Li et al.
[16]
Ours
[12]
[16]
Ours
5
4
3
2
1
0
5
4
3
2
1
0
[12]
[16]
Ours
[12]
[16]
Ours
5
4
3
2
1
0
Content score
Style score
Overall score
[12]
[16]
Ours
5
4
3
2
1
0
Subjective scores
5
4
3
2
1
0
[12]
[16]
Ours
Fig. 10 Result comparison with the baseline NST methods (Gatys et al.[12]. and Li et al. [16].). The chart under each style of pictures shows the
average subjective score of different method assessed by 15 painters
6 Multi-exemplar feature synthesis
Sometimes, one style exemplar cannot provide enough stroke
textures to describe the content, which leads to inaccurate
content representation or poor artistic sense. Addressing this
problem requires sampling from different style exemplars to
synthesize the target features. Our single-exemplar synthesis
algorithm can be easily extended to a multi-exemplar version.
In multi-exemplar feature synthesis, using more than one
exemplar to search for matching features, the candidate
points come from different exemplars. Therefore, the sam-
pled exemplar from which each synthesized point comes
should be recorded during the feature synthesis. Based on the
algorithm introduced in Sect. 4.2, if all exemplars are num-
bered, all we should do is adding an exemplar number index
in the location index array. The up-sampled feature maps
are derived from the previous layer, so the exemplar selec-
tion only happens at the initial layer. There are two exemplar
selection strategies, implicit selection via feature matching
and explicit selection by user-assigned information (such as
semantic mask).
In many cases, multi-exemplar feature synthesis can sig-
niﬁcantly improve the stylization result. Below, we introduce
its two primary applications: stroke direction matching and
multi-style mixing.
6.1 Stroke direction matching
Brushstroke is the fundamental painting language, and artists
often describe the structure and appearance of objects
through reasonable stroke directions. Unfortunately, up to
now, deep learning has not been able to get a disentangled
representation of stroke directions, and the previous NST
123

D. Li, W. Gao
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
           
            
            
            
            
            
           
              
             
Li  et al. 
[17]
Wang et 
al.[14]
Gu et 
al.[33]   
Ours
Kolkin et 
al. [34]
Subjective scores
5
4
3
2
1
0
[17]
[14]
Ours
[33] [34]
[17]
[14]
Ours
[33] [34]
[17]
[14]
Ours
[33] [34]
[17]
[14]
Ours
[33] [34]
Content score
Style score
Overall score
[17]
[14]
Ours
[33] [34]
Fig. 11 Comparison with other state-of-the-art methods. The chart below shows the average subjective score of each method assessed by 15 painters
models cannot effectively control the stroke direction. In
our style transfer framework, multi-exemplar feature syn-
thesis can efﬁciently solve this problem. What we should do
is rotate the original style image to generate several copies
with different stroke directions, and use these rotated images
for multi-exemplar synthesis. Feature maps encoded by a
classiﬁcation network certainly contain the information of
brush stroke directions. Therefore, it is unnecessary to spec-
ify explicitly which exemplar to select at the initial layer. We
only should search for the best match points in all exemplars,
and these points must have reasonable directions matching
the content.
Figures 13 and 14 show the stylization results of one-
exemplar and multi-exemplar synthesis. In Fig. 14, three
exemplars are applied, including the original and its rotated
123

Neural style transfer based on deep feature synthesis
Monet 
Chen et al. 
[50]
Ours
Van Gogh 
Cezanne 
Ukiyo-e 
Ink painting
Contents
Zhu et al. 
[46]
Content score
Style score
Overall score
5
4
3
2
1
0
[46]
[50]
Ours
5
4
3
2
1
5
[46]
[50]
Ours
[46]
[50]
Ours
[46]
[50]
Ours
5
4
3
2
1
0
[46]
[50]
Ours
Subjective scores
Fig. 12 Result comparison with GAN-based models ([46] and [50])
     
Content                  (1)                (2)              (3)             (4)
  Using (1)              (1)+(2)                (1)+(2)+(3)       (1)+(2)+(3)+(4)
Style exemplar and its rotated copies
Fig. 13 The inﬂuence of different numbers of exemplars on stylization
results
copies by π/4 and π/2 radians. We can see that multi-
exemplar synthesis yields more impressive results in both
style and content representation. In the single-exemplar case,
many spatially incoherent candidates may be selected to
match the local content because there is no stroke with
suitable orientation. As a result, the integrity of strokes is
destroyed. In the multi-exemplar case, each brush stroke has
several versions in different orientations used for sampling,
which reduces the probability of spatially incoherent sam-
pling and leads to larger texture blocks. This is evident in
Fig. 13 and the styles (b) and (d) in Fig. 14, where the strokes
of one-exemplar synthesis are trivial and short, while those of
three-exemplar are more complete and closer to the original
style.
123

D. Li, W. Gao
Styles
Contents
One exemplar
Three exemplars
Three exemplars
One exemplar
Three exemplars
One exemplar
(a)                                        (b)                                            (c)                                         (d)
Fig. 14 Result comparison between the single-exemplar synthesis and the three-exemplar synthesis. The initial layer is “conv4_1”, and the target
layer is “conv3_1.” The exemplars are the original image and its two rotated copies by π/4 and π/2 radians
123

Neural style transfer based on deep feature synthesis
Style1                                               Content
Style2                                           Result
Fig. 15 Multi-style mixing through implicit exemplar selection
Undoubtedly, it is beneﬁcial to produce faithful styles and
semantics if more rotated copies are used, but it also increases
the computational overhead. How many rotated copies are
used depends on the content image structure and style char-
acteristics. Sometimes, more exemplars are not always better.
For those styles with no clear stroke orientations (e.g. the
oil-painting strokes in Fig. 8) or with enough stroke direc-
tions to match the content image structure (e.g. the two art
styles on the right in Fig. 10), a single exemplar is sufﬁcient.
Figure 13 illustrates the results using one to four directions of
the exemplars. Stylization improves signiﬁcantly when using
the ﬁrst three exemplars, but little improvement when adding
the fourth exemplar. Commonly, for most styles, an exem-
plar group with 2 to 3 directions of copies can produce ideal
results.
6.2 Multi-style mixing
Another application of multi-exemplar feature synthesis is
multi-stylemixing.Weexperimentedwithtwostyleselection
strategies for style mixing, the implicit exemplar selection
through feature matching and the explicit selection by a
semantic mask similar to [19] and [32]. Figure 15 shows the
result of mixing oil (style1) and watercolor (style 2) brush
strokes by implicit exemplar selection. As the plants in the
content image have a better semantic match with the plants
of style 1, style 1 is automatically selected to synthesize the
plants. While the buildings and style 2 are more semanti-
cally matched, style 2 is selected to synthesize the buildings.
Figure 16 is an example of multi-style mixing guided by
semantic masks as other authors [19, 42]. The upper right
picture of the content image is the mask map in which the
Style1                            Style2                        Style3  
Content and mask                                         Result
Fig. 16 Multi-style mixing via explicit exemplar selection. The upper
right picture of the content is the mask map in which the blue region
selects style 1, the green region selects style 2 and the red region selects
style 3
blue region selects style 1, the green region selects style 2
and the red region selects style 3.
7 Conclusions and future work
We proposed a new nonparametric NST framework: Fea-
ture Synthesis Based Neural Style Transfer and developed a
layer-by-layer feature synthesis algorithm. Feature synthesis
in deep feature space can ensure that style features match
the content in high-level semantics and get powerful repre-
sentations of the content and style. As feature synthesis is
a locally-statistical-based manipulation, it is locally control-
lable in content details, texture structure and style mixing.
Experiments show that our model has an excellent perfor-
mance in both content description and style preservation. The
stroke direction controlling is one of the intractable problems
inpreviousNSTmodels,whilethefeaturesynthesiscansolve
the problem well with its multi-exemplar version.
The proposed feature synthesis algorithm is an exten-
sion of the spatial-coherence texture synthesis [57]. But we
believethatsomeothertraditionalnonparametrictexturesyn-
thesis algorithms can also be extended to feature synthesis
(such as [58] and [59]), which is an open problem for future
studies. Feature synthesis is a process independent of neu-
ral networks, which provides high feasibility for designing
arbitrary-style transformation networks. Based on our frame-
work, developing controllable arbitrary-style transformation
networks is another open problem.
123

D. Li, W. Gao
Acknowledgements This work was supported by the National Natural
Science Foundation of China (Project No. 61340019).
Declarations
Conﬂict of interest Dajin Li declares that he has no conﬂict of interest.
Wenran Gao declares that he has no conﬂict of interest.
References
1. Ashikhmin,N.:Fasttexturetransfer.IEEEComput.GraphicsAppl.
23(4), 38–43 (2003)
2. Efros, A.A., and Freeman, W.T.: Image quilting for texture synthe-
sis and transfer. In: Proceedings of ACM Conf. Computer Graphics
and Interactive Techniques (SIGGRAPH) (2001)
3. Elad, M., Milanfar, P.: Style transfer via texture synthesis. IEEE
Trans. Image Process. 26(5), 2338–2351 (2017)
4. Frigo O. et al.: Split and Match: Example-based adaptive patch
sampling for unsupervised style transfer. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 553–561
(2016)
5. Wang, M., et al.: Towards photo watercolorization with artis-
tic verisimilitude. IEEE Trans. Visual Comput. Graphics 20(10),
1451–1460 (2014)
6. Kolliopoulos A. et al.: Segmentation-based 3D artistic render-
ing. In: Proceedings of Eurographics Symposium on Rendering,
361–370 (2006)
7. Winnemller, H.: XDoG: Advanced image stylization with extended
Difference-of-Gaussians. In: Proceedings of Non-Photorealistic
Animation and Rendering (NPAR), (2011)
8. Gao, J., Li, D., Gao, W.: Oil painting style rendering based on
Kuwahara ﬁlter. IEEE Access 7, 104168–104178 (2019)
9. Hertzmann, A.: A survey of stroke-based rendering. IEEE Comput.
Graphics Appl. 23(4), 70–81 (2003)
10. Zeng K. et al. From image parsing to painterly rendering. ACM
Trans. Graphics, 29(1), Article 2 (2009)
11. Dong, L., et al.: Real-time image-based Chinese ink painting ren-
dering. Multimedia Tools Appl. 69(3), 605–620 (2014)
12. Gatys L.A. et al.: Image style transfer using convolutional neural
networks.In:ProceedingsofIEEEConferenceonComputerVision
and Pattern Recognition (CVPR), 2414–2423 (2016)
13. Johnson J. et al.: Perceptual losses for real-time style transfer and
super resolution. In: Proceedings of European Conference on Com-
puter Vision, 694–711 (2016)
14. Wang X. et al.: Multimodal transfer: a hierarchical deep convolu-
tional neural network for fast artistic style transfer. In Proceedings
of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 7178–7186 (2017)
15. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with
adaptive instance normalization. In Proceedings of IEEE Interna-
tionalConferenceonComputerVision(ICCV),Venice,1510–1519
(2017)
16. Li, C., Wand, M.: Combining markov random ﬁelds and convolu-
tional neural networks for image synthesis. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition (CVPR),
2479–2486 (2016)
17. Li et al. S. Laplacian-steered neural style transfer. In Proceedings
of ACM on Multimedia Conference, 1716–1724 (2017)
18. Cheng, M., et al.: Structure-preserving neural style transfer. IEEE
Trans. Image Process. 29, 909–920 (2020)
19. GatysL.A.et al.Controllingperceptual factorsinneural style trans-
fer. In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 3730–3738 (2017)
20. Yamaguchi S. et al.: Region-based painting style transfer. ACM
SIGGRAPH Asia 2015 Technical Briefs, Article no. 8 (2015)
21. Fišer J. et al.: Example-based synthesis of stylized facial anima-
tions. ACM Trans. Graph., 36(4), Article 155 (2017)
22. Lee, H., et al.: Directional texture transfer with edge enhancement.
Comput. Graph. 35(1), 81–94 (2011)
23. Wang, B., et al.: Efﬁcient example-based painting and synthesis
of 2D directional texture. IEEE Trans. Visual Comput. Graphics
10(3), 266–277 (2004)
24. Frigo, O., et al.: Video style transfer by consistent adaptive patch
sampling. Vis. Comput. 35(3), 429–443 (2019)
25. Hertzmann, A. et al.: Image analogies. In: Proceedings of ACM
Conf. Computer Graphics and Interactive Techniques (SIG-
GRAPH), 327–340 (2001)
26. BarnesC.etal.PatchTable:Efﬁcientpatchqueriesforlargedatasets
and applications. ACM Trans. Graph., 34(4), Article 97 (2015)
27. Wang, G., et al.: Deringing cartoons by image analogies. ACM
Trans. Graph. 25(4), 1360–1379 (2006)
28. Zhang, W., et al.: Style transfer via image component analysis.
IEEE Trans. Multimedia 15(7), 1594–1601 (2013)
29. Bénard, P. et al. (2013) Stylizing animation by example. ACM
Trans. Graphics, 32(4), Article 119
30. Fišer J. et al.: StyLit: Illumination-guided example-based styliza-
tion of 3D renderings. ACM Trans. Graphics, 35(4), Article 2,
2016)
31. Ondˇrej Jamriška, et al. Stylizing video by example. ACM Transac-
tions on Graphics, 38(4), (2019)
32. Champandard, A.J.: Semantic style transfer and turning two-
bit
doodles
into
ﬁne
artworks.
2016,
arXiv:1603.01768,
[Online].Available: https://arxiv.org/abs/1603.01768
33. Gu, S. et al.: Arbitrary style transfer with deep feature reshufﬂe.
In: Proceedings of IEEE/CVF Conference on Computer Vision and
Pattern Recognition, Salt Lake City, UT, 8222–8231 (2018)
34. Kolkin, N., Salavon, J., Shakhnarovich, G.: Style transfer by
relaxed optimal transport and self-similarity. In: Proceedings
of IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) IEEE (2019)
35. Ye, W., Zhu, X., Liu, Y.: Multi-semantic preserving neural style
transfer based on Y channel information of image. Visual Com-
puter, 1–15. (2022)
36. Risser,E.,etal.:Stableandcontrollableneuraltexturesynthesisand
style transfer using histogram losses. (2017), arXiv:1701.08893,
[Online]. Available: https://arxiv.org/abs/1701.08893
37. Huang, Z., et al.: Style mixer: semantic-aware multi-style transfer
network. Computer Graphics Forum 38(7), 469–480 (2019)
38. Zhang, Y., et al.: A uniﬁed framework for generalizable style trans-
fer: style and content separation. IEEE Trans. Image Process. 29,
4085–4098 (2020)
39. Liao et al. J.: Visual attribute transfer through deep image analogy.
ACM Trans. Graph., 36(4) Article 120 (2017)
40. Zhao, H.H., Zheng, J.H., Wang, Y.N., et al.: Portrait style transfer
using deep convolutional neural networks and facial segmentation.
Comput. Electr. Eng. 85, 106655 (2020)
41. Zhao, H.H., Rosin, P.L., Lai, Y.K., et al.: Automatic semantic style
transfer using deep convolutional neural networks and soft masks.
Vis. Comput. 36, 1307–1324 (2020)
42. Jing, Y. et al.: Stroke controllable fast style transfer with adap-
tive receptive ﬁelds. In: Proceedings of European Conference on
Computer Vision, 244–260 (2018)
43. Reimann, M., Buchheim, B., Semmo, A. et al.: Controlling strokes
in fast neural style transfer using content transforms. Visual Com-
puter (2022)
44. Mahendran, A., Vedaldi, A.: Visualizing deep convolutional neural
networks using natural pre-images. Int. J. Comput. Vision 120,
233–255 (2016)
123

Neural style transfer based on deep feature synthesis
45. Ulyanov, D. et al.: Texture networks: feed-forward synthesis of
textures and stylized images. In: Proceedings of Int. Conference
on Machine Learning (ICML), 1349–1357 (2016)
46. Zhu, J., Park, T., Isola, P., et al.: Unpaired image-to-image transla-
tion using cycle-consistent adversarial networks. In: Proceedings
of IEEE International Conference on Computer Vision (ICCV),
pp. 2242–2251 (2017)
47. Sketch to portrait generation with generative adversarial networks
and edge constraint. Comput. Electr. Eng., 95(10), 107338 (2021)
48. Chen, Y., Lai, Y.K., Liu, Y.J.: CartoonGAN: Generative adversar-
ial networks for photo cartoonization. IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 9465–9474
(2018)
49. Dumoulin V. et al.: A learned representation for artistic style. In:
Proceedings of International Conference on Learning Representa-
tions (ICLR) (2017)
50. Chen, X., Xu, C., Yang, X., et al.: Gated-GAN: Adversarial gated
networks for multi-collection style transfer. IEEE Trans. Image
Process. 28(2), 546–560 (2019)
51. Chen, D., Yuan, L., Liao, J., et al.: Explicit ﬁlterbank learning
for neural image style transfer and image processing. IEEE Trans.
Pattern Anal. Mach. Intell. 43(7), 2373–2387 (2021)
52. Zhang, S., Su, S., Li, L., et al.: CSST-Net: an arbitrary image style
transfer network of coverless steganography. Visusal Computer 38,
2125–2137 (2022)
53. Li Y. et al.: Universal style transfer via feature transforms. In:
Proceedings of Conference and Workshop on Neural Information
Processing Systems (2017)
54. Mahendran, A., Vedaldi A.: Understanding deep image represen-
tations by inverting them. In: Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 5188–5196
(2015)
55. Dosovitskiy, A., Brox, T.: Inverting visual representations with
convolutional networks. In: Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 4829–4837
(2016)
56. Simonyan, K. and Zisserman, A.: Very deep convolutional net-
works for large-scale image recognition. In Proceedings of Int.
Conf. Learn. Represent. 1–14 (2015)
57. Ashikhmin, M.: Synthesizing natural textures. In: Proceedings of.
Symposium on Interactive 3D graphics, 217–226 (2001)
58. Kwatra, V., et al.: Graphcut textures: Image and video synthesis
using graph cuts. ACM Trans. Graphics 22(3), 277–286 (2003)
59. Efros, A., Freeman, W.T.: Image quilting for texture synthesis and
transfer. In: Proceedings of ACM Conf. Computer Graphics and
Interactive Techniques (SIGGRAPH), 341–346
60. Jing Y. et al.: Neural style transfer: A Review. IEEE Trans. Visual.
Comput. Graph., (2019)
Publisher’s Note Springer Nature remains neutral with regard to juris-
dictional claims in published maps and institutional afﬁliations.
Springer Nature or its licensor holds exclusive rights to this article
undera publishingagreement withthe author(s)orotherrightsholder(s);
author self-archiving of the accepted manuscript version of this article
is solely governed by the terms of such publishing agreement and appli-
cable law.
Dajin Li received B.S. degree in
civil engineering from University
of Science and Technology of
Beijing, Beijing, China, in 1995,
and M.S in Computer Science and
Technology from Communication
University
of
China,
Beijing,
China, in 2007. He is currently an
associate professor and graduate
supervisor in the Department of
Digital Media, Shandong Normal
University.
His
research
inter-
ests
include
computer
vision,
computer graphics and digital art.
Wenran Gao received the B.A.
degree in Digital media arts from
the Shandong Normal Univer-
sity. He is currently pursuing
the M.E. degree of Shandong
Normal University. His research
interests include digital image
processing, neural network, and
deep learning.
123

