Engineering Applications of Artificial Intelligence 114 (2022) 105036
Contents lists available at ScienceDirect
Engineering Applications of Artificial Intelligence
journal homepage: www.elsevier.com/locate/engappai
Survey paper
A survey of visual navigation: From geometry to embodied AI
Tianyao Zhang a,b, Xiaoguang Hu a, Jin Xiao a,âˆ—,1, Guofeng Zhang a
a The School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China
b The ShenYuan Honors College of Beihang University, Beihang University, Beijing, 100191, China
A R T I C L E
I N F O
Keywords:
Embodied AI
Learning for navigation
Visual exploration
Visual navigation
A B S T R A C T
The capacity to extract information and comprehend an unseen environment is critical for mobile robots
to navigate. Few surveys has mentioned the combinatorial-non-optimality problem of the traditional visual
navigation methods. As computer vision technology has improved in recent years, visual navigation approaches
have escalated drastically, particularly after the appearance of the CVPR Embodied AI workshop. However, few
studies take these important changes into account. This survey fills this research gap by collecting, analyzing,
and summarizing more than 100 recent papers. The majority of them are published within 5 years and are cited
over 80 times, which provide more credible results. Based on our thorough comparison, this survey categorizes
all visual navigation methods into two styles: geometry style and embodied AI style. This survey examines these
two styles from the perspective of inputâ€“output. In addition, this survey attempts to provide mathematical
formulations for each style. This paper provides a case study to illustrate the methodological paradigm
with greatest potential. This methodological paradigm using photo-realistic simulation in the Embodied AI
style, which could solve the combinatorial-non-optimality problem. Thereafter, this survey discusses several
issues including prosâ€“cons analysis, problem formulation, common framework, task generalization, dynamic
environment consideration, sim-to-real, and inspiring approaches, which are all based on the scholars who
have cited the method. In the last part, challenges and future trends are summarized. This survey would assist
researchers who work on AI-empowered visual navigation systems.
1. Introduction
Visual navigation is a crucial aspect of an automated mobile robotâ€™s
ability to extract information from its surroundings and determine its
activity in an unseen world. In practical application, the variability
and complexity of the real world leads to impractical modeling of the
surrounding environment. Moreover, the unseen world means the prior
information (e.g., GPS, position, map, etc.) is not provided to the robot.
Thus, visual perception and self-decision-making are important and
worthwhile for investigation.
Researchers have often divided the visual navigation system into
five isolated functional parts due to the constraints of a robotâ€™s com-
putational ability and resources: (Yasuda et al., 2020; Desouza and
Kak, 2002): mapping, localization, path planning (global navigation
Gul et al., 2019), locomotion (local navigation Gul et al., 2019), and in-
teraction. Although some researchers have combined several functional
parts (e.g., SLAM Saputra et al., 2018), they still focus on sub-problem
optimization. Nevertheless, the combination of optimal methods for the
sub-problem may not provifr the optimal method for the final prob-
lem. This survey defines this problem as combinatorial-non-optimality.
An example of combinatorial-non-optimality is shown in Fig. 1. The
âˆ—Corresponding author.
E-mail addresses: tianyao@buaa.edu.cn (T. Zhang), xiaoguang@buaa.edu.cn (X. Hu), xiaojin@buaa.edu.cn (J. Xiao), gfzhang@buaa.edu.cn (G. Zhang).
1 This work was supported in part by the National Natural Science Foundation of China (No. KZ73096202).
optimal variable of the function ğ‘¦1 or ğ‘¦2 are not the optimal variable
of the function ğ‘¦3, which is the combination of ğ‘¦1 and ğ‘¦2. With the
development of computational ability of the hardware as well as the
visual navigation algorithms, there should be some methods to handle
this combinatorial-non-optimality. However, to the authorâ€™s knowl-
edge, few surveys have already covered this issue. Thus, this survey
investigates the development of visual navigation methods and tries to
analyze those methods while keeping the combinatorial-non-optimality
in mind.
This survey divide the mobile robot navigation system into three
levels: Mission, Goal Task, and Functional Component (as shown in
Fig. 2) based on work of Fallah et al. (2013) (human navigation
system), Desouza and Kak (2002) (map-based, map-building and map-
less methods), Anderson et al. (2018a) (empirical methodology of the
embodied navigation) and Yasuda et al. (2020) (statistics report of
visual navigation). Based on its first-person visual observations, the
robot should move towards a user-specified goal in a static or dy-
namic environment using on-board computing and sensory resources
(typically the RGB images). The robot could be Automated Ground
Vehicles (AGV) or Unmanned Aerial Vehicle (UAV). The foundations
https://doi.org/10.1016/j.engappai.2022.105036
Received 30 January 2022; Received in revised form 3 April 2022; Accepted 3 June 2022
Available online 18 June 2022
0952-1976/Â© 2022 Elsevier Ltd. All rights reserved.

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 1. An example of combinatorial-non-optimality.
of navigation include five functional components: understanding the
environment (mapping), inferring the current location of the robot
(localization), planning a trajectory (planning), and executing an action
(locomotion and interaction).
In the computer vision community, as pointed out by Savva et al.
(2019), the visual navigation problem is also formulated as embodied
AI, which enables action by an embodied agent (e.g., a robot) in a
photo-realistic simulation environment. In other words, the embodied
AI involves end-to-end learning-style approaches deployed in simulated
photo-realistic environments. As pointed out by Ramakrishnan et al.
(2021), embodied AI is a stem of active perception, actively capturing
the necessary visual observations (Bajcsy, 1988). The solutions to the
visual navigation problem have changed dramatically in recent years,
particularly after the appearance of the CVPR Embodied AI workshop2
in 2020.
However, few surveys in the visual navigation domain, to the
authorsâ€™ knowledge, has covered these significant changes. Existing sur-
veys focus more on internal functional components of navigation. They
generally ignore the mission-level of navigation in Fig. 2, such as an-
swering questions (Embodied Question Answering (EmbodiedQA), Das
et al., 2018), following language commands (Visual-Language Naviga-
tion (VLN, Anderson et al., 2018b), Audioâ€“Visual Navigation (Younes,
2022), and rearranging objects (Rearrangement, Batra et al., 2020).
Moreover, they have a noticeable lack of the relationship between
classic visual navigation and embodied AI methods.
1.1. Excellent complementary surveys
This section highlights the differences between our paper and nu-
merous notable research papers. These surveys cover a wide range of
topics and are complementary. The work listed in Table 1 can provide
an overview of the domain to those who are unfamiliar with it. The
â€˜â€˜Fâ€™â€™ in the Table 1 represents the functional component level in Fig. 2,
and the â€˜â€˜Pâ€™â€™ (upper-level) in the table represents the goal task level and
mission level in Fig. 2.
Some articles fall under the purview of classic navigation methods
with little machine learning techniques. Yasuda et al. (2020) have
reviewed the work conducted between 2000 and 2017 limited to three
digital libraries: ACM digital library, IEEE Xplore, and SpringerLink.
They analyzed the statistical information of the published paper on
an autonomous robot. Pandey et al. (2017) have studied various tech-
niques that are used by mobile robots for navigation and obstacle
avoidance. They have classified the mobile robot navigation algorithms
into three categories: deterministic, nondeterministic, and evolution-
ary. Gul et al. (2019) have studied the algorithms in two types: global
2 https://embodied-ai.org/.
navigation (off-line mode for path planning) and local navigation (robot
decides its position, orientation, and locomotion). Tzafestas (2018)
have presented an in-depth discussion of the existing mobile robot
control and navigation methodologies. Huang (2019) described the
Inertial Measurement Unit (IMU) propagation and camera measure-
ment models within the EKF framework, especially in the visual-inertial
navigation (VIN) domain. McGuire (2019) focused on indoor swarm
exploration with a swarm of pocket drones. They gave a detailed
description of low-level control (obstacle avoidance with optical flow)
and high-level control (e.g., search-and-rescue mission). Perumal et al.
(2021) deeply explore crash avoidance and overtaking advice sub-
systems for advanced driver assistance systems. They focus on the
functional components level of visual navigation, while we focus more
on the upper-level purpose of navigation.
Some articles are learning-based and meaningful complementary
materials, although different from ours. Grigorescu et al. (2020) have
studied how researchers use deep learning techniques in autonomous
driving. Tai et al. (2018) have studied learning-based methods to
control the robot, from reinforcement learning to imitation learning.
They pay more attention to algorithm optimization levels instead of
the mapping from observation into action, which is our concern. Janai
et al. (2020) focus on the relationship between computer vision and
autonomous vehicles, while ours concentrates on comparing the imple-
mentation of active perception methods for autonomous vehicles with
vision ability. Roy and Chowdhury (2021) have studied indoor local-
ization with different sensors and machine learning techniques, while
we focus on action policy generation from fewer sensors, typically
RGB cameras. Lowry et al. (2016a) focused on memory methods for
recognizing previously visited places; Crespo et al. (2020) investigated
methods for representing the environment, particularly topological,
cognitive, and semantic maps; and we focus on achieving and using this
memory to compute the next action in an unseen place. Ramakrishnan
et al. (2021) proposed a novel benchmark for evaluating exploration
algorithms consistently. Their work is excellent and meaningful for the
visual exploration task. Our research endeavors to accomplish more
complex task beyond exploration. Ye and Yang (2020) have studied
object goal navigation, especially specified by labels, images, and lan-
guage; Duan et al. (2021) have studied embodied AI from simulators
to research tasks, while we emphasize the implementation difference
of visual navigation methods.
1.2. Contributions
To analyze this relationship between classic visual navigation meth-
ods and embodied AI methods, this survey compares the implementa-
tion details of recent high-cited investigations. The logic structure of
this survey is shown in Fig. 3. Firstly, Section 2 gives a brief description
of implementation comparison and the detailed comparison is shown
in Appendix A. This application-oriented survey thoroughly explores
the implementation of approximately 100 studies through input data,
environment representation, action space, implementation approach,
study constraints as well as the research gap. Secondly, based on
the comparison, this survey categorizes the visual navigation methods
into two styles: geometry style (Section 3) and Embodied AI style
(Section 4), and views them from the aspect of an inputâ€“output view. In
addition, this survey attempts to provide a mathematical formulation
for each style. Thirdly, this study contends that the photo-realistic-level
simulator has the greatest potential, and thus provides a case study for
this method group (Section 4.5). Finally, the discussion is conducted
among the eight aspects (Section 5) .
Our paper contributes to the existing literature in four ways.
1. This application-oriented survey compares the implementation
of visual navigation methods from a variety of perspectives,
including input, representation, action, methods, models, and
so on, all of which are critical in this research topic but have
received little attention.
2

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 2. Navigation system for a mobile robot. There are three levels abilities for robots: mission, goal task, and functional component. The environment includes static and dynamic
elements. The less a human participates, the more automated and intelligent the robot becomes.
Table 1
Existing surveys focus more on internal functional components of navigation (marked by â€˜â€˜Fâ€™â€™). Most of them discuss traditional methods (T) or AI-empowered (A). Moreover, they
have a noticeable lack of the relationship between classic visual navigation methods and the embodied AI. However, ours discusses the functional components and upper-level
purpose (F/P) from traditional to AI-empowered (T/A).
Paper
Year
F/P
T/A
Subject
KeyPoints
Lowry et al. (2016a)
2016
F
A
Visual place recognition
Pure image retrieval; topological maps;
topological-metric maps
Pandey et al. (2017)
2017
F
T
Obstacle avoidance techniques
Planning (soft computing techniques)
Tzafestas (2018)
2018
F
T
Control and navigation
Locomotion (mathematics in traditional controller;
motion control)
Tai et al. (2018)
2018
F
A
Deep network solutions for
learning control in robotics
Deep RL; reality gap; imitation learning
Gul et al. (2019)
2019
F
T
Navigation techniques
Planning (global navigation type, local navigation)
Huang (2019)
2019
F
T
Visual-inertial navigation
Visual-inertial navigation; visual corner features
match.
McGuire (2019)
2019
F
T
Indoor swarm exploration with
pocket drones
Locomotion (algorithm, implementational detail)
Grigorescu et al. (2020)
2020
F
A
Deep learning
Deep learning for perception, localization, planning
and locomotion; deployment
Janai et al. (2020)
2020
F
T/A
Computer vision for autonomous
vehicles
Computer vision for self-driving
Crespo et al. (2020)
2020
F
T/A
Semantic information for
navigation
Represent the environment
Yasuda et al. (2020)
2020
F
T
Autonomous Visual Navigation
Statistical information in traditional methods
Ye and Yang (2020)
2020
P
A
Visual Indoor Navigation (VIN)
object navigation
Roy and Chowdhury (2021)
2021
F
T/A
Indoor localization and navigation
Indoor localization with different sensors
Ramakrishnan et al. (2021)
2021
P
T/A
Visual exploration
Empirical study framework;
impact of sensor noise on exploration;
scale factors that influence performance
Duan et al. (2021)
2021
P
A
Embodied AI: from simulators to
research tasks
Understand the simulators;
main research tasks in embodied AI
Zhu et al. (2021b)
2021
P
A
Deep learning on embodied
navigation
Benchmarks, simulators, real-world, sim-to-real
Shenavarmasouleh et al. (2022)
2022
P
A
Embodied AI for smart cities
Breakdown of Embodied AI; simulators
ours
2022
F/P
T/A
Geometry style and embodied
AI style
Implementation detail comparison;
learning-based method case study;
comments collection;
2. It is the first time to divide the visual navigation methods into
two styles: geometry style and embodied AI style. This survey
keeps the combinatorial-non-optimality problem in mind during
the analysis and covers significant changes in visual navigation
solution paradigms.
3. This paper discusses the problem formulation, common frame-
work, task generalization, dynamic environment consideration,
sim-to-real, and inspired techniques in this subject, based on
a thorough comparison and case study. These provide useful
inspiration and assistance in the development of new active
perception approaches.
4. This survey presents the obstacles, open issues, and future trends,
which is helpful for future research based on the authorsâ€™ re-
marks.
3

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 3. The logic structure of this article.
2. Brief description of comparison
As demonstrated in Appendix A, this section compares the imple-
mentation of visual navigation methods from multiple perspectives:
environment representation type, input data, action space, datasets,
and platform (or simulator), task, method, model detail, and evaluation
metrics. Based on the above-mentioned deep comparison, this survey
divides the methodologies into two categories: geometry style, which
directly relies on geometry information, and embodied AI style, which
uses end-to-end learning-based ways to train the model in a photo-
realistic simulator (utilizing geometry information implicitly). Later
in this part, this survey quickly describes the methods for gathering
researches (Section 2.1) and the outcomes of thorough analysis on these
researches (Section 2.2). Then, for readersâ€™ convenience, the notation
of tables in Appendix A is explained (Section 2.3).
2.1. Methodology
This part elaborates the principle and approach to conducting our
comparison in a convincible way.
The comparison is carried out with the following three questions
in mind:( 1) How can a visual navigation system be designed to
create action on visuals in a human-like sequence? (2) How should the
environment be represented? (3) how to make use of the advantages
of computer techniques? (4) Are there any methods to handle the
combinatorial-non-optimality?
Initially, we constructed a set ğ´that includes over a hundred of the
most significant studies as well as works that have won the PointGoal
Navigation.3 We took â€˜â€˜visual navigationâ€™â€™ as the key word on Google
Scholar and sorted these papers by the cited quantity. The first 100
high-quality papers are selected for set ğ´. Then, we sorted the search
results by the date and selected the 50 newest papers into set ğ´. Based
on the papers in set ğ´, we create a list of related work that authors
mentioned and label it as set ğµ. Then, we add these two sets together
to make a new set called ğ¶= ğ´âˆªğµ. In ğ¶, we compare the article
from several angles, including input, environment representation, agent
action, task, dataset, technique, platform, evaluation metrics, training
detail, test detail, constraint, assumption, contributions, and future
work. We also gather comments from writers and scholars who have
mentioned these works in order to provide suggestions and insights
3 Embodied AI challenges: https://embodied-ai.org/.
for future research. Following that, based on the novelty and the cited
quantity, we chose a subset ğ·âˆ¶ğ·âŠ‚ğ¶that included the most
representative work. In addition, a more complete comparison of paper
in ğ·is shown in Appendix A.
This survey views the visual navigation methods through a unified
lens: the inputs and the outputs. It is motivated by the fact that
every algorithm requires input variables and outputs specific results.
As is known widely, visual navigation is a multi-field intersection
(e.g., Vision, Robotics, NLP) with a variety of goal tasks (e.g., Mapping,
Localization, SLAM, PointGoal Navigation, VLN, EmbodiedQA, and so
on), with approaches ranging from classical to learning-based. Thus,
viewing the visual navigation through the tasks (e.g., Duan et al.,
2021, Yasuda et al., 2020), sensors (e.g., Roy and Chowdhury, 2021),
or image processing (e.g., Janai et al., 2020) only covers partial as-
pects. However, based on the implementation comparison, we believe
that viewing it through the inputs and outputs will give the visual
navigation a more overall structure.
2.2. Grouping algorithms: Geometry style and embodied AI style
This part summarizes the development of visual navigation along
the characteristic of the environment and along the solution of the
functional component. Following that, the algorithms are divided into
two styles.
Along the characteristic of the environment, Desouza and Kak
(2002) divided the visual navigation into indoor navigation and out-
door navigation. Combining the work of Desouza and Kak (2002)
and Yasuda et al. (2020), we divide the work environment of a robot
into the dynamic (indoor and outdoor) and static environment (indoor
and outdoor). As shown in Fig. 2, the static environment could be
divided into known environment (all of the information is provided to
the robot) and structured environment (partially structural information
of the environment is provided). The dynamic environment could be
divided into semi-structured environment (there is a lot of dynamism,
but there is also some predefined characteristic) and unstructured
environment (All of the information including the dynamic object is
unknown to the robot) (Terashima and Hasegawa, 2017; Morioka et al.,
2011). In known, structured, semi-structured, and an unstructured
environment, the uncertainty increases in sequence.
Along the solution of the functional component, the visual nav-
igation technique could be divided into two stages: independent com-
ponent stage and end-to-end stage. The majority of these componentsâ€™
previous work is fragmented (Desouza and Kak, 2002), especially works
4

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 4. Two styles of visual navigation: the geometry style and the embodied AI style. Geometry style methods extract 2D or 3D geometry information to compute essential
inter-information. Different levels of human involvement correspond to different types of visual navigation (e.g., map-based methods utilize a complete map given by humans).
Embodied AI style methods optimize all functional components simultaneously in a photo-realistic simulation environment.
before 2002. Later, several components are studied simultaneously and
it is proofed that fusion several components tend to be more accu-
rate (Fuentes-Pacheco et al., 2015). Simplifying the visual navigation
challenge into manageable components has a significant impact and
yields numerous useful outcomes. However, there are two restrictions
when combining the components to form a basic navigation system. To
begin with, when data is transmitted from one component to another,
calculation error and uncertainty accrue. Second, different methods
and components have varied information formats. The mapping com-
ponentâ€™s output could be a point cloud, grid map, or topological map,
for example. The overall performance would be harmed as a result of
the limitations. To handle the limitations, the end-to-end method comes
into play.
In 1997, Gaussier et al. (1997) used a neural network to map the
image into an action command. This method is similar to solving a
classification problem. After concatenation of the surrounding images,
the robot used a multi-layer network to process the images and output
the turning angle. The dataset with action and images is needed be-
fore navigation. A typical end-to-end method is the work conducted
by Watkins-Valls et al. (2020) in 2020. They used first-person-view
RGB images of their current position and a panoramic view of the
target position as input, trained the network with an imitation learning
framework in a pre-built 3D scan of a real environment, and output the
action command. The robot does not need other information about the
environment, and the network could be generalized to unseen targets
(Watkins-Valls et al., 2020). There are three models in their network:
the auto-encoder model, the goal-checker model, and the policy model.
This method fused localization, mapping, planning, and locomotion.
Researchers recently combined all of the components into a neu-
ral network to form an end-to-end solution, and the performance
outperforms traditional methods (Bansal et al., 2020; Chaplot et al.,
2020a; Watkins-Valls et al., 2020; Ostad-Ali-Askari and Shayan, 2021;
Ostad-Ali-Askari et al., 2017). As demonstrated by Mnih et al. (2015),
end-to-end method is a meaningful and potential solution for policy
learning. They demonstrated that a deep Q-network agent could figure
out the action only from the pixels and the game score. Furthermore,
this agent outperforms all existing algorithms in terms of performance.
This survey divides all visual navigation methods into two styles:
geometry style and embodied AI style, based on the above development
and the detailed comparison in Appendix A. Note that the map-less
methods defined by Desouza and Kak (2002) (in Fig. 4) are viewed as
locomotion part of the navigation process. Compared with embodied
AI methods, the map-less methods focus on pixel-level changes (opti-
cal flow) and prior information (appearance-based match and object
recognition), not long-term mission-level intelligent decision making in
an unseen environment. These two styles will be discussed in greater
depth later.
2.3. Notations
In this part, we construct a lot of notations in comparison, as shown
in Table 2. They are helpful for comprehending Appendix A and later
parts of this survey.
The allocentric map is equal to the metric occupancy map. Here
we use the terminology â€˜â€˜allocentricâ€™â€™ from the work of Henriques and
Vedaldi (2018) to distinguish the â€˜â€˜egocentricâ€™â€™ metric occupancy map
introduced by Gupta et al. (2017).
As shown in Fig. 5, the realism of the simulator is divided into
four levels: real image, photo-realistic, synthetic and poor. Due to the
limitation of pages, we only give several examples of the four levels.
Real image means the method use the images captured in the real world,
e.g., Bojarski et al. (2016) and Gandhi et al. (2017); photo realistic that
uses the images rendered by photo-realistic simulator, e.g., Xia et al.
(2018) and Savva et al. (2019); synthetic means the method uses the
images synthesized by computer, e.g., Sadeghi and Levine (2017); poor
that uses pictures with unreal color and geometric shape, e.g., Mirowski
et al. (2017) and Savinov et al. (2018a).
Methods are categorized into: end-to-end, modular architecture and
classic. The end-to-end means the methods designed to create a network
that directly maps the observation into action without any internal
feature space for representing the environment, e.g., Bojarski et al.
(2016) and Mirowski et al. (2018). The modular architecture means
the methods use several modules to perceive the environment from
observation and then compute the action, e.g., Zhu et al. (2017b) and
Savinov et al. (2018a). The classic means the methods using the classic
pipeline, including mapping, localization, planning and locomotion,
e.g., Campos-MacÃ­as et al. (2020), Foehn et al. (2020) and Falanga et al.
(2020).
5

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 5. An illustration of the realism of the simulator: (A): real image (Mirowski et al., 2018). It almost has no simulation-to-reality gap, but it struggles to respond to each random
action; (B): photo-realistic (Xia et al., 2018). It has a little sim-to-real gap and is suitable for rendering correspondence views for each random action; (C): synthetic (Sadeghi and
Levine, 2017). It has a larger sim-to-real gap, but it is sufficient for policy learning; (D): poor (Savinov et al., 2018b). It has the biggest sim-to-real gap and is used more in the
game domain. Less rendering computation leads to quicker iteration of learning action policies, but increases the sim-to-real gap. More examples are listed in Appendix B.
Table 2
Notations and abbreviations.
Notation
Description
allocentric map
the map in the world coordinates
egocentric map
the map in the local coordinates
real image
using real images for training
photo-realistic
using photo-realistic simulator
synthetic
using images synthesized by computer
poor
using images stacked by simple pixels
end-to-end
mapping images and actions directly
modular architecture
mapping images and actions indirectly
classic
using classic pipeline without network
low-level navigation
steering control, velocity control, etc.
high-level navigation
VLN, NDH, IQA, SOON, etc.
A3C
Asynchronous Advantage Actorâ€“Critic
CNN
Convolutional Neural Network
DQN
Deep Q Network (RL)
DRL
Deep Reinforcement Learning
EKF
Extended Kalman Filter
FCN
Fully Connected Network
GCN
Graph Convolutional Network
GRU
Gated Recurrent Unit
IL
Imitation Learning
LSTM
Long Short-Term Memory
MLP
Multi-Layer Perceptron
PPO
Proximal Policy Optimization (RL)
RL
Reinforcement Learning
SPL
Success rate weighted by Path Length
Tasks could be divided into high-level navigation and low-level
navigation as defined by McGuire (2019). It could also be divided
into global navigation types and local navigation types as defined
by Gul et al. (2019). In the comparison, steering control, collision
avoidance, and velocity control are seen as low-level navigation. Visual
exploration, visual navigation, Visual-Language Navigation (VLN, An-
derson et al., 2018b), Navigation from Dialog History (NDH, Thomason
et al., 2020), Embodied Question Answering (EmbodiedQA, Das et al.,
2018), Interactive Question Answering (IQA, Gordon et al., 2018),
Scenario Oriented Object Navigation (SOON, Zhu et al., 2021a) and
TouchDown (Chen et al., 2019b) are seen as high-level navigation. For
the relationship of these high-level navigation in complexity, readers
could refer to Duan et al. (2021).
The SPL measures the quality of the trajectory planned by the
intelligent robot. It could be defined as follow:
ğ‘†ğ‘ƒğ¿= 1
ğ‘
ğ‘
âˆ‘
ğ‘–=1
ğ‘†ğ‘–
ğ‘™ğ‘–
ğ‘šğ‘ğ‘¥(ğ‘ğ‘–, ğ‘™ğ‘–) ,
where ğ‘represents the number of test episodes, ğ‘†ğ‘–is a binary indicator
of success in episode ğ‘–, the ğ‘ğ‘–represents the length of the actual path in
this episode, and ğ‘™ğ‘–represents the shortest-path distance in this episode
from the starting position to the goal. If only the reachable distance is
shorter than the threshold and the finish signal occurs, the episode is
successful. Anderson et al. (2018a) emphasized that the SPL is a rather
stringent measure and that ğ‘†ğ‘ƒğ¿= 0.5 means a good level of navigation
performance in an unseen environment.
3. Geometry style
This style of algorithm depends on the geometry information and
has been investigated since the 1980s (Desouza and Kak, 2002). The
geometry information here includes the distance, angle, and connectiv-
ity. They focus more on Functional Component (mapping, localization,
planning, locomotion, interaction) in Fig. 2, and each component is
investigated independently because of the limitations of the computa-
tional capability. The geometry style methods explicitly rely the 2D or
3D geometry information. Typically, they extract the 3D coordinates
of feature points that are matched across many image frames. These
feature points could be represented by a point cloud, a grid map, or
a topological map to distinguish the obstacles from the obstacle-free
space. Then, based on the â€˜â€˜mapâ€™â€™ and the localization of the robot,
planning algorithms are utilized to find a safe (or the shortest) path
for navigation. With the constraints of robot dynamics, robot motion
could be computed during the locomotion period. This section briefly
describes the geometry style methods to distinguish the difference
between embodied AI methods . For a more detailed explanation of
geometry style methods, readers could refer to Cadena et al. (2016) for
mapping and localization, Yang et al. (2016) for planning, Mohanan
and Salgoankar (2018) for locomotion, and Kruse et al. (2013) for
interaction.
3.1. Mapping and localization
Mapping and localization are always treated simultaneously to in-
crease the accuracy of mapping and localization (Yasuda et al., 2020)
(e.g., SLAM is a main set of methods for mapping and localization).
Therefore, this subsection brings them together for discussion.
Mapping is the process of creating a map, which is a representation
of the environment. The classification of the environment represen-
tation would be discussed in Section 5. Earlier work creates maps
manually; therefore, they are classified into map-based methods (Des-
ouza and Kak, 2002). Later, the map is created during the navigation
process, and the robot is operated by a human. Thus, these kinds of
methods are passive map-building methods (Desouza and Kak, 2002).
Then, the map is then built automatically by a robot without the
need for human intervention. These are active map-building methods
(Chen et al., 2019a). Besides automation, another research hot-sport is
mapping methods for multi-robot (Romero and Costa, 2010).
Localization is the process of inferring the pose of the robot (cam-
era) with respect to a previous position or a map through visual
observation. As pointed out by Fallah et al. (2013), the localization
methods can be grouped into four different techniques: dead-reckoning,
direct-sensing, triangulation, and pattern-recognition. Visual Odometry
is one of the dead-reckoning methods that estimates robotic localiza-
tion based on a previously estimated or known position (Scaramuzza
and Fraundorfer, 2011). In direct-sensing method, the robot scans the
landmark (artificial landmark, e.g., barcodes and QR codes, or a natural
6

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
landmark, e.g., a predefined building) or measures radio beacons to lo-
calize itself. As is concluded by Taketomi et al. (2017), the relationship
between VO and visual SLAM is:
ğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™ğ‘†ğ¿ğ´ğ‘€= ğ‘‰ğ‘‚+ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ‘šğ‘ğ‘ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›.
McGuire et al. (2019) located the robots and guided them back to the
base station by the received signal strength intensity (RSSI) to a radio
beacon located at the base station. Triangulation-based localization
methods use the locations of at least three known points to determine a
robotic location. For example, Mirowski et al. (2018) used five nearby
fixed landmarks to describe the goal position. The pattern-recognition
method uses computer vision techniques to infer the robotic location.
To identify recent location, Marie et al. (2013) used a 3-dimension
invariant feature computed from the surrounding visual observation as
a place signature .
Mathematically, mapping and localization is the process of estimat-
ing an unknown variable î‰„from a set of measurements ğ‘= {ğ‘§ğ‘˜âˆ¶
ğ‘˜= 1, 2, â€¦ , ğ‘š}. The variable î‰„typically includes the trajectory of the
robot (as a discrete set of poses) and the position of landmarks in the
environment. Each measurement can be expressed as a function of î‰„,
e.g.,
ğ‘§ğ‘˜= â„ğ‘˜(î‰„ğ‘˜) + ğœ–ğ‘˜
(1)
where î‰„ğ‘˜âŠ†î‰„is a subset of the variables, â„ğ‘˜(â‹…) is a known function (the
measurement or observation model), and ğœ–ğ‘˜is random measurement
noise.
This estimation is the Maximum A Posteriori (MAP) Estimation to
estimate î‰„by computing the maximum of posterior ğ‘(î‰„|ğ‘):
î‰„âˆ—= arg max
î‰„
ğ‘(î‰„|ğ‘)
= arg max
î‰„
ğ‘(ğ‘|î‰„)ğ‘(î‰„)
(2)
= arg max
î‰„
ğ‘(î‰„)
ğ‘š
âˆ
ğ‘˜=1
ğ‘(ğ‘§ğ‘˜|î‰„ğ‘˜)
(3)
where the Eq. (2) follows the Bayes theorem, the Eq. (3) comes from the
assumption that the measurements ğ‘are independent, ğ‘(î‰„|ğ‘) is the
belief over X given the measurements, ğ‘(ğ‘|î‰„) is the likelihood of the
measurements ğ‘given the assignment î‰„, and ğ‘(î‰„) is the probability
over î‰„.
Before this estimation, an important procedure of SLAM is data
association: associating each measurement to a specific landmark in
the environment (say, two pixels in consecutive frames are picturing
the same 3-D point). After data association, the transformation of two
robotic position could be calculated through epipolar geometry:
Ì‚ğ±ğ‘‡
1 ğ„Ì‚ğ±0 = Ì‚ğ±ğ‘‡
1 [ğ­]Ã—ğ‘Ì‚ğ±0 = 0
(4)
where Ì‚ğ±ğ‘—are the ray direction vectors of pixel ğ±ğ‘—, ğ­is a translation
vector, ğ‘is a rotation matrix, and ğ„= [ğ­]Ã—ğ‘is called the essential
matrix (Longuet-Higgins, 1981). The detailed derivation of Eq. (4) is
illustrated by Szeliski (2010).
In terms of inputâ€“output, the mapping and the localization take
sensors measurements ğ‘as inputs. The outputs include the position es-
timation of the robot and environment landmarks in a wold coordinate
system.
3.2. Planning
The planning in this article focuses on path planning, which is the
process of designing a path between the current location and the target
location (LaValle, 2006). Kinematic limitations, such as geometric,
physical, and temporal constraints, are taken into account during path
planning. The path needs to be planned in a way that maximizes
usability and success rate while minimizing usersâ€™ chance of getting
lost (Fallah et al., 2013). Motion planning is another concept that is
commonly misunderstood. Because motion planning takes into account
dynamics, it will be classified as locomotion and handled subsequently.
The planning includes five categories: sampling-based algorithms,
node-based optimal algorithms (or grid-based optimal algorithms),
mathematics-model-based algorithms, bio-inspired algorithms, and
multi-fusion-based algorithms (Yang et al., 2016). Lots of researchers
focus on evolutionary algorithms (bio-inspired) to address non-linear or
multi-objective problems in dynamic environments. These evolutionary
algorithms are based on micro-organisms, aquatic life, birds, animals,
and humans (Agarwal and Bharti, 2020). Some researchers focus on
meta-heuristic algorithms to plan a path (Dokeroglu et al., 2019). As
pointed out by Wahab et al. (2020), these meta-heuristic algorithms
include Genetic Algorithms (GA), Differential Evolution (DE), Particle
Swarm Optimization (PSO), Cuckoo Search Algorithm (CSA), etc. Most
of them rely heavily on geometry information, such as the coordinates
of obstacles, obstacle-free area, and the distance between the way-point
and the obstacles.
Mathematically, the planning problem is defined by a triplet
(î‰„ğ‘“ğ‘Ÿğ‘’ğ‘’, ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡, î‰„ğ‘”ğ‘œğ‘ğ‘™) (Yang et al., 2016), where î‰„ğ‘“ğ‘Ÿğ‘’ğ‘’is the obstacle-free
space, ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡is the initial point of the robot in î‰„ğ‘“ğ‘Ÿğ‘’ğ‘’, î‰„ğ‘”ğ‘œğ‘ğ‘™is an open
subset of î‰„ğ‘“ğ‘Ÿğ‘’ğ‘’. The obstacle-free space î‰„ğ‘“ğ‘Ÿğ‘’ğ‘’and the obstacle space
î‰„ğ‘œğ‘ğ‘ make up configuration space î‰„where the robot is operated. The
planning is to find a continuous function ğœ(ğœ) âˆ¶[0, 1] â†’î‰„meets the
following constraint:
ğœ(ğœ) =
{
ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡,
if ğœ= 0
âˆˆî‰„ğ‘”ğ‘œğ‘ğ‘™,
if ğœ= 1.
(5)
If âˆ€ğœâˆˆ[0, ğ‘‡], there is ğœ(ğœ) âˆˆî‰„ğ‘“ğ‘Ÿğ‘’ğ‘’, then the found path ğœ(ğœ) is
collision-free path (Karaman and Frazzoli, 2011).
From an inputâ€“output aspect of view, the planning takes the model
(e.g., grid, node, etc.) of the environment measured during the mapping
process and the self-location measured during the localization process
as inputs. The output of planning is a continuous function ğœ(ğœ), also
known as path.
3.3. Locomotion
The path found during the planning procedure is a geometric path
without time information. To guarantee the kinodynamic feasibility of
a robot, the initial geometric path needs to be parameterized in time,
which is the main objective of locomotion. Thus, the locomotion relies
on the geometric and dynamic information of the robot as well as the
path in the environment. To some extent, the above-mentioned active
map-building methods include decision-making for locomotion.
Locomotion is minimizing an objective function such as total control
cost while satisfying safety and kinodynamic feasibility constraints
(Quan et al., 2020). Locomotion is the physical movement of robots in
the environment (Yasuda et al., 2020). The physical movements include
obstacle avoidance, hallway traversal, junction turning, path deciding,
following, etc. The accomplishment of these movements reflects the
ability to navigate in a dynamic environment. For example, Falanga
et al. (2020) implemented dynamic obstacle avoidance with event
cameras and onboard computation.
Mathematically, the locomotion is computing the control function
ğ‘¢(ğ‘¡) âˆ¶[ğ‘¡0, ğ‘¡ğ‘“] â†’ğ‘ˆ
= {ğ‘¢ğ‘–|ğ‘¢ğ‘šğ‘–ğ‘›â‰¤ğ‘¢ğ‘–â‰¤ğ‘¢ğ‘šğ‘ğ‘¥} that minimizes the
performance function (Mohanan and Salgoankar, 2018; Fiorini and
Shiller, 1995):
ğ½= ğ›·(ğ‘¥ğ‘¡ğ‘“, ğ‘¡ğ‘“) + âˆ«
ğ‘¡ğ‘“
ğ‘¡0
ğ¿(ğ‘¥ğœ, ğ‘¢(ğœ), ğœ)ğ‘‘ğœ,
(6)
subject to constraints:
ğ¹0(ğ‘¥ğ‘¡0, ğ‘¡0) = 0
(7)
ğ¹1(ğ‘¥ğ‘¡ğ‘“, ğ‘¡ğ‘“) = 0
(8)
ğ¹2(ğ‘ ğ‘–, ğ‘¥ğ‘¡, ğ‘¡) = 0,
âˆ€ğ‘ ğ‘–âˆˆî‰„ğ‘œğ‘ğ‘ and âˆ€ğ‘¡âˆˆ[ğ‘¡0, ğ‘¡ğ‘“]
(9)
7

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Ì‡ğ‘¥ğ‘¡= ğ¹3(ğ‘¥ğ‘¡, ğ‘¢(ğ‘¡))
(10)
where ğ›·(ğ‘¥ğ‘¡ğ‘“, ğ‘¡ğ‘“) in Eq. (6) is the performance function at the end of the
time, ğ¿(ğ‘¥ğœ, ğ‘¢(ğœ), ğœ) is the performance function at intermediate time ğœ,
ğ‘¥ğœis the statement (e.g., position, velocity, accelerate, etc.) at time ğœ,
ğ‘¢(ğœ) is the robotâ€™s control input at time ğœ.
Kinematic constraints are represented by Eqs. Eqs. (7)â€“(9). Eq. (7) is
the initial manifold and Eq. (8) is the final manifold. They restrict the
beginning and end of the trajectory. Eq. (9) restricts the robot away
from all the obstacles at any time. Eq. (10) is the robotâ€™s dynamic
constraint.
From the inputâ€“output aspect of view, the locomotion takes the
path generated during the planning procedure as input and outputs the
trajectory with time information based on the kinematic and dynamic
constraints.
3.4. Interaction
Interaction refers to the connection between a robot and a human,
as well as between the robot and its surroundings. It is based on
geometric style methods (e.g., mapping, localization, planning, and
locomotion) or embodied AI style perception of the environment. In-
teraction is more like an assistant procedure for visual navigation. As
a result, this study categorizes interaction as a geometric style in order
to organize the context in a more logical manner.
Interacting with the environment (e.g., opening the door) is an
important ability for robots to execute more complex missions in
unknown environments (Kolve et al., 2017). Interacting with humans
(e.g., feedback information to the operator, and receiving mission
command from the operator) could be achieved through vision, audio,
haptic signals, and so on. For example, Zhang et al. (2019) implemented
voice recognition on the aircraft to allow it to understand the operatorâ€™s
voice command.
From an inputâ€“output aspect of view, the interaction takes human
commands (or robot statements) as input and outputs the respon-
dent action (e.g., opening the door, displaying the statement, moving
following the commands, etc.).
3.5. Summary
With the development of geometry style methods in robot visual
navigation, each component of visual navigation has progressed dra-
matically, such as mapping, localization, and planning. These methods
are applicable and work well on current robots.
However, there are still two main challenges. Firstly, there are
issues about maximum system performance. The geometry style meth-
ods explicitly decompose the visual navigation problem into mapping,
localization, planning, locomotion, and interaction. By optimizing each
component separately, the maximum system performance is not auto-
matically guaranteed (Bojarski et al., 2016). Secondly, there are issues
related to high-level inputs (e.g., images, language, etc.). Most of the
measurements are the coordinates of the robot, obstacles, and target
goal location. Besides, geometry style methods rely heavily on precision
measurements and are sensitive to noise. Thus, high-level inputs, like
image-specified goals, are still a challenge for geometry style methods
(Zhu et al., 2017b). To overcome the challenges, the embodied AI style
was introduced by Zhu et al. (2017b) and Savva et al. (2019).
4. Embodied AI style
Unlike geometry style methods that measure and calculate geometry
information explicitly, embodied AI style takes the advantages of deep
learning and optimizes all components in Fig. 2 simultaneously. To
our knowledge, the first embodied AI style is target-driven visual
navigation that is introduced by Zhu et al. (2017b) in 2017. The
definition of embodied AI is enabling action by an embodied agent
(e.g., a robot) in an environment, which was introduced by Savva et al.
(2019) in 2019. The realism of the simulator has an impact on practical
application. Thus, we classify these methods according to the realism of
the simulation as shown in Fig. 5. For lack of space, we briefly describe
several methods in the latter subsection.
The main idea of embodied AI style methods is training the neural
network in a virtual environment and then deploying the trained model
on the robots that work in the real world. The embodied AI style
methods focus more on goal task level and mission level in Fig. 2. The
mission level includes high-level tasks as mentioned in Section 2.3. Goal
Task Level includes Point Goal, Object Goal, and Area Goal. Anderson
et al. (2018a) distinguishes them by the nature of their goal.
Point Goal means the target of destination is typically specified
by coordinates ğ‘¥ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡(Anderson et al., 2018a). This task is similar
to a planning problem, except that there is no prior knowledge of
obstacle space î‰„ğ‘œğ‘ğ‘ and obstacle-free space î‰„ğ‘“ğ‘Ÿğ‘’ğ‘’. Thus, the Point Goal
navigation problem could be defined by (ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘¥ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡).
Object Goal means the target of destination is specified by a spe-
cific category ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡(Anderson et al., 2018a). The basic classification
method is categorical labels ğ¿ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡. Alternatively, the classification
method could be the image ğ¼ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡and language sentence ğ‘†ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡(Ye and
Yang, 2020). Thus, the object detection ability of robots is important
in Object Goal navigation (Zhang et al., 2020a). The Object Goal
navigation problem could be defined by (ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡), where ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡âˆˆ
{ğ¿ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡, ğ¼ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡, ğ‘†ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡}, the initial localization ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡is randomized.
Area Goal means navigating to an area of a specific category (Ander-
son et al., 2018a). It relies on prior knowledge about the appearance
and layout of different areas. The Area Goal navigation problem can
be defined as (ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡), where ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡âˆˆ{ğ¿ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡, ğ¼ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡, ğ‘†ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡} and the
initial localization ğ‘¥ğ‘–ğ‘›ğ‘–ğ‘¡is randomized. Note that the ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡in Area Goal
specifies an area, not an item as in Object Goal Navigation.
The purpose of Embodied AI methods is to find an observation
encoder ğœ™(â‹…) and a recurrent policy ğ‘“(â‹…) (Desai and Lee, 2021):
â„ğ‘¡= ğ‘“(ğœ™(ğ¼ğ‘¡), ğ‘”ğ‘¡, â„ğ‘¡âˆ’1)
(11)
ğ‘ğ‘¡âˆ¼ğœ‹(â„ğ‘¡) = categorical(ğ‘Šğ‘â„ğ‘¡)
(12)
where ğ¼ğ‘¡is the visual observation (e.g., RGB or RGB-D) at time ğ‘¡, ğ‘”ğ‘¡
is current goal computed from the current pose ğ‘¥ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡and the target
goal ğ‘”ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡âˆˆ{ğ‘ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡, ğ‘¥ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡}, â„ğ‘¡is a hidden state that represents the
navigation memory, and ğ‘Šğ‘is a learned linear layer to extract the
distribution ğœ‹(â‹…|â„ğ‘¡) of action ğ‘ğ‘¡âˆ¼ğœ‹(â‹…|â„ğ‘¡). Usually, ğ‘“(â‹…) is implemented as
a Gated Recurrent Unit (GRU, Cho et al., 2014) and ğœ™(â‹…) is implemented
as a CNN.
4.1. Using real images
Methods utilizing Real Images are compared in Appendix A. Further,
these methods could be divided into three types: end2end, modular and
classic.
Mirowski et al. (2018) represent the environment with a set of
geolocated 360 panoramic images that serve as nodes in an undi-
rected graph. Using fixed landmarks, they accomplished the courier
task, which is navigating to a series of random locations in a city.
Giovannangeli et al. (2006) use biologically inspired place cells, the
DoG (Difference of Gaussian) filter, and Short Term Memory (STM) to
generate a simple set of place-action associations, which enables a robot
to go back to a learned location. Chen et al. (2019b) take panorama
RGB and natural language instructions as input, use LingUNet (Misra
et al., 2018) to encode spatial description and use pretrained ResNet18
(He et al., 2016) to encode the image. They accomplish a task that
is navigating to a place and then identifying the point in the image
that is referred to in the description. Foehn et al. (2020) accomplish a
vision-based drone racing task that involves autonomously navigating
through a sequence of gates with distinct appearances in the correct
order and terminating at a designated finish gate. They use CNN to
figure out the keypoints of the door, but the control strategy and path
planning are done with the help of VIO and the extended Kalman filter
8

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
(EKF). McGuire et al. (2017) propose the Edge-FS method to process
stereo RGB images, control dronesâ€™ velocity, and avoid obstacles. These
methods all process the images captured in the real world.
4.2. Using photo-realistic level simulator
Methods utilizing the photo-realistic level simulator are compared
with implementation details in Appendix A. For lack of space, this
subsection introduces some representative work.
Xia et al. (2018) propose a photo-realistic level simulator, Gibson.
Agent in this virtual environment receives no external odometry or GPS
information, and needs to form a contextual map to succeed. Thus, they
use VGG-16 to encode the RGB images and a PPO to make decisions for
active perceptual tasks (obstacle avoidance, distant visual navigation,
stair climbing) and static-recognition tasks (depth estimation, scene
classification). Das et al. (2018) propose a hierarchical structure for the
navigation module, with a planner that selects actions (directions) and
a controller that decides how far to move following this action. Their
Adaptive Computation Time (ACT) navigator gives out the execution
time of each action. They also present a new AI task â€” Embodied Ques-
tion Answering (EmbodiedQA), where a question and RGB observation
are processed into action commands for agent navigation, and then
the answer is figured out by the agent. Mishkin et al. (2019) compare
the learning-based method to the traditional Point Goal Navigation
method. For the learning-based method, they propose Belief DFP, which
makes the DFPâ€™s black-box policy (Dosovitskiy and Koltun, 2016) more
interpretable by introducing an intermediate map-like representation
in future measurement prediction. For the classic method, they design
classic modular navigation with the help of OpenCV and ORB-SLAM2.
They find that the learning-based method is worse than the classic one,
but their conclusion is overturned by Savva et al. (2019). Gupta et al.
(2017) propose using Cognitive Mapper and Planner (CMP) to encode
panoramic RGB images into egocentric occupancy maps with the help
of pre-trained ResNet50. Their agent could accomplish Point Goal
Navigation and Image-specific Object Goal Navigation. Anderson et al.
(2018b) collect the first benchmark dataset for Vision-and-Language
Navigation, Room-to-Room (R2R). Their mode, Seq2Seq, is based on
LSTM, attention mechanism, and pretrained ResNet-152.
All in all, with the help of a photo-realistic simulator, researchers
could design and test more complex embodied AI tasks, such as Visual
Language Navigation, Embodied Question Answer, Navigation from
Dialog History, Scenario Oriented Object Navigation, and so on.
4.3. Using synthetic-level simulator
Methods utilizing synthetic level simulators are compared with
implementation details in Appendix A. For lack of space, this subsection
introduces some representative work.
Sadeghi and Levine (2017) train the collision avoidance policy in
3D CAD models simulator with the help of DRL and VGG-16. They
demonstrate that simulated mobility policies can be transferred to the
real world. Pan et al. (2017) use synthetic real images as training
data. Their synthetic images make use of segment semantic information
with the help of SegNet (Badrinarayanan et al., 2017). Their agent
generalizes better in a real environment than pure training with vir-
tual data or training with domain randomization. Wu et al. (2021)
propose a new intentional representation for multi-agent vision-based
deep reinforcement learning. Their method improves coordination be-
tween decentralized mobile manipulators and enables the agents with
foraging, search, and rescue ability.
All in all, lots of research focuses on policy learning and reducing
the sim2real gap. Although the RGB images in the simulator are not
photo-realistic, their methods to reduce the sim2real gap are meaning-
ful and inspiring.
4.4. Using poor-level simulator
Methods
utilizing
Poor-Level
simulators
are
compared
in
Appendix A. Further, these methods could be divided into end2end,
modular and classic types.
Mirowski et al. (2017) make use of auxiliary depth prediction and
loop closure classification tasks. They design a network to represent the
environment and they also map RGB, velocity, and reward into an ac-
tion command for goal-directed navigation. Pathak et al. (2018) argue
that different actions may lead to the same desired state. They utilize
the difference between the result of the ground truth and that of the
predicted action, rather than the difference between the ground truth
and the predicted action. Dosovitskiy and Koltun (2016) learn sensory-
motor control by predicting future values of behaviorally relevant
quantities â€“ measurements â€“ via supervised learning. Their action-
expectation network outputs the future measurement predictions for
all actions and future time steps at once. Tobin et al. (2017) create
a network with VGG-16 as the backbone for extracting feature and
locating the object in images. They also study domain randomiza-
tion, which involves randomizing colors, textures, lighting, and camera
poses, to increase the generalization of simulation to the real world.
Henriques and Vedaldi (2018) propose MapNet to process RGB in-
put into allocentric map (world-centric map). Savinov et al. (2018b)
propose semi-parametric topological memory (SPTM), a deep-learning-
based memory architecture for navigation, inspired by landmark-based
navigation in animals. It consists of a (non-parametric) memory graph
where each node represents a location in the environment, and a
(parametric) deep network capable of retrieving nodes from the graph
based on observations. Zhang et al. (2017) propose a successor feature
reinforcement learning (SF-RL) method based on the assumption that
the reward function can be approximately represented as a linear
combination of learned features. The SF-RL decomposes the network
into one that learns the dynamics of the environment separate from the
specified task. Surmann et al. (2020) use hector SLAM (Kohlbrecher
et al., 2011) to process laserâ€™s data into allocentric map. They use
A3C to make decisions and control linear and angular velocities. These
methods process the images generated by the computer with unreal
color and geometric shape.
4.5. Case study of embodied AI
This section dives into recent representative research to describe a
research paradigm. We have reproduced OccAnt, the work of Ramakr-
ishnan et al. (2020) and share the experience and basic paradigm of
this work. This method combines the classic processing pipeline and
learning-based method. Their method is based on an Active Neural
Network (ANS, Chaplot et al., 2020a). This subsection is just the pre-
sentation of an idea. The reproduced results are shown in Appendix C.
Readers could reproduce this work by following the Ramakrishnan
et al. (2020).
4.5.1. Task setup and problem formulation
The main objective is to train a policy (action perception regularity)
that takes in an observation ğ‘œğ‘¡of the environment at each time step ğ‘¡
and outputs an action ğ‘ğ‘¡to navigate.
The main task is visual exploration, where the objective is to max-
imize the coverage within a fixed time budget. The traditional SLAM
method, which is operated by a human, falls under the purview of pas-
sive SLAM (Chen et al., 2019a). However, OccAnt is an active paradigm
where the agent makes a decision by itself. Besides the exploration, the
OccAnt could also accomplish the Point Goal Navigation task and win
the first place in the 2020 Habitat PointNav Challenge (Ramakrishnan
et al., 2020).
There are two core problems: understanding the environment and
making decisions. Ramakrishnan et al. (2020) formulate the former as
9

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 6. The architecture of OccAnt (introduced by Ramakrishnan et al., 2020) for case study. The purpose is to have an insight into embodied AI style methods.
a pixelwise classification task and the latter as a Partially Observable
Markov Decision Process (POMDP).
The action space consists of four actions: moving forward, turning
right, turning left, and stopping. They use spatio-temporal memory
(metric occupancy map) to represent the environment. The photo-
realistic simulator Habitat Savva et al. (2019) with two datasets (Gibson
Xia et al., 2018, Matterport3D Chang et al., 2017) is used to train and
test the model.
4.5.2. Architecture
The network consists of five modules: Mapper, Pose Estimator, Global
Policy, Planner, and Local Policy as shown in Fig. 6. The observation
ğ‘œğ‘¡of environment state ğ‘ ğ‘¡consists of the RGB(D) images and agentâ€™s
position.
The Mapper takes observation ğ‘œğ‘¡and the last world-centric metric
map ğ‘šğ‘¡âˆ’1 as input and updates the new map ğ‘šğ‘¡. The spatial map ğ‘šğ‘¡is a
2Ã—ğ‘€Ã—ğ‘€matrix where ğ‘€donates the size of the map. The first channel
of ğ‘šğ‘¡is a 2D map (ğ‘€Ã— ğ‘€) donating the probability of an obstacle in
each grid. The second channel contains the probability of the location
being explored.
The Pose Estimator takes the sensor pose reading ğ‘¥â€²
ğ‘¡and two egocen-
tric map ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’2âˆ¶ğ‘¡âˆ’1, ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’1âˆ¶ğ‘¡as inputs. It outputs the estimated pose Ì‚ğ‘¥ğ‘¡for
updating the new map.
The Global Policy takes the spatial map ğ‘šğ‘¡, current position and
visited locations as inputs â„ğ‘¡and predicts a long-term goal (ğ‘”ğ‘™
ğ‘¡) using
RL: ğ‘”ğ‘™
ğ‘¡= ğœ‹ğº(â„ğ‘¡|ğœƒğº), where ğœƒğºare the parameters. This Global Policy
is an actorâ€“critic model with multi-convolution layers. The outputs of
the multi convolution layers are the distribution of the goal at each
location and the value estimating the action. This value could be seen
as the expectation of the accumulated future reward, conditional on the
action.
The Planner takes long-term goal ğ‘”ğ‘™
ğ‘¡, current map ğ‘šğ‘¡and estimated
pose Ì‚ğ‘¥ğ‘¡as input. It uses the Fast Marching Method to figure out the path
based on the current observation. The unexplored area is considered
free space. A short-term goal ğ‘”ğ‘ 
ğ‘¡is sampled from the planned path with
a fixed length between the current position and the ğ‘”ğ‘ 
ğ‘¡.
The Local Policy takes the observation ğ‘œğ‘¡and the short-term goal ğ‘”ğ‘ 
ğ‘¡
as inputs and predicts an action ğ‘ğ‘¡= ğœ‹ğ¿(ğ‘œğ‘¡, ğ‘”ğ‘ 
ğ‘¡|ğœƒğ¿), where ğœƒğ¿are the
parameters of the Local Policy. Firstly, ResNet-18 is used to encode
the RGB(D) of observation ğ‘œğ‘¡into 256-dimensional features ğ‘“ğ‘Ÿğ‘”ğ‘. The
word embedding method is used to embed the scalars of position
ğœŒ,angle ğœ‘, and time ğ‘¡into 32-dimensional features ğ‘“ğœŒ, ğ‘“ğœ‘, and ğ‘“ğ‘¡. The
concatenation of these features is fed into the Gate Recurrent Unit
(GRU, Misra et al., 2018). As in Global Policy, the outputs of GRU
are mapped into two parts: the distribution of action and the value of
action.
In ANS, the authors use pre-trained ResNet-18 to extract informa-
tion from RGB images into feature space, and then use fully connected
layers to figure out the egocentric map (ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’1âˆ¶ğ‘¡). The OccAnt goes beyond
the visible cues and anticipates maps for unseen regions to accelerate
navigation. They propose another network beside the ANS to predict
the anticipation ğ‘“ğ‘ğ‘›ğ‘¡. Two features (ğ‘“ğ‘ğ‘›ğ‘¡and ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’1âˆ¶ğ‘¡) are merged by a
UNet.
The ground truth label to train the network of OccAnt is generated
by the ground projection from the depth information provided by
the simulator. Global Policy is trained using Reinforcement Learning
(RL) with the IoU loss (Intersection over Union) to increase coverage.
The Local Policy is trained using Imitation Learning (IL, or behavioral
cloning technique) to imitate the behavior of the oracle agent using the
A-star algorithm with a fully observed map.
4.5.3. Information flow
From the inputâ€“output aspect of view, the information flow of
OccAnt in Fig. 6 could be described as follows.
Visual information ğ‘œğ‘Ÿğ‘”ğ‘(ğ‘‘)
ğ‘¡
of the OccAnt could be RGB, RGB-D or
Depth images. The ğ‘œğ‘Ÿğ‘”ğ‘(ğ‘‘)
ğ‘¡
coupled with the position ğ‘¥â€²
ğ‘¡from odometry
sensor is the begin of the information flow.
The ğ‘œğ‘Ÿğ‘”ğ‘(ğ‘‘)
ğ‘¡
is fed into Mapper to get the egocentric map ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’1âˆ¶ğ‘¡. The
adjacent egocentric maps (ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’2âˆ¶ğ‘¡âˆ’1 and ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’1âˆ¶ğ‘¡) are fed into the Pose
Estimator to get the relative pose change ğ‘‘Ì‚ğ‘¥. The estimated pose Ì‚ğ‘¥ğ‘¡is
computed from ğ‘¥â€²
ğ‘¡and ğ‘‘Ì‚ğ‘¥. The Mapper combines ğ‘ğ‘’ğ‘”ğ‘œ
ğ‘¡âˆ’1âˆ¶ğ‘¡and Ì‚ğ‘¥ğ‘¡to update
the ğ‘šğ‘¡âˆ’1 into ğ‘šğ‘¡.
To let the agent make decisions by itself, the Global Policy takes
the task process information â„ğ‘¡as input and samples the ğ‘”ğ‘™
ğ‘¡from the
distribution. The ğ‘”ğ‘™
ğ‘¡is fed into the Planner to get the ğ‘”ğ‘ 
ğ‘¡. The ğ‘”ğ‘ 
ğ‘¡is
fed into the Local Policy with historical information embedded in the
hidden state of GRU. Then the action ğ‘ğ‘¡is sampled from the distribution
generated by Local Policy.
5. Discussion
5.1. Pros and cons between two styles
This part discusses the main advantages and disadvantages of the
published works. The analysis is based on the detailed comparison
mentioned in Section 2. The results are summarized in Table 3. In
summary, geometry style methods separate the problem into different
sub-problems to simplify it, but they suffer from the combinatorial-non-
optimality problem, noise sensitivity, and error accumulation. Embod-
ied AI style methods handle the combinatorial-non-optimality problem
and reduce the influence of error accumulation. Although the theoret-
ical support of designing a neural network is still an open issue for the
community, the performance of the Embodied AI style is impressive.
Nowadays, the NVIDIA Nano4 could cover the majority computing
needs of the Embodied AI style methods. The computational ability
4 NVIDIA
Jetson
Nano:
https://www.nvidia.cn/autonomous-machines/
embedded-systems/jetson-nano/.
10

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Table 3
Pros and cons between two styles.
Style
Category
Pros
Cons
Geometry
Mapping
(1) generate an accurate representation of the
environment
(1) need depth information;
(2) use sparse feature points;
(3) use less image information
Localization
(1) estimate an accurate position in the map
(1) assume that the map is built or being built;
(2) need the internal and external parameters of the camera
Planning
(1) generate a shortest or safest path to the
destination
(1) assume that the map is provided to the robot;
(2) assume that positions of obstacles, robots, and
destinations are provided in advance;
Locomotion
(1) generate the input for the motion control with
kinematic constraints
(1) rely on the dynamic characteristics of the robot;
(2) assume that the sensors reading and the action execution
are noise free
Interaction
(1) enable intelligent ability;
(2) expand the action space
â€“
â€“
(1) separate the problem into different
sub-problems to simplify it
(1) suffer from the combinatorial-non-optimality problem;
(2) ignore the semantic information;
(3) be sensitive to the noise;
(4) accumulate error along the processing
Embodied AI
Real Images
(1) almost have no simulation-to-reality gap
struggle to respond to each random action
Photo-realistic
(1) have a little sim-to-real gap;
(2) be suitable for rendering correspondence views
for each random action
(1) need more 3D environment datasets;
(2) need powerful simulator to render the scenes
Synthetic
(1) be sufficient for policy learning
(1) have a larger sim-to-real gap
Poor
(1) can be used in the game domain to train the
decision policy
(1) have the biggest sim-to-real gap
â€“
(1) handle the combinatorial-non-optimality
problem by updating the parameters of the
end-to-end network simultaneously;
(2) include the semantic information;
(3) be relatively insensitive to noise;
(4) avoid error accumulation
(1) need more computational ability;
(2) need theoretical support of designing a neural network;
(3) need more datasets of the 3D environment;
(4) need the investigation of transferring the model from
simulation to reality
of the hardware increases day by day, so this would not be the main
challenge of the application of the Embodied AI style methods. Lots
of researchers pay attention to 3D photo-realistic simulation environ-
ments, such as Ai2-Thor (Kolve et al., 2017), Gibson (Xia et al., 2019),
iGibson (Shen et al., 2020), Robothor (Deitke et al., 2020), Habitat Szot
et al. (2021), etc. Some researchers focus on the transferring problem
using domain randomization (Tobin et al., 2017), progressive networks
(Rusu et al., 2017), dynamics randomization (Peng et al., 2018), etc.
Therefore, the disadvantages of the Embodied AI style methods would
not harm the potential of these methods. In particular, the photo-
realistic category has a high probability of becoming the dominant
method in the last few years.
5.2. Representation of an environment
The significance of representing an environment is that it enables
the storing and retrieving of different types of information (Fallah
et al., 2013). After storing the information about the environment, the
intelligent robot could retrieve the information to recognize a visited
place. Representing the environment and spatial perception require
detecting the features of the environment. These features (e.g., line,
edge, corner, and key point) can be seen as landmarks to locate and
guide the robot. Inspired by the survey of human indoor navigation
systems (Fallah et al., 2013), we categorized the representation of
the environment into the physical map and the cognitive map. After
describing these maps, this part discusses the decision-making based
on these maps.
5.2.1. Physical map
On a physical map, to represent the environment, metrical informa-
tion, topological information, and hybrids of these are used. A physical
map could be stored in the format of a grid (matrix), graph, or point
cloud.
A Metrical map contains the distance information of an object
in the real world. The basic metrical map is a grid map proposed
by Moravec and Elfes (1985). The main idea of the grid map is to divide
the environment into cells of equal size. Each cell contains information
indicating whether it is empty or occupied. The occupancy map is a
grid map that contains the probability of being occupied in each cell.
Later, Borenstein and Koren (1989) improved the occupancy map by
incorporating virtual force fields (VFF). A VFF is an occupancy map
where the goal cell has an attractive force on the robot and the occupied
cell has a repulsive force on the robot. Addition and subtraction of
the force follow the principle of vector operation. The direction and
velocity of the robot are decided by the physical model of the robot
and the joint force on the robot. Then the grid map is developed from
2D to 3D. In 2012, Fraundorfer et al. (2012) used images captured by
the onboard camera to generate a 3D global occupancy map. Memory
occupancy and computational cost increase dramatically as the scale of
the environment grows larger. To handle this problem, Fournier et al.
(2007) used multi-resolution octree to optimize the grid map. A 3D
grid (root nodes) could be divided equally into eight volumes (leaf
nodes). And Fournier et al. (2007) used octree structure to represent
this relationship. Eight leaf nodes could be merged into one root leaf
only if all the attributes (e.g., occupied) of the eight leaf nodes are the
same. Using root nodes to represent eight leaf nodes with the same
attributes can decrease the memory requirement. MartÃ­n et al. (2019)
propose an octree-based probabilistic self-localization method that is
computationally affordable. However, they use an a priori known map,
previously built, that keeps information about the environmentâ€™s struc-
ture and color. Another way to represent metrical data is with a point
cloud (Kanellakis and Nikolakopoulos, 2017). Faessler et al. (2016)
used the images captured by the quadrotor to rebuild a 3D point cloud.
A Topological map removes the metrical information and per-
sists the connectivity information to simplify the environment. In
1993, Meng and Kak (1993) used the topological graph to represent the
corridors. Different types of nodes are designed to represent corridors,
11

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Table 4
The comparison of the studies utilizing real images and the studies utilizing poor-level simulator.
Papera
Year
Inputb
RoEc
Actiond
Taske
Dataset
Method
Platform
Metricsf
Giovannangeli et al. (2006)ğ´,ğ‘
2006
p-RGB
Landmark
Direction
ObjectGoal-1
â€“
DoG filter, STM,
behaviorist sensory
Koala, Labo3
-
Bojarski et al. (2016)ğ´,ğ‘
2016
RGB
â€“
value
Steering
control
Human data
CNN, MLP, FCN
DRIVETM PX
Autonomy
Gandhi et al. (2017)ğ´,ğ‘
2017
RGB
â€“
F, L, R
Collision
avoidance
Crashes data
CNN,
pretrained-AlexNet
Parrot
Ar-Drone 2.0
Distance
time
Mirowski et al. (2018)ğ´,ğ‘
2018
RGB
Topological
map
rotate, F
Courier task
StreetLearn
DRL, LSTM,
MultiCityNav
StreetLearn
Reward,
time
Chen et al. (2019b)ğ´,ğ‘
2019
p-RGB,
language
topological
map
F, R, L,
stop
VLN,
spatial
description
TOUCHDOWN
CNN, LSTM,
LingUNet Misra
et al. (2018),
Resnet-18
TOUCHDOWN
SR, SPL,
Accuracy,
distance
Foehn et al. (2020)*ğ´,ğ‘
2020
RGB,IMU
LIDAR
(height)
The global
gate map
Thrust,
angular
velocity
Vision-based
drone racing
28k images
VIO, CNN, EKF,
U-net
Drone
IoU
McGuire et al. (2017)*ğ´,ğ‘
2017
Stereo
RGB
â€“
F, L, R,
hover
Velocity, avoid
obstacles
â€“
Edge-FS
Lisa-MXs
Velocity
error,
depth
Dosovitskiy and Koltun (2016)ğ·,ğ‘
2016
RGBD,
actions
network
F, B, L, R,
run, shoot
PointGoal
â€“
DQN, CNN, DFP,
action-expectation
network
ViZDoom
health,
frag count
Tobin et al. (2017)*ğ·,ğ‘
2017
RGB
â€“
localization
sim2real,
object
localization
YCB Dataset
Domain
Randomization,
VGG-16
Fetch robot
Localiza-
tion
accuracy
Mirowski et al. (2017)ğ·,ğ‘
2017
RGB,
velocity,
reward
Network
Rotate,
F, B
PointGoal
â€“
RL, A3C, LSTM,
CNN, Nav A3C
DeepMind
Lab-Env
AUC,
Score
Pathak et al. (2018)ğ·,ğ‘
2018
RGB
â€“
â€“
Zero-shot
visual
imitation
â€“
Forward
consistency loss,
GSP
TurtleBot
VizDoom
Kempka et al.
(2016)
Distance
Zhang et al. (2017)*ğ·,ğ‘
2017
Depth
network
F, L, R,
still
sim2real,
PointGoal
â€“
Successor feature,
RL, SF-RL-Transfer
Robotino robot
Average
reward
Henriques and Vedaldi (2018)ğ·,ğ‘
2018
RGB or
RGB-D
Allocentric
map
â€“
PointGoal
2D mazes,
Doom, AVD
LSTM, MapNet
â€“
APE
Savinov et al. (2018b)ğ·,ğ‘
2018
RGB
Topological
map
still, F, B,
L, R
ObjectGoal-2
Exploring data
Self supervision,
SPTM, retrieval
Doom
SR
Surmann et al. (2020)*ğ·,ğ‘
2020
Laser,
RGBD,
compass
Allocentric
map
linear and
angular
velocities
PointGoal
â€“
A3C, RL, CNN,
hector slam
Kohlbrecher et al.
(2011)
ROS
SR
aThe upper latter represents the realism of simulator (e.g., A: real image; B: photo-realistic; C: synthetic; D: poor), the lower latter represents the category of method (e.g., a:
end-to-end; b: modular architecture; c: classic), and the mark of â€˜â€˜*â€™â€™, if exist, represents that the method concerns either the action noise or measurement noise.
bp-RGB means panoramic RGB image.
cThe representation method of environment.
dN: Navigate, O: Open, C: Close, Pu: Pick up, P: Put, U: Look up and D: Look Down, F: move forward, B: move backward, L: turn left, R: turn right.
esome task here follow the definitions of Anderson et al. (2018a) (PointGoal, ObjectGoal and AreaGoal represents different type of goal-driven navigation) and Ye and Yang (2020)
(ObjectGoal-1: label-specified Object Goal navigation; ObjectGoal-2: image-specified Object Goal navigation, ObjectGoal-3: language-specified Object Goal navigation).
fAverage Position Error (APE), Goal Progress (GP), Maximum Mean Discrepancy (MMD), Navigation Error (NE), Navigation Length (NL), Oracle Success Rate (OSR), Oracle Path
Success Rate (OPSR), Path Length (PL), Success Rate (SR), and the Success Rate weighted by Path Length (SPL).
junction, and dead ends. Edges between two nodes represent the
connectivity of two locations in the real world. The location of the robot
is represented by a node in the graph. Grisetti et al. (2010) reviewed the
graph-based SLAM, which uses topological map to reduce the memory
occupancy and computation cost. The topological map lacks distance
information, so it is hard to guide the robot directly. To address this
issue, Konolige et al. (2011) used a combination of topological and
metrical maps to represent the environment.
5.2.2. Cognitive map
A Cognitive map is a kind of mental representation of the en-
vironment (Ungar, 2000). This map restores the navigation decision
principle and the navigation knowledge. Compared with a physical
map, which uses artificial features, the cognitive map is a more ab-
stract representation and is more suitable for representing the unseen
environment (Rosano et al., 2020; Chaplot et al., 2020a).
Concept of Cognitive Map. From the point of view of the engineer,
this method of representation is map-less, since a cognitive map is too
abstract to be implemented in a 2D-array or graph. However, from
the standpoint of neural scientists, this is a map that connects visual
observation and knowledge. In 1948, Tolman (1948) proposed the
cognitive map from the research on rats navigating mazes. It contained
information about the relationship between neural activity and the real
world. The discovery of head direction cells in the dorsal presubiculum
and grid cells in the medial entorhinal cortex (MEC) leads to further
understanding of this relationship (Lowry et al., 2016b). Recently,
methods of creating a cognitive map include image-processing-based
and artificial-neural-network-based approaches.
Image Processing Based Cognitive Map. This method utilizes
traditional image processing technology to create a cognitive map
and represent the environment. This kind of method could be divided
12

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
into local feature descriptors, global descriptors, local and global tech-
niques, and 3D information descriptors. Based on these methods, the
intelligent robot could recognize the place (Lowry et al., 2016b). Local
feature descriptor selects parts of the image to extraction, description,
and description. It requires a detection phase first to select features
such corners, lines and key points. Some local features change when the
position of the camera changes. To handle this problem, Lowe (1999)
designed the SIFT (Scale Invariant Feature Transforms) feature, which
is invariant to image scaling, translation, and rotation. The SIFT feature
is extracted from a patch of image and is originally used for object
recognition in an image. Global descriptor does not have a detection
phase but processes the whole image. An example of this method is Gist
(Oliva and Torralba, 2006). Oliva and Torralba (2006) were inspired by
the fact that humans can recognize the gist of a novel image in a single
glance. They divided the whole image into blocks and processed each
block regardless of the content. A typical example of creating a cogni-
tive map by traditional image processing is conducted by Marie et al.
(2013) in 2013. They segmented the omnidirectional image into free
space and surrounding parts (complementary domain of the free space).
The skeleton that is extracted from the free space part of the image is
used as a topological map to represent the topology information. They
designed a 3-dimensional invariant feature to represent the surrounding
environment. Combining those two parts, they got a cognitive map.
Artificial Neural Network Based Cognitive Map. With the de-
velopment of artificial intelligence, researchers attempt to utilize the
network to represent the informational environment. The architecture
and parameters of the network could be seen as a cognitive map. Some
networks produce mid-products to represent the environment. Gupta
et al. (2017) trained a network called CMP (Cognitive Mapper and
Planner) on the S3DIS (Stanford large-scale 3D Indoor Spaces) dataset
in 2017. The mapper module of CMP generates an egocentric map of
the environment from first-person-view images. This map is fed into the
remaining part of CMP to get the action commands. Some networks
encode the cognitive map into the parameters. Watkins-Valls et al.
(2020) used Autoencoder Model consists of a 6-layer convolutional
neural network to extract the features from images. These features are
fed into the Goal Checker model, which consists of fully connected
layers for the decision of reaching the goal. The Policy model would
generate the locomotion command if the robot does not reach the goal.
5.3. Problem formulation
The visual navigation problem can be expressed as revealing regu-
larities in the combined space ğ‘†Ã—ğ´Ã—ğ‘¡, where ğ‘†is sensors information,
ğ´is action parameters and ğ‘¡is time (Bohg et al., 2017). From the
regularities, the agent could perform action selection for a task and
infer the resulting sensory observation of the action.
Some researchers formulate the navigation task as a Markov Decision
Process (MDP), where an agent interacts with the environment through
a sequence of observations, actions and reward signals (Zhang et al.,
2017). The agentâ€™s goal is to determine the policy ğœ‹that maximizes
the cumulative expected future reward:
ğœ‹âˆ—= arg min
ğœ‹
E[
âˆ
âˆ‘
ğ‘¡=0
ğ›¾ğ‘¡ğ‘…(ğ‘ ğ‘¡, ğ‘ğ‘¡)|ğ‘ 0 = ğ‘ , ğ‘0 = ğ‘, ğœ‹],
where ğ‘ ğ‘¡, ğ‘ 0 âˆˆğ‘†represent the state of the environment at time ğ‘¡and ğ‘¡0,
respectively. The ğ‘0 represent the chosen action at time ğ‘¡0, ğ›¾represents
the discount factor, ğ‘…(ğ‘ ğ‘¡, ğ‘ğ‘¡) âˆˆğ‘…represents the received reward after
performing action ğ‘ğ‘¡at state ğ‘ ğ‘¡. The action ğ‘ğ‘¡âˆˆğ´is determined by
the policy ğœ‹: ğ‘ğ‘¡âˆ¼ğœ‹(ğ‘ ğ‘¡). The next state ğ‘ ğ‘¡+1 follows the dynamics of the
environment: ğ‘(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡, ğ‘ğ‘¡).
Since most of the robots work in an unknown environment, re-
searchers formulate the task as a partially observed Markov decision
process (POMDP, Georgakis et al., 2019). A POMDP is represented as
{ğ‘†, ğ´, ğ‘‚, ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡, ğ‘ğ‘¡), ğ‘…(ğ‘ ğ‘¡, ğ‘ğ‘¡)}, where ğ‘†is the state space consisting of
the agentâ€™s pose, ğ´is the action space, ğ‘‚is observation space comprised
of the egocentric RGB images, ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡, ğ‘ğ‘¡) is the transition probabilities
of the environment, ğ‘…(ğ‘ ğ‘¡, ğ‘ğ‘¡) âˆˆğ‘…is the received reward. Usually,
the navigation policy is represented by a neural network ğœ‹(ğ‘ğ‘¡|ğ‘œğ‘¡, ğ‘”ğ‘œğ‘ğ‘™)
which predicts the distribution of action ğ‘ğ‘¡from observation ğ‘œğ‘¡âˆˆğ‘‚
given the ğ‘”ğ‘œğ‘ğ‘™.
Some researchers formulate navigation as a learning problem from
the point of view of computer vision (Chen et al., 2019a). Chen et al.
(2019a) formulate estimation of policy ğœ‹âˆ—as a learning problem using
RL. The ğœ‹is trained on a set of environments ğœ€ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and tested on a set of
environments ğœ€ğ‘¡ğ‘’ğ‘ ğ‘¡that have been held out. Ramakrishnan et al. (2020)
formulate occupancy anticipation as a pixel-wise classification task to
generate a metric map of the environment. Henriques and Vedaldi
(2018) formulate localization and registration of sensory information as
a dual pair of convolution/deconvolution operators in memory space.
5.4. Common framework: Modular architecture
Typically, the framework is converted from traditional to learning-
based. The common framework is a modular architecture consisting of
a perception module and an action module. With the help of a photo-
realistic simulator and a transfer learning method, the learned policy
could be deployed on a real robot.
5.4.1. From classic to learning based
Savva et al. (2019) compared classic methods and learning-based
methods, and demonstrated that the latter performs better. They over-
turn the conclusion of Mishkin et al. (2019).
In 2017, Zhang et al. (2020b) proposed a completely differential
deep neural work model mimicking traditional simultaneous localiza-
tion and mapping. Their method only needs a forward pass through the
trained model, which runs at 200 Hz on the CPU, to give out planning
decisions from laser data. In 2018, Henriques and Vedaldi (2018)
introduced MapNet to model the RGB observation into 2.5D memories
(world-centric map). Their method outperforms a learned LSTM policy
without a map in previously unseen environments. In 2019, Georgakis
et al. (2019) improved MapNet by adding navigator module, which
allows them to combine features from RGB encoding, detection, and
segmentation task. Their method could accomplish label-specific Ob-
ject Goal Navigation and achieve superior performance with respect
to traditional ORB-SLAM. In 2020, Chaplot et al. (2020a) proposed
Active Neural SLAM (ANS) to combine both classic and learning-based
approaches for Point Goal Navigation. They model the navigation
semantics in graphs and achieve great success in embodied naviga-
tion tasks. Later, Ramakrishnan et al. (2020) formulated occupancy
anticipation as a pixelwise classification task. Their method antici-
pates the unseen parts with fewer registration errors. Chaplot et al.
(2020b) proposed Neural Topological SLAM (NTS), which effectively
leverages semantics and affords approximate geometric reasoning, for
image-specific object goal navigation.
5.4.2. Modular architecture
Modular systems have better generalization and faster adaptability,
which attracts the attention of researchers (Gordon et al., 2019). By
disentangling the perception module from policy for a task, a modular
system could learn more robust features with less data.
Inspired by the success of end-to-end policy learning (Levine et al.,
2016), much research has focused on policy learning from visual inputs
in end-to-end paradigm (Gordon et al., 2019). For the purpose of
learning compact representations and generalizable policies, it is often
necessary to go beyond the end-to-end training paradigm (Gordon
et al., 2019).
Zhu et al. (2017b) use pre-trained ResNet-50 as an extraction mod-
ule to process the RGB observation and propose an actorâ€“critic model
to figure out the action and the corresponding reward expectation of
the action. SplitNet is proposed by Gordon et al. (2019) with two
main modules: shared visual encoder and task decoders. Some auxiliary
13

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Table 5
The comparison of the studies utilizing photo-realistic level simulator. This paradigm is benefited from the development of photo-realistic virtual environment (Duan et al., 2021)
and attracts the attention of the computer vision domain.
Paper
Year
Input
RoE
Action
Task
Dataset
Method
Platform
Metrics
Xia et al. (2018)ğµ,ğ‘
2018
depth
and/or
RGB
network
F, B, L, R
active
perceptual,
static-
recognition
Gibson
PPO, VGG16
Gibson Xia
et al. (2018)
MMD,
CORAL
Das et al. (2018)ğµ,ğ‘
2018
RGB,
question
network
F, L, R,
stop
EmbodiedQA
EmbodiedQA,
House3D,
SUNCG
IL, RL, LSTM, ACT
navigator
House3D Wu
et al. (2018)
MR,
distance
Mishkin et al. (2019)ğµ,ğ‘
2019
RGBD,
distance,
actions
network
F, L, R
PointGoal
SunCG Song
et al. (2017a),
Matterport3D
Chang et al.
(2017)
CNN, FCN, Belief
DFP (BDFP)
MINOS
Savva et al.
(2017)
SR, SPL,
pace
Savva et al. (2019)ğµ,ğ‘
2019
RGBD,
GPS,
compass
network
F, L, R,
stop.
PointGoal
Matterport3D
Gibson
PPO, CNN
GRU
habitat
SPL, SR
Gupta et al. (2017)ğµ,ğ‘
2017
p-RGB
egocentric
map
still, F, B,
L, R
PointGoal
ObjectGoal-1
S3DIS
IL, CMP
â€“
distance;
SR
Anderson et al. (2018b)ğµ,ğ‘
2018
RGB,
language
network
F, L, R, U,
D, stop
VLN
R2R
student-forcing,
attention, Seq2Seq
Matterport3D
Simulator
NE
Fried et al. (2018)ğµ,ğ‘
2018
RGB,
language
network
Panoramic
Action
Space
VLN
R2R
Speakerâ€“Follower
Models, synthetic
data, LSTM
Matterport3D
Simulator
NE, SR,
OSR
Gordon et al. (2018)ğµ,ğ‘
2018
RGB,
question
semantic
map
F, L, R, U,
D
IQA
IQUAD V1
YOLOv3, HIMN,
esGRU, planner
AI2- THOR
accuracy,
time
Gordon et al. (2019)ğµ,ğ‘
2019
RGB
current
image
F, L, R
PointGoal,
exploration,
Flee
Gibson
IndoorEnv
Matterport3D
IL, PPO, SplitNet
(CNN, GRU)
Gibson
IndoorEnv
Matterport3D
SPL,
coverage,
distance
Wang et al. (2019)ğµ,ğ‘
2019
RGB,
language
network
direction
VLN
R2R
RCM, GloVe, IL, RL
Matterport3D
Simulator
PL, NE,
OSR, SR,
SPL
Tan et al. (2019)ğµ,ğ‘
2019
RGB,
language
network
direction
VLN
R2R
LSTM, Back
Translation
Sennrich et al.
(2015)
Matterport3D
Simulator
SR, NL,
NE, SPL
Ramakrishnan et al. (2019)ğµğ¶,ğ‘
2019
RGB
network
rotate the
object,
move the
camera
exploration
(active
observation
completion)
SUN360
ModelNet
RL, LSTM, CNN,
FCN,
sidekick policy
learning
â€“
coverage
IoU
Georgakis et al. (2019)ğµ,ğ‘
2019
RGB
allocentric
map
move
ObjectGoal-1
AVD
Matterport3D
LSTM, Auxiliary
task, mapper,
navigator
Matterport3D
SR
Wijmans et al. (2019)ğµ,ğ‘
2019
RGBD,
GPS,
compass
network
F, L, R
PointGoal
Gibson
Matterport3D
Transfer Learning,
DDPPO, policy
network, LSTM
habitat
SPL, SR
Ye et al. (2020)ğµ,ğ‘
2020
RGBD,
GPS,
compass
network
F, L, R
PointGoal
Gibson
Matterport3D
attention, auxiliary
tasks, LSTM,
DDPPO, policy
network
habitat
SPL, SR
Thomason et al. (2020)ğµ,ğ‘
2020
RGB
dialogue
network
F, L, R, U,
D, stop
VDN
(or NDH)
CVDN
LSTM, Seq2Seq
Anderson et al.
(2018b)
Matterport
Simulator
GP
Zhu et al. (2020b)ğµ,ğ‘
2020
RGB
dialogue
network
panoramic
action
space
VDN
(or NDH)
CVDN
LSTM, Attention,
GloVe Pennington
et al. (2014), CMN
Matterport
Simulator
SR, OSR,
GP, OPSR
Chaplot et al. (2020a)*ğµ,ğ‘
2020
RGB(D)
pose
allocentric
map
F, L, R,
stop
exploration
PointGoal
Gibson
Matterport3D
PPO, IL, ANS
Habitat
coverage
explored
Ramakrishnan et al. (2020)*ğµ,ğ‘
2020
RGB(D)
pose
allocentric
map
F, L, R
exploration
PointGoal
Gibson
Matterport3D
DRL, CNN, PPO,
OccAnt
habitat
IoU, Area
seen,SR,
SPL, time
Chaplot et al. (2020b)*ğµ,ğ‘
2020
p-RGB
pose
topological
map
F, L, R,
stop
ObjectGoal-2
Gibson
ANS, GRU, NTS
Habitat
SR
SPL
Zhu et al. (2020a)ğµ,ğ‘
2020
RGB,
language
network
direction
VLN
R2R
IL, Auxiliary task,
AuxRN
Matterport3D
Simulator
PL, NE,
OSR, SR,
SPL
(continued on next page)
14

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Table 5 (continued).
Paper
Year
Input
RoE
Action
Task
Dataset
Method
Platform
Metrics
Deng et al. (2020)ğµ,ğ‘
2020
RGB,
language
progressive
graph
direction
VLN
R2R, R4R
Jain et al.
(2019)
IL, RL, GNN, LSTM,
EGP
Matterport3D
Simulator
PL, NE,
OSR, SR,
SPL
Zhu et al. (2021a)ğµ,ğ‘
2021
RGB,
language
topological
map
move to
the node
SOON Zhu
et al. (2021a),
TouchDown
FAO
R2R
GBE, RL, IL, CNN,
GCN
Matterport3D
Simulator
NE, SR,
OSR, SPL
Sang et al. (2022)ğµ,ğ‘
2022
RGBD
network
F, L ,R,
stop
ObjectGoal-1
Matterport3D
GRU, RL, CNN,
LSTM
Habitat
SR, SPL
Mishkin et al. (2019)ğµ,ğ‘
2019
RGBD,
distance,
actions
allocentric
map
F, L, R
PointGoal
SunCG,
Matterport3D
CNN, OpenCV,
ORB-SLAM2
MINOS
Savva et al.
(2017)
SR, SPL,
pace [t]
Savva et al. (2019)ğµ,ğ‘
2019
RGBD,
GPS,
compass
allocentric
map
F, L, R,
stop.
PointGoal
Matterport3D
Gibson
CNN, OpenCV,
ORB-SLAM2
habitat
SPL, SR
[b]
1The notation here is the same as that in Table 4.
tasks (e.g., depth estimation) are trained to ensure the encoder has
extracted specific information (e.g., depth and geometric information).
With six times less data, their method could match the performance of
end-to-end methods (Gordon et al., 2019).
5.4.3. Pre-trained parameters increase the performance
Utilizing pre-trained parameters is a common way to improve per-
formance.
Song et al. (2017a) pre-train their model on synthetic scene data
(SUNCG) and then fine-tune it on the NYU dataset. Ramakrishnan et al.
(2020) use pre-trained ResNet-18 as feature extraction part of the map-
per module. Zhu et al. (2020b) use pre-trained GloVe (Pennington et al.,
2014) model to embed the natural language. Yang et al. (2018) use pre-
trained ResNet-50 to extract semantic prior. Mousavian et al. (2019)
use pre-trained ResNet-50 to detect objects in an unseen environment.
Most of them pre-train the visual module on the ImageNet dataset.
An interesting finding is that Gordon et al. (2019) argue that the
pretraining process on the ImageNet dataset does not offer better
generalization and performance.
Thus, we have reproduced a representative work as mentioned in
Section 4.5, and compared the performance of using or not using
pretrained parameters. We find that with pretrained parameters, the
performance improves nearly 5%. We consider the reason: the designed
visual encoder proposed by Gordon et al. (2019) may lack learning abil-
ity from common datasets, because they do not give the performance
on the task of ImageNet compared with others like ResNet. In other
words, they simply pretrain their model on ImageNet rather than using
a cutting-edge model listed on ImageNet as others do.
5.5. Task generalization
The generalization of tasks could be addressed by the design of the
network and the training strategy.
Zhu et al. (2017b) pointed that standard DRL models find a direct
mapping (represented by a deep neural network ğœ‹) from state ğ‘ ğ‘¡to
policy ğœ‹âˆ¶ğ‘ğ‘¡âˆ¼ğœ‹(ğ‘ ğ‘¡). That means the goal is hard-coded in neural
network parameters, and the network should be retrained for each task
and each environment. Thus, they propose a modular architecture with
an RGB encoder to understand the environment and an actorâ€“critic
model to deal with the task-specific policy.
Using meta-learning techniques, Wortsman et al. (2019) propose a
self-adaptive visual navigation method (SAVN) which learns to adapt
to new environments without any explicit supervision.
Zhu et al. (2020a) propose Auxiliary Reasoning Navigation (AuxRN)
to handle the rich semantic information contained in the environment.
Their method improves both the performance of the main task and the
modelâ€™s generalizability to other tasks.
Pathak et al. (2018) propose a zero-shot visual imitation, which
means the agent never has access to expert actions during training or
for the task demonstration of inference. Their forward consistency loss
is inspired by the intuition that reaching the goal is more important
than how it is reached.
Gordon et al. (2019) decompose leaning tasks into learning of a
visual encoder and learning of a task decoder. They use auxiliary tasks
to polish the visual encoder and only train the task decoder when
transferring the model to different tasks.
5.6. Handling dynamic environment
The dynamic of the environment consists of three aspects: (1) noise;
(2) dynamic obstacles; and (3) interacting with the environment to
obtain information.
Noise. Lots of researchers considered the action noise and sensor
noise as listed in tables (in Appendix A) marked with â€˜â€˜*â€™â€™. An example
is that Chaplot et al. (2020a) use Pose Estimator to estimate the sensorâ€™s
reading and reduce the influence of noise.
Dynamic Obstacles Gandhi et al. (2017) trained the network based
on the drone crash dataset. Their method is effective in navigating the
UAV even in extremely cluttered environments with dynamic obstacles
like humans. Falanga et al. (2020) used an event-based camera and
proposed a novel algorithm to enable the drone to have the capability of
avoiding multiple obstacles at relative speeds of up to 10 m/s. Standard
vision algorithms cannot be applied because of the cameraâ€™s per-pixel
intensity changes.
Interaction with the Environment Gordon et al. (2018) intro-
duced Interactive Question Answering (IQA) with a dynamic visual
environment. Questions are limited to existence and counting tasks.
Their Hierarchical Interactive Memory Network (HIMN) could output
two interaction actions: open and close.
5.7. From simulation to reality
To reduce the gap between simulation and reality, lots of techniques
could be utilized, such as domain adaptation, photo-realistic virtual
environments, domain randomization, semantic visual representation,
progressive networks, etc.
Rosano et al. (2020) employ domain adaptation to bridge the gap
between virtual and real observations. Xia et al. (2018) propose a
photo-realistic virtual environment, Gibson, with an internal synthesis
mechanism â€˜â€˜Gogglesâ€™â€™ to reduce the gap. Tobin et al. (2017) use domain
randomization for transferring the model from simulation to the real
world. Mousavian et al. (2019) concatenate the features extracted
from an off-the-shelf detector and segmenter. Their semantic visual
representation alleviates the need for domain adaptation or domain
randomization. Rusu et al. (2017) propose progressive networks to
bridge the gap. Their progressive net has different columns for different
tasks. Since the task in simulation and reality is the same, the output
layers do not use layer-wise adapters. Pan et al. (2017) propose an
15

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Table 6
The comparison of studies utilizing synthetic level simulator. They have positive impacts on policy learning.
Paper
Year
Input
RoE
Action
Task
Dataset
Method
Platform
Metrics
Rusu et al. (2017)ğ¶,ğ‘
2017
RGB
â€“
velocity
sim2real,
pixel-driven
control
simulation
DRL, LSTM, A3C,
progressive networks
MuJoCo
simulator [26]
final
reward
Sadeghi and Levine (2017)ğ¶,ğ‘
2017
RGB
network
still, F, L,
R
collision
avoidance
3D CAD
models
DRL, CNN, VGG-16
Parrot Bebop,
ROS
precisionâ€“
recall
Zhu et al. (2017a)ğ¶,ğ‘
2017
RGB, held
object
network
N, O, C,
Pu, P, U,
D
task
generalization
simulation
successor feature,
STRIPS Fikes and
Nilsson (1971) , SR
IL + RL
AI2-THOR
SR, time
Pan et al. (2017)*ğ¶,ğ‘
2017
RGB
â€“
â€“
sim2real
TORCS
Wymann
et al. (2000)
DRL, CNN, U-Net,
DRL, SegNet
Badrinarayanan
et al. (2017)
TORCS
training
reward
Song et al. (2017b)ğ¶,ğ‘
2017
depth
volumetric
occupancy,
semantic
labels
â€“
semantic
scene
completion
SUNCG
NYU
TSDF, SSCNet
â€“
voxel-level
IoU
Peng et al. (2018)*ğ¶,ğ‘
2018
positions
velocities
â€“
7D action
space
sim2real,
object
pushing
simulation
Dynamics
Randomization,
LSTM, FCN
MuJoCo,
Fetch Robotics
SR
Chen et al. (2019a)*ğ¶,ğ‘
2019
RGB,
bump
sensor
allocentric
map
F, B, L, R
exploration
SUNCG
IL, PPO, pretrained-
ResNet-18,
RNN
House3D
average
coverage
(m2)
Luong and Pham (2021)ğ¶,ğ‘
2020
laser data
allocentric
map
F, L, R
PointGoal
Gazebo, ROS
DRL, PPO, ACKTR
Gazebo
ROS
training
reward
Zhu et al. (2017b)ğ¶,ğ‘
2016
RGB
network
F, B, L, R
ObjectGoal-2
AI2-THOR
RL, CNN, actorâ€“critic
model
AI2-THOR,
real robot
SR
Yang et al. (2018)ğ¶,ğ‘
2018
RGB
network
F, B, L, R,
stop
ObjectGoal-1
AI2-THOR
Genome
Krishna et al.
(2016)
DRL, GCN,
ResNet50, fastText
Joulin et al. (2016),
knowledge graphs
AI2-THOR
SPL, SR
Wortsman et al. (2019)ğ¶,ğ‘
2019
RGB
object
class
network
F, L, R, U,
D, done
ObjectGoal-1
AI2-THOR
Kolve et al.
(2017)
meta-RL, LSTM,
SAVN
AI2-THOR
SPL, SR
Mousavian et al. (2019)ğ¶,ğ‘
2019
RGBD
network
F, B, L, R,
stop
ObjectGoal-1
AVD,
SunCG
LSTM, detector,
segment
â€“
SR
Zhang et al. (2020b)*ğ¶,ğ‘
2020
laser data
allocentric
map
still, F, L,
R
exploration
â€“
A3C, GAE, Neural
SLAM
Gazebo
SR
Du et al. (2020)ğ¶,ğ‘
2020
RGB,
object
class
network
F, L, R, U,
D, done
ObjectGoal-1
AI2-THOR
ORG, TPN, GCN,
A3C, IL, attention,
ResNet18, Faster
RCNN
AI2-THOR
SPL, SR
Gan et al. (2020)ğ¶,ğ‘
2020
RGB,
sound
signal
allocentric
map
F, B, L, R,
stop
AVN
Visual-Audio-
Room
Short-Time Fourier
Transform,
pretrained-ResNet
AI2-THOR,
Resonance
Audio API
SPL, SR
Wu et al. (2021)ğ¶,ğ‘
2021
RGBD
metric
occupancy
map
pick up,
push,
throw,
rescue
Intention-
aware
multi-agent
â€“
DQN, FCN
PyBullet
gathered
objects
number
Li et al. (2019)ğ¶,ğ‘
2019
laser,
odometry
allocentric
map
move
exploration
â€“
DRL, Karto SLAM,
AFCQN
ROS
coverage,
PL, time
Campos-MacÃ­as et al. (2020)*ğ¶,ğ‘
2020
depth,
pose
Linear
octrees
waypoints
PointGoal
exploration
â€“
simultaneously
generating a map,
Map, planner
drone, ROS,
Rotor-S,
Gazebo
Map
Generation
Time
1The notation here is the same as that in Table 4.
image-to-image network to augment the synthetic image. Using syn-
thetic real images as training data, the agent generalizes better in a
real environment than with pure training with virtual data or training
with domain randomization.
Lots of researchers reported the real-world experiments of their
models that were trained in simulation environments (Bojarski et al.,
2016; Zhu et al., 2017b; Sadeghi and Levine, 2017; Peng et al., 2018;
Rusu et al., 2017; Tobin et al., 2017; Pathak et al., 2018; Wu et al.,
2021; Xia et al., 2018; Surmann et al., 2020).
5.8. SLAM or learning based method
Visual SLAM (Simultaneous Localization And Mapping) is the most
disruptive technique in the last 20 years. However, Visual SLAM, visual
odometry, photogrammetry, optical flow and other visual techniques
for navigation purposes are out of our scope for four reasons.
Firstly, Savva et al. (2019) demonstrated that learning-based ap-
proaches outperform classic approaches (ORB-SLAM2 Mur-Artal and
Tardos, 2017) for point navigation when the agent has more learning
steps and data.
16

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 7. Examples of photo-realistic simulator (Zhu et al., 2021c).
Secondly, many scholars treat exploration as a geometry problem
and obtain accurate metric maps but entirely ignore semantic infor-
mation (Chen et al., 2019a). Taking the ORB-SLAM3 (Campos et al.,
2021) as an example, VSLAM focuses on keyframe and point matching,
updating, and tracking.
Thirdly, SLAM relies heavily on sensors to build maps and local-
ize, making it more susceptible to measurement noise (Duan et al.,
2021) and requiring more computational resources (Scaramuzza and
Fraundorfer, 2011). After empirical study, Ramakrishnan et al. (2021)
proved that the performance trends among learned approaches remain
consistent in noisy and noise-free test conditions, whereas a purely ge-
ometric approach like frontier-exploration tends to deteriorate rapidly
in the presence of sensor noise. While learning-based approaches that
use RGB(D) sensors are more robust to noise (Chaplot et al., 2020a).
Last but not least, decision-making policies are much less studied in
SLAM (Chen et al., 2019a). Most of the research starts with a human-
operated traversal of the environment, while some researchers use
heuristics such as frontier-based exploration or choosing actions that
reduce uncertainty in the estimated map to make decision (Cadena
et al., 2016). While learning-based methods usually use Reinforcement
Learning (RL) to compute the decision, which has been demonstrated
to achieve human-level control (Mnih et al., 2015).
As a result, while SLAMs are excellent works and capable of real-
time performance, they are incapable of achieving active perception
missions such as Visual-Language Navigation (VLN, Anderson et al.,
2018b), Navigation from Dialog History (NDH, Thomason et al., 2020),
Embodied Question Answering (EmbodiedQA, Das et al., 2018), In-
teractive Question Answering (IQA, Gordon et al., 2018), Scenario
Oriented Object Navigation (SOON, Zhu et al., 2021a). Under this
context, learning-based methods stands out and catches the researchersâ€™
attention.
5.9. Inspiring methods
During the comparison, several excellent methods really attracted
our attention and inspired us. We list them here with a brief introduc-
tion because we believe their work would benefit the community.
Zhu et al. (2021a) proposed a new benchmark named From Any-
where to Object (FAO) Dataset and a new task named Vision Situated
Object Navigation (SOON). This task requires the agent to obtain a
comprehensive and structured understanding of the observed informa-
tion. They present three findings: (1) Human performance outperforms
all models; (2) merging actions reduces the number of predictions
in a navigation process, making model training more stable; and (3)
combining imitation learning and reinforcement learning outperforms
pure imitation learning.
Das et al. (2018) proposed an Adaptive Computation Time (ACT)
navigator with a planner that selects actions (directions) and a con-
troller that decides how far to move following this action. Few re-
searchers pay attention to the prediction of execution time on the
predicted action. This method reduces the computation budget for
long-term navigation.
Gordon et al. (2018) proposed a Hierarchical Interactive Memory
Network (HIMN). They used an explicit external memory, which was
filled by the agent on the fly, to hold the information about the
environment. They represent this memory with an egocentric spatial
GRU that contains both geometric and semantic information.
Du et al. (2020) proposed an object relation graph (ORG) built
during the visual exploration phase and a Tentative Policy Network
(TPN). The ORG includes category closeness and spatial correlations
among different classes. The TPN provides a deadlock breaking pol-
icy in the testing phase. Few researchers pay attention to deadlock
detection during navigation.
Campos-MacÃ­as et al. (2020) used linear octrees to represent the 3D
environment. They have reported real world experiments to cross the
narrow, complex, unseen environment.
6. Challenges, open issues and future work
This section discusses the constraints, contributions, and future
work of the compared research. These comments mostly come from
the authors themselves and the authors who have cited the research.
We summarize the challenges based on the constraints and assumptions
mentioned by the authors and on our case study. We summarize the
future direction from the future work mentioned by the authors and
from the international competition.
Improve the performance. As mentioned by Chen et al. (2019a),
it is meaningful to use more expressive policy architecture, reward
functions, and training techniques. Bojarski et al. (2016) pointed out
that the robustness of the network should be improved. Mirowski et al.
(2017) would explore other auxiliary tasks to improve the performance.
Chaplot et al. (2020a) would combine their method with prior work on
localization. Anderson et al. (2018b) would look into visual attention
in order to combine their features. Deng et al. (2020) would address
computation challenges and increase the planning time-scale to enable
better decision making. Zhu et al. (2020b) would consider a more
explicit way that directly concatenates the embedding of the order and
the memory features.
In summary, designing a more powerful network to improve per-
formance is a potential future direction, but the principles and rules of
design are still open issues.
Consider the dynamic obstacles. Most of the research is conducted
on the static environment without dynamic obstacles like humans. As
pointed out by PÃ©rez-Dâ€™Arpino et al. (2020), deploying the model for a
mobile robot in human environments is still a research challenge.
PÃ©rez-Dâ€™Arpino et al. (2020) focused on navigating safely among
humans, but they follow a given path. Giovannangeli et al. (2006)
mentioned that future experiments will focus on the extended model in
dynamical indoor and outdoor environments. Gupta et al. (2017) said
extensions to dynamic environments would be interesting to explore.
Foehn et al. (2020) would handle unknown environments, perceive
obstacles and react accordingly. Notably, Xia et al. (2020) proposed
a benchmark for interactive navigation in cluttered environments.
In summary, empowering the robot with the ability to work in an
unseen environment with dynamic obstacles is a worthwhile future
direction and is still an open issue.
Transfer the model into a real-world application. Deploying
the model on the real robot is an essential part of the research.
According to Savinov et al. (2018b), noisy ego-motion estimation and
path integration are useful for navigation in the real world. Chaplot
et al. (2020b) would deploy their models on real robots. Zhu et al.
(2017a) would explore knowledge transfer from a virtual simulator to
a real-world environment.
In summary, Embodied AI style methods work well in virtual envi-
ronments, but transferring the model trained in a virtual environment
17

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Fig. 8. The experiment results in the case study.
into a real-world robot still needs lots of effort. Therefore, the training
strategy is still an open issue.
Consider more complex tasks. Zhang et al. (2020b) would evalu-
ate their approach with higher dimensional input. Zhu et al. (2020a)
would build a general framework for auxiliary reasoning tasks to
exploit common sense information. Zhu et al. (2017a) would examine
the possibilities of more complicated tasks with a richer set of actions.
Ye et al. (2020) would further study their approach for other embodied
tasks like QA and VLN.
In summary, the down-stream navigation task should be more com-
plex for practical application. More and more visual navigation tasks
are appearing, such as VLN, IQA, SOON, etc. However, the definition
of â€˜â€˜complex tasksâ€™â€™ is still an open issue.
Recent embodied AI task. There is an Embodied AI workshop.5
aimed at discussing the current state of intelligent agents that can: See,
Talk, Listen, Act and Reason
In other words, See, Talk, Listen, Act and Reason are still the
challenges and are the future directions. See: perceive their environ-
ment through vision or other senses. Talk: hold a natural language
dialog grounded in their environment. Listen: understand and react to
audio input anywhere in a scene. Act: navigate and interact with their
environment to accomplish goals. Reason: consider and plan for the
long-term consequences of their actions.
The international competitions also lead the direction of the active
perception method, including Robotic Vision Scene Understanding,6
Audio Visual Navigation,7 Multi-Object Navigation,8 Rearrangement9
ObjectNav and PointNav.
5 https://embodied-ai.org/.
6 http://cvpr2021.roboticvisionchallenge.org/.
7 https://soundspaces.org/challenge.
8 http://multion-challenge.cs.sfu.ca/.
9 https://ai2thor.allenai.org/rearrangement/.
7. Conclusion
We detailedly compared the implementation of the vision-based
method for robot navigation. In case study, we described the recent
research paradigm from the aspects of the task setup, architecture, and
information flow. Based on the comparison results and case study, we
discussed the problem formulation, common framework, task general-
ization, approaches for handling dynamic environments, transferring
models from simulation to reality, and inspiring methods. The chal-
lenges and future work are summarized based on the comments of
the authors themselves and other researchers, and on the international
competitions. The most mentioned future work is improving the perfor-
mance, considering the dynamic obstacles, transferring the model into
the real world and considering more complex tasks.
CRediT authorship contribution statement
Tianyao Zhang: Conceptualization, Methodology, Software, Writ-
ing â€“ original draft. Xiaoguang Hu: Project administration, Supervi-
sion. Jin Xiao: Data curation, Supervision. Guofeng Zhang: Visualiza-
tion, Supervision.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Appendix A. Detailed comparison of studies for visual navigation
This appendix illustrates the detailed comparison of works men-
tioned in Section 2. To bring the geometry style methods and the
embodied AI style methods in a unified lens, this survey views them
from the aspect of an inputâ€“output view. Considering the typeset-
ting, this survey separates the complete comparison into three tables:
Table 4, Table 5, and Table 6.
18

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Appendix B. Photo-realistic simulator
Zhu et al. (2021c) and Duan et al. (2021) discussed the photo-
realistic simulator in more detail. Fig. 7 gives more examples of photo-
realistic simulators (Zhu et al., 2021c) for the readerâ€™s interpretation.
Appendix C. The experiment results in the case study
The case study (Section 4.5) is just the presentation of an idea.
Readers could reproduce this work by following Ramakrishnan et al.
(2020). The experiment results that this survey reproduced are shown
in Fig. 8. The detailed analysis and improvement of these results will
be made publically available in our next article.
References
Agarwal, D., Bharti, P.S., 2020. Nature inspired evolutionary approaches for robot
navigation: survey. J. Inf. Opt. Sci. 41 (2), 421â€“436. http://dx.doi.org/10.1080/
02522667.2020.1723938.
Anderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V.,
Kosecka, J., Malik, J., Mottaghi, R., Savva, M., Zamir, A.R., 2018a. On evaluation
of embodied navigation agents.
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sunderhauf, N., Reid, I.,
Gould, S., van den Hengel, A., 2018b. Vision-and-language navigation: interpreting
visually-grounded navigation instructions in real environments. In: 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition. IEEE, Salt Lake City, UT,
pp. 3674â€“3683. http://dx.doi.org/10.1109/CVPR.2018.00387.
Badrinarayanan,
V.,
Kendall,
A.,
Cipolla,
R.,
2017.
Segnet:
A
deep
convolu-
tional encoder-decoder architecture for image segmentation. IEEE Trans. Pattern
Anal. Mach. Intell. 39 (12), 2481â€“2495. http://dx.doi.org/10.1109/TPAMI.2016.
2644615.
Bajcsy, R., 1988. Active perception. Proc. IEEE 76 (8), 966â€“1005. http://dx.doi.org/
10.1109/5.5968.
Bansal, S., Tolani, V., Gupta, S., Malik, J., Tomlin, C., 2020. Combining optimal control
and learning for visual navigation in novel environments. In: Conference on Robot
Learning. PMLR, pp. 420â€“429.
Batra, D., Chang, A.X., Chernova, S., Davison, A.J., Deng, J., Koltun, V., Levine, S.,
Malik, J., Mordatch, I., Mottaghi, R., 2020. Rearrangement: A challenge for
embodied ai. arXiv preprint arXiv:2011.01975.
Bohg, J., Hausman, K., Sankaran, B., Brock, O., Kragic, D., Schaal, S., Sukhatme, G.,
2017. Interactive perception: leveraging action in perception and perception in
action. IEEE Trans. Robot. 33 (6), 1273â€“1291. http://dx.doi.org/10.1109/tro.2017.
2721939.
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P.,
Jackel, L.D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., Zieba, K.,
2016. End to end learning for self-driving cars. arXiv:1604.07316
[Cs].
Borenstein, J., Koren, Y., 1989. Real-time obstacle avoidance for fast mobile robots.
IEEE Trans. Syst. Man Cybern. 19 (5), 1179â€“1187. http://dx.doi.org/10.1109/21.
44033.
Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., Reid, I.,
Leonard, J.J., 2016. Past, present, and future of simultaneous localization and
mapping: toward the robust-perception age. IEEE Trans. Robot. 32 (6), 1309â€“1332.
http://dx.doi.org/10.1109/tro.2016.2624754.
Campos, C., Elvira, R., Rodriguez, J.J.G., Montiel, J.M., Tardos, J.D., 2021. Orb-
slam3: an accurate open-source library for visual, visualâ€“inertial, and multimap
slam. IEEE Trans. Robot. (ISSN: 1552-3098, 1941-0468) 37 (6), 1874â€“1890. http:
//dx.doi.org/10.1109/TRO.2021.3075644.
Campos-MacÃ­as,
L.,
Aldana-LÃ³pez,
R.,
Guardia,
R.,
Parra-Vilchis,
J.I.,
GÃ³mez-
GutiÃ©rrez, D., 2020. Autonomous navigation of MAVs in unknown cluttered
environments. J. Field Robotics rob.21959. http://dx.doi.org/10.1002/rob.21959.
Chang, A., Dai, A., Funkhouser, T., Halber, M., Niebner, M., Savva, M., Song, S.,
Zeng, A., Zhang, Y., 2017. Matterport3D: learning from RGB-D data in indoor
environments. In: 2017 International Conference on 3D Vision (3DV). IEEE,
Qingdao, pp. 667â€“676. http://dx.doi.org/10.1109/3DV.2017.00081.
Chaplot, D.S., Gandhi, D., Gupta, S., Gupta, A., Salakhutdinov, R., 2020a. Learning to
explore using active neural SLAM. arXiv:2004.05155
[Cs].
Chaplot, D.S., Salakhutdinov, R., Gupta, A., Gupta, S., 2020b. Neural topological SLAM
for visual navigation. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 12875â€“12884.
Chen, T., Gupta, S., Gupta, A., 2019a. Learning exploration policies for navigation.
arXiv preprint arXiv:1903.01959.
Chen, H., Suhr, A., Misra, D., Snavely, N., Artzi, Y., 2019b. TOUCHDOWN: natural
language navigation and spatial reasoning in visual street environments. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 12538â€“12547.
Cho, K., Merrienboer, B.V., Gulcehre, C., Ba Hdanau, D., Bougares, F., Schwenk, H.,
Bengio, Y., 2014. Learning phrase representations using RNN encoder-decoder for
statistical machine translation. Comput. Sci..
Crespo, J., Castillo, J.C., Mozos, O.M., Barber, R., 2020. Semantic information for
robot navigation: a survey. Appl. Sci. 10 (2), 497. http://dx.doi.org/10.3390/
app10020497.
Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D., 2018. Embodied question
answering. In: Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition. pp. 1â€“10. http://dx.doi.org/10.1109/cvpr.2018.
00008.
Deitke, M., Han, W., Herrasti, A., Kembhavi, A., Kolve, E., Mottaghi, R., Salvador, J.,
Schwenk, D., VanderBilt, E., Wallingford, M., Weihs, L., Yatskar, M., Farhadi, A.,
2020. Robothor: an open simulation-to-real embodied AI platform. In: Proceed-
ings of the IEEE Computer Society Conference on Computer Vision and Pattern
Recognition. pp. 3161â€“3171. http://dx.doi.org/10.1109/CVPR42600.2020.00323.
Deng, Z., Narasimhan, K., Russakovsky, O., 2020. Evolving graphical planner: con-
textual global planning for vision-and-language navigation. arXiv:2007.05655
[Cs].
Desai, S., Lee, S., 2021. Auxiliary tasks for efficient learning of point-goal navigation.
In: Proc. - IEEE Winter Conf. Appl. Comput. Vis., WACV. Institute of Electrical and
Electronics Engineers Inc., pp. 717â€“725. http://dx.doi.org/10.1109/WACV48630.
2021.00076.
Desouza, G.N., Kak, A.C., 2002. Vision for mobile robot navigation: A survey. IEEE
Trans. Pattern Anal. Mach. Intell. 24 (2), 237â€“267. http://dx.doi.org/10.1109/34.
982903.
Dokeroglu, T., Sevinc, E., Kucukyilmaz, T., Cosar, A., 2019. A survey on new generation
metaheuristic algorithms. Comput. Ind. Eng. 137, 106040. http://dx.doi.org/10.
1016/j.cie.2019.106040.
Dosovitskiy, A., Koltun, V., 2016. Learning to act by predicting the future.
Du, H., Yu, X., Zheng, L., 2020. Learning object relation graph and tentative policy
for visual navigation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (Eds.),
Computer Vision. ECCV 2020, In: Lecture Notes in Computer Science, Springer
International Publishing, Cham, pp. 19â€“34. http://dx.doi.org/10.1007/978-3-030-
58571-6_2.
Duan, J., Yu, S., Tan, H.L., Zhu, H., Tan, C., 2021. A survey of embodied AI: from
simulators to research tasks.
Faessler, M., Fontana, F., Forster, C., Mueggler, E., Pizzoli, M., Scaramuzza, D., 2016.
Autonomous, vision-based flight and live dense 3D mapping with a quadrotor micro
aerial vehicle. J. Field Robotics 33 (4), 431â€“450. http://dx.doi.org/10.1002/rob.
21581.
Falanga, D., Kleber, K., Scaramuzza, D., 2020. Dynamic obstacle avoidance for quadro-
tors with event cameras. Science Robotics 5 (40), eaaz9712. http://dx.doi.org/10.
1126/scirobotics.aaz9712.
Fallah, N., Apostolopoulos, I., Bekris, K., Folmer, E., 2013. Indoor human navigation
systems: A survey. Interact. Comput. 25 (1), 21â€“33. http://dx.doi.org/10.1093/iwc/
iws010.
Fikes, R.E., Nilsson, N.J., 1971. STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence 2 (3â€“4), 189â€“208. http://dx.doi.
org/10.1016/0004-3702(71)90010-5.
Fiorini, P., Shiller, Z., 1995. Robot motion planning among moving obstacles.
Foehn, P., Brescianini, D., Kaufmann, E., Cieslewski, T., Gehrig, M., Muglikar, M.,
Scaramuzza, D., 2020. AlphaPilot: autonomous drone racing. In: Robotics: Science
and Systems XVI. Robotics: Science and Systems Foundation, http://dx.doi.org/10.
15607/RSS.2020.XVI.081.
Fournier, J., Ricard, B., Laurendeau, D., 2007. Mapping and exploration of complex
environments using persistent 3D model. In: Fourth Canadian Conference on
Computer and Robot Vision. CRVâ€™07, IEEE, pp. 403â€“410. http://dx.doi.org/10.
1109/crv.2007.45.
Fraundorfer, F., Heng, L., Honegger, D., Lee, G.H., Meier, L., Tanskanen, P., Polle-
feys, M., 2012. Vision-based autonomous mapping and exploration using a
quadrotor MAV. In: 2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems. pp. 4557â€“4564. http://dx.doi.org/10.1109/iros.2012.6385934.
Fried,
D.,
Hu,
R.,
Cirik,
V.,
Rohrbach,
A.,
Andreas,
J.,
Morency,
L.-P.,
Berg-
Kirkpatrick, T., Saenko, K., Klein, D., Darrell, T., 2018. Speaker-follower models
for vision-and-language navigation. arXiv:1806.02724
[Cs].
Fuentes-Pacheco, J., Ruiz-Ascencio, J., RendÃ³n-Mancha, J.M., 2015. Visual simultaneous
localization and mapping: A survey. Artif. Intell. Rev. 43 (1), 55â€“81. http://dx.doi.
org/10.1007/s10462-012-9365-8.
Gan, C., Zhang, Y., Wu, J., Gong, B., Tenenbaum, J.B., 2020. Look, listen, and act:
Towards audio-visual embodied navigation. In: 2020 IEEE International Conference
on Robotics and Automation. ICRA, pp. 9701â€“9707. http://dx.doi.org/10.1109/
ICRA40945.2020.9197008.
Gandhi, D., Pinto, L., Gupta, A., 2017. Learning to fly by crashing. In: IEEE In-
ternational Conference on Intelligent Robots and Systems, Vol. 2017-September.
pp. 3948â€“3955. http://dx.doi.org/10.1109/IROS.2017.8206247.
Gaussier, P., Joulain, C., Zrehen, S., Banquet, J., Revel, A., 1997. Visual navigation
in an open environment without map. In: Proceedings of the 1997 IEEE/RSJ
International Conference on Intelligent Robot and Systems. Innovative Robotics for
Real-World Applications. IROS â€™97, Vol. 2. IEEE, Grenoble, France, pp. 545â€“550.
http://dx.doi.org/10.1109/iros.1997.655065.
Georgakis, G., Li, Y., Kosecka, J., 2019. Simultaneous mapping and target driven
navigation. arXiv:1911.07980
[Cs].
Giovannangeli, C., Gaussier, P., Desilles, G., 2006. Robust mapless outdoor vision-based
navigation. In: 2006 IEEE/RSJ International Conference on Intelligent Robots and
Systems. pp. 3293â€“3300. http://dx.doi.org/10.1109/IROS.2006.282501.
19

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Gordon, D., Kadian, A., Parikh, D., Hoffman, J., Batra, D., 2019. SplitNet: Sim2Sim and
Task2Task transfer for embodied visual navigation. In: 2019 IEEE/CVF International
Conference on Computer Vision. ICCV, pp. 1022â€“1031. http://dx.doi.org/10.1109/
ICCV.2019.00111.
Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A., 2018. IQA:
Visual question answering in interactive environments. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 4089â€“4098.
Grigorescu, S., Trasnea, B., Cocias, T., Macesanu, G., 2020. A survey of deep learning
techniques for autonomous driving. J. Field Robotics 37 (3), 362â€“386. http://dx.
doi.org/10.1002/rob.21918.
Grisetti, G., KÃ¼mmerle, R., Stachniss, C., Burgard, W., 2010. A tutorial on graph-based
SLAM. IEEE Intell. Transp. Syst. Mag. 2 (4), 31â€“43. http://dx.doi.org/10.1109/
MITS.2010.939925.
Gul, F., Rahiman, W., Alhady, S.S.N., 2019. A comprehensive study for robot navigation
techniques. In: Chen, K. (Ed.), Cogent Eng. 6 (1), 1632046. http://dx.doi.org/10.
1080/23311916.2019.1632046.
Gupta, S., Davidson, J., Levine, S., Sukthankar, R., Malik, J., 2017. Cognitive mapping
and planning for visual navigation. In: 2017 IEEE Conference on Computer Vision
and Pattern Recognition. CVPR, IEEE, Honolulu, HI, pp. 7272â€“7281. http://dx.doi.
org/10.1109/cvpr.2017.769.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition.
In: 2016 IEEE Conference on Computer Vision and Pattern Recognition. CVPR,
IEEE, Las Vegas, NV, USA, pp. 770â€“778. http://dx.doi.org/10.1109/CVPR.2016.90.
Henriques, J.a.F., Vedaldi, A., 2018. MapNet: an allocentric spatial memory for mapping
environments. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 8476â€“8484.
Huang, G., 2019. Visual-inertial navigation: A concise review. In: 2019 International
Conference on Robotics and Automation. ICRA, IEEE, Montreal, QC, Canada,
pp. 9572â€“9582. http://dx.doi.org/10.1109/ICRA.2019.8793604.
Jain, V., Magalhaes, G., Ku, A., Vaswani, A., Ie, E., Baldridge, J., 2019. Stay on
the path: instruction fidelity in vision-and-language navigation. arXiv preprint
arXiv:1905.12255.
Janai, J., GÃ¼ney, F., Behl, A., Geiger, A., 2020. Computer vision for autonomous
vehicles: problems, datasets and state of the art. Found. TrendsÂ® Comput. Graph.
Vision 12 (1â€“3), 1â€“308. http://dx.doi.org/10.1561/0600000079.
Joulin, A., Grave, E., Bojanowski, P., Mikolov, T., 2016. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759.
Kanellakis, C., Nikolakopoulos, G., 2017. Survey on computer vision for uavs: Current
developments and trends. J. Intell. Robot. Syst. 87 (1), 141â€“168. http://dx.doi.org/
10.1007/s10846-017-0483-z.
Karaman, S., Frazzoli, E., 2011. Sampling-based algorithms for optimal motion
planning.
Int.
J.
Robot.
Res.
30
(7),
846â€“894.
http://dx.doi.org/10.1177/
0278364911406761.
Kempka, M., Wydmuch, M., Runc, G., Toczek, J., JaÅ›kowski, W., 2016. ViZDoom:
a doom-based AI research platform for visual reinforcement learning. In: 2016
IEEE Conference on Computational Intelligence and Games. CIG, pp. 1â€“8. http:
//dx.doi.org/10.1109/cig.2016.7860433.
Kohlbrecher, S., Von Stryk, O., Meyer, J., Klingauf, U., 2011. A flexible and scalable
SLAM system with full 3D motion estimation. In: 9th IEEE International Symposium
on Safety, Security, and Rescue Robotics. SSRR 2011, pp. 155â€“160. http://dx.doi.
org/10.1109/ssrr.2011.6106777.
Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D.,
Zhu, Y., Gupta, A., Farhadi, A., 2017. Ai2-thor: an interactive 3D environment for
visual ai. arXiv preprint arXiv:1712.05474.
Konolige, K., Marder-Eppstein, E., Marthi, B., 2011. Navigation in hybrid metric-
topological
maps.
In:
2011
IEEE
International
Conference
on
Robotics
and
Automation. IEEE, pp. 3041â€“3047. http://dx.doi.org/10.1109/icra.2011.5980074.
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y.,
Li, L.-J., Shamma, D.A., 2016. Visual genome: connecting language and vision using
crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332.
Kruse, T., Pandey, A.K., Alami, R., Kirsch, A., 2013. Human-aware robot navigation: A
survey. Robot. Auton. Syst. 61 (12), 1726â€“1743. http://dx.doi.org/10.1016/j.robot.
2013.05.007.
LaValle, S.M., 2006. Planning Algorithms. Cambridge University Press.
Levine, S., Finn, C., Darrell, T., Abbeel, P., 2016. End-to-end training of deep visuomotor
policies. J. Mach. Learn. Res. 17 (1), 1334â€“1373.
Li, H., Zhang, Q., Zhao, D., 2019. Deep reinforcement learning-based automatic
exploration for navigation in unknown environment. IEEE Trans. Neural Netw.
Learn. Syst. 31 (6), 2064â€“2076. http://dx.doi.org/10.1109/TNNLS.2019.2927869.
Longuet-Higgins, H.C., 1981. A computer algorithm for reconstructing a scene from two
projections. Nature 293 (5828), 133â€“135. http://dx.doi.org/10.1038/293133a0.
Lowe, D.G., 1999. Object recognition from local scale-invariant features. In: Proceed-
ings of the Seventh IEEE International Conference on Computer Vision, Vol. 2,
Ieee, pp. 1150â€“1157. http://dx.doi.org/10.1109/iccv.1999.790410.
Lowry, S., SÃ¼nderhauf, N., Newman, P., Leonard, J.J., Cox, D., Corke, P., Milford, M.J.,
2016a. Visual place recognition: A survey. IEEE Trans. Robot. 32 (1), 1â€“19.
http://dx.doi.org/10.1109/tro.2015.2496823.
Lowry, S., SÃ¼nderhauf, N., Newman, P., Leonard, J.J., Cox, D., Corke, P., Milford, M.J.,
2016b. Visual place recognition: a survey. IEEE Trans. Robot. 32 (1), 1â€“19. http:
//dx.doi.org/10.1109/tro.2015.2496823.
Luong, M., Pham, C., 2021. Incremental learning for autonomous navigation of mobile
robots based on deep reinforcement learning. J. Intell. Robot. Syst. 101 (1), 1.
http://dx.doi.org/10.1007/s10846-020-01262-5.
Marie, R., Labbani-Igbida, O., Merveilleux, P., Mouaddib, E.-M., 2013. Autonomous
robot exploration and cognitive map building in unknown environments using
omnidirectional visual information only. In: 2013 IEEE Workshop on Robot Vision.
WORV, IEEE, Clearwater Beach, FL, pp. 191â€“196. http://dx.doi.org/10.1109/
WORV.2013.6521937.
MartÃ­n, F., MatellÃ¡n, V., RodrÃ­guez, F.J., GinÃ©s, J., 2019. Octree-based localization
using RGB-D data for indoor robots. Eng. Appl. Artif. Intell. 77, 177â€“185. http:
//dx.doi.org/10.1016/j.engappai.2018.10.002.
McGuire, K., 2019. Indoor Swarm Exploration with Pocket Drones (Ph.D. thesis). Delft
University of Technology.
McGuire, K., de Croon, G., De Wagter, C., Tuyls, K., Kappen, H., 2017. Efficient
optical flow and stereo vision for velocity estimation and obstacle avoidance
on an autonomous pocket drone. IEEE Robot. Autom. Lett. 2 (2), 1070â€“1076.
http://dx.doi.org/10.1109/LRA.2017.2658940.
McGuire, K.N., De Wagter, C., Tuyls, K., Kappen, H.J., de Croon, G.C.H.E., 2019.
Minimal navigation solution for a swarm of tiny flying robots to explore an
unknown environment. Science Robotics 4 (35), eaaw9710. http://dx.doi.org/10.
1126/scirobotics.aaw9710.
Meng, M., Kak, A.C., 1993. Mobile robot navigation using neural networks and
nonmetrical environmental models. IEEE Control Syst. Mag. 13 (5), 30â€“39. http:
//dx.doi.org/10.1109/37.236323.
Mirowski,
P.,
Grimes,
M.K.,
Malinowski,
M.,
Hermann,
K.M.,
Anderson,
K.,
Teplyashin, D., Simonyan, K., Kavukcuoglu, K., Zisserman, A., Hadsell, R., 2018.
Learning to navigate in cities without a map. arXiv:1804.00168
[Cs].
Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A., Denil, M.,
Goroshin, R., Sifre, L., Kavukcuoglu, K., Kumaran, D., Hadsell, R., 2017. Learning
to navigate in complex environments. In: ICLR.
Mishkin, D., Dosovitskiy, A., Koltun, V., 2019. Benchmarking classic and learned
navigation in complex 3D environments. arXiv:1901.10915
[Cs].
Misra, D., Bennett, A., Blukis, V., Niklasson, E., Shatkhin, M., Artzi, Y., 2018. Mapping
instructions to actions in 3D environments with visual goal prediction. arXiv
preprint arXiv:1809.00786.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Beattie, C.,
Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D.,
2015. Human-level control through deep reinforcement learning. Nature 518
(7540), 529â€“533. http://dx.doi.org/10.1038/nature14236.
Mohanan, M.G., Salgoankar, A., 2018. A survey of robotic motion planning in dynamic
environments. Robot. Auton. Syst. 100, 171â€“185. http://dx.doi.org/10.1016/j.
robot.2017.10.011.
Moravec, H., Elfes, A., 1985. High resolution maps from wide angle sonar. In:
Proceedings. 1985 IEEE International Conference on Robotics and Automation, Vol.
2. IEEE, pp. 116â€“121. http://dx.doi.org/10.1109/robot.1985.1087316.
Morioka, H., Yi, S., Hasegawa, O., 2011. Vision-based mobile robotâ€™s slam and
navigation in crowded environments. In: 2011 IEEE/RSJ International Conference
on Intelligent Robots and Systems. IEEE, pp. 3998â€“4005. http://dx.doi.org/10.
1109/IROS.2011.6094847.
Mousavian, A., Toshev, A., FiÅ¡er, M., KoÅ¡eckÃ¡, J., Wahid, A., Davidson, J., 2019.
Visual representations for semantic target driven navigation. In: 2019 International
Conference on Robotics and Automation. ICRA, pp. 8846â€“8852. http://dx.doi.org/
10.1109/icra.2019.8793493.
Mur-Artal, R., Tardos, J.D., 2017. ORB-SLAM2: an open-source SLAM system for
monocular, stereo, and RGB-D cameras. IEEE Trans. Robot. 33 (5), 1255â€“1262.
http://dx.doi.org/10.1109/TRO.2017.2705103.
Oliva, A., Torralba, A., 2006. Building the gist of a scene: the role of global image
features in recognition. Prog. Brain Res. 155, 23â€“36. http://dx.doi.org/10.1016/
S0079-6123(06)55002-2.
Ostad-Ali-Askari, K., Shayan, M., 2021. Subsurface drain spacing in the unsteady
conditions by HYDRUS-3D and artificial neural networks. Arab. J. Geosci. 14 (18),
1936. http://dx.doi.org/10.1007/s12517-021-08336-0.
Ostad-Ali-Askari, K., Shayannejad, M., Ghorbanizadeh-Kharazi, H., 2017. Artificial
neural network for modeling nitrate pollution of groundwater in marginal area
of Zayandeh-rood River, Isfahan, Iran. KSCE J. Civ. Eng. 21 (1), 134â€“140. http:
//dx.doi.org/10.1007/s12205-016-0572-8.
Pan, X., You, Y., Wang, Z., Lu, C., 2017. Virtual to real reinforcement learning for
autonomous driving. arXiv:1704.03952
[Cs].
Pandey, A., Pandey, S., Parhi, D.R., 2017. Mobile robot navigation and obstacle
avoidance techniques: A review. Int. Robot. Autom. J. 2 (Issue 3), http://dx.doi.
org/10.15406/iratj.2017.02.00023.
Pathak, D., Mahmoudieh, P., Luo, G., Agrawal, P., Chen, D., Shentu, Y., Shelhamer, E.,
Malik, J., Efros, A.A., Darrell, T., 2018. Zero-shot visual imitation. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.
pp. 2050â€“2053.
Peng, X.B., Andrychowicz, M., Zaremba, W., Abbeel, P., 2018. Sim-to-real transfer
of robotic control with dynamics randomization. In: 2018 IEEE International
Conference on Robotics and Automation. ICRA, pp. 3803â€“3810. http://dx.doi.org/
10.1109/ICRA.2018.8460528.
20

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Pennington, J., Socher, R., Manning, C.D., 2014. Glove: global vectors for word
representation. In: Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing. EMNLP, pp. 1532â€“1543. http://dx.doi.org/10.3115/
v1/D14-1162.
PÃ©rez-Dâ€™Arpino, C., Liu, C., Goebel, P., MartÃ­n-MartÃ­n, R., Savarese, S., 2020. Robot
navigation in constrained pedestrian environments using reinforcement learning.
arXiv:2010.08600
[Cs].
Perumal, P.S., Sujasree, M., Chavhan, S., Gupta, D., Mukthineni, V., Shimgekar, S.R.,
Khanna, A., Fortino, G., 2021. An insight into crash avoidance and overtaking
advice systems for autonomous vehicles: A review, challenges and solutions.
Eng. Appl. Artif. Intell. 104, 104406. http://dx.doi.org/10.1016/j.engappai.2021.
104406.
Quan, L., Han, L., Zhou, B., Shen, S., Gao, F., 2020. Survey of UAV motion planning.
IET Cyber-Syst. Robot. 2 (1), 14â€“21. http://dx.doi.org/10.1049/iet-csr.2020.0004.
Ramakrishnan, S.K., Al-Halah, Z., Grauman, K., 2020. Occupancy anticipation for
efficient exploration and navigation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-
M. (Eds.), Computer Vision. ECCV 2020, In: Lecture Notes in Computer Science,
Springer International Publishing, Cham, pp. 400â€“418. http://dx.doi.org/10.1007/
978-3-030-58558-7_24.
Ramakrishnan, S.K., Jayaraman, D., Grauman, K., 2019. Emergence of exploratory look-
around behaviors through active observation completion. Science Robotics 4 (30),
http://dx.doi.org/10.1126/scirobotics.aaw6326.
Ramakrishnan, S.K., Jayaraman, D., Grauman, K., 2021. An exploration of embodied
visual exploration. Int. J. Comput. Vis. 129 (5), 1616â€“1649. http://dx.doi.org/10.
1007/s11263-021-01437-z.
Romero, V., Costa, O., 2010. Map merging strategies for multi-robot fastslam: a
comparative survey. In: 2010 Latin American Robotics Symposium and Intelligent
Robotics Meeting. pp. 61â€“66. http://dx.doi.org/10.1109/LARS.2010.20.
Rosano, M., Furnari, A., Gulino, L., Farinella, G.M., 2020. On embodied visual
navigation in real environments through habitat. arXiv:2010.13439
[Cs].
Roy, P., Chowdhury, C., 2021. A survey of machine learning techniques for indoor
localization and navigation systems. J. Intell. Robot. Syst. 101 (3), 63. http:
//dx.doi.org/10.1007/s10846-021-01327-z.
Rusu, A.A., VeÄerÃ­k, M., RothÃ¶rl, T., Heess, N., Pascanu, R., Hadsell, R., 2017. Sim-
to-real robot learning from pixels with progressive nets. In: Conference on Robot
Learning. PMLR, pp. 262â€“270.
Sadeghi, F., Levine, S., 2017. CAD2RL: real single-image flight without a single real
image. In: Robotics: Science and Systems XIII. Robotics: Science and Systems
Foundation, http://dx.doi.org/10.15607/RSS.2017.XIII.034.
Sang, H., Jiang, R., Wang, Z., Zhou, Y., He, B., 2022. A novel neural multi-store memory
network for autonomous visual navigation in unknown environment. IEEE Robot.
Autom. Lett. 7 (2), 2039â€“2046. http://dx.doi.org/10.1109/LRA.2022.3140795.
Saputra, M.R.U., Markham, A., Trigoni, N., 2018. Visual SLAM and structure from
motion in dynamic environments: A survey. ACM Comput. Surv. 51 (2), 37:1â€“37:36.
http://dx.doi.org/10.1145/3177853.
Savinov, N., Dosovitskiy, A., Koltun, V., 2018a. Semi-parametric topological memory
for navigation. arXiv preprint arXiv:1803.00653.
Savinov, N., Dosovitskiy, A., Koltun, V., 2018b. Semi-parametric topological memory
for navigation. arXiv:1803.00653
[Cs].
Savva, M., Chang, A.X., Dosovitskiy, A., Funkhouser, T., Koltun, V., 2017. MINOS:
multimodal indoor simulator for navigation in complex environments. arXiv:1712.
03931
[Cs].
Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J.,
Koltun, V., Malik, J., Parikh, D., Batra, D., 2019. Habitat: A platform for embodied
AI research. In: Proceedings of the IEEE International Conference on Computer
Vision, Vol. 2019-October. pp. 9338â€“9346. http://dx.doi.org/10.1109/ICCV.2019.
00943.
Scaramuzza, D., Fraundorfer, F., 2011. Visual odometry [Tutorial]. IEEE Robot. Autom.
Mag. 18 (4), 80â€“92. http://dx.doi.org/10.1109/MRA.2011.943233.
Sennrich, R., Haddow, B., Birch, A., 2015. Improving neural machine translation models
with monolingual data. arXiv preprint arXiv:1511.06709.
Shen, B., Xia, F., Li, C., MartÃ­n-MartÃ­n, R., Fan, L., Wang, G., Buch, S., Dâ€™Arpino, C.,
Srivastava, S., Tchapmi, L.P., Tchapmi, M.E., Vainio, K., Fei-Fei, L., Savarese, S.,
2020. Igibson, a simulation environment for interactive tasks in large realistic
scenes. arXiv:2012.02924
[Cs].
Shenavarmasouleh, F., Mohammadi, F.G., Amini, M.H., Reza Arabnia, H., 2022.
Embodied AI-driven operation of smart cities: A concise review. In: Amini, M.H.,
Shafie-khah, M. (Eds.), Cyberphysical Smart Cities Infrastructures, First ed. Wiley,
pp. 29â€“45. http://dx.doi.org/10.1002/9781119748342.ch3.
Song, S., Yu, F., Zeng, A., Chang, A.X., Savva, M., Funkhouser, T., 2017a. Semantic
scene completion from a single depth image. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pp. 1746â€“1754.
Song, S., Yu, F., Zeng, A., Chang, A.X., Savva, M., Funkhouser, T., 2017b. Semantic
scene completion from a single depth image. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pp. 1746â€“1754.
Surmann, H., Jestel, C., Marchel, R., Musberg, F., Elhadj, H., Ardani, M., 2020. Deep
reinforcement learning for real autonomous mobile robot navigation in indoor
environments. arXiv:2005.13857
[Cs].
Szeliski, R., 2010. Computer vision: algorithms and applications. Springer Science &
Business Media.
Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N.,
Mukadam, M., Chaplot, D.S., Maksymets, O., 2021. Habitat 2.0: training home
assistants to rearrange their habitat. Adv. Neural Inf. Process. Syst. 34.
Tai, L., Zhang, J., Liu, M., Boedecker, J., Burgard, W., 2018. A survey of deep
network solutions for learning control in robotics: from reinforcement to imitation.
arXiv:1612.07139
[Cs].
Taketomi, T., Uchiyama, H., Ikeda, S., 2017. Visual SLAM algorithms: A survey from
2010 to 2016. IPSJ Trans. Comput. Vis. Appl. 9 (1), 16. http://dx.doi.org/10.1186/
s41074-017-0027-2.
Tan, H., Yu, L., Bansal, M., 2019. Learning to navigate unseen environments: back
translation with environmental dropout. arXiv:1904.04195
[Cs].
Terashima, T., Hasegawa, O., 2017. A visual-SLAM for first person vision and
mobile robots. In: 2017 Fifteenth IAPR International Conference on Machine
Vision Applications. MVA, IEEE, pp. 73â€“76. http://dx.doi.org/10.23919/MVA.2017.
7986779.
Thomason, J., Murray, M., Cakmak, M., Zettlemoyer, L., 2020. Vision-and-dialog
navigation. In: Conference on Robot Learning. PMLR, pp. 394â€“406.
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P., 2017. Domain
randomization for transferring deep neural networks from simulation to the real
world. In: 2017 IEEE/RSJ International Conference on Intelligent Robots and
Systems. IROS, pp. 23â€“30. http://dx.doi.org/10.1109/IROS.2017.8202133.
Tolman, E.C., 1948. Cognitive maps in rats and men. Psychol. Rev. 55 (4), 189.
http://dx.doi.org/10.1037/h0061626.
Tzafestas, S.G., 2018. Mobile robot control and navigation: a global overview. J. Intell.
Robot. Syst. 91 (1), 35â€“58. http://dx.doi.org/10.1007/s10846-018-0805-9.
Ungar, S., 2000. Cognitive Mapping: Past, Present, and Future. Psychology Press.
Wahab, M.N.A., Nefti-Meziani, S., Atyabi, A., 2020. A comparative review on mobile
robot path planning: classical or meta-heuristic methods? Annu. Rev. Control 50,
233â€“252. http://dx.doi.org/10.1016/j.arcontrol.2020.10.001.
Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.-F., Wang, W.Y.,
Zhang, L., 2019. Reinforced cross-modal matching and self-supervised imita-
tion learning for vision-language navigation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 6629â€“6638.
Watkins-Valls, D., Xu, J., Waytowich, N., Allen, P., 2020. Learning your way without
map or compass: panoramic target driven visual navigation. arXiv:1909.09295
[Cs].
Wijmans, E., Kadian, A., Morcos, A., Lee, S., Essa, I., Parikh, D., Savva, M., Batra, D.,
2019. DD-PPO: learning near-perfect PointGoal navigators from 2.5 billion frames.
arXiv E-Prints, 1911, arXiv:1911.00357.
Wortsman, M., Ehsani, K., Rastegari, M., Farhadi, A., Mottaghi, R., 2019. Learning to
learn how to learn: self-adaptive visual navigation using meta-learning. In: CVPR.
pp. 6750â€“6759.
Wu, J., Sun, X., Zeng, A., Song, S., Rusinkiewicz, S., Funkhouser, T., 2021. Spatial
intention maps for multi-agent mobile manipulation. arXiv:2103.12710
[Cs].
Wu, Y., Wu, Y., Gkioxari, G., Tian, Y., 2018. Building generalizable agents with a
realistic and rich 3D environment. arXiv preprint arXiv:1801.02209.
Wymann, B., EspiÃ©, E., Guionneau, C., Dimitrakakis, C., Coulom, R., Sumner, A., 2000.
Torcs, the open racing car simulator. Software Available At http://torcs.sourceforge.
net, 4 6, 2.
Xia, F., Li, C., Chen, K., Shen, W.B., MartÄ±n-MartÄ±n, R., Hirose, N., Zamir, A.R., Fei-
Fei, L., Savarese, S., 2019. Gibson Env V2: embodied simulation environments for
interactive navigation. Stanford Univ. 5.
Xia, F., Shen, W.B., Li, C., Kasimbeg, P., Tchapmi, M.E., Toshev, A., MartÃ­n-MartÃ­n, R.,
Savarese, S., 2020. Interactive Gibson benchmark: A benchmark for interactive
navigation in cluttered environments. IEEE Robot. Autom. Lett. 5 (2), 713â€“720.
http://dx.doi.org/10.1109/LRA.2020.2965078.
Xia, F., Zamir, A.R., He, Z., Sax, A., Malik, J., Savarese, S., 2018. Gibson env:
real-world perception for embodied agents. In: 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition. IEEE, Salt Lake City, UT, pp. 9068â€“9079.
http://dx.doi.org/10.1109/CVPR.2018.00945.
Yang, L., Qi, J., Song, D., Xiao, J., Han, J., Xia, Y., 2016. Survey of robot 3D path
planning algorithms. J. Control Sci. Eng. 2016, e7426913. http://dx.doi.org/10.
1155/2016/7426913.
Yang, W., Wang, X., Farhadi, A., Gupta, A., Mottaghi, R., 2018. Visual semantic
navigation using scene priors.
Yasuda, Y.D.V., Martins, L.E.G., Cappabianco, F.A.M., 2020. Autonomous visual navi-
gation for mobile robots: A systematic literature review. ACM Comput. Surv. 53
(1), 13:1â€“13:34. http://dx.doi.org/10.1145/3368961.
Ye, J., Batra, D., Wijmans, E., Das, A., 2020. Auxiliary tasks speed up learning PointGoal
navigation. arXiv:2007.04561
[Cs].
Ye, X., Yang, Y., 2020. From seeing to moving: a survey on learning for visual indoor
navigation (VIN). arXiv:2002.11310
[Cs].
Younes, A., 2022. Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound
Sources in Unmapped 3D Environments (Ph.D. thesis).
Zhang, T., Hu, X., Xiao, J., Zhang, G., 2020a. A machine learning method for vision-
based unmanned aerial vehicle systems to understand unknown environments.
Sensors 20 (11), 3245. http://dx.doi.org/10.3390/s20113245.
Zhang,
T.,
Hu,
X.,
Xiao,
J.,
Zhang,
G.,
Fu,
L.,
2019.
An
implementation
of
non-electronic human-swarm interface for multi-agent system in cooperative search-
ing. In: 2019 IEEE 15th International Conference on Control and Automation,
ICCA, pp. 1355â€“1360. http://dx.doi.org/10.1109/ICCA.2019.8899992.
21

T. Zhang, X. Hu, J. Xiao et al.
Engineering Applications of Artificial Intelligence 114 (2022) 105036
Zhang, J., Springenberg, J.T., Boedecker, J., Burgard, W., 2017. Deep reinforcement
learning with successor features for navigation across similar environments. In:
2017 IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS,
pp. 2371â€“2378. http://dx.doi.org/10.1109/IROS.2017.8206049.
Zhang, J., Tai, L., Liu, M., Boedecker, J., Burgard, W., 2020b. Neural SLAM: learning
to explore with external memory. arXiv:1706.09520
[Cs].
Zhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., Mottaghi, R.,
Farhadi, A., 2017a. Visual semantic planning using deep successor representa-
tions. In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 483â€“492.
Zhu, F., Liang, X., Zhu, Y., Yu, Q., Chang, X., Liang, X., 2021a. SOON: scenario oriented
object navigation with graph-based exploration. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 12689â€“12699.
Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi, A., 2017b.
Target-driven visual navigation in indoor scenes using deep reinforcement learning.
In: 2017 IEEE International Conference on Robotics and Automation. ICRA,
pp. 3357â€“3364. http://dx.doi.org/10.1109/icra.2017.7989381.
Zhu, F., Zhu, Y., Chang, X., Liang, X., 2020a. Vision-language navigation with self-
supervised auxiliary reasoning tasks. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 10012â€“10022.
Zhu, F., Zhu, Y., Lee, V., Liang, X., Chang, X., 2021b. Deep learning for embodied
vision navigation: a survey. arXiv preprint arXiv:2108.04097.
Zhu, F., Zhu, Y., Lee, V.C., Liang, X., Chang, X., 2021c. Deep learning for embodied
vision navigation: A survey. arXiv:2108.04097
[Cs].
Zhu, Y., Zhu, F., Zhan, Z., Lin, B., Jiao, J., Chang, X., Liang, X., 2020b. Vision-dialog
navigation by exploring cross-modal memory. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 10730â€“10739.
Tianyao Zhang received his B.S. degree from the School
of Automation Science and Electrical Engineering, Beihang
University, Beijing, China in 2018. He is currently pursuing
his Ph.D. degree in School of Automation Science and
Electrical Engineering of Beihang University, Beijing, China.
His research topics include computer vision, unmanned
aerial vehicles, object detection and deep learning.
XiaoGuang Hu received the B.S. degree from Northeast
Dianli University, Jilin, China, in 1983, the M.S. degree
from Wuhan University of Hydraulic and Electric Engineer-
ing, Wuhan, China, in 1997, and Ph.D. degree from Harbin
Institute of Technology, Harbin, China, in 2003. Currently,
she is a professor as the school party secretary at the School
of Automation Science and Electrical Engineering, Beihang
University, Beijing, China. Her current research interests
include image processing, smart grid and embedded test
system.
Jin Xiao received the B.S. degree in Hunan University,
Changsha, China, in 2005, and the M.S. and Ph.D. degrees
in detection technology and automation devices from the
School of Automation Science and Electrical Engineering,
Beihang University, in 2008 and 2015. Currently, she is
an associate professor at the School of Automation Sci-
ence and Electrical Engineering, Beihang University, Beijing,
China. She has authored over 40 journals and conference
papers. Her current research interests mainly include image
processing, pattern recognition and embedded test system.
She presided over 20 research projects of the National
Natural Science Foundation of China, Aerospace Science and
Technology Foundation, and Aviation Science Foundation.
Guofeng Zhang received the B.S., M.S., and Ph.D. degrees
from the Harbin Institute of Technology, Harbin, China,
in 1984, 1987, and 1996. Currently, he is an associate
professor at the School of Automation Science and Electrical
Engineering, Beihang University, Beijing, China. His current
research interests include robust control, virtual reality,
computer control and Simulation.
22

