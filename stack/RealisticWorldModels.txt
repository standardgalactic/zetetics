Formalizing Two Problems of Realistic World-Models
Nate Soares
Machine Intelligence Research Institute
nate@intelligence.org
Abstract
An intelligent agent embedded within the real
world must reason about an environment which
is larger than the agent, and learn how to achieve
goals in that environment. We discuss attempts
to formalize two problems: one of induction,
where an agent must use sensory data to infer
a universe which embeds (and computes) the
agent, and one of interaction, where an agent
must learn to achieve complex goals in the uni-
verse. We review related problems formalized by
Solomonoﬀand Hutter, and explore challenges
that arise when attempting to formalize analo-
gous problems in a setting where the agent is
embedded within the environment.
1
Introduction
An intelligent agent embedded in the real world faces an
induction problem: how can it learn about the environ-
ment in which it is embedded, about the universe which
computes it? Solomonoﬀ(1964) formalized an induction
problem faced by agents which must learn to predict an
environment which does not contain the agent, and this
formalism has inspired the development of many useful
tools, including Kolmogorov complexity and Hutter’s
AIXI. However, a number of new diﬃculties arise when
the agent must learn about the environment in which it
is embedded.
An agent embedded in the world also faces an inter-
action problem: how can an agent learn to achieve a
complex set of goals within its own universe? Legg and
Hutter (2007) have formalized an “intelligence measure”
which scores the performance of agents that learn about
and act upon an environment that does not contain the
agent, but again, new diﬃculties arise when attempting
to do the same in a naturalized setting.
This paper examines both problems. Section 2 intro-
duces Solomonoﬀ’s formalization of an induction prob-
lem where the agent is separate from the environment,
Research supported by the Machine Intelligence Research
Institute (intelligence.org). Published as Technical report
2015–3.
and Section 3 discusses troubles that arise when at-
tempting to formalize the analogous naturalized induc-
tion problem. Section 4 discusses Hutter’s interaction
problem, and Section 5 discusses an open problem re-
lated to formalizing an analogous naturalized interaction
problem.
Formalizing these problems is important in order
to fully understand the problem faced by an intelligent
agent embedded within the universe: a general artiﬁcial
intelligence must be able to learn about the environment
which computes it, and learn how to achieve its goals
from inside its universe. Section 6 concludes with a
discussion of why a theoretical understanding of agents
interacting with their own environment seems necessary
in order to construct highly reliable smarter-than-human
systems.
2
Solomonoﬀ’s Induction Problem
Solomonoﬀ(1964) posed one of the earliest and sim-
plest descriptions of a problem in which an agent must
construct realistic world-models and promote correct
hypotheses based on observations, performing reasoning
akin to scientiﬁc induction. Intuitively, the problem
considered by Solomonoﬀruns as follows: the universe
is separated into an agent and an environment. Every
turn, the agent observes one bit of output from the
environment. The task of the agent is to, in each turn,
predict its next observation.
To formally describe the agent’s performance, it is
necessary to decide what counts as a possible environ-
ment, then to decide how to measure how well an agent
predicts an environment, and then to choose the distri-
bution over environments against which the agent will
be scored.
What counts as a possible environment?
In
Solomonoﬀ’s formalization, the goal is to consider hypo-
thetical agents which can learn an arbitrarily complex
environment, and so Solomonoﬀchooses the set of envi-
ronments to be anything that is computable. This can
be formalized by deﬁning the set of all environments
as the set T of all Turing machines with access to an
advance-only output tape.
1

How are an agent’s predictions scored? Consider an
environment M ∈T where Mn denotes the nth bit on
the output tape of M. Let an agent A be a function
which takes a string M≺t of observations made before
turn t, which outputs a prediction of Mt in the form of
a rational number interpreted as the probability that
Mt = 1. For convenience, deﬁne At := A(M≺t). To
score A against M on all time steps, it is necessary
to account for the fact that M may eventually stop
outputting bits; deﬁne ⌈M⌉to be the last turn in which
M outputs a bit (this value may be ∞). Then A may
be scored against M using standard logarithmic loss:1
SM(A) :=
⌈M⌉
X
t=1
Mt log(At) + (1 −Mt) log(1 −At). (1)
Against which distribution over Turing machines
should the agent be scored? The answer determines
which agents are considered to be “good predictors.” If
the agent is to be evaluated against its ability to learn
one speciﬁc environment, the trivial distribution con-
taining only that environment may be chosen—but then
the high-scoring agents would be agents which have that
environment hard-coded into them; this would hardly
be a problem of learning. The choice of distribution
deﬁnes the manner in which agents must be biased to
achieve a high score: how should predictors be biased?
The natural answer comes in the form of an intuition
canonized by William of Ockham seven hundred years
ago: predictors in the real world do well to prefer the
simplest explanation which ﬁts the facts. There are
exponentially more possible explanations of increasing
complexity (e.g. 2N N-bit explanations) and so any
particular explanation of greater complexity should have
less probability. Thus it seems natural to score the agent
according to a distribution in which simple environments
have greater weight than complex environments. The
most natural way to deﬁne a simplicity distribution over
Turing machines is to ﬁx some universal Turing machine
U,2 and assign probability 2−⟨M⟩to each Turing machine
M, where ⟨M⟩is the number of bits needed to specify
M to U.
Now Solomonoﬀ’s induction problem may be fully
described: An environment is any Turing machine M
with an advance-only output tape. An agent A is a
function which takes an output history and produces
a rational number interpreted as the probability that
the next observation will be 1. The agent is scored
according to SM(A) against a simplicity distribution.
Formally, Solomonoﬀ’s induction problem is the problem
of maximizing the “Solomonoﬀinduction” score
SI(A) :=
X
M∈T
2−⟨M⟩· SM(A).
(2)
1. This score may not converge, in the inﬁnite case, but
it is nevertheless useful for comparing agents.
2. U must be chosen such that P
M∈T 2−⟨M⟩= 1.
Like many good problem descriptions, this one lends
itself readily to an idealized unbounded solution, known
as Solomonoﬀinduction:
Solomonoﬀinduction. The agent starts
with a simplicity distribution over T . Upon
receiving the nth observation on, it condi-
tions its distribution on this observation by
removing all Turing machines that do not
produce n bits, or that do not write on as
the nth bit on their output tape. It then
predicts that the (n + 1)th bit is a 1 with
probability equal to the measure on remain-
ing Turing machines which write 1 as the
(n + 1)th bit on their output tape.
Indeed, it is in terms of this idealized solution that
Solomonoﬀoriginally posed his induction problem
(Solomonoﬀ1964).
A Solomonoﬀinductor is a very powerful predictor.
It can “learn” any computable environment: Solomonoﬀ
(1978) showed that given any computable probability
distribution over bit strings, a Solomonoﬀinductor’s
predictions will converge to the true probabilities.
With his induction problem, Solomonoﬀprovides a
full description of a scenario in which an agent must learn
an arbitrarily complex computable environment separate
from the agent. Insights from the induction problem
have proven useful in practice: This problem became the
basis of algorithmic information theory (Hutter, Legg,
and Vitanyi 2007). The simplicity prior over Turing
machines is the celebrated “universal prior” (Solomonoﬀ
2003). Solomonoﬀinduction is a crucial ingredient in
Hutter’s AIXI (2000) that solves the analogous universal
decision problem, and many of Solomonoﬀ’s insights
are present in the Legg-Hutter “universal measure of
intelligence” (2007). Solomonoﬀ’s work served as the
basis for Kolmogorov complexity (Solomonoﬀ1960), a
powerful conceptual tool in computer science.
Unfortunately, the prediction problem faced by
agents acting in the real world is not Solomonoﬀ’s in-
duction problem: it is a problem of an agent modeling
a world in which the agent is embedded as a subpro-
cess, where the agent is made out of parts of the world
and computed by the universe. Formally describing this
more realistic problem turns out to be signiﬁcantly more
diﬃcult.
3
The Naturalized Induction Problem
In Solomonoﬀ’s induction problem, the agent and its
environment are fundamentally separate processes, con-
nected only by an observation channel.
In reality,
agents are embedded within their environment; the uni-
verse consists of some ontologically continuous substrate
(atoms, quantum ﬁelds) and the “agent” is just a part
of the universe in which we have a particular interest.
What, then, is the analogous prediction problem for
2

agents embedded within (and computed by) their envi-
ronment?
This is the naturalized induction problem, and it
is not yet well understood. A good formalization of
this problem, on par with Solomonoﬀ’s formalization
of the computable sequence induction problem, would
represent a signiﬁcant advance in the theory of general
reasoning.
An analogous formalization of the naturalized in-
duction problem must yield a scoring metric akin to
SI(·), which scores an algorithm’s ability to predict its
environment. But what metric is this? There are at
least three open questions of naturalized induction:
First, given an algorithm, what is the set of all en-
vironments that the algorithm could have to induce?
It would be strange to score the agent against all com-
putable environments, as almost all Turing machines
will not in fact embed that algorithm.
Perhaps the
set of environments could be deﬁned with respect to
the proposed algorithm, as the set of Turing machines
which embed it. But how is that set deﬁned? What
does it mean for a Turing machine to “embed” an algo-
rithm? Intuitions about embeddings have been diﬃcult
to formalize.
If the naturalized induction problem is to capture the
problem of an agent learning about the real world, then
the set of environments must contain reality. The set of
all environments, therefore, must be a set of “possible
realities”: what structure is this? Does the set of all Tur-
ing machines actually contain our universe? Currently,
the Standard Model of physics seems computable to any
desired ﬁnite precision. But then again, reality looked
Newtonian to scientists in centuries past. If artiﬁcial
agents are to be able to surpass their programmers’ sci-
entiﬁc knowledge, a formalization of intelligent learning
should not presuppose the correctness of present-day
physical science. Modern theories as to the nature of
physical reality may turn out to be mistaken or in-
complete, and an ideal reasoner must be able to adapt
to such surprises. What set of possible environments
deﬁnitely contains reality, in light of the potential for
surprises?
Second, given an environment drawn from this set
of possible environments, how is the agent’s ability to
learn that environment scored? Are agents scored better
for constructing new sensors? Are they scored better
for ﬁnding some way to aﬀect their environment so as
to make it easier to predict? These are not questions
that can be reduced to Solomonoﬀ’s prediction task.
Formalizing inductive success is much more diﬃcult
when the environment can act on the agent’s internals,
and when the agent-environment boundary can shift
over time. Questions of evaluation are further covered
in sections 4 and 5.
Third, given a set of possible environments and a
scoring metric, what is the distribution against which an
agent should be scored? As in Solomonoﬀ’s induction
problem, this distribution must capture reality’s bias
towards simplicity, but deﬁning a simplicity distribution
over some set of “possible realities” may be nontrivial.
Of course, answers to these questions would be im-
practical at best and almost certainly uncomputable,
but they would yield conceptual tools by which practi-
cal programs implementing suﬃciently advanced agents
(that face the naturalized induction problem) could be
evaluated. For example, a formalization of naturalized
induction would likely shed light on questions about
how a reasoner should let the fact that it exists aﬀect
its beliefs, and may further lend insight into what sort
of priors an ideal reasoner would use. Unfortunately, it
is not yet clear how to approach the problems outlined
above.3
Can Solomonoﬀinduction be ported into a natu-
ralized context? Perhaps, but the application is not
straightforward. Even ignoring problems of ensuring
that the environment has something corresponding to
the “turns” and “observations” of Solomonoﬀ’s induction
problem, Solomonoﬀ’s approach solves the problem by
simply being larger than the environment: a Solomonoﬀ
inductor contains a distribution over all Turing ma-
chines, and one of those is, by assumption, the “real”
environment. Solutions of this form don’t apply when
the agent is a subprocess within the environment.
Computable approximations of Solomonoﬀinduction
can be limited to the consideration of only “reasonably
sized” environments, but this does not much help. Imag-
ine a Solomonoﬀinductor which only considers Turing
machines which can be speciﬁed in length less than l
and which run for at most t steps between each turn:4
this inductor would run for more than t steps per turn,
and therefore the environment it is in would run for
more than t steps per turn. The inductor would assign
zero probability to its own existence!
An agent embedded in an environment must reason
about an environment that is larger than itself; this con-
straint is inherent to naturalized induction. Solutions
will require agents to reason about environments which
they cannot compute. Reasoning of this form is known
as “logically uncertain” reasoning, and it may be possi-
ble to port a logically uncertain variant of Solomonoﬀ
into a naturalized context. However, a satisfactory the-
ory of reasoning under logical uncertainty does not yet
exist. (For further discussion, see Soares and Fallenstein
[2015a].)
4
Hutter’s Interaction Problem
Even a full description of naturalized induction would
not completely describe the problem faced by an intel-
3. Orseau and Ring (2012) give a characterization of the
problem which humans face, in implementing a space-time
embedded agent, but their problem description requires that
we provide a distribution ρ which already characterizes our
beliefs about the environment, and so sheds little light on
questions of naturalized induction.
4. Such as AIXItl, a computable approximation of Hutter’s
AIXI (2000).
3

ligence acting in the world. Real agents must not only
predict their environment, but act upon it.
With
this
in
mind,
Hutter
(2000)
extends
Solomonoﬀ’s induction problem to an interaction prob-
lem, in which an agent must not only learn the external
environment but interact with it. Hutter’s interaction
problem runs similarly to Solomonoﬀ’s induction prob-
lem: the universe is separated into agent and environ-
ment, and the agent gets to observe the environment
through an input channel. But now, an “output channel”
is added, which lets the agent aﬀect the environment by
one “action” per turn.
As before, a formalization requires answers to the
questions of (1) what counts as an environment; (2)
how an agent is scored on each environment; and (3)
against which distribution over environments the agent
is scored. In Hutter’s interaction problem, the ﬁrst and
last answers follow readily from Solomonoﬀinduction,
with some minor tweaks. It is the answer to the second
question, of scoring, where Hutter’s interaction problem
provides new insight.
Again, the set of all environments can naturally be
deﬁned as the set T of all Turing machines. However,
instead of having an advance-only output tape, environ-
ments are now Turing machines which take an observa-
tion/action history and compute the next observation
to be sent to the agent. That is, ﬁx some countable set
O of observations which can be sent from environment
to agent, and some countable set A of actions which
can be sent from agent to environment, and then con-
sider Turing machines which take a ﬁnite list of actions
and compute a new observation O. An agent, then, is
any function which takes a ﬁnite list of observations
and computes a new action A.5 Again, the distribution
over environments will be the “universal” simplicity dis-
tribution (with respect to some ﬁxed universal Turing
machine U).
It remains to decide how an agent is scored: what
counts as the “success” of an agent A interacting with an
environment M? Hutter (2000) formalizes interaction
as follows. First, observations are deﬁned such that one
part of the observation is a reward; that is, elements
of O are tuples (o, r) where r is a rational number
between 0 and 1, and o is additional observation data.
Let M A
t ∈O denote the tth output of the machine M
when interacting with A, and let AM
t
∈A denote the
5. Hutter (2000) uses a generalization in which both agent
and environment may be stochastic; in this case it is neces-
sary for agent and environment to receive a history of both
observations and actions. In the deterministic version, used
here for ease of exposition, the agent (environment) does not
need to be told the history of actions (observations) because
past actions (observations) may simply be recomputed.
tth action of A when interacting with M. That is,
M A
1 :=
M()
AM
1 :=
A(M())
M A
t :=
M(AM
≺t)
AM
t
:=
A(M A
⪯t).
Let rA
t denote the reward part of the observation M A
t .
Restrict consideration to the set Tr of environments
where rewards converge, that is, to environments M
such that 0 ≤P⌈M⌉
t=1 rA
t ≤1 for all agents A. The total
rewards observed by an agent A interacting with M are
then used to score the agent, that is, deﬁne
RM(A) :=
⌈M⌉
X
t=1
rA
t .
(3)
This function measures the ability of A to learn and
manipulate M in order to maximize observed rewards
over time.
This choice of scoring mechanism yields a full de-
scription of Hutter’s interaction problem: it describes a
setting where an agent must interact with an environ-
ment in order to learn and maximize rewards. Indeed,
this scoring metric is used to deﬁne the “universal mea-
sure of intelligence” of Legg and Hutter (2007):
LH(A) :=
X
M∈Tr
2−⟨M⟩· RM(A).
(4)
We refer to the problem of ﬁnding agents which score
highly according to LH(·) as Hutter’s interaction prob-
lem.
As before, this problem description lends itself read-
ily to an idealized solution. In this case, the solution is
Hutter’s AIXI (2000), which in fact was the mechanism
by which Hutter ﬁrst posed the interaction problem:
AIXI. The agent starts with a universal
prior, which it keeps consistent with obser-
vation using Solomonoﬀinduction (modiﬁed
in the natural way for this problem). AIXI
chooses its action as follows: it has some
ﬁxed time horizon h, and considers all possi-
ble sequences of h actions. It computes the
expected reward (according to its distribu-
tion over environments) for each sequence,
and then outputs the ﬁrst action in the se-
quence that leads to highest rewards.
AIXI is an incredibly powerful and elegant agent. As
noted by Veness et al. (2011), AIXI captures “the major
ideas of Bayes, Ockham, Epicurus, Turing, von Neu-
mann, Bellman, Kolmogorov, and Solomonoﬀ” in a sin-
gle equation. Barring a few minor quibbles,6 AIXI fully
6. The ﬁnite time horizon of AIXI is both arbitrary and
4

characterizes a solution to Hutter’s interaction prob-
lem: while AIXI is uncomputable, it demonstrates the
method by which a high LH(·) score may be attained. In-
deed, computable approximations of AIXI have already
yielded interesting results (Veness et al. 2011).
If the problem faced by intelligent agents acting in
the real world to achieve goals was characterized by
Hutter’s interaction problem, then this problem descrip-
tion would fully characterize the problem of construct-
ing smarter-than-human systems, and the problem of
general intelligence would be reduced to one of approxi-
mating AIXI.
However, Hutter’s interaction problem does not cap-
ture the problem faced by an agent acting in the real
world to achieve goals. Rather, it characterizes the prob-
lem of an agent attempting to maximize sensory rewards
from an environment that can only aﬀect the agent via
sensory information.
While this problem description has yielded many
insights, the distinction is important: the simplifying
assumptions of Hutter’s interaction problem mask a
number of diﬃcult open problems.
4.1
The Agent is Not Separate from the
Environment
Hutter’s interaction problem, like Solomonoﬀ’s induction
problem, assumes an impregnable separation between
the agent and the environment. In Solomonoﬀ’s case,
there is (ﬁguratively speaking) a small slit through which
the environment feeds sensory information to the agent.
Hutter adds a second slit, through which the agent
outputs motor signals to the environment. However,
the separation remains otherwise complete. Thus, the
questions of naturalized induction remain unanswered,
and Hutter’s interaction problem yields little new insight
there.
For this reason, Hutter’s interaction problem cannot
capture certain realistic scenarios that intelligent agents
may actually face: the Legg-Hutter measure of intelli-
gence is ill-deﬁned in any situation where the universe
cannot crisply be divided into “agent” and “environ-
ment,” when interactions cannot be crisply divided into
“input” and “output.” For example, consider the follow-
ing simple setting in which it matters that the agent is
embedded within its environment:
The Heating Up game. An agent A faces
a box containing prizes. The box is designed
to allow only one prize per agent, and A
may execute the action P to take a single
prize. However, there is a way to exploit
the box, cracking it open and allowing A to
take all ten prizes. A can attempt to do this
disconcerting: for any time horizon h, consider an environ-
ment with a button that gives −1 when pressed, pays +10
h steps thereafter, and pays out −100 on the step after that.
AIXI with time horizon h, after learning that this is the
environment, presses the button indeﬁnitely.
by executing the action X. However, this
procedure is computationally very expensive:
it requires reversing a hash. The box has a
simple mechanism to prevent this exploita-
tion: it has a thermometer, and if it detects
too much heat emanating from the agent, it
self-destructs, destroying all its prizes.
If the agent heats up too much, it gets reward 0, no
matter what action it takes. If it does not heat up too
much, then it gets reward 1 for action P or reward 10
for action X. But the amount of heat generated by the
agent, of course, is dependent upon which action the
agent chooses.
This scenario captures an important aspect of reality:
a generally intelligent agent must be able to consider
the consequences of overheating (along with many other
consequences of being embedded within a universe).
However, this scenario can’t be modeled as an interaction
problem. The Legg-Hutter measure of intelligence does
not pit agents against scenarios such as these; there is
no combination of M ∈Tr and A which captures this
sort of problem.
When evaluating an agent in a Heating Up game,
the agent cannot be treated as separate from the en-
vironment. Rather, the agent must be located within
the environment, and then somehow scored according
to what it “could have done.” Is it possible for a clever
agent to compute X without ever once getting too hot?
This question depends upon the speciﬁc environment
and upon the agent’s speciﬁc hardware.
This highlights a host of new “naturalized” questions:
Given an environment that embeds an agent, how is the
agent located in that environment? How are the actions
that it “could have taken” identiﬁed? In Hutter’s inter-
action problem, these questions are simpliﬁed away: the
input and output channels are clearly demarcated; the
environment is deﬁned in terms of the agent’s actions.
AIXI, when considering the eﬀects of various sequences
of actions, can simply run a Turing machine on the con-
sidered action sequence; the behavior of the environment
on that sequence of actions is well-deﬁned. When an
agent is embedded within an environment, the question
is more diﬃcult. For simplicity, consider a deterministic
agent embedded in a deterministic environment. What
does it mean to ask what “would happen” if the part of
the environment labeled “agent” outputs something it
doesn’t? How is the counterfactual deﬁned? Counter-
factual reasoning is not yet well understood (Soares and
Fallenstein 2015b).
Hutter’s interaction problem extends Solomonoﬀ’s
induction problem to capture a critical aspect of the
problem faced by intelligent agents: environments that
can be altered by agent decisions. This yields many
insights, but moving forward, a naturalized interaction
problem is necessary: how can an agent learn and ma-
nipulate the environment in which it is embedded, to
achieve some set of goals? It is this problem which
would fully characterize the problem faced by intelligent
5

systems acting in the real world.
4.2
Goals Cannot Be Speciﬁed in Terms of
Observation
Ignoring the need for a naturalized interaction problem,
Hutter’s interaction problem still does not quite cap-
ture the problem faced by an agent which must learn
and manipulate an environment to achieve some set of
goals. Rather, it characterizes the problem faced by
an agent which must maximize rewards, speciﬁed in
terms of observations. But most sets of goals cannot be
characterized in terms of the agent’s observations!
Consider an interaction problem in some approxima-
tion of reality where there is a crisp separation between
“agent” and “environment,” where the input and output
channels are clearly demarcated. The agent’s input is a
video stream, and rewards are only nonzero when there
are smiling human faces on the video screen. This agent,
if possessing of a high LH(·) score, will very likely gain
control of its input stream, such as by placing a photo
with many smiling faces in front of the camera and then
acting to ensure that it stays there.
Agents with high LH(·) scores are extremely eﬀective
at optimizing the extent to which their observations
contain rewards; these are not likely to be agents which
optimize the desirable feature of the world that the
rewards are meant to serve as a proxy for. Rather, they
are likely to be agents which are very good at taking
over their input channels.
Reinforcement learning techniques, such as having
the humans dole out rewards via some reward channel,
would not solve the problem. Humans could attempt to
prevent the agent from taking over its reward channel by
penalizing the agent whenever they notice it performing
actions that would give it control over rewards, and this
may prevent the agent from executing those plans for
a time. However, if the agent scores suﬃciently high
by LH(·), then once its dominant hypotheses about the
environment agree that the humans are controlling the
reward channel, it would act to mollify the programmers
while searching for ways to gain a decisive advantage over
them. If the agent is a suﬃciently intelligent problem-
solver, it may eventually ﬁnd a way to wrest control
of the reward channel away from the programmers and
maintain it permanently (Bostrom 2014, chap. 8).
Even faced with incredibly high-ﬁdelity input chan-
nels designed to be expensive to deceive, LH(·) rewards
agents that set up Potemkin villages7 which trigger the
reward using minimum resources. An agent optimiz-
ing a reward function only optimizes the actual goals
if achieving the goals is the cheapest possible way to
get the reward inputs. Guaranteeing such a thing is
nigh impossible: consider the genetic search process of
Bird and Layzell (2002), which, tasked with designing
an oscillating circuit, re-purposed the circuit tracks on
7. A common idiom named after Gregory Potemkin, who
set up fake villages to impress Empress Catherine II.
its motherboard to use as a radio which ampliﬁed oscil-
lating signals from nearby computers. Highly intelligent
systems might well ﬁnd ways to maximize rewards us-
ing clever strategies that the designers assumed were
impossible, or that they never considered in the ﬁrst
case.
A high LH(·) score indicates that an agent is ex-
tremely proﬁcient at commandeering its reward channel.
Therefore, this intelligence metric does not quite capture
the intuitive notion of how well an agent would fulﬁll a
given set of goals.
There is no all-purposes patch for this problem within
a sensory rewards framework. We do not care about
what the agent observes; rather, we care about what
actually happens. To evaluate the performance of an
agent, it is not suﬃcient to look only at the inputs
which the agent has received: one must also look at the
outcomes which the agent achieves.
5
Ontology Identiﬁcation
To evaluate how well an agent achieves some set of goals,
it is important to measure the resulting environment
history, not just the agent’s observation history.
In
Hutter’s interaction problem, an “environment history”
is the combination of a Turing machine along with an
observation/action history. But goals are not deﬁned
in terms of Turing machines and O/A histories; goals
are deﬁned in terms of things like money, or eﬃcient
airplane ﬂight patterns, or ﬂourishing humans. How do
you measure those things, given a Turing machine and
an O/A history?
As a matter of fact, it is quite diﬃcult to say what
terms our goals are speciﬁed in. To leave aside problems
of philosophy, and highlight the problem as it pertains
to world models, let us imagine that our goals are simple
and can be speciﬁed according to a structure that seems
fairly objective in our environment: assume that agents
will be evaluated by how much diamond they create in
their environment, where “diamond” is speciﬁed con-
cretely in terms of a speciﬁc atomic structure. That
is, the score of an agent is the count of carbon atoms
covalently bound to four other carbon atoms over time.
Now the goals are given in terms of atomic structures,
and the environment-history is given in terms of a Turing
machine paired with an O/A history. How is the Turing
machine’s representation of atoms identiﬁed?
This is the ontological identiﬁcation problem. What-
ever set is used for the set of all environments against
which the agent is measured, it must be possible to
inspect elements of that set and rate them according to
our goals, and for that it is necessary to interpret fea-
tures of the environment in terms of the ontology of the
goals. This is an aspect of the naturalized interaction
problem where Hutter’s interaction problem sheds little
insight.
It seems intuitively plausible that any detailed model
of reality, in an environment such as the real world
6

where diamond actually exists, must have some part
of its internal structure which roughly corresponds to
“atoms.” However, the problem is made more diﬃcult by
the fact that the ontology of the goals will not actually
perfectly match the ontology of reality: how are the
atoms identiﬁed in a model of reality which runs on
quantum mechanics? The model (if accurate) will still
have systems that correspond, in some fashion, to the
objects we call “atoms,” much as the atomic model has
systems corresponding to what we call “water.” However,
the correspondence may be convoluted and full of edge
cases. How can the ontology of the goals be reliably
mapped onto the ontology of the model? de Blanc (2011)
provides a preliminary examination of these questions,
but the problem remains open.
Ontology identiﬁcation is the ﬁnal step in the formal
speciﬁcation of the problem which is actually faced by an
intelligent agent acting in the real world and attempting
to fulﬁll some set of goals. To specify a measure of
how well an agent would achieve the intended goals
from within a universe, it must be possible to evaluate
a model of the universe in terms of the goals. This
requires the ability to take a model of reality, running
on unknown and potentially surprising physics, and ﬁnd
within it the ﬂawed and leaky abstractions with respect
to which our goals are deﬁned.
6
Discussion
The development of smarter-than-human machines could
have a large impact upon humanity (Bostrom 2014),
and if those systems are not aligned with human in-
terests, the result could be catastrophic (Yudkowsky
2008). Highly reliable agent designs are crucial, and
when constructing smarter-than-human systems, testing
alone is not enough to guarantee high reliability (Soares
and Fallenstein, forthcoming).
In order to justify high conﬁdence that a practi-
cal smarter-than-human system will perform well in
application, it is important to have a theoretical un-
derstanding of the formal problem that the practical
system is intended to solve. The problems faced by
smarter-than-human systems reasoning within reality
are inherently naturalized problems: real systems must
reason about a universe which computes the system, a
universe that the system is built from.
The formalization of Solomonoﬀ’s induction problem
yielded conceptual tools, such as the universal prior and
Kolmogorov complexity, which are useful for reasoning
about programs which predict computable sequences.
It would be diﬃcult indeed to construct highly reliable
practical heuristics that predict computable sequences
without understanding concepts such as simplicity pri-
ors.
We expect that naturalized analogs of the induc-
tion problem of Solomonoﬀand the interaction problem
of Hutter will yield analogous conceptual tools useful
for constructing systems that reason reliably about the
universe in which they are embedded. Just as the in-
telligence metric of Legg and Hutter (2007) fully char-
acterizes the problem of an agent interacting with a
separate computable environment to maximize rewards,
a corresponding metric derived from a naturalized in-
teraction problem would fully characterize the problem
faced by an intelligent agent achieving goals from within
a universe.
It is not yet clear, in principle, what sort of reason-
ers perform well when tasked with acting upon their
environment from within. Without a formal understand-
ing of the problem, it would be diﬃcult to justify high
conﬁdence in a system intended to face a naturalized in-
teraction problem in reality. It is our hope that gaining a
better understanding of these problems today will make
it easier to design highly reliable smarter-than-human
systems in the future.
References
Bird, Jon, and Paul Layzell. 2002. “The Evolved Radio and
Its Implications for Modelling the Evolution of Novel
Sensors.” In Congress on Evolutionary Computation.
CEC-’02, 2:1836–1841. Honolulu, HI: IEEE.
Bostrom, Nick. 2014. Superintelligence: Paths, Dangers,
Strategies. New York: Oxford University Press.
de Blanc, Peter. 2011. Ontological Crises in Artiﬁcial Agents’
Value Systems. The Singularity Institute, San Francisco,
CA, May 19. arXiv: 1105.3821 [cs.AI].
Hutter, Marcus. 2000. “A Theory of Universal Artiﬁcial
Intelligence based on Algorithmic Complexity.” arXiv:
0004001 [cs.AI].
Hutter, Marcus, Shane Legg, and Paul M. B. Vitanyi. 2007.
“Algorithmic Probability.” Scholarpedia 2 (8): 2572.
Legg, Shane, and Marcus Hutter. 2007. “Universal Intelli-
gence: A Deﬁnition of Machine Intelligence.” Minds and
Machines 17 (4): 391–444.
Orseau, Laurent, and Mark Ring. 2012. “Space-Time Em-
bedded Intelligence.” In Artiﬁcial General Intelligence:
5th International Conference, AGI 2012, 209–218. Lec-
ture Notes in Artiﬁcial Intelligence 7716. New York:
Springer.
Soares, Nate, and Benja Fallenstein. Forthcoming. “Aligning
Superintelligence with Human Interests: A Technical Re-
search Agenda.” In The Technological Singularity: Man-
aging the Journey, edited by Jim Miller, Roman Yam-
polskiy, Stuart Armstrong, and Vic Callaghan, vol. 2.
Springer.
. 2015a. Questions of Reasoning Under Logical Uncer-
tainty. Technical report 2015–1. Berkeley, CA: Machine
Intelligence Research Institute. https://intelligence.
org/files/QuestionsLogicalUncertainty.pdf.
. 2015b. “Toward Idealized Decision Theory.” arXiv:
1507.01986 [cs.AI].
Solomonoﬀ, Ray J. 1960. A Preliminary Report on a General
Theory of Inductive Inference. Technical report ZTB-138.
Cambridge, MA: Zator Co., November.
7

Solomonoﬀ, Ray J. 1964. “A Formal Theory of Inductive
Inference. Part I.” Information and Control 7 (1): 1–22.
. 1978. “Complexity-Based Induction Systems: Com-
parisons and Convergence Theorems.” IEEE Transac-
tions on Information Theory 24 (4): 422–432.
. 2003. “The Kolmogorov Lecture: The Universal Dis-
tribution and Machine Learning.” The Computer Jour-
nal 46 (6): 598–601.
Veness, Joel, Kee Siong Ng, Marcus Hutter, William Uther,
and David Silver. 2011. “A Monte-Carlo AIXI Approx-
imation.” Journal of Artiﬁcial Intelligence Research
40:95–142.
Yudkowsky, Eliezer. 2008. “Artiﬁcial Intelligence as a Pos-
itive and Negative Factor in Global Risk.” In Global
Catastrophic Risks, edited by Nick Bostrom and Mi-
lan M. Ćirković, 308–345. New York: Oxford University
Press.
8

