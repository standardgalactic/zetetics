Vol.:(0123456789)
Neural Processing Letters (2019) 50:2481–2492
https://doi.org/10.1007/s11063-019-10024-w
1 3
Stable and Refined Style Transfer Using Zigzag Learning 
Algorithm
Lingli Zhan1   · Yuanqing Wang1
Published online: 27 March 2019 
© Springer Science+Business Media, LLC, part of Springer Nature 2019
Abstract
Recently, style transfer based on the convolutional neural network has achieved remark-
able results. In this paper, we extend the original neural style transfer algorithm to amelio-
rate the instability in the reconstruction of certain structural information, and improve the 
ghosting artefacts in the background of image which with low texture and homogeneous 
areas. For that end, we adopt zigzag learning strategy: The model parameters are optimized 
to an intermediate target firstly, then let the model converge to the final goal. We show the 
zigzag learning to multi-sample model which is fabricated from resampling the style input 
and to loss function that is split into two sections. And also, we demonstrate experimentally 
the effectiveness of the proposed algorithm and provide its theoretical analysis. Finally we 
show how to integrate the zigzag learning strategy in fast neural style transfer framework.
Keywords  Style transfer · Neural networks · Deep learning · Painting transfer
1  Introduction
Style transfer is re-draw image in a particular style such as the style of fantastic artworks. 
Neural style transfer algorithm inspired by deep convolutional neural networks(CNN) is pro-
posed by Gatys et al. [1–3] first. It takes two images 퐱퐬 and 퐱퐜 as input, and applies to migrat-
ing the style of 퐱퐬 to the semantic content of 퐱퐜 , produces a new image ̂퐲 finally. The variables 
of “style” and “content” are both defined in terms of CNN layers statistics. Since Gatys et al. 
[2, 3] reproduced some famous painting styles on natural image with state-of-the-art quality, 
lots of follow up studies were proposed. One of the major research directions is to address 
the speed and computational cost issue at the expense of losing some flexibilities [4–8]. Such 
as Johnson et al. [4] added generator network model based on Gatys’ algorithm, the network 
was optimized by updating the model iteratively instead of updating pixels directly.
However there are still other limitations in transfer quality and stability [13]. One possible 
solution to these issues is specific to a particular style, or subject content image. Such as [9] 
 *	 Lingli Zhan 
	
zhan_lingli@163.com
1	
School of Electronic Science and Engineering/Key Laboratory of Intelligent Optical Sensing 
and Manipulation, Ministry of Education, Nanjing University, 163 Xianlin Road, Qixia District, 
Nanjing, Jiangsu Province, China

2482
	
L. Zhan, Y. Wang 
1 3
focused on fine turning the results of head portraits. [10–12] present another solution to the 
problem. By introducing spatial mask separating image in different scenes, ‘content-aware’ 
neural style transfer specific spatial style correspond to specific location. The disadvantage of 
this method is that a mask must be given for each image.
We give our solutions to the above problems. In this paper, we propose a zigzag learning 
method on the baseline approach of Gatys et al. [3] to improving the transfer effect while 
using less manual tagging to get a wider range of applications. We ameliorate the instabil-
ity in the reconstruction of certain structural information by applying zigzag learning to 
style image. We improve the ghosting artefacts in the background of image which with 
low texture and homogeneous areas by applying zigzag learning to loss function. The cor-
responding experimental results are given in the following sections. Importantly, we dem-
onstrate the proposed algorithm quantitatively by introducing a new metric for assessing 
the transfer quality. In addition, we also provide their theoretical analysis in the discussion 
sections. Although we haven’t solved these problems thoroughly, the proposed algorithm 
also provides a promising direction to design style transfer network with more user control.
2  Approach
2.1  Neural Style Transfer
In this section, we briefly introduce the neural style transfer methods of Gatys et al. [3]. 
Input image 퐱 is mapped to feature space Fl
ij by a convolution neural network :
Fl
ij ∈RNl×Ml , where Fl
ij is the activation of the ith filter at position j in layer l , and Ml is 
the number of the feature maps in the layer l and Ml is the height times the width of the 
feature map. Image style G is represented by the Gram matrices of the neural activations:
The basic idea behind G is to consider neural style transfer as a domain adaptation prob-
lem which can be solved by minimizing a specific Maximum Mean Discrepancy.
The loss network is represented in Fig.  1a, the pastiche image 
⌢퐲 alone with content 
image 퐱퐜 and style image 퐱퐬 are fed into the loss network to calculate the mean-squared 
distance. By pass image through a convolutional neural network such as VGG [14], image 
is decomposed into two components: content representation F and style representation G . 
The content loss is defined as:
The style loss is defined as:
(1)
Fl
ij = (퐱)
(2)
Gl
ij =
Ml
∑
k=1
Fl
ikFl
jk
(3)
Lcontent=1
2
Nl
∑
i=1
Ml
∑
j=1
(
Fl
ij(퐱퐜) −Fl
ij(̂퐲)
)2
(4)
Lstyle=
L
∑
l=0
휔lEl,

2483
Stable and Refined Style Transfer Using Zigzag Learning…
1 3
The objective function Ltotal is a weighted sum between the style loss and the content loss:
For a given input (퐱퐜, 퐱퐬, 퐲ퟎ) and the objective function Ltotal , the pastiche image 
⌢퐲 is solved 
by the gradient ascent algorithm, as shown in Fig. 1b.
(5)
El=
1
4N2
l M2
l
Nl
∑
i=1
Nl
∑
j=1
(
Gl
ij(퐱퐬) −Gl
ij(̂퐲)
)2
,
(6)
Ltotal = 훼Lcontent(xc,̂y) + 훽Lstyle(xs,̂y)
Content representation 
cx
sx
ˆy
Style 
Representation 
Style 
Representation 
Style 
Representation 
Style 
Representation 
2
G
3
G
4
G
Convolutional Neural Network
Loss Network
iF
α
β
=
+
total
content
style
1
G
Loss network diagram. 
(
)
s
c
0
x ,x ,y
ˆy
→
total
 Network training diagram. 
(a)
(b)
Fig. 1   Neural style transfer network diagram. a Loss network diagram. b Network training diagram

2484
	
L. Zhan, Y. Wang 
1 3
2.2  Zigzag Learning to Style Image
The original algorithm is not always robust in any case. For example, Picasso’s “Deux 
Femmes” is selected as the style image to be imitated, a picture of a lizard downloaded 
online selected as the content image of the style to be transferred. It can be seen in Fig. 4c 
that the style transfer image obtained through the original algorithm has shown significant 
distortion on the lizard’s mouth.
Next, it will be illustrated how to enhance the stability of the algorithm with the idea of 
zigzag learning. First through sampling, for example, some operations  such as tailoring, 
rotation, zooming, and stitching are carried out for the original style image 퐱퐬 , to artificially 
acquire some extra samples:
Then, different from the original algorithm, the iteration on pastiche image ̂퐲 is divided 
into multiple stages, where the style image 퐱퐬퐢 is set as the input corresponding to the ith 
learning stage, proceeding to learn and update based on the previous stage ̂퐲퐢−ퟏ . Therein, 퐱 
is the random white noise, and 퐱퐜 is the input content image. The Fig. 2 shows the circum-
stance when performing zigzag learning once with the two samples. The L1
total and L2
total are 
defined in Eq. (8).
2.3  Zigzag Learning to Loss Function
When image containing background or low texture areas, poor effects will appear as shown 
in Fig. 7c. It is expected to improve this problem without increasing the complexity of the 
algorithm or adding manual marking information, enable the composite images to bring 
more visual pleasure.
In the original algorithm, the loss function is a sum of style loss Lstyle and content loss 
Lcontent . Different from the original algorithm, the content component and the style com-
ponent in the objective function are distributed into two stages for successive approxima-
tion. As shown in Fig. 3, first, the iterative computation is executed on the content dis-
tance Lcontent as the loss function, until reaching a slightly convergent value, to output the 
pastiche image ̂퐲ퟏ , which is then considered as the initialization condition, to conduct the 
second stage learning with Ltotal as the loss functions, for the iterative computation until ̂퐲 
is absolutely converged.
(7)
(퐱퐬) →{퐱퐬ퟏ, 퐱퐬ퟐ, … 퐱퐬퐢… 퐱퐬퐧
}
(8)
{
L1
total = 훼Lcontent(퐱c, ̂퐲)+훽Lstyle(퐱s1, ̂퐲)
L2
total = 훼Lcontent(퐱c, ̂퐲)+훽Lstyle(퐱s2, ̂퐲)
(
)
S1
C
0
x ,x ,y
ˆ 1
y
→
ˆ
(
)
S2
C
1
x ,x ,y
ˆ 2
y
→
1
total
2
total
Fig. 2   Zigzag learning to style image training diagram

2485
Stable and Refined Style Transfer Using Zigzag Learning…
1 3
Fig. 3   Zigzag learning to loss 
function training diagram
0
(
)
S
C
x ,x ,y
ˆ 1
y
→
ˆ 2
y
→
content
total
Fig. 4   Zigzag learning to style image. a Input with content image and original style image S. b Input style 
image set {퐱퐬ퟏ, 퐱퐬ퟐ
} . c Output from neural style transfer [3]. d Output with zigzag learning to style image

2486
	
L. Zhan, Y. Wang 
1 3
3  Experiments and Discussions
3.1  Zigzag Learning to Style Image
3.1.1  Experiments
In order to verify the effectiveness of the algorithm, the actual effect of the our algorithm 
is compared with the baseline approach of Gatys et al. [3]. “Deux Femmes” is the style 
image, and the lizard picture is the content image. All the model and initialization param-
eters applied in the experiment keep consistency with that of the baseline approach, both 
adopting relu3_3 layer as the content description. First, the style image is resampled: the 
original style image is left shifted by 40 pixels, to obtain new style samples 퐱퐬ퟏ , and hori-
zontally flipped to obtain new style samples 퐱퐬ퟐ . The images resampled twice compose one 
style image set {퐱퐬ퟏ, 퐱퐬ퟐ
} as the sample of zigzag learning. Figure 4c is the result apply-
ing the original style image with stopping criterion eps = 0.005, while Fig. 4d is the result 
applying zigzag optimization method. In the test, 퐱퐬ퟏ is fed into the network for iteration 
until the relative change is less than 0.10, and then 퐱퐬ퟐ is introduced for continuous iteration 
until the eps less than 0.005. As shown in the Fig. 4d, the distortion of the lizard’s mouth 
is well eliminated. Besides, the performance of the image texture on lizard’s legs has also 
been further improved.
To better compare the result with original method,We also import structural similarity 
index (SSIM) [15] to give a quantitative comparison on the proposed algorithms. SSIM is 
one of the best methods of image quality assessment, and this metric is introduced by [16] 
firstly for assessing the quality for image synthesis. SSIM values range between 0.0 and 
1.0, higher SSIM values correspond to higher structure similarity. As shown in Table 1. 
The numerical experiments further prove that our method has improved the result. And 
Fig. 5 is supplementary experiment of style zigzag learning.
3.1.2  Discussion
Through the experiment, it is discovered that satisfying effects are obtained after the same 
iterations, without increase in the time complexity. The reason why it can be improved 
depends on that, on one hand there is only one style sample, so that the features extracted 
by the receptive field of neural network are limited; on the other hand, the objective func-
tion just evaluates the style consistency between the generative image and style image in 
the second-order statistical significance, which is not complete. When part of variables fall 
into local minimum, the loss has tended to zero, causing the model parameters unable to 
Table 1   SSIM scores, evaluated 
on the experimental results in 
Fig. 4
Batch label
Gatys et al. [3]
Ours
Batch 1
0.1722
0.1773
Batch 2
0.2849
0.3272
Batch 3
0.2743
0.5021
Batch 4
0.2039
0.2595
Full image
0.2686
0.3306

2487
Stable and Refined Style Transfer Using Zigzag Learning…
1 3
update. The artificial pseudo samples can provide new optimization goals for the model, 
thus to continue to optimize model parameters again to solve the problem.
We can also explain the zigzag learning to image style from the perspective of ensemble 
learning. Our algorithm actually draws lessons from both bagging algorithm [17] using 
overlapping sampling subsets (style transfer is one shot learning) and boosting algorithm 
[18] with serial training. In theory, there is no correlation between the quality of the learner 
and the input order of the sub-samples. From the experimental results, the input order of 
sub-samples has no significant effect on the final results. However, how to make learners 
choose sub-samples adaptively in the training process to get better results is worth further 
study.
This method can also be applied for multi-style fusion, that is, to directly apply the pro-
vided paintings of diverse styles as the input at different stages, instead of resampling the 
style images manually to obtain them (Fig. 6).
3.2  Zigzag Learning to Loss Function
3.2.1  Experiments
The effectiveness of the algorithm continues to be demonstrated by experiments as follows, 
still compared with the baseline approach of Gatys et al. [3]. Picasso’s “Deux Femmes” 
is selected as the style image, because compared with paintings of other styles, Picasso’s 
paintings reflect more prominently on this problem, as shown in Fig.  7c, f which is a 
Gatys et al. [3]
ours
content
style
Fig. 5   Supplementary experiment of style zigzag learning

2488
	
L. Zhan, Y. Wang 
1 3
Chinese ink painting. This is mainly because Chinese ink paintings mostly consist of points 
of different forms, while the Picasso’s paintings tend to be two-dimensional, mostly com-
posed of line/plane and color space. Thus, Chinese ink painting has a more exquisite style 
than Picasso’s “The Sitting Woman”). The content image is an eagle picture that contains 
an absolute black background and duplicate texture. In the experiment, Lcontent is applied 
as the objective function, to be trained until eps < 0.15, and then the obtained generative 
image is regarded as the initialization parameter for the second stage, to be computed by 
the network with Ltotal . After only 50 iterations, the loss is less than 0.005, to obtain the 
final generative image as shown in Fig. 7d, c is the result generated by the method of Gatys 
et al. [3]. Comparing the background of the two images, it can be seen that our algorithm 
gains remarkable improvement in effect. In addition, the generative effect on the white 
feather of the eagle is also more excellent than that of the original algorithm, with the 
details more smooth and slightly.
The numerical experiments are showed in Table 2. And supplementary experiments of 
loss function zigzag learning are show in Fig. 8.
3.2.2  Discussion
Here below are the analysis and reason why the algorithm works. First of all, Lcontent is 
applied as the loss function for computation, which actually means reconstructing high-
level semantic representations about content images. Then the high-level semantic repre-
sentations of the content image is applied as the initial input for the style transfer, that is, a 
(a)
(b)
(c)
(a) 
S1
(a) 
S2
Fig. 6   Multi-style fusion with zigzag learning to style images. a Content image and two different style 
images. b Output from the style S1. c Output combining styles S1 and S2

2489
Stable and Refined Style Transfer Using Zigzag Learning…
1 3
Fig. 7   Zigzag learning to loss function. a Content image with low texture and homogeneous areas and style 
image “Deux Femmes”. b Output of 50 iterations using content representation loss Lcontent . c Output from 
neural style transfer [3] with style “Deux Femmes”. d Output with zigzag learning to loss function. e The 
same content image as a and a Chinese ink painting as the style image. f Output from neural style transfer 
[3] with Chinese ink painting
Table 2   SSIM scores, evaluated 
on the experimental results in 
Fig. 7
Batch label
Gatys et al. [3]
Ours
Batch 1
0.0778
0.0969
Batch 2
0.3023
0.2448
Batch 3
0.1043
0.4670
Batch 4
0.0929
0.1763
Batch 5
0.1338
0.2030
Full image
0.1844
0.2102

2490
	
L. Zhan, Y. Wang 
1 3
priori information is transmitted to the network, thus, it improves the stability of computa-
tion in a certain extent, and avoids overfitting in low texture or duplicate texture area.
3.3  Zigzag Learning to Fast Neural Style Transfer Model
For fixed style images, the generative neural methods based on model iteration algorithm 
proposed by Johnson et al. [4]. can rapidly generate style transfer images of any content, 
which is a fast neural style transfer algorithm. In this section, it will be introduced how to 
transfer zigzag learning algorithm to the fast neural style transfer algorithm.
The difference between the fast neural style transfer algorithm and the neural style trans-
fer algorithm lies in that once a generative model 휙 is added during the fast algorithm, the 
loss function is utilized to optimize the parameter of the model, instead of directly optimiz-
ing the pixel value of the generative image. Therefore, the application of the zigzag method 
in the Fast Neural Style Transfer algorithm is reflected in the zigzag learning for the gen-
erative model. The flow chart of the algorithm is shown in Figs. 9, 10.
Fig. 8   Supplementary experiment of style zigzag learning. I Output from neural style transfer [3]. II Output 
with zigzag learning to loss function

2491
Stable and Refined Style Transfer Using Zigzag Learning…
1 3
3.3.1  Zigzag Learning to Style Image
Similar to the application in Sect. 2.2, a group of style image set {퐱퐬ퟏ, 퐱퐬ퟐ, … 퐱퐬퐢… 퐱퐬퐧
} 
is obtained after resampling the style images first. At the first stage, (퐱퐬ퟏ, 퐱퐜
) is input to 
train the generative model 휙1 , followed by the second stage of learning with (퐱퐬ퟐ, 퐱퐜
) 
as initialized variables, finally to obtain the network parameter of a stable generative 
model 휙.
3.3.2  Zigzag Learning to Loss Function
The network adopts loss function Lcontent for training first, to obtain the generative model 
휙1 . Then it applies the parameter of 휙1 as the initialization parameter of the generative 
model and Ltotal as the loss function, to execute the training until convergence, thus to 
obtain the final generative model 휙.
4  Conclusion
The key insight of this paper is that the style transfer introduced by Gaty et al. [3]. can 
be improved by introducing zigzag learning method. Based on the zigzag learning to 
multi-sample which are fabricated from resampling input style image, we have improved 
the stability of the algorithm; and based on the zigzag learning to loss function, we have 
ameliorated the ghosting artefacts in background of image which with low texture and 
Fig. 9   Zigzag learning to style 
image training diagram for fast 
neural style transfer
(
)
s1
c
0
x ,x ,y
1φ
→
(
)
s2
c
0
x ,x ,y
2φ
→
ˆy
→
ˆy
total
total
Fig. 10   Zigzag learning to loss 
function training diagram for fast 
neural style transfer
(
)
s
c
0
x ,x ,y
1φ
→
(
)
s
c
0
x ,x ,y
2φ
→
ˆy
→
ˆy
content
total

2492
	
L. Zhan, Y. Wang 
1 3
homogeneous areas. Furthermore, how to transfer zigzag learning algorithm in the genera-
tive neural style transfer framework has been briefly discussed.
How to obtain high-level semantic representation of images and decompose images into 
the independent factors of human visual perception have always been a difficult problem in 
machine vision. Neural style transfer provides a possibility for the solution of this problem. 
In the future work, we will focus on further improving the effect of style transfer by intro-
ducing more priori knowledge to the neural network.
References
	 1.	 Gatys LA, Ecker AS, Bethge M (2015) Texture synthesis using convolutional neural networks. Adv 
Neural Inf Process Syst 70(1):262–270
	 2.	 Gatys LA, Ecker AS, Bethge M (2015) A neural algorithm of artistic style. arXiv preprint arXiv​
:1508.06576​
	 3.	 Gatys LA, Ecker AS, Bethge M (2016) Image style transfer using convolutional neural networks. In: 
Computer vision and pattern recognition. IEEE, pp 2414–2423
	 4.	 Johnson J, Alahi A, Li FF (2016) Perceptual losses for real-time style transfer and super-resolution. In: 
European conference on computer vision. Springer, Cham, pp 694–711
	 5.	 Ulyanov D, Lebedev V, Vedaldi A, Lempitsky V (2016) Texture networks: feed-forward synthesis of 
textures and stylized images. arXiv preprint arXiv​:1603.03417​
	 6.	 Ulyanov D, Vedaldi A, Lempitsky V (2016) Instance normalization: the missing ingredient for fast 
stylization. arXiv preprint arXiv​:1607.08022​
	 7.	 Dumoulin V, Shlens J, Kudlur M (2016) A learned representation for artistic style. arXiv preprint 
arXiv​:1610.07629​
	 8.	 Li C, Wand M (2016) Precomputed real-time texture synthesis with markovian generative adversar-
ial networks.  In: European Conference on Computer Vision. Springer International Publishing, pp 
702–716
	 9.	 Selim A, Elgharib M, Doyle L (2016) Painting style transfer for head portraits using convolutional 
neural networks. ACM Trans Graph 35(4):1–18
	10.	 Yin R (2016) Content aware neural style transfer. arXiv preprint arXiv​:1601.04568​
	11.	 Chen YL, Hsu CT (2016) Towards deep style transfer: a content-aware perspective. In: British machine 
vision conference, pp. 8.1–8.11
	12.	 Champandard AJ (2016) Semantic style transfer and turning two-bit doodles into fine artworks. arXiv 
preprint arXiv​:1603.01768​
	13.	 Risser E, Wilmot P, Barnes C (2017) Stable and controllable neural texture synthesis and style transfer 
using histogram losses. arXiv preprint arXiv​:1701.08893​
	14.	 Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition. 
arXiv preprint arXiv​:1409.1556
	15.	 Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image quality assessment: from error visibility 
to structural similarity. IEEE Trans Image Process 13(4):600–612
	16.	 Odena A, Olah C, Shlens J (2016) Conditional image synthesis with auxiliary classifier gans. arXiv 
preprint arXiv​:1610.09585​
	17.	 Breiman L (1994) Bagging predictors. Mach Learn 24(2):123–140
	18.	 Freund Y, Schapire RE (1996) Experiments with a new boosting algorithm. In: Machine learning: pro-
ceedings of thirteenth international conference. ACM, pp 148–156
Publisher’s Note  Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.

