arXiv:2110.03789v1  [cs.LG]  7 Oct 2021
Knowledge Sheaves: A Sheaf-Theoretic Framework
for Knowledge Graph Embedding
Thomas Gebhart
Department of Computer Science
University of Minnesota
gebhart@umn.edu
Jakob Hansen
Department of Mathematics
The Ohio State University
hansen.612@osu.edu
Paul Schrater
Department of Computer Science
University of Minnesota
schrater@umn.edu
Abstract
Knowledge graph embedding involves learning representations of entities—the
vertices of the graph—and relations—the edges of the graph—such that the re-
sulting representations encode the known factual information represented by the
knowledge graph are internally consistent and can be used in the inference of new
relations. We show that knowledge graph embedding is naturally expressed in
the topological and categorical language of cellular sheaves: learning a knowl-
edge graph embedding corresponds to learning a knowledge sheaf over the graph,
subject to certain constraints. In addition to providing a generalized framework
for reasoning about knowledge graph embedding models, this sheaf-theoretic per-
spective admits the expression of a broad class of prior constraints on embed-
dings and offers novel inferential capabilities. We leverage the recently developed
spectral theory of sheaf Laplacians to understand the local and global consistency
of embeddings and develop new methods for reasoning over composite relations
through harmonic extension with respect to the sheaf Laplacian. We then imple-
ment these ideas to highlight the beneﬁts of the extensions inspired by this new
perspective.
1
Introduction
Knowledge graphs are structured knowledge bases which encode information about entities and their
relationships. They are deﬁned by sets of triplets—two entities and a relation between them—that
represent facts about some domain. Modern knowledge graphs may contain millions of entities and
billions of relational facts. As a result, representing the knowledge about the world encoded by a
knowledge graph in a way amenable to inferential analysis is of great practical importance.
Knowledge graph embedding has emerged as an important approach to encoding the entities and
relations represented by knowledge graphs. In this approach, a vectorized structure for graph entities
and relations is learned, providing a representation of fact triplets that can be used for tasks like
knowledge graph completion, multi-hop reasoning, and other forms of inference [7, 22]. These
knowledge graph embedding approaches attempt to jointly embed entities and relations into vector
spaces so that the resulting embeddings reﬂect the truth values of the knowledge graph triplets. A
wide variety of knowledge graph embedding approaches exist [4, 5, 20, 21, 23, 27, 32, 38, 41, 44],
each with its own motivation and theoretical framework.
Preprint. Under review.

There are various taxonomies classifying these approaches [22], but these are far from exhausting
the space of possibilities. In particular, describing and enforcing priors about the entities and rela-
tions by embedding in more complex spaces has attracted recent interest [9, 35, 44], but a general
framework for formulating these types of constraints is still lacking. Our aim is to produce a theo-
retically sophisticated framework that gives practitioners freedom to more fully explore the range of
possibilities for knowledge graph embeddings.
Our approach stems from a category-theoretic and algebro-topological perspective. We use the re-
cently developed tools of cellular sheaves [8] and their Laplacians [17] to construct a natural frame-
work for knowledge graph embedding. After introducing cellular sheaves, we show that many of the
goals of knowledge graph embedding—e.g., local agreement across relations, global consistency,
typed representations, and multi-hop reasoning—can be readily formalized and interpreted in this
sheaf-theoretic language. We show that knowledge graph embedding may be viewed as a problem
of learning a cellular sheaf describing the knowledge graph that, in the simplest formulation, in-
volves the sheaf Laplacian, a generalization of the graph Laplacian [16]. We observe that a number
of popular knowledge graph embedding approaches may be subsumed within this sheaf embedding
formalization, providing a uniform language for reasoning about the regularization and extension of
existing embedding approaches. We discuss a number of representational beneﬁts for knowledge
graph embedding inherited from this sheaf-theoretic perspective: the freedom to embed entities in
vector spaces of varying dimension, the ability to control symmetry or antisymmetry of relations,
and a natural method for reasoning over complex, multi-hop queries by harmonic extension. We
implement these ideas and validate the performance of a number of models in answering a range
of complex query structures derived from two benchmark datasets. This validation process also
showcases a theoretically-sound method for adapting—via harmonic extension—a number of prior
knowledge graph embedding approaches based on simple knowledge graph completion to more
complex queries.
2
Cellular Sheaves
Sheaf theory has played a central role in the development of modern algebraic and differential ge-
ometry, along with many other uses in contemporary mathematics. Abstractly, a sheaf is a mathe-
matical object which tracks the assignment of data to open sets of a topological space. While sheaf
theory has existed for nearly a century, only in the past decade has a discretized and computation-
ally tractable theory of sheaves received signiﬁcant attention and development [8]. These cellular
sheaves are constructed on cell complexes, a class of combinatorial spaces which include graphs as
a special case. For knowledge graph embeddings, it is sufﬁcient to consider only cellular sheaves
over graphs.
Deﬁnition 1 A cellular sheaf F on a graph G = (V, E) consists of the following data: a vector
space F(v) for each vertex v ∈V of G, a vector space F(e) for each edge e ∈E of G, and a linear
map Fv P e : F(v) →F(e) for each incident vertex-edge pair v P e of G.
The sheaf structure over a graph associates a space of data to each node and edge. These spaces of
data are called the stalks of the sheaf, while the linear maps between them are termed restriction
maps. It is important to note that this deﬁnition does not require each vertex stalk F(v) to be
identical for all vertices v; the same freedom holds for edge stalks spaces F(e). The vector space of
assignments of data to all vertices of G is denoted C0(G; F) and is called the space of 0-cochains
valued in F. That is, an element of C0(G; F) is a tuple (xv)v∈V with xv ∈F(v) for all v. Likewise,
the vector space of tuples (xe)e∈E is denoted C1(G; F) and is called the space of 1-cochains. It
is often convenient to view cochains as a vector constructed by concatenating the stalkwise vectors
(xv∈V ) = x ∈Ci(G; F). The status of x as either a 0 or 1-cochain will be made explicit if not
obvious from context.
For an edge e between vertices u and v, we say that a choice of data xv ∈F(v), xu ∈F(u) is
consistent over e if Fv P exv = Fu P exu. Each edge of G imposes a constraint on C0(G; F) by
requiring the data over its two incident vertices to be consistent over the edge. The subspace of
C0(G; F) consisting of cochains that satisfy all these constraints is the space of global sections of
F, and is denoted H0(G; F).
2

The space of global sections H0(G; F) is the kernel of a linear map δ : C0(G; F) →C1(G; F)
called the coboundary. The coboundary is deﬁned by
(δx)e = Fv P exv −Fu P exu
for each oriented edge e = u →v. Therefore, if x ∈ker δ, then Fv P exv = Fu P exu for every
edge e = u ∼v. From the coboundary operator we may construct the sheaf Laplacian LF = δT δ,
which is a positive semideﬁnite linear operator on C0(G; F) with kernel H0(G; F) [17].
A cellular sheaf is an extension of the structure of a graph. Rather than simply recording connections
between nodes, it speciﬁes relationships between data associated with those nodes. Standard con-
structions like graph Laplacians implicitly work with the constant sheaf on a graph: the sheaf where
every data space is the same vector space (e.g., R or Rn), and all restriction maps are the identity.
This is a very simple relationship between nodes, and can be greatly generalized in the sheaf setting.
The sheaf Laplacian provides a measure of “smoothness” or consistency for elements of C0(G; F).
While the elements of H0(G; F) represent globally consistent choices of data on vertices, we can
use the sheaf Laplacian to quantify how close any data assignment in C0(G; F) is to consistency:
xT LFx =
X
e=u∼v∈E
∥Fu P exu −Fv P exv∥2.
(1)
When xT LFx = 0, x ∈H0(G; F). The closer xT LFx is to zero, the more nearly consistent x is.
The behavior of this quadratic form is closely related to the spectrum of LF. For example, the eigen-
vectors corresponding to the smallest non-zero eigenvalue1 of LF are the elements of C0(G; F)
nearest to being global sections of F while being orthogonal to the space of global sections, and
can be thought of as the smoothest non-constant signal with respect to the structure imposed by F.
Importantly, Equation 1 allows us to quantify the degree of consistency of a choice of 0-cochain
and restriction maps through an implicit representation of the Laplacian. We will exploit this repre-
sentation as we learn a sheaf structure from knowledge graph triplets, embedding entities and their
relations according to their global consistency as measured by the sheaf Laplacian. Before deﬁning
knowledge graph embeddings, we require the notion of a pullback of a sheaf over a graph morphism.
Deﬁnition 2 Given multigraphs G and H, a graph morphism k : G →H sending nodes to nodes
and edges to edges, and FH a sheaf on H, the pullback sheaf k∗FH is a sheaf on G with stalks
k∗FH(σ) = FH(k(σ)) and restriction maps k∗FH
v P e = FH
k(v) P k(e).
This operation may be thought of as replicating the local structure of FH on all the parts of G that
map to the same part of H. Cochains of FH may be pulled back to cochains of k∗FH by a similar
process. If x ∈Ci(H; FH), we deﬁne k∗x ∈Ci(G; FH) by (k∗x)σ = xk(σ). It can be shown
that if x ∈H0(H; FH), then k∗x ∈H0(G; k∗FH), that is, that sections of the initial sheaf induce
sections of any pullback of the sheaf.
3
Knowledge Graph Embedding as Sheaf Learning
A knowledge graph is often deﬁned as a set of entities E together with a set of relations R between
these entities. To make the relationship with cellular sheaves as clear as possible, we make a more
formal deﬁnition.
Deﬁnition 3 Let S be a set of entity types, and R be a set of relation types. Each relation r ∈R
may hold between an entity of type h(r) and an entity of type t(r). This tuple Q = (S, R, h, t) is a
knowledge database schema which itself forms a multigraph.
A knowledge graph is an instantiation of this schema in the form of a set of factual triplets which
respect the typing from Q as given by a (multi)graph morphism:
Deﬁnition 4 Given a knowledge database schema Q = (S, R, h, t), a set E of entities, and a label-
ing function s : E →S which gives the type of each entity, a knowledge graph G instantiating Q
is a graph with vertex set E and edges T ⊆E × R × E whose elements (h, r, t) ∈T must satisfy
1The eigenvectors corresponding to the eigenvalue 0 are the elements of H0(G; F).
3

the type consistency conditions h(r) = s(h) and t(r) = s(t). G carries a natural graph morphism
k : G →Q mapping vertices to vertices and edges to edges: the image of a vertex (entity) e is s(e),
and the image of an edge (relational fact) (h, r, t) is r. The type consistency condition on T ensures
that k preserves the incidence relation of vertices and edges and is therefore a graph morphism.
The schema Q and graph morphism k are often given externally and can be quite simplistic, so
we generally refer to G itself as the knowledge graph without reference to the other type-enforcing
structures. Most popular knowledge graph datasets assume only one entity type (one vertex in Q)
with a collection of relations mapping this type to itself. This typing scheme is often chosen for con-
venience and may deserve further consideration if one wishes to embed hierarchical, type-speciﬁc
representational biases within knowledge graph embeddings [19]. We hint here at an alternative
category-theoretic expression of the information contained in a knowledge graph and its schema.
In this case, the schema Q is a ﬁnitely presented category with objects S and morphisms gener-
ated by R, and the knowledge database is a functor from Q to the category of sets and relations.
The knowledge graph representation of a knowledge database then corresponds to the category of
elements of this knowledge database functor: a category D together with a functor D →Q with
discrete ﬁbers. This construction is connected with recent work on categorical models for relational
databases [12, 28, 33, 34], a relationship that will be explored more fully in future work.
The problem of knowledge graph embedding is, broadly speaking, that of ﬁnding representations xh
for each entity h and representations Rr for each relation type r such that the truth value of the tuple
(h, r, t) (or the existence of an edge e : h →t with re = r) may be recovered from (xh, Rr, xt).
We denote the collection of entity embeddings by E and the collection of relation embeddings by
R. A common inferential goal for a knowledge graph embedding is to predict the truth value of
new triples from their learned representations alone. As such, truth values across relation r are often
determined from the output of some scoring function f : E × R × E →R which takes a vectorized
triplet to a value representing the model’s degree of conﬁdence in its truth.
We can formulate the knowledge graph embedding problem as learning, from an instantiated knowl-
edge graph G, restriction maps and entity embeddings modeled on the schema Q that are as consis-
tent as possible. More formally:
Deﬁnition 5 Given a knowledge database schema Q = (S, R, h, t), a knowledge sheaf F modeled
on Q corresponds to a choice of vertex stalk spaces F(s) for each entity type s ∈S, edge stalk
spaces F(r) for each relation type r ∈R, and linear maps Fh P r : F(h(r)) →F(r) and Ft P r :
F(t(r)) →F(r) for each r ∈R. That is, a knowledge sheaf is simply a cellular sheaf on the
multigraph determined by Q.
A knowledge sheaf F modeled on Q deﬁnes a cellular sheaf on any knowledge graph G with schema
Q through a sheaf pullback.
Deﬁnition 6 Given a graph morphism k which instantiates knowledge graph G from Q, a sheaf
embedding of G is a knowledge sheaf F modeled on Q together with a 0-cochain x ∈C0(G; k∗F),
such that x is a section of k∗F.
We will also give a more concrete interpretation of the pullback sheaf k∗F, which we will denote
FG to emphasize its base space and relationship with the knowledge sheaf F. For an entity v,
FG(v) = F(s(v)), and for an edge e = (h, r, t), FG(e) = F(r). The restriction maps for e are
given by FG
h P e = Fh P re and FG
t P e = Ft P re. A sheaf embedding of a knowledge graph G gives
a cellular sheaf FG on G, but this sheaf satisﬁes a number of additional restrictions. The vertex stalk
spaces and edge stalk spaces are determined by the types of entities and relations, respectively, and
the restriction maps for an edge are determined entirely by the relation described by that edge. This
is a form of parameter sharing and greatly reduces the complexity of the embedding.
In the framework described here, the stalks of the sheaf are vector spaces, and the restriction maps are
linear transformations. This is a simple and analytically tractable framework with deep connections
to linear algebra and algebraic topology. However, this constraint may be limiting in practice, and
the framework may be extended to more general sorts of spaces and restriction maps, representing
more general classes of transformations. The appropriate mathematical language for this extension
is that of category theory; the appendix contains a description of the construction of cellular sheaves
valued in other categories. However, in experiments we focus on the more familiar setting of vector
spaces and linear transformations.
4

3.1
Training
Given a knowledge graph G, we now seek to learn all of the data involved in a sheaf embedding
of G. To learn the knowledge sheaf F giving the relation embeddings and the approximate section
(xv)v∈V ∈C0(G; FG) giving the entity embeddings, we minimize the total pairwise discrepancy
of the embeddings as measured by the Laplacian quadratic form (1). This is equivalent to using the
edgewise scoring function
f Shv(h, r, t) = ∥Fh P rxh −Ft P rxt∥2.
(2)
This scoring function is familiar in the literature. For unconstrained restriction maps Fh P r, Ft P r
of size (d × d) and d-dimensional entity embeddings xh and xt, this scoring function is identical to
that of Structured Embedding [4]. Provided a set of true training triplets T = {(h, r, t)}, we would
then like to minimize the aggregate error over all triplets with the loss
Lq =
X
(h,r,t)∈T
∥Fh P rxh −Ft P rxt∥2 = xT LF Gx
(3)
where x ∈C0(G; FG) is the section approximated by the entity embeddings and LF G is the sheaf
Laplacian deﬁned by the restriction maps for the knowledge graph. The training objective (3) seeks
to learn entity embeddings and relational restriction maps which are as close as possible to consis-
tency.
Deﬁnition 6 does not capture all possible desiderata for a knowledge graph embedding. For instance,
it would be satisﬁed by a trivial embedding where all restriction maps and embedding vectors are
zero. These embeddings give the smallest possible value for the loss Lq, but contain no information
about the entities or their relations. Penalties must be added to ensure that embeddings encode mean-
ingful information. The possibility of trivial embeddings is not unique to the sheaf framework, and it
is standard practice to include negative examples while training knowledge graph embeddings. This
corresponds to adding another sheaf Laplacian term LF neg corresponding to the negative examples,
so that the loss is Lq = xT LF Gx−xT LF negx, which is, structurally, a pairwise loss function [2]. In
practice, we ﬁnd better performance by applying another pairwise loss function, the margin ranking
loss:
Lm =
X
(h,r,t)∈T ,(h′,r,t′)∈T neg
max(0, f Shv(h, r, t) + γ −f Shv(h′, r, t′))
(4)
where T neg denotes the set of negative triplets and γ the margin. If the positive scores are always
at least as large as the scores of the corresponding corrupted triplet, Lm will be a scalar multiple of
Lq.
3.2
Translational Embeddings
As noted above, sheaf embedding as given in Deﬁnition 6 represents a generalization of Structured
Embedding. However, there are many other knowledge graph embedding approaches [5, 9, 24, 39,
41] which are distinct from, and have been shown to outperform, Structured Embedding. Many pop-
ular approaches extend the TransE [5] embedding method. TransE and its derivatives are deﬁned by
the use of a translation-like operation g in the scoring function which depends on a learned relation
representation rr, such that g(xh, rr) ≈xt for a true triplet (h, r, t). TransE uses embeddings
xh, xt, rr ∈Rd and the scoring function
f TransE(h, r, t) = ∥xh + rr −xt∥2
2.
(5)
We may extend Deﬁnition 6 to cover translational models by specifying an additional 1-cochain
r = (r)r∈R ∈C1(G; F). From a sheaf-theoretic perspective, r may be interpreted as the desired
“discrepancy” across each relation type r for any choice of head and tail entities in that δx ≈k∗r.
From the translational embedding perspective, this 1-cochain forms the relation embeddings for the
knowledge graph.
We can now generalize the scoring function given in Equation 2 to the translational class of models:
f ShvT(h, r, t) = ∥Fh P rxh + rr −Ft P rxt∥2.
(6)
When Fh P r = Ft P r, this scoring function is equivalent to TransR [24], and if we further enforce
Fh P r = Ft P r = I, this scoring function is equivalent to Equation 5. See the Appendix for more
information regarding the generalization of other knowledge graph embedding approaches.
5

3.3
Learning Multiple Sections
It is typically desirable to produce knowledge graph embeddings which encode knowledge in a
robust and non-speciﬁc manner so that these embeddings may be useful in downstream knowledge
tasks on unseen data. From the purview of sheaf embeddings, one way this may be achieved is by
learning knowledge sheaves whose space of approximate sections is large. In other words, we would
like xT LF Gx to be small for as many choices of 0-cochain x as possible, not just the x learned
from the training data. Up to this point, we have assumed that we are learning a single 0-cochain
x ∈C0(G; FG) and, in the translational case, 1-cochain r ∈C1(G; FG) that represent the entity
and relation embeddings, respectively. One way to improve the robustness of our sheaf embedding
is to learn multiple sections simultaneously which may be useful in mitigating learning errors due to
initialization, sampling and labeling via ensemble methods [1]. Instead of learning a vector xv for
each entity, we learn a matrix Xv whose columns are unique cochains. We update our loss function
accordingly:
Lqo = tr(XT LF GX −XT LF negX) + α
X
v∈E
∥XT
v Xv −I∥2
F .
(7)
Here, α is a hyperparameter determining the extent to which the learned cochains are penalized for
being collinear. This approach extends to multiple 1-cochains as well. The optimization can be
viewed as a meta-learning procedure which produces a minimally overlapping ensemble of embed-
dings that maximally agree on the data.
3.4
Modeling Knowledge Priors
In addition to integrating and generalizing a number of popular knowledge graph embedding ap-
proaches, this sheaf-theoretic framework provides powerful methods that can constrain knowledge
graph embeddings to better capture the semantics of the underlying knowledge domain. The struc-
ture of the restriction maps F• P r for each relation r provides control for modeling symmetric,
asymmetric, one-to-many, many-to-one, or one-to-one relations by choosing the proper structure for
the restriction maps across each edge type in R. For example, symmetry may be enforced by setting
Fh P r = Ft P r. The choice of edge stalk space F(r) for each relation type r provides ﬂexibility in
the space within which entity embeddings are compared across incident edges. For example, setting
dim F(r) < dim F(s) implies Fh P r and Ft P r act as projections, forcing entities to be compared
across fewer dimensions which may be useful in modeling many-to-many or many-to-one relations.
The transformations these restriction maps encode can also act to regularize the learned embeddings:
forcing F• P r to be orthogonal requires entity embeddings to be comparable as a rotation across r.
Finally, when the schema Q has multiple entity types, the embedding stalk space can vary across
these types, decreasing parameterization for types which can be modeled using few dimensions. See
the Appendix for further discussion.
3.5
Inference with Sheaf Embeddings
Knowledge graph completion involves ﬁnding pairs of entities linked by a given relationship which
is not already encoded in the knowledge graph. Assume we have a knowledge graph G based on a
schema Q, with a sheaf embedding of G. To query the existence of a relation r between two entities
u, v ∈E, simply check the discrepancy of xu and xv over a hypothetical edge e : u →v with
relation re = r by computing ∥FG
u P exu −FG
v P exv∥2 = ∥Fh P rxu −Ft P rxv∥2 = f Shv(u, r, v).
If, given an entity u, we wish to ﬁnd the most likely r-related entity v, we can ﬁnd the entity v
minimizing this discrepancy, namely arg minv∈E ∥Fh P rxu −Fv P rxv∥2. In essence, we ﬁnd the
known entity v whose embedding xv most nearly extends xu as a section of the knowledge sheaf
over a hypothetical edge e : u →v with relation r. The value of this minimum is an upper bound
on the extra embedding cost of adding this edge to G. This approach extends to sheaf embeddings
with non-trivial 1-cochains by using the translational cost term f ShvT(u, r, v).
3.5.1
Multi-Hop Reasoning
It is possible to query the knowledge graph for more complex relations composed from the basic
relations of the graph. For instance, the relations “x is a child of y” and “y is a child of z” com-
pose to “x is a grandchild of z.” Deductions of this sort are often denoted by the term “multi-hop
reasoning” [13, 14, 37]. A natural sheaf-theoretic tool for making composite queries is the notion
6

of harmonic extension. We describe here the construction for non-translational sheaf embeddings
as in Deﬁnition 6, but this extends to embeddings with non-trivial 1-cochains, as detailed in the
Appendix.
If we wish to infer the possible endpoints of a sequence of relations r1; r2; · · · ; rk, beginning at
known entity u0 and ending at some to-be-determined entity uk, we can construct a chain of edges
with these relations, and optimize for their combined discrepancy. We have thus constructed a new
knowledge graph H modeled on Q with vertex set v0, . . . , vk. The knowledge sheaf F also induces
a sheaf FH on H as before. We wish to ﬁnd a matching of the entities of H with entities of G that is
consistent with the sheaf FH and the entity embedding x. That is, we wish to ﬁnd an approximate
section of FH. The optimization problem over this graph H is then
arg min
u1,...,uk∈E
k
X
i=1
∥FH
vi−1 P eixui−1 −FH
vi P eixui∥2.
(8)
For long chains and large databases, this problem can be difﬁcult to solve. Naively, ﬁnding the best
ﬁt for a chain of length k requires evaluating the objective function at |E|k tuples of entities. Other
approaches to this problem try to ﬁnd approximate solutions, e.g. by simply greedily extending
to the best entity at each step or ignoring the interior nodes altogether and constructing some joint
composite relation, thus simplifying to single-hop graph completion.
A related optimization problem has an analytical solution. Rather than optimizing over entities
in the knowledge database for the intervening nodes u1, . . . , uk−1, we optimize directly over the
embedding space, giving the intermediate cost function
V (y) =
k
X
i=1
∥FH
vi−1 P eiyi−1 −FH
vi P eiyi∥2 = yT LF Hy.
(9)
This is a relaxation of the problem (8), as we do not require yi to be the embedding of a known
entity. The problem of ﬁnding the best-ﬁtting entity uk for the composed sequence of relations then
becomes
arg min
uk∈E

min
y∈C0(H;F) V (y) s.t. y0 = xu0, yk = xuk

.
(10)
The inner optimization problem, depending on u0 and uk, is the problem of harmonic extension
of a 0-cochain deﬁned on a boundary subset of vertices B, which here is {v0, vk}. This problem is
convex and quadratic, so the optimal value is unique, but the optimizer may not be. A brief argument
using Lagrange multipliers shows that an equivalent problem is to ﬁnd a 0-cochain y ∈C0(H; FH)
such that y0 = xu0, yk = xuk, and LF Hy = 0 on nodes not in B. The values of y on U, the
complement of B, are given by the formula yU = −L[U, U]−1L[U, B]yB, where yB is determined
by the embeddings xu0 and xuk, and we drop the subscript on LF H. Then the minimum value of
the inner optimization problem in (10) is
V (y∗) = yT
B
 L[B, B] −L[B, U]L[U, U]−1L[U, B]

yB.
The matrix in this form is the Schur complement L/L[U, U] of L[U, U] in L. When L[U, U] is
not invertible, we may use its Moore–Penrose pseudoinverse L[U, U]†. In this case, the minimizer
of U is not unique, and using y∗
U = −LF[U, U]†LF[U, B]yB selects the 0-cochain of minimum
norm solving the extension problem. When H is a linear chain and B consists of the two endpoints,
the resulting matrix L/L[U, U] may be viewed as the Laplacian of another sheaf on the graph with
vertex set B and a single edge. This is known as the Kron reduction of the sheaf to B, and the
resulting pair of restriction maps deﬁnes an inferred embedding for the composite relation. A more
thorough discussion of harmonic extension and Kron reduction is found in [17, sec. 4] and [15,
ch. 3].
3.5.2
Complex Composite Relations
We need not limit ourselves to composing relations in linear chains: harmonic extension adapts
effortlessly to more complex networks of relations. Let H be any knowledge graph with schema
Q. The learned knowledge sheaf extends to H as before, and its sections over H correspond to
7

collections of entity embeddings jointly satisfying the relations. We select a boundary set of vertices
B with complement U and compute the Schur complement LF H/LF H[U, U]. The quadratic form
V (yB) = yT
B(LF H/LF H[U, U])yB ﬁnds the minimal value of a problem analogous to the inner
problem in (10), constraining the values of y on B to equal yB. We can then ﬁx the values of yB
on some source subset S of vertices to be equal to the embeddings of some given entities {us}s∈S,
and test the embeddings xt for other entities t to ﬁnd the entities that minimize V (yB) subject
to yS = xS. See the Appendix for a simple example. For further insight regarding harmonic
extension as a method for solving complex queries, it is helpful to note the relationship between
the Schur complement and marginalization when entity embeddings are distributed as multivariate
Gaussians [40]. This relationship is detailed in the Appendix. Finally, recall that sheaf embedding
generalizes Structured Embedding (2), and with the addition of non-trivial 1-cochains (6) represents
a generalization of TransR. Harmonic extension provides a way to apply any of these models to
multi-hop and complex composite queries in a theoretically justiﬁed manner which, to the authors’
knowledge, is a ﬁrst for models like Structured Embedding or TransR that are not purely translational
or bilinear [14].
4
Experiments
To validate the approach to answering complex queries detailed in the previous section and to high-
light the ﬂexibility of sheaf embedding, we compare the performance of a number of sheaf embed-
ding models on two benchmark datasets: NELL-995 [42] and FB15k-237 [36], both freely available
online. These experiments are not intended to achieve state-of-the-art performance. Rather, our aim
is to investigate the effect of model regularization choices detailed in Section 3 and to showcase the
extensibility of this framework to the evaluation of complex queries. We implement these models
as extensions of the model class within the open-source (MIT) Pykeen [3], allowing us to train and
evaluate these models in a manner that is both reproducible and comparable to other embedding
techniques. Models are trained and tested on a Nvidia GeForce GTX 1080 GPU with 8GB RAM
with the most complex model variations taking approximately 5 hours to train.
We train each model according to the graph completion task, learning embeddings by minimizing
the loss associated to scoring both true triplets and corrupted triplets (Equations 4 and 7). At test
time, we evaluate the 1p, 2p, 3p, 2i, 3i, ip, and pi complex query structures as detailed in Ren
and Leskovec [29] by scoring entities based on their solution to the associated optimization prob-
lem (10). These query structures correspond to multi-hop path query structures (p*), intersectional
queries (i*) and a combination of the two (ip and pi). We train two types of models. The Shv
model implements scoring function (2) which, depending on the restriction map structure, is a gen-
eralization of Structured Embedding [4]. To observe the effects of compressive comparison across
relations, we vary the edge stalk space dim F(r) and assume all edge types share this space for this
model. The ShvT model implements scoring function (6) and sets Fh P r = Ft P r = I which
is equivalent to TransE [5]. We use the Euclidean norm for both scoring functions. For models
trained on FB15k-237, we set the embedding dimension dim F(s) = 64 for all entities. For models
trained on NELL995, we set dim F(s) = 32. We assume a single entity type for the schema of
both datasets. We also train versions of these models to learn multiple sections simultaneously, as
detailed in Section 3.3. For these models, we vary the α parameter across [0, 0.01, 0.1] and vary
the number of sections to observe the effect of this regularization method. We set the margin γ = 1
and train each model for 250 epochs, repeating this training three times for three different random
seeds. For each complex query type, we compute two canonical performance measures: the mean
reciprocal ranking (MRR) and the hits at 10 (H@10) of all entities in the test set of the knowledge
graph.
Table 4 depicts the results of these experiments for FB15k-237. All models appear to overﬁt towards
the 1p (knowledge graph completion) task, as performance across the harder, non-intersectional
structures is low. However, this is expected considering models were trained using only queries
of the 1p structure. For the Shv model, uniformly reducing the dimension of the relation space
does not appear to increase performance, as models with dim F(s) > dim F(r) underperform
those with square restriction maps. Interestingly, regularization with multiple sections generally
increases performance, but enforcing orthogonality within the sections diminishes these beneﬁts.
Notably, the solutions to non-intersectional queries derived via harmonic extension for the ShvT
model outperform more naive methods for answering such queries with translational models [14].
8

The results for these prior approaches, the NELL995 results, along with the standard deviations
across random trials for both datasets are listed in the Appendix.
model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
16
1
0
8.62
0.08
0.09
0.57
0.38
0.05
0.05
32
1
0
9.44
0.08
0.08
0.92
0.58
0.06
0.06
64
1
0
10.16
0.08
0.08
1.16
0.69
0.06
0.06
16
0
20.28
0.07
0.08
15.95
10.04
0.07
0.06
0.01
16.7
0.07
0.07
10.18
6.86
0.07
0.07
0.1
6.13
0.07
0.07
3.05
2.95
0.07
0.07
32
0
22.89
0.07
0.08
17.05
9.94
0.07
0.07
0.01
9.18
0.07
0.07
3.75
2.93
0.07
0.07
0.1
5.76
0.07
0.07
2.64
2.53
0.07
0.07
64
0
24.7
0.07
0.08
18.74
11.06
0.07
0.07
0.01
0.49
0.07
0.08
0.43
0.46
0.07
0.07
ShvT
64
1
0
10.16
1.88
1.48
1.52
1.44
3.18
2.38
16
0
10.9
4.02
3.77
1.34
1.06
4.15
3.51
0.01
3.21
0.84
0.94
0.62
0.68
0.78
0.66
0.1
0.59
0.1
0.08
0.07
0.08
0.21
0.1
32
0
10.93
3.91
3.86
1.3
1.09
4.52
3.36
0.01
3.93
1.21
1.35
0.85
0.86
1.65
1.67
0.1
1.21
0.21
0.21
0.26
0.33
0.38
0.36
64
0
10.9
3.98
3.93
1.08
1
5.17
3.28
0.01
4.16
1.82
1.95
1.03
1
3.01
2.38
0.1
1.46
0.42
0.44
0.56
0.84
0.81
0.96
Table 1: MRR % results for complex query structures on FB15k-237.
5
Discussion
The sheaf-theoretic generalization of knowledge graph embedding presented in this paper provides
an encompassing perspective for integrating a number of prior embedding approaches within a co-
hesive theoretical framework. In addition to facilitating the comparison of these prior embedding
approaches within a common language, we have exploited this generalization to expand the types of
queries that can be answered through harmonic extension and the incorporation of structured prior
knowledge. By formalizing the relationship between the typing introduced by knowledge graph
schemas and the instantiation of these schemas within knowledge graphs, this sheaf-theoretic frame-
work allows for the natural modeling of typed, hierarchical knowledge bases which provides further
control over the representation of priors placed on the embeddings. By viewing knowledge graph
embedding as sheaf learning, we have immediate access to tools for reasoning about the local and
global consistency of embeddings through the sheaf Laplacian. Finally, and perhaps most impor-
tantly, this generalized perspective of knowledge graph embedding opens the door to an array of
extensions to knowledge graph embedding like introducing uncertainty through a statistical perspec-
tive, enforcing hierarchical typing within embeddings, or embedding knowledge graphs within more
exotic categories.
5.1
Societal Impacts
Many modern knowledge graphs are constructed from data obtained from the internet and other
public domains, and the knowledge contained in these sources is often curated by a large number
of anonymous contributors. Because of this, knowledge graphs are likely to inherit the (often un-
known) biases of the social structures which generate the underlying knowledge, either explicitly in
the knowledge itself, or implicitly in the distribution of factual statements contained in the knowl-
edge graph. These biases in knowledge graphs can propagate to the embeddings which are used
to represent the underlying information. For example, Fisher et al. [11] have found evidence of
harmful social biases related to professions being encoded in knowledge graph embeddings with
respect to gender, religion, ethnicity and nationality. Despite their lack of speciﬁc personal infor-
mation and explicit content, the benchmark datasets we have incorporated within this work are un-
likely to be immune from such societal-level biases.Although preliminary work exists which aims
to negate biases within knowledge graph embeddings [10], their amelioration remains a challenging
9

open problem. While the bias-amelioration problem was not the focus of this paper, the general-
ized theoretical framework for knowledge graph embedding presented provides powerful tools for
incorporating constraints and ethical requirements that may better constrain and control the learned
representations of knowledge graphs in order to reduce the prevalence of biases within the model’s
representations, and to better align these representations with a more just ethical standard.
References
[1] B. Adlam and J. Pennington.
Understanding double descent requires a ﬁne-grained bias-
variance decomposition, 2020.
[2] M. Ali, M. Berrendorf, C. T. Hoyt, L. Vermue, M. Galkin, S. Sharifzadeh, A. Fischer, V. Tresp,
and J. Lehmann. Bringing light into the dark: A large-scale evaluation of knowledge graph
embedding models under a uniﬁed framework. arXiv preprint arXiv:2006.13365, 2020.
[3] M. Ali, M. Berrendorf, C. T. Hoyt, L. Vermue, S. Sharifzadeh, V. Tresp, and J. Lehmann. Py-
keen 1.0: A python library for training and evaluating knowledge graph embeddings. Journal
of Machine Learning Research, 22(82):1–6, 2021.
[4] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowl-
edge bases. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2011.
[5] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embed-
dings for modeling multi-relational data. In Advances in neural information processing sys-
tems, pages 2787–2795, 2013.
[6] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic matching energy function for
learning with multi-relational data. Machine Learning, 94(2):233–259, 2014.
[7] X. Chen, S. Jia, and Y. Xiang. A review: Knowledge reasoning over knowledge graph. Expert
Systems with Applications, 141:112948, 2020.
[8] J. Curry. Sheaves, Cosheaves, and Applications. PhD thesis, University of Pennsylvania, 2014.
[9] T. Ebisu and R. Ichise. TorusE: Knowledge graph embedding on a Lie group. arXiv preprint
arXiv:1711.05435, 2017.
[10] J. Fisher, A. Mittal, D. Palfrey, and C. Christodoulopoulos. Debiasing knowledge graph em-
beddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 7332–7345, 2020.
[11] J. Fisher, D. Palfrey, C. Christodoulopoulos, and A. Mittal. Measuring social bias in knowledge
graph embeddings. In AKBC Workshop on Bias in Automatic Knowledge Graph Construction,
2020.
[12] B. Fong and D. I. Spivak. Seven sketches in compositionality: An invitation to applied category
theory. arXiv:1803.05316 [math], 2018. URL http://arxiv.org/abs/1803.05316.
[13] M. Gardner, P. Talukdar, J. Krishnamurthy, and T. Mitchell. Incorporating vector space simi-
larity in random walk inference over knowledge bases. In Proceedings of the 2014 conference
on empirical methods in natural language processing (EMNLP), pages 397–406, 2014.
[14] K. Guu, J. Miller, and P. Liang. Traversing knowledge graphs in vector space. In Proceedings
of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 318–
327, 2015.
[15] J. Hansen. Laplacians of Cellular Sheaves: Theory and Applications. PhD thesis, University
of Pennsylvania, 2020.
[16] J. Hansen and R. Ghrist. Learning sheaf Laplacians from smooth signals. In Proceedings of
ICASSP, 2019.
[17] J. Hansen and R. Ghrist. Toward a spectral theory of cellular sheaves. Journal of Applied and
Computational Topology, 3(4):315–358, Dec. 2019. ISSN 2367-1734.
10

[18] K. Hayashi and M. Shimbo. On the equivalence of holographic and complex embeddings for
link prediction. In Proceedings of the 55th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages 554–559, 2017.
[19] N. Jain, J.-C. Kalo, W.-T. Balke, and R. Krestel. Do embeddings actually capture knowledge
graph semantics? In Eighteenth Extended Semantic Web Conference - Research Track, 2021.
URL https://openreview.net/forum?id=vsxYOZoPvne.
[20] R. Jenatton, N. L. Roux, A. Bordes, and G. R. Obozinski. A latent factor model for highly
multi-relational data. In Advances in neural information processing systems, pages 3167–3175,
2012.
[21] G. Ji, K. Liu, S. He, and J. Zhao. Knowledge graph completion with adaptive sparse transfer
matrix. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.
[22] S. Ji, S. Pan, E. Cambria, P. Marttinen, and P. S. Yu. A survey on knowledge graphs: Repre-
sentation, acquisition and applications. arXiv preprint arXiv:2002.00388, 2020.
[23] H. Lin, Y. Liu, W. Wang, Y. Yue, and Z. Lin. Learning entity and relation embeddings for
knowledge resolution. Procedia Computer Science, 108:345–354, 2017.
[24] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning entity and relation embeddings for
knowledge graph completion. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 29, 2015.
[25] D. M. Malioutov, J. K. Johnson, and A. S. Willsky. Walk-sums and belief propagation in
gaussian graphical models. The Journal of Machine Learning Research, 7:2031–2064, 2006.
[26] M. Nickel, V. Tresp, and H.-P. Kriegel. A three-way model for collective learning on multi-
relational data. In Icml, volume 11, pages 809–816, 2011.
[27] M. Nickel, L. Rosasco, and T. Poggio. Holographic embeddings of knowledge graphs. In
Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, pages 1955–1961,
2016.
[28] E. Patterson. Knowledge Representation in Bicategories of Relations. arXiv:1706.00526 [cs,
math], Nov. 2017.
[29] H. Ren and J. Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge graphs.
Advances in Neural Information Processing Systems, 33, 2020.
[30] H. Ren, W. Hu, and J. Leskovec.
Query2box: Reasoning over knowledge graphs in vec-
tor space using box embeddings. In International Conference on Learning Representations
(ICLR), 2020.
[31] E. Riehl. Category theory in context. Aurora: Dover Modern Math Originals. Dover, 2017.
[32] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning with neural tensor networks for
knowledge base completion. In Advances in neural information processing systems, pages
926–934, 2013.
[33] D. I. Spivak. Functorial data migration. Information and Computation, 217:31–51, Aug. 2012.
ISSN 08905401. doi: 10.1016/j.ic.2012.05.001.
[34] D. I. Spivak. Database queries and constraints via lifting problems. Mathematical Structures
in Computer Science, 24(6):e240602, Dec. 2014. ISSN 0960-1295, 1469-8072. doi: 10.1017/
S0960129513000479.
[35] Z. Sun, Z.-H. Deng, J.-Y. Nie, and J. Tang. RotatE: Knowledge graph embedding by relational
rotation in complex space. arXiv preprint arXiv:1902.10197, 2019.
[36] K. Toutanova, D. Chen, P. Pantel, H. Poon, P. Choudhury, and M. Gamon. Representing text
for joint embedding of text and knowledge bases. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing, pages 1499–1509, Lisbon, Portugal,
Sept. 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1174. URL
https://www.aclweb.org/anthology/D15-1174.
11

[37] K. Toutanova, X. V. Lin, W.-t. Yih, H. Poon, and C. Quirk. Compositional learning of embed-
dings for relation paths in knowledge base and text. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1434–1444,
2016.
[38] T. Trouillon and M. Nickel. Complex and holographic embeddings of knowledge graphs: a
comparison. arXiv preprint arXiv:1707.01475, 2017.
[39] T. Trouillon, J. Welbl, S. Riedel, É. Gaussier, and G. Bouchard. Complex embeddings for
simple link prediction. In International Conference on Machine Learning (ICML), 2016.
[40] R. Von Mises. Mathematical theory of probability and statistics. Academic Press, 2014.
[41] Z. Wang, J. Zhang, J. Feng, and Z. Chen. Knowledge graph embedding by translating on
hyperplanes. In Aaai, pages 1112–1119. Citeseer, 2014.
[42] W. Xiong, T. Hoang, and W. Y. Wang. Deeppath: A reinforcement learning method for knowl-
edge graph reasoning. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, pages 564–573, 2017.
[43] T. Yang, L. Sha, and P. Hong. Nage: Non-abelian group embedding for knowledge graphs. In
Proceedings of the 29th ACM International Conference on Information & Knowledge Manage-
ment, pages 1735–1742, 2020.
[44] S. Zhang, Y. Tay, L. Yao, and Q. Liu. Quaternion knowledge graph embeddings. In Advances
in Neural Information Processing Systems, pages 2735–2745, 2019.
12

Other Embedding Models
Many knowledge graph embedding approaches may be decomposed into a combination of multi-
plicative and additive interactions of entity and relation vectors. We show in this section that these
additive components correspond to sheaf-theoretic coboundary operators across incident edges and
discuss how different embedding approaches alter this coboundary operator. The models discussed
in this section are a non-exhaustive subset of the total number of models in existence, but we make
an attempt to cover most of the popular choices.
Structured Embedding. One of the earliest approaches to embedding knowledge graphs is Struc-
tured Embedding (SE) [4]. Structured Embedding models entities xh ∈Rd as d-dimensional
vectors and relations as a pair of (d × d)-dimensional matrices (Rh
r , Rt
r). The scoring function
between entities is then determined as f SE(h, r, t) = ∥Rh
rxh −Rt
rxt∥. Setting Rh
r = Fh P r
and Rt
r = Ft P r, we see the scoring function computes precisely boundary function of the sheaf
f SE(h, r, t) = ∥(∂E)r∥= ∥Fh P rxh −Ft P rxt∥. In other words, SE attempts to learn entity
and relation embeddings that minimize the local discrepancy between adjacent entities along each
relation. Therefore,
X
(h,r,t)
f SE(h, r, t)2 = xT Lx
where L is the sheaf Laplacian formed from the matrices R•
r, and x = (x)v∈V ∈C0(G; FG).
Unstructured Model. The unstructured model [6], often used as a baseline model, is equivalent to
Structured Embedding when Rh
r = Rt
r = I, and therefore also ﬁts within our modeling framework.
TransX. A number of related embedding methods have been developed which seek to model rela-
tions as translations in a vector space which we refer to as the TransX class of embedding methods.
These models seek to ﬁnd embeddings of triples (xh, rr, xt) such that g(xh, rr) ≈xt where g is a
simple function representing a translation-like operation within the chosen embedding space.
As discussed in the main text, TransE [5] is an early translation-based model which aims to ﬁnd
embeddings that result in
f TransE(h, r, t) = ∥xh + rr −xt∥2
(11)
being small when (h, r, t) is true and large otherwise. Here, both the entity and relation embeddings
are vectors in Rd.
We can formulate this kind of translational scoring within our sheaf-theoretic framework by viewing
the relation vector as a rr as a 1-cochain across edge r. More formally, we wish to learn some
1-cochain r ∈C1(G; FG), representing a choice of vectors over each relation type in the knowl-
edge graph, such that the discrepancy of entity embeddings xh and xt across each relation r is
approximately equal to rr:
f ShvT(h, r, t) = ∥Fh P rxh + rr −Ft P rrxt∥2.
(12)
This is equivalent in form to TransR [24] when both restriction maps are equivalent at the head and
tail of r. Taking Fh P r = Ft P r = I, our scoring function simpliﬁes to exactly Equation 11 and is
thus equivalent to TransE embedding.
TorusE and RotatE. More recently, RotatE [35] was introduced as a hybrid between ComplEx and
the TransX approach. RotatE computes embeddings xh, xt, rr ∈Cd and scores triplets translation-
ally:
f RotatE(xh, rr, xt) = ∥xh ◦rr −xt∥
(13)
where ◦is the Hadamard product. We can encode this scoring function through restriction maps as
follows. Taking our edge and node stalk spaces to be in Cd, setting Fh P r to be the diagonal matrix
with rr on the diagonal2, and setting Ft P r = I, we obtain an equivalent score for f RotatE. The
TorusE model [9] is a special case of RotatE where the modulus of the embeddings are ﬁxed.
Finally, Yang et al. [43] propose a number of embedding methods which ﬁt within this sheaf embed-
ding framework as ﬁxed restriction maps which introduce both inductive priors on the interactions
between, and therefore the embeddings of, entities.
2Equivalently, we can represent rr as a diagonal matrix with eiφr on the diagonal where φr is a vector of
phases ranging from 0 to 2π.
13

Non-conforming Models
The sheaf-theoretic framework presented in the main text does not generalize all knowledge graph
embedding approaches that have been proposed in the literature. In general, any model with a bilin-
ear interaction between entity embeddings and relations does not currently ﬁt within our proposed
framework. Models of this form include the Neural Tensor Network [32], ComplEx (equivalently,
HolE) [18, 27, 39], Rescal/Bilinear [20, 26], and QuatE [44]. TransH [41] also does not conform to
our modeling framework, but do conform once entities are in their post-projection form. Investigat-
ing the extent to which these bilinear models may be incorporated into the sheaf embedding form is
an interesting avenue for future work.
Recently, a distinct lineage of knowledge graph embedding models have been proposed which repre-
sent a departure from translational/bilinear classiﬁcation given above. Targeting the task of complex
logical query answering, models like BetaE [29] and Query2Box [30] look to embed the queries
themselves within some representational space. It is currently unclear to the authors whether this
family of models is amenable to a sheaf-theoretic representation. Casting these models within our
sheaf embedding framework may require introducing sheaves valued in other categories than vector
space stalks with linear restriction maps. The next section details this generalization.
Sheaves valued in other categories
While the algebraic properties of the theory of cellular sheaves rely in part on the linear structure of
the vector spaces that serve as stalks, the theory may be developed in a more general setting. For
this we adopt the language of category theory (see [12, 31]). Let G be a graph, which we view as
a category with one object for each vertex and edge, and a unique morphism v P e : v →e for
each incident vertex-edge pair. To treat orientations properly, we also assume that morphisms are
tagged with an orientation; that is, the morphism v P e also records whether the pair is to be viewed
as deﬁning the head or tail of the edge, which we will write as v Ph e or v Pt e. This is particularly
important for graphs with self-loops, where for a given vertex-edge pair (v, e) there may be two
distinct morphisms v Ph e and v Pt e.
A cellular sheaf on G valued in the data category C is a functor F : G →C. We assume C is
complete (i.e. has all small limits), and deﬁne the global sections of F to be the limit lim F, an
object of C. The stalks of F are the values the functor F takes on objects of G, and the restriction
maps are the values of F on the morphisms of G. Thus, for a vertex-edge pair v P e, we have a
restriction map Fv P e : F(v) →F(e), which is a morphism in C.
The pullback of C-valued sheaves over a graph morphism k is well-deﬁned for graph morphisms
that send edges to edges and vertices to vertices. It is constructed in the same way as for sheaves
of vector spaces. For each vertex v, k∗F(v) = F(k(v)), and for each edge e, k∗F(e) = F(k(e)).
Then k∗Fv P e = Fk(v) P k(e).
Since C is complete, we can deﬁne the object C0(G; F) = Q
v F(v) in C, the product of all vertex
stalks of F. The global sections of F naturally form a subobject of C0(G; F); when C is a concrete
category, we can think of sections of F as elements (xv)v∈V (G) ∈C0(G; F) such that for every
edge e = u →v, Fu Ph exu = Fv Pt exv.
We can similarly deﬁne C1(G; F) = Q
e∈E(G) F(e) as an object in C. If C is the category of groups
(or a subcategory thereof), we can deﬁne a coboundary map δ : C0(G; F) →C1(G; F) by letting
(δx)e = (Fu Ph exv)−1(Fv Pt exv). When C = Vect, the category of vector spaces, this deﬁnition
recovers the deﬁnition of the coboundary given in the paper.
We actually require slightly less structure to deﬁne a coboundary map; it is sufﬁcient for C to be a
category of group torsors. These are in essence spaces in which multiplication is not possible, but
division is. For a group S, an S-torsor A is a set equipped with an action · : S × A →A and
a division map D : A × A →S, such that D(a, b) · b = a. We can formally think of D(a, b)
as being ab−1, in which case the formula is the natural ab−1 · b = a. One way to think of an S-
torsor A is as the same underlying set as S, but with the identity of the group forgotten. We can
still translate the elements of A in directions given by group elements, but there is no distinguished
origin. The reason for this extension is to allow a broader class of maps between embedding spaces.
A morphism of torsors is not required to preserve the origin, but does preserve the division operation.
14

To compute the coboundary operator of a torsor-valued sheaf, which we think of as a function
between the underlying sets of C0(G; F) and C1(G; F), we let (δx)e = D(Fu Ph exu, Fv Pt exv)
for e = u →v. The coboundary is then valued in a product of groups: if the stalk F(e) is an Se-
torsor, the coboundary δx is in Q
e Se. When these groups are given a metric, we can then compute
a cost function for a 0-cochain x by letting UF(x) = P
e∈E(G) d((δx)e, 1Se), where 1Se is the
identity of the group Se. The cost function UF vanishes exactly on those 0-cochains x which are
sections of F.
Every vector space is an abelian group under addition, and the category of torsors over vector spaces
is equivalent to the category Aﬀof vector spaces and afﬁne maps. In this category, a morphism
f : V →W is given by a formula of the form f(v) = T (v) + b, where T is a linear transformation
V →W and b ∈W. The coboundary map of a sheaf valued in Aﬀis given on edges by (δx)e =
Fv P exv + bv P e −Fu P exu −bu P e. This is equivalent to the coboundary map of a linear sheaf
F with an edgewise afﬁne correction term. Thus, for the purposes of knowledge graph embedding,
working with sheaves valued in Aﬀis equivalent to using a sheaf valued in Vect and learning x and
b such that δx ≈b rather than δx ≈0. Passing to sheaves valued in Aﬀthus adds a translational
component to the embedding model.
As a result, we can think of the relation between embeddings with group-valued sheaves and embed-
dings with torsor-valued sheaves as analogous the relationship between pure sheaf embeddings and
sheaf embeddings with a translational component.
If we abandon the prospect of a translational component to the embedding, we can further relax our
requirements on the data category C. If C is a category of metric spaces, we can construct a measure
of the discrepancy of a 0-cochain x ∈C0(G; F) by
UF(x) =
X
e∈E(G)
dF(e)(Fu P e(xu), Fv P e(xv)).
Optimizing this function with respect to the restriction maps Fv P e and the 0-cochain x produces
a generalized knowledge graph embedding. In this setting, it is most reasonable to take C to be a
category of Euclidean spaces and smooth (or at least almost-everywhere differentiable) maps, so
that we can apply automatic differentiation and gradient descent.
The common thread in all these sheaf-theoretic constructions is the notion of comparison. To evalu-
ate the plausibility of a relation holding between two entities, the entity embeddings are both trans-
formed into a comparison space, and some measure of discrepancy between these transformed em-
beddings is calculated, giving a scoring function. Many commonly used knowledge graph scoring
functions ﬁt neatly into this framework, but some do not.
Modeling knowledge priors
The choice of edge stalk space F(r) in coordination with the associated head and tail restriction
maps Fh P r, Ft P r provides a number of ways to specify biases in the modeling of particular
relations and the entity interactions across them. Lower-dimensional F(r) (relative to F(s)) forces
the head and tail entities across the relation to be comparable in fewer dimensions and making
consistency easier to achieve, whereas higher-dimensional edge stalks imply the difference between
entity embeddings may be complex in its speciﬁcation.
One way to systematically choose edge stalk dimensionality is to learn these dimensions during
training. We can infer whether we need to increase dim F(r) by decomposing the loss Lq along
each relation. Denoting the subset of triplets across a particular relation r as Tr ⊂T , we can
compute the average discrepancy across edges with that relation as
Lq,r =
X
(h,r,t)∈Tr
∥Fh P rxh −Ft P rxt∥2.
Observing the distribution of discrepancies across all edges, we can alter F(r) accordingly by, for
example, decreasing dim F(r) if Lq,r is in the top quartile in discrepancy across edges. Changing
the edge stalk dimensionality of some relation to d amounts to re-structuring the head and tail re-
striction maps Fh P r, Ft P r such that the dimension of their codomains is d. There are various
ways to accomplish this restructuring, but the simplest is to increase or decrease the number of rows
in Fh P r and Ft P r accordingly, reinitializing extra dimensions as needed.
15

We can also regularize the restriction maps themselves, providing another place to encode represen-
tational biases in the embeddings. If we have some prior knowledge about the relation types within
the knowledge network, we can encode this prior knowledge as constraints on the restriction maps.
A simple example of this is when r is symmetric or anti-symmetric. In the symmetric case, we can
tie the head and tail restriction maps so that Fh P r = Ft P r, or if r is anti-symmetric, we can
require Fh P r = −Ft P r. Similarly, we may expect many-to-one or many-to-many relationships
to require representational compression of entity vectors onto lower-dimensional edge stalks so that
across such a many-to-many relation r, F h P r and Ft P r act as projections. If we allow vertex
stalks to vary in dimension, we can approach the encoding of many-to-one relations like hypernymy
as an approximate canonical injection along one restriction map and projection along the other. We
can also regularize the restriction maps by enforcing their transformation to be of a particular struc-
ture. For example, we might enforce the restriction maps to be drawn from O(d), restricting relations
to act as rotations on entity embeddings.
The ability to represent entities with vectors of varying dimensionality represents a departure from
traditional KG embedding approaches which assume a ﬁxed representation dimension across entities.
This choice of entity embedding dimension allows for better control of representational priors and
provides additional regularization and interpretability for the embeddings. Leveraging this freedom
requires a knowledge database schema with a nontrivial set of entity types. This information may
be included in some datasets, or may be extracted (particularly in the case of small knowledge
graphs) from semantic and graph-theoretic properties. As a simple example, one of the relata of
the relation “has gender” must always be a gender, while the other relatum is never a gender, which
induces a simple partition of the entities into two types, and with the freedom to choose embedding
spaces for entities by type, we can then craft a suitable embedding space for the entities which may
appear at the tail of this relation independent of the other entity type spaces. The sheaf embedding
approach provides signiﬁcant ﬂexibility in modeling a diverse range of knowledge priors, whether
it be through constraints on restriction maps and variety in the stalk spaces by edges, or through the
choice of the embedding category itself, as discussed in the previous section.
Harmonic extension for translational embeddings
The problem of ﬁnding a harmonic extension in the afﬁne or translational setting may be formulated
as follows. Let H be a graph, F a sheaf on H, and B a subset of vertices of H with complement
U. We further assume that the translations are given by a 1-cochain b ∈C1(H; F), and we have a
known boundary condition xB deﬁned on vertices in B. Harmonic extension is then the following
optimization problem:
min
y∈C0(H;F) ∥δy −b∥2
s.t. yB = xB.
Expanding the objective gives an expression in terms of the Laplacian:
min
y∈C0(H;F) yT Ly −2bT δy + bT b
s.t. yB = xB.
The Lagrange multiplier conditions for optimality are
L[U, U]yU + L[U, B]yB = (δT b)U
L[B, U]yU + L[B, B]yB = λ
yB = xB.
Since λ is free, these equations are readily solved for yU:
yU = L[U, U]−1((δT b)U −L[U, B]xB)
= −L[U, U]−1L[U, B]xB + L[U, U]−1(δT b)U
= yF
U + L[U, U]−1(δT b)U,
where yF
U is the harmonic extension of xB for the sheaf F without the afﬁne term b. We now wish
to compute the optimal value; this is
∥δy −b∥2 = yT Ly −2bT Ly + bT b.
16

u0
u1
u2
g2 = male
g1 = female
U
T
S
Figure 1: The template knowledge graph for ﬁnding the maternal grandfather u2 of entity u0. The
interior U, source set S, and target set T are labeled.
We write y = yF + yb, where yF is the standard harmonic extension of xB and yb =
L[U, U]−1(δT b)U is the afﬁne correction computed above (extended to v ∈B by zero). Then
the optimal value is
yT
FLyF + 2yT
FLyb + yT
b Lyb −2bT δyF −2bT δxb + bT b.
After substituting known values of yF and yb in terms of xB and dropping terms that do not depend
on xB, we have
yT
FLyF −2bT δyF.
This means that in order to calculate the afﬁne harmonic extension cost, it sufﬁces to compute
the standard linear harmonic extension. The ﬁrst term can be computed from xB using the Schur
complement L/L[U, U], while the second term is equal to 2bT  δ|UL[U, U]−1L[U, B] + δ|B

xB.
This term is linear in xB and hence is easily computed.
Note that when b = 0 this reduces to the standard harmonic extension problem, and hence gives a
proof of the Schur complement formula given in the main text.
Example of complex query estimation
Consider the problem of ﬁnding the maternal grandfather of a person in a knowledge database, from
constituent relations “is a child of” and “has gender.” That is, u0 is the person whose maternal
grandfather we wish to ﬁnd, and we seek entities u1 and u2 satisfying the following relations: u0 is
a child of u1; u1 is a child of u2; u1 has gender female; u2 has gender male.
There are ﬁve entities in this knowledge graph: the known source vertex u0 of the desired relation,
the unknowns u1 and u2, and the entities female and male. The boundary set B consists of all
vertices but u1, and the source subset is S = {u0, female, male}, while the target subset is simply
T = {u2}, as shown in Figure 1. To ﬁnd the maternal grandfather, we construct the sheaf on the
relational graph H, ﬁnd its Laplacian, and compute the Schur complement LF H/LF H[U, U]. Then
we ﬁx yu0 = xu0, yg1 = xfemale and yg1 = xmale to be the known embeddings of these entities,
and search the entities for the entity u2 whose embedding xu2 gives the smallest value of Q(yB)
when yu2 = xu2. Note that by changing the values of y on the input set S, the computed Schur
complement can also be used to ﬁnd grandparents of any type for any initial entity u0. We can thus
think of the reduced matrix as describing a polyadic relation R(u0, u2, g1, g2) which holds when u0
is the child of someone of gender g1 who is the child of u2, who has gender g2.
Harmonic extension as marginalization
Assume entity embeddings of a knowledge graph G are distributed as 0-mean multivariate normal:
p(xv) =
p
(2π)k det Σ
−1 exp −1
2(xT
v Σ−1xv). For a set of boundary vertices B and their comple-
ment U, their collection of embeddings xH = (xB; xU) is also multivariate normal with zero mean
17

and covariance ΣH a block matrix with ΣB and ΣU as diagonal blocks and the covariance ΣBU
ﬁlling off-diagonal blocks. The conditional covariance of the boundary embeddings xB given xU
is the Schur compelement of ΣU in ΣH:
E(xB | xU) = E(xB) + ΣBUΣ−1
U (xU −E(xU)) = ΣBUΣ−1
U xU
Cov(xB | xU) = ΣB −ΣBUΣ−1
U ΣT
BU.
In this form, we see that the Laplacian of this knowledge sheaf LF G corresponds to the inverse
covariance matrix:
Cov(xB | xU)−1 = LF G[B, B] −LF G[B, U]LF G[U, U]−1LF G[U, B].
This relationship between harmonic extension and marginalization under Gaussian priors implies a
number of interesting directions for future work. For example, it can be shown that the probability of
observing any choice of embedding decomposes as a product of node and edge potential functions
which are parameterized by LF G which provides a statistical interpretation of observing an entity
embedding within a knowledge graph as inversely proportional to the discrepancy it introduces with
respect to its neighbors [25]. This statistical interpretation is reminiscent of problems like covari-
ance selection or graphic lasso, and the relationship between sheaf embedding, graphical likelihood
estimation, and belief propagation deserves further investigation.
Additional experimental details
For simplicity, we test on the “easy” test set for each complex query structure which consists of
query structures composed of individual triplets that have been seen at least once in the training set.
Because each complex query in the test set may have a number of feasible answers, we compute
these measures on the ﬁltered dataset which corresponds to reducing the ranking of entities by the
number of acceptable answers for each query.
We compute the mean reciprocal rank (MRR), hits at 1 (H@1), and hits at 10 (H@10) from the
rankings, according to their assigned score per test query, across all entities in the knowledge graph.
Given a set of test triplets Ttest, MRR is deﬁned as
MRR =
1
|Ttest|
X
(h,r,t)∈Ttest
1
rank(t)
For a ranking, denoted rank(t), of the “true” entity t over all other entities in the knowledge graph.
The hits at K metrics are deﬁned as the proportion of true entities with ranking below some threshold
K:
Hits@K = |{(h, r, t) ∈Ttest | rank(t) ≤K}|
|Ttest|
1p
2p
3p
2i
3i
ip
pi
H@1
4.76
0.64
0.28
0.46
0.32
0.45
0.23
H@10
21.46
2.85
1.38
4.32
3.93
2.75
2.65
MRR
10.16
1.48
0.75
1.90
1.80
1.29
1.29
Table 2: Results (%) for naive complex query answering approach for ShvT model with 64 embed-
ding dimensions and 1 section on FB15k-237.
We also compare the performance of the harmonic extension approach to a more naive method for
answering complex queries within the ShvT model. This approach, as detailed in Guu et al. [14],
amounts to summation across all entity and relation embeddings involved in the complex query.
From the outperformance of the harmonic extension approach relative to this naive method (Ta-
ble 5.1), we can conclude that optimization implemented by harmonic extension effectively con-
strains the intermediate embeddings, perhaps limiting cascading errors across the composite rela-
tions.
18

model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
8
1
0
14.9
0.01
0.06
0.15
0.11
0.01
0
16
1
0
17.21
0.01
0.02
0.33
0.28
0.01
0
32
1
0
18.78
0.01
0.02
0.58
0.5
0.02
0.01
8
0
36.39
0.02
0.02
5.3
4.85
0.02
0.01
0.01
6.19
0.45
0.42
1.37
1.58
0.18
0.4
0.1
3.1
0.19
0.18
0.6
0.72
0.07
0.2
16
0
38.48
0.02
0.02
5.47
4.84
0.02
0.01
0.01
5.35
0.38
0.36
1.19
1.35
0.14
0.39
0.1
2.27
0.16
0.15
0.5
0.6
0.06
0.2
32
0
40.94
0.02
0.02
5.51
5.09
0.02
0.01
0.01
0.79
0.02
0.02
0.14
0.1
0.02
0.18
0.1
0.69
0.02
0.02
0.11
0.07
0.02
0.23
ShvT
32
1
0
19.65
0.01
0.01
7.51
6.44
0.03
0.16
8
0
12.18
0.04
0.01
8.53
8.94
0.22
0.05
0.01
2.18
0.03
0.01
0.98
1.2
0.21
0.01
0.1
0.12
0.04
0.01
0.05
0.06
0.23
0.02
16
0
11.84
0.06
0.01
7.26
7.81
0.23
0.05
0.01
3.84
0.06
0.01
1.18
1.18
0.22
0.03
0.1
0.14
0.07
0.02
0.05
0.06
0.22
0.02
32
0
12.6
0.09
0.03
6.91
7.63
0.23
0.06
0.01
10.24
0.09
0.03
2.06
1
0.22
0.07
0.1
0.21
0.11
0.03
0.06
0.06
0.22
0.04
Table 3: MRR % results for complex query structures on NELL995.
model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
16
1
0
0.16
0.06
0.11
0.23
0.19
0.01
0.01
32
1
0
0.11
0.02
0.03
0.08
0.07
0.01
0.01
64
1
0
0.04
0.02
0.09
0.09
0.09
0.01
0.01
16
0
0.24
0
0
0.12
0.21
0
0
0.01
7.28
0.04
0.03
4.78
2.78
0.02
0.01
0.1
2.5
0.02
0.02
1.15
1.05
0
0
32
0
0.1
0.32
0.52
0.21
0.41
0.07
0.06
0.01
3.64
0.03
0.02
1.22
0.7
0.01
0.01
0.1
2.39
0.01
0.01
0.91
0.82
0
0
64
0
0.13
0.28
0.44
0.41
0.45
0.07
0.06
0.01
0.07
0.01
0.01
0.32
0.36
0
0.01
0.1
0.02
0.01
0.01
0.05
0.06
0
0
ShvT
64
1
0
0.01
0.1
0.04
0.07
0.05
0.07
0.05
16
0
0.05
0.02
0.05
0.04
0.01
0.17
0.14
0.01
0.01
0.02
0.01
0.07
0.07
0.05
0.18
0.1
0.02
0
0
0.01
0
0
0.01
32
0
0.1
0.01
0.05
0.06
0.1
0.09
0.13
0.01
0.03
0.05
0.03
0.01
0.02
0.06
0.05
0.1
0.06
0
0
0.02
0.03
0.02
0.02
64
0
0.07
0.11
0.15
0
0.02
0.12
0.11
0.01
0.04
0.03
0.02
0.02
0.03
0.09
0.05
0.1
0.04
0.01
0.01
0.01
0.06
0.02
0.05
Table 4: Standard deviation of MRR (%) results for complex query structures on FB15k-237.
19

model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
8
1
0
0.14
0
0.02
0.05
0.01
0
0
16
1
0
0.22
0
0
0.03
0.01
0
0
32
1
0
0.16
0
0
0.07
0.1
0
0
8
0
0.29
0
0
0.05
0.28
0
0
0.01
0.16
0.21
0.2
0.04
0.05
0.08
0.19
0.1
0.25
0.08
0.08
0.02
0.03
0.02
0.1
16
0
0.23
0
0
0.09
0.12
0
0
0.01
0.15
0.18
0.17
0.03
0.02
0.06
0.18
0.1
0.17
0.07
0.07
0.03
0.06
0.02
0.09
32
0
0.25
0
0
0.08
0.29
0
0
0.01
0.01
0.01
0
0.01
0.01
0
0.08
0.1
0.03
0
0
0.01
0.01
0
0.11
ShvT
32
1
0
16.96
0.01
0.01
6.46
5.54
0.01
0.12
8
0
0.14
0
0
0.12
0.31
0
0
0.01
0.08
0
0
0.11
0.17
0
0
0.1
0.06
0
0
0.01
0.02
0
0
16
0
0.06
0
0
0.08
0.15
0
0
0.01
0.09
0
0
0.05
0.03
0
0
0.1
0.02
0
0
0
0
0
0
32
0
0.06
0
0
0.09
0.09
0
0
0.01
0.03
0
0.01
0.03
0
0
0
0.1
0.01
0
0
0
0
0
0
Table 5: Standard deviation of MRR (%) results for complex query structures on NELL995.
20

model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Multisection
16
1
0
4.03
0.03
0.04
0.2
0.15
0.01
0.01
32
1
0
4.65
0.01
0.02
0.28
0.18
0.01
0.01
64
1
0
5.1
0.05
0.15
0.41
0.23
0.01
0.01
16
0
13.91
0.01
0.01
8.84
4.55
0.01
0
0.01
5.12
0.04
0.03
1.97
1.3
0.02
0.01
0.1
1.87
0.02
0.02
0.78
0.7
0.01
0.01
32
0
16.69
0.21
0.35
9.64
4.48
0.05
0.03
0.01
2.75
0.03
0.03
0.79
0.68
0.01
0.01
0.1
1.82
0.02
0.02
0.66
0.57
0.01
0.01
64
0
19.16
0.23
0.35
10.9
5.14
0.06
0.03
0.01
0.4
0
0
0.46
0.46
0.01
0.01
0.1
0.5
0.01
0.01
0.52
0.48
0.01
0.01
Translational
64
1
0
4.76
0.71
0.46
0.36
0.37
1.09
0.59
16
0
0.01
1.86
1.74
0.41
0.32
1.05
1.01
0.01
0
0.23
0.37
0.22
0.24
0.22
0.2
0.1
0
0.03
0.02
0
0
0.14
0.01
32
0
0.01
1.3
1.58
0.29
0.27
1.18
0.79
0.01
0
0.23
0.47
0.19
0.2
0.4
0.39
0.1
0
0.05
0.06
0.01
0.03
0.17
0.06
64
0
0.01
0.75
1.28
0.05
0.15
1.32
0.52
0.01
0
0.22
0.6
0.18
0.18
0.85
0.43
0.1
0
0.06
0.15
0.03
0.09
0.26
0.18
Table 6: Hits @ 1 % results for complex query structures on FB15k-237.
model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
16
1
0
18.48
0.17
0.27
1.32
0.94
0.09
0.08
32
1
0
19.91
0.13
0.19
1.72
1.15
0.06
0.07
64
1
0
21.25
0.32
0.67
2.48
1.44
0.09
0.11
16
0
33.19
0.07
0.08
29.88
20
0.07
0.06
0.01
14.7
0.15
0.14
9.96
8.12
0.09
0.1
0.1
5.88
0.11
0.11
3.14
3.28
0.07
0.08
32
0
35.11
0.67
1.03
31.9
20.77
0.21
0.21
0.01
9.45
0.12
0.12
4.74
4.34
0.07
0.09
0.1
5.24
0.1
0.09
3.19
3.31
0.07
0.07
64
0
36.21
0.8
1.25
33.17
21.06
0.25
0.26
0.01
0.71
0.04
0.04
1.76
2
0.07
0.07
0.1
0.74
0.06
0.05
1.84
1.79
0.08
0.07
ShvT
64
1
0
21.46
3.65
3.07
3.23
3.01
6.67
4.75
16
0
31.53
7.79
7.5
3.36
2.63
9.77
7.47
0.01
9.52
1.77
1.8
1.57
1.74
1.66
1.71
0.1
1.33
0.12
0.1
0.1
0.09
0.22
0.14
32
0
31.2
8.45
8.03
3.38
2.69
10.67
7.82
0.01
11.71
2.75
2.75
2.2
2.15
3.82
3.81
0.1
3.12
0.4
0.38
0.79
1.05
0.6
0.83
64
0
30.69
9.67
8.81
3.22
2.67
12.33
8.25
0.01
11.99
4.45
4.21
2.67
2.69
7.28
5.71
0.1
4
0.94
0.85
1.73
2.27
1.69
2.22
Table 7: Hits @ 10 % results for complex query structures on FB15k-237.
21

model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
8
1
0
7.16
0
0.01
0.03
0.01
0
0
16
1
0
8.78
0
0
0.09
0.06
0
0
32
1
0
9.89
0
0
0.19
0.15
0
0
8
0
24.45
0
0
2.19
1.83
0
0
0.01
3.93
0.33
0.32
0.61
0.67
0.11
0.2
0.1
2.11
0.13
0.13
0.4
0.46
0.04
0.12
16
0
26.64
0
0
2.27
1.77
0
0
0.01
3.55
0.28
0.28
0.6
0.6
0.09
0.2
0.1
1.56
0.12
0.11
0.35
0.42
0.03
0.12
32
0
29.33
0
0
2.28
1.99
0
0
0.01
0.57
0
0
0.07
0.04
0
0.09
0.1
0.51
0
0
0.05
0.03
0
0.13
ShvT
32
1
0
12.46
0
0
3.14
2.44
0
0.01
8
0
0.06
0.03
0
2.29
2.32
0.2
0.01
0.01
0.02
0.02
0
0.3
0.33
0.2
0
0.1
0.02
0.02
0
0.02
0.03
0.21
0
16
0
0.06
0.05
0.01
1.2
1.24
0.22
0.01
0.01
0.03
0.05
0.01
0.21
0.21
0.21
0
0.1
0.02
0.06
0
0.02
0.03
0.21
0
32
0
0.07
0.08
0.02
0.68
1.15
0.22
0.01
0.01
0.08
0.07
0.02
0.18
0.15
0.21
0.01
0.1
0.02
0.1
0.02
0.02
0.02
0.21
0.01
Table 8: Hits @ 1 % results for complex query structures on NELL995.
model
dim F(r)
Sections
α
1p
2p
3p
2i
3i
ip
pi
Shv
8
1
0
30.56
0.01
0.09
0.24
0.17
0.01
0
16
1
0
34.25
0.01
0.01
0.62
0.5
0.01
0
32
1
0
36.91
0.01
0.02
1.05
0.85
0.02
0
8
0
61.23
0.02
0.02
11.56
11.12
0.02
0.01
0.01
10.55
0.61
0.54
2.83
3.51
0.26
0.7
0.1
4.94
0.24
0.23
0.9
1.12
0.08
0.31
16
0
62.65
0.02
0.02
11.87
11.11
0.02
0
0.01
8.82
0.5
0.45
2.2
2.74
0.2
0.67
0.1
3.59
0.2
0.19
0.72
0.88
0.07
0.31
32
0
64.55
0.02
0.02
11.98
11.31
0.02
0
0.01
1.14
0.02
0.01
0.22
0.15
0.02
0.3
0.1
0.98
0.02
0.02
0.17
0.1
0.02
0.36
ShvT
32
1
0
34.13
0.01
0.01
15.92
14.05
0.03
0.4
8
0
37.47
0.06
0.01
21.21
22.94
0.24
0.09
0.01
5.75
0.04
0
1.84
2.4
0.22
0.02
0.1
0.25
0.05
0.01
0.07
0.07
0.22
0.01
16
0
35.75
0.07
0.02
20.82
22.76
0.24
0.09
0.01
10.6
0.06
0.02
2.59
2.47
0.22
0.04
0.1
0.3
0.08
0.02
0.07
0.06
0.22
0.03
32
0
37.72
0.1
0.03
21.94
23.2
0.24
0.12
0.01
25.27
0.1
0.04
5.84
2.16
0.23
0.17
0.1
0.5
0.11
0.03
0.1
0.1
0.22
0.06
Table 9: Hits @ 10 % results for complex query structures on NELL995.
22

