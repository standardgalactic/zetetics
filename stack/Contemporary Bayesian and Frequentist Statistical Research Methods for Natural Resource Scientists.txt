
CONTEMPORARY
BAYESIAN AND
FREQUENTIST
STATISTICAL RESEARCH
METHODS FOR NATURAL
RESOURCE SCIENTISTS


CONTEMPORARY
BAYESIAN AND
FREQUENTIST
STATISTICAL RESEARCH
METHODS FOR NATURAL
RESOURCE SCIENTISTS
Howard B. Stauffer
Mathematics Department, Humboldt State University, Arcata, California

Copyright # 2008 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except
as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the
prior written permission of the Publisher, or authorization through payment of the appropriate per-copy
fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978)
750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for
permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River
Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.
com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States
at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in variety of electronic formats. Some content that appears in print
may not be available in electronic formats. For more information about Wiley products, visit our
web site at www.wiley.com.
Wiley Bicentennial Logo: Richard J. Paciﬁco
Library of Congress Cataloging-in-Publication Data:
Stauffer, Howard B., 1941-
Contemporary Bayesian and frequentist statistical research methods for natural resource
scientists/Howard B. Stauffer.
p. cm.
ISBN 978-0-470-16504-1 (cloth)
1.
Bayesian statistical decision theory.
2.
Mathematical statistics.
I.
Title.
QA279.5.S76 2008
519.5’42—dc22
2007015575
Printed in the United States of America
10
9
8
7
6
5
4
3
2
1

To my parents,
Howard Hamilton Stauffer and Elizabeth Boyer Stauffer,
and to my family,
wife Rebecca Ann Stauffer,
daughter Sarah Elizabeth Stauffer,
and son Noah Hamilton Stauffer.
Their love and support has sustained me and provided
meaning and joy in my life.


CONTENTS
Preface
xiii
1
Introduction
1
1.1
Introduction
2
1.2
Three Case Studies
2
1.2.1
Case Study 1: Maintenance of a Population Parameter
above a Critical Threshold Level
2
1.2.2
Case Study 2: Estimation of the Abundance of a Discrete
Population
3
1.2.3
Case Study 3: Habitat Selection Modeling of a Wildlife
Population
4
1.2.4
Case Studies Summary
5
1.3
Overview of Some Solution Strategies
5
1.3.1
Sample Surveys and Parameter Estimation
5
1.3.2
Experiments and Hypothesis Testing
8
1.3.3
Multiple Linear Regression, Generalized Linear Modeling,
and Model Selection
9
1.3.4
A Preview of Bayesian Statistical Inference
10
1.3.5
A Preview of Model Selection Strategies and
Information-Theoretic Criteria for Model Selection
11
1.3.6
A Preview of Mixed-Effects Modeling
14
1.4
Review: Principles of Project Management
14
1.5
Applications
15
1.6
S-Plusw and R Orientation I: Introduction
16
1.6.1
Orientation I
16
1.6.2
Simple Manipulations
17
1.6.3
Data Structures
21
1.6.4
Random Numbers
21
1.6.5
Graphs
21
1.6.6
Importing and Exporting Files
22
1.6.7
Saving and Restoring Objects
22
vii

1.6.8
Directory Structures
22
1.6.9
Functions and Control Structures
22
1.6.10
Linear Regression Analysis in S-Plus and R
23
1.7
S-Plus and R Orientation II: Distributions
23
1.7.1
Uniform Distribution
23
1.7.2
Normal Distribution
24
1.7.3
Poisson Distribution
26
1.7.4
Binomial Distributions
27
1.7.5
Simple Random Sampling
33
1.8
S-Plus and R Orientation III: Estimation of Mean and
Proportion, Sampling Error, and Conﬁdence Intervals
34
1.8.1
Estimation of Mean
34
1.8.2
Estimation of Proportion
36
1.9
S-Plus and R Orientation IV: Linear Regression
36
1.10
Summary
39
Problems
40
2
Bayesian Statistical Analysis I: Introduction
47
2.1
Introduction
47
2.1.1
Historical Background
47
2.1.2
Limitations to the Use of Frequentist Statistical Inference for
Natural Resource Applications: An Example
49
2.2
Three Methods for Fitting Models to Datasets
50
2.2.1
Least-Squares (LS) Fit—Minimizing a
Goodness-of-Fit Proﬁle
51
2.2.2
Maximum-Likelihood (ML) Fit—Maximizing the
Likelihood Proﬁle
52
2.2.3
Bayesian Fit—Bayesian Statistical Analysis and Inference
54
2.2.4
Examples
56
2.3
The Bayesian Paradigm for Statistical Inference: Bayes Theorem
61
2.4
Conjugate Priors
63
2.4.1
Continuous Data with the Normal Model
64
2.4.2
Count Data with the Poisson Model
66
2.4.3
Binary Data with the Binomial Model
69
2.4.4
Conjugate Priors for Other Datasets
71
2.5
Other Priors
72
2.5.1
Noninformative, Uniform, and Proper or Improper Priors
73
2.5.2
Jeffreys Priors
73
2.5.3
Reference Priors, Vague Priors, and Elicited Priors
74
viii
CONTENTS

2.5.4
Empirical Bayes Methods
74
2.5.5
Sensitivity Analysis: An Example
74
2.6
Summary
77
Problems
77
3
Bayesian Statistical Inference II: Bayesian Hypothesis Testing
and Decision Theory
81
3.1
Bayesian Hypothesis Testing: Bayes Factors
81
3.1.1
Proportion Estimation of Nesting Northern
Spotted Owl Pairs
83
3.1.2
Medical Diagnostics
83
3.2
Bayesian Decision Theory
88
3.3
Preview: More Advanced Methods of Bayesian Statiscal
Analysis—Markov Chain Monte Carlo (MCMC)
Alogrithms and WinBUGS Software
90
3.4
Summary
91
Problems
91
4
Bayesian Statistical Inference III: MCMC Algorithms and
WinBUGS Software Applications
93
4.1
Introduction
93
4.2
Markov Chain Theory
94
4.3
MCMC Algorithms
96
4.3.1
Gibbs Sampling
96
4.3.2
The Metropolis–Hastings Algorithm
98
4.4
WinBUGS Applications
101
4.4.1
The Normal Mean Model for Continuous Data
106
4.4.2
Models for Count Data: The Poisson Model, Poisson–Gamma
Negative Binomial Model, and Overdispersed
Mixed-Effects Poisson Model
110
4.4.3
The Linear Regression Model
112
4.5
Summary
115
Problems
115
5
Alternative Strategies for Model Selection and Inference Using
Information-Theoretic Criteria
121
5.1
Alternative Strategies for Model Selection and Inference:
Descriptive and Predictive Model Selection
121
5.1.1
Introduction
121
5.1.2
The Metaphor of the Race
123
CONTENTS
ix

5.2
Descriptive Model Selection: A Posteriori Exploratory Model
Selection and Inference
124
5.3
Predictive Model Selection: A Priori Parsimonious
Model Selection and Inference Using
Information-Theoretic Criteria
127
5.4
Methods of Fit
128
5.5
Evaluation of Fit: Goodness of Fit
129
5.6
Model Averaging
131
5.6.1
Unconditional Estimators for Parameters:
Covariate Coefﬁcient Estimators, Errors,
and Conﬁdence Intervals
131
5.6.2
Unconditional Estimators for Prediction
133
5.6.3
Importance of Covariates
133
5.7
Applications: Frequentist Statistical Analysis in S-Plus and R;
Bayesian Statistical Analysis in WinBUGS
134
5.7.1
Frequentist Statistical Analysis in S-Plus and R: Predictive
A Priori Parsimonious Model Selection and Inference Using
the Akaike Information Criterion (AIC)
136
5.7.2
Frequentist Statistical Analysis in S-Plus and R: Descriptive
A Posteriori Model Selection and Inference
137
5.7.3
Bayesian Statistical Analysis in WinBUGS: A Priori
Parsimonious Model Selection and Inference Using the
Deviance Information Criterion (DIC)
146
5.8
Summary
150
Problems
151
6
An Introduction to Generalized Linear Models: Logistic
Regression Models
155
6.1
Introduction to Generalized Linear Models (GLMs)
155
6.2
GLM Design
156
6.3
GLM Analysis
157
6.4
Logistic Regression Analysis
159
6.4.1
The Link Function and Error Assumptions of the
Logistic Regression Model
161
6.4.2
Maximum-Likelihood (ML) Fit of the Logistic
Regression Model
162
6.4.3
Logistic Regression Statistics
162
6.4.4
Goodness of Fit of the Logistic Regression Model
167
6.5
Other Generalized Linear Models (GLMs)
175
x
CONTENTS

6.6
S-Plus or R and WinBUGS Applications
176
6.6.1
Frequentist Logistic Regression Analysis in
S-Plus and R
176
6.6.2
Bayesian Analysis in WinBUGS
178
6.7
Summary
185
Problems
187
7
Introduction to Mixed-Effects Modeling
191
7.1
Introduction
191
7.2
Dependent Datasets
192
7.3
Linear Mixed-Effects Modeling: Frequentist Statistical
Analysis in S-Plus and R
194
7.3.1
Generalization of Analysis of Variance (ANOVA)
194
7.3.2
Generalization of the Multiple Linear Regression Model
205
7.3.3
Variance–Covariance Structure Between-Groups
Random Effects
220
7.3.4
Variance Structure Within Group Random Effects
222
7.3.5
Covariance Structure Within-Group Random Effects:
Time-Series and Spatially Dependent Models
224
7.4
Nonlinear Mixed-Effects Modeling: Frequentist Statistical Analysis
in S-Plus and R
232
7.5
Conclusions: Frequentist Statistical Analysis in S-Plus and R
238
7.5.1
Conclusions: The Analysis
238
7.5.2
Conclusions: The Reality of the Dataset
238
7.6
Mixed-Effects Modeling: Bayesian Statistical Analysis in WinBUGS
239
7.7
Summary
241
Problems
241
8
Summary and Conclusions
247
8.1
Summary of Solutions to Chapter 1 Case Studies
247
8.1.1
Case Study 1: Maintenance of a Population Parameter above
a Critical Threshold Level
248
8.1.2
Case Study 2: Estimation of the Abundance of a Discrete
Population
249
8.1.3
Case Study 3: Habitat Selection Modeling of a Wildlife
Population
249
8.2
Appropriate Application of Statistics in the Natural
Resource Sciences
250
8.3
Statistical Guidelines for Design of Sample Surveys
and Experiments
252
CONTENTS
xi

8.4
Two Strategies for Model Selection and Inference
253
8.5
Contemporary Methods for Statistical Analysis I: Generalized Linear
Modeling and Mixed-Effects Modeling
254
8.6
Contemporary Methods in Statistical Analysis II: Bayesian Statistical
Analysis Using MCMC Methods with WinBUGS Software
255
8.7
Concluding Remarks: Effective Use of Statistical Analysis and
Inference
256
8.8
Summary
256
Appendix A
Review of Linear Regression and Multiple Linear
Regression Analysis
259
A.1
Introduction
259
A.2
Least-Squares Fit: The Linear Regression Model
261
A.3
Linear Regression and Multiple Linear Regression Statistics
262
A.3.1
Estimates of Coefﬁcients and Their Signiﬁcance: Conﬁdence
Intervals and t Tests
262
A.3.2
The Coefﬁcient of Determination R2
263
A.3.3
The Residual Standard Error syjx
267
A.3.4
The F Test
267
A.3.5
Adjusted R2
269
A.3.6
Mallow’s Cp
269
A.3.7
Akaike’s Information Criterion: AIC and AICc
270
A.3.8
Bayesian Information Criterion (BIC)
271
A.4
Stepwise Multiple Linear Regression Methods
272
A.5
Best-Subsets Selection Multiple Linear Regression
273
A.6
Goodness of Fit
274
A.6.1
Residual Analysis
274
A.6.2
Conﬁdence Intervals
275
A.6.3
Prediction Intervals
275
A.6.4
Cross-Validation and Testing Techniques
276
Appendix B
Answers to Problems
277
References
383
Index
389
xii
CONTENTS

PREFACE
This book began as a critique against the current misuses of statistics in the natural
resource sciences. I had worked for many years as a forestry and wildlife management
statistician, in academia, government, and industry. I was frustrated with the frequent
misuse of statistical analysis and inference with natural resource data. Hypothesis
testing was commonly misused with observational data to compare so-called
habitat treatments such as old-growth and young-growth forest habitat for their
effects on wildlife species. Such hypotheses were statistical rather than scientiﬁc,
referring to speciﬁc stands of interest. Many null hypotheses were “silly” and
clearly not true. Sample datasets were not completely randomized, and “experimen-
tal” conditions were not effectively controlled. I was reviewing manuscripts and
attending seminars where null hypotheses were being rejected that were clearly
false a priori, and effect sizes between treatments, the differences of biological
importance that were of interest to wildlife managers, were not even being estimated.
More seriously, null hypotheses that were clearly false were being “supported” by
hypothesis testing results that failed to reject, in studies where sample sizes were
small, effect sizes of importance, and power to detect these effect sizes were not
speciﬁed, and this power was very likely small.
Natural resource scientists did not clearly understand how to interpret their
inferences from frequentist statistical analysis. The indirect logic of frequentist
statistical inference, in interpreting the meaning of conﬁdence intervals or the test
statistics and p values from hypothesis testing, was proving to be very confusing to
natural resource scientists. The challenge of natural resource scientists in explaining
such frequentist inferences to managers, attorneys, politicians, and the public was
proving to be even more daunting.
I was concerned with the extent of data dredging that was common in the ﬁeld. I
was commonly seeing datasets collected for habitat selection modeling using multiple
linear regression or logistic regression analysis with measurements for over 100 cov-
ariates and sample sizes under 100. Scientists were not giving enough thought to
sampling design and the type of analysis appropriate for their studies, prior to data
collection. Stepwise and best-subsets selection methods were being utilized
without concern for their potential for overﬁtting sample datasets with large and
unspeciﬁed amounts of compounded error.
Then, around 1998/99, several pioneering applied statisticians with many years of
experience in the ﬁeld of wildlife management began to show the way out of this
wilderness. Ken Burnham and David Anderson (1998, 2002) published their
xiii

landmark book advocating the use of a priori model selection and inference using the
Akaike information criterion (AIC) as a way of reducing model overﬁtting and com-
pounding of error with model selection and inference. Doug Johnson’s (1999) article
critiquing the misuses of hypothesis testing in wildlife management research was
published in the Journal of Wildlife Management. These ideas took the wildlife man-
agement research community by storm. Ray Hilborn and Mark Mangel (1997) pub-
lished a seminal book advocating the use of Bayesian statistical analysis and inference
in the ﬁelds of ecology and ﬁsheries management. Pinhiero and Bates (2000) pub-
lished an important book describing the applications of mixed-effects modeling in
S-Plus. Ramsey and Schafer (2002) warned natural resource scientists about the dis-
tinctions between observational and experimental data. Because of some of these
inﬂuences, a priori model selection and inference has become the accepted dominant
paradigm for model selection and inference in the ﬁeld of wildlife management. The
misapplication of hypothesis testing has been reduced. Perhaps even too hastily, the
old ways of doing statistics have been discarded in the rush to remain “current.”
Meanwhile, many other important contemporary methods of applied statistics
remain relatively unknown among natural resource scientists, methods such as gener-
alized linear modeling, mixed-effects modeling, and Bayesian statistical analysis and
inference.
This book was written to introduce these newer contemporary methods of statisti-
cal analysis to natural resource scientists and strike a balance between the old and new
ways of doing statistics. Chapter 1 introduces three case studies that illustrate the need
for newer contemporary methods of statistical analysis and inference for natural
resource science applications. It also reviews some of the most important fundamen-
tal methods of traditional frequentist statistical analysis and inference and ends with a
brief introduction to the frequentist software S-Plus and R that are used throughout
the book. Chapters 2–4 introduce an alternative approach to traditional frequentist
statistical analysis and inference, namely, Bayesian statistical analysis and inference.
These three chapters provide an introduction to the fundamental concepts of Bayesian
statistical analysis, its historical background, conjugate solutions, Bayesian hypoth-
esis testing and decisionmaking, Markov Chain Monte Carlo (MCMC) solutions,
and applications in WinBUGS (Windows version of Bayesian statistical inference
Using Gibbs Sampling) software. Chapter 5 presents two alternative strategies to
model selection and inference, a posteriori model selection and inference, and a
priori parsimonious model selection and inference using AIC and the deviance infor-
mation criterion (DIC). Chapter 6 introduces the ideas of generalized linear modeling
(GLM), focusing on the most popular GLM of logistic regression. Chapter 7 presents
an introduction to mixed-effects modeling in S-Plusw and R. Chapters 5–7 provide
applications with both frequentist and Bayesian statistical analysis and inference
approaches, illustrating the strengths and limitations of each approach. Chapter 8
concludes with a summary of the contemporary methods introduced in this book.
This book can be used as a textbook for an intermediate undergraduate or introduc-
tory graduate semester course in contemporary research statistics for natural resources
sciences. It assumes a minimum prerequisite undergraduate course in introductory
statistics that includes the estimation of parameters such as mean and proportion;
xiv
PREFACE

hypothesis testing with t tests, F tests for analysis of variance (ANOVA), and
chi-square (x2) tests; and linear regression analysis. Parts of the book can be read
independently along with the introductory Chapter 1, Chapters 2–4 on Bayesian
statistical analysis and inference, Chapter 5 on strategies for model selection and
inference, Chapter 6 as an introduction to generalized linear modeling, and
Chapter 7 on mixed-effects modeling. The book can also be read and used as a
refresher manual or a reference book by natural resource scientists. Parts of the
book have served as resource materials that I have used for 2-day workshops on
topics of statistics such as Bayesian statistical analysis and inference using
WinBUGS and capture–recapture analysis using MARK.
I’d like to thank many colleagues who have provided advice, encouragement, and
support throughout my career as an applied statistician and inﬂuenced a perspective
that has led to the writing of this book: David Anderson, Doug Johnson, Barry
Noone, Bill Zielinski, C. J. Ralph, Cindy Zabel, Hart Welsh, Cynthia Perrine,
Larry Fox, Jan Derksen, Rich Padula, Bryan Gaynor, Ken Mitchell, Sam Otukol,
A. Y. Omule, David Gilbert, Les Safranyik, Mark Rizzardi, Yoon Kim, Butch
Weckerly, David Hankin, John Sawyer, Mike Messler, Andrea Pickart, Matt
Johnson, and Mark Colwell. I’d also like to thank the Wiley staff who were so
helpful during the publication process: editor Susanne Steitz-Filler, senior production
editor Kris Parrish, and copy editor Cathy Hertz. My career has been a most interest-
ing one, working with natural resource scientists in academia, government, and indus-
try, applying traditional and contemporary ideas in the application of statistical design
and analysis to the natural resource sciences. It is up to natural resource scientists to
make the most appropriate and effective choices on the applications of statistical
analysis to their research problems. It is my hope that this book will help in providing
the tools to make that possible.
HOWARD B. STAUFFER
Mathematics Department
Humboldt State University
Arcata, California
PREFACE
xv


1
Introduction
We will begin this initial chapter by introducing three case studies that illustrate some
of the fundamental general statistical problems challenging the contemporary natural
resource scientist. We will then present a review and preview of some solution strat-
egies to these general problems. The ﬁrst solution strategies that we will review are
traditional frequentist approaches: parameter estimation from sample surveys, hypoth-
esis testing from experiments, and linear regression modeling. Each of these methods
is summarized using a frequentist approach to statistical analysis. We will then
preview some more contemporary solution strategies: an alternative Bayesian
approach to statistical analysis and other more advanced solutions to the case
studies, generalized linear modeling, and mixed-effects modeling using both frequen-
tist and Bayesian approaches to statistical analysis. We will also preview a more con-
temporary approach to model selection and inference using information-theoretic
criteria such as Akaike’s information criterion for frequentist statistical analysis
and the deviance information criterion for Bayesian statistical analysis. All of these
contemporary methods will be discussed in greater detail throughout the remainder
of this book and illustrated with examples.
In this initial chapter we include a reminder of the importance of project manage-
ment in natural resource studies with statistical components. Project management
consists of organizing projects into three phases: a planning phase, a data collection
phase, and a concluding phase. The planning phase includes an identiﬁcation of the
problem and the objectives of the project, along with a statistical design for the col-
lection of the dataset. The concluding phase includes a statistical analysis of the
dataset, along with interpretation and conclusions drawn from the analysis. All of
these statistical components—the statistical design, the collection of the dataset,
and the statistical analysis—provide essential tools for the solutions to the objectives
of the project.
We conclude this initial chapter with an introduction to the frequentist statistical
analysis software used throughout the book: the proprietary software S-Plus and its
freeware “equivalent” R. The Bayesian statistical analysis software WinBUGS will
be introduced in Chapters 2–4 when Bayesian ideas are discussed.
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
1

1.1
INTRODUCTION
In recent years there have been major advances in the methods of statistics used for
research in the natural resource sciences. Yet, little of this is known outside selected
research circles. Students and scientists in the natural resource sciences have contin-
ued to use traditional frequentist methods, such as the estimation of parameters from
sample surveys, t tests and ANOVA hypothesis testing from experiments, and linear
regression modeling. However, extraordinary newer methods are now available that
enhance, complement, and extend these basic techniques, methods such as
Bayesian statistical inference, information-theoretic approaches to model selection,
generalized linear modeling, and mixed-effects modeling. It is the primary objective
of this book to introduce these newer contemporary methods to natural resource
students and scientists.
This book must begin by emphasizing critical statistical issues that have too often
been neglected in natural resource studies in the past. We stress the importance of the
planning and concluding phases in a data collection project. We particularly highlight
the essential role of statistical design and analysis that help ensure the efﬁcient, power-
ful, and effective use of data. Our approach throughout the book will be “hands-on,”
illustrating concepts with examples using the software languages of S-Plus or R for
frequentist statistical analysis and WinBUGS for Bayesian statistical analysis.
Let’s begin with a description of several case studies that illustrate problems of
fundamental interest to contemporary natural resource scientists.
1.2
THREE CASE STUDIES
1.2.1
Case Study 1: Maintenance of a Population Parameter Above a
Critical Threshold Level
A fundamental problem of interest to contemporary natural resource scientists is to
assess whether a critical population parameter, such as a proportion parameter p,
has been maintained above (or below) a speciﬁed critical threshold level: p  pc
(or p  pc)?
Many examples in natural resource science illustrate this problem:
1. A timber company is required to maintain the proportion p of its timberlands
occupied by nesting Northern Spotted Owl pairs above a speciﬁed threshold
level pc. The threshold pc is a level determined by biologists to ensure the via-
bility of the local population of owls.
2. Federal managers of a national forest are interested in maintaining the pro-
portion p of forest covered by dense undergrowth below a speciﬁed threshold
level pc, to limit the risk of ﬁre.
3. The managers of a national park are interested in maintaining the proportion
p of a disease or insect infestation below a speciﬁed threshold level pc to
control its spread.
2
INTRODUCTION

4. Fishery biologists managing a watershed are interested in maintaining the
proportional abundance p of a ﬁshery above a speciﬁed threshold level pc of
its carrying capacity to ensure its long-term sustainability.
5. A government agency implementing a natural resource conservation policy is
interested in ensuring that the proportion p of the public in favor of one of its
controversial policies is maintained above a certain threshold level pc.
Besides the proportion parameter p in the examples presented above, there are
many other biological parameters of interest to natural resource managers with
similar threshold issues, such as the mean abundance m, survival rate fi from year
i to year i þ 1, ﬁtness li ¼ Niþ1/Ni (where Ni and Niþ1 are the population abun-
dances in years i and i þ 1), ecological diversity index such as the Shannon–
Wiener diversity index H, and population total t.
The failure to maintain the population parameter p above (or below) the threshold
level pc might suggest the need for a “corrective action” decision in the examples
listed above, such as
1. Reducing the timber harvesting
2. Applying ﬁre suppression treatment
3. Applying disease or insect treatment
4. Increasing the watershed river ﬂow by releasing more water from a dam
5. Altering the natural resource conservation policy
Alternatively, success at maintaining the population parameter p above (or below) the
threshold pc might suggest a decision of “no action.”
In such circumstances, a common approach employed by natural resource scien-
tists is to begin monitoring the population and collecting sample data, say, on an
annual basis, in order to assess the status of the population parameter. The intent is
to conduct statistical analysis on the sample data and make inferences about the popu-
lation parameter to determine whether it is above (or below) the threshold, and thus
whether corrective action or no action is needed at the management level.
1.2.2
Case Study 2: Estimation of the Abundance of a
Discrete Population
Our second case study focuses on the analysis of population count data. Often
biological populations, such as birds, amphibians, or mammals, are sampled with
discrete measurements such as plot counts, in ﬁxed-area plots called quadrants.
The intent is to estimate population size or density in an area using total count esti-
mates of abundance or mean estimates of density.
The analysis consists of estimates of total or mean. Traditional estimates of total or
mean are based on the assumption of the normal distribution of the population
measurements. For plot counts, however, measurements are discrete and noncontin-
uous, consisting of nonnegative integers in a skewed distribution. If the biological
1.2
THREE CASE STUDIES
3

population is randomly dispersed spatially, a proper model for the analysis should be
based on the Poisson distribution rather than the normal distribution for the plot
counts. If, however, the population is spatially aggregated or clumped, the analysis
should be based on a more general model for the population measurements, such
as a negative binomial distribution. Furthermore, the plot counts will likely be
sampled without complete certainty of detection. Animals may be within the plot
and yet be undetected by the sample surveyor. A rigorous analysis of the population
therefore must factor in the Poisson or negative binomial distribution of the plot
measurements, sampled with an uncertainty of detection. We will examine such
analyses in Chapters 2–4 with Bayesian statistical analysis and Chapters 6–7 with
generalized linear models and mixed-effects models.
1.2.3
Case Study 3: Habitat Selection Modeling of a Wildlife Population
In general, it can be quite difﬁcult to estimate the presence or abundance of a wildlife
population. Many important biological populations whose presence or abundance
needs to be estimated are endangered or locally threatened wildlife species, such as
the Northern Spotted Owl and Marbled Murrelet bird populations, Del Norte
salamander amphibian population, and grizzly bear mammal population. These
endangered species are often of particular importance because they are associated with
old-growth ecosystems that are also in danger of extinction. Therefore it is important
to monitor these populations, estimating their presence or abundance over time, to
assess the status of the old-growth ecosystems. A particularly effective approach to
estimating these mobile populations is to model their relationship with habitat.
With habitat selection modeling, the presence or abundance of a mobile popu-
lation species is treated as a dependent response variable. Its relationship with
“independent” predictor explanatory habitat variables such as vegetation, geologic,
and meteorologic attributes can be assessed with statistical modeling. The intent of
the habitat selection modeling is to analyze the relationship between the mobile wild-
life population variable and the habitat variables and use it to describe or predict the
presence or abundance of the endangered species as a function of the habitat vari-
ables. The idea behind the modeling is that many habitat variables can be more
easily and less expensively sampled than can the mobile wildlife population.
The relationship in such circumstances is assumed to be associative rather than
causal; thus, the modeling is descriptive, based on population monitoring with
sample survey data, and not on experimental manipulation to establish evidence
for cause and effect. The mobile wildlife population may have access to only a
limited amount of habitat attributes and be able to express a restricted preference
among what remains. Other habitat attributes that the mobile wildlife population
most prefers may no longer be available for selection. Hence the habitat “selection”
relationship must be interpreted within this context.
Habitat selection modeling is often based on regression analysis. For continuous-
abundance response variables such as biomass, multiple linear regression analysis
may indeed be applicable. For discrete-abundance response variables such as popu-
lation counts, however, Poisson regression or negative binomial regression may be
4
INTRODUCTION

more appropriate. For binary response variables, such as the presence or absence, or
occupancy versus nonoccupancy, of a population, logistic regression analysis or some
other form of generalized linear modeling may be more appropriate. We will examine
these methods of analysis, along with strategies for model selection, in Chapters 5
and 6. Traditional multiple linear regression analysis is discussed in Chapter 5.
Logistic regression analysis, Poisson regression
analysis, negative binomial
regression analysis, and other forms of generalized linear modeling are discussed
in Chapter 6.
1.2.4
Case Studies Summary
This book presents various contemporary statistical options available to the natural
resource scientist to analyze and interpret sample data for these case studies and
other general statistical problems of current interest to natural resource scientists.
We will ﬁrst review more familiar traditional statistical methods of sample survey par-
ameter estimation, experimental hypothesis testing, and multiple linear regression
modeling, and then describe the less familiar contemporary methods of Bayesian
statistical inference, model selection strategies, generalized linear modeling, and
mixed-effects modeling. These methods provide contemporary natural resource
scientists with an up-to-date statistical toolbox of methods to tackle many important
challenging problems of current interest.
1.3
OVERVIEW OF SOME SOLUTION STRATEGIES
In this section we present both a review of traditional statistical methods and a
preview of contemporary statistical methods that provide solutions to the case
studies that were presented in the previous section: assessing whether a population
parameter has been maintained above (or below) a critical threshold level, the
estimation of abundance of a discrete population, and habitat selection modeling.
Further details on the contemporary methods will follow in later chapters.
1.3.1
Sample Surveys and Parameter Estimation
A ﬁrst traditional statistical approach to addressing the fundamental case study pro-
blems of Section 1.2 is to conduct a sample survey of the population and collect
sample data using a rigorous sampling design. The aim of the survey in case study
1 is to estimate a proportion parameter p or mean parameter m from the sample
data and compare it with a critical threshold level pc or mc. The aim of the survey
in case study 2 is to estimate the mean abundance parameter m of a discrete popu-
lation. The aim of the survey in case study 3 is to model a mobile wildlife population
as a function of habitat attributes and estimate the proportion parameter p or abundance
parameter mean m of the species in the habitat or at a speciﬁc site. Ideally, a natural
resource scientist would like to use an approximately unbiased estimator uˆ ¼ pˆ or mˆ
for the estimate of the parameter u ¼ p or m, respectively, of minimum sampling
1.3
OVERVIEW OF SOME SOLUTION STRATEGIES
5

error E, with a speciﬁed level of conﬁdence P (or level of signiﬁcance
a ¼ 12P).
If simple randomly sampled measurements fyig are continuous and normally
distributed with sample size n, the mean estimator is given by
^m ¼
X
n
i¼1
yi
n ,
with standard deviation
s ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
n
i¼1
(yi  ^m)2
n  1
,
s
standard error
se ¼ sﬃﬃﬃn
p ,
and sampling error
E ¼ t(1a=2), n1  se ¼ t(1a=2), n1  sﬃﬃﬃn
p ,
where t(12a/2), n21 is the t value with (n21) degrees of freedom at the (12a/2) per-
centile with a level of signiﬁcance.
If simple randomly sampled measurements fyig are binary and binomially distrib-
uted with sample size n  30, the proportion estimator is given by
^p ¼ y
n ,
where
y ¼
X
n
i¼1
yi,
with standard error
se ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p  (1  ^p)
n  1
r
,
and sampling error
E ¼ t(1a=2), n1  se,
where t(12a/2),n21 is the t value with (n 2 1) degrees of freedom at the (1 2 a/2)
percentile with a level of signiﬁcance.
Recall that an unbiased estimator has the property that the average of all esti-
mates, with repeated sampling, is equal to the parameter value (Fig. 1.1).
Conﬁdence levels of P ¼ 95%, 90%, or 80% are commonly used for natural resource
survey sampling with levels of signiﬁcance a ¼ 12P ¼ 5%, 10%, and 20%, respect-
ively. A conﬁdence interval
CI ¼ ½^u  E, ^u þ E
6
INTRODUCTION

can be calculated and the frequentist inference drawn that there is a probability P that
conﬁdence intervals will contain the parameter u, with repeated sampling (Fig. 1.2).
Note that the logic of frequentist inference is of the form “if (parameter), then
probability(data).” It assumes that the parameter is ﬁxed and provides conditional
probability properties for statistics from the sample datasets.
Figure 1.1. Sample mean density estimates (tickmarks X), based on 100 repeated sample
surveys of a normally distributed population with mean density parameter value 50.0. Note
that the average of these sample mean density estimates is approximately equal to the mean
density parameter value for this unbiased mean estimator.
Figure 1.2. Twenty 95% conﬁdence intervals estimated from samples obtained from repeated
sample surveys of a population with mean parameter value 50.0 (“j”). Note that 19 of these
conﬁdence intervals contain the mean parameter, as is expected.
1.3
OVERVIEW OF SOME SOLUTION STRATEGIES
7

For case study 1, a decision protocol should be speciﬁed in advance, before the
sample data are collected. For example, one such decision protocol would be to
compare the estimate uˆ obtained from the sample data with the critical threshold
level uc. If the estimate is above (or below) the critical threshold level uc, then the
recommended management decision would be “corrective action.” Otherwise, if
the estimate is below (or above) uc, the recommended management decision would
be “no action.”
An alternative decision protocol for case study 1 would be to compare the conﬁ-
dence interval CI with the critical threshold level. If the conﬁdence interval is above
(or below) the critical threshold level, then the recommended management decision
would be “corrective action.” If the conﬁdence interval is below (or above) the critical
threshold level, the recommended management decision would be “no action.” If the
conﬁdence interval overlaps the critical threshold level, the situation would be ambig-
uous and need to be reassessed, perhaps with an additional survey with larger sample
size. The precision of the estimate, the size of the sampling error, would obviously
affect the results. A larger sample size would reduce the sampling error and hence the
size of the conﬁdence interval. Therefore, the population should be sampled with a
sample size large enough to reduce the sampling error so that the conﬁdence interval
will (hopefully!) fall on one side or the other of the critical threshold level.
Other decision protocols could be chosen for case study 1 using this general
approach of sample surveys with parameter estimation and estimation of error. The
important point, however, is that a decision protocol for a sample survey should be
speciﬁed in advance of data collection so that a decision can be made, clearly and
unambiguously, at the end of the survey. In Chapters 2–4 we shall see how a
Bayesian statistical analysis approach can facilitate the use of a decision protocol.
For case studies 2 and 3, estimates and conﬁdence intervals can be used to make
inferences on the abundance of a discrete population, and the population habitat
selection probability of presence or mean abundance, respectively. For further
review of the basic concepts of sampling design and analysis, see Cochran (1977),
Scheaffer et al. (1996), Thompson (1992), Sarndal et al. (1992), Thompson and
Seber (1996), Thompson et al. (1998), Stauffer (1982a, 1982b), Hansen and
Hurwitz (1943), Horvitz and Thompson (1952), and Gregoire (1998).
1.3.2
Experiments and Hypothesis Testing
A second statistical approach to addressing the problems posed by some of the case
studies of Section 1.2 is to conduct an experiment and use frequentist hypothesis
testing, developed by Neyman and Pearson (1928a, 1928b, 1933, 1936) and Fisher
(1922, 1925a, 1925b, 1934, 1958). With case study 1, for example, the scientist
could formulate the null hypothesis
H0 : p ¼ pc
and the one-tailed alternative hypothesis
HA : p , pc,
8
INTRODUCTION

collect experimental data using a rigorous experimental design, and test the null
hypothesis. If the null hypothesis is rejected, the recommended management decision
would be “corrective action.” If the null hypothesis is not rejected, the recommended
management decision would be “no action.” Note, that with this approach, the burden
of proof would be on “corrective action.” If the direction of the alternative hypothesis
is reversed, the burden of proof would be on “no action.” Regardless, the burden of
proof would not be equal for the two hypotheses.
A one-sample z test could be used for the hypothesis testing, with a one-sided
alternative hypothesis. If the null hypothesis is true, the test statistic
zs ¼
^p  pc
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p  (1  ^p)
n  1
r
is standard normally distributed. The Neyman–Pearson hypothesis testing protocol
requires that a type I error a be speciﬁed, with conﬁdence P ¼ 12a (say, a ¼ 5%
and P ¼ 95%), prior to the data collection and analysis, assuming the null hypothesis
to be true. If the test statistic zs, calculated from the experimental dataset, is in the
rejection region, the a percentile left tail of the standard normal distribution (equiv-
alent to p , a), then the null hypothesis would be rejected. Otherwise, the null
hypothesis would not be rejected.
Note again, that the logic of frequentist inference is of the form “if (parameter),
then probability (data).” It assumes the null hypothesis that the parameter is ﬁxed
and provides conditional probability properties for statistics from the experimental
datasets.
To review the basic concepts of experimental design and hypothesis testing,
see Hicks (1993), Kuehl (1994), Dowdy and Wearden (1991), Sokal and Rohlf
(1995), Zar (1996), Winer et al. (1991), Cohen (1988), Siegel and Castellan (1988),
Conover (1980), Daniel (1990), PASS (2002), and nQuery (2002). However,
beware of the overuse and misuse of hypothesis testing in natural resource science,
particularly with observational, rather than experimental, datasets; see Johnson
(1999), Anderson et al. (2001, 2002), Ramsey and Schafer (2002), and Robinson
and Wainer (2002).
1.3.3
Multiple Linear Regression, Generalized Linear Modeling,
and Model Selection
Habitat selection modeling can often be used effectively to address case study 3 of
Section 1.2. Mobile wildlife populations may be difﬁcult to sample directly, but
their responses often vary with habitat attributes that are more easily sampled. For
instance, the response of endangered wildlife species associated with old-growth
habitat may tend to be larger in value in late-seral-stage rather than early-seral-
stage habitat. In such circumstances, it may be useful, more cost-effective, and less
time-consuming to ﬁt and compare habitat selection models that describe the
mobile biological population response as a function of various habitat variables.
1.3
OVERVIEW OF SOME SOLUTION STRATEGIES
9

The objective is to express the wildlife population response as a function of variables
describing the vegetation, geologic, and climatic characteristics of its habitat. If the
population response is continuous with normally distributed error, the population
may be described by a multiple linear regression model.
If, however, the population response measurements are categorical, such as binary
with values of 0 or 1 (e.g., “present” or “absent,” “occupied” or “unoccupied,”
“alive” or “dead,” “yes” or “no”), or discrete with integer values such as plot
counts with error that may not be normally distributed, then it may be possible to
“link” the response measurements to a linear function described by a generalized
linear model (GLM) such as logistic regression.
Multiple linear regression models and generalized linear models can be used to
describe the response values at speciﬁc sites as a function of habitat characteristics.
With such a modeling approach to statistical analysis, model selection strategies
are required to effectively compare models for goodness of ﬁt to sample datasets
and to avoid overﬁtting and compounding of error. The utility of such an approach
depends on the goodness of ﬁt of the best-ﬁtting models to the population and
their predictive accuracy. Chapter 5 includes a basic review of multiple linear
regression and a description of contemporary strategies that can be used for model
selection and inference. To further review the details of multiple linear regression,
see Appendix A at the end of this book, or see Seber (1977), Draper and Smith
(1981), Hocking (1996), Ryan (1997), Cook (1998), and Cook and Weisberg
(1999). Generalized linear modeling is introduced in Chapter 6.
1.3.4
A Preview of Bayesian Statistical Inference
Bayesian statistical analysis and inference provides an important alternative approach
to frequentist statistical analysis and inference for natural resource scientists, yet it has
become practical and accessible for general use only relatively recently. Traditional
frequentist statistical analysis and inference provides probabilities for sample datasets,
based on assumptions for parameters, with an interpretation of results in the context
of repeated surveys or experiments. Although frequentist statistical analysis methods
are well known by natural resource scientists, these methods are often incorrectly
applied with inferences that are frequently misunderstood (Johnson 1999, 2002). If
properly applied and correctly interpreted, frequentist statistical analysis provides rig-
orous standards for inferences: the unbiased and minimum error properties of estima-
tors, the accuracy probabilities of conﬁdence intervals, and the type I and type II
errors of hypothesis testing.
Alternatively, Bayesian statistical analysis and inference provides probabilities for
parameters, based on sample datasets (Iversen 1984, Berger 1985). Inferences from
Bayesian statistical analysis are directly applicable to parameters that are of central
interest to natural resource scientists. Unfortunately, Bayesian statistical inference
has not been of leading interest to a majority of statisticians and practicing natural
resource scientists in the past because its use has been impractical and inaccessible
until very recently (as of 2007). Bayesian statistical analysis and inference requires
an assumption of a prior distribution for the parameters. Using the sample dataset
10
INTRODUCTION

and a likelihood model for the dataset, Bayesian statistical analysis provides a pos-
terior distribution for the parameters, based on the prior distribution, the dataset,
and the model. The posterior distribution thus updates a scientist’s understanding
of the parameters. The Bayesian approach combines previous information about
the parameter with an analysis of the sample dataset to obtain an updated assessment
of the parameters. The posterior distribution provides probabilities for the parameters
that can be useful for natural resource scientists and managers. They can utilize
summary statistics of the posterior distribution, such as the mean, median, mode,
standard deviation, and percentiles, or use the entire posterior distribution itself to
evaluate the parameters. A probability region, the smallest middle interval encom-
passing 95% of the posterior distribution, provides a Bayesian 95% credible inter-
val. This interval can be directly interpreted as the region within which the
parameter is likely to be found, with 95% probability. Thus, with Bayesian statistical
inference, there is no need to interpret the results indirectly as a frequentist does, in
terms of probabilities of datasets with repeated surveys or experiments. The logic
of Bayesian statistical analysis provides probabilities for parameters, given the
data, in contrast to the logic of frequentist statistical analysis, which provides
probabilities for datasets, given the parameter.
Bayesians must, however, bear responsibility for the appropriate selection of
priors and the standards of results. As with frequentist results, Bayesian results
must be assessed for goodness of ﬁt to assess the reliability of model predictions.
Priors inﬂuence posteriors, particularly with small-sample datasets, and must be
chosen judiciously. Until relatively recently, Bayesian solutions to complex problems
were seldom computable. However, owing to a collection of computer simulation
algorithms, the Markov Chain Monte Carlo (MCMC) algorithms developed in the
mid-twentieth century (Bremaud 1999, Carlin and Louis 2000, Congdon 2001,
Gill 2002, Link et al. 2002) and to public-domain software such as WinBUGS
that is now downloadable from the Web (Spiegelhalter et al. 2001), natural
resource scientists now have the resources needed for the practical use of Bayesian
statistical inference.
This book describes and illustrates both frequentist and Bayesian paradigms for
statistical analysis and inference, emphasizing the advantages and disadvantages of
each in particular contexts. It is the practical perspective of this book that the contem-
porary natural resource scientist should be familiar with both. Bayesian statistical
inference is introduced in Chapters 2–4 and applied comparatively, along with fre-
quentist statistical inference, in Chapters 5–7, with other important contemporary
research methods of statistical analysis.
1.3.5
A Preview of Model Selection Strategies and Information-Theoretic
Criteria for Model Selection
With either frequentist or Bayesian statistical analysis approaches, a rigorous and
theoretically justiﬁable approach to model ﬁtting, selection, and inference is required.
Traditionally, with multiple linear regression modeling, analysts have used statistics
such as parameter coefﬁcient estimates and their signiﬁcance, the coefﬁcient of
1.3
OVERVIEW OF SOME SOLUTION STRATEGIES
11

determination R2, the residual standard error syjx, the ANOVA F test, the adjusted R2,
and Mallows’Cp to evaluate the relative ﬁt of models (Seber 1977, Draper and Smith
1981, Hocking 1996, Ryan 1997, Cook and Weisberg 1999). These statistics test
various assumptions of the model ﬁt, such as whether the model is statistically equiv-
alent to the null model. They do not directly assess the issue of whether the model is
the best ﬁtting to the sample dataset. Unfortunately, they also sometimes tend to
overﬁt the model to the sample dataset, with compounding of error. We shall say
more about this later on. We recommend a more modern information-theoretic
approach to model ﬁtting, using Akaike’s information criterion (AIC), the corrected
Akaike information criterion (AICc), or the Bayesian information criterion (BIC) with
frequentist statistical analysis (Burnham and Anderson 1998, 2002), and the deviance
information criterion (DIC) with Bayesian statistical analysis (Spiegelhalter et al.
2001, Carlin and Louis 2000). These criteria provide a more rigorous and theoreti-
cally justiﬁed approach to model ﬁtting that avoids the overﬁtting of models to the
sample dataset and the compounding of error.
Akaike’s information criterion was developed relatively recently by the Japanese
mathematician Hirotugu Akaike (1973, 1974). It is an information-theoretic measure-
ment of the relative Kullback–Leibler distance (KL distance) between a model and
the reality. The Akaike’s information criterion (AIC) is the linear Taylor series
approximation of the relative KL distance, whereas the corrected Akaike infor-
mation criterion (AICc) is a second-order Taylor series approximation. Since
AICc is more precise, we recommend that it be used in preference to AIC, particularly
for datasets with small numbers of samples. The best-ﬁtting model in a collection of
models has the lowest AIC or AICc value.
For any probabilistic statistical model with a likelihood function L (more on
this in Chapters 2, 5, and 6), AIC and AICc are deﬁned using the deviance ¼
D ¼ 22 . log(L) ¼ 22 . l
AIC ¼ D þ 2  k
¼ 2  log (L) þ 2  k
and
AICc ¼ D þ 2  k þ 2  k  (k þ 1)
n  k  1
¼ 2  log (L) þ 2  k þ 2  k  (k þ 1)
n  k  1 ,
where k ¼ the number of parameters in the model and n ¼ the sample size.
The AIC and AICc criteria for multiple linear regression are given by the formulas
AIC ¼ n  log ^s2  n  p  1
n


þ 2  k
¼ n  log ^s2  n  k þ 1
n


þ 2  k
12
INTRODUCTION

and
AICc ¼ n  log ^s2  n  p  1
n


þ 2  k þ 2  k  (k þ 1)
n  k  1
¼ n  log ^s2  n  k þ 1
n


þ 2  k þ 2  k  (k þ 1)
n  k  1 ,
where sˆ ¼ syjx is the residual standard error, p is the number of covariates or expla-
natory variables, and k ¼ p þ 2 is the number of parameters (including the covariates
coefﬁcients, the intercept, and s). For the linear regression model with the parameters
b0, b1, and s, p ¼ 1 and k ¼ 3.
The AICc criterion penalizes a model with too many covariates from overﬁtting
the sample data. It determines the most parsimonious model, the one with an
optimum mix of minimal bias and maximal precision. As the number of parameters
increases, models more closely ﬁt sample datasets, reducing the bias. However, as the
number of parameters per sample increases, the precision of the parameter estimates
tends to decrease. The AICc criterion moderates this process, striking the most
optimal compromise between reduced bias and maximal precision.
The corrected Akaike information criterion measures the amount of noise, or
entropy, in the sample data, separating it from the signal or information. It is a rela-
tive measure of the KL distance between the model and the reality. The absolute
measure of the entropy is the calculated AICc plus a constant. The constant
remains unknown since the reality is unknown. However, since each model has the
same constant, AICcs may be compared to determine the relatively best-ﬁtting
model. The reader should be warned, however, that this ﬁt is relative. Goodness-
of-ﬁt tests must additionally be used in the concluding analysis to assess the absolute
ﬁt of the best-ﬁtting models with the lowest AICcs.
The AICc is most applicable to models of realities that are complex and inﬁnite- or
high-dimensional, as are most natural resource populations. For such complex realities,
ﬁnite-dimensional models will necessarily be at best only an approximation. For reali-
ties that are ﬁnite-dimensional, of fairly low dimension, such as k ¼ 1–5, with k ﬁxed
as the sample size n increases, “dimension-consistent” criteria such as the Bayesian
information criterion are more applicable (Burnham and Anderson 1998, 2002).
The Bayesian information criterion (BIC), developed by Schwarz (1978), also
uses a formula based on the deviance or log likelihood and “penalizes” models for the
overuse of covariates
BIC ¼ D þ k  log (n)
¼ 2  log (L) þ k  log (n):
For multiple linear regression models, BIC is given by
BIC ¼ n  log ^s2  n  p  1
n


þ k  log (n)
¼ n  log ^s2  n  k þ 1
n


þ k  log (n):
1.3
OVERVIEW OF SOME SOLUTION STRATEGIES
13

The BIC is derived using Bayesian assumptions of equal priors for each model and
vague priors on the parameters (Burnham and Anderson 1998), with the objective of
predicting rather than understanding the process of a system. The BIC penalizes more
heavily for increases in the number of parameters and hence sometimes tends to select
models that are underﬁt with excessive bias and precision. For natural resource mod-
eling, most realities are complex and inﬁnite-dimensional; hence, AICc is the more
appropriate criterion for comparing statistical models in the natural resource sciences.
1.3.6
A Preview of Mixed-Effects Modeling
Data collected from monitoring do not always fulﬁll the assumptions of independence
required for the use of many traditional statistical methods: with parameter estimation
in sample surveys, with hypothesis testing in experiments, and with model ﬁtting and
selection in multiple linear regression or generalized linear modeling. Rather, data are
often dependent, clustered or grouped by location or time, or collected from perma-
nent plots or at sites repeatedly over time or from subpopulations of larger popu-
lations (e.g., as with meta-population data). Traditionally, scientists have often
been discouraged from collecting dependent or pseudoreplicated datasets to avoid
these problems with the analysis. But dependencies in biological populations are
quite common, and it may be difﬁcult or impossible to avoid collecting data with
such dependencies. It would make more sense to collect data with such dependencies
and account for them in the analysis. This is now possible with mixed-effects mod-
eling, which incorporates both traditional ﬁxed effects describing the inﬂuences of
treatments on the population and random effects describing dependencies in the
data created by groupings. This is achieved in an efﬁcient manner with mixed-
effects modeling, incorporating variance components into the models to describe
the random effects due to the clusters. We will provide an introduction to mixed-
effects modeling using the powerful utilities now available in S-Plus and R for
frequentist analysis and WinBUGS for Bayesian analysis in Chapter 7.
1.4
REVIEW: PRINCIPLES OF PROJECT MANAGEMENT
In this section of this introductory chapter, we remind the reader of the critical import-
ance in a natural resource data collection project of practicing the principles of sound
project management. A data collection project consists of three phases: a planning
phase at the beginning, a data collection phase in the middle, and a concluding
phase at the end.
It is very important at the beginning of a project to devote sufﬁcient attention to the
planning phase in order to develop a rigorous and effective statistical design for the
data collection. To properly determine the appropriate statistical design, the problem,
objectives, and methods of analysis for the project must be clearly speciﬁed prior to
data collection. Far too often in natural resource data collection projects, the problem
and objectives are not speciﬁed clearly enough in quantitative terms. If a project, for
example, has the objective of examining the downward trend of a declining species,
14
INTRODUCTION

the amount of downward trend that is biologically important should be speciﬁed in
advance of data collection. Then the sample size for the data collection should be cal-
culated to ensure with a high probability that the biologically signiﬁcant trend will be
detected with statistical signiﬁcance if it exists.
It is unfortunately also far too common in data collection projects to wait until after
the data have been collected to decide on the method of analysis. As the method of
analysis required by the objectives may impose restrictions on the statistical design
of the data collection, it should be determined prior to data collection. The method
of analysis may require a certain amount of precision or power to realize the objec-
tives of the project, and this will require a sufﬁciently large sample or experimental
dataset. Hence, the sample size or numbers of replicates must be determined prior to
data collection.
It is also vitally important to devote sufﬁcient attention to the concluding phase at
the end of a project, to allow time for a comprehensive analysis and thoughtful
interpretation and conclusions. Comprehensive analysis will seldom be “turnkey,”
which can be ﬁnished in a few hours or days, but rather a far more lengthy
process. The analysis process may consist of examining a range of candidate
models to determine the best ﬁtting. This collection of models may be small and
ﬁnite or wide-ranging in number. We shall describe alternative strategies for the
model selection and inference in Chapter 6.
It is tempting in a natural resource data collection project to devote a majority of
the time to data collection. This unfortunately may leave an insufﬁcient amount of
time at the beginning of the project for planning and at the end of the project for a
thorough analysis and thoughtful period of reﬂection on the conclusions and
interpretation of the results. A good principle in general is to spend equal amounts
of effort on all phases of the project. This will help ensure that the results are efﬁcient,
powerful, and effective. The aim is to extract the biological “information” from the
data, separating the “signal” from the “noise” in as optimal a manner as possible.
Time and effort devoted to rigorous design and analysis in a data collection project
are indeed well spent.
1.5
APPLICATIONS
The practical application of theory is where the learning process really crystallizes.
This is particularly true with statistical analysis. We will use a hands-on approach
to emphasize the practical use of statistics with the application of theory. We will
use both simulated and real-world sample datasets as examples and encourage the
reader to do likewise with other datasets.
We encourage readers to analyze their own datasets while progressing through the
book. Readers are especially encouraged to conduct a project while reading this book,
designing, collecting, and analyzing their own sample or experimental datasets.
Address a biologically interesting question, but one sufﬁciently limited in scope that
it can be investigated with a data collection project completed within the timeframe
of the reading of this book. For example, examine the abundance level or trend of a
1.5
APPLICATIONS
15

local biological population of interest such as a bird or plant population. Write a
2–5-page proposal for the project. Keep the project small and realistically simple;
be especially careful to keep the scope of the project within a realistic timeframe.
We will use simulated datasets to illustrate many of the ideas. The reader will thus
be able to compare the statistical results with the known “realities” used to generate
the simulated datasets. We will also provide real-world datasets for the analyses of
important species such as the Northern Spotted Owl, Siskiyou Mountains salaman-
der, and beach layia, some of which are endangered. These datasets will provide prac-
tical, realistic experience. The interested reader can also obtain additional sample and
experimental datasets on the Web and in many other standard references, such as
Sokal and Rohlf (1995), Zar (1996), and Ramsey and Schafer (2002). These refer-
ences contain excellent examples, problems, and datasets.
1.6
S-Plusw AND R ORIENTATION I: INTRODUCTION
1.6.1
Orientation I
In this section, we provide an introductory orientation to the statistical software used
for the frequentist analysis throughout the book, the research-oriented proprietary
statistical modeling software S-Plusw (2000) and the de facto equivalent freeware
R (R Development Core Team 2005). S-Plus was ﬁrst developed in the 1970s at
Bell Labs by Rick Becker, John Chambers, and Allan Wilks with the goal of deﬁning
a language to perform repetitive tasks in data analysis. These authors did not consider
the original language to be primarily statistical and most of the statistical functionality
was added later. S-Plus provides a ﬂexible, interactive, integrated modern environ-
ment for statistical analysis, with particular emphasis on the modeling of linear
systems. Much of the syntax of S-Plus is reminiscent of the Unix and C environments.
The current version of R is the result of a collaborative effort with contributions
from all over the world. The R language was initially written by Robert Gentleman
and Ross Ihaka of the Statistics Department of the University of Auckland. It is
freeware, the de facto equivalent of S-Plus.
Details on S-Plus can be found in the S-Plus Help menu, the S-Plus Manual
(S-Plus 2000), or Krause and Olson (2000). Details on R can be found in the R
Help menu. S-Plus and R are object-oriented languages with datasets and functions
consisting of objects. Objects can be created and manipulated by the user and added
to the standard library of defaulted objects that are available in S-Plus and R. To begin
this introductory S-Plus–R session, the user should sign onto either S-Plus or R. In
S-Plus, the user should enter the Commands Window mode from the Window menu
to begin at the > prompt. In R, the user should begin at the > prompt in the R Console.
Although S-Plus and R do have some menu features, the S-Plus command mode and
R Console options will be emphasized throughout this book, leaving it to readers to
explore the menu options. We will present frequentist software code that is sufﬁ-
ciently general for use in either S-Plus or R and will be sufﬁciently careful to
point out where there are differences.
16
INTRODUCTION

1.6.2
Simple Manipulations
Let’s begin by illustrating simple data manipulation capabilities in S-Plus and R,
starting with arithmetic using +, -, *, /, and ^ for addition, subtraction, multipli-
cation, division, and exponentiation, respectively. Proceed with the following code
(Fig. 1.3a), typing the ﬁrst line, pressing the <ENTER> key, receiving the second
line response from S-Plus or R, typing the third line and pressing the <ENTER> key
to receive the fourth-line response, and so on. The index [1] at the beginning of
each response line refers to the ﬁrst indexed entry of the arithmetic operation
response, a vector of size 1.
For the next illustration, the c concatenation, rep repetition, 1 : k integer
sequence, and seq general sequence commands aid in constructing data objects in
Figure 1.3. Command code for S-Plus and R Orientation I. (a) Simple manipulation: arith-
metic; (b) Simple manipulations: creation of vectors; (c) Simple manipulations: object
removal; (d) Data structures: vectors, data frames, lists; (e) Data structures: testing; (f) Data
structures: coercion; (g) Random numbers: normal distribution; (h) Random numbers:
sampling; (i) Directory structure; (j) Functions and control structures: functions code. (k)
Functions and control structures: functions execution; (l) Linear regression analysis.
1.6
S-Plusw AND R ORIENTATION I: INTRODUCTION
17

Figure 1.3. Continued.
18
INTRODUCTION

Figure 1.3. Continued.
1.6
S-Plusw AND R ORIENTATION I: INTRODUCTION
19

S-Plus and R (hereafter we omit reference to the <ENTER> key at the end of each input
command where this is clear from the context) (Fig. 1.3b). All of these data object
reponses are vectors of values, starting with the ﬁrst entry indexed by [1] at the
beginning of the output line. S-Plus and R are case-sensitive. Use the Help
menu Search S-Plus Help in S-Plus or the Help menu R functions
(text) . . . option in R to learn more about the commands and their syntax.
The <- operation assigns values to objects, and the rm command removes or
deletes objects in both S-Plus and R (Fig. 1.3c). In S-Plus, the underscore _ can
be substituted for <- to indicate assignment. The list of objects currently available
at the top-level directory of S-Plus and R can be viewed by using the
objects() command.
Figure 1.3. Continued.
20
INTRODUCTION

1.6.3
Data Structures
In S-Plus and R, datasets are organized into simple data structures of type numeric
for quantitative or numerical values, factor for qualitative or categorical values,
character for character strings, and logical for objects with logical values true T
or false F. These simple atomic types can be combined into complex data structures
such as one-dimensional vectors of simple values of the same type, two-dimensional
matrices of columns of vectors of the same type and length, two-dimensional data
frames with columns of vectors of possibly different types but the same size, and lists
of simple and complex types of varying size. Vectors are matrices, matrices are data
frames, and data frames are lists, but the converse is rarely true. Let’s combine simple
values into a vector with the concatenate c command, vectors into matrices with the
matrix command, column bind cbind command, or row bind rbind command,
vectors into data frames with the data.frame command, and data structures into
lists with the list command (Fig. 1.3d). Notice how the row entries in the
matrix m1 are indicated by [1,], [2,], and [3,]; the column vector entries in the
data frame d1 are indicated by v1 and v3; and the entries in the list list1 are indi-
cated by [[1]] and [[2]]. The type of values in data structures can be tested with
the is.numeric, is.character, is.logical, is.vector, is.matrix,
is.data.frame, or is.list commands (Fig. 1.3e). Data structures can
sometimes be coerced into other types of more complex objects and values
with as.matrix to convert a vector into a matrix, as.data.frame to convert
a matrix into a data frame, as.list to convert a data frame into a list, and
as.character to convert a numeric vector into a character vector (Fig. 1.3f).
1.6.4
Random Numbers
Probability density, cumulative probability, quantile, and random values can be gen-
erated in S-Plus and R by using the d, p, q, and r preﬁxes with common probability
distributions such as the normal, uniform, gamma, exponential, t, F, chi-square (x2),
lognormal, Poisson, negative binomial, and binomial distributions using norm,
unif, gamma, exp, t, f, chisq, lnorm, pois, nbinom, and binom sufﬁxes
and their speciﬁed parameters (Fig. 1.3g). For example, in the ﬁgure, y is a vector
with 10 numeric values randomly sampled from the normal distribution N(5, 1)
with mean m ¼ 5 and standard deviation s ¼ 1. The cumulative probability at 4,
quantile of 0.1586553, and density value at 5 for N(5, 1) are also calculated.
The sample command can be used to generate random data from a vector,
without or with replacement (Fig. 1.3h). The default of this command is to sample
without replacement.
1.6.5
Graphs
Graphs can be obtained for numeric values using the dotplot (stripchart
in R) and hist commands on vectors, plot command on pairs of vectors,
and pairs command on matrices or data frames of columns. You can also create
1.6
S-Plusw AND R ORIENTATION I: INTRODUCTION
21

an
n  m
trellis
of
graphs
with
n
rows
and
m
columns
using
the
par(mfrow=c(n,m)) command.
1.6.6
Importing and Exporting Files
Data ﬁles can be imported or exported as objects using the Import Data and
Export Data options in the File menu in S-Plus. Data ﬁles are imported
as data frames unless speciﬁed otherwise. In R, data can be imported and exported
using copy and paste commands in the data ﬁle and the Data editor
option in the Edit menu. Alternatively, in R, text ﬁles of data with the txt exten-
sion data.txt from the defaulted R-2.2.1 folder can be imported using the
read.table(“data.txt”) command.
1.6.7
Saving and Restoring Objects
You can save and restore your directory of objects or individual objects in S-Plus
by using data.dump(objects(), “directory and ﬁlename”) or data.
dump(c(“object1”,“object2”, . . .), “directory and ﬁlename”)
commands and data.restore(“directory and ﬁlename”) commands.
Alternatively, in S-Plus, you can use the Workspace Save and Workspace
Open options in the File menu. In R, you can save and restore your directory of
objects by using the Save Workspace and Load Workspace options in the
File menu.
1.6.8
Directory Structure
The S-Plus and R directory structures are hierarchical in structure and can be viewed
using the search() command (Fig. 1.3i). The directory of an object can be deter-
mined using the ﬁnd (object) command. An object such as a data frame or list
can be opened and closed, with internal objects such as vectors made available at
a speciﬁed directory level (defaulted to level 2), by using the attach(object,
level) and detach(level) commands. The # symbol in a line of code
indicates to S-Plus or R that the remainder of the command is a comment.
1.6.9
Functions and Control Structures
Function subprograms can be created as objects using the ﬁx(name of object)
command. If a mistake is made in creating and editing the subprogram object, an
error message will be issued. Use the ﬁx() command to return to editing to
correct the mistake in the object before signing off from S-Plus or R. Otherwise
the function object will not be saved. The standard programming control structures
are available in S-Plus and R: (1) sequential; (2) conditional, with if (test)
then fg else fg syntax; (3) repetition, with for (comparison or name in
values) expr or for (comparison or name in values) expr else
22
INTRODUCTION

expr syntax; and (4) subprograms. Objects enter subprogram as parameters in
the initial function(parameters) statement and exit in a ﬁnal return
(objects)statement. Braces fg are used to delineate blocks of code. We illustrate
with a function add that adds the values in the vector x (Fig. 1.3j). This function add
could be created using the ﬁx(add) command and executed (Fig. 1.3k).
1.6.10
Linear Regression Analysis in S-Plus and R
We conclude this introductory section on S-Plus and R by illustrating the use of linear
regression modeling, with vectors of independent values x and dependent response
values y (Fig. 1.3l). The sample data (x, y) were simulated from a “reality” using
the linear relationship y ¼ 10þ1.5 . x with normal errors e  N(0, 1) having mean
m ¼ 0 and standard deviation s ¼ 1. The output results of the linear regression analy-
sis are statistically compatible with the simulated reality (Fig. 1.4).
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
We encourage the reader at this point to review the most important distributions that
are fundamental to statistical analysis: the uniform and normal distributions for con-
tinuous data, the Poisson distribution for count data, and the binomial distribution for
binary data. In this section, we review these distributions in S-Plus and R.
1.7.1
Uniform Distribution
Let us start with an example where we generate n ¼ 30 randomly located plots in a
forest stand illustrating the application of the uniform distribution. This can be
Figure 1.4. Linear regression model ﬁt to the sample dataset y ¼ 10þ1.5 * xþ error where
error  N(0,1) with x  Unif (2,8) and n ¼ 20 samples.
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
23

accomplished by ﬁrst circumscribing the forest stand with a rectangular area, say,
m1  m2 ¼ 200  100 units in size. Then we use S-Plus or R to generate n0 plot
centers (xi, yi), i ¼ 1, 2, . . . , n0, within this 200  100 rectangular area that circum-
scribes the forest stand. We choose n0 . n large enough so that n ¼ 30 of the
plots will be within the forest stand subarea, say, n0 ¼ 50. Then we select the ﬁrst
n ¼ 30 plot centers from the list of n0 ¼ 50 that are within the forest stand subarea
of the rectangular area. To determine the plot locations, generate n0 ¼ 50 xi
values from the uniform distribution Unif(x; 0, 200) with limits 0 and 200 and
n0 ¼ 50 yi values from the uniform distribution Unif(y; 0, 100) with limits 0–100
(Fig. 1.5a). Histograms of the x and y samples and the plot locations are displayed
in Figs. 1.6a–c. The histograms of the samples from the uniform distributions
are not, of course, entirely ﬂat because of the small sample size. Generate
samples from other uniform distributions and examine their distribution with the
hist command. The uniform distribution is a constant, ﬂat distribution with
an equal probability of sampling any value within its range of limits, so it is a
useful distribution for generating simple random sample locations of values from con-
tinuous variables. The uniform distribution Unif(y; u1, u2) of the continuous random
variable y has two parameters, u1 and u2, that deﬁne the limits of the range of y
(Fig. 1.6d).
1.7.2
Normal Distribution
Next, we illustrate the normal distribution N(y; m, s) of the continuous random
variable y with the two parameters m, the mean average, and the standard deviation
s, by generating a simulated sample from a population of tree diameters in a com-
munity forest. Let’s assume that this population consists of even-aged tree diameters
that are normally distributed with mean 3000 and standard deviation 500. Begin by
simulating a large sample of 10,000 measurements and looking at the histogram
that approximates closely that of the entire population distribution. Then generate
a sample of size n ¼ 50 (Fig. 1.5b). Our analysis ﬁrst examines the histogram of
a large sample that approximates the population (Fig. 1.6e). Then it examines the
values at the 2.5 and 97.5 percentiles of this large sample dataset and compares
these sampled percentiles with the actual theoretical quantiles of the population
using the qnorm command. It also calculates the cumulative probabilities 2 and 1
standard deviations away from the mean (i.e., at diameter at breast height dbh ¼
20, 40, 25, and 35, respectively) using the qnorm command. Finally we plot the
dbh normal distribution N(dbh ; 30,5) within the range [20, 40] using the dnorm
command (Fig. 1.6f).
We encourage the reader to generate samples from other normal distributions and
look at their sample histograms, quantiles, cumulative probabilities, and density
curves. Many continuous natural resource datasets are approximately normal.
Recall also that the Central-Limit Theorem states that, if the data are normally dis-
tributed, the mean estimates, with repeated sampling, are also normally distributed.
24
INTRODUCTION

Figure 1.5. Command code for S-Plus and R Orientation II: distributions. (a) Uniform distri-
bution. (b) Normal distribution. (c) Poisson distribution. (d) Binomial distribution: Bernoulli
trials. (e) Binomial distribution: binomial trials. (f) Binomial distribution: other examples.
(g) Binomial distribution: yet other examples. (h) Simple random sampling.
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
25

1.7.3
Poisson Distribution
Next, we examine the Poisson distribution Pois(y ¼ count;l) of the discrete random
variable y with mean average parameter l for count data of populations that are ran-
domly dispersed. We start by examining Poisson data with a mean average parameter
Figure 1.5. Continued.
26
INTRODUCTION

of l ¼ 0.5 (Fig. 1.5c). The Poisson distribution is deﬁned for the discrete random
variable that assumes nonnegative integer values only, and the analysis examines it
for the ﬁrst 31 y ¼ count values, 0, 1, 2, . . . , 30. For small values of the parameter,
such as l ¼ 0.5, the Poisson distribution has an “exponentially declining” shape, with
a mode at the origin, asymptotically approaching the ordinal axis as count 2. 0. As
the mean average parameter l increases, the Poisson becomes unimodal with mode at
the mean or l21, again asymptotically approaching the ordinal axis as count 2. 0
(Figs. 1.6g–j).
The Poisson distribution models count data for species that are randomly distrib-
uted spatially (Pielou 1969, Hilborn and Mangel 1997, Rice 1995). The Poisson
distribution Pois(y ¼ count; l) is characterized by one parameter only, the mean
l . 0. The variance s2 of the Poisson distribution is equal to its mean l: s2 ¼ l.
If the species is aggregated, or clustered spatially, as is often the case for natural
resource data, the variance is greater than the mean, s2 . m, and the random variable
y ¼ count is better modeled with the negative binomial distribution. The negative
binomial distribution NB(y ¼ count; n, a) for the discrete random variable y has
two parameters, n and a, where the mean average m ¼ n/a and the variance s2 ¼
n/a þ n/a2 ¼ m þ m2/n (Hilborn and Mangel 1997). If the species is regularly or
uniformly distributed spatially, the variance is less than the mean, s2 , l. The
expression I ¼ s2/m is sometimes used as an index of nonrandomness for count
data (Pielou 1969). Hence I , 1, I ¼ 1, or I . 1 depending on whether the
population is regular, random, or aggregated, respectively.
1.7.4
Binomial Distribution
We conclude this section by examining the binomial distribution. The binomial
distribution B(y; n, p) is the probability distribution of the discrete random variable
y ¼ the number of successes in a binomial experiment consisting of a sequence of n
independent Bernoulli trials, each of which has two possible outcomes, “success”
or 1, and “failure” or 0, with probabilities p and q ¼ 12p, respectively. Note that
the random variable can assume the values y ¼ 0, 1, 2, . . . , n. A Bernoulli trial is
a binomial experiment with n ¼ 1 trial. First we look at an example in S-Plus
and R of a binomial experiment with n ¼ 30 and p ¼ 0.40 (Fig. 1.5d). This binomial
experiment was simulated in S-Plus and R as a sequence of n ¼ 30 independent
Bernoulli trials. The analysis calculated the random variable value y as the sum
of the Bernoulli outcomes, that is, the number of successes. The analysis also cal-
culated the proportion estimate
phat ¼ ^p ¼ y=n ¼ 0:33
from the sample. This estimate pˆ ¼ 0.33 approximates the parameter value p ¼ 0.40
for the population.
Next, we look at the distribution of the binomial random variable y itself in the
binomial experiment B(y; n ¼ 30, p ¼ 0.40), simulating 10,000 such experiments
(Fig. 1.5e). Note that the distribution looks bell-shaped (Fig. 1.6k). It can be
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
27

Figure 1.6. Simulated randomly generated data from distributions, with S-Plus and R code.
(a) Histogram of sample data x from the uniform distribution Unif(x; 0, 200) with parameter
limits 0 and 200 and sample size n = 50 from the code
> x <- runif(50,0,200)
> hist(x)
(b) Histogram of sample data y from the uniform distribution Unif(y; 0, 100) with parameter
limits 0 and 100 and sample size n ¼ 50 from the code
> y <- runif(50,0,100)
> hist(y)
(c) Plot of randomly sampled (x, y) plot centers in a 200100 area from plots (a) and (b) above
from the code
> plot(x,y)
28
INTRODUCTION

Figure 1.6. Continued.
(d) Density function of the uniform distribution Unif(y; u1 ¼ 0, u2 ¼ 200).
(e) Histogram of n ¼ 10,000 tree diameters (dbh values) sampled in S-Plus or R from a normal
distribution N(dbh; m, u) with mean m ¼ 30 and standard deviation s ¼ 5 from the code
> dbh <- rnorm(10000,30,5)
> hist(dbh)
(f) Density curve of the tree diameter (dbh) normal distribution N(dbh; m, u) with mean m ¼ 30
and standard deviation u ¼ 5 from the code
> dbh <- seq(20,40,0.05)
> normal <- dnorm(dbh,30,5)
> plot(dbh,normal)
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
29

shown that the binomial distribution is approximately normally distributed if the
following conditions hold:
1. n . p5
2. n . q  5.
The mean of y is
m ¼ n  p ¼ 30  0:40 ¼ 12
and the variance is
s2 ¼ n  p  q ¼ 30:0  40  0  60 ¼ 7:2:
Figure 1.6 Continued.
(g) Distribution of the count data Poisson distribution P(count; l) with mean l ¼ 0.5 from the
code
> count <- 0:30
> poisson1 <- dpois(count,0.5)
> plot(count,poisson1)
(h) Distribution of the count data Poisson distribution P(count; l) with mean l ¼ 1.0 from the
code
> count <- 0:30
> poisson2 <- dpois(count,1.0)
> plot(count,poisson2)
30
INTRODUCTION

The mode of y occurs close to the mean since the distribution is approximately
normal and symmetric. If p ¼ 0.50, the binomial distribution is symmetric. Let’s
look at some other examples of binomial distributions (Figs. 1.5f and 1.6l–n).
Note that the distributions are skewed for B(y; 30, 0.10) with p ¼ 0.10 and
B(y; 30, 0.90) with p ¼ 0.90, and symmetric for B(y; 10, 0.50) with p ¼ 0.50.
They are nonnormal for B(y; 30, 0.10) since n . p ¼ 3 , 5 and B(y; 30, 0.90)
since n . q ¼ 3 , 5. Note also that the modes are close to the distribution means
m ¼ n . p ¼ 3, 27, and 5, respectively, at the means if the distribution is symmetric
and toward the tail if the distribution is skewed.
Figure 1.6. Continued.
(i) Distribution of the count data Poisson distribution P(count; l) with mean l ¼ 3.0 from the
code
< count c <- 0:30
< poisson3 <- dpois(count,3.0)
< plot(count,poisson3)
(j) Distribution of the count data Poisson distribution P(count; l) with mean l ¼ 5.0 from the
code
< count <- 0:30
< poisson4 <- dpois(count,5.0)
< plot(count,poisson4)
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
31

Figure 1.6. Continued.
(k) Histogram of 10,000 samples in S-Plus or R from the binomial distribution B(y;n,p) with
n ¼ 30 and p ¼ 0.40 from the code
< y <- rbinom(10000,30,0.40)
< hist(y)
(l) Histogram of 10,000 samples in S-Plus or R from the binomial distribution B(y;n,p) with
n ¼ 30 and p ¼ 0.10 from the code
< y <- rbinom(10000,30,0.10)
< hist(y)
(m) Histogram of 10,000 samples in S-Plus or R from the binomial distribution B(y;n,p) with
n ¼ 30 and p ¼ 0.90 from the code
< y <- rbinom(10000,30,0.90)
< hist(y)
32
INTRODUCTION

The interested reader can further investigate the binomial distribution by looking at
density functions such as B(y; n ¼ 20, p ¼ 0.70) and examining samples of realistic
size, simulating either a sequence of Bernoulli trials or one binomial experiment
(Figs. 1.5g and 1.6o).
1.7.5
Simple Random Sampling
Let’s look at one ﬁnal example in S-Plus and R to conclude this section, one that
illustrates an additional application of random sampling. Let’s randomly select a
sample of size n ¼ 30 from a ﬁnite frame of sampling units for a population labeled by
integers from 1 to N ¼ 1000 (Fig. 1.5h). The sampling is without replacement. To
samplewith replacement, use the command sample (frame, 30, replace ¼ T).
Figure 1.6. Continued.
(n) Histogram of 10,000 samples in S-Plus or R from the binomial distribution B(y;n,p) with
n ¼ 10 and p ¼ 0.50 from the code
< y <- rbinom(10000,10,0.50)
< hist(y)
(o) Density function of the binomial distribution B (y;n,p) with n ¼ 20 and p ¼ 0.70 from the
code
< y <- 0:20
< binomial <- dbinom(y,20,0.70)
< plot(y,binomial)
1.7
S-Plus AND R ORIENTATION II: DISTRIBUTIONS
33

1.8
S-Plus AND R ORIENTATION III: ESTIMATION OF MEAN
AND PROPORTION, SAMPLING ERROR, AND CONFIDENCE
INTERVALS
1.8.1
Estimation of Mean
This chapter continues with further S-Plus and R examples, illustrating its use for the
estimation of parameters. Let’s ﬁrst simulate the simple random sampling of data fyig
from a normally distributed population y  N(m ¼ 20, s ¼ 5) with a sample size of
n ¼ 30 and calculate a conﬁdence interval for the mean average parameter (Figs. 1.7a
and 1.8). Notice the proximity of the estimates y¯ ¼ 20.73598 and s ¼ 2.09667 to
the parameters m ¼ 20 and s ¼ 2 that they are estimating. This sample dataset
produces one of the 95% CIs that correctly includes the mean parameter m ¼ 20.
Figure 1.7. Command code for S-Plus and R Orientation III: Estimation of mean and pro-
portion, sampling error, and conﬁdence intervals. (a) Estimation of mean. (b) Estimation of
proportion.
34
INTRODUCTION

The t.test(y)$conf.int command in S-Plus and R also can be used to calcu-
late 95% conﬁdence intervals of simple random samples fyg. To use an alternative
conﬁdence level P in the command, such as P ¼ 0.90, insert the conf.level ¼
P option in the command (e.g., t.test(y,conf.level=0.90)$conf.int).
Figure 1.7. Continued.
Figure 1.8. Histogram of continuous sample data y  N (m ¼ 20, s ¼ 5) with sample size
n ¼ 30.
1.8
S-Plus AND R ORIENTATION III
35

1.8.2
Estimation of Proportion
Next let’s simulate the simple random sampling of binary data fyig from a binomial
distribution B(y; n ¼ 30, p ¼ 0.20), where y ¼ Syi with a sample size of n ¼ 30
(Fig. 1.7b and 1.9). Note here the relatively large amount of sampling error E ¼
8.9% for the estimate pˆ, due to the relatively small sample size n ¼ 30 for the pro-
portion estimator. The conﬁdence interval from this sample includes the parameter
p ¼ 0.20, as is expected of 19 out of 20 conﬁdence intervals, with repeated SRS
sampling of the population.
1.9
S-Plus AND R ORIENTATION IV: LINEAR REGRESSION
To concludethisintroductorychapter,let’s illustratelinear regression modelinginS-Plus
and R with an example. Generate a sample of size n ¼ 80 from a linear “reality”
y ¼ b0 þ b1  x þ e ¼ 50 þ 1:2  x þ e,
where the error e  N(m ¼ 0, s ¼ 20) is normally distributed, and ﬁt it with a linear
regression model (Figs. 1.10a and 1.11).
The estimated statistics from the analysis are as follows:
1. The estimates of the constant and slope coefﬁcients b0 ¼ 50 and b1 ¼ 1.2
respectively are bˆ 0 ¼ 53.8435 and bˆ 1 ¼ 1.1893. Both are highly signiﬁcant
with p , 0.00005. The 95% conﬁdence intervals, based on these estimates
and the estimated standard errors, both contain the respective parameters.
2. The coefﬁcient of determination is R2 ¼ 0.9288.
3. The residual standard error is syjx ¼ 20.33, which is close to the parameter s ¼
20 that it is estimating.
4. The ANOVA F statistic Fs ¼ 1018 is highly signiﬁcant with p , 0.00005.
5. AIC ¼ 712.9258.
Figure 1.9. Histogram of binary sample data y  B(n ¼ 30, p ¼ 0.20) with sample size
n ¼ 30.
36
INTRODUCTION

Figure 1.10. Command code for S-Plus and R Orientation IV: Linear Regression. (a) Linear
regression model, with a constant; (b) Linear regression analysis, without a constant;
(c) Comparison of models.
1.9
S-Plus AND R ORIENTATION IV: LINEAR REGRESSION
37

We compare this full linear model with two other more parsimonious models, the null
model without a linear term and the ratio model without a constant (Fig. 1.10b). Both
of these models have coefﬁcient estimates that are at odds with reality, bˆ 0 ¼ 155.3716
for b0 ¼ 50 for the null model and bˆ 1 ¼ 1.6070 for b1 ¼ 1.2 for the ratio model.
The R2 statistic R2 , 0.00005 is very low for the null model, the syjx statistics
syjx ¼ 75.70 and 37.41 are high and far from the reality s ¼ 20, and the AICs ¼
922.3147 and 809.5328 are much larger than that of the full model. The conclusion
is that the full linear regression model is the best ﬁtting model, with a high R2, lower
syjx, and lower AIC, as expected. The nested ﬁrst and second models and ﬁrst and
third models are also compared in S-Plus and R with an ANOVA F test (Seber
1977, Draper and Smith 1981, Hocking 1996, Ryan 1997, Cook and Weisberg
1999) (Fig. 1.10c). The second and third models cannot be compared with the F
Figure 1.10. Continued.
Figure 1.11. Scatterplot of the (x, y) sample dataset generated from the reality y ¼ 50 þ
1.2 . x þ e, where e  N (m ¼ 0, s ¼ 20) and sample size n ¼ 80.
38
INTRODUCTION

test since they are not nested. The ANOVA F-test results indicate that the full linear
regression model is a statistically signiﬁcantly improvement compared to the other
two models. The second and third nonnested models can, however, be compared
using AIC, which indicates that the ratio model is better, ﬁtting than the constant
null model (AICmodel3 ¼ 809.53 vs. AICmodel2 ¼ 922.31).
1.10
SUMMARY
We began this chapter by introducing three case studies of fundamental general
importance to natural resource scientists. These case studies provide a framework
for the methods of solution presented throughout this book. The ﬁrst case study
posed the problem of maintaining a population parameter above a critical threshold
level. The parameter could be a proportion parameter such as the proportion of a
timber ownership that is occupied by nesting pairs of Northern Spotted Owls. Or,
it could be other parameters of interest such as mean abundance m, survival rate fi
from year i to year i þ 1; ﬁtness li ¼ Niþ1/Ni, where Ni (Niþ1) is the population
abundance in year i (iþ1); ecological diversity index H, such as the Shannon–
Wiener diversity index; or population total t. The second case study posed the
problem of estimating discrete population abundance of a biological population
such as a bird, amphibian, or mammal population. The third case study posed the
problem of habitat selection modeling of a mobile biological population such as an
endangered bird, amphibian, or mammal population. The habitat can be described
by vegetation, geologic, and climatic attributes.
We then presented a review and preview of solutions to the problems introduced
by the case studies: a review of traditional methods such as the estimation of par-
ameters and error, hypothesis testing, and multiple linear regression modeling; and
a preview of more contemporary methods such as Bayesian statistical inference,
model selection and inference, generalized linear modeling, and mixed-effects
modeling. These methods provide contemporary natural resource scientists with a
statistical toolbox of methods of solution to tackle many of their most important
and challenging current problems. The remainder of this book focuses on descriptions
and illustrations of these contemporary methods.
We emphasize the importance of the planning and concluding phases of natural
resource data collection projects, and particularly the critical role of statistical
design and analysis. The datasets used throughout the book consist of both simulated
datasets that allow comparison of statistical results with known realities and real-
world datasets that provide realistic practical experience. The book encourages a
hands-on approach to the use of statistics, including examples and problems in
each chapter as topics are introduced. This ﬁrst chapter concluded by providing an
introduction to S-Plus and R, the statistical software tool for the frequentist statistical
analysis applications throughout the book. WinBUGS, the statistical software tool for
Bayesian analysis, will be introduced in later chapters.
1.10
SUMMARY
39

PROBLEMS
1.1
Estimation and conﬁdence interval solutions for the mean parameter: case
study 1. Conduct frequentist statistical analysis in S-Plus or R of the tree diam-
eter data (i.e., dbh ¼ diameter at breast height) d ¼ fdig of sample size n ¼ 30
in Fig. 1.12 collected using simple random sampling. Calculate an estimate d¯ of
the mean tree diameter d of the forest from this simple random sample along
with the standard deviation s, standard error SE, sampling error E, and P ¼
12a ¼ 0.95 ¼ 95% conﬁdence interval CI ¼ [d¯2E, d¯þE]. Use the S-Plus
or R commands mean, stdev (in S-Plus) or sd (in R), and t. test on
the data vector object d to calculate the statistics. Provide frequentist statistical
Figure 1.12. Tree diameter sample data d (diameter at breast height ¼ dbh): simulated with
normal distribution N (d; m ¼ 25.0, s ¼ 3.0).
40
INTRODUCTION

inference statements about the unbiasedness and minimum error properties of
the mean dbh estimate d¯ and the conﬁdence property of the 95% CI. Examine
the histogram of the dbh data d and discuss whether these frequentist inference
properties, based upon the basis normal distribution theory, of the estimate and
the CI are reasonable. Using the threshold mean dbh parameter value dc ¼ 26.0,
provide a decision for the problem described in case study 1 in terms of (a) the
mean dbh estimate d¯ and (b) the conﬁdence interval CI as to whether the mean
dbh d of the forest has exceeded the threshold. Are these decision results (a) and
(b) in conﬂict with each other? If the conﬁdence level of the conﬁdence interval
is reduced to 80%, 67%, or 50%, is decision (b) above altered? Why are
these conﬁdence levels particularly of interest? How do these reductions of con-
ﬁdence levels affect the error rates of the results? How do you explain the
differing results of the decisionmaking? In Chapter 5 we will demonstrate
how one might use habitat variables x to more effectively model normal data
y with multiple linear regression modeling.
1.2
Estimation and conﬁdence interval solutions for the proportion parameter: case
study 1. Examine the presence–absence data of sample size n ¼ 60 with y ¼ 34
ones (1s) and (n 2 y) ¼ 26 zeros(0s) in sample plots collected using simple
random sampling. Conduct an analysis of this dataset using the binomial
model y  B(y; n, p). Calculate the estimate pˆ ¼ y/n of the proportion p of
the habitat occupied by the endangered species. Calculate the standard error
SE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p  (1  ^p)
n  1
r
,
sampling error E ¼ t12a/2, n21 . SE, and P ¼ 12a ¼ 95% conﬁdence interval
CI ¼ [ pˆ2E, pˆþE]. Provide frequentist statistical inference statements about
the properties of the proportion estimate pˆ and the 95% CI. Using the threshold
proportion parameter value pc ¼ 65%, provide a decision for the threshold
problem described in case study 1 based on (a) the proportion estimate pˆ and
(b) CI for the proportion parameter p. Are these decision results (a) and (b) in con-
ﬂict with each other? If the conﬁdence level of the conﬁdence interval is reduced
to 80%, is decision (b) above altered? If the conﬁdence level is reduced to 50%, is
decision (b) altered? How do these reductions of conﬁdence levels affect the error
rates of the results? How do you explain the differing results of the decisionmak-
ing? In Chapter 6 we will demonstrate how one might use habitat variables x to
more effectively model binomial data y with generalized linear modeling.
1.3
Hypothesis testing solutions: case study 1. Conduct Neyman–Pearson hypoth-
esis testing on the sample tree diameter dataset d ¼ fdig in Fig. 1.12. Use the
null hypothesis
H0 : d ¼ 26:0
PROBLEMS
41

and one-sided alternative hypothesis
HA : d , 26:0
with a conﬁdence level P ¼ 0.95 ¼ 95%. Use the command t.test(d,
mu=dc,alternative=“less”) in S-Plus or R for the hypothesis
testing. Provide a frequentist statistical inference statement for your results. In
terms of the histogram of the data d, discuss whether the frequentist inference
property of the hypothesis testing, based on normal theory, is reasonable. Using
the threshold mean dbh parameter dc ¼ 26.0, provide a decision based on
hypothesis testing for the problem described in case study 1. Do the results
change for conﬁdence levels of 80%, 67%, or 50%? Why or why not? How
do these decisionmaking results, based on one-tailed hypothesis testing, agree
or disagree with those based on the parameter estimation and CIs of Problem
1.1? Explain the differences.
1.4
Bayesian statistical analysis solutions: case study 1. Conduct a Bayesian statisti-
cal analysis on the sample tree diameter dataset d ¼ fdig in Fig. 1.12 as follows.
Suppose that a Bayesian statistical analysis of sample dataset d with a normal
model, based on a uniform prior Unif(d; 0, 60), provides a posteriori
distribution for d that is normal N(d; mpost, spost) with mean mpost ¼ 25.5 and
standard deviation spost ¼ 3.1. Using the threshold mean dbh parameter dc ¼
26.0, provide a decision based on Bayesian statistical inference for the
problem described in case study 1. What is the assessment of risk of an incorrect
decision of whether the threshold has been exceeded with this approach to stat-
istical analysis? How do the results of this approach differ from the frequentist
approaches of Problems 1.1 and 1.3?
1.5
Estimation of population abundance with count data: case study 2. Conduct a
frequentist statistical analysis using S-Plus or R of the bird count data c ¼
fcig in Fig. 1.13. Estimate the mean c¯, standard deviation s, standard error
SE ¼ s/pn, sampling error E ¼ t12a/2, n21 . SE, and 95% conﬁdence interval
CI ¼ [c¯2E, c¯þE]. Examine the histogram of the count data and discuss
whether the frequentist inference properties, based on normal distribution
theory, of the estimate and CI are reasonable.
1.6
Habitat selection modeling with linear regression: case study 3. Conduct habitat
selection modeling analysis of the dataset (x, y) in Fig. 1.14 with independent
predictor habitat variable x and dependent wildlife response variable y using
linear regression modeling. The habitat variable x measures the amount of old-
growth habitat on the sample plot. The response variable y measures the
amount of biomass of an endangered species on the sample plot. Assume that
the data were collected using simple random sampling. Use the linear modeling
command output ,- lm(yx) and summary(output) in S-Plus or R to
conduct the linear regression analysis and examine the results. Examine the
42
INTRODUCTION

Figure 1.13. Count data: Poisson simulated count data with mean l ¼ 1.25 and sample size
n ¼ 50.
PROBLEMS
43

Figure 1.14. Habitat selection modeling data (x, y): simulated with habitat variable x 
Unif(0,100) and wildlife variable y ¼ 30.0 þ 1.2x þ e with e  N(0, 10.0).
44
INTRODUCTION

estimates bˆ 0 and bˆ 1 and t tests of the coefﬁcients of the linear regression model
y ¼ b0 þ b1  x þ e,
where b0 is the y-intercept constant coefﬁcient, b1 is the coefﬁcient of x, and
eN (e; m ¼ 0, s) is the normally distributed error with mean m ¼ 0 and
standard deviation s. Are these estimates statistically signiﬁcant at the 95%
conﬁdence level? Examine the residual standard error estimate syjx of s, the
R-squared statistic, and the F-test statistic. Also examine the AIC of the
modeling results using AIC(output) in S-Plus or R. Do these statistics,
coupled with the plot of the data (use plot(x,y) followed by abline
(bˆ 0, bˆ 1) in S-Plus or R), suggest that the modeling results provide a good ﬁt
of the model to the sample dataset?
PROBLEMS
45


2
Bayesian Statistical Analysis I:
Introduction
The best way to have a good idea is to have lots of ideas.
—Linus Pauling
In this chapter we will present an introduction to Bayesian statistical inference. We
begin with an introduction containing an historical overview and a discussion of
some of the limitations of frequentist statistical inference for natural resource appli-
cations. We then discuss several different approaches to model ﬁtting, concluding
with the Bayesian approach. We discuss the fundamental concepts of Bayesian
statistical inference, including the Bayes’ Theorem for conditional probability.
Bayesian statistical inference uses prior and posterior distributions, along with likeli-
hood functions derived from sample datasets and models, to assess parameters. We
conclude this chapter by discussing the range of options available for choices of
priors, including noninformative priors, Jeffreys’ priors, and conjugate priors.
Conjugate priors provide closed-form mathematical solutions for Bayesian analyses
of many important types of natural resource datasets, such as binary data with the
binomial model, count data with the Poisson model, and continuous data with the
normal model.
2.1
INTRODUCTION
2.1.1
Historical Background
Frequentist statistical inference has been the dominant paradigm for statistical analy-
sis for many years. Parameter estimation, hypothesis testing, and statistical modeling
applications in natural resource sciences have been based primarily on frequentist
statistical analysis, developed by Fisher (1922, 1925a, 1925b, 1934, 1958),
Neyman and Pearson (1928a, 1928b, 1933, 1936), and other leading statisticians
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
47

of the early and midtwentieth century. Bayes’ Theorem, about conditional prob-
ability, provides the foundation for Bayesian statistical inference. Although
Bayes formulated his famous theorem way back in the late eighteenth century,
Bayesian statistical analysis solutions in general have remained elusive, if not intract-
able, for mathematicians until relatively recently. Hence, the practical use of Bayesian
statistical inference in natural resource science has been limited.
Bayesian statistical analysis is derived from the Bayes Theorem, originally pub-
lished posthumously by the Reverend Thomas Bayes in 1763. Gill (2002) comments
that, contrary to the current requirement of “publish or perish,” Bayes “perished
before publishing.” It is doubtful that Bayes realized at that time that mathematical
statisticians would one day develop an alternative approach to statistical analysis
and inference based on his idea. A small group of Bayesian statisticians pursued
this alternative approach in the twentieth century despite a notable lack of enthusiasm
from other statisticians in the frequentist mainstream. Bayesians have, however,
historically been relegated to a minority status at most universities, although a few
hotspots for Bayesians have provided sanctuary, at academic research centers such
as Duke University and University of Minnesota in the United States and Imperial
College in London, England. Until relatively recently, Bayesian statistical analysis
has not been regarded by the vast majority of statisticians as the most reasonable
approach to statistical analysis because of its intractability to solution, its potential
susceptibility to the use of “subjective” prior information, and other less legitimate
reasons. Consequently, Bayesian statistical inference has played a minor role in the
statistical curriculum of most universities and has remained inaccessible to most
natural resource scientists.
However, the advent of computers in the midtwentieth century provided the tech-
nology for a revolution in the development and use of Bayesian statistical analysis.
The mathematician Nicholas Metropolis (Metropolis et al. 1953), working in collab-
oration with the nuclear physicist Edward Teller in the 1950s, developed a Monte
Carlo simulation technique that generates samples in a stationary Markov chain, pro-
viding general Bayesian solutions for statistical problems. Gibbs sampling (Gelfand
and Smith 1990) and an extension due to Hastings (1970) provided further contri-
butions to the general solution. These developments, along with other contributions,
have provided a collection of Bayesian iterative solutions to statistical problems
known as the Markov Chain Monte Carlo (MCMC) algorithms (Gamerman 1997,
Bremaud 1999, Draper 2000, Link et al. 2002). These methods have been pro-
grammed and made available on the Web, as of this printing, as WinBUGS freeware
(Spiegelhalter et al. 2001). WinBUGS is readily accessible and user-friendly for prac-
ticing natural resource scientists. We will introduce these MCMC algorithms and the
use of the WinBUGS software in Chapter 4.
The practical availability of these new tools for Bayesian statistical analysis, along
with a growing realization of the limitations to the use of frequentist methods, provide
natural resource scientists the unique opportunity to use alternative methods for
analyzing challenging datasets. But ﬁrst, let’s review some of the key problematic
issues of concern with the use of frequentist inference for the analysis of natural
resource data. The interested reader is encouraged to explore the Bayesian ideas
48
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

that are introduced in the next three chapters in more depth in the publications by Gill
(2002), Hilborn and Mangel (1997), Iversen (1984), Congdon (2001), Berry and
Strangl (1996), Carlin and Louis (2000), and Link et al. (2002).
2.1.2
Limitations to the Use of Frequentist Statistical Inference for
Natural Resource Applications: An Example
We begin by presenting an example illustrating some of the problems with the use of
frequentist statistical inference for many natural resource applications. The example
will suggest some of the advantages of the alternative use of Bayesian statistical
inference. Suppose that we wish to determine whether there is a difference in the
abundance of a particular critical wildlife species between some old-growth and
clear-cut redwood forest stands in northern California, and have randomly collected
observational data from each type of forest. Recall that observational data, in contrast
to experimental data, are not collected with a completely randomized design.
Practically speaking, the so-called old growth and clear-cut treatments were not ran-
domly applied to randomly chosen experimental units from all forest stand habitat
located throughout northern California, but instead conveniently sampled from
stands that were available. We shall follow Johnson’s (1999, 2002) advice and
avoid the misuse of hypothesis testing with such observational data, instead modeling
it with descriptive analysis and limiting inferences to the speciﬁcally sampled old-
growth and clear-cut redwood forest stands subpopulations, rather than treating
results as predictive and making inferences to a larger population. We note that the
hypotheses for such observational data sampled from speciﬁc stands are statistical
rather than scientiﬁc, and the null hypotheses are “silly” and clearly untrue.
There are some serious disadvantages to the use of frequentist statistical analysis in
this context that the Bayesian approach can remedy. Perhaps the most important dis-
advantage to the use of frequentist statistical analysis for the natural resource scientist
with studies of this type is in the understanding of its inference. It is difﬁcult to visu-
alize the frequentist inference concept of repeatedly sampling such dynamic popu-
lations. The stands in this example are unique, in time and place, and such an
“experiment” is unlikely to be replicable. The nature of many existing biological
populations of interest to natural resource scientists is that they are in ﬂux, endan-
gered, and declining. Conditions are dynamic and will likely never be static. The
dominant current ecological paradigm is one of change and lack of stability.
Frequentist statistical inference, however, provides probabilities for datasets based
on the concept of repeated surveys or experiments in a static population. In natural
resource science, however, such conditions are often unrealistic, even conceptually,
to conceive. Frequentist statistical inference is nonintuitive and confusing to under-
stand in this context. Natural resource scientists and managers view such a study
as a single event in a sequential cumulative process of scientiﬁc exploration and
would prefer to make an inference that is a direct Bayesian probability assessment
for a parameter, based on the sample dataset, rather than make a frequentist prob-
ability assessment for the dataset with repeated sample surveys or experiments,
based upon the parameter.
2.1
INTRODUCTION
49

It is difﬁcult in such a natural resource context to understand the frequentist
meaning of the probability of data. Recall the frequentist interpretation of a 95% con-
ﬁdence interval (CI) in a sample survey. The frequentist inference is that, for a ﬁxed
parameter value, repeated simple random sample surveys of the population will
produce CIs containing the population parameter with a probability of 95%. Recall
the frequentist interpretion of the test statistic that is calculated from the sample
dataset in an experiment. The frequentist inference is that, if the null hypothesis
H0 is true with a conﬁdence level P ¼ 95% and a complementary type I error a ¼
12 P ¼ 5%, repeated experiments of the population will produce test statistics in
the nonrejection region with a probability of 95% and in the rejection region with
a probability of 5%. It is difﬁcult to grasp this frequentist interpretation for the
probabilities of datasets in the context of repeated surveys or experiments when
only one survey or experiment will actually occur in a natural resource study. The
frequentist inference is an indirect assessment for the parameter, a conditional
probability statement for the data with repeated surveys or experiments, given the par-
ameter. What the natural resource scientist requires is a direct assessment for the
parameter, a conditional probability statement for the parameter, given the sample
dataset.
Another major objection to the use of frequentist statistical analysis with natural
resource datasets is the independence of its analysis. Frequentist statistical analysis
usually formally ignores prior information known about a population. This is so
counter to the Western scientiﬁc method, where the assessment of populations is
based on previously accumulated knowledge gained from experience, study, and
experimentation. Natural resource scientists would like to utilize previous studies
and incorporate those results more formally into their analysis. This is particularly
true with the adaptive management and monitoring of endangered species. With
population monitoring, the objective is to proceed sequentially, assessing a popu-
lation and its progress repeatedly, with sample datasets and analysis. The objective
is to utilize an analysis strategy for reassessing the condition of a population in a
sequential, cumulative manner.
There are other drawbacks to the frequentist approach that we need not highlight
here. We refer the interested reader to Johnson (1999, 2002), Anderson et al. (2002),
and Robinson and Wainer (2002) for further details. To summarize, however, there
is a need in natural resource science for an alternative approach to statistical analysis
and inference, one that addresses these disadvantages of the frequentist approach, the
indirectness of its inferences, and the independence of its analysis. In these next three
chapters, we will provide an introduction to an alternative approach that will address
these needs of natural resource scientists: Bayesian statistical analysis and inference.
2.2
THREE METHODS FOR FITTING MODELS TO DATASETS
In this section, we present an overview of three important statistical methods useful
for ﬁtting models to independent data: least-squares (LS), maximum-likelihood
(ML), and Bayesian parameter estimation (Hilborn and Mangel 1997). These
50
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

methods use optimality criteria to estimate parameters in ﬁtting models to sample
datasets. It is important to understand each method and appreciate their interrelation-
ships. The ﬁrst two methods, LS and ML estimation, are frequentist approaches used
for many popular classical models, such as the mean and proportion estimators,
ANOVA, and multiple linear regression. The third Bayesian method can be viewed
as a generalization of the ML method. We assert that, in many instances, it is particu-
larly suited for the statistical analysis of natural resource datasets, for populations that
are in ﬂux under conditions of adaptive management.
2.2.1
Least-Squares (LS) Fit—Minimizing a Goodness-of-Fit Proﬁle
The ﬁrst method of parameter estimation, least-squares (LS) ﬁt, minimizes a good-
ness-of-ﬁt proﬁle (Rice 1995, Hocking 1996, Hilborn and Mangel 1997, Ryan 1997)
GOF(f j y) ¼
X
n
i¼1
( yi  f (f, y))2,
the sum of the squared residual errors between the independent predictor data y and
the predicted model response f(f, y), expressed as a function of the parameter vector
f and the data y. The goodness-of-ﬁt proﬁle GOF(f j y) is a function of the parameter
vector f, conditional to the sample dataset y ¼ fy1, y2, . . . , yng. The LS ﬁt method
selects the value of the parameter f that minimizes the goodness-of-ﬁt proﬁle
GOF, the sum of the squared error terms.
For example, consider the mean model f(m, y) ¼ m for the mean parameter m and
the dataset y ¼ fy1, y2, . . . ,yng of continuous real measurements yi:
GOF(m j y) ¼
X
n
i¼1
( yi  m)2:
It can be easily demonstrated using calculus, setting the derivative of the GOF proﬁle
with respect to m equal to 0, that the minimum value occurs at the arithmetic mean for
the data y, the classical estimator for the mean parameter
^m ¼
Pn
i¼1 yi
n
:
Considera second example, the linear regression model f(b0, b1, (x, y)) ¼ b0 þ b1 .
x for parameters b0 and b1 and the independent dataset (x, y) ¼ f(x1, y1), (x2, y2), . . . ,
(xn, yn)g of pairs of continuous real measurements (xi, yi):
GOF(b1, b0j(x, y)) ¼
X
n
i¼1
( yi  (b0 þ b1xi))2:
2.2
THREE METHODS FOR FITTING MODELS TO DATASETS
51

It can be shown using calculus, setting partial derivatives of the GOF proﬁle with
respect to b0 and b1 equal to 0, that the minimum value occurs at the classical para-
metric solution of normal equations for the linear regression model:
^b1 ¼
Pn
i¼1 xi  yi
Pn
i¼1 x2
i
,
^b0 ¼ y  ^b1  x:
Least-squares (LS) ﬁt is the method commonly used for linear regression and mul-
tiple linear regression estimation and for estimators used in ANOVA. Least-squares
estimation is distribution-free; there are no distributional assumptions required for
the error terms, or probability models required for the LS estimation. However,
with some mild probability distributional assumptions on the residuals, the Gauss–
Markov Theorem holds: LS estimators for the linear regression and multiple linear
regression models are BLUE, providing best linear unbiased estimators. In other
words, of all linear estimators for linear regression and multiple linear regression
models having independent and identically distributed residuals with mean 0 and
constant error variance (homoscedasticity), LS estimators provide unbiased estima-
tors for the coefﬁcients b0, b1, . . . , bk with the minimum variance (Seber 1977,
and Smith Draper 1981, Cook and Weisberg 1999, Gill 2002). The LS estimators
are unbiased; the averages of the estimates, with repeated sampling, equal the coefﬁ-
cient parameters. Furthermore, of all such unbiased linear estimators, these estimators
have the minimum variance. In other words, the LS estimators for linear and multiple
regression are the most efﬁcient unbiased estimators.
Least-squares
estimation
can
be
generalized
to
generalized
least-squares
estimation (GLS), for LS estimation of generalized linear regression and generalized
multiple linear regression models with nonconstant error variance (heteroscedasti-
city), using weighted LS estimation. This more general approach uses weights for
the squared residuals that are inverses of the error variances, to compensate for
varying error variances. Under these more general assumptions, it can also be
shown that GLS is also BLUE. The S-Plus and R commands for LS and GLS
estimation are lm and gls, respectively.
2.2.2
Maximum-Likelihood (ML) Fit—Maximizing the Likelihood Proﬁle
The second method of parameter estimation, ML ﬁt, maximizes the likelihood proﬁle
(Dobson 1990, Rice 1995, Hilborn and Mangel 1997, Burnham and Anderson 2002)
L(f j y) ¼
Y
n
i¼1
L(f j yi),
where the individual likelihood values L(fjyi) ¼ P(yijf) are the model probabilities
for the independent predictor data points yi conditional to the parameter vector
52
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

values f. The likelihood proﬁle function itself is a function of the parameter vector f
conditional to the data y. Statisticians commonly maximum this function with respect
to f by maximizing its log transformation, the log-likelihood function. Since the log
transformation is monotonically increasing, it provides the same maximal solution
and the log-transformed function is more tractable to differentiation since it is a
sum rather than a product of terms. The log-transformed function can be differen-
tiated and set equal to 0 to ﬁnd the maximum with respect to f, or it can be
solved using numerical techniques. Furthermore, the log-transformed function has
some nice statistical properties.
For example, if we assume the normal model
N( yi; m, s) ¼
1
s 
ﬃﬃﬃﬃﬃﬃ
2p
p
 exp  ( yi  m)2
2s2


for the continuous real measurements yi with mean parameter m and standard devi-
ation parameter s . 0, the likelihood proﬁle for the mean model becomes
L(mjy) ¼
Y
n
i¼1
1
s 
ﬃﬃﬃﬃﬃﬃ
2p
p
 exp  ( yi  m)2
2s2


¼
1
sn  (2p)n=2  exp 
X
n
i¼1
(yi  m)2
2s2
"
#
:
Assuming a constant s, the ML solution is identical to the LS solution for the esti-
mator of the mean parameter m, since the maximum of the likelihood function is
the minimum of the positive value of its exponent.
For the linear regression model, assuming normally distributed errors for the data
points (xi, yi), the likelihood proﬁle is given by
L(b0, b1j(x, y)) ¼
Y
n
i¼1
1
s 
ﬃﬃﬃﬃﬃﬃ
2p
p
 exp  ( yi  (b0 þ b1  xi))2
2s2


¼
1
sn  (2p)n=2  exp 
X
n
i¼1
( yi  (b0 þ b1  xi))2
2s2
"
#
:
Again, assuming a constant parameter s, the ML solution for the estimators of the
parameters b0 and b1 is identical to that of the LS ﬁt. The LS and ML estimators
used for s are slightly different, however:
^sLS ¼
Pn
i¼1 (yi  (ˆb0 þ ˆb1  xi))2
n  2
2.2
THREE METHODS FOR FITTING MODELS TO DATASETS
53

and
^sML ¼
Pn
i ¼ 1 ( yi  (ˆb0 þ ˆb1  xi))2
n
:
The ML method provides estimators that are asymptotically (i.e., as the sample
size becomes large, increasing to inﬁnity) unbiased of minimum variance. As demon-
strated, the LS mean estimator y¯ ¼P
i¼1
n
yi)/n is also the ML mean estimator for
normal data. The LS b coefﬁcient estimators for linear regression are also the ML
estimators for data with normally distributed error. The ML estimator for variance
sˆ 2 ¼ (1/n).P
i¼1
n
(yi 2y¯)2 with the mean model for normal data, with simple
random sampling, is slightly biased with correction factor n/(n 21). The bias
becomes insigniﬁcant with large sample size. Multiplying the ML estimator for var-
iance by the correction factor converts it to the unbiased LS estimator. The general-
ized linear models (GLMs) presented in Chapter 6 are based on ML estimators.
Maximum-likelihood estimation examines the likelihood, based on a probability
model such as the normal, Poisson, or binomial probability distribution for sample
data, and uses the maximum for the estimated value. Note that this maximum
would be the mode if the likelihood function were scaled to become a probability dis-
tribution for the parameter. It can be shown that estimates of the error for the ML par-
ameter estimator are related to the curvature of the likelihood function at the mode,
described by the negative reciprocal of the second derivative, as will be described
in Chapter 6. Hence it can be stated that frequentist ML estimation provides the
mode and curvature of the likelihood function at the mode as a “likelihoodist” esti-
mation of the parameter for the best-ﬁtting model.
We shall next demonstrate how this compares with the third method for ﬁtting
models to datasets, the Bayesian approach to parameter estimation. This third
method of parameter estimation, Bayesian statistical inference, uses the entire likeli-
hood function, along with a prior distribution, to calculate a posterior distribution for
the parameter.
2.2.3
Bayesian Fit—Bayesian Statistical Analysis and Inference
Bayesian statistical analysis is based on an alternative approach to model ﬁt and esti-
mation of parameters (Iversen 1984, Berger 1985, Carlin and Lewis 2000, Draper
2000, Congdon 2001, Gill 2002). The Bayesian method is based on an assumption
of a prior distribution pprior(b) representing initial understanding, or belief, about
the parameter b, prior to data analysis. If little is known about the value of the par-
ameter, this prior distribution could be “noninformative,” such as a ﬂat, uniform dis-
tribution, assuming equal probability for the parameter value within a realistic range.
Alternatively, the prior could be more informative such as a normal distribution or
other plausible distribution that represents an initial assessment of what is known
about the parameter, prior to data collection and analysis. The initial assessment
could be based on the results of previous studies. The prior affects the analysis
54
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

results, so it must be chosen carefully to be realistically and conservatively plausible,
based on prior evidence regarding the probabilities for the parameter value. If there is
controversy about the choice of priors, a collection of plausible priors representing a
realistic range of possibilities could be examined in the analysis for their effects on
the posterior results, with a sensitivity analysis.
The Bayesian method also requires the use of the likelihood function L(b) for the
parameter b, the likelihood proﬁle based on the sample dataset and an assumed
model. Commonly assumed models for natural resource datasets include the
normal model for continuous data, the Poisson and negative binomial models
for count data, the binomial model for binary data, and the multinomial model for
mark–recapture datasets. Recall that the likelihood, representing a function of the
parameter given the data, is based on the probability distribution of the data at
each point, given the parameter. We have previously seen that this same likelihood
function is analyzed with frequentist ML ﬁt.
For the Bayesian method, however, the likelihood function L(b) is multiplied by
the prior pprior(b) to obtain a new function, the product p(b) ¼ L(b) . pprior(b) of the
likelihood and prior (see Bayes Theorem in Section 2.3). This new function is then
scaled to obtain a probability distribution function, with a total sum in the case of
a discrete parameter, or integral in the case of a continuous parameter, over the
range of the parameter that is equal to 1. This scaled probability distribution function
is called the posterior distribution for the parameter. The posterior distribution rep-
resents a revised, updated understanding, or assessment, of the parameter. It is
based, as we have seen, on the prior distribution of the parameter, the data, and the
model assumed for the data.
Hence the frequentist uses the data and a model for the data to obtain a likelihood
proﬁle and calculates the maximum of the likelihood proﬁle for the ML estimate of
the parameter, along with an estimate of error based on the curvature of the likelihood
proﬁle at the maximum value. The Bayesian, on the other hand, combines the data
and the model with a prior distribution for the parameter to obtain a scaled posterior
distribution for the parameter, using the entire posterior distribution for an updated
assessment of the parameter.
The Bayesian can then assess the parameter by examining the posterior distri-
bution and its descriptive statistics, such as the mean, median, mode, standard devi-
ation, and percentiles. The Bayesian can calculate a credible interval for the
parameter, using boundaries of a middle percentile range for the parameter. A 95%
(or P%) credible interval, for example, can be based on the 2.5% and 97.5%
[or (100 2 P)/2 and 100 2(100 2P)/2] percentile points of the posterior distri-
bution. More rigorously, Bayesian credible intervals are based on the highest pos-
terior density (HPD), the smallest interval encompassing at least 95% (or P) of the
probability distribution (Carlin and Louis 2000, pp. 36–37). The credible interval
is the Bayesian analog to the frequentist conﬁdence interval. There is, however, an
entirely different interpretation for the meaning of the Bayesian credible interval;
there is a 95% (or P%) probability of the parameter occurring within the speciﬁed
95% (or P%) credible interval, assuming the prior distribution, the sample dataset,
and the model. The Bayesian interpretation is a direct probabilistic statement about
2.2
THREE METHODS FOR FITTING MODELS TO DATASETS
55

the parameter, rather than the frequentist interpretation of a conﬁdence interval, a
probabilistic statement about the data, based on the concept of repeated sampling
or experimentation of the population.
2.2.4
Examples
Let’s look at some examples to illustrate these ideas. We will examine three prototype
examples of natural resource datasets, continuous, count, and binary datasets, with
the normal, Poisson, and binomial models, respectively. For these examples,
we will use very small sample datasets with just a few points, to illustrate clearly
the concepts, leaving it to the reader to extrapolate these results to larger, more
realistic datasets.
The ﬁrst example is based on the normal model for continuous data, with the
mean parameter. Let’s consider sampling the community forest of a small town
in northern California, measuring tree diameters at breast height (i.e., dbh, at
4.5 ft above the ground). Assume that this forest is a 100-year-old even-aged
second-growth forest of predominantly redwood and Douglas ﬁr trees that have
diameters generally ranging between 20 and 40 in., with preliminary estimates of
a mean around 30 in. The city forester who knows the forest well insists that no
tree in the forest exceeds 60 in. dbh. None of the existing dbh data collected by
the city in previous years exceeded 60 in., and no large old-growth trees or snags
still exist in the forest. Let’s collect n ¼ 100 simple random sample measurements
with the objective of assessing the mean tree diameter for the forest. We do not
have any prior information about the mean of the tree diameters in this community
forest, so we will use a noninformative prior distribution that is uniform, with a
range between 0 and 60. The range of this uniform distribution encompasses the
practical range of dbh measurement values for the forest, and hence of the mean
dbh parameter.
We will also assume that the dbh measurements for trees in this forest are approxi-
mately normally distributed, a reasonable assumption since it is an even-aged mature
stand. So let’s calculate the likelihood function from our data and multiply the like-
lihood by the prior, scaling the result to produce the posterior distribution. Since the
prior is constant and the range of the most signiﬁcant part of the likelihood function
falls well within the range of the prior, the posterior will resemble the likelihood func-
tion, truncated at 0 and 60, and scaled to a probability distribution function. The fre-
quentist ML estimate is the mode of this function. The Bayesian solution is provided
by the entire posterior distribution. This posterior distribution can be used to make
probability inferences about the mean dbh parameter for trees in this community
forest population, with statistics of the posterior such as the mean, median, mode,
median, and standard deviation, and percentiles such as the 95% credible interval
given by the 2.5% and 97.5% percentile limits.
To illustrate speciﬁcally, let’s examine a simple example, a sample dbh dataset con-
sisting of just two measurements, 28 and 34 in. Let’s assume that s ¼ 5 to illustrate
the method. This is a reasonable estimate for s since the range of the data varies gen-
erally between 20 and 40 in. and approximately 95% of, normal data falls within 4 . s
56
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

of this range. The goodness-of-ﬁt proﬁle for m is (Fig. 2.1)
GOFðm j yÞ ¼ ð28  mÞ2 þ ð34  mÞ2:
The likelihood proﬁle, or likelihood function, for m is (Fig. 2.2)
L(m j y) ¼
1
s 
ﬃﬃﬃﬃﬃﬃ
2p
p
 e½(28m)2=2s2



1
s 
ﬃﬃﬃﬃﬃﬃ
2p
p
 e½(34m)2=2s2


¼
1
s2  (2p)  exp  (28  m)2
2s2
þ (34  m)2
2s2




:
Note in the ﬁgures that the LS and ML estimates for the mean parameter are identical
at mˆ ¼ 31. The posterior is the scaled truncated likelihood, since the prior is ﬂat. It
represents a probability distribution describing an updated assessment of the mean
parameter, based on the sample dataset. If the prior were “informative” and described
by a normal distribution, we could well imagine that the posterior of normal data and
a normal prior would be normal. This is, in fact, the case; the posterior can also be
described using conjugacy theory with normal priors or approximated using
Markov chain Monte Carlo (MCMC) methods with the Bayesian statistical software
WinBUGS. These methods will be examined later in Section 2.4 and in Chapter 4.
The parameter s ¼ 5 was assumed to be known in this example for the sake of
simplicity. However, the parameter s could also have been analyzed with this
Bayesian approach, using priors, a likelihood function, and posteriors for the two-
dimensional parameter space (m, s).
Figure 2.1. Goodness-of-ﬁt proﬁle of the mean m for the dataset y ¼ f28, 34g using S-Plus or
R code
. mu , - seq(0,60,0.1)
. gof ,- (28-mu)^2 þ (34-mu)^2
. plot(mu,gof)
2.2
THREE METHODS FOR FITTING MODELS TO DATASETS
57

We look next at a Poisson model for a count dataset. Let’s imagine conducting a
study of curlew abundance in a bay north of San Francisco, estimating mean density
using count data collected on n ¼ 50 ﬁxed area plots with simple random sampling.
We will use the Poisson model as a ﬁrst approximation for the count data, noting that
a necessary assumption for the Poisson model is the random dispersal of the birds on
the bay, an assumption that may not in fact be accurate. Recall that the Poisson model
has just one parameter, the mean l, with variance s2 ¼ l. Note that, if the birds are
aggregated in clusters with s2 . l, it would be better to use the negative binomial
model to incorporate overdispersion into the model. We will discuss that possibility
in Chapter 4. Another complication is that the probability of detection may not be
equal to 1, nor constant from plot to plot. However, let’s ignore these complications
for the time being and base the analysis on the Poisson model, assuming random dis-
persal and complete certainty of detection, for simple illustration. Let’s calculate the
likelihood function of the mean l, assuming the Poisson model Pois (y; l) ¼ e2l.
(ly/y!) with mean parameter l for the sample data y ¼ fy1, y2, . . . , yng of nonnega-
tive independent integer counts:
L(l) ¼
Y
n
i¼1
el  lyi
yi!
¼ enl  l
Pn
i¼1 yi
Q
n
i¼1
yi!
:
We multiply this by a prior and scale the product to obtain the posterior. We can again
use a noninformative ﬂat prior, say, a uniform distribution ranging between 0 and 20,
Figure 2.2. Likelihood proﬁle of the mean m for the dataset y ¼ f28, 34g with the normal
model N(y; m,s ¼ 5) using S-Plus or R code
. mu , - seq(0,60,0.1)
. likelihood ,-
(1/(5^2*(2*pi)))*exp(-((28-mu)^2+(34-mu)^2)/(2*5^2))
. plot(mu,likelihood)
58
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

assuming that the nonnegative count measurements do not exceed 20. If another prior
were more plausible, such as a posterior distribution obtained from a Bayesian stat-
istical analysis of a dataset from a previous study of the population, that distribution
could be used for the prior. Using the uniform prior between 0 and 20, the posterior
will be the scaled likelihood function, truncated at 0 and 20.
Illustrating with a simple example, let’s analyze a dataset of three sample data
points, plot counts y1 ¼ 2, y2 ¼ 0, and y3 ¼ 4. Then the GOF proﬁle will be (Fig. 2.3)
GOF(l) ¼ (2  l)2 þ (0  l)2 þ (4  l)2:
The likelihood function for l is (Fig. 2.4)
L(l) ¼
el  l2
2!


 el  l0
0!


 el  l4
4!


¼ e3l  l(2þ0þ4)
2!  0!  4!
¼ e3l  l6
48 :
Note in the ﬁgures that the LS and ML estimates for the mean parameter are identical at
lˆ ¼ 2. The posterior is the scaled truncated likelihood since the prior is ﬂat throughout
the most signiﬁcant part of the range of the likelihood function. It represents a prob-
ability distribution describing the updated assessment of l, the mean density parameter,
based on the sample dataset. This posterior distribution can be determined exactly using
Figure 2.3. Goodness-of-ﬁt proﬁle of the mean l for the dataset y ¼ f2, 0, 4g using the S-Plus
or R code
. lambda ,- seq(0,20,0.1)
. gof ,- (2-lambda)^2 þ (0-lambda)^2 þ(4-lambda)^2
. plot(lambda,gof)
2.2
THREE METHODS FOR FITTING MODELS TO DATASETS
59

conjugacy theory with gamma priors or approximated using MCMC methods with the
Bayesian statistical software WinBUGS (see Section 2.4.2 and Chapter 4).
Let’s conclude this section with a third example, examining the ML and Bayesian
methods of parameter estimation for binary data using the binomial model. The par-
ameter estimated in this example is the proportion parameter. Let’s imagine randomly
sampling a binary dataset of presence–absence measurements for Northern Spotted
Owls nesting pairs at n ¼ 80 200-hectare (ha) sites in a national forest along the
Paciﬁc north coast. Suppose that recent research studies have estimated the home
range of Northern Spotted Owl nesting pairs to be approximately 200 ha in size,
and use that for the plot size. We will use the binomial model
B( y; n, p) ¼
n
y


 p y  (1  p)ny
¼
n!
y!  (n  y)!  p y  (1  p)ny
for the dataset,wherey isthe sum of the 1s, oroccupied sites; n isthe samplesize; and p is
the proportion parameter. The parameter p isthe proportion of the habitat that is occupied
by nesting pairs. Equivalently, the parameter p isthe probabilityof a random sample con-
taining a nesting pair of Northern Spotted Owls. We again assume complete certainty of
detection at the sampled nesting sites. This can be approximately achieved by repeatedly
visiting to each site. The likelihood function for this binary dataset is
L( p) ¼
n
y


 p y  (1  p)ny:
Let’s again use a noninformative ﬂat prior, a uniform distribution for p ranging between
0 and 1. The posterior distribution is then the scaled product of this likelihood and the
noninformative prior, the scaled likelihood function on the interval [0,1].
Figure 2.4. Likelihood proﬁle of the mean l for the dataset y ¼ f2, 0, 4g with the Poisson
model Pois(y; l) using the S-Plus or R code
. lambda ,- seq(0,20,0.1)
. likelihood ,- exp(-3*lambda)*lambda^6/48
. plot(lambda,likelihood)
60
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

To illustrate, suppose that we collect data at n ¼ 80 sites and measure y ¼ 20 to be
occupied. Then the likelihood function is (Fig. 2.5)
L( p) ¼
80
20


 p20  (1  p)8020 / p20  (1  p)60:
Note that the ML estimate of the parameter is pˆ ¼ 20/80 ¼ 25.0%. The posterior is
the scaled likelihood function since the prior is ﬂat. It represents a probability
distribution describing an updated assessment of the parameter p. It can be determined
exactly with beta priors using conjugacy theory, or approximated using MCMC methods
with the Bayesian statistical software WinBUGS (Section 2.4.3 and Chapter 4).
2.3
THE BAYESIAN PARADIGM FOR STATISTICAL
INFERENCE: BAYES THEOREM
Bayesian statistical analysis provides a probability distribution representing a natural
resource scientist’s current assessment of a parameter. The analysis process begins
with an assumption of a prior distribution representing the scientist’s understanding
of the parameter preceding the study. The analysis then combines the prior distri-
bution for the parameter with the likelihood function derived from a sample
dataset and a model for the dataset to produce a posterior distribution for the par-
ameter. The posterior distribution is obtained by multiplying the prior distribution
function by the likelihood function and scaling this product to provide a probability
distribution function, a posterior distribution representing a revised understanding of
the parameter. The posterior distribution therefore is a function of the prior
Figure 2.5. Likelihood proﬁle of the proportion parameter for the binary dataset y ¼ 20 with
the binomial model B(y; n ¼ 80, p) using the S-Plus or R code
. p ,- seq(0,1,0.01)
. likelihood ,- p^20*(1-p)^60
. plot(p,likelihood)
2.3
THE BAYESIAN PARADIGM FOR STATISTICAL INFERENCE
61

distribution, the sample dataset, and the model that is assumed for the dataset. The
entire process is based on a property of conditional probability ﬁrst recognized by
Reverend Thomas Bayes in the eighteenth century (Bayes 1763). This property is
now formally stated with the following theorem.
Theorem 2.1: Bayes Theorem. If b is a parameter and D is the dataset, then
1. The discrete parameter conditional probability distribution P, subject to the
data, is given by
P(bjD) ¼ P(Djb)  P(b)
SP(Djb)  P(b)
or
posterior (bjD) ¼ likelihood (bjD)  prior (b)
S likelihood (bjD)  prior (b)
2. The continuous parameter conditional probability distribution P, subject to the
data, is given by
P(bjD) ¼
P(Djb)  P(b)
Ð
P(Djb)  P(b)  db
or
posterior (bjD) ¼
likelihood (bjD)  prior (b)
Ð likelihood (bjD)  prior (b)  db :
Note that the denominators are the scaling constants that ensure the probability dis-
tributions sum (discrete case) or integrate (continuous case) to one. The posteriors
represent an updated understanding of the parameter b, conditional to the data D.
The Bayes deﬁnition of conditional probability is as follows: the probability P
of A conditional to B ¼ P(AjB) ¼ P(A>B)/P(B) for subsets of events A and B in a
sample space of outcomes of an experiment. An outline of the proof to the Bayes
Theorem for the discrete case is as follows (Fig. 2.6):
P(bijD) ¼ P(bi > D)=P(D)
¼ P(Djbi)  P(bi)=P(D)
¼ P(Djbi)  P(bi)=SjP(bj > D)
¼ P(Djbi)  P(bi)=SjP(Djbj)  P(bj)
or
Posterior (bijD) ¼
likelihood (bijD)  prior (bi)
S j likelihood (b jjD)  prior (b j) :
62
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

Here S is the sample space of all possible outcomes in an experiment, the bi constitute
all the possible distinct values of the parameter b, and D is a dataset. Reverse bi and
D in the ﬁrst line and solve for P(bi>D) ¼ P(D>bi) ¼ P(Djbi) . P(bi) in the numer-
ator to obtain the second line. Substitute SjP(bj>D) for P(D) the second-line
denominator to obtain the third line, since D is the disjoint union of its subsets
bi>D. Finally, use the second-line substitution of P(bi>D) ¼ P(Djbi) . P(bi) in
the denominator of the third line to complete the derivation in the fourth and
ﬁfth lines.
2.4
CONJUGATE PRIORS
Unfortunately, Bayes’ Theorem provides a formula that is not always mathematically
solvable in closed form, especially for complex formulations. The integrals in the
denominator for the continuous case can be extremely challenging if not impossible
to solve mathematically in closed form, particularly for higher-dimensional parameter
spaces. However, there are many common cases where Bayesian closed-form sol-
utions are possible. For some important types of natural resource datasets and
models, it is possible to obtain closed-form solutions by judiciously choosing particu-
lar family forms for priors that also provide posteriors. For example, for a continuous
dataset modeled by a normal model with known standard deviation s, a normal prior
for the mean parameter m provides a normal posterior solution. For a count dataset
modeled by the Poisson distribution, a choice of gamma prior for the mean parameter
l provides a gamma posterior solution. For a binary dataset modeled by the binomial
distribution, a beta prior for the proportion parameter p provides a beta posterior sol-
ution. A gamma prior for a dataset such as arrival times modeled by the exponential
distribution provides a gamma posterior solution. A Dirichlet prior for a dataset
modeled by the multinomial distribution, such as mark–recapture data, provides a
Dirichlet posterior solution. These judicious choices of family distributional forms
for priors are called conjugate priors.
Figure 2.6. Sample space S of outcomes of an experiment, with discrete parameter values b ¼
fb1, b2, b3, b4g and with dataset D.
2.4
CONJUGATE PRIORS
63

The closed-form solutions for the posteriors of sample datasets and models with
conjugate priors are expressible as transformations of the parameter values of the
prior distributions. The transformations are expressable in terms of sample statistics
calculated from the data. We illustrate these ideas by presenting several important
conjugate priors.
2.4.1
Continuous Data with the Normal Model
Let’s consider a continuous dataset from a normally distributed population N(m,s).
We assume for the sake of simplicity that the parameter s is known. Let’s assume
that the prior for m is normally distributed
m  N(mpr, spr),
with mean mpr, standard deviation spr, and precision tpr ¼ 1/spr
2 .
Bayesians prefer to use precision, the inverse of the variance, rather than variance
or standard deviation for a parameter for reasons that will shortly become apparent.
We estimate the mean y¯ from a sample dataset, with standard deviation s, standard
error se ¼ s/pn, and precision t ¼ n/s2. Then it can be shown that the posterior
distribution for m is also normally distributed (Iversen 1984, Gill 2002)
m  N(mpost,spost)
with mean
mpost ¼
tpr
t pr þ t


 mpr þ
t
tpr þ t


 y
and precision
tpost ¼ tpr þ t (hence, spost ¼ 1=
ﬃﬃﬃﬃﬃﬃﬃﬃ
tpost
p
):
These properties can be derived mathematically since the prior function and likeli-
hood function for the mean both have the same normal form. The interested reader
is referred to Hilborn and Mangel (1997) for details.
Thus, the normal posterior distribution for the mean parameter m has a mean that is
the weighted average of the mean of the prior and the estimated mean obtained from
the sample dataset. The weights are given by the precisions of the prior and the
dataset. The precision of the posterior is the sum of the precisions of the prior and
the dataset. Note how the sample size of the dataset directly affects the weights
used to calculate the posterior mean and precision. For example, doubling the
sample size of the dataset doubles its precision and hence increases the relative
role of the sample estimate in the posterior weighted mean and precision accordingly.
For example, let’s analyze the tree diameter dataset described in Section 2.2.4.
Assume a prior that is normally distributed with mean mpr ¼ 30 and standard
64
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

deviation spr ¼ 5 (hence tpr ¼ 1/52 ¼ 0.04). Suppose that the estimate from the
sample dataset for the mean is y¯ ¼ 26. Assume a known s ¼ 5 for the
dataset population, with standard error se ¼ s/
ﬃﬃﬃn
p ¼ 5/
ﬃﬃﬃﬃﬃﬃﬃﬃ
100
p
¼ 0.5, and precision
t ¼ 1/se2 ¼ 4.0. Then the posterior distribution for the mean parameter m has mean
mpost ¼
tpr
tpr þ t


 mpr þ
t
t pr þ t


 y
¼
0:04
0:04 þ 4:00


 30 þ
4:00
0:04 þ 4:00


 26
¼ 0:30 þ 25:74 ¼ 26:04
and precision
tpost ¼ tpr þ t
¼ 0:04 þ 4:00 ¼ 4:04:
Note how the sample size n ¼ 100 for the dataset affected the precision of the mean
estimate and the weights used to calculate the posterior mean. Note also the “shrink-
age” of the mean estimate toward the prior value, slight in this example because of the
relatively large sample size. Our conclusion is that the original prior assessment of the
mean parameter was biased upward, and that the sample dataset provided evidence to
revise that assessment downward.
In summary, for continuous data with the normal model, Bayesian statistical analy-
sis using the normal conjugate prior provides the following transformation for the
posterior:
prior N(m; mpr, spr) ! (data y þ normal model N( y; m, s))
! posterior N(m; mpost, spost),
where
mpost ¼
tpr
tpr þ t


 mpr þ
t
tpr þ t


 y
and
tpost ¼ tpr þ t,
with tpr ¼ 1/spr
2 , t ¼ 1/se2 ¼ n/s2, and tpost ¼ 1/spost
2
.
Although this result depends on the assumption of the normality of the prior and
the normality and known variance of the data, it is approximately true in general for
posteriors; as a ﬁrst approximation, the precision of a posterior is the sum of the pre-
cisions of the prior and the data and the posterior mean is a weighted average of the
2.4
CONJUGATE PRIORS
65

means of the prior and the data, weighted by precision. This is an intuitively pleasing
condition that one would expect as additional datasets are collected.
2.4.2
Count Data with the Poisson Model
Next, let’s conduct Bayesian statistical analysis on count data with the Poisson model,
using a conjugate prior. The appropriate conjugate distribution to use for these data
and model is the gamma distribution G(x; a, b): (0, 1) ! (0, 1), with domain
and range consisting of positive real numbers, given by
G(x; a, b) ¼ (ba=G(a))  ebx  xa1,
where
G(a) ¼
ð1
0
et  t a1  dt
is the gamma function and a . 0 and b . 0 are the parameters (Rice 1995, Wackerly
et al. 2002). For positive integers m, G(m) ¼ (m 2 1)!, the factorial function. Recall
that 0! ¼ 1, 1! ¼ 1, 2! ¼ 1. 2 ¼ 2, 3! ¼ 1. 2. 3 ¼ 6, . . . . The gamma distribution is a
very ﬂexible and useful distribution, particularly to physical scientists and to
Bayesians. Let’s make practical use of this distribution by better understanding the
function of its two parameters, the shape parameter a and the scale parameter b. If
the shape parameter a , 1, the gamma distribution G declines exponentially with
increasing x. If a  1, G is unimodal and skewed to the right with
mode ¼ (a  1)=b:
The mean of G is
m ¼ a=b,
and the variance is
s2 ¼ a=b2:
Conversely, if the mean m and variance s2 are speciﬁed, the parameters a and b
are given by a ¼ m2/s2 and b ¼ m/s2. See Fig. 2.7 for examples of the
gamma distribution.
A popular choice for a noninformative gamma prior is given by G(x; a ¼ 1023,
b ¼ 1023) (Fig. 2.8). This distribution is relatively ﬂat except close to the origin,
where parameter values are unlikely to occur, and it has some nice properties as
well. (It is an approximation to a Jeffreys prior; see Section 2.5.2.)
66
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

Assuming the Poisson model Pois (y; l) ¼ e2l . (ly/y!) for the dataset y ¼ fy1,
y2, . . . ,yng, it can be shown (Hilborn and Mangel 1997) that the posterior distribution
with the gamma prior G(l; apr, bpr) for the parameter l is given by G(l; apost, bpost),
where
apost ¼ apr þ Siyi,
bpost ¼ bpr þ n:
Figure 2.7. Gamma distributions G(x; a, b) for a selection of parameters a and b using S-Plus
or R code (a) a ¼ 0.5, b ¼ 1. (b) a ¼ 1, b ¼ 1. (c) a ¼ 5, b ¼ 1. (d) a ¼ 10, b ¼ 1. (e) a ¼
5, b ¼ 5. (f) a ¼ 10, b ¼ 5
. x,- seq(0, range, 0.01)
. gamma ,- dgamma (x, a,b)
. plot(x, gamma, type="1")
2.4
CONJUGATE PRIORS
67

In other words, to obtain the conjugate gamma posterior for count data, simply add
the sum of the counts and the sample size to the prior gamma distribution parameters
a and b, respectively.
So, suppose that the 50 sites sampled in the second example of Section 2.2.4 for the
count data above yielded a total sum of counts Siyi ¼ 150 curlews, with an estimated
mean of y¯ ¼ 150/50 ¼ 3 birds per site. If we assume a noninformative gamma prior
G(l; 1023, 1023) for the Poisson mean parameter l, the posterior gamma distribution
will be given by G(l; 1023þ150, 1023þ 50) ¼ G(l; 150.001, 50.001) (Fig. 2.9).
Note that the posterior distribution for the dataset with this noninformative prior
resembles the scaled likelihood function with a mode at the ML estimator
value y¯ ¼ 3.
Figure 2.8. The noninformative gamma distribution G(x; a ¼ 0.001, b ¼ 0.001).
Figure 2.9. Posterior distribution G(l; apost ¼ 150.001, bpost ¼ 50.001) for the count dataset
y with Siyi ¼ 150, sample size n ¼ 50, and noninformative gamma prior G(l; apr ¼ 0.001,
bpr ¼ 0.001).
68
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

In summary, for count data with the Poisson model, Bayesian statistical analysis
using the gamma conjugate prior yields the following transformation for the
posterior:
prior G(l; apr, bpr) ! ½data y þ Poisson model Pois (y; l)
! posterior G(l; apost, bpost),
where
apost ¼ apr þ Siyi,
bpost ¼ bpr þ n:
2.4.3
Binary Data with the Binomial Model
Let’s conclude this discussion with a third example of conjugacy, analyzing binary
data with the binomial model
B( y; n, p) ¼
n
y


 p y  (1  p)ny:
The conjugate prior for the parameter p of the binomial model is the beta distri-
bution BE(p; a, b): [0, 1] ! [0, 1) given by
BE( p; a, b) ¼ (G(a þ b)=(G(a)  G(b)))  ya1  (1  y)b1,
where G(a) ¼
Ð
0
1 e2t . ta21. dt is the gamma function. The beta distribution is a
ﬂexible distribution with two parameters a and b for a random variable p ranging
between 0 and 1. As such, it is particularly appropriate to describe a probability par-
ameter such as p. The mean and variance of the beta distribution BE( p; a, b) are
given by
m ¼ a=(a þ b),
s2 ¼ a  b=½(a þ b)2  (a þ b þ 1):
Conversely, if m and s2 are known, the parameters a and b are given by
a ¼ m2  m3  m  s2
s2
b ¼ 1
s2  m  (1  m)2 þ (m  1):
2.4
CONJUGATE PRIORS
69

The mode of the beta distribution is as follows:
mode ¼ (a  1)=(a þ b  2)
if
a . 1
and
b . 1,
0
and
1
if
a , 1
and
b , 1,
0
if
a , 1
and
b  1
or
if
a ¼ 1
and
b . 1,
1
if
a  1
and
b , 1
or
if
a . 1
and
b ¼ 1,
and
does not exist if
a ¼ b ¼ 1:
The ﬂat uniform beta distribution is BE( p; 1, 1) ¼ 1. If a ¼ b, BE( p; a, b) is
symmetric. If a ¼ b , 1, BE is concave upward. If a ¼ b . 1, BE is convex down-
ward. See Fig. 2.10 for examples.
The effect of binary data with the binomial model B(y; n, p) on the conjugate prior
BE( p; apr, bpr) is simple to describe: add y, the number of 1s, to a and add n 2 y, the
number of 0s, to b (Hilborn and Mangel 1997). So the posterior distribution for p is
given by BE( p, apost, bpost) where
apost ¼ apr þ y,
bpost ¼ bpr þ (n  y):
For example, let’s return to our binary dataset of Section 2.2.4. Suppose that we
detected the presence of Northern Spotted Owl pairs on y ¼ 20 of the n ¼ 80
sampled sites. Using a noninformative beta prior BE( p; 1, 1) for p, the posterior
prior would be BE( p; 1 þ 20, 1 þ 60) ¼ BE( p; 21, 61) (Fig. 2.11).
Note that the mode of the posterior occurs at the ML estimate pˆ ¼ 20/80 ¼ 0.25
as expected with a ﬂat prior. The mean of the posterior occurs at
^m ¼
apost
apost þ bpost
¼
21
21 þ 61 ¼ 0:256:
In summary, for binary data with the binomial model, Bayesian statistical analysis
using the BE conjugate prior yields the following transformation for the posterior:
prior BE( p; apr, bpr) ! (data y þ binomial model B( y; n, p))
! posterior BE(p; apost, bpost)
where
apost ¼ apr þ y,
bpost ¼ bpr þ (n  y):
70
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

2.4.4
Conjugate Priors for Other Datasets
There are many other datasets and models with conjugate priors. Most notably,
multinomial models, useful for capture–recapture datasets, can be analyzed with
closed-form Bayesian solutions using a Dirichlet conjugate prior distribution. The
multinomial distribution generalizes the binomial distribution to data that are categ-
orical with k  2 classes, in contrast to the binomial case where k ¼ 2.
Analogously, the Dirichlet distribution generalized the beta distribution to the case
with k probabilities p1, p2, . . ., pk for each of k classes of the Dirichlet distribution
Figure 2.10. Beta distributions BE( p; a, b) for a range of parameters a and b using the S-Plus
or R code (a) a¼0.5, b¼0.5. (b) a ¼ 1, b ¼ 1. (c) a ¼ 2, b ¼ 2. (d) a ¼ 1, b ¼ 5. (e) a ¼ 1,
b ¼ 10. (f) a ¼ 2, b ¼ 10
. p ,- seq(0, 1,0.01)
. beta ,- dbeta (p, a,b)
. plot(p, beta, type="1")
2.4
CONJUGATE PRIORS
71

instead of two probabilities p1 ¼ p and p2 ¼ q ¼ (12p) for the two classes of the
binomial distribution.
Another classical case of conjugacy applies to datasets such as arrival times that
satisfy the assumptions of the exponential model. In this case, the gamma distribution
again provides a conjugate prior.
Aggregated count datasets satisfying the assumptions of the negative binomial
model can be analyzed using a beta conjugate prior. Recall that count data for popu-
lations that are randomly dispersed should be analyzed with the Poisson model,
which has a variance equal to the mean s2 ¼ m. If the population is aggregated or
clumped with a variance greater than the mean s2 . m, the dataset will be overdis-
persed and should be analyzed with the negative binomial model. We will examine
this model in greater detail in Chapter 4.
All of these models are special cases of an important class of models, the exponen-
tial family of distributions, all of which have conjugate priors (Gill 2002). The import-
ance of this general class of distributions will become clearer when we examine
generalized linear models (GLMs) in Chapter 6. Many common distributions are
special cases of the exponential family of distributions, such as the normal, Poisson,
negative binomial, binomial, multinomial, exponential, and gamma distributions.
Sufﬁcient statistics necessary to calculate ML estimates are guaranteed for the expo-
nential family of distributions. The reader is referred to the texts by Dobson (1990),
Hilborn and Mangel (1997), and Gill (2002) for further details.
2.5
OTHER PRIORS
In the previous section, we have demonstrated that many important natural resource
datasets and models have associated conjugate priors with closed-form Bayesian
Figure 2.11. Posterior distribution BE( p; a ¼ 21, b ¼ 61) for the binary dataset y with y ¼
20 and sample size n ¼ 80, and a noninformative beta prior BE( p; a ¼ 1, b ¼ 1).
72
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

solutions for their posteriors. However, for many other more complex datasets and
models without conjugate priors, the natural resource scientist can choose from a
range of other priors. Priors can have a signiﬁcant inﬂuence on posteriors, particularly
for datasets of small sample size, so they must be chosen wisely. Let’s examine some
of the other types of priors that are of common use: noninformative priors, uniform
priors, Jeffreys priors, reference priors, proper and improper priors, vague priors,
elicited priors, and empirical Bayes methods of choosing priors.
2.5.1
Noninformative, Uniform, and Proper or Improper Priors
A conservative approach to choosing priors is to assume that little information is
known a priori about a parameter and to use a noninformative prior. The most
common choice of noninformative prior is a uniform prior, a ﬂat distribution that
assigns the same probability to every value of the parameter. However, problems
arise if the parameter is inﬁnite in range and cannot be described by a ﬂat distribution
that is a proper prior with bounded sum or integral, but rather must be described by
an improper prior with unbounded sum or integral. This problem need not necess-
arily be of serious concern if the likelihood of the dataset has a bounded range of
support where the function is nonzero. In that case, the posterior may still be a
well-deﬁned proper distribution with bounded sum or integral equal to 1 even if
the prior is not proper. In many practical situations with natural resource datasets
and models, this is the case since the realistic range of most parameters usually is
bounded. For instance, mean tree diameters or animal abundance levels for many
populations of interest are bounded in range. The mean tree diameter parameter of
a forest is bounded by 0 in. and the dbh,max of the largest conceivable tree, so a
uniform distribution Unif (m; 0, dbh,max) with limits 0 and dbh,max can serve as a non-
informative prior for the parameter m. Similarly, the abundance of many animal popu-
lations is of limited size Nmax, so it is bounded by 0 and Nmax.
2.5.2
Jeffreys Priors
There is an apparent paradox inherent in Bayesian statistical inference in that priors
are seldom invariant to transformations of parameters. For example, if a parameter
u is uniformly distributed, it is not generally true that u2 is uniformly distributed.
The Bayesian statistician Jeffreys (1961) resolved this apparent difﬁculty by deﬁn-
ing a set of conditions required for a class of priors, now called Jeffreys priors, to
have posteriors that are invariant to transformations of parameters. The trick is to
utilize the shape of the likelihood function of the model describing the dataset and
choose a prior on the basis of its curvature. The prior uses the square root of the deter-
minant of the negative expected Fisher information matrix obtained from the Hessian
of second derivatives of the likelihood function. The details are technical and beyond
the scope of this book. A simple derivation for binary data with the binomial model is
given in Gill (2002, pp. 123–125). The more general problem is discussed in Berger
(1985). The Jeffreys prior for the proportion parameter p for binary data with the
binomial model is the beta distribution BE( p; a ¼ 0.5, b ¼ 0.5) with parameters a
2.5
OTHER PRIORS
73

and b equal to 0.5. The Jeffreys prior for the mean parameter l for count data with the
Poisson model is approximated by the gamma distribution G(l; a ¼ 0.001, b ¼
0.001) with parameters a and b equal to 0.001 (Fig. 2.8) (Gill 2002). Jeffreys
priors can often be chosen that are noninformative, or approximately uniform,
around the signiﬁcant areas of support of the dataset model likelihood function,
where it has nonzero probability, although that is not always the case.
2.5.3
Reference Priors, Vague Priors, and Elicited Priors
Several other types of priors are commonly used by Bayesians, which we will
mention here. Reference priors are priors, such as conjugate priors, that are
convenient to use for certain types of datasets. Other examples of reference priors
are diffuse priors or vague priors such as imprecise normal distributions centered
at the expected modes of posteriors with very large standard deviations. These
priors are reasonably uniform around the region of interest, yet can be easy to manip-
ulate mathematically.
Elicited priors are priors that are derived from previous knowledge about a
parameter. Elicited priors can be constructed from expert opinion, using a speciﬁed
particular parametric form. Conjugate priors with speciﬁed parameter values based
on previous experience are examples of elicited priors. It is often a challenge to trans-
late expert opinion into a prior so that a range of priors can be speciﬁed, representing
the variety of expert opinion. A sensitivity analysis can then be conducted to deter-
mine the amount of variation among the posteriors for the range of priors.
2.5.4
Empirical Bayes Methods
Empirical results may be used to describe characteristics of the priors. For example,
frequentist statistical estimates from previous studies may be used to specify prior
characteristics. Bayesians have shown particular interest more recently in hierarchical
models with a hierarchy of parameters at several levels where parameters at each level
are expressed in terms of parameters at a lower level. For hierarchical models,
parameters at the lowest level, called hyperparameters, need to be speciﬁed.
Frequentist estimates can fulﬁll this purpose. See Section 4.4.2 for an example of
hierarchical modeling and Chapter 5 for an example of a parameter space of
models, estimated with frequentist statistics, having AIC weights that can be inter-
preted as Bayesian posterior probabilities. See Carlin and Louis (2000) for a more
detailed account of empirical Bayes methods.
2.5.5
Sensitivity Analysis: An Example
Since prior distributions inﬂuence posterior distributions, priors must be chosen with
care. If the choice of priors is controversial, it is wise to include a sensitivity analysis
in the methodology, examining the effects of a realistic range of plausible priors on
the posterior distributions and statistics. If the posteriors and their statistics are mark-
edly different for varying priors, the results are sensitive to the choice of priors, and
74
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

that choice must be made with extreme caution. Alternatively, if the posteriors are
similar regardless of priors, the posteriors are robust to the choice of priors, and
the choice of priors is not such a critical issue. As the size of the sample dataset
used in the analysis increases, the effect of the prior on the posterior is diminished,
as demonstrated in Section 2.4.1.
Let’s illustrate a simple sensitivity analysis with the binary dataset example of
Sections 2.2.4 and 2.4.3 where y ¼ 20 Northern Spotted Owl occupied sites were
Figure 2.12. Sensitivity analysis results for the binary dataset with the binomial model B(y; n,
p) ¼ B(20; 80, p). (a) Noninformative beta priors for p: Jeffreys prior “beta1” ¼ BE( p; a ¼
0.5, b ¼ 0.5) (“J”), uniform prior “beta2” ¼ BE( p; a ¼ 1, b ¼ 1) (“U”), and skeptical prior
“beta3” ¼ BE( p; a ¼ 2, b ¼ 2) (“S”). (b) Beta posteriors for beta priors for p: beta posterior
“beta4” ¼ BE( p; a ¼ 20.5, b ¼ 60.5) (“J”) for Jeffreys prior “beta1” ¼ BE( p; a ¼ 0.5, b ¼
0.5), beta posterior “beta5” ¼ BE( p; a ¼ 21, b ¼ 61) (“U”) for uniform prior “beta2” ¼
BE( p; a ¼ 1, b ¼ 1), and beta posterior “beta6” ¼ BE( p; a ¼ 22, b ¼ 62) (“S”) for skeptical
prior “beta3” ¼ BE( p; a ¼ 2, b ¼ 2). (c) Means, variances, medians, and 95% credible
intervals of the posteriors in (b) using the S-Plus or R code (c) Means, variances, medians,
and 95% credible intervals of the posteriors in (b) using the S-Plus or R code.
2.5
OTHER PRIORS
75

detected in n ¼ 80 samples. Here the model is binomial B(y; n, p) ¼ B(y ¼ 20; n ¼
80, p) with the proportion parameter p. Let’s choose a plausible range of “noninfor-
mative” priors for the sensitivity analysis, using beta conjugate priors for the binomial
model. We assume that there is no other information available about p, with no pre-
vious studies or plausible alternatives to noninformative priors. Let’s examine three
noninformative priors: the transformation-invariant Jeffreys prior BE( p; 0.5, 0.5)
(Section 2.5.2), the uniform prior BE( p; 1, 1), and a “skeptical” prior BE( p; 2, 2)
(Fig. 2.12a). The skeptical prior places a higher weight of credibility, or prior prob-
ability, on the parameter p being closer to 50% than nearer to 0% or 100%. By con-
jugacy theory, the posterior distributions of these priors are BE( p; 20.5, 60.5), BE( p;
21, 61), and BE( p; 22, 62), respectively (Fig. 2.12b). As the ﬁgure reveals, their pos-
terior distributions are very similar. Furthermore, their statistics, the posterior means,
variances, medians, and 95% credible interval limits are very similar (Fig. 2.12c).
The relative differences in the posterior statistics for the different priors are small,
Figure 2.12. Continued.
76
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

bounded by 2%. The dataset was large enough to dominate the differences between
the priors in the posteriors. In conclusion, where there is controversy regarding the
selection of priors, natural resource scientists can examine such differences with a
sensitivity analysis to assess whether they are biologically and statistically important.
2.6
SUMMARY
This chapter provided an introduction to Bayesian statistical inference. We began
with some historical background. We discussed some inherent limitations of frequen-
tist statistical inference for natural resource datasets. We presented three methods
commonly used by statisticians for ﬁtting models to datasets, least-squares (LS) ﬁt,
maximum-likelihood (ML) ﬁt, and Bayesian ﬁt, and illustrated with examples of con-
tinuous data with the normal model, count data with the Poisson model, and binary
data with the binomial model. We introduced the Bayesian paradigm for statistical
inference, with a heuristic proof to the Bayes Theorem. We discussed conjugate
priors for datasets and models with closed-form solution, presenting important
examples for continuous, count, and binary datasets with normal, Poisson, and bino-
mial models, respectively. We described other alternatives for priors: noninformative
priors, uniform priors, Jeffreys priors, reference priors, proper versus improper priors,
vague priors, elicited prior, and empirical Bayes methods. We concluded this chapter
with a discussion of sensitivity analysis for priors, along with an example.
PROBLEMS
2.1
An occupied Marbled Murrelet nesting site is repeatedly and independently
visited m times. There is a probability of at least 50% of detecting the murrelet
nesting activity with each site visit.
(a)
How many visits m are required to ensure that there is at least a 95% prob-
ability of at least one detection of nesting activity, with the m repeated
visits?
(b)
How is the probability of at least one detection increased, if the site is
visited (m þ 1) times; that is, what is the probability of at least one detec-
tion, with (m þ 1) visits?
(c)
What is the probability of at least one detection with (m þ 1) visits, if the
ﬁrst m visits result in no detections? Solve this conditional probability
problem using Bayes’ Theorem. How does this conditional probability
result compare with the unconditional results? What property of the
visit events is illustrated with this result ?
2.2
Bald Eagles are nesting at just one of three possible sites in the Lewiston Lake
watershed. The local wildlife ofﬁcer knows which one of the sites is occupied.
PROBLEMS
77

You do not know which site is occupied. You are given one of three possible
strategies for deciding which site to visit:
(a)
You can choose one of the three sites.
(b)
You tentatively select one of the sites. The wildlife ofﬁcer then identiﬁes
one of the other two sites that is unoccupied. You choose the remaining site.
(c)
You tentatively select one of the sites. The wildlife ofﬁcer then identiﬁes
one of the other two sites that is unoccupied. You keep your original choice.
Which of the three strategies will maximize your probability of correctly
choosing the occupied site?
2.3
Consider the population monitoring scenario as described in Chapter 1. A
coastal northern California forest timberlands is being monitored for occupancy
or nonoccupancy of Marbled Murrelets, with annual surveys conducted each
summer during the nesting season. The sites are chosen independently each
year with simple random sampling. Sample sites are visited several times
each summer season to ensure a high probability of detection if the murrelets
are occupying the sites, so we will assume this conditional probability of detec-
tion to be equal to 1. The binary data for the ﬁrst four summer surveys are as
follows (with y ¼ number of occupied sites, n ¼ sample size):
(a)
y1 ¼ 31, n1 ¼ 100
(b)
y2 ¼ 13, n2 ¼ 50
(c)
y3 ¼ 22, n3 ¼ 100
(d)
y4 ¼ 21, n4 ¼ 100
The sampling was reduced the second year because of budgetary constraints. A
critical threshold level of 25% occupancy of the timberlands has been mandated
by regulatory agencies for the timberlands to be sufﬁciently occupied by mur-
relets to ensure the local viability of the species.
(a)
Conduct three Bayesian statistical analyses of the ﬁrst summer’s dataset,
using conjugate beta priors with (a, b) ¼ (1, 1), (0.5, 0.5), and (2, 2),
respectively. Compare the means, medians, and 95% credible interval
limits of the posteriors for these three priors. Are the results sufﬁciently
robust regardless of priors; that is, are the differences between the pos-
terior statistics for the means, medians, and credible interval limits
within an acceptable 3% relative “error” rate? Compute the risk, the prob-
ability, of not being above the critical 25% threshold for each prior. Is the
risk of not being above the critical 25% threshold below 50% for each
prior?
(b)
Assume the Jeffreys beta prior with a ¼ b ¼ 0.5 for the ﬁrst year’s data
and conduct a sequential, cumulative Bayesian statistical analysis on the 4
years of monitoring data. Use the posterior from the previous year’s analy-
sis for the prior of the following year. What do you conclude is the risk of
not being above the critical 25% threshold of occupancy of the forest
78
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

timberlands of marbled murrelets for each year? Is the risk, of not being
above the critical 25% threshold, below 50% for each of the cumulative
years of monitoring? Additionally, is there any disturbing trend for the
risk over the four years of monitoring and, if so, how might the data be
analyzed to take into account this trend? [Hint: look at the change in
the odds ratio of the risk OR(risk) ¼ probability(risk)/probability(non-
risk) for each year’s analysis.] We will examine this change in the odds
ratio in further detail in Chapter 3.
(c)
In a rebuttal by the logging company, their attorney cites a previous
sample survey of the company timberlands taken 5 years earlier that deter-
mined that y ¼ 10 of n ¼ 30 sample sites were occupied by murrelets. If
these earlier data are used to estimate a beta prior for p with estimated
mean m ¼ pˆ ¼ (y/n) ¼ 10
30 ¼ 0.333 and estimated standard deviation
s ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p  (1  ^p)
n  1
r
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1=3  2=3
29
r
¼ 0:0875
for the ﬁrst year’s data analysis, will the annual results of new sequential
analysis yield similar or different results for the risk of not being above
the critical 25% threshold with a probability below 50%, compared to
the previous analysis that did not include the logging company’s
dataset? If different, demonstrate how the change in the odds ratio of
the risk might be used to indicate that there is still a disturbing trend in
the risk.
2.4
A Marbled Godwit monitoring survey is conducted in a local marsh conserva-
tion habitat. Godwit count data fy1, y2, . . . , yng are independently collected for
each of 3 years on 0.1-ha hectare ﬁxed-area plots using a simple random
sampling design. The total number of counts y ¼ P
iyi and sample size n for
each year are as follows:
(a)
y1 ¼ 400, n1 ¼ 100
(b)
y2 ¼ 260, n2 ¼ 75
(c)
y3 ¼ 140, n3 ¼ 50
Use an approximate Jeffreys gamma prior with a ¼ b ¼ 0.001 to conduct a
sequential, cumulative Bayesian statistical analysis on the datasets. The moni-
toring plan for the marsh conservation habitat speciﬁes that a critical mean
threshold density level of three godwits per plot be maintained to ensure the
local viability of the population. Does the analysis for the monitoring data indi-
cate that the threshold has been exceeded with a probability of 50% or better,
for each of the 3 years? Are there are any indications of possible trouble
ahead? (Hint: Look at the odds ratio of the risks.)
PROBLEMS
79

2.5
Discuss in general the relative effects of the sample on the mean and precision
of the posterior, using the mean parameter of continuous normally distributed
data as an approximate guideline. If the prior distribution is noninformative,
then
(a)
What will be the approximate mean and precision of the posterior?
(b)
How will the precision and standard deviation of the posterior be affected
by a doubling of the sample dataset?
(c)
What must be done to the sample size to half the standard deviation of the
posterior?
80
BAYESIAN STATISTICAL ANALYSIS I: INTRODUCTION

3
Bayesian Statistical Analysis II:
Bayesian Hypothesis Testing and
Decision Theory
In this chapter we will present an introduction to Bayesian hypothesis testing and
decisionmaking. We will demonstrate how the Bayesian approach can consider
more than one or two hypotheses with its method of hypothesis testing. Bayesian
statistical inference can also test hypotheses directly, examining the probability of
hypotheses conditional to the data rather than the probability of data conditional to
the hypotheses. Bayesians can assess either the status of the probability of the hypoth-
esis or the ratio of the odds of the hypothesis, indicating the rate of change of the odds
conditional to the data. Bayesians can also assess decisionmaking with a statistical
decision theory fully integrated into Bayesian statistical analysis.
3.1
BAYESIAN HYPOTHESIS TESTING: BAYES FACTORS
In Chapter 1 we described various objections to the use of classical frequentist analy-
sis and inference in the context of contemporary natural resource science. The fre-
quentist logic for hypothesis testing is indirect, providing inferences based on the
probability of data, conditional to null hypothesis assumptions about the parameter.
A direct inference would reverse this logic, providing inferences for the probability of
hypotheses, conditional to the data. The frequentist paradigm for hypothesis testing
subjects one hypothesis, the null hypothesis H0, to a test, in contrast to its alternative
hypothesis HA. It would be more realistic in natural resource science to be able to test
a multitude of hypotheses, to assess their relative probabilities, given sample data. It
would also be more useful to incorporate prior information into hypothesis testing
rather than relying solely on independent assessments. In summary, it would be
useful for the natural resource scientist to have a method of hypothesis testing
that is direct, allows a multitude of hypotheses, and takes advantage of prior
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
81

information. All of these properties can be satisﬁed with the Bayesian approach to
hypothesis testing.
A Bayesian tests a hypothesis H consisting of a subset of parameter values u in H
contained in a parameter space F (i.e., H # F) by examining the posterior distri-
bution pposterior ¼ p(ujy) of the parameter space based upon the experimental
dataset y and calculating the posterior probability of the hypothesis
pposterior(Hjy)
and its odds ratio relative to the prior distribution
pposterior(Hjy)=(1  pposterior(Hjy))
pprior(Hjy)=(1  pprior(Hjy))
:
Such an odds ratio approach is analogous to the frequentist use of descriptive statistics
describing the hypothesis with an odds ratio test. The Bayesian, however, calculates
probabilities for the hypothesis conditional to the data, whereas the frequentist calcu-
lates probabilities for the data conditional to the hypothesis.
A Bayesian can also test competing hypotheses H1 and H2, in a manner analogous
to the frequentist likelihood ratio testing protocol, by examining the Bayes factor
(Jeffreys 1961)
BF(H1=H2jy) ¼ pposterior(H1jy)=pposterior(H2jy)
pprior(H1)=pprior(H2)
:
The Bayes factor is the odds ratio of the posterior odds to the prior odds of the hypoth-
esis H1 relative to H2. Note that H1 and H2 need not be nested, as is required with
frequentist hypothesis testing using likelihood ratio tests. Jeffreys recommends the
following guidelines for degrees of evidence for H1 and H2, based on ranges of
Bayes factor values:
1. BF(H1/H2jy) , 1/100: decisive evidence for H2
2. 1/100  BF(H1/H2jy) , 1/10: strong evidence for H2
3. 1/10  BF(H1/H2jy) , 1/p10: substantial evidence for H2
4. 1/p10  BF(H1/H2jy) , 1: minimal evidence for H2
5. 1  BF(H1/H2jy) , p10: minimal evidence for H1
6. p10  BF(H1/H2jy) , 10: substantial evidence for H1
7. 10  BF(H1/H2jy) , 100: strong evidence for H1
8. BF(H1/H2jy) . 100: decisive evidence for H1
Note that H2 may be set equal to the set complement of H1, H2 ¼ F2H1, the remain-
der of the parameter space, and we are back to the Bayesian method of testing one
hypothesis H1. Let’s look at some examples.
82
BAYESIAN STATISTICAL ANALYSIS II: BAYESIAN HYPOTHESIS TESTING

3.1.1
Proportion Estimation of Nesting Northern Spotted Owl Pairs
A habitat conservation plan for a timber company requires that H1: p  60% of its
ownership be occupied by nesting Northern Spotted Owl pairs to ensure a locally
viable population. The prior odds for H1/H2 ¼ H1/(F2H1), with H2: p , 60%
the complement of H1, based on a ﬂat noninformative prior BE( p;1,1), is
pprior(H1)/pprior(H2) ¼ 0.40/0.60 ¼ 0.67. A random sample survey of n ¼ 100
sites of binary data with the binomial model provide y ¼ 70 occupied sites and a pro-
portion estimate of ˆp ¼ 70
100 ¼ 70%. The posterior distribution based on the conjugate
prior is BE( p;71,31) with posterior odds pposterior(H1jy)/pposterior(H2jy) ¼ 0.979/
0.021 ¼ 46.619 and Bayes factor
BF(H1=H2jy) ¼ pposterior(H1jy)=pposterior(H2jy)
pprior(H1)=pprior(H2)
¼ 0:979=0:021
0:40=0:60 ¼ 46:619
0:667 ¼ 69:894
providing strong evidence for H1: p  0.60. The probabilities for the posterior beta
distribution are calculated using the S-Plus or R cumulative distribution command
[e.g., pbeta (0.60,71,31) for H2]. The posterior probability of H1: p  0.60,
pposterior(H1jy) ¼ 97.9%, is also compelling evidence for H1, based on the dataset
(Fig. 3.1a).
Alternatively, if the estimate for p from the sampling had been ^p ¼ 62
100 ¼ 62%,
based on y ¼ 62 with n ¼ 100, then the posterior probability for H1 would have
been pposterior(H1jy) ¼ 0.648 and the Bayes factor would have been BF(H1/
H2jy) ¼ 2.761, based on the posterior distribution BE( p;63,39) (Fig. 3.1b). This
would have been much less compelling evidence in favor of H1, just minimal evi-
dence, although the posterior probability of H1: p  0.60 would be 65.0%, well
over 50%. Note, therefore, that the Bayes factor “measures” the change of the odds
from the prior to the posterior distributions, derived from the analysis of the data
that reassesses the distribution of the parameter. The Bayes factor is a relative
measure, based not solely on the current odds of the hypothesis in the posterior dis-
tribution, but on its change relative to the odds in the prior distribution. In conclusion,
natural resource scientists should take both Bayes factors and posterior distribution
probabilities and odds into account before drawing conclusions about their
hypotheses.
3.1.2
Medical Diagnostics
Medical diagnostics provides an example that is perhaps outside the professional
realm of the natural resource scientist. It is a context, however, that is familiar
to most of us and provides an illustrative example of Bayes factors and Bayesian
decision theory that will generalize to a natural resource setting. Consider the
3.1
BAYESIAN HYPOTHESIS TESTING: BAYES FACTORS
83

dilemma presented when someone visits a physician’s ofﬁce to report symptoms
suggesting the possibility of a certain disease such as lung cancer. Let’s say that
the patient is a white male who has lived in Los Angeles most of his life, and is a
smoker who doesn’t exercise. His symptoms are a severe cough, a pain in the
chest, and several other abnormal indicators. The physician is able to reference stat-
istics that estimate that a patient with these characteristics and identiﬁable symptoms
has a 10% probability of having the disease. The physician and patient are consider-
ing conducting a test, such as a computerized tomography (CT) scan, to obtain
additional information on the condition of the patient, to help decide whether to
Figure 3.1. Posterior distributions BE( p;a,b) for the binary y with binomial models B(y;n,p) in
Section 3.1.1 with posteriorprobability regions for hypothesis H1: p  60%. (a) BE(p;71,31) with
y ¼ 70 and sample size n ¼ 100, and noninformative prior BE (p;1,1). (b) BE (p;63,39) with
y ¼ 62 and sample size n ¼ 100, and noninformative prior BE (p;1,1).
84
BAYESIAN STATISTICAL ANALYSIS II: BAYESIAN HYPOTHESIS TESTING

operate. Let’s say that the physician also has information, on the basis of previous
scientiﬁc studies, indicating that the sensitivity and speciﬁcity of the test, the CT
scan, are 95% and 80%, respectively. The sensitivity of the test is the probability
of a positive result, if the patient has the disease. The speciﬁcity of the test is the prob-
ability of a negative result, if the patient doesn’t have the disease. So there is a 5%
chance of error of a negative test result if the patient has the disease and a 20%
chance of error of a positive test result if the patient does not have the disease.
The initial 10% statistic for the probability of the patient having the disease pro-
vides a Bayesian prior probability distribution for a parameter space. The parameter
space D is categorical, ﬁnite, and discrete, with two values, say, D0 for the non-
diseased state and D1 for the diseased state. The dataset T consists of two values,
T ¼ T2 for a negative test result, and T ¼ Tþ for a positive test result. If the
patient does not have the disease, then D ¼ D0 and the probability distribution for
the data consists of two values:
p(TjD0) ¼ specificity,
p(TþjD0) ¼ (1  specificity):
If the patient does have the disease, then D ¼ D1 and the probability distribution for
the data is given by
p(TjD1) ¼ (1  sensitivity),
p(TþjD1) ¼ sensitivity:
The prior distribution for D, based on the researched statistics, is given by
pprior(D0) ¼ 0:90,
pprior(D1) ¼ 0:10:
From Bayes’ Theorem, the posterior distribution for D, with negative test results T2,
is given by
pposterior(D0jT) ¼
p(TjD0)  pprior(D0)
p(TjD0)  pprior(D0) þ p(TjD1)  pprior(D1)
¼
specificity  pprior(D0)
specificity  pprior(D0) þ (1  sensitivity)  pprior(D1)
¼
0:80  0:90
0:80  0:90 þ 0:05  0:10 ¼ 0:993
3.1
BAYESIAN HYPOTHESIS TESTING: BAYES FACTORS
85

and
pposterior(D1jT) ¼
p(TjD1)  pprior(D1)
p(TjD0)  pprior(D0) þ p(TjD1)  pprior(D1)
¼
(1  sensitivity)  pprior(D1)
specificity  pprior(D0) þ (1  sensitivity)  pprior(D1)
¼
0:05  0:10
0:80  0:90 þ 0:05  0:10 ¼ 0:007:
For positive test results Tþ, the posterior distribution for D is given by
pposterior(D0jTþ) ¼
p(TþjD0)  pprior(D0)
p(TþjD0)  pprior(D0) þ p(TþjD1)  pprior(D1)
¼
(1  specificity)  pprior(D0)
(1  specificity)  pprior(D0) þ sensitivity  pprior(D1)
¼
0:20  0:90
0:20  0:90 þ 0:95  0:10 ¼ 0:655,
pposterior(D1jTþ) ¼
p(TþjD1)  pprior(D1)
p(TþjD0)  pprior(D0) þ p(TþjD1)  pprior(D1)
¼
sensitivity  pprior(D1)
(1  specificity)  pprior(D0) þ sensitivity  pprior(D1)
¼
0:95  0:10
0:20  0:90 þ 0:95  0:10 ¼ 0:345:
So, if the test is positive, the posterior probability of having the disease is 34.5%,
whereas if the test is negative, the posterior probability of having the disease is
just 0.7%.
The Bayes factor is obtained by examining the ratio of the prior and posterior odds
BF(D1=D0jTþ) ¼ pposterior(D1jTþ)=pposterior(D0jTþ)
pprior(D1)=pprior(D0)
¼ 0:345=0:655
0:10=0:90 ¼ 4:74,
BF(D1=D0jT) ¼ pposterior(D1jT)=pposterior(D0jT)
pprior(D1)=pprior(D0)
¼ 0:007=0:993
0:10=0:90 ¼ 0:063:
So the odds of having the disease increase by approximately 5-fold if the test results
are positive, and decrease by approximately 20-fold if the test results are negative. Is it
86
BAYESIAN STATISTICAL ANALYSIS II: BAYESIAN HYPOTHESIS TESTING

worthwhile to take the test? This is a decision for the patient, in consultation with the
physician. However, this decision may also depend on the ﬁnancial costs and the
health risks involved with this process, and it would make sense to consider these
factors as well. Bayesian statistical analysis and inference has been nicely integrated
with decision theory to provide a rigorous answer to this question. We will introduce
this idea in the next section.
Before leaving this topic, however, let’s point out that, for this simple example of
medical diagnostics with a two-valued parameter space and a two-valued dataset,
there is a simple general formula for the Bayes factor. Returning to the formulas
above for the posterior distribution probabilities, the Bayes factors reduce algebrai-
cally to simple functions of sensitivity and speciﬁcity
BF(D1=D0jTþ) ¼ pposterior(D1jTþ)=pposterior(D0jTþ)
pprior(D1)=pprior(D0)
¼ sensitivity  pprior(D1)=(1  specificity)  pprior(D0)
pprior(D1)=pprior(D0)
¼
sensitivity
(1  specificity) ,
BF(D0=D1jT) ¼ pposterior(D0jT)=pposterior(D1jT)
pprior(D0)=pprior(D1)
¼ specificity  pprior(D0)=(1  sensitivity)  pprior(D1)
pprior(D0)=pprior(D1)
¼
specificity
1  sensitivity :
Similarly, it can be shown that
BF(D1=D0jT) ¼ (1  sensitivity)
specificity
,
BF(D0=D1jTþ) ¼ (1  specificity)
sensitivity
:
Although natural resource scientists may be wondering how this medical example
relates to applications in their disciplines, analogous examples may be found in
natural resource applications. Habitat selection modeling with logistic regression pro-
vides one such example. An important diagnostic in examining the comparative
goodness of ﬁt of logistic regression models is the receiver operating curve (ROC)
c statistic, which measures the area under the sensitivity/(1 2 speciﬁcity) curve for
various cutoff points (see Chapter 6). Hence, we see that this comparative statistic
measures the Bayes factors for various cutoff points of logistic regression models.
3.1
BAYESIAN HYPOTHESIS TESTING: BAYES FACTORS
87

The model with the highest Bayes factors overall has the highest c statistic and is the
best-ﬁtting overall model.
The original case study problem of deciding whether a critical threshold for a par-
ameter has been exceeded that was described in Section 1.1 can also be couched in
the same framework as this medical problem. The two states for the parameter space
of exceeding the threshold or not exceeding the threshold are analogous to the disease
state D1 and the nondisease state D0. Sensitivity is the probability of deciding from
the monitoring (i.e., the “test”) that the threshold has been exceeded (i.e., D1, the “dis-
eased” state). Speciﬁcity is the probability of deciding from the monitoring that the
threshold has not been exceeded. Sensitivity and speciﬁcity depend on the sample
sizes used for the monitoring and can be determined by the probabilities calculated
from the binomial model and the prior distribution for the parameter p. A particular
example of this application will be illustrated later in the problem assignments at the
end of this chapter.
3.2
BAYESIAN DECISION THEORY
An additional advantage to Bayesian statistical inference is that it has been fully inte-
grated with decision theory to provide a completely coherent theory, Bayesian
decision theory. This theory incorporates losses
l(u,a) ¼ l(u,d( y))
that are a function of parameters u and actions a ¼ d(y) resulting from decisions d
that are based on data y. A loss may be interpreted as an expense or opportunity
cost or some other liability. A gain or beneﬁt or proﬁt from the decision is considered
a negative loss. This results in a risk function that is the expected loss
R(u,d) ¼ E½l(u, d( y))
¼
P
i
l(u,d( yi))  f ( yiju)
(discrete case)
Ð l(u,d( yi))  f ( yju)  dy
(continuous case)
;
(
a function of the parameter u and the decision d. The risk function is the average loss,
the expectation weighted with respect to the conditional probability distribution func-
tion f(yju) of the data y, given parameter value u. The Bayes risk B(d) of a decision d
is the average risk over the distribution function of the parameter
B(d) ¼ E½R(u,d)
¼
P
i
R(ui,d)  p(ui)
(discrete case)
Ð R(u,d)  p(u)  du
(continuous case)
(
88
BAYESIAN STATISTICAL ANALYSIS II: BAYESIAN HYPOTHESIS TESTING

The Bayes rule is the principle of choosing the decision d that minimizes the Bayes
risk B(d). The idea is simple in concept; look at the risk functions of the decision
options and select the decision that minimizes your risk. In practice, however, this
may be difﬁcult to apply. In particular, it may be difﬁcult to assign losses with sufﬁ-
cient precision to apply the theory with practical value. Even if that is the case,
however, Bayesian decision theory is still useful because it points out the important
role that costs and beneﬁts play, along with probabilities, in the decision-
making process.
Bayes risk and the Bayes rule are formulated in terms of a distribution function of
a parameter, say, a prior distribution. Suppose you collect data y and calculate the
posterior risk of an action a ¼ d(y) as the expected loss where the expectation is
taken with respect to the posterior distribution of the parameter u. Note that the pos-
terior risk is deﬁned solely in terms of a sample dataset and not the entire population,
unlike the Bayes risk, which requires the entire population of data. It can be shown
that a decision d ¼ d(y) that minimizes the posterior risk is a Bayes rule. So
Bayesian decision theory permits a choice of decisions to be based on sample data,
rather than the entire population of data. The interested reader may seek further
details in Rice (1995, Chapter 15).
Let’s illustrate Bayesian decision theory with the medical diagnostics example in
Section 3.1.2. Let’s consider a decision process d1 where a treatment is applied if the
test is positive Tþ and not applied if the test is negative T2. We specify the following
losses for the data, the test results, for this decision d1 with the two parameter values,
D0 and D1. If u ¼ D0, we will assign losses
l(u,a) ¼ l(D0,d1(T)) ¼ 0
¼ l(D0,d1(Tþ)) ¼ L0
þ . 0:
If u ¼ D1, we will assign losses
l(u,a) ¼ l(D1,d1(Tþ)) ¼ 0
¼ l(D1,d1(T)) ¼ L1
 . 0:
If the patient does not have the disease, we are assigning a zero loss for a negative test
result and a positive loss L þ
0
for a positive test result and the potentially resulting
unnecessary treatment. If the patient does have the disease, we are assigning a zero
loss for a positive test result and a positive loss L 2
1
for a negative test result and
the potentially resulting failure to apply treatment for the disease. The risk function
for this decision for the two parameter values is given by
R(D0,d1) ¼ l(D0,d1(T))  p(TjD0) þ l(D0, d1(Tþ))  p(TþjD0)
¼ 0  specificity þ L0
þ  (1  specificity)
¼ L0
þ  (1  specificity)
3.2
BAYESIAN DECISION THEORY
89

and
R(D1,d1) ¼ l(D1,d1(T))  p(TjD1) þ l(D1,d1(Tþ))  p(TþjD1)
¼ L1
  (1  sensitivity) þ 0  sensitivity
¼ L0
þ  (1  sensitivity):
The Bayes risk is the average of this risk over the parameter space
B(d1) ¼ R(D0,d1)  p(D0) þ R(D1,d1)  p(D1)
¼ L0
þ  (1  specificity)  pprior(D0)
þ L1
  (1  sensitivity)  pprior(D1):
So the Bayes risk is affected by the losses, the probabilities arising from incorrect
decisions, and the probabilities of having the disease or not, as we would expect.
This risk can be compared with Bayes risks for other alternative decisions using
the Bayes rule. For example, it is interesting to compare this decision process with
the alternative decisions of “no treatment regardless of test results” or “treatment
regardless of test results.” This question will be examined in more detail in the
problems at the end of this chapter.
3.3
PREVIEW: MORE ADVANCED METHODS OF
BAYESIAN STATISTICAL ANALYSIS—MARKOV
CHAIN MONTE CARLO (MCMC) ALGORITHMS
AND WinBUGS SOFTWARE
Datasets with conjugate priors, such as continuous, count, and binary datasets with
normal, Poisson, and binomial models, respectively, are important datasets com-
monly sampled by natural resource scientists. These datasets can be analyzed with
closed-form mathematical solutions, as was illustrated in Chapter 2. However,
many other important natural resource datasets require more sophisticated models
that are not necessarily solvable with closed-form mathematical solutions. The analy-
sis of many natural resource datasets requires the use of more complicated models
such as multiple linear regression models and generalized linear models. Binary
and count datasets may be overdispersed and require models such as hierarchical
models or mixed-effects models with random effects. In Chapter 4, we will discuss
ways of solving more complicated problems with Bayesian statistical analysis
using Markov Chain Monte Carlo (MCMC) algorithms. These algorithms, developed
since the mid-1950s, sample parameter values from posterior distributions that
provide solutions to general Bayesian problems. The MCMC algorithms generate
Markov chains of values for iterative solutions to the Bayesian analysis. The
MCMC algorithms have now been implemented in WinBUGS software that can be
90
BAYESIAN STATISTICAL ANALYSIS II: BAYESIAN HYPOTHESIS TESTING

downloaded from the Web and is free and relatively easy to use for the natural
resource scientist.
3.4
SUMMARY
In this chapter we presented an introduction to Bayesian hypothesis testing of mul-
tiple hypotheses using Bayes factors. Bayes factors are odds ratios representing the
change in probability of hypotheses from the prior to posterior distributions. We illus-
trated these concepts with an example based on binomial data. We also introduced
Bayesian decision theory using a medical example for illustration. The medical
example can be extrapolated to the case study 1 example of Chapter 1 of determining
whether a parameter has exceeded a critical threshold. We concluded by brieﬂy pre-
viewing the ideas that will be presented in the next chapter: Bayesian statistical analy-
sis of more complex natural resource datasets using MCMC algorithms that are
implemented with WinBUGS software.
PROBLEMS
3.1
A logging company is initiating a habitat restoration plan for their timberlands
and monitoring the progress of an endangered species as they implement the
plan. The monitoring procedure calls for annual surveys of presence–absence
data using simple random sampling to estimate the proportion of the habitat
occupied by the endangered species. The objective of the restoration plan is
to restore the presence of the species to at least 75% of the timberlands. The
ﬁrst 2 years of survey data resulted in the species being present in 60 and 74
of 100 sites, respectively, independently sampled each year. Assuming a
uniform beta prior for the ﬁrst year in a sequential Bayesian statistical analysis,
what are the posterior distributions for each of the ﬁrst two years of analysis?
What are the posterior probabilities of exceeding the 75% threshold each
year? What are the Bayes factors for the analysis in each of the 2 years? On
the basis of the posterior probabilities of exceeding the threshold and the
Bayes factors, what are your conclusions about the status and progress of the
owl under the habitat restoration plan?
3.2
A watershed study of the viability of a bird population includes the monitoring
of trends in its abundance with bird count data collected independently
annually on 50 randomly selected sites. The ﬁrst 3 years of data collection
yield total counts of Siyi ¼ 30, 48, and 55, respectively. Assuming certainty
of detection and random spatial distribution of the birds, analyze the
datasets for each of the 3 years, using gamma conjugate priors with an initial
Jeffreys gamma prior G(l;aprior ¼ 0.001, bprior ¼ 0.001) for the ﬁrst year’s
data and a sequential Bayesian statistical analysis. Examine the Bayes factors
PROBLEMS
91

for the ﬁrst, second, and third year’s analyses. What is the degree of evidence in
support of the hypothesis H1 that the mean plot density of the birds is exceeding 1
H1 : l . 1?
3.3
A patient has symptoms indicating a 5% chance of having of a malevolent form
of cancer. Her physician proposes a DNA scan testing procedure with a sensi-
tivity of 80% and speciﬁcity 95%. If the patient takes the test and it is positive,
how would that increase the assessment of the patient’s odds of having the
cancer, if she has the cancer and if she doesn’t have the cancer? If the test is
negative, how would that decrease the assessment of her odds of having the
cancer, if she has the cancer and if she doesn’t have the cancer?
3.4
Consider the general medical diagnostics decision problem of Section 3.2 with
decision d1 having Bayes risk B(d1) ¼ L0
þ. (1 2 speciﬁcity) . pprior(D0) þ L1
2.
(1 2 sensitivity) . pprior(D1). Consider the alternative decisions d2 of no treat-
ment without the test with loss L ¼ L1
2 if the patient has the disease and 0
otherwise, and d3 of treatment without the test with loss L ¼ L0
þ if the
patient does not have the disease and 0 otherwise. What are the Bayes risks
for these decisions d2 and d3? Use Bayes’ rule to determine when each
decisions is optimal in minimizing risk. What does the Bayes rule suggest
about the need for testing under certain circumstances?
3.5
In the medical example above in Problems 3.3 and 3.4, suppose that the cost of
the treatment and its side effects is estimated to be L0
þ ¼ $3000. Suppose also
that the opportunity cost of the delay caused by waiting until other symptoms
appear if the patient does have cancer is estimated to be L1
2 ¼ $20,000.
What is the Bayes risk of the decision d1 to take the DNA scan and apply
the treatment to correct the disease depending on the results of the test? How
does this compare with the Bayes risk for the decision d2 of not taking the
test and not applying the corrective treatment? How does this compare with
the Bayes risk for the decision d3 of not taking the test but applying the treat-
ment to correct the disease without taking the test? Use the Bayes rule to deter-
mine which decision d1, d2, and d3 is optimal.
3.6
Extrapolate the concepts of Section 3.3 and Problem 3.4 to case study 1 of
Chapter 1 illustrated by the critical threshold problem in Section 3.1.1 for esti-
mating the of proportion of nesting Northern Spotted Owl pairs. What are sen-
sitivity and speciﬁcity in this context? What are the meanings of the losses L0
þ
and L1
2 in terms of the value of the restoration of the viability of the owls and
the opportunity costs of the harvesting of the timber? What does the Bayes rule
suggest about the need for monitoring of the owls under certain circumstances?
92
BAYESIAN STATISTICAL ANALYSIS II: BAYESIAN HYPOTHESIS TESTING

4
Bayesian Statistical Inference III:
MCMC Algorithms and WinBUGS
Software Applications
Machines should work;
people should think.
—IBM Pollyanna Principle
In this chapter we introduce the methods of Markov chain Monte Carlo (MCMC)
simulation and provide an overview of the Markov chain theory necessary to appreci-
ate the properties of the MCMC algorithms. We describe the leading MCMC algor-
ithms: Gibbs sampling and the Metropolis–Hastings algorithm, and conclude by
presenting WinBUGS applications of the MCMC algorithms for several examples.
The examples include the normal model for continuous data and the linear regression
model for paired data. For these examples, we compare the Bayesian results with non-
informative priors with frequentist results. We also include several other examples,
for count datasets using the Poisson model, for count datasets with overdispersion
using the negative binomial and mixed-effects models, and for datasets with hierar-
chical models.
4.1
INTRODUCTION
In Chapters 2 and 3 we introduced some of the principal ideas of Bayesian statistical
analysis and described some of the advantages of using this approach for statistical
inference with natural resource datasets. We included in this discussion the topic of
conjugate closed-form Bayesian analysis solutions for many important natural
resource datasets. However, many other important natural resource datasets require
models that are more complex, with hierarchical structures, random effects, and
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
93

large numbers of parameters with and without constraints. Closed-form Bayesian
analysis solutions for these models may be difﬁcult if not intractable to derive math-
ematically and may not be available or accessible to the natural resource scientist.
Bayesian statistical inference has been signiﬁcantly limited historically because of
this lack of solution to many practical problems.
All this has changed, however, since the mid-1950s with the development of
MCMC algorithms and their recent implementation with the Bayesian MCMC soft-
ware WinBUGS. Bayesian statistical analysis solutions using these algorithms are
now generally available and accessible for the natural resource scientist. We will
describe the most important of these MCMC algorithms in this chapter and discuss
their statistical properties, illustrating with examples in WinBUGS.
4.2
MARKOV CHAIN THEORY
Let’s begin by presenting an overview of the theory behind the MCMC methods that
provide general Bayesian analysis solutions for statistical models of natural resource
datasets. The mathematical details of this theory are beyond the scope of this book.
The interested reader, however, is referred to the many excellent references providing
more details and examples of these methods, including Metropolis et al. (1953),
Hastings (1970), Gemen and Geman (1984), Gelfand and Smith (1990), Gelman
(1992), Draper (2000), Carlin and Louis (2000), Congdon (2001), and Gill (2002).
One of the leading problems historically with the Bayesian approach to statistical
analysis has been the apparent requirement for the computation of complex integrals
that may be intractable. Recall that the formulation of the Bayesian solution in Bayes’
Theorem requires computation of the product of the prior times the likelihood of a
dataset, based on a model, for the numerator. This part of the computation presents no
problem. However, Bayes’ Theorem also requires, in the continuous case, computation
of the integral of this product with respect to the parameter in the denominator. This inte-
gral may be very complex, multidimensional, and intractable to mathematical closed-
form solution. Additionally, for multidimensional models with many parameters, the
marginal posterior distribution for each parameter must be calculated.
There are many important cases where these integrals can be calculated. Chapter 2
has revealed that the forms of the likelihoods of some models for datasets lend them-
selves to the use of particular families of prior distributions called conjugate priors.
Conjugate prior families of distributions provide closed-form solutions with pos-
teriors from the same family of distributions. Furthermore, the posterior solutions
are functions of the parameter values used for the priors and statistics computed
from the sample dataset. Conjugate priors for several important examples of
natural resource datasets were examined, including normal conjugate priors for the
normal model with continuous data, gamma conjugate priors for the Poisson
model with count data, and beta conjugate priors for the binomial model with
binary data. However, models for natural resource datasets are often far more compli-
cated: multidimensional, hierarchical, and intractable to closed-form solution. With
the advent of computers, it might seem that a Monte Carlo simulation approach,
94
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

using computer iteration to approximate solutions by sampling from the posterior dis-
tributions, would provide an effective solution. What is particularly interesting about
the MCMC simulation approach, however, is that the Monte Carlo samples are not
completely independent, but rather are dependent in a Markov chain.
The MCMC methods represent a collection of algorithms prescribing the sampling
of parameter values from posterior distributions that circumvent the need to calculate
the integrals required in Bayes’ Theorem. An MCMC algorithm prescribes a stochas-
tic process that samples random variable values from a state space (here, a parameter
space). The sampling consists of a sequence, or “chain,” of values. The stochastic
process consists of a Markov chain: each value ui ¼ uijui21 from the parameter
space that is sampled depends conditionally only on the previous value ui21. In
other words, any speciﬁed state in this chain is conditionally independent of all pre-
vious values except the last one. The process that deﬁnes the probabilities of moving
from one state to the next one in the chain is called the transition kernel. If the state
space has only a ﬁnite number of values, then the transition kernel can be described
by a matrix [ pij], where pij ¼ the probability of moving from state i to state j in the
chain. Typically, however, the parameter spaces will be inﬁnite and continuous, so a
speciﬁed rule will be required.
Furthermore, the Markov chain must satisfy certain properties in order to converge
to a stationary distribution where it will representatively sample the values from
the posterior distribution. The Markov chain must be ergodic; that is, it must be
irreducible, positive Harris recurrent, and aperiodic. Let’s describe each of
these properties.
A Markov chain is irreducible for a subspace A # S of the state space S if every
point or collection of points in the subspace can be reached from every other point or
collection of points in the subspace. The idea is to continue sampling every point in
the subspace while moving through the chain. For the Bayesian solution, the subspace
of importance consists of the support of the posterior distribution, the set of par-
ameter values in the parameter space with nonzero posterior probability.
An irreducible Markov chain is recurrent for a subspace A # S if the probability
that the chain occupies A inﬁnitely often over unbounded time is nonzero. An irredu-
cible recurrent Markov chain is positive recurrent if the mean amount of time [i.e.,
time between points ui and uj ¼ t(ui, uj)] required to return to A is bounded. The idea
is to be able to continue to sample every point not only in the subspace, the support of
the posterior distribution, but also on average within a ﬁnite bounded number of iter-
ations. With unbounded continuous state spaces, we need a slightly stricter deﬁnition
of recurrence, Harris recurrence, which guarantees that the probability of visiting A
inﬁnitely often in the limit is 1. We refer the reader to the more detailed technical
references given earlier for further explanation.
The last requirement is that the irreducible, positive Harris recurrent Markov chain
be aperiodic. The period of a Markov chain is the length of time required to repeat an
identical cycle of chain values. The requirement is that the Markov chain be aperio-
dic, where the only length of time for which the chain repeats some cycle of values is
the trivial case of cycle length 1. The requirement ensures that the Markov chain
reaches a state where it will sample representatively from the posterior distribution
4.2
MARKOV CHAIN THEORY
95

and not just repeat some cycle, even if that cycle includes every value from
the subspace.
In summary, the MCMC algorithms produce ergodic Markov chains that are irre-
ducible, positive Harris recurrent, and aperiodic. An ergodic Markov chain converges
to a subspace of the parameter space where it proportionally samples the posterior dis-
tribution with a stationary distribution. A Bayesian statistical analysis solution can be
provided, hence, by utilizing this property and simulating the sampling iteratively on
the computer, as speciﬁed by the MCMC algorithm. The sampling will require a
“burn-in period” until it converges to the stationary distribution in the Markov
chain. Sample values in the stationary chain can then be collected for a subset of suf-
ﬁcient size to represent the posterior distribution. Statistics and graphs of the samples
provide an empirical summary of the posterior distribution of the parameter space.
The statistics include means, medians, modes, percentiles, and credible interval
limits. No computation of the integral in Bayes’ Theorem is required, and marginal
distributions of the parameters can be obtained from the samples. Let’s look at the
most important MCMC algorithms.
4.3
MCMC ALGORITHMS
Many classical Monte Carlo methods provide Bayesian analysis solutions, such as
Monte Carlo integration, rejection sampling, classical numerical integration, and
importance sampling (Gill 2002). These methods produce sets of independent simu-
lated values from a desired probability distribution.
The MCMC algorithms, on the other hand, produce chains of simulated values
that are mildly dependent. Each simulated value is dependent upon the previous
value in the chain. The chains are ergodic, converging to the desired posterior distri-
butions of interest. Summary statistics and graphs of the convergent stationary
sampled values provide descriptions of the posterior distributions. The most import-
ant MCMC algorithms are Gibbs sampling and the Metropolis–Hastings algorithm,
which are described next.
4.3.1
Gibbs Sampling
The Gibbs sampler (Geman and Geman 1984, Gelfand and Smith 1990) is the most
widely used MCMC algorithm. Although it is less ﬂexible than the Metropolis–
Hastings algorithm, it is conceptually simpler. Given the parameter vector u ¼ (u1,
u2, . . . , uk) and the data y ¼ fy1, y2, . . . , yng, it assumes that the posterior p(ujy)
can be characterized by its complete set of full conditional distributions p(uijy,
u2i), for i ¼ 1, . . . , k, where u2i refers to the vector u without the ui component.
The conditional distributions p(uijy, u2i) will often be easier to express in closed
form than will either p(ujy) or p(uijy). The Gibbs sampling algorithm proceeds
as follows:
1. Choose starting values u[0] ¼ (u1
[0], u2
[0], . . . , uk
[0]).
96
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

2. At the ith iteration, complete the single cycle by drawing values from the
k distributions
u½i
1  p u1ju½i1
2
, u½i1
3
, . . . , u½i1
k1 , u½i1
k


,
u½i
2  p u2ju½i
1 , u½i1
3
, . . . , u½i1
k1 , u½i1
k


,
u½i
3  p ½u3ju½i
1 , u½i
2 , u½i1
4
, . . . , u½i1
k1 , u½i1
k


,
...
u½i
k1  p uk1ju½i
1 , u½i
2 , . . . , u½i
k2, u½i1
k


,
u½i
k  p ukju½i
1 , u½i
2 , . . . , u½i
k2, u½i
k1


:
3. Increment i and repeat until convergence. It can be shown that Gibbs sampling
produces an ergodic Markov chain (i.e., irreducible, positive Harris recurrent,
and aperiodic) that converges to a limiting distribution that is the posterior
distribution p(ujy) of the parameter vector. The objective of Gibbs sampling
is to move toward and then around this distribution, providing representative
samples throughout the posterior distribution.
Although the full set of conditional distributions is required in order to run the
Gibbs sampling algorithm, many examples lend themselves to this approach.
Hierarchical models, for instance, have natural conditioning structures in their formu-
lations. The interested reader may ﬁnd examples in Gill (2002, pp. 26–29, 313–317)
and Congdon (2001, pp. 216–217). Casella and George (1992) provide a very clear
basic introduction to Gibbs sampling in a paper originally entitled “Gibbs for kids.” It
can be shown that Gibbs sampling is actually a special case of the Metropolis–
Hastings algorithm.
Let’s conclude this section with an example of Gibbs sampling. We consider the
problem of estimating population size of an animal population with mark–recapture
estimation using Gibbs sampling as illustrated in Manly (1997). Assume that the
population is closed during a short sampling period, without births and deaths, and
without immigration or emigration. Suppose that M ¼ 50 animals are marked and
released into the population and allowed to mix freely with U unmarked animals.
A random sample is then taken, assuming a ﬁxed probability p of each animal
being captured, with m ¼ 25 marked animals and u ¼ 10 unmarked animals. We
will estimate U and p using Gibbs sampling, assuming that U can vary between 10
and 100 and that p can vary between 0.0 and 1.0 in increments of 0.01. We also
assume that the priors for U and p are noninformative uniform distributions, hence
constant
pprior(U, p) ¼ C:
4.3
MCMC ALGORITHMS
97

Since m  B(m; p, M ) and u  B(u; p, U) are both binomially distributed, we have a
probability distribution P for m and u, conditional to U and p, given by
P(m; u; U; p) ¼ B(m; p; M)  B(u; p; U)
¼ MCm  pm  (1  p)
Mm  UCu  pu  (1  p)Uu
and the likelihood–prior function for U and p given by
P(U, p; m, u) ¼ C  MCm  pm  (1  p)Mm UCu  pu  (1  p)Uu:
For Gibbs sampling, therefore, the conditional distribution of U, given p, is
P(U; p, m, u) / UCu  (1  p)U,
and the conditional distribution of p, given U, is
P( p; U, m, u) / pmþu  (1  p)MmþUu:
The S-Plus or R code for the Gibbs sampling uses the standardized forms of these
expressions in an iterative process with n iterations, starting with initial values
U1 ¼ 55 and p1 ¼ 0.50, as indicated in Fig. 4.1a. Figures 4.1b and 4.1c illustrate
the trajectories of (U, p) for the ﬁrst 100 and 1000 iterations. The means of the
samples for the ﬁrst 1000 iterations are U¯ ¼ 21.931 and p¯ ¼ 0.491.
4.3.2
The Metropolis–Hastings Algorithm
The groundbreaking event in the development of the MCMC algorithms occurred in
1953 when Nicholas Metropolis and his colleagues Arianna Rosenbluth, Marshall
Rosenbluth, Augusta Teller, and Edward Teller (developer of the hydrogen bomb)
published their algorithm in the Journal of Chemical Physics. Metropolis et al.
(1953) were interested in the potential positions of all molecules in an enclosure.
Their idea was, rather than try to track and calculate future positions, to establish a
molecule movement criterion and simulate a series of potential positions, to deter-
mine probabilistically where the molecules were. Their interest was focused at that
time primarily on nuclear physics. However, their algorithm was to have more far-
reaching application. Concurrently, throughout the 1950s and 1960s, mathematicians
were developing the foundations of ergodic Markov chain theory that explain the
properties of the simulated series of positions in the Metropolis algorithm.
In 1970 Hastings reﬁned and generalized the Metropolis algorithm, showing that a
reversibility condition could be substituted for a symmetry requirement. Hastings’
paper in Biometrika (Hastings 1970), along with a paper by Peskun (1973), helped
introduce these ideas to the statistics community. Two papers by Geman and
Geman (1984) and Gelfand and Smith (1990) later clariﬁed the usefulness of
98
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

Figure 4.1. Trajectories of the parameters U and p from Gibbs sampling for mark–recapture
analysis of a closed animal population with M ¼ 50 marked animals, U unmarked animals,
m ¼ 25 recaptured marked animals, and u ¼ 10 captured marked animals. (a) Gibbs sampling
program code in S Plus or R. (b) First 100 iterations. (c) First 1000 iterations.
4.3
MCMC ALGORITHMS
99

Gibbs sampling for formulating Markov chains to estimate posterior distributions.
Gibbs sampling is a special case of Metropolis–Hastings, and Metropolis is a special
case of Hastings. These algorithms, along with the explosion of computing power in
the 1990s, provide the practical tools for solving realistically formulated complex
general problems in natural resource science with Bayesian statistical analysis.
In simplest form, for a parameter u with posterior p(u) and data y ¼ fy1, y2, . . . ,
yng, the Metropolis–Hastings algorithm is as follows:
1. Choose a starting value u0.
2. At the ith iteration, given ui21,
a. choose a candidate value ui0 from a proposal distribution qi(ui0jui21). This
proposal distribution qi must have a support that includes the support of the
posterior distribution p(u).
b. Deﬁne the acceptance ratio
r(u0
ijui1) ¼ p (u 0
i )=qi(u 0
i jui1)
p (ui1)=qi(ui1ju 0
i )
c. Set
ui ¼
u 0
i
with probability min(r(u 0
i jui1); 1)
ui1
with probability 1  min(r(u 0
i jui1; 1)
(
3. Increment i, go to step 2, and repeat until convergence occurs.
Note that the Metropolis–Hastings algorithm does not require movement for each
iteration as does Gibbs sampling. The posterior distribution p(u) is required only up
to a proportionality constant since the constant would be canceled in the ratio shown
above. So the Bayesian prior–likelihood function whose scaled value equals the pos-
terior may be used in place of the posterior, hence circumventing the problem
of scaling with integration in the denominator in Bayes’ Theorem. The proposal
distribution must cover the nonzero support portion of the domain of the posterior
distribution. The divisors in the numerator and denominator of the ratio in the
Metropolis–Hastings algorithm adjust for the probabilities of selection from the pro-
posal distribution. The original theorem of Metropolis et al. (1953) imposed
symmetry constraints, requiring the two conditionals to be equal: qi(ui0jui21) ¼ qi
(ui21jui0). Hastings determined later that this symmetry constraint was unnecessary
using the acceptance ratio in step 2b above that could be simpliﬁed to the ratio
r(u0
ijui1) ¼ p(u0
i)
p(ui1)
in the Metropolis algorithm with the symmetry condition. A convenient symmetric
proposal distribution to use is the normal distribution with center at the current
value ui21 and a variance s2
ui1 adjusted periodically using the rejection rates of ui0
determined from previous iterations (with s2
u0 ¼ 1). So, to apply the special case of
100
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

the Metropolis algorithm, use a symmetric proposal distribution to generate candidate
values for the posterior distribution and the ratio of the posterior distribution values to
decide whether to accept candidates ui0. Note that it is important to give candidates
with low posterior distribution values at least a chance to be selected, to sample repre-
sentatively from the entire posterior distribution, driven by the probabilities.
As with Gibbs sampling, Metropolis–Hastings sampling produces an ergodic
Markov chain that converges to a limiting distribution that is the posterior distribution
p(ujy) of the parameter vector. Metropolis–Hastings sampling moves toward and then
around the posterior distribution, providing samples with probabilities proportional to
that distribution. For an example of Metropolis–Hastings sampling (actually
Metropolis sampling), consider the case of normally distributed data x, with mean mu
and known standard deviation sigma.model. We will assume a normally distributed
prior with mean mu.prior and standard deviation sigma.prior, and a normally
distributed proposal distribution with standard deviation sigma.proposal. An
analysis of the model parameter mu, with initial value mu.start, with Metropolis–
Hastings samples from begin to end, is illustrated with S-Plus or R code in Fig. 4.2.
4.4
WinBUGS APPLICATIONS
Although MCMC algorithms provide the required procedure for Bayesian analysis of
complex statistical models, computer software furnishes the key to practical appli-
cation for the natural resource scientist. WinBUGS can be downloaded as freeware
from the Web at http://www.mrc-bsu.cam.ac.uk/bugs/. It is reasonably easy to
use and comes with a manual and many helpful examples. WinBUGS, the
Windows version of BUGS (Bayesian inference Using Gibbs Sampling), was devel-
oped as part of a statistical research project at the Medical Research Unit (MRC),
Biostatistics Unit of Britain, the United Kingdom Government, starting in 1989,
and is now developed jointly with the Imperial College School of Medicine at
St. Mary’s, London (Spiegelhalter et al. 2001). The Website includes the following:
the earlier version, Classic BUGS; a spatial statistics version, GeoBUGS; a conver-
gence output and diagnostic analysis module, CODA; and an e-mail discussion list
address. The Classic BUGS program uses a command-line interface with an S-Plus
or R text-based model description, whereas WinBUGS has the added option of a
graphical user interface called DoodleBUGS.
The novice user can become oriented to WinBUGS by downloading the software,
reading the abbreviated manual, and trying some of the examples. The user should be
aware of the possible dangers of misleading output from MCMC sampling due to lack
of convergence and check the error diagnostics using CODA. Multiple starting values
for the iterated chains are recommended, as well as a burn-in period to discard initial
chain values that may not have converged to the posterior distribution.
The general procedure to use with WinBUGS programming is as follows:
1. Create a WinBUGS program consisting of three sections, with program code,
data, and initial values.
4.4
WinBUGS APPLICATIONS
101

Figure 4.2. Example of Metropolis–Hasting sampling of the normal distribution parameter
mu with normal data x  N(mu, sigma.model), prior distribution N(mu.prior, sigma.prior),
proposal distribution N(, sigma.proposal), starting value mu.start, and posterior samples
from begin to end. (a) S-Plus or R program code. (b) Histogram of posterior distribution
samples for mu with normally distributed data x  N(mu ¼ 15, sigma.model ¼ 2) and
sample size n ¼ 30, prior distribution N(mu.prior ¼ 10, sigma.prior ¼ 2), proposal distribution
N(,sigma.proposal ¼ 4), starting value mu.start ¼ 10, and posterior samples from begin ¼ 500
to end ¼ 10000, with 10,000 interations.
102
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

2. Use the .Model .Speciﬁcation Tool menu option (Fig. 4.3) to
(a) check the model, (b) load the data, (c) specify the number of chains
and compile, and (d) load and generate initial values for the speciﬁed
number of chains. Using these operations, WinBUGS checks on the syntac-
tical correctness of the program code, data, and initial values. Multiple
chains can be generated using initial values spread throughout the parameter
range to check for convergence. To execute each step in this process, begin
by blocking the ﬁrst string of characters, or word, in the appropriate
location in the program, the model program code, the data, and the
initial values sections, respectively. After executing each step, look at the
lower left corner of the window for a message indicating the success or
failure of the step.
3. Use the .Model .Update Tool menu option (Fig. 4.4) to indicate a speci-
ﬁed number of posterior sample updates, that are refreshed, thinned, overre-
laxed, and adapted, as requested. Refreshing displays the samples on a trace
graph periodically as requested. Refreshing every sample (i.e., 1) provides a
complete picture but slows down the processing. Refreshing every 10th
sample (i.e., 10) or every 100th sample (i.e., 100) is more common.
Thinning stores every kth sample only, as speciﬁed, to reduce the autocorrela-
tion between the samples and reduce the storage requirements. Overrelaxing
produces multiple samples per iteration and chooses the one that is most nega-
tively correlated with the current value, to reduce the autocorrelation between
the selected samples. Adapting activates an initial adaptive phase for the tuning
of some of the parameters used by Metropolis sampling, such as the standard
deviation of a normal proposal distribution. The summary statistics do not
include samples collected during this initial adaptive phase. The update
button can be turned on and off at the user’s discretion, to begin, pause, and
resume the MCMC sampling process.
Figure 4.2. Continued.
4.4
WinBUGS APPLICATIONS
103

4. Use the .Inference .Sample Monitor Tool menu option (Fig. 4.5) to
specify the nodes of interest, sampled with speciﬁed beginning and end, and
thinned to select every kth sample for output summary. Select a burn-in
period at the beginning to allow the MCMC process to converge before collect-
ing samples. Before executing the program, use the asterisk * to select all speci-
ﬁed nodes, and select trace to view dynamical graphical output. At the end of
the simulation run, output of the simulated MCMC parameter sample values
Figure 4.3. WinBUGS implementation: program code and >Model >Speciﬁcation
Tool windows. (a) Program code with >Model >Speciﬁcation Tool dialog box. (b)
>Model >Speciﬁcation Tool dialog box.
104
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

can be displayed as follows: (a) density, (b) statistics, (c) quantiles, (d) history,
(e) autocorrelations, (f) Brooks–Gelman–Rubin diagnostic convergence
statistics, and (g) output for CODA.
5. Another important option to activate in the menu options is the .DIC
(Fig. 4.6), which calculates the deviance information criterion, to compare
models with the Bayesian equivalent to Akaike’s information criterion (AIC)
(see Chapter 1). Set the DIC, once it activates at the beginning of the
MCMC run. The model with the lowest DIC value is the model that best ﬁts
the sample dataset.
Figure 4.4. WinBUGS implementation: program code, .Model .speciﬁcation Tool,
and .Model .Update Tool windows. (a) Program code with Model .Speciﬁcation
Tool and .Model .Update Tool dialog boxes. (b) .Model .Update Tool dialog
boxes.
4.4
WinBUGS APPLICATIONS
105

We will illustrate the use of WinBUGS with some introductory examples.
4.4.1
The Normal Mean Model for Continuous Data
Let’s conduct a Bayesian statistical analysis with WinBUGS of a simple normal mean
model for continuous data. The sample dataset consists of y  N(30, 5) from a normal
Figure 4.5. WinBUGS implementation: program code, .Mode1 .Speciﬁcation Tool,
.Model .Update Tool, and .Inference .Sample Monitor Tool windows. (a)
Program code with .Model .Speciﬁcation Tool, .Model .Update Tool, and
.Inference .Sample Monitor Tool dialog boxes. (b) .Inference .Sample
Monitor Tool dialog box.
106
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

population with mean m ¼ 30 and standard deviation s ¼ 5, of sample size n ¼ 25.
The WinBUGS code is given in Fig. 4.7a. The program consists of program code,
data, and initial values. WinBUGS requires use of the precision parameter tau ¼
t ¼ 1/s2 in the program code as a parameter for the normal distribution. Recall
that precision is the inverse of the variance. The derived parameter sigma ¼ s ¼
1/t1/2 is calculated for each MCMC iteration. Noninformative, vague, approximately
“ﬂat” priors are used for the parameters, a normal distribution for the mean parameter
m with precision 10212 (hence standard deviation s ¼ 106), and the approximate
Jeffreys transformation-invariant gamma prior G(0.001, 0.001) for the nonnegative
precision parameter t (Fig. 4.7b). The parameters are initialized at m ¼ 1 and t ¼
1. We blocked the words model in the program section, list in the data section,
Figure 4.6. WinBUGS implementation: program code, .Model .speciﬁcation Tool,
.Model
.Update
Tool,
.Inference
.Sample
Monitor
Tool,
and
.Inference .DIC windows. (a) Program code with .Model .Speciﬁcation
Tool, .Model .Update Tool, .Inference .Sample Monitor Tool, and
.Inference .DIC Tool dialog boxes. (b) Inference .DIC Tool dialog box.
4.4
WinBUGS APPLICATIONS
107

and list in the initial values section to run the (1) check, (2) load data and
compile, and (3) load initial values steps in the model speciﬁcation
phase of the run. We requested 10,000 iterations, with a burn-in period of 1000 iter-
ations. Recommended burn-in periods range from 500 to 5000 iterations, and total
numbers of iterations range from 5000 to 100,000, depending on the complexity of
the model.
Figure 4.7. Graphs and tables for the Bayesian statistical analysis of the normal mean model
for the dataset y  N(30, 5) with n ¼ 25 using WinBUGS. (a) Program code for the normal
mean model and dataset y. (b) The Jeffreys noninformative gamma prior G(0.001,0.001) for
t. (c) Dynamic trace output of initial posterior samples for the parameters. (d) Posterior distri-
bution density functions for the parameters, from MCMC samples. (e) Posterior distribution
statistics for the parameters, estimated from MCMC samples. (f) DIC statistics for the
normal mean model.
108
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

Figure 4.7. Continued.
4.4
WinBUGS APPLICATIONS
109

The dynamic trace output at the beginning of the simulation reveals that MCMC
immediately jumped to the range of the parameter samples around m ¼ 30, s ¼ 5,
and t ¼ 1/s2 ¼ 1/52 ¼ 0.04, away from the initial values m ¼ 1 and t ¼ 1
(Fig. 4.7c). The posterior distribution density functions for m and s, graphed from
the sampled MCMC simulated values between iterations 1000 and 10,000, along
with the output statistics, are illustrated in Figs. 4.7d–4.7f. The statistics include
mean, median, and 95% credible interval estimates (with limits at the 2.5% and
97.5% percentile points) of 30.57, 30.57, and [28.97, 32.19] for the m mean par-
ameter samples and 4.14, 4.074, and [3.14, 5.55] for the s sigma parameter
samples, respectively. The DIC ¼ 143.482 could be used to compare this normal
mean model with other models analyzed with this dataset.
The frequentist statistics estimates from S-Plus and R for this sample dataset are
ˆm ¼ 30:576;
ˆ
median ¼ 30:4; ˆs ¼ 4:01552, and 95% conﬁdence interval (CI) ¼
[28.91847, 32.23353] for the mean, in close agreement with the Bayesian statistics
for the model with noninformative priors, as we expect. Although the estimates
using both approaches to statistical analysis are in close agreement, the two interpret-
ations are, of course, quite different. We could have used informative priors on the
basis of previous analysis posterior results if available, or use these posterior
results as priors for analysis of additional sample datasets in the future.
4.4.2
Models for Count Data: The Poisson Model,
Poisson–Gamma Negative Binomial Model, and Overdispersed
Mixed-Effects Poisson Model
Suppose that the sample consists of count data y for a randomly dispersed population.
The normal mean model of Section 4.4.1 can be readily adapted to a Poisson model,
y  Pois(l), with the nonnegative mean parameter l . 0, using the WinBUGS
program code given in Fig. 4.8a. For the Poisson model, with count data that are ran-
domly dispersed, the variance parameter is equal to the mean parameter: s2 ¼ m.
For count data that are overdispersed, describing an aggregated or clustered popu-
lation, the variance parameter is greater than the mean parameter: s2 . m. An overdis-
persed population can be modeled by using either a negative binomial model or a
Poisson model with dispersion incorporated as a random effect. The negative binomial
model is a hierarchical model, a Poisson–gamma distribution of the random variable
y  Pois(l) that is Poisson-distributed with a mean l  G(a, b) that is gamma-distrib-
uted with parameters a . 0 and b . 0 (Hilborn and Mangel 1997). The WinBUGS
program code for this model is given in Fig. 4.8b. The parameters of this hierarchical
model are the shape and scale hyperparameters a and b that are “estimated” with pos-
teriors, using Jeffreys noninformative gamma priors. It can be shown that the mean and
variance parameters of the negative binomial distribution are given by m ¼ a/b and
s2 ¼ (a/b þ a/b2), so these parameters can also be programmed in the WinBUGS
code as derived parameters whose posteriors can be sampled.
The WinBUGS code for a model incorporating overdispersion as a random effect
e  N(0, s ¼ 1/pt) in a mixed-effects Poisson model is given in Fig. 4.8c. Note that
110
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

Figure 4.8. Program code for the Poisson and negative binomial models using WinBUGS. (a)
Program code for the Poisson model. (b) Program code for the negative binomial model using
the Poisson–gamma distrubition. (c) Program code for the negative binomial model using
random effects. (d) Program code for the negative binomial model with multiplicative
random effects.
4.4
WinBUGS APPLICATIONS
111

the mean li of the count data must be forced to be positive in the WinBUGS code to
enable the program to execute.
There are other ways of modeling overdispersion in a population in WinBUGS,
such as with a multiplicative random effect and a log transformation (see Fig. 4.8d).
WinBUGS now also includes the negative binomial model for data y  dnegbin ( p,
r) with parameters p and r as one of its model options. We will leave it to interested
readers to explore these alternative approaches (Hilborn and Mangel 1997).
4.4.3
The Linear Regression Model
Next, let’s conduct a Bayesian statistical analysis with WinBUGS of a linear
regression model using simulated sample data x  Unif(0, 200), y  100 þ
1.2x þ N(0, 40), with sample size n ¼ 30 (Fig. 4.9a). The linear regression model
“reality” is given by
yi ¼ 100 þ 1:2  xi þ ei
with residual errors ei  N(0, 40) that are normally distributed with mean m ¼ 0 and
standard deviation s ¼ 40. For the simulated population, b0 ¼ 100, b1 ¼ 1.2, and
s ¼ 40. The WinBUGS code is given in Fig. 4.9b. We use noninformative priors
for the parameters, “ﬂat” normal distributions for b0 and b1 with precision 10212
and standard deviation ¼ 106, and the approximate Jeffreys gamma prior G(0.001,
0.001) for the nonnegative precision parameter t. The dynamic trace output at the
beginning of the simulation reveals that MCMC quickly jumps to the realistic
range for the parameter samples centered around b0 ¼ 100, b1 ¼ 1.2, and s ¼ 40,
far from the initial values b0 ¼ 0, b1 ¼ 0, and s ¼ 1 (Fig. 4.9c). The posterior dis-
tribution density functions for b0, b1, and s, graphed from the simulated MCMC
samples between iterations 1000 and 10,000, along with the output statistics, are illus-
trated in Figs. 4.9d–4.9f. The parameter b0, b1, and s posterior sample statistics are
as follows: means (and standard errors) 94.56 (14.1), 1.264 (0.12), and 36.64 (5.17);
medians 94.62, 1.26, and 36.08; and 95% credible intervals [66.02, 122.5], [1.02,
1.51], and [28.25, 48.38], respectively. The DIC ¼ 303.818 could be compared
with other models analyzed with this dataset.
The S-Plus and R frequentist estimates are reasonably similar to the Bayesian
output for the linear regression model with noninformative priors, as expected:
bˆ 0 ¼ 94.7946, bˆ 1 ¼ 1.2626, and sˆ ¼ 35.68.
These examples illustrate the power of the WinBUGS approach to statistical mod-
eling. Data can be easily and accurately modeled, using realistic assumptions. The
focus for the natural resource scientist can be on the model speciﬁcations rather
than the development of methods for analytical solution. WinBUGS is particularly
suited for hierarchical structures that are so often present in biological systems, as
illustrated
by
the
overdispersed
Poisson–gamma
model
in
Section
4.4.2.
WinBUGS has a large collection of examples in its .help menu that illustrate a
wide range of important models for data analysis, including generalized linear
models (GLMs) and mixed-effects models. The interested reader is encouraged to
112
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

Figure 4.9. Graphs and tables for the Bayesian statistical analysis of the linear regression
model for the dataset (x,y), with x  Unif (0, 200) and y  100 þ 1.2
x þ N(0, 40) with
n ¼ 30 for the linear regression model using WinBUGS. (a) Scatterplot of the dataset (x, y).
(b) Program code. (c) Initial dynamic trace output of posterior samples for the parameters.
(d) Posterior distribution density functions for the parameters, estimated from MCMC
samples. (e) Posterior distribution statistics for the parameters, estimated from MCMC
samples. (f) Statistics for linear regression conditional means, m[i] terms estimated from
MCMC samples. (g) DIC statistics for the linear regression model, estimated from MCMC.
4.4
WinBUGS APPLICATIONS
113

Figure 4.9. Continued.
114
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

analyze these examples in WinBUGS. We will continue to illustrate WinBUGS
Bayesian statistics MCMC solutions to natural resource problems throughout the
remainder of this book, analyzing problems of increasing complexity, and comparing
their results to frequentist results. As always, the ultimate advantage of the Bayesian
approach to statistical analysis for the natural resource scientist and manager is its
direct interpretation of probabilistic inferences to parameters, rather than the indirect
interpretation of probabilistic inferences to sample datasets, with repeated surveys or
experiments, which is provided by frequentist statistical analysis.
4.5
SUMMARY
In this chapter, we introduced the topic of MCMC simulation, providing an overview
of the MCMC algorithms and the Markov chain theory necessary to appreciate their
ergodic properties. We described the most important MCMC algorithms, Gibbs
sampling,
and
the
Metropolis–Hastings
algorithm,
and
presented
several
WinBUGS applications of these algorithms that provided Bayesian statistical analysis
for model examples. We included examples of Bayesian MCMC statistical analysis in
WinBUGS for normal, Poisson, negative binomial, and linear regression models and
compared the Bayesian and frequentist results. Markov chain Monte Carlo algor-
ithms, and their implementation in WinBUGS software, provide general Bayesian
statistical analysis solutions for natural resource datasets. The solutions provide the
Bayesian statistical inferences, based on posterior distributions for the parameters,
obtained from datasets, models, and prior distributions, which can be particularly
useful for natural resource scientists and management decisionmakers to provide
answers to adaptive management problems.
PROBLEMS
4.1
WinBUGS example. Implement the models discussed in Section 4.4.2
in
WinBUGS for count data, the Poisson model, the Poisson–gamma negative
binomial model, and the overdispersed mixed-effects Poisson model, with
noninformative priors, as follows. Analyze the simulated Poisson dataset with
mean l ¼ 3.0 and sample size n ¼ 30: data ¼ c(4,2,2,4,5,1,2,2,2,4,5,6,
0,0,1,3,3,4,3,2,3,3,4,1,4,6,4,4,1,3). Draw inferences on the meaning of the par-
ameters of the models in light of the fact that this Poisson dataset is not over-
dispersed (i.e., examine the relationship between the derived parameters
mean ¼ m and variance ¼ s2 in the negative binomial Poisson–gamma
model and the signiﬁcance of the parameter sigma ¼ s in the mixed-effects
Poisson model). Your results provide solution options for case study 2
presented in Section 1.2.2.
4.2
WinBUGS example. Implement a negative binomial multiplicative mixed-
effects model for count data in WinBUGS using the Poisson count dataset in
PROBLEMS
115

Figure 4.10. Normally distributed sample data x  N(m ¼ 15, s ¼ 2) with sample
size n ¼ 100.
116
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

Figure 4.10. Continued.
PROBLEMS
117

Problem 4.1. Use a multiplicative random effect and a log transformation for the
mixed-effects model. Use noninformative priors. Draw inferences on the
meaning of the parameters in the model in light of the fact that this Poisson
dataset is not overdispersed (i.e., examine the signiﬁcance of the parameter
sigma ¼ s in this multiplicative mixed-effects model). Your results provide
an additional solution option for case study 2 presented in Section 1.2.2.
4.3
Gibbs sampling program. Write a program in S-Plus or R to implement Gibbs
sampling on Poisson data y ¼ fyA ¼ y1,y2, . . ., yk; yB ¼ ykþ1, ykþ2,. . ., yng, with
k ¼ 20 and sample size n . k, that is, randomly sampled from two strata, A and
B, with yA  Pois(lA) and yB  Pois(lB). Use approximate Jeffreys gamma
priors G(a, b) with a¼b ¼ 0.001 for lA and G(x, d) with x ¼ d ¼ 0.001 for
lB, with starting values lA0 ¼ 1:0 and lB0 ¼ 1:0. You can use conjugacy prop-
erties of the gamma prior for the mean parameter l of the Poisson model to
encode the program (Gelfand and Smith 1990; Gill 2002, pp. 313–317,
348–349). Implement the program on the dataset in Problem 4.1. Interpret
your results in light of the fact that lA ¼ lB ¼ l ¼ 3.0 for the simulated
sample dataset.
4.4
Metropolis–Hastings iterations. Use the Metropolis–Hastings algorithm to
generate the ﬁrst three sample values p1, p2, and p3, in the Markov chain for
the posterior distribution of the parameter p for the binary sample dataset
data ¼ c(1,0,1)
with
the
binomial
model.
Use
a
uniform
beta
prior
BE( p; a ¼ 1, b ¼ 1), with starting value p0 ¼ 0, and candidate values p10 ¼
0.34, p20 ¼ 0.67, and p30 ¼ 0.52 obtained from a proposal distribution. Use
random numbers 0.91, 0.17, 0.45  Unif(0,1), as many as are required for
the ratios ri , 1, i ¼ 1 – 3.
4.5
Metropolis–Hastings program. Utilize the S-Plus and R program in Fig. 4.2a to
implement the Metropolis–Hastings algorithm for normally distributed
measurements y  N(m, s) with mean m and known standard deviation s ¼
2.0.
The
program
generates
an
ergodic
Markov
chain
of
samples
fm0,m1,m2,. . .g converging to the posterior distribution of m. Use a normally
distributed proposal distribution for m  N(mproposal, sproposal) with mean
mproposal ¼ mi and standard deviation sproposal. Apply the program to the simu-
lated sample dataset in Fig. 4.10 with mean m ¼ 15.0. Use a normally distrib-
uted prior for m  N(mprior ¼ 10.0, sprior ¼ 2.0) with mean mprior ¼ 10.0 and
standard deviation sprior ¼ 2.0, and an initial value for the Markov chain
m0 ¼ 15.0 (close to the mean of the sample data y). Use a standard deviation
sproposal ¼ 0.24 for the normally distributed proposal distribution that provides
a rejection rate of approximately 33%. Run 500,000 iterations with burn-in of
5000 samples. Interpret the results of the simulation, describing the relationship
of the precision and mean of the posterior to the precision and means of the
prior and sample dataset (see Section 2.4.1).
4.6
Analyze the sample dataset in Fig. 4.10 again, using WinBUGS. Use the same
initial conditions as in the MCMC simulation above in Problem 4.5: sample
118
BAYESIAN STATISTICAL INFERENCE III: MCMC ALGORITHMS

dataset y  N(m, s) with mean m and known standard deviation s ¼ 2.0, nor-
mally distributed prior for m  N(mprior ¼ 10.0, sprior ¼ 2.0) with mean
mprior ¼ 10.0 and standard deviation sprior ¼ 2.0, an initial value for the
Markov chain of m0 ¼ 15.0, and 500,000 iterations with a burn-in of 5000
samples. Compare the posterior results with those obtained from the MCMC
simulation sampling in Problem 4.5 and with the theoretically expected
results (see Section 2.4.1).
PROBLEMS
119


5
Alternative Strategies for Model
Selection and Inference Using
Information-Theoretic Criteria
In this chapter we describe two contrasting strategies for model selection and
inference, a descriptive strategy consisting of a posteriori exploratory model selection
and inference and a predictive strategy consisting of a priori parsimonious model
selection and inference using information-theoretic criteria. We will ﬁrst discuss
the descriptive strategy of a posteriori exploratory model selection and inference,
sometimes known pejoratively as “data dredging,” and illustrate with examples.
We will then discuss an alternative predictive strategy of a priori parsimonious model
selection and inference using information-theoretic criteria, such as Akaike’s infor-
mation criterion (AIC). We will include in the discussion a review of the standard
methods of least squares, maximum likelihood, and Bayesian ﬁt for the estimation
of model parameters. We can use these two strategies of model selection and infer-
ence with either a frequentist or a Bayesian approach to statistical analysis and infer-
ence. We will also review some standard methods of evaluation of ﬁt, or goodness of
ﬁt, based on cross-validation techniques and test datasets. We conclude the chapter
with a discussion of model averaging, for prediction, coefﬁcient estimates and
error, and importance of covariates.
5.1
ALTERNATIVE STRATEGIES FOR MODEL SELECTION
AND INFERENCE: DESCRIPTIVE AND PREDICTIVE
MODEL SELECTION
5.1.1
Introduction
In this section we describe two alternative strategies for statistical model selection and
inference. We will be able to use either of these two strategies with either a frequentist
or Bayesian approach to statistical analysis and inference. Let’s assume that we are
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
121

interested in formulating, ﬁtting, and evaluating statistical models of a dependent
response that is a function of a collection of covariates. We restrict ourselves to
numerically valued covariates, although categorically valued treatment factors may
be incorporated as well. The model could be a habitat selection model (Manly
et al. 1995, 2004) that expresses the preference of a wildlife species for habitat
expressed as a function of its attributes. For example, the wildlife species might be
a bird or mammal species threatened with extinction, and the habitat attributes
might be late-seral-stage, moisture, temperature, and proximity-to-water conditions
particularly suitable to the animal. More generally, the habitat attributes may
include vegetation, geologic, and climatic attributes. The habitat attributes, obtained
from computerized GIS (geographic information system) information, may be more
easily sampled than the wildlife species itself. The dependent variable y, the response,
would measure the abundance, or presence–absence, of the wildlife species. The
“independent,” “predictor,” variables or covariates, x1, x2, . . . , xk, would measure
habitat attributes. The relationship can be expressed by the function f given by y ¼
f(x1, x2, . . . , xk), which may be linear or otherwise. We will use multiple linear
regression throughout this chapter to illustrate.
The ﬁrst of the two model selection strategies is most appropriate for populations
where the relationship between the independent covariates and dependent response
variable is not well understood. For this strategy, sample datasets consist of measure-
ments of the response and candidate covariates that are believed to be related to the
response variable. The model selection is determined a posteriori after data collection.
After the sample data have been collected, graphs and statistics are examined to
evaluate the apparent relationship of the covariates with the response. Tests of the
relationships such as correlation tests and linear regression modeling can be con-
ducted to determine the statistical signiﬁcance of subsets of the covariates with the
response. It is permissible to examine and compare the graphs and test statistics for
any subset of covariates. However, because of the multiplicity of tests, compounded
type I–type II error may occur. Also, because of the possible overreliance of the
results on the particular sample dataset, models may tend to overﬁt the data. Hence
the statistical results must be regarded as preliminary, exploratory, and tentative,
descriptive of the sample dataset rather than predictive of the population in
general. The results of analysis using this strategy can be useful in formulating
hypotheses and selecting important covariates for further study. Although this strat-
egy is sometimes pejoratively called “data dredging” because of its propensity for
error, we wish to emphasize that it does have its place as a statistical strategy for
data analysis and inference, particularly for natural resource populations that are
not well understood.
An alternative strategy is more appropriate for populations that are better under-
stood. This strategy, a priori parsimonious model selection and inference
(Burnham and Anderson 1998), has come into favor in more recent years with
ecology and wildlife management scientists and managers to address the problems
associated with compounding of error and overﬁtting of sample data associated
with the ﬁrst strategy. For this alternative strategy, a collection of candidate models
must be speciﬁed prior to data collection. The collection of models must be
122
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

parsimonious; the number of models in the collection must not be too large, and each
model must not contain too many covariates. Typically, the collection must be limited
to 10–30 models, although in practical application, the collection may contain up to
several hundred models. The proposed candidate models may contain covariates (and
factors) xi of various speciﬁed forms such as linear xi, quadratic xi
2, and pseudothres-
hold (i.e., logarithmic) log(xi), and their lower-order interactions such as the second-
order xi . xj. The candidate models are hypothesized on the basis of a biological
assessment of those relatively independent covariates that are most likely in combi-
nation to efﬁciently describe the response. The results of previous studies, collective
current scientiﬁc wisdom, and cumulative personal experience and intuition can be
used to identify the candidate models. After the collection of candidate models has
been speciﬁed, the sample data can be collected and analyzed. The analysis consists
of assessing the relative competitiveness of the candidate models in ﬁtting the data.
Information-theoretic criteria have proved to be most effective at providing this
assessment. For most models of biological populations that are high-dimensional
or inﬁnite-dimensional in complexity, with frequentist statistical analysis and infer-
ence, the corrected AIC (AICc) (Akaike 1973, 1974; Burnham and Anderson
1998) is a particularly effective criterion. The best ﬁtting models are those with the
lowest AICc. The AICc criterion measures relative error between the comparative
models, and not absolute error, so additional methods must be used to examine
goodness of ﬁt of the best-ﬁtting models. With Bayesian statistical analysis and infer-
ence, the deviance information criterion DIC (Carlin and Louis 2000, Spiegelhalter
et al. 2001) is an analogous relative measure of goodness of ﬁt, particularly with hier-
archical models. This strategy circumvents the problems with compounding of error
and overﬁtting of data that are inherent in the ﬁrst strategy. It is more appropriate for
populations that are better understood, however, where the collection of candidate
models is likely to include good-ﬁtting models. If the population is not well under-
stood, the analyst runs the risk with this second strategy of not choosing good-
ﬁtting models in the collection of candidate models. So this strategy should be
used with a degree of caution. In successful application, however, with populations
that are better understood, and with proper testing of goodness of ﬁt, this approach
to statistical modeling, predictive a priori parsimonious model selection and inference
using AICc, provides a reliable strategy for predictive model selection and inference
of natural resource populations.
5.1.2
The Metaphor of the Race
The advantages and disadvantages of the two model selection and inference strategies
are illustrated with a metaphor of the race. The question is as follows:
How might we decide with one race who is the best automobile racing car driver
in the world?
One strategy would be to conduct an open racing event. All racers in the world
would be invited to participate. The number of entrants might be huge, with
5.1
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE
123

many candidates of varying levels of skill participating. Many unlikely and
uncontrollable events might occur during the course of the race. Leading contenders
might fall by the wayside because of the crowded conditions and collisions with less
skilled and amateurish opponents. Local racers accustomed to conditions of the track
might have an advantage and rise to the lead. Although all candidates could partici-
pate, with conditions of apparent fairness and equality, the results would be unpre-
dictable and subject to a signiﬁcant and indeterminable amount of error. Leading
racers of the world might very well lose the race. Winners of the race would need
to be further tested to determine whether their performance could withstand the
test of time.
An alternative strategy for determining the best racer in the world would be to
choose the initial contestants according to their previous track records. A small
number of ﬁnalists would be chosen among those who had previously proved them-
selves by consistently winning races. The race would be a ﬁnal contest, among the
leading racers of the world, to determine the ultimate winner. This strategy would
be less subject to error than the ﬁrst strategy because conditions would be better con-
trolled. Although some uncertainty would still exist, this ﬁnal race would be better
suited to determining the best overall racer in the world. If, however, not much is
known about the leading racers in the world prior to the contest, if other preliminary
races had not occurred to provide a screening process, this preselection process might
not work out well, failing to identify the leading contestants for the ﬁnal race. So the
key to making this strategy work well is a prior knowledge of the leading contestants
for the ﬁnal race.
5.2
DESCRIPTIVE MODEL SELECTION: A POSTERIORI
EXPLORATORY MODEL SELECTION AND INFERENCE
We discuss here in greater detail the ﬁrst strategy, the descriptive a posteriori explora-
tory approach to model selection and inference based on “data dredging.” This strat-
egy provides a large degree of ﬂexibility in exploring graphical and analysis methods
to assess models ﬁtting the sample data. However, this ﬂexibility in examining and
assessing models ﬁtting the sample dataset must be provided in tandem with
increased amounts of risk of compounded error and the overﬁtting of models to
the dataset.
Numerous techniques may be used with this strategy once the sample dataset has
been collected. Graphs such as scatterplots may be examined to look at possible
relationships between the covariates and response, as well as relationships among
the covariates. Correlations may be examined, between the covariates and response,
and among the covariates. The general idea is to choose covariates for a model that
are highly correlated with the response but not with each other. The ideal design is an
orthogonal design where the covariates are independent of each other. One approach
is to group the covariates by attribute type, such as vegetation, geologic, and climatic
attributes. Covariate attributes can also be grouped by scale, such as microhabitat,
124
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

macrohabitat, and landscape scales. The idea is to arrange the covariates into attribute
type and scale groups such that covariates within groups are correlated with each
other and covariates between groups are not correlated with each other. The
leading attribute or attributes within each group that is correlated with the response
can then be selected for inclusion in the best-ﬁtting models. Numerous multivariate
statistical methods exist to assess the relationships among covariates, such as discri-
minant analysis, principal-components analysis, factor analysis, and canonical
correlation analysis.
Stepwise and best-subsets selection methods can then be used, with statistical
modeling such as multiple linear regression, to assess the collection of all possible
models of linear combinations of covariates for their relationship with the response.
These standard methods with frequentist analysis are well known and are discussed in
detail in many leading references (Seber 1977, Draper and Smith 1981, Manly 1994,
Ryan 1997, Cook and Weisberg 1999).
With descriptive a posteriori exploratory model selection strategy, all plausible
models may be examined in the quest to ﬁnd the best-ﬁtting model. The danger
with this strategy, however, is the potential for compounding type I and II errors
from multiple tests, of choosing covariates that shouldn’t be in leading models, and
failing to choose covariates that should be in leading models. In the “ﬁshing
expedition” that can ensue with abuse of this strategy, specious covariates can enter
into models that are only fortuitously related in sample datasets by random chance,
covariates that are not biologically related to the response in the population in general.
Care must be exercised to avoid overﬁtting of models too closely to sample data-
sets. It is the signal or information in a sample dataset that we wish to model, and not
the noise. Burnham and Anderson (1998) suggest the analogy of drawing a proﬁle of
an elephant for the ﬁtting of a model to a reality. The proﬁle of an elephant based on
one sample elephant should not be overdrawn, capturing every detail, every mole and
whisker, every idiosyncrasy of the sample elephant. The proﬁle should rather be an
attempt to capture the general characterizes of all elephants, such as the large trunk
and the lumbering feet.
Figure 5.1 illustrates this point. The scatterplot graph of a randomly sampled
dataset of small sample size n1 ¼ 3 from a larger population is illustrated in
Fig. 5.1a. Figure 5.1b presents an apparently reasonably ﬁtting quadratic model to
this sample dataset that is curvilinear in shape. This sample size, although extremely
small, is illustrative of the small sizes of many sample datasets currently used in
natural resource studies. It is quite common, for instance, for natural resource research
studies to be based on sample datasets with 100 or fewer observations and 30 or more
covariates, with three samples per covariate.
A larger randomly sampled dataset from the same population reveals, however,
that a linear model, not a quadratic model, provides a much better ﬁt (Figs. 5.1c
and 5.1d). We were apparently misled into overﬁtting a model to the smaller
sample dataset, overcharacterizing that particular sample dataset, its “whiskers
and moles,” with our model. The quadratic model for the smaller sample dataset
required, however, estimates of three parameters, a constant, linear coefﬁcient, and
5.2
DESCRIPTIVE MODEL SELECTION
125

second-degree coefﬁcient, with just three sample data points: 1 sample per parameter.
The linear model, on the other hand, would require estimates of just two parameters, a
constant and linear coefﬁcient: 1.5 samples per parameter. The precision of the esti-
mates for the linear model would generally be higher than those for the overﬁt quad-
ratic model, with more samples per parameter. A model that is well-ﬁtting must
balance the requirements of minimizing the bias or error of the ﬁt, using more par-
ameters, with maximizing the precision of the parameter estimates, using fewer
parameters.
Therefore, modeling results based on this strategy of a posteriori exploratory
model selection and inference should be viewed as preliminary and tentative,
descriptive of sample datasets, but subject to further study, analysis, and conﬁr-
mation before predictive inferences can be made with assurance about populations.
Figure 5.1. Schematic representations of overﬁtting a model to a sample dataset. (a) Sample
dataset of size n1 ¼ 3 from a population and (b) Quadratic model overﬁtting the sample dataset
of size n1 ¼ 3. (c) Larger sample dataset of size n2 ¼ 50 from the same population, with the
smaller sample of size n1 ¼ 3. (d) Larger sample dataset of size n2 ¼ 50 from the same
population, with the smaller sample of size n1 ¼ 3, with the overﬁt model.
126
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

This strategy is best applied to populations that are not well understood. Results can
then be used to formulate hypotheses for more deﬁnitive studies with additional
sample datasets.
We next discuss an alternative strategy of model selection: a priori parsimonious
model selection and inference. The principle of parsimony will be discussed in
greater detail and an information-theoretic criterion, AICc, will be described for iden-
tifying the best-ﬁtting model. The best-ﬁtting model will have the lowest AICc value.
Furthermore, AICc values can provide Akaike weights that estimate the probabilities
of the models being the best-ﬁtting to the population, based on the sample dataset.
This criterion, coupled with the principle of parsimony, provides an alternative strat-
egy for model selection and inference that is a highly effective way of avoiding the
overﬁtting of models to sample datasets.
5.3
PREDICTIVE MODEL SELECTION: A PRIORI
PARSIMONIOUS MODEL SELECTION AND INFERENCE
USING INFORMATION-THEORETIC CRITERIA
The exploratory data analysis strategy described in the previous section provides an
effective approach for the descriptive modeling of a sample dataset. Powerful and
easy-to-use statistical software is available to implement this strategy, such as with
stepwise and best-subsets selection for multiple linear regression modeling. This
strategy has the advantage of considering a wide range of models, even after data
have been collected. It is exploratory and open to the possibility that unknown
relationships between covariates and response may be uncovered from the sample
dataset. It is particularly effective with populations that are little understood.
However, as we have pointed out, the descriptive modeling strategy does have its
limitations. It is subject to compounded error and the overﬁtting of data. Specious
variables may enter into the best-ﬁtting models from type I error, variables that are
not biologically signiﬁcantly related to the response within the population in
general but statistically signiﬁcant because of the vagaries of the sample dataset.
Other variables that are biologically signiﬁcantly related to the response, but not stat-
istically signiﬁcant, may be overlooked with the analysis from type II error. This a
posteriori strategy places a premium on the analysis phase of a study, after data are
collected, rather than on careful reﬂection and biological thinking at the beginning
of a study, before data are collected.
For populations that are better understood, with biological understanding that
has cumulated from prior studies, there is an alternative strategy to model selection
and inference that avoids compounded error and the overﬁtting of sample data.
This alternative strategy, a priori parsimonious model selection and inference,
using AICc for frequentist statistical analysis and inference or DIC for Bayesian
statistical
analysis
and
inference,
provides
more reliable
predictive results.
Burnham and Anderson (1998) proposed this statistical data analysis strategy for
model selection and inference to address the problems of compounded error and
5.3
PREDICTIVE MODEL SELECTION
127

overﬁtting of natural resource sample datasets. Their strategy consists of several
components:
1. Hypothesize a relatively small collection of candidate models for analysis, a
priori to data collection.
2. Choose a parsimonious number of candidate models for the collection and a
parsimonious number of covariates for each of the candidate models.
3. Use AICc with frequentist statistical analysis or DIC with Bayesian statistical
analysis as a criterion for selection of the best-ﬁtting models; these are the
models with the lowest AICc or DIC.
4. Use AICc or DIC weights for assessment of the relative competitiveness of the
models and for the model averaging of the estimates of covariate coefﬁcients
and error, predictive values, and the importance of covariates.
The parsimonious a priori model selection–inference strategy advocated by
Burnham and Anderson provides a way of avoiding compounded type I and type
II error and the overﬁtting of sample data. Compound error is avoided by using
information-theoretic criteria such as AICc, rather than multiple tests of hypotheses.
Overﬁtting is avoided by limiting the number of models and their numbers of covari-
ates. This strategy, combined with goodness of ﬁt tests, provides a more rigorous
method for obtaining reliable predictive models in contrast to the descriptive strategy
of a posteriori exploratory model selection. On the other hand, this strategy can miss
important models with covariates that are overlooked in the initial selection of the
candidate models with populations that are inadequately understood. So it is import-
ant to use the descriptive a posteriori model selection strategy for populations that are
poorly understood and the predictive a priori model selection strategy for populations
that are better understood.
5.4
METHODS OF FIT
Model selection and inference presupposes the use of appropriate methods of model
ﬁt. We described the three most common methods of ﬁtting statistical models to
sample datasets in Chapter 2: least-squares (LS) ﬁt, maximum-likelihood (ML) ﬁt,
and Bayesian ﬁt. The two alternative strategies of model selection and inference
described in this chapter can be used with any of these three methods of model ﬁt,
whether frequentist LS, frequentist ML, or Bayesian. Least-squares ﬁt, based on
the minimization of the sums of the squared residuals, provides frequentist estimators
of parameter covariate coefﬁcients with multiple linear regression that are BLUE
(best linear unbiased estimators), with estimators that are unbiased with minimum
variance. We introduced the analysis of linear regression models in Chapter 1. We
will illustrate multiple linear regression analysis using both strategies for model selec-
tion and inference later in this chapter. See Appendix A for a review of linear
regression and multiple linear regression analysis. Maximum-likelihood ﬁt, based
128
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

on the maximization of the likelihood function, provides frequentist estimators that
are asymptotically (i.e., as sample size n ! 1) unbiased of minimum variance.
We will illustrate maximum likelihood estimation with the two model selection
and inference strategies with generalized linear modeling (GLM) in Chapter 6.
Bayesian estimators are based on the posterior distributions for parameters that are
obtained from Bayes’ Theorem expressing the conditional probability for parameters,
given the sample dataset. The posterior distributions are the scaled products of priors
and likelihoods. The posteriors can be obtained for some datasets and models by
using conjugate priors or more generally by Markov Chain Monte Carlo (MCMC)
simulation. We introduced Bayesian methods in Chapters 2–4. We will continue
to illustrate the use of Bayesian statistical analysis with multiple linear regression
in this chapter, generalized linear modeling (GLM) in Chapter 6, and mixed-
effects modeling in Chapter 7.
5.5
EVALUATION OF FIT: GOODNESS OF FIT
Regardless of which strategy is used to evaluate the best-ﬁtting models, whether it is a
posteriori descriptive modeling with exploratory data analysis, or a priori parsimo-
nious predictive modeling using AICc or DIC, it is still necessary to evaluate good-
ness of ﬁt of the leading models. In certain cases all models under consideration, even
the most competitive models with lowest AICc or DIC, may provide poor ﬁt to the
population. In other cases, all of the most competitive models may provide reasonable
ﬁt. Remember that AICc and DIC provide an information-theoretic criterion (see
Section 1.3.5) that measures the relative goodness of ﬁt among a collection of com-
peting models. All the statistics used to evaluate the best-ﬁtting models that we have
mentioned previously, including AICc and DIC, do not adequately address the issue
of absolute goodness of ﬁt. It is therefore critical, particularly with a priori modeling
where the objective is predictive results, to follow an analysis of the best-ﬁtting
models with a goodness of ﬁt analysis. A review of research articles in leading bio-
logical and natural resource journals revealed that only 7% of studies included a
goodness of ﬁt analysis, 5% with cross-validation and the remaining 2% with test
datasets (Stauffer 1999). Hence we will outline here goodness of ﬁt approaches
that should be an integral part of any analysis. We will use multiple linear regression
modeling as as example.
Assume, ﬁrst, that the statistics for the leading models have been examined for
statistical signiﬁcance, say, at the 95% probability level. With frequentist (or
Bayesian) multiple linear regression analysis, for instance, check that the following
conditions generally apply:
1. The estimates of the parameters are signiﬁcant, with conﬁdence intervals
(credible intervals) that do not include 0, or equivalently with p values for
t tests or F tests that do not exceed a (i.e., t or F statistics are in the rejection
region).
5.5
EVALUATION OF FIT: GOODNESS OF FIT
129

2. The R2 statistic is acceptably large.
3. The residual standard error syjx is acceptably small.
4. The F test is signiﬁcant.
5. The AICc (DIC) is relatively small.
6. Other statistics such as the adjusted R2 (and Mallows’ Cp) are acceptably large
(small).
Next, conduct a residual analysis to assess whether the residual errors between the
observed and predicted response values are acceptably small and satisfy the assump-
tions of the model. For multiple linear regression, the residual errors must be indepen-
dent, homoscedastic, and normally distributed.
Finally, check the leading models for predictive accuracy. There are two
primary ways to accomplish this: with cross-validation and with test datasets.
With multiple linear regression, the model prediction interval can be calculated
and checked for its predictive accuracy with test datasets. A prediction interval
with a 95% conﬁdence level should contain on average 19 of 20 points in the
test dataset. It is better to use independent test datasets rather than the original
developmental dataset that was used to estimate and assess the competitiveness
of the models. The predictive accuracy will tend to be overly optimistic on the
developmental dataset used to estimate the models. If test datasets are not avail-
able, divide the sample dataset into two subsamples, if it is not too small,
before analysis, with one used for development and the other for testing. If this
is not possible, and test datasets are not available, cross-validation should be
used as the next best option.
With cross-validation, individual points are omitted from the developmental
sample dataset, the model is estimated on the remaining subsample dataset,
and the omitted point is then checked to determine whether it falls within the
prediction interval. This process is repeated for each point in the sample
dataset. The predictive accuracy of the model is the proportion of the individually
omitted points that fell within the prediction interval of the model ﬁt to the
remaining subsample dataset. This proportion can then be compared with the con-
ﬁdence-level standard that is used for the predictive accuracy of the model pre-
diction intervals.
With test datasets, the prediction interval for the model can be estimated from the
developmental sample dataset. The proportion of points in the test sample dataset that
fall within the prediction interval can then be compared with the conﬁdence-level
standard that is used for the predictive accuracy of the model predictive intervals to
assess the goodness of ﬁt of the model. With rigorous studies that have the objective
of developing reliable predictive models for populations, multiple test datasets are
highly recommended. It should be borne in mind that populations are dynamic and
may change with respect to location and time. Hence, degraded prediction accuracies
of best-ﬁtting models with test datasets may be due to changes in a dynamic popu-
lation with respect to location and time, as well as to the poor ﬁt of the models to
a static population.
130
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

With other forms of modeling, such as logistic regression, which predicts the
probabilities of success for data consisting of binary responses of “success” (1)
or “failure” (0), a classiﬁcation analysis is recommended. With logistic regression,
optimal cutoff points can be determined, based on correct classiﬁcation, sensi-
tivity, and speciﬁcity rates for the developmental dataset, and applied to test data-
sets or to deleted points with cross-validation, to determine the predictive accuracy
of the best-ﬁtting models. This process will be described in greater detail in
Chapter 6.
5.6
MODEL AVERAGING
One advantage of the Burnham and Anderson (1998, 2002) strategy for frequentist
model selection and inference with the use of AICc is that Akaike weights can be cal-
culated for each model in a collection of the m candidate models fM1,M2, . . . , Mmg
that are being evaluated as best-ﬁtting models using AICc. The Akaike weights are
calculated as
wi ¼
exp½(AICci  mink¼1, 2, ..., m (AICck))=2
Pm
j¼1 exp½(AICcj  mink¼1, 2, ..., m (AICck))=2 , i ¼ 1, 2, . . . , m:
The Akaike weights wi, which sum to 1, are the scaled likelihoods for the models and
can be interpreted as the probability of the ith model being the best-ﬁtting model,
given the collection of candidate models and the sample dataset. These weights not
only provide a probability for the relative competitiveness of the candidate models
but also may be used as weights for model averaging. Similar ideas may sometimes
be used for Bayesian statistical analysis and inference, with DIC weights instead of
AICc weights. There are three primary ways in which these weights may be utilized
for model averaging.
5.6.1
Unconditional Estimators for Parameters: Covariate Coefﬁcient
Estimators, Errors, and Conﬁdence Intervals
One way to utilize model averaging is to obtain unconditional estimators for par-
ameters such as covariate coefﬁcient estimators, errors, and conﬁdence intervals
(CIs). An unconditional shrinkage estimator ^uun1 (and its unconditional standard
error se^uun1) for a parameter u can be obtained by taking the average of the candidate
model Mj conditional estimators uˆj (and their conditional standard errors seuˆj) for u,
weighted by the Akaike weights wj
^uun1 ¼
X
m
j¼1
wj  ^uj
se^uun1 ¼
X
m
j¼1
wj  se^uj
 
!
:
5.6
MODEL AVERAGING
131

This summation for j ¼1, 2, . . ., m is over all m candidate models. If the parameter uj
is not in model Mj, then assume uj ¼ 0 and uˆj ¼ 0 (seuˆj ¼ 0).
If the summation is restricted only to those models with the parameter and the
weights wj adjusted accordingly to wj 0 that sum to one, we obtain another uncondi-
tional estimator (and unconditional standard error)
^uun2 ¼
X
m
j0¼1;
u j0 in M j0
w j0  ^u j0
se^uun2 ¼
X
m
j0¼1;
u j0 in M j0
w j0  se^u j0 :
The estimator uˆj for parameter uj and its error are calculated conditional to the model
Mj being selected. The ﬁrst of these two unconditional estimators is sometimes called
a shrinkage estimator because it is smaller than the second unconditional estimator
and “shrunk” toward zero.
For example, with multiple linear regression models, we can obtain unconditional
estimator bˆ u for the covariate coefﬁcient parameter b, along with its unconditional
standard error se^bu, sampling error E^bu, and conﬁdence interval CI^bu, as follows
^bu ¼
X
m
j¼1
wj  ^bj,
se^bu ¼
X
m
j¼1
wj  se^bj,
E^bu ¼ tnp1  se^bu,
CI^bu ¼ ^bu + E^bu ¼ ½^bu  E^bu, ^bu þ E^bu,
where bˆ j is the conditional estimator of b in Mj, se^bj is its conditional standard error,
and tn2p21 is the critical t value with the appropriate degrees of freedom (n2p21) ¼
(n2k) for the multiple linear regression model with p covariates and k ¼ pþ1 par-
ameters (including the constant). Burnham and Anderson (1998, 2002) recommend
using the estimated mean-square error (MSˆE) for the unconditional standard error,
given by
^
MSE ¼ ^
var ^bu ¼
X
m
j¼1
wj 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
var ^bj þ (^bj  ^bu)2
q
"
#2
¼
X
m
j¼1
wj 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
(se)2
^bj þ (^bj  ^bu)2
q
"
#2
132
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

with
seˆbu ¼
ﬃﬃﬃﬃﬃﬃﬃ
^
var
p
ˆbu ¼
X
m
j¼1
wj 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
var ˆbj þ (ˆbj  ˆbu)2
q
:
The MSE incorporates both error and bias in its expression.
5.6.2
Unconditional Estimators for Prediction
Model averaging can also be used for prediction. Each model function f provides a
predictive value at each p-dimensional covariate point xi ¼ (x1i, x2i, . . . , x pi)
^yi ¼ f (x1i, x2i, . . . , x pi)
that estimates the response value at that point, along with conﬁdence intervals CI^yi
and prediction intervals PI^yi for speciﬁed levels of conﬁdence. For multiple linear
regression, yˆi is the estimated mean response at xi. For logistic regression, yˆi is the esti-
mated probability at xi. These predictive values for covariate points are conditional on
the “correctness” of the model. Unconditional model averaged prediction values
can be estimated from a collection of models fM1, M2, . . . , Mmg, using Akaike
weights wj for each of the Mj model prediction functions yˆj, j ¼ 1, 2, . . . , m:
^yu ¼
X
m
j¼1
wj  ^yj:
The unconditional predictive value at covariate point xi ¼ (x1i, x2i, . . . , x pi) is then
given by
^yu, i ¼
X
m
j¼1
wj  ^yj(x1i, x2i, . . . , x pi):
Unconditional conﬁdence and prediction intervals can be obtained by taking
weighted averages of the model conditional standard errors to calculate unconditional
standard errors, sampling errors, and conﬁdence and prediction intervals.
5.6.3
Importance of Covariates
Model averaging using Akaike weights also provides a method for assessing the rela-
tive importance of the covariates, the “independent” predictor variables xk, in the
candidate models. The importance of each covariate xk is given by the following
Importance (xk) ¼
X
xk is in Mj
wj:
5.6
MODEL AVERAGING
133

Traditionally, a covariate was deemed “important” when it occurred in a “best-
ﬁtting model.” Alternatively, the “importance” of a covariate has been measured
by the number of models in an analysis that included it. However, Akaike weights
provide a more sensitive way of weighing these numbers, incorporating the probabil-
ities estimating the relative competitiveness of the models into a cumulative sum of
the weights of the models containing the covariate.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN
S-Plus AND R; BAYESIAN STATISTICAL ANALYSIS IN WINBUGS
We now will illustrate the ideas of this chapter by analyzing a sample dataset with
habitat selection modeling (Manly et al. 1995, 2004) using multiple linear regression,
employing both strategies, predictive a priori parsimonious model selection and infer-
ence with AIC and DIC, and descriptive a posteriori model selection and inference.
The sample dataset is simulated from a known “reality” representing the response of a
wildlife species to habitat selection. The habitat is described by seven covariates
distributed as follows for measurements on 200-ha sites:
1. aspect: binomial with probabilities ¼ 0.5, for south- (0) and north- (1) facing
sites
2. species: binomial with probabilities ¼ 0.2 and 0.8, respectively, for RW
(redwood) (1) and Other (0).
3. old.growth: uniform (0, 200)
4. rock: binomial with probabilities ¼ 0.8 and 0.2, respectively, for absence (0)
and presence (1)
5. moss: binomial with probabilities ¼ 0.6 and 0.4, respectively for absence (0)
and presence (1)
6. temp: uniform (15, 30)
7. moist: uniform (0, 100)
Note that aspect, species, rock, and moss are represented by so-called design or
dummy variables. The wildlife biomass response is described by the “reality”
response = 5.0+2.0*rock+3.0*moss+0.5*temp+0.1*moist
+error
where error  N(0, 3) is normally distributed with mean m ¼ 0 and standard deviation
s ¼ 3. The S-Plus and R code used to generate the sample with a sample size of n ¼
50, along with the simulated sample dataset data1, is presented in Fig. 5.2.
134
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

Figure 5.2. The S-Plus and R code used to generate the simulated habitat selection dataset
data1.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
135

5.7.1
Frequentist Statistical Analysis in S-Plus and R: Predictive
A Priori Parsimonious Model Selection and Inference Using the
Akaike Information Criterion (AIC)
For the a priori parsimonious model selection and inference strategy applied to the
sample dataset data1, we consider a collection of 10 models, hypothesized from
biological considerations, with the following covariates:
1. faspect,species,old.growthg
2. faspect,species,mossg
3. faspect,species,tempg
4. faspect,species,moistg
5. faspect,old.growth,tempg
6. fspecies,old.growth,tempg
7. ftemp,moistg
8. frock,moss,temp,moistg (the correct model)
9. f1g (the null model)
10. faspect,species,old.growth,rock,moss,temp,moistg (the full model)
Figure 5.2. Continued.
136
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

The frequentist statistical analysis results of the LS ﬁt of the 10 multiple linear
regression models with the collections of covariates to the sample data listed above
are presented in Fig. 5.3. Note that, for the most part, the coefﬁcient estimates bˆ i
for the covariates xi are statistically insigniﬁcant at the 95% conﬁdence level (i.e.,
we cannot reject the null hypothesis H0: bi ¼ 0 for the covariate xi, with 95% conﬁ-
dence) for the biologically insigniﬁcant covariates aspect, species, and old.growth
(i.e., bi ¼ 0), and statistically signiﬁcant at the 95% conﬁdence level (i.e., we can
reject the null hypothesis H0: bi ¼ 0 for the covariate xi, with 95% conﬁdence) for
the biologically signiﬁcant covariates rock, moss, temp, and moist (i.e., bi=0).
There are two interesting exceptions, however, for the correct model 8, where the
coefﬁcient estimate for rock is statistically insigniﬁcant at the 95% conﬁdence
level even though the covariate is biologically signiﬁcant, and for model 2, where
the coefﬁcient estimate for moss is statistically insigniﬁcant at the 95% conﬁdence
level even though the covariate is biologically signiﬁcant.
Figure 5.4 summarizes the statistics from the analysis of data1. Note that the
residual standard error Syjx usually decreases and the coefﬁcient of determination
R2 increases as nested models contain increasing numbers of covariates. The F-test
statistic is statistically signiﬁcant at the 5% level of signiﬁcance (i.e., 95% conﬁdence
level) for only some of the models containing the biologically signiﬁcant covariates
rock, moss, temp, and moist: models 4, 7, 8, and 10. Akaike’s information criterion
correctly identiﬁes model 8, with the biologically signiﬁcant covariates, as the best-
ﬁtting model, with minimal AIC of 218.437 and maximal Akaike weight of 94.8%.
The full model 10 is the only slight competitor, with AIC of 224.246 and Akaike
weight of 5.2%. The null model 9 and models 1 and 2 with specious covariates
only, except for model 2’s moss covariate, are the poorest-ﬁtting, with the highest
AIC values. In conclusion, the a priori model selection–inference strategy correctly
identiﬁes model 8 as the best-ﬁtting model.
5.7.2
Frequentist Statistical Analysis in S-Plus and R: Descriptive A
Posteriori Model Selection and Inference
We perform descriptive a posteriori model selection and inference in S-Plus and R,
beginning with the command pairs(data1) applied to the data frame data1 of
covariates and response to examine the scatterplots of the paired variables
(Fig. 5.5a). We use the S-Plus and R commands with type ,- “forward”,
type ,- “backward”, or the default type ,- “both” for stepwise multiple
linear regression (Figs. 5.5b and 5.5c). The initial model formula should be that of
the null model response~1 for forward stepwise multiple regression and the
full model response~aspect+species+old.growth+rock+moss+temp+
moist for backward stepwise multiple regression. The option both is either forward
and backward or backward and forward depending on the initial model. The addition
or deletion of covariates is based on minimal Mallows Cp (S-Plus and R call it AIC).
For best subsets multiple linear regression in S-Plus and R, we construct the vector
of strings covariates (Fig. 5.5d) for the models consisting of all linear
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
137

Figure 5.3. The S-Plus and R frequentist statistical analysis results of dataset data1, using
predictive a priori parsimonious model selection and inference with AIC, described in
Section 5.7.1.
138
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

Figure 5.3. Continued.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
139

Figure 5.3. Continued.
140
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

combinations of the covariates under consideration and perform multiple linear
regression on all its entries (Fig. 5.5e).
The stepwise regression method chose the model with covariates fmoist, temp,
moss, rockg, selected in that order, based on lowest Mallows Cp. The best-subsets
selection method also chose the model with covariates faspect, moss, temp,
moistg as best-ﬁtting, using lowest AIC for the selection criterion. We conclude
that Mallows Cp and AIC successfully identiﬁed the best-ﬁtting model 8, using
stepwise and best-subsets selection a posteriori methods, respectively.
Generally, however, stepwise and best-subsets selection a posteriori methods do
tend to overﬁt models and compound error. Regardless of strategy, AIC is the
most appropriate indicator for model selection and inference for inﬁnite-dimensional
realities. We used the defaulted AIC function that is available in both S-Plus and R.
However, we could obtain the most accurate results by using the corrected AICc in
this example, particularly since the sample size was relatively small at n ¼ 50.
Figure 5.3. Continued.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
141

Figure 5.4. Statistics for 10 models: a priori parsimonious model selection and inference using AIC and Akaike weights.
142

Figure 5.5. S-Plus and R results for exploratory data analysis using stepwise and best-subsets
selection of multiple linear regression. (a) Pairwise scatterplots. (b) Stepwise selection method
code. (c) Stepwise selection method results. (d) Best-subsets selection code. (e) Best subsets
selection results.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
143

Figure 5.5. Continued.
144
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

Figure 5.5. Continued.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
145

5.7.3
Bayesian Statistical Analysis in WinBUGS: A Priori Parsimonious
Model Selection and Inference Using the Deviance Information
Criterion (DIC)
The Bayesian statistical analysis of data1 in WinBUGS, using the parsimonious a
priori model selection and inference strategy, is presented in Figs. 5.6 and 5.7.
Figure 5.6 provides the WinBUGS code used for model 8, the “correct model.”
Note the straightforward speciﬁcation of the multiple linear regression model with
normal errors in the program code, and the noninformative, approximately ﬂat, nor-
mally distributed priors with precision t ¼ 10212 and standard deviation s ¼ 106.
Figure 5.7 presents the WinBUGS DIC and DIC weight statistics for the Bayesian
MCMC analysis, along with the AIC and AIC weights for the frequentist analysis.
The Bayesian DIC and DIC weights are very similar to the frequentist AIC and
AIC weights, with rankings and weights of the models preserved, regardless of
frequentist or Bayesian approaches.
Although the statistics are similar for the frequentist approach and Bayesian
approach with noninformative priors, the advantages and disadvantages of each
should be considered by the natural resource scientist when deciding which to use
in the planning phase of a data collection project. The frequentist approach to multiple
Figure 5.5. Continued.
146
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

Figure 5.6. Bayesian statistical analysis WinBUGS program code and output results for the
“correct model” 8. (a) Program code. (b) Output results: kernel density graphys of the pos-
teriors. (c) Output results: sample statistics. (d) Output results: DIC.
5.7
APPLICATIONS: FREQUENTIST STATISTICAL ANALYSIS IN S-Plus AND R
147

Figure 5.6. Continued.
148
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

Figure 5.7. Statistics for 10 models: AIC and Akaike weights from frequentist statistical analysis; DIC and DIC weights from Bayesian
statistical analysis.
149

linear regression analysis is reliable and well known, with standards that are well
established. Alternatively, the Bayesian approach to multiple linear regression incor-
porates previous information with priors that can be generalized and provides direct
probability inferences for parameters, of interest to natural resource managers. Each
has its place in the toolbox of contemporary statistical methods available to the
natural resource scientist.
Similarly, the natural resource scientist should consider which model selection and
inference strategy to employ during the planning phase of a data collection project. The
a posteriori strategy provides descriptive models for populations that are not well
understood. Alternatively, a priori strategy provides predictive models for populations
that are better understood. Each also has its place in the toolbox of contemporary model
selection and inference strategies available to the natural resource scientist.
5.8
SUMMARY
In this chapter we described two contrasting strategies for model selection and
inference and illustrated with a multiple linear regression example. The two strategies
are a descriptive strategy of a posteriori exploratory data analysis, sometimes
pejoratively called “data dredging,” and a predictive strategy of a priori parsimonious
model selection and inference using information-theoretic criteria such as AIC
and DIC.
The descriptive strategy is particularly appropriate for populations that are poorly
understood. This strategy has the advantage of examining models selected a posteriori
to data collection. However, models ﬁt to sample datasets using this approach may
experience compounded error and overﬁtting. Hence, this strategy provides descrip-
tive results that should be viewed as preliminary, tentative, and hypothetical.
Predictive conclusions based on this approach should be inferred with extreme
caution. Additional studies and analyses, using independent datasets, may be
necessary in order to ensure reliable predictive results.
We contrasted this approach with an alternative predictive strategy of a priori
parsimonious model selection and inference using information-theoretic criteria,
such as AIC and AICc with frequentist statistical analysis and DIC with
Bayesian statistical analysis. This strategy is more appropriate for populations
that are better understood. Its results avoid the compounding of error and overﬁtting
of models and, if subjected to additional testing for goodness of ﬁt, are more
reliable for prediction.
Each of these strategies can be used with both frequentist and Bayesian statistical
analysis. We reviewed in this chapter the standard methods of ﬁt for the estimation of
parameters in models: least-squares ﬁt, maximum-likelihood ﬁt, and Bayesian ﬁt. We
also reviewed the standard methods of evaluation of ﬁt, using cross-validation and
test datasets. Finally, we discussed the use of model averaging with AICc and DIC
weights, for coefﬁcient estimates and error, prediction, and the importance of covari-
ates. We concluded with a multiple linear regression example using S-Plus and R for
frequentist statistical analysis and WinBUGS for Bayesian statistical analysis.
150
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

PROBLEMS
5.1
Conduct a frequentist statistical analysis of the sample dataset data2 (Fig. 5.8)
in S-Plus or R, using a predictive a priori parsimonious model selection
and inference strategy with AICc. The dataset data2 consists of habitat selec-
tion modeling data for salamander biomass response with covariates aspect,
species, old.growth, rock, moss, temp, and moist. Use multiple linear
regression for a comparative analysis of the following 12 models with linear
covariates:
1. faspect,species,old.growthg
2. faspect,species,rockg
3. faspect,species,mossg
4. faspect,species,tempg
5. faspect,species,moistg
6. faspect,old.growth,tempg
7. fspecies,old.growth,tempg
8. fmoss,temp,moistg
9. frock,tempg
10. ftemp,moistg
11. frock,moss,temp,moistg
12. faspect,species,old.growth,rock,moss,temp,moistg (full model)
5.2
Conduct a frequentist statistical analysis of the sample dataset data2 (Fig. 5.8)
in S-Plus or R, using a descriptive a posteriori model selection and inference
strategy. Use multiple linear regression for a comparative analysis of models,
including stepwise and best-subsets selection of the models with the 128 differ-
ent combinations of covariates. Use the Mallows Cp and AIC as the model
selection criteria for the stepwise and best-subsets selection methods,
respectively.
5.3
Conduct a Bayesian statistical analysis of the sample dataset data2 (Fig. 5.8)
in WinBUGS, using a predictive a priori parsimonious model selection and
inference strategy with DIC. Use multiple linear regression for a comparative
analysis of the 12 models with the linear covariates prescribed in Problem
5.1 (above).
5.4
Conduct model averaging on the a priori parsimonious model selection results
of Problems 5.1 and 5.3, using AIC and DIC weights. Use the mean and stan-
dard deviation of the posteriors with the DIC weights. Calculate the uncondi-
tional estimates of the coefﬁcients of temp and moist, with and without
shrinkage, along with their unconditional estimates of standard error. Also cal-
culate the importance of the covariates. Are the results very different, with and
without shrinkage, and with AIC and DIC weights?
PROBLEMS
151

Figure 5.8. Habitat selection modeling dataset data2 for Problems 5.1–5.6.
152
ALTERNATIVE STRATEGIES FOR MODEL SELECTION AND INFERENCE

5.5
Conduct a goodness of ﬁt analysis of the best-ﬁtting model from the a priori
model selection and inference results of Problems 5.1 and 5.3, including the
following:
(a) Examine the residuals for independence and normality.
(b) Conduct a cross-validation analysis in S-Plus or R, developing and utilizing
a program to delete successive points, ﬁt the best-ﬁtting model to the
remaining data, and assessing the predicted versus observed ﬁts of the
deleted points with respect to independence, normality, and prediction
interval.
5.6
Write a 3–5-page report, with appendix, summarizing the results of the analysis
in Problems 5.1–5.5. Specify the statistics used for the comparative analysis of
the different multiple linear regression models, with ﬁgures and tables when
helpful. Contrast the results and inferences from the two strategies for model
selection. What can be inferred for the best-ﬁtting model
response  b0 þ b1  aspect þ b2  species þ b3  old.growth
þb4  rock þ b5  moss þ b6  temp þ b7  moist þ e
where e  N(0,s)? Interpret the biological meaning of the statistical results
from a management perspective. Include in your report an abstract and intro-
duction, problem statement, objectives, methods, results, discussion, and
conclusions sections, along with an appendix with ﬁgures and tables.
PROBLEMS
153


6
An Introduction to Generalized
Linear Models: Logistic
Regression Models
In this chapter we will provide an introduction and overview of an exciting
contemporary frequentist approach to statistical modeling, one that generalizes mul-
tiple linear regression modeling: generalized linear modeling (GLM). Generalized
linear modeling generalizes multiple linear regression modeling by permitting
varying types of error structures and incorporating a link function for the response.
It also uses frequentist maximum likelihood (ML) estimation of model parameters
rather than least-squares (LS) ﬁt. We will discuss the importance of design for gen-
eralized linear modeling and present the analysis, including the assumptions, ﬁt, sig-
niﬁcant statistics, and selection of models for the exponential family of distributions.
We will then concentrate on the most important generalized linear model used in
natural resource science, the logistic regression model, including its assumptions,
ﬁt, signiﬁcant statistics, model selection, and goodness of ﬁt. We will brieﬂy describe
alternative generalized linear models such as the probit and complementary log–log
models for binomial data, the Poisson and negative binomial models for count data,
and the log-linear model for categorical data in contingency tables. We will conclude
by illustrating the application of frequentist GLM with logistic regression examples in
S-Plus and R. We will also illustrate the Bayesian approach to GLM with examples
in WinBUGS.
6.1
INTRODUCTION TO GENERALIZED LINEAR MODELS (GLMs)
Although general linear models such as ANOVA and multiple linear regression have
traditionally provided useful statistical tools for natural resource scientists and man-
agers, they are based on important assumptions that limit their application; residual
errors are required to be normally distributed, and the mean response is required to
be a linear function of covariates and factors. Increasingly, natural resource scientists
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
155

and managers require the use of models with error terms that are not normally distrib-
uted, for binary datasets that are binomially distributed and count datasets that are
Poisson or negative binomially distributed. Additionally, mean responses for
natural resource datasets may be probabilities or proportions that are nonlinear and
bounded. Because of these limitations, more general forms of models are required
such as generalized linear models, which are of increasing importance in natural
resource science applications.
Generalized linear models (GLMs) relax the assumptions of normally distributed
error and linear mean response. They achieve the latter by requiring the incorporation
of a link function of the mean response to a form that is a linear function of covariates
and factors. The most important of these models can be described by a general family
of distributions, the exponential family of distributions, with mean responses that
are nonlinear in form. In this chapter, we introduce this contemporary method,
which should become an integral part of the natural resource scientist’s statistical
analysis toolbox. We will focus on a model for binomial response, the most
popular GLM of interest to natural resource scientists, logistic regression modeling.
We will also brieﬂy introduce the probit and complementary log–log GLMs for
binomial data, and the Poisson and negative binomial regression GLMs for count
data. We refer the interested reader to the extensive literature on this subject,
for further detail beyond this brief introduction (Dobson 1990, McCullagh and
Nelder 1996, Hosmer and Lemeshow 2000, Hardin and Hilbe 2001, McCulloch
and Searle 2001). We particularly recommend Dobson (1990) for an elegant,
more mathematical, introductory discussion of the topic that is beyond the scope
of this book.
6.2
GLM DESIGN
All GLM data should be collected with careful a priori consideration given to design.
As with any data used for statistical modeling, the analyst should be cognizant of the
distinction between design-based and model-based inference. Design-based inference
provides estimators with frequentist properties such as unbiasedness and precision,
which are a function of the statistical design for the study and not model assumptions
for the distribution of the population. Model-based inference, on the other hand, pro-
vides estimators with properties that depend on distributional assumptions for the
population that are required by the model. Hence, a study based on a rigorous statisti-
cal design can provide a dataset with design-based analytical results that are robust
and not dependent on the adherence of the population to the distributional assump-
tions required by model-based inference. On the other hand, model-based analytical
results can beneﬁt from a statistical design that maximizes efﬁciency in the analysis of
a sample survey and power in an experiment.
For example, consider a GLM model examining the linear relationship between an
independent variable x and a dependent response variable y. With a model-based
approach, where the form of the model is known, independent x values for the
dataset should be chosen that are concentrated at important regions of its domain.
156
LOGISTIC REGRESSION MODELS

With a design-based approach, alternatively, x values should be randomly selected
throughout its domain. In both cases, the y values for the ﬁxed x values should be
chosen randomly. With a design-based approach, therefore, the x values and y
values should both be selected randomly, whereas with a model-based approach,
only the y values should be randomly chosen. The danger with a model-
based design is that the results provide inferences only for a population that
satisﬁes the distributional assumptions of the model. However, the advantage of a
model-based design is that the analysis results can be maximized for efﬁciency
and power.
6.3
GLM ANALYSIS
Traditionally, statistical analyses of continuous data for natural resource scientists and
managers have been based on general linear models of the form
yi ¼ b1  xi1 þ b2  xi2 þ    þ bk  xik þ ei, i ¼ 1, 2, . . . , n,
or in matrix notation
y ¼ x  b þ e,
where y is an n  1 vector, x is an n  k design matrix of explanatory covariate or
factor measurements, b is a k  1 parameter vector, and e is an n  1 error vector
of independent, identically, and normally distributed (iid) elements. In this formu-
lation, n is the sample size and k is the number of parameters. If a constant is used
in the model, xi1 ¼ 1 and b1 is the constant. Both ANOVA and multiple linear
regression are described by these models.
More recent advances in statistical theory and computer software, however,
provide a generalization of these methods to response variables that are more gener-
ally distributed, categorical as well as continuous, from the exponential family of
distributions (Dobson 1990). Additionally, these methods permit a relationship
between the independent explanatory x and dependent response y variables to be non-
linear, of the form y ¼ f(x . b), and the estimation procedure for the parameters b of
the model is based on ML estimation rather than LS ﬁt (see Section 2.2). The ML
estimation uses an iterative weighted LS algorithm and is included in many statistical
packages. Maximum-likelihood estimation has the important property of asymptotic
unbiasedness and efﬁciency, providing approximately unbiased estimators with
minimum error. Least-squares estimation also provides unbiased estimators of
maximal efﬁciency, is computationally simpler, and does not require probability dis-
tributional assumptions. However, there is little advantage in using it since prob-
ability distributional assumptions are required anyway to ensure the distributional
properties of LS estimators.
6.3
GLM ANALYSIS
157

The exponential family of distributions is of the form
f (y; u) ¼ s(y)  t(u)  ea(y)b(u)
¼ ea(y)b(u)þc(u)þd(y),
with s(y) ¼ ed( y) and t(u) ¼ ec(u). Here y is the data and u is the parameter. If a(y) ¼
y, the distribution is said to be in canonical form, and b(u) is called the natural para-
meter of the distribution. It can be shown that the normal, binomial, Poisson, nega-
tive binomial, and exponential distributions all belong to the exponential family of
distributions. To illustrate this, the Poisson distribution is in the exponential family
since its distribution is given by
f (y; l) ¼ ly  el=y!
¼ eylog (l)llog (y!),
so that a(y) ¼ y, b(l) ¼ log(l), c(l) ¼ 2l, and d(y) ¼ 2log(y!). It is in the
canonical form with natural parameter log(l). Similarly, it can be shown that
the normal, binomial, negative binomial, and exponential distributions are all in
canonical form with a(y) ¼ y (Dobson 1990). The natural parameters b, c, and
d for these important distributions in the exponential family are indicated as
follows:
Distribution
b(u)
c(u)
d(y)
Normal f(y; m)
m=s2
1
2  ðm2=s2  logð2  p  s2ÞÞ
 1
2  y2=s2
Binomial f(y; p)
log( p/(12p))
n . log(12p)
log
y
n


Poisson f(y; l)
log(l)
2l
2log(y!)
Negative binomial
f(y; u)
log(12u)
r . log(u)
log
y þ r  1
r  1


Exponential f(y; l)
l
log(l)
0
It can be shown that the expected value, or mean, and variance of a(Y ) are given by
the ﬁrst- and second-derivative expressions (Dobson 1990)
E½a(Y) ¼ c0(u)=b0(u),
var½a(Y) ¼ ½b00(u)  c0(u)  c00(u)  b0(u)=b0(u)3:
Generalized linear models unify statistical methods that incorporate a linear
combination of parameters (Nelder and Wedderburn 1972). A GLM consists of
a set of independent random response variables y1, y2, . . . , yn, each with the
same distribution from the exponential family of canonical form depending on a
single parameter ui, i ¼ 1, 2, . . . , n. Thus the joint probability density function
158
LOGISTIC REGRESSION MODELS

of y1, y2 , . . . , yn, is given by
f (y1, y2, . . . , yn; u1, u2, . . . , un) ¼ exp
X
n
i¼1
yi  b(ui) þ
X
n
i¼1
c(ui) þ
X
n
i¼1
d(yi)
"
#
:
The parameters ui are usually not of direct interest since there is one for each obser-
vation, so we consider a smaller set of parameters b1, b2, . . . , bk where k , n such
that
g(mi) ¼ b1  xi1 þ b2  xi2 þ    þ bk  xik,
i ¼ 1, 2, . . . , n,
or, in the language of matrices,
g(m) ¼ x  b,
where g is a monotone, differentiable function, called the link function, of the
expected value mi of yi. Here m is the n  1 vector of mean responses, x is the
n  k design matrix of explanatory variables (covariates and factors), and b is the
k  1 vector of parameters.
Both ANOVA and multiple linear regression are examples of a generalized linear
model with a normally distributed response and link function equal to the identity.
Logistic regression and Poisson and negative binomial regression are also examples
of generalized linear models for binary and count data, respectively.
The method of ﬁt for generalized linear models is ML estimation of the para-
meters, based on the sample dataset. The parameters are the coefﬁcients of the
linear model, related to the mean response by the link function. Maximum-likelihood
estimation is based on an iterative weighted LS procedure. Models can be compared
using AIC or AICc and their Akaike weights. The models can be evaluated for good-
ness of ﬁt by examining their deviances and comparing them with a x2 distribution.
Readers interested in further details should consult the appropriate references
(Dobson 1990, McCullagh and Nelder 1996). We will illustrate these ideas by focus-
ing on the most commonly used GLM for natural resource applications, logistic
regression.
6.4
LOGISTIC REGRESSION ANALYSIS
Logistic regression analysis is based on a generalized linear model for binary
response using a logit link function. A binary random variable z has one of two possi-
ble values, “success” or “failure,” “yes” or “no,” “present” or “absent,” or 0 or 1, with
probability(z ¼ 1) ¼ p and probability(z ¼ 0) ¼ 12p with 0  p  1. If there are n
independent random variables z1, z2 , . . . , zn with probability (zi ¼ 1) ¼ pi, then
6.4
LOGISTIC REGRESSION ANALYSIS
159

their joint probability distribution is given by
f (z1, z2, . . . , zn; u1, u2, . . . , un) ¼
Y
n
i¼1
pzi
i (1  pi)1zi
¼ exp
X
n
i¼1
zi  log
pi
1  pi


þ
X
n
i¼1
log (1  pi)
"
#
and is a member of the exponential family. If the pi values are all equal and y ¼
P
i¼1
n zi, then the random variable y has the binomial distribution
B(y; n, p) ¼ probability(y) ¼
y
n


 py  (1  p)ny:
If there are n such independent random variables y1, y2, . . . , yn, the log-likelihood
function is given by
‘(p1, p2, . . . , pn; y1; y2, . . . , yn)
¼
X
n
i¼1
yi  log
pi
1  pi


þ ni  log 1  pi
ð
Þ þ log
ni
yi




:
This distribution, if written in terms of the binary variables zi, can be shown to belong
to the exponential family of distributions.
The link function for logistic regression is given by the logit function
logit ¼ g(p) ¼ log
p
1  p


,
the log of the odds ratio. For the logistic regression GLM with one explanatory vari-
able x, the link function, or logit, is given by
logit ¼ g(p) ¼ log
p
1  p


¼ b1 þ b2  x:
For the more general logistic regression GLM with one or more explanatory variable,
the link function is given by
logit ¼ g(p) ¼ log
p
1  p


¼ x  b
¼
X
k
i¼1
bixi ¼ b1  x1 þ b2  x2 þ    þ bk  xk,
160
LOGISTIC REGRESSION MODELS

where k is the number of parameters and x1 ¼ 1. The inverse f ¼ g21 of the logit link
function g is given by
p ¼ f (x  b) ¼
exp (b1  x1 þ b2  x2 þ    þ bk  xk)
1 þ exp (b1  x1 þ b2  x2 þ    þ bk  xk) ¼
exp (x  b)
1 þ exp (x  b) :
Note that the complement of p is given by
(1  p) ¼ 1 
exp (b1  x1 þ b2  x2 þ    þ bk  xk)
1 þ exp (b1  x1 þ b2  x2 þ    þ bk  xk)
¼
1
1 þ exp (b1  x1 þ b2  x2 þ    þ bk  xk) ¼
1
1 þ exp (x  b) :
Two other generalized linear models are sometimes used for binary response data,
the probit model and the complementary log–log model. The probit model uses an
inverse cumulative normal probability function
g(p) ¼ F1(p) ¼ x  b
as its link function, where F is the cumulative probability function for the standard
normal distribution
F x  m
s


¼
1
s
ﬃﬃﬃﬃﬃﬃ
2p
p
ðx
1
e 1
2 ½(t  m)=s2
dt:
The complementary log–log model uses the complementary log–log function
g(p) ¼ log½ log (1  p) ¼ x  b
for its link function. Although all three models can be applied to binary data, they
differ in shape, particularly in the tails for values of p near 0 and 1. Each model is
based on different assumptions for its derivation, but the logistic regression model
has a particularly appealing interpretation for its odds ratios, as discussed in
Section 6.4.3.
6.4.1
The Link Function and Error Assumptions of the
Logistic Regression Model
Recall that the link function for logistic regression is given by the logit function
logit ¼ g(p) ¼ log
p
1  p


¼ x  b
6.4
LOGISTIC REGRESSION ANALYSIS
161

with inverse
f (x  b) ¼
exp (b1  x1 þ b2  x2 þ    þ bk  xk)
1 þ exp (b1  x1 þ b2  x2 þ    þ bk  xk) ¼
exp (x  b)
1 þ exp (x  b) :
The errors for logistic regression are binomially distributed B(yi; ni, pi).
6.4.2
Maximum-Likelihood (ML) Fit of the Logistic Regression Model
The log-likelihood function for logistic regression is given by
‘(p1, p2, . . . , pn; y1, y2, . . . , yn)
¼
X
n
i¼1
yi  log
pi
1  pi


þ ni  log (1  pi) þ log
ni
yi




¼
X
n
i¼1
yi  log (pi) þ (ni  yi)  log (1  pi) þ log
ni
yi




:
This function is maximized with respect to the parameters in the generalized linear
model b1, b2, . . . , bk by using an iterative weighted LS algorithm (Dobson 1990,
pp. 39–41) to derive the GLM ML solution.
6.4.3
Logistic Regression Statistics
The ML solution for the logistic regression GLM results in estimates bˆ 1, bˆ 2, . . . , bˆ k
for the k parameters b1, b2, . . . , bk in the linear model, along with estimates of their
standard error se bˆ 1, se bˆ 2, . . . , se bˆ k. These estimates can be assessed for statistical
signiﬁcance by checking whether their conﬁdence intervals do not contain zero or
equivalently by testing the null hypothesis H0: bi ¼ 0. The estimated model can be
evaluated by examining its deviance, R2, and AIC or AICc statistics. These statistics
are discussed next.
Conﬁdence intervals for the parameters can be calculated using estimates bˆ 1,
bˆ 2, . . . , bˆ k for the p parameters b1, b2 , . . . , bk in the linear model, and standard
error se bˆ 1, se bˆ 2, . . . , se bˆ k. The conﬁdence intervals are given by
CIbi ¼ ^bi + E^bi ¼
^bi  E^bi; ^bi þ E^bi
h
i
,
i ¼ 1, 2, . . . , k,
where the sampling error for the estimate of bi is
E^bi ¼ t1a=2, nk  se^bi:
The estimates bˆ i can then be examined for their statistical signiﬁcance by deter-
mining whether each CIbi is distinct from 0. Alternatively and equivalently, the
162
LOGISTIC REGRESSION MODELS

Wald test for the null hypothesis H0: bi ¼ 0 with alternative hypothesis HA: bi =
0 can be conducted on the basis of the test statistic
ts ¼
^bi
se^bi
,
i ¼ 1, 2, . . . , k,
which is tn-k-distributed if H0 is true. With speciﬁed type I error a and conﬁdence
level of P ¼ 12a, reject the null hypothesis and conclude that the estimates are
statistically signiﬁcant if the p value satisﬁes the following Neyman–Pearson con-
dition, equivalent to the test statistic being in the rejection region: p  a. The
comparative capability of the model at ﬁtting the sample dataset can be assessed
by using the measurement of the “error,” based on the log-likelihood ratio
deviance statistic
D ¼ 2  log L(^pmax; y)
L(^p; y)


¼ 2  ½‘(^pmax; y)  ‘(^p; y):
The log-likelihood ratio is multiplied by 2 because the D statistic under certain
conditions is x2-distributed. The maximum model, or saturated model, is the
GLM that uses the same distribution and link function as the model of interest
and has the number of parameters equal to the number of observations. The
maximum model assumes that the pi terms are the parameters to be estimated,
so the ML estimates for this model, obtained by setting the derivatives of the
log-likelihood function equal to 0
@‘
@pi
¼ yi
pi
 ni  yi
1  pi
¼ 0,
are given by
^pmaxi ¼ yi
ni
:
Therefore the log-likelihood ratio for the maximum model is given by
‘(^pmax; y) ¼
X
n
i¼1
yi  log yi
ni
 
þ (ni  yi)  log 1  yi
ni


þ log
ni
yi




:
If the estimates of p for the model of interest, corresponding to the estimates
bˆ 1, bˆ 2, . . . , bˆ k for the k parameters b1, b2, . . . , bk, are given by pˆi, then the
6.4
LOGISTIC REGRESSION ANALYSIS
163

deviance is given by
D ¼ 2 
X
n
i¼1
yi  log
yi
ni  ^pi


þ (ni  yi)  log
ni  yi
ni  ni  ^pi




¼ 2 
X
n
i¼1
oi  log oi
ei
 


,
where oi are the observed frequencies and ei are the estimated expected fre-
quencies. If the estimated model of interest ﬁts the data well, then its likeli-
hood should be similar to that of the maximum model and therefore the
deviance is x2-distributed
D  x2
nk:
In other words, if the estimated model is correct, the deviance should be x2-
distributed with n2k degrees of freedom. In conclusion, the null hypothesis
that the model is correct
H0 : M ¼ Mmax
versus the alternative hypothesis
HA : M = Mmax
can be tested by comparing its deviance statistic with the xn-k
2
distribution. The
test statistic, on average, should be equal to n2k, the mean of this x2 distri-
bution, if the null hypothesis H0 is true, and the p value can be compared
with a prescribed type I error a.
Nagelkerke (1991) proposed an adjusted R2 coefﬁcient of determination for
logistic regression, comparable to the adjusted R2 statistic for multiple linear
regression, given by
R2
adj ¼ R2
R2
max
,
where
R2 ¼ 1  L(0)
L(^b)

2=n
and
R2
max ¼ 1  ½L(0)2=n:
164
LOGISTIC REGRESSION MODELS

Here L(0) is the likelihood of the null model, the intercepts-only model, L(bˆ) is the
likelihood of the speciﬁed model, and n is the sample size. The Nagelkerke statistic
varies between 0 and 1. The Nagelkerke adjusted R2 statistic “measures” the pro-
portion of variation in the response explained by the speciﬁed model, with consider-
ation given to parsimony.
There are limitations to the use of statistics such as Nagelkerke’s R2 and deviance
error D in comparing competitive models
for goodness of ﬁt. Although
Negalkerke’s R2 measures the proportion of variation of the response explained by
the model, it tends to favor models that are overﬁt, those with “steeper” relationships
between the response and explanatory variables. Deviance error D also does not ade-
quately account for the need for parsimony; as the number of covariates and factors in
the model increases, the error or “bias” of the model ﬁt becomes less, but the precision
of the estimates for the coefﬁcient parameters also becomes less. As the number of cov-
ariates and factors in a linear model increases, so does the number of parameters. Since
each parameter estimate is based on a dataset with a ﬁxed number of shared samples,
the precision of the parameter estimates tends to decline as the number of parameters
increases. A balance must be struck between the reduction of error or bias and the
reduction of precision that occurs as the number of parameters in the models increases.
The statistic that most efﬁciently separates the noise from the information in the sample
dataset, which compares natural resource models for goodness of ﬁt with due consider-
ation given to parsimony, is the AIC criterion (Akaike 1973, 1974).
Akaike’s information criterion is given by
AIC ¼ D þ 2  k,
where the deviance D for the logistic regression model is
D ¼ 2 
X
n
i¼1
yi  log
yi
ni^pi


þ (ni  yi)  log
ni  yi
ni  ni  ^pi




,
the estimates pˆi of p are derived from the estimates bˆ 1, bˆ 2, . . . ,bˆ k in the logit and the
values for the covariates and factors at the ith covariate pattern, ni is the number of
observations at the ith covariate pattern, and k is the number of parameters in the
model. A covariate pattern is a single set of values for the covariates in the
model; that is, several observations may have the same covariate values, and hence
may have a covariate pattern. The AIC criterion is (up to a constant) a ﬁrst-order
Taylor series approximation of the Kullback–Leibler distance between the model
and the data (Burnham and Anderson 1998). The Kullback–Liebler distance
measures the amount of entropy, noise, or error, separated from the information,
in the sample dataset. A more precise second-order approximation of the
Kullback–Liebler distance is given by
AICc ¼ D þ 2  k þ 2  k  (k þ 1)
n  k  1
,
6.4
LOGISTIC REGRESSION ANALYSIS
165

where n is the sample size. Since AICc is more precise than AIC, we always
recommend using AICc, particularly for datasets with small sample size.
Models with lower AICc are better-ﬁtting models. Because AICc is a relative
measure of error, correct up to a ﬁxed constant, its value should not be interpreted
as an absolute measure of error. Models may therefore be compared using AICc
but should not be evaluated for goodness of ﬁt solely on the basis of this measure-
ment. Other methods should be used to evaluate goodness of ﬁt, such as the
Hosmer–Lemeshow test (Hosemer and Lemeshow 2000), classiﬁcation methods
including ROC, concordance, and residual analysis for logistic regression (see discus-
sion below).
Burnham and Anderson (2002) recommend that models with AICc within two
units of the best-ﬁtting model with minimum AICc be considered highly competitive.
Akaike weight may also be used, however, to compare the models. Recall that the
Akaike weight wi is given by
wi ¼
exp
(AICci  minm
j¼1(AICcj))
h
i.
2
n
o
Pm
k¼1  exp
(AICck  minm
j¼1(AICcj))
h
i.
2
n
o ,
and may be interpreted as the likelihood, or probability, that the model Mi is the best-
ﬁtting model of the collection of models Mj, j ¼ 1, 2, . . . , m, under consideration.
The leading models with a cumulative Akaike weight of 95% may be interpreted
as a 95% credible interval of models best ﬁtting the sample dataset, given the
collection of models under consideration in the analysis.
Reasons for the importance of the logistic regression GLM among natural resource
scientists for the analysis of data with binary response include (1) the shape of the
model curve, particularly for p near 0 and 1; and (2) interpretation of the linear
coefﬁcient bi of a covariate xi as the log of the odds ratio C of the speciﬁed linear
model in comparison with the null model without the covariate. We will explain
this idea next.
For a model M1 with one dichotomous independent variable x and logit
M1: logit ¼ g(p) ¼ b0 þ b1 x,
we can show that the coefﬁcient b1 is equal to the log of the ratio C of the odds, with
and without the variable x in the model. In other words, if the odds for a model are
given by
Odds ¼ Pr( y ¼ 1)
Pr( y ¼ 0) ¼
Pr( y ¼ 1)
(1  Pr( y ¼ 1)) ,
166
LOGISTIC REGRESSION MODELS

where Pr is the probability of the response y, then the odds ratio is given by
C ¼
PrM1(y ¼ 1)=1  PrM1(y ¼ 1)
PrM0(y ¼ 1)=1  PrM0(y ¼ 1)


¼ ½eb0þb1=(1 þ eb0þb1)=½1=(1 þ eb0þb1)
½eb0=(1 þ eb0)=½1=(1 þ eb0)
¼ eb0þb1
eb0
¼ eb1,
where M0 is the null model without the independent variable
M0 : logit ¼ g(p) ¼ b0
and M1 is the speciﬁed model. Hence
b1 ¼ log (C) ¼ log PrM1(y ¼ 1)=PrM1(y ¼ 0)
PrM0(y ¼ 1)=PrM0(y ¼ 0)


¼ log PrM1(y ¼ 1)=(1  PrM1(y ¼ 1))
PrM0(y ¼ 1)=(1  PrM0(y ¼ 1))


Alternatively, C can be expressed in terms of the linear coefﬁcient b1 as C ¼ eb1. If
the covariate is continuous rather than dichotomous, then this relationship can be
interpreted in terms of the addition of one unit to the covariate x, the change from
x ¼ 0 to x ¼ 1 in value. This amount of change in value of x will result in the mag-
nitude of change in the odds given by the odds ratio C ¼ eb1. See Hosmer and
Lemeshow (2000) for further details.
As with multiple linear regression, exploratory methods for model selection with
logistic regression can be utilized such as stepwise and best subsets selection. The
selection procedure for the stepwise methods is commonly based on the signiﬁcance
levels of the variables. As usual, be aware of the danger of compounded error and the
overﬁtting of data with these methods. Hosmer and Lemeshow (2000) provide more
detailed descriptions of these methods.
6.4.4
Goodness of Fit of the Logistic Regression Model
The statistics for the logistic regression model provide evidence for the comparative
capability, or competitiveness, of the models to ﬁt the sample dataset. However, these
statistics do not provide deﬁnitive evidence for goodness of ﬁt. The following
methods can be used to examine the goodness of ﬁt of the most competitive best-
ﬁtting models in order to complete the analysis.
The Hosmer–Lemeshow test is perhaps the most popular test for goodness of ﬁt
of the logistic regression model (SAS Institute Inc. 1995, Hosmer and Lemeshow
2000). The test consists of dividing the data that have been sorted by increasing
order of estimated probabilities into m percentile groups of approximately equal
size. The test is based on the Pearson x2 test statistic obtained from the 2  m
6.4
LOGISTIC REGRESSION ANALYSIS
167

table of observed and expected frequencies, where m is the number of groups and 2
refers to the “yes” and “no” responses. The number m of groups is often set to 10. The
statistic is given by
X2
HL ¼
X
m
i¼1
(oi  ni  ^pi)2
½ni  ^pi  (1  ^pi)
where ni is the number of observations in the group, oi is the number of “yes” obser-
vations in the group, and pˆi is the mean estimated probability of an event for the
group. If the null hypothesis H0 is true, that is, that the model provides a good ﬁt
to the data, this statistic XHL
2
is xm22
2
, x2-distributed with (m22) degrees of
freedom. The test can be conducted by comparing the p-value, derived from the
XHL
2
test statistic, with the prescribed type I error a. The Hosmer–Lemeshow test
is a conservative test with low power. It is dependent on how the observations are
grouped and the number of groups. Nonetheless, it is easy to apply and is the most
common test for goodness of ﬁt applied to the logistic regression model.
Another statistical method for testing goodness of ﬁt is based on nonparametric
bootstrapping, with computer-intensive Monte Carlo resampling of the original
observed dataset. Nonparametric bootstapping provides robust estimates of variance,
standard error, and conﬁdence intervals. The method was ﬁrst described by Bradley
Efron (1979) and has prove useful in a wide range of applications (Efron and
Tibshirani 1993, Manly 1997). With nonparametric bootstrapping, statistics are gene-
rated from the resampled data and used to provide estimates associated with the stati-
stics from the originally observed dataset. In practical application, the original sample
dataset is resampled with replacement, with 5000–10,000 samples usually required
to obtain satisfactory results. Nonparametric bootstrapping estimates of the standard
error of a parameter estimate from a sample dataset can be obtained by resampling the
original dataset with the same sample size, with replacement, and calculating the stan-
dard deviation of the resampled parameter estimates. The empirically generated
sample estimates are therefore used to estimate the standard deviation, or standard
error of the original estimate.
Nonparametric bootstrapping can also be used to estimate model selection fre-
quencies and estimation of precision based on model selection uncertainty in a col-
lection of candidate models (Burnham and Anderson 1998). The observed dataset
is resampled with replacement. The frequencies of occurrence of the best-ﬁtting
models based on Akaike’s information criterion (AIC or AICc) provide estimates
of the model selection frequencies. Estimates of variance and standard deviation
based on model selection uncertainty are obtained by calculating the means of
the resampled variance and standard deviation parameter estimates of the lead-
ing models. The reader is warned that, although nonparametric bootstrapping is gene-
rally successful for large sample sizes, it is not always reliable for small sample sizes
n ¼ 5–20 (Efron and Gong 1983). Furthermore, it is not always successful for model
selection (Burnham and Anderson 1998, 2002).
An alternative approach to bootstrapping, parametric bootstrapping, does not
resample the originally observed sample dataset, but instead generates simulated
168
LOGISTIC REGRESSION MODELS

new sample datasets, based on the model parameter estimates obtained by ﬁtting it to
the original observed sample dataset. Statistics are generated iteratively from the
simulated new sample datasets. Statistics generated from the original sample
dataset are then compared to the statistics generated from the simulated new
sample datasets. If the original sample dataset statistics are incongruent with the
simulated new dataset statistics, then the null hypothesis H0 of goodness of ﬁt is
rejected. Otherwise, the null hypothesis H0 is not rejected.
The deviance statistic for logistic regression models can be compared with simu-
lated new sample statistics for goodness of ﬁt with the deviance test using para-
metric bootstrapping. If the deviance of the original sample dataset falls outside
the conﬁdence region of the empirical distribution of the simulated sample
deviances, then the null hypothesis H0 of goodness of ﬁt is rejected. For
example, suppose that a deviance is estimated from an original sample dataset
and this statistic is at the 96.5% percentile among the deviance statistics generated
from the Monte Carlo simulation process. Then, with a conﬁdence level of 95% in a
one-tailed test, the null hypothesis H0 of goodness of ﬁt would be rejected.
Alternatively, if the deviance statistic were at the 67.2% percentile of the parametri-
cally bootstrapped deviances, then the null hypothesis H0 would not be rejected.
Parametric bootstrapping is a standard method for accessing goodness of ﬁt of logis-
tic regression models.
Another indication of model goodness of ﬁt is provided by the empirical evidence
of a model’s predictive effectiveness with classiﬁcation analysis. By evaluating the
extent of accuracy of the model at predicting the response values for samples from the
population, the analyst can assess the reliability of the model as a predictive tool and
can also assess accuracy on the original developmental dataset used to ﬁt the model.
Better and more rigorously, the analyst can resample the original sample with
nonparametric bootstrapping, viewing the new samples as representative of the
population, and assess the accuracy of the model on the new samples. With cross-
validation, the analyst sequentially deletes individual points in the sample and
assesses the accuracy of the model on the remaining subsets. Best of all and most
rigorously, the analyst can assess the accuracy of the model on new test sample data-
sets to provide the best indication of the predictive accuracy of the model for a
population.
The accuracy of logistic regression models is assessed with classiﬁcation analysis
as follows. A probability cutoff point pc for the ﬁtted model must ﬁrst be speciﬁed.
Observations with predicted response probability values pˆ  pc are assigned predic-
tive values of yp ¼ 1. Otherwise, observations with predicted response probability
values pˆ , pc are assigned predictive values of yp ¼ 0. An observed–prediction
classiﬁcation table is then constructed that provides the frequency counts of the
sample dataset with respect to observed response values y(x) and predicted response
values yp(x) for all observed values x, as follows:
In the table, frequency0,0 is the number of response values whose observed value is
0 and predicted value is 0, frequency0,1 is the number of response values whose
observed value is 0 and predicted value is 1, and so forth; ny¼0 is the number of
6.4
LOGISTIC REGRESSION ANALYSIS
169

responses with observed values equal to 0 with
ny¼0 ¼ frequency0, 0 þ frequency0, 1,
and ny¼1 is the number of responses with observed values equal to 1 with
ny¼1 ¼ frequency1, 0 þ frequency1, 1:
Similarly, ny p¼0 is the number of responses with predicted values equal to 0 with
nyp¼0 ¼ frequency0, 0 þ frequency1, 0,
and ny p¼1 is the number of responses with predicted values equal to 1 with
ny p¼1 ¼ frequency0, 1 þ frequency1, 1:
For example, consider the sample dataset of (x,y) values given by
x ¼ (6:6, 6:1, 4:3, 1:5, 9:6, 6:8, 1:8, 2:3, 3:7, 5:5)
and
y ¼ (0, 1, 0, 0, 1, 1, 0, 0, 0, 1)
with ﬁtted logistic regression model values given by
logit
^
¼ 7:3 þ 1:3  x
¼ (1:3, 0:6, 1:7, 5:4, 5:2, 1:6, 4:9, 4:3, 2:5, 0:1),
170
LOGISTIC REGRESSION MODELS

Figure 6.1. Plots of logistic regression response values for observed independent covariate
values x: dependent observed values y, estimated logit values, and estimated probability
values. (a) y versus x, (b) logit versus x, (c) Probability versus x.
6.4
LOGISTIC REGRESSION ANALYSIS
171

(i.e., bˆ 1¼ 27.3 and bˆ 2 ¼ 1.3) and predicted probability values
Pr ¼ elogit=(1 þ elogit) ¼ (0:79, 0:65, 0:15, 0:01, 0:99, 0:83, 0:01, 0:01, 0:08, 0:48):
See Fig. 6.1 for plots of these values. It is apparent from these graphs that a cutoff
point of, say, pc ¼ 0.40 ¼ 40%, will provide prediction values
yp ¼ (1, 1, 0, 0, 1, 1, 0, 0, 0, 1)
and an overall correct classiﬁcation rate of 0.90 ¼ 90.0% accuracy, with 9 of
10 points correctly classiﬁed. The observation–prediction classiﬁcation table is
given by
The probability cutoff point pc can be chosen using various criteria. It can be
chosen, for instance, to be the value that provides the best overall predictive accuracy,
or a certain level of sensitivity or speciﬁcity (see text below), or a compromise
between the two. A predictive logistic regression model can be recommended, there-
fore, from goodness-of-ﬁt classiﬁcation analysis not only with estimates for its para-
meters but also with a recommended cutoff point for its most effective application.
The overall correct classiﬁcation rate CC and overall error rate E can be calcu-
lated from a sample dataset and speciﬁed cutoff point pc by using the observation–
prediction classiﬁcation table as follows:
CC ¼ ( frequency0,0 þ frequency1,1)=n,
E ¼ ( frequency0,1 þ frequency1,0)=n:
In the example, CC ¼ 9
10 ¼ 0:90 ¼ 90% and E ¼ 1
10 ¼ 0:10 ¼ 10%:
Speciﬁcity is the proportion of the responses with observed values equal to 0 that
are predicted to be equal to 0. Recall that type I error is the probability of rejecting
the null hypothesis in an experiment, if the null hypothesis is true. In this context, the
null hypothesis is that the response is equal to 0. A type I error therefore is
an observation equal to 0 that is predicted to be equal to 1. Hence, given the
172
LOGISTIC REGRESSION MODELS

sample dataset and speciﬁed cutoff point pc, the speciﬁcity and type I error a are
given by
Specificity ¼ frequency0,0=ny¼0,
a ¼ frequency0,1=ny¼0:
In
this
example,
speciﬁcity ¼ 5
6 ¼ 0:833 or 83:3%
and
type
I
error
a ¼ 1
6 ¼ 0:167 or 16:7%.
An error of commission, or false positive, on the other hand, is an observed value
of 0 that is predicted to be equal to 1. The proportion of errors of commission there-
fore is given by
Errors of commission ¼ frequency0,1=ny p¼1:
In the example, errors of commission ¼ 1
5 ¼ 0:20 or 20%.
Sensitivity is the proportion of the responses with observed values equal to 1 that
are predicted to be equal to 1. Recall that type II error is the probability of not reject-
ing the null hypothesis, if the alternative hypothesis is true. The alternative hypothesis
in this context is that the response is equal to 1. So the type II error is an observation
equal to 1 that is predicted to be equal to 0. The sensitivity and type II error b for a
given cutoff point pc are given by
Sensitivity ¼ frequency1,1=ny¼1,
b ¼ frequency1,0=ny¼1:
In the example, sensitivity 4
4 ¼ 1:0 or 100% and type II error b ¼ 0
4 ¼ 0:0 or 0%.
An error of omission, or false negative, on the other hand, is an observed value of
1 that is predicted to be equal to 0. The proportion of errors of omission therefore is
given by
Errors of omission ¼ frequency1;0=nyp¼0:
In the example, errors of omission ¼ 0
5 ¼ 0:0 or 0%.
The receiver operating characteristic (ROC) curve is the graph of the function
ROC ¼ sensitivity=(1  specificity)
obtained for varying cutoff points pc. When the cutoff point pc ¼ 0, it is easy to see
that sensitivity ¼ 1.0 or 100% and speciﬁcity ¼ 0.0 or 0%, so ROC ¼ 1.0. As the
cutoff point increases toward pc ¼ 1, sensitivity decreases toward 0.0 or 0% and
speciﬁcity increases toward 1.0 or 100%, so (12speciﬁcity) also decreases toward
6.4
LOGISTIC REGRESSION ANALYSIS
173

0.0 or 0%, and the ROC curve tends toward their limit. Best-ﬁtting models tend to
have higher sensitivity and speciﬁcity and hence high ROC curves with a large
amount of area underneath them. The c statistic is a measurement of the area under-
neath the ROC curve and is sometimes used as an indicator of the ﬁtness of the
model. We have seen in Chapter 3 that sensitivity/(1 2 speciﬁcity) describes
Bayes factors that are useful for Bayesian statistical hypothesis testing. Therefore,
the best-ﬁtting models tend to be those with the highest c statistics from the ROC
curves, speciﬁcally, those models with the largest Bayes factors.
Another statistic that is sometimes useful in evaluating goodness of ﬁt of a logistic
regression model is the concordance statistic (SAS Institute Inc. 1995). Concordance
is the extent to which the order of the predicted response values yp is in agreement with
the order of the observed response values y for pairs of covariate values x. For example,
suppose that a pair of covariate values x1 and x2 have observed response values y1 ¼
0 , y2 ¼ 1. Then the predicted response values y1 p and y2 p are concordant with the
observed response values if y1 p  y2 p. If y1 p . y2 p, then the predicted response
values are discordant. The concordance statistic is the proportion of pairs of covariate
values (xi, xj), i = j, whose observed and predicted responses are concordant. The
concordance statistic ranges in value between 0.0 and 1.0. Concordance is calculated
by considering all pairs of covariate values with different values of the response vari-
able. If there are ni 0s and nj 1s in the sample dataset, there are a total of ni.nj ¼ nt pairs
that need to be examined for concordance. Ties are not included in the calculation. If
there are nt total distinct pairs (xi, xj) with different observed response values (yi, yj),
with nc concordant pairs and nd discordant pairs, then
Concordance ¼ nc=nt,
Discordance ¼ nd=nt:
The example will serve to illustrate this concept. There are a total of six 0s and
four 1s, so we need to examine 6.4 ¼ 24 pairs. Of these 24 pairs, there are 5.4 þ
1 ¼ 21 concordant pairs (look at the ﬁve 0s on the left and four 1s on the right,
and the lone 0 to the right with a singleton 1 to its right, in Fig. 6.1a).
Alternatively, there are just three discordant pairs (look at the lone 0 on the right
in the graph and the three 1s to its left). All other pairs are ties. Hence the concor-
dance is 21
24 ¼ 0:875 or 87:5%.
Several other statistics can be calculated from the concordance statistic nc, the Somer
statistic D, the gamma statistic g, the tau statistic t, and the c statistic, the area under a
ROC curve given by the sensitivity/(12speciﬁcity) for varying cutoff points
Somers D ¼ (nc  nd)=nt,
Gamma g ¼ (nc  nd)=(nc þ nd),
Tau t ¼ (nc  nd)= 1
2  n  (n  1)


,
c ¼
nc þ 1
2  (t  nc  nd)


=nt:
174
LOGISTIC REGRESSION MODELS

The gamma statistic g is a particularly popular statistic for assessing the extent of agree-
ment between observed and predicted values in the modeling of categorical data (Siegel
and Castellan 1988).
A goodness-of-ﬁt assessment would not be complete without a diagnostic analysis
of the residuals. Further discussion of this issue can be found in Hosmer and
Lemeshow (2000), SAS Institute, Inc. (1995), Dobson (1990), and McCullagh and
Nelder (1996).
6.5
OTHER GENERALIZED LINEAR MODELS (GLMS)
Numerous other GLMs are available for analysis of natural resource datasets
(McCullagh and Nelder 1996, Hardin and Hilbe 2001, S-Plus 2000). These GLMs
satisfy a range of model assumptions for population sample datasets with
varying error assumptions and link functions. All of these models satisfy the funda-
mental requirements for generalized linear models, with response variable values y1,
y2, . . . , yn assumed to have a distribution from an exponential family, a set of para-
meters b and explanatory variables x, and a monotonic link function g such that
g(my) ¼ x . b.
For binomial datasets, there are two other popular models besides the logit model
of logistic regression: the probit model and the complementary log–log model. Probit
models use an inverse cumulative normal distribution function as a link function
g(my) ¼ F1(my),
where my ¼ E[y] and its inverse is given by
f (x  b) ¼ F(x  b) ¼
1
s 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2  p
p
ðxb
1
e½(sm)2=(2s2)ds:
The probit model has proved useful for certain kinds of biological and social science
applications, such as the analysis of bioassay data. It provides a higher tail for the
response curve than the logit model.
The complementary log–log model uses the link function
g(my) ¼ log(log(1  my)):
It has also proved useful for certain kinds of biological applications, such as dose
response models, and also differs from the logit and probit models in the tail of
the response curve.
For count datasets of populations that are randomly dispersed, the Poisson
regression GLM model has proved useful. Poisson regression uses a logarithmic
link function and assumes that the means and variances are equal: E[y] ¼ my ¼
sy
2 ¼ E[(y 2 my)2]. If the count data are overdispersed, with m . s2, it is more
6.5
OTHER GENERALIZED LINEAR MODELS (GLMs)
175

appropriate to replace the Poisson regression model with the negative binomial GLM
model, using also a logarithmic link function.
Finally, for categorical contingency data, with cell frequencies that are Poisson-
distributed or multinomial-distributed with constraints, the multiplicative log-linear
GLM model with a logarithmic link function has been shown to be useful. With
this modeling approach, main effects and interaction effects can be examined, with
testing procedures and with model comparison.
The interested reader should consult more speciﬁc references for a fuller picture of
GLMs, such as Dobson (1990), McCullagh and Nelder (1996), Hardin and Hilbe
(2001), and McCulloch and Searle (2001).
6.6
S-Plus OR R AND WinBUGS APPLICATIONS
Let’s now illustrate the ideas of this chapter with an example. We ﬁrst generate a stan-
dardized sample dataset data1 from a habitat selection logistic regression “reality”
with standardized habitat variables aspect, species, old.growth, rock, moss, temp,
and moist, along with the logit function
logit ¼ 5  temp þ 10  moist
and binary response, as presented in Fig. 6.2.
6.6.1
Frequentist Logistic Regression Analysis in S-Plus and R
Next, let’s use an a priori parsimonious model selection and inference strategy on
dataset data1, analyzing the following collection of habitat selection models,
with covariates as follows:
1. Model 1: faspect, species, old.growthg
2. Model 2: faspect, species, mossg
3. Model 3: faspect, species, tempg
4. Model 4: faspect, species, moistg
5. Model 5: faspect, old.growth, tempg
6. Model 6: fspecies, old.growth, tempg
7. Model 7: fmoist, tempg (correct model)
8. Model 8: frock, moss, temp, moistg
9. Model 9: fg (null model)
10. Model 10: faspect, species, old.growth, rock, moss, temp, moistg (full
model).
The frequentist S-Plus and R logistic regression analysis of dataset data1 is
presented in Fig. 6.3.
176
LOGISTIC REGRESSION MODELS

The statistical results are presented in Fig. 6.4. Model 7, the correct model with the
two biologically signiﬁcant covariates, temp and moist, performed best with the lowest
AIC ¼ 20.08825. Model 9, with two specious covariates, rock and moss, as well as the
two biologically signiﬁcant covariates, temp and moist, performed second best with
AIC ¼ 23.09453. Model 10, the full model with all covariates, performed third best
with AIC ¼ 23.34713. The reader is encouraged to examine the covariates in each
of the models for statistical signiﬁcance at the 95% conﬁdence level (i.e., statistically
signiﬁcant if ts  tdf¼n2k, a¼0.025 ﬃ2). The statistical results conﬁrm that temp and
moist are the two biologically signiﬁcant covariates in the models.
Goodness-of-ﬁt analysis statistics for the best-ﬁtting correct model 7, including
Nagelkerke’s adjusted R2 and concordance statistics, are presented in Fig. 6.5a.
The partition and test results for the Hosmer–Lemeshow test are presented in
Fig. 6.5b and 6.5c. The classiﬁcation table for model 7 with results for varying
Figure 6.2. The S-Plus code used to generate the simulated habitat selection dataset datal.
In R, use the same code (below) except for the replacement of sd for stdev.
6.6
S-Plus OR R AND WinBUGS APPLICATIONS
177

cutoff points are presented in Fig. 6.5d. The highest correct classiﬁcation rate is
97.0%, which occurs at cutoff points pc ¼ 0.250–0.400. The sensitivity and speci-
ﬁcity rates at those cutoff points are 98.3% and 95.2%, respectively.
6.6.2
Bayesian Analysis in WinBUGS
Let’s next illustrate Bayesian statistical analysis of this same sample dataset data1
using WinBUGS, with an a priori parsimonious model selection and inference
Figure 6.2. Continued.
178
LOGISTIC REGRESSION MODELS

strategy on the 10 candidate models described in Section 6.6.1. The WinBUGS code
for the full model 10 is presented in Fig. 6.6a. The WinBUGS code for the other
models can be obtained by removing the appropriate covariates from the program
Figure 6.3. The S-Plus and R frequentist logistic regression statistical analysis results of data
datal, using predictive a priori parsimonious model selection and inference with AIC.
6.6
S-Plus OR R AND WinBUGS APPLICATIONS
179

Figure 6.3. Continued.
180
LOGISTIC REGRESSION MODELS

Figure 6.3. Continued.
6.6
S-Plus OR R AND WinBUGS APPLICATIONS
181

Figure 6.4. Comparative frequentist statistics for the logistic regression habitat selection models.
182

Figure 6.5. Goodness-of-ﬁt analysis results for the best-ﬁtting correct model 7 with logistic
regression analysis of dataset data1 (CAS Institute Inc., 1995). (a) Goodness-of-ﬁt analysis
statistics for model 7. (b) Partition for Hosmer–Lemeshow test for model 7. (c) Hosmer–
Lemeshow test results for model 7. (d) Classiﬁcation results for model 7.
6.6
S-Plus OR R AND WinBUGS APPLICATIONS
183

code logit and prior statements, data code list, and initial values code list. The
WinBUGS output for the leading model 7 is presented in Fig. 6.6b.
The WinBUGS comparative model output statistics are presented in Fig. 6.7. The
DIC results are very similar to the AIC results for the comparative model frequentist
analysis with the a priori parsimonious model selection and inference strategy for the
collection of 10 candidate models. In both cases, we conclude that model 7, the
“correct” model, is the best-ﬁtting model. The inferences, of course, are different,
with the Bayesian results providing posterior probability distributions for the para-
meters and permitting prior information to be included in the analysis as priors.
Figure 6.6. WinBUGS code and output for logistic regression models. (a) WinBUGS code for
the full model 10. (b) WinBUGS output for the leading model 7.
184
LOGISTIC REGRESSION MODELS

The frequentist results, on the other hand, provide probability assessments for the
dataset, without the beneﬁt of incorporating prior information formally into
the analysis.
6.7
SUMMARY
We began this chapter with an introduction and overview of frequentist generalized
linear modeling (GLMs). We discussed the importance of design for GLM modeling,
emphasizing the contrast between design-based and model-based approaches. We
presented GLM analysis for the exponential family of distributions, including the
assumptions, ML ﬁt, and statistics. We discussed the selection of models using
varying selection criteria. We then devoted most of the rest of the chapter to a discus-
sion of the most important GLM for the natural resource scientist, the logistic
regression model, presenting its assumptions, ML ﬁt, and statistics: parameter esti-
mates and Wald tests, deviance, R2, and AIC and AICc. We discussed model selec-
tion techniques using AIC and AIC weights, and goodness-of-ﬁt methods such as the
Hosmer–Lemeshow test, classiﬁcation statistics, and residual analysis. We brieﬂy
mentioned some alternative GLMs such as the probit and complementary log–log
models for binomial data and the Poisson and negative binomial models for count
Figure 6.6. Continued
6.7
SUMMARY
185

Figure 6.7. Comparative Bayesian and frequentist statistics for the logistic regression habitat selection models.
186

data. We concluded the chapter by illustrating the ideas of GLM using an example
with logistic regression frequentist analysis in S-Plus and R and Bayesian statistical
analysis in WinBUGS.
PROBLEMS
6.1
Conduct a frequentist statistical analysis of the sample dataset data2 (Fig. 6.8)
in S-Plus or R, using a predictive a priori parsimonious model selection and
inference strategy with AICc. The dataset data2 consists of habitat selection
modeling data for salamander binary presence–absence response with covari-
ates aspect, species, old.growth, rock, moss, temp, and moist. Use logistic
regression GLM with the logit link function and binomial response for a com-
parative analysis of the following 12 models with linear covariates:
(a) faspect, species, old.growthg
(b) faspect, species, rockg
(c) faspect, species, mossg
(d) faspect, species, tempg
(e) faspect, species, moistg
(f) faspect, old.growth, tempg
(g) frock, temp, moistg
(h) fmoss, temp, moistg
( i) fold.growth, tempg
(j) frock, moistg
(k) frock, moss, temp,moistg
( l) faspect, species, old.growth, rock, moss, temp, moistg (full model).
6.2
Conduct a frequentist statistical analysis of the sample dataset data2 (Fig. 6.8)
in S-Plus or R, using a descriptive a posteriori model selection–inference strat-
egy. Use logistic regression for a comparative analysis of models, including
best-subsets selection of the models with the 128 different combinations of
linear covariates. Use AIC as the model selection criterion.
6.3
Conduct a Bayesian statistical analysis of the sample dataset data2 (Fig. 6.8)
in WinBUGS, using a predictive parsimonious a priori model selection–
inference strategy with the deviance information criterion (DIC). Use logistic
regression GLM with a logit link function and binomial response for a compara-
tive analysis of the 12 models with the linear covariates prescribed in Problem
6.1 (above). Use normal priors on the beta parameters for the models, with a
mean of 0 and precision of 1026, except for the poor-ﬁtting overparameterized
full model, where a somewhat informative prior with mean of 0 and precision of
0.10 may be required to ensure convergence. Compare the Bayesian results with
the frequentist results in Problem 6.1.
PROBLEMS
187

Figure 6.8. Habitat selection dataset data2 for Problems 6.1–6.6.
188
LOGISTIC REGRESSION MODELS

Figure 6.8. Continued.
PROBLEMS
189

6.4
Conduct model averaging on the results of Problems 6.1 and 6.3, using AICc
and DIC weights. Calculate the unconditional estimates of the coefﬁcients of
old.growth and temp, with and without shrinkage, along with their uncondi-
tional estimates of standard error. Also calculate the importance of the
covariates.
6.5
Conduct a goodness-of-ﬁt classiﬁcation analysis of the best-ﬁtting model from
the a priori model selection–inference results of Problems 6.1 and 6.3, compil-
ing a classiﬁcation table with sensitivity, speciﬁcity, correct classiﬁcation, and
c ¼ sensitivity/(12speciﬁcity)
proportions
for
probability
cutoff
points
ranging from 0.05 to 0.95 in increments of 0.05. What is the optimal cutoff
point, based on correct classiﬁcation proportion?
6.6
Write a 3–5-page report, with attachments, summarizing the results of the
analysis in Problems 6.1–6.5. Include the following sections in the report:
abstract, introduction, statement of the problem, objectives, methods, results,
discussion, conclusions, and references. Include ﬁgures and tables in an appen-
dix. Specify the statistics used for the comparative analysis of the different
logistic regression models, with tables. What can be inferred for the best-
ﬁtting model:
logit~b0 + b1.aspect + b2.species + b3.old.growth +
b4.rock + b5.moss + b6.temp + b7.moist ?
Interpret the biological meaning of the statistical results from a management
perspective.
190
LOGISTIC REGRESSION MODELS

7
Introduction to Mixed-Effects
Modeling
For every complex problem, there is a solution that is simple, neat, and wrong.
—Henry L. Mencken
In this chapter we present an introduction to mixed-effects modeling. Mixed-effects
modeling provides an efﬁcient way of incorporating random effects, along with ﬁxed
effects, into the modeling of natural resource data. We begin this introduction by
describing the types of dependent datasets suitable for mixed-effects modeling. We
then describe mixed-effects generalizations of the mean model, with both ﬁxed
effects and random effects. We describe mixed-effects generalizations of the
ANOVA and multiple linear regression models. We describe variance–covariance
structures between groups and variance structures within groups with mixed-effects
modeling. Finally, we describe covariance structures within groups with mixed-
effects modeling, including serial and spatial correlation. We conclude with a brief
introduction to nonlinear mixed-effects modeling. Throughout this chapter, the
ideas will be illustrated with frequentist statistical analysis applications using the
rich library of mixed-effects utilities available in S-Plus and R and with Bayesian stat-
istical analysis applications in WinBUGS, employing the a posteriori exploratory
model selection and inference strategy. Users of R will want to activate the linear
and nonlinear mixed-effects modeling library by using the command library
(nlme) to initiate the frequentist statistical analysis described in this chapter.
7.1
INTRODUCTION
Multiple linear regression and ANOVA are based on an assumption of independence
for the sample datasets, requiring the residual errors to be independent. With many
natural resource datasets, however, this assumption of independence is clearly vio-
lated. Mixed-effects modeling provides a tool for modeling such dependent datasets.
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
191

It models ﬁxed effects with estimates of parameters for treatment and covariates
coefﬁcient effects, along with random effects describing the variation between
groups, or experimental units, of dependent observations in the dataset. There are
many types of dependent datasets common in natural resource applications, such
as pseudoreplicated data; temporal data such as repeated measures, time series, or
longitudinal data; spatial data; hierarchical data; and metapopulation data, all with
clustered dependences in the datasets. Random effects can be added to traditional
ﬁxed-effects models, such as ANOVA and multiple linear regression, to explain vari-
ation between the groups or clusters within the dependent datasets. The variance–
covariance structure between the random effects that describe the groups can also
be modeled, as can the variance–covariance structure within the groups.
Natural resource scientists have long been advised to avoid collecting dependent
datasets because of the independence assumptions required by many classical statisti-
cal methods. Simple random sampling requires sampling units to be collected inde-
pendently of each other. However, dependences are often an inherent characteristic of
natural resource populations. Mixed-effects modeling provides natural resource scien-
tists the tools to analyze these inherent dependences, sampling to incorporate them
into their datasets, and analyzing them with mixed-effects modeling.
Scientists analyzing datasets with groups of dependent observations are con-
fronted with two extreme alternatives. They can model (1) the entire population
with a classical ﬁxed-effects model, such as ANOVA or multiple linear regression
or (2) each individual group of data with a ﬁxed-effects model. Mixed-effects
models provide an effective and efﬁcient compromise between these two extremes,
one that is more accurate than the ﬁxed-effects modeling of the entire population
because it is less biased for each group. But it is also more efﬁcient than modeling
of each individual group since there are fewer parameters to estimate and the errors
in the parameter estimates will hence be smaller.
This chapter provides an introduction to the subject of mixed-effects modeling
using examples and the powerful capabilities available in S-Plus or R for frequentist
statistical analysis and in WinBUGS for Bayesian statistical analysis. There have been
major theoretical advances in this ﬁeld in recent years that have been implemented
into S-Plus and R, and we will take advantage of this opportunity. The reader can
investigate this subject in greater depth by referencing the more detailed account of
mixed-effects modeling in S-Plus by Pinheiro and Bates (2000). This chapter illus-
trates the exploratory data analysis that is possible in S-Plus and R. It also illustrates
how easily mixed-effects modeling can be incorporated into Bayesian statistical
analysis in WinBUGS.
7.2
DEPENDENT DATASETS
We begin by illustrating the most common types of dependent datasets that are
candidates for mixed-effects modeling: pseudoreplicated, temporally dependent,
spatially dependent, hierarchical, and metapopulation datasets.
192
INTRODUCTION TO MIXED-EFFECTS MODELING

A ﬁrst example of dependent data is pseudoreplicated data, consisting of measure-
ments collected within clusters. It may indeed be convenient, or biologically interest-
ing, to collect more than one measurement at a site or cluster center in a sample survey
or experiment. However, clusters create dependences in datasets since measurements
tend to be more similar within clusters than between clusters. The clusters create
groups of dependent data within the dataset. A natural resource scientist may be inter-
ested in examining trends or relational patterns in the datathat vary from groupto group.
Mixed-effects modeling will incorporate the group dependences into the analysis,
using random effects to model the variation between groups.
A second example of dependent data is temporally dependent data that are collected
at permanent plots or with subjects repeatedly measured in time. If the timepoints are
few in number, such data are commonly called repeated-measures data. If there are
larger numbers of timepoints, the data are called time-series data. Generally,
repeated-measures and time-series data are called longitudinal data. Repeated-
measures design is a commonly used experimental design for data that are collected
repeatedly in time. With natural resource applications, data may be collected over
time with respect to permanent sample plots oranimal subjects. Scientists may be inter-
ested in examining trends over time that vary between subjects or plots. Mixed-effects
modeling will incorporate the dependences in the data into the analysis, using random
effects to model the variation between subjects or plots.
A third example of dependent data is spatially dependent data, measurements
collected within close physical proximity that may be more dependent than those
at greater distances. Such dependences have long been recognized in geologic data
with geostatistical analysis. Natural resource scientists are increasingly recognizing
such dependences in their data and the need to include assumptions of dependence
in their analysis. Mixed-effects modeling will incorporate the dependences into the
analysis, using random effects to model the variation between locations.
A fourth example of dependent data is hierarchical data. Many natural resource
datasets are inherently hierarchical in structure. For example, collections of spores on
leaves, on branches, on trees, positioned at microsites, in forest stands, in national
forests can be conveniently sampled using a hierarchical multistage sampling
design, such as a six-stage design for this example. With hierarchical structure,
there are dependences in the data due to the grouping structure at the different
stages. Measurements in closer proximity to each other at the different stages of
the design may tend to be more similar, to be dependent. It will be more accurate
to model these dependences in the analysis. Mixed-effects modeling can incorporate
such dependences into the analysis.
A ﬁnal example of dependent data is metapopulation data. Many natural
resource populations consist of collections of subpopulations, distinguished by geo-
graphic or genetic differences. A metapopulation analysis provides a model for both
the overall population and the individual subpopulations. Mixed-effects modeling
provides models for both, with ﬁxed effects describing the overall population, and
the random effects describing the individual subpopulation differences.
In the next section, we will introduce linear mixed-effects modeling. We will
generalize the classical ﬁxed-effects ANOVA model to a mixed-effects model,
7.2
DEPENDENT DATASETS
193

incorporating random effects to model dependences in a dataset. Next, we will gen-
eralize the classical multiple linear regression model to include random effects. We
will conclude the chapter with a brief introduction to nonlinear mixed-effects model-
ing. Throughout this chapter, we shall use S-Plus or R and WinBUGS examples to
illustrate the ideas.
7.3
LINEAR MIXED-EFFECTS MODELING: FREQUENTIST
STATISTICAL ANALYSIS IN S-Plus AND R
A good example is the best sermon.
—Benjamin Franklin
In this section we focus on linear mixed-effects modeling using S-Plus or R. We
assume that the ﬁxed effects and random effects can be modeled with linear forms.
Later, we will generalize this modeling to include nonlinear mixed effects. We
start with a mixed-effects generalization to the mean model for normal data. With
mixed-effects modeling, we will assume that residual errors are independent and
identically normally distributed.
7.3.1
Generalizations of Analysis of Variance (ANOVA)
For the ﬁrst example, let’s generalize the mean model to a mixed-effects model, using
the dataset Seedlings1 in S-Plus or R in Fig. 7.1 to illustrate. The dataset
Seedlings1 consists of Douglas ﬁr seedling growth measurements (growth)
taken from ﬁve trays (Tray) in a greenhouse experiment. Seedlings1 is a
grouped data frame, grouped by Tray, derived from a data frame Seedlings, in
preparation for mixed-effects modeling in S-Plus or R.
Let’s ﬁrst ignore the obvious grouping created by the trays and analyze the dataset
using the normal mean model. The normal mean model assumes that the experimen-
tal design for the Seedlings1 dataset is completely randomized with Douglas ﬁr
seedling growth measurements randomly sampled from the greenhouse population
(Fig. 7.2a). The design of this sample dataset is not completely randomized,
obviously, because of the clusters of the seedlings in the trays (Fig. 7.2b). Ignoring
this problem momentarily, the mean model is given by
yi ¼ m þ ei,
where m is the mean growth and the residual errors are independent and identically
normally distributed ei  iid N(0, s) with mean 0 and standard deviation s. For
the analysis, we estimate the two parameters m and s using least-squares (LS) esti-
mation in S-Plus or R (Fig. 7.2c). The estimated mean and standard deviation
parameters are mˆ ¼ 6.01 and sˆ ¼ 1.93, respectively, to two digits of accuracy.
194
INTRODUCTION TO MIXED-EFFECTS MODELING

The residual standard error is large and highly affected by the variation between the
trays, as illustrated in Fig. 7.2d.
The problem with the mean model analysis is that it has ignored the obviously
large amount of variation in the growth measurements between trays in the dataset.
We might ﬁrst attempt to model this variation by letting Tray be a “treatment”
factor with ﬁve levels, 1–5. To do this properly, we would want to use a completely
randomized experimental design, randomly assigning measurements of growth to the
trays, viewed as ﬁxed treatment levels. The one-factor, ﬁxed-effects ANOVA model
would then be
yij ¼ m j þ eij,
i ¼ sample, j ¼ treatment (Tray),
where mj ¼ m þ tj is the jth Tray mean growth with treatment tray effect tj and the
residual errors are eij  iid N(0, s). For the analysis, let’s estimate the six parameters
mj, j ¼ 1–5, and s using LS estimation in S-Plus or R (Fig. 7.2e). The S-Plus or R
default options for unordered and ordered treatment factors in the lm command are
the Helmert and polynomial contrasts, respectively. However, the insertion of the
character string -1 in command formula growth~Tray-1 (above) prescribes the
Figure 7.1. The Seedlings1 dataset, generated as grouped data from the data frame
Seedlings, in S-Plus or R.
7.3
LINEAR MIXED-EFFECTS MODELING
195

cell means form option in S-Plus and R that estimates the treatment mean parameters
mj. The estimated ﬁxed-effects treatment mean parameters here are mˆ 1 ¼ 3.73, mˆ 2 ¼
4.78, mˆ 3 ¼ 6.20, mˆ 4 ¼ 6.28, and mˆ 5 ¼ 9.05, and the estimated standard deviation par-
ameter is sˆ ¼ 0.66, respectively. This analysis has “captured” the variation between
trays, reducing the residual error to one-third of its previous amount (Model 1.
Seedlings1: sˆ ¼ 1.93; Model 2. Seedlings1: sˆ ¼ 0.66).
Figure 7.2. Graphs and analyses of the Seedlings1 dataset for the mean and ﬁxed-effects
models
with
S-Plus
or
R
code.
(a)
Dotplot
of
the
growth
measurements
(dotplot(Seedlings1$growth)) (use stripchart instead of dotplot in R).
(b) Plot of the growth/Tray measurements (plot(Seedlings1)). (c) Analysis of
the mean model for Seedlings1. (d) Boxplot of the mean model growth/Tray residual
errors (plot(Seedlings$Tray,output1.Seedlings1$resid)). (e) Analysis of
the ﬁxed-effects model for the Seedlings1 dataset.
196
INTRODUCTION TO MIXED-EFFECTS MODELING

However, we are not really interested in Tray as a treatment variable but rather as
a “nuisance” variable. Tray is a variable that naturally and unavoidably occurs in the
greenhouse population, causing variation in the response. It is not a treatment variable
that can be manipulated by managers as a result of this experiment. We are not inter-
ested in determining which trays provide optimal growth response so that a manager
could manipulate the environment and select only the optimally performing trays. We
only need to acknowledge the natural occurrence of the trays in the population and the
unavoidable effect of this occurrence on the variation in response. Furthermore, we
cannot discern any particular patterns for its effect on the response. The trays
appear to have a random effect on the response that isn’t a function of their size or
any other characteristic. If there were an effect on the response due to the size of
the trays, we might then want to use the size of the tray as a variable in our modeling
for a ﬁxed effect. However, the variable Tray in our dataset has been measured only
by its number as a label and, in that sense, it is a random effect. Additionally, the
ﬁxed-effects ANOVA analysis is inefﬁcient in the sense that we have estimated
ﬁve different parameters, one for each of the Tray mean effects on the response.
Figure 7.2. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
197

Again, we are not really interested in the effect of each tray, only in the variation
between the trays. What we need is a way to estimate this variation only, with one
parameter, say, sb (b for block), using a random-effects model.
So let’s examine a mixed-effects model, to again analyze the Seedlings1
dataset, this time incorporating a random effect to capture the variation of the
response between trays. Here we would like to collect measurements that are random-
ized for each tray. The model will be
yij ¼ m þ b j þ eij,
i ¼ sample, j ¼ group (Tray)
where m is the mean growth, bj  iid N(0, sb) are the j Tray growth random effects
with the standard deviation parameter sb measuring the variation between the trays,
and eij  iid N(0, s) are the residual errors. We will estimate three parameters, m, sb,
and s, using restricted ML (REML) estimation in S-Plus or R. Restricted ML
(REML) integrates the likelihood function with respect to the ﬁxed-effects par-
ameters, and then estimates the random-effects parameters using maximal likelihood
(Pinheiro and Bates 2000) with the linear mixed-effects command lme in S-Plus and
R (see Fig. 7.3a). The estimated ﬁxed-effects mean parameter is mˆ ¼ 6.01, and the
random-effects standard deviation parameters are sˆ b ¼ 1.98 and sˆ ¼ 0.66. The stan-
dard deviation s for the residual error is actually also a random-effects parameter. The
random-effects model has captured most of the variation with the Tray grouping
variable random effects. The addition of just one random-effects parameter to the
mean model has reduced the noise to a third of its original amount in the mean
model (Model 1. Seedlings1: sˆ ¼ 1.93; Model 3. Seedlings1: sˆ ¼
0.66). Akaike’s information criterion (AIC) along with the Bayesian information
criterion (BIC) and the log likelihood are also included in the output, allowing us
to compare models having the same ﬁxed effects. None of the 95% conﬁdence inter-
vals for the estimates of the parameters contain 0, so the parameter estimates are all
signiﬁcant at a 95% conﬁdence level. The predicted random effects bj for each tray
can be added to the estimated ﬁxed effect mˆ to obtain the mean growth estimates.
For example, for Tray 1, mˆ þ b1 ¼ 6.0122.22 ¼ 3.79 (Fig. 7.3a). The residual
errors for this mixed-effects model are more reasonable, given the assumptions
(Figs. 7.3b and 7.3c).
The sample dataset Seedlings1 was simulated, using parameters m ¼ 6.0,
sb ¼ 1.5 (with random effects b1 ¼ 22.45772301, b2 ¼ 20.74215638, b3 ¼
0.05353809, b4 ¼ 0.30999812, and b5 ¼ 2.83634318), and s ¼ 0.5. The output stat-
istics are compatible with these parameter values for the population.
Next, let’s examine a second example, generalizing a one-factor ﬁxed-effects
ANOVA model to a mixed-effects model with one ﬁxed effect and one random
effect. We shall use the grouped data frame Seedlings2 in S-Plus or R to illustrate
the ideas (Fig. 7.4). The dataset Seedlings2 consists of growth response
measurements subjected to four levels, 1–4, of treatment variable Treatment.
The dataset contains measurements from an experiment examining the effects of
four nitrogen fertilizer treatment levels on Douglas ﬁr seedling growth in ﬁve differ-
ent trays in a greenhouse (Fig. 7.5a). We shall initially ignore the grouping variable
198
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.3. Analysis and graphs of the Seedlings1 dataset, for the mixed-effects general-
ization of the mean model with S-Plus or R code. (a) Analysis of the mixed-effects mean model
in S-Plus or R. (b) Boxplot of the mixed-effects mean model growth/Tray residual errors
(plot(Seedlings1$Tray,output3. Seedlings1$resid)). (c) Scatterplot of
the mixed-effects mean model growth standardized residual errors (plot(output3.
Seedlings1)).
7.3
LINEAR MIXED-EFFECTS MODELING
199

Tray that makes this dataset a candidate for mixed-effects modeling with a random
effect for between-subject variation of the response, that of the groups created by the
trays. Ignoring this Tray group effect for a moment, we ﬁrst examine a ﬁxed-effects
one-factor ANOVA model.
We will assume a completely randomized experimental design for the ﬁxed treat-
ment levels. We model the dataset with a one-factor, ﬁxed-effects ANOVA model:
yij ¼ m j þ eij,
i ¼ sample, j ¼ treatment (Treatment),
where mj ¼ m þ tj are the j treatment means and eij  iid N(0, s). We estimate ﬁve
parameters mj, j ¼ 1–4, for the ﬁxed effects, and the residual error standard deviation
s, using LS estimation (see Fig. 7.5b). The estimated ﬁxed-effects mean parameters for
the treatments are mˆ 1 ¼ 7.93, mˆ 2 ¼ 10.12, mˆ 3 ¼ 12.29, and mˆ 4 ¼ 14.24, and the esti-
mated residual standard deviation parameter is sˆ ¼ 1.78, respectively. We reject the
null hypothesis that the treatment mean parameters are the same, based on the
ANOVA F statistic. The estimated residual error standard deviation sˆ ¼ 1.78 is
small, but we should compare that with a more accurate model that factors in the
Figure 7.3. Continued.
200
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.4. The Seedlings2 dataset, generated as grouped data from the data frame
Seedlings, in S-Plus or R.
7.3
LINEAR MIXED-EFFECTS MODELING
201

response variation between the trays and better reﬂects the randomized block design of
the experiment, with treatment measurements taken within the “subjects,” the trays.
We need to account for the variation in response due to the trays. This “nuisance”
variable creates groupings of dependent measurements in the dataset and can best be
modeled as a random effect. Our model will be
yij ¼ mi þ bj þ eij,
i ¼ treatment (Treatment), j ¼ group (Tray),
where mi are the treatment effects, bj  iid N(0, sb) are the tray random effects with
standard deviation parameter sb “measuring” the variation between the trays, and
eij  iid N(0, s). Here we will estimate six parameters, mi for i ¼ 1–4, sb, and s,
using restricted ML estimation (REML) in S-Plus or R. The analysis is a two-way
ANOVA with a ﬁxed effect (Treatment) and a random effect (Tray), along
with residual error that is also a random effect (see Fig. 7.5c). The estimated ﬁxed-
effects mean parameters are mˆ 1 ¼ 7.93, mˆ 2 ¼ 10.12, mˆ 3 ¼ 12.29, and mˆ 4 ¼ 14.24,
and the random-effects standard deviation parameters estimates are sˆ b ¼ 1.85 and
sˆ ¼ 0.45. The random-effects model has captured most of the variation with the
Tray grouping variable random effects. The addition of the one random-effects par-
ameter to the ﬁxed-effects one-factor ANOVA model has reduced the noise to less
than
one-third
of
its
original
amount
in
the
mean
model
(Model
1.
Seedlings2: sˆ ¼ 1.78; Model 2. Seedlings2: sˆ ¼ 0.45). The 95% conﬁ-
dence intervals tell us that the parameter estimates are all statistically signiﬁcant at
the 95% conﬁdence level. The predicted random effects bj for each tray can be
added to the estimated ﬁxed effects mˆ i to obtain the mean growth estimates for
Figure 7.4. Continued.
202
INTRODUCTION TO MIXED-EFFECTS MODELING

each treatment and tray. For example, for Treatment 1 and Tray 1, mˆ 1 þ b1 ¼
7.93 þ (22.52) ¼ 5.41. The residual errors for this mixed-effects model are reason-
able, given the assumptions (Fig. 7.5d).
The sample dataset Seedlings2 was simulated, using parameters m1 ¼ 8.0, m2 ¼
10.0, m3 ¼ 12.0, m4 ¼ 14.0, sb ¼ 1.5 (b1 ¼ 22.45772301, b2¼20.74215638, b3 ¼
0.05353809, b4 ¼ 0.30999812, and b5 ¼ 2.83634318), and s ¼ 0.5. The analysis
results are compatible with these population parameters.
Figure 7.5. Graphs of the Seedlings2 dataset for the mixed-effects generalization of the
ANOVA model with S-Plus or R code. (a) Plot of the growth/Tray measurements for treat-
ments
1–4
(plot(Seedlings2)).
(b)
Fixed-effects
ANOVA
analysis
of
the
Seedlings2 dataset in S-Plus or R. (c) Mixed-effects analysis of the Seedlings2
dataset. (d) Scatterplot of the mixed-effects ANOVA growth standardized residual error
(plot(output2.Seedlings2)).
7.3
LINEAR MIXED-EFFECTS MODELING
203

Figure 7.5. Continued.
204
INTRODUCTION TO MIXED-EFFECTS MODELING

7.3.2
Generalizations of the Multiple Linear Regression Model
Let’s turn next to a mixed-effects generalization of the multiple linear regression
model. We shall illustrate with a linear regression example in S-Plus or R. We
examine the grouped dataset birds.density that measures the density of bird
counts (density) and amount of riparian habitat (riparian) at 100 sample
sites, 25 each in four different watersheds (Watershed) (see Fig. 7.6). The scatter-
plot of the (x, y) ¼ (riparian,density) measurements (Fig. 7.7a) and a signiﬁ-
cant correlation estimate rˆ ¼ 0.83 ( p , 0.01) suggest that it is reasonable statistically
to examine a linear relationship between the riparian covariate and the density
response.
We start by examining a linear regression model to analyze the dataset
birds.density. We will initially ignore the possible differences of the linear
relationship between watersheds. Recall that the linear regression model assumes a
linear relationship between an independent variable x (here, riparian) and a
dependent variable y (density) satisfying
yi ¼ b0 þ b1  xi þ ei,
with the ﬁxed-effects parameters b0 and b1 describing the y-intercept and slope of the
linear relationship, respectively, and with the residual errors ei  iid N(0, s). We
assume the following for the conditional population yjx ¼ fyj(x, y), x is ﬁxedg:
1. myjx ¼ b0 þ b1.x (linearity of the mean).
2. syjx ¼ s is constant (homoscedasticity).
3. yjx are normally distributed yjx  N(myjx, s).
4. yjx are randomly sampled (x can be either randomly sampled or ﬁxed).
5. x are measured without error.
Figure 7.5. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
205

Figure 7.6. The grouped dataset birds.density, in S-Plus or R.
206
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.6. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
207

The S-Plus or R results are presented in Fig. 7.7b. The linear regression estimates
bˆ 0 ¼ 39.95 for the y intercept and bˆ 1 ¼ 1.55 for the slope and the F statistic compar-
ing this model with the null model are all statistically signiﬁcant at the 95% conﬁ-
dence level. The R2 statistic is 68.4%. The estimated residual standard deviation of
Figure 7.6. Continued.
Figure 7.7. Graphs and analysis results of the birds.density dataset for the linear
regressionmodelwith S-Plus orRcode. (a) Scatterplot of the (riparian,density) measure-
ments (plot(birds.density$riparian,birds.density$density)). (b) Linear
regressionanalysis results.(c) Plotofthe linearregressionmodel (abline(output1.birds.
density$coef [1],output1.birds.density$coef [2])). (d) Scatterplot of the
linear regression residual errors (plot(output1.birds.density)). (e) Plot of the
linear regression model watershed residuals (plot(as.numeric(birds.density$
Watershed),output1.birds.density$resid)).
208
INTRODUCTION TO MIXED-EFFECTS MODELING

15.8 is somewhat large, and the residual errors include two outliers (Fig. 7.7d), but
the linear regression model otherwise looks promising overall for the given dataset
(Fig. 7.7c). However, we are ignoring the variation in the dataset due to the differ-
ences between the watersheds (Fig. 7.7e). Let’s look at that next.
The linear regression model has ignored the variation between the watersheds.
S-Plus and R provide scatterplots for each of the watersheds (Fig. 7.8a), where
the sample data points are connected with a local regression smoother (loess).
Figure 7.7. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
209

Figure 7.8. Graphs of the birds.density dataset for the individual linear regression
models with S-Plus or R code. (a) Scatterplot of the (riparian, density) measurements
for each watershed (plot(birds.density)). (b) Linear regression analysis for each
watershed. (c) Conﬁdence intervals of the individual linear regression estimates of constant
and slope (plot(intervals(output2.birds.density))). (d) Plot of the individ-
ual linear regression models (plot(augPred(output2.birds.density))). (e) Plot
of the individual linear regression residuals (plot(output2.birds.density)).
210
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.8. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
211

The linear relationships appear to be quite different between watersheds. The linear
relationship in watershed 3 is ﬂat, whereas that in watershed 1 is steep, with the
others falling in between. Also, the watershed 1 relationship is generally lower and
the watershed 3 relationship is higher, with the others in between. In fact, we
shouldnote that there isevidence ofa possiblenegative correlationbetween the steepness
and the height of the linear relationships for thewatersheds;the steeper relationships have
lower height, and vice versa. We shall examine more of that later. Let’s model the indi-
vidual linear regression relationships foreach watershed and compare using the S-Plus or
R lmList command (see Fig. 7.8b). The individual linear regression models for the
watersheds all have statistically signiﬁcant estimates for constant and slope. However,
the estimates are quite distinct between watersheds, and it is apparent that there is a
negative correlation between constants and slopes for the watersheds (Figs. 7.8c–7.8e).
We might be tempted to conclude our analysis at this point with separate linear
regression models estimated for each of the four watersheds. However, this con-
clusion would be ignoring the fact that this pooled dataset was collected from water-
shed subpopulations of one larger population. We would prefer to have a holistic
model that represents the entire population, but one that accounts for the distinctions
between the watershed subpopulations. Furthermore, a statistical objection to our
modeling of individual watersheds is that we have “over parameterized” our modeling
effort. We have estimated too many parameters, nine in total, a constant and slope for
each of four watersheds, plus an overall residual error standard deviation. Our indi-
vidual estimates are relatively imprecise, since there are so many of them obtained
from our dataset of ﬁxed sample size. How might we more efﬁciently incorporate
individual watershed differences into one model?
We will address this challenge by using the linear mixed-effects lme command in
S-Plus or R to examine mixed-effects models incorporating random effects. We will
start with the most general mixed-effects model. We assume a mixed-effects linear
relationship between an independent variable x (riparian) and a dependent vari-
able y (density) satisfying
yi ¼ (b0 þ b0) þ (b1 þ b1)  xi þ ei,
where the ﬁxed-effects parameters are b0 for the constant and b1 for the slope, the
watershed random effects for the constant b0 are iid N(0, sb0) and for the slope b1
are iid N(0, sb1), and the residual errors ei are iid N(0, s). We again assume the follow-
ing for the conditional populations yjx ¼ fyj(x, y), x is ﬁxedg and their parameters:
1. myjx ¼ (b0 þ b0) þ (b1 þ b1) . x (linearity of the mean).
2. syjx ¼ s is constant (homoscedasticity).
3. yjx are normally distributed: y  N(myjx, s).
4. yjx are randomly sampled (x can be either randomly sampled or ﬁxed).
5. x are measured without error.
Our S-Plus or R analysis results for this general mixed-effects linear regression model
are given in Fig. 7.9a. The estimates for the parameters are bˆ 0 ¼ 39.61 and bˆ 1 ¼ 1.56
212
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.9. Analysis of the birds.density dataset for mixed-effects linear regression
models with S-Plus and R code. (a) The general mixed-effects linear regression model analysis.
(b) Generalized least squares linear regression analysis on the birds.density dataset. (c)
Analysis for two reduced mixed-effects linear regression models. (d) Analysis of mixed-effects
linear regression models with variance–covariance structures between-groups random effects.
(e) Analysis of mixed-effects linear regression models with variance structures within-group
random effects. (f) Approximate 95% conﬁdence intervals for the added parameters in the
Power and Exp mixed-effects linear regression models with variance–covariance structures
between-groups random effects.
7.3
LINEAR MIXED-EFFECTS MODELING
213

Figure 7.9. Continued.
214
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.9. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
215

Figure 7.9. Continued.
216
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.9. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
217

Figure 7.9. Continued.
218
INTRODUCTION TO MIXED-EFFECTS MODELING

for the constant and slope ﬁxed effects, respectively, and sˆ b0 ¼ 25.06, sˆ b1 ¼ 0.60,
and sˆ ¼ 9.83 for the constant, slope, and residual random effects. The 95% conﬁ-
dence intervals for these estimates are all signiﬁcant. Note that the conﬁdence interval
for the estimated correlation between the constant and slope random effects con-
tains 0, so the correlation is not statistically signiﬁcant. The predicted watershed
random effects for the constant are b0 ¼ [223.54, 212.93, 33.38, 3.09] and for
the slope are b1 ¼ [0.57, 0.31, 20.80, 20.07]. The model estimates for data in water-
shed 1 are given by
yi ¼ (^b0 þ b0) þ (^b1 þ b1)  xi þ ei
¼ (39:61  23:54) þ (1:56 þ 0:57)  xi þ ei
¼ 16:07 þ 2:13  xi þ ei,
so that the ﬁrst data point in birds.density with y1 ¼ 41.91, x1 ¼ 19.02, and
e1 ¼ 214.70 is described by the estimated model with
41:91 ¼ 16:07 þ 2:13  (19:02)  14:70;
which is correct within roundoff error. The other data points in birds.density
can be obtained similarly.
The estimated residual standard deviation sˆ ¼ 9.83 is considerably less than that
of the ﬁxed-effects linear regression model sˆ ¼ 15.81. We can use the S-Plus or R
generalized LS command gls rather than the more restricted LS command ls to
execute the original ﬁxed-effects linear regression model analysis and also calculate
Figure 7.9. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
219

AIC so we can compare it with our mixed-effects model (Fig. 7.9b). We see that the
AIC statistic is considerably lower (764.35 vs. 839.78) for the mixed-effects model.
The anova command in S-Plus or R used in this context conducts a likelihood ratio
test between nested models with equal ﬁxed effects. Here our linear regression model
is nested inside the more general mixed-effects model; both have the same ﬁxed
effects. We see that the likelihood ratio test rejected the null hypothesis that the
two models are equal, providing yet more corroborative evidence along with the
other statistics that the more general mixed-effects linear regression model better
ﬁts the data than does the simple ﬁxed-effects linear regression model. Before
ﬁnally, concluding that we have a best-ﬁtting model, however, we should compare
these results with two other reduced mixed-effects models and with mixed-effects
models incorporating variance–covariance structures.
We have analyzed the most general mixed-effects model for the linear regression
relationship, one with watershed random effects with respect to both constant and
slope, Model #3a.birds.density. Now let’s look at the two reduced mixed-
effects models, one with watershed random effects for the constant only, Model
#3b.birds.density, and the other with watershed random effects for the
slope only, Model #3c.birds.density (Fig. 7.9c). Note that Model
#3b.birds.density estimates only sb0, assuming sb1 ¼ 0, and Model
#3c.birds.density estimates only sb1, assuming sb0 ¼ 0. The AIC and
ANOVA likelihood ratio results indicate that the most general mixed-effects
model, Model #3a.birds.density, is best-ﬁtting. This is certainly what we
expected from the original graphs and estimates of the linear regression models for
the individual watersheds (Figs. 7.8c–7.8e).
7.3.3
Variance–Covariance Structure Between-Groups Random Effects
Both S-Plus and R provide the capabilities to model variance–covariance structure
between the watershed group random effects. The graphs and estimates of the
linear regression models for the individual watersheds have suggested a negative cor-
relation between the constant random effects b0 and the slope random effects b1 in the
general mixed-effects model. We will explore this possibility by examining model
options for the variance–covariance structure between the random effects that are
available in S-Plus and R.
Recall ﬁrst the relationship between covariance sij and correlation rij of two
random variables indexed by i and j
rij ¼ sij=(sis j);
where si and sj are the standard deviations and si
2 and sj
2 are the variances. The
variance–covariance positive-deﬁnite matrix, illustrated in three dimensions, is of
the positive-deﬁnite form
s2
1
s12
s13
s2
2
s23
s2
3
2
4
3
5
220
INTRODUCTION TO MIXED-EFFECTS MODELING

The lower-left half of the matrix is omitted since the matrix must be symmetric:
sij ¼ s ji. S-Plus and R have 5 pdMat structures (pdMat ¼ positive 2 deﬁnite
matrix) that can be modeled:
1. pdSymm: symmetric matrix structure (most general, the default)
s2
1
s12
s13
s2
2
s23
s2
3
2
4
3
5
2. pdCompSymm: compound symmetry matrix structure, with equal variances si
2
and equal covariances sij (use for split-plot designs)
s2
i
sij
sij
s2
i
sij
s2
i
2
4
3
5
3. pdDiag: diagonal matrix structure, with covariances sij¼0
s2
1
0
0
s2
2
0
s2
3
2
4
3
5
4. pdIdent: multiple of an identity matrix structure, with covariances sij¼0 and
equal variances si
2
s2
i
0
0
s2
i
0
s2
i
2
4
3
5
5. pdBlocked: blocks of matrices down the diagonal (use for blocks of correlated
random effects)
S-Plus and R model variance–covariance structures as standard deviation–correlation
structures, using the lme command parameter syntax random¼~pdStructure
(covariates) where Structure
¼Symm, CompSymm, Diag, Ident, or
Blocked and the grouping structure is defaulted to that of the grouped database
object (here, Watershed in birds . density).
In the birds . density example, we can model a two-dimensional positive-
deﬁnite matrix for the two linear regression random effects
s2
1
s12
s2
2


:
We illustrate these methods with a pdDiag structure, Model #3d.birds.density,
and a pdIdent structure, Model #3e . birds . density in Fig. 7.9d. The Model
7.3
LINEAR MIXED-EFFECTS MODELING
221

#3d . birds . density and #3e . birds . density AIC results are 777.62 and 840.93,
respectively, compared to 764.35 for our most general Model #3a . birds . density
with default summetric positive-deﬁnite structure. Also, the S-Plus and R ANOVA
likelihood ratio tests indicate that Model #3a . birds . density is better-ﬁtting than
either of these two models with restricted variance–covariance structures. Note
that we make this inference, based on AIC, despite the fact that the estimated corre-
lation for the more general model wasn’t statistically signiﬁcant at the 95% conﬁ-
dence level.
We will not be able to illustrate the blocked pdMat variance–covariance structure
between groups with this simple two-dimensional example, but refer the reader to
Pinheiro and Bates (2000) for more complex examples.
7.3.4
Variance Structure Within-Group Random Effects
Next, we examine variance structures within the groups that we can model in S-Plus
and R. If we examine the scatterplot of the linear regression model and residuals in
Fig. 7.7 for the pooled dataset, we ﬁnd that there is some evidence to suggest that
the variance within the groups does not remain constant, a necessary assumption of
linear regression modeling. Although two key outliers in particular contribute to this
variation (see outlier points 61 and 66 in Fig. 7.7d), in general there does appear to
be a decrease in the variance as the riparian covariate increases.
So let’s explore the within-group variance structures that are available in S-Plus
and R, to model such change in variance. There are 6 such varFunct classes available
in S-Plus and R that model the within-group variance structure of the residual errors.
The S-Plus and R syntax for implementing this structure is to add the following
parameter to the lme command syntax
weights ¼ varStructureð½value,formÞ,
where value is an optional initial value for the parameter in the model structure,
form ¼ covariate for an outer covariate that varies within the groups, or
form ¼ ﬁtted(.)
for
the
model
ﬁtted
values
within
the
groups,
and
Structure ¼ Fixed, Ident, Power, ConstPower, Exp, or Comb. The
default variance for the residual errors is assumed constant within the groups
var(eij) ¼ s2 or se(eij) ¼ s for the standard deviation of the errors. S-Plus and R
model a “variance” function g (covariates) for the variance as a function of the cov-
ariate that varies within the groups var (eij) ¼ g2(covariate) . s2 or se(eij) ¼ g(covari-
ate) . s so that the default variance function is g(covariate) ¼ 1. The varFunct
classes are
1. varFixed:
a. Variance increases linearly with a covariate
var(eij) ¼ covariateij  s2,
so that g(covariate) ¼ covariate1/2.
222
INTRODUCTION TO MIXED-EFFECTS MODELING

b. Syntax is weights ¼ varFixed([value,]covariate).
c. Note that there are no parameters for this structure;
2. varIdent:
a. Variance is a function of the levels of a stratiﬁcation variable s for
the groups that is constant in value within the groups (i.e., an outer
factor)
var(eij) ¼ d2
ij  s2,
so that g (sij, d) ¼ dij.
b. Syntax is weights ¼ varIdent([value,]1js).
c. The parameters are the dij.
3. varPower:
a. Variance is a function of a power of the covariate
var(eij) ¼ jcovariateijj2d  s2,
so that g (covariate, d) ¼ jcovariatej.
b. Syntax is weights ¼ varPower([value,]covariate).
c. The parameter is d.
d. This
may
be
combined
with
stratiﬁcation
weights ¼ varPower
([value,]covariatejs).
4. varConstPower:
a. Variance is a function of a constant plus a power of the covariate
var(eij) ¼ (d1 þ jcovariateijjd2)2  s2
so that g (covariate, d1, d2) ¼ d1 þ jcovariatejd2.
b. Syntax is weights ¼ varConstPower([value,]covariate).
c. The parameters are d1 and d2.
d. Note
that
this
may
be
combined
with
stratiﬁcation
weights ¼
varConstPower([value,]covariatejs).
5. varExp:
a. Variance is a function of an exponential power of the covariate
var(eij) ¼ e(2dcovariateij)  s2,
so that g (covariate, d) ¼ e(dcovariate
ij
).
b. Syntax is weights ¼ varExp([value,]covariate).
c. The parameter is d.
d. Note
that
this
may
be
combined
with
stratiﬁcation
weights ¼
varExp([value,]covariatejs).
7.3
LINEAR MIXED-EFFECTS MODELING
223

6. varComb:
a. Variance is a combination of varFunct structures (above)
var(eij) ¼ g2
1  g2
2  s2,
so that g (covariate) ¼ g1 . g2.
b. Syntax
is
weights ¼ varComb([value,]
varStructure1(),
var Structure2()).
c. Note that this may be combined with stratiﬁcation.
We illustrate several of these varFunct structures in S-Plus and R with the birds.
density dataset in Fig. 7.9e.
The AIC values are 842.69, 766.31, 768.31, and 766.33 for the Fixed, Power,
ConstPower, and Exp varFunct models, respectively, compared to the general
Model #3a.birds.density AIC of 764.35. Although the Power and Exp
models are somewhat competitive with the general model, the AIC values nonethe-
less suggest that there is little advantage in adding a parameter to model the variance
within groups. Furthermore, the 95% conﬁdence intervals for the added parameters in
the Power and Exp models indicate that these added parameters are statistically
insigniﬁcant at the 95% conﬁdence level (see Fig. 7.9f).
7.3.5
Covariance Structure Within-Group Random Effects: Time-Series
and Spatially Dependent Models
Let’s conclude the frequentist S-Plus and R analysis of the birds.density dataset
by examining covariance structures within watershed groups. This structure models
dependences of the response errors within groups. Our dataset suggests that there
may some dependence, with the residual errors ﬂuctuating up and down within the
groups (see Figs. 7.8a and 7.8d); there may be an autocorrelation dependency of the
response errors with respect to the riparian within-group covariate. So let’s examine
the statistics from analyses modeling covariance structures within the groups.
S-Plus and R have corStruct classes to model covariance structure within
groups: (1) Symm, CompSymm, AR1, CAR1, and ARMA(p,q) for serial correlation
and (2) Exp, Gaus, Lin, Ratio, and Spher for spatial correlation. The within-
group covariance structure is modeled as
cor(eij, eij0) ¼ h(d(pij, pij0), r),
where d is a distance function between position vectors pij and pij0, r is a vector of cor-
relation parameters, and h is a correlation function. S-Plus and R model corStruct
classes for within-group covariance structures using the parameter syntax
correlation ¼ corStructure([ value,] form[, nugget=F])
wherevalue ¼ initialvaluesfortheparameters,form ¼  covariate(s)[jouter
factor(s)]or ﬁtted(.), and nugget ¼ F(default) or T for spatial structures.
224
INTRODUCTION TO MIXED-EFFECTS MODELING

Let’s ﬁrst focus on the serial correlation structures, sometimes called temporal,
time-series, or longitudinal structures. S-Plus and R model serial correlation as
the intraclass correlation coefﬁcient
cor(eij, eij0) ¼ h(jpij  pij0jr) ¼ r,
j = j0
using the empirical autocorrelation estimator of h at lag k ¼ jpij2pij0j with j0 ¼ j þ k
and pij the position of residual eij
^r(k) ¼
Pn
i¼1
Pnik
j¼1 rij  ri(jþk)=N(k)
Pn
i¼1
Pni
j¼1 r2
ij=N(0)
where N(k) denotes the number of residual pairs and rij ¼ (yij2yˆij)/sˆ ij are the stan-
dardized residuals with sij
2 ¼ var (1ij). S-Plus and R model serial covariance struc-
tures within groups with corStruct classes as follows:
1. corSymm: symmetric (default)
a. h(k, r) ¼ rk.
b. Syntax is
correlation ¼ corSymm([value,]form ¼ covariate).
c. Parameters are rk.
d. Note that this is overparameterized, to be used only as an exploratory tool to
determine more parsimonious models.
2. CompSymm: compound symmetric
a. h(k, r) ¼ r.
b. Syntax is
correlation ¼ corCompSymm([value,]form ¼ covariate).
c. Parameter is r.
d. Note that this is usually unrealistic, since it is usually better to assume that
the correlation decreases with the distance function, except for short time
series or with split-plot experiments having observations collected at the
same time within groups.
3. AR1: autoregressive, of order 1
a. h(k, r) ¼ rk, k ¼ lag.
b. Syntax is
correlation ¼ corAR1([value,]form ¼ covariate).
c. Parameter is r, 21  r  þ 1.
d. Data et are collected at integer timepoints t, where the distance between
observations et and es is the lag k ¼ jt 2 sj.
e. The correlation decreases in absolute value exponentially with lag k.
4. CAR1: continuous autoregressive, of order 1
7.3
LINEAR MIXED-EFFECTS MODELING
225

a. h(s, r) ¼ rs, s continuous,
b. Syntax is
correlation ¼ corCAR1([value,]form ¼ covariate).
c. Parameter is r, 21r þ 1.
d. Data et are collected at continuous timepoints t, where the distance between
observations et and es is the lag k ¼ jt2sj.
e. The correlation decreases in absolute value exponentially with continuous
time s.
5. ARMA (p, q): autoregressive, moving-average model, of orders p and q,
respectively, with
a. Autoregressive model AR(p): The current observation et is a linear func-
tion of the previous observations et2i, i ¼ 1, 2, . . . , p, plus a homoscedastic
noise term at with E[at] ¼ 0
et ¼ f1  et1 þ f2  et2 þ    þ f p  etp þ at,
where p is the order of the AR(p) model and the correlation parameters are
f ¼ (f1, f2, . . ., fp),
b. Moving-average model MA(q): The current observation et is a linear func-
tion of iid noise terms
et ¼ u1  at1 þ u2  at2 þ    þ uq  atq þ at,
where q is the order of the MA(q) model and the correlation parameters are
u ¼ (u1, u2, . . ., uq) with correlation function
h(k,u) ¼ uk þ u1  uk1 þ    þ ukq  uq
1 þ u2
1 þ    þ u2
q
:
c. ARMA(p,q) model ¼ AR(p) þ MA(q)
et ¼
X
p
i¼1
wi  eti þ
X
q
j¼1
uj  atj þ at,
with correlation parameters r given by the p autoregression parameters f ¼
(f1, f2, . . ., fp) and the q moving-average parameters u ¼ (u1, u2, . . ., uq).
d. Syntax is correlation ¼ corARMA(value,form,p,q).
e. Examine the partial autocorrelogram to determine p for AR(p) and
autocorrelogram to determine q for MA(q).
Let’s examine the ARMA correlation structure to model within-group correlation. We
start by looking for signiﬁcant correlation in the partial correlogram and correlogram
in S-Plus and R to suggest order values for p and q for the ARMA(p,q) model auto-
regression and moving average (Figs. 7.10a and 7.10b). There is no particular
226
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.10. Correlograms, semivariogram, and mixed-effects linear regression analyses of
the birds.density dataset for correlation within watershed group, with S-Plus and R
code. (a) Partial correlogram of the riparian density measurements within watersheds
(acf(output3a.birds.density$resid
[,2],
type ¼ “partial”,plot ¼
T)). (b) Correlogram of the riparian density measurements within watersheds (acf
(output3a.birds.density$resid
[,2],
type ¼ “correlation”,plot ¼
T)). (c) Analysis of the birds.density dataset for mixed-effects linear regression
models with S-Plus or R code to model ARMA correlation structure within-group
ARMA(1,0), ARMA(0,1), and ARMA(1,1) models. (d) Semivariogram of the riparian
density
measurements
within
watersheds
(plot(Variogram(output3a.birds.
density))). (e) Analysis of the birds.density dataset for mixed-effects linear
regression models with S-Plus or R code to model exponential spatial correlation structure
with a nugget for within-group error.
7.3
LINEAR MIXED-EFFECTS MODELING
227

Figure 7.10. Continued.
228
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.10. Continued.
7.3
LINEAR MIXED-EFFECTS MODELING
229

indication in the correlograms of signiﬁcant autocorrelation falling outside the 95%
conﬁdence bands, but we will proceed anyway, examining the ARMA (1,0), ARMA
(0,1), and ARMA (1,1)models, for the sake of illustration (see Fig. 7.10c). The
AIC values for the three models are 765.32, 765.23, and 767.01, compared to the
general Model #3a.birds.density AIC of 764.35, so the correlation modeling
has not helped. Furthermore, the 95% conﬁdence intervals for the correlation par-
ameters both intersect 0, so these parameters are not statistically signiﬁcant [we
show the ARMA (1,1) results only]. From the graphs of the dataset, the correlograms,
and these results, we can safely conclude that there is no further need to continue
within-group correlation analysis, examining higher-order ARMA models.
S-Plus and R also include spatial correlation structures for within-group variation
of the response residual errors with mixed-effects modeling that provide spatial stat-
istics. S-Plus and R provide models of isotropic spatial correlation structures, which
are continuous functions of distance between position vectors, in contrast to anisotro-
pic spatial correlation structures, which are continuous functions of both distance and
direction between position vectors. Response residual errors ex are observed at pos-
ition x and are assumed to be a function of the within-group covariate (riparian,
in our example). Three distance metrics are available in S-Plus and R (where r is the
dimension of the position vectors):
1. Euclidean or L2: dE(ex, ey) ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Pr
i¼1ðxi  yiÞ2
q
Figure 7.10. Continued.
230
INTRODUCTION TO MIXED-EFFECTS MODELING

2. Manhattan or L1: dMan(ex, ey) ¼ P
i¼ 1
r
jxi2yij
3. Maximum distance: dmax(ex, ey) ¼ maxi¼ 1,2, . . .,r jxi – yij
Spatial correlation structures are represented by their semivariogram g (d (ex, ey), r) ¼
1/2 . var(ex2ey) instead of their correlation function h(d(ex, ey), r), where r is a par-
ameter vector. They are complementary with respect to 1 if we assume the normally
distributed residuals are standardized exiid N(0, 1), since
g(d(ex ,ey), r) ¼ 1
2  var(ex  ey) ¼ 1
2  E½(ex  ey)2
¼ 1
2  (E½e2
x  2  E½ex  ey þ E½e2
y)
¼ 1
2  (var(e2
x)  2  cov(ex, ey) þ var(e2
y))
¼ 1
2  (1  2  cor(ex, ey) þ 1) ¼ 1  cor(ex, ey)
¼ 1  h(d(ex, ey), r):
So g(d, r) ¼ 1 – h(d, r) and g(0, r) ¼ 1 – h(0, r) ¼ 0. We note that semivariograms
increase with increasing distance and that correlations decrease with increasing dis-
tance, both ranging between 0 and 1. An estimator of the semivariogram is given by
^g(d) ¼
1
2  N(d)
X
i¼1n
X
for all j, j0 such that d(pij;pij0)¼d
(rij  rij0)2
where N(d) denotes the number of residual pairs at a distance d from each other and
rij ¼ (yij-yˆij)/sˆ ij are the standardized residuals with sij
2 ¼ var(1ij).
A nugget can be incorporated into the model, allowing a small discontinuity c0,
say, for measurement error, for g at 0: g(d, r) -. c0 as d -. 0 with 0 , c0 , 1.
Thus h(d, r) -. 12c0 as d -. 0 with 0 , c0 , 1.
There are ﬁve isotropic variogram models available in S-Plus and R (Pinheiro and
Bates 2000, pp. 232–233):
1. Exponential: g(d, r) ¼ 12exp(2d/r), generalizing the CAR1 serial model
2. Gaussian: g(d, r) ¼ 12exp(2(d/r)2)
3. Linear: g(d, r) ¼ 12(12d/r) . I(d , r)
4. Rational quadratic: g(d, r) ¼ (d/r)2/(1 þ (d/r)2)
5. Spherical: g(d, r) ¼ 12(121.5 . (d/r) þ 0.5 . (d/r)3) . I(d , r)
In the third and ﬁfth models I(d , r) ¼ 1 when d , r and ¼ 0 otherwise, and r is
the range parameter. The S-Plus and R corStruct parameter syntax for spatial
modeling is given by the command correlation ¼ corStruct(value,
form,nugget,metric),
where
value ¼ c(range.0,0nugget1)
and
value ¼ numeric(0)
is the
default,
form ¼ positionjgroup,
nugget ¼ T (or F) (nugget ¼ F is the default), and metric ¼ euclidean
(or manhattan or maximum). The ﬁve corStruct classes are corExp,
7.3
LINEAR MIXED-EFFECTS MODELING
231

corGaus, corLin, corRatio, and corSpher for the exponential, Gaussian,
linear, rational quadratic, and spherical spatial structures, respectively.
Let’s illustrate these ideas by examining the exponential spatial correlation structure
for the watershed within-group residual error in S-Plus and R. An assessment of model
form based on the semivariogram (Fig. 7.10d) is inconclusive, but let’s try an exponen-
tial model with a nugget to ﬁt this semivariogram (see Fig. 7.10e). The AIC doesn’t
compare favorably with the general mixed-effects model, increasing slightly to
766.19 compared to 764.35 for the general Model #3a.birds.density.
Removing the nugget would likely decrease the AIC, possibly by up to two units.
The estimates for the range and nugget are very small, close to insigniﬁcant, and the
semivariogram suggested no obvious form, so we risk overﬁtting models to our
sample data. The reader is encouraged to experiment with other spatial structures,
with and without nuggets. We conclude at this point that the general model, Model
#3a.birds.density, has provided the best ﬁt to our dataset. To complete the analy-
sis process, we should examine goodness of ﬁt of this model and competing models.
To further pursue the subject of spatial statistics, the interested reader is encouraged
to explore the capabilities of GSþ software (GSþ : Geostatistics for the Enviromental
Sciences 1998). With this software, spatial data can be more fully analyzed, modeling
dependences associated with each spatial location. The analyst can use both isotropic
and anisotropic semivariograms and kriging to model spatially dependent data and
display the results on maps. Kriging provides interpolated values for points not
physically sampled, based on the semivariograms. Spherical, exponential, linear,
and Gaussian forms are available in GSþ for the model forms. GSþ provides a
friendly and powerful platform for spatial statistical analysis.
7.4
NONLINEAR MIXED-EFFECTS MODELING: FREQUENTIST
STATISTICAL ANALYSIS IN S-Plus AND R
Let’s conclude the discussion of frequentist mixed-effects modeling by brieﬂy exam-
ining nonlinear mixed-effects modeling in S-Plus and R. We illustrate this application
by adding a trigonometric ﬁxed-effects component to the linear model, one that incor-
porates a periodic effect with amplitude, frequency, and initial value parameters.
Then we will add random effects to the amplitude, frequency, and initial value, as
well as the parameters of the linear component. As usual, the random effects are
due to the watershed groups for the mixed-effects nonlinear model. First we need
to deﬁne a nonlinear function in S-Plus or R, namely, trig, which is a function
of the covariate riparian and the parameters alpha, beta, amplitude, fre-
quency, and initial (Fig. 7.11a). We begin the nonlinear analysis with a ﬁxed-
effects model using the nonlinear least-squares command nls in S-Plus or R
(Fig. 7.11b). Note that we needed to declare “reasonable” initial values for the par-
ameters of the nonlinear nonlinear model that will be approximated with computer
iteration. Plots of the residuals of our results are not satisfactory because of the water-
shed differences (Fig. 7.11c), so let’s next examine ﬁxed-effects models for each
watershed group (Fig. 7.11d). We examine and compare conﬁdence intervals for
the estimates of the watershed parameters, and plot the results (Fig. 7.11e).
232
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.11. Analysis and plots of the birds.density dataset for nonlinear mixed-effects
modeling, with S-Plus and R code. (a) Creation of the function trig. (b) Nonlinear analysis
with a ﬁxed-effects model using the nonlinear least-squares command nls. (c) Plots of the
residuals of the nonlinear analysis with a ﬁxed-effects model (plot(output3r.birds.
density, Watershedresid(.), abline ¼ 0)). (d) Nonlinear analysis with ﬁxed-
effects models for each of the watershed groups. (e) Plot of the conﬁdence intervals for the
estimates of the watershed parameters for the nonlinear analysis with a ﬁxed-effects model for
each of the watershed groups (plot(intervals(output3s.birds.density))).
(f) Nonlinear analysis of a general mixed-effects model. (g) Nonlinear analysis of the
mixed-effects model without the frequency and initial random effects. (h) Plot of the nonlinear
analysis of the mixed-effects model without the frequency and initial random effects
(plot(output3u.birds.density,Watershedresid(.),abline ¼ 0)). (i)
Plot of the nonlinear analysis of the mixed-effects model without the frequency and initial
random effects (plot(output3u.birds.density)).
7.4
NONLINEAR MIXED-EFFECTS MODELING
233

On the basis of these results, we examine a general nonlinear mixed-effects model
(Fig. 7.11f). Note the ﬁxed ¼ . . . and random ¼ . . . parameters required for the
nlme command. We have chosen the pdDiag variance–covariance structure for
the parameters between the watershed groups to avoid overparameterizing our
model. We chose our starting values using the parameter estimates from the ﬁxed-
effects model. On the basis of AIC, our nonlinear mixed-effects model is certainly
an improvement over the nonlinear ﬁxed-effects model (AIC ¼ 788.11 vs. 845.23).
Figure 7.11. Continued.
234
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.11. Continued.
7.4
NONLINEAR MIXED-EFFECTS MODELING
235

Figure 7.11. Continued.
236
INTRODUCTION TO MIXED-EFFECTS MODELING

The insigniﬁcant estimates of the frequency and initial random effects suggest that
we delete them from the model (Fig. 7.11g). Our model results are an improvement,
with the lower AIC ¼ 784.11 and the statistically signiﬁcant parameter estimates as
indicated by the 95% conﬁdence intervals. Plots of the results are also indicative
of this improvement (Fig. 7.11h and 7.11i).
Unfortunately, we cannot compare AICs between the linear and nonlinear mixed-
effects models since the ﬁxed effects are different. However, looking at the estimates
of the ﬁxed-effects parameters for the nonlinear function, we conclude that there is
some evidence that a nonlinear mixed-effects model is an improvement over the
linear mixed-effects models that we have examined, at least for this dataset. We are
in danger, however, of overﬁtting this dataset, with a nonlinear model having 11
Figure 7.11 Continued.
7.4
NONLINEAR MIXED-EFFECTS MODELING
237

parameters for a dataset with n ¼ 100 observations. We would want to explore this
issue further before drawing a ﬁrm conclusion, analyzing goodness of ﬁt and exam-
ining further sample datasets, if possible, to see whether there really is the “periodic”
effect described by the trigonometric function in this model for the population.
We could continue with this exploratory analysis, but we will stop and leave it to
the curious reader to further investigate this fascinating and powerful world of linear
and nonlinear mixed-effects modeling. We refer you to Pinheiro and Bates (2000) and
Wolﬁnger (2000) for further details and examples.
7.5
CONCLUSIONS: FREQUENTIST STATISTICAL ANALYSIS
IN S-Plus AND R
7.5.1
Conclusions: The Analysis
We have conducted a partial exploratory a posteriori model selection analysis of the
birds.density dataset, using mixed-effects modeling in S-Plus or R. Our modeling
efforts provided linear and nonlinear regression models based on the fundamental form
density  riparian,
generalized to include random effects due to the watershed groups. Our linear results
are summarized in Fig. 7.12. The general mixed-effects model, Model #3a.birds.
density, with constant and slope random effects and general defaulted variance–
covariance structures, was the best-ﬁtting model, with an Akaike weight of 26.254%.
Two other models with temporal covariance structure within groups also performed
competitively, the ARMA(1,0) Model #3j.birds.density and ARMA(0,1)
Model #3k.birds.density, having Akaike weights of approximately 16%
each. However, given the limited sample size of the dataset, and the descriptive a pos-
teriori model selection and inference strategy we used, we may have experienced
some overﬁtting, particularly with the ARMA models.
7.5.2
Conclusions: The Reality of the Dataset
The dataset birds.density was simulated using the ﬁxed-effects linear relationship
y ¼ 10 þ 1.5x þ N(0, 30), with watershed random effects for the constant of N(0, 20)
(b0 ¼ [29.51, 21.78, 39.76, 18.75]) and for the slope of N(0, 0.50) (b1 ¼ [0.38,
0.26, 20.64, 20.17]). Hence b0 ¼ 10.0, b1 ¼ 1.5, s0 ¼ 20.0, s1 ¼ 0.50, and s ¼
30.0. It is interesting to compare this “reality” with the mixed-effects modeling results.
Of the best-ﬁtting models, we see that the estimated general mixed-effects model is com-
patible with the reality, whereas the models with covariance structures within the water-
shed groups are not but are examples of model overﬁtting. There is a specious correlation
in the dataset induced because the watersheds with the ﬂatter trends were higher; that is,
there isa negative correlation imposed between the constantand slope random effects.So
the general mixed-effects model was better-ﬁtting than the pdDiag mixed-effects
model used for the simulated data.
238
INTRODUCTION TO MIXED-EFFECTS MODELING

7.6
MIXED-EFFECTS MODELING: BAYESIAN STATISTICAL
ANALYSIS IN WinBUGS
Let’s illustrate Bayesian mixed-effects modeling in WinBUGS by returning to the
mixed-effects multiple linear regression example for the birds.density
dataset in Section 7.3.2. The WinBUGS code for the full mixed-effects model with
random effects for both the constant and slope in the linear regression model (cf.
output3a.birds.density)
is
presented
in
Fig.
7.13a.
The
Bayesian
WinBUGS output results with the noninformative priors (Fig. 7.13b) are similar to
the frequentist results, although the posterior samples of the random-effects par-
ameters are skewed and have means and medians that differ somewhat from those
of the frequentist mode estimates (e.g., frequentist sˆ ML ¼ 9.83 versus Bayesian
mean
of
s ¼ 10.08
and
median
of
s ¼ 10.03,
frequentist
sˆ 0ML ¼ 25.06
versus Bayesian mean of s0 ¼ 32.03 and median of s0 ¼ 26.97, and frequentist
Figure 7.12. Frequentist mixed-effects modeling results for the birds.density dataset,
with the number of parameters (k), AIC, and Akaike weights.
7.6
MIXED-EFFECTS MODELING: BAYESIAN STATISTICAL
239

Figure 7.13. Bayesian statistical analysis of the birds.density dataset for linear mixed-
effects modeling, with WinBUGS. (a) WinBUGS code for the full mixed-effects model, with
240
INTRODUCTION TO MIXED-EFFECTS MODELING

sˆ 1ML ¼ 0.6033 versus Bayesian mean of s1 ¼ 0.7891 and median of s1 ¼ 0.6472.
The DICs of this and the other Bayesian WinBUGS mixed-effects model analyses
comparable to the frequentist output3b.birds.density and output3c.
birds.density with constant random effects and with slope random effects
only indicate a ranking very similar to the frequentist AIC ranking of the models
3a, 3b, and 3c (see Fig. 7.14). The DIC criterion tends to slightly overﬁt sometimes,
as is illustrated with the DIC ranking of the mixed-effects model 3c with slope
random effect slightly ahead of the ﬁxed-effects linear regression model 1b, contrary
to the AIC ranking. As the reader can see from this example, mixed effects are
remarkably easy to incorporate into Bayesian modeling with WinBUGS.
7.7
SUMMARY
Mixed-effects modeling is an important, innovative statistical tool that generalizes
ﬁxed-effects methods such as ANOVA and multiple regression. It efﬁciently and par-
simoniously incorporates random effects into models, along with ﬁxed effects, to
describe dependences in datasets. Mixed-effects modeling is appropriate for
dependent datasets such as pseudoreplicated, temporally dependent, spatially depen-
dent, hierarchical, and metapopulation datasets. In this chapter we described mixed-
effects generalizations of the mean, ANOVA, and linear regression models. We
described the modeling of variance–covariance structures between groups with
mixed-effects models. We also described the modeling of variance and covariance
structures within groups with mixed-effects models, including serial and spatial cor-
relation. Finally, we brieﬂy introduced the topic of nonlinear mixed-effects modeling.
Mixed-effects modeling with frequentist statistical analysis was illustrated in S-Plus
and R and, with Bayesian statistical analysis, was illustrated in WinBUGS. Mixed-
effects modeling can be generally incorporated into other modeling methods, such
as generalized linear models (GLMs) and marked data analysis.
PROBLEMS
7.1
Conduct a frequentist statistical analysis of the S-Plus dataset Orthodont in
S-Plus or R (see Fig. 7.15). The Orthodont dataset available in S-Plus
Figure 7.14. Frequentist and Bayesian statistics for the four linear regression mixed-effects
models, with AIC and Akaike weights, and DIC and DIC weights.
7.7
SUMMARY
241

Figure 7.15. The Orthodont dataset in S-Plus (Pinheiro and Bates 2000, Potthoff and Roy
1964).
242
INTRODUCTION TO MIXED-EFFECTS MODELING

Figure 7.15. Continued.
PROBLEMS
243

(Pinheiro and Bates 2000, Potthoff and Roy 1964) consists of the response vari-
able distance, covariate variable age, and clustering categorical variables
Sex and Subject nested in Sex. The variable distance is a measurement
of the length between the pituitary gland and the pterygomaxillary ﬁssure, two
easily measured points on x-rays of skulls of 27 children, 16 males, and 11
females, taken from ages 8 to 14. Use an a priori parsimonious model selection
and inference strategy. First analyze and compare ﬁxed-effects constant, linear,
and quadratic models of distance with age:
distance ¼ b0 þ N(0, s),
distance ¼ b0 þ b1  age þ N(0, s),
distance ¼ b0 þ b1  age þ b2  age2 þ N(0, s):
Use the generalized least-squares gls command for the linear regression analy-
sis. Use AIC to compare the models, constructing a table with AIC and Akaike
weights to compare the ﬁxed-effects models.
7.2
Next, on the best-ﬁtting ﬁxed-effects model(s) from Problem 7.1 (above),
analyze and compare mixed-effects models, adding random effects due to
Sex, Subject, and Subject nested in Sex (use jSex/Subject in
S-Plus or R). Use an a priori model selection–inference strategy on the
mixed-effects models, the different random effects models with the same
ﬁxed-effects. Construct a table with AIC and Akaike weights to compare the
models, using AIC.
7.3
Examine the clustered datasets in Orthodont, using the S-Plus or R com-
mands plot(Orthodont) and plot(augPred(output)). Do the
model analysis results substantiate the visual differences observed between
the clustered datasets in Orthodont?
Figure 7.15. Continued.
244
INTRODUCTION TO MIXED-EFFECTS MODELING

7.4
Conduct a Bayesian statistical analysis of the S-Plus dataset Orthodont in
WinBUGS. Use an a priori parsimonious model selection–inference strategy.
First analyze and compare ﬁxed effects constant, linear, and quadratic models
of distance with age:
Distance ¼ b0 þ N(0, s),
Distance ¼ b0 þ b1  age þ N(0, s),
Distance ¼ b0 þ b1  age þ b2  age2 þ N(0, s):
Use DIC to compare the models, constructing a table with DIC and DIC
weights to compare the various ﬁxed-effects models.
7.5
Next, on the best-ﬁtting ﬁxed-effects model(s) from Problem 7.4, analyze and
compare mixed-effects models, adding random effects due to Sex, Subject,
and Subject nested in Sex. Use an a priori model selection–inference strat-
egy on the mixed-effects models, the different random-effects models with the
same ﬁxed-effects. Use DIC to compare the models, expanding the table of
Problem 7.4 with DIC and DIC weights to compare the models.
7.6
Do the model analysis results substantiate the visual differences observed
between the clustered datasets in Orthodont?
PROBLEMS
245


8
Summary and Conclusions
In this ﬁnal chapter, we will present a summary of the contemporary Bayesian and
frequentist statistical research methods for natural resource scientists that have been
introduced in this book, and draw some ﬁnal conclusions. We introduced this
subject in Chapter 1 with three important case studies in natural resource science
and offered some partial solutions to these problems, using traditional frequentist
methods of statistical analysis: parameter estimation, hypothesis testing, and linear
regression analysis. More complete solutions to these case studies were presented
throughout the remainder of this book.
Besides reviewing these methods, we cautioned the reader in Chapter 1 to provide
adequate attention to the planning and conclusion phases of a natural resource project
that surround the data collection phase. It is particularly important to consider the
statistical design in the planning phase and the statistical analysis and inference
methods in the conclusion phase of a project. Chapters 2–4 provided an introduction
to an alternative approach to the traditional frequentist approach to statistical analysis
and inference: Bayesian statistical analysis and inference. Chapter 5 presented two
contrasting strategies for model selection and inference and illustrated them using
multiple linear regression analysis models. Chapters 6 and 7 introduced two import-
ant contemporary methods of statistical analysis: generalized linear modeling and
mixed-effects modeling, illustrated with both frequentist and Bayesian approaches
to statistical analysis and inference. These contemporary approaches and methods,
Bayesian statistical analysis and inference, alternative strategies for model selection
and inference, generalized linear modeling, and mixed-effects modeling, provide
the natural resource scientist with an enriched toolbox of contemporary statistical
research methods for the analysis of their datasets.
8.1
SUMMARY OF SOLUTIONS TO CHAPTER 1 CASE STUDIES
Chapter 1 presented three case studies that served to illustrate some of the most
important statistical problems currently of interest to natural resource scientists.
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
247

These problems required solutions beyond the scope of the more common traditional
statistical methods available to the natural resource scientist. It has been the mission
of this book to present more advanced contemporary statistical research methods that
address these problems and enhance the toolbox of statistical methods available to the
natural resource scientist. We will summarize these methods in this chapter and draw
some ﬁnal conclusions.
8.1.1
Case Study 1: Maintenance of a Population Parameter Above
a Critical Threshold Level
Case study 1 in Chapter 1 addressed the problem of assessing the maintenance of a
population parameter above a critical threshold level. We illustrated this problem with
a proportion parameter p and critical threshold pc. We looked at the example of a
timber company required to maintain the proportion of its timberlands occupied by
nesting Northern Spotted Owl pairs above a speciﬁed threshold level pc determined
by biologists to ensure the viability of the local population of owls. Failure to main-
tain the population parameter p above the critical threshold level pc should result in a
decision of “corrective action,” the reduction of timber harvesting. Alternatively,
success at maintaining the parameter p above the threshold level should result in a
decision of “no action,” of maintaining the timber harvesting at current levels.
We looked at several common traditional statistical methods for analyzing datasets
obtained from monitoring the population. The ﬁrst traditional method consisted of a
sampling survey and parameter estimation. The parameter estimate pˆ based on the
sample could be compared with the critical threshold level pc in order to draw a con-
clusion. Or, the conﬁdence interval [ pˆ 2 E, pˆ þ E], where E is the sampling error,
based on a speciﬁed level of conﬁdence, could be compared with the critical
threshold level pc in order to draw a conclusion. Either of these methods poses
some
interesting
and
difﬁcult
challenges
in
application
and
interpretation.
Conclusions could be ambiguous, with estimates and conﬁdence intervals close to
or overlapping the critical threshold level. Frequentist interpretations are difﬁcult to
comprehend for many natural resource scientists in the context of repeated sampling,
since these methods provide interpretations based on probabilities for the sample
dataset, rather than probabilities for the parameter. They also provide no estimates
of risk.
The second common traditional statistical method for analyzing datasets obtained
from monitoring the population consisted of an experiment and hypothesis testing.
The null hypothesis could be formulated that the proportion parameter was equal
to the critical threshold level with an alternative hypothesis that the proportion para-
meter was below the critical threshold level. Again, there are problems with this
method used as a solution to the case study. Frequentist conclusions can be ambig-
uous, with the burden of proof placed on “corrective action.” Additionally, a p
value can be close to the level of signiﬁcance a. Frequentist interpretations pose a
challenge of interpretation to many natural resource scientists in the context of
repeated experimentation, since this method also provides an interpretation based
248
SUMMARY AND CONCLUSIONS

on probabilities for the sample dataset, rather than probabilities for the parameter. As
with parameter estimation, this method provides no estimates of risk.
Throughout this book, we have introduced some contemporary statistical methods
that address these concerns and provide more satisfactory solutions to case study 1.
Bayesian statistical analysis, presented in Chapters 2–4, provides probability distri-
bution assessments for the parameter. We saw that a choice of conjugate beta
priors for the proportion parameter p provide beta posterior solutions for binomially
distributed binary datasets. The Bayesian statistical solution provides inferences with
probability statements for parameters in the critical threshold problem, and it provides
estimates of risk. Hence, Bayesian statistical analysis provides a contemporary
alternative solution for case study 1.
8.1.2
Case Study 2: Estimation of the Abundance of a Discrete Population
Case study 2 presented the problem of estimating the abundance of a discrete popu-
lation. The abundance is measured with count data, nonnegative integer measure-
ments that are not normally distributed. However, the traditional method of
estimating mean abundance and conﬁdence intervals is based on normal distribu-
tional assumptions for the data, or, more speciﬁcally, for the mean estimates. In
the course of this book, we have provided more contemporary methods for solution
of this problem. We presented the Poisson model, which provides a solution if the
population is randomly distributed both spatially and temporally. This model can
be readily analyzed with Bayesian statistical analysis, using either a conjugate
gamma prior and posterior solution or a Markov Chain Monte Carlo (MCMC)
solution in WinBUGS for the Poisson distributed data. If the distribution of the popu-
lation is aggregated spatially and temporally, the negative binomial model may be
used to model the data, using an MCMC solution in WinBUGS, providing a solution
to the problem in case study 2. These ideas were presented in Chapters 2 and 4.
8.1.3
Case Study 3: Habitat Selection Modeling of a Wildlife Population
Case study 3 presented the problem of the habitat selection modeling of a wildlife
population. The objective of habitat selection modeling is to model the response of
a wildlife population as a function of habitat attributes such as vegetation, geologic,
and climatic variables. In Chapter 5 we examined a traditional approach to habitat
selection modeling using multiple linear regression. This method is appropriate for
response data that are normally distributed, such as biomass or abundance measure-
ments. For binary response data such as presence–absence observations that are bino-
mially distributed, the contemporary method of generalized linear modeling that was
presented in Chapter 6 is more appropriate. The method of generalized linear model-
ing can be applied to response data that are constrained and not necessarily normally
distributed. It provides linear models for constrained response measurements by using
link functions. Multiple linear regression provides a traditional method, and the gen-
eralized linear model of logistic regression provides a more contemporary method for
the habitat selection modeling of wildlife populations. With both of these methods,
8.1
SUMMARY OF SOLUTIONS TO CHAPTER 1 CASE STUDIES
249

either frequentist or Bayesian approaches to the statistical modeling may be used.
Either of the two strategies for model selection and inference that are described in
Chapter 5 may also be used with either of these methods: a priori parsimonius
model selection and inference using AIC and a posteriori model selection and infer-
ence. The former is generally more appropriate for predictive modeling, whereas the
latter is more appropriate for descriptive modeling.
8.2
APPROPRIATE APPLICATION OF STATISTICS IN THE
NATURAL RESOURCE SCIENCES
In this section we will summarize some recommendations for the appropriate appli-
cation of statistics in the most common areas of neglect and misuse of statistics in the
natural resource sciences. These common areas of misapplication include
1. Insufﬁcient planning and development of sufﬁciently rigorous statistical design
for data collection
2. The overuse of hypothesis testing
3. The misuse of observational data
4. Inadequate attention given to analysis, interpretation, and conclusions
5. The overuse of a posteriori “data dredging” methods for comparison of statisti-
cal models
6. The misuse of traditional statistical methods on dependent data
These problem areas have been identiﬁed and discussed in previous chapters.
A leading area of neglect in the use of statistics in the natural resource sciences has
been the lack of sufﬁcient planning and failure to develop a sufﬁciently rigorous
statistical design for data collection. A signiﬁcant amount of attention should be dedi-
cated to the initial planning phase of a data collection project, particularly to the
development of a rigorous statistical design for the data collection. With a sample
survey, the sampling design should be clearly identiﬁed before the data are collected.
Consideration should be given to incorporating a design that provides unbiased esti-
mators that are efﬁcient, with acceptable bounds on error. If the estimators are biased,
that bias should be estimable. The principles of randomization should be applied in
the sampling design as appropriate. Many effective sampling designs, such as simple
random sampling, stratiﬁed sampling, cluster sampling, systematic sampling, multi-
stage sampling, multiphase sampling, variable-probability sampling, and adaptive
sampling, are available for use with data collection. Sample size should be estimated
to provide an adequate bound on sampling error to meet the objectives of the study.
Further details on survey sampling design and analysis are given in Cochran (1977),
Scheaffer et al. (1996), Thompson (1992), and Sarndal et al. (1992).
Similarly, with experiments, adequate attention should be devoted to the speciﬁ-
cation of rigorous experimental design before data collection. Again, randomization
and replication are of paramount concern. Effective experiments should be conducted
250
SUMMARY AND CONCLUSIONS

under controlled conditions constraining the values of attributes affecting the
response that are not of interest in the study. Such controlled conditions are often dif-
ﬁcult to attain in natural resource settings. The number of replicates for the experi-
ment should be determined prior to data collection and should be sufﬁcient to
ensure a speciﬁed level of power for a given conﬁdence level to statistically dis-
tinguish biologically important differences in the response between treatment
levels. The distinction between biologically and statistically signiﬁcant differences
should be minimized. Further details on experimental design and analysis are
given in Hicks (1993) and Kuehl (1994).
Hypothesis testing, which has been overused with natural resource science data,
should be applied to scientiﬁc hypotheses, not statistical hypotheses, in natural
resource experiments. The mean growth of seedlings due to different fertilizer treat-
ments can be described with clearly deﬁned scientiﬁc hypotheses, whereas the differ-
ences in the abundance of wildlife between two speciﬁc forest stands can be
described only by a statistical hypothesis. “Silly” null hypotheses that are patently
false should be avoided at all cost. There will clearly be some amount of difference
in the abundance of wildlife between two speciﬁc forest stands, no matter how small,
and a sufﬁciently large amount of data replication will be able to statistically dis-
tinguish between these differences. In this case where the alternative hypothesis is
clearly true, hypothesis testing will provide statistical results that depend on the mag-
nitude of biological difference and the amount of data collected rather than whether
there is a biologically signiﬁcant difference. This misuse of hypothesis testing can
be avoided by placing emphasis on estimation of the difference with a sufﬁcient
large sample size to provide the precision required to detect biologically
signiﬁcant differences.
The data collected in many natural resource studies is observational, rather than
experimental. Observational data, unlike experimental data, are collected without
rigorous adherence to randomization. With rigorous experimental design, experimen-
tal units must be both randomly selected from a population and randomly assigned to
treatment levels. These two requirements are often violated in natural resource
studies, particularly the ﬁrst requirement, where the small amount of population
remaining from an original population does not provide adequately representative
units for an “experiment.” This situation is illustrated with studies comparing old-
growth with young-growth habitat in forest stands along the coast of northern
California. The few stands of old-growth habitat that remain extant clearly do not
provide a representative sample from the original population that existed several cen-
turies ago before human intervention. So inferences obtained from analysis of obser-
vational data obtained in studies comparing conditions in these remaining old-growth
stands with conditions in other stands must be regarded with due caution. See
Johnson (1999) for further discussion of this issue.
Be sure to devote adequate attention to analysis, interpretation, and the drawing of
conclusions at the end of natural resource data collection projects. The methods of
analysis should be clearly identiﬁed prior to data collection so that the statistical
design for the data collection can be speciﬁed for the analysis to address the objec-
tives of the study. The analysis should receive an adequate amount of attention in
8.2
APPROPRIATE APPLICATION OF STATISTICS
251

order to sufﬁciently extract the biological information from the sample dataset. The
analysis, interpretation, and conclusions drawn from the dataset are a key component
of the study and should be treated as such.
“Data dredging” methods of analysis have been overused in the natural resource
sciences. This strategy for identifying models for comparison after the collection
of data can often be helpful in detecting attributes and models of importance for bio-
logical populations that are not well understood. However, data dredging tends to
overﬁt sample data and compound error in the model selection process. Results
from this strategy should be viewed tentatively, as descriptive rather than predictive.
To avoid overﬁtting of data and compounding of error, use the a prior parsimonious
model selection–inference strategy with information-theoretic criteria such as
Akaike’s information criterion (AIC) and the deviance information criterion (DIC).
This strategy can be particularly effective if the population of interest is somewhat
understood biologically so that a collection of reasonably plausible models can be
hypothesized prior to data collection.
Avoid using traditional methods of statistical analysis for dependent data.
Traditional methods often require assumptions of independence that are violated
with dependent sample datasets. Historically, natural resource scientists have been
cautioned to avoid collecting dependent datasets for that reason. Now however,
with the development of methods such as mixed-effects modeling, natural resource
scientists can be encouraged to collect sample datasets that are representative of
the biological processes of interest even if those systems contain dependences. As
we have seen, mixed-effects modeling can then be used to model these dependences
by incorporating random effects that explain the variation between the clustered
groups in the dependent data.
The objective of this book has been to present an introduction to contemporary
methods of statistical analysis that address many of these problem areas of neglect
and misuse of statistics in the natural resource sciences.
8.3
STATISTICAL GUIDELINES FOR DESIGN OF SAMPLE
SURVEYS AND EXPERIMENTS
Here are some important statistical guidelines for the natural resource scientist
for the design of sample surveys and experiments, in the planning phase of the
project:
1. Projects proposing to estimate parameters, such as abundance, presence or
absence, productivity, survival, recruitment, movement, or ﬁtness, should indi-
cate the following prior to data collection:
a. The amount of precision at a speciﬁed conﬁdence level required to meet the
objectives of the project
b. The sample size required to realize that level of precision
252
SUMMARY AND CONCLUSIONS

2. Projects proposing to conduct an experiment with a comparison of treatments using
hypothesis testing should include a power analysis, prior to data collection, indicating
a. The effect size, the biologically signiﬁcant difference of interest in the
alternative hypothesis, required to meet the objectives of the project
b. The conﬁdence level
c. The number of replicates required to attain a speciﬁed level of power in
the experiment
3. Any project proposing to detect and estimate a trend in a parameter, such as
abundance, presence or absence, productivity, survival, recruitment, move-
ment, or ﬁtness, should indicate the following prior to data collection:
a. The effect size, the amount of decrease or increase in the trend that is of
biological interest
b. The number of replicates required at a speciﬁed conﬁdence level and
power to detect this trend, based on a power analysis.
8.4
TWO STRATEGIES FOR MODEL SELECTION AND
INFERENCE
In Chapter 5, we contrasted two alternative strategies for model selection and infer-
ence. Traditional model selection and inference have been based on a descriptive a
posteriori model selection–inference strategy. In this strategy, models are selected,
analyzed, and compared after data collection. Burnham and Anderson’s (1998,
2002) important book has identiﬁed an important alternative strategy for model selec-
tion and inference based on predictive a priori parsimonious model selection and
inference using information-theoretic criteria. In their strategy, a parsimonious collec-
tion of models is prescribed for analysis prior to data collection. We suggest that each
strategy has its place and is effective in appropriate contexts.
The a posteriori strategy is useful for the descriptive modeling of sample datasets.
A multitude of a posteriori methods may be utilized with this strategy. Scatterplots
and other graphs may be examined, and correlations may be assessed for the
sample dataset, along with other traditional statistics. Stepwise and best-subsets selec-
tion methods may be employed. Powerful algorithms perform these procedures with
fast and efﬁcient software programs that implement these methods. These methods
can be temptingly easy to use with current software, and therein lies a danger, for
these methods are prone to compounded error and model overﬁtting. Hence,
results based on this strategy of model selection should be interpreted with caution
and viewed as descriptive and preliminary, tentative, and subject to further validation.
Results stemming from this strategy can be used to formulate hypotheses for model
selection with additional sample datasets. Sometimes pejoratively called “data dred-
ging” by some statisticians because of its susceptibility to overuse, this a posteriori
strategy is nonetheless highly useful, particularly for the exploratory data analysis
of populations that are not well understood.
8.4
TWO STRATEGIES FOR MODEL SELECTION AND INFERENCE
253

The second alternative strategy for model selection and inference suggested by
Burnham and Anderson is appropriate for populations that are better understood.
Burnham and Anderson recommend identifying a collection of candidate models a
priori to data collection. The collection of candidate models should be parsimonious:
relatively small in number, with each model having a relatively small number of cov-
ariates. Burnham and Anderson also recommend the use of the information-theoretic
criterion, Akaike’s information criterion (AIC) or the corrected AICc (Akaike 1973)
for evaluating the frequentist relative ﬁt of statistical models to sample datasets. This
relative measure of the Kullback–Liebler distance between the model and reality pro-
vides a way, along with the restriction of the analysis to a parsimonious collection of
candidate models, of avoiding the problems associated with the compounded error and
model overﬁtting of the traditional a posteriori strategy of model selection and infer-
ence. The Akaike information criterion effectively separates signal from noise in a
sample dataset. The model with the lowest AIC value is the best-ﬁtting model
among the collection of candidate models to the population reality represented by
the sample dataset. The statistical inferences from this strategy are hence dependent
on both the collection of candidate models and the sample dataset. Therefore, this strat-
egy is most suitable for populations that are better understood biologically, to ensure
that viable leading models are well represented among the collection of candidate
models. Such a strategy has been successfully applied to endangered species popu-
lations such as the Northern Spotted Owl and the Marbled Murrelet, which have
more recently been the source of numerous studies by natural resource scientists. If
populations are not sufﬁciently understood, a collection of candidate models may
possibly not contain any well-ﬁtting models. This strategy will then break down and
accomplish little beyond ranking a collection of poorly ﬁtting models. It is also possi-
ble, if a variety of contrasting models are not well chosen, that many or most of the
models will be reasonably well ﬁtting, and the strategy will then merely rank a collec-
tion of well-ﬁtting models, many of which might be suitably useful for natural resource
managers. Hence, good judgment suggests that this strategy should be applied to popu-
lations for which there is at least some reasonable level of biological understanding.
Of course, regardless of strategy and competitiveness of ranking, goodness-of-ﬁt
methods should always be included in the analysis to ensure that the best-ﬁtting
models are indeed also well-ﬁtting.
8.5
CONTEMPORARY METHODS IN STATISTICAL ANALYSIS I:
GENERALIZED LINEAR MODELING AND MIXED-EFFECTS
MODELING
Two important contemporary methods in statistical analysis developed over the last
several decades enable natural resource scientists to more accurately and effectively
analyze their datasets. These methods model errors in data that are not necessarily
normally or independently distributed. Furthermore, they can be implemented
using either a frequentist or a Bayesian approach to analysis and interpretation.
254
SUMMARY AND CONCLUSIONS

Generalized linear modeling (GLM) models error structures that may not be nor-
mally distributed, such as with binomially distributed binary datasets, and Poisson or
negatively binomially distributed count datasets. The GLM models use link functions
to connect linear models to response data that may be restricted in value. With fre-
quentist analysis, maximum-likelihood (ML) estimation is used to estimate model
parameters for GLMs, in contrast to least-squares (LS) estimation, which is used to
estimate model parameters for multiple linear regression. Frequentist software such
as S-Plus or R can be used to analyze GLMs.
Bayesian statistical analysis can also be used to analyze linear regression models
using posterior distributions for the model parameters that are based on prior distri-
butions for the parameters and likelihood functions for the model data. Bayesian
statistical software such as WinBUGS can also be used to analyze GLMs.
Dependent data, sampled in groups, can be analyzed using random effects gener-
ated parsimoniously with a minimum number of parameters in mixed-effects model-
ing, with either frequentist software S-Plus or R or Bayesian statistical software
WinBUGS. Random effects can be estimated in mixed-effects modeling with var-
iance and covariance components accounting for the variation between and within
the groups in the data.
These breakthroughs in technology have provided the natural resource scientist
with the analysis tools needed to model nonnormally distributed dependent natural
resource data. Prior to these developments, natural resource scientists have had to
either avoid the collection of such datasets or ignore the problems associated with
its analysis. Now natural resource scientists can be encouraged to collect such data-
sets, with designs that reﬂect these natural structures in biological populations, since
appropriate techniques are available for its analysis.
8.6
CONTEMPORARY METHODS IN STATISTICAL ANALYSIS II:
BAYESIAN STATISTICAL ANALYSIS USING MCMC METHODS
WITH WinBUGS SOFTWARE
In 1763 the Reverend Thomas Bayes ﬁrst identiﬁed the basic concept of conditional
probability that is now summarized in a theorem named in his honor. Yet, more than
two centuries transpired before methods were fully developed to utilize the power of
his theorem in providing statistical analysis solutions for sample datasets. The general
application of Bayes’ theorem has been an exceedingly challenging problem, with
mathematical solutions for general statistical problems formulated using the
Bayesian method of parameter estimation proving to be limited. In the mid nineteenth
century, however, advances in computer technology created conditions conducive to
the development of algorithms that provided general Bayesian solutions using
MCMC simulation methods. Metropolis et al. (1953), Hastings (1970), and others
provided a collection of MCMC techniques for producing representative samples
from the posterior distribution of parameters for statistical models. More recently,
the development of WinBUGS software has provided a practical tool for the
natural resource scientist to access this methodology.
8.6
CONTEMPORARY METHODS IN STATISTICAL ANALYSIS II
255

Bayesian methodology turns frequentist statistical inference on its head. Instead of
providing frequentist probabilistic statements about data, conditional on assumed
parameter values, Bayesian statistical inference provides probability statements for
parameters, conditional on sample datasets. Bayesian statistical analysis incorporates
prior information into the process. It combines prior information about parameters
along with sample data and statistical models for the data to provide a posterior dis-
tribution for the parameters. Parameter estimation, hypothesis testing, and model
selection and inference can all be achieved using Bayesian statistical analysis and
inference. Bayesian statistical analysis provides a method of inference similar to
that of the scientiﬁc method; human inquiry can advance incrementally and cumulati-
vely as new data are collected revising existing knowledge represented by prior dis-
tributions with adjustments represented by posterior distributions. Bayesian statistical
analysis has come of age, and we can look forward to exciting new possibilities with
its application to problems in the natural resource sciences.
8.7
CONCLUDING REMARKS: EFFECTIVE USE OF STATISTICAL
ANALYSIS AND INFERENCE
This book has been written to provide contemporary Bayesian and frequentist statisti-
cal research methods for the appropriate use of statistics in natural resource science.
The author has appealed to scientists to bring increased attentiveness and rigor to stat-
istical design in the planning phase and to statistical analysis and interpretations in the
conclusions phase of natural resource data collection projects. Hypothesis testing
techniques using frequentist statistical inference should be used only in proper
context, for randomized, replicated, controlled experiments, challenging scientiﬁc,
non-“silly,” null hypotheses. The results of frequentist parameter estimation and
hypothesis testing should be properly interpreted, as probability statements for data-
sets, not as probability statements for parameters or hypotheses.
Probability statements for parameters, however, may be obtained by using
Bayesian statistical analysis and inference. Bayesian hypothesis testing uses Bayes
factors for analysis and may be integrated into a decision-theoretic context. Two
alternative strategies for model selection, an a posteriori descriptive strategy and an
a priori parsimonious predictive strategy that avoids compounded error and overﬁtting,
can be utilized to select best-ﬁtting models. Contemporary methods such as general-
ized linear modeling and mixed-effects modeling provide effective tools for modeling
dependence and nonnormality in datasets. The mission of this book has been to com-
municate these new ideas to you so that they can provide more appropriate and effec-
tive statistical solutions to many important contemporary natural resource problems.
8.8
SUMMARY
In this ﬁnal chapter, we have summarized the topics presented in this book, beginning
with the original three case studies in Chapter 1. We reviewed these problems and
256
SUMMARY AND CONCLUSIONS

summarized the solutions presented throughout the book. We next summarized the
appropriate use of statistics in six important areas in natural resource science with:
(1) sufﬁcient planning and speciﬁcation of statistical design prior to data collection;
(2) the proper use of hypothesis testing; (3) the appropriate analysis of observational
data; (4) adequate attention given to analysis, interpretation, and conclusions after
data collection; (5) the proper use of a posteriori model selection and inference
“data dredging” methods for comparison of statistical models; and (6) the appropriate
analysis of dependent and nonnormally distributed sample datasets. We presented
three important guidelines for sample surveys and experiments, addressing issues
of sample size and replication prior to data collection. We summarized the relative
merits and appropriate use of two strategies for model selection and inference, a pos-
teriori description and a priori predictive model selection and inference using AIC
and DIC. We summarized the key features of two important contemporary statistical
methods, generalized linear modeling and mixed-effects modeling, which are useful
for analyzing data with errors that are not normally or independently distributed. We
highlighted the alternative approach to statistical analysis using Bayesian statistical
analysis and inference. We concluded with ﬁnal remarks about the most effective
use of contemporary Bayesian and frequentist statistical research methods for the
natural resource scientist at the beginning of this twenty-ﬁrst century.
8.8
SUMMARY
257


APPENDIX A
Review of Linear Regression and
Multiple Linear Regression Analysis
In this appendix we review the concepts of linear regression and multiple linear
regression analysis.
A.1
INTRODUCTION
Let’s begin this review by discussing the linear regression model, based on a single
covariate, an independent “predictor” variable x, and a response, a dependent variable
y, with a linear relationship given by
yi ¼ b0 þ b1  xi þ ei,
where ei  iid N(0, s) are independent and identically distributed error (stochastic)
elements or “residuals,” normally distributed with mean 0 and standard deviation s.
Assume the x to be ﬁxed and measured without error. There are two parameters, b0,
b1, along with s, that can be estimated with estimates b0 ¼ bˆ 0 and b1 ¼ bˆ 1 along
with s ¼ sˆ in this linear regression model. The assumptions for the linear regression
model are as follows for the conditional population yjx ¼ fyj(x, y) is in the populationg:
1. myjx¼b0þb1 . x (linearity of the mean).
2. syjx¼s is constant (homoscedasticity).
3. yjx is normally distributed: yjx  N(myjx, s) (normality).
4. yjx are randomly sampled and independent, with x either randomly sampled or
ﬁxed, and measured without error.
The data consist of ordered pairs of x and y sample measurements (xi, yi),
i ¼ 1,2, . . . , n, from the larger population.
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
259

We can begin the analysis by examining the sample dataset graphically, looking at
an (x, y) scatterplot for the linearity of the data. The Pearson sample correlation
statistic
r ¼ ^r ¼
P
n
i¼1
(xi  x)  (yi  y)
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
(xi  x)2
s

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
(yi  y)2
s
,
which estimates the Pearson correlation parameter in the case of a population of
ﬁnite size N
r ¼
P
N
i¼1
(xi  mx)  (yi  my)
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
(xi  mx)2
s

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
N
i¼1
(yi  my)2
s
,
should ﬁrst be examined for statistical signiﬁcance before a linear regression model-
ing analysis is conducted. Correlation, varying between 21 and þ1, “measures”
whether there is a signiﬁcant linear relationship between the variables x and y.
Values near þ1 or 21 suggest a linear relationship with a positive or negative
slope, respectively, whereas values near 0 are indicative of no linear relationship.
The analyst should generally proceed with a linear regression modeling analysis
only if there is signiﬁcant nonzero correlation. The correlation can be tested for
signiﬁcance
H0 : r ¼ 0
by using the test statistic
ts ¼ r 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n  2
1  r2
r
:
This test statistic is tn22-distributed, if the null hypothesis is true.
The linear regression model with a single covariate x can be generalized to the
multiple linear regression model with p  1 “independent” “predictor” covariates
fx1, x2, . . . , xpg and dependent response y
yi ¼ b0 þ b1  xi1 þ b2  xi2 þ    þ b p  xip þ ei,
where ei  iid N(0, s) are independent and identically distributed error (stochastic)
elements or “residuals,” normally distributed with mean 0 and standard deviation s.
260
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

There are k ¼ pþ1 parameters, b0, b1, . . . , and bp, along with s, which can be
estimated with estimates b0 ¼ bˆ 0, b1 ¼ bˆ 1, . . . , and bˆ p, along with the residual stan-
dard error syjx ¼ sˆ, in this multiple linear regression model. All the linear regression
results described in this appendix will also generalize to the multiple linear regression
model (Seber 1977, Draper and Smith 1981, Manly 1994, Hocking 1996, Ryan 1997,
Cook and Weisberg 1999).
A.2
LEAST-SQUARES FIT: THE LINEAR REGRESSION MODEL
For linear regression analysis, a least-squares (LS) method is used to ﬁt the linear
regression model to the sample data, minimizing the goodness-of-ﬁt proﬁle and pro-
viding unbiased, minimum variance estimators for the parameters b0 and b1. In
Section 2.2, we examined other ways of ﬁtting models to data: maximum-likelihood
(ML) estimators that maximize the likelihood proﬁle and Bayesian methods. In this
chapter, with linear regression modeling, we focus on LS ﬁt.
The least-squares (LS) ﬁt of the linear regression model estimates the parameters
b0 and b1 by minimizing the sum of squared residuals, the goodness-of-ﬁt proﬁle
GOF ¼
X
n
i¼1
(yi  (b0 þ b1  xi))2 ¼
X
n
i¼1
e2
i
(Hilborn and Mangel 1997). Taking partial derivatives of the GOF proﬁle with
respect to the unknown parameters b0 and b1 and setting these partial derivatives
equal to 0, the solution for the parameter estimators is obtained with the normal
equations
^b1 ¼
P
n
i¼1
(xi  x)  (yi  y)
P
n
i¼1
(xi  x)  (xi  x)
¼ SPxy
SSx
^b0 ¼ y  ^b1  x:
The second equation can also be obtained from the fact that the mean average or
centroid (x¯, y¯) of the x and y sample values falls on the regression line. Estimators
for the standard errors of bˆ 1 and bˆ 0 are given by
se^b1 ¼ ^s 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
(n  1)  s2
x
s
,
A.2
LEAST-SQUARES FIT: THE LINEAR REGRESSION MODEL
261

se^b0 ¼ ^s 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n þ
x2
(n  1)  s2
x
,
s
where the estimator for the standard deviation of the residual error, the residual
standard error, sˆ ¼ syjx is
ˆs ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
GOF
n  2
r
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
(yi  (^b0 þ ^b1  xi))2
n  2
v
u
u
u
t
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
e2
i
n  2
v
u
u
u
t
¼ syjx:
The estimators for b0 and b1 are BLUE (the best linear unbiased estimators); that
is, they are unbiased estimators of minimum variance.
Similarly, for multiple linear regression analysis, a LS method is used to ﬁt the
multiple linear regression model to sample data, minimizing the goodness-of-ﬁt
proﬁle and providing unbiased, minimum variance estimators for the parameters,
b0, b1, . . . , bp.
A.3
LINEAR REGRESSION AND MULTIPLE LINEAR
REGRESSION STATISTICS
In this section we discuss eight leading statistics that form the basis for evaluating
linear and multiple linear regression models:
1. The coefﬁcient estimates and their signiﬁcance using either conﬁdence inter-
vals or t tests
2. The coefﬁcient of determination R2
3. The residual standard error syjx
4. The ANOVA F test
5. The adjusted R2
6. The Mallows Cp
7. The Akaike information criterion (AIC) and the corrected Akaike information
criterion (AICc)
8. The Bayesian information criterion (BIC)
A.3.1
Estimates of Coefﬁcients and Their Signiﬁcance: Conﬁdence
Intervals and t Tests
The estimates for slope (bˆ 1) and y intercept (bˆ 0) for the linear regression model can
be tested for statistical signiﬁcance at the P ¼ 12a level of conﬁdence with the null
262
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

hypotheses respectively
H0 : b0 ¼ 0
H0 : b1 ¼ 0
in two equivalent ways (where the degrees of freedom df are given by df ¼ n 2 2):
1. Determine whether 0 is outside the conﬁdence intervals:
^b0 + tdf,1a=2  se^b0
^b1 + tdf,1a=2  se ^b1
2. Examine the signiﬁcance of the t statistics:
ts ¼
^b0
se^b0
¼
^b1
se^b1
Reject H0 if and only if the conﬁdence interval does not contain 0. This occurs if and
only if the test statistic falls within the rejection region, or equivalently if the p value
is less than or equal to the type I error a. Otherwise, do not reject H0.
It is important to determine whether the estimates for b0 and b1 are statistical sig-
niﬁcant. If the estimate for b1 is not statistically signiﬁcant, then b1 is statistically
equivalent to 0 and the linear term for x should be eliminated, reducing the model
to the null model:
yi ¼ b0 þ ei:
If the estimate for b0 is not statistically signiﬁcant, then b0 is statistically equivalent to
0 and the constant term should be eliminated from the model and the ratio model
used instead if b1 is statistically signiﬁcant:
yi ¼ b1  xi þ ei:
A.3.2
The Coefﬁcient of Determination R2
The coefﬁcient of determination R2 is another important statistic used to evaluate
the linear model with the LS ﬁt. It varies between 0 and 1, with a higher R2 value
indicating a linear relationship. For the linear regression with one independent
A.3
LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION STATISTICS
263

variable, R ¼ r, the Pearson correlation coefﬁcient, whereas with multiple linear
regression with more than one independent covariate, R generalizes the Pearson cor-
relation coefﬁcient to more than one dimension. The R2 index “measures” the pro-
portion of the variation of y “captured” by the regression model
R2 ¼ SSregression
SStotal
¼
P
i (^yi  y)2
P
i ( yi  y)2
¼ 1  SSerror
SStotal
¼ 1 
P
i ( yi  ^yi)2
P
i ( yi  y)2
where the three components of variation are given by
SSregression ¼
X
i
(^yi  y)2,
SSerror ¼
X
i
( yi  ^yi)2,
SStotal ¼ SSy ¼
X
i
( yi  y)2,
with
SStotal ¼ SSy ¼ SSregression þ SSerror
and yˆi ¼ bˆ 0 þ bˆ 1 . xi.
Figure A.1 displays scatterplots of (x,y) samples from populations satisfying the
assumptions of linear models, with four prototype cases of R2 values exhibited
with extremes of slope and error. The highest R2 values occur with datasets having
maximal slope and minimal error. The lowest R2 values occur with datasets
having minimal slope and maximal error. Moderate R2 values occur with datasets
having moderate slope and moderate error. The R2 index effectively “measures”
both steepness and slope.
However, R2 alone does not provide a deﬁnitive indicator of ﬁt, despite
common misperception. Datasets may have a high R2, with a large amount of
steepness and small amount of error, yet be curvilinear, rather than linear, in
shape. Additionally, most natural resource datasets will contain an appreciable
amount of stochastic error that cannot readily be explained by identiﬁable and
measurable independent covariates, so there is a limit to the amount of error
that a model should minimize. Yet a linear regression model may still prove
useful and provide a reasonable ﬁt, with a signiﬁcant amount of error estimated
by syjx. A natural resource relationship may be gradual, with moderate slope,
and with a moderate amount of R2 value, say, between 20% and 60%, yet still
be statistically and biologically signiﬁcant.
Models with high R2 values may overﬁt sample datasets and not serve well as
predictive models (Burnham and Anderson 1998). As the number of independent
covariates in a model increases, R2 values also increase, since the model will have
264
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

Figure A.1. Scatterplot of (x, y) sample datasets from populations satisfying the assumptions
of linear regression, with extremes of slope and error, and their effects on the estimated coefﬁ-
cients of determination R2. (a) High slope, low error: x ,- runif(80,0,100), y1 ,-
30þ5.0xþrnorm(80,0,20),
R2 ¼ 0.992.
(b)
Low
slope,
low
error:
x
,-
runif(80,0,100), y2 ,- 30þ1.2xþrnorm(80,0,20), R2 ¼ 0.984. (c) high
slope, high error: x ,- runif(80,0,100), y3 ,- 30þ5.0xþrnorm(80,0,40),
R2 ¼ 0.812. (d) Low slope, high error: x ,- runif(80,0,100), y4 ,- 30þ1.2xþ
rnorm(80,0,40), R2 ¼ 0.429. (e) Comparison of all prototype cases (a)–(d) above.
A.3
LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION STATISTICS
265

reduced error in ﬁtting sample datasets. However, a model that overﬁts a sample
dataset may not necessarily ﬁt the population data very well. A model with
n21 covariates can in some cases exactly ﬁt a dataset of sample size n. If the
model more closely ﬁts a sample dataset with additional independent covariates,
the error, or bias, will be less and the R2 value may be higher. However, the pre-
cision of the estimates of the parameters in the model may be reduced because of
more parameter coefﬁcients to estimate with the same size sample dataset. The pre-
cision of the estimates may also be reduced if the covariates are correlated with
each other, thereby reducing the “stability” and increasing the error of the
Figure A.1. Continued.
266
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

estimates. So, although a higher R2 value may in general be a worthwhile indicator
of a better-ﬁtting model, there is a danger of overﬁtting models using R2 as a sole
criterion for model ﬁtting.
In conclusion, the analyst should examine the R2 statistic but use it judiciously
along with other statistics to evaluate the ﬁt of models. Biological models with a sig-
niﬁcant amount of stochasticity may have low R2 values, yet be well- or best-ﬁtting,
and, conversely, models with high R2 statistics may overﬁt sample datasets and be
overparameterized. Models with best-ﬁtting relationships to sample datasets are not
exclusively characterized by high R2 values.
A.3.3
The Residual Standard Error syjx
Another important statistic useful for evaluating the ﬁt of models with the linear
regression and multiple linear regression models is the residual standard
error syjx ¼ sˆ is the approximately unbiased estimator of the standard deviation
parameter s for the error given by
syjx ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
(yi  ^yi)2
(n  p  1)
v
u
u
u
t
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
(yi  (^b0 þ P
p
j¼1
^bj  xj))2
(n  p  1)
v
u
u
u
t
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
n
i¼1
e2
i
(n  p  1)
v
u
u
u
t
,
where n is the sample size and p is the number of covariates in the model. Its square
syjx
2 is the unbiased estimator of the variance s2. In the case of linear regression, p ¼ 1
and the parameters are b0, b1, and s. Since syjx estimates the standard deviation of the
error for the model, better-ﬁtting models will tend to have lower residual standard
errors. However, analogous to increases in R2, this is true only up to a point. As
with increases in R2, syjx tends to decrease with increasing numbers of covariates
that reduce the amount of error in the ﬁt of the model with the sample dataset.
However, again, there is a danger of overﬁtting that must be considered in evaluating
the ﬁt of the model. So look for models that decrease syjx but also examine other statis-
tics such as the adjusted R2, the Mallows Cp, and AIC, which penalize models with
too many “independent” covariates for overﬁtting sample datasets.
A.3.4
The F Test
As with experimental data and analysis of variance (ANOVA), an F test can be
conducted for linear regression and multiple linear regression analysis, calculating
A.3
LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION STATISTICS
267

the components of variance for the regression model, the regression (regr), error
(resid), and total components, in the following table
Components of Variance
df
SS
MS
F
p Value
regr
dfregr
SSregr
MSregr
Fs
—
resid
dfresid
SSresid
MSresid
Total
dftotal
SStotal
The degrees of freedom (df) formulas for the linear regression model are given by
dfregr ¼ 1,
dfresid ¼ n  2,
dftotal ¼ n  1,
where n is the sample size. Note that
dftotal ¼ dfregr þ dfresid:
For multiple regression with p independent variables, the degrees of freedom
formulas are given by
dfregr ¼ p,
dfresid ¼ n  p  1,
dftotal ¼ n  1:
The sums of squares (SS) formulas are given by
SSregr ¼
X
n
i¼1
(^yi  y)2,
SSresid ¼
X
n
i¼1
( yi  ^yi)2,
SStotal ¼
X
n
i¼1
( yi  y)2:
As usual
SStotal ¼ SSregr þ SSresid:
The mean-squares (MS) formulas are
MSregr ¼ SSregr=dfregr,
MSresid ¼ SSresid=dfresid,
268
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

and the Fs-test statistic is
Fs ¼ MSregr=MSresid:
The null and alternative hypotheses are given by
H0: yi ¼ b0 þ ei
(null model); i.e., b1 ¼ 0,
HA: yi ¼ b0 þ b1  xi þ ei
(linear regression model)
(i.e., b1=0). The F test may be signiﬁcant for covariates even without good model ﬁt
since it assesses whether a model with covariates ﬁts the data better than does the null
model without covariates, so other statistics should also be examined. If the F-test
results are insigniﬁcant; however, that is a good indication that the model with the
covariate is not helpful.
A.3.5
Adjusted R2
If R2 is not always a reliable statistic for evaluating the competitive ﬁt of models
because of possible overﬁtting, what statistics will serve in its place? The adjusted
R2 statistic compensates for the problem of overﬁtting by extracting a “penalty”
for the use of too many covariates. The adjusted R2 statistic is given by the formula
Adjusted R2 ¼ R2
adj ¼ 1  n  i
n  k  (1  R2),
where n ¼ sample size, i ¼ 1 if there is intercept and 0 otherwise, and k ¼ the number
of parameters. Models with the highest adjusted R2 are the best-ﬁtting models. It is clear
from the formula that increasing numbers of covariates negatively affect the magnitude
of R2
adj. You can avoid the problem of models overﬁtting sample datasets by examining
the adjusted R2 statistic rather than the R2 statistic. However, the adjusted R2 statistic
still may tend to overﬁt models to sample datasets, favoring models with too many vari-
ables. In Sections A.3.6–A.3.8, we will examine other statistics, Mallows’ Cp, AIC,
AICc, and BIC that most effectively address the problem of overﬁtting.
A.3.6
Mallows’ Cp
Another statistic designed to assess LS ﬁt for models with normal residuals having
constant variance is the Mallows Cp statistic
C p ¼ p þ (n  p)  (^s2  ^s2
full)
ˆs2
,
where p is the number of covariates in the model, the full model is the model with all
the explanatory covariates under consideration, and
ˆs2 ¼
P
n
i¼1
(yi  ^yi)2
n
¼ n  p  1
n
 s2
yjx 
A.3
LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION STATISTICS
269

Models with a lower Mallows Cp value are better-ﬁtting, or more accurately, models
with Cp ﬃp are the best-ﬁtting models. The Mallows Cp provides approximately the
same ranking for models as AIC and AICc (see Section A3.7). If the full model is not
overﬁtting, then presumably models with Cp approximately equal to p will also not be
overﬁtting. Models with Cp below p will have underestimated the error and be over-
ﬁtting. Because of these rather questionable assumptions, we recommend the use of
AIC and AICc (below) as the most rigorous and theoretically justiﬁed approach to
model ﬁtting of natural resource datasets.
A.3.7
Akaike’s Information Criterion: AIC and AICc
We now describe the important information-theoretic statistic that is most effective
for evaluating the competitiveness of models at ﬁtting natural resource data.
Akaike’s information criterion was developed in the early 1970s by the Japanese
mathematician Hirotugu Akaike (1973). It is an information-theoretic measurement
of the Kullback–Liebler distance between a model and reality. Akaike’s infor-
mation criterion (AIC) and the corrected Akaike information criterion (AICc)
for multiple linear regression are given by the formulas
AIC ¼ n  log s2
yjx  n  p  1
n


þ 2  k
¼ n  log s2
yjx  n  k þ 1
n


þ 2  k,
AICc ¼ n  log s2
yjx  n  p  1
n


þ 2  k þ 2  k  (k þ 1)
(n  k  1)
¼ n  log s2
yjx  n  k þ 1
n


þ 2  k þ 2  k  (k þ 1)
(n  k  1) ,
where p is the number of covariates and k ¼ p þ 2 is the number of parameters
(including the intercept and s). For the linear regression model with the parameters
b0, b1, and s, p ¼ 1 and k ¼ 3. AIC is the linear Taylor series approximation of the
Kullback–Leibler distance, whereas AICc is a second-order Taylor series approxi-
mation. Since AICc is more precise, it should always be used in preference to AIC,
particularly for datasets with small sample size. The best-ﬁtting model has the
lowest AICc value.
The AICc criterion can be used to determine the most parsimonious model, the one
with the most optimal combination of minimal bias and maximal precision. It pena-
lizes a model with too many covariates from overﬁtting sample data. As we have
emphasized, other traditionally popular regression statistics, such as R2 and syjx,
tend to favor models that overﬁt sample datasets, whereas AICc prevents such over-
ﬁtting from occurring. For sound predictive models, the objective is to develop good
models from sample datasets that provide reliable predictions for populations. The
270
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

objective is not merely to describe sample datasets. As the number of covariates
increases, models tend to more closely ﬁt sample datasets and reduce the bias.
However, as the number of covariates increases and the number of samples in the
dataset remains ﬁxed, the precision of the covariate coefﬁcient parameter estimates
tends to decrease. The AICc criterion moderates this process, producing an optimal
compromise between reduced bias and maximal precision. It determines the most par-
simonious model, the one with the combined least amount of bias, closest to the
sample dataset, and most amount of precision, with the lowest amount of sampling
error, for the coefﬁcient estimates relative to the reduced bias.
The AICc criterion measures the relative amount of noise, or entropy, in the
sample data, separating it from the signal or information. It is a relative measure,
since the reality is unknown; the absolute measure of entropy is the calculated
AICc plus a constant. The constant remains unknown, but since each model has the
same constant, AICcs may be compared to determine the best ﬁtting models. As AICc
provides comparative measures of ﬁt between models only, goodness-of-ﬁt tests must
also be used in analysis to assess how well-ﬁtting are the best-ﬁtting models.
In general, for any probabilistic statistical model for a sample dataset with a like-
lihood function L (see Chapters 2–4, and 6, and 7), AIC and AICc are deﬁned using
the deviance¼D¼22 . log(L):
AIC ¼ D þ 2  k
¼ 2  log (L) þ 2  k,
AICc ¼ D þ 2  k þ 2  k  (k þ 1)
n  k  1
¼ 2  log (L) þ 2  k þ 2  k  (k þ 1)
n  k  1 :
A.3.8
Bayesian Information Criterion (BIC)
The AICc criterion should be applied to models of realities that are complex and inﬁ-
nite- or high-dimensional as are most biological populations (Burnham and Anderson
1998). For such complex realities, ﬁnite-dimensional models will necessarily be inac-
curate and, at best, approximations. For realities that are ﬁnite-dimensional, of fairly
low dimension such as k ¼ 1–5, with k ﬁxed as the sample size n increases, so-called
dimension-consistent criteria such as the Bayesian information criterion (BIC) should
be applied.
The Bayesian information criterion, developed by Schwarz (1978), also uses a
formula based on the deviance or log likelihood and “penalizes” models for the
overuse of covariates
BIC ¼ D þ k  log (n)
¼ 2  log (L) þ k  log (n):
A.3
LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION STATISTICS
271

For multiple linear regression models, BIC is given by
BIC ¼ n  log s2
yjx  n  p  1
n


þ k  log (n)
¼ n  log s2
yjx  n  k þ 1
n


þ k  log (n)
and is derived using Bayesian assumptions of equal priors on each model and vague
priors on the parameters (Burnham and Anderson 1998), with the objective of pre-
dicting rather than understanding the process of a system. It penalizes more heavily
for increases in the number of parameters and hence sometimes tends to select
models that are underﬁt with excessive bias. For natural resource modeling, most rea-
lities are likely complex and inﬁnite-dimensional; hence, AICc is a more appropriate
criteria for comparing statistical models.
A.4
STEPWISE MULTIPLE LINEAR REGRESSION METHODS
In this section we will brieﬂy describe methods for model selection that are applicable
to the multiple linear regression case where there are p covariates fx1, x2, . . . , xpg
along with the response y with n samples. We shall discuss two exploratory, descrip-
tive, “data dredging” methods for model selection, ways of selecting from among
collections of models consisting of linear combinations of covariates. For example,
suppose that we are interested in developing a habitat selection model (Manly
et al. 1995, 2004) using a collection of habitat covariates as predictor variables and
an animal abundance or presence–absence response variable. We could use any of
the following criteria as a basis for model selection:
1. Most signiﬁcant coefﬁcient estimate (i.e., lowest p value)
2. Lowest residual standard error syjx,
3. Highest coefﬁcient of determination R2
4. Highest adjusted R2
5. Lowest Mallows Cp, or one closest to p
6. Lowest AIC
7. Lowest BIC
With stepwise selection multiple linear regression, we can choose a “best”
model according to the covariate coefﬁcient estimate p values, using the following
iterative procedure:
1. Choose type I error bounds ae for the entering covariate coefﬁcients and as for
the staying covariate coefﬁcients with ae,as.
2. Start with the null model with no covariates yi ¼ b0 þ ei.
272
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

3. Consider all single-covariate models ﬁtted to the sample dataset and select the
covariate, say, xc 1, with coefﬁcient estimate ^bc1 having the lowest p value below
ae, obtaining the single covariate model yi ¼ b0 þ bc1  xc1, i þ ei.
4. Consider the addition of another covariate, xc jþ1, to the currently selected
covariate model with xc1, xc2, . . . , xc j ﬁtted to the sample dataset and add the
covariate, say, xc jþ1, with coefﬁcient estimate ^bc jþ1 having the lowest p value
below
ae,
if
there
is
one,
obtaining
the
(kþ1)
covariate
model
yi ¼ b0 þ bc1  xc1, i þ þ bc2  xc2,i þ    þ bc jþ1  xc jþ1, i þ ei; or otherwise stop.
5. Consider the p values of the current model covariate coefﬁcient estimates and
drop the covariates with p values above as, if there are any.
6. Continue iterating steps 4 and 5, adding and dropping covariates as appropriate
until the process stops.
Note that it is important that ae , as, or otherwise the process could continue inde-
ﬁnitely, with covariates added and dropped repeatedly.
Forward selection multiple linear regression adds covariates, without dropping
any, until the process stops, whereas backward elimination multiple linear
regression begins with the full model consisting of all covariates and drops covari-
ates with the highest p values, without adding any, until the process stops. Stepwise
selection multiple linear regression combines both forward selection and backward
elimination multiple linear regression. As mentioned earlier, other optimization cri-
teria besides p values for the coefﬁcient estimates could be used for the covariate
selection, such as lowest residual standard error syjx, highest R2, highest adjusted
R2, lowest Mallows Cp, lowest AIC, or lowest BIC.
Stepwise selection, forward selection, and backward elimination multiple linear
regression methods provide convenient methods for choosing models with multi-
variate sample datasets. However, these methods tend to overﬁt sample data
because of the compounding of type I error caused by the multiple testing of hypoth-
eses in the selection criteria. Therefore, these “data dredging” methods should be
viewed as exploratory and descriptive. Results should be interpreted tentatively.
These methods are most suitable for formulating model hypotheses that can be
tested with additional sample datasets. Only with sufﬁcient goodness-of-ﬁt testing
of the inferences, preferably with additional sample datasets, should the results of
these methods be used for prediction.
A.5
BEST-SUBSETS SELECTION MULTIPLE LINEAR REGRESSION
Best-subsets selection multiple linear regression selects best-ﬁtting models from
the collection of all models that are linear combinations of covariates using some
of the following criteria:
1. Highest R2
2. Highest adjusted R2
A.5
BEST-SUBSETS SELECTION MULTIPLE LINEAR REGRESSION
273

3. Lowest Mallows Cp, or Cp closest to p
4. Lowest AIC
5. Lowest BIC
As with stepwise selection multiple linear regression, best-subsets selection multiple
regression is prone to type I error and tends to overﬁt sample data. As such, results
should be viewed as tentative, subject to further study with additional sample data-
sets, if inferences are to be used for prediction.
A.6
GOODNESS OF FIT
The discussion so far has focused on the interpretation of statistics evaluating the sig-
niﬁcance and competitiveness of models at ﬁtting sample data. The signiﬁcance of
the model coefﬁcient estimates tests the null hypothesis that the coefﬁcients are
equal to 0. The signiﬁcance of the F test addresses the hypothesis of whether the
model differs from the null model yi ¼ b0 þ ei. Other statistics, such as R2, syjx,
the adjusted R2, Mallows Cp, and, most importantly, AICc or BIC, evaluate the ﬁt
of the model and serve as comparative statistics useful in evaluating the relative com-
petitiveness of models at differing from the null model and ﬁtting the sample dataset.
None of these tests or statistics, however, serve adequately in evaluating model good-
ness of ﬁt to the reality represented by the sample dataset.
So it is very important to examine goodness of ﬁt of the best-ﬁtting models as part of
the overall analysis process. In the following sections, wewill brieﬂy address this issue.
A.6.1
Residual Analysis
An important part of goodness-of-ﬁt analysis is always to examine the residuals of a
linear regression or multiple linear regression model
^ei ¼ yi  ^yi,
where ^yi ¼ ˆb0 þ P p
j¼1 ˆb j  x j, to determine whether the assumptions of the model
might be violated. Recall that the linear regression and multiple linear regression
models are based on the following assumptions for the population residuals ei:
1. Independence
2. Normality
3. Homoscedasticity (s2 constant)
The residuals eˆi are estimates of the population residuals ei and should be examined
for apparent violations of these three assumptions. Residual plots that graph the
residuals as a function of model ﬁt are readily available in most statistical software.
They should be examined for any apparent dependences among the residuals, lack
of normality, or heteroscedasticity.
274
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

The residuals should occur “randomly,” both positively and negatively above and
below the model ﬁt axis, without any apparent dependence relationships among them.
They should also be normally distributed, with a standard deviation estimated by syjx.
To check for normalcy, a normal plot of the residuals may be examined for linearity
and a test for normality such as the Anderson–Darling test may be applied. The
homoscedasticity assumption of constant variance of the residuals should be evalu-
ated visually in the graph of the residuals, and tested, with Bartlett’s test or other
tests for homoscedasticity such as Levene’s test or the F test. Outliers should be scru-
tinized for their size and quantity. If the conﬁdence level is 95%, the proportion of
outliers beyond the 95% conﬁdence band should not greatly exceed the expected
5%. With small sample sizes, residual analysis can be more of an art than a
science in practical application. So a biological explanation may provide the best
rationale for the validity of the assumptions. Interested readers may consult Cook
(1998) and Cook and Weisberg (1999) to examine this topic of residual analysis in
more detail.
A.6.2
Conﬁdence Intervals
Conﬁdence intervals for the predicted mean yˆjx ¼ bˆ 1 þ bˆ 1.x may be calculated
using the standard error for the mean given by
se^yjx ¼ syjx 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n þ
(x  x)2
(n  1)  s2
x
s
,
where syjx is the standard error, x¯ is the estimated mean of the fxig, s2
x is the estimated
variance of the fxig, and n is the sample size. Conﬁdence intervals CIyˆjx may then be
calculated and graphed using the formula
CI^yjx ¼ ^yjx + td f ,1a=2  se^yjx,
where the degrees of freedom df ¼ n2k with k parameters in the model and
P ¼ 12a is the conﬁdence level. The frequentist interpretation of this conﬁdence
interval with conﬁdence level P is that, with repeated sampling, P of the sample
conﬁdence intervals will on average contain the population mean mjx at each x.
A.6.3
Prediction Intervals
Prediction intervals for the predicted yjx may also be calculated and graphed. The
expected proportions of the (xi,yi) points in the developmental and test datasets
should fall within these prediction intervals, for example, 95% of the points within
the 95% prediction intervals. Prediction intervals can be calculated using the
A.6
GOODNESS OF FIT
275

formula for the standard error for the predicted yjx given by
seyjx ¼ syjx 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 1
n þ
(x  x)2
(n  1)  s2
x
s
,
where syjx is the standard error, x¯ is the sample mean of the fxig, sx
2 is the variance of
the fxig, and n is the sample size. Note that there are three components of error in this
standard error formula, indicated by the three terms in the square root, the ﬁrst (i.e., 1)
due to variation of the y for a ﬁxed x, and the latter two [1/n and (x 2 x¯)2/(n 2 1) .sx
2]
due to variation of the estimates for the linear regression. Using the degrees of
freedom, df ¼ n 2 k with k parameters in the model, prediction intervals PIyjx with
P ¼ 1 2 a level of conﬁdence can be calculated:
PIyjx ¼ ^yjx + tdf;1a=2  seyjx:
A.6.4
Cross-Validation and Testing Techniques
For a linear or multiple linear regression model to provide a predictive tool, additional
conﬁrmation of the reliability of the model may be examined using either cross-
validation with the developmental dataset or testing techniques with additional test
datasets. If additional test datasets are available, randomly sampled from the popu-
lation, goodness of ﬁt can be examined by evaluating the predictive performance
of the prediction intervals on the test datasets. For example, 95% prediction intervals
can be examined on the test datasets to determine whether they perform as expected,
with approximately 95% of the samples in the test dataset within the prediction inter-
val. Alternatively, if additional test datasets are unavailable, the next best alternative
is to perform cross-validation analysis on the developmental dataset. The most
common method of cross-validation consists of successively omitting individual
points, ﬁtting the model to the remaining sample dataset, and examining the
deleted point with respect to the prediction interval. The overall predictive accuracy
of the deleted points falling within the prediction intervals is an indication of the
predictive capabilities of the ﬁtted model.
276
REVIEW OF LINEAR REGRESSION AND MULTIPLE LINEAR REGRESSION

APPENDIX B
Answers to Problems
Problem 1.1
> d
[1] 29.6 22.7 28.6 23.1 27.3 28.3 23.7 28.4 25.0 24.5 27.7 22.7
[13] 27.6 22.7 27.0 21.2 26.8 28.4 21.2 23.6 24.6 24.0 22.0 28.2
[25] 27.0 22.8 28.4 27.7 31.9 18.9
> dbar <- mean(d)
> s <- stdev(d)
> n <- length(d)
> se <- s/sqrt(n)
> #
95% conﬁdence
> t <- qt(.975,n-1)
> E <- t*se
> left.lim <- dbar-E
> right.lim <- dbar+E
> c(dbar,s,n,se,t,E,left.lim,right.lim)
[1] 25.520000
3.061260 30.000000 0.558907 2.045230 1.143093
[7] 24.376907 26.663093
> #
80% conﬁdence
> t <- qt(.90,n-1)
> E <- t*se
> left.lim <- dbar-E
> right.lim <- dbar+E
> c(dbar,s,n,se,t,E,left.lim,right.lim)
[1] 25.5200000 3.0612596 30.0000000 0.5589070 1.3114336
[6] 0.7329694 24.7870306 26.2529694
> #
67% conﬁdence
> t <- qt(.835,n-1)
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
277

> E <- t*se
> left.lim <- dbar-E
> right.lim <- dbar+E
> c(dbar,s,n,se,t,E,left.lim,right.lim)
[1] 25.5200000 3.0612596 30.0000000 0.5589070 0.9907557
[6] 0.5537403 24.9662597 26.0737403
> #
50% conﬁdence
> t <- qt(.75,n-1)
> E <- t*se
> left.lim <- dbar-E
> right.lim <- dbar+E
> c(dbar,s,n,se,t,E,left.lim,right.lim)
[1] 25.5200000 3.0612596 30.0000000 0.5589070 0.6830439
[6] 0.3817580 25.1382420 25.9017580
> t.test(d,mu=26,conf.level=.95)
One-sample t-Test
data: d
t = -0.8588, df = 29, p-value = 0.3975
alternative hypothesis: true mean is not equal to 26
95 percent confidence interval:
24.37691 26.66309
sample estimates:
mean of x
25.52
> t.test(d,mu=26,conf.level=.80)
One-sample t-Test
data: d
t = -0.8588, df = 29, p-value = 0.3975
alternative hypothesis: true mean is not equal to 26
80 percent confidence interval:
24.78703 26.25297
sample estimates:
mean of x
25.52
> t.test(d,mu=26,conf.level=.67)
One-sample t-Test
278
ANSWERS TO PROBLEMS

data: d
t = -0.8588, df = 29, p-value = 0.3975
alternative hypothesis: true mean is not equal to 26
67 percent confidence interval:
24.96626 26.07374
sample estimates:
mean of x
25.52
> t.test(d,mu=26,conf.level=.50)
One-sample t-Test
data: d
t = -0.8588, df = 29, p-value = 0.3975
alternative hypothesis: true mean is not equal to 26
50 percent confidence interval:
25.13824 25.90176
sample estimates:
mean of x
25.52
. # With a conﬁdence of 95%, 80%, and 67%, the estimate is below the threshold
but the CI overlaps the threshold, creating a conﬂict between the decisionmaking
options. However, if the conﬁdence is lowered to 50%, the estimate and the CI are
below the threshold, creating no conﬂict between the decisionmaking options. This
is caused by the lowering of the level of conﬁdence in the decisionmaking. The con-
ﬁdence levels of 95% and 80% are commonly used in natural resource science. A
conﬁdence level of 67% provides a t value approximately equal to 1. A conﬁdence
level of 50% provides a decision that is correct half of the time.
. # The histogram of {di} does not suggest that there are any apparent problems with
normality of the population based the sample data.
ANSWERS TO PROBLEMS
279

Problem 1.2
> phat <- 34/60
> phat
[1] 0.5666667
> se <- sqrt(phat*(1-phat)/59)
> se
[1] 0.06451324
> t <- qt(.975,59)
> t
[1] 2.000995
> E <- t*se
> E
[1] 0.1290907
> phat+E
[1] 0.6957574
> t <- qt(.90,59)
> t
[1] 1.296066
> E <- t*se
> phat+E
[1] 0.6502801
> t <- qt(.75,59)
> E <- t*se
> phat+E
[1] 0.6104499
. # With 95% and 80% conﬁdence, the estimate is below the threshold but the CI
overlaps the threshold, creating a conﬂict between the decisionmaking options.
However, with the lower 50% conﬁdence, both the estimate and the CI are below
the threshold with no conﬂict created between the decisionmaking options. This
disparity is caused by the reduced level of conﬁdence in the decisionmaking.
Problem 1.3
> t.test(d,mu=26,alternative=‘‘less’’)
One-sample t-Test
data: d
t = -0.8588, df = 29, p-value = 0.1987
alternative hypothesis: true mean is less than 26
95 percent confidence interval:
NA 26.46965
280
ANSWERS TO PROBLEMS

sample estimates:
mean of x
25.52
. # For conﬁdence levels of 95% and 80% we fail to reject the null hypothesis that
the mean is at or above the threshold. However, for conﬁdence levels of 67% and
50%, we reject the null hypothesis that the mean is at or above the threshold.
These decision results differ from those based on the CIs in Problem 1.1 because
this test is one-tailed whereas the CIs are two-tailed.
Problem 1.4
> 1-pnorm(26.0,25.5,3.1)
[1] 0.4359324
. There is a risk of 0.4359324 that a decision that the threshold has been exceeded
will be incorrect.
Problem 1.5
> counts
[1] 2 1 0 0 2 0 2 1 2 1 1 0 5 1 0 1 4 1 0 4 0 1 2 2 0 4
[27] 2 2 1 0 2 0 3 0 1 0 1 2 1 1 2 3 0 0 2 2 1 1 1 2
> chat <- mean(counts)
> chat
[1] 1.34
> s <- stdev(counts)
> s
[1] 1.22241
> n <- length(counts)
> n
[1] 50
> se <- s/sqrt(n)
> se
[1] 0.1728749
> t <- qt(.975,n-1)
> t
[1] 2.009575
> E <- t*se
> chat-E
[1] 0.992595
> chat+E
[1] 1.687405
. # The 95% conﬁdence interval is [0.992595, 1.687405].
ANSWERS TO PROBLEMS
281

. # The data are quite skewed and nonnormal, so the analysis results may be
inaccurate.
Problem 1.6
> data
x
y
1
14.9
51.9
2
35.4
72.3
3
17.4
68.4
4
30.0
62.3
5
83.6
141.8
6
38.2
82.7
7
65.2
89.4
8
25.4
69.6
9
5.9
28.3
10
51.5
94.2
11
31.6
54.9
12
13.1
45.9
13
31.4
72.7
14
98.9
156.0
15
30.4
52.5
16
38.2
77.4
17
94.6
151.6
18
21.8
67.8
19
17.9
48.8
20
45.4
58.3
21
76.5
121.0
282
ANSWERS TO PROBLEMS

22
80.2
129.2
23
10.6
37.9
24
98.5
147.3
25
66.1
131.1
26
1.8
40.6
27
29.0
59.1
28
98.4
131.6
29
99.4
168.1
30
59.9
108.6
31
46.1
64.9
32
48.3
99.2
33
58.8
93.6
34
10.9
48.5
35
1.1
34.7
36
90.7
146.4
37
28.0
56.0
38
89.1
121.7
39
32.1
92.9
40
77.1
111.8
41
99.2
149.1
42
14.4
60.8
43
67.1
90.2
44
19.2
52.7
45
99.1
140.7
46
38.1
78.9
47
4.3
25.8
48
31.1
75.5
49
94.3
146.1
50
8.7
47.9
> model <- lm(y~x,data)
> summary(model)
Call: lm(formula = y ~ x, data = data)
Residuals:
Min
1Q Median
3Q
Max
-26.55 -7.145
1.471 7.312 23.66
Coefficients:
Value Std. Error t value Pr(>|t|)
(Intercept) 31.5491
2.8722
10.9845
0.0000
x
1.1741
0.0502
23.3686
0.0000
Residual standard error: 11.37 on 48 degrees of freedom
ANSWERS TO PROBLEMS
283

Multiple R-Squared: 0.9192
F-statistic: 546.1 on 1 and 48 degrees of freedom, the p
-value is 0
Correlation of Coefficients:
(Intercept)
x -0.8288
> AIC(model)
[1] 388.9069
. # The estimates for b0 and b1 are both statistically signiﬁcant at the 95% level of
conﬁdence, since their CIs do not include 0 and their p values are both less than 5%.
. # The scatterplot suggests that the linear regression model provides a good ﬁt.
Problem 2.1
> p <- .50
> 1-(1-p)
^2
[1] 0.75
> 1-(1-p)^3
[1] 0.875
> 1-(1-p)^4
[1] 0.9375
> 1-(1-p)^5
[1] 0.96875
> 1-(1-p)^6
[1] 0.984375
> (1-p)^5*p/(1-p)^5
[1] 0.5
284
ANSWERS TO PROBLEMS

1. m ¼ 5 with P ¼ 0.96875.
2. m þ 1 ¼ 6 with P ¼ 0.984375, so probability is increased by 0.015625.
3. Bayes
probability ¼ P(a > b)/P(b) ¼ 0.03125*0.5/0.03125 ¼ 0.5,
where
b ¼ m þ 1 visits with no detections in ﬁrst m visits and a ¼ m þ 1 visits
with no detections in ﬁrst m visits and a detection in the (m þ 1)st visit.
This conditional probability equals the unconditional probability because the
visit events are independent.
Problem 2.2
The probability of choosing the correct occupied site for each of the three strategies
all depend on your initial choice, with three possible ordered combinations of nests
(O ¼ occupied, N ¼ nonoccupied), ONN, NON, and NNO, each with probability 1
3 of
occurring (ﬁrst position – your initial choice, second or third position can be ident-
iﬁed by the wildlife ofﬁcer):
1. ONN—win, NON—lose, NNO—lose: probability ¼ 1
3
2. ONN—lose, NON—win, NNO—win: probability ¼ 2
3
3. ONN—win, NON—lose, NNO—lose: probability ¼ 1
3.
Conclusion: the second option has the unique maximum probability of correctly
choosing the occupied site.
Problem 2.3
. # (i) Posteriors are BE(32, 70), BE(31.5, 69.5), BE(33, 71)
. # means:
> 32/102
[1] 0.3137255
> 31.5/101
[1] 0.3118812
> 33/104
[1] 0.3173077
> (.3173077-.3118812)/.3118812
[1] 0.01739925
. # medians:
> qbeta(.5,32,70)
[1] 0.3125037
> qbeta(.5,31.5,69.5)
[1] 0.3106351
> qbeta(.5,33,71)
[1] 0.3161325
ANSWERS TO PROBLEMS
285

> (.3161325-.3106351)/.3106351
[1] 0.01769729
> # CI limits:
> qbeta(.025,32,70)
[1] 0.2278132
> qbeta(.025,31.5,69.5)
[1] 0.2257483
> qbeta(.025,33,71)
[1] 0.231841
> (.231841-.2257483)/.2257483
[1] 0.02698891
> qbeta(.975,32,70)
[1] 0.4065568
> qbeta(.975,31.5,69.5)
[1] 0.4050706
> qbeta(.975,33,71)
[1] 0.40943
> (.40943-.4050706)/.4050706
[1] 0.01076207
> # All relative differences for means, medians, and CIs are less than 3%.
> # risks:
> pbeta(.25,32,70)
[1] 0.07795848
> pbeta(.25,31.5,69.5)
[1] 0.08524389
> pbeta(.25,33,71)
[1] 0.0650023
> # The risks are all well below 50%, regardless of prior, based on analysis of the
ﬁrst year’s dataset.
> # (ii) Posteriors are BE (31.5, 69.5), BE (44.5, 106.5), BE (66.5, 184.5) and
BE (87.5, 263.5) for the 4 years of sequential analysis
> pbeta(0.25,31.5,69.5)
[1] 0.0852439
> pbeta(0.25,44.5,106.5)
[1] 0.1114486
> pbeta(0.25,66.5,184.5)
[1] 0.3015056
> pbeta(0.25,87.5,263.5)
[1] 0.5205103
> # All risks are below 50% except for the ﬁnal (fourth), year’s analysis. There is an
upward trend in risk that can be analyzed by looking at the change in odds ratios.
286
ANSWERS TO PROBLEMS

> 0.0852439/(1-0.0852439)
[1] 0.09318757
> 0.1114486/(1-0.1114486)
[1] 0.1254273
> 0.3015056/(1-0.3015056)
[1] 0.4316507
> 0.5205103/(1-0.5205103)
[1] 1.085551
> # The odds ratios are increasing, quadrupling and doubling for the last 2 years,
respectively.
> # (iii) For the beta prior of the ﬁrst year’s analysis with mean m ¼ 0.333 and
s ¼ 0.0875 and the parameters a and b are given by
> sqrt((1/3)*(2/3)/29)
[1] 0.08753762
> (0.333^2-0.333^3-(0.333)*(0.0875)^2)/(0.0875)^2
[1] 9.327469 # alpha
> (1/0.0875^2)*0.333*(1.0-0.333)^2+(0.333-1.0)
[1] 18.68295 # beta
> pbeta(0.25,40.327469,87.68295)
[1] 0.0516386
> pbeta(0.25,53.327469,124.68295)
[1] 0.07038743
> pbeta(0.25,75.327469,202.68295)
[1] 0.2182356
> pbeta(0.25,97.327469,280.68295)
[1] 0.3763185
> # All risks are now below 50% if the logging company’s earlier data are included in
the analysis for the ﬁrst year’s prior. The odds of risk for each year is increasing,
quadrupling and doubling, for the last 2 years, respectively.
> 0.0516386/(1-0.0516386)
[1] 0.05445034
> 0.07038743/(1-0.07038743)
[1] 0.07571695
> 0.2182356/(1-0.2182356)
[1] 0.2791578
> 0.3763185/(1-0.3763185)
[1] 0.6033825
Problem 2.4
> pgamma(3,0.001+400,0.001+100)
[1] 2.208268e-08
> 1-pgamma(3,0.001+400,0.001+100)
[1] 1
ANSWERS TO PROBLEMS
287

> pgamma(3,0.001+400,0.001+100)/(1-pgamma(3,0.001+400,0.001+100))
[1] 2.208268e-08
> pgamma(3,0.001+400+260,0.001+100+75)
[1] 8.031383e-09
> 1-pgamma(3,0.001+400+260,0.001+100+75)
[1] 1
> pgamma(3,0.001+400+260,0.001+100+75)/(1-
pgamma(3,0.001+400+260,0.001+100+75))
[1] 8.031383e-09
> pgamma(3,0.001+400+260+140,0.001+100+75+50)
[1] 1.572358e-06
> 1-pgamma(3,0.001+400+260+140,0.001+100+75+50)
[1] 0.9999984
> pgamma(3,0.001+400+260+140,0.001+100+75+50)/(1-
pgamma(3,0.001+400+260+140,0.001+100+75+50))
[1] 1.57236e-06
> # The probabilities of exceeding the threshold for the 3 years are 1.0, 1.0, and
0.9999984, approximately.
> # The risks for the 3 years are 2.2  1028, 8.0  1029, and 1.6  1026.
> # The odds for the risks for the 3 years are 2.2  1028, 8.0  1029, and
1.6  1026.
> # The odds for the risks are increasing by a factor of 1600/8 ¼ 200 between years
2 and 3, so there is some indication of possible trouble ahead.
Problem 2.5
Use the formulas for the mean and precision of the posterior, expressed as a function
of the mean and precision of the prior and the dataset found in Section 2.4.1.
The precision of the posterior is the sum of the precisions of the prior and the
dataset; the mean of the posterior is the weighted average of the means of the prior
and the dataset, weighted by the respective precisions. If the prior is noninformative,
tprior ﬃ0 and the posterior mean ﬃthe sample mean. If the sample size is doubled,
the precision will be doubled, the variance will be reduced by 1
2, and the standard
deviation will be reduced by 1/p2 ¼ 0.707. If the sample size is quadrupled, the pre-
cision will be quadrupled, the variance will be reduced by 1
4, and the standard devi-
ation will be reduced by 1
2. Therefore the sample size needs to be quadrupled to half
the standard deviation.
Problem 3.1
> 1-pbeta(.75,61,41)
[1] 0.0004146221
> 1-pbeta(.75,135,67)
[1] 0.004968324
288
ANSWERS TO PROBLEMS

> (1-pbeta(.75,135,67))/pbeta(.75,135,67)
[1] 0.004993132
> (1-pbeta(.75,61,41))/pbeta(.75,61,41)
[1] 0.0004147941
> 4993132/414794
[1] 12.03762
> (1-pbeta(.75,135,67))/pbeta(.75,135,67)/((1-pbeta(.75,61,41))/
pbeta(.75,61,41))
[1] 12.03761
. # The Bayes factor for the year 2 analysis is 12.03762, providing strong evidence
in favor of H1: p exceeds 75%, even though the posterior probability for H1 is only
0.005.
Problem 3.2
> # Syi ¼ 30, 48, and 55 with n ¼ 50.
> (1-pgamma(1,0.001,0.001)) # prior probability above 1
[1] 0.006312353
> (1-pgamma(1,0.001,0.001))/pgamma(1,0.001,0.001) # prior odds
[1] 0.006352452
> (1-pgamma(1,30.001,50.001)) # 1st posterior probability above 1
[1] 0.0009169443
> (1-pgamma(1,30.001,50.001))/pgamma(1,30.001,50.001) # 1st posterior
odds
[1] 0.0009177859
> 0.0009177859/0.006352452 # 1st Bayes factor
[1] 0.1444774
> (1-pgamma(1,78.001,100.001)) # 2nd posterior probability above 1
[1] 0.01000833
> (1-pgamma(1,78.001,100.001))/pgamma(1,78.001,100.001) # 2nd
posterior odds
[1] 0.01010951
> 0.01010951/0.009177859 # 2nd Bayes factor
[1] 1.101511
> (1-pgamma(1,133.001,150.001)) # 3rd posterior probability above 1
[1] 0.07434242
> (1-pgamma(1,133.001,150.001))/pgamma(1,133.001,150.001) # 3rd
posterior odds
[1] 0.0803131
> 0.0803131/0.01010951 # 3rd Bayes factor
[1] 7.944312
> # None of the posterior probabilities above 1 are substantial.
> # Bayes factors indicate substantial evidence for below 1, minimal evidence for
above 1, and substantial evidence for above 1.
ANSWERS TO PROBLEMS
289

Problem 3.3
> # sen ¼ .80, spec ¼ .95
> # þ test with cancer
> # þ test, with cancer: sen/(1 2 spec)
> .80/.05
[1] 16
> # þ test, without cancer: (1 2 spec)/sen
> .05/.80
[1] 0.0625
> # 2 test, with cancer: (1 2 sen)/spec
> .20/.95
[1] 0.2105263
> # 2 test, without cancer: spec/(1 2 sen)
> .95/.20
[1] 4.75
> # Tþ increases the odds that she has the disease by 16 if she has the disease and
decreases the odds that she has the disease by 0.0625 if she doesn’t have the disease.
> # T2 increases the odds that she doesn’t have the disease by 4.75 if she doesn’t
have the disease and decreases the odds that she doesn’t have the disease by
0.2105263 if she doesn’t have the disease.
Problem 3.4
The Bayes risks are:
d1: L0
+.(1-specificity).pprior(D0)+ L1
2.(1-sensitivity).pprior(D1)
d2: L1
2.pprior(D1)
d3: L0
+.pprior(D0)
The Bayes rule is to choose the decision d that minimizes risk:
Let I = [L0
+/L1
2].[p(D0)/p(D1)]
If (sensitivity+specificity)  1
Choose d3 if
I  (1-sensitivity)/specificity
Choose d1 if
(1-sensitivity)/specificity , I , sensitivity/
(1-specificity)
Choose d2 if
I  sensitivity/(1-specificity)
If (sensitivity+specificity) , 1
Choose d3 if
I  1
Choose d2 if
I . 1
290
ANSWERS TO PROBLEMS

Problem 3.5
> 3000*.05*.90+20000*.20*.10
[1] 535
> 20000*.10
[1] 2000
> >3000*.90
[1] 2700
# Bayes risks are d1: 535, d2: 2000, and d3: 2700
> # Best decision is d1: take the test and decide on the basis of the test
Note: I ¼ [3000/20000].[.90/.10] ¼ 1.35
(12sensitivity)/speciﬁcity ¼ .20/.95 ¼ 0.2105263
sensitivity/(12speciﬁcity) ¼ .80/.05 ¼ 16
so choose d1
Problem 3.6
Sensitivity is the probability of deciding whether the threshold has not been
exceeded, based on the sample monitoring, if the threshold has not been exceeded.
Speciﬁcity is the probability of deciding, whether the threshold has been
exceeded, based on the sample monitoring, if the threshold has been exceeded.
The decision can be based on the risk of not exceeding the threshold, the posterior
cumulative probability of p  pc. If this risk is , 50%, the decision is that the
threshold has been exceeded. Otherwise, if this risk  50%, the decision is that the
threshold has not been exceeded.
Assuming D0 is the state of exceeding the threshold (i.e., local viability of the
owl) and D1 is the state of not exceeding the threshold (i.e., local extinction of the
owl), the loss L0
þ is the opportunity cost of the timber not harvested caused by
reduced harvesting decided from the monitoring results that conclude the threshold
has not been exceeded, when the threshold really has been exceeded and the local
owl population is viable. The loss L1
2 is the cost of the loss of the local owl population
caused by the failure to reduce the harvesting decided from the monitoring results
that conclude the threshold has been exceeded, when the threshold really has not
been exceeded.
The Bayes rule is to choose the decision d that minimizes risk, as follows: let
I ¼ [L0
þ/L1
2] . [p(D0)/p(D1)].
If (sensitivity+specificity)  1
Choose d3 if
I  (1-sensitivity)/specificity
ANSWERS TO PROBLEMS
291

Choose d1 if
(1-sensitivity)/specificity , I , sensitivity/(1-specificity)
Choose d2 if
I  sensitivity/(1-specificity)
If (sensitivity+specificity) , 1
Choose d3 if
I  1
Choose d2 if
I . 1
The Bayes rule suggests that in certain instances the monitoring is not necessary,
depending on the relative magnitudes of I ¼ [L0
þ/L1
2] . [p(D0)/p(D1)] [i.e., ratios
(cost of timber)/(loss of owl) and probability (exceeding threshold)/probability
(not exceeding threshold)].
Problem 4.1
Program code:
Bayesian Poisson Model
Non-informative prior
y ~ Pois(lambda=3.0)
1. Program
model
{
for(i in 1:n){y[i] ~ dpois(lambda)}
lambda ~ dgamma(0.001,0.001)
}
2. Data
list(y = c(4, 2, 2, 4, 5, 1, 2,
2, 2, 4, 5, 6, 0, 0, 1, 3, 3, 4,
3, 2, 3, 3, 4, 1, 4, 6, 4, 4, 1, 3),
n = 30)
3. Initial values
list(lambda = 1)
292
ANSWERS TO PROBLEMS

Outputs:
ANSWERS TO PROBLEMS
293

Note that the mean and variance are statistically equal in the Poisson–gamma model
results and that sigma is statistically equal to 0 in the mixed-effects model.
Problem 4.2
Program code:
1. Program
model
{
for(i in 1:n)
{
y[i] ~ dpois(lambda[i])
log(lambda[i]) <- mu+e[i]
e[i]~ dnorm(0,tau)
}
mu ~ dgamma(0.001,0.001)
tau ~ dgamma(0.001,0.001)
sigma <- 1/sqrt(tau)
}
294
ANSWERS TO PROBLEMS

Output:
Note that the error sigma is statistically equivalent to 0 in the multiplicative
mixed-effects model.
Problem 4.3
function(y, k, m, initlambda1, initlambda2)
# Program Gibbs
{
n <- length(y)
y1 <- sum(y[1:k])
y2 <- sum(y[(k+1):n])
lambda1 <- rep(NA, m)
lambda2 <- rep(NA, m)
lambda1[1] <- initlambda1
lambda2[1] <- initlambda2
for(i in 2:m) {
lambda1[i] <- rgamma(1, 0.001 + y1, k)
lambda2[i] <- rgamma(1, 0.001 + y2, (n - k))
}
return(lambda1, lambda2)
ANSWERS TO PROBLEMS
295

> y
[1] 4 2 2 4 5 1 2 2 2 4 5 6 0 0 1 3 3 4 3 2 3 3 4 1 4 6 4 4 1 3
> output <- Gibbs(y,20,100000,1,1)
> mean(output[[1]])
[1] 2.749199
> stdev(output[[1]])
[1] 0.3708539
> mean(output[[2]])
[1] 3.298029
> stdev(output[[2]])
[1] 0.5749896
> sum(y[1:20])/20
[1] 2.75
> sum(y[21:30])/10
[1] 3.3
> hist(output[[1]])
> hist(output[[2]])
Problem 4.4
> dbeta(0,3,2)
[1] 0
> dbeta(.34,3,2)
[1] 0.915552
> # choose p1 = .34
> dbeta(.67,3,2)
[1] 1.777644
> # choose p2 = .67
> dbeta(.52,3,2)
[1] 1.557504
> dbeta(.52,3,2)/dbeta(.67,3,2)
[1] .8761619
> # .91> .87 so choose p3 = p2 = .67
296
ANSWERS TO PROBLEMS

Problem 4.5
> y <- rnorm(100,15,2)
> y
[1] 16.85654 19.59268 16.17593 16.44335 15.13185 17.14970
15.69704 17.20139 11.28192
[10] 16.94269 11.79007 14.35370 17.00237 14.01508 14.81110
15.38694 18.01536 15.59951
[19] 18.15602 11.71837 18.65446 12.77382 12.47099 12.92969
16.92157 16.96831 14.88682
[28] 15.96130 15.82643 16.54236 12.81923 14.12794 16.09028
16.81438 14.44691 15.05185
[37] 19.17883 14.67723 14.62800 14.89441 13.80578 14.68562
15.46704 12.75604 16.55266
[46] 15.44572 16.95620 14.67055 14.27893 16.83547 19.83804
14.97178 14.31355 17.30323
[55] 17.13452 14.30720 10.30112 14.03593 15.19265 11.11533
13.54244 17.65803 17.88774
[64] 16.37850 12.75012 16.72239 14.50407 16.16680 14.70039
12.92720 19.98371 13.56213
[73] 19.32531 13.71453 13.58275 13.30632 13.07786 14.90847
15.15850 13.49658 12.90805
[82] 12.28615 14.45550 12.52626 14.45088 12.50655 11.95776
12.53209 19.57162 14.65868
[91] 13.36388 15.84884 15.96699 14.93516 12.78427 12.97294
14.26660 14.45177 12.60170
[100] 14.89966
> mean(y)
[1] 15.03253
> var(y)
[1] 4.366266
> 100/4 # tau data
[1] 25
> 1/4 # tau prior
[1] 0.25
> 25+.25 # tau posterior - theoretical
[1] 25.25
> (.25/25.25)*10+(25/25.25)*15.03253 # mean posterior -
theoretical
[1] 14.9827
> output1_MCMC(10,2,2,y,.24,15,5000,500000)
> output1[[2]] # rejection rate
[1] 0.3459367
> mean(output1[[1]]) # mean posterior - empirical mean
[1] 14.98291
ANSWERS TO PROBLEMS
297

> 1/var(output1[[1]]) # tau posterior - empirical precision
[1] 25.56797
Problem 4.6
mposterior ¼ 14.98, sposterior ¼ 0.1987, tposterior ¼ 25.3282.
Problem 5.1
> data2
sample aspect species old.growth rock moss temp moist response
1
1
0
0
0.812
1
1 22.5
12.1
33.6
2
2
1
0
0.564
0
0 22.9
35.2
45.5
3
3
0
0
0.455
0
1 29.7
52.3
67.6
4
4
1
1
0.808
0
0 15.2
15.2
26.9
5
5
0
1
0.225
0
0 17.5
38.7
53.1
6
6
0
0
0.765
0
0 29.8
93.0
105.5
7
7
1
0
0.059
0
0 17.4
2.0
15.6
8
8
1
1
0.038
0
0 20.6
80.5
84.3
9
9
1
0
0.277
0
0 27.9
90.9
102.1
10
10
1
0
0.417
0
1 16.1
63.6
65.9
298
ANSWERS TO PROBLEMS

11
11
0
0
0.757
0
0 27.8
51.4
62.5
12
12
1
0
0.806
0
0 21.9
96.8
108.7
13
13
1
1
0.015
0
1 21.6
77.9
90.7
14
14
0
0
0.050
0
1 25.3
3.5
20.3
15
15
0
0
0.006
0
1 16.8
49.2
50.8
16
16
0
0
0.646
0
1 15.5
59.6
68.0
17
17
0
1
0.800
0
1 19.9
40.5
48.3
18
18
1
1
0.358
1
0 26.7
81.5
90.8
19
19
1
1
0.151
0
1 18.6
20.3
37.9
20
20
0
0
0.745
0
0 29.1
11.3
25.2
21
21
1
0
0.905
0
1 19.7
93.4
100.2
22
22
0
0
0.206
0
1 16.6
25.0
29.1
23
23
1
1
0.576
0
0 18.4
34.1
40.4
24
24
1
0
0.644
0
1 25.4
46.3
57.7
25
25
1
0
0.501
0
1 22.8
43.0
58.5
26
26
1
0
0.204
0
0 22.0
96.0
105.6
27
27
0
0
0.575
0
1 15.0
33.8
37.7
28
28
1
0
0.777
0
0 25.5
62.5
76.6
29
29
0
0
0.484
0
1 25.0
57.0
67.3
30
30
0
0
0.231
0
1 23.9
4.2
12.9
31
31
1
0
0.489
0
1 20.7
94.5
102.5
32
32
1
1
0.012
0
1 25.4
36.5
55.3
33
33
1
1
0.205
0
0 17.6
7.5
25.6
34
34
0
0
0.238
0
1 22.8
84.6
88.0
35
35
0
0
0.144
0
0 19.7
22.9
26.8
36
36
1
0
0.887
0
0 17.6
54.8
62.9
37
37
0
0
0.001
0
0 15.7
66.8
78.4
38
38
1
1
0.388
0
0 29.1
42.5
57.6
39
39
0
0
0.396
0
1 17.7
72.4
77.1
40
40
0
0
0.751
0
0 18.4
3.8
19.0
41
41
0
0
0.687
0
1 20.9
14.8
27.6
42
42
1
1
0.514
0
0 21.9
52.8
60.3
43
43
1
0
0.371
1
1 27.8
64.2
71.7
44
44
0
0
0.494
0
0 21.5
92.4
95.1
45
45
1
0
0.205
1
0 21.5
22.3
36.3
46
46
0
0
0.919
0
1 29.4
57.9
74.7
47
47
0
0
0.746
1
0 27.6
90.0
98.7
48
48
1
0
0.197
0
0 22.1
5.7
21.1
49
49
1
1
0.655
1
1 26.4
68.1
77.2
50
50
0
0
0.666
1
0 24.8
29.5
44.6
> output1 <- lm(response~aspect+species+old.growth,data=data2)
>
summary(output1)
Call: lm(formula = response ~ aspect + species + old.growth, data
= data2)
ANSWERS TO PROBLEMS
299

Residuals:
Min
1Q
Median
3Q
Max
-46.32 -22.96
-0.2043 24.86 46.09
Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
47.7477
9.2131
5.1826
0.0000
aspect
13.2737
8.5660
1.5496
0.1281
species
-6.9754
9.8722
-0.7066
0.4834
old.growth
15.2475
14.3799
1.0603
0.2945
Residual standard error: 27.87 on 46 degrees of freedom
Multiple R-Squared: 0.07033
F-statistic: 1.16 on 3 and 46 degrees of freedom, the p-value is
0.3353
Correlation of Coefficients:
(Intercept)
aspect species
aspect -0.4156
species -0.2125
-0.3733
old.growth -0.7817
0.0507
0.1606
> output2 <- lm(response~aspect+species+rock,data=data2)
>
summary(output2)
Call: lm(formula = response ~ aspect + species + rock, data =
data2)
Residuals:
Min
1Q Median
3Q
Max
-51.81 -24.71 -1.017 23.34 50.76
Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
54.7364
5.9762
9.1591
0.0000
aspect
12.6755
8.6450
1.4662
0.1494
species
-8.6932
9.8408
-0.8834
0.3816
rock
5.2043
11.4807
0.4533
0.6525
Residual standard error: 28.14 on 46 degrees of freedom
Multiple R-Squared: 0.05184
F-statistic: 0.8383 on 3 and 46 degrees of freedom, the p-value
is 0.4799
300
ANSWERS TO PROBLEMS

Correlation of Coefficients:
(Intercept)
aspect species
aspect -0.5773
species -0.1352
-0.3864
rock -0.2390
-0.0351 -0.0082
> output3 <- lm(response~aspect+species+moss,data=data2)
>
summary(output3)
Call: lm(formula = response ~ aspect + species + moss, data =
data2)
Residuals:
Min
1Q
Median
3Q
Max
-52.38 -22.36 -0.3957 22.88 50.44
Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
55.0630
7.5336
7.3090
0.0000
aspect
12.9127
8.7850
1.4699
0.1484
species
-8.6300
9.8699
-0.8744
0.3865
moss
0.5463
8.1530
0.0670
0.9469
Residual standard error: 28.2 on 46 degrees of freedom
Multiple R-Squared: 0.0477
F-statistic: 0.768 on 3 and 46 degrees of freedom, the p-value is
0.5179
Correlation of Coefficients:
(Intercept)
aspect species
aspect -0.5667
species -0.1345
-0.3743
moss -0.6357
0.1691
0.0401
> output4 <- lm(response~aspect+species+temp,data=data2)
>
summary(output4)
Call: lm(formula = response ~ aspect + species + temp, data =
data2)
Residuals:
Min
1Q
Median
3Q
Max
-46.49 -20.13 -0.5598
24.63 41.47
ANSWERS TO PROBLEMS
301

Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
15.3126
20.4509
0.7488
0.4578
aspect
12.5388
8.2943
1.5117
0.1374
species
-6.9314
9.4837
-0.7309
0.4686
temp
1.7982
0.8830
2.0364
0.0475
Residual standard error: 27.01 on 46 degrees of freedom
Multiple R-Squared: 0.1264
F-statistic: 2.218 on 3 and 46 degrees of freedom, the p-value is
0.09875
Correlation of Coefficients:
(Intercept)
aspect species
aspect -0.1488
species -0.1243
-0.3868
temp -0.9622
-0.0162
0.0893
> output5 <- lm(response~aspect+species+moist,data=data2)
>
summary(output5)
Call: lm(formula = response ~ aspect + species + moist, data =
data2)
Residuals:
Min
1Q
Median
3Q
Max
-8.233 -3.005
0.5891
3.31 8.938
Coefficients:
Value Std. Error
t value
Pr(>|t|)
(Intercept) 13.4517
1.3370
10.0608
0.0000
aspect
1.4473
1.3807
1.0483
0.3000
species
0.4119
1.5577
0.2644
0.7926
moist
0.9264
0.0216
42.8091
0.0000
Residual standard error: 4.414 on 46 degrees of freedom
Multiple R-Squared: 0.9767
F-statistic: 642.2 on 3 and 46 degrees of freedom, the p-value is
0
Correlation of Coefficients:
(Intercept)
aspect species
aspect -0.2623
species -0.1949
-0.4024
moist -0.7326
-0.1923
0.1360
302
ANSWERS TO PROBLEMS

> output6 <- lm(response~aspect+old.growth+temp,data=data2)
>
summary(output6)
Call: lm(formula = response ~ aspect + old.growth + temp, data =
data2)
Residuals:
Min
1Q Median
3Q
Max
-44.11 -19.66
-3.4
25.7
43.62
Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
11.1951 20.4572
0.5472
0.5869
aspect
10.9374
7.6936
1.4216
0.1619
old.growth
11.3723 14.0432
0.8098
0.4222
temp
1.7058
0.8977
1.9003
0.0637
Residual standard error: 26.98 on 46 degrees of freedom
Multiple R-Squared: 0.1286
F-statistic: 2.264 on 3 and 46 degrees of freedom, the p-value is
0.09363
Correlation of Coefficients:
(Intercept)
aspect old.growth
aspect -0.2279
old.growth -0.1364
0.1193
temp -0.9047
-0.0053 -0.2064
> output7 <- lm(response~species+old.growth+temp,data=data2)
>
summary(output7)
Call: lm(formula = response ~ species + old.growth + temp, data =
data2)
Residuals:
Min
1Q Median
3Q
Max
-49.21 -20.02 -2.366 25.05 48.14
Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
18.0175
20.8655
0.8635
0.3923
species
-0.3860
9.0714
-0.0426
0.9662
old.growth
8.8792
14.4813
0.6131
0.5428
temp
1.7105
0.9184
1.8625
0.0689
Residual standard error: 27.57 on 46 degrees of freedom
Multiple R-Squared: 0.09039
ANSWERS TO PROBLEMS
303

F-statistic: 1.524 on 3 and 46 degrees of freedom, the p-value is
0.221
Correlation of Coefficients:
(Intercept) species old.growth
species -0.2206
old.growth -0.1480
0.1797
temp -0.9177
0.0520 -0.1942
> output8 <- lm(response~moss+temp+moist,data=data2)
>
summary(output8)
Call: lm(formula = response ~ moss + temp + moist, data = data2)
Residuals:
Min
1Q Median
3Q
Max
-8.174 -2.975 0.3753 2.285 7.776
Coefficients:
Value Std. Error
t value Pr(>|t|)
(Intercept)
4.1456
2.9379
1.4111
0.1649
moss
-0.6874
1.0988
-0.6256
0.5347
temp
0.5026
0.1294
3.8856
0.0003
moist
0.9138
0.0191
47.9490
0.0000
Residual standard error: 3.877 on 46 degrees of freedom
Multiple R-Squared: 0.982
F-statistic: 836.7 on 3 and 46 degrees of freedom, the p-value is
0
Correlation of Coefficients:
(Intercept)
moss
temp
moss -0.2248
temp -0.9142
0.0483
moist -0.1091
-0.0056 -0.2141
> output9 <- lm(response~rock+temp,data=data2)
>
summary(output9)
Call: lm(formula = response ~ rock + temp, data = data2)
Residuals:
Min
1Q Median
3Q
Max
-50.4 -19.93 -1.012 24.9 49.13
304
ANSWERS TO PROBLEMS

Coefficients:
Value Std. Error
t value Pr(.|t|)
(Intercept)
18.7850
20.5933
0.9122
0.3663
rock
-1.2595
11.6948
-0.1077
0.9147
temp
1.8625
0.9340
1.9941
0.0520
Residual standard error: 27.39 on 47 degrees of freedom
Multiple R-Squared: 0.08271
F-statistic: 2.119 on 2 and 47 degrees of freedom, the p-value is
0.1315
Correlation of Coefficients:
(Intercept)
rock
rock
0.2199
temp -0.9792
-0.2985
> output10 <- lm(response~temp+moist,data=data2)
>
summary(output10)
Call: lm(formula = response ~ temp + moist, data = data2)
Residuals:
Min
1Q Median
3Q
Max
-7.836 -2.709 0.4237 2.346 7.414
Coefficients:
Value Std. Error t value Pr(.|t|)
(Intercept)
3.7324
2.8441
1.3123
0.1958
temp
0.5065
0.1284
3.9460
0.0003
moist
0.9138
0.0189
48.2598
0.0000
Residual standard error: 3.852 on 47 degrees of freedom
Multiple R-Squared: 0.9819
F-statistic: 1271 on 2 and 47 degrees of freedom, the p-value is
0
Correlation of Coefficients:
(Intercept)
temp
temp -0.9281
moist -0.1132
-0.2140
> output11 <- lm(response~rock+moss+temp+moist,data=data2)
>
summary(output11)
Call: lm(formula = response ~ rock + moss + temp + moist, data =
data2)
ANSWERS TO PROBLEMS
305

Residuals:
Min
1Q Median
3Q
Max
-8.159 -2.948
0.395 2.29 7.649
Coefficients:
Value Std. Error
t value Pr(.|t|)
(Intercept)
4.2009
3.0349
1.3842
0.1731
rock
0.1485
1.6749
0.0886
0.9298
moss
-0.6846
1.1113
-0.6160
0.5410
temp
0.4990
0.1368
3.6472
0.0007
moist
0.9139
0.0193
47.4231
0.0000
Residual standard error: 3.92 on 45 degrees of freedom
Multiple R-Squared: 0.982
F-statistic: 614 on 4 and 45 degrees of freedom, the p-value is 0
Correlation of Coefficients:
(Intercept)
rock
moss
temp
rock
0.2055
moss -0.2141
0.0284
temp -0.9155
-0.2942
0.0378
moist -0.1030
0.0180 -0.0051 -0.2098
> output12 <-
lm(response~aspect+species+old.growth+rock+moss+temp+moist,
data=data2)
>
summary(output12)
Call: lm(formula = response ~ aspect + species + old.growth +
rock + moss + temp + moist, data = data2)
Residuals:
Min
1Q Median
3Q
Max
-6.644 -2.945 0.1736 2.689 8.034
Coefficients:
Value Std. Error
t value Pr(.|t|)
(Intercept)
2.4261
3.2115
0.7554
0.4542
aspect
1.5790
1.2498
1.2634
0.2134
species
0.8601
1.4014
0.6138
0.5427
old.growth
1.3029
2.0826
0.6256
0.5349
rock
-0.0954
1.6831
-0.0567
0.9551
moss
-0.2597
1.1378
-0.2282
0.8206
temp
0.5084
0.1388
3.6616
0.0007
moist
0.9089
0.0198
46.0020
0.0000
306
ANSWERS TO PROBLEMS

Residual standard error: 3.918 on 42 degrees of freedom
Multiple R-Squared: 0.9832
F-statistic: 351.6 on 7 and 42 degrees of freedom, the p-value is
0
Correlation of Coefficients:
(Intercept)
aspect species old.growth
rock
moss
temp
aspect -0.1812
species -0.1645
-0.3679
old.growth -0.1734
0.0855
0.1408
rock
0.2240
-0.0390 -0.0447 -0.0845
moss -0.2632
0.1758
0.0492
0.0672
0.0108
temp -0.8561
0.0310
0.0501 -0.1380
-0.2826
0.0452
moist -0.0555
-0.2032
0.1021 -0.0965
0.0296
-0.0395
-0.1924
>
AIC(output1,output2,output3,output4,output5,output6,output7,
output8,output9,output10,output11,output12)
df
AIC
output1
5 480.4718
output2
5 481.4564
output3
5 481.6743
output4
5 477.3635
output5
5 296.1967
output6
5 477.2330
output7
5 479.3809
output8
5 283.2389
output9
4 477.8016
output10
4 281.6625
output11
6 285.2302
output12
9 287.7390
See Fig. B5.9 for a summary of the AIC and AIC weights of the comparative
models.
Conclusion: Model 10, with temp and moist, is the best-ﬁtting. The correct
model is model 10 with {temp,moist}, best ﬁtting the simulated “reality”
response  0.7*temp + 0.9*moist + N(0,4).
Model 10 analysis provides statistics that are compatible with the “reality.”
ANSWERS TO PROBLEMS
307

Figure B5.9. AIC, AIC weight, DIC, and DIC weights for Problems 5.1, 5.3, and 5.4.
308

Problem 5.2
> #
Stepwise multiple linear regression analysis in S-Plus and R
> object0 <- lm(response~1,data2)
> scope <- “.~.+aspect+species+old.growth+rock+moss+temp+moist”
> step(object0,scope,direction=“forward”)
Start:
AIC= 39994.97
response ~ 1
Single term additions
Model:
response ~ 1
scale:
784.2151
Df Sum of Sq
RSS
Cp
,none.
38426.54 39994.97
aspect
1
1216.28 37210.26 40347.12
species
1
87.11 38339.43 41476.29
old.growth
1
822.78 37603.76 40740.62
rock
1
195.75 38230.79 41367.65
moss
1
16.86 38409.68 41546.54
temp
1
3169.38 35257.16 38394.02
moist
1
37498.06
928.48
4065.34
Step:
AIC= 4065.339
response ~ moist
Single term additions
Model:
response ~ moist
scale:
784.2151
Df Sum of Sq
RSS
Cp
,none.
928.4784 4065.339
aspect
1
30.9901 897.4883 5602.779
species
1
10.9459 917.5324 5622.823
old.growth
1
16.9201 911.5583 5616.849
rock
1
23.9238 904.5545 5609.845
moss
1
9.9676 918.5108 5623.801
temp
1
231.0531 697.4253 5402.716
Call:
lm(formula = response ~ moist, data = data2)
ANSWERS TO PROBLEMS
309

Coefficients:
(Intercept)
moist
14.14828 0.9297645
Degrees of freedom: 50 total; 48 residual
Residual standard error (on weighted scale): 4.398102
> #
The stepwise mlr best ﬁtting model is {moist}.
> #
Best subsets multiple linear regression in S-Plus and R
> models <- list(rep(NA,length(covariates)))
> for (i in 1:length(covariates)) {models[[i]] <-
lm(paste(“response~”,covariates[i]),data2)}
> aic <- rep(NA,length(covariates))
> for (i in 1:length(covariates)) {aic[i] <-
AIC(lm(paste(“response~”,covariates[i]),data2))}
> min(aic)
[1] 280.6349
> for (i in 1:length(covariates)) {if (aic[i]==min(aic)) k <- i}
> k
[1] 43
> covariates[k]
[1] “aspect+temp+moist”
> summary(models[[k]])
Call: lm(formula = paste(“response~”, covariates[i]), data =
data2)
Residuals:
Min
1Q
Median
3Q
Max
-6.987 -2.947 0.01178 2.677 8.176
Coefficients:
Value Std. Error t value Pr(>|t|)
(Intercept)
2.7712
2.8462
0.9736
0.3353
aspect
1.8359
1.0835
1.6945
0.0969
temp
0.5181
0.1261
4.1098
0.0002
moist
0.9087
0.0188
48.3091
0.0000
Residual standard error: 3.778 on 46 degrees of freedom
Multiple R-Squared: 0.9829
F-statistic: 882.2 on 3 and 46 degrees of freedom, the p-value is
0
Correlation of Coefficients:
(Intercept)
aspect
temp
aspect -0.1993
temp -0.9190
0.0543
moist -0.0777
-0.1597 -0.2197
310
ANSWERS TO PROBLEMS

> #
The best subsets selection mlr best ﬁtting model is {aspect, temp, moist}.
> #
Conclusion:
(i) stepwise mlr - model with {moist} is best ﬁtting
(ii) best subsets mlr - model with {aspect, temp, and moist} is best ﬁtting
Note:
> outputa <- lm(response~moist,data2)
> outputb <- lm(response~aspect+temp+moist,data2)
> AIC(outputa,outputb,output10)
df
AIC
outputa
3 293.9701
outputb
5 280.6349
output10
4 281.6625
> #
The correct model is Model #10 with {temp,moist}, ﬁtting the “reality:”
response  0.7*temp + 0.9*moist + N(0,4)
> #
Best subsets selection slightly overﬁt the sample dataset, with an additional
specious covariate aspect.
> #
AICc might provide the proper ranking of the models.
> #
Note that {aspect, temp, moist} was not among the parsimonious collec-
tion of models considered with the a priori strategy of model selection and inference
used in Problem 5.1.
Problem 5.3
ANSWERS TO PROBLEMS
311

312
ANSWERS TO PROBLEMS

ANSWERS TO PROBLEMS
313

314
ANSWERS TO PROBLEMS

ANSWERS TO PROBLEMS
315

316
ANSWERS TO PROBLEMS

See Fig. B5.9 for a summary of the DIC and DIC weights of the comparative models.
Conclusion: The Bayesian DIC values are similar to the frequentist AIC values in
Problem 5.1 with the same model ranking. Model 10 is the best-ﬁtting model with the
Bayesian statistical analysis with DIC ¼ 281.890.
Problem 5.4
> #
The unconditional shrinkage estimates of the coefﬁcients of temp (beta6) and
moist (beta7), using AIC weights, are:
> #
unconditional shrinkage estimates of beta6 using AIC weights
> .2720*.5026+.5983*.5065+.1005*.4990+.0287*.5084
[1] 0.5044867
> #
unconditional shrinkage estimates of beta6 se using AIC weights
> .2720*.1294+.5983*.1284+.1005*.1368+.0287*.1388
[1] 0.1297505
> #
unconditional shrinkage estimates of beta7 using AIC weights
> .0004*.9264+.2720*.9138+.5983*.9138+.1005*.9139+.0287*.9089
[1] 0.913583
> #
unconditional shrinkage estimates of beta7 se using AIC weights
> .0004*.0216+.2720*.0191+.5983*.0189+.1005*.0193+.0287*.0198
[1] 0.01901962
> #
The results without shrinkage are very similar to those with shrinkage since the
AIC weights for the models without temp and moist are negligible.
ANSWERS TO PROBLEMS
317

> #
The results, using DIC weights, are very similar to those using AIC weights
since the weights are similar and the model coefﬁcient estimates are similar.
> #
The importance of the covariates, using AIC weights are:
> #
importance of aspect
> .0004+.0287
[1] 0.0291
> #
importance of species
> .0004+.0287
[1] 0.0291
> #
importance of old.growth
> .0287
[1] 0.0287
> #
importance of rock
> .1005+.0287
[1] 0.1292
> #
importance of moss
> .2720+.1005+.0287
[1] 0.4012
> #
importance of temp
> .2720+.5983+.1005+.0287
[1] 0.9995
> #
importance of moist
> .0004+.2720+.5983+.1005+.0287
[1] 0.9999
> #
The importance of the covariates, using DIC weights are very similar, since the
AIC and DIC weights are similar.
Problem 5.5
> # Recall, the simulated “reality” is
response ~ 0.7*temp + 0.9*moist + N(0, 4).
> We examine goodness-of-fit for the Correct Model #10
> summary(output10)
Call: lm(formula = response ~ temp + moist, data = data2)
Residuals:
Min
1Q Median
3Q
Max
-7.836 -2.709 0.4237 2.346 7.414
Coefficients:
Value Std. Error t value Pr(>|t|)
(Intercept)
3.7324
2.8441
1.3123
0.1958
temp
0.5065
0.1284
3.9460
0.0003
moist
0.9138
0.0189
48.2598
0.0000
318
ANSWERS TO PROBLEMS

Residual standard
error: 3.852 on 47 degrees of freedom
Multiple R-Squared:
0.9819
F-statistic: 1271 on 2 and 47 degrees of freedom, the p-value is
0
Correlation of Coefficients:
(Intercept)
temp
temp -0.9281
moist -0.1132
-0.2140
> # Histogram of the Model #10 residuals
> # Graph of model 10 residuals/ﬁtted values shows no indications of dependence or
outliers.
ANSWERS TO PROBLEMS
319

> # Normal plot of residuals shows no abnormalities of note
> # Cross validation program:
> cross.validation
function(model, j, k, data1)
{
# model is of the form response~sum of 2 covariates
# j,k are the data1 covariate data1 columns in the model
n <- dim(data1)[1]
residuals <- rep(NA, n)
for(i in 1:n) {
output <- lm(model, data = data1[ - i, ])
residuals[i] <- data1$response[i] - sum(output$
coefficients * c(1, data1[i, j], data1[i, k]))
}
return(residuals)
}
> output <- cross.validation("response~temp+moist",7,8,data2)
> output
[1]
7.8395715 -2.0498825
1.1267600
1.7248288
5.3709663
1.8987201
[7]
1.3372920 -3.6043921
1.2794691 -4.4184753 -2.4144195
5.8661706
[13]
5.0531451
0.6105728 -6.7461355
2.1169389 -2.5867780
-0.9862320
320
ANSWERS TO PROBLEMS

[19]
6.5000915 -4.1219727
1.2442463 -6.2503065 -3.9577061
-1.2464063
[25]
4.0140447
3.2404994 -4.8776173
2.9403953 -1.2156973
-7.3480755
[31]
2.0913613
5.5649725
6.5676859 -4.8236869 -8.1508647
0.1866867
[37]
6.1770243
0.3176671 -1.8707143
2.6705973 -0.2543623
-2.8300577
[43] -5.0577898 -4.2416706
1.3498520
3.4305151 -1.3577022
1.0327350
[49] -2.2290007
1.4074559
> hist(output)
]]>
Problem 5.6 (Report)
ANSWERS TO PROBLEMS
321

Problem 6.1
322
ANSWERS TO PROBLEMS

> model21 _ glm(response~aspect+species+old.growth,data=data2,
family=binomial(link=logit))
> summary(model21)
Call: glm(formula = response ~ aspect + species + old.growth,
family =binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-2.018374 -0.6262515 -0.2038182 0.4335786 2.374611
Coefficients:
Value Std. Error
t value
(Intercept) -0.3812985
0.2945342 -1.294581
aspect -0.3804848
0.3108366 -1.224067
species -0.2735954
0.3053769 -0.895927
old.growth
2.5162455
0.4964602
5.068373
ANSWERS TO PROBLEMS
323

(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 73.6352 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 5
Correlation of Coefficients:
(Intercept)
aspect
species
aspect
0.0947743
species -0.0070858
0.3118578
old.growth -0.0227379
-0.1806913 -0.2075402
> model22 _
glm(response~aspect+species+rock,data=data2,family=binomial
(link=logit))
> summary(model22)
Call: glm(formula = response ~ aspect + species + rock, family =
binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.168238 -1.009788 -0.9664637
1.234161
1.469901
Coefficients:
Value Std. Error
t value
(Intercept) -0.28445294
0.2029802 -1.4013828
aspect -0.19315599
0.2052469 -0.9410909
species -0.04381164
0.2065612 -0.2121001
rock -0.08732775
0.2080100 -0.4198247
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 135.6378 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 2
Correlation of Coefficients:
(Intercept)
aspect
species
aspect
0.0241944
species
0.0051767
0.1222465
rock
0.0104506
0.0726275
0.0463164
324
ANSWERS TO PROBLEMS

> model23 _ glm(response~aspect+species+moss,data=data2,
family=binomial(link=logit))
> summary(model23)
Call: glm(formula = response ~ aspect + species + moss,
family = binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.213441 -1.07437 -0.9117188
1.284061
1.521494
Coefficients:
Value Std. Error
t value
(Intercept) -0.28777002
0.2038046 -1.4119897
aspect -0.16581842
0.2064232 -0.8032935
species -0.04631863
0.2072606 -0.2234801
moss -0.20580646
0.2075097 -0.9917918
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 134.8263 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 2
Correlation of Coefficients:
(Intercept)
aspect
species
aspect
0.0204747
species
0.0061224
0.1165137
moss
0.0329830 -0.0966413
0.0332432
> model24 _ glm(response~aspect+species+temp,data=data2,family=
binomial(link=logit))
> summary(model24)
Call: glm(formula = response ~ aspect + species + temp, family =
binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.470158 -1.00431 -0.6476229
1.078913
1.840854
ANSWERS TO PROBLEMS
325

Coefficients:
Value Std. Error
t value
(Intercept) -0.31972304
0.2158474 -1.4812456
aspect -0.09995994
0.2172582 -0.4600974
species -0.02904393
0.2172947 -0.1336615
temp
0.72868208
0.2315928
3.1463938
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 124.6936 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 3
Correlation of Coefficients:
(Intercept)
aspect
species
aspect
0.0013862
species
0.0007743
0.0889408
temp -0.1041291
0.1074510
0.0054037
> model25 _ glm(response~aspect+species+moist,data=data2,family=
binomial(link=logit))
> summary(model25)
Call: glm(formula = response ~ aspect + species + moist,family =
binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.151355 -0.9987872 -0.9974408
1.247247
1.412807
Coefficients:
Value Std. Error
t value
(Intercept) -0.284168027
0.2028145 -1.401122896
aspect -0.187356391
0.2096769 -0.893548046
species -0.039878178
0.2063326 -0.193271357
moist
0.001078895
0.2089821
0.005162618
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 135.8159 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 2
326
ANSWERS TO PROBLEMS

Correlation of Coefficients:
(Intercept)
aspect
species
aspect
0.0231497
species
0.0044574
0.1253947
moist
0.0000237 -0.2203928 -0.0456131
> model26 _ glm(response~aspect+old.growth+temp,data=data2,family
=binomial(link=logit))
> summary(model26)
Call: glm(formula = response ~ aspect + old.growth + temp, family
= binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.818713 -0.2298326 -0.01874344
0.10295
2.162767
Coefficients:
Value Std. Error
t value
(Intercept) -0.7432215
0.4558625 -1.6303633
aspect -0.2792430
0.4280498 -0.6523611
old.growth
4.8563381
1.1088344
4.3796785
temp
3.0948623
0.7969715
3.8832782
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 36.31113 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 6
Correlation of Coefficients:
(Intercept)
aspect old.growth
aspect
0.0775643
old.growth -0.1883359 -0.0578475
temp -0.3239957 -0.0217232
0.7861620
> model27 _ glm(response~rock+temp+moist,data=data2,
family=binomial(link=logit))
> summary(model27)
Call: glm(formula = response ~ rock + temp + moist, family =
binomial(link = logit), data = data2)
ANSWERS TO PROBLEMS
327

Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.549069 -0.9826195 -0.6460244
1.054946
1.800411
Coefficients:
Value
Std.Error
t value
(Intercept) -0.32192768
0.2164081 -1.4875951
rock -0.15748363
0.2229356 -0.7064087
temp
0.76042412
0.2317981
3.2805447
moist -0.05660421
0.2164813 -0.2614739
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 124.3217 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 3
Correlation of Coefficients:
(Intercept)
rock
temp
rock
0.0277688
temp -0.1102726 -0.1297368
moist
0.0023346 -0.0240196 -0.0314682
> model28 _ glm(response~moss+temp+moist,data=data2,family=
binomial(link=logit))
> summary(model28)
Call: glm(formula = response ~ moss + temp + moist, family =
binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.555659 -0.9738612 -0.6638551
1.079298
1.748465
Coefficients:
Value Std. Error
t value
(Intercept) -0.31974716
0.2161358 -1.4793808
moss -0.14699914
0.2183618 -0.6731908
temp
0.72619244
0.2309462
3.1444225
moist -0.05459733
0.2167654 -0.2518729
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
328
ANSWERS TO PROBLEMS

Residual Deviance: 124.3772 on 96 degrees of freedom
Number of Fisher Scoring Iterations: 3
Correlation of Coefficients:
(Intercept)
moss
temp
moss
0.0139125
temp -0.0983389
0.0868243
moist -0.0003751 -0.0499147 -0.0430762
> model29 _ glm(response~old.growth+temp,data=data2,family=
binomial(link=logit))
> summary(model29)
Call: glm(formula = response ~ old.growth + temp, family =
binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.949256 -0.250522 -0.02094977
0.09681103
2.275508
Coefficients:
Value Std. Error
t value
(Intercept) -0.7291304
0.4517408 -1.614046
old.growth
4.8729916
1.1152687
4.369343
temp
3.1216931
0.8002679
3.900810
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 36.7349 on 97 degrees of freedom
Number of Fisher Scoring Iterations: 6
Correlation of Coefficients:
(Intercept) old.growth
old.growth -0.1809192
temp -0.3234724
0.7954179
> model30 _ glm(response~rock+moist,data=data2,family=binomial
(link=logit))
> summary(model30)
Call: glm(formula = response ~ rock + moist, family = binomial
(link = logit), data = data2)
ANSWERS TO PROBLEMS
329

Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.098287 -1.069386 -0.9886188
1.290709
1.388836
Coefficients:
Value Std. Error
t value
(Intercept) -0.28211668
0.2021157 -1.3958176
rock -0.07089319
0.2069095 -0.3426290
moist -0.03597933
0.2034385 -0.1768561
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 136.5051 on 97 degrees of freedom
Number of Fisher Scoring Iterations: 2
Correlation of Coefficients:
(Intercept)
rock
rock
0.0085030
moist
0.0048325 -0.0607497
> model31 _ glm(response~rock+moss+temp+
moist,data=data2,family=binomial(link=logit))
> summary(model31)
Call: glm(formula = response ~ rock + moss + temp + moist, family
= binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.602848 -0.9767413 -0.6729573
1.092539
1.731579
Coefficients:
Value Std. Error
t value
(Intercept) -0.3225569
0.2169275 -1.4869338
rock -0.1734555
0.2249926 -0.7709385
moss -0.1633909
0.2201368 -0.7422243
temp
0.7446446
0.2326483
3.2007304
moist -0.0468392
0.2170510 -0.2157982
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 123.7693 on 95 degrees of freedom
330
ANSWERS TO PROBLEMS

Number of Fisher Scoring Iterations: 3
Correlation of Coefficients:
(Intercept)
rock
moss
temp
rock
0.0293166
moss
0.0174321
0.1009758
temp -0.1032482 -0.1282630
0.0670949
moist
0.0073141 -0.0337258 -0.0591562 -0.0411908
> model32 _ glm(response~aspect+species+old.growth+rock+moss+temp
+moist,data=data2,family=binomial(link=logit))
> summary(model32)
Call: glm(formula = response ~ aspect + species + old.growth +
rock + moss + temp + moist, family = binomial(link = logit), data
= data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.920072 -0.1728077 -0.01371914 0.09407092 2.285338
Coefficients:
Value Std. Error
t value
(Intercept) -0.9210613
0.5222776 -1.7635475
aspect -0.6858480
0.5868230 -1.1687476
species -0.2351692
0.4874347 -0.4824630
old.growth
5.2572678
1.3176968
3.9897400
rock -0.7270474
0.6207932 -1.1711587
moss
0.1372742
0.4664094
0.2943213
temp
3.3465016
0.9158027
3.6541732
moist
0.3283434
0.5127001
0.6404199
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 34.11214 on 92 degrees of freedom
Number of Fisher Scoring Iterations: 7
ANSWERS TO PROBLEMS
331

Correlation of Coefficients:
(Intercept)
aspect
species old.growth
rock
moss
temp
aspect
0.3120590
species -0.0601317
0.3005190
old.growth -0.3098927 -0.3738107 -0.0983157
rock
0.3635868
0.3579570 -0.1410673 -0.3311216
moss -0.0417944 -0.1844661
0.0559806
0.0685948 -0.1298313
temp -0.4361095 -0.2503376
0.0753317
0.8131769 -0.3834336
0.0947506
moist -0.3174575 -0.4535135
0.0229843
0.2760200 -0.3411515 -0.0497759
0.2569112
> attributes(model21)
$names:
[1] “coefficients”
“residuals”
“fitted.values”
“effects”
“R”
[6] “rank”
“assign”
“df.residual”
“weights”
“family”
[11] “linear.predictors” “deviance”
“null.deviance”
“call”
“iter”
[16] “y”
“contrasts”
“terms”
“formula”
“control”
$class:
[1] “glm” “lm”
> aic21 _ model21$deviance+2*model21$rank
> aic22 _ model22$deviance+2*model22$rank
> aic23 _ model23$deviance+2*model23$rank
> aic24 _ model24$deviance+2*model24$rank
> aic25 _ model25$deviance+2*model25$rank
> aic26 _ model26$deviance+2*model26$rank
> aic27 _ model27$deviance+2*model27$rank
> aic28 _ model28$deviance+2*model28$rank
> aic29 _ model29$deviance+2*model29$rank
> aic30 _ model30$deviance+2*model30$rank
> aic31 _ model31$deviance+2*model31$rank
> aic32 _ model32$deviance+2*model32$rank
> c(aic21,aic22,aic23,aic24,aic25,aic26,aic27,aic28,aic29,aic30,
aic31,aic32)
[1]
81.63520
[2] 143.63784
[3] 142.82631
[4] 132.69362
[5] 143.81594
[6]
44.31113
[7] 132.32165
[8] 132.37720
[9]
42.73490
[10] 142.50506
332
ANSWERS TO PROBLEMS

[11] 133.76932
[12]
50.11214
> aicc21 _ aic21+2*model21$rank*(model21$rank+1)/
(length(model21$residuals)-model21$rank-1)
> aicc22 _ aic22+2*model22$rank*(model22$rank+1)/
(length(model22$residuals)-model22$rank-1)
> aicc23 _ aic23+2*model23$rank*(model23$rank+1)/
(length(model23$residuals)-model23$rank-1)
> aicc24 _ aic24+2*model24$rank*(model24$rank+1)/
(length(model24$residuals)-model24$rank-1)
> aicc25 _ aic25+2*model25$rank*(model25$rank+1)/
(length(model25$residuals)-model25$rank-1)
> aicc26 _ aic26+2*model26$rank*(model26$rank+1)/
(length(model26$residuals)-model26$rank-1)
> aicc27 _ aic27+2*model27$rank*(model27$rank+1)/
(length(model27$residuals)-model27$rank-1)
> aicc28 _ aic28+2*model28$rank*(model28$rank+1)/
(length(model28$residuals)-model28$rank-1)
> aicc29 _ aic29+2*model29$rank*(model29$rank+1)/
(length(model29$residuals)-model29$rank-1)
> aicc30 _ aic30+2*model30$rank*(model30$rank+1)/
(length(model30$residuals)-model30$rank-1)
> aicc31 _ aic31+2*model31$rank*(model31$rank+1)/
(length(model31$residuals)-model31$rank-1)
> aicc32 _ aic32+2*model32$rank*(model32$rank+1)/
(length(model32$residuals)-model32$rank-1)
> c(aicc21,aicc22,aicc23,aicc24,aicc25,
aicc26,aicc27,aicc28,aicc29,aicc30,aicc31,aicc32)
[1]
82.05625
[2] 144.05889
[3] 143.24736
[4] 133.11467
[5] 144.23699
[6]
44.73218
[7] 132.74270
[8] 132.79825
[9]
42.98490
[10] 142.75506
[11] 134.40762
[12]
51.69456
> # Model 9, with old.growth and temp, is the best-ﬁtting model, with statistics
that are compatible with the “reality,” based on an analysis strategy of a priori parsi-
monious model selection and inference using AIC.
ANSWERS TO PROBLEMS
333

Problem 6.2
> covariates
[1] “aspect”
[2] “species”
[3] “old.growth”
[4] “rock”
[5] “moss”
[6] “temp”
[7] “moist”
[8] “aspect+species”
[9] “aspect+old.growth”
[10] “aspect+rock”
[11] “aspect+moss”
[12] “aspect+temp”
[13] “aspect+moist”
[14] “species+old.growth”
[15] “species+rock”
[16] “species+moss”
[17] “species+temp”
[18] “species+moist”
[19] “old.growth+rock”
[20] “old.growth+moss”
[21] “old.growth+temp”
[22] “old.growth+moist”
[23] “rock+moss”
[24] “rock+temp”
[25] “rock+moist”
[26] “moss+temp”
[27] “moss+moist”
[28] “temp+moist”
[29] “aspect+species+old.growth”
[30] “aspect+species+rock”
[31] “aspect+species+moss”
[32] “aspect+species+temp”
[33] “aspect+species+moist”
[34] “aspect+old.growth+rock”
[35] “aspect+old.growth+moss”
[36] “aspect+old.growth+temp”
[37] “aspect+old.growth+moist”
[38] “aspect+rock+moss”
[39] “aspect+rock+temp”
[40] “aspect+rock+moist”
334
ANSWERS TO PROBLEMS

[41] “aspect+moss+temp”
[42] “aspect+moss+moist”
[43] “aspect+temp+moist”
[44] “species+old.growth+rock”
[45] “species+old.growth+moss”
[46] “species+old.growth+temp”
[47] “species+old.growth+moist”
[48] “species+rock+moss”
[49] “species+rock+temp”
[50] “species+rock+moist”
[51] “species+moss+temp”
[52] “species+moss+moist”
[53] “species+temp+moist”
[54] “old.growth+rock+moss”
[55] “old.growth+rock+temp”
[56] “old.growth+rock+moist”
[57] “old.growth+moss+temp”
[58] “old.growth+moss+moist”
[59] “old.growth+temp+moist”
[60] “rock+moss+temp”
[61] “rock+moss+moist”
[62] “rock+temp+moist”
[63] “moss+temp+moist”
[64] “aspect+species+old.growth+rock”
[65] “aspect+species+old.growth+moss”
[66] “aspect+species+old.growth+temp”
[67] “aspect+species+old.growth+moist”
[68] “aspect+species+rock+moss”
[69] “aspect+species+rock+temp”
[70] “aspect+species+rock+moist”
[71] “aspect+species+moss+temp”
[72] “aspect+species+moss+moist”
[73] “aspect+species+temp+moist”
[74] “aspect+old.growth+rock+moss”
[75] “aspect+old.growth+rock+temp”
[76] “aspect+old.growth+rock+moist”
[77] “aspect+old.growth+moss+temp”
[78] “aspect+old.growth+moss+moist”
[79] “aspect+old.growth+temp+moist”
[80] “aspect+rock+moss+temp”
[81] “aspect+rock+moss+moist”
[82] “aspect+rock+temp+moist”
[83] “aspect+moss+temp+moist”
[84] “species+old.growth+rock+moss”
ANSWERS TO PROBLEMS
335

[85] “species+old.growth+rock+temp”
[86] “species+old.growth+rock+moist”
[87] “species+old.growth+moss+temp”
[88] “species+old.growth+moss+moist”
[89] “species+old.growth+temp+moist”
[90] “species+rock+moss+temp”
[91] “species+rock+moss+moist”
[92] “species+rock+temp+moist”
[93] “species+moss+temp+moist”
[94] “old.growth+rock+moss+temp”
[95] “old.growth+rock+moss+moist”
[96] “old.growth+rock+temp+moist”
[97] “old.growth+moss+temp+moist”
[98] “rock+moss+temp+moist”
[99] “aspect+species+old.growth+rock+moss”
[100] “aspect+species+old.growth+rock+temp”
[101] “aspect+species+old.growth+rock+moist”
[102] “aspect+species+old.growth+moss+temp”
[103] “aspect+species+old.growth+moss+moist”
[104] “aspect+species+old.growth+temp+moist”
[105] “aspect+species+rock+moss+temp”
[106] “aspect+species+rock+moss+moist”
[107] “aspect+species+rock+temp+moist”
[108] “aspect+species+moss+temp+moist”
[109] “aspect+old.growth+rock+moss+temp”
[110] “aspect+old.growth+rock+moss+moist”
[111] “aspect+old.growth+rock+temp+moist”
[112] “aspect+old.growth+moss+temp+moist”
[113] “aspect+rock+moss+temp+moist”
[114] “species+old.growth+rock+moss+temp”
[115] “species+old.growth+rock+moss+moist”
[116] “species+old.growth+rock+temp+moist”
[117] “species+old.growth+moss+temp+moist”
[118] “species+rock+moss+temp+moist”
[119] “old.growth+rock+moss+temp+moist”
[120] “species+old.growth+rock+moss+temp+moist”
[121] “aspect+old.growth+rock+moss+temp+moist”
[122] “aspect+species+rock+moss+temp+moist”
[123] “aspect+species+old.growth+moss+temp+moist”
[124] “aspect+species+old.growth+rock+temp+moist”
[125] “aspect+species+old.growth+rock+moss+moist”
[126] “aspect+species+old.growth+rock+moss+temp”
[127] “aspect+species+old.growth+rock+moss+temp+moist”
336
ANSWERS TO PROBLEMS

> models <- list(rep(NA,length(covariates)))
> for (i in 1:length(covariates)) {models[[i]] <-
glm(paste(“response~”,covariates[[i]]),data=data2,family=
binomial(link=logit))}
> aic <- rep(NA,length(covariates))
> for (i in 1:length(covariates)) {aic[i] <- models[[i]]
$deviance+2*models[[i]]$rank}
> aic
[1] 139.85339 140.65531
79.51750 140.53631 139.48914 128.91362
140.62353 141.81597
80.46451 141.68293 140.87633 130.71150 141.85338
[14]
81.17793 142.52625 141.47155 130.90486 142.61660
81.40933
81.26398
42.73490
81.49832 141.29015 130.38997 142.50506 130.44062
[27] 141.47541 130.83112
81.63520 143.63784 142.82631 132.69362
143.81594
82.29365
82.31451
44.31113
82.27267 142.64205 132.15225
[40] 143.68207 132.28469 142.87395 132.67354
83.06473
82.88769
44.50219
83.16640 143.26811 132.37783 144.49579 132.43030 143.45855
[53] 132.82489
83.11493
43.73011
83.38248
44.72662
83.20725
44.73473 131.81585 143.28249 132.32165 132.37720 83.41078 83.46074
[66]
45.72469
83.43748 144.58208 134.12878 145.63625 134.26597
144.82273 134.65905
84.11005
44.94795
84.06180
46.23969
84.05198
[79]
46.22166 133.63155 144.63436 134.12704 134.25567
84.73434
45.67655
85.04619
46.49698
84.84144
46.49710 133.80105 145.26111
[92] 134.31255 134.36900
45.71764
85.03685
45.67334
46.72661
133.76932
85.20296
46.65120
85.15489
47.64900
85.17926
47.64217
[105] 135.60616 146.57197 136.10700 136.23973
46.80726
85.78260
46.45420
48.16111 135.61598
47.66746 86.66670
47.64091
48.48968
[118] 135.75705
47.66931
49.63741
48.34955 137.59319
49.57600
48.19900
86.83034
48.53391
50.11214
> min(aic)
[1] 42.7349
> for (i in 1:length(covariates)) {if (aic[i]==min(aic)) k <- i}
> k
[1] 21
> covariates[[k]]
[1] “old.growth+temp”
> summary(models[[k]])
Call: glm(formula = response ~ old.growth + temp, family =
binomial(link = logit), data = data2)
Deviance Residuals:
Min
1Q
Median
3Q
Max
-1.949256 -0.250522 -0.02094977 0.09681103 2.275508
ANSWERS TO PROBLEMS
337

Coefficients:
Value Std. Error
t value
(Intercept) -0.7291304
0.4517408 -1.614046
old.growth
4.8729916
1.1152687
4.369343
temp
3.1216931
0.8002679
3.900810
(Dispersion Parameter for Binomial family taken to be 1 )
Null Deviance: 136.663 on 99 degrees of freedom
Residual Deviance: 36.7349 on 97 degrees of freedom
Number of Fisher Scoring Iterations: 6
Correlation of Coefficients:
(Intercept) old.growth
old.growth -0.1809192
temp -0.3234724
0.7954179
> # Model 21, consisting of old.growth and temp, is the best-ﬁtting model, with
statistics that are compatible with the “reality,” based on an analysis strategy of a
posteriori model selection and inference.
Problem 6.3
Model #1
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <-beta0+beta1*aspect[i]+beta2*species[i]
+beta3*old.growth[i]
}
beta0 ~ dnorm(0,0.1)
beta1 ~ dnorm(0,0.1)
beta2 ~ dnorm(0,0.1)
beta3 ~ dnorm(0,0.1)
}
338
ANSWERS TO PROBLEMS

Model #2
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0+beta1*aspect[i]+beta2*species[i]
+beta4*rock[i]
}
beta0 ~ dnorm(0,0.000001)
beta1 ~ dnorm(0,0.000001)
beta2 ~ dnorm(0,0.000001)
beta4 ~ dnorm(0,0.000001)
}
ANSWERS TO PROBLEMS
339

Model #3
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0+beta1*aspect[i]+beta2*species[i]
+beta5*moss[i]
}
beta0 ~ dnorm(0,0.000001)
beta1 ~ dnorm(0,0.000001)
beta2 ~ dnorm(0,0.000001)
beta5 ~ dnorm(0,0.000001)
}
340
ANSWERS TO PROBLEMS

Model #4
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0+beta1*aspect[i]+beta2*species[i]
+beta6*temp[i]
}
beta0 ~ dnorm(0,0.000001)
beta1 ~ dnorm(0,0.000001)
beta2 ~ dnorm(0,0.000001)
beta6 ~ dnorm(0,0.000001)
}
ANSWERS TO PROBLEMS
341

Model #5
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0+beta1*aspect[i]+beta2*species[i]
+beta7*moist[i]
}
beta0 ~ dnorm(0,0.000001)
beta1 ~ dnorm(0,0.000001)
beta2 ~ dnorm(0,0.000001)
beta7 ~ dnorm(0,0.000001)
}
342
ANSWERS TO PROBLEMS

Model #6
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0+beta1*aspect[i]
+beta3*old.growth[i]
+beta6*temp[i]
}
beta0 ~ dnorm(0,0.000001)
beta1 ~ dnorm(0,0.000001)
beta3 ~ dnorm(0,0.000001)
beta6 ~ dnorm(0,0.000001)
}
ANSWERS TO PROBLEMS
343

Model #7
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0
+beta4*rock[i]
+beta6*temp[i] + beta7*moist[i]
}
beta0 ~ dnorm(0,0.000001)
beta4 ~ dnorm(0,0.000001)
beta6 ~ dnorm(0,0.000001)
beta7 ~ dnorm(0,0.000001)
}
344
ANSWERS TO PROBLEMS

Model #8
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0
+beta5*moss[i]
+beta6*temp[i] + beta7*moist[i]
}
beta0 ~ dnorm(0,0.000001)
beta5 ~ dnorm(0,0.000001)
beta6 ~ dnorm(0,0.000001)
beta7 ~ dnorm(0,0.000001)
}
ANSWERS TO PROBLEMS
345

Model #9
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0
+beta3*old.growth[i]
+beta6*temp[i]
}
beta0 ~ dnorm(0,0.000001)
beta3 ~ dnorm(0,0.000001)
beta6 ~ dnorm(0,0.000001)
}
346
ANSWERS TO PROBLEMS

Model #10
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0
+beta4*rock[i]
+ beta7*moist[i]
}
beta0 ~ dnorm(0,0.000001)
beta4 ~ dnorm(0,0.000001)
beta7 ~ dnorm(0,0.000001)
}
ANSWERS TO PROBLEMS
347

Model #11
model
{
for(i in 1:n)
{
response[i]
~ dbin(p[i],1)
logit(p[i])
<- beta0
+beta4*rock[i]+beta5*moss[i]
+beta6*temp[i] + beta7*moist[i]
}
beta0 ~ dnorm(0,0.000001)
beta4 ~ dnorm(0,0.000001)
beta5 ~ dnorm(0,0.000001)
beta6 ~ dnorm(0,0.000001)
beta7 ~ dnorm(0,0.000001)
}
348
ANSWERS TO PROBLEMS

Model #12
model
{
for(i in 1:n)
{
response[i] ~ dbin(p[i],1)
logit(p[i]) <- beta0+beta1*aspect[i]+beta2*species[i]
+beta3*old.growth[i]+beta4*rock[i]+beta5*moss[i]
+beta6*temp[i] +beta7*moist[i]
}
beta0 ~ dnorm(0,0.1)
beta1 ~ dnorm(0,0.1)
beta2 ~ dnorm(0,0.1)
beta3 ~ dnorm(0,0.1)
beta4 ~ dnorm(0,0.1)
beta5 ~ dnorm(0,0.1)
beta6 ~ dnorm(0,0.1)
beta7 ~ dnorm(0,0.1)
# Note: the priors must be very precise in order to have
#
convergence for this poor fitting model
ANSWERS TO PROBLEMS
349

# Note: change models by deleting appropriate terms
#
in the description of logit(p[i]) (above), priors for
#
beta (above),
#
data in the list (below), and starting values for beta (below)
}
Summary (see Fig. B6.9).
Conclusion: Model 9, with old.growth and temp, is the best-ﬁtting model, based
on an analysis strategy of a priori parsimonious model selection and inference
using DIC with Bayesian statistical analysis. The Bayesian results with DIC are
similar to the frequentist results with AIC in Problem 6.1.
Problem 6.4
Summary (see Fig. B6.10).
350
ANSWERS TO PROBLEMS

Figure B6.9. Answers to Problems 6.1–6.3, statistics for 12 models: AIC and Akaike weights; DIC and DIC weights.
351

Figure B6.10. Answer to Problem 6.4, statistics for 12 models: AIC and Akaike weights; DIC and DIC weights; model averaging
results.
352

Conclusion: The unconditional estimates of old.growth coefﬁcient (and stan-
dard error) are 5.6314 (1.2954) and of temp coefﬁcient (and standard error) are
3.5755 (0.9043). The shrinkage estimators are very similar since most of the insignif-
icant weights are very close to 0. These estimates are calculated using DIC weights
and means (and standard deviations) of the posteriors for old.growth and
temp. The results for AIC weights will be similar since the coefﬁcient estimates
and standard errors and AIC weights for the frequentist statistical analysis are very
similar to those for the Bayesian statistical analysis.
> # The importance of the covariates, using DIC weights, are:
> # aspect
> .2959+.0325
[1] 0.3284
> # species
> .0325
[1] 0.0325
> # old.growth
> .2959+.6716+.0325
[1] 1
> # rock
> .0325
[1] 0.0325
> # moss
> .0325
[1] 0.0325
> # temp
> .2959+.6716+.0325
> # moist
> .0325
[1] 0.0325
> # The results, using AIC weights, will be similar, since the AIC weights are similar
to the DIC weights.
ANSWERS TO PROBLEMS
353

Problem 6.5
The classiﬁcation results for the frequentist statistical analysis best-ﬁtting model 9 are
as follows:
function(output, data)
{
freq00 <- 0
freq01 <- 0
freq10 <- 0
freq11 <- 0
n <- dim(data)[1]
sensitivity <- rep(NA, 19)
specificity <- rep(NA, 19)
correct <- rep(NA, 19)
c <- rep(NA,19)
j <- 1
for(pcutoff in seq(0.05, 0.95, 0.05)) {
for(i in 1:n) {
if(data$response[i] == 0 & output$fitted.values[i] <= pcutoff)
freq00 <- freq00 + 1
if(data$response[i] == 0 & output$fitted.values[i] <= pcutoff)
freq01 <- freq01 + 1
if(data$response[i] == 1 & output$fitted.values[i] <= pcutoff)
freq10 <- freq10 + 1
if(data$response[i] == 1 & output$fitted.values[i] <= pcutoff)
freq11 <- freq11 + 1
}
sensitivity[j] <- freq11/(freq10 + freq11)
specificity[j] <- freq00/(freq00 + freq01)
correct[j] <- (freq00 + freq11)/(freq00 + freq01 +freq10 + freq11)
c <- sensitivity/max(1-specificity,0.00001)
j <- j + 1
}
prob.cutoff <- seq(0.05, 0.95, 0.05)
results <- data.frame(prob.cutoff, sensitivity, specificity,
correct, c)
return(results)
}
354
ANSWERS TO PROBLEMS

> classification(model9,data2)
prob.cutoff sensitivity specificity
correct
c
1
0.05
1.0000000
0.6491228 0.8000000 2.850000
2
0.10
0.9883721
0.6929825 0.8200000 2.816860
3
0.15
0.9844961
0.7426901 0.8466667 2.805814
4
0.20
0.9767442
0.7675439 0.8575000 2.783721
5
0.25
0.9720930
0.7859649 0.8660000 2.770465
6
0.30
0.9651163
0.8011696 0.8716667 2.750581
7
0.35
0.9568106
0.8145363 0.8757143 2.726910
8
0.40
0.9476744
0.8267544 0.8787500 2.700872
9
0.45
0.9405685
0.8382066 0.8822222 2.680620
10
0.50
0.9348837
0.8473684 0.8850000 2.664419
11
0.55
0.9302326
0.8548644 0.8872727 2.651163
12
0.60
0.9244186
0.8625731 0.8891667 2.634593
13
0.65
0.9194991
0.8690958 0.8907692 2.620572
14
0.70
0.9152824
0.8759398 0.8928571 2.608555
15
0.75
0.9116279
0.8830409 0.8953333 2.598140
16
0.80
0.9084302
0.8892544 0.8975000 2.589026
17
0.85
0.8974008
0.8947368 0.8958824 2.557592
18
0.90
0.8875969
0.9005848 0.8950000 2.529651
19
0.95
0.8763770
0.9058172 0.8931579 2.497674
The optimal cutoff point is 0.80, with correct classiﬁcation 0.8975, sensitivity
0.9084, and speciﬁcity 0.8893.
Problem 7.1
> Orthodont
Grouped Data: distance ~ age | Subject
distance age
Subject
Sex
1
26.0
8
M01
Male
2
25.0
10
M01
Male
3
29.0
12
M01
Male
4
31.0
14
M01
Male
5
21.5
8
M02
Male
6
22.5
10
M02
Male
7
23.0
12
M02
Male
8
26.5
14
M02
Male
9
23.0
8
M03
Male
10
22.5
10
M03
Male
11
24.0
12
M03
Male
12
27.5
14
M03
Male
13
25.5
8
M04
Male
14
27.5
10
M04
Male
15
26.5
12
M04
Male
ANSWERS TO PROBLEMS
355

distance age
Subject
Sex
16
27.0
14
M04
Male
17
20.0
8
M05
Male
18
23.5
10
M05
Male
19
22.5
12
M05
Male
20
26.0
14
M05
Male
21
24.5
8
M06
Male
22
25.5
10
M06
Male
23
27.0
12
M06
Male
24
28.5
14
M06
Male
25
22.0
8
M07
Male
26
22.0
10
M07
Male
27
24.5
12
M07
Male
28
26.5
14
M07
Male
29
24.0
8
M08
Male
30
21.5
10
M08
Male
31
24.5
12
M08
Male
32
25.5
14
M08
Male
33
23.0
8
M09
Male
34
20.5
10
M09
Male
35
31.0
12
M09
Male
36
26.0
14
M09
Male
37
27.5
8
M10
Male
38
28.0
10
M10
Male
39
31.0
12
M10
Male
40
31.5
14
M10
Male
41
23.0
8
M11
Male
42
23.0
10
M11
Male
43
23.5
12
M11
Male
44
25.0
14
M11
Male
45
21.5
8
M12
Male
46
23.5
10
M12
Male
47
24.0
12
M12
Male
48
28.0
14
M12
Male
49
17.0
8
M13
Male
50
24.5
10
M13
Male
51
26.0
12
M13
Male
52
29.5
14
M13
Male
53
22.5
8
M14
Male
54
25.5
10
M14
Male
55
25.5
12
M14
Male
56
26.0
14
M14
Male
57
23.0
8
M15
Male
58
24.5
10
M15
Male
59
26.0
12
M15
Male
356
ANSWERS TO PROBLEMS

distance age
Subject
Sex
60
30.0
14
M15
Male
61
22.0
8
M16
Male
62
21.5
10
M16
Male
63
23.5
12
M16
Male
64
25.0
14
M16
Male
65
21.0
8
F01 Female
66
20.0
10
F01 Female
67
21.5
12
F01 Female
68
23.0
14
F01 Female
69
21.0
8
F02 Female
70
21.5
10
F02 Female
71
24.0
12
F02 Female
72
25.5
14
F02 Female
73
20.5
8
F03 Female
74
24.0
10
F03 Female
75
24.5
12
F03 Female
76
26.0
14
F03 Female
77
23.5
8
F04 Female
78
24.5
10
F04 Female
79
25.0
12
F04 Female
80
26.5
14
F04 Female
81
21.5
8
F05 Female
82
23.0
10
F05 Female
83
22.5
12
F05 Female
84
23.5
14
F05 Female
85
20.0
8
F06 Female
86
21.0
10
F06 Female
87
21.0
12
F06 Female
88
22.5
14
F06 Female
89
21.5
8
F07 Female
90
22.5
10
F07 Female
91
23.0
12
F07 Female
92
25.0
14
F07 Female
93
23.0
8
F08 Female
94
23.0
10
F08 Female
95
23.5
12
F08 Female
96
24.0
14
F08 Female
97
20.0
8
F09 Female
98
21.0
10
F09 Female
99
22.0
12
F09 Female
100
21.5
14
F09 Female
101
16.5
8
F10 Female
102
19.0
10
F10 Female
103
19.0
12
F10 Female
ANSWERS TO PROBLEMS
357

distance age
Subject
Sex
104
19.5
14
F10 Female
105
24.5
8
F11 Female
106
25.0
10
F11 Female
107
28.0
12
F11 Female
108
28.0
14
F11 Female
names(Orthodont)
[1] “distance” “age” “Subject” “Sex”
> out1 <- gls(distance~1,Orthodont)
> out2 <- gls(distance~age,Orthodont)
> out2a <- gls(distance~age-1,Orthodont)
> out3 <- gls(distance~age+age^2,Orthodont)
> out3a <- gls(distance~age+age^2-1,Orthodont)
> out3b <- gls(distance~age^2-1-age,Orthodont)
> AIC(out1,out2,out2a,out3,out3a,out3b)
df
AIC
out1
2 542.2815
out2
3 515.1695
out2a
2 624.2148
out3
4 520.6967
out3a
3 532.0595
out3b
2 762.6865
> summary(out1)
Generalized least squares fit by REML
Model: distance ~ 1
Data: Orthodont
AIC
BIC
logLik
542.2815
547.6272
-269.1408
Coefficients:
Value Std.Error
t-value p-value
(Intercept)
24.02315 0.2818024 85.24819
<.0001
Standardized residuals:
Min
Q1
Med
Q3
Max
-2.568875 -0.6908298 -0.09326993 0.6750213 2.553067
Residual standard error: 2.928577
Degrees of freedom: 108 total; 107 residual
> summary(out2)
Generalized least squares fit by REML
Model: distance ~ age
Data: Orthodont
AIC
BIC
logLik
515.1695 523.1598 -254.5847
358
ANSWERS TO PROBLEMS

Coefficients:
Value Std.Error
t-value p-value
(Intercept)
16.76111
1.225560 13.67629
<.0001
age
0.66019
0.109182
6.04667
<.0001
Correlation:
(Intr)
age -0.98
Standardized residuals:
Min
Q1
Med
Q3
Max
-2.563389
-0.62187
-0.07225954
0.5328229
2.48967
Residual standard error: 2.537151
Degrees of freedom: 108 total; 106 residual
> summary(out2a)
Generalized least squares fit by REML
Model: distance ~ age - 1
Data: Orthodont
AIC
BIC
logLik
624.2148
629.5605
-310.1074
Coefficients:
Value
Std.Error
t-value p-value
age 2.123457 0.03599327 58.99595
<.0001
Standardized residuals:
Min
Q1
Med
Q3
Max
-2.436066 -0.546902 0.09409067 0.9115034 2.503694
Residual standard error: 4.198734
Degrees of freedom: 108 total; 107 residual
> summary(out3)
Generalized least squares fit by REML
Model: distance ~ age + age^2
Data: Orthodont
AIC
BIC
logLik
520.6967
531.3125
-256.3483
Coefficients:
Value Std.Error
t-value p-value
(Intercept) 20.11759
7.211753
2.789557
0.0063
age
0.02361
1.352152
0.017462
0.9861
I(age^2)
0.02894
0.061259
0.472340
0.6377
ANSWERS TO PROBLEMS
359

Correlation:
(Intr)
age
age -0.996
I(age^2)
0.985 -0.997
Standardized residuals:
Min
Q1
Med
Q3
Max
-2.599429 -0.6155872 -0.0545411 0.547229 2.52598
Residual standard error: 2.5465
Degrees of freedom: 108 total; 105 residual
> summary(out3a)
Generalized least squares fit by REML
Model: distance ~ age + age^2 - 1
Data: Orthodont
AIC
BIC
logLik
532.0595 540.0498 -263.0297
Coefficients:
Value
Std.Error
t-value
p-value
age
3.779112
0.1299307
29.08559
<.0001
I(age^2)
-0.139447
0.0107778
-12.93837
<.0001
Correlation:
age
I(age^2) -0.985
Standardized residuals:
Min
Q1
Med
Q3
Max
-2.386645 -0.6183548 0.01477009 0.6331542 2.357222
Residual standard error: 2.626696
Degrees of freedom: 108 total; 106 residual
> summary(out3b)
Generalized least squares fit by REML
Model: distance ~ age^2 - 1 - age
Data: Orthodont
AIC
BIC
logLik
762.6865 768.0321 -379.3432
Coefficients:
Value
Std.Error
t-value
p-value
I(age^2) 0.1692879 0.005571219
30.38615
<.0001
360
ANSWERS TO PROBLEMS

Standardized residuals:
Min
Q1
Med
Q3
Max
-1.746102 -0.3769319 0.4909963
1.112952
2.127112
Residual standard error: 7.834838
Degrees of freedom: 108 total; 107 residual
> # Model 2 is best-ﬁtting, with distance ~ age.
Problem 7.2
> out4a <- lme(distance~age,Orthodont,random=~1|Sex)
> out4b <- lme(distance~age,Orthodont,random=~age|Sex)
> out4c <- lme(distance~age,Orthodont,random=~age-1|Sex)
> out5a <- lme(distance~age,Orthodont,random=~1|Subject)
> out5b <- lme(distance~age,Orthodont,random=~age|Subject)
> out5c <- lme(distance~age,Orthodont,random=~age-1|Subject)
> out6a <- lme(distance~age,Orthodont,random=~1|Sex/Subject)
> out6b <- lme(distance~age,Orthodont,random=~age|Sex/Subject)
> out6c <- lme(distance~age,Orthodont,random=~age-1|Sex/Subject)
> AIC(out4a,out4b,out4c,out5a,out5b,out5c,out6a,out6b,out6c)
df
AIC
out4a
4 497.0458
out4b
6 498.7545
out4c
4 494.9654
out5a
4 455.0025
out5b
6 454.6367
out5c
4 453.0857
out6a
5 452.0344
out6b
9 453.2491
out6c
5 449.5305
> summary(out4a)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
497.0458 507.6995 -244.5229
Random effects:
Formula:
~ 1 | Sex
(Intercept) Residual
StdDev:
1.61078 2.271713
Fixed effects: distance ~ age
Value Std.Error
DF
t-value p-value
(Intercept) 16.55410
1.582118 105
10.46325
<.0001
age
0.66019
0.097759 105
6.75319
<.0001
ANSWERS TO PROBLEMS
361

Correlation:
(Intr)
age -0.68
Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-2.620689 -0.6398049 -0.01048698
0.5397587 2.379617
Number of Observations: 108
Number of Groups: 2
> intervals(out4a)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 13.4170490 16.5540974
19.6911457
age
0.4663472
0.6601852
0.8540231
Random Effects:
Level: Sex
lower
est.
upper
sd((Intercept)) 0.3789406 1.61078
6.847011
Within-group standard error:
lower
est.
upper
1.984329 2.271713 2.600718
> summary(out4b)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
498.7545 514.7351 -243.3772
Random effects:
Formula: ~ age | Sex
Structure: General positive-definite
StdDev
Corr
(Intercept) 0.7178830 (Inter
age 0.2119552 -1
Residual 2.2461781
Fixed effects: distance ~ age
Value Std.Error
DF
t-value
p-value
(Intercept) 16.85355
1.198003
105
14.06804
<.0001
age
0.63289
0.178413
105
3.54734
0.0006
Correlation:
(Intr)
age -0.837
362
ANSWERS TO PROBLEMS

Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-2.49162 -0.5691045 -0.08825623
0.5978222
2.351662
Number of Observations: 108
Number of Groups: 2
> intervals(out4b)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 14.4781327 16.8535517
19.2289707
age
0.2791308
0.6328906
0.9866505
Random Effects:
Level: Sex
lower
est.
upper
sd((Intercept))
0.002676612
0.7178830 192.540401
sd(age)
0.022181098
0.2119552
2.025374
cor((Intercept),age) -1.000000000
-0.9999710
NA
Within-group standard error:
lower
est.
upper
1.962026 2.246178 2.571483
> summary(out4c)
Linear mixed-effects model fit by REML
> Data: Orthodont
AIC
BIC
logLik
494.9654 505.6192 -243.4827
Random effects:
Formula:
~ age - 1
| Sex
age
Residual
StdDev: 0.1492784
2.248511
Fixed effects: distance ~ age
Value Std.Error
DF
t-value
p-value
(Intercept) 16.76111
1.086134
105 15.43190
<.0001
age
0.64097
0.143239
105
4.47481
<.0001
Correlation:
(Intr)
age -0.662
ANSWERS TO PROBLEMS
363

Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-2.543496 -0.5491384 -0.0857501
0.5744827 2.357976
Number of Observations: 108
Number of Groups: 2
> intervals(out4c)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 14.6075078 16.7611111 18.914714
age
0.3569506
0.6409668
0.924983
Random Effects:
Level: Sex
lower
est.
upper
sd(age) 0.0355699 0.1492784 0.6264859
Within-group standard error:
lower
est.
upper
1.964065 2.248511 2.574153
> summary(out5a)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
455.0025 465.6563 -223.5013
Random effects:
Formula:
~ 1 | Subject
(Intercept) Residual
StdDev:
2.114724 1.431592
Fixed effects: distance ~ age
Value Std.Error DF
t-value
p-value
(Intercept) 16.76111 0.8023952 80
20.88885
<.0001
age
0.66019 0.0616059 80
10.71626
<.0001
Correlation:
(Intr)
age -0.845
Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-3.664539 -0.5350798 -0.01289591
0.4874286
3.721785
Number of Observations: 108
364
ANSWERS TO PROBLEMS

Number of Groups: 27
> intervals(out5a)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 15.1642937 16.7611111 18.3579285
age
0.5375855
0.6601852
0.7827849
Random Effects:
Level: Subject
lower
est.
upper
sd((Intercept)) 1.561205 2.114724 2.86449
Within-group standard error:
lower
est.
upper
1.226093 1.431592 1.671535
> summary(out5b)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
454.6367 470.6173 -221.3183
Random effects:
Formula:
~ age | Subject
Structure: General positive-definite
StdDev
Corr
(Intercept) 2.327037 (Inter
age 0.226427 -0.609
Residual 1.310040
Fixed effects: distance ~ age
Value Std.Error DF
t-value
p-value
(Intercept) 16.76111 0.7752464 80 21.62037
<.0001
age
0.66019 0.0712532 80
9.26534
<.0001
Correlation:
(Intr)
age -0.848
Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-3.223106 -0.4937604 0.007316863 0.472151 3.916033
Number of Observations: 108
Number of Groups: 27
ANSWERS TO PROBLEMS
365

> intervals(out5b)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 15.2183215 16.7611111 18.3039007
age
0.5183868
0.6601852
0.8019835
Random Effects:
Level: Subject
lower
est.
upper
sd((Intercept))
0.9503642
2.3270368 5.6979214
sd(age)
0.1026589
0.2264270 0.4994129
cor((Intercept),age) -0.9379522 -0.6093317 0.2959052
Within-group standard error:
lower
est.
upper
1.084904 1.31004 1.581896
> summary(out5c)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
453.0857 463.7394 -222.5428
Random effects:
Formula:
~ age - 1 | Subject
age Residual
StdDev: 0.1895482 1.412641
Fixed effects: distance ~ age
Value Std.Error DF
t-value p-value
(Intercept) 16.76111 0.6823703 80 24.56307
<.0001
age
0.66019 0.0708954 80
9.31210
<.0001
Correlation:
(Intr)
age -0.84
Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-3.93173 -0.4520521 0.002636414 0.4834305 3.639918
Number of Observations: 108
Number of Groups: 27
> intervals(out5c)
Approximate 95% confidence intervals
366
ANSWERS TO PROBLEMS

Fixed effects:
lower
est.
upper
(Intercept) 15.4031509 16.7611111 18.1190713
age
0.5190989
0.6601852
0.8012715
Random Effects:
Level: Subject
lower
est.
upper
sd(age) 0.1401078 0.1895482 0.2564348
Within-group standard error:
lower
est.
upper
1.209871 1.412641 1.649393
> summary(out6a)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
452.0344 465.3516 -221.0172
Random effects:
Formula:
~ 1 | Sex
(Intercept)
StdDev:
1.55039
Formula:
~ 1 | Subject %in% Sex
(Intercept) Residual
StdDev:
1.807425 1.431592
Fixed effects: distance ~ age
Value Std.Error DF
t-value p-value
(Intercept) 16.56933
1.343685 80 12.33126
<.0001
age
0.66019
0.061606 80 10.71626
<.0001
Correlation:
(Intr)
age -0.504
Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-3.739259 -0.5466211 -0.01599553 0.4519955 3.667103
Number of Observations: 108
Number of Groups:
Sex Subject %in% Sex
2
27
ANSWERS TO PROBLEMS
367

> intervals(out6a)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 13.8953121 16.5693297 19.2433473
age
0.5375855
0.6601852
0.7827849
Random Effects:
Level: Sex
lower
est.
upper
sd((Intercept)) 0.3261313 1.55039 7.370373
Level: Subject
lower
est.
upper
sd((Intercept)) 1.310314 1.807425 2.49313
Within-group standard error:
lower
est.
upper
1.226087 1.431592 1.671542
> summary(out6b)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
453.2491 477.22 -217.6245
Random effects:
Formula:
~ age | Sex
Structure: General positive-definite
StdDev
Corr
(Intercept) 0.6998300 (Inter
age 0.2070146 -1
Formula:
~ age | Subject %in% Sex
Structure: General positive-definite
StdDev
Corr
(Intercept) 2.2754327 (Inter
age 0.1726564 -0.634
Residual 1.3099227
Fixed effects: distance ~ age
Value Std.Error DF
t-value p-value
(Intercept) 16.84921 0.9152379 80 18.40965
<.0001
age
0.63412 0.1605152 80
3.95054
0.0002
Correlation:
(Intr)
age -0.795
368
ANSWERS TO PROBLEMS

Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-3.247803 -0.4087215 0.008375599 0.432053 3.861081
Number of Observations: 108
Number of Groups:
Sex Subject %in% Sex
2
27
> intervals(out6b)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 15.0278323 16.8492139 18.6705955
age
0.3146863
0.6341218
0.9535573
Random Effects:
Level: Sex
lower
est.
upper
sd((Intercept))
0.02486172
0.6998300 19.699442
sd(age)
0.04045817
0.2070146
1.059243
cor((Intercept),age) -1.00000000 -0.9999519
1.000000
Level: Subject
lower
est.
upper
sd((Intercept))
0.90309418
2.2754327 5.7331717
sd(age)
0.05280769
0.1726564 0.5645057
cor((Intercept),age) -0.96232650 -0.6337161 0.4472527
Within-group standard error:
lower
est.
upper
1.084813 1.309923 1.581745
> summary(out6c)
Linear mixed-effects model fit by REML
Data: Orthodont
AIC
BIC
logLik
449.5305 462.8477 -219.7652
Random effects:
Formula:
~ age - 1 | Sex
age
StdDev: 0.1442001
Formula:
~ age - 1 | Subject %in% Sex
age Residual
StdDev: 0.1596937 1.412641
ANSWERS TO PROBLEMS
369

Fixed effects: distance ~ age
Value Std.Error DF
t-value p-value
(Intercept) 16.76111 0.6823703 80 24.56307
<.0001
age
0.64225 0.1227672 80
5.23147
<.0001
Correlation:
(Intr)
age -0.485
Standardized Within-Group Residuals:
Min
Q1
Med
Q3
Max
-3.977661 -0.4036487 -0.02843643 0.503207 3.582271
Number of Observations: 108
Number of Groups:
Sex Subject %in% Sex
2
27
> intervals(out6c)
Approximate 95% confidence intervals
Fixed effects:
lower
est.
upper
(Intercept) 15.4031509 16.7611111 18.1190713
age
0.3979377
0.6422521
0.8865666
Random Effects:
Level: Sex
lower
est.
upper
sd(age) 0.03101527 0.1442001 0.670433
Level: Subject
lower
est.
upper
sd(age) 0.1158322 0.1596937 0.220164
Within-group standard error:
lower
est.
upper
1.209877 1.412641 1.649386
Conclusion: Model 6c is best-ﬁtting, with distance  age, random ¼ age-1jSex/
Subject.
Answers to Problem 7.2 are summarized in Fig. B7.16.
370
ANSWERS TO PROBLEMS

Figure B.7.16. Answer to Problem 7.2: comparative frequentist statistics for linear
regression and mixed-effects models.
ANSWERS TO PROBLEMS
371

Problem 7.3
> plot(Orthodont)
372
ANSWERS TO PROBLEMS

> plot(augPred(out6c))
Yes, the graphs indicate random effects with respect to both Sex and Subject
within Sex.
ANSWERS TO PROBLEMS
373

Problem 7.4
See Problems 7.1 and 7.2 (above) for model speciﬁcations. Answers to Problems 7.4
and 7.5 are summarized in Fig. B7.17.
374
ANSWERS TO PROBLEMS

ANSWERS TO PROBLEMS
375

376
ANSWERS TO PROBLEMS

Problem 7.5
ANSWERS TO PROBLEMS
377

378
ANSWERS TO PROBLEMS

ANSWERS TO PROBLEMS
379

380
ANSWERS TO PROBLEMS

Conclusion: Model 6b is best-ﬁtting, with distance  age, random ¼ agejSex/
Subject. These results differ slightly from the frequentist results that favored
model 6c. (See Figure B.7.17.)
Problem 7.6
Yes.
ANSWERS TO PROBLEMS
381

Figure B.7.17. Answers to Problems 7.4 and 7.5: comparative Bayesian statistics for linear
regression and mixed-effects models.
382

REFERENCES
Akaike, H. 1973. Information theory as an extension of the maximum likelihood principle. In
B. N. Petrov and F. Csaki, editors, Proceedings of 2nd International Symposium on
Information Theory, Akademiai Kiado, Budapest, pp. 267–281.
Akaike, H. 1974. A new look at the statistical model identiﬁcation. IEEE Transactions on
Automatic Control AC 19:716–723.
Anderson, D. R., K. P. Burnham, and W. L. Thompson. 2002. Null hypothesis testing:
Problems, prevalence, and an alternative. Journal of Wildlife Management 64:912–923.
Anderson, D. R., W. A. Link, D. H. Johnson, and K. P. Burnham. 2001. Suggestions for pre-
senting the results of data analysis. Journal of Wildlife Management 65(3):373–378.
Bayes, T. 1763. An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society of London 53:370–418.
Berger, J. O. 1985. Statistical Decision Theory and Bayesian Analysis, 2nd edition. Springer-
Verlag, New York.
Berry, D. A., and D. K. Strangl, editors. 1996. Bayesian Biostatistics. Marcel Dekker,
New York.
Bremaud, P. 1999. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues.
Springer-Verlag, New York.
Burnham, K. P., and D. R. Anderson. 1998. Model Selection and Inference. Springer-Verlag,
New York.
Burnham, K. P., and D. R. Anderson. 2002. Model Selection and Multimodel Inference, 2nd
edition. Springer-Verlag, New York.
Carlin, B. P., and T. A. Louis. 2000. Bayes and Empirical Bayes Methods for Data Analysis,
2nd edition. Chapman & Hall, London.
Casella, G., and E. I. George. 1992. Explaining the Gibbs sampler. The American Statistician
46:167–174.
Cochran, W. G. 1977. Sampling Techniques, 3rd edition. Wiley, New York.
Cohen, J. 1988. Statistical Power Analysis for the Behavioral Sciences, 2nd edition. Lawrence
Erlbaum Associates, Publishers, Hillsdale, NJ.
Congdon, P. 2001. Bayesian Statistical Modeling. Wiley, New York.
Conover, W. J. 1980. Practical Nonparametric Statistics, 2nd edition. Wiley, New York.
Cook, R. D. 1998. Regression Graphics. Wiley, New York.
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.
383

Cook, R. D., and S. Weisberg. 1999. Applied Regression Including Computing and Graphics.
Wiley, New York.
Daniel, W. W. 1990. Applied Nonparametric Statistics, 2nd edition. PWS-Kent Publishing
Company, Boston.
Dobson, A. J. 1990. An Introduction to Generalized Linear Models. Chapman & Hall, London.
Dowdy, S., and S. Wearden. 1991. Statistics for Research, 2nd edition. Wiley, New York.
Draper, D. 2000. Bayesian Hierarchical Modeling. Unpublished Notes, University of Bath,
Bath, UK.
Draper, N., and H. Smith. 1981. Applied Regression Analysis, 2nd edition. Wiley, New York.
Efron, B. 1979. Bootstrap methods: Another look at the jackknife. The American Statistician
7:1–26.
Efron, B., and G. Gong. 1983. A leisurely look at the bootstrap, jackknife, and cross validation.
The American Statistician 37:36–48.
Efron, B., and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall,
London.
Fisher, R. A. 1922. On the mathematical foundations of theoretical statistics. Royal Society of
London Philosophical Transactions (Series A) 222:309–368.
Fisher, R. A. 1925a. Statistical Methods for Research Workers. 1st edition. Oliver and Boyd,
Edinburgh, Scotland.
Fisher, R. A. 1925b. Theory of statistical estimation. Proceedings of the Cambridge
Philosophical Society 22:700–725.
Fisher, R. A. 1934. Statistical Methods for Research Workers, 5th edition. Oliver and Boyd,
Edinburgh, Scotland.
Fisher, R. A. 1958. Statistical Methods for Research Workers, 13th edition. Hafner, New York.
Gamerman, D. 1997. Markov Chain Monte Carlo. Chapman & Hall, London.
Gelfand, A. E., and A. F. M. Smith. 1990. Sampling-based approaches to calculating marginal
densities. Journal of the American Statistical Association 85:398–409.
Gelman, A. 1992. Iterative and non-iterative simulation algorithms. Computing Science and
Statistics 24:433–438.
Geman, S., and D. Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence
6:721–741.
Gill, J. 2002. Bayesian Methods. Chapman & Hall, London.
Gregoire, T. G. 1998. Design-based and model-based inference in survey sampling:
Appreciating the difference. Canadian Journal of Forest Research 28:1429–1447.
GSþ: Geostatistics for the Environmental Sciences. 1998. Gamma Design Software,
Plainwell, MI.
Hansen M. H., and W. N. Hurwitz. 1943. On the theory of sampling from ﬁnite populations.
Annals of Mathematical Statistics 14:333–362.
Hardin, J., and J. Hilbe. 2001. Generalized Linear Models and Extensions. Stata Press, College
Station, TX.
Hastings, W. K. 1970. Monte Carlo sampling methods using Markov chains and their appli-
cations. Biometrika 57:97–109.
384
REFERENCES

Hicks, C. R. 1993. Fundamental Concepts in the Design of Experiments, 4th edition. Oxford
University Press, New York.
Hilborn, R., and M. Mangel. 1997. The Ecological Detective. Princeton University Press,
Princeton, NJ.
Hocking, R. R. 1996. Methods and Applications of Linear Models: Regression and the
Analysis of Variance. Wiley, New York.
Horvitz, D. G., and D. J. Thompson. 1952. A generalization of sampling without replacement
from a ﬁnite universe. Journal of the American Statistical Association 47:663–685.
Hosmer, D. W., and S. Lemeshow. 2000. Applied Logistic Regression, 2nd edition. Wiley,
New York.
Iversen, G. R. 1984. Bayesian Statistical Inference. Sage University Press, London.
Jeffreys, H. 1961. Theory of Probability, 3rd edition. Oxford University Press, Oxford, UK.
Johnson, D. H. 1999. The insigniﬁcance of statistical signiﬁcance testing. Journal of Wildlife
Management 63:763–772.
Johnson, D. H. 2002. The role of hypothesis testing in wildlife science. Journal of Wildlife
Management 66(2):272–276.
Krause, A., and M. Olson. 2000. The Basics of S and S-Plus, 2nd edition. Springer, New York.
Kuehl, R. O. 1994. Statistical Principles of Research Design and Analysis. Duxbury Press,
Belmont, CA.
Link, W. A., E. Cam, J. D. Nichols, and E. G. Cooch. 2002. Of BUGS and birds: Markov chain
Monte Carlo for hierarchical modeling in wildlife research. Journal of Wildlife
Management 66(2):277–291.
Manly, F. J., L. L. McDonald, and D. L. Thomas. 1995. Resource Selection by Animals.
Chapman & Hall, London.
Manly, F. J., L. L. McDonald, D. L. Thomas, T. L. McDonald, and W. P. Erickson. 2004.
Resource Selection by Animals, 2nd edition. Kluwer Academic Publishers, Norwell, MA.
Manly, B. F. J. 1994. Multivariate Statistical Methods: A Primer, 2nd edition. Chapman &
Hall, London.
Manly, B. F. J. 1997. Randomization, Bootstrap and Monte Carlo Methods in Biology, 2nd
edition. Chapman & Hall, London.
McCullagh, P., and J. A. Nelder. 1996. Generalized Linear Models, 2nd edition. Chapman &
Hall, New York.
McCulloch, C. E., and S. R. Searle. 2001. Generalized, Linear, and Mixed Models. Wiley,
New York.
Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. 1953.
Equations of state calculations by fast computing machine. Journal of Chemical Physics
21:1087–1091.
Nagelkerke, N. J. D. 1991. A note on a general deﬁnition of the coefﬁcient of determination.
Biometrika 78:691–692.
Nelder, J. A. and R. W. M. Wedderburn. 1972. Generalized linear models. Journal of the Royal
Statistical Society of America 135:370–384.
Neyman, J., and E. S. Pearson. 1928a. On the use and interpretation of certain test criteria for
purposes of statistical inference. Part I. Biometrika 20A:175–240.
Neyman, J., and E. S. Pearson. 1928b. On the use and interpretation of certain test criteria for
purposes of statistical inference. Part II. Biometrika 20A:263–294.
REFERENCES
385

Neyman, J., and E. S. Pearson. 1933. The testing of statistical hypothesis in relationship to
probabilities a priori. Proceedings of the Royal Statistical Society (Series A) 231:289–510.
Neyman, J., and E. S. Pearson. 1936. Contributions to the theory of testing statistical
hypotheses. Statistical Research Memorandum 1:1–37.
nQuery Advisor version 5.0. 2002. Statistical Solutions. Saugus, MA.
PASS. 2002. NCSS Statistical Software. Kaysville, Utah.
Peskun, P. H. 1973. Optimal Monte-Carlo sampling using Markov chains. Biometrika
60(3):607–612.
Pielou, E. C. 1969. An Introduction to Mathematical Ecology. Wiley-Interscience, New York.
Pinheiro, J. C., and D. M. Bates. 2000. Mixed-Effects Models in S and S-Plus. Springer-Verlag,
New York.
Potthoff, R. F., and S. N. Roy. 1964. A generalized multivariate analysis of variance model
useful especially for growth curve problems. Biometrika 51:313–326.
Ramsey, F. L., and D. W. Schafer. 2002. The Statistical Sleuth, 2nd edition. Duxbury Press,
Belmont, CA.
R Development Core Team. 2005. R 2.2.1: A Language and Environment for Statistical
Computing. R Foundation for Statistical Computing, Vienna, Austria (ISBN 3-900051-07-0,
URL http://www.R-project.org).
Rice, J. A. 1995. Mathematical Statistics and Data Analysis, 2nd edition. Duxbury Press,
Belmont, CA.
Robinson, D. H., and H. Wainer. 2002. On the past and future of null hypothesis signiﬁcance
testing. Journal of Wildlife Management 66(2):263–271.
Ryan, T. P. 1997. Modern Regression Methods. Wiley, New York.
Sarndal, C. E., B. Swensson, and J. Wretman. 1992. Model Assisted Survey Sampling.
Springer-Verlag, New York.
SAS Institute Inc. 1995. Logistic Regression Examples Using the SAS System, version 6. SAS
Institute Incorporated, Cary, NC.
Scheaffer, R. L., W. Mendenhall III, and R. L. Ott. 1996. Elementary Survey Sampling, 5th
edition. Duxbury Press, Belmont, CA.
Schwarz, G. 1978. Estimating the dimension of a model. Annals of Statistics 6:461–464.
Seber, G. A. F. 1977. Linear Regression Analysis. Wiley, New York.
Siegel, S., and N. J. Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral Sciences,
2nd edition. McGraw-Hill, New York.
Sokal, R. R., and F. J. Rohlf. 1995. Biometry, 3rd edition. Freeman, New York.
Spiegelhalter, D., A. Thomas, and N. Best. 2001. WinBUGS, version 1.4. MRC Biostatistics
Unit, Cambridge, UK.
S-Plus 2000, release 3. 2000. MathSoft Incorporated, Seattle, WA.
Stauffer, D. F. 1999. Linking populations and habitats: Where have we been? Where are we
going? Invited plenary paper presented at symposium, Predicting Species Occurrences:
Issues of Scale and Accuracy. Snowbird, UT, October 1999.
Stauffer, H. B. 1982a. Some sample size tables for forest sampling. Research Note 90. British
Columbia Ministry of Forests, Victoria, BC, Canada.
Stauffer, H. B. 1982b. A sample size table for forest sampling. Forest Science 28(4):777–784.
Thompson, S. K. 1992. Sampling. Wiley, New York.
386
REFERENCES

Thompson, S. K., and G. A. F. Seber. 1996. Adaptive Sampling. Wiley, New York.
Thompson, W. L., G. C. White, and C. Gowan. 1998. Monitoring Vertebrate Populations.
Academic Press, San Diego, CA.
Wackerly, D. D., W. Mendenhall III, and R. L. Scheaffer. 2002. Mathematical Statistics with
Applications, 6th edition. Duxbury Press, New York.
Winer, B. J., D. R. Brown, and K. M. Michels. 1991. Statistical Principles in Experimental
Design, 3rd edition. McGraw-Hill, New York.
Wolﬁnger, R. Fitting nonlinear and generalized mixed models with PROC LNMIXED. 2000.
200 Joint Statistical Meetings Continuing Education Series, Indianapolis.
Zar, J. H. 1996. Biostatistical Analysis, 3rd edition. Prentice-Hall, Upper Saddle River, NJ.
REFERENCES
387


INDEX
Abundance, see Mean, parameter
estimation of a discrete population,
3–4, 249
Actions, 88
Adaptive management, see Management,
adaptive
AIC, see Akaike’s information criterion
(AIC)
AICc, see Corrected Akaike’s information
criterion (AICc)
Akaike’s information
criterion (AIC), 12–13, 123, 128,
142, 149, 165–166, 177, 181, 182,
184, 186, 198, 199, 204, 213–220,
224, 227–230, 232, 233–237, 239,
241, 241 & 244, 252, 254, 270–271
corrected, see Corrected Akaike’s
information criterion
Akaike weights, 131, 142, 149, 165–166,
182, 186, 238–239, 241, 244
Analysis of Variance (ANOVA), 2, 52,
155, 157, 159, 191–194
generalization of, 194–205
Anisotropic correlation structure, 230,
232. See also Model, mixed-effects,
variance-covariance structures
within-groups random effects,
anisotropic correlation structure
A posteriori model selection and inference
strategy, see Model, selection and
inference strategy, a posteriori
A priori parsimonious model selection
and inference strategy, see Model,
selection and inference strategy,
a priori parsimonious
Autocorrelation, see Model, mixed-
effects, variance-covariance
structures within-groups random
effects, autocorrelation; WinBUGS,
autocorrelation
Autocorrelogram, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, autocorrelation
Backward elimination multiple linear
regression, 137, 272–273. See also
Stepwise selection
Bald Eagles, 77–78
Bayes factors, 82, 83, 83–88, 91–92,
174, 256
Bayesian
credible interval, see Credible interval
decision theory, 88–90, 92, 256
deviance information criterion, see
Deviance information criterion
(DIC)
ﬁt, 54–56, 56–61, 128–129
goodness of ﬁt, see Goodness of ﬁt
highest posterior density, see Highest
posterior density (HPD)
hypothesis testing, 8, 82, 255–256
information criterion, see Bayesian
information criterion (BIC)
interpretation, 10–11, 54–56, 110, 112
& 115, 239 & 241, 245, 249,
255–256
Markov chain Monte Carlo, see
Markov chain Monte Carlo
(MCMC)
model selection and inference,
146–150, 239 & 241, 255–256
parameter estimation, 255–256
posterior distribution, see Posterior
distribution
prior distribution, see Prior
distribution(s)
389
Contemporary Bayesian and Frequentist Statistical Research Methods for Natural
Resource Scientists. By Howard B. Stauffer
Copyright # 2008 John Wiley & Sons, Inc.

Bayesian (Continued)
statistical analysis and inference, 4,
10–11, 42, 48–49, 54–56, 88, 110,
123, 128, 131, 146–150, 151, 178–
179 & 184–185, 187, 239–241,
245, 248–249, 249, 249–250, 254–
255, 255–256
WinBUGS software, see WinBUGS
software
Bayesian information criterion (BIC), 12–
14, 198, 262, 271–272. See also
Multiple linear regression, Bayesian
information criterion (BIC)
Bayes
Reverend Thomas, 48, 62, 255
risk, 88–90, 92
rule, 88–90, 92
Theorem, 55, 61–63, 77, 85–86, 96
Beach layia, 16
Bernoulli trials, 27
Best linear unbiased estimators (BLUE),
52, 128, 262
Best subsets selection, 125, 127, 137 &
141, 145–146, 151, 167, 187,
253, 273–274. See also Model,
selection and inference strategy,
a posteriori
Beta distribution, see Prior distribution(s),
beta
Binomial distribution, see Model,
binomial
BUGS, 101
Canonical form, 158
Case studies, 2–5, 247–250
case study 1, 2–3, 5–9, 10–15, 40–42,
88, 91–92, 247–249
case study 2, 3–4, 8–9, 10–15, 42–43,
115 & 118, 249
case study 3, 4–5, 8–9, 9–10, 10–15,
42 & 44–45, 249–250
Central Limit Theorem, 24
Chi-square distribution, 21, 159,
163–164, 167–168, 183
CODA, 101, 105
Complementary log-log link function, see
Link function, complementary log-
log
Compounded error, 122–123, 125, 127,
150, 167, 253, 256
Concordance, see Logistic regression,
concordance
Conﬁdence
interval(s), 5–8, 34–35, 40–42, 50,
55, 110, 129, 162, 168, 198,
199–200, 202–203, 210–211,
213–219, 224, 230, 232,
236–237, 248, 249, 262–263,
275–276. See also Mean,
conﬁdence interval; Proportion,
conﬁdence interval
level, 6, 9, 35, 40–42, 50, 130, 253
Conjugacy, 57, 59–60, 61, 63–72,
75–77, 78–79, 83, 84, 90, 91–92,
93–94, 249
Corrected Akaike’s information criterion
(AICc), 12–13, 123, 127–128, 165–
166, 254, 270–271
Correlation
function, see Model, mixed-effects,
variance-covariance structures
within-groups random effects,
correlation function
statistic, Pearson sample, 260
Covariate pattern, see Logistic regression,
covariate pattern
Credible interval, 10–11, 55–56,
75–77, 78–79, 96, 110, 112 & 114,
129, 166
Cross-validation, see Goodness of ﬁt,
cross-validation
c statistic, see Logistic regression,
c statistic
Data dredging, 122, 124, 250, 252, 253.
See also Model, selection and
inference strategy, a posteriori
Datasets
arrival times, 63, 72
binary, 5, 6–7, 9–10, 60–61, 63,
69–72, 75–77, 78–79, 83, 84, 94,
118, 156, 159–176, 176–187,
187 & 190, 188–189, 255
bird count, 42–43, 58–60, 79, 91–92,
birds. density, 205–241
capture-recapture, see Datasets,
mark-recapture
categorical continguency, 176
continuous, 3–4, 56–58, 63, 64–66,
94, 102–103, 106–110
count, 42–43, 58–60, 63, 66–69, 79,
91–92, 94, 110–112, 115 & 118,
116–117, 156, 159,
175–176, 249, 255
data1 (Logistic regression),
176–186
390
INDEX

data1 (Multiple linear regression),
134–150
data2 (Logistic regression), 187–190
data2 (Multiple linear regression),
151–153
Del Norte salamander, see Del Norte
salamander
dependent, 14, 192–194, 250, 252
discrete, 3–4, 42–43, 249
experimental, 8–9, 49, 250–251
habitat selection modeling, 4–5,
42 & 45, 44, 134–150, 151–153,
176–186, 187–190, 249–250
hierarchical, 193
longitudinal, 193
Marbled Murrelet, see Marbled
Murrelet
mark-recapture, 63, 71, 97–98, 99
metapopulation, 193
normal, 116–117, 255. See also Mean,
model; Model, normal
Northern Spotted Owl, see Northern
Spotted Owl
observational, 9, 49, 250–251
occupancy versus non-occupancy, 5,
10, 75–77, 77–79, 83
Orthodont, 242–244, 241 & 244–245
Poisson, 115 & 118, 255. See also
Datasets, bird count; Datasets, count
presence or absence, 4, 5, 10, 69–71,
91, 249–250
pseudo-replicated, 193
repeated-measures, 193
Seedlings1, 194–200
Seedlings2, 198 & 200–205
spatially dependent, 193
temporally dependent, 193
test, see Goodness of ﬁt, with test
datasets
time-series, 193
tree diameter (dbh), 40–41, 56–58,
64–66, 73
Decision(s), 3, 5–9, 88–90, 91–92
protocol, 8
theory, 88–90, 91–92
Del Norte salamander, 4
Descriptive modeling, see Model,
selection and inference strategy, a
posteriori
Design-based inference, 156
Detection, probability of, 77–78
Deviance
statistic, 12. See also Logistic
regression, deviance statistic
test, see Goodness of ﬁt, deviance test
Deviance information criterion (DIC), 12,
105, 109–110, 112 & 114, 123,
127–128, 130, 146, 148, 149, 151,
184–185, 186, 187 & 190, 240–
241, 245, 252
DIC, see Deviance information criterion
(DIC)
DIC weights, 128, 146, 148, 149, 151,
186, 187 & 190, 241, 245
Dirichlet distribution, see Prior
distribution(s), Dirichlet
Discordance, see Logistic regression,
discordance
Discrete population, see Population,
discrete
Distance metrics, see Model, mixed-
effects, variance-covariance
structures within-groups random
effects, distance metrics
Distributions
beta, see Prior distribution(s), beta
binomial, see Model, binomial;
S-Plus software, distributions,
binomial
chi-square, see Chi-square distribution
Dirichlet, see Prior distribution(s),
Dirichlet
exponential, see Model, exponential
exponential family of, see Exponential
family of distributions
gamma, see Prior distribution(s),
gamma
multinomial, see Model, multinomial
negative binomial, see Model, negative
binomial
normal, see Model, normal; Prior
distribution(s), normal;
S-Plus software, distributions,
normal
Poisson, see Model, Poisson; S-Plus
software, distributions, Poisson
posterior, see Posterior distribution(s)
prior, see Prior distribution(s)
uniform, see Prior distribution(s),
uniform; S-Plus software,
distributions, uniform
Diversity index, see Ecological diversity
index
Doodle BUGS, 101
INDEX
391

Ecological diversity index, 3
Ecosystems, old-growth, 4, 9–10
Effect size, see Project Management,
planning phase, effect size
Empirical Bayes methods, 74
Entropy, 13, 165, 271
Error
compounded, see Compounded error
of commission, see Logistic regression,
error of commission
of omission, see Logistic regression,
error of omission
sampling, see Sampling error
standard, see Standard error
type I, see Type I error
type II, see Type II error
Estimate, 5
Estimator, 5–6
mean, see Mean, estimator
proportion, see Proportion, estimator
unbiased, see Unbiased estimator
Experiments, 8–9, 248–249, 252–253,
256
Exponential distribution, see Model,
exponential
Exponential family of distributions, 72,
156, 157–159
binomial distribution, see Model,
binomial
exponential distribution, see Model,
exponential
gamma distribution, see Model, gamma
multinomial distribution, see Model,
multinomial
negative binomial distribution, see
Model, negative binomial
normal distribution, see Model, normal
Poisson distribution, see Model,
Poisson
Factor, 195, 197, 198 & 200 & 202–204
False
negative, see Logistic regression, false
negative
positive, see Logistic regression, false
positive
Fisher
hypothesis testing, see Frequentist,
hypothesis testing
information matrix, 73
Fitness, 3
Fixed effects, see Model, ﬁxed-effects
Forward selection multiple linear
regression, 272–273
Frequentist
hypothesis testing, 8–9, 41–42,
49–50, 81–82, 163, 167–169,
248–249, 250–251, 252–253,
256
inference, 5–10, 40–45, 47, 49–50,
81–82, 110, 134–146, 159–175,
176–183 & 185, 187 & 190,
194–239, 241 & 244, 248–249,
249–250, 250–252, 254–255,
256, 259–276
statistical analysis, 5–10, 40–45, 110,
134–146, 159–175, 176–183 &
185, 187 & 190, 194–239, 241 &
244, 248–249, 249–250, 250–
252, 254–255, 256, 259–276
Gamma distribution, see also
Model, gamma; Prior distribution(s),
gamma
Gamma statistic g, see Logistic regression,
gamma statistic g
Gauss-Markov Theorem, 52
Generalized least squares (gls) ﬁt, 52, 214,
219–220, 244
Generalized linear modeling, 4, 9–10,
155–159, 175–176, 249–250,
254–255
GeoBUGS, 101
Gibbs sampling, 48, 96–98,
99, 118
GLM, see Generalized linear modeling
Goodness of ﬁt, 11, 129–131, 153,
167–175, 274–276. See also
Multiple linear regression, goodness
of ﬁt
classiﬁcation analysis, see Logistic
regression, classiﬁcation analysis
concordance, see Logistic regression,
concordance
correct classiﬁcation, see Logistic
regression, correct classiﬁcation
cross-validation, 130, 153, 169, 276
c statistic, see Logistic regression, c
statistic
deviance test, 169
discordance, see Logistic regression,
discordance
gamma statistic g, see Logistic
regression, gamma statistic g
Hosmer and Lemeshow test, see
Logistic regression, Hosmer and
Lemeshow test
non-parametric bootstrapping, 168
392
INDEX

parametric bootstrapping,
168–169
prediction intervals, see Multiple linear
regression, prediction intervals
proﬁle 51–52, 56–60, 262
residual analysis, 130, 153, 175,
274–275
sensitivity, see Logistic regression,
sensitivity
speciﬁcity, see Logistic regression,
speciﬁcity
Somer statistic D, see Logistic
regression, Somer statistic D
tau statistic t, see Logistic regression,
tau statistic t
with test datasets, 130, 169, 276
Grizzly bear, 4
Habitat conservation plan, 83
Habitat restoration plan, 91
Habitat selection modeling, 4–5, 9–10,
42 & 45, 134–150, 176–186,
187–190, 249–250
Hastings, W. K., 48, 94, 98 & 100–101,
255–256. See also Metropolis-
Hastings algorithm
Heteroscedasticity, 52,
274–275
Hierarchical structures, 93, 112, 193
Highest posterior density (HPD), 55
Homoscedasticity, 52, 205, 212, 259,
274–275
Hosmer and Lemeshow test, see Logistic
regression, Hosmer and Lemeshow
test
Hyperparameters, 74, 110
Hypothesis
scientiﬁc versus statistical, 49, 251, 256
silly null, 49, 251, 256
Hypothesis testing
Bayesian, see Bayesian, hypothesis
testing
frequentist, see Frequentist, hypothesis
testing
Importance of covariates, see Model,
averaging, importance of covariates
Importance sampling, 96
Index of non-randomness, 27
Information, 13, 15, 125, 165, 271
Information-theoretic approach/criteria,
12–13, 123, 128, 129, 252, 253,
270–272
Isotropic correlation structure, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, isotropic correlation structure
Isotropic variogram models, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, isotropic variogram models
Jeffreys
priors, see Prior distribution(s), Jeffreys
guidelines for Bayes factors, 82
Kriging, see Model, mixed-effects,
variance-covariance structures
within-groups random effects,
kriging
Kullback-Leibler distance (KL distance),
12, 165, 254, 270–271
Least squares (LS) ﬁt, 51–52, 56–60,
128–129, 255, 261–262
Likelihood
function, 11, 52–54, 54–56, 61–63
proﬁle, 52–54, 56–61
ratio test, 212–220
Linear models, 157
Linear regression, see Multiple linear
regression
Link function, 156, 159, 160–162, 255
complementary log-log, 156, 161, 175
identity, 159
logarithmic, 175–176
logit, 160–162
probit, 156, 161, 175
Logit link function, see Link function,
logit
Log-likelihood function, 53, 160, 162
Log-linear model, see Model, log-linear
Log transformation, 53
Logistic regression, 131, 159–175,
176–185, 187 & 190, 249–250
analysis, 5, 10, 131, 159–175,
176–185, 187 & 190, 249–250
classiﬁcation analysis, 131, 169–170 &
172–174, 177–178 & 183, 190
concordance, 174, 177, 183
correct classiﬁcation, 131, 169–170 &
172–175, 177–178, 183, 190
covariate pattern, 165
c statistic, 174, 183
cutoff points, 131, 169–170, 172–175,
177–178, 183, 190
INDEX
393

Logistic regression (Continued)
deviance statistic, 163–164, 179–181
discordance, 174
error of commission, 173, 184
error of omission, 173, 184
error rate, 172
estimates of coefﬁcients, 162
conﬁdence intervals, 162
sampling error, 162
standard error, 162
false negative, 173, 183
false positive, 173, 183
gamma statistic g, 174–175, 183
Hosmer and Lemeshow test, 167–168,
177, 183
Nagelkerke’s R2 adjusted coefﬁcient of
determination, 164–165, 177, 183
null model, 167
odds ratio, 166–167
receiver operating curve (ROC),
173–174, 190
saturated model, 163
sensitivity, 131, 173, 177–178, 183,
190
Somer statistic D, 174, 183
speciﬁcity, 131, 172, 177–178, 183,
190
statistics, 162–167
tau statistic t, 174, 183
type I error, 172
type II error, 173
Longitudinal structures, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, serial correlation structures
Losses, 88–90, 92
Management
adaptive, 50, 51, 115
decision, 8, 88–90, 92
level, 3
Marbled Godwit, 79
Marbled Murrelet, 4, 77, 78–79, 254
Margin of error, see Sampling error
Markov chains, 94–96
aperiodic, 94–96
ergodic, 94–96
irreducible, 94–96
positive Harris recurrent, 94–96
positive recurrent, 95
recurrent, 95
Markov chain Monte Carlo (MCMC), 11,
48, 57, 60, 61, 90–91, 94–96, 118,
249, 255–256
Mark-recapture estimation, 71–72,
97–98, 99
Maximum likelihood (ML) ﬁt, 52–54,
56–61, 128–129, 159, 162, 255
MCMC, see Markov chain Monte Carlo
Mean
conﬁdence interval, 6–7, 40–41
estimates, 6–7, 40–41, 56–58
estimator, 6
model, 56–58, 106–110, 194–195
parameter, 3, 5, 51–56, 56–58
sampling error, 6, 40–41
standard error, 6, 40–41
Mean-square error (MSE), see Model,
averaging, mean-square error (MSE)
Medical diagnostics, 83–88, 88–90, 92
Metaphor of the race, 123–124
Metrics, distance, see Model, mixed-
effects, variance-covariance
structures within-groups random
effects, distance metrics
Metropolis-Hastings algorithm,
98 & 100–101, 102, 118,
255–256
acceptance ratio, 100, 118
symmetry requirement, 100–101
proposal distribution, 100–101, 118
Metropolis, Nicholas, 48, 94, 98 &
100–101, 255
Mixed-effects modeling, see Model,
mixed-effects
Model
averaging, 131–134, 151, 190
importance of covariates, 133–134,
151, 190
mean-square error (MSE), 132–133
unconditional
conﬁdence intervals, 131–133
estimators, 131–133, 151, 190
estimators for prediction, 133
prediction intervals, 133
shrinkage estimator, 131–133,
151, 190
standard error, 131–133, 151, 190
binomial, 26, 27 & 30–31 & 33,
32–33, 60–61, 63, 69–71, 72,
74–77, 78–79, 84, 91, 94, 118,
158, 175, 255
exponential, 63, 72, 158
ﬁtting, 12, 50–56, 56–61,
128–129
Bayesian, see Bayesian, ﬁt
least squares, see Least squares
(LS) ﬁt
394
INDEX

maximum likelihood, see Maximum
likelihood (ML) ﬁt
restricted maximum likelihood, see
Restricted maximum
likelihood (REML) ﬁt
ﬁxed-effects, 14, 195–200, 200–202,
202–205, 205–241, 241 &
244–245
gamma, 66–69, 72
goodness of ﬁt, see Goodness of ﬁt
log-linear, 176
mixed-effects, 4, 14, 110–112, 115 &
118, 197–200, 202–205,
209–241 & 244–245,
254–255
nonlinear, 232–238
nonlinear function, 232, 233
variance-covariance structures
between-groups random effects,
215–216, 220–222, 255
blocked positive deﬁnite matrix
structure, 221
compound symmetry positive
deﬁnite matrix structure, 221
diagonal positive deﬁnite matrix
structure, 221–222
identity positive deﬁnite matrix
structure, 221–222
positive deﬁnite matrix structures,
220–222
symmetric positive deﬁnite matrix
structure, 221–222
variance-covariance structures
within-groups random effects,
216–219, 222–232, 255
anisotropic correlation structure,
230, 232
autocorrelation, 224–225, 227
autoregressive covariance
structure, 225–226, 227
autoregressive, moving-average
covariance structure
(ARMA(p,q)), 226 & 230,
227–230
combination variance structure,
224
compound symmetric covariance
structure, 225
constant plus power variance
structure, 217, 223
continuous autoregressive
covariance structure,
225–226
correlation function, 231
covariance structures, 224–232
distance metrics, 230–231
Euclidean distance metric, 230
exponential isotropic variogram
model, 230, 231–232
exponential variance structure,
218, 223
ﬁxed variance structure, 216,
222–223
Gaussian isotropic variogram
model, 231–232
identity variance structure, 223
isotropic correlation structure,
230, 232
isotropic variogram models,
230–231
kriging, 232
linear isotropic variogram model,
231–232
Manhattan distance metric, 231
maximum distance metric, 231
nugget, 231–232
power variance structure, 217, 223
range parameter, 231
rational quadratic isotropic
variogram model, 231–232
semivariogram, 229, 231
serial correlation structures, 224,
225–226 & 230, 227–229
spatial correlation structures, 224,
229–230, 230–232
spherical isotropic variogram
model, 231–232
symmetric covariance structure,
225
variance structures, 222–224
multinomial, 63, 72, 176
multiplicative mixed effects,
111–112, 115 & 118
negative binomial, 4, 4–5, 27, 58,
72, 110–112, 115 & 118, 158,
175–176, 249, 255
normal, 24, 56–58, 64–66, 72, 94,
102–103, 106–110, 116–117,
118, 158, 194–195, 255
over-dispersed, 58, 110–112, 115,
175–176, 259
Poisson, 4, 4–5, 25, 26–27, 42,
43–44, 58–60, 63, 66–69,
72, 91–92, 94, 110–112,
115, 118, 158, 175–176,
249, 255
INDEX
395

Model (Continued)
Poisson-gamma, 110–111, 115
random effects, 14, 110–112, 115 &
118, 197–200, 202–205, 209–
241, 241 & 244–245, 254–255
selection and inference strategy, 9–
10, 11–14, 121–128, 151,
176–177 & 179–182, 186, 187
& 190, 253–254, 241 & 244–
245, 256
a posteriori, 121–123, 124–127,
137 & 141, 143–146, 151,
187, 192–241, 250, 252,
253, 256
a priori parsimonious, 121–123,
127–128, 136–137, 138–
141, 142, 146–150, 151,
176–177 & 179–182, 187 &
190, 186, 241 & 244–245,
252, 253–254, 256
information-theoretic approach/
criteria, see Information-
theoretic approach/criteria
parsimonious, see Model,
selection and inference
strategy, a priori
parsimonious
Model-based inference, 156
Modeling
ﬁxed-effects, see Model, ﬁxed-effects
generalized linear, see Generalized
linear modeling
mixed-effects, see Model, mixed-
effects
multiple linear regression, see Multiple
linear regression
random-effects, see Model, random-
effects
Monitoring, 3, 50, 92
Monitoring plan, 78–79
Monte Carlo integration, 96
Multinomial distribution, see Model,
multinomial
Multiple linear regression
adjusted R2, 11–12, 130, 262, 269
AIC, 12–13, 136–146, 151, 262,
270–271
AICc, 12–13, 130, 262, 270–271
analysis 5, 9–10, 11–14, 42 & 45,
112–115, 136–150, 151 & 153,
155, 157, 159, 191–192, 194,
205–209, 249–250,
259–276
ANOVA F test, 11–12, 130, 136–146,
208, 262, 267–269
Bayesian information criterion (BIC),
13–14, 262, 271–272
coefﬁcient of determination R2, 11–12,
130, 136–146, 208, 262, 263–267
conﬁdence intervals, 136–146, 275
DIC, 130, 146–150
estimates of coefﬁcient parameters,
36–39, 51–52, 54, 129,
136–146, 262–263
generalizations of, 205–241
goodness of ﬁt, 274–276
Mallows Cp, 11–12, 130, 151, 262,
269–270
modeling, 36–39, 51–52, 112–115,
136–150, 205–209, 249–250,
259–276
prediction intervals, 130, 275–276
residual analysis, 130, 274–275
residual errors
homoscedastic, 130, 274–275
independent, 130, 274–275
normally distributed, 274–275
residual standard error, 11–12, 130,
136–146, 208–209, 262, 267
Nagelkerke’s R2 adjusted coefﬁcient of
determination, see Logistic
regression, Nagelkerke’s R2 adjusted
coefﬁcient of determination
Natural parameter, 158
Negative binomial distribution, see
Model, negative binomial
Neyman Pearson hypothesis testing, see
Frequentist, hypothesis testing
Noise, 13, 15, 125, 165, 254, 271
Non-informative priors, see Prior
distribution(s), non-informative
Nonlinear mixed-effects modeling, see
Model, mixed-effects, nonlinear
Nonlinear function, see Model,
mixed-effects, nonlinear function
Non-parametric bootstrapping, see
Goodness of ﬁt, non-parametric
bootstrapping
Normal distribution, see Model,
normal; Prior distribution(s), normal;
S-Plus software, distributions,
normal
Northern Spotted Owl, 2, 4, 16, 60, 70,
75–76, 83, 92, 248, 254
Nugget, see Model, mixed-effects,
variance-covariance structures
within-groups random effects,
nugget
396
INDEX

Numerical integration, 96
Nuisance variable, 197, 202
Odds ratio, 78–79, 81–83, 83–88,
91–92, 160–161, 166–167. See
also Bayes factors; Logistic
regression, odds ratio
Order, see Model, mixed-effects,
variance-covariance structures
within-groups random effects,
autoregressive moving-average
covariance structure (ARMA(p,q))
Orthogonal design, 124
Over-dispersion, 58, 72, 90, 110–112,
115 & 118, 175–176
Over-ﬁtting, 12–14, 125–128,
150, 167, 232, 237–238, 241,
253, 256
Parameter, 2, 3, 5–7
estimate, see Estimate
estimator, see Estimator
mean, see Mean, parameter
natural, see Natural parameter
proportion, see Proportion,
parameter
range, see Range parameter
Parametric bootstrapping, see Goodness
of ﬁt, parametric bootstrapping
Parsimonious model selection and
inference strategy, see Model,
selection and inference strategy, a
priori parsimonious
Partial autocorrelation, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, autoregressive moving-
average covariance structure
(ARMA(p,q))
Poisson distribution, see Model, Poisson;
S-Plus software, distributions,
Poisson
Population
abundance, see Abundance
closed, 97
discrete, 3–4
ﬁtness, see Fitness
mean parameter, see Mean, parameter
parameter, see Parameter
proportion parameter, see Proportion
parameter
survival rate, see Survival rate
total, 3
wildlife
Del Norte salamander, see Del Norte
salamander
Bald Eagles, see Bald Eagles
endangered, 9, 50, 91
grizzly bear, see Grizzly bear
Marbled Murrelet, see Marbled
Murrelet
mobile, 9
Northern Spotted Owl, see Northern
Spotted Owl
Siskiyou Mountains salamander, see
Siskiyou Mountains salamander
Posterior distribution(s), 10–11,
54–56, 56–61, 61–63
beta, 60–61, 69–71, 84, 91
gamma, 58–60, 66–69, 91–92
normal, 56–58, 64–66
odds, 82, 83, 83–88
risk, 88–90, 248–249
Precision, 8, 14–15, 64–66, 80, 107,
112–113, 146, 252
Prediction intervals, see Multiple linear
regression, prediction intervals
Predictive modeling, see Model, selection
and inference strategy, a priori
parsimonious
Prior distribution(s), 10–11, 54–56,
56–61, 61–63
beta, 63, 69–71, 72, 73–74, 75–77,
78–79, 84, 91, 94, 249
conjugate, see Conjugacy
diffuse, 74
Dirichlet, 63, 71–72
elicited, 74
gamma, 63, 66–69, 72, 74, 79, 91–92,
94, 107, 112–113, 118, 249
improper, 73
Jeffreys, 66, 68, 73–74, 74–77, 78–
79, 91–92, 107, 112–113, 118
normal, 63, 64–66, 94, 102, 107,
112–113, 118
non-informative, 56, 58, 60, 66, 68, 70,
73, 74–77, 83, 84, 107,
112–113
odds, 82, 83, 83–88
proper, 73
reference, 74
skeptical, 75–77, 78–79
uniform, 56, 58, 60, 70, 73, 75–77,
78–79, 91
vague, 74, 107
INDEX
397

Prior information, 81
Probit link function, see Link function,
probit
Project management, 14–15, 190,
250–254, 256
planning phase, 14–15, 250, 252–253,
256
effect size, 253
efﬁciency, see Precision; Project
management, planning phase,
precision
number of replicates, 14–15,
250–251, 253
objectives, 14–15
power, 14–15, 250–251, 253
power analysis, 253
precision, 14–15, 250, 252. See also
Precision
problem deﬁnition, 14–15
sample size, 14–15, 250, 252
statistical design, 14–15, 250
data collection phase, 14–15,
250, 251
concluding phase, 14–15, 250–252,
256
method of analysis, 14–15, 250,
251–252
Proportion
conﬁdence interval, 6–7, 41, 248
estimator, 6, 41, 248
parameter, 2, 60–61, 69–71,
248–249
sampling error, 6, 41, 248
standard error, 6, 41
Race, see Metaphor of the race
Random effects, see Model,
random-effects
Range parameter, see Model, mixed-
effects, variance-covariance
structures within-groups random
effects, range parameter
Receiver operating curve (ROC),
87–88. See also Logistic regression,
receiver operating curve (ROC)
Regression analysis
logistic, see Logistic regression
multiple linear, see Multiple linear
regression
negative binomial, 4–5, 156,
158–159, 175–176
Poisson, 4–5, 156, 158–159,
175–176
Rejection sampling, 96
Relationship, associative versus
causal, 4
Restricted maximum likelihood (REML)
ﬁt, 202
Risk, see Posterior distribution(s), risk
function, 88–90
R software, 16–39. See also S-Plus
software console, 16
Sample
size, see Project management,
planning phase, sample size space,
61–63
surveys, 5–8, 248–249, 252–253
Sampling error, 5–6, 54
mean, see Mean, sampling error
proportion, see Proportion, sampling
error
Saturated model, see Logistic regression,
saturated model
Scientiﬁc method, 50, 256
Semivariogram, see Model, mixed-
effects, variance-covariance
structures within-groups random
effects, semivariogram
Sensitivity, 85, 88, 88–90, 92. See also
Logistic regression, sensitivity
Sensitivity analysis, 55, 74–77, 78,
85–88
Shannon-Wiener diversity index, see
Ecological diversity index
Shrinkage, 65, 131–132, 151, 190
Signal, 13, 15, 125, 254, 271
Signiﬁcance, level of, 6
Siskiyou Mountains salamander, 16
Solution strategies, 5–14
Somer statistic D, see Logistic regression,
Somer statistic D
Speciﬁcity, 85, 88–90, 92. See also
Logistic regression, speciﬁcity
Specious covariates, 125, 127, 137, 177
S-Plus software, 16–39, 138–141,
177–178, 179–181. See also
R software
ANOVA, 213–216, 220
arithmetic, 17
assignment, 20
coercion, 18
command window, 16
control structures, 20, 22–23
data structures, 17–20, 21
data frames, 18, 21
lists, 18, 21
398
INDEX

matrices, 18, 21
vectors, 18, 21
data structure types, 21
character, 21
factor, 21
logical, 12
numeric, 12
directory structure, 19, 22
distributions, 21, 23–33
binomial, 21, 26, 27 & 30–31 & 33
32–33. See also Model,
binomial
normal, 19, 21, 24, 25, 29. See also
Model, normal
uniform, 21, 23–24, 25,
28–29. See also Prior
distribution(s), uniform
Poisson, 21, 26–27, 30–31.
See also Model, Poisson
estimation, 34–36
conﬁdence intervals, 34–35
mean, 34–35
proportion, 35–36
sampling error, 34, 35
exporting ﬁles, 22
functions, 20, 22–23
generalized least squares analysis, 214,
219–220
graphs, 21–22
grouped data, 201–202
help menu, 20
importing ﬁles, 22
linear regression analysis, 20, 23,
36–39, 208–209, 210–211. See
also Multiple linear regression
coefﬁcient of determination, 36–38,
138–141
residual standard error, 36–38,
138–141
ANOVA F test, 36–39, 138–141
AIC, 36–38, 141, 142
mixed-effects analysis, 209–239,
254–255
nonlinear least squares analysis,
232–238
objects, 17–20
orientation, 16–39
random numbers, 19, 21
restoring objects, 22
saving objects, 22
simple manipulations, 17–20
simple random sampling, 26, 28, 33
testing, 18
Standard deviation, 6
Standard error
mean, see Mean, standard error
proportion, see Proportion, standard
error
State space, 95
Stepwise selection 125, 137 & 141,
143–145, 151, 167, 253,
272–273. See also Model, selection
and inference strategy, a posteriori
Stochastic process, 95
Strategies for model selection and
inference, see Model, selection and
inference strategy
Survival rate, 3
Tau statistic t, see Logistic regression, tau
statistic t
Temporal structures, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, serial correlation structures
Test datasets, see Goodness of ﬁt, with test
datasets
Threshold level, 2, 8, 78–79, 248–249
Time series structures, see Model, mixed-
effects, variance-covariance
structures
within-groups random effects, serial
correlation structures
Transition kernel, 95
Treatments, 14, 195, 197–198
Type I error, 9, 50, 127. See also Logistic
regression, type I error
Type II error, see Logistic regression,
type II error
Unbiased estimator, 6, 52
asymptotically, 54
Unconditional estimators, see Model,
averaging, unconditional, estimators
Unconditional estimators for prediction,
see Model, averaging, unconditional,
estimators for prediction
Unconditional conﬁdence intervals, see
Model, averaging, unconditional,
conﬁdence intervals
Unconditional prediction intervals, see
Model, averaging, unconditional,
prediction intervals
Unconditional shrinkage estimator, see
Model, averaging, unconditional,
shrinkage estimator
INDEX
399

Unconditional standard error, see Model,
averaging, unconditional, standard
error
Uniform distribution, see Prior
distribution(s), uniform; S-Plus
software, distributions, uniform
Variogram models, see Model,
mixed-effects, variance-covariance
structures within-groups random
effects, semivariogram
Weighted least squares estimation, 52, 159
WinBUGS software, 11, 48, 57, 60, 61,
90–91, 94, 101 & 103–115, 115,
118–119, 146–150, 151,
178–179 & 184–185, 187 & 190,
239–241, 249, 254–255,
255–256
adapted, 103
autocorrelation, 103, 105
Brooks-Gelman-Rubin diagnostic
convergence statistic, 105
burn-in period, 104, 108,
118–119
code, 101 & 103, 104, 107–108, 111,
113, 147–148, 184, 240
compile, 103, 108
data section, 101 & 103, 104, 107–108,
113, 147–148, 184, 240
DIC, 105, 107, 109–110, 114,
146 & 148–149, 185, 186,
240–241
dynamic trace output, 104,
109, 114
initial values, 101 & 103, 107–108
initial values section, 101 &
103, 104, 107–108, 113,
148, 184, 240
over-relaxed, 103
posterior
credible intervals, 109–110, 114,
148, 185, 240
kernel density functions,
109–110, 114, 148, 185, 240
statistics, 109–110, 112 & 114, 148,
149, 184–185, 186, 240–241
program code section, 101 & 103, 104,
107–108, 111, 113, 147, 184, 240
refreshed, 103
Sample Monitor Tool menu option,
104, 106
Speciﬁcation Tool menu option, 103,
104
thinned, 103
Update Tool menu option, 103, 105
400
INDEX

