Multiscale Structure Guided Diffusion for Image Deblurring
Mengwei Ren†*
Mauricio Delbracio‡
Hossein Talebi‡
Guido Gerig†
Peyman Milanfar‡
†New York University
‡Google Research
Abstract
Diffusion Probabilistic Models (DPMs) have recently
been employed for image deblurring, formulated as an
image-conditioned generation process that maps Gaussian
noise to the high-quality image, conditioned on the blurry
input.
Image-conditioned DPMs (icDPMs) have shown
more realistic results than regression-based methods when
trained on pairwise in-domain data. However, their robust-
ness in restoring images is unclear when presented with out-
of-domain images as they do not impose speciﬁc degrada-
tion models or intermediate constraints. To this end, we in-
troduce a simple yet effective multiscale structure guidance
as an implicit bias that informs the icDPM about the coarse
structure of the sharp image at the intermediate layers. This
guided formulation leads to a signiﬁcant improvement of
the deblurring results, particularly on unseen domain. The
guidance is extracted from the latent space of a regression
network trained to predict the clean-sharp target at mul-
tiple lower resolutions, thus maintaining the most salient
sharp structures. With both the blurry input and multiscale
guidance, the icDPM model can better understand the blur
and recover the clean image. We evaluate a single-dataset
trained model on diverse datasets and demonstrate more
robust deblurring results with fewer artifacts on unseen
data. Our method outperforms existing baselines, achiev-
ing state-of-the-art perceptual quality while keeping com-
petitive distortion metrics.
1. Introduction
Image deblurring is a fundamentally ill-posed inverse
problem that aims to estimate one (or several) high-quality
image(s) given a blurry observation.
Deep networks al-
low for end-to-end image deblurring with pairwise super-
vised learning. While deep regression-based methods [86,
96, 102, 84, 6, 94, 7, 88, 83, 41, 65, 57] optimize dis-
tortion metrics such as PSNR, they often produce over-
smoothed outputs that lack visual ﬁdelity [39, 5, 13, 4].
Therefore, perceptual-driven methods [43, 26] aim to pro-
1Work done during an internship at Google Research.
MPRNet
Input
DeblurGanV2
UFormer
icDPM w/ guidance
icDPM
Figure 1. Deblurring example on Realblur-J dataset [66] with
models only trained on synthetic GoPro data [56], from recent
regression-based [94, 87] (MPRNet, UFormer), GAN-based [37]
(DeblurGANV2) and image-conditioned diffusion probabilistic
methods (icDPM). We introduce a guidance module onto the
icDPM formulation, and improves its robustness on unseen image.
duce sharp and visually pleasing images that are still faith-
ful to the sharp reference image, typically with a slight
compromise on distortion performance, i.e., a less than
3dB drop on PSNR [4, 60] allows for signiﬁcantly better
visual quality while still being close to the target image.
GANs [17] are leveraged for improved deblurring percep-
tion [36, 37]. However, GAN training suffers from insta-
bility, mode-collapse and artifacts [52], which may hamper
the plausibility of the generated images.
Recently, DPMs [19] further improved the photo-realism
in a variety of imaging inverse problems [72, 42, 88, 70],
formulated as an image-conditioned generation process,
where the DPM takes the degraded estimation as an aux-
iliary input. Image-conditioned DPMs (icDPMs) do not es-
timate the degradation kernel nor impose any intermediate
constraints. These models are trained using a standard de-
noising loss [19] with pairwise training data in a supervised
fashion. In image restoration, such pairwise training dataset
is typically artiﬁcially curated by applying known degrada-
tion models on a group of clean images, which inevitably
introduces a domain gap between the synthetic training
dataset and real-world blurry images. When presented with
unseen data, the robustness of icDPMs are rather unclear
as the intermediate restoration process is intractable. E.g.,
we observe a noticeable performance drop when we apply
arXiv:2212.01789v2  [cs.CV]  20 Mar 2023

the synthetically trained icDPM to out-of-domain data, in-
cluding failure to deblur the input (Fig. 1) and injection of
artifacts (Fig. 4 ‘icDPM’ and Fig. 7 ‘DvSR’).
We empirically established a connection between do-
main sensitivity and image-conditioning in the existing de-
blurring icDPMs [70, 72, 88], where the observed poor gen-
eralization is attributed to the naive input-level concatena-
tion and the lack of intermediate constraints during the de-
blurring process. When optimized on the synthetic training
set, overﬁtting or memorization [76] may occur, making
the model vulnerable to shift of the input distribution. Cur-
rently, conditioning DPM on blurred or corrupted images is
under-explored [67], and we hypothesize that more effective
image conditioning for icDPM is crucial to make the model
more constrained and robust towards unseen domain.
Inspired by traditional blind deblurring algorithms where
optimization is made using explicit structural priors (e.g.,
containing image saliency [61, 91]), we enhance the icDPM
backbone (UNet [69]) with a multiscale structure guidance
at intermediate layers. These guidance features are obtained
through a regression network trained to predict salient sharp
features from the input. The guidance, in conjunction with
the blurry image, provide more informative cues to the
model regarding the speciﬁc degradation in the image. As
a result, the model can more accurately recover the clean
image and generalize more effectively.
Our contributions are threefold: (1) we investigate and
analyze the domain generalization of conditional diffu-
sion models in motion deblurring task, and empirically
ﬁnd a relationship between model robustness and image-
conditioning; (2) we propose an intuitive but effective guid-
ance module that projects the input image to a multiscale
structure representation, which is then incorporated as an
auxiliary prior to make the diffusion model more robust;
(3) Compared with existing benchmarks, our single-dataset
trained model shows more robust results across different
test sets by producing more plausible deblurring and fewer
artifacts, quantiﬁed by the state-of-the-art perceptual qual-
ity and on par distortion metrics.
2. Related Works
Single image deblurring is the inverse process of recov-
ering one or multiple high-quality, sharp images from the
blurry observation. Typically, classic deblurring approaches
involve variational optimization [16, 35, 40, 53, 62, 91,
1, 25], with prior assumptions on blur kernels, images or
both, to alleviate the ill-posedness of the inverse problem.
Handcrafted structural priors, such as edges and shapes,
have been used successfully in many algorithms to guide
the deblurring process towards preserving important fea-
tures in the image while removing blur [61, 62, 91]. Our
design principle is inspired by these approaches and in-
volves a learned guidance as implicit structural bias. With
the emergent of deep learning, deblurring can be cast as a
particular image-to-image translation problem where a deep
model takes the blurry image as its input, and predicts a
high quality counterpart, supervised by pixel wise losses
between the recovered image and the target [86, 96, 102,
84, 6, 94, 7, 88, 83, 41, 65, 57, 24].
Pixel-wise losses,
such as L1 and L2, are known to result in over-smoothed
images [39, 5, 13] given their ‘regression to the mean’ na-
ture. To this end, perception-driven losses including percep-
tual [26, 98, 50, 49, 100, 13] and adversarial losses [36, 37]
are added on top of the pixel-wise constraints, to improve
the visual ﬁdelity of the deblurred image, with a compro-
mising drop in distortion scores [4, 60]. Tangentially, recent
works seek to improve the architectural design by exploring
attention mechanisms [59, 93, 87, 92, 83, 84], multi-scale
paradigms [56, 7] and multi-stage frameworks [95, 6, 94].
Diffusion Probablistic Models (DPM) [75, 19, 77, 14],
Score-based models [79, 80, 81] and their recent ex-
ploratory generalizations [2, 23, 12] achieved remarkable
results in a varied range of applications[10], from image and
video synthesis[71, 63, 68, 20, 30, 21], to solving general
imaging inverse problems[11, 27, 32, 29, 38, 8]. DPMs are
characterized for having stable training [19, 14, 28], diverse
mode coverage [78, 33], and high perception [71, 14, 63].
DPM formulation involves a ﬁxed forward process of grad-
ually adding Gaussian noise to the image, and a learnable
reverse process to denoise and recover the clean image, op-
erated with a Markov chain structure. Conditional DPMs
aim to perform image synthesis with an additional input
(class [14], text [71, 63], source image).
Image-conditioned DPMs (icDPMs) have been success-
fully re-purposed for image restoration tasks such as super-
resolution [72, 42], deblurring [88], JPEG restoration [70,
31]. This is achieved by concatenating the corrupted ob-
servation at input level. They do not require task-speciﬁc
losses or architectural designs, and have been adopted due
to high sample perceptual quality. Nevertheless, general-
ization of DPMs to unseen shifts in domain, and their low-
quality/corrupted image conditioning remains unexplored.
Generalization to unseen domain As mentioned above,
deep restoration models for deblurring rely on synthetic
pairwise training data.
However, any well-trained deep
restoration model may fail to produce comparable results
on out-of-domain data (Fig. 1). To address this issue, re-
searchers have pursued two main directions for improving
model generalization: enhancing the representativeness and
realism of the training data, or improving the model’s do-
main generalization ability.
Our method focuses on the
latter, but it is not mutually exclusive with the former di-
rection and can be combined to further improve the re-
sults.
To tackle the data limitations, previous works fo-
cus on acquiring or combining more representative train-
ing data [55, 66, 65, 101], and/or generate realistic de-

Forward 
diffusion
UNet 
decoder
UNet 
encoder
…
scale 1
scale k
scale 2
Blurry input
Predicted 
noise
Multiscale 
Structure Guidance
 
Clean image
x
y
xt
𝜖
Figure 2. The training process of the proposed deblurring method.
The backbone model is a standard image-conditioned DPM
(icDPM) where a UNet learns to perform denoising towards the
clean image conditioned on the blurry input. We equip the icDPM
with a structure guidance module (detailed in Fig. 3) to better in-
form the model of the coarse sharp structure at multiple scales.
graded images using generative approaches [97, 89]. Other
prior works focus on explicit domain adaptation, leveraging
transfer learning techniques to reduce domain gaps. These
approaches include unpaired image translation [22, 64] and
domain adaptation [85, 73, 46, 57], which typically involve
an adversarial formulation and joint training between two
speciﬁed domains. However, these methods may require re-
training when a new dataset is introduced. In contrast, our
method does not involve explicit adaptation between spec-
iﬁed domains. Instead, we focus on introducing more ef-
fective image conditioning mechanism that naturally makes
the model more robust towards distribution shift.
3. Method
3.1. Overview
We assume access to a paired dataset with samples
(x, y) ∼ptrain(x, y), where x represents the high-quality
sharp image, and y is the respective low-quality blurry ob-
servation (denoted in Fig. 2). Such paired dataset is typ-
ically generated by simulating degraded images from the
high-quality ones adopting a speciﬁc degradation model.
The goal is to reconstruct one or multiple clean, sharp im-
ages x from the low-quality observation ˆy ∼preal(ˆy). Gen-
erally, the distribution of the training set ptrain differs from
that of unseen images preal. It is thus crucial that a model
not only performs well on ptrain but also generalizes to preal.
DPMs We consider a general-purpose DPM for our formu-
lation given its superior performance in high-quality image
restoration [72, 88]. In what follows, we brieﬂy describe the
training and sampling of a DPM to contextualize our work.
Unconditional DPMs aim to sample from the data distribu-
tion p(x) by iteratively denoising samples from a Gaussian
distribution and converting them into samples from the tar-
get data distribution. To train such model, a forward dif-
fusion process and a reverse process are involved. As illus-
trated in Fig. 2, at a diffusion step t, a noisy version xt of the
target image x is generated by xt = √αtx +
p
(1 −αt)ϵ,
ϵ ∼N(0, Id), where ϵ is sampled from a standard Gaus-
Guidance 
network
Regressor
scale k
↓2k
Grayscale 
space 
Downsampled 
space
RGB 
space 
Downsampled 
Clean space
Structure Guidance                                
Predicted 
clean image
Blurry
input
Guidance
y
Figure 3. The learned structure guidance (red box in Fig. 2) is ex-
tracted from the latent features of a regression network trained to
predict the luma channel of the sharp target at multiple lower reso-
lutions. In this way, the guidance maintains only structure-relevant
information, representing the underlying sharp image.
sian distribution N(0, Id), and αt controls the amount of
noise added at each step t.
In the reverse process, an
image-to-image network (i.e. UNet) Gθ(xt, t) parameter-
ized by θ learns to estimate the clean image from the par-
tially noisy input xt. In practice, a reparameterization of
the model to predict the noise instead of the clean image
leads to better sample quality [19]. Once trained, it sam-
ples a clean image by iteratively running for T steps start-
ing from a pure Gaussian noise xT ∼N(0, Id).Image-
conditioned DPMs further inject an input image y so as to
generate high-quality samples that are paired with the low-
quality observation. This involves generating samples from
the conditional distribution of p(x|y) (posterior). A con-
ditional DPM Gθ([xt, y], t) is used, where the image con-
ditioning is typically implemented via concatenation of y
and xt at input-level [72, 88, 70]. However, we found that
this formulation is sensitive to domain shift in input images,
and leads to poor generalization (‘DPM’ in Fig. 1). More-
over, in many cases it introduces visual artifacts (‘DvSR’ in
Fig. 7). We speculate that this is due to the naive image-
conditioning (input-level concatenation), which lacks con-
straints in the intermediate process. Therefore, we integrate
a multiscale structure guidance h(y) into the latent space
of the icDPM backbone, to inform the model about salient
image features, such as signiﬁcant coarse structures that are
essential for reconstructing a high-quality image, while dis-
entangling irrelevant information, such as the footprint of
blur kernels and color information. To obtain such guidance
with the aforementioned characteristics, we propose an aux-
iliary regression network and leverage its learned features as
the realization of the guidance, described below in Sec. 3.2.
3.2. Multiscale structure guidance
Fig. 3 shows the details of our proposed guidance, de-
noted as h(·). The DPM equipped with such multiscale
guidance is better aware of the underlying salient structures
of the input, thus it learns to better sample from the target
conditional distribution to preal(x|y). Moreover, the dis-
tribution of h(y) does not change signiﬁcantly when the
input domain changes, so that it can reliably provide the

auxiliary structure guidance even when applied to unseen
domain. To both ends, we construct the guidance module
as hk(·) = H(φk(·)). At scale k, it consists of an image
transformation function φk(·), followed by a regression-
driven guidance network H. Speciﬁcally, φk(·) transforms
the input image y to suppress information not relevant to
the coarse sharp image structures (e.g., color, and informa-
tion about the domain-speciﬁc degradation). This ensures
that H operates on a less input-domain-sensitive space. We
ﬁrst convert y to the grayscale space ¯y, and then, ¯y gets
downsampled by a factor of 2k, where k = 1, 2, 3. This
removes ﬁne details (including the footprint of blur to cer-
tain amount), while preserving coarse structures at multiple
lower resolutions. Motivated by [89], we also add a small
amount of Gaussian noise to mask other domain speciﬁc
degradations/characteristics, and make the output less sen-
sitive to input domain shift. Thus,
φk(y) = d↓k(¯y) + n,
n ∼N(0, σ2I).
(1)
Then, the guidance network Hϕ extracts the guidance
feature by mapping φk(y) onto the representation/latent
space as hk(y) = Hϕ(φk(y)). To make sure it obtains
salient structure features and further ﬁlters out insigniﬁcant
information, we apply a regression task Rϕ on top of hk(y),
and constraints the output to be closer to its sharp target
φk(x). In this way, the guidance hk(y) at scale k is en-
forced to maintain information that is relevant to a sharp im-
age, and suppress other signals that are input-speciﬁc (e.g.,
trace of blurs).
Finally, we incorporate the multiscale guidance {hk(y)}
to the original diffusion UNet by adding the extracted rep-
resentation to the feature map at the respective scale on the
diffusion encoder (Fig 2) as an extra bias. To compensate
for the difference in depth, at each corresponding scale, we
apply a convolutional layer that has the same number of fea-
tures as in the diffusion encoder. Detailed diagram is pro-
vided in the appendix.
3.3. Training loss
Our model is trained end-to-end with both a multiscale
regression loss for optimizing the guidance network, and a
denoising loss in icDPM. The regression loss is the mean
squared error at each scale k deﬁned as:
Lk
guidance = E(x,y)∼ptrain∥Rϕ(Hϕ(φk(y))) −φk(x)∥2,
(2)
where Hϕ is the guidance feature extractor, and R is instan-
tiated as a single convolutional layer that projects the guid-
ance feature to the ﬁnal output towards the clean image (as
depicted in Fig. 3). The total regression loss is the average
over different scales Lguidance = P
k Lk
guidance. Note that we
do not use any additional downsampling/upsampling oper-
ation in the guidance network, so the spatial dimension re-
mains the same at each scale. We empirically observe that
the best performance is obtained by integrating three differ-
ent scales with k = 1, 2, 3 with details discussed in Sec. 4.6.
By aggregating the information from the input image
y, and the multi-scale guidance {hk(y)}, our icDPM G is
trained by minimizing the denoising loss,
LDPM = E(x,y)∼ptrainEt∼Unif(0,1)Eϵ∼N(0,I)
(3)
∥Gθ(xt, y, {Hϕ(φk(y))}, αt) −ϵ∥1.
The denoising model parameterized by θ predicts the
noise ϵ, given the noisy corruption xt, the blurry input
y, the noise scheduler αt as well as the proposed multi-
scale guidance {Hϕ(φk(y))}. The total training loss L =
Lguidance + LDP M, which is used to optimize the guidance
network H, the regression layer R, and icDPM G in an
end-to-end manner. During the inference, the model starts
with a Gaussian noise, and iteratively recovers the clean im-
age, conditioned on both the blurry input and the multiscale
guidance at each denoising step.
4. Experiments
4.1. Setup and metrics
As motivated above, we are particularly interested in
the model generalization of DPMs to unseen blurry data.
Therefore, we set up our experiments under the scenario
that the model will be only trained with synthetic paired
dataset, and will be evaluated on a few unseen testing sets
where the images may present different content and distor-
tions than the in-domain data. To benchmark, we use the
widely adopted motion deblurring dataset GoPro [56] as our
training data, and assume Realblur-J [66], REDS [55] and
HIDE [74] are representatives of unseen test sets.
In GoPro [74], 3214 pairs of blurry/clean training exam-
ples are provided for training, and 1111 images are held-out
for evaluation. Realblur-J [66] is a recent realistic dataset
mainly consisting of low-light scenes with motion blur with
980 test images provided. We consider it to present the
largest domain gap with GoPro.
REDS [55] presents a
complimentary video deblurring dataset with more realistic
motion blur. We follow [55, 6] and extract 300 validation
images for the motion deblur test. HIDE [74] is the most
commonly adopted dataset to test the model generalization
ability trained from GoPro with 2025 test images.
4.2. Implementation details
Our framework is implemented in TensorFlow 2.0 and
trained on 32 TPU v3 cores. We warm start the training with
only regression loss, and linearly increase the weight of the
denoising loss to 1 within the ﬁrst 60k iterations. Adam

x2 output
x4 output
x8 output
Input
icDPM w/ guide
icDPM
Figure 4. Top row: We compare the visual deblurring results on
an out-of-domain image between a standard icDPM and icDPM
with the proposed guidance module. While icDPM is prone to
producing artifacts, our method that incorporates the guidance is
more robust. Bottom row: We also visualize our multiscale regres-
sion outputs from scales of ×8, ×4, ×2, indicating the prediction
at spatial resolutions of 1/8, 1/4, 1/2 the input image.
optimizer [34] is used during the training (β1 = 0.5, β2 =
0.999), with batch size 256 on 128 × 128 random crops.
We use linear increasing learning rate within the ﬁrst 20k
iterations, then with a constant learning rate 1 × 10−4. We
use a fully-convolutional UNet architecture [88] for icDPM
to ensure the model can be used at arbitrary image resolu-
tions. During the inference, we follow [88] and perform
a sequence of sampling under different parameters. More
details are included in the appendix.
4.3. Effectiveness of the guidance
We ﬁrst validate the effectiveness of the proposed guid-
ance module by qualitatively comparing with our baseline
setup, which is a standard image-conditioned DPM (abbrev
as ‘icDPM’), on top of which we will introduce the guid-
ance module (abbrev as ‘icDPM w/ Guide’).
Table 1. Inception distances analysis (domain-shift) between two
different domains (GoPro v.s. Realblur-J) at different scales within
different image space. At each scale, the guidance network output
consistently reduces the gap compared to the downsampled input
images, as expected. The distance between grayscale inputs (at
original spatial resolution) are also given as a reference. KID val-
ues are scaled by a factor of 100 for readability.
Space
FID ↓
KID ↓
Input
61.115
3.07
Input ×2 downsampled
58.266
3.02
Guidance ×2 output
49.437
3.00
Input ×4 downsampled
56.313
3.60
Guidance ×4 output
47.984
3.46
Input ×8 downsampled
49.684
4.91
Guidance ×8 output
44.649
4.70
Guidance and domain gap. As the guidance is designed
to improve the robustness towards domain shift, we perform
an analysis on the Inception distances from different inter-
mediate ‘image space’ to verify whether the guidance mod-
ule is progressively reducing the gap between inputs from
different sources (i.e. different blurry images in our sce-
nario). In Table 1, we start by calculating the per-scale In-
ception distance between GoPro (in-domain) and Realblur-J
(out-of-domain) images. At each scale of ×2, ×4 and ×8
downsampled space, we observe a consistent reduction of
FID and KID on the guidance network outputs, compared
with the downsampled grayscale inputs. This demonstrates
that the introduction of the learned guidance may provide
more domain-agnostic information and beneﬁt generaliza-
tion of the model on unseen domains. In Fig. 4, we dis-
play the multiscale regression outputs at different scales on
an out-of-domain input. The results align with our expecta-
tions, as the grayscale prediction at each scale progressively
approached a clean image. Further, we noticed pronounced
sampling artifacts from icDPM in this example, which are
effectively eliminated with the proposed guidance.
Guidance and model capacity. As the guidance network
introduces more parameters, we investigate if its perfor-
mance improvement is solely due to larger models. We per-
form a joint analysis of varying model size with and without
the guidance network, and results are presented in Table 2.
We refer results on the GoPro test set as ‘In-domain’ and
on the Realblur-J dataset as ‘Out-of-domain’, using a sin-
gle GoPro-trained model. We keep the number of building
blocks constant and modulate the network size by changing
only the number of convolutional ﬁlters. ‘-S’ and ‘-L’ in-
dicate a smaller and larger models, respectively. We start
27.8
28.0
28.2
28.4
28.6
28.8
Distortion (PSNR)
6.5
7.0
7.5
8.0
Perception (1/LPIPS)
icDPM-S w/ Guide-S
icDPM-L w/ Guide-L
icDPM-L
icDPM-S
Figure 5. Perception-distortion plot as supplementary for Table 2,
under varying sampling parameters.
Under different network
capacities (‘-S’ and ‘-L’ refer to small and large respectively),
the guidance mechanism allows for consistently better perceptual
quality and lower distortions compared to icDPM. Plots for other
datasets are in supplementary, where we observe similar trends.
with the image-conditioned DPM (icDPM) without the pro-
posed guidance network under different network sizes. In
Table 2 row (a) and (b), we observe a signiﬁcant improve-
ment of the in-domain deblurring performance by increas-
ing the UNet capacity, in terms of both perception and dis-
tortion qualities. However, the out-of-domain testing results
become much worse with a larger network, suggesting po-

Table 2. The effectiveness of the proposed guidance on top of image-conditioned DPM (icDPM), under different network size (‘-S’ and
‘-L’ refer to small and large networks respectively). We show both In-Domain (train on GoPro, test on GoPro), and Out-of-Domain (train
on GoPro, test on Realblur-J) results. Based on (a)-(b), we observe a larger icDPM boost in-domain performance, while not necessarily
lead to better out-of-domain results. With guidance (c)-(d), we observe consistent improvements both in-domain and out-of-domain.
Guidance
Diffusion
#Params
In-Domain
Out-of-domain
network
network
best LPIPS ↓
best PSNR ↑
best LPIPS ↓
best PSNR ↑
(a) icDPM-S
-
ch=32
6M
0.077
30.555
0.150
28.209
(b) icDPM-L
-
ch=64
27M
0.058
32.105
0.156
27.996
(c) icDPM-S w/ Guide-S
ch=32
ch=32
10M
0.068
31.298
0.145
28.286
(d) icDPM-L w/ Guide-L
ch=64
ch=64
52M
0.057
32.254
0.123
28.711
Table 3. Average deblurring results across GoPro [56], HIDE [74]
Realblur-J [66], REDS [55] dataset with GoPro [56]-only trained
model, indicating the model robustness on various unseen data.
Perceptual
Distortion
LPIPS ↓NIQE ↓FID ↓KID ↓PSNR ↑SSIM ↑
DeblurGAN-v2 [37]
0.149
3.42
14.57 5.28
28.09
0.871
MPRNet [94]
0.140
3.70
20.22 8.49
29.78
0.897
UFormer [87]
0.133
3.65
18.99 8.13
30.06
0.903
Restormer [92]
0.139
3.69
19.90 8.36
30.00
0.895
Ours-SA
0.124
3.64
14.36 6.79
29.98
0.902
Ours
0.104
2.94
8.41
2.39
28.81
0.881
tential overﬁtting during the training. Through visual in-
spection, we also found that the larger DPM is prone to arti-
facts when presented with unseen data, as shown in Fig. 1, 4
and 8. By introducing the guidance network, we observe
both in-domain and out-of-domain performance gains. We
additionally present a distortion-perception plot in Fig. 5,
with samples acquired from varying sampling parameters
(i.e. number of steps and the standard deviation of noise).
Similar to [88], we found a general trade-off between per-
ceptual quality and distortion metrics. Also, we observed
that all guided models consistently outperform the baseline
DPMs under varying sampling parameters. We provide ad-
ditional results on other datasets in appendix and observe
similar effects and beneﬁts of using the guidance module
over the baseline icDPM.
4.4. Deblurring results
We compare our deblurring results with the state-of-
the-art methods, loosely categorized into distortion-driven
models [6, 94, 7], perception-driven GAN based meth-
ods [36, 37], as well as recent diffusion-based method [88].
In this work, we place particular emphasis on evaluating
(1) the generalization ability on unseen data, and (2) the
perceptual quality of the output, willing to compromise a
slight drop on average distortion scores towards a better
trade-off between distortion and perception [4]. For bench-
marking, we mainly consider perceptual quality of the re-
sults quantiﬁed by standard metrics for generative models
including LPIPS [99], NIQE [54], FID (Fr´echet Inception
Distance) [18], and KID (Kernel Inception Distance) [3].
We also present distortion metrics including PSNR and
SSIM for completeness. However, we note that they are
less correlated with human perception [58] and maximiz-
ing PSNR/SSIM results in a compromise of visual percep-
tion [4]. Our method is based on generative models and
performs stochastic posterior sampling. The reference im-
age provided in the training dataset is only one of the pos-
sible restoration result among other possibilities (due to the
ill-posed nature of inverse problems). Therefore, similar
to [4, 60], our results compromise a certain amount of pix-
elwise average distortion while still being faithful to the tar-
get. We highlight best and second-best values for each
metric. KID values are scaled by a factor of 1000 for read-
ability.
Table 4. Image deblurring results on GoPro [56] dataset.
Perceptual
Distortion
LPIPS ↓NIQE ↓FID ↓KID ↓PSNR ↑SSIM ↑
HINet [6]
0.088
4.01
17.91 8.15
32.77
0.960
MPRNet [94]
0.089
4.09
20.18 9.10
32.66
0.959
MIMO-UNet+ [7]
0.091
4.03
18.05 8.17
32.44
0.957
SAPHNet [82]
0.101
3.99
19.06 8.48
31.89
0.953
SimpleNet [43]
0.108
-
-
-
31.52
0.950
DeblurGANv2 [37]
0.117
3.68
13.40 4.41
29.08
0.918
DvSR [88]
0.059
3.39
4.04
0.98
31.66
0.948
DvSR-SA [88]
0.078
4.07
17.46 8.03
33.23
0.963
UFormer [87]
0.087
4.08
19.66 9.09
32.97
0.967
Restormer [92]
0.084
4.12
19.33 8.75
32.92
0.961
Ours-SA
0.078
4.10
8.69
7.06
33.20
0.963
Ours
0.057
3.27
3.50
0.77
31.19
0.943
We ﬁrst present the in-domain GoPro performance in Ta-
ble 4. Our model achieved state-of-the-art perceptual met-
rics across the board, while maintaining competitive distor-
tion metrics by taking average of multiple samples (‘Ours-
SA’). Moreover, we are interested in the domain general-
ization and out-of-domain results on Realblur-J (Table 5),
REDS (Table 7) and HIDE (Table 6). We achieved a sig-
niﬁcantly better perceptual quality on the unseen Realblur-J
and REDS, and competitive results on HIDE. Further, we
analyze the robustness the best-performing single-dataset
trained models by comparing their average performance
across all four test sets summarized in Table 3. Our method

Figure 6. Deblurring results on GoPro [56] (top row) and HIDE [74] (bottom row) test set from MIMO UNet+ [7], DvSR [88],
UFormer [87], and Ours. All models are trained only on GoPro [56] training set. Our method generates perceptually much sharper
images, and reduce artifacts when applied to unseen images (HIDE). Enlarged results are provided in the appendix.
Figure 7. Test examples on Realblur-J [66], with all models trained only on GoPro dataset [56]. Best viewed electronically. We empirically
found DeblurGANv2 [37] and DvSR [88] are prone to artifacts, and regression-based UFormer [87] produces over-smoothing images. Our
methods alleviate artifacts, and produce high-ﬁdelity deblurring on unseen data, even when the domain gap is large.
Table 5. Results on Realblur-J [66] with GoPro trained models.
Perceptual
Distortion
LPIPS ↓NIQE ↓FID ↓KID ↓PSNR ↑SSIM ↑
UNet [69]
0.175
3.911
22.24
8.07
28.06
0.857
DeblurGAN [36]
-
-
-
-
27.97
0.834
DeblurGAN-v2 [37]
0.139
3.870
14.40
4.64
28.70
0.866
MPRNet [94]
0.153
3.967
20.25
7.57
28.70
0.873
DvSR [88]
0.153
3.277
18.73
6.00
28.02
0.851
DvSR-SA [88]
0.156
3.783
20.09
7.43
28.46
0.863
Restormer [92]
0.149
3.916
19.55
7.12
28.96
0.879
UFormer-B [87]
0.140
3.857
18.56
7.02
29.06
0.884
Ours-SA
0.139
3.809
16.84
6.25
28.81
0.872
Ours
0.123
2.976
12.95
3.58
28.56
0.862
signiﬁcantly improves the perceptual scores, while main-
taining highly competitive distortion scores with < 0.08
dB difference from the best PSNR, and < 0.001 difference
from the best SSIM with averaging samples (‘Ours-SA’).
Visual deblurring examples are provided in Fig. 6 on Go-
Pro [56] and HIDE [74], Fig. 7 on RealblurJ [66] and Fig. 8
on REDS [55], respectively.
On the GoPro (in-domain)
test example, we ﬁnd that all methods are able to produce
reasonable artifact-free deblurring, and our method gener-
ates sharper and more visual realistic results. On the three
Table 6. Results on HIDE [74] with GoPro [56] trained models.
Perceptual
Distortion
LPIPS ↓NIQE ↓FID ↓KID ↓PSNR ↑SSIM ↑
HINet [6]
0.120
3.20
15.17 7.33
30.33
0.932
MIMO-UNet+ [7]
0.124
3.24
16.01 7.91
29.99
0.930
MPRNet [94]
0.114
3.46
16.58 8.35
30.96
0.940
SAPHNet [82]
0.128
3.21
16.78 8.39
29.99
0.930
DeblurGAN-v2 [37]
0.159
2.96
15.51 6.96
27.51
0.885
DvSR-SA [88]
0.105
3.29
15.34 8.00
30.94
0.940
DvSR [88]
0.089
2.69
5.43
1.61
29.77
0.922
UFormer [87]
0.113
3.40
16.27 8.51
30.89
0.920
Restormer [92]
0.108
3.41
15.84 8.28
31.22
0.923
Ours-SA
0.104
3.40
14.62 7.62
30.96
0.938
Ours
0.088
2.91
5.28
1.68
29.14
0.910
out-of-domain datasets, performance degradation starts to
occur from baseline methods.
For instance, GAN-based
model [37] and previous diffusion based model [88] tend
to produce artifacts on out-of-domain data, and state-of-the-
art regression based model [87] produces over-smoothed re-
sults. Our formulation performs more consistently better
across different datasets, signiﬁcantly reducing artifacts on
unseen data with high perceptual realism. More enlarged
visual examples are in the appendix.

Ours
Reference
icDPM w/o guide
 Input
HINet
DeblurGANv2
Figure 8. REDS [55] deblurring examples from HINet [6], DeblurGAN-v2 [37], icDPM without guidance and Ours. When trained only
on GoPro [56], our method better removes the blur trace from the input image, while eliminating artifacts when applied on unseen data.
Table 7. Results on REDS [55] with GoPro-trained models.
Perceptual
Distortion
LPIPS ↓NIQE ↓FID ↓KID ↓PSNR ↑SSIM ↑
HINet [6]
0.195
3.223
21.48
7.91
26.72
0.818
DeblurGAN-v2 [37]
0.181
3.172
14.98
5.12
27.08
0.814
MPRNet [94]
0.204
3.282
23.90
8.94
26.80
0.814
Restormer [92]
0.213
3.326
24.86
9.30
26.91
0.818
UFormer-B [87]
0.192
3.272
21.48
7.91
27.31
0.842
Ours-SA
0.178
3.248
17.27
6.21
26.95
0.834
Ours
0.147
2.610
11.91
3.51
26.36
0.810
4.5. Perceptual Study
We further conducted a user study with human subjects
to verify the perceptual quality of the deblurring perfor-
mance on unseen data, with all models trained on GoPro
and tested on Relblur-J. We asked Amazon Mechanical
Turk raters to select the best quality image from a given pair.
We used 30 unique pairs of size 512×512 and averaged the
750 ratings from 25 raters. In Table 8, each value represents
the fraction of times that the raters preferred the row over
the column. As can be seen, our method outperforms the
existing solutions. Also, it is worth pointing out that there
is a signiﬁcant gap in the preference of our method with and
without the guidance mechanism (denoted as icDPM).
Table 8. Perceptual study with human subjects on Realblur-J [66]
dataset using GoPro [56] trained models. Each value represents
the fraction of times that raters preferred the row over the column.
DGANv2
[37]
UFormer
[87]
DvSR
[88]
icDPM Ours-
SA
Ours
DGANv2
-
0.28
0.43
0.56
0.26
0.17
UFormer
0.72
-
0.53
0.58
0.44
0.35
DvSR
0.57
0.47
-
0.61
0.32
0.29
icDPM
0.44
0.42
0.39
-
0.28
0.21
Ours-SA
0.74
0.56
0.68
0.72
-
0.38
Ours
0.83
0.65
0.71
0.79
0.62
-
4.6. Additional modeling choices
We carried out additional ablation studies for the mod-
eling choices of the guidance network on regression target
(RGB v.s. grayscale), the number of scales to adopt for
the guidance (single v.s. multiscale), and the mechanism
of incorporating the guidance (input-level vs latent space).
Table 9 (a) indicates our baseline icDPM without any guid-
ance.We ﬁrst compare the difference between incorporating
the guidance at input-level and at latent space (Table 9 (b)
and (c)). In (b), we upscale the regression output to the
original input size, and concatenate the result to the diffu-
sion UNet. In (c), we incorporate the feature maps before
regression output into the UNet latent space via addition
operation described above. The results indicate the bene-
ﬁt of latent-space guidance over input-level concatenation.
Both (b)(c) improve on (a), showing the overall beneﬁt of
introducing the guidance. We also observe a moderate im-
Table 9. Effect of various settings on the domain invariant guid-
ance. The scale column denotes the downsampling factors.
Regression
Scale(s)
Guidance LPIPS PSNR
(a)
-
-
-
0.156
28.21
(b)
RGB
×8
input
0.145
28.34
(c)
RGB
×8
latent
0.143
28.45
(d)
RGB
×2, ×4, ×8
latent
0.141
28.45
(e)
Grayscale
×2, ×4, ×8
latent
0.137
28.63
provement by using multiscale guidance rather than single
scale guidance in row (d) over (c). In row (e), we simpliﬁed
the regression target from color space to grayscale space,
which further improved the results.
5. Discussion
We present a learned multiscale structure guidance
mechanism for icDPM that acts as an implicit bias which
enhances its deblurring robustness. Yet, we acknowledge
that limitations exist and require further investigation. Al-
though our focus is on improving the model’s ability to gen-
eralize to unseen data without access to large-scale realistic
training data, we recognize that the quality and realism of
the training dataset ultimately bounds the deblurring capa-
bility of the model. In our experiments, we are restricted
to the GoPro training dataset for benchmarking, which does
not adequately cover all real-world scenarios that may be
encountered, such as, saturated regions with poor light con-
ditions, light streaks at night. We observe that almost all

methods fail on deblurring such images and we include fail-
ure cases in appendix. In practice, we believe our method
can further beneﬁt from access to large-scale diverse sets of
training data.
References
[1] J´er´emy Anger, Mauricio Delbracio, and Gabriele Facciolo.
Efﬁcient blind deblurring under high noise levels. In 2019
11th International Symposium on Image and Signal Pro-
cessing and Analysis (ISPA), pages 123–128. IEEE, 2019.
[2] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li,
Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas
Geiping, and Tom Goldstein.
Cold diffusion: Inverting
arbitrary image transforms without noise. arXiv preprint
arXiv:2208.09392, 2022.
[3] Mikołaj Bi´nkowski, Dougal J. Sutherland, Michael Arbel,
and Arthur Gretton. Demystifying MMD GANs. In Inter-
national Conference on Learning Representations, 2018.
[4] Yochai Blau and Tomer Michaeli. The perception-distortion
tradeoff. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pages 6228–6237,
2018.
[5] Joan Bruna, Pablo Sprechmann, and Yann LeCun. Super-
resolution with deep convolutional sufﬁcient statistics. In
International Conference on Learning Representations,
2016.
[6] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Cheng-
peng Chen. Hinet: Half instance normalization network for
image restoration. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR)
Workshops, pages 182–192, June 2021.
[7] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won
Jung, and Sung-Jea Ko.
Rethinking coarse-to-ﬁne ap-
proach in single image deblurring. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 4641–4650, October 2021.
[8] Hyungjin Chung,
Byeongsu Sim,
Dohoon Ryu,
and
Jong Chul Ye.
Improving diffusion models for inverse
problems using manifold constraints.
arXiv preprint
arXiv:2206.00941, 2022.
[9] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-
closer-diffuse-faster:
Accelerating conditional diffusion
models for inverse problems through stochastic contraction.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 12413–12422, 2022.
[10] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
and Mubarak Shah. Diffusion Models in Vision: A Survey.
arXiv preprint arXiv:2209.04747, 2022.
[11] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and
Constantinos Daskalakis. Score-guided intermediate layer
optimization: Fast langevin mixing for inverse problem.
arXiv preprint arXiv:2206.09104, 2022.
[12] Giannis Daras,
Mauricio Delbracio,
Hossein Talebi,
Alexandros G Dimakis, and Peyman Milanfar.
Soft dif-
fusion: Score matching for general corruptions.
arXiv
preprint arXiv:2209.05442, 2022.
[13] Mauricio Delbracio, Hossein Talebei, and Pevman Milan-
far.
Projected distribution loss for image enhancement.
In 2021 IEEE International Conference on Computational
Photography (ICCP), pages 1–12. IEEE, 2021.
[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Informa-
tion Processing Systems, 34:8780–8794, 2021.
[15] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie:
Higher-order denoising diffusion solvers.
arXiv preprint
arXiv:2210.05475, 2022.
[16] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T
Roweis, and William T Freeman. Removing camera shake
from a single photograph. In Acm Siggraph 2006 Papers,
pages 787–794. 2006.
[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. Advances
in neural information processing systems, 27, 2014.
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by
a two time-scale update rule converge to a local nash equi-
librium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wal-
lach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems, vol-
ume 30. Curran Associates, Inc., 2017.
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in Neural Informa-
tion Processing Systems, 33:6840–6851, 2020.
[20] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications, 2021.
[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv:2204.03458, 2022.
[22] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adap-
tation. In International conference on machine learning,
pages 1989–1998. Pmlr, 2018.
[23] Emiel Hoogeboom and Tim Salimans. Blurring diffusion
models. arXiv preprint arXiv:2209.05557, 2022.
[24] Seo-Won Ji, Jeongmin Lee, Seung-Wook Kim, Jun-Pyo
Hong, Seung-Jin Baek, Seung-Won Jung, and Sung-Jea Ko.
Xydeblur: Divide and conquer for single image deblurring.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 17421–17430, 2022.
[25] Meiguang Jin, Stefan Roth, and Paolo Favaro. Normalized
blind deconvolution. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 668–684, 2018.
[26] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
European conference on computer vision, pages 694–711.
Springer, 2016.
[27] Zahra Kadkhodaie and Eero Simoncelli. Stochastic solu-
tions for linear inverse problems using the prior implicit in
a denoiser. Advances in Neural Information Processing Sys-
tems, 34:13242–13254, 2021.

[28] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. arXiv preprint arXiv:2206.00364, 2022.
[29] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. In Advances
in Neural Information Processing Systems, 2022.
[30] Bahjat Kawar, Roy Ganz, and Michael Elad. Enhancing
diffusion-based image synthesis with robust classiﬁer guid-
ance. arXiv preprint arXiv:2208.08664, 2022.
[31] Bahjat Kawar, Jiaming Song, Stefano Ermon, and Michael
Elad.
Jpeg artifact correction using denoising diffusion
restoration models.
arXiv preprint arXiv:2209.11888,
2022.
[32] Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips:
Solving noisy inverse problems stochastically. Advances in
Neural Information Processing Systems, 34:21757–21769,
2021.
[33] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. Advances in neural in-
formation processing systems, 34:21696–21707, 2021.
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[35] Dilip Krishnan, Terence Tay, and Rob Fergus. Blind de-
convolution using a normalized sparsity measure. In CVPR
2011, pages 233–240. IEEE, 2011.
[36] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych,
Dmytro Mishkin, and Jiri Matas. Deblurgan: Blind mo-
tion deblurring using conditional adversarial networks. In
2018 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 8183–8192. IEEE, 2018.
[37] Orest
Kupyn,
Tetiana
Martyniuk,
Junru
Wu,
and
Zhangyang Wang. Deblurgan-v2: Deblurring (orders-of-
magnitude) faster and better.
In The IEEE International
Conference on Computer Vision (ICCV), Oct 2019.
[38] R´emi Laumont, Valentin De Bortoli, Andr´es Almansa, Julie
Delon, Alain Durmus, and Marcelo Pereyra.
Bayesian
imaging using plug & play priors: when langevin meets
tweedie. SIAM Journal on Imaging Sciences, 15(2):701–
737, 2022.
[39] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Ca-
ballero, Andrew Cunningham, Alejandro Acosta, Andrew
Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.
Photo-realistic single image super-resolution using a gener-
ative adversarial network. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
4681–4690, 2017.
[40] Anat Levin, Yair Weiss, Fredo Durand, and William T Free-
man.
Efﬁcient marginal likelihood optimization in blind
deconvolution. In CVPR 2011, pages 2657–2664. IEEE,
2011.
[41] Dasong Li, Yi Zhang, Ka Chun Cheung, Xiaogang Wang,
Hongwei Qin, and Hongsheng Li. Learning degradation
representations for image deblurring. In European Confer-
ence on Computer Vision, pages 736–753. Springer, 2022.
[42] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun
Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single
image super-resolution with diffusion probabilistic models.
Neurocomputing, 479:47–59, 2022.
[43] Jichun Li, Weimin Tan, and Bo Yan. Perceptual variousness
motion deblurring with light global context reﬁnement. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 4116–4125, October 2021.
[44] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song
Han, and Jun-Yan Zhu. Efﬁcient spatially sparse inference
for conditional gans and diffusion models. arXiv preprint
arXiv:2211.02048, 2022.
[45] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo nu-
merical methods for diffusion models on manifolds. In In-
ternational Conference on Learning Representations, 2021.
[46] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. Unsu-
pervised domain-speciﬁc deblurring via disentangled rep-
resentations. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 10225–
10234, 2019.
[47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-
sion probabilistic model sampling in around 10 steps. arXiv
preprint arXiv:2206.00927, 2022.
[48] Hengyuan Ma, Li Zhang, Xiatian Zhu, and Jianfeng Feng.
Accelerating score-based generative models with precon-
ditioned diffusion sampling. In European Conference on
Computer Vision, pages 1–16. Springer, 2022.
[49] Roey Mechrez, Itamar Talmi, Firas Shama, and Lihi Zelnik-
Manor. Maintaining natural image statistics with the con-
textual loss.
In Asian Conference on Computer Vision,
pages 427–443. Springer, 2018.
[50] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The
contextual loss for image transformation with non-aligned
data. In European Conference on Computer Vision (ECCV),
pages 768–783, 2018.
[51] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Er-
mon, Jonathan Ho, and Tim Salimans. On distillation of
guided diffusion models. arXiv preprint arXiv:2210.03142,
2022.
[52] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
Which training methods for gans do actually converge? In
International conference on machine learning, pages 3481–
3490. PMLR, 2018.
[53] Tomer Michaeli and Michal Irani. Blind deblurring using
internal patch recurrence. In Computer Vision–ECCV 2014:
13th European Conference, Zurich, Switzerland, Septem-
ber 6-12, 2014, Proceedings, Part III 13, pages 783–798.
Springer, 2014.
[54] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik.
Making a “completely blind” image quality analyzer. IEEE
Signal processing letters, 20(3):209–212, 2012.
[55] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik
Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu
Lee. Ntire 2019 challenge on video deblurring and super-
resolution: Dataset and study. In CVPR Workshops, June
2019.
[56] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene

deblurring. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
[57] Seungjun Nah, Sanghyun Son, Jaerin Lee, and Kyoung Mu
Lee. Clean images are hard to reblur: Exploiting the ill-
posed inverse task for dynamic scene deblurring. In Inter-
national Conference on Learning Representations, 2022.
[58] Jim Nilsson and Tomas Akenine-M¨oller.
Understanding
ssim. arXiv preprint arXiv:2006.13846, 2020.
[59] Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lian-
ping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao,
and Haifeng Shen.
Single image super-resolution via a
holistic attention network. In European conference on com-
puter vision, pages 191–207. Springer, 2020.
[60] Guy Ohayon, Theo Adrai, Gregory Vaksman, Michael
Elad, and Peyman Milanfar. High perceptual quality image
denoising with a posterior sampling cgan. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 1805–1813, 2021.
[61] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang.
Deblurring text images via l0-regularized intensity and gra-
dient prior.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2901–
2908, 2014.
[62] Jinshan Pan, Deqing Sun, Hanspeter Pﬁster, and Ming-
Hsuan Yang. Blind image deblurring using dark channel
prior. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1628–1636, 2016.
[63] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen.
Hierarchical text-conditional
image generation with clip latents.
arXiv preprint
arXiv:2204.06125, 2022.
[64] Mengwei Ren, Neel Dey, James Fishbaugh, and Guido
Gerig.
Segmentation-renormalized deep feature modula-
tion for unpaired image harmonization. IEEE Transactions
on Medical Imaging, 40(6):1519–1530, 2021.
[65] Jaesung Rim, Geonung Kim, Jungeon Kim, Junyong Lee,
Seungyong Lee, and Sunghyun Cho. Realistic blur syn-
thesis for learning image deblurring. In Proceedings of the
European Conference on Computer Vision (ECCV), 2022.
[66] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun
Cho. Real-world blur dataset for learning and benchmark-
ing deblurring algorithms.
In European Conference on
Computer Vision, pages 184–201. Springer, 2020.
[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684–10695, 2022.
[68] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684–10695, 2022.
[69] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015.
[70] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi.
Palette: Image-to-image diffusion models.
In
ACM SIGGRAPH 2022 Conference Proceedings, pages 1–
10, 2022.
[71] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022.
[72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-
imans, David J Fleet, and Mohammad Norouzi.
Image
super-resolution via iterative reﬁnement.
IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2022.
[73] Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, and
Nong Sang. Domain adaptation for image dehazing. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 2808–2817, 2020.
[74] Ziyi Shen, Wenguan Wang, Jianbing Shen, Haibin Ling,
Tingfa Xu, and Ling Shao. Human-aware motion deblur-
ring. In IEEE International Conference on Computer Vi-
sion, 2019.
[75] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics.
In International Con-
ference on Machine Learning, pages 2256–2265. PMLR,
2015.
[76] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas
Geiping, and Tom Goldstein.
Diffusion art or digital
forgery? investigating data replication in diffusion models.
arXiv preprint arXiv:2212.03860, 2022.
[77] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations, 2021.
[78] Yang Song, Conor Durkan, Iain Murray, and Stefano Er-
mon. Maximum likelihood training of score-based diffu-
sion models. Advances in Neural Information Processing
Systems, 34:1415–1428, 2021.
[79] Yang Song and Stefano Ermon. Generative modeling by
estimating gradients of the data distribution. Advances in
Neural Information Processing Systems, 32, 2019.
[80] Yang Song and Stefano Ermon. Improved techniques for
training score-based generative models. Advances in neural
information processing systems, 33:12438–12448, 2020.
[81] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential
equations. In International Conference on Learning Repre-
sentations, 2021.
[82] Maitreya Suin, Kuldeep Purohit, and A. N. Rajagopalan.
Spatially-attentive patch-hierarchical network for adaptive
motion deblurring.
In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2020.
[83] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi
Tsai, and Chia-Wen Lin.
Stripformer: Strip transformer

for fast image deblurring. In Proceedings of the European
Conference on Computer Vision (ECCV), 2022.
[84] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim:
Multi-axis mlp for image processing.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5769–5780, 2022.
[85] Wei Wang, Haochen Zhang, Zehuan Yuan, and Changhu
Wang.
Unsupervised real-world super-resolution: A do-
main adaptation perspective.
In 2021 IEEE/CVF Inter-
national Conference on Computer Vision (ICCV), pages
4298–4307, 2021.
[86] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 606–615, 2018.
[87] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A gen-
eral u-shaped transformer for image restoration. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 17683–17693,
June 2022.
[88] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan
Saharia, Alexandros G Dimakis, and Peyman Milanfar. De-
blurring via stochastic reﬁnement. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16293–16303, 2022.
[89] Valentin Wolf, Andreas Lugmayr, Martin Danelljan, Luc
Van Gool, and Radu Timofte. Deﬂow: Learning complex
image degradations from unpaired data with conditional
ﬂows.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 94–103,
2021.
[90] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling
the generative learning trilemma with denoising diffusion
GANs. In International Conference on Learning Represen-
tations (ICLR), 2022.
[91] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse
representation for natural image deblurring. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 1107–1114, 2013.
[92] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efﬁcient transformer for high-resolution image
restoration. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2022.
[93] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for real image restoration
and enhancement. In European Conference on Computer
Vision, pages 492–511. Springer, 2020.
[94] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2021.
[95] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr
Koniusz. Deep stacked hierarchical multi-patch network for
image deblurring. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
5978–5986, 2019.
[96] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Lin-
chao Bao, Rynson WH Lau, and Ming-Hsuan Yang. Dy-
namic scene deblurring using spatially variant recurrent
neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2521–
2529, 2018.
[97] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn
Stenger, Wei Liu, and Hongdong Li. Deblurring by realis-
tic blurring. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2737–
2746, 2020.
[98] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In IEEE conference on
computer vision and pattern recognition, pages 586–595,
2018.
[99] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2018.
[100] Xuaner Zhang, Qifeng Chen, Ren Ng, and Vladlen Koltun.
Zoom to learn, learn to zoom. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 3762–3770,
2019.
[101] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Led-
net: Joint low-light enhancement and deblurring in the dark.
arXiv preprint arXiv:2202.03373, 2022.
[102] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie,
Wangmeng Zuo, and Jimmy Ren.
Spatio-temporal ﬁlter
adaptive network for video deblurring. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 2482–2491, 2019.

Appendix
A. Additional Results
A.1. Effectiveness of the guidance on GoPro, HIDE and REDS
We include additional perception-distortion plots for GoPro [56], HIDE [74] and REDS [55] datasets in Fig. 9, as supple-
mentary for Section 4.3 of the main paper, to verify the effectiveness of the proposed guidance.
28.0
28.5
29.0
29.5
30.0
Distortion (PSNR)
8.5
9.0
9.5
10.0
10.5
11.0
11.5
Perception (1/LPIPS)
HIDE (Out-of-domain)
GoPro (In-domain)
REDS (Out-of-domain)
26.0
26.2
26.4
26.6
26.8
Distortion (PSNR)
5.6
5.8
6.0
6.2
6.4
6.6
6.8
Perception (1/LPIPS)
30.0
30.5
31.0
31.5
32.0
32.5
33.0
Distortion (PSNR)
11
12
13
14
15
16
17
Perception (1/LPIPS)
icDPM-S
icDPM-L
icDPM-S w/ Guide-S
icDPM-L w/ Guide-L
Figure 9. Additional perception-distortion plots as supplementary for Sec. 4.3 in the main paper. All models are trained only on GoPro [56].
The guidance mechanism allows for consistent better perceptual qualities and lower distortions compared to image-conditioned diffusion
probablistic model (icDPM) under different network capacities (’icDPM-S w/ Guide-S’ > ‘icDPM-S’; ‘icDPM-L w/ Guide-L’ > ‘icDPM-
L’), both in-domain (GoPro) and out-of-domain (HIDE, REDS) ‘-S’ and ‘-L’ refer to small and large networks respectively.
A.2. Additional visual results
To supplement main paper Fig.6,7,8, we provide additional and enlarged qualitative results for all datasets below.
Realblur-J (out-of-domain) deblurring examples are shown in Fig. 25, 26, 27, 28.
REDS (out-of-domain) deblurring examples are shown in Fig. 13, 14, 15, 16.
HIDE (out-of-domain) deblurring examples are shown in Fig. 17, 18, 19, 20.
GoPro (in-domain) deblurring examples are shown in Fig. 21, 22, 23, 24.

A.3. Guidance feature
We qualitatively analyze the channelwise guidance feature maps in Fig. 10. In general, these feature maps are qualitatively
related to edges and overall structures, which provide as auxiliary information to the icDPM about the coarse structures of
its sharp reconstruction at multiple resolutions. We empirically verify that using such features as additional guidance is
beneﬁcial for qualitatively/quantitatively more robust deblurring, as well as improved domain generalization.
Figure 10. An out-of-domain deblurring result on a test image from Realblur-J [66] (left), alongside 12 (out of 64) selected channelwise
guidance feature maps at the scale of k = 1 (right).
A.4. Failure cases
As discussed in the main paper, we acknowledge that the domain generalization of the model is still extensively bounded
by the quality of the training set. In our experiments, we only train with GoPro [56] for the sake of benchmarking. However,
the data diversity and representativity from GoPro is limited, i.e., it only contains daytime scenes, acquired outdoor under
sufﬁcient lighting conditions. Moreover, the synthesis of blur in GoPro by simple averaging of consecutive frames is less
realistic [101]. Lastly, the ground truth images in GoPro dataset are rather low-quality, which may further hurt the out-of-
domain performance. Therefore, it is expected that it will be extremely hard for the model to perform decent deblurring on
scenes signiﬁcantly different from GoPro, such as low-light images with saturated regions, in Realblur-J [66].
We include a few failure cases on such scenes in Fig. 29, 30, 31 32, where all methods fail to remove blur from the night
scenes, especially with night streaks. We believe that in practice, more realistic training datasets [65] will further increase the
model generalization.
B. Additional Ablation
Input concatenation During prototyping, we also explored the possibility of removing input-level concatenation, and
only rely on the intermediate representations from regression as the condition of the diffusion model, similar as in [42] for
super-resolution. Potentially, we expect such setting will further make the model domain-generalizable as it does not directly
interact with images from different domains, although it may also risk losing detailed information from the input.
As proof of concept, we use the same multiscale regression networks, and compare the models with or without input
concatenation. Further, since the diffusion model now only takes the intermediate representations as input, we reintroduce
the RGB information by using our model variants (d) in Table 8. in the main paper (i.e., regression targets are downsampled
RGB images instead of grayscale images). From Table 10, we observe that the input concatenation obtained a much better
performance both in-domain and out-of-domain than without concatenation. Therefore, in our ﬁnal model, we keep the input
concatenation and only rely on the guidance features to provide additional information.

Table 10. Effects of input-level concatenation. From our model variant (d) in Table 8. of the main paper, we remove the input concatenation,
loosely inspired by [42] (super-resolution). In the context of deblurring, we observe deteriorate results indicated in row ‘w/o input
concatenation’, compared to the setting with additional input concatenation.
In-domain
Out-of-domain
PSNR ↑
LPIPS ↓
PSNR ↑
LPIPS ↓
w/o input concatenation
25.20
0.230
28.29
0.177
w/ concatenation
30.65
0.090
28.45
0.141
Further cross-domain alignment. We also explored the potential effects of ﬁnetuning the DPMs with adversarial for-
mulation where we used additional discriminators on the guidance features between different datasets (e.g., GoPro and
Realblur-J) so that the features extracted from different domains become indistinguishable, similar to the feature alignment
strategy in [85]. However, we do not observe extra beneﬁts, and ﬁnd that such ﬁnetuning may even hurt the performance as
shown in Fig. 11. We speculate that it could be a result of training instability of GANs, or perhaps the suboptimal formulation
under the image-conditioned DPM framework. We will leave this for future investigation.
28.45 28.50 28.55 28.60 28.65 28.70 28.75 28.80
Distortion (PSNR)
7.0
7.2
7.4
7.6
7.8
8.0
Perception (1/LPIPS)
w/o adaptation
w/ adaptation
Figure 11. A comparison between our models with or without further domain adaptation with Realblur-J, on a GoPro trained model.
Surprisingly, further adversarial domain adaptation on the guidance features between GoPro and Realblur-J hurt the performance.

C. Additional implementation details
C.1. Architectures
The architectural details for the diffusion network and the guidance network are illustrated in Fig. 12.
xt
h⨉w⨉3
y
h⨉w⨉3
conv 3x3
h⨉w⨉c
ResBlock
h⨉w⨉2c
Downsample
h/2⨉w/2⨉2c
Guided ResBlock
h/2⨉w/2⨉3c
Guided ResBlock
h/4⨉w/4⨉4c
Downsample
h/8⨉w/8⨉4c
Downsample
h/4⨉w/4⨉3c
Guided ResBlock
h/8⨉w/8⨉4c
Guided ResBlock
h/8⨉w/8⨉4c
Upsample
h/4⨉w/4⨉4c
ResBlock
h/4⨉w/4⨉3c
Upsample
h/2⨉w/2⨉3c
ResBlock
h/2⨉w/2⨉2c
Upsample
h⨉w⨉2c
ResBlock
h⨉w⨉c
conv 3x3
h⨉w⨉3
concatenate
𝜖
h⨉w⨉3
Input
h/n⨉w/n⨉c
Guidance
h/n⨉w/n⨉4d
swish
conv 3x3
Output
h/n⨉w/n⨉c
αt
conv 1x1
swish
Position 
encoding
MLP
Guided 
ResBlock
conv 1x1
residual
y
h⨉w⨉3
ResBlock
h/n⨉w/n⨉d
ResBlock
h/n⨉w/n⨉2d
ResBlock
h/n⨉w/n⨉3d
ResBlock
h/n⨉w/n⨉4d
conv 3x3
h⨉w⨉1
RGB to Grayscale,
Downsample
h/n⨉w/n⨉1
conv 3x3
h/n⨉w/n⨉c
x’
h⨉w⨉3
y
Blurry input
𝜖
Predicted noise
x
Clean target
xt
Partially noisy image
αt
Noise at diffusion step t
x’
Predicted downsampled 
and grayscale clean image
+
+
swish
conv 3x3
Input
h/n⨉w/n⨉c
swish
conv 3x3
Output
h/n⨉w/n⨉c
αt
swish
Position 
encoding
MLP
ResBlock
residual
+
+
swish
conv 3x3
Figure 12. The detailed architecture of the proposed method. Left: the image-conditioned diffusion network based on a fully-convolutional
UNet similar to [88], where we replace the residual blocks from the UNet encoder with the proposed guided residual block. Middle
column illustrates the difference between a standard residual block and the proposed guided block, where we additionally incorporate
multiscale structure guidance. Right: The proposed guidance network for extracting the coarse structure features from the input at multiple
resolutions. At each scale, the blurry image is ﬁrst converted to grayscale, downsampled, and lastly fed into the network to predict its clean
counterpart. The output from the last residual block is leveraged as the guidance feature.

C.2. Inference
As we use continuous noise level sampling during training, it enables the use of different noise schedulers during the
inference to potentially obtain samples with different distortion-perception trade-off. We therefore perform a grid search over
a set of different diffusion steps T, as well as the upper bound of the noise variance 1 −αT . For efﬁciency, we also exclude
certain combinations that do not produce reasonable sampling (i.e., sampling results are pure noise or blank image), and the
ﬁnal combinations are indicated in Table 11.
Table 11. The sampling parameters for inference.
Maximum noise variance 1 −αT
0.01
0.02
0.05
0.1
0.2
0.5
Steps (T)
20
✓
30
✓
50
✓
✓
100
✓
✓
✓
200
✓
✓
✓
✓
500
✓
✓
✓
✓
1000
✓
✓
✓
✓
C.3. Computational cost
In Table 12, we report ﬂoating point operations per second (FLOPs) under different model conﬁgurations, calculated
based on an input image of 720 × 1280 × 3. For diffusion networks (c)-(d), the FLOPS are calculated based on a single
diffusion step. While optimizing sampling speed is out-of-scope of this work, we believe recent advance in speeding up
DPM sampling [77, 9, 90, 45, 47, 48, 15, 51, 44] could be further incorporated into our framework.
Table 12. FLOPs under different model conﬁgurations, calculated based on a full-size input image of 720 × 1280 × 3. For diffusion
networks (c)-(d), the FLOPs are calculated based on a single diffusion step.
Guidance network
Diffusion network
# Params
FLOPs
(a) icDPM-S
-
ch=32
6M
1200B
(b) icDPM-L
-
ch=64
27M
4800B
(c) icDPM-S w/ Guide-S
ch=32
ch=32
10M
2500B
(d) icDPM-L w/ Guide-L
ch=64
ch=64
52M
10000B
C.4. Benchmark results
We performed a consistent computation over all benchmarks for fair comparisons. To acquire the benchmark results, we
use the author provided results whenever possible. On the cross-domain set up of Realblur-J with GoPro trained only models,
we use author provided results of DvSR [88], UFormer [87], Restormer [92]. For DeblurGAN-v2 [37] and MPRNet [94], we
use ofﬁcial code repository along with the provided GoPro checkpoints for inference. On REDS [55], all results are obtained
by running their ofﬁcial models with the GoPro checkpoints.

HINet
Restormer
Ours
MPRNet
UFormer
Reference
Input
DeblurGAN-v2
icDPM w/o guidance
Figure 13. REDS [55] deblurring examples from MPRNet [94], HINet [6], DeblurGAN-v2 [37], Restormer [92], UFormer [87], icDPM
without guidance and Ours (icDPM with guidance).

HINet
Restormer
Ours
MPRNet
UFormer
Reference
Input
DeblurGAN-v2
icDPM w/o guidance
Figure 14. REDS [55] deblurring examples from MPRNet [94], HINet [6], DeblurGAN-v2 [37], Restormer [92], UFormer [87], icDPM
without guidance and Ours (icDPM with guidance).

HINet
Restormer
Ours
MPRNet
UFormer
Reference
Input
DeblurGAN-v2
icDPM w/o guidance
Figure 15. REDS [55] deblurring examples from MPRNet [94], HINet [6], DeblurGAN-v2 [37], Restormer [92], UFormer [87], icDPM
without guidance and Ours (icDPM with guidance).

HINet
Restormer
Ours
MPRNet
UFormer
Reference
Input
DeblurGAN-v2
icDPM w/o guidance
Figure 16. REDS [55] deblurring examples from MPRNet [94], HINet [6], DeblurGAN-v2 [37], Restormer [92], UFormer [87], icDPM
without guidance and Ours (icDPM with guidance).

Figure 17. HIDE [74] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

UFormer
MIMO UNet+
Reference
SAPHNet
Input
DvSR
Restormer
Ours
MPRNet
Figure 18. HIDE [74] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

UFormer
MIMO UNet+
Reference
SAPHNet
Input
DvSR
Restormer
Ours
MPRNet
Figure 19. HIDE [74] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 20. HIDE [74] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 21. GoPro [56] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

MIMO UNet+
Restormer
Ours
MPRNet
UFormer
Reference
Input
SAPHNet
DvSR
Figure 22. GoPro [56] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 23. GoPro [56] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 24. GoPro [56] deblurring examples from MPRNet [94], MIMO UNet+ [7], SAPHNet [82], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 25. Realblur-J [66] deblurring examples from UNet [69], MPRNet [94], DeblurGAN-v2 [37], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 26. Realblur-J [66] deblurring examples from UNet [69], MPRNet [94], DeblurGAN-v2 [37], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 27. Realblur-J [66] deblurring examples from UNet [69], MPRNet [94], DeblurGAN-v2 [37], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Figure 28. Realblur-J [66] deblurring examples from UNet [69], MPRNet [94], DeblurGAN-v2 [37], Restormer [92], UFormer [87],
DvSR [88] and Ours.

Input
DeblurGAN-v2
MPRNet
Restormer
UFormer
DvSR
Ours
Reference
UNet
Figure 29. Failure case from Realblur-J [66] in low-light scenes.

Input
DeblurGAN-v2
DvSR
Restormer
Ours
UNet
MPRNet
UFormer
Reference
Figure 30. Failure case from Realblur-J [66] with strong light streaks.

Input
DeblurGAN-v2
DvSR
Restormer
Ours
UNet
MPRNet
UFormer
Reference
Figure 31. Failure case from Realblur-J [66] in night scenes.

Figure 32. Failure case from Realblur-J [66] in low-light condition.

