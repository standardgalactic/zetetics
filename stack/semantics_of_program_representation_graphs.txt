10
Semantics of program representation graphs
G. Ramalingam
Microsoft Research India; Bangalore, India
Thomas Reps
University of Wisconsin; Madison, WI; USA
Dedicated to the memory of Gilles Kahn, 1946-2006.
Abstract
Program representation graphs (PRGs) are an intermediate represen-
tation for programs. (They are closely related to program dependence
graphs.) In this paper, we develop a mathematical semantics for PRGs
that, inspired by Kahn’s semantics for a parallel programming language,
interprets PRGs as dataﬂow graphs. We also study the relationship
between this semantics and the standard operational semantics of
programs. We show that (i) the semantics of PRGs is more deﬁned
than the standard operational semantics, and (ii) for states on which
a program terminates normally, the PRG semantics is identical to the
standard operational semantics.
10.1 Introduction
In this paper, we develop a mathematical semantics for program
representation graphs (PRGs) and study its relationship to a standard
(operational) semantics of programs. Program representation graphs are
an intermediate representation of programs, introduced by Yang et al.
[27] in an algorithm for detecting program components that exhibit
identical execution behaviors. They combine features of static-single-
assignment forms (SSA forms) [23, 2, 5, 21] and program dependence
graphs (PDGs) [13, 6, 11]. (See Fig. 10.1 for an example program and its
PRG.) PRGs have also been used in an algorithm for merging program
variants [28].
From Semantics to Computer Science Essays in Honour of Gilles Kahn,
eds Yves
Bertot, G´erard Huet, Jean-Jacques L´evy and Gordon Plotkin. Published by Cambridge
University Press.
c
⃝Cambridge University Press 2009.
205
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

206
G. Ramalingam and T. Reps
Program dependence graphs have been used as an intermediate
program representation in various applications such as vectorization,
parallelization [13], and merging program variants [11]. A number
of variants of the PDG have been used as the basis for eﬃcient
program analysis by optimizing compilers as well as other tools (e.g.,
see [15, 17, 24]). All these uses of dependence graph representations
vitally depend on a usually unstated or unproven assumption that the
representation adequately captures the program semantics. Horwitz
et al. [10] were the ﬁrst to address the question of whether PDGs were
“adequate” as program representations. They showed (for a simpliﬁed
programming language) that if the program dependence graphs of two
programs are isomorphic, the programs are equivalent in the following
sense: for any initial state σ, either both programs diverge or both halt
with the same ﬁnal state.
Such an equivalence theorem makes it reasonable to try to develop
a semantics for program dependence graphs that is consistent with the
program semantics. In contrast to the indirect proof of the equivalence
theorem given in [10], such a semantics would provide a direct proof of
the theorem.
Two diﬀerent semantics have so far been developed for PDGs (and
thus each provides a direct proof of the equivalence theorem). Selke
[22] provides a graph-rewriting semantics for PDGs. This semantics
represents computation steps as graph transformations. The dependence
edges are used to make sure that statements are executed in the right
order. The store is embedded in the graph. When assignment statements
are executed, the relevant portions of the graph are updated to reﬂect
the new value of the corresponding variable. Evaluation of if predicates
results in deletion of the part of the graph representing the true or
false branch, as appropriate. Evaluation of while predicates results in the
deletion of the body of the loop or creating a copy of it, as necessary.
Cartwright and Felleisen [4] start with a non-strict generalization
of the denotational semantics of the programming language and
use a staging analysis to decompose the meaning function into two
functions: a compiler function that transforms programs into code trees,
which resemble PDGs, and an interpreter function for code trees. The
interpreter function provides an operational semantics for code trees.
A diﬀerent (and perhaps more natural) way to develop a semantics for
program dependence graphs would be to treat them as graphs of some
dataﬂow programming language and use the conventional operational
semantics of such programming languages. Although analogies between
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
207
PDGs and dataﬂow graphs have been made previously, this idea has not
actually been formalized (i.e., to date no semantics has been developed
that interprets PDGs as dataﬂow graphs). In fact, there are some
problems in doing so, as will be explained in Section 10.4.
In [12], Kahn introduced a model for parallel or distributed
computation consisting of a group of processing units connected by
unidirectional communication channels to form a network of processes.
In this setting, the output of each process (i.e. the possibly inﬁnite
sequence of values on the output channel of the process) is deﬁned as a
function of the sequence of values on the input channels of the process. In
this paper, we show that, with minor modiﬁcations, PRGs—as opposed
to PDGs—can be interpreted in a very similar fashion, with each vertex
being interpreted as a process and each dependence edge as a communi-
cation channel. That is, we show how to develop a mathematical
semantics for PRGs by formalizing the analogy between PRGs and the
dataﬂow graphs used by Kahn in developing a semantics for a parallel
programming language. We create a set of possibly mutually recursive
equations that, as a function of the initial store, associate a sequence
of values with each vertex in the PRG. The semantics of the PRG is
deﬁned to be the least-ﬁxed-point solution of these equations.
The dataﬂow semantics for PRGs can be restricted so as to give a
semantics for PRGs as store-to-store transformers. However, for some
applications of PRGs, such as merging program variants, the more
general semantic deﬁnition is preferable. The more general semantic
deﬁnition also leads to a stronger form of the equivalence theorem for
PRGs that relates the sequences of values computed at corresponding
vertices of programs that have isomorphic PRGs.
In particular, we show that: (1) the sequence of values computed at any
program point (according to the operational semantics) is, in general,
a preﬁx of the sequence associated with that program point by the
PRG semantics and (2) for normally terminating program executions
the two sequences are identical. This yields the following equivalence
theorem: If the PRGs of two programs are isomorphic, then for any
initial state σ, either (1) both programs terminate normally, and the
sequence of values computed at corresponding vertices are equal, or (2)
neither program terminates normally and for any pair of corresponding
vertices, the sequence of values computed at one of them will be a preﬁx
of the sequence of values computed at the other. A similar equivalence
theorem for program slices also follows as a consequence: If the PRG
of one program is isomorphic to a subgraph of the PRG of another
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

208
G. Ramalingam and T. Reps
program, then whenever the second program terminates normally,
the ﬁrst program will also terminate normally, and produce the same
sequence of values at every program point as the sequence produced by
the second program at the corresponding program point. Indirect proofs
of such equivalence theorems have been previously derived for PDGs [20]
and PRGs [26]; this paper provides the ﬁrst direct proof of the result.
The remainder of the paper is organized as follows: Section 10.2
describes the programming language under consideration. Section 10.3
deﬁnes program representation graphs, and Section 10.4 extends this
deﬁnition. Section 10.5 presents the semantics of PRGs. Section 10.6
deals with various properties of the standard operational semantics.
Section 10.7 considers the relationship between a program’s standard
operational semantics and the semantics of its PRG. Section 10.9
discusses related work. (In the interests of brevity, two of the proofs
have been omitted; they may be found in reference [18].)
10.2 The programming language under consideration
We are concerned with a programming language with the following
characteristics: expressions contain only scalar variables and constants;
statements are either assignment statements, conditional statements,
while-loops, or end statements. An end statement, which can only appear
at the end of a program, names zero or more of the variables used in
the program. The variables named in the end statement are those whose
ﬁnal values are of interest to the programmer. An example program is
shown in the upper-left-hand corner of Figure 10.1 below.
Our discussion of the language’s semantics is in terms of the following
informal model of execution. We assume a standard operational semantics
for sequential execution; the statements and predicates of a program
are executed in the order speciﬁed by the program’s control-ﬂow graph;
at any moment there is a single locus of control; the execution of each
assignment statement or predicate passes control to a single successor;
the execution of each assignment statement changes a global execution
state. An execution of the program on an initial state yields a (possibly
inﬁnite) sequence of values for each predicate and assignment statement
in the program; the ith element in the sequence for program component
c consists of the value computed when c is executed for the ith time.
10.3 Program representation graphs
As mentioned previously, PRGs combine features of SSA forms and
PDGs. In the SSA form of a program, special assignment statements (φ
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
209
assignments) are inserted so that exactly one assignment to a variable x,
either an assignment from the original program or a φ assignment, can
reach a use of x from the original program. The φ statements assign the
value of a variable to itself; at most two assignments to a variable x can
reach the use of x in a φ statement. For instance, consider the following
example program fragments:
L1:
x : = 1
if p then
L2:
x : = 2
ﬁ
L4:
y : = x + 3
L1:
x : = 1
if p then
L2:
x : = 2
ﬁ
L3:
x : = φif(x)
L4:
y : = x + 3
In the source program (on the left), both assignments to x at L1 and L2
can reach the use of x at L4; after the insertion of “x : = φif(x)” at L3
(on the right), only the φ assignment to x can reach the use of x at L4.
Both assignments to x at L1 and L2 can reach the use of x at L3.
Diﬀerent deﬁnitions of program dependence graphs have been given,
depending on the intended application; nevertheless, they are all
variations on a theme introduced in [14], and share the common feature
of having an explicit representation of data dependences. The program
dependence graph deﬁned in [6] introduced the additional feature
of an explicit representation for control dependences. The program
representation graph, deﬁned below, has edges that represent control
dependences and one kind of data dependence, called ﬂow dependence.
The program representation graph of a program P, denoted by RP ,
is constructed in two steps. First an augmented control-ﬂow graph is
built and then the program representation graph is constructed from
the augmented control-ﬂow graph. An example program, its augmented
control-ﬂow graph, and its program representation graph are shown in
Fig. 10.1.
Step 1.
The control-ﬂow graph1 [1] of program P is augmented by
adding Initialize, FinalUse, φif, φEnter, and φExit vertices, as follows:
(i) A vertex labeled “x : =
Initializex” is added at the beginning of
the control-ﬂow graph for each variable x that may be used before
1 In control-ﬂow graphs, vertices represent the program’s assignment statements and
predicates; in addition, there are two additional vertices, Start and Exit, which
represent the beginning and the end of the program. The Start vertex is interpreted
as an if predicate that evaluates to true, and the whole program is interpreted as
the true branch of the if statement (see [6]).
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

210
G. Ramalingam and T. Reps
program Main
sum : = 0
x : = 1
while x < 11 do
sum : = sum + x
x : = x + 1
od
result : = result + sum
end(result)
result := result + sum
FinalUse(result)
result := Initializeresult
Start
sum := 0
x := 1
sum := φenter(sum)
x := φenter(x)
while x < 11
sum := sum + x
x := x + 1
sum := φexit(sum)
Start
result := Initializeresult
sum := 0
x := 1
sum := φenter(sum)
x := φenter(x)
while x < 11
sum := sum + x
x := x + 1
sum := φexit(sum)
result := result + sum
FinalUse(result)
Exit
T
F
T
F
Fig. 10.1. An example program is shown on the top left. This example sums
the integers 1 to 10 and adds the sum to the variable result. On the right is the
augmented control-ﬂow graph for the program. Note the absence of Initialize
and FinalUse vertices for sum and x and of a φExit vertex for x. On the
bottom left is the program representation graph for the program. Note that
there is a control-dependence edge from the while predicate x < 11 to itself.
The boldface arrows represent control-dependence edges; thin arrows represent
ﬂow-dependence edges. The label on each control-dependence edge—true or
false—has been omitted.
being deﬁned in the program. (We say that a variable x may be used
before being deﬁned if there exists some path in the control-ﬂow graph
from the Start vertex to a vertex v that uses x such that none of the
vertices in the path, excluding v, contains an assignment to variable
x.) If there are many Initialize vertices for a program, their relative
order is not important as long as they come immediately after the
Start vertex.
(ii) A vertex labeled “FinalUse(x)” is added at the end of the control-
ﬂow graph for each variable x that appears in the end statement
of the program. If there are many FinalUse vertices for a program,
their relative order is not important as long as they come immediately
before the Exit vertex.
(iii) For every variable x that is deﬁned within an if statement, and that
may be used before being redeﬁned after the if statement, a vertex
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
211
labeled “x : = φif(x)” is added immediately after the if statement. If
there are many φif vertices for an if statement, their relative order is
not important as long as they come immediately after the if statement.
(iv) For every variable x that is deﬁned inside a loop, and that may be used
before being redeﬁned inside the loop or may be used before being
redeﬁned after the loop, a vertex labeled “x : = φEnter(x)” is added
immediately before the predicate of the loop. If there are many φEnter
vertices for a loop, their relative order is not important as long as
they come immediately before the loop predicate. After the insertion
of φEnter vertices, the ﬁrst φEnter vertex of a loop becomes the entry
point of the loop.
(v) For every variable x that is deﬁned inside a loop, and that may be used
before being redeﬁned after the loop, a vertex labeled “x : = φExit(x)”
is added immediately after the loop. If there are many φExit vertices
for a loop, their relative order is not important as long as they come
immediately after the loop.
Note that φEnter vertices are placed inside of loops, but φExit vertices
are placed outside of loops.
Step 2.
Next, the program representation graph is constructed from
the augmented control-ﬂow graph. The vertices of the program represen-
tation graph are those in the augmented control-ﬂow graph (except the
Exit vertex). Edges are of two kinds: control-dependence edges and ﬂow-
dependence edges.
A control-dependence edge from a vertex u, representing an if or
while predicate, to a vertex v, denoted by u →c v, means that, during
execution, whenever the predicate represented by u is evaluated and
its value matches the label—true or false—on the edge to v, then the
program component represented by v will eventually be executed if the
program terminates normally. Recall that we model the Start statement
as an if predicate. Thus, the source of any control-dependence edge is
either the Start vertex or some other predicate vertex.
There is a control-dependence edge from a predicate vertex u to a
vertex v if, in the augmented control-ﬂow graph, v occurs on every path
from u to Exit along one branch out of u but not the other. This control-
dependence edge is labeled by the truth value of the branch in which v
always occurs.
Note that there is a control-dependence edge from a while predicate to
itself. Methods for determining control-dependence edges for programs
with unrestricted ﬂow of control are given in [6, 5]; however, for our
restricted language, control-dependence edges can be determined in a
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

212
G. Ramalingam and T. Reps
simpler fashion: Except for the extra control-dependence edge incident
on a φEnter vertex, the control-dependence edges merely reﬂect the
nesting structure of the program.
A ﬂow-dependence edge from a vertex u to a vertex v, denoted by
u →f v, means that the value produced at u may be used at v. There is
a ﬂow-dependence edge u →f v if there is a variable x that is assigned a
value at u and used at v, and there is an x-deﬁnition-free path from u to
v in the augmented control-ﬂow graph. The ﬂow-dependence edges of a
program representation graph can be computed using dataﬂow analysis.
The imported variables of a program P, denoted by ImpP , are the
variables that might be used before being deﬁned in P, i.e. the variables
for which there are Initialize vertices in the PRG of P.
Textually diﬀerent programs may have isomorphic program represen-
tation graphs. However, it has been shown that if two programs have
isomorphic program representation graphs, then the programs are
semantically equivalent:
Theorem 10.1 (equivalence theorem for PRGs [26].) Suppose that
P and Q are programs for which RP is isomorphic to RQ. If σ is a
state on which P halts, then for any state σ′ that agrees with σ on the
imported variables of P,
(i) Q halts on σ′,
(ii) P and Q compute the same sequence of values at each corres-
ponding program component, and
(iii) the ﬁnal states of P and Q agree on all variables for which there
are ﬁnal-use vertices in RP and RQ.
10.4 Extensions to program representation graphs
Our aim is to treat PRGs as pure dataﬂow graphs. Dataﬂow graphs are
a model of parallel computation, where vertices represent computing
agents and edges represent unidirectional communication channels.
Values computed at one vertex are transmitted to other vertices along
the edges. In this model, the sequence of values “ﬂowing” along an edge
out of vertex u is a function of the sequences of values “ﬂowing” along
edges incident on vertex u.
The trouble with treating PDGs (as opposed to PRGs) as dataﬂow
graphs is that multiple deﬁnitions of a variable may reach a vertex.
In contrast, vertices in dataﬂow graphs tend to have only one incident
edge per variable, the only exception being certain control vertices that
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
213
choose the value from one of two incident edges based on a Boolean
input. In contrast with PDGs, PRGs resemble dataﬂow graphs in this
respect—normally only one deﬁnition of any variable reaches any vertex.
The exceptions are the φif and φEnter vertices, which are reached by
two deﬁnitions of a variable. Both φif and φEnter vertices are associated
with predicate nodes, and are similar to the control vertices in dataﬂow
graphs.
There is a small problem in treating PRGs as dataﬂow graphs. If
u →f v is a data dependence, then a particular value computed at u
may be used zero or more times at v! However, in dataﬂow graphs a
value ﬂowing along an edge is consumed exactly once. To get around this
problem, we introduce several new kinds of φ nodes that can consume
unused data or duplicate them a certain number of times. These extra
nodes make it possible to view PRGs as dataﬂow graphs and simplify
the deﬁnition of PRG semantics.
The essential idea is to replace all data dependences u →f v that can
cause the above-mentioned problem by two data dependences u →f w
and w →f v, where w is an appropriate φ node, as described below.
(i) We ﬁrst consider any data dependence u →f v such that the value
computed at u gets used at v only if an if predicate t evaluates to
true at the appropriate instance. We introduce a φT vertex that,
in the dataﬂow semantics, acts as a ﬁlter that transmits only those
values that correspond to the if predicate evaluating to true. Let t
be an if statement predicate and u a vertex outside the if statement.
If there exists at least one vertex v such that (1) u →f v is an
edge in the PRG and (2) v is either in the true branch of the if
statement or is a φif vertex associated with the if statement such
that the deﬁnition u reaches v along the true branch of the control-
ﬂow graph, then introduce a φT vertex for the variable deﬁned in
u. Let w denote the new vertex. Add the control-dependence edge
t →c w labeled true and the data dependence edge u →f w. For
each vertex v satisfying the above condition, replace u →f v by the
edge w →f v. See Figure 10.2 for an illustration of this deﬁnition.
Similarly, φF vertices are introduced in the false branches of if
statements.
(ii) We now consider any data dependence u →f v such that the value
computed at u may be used several times at v due to multiple
iterations of a while loop. We introduce a φcopy vertex that, in the
dataﬂow semantics, creates multiple copies of the value computed
at u, one for each evaluation of the while predicate during the
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

214
G. Ramalingam and T. Reps
Start
u: x := 0
t: if P
v:  y := x
T
T
T
R1:
Start
u: x := 0
t: if P
v:  y := x
T
T
T
w:  x := φT(x)
T
R2:
Fig. 10.2. The above example shows how φT vertices are introduced into
PRGs.
execution of the loop. Let t be a while statement predicate and u
a vertex outside the while statement. If there exists at least one
non-φEnter vertex v inside the while statement such that u →f v is
an edge in the PRG, then introduce a φcopy vertex for the variable
deﬁned in u. Let w denote the new vertex. Add control dependences
t →c w and s →c w, where s is t’s parent, just as for a φEnter
vertex. Add data dependence u →f w. For each vertex v satisfying
the above condition, replace u →f v by w →f v. See Figure 10.3
for an illustration of this deﬁnition.
(iii) We now consider any data dependence u →f v such that the value
computed at u gets used at v only if a while predicate t evaluates to
true at the appropriate instance. We introduce a φwhile vertex that,
in the dataﬂow semantics, ﬁlters out the values corresponding to an
evaluation of the while predicate to false. Let t be a while statement
predicate and u a φEnter or φcopy vertex associated with the while
statement. If there exists at least one vertex v ̸= t inside the while
statement such that u →f v is an edge in the PRG, then introduce
a φwhile vertex w corresponding to the relevant variable. Add the
control dependence t →c w labeled true and the data dependence
u →f w. For each vertex v satisfying the above condition, replace
u →f v by w →f v. See Figure 10.4 for an illustration of this
deﬁnition.
Note: As may be seen from the informal explanations above, and
the formal semantics presented later, φwhile and φT vertices have the
same semantics and, hence, could be represented using the same type
of φ vertex. Similarly, φExit and φF vertices have the same semantics.
However, we retain these distinct names for their mnemonic value.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
215
Start
u: x := 0
t: while P
v:  y := x
T
T
T
R3:
Start
u: x := 0
t: while P
v:  y := x
T
T
T
w:  x := φcopy(x)
T
R4:
T
Fig. 10.3. The above example shows how φcopy vertices are introduced into
PRGs. Note that a φwhile vertex will subsequently be introduced between w
and v.
Start
i := 1
t: while i < 10
v:  i := i + 1
T
T
T
u:  i := φenter(i)
T
R5:
T
Start
i := 1
t: while i < 10
v:  i := i + 1
T
T
T
u:  i := φenter(i)
T
R6:
T
w:  i := φwhile(i)
T
Fig. 10.4. The above example shows how φwhile vertices are introduced into
PRGs.
The above transformations are performed in the following order:
Traverse the control-dependence subtree of the PRG in a top-down
fashion. For each predicate vertex t, for each suitable vertex u, perform
either (1) or (2) and (3) (in that order) as appropriate.
Let us call the resulting structure an extended PRG. Note that the
process guarantees that if there is any data dependence u →f v and v is
a non-φ vertex, then u and v have the same set of control-dependence
predecessors, i.e., u and v execute under the same conditions. Thus,
barring nontermination or abnormal termination, each value computed
at u gets used exactly once at v. (Normally, a control-dependence
predecessor of a vertex u is just a vertex v such that there exists a
control-dependence edge v →c u. But occasionally, as above, we use the
phrase to refer to a pair ⟨v, b⟩such that there exists a control-dependence
edge v →c u labeled b).
The above extensions may be viewed as describing a graph transfor-
mation function E—thus, if G is a PRG, then E(G) is the extended
PRG. In the following section, we present a semantics for extended
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

216
G. Ramalingam and T. Reps
PRGs, represented by the semantic function M. The semantics of the
(unextended) PRG G is then deﬁned to be M(E(G)).
Let G be the PRG of a program P. We would like to relate the PRG
semantics of G to the standard operational semantics of program P.
To do this, we augment program P with φ-statements so that there
is a one-to-one correspondence between the statements and predicates
of the program so obtained (denoted by P ′) and the vertices of E(G).
This is done just as in Section 10.3, with appropriate φ-statements
being added to correspond to the φ vertices introduced in this section.
(Thus, what we get is really an augmented control-ﬂow graph, which
has a standard operational semantics.) This simpliﬁes various proofs
relating the PRG semantics to the program semantics. Since each φ
statement is an assignment of some variable to itself, the introduction of
such statements hardly changes the standard semantics of the program.
Consequently, the results we derive relating the semantics of E(G) to
the semantics of extended program do relate the PRG semantics to the
standard program semantics.
Here, it should be noted that E(G) may not be the PRG of the
program P ′. More precisely, the data dependence edges in E(G) may not
correspond to the true data dependences in P ′. For instance, consider
the PRG G shown in Figure 10.5. G is the PRG of both programs P1 and
P2, shown in augmented form below. The extended PRG E(G) turns
out to be the PRG of extended program P ′
2, but not of P ′
1.
L1:
x : = 0
if p then
L2:
x : = φT (x)
L3:
y : = x
ﬁ
L4:
z : = x
Program P ′
1
L1:
x : = 0
L4:
z : = x
if p then
L2:
x : = φT (x)
L3:
y : = x
ﬁ
Program P ′
2
However, this diﬀerence between the actual dependences and the edges
in the extended PRG causes no problem, as shown later.
Henceforth, the terms PRGs and programs will refer to extended
PRGs (like E(G)) and extended programs (like P ′), respectively.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
217
Start
x := 0
if P
y := x
T
T
T
G:
z := x
T
Start
x := 0
if P
y := x
T
T
T
x := φT(x)
T
E(G):
z := x
T
Fig. 10.5. An example of how extended PRGs may not represent the true data
dependences of the extended program.
10.5 PRG semantics
10.5.1 Notation
Let u be a vertex in an extended PRG G. The vertex type of u must
be one of {assignment, if, while, φT , φF , φif, φwhile, φEnter, φExit, φcopy,
Start, Initialize, FinalUse}, and will be denoted by typeOf(u). If u is
an assignment, if, or while vertex, then functionOf(u), a function of n
variables, will represent the n-variable expression in vertex u. When
the variables in the expression are abstracted to create a function,
they have to be done so in some particular order. The variables, in the
same order, are denoted by var1(u), . . . , varn(u). The data-dependence
predecessor corresponding to vari(u) at u (where u is an assignment,
if, or while vertex) is denoted by dataPredi(u). If u has a unique data
dependence predecessor, dataPred(u) will denote the predecessor vertex.
Let parent(u) denote the unique control-dependence predecessor of
vertex u, ignoring self-loops; in the case that u is a φEnter or a φcopy
vertex, parent(u) denotes the corresponding while predicate vertex. If
u →c v is a control-dependence edge, label(u, v) will denote the label
(true or false) on that control-dependence edge. Let controlLabel(u)
denote label(parent(u), u). If u is an Initialize, FinalUse, or φ vertex,
then varOf(u) denotes the corresponding variable.
If u is a φif, φT , or φF vertex, then ifNode(u) will denote the
corresponding if predicate vertex. Similarly, if u is some φ node
associated with a while loop, whileNode(u) will denote the corresponding
while predicate vertex. If u is a φEnter vertex, then innerDef(u) and
outerDef(u) denote the deﬁnitions that reach u from inside and outside
the loop, respectively. If u is a φif vertex, then trueDef(u) and falseDef(u)
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

218
G. Ramalingam and T. Reps
are deﬁned similarly. Let CDP(u) denote the set of control-dependence
predecessors of u, along with their associated labels.
Let Var denote the domain of variable names. Let Val denote the
basic domain of ﬁrst-order values manipulated by the programs—it
includes the domain of Booleans and any other domains of interest:
Val = Boolean + integer + . . .
The domain Sequence, consisting of ﬁnite and inﬁnite sequences of
values belonging to Val, is deﬁned by the following recursive domain
equation, where NIL denotes the domain consisting of the single value
nil.
Sequence = (NIL + (Val × Sequence))⊥
The cons operator is denoted by ·, as in head · tail. Elements of the
Sequence domain may be classiﬁed into three kinds: Finite sequences
(such as a1 ·a2 ·. . .·an ·nil), inﬁnite sequences (a1 ·a2 ·. . .) and sequences
whose suﬃx is unknown or undeﬁned (such as a1 · a2 · . . . · an · ⊥). If
a sequence X has at least i elements, then X(i) is deﬁned to be the
ith element of X; otherwise, it is undeﬁned. Similarly, X(i . . . j) denotes
the corresponding subsequence of X, if all those elements are deﬁned;
otherwise, it is undeﬁned. If X contains at least j occurrences of the value
x, then index(X, j, x) is deﬁned and denotes the position in sequence X
of the jth occurrence of x. #(X, j, x) denotes the number of occurrences
of x in X(1 . . . j). A sequence X is said to be a preﬁx of sequence Y iﬀ
for every X(i) that is deﬁned, Y (i) is deﬁned and equal to X(i).
The meaning function M that we want to deﬁne belongs to the domain
PRG →Store →Vertex →Sequence,
where Store = Var →Val.
10.5.2 The semantics
Let G be the PRG under consideration and σ the initial store. For each
vertex u in G, we deﬁne a sequence S(u) by an equation, which depends
on the type of the vertex u. This set of equations can be combined into
one recursive equation of the form
s = Fs,
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
219
where s combines all the sequences and hence eﬀectively belongs to the
domain Vertex →Sequence. The least ﬁxed point S of this equation is
given by
S =
∞

i=0
F i(⊥),
(where ⊥maps each vertex to the bottom value of Sequence). This least
ﬁxed point is taken to be the semantics of the given PRG with respect
to the given store, i.e., M[G]σ = S.
The equations are described below. (Note that the treatment of the
bottom value in the following equations follows from the treatment of
bottom by the primitive operations. Most importantly, the cons operator
is non-strict in its second parameter. Similarly, “if b then e1 else e2”
evaluates to e1 if b evaluates to true, even if e2 evaluates to bottom.)
• If u is the Start vertex, S(u) = true · nil.
• If u is an Initialize vertex, S(u) = σ(varOf(u)) · nil.
• If u is a FinalUse vertex, S(u) = S(dataPred(u)).
• If u is a φT vertex, S(u) = select(true, S(parent(u)), S(dataPred(u))),
where
select(x, y · tail1, z · tail2) =
 z · select(x, tail1, tail2)
if x = y
select(x, tail1, tail2)
otherwise
select(x, nil, z) = nil
select(x, y, nil) = nil
Note: The value of select(x, s1, s2) is the sequence consisting of
the values s2(i) such that s1(i) is the value x. More formally, let
s3 = select(x, s1, s2). If s1(1 . . . j) and s2(1 . . . j) are deﬁned and j =
index(s1, i, x), then s3(i) is deﬁned and equal to s2(j).
Conversely, if s3(i) is deﬁned, then there must be a j such that
s1(1 . . . j) and s2(1 . . . j) are deﬁned and j = index(s1, i, x).
• If u is a φF vertex, S(u) = select(false, S(parent(u)), S(dataPred(u))).
• If u is a φif vertex, S(u) = merge(S(ifNode(u)), S(trueDef(u)), S(falseDef(u))),
where
merge(true · tail1, x · tail2, s) = x · merge(tail1, tail2, s)
merge(true · tail1, nil, s) = nil
merge(false · tail1, s, x · tail2) = x · merge(tail1, s, tail2)
merge(false · tail1, s, nil) = nil
merge(nil, y, z) = nil
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

220
G. Ramalingam and T. Reps
Note: Let s4 = merge(s1, s2, s3). s4 is the sequence obtained by
merging the two sequences s2 and s3 according to s1, a sequence of
Boolean values. More formally, s4(i) is deﬁned iﬀs1(1 . . . i) is deﬁned
and s2(1 . . . j) and s3(1 . . . i −j) are deﬁned, where j = #(s1, i, true)
and i −j = #(s1, i, false). Furthermore, if the latter conditions are
true, s4(i) will equal s2(j) if s1(i) equals true and s3(i −j) if s1(i)
equals false.
• If u is a φExit vertex, S(u) = select(false, S(whileNode(u)), S(dataPred(u))).
• If u is a φwhile vertex, S(u) = select(true, S(whileNode(u)), S(dataPred(u))).
• If u is a φEnter vertex,
S(u) = whileMerge(S(whileNode(u)), S(innerDef(u)), S(outerDef(u))),
where
whileMerge(s1, s2, x · tail) = x · merge(s1, s2, tail)
whileMerge(s1, s2, nil) = nil
Note: From the deﬁnition, it can be seen that the function
whileMerge is quite related to merge. Let s = whileMerge(s1, s2, s3).
s(1) is deﬁned and equal to s3(1) iﬀs3(1) is deﬁned. For i ≥1, s(i+1)
is deﬁned iﬀs1(1 . . . i) is deﬁned and s2(1 . . . j) and s3(1 . . . i + 1 −j)
are deﬁned, where j = #(s1, i, true) and i −j = #(s1, i, false). If the
latter conditions are true, then s(i + 1) will equal s2(j) if s1(i) equals
true and s3(i + 1 −j) if s1(i) equals false.
• If u is a φcopy vertex, S(u) = whileCopy(S(whileNode(u)), S(dataPred(u))),
where
whileCopy(s, x · tail) = x · copy(s, x · tail)
whileCopy(s, nil) = nil
copy(true · tail1, x · tail2) = x · copy(tail1, x · tail2)
copy(false · tail1, x · tail2) = whileCopy(tail1, tail2)
Note: whileCopy(s1, s2) is the sequence obtained by duplicating
each element s2(i) (of the sequence s2) n + 1 times, where n is the
number of occurrences of ‘true‘ between the i −1th and ith occurrence
of ‘false‘ in the sequence s1. More formally, if s = whileCopy(s1, s2),
then (1) s(1) is deﬁned and equal to s2(1) iﬀs2(1) is deﬁned and (2)
s(i + 1) is deﬁned iﬀs1(1 . . . i) is deﬁned and s2(1 . . . j) is deﬁned,
where j = #(s1, i, false)+1. If the latter conditions hold, then s(i+1)
equals s2(j).
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
221
• The remaining possibilities are that u is an assignment vertex, an if
predicate vertex, or a while predicate vertex. In these cases, if u has
n data dependence predecessors, where n > 0, then
S(u) = map(functionOf(u))(S(dataPred1(u)), . . . S(dataPredn(u))),
where
map(f)(x1 · tail1, x2 · tail2, . . . , xn · tailn)
=
f(x1, x2, . . . , xn) · map(f)(tail1, tail2, . . . , tailn)
map(f)(nil, nil, . . . , nil)
=
nil
Note: Let f be an n-variable function. Let s = map(f)(s1, . . . , sn).
Then, s(i) is deﬁned and equal to f(s1(i), . . . , sn(i)) if and only if
s1(1 . . . i), . . . , sn(1 . . . i) are all deﬁned.
If n is 0, then the expression in vertex u is a constant-valued
expression. This case divides into two subcases. If u is anything other
than a true-valued while predicate vertex, then
S(u) = replace(controlLabel(u), functionOf(u), S(parent(u))),
where
replace(x, y, z · tail) =
 y · replace(x, y, tail)
if x = z
replace(x, y, tail)
otherwise
replace(x, y, nil) = nil
Note: Let s2 = replace(x, y, s1). s2 is essentially a sequence
consisting of as many ‘y’s as s1 has ‘x’s. Thus, all elements in s1
that do not equal x are ignored, and the remaining elements are each
replaced by y. More formally, s2(i) is deﬁned and equal to y if and
only if index(s1, i, x) is deﬁned.
• If u is a true-valued while predicate vertex, then
S(u) = whileReplace(controlLabel(u), S(parent(u)),
where
whileReplace(x, nil) = nil
whileReplace(x, z · tail) =
 inﬁniteTrues
if x = z
whileReplace(x, tail)
otherwise
and inﬁniteTrues = true · inﬁniteTrues.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

222
G. Ramalingam and T. Reps
Note: whileReplace(x, s) is an inﬁnite sequence of ‘true’s if the
value x occurs in the sequence s, and the empty sequence otherwise.
10.6 Standard semantics
Consider the sequential execution of a program P on initial store σ,
under the standard operational semantics. Let I denote the sequence of
program points executed—i.e., I(i) is the program point executed in the
ith step. Let V denote the corresponding sequence of values computed.
Thus, V (i) denotes the value computed during the ith execution step.
If a program point u executes at least i times, then step(u, i) denotes
the step number at which u executes for an ith time—i.e., step(u, i) =
index(I, i, u).
Let A(u) denote the (possibly inﬁnite) sequence of values computed
at program point u. Thus, A(u)(i) is deﬁned iﬀstep(u, i) is deﬁned,
in which case it equals V (step(u, i)). Let value(x, u, i) denote the value
of the variable x at the begining of the step(u, i)th execution step. We
will be interested in value(x, u, i) only if x is used at program point u.
We now observe some of the properties that hold among the various
sequences. Our aim is to express A(u)(i) in terms of values computed
before step(u, i).
All φ statements, Initialize statements, and FinalUse statements
represent an assignment of a variable to itself. Other statements u
compute the value
functionOf(u)(var1(u), . . . , varn(u)).
This gives us the following property:
Property 1. If u is a φ, Initialize, or FinalUse statement, then the
value of A(u)(i) equals value(varOf(u), u, i). Otherwise, A(u)(i) equals
functionOf(u)(value(var1(u), u, i), . . . , value(varn(u), u, i)).
In the standard semantics, the store is used to communicate values
between statements in the program. The following property follows
directly from the way a store is used.
Property 2. Let A = {j | 1 ≤j < step(u, i) and I(j) assigns to x}
and let max(A) denote the maximum element in the set A. Then,
value(x, u, i) =
 V (max(A))
if A is non-empty
σ(x)
otherwise
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
223
The introduction of Initialize statements guarantees the following
property.
Property 3. If x is some variable used at u and step(u, i) is deﬁned,
then the set {j | 1 ≤j < step(u, i) and I(j) assigns to variable x} is
empty iﬀu is an Initialize statement.
Note that because the programming language has no aliasing
mechanism, we can talk about assignments to variables rather than
locations. It also makes it possible to compute statically the variable to
which a statement dynamically assigns a value. Let RD(u, x) denote the
set of reaching deﬁnitions of variable x at statement u. (A statement
v that assigns a value to variable x is said to be a reaching deﬁnition
(of variable x) for statement u if there exists a path in the control-ﬂow
graph from v to u such that none of the vertices in the path, excluding
v and u, is an assignment to variable x.) The following is a property of
reaching deﬁnitions.
Property 4. If u is not an Initialize statement and x is some variable
used at u, then max({j | 1 ≤j < step(u, i) and I(j) assigns to variable x})
is equal to max({j | 1 ≤j < step(u, i) and I(j) ∈RD(u, x)}).
The preceding three properties imply the following.
Property 5. If x is some variable used at u and A denotes the set
{j | 1 ≤j < step(u, i) and I(j) ∈RD(u, x)}, then
value(x, u, i) =
 σ(x)
if u is an Initialize statement
V (max(A))
otherwise
Let DDPG(u, x) denote the set of data-dependence predecessors that
correspond to variable x of vertex u in graph G. We drop the subscript
G if G is the extended PRG. If G is the PDG or PRG of the extended
program, then DDPG(u, x) = RD(u, x), by deﬁnition (assuming that x
is used at program point u). However, this need not be true if G is the
extended PRG, as observed earlier. Yet, the data-dependence edges in
the extended PRG are a suﬃcient-enough approximation to the reaching
deﬁnitions for the following property to hold.
Property 6. If u is not an Initialize statement and x is some variable
used at u, then
value(x, u, i) = V (max({j | 1 ≤j < step(u, i) and I(j) ∈DDP(u, x)}))
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

224
G. Ramalingam and T. Reps
The justiﬁcation for the above claim follows. Let k = max({j | 1 ≤
j < step(u, i) and I(j) ∈DDP(u, x)}). Since I(k) ∈DDP(u, x), I(k)
must assign to x. Now, for any j such that k < j < step(u, i), if I(j)
is an assignment to x, then I(j) must be one of the new φ-statements
introduced in the extension of PRGs (Section 10.4). (If not, consider the
maximum j such that k < j < step(u, i), I(j) is an assignment to x
and I(j) is not one of the new φ-statements. Then, the data dependence
I(j) →f u must have been in the original PRG. Consequently, either
I(j) →f u must be present in the extended PRG, or there must be an m
such that j < m < step(u, i) and I(m) →f u is present in the extended
PRG. Either way, we have a contradiction with the maximality of k in its
deﬁnition.) Since all the new φ-statements are assignments of a variable
to itself, the above result follows.
Observe that all statements other than φEnter and φif statements have
only one data dependence predecessor per variable. In such cases the
above equation may be simpliﬁed to yield the following property.
Property 7. If DDP(u, x) = {v}, then
value(x, u, i)
=
V (max({j | 1 ≤j < step(u, i) and I(j) = v}))
=
V (max({step(v, k) | step(v, k) < step(u, i)}))
=
A(v)(k),
where k is such that
step(v, k) < step(u, i) < step(v, k + 1)
The notation step(x, i) < step(y, j) essentially means that program
point y executes for the jth time only after program point x executes for
the ith time. However, we will also say that step(x, i) < step(y, j) even
if y does not execute j times.
The following properties concern control dependence and help identify
the value of k in property 7. Observe that if v is a data-dependence
predecessor of u, and u and v are non-φ statements, then u and v have
the same control-dependence predecessor and v occurs to the left of u
in the program’s abstract syntax tree. More generally, if v is any kind of
data-dependence predecessor of u and u is a non-φ vertex, then u and
v have the same control-dependence predecessors and v dominates u in
the extended control-ﬂow graph.
Property 8. If CDP(u) = CDP(v) and v dominates u, then step(v, i) <
step(u, i) < step(v, i+1), for all i. Furthermore, if the program execution
terminates normally, u and v execute the same number of times.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
225
The previous two properties combined with Property 1 gives us the
following result.
Property 9. If u is an assignment statement, if predicate, or while
predicate, and executes i times, then
A(u)(i) = functionOf(u)(A(dataPred1(u))(i), . . . , A(dataPredn(u))(i)).
Let u have only one control-dependence predecessor, say v. Let this
control-dependence edge be labeled true. Then, barring nontermination
or abnormal termination, u is executed exactly once each time v is
evaluated to true. More formally, we can state the following property:
Property
10. Let v
→c
u, labeled true, be the sole control-
dependence edge that is incident on u. Then, if A(u)(i) is deﬁned,
j = index(A(v), i, true) must be deﬁned and step(v, j) < step(u, i).
Conversely, if index(A(v), i, true) is deﬁned, then A(u)(i) must be
deﬁned, barring nontermination or abnormal termination.
The above property can be extended to state that step(u, i) occurs
“soon after” step(v, j)—i.e., before any other statement at the same
nesting level as v can be executed. Let w denote some vertex with the
same control-dependence predecessors as v and occuring to the left of v.
(As a speciﬁc example, let u be a φT vertex. Let v be parent(u) and w be
dataPred(u).) It is easy to see that step(w, j) < step(u, i) < step(w, j+1).
This gives us the following property.
Property 11. Let u be a φT vertex. Let v denote parent(u) and w
denote dataPred(u). If A(u)(i) is deﬁned, then j = index(A(v), i, true)
must be deﬁned and
step(w, j) < step(v, j) < step(u, i) < step(w, j + 1)
and consequently, from Properties 1 and 7,
A(u)(i) = A(w)(j)
A similar extension of Property 10 to consider a vertex w with the
same control-dependence predecessors as v and occurring to the right of
v yields the following property.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

226
G. Ramalingam and T. Reps
Property 12. Let v be an if predicate and let v →c u, labeled true,
be the only control-dependence edge incident on u. Let w be a φif
vertex associated with v. Let j = #(A(v), i, true). Then step(u, j) <
step(w, i) < step(u, j + 1).
Related versions of the above two properties may be obtained by
replacing true by false and T by F. The following property of φif vertices
is obtained from Properties 1, 6, and 12.
Property 13. Let u be a φif vertex. Let v be ifNode(u), x be trueDef(u)
and y be falseDef(u). If A(u)(i) is deﬁned, then
A(u)(i) =
 A(x)(j)
if A(v)(i)
A(y)(i −j)
otherwise
where j = #(A(v), i, true).
A formal derivation of the above property follows. Assume that A(v)(i)
is true. Let #(A(v), i, true) be j. Obviously, #(A(v), i −1, true) = j −
1, while #(A(v), i, false) = #(A(v), i −1, false) = i −j. Hence, from
property 12,
step(y, i −j) < step(u, i −1) < step(x, j) < step(u, i) < step(x, j + 1)
and
step(u, i) < step(y, i −j + 1)
Properties 1 and 6 imply that A(u)(i) must be A(x)(j). Similarly, if
A(v)(i) is false, then A(u)(i) must be A(y)(i −j).
The following property concerns the execution behavior of a φEnter
vertex. Here, it is useful to consider the execution of the whole loop
(rather than just the loop predicate). The loop completes an execution
when the loop predicate evaluates to false. Suppose the loop predicate
v has been executed i times. Then, the number of times the loop has
completed an execution is given by #(A(v), i, false).
Property 14. Let u be a φEnter vertex. Let v, x, and y be whileNode(u),
outerDef(u), and innerDef(u), respectively. Let w be the parent of x and
v. Let the control dependences w →c v and w →c u be labeled true. If
i > 1, then the following hold
step(x, 1) < step(u, 1) < step(y, 1)
step(u, i −1) < step(v, i −1) < step(u, i)
step(x, j) < step(u, i) < step(x, j + 1), where j = #(A(v), i −1, false) + 1
step(y, j) < step(u, i) < step(y, j + 1), where j = #(A(v), i −1, true)
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
227
In particular, from Property 6, if A(u)(1) is deﬁned, then
A(u)(1) = A(x)(1)
and for i > 1, if A(u)(i) is deﬁned, then
A(u)(i) =
 A(y)(i −j)
if A(v)(i −1)
A(x)(j)
otherwise
where j = #(A(v), i −1, false) + 1.
The derivation of the above property is very similar to the derivation
of Property 13.
The following property concerns φcopy vertices. It is similar to, though
simpler than, the previous property.
Property 15. Let u be a φcopy vertex. Let v denote whileNode(u), and
w denote dataPred(u). Let j = #(A(v), i −1, false) + 1 Then,
step(u, i −1) < step(v, i −1) < step(u, i)
and
step(w, j) < step(u, i) < step(w, j + 1)
and if A(u)(i) is deﬁned, it must be equal to A(w)(j).
10.7 Relationship to the standard operational semantics
We now consider the relationship between the semantics of the PRG of
a program, as deﬁned earlier, and the standard operational semantics
of the program. We show that in general the sequence S(u) (which is
deﬁned by the PRG semantics) may be more deﬁned than the sequence
A(u) (the sequence of values computed by program point u, as deﬁned
by the operational semantics of the program)—or more formally, that
A(u) will be a preﬁx of S(u). However, for input stores on which the
program terminates normally, the sequence S(u) will be shown to be
equal to the sequence A(u).
This diﬀerence
in the case
of
nonterminating
(or
abnormally
terminating) program execution maybe explained as follows. Dataﬂow
semantics exposes and exploits the parallelism in programs. The eager
or data-driven evaluation semantics lets a program point execute as soon
as the data it needs is available. In the standard sequential execution
of a program, however, the execution of a program point u may have
to be delayed until completion of execution of some other part of the
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

228
G. Ramalingam and T. Reps
program, even if the result of that computation is unnecessary for the
computation to be done at u. Moreover, if that computation never
terminates or terminates abnormally, execution of program point u does
not occur.
Let S(u) denote the least-ﬁxed-point solution of the set of recursive
equations for the PRG of program P and initial store σ as deﬁned in
Section 10.5. As observed earlier, the set of equations can be combined
into one recursive equation of the form s = Fs. Let Sk(u) denote
F k(⊥)(u), the kth approximation to the solution at vertex u. Now we
are ready to state a sequence of lemmas and theorems that relate the
standard operational semantics and the PRG semantics.
Lemma 10.2 Let G be the extended PRG of a program P and σ be an
input store. Let S(u) denote M[G](σ)(u), Sk(u) denote the kth approxi-
mation to S(u), and A(u) denote the sequence of values computed at
program point u for input σ under the standard operational semantics.
If A(u)(i) is deﬁned, then there exists a k such that Sk(u)(i) is deﬁned
and equal to A(u)(i).
Proof: See [18].
Theorem 10.3 Let G be the extended PRG of a program P and σ an
input store. Let S(u) denote M[G](σ)(u) and A(u) denote the sequence
of values computed at program point u for input σ under the standard
operational semantics. Then, A(u) is a preﬁx of S(u).
Proof: The theorem follows immediately from the previous lemma. Let
Sk(u) denote the kth approximation to S(u). Thus, S(u) = ∞
i=0 Si(u).
If A(u)(i) is deﬁned, then there exists a k such that Sk(u)(i) is deﬁned
and equal to A(u)(i), from the previous lemma. Consequently, S(u)(i)
is deﬁned and equal to A(u)(i).
The
preceding
theorem
concerns
possibly
nonterminating
(or
abnormally terminating) executions of the program. We now consider
executions that terminate normally and show the stronger result that
for all program points u, S(u) = A(u).
Lemma 10.4 Let G be the extended PRG of a program P and σ an input
store on which P terminates normally. Let S(u) denote M[G][σ](u),
Sk(u) denote the kth approximation to S(u), and A(u) denote the
sequence of values computed at program point u for input σ under the
standard operational semantics. For any k, Sk(u) is a preﬁx of A(u).□
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
229
Proof: See [18].
Theorem 10.5 Let G be the extended PRG of a program P and σ an
input store on which P terminates normally. Let S(u) denote M[G][σ](u)
and A(u) denote the sequence of values computed at program point u for
input σ under the standard operational semantics. Then S(u) is a preﬁx
of A(u).
□
Proof: Let Sk(u) denote the kth approximation to S(u). If S(u)(i) is
deﬁned, then there must be a k such that Sk(u)(i) is deﬁned (and,
obviously, equal to S(u)(i)). It follows from the previous lemma that
A(u)(i) is deﬁned and equal to S(u)(i).
Theorem 10.6 Let G be the extended PRG of a program P and σ
an input store on which P terminates normally. M[G][σ](u) is equal to
A(u), the sequence of values computed at program point u for input σ
under the standard operational semantics.
□
Proof: It follows from the last two theorems that A(u) is a preﬁx
of M[G][σ](u) and M[G][σ](u) is a preﬁx of A(u). Hence, A(u) and
M[G][σ](u) must be equal.
A stronger form of the equivalence theorem for PRGs [26] (see
Section 10.3) follows directly from the previous theorems.
Theorem 10.7 Let P and Q be programs with isomorphic PRGs, RP
and RQ, respectively. Let σ1 and σ2 be two states that agree on the
imported variables of P and Q. Let x1 and x2 be two corresponding
vertices of P and Q. Let AP (x1) and AQ(x2) denote the sequence of
values computed at x1 and x2, on states σ1 and σ2, respectively. Then,
either (1) P and Q terminate normally on σ1 and σ2, respectively, and
AP (x1) equals AQ(x2), or (2) neither P nor Q terminates normally on
σ1 and σ2, respectively, and AP (x1) is a preﬁx of AQ(x2) or vice versa.
□
Proof: Note that the dependence of the PRG semantics on the initial
state is restricted to the values of the imported variables. Consequently,
the semantics of the isomorphic PRGS RP and RQ for initial states
σ1 and σ2, respectively, say SP and SQ, are identical. Thus SP (x1) =
SQ(x2). From the previous section, we also know that AP (x1) is a preﬁx
of SP (x1) and that AQ(x2) is a preﬁx of SQ(x2). Consequently, AP (x1)
must be a preﬁx of AQ(x2) or vice versa.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

230
G. Ramalingam and T. Reps
Note that AP (x) and SP (x) are ﬁnite for all vertices x in P iﬀthe
program P terminates normally. Hence, either P and Q both terminate
normally (in which case AP (x1) = SP (x1) = SQ(x2) = AQ(x2)) or
neither P nor Q terminates normally. The theorem follows immediately.
However, note that this stronger equivalence theorem can be derived
from the Sequence–Congruence theorem [27], too.
10.8 Application to program slicing
We now show how the results established above can be used to reason
about and establish the correctness of applications of a PRG. Speci-
ﬁcally, we consider the application of PRGs to program slicing and derive
an analogue of the Slicing theorem of [20] relating the semantics of a
program’s slice to the semantics of the program. A slice of a program
with respect to a program point p and a variable v consists of all
statements of the program that might aﬀect the value of v at point p [25].
When the variable v is used or deﬁned at program point p, the slice with
respect to v at p can be obtained by identifying the subgraph of a PDG
(or PRG) induced by all vertices that can reach p in the PDG (or PRG)
and transforming the subgraph into a program [16].
Theorem 10.8 Let P and Q be programs with PRGs RP and RQ,
respectively; let q be a vertex of RQ, and RP be isomorphic to a subgraph
of RQ induced by all vertices that can reach q. Let σ1 and σ2 be two
states that agree on the imported variables of P. Let x1 and x2 be two
corresponding vertices of P and Q. Let AP (x1) and AQ(x2) denote the
sequence of values computed at x1 and x2, on states σ1 and σ2, respec-
tively. Then, (1) AP (x1) is a preﬁx of AQ(x2) or vice versa, and (2) if
Q terminates normally on σ2 then P terminates normally on σ1 and
AP (x1) equals AQ(x2).
□
Proof: Note that the set of equations induced by RP is isomorphic to the
corresponding subset of the equations induced by RQ. For T ∈{P, Q},
let ST denote the semantics of the PRG RT , and let Sk
T (u) denote the kth
approximation to ST (u). One can establish by induction that Sk
P (x1) =
Sk
Q(x2) for any corresponding pair of vertices x1 in P and x2 in Q. It
follows that SP (x1) = SQ(x2).
Because AP (x1) is a preﬁx of SP (x1) and AQ(x2) is a preﬁx of SQ(x2),
it follows that either AP (x1) must be a preﬁx of AQ(x2) or AQ(x1) must
be a preﬁx of AP (x2).
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
231
Furthermore, if Q terminates normally on σ2, then SQ(x2) is ﬁnite
and equal to AQ(x2) for every vertex x2 in Q. It follows that SP (x1) is
ﬁnite for every vertex x1 in P and that P must terminate normally on
σ1. Hence, AP (x1) = SP (x1) = SQ(x2) = AQ(x2), where x1 and x2 are
corresponding vertices.
10.9 Related work
Selke’s graph rewriting semantics for PDGs [22], and Cartwright and
Felleisen’s derivation of a semantics for PDGs [4] have already been
discussed in Section 10.1.
The work described in this paper, which was originally described in a
1989 technical report [19], was motivated by the desire to apply Kahn’s
semantics for a parallel programming language [12] to the problem that
Selke’s and Cartwright and Felleisen’s papers addressed. A somewhat
similar approach was taken in two other papers from the same era.
• Ottenstein et al. [15] describe an augmented program-dependence
representation, similar to the extended PRG, for programs with
unstructured control ﬂow, and indicate that it could be interpreted
as a dataﬂow graph. Their work, however, focuses on constructing
the representation, and they do not present a formal semantics for
the representation.
• Pingali et al. [17] argue that intermediate representations of programs
should themselves be programs, i.e., have a local execution semantics,
to permit abstract interpretation over the intermediate representation.
They describe yet another dependence-graph representation, for which
they present a dataﬂow-like semantics; however, this semantics is
based on a global store, rather than being a “pure dataﬂow” semantics.
They show how the representation can be used for abstract interpre-
tation to perform constant propagation.
Field [7] presents a rewriting semantics for programs, where programs
are viewed as terms, and a set of equational axioms for such terms is used
as the basis for rewriting terms. Rewriting applied to a term consisting of
a program and its input corresponds to program execution. On the other
hand, rewriting can also be applied to a program, without its input, to
produce a “simpliﬁed” program equivalent to the original program. Field
shows that this rewriting process can produce terms that are similar to
program representations such as PRGs.
Weise
et
al.
[24]
present
another
dependence-graph
represen-
tation, called the Value Dependence Graph; they argue that such
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

232
G. Ramalingam and T. Reps
representations are a better basis for compiler optimizations, and
illustrate this via an algorithm for partial redundancy elimination.
Weise et al. address a more complete language than we do.
Giacobazzi and Mastroeni [8] present a framework for deﬁning
programming language semantics for which the semantics of a program
and a slice of the program are related, even when the slice changes
the termination behavior. Their approach uses transﬁnite state traces,
which allows the semantics to observe what happens “after” a divergent
loop. This allows a program’s behavior to be related to the behaviors of
slices that change the termination status: the ﬁnite behavior of a slice
that terminates on inputs that cause the original program to diverge
can be related to the program’s transﬁnite behavior.
Hatcliﬀet al. [9] relate the semantics of a concurrent program and the
semantics of a slice of the program using the notion of weak simulation:
a correspondence relation is deﬁned between the execution states of
the program and the execution states of the slice, and it is shown that
for any observable transition that the program can make, the slice can
make a corresponding transition. Amtoft [3] uses a similar approach for
sequential programs.
Bibliography
[1]
A. Aho, R. Sethi and J. Ullman. Compilers: Principles, Techniques and
Tools. Addison-Wesley, 1985.
[2]
B. Alpern, M. Wegman, and F. Zadeck. Detecting equality of variables
in programs. In POPL, pp. 1–11, 1988.
[3]
T.
Amtoft.
Slicing
for
modern
program
structures:
a
theory
for
eliminating irrelevant loops. Information Processing Letters, 106:45–51,
2008.
[4]
R. Cartwright and M. Felleisen. The semantics of program dependence.
In PLDI, pp. 13–27, 1989.
[5]
R. Cytron, J. Ferrante, B. Rosen, M. Wegman and F. Zadeck. An eﬃcient
method of computing static single assignment form. In POPL, pp. 25–35,
1989.
[6]
J. Ferrante, K. Ottenstein and J. Warren. The program dependence graph
and its use in optimization. TOPLAS, 3(9):319–349, 1987.
[7]
J. Field. A simple rewriting semantics for realistic imperative programs
and its application to program analysis. In PEPM, pp. 98–107, 1992.
[8]
R. Giacobazzi and I. Mastroeni. Non-standard semantics for program
slicing. HOSC, 16(4):297–339, 2003.
[9]
J. Hatcliﬀ, J. Corbett, M. Dwyer, S. Sokolowski and H. Zheng. A
formal study of slicing for multi-threaded programs with JVM concurrency
primitives. In SAS, 1999.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

Semantics of program representation graphs
233
[10]
S. Horwitz, J. Prins and T. Reps. On the adequacy of program
dependence graphs for representing programs. In POPL, pp. 146–157, 1988.
[11]
S. Horwitz, J. Prins and T. Reps. Integrating non-interfering versions of
programs. TOPLAS, 11(3):345–387, 1989.
[12]
G. Kahn. The semantics of simple language for parallel programming.
In IFIP Congress, pp. 471–475, 1974.
[13]
D. Kuck, R. Kuhn, B. Leasure, D. Padua and M. Wolfe. Dependence
graphs and compiler optimizations. In POPL, pp. 207–218, 1981.
[14]
D. Kuck, Y. Muraoka, and S. Chen. On the number of operations
simultaneously executable in FORTRAN-like programs and their resulting
speed-up. IEEE Trans. on Computers, C-21(12):1293–1310, 1972.
[15]
K. Ottenstein, R. Ballance and A. MacCabe. The program dependence
web: A representation supporting control-, data-, and demand-driven
interpretation of imperative languages. In PLDI, pp. 257–271, 1990.
[16]
K. Ottenstein and L. Ottenstein. The program dependence graph in
a software development environment. In Softw. Eng. Symp. on Practical
Softw. Dev. Environments, pp. 177–184, 1984.
[17]
K. Pingali, M. Beck, R. Johnson, M. Moudgill, and P. Stodghill.
Dependence ﬂow graphs: An algebraic approach to program dependencies.
In Advances in Languages and Compilers for Parallel Processing, pp. 445–
467. M.I.T. Press, 1991.
[18]
G. Ramalingam and T. Reps. Appendix A: Proofs. “www.cs.wisc.
edu/wpis/papers/soprgs08-proofs.pdf”.
[19]
G. Ramalingam and T. Reps. Semantics of program representation
graphs. TR-900, Computer Science Department, University of Wisconsin,
Madison, WI, 1989.
[20]
T. Reps and W. Yang. The semantics of program slicing and program
integration. In CCIPL, pp. 360–374, 1989.
[21]
B. Rosen, M. Wegman, and F. Zadeck. Global value numbers and
redundant computations. In POPL, pp. 12–27, 1988.
[22]
R. Selke. A rewriting semantics for program dependence graphs. In
POPL, pp. 12–24, 1989.
[23]
R. Shapiro and H. Saint. The representation of algorithms. Tech. Rep.
CA-7002-1432, Massachusetts Computer Associates, 1970.
[24]
D. Weise, R. Crew, M. Ernst and B. Steensgaard. Value dependence
graphs: Representation without taxation. In POPL, pp. 297–310, 1994.
[25]
M. Weiser. Program slicing. In ICSE, pp. 439–449, 1981.
[26]
W. Yang. A New Algorithm for Semantics-Based Program Integration.
PhD thesis, Computer Science Department, University of Wisconsin,
Madison, WI, Aug. 1990.
[27]
W. Yang, S. Horwitz and T. Reps. Detecting program components with
equivalent behaviors. TR-840, Computer Science Department, University of
Wisconsin, Madison, WI, Apr. 1989.
[28]
W. Yang, S. Horwitz and T. Reps. A program integration algorithm that
accommodates semantics-preserving transformations. TOSEM, 1(3):310–
354, July 1992.
https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

https://doi.org/10.1017/CBO9780511770524.011 Published online by Cambridge University Press

