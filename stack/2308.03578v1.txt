TeraHAC: Hierarchical Agglomerative Clustering
of Trillion-Edge Graphs
Laxman Dhulipala
UMD and Google Research
USA
laxman@umd.edu
Jason Lee
Google Research
USA
jdlee@google.com
Jakub ÅÄ…cki
Google Research
USA
jlacki@google.com
Vahab Mirrokni
Google Research
USA
mirrokni@google.com
ABSTRACT
We introduce TeraHAC, a (1 + ğœ–)-approximate hierarchical
agglomerative clustering (HAC) algorithm which scales to
trillion-edge graphs. Our algorithm is based on a new approach
to computing (1 + ğœ–)-approximate HAC, which is a novel
combination of the nearest-neighbor chain algorithm and the
notion of (1 + ğœ–)-approximate HAC. Our approach allows us
to partition the graph among multiple machines and make
significant progress in computing the clustering within each
partition before any communication with other partitions is
needed.
We evaluate TeraHAC on a number of real-world and syn-
thetic graphs of up to 8 trillion edges. We show that TeraHAC
requires over 100x fewer rounds compared to previously known
approaches for computing HAC. It is up to 8.3x faster than
SCC, the state-of-the-art distributed algorithm for hierarchi-
cal clustering, while achieving 1.16x higher quality. In fact,
TeraHAC essentially retains the quality of the celebrated HAC
algorithm while significantly improving the running time.
1
INTRODUCTION
Hierarchical agglomerative clustering (HAC) is a widely-used
clustering algorithm [28, 44â€“47, 55] known for its high quality
in a variety of applications [11, 19, 30, 34, 61].
The algorithm takes as input a collection of ğ‘›points, as well
as a function giving similarities between pairs of points. At a
high level, the algorithm works as follows. Initially, each point
is put in a separate cluster of size 1. Then, the algorithm runs
a sequence of steps. In each step, the algorithm finds a pair of
most similar clusters and merges them together, that is, the
two clusters are replaced by their union. Here, the similarity
between the clusters is computed based on the similarities
between the points in the clusters. The exact formula used is
configurable and referred to as linkage function.
Common linkage functions include single-linkage â€“ the
similarity between two clusters ğ¶and ğ·is the maximum
similarity of two points belonging to ğ¶and ğ·, and average-
linkage â€“ the similarity between ğ¶and ğ·is the total similarity
between points in ğ¶and ğ·divided by |ğ¶| Â· |ğ·|. Due to the
particularly good empirical quality, HAC using average linkage
similarity is of particular interest [17, 30, 34, 40, 43, 61].
The output of the algorithm is a dendrogram â€“ a rooted
binary tree (or a collection thereof) describing the merges per-
formed by the algorithm. The leaves of the tree correspond to
the initial clusters of size 1, and each internal vertex represents
a cluster obtained by merging its two children.
Up until recently, a major shortcoming of HAC was very
limited scalability, as the best known algorithms required time
that is quadratic in the number of input points. In addition to
that, it was commonly assumed that the input to the algorithm
is a complete ğ‘›Ã—ğ‘›matrix giving similarities between all input
points, which posed another scalability barrier.1
Recent results overcome the quadratic-time bottleneck by
allowing (1 + ğœ–)-approximation [25, 26]. A HAC algorithm
is (1 + ğœ–)-approximate if in each step it merges together two
clusters whose similarity is within a (1 + ğœ–) factor from the
maximum similarity between two clusters (at that point).
By allowing approximation and assuming that the input to
the algorithm is a sparse similarity graph (e.g. the k-nearest
neighbors graph of the input points), it was shown that HAC
can be implemented in near-linear time [26] and efficiently
parallelized [25]. At the same time, both relaxations (approxi-
mation and using a sparse graph) were shown not to negatively
affect the empirical quality of the obtained solutions [25, 26].
This line of work led to single machine parallel HAC implemen-
tations scaling to graphs containing several billion vertices
and about 100 billion edges [25].
Nevertheless, some large-scale applications require clus-
tering even larger datasets. To address these needs, several
distributed hierarchical clustering algorithms have been pro-
posed [7, 31, 32, 40, 56]. These algorithms can handle up to
tens of billions of datapoints and several trillion edges. How-
ever, in order to provide high quality results and/or provable
guarantees on the quality of the output, the algorithms need
to run typically a significant number of parallel rounds on
large-scale datasets [40, 56], and for algorithms that can run
in few rounds, no provable quality guarantees are known [40].
An intriguing open question, then, is whether it is possible to
achieve provable quality guarantees for hierarchical clustering,
while running in very few rounds.
1Even if only a linear number of pairs of points had nonzero similarity, the
previously known algorithms would still require quadratic time.
arXiv:2308.03578v1  [cs.DS]  7 Aug 2023

1.1
Our Contribution
In this paper we introduce TeraHAC â€“ a (1 + ğœ–)-approximate
HAC algorithm, which runs in very few rounds and is thus
amenable to a distributed implementation. We obtain TeraHAC
by developing a new approach to computing (1+ğœ–)-approximate
HAC, which may be of independent interest. We evaluate
TeraHAC empirically and show that our implementation scales
to datasets of tens of billions of points and trillions of edges.
Our key algorithmic contribution is a new approach to
computing (1 + ğœ–)-approximate HAC, which we now out-
line. The exact (1-approximate) HAC algorithm defines a very
specific order, in which cluster merges must be performed
â€“ only a highest similarity pair of clusters can be merged in
each step. However, the very same output dendrogram (up
to different tiebreaking) can be computed using a (parallel)
RAC algorithm [56] (which is based on the idea behind the
nearest-neighbor chain algorithm [8, 60]). In each step, the
RAC algorithm finds all reciprocally most similar clusters. That
is, it finds each pair of clusters ğ¶and ğ·, such that ğ·is the
most similar cluster to ğ¶, and ğ¶is the most similar cluster to
ğ·(for simplicity, let us assume that there are no ties). It is
easy to see that the reciprocally most similar pairs of clusters
form a matching. Then, the algorithm merges both clusters
of each pair in parallel. Clearly, this approach allows poten-
tially making multiple merges in one step, some of which may
merge pairs of clusters whose similarity is much smaller than
the maximum similarity between two clusters. Crucially, this
yields the same result as the basic algorithm, even though both
algorithms may perform merges in a very different order.
We generalize the nearest-neighbor chain/RAC algorithm
to a (1 + ğœ–)-approximate algorithm, by introducing a notion
of a good merge. Namely, we call the merge of two clusters
(1 +ğœ–)-good if it satisfies a certain condition (see Definition 2).
Crucially, whether a merge of two clusters is good can be
checked locally, essentially by looking at the clusters and their
incident edges. We show that by performing the (1 + ğœ–)-good
merges in any order (for example, in parallel) one obtains a
(1+ğœ–)-approximate dendrogram, i.e., one that can be obtained
by performing a sequence of (1 + ğœ–)-approximate merges.
To put our algorithmic result in context, let us compare the
amount of parallelism available to different HAC algorithms
by looking at what edges can be merged in the beginning of
the algorithm. Fig. 1 compares the amount of parallelism in the
RAC algorithm, TeraHAC and ParHAC. ParHAC is a parallel
(1+ğœ–)-approximate parallel HAC algorithm [26], which allows
merging any edge whose weight is at most a (1+ğœ–) factor away
from the globally maximum weight. If we consider the sets of
edges that can be merged by RAC and ParHAC, it is easy to see
that none of them is a superset of the other one. At the same
time, the set of edges that TeraHAC can merge is a superset of
both sets (this is the case in general, not just in the provided
example). As shown in Fig. 2 the increased parallelism leads
to over 100x reduction in the number of rounds. Importantly,
the reduction in the number of rounds also yields between
3.5â€“10.5x improvements in running time as shown in Fig. 3.
We evaluate TeraHAC on a number of graph datasets of up
to 8 trillion edges. We first study its quality on several smaller
1
0.95
0.8
0.9
0.7
0.82
0.9
ğ–¯ğ–ºğ—‹ğ–§ğ– ğ–¢, Ïµ = 0.1
ğ–±ğ– ğ–¢
ğ–³ğ–¾ğ—‹ğ–ºğ–§ğ– ğ–¢, Ïµ = 0.1
Figure 1: Comparison of the merges available at the start of
the algorithm for different parallel HAC algorithms.
datasets with ground truth. We show that when using ğœ–= 0.1
TeraHAC, the difference in quality compared to exact HAC
is only 1%â€“3% (according to standard clustering similarity
measures, ARI and NMI), which is in line with previous results
on (1 + ğœ–)-approximate HAC [25, 26]. Moreover, TeraHAC is
more scalable and accurate than SCC [40] â€“ the state of the art
distributed algorithm for hierarchical graph clustering; it is on
average 8.3x faster than SCC in SCCâ€™s highest-quality setting
(using 100 rounds) on a set of large real-world web graphs. For
the same high-quality setting, we find that TeraHAC achieves
1.16x higher quality than SCC.
Second, we study the scalability of TeraHAC on a num-
ber of large graphs. We show that for ğœ–= 0.1 the number of
rounds TeraHAC runs is very low (at most 17 rounds over all
our datasets), and that the residual graph shrinks at a geo-
metric rate. We also find that allowing approximation yields
over an order of magnitude more (1 + ğœ–)-good edges (edges
that TeraHAC can potentially merge), compared with edges
mergeable by RAC (see Fig. 15).
Finally, we study the performance and quality of TeraHAC
on an 8 trillion-edge graph representing similarities between
31 billion web queries using a set of human-generated labels.
We observe that TeraHAC performs consistently better along
a precision-recall curve, in some cases improving recall by up
to 20% (relative), while being 30%â€“50% faster.
1.2
Further Related Work
The HAC algorithm is particularly useful for semantic cluster-
ing applications, where the desired number of clusters is not
known upfront, for example for de-duplication or, more gener-
ally, detecting groups of closely related entities [30, 34, 40, 61].
In these cases in large-scale applications the number of clusters
of interest is very high (close to the number of input entities),
and so the average cluster size is low [34].
For other prominent applications of clustering, multiple
successful algorithms have been developed. For example DB-
SCAN [52] and correlation clustering [6] are particularly well
suited for finding anomalously dense clusters. Modularity clus-
tering [48] is widely used for community detection. Graph par-
titioning methods [9, 13] split the data into a typically small
number of roughly-equally sized clusters, while ensuring that
related entities are not split across different clusters (hence,
it is in a sense a dual problem to the one addressed by HAC).
Finally, k-means/k-median/k-center methods as well as spec-
tral clustering [23, 49] are particularly well-suited when the
desired number of clusters is known upfront.
The notion of approximate HAC [41] was used to give more
efficient HAC algorithms in the case when the input is a metric
space [1, 41, 42, 59] as well as a graph [25, 26].
2

OK
TW
FS
CW
23
25
27
29
211
213
Number of Rounds
TeraHACÏµ=0.1
OptimizedRAC
RAC
ParHACÏµ=0.1
Figure 2: Number of rounds used by TeraHAC compared with
OptimizedRAC (TeraHAC using ğœ–= 0), ParHAC and RAC on four
large real-world graph datasets. All algorithms use a weight
threshold of ğ‘¡= 0.01 (see Section 6).
OK
TW
FS
CW
25000
50000
75000
100000
125000
150000
175000
200000
Running Time in Seconds
TeraHACÏµ=0.1
OptimizedRAC
Figure 3: Distributed running times of TeraHAC compared
with OptimizedRAC (TeraHAC using ğœ–= 0) on the same graphs
and threshold as Figure 2.
For average linkage HAC, to the best of our knowledge, the
fastest single-machine implementations scale to tens of mil-
lions of points in metric space [60] or billion-vertex graphs [25].
A number of graph clustering problems have been stud-
ied in the distributed setting, for example in MapReduce [21],
Pregel/Giraph [38, 51] and other frameworks captured by the
MPC model of computation [33]. Examples include efficient
algorithms for correlation clustering [16, 18], connected com-
ponents [36] and balanced partitioning [4, 5].
For HAC, efficient distributed algorithms are known for
the single-linkage variant [31, 32, 59]. We note that single-
linkage hierarchical agglomerative clustering generally deliv-
ers worse quality compared to average linkage [17, 61]. Several
HAC algorithms have been recently developed in the shared-
memory setting, including SeqHAC, a nearly linear time (1+ğœ–)-
approximate sequential HAC algorithm [26] and ParHAC, a
nearly linear work (1 + ğœ–)-approximate HAC algorithm with
low depth [25]. ParHAC provably runs in poly-logarithmic
rounds [25], whereas TeraHAC uses a novel relaxation of the
reciprocal clustering approach of RAC, and neither TeraHAC
nor RAC are known to admit non-trivial round-complexity
bounds. However, empirically TeraHAC uses the fewest rounds
of any modern HAC algorithm we study (see Figure 8).
The most scalable hierarchical clustering algorithms have
been obtained by slightly diverging from the HAC algorithm
definition. Affinity clustering [7], and its successor SCC [40]
are two highly scalable hierarchical clustering algorithms de-
signed for the MapReduce model. Both take graphs as input
and were shown to scale to graphs with trillions of edges.
While both algorithms are similar to HAC by design, in prac-
tice they can give very different results from what the HAC
algorithm gives. We compare with SCC in our empirical analy-
sis. We conclude that while SCC can be tuned to obtain quality
which is comparable with HAC, this comes at a cost of signif-
icantly higher running time of SCC. We note that SCC was
shown to deliver higher quality than affinity clustering.
Another highly scalable HAC implementation is the RAC
algorithm [56] obtained by parallelizing the nearest neighbor
chain algorithm. The algorithm was shown to scale to datasets
of billion points. Since RAC is based on the exact HAC al-
gorithm, the number of rounds it uses is an order of magni-
tude larger than what can be achieved by (1 + ğœ–)-approximate
HAC [25], which negatively affects performance. As we show,
TeraHAC using ğœ–= 0 can be viewed as an optimized imple-
mentation of RAC, which we refer to as OptimizedRAC. At
the same time TeraHAC using ğœ–= 0 uses up to two orders of
magnitude fewer rounds (see Figure 2).
2
PRELIMINARIES
Let ğº= (ğ‘‰, ğ¸,ğ‘¤) be a weighted and undirected graph, where
|ğ‘‰| = ğ‘›. We assume that all edge weights of ğºare positive.
Let us now describe how the HAC algorithm works given
ğºas an input. Initially there are ğ‘›clusters, each containing a
distinct vertex of ğº. The algorithm proceeds in at most ğ‘›âˆ’1
steps. In each step, the algorithm finds two clusters of nonzero
similarity and merges them together. Let ğ¶1, . . . ,ğ¶ğ‘˜, be a se-
quence of clusterings, where ğ¶ğ‘–is the clustering obtained after
running ğ‘–âˆ’1 steps of the algorithm. In particular, ğ¶1 is the
clustering containing ğ‘›clusters of size 1.
We define a sequence of graphs ğº1, . . . ,ğºğ‘˜, where ğºğ‘–is
obtained fromğºby contracting the clustersğ¶ğ‘–. Specifically, the
vertex set ofğºğ‘–isğ¶ğ‘–, which means that each vertex ofğºğ‘–is a set
of vertices ofğº. We define the functionğ‘¤: 2ğ‘‰Ã—2ğ‘‰â†’R giving
edge weights in ğºğ‘–as ğ‘¤(ğ‘¢, ğ‘£) = Ã
ğ‘¥ğ‘¦âˆˆ((ğ‘¢Ã—ğ‘£)âˆ©ğ¸) ğ‘¤(ğ‘¥ğ‘¦)/(|ğ‘¢| Â·
|ğ‘£|). Note that the weight function ğ‘¤is the same for all graphs
ğºğ‘–. There is an edge between two vertices ğ‘¢and ğ‘£in ğºğ‘–if and
only if ğ‘¤(ğ‘¢, ğ‘£) > 0.
Definition 1 ([8]). A cluster similarity function ğ‘“: 2ğ‘‰Ã—2ğ‘‰â†’
R is called reducible if and only if for any clusters ğ‘¥,ğ‘¦,ğ‘§, we
have ğ‘“(ğ‘¥,ğ‘¦âˆªğ‘§) â‰¤max(ğ‘“(ğ‘¥,ğ‘¦), ğ‘“(ğ‘¥,ğ‘§)).
In particular, the function ğ‘¤defined above is reducible [8].
While in the following we work with this specific similarity
function, we note that our main theoretical results hold for
any similarity function satisfying Definition 1, which includes
e.g. single linkage, complete linkage and median linkage.
Note that ğºğ‘–+1 is obtained from ğºğ‘–by contracting a single
edge and updating the weights of the edges incident to the
vertex created by the contraction. We call each such operation
a merge of the endpoints of the contracted edge. Let ğ‘¤ğ‘šğ‘ğ‘¥(ğºğ‘–)
be the largest edge weight inğºğ‘–. We say that a merge of an edge
ğ‘¢ğ‘£in ğºğ‘–is (1 + ğœ–)-approximate, if (1 + ğœ–)ğ‘¤(ğ‘¢ğ‘£) â‰¥ğ‘¤ğ‘šğ‘ğ‘¥(ğºğ‘–).
A dendrogram is a tree describing all merges performed
by running a HAC algorithm. The tree has between ğ‘›and
2ğ‘›âˆ’1 nodes2 corresponding to clusters that are created in
the course of the algorithm. Specifically, there are ğ‘›leaf nodes
corresponding to the initial singleton clusters. In addition, for
each merge that creates a cluster ğ‘¥âˆªğ‘¦by merging ğ‘¥and ğ‘¦,
2We use nodes when talking about trees and vertices in the context of graphs.
3

there is a node ğ‘¥âˆªğ‘¦, whose children are ğ‘¥and ğ‘¦. Hence, each
internal node of the dendrogram has exactly two children.
In the description of the algorithm, instead of using a den-
drogram, it is more convenient to use a merge tree. Observe
that if we remove all leaf nodes from the dendrogram, there
is a natural 1-to-1 correspondence between the remaining
nodes and the merges made by the algorithm. We call the tree
obtained this way, in which each node is a merge, a merge tree.
Observe that there may be multiple different sequences of
merges made by the algorithm that produce the very same
merge tree (and dendrogram). Specifically, given a fixed merge
tree, all we know is that the first merge made by the algorithm
must have been some merge corresponding to one of its leaves.
Given a merge tree, and a sequence ğ‘š1, . . . ,ğ‘šğ‘˜of ğ‘˜distinct
merges, we say that the sequence is consistent with the tree,
if and only if for each 1 â‰¤ğ‘–â‰¤ğ‘˜, either ğ‘šğ‘–is a leaf in the
merge tree, or ğ‘šğ‘–is an internal node of the merge tree and the
children of ğ‘šğ‘–are ğ‘šğ‘and ğ‘šğ‘, where max(ğ‘,ğ‘) < ğ‘–.
We say that a dendrogram is (1 + ğœ–)-approximate if and
only if there exists a sequence of (1 + ğœ–)-approximate merges,
such that (a) the sequence is consistent with the merge tree
of the dendrogram, and (b) after performing all merges in the
sequence, no pair of clusters has nonzero similarity. Finally, a
(1 + ğœ–)-approximate HAC algorithm is one which produces a
(1 + ğœ–)-approximate dendrogram.
3
APPROXIMATE NEAREST-NEIGHBOR
CHAIN ALGORITHM
In this section we extend the 40-year old nearest-neighbor
chain algorithm [8] (NN-chain) with the notion of approxima-
tion. Let ğºbe an edge-weighted graph. For each vertex ğ‘£of
ğº, we denote by ğ‘¤max(ğ‘£) the maximum weight of any edge
incident to ğ‘£. It follows from the NN-chain / RAC algorithm
that the following algorithm is equivalent to the (exact) HAC
algorithm. As long as the graph has nonzero edges, pick any
edge ğ‘¢ğ‘£, such that ğ‘¤(ğ‘¢ğ‘£) = ğ‘¤max(ğ‘¢) = ğ‘¤max(ğ‘£) and merge
together its endpoints.
Somewhat surprisingly, even though the HAC algorithm
specifies a concrete order of merging edges, the NN-chain
algorithm computes the very same dendrogram (up to ties
among edge weights) as the standard HAC algorithm. At the
same time, the NN-chain algorithm has an important feature:
the decision on whether to merge two vertices can be made
entirely locally, which is a very useful property in a parallel
setting. In this section, we give an (1 + ğœ–)-approximate HAC
algorithm also based on a simple local criterion for deciding
whether two vertices can be merged, which we define below.
Definition 2 (Good merge). Let ğœ–â‰¥0 and ğºğ‘–be a graph
obtained from ğºby performing some sequence of merges. For
each vertex ğ‘£of ğºğ‘–, we define M(ğ‘£) to be the smallest linkage
similarity among all merges that were performed to create cluster
ğ‘£. Specifically, for each singleton cluster ğ‘£we have M(ğ‘£) = âˆ,
and whenever two clusters ğ‘¢and ğ‘£are merged and create cluster
ğ‘¢âˆªğ‘£, we have M(ğ‘¢âˆªğ‘£) = min(M(ğ‘¢), M(ğ‘£),ğ‘¤(ğ‘¢ğ‘£)). With this
notation, we say that a merge of an edgeğ‘¢ğ‘£inğºğ‘–is (1+ğœ–)âˆ’good
1
1 + Ïµ
1 + Ïµ
a
b
c
d
(1 + Ïµ)2
{a, b}
c
d
M({a, b}) = 1
1 + Ïµ
(1 + Ïµ)2
Figure 4: Example showing the need for ğ‘€(Â·) values in Defi-
nition 2. Green edges correspond to merges which are (1 + ğœ–)-
good, and red edges correspond to merges which are not (1 +ğœ–)-
good. After merging ğ‘ğ‘(which is (1 + ğœ–)-good) we obtain a
vertex {ğ‘,ğ‘} such that ğ‘€({ğ‘,ğ‘}) = 1. Therefore, the merge of
{ğ‘,ğ‘} with ğ‘in the resulting graph is not (1 + ğœ–)-good, since
max(1 + ğœ–, (1 + ğœ–)2)/min(1, âˆ, 1 + ğœ–) = (1 + ğœ–)2 > 1 + ğœ–. Hence,
the algorithm is forced to merge ğ‘with ğ‘‘. It is easy to see that
allowing a merge of {ğ‘,ğ‘} with ğ‘would create a dendrogram,
which is not (1 + ğœ–)-approximate.
if and only if
max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£))
min(M(ğ‘¢), M(ğ‘£),ğ‘¤(ğ‘¢ğ‘£)) â‰¤1 + ğœ–.
When ğœ–is clear from the context or irrelevant, we will
sometimes say good instead of (1 + ğœ–)-good. Observe that the
value of M(ğ‘¢) only depends on the sequence of merges which
created ğ‘¢. In contrast, ğ‘¤max(ğ‘¢) is a function of both ğ‘¢and
some merges outside of ğ‘¢, as it also depends on the current
set of neighbors of ğ‘¢. This means that M(ğ‘¢) is well defined
given a particular merge tree. However, to compute ğ‘¤max(ğ‘¢)
we also need to specify which merges have been performed.
We will show that applying (1 + ğœ–)-good merges leads to
a (1 + ğœ–)-approximate dendrogram (see Lemma 4). For ğœ–= 0
this generalizes the RAC algorithm, as in this case the ğ‘€(ğ‘¢)
and ğ‘€(ğ‘£) terms in the denominator become irrelevant (see
Observation 1). However, they are crucially important for
ğœ–> 0, as shown in Figure 4.
In the initial graph ğº1, a merge is good iff we have that
max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£))/ğ‘¤(ğ‘¢ğ‘£) â‰¤1+ğœ–, regardless of the max-
imum weight of an edge in the graph. In general, each (1 + ğœ–)-
approximate is (1+ğœ–)-good, but the converse is not necessarily
true. Nevertheless, we can show that performing a sequence
of good merges produces a (1 + ğœ–)-approximate dendrogram.
Before we formally state and prove this property, we first show
a few auxiliary lemmas.
Lemma 1. Let ğº1, . . . ,ğºğ‘›be a sequence of graphs, in which
each graph is obtained from the previous one by performing
an arbitrary merge. Let ğ‘£be a vertex (cluster) which exists in
ğºğ‘™, . . . ,ğºğ‘Ÿ. Let ğ‘¤ğ‘–max(ğ‘£) be the value of ğ‘¤max(ğ‘£) in ğºğ‘–, where
ğ‘™â‰¤ğ‘–â‰¤ğ‘Ÿ. Then ğ‘¤ğ‘™max(ğ‘£) â‰¥ğ‘¤ğ‘™+1
max(ğ‘£) â‰¥. . . â‰¥ğ‘¤ğ‘Ÿmax(ğ‘£).
Now, we prove a useful invariant, which is satisfied when
performing good merges.
Lemma 2. Let ğºğ‘–be a graph obtained from ğº1 by performing
(1 + ğœ–)-good merges. Then, for each vertex ğ‘£of ğºğ‘–, we have that
ğ‘¤max(ğ‘£)/M(ğ‘£) â‰¤1 + ğœ–.
Proof. We prove the claim by induction on the number of
good merges made. The base case (no merges) follows trivially
from the fact that M(ğ‘£) = âˆfor each vertex ğ‘£. Now, consider
4

that some number of good merges have been made, and the
resulting graph is ğºğ‘–+1. Fix a vertex ğ‘£. If the vertex ğ‘£also
exists in ğºğ‘–, the property follows by Lemma 1. Otherwise, ğ‘£is
created by merging two vertices ğ‘£1 and ğ‘£2 (ğ‘£= ğ‘£1 âˆªğ‘£2) using
a (1 + ğœ–)-good merge. We have
ğ‘¤max(ğ‘£)
M(ğ‘£)
â‰¤max(ğ‘¤max(ğ‘£1),ğ‘¤max(ğ‘£2))
M(ğ‘£)
â‰¤1 + ğœ–.
In the first inequality we used reducibility (Definition 1), which
implies ğ‘¤max(ğ‘£) â‰¤max(ğ‘¤max(ğ‘£1),ğ‘¤max(ğ‘£2)). The second in-
equality follows from the definition of a good merge.
â–¡
Observation 1. Let ğºğ‘–be obtained from ğº1 by performing
1-good merges. Then, a merge ğ‘¢ğ‘£in ğºğ‘–is 1-good if and only if
ğ‘¤(ğ‘¢ğ‘£) = max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£)).
The proof uses Lemma 2; we provide it in the Appendix.
Definition 3. Let ğ·be a dendrogram and ğ‘€be its correspond-
ing merge tree. Consider a sequence of merges ğ‘š1, . . . ,ğ‘šğ‘˜which
contains all merges of ğ‘€and is consistent with ğ‘€. We say that
this sequence is a greedy merge sequence of ğ·if its obtained
as follows. Start with an empty sequence. At each step append
a merge ğ‘šğ‘–of maximum weight, chosen such that the new ex-
panded sequence is consistent with ğ‘€.
A greedy merge sequence is a canonical merge sequence,
in the sense that it achieves the best approximation ratio (i.e.
the maximum approximation ratio of a merge is minimized)
out of all consistent orderings of merges of ğ‘€.
Let ğ·be a dendrogram and ğ‘€be its corresponding merge
tree. Let ğ‘š1, . . . ,ğ‘šğ‘˜be a sequence of merges consistent with
ğ‘€and ğºğ‘˜be the graph obtained after applying them. We
say that the error of a merge ğ‘šwhich merges ğ‘¢and ğ‘£is the
maximum weight of an edge in ğºğ‘˜divided by ğ‘¤(ğ‘¢ğ‘£).
Lemma 3. Let ğº= (ğ‘‰, ğ¸,ğ‘¤) be a weighted graph, ğ·be a
dendrogram of ğºand ğ‘š1, . . . ,ğ‘šğ‘›be its greedy merge sequence.
Let 1 + ğœ–be the maximum error of a merge in the produced
sequence. Then, ğ·is (1 + ğœ–)-approximate and is not (1 + ğœ–â€²)-
approximate for any ğœ–â€² < ğœ–.
Proof. Let ğ‘1, . . . ,ğ‘ğ‘›be an optimal merge sequence consis-
tent with ğ‘€, that is, a sequence in which the maximum error
of a merge is minimum possible. Let 1 + ğœ–be the maximum
error of a merge in this sequence.
We refer to the algorithm from the lemma statement as the
greedy algorithm. Note that the merge sequence ğ‘š1, . . . ,ğ‘šğ‘›
it produces is a permutation of ğ‘1, . . . ,ğ‘ğ‘›, as both sequences
consist of all merges of ğ‘€. It suffices to prove that each merge
appended by the algorithm has error at most 1 + ğœ–. We prove
that this property holds for each prefix ğ‘š1, . . . ,ğ‘šğ‘˜by induc-
tion on ğ‘˜. The case of ğ‘˜= 0 is trivial. Consider ğ‘˜> 0.
Let ğ‘—â‰¥0 be the minimum value such that ğ‘ğ‘—+1 is not one of
ğ‘š1, . . . ,ğ‘šğ‘˜. This implies that the set of merges {ğ‘š1, . . . ,ğ‘šğ‘˜}
is a superset of {ğ‘1, . . . ,ğ‘ğ‘—}. This in turn implies that the max-
imum weight in the graph after applying merges ğ‘š1, . . . ,ğ‘šğ‘˜
is not larger than the maximum weight in the graph after
applying ğ‘1, . . . ,ğ‘ğ‘—. Hence, the error of the merge ğ‘ğ‘—+1 after
applying ğ‘š1, . . . ,ğ‘šğ‘˜is at most the error of ğ‘ğ‘—+1 after applying
ğ‘1, . . . ,ğ‘ğ‘—, which by definition is at most 1+ğœ–. Since the greedy
algorithm chooses the merge of minimum error, we conclude
that the error of ğ‘šğ‘˜+1 is at most 1 + ğœ–.
â–¡
We conclude this section with our main theoretical result,
which shows that applying (1 + ğœ–)-good merges produces a
(1 + ğœ–)-approximate dendrogram. We stress that the (1 + ğœ–)-
good merges can be applied in any order (as long as they
are (1 + ğœ–)-good in that order). This allows us to remove the
greedy aspect of HAC from consideration and achieve better
parallelism.
Lemma 4. Let ğ·be a dendrogram of ğºobtained by applying
(1 + ğœ–)-good merges and ğœ–> 0. Then, ğ·is (1 + ğœ–)-approximate.
Proof. Let ğ‘š1, . . . ,ğ‘šğ‘Ÿbe a greedy merge sequence of ğ·.
We use induction on ğ‘˜to show that all merges ğ‘š1, . . . ,ğ‘šğ‘˜are
(1 + ğœ–)-approximate. For ğ‘˜= 0 the lemma is trivial, so let us
assume 0 < ğ‘˜â‰¤ğ‘Ÿ. Let ğºğ‘˜be the graph obtained by applying
the mergesğ‘š1, . . . ,ğ‘šğ‘˜âˆ’1. Let ğ‘¥ğ‘¦be the maximum weight edge
in ğºğ‘˜(chosen arbitrarily in case of a tie).
We will say that a merge ğ‘šâ€² is available if itâ€™s one of the
merges in the merge tree of ğ·, andğ‘š1, . . . ,ğ‘šğ‘˜,ğ‘šâ€² is consistent
with the merge tree of ğ·. Our goal is to show thatğ‘šğ‘˜is (1+ğœ–)-
approximate. Hence, we need to show that the merge similarity
of ğ‘šğ‘˜is at least ğ‘¤(ğ‘¥ğ‘¦)/(1 + ğœ–). To that end, it suffices to show
that there is an available merge of similarity at leastğ‘¤(ğ‘¥ğ‘¦)/(1+
ğœ–). This is because, thanks to the greedy construction, ğ‘šğ‘˜is
the available merge of maximum merge similarity.
Consider the first merges in the dendrogram ğ·, which in-
volved vertices ğ‘¥and ğ‘¦. That is, let ğ‘¥â€² be the sibling vertex of
ğ‘¥in the dendrogram ğ·and ğ‘¦â€² be the sibling vertex of ğ‘¦(i.e.,
ğ‘¥â€² is the cluster with which ğ‘¥is merged in the dendrogram).
If ğ‘¥â€² = ğ‘¦, then also ğ‘¦â€² = ğ‘¥and the merge of ğ‘¥with ğ‘¦is
available. The weight of this merge is ğ‘¤(ğ‘¥ğ‘¦), which is trivially
more than ğ‘¤(ğ‘¥ğ‘¦)/(1+ğœ–). This concludes the proof in this case.
Now consider the case when ğ‘¥â€² â‰ ğ‘¦(which also means
that ğ‘¦â€² â‰ ğ‘¥). Without loss of generality, assume that in the
merge sequence which produced dendrogram ğ·, the merge of
ğ‘¥â€² with ğ‘¥happened before the merge of ğ‘¦â€² with ğ‘¦.
Claim 1. We have ğ‘¤(ğ‘¥ğ‘¦)/min(M(ğ‘¥â€²), M(ğ‘¥),ğ‘¤(ğ‘¥ğ‘¥â€²)) =
ğ‘¤(ğ‘¥ğ‘¦)/M(ğ‘¥â€² âˆªğ‘¥) â‰¤1 + ğœ–.
The proof can be found in the Appendix.
Consider the subtree of the merge tree rooted at ğ‘¥â€² âˆªğ‘¥.
We will show that that all merges in that subtree have merge
similarity at least ğ‘¤(ğ‘¥ğ‘¦)/(1 + ğœ–). This would finish the proof,
as the subtree clearly contains at least one available merge.
Consider any available merge in that subtree and assume
that it merges together ğ‘§1 and ğ‘§2, where ğ‘§1,ğ‘§2 are both subsets
of ğ‘¥â€² âˆªğ‘¥. By the definition of M and Claim 1, ğ‘¤(ğ‘§1ğ‘§2) â‰¥
M(ğ‘¥â€² âˆªğ‘¥) â‰¥ğ‘¤(ğ‘¥ğ‘¦)/(1 + ğœ–), which completes the proof.
â–¡
4
TeraHAC ALGORITHM
The TeraHAC algorithm is presented as Algorithm 1. It takes
as input a weighted graph ğº, the accuracy parameter ğœ–and a
weight threshold ğ‘¡> 0. Each vertex ğ‘£of ğºis assigned a M(ğ‘£)
value which is initially equal to âˆ. The weight threshold ğ‘¡is
used to prune the dendrogram by performing vertex pruning,
5

Algorithm 1 TeraHAC(ğº= (ğ‘‰, ğ¸,ğ‘¤, M),ğœ–,ğ‘¡)
Input: Similarity graph ğº, ğœ–> 0, ğ‘¡â‰¥0.
Output: (1 + ğœ–)-approximate HAC dendrogram.
1: while |ğ¸| > 0 do
2:
Partition ğºinto clusters ğ¶â€²
1, . . . ,ğ¶â€²
ğ‘˜
3:
for each cluster ğ¶in ğ¶â€²
ğ‘–in parallel do
4:
ğºğ¶:= subgraph of ğºconsisting of vertices in ğ¶
and all their incident edges
5:
ğ‘€:= SubgraphHAC(ğºğ¶,ğ¶)
6:
Apply the merges ğ‘€in the graph ğº
7:
Remove from ğºvertices ğ‘£s.t. ğ‘¤max(ğ‘£) < ğ‘¡/(1 + ğœ–)
Algorithm 2 SubgraphHAC(ğº= (ğ‘‰, ğ¸,ğ‘¤, M),ğ´,ğœ–)
Input: ğºğ´= (ğ‘‰, ğ¸,ğ‘¤, M), ğ´âŠ†ğ‘‰, ğœ–> 0.
Output: A set of merges of vertices in ğ´
1: Mark vertices of ğ‘‰\ ğ´as inactive
2: ğ‘‡:= priority queue with edges ofğº[ğ´] keyed by goodness.
3: while min(ğ‘‡) â‰¤1 + ğœ–do
4:
ğ‘¢ğ‘£:= RemoveMin(ğ‘‡)
5:
Remove from ğ‘‡all edges incident to ğ‘¢or ğ‘£
6:
Mergeğ‘¢and ğ‘£inğº, and update the weights of all edges
incident to ğ‘¢âˆªğ‘£in ğº
7:
Add to ğ‘‡incident edges of ğ‘¢âˆªğ‘£that do not have an
inactive endpoint
8: Return all merges made
i.e., stop the algorithm once it performs all merges of suffi-
ciently high similarity. The effect of this parameter will be
discussed later. For now, let us assume that ğ‘¡= 0, which makes
line 7 redundant. We analyze the ğ‘¡> 0 case in Section 4.1.
The algorithm runs in a loop. Each iteration of the loop
is a round, which merges together some vertices of ğº. The
loop runs until the graph has no edges. In each round, we
first partition the graph ğºinto clusters. The choice of the
partitioning method affects the running time of the algorithm,
but from the correctness point of view, the partitioning can
be arbitrary.
Given a graph ğº= (ğ‘‰, ğ¸,ğ‘¤) and a subset of its vertices ğ¶,
we use ğºğ¶to denote a graph consisting of all vertices of ğ¶and
their immediate neighbors, as well as edges that have at least
one endpoint in ğ¶. Formally, the vertex set of ğºğ¶is ğ¶âˆª{ğ‘¥âˆˆ
ğ‘‰| âˆƒğ‘¦âˆˆğ¶ğ‘¥ğ‘¦âˆˆğ¸} and the edge set is {ğ‘¥ğ‘¦âˆˆğ¸| ğ‘¥âˆˆğ¶âˆ¨ğ‘¦âˆˆğ¶}.
Once the partitioning is computed, for each cluster ğ¶we
compute a graph . Observe that each intra-cluster edge belongs
to exactly one graph and each inter-cluster edge belongs to
exactly two graphs computed this way.
Next, the algorithm runs SubgraphHAC algorithm on each
graph ğºğ¶. The goal of SubgraphHAC is to find a longest se-
quence of merges which are (1 + ğœ–)-good (see Definition 2).
The challenge is that while the algorithm only sees a subgraph
ğºğ¶of ğº, the merges it makes should be good with respect to
the entire graph ğº. Once we have computed all merges made
by all SubgraphHAC calls, we apply them in the global graph.
Let us now describe SubgraphHAC algorithm in detail. The
algorithm is given a set ğ¶of vertices of ğºand a graph ğºğ¶.
The vertices of ğ¶in ğºğ¶are called active, and the remaining
vertices are inactive. The goal of the algorithm is to perform a
maximal sequence of good merges of active vertices in ğ¶. We
assume that vertices formed by merging active vertices are
also active. We later show that the good merges in ğºğ¶are also
good merges in the "global" graph ğº.
To decide whether a merge of ğ‘¢and ğ‘£is good, we need to
know all incident edges of ğ‘¢and ğ‘£. This is why the graph ğºğ¶
contains not only vertices of ğ¶, but also their neighbors. Since
each merge can only involve vertices of ğ¶(i.e., they do not
affect the inactive vertices), the merges performed by parallel
invocations of SubgraphHAC affect disjoint sets of vertices.
Given an edge ğ‘¢ğ‘£of ğºğ¶, where both ğ‘¢and ğ‘£are active
(that is, not inactive) we define goodness(ğ‘¢ğ‘£) = max(ğ‘¤max(ğ‘¢),
ğ‘¤max(ğ‘£))/M(ğ‘¢âˆªğ‘£). By Definition 2, a merge of ğ‘¢with ğ‘£is
(1 +ğœ–)-good iff goodness(ğ‘¢ğ‘£) â‰¤1 +ğœ–.3 At a high level, in each
step the algorithm finds the merge of the smallest goodness.
If the goodness is more than 1 + ğœ–, it terminates. Otherwise, it
performs the merge and continues.
We give a near-linear time implementation of SubgraphHAC
which performs a nearly-maximal set of (1 + ğœ–)-good merges.
More specifically, the algorithm is guaranteed to perform all
merges in the subgraph with goodness in the range [1,ğ‘‡)
where ğ‘‡= 1 + Î˜(ğœ–), and no merges with goodness > 1 + ğœ–.
We bound the running time of SubgraphHAC as follows.
Theorem 1. The running time of SubgraphHAC on a graph
containing ğ‘›vertices and ğ‘šedges is ğ‘‚((ğ‘š+ ğ‘›) log2 ğ‘›).
We provide the proof of this result in the Appendix. Our
algorithm is similar to a recent (1 + ğœ–)-approximate HAC
algorithm [26], but is significantly more complicated due to
needing to maintain not only edge weights as merges are
performed, but also the goodness values of edges.
Let us now discuss the correctness of the algorithm. It is
easy to see that each merge SubgraphHAC performs is good
(with respect to its input graph), and that it returns once no
good merges can be made. However, it is not obvious whether
the merges computed by all SubgraphHAC calls are also good
when applied on the global graph. In particular, there are two
possible issues. First, each SubgraphHAC calls only sees a
subgraph of the entire graph and computes good merges with
respect to that subgraph. Second, all merges produced by the
parallel calls are then applied on the entire graph, but we do
not specify in what order they need to be applied. We address
these in the following two lemmas.
Lemma 5. For some ğ¶âŠ†ğ‘‰, consider the graph ğºğ¶and let
ğ‘š1, . . . ,ğ‘šğ‘˜be a sequence of merges in ğºğ¶, such that (a) the
merges are (1 + ğœ–)-good with respect to ğºğ¶and (b) the merges
only involve merging active vertices of ğºğ¶. Then, the merges
ğ‘š1, . . . ,ğ‘šğ‘˜are also (1 + ğœ–)-good with respect to ğºğ‘–.
Lemma 6. Let ğœ–â‰¥0, and ğº= (ğ‘‰, ğ¸,ğ‘¤) be a graph obtained by
performing a sequence of (1 + ğœ–)-good merges. Consider ğ¶âŠ†ğ‘‰.
Letğ‘š1, . . . ,ğ‘šğ‘˜be a sequence of (1+ğœ–)-good merges with respect
to ğº, such that each merge involves two vertices of ğ¶(or vertices
created by merging them). Moreover, let Let ğ‘šâ€²
1, . . . ,ğ‘šâ€²
ğ‘™be a
3Somewhat counter-intuitively edges of lower goodness are better. Renaming
goodness to badness would be more intuitive, but would also sound negative.
6

Algorithm 3 Flatten(ğ·,ğ‘¡)
Input: A dendrogram ğ·and a linkage similarity threshold ğ‘¡
Output: A flat clustering
1: C := {}
2: for ğ‘‘âˆˆset of nodes of ğ·do
3:
if the linkage similarity of ğ‘‘â‰¥ğ‘¡and each ancestor of
ğ‘‘has linkage similarity < ğ‘¡then
4:
Add the cluster corresponding to ğ‘‘to C
5: return C
sequence of (1+ğœ–)-good merges with respect to ğº, each involving
two vertices of ğ‘‰\ ğ¶(or vertices created by merging them).
Then, any interleave of these sequences, that is any sequence
of length ğ‘˜+ ğ‘™which contains both sequences as subsequences,
is a sequence of (1 + ğœ–)-good merges with respect to ğº
Proof. Whether a merge of ğ‘¢and ğ‘£is good depends only
on ğ‘¤(ğ‘¢ğ‘£), M(ğ‘¢), M(ğ‘£), ğ‘¤max(ğ‘¢), ğ‘¤max(ğ‘£). Among these, only
ğ‘¤max(ğ‘¢) and ğ‘¤max(ğ‘£) can change due to merges not involving
vertices of ğ·. However, by Lemma 1, these values only decrease
as a result of merges. By Definition 2, the lemma follows.
â–¡
This lets us show the correctness of the TeraHAC algorithm
for any partitioning method for the case when ğ‘¡= 0.
Lemma 7. Let ğ·be a dendrogram computed by Algorithm 1
for ğ‘¡= 0 and any partitioning method used in line 2. Then, ğ·is
a (1 + ğœ–)-approximate dendrogram.
Proof. By Lemma 4 it suffices to show that ğ·is obtained by
performing a sequence of (1 +ğœ–)-good merges. Consider a sin-
gle iteration of Algorithm 1. Denote by ğºthe current graph in
the beginning of the iteration. The iteration begins by comput-
ing a clustering ğ¶1, . . . ,ğ¶ğ‘˜of ğº. Then, it runs SubgraphHAC
separately in each cluster. Hence, for each cluster ğ¶ğ‘–we obtain
a sequence of (1 + ğœ–)-good merges with respect to ğº, which
we denote by ğ‘€ğ‘–. The resulting dendrogram can be updated
with each of these sequences of merges in parallel, since each
of these sequences affects a disjoint set of vertices of ğº.
We now use Lemma 7 to show that any interleave of the
sequences ğ‘€1, . . . , ğ‘€ğ‘˜is a sequence of (1 + ğœ–)-good merges.
This can be done by using a simple induction. For ğ‘˜= 1 the
claim follows immediately, as the only sequence that can be
obtained by interleaving sequences from the set {ğ‘€1} is ğ‘€1
itself. Now assume that for ğ‘–â‰¥1 we know that any interleave
of sequences ğ‘€1, . . . , ğ‘€ğ‘–is consists of (1 + ğœ–)-good merges.
Consider any interleave Ëœğ‘€of ğ‘€1, . . . , ğ‘€ğ‘–+1. Clearly Ëœğ‘€can be
obtained by interleaving some interleave of Ëœğ‘€ğ‘–of ğ‘€1, . . . , ğ‘€ğ‘–
with ğ‘€ğ‘–+1. We now use Lemma 7 applied to sequences Ëœğ‘€ğ‘–
and ğ‘€ğ‘–+1. The former consists of (1 + ğœ–)-good merges by the
inductive assumption and the second one by the correctness
of SubgraphHAC. To apply the lemma we set ğ¶= Ãğ‘–
ğ‘—=1 ğ¶ğ‘–.
Hence, we get that Ëœğ‘€consists of (1 + ğœ–)-good merges.
â–¡
4.1
Flattening the Dendrogram
While TeraHAC computes a dendrogram, numerous clustering
applications require the algorithm to compute a flat cluster-
ing, that is a partition of the input graph vertices. A single
dendrogram induces multiple flat clusterings of varying scales.
We now show how to flatten the dendrogram, i.e., compute a
canonical collection of flat clusterings consistent with it.
Let us assume that each internal node of the dendrogram
is assigned the linkage similarity that was used to compute
the corresponding cluster. Let us also assume that the linkage
similarity of each leaf node is infinite. The algorithm that we
use for flattening a dendrogram is given as Algorithm 3. Given
a threshold ğ‘¡, it picks each dendrogram node (cluster) which
satisfies two conditions: (i) the linkage similarity of the node is
at least ğ‘¡, and (ii) the linkage similarities of all ancestor nodes
of ğ‘¡are below ğ‘¡. It is easy to see that this way we compute a
flat clustering, that is each node is in exactly one cluster.
In a dendrogram returned by an exact HAC algorithm (i.e., a
1-approximate one) the linkage similarites of the nodes along
each leaf-to-root path form a nonincreasing sequence. Hence,
flattening a dendrogram using a threshold ğ‘¡is equivalent to
performing exactly the subset of merges described by the
dendrogram whose linkage similarities are at least ğ‘¡.
This is not necessarily the case for (1 + ğœ–)-approximate
dendrograms. However, we can still show that the linkage
similarities along each leaf-to-root path are almost monotone,
as shown in the following lemma.
Lemma 8. Let ğœ–â‰¥0 and ğ·be a dendrogram obtained by
performing (1 + ğœ–)-good merges. Assume we use Algorithm 3 to
flatten ğ·using parameter ğ‘¡. Then, for each returned cluster the
minimum linkage similarity of any merge used to create it is at
least ğ‘¡/(1 + ğœ–).
Proof. Fix a cluster returned by Algorithm 3. If the clus-
ter has size 1, the lemma is trivial. Otherwise, the cluster is
obtained by merging two nodes ğ‘¥and ğ‘¦. Since the merge is
(1 + ğœ–)-good, at the point when the merge was performed
max(ğ‘¤max(ğ‘¥),ğ‘¤max(ğ‘¦))
min(M(ğ‘¥), M(ğ‘¦),ğ‘¤(ğ‘¥ğ‘¦)) â‰¤1 + ğœ–
where min(M(ğ‘¥), M(ğ‘¦),ğ‘¤(ğ‘¥ğ‘¦)) = M(ğ‘¥âˆªğ‘¦) is exactly the
minimum merge similarity of any merge used to build the
cluster. Moreover, Algorithm 3 ensures ğ‘¤(ğ‘¥ğ‘¦) â‰¥ğ‘¡. We have
M(ğ‘¥âˆªğ‘¦) â‰¥max(ğ‘¤max(ğ‘¥),ğ‘¤max(ğ‘¦))/(1 + ğœ–)
â‰¥ğ‘¤(ğ‘¥ğ‘¦)/(1 + ğœ–) â‰¥ğ‘¡/(1 + ğœ–)
â–¡
Vertex Pruning Optimization. We now analyze the vertex
pruning optimization (line 7 of Algorithm 1), which, after each
round removes vertices whose highest incident edge weight is
below ğ‘¡/(1 + ğœ–). We show that vertex pruning with parameter
ğ‘¡â€² does not affect the final result, as long as we flatten the
resulting dendrogram using a threshold ğ‘¡â‰¥ğ‘¡â€². At the same
time, as we show in our empirical evaluation, vertex pruning
brings significant performance benefits.
Lemma 9. Let ğº= (ğ‘‰, ğ¸,ğ‘¤) be a graph and ğœ–â‰¥0. Consider a
run of Algorithm 1 with a threshold parameter ğ‘¡â€² followed by
flattening the resulting dendrogram using a threshold ğ‘¡. Then,
the output of the algorithm is the same for any ğ‘¡â€² âˆˆ[0,ğ‘¡].
7

1
KVTable<VertexId, DendrogramNode> TeraHAC(
2
KVTable<VertexId, Vertex> graph, double eps,
3
double t) {
4
5
// Initialize vertex metadata: set the cluster size to
6
// 1, and the min_merge_similarity to infinity.
7
graph = Initialize(graph);
8
9
// Number of edges of weight at least t.
10
int64 num_edges = NumberOfHeavyEdges(graph, t);
11
12
// Stores the dendrogram nodes computed in each round
13
// of the while loop below.
14
std::vector<KVTable<VertexId, DendrogramNode>>
15
dendrogram_nodes;
16
17
while(num_edges > 0) {
18
// Partition the graph into clusters of <= 10M edges.
19
// Computes a cluster id of each vertex.
20
KVTable<VertexId, ClusterId> cluster_ids =
21
AffinityClustering(graph);
22
23
// Join graph and cluster_ids on matching VertexId,
24
// and key vertices by the cluster id.
25
KVTable<ClusterId,
26
pair<VertexId, Vertex>> clusters =
27
KeyByClusterId(graph, cluster_ids);
28
29
KVTable<VertexId, Dendrogram> new_nodes;
30
KVTable<VertexId, ClusterId> sh_cluster_ids;
31
KVTable<ClusterId, double> min_merge;
32
33
// Run SubgraphHAC within each cluster.
34
(new_nodes, sh_cluster_ids, min_merge) =
35
SubgraphHac(clusters.GroupByKey(), eps);
36
dendrogram_nodes.push_back(new_nodes);
37
38
// Vertex pruning: remove nodes, whose max incident
39
// edge weight is below t/(1+eps).
40
graph = graph.Apply(Prune(graph, t/(1+eps)))
41
// Contract each cluster to a node.
42
.Apply(Contract(graph, sh_cluster_ids))
43
// Join the graph with min_merge on
44
//matching keys.
45
.Apply(SetMinMerge(min_merge))
46
// Remove each vertex with no edges.
47
.Apply(RemoveIsolatedVertices());
48
49
num_edges = NumberOfHeavyEdges(graph, t);
50
}
51
52
return MergeDendrograms(dendrogram_nodes);
53
}
Figure 5: TeraHAC pseudocode
We acknowledge that the fact that TeraHAC requires the
pruning optimization (which limits the output to the "bottom"
part of the dendrogram) to achieve the best performance is a
limitation of the algorithm. At the same time, this limitation
is also present in the existing methods providing similar scala-
bility, that is affinity clustering [7] and SCC [40]. We note that
on the datasets we studied in Section 6 using this optimization
makes very little impact on the clustering quality.
5
TeraHAC IMPLEMENTATION
We have implemented TeraHAC in Flume-C++ [2], an opti-
mized distributed data-flow system similar to Apache Beam [3].
A C++-based pseudocode for TeraHAC is given in Fig. 5. A
KVTable<K, V> is a distributed collection of key-value pairs,
where the keys have type K and values have type V [15]. In par-
ticular, the input graph to TeraHAC is a KVTable<VertexId,
Vertex>. Here, VertexId is a type used to represent unique
ids of vertices and Vertex represents a vertex and its metadata.
Specifically, each vertex consists of
â€¢ the list of its incident edges, where each edge is repre-
sented as a pair of a VertexId specifying the edge end-
point and a floating-point number giving the weight,
â€¢ the size of the cluster represented by the vertex (initial-
ized to 1 on Line 7),
â€¢ the minimum merge similarity used to create the cluster
represented by the vertex (initialized to âˆon Line 7).
The result of the TeraHAC algorithm is a dendrogram rep-
resented as a KVTable<VertexId, DendrogramNode>. Each
node is represented by DendrogramNode, containing two fields:
the id of the parent node in the dendrogram, and the linkage
similarity of the merge that created the parent node. Both fields
are unset if the node represents a root in the dendrogram.
The algorithm runs in a loop, until the graph has no edges
of weight at least ğ‘¡. Each round first partitions the graph (see
Line 2 of Algorithm 1). To this end we use the affinity clustering
algorithm of Bateni et al. [7], which works as follows. First,
each vertex marks its highest weight incident edge. Then, we
find connected components spanned by the marked edges,
which define the clusters. Since each cluster computed by
affinity clustering is later processed on a single machine, we
use a size-constrained version of the algorithm, which ensures
that each cluster contains at most 10 million edges [27].
The choice of affinity clustering is motivated by the follow-
ing heuristic argument. Intuitively, the goal is to use partition-
ing in which many edges inducing good merges (Definition 2)
have both endpoints in the same cluster. From a vertex point of
view its highest weight incident edge is most likely to induce
a good merge (if we donâ€™t know its neighborsâ€™ metadata), and
each such edge is an intra-cluster one in affinity clustering
(unless the cluster is split to ensure the size constraint).
SubgraphHAC. Once we have found affinity clusters, we
aggregate the vertices of each cluster on a single machine
and run SubgraphHAC on each cluster. This returns three re-
sults. First, we obtain the set of new dendrogram nodes corre-
sponding to the merges made by the SubgraphHAC algorithm
(new_nodes). Second, SubgraphHAC computes the clustering
(sh_cluster_ids) induced by performing all (1 + ğœ–)-good
merges made in this round. This clustering maps the vertex
ids of the graph in this round to cluster ids. Third, for each
cluster computed by SubgraphHAC, it computes the minimum
merge similarity used to obtain that cluster. For a fixed cluster
returned by SubgraphHAC, the cluster is obtained by merging
together some vertices, which correspond to clusters formed in
the previous rounds of TeraHAC. Hence, to compute the corre-
sponding minimum merge similarity, we take the minimum of
the similarities of the merges used to create the cluster in this
particular SubgraphHAC call, as well as the minimum merge
similarities stored in the vertices in the cluster. SubgraphHAC
processes each cluster on a single machine, so all its three
return values can be easily computed based on the vertices of
the provided cluster, and the merges which were made.
Vertex Pruning and Contraction. To complete the round,
we first apply vertex pruning, and update the graph based
8

0
0.001
0.01
0.05 0.1
0.5
1.0
Ïµ
1.0
1.2
1.4
1.6
1.8
Score
ARI
NMI
Empirical Approximation
Purity
0
0.001
0.005 0.01
0.05 0.1
0.5
Weight Threshold (t)
0.86
0.88
0.90
0.92
0.94
Score
ARI
NMI
Purity
0
0.0001
0.001
0.01
0.1
Weight Threshold (t)
10
20
30
40
50
Num Rounds
OK
TW
FS
Figure 6: Quality of TeraHACğ‘¡=0
ğœ–
run
with varying ğœ–on the Digits dataset
graph constructed using ğ‘˜= 25 (qual-
ity measures defined in Section 6.1).
Figure 7: Quality of TeraHACğ‘¡
ğœ–=0.1 run with
varying thresholds ğ‘¡on the Iris dataset graph
constructed using ğ‘˜= 25.
Figure 8: Num. rounds of TeraHACğ‘¡
ğœ–=0.1
run with varying weight threshold (ğ‘¡) on
OK, TW, and FS. Higher thresholds use
fewer rounds.
Table 1: Graph inputs, including the number of vertices (ğ‘›),
number of directed edges (ğ‘š), and the average degree (ğ‘š/ğ‘›).
We show the statistics of the largest rMAT graph that we use.
Graph Dataset
Num. Vertices
Num. Edges
Avg. Deg.
com-Orkut (OK)
3,072,627
234,370,166
76.2
Twitter (TW)
41,652,231
2,405,026,092
57.7
Friendster (FS)
65,608,366
3,612,134,270
55.0
rMAT-28 (RM28)
268,435,456
25,814,014,562
96.3
ClueWeb (CW)
978,408,098
74,744,358,622
76.3
Hyperlink (HL)
3,563,602,789
225,840,663,232
63.3
Web-Query (WQ)
31,764,801,992
8,670,265,361,938
272.9
on the result of SubgraphHAC. The key operation is to apply
all the merges performed by SubgraphHAC in this round. We
note that this is equivalent to contracting each cluster com-
puted by SubgraphHAC to a single vertex. This is what the
Contract function does. Assume that the vertex to cluster id
mapping is given by a function ğ¶. Contract first remaps each
vertex id ğ‘¥to ğ¶(ğ‘¥), both in the keys of the graph KVTable
and in the adjacency lists stored in Vertex objects. This is
achieved by two joins of graph with cluster_ids. Once all
ids are remapped, vertices from the same cluster share the
same KVTable key, and so we group them by key and merge
together to obtain the final vertices. Hence, the vertex ids in the
new graph are the cluster ids computed by SubgraphHAC4.
Next, we update the minimum merge similarity metadata,
by joining the graph with the respective metadata computed by
SubgraphHAC. We finish updating the graph by applying an
optimization: each vertex with no outgoing edges is removed,
as it would clearly not participate in any merge anymore.
Our implementation is optimized to minimize the num-
ber of joins/shuffle steps. In particular, while a standalone
implementation of Prune and SetMinMerge would require a
join/shuffle, in the implementation we actually extended the
joins performed in Contract to perform these operations.
Shared-Memory Implementation. To enable reproducibil-
ity, we also provide a shared-memory implementation of TeraHAC5
Our shared-memory implementation of TeraHAC is written
in the same parallel framework as ParHAC, using the Parlay
libraries for parallel primitives [10] and the CPAM system for
representing dynamic graphs [24]. We provide (1) a faithful
4This means that we use the same type to represent vertex and cluster ids.
However, we decided to make the distinction to improve readability.
5https://github.com/google-research/google-research/tree/master/terahac
implementation of the size-constrained affinity algorithm [27]
used in our distributed implementation, (2) an efficient im-
plementation of SubgraphHAC that runs in ğ‘‚(ğ‘šlog2 ğ‘›) time,
and (3) an efficient implementation of weighted graph con-
traction to maintain the edge weights of the contracted graph
after each round of partitioning and SubgraphHAC.
Although optimizing the running time of approximate HAC
in the shared-memory setting was not the main goal of this
paper, we found that our implementation of TeraHAC actu-
ally achieves consistent speedups over ParHAC, even after
optimizing ParHAC to apply the vertex pruning optimizations
used in TeraHAC (see Section 6.2).
6
EMPIRICAL EVALUATION
Graph Data. We list information about graphs used in our
experiments in Table 1. com-DBLP (DB) is a co-authorship
network sourced from the DBLP computer science bibliog-
raphy. 6 YouTube (YT) is a social-network formed by user-
defined groups on the YouTube site. 7 LiveJournal (LJ) is
a directed graph of the social network. 8 com-Orkut (OK)
is an undirected graph of the Orkut social network. Friend-
ster (FS) is an undirected graph describing friendships from
a gaming network. Both graphs are sourced from the SNAP
dataset [37].9 Twitter (TW) is a directed graph of the Twitter
network, where edges represent the follower relationship [35].
10 ClueWeb (CW) is a web graph from the Lemur project at
CMU [12]. 11 Hyperlink (HL) is a hyperlink graph obtained
from the WebDataCommons dataset where nodes represent
web pages [39]. 12 The Web-Query graph consists of vertices
representing 31 billion web queries, with 8.6 trillion edge
weights corresponding to similarities between the queries
(see Section 6.3 for details). Lastly, we run a set of scaling
experiments on synthetic rMAT graphs [14], which are con-
structed using the parameters ğ‘= 0.6,ğ‘= ğ‘= 0.15,ğ‘‘= 0.1.
The rMAT parameters were chosen to produce real-world-
like graphs, following [58]. rMAT-ğ‘‹denotes an RMAT graph
6Source: https://snap.stanford.edu/data/com-DBLP.html.
7Source: https://snap.stanford.edu/data/com-Youtube.html.
8Source: https://snap.stanford.edu/data/soc-LiveJournal1.html.
9Source: https://snap.stanford.edu/data/.
10Source: http://law.di.unimi.it/webdata/twitter-2010/.
11Source: https://law.di.unimi.it/webdata/clueweb12/.
12Source: http://webdatacommons.org/hyperlinkgraph/.
9

with 2ğ‘‹nodes and 50 Â· 2ğ‘‹undirected edges (before removing
duplicate edges).
We note that the large real-world graphs that we study are
not weighted, and so we set the similarity of an edge (ğ‘¢, ğ‘£) to
1
log(ğ‘‘ğ‘’ğ‘”(ğ‘¢)+ğ‘‘ğ‘’ğ‘”(ğ‘£)) . We use this weighting scheme since it pro-
motes merging low-degree vertices over high-degree vertices
(unless there are sufficiently many edges); the same scheme
was also used by recent work on the topic [25, 26].
Building Similarity Graphs from Pointsets. Our quality
experiments run on graphs built from a pointset by computing
the approximate nearest neighbors (ANN) of each point, and
converting the distances to similarities. We convert distances
to similarities using the formula sim(ğ‘¢, ğ‘£) =
1
1+dist(ğ‘¢,ğ‘£) . We
then scale the similarities by dividing each similarity by the
maximum similarity to ensure that the maximum similarity is
1. The same method of similarity graph construction was also
used in a recent paper on shared-memory parallel HAC [25].
Machine Configuration. Our experiments are performed
in a shared datacenter managed using Borg [57]. Hence, our
jobs compete for resources with other jobs, and run alongside
other jobs on the same machines. We use a maximum of 100
machines to solve all problems and a maximum of 800 hyper-
threads across all machines, unless mentioned otherwise. For
details of the network topology, see [50]. We ran each experi-
ment three times, and find that the difference in running times
across different trials was not significant (within 10%), unless
specified otherwise.
Algorithms. We compare TeraHAC with several HAC base-
lines. We use TeraHACğ‘¡=ğ‘¦
ğœ–=ğ‘¥to denote TeraHAC run with pa-
rameters ğœ–= ğ‘¥and weight threshold ğ‘¡= ğ‘¦. When not spec-
ified, TeraHAC denotes running the algorithm with ğœ–= 0.1
and ğ‘¡= 0.01, which are default parameter settings that we
empirically find work well across a wide range of datasets.
Importantly, setting ğœ–= 0 yields the exact HAC algorithm.
We also compare our algorithm with an optimized distributed
implementation (written in the same framework) of the re-
cently developed SCC algorithm [40], which we refer to as
SCC. The SCC algorithm is parameterized by ğ‘Ÿ, the number
of rounds used (denoted SCC-r). Increasing ğ‘Ÿwas observed to
increase the quality of the algorithm [40]. We evaluate a lower-
quality (SCC-5), medium-quality (SCC-25) and high-quality
(SCC-100) setting. We also (indirectly) compare our algorithm
with RAC (e.g., in Fig. 2, discussed in Section 1). Although we
did not perform a running time comparison between TeraHAC
and RAC in this paper, TeraHAC using ğœ–= 0 should be strictly
superior to RAC in practice and can be viewed essentially as
an optimized version of the RAC algorithm. This is because
for ğœ–= 0 TeraHAC performs 1-good merges, which are exactly
the merges that the RAC algorithm can perform (see Obser-
vation 1). However, contrary to RAC, TeraHAC may perform
multiple merges involving the same vertex in a single round.
6.1
Quality
We start by understanding the quality of TeraHAC as a func-
tion of both ğœ–and the weight threshold ğ‘¡. All results in this sec-
tion can be reproduced using our shared-memory implemen-
tation. We compare the quality to known ground-truth cluster-
ings for the iris, wine, digits, and faces classification datasets
from the UCI dataset repository (found in the sklearn.datasets
package). Unfortunately, we are not aware of other large-scale
publicly available datasets providing ground truth clustering
labels. Our goal in this sub-section is to understand the largest
values of ğœ–and ğ‘¡that can be used without damaging accu-
racy. Note that in our experiments, weights in the graph are
always within [0, 1]. To measure quality, we use the Adjusted
Rand-Index (ARI) and Normalized Mutual Information
(NMI) scores, which are standard measures of the quality of a
clustering with respect to a ground-truth clustering. We also
use the Dendrogram Purity measure [29], which takes on
values between [0, 1] and takes on a value of 1 if and only if the
tree contains the ground truth clustering as a tree consistent
partition (i.e., each class appears as exactly the leaves of some
subtree of the tree). Given a tree ğ‘‡tree with leaves ğ‘‰, and
a ground truth partition of ğ‘‰into ğ¶= {ğ¶1, . . . ,ğ¶ğ‘™} classes,
define the purity of a subset ğ‘†âŠ†ğ‘‰with respect to a class ğ¶ğ‘–
to be P(ğ‘†,ğ¶ğ‘–) = |ğ‘†âˆ©ğ¶ğ‘–|/|ğ‘†|. Then, the purity of ğ‘‡is
P(ğ‘‡) =
1
|Pairs|
ğ‘™âˆ‘ï¸
ğ‘–=1
âˆ‘ï¸
ğ‘¥,ğ‘¦âˆˆğ¶ğ‘–,ğ‘¥â‰ ğ‘¦
P(lcağ‘‡(ğ‘¥,ğ‘¦),ğ¶ğ‘–)
where Pairs = {(ğ‘¥,ğ‘¦) | âˆƒğ‘–s.t. {ğ‘¥,ğ‘¦} âŠ†ğ¶ğ‘–} and lcağ‘‡(ğ‘¥,ğ‘¦) is
the set of leaves of the least common ancestor of ğ‘¥and ğ‘¦
in ğ‘‡. We also study the unsupervised Dasgupta Cost [20]
measure of our dendrograms, which is measured with respect
to an underlying similarity graph ğº(ğ‘‰, ğ¸,ğ‘¤) and is defined as:
Ã
(ğ‘¢,ğ‘£)âˆˆğ¸|lcağ‘‡(ğ‘¢, ğ‘£)| Â·ğ‘¤(ğ‘¢, ğ‘£). The Dasgupta cost is computed
over the complete similarity graph obtained by computing
all point-to-point distances. Lastly, given a dendrogram and
its greedy merge sequence we compute for each merge the
ratio between the highest similarity edge in the graph and the
similarity of the merge, and take the maximum of these ratios
to be the Empirical Approximation Ratio (see Lemma 3). For
a (1 + ğœ–)-approximate algorithm, the empirical approximation
ratio is at most 1 + ğœ–.
Before jumping in, we make a few observations. If we use
a weight threshold of ğ‘¡= 0, the algorithm will compute a
complete (1 + ğœ–)-HAC dendrogram (similar to ParHAC or
SeqHAC), but as we shall see in Section 6.2, computing the full
(1 + ğœ–)-dendrogram results in the size of the graph shrinking
more slowly, and thus requires more rounds. On the other
hand, using a weight threshold of ğ‘¡> 0 will result in the
algorithm potentially running much faster due to the size of the
graph shrinking rapidly (as we show in Section 6.2). However,
from a quality perspective, a worry is that in exchange for
speed, we will sacrifice quality. In this sub-section, we will
understand how to set ğœ–and ğ‘¡, and will demonstrate that this
worry is unfounded. We will show the following key points:
(1) Without thresholding, values of ğœ–âˆˆ[0, 0.5] yield results
that are typically within a few percent of the accuracy of
10

Table 2: Adjusted Rand-Index (ARI), Normalized Mutual Information (NMI), Dendrogram Purity, and Dasgupta cost of TeraHAC
(columns 2â€“5) versus SCC-5, SCC-25, SCC-100 (columns 6â€“8), the HAC implementation of average linkage from SciPy (column 9),
and the DBSCAN implementation from SciPy (column 10). Note that TeraHACğ‘¡=0
ğœ–=0 is the same as the RAC algorithm. For DBSCAN, we
perform a grid search forğœ–âˆˆ[0.01, 10000] and minpts âˆˆ[2, 128] and use the best scores that we obtain. All graph-based implementations
are run over the similarity graph constructed from an approximate ğ‘˜-NN graph with ğ‘˜= 25. The Dasgupta cost is computed over the
complete similarity graph generated from the all-pairs distance graph, and thus takes into account all pairwise similarities. In the
case of Dasgupta cost, lower values are better. For the remaining measures, higher values are better. We display the best quality
score for each graph in green and underlined. We show the relative improvement of each score over the score for TeraHACğ‘¡=0
ğœ–=0 in
parenthesis after each score.
Dataset
TeraHACğ‘¡=0
ğœ–=0
TeraHACğ‘¡=0.01
ğœ–=0
TeraHACğ‘¡=0
ğœ–=0.1
TeraHACğ‘¡=0.01
ğœ–=0.1
SCC-5
SCC-25
SCC-100
Sci-Avg
DBSCAN
ARI
iris
0.92 (1x)
0.92 (1x)
0.92 (1x)
0.92 (1x)
0.74 (.80x) 0.79 (.86x) 0.87 (.94x)
0.75 (.82x)
0.56 (.62x)
wine
0.37 (1x)
0.37 (1x)
0.37 (1x)
0.37 (1x)
0.30 (.81x) 0.22 (.61x) 0.24 (.64x)
0.35 (.94x)
0.39 (1.05x)
digits
0.88 (1x)
0.88 (1x)
0.87 (.99x)
0.85 (.97x)
0.87 (.99x) 0.83 (0.94x) 0.82 (.93x)
0.69 (.78x)
0.31 (.36x)
faces
0.57 (1x)
0.57 (1x)
0.56 (.98x)
0.56 (.98x)
0.39 (.68x) 0.51 (.89x) 0.55 (.96x)
0.52 (.92x)
0.098 (.17x)
NMI
iris
0.89 (1x)
0.89 (1x)
0.89 (1x)
0.89 (1x)
0.75 (.84x) 0.77 (.86x) 0.84 (.94x)
0.80 (.89x)
0.73 (.82x)
wine
0.42 (1x)
0.41 (1x)
0.42 (1x)
0.42 (1x)
0.37 (.88x) 0.38 (.90x) 0.38 (.90x)
0.42 (1x)
0.45 (1.07x)
digits
0.90 (1x)
0.90(1x)
0.89 (.98x)
0.89 (.98x)
0.90 (1x)
0.87 (.96x) 0.87 (.96x)
0.83 (.92x)
0.66 (.73x)
faces
0.86 (1x)
0.86 (1x)
0.86 (1x)
0.86 (1x)
0.83
0.85
0.86 (1x)
0.86 (1x)
0.68
Purity
iris
0.94 (1x)
0.94 (1x)
0.95 (1.01x)
0.95 (1.01x)
â€“
â€“
â€“
0.86 (.90x)
â€“
wine
0.62 (1x)
0.60 (.96x)
0.62 (1x)
0.62 (1x)
â€“
â€“
â€“
0.62 (1x)
â€“
digits
0.88 (1x)
0.87 (.98x)
0.87 (.98x)
0.85 (.96x)
â€“
â€“
â€“
0.75 (.85x)
â€“
faces
0.61 (1x)
0.61 (1x)
0.58 (.95x)
0.58 (.95x)
â€“
â€“
â€“
0.62 (1.01x)
â€“
Dasgupta
iris
321290 (1x)
321290 (1x)
320846 (1x)
320846 (1x)
â€“
â€“
â€“
310957 (1.03x)
â€“
wine
26904 (1x)
28581 (.94x)
26902 (1x)
26902 (1x)
â€“
â€“
â€“
27324 (.98x)
â€“
digits
243191685 (1x) 243087881 (1x) 243323801 (.99x) 245791675 (.98x)
â€“
â€“
â€“
240476750 (1.01x)
â€“
faces
4631561 (1x)
4631561 (1x)
4627286 (1x)
4627286 (1x)
â€“
â€“
â€“
4569916 (1.01x)
â€“
the exact algorithm. Based on our results, we choose a
value of ğœ–= 0.1 for our experiments.
(2) Fixing ğœ–= 0.1, values of ğ‘¡â‰¤0.01 reliably achieve very
high quality on our datasets with ground-truth.
Tuning ğœ–. We ran TeraHAC with varying values of ğœ–with-
out thresholding to understand what value of ğœ–is sufficient
for achieving good quality relative to the exact baseline of
ğœ–= 0. The experiment runs TeraHAC on similarity graphs
constructed from a ğ‘˜-NN graph of the underlying pointset,
with ğ‘˜= 25, as described earlier in Section 6, and consistent
with prior work [25, 26]. We use all 4 UCI datasets mentioned
earlier; due to space constraints we show a representative
result for the digits dataset in Fig. 6 for all quality measures.
Perhaps surprisingly, Fig. 6 shows that the quality measures
do not degrade significantly as ğœ–is increased. We noticed sim-
ilar trends for other values of ğ‘˜; the reason is likely due to
the fact that in our experiments, SubgraphHAC makes very
similar merges to ğœ–= 0 even when using larger values of ğœ–.
We select the value ğœ–= 0.1 to be consistent with the choice
of ğœ–in prior papers on approximate HAC [25, 26]. Across all
datasets, we found that for ğœ–â‰¤0.1 on average the outputs
are within 1.3% of the best ARI score, within 0.25% of the best
NMI score, within 2.6% of the best Purity score, and within
1.6% of the best Dasgupta score. Fig. 6 empirically confirms
our theoretical result that the approximation factor is always
within a factor of (1 + ğœ–). For the remainder of the paper, we
use a value of ğœ–= 0.1 unless mentioned otherwise.
Tuning the Threshold (ğ‘¡) with ğœ–= 0.1. Computing the
complete dendrogram can be unnecessary if the flat clustering
we seek uses a large threshold (i.e., does not merge edges
with similarity below ğ‘¡). As Fig. 8 illustrates, larger thresholds
require fewer rounds to compute, and can thus significantly
improve the running time. To understand how to select an
appropriate threshold, ğ‘¡, we run TeraHAC with varying values
of ğ‘¡, while fixing ğœ–= 0.1. Fig. 7 shows a representative result
on the iris dataset, showing essentially no difference across
ğ‘¡âˆˆ[0, 0.1]. We tuned the threshold and find that for ğ‘¡= 0.01,
across all datasets, the accuracy results are very close to that
of ğ‘¡= 0; on average within 0.4% of the ARI score of ğ‘¡= 0,
within 2.6% of the Purity score of ğ‘¡= 0, and within 1.6% of
the Dasgupta cost of ğ‘¡= 0. We find that ğ‘¡= 0.01 achieves the
same NMI as ğ‘¡= 0. We set ğ‘¡= 0.01 in the remainder of the
paper.
Quality Comparison. Table 2 shows the results of our quality
study when comparing TeraHAC with different settings of ğœ–
and ğ‘¡with the SCC algorithm. The non-zero threshold we use
(ğ‘¡= 0.01) is tuned as described above. The SCC algorithm can
vary the number of rounds, ğ‘Ÿ, that is uses, which results in ğ‘Ÿ
flat clusterings. We compute the quality of SCC by evaluating
every flat clustering it produces and using the best score across
any clustering (i.e., for ğ‘Ÿ= 25 we evaluate all 25 flat cluster-
ings). We also compare with the exact ğ‘‚(ğ‘›2) time average-
linkage metric HAC implementation from sklearn [54] as well
as the DBSCAN implementation from sklearn [53]. Both algo-
rithms compute the full distance matrix and thus run in ğ‘‚(ğ‘›2)
time. For DBSCAN, we set ğœ–and minpts by searching over the
11

12
14
16
18
20
22
24
26
28
RMAT Scale
0
50000
100000
150000
200000
Running Time in Seconds
SCC-5
SCC-25
SCC-100
TeraHAC
OK
TW
FS
CW
HL
100
101
102
Relative performance
6.3e+03
1.04e+04
1.05e+04
2.88e+04
6.12e+04
3.76e+03
6.66e+03
8.1e+03
2.29e+04
3.35e+04
1.2e+04
2.3e+04
2.99e+04
6.56e+04
6.04e+04
3.93e+04
8.71e+04
1.11e+05
2.35e+05
x
Values on top of bars are running times (in seconds)
TeraHAC
SCC-5
SCC-25
SCC-100
0
2
4
6
8
10
Round Number
10âˆ’2
10âˆ’1
100
Fraction of Edges (Nodes) Remaining
Edges
Nodes
Figure 9: Running time in seconds
of TeraHACğ‘¡=0.01
ğœ–=0.1 and SCC using low,
medium, and high-quality settings
(ğ‘Ÿ= {5, 25, 100},ğ‘¡= 0.01) on rMAT
graphs of varying scales.
Figure 10: Relative performance (speedup
over the fastest algorithm for each graph) of
TeraHACğ‘¡=0.01
ğœ–=0.1 and SCC using low, medium, and
high-quality settings (ğ‘Ÿ= {5, 25, 100},ğ‘¡= 0.01)
on real-world social and Web graphs.
Figure 11: Decrease in number of edges
and nodes as a function of the round
number in TeraHACğ‘¡=0.01
ğœ–=0.1 for rMAT-28.
DB
YT
RD
LJ
OK
TW
FS
100
101
102
103
104
Relative performance
6.8
96
230
1.2e+03
4.18e+03
x
x
1.12
3.44
11.2
18.4
32.2
623
951
0.74
1.52
6.08
10.8
23.5
337
423
0.14
0.53
5.42
4.77
22.5
132
246
Values on top of bars are running times (in seconds)
SeqHAC
ParHAC
ParHAC-Prune
TeraHAC
Figure 12: Relative performance of TeraHAC vs. ParHAC and
SeqHAC, all using ğœ–= 0.1. ParHAC is the original algorithm, and
ParHAC-Prune is the original algorithm using vertex pruning.
range ğœ–âˆˆ[0.01, 10000] and minpts âˆˆ[2, 128]. We consider
four settings for TeraHAC; ğœ–= 0,ğ‘¡= 0, which yields an exact
HAC dendrogram that is equivalent to the RAC algorithm;
ğœ–= 0,ğ‘¡= 0.01; ğœ–= 0.1,ğ‘¡= 0; and lastly, using ğœ–= 0.1,ğ‘¡= 0.01.
All settings of TeraHAC achieve high-quality results across all
of the quality measures that we evaluate, across all datasets.
Our results in Table 2 show that TeraHAC achieves quality
within a few percentage points of the best result on all datasets:
we find that for ğœ–â‰¤0.1 on average the outputs are within
1.3% of the best ARI score, within 0.25% of the best NMI score,
within 2.6% of the best Purity score, and within 1.6% of the best
Dasgupta score. The results for the exact parameter settings
of TeraHAC show that thresholding using ğ‘¡= 0.01 has little
effect. We also observe that SCC is similarly affected, and this
algorithm, which is the prior state-of-the-art in scalable HAC,
also requires careful selection of ğ‘¡. SCC generally improves
in quality as we increase the number of compression rounds,
with the exception of the digits dataset, where ğ‘Ÿ= 5 achieves
good quality. Comparing SCC to TeraHAC, TeraHACğ‘¡=0.01
ğœ–=0.1 is
superior to SCC with the exception of the digits dataset, where
SCC-5 achieves 0.4% better ARI and 1.2% better NMI. Based
on our results, we classify ğ‘Ÿ= {5, 25, 100} as low, medium, and
high-quality settings of SCC, respectively. Lastly, compared to
DBSCAN, for these datasets, HAC-based algorithms typically
yield much higher quality. However, on the Wine dataset,
DBSCAN finds clusterings that have up to 5.6% higher ARI
and 6.5% higher NMI than TeraHAC (and also exact HAC).
6.2
Scalability
Thus far, we have seen that the quality of TeraHAC with ğœ–=
0.1,ğ‘¡= 0.01 is close to that of an exact HAC baseline. Next we
evaluate the scalability of our algorithm under these settings.
Scalability on Large Graphs. We study the scalability of
TeraHAC compared with SCC on the rMAT graph family de-
scribed at the start of this section. The average-degree of these
graphs is close to 100, and is thus similar to our other real-
world graphs (see Table 1). Fig. 9 shows the result of our ex-
periments. We observe that the running time for TeraHAC
lies between that of SCC-5 and SCC-25. For the largest rMAT
graph that we study, which contains 268 million vertices and
25.8 billion edges, TeraHAC is 11.4x faster than SCC-100, 2.23x
faster than SCC-25, and only 1.45x slower than SCC-5.
Fig. 10 shows the scalability of TeraHAC compared with
SCC on five large real-world graphs, including the largest pub-
licly available graph, the WebDataCommons Hyperlink graph
(HL). Similar to our results on the rMAT graph family, we
find that TeraHAC performs between SCC-5 (0.67x as fast on
average) and SCC-25 (2.04x as fast on average). The algorithm
is significantly faster than SCC-100 (8.3x faster on average),
which is unable to finish within four days on HL.
Taken together with our results in Section 6.1, TeraHAC is
nearly as fast as the low-quality variant of SCC, while achiev-
ing much higher quality than the high-quality variant of SCC.
Good Edges and Round-Complexity. As our results in Fig. 2
in Section 1 show, TeraHAC uses several orders of magnitude
fewer rounds than RAC and ParHAC. To better understand
the reason behind the low round-complexity, we study the
number of good edges (i.e., (1 + ğœ–)-good edges) available to the
algorithm in each round. This measure directly captures edges
that can potentially participate in a merge. Fig. 15 shows a
representative result for the CW graph. The plots for other
graphs are similar. The number of rounds used by TeraHAC
with ğœ–= 0.1 is significantly lower than TeraHAC with ğœ–= 0.
Initially, the number of good edges with ğœ–= 0.1 is one order
of magnitude larger than with ğœ–= 0. Although TeraHAC with
ğœ–= 0 uses more rounds than with ğœ–= 0.1, as shown in Fig. 2,
TeraHAC using ğœ–= 0 still uses many fewer rounds than RAC,
although both algorithms solve HAC exactly.
12

0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
Precision
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
0.225
Recall
TeraHAC
SCC-50
DBSCAN
SCC-5
0
10
20
30
40
50
Round Number
10âˆ’8
10âˆ’6
10âˆ’4
10âˆ’2
100
Fraction of Edges Remaining
TeraHAC
SCC-50
SCC-5
0
25
50
75
100
125
150
175
Round Number
101
103
105
107
109
1011
Num Edges Remaining
directed-edges, Ïµ = 0.1
directed-edges, Ïµ = 0.0
good-edges, Ïµ = 0.1
good-edges, Ïµ = 0.0
Figure 13: Precision-recall tradeoffs
for TeraHAC and SCC on the large-
scale web-query dataset.
Figure 14: Reduction in the number of edges
for the web-query dataset. We note that the fig-
ure plotting the reduction in the number of
nodes looks almost identical.
Figure 15: Number of good and directed
edges in each round on the CW graph
(ğ‘›=978M, ğ‘š=74.7B). The plots for other
graphs show similar behavior when com-
paring TeraHAC with ğœ–= 0.1 and ğœ–= 0.
Table 3: Median running times on the web-query dataset.
TeraHAC
SCC-50
SCC-5
DBSCAN
1280
2634
690
195
Shared-Memory Performance. TeraHAC has the ability to
significantly reduce the number of rounds required for ap-
proximately solving HAC at a given weight threshold. We
have found that our preliminary experiments with a shared-
memory implementation of the algorithm show that its low
round-complexity also translates into strong shared-memory
performance. We study the scalability of TeraHAC compared
with state-of-the-art single-machine HAC implementations,
namely SeqHAC [26] and ParHAC [25] on a 72-core Dell Pow-
erEdge R930 (with two-way hyper-threading) with 4Ã—2.4GHz
processors and 1TB of main memory. We show the results
of our shared-memory experiment in Fig. 12. TeraHAC is al-
ways as fast or faster than all baselines, ranging from 1.45â€“8x
speedup over ParHAC and 48.5â€“185x speedup over SeqHAC.
We also obtain between 1.04â€“5.28x speedup over a version
of ParHAC using the same vertex pruning optimization as
TeraHAC before processing each bucket. TeraHAC seems to
have significant potential in the shared-memory setting and
we plan to further investigate this direction in future work.
6.3
Large Scale Clustering of Web Queries
Lastly, we study the quality of TeraHAC on a large-scale dataset
of web search queries. We use a graph whose vertices repre-
sent queries, and edges connect queries of similar meaning.
The weight of each edge is computed using a machine-learned
model based on BERT [22]. The graph has about 31 billion
vertices and 8 trillion edges, and resembles the graph used in
the evaluation of the SCC algorithm [40]. We use a maximum
of 48 000 cores across 6 000 machines.
For evaluation, we use a sample of 53 659 pairs of queries.
Each pair is assigned a human-generated binary label specify-
ing whether the two queries likely carry the same intent and
thus should belong to the same cluster. 7104 (about 13%) of
the pairs have positive labels, and the remaining are negative.
In Fig. 13 we show precision and recall with respect to these
labels for TeraHAC, SCC and DBSCAN. We obtained our re-
sults by running TeraHAC with ğ‘¡= 0.05 and ğœ–= 0.1, and SCC
with 5 or 50 rounds of compression decreasing the weight
threshold down to 0.05. We evaluate different points on the
precision-recall curve by considering different levels of clus-
tering for SCC. For TeraHAC, different points are obtained by
flattening the hierarchical clustering using different flattening
thresholds. The flattening algorithm uses batching to compute
all flattenings simultaneously. Our implementation requires
140 minutes to compute all clusterings we present in Fig. 13.
The implementation of DBSCAN that we use is a natural
adaptation of the DBSCAN algorithm (which in its vanilla
version takes a set of points as an input) to a setting where
the input is a similarity graph. The algorithm takes two pa-
rameters: ğœ–and ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ . First, the algorithm considers each
vertex a core vertex if it contains at least ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ incident
edges of weight â‰¥ğœ–. Then, we find connected components
of the subgraph of the input graph consisting of core points
and all edges of weight â‰¥ğœ–between them (using the algo-
rithm described in [36]). These components form core clus-
ters. Next, each non-core vertex which does not have a core
vertex of similarity at least ğœ–forms a singleton cluster. Fi-
nally, for each remaining non-core vertex ğ‘£, we assign ğ‘£to the
cluster of its most similar core neighbor. In Fig. 13 we show
results for (ğœ–,ğ‘šğ‘–ğ‘›ğ‘ƒğ‘¡ğ‘ ) âˆˆ{(0.97, 128), (0.97, 256), (0.98, 64),
(0.98, 96), (0.98, 128), (0.99, 32), (0.99, 64), (0.99, 128)}.
The median running times for the algorithms are given in
Table 3. We find that TeraHAC achieves the highest recall at
every value of precision in the range that we consider. DB-
SCAN, while significantly faster than both TeraHAC and SCC,
consistently obtains over 2x smaller recall than TeraHAC.
TeraHAC improves in quality over both SCC-5 and SCC-50,
in particular delivering about 20% better recall than SCC-5.
At the same time, it runs about 2x faster than SCC-50 and
about 2x slower than SCC-5. This difference in performance
seems to be explained by reduction in the number of edges
and nodes present in the graph for TeraHAC, as compared
with SCC. Fig. 14 shows the reduction in the number of edges
over the rounds of both algorithms. For example, there are 8.6
trillion edges in this graph, and after 10 rounds, SCC-50 still
has 561 billion edges remaining, whereas TeraHAC only has 41
billion edges remaining, which is 13.4x lower than SCC-50. We
observe a similar reduction in the number of nodes (clusters)
that remain. Both runs start with 31 billion nodes; after 10
13

rounds, 5.4 billion nodes remain for SCC-50, whereas only
799 million nodes remain for TeraHAC, which is 6.7x lower
than SCC-50. Our results show that even on extremely large
real-world graphs, TeraHAC using conservative values ofğœ–and
weight threshold achieves excellent scalability relative to state-
of-the-art distributed algorithms, and in fact out-performs
these baselines. Crucially, these performance advantages are
obtained while increasing accuracy at every point along the
precision-recall tradeoff curve.
7
CONCLUSION
In this paper we introduced the TeraHAC algorithm and demon-
strated its high quality and scalability on graphs of up to 8
trillion edges. Our results indicate that TeraHAC may be the
algorithm of choice for clustering large-scale graph datasets.
As a future work, it would be interesting to see whether
we can theoretically bound the number of rounds required
by the TeraHAC algorithm (possibly for a carefully chosen
graph partitioning method). Another open question is whether
TeraHAC could be extended from computing the bottom (high-
similarity) part of the dendrogram to computing the entire
dendrogram. Finally, we believe the notion of (1 + ğœ–)-good
merges may be useful for designing efficient HAC algorithms
in other models, for example in the dynamic setting.
REFERENCES
[1] Amir Abboud, Vincent Cohen-Addad, and Hussein Houdrouge.
Sub-
quadratic high-dimensional hierarchical clustering. In Advances in Neural
Information Processing Systems (NeurIPS), volume 32, 2019.
[2] Tyler Akidau, Slava Chernyak, and Reuven Lax. Streaming systems: the
what, where, when, and how of large-scale data processing. 2018.
[3] Apache Software Foundation. Apache Beam.
[4] Dmitrii Avdiukhin, Sergey Pupyrev, and Grigory Yaroslavtsev. Multi-
dimensional balanced graph partitioning via projected gradient descent.
Proc. VLDB Endow., 12(8):906â€“919, 2019.
[5] Kevin Aydin, MohammadHossein Bateni, and Vahab Mirrokni. Distributed
balanced partitioning via linear embedding. In Proceedings of the Ninth
ACM International Conference on Web Search and Data Mining, WSDM â€™16,
page 387â€“396, New York, NY, USA, 2016.
[6] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering.
Machine learning, 56:89â€“113, 2004.
[7] Mohammadhossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, Mo-
hammadTaghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, and Vahab
Mirrokni. Affinity clustering: Hierarchical clustering at scale. In Advances
in Neural Information Processing Systems (NeurIPS), volume 30, 2017.
[8] J-P BenzÃ©cri. Construction dâ€™une classification ascendante hiÃ©rarchique
par la recherche en chaÃ®ne des voisins rÃ©ciproques. Cahiers de lâ€™analyse
des donnÃ©es, 7(2):209â€“218, 1982.
[9] Charles-Edmond Bichot and Patrick Siarry. Graph partitioning, 2013.
[10] Guy E. Blelloch, Daniel Anderson, and Laxman Dhulipala. Parlaylib - a
toolkit for parallel algorithms on shared-memory multicore machines. In
ACM Symposium on Parallelism in Algorithms and Architectures (SPAA),
pages 507â€”-509, 2020.
[11] Charles Blundell and Yee Whye Teh. Bayesian hierarchical community
discovery. In Advances in Neural Information Processing Systems (NeurIPS),
volume 26, 2013.
[12] Paolo Boldi and Sebastiano Vigna. The WebGraph framework I: Compres-
sion techniques. In International World Wide Web Conference (WWW),
pages 595â€“601, 2004.
[13] AydÄ±n BuluÃ§, Henning Meyerhenke, Ilya Safro, Peter Sanders, and Christian
Schulz. Recent advances in graph partitioning. Springer, 2016.
[14] Deepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. R-MAT:
A recursive model for graph mining. In Proceedings of the 2004 SIAM
International Conference on Data Mining, pages 442â€“446, 2004.
[15] Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams, Robert
Henry, Robert Bradshaw, and Nathan. Flumejava: Easy, efficient data-
parallel pipelines. In ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI), pages 363â€“375, 2 Penn Plaza, Suite 701
New York, NY 10121-0701, 2010.
[16] Flavio Chierichetti, Nilesh Dalvi, and Ravi Kumar. Correlation clustering
in MapReduce. In Proceedings of the 20th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, KDD â€™14, page 641â€“650,
New York, NY, USA, 2014.
[17] Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and
Claire Mathieu. Hierarchical clustering: Objective functions and algo-
rithms. J. ACM, 66(4), 2019.
[18] Vincent Cohen-Addad, Silvio Lattanzi, Slobodan MitroviÄ‡, Ashkan Norouzi-
Fard, Nikos Parotsidis, and Jakub Tarnawski. Correlation clustering in
constant many parallel rounds. In Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pages 2069â€“2078, 2021.
[19] Aron Culotta, Pallika Kanani, Robert Hall, Michael Wick, and Andrew
McCallum. Author disambiguation using error-driven machine learning
with a ranking loss function. In Sixth International Workshop on Information
Integration on the Web (IIWeb-07), Vancouver, Canada, 2007.
[20] Sanjoy Dasgupta. A cost function for similarity-based hierarchical cluster-
ing. In ACM Symposium on Theory of Computing (STOC), page 118â€“127,
New York, NY, USA, 2016.
[21] Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified data process-
ing on large clusters. In OSDIâ€™04: Sixth Symposium on Operating System
Design and Implementation, pages 137â€“150, San Francisco, CA, 2004.
[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understanding.
In Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 4171â€“4186, 2018.
[23] Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral
clustering and normalized cuts. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data mining, pages
551â€“556, 2004.
[24] Laxman Dhulipala, Guy E Blelloch, Yan Gu, and Yihan Sun. Pac-trees:
Supporting parallel and compressed purely-functional collections. In ACM
SIGPLAN Conference on Programming Language Design and Implementation
(PLDI), 2022.
[25] Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab Mirrokni, and Jes-
sica Shi. Hierarchical agglomerative graph clustering in poly-logarithmic
depth.
In 2022 Conference on Neural Information Processing Systems
(NeurIPS).
[26] Laxman Dhulipala, David Eisenstat, Jakub ÅÄ…cki, Vahab Mirrokni, and
Jessica Shi. Hierarchical agglomerative graph clustering in nearly-linear
time. In International Conference on Machine Learning (ICML), pages 2676â€“
2686, 2021.
[27] Alessandro Epasto, AndrÃ©s MuÃ±oz Medina, Steven Avery, Yijian Bai, Robert
Busa-Fekete, CJ Carey, Ya Gao, David Guthrie, Subham Ghosh, James
Ioannidis, et al. Clustering for private interest-based advertising. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining, pages 2802â€“2810, 2021.
[28] Ilan Gronau and Shlomo Moran. Optimal implementations of upgma
and other common clustering algorithms. Information Processing Letters,
104(6):205â€“210, 2007.
[29] Katherine A. Heller and Zoubin Ghahramani. Bayesian hierarchical clus-
tering. In International Conference on Machine Learning (ICML), pages
297â€“304, 2005.
[30] Guan-Jie Hua, Che-Lun Hung, Chun-Yuan Lin, Fu-Che Wu, Yu-Wei Chan,
and Chuan Yi Tang. MGUPGMA: a fast UPGMA algorithm with multi-
ple graphics processing units using NCCL. Evolutionary Bioinformatics,
13:1176934317734220, 2017.
[31] Chen Jin, Ruoqian Liu, Zhengzhang Chen, William Hendrix, Ankit
Agrawal, and Alok Choudhary. A scalable hierarchical clustering algo-
rithm using spark. In 2015 IEEE First International Conference on Big Data
Computing Service and Applications, pages 418â€“426, 2015.
[32] Chen Jin, Md Mostofa Ali Patwary, Ankit Agrawal, William Hendrix,
Wei-keng Liao, and Alok Choudhary. DiSC: A distributed single-linkage
hierarchical clustering algorithm using MapReduce. IEEE Transactions on
Big Data, 23:27, 2013.
[33] Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii. A model of
computation for mapreduce. In Proceedings of the twenty-first annual
ACM-SIAM symposium on Discrete Algorithms, pages 938â€“948, 2010.
[34] Ari Kobren, Nicholas Monath, Akshay Krishnamurthy, and Andrew Mc-
Callum. A hierarchical algorithm for extreme clustering. In Proceedings of
the 23rd ACM SIGKDD international conference on knowledge discovery and
data mining, pages 255â€“264, 2017.
[35] Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. What is
twitter, a social network or a news media? pages 591â€“600, 2010.
[36] Jakub ÅÄ…cki, Vahab Mirrokni, and MichaÅ‚ WÅ‚odarczyk. Connected com-
ponents at scale via local contractions. arXiv preprint arXiv:1807.10727,
2018.
14

[37] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network
dataset collection, 2014.
[38] Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert,
Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for
large-scale graph processing. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data, pages 135â€“146, 2010.
[39] Robert Meusel, Sebastiano Vigna, Oliver Lehmberg, and Christian Bizer.
The graph structure in the webâ€“analyzed on different aggregation levels.
The Journal of Web Science, 1(1):33â€“47, 2015.
[40] Nicholas Monath, Kumar Avinava Dubey, Guru Guruganesh, Manzil Za-
heer, Amr Ahmed, Andrew McCallum, Gokhan Mergen, Marc Najork, Mert
Terzihan, Bryon Tjanaka, et al. Scalable hierarchical agglomerative clus-
tering. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining, pages 1245â€“1255, 2021.
[41] Benjamin Moseley, Kefu Lu, Silvio Lattanzi, and Thomas Lavastida. A
framework for parallelizing hierarchical clustering methods. In Machine
Learning and Knowledge Discovery in Databases: European Conference
(ECML PKDD), 2019.
[42] Benjamin Moseley, Sergei Vassilvtiskii, and Yuyan Wang. Hierarchical
clustering in general metric spaces using approximate nearest neighbors.
In Proceedings of The 24th International Conference on Artificial Intelligence
and Statistics, volume 130 of Proceedings of Machine Learning Research,
pages 2440â€“2448, 2021.
[43] Benjamin Moseley and Joshua R. Wang. Approximation bounds for hier-
archical clustering: Average linkage, bisecting k-means, and local search.
In Advances in Neural Information Processing Systems (NeurIPS), pages
3094â€“3103, 2017.
[44] Daniel MÃ¼llner. Modern hierarchical, agglomerative clustering algorithms.
arXiv preprint arXiv:1109.2378, 2011.
[45] Daniel MÃ¼llner. fastcluster: Fast hierarchical, agglomerative clustering
routines for r and python. Journal of Statistical Software, 53:1â€“18, 2013.
[46] Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clus-
tering: an overview. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 2(1):86â€“97, 2012.
[47] Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clus-
tering: an overview, II. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 7(6):e1219, 2017.
[48] Mark EJ Newman. Modularity and community structure in networks.
Proceedings of the national academy of sciences, 103(23):8577â€“8582, 2006.
[49] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analy-
sis and an algorithm. In Advances in Neural Information Processing Systems
(NeurIPS), volume 14, 2001.
[50] Leon Poutievski, Omid Mashayekhi, Joon Ong, Arjun Singh, Mukarram
Tariq, Rui Wang, Jianan Zhang, Virginia Beauregard, Patrick Conner, Steve
Gribble, et al. Jupiter evolving: Transforming googleâ€™s datacenter net-
work via optical circuit switches and software-defined networking. In
Proceedings of the ACM SIGCOMM 2022 Conference, pages 66â€“85, 2022.
[51] Sherif Sakr, Faisal Moeen Orakzai, Ibrahim Abdelaziz, and Zuhair Khayyat.
Large-scale graph processing using Apache Giraph. 2016.
[52] Erich Schubert, JÃ¶rg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei
Xu. Dbscan revisited, revisited: why and how you should (still) use dbscan.
ACM Transactions on Database Systems (TODS), 42(3):1â€“21, 2017.
[53] Sklearn Authors. scipy.cluster.DBSCAN. https://scikit-learn.org/stable/
modules/generated/sklearn.cluster.DBSCAN.html.
[54] Sklearn
Authors.
sklearn.cluster.AgglomerativeClustering.
https://scikit-learn.org/stable/modules/generated/sklearn.cluster.
AgglomerativeClustering.html.
[55] T Stefan Van Dongen and Birgitta Winnepenninckx. Multiple upgma and
neighbor-joining trees and the performance of some computer packages.
Mol. Biol. Evol, 13(2):309â€“313, 1996.
[56] Baris Sumengen, Anand Rajagopalan, Gui Citovsky, David Simcha, Olivier
Bachem, Pradipta Mitra, Sam Blasiak, Mason Liang, and Sanjiv Kumar. Scal-
ing hierarchical agglomerative clustering to billion-sized datasets. arXiv
preprint arXiv:2105.11653, 2021.
[57] Muhammad Tirmazi, Adam Barker, Nan Deng, Md Ehtesam Haque, Zhi-
jing Gene Qin, Steven Hand, Mor Harchol-Balter, and John Wilkes. Borg:
the next generation. In EuroSysâ€™20, Heraklion, Crete, 2020.
[58] Axel Wassington and Sergi Abadal. Bias reduction via cooperative bargain-
ing in synthetic graph dataset generation. arXiv preprint arXiv:2205.13901,
2022.
[59] Grigory Yaroslavtsev and Adithya Vadapalli. Massively parallel algorithms
and hardness for single-linkage clustering under â„“p distances. In Interna-
tional Conference on Machine Learning (ICML), volume 80 of Proceedings of
Machine Learning Research, pages 5596â€“5605, 2018.
[60] Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, and Julian Shun.
Parchain: A framework for parallel hierarchical agglomerative clustering
using nearest-neighbor chain. Proc. VLDB Endow., 15(2), 2021.
[61] Ying Zhao and George Karypis. Evaluation of hierarchical clustering algo-
rithms for document datasets. In Proceedings of the eleventh international
conference on Information and knowledge management, pages 515â€“524,
2002.
15

A
MISSING PROOFS
Lemma 1. Let ğº1, . . . ,ğºğ‘›be a sequence of graphs, in which
each graph is obtained from the previous one by performing
an arbitrary merge. Let ğ‘£be a vertex (cluster) which exists in
ğºğ‘™, . . . ,ğºğ‘Ÿ. Let ğ‘¤ğ‘–max(ğ‘£) be the value of ğ‘¤max(ğ‘£) in ğºğ‘–, where
ğ‘™â‰¤ğ‘–â‰¤ğ‘Ÿ. Then ğ‘¤ğ‘™max(ğ‘£) â‰¥ğ‘¤ğ‘™+1
max(ğ‘£) â‰¥. . . â‰¥ğ‘¤ğ‘Ÿmax(ğ‘£).
Proof. We prove the lemma by contradiction. Pick the
smallest ğ‘—â‰¥ğ‘™such that ğ‘¤ğ‘—
max(ğ‘£) < ğ‘¤ğ‘—+1
max(ğ‘£). Then, ğºğ‘—+1 is
obtained from ğºğ‘—by merging some vertices ğ‘¥and ğ‘¦, where
ğ‘¤(ğ‘£,ğ‘¥âˆªğ‘¦) = ğ‘¤ğ‘—+1
max(ğ‘£). By Definition 1 we have ğ‘¤ğ‘—+1
max(ğ‘£) =
ğ‘¤(ğ‘£,ğ‘¥âˆªğ‘¦) â‰¤max(ğ‘¤(ğ‘£ğ‘¥),ğ‘¤(ğ‘£ğ‘¦)) â‰¤ğ‘¤ğ‘—
max(ğ‘£), a contradiction.
â–¡
Observation 1. Let ğºğ‘–be obtained from ğº1 by performing
1-good merges. Then, a merge ğ‘¢ğ‘£in ğºğ‘–is 1-good if and only if
ğ‘¤(ğ‘¢ğ‘£) = max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£)).
Proof. ( =â‡’) By the definition of ğ‘¤max we have ğ‘¤(ğ‘¢ğ‘£) â‰¤
max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£)). Moreover, ğ‘¤(ğ‘¢ğ‘£) â‰¥max(ğ‘¤max(ğ‘¢),
ğ‘¤max(ğ‘£)) since, by the definition of 1-good merge, we have
max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£)) = min(M(ğ‘¢), M(ğ‘£),ğ‘¤(ğ‘¢ğ‘£)).
( â‡= ) Since ğ‘¤(ğ‘¢ğ‘£) â‰¤ğ‘¤max(ğ‘¢) and ğ‘¤(ğ‘¢ğ‘£) â‰¤ğ‘¤max(ğ‘£),
ğ‘¤(ğ‘¢ğ‘£) = max(ğ‘¤max(ğ‘¢),ğ‘¤max(ğ‘£)) impliesğ‘¤(ğ‘¢ğ‘£) = ğ‘¤max(ğ‘¢) =
ğ‘¤max(ğ‘£). By Lemma 2, ğ‘€(ğ‘£) â‰¥ğ‘¤max(ğ‘£) and ğ‘€(ğ‘£) â‰¥ğ‘¤max(ğ‘£).
This implies that the merge is 1-good.
â–¡
Claim 1. We have ğ‘¤(ğ‘¥ğ‘¦)/min(M(ğ‘¥â€²), M(ğ‘¥),ğ‘¤(ğ‘¥ğ‘¥â€²)) =
ğ‘¤(ğ‘¥ğ‘¦)/M(ğ‘¥â€² âˆªğ‘¥) â‰¤1 + ğœ–.
Proof. Consider the sequence of merges that were made to
obtain the dendrogram ğ·and letğºâ€²
1, . . . ,ğºâ€²ğ‘›be the correspond-
ing graphs. Assume that the merge of ğ‘¥â€² and ğ‘¥was done in
ğºâ€²
ğ‘–(and resulted in obtaining graph ğºâ€²
ğ‘–+1). Let ğ‘¤â€²max(ğ‘£) be the
value of ğ‘¤max(ğ‘£) in graph ğºâ€²
ğ‘–. Graph ğºâ€²
ğ‘–does not necessarily
have a vertex ğ‘¦, but since we assumed that the merge of ğ‘¥â€²
and ğ‘¥happens before the merge of ğ‘¦â€² and ğ‘¦, ğºâ€²
ğ‘–contains a set
of vertices {ğ‘¦1, . . . ,ğ‘¦ğ‘¡}, such that Ãğ‘¡
ğ‘–=1 ğ‘¦ğ‘–= ğ‘¦. By Definition 1
we have that ğ‘¤(ğ‘¥ğ‘¦) = ğ‘¤(ğ‘¥, Ãğ‘¡
ğ‘–=1 ğ‘¦ğ‘–) â‰¤maxğ‘¡
ğ‘–=1 ğ‘¤(ğ‘¥,ğ‘¦ğ‘–) â‰¤
ğ‘¤â€²max(ğ‘¥), which implies:
ğ‘¤(ğ‘¥ğ‘¦)
min(M(ğ‘¥), M(ğ‘¥â€²),ğ‘¤(ğ‘¥ğ‘¥â€²)) â‰¤max(ğ‘¤â€²max(ğ‘¥),ğ‘¤â€²max(ğ‘¥â€²))
min(M(ğ‘¥), M(ğ‘¥â€²),ğ‘¤(ğ‘¥ğ‘¥â€²)) â‰¤1+ğœ–.
Here, the second inequality follows from the fact that the
merge of ğ‘¥and ğ‘¥â€² in ğºâ€²
ğ‘–is good.
â–¡
Lemma 5. For some ğ¶âŠ†ğ‘‰, consider the graph ğºğ¶and let
ğ‘š1, . . . ,ğ‘šğ‘˜be a sequence of merges in ğºğ¶, such that (a) the
merges are (1 + ğœ–)-good with respect to ğºğ¶and (b) the merges
only involve merging active vertices of ğºğ¶. Then, the merges
ğ‘š1, . . . ,ğ‘šğ‘˜are also (1 + ğœ–)-good with respect to ğºğ‘–.
Proof. We observe that whether a merge is good only de-
pends on the vertices involved in a merge, their M values and
their incident edges. Hence, the lemma easily follows when
ğ‘˜= 1. Let us now consider the case of ğ‘˜= 2, the argument can
be easily continued inductively. Let ğ¶1 be the set of vertices
obtained from ğ¶by performing merge ğ‘š1, and let ğº1 be the
graph obtained from ğºby performing merge ğ‘š1. It is easy to
see that by performing merge ğ‘š1 on ğºğ¶we obtain the graph
ğºğ¶1
1 . That is, the graph that SubgraphHAC sees after applying
the first merge, is the same graph that we would obtain if we
applied the merge on the global graph, and then computed the
input to SubgraphHAC. The lemma follows.
â–¡
Lemma 9. Let ğº= (ğ‘‰, ğ¸,ğ‘¤) be a graph and ğœ–â‰¥0. Consider a
run of Algorithm 1 with a threshold parameter ğ‘¡â€² followed by
flattening the resulting dendrogram using a threshold ğ‘¡. Then,
the output of the algorithm is the same for any ğ‘¡â€² âˆˆ[0,ğ‘¡].
Proof. By Lemma 8, flattening the dendrogram with thresh-
old ğ‘¡only uses nodes of the dendrogram whose linkage simi-
larity is at least ğ‘¡/(1+ğœ–). Pruning removes vertices whose max-
imum incident edge has weight strictly less than ğ‘¡â€²/(1 + ğœ–) â‰¤
ğ‘¡/(1 + ğœ–). By Definition 1, each merge that such vertex partici-
pates in has merge of similarity below ğ‘¡/(1 +ğœ–). Moreover, the
removal of such vertices does not affect whether a merge of
similarity at least ğ‘¡/(1 + ğœ–) is good, or does not affect the edge
weights participating in such merges. The lemma follows.
â–¡
B
SubgraphHAC
In this section, we give a near-linear time algorithm for com-
puting a set of (1+ğœ–)-good merges by dynamically maintaining
the goodness values of the edges, which we call SubgraphHAC.
The algorithm has the following specification: it performs only
(1 + ğœ–)-good merges, and ensures that upon completion, there
are no (1 + ğœ–â€²)-good merges for ğœ–â€² = ğ‘‚(ğœ–) (see Lemma 13).
Recall that SubgraphHAC is run on the partitions (i.e., sub-
graphs) of the input graph, where vertices assigned to this
partition are called active and the neighbors of these vertices
that are not in this partition are inactive. Furthermore, the
vertices carry their min-merge values from prior rounds, M(ğ‘£).
In what follows, we use vertex and cluster interchangeably.
The input to SubgraphHAC is therefore a graph where ver-
tices are marked active and inactive, where each vertex ğ‘£has
a min-merge value M(ğ‘£), as well as a corresponding cluster
size. The goal of the algorithm is to merge a subset of the good
edges, and run in near-linear time.
Challenges. Designing a near-linear time algorithm for this
problem is surprisingly non-trivial. One challenge is that the
goodness of a vertexğ‘£â€™s incident edges can change significantly
even without ğ‘£participating in merges itself. For example, this
could occur is if a neighbor of ğ‘£merges with another vertex,
resulting in the neighboring vertex increasing in size, causing
ğ‘¤ğ‘¢ğ‘£to decrease. A simple example, e.g., a sequence of ğ‘›âˆ’1
merges of the degree-1 vertices of a star into the center of
the star illustrates that exact maintenance of goodness values
requires ğ‘‚(ğ‘›2) work. Another challenge is that the goodness
of all edges incident to a vertex ğ‘£depends on the ğ‘¤max(ğ‘¢)
values for all neighbors of ğ‘£, which in turn depends on the
cluster sizes of their neighbors. Hence, the goodness of an
edge incident to ğ‘£may change when a vertex / cluster two
hops away from ğ‘£chages its size.
These examples serve to illustrate that attempting to exactly
maintain the goodness values is probably hopeless. One could
ask why bother with goodnessâ€”perhaps the algorithm can
simply keep a heap induced only on the active edges and their
weights? The prior near-linear time (1 + ğœ–)-approximate HAC
16

algorithm [26] used such an approach (this was called the
heap-based approach in the paper). However, the heap-based
approach alone as used in SeqHAC is insufficient for our needs,
since SeqHAC cannot merge any edge that has weight smaller
than a factor of (1 + ğœ–) from the current maximum weight in
the graph. Still, our algorithm borrows and builds on some
ideas from the heap-based approach. For example, it indexes a
heap over the vertices, using the lowest (i.e., best) goodness
as the priority for each vertex. It also maintains weights using
partial weights, where an edge to a neighbor ğ‘¥of a vertex ğ‘£is
stored as a partial average-linkage weight, ğ‘¤(ğ‘£,ğ‘¥) Â· |ğ‘£|, i.e. the
average-linkage weight between vertices (ğ‘£,ğ‘¥) but multiplied
by ğ‘£, the source endpointâ€™s cluster size. However, many new
ideas are required to efficiently maintain the goodness values
and argue that the algorithm ensures a (1 + ğœ–)-approximation.
Our Approach. We design an algorithm based on a lazy heap-
based approach, which we give some high-level pseudocode
for in Algorithm 4. The algorithm maintains a priority queue
(min heap) over the active vertices, with a vertexâ€™s priority cor-
responding to the goodness of its smallest goodness neighbor.
The algorithm cannot maintain these goodness values exactly,
but instead uses approximate goodness values, Ëœğ‘”. There are
two sources of approximation that play a role. First, similar
to the SeqHAC algorithm, the algorithm maintains (1 + ğ›¼) ap-
proximations of the edge weights, Ëœğ‘¤(ğ‘¢, ğ‘£), by â€œbroadcastingâ€ a
vertexâ€™s cluster size to its neighbors and updating the weights
based on the next cluster size. Here, ğ›¼â‰¥0 is a parameter of
SubgraphHAC. This broadcast operation is done only when
a vertexâ€™s cluster size increases by a (1 + ğ›¼) factor. Second,
it maintains an approximate view of the ğ‘¤max(ğ‘£) values for
each vertex ğ‘£, Ëœğ‘¤max(ğ‘£). We assign every active edge (an edge
between two active endpoints) to the endpoint with larger
Ëœğ‘¤max value; call this the assigned endpoint for an edge. Each
time the Ëœğ‘¤max value of a vertex changes by a (1 + ğ›¼) factor,
the algorithm goes over all assigned edges and reassigns them
to whichever endpoint has higher Ëœğ‘¤max value. This approach
yields good guarantees on (1) the approximation quality of
each edge we merge and (2) termination conditions for the
algorithm (i.e., when it terminates, no edges with goodness
below a given threshold remain). The algorithm itself (Algo-
rithm 4) is relatively simple, with most of the work being done
by the graph representation, described next.
Graph Representation
The graph representation takes as input parameters ğœ–â‰¥0, ğ›¼â‰¥
0; ğœ–controls the accuracy and ensures that every merge is (1 +
ğœ–)-good; ğ›¼controls the frequency with which the algorithm
performs broadcasts, and affects the range of goodness for
which edges are guaranteed to be merged.
The graph representation (1) support queries for the approx-
imate goodness value Ëœğ‘”(ğ‘¢, ğ‘£) of each edge, (2) support queries
for the Ëœğ‘¤max(ğ‘£) value for each vertex, (3) support efficiently
merging pairs of active vertex, and (4) support querying for
(approximately) the lowest goodness edge incident to a vertex
ğ‘£. It supports (1â€“4) above with an overall running time of all
operations over any sequence of merges in Ëœğ‘‚(ğ‘š+ğ‘›) time. We
Algorithm 4 SubgraphHAC(ğº= (ğ‘‰, ğ¸,ğ‘¤, M),ğ´,ğœ–, ğ›¼)
Input: ğºğ´= (ğ‘‰, ğ¸,ğ‘¤, M), ğ´âŠ†ğ‘‰, ğœ–> 0.
Output: A set of merges of vertices in ğ´
1: Mark vertices of ğ‘‰\ ğ´as inactive
2: For each active edge (ğ‘¢, ğ‘£) assign it to the endpoint with
higher ğ‘¤max value.
3: ğ‘‡:= priority queue on active vertices of ğº[ğ´] keyed by
the goodness of their lowest-goodness assigned edge.
4: while ğ‘‡is not yet empty do
5:
(ğ‘¢, (ğ‘£,ğ‘ğ‘¢ğ‘£)) := RemoveMin(ğ‘‡)
6:
if ğ‘ğ‘¢ğ‘£> (1 + ğœ–) then
7:
(ğ‘¢, ğ‘£â€²,ğ‘ğ‘¢ğ‘£â€²) := min goodness assigned edge of ğ‘¢
8:
Reinsert (ğ‘¢, (ğ‘£â€²,ğ‘ğ‘¢ğ‘£â€²)) to ğ‘‡if ğ‘ğ‘¢ğ‘£â€² â‰¤(1 + ğœ–).
9:
else
10:
Merge ğ‘¢and ğ‘£in ğº
11: Return all merges made
can support (4) only for edges with true goodness value in a
given range [1,ğ‘‡], and elaborate more on this in Lemma 13.
Data Structures and Initialization. Each active edge (ğ‘¢, ğ‘£)
(an edge where both ğ‘¢and ğ‘£are active) is initially assigned to
whichever of its endpoints has larger Ëœğ‘¤max value. Let ğ´(ğ‘¢, ğ‘£)
be the endpoint that the edge (ğ‘¢, ğ‘£) is assigned to. Initially,
the Ëœğ‘¤max values are exact, and can be computed by scanning
all the edges. For each active ğ‘£âˆˆğ‘‰, we store all edges as-
signed to the vertex in a priority queue ğ´(ğ‘£), which is keyed
by the approximate goodness of the edge (from smallest to
largest). Initially, the goodness value for an assigned edge
(ğ‘¢, ğ‘£) is Ëœğ‘”(ğ‘¢, ğ‘£). We maintain for each active vertex ğ‘£its full
neighborhood, including both active and inactive neighbors.
ğ‘(ğ‘£) stores the weights of all of its incident edges using the
one-sided partial weight representation: for each (ğ‘¢, ğ‘£) edge
incident toğ‘¢, we storeğ‘¤(ğ‘¢, ğ‘£)Â·|ğ‘¢|, i.e., the true average-linkage
weight, ğ‘¤(ğ‘¢, ğ‘£) multiplied by the cluster size of ğ‘¢. The neigh-
borhood is stored as a sparse set, e.g., using a hashtable. Each
active vertex ğ‘¢stores two quantities: (a) the initial cluster size
of the vertex and (b) the approximate ğ‘¤max value of the vertex.
The stored values are updated whenever the cluster size of
the vertex increases sufficiently, or whenever the ğ‘¤max value
changes sufficiently, as we discuss shortly. Lastly, we maintain
a list of edges ğ‘…to be reassigned which is cleared at the end of
each merge.
Invariants. The invariants are motivated by the following
two broadcast operations, which update information along all
neighboring edges whenever a vertexâ€™s cluster size or Ëœğ‘¤max
value change by a (1+ğ›¼) factor. The first type of broadcast up-
dates a nodeâ€™s cluster size in all of its neighborâ€™s data structures
after its cluster size increases by a (1 + ğ›¼)-factor. The second
type of broadcast is based on the Ëœğ‘¤max values: if the Ëœğ‘¤max(ğ‘¢)
drops by a (1 + ğ›¼) factor, we reassign all edges assigned to ğ‘¢.
We first state a useful invariant about the approximate
weights, Ëœğ‘¤. Before any merges are performed, the weights
are exact. There may be three things affecting ğ‘¤(ğ‘¢, ğ‘£): the
cut-size changing, or the cluster size of either ğ‘¢or ğ‘£changing.
Changes of the first type are easy to handle, and can update
the weight of the edge to be exact. Changing the cluster size
17

of the endpoint to which the edge is assigned to (say ğ‘¢) is
also easy, since the weight is implicitly normalized by |ğ‘¢|. So
the only error comes from changes to the neighbor cluster
size, ğ‘£, but since we broadcast and make the incident edge
weights exact after a cluster grows in size by a (1 + ğ›¼) factor,
the edge weight can be at most a (1 + ğ›¼) factor larger than the
true weight. This guarantee for the approximate weights, Ëœğ‘¤,
provided by the broadcasting procedure is summarized by the
next invariant:
Invariant 1. For every edge (ğ‘¢, ğ‘£) âˆˆğ¸,ğ‘¤(ğ‘¢, ğ‘£) â‰¤Ëœğ‘¤(ğ‘¢, ğ‘£) â‰¤
ğ‘¤(ğ‘¢, ğ‘£)(1 + ğ›¼).
In other words, the approximate weights maintained for each
edge are an upper-bound on the true edge weight. Note that
we can compute the exact weight of any edge in ğ‘‚(1) at any
given time since the cut weight, and cluster sizes are exact;
we need the approximate weights and Invariant 1 only when
computing ğ‘¤max(ğ‘¢), since we cannot afford to scan over all
incident edges to ğ‘¢to compute ğ‘¤max exactly.
Let Â¥ğ‘”(ğ‘¢, ğ‘£) be the stored goodness of the edge (ğ‘¢, ğ‘£), i.e. the
stored goodness value in the priority queue ğ´(ğ‘¢) if (ğ‘¢, ğ‘£) is
assigned to ğ‘¢. At initialization, Â¥ğ‘”= Ëœğ‘”= ğ‘”. However, as merges
start to occur in the graph, the stored goodness values, Â¥ğ‘”may
become approximate. Understanding the stored goodness val-
ues is necessary, since when scanning the assigned edges of a
vertex to find a potentially mergeable edge, we use the stored
goodness values, Â¥ğ‘”. The next invariant summarizes the rela-
tionship between the Â¥ğ‘”values and the exact goodness at any
point in time in the algorithm (we provide the proof that the
invariant is maintained in Lemma 12).
Invariant 2. For every edge (ğ‘¢, ğ‘£) âˆˆğ¸, Â¥ğ‘”(ğ‘¢, ğ‘£) â‰¤ğ‘”(ğ‘¢, ğ‘£)(1+ğ›¼)2
So at any point in time during the algorithm, if the goodness
value of an edge is small, then the stored goodness value of the
edge is also small. Again, we emphasize the difference between
the three types of goodness values:ğ‘”is the true goodness value
of an edge; Ëœğ‘”is a (1+ğ›¼) over-approximation of the ğ‘”value that
we can compute at any instant; finally, Â¥ğ‘”is the goodness value
that is stored in the priority queue of the assigned endpoint.
We further note that the invariant says nothing about a lower
bound on Â¥ğ‘”(ğ‘¢, ğ‘£) in terms of ğ‘”(ğ‘¢, ğ‘£). For example, it could be
the case that ğ‘”(ğ‘¢, ğ‘£) â‰«(1 + ğœ–) while Â¥ğ‘”(ğ‘¢, ğ‘£) â‰¤(1 + ğœ–), but this
is not an issue, as we can amortize the cost of checking the
edge against the increase in goodness for the edge.
Initially, Invariant 1 is satisfied since the weights are ex-
act. Lastly, Invariant 2 is satisfied initially since the goodness
values are exact.
Useful Lemmas. Next, we state some useful lemmas about
the approximation error of computing Ëœğ‘¤max and Ëœğ‘”at a given
point in time, based on the invariants above.
Lemma 10. âˆ€ğ‘¢âˆˆğ‘‰, ğ‘¤max(ğ‘¢) â‰¤Ëœğ‘¤max(ğ‘¢) â‰¤ğ‘¤max(ğ‘¢)(1 + ğ›¼).
In other words, the approximate Ëœğ‘¤max values s a (1 + ğ›¼) over-
approximation of the true ğ‘¤max values.
Proof. Since we store a map indexed on the approximate
partial weights of the neighbors and sorted by the approxi-
mate edge weights Ëœğ‘¤, we can simply use the largest Ëœğ‘¤of an
incident edge to be Ëœğ‘¤max(ğ‘¢). The returned weight is a (1 + ğ›¼)
approximation to ğ‘¤max by Invariant 1.
â–¡
Using Lemma 10, we obtain guarantees on calculating a
particular edgeâ€™s goodness value. For the calculation, we use
the exact value of the edge weight, but suffer an approximation
in the numerator; when computing max( Ëœğ‘¤max(ğ‘¢), Ëœğ‘¤max(ğ‘£)),
the value can be at most a multiplicative (1 + ğ›¼) factor larger
than the true value. This is summarized in the next lemma:
Lemma 11. âˆ€(ğ‘¢, ğ‘£) âˆˆğ¸, ğ‘”(ğ‘¢, ğ‘£) â‰¤Ëœğ‘”(ğ‘¢, ğ‘£) â‰¤ğ‘”(ğ‘¢, ğ‘£)(1 + ğ›¼).
Proof. The approximate goodness of an edge isğ‘ˆ/ğ¿where
ğ‘ˆ= max( Ëœğ‘¤max(ğ‘¢), Ëœğ‘¤max(ğ‘£)) and ğ¿= min(M(ğ‘¢), M(ğ‘£),ğ‘¤(ğ‘¢, ğ‘£)).
Since ğ‘ˆis a (1 + ğ›¼) approximation of the true numerator and
cannot be smaller, and ğ¿is exact, the worst case is when ğ‘ˆis a
(1 + ğ›¼) approximation. Therefore, we have ğ‘”(ğ‘¢, ğ‘£) â‰¤Ëœğ‘”(ğ‘¢, ğ‘£) â‰¤
(1 + ğ›¼)ğ‘”(ğ‘¢, ğ‘£).
â–¡
We describe how Ëœğ‘¤and Ëœğ‘”are calculated, and why the lem-
mas are true in more detail below. Lemma 11 is crucial since
by only merging edges where Ëœğ‘”(ğ‘¢, ğ‘£) â‰¤(1 + ğœ–), the edge must
truly be (1 + ğœ–)-good.
Graph Representation Operations
Merge(ğ‘¢, ğ‘£). We merge the vertex with smaller cluster size
into the vertex with larger cluster size. Suppose we are merg-
ing ğ‘¢into ğ‘£(thus ğ‘£will still be active after the merge). Both
ğ‘¢and ğ‘£are initially active. Merging the weights can be done
in ğ‘‚(|ğ‘(ğ‘¢)|) time by iterating over edges in ğ‘(ğ‘¢) and updat-
ing weights in ğ‘(ğ‘£) using the average-linkage formula. After
merging the neighborhoods, we push all edges in ğ´(ğ‘¢) (edges
assigned to ğ‘¢) to ğ‘…to be fixed at the end of the merge. Finally,
we see if we need to perform a broadcast operation due to ei-
ther the cluster size increasing sufficiently, or the ğ‘¤max value
decreasing sufficiently:
(1) If ğ‘£â€™s cluster size grows larger by a (1 + ğ›¼) factor (from
the last broadcast value), we iterate over all edges in
ğ‘(ğ‘£) and update their weights in both ğ‘£and ğ‘£â€™s neigh-
borâ€™s queues.
(2) If Ëœğ‘¤max(ğ‘£) either grows or decreases by a (1 +ğ›¼) factor
from the last value of Ëœğ‘¤max(ğ‘£) that a broadcast occurred
at, we iterate over all of ğ‘£â€™s edges and reassign them to
the endpoint with larger Ëœğ‘¤max value.
This concludes the description of the merge procedure. We de-
scribe the correctness details (i.e., showing that the invariants
are preserved after this operation) shortly.
BestAssignedNeighbor(ğ‘¢). This operation is used to com-
pute the min goodness assigned neighbor of ğ‘¢in Algorithm 4.
Similar to ğ‘¤max, getting the true best assigned neighbor would
require scanning all edges assigned to ğ‘¢. Instead, we iterate
over every edge in ğ´(ğ‘¢) in increasing order of the stored good-
ness value, Â¥ğ‘”(ğ‘¢, ğ‘£) until we either find an edge whose approxi-
mate goodness Ëœğ‘”(ğ‘¢, ğ‘£) is smaller than (1 + ğœ–), or we exhaust
all edges with Â¥ğ‘”(ğ‘¢, ğ‘£) in the range [1, (1 + ğœ–)/(1 + ğ›¼)].
Each edge unsuccessfully considered in this range has Â¥ğ‘”(ğ‘¢, ğ‘£) â‰¤
(1+ğœ–)/(1+ğ›¼), but Ëœğ‘”(ğ‘¢, ğ‘£) > 1+ğœ–. When analyzing the running
time, this gap helps show that an edge can be unsuccessfully
checked only ğ‘‚(logğ‘›) times.
18

Correctness
Lemma 12. The Merge(ğ‘¢, ğ‘£) operation preserves Invariants 1
and 2.
Proof. Suppose that the invariants are true before the
merge operation, and suppose ğ‘¢merges into ğ‘£(i.e. ğ‘£remains
active at the end of the merge).
Invariant 1. The only edges whose weights are affected by
the merge are those incident to ğ‘£. Let (ğ‘£,ğ‘¥) be such an edge. If
(ğ‘¢,ğ‘¥) was in ğºpreviously, then the weight of the (ğ‘£,ğ‘¥) edge
will be exact. Otherwise, Invariant 1 previously held for (ğ‘£,ğ‘¥).
It is clear that Invariant 1 continues to hold if the cluster size
of ğ‘£has not yet grown by a factor of (1+ğ›¼); on the other hand
if it grows by more than this factor, ğ‘£broadcasts, and the (ğ‘£,ğ‘¥)
weight will be exact.
Invariant 2. Consider an arbitrary (ğ‘¥,ğ‘¦) edge. When the
edge was last assigned, say to ğ‘¥, Â¥ğ‘”(ğ‘¥,ğ‘¦) = Ëœğ‘”(ğ‘¥,ğ‘¦) â‰¤(1 +
ğ›¼)ğ‘”(ğ‘¥,ğ‘¦). The invariant can be violated if ğ‘”(ğ‘¥,ğ‘¦) decreases,
which can only occur by max(ğ‘¤max(ğ‘¥),ğ‘¤max(ğ‘¦)) decreas-
ing. The algorithm will update the goodness of (ğ‘¥,ğ‘¦) once
max( Ëœğ‘¤max(ğ‘¥), Ëœğ‘¤max(ğ‘¦)) decreases by a (1 + ğ›¼) factor. Since
max( Ëœğ‘¤max(ğ‘¥), Ëœğ‘¤max(ğ‘¦)) â‰¤(1 + ğ›¼) max(ğ‘¤max(ğ‘¥),ğ‘¤max(ğ‘¦)),
after max(ğ‘¤max(ğ‘¥),ğ‘¤max(ğ‘¦)) decreases by a (1 + ğ›¼) factor,
max( Ëœğ‘¤max(ğ‘¥), Ëœğ‘¤max(ğ‘¦)) must also decrease by a (1 + ğ›¼) fac-
tor. At this point, we perform a broadcast and update the
goodness of this edge to Ëœğ‘”(ğ‘¥,ğ‘¦). The ratio of Â¥ğ‘”(ğ‘¥,ğ‘¦)/ğ‘”(ğ‘¥,ğ‘¦)
is always upper bounded by (1 + ğ›¼)2, and is largest when
initially ğ‘”(ğ‘¥,ğ‘¦) = Â¥ğ‘”(ğ‘¥,ğ‘¦)/(1 + ğ›¼) and ğ‘”(ğ‘¢, ğ‘£) decreases by a
(1 + ğ›¼) factor.
â–¡
Lemma 13. Once the algorithm terminates, no edges with
goodness (ğ‘”) in the range [1,ğ‘‡] with ğ‘‡= (1 + ğœ–)/(1 + ğ›¼)3
remain.
Proof. Suppose otherwise. Let this edge be (ğ‘¢, ğ‘£) and its
goodness value be ğ‘”ğ‘¢ğ‘£. Suppose (ğ‘¢, ğ‘£) is assigned to ğ‘¢. By
Invariant 2, Â¥ğ‘”(ğ‘¢, ğ‘£) â‰¤ğ‘”(ğ‘¢, ğ‘£)(1 + ğ›¼)2. Since ğ‘”ğ‘¢ğ‘£â‰¤ğ‘‡= (1 +
ğœ–)/(1+ğ›¼)3, by Invariant 2, Â¥ğ‘”(ğ‘¢, ğ‘£) â‰¤ğ‘”ğ‘¢ğ‘£(1+ğ›¼) â‰¤(1+ğœ–)/(1+ğ›¼).
However, this means that the algorithm incorrectly missed an
edge in the range [0, (1 +ğœ–)/(1 +ğ›¼)] for node ğ‘¢, contradicting
the definition of BestAssignedNeighbor(ğ‘¢).
â–¡
Running Time Analysis
We will show that ğ‘‚(logğ‘›) total broadcasts and reassignment
operations are performed for any vertex over the course of
the algorithm.
Let ğ‘1, . . . ,ğ‘ğ‘˜be the sizes of the cluster containing a vertex
ğ‘. We have ğ‘1 = 1 and ğ‘ğ‘˜â‰¤ğ‘›. When a cluster of size ğ‘
merges with a cluster of size ğ‘(ğ‘â‰¥ğ‘), the Ëœğ‘¤max(ğ‘) value can
increase from ğ‘¤to at most ğ‘¤((1 + ğœ–)ğ‘+ ğ‘)/(ğ‘+ğ‘), which is a
multiplicative increase of ((1 + ğœ–)ğ‘+ ğ‘)/(ğ‘+ ğ‘). The increase
is maximized when ğ‘= ğ‘, for an increase of at most (2 + ğœ–)/2.
We would like to show that the overall increase in Ëœğ‘¤max over
any sequence of merges cannot be too large. We will use the
following lemma for the analysis:
Lemma 14. Let ğ‘1 < ğ‘2 < . . . < ğ‘ğ‘˜. Then, Ãğ‘˜âˆ’1
ğ‘–=1 (1 +
ğ‘¡(ğ‘ğ‘–+1âˆ’ğ‘ğ‘–)
ğ‘ğ‘–+1
) = ğ‘‚(ğ‘ğ‘¡
ğ‘˜).
Proof. Since 1 + ğ‘¥â‰¤ğ‘’ğ‘¥, for any ğ‘¥1, . . . ,ğ‘¥ğ‘˜we have
ğ‘˜
Ã–
ğ‘–=1
(1 + ğ‘¥ğ‘–) â‰¤ğ‘’
Ãğ‘˜
ğ‘–=1 ğ‘¥ğ‘–.
Thus, to complete the proof it suffices to show that
ğ‘˜âˆ’1
âˆ‘ï¸
ğ‘–=1
ğ‘¡(ğ‘ğ‘–+1 âˆ’ğ‘ğ‘–)
ğ‘ğ‘–+1
â‰¤ğ‘¡lnğ‘ğ‘˜+ ğ‘‚(1),
(1)
and apply Equation B. Observe that
ğ‘ğ‘–+1 âˆ’ğ‘ğ‘–
ğ‘ğ‘–+1
=
ğ‘ğ‘–+1âˆ’ğ‘ğ‘–âˆ’1
âˆ‘ï¸
ğ‘—=0
1
ğ‘ğ‘–+1
â‰¤
ğ‘ğ‘–+1âˆ’ğ‘ğ‘–âˆ’1
âˆ‘ï¸
ğ‘—=0
1
ğ‘ğ‘–+1 âˆ’ğ‘—=
ğ‘ğ‘–+1
âˆ‘ï¸
ğ‘—=ğ‘ğ‘–+1
1
ğ‘—,
which implies
ğ‘˜âˆ’1
âˆ‘ï¸
ğ‘–=1
ğ‘¡(ğ‘ğ‘–+1 âˆ’ğ‘ğ‘–)
ğ‘ğ‘–+1
â‰¤ğ‘¡
ğ‘˜âˆ’1
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘–+1
âˆ‘ï¸
ğ‘—=ğ‘ğ‘–+1
1
ğ‘—= ğ‘¡
ğ‘ğ‘˜
âˆ‘ï¸
ğ‘—=1
1
ğ‘—= ğ‘¡lnğ‘ğ‘˜+ ğ‘‚(1).
â–¡
By setting ğ‘¡= ğœ–, and ğ‘ğ‘–+1 = ğ‘+ğ‘and ğ‘ğ‘–= ğ‘, 1+ ğ‘¡(ğ‘ğ‘–+1âˆ’ğ‘ğ‘–)
ğ‘ğ‘–+1
=
((1 + ğœ–)ğ‘+ ğ‘)/(ğ‘+ ğ‘). Therefore, if ğ‘1 < ğ‘2 < . . . < ğ‘ğ‘˜are
the cluster sizes of a vertex ğ‘£, then, Ãğ‘˜âˆ’1
ğ‘–=1 (1 + ğœ–(ğ‘ğ‘–+1âˆ’ğ‘ğ‘–)
ğ‘ğ‘–+1
) =
ğ‘‚(ğ‘ğœ–
ğ‘˜) = ğ‘‚(ğ‘›ğœ–). Therefore, the total increase in Ëœğ‘¤max(ğ‘£) over
the course of the algorithm is at most ğ‘‚(ğ‘›ğœ–) Assuming a poly-
nomially bounded aspect ratio, i.e., letting Wmax be the largest
edge weight initially and Wmin be the smallest edge weight
initially, we have that Wmax/Wmin âˆˆğ‘‚(poly(ğ‘›)). Therefore
the overall range for the best values of a vertex over a sequence
of merges is also ğ‘‚(poly(ğ‘›)). For any constant ğ›¼> 0, the total
number of broadcasts performed by the vertex over its lifetime
is ğ‘‚(logğ‘›). This is summarized in the following lemma:
Lemma 15. For any constants ğœ–> 0, ğ›¼> 0, and a graph
with polynomially bounded aspect ratio, the total number of
broadcast operations performed by any vertex over the sequence
of the algorithm is ğ‘‚(logğ‘›).
Next, we show that the total number of times a given edge
(ğ‘¢, ğ‘£) in unsuccessfully checked is low. We use the same con-
stants and setting (bounded aspect ratio):
Lemma 16. Each edge (ğ‘¢, ğ‘£) is checked unsuccessfully over all
calls to BestAssignedNeighbor(ğ‘¢) at most ğ‘‚(logğ‘›) times.
Proof. Recall that a check of an edge (ğ‘¢, ğ‘£) is unsuccess-
ful if Â¥ğ‘”(ğ‘¢, ğ‘£) â‰¤(1 + ğœ–)/(1 + ğ›¼) but Ëœğ‘”(ğ‘¢, ğ‘£) > (1 + ğœ–). Since
Â¥ğ‘”(ğ‘¢, ğ‘£) = Ëœğ‘”(ğ‘¢, ğ‘£) when this edge was assigned to ğ‘¢, in the
meantime, Ëœğ‘”(ğ‘¢, ğ‘£) grew by at least a (1 + ğ›¼) factor. This could
occur by ğ‘ˆ= Ëœğ‘¤max(ğ‘¢) growing by a (1 + ğ›¼) factor, or ğ·=
min(M(ğ‘¢), M(ğ‘£),ğ‘¤(ğ‘¢, ğ‘£)) decreasing by a (1 + ğ›¼) factor, or
some combination of the two. In either case, either ğ‘ˆmust
grow or ğ·must shrink by at least a
âˆš
1 + ğ›¼factor i.e., a con-
stant > 1. Using the same idea as the proof of Lemma 15, since
ğºhas bounded aspect ratio, the total amount ğ‘ˆcan grow is
log1+ğ›¼ğ‘›1+ğœ–= ğ‘‚(logğ‘›), and so the number of unsuccessful
checks to due ğ‘ˆis ğ‘‚(logğ‘›). Similarly, since the three terms
in ğ·can be at most Wmax, and can become no smaller than
Wmin/ğ‘›2, the overall number of unsuccessful checks due to
ğ·is also ğ‘‚(logğ‘›), completing the proof.
â–¡
19

Theorem 2. The overall running time of the algorithm is
ğ‘‚((ğ‘š+ ğ‘›) log2 ğ‘›).
Proof. Each broadcast requires updating the values of all
neighbors of a node ğ‘¢in a heap at a cost of ğ‘‚(logğ‘›) per edge.
Since each node requires at most ğ‘‚(logğ‘›) broadcasts in to-
tal, the overall time is ğ‘‚(ğ‘šlog2 ğ‘›). Similarly, the cost over
all calls to BestAssignedNeighbor is ğ‘‚(ğ‘šlog2 ğ‘›) since each
unsuccessful edge must be reinserted into a heap. Lastly, the
extract operations over the min heap indexed on the nodes
costs ğ‘‚(ğ‘›logğ‘›) in total. Although the queue may be updated
more times than this due to cases where all edges in a call to
BestAssignedNeighbor are unsuccessful, but this cost can be
charged to one of the unsuccessful edges.
â–¡
20

