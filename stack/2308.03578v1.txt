TeraHAC: Hierarchical Agglomerative Clustering
of Trillion-Edge Graphs
Laxman Dhulipala
UMD and Google Research
USA
laxman@umd.edu
Jason Lee
Google Research
USA
jdlee@google.com
Jakub Łącki
Google Research
USA
jlacki@google.com
Vahab Mirrokni
Google Research
USA
mirrokni@google.com
ABSTRACT
We introduce TeraHAC, a (1 + 𝜖)-approximate hierarchical
agglomerative clustering (HAC) algorithm which scales to
trillion-edge graphs. Our algorithm is based on a new approach
to computing (1 + 𝜖)-approximate HAC, which is a novel
combination of the nearest-neighbor chain algorithm and the
notion of (1 + 𝜖)-approximate HAC. Our approach allows us
to partition the graph among multiple machines and make
significant progress in computing the clustering within each
partition before any communication with other partitions is
needed.
We evaluate TeraHAC on a number of real-world and syn-
thetic graphs of up to 8 trillion edges. We show that TeraHAC
requires over 100x fewer rounds compared to previously known
approaches for computing HAC. It is up to 8.3x faster than
SCC, the state-of-the-art distributed algorithm for hierarchi-
cal clustering, while achieving 1.16x higher quality. In fact,
TeraHAC essentially retains the quality of the celebrated HAC
algorithm while significantly improving the running time.
1
INTRODUCTION
Hierarchical agglomerative clustering (HAC) is a widely-used
clustering algorithm [28, 44–47, 55] known for its high quality
in a variety of applications [11, 19, 30, 34, 61].
The algorithm takes as input a collection of 𝑛points, as well
as a function giving similarities between pairs of points. At a
high level, the algorithm works as follows. Initially, each point
is put in a separate cluster of size 1. Then, the algorithm runs
a sequence of steps. In each step, the algorithm finds a pair of
most similar clusters and merges them together, that is, the
two clusters are replaced by their union. Here, the similarity
between the clusters is computed based on the similarities
between the points in the clusters. The exact formula used is
configurable and referred to as linkage function.
Common linkage functions include single-linkage – the
similarity between two clusters 𝐶and 𝐷is the maximum
similarity of two points belonging to 𝐶and 𝐷, and average-
linkage – the similarity between 𝐶and 𝐷is the total similarity
between points in 𝐶and 𝐷divided by |𝐶| · |𝐷|. Due to the
particularly good empirical quality, HAC using average linkage
similarity is of particular interest [17, 30, 34, 40, 43, 61].
The output of the algorithm is a dendrogram – a rooted
binary tree (or a collection thereof) describing the merges per-
formed by the algorithm. The leaves of the tree correspond to
the initial clusters of size 1, and each internal vertex represents
a cluster obtained by merging its two children.
Up until recently, a major shortcoming of HAC was very
limited scalability, as the best known algorithms required time
that is quadratic in the number of input points. In addition to
that, it was commonly assumed that the input to the algorithm
is a complete 𝑛×𝑛matrix giving similarities between all input
points, which posed another scalability barrier.1
Recent results overcome the quadratic-time bottleneck by
allowing (1 + 𝜖)-approximation [25, 26]. A HAC algorithm
is (1 + 𝜖)-approximate if in each step it merges together two
clusters whose similarity is within a (1 + 𝜖) factor from the
maximum similarity between two clusters (at that point).
By allowing approximation and assuming that the input to
the algorithm is a sparse similarity graph (e.g. the k-nearest
neighbors graph of the input points), it was shown that HAC
can be implemented in near-linear time [26] and efficiently
parallelized [25]. At the same time, both relaxations (approxi-
mation and using a sparse graph) were shown not to negatively
affect the empirical quality of the obtained solutions [25, 26].
This line of work led to single machine parallel HAC implemen-
tations scaling to graphs containing several billion vertices
and about 100 billion edges [25].
Nevertheless, some large-scale applications require clus-
tering even larger datasets. To address these needs, several
distributed hierarchical clustering algorithms have been pro-
posed [7, 31, 32, 40, 56]. These algorithms can handle up to
tens of billions of datapoints and several trillion edges. How-
ever, in order to provide high quality results and/or provable
guarantees on the quality of the output, the algorithms need
to run typically a significant number of parallel rounds on
large-scale datasets [40, 56], and for algorithms that can run
in few rounds, no provable quality guarantees are known [40].
An intriguing open question, then, is whether it is possible to
achieve provable quality guarantees for hierarchical clustering,
while running in very few rounds.
1Even if only a linear number of pairs of points had nonzero similarity, the
previously known algorithms would still require quadratic time.
arXiv:2308.03578v1  [cs.DS]  7 Aug 2023

1.1
Our Contribution
In this paper we introduce TeraHAC – a (1 + 𝜖)-approximate
HAC algorithm, which runs in very few rounds and is thus
amenable to a distributed implementation. We obtain TeraHAC
by developing a new approach to computing (1+𝜖)-approximate
HAC, which may be of independent interest. We evaluate
TeraHAC empirically and show that our implementation scales
to datasets of tens of billions of points and trillions of edges.
Our key algorithmic contribution is a new approach to
computing (1 + 𝜖)-approximate HAC, which we now out-
line. The exact (1-approximate) HAC algorithm defines a very
specific order, in which cluster merges must be performed
– only a highest similarity pair of clusters can be merged in
each step. However, the very same output dendrogram (up
to different tiebreaking) can be computed using a (parallel)
RAC algorithm [56] (which is based on the idea behind the
nearest-neighbor chain algorithm [8, 60]). In each step, the
RAC algorithm finds all reciprocally most similar clusters. That
is, it finds each pair of clusters 𝐶and 𝐷, such that 𝐷is the
most similar cluster to 𝐶, and 𝐶is the most similar cluster to
𝐷(for simplicity, let us assume that there are no ties). It is
easy to see that the reciprocally most similar pairs of clusters
form a matching. Then, the algorithm merges both clusters
of each pair in parallel. Clearly, this approach allows poten-
tially making multiple merges in one step, some of which may
merge pairs of clusters whose similarity is much smaller than
the maximum similarity between two clusters. Crucially, this
yields the same result as the basic algorithm, even though both
algorithms may perform merges in a very different order.
We generalize the nearest-neighbor chain/RAC algorithm
to a (1 + 𝜖)-approximate algorithm, by introducing a notion
of a good merge. Namely, we call the merge of two clusters
(1 +𝜖)-good if it satisfies a certain condition (see Definition 2).
Crucially, whether a merge of two clusters is good can be
checked locally, essentially by looking at the clusters and their
incident edges. We show that by performing the (1 + 𝜖)-good
merges in any order (for example, in parallel) one obtains a
(1+𝜖)-approximate dendrogram, i.e., one that can be obtained
by performing a sequence of (1 + 𝜖)-approximate merges.
To put our algorithmic result in context, let us compare the
amount of parallelism available to different HAC algorithms
by looking at what edges can be merged in the beginning of
the algorithm. Fig. 1 compares the amount of parallelism in the
RAC algorithm, TeraHAC and ParHAC. ParHAC is a parallel
(1+𝜖)-approximate parallel HAC algorithm [26], which allows
merging any edge whose weight is at most a (1+𝜖) factor away
from the globally maximum weight. If we consider the sets of
edges that can be merged by RAC and ParHAC, it is easy to see
that none of them is a superset of the other one. At the same
time, the set of edges that TeraHAC can merge is a superset of
both sets (this is the case in general, not just in the provided
example). As shown in Fig. 2 the increased parallelism leads
to over 100x reduction in the number of rounds. Importantly,
the reduction in the number of rounds also yields between
3.5–10.5x improvements in running time as shown in Fig. 3.
We evaluate TeraHAC on a number of graph datasets of up
to 8 trillion edges. We first study its quality on several smaller
1
0.95
0.8
0.9
0.7
0.82
0.9
𝖯𝖺𝗋𝖧𝖠𝖢, ϵ = 0.1
𝖱𝖠𝖢
𝖳𝖾𝗋𝖺𝖧𝖠𝖢, ϵ = 0.1
Figure 1: Comparison of the merges available at the start of
the algorithm for different parallel HAC algorithms.
datasets with ground truth. We show that when using 𝜖= 0.1
TeraHAC, the difference in quality compared to exact HAC
is only 1%–3% (according to standard clustering similarity
measures, ARI and NMI), which is in line with previous results
on (1 + 𝜖)-approximate HAC [25, 26]. Moreover, TeraHAC is
more scalable and accurate than SCC [40] – the state of the art
distributed algorithm for hierarchical graph clustering; it is on
average 8.3x faster than SCC in SCC’s highest-quality setting
(using 100 rounds) on a set of large real-world web graphs. For
the same high-quality setting, we find that TeraHAC achieves
1.16x higher quality than SCC.
Second, we study the scalability of TeraHAC on a num-
ber of large graphs. We show that for 𝜖= 0.1 the number of
rounds TeraHAC runs is very low (at most 17 rounds over all
our datasets), and that the residual graph shrinks at a geo-
metric rate. We also find that allowing approximation yields
over an order of magnitude more (1 + 𝜖)-good edges (edges
that TeraHAC can potentially merge), compared with edges
mergeable by RAC (see Fig. 15).
Finally, we study the performance and quality of TeraHAC
on an 8 trillion-edge graph representing similarities between
31 billion web queries using a set of human-generated labels.
We observe that TeraHAC performs consistently better along
a precision-recall curve, in some cases improving recall by up
to 20% (relative), while being 30%–50% faster.
1.2
Further Related Work
The HAC algorithm is particularly useful for semantic cluster-
ing applications, where the desired number of clusters is not
known upfront, for example for de-duplication or, more gener-
ally, detecting groups of closely related entities [30, 34, 40, 61].
In these cases in large-scale applications the number of clusters
of interest is very high (close to the number of input entities),
and so the average cluster size is low [34].
For other prominent applications of clustering, multiple
successful algorithms have been developed. For example DB-
SCAN [52] and correlation clustering [6] are particularly well
suited for finding anomalously dense clusters. Modularity clus-
tering [48] is widely used for community detection. Graph par-
titioning methods [9, 13] split the data into a typically small
number of roughly-equally sized clusters, while ensuring that
related entities are not split across different clusters (hence,
it is in a sense a dual problem to the one addressed by HAC).
Finally, k-means/k-median/k-center methods as well as spec-
tral clustering [23, 49] are particularly well-suited when the
desired number of clusters is known upfront.
The notion of approximate HAC [41] was used to give more
efficient HAC algorithms in the case when the input is a metric
space [1, 41, 42, 59] as well as a graph [25, 26].
2

OK
TW
FS
CW
23
25
27
29
211
213
Number of Rounds
TeraHACϵ=0.1
OptimizedRAC
RAC
ParHACϵ=0.1
Figure 2: Number of rounds used by TeraHAC compared with
OptimizedRAC (TeraHAC using 𝜖= 0), ParHAC and RAC on four
large real-world graph datasets. All algorithms use a weight
threshold of 𝑡= 0.01 (see Section 6).
OK
TW
FS
CW
25000
50000
75000
100000
125000
150000
175000
200000
Running Time in Seconds
TeraHACϵ=0.1
OptimizedRAC
Figure 3: Distributed running times of TeraHAC compared
with OptimizedRAC (TeraHAC using 𝜖= 0) on the same graphs
and threshold as Figure 2.
For average linkage HAC, to the best of our knowledge, the
fastest single-machine implementations scale to tens of mil-
lions of points in metric space [60] or billion-vertex graphs [25].
A number of graph clustering problems have been stud-
ied in the distributed setting, for example in MapReduce [21],
Pregel/Giraph [38, 51] and other frameworks captured by the
MPC model of computation [33]. Examples include efficient
algorithms for correlation clustering [16, 18], connected com-
ponents [36] and balanced partitioning [4, 5].
For HAC, efficient distributed algorithms are known for
the single-linkage variant [31, 32, 59]. We note that single-
linkage hierarchical agglomerative clustering generally deliv-
ers worse quality compared to average linkage [17, 61]. Several
HAC algorithms have been recently developed in the shared-
memory setting, including SeqHAC, a nearly linear time (1+𝜖)-
approximate sequential HAC algorithm [26] and ParHAC, a
nearly linear work (1 + 𝜖)-approximate HAC algorithm with
low depth [25]. ParHAC provably runs in poly-logarithmic
rounds [25], whereas TeraHAC uses a novel relaxation of the
reciprocal clustering approach of RAC, and neither TeraHAC
nor RAC are known to admit non-trivial round-complexity
bounds. However, empirically TeraHAC uses the fewest rounds
of any modern HAC algorithm we study (see Figure 8).
The most scalable hierarchical clustering algorithms have
been obtained by slightly diverging from the HAC algorithm
definition. Affinity clustering [7], and its successor SCC [40]
are two highly scalable hierarchical clustering algorithms de-
signed for the MapReduce model. Both take graphs as input
and were shown to scale to graphs with trillions of edges.
While both algorithms are similar to HAC by design, in prac-
tice they can give very different results from what the HAC
algorithm gives. We compare with SCC in our empirical analy-
sis. We conclude that while SCC can be tuned to obtain quality
which is comparable with HAC, this comes at a cost of signif-
icantly higher running time of SCC. We note that SCC was
shown to deliver higher quality than affinity clustering.
Another highly scalable HAC implementation is the RAC
algorithm [56] obtained by parallelizing the nearest neighbor
chain algorithm. The algorithm was shown to scale to datasets
of billion points. Since RAC is based on the exact HAC al-
gorithm, the number of rounds it uses is an order of magni-
tude larger than what can be achieved by (1 + 𝜖)-approximate
HAC [25], which negatively affects performance. As we show,
TeraHAC using 𝜖= 0 can be viewed as an optimized imple-
mentation of RAC, which we refer to as OptimizedRAC. At
the same time TeraHAC using 𝜖= 0 uses up to two orders of
magnitude fewer rounds (see Figure 2).
2
PRELIMINARIES
Let 𝐺= (𝑉, 𝐸,𝑤) be a weighted and undirected graph, where
|𝑉| = 𝑛. We assume that all edge weights of 𝐺are positive.
Let us now describe how the HAC algorithm works given
𝐺as an input. Initially there are 𝑛clusters, each containing a
distinct vertex of 𝐺. The algorithm proceeds in at most 𝑛−1
steps. In each step, the algorithm finds two clusters of nonzero
similarity and merges them together. Let 𝐶1, . . . ,𝐶𝑘, be a se-
quence of clusterings, where 𝐶𝑖is the clustering obtained after
running 𝑖−1 steps of the algorithm. In particular, 𝐶1 is the
clustering containing 𝑛clusters of size 1.
We define a sequence of graphs 𝐺1, . . . ,𝐺𝑘, where 𝐺𝑖is
obtained from𝐺by contracting the clusters𝐶𝑖. Specifically, the
vertex set of𝐺𝑖is𝐶𝑖, which means that each vertex of𝐺𝑖is a set
of vertices of𝐺. We define the function𝑤: 2𝑉×2𝑉→R giving
edge weights in 𝐺𝑖as 𝑤(𝑢, 𝑣) = Í
𝑥𝑦∈((𝑢×𝑣)∩𝐸) 𝑤(𝑥𝑦)/(|𝑢| ·
|𝑣|). Note that the weight function 𝑤is the same for all graphs
𝐺𝑖. There is an edge between two vertices 𝑢and 𝑣in 𝐺𝑖if and
only if 𝑤(𝑢, 𝑣) > 0.
Definition 1 ([8]). A cluster similarity function 𝑓: 2𝑉×2𝑉→
R is called reducible if and only if for any clusters 𝑥,𝑦,𝑧, we
have 𝑓(𝑥,𝑦∪𝑧) ≤max(𝑓(𝑥,𝑦), 𝑓(𝑥,𝑧)).
In particular, the function 𝑤defined above is reducible [8].
While in the following we work with this specific similarity
function, we note that our main theoretical results hold for
any similarity function satisfying Definition 1, which includes
e.g. single linkage, complete linkage and median linkage.
Note that 𝐺𝑖+1 is obtained from 𝐺𝑖by contracting a single
edge and updating the weights of the edges incident to the
vertex created by the contraction. We call each such operation
a merge of the endpoints of the contracted edge. Let 𝑤𝑚𝑎𝑥(𝐺𝑖)
be the largest edge weight in𝐺𝑖. We say that a merge of an edge
𝑢𝑣in 𝐺𝑖is (1 + 𝜖)-approximate, if (1 + 𝜖)𝑤(𝑢𝑣) ≥𝑤𝑚𝑎𝑥(𝐺𝑖).
A dendrogram is a tree describing all merges performed
by running a HAC algorithm. The tree has between 𝑛and
2𝑛−1 nodes2 corresponding to clusters that are created in
the course of the algorithm. Specifically, there are 𝑛leaf nodes
corresponding to the initial singleton clusters. In addition, for
each merge that creates a cluster 𝑥∪𝑦by merging 𝑥and 𝑦,
2We use nodes when talking about trees and vertices in the context of graphs.
3

there is a node 𝑥∪𝑦, whose children are 𝑥and 𝑦. Hence, each
internal node of the dendrogram has exactly two children.
In the description of the algorithm, instead of using a den-
drogram, it is more convenient to use a merge tree. Observe
that if we remove all leaf nodes from the dendrogram, there
is a natural 1-to-1 correspondence between the remaining
nodes and the merges made by the algorithm. We call the tree
obtained this way, in which each node is a merge, a merge tree.
Observe that there may be multiple different sequences of
merges made by the algorithm that produce the very same
merge tree (and dendrogram). Specifically, given a fixed merge
tree, all we know is that the first merge made by the algorithm
must have been some merge corresponding to one of its leaves.
Given a merge tree, and a sequence 𝑚1, . . . ,𝑚𝑘of 𝑘distinct
merges, we say that the sequence is consistent with the tree,
if and only if for each 1 ≤𝑖≤𝑘, either 𝑚𝑖is a leaf in the
merge tree, or 𝑚𝑖is an internal node of the merge tree and the
children of 𝑚𝑖are 𝑚𝑎and 𝑚𝑏, where max(𝑎,𝑏) < 𝑖.
We say that a dendrogram is (1 + 𝜖)-approximate if and
only if there exists a sequence of (1 + 𝜖)-approximate merges,
such that (a) the sequence is consistent with the merge tree
of the dendrogram, and (b) after performing all merges in the
sequence, no pair of clusters has nonzero similarity. Finally, a
(1 + 𝜖)-approximate HAC algorithm is one which produces a
(1 + 𝜖)-approximate dendrogram.
3
APPROXIMATE NEAREST-NEIGHBOR
CHAIN ALGORITHM
In this section we extend the 40-year old nearest-neighbor
chain algorithm [8] (NN-chain) with the notion of approxima-
tion. Let 𝐺be an edge-weighted graph. For each vertex 𝑣of
𝐺, we denote by 𝑤max(𝑣) the maximum weight of any edge
incident to 𝑣. It follows from the NN-chain / RAC algorithm
that the following algorithm is equivalent to the (exact) HAC
algorithm. As long as the graph has nonzero edges, pick any
edge 𝑢𝑣, such that 𝑤(𝑢𝑣) = 𝑤max(𝑢) = 𝑤max(𝑣) and merge
together its endpoints.
Somewhat surprisingly, even though the HAC algorithm
specifies a concrete order of merging edges, the NN-chain
algorithm computes the very same dendrogram (up to ties
among edge weights) as the standard HAC algorithm. At the
same time, the NN-chain algorithm has an important feature:
the decision on whether to merge two vertices can be made
entirely locally, which is a very useful property in a parallel
setting. In this section, we give an (1 + 𝜖)-approximate HAC
algorithm also based on a simple local criterion for deciding
whether two vertices can be merged, which we define below.
Definition 2 (Good merge). Let 𝜖≥0 and 𝐺𝑖be a graph
obtained from 𝐺by performing some sequence of merges. For
each vertex 𝑣of 𝐺𝑖, we define M(𝑣) to be the smallest linkage
similarity among all merges that were performed to create cluster
𝑣. Specifically, for each singleton cluster 𝑣we have M(𝑣) = ∞,
and whenever two clusters 𝑢and 𝑣are merged and create cluster
𝑢∪𝑣, we have M(𝑢∪𝑣) = min(M(𝑢), M(𝑣),𝑤(𝑢𝑣)). With this
notation, we say that a merge of an edge𝑢𝑣in𝐺𝑖is (1+𝜖)−good
1
1 + ϵ
1 + ϵ
a
b
c
d
(1 + ϵ)2
{a, b}
c
d
M({a, b}) = 1
1 + ϵ
(1 + ϵ)2
Figure 4: Example showing the need for 𝑀(·) values in Defi-
nition 2. Green edges correspond to merges which are (1 + 𝜖)-
good, and red edges correspond to merges which are not (1 +𝜖)-
good. After merging 𝑎𝑏(which is (1 + 𝜖)-good) we obtain a
vertex {𝑎,𝑏} such that 𝑀({𝑎,𝑏}) = 1. Therefore, the merge of
{𝑎,𝑏} with 𝑐in the resulting graph is not (1 + 𝜖)-good, since
max(1 + 𝜖, (1 + 𝜖)2)/min(1, ∞, 1 + 𝜖) = (1 + 𝜖)2 > 1 + 𝜖. Hence,
the algorithm is forced to merge 𝑐with 𝑑. It is easy to see that
allowing a merge of {𝑎,𝑏} with 𝑐would create a dendrogram,
which is not (1 + 𝜖)-approximate.
if and only if
max(𝑤max(𝑢),𝑤max(𝑣))
min(M(𝑢), M(𝑣),𝑤(𝑢𝑣)) ≤1 + 𝜖.
When 𝜖is clear from the context or irrelevant, we will
sometimes say good instead of (1 + 𝜖)-good. Observe that the
value of M(𝑢) only depends on the sequence of merges which
created 𝑢. In contrast, 𝑤max(𝑢) is a function of both 𝑢and
some merges outside of 𝑢, as it also depends on the current
set of neighbors of 𝑢. This means that M(𝑢) is well defined
given a particular merge tree. However, to compute 𝑤max(𝑢)
we also need to specify which merges have been performed.
We will show that applying (1 + 𝜖)-good merges leads to
a (1 + 𝜖)-approximate dendrogram (see Lemma 4). For 𝜖= 0
this generalizes the RAC algorithm, as in this case the 𝑀(𝑢)
and 𝑀(𝑣) terms in the denominator become irrelevant (see
Observation 1). However, they are crucially important for
𝜖> 0, as shown in Figure 4.
In the initial graph 𝐺1, a merge is good iff we have that
max(𝑤max(𝑢),𝑤max(𝑣))/𝑤(𝑢𝑣) ≤1+𝜖, regardless of the max-
imum weight of an edge in the graph. In general, each (1 + 𝜖)-
approximate is (1+𝜖)-good, but the converse is not necessarily
true. Nevertheless, we can show that performing a sequence
of good merges produces a (1 + 𝜖)-approximate dendrogram.
Before we formally state and prove this property, we first show
a few auxiliary lemmas.
Lemma 1. Let 𝐺1, . . . ,𝐺𝑛be a sequence of graphs, in which
each graph is obtained from the previous one by performing
an arbitrary merge. Let 𝑣be a vertex (cluster) which exists in
𝐺𝑙, . . . ,𝐺𝑟. Let 𝑤𝑖max(𝑣) be the value of 𝑤max(𝑣) in 𝐺𝑖, where
𝑙≤𝑖≤𝑟. Then 𝑤𝑙max(𝑣) ≥𝑤𝑙+1
max(𝑣) ≥. . . ≥𝑤𝑟max(𝑣).
Now, we prove a useful invariant, which is satisfied when
performing good merges.
Lemma 2. Let 𝐺𝑖be a graph obtained from 𝐺1 by performing
(1 + 𝜖)-good merges. Then, for each vertex 𝑣of 𝐺𝑖, we have that
𝑤max(𝑣)/M(𝑣) ≤1 + 𝜖.
Proof. We prove the claim by induction on the number of
good merges made. The base case (no merges) follows trivially
from the fact that M(𝑣) = ∞for each vertex 𝑣. Now, consider
4

that some number of good merges have been made, and the
resulting graph is 𝐺𝑖+1. Fix a vertex 𝑣. If the vertex 𝑣also
exists in 𝐺𝑖, the property follows by Lemma 1. Otherwise, 𝑣is
created by merging two vertices 𝑣1 and 𝑣2 (𝑣= 𝑣1 ∪𝑣2) using
a (1 + 𝜖)-good merge. We have
𝑤max(𝑣)
M(𝑣)
≤max(𝑤max(𝑣1),𝑤max(𝑣2))
M(𝑣)
≤1 + 𝜖.
In the first inequality we used reducibility (Definition 1), which
implies 𝑤max(𝑣) ≤max(𝑤max(𝑣1),𝑤max(𝑣2)). The second in-
equality follows from the definition of a good merge.
□
Observation 1. Let 𝐺𝑖be obtained from 𝐺1 by performing
1-good merges. Then, a merge 𝑢𝑣in 𝐺𝑖is 1-good if and only if
𝑤(𝑢𝑣) = max(𝑤max(𝑢),𝑤max(𝑣)).
The proof uses Lemma 2; we provide it in the Appendix.
Definition 3. Let 𝐷be a dendrogram and 𝑀be its correspond-
ing merge tree. Consider a sequence of merges 𝑚1, . . . ,𝑚𝑘which
contains all merges of 𝑀and is consistent with 𝑀. We say that
this sequence is a greedy merge sequence of 𝐷if its obtained
as follows. Start with an empty sequence. At each step append
a merge 𝑚𝑖of maximum weight, chosen such that the new ex-
panded sequence is consistent with 𝑀.
A greedy merge sequence is a canonical merge sequence,
in the sense that it achieves the best approximation ratio (i.e.
the maximum approximation ratio of a merge is minimized)
out of all consistent orderings of merges of 𝑀.
Let 𝐷be a dendrogram and 𝑀be its corresponding merge
tree. Let 𝑚1, . . . ,𝑚𝑘be a sequence of merges consistent with
𝑀and 𝐺𝑘be the graph obtained after applying them. We
say that the error of a merge 𝑚which merges 𝑢and 𝑣is the
maximum weight of an edge in 𝐺𝑘divided by 𝑤(𝑢𝑣).
Lemma 3. Let 𝐺= (𝑉, 𝐸,𝑤) be a weighted graph, 𝐷be a
dendrogram of 𝐺and 𝑚1, . . . ,𝑚𝑛be its greedy merge sequence.
Let 1 + 𝜖be the maximum error of a merge in the produced
sequence. Then, 𝐷is (1 + 𝜖)-approximate and is not (1 + 𝜖′)-
approximate for any 𝜖′ < 𝜖.
Proof. Let 𝑞1, . . . ,𝑞𝑛be an optimal merge sequence consis-
tent with 𝑀, that is, a sequence in which the maximum error
of a merge is minimum possible. Let 1 + 𝜖be the maximum
error of a merge in this sequence.
We refer to the algorithm from the lemma statement as the
greedy algorithm. Note that the merge sequence 𝑚1, . . . ,𝑚𝑛
it produces is a permutation of 𝑞1, . . . ,𝑞𝑛, as both sequences
consist of all merges of 𝑀. It suffices to prove that each merge
appended by the algorithm has error at most 1 + 𝜖. We prove
that this property holds for each prefix 𝑚1, . . . ,𝑚𝑘by induc-
tion on 𝑘. The case of 𝑘= 0 is trivial. Consider 𝑘> 0.
Let 𝑗≥0 be the minimum value such that 𝑞𝑗+1 is not one of
𝑚1, . . . ,𝑚𝑘. This implies that the set of merges {𝑚1, . . . ,𝑚𝑘}
is a superset of {𝑞1, . . . ,𝑞𝑗}. This in turn implies that the max-
imum weight in the graph after applying merges 𝑚1, . . . ,𝑚𝑘
is not larger than the maximum weight in the graph after
applying 𝑞1, . . . ,𝑞𝑗. Hence, the error of the merge 𝑞𝑗+1 after
applying 𝑚1, . . . ,𝑚𝑘is at most the error of 𝑞𝑗+1 after applying
𝑞1, . . . ,𝑞𝑗, which by definition is at most 1+𝜖. Since the greedy
algorithm chooses the merge of minimum error, we conclude
that the error of 𝑚𝑘+1 is at most 1 + 𝜖.
□
We conclude this section with our main theoretical result,
which shows that applying (1 + 𝜖)-good merges produces a
(1 + 𝜖)-approximate dendrogram. We stress that the (1 + 𝜖)-
good merges can be applied in any order (as long as they
are (1 + 𝜖)-good in that order). This allows us to remove the
greedy aspect of HAC from consideration and achieve better
parallelism.
Lemma 4. Let 𝐷be a dendrogram of 𝐺obtained by applying
(1 + 𝜖)-good merges and 𝜖> 0. Then, 𝐷is (1 + 𝜖)-approximate.
Proof. Let 𝑚1, . . . ,𝑚𝑟be a greedy merge sequence of 𝐷.
We use induction on 𝑘to show that all merges 𝑚1, . . . ,𝑚𝑘are
(1 + 𝜖)-approximate. For 𝑘= 0 the lemma is trivial, so let us
assume 0 < 𝑘≤𝑟. Let 𝐺𝑘be the graph obtained by applying
the merges𝑚1, . . . ,𝑚𝑘−1. Let 𝑥𝑦be the maximum weight edge
in 𝐺𝑘(chosen arbitrarily in case of a tie).
We will say that a merge 𝑚′ is available if it’s one of the
merges in the merge tree of 𝐷, and𝑚1, . . . ,𝑚𝑘,𝑚′ is consistent
with the merge tree of 𝐷. Our goal is to show that𝑚𝑘is (1+𝜖)-
approximate. Hence, we need to show that the merge similarity
of 𝑚𝑘is at least 𝑤(𝑥𝑦)/(1 + 𝜖). To that end, it suffices to show
that there is an available merge of similarity at least𝑤(𝑥𝑦)/(1+
𝜖). This is because, thanks to the greedy construction, 𝑚𝑘is
the available merge of maximum merge similarity.
Consider the first merges in the dendrogram 𝐷, which in-
volved vertices 𝑥and 𝑦. That is, let 𝑥′ be the sibling vertex of
𝑥in the dendrogram 𝐷and 𝑦′ be the sibling vertex of 𝑦(i.e.,
𝑥′ is the cluster with which 𝑥is merged in the dendrogram).
If 𝑥′ = 𝑦, then also 𝑦′ = 𝑥and the merge of 𝑥with 𝑦is
available. The weight of this merge is 𝑤(𝑥𝑦), which is trivially
more than 𝑤(𝑥𝑦)/(1+𝜖). This concludes the proof in this case.
Now consider the case when 𝑥′ ≠𝑦(which also means
that 𝑦′ ≠𝑥). Without loss of generality, assume that in the
merge sequence which produced dendrogram 𝐷, the merge of
𝑥′ with 𝑥happened before the merge of 𝑦′ with 𝑦.
Claim 1. We have 𝑤(𝑥𝑦)/min(M(𝑥′), M(𝑥),𝑤(𝑥𝑥′)) =
𝑤(𝑥𝑦)/M(𝑥′ ∪𝑥) ≤1 + 𝜖.
The proof can be found in the Appendix.
Consider the subtree of the merge tree rooted at 𝑥′ ∪𝑥.
We will show that that all merges in that subtree have merge
similarity at least 𝑤(𝑥𝑦)/(1 + 𝜖). This would finish the proof,
as the subtree clearly contains at least one available merge.
Consider any available merge in that subtree and assume
that it merges together 𝑧1 and 𝑧2, where 𝑧1,𝑧2 are both subsets
of 𝑥′ ∪𝑥. By the definition of M and Claim 1, 𝑤(𝑧1𝑧2) ≥
M(𝑥′ ∪𝑥) ≥𝑤(𝑥𝑦)/(1 + 𝜖), which completes the proof.
□
4
TeraHAC ALGORITHM
The TeraHAC algorithm is presented as Algorithm 1. It takes
as input a weighted graph 𝐺, the accuracy parameter 𝜖and a
weight threshold 𝑡> 0. Each vertex 𝑣of 𝐺is assigned a M(𝑣)
value which is initially equal to ∞. The weight threshold 𝑡is
used to prune the dendrogram by performing vertex pruning,
5

Algorithm 1 TeraHAC(𝐺= (𝑉, 𝐸,𝑤, M),𝜖,𝑡)
Input: Similarity graph 𝐺, 𝜖> 0, 𝑡≥0.
Output: (1 + 𝜖)-approximate HAC dendrogram.
1: while |𝐸| > 0 do
2:
Partition 𝐺into clusters 𝐶′
1, . . . ,𝐶′
𝑘
3:
for each cluster 𝐶in 𝐶′
𝑖in parallel do
4:
𝐺𝐶:= subgraph of 𝐺consisting of vertices in 𝐶
and all their incident edges
5:
𝑀:= SubgraphHAC(𝐺𝐶,𝐶)
6:
Apply the merges 𝑀in the graph 𝐺
7:
Remove from 𝐺vertices 𝑣s.t. 𝑤max(𝑣) < 𝑡/(1 + 𝜖)
Algorithm 2 SubgraphHAC(𝐺= (𝑉, 𝐸,𝑤, M),𝐴,𝜖)
Input: 𝐺𝐴= (𝑉, 𝐸,𝑤, M), 𝐴⊆𝑉, 𝜖> 0.
Output: A set of merges of vertices in 𝐴
1: Mark vertices of 𝑉\ 𝐴as inactive
2: 𝑇:= priority queue with edges of𝐺[𝐴] keyed by goodness.
3: while min(𝑇) ≤1 + 𝜖do
4:
𝑢𝑣:= RemoveMin(𝑇)
5:
Remove from 𝑇all edges incident to 𝑢or 𝑣
6:
Merge𝑢and 𝑣in𝐺, and update the weights of all edges
incident to 𝑢∪𝑣in 𝐺
7:
Add to 𝑇incident edges of 𝑢∪𝑣that do not have an
inactive endpoint
8: Return all merges made
i.e., stop the algorithm once it performs all merges of suffi-
ciently high similarity. The effect of this parameter will be
discussed later. For now, let us assume that 𝑡= 0, which makes
line 7 redundant. We analyze the 𝑡> 0 case in Section 4.1.
The algorithm runs in a loop. Each iteration of the loop
is a round, which merges together some vertices of 𝐺. The
loop runs until the graph has no edges. In each round, we
first partition the graph 𝐺into clusters. The choice of the
partitioning method affects the running time of the algorithm,
but from the correctness point of view, the partitioning can
be arbitrary.
Given a graph 𝐺= (𝑉, 𝐸,𝑤) and a subset of its vertices 𝐶,
we use 𝐺𝐶to denote a graph consisting of all vertices of 𝐶and
their immediate neighbors, as well as edges that have at least
one endpoint in 𝐶. Formally, the vertex set of 𝐺𝐶is 𝐶∪{𝑥∈
𝑉| ∃𝑦∈𝐶𝑥𝑦∈𝐸} and the edge set is {𝑥𝑦∈𝐸| 𝑥∈𝐶∨𝑦∈𝐶}.
Once the partitioning is computed, for each cluster 𝐶we
compute a graph . Observe that each intra-cluster edge belongs
to exactly one graph and each inter-cluster edge belongs to
exactly two graphs computed this way.
Next, the algorithm runs SubgraphHAC algorithm on each
graph 𝐺𝐶. The goal of SubgraphHAC is to find a longest se-
quence of merges which are (1 + 𝜖)-good (see Definition 2).
The challenge is that while the algorithm only sees a subgraph
𝐺𝐶of 𝐺, the merges it makes should be good with respect to
the entire graph 𝐺. Once we have computed all merges made
by all SubgraphHAC calls, we apply them in the global graph.
Let us now describe SubgraphHAC algorithm in detail. The
algorithm is given a set 𝐶of vertices of 𝐺and a graph 𝐺𝐶.
The vertices of 𝐶in 𝐺𝐶are called active, and the remaining
vertices are inactive. The goal of the algorithm is to perform a
maximal sequence of good merges of active vertices in 𝐶. We
assume that vertices formed by merging active vertices are
also active. We later show that the good merges in 𝐺𝐶are also
good merges in the "global" graph 𝐺.
To decide whether a merge of 𝑢and 𝑣is good, we need to
know all incident edges of 𝑢and 𝑣. This is why the graph 𝐺𝐶
contains not only vertices of 𝐶, but also their neighbors. Since
each merge can only involve vertices of 𝐶(i.e., they do not
affect the inactive vertices), the merges performed by parallel
invocations of SubgraphHAC affect disjoint sets of vertices.
Given an edge 𝑢𝑣of 𝐺𝐶, where both 𝑢and 𝑣are active
(that is, not inactive) we define goodness(𝑢𝑣) = max(𝑤max(𝑢),
𝑤max(𝑣))/M(𝑢∪𝑣). By Definition 2, a merge of 𝑢with 𝑣is
(1 +𝜖)-good iff goodness(𝑢𝑣) ≤1 +𝜖.3 At a high level, in each
step the algorithm finds the merge of the smallest goodness.
If the goodness is more than 1 + 𝜖, it terminates. Otherwise, it
performs the merge and continues.
We give a near-linear time implementation of SubgraphHAC
which performs a nearly-maximal set of (1 + 𝜖)-good merges.
More specifically, the algorithm is guaranteed to perform all
merges in the subgraph with goodness in the range [1,𝑇)
where 𝑇= 1 + Θ(𝜖), and no merges with goodness > 1 + 𝜖.
We bound the running time of SubgraphHAC as follows.
Theorem 1. The running time of SubgraphHAC on a graph
containing 𝑛vertices and 𝑚edges is 𝑂((𝑚+ 𝑛) log2 𝑛).
We provide the proof of this result in the Appendix. Our
algorithm is similar to a recent (1 + 𝜖)-approximate HAC
algorithm [26], but is significantly more complicated due to
needing to maintain not only edge weights as merges are
performed, but also the goodness values of edges.
Let us now discuss the correctness of the algorithm. It is
easy to see that each merge SubgraphHAC performs is good
(with respect to its input graph), and that it returns once no
good merges can be made. However, it is not obvious whether
the merges computed by all SubgraphHAC calls are also good
when applied on the global graph. In particular, there are two
possible issues. First, each SubgraphHAC calls only sees a
subgraph of the entire graph and computes good merges with
respect to that subgraph. Second, all merges produced by the
parallel calls are then applied on the entire graph, but we do
not specify in what order they need to be applied. We address
these in the following two lemmas.
Lemma 5. For some 𝐶⊆𝑉, consider the graph 𝐺𝐶and let
𝑚1, . . . ,𝑚𝑘be a sequence of merges in 𝐺𝐶, such that (a) the
merges are (1 + 𝜖)-good with respect to 𝐺𝐶and (b) the merges
only involve merging active vertices of 𝐺𝐶. Then, the merges
𝑚1, . . . ,𝑚𝑘are also (1 + 𝜖)-good with respect to 𝐺𝑖.
Lemma 6. Let 𝜖≥0, and 𝐺= (𝑉, 𝐸,𝑤) be a graph obtained by
performing a sequence of (1 + 𝜖)-good merges. Consider 𝐶⊆𝑉.
Let𝑚1, . . . ,𝑚𝑘be a sequence of (1+𝜖)-good merges with respect
to 𝐺, such that each merge involves two vertices of 𝐶(or vertices
created by merging them). Moreover, let Let 𝑚′
1, . . . ,𝑚′
𝑙be a
3Somewhat counter-intuitively edges of lower goodness are better. Renaming
goodness to badness would be more intuitive, but would also sound negative.
6

Algorithm 3 Flatten(𝐷,𝑡)
Input: A dendrogram 𝐷and a linkage similarity threshold 𝑡
Output: A flat clustering
1: C := {}
2: for 𝑑∈set of nodes of 𝐷do
3:
if the linkage similarity of 𝑑≥𝑡and each ancestor of
𝑑has linkage similarity < 𝑡then
4:
Add the cluster corresponding to 𝑑to C
5: return C
sequence of (1+𝜖)-good merges with respect to 𝐺, each involving
two vertices of 𝑉\ 𝐶(or vertices created by merging them).
Then, any interleave of these sequences, that is any sequence
of length 𝑘+ 𝑙which contains both sequences as subsequences,
is a sequence of (1 + 𝜖)-good merges with respect to 𝐺
Proof. Whether a merge of 𝑢and 𝑣is good depends only
on 𝑤(𝑢𝑣), M(𝑢), M(𝑣), 𝑤max(𝑢), 𝑤max(𝑣). Among these, only
𝑤max(𝑢) and 𝑤max(𝑣) can change due to merges not involving
vertices of 𝐷. However, by Lemma 1, these values only decrease
as a result of merges. By Definition 2, the lemma follows.
□
This lets us show the correctness of the TeraHAC algorithm
for any partitioning method for the case when 𝑡= 0.
Lemma 7. Let 𝐷be a dendrogram computed by Algorithm 1
for 𝑡= 0 and any partitioning method used in line 2. Then, 𝐷is
a (1 + 𝜖)-approximate dendrogram.
Proof. By Lemma 4 it suffices to show that 𝐷is obtained by
performing a sequence of (1 +𝜖)-good merges. Consider a sin-
gle iteration of Algorithm 1. Denote by 𝐺the current graph in
the beginning of the iteration. The iteration begins by comput-
ing a clustering 𝐶1, . . . ,𝐶𝑘of 𝐺. Then, it runs SubgraphHAC
separately in each cluster. Hence, for each cluster 𝐶𝑖we obtain
a sequence of (1 + 𝜖)-good merges with respect to 𝐺, which
we denote by 𝑀𝑖. The resulting dendrogram can be updated
with each of these sequences of merges in parallel, since each
of these sequences affects a disjoint set of vertices of 𝐺.
We now use Lemma 7 to show that any interleave of the
sequences 𝑀1, . . . , 𝑀𝑘is a sequence of (1 + 𝜖)-good merges.
This can be done by using a simple induction. For 𝑘= 1 the
claim follows immediately, as the only sequence that can be
obtained by interleaving sequences from the set {𝑀1} is 𝑀1
itself. Now assume that for 𝑖≥1 we know that any interleave
of sequences 𝑀1, . . . , 𝑀𝑖is consists of (1 + 𝜖)-good merges.
Consider any interleave ˜𝑀of 𝑀1, . . . , 𝑀𝑖+1. Clearly ˜𝑀can be
obtained by interleaving some interleave of ˜𝑀𝑖of 𝑀1, . . . , 𝑀𝑖
with 𝑀𝑖+1. We now use Lemma 7 applied to sequences ˜𝑀𝑖
and 𝑀𝑖+1. The former consists of (1 + 𝜖)-good merges by the
inductive assumption and the second one by the correctness
of SubgraphHAC. To apply the lemma we set 𝐶= Ð𝑖
𝑗=1 𝐶𝑖.
Hence, we get that ˜𝑀consists of (1 + 𝜖)-good merges.
□
4.1
Flattening the Dendrogram
While TeraHAC computes a dendrogram, numerous clustering
applications require the algorithm to compute a flat cluster-
ing, that is a partition of the input graph vertices. A single
dendrogram induces multiple flat clusterings of varying scales.
We now show how to flatten the dendrogram, i.e., compute a
canonical collection of flat clusterings consistent with it.
Let us assume that each internal node of the dendrogram
is assigned the linkage similarity that was used to compute
the corresponding cluster. Let us also assume that the linkage
similarity of each leaf node is infinite. The algorithm that we
use for flattening a dendrogram is given as Algorithm 3. Given
a threshold 𝑡, it picks each dendrogram node (cluster) which
satisfies two conditions: (i) the linkage similarity of the node is
at least 𝑡, and (ii) the linkage similarities of all ancestor nodes
of 𝑡are below 𝑡. It is easy to see that this way we compute a
flat clustering, that is each node is in exactly one cluster.
In a dendrogram returned by an exact HAC algorithm (i.e., a
1-approximate one) the linkage similarites of the nodes along
each leaf-to-root path form a nonincreasing sequence. Hence,
flattening a dendrogram using a threshold 𝑡is equivalent to
performing exactly the subset of merges described by the
dendrogram whose linkage similarities are at least 𝑡.
This is not necessarily the case for (1 + 𝜖)-approximate
dendrograms. However, we can still show that the linkage
similarities along each leaf-to-root path are almost monotone,
as shown in the following lemma.
Lemma 8. Let 𝜖≥0 and 𝐷be a dendrogram obtained by
performing (1 + 𝜖)-good merges. Assume we use Algorithm 3 to
flatten 𝐷using parameter 𝑡. Then, for each returned cluster the
minimum linkage similarity of any merge used to create it is at
least 𝑡/(1 + 𝜖).
Proof. Fix a cluster returned by Algorithm 3. If the clus-
ter has size 1, the lemma is trivial. Otherwise, the cluster is
obtained by merging two nodes 𝑥and 𝑦. Since the merge is
(1 + 𝜖)-good, at the point when the merge was performed
max(𝑤max(𝑥),𝑤max(𝑦))
min(M(𝑥), M(𝑦),𝑤(𝑥𝑦)) ≤1 + 𝜖
where min(M(𝑥), M(𝑦),𝑤(𝑥𝑦)) = M(𝑥∪𝑦) is exactly the
minimum merge similarity of any merge used to build the
cluster. Moreover, Algorithm 3 ensures 𝑤(𝑥𝑦) ≥𝑡. We have
M(𝑥∪𝑦) ≥max(𝑤max(𝑥),𝑤max(𝑦))/(1 + 𝜖)
≥𝑤(𝑥𝑦)/(1 + 𝜖) ≥𝑡/(1 + 𝜖)
□
Vertex Pruning Optimization. We now analyze the vertex
pruning optimization (line 7 of Algorithm 1), which, after each
round removes vertices whose highest incident edge weight is
below 𝑡/(1 + 𝜖). We show that vertex pruning with parameter
𝑡′ does not affect the final result, as long as we flatten the
resulting dendrogram using a threshold 𝑡≥𝑡′. At the same
time, as we show in our empirical evaluation, vertex pruning
brings significant performance benefits.
Lemma 9. Let 𝐺= (𝑉, 𝐸,𝑤) be a graph and 𝜖≥0. Consider a
run of Algorithm 1 with a threshold parameter 𝑡′ followed by
flattening the resulting dendrogram using a threshold 𝑡. Then,
the output of the algorithm is the same for any 𝑡′ ∈[0,𝑡].
7

1
KVTable<VertexId, DendrogramNode> TeraHAC(
2
KVTable<VertexId, Vertex> graph, double eps,
3
double t) {
4
5
// Initialize vertex metadata: set the cluster size to
6
// 1, and the min_merge_similarity to infinity.
7
graph = Initialize(graph);
8
9
// Number of edges of weight at least t.
10
int64 num_edges = NumberOfHeavyEdges(graph, t);
11
12
// Stores the dendrogram nodes computed in each round
13
// of the while loop below.
14
std::vector<KVTable<VertexId, DendrogramNode>>
15
dendrogram_nodes;
16
17
while(num_edges > 0) {
18
// Partition the graph into clusters of <= 10M edges.
19
// Computes a cluster id of each vertex.
20
KVTable<VertexId, ClusterId> cluster_ids =
21
AffinityClustering(graph);
22
23
// Join graph and cluster_ids on matching VertexId,
24
// and key vertices by the cluster id.
25
KVTable<ClusterId,
26
pair<VertexId, Vertex>> clusters =
27
KeyByClusterId(graph, cluster_ids);
28
29
KVTable<VertexId, Dendrogram> new_nodes;
30
KVTable<VertexId, ClusterId> sh_cluster_ids;
31
KVTable<ClusterId, double> min_merge;
32
33
// Run SubgraphHAC within each cluster.
34
(new_nodes, sh_cluster_ids, min_merge) =
35
SubgraphHac(clusters.GroupByKey(), eps);
36
dendrogram_nodes.push_back(new_nodes);
37
38
// Vertex pruning: remove nodes, whose max incident
39
// edge weight is below t/(1+eps).
40
graph = graph.Apply(Prune(graph, t/(1+eps)))
41
// Contract each cluster to a node.
42
.Apply(Contract(graph, sh_cluster_ids))
43
// Join the graph with min_merge on
44
//matching keys.
45
.Apply(SetMinMerge(min_merge))
46
// Remove each vertex with no edges.
47
.Apply(RemoveIsolatedVertices());
48
49
num_edges = NumberOfHeavyEdges(graph, t);
50
}
51
52
return MergeDendrograms(dendrogram_nodes);
53
}
Figure 5: TeraHAC pseudocode
We acknowledge that the fact that TeraHAC requires the
pruning optimization (which limits the output to the "bottom"
part of the dendrogram) to achieve the best performance is a
limitation of the algorithm. At the same time, this limitation
is also present in the existing methods providing similar scala-
bility, that is affinity clustering [7] and SCC [40]. We note that
on the datasets we studied in Section 6 using this optimization
makes very little impact on the clustering quality.
5
TeraHAC IMPLEMENTATION
We have implemented TeraHAC in Flume-C++ [2], an opti-
mized distributed data-flow system similar to Apache Beam [3].
A C++-based pseudocode for TeraHAC is given in Fig. 5. A
KVTable<K, V> is a distributed collection of key-value pairs,
where the keys have type K and values have type V [15]. In par-
ticular, the input graph to TeraHAC is a KVTable<VertexId,
Vertex>. Here, VertexId is a type used to represent unique
ids of vertices and Vertex represents a vertex and its metadata.
Specifically, each vertex consists of
• the list of its incident edges, where each edge is repre-
sented as a pair of a VertexId specifying the edge end-
point and a floating-point number giving the weight,
• the size of the cluster represented by the vertex (initial-
ized to 1 on Line 7),
• the minimum merge similarity used to create the cluster
represented by the vertex (initialized to ∞on Line 7).
The result of the TeraHAC algorithm is a dendrogram rep-
resented as a KVTable<VertexId, DendrogramNode>. Each
node is represented by DendrogramNode, containing two fields:
the id of the parent node in the dendrogram, and the linkage
similarity of the merge that created the parent node. Both fields
are unset if the node represents a root in the dendrogram.
The algorithm runs in a loop, until the graph has no edges
of weight at least 𝑡. Each round first partitions the graph (see
Line 2 of Algorithm 1). To this end we use the affinity clustering
algorithm of Bateni et al. [7], which works as follows. First,
each vertex marks its highest weight incident edge. Then, we
find connected components spanned by the marked edges,
which define the clusters. Since each cluster computed by
affinity clustering is later processed on a single machine, we
use a size-constrained version of the algorithm, which ensures
that each cluster contains at most 10 million edges [27].
The choice of affinity clustering is motivated by the follow-
ing heuristic argument. Intuitively, the goal is to use partition-
ing in which many edges inducing good merges (Definition 2)
have both endpoints in the same cluster. From a vertex point of
view its highest weight incident edge is most likely to induce
a good merge (if we don’t know its neighbors’ metadata), and
each such edge is an intra-cluster one in affinity clustering
(unless the cluster is split to ensure the size constraint).
SubgraphHAC. Once we have found affinity clusters, we
aggregate the vertices of each cluster on a single machine
and run SubgraphHAC on each cluster. This returns three re-
sults. First, we obtain the set of new dendrogram nodes corre-
sponding to the merges made by the SubgraphHAC algorithm
(new_nodes). Second, SubgraphHAC computes the clustering
(sh_cluster_ids) induced by performing all (1 + 𝜖)-good
merges made in this round. This clustering maps the vertex
ids of the graph in this round to cluster ids. Third, for each
cluster computed by SubgraphHAC, it computes the minimum
merge similarity used to obtain that cluster. For a fixed cluster
returned by SubgraphHAC, the cluster is obtained by merging
together some vertices, which correspond to clusters formed in
the previous rounds of TeraHAC. Hence, to compute the corre-
sponding minimum merge similarity, we take the minimum of
the similarities of the merges used to create the cluster in this
particular SubgraphHAC call, as well as the minimum merge
similarities stored in the vertices in the cluster. SubgraphHAC
processes each cluster on a single machine, so all its three
return values can be easily computed based on the vertices of
the provided cluster, and the merges which were made.
Vertex Pruning and Contraction. To complete the round,
we first apply vertex pruning, and update the graph based
8

0
0.001
0.01
0.05 0.1
0.5
1.0
ϵ
1.0
1.2
1.4
1.6
1.8
Score
ARI
NMI
Empirical Approximation
Purity
0
0.001
0.005 0.01
0.05 0.1
0.5
Weight Threshold (t)
0.86
0.88
0.90
0.92
0.94
Score
ARI
NMI
Purity
0
0.0001
0.001
0.01
0.1
Weight Threshold (t)
10
20
30
40
50
Num Rounds
OK
TW
FS
Figure 6: Quality of TeraHAC𝑡=0
𝜖
run
with varying 𝜖on the Digits dataset
graph constructed using 𝑘= 25 (qual-
ity measures defined in Section 6.1).
Figure 7: Quality of TeraHAC𝑡
𝜖=0.1 run with
varying thresholds 𝑡on the Iris dataset graph
constructed using 𝑘= 25.
Figure 8: Num. rounds of TeraHAC𝑡
𝜖=0.1
run with varying weight threshold (𝑡) on
OK, TW, and FS. Higher thresholds use
fewer rounds.
Table 1: Graph inputs, including the number of vertices (𝑛),
number of directed edges (𝑚), and the average degree (𝑚/𝑛).
We show the statistics of the largest rMAT graph that we use.
Graph Dataset
Num. Vertices
Num. Edges
Avg. Deg.
com-Orkut (OK)
3,072,627
234,370,166
76.2
Twitter (TW)
41,652,231
2,405,026,092
57.7
Friendster (FS)
65,608,366
3,612,134,270
55.0
rMAT-28 (RM28)
268,435,456
25,814,014,562
96.3
ClueWeb (CW)
978,408,098
74,744,358,622
76.3
Hyperlink (HL)
3,563,602,789
225,840,663,232
63.3
Web-Query (WQ)
31,764,801,992
8,670,265,361,938
272.9
on the result of SubgraphHAC. The key operation is to apply
all the merges performed by SubgraphHAC in this round. We
note that this is equivalent to contracting each cluster com-
puted by SubgraphHAC to a single vertex. This is what the
Contract function does. Assume that the vertex to cluster id
mapping is given by a function 𝐶. Contract first remaps each
vertex id 𝑥to 𝐶(𝑥), both in the keys of the graph KVTable
and in the adjacency lists stored in Vertex objects. This is
achieved by two joins of graph with cluster_ids. Once all
ids are remapped, vertices from the same cluster share the
same KVTable key, and so we group them by key and merge
together to obtain the final vertices. Hence, the vertex ids in the
new graph are the cluster ids computed by SubgraphHAC4.
Next, we update the minimum merge similarity metadata,
by joining the graph with the respective metadata computed by
SubgraphHAC. We finish updating the graph by applying an
optimization: each vertex with no outgoing edges is removed,
as it would clearly not participate in any merge anymore.
Our implementation is optimized to minimize the num-
ber of joins/shuffle steps. In particular, while a standalone
implementation of Prune and SetMinMerge would require a
join/shuffle, in the implementation we actually extended the
joins performed in Contract to perform these operations.
Shared-Memory Implementation. To enable reproducibil-
ity, we also provide a shared-memory implementation of TeraHAC5
Our shared-memory implementation of TeraHAC is written
in the same parallel framework as ParHAC, using the Parlay
libraries for parallel primitives [10] and the CPAM system for
representing dynamic graphs [24]. We provide (1) a faithful
4This means that we use the same type to represent vertex and cluster ids.
However, we decided to make the distinction to improve readability.
5https://github.com/google-research/google-research/tree/master/terahac
implementation of the size-constrained affinity algorithm [27]
used in our distributed implementation, (2) an efficient im-
plementation of SubgraphHAC that runs in 𝑂(𝑚log2 𝑛) time,
and (3) an efficient implementation of weighted graph con-
traction to maintain the edge weights of the contracted graph
after each round of partitioning and SubgraphHAC.
Although optimizing the running time of approximate HAC
in the shared-memory setting was not the main goal of this
paper, we found that our implementation of TeraHAC actu-
ally achieves consistent speedups over ParHAC, even after
optimizing ParHAC to apply the vertex pruning optimizations
used in TeraHAC (see Section 6.2).
6
EMPIRICAL EVALUATION
Graph Data. We list information about graphs used in our
experiments in Table 1. com-DBLP (DB) is a co-authorship
network sourced from the DBLP computer science bibliog-
raphy. 6 YouTube (YT) is a social-network formed by user-
defined groups on the YouTube site. 7 LiveJournal (LJ) is
a directed graph of the social network. 8 com-Orkut (OK)
is an undirected graph of the Orkut social network. Friend-
ster (FS) is an undirected graph describing friendships from
a gaming network. Both graphs are sourced from the SNAP
dataset [37].9 Twitter (TW) is a directed graph of the Twitter
network, where edges represent the follower relationship [35].
10 ClueWeb (CW) is a web graph from the Lemur project at
CMU [12]. 11 Hyperlink (HL) is a hyperlink graph obtained
from the WebDataCommons dataset where nodes represent
web pages [39]. 12 The Web-Query graph consists of vertices
representing 31 billion web queries, with 8.6 trillion edge
weights corresponding to similarities between the queries
(see Section 6.3 for details). Lastly, we run a set of scaling
experiments on synthetic rMAT graphs [14], which are con-
structed using the parameters 𝑎= 0.6,𝑏= 𝑐= 0.15,𝑑= 0.1.
The rMAT parameters were chosen to produce real-world-
like graphs, following [58]. rMAT-𝑋denotes an RMAT graph
6Source: https://snap.stanford.edu/data/com-DBLP.html.
7Source: https://snap.stanford.edu/data/com-Youtube.html.
8Source: https://snap.stanford.edu/data/soc-LiveJournal1.html.
9Source: https://snap.stanford.edu/data/.
10Source: http://law.di.unimi.it/webdata/twitter-2010/.
11Source: https://law.di.unimi.it/webdata/clueweb12/.
12Source: http://webdatacommons.org/hyperlinkgraph/.
9

with 2𝑋nodes and 50 · 2𝑋undirected edges (before removing
duplicate edges).
We note that the large real-world graphs that we study are
not weighted, and so we set the similarity of an edge (𝑢, 𝑣) to
1
log(𝑑𝑒𝑔(𝑢)+𝑑𝑒𝑔(𝑣)) . We use this weighting scheme since it pro-
motes merging low-degree vertices over high-degree vertices
(unless there are sufficiently many edges); the same scheme
was also used by recent work on the topic [25, 26].
Building Similarity Graphs from Pointsets. Our quality
experiments run on graphs built from a pointset by computing
the approximate nearest neighbors (ANN) of each point, and
converting the distances to similarities. We convert distances
to similarities using the formula sim(𝑢, 𝑣) =
1
1+dist(𝑢,𝑣) . We
then scale the similarities by dividing each similarity by the
maximum similarity to ensure that the maximum similarity is
1. The same method of similarity graph construction was also
used in a recent paper on shared-memory parallel HAC [25].
Machine Configuration. Our experiments are performed
in a shared datacenter managed using Borg [57]. Hence, our
jobs compete for resources with other jobs, and run alongside
other jobs on the same machines. We use a maximum of 100
machines to solve all problems and a maximum of 800 hyper-
threads across all machines, unless mentioned otherwise. For
details of the network topology, see [50]. We ran each experi-
ment three times, and find that the difference in running times
across different trials was not significant (within 10%), unless
specified otherwise.
Algorithms. We compare TeraHAC with several HAC base-
lines. We use TeraHAC𝑡=𝑦
𝜖=𝑥to denote TeraHAC run with pa-
rameters 𝜖= 𝑥and weight threshold 𝑡= 𝑦. When not spec-
ified, TeraHAC denotes running the algorithm with 𝜖= 0.1
and 𝑡= 0.01, which are default parameter settings that we
empirically find work well across a wide range of datasets.
Importantly, setting 𝜖= 0 yields the exact HAC algorithm.
We also compare our algorithm with an optimized distributed
implementation (written in the same framework) of the re-
cently developed SCC algorithm [40], which we refer to as
SCC. The SCC algorithm is parameterized by 𝑟, the number
of rounds used (denoted SCC-r). Increasing 𝑟was observed to
increase the quality of the algorithm [40]. We evaluate a lower-
quality (SCC-5), medium-quality (SCC-25) and high-quality
(SCC-100) setting. We also (indirectly) compare our algorithm
with RAC (e.g., in Fig. 2, discussed in Section 1). Although we
did not perform a running time comparison between TeraHAC
and RAC in this paper, TeraHAC using 𝜖= 0 should be strictly
superior to RAC in practice and can be viewed essentially as
an optimized version of the RAC algorithm. This is because
for 𝜖= 0 TeraHAC performs 1-good merges, which are exactly
the merges that the RAC algorithm can perform (see Obser-
vation 1). However, contrary to RAC, TeraHAC may perform
multiple merges involving the same vertex in a single round.
6.1
Quality
We start by understanding the quality of TeraHAC as a func-
tion of both 𝜖and the weight threshold 𝑡. All results in this sec-
tion can be reproduced using our shared-memory implemen-
tation. We compare the quality to known ground-truth cluster-
ings for the iris, wine, digits, and faces classification datasets
from the UCI dataset repository (found in the sklearn.datasets
package). Unfortunately, we are not aware of other large-scale
publicly available datasets providing ground truth clustering
labels. Our goal in this sub-section is to understand the largest
values of 𝜖and 𝑡that can be used without damaging accu-
racy. Note that in our experiments, weights in the graph are
always within [0, 1]. To measure quality, we use the Adjusted
Rand-Index (ARI) and Normalized Mutual Information
(NMI) scores, which are standard measures of the quality of a
clustering with respect to a ground-truth clustering. We also
use the Dendrogram Purity measure [29], which takes on
values between [0, 1] and takes on a value of 1 if and only if the
tree contains the ground truth clustering as a tree consistent
partition (i.e., each class appears as exactly the leaves of some
subtree of the tree). Given a tree 𝑇tree with leaves 𝑉, and
a ground truth partition of 𝑉into 𝐶= {𝐶1, . . . ,𝐶𝑙} classes,
define the purity of a subset 𝑆⊆𝑉with respect to a class 𝐶𝑖
to be P(𝑆,𝐶𝑖) = |𝑆∩𝐶𝑖|/|𝑆|. Then, the purity of 𝑇is
P(𝑇) =
1
|Pairs|
𝑙∑︁
𝑖=1
∑︁
𝑥,𝑦∈𝐶𝑖,𝑥≠𝑦
P(lca𝑇(𝑥,𝑦),𝐶𝑖)
where Pairs = {(𝑥,𝑦) | ∃𝑖s.t. {𝑥,𝑦} ⊆𝐶𝑖} and lca𝑇(𝑥,𝑦) is
the set of leaves of the least common ancestor of 𝑥and 𝑦
in 𝑇. We also study the unsupervised Dasgupta Cost [20]
measure of our dendrograms, which is measured with respect
to an underlying similarity graph 𝐺(𝑉, 𝐸,𝑤) and is defined as:
Í
(𝑢,𝑣)∈𝐸|lca𝑇(𝑢, 𝑣)| ·𝑤(𝑢, 𝑣). The Dasgupta cost is computed
over the complete similarity graph obtained by computing
all point-to-point distances. Lastly, given a dendrogram and
its greedy merge sequence we compute for each merge the
ratio between the highest similarity edge in the graph and the
similarity of the merge, and take the maximum of these ratios
to be the Empirical Approximation Ratio (see Lemma 3). For
a (1 + 𝜖)-approximate algorithm, the empirical approximation
ratio is at most 1 + 𝜖.
Before jumping in, we make a few observations. If we use
a weight threshold of 𝑡= 0, the algorithm will compute a
complete (1 + 𝜖)-HAC dendrogram (similar to ParHAC or
SeqHAC), but as we shall see in Section 6.2, computing the full
(1 + 𝜖)-dendrogram results in the size of the graph shrinking
more slowly, and thus requires more rounds. On the other
hand, using a weight threshold of 𝑡> 0 will result in the
algorithm potentially running much faster due to the size of the
graph shrinking rapidly (as we show in Section 6.2). However,
from a quality perspective, a worry is that in exchange for
speed, we will sacrifice quality. In this sub-section, we will
understand how to set 𝜖and 𝑡, and will demonstrate that this
worry is unfounded. We will show the following key points:
(1) Without thresholding, values of 𝜖∈[0, 0.5] yield results
that are typically within a few percent of the accuracy of
10

Table 2: Adjusted Rand-Index (ARI), Normalized Mutual Information (NMI), Dendrogram Purity, and Dasgupta cost of TeraHAC
(columns 2–5) versus SCC-5, SCC-25, SCC-100 (columns 6–8), the HAC implementation of average linkage from SciPy (column 9),
and the DBSCAN implementation from SciPy (column 10). Note that TeraHAC𝑡=0
𝜖=0 is the same as the RAC algorithm. For DBSCAN, we
perform a grid search for𝜖∈[0.01, 10000] and minpts ∈[2, 128] and use the best scores that we obtain. All graph-based implementations
are run over the similarity graph constructed from an approximate 𝑘-NN graph with 𝑘= 25. The Dasgupta cost is computed over the
complete similarity graph generated from the all-pairs distance graph, and thus takes into account all pairwise similarities. In the
case of Dasgupta cost, lower values are better. For the remaining measures, higher values are better. We display the best quality
score for each graph in green and underlined. We show the relative improvement of each score over the score for TeraHAC𝑡=0
𝜖=0 in
parenthesis after each score.
Dataset
TeraHAC𝑡=0
𝜖=0
TeraHAC𝑡=0.01
𝜖=0
TeraHAC𝑡=0
𝜖=0.1
TeraHAC𝑡=0.01
𝜖=0.1
SCC-5
SCC-25
SCC-100
Sci-Avg
DBSCAN
ARI
iris
0.92 (1x)
0.92 (1x)
0.92 (1x)
0.92 (1x)
0.74 (.80x) 0.79 (.86x) 0.87 (.94x)
0.75 (.82x)
0.56 (.62x)
wine
0.37 (1x)
0.37 (1x)
0.37 (1x)
0.37 (1x)
0.30 (.81x) 0.22 (.61x) 0.24 (.64x)
0.35 (.94x)
0.39 (1.05x)
digits
0.88 (1x)
0.88 (1x)
0.87 (.99x)
0.85 (.97x)
0.87 (.99x) 0.83 (0.94x) 0.82 (.93x)
0.69 (.78x)
0.31 (.36x)
faces
0.57 (1x)
0.57 (1x)
0.56 (.98x)
0.56 (.98x)
0.39 (.68x) 0.51 (.89x) 0.55 (.96x)
0.52 (.92x)
0.098 (.17x)
NMI
iris
0.89 (1x)
0.89 (1x)
0.89 (1x)
0.89 (1x)
0.75 (.84x) 0.77 (.86x) 0.84 (.94x)
0.80 (.89x)
0.73 (.82x)
wine
0.42 (1x)
0.41 (1x)
0.42 (1x)
0.42 (1x)
0.37 (.88x) 0.38 (.90x) 0.38 (.90x)
0.42 (1x)
0.45 (1.07x)
digits
0.90 (1x)
0.90(1x)
0.89 (.98x)
0.89 (.98x)
0.90 (1x)
0.87 (.96x) 0.87 (.96x)
0.83 (.92x)
0.66 (.73x)
faces
0.86 (1x)
0.86 (1x)
0.86 (1x)
0.86 (1x)
0.83
0.85
0.86 (1x)
0.86 (1x)
0.68
Purity
iris
0.94 (1x)
0.94 (1x)
0.95 (1.01x)
0.95 (1.01x)
–
–
–
0.86 (.90x)
–
wine
0.62 (1x)
0.60 (.96x)
0.62 (1x)
0.62 (1x)
–
–
–
0.62 (1x)
–
digits
0.88 (1x)
0.87 (.98x)
0.87 (.98x)
0.85 (.96x)
–
–
–
0.75 (.85x)
–
faces
0.61 (1x)
0.61 (1x)
0.58 (.95x)
0.58 (.95x)
–
–
–
0.62 (1.01x)
–
Dasgupta
iris
321290 (1x)
321290 (1x)
320846 (1x)
320846 (1x)
–
–
–
310957 (1.03x)
–
wine
26904 (1x)
28581 (.94x)
26902 (1x)
26902 (1x)
–
–
–
27324 (.98x)
–
digits
243191685 (1x) 243087881 (1x) 243323801 (.99x) 245791675 (.98x)
–
–
–
240476750 (1.01x)
–
faces
4631561 (1x)
4631561 (1x)
4627286 (1x)
4627286 (1x)
–
–
–
4569916 (1.01x)
–
the exact algorithm. Based on our results, we choose a
value of 𝜖= 0.1 for our experiments.
(2) Fixing 𝜖= 0.1, values of 𝑡≤0.01 reliably achieve very
high quality on our datasets with ground-truth.
Tuning 𝜖. We ran TeraHAC with varying values of 𝜖with-
out thresholding to understand what value of 𝜖is sufficient
for achieving good quality relative to the exact baseline of
𝜖= 0. The experiment runs TeraHAC on similarity graphs
constructed from a 𝑘-NN graph of the underlying pointset,
with 𝑘= 25, as described earlier in Section 6, and consistent
with prior work [25, 26]. We use all 4 UCI datasets mentioned
earlier; due to space constraints we show a representative
result for the digits dataset in Fig. 6 for all quality measures.
Perhaps surprisingly, Fig. 6 shows that the quality measures
do not degrade significantly as 𝜖is increased. We noticed sim-
ilar trends for other values of 𝑘; the reason is likely due to
the fact that in our experiments, SubgraphHAC makes very
similar merges to 𝜖= 0 even when using larger values of 𝜖.
We select the value 𝜖= 0.1 to be consistent with the choice
of 𝜖in prior papers on approximate HAC [25, 26]. Across all
datasets, we found that for 𝜖≤0.1 on average the outputs
are within 1.3% of the best ARI score, within 0.25% of the best
NMI score, within 2.6% of the best Purity score, and within
1.6% of the best Dasgupta score. Fig. 6 empirically confirms
our theoretical result that the approximation factor is always
within a factor of (1 + 𝜖). For the remainder of the paper, we
use a value of 𝜖= 0.1 unless mentioned otherwise.
Tuning the Threshold (𝑡) with 𝜖= 0.1. Computing the
complete dendrogram can be unnecessary if the flat clustering
we seek uses a large threshold (i.e., does not merge edges
with similarity below 𝑡). As Fig. 8 illustrates, larger thresholds
require fewer rounds to compute, and can thus significantly
improve the running time. To understand how to select an
appropriate threshold, 𝑡, we run TeraHAC with varying values
of 𝑡, while fixing 𝜖= 0.1. Fig. 7 shows a representative result
on the iris dataset, showing essentially no difference across
𝑡∈[0, 0.1]. We tuned the threshold and find that for 𝑡= 0.01,
across all datasets, the accuracy results are very close to that
of 𝑡= 0; on average within 0.4% of the ARI score of 𝑡= 0,
within 2.6% of the Purity score of 𝑡= 0, and within 1.6% of
the Dasgupta cost of 𝑡= 0. We find that 𝑡= 0.01 achieves the
same NMI as 𝑡= 0. We set 𝑡= 0.01 in the remainder of the
paper.
Quality Comparison. Table 2 shows the results of our quality
study when comparing TeraHAC with different settings of 𝜖
and 𝑡with the SCC algorithm. The non-zero threshold we use
(𝑡= 0.01) is tuned as described above. The SCC algorithm can
vary the number of rounds, 𝑟, that is uses, which results in 𝑟
flat clusterings. We compute the quality of SCC by evaluating
every flat clustering it produces and using the best score across
any clustering (i.e., for 𝑟= 25 we evaluate all 25 flat cluster-
ings). We also compare with the exact 𝑂(𝑛2) time average-
linkage metric HAC implementation from sklearn [54] as well
as the DBSCAN implementation from sklearn [53]. Both algo-
rithms compute the full distance matrix and thus run in 𝑂(𝑛2)
time. For DBSCAN, we set 𝜖and minpts by searching over the
11

12
14
16
18
20
22
24
26
28
RMAT Scale
0
50000
100000
150000
200000
Running Time in Seconds
SCC-5
SCC-25
SCC-100
TeraHAC
OK
TW
FS
CW
HL
100
101
102
Relative performance
6.3e+03
1.04e+04
1.05e+04
2.88e+04
6.12e+04
3.76e+03
6.66e+03
8.1e+03
2.29e+04
3.35e+04
1.2e+04
2.3e+04
2.99e+04
6.56e+04
6.04e+04
3.93e+04
8.71e+04
1.11e+05
2.35e+05
x
Values on top of bars are running times (in seconds)
TeraHAC
SCC-5
SCC-25
SCC-100
0
2
4
6
8
10
Round Number
10−2
10−1
100
Fraction of Edges (Nodes) Remaining
Edges
Nodes
Figure 9: Running time in seconds
of TeraHAC𝑡=0.01
𝜖=0.1 and SCC using low,
medium, and high-quality settings
(𝑟= {5, 25, 100},𝑡= 0.01) on rMAT
graphs of varying scales.
Figure 10: Relative performance (speedup
over the fastest algorithm for each graph) of
TeraHAC𝑡=0.01
𝜖=0.1 and SCC using low, medium, and
high-quality settings (𝑟= {5, 25, 100},𝑡= 0.01)
on real-world social and Web graphs.
Figure 11: Decrease in number of edges
and nodes as a function of the round
number in TeraHAC𝑡=0.01
𝜖=0.1 for rMAT-28.
DB
YT
RD
LJ
OK
TW
FS
100
101
102
103
104
Relative performance
6.8
96
230
1.2e+03
4.18e+03
x
x
1.12
3.44
11.2
18.4
32.2
623
951
0.74
1.52
6.08
10.8
23.5
337
423
0.14
0.53
5.42
4.77
22.5
132
246
Values on top of bars are running times (in seconds)
SeqHAC
ParHAC
ParHAC-Prune
TeraHAC
Figure 12: Relative performance of TeraHAC vs. ParHAC and
SeqHAC, all using 𝜖= 0.1. ParHAC is the original algorithm, and
ParHAC-Prune is the original algorithm using vertex pruning.
range 𝜖∈[0.01, 10000] and minpts ∈[2, 128]. We consider
four settings for TeraHAC; 𝜖= 0,𝑡= 0, which yields an exact
HAC dendrogram that is equivalent to the RAC algorithm;
𝜖= 0,𝑡= 0.01; 𝜖= 0.1,𝑡= 0; and lastly, using 𝜖= 0.1,𝑡= 0.01.
All settings of TeraHAC achieve high-quality results across all
of the quality measures that we evaluate, across all datasets.
Our results in Table 2 show that TeraHAC achieves quality
within a few percentage points of the best result on all datasets:
we find that for 𝜖≤0.1 on average the outputs are within
1.3% of the best ARI score, within 0.25% of the best NMI score,
within 2.6% of the best Purity score, and within 1.6% of the best
Dasgupta score. The results for the exact parameter settings
of TeraHAC show that thresholding using 𝑡= 0.01 has little
effect. We also observe that SCC is similarly affected, and this
algorithm, which is the prior state-of-the-art in scalable HAC,
also requires careful selection of 𝑡. SCC generally improves
in quality as we increase the number of compression rounds,
with the exception of the digits dataset, where 𝑟= 5 achieves
good quality. Comparing SCC to TeraHAC, TeraHAC𝑡=0.01
𝜖=0.1 is
superior to SCC with the exception of the digits dataset, where
SCC-5 achieves 0.4% better ARI and 1.2% better NMI. Based
on our results, we classify 𝑟= {5, 25, 100} as low, medium, and
high-quality settings of SCC, respectively. Lastly, compared to
DBSCAN, for these datasets, HAC-based algorithms typically
yield much higher quality. However, on the Wine dataset,
DBSCAN finds clusterings that have up to 5.6% higher ARI
and 6.5% higher NMI than TeraHAC (and also exact HAC).
6.2
Scalability
Thus far, we have seen that the quality of TeraHAC with 𝜖=
0.1,𝑡= 0.01 is close to that of an exact HAC baseline. Next we
evaluate the scalability of our algorithm under these settings.
Scalability on Large Graphs. We study the scalability of
TeraHAC compared with SCC on the rMAT graph family de-
scribed at the start of this section. The average-degree of these
graphs is close to 100, and is thus similar to our other real-
world graphs (see Table 1). Fig. 9 shows the result of our ex-
periments. We observe that the running time for TeraHAC
lies between that of SCC-5 and SCC-25. For the largest rMAT
graph that we study, which contains 268 million vertices and
25.8 billion edges, TeraHAC is 11.4x faster than SCC-100, 2.23x
faster than SCC-25, and only 1.45x slower than SCC-5.
Fig. 10 shows the scalability of TeraHAC compared with
SCC on five large real-world graphs, including the largest pub-
licly available graph, the WebDataCommons Hyperlink graph
(HL). Similar to our results on the rMAT graph family, we
find that TeraHAC performs between SCC-5 (0.67x as fast on
average) and SCC-25 (2.04x as fast on average). The algorithm
is significantly faster than SCC-100 (8.3x faster on average),
which is unable to finish within four days on HL.
Taken together with our results in Section 6.1, TeraHAC is
nearly as fast as the low-quality variant of SCC, while achiev-
ing much higher quality than the high-quality variant of SCC.
Good Edges and Round-Complexity. As our results in Fig. 2
in Section 1 show, TeraHAC uses several orders of magnitude
fewer rounds than RAC and ParHAC. To better understand
the reason behind the low round-complexity, we study the
number of good edges (i.e., (1 + 𝜖)-good edges) available to the
algorithm in each round. This measure directly captures edges
that can potentially participate in a merge. Fig. 15 shows a
representative result for the CW graph. The plots for other
graphs are similar. The number of rounds used by TeraHAC
with 𝜖= 0.1 is significantly lower than TeraHAC with 𝜖= 0.
Initially, the number of good edges with 𝜖= 0.1 is one order
of magnitude larger than with 𝜖= 0. Although TeraHAC with
𝜖= 0 uses more rounds than with 𝜖= 0.1, as shown in Fig. 2,
TeraHAC using 𝜖= 0 still uses many fewer rounds than RAC,
although both algorithms solve HAC exactly.
12

0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
Precision
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
0.225
Recall
TeraHAC
SCC-50
DBSCAN
SCC-5
0
10
20
30
40
50
Round Number
10−8
10−6
10−4
10−2
100
Fraction of Edges Remaining
TeraHAC
SCC-50
SCC-5
0
25
50
75
100
125
150
175
Round Number
101
103
105
107
109
1011
Num Edges Remaining
directed-edges, ϵ = 0.1
directed-edges, ϵ = 0.0
good-edges, ϵ = 0.1
good-edges, ϵ = 0.0
Figure 13: Precision-recall tradeoffs
for TeraHAC and SCC on the large-
scale web-query dataset.
Figure 14: Reduction in the number of edges
for the web-query dataset. We note that the fig-
ure plotting the reduction in the number of
nodes looks almost identical.
Figure 15: Number of good and directed
edges in each round on the CW graph
(𝑛=978M, 𝑚=74.7B). The plots for other
graphs show similar behavior when com-
paring TeraHAC with 𝜖= 0.1 and 𝜖= 0.
Table 3: Median running times on the web-query dataset.
TeraHAC
SCC-50
SCC-5
DBSCAN
1280
2634
690
195
Shared-Memory Performance. TeraHAC has the ability to
significantly reduce the number of rounds required for ap-
proximately solving HAC at a given weight threshold. We
have found that our preliminary experiments with a shared-
memory implementation of the algorithm show that its low
round-complexity also translates into strong shared-memory
performance. We study the scalability of TeraHAC compared
with state-of-the-art single-machine HAC implementations,
namely SeqHAC [26] and ParHAC [25] on a 72-core Dell Pow-
erEdge R930 (with two-way hyper-threading) with 4×2.4GHz
processors and 1TB of main memory. We show the results
of our shared-memory experiment in Fig. 12. TeraHAC is al-
ways as fast or faster than all baselines, ranging from 1.45–8x
speedup over ParHAC and 48.5–185x speedup over SeqHAC.
We also obtain between 1.04–5.28x speedup over a version
of ParHAC using the same vertex pruning optimization as
TeraHAC before processing each bucket. TeraHAC seems to
have significant potential in the shared-memory setting and
we plan to further investigate this direction in future work.
6.3
Large Scale Clustering of Web Queries
Lastly, we study the quality of TeraHAC on a large-scale dataset
of web search queries. We use a graph whose vertices repre-
sent queries, and edges connect queries of similar meaning.
The weight of each edge is computed using a machine-learned
model based on BERT [22]. The graph has about 31 billion
vertices and 8 trillion edges, and resembles the graph used in
the evaluation of the SCC algorithm [40]. We use a maximum
of 48 000 cores across 6 000 machines.
For evaluation, we use a sample of 53 659 pairs of queries.
Each pair is assigned a human-generated binary label specify-
ing whether the two queries likely carry the same intent and
thus should belong to the same cluster. 7104 (about 13%) of
the pairs have positive labels, and the remaining are negative.
In Fig. 13 we show precision and recall with respect to these
labels for TeraHAC, SCC and DBSCAN. We obtained our re-
sults by running TeraHAC with 𝑡= 0.05 and 𝜖= 0.1, and SCC
with 5 or 50 rounds of compression decreasing the weight
threshold down to 0.05. We evaluate different points on the
precision-recall curve by considering different levels of clus-
tering for SCC. For TeraHAC, different points are obtained by
flattening the hierarchical clustering using different flattening
thresholds. The flattening algorithm uses batching to compute
all flattenings simultaneously. Our implementation requires
140 minutes to compute all clusterings we present in Fig. 13.
The implementation of DBSCAN that we use is a natural
adaptation of the DBSCAN algorithm (which in its vanilla
version takes a set of points as an input) to a setting where
the input is a similarity graph. The algorithm takes two pa-
rameters: 𝜖and 𝑚𝑖𝑛𝑃𝑡𝑠. First, the algorithm considers each
vertex a core vertex if it contains at least 𝑚𝑖𝑛𝑃𝑡𝑠incident
edges of weight ≥𝜖. Then, we find connected components
of the subgraph of the input graph consisting of core points
and all edges of weight ≥𝜖between them (using the algo-
rithm described in [36]). These components form core clus-
ters. Next, each non-core vertex which does not have a core
vertex of similarity at least 𝜖forms a singleton cluster. Fi-
nally, for each remaining non-core vertex 𝑣, we assign 𝑣to the
cluster of its most similar core neighbor. In Fig. 13 we show
results for (𝜖,𝑚𝑖𝑛𝑃𝑡𝑠) ∈{(0.97, 128), (0.97, 256), (0.98, 64),
(0.98, 96), (0.98, 128), (0.99, 32), (0.99, 64), (0.99, 128)}.
The median running times for the algorithms are given in
Table 3. We find that TeraHAC achieves the highest recall at
every value of precision in the range that we consider. DB-
SCAN, while significantly faster than both TeraHAC and SCC,
consistently obtains over 2x smaller recall than TeraHAC.
TeraHAC improves in quality over both SCC-5 and SCC-50,
in particular delivering about 20% better recall than SCC-5.
At the same time, it runs about 2x faster than SCC-50 and
about 2x slower than SCC-5. This difference in performance
seems to be explained by reduction in the number of edges
and nodes present in the graph for TeraHAC, as compared
with SCC. Fig. 14 shows the reduction in the number of edges
over the rounds of both algorithms. For example, there are 8.6
trillion edges in this graph, and after 10 rounds, SCC-50 still
has 561 billion edges remaining, whereas TeraHAC only has 41
billion edges remaining, which is 13.4x lower than SCC-50. We
observe a similar reduction in the number of nodes (clusters)
that remain. Both runs start with 31 billion nodes; after 10
13

rounds, 5.4 billion nodes remain for SCC-50, whereas only
799 million nodes remain for TeraHAC, which is 6.7x lower
than SCC-50. Our results show that even on extremely large
real-world graphs, TeraHAC using conservative values of𝜖and
weight threshold achieves excellent scalability relative to state-
of-the-art distributed algorithms, and in fact out-performs
these baselines. Crucially, these performance advantages are
obtained while increasing accuracy at every point along the
precision-recall tradeoff curve.
7
CONCLUSION
In this paper we introduced the TeraHAC algorithm and demon-
strated its high quality and scalability on graphs of up to 8
trillion edges. Our results indicate that TeraHAC may be the
algorithm of choice for clustering large-scale graph datasets.
As a future work, it would be interesting to see whether
we can theoretically bound the number of rounds required
by the TeraHAC algorithm (possibly for a carefully chosen
graph partitioning method). Another open question is whether
TeraHAC could be extended from computing the bottom (high-
similarity) part of the dendrogram to computing the entire
dendrogram. Finally, we believe the notion of (1 + 𝜖)-good
merges may be useful for designing efficient HAC algorithms
in other models, for example in the dynamic setting.
REFERENCES
[1] Amir Abboud, Vincent Cohen-Addad, and Hussein Houdrouge.
Sub-
quadratic high-dimensional hierarchical clustering. In Advances in Neural
Information Processing Systems (NeurIPS), volume 32, 2019.
[2] Tyler Akidau, Slava Chernyak, and Reuven Lax. Streaming systems: the
what, where, when, and how of large-scale data processing. 2018.
[3] Apache Software Foundation. Apache Beam.
[4] Dmitrii Avdiukhin, Sergey Pupyrev, and Grigory Yaroslavtsev. Multi-
dimensional balanced graph partitioning via projected gradient descent.
Proc. VLDB Endow., 12(8):906–919, 2019.
[5] Kevin Aydin, MohammadHossein Bateni, and Vahab Mirrokni. Distributed
balanced partitioning via linear embedding. In Proceedings of the Ninth
ACM International Conference on Web Search and Data Mining, WSDM ’16,
page 387–396, New York, NY, USA, 2016.
[6] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering.
Machine learning, 56:89–113, 2004.
[7] Mohammadhossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, Mo-
hammadTaghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, and Vahab
Mirrokni. Affinity clustering: Hierarchical clustering at scale. In Advances
in Neural Information Processing Systems (NeurIPS), volume 30, 2017.
[8] J-P Benzécri. Construction d’une classification ascendante hiérarchique
par la recherche en chaîne des voisins réciproques. Cahiers de l’analyse
des données, 7(2):209–218, 1982.
[9] Charles-Edmond Bichot and Patrick Siarry. Graph partitioning, 2013.
[10] Guy E. Blelloch, Daniel Anderson, and Laxman Dhulipala. Parlaylib - a
toolkit for parallel algorithms on shared-memory multicore machines. In
ACM Symposium on Parallelism in Algorithms and Architectures (SPAA),
pages 507—-509, 2020.
[11] Charles Blundell and Yee Whye Teh. Bayesian hierarchical community
discovery. In Advances in Neural Information Processing Systems (NeurIPS),
volume 26, 2013.
[12] Paolo Boldi and Sebastiano Vigna. The WebGraph framework I: Compres-
sion techniques. In International World Wide Web Conference (WWW),
pages 595–601, 2004.
[13] Aydın Buluç, Henning Meyerhenke, Ilya Safro, Peter Sanders, and Christian
Schulz. Recent advances in graph partitioning. Springer, 2016.
[14] Deepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. R-MAT:
A recursive model for graph mining. In Proceedings of the 2004 SIAM
International Conference on Data Mining, pages 442–446, 2004.
[15] Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams, Robert
Henry, Robert Bradshaw, and Nathan. Flumejava: Easy, efficient data-
parallel pipelines. In ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI), pages 363–375, 2 Penn Plaza, Suite 701
New York, NY 10121-0701, 2010.
[16] Flavio Chierichetti, Nilesh Dalvi, and Ravi Kumar. Correlation clustering
in MapReduce. In Proceedings of the 20th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, KDD ’14, page 641–650,
New York, NY, USA, 2014.
[17] Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and
Claire Mathieu. Hierarchical clustering: Objective functions and algo-
rithms. J. ACM, 66(4), 2019.
[18] Vincent Cohen-Addad, Silvio Lattanzi, Slobodan Mitrović, Ashkan Norouzi-
Fard, Nikos Parotsidis, and Jakub Tarnawski. Correlation clustering in
constant many parallel rounds. In Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pages 2069–2078, 2021.
[19] Aron Culotta, Pallika Kanani, Robert Hall, Michael Wick, and Andrew
McCallum. Author disambiguation using error-driven machine learning
with a ranking loss function. In Sixth International Workshop on Information
Integration on the Web (IIWeb-07), Vancouver, Canada, 2007.
[20] Sanjoy Dasgupta. A cost function for similarity-based hierarchical cluster-
ing. In ACM Symposium on Theory of Computing (STOC), page 118–127,
New York, NY, USA, 2016.
[21] Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified data process-
ing on large clusters. In OSDI’04: Sixth Symposium on Operating System
Design and Implementation, pages 137–150, San Francisco, CA, 2004.
[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understanding.
In Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 4171–4186, 2018.
[23] Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral
clustering and normalized cuts. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data mining, pages
551–556, 2004.
[24] Laxman Dhulipala, Guy E Blelloch, Yan Gu, and Yihan Sun. Pac-trees:
Supporting parallel and compressed purely-functional collections. In ACM
SIGPLAN Conference on Programming Language Design and Implementation
(PLDI), 2022.
[25] Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab Mirrokni, and Jes-
sica Shi. Hierarchical agglomerative graph clustering in poly-logarithmic
depth.
In 2022 Conference on Neural Information Processing Systems
(NeurIPS).
[26] Laxman Dhulipala, David Eisenstat, Jakub Łącki, Vahab Mirrokni, and
Jessica Shi. Hierarchical agglomerative graph clustering in nearly-linear
time. In International Conference on Machine Learning (ICML), pages 2676–
2686, 2021.
[27] Alessandro Epasto, Andrés Muñoz Medina, Steven Avery, Yijian Bai, Robert
Busa-Fekete, CJ Carey, Ya Gao, David Guthrie, Subham Ghosh, James
Ioannidis, et al. Clustering for private interest-based advertising. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining, pages 2802–2810, 2021.
[28] Ilan Gronau and Shlomo Moran. Optimal implementations of upgma
and other common clustering algorithms. Information Processing Letters,
104(6):205–210, 2007.
[29] Katherine A. Heller and Zoubin Ghahramani. Bayesian hierarchical clus-
tering. In International Conference on Machine Learning (ICML), pages
297–304, 2005.
[30] Guan-Jie Hua, Che-Lun Hung, Chun-Yuan Lin, Fu-Che Wu, Yu-Wei Chan,
and Chuan Yi Tang. MGUPGMA: a fast UPGMA algorithm with multi-
ple graphics processing units using NCCL. Evolutionary Bioinformatics,
13:1176934317734220, 2017.
[31] Chen Jin, Ruoqian Liu, Zhengzhang Chen, William Hendrix, Ankit
Agrawal, and Alok Choudhary. A scalable hierarchical clustering algo-
rithm using spark. In 2015 IEEE First International Conference on Big Data
Computing Service and Applications, pages 418–426, 2015.
[32] Chen Jin, Md Mostofa Ali Patwary, Ankit Agrawal, William Hendrix,
Wei-keng Liao, and Alok Choudhary. DiSC: A distributed single-linkage
hierarchical clustering algorithm using MapReduce. IEEE Transactions on
Big Data, 23:27, 2013.
[33] Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii. A model of
computation for mapreduce. In Proceedings of the twenty-first annual
ACM-SIAM symposium on Discrete Algorithms, pages 938–948, 2010.
[34] Ari Kobren, Nicholas Monath, Akshay Krishnamurthy, and Andrew Mc-
Callum. A hierarchical algorithm for extreme clustering. In Proceedings of
the 23rd ACM SIGKDD international conference on knowledge discovery and
data mining, pages 255–264, 2017.
[35] Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. What is
twitter, a social network or a news media? pages 591–600, 2010.
[36] Jakub Łącki, Vahab Mirrokni, and Michał Włodarczyk. Connected com-
ponents at scale via local contractions. arXiv preprint arXiv:1807.10727,
2018.
14

[37] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network
dataset collection, 2014.
[38] Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert,
Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for
large-scale graph processing. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data, pages 135–146, 2010.
[39] Robert Meusel, Sebastiano Vigna, Oliver Lehmberg, and Christian Bizer.
The graph structure in the web–analyzed on different aggregation levels.
The Journal of Web Science, 1(1):33–47, 2015.
[40] Nicholas Monath, Kumar Avinava Dubey, Guru Guruganesh, Manzil Za-
heer, Amr Ahmed, Andrew McCallum, Gokhan Mergen, Marc Najork, Mert
Terzihan, Bryon Tjanaka, et al. Scalable hierarchical agglomerative clus-
tering. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining, pages 1245–1255, 2021.
[41] Benjamin Moseley, Kefu Lu, Silvio Lattanzi, and Thomas Lavastida. A
framework for parallelizing hierarchical clustering methods. In Machine
Learning and Knowledge Discovery in Databases: European Conference
(ECML PKDD), 2019.
[42] Benjamin Moseley, Sergei Vassilvtiskii, and Yuyan Wang. Hierarchical
clustering in general metric spaces using approximate nearest neighbors.
In Proceedings of The 24th International Conference on Artificial Intelligence
and Statistics, volume 130 of Proceedings of Machine Learning Research,
pages 2440–2448, 2021.
[43] Benjamin Moseley and Joshua R. Wang. Approximation bounds for hier-
archical clustering: Average linkage, bisecting k-means, and local search.
In Advances in Neural Information Processing Systems (NeurIPS), pages
3094–3103, 2017.
[44] Daniel Müllner. Modern hierarchical, agglomerative clustering algorithms.
arXiv preprint arXiv:1109.2378, 2011.
[45] Daniel Müllner. fastcluster: Fast hierarchical, agglomerative clustering
routines for r and python. Journal of Statistical Software, 53:1–18, 2013.
[46] Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clus-
tering: an overview. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 2(1):86–97, 2012.
[47] Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clus-
tering: an overview, II. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 7(6):e1219, 2017.
[48] Mark EJ Newman. Modularity and community structure in networks.
Proceedings of the national academy of sciences, 103(23):8577–8582, 2006.
[49] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analy-
sis and an algorithm. In Advances in Neural Information Processing Systems
(NeurIPS), volume 14, 2001.
[50] Leon Poutievski, Omid Mashayekhi, Joon Ong, Arjun Singh, Mukarram
Tariq, Rui Wang, Jianan Zhang, Virginia Beauregard, Patrick Conner, Steve
Gribble, et al. Jupiter evolving: Transforming google’s datacenter net-
work via optical circuit switches and software-defined networking. In
Proceedings of the ACM SIGCOMM 2022 Conference, pages 66–85, 2022.
[51] Sherif Sakr, Faisal Moeen Orakzai, Ibrahim Abdelaziz, and Zuhair Khayyat.
Large-scale graph processing using Apache Giraph. 2016.
[52] Erich Schubert, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei
Xu. Dbscan revisited, revisited: why and how you should (still) use dbscan.
ACM Transactions on Database Systems (TODS), 42(3):1–21, 2017.
[53] Sklearn Authors. scipy.cluster.DBSCAN. https://scikit-learn.org/stable/
modules/generated/sklearn.cluster.DBSCAN.html.
[54] Sklearn
Authors.
sklearn.cluster.AgglomerativeClustering.
https://scikit-learn.org/stable/modules/generated/sklearn.cluster.
AgglomerativeClustering.html.
[55] T Stefan Van Dongen and Birgitta Winnepenninckx. Multiple upgma and
neighbor-joining trees and the performance of some computer packages.
Mol. Biol. Evol, 13(2):309–313, 1996.
[56] Baris Sumengen, Anand Rajagopalan, Gui Citovsky, David Simcha, Olivier
Bachem, Pradipta Mitra, Sam Blasiak, Mason Liang, and Sanjiv Kumar. Scal-
ing hierarchical agglomerative clustering to billion-sized datasets. arXiv
preprint arXiv:2105.11653, 2021.
[57] Muhammad Tirmazi, Adam Barker, Nan Deng, Md Ehtesam Haque, Zhi-
jing Gene Qin, Steven Hand, Mor Harchol-Balter, and John Wilkes. Borg:
the next generation. In EuroSys’20, Heraklion, Crete, 2020.
[58] Axel Wassington and Sergi Abadal. Bias reduction via cooperative bargain-
ing in synthetic graph dataset generation. arXiv preprint arXiv:2205.13901,
2022.
[59] Grigory Yaroslavtsev and Adithya Vadapalli. Massively parallel algorithms
and hardness for single-linkage clustering under ℓp distances. In Interna-
tional Conference on Machine Learning (ICML), volume 80 of Proceedings of
Machine Learning Research, pages 5596–5605, 2018.
[60] Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, and Julian Shun.
Parchain: A framework for parallel hierarchical agglomerative clustering
using nearest-neighbor chain. Proc. VLDB Endow., 15(2), 2021.
[61] Ying Zhao and George Karypis. Evaluation of hierarchical clustering algo-
rithms for document datasets. In Proceedings of the eleventh international
conference on Information and knowledge management, pages 515–524,
2002.
15

A
MISSING PROOFS
Lemma 1. Let 𝐺1, . . . ,𝐺𝑛be a sequence of graphs, in which
each graph is obtained from the previous one by performing
an arbitrary merge. Let 𝑣be a vertex (cluster) which exists in
𝐺𝑙, . . . ,𝐺𝑟. Let 𝑤𝑖max(𝑣) be the value of 𝑤max(𝑣) in 𝐺𝑖, where
𝑙≤𝑖≤𝑟. Then 𝑤𝑙max(𝑣) ≥𝑤𝑙+1
max(𝑣) ≥. . . ≥𝑤𝑟max(𝑣).
Proof. We prove the lemma by contradiction. Pick the
smallest 𝑗≥𝑙such that 𝑤𝑗
max(𝑣) < 𝑤𝑗+1
max(𝑣). Then, 𝐺𝑗+1 is
obtained from 𝐺𝑗by merging some vertices 𝑥and 𝑦, where
𝑤(𝑣,𝑥∪𝑦) = 𝑤𝑗+1
max(𝑣). By Definition 1 we have 𝑤𝑗+1
max(𝑣) =
𝑤(𝑣,𝑥∪𝑦) ≤max(𝑤(𝑣𝑥),𝑤(𝑣𝑦)) ≤𝑤𝑗
max(𝑣), a contradiction.
□
Observation 1. Let 𝐺𝑖be obtained from 𝐺1 by performing
1-good merges. Then, a merge 𝑢𝑣in 𝐺𝑖is 1-good if and only if
𝑤(𝑢𝑣) = max(𝑤max(𝑢),𝑤max(𝑣)).
Proof. ( =⇒) By the definition of 𝑤max we have 𝑤(𝑢𝑣) ≤
max(𝑤max(𝑢),𝑤max(𝑣)). Moreover, 𝑤(𝑢𝑣) ≥max(𝑤max(𝑢),
𝑤max(𝑣)) since, by the definition of 1-good merge, we have
max(𝑤max(𝑢),𝑤max(𝑣)) = min(M(𝑢), M(𝑣),𝑤(𝑢𝑣)).
( ⇐= ) Since 𝑤(𝑢𝑣) ≤𝑤max(𝑢) and 𝑤(𝑢𝑣) ≤𝑤max(𝑣),
𝑤(𝑢𝑣) = max(𝑤max(𝑢),𝑤max(𝑣)) implies𝑤(𝑢𝑣) = 𝑤max(𝑢) =
𝑤max(𝑣). By Lemma 2, 𝑀(𝑣) ≥𝑤max(𝑣) and 𝑀(𝑣) ≥𝑤max(𝑣).
This implies that the merge is 1-good.
□
Claim 1. We have 𝑤(𝑥𝑦)/min(M(𝑥′), M(𝑥),𝑤(𝑥𝑥′)) =
𝑤(𝑥𝑦)/M(𝑥′ ∪𝑥) ≤1 + 𝜖.
Proof. Consider the sequence of merges that were made to
obtain the dendrogram 𝐷and let𝐺′
1, . . . ,𝐺′𝑛be the correspond-
ing graphs. Assume that the merge of 𝑥′ and 𝑥was done in
𝐺′
𝑖(and resulted in obtaining graph 𝐺′
𝑖+1). Let 𝑤′max(𝑣) be the
value of 𝑤max(𝑣) in graph 𝐺′
𝑖. Graph 𝐺′
𝑖does not necessarily
have a vertex 𝑦, but since we assumed that the merge of 𝑥′
and 𝑥happens before the merge of 𝑦′ and 𝑦, 𝐺′
𝑖contains a set
of vertices {𝑦1, . . . ,𝑦𝑡}, such that Ð𝑡
𝑖=1 𝑦𝑖= 𝑦. By Definition 1
we have that 𝑤(𝑥𝑦) = 𝑤(𝑥, Ð𝑡
𝑖=1 𝑦𝑖) ≤max𝑡
𝑖=1 𝑤(𝑥,𝑦𝑖) ≤
𝑤′max(𝑥), which implies:
𝑤(𝑥𝑦)
min(M(𝑥), M(𝑥′),𝑤(𝑥𝑥′)) ≤max(𝑤′max(𝑥),𝑤′max(𝑥′))
min(M(𝑥), M(𝑥′),𝑤(𝑥𝑥′)) ≤1+𝜖.
Here, the second inequality follows from the fact that the
merge of 𝑥and 𝑥′ in 𝐺′
𝑖is good.
□
Lemma 5. For some 𝐶⊆𝑉, consider the graph 𝐺𝐶and let
𝑚1, . . . ,𝑚𝑘be a sequence of merges in 𝐺𝐶, such that (a) the
merges are (1 + 𝜖)-good with respect to 𝐺𝐶and (b) the merges
only involve merging active vertices of 𝐺𝐶. Then, the merges
𝑚1, . . . ,𝑚𝑘are also (1 + 𝜖)-good with respect to 𝐺𝑖.
Proof. We observe that whether a merge is good only de-
pends on the vertices involved in a merge, their M values and
their incident edges. Hence, the lemma easily follows when
𝑘= 1. Let us now consider the case of 𝑘= 2, the argument can
be easily continued inductively. Let 𝐶1 be the set of vertices
obtained from 𝐶by performing merge 𝑚1, and let 𝐺1 be the
graph obtained from 𝐺by performing merge 𝑚1. It is easy to
see that by performing merge 𝑚1 on 𝐺𝐶we obtain the graph
𝐺𝐶1
1 . That is, the graph that SubgraphHAC sees after applying
the first merge, is the same graph that we would obtain if we
applied the merge on the global graph, and then computed the
input to SubgraphHAC. The lemma follows.
□
Lemma 9. Let 𝐺= (𝑉, 𝐸,𝑤) be a graph and 𝜖≥0. Consider a
run of Algorithm 1 with a threshold parameter 𝑡′ followed by
flattening the resulting dendrogram using a threshold 𝑡. Then,
the output of the algorithm is the same for any 𝑡′ ∈[0,𝑡].
Proof. By Lemma 8, flattening the dendrogram with thresh-
old 𝑡only uses nodes of the dendrogram whose linkage simi-
larity is at least 𝑡/(1+𝜖). Pruning removes vertices whose max-
imum incident edge has weight strictly less than 𝑡′/(1 + 𝜖) ≤
𝑡/(1 + 𝜖). By Definition 1, each merge that such vertex partici-
pates in has merge of similarity below 𝑡/(1 +𝜖). Moreover, the
removal of such vertices does not affect whether a merge of
similarity at least 𝑡/(1 + 𝜖) is good, or does not affect the edge
weights participating in such merges. The lemma follows.
□
B
SubgraphHAC
In this section, we give a near-linear time algorithm for com-
puting a set of (1+𝜖)-good merges by dynamically maintaining
the goodness values of the edges, which we call SubgraphHAC.
The algorithm has the following specification: it performs only
(1 + 𝜖)-good merges, and ensures that upon completion, there
are no (1 + 𝜖′)-good merges for 𝜖′ = 𝑂(𝜖) (see Lemma 13).
Recall that SubgraphHAC is run on the partitions (i.e., sub-
graphs) of the input graph, where vertices assigned to this
partition are called active and the neighbors of these vertices
that are not in this partition are inactive. Furthermore, the
vertices carry their min-merge values from prior rounds, M(𝑣).
In what follows, we use vertex and cluster interchangeably.
The input to SubgraphHAC is therefore a graph where ver-
tices are marked active and inactive, where each vertex 𝑣has
a min-merge value M(𝑣), as well as a corresponding cluster
size. The goal of the algorithm is to merge a subset of the good
edges, and run in near-linear time.
Challenges. Designing a near-linear time algorithm for this
problem is surprisingly non-trivial. One challenge is that the
goodness of a vertex𝑣’s incident edges can change significantly
even without 𝑣participating in merges itself. For example, this
could occur is if a neighbor of 𝑣merges with another vertex,
resulting in the neighboring vertex increasing in size, causing
𝑤𝑢𝑣to decrease. A simple example, e.g., a sequence of 𝑛−1
merges of the degree-1 vertices of a star into the center of
the star illustrates that exact maintenance of goodness values
requires 𝑂(𝑛2) work. Another challenge is that the goodness
of all edges incident to a vertex 𝑣depends on the 𝑤max(𝑢)
values for all neighbors of 𝑣, which in turn depends on the
cluster sizes of their neighbors. Hence, the goodness of an
edge incident to 𝑣may change when a vertex / cluster two
hops away from 𝑣chages its size.
These examples serve to illustrate that attempting to exactly
maintain the goodness values is probably hopeless. One could
ask why bother with goodness—perhaps the algorithm can
simply keep a heap induced only on the active edges and their
weights? The prior near-linear time (1 + 𝜖)-approximate HAC
16

algorithm [26] used such an approach (this was called the
heap-based approach in the paper). However, the heap-based
approach alone as used in SeqHAC is insufficient for our needs,
since SeqHAC cannot merge any edge that has weight smaller
than a factor of (1 + 𝜖) from the current maximum weight in
the graph. Still, our algorithm borrows and builds on some
ideas from the heap-based approach. For example, it indexes a
heap over the vertices, using the lowest (i.e., best) goodness
as the priority for each vertex. It also maintains weights using
partial weights, where an edge to a neighbor 𝑥of a vertex 𝑣is
stored as a partial average-linkage weight, 𝑤(𝑣,𝑥) · |𝑣|, i.e. the
average-linkage weight between vertices (𝑣,𝑥) but multiplied
by 𝑣, the source endpoint’s cluster size. However, many new
ideas are required to efficiently maintain the goodness values
and argue that the algorithm ensures a (1 + 𝜖)-approximation.
Our Approach. We design an algorithm based on a lazy heap-
based approach, which we give some high-level pseudocode
for in Algorithm 4. The algorithm maintains a priority queue
(min heap) over the active vertices, with a vertex’s priority cor-
responding to the goodness of its smallest goodness neighbor.
The algorithm cannot maintain these goodness values exactly,
but instead uses approximate goodness values, ˜𝑔. There are
two sources of approximation that play a role. First, similar
to the SeqHAC algorithm, the algorithm maintains (1 + 𝛼) ap-
proximations of the edge weights, ˜𝑤(𝑢, 𝑣), by “broadcasting” a
vertex’s cluster size to its neighbors and updating the weights
based on the next cluster size. Here, 𝛼≥0 is a parameter of
SubgraphHAC. This broadcast operation is done only when
a vertex’s cluster size increases by a (1 + 𝛼) factor. Second,
it maintains an approximate view of the 𝑤max(𝑣) values for
each vertex 𝑣, ˜𝑤max(𝑣). We assign every active edge (an edge
between two active endpoints) to the endpoint with larger
˜𝑤max value; call this the assigned endpoint for an edge. Each
time the ˜𝑤max value of a vertex changes by a (1 + 𝛼) factor,
the algorithm goes over all assigned edges and reassigns them
to whichever endpoint has higher ˜𝑤max value. This approach
yields good guarantees on (1) the approximation quality of
each edge we merge and (2) termination conditions for the
algorithm (i.e., when it terminates, no edges with goodness
below a given threshold remain). The algorithm itself (Algo-
rithm 4) is relatively simple, with most of the work being done
by the graph representation, described next.
Graph Representation
The graph representation takes as input parameters 𝜖≥0, 𝛼≥
0; 𝜖controls the accuracy and ensures that every merge is (1 +
𝜖)-good; 𝛼controls the frequency with which the algorithm
performs broadcasts, and affects the range of goodness for
which edges are guaranteed to be merged.
The graph representation (1) support queries for the approx-
imate goodness value ˜𝑔(𝑢, 𝑣) of each edge, (2) support queries
for the ˜𝑤max(𝑣) value for each vertex, (3) support efficiently
merging pairs of active vertex, and (4) support querying for
(approximately) the lowest goodness edge incident to a vertex
𝑣. It supports (1–4) above with an overall running time of all
operations over any sequence of merges in ˜𝑂(𝑚+𝑛) time. We
Algorithm 4 SubgraphHAC(𝐺= (𝑉, 𝐸,𝑤, M),𝐴,𝜖, 𝛼)
Input: 𝐺𝐴= (𝑉, 𝐸,𝑤, M), 𝐴⊆𝑉, 𝜖> 0.
Output: A set of merges of vertices in 𝐴
1: Mark vertices of 𝑉\ 𝐴as inactive
2: For each active edge (𝑢, 𝑣) assign it to the endpoint with
higher 𝑤max value.
3: 𝑇:= priority queue on active vertices of 𝐺[𝐴] keyed by
the goodness of their lowest-goodness assigned edge.
4: while 𝑇is not yet empty do
5:
(𝑢, (𝑣,𝑏𝑢𝑣)) := RemoveMin(𝑇)
6:
if 𝑏𝑢𝑣> (1 + 𝜖) then
7:
(𝑢, 𝑣′,𝑏𝑢𝑣′) := min goodness assigned edge of 𝑢
8:
Reinsert (𝑢, (𝑣′,𝑏𝑢𝑣′)) to 𝑇if 𝑏𝑢𝑣′ ≤(1 + 𝜖).
9:
else
10:
Merge 𝑢and 𝑣in 𝐺
11: Return all merges made
can support (4) only for edges with true goodness value in a
given range [1,𝑇], and elaborate more on this in Lemma 13.
Data Structures and Initialization. Each active edge (𝑢, 𝑣)
(an edge where both 𝑢and 𝑣are active) is initially assigned to
whichever of its endpoints has larger ˜𝑤max value. Let 𝐴(𝑢, 𝑣)
be the endpoint that the edge (𝑢, 𝑣) is assigned to. Initially,
the ˜𝑤max values are exact, and can be computed by scanning
all the edges. For each active 𝑣∈𝑉, we store all edges as-
signed to the vertex in a priority queue 𝐴(𝑣), which is keyed
by the approximate goodness of the edge (from smallest to
largest). Initially, the goodness value for an assigned edge
(𝑢, 𝑣) is ˜𝑔(𝑢, 𝑣). We maintain for each active vertex 𝑣its full
neighborhood, including both active and inactive neighbors.
𝑁(𝑣) stores the weights of all of its incident edges using the
one-sided partial weight representation: for each (𝑢, 𝑣) edge
incident to𝑢, we store𝑤(𝑢, 𝑣)·|𝑢|, i.e., the true average-linkage
weight, 𝑤(𝑢, 𝑣) multiplied by the cluster size of 𝑢. The neigh-
borhood is stored as a sparse set, e.g., using a hashtable. Each
active vertex 𝑢stores two quantities: (a) the initial cluster size
of the vertex and (b) the approximate 𝑤max value of the vertex.
The stored values are updated whenever the cluster size of
the vertex increases sufficiently, or whenever the 𝑤max value
changes sufficiently, as we discuss shortly. Lastly, we maintain
a list of edges 𝑅to be reassigned which is cleared at the end of
each merge.
Invariants. The invariants are motivated by the following
two broadcast operations, which update information along all
neighboring edges whenever a vertex’s cluster size or ˜𝑤max
value change by a (1+𝛼) factor. The first type of broadcast up-
dates a node’s cluster size in all of its neighbor’s data structures
after its cluster size increases by a (1 + 𝛼)-factor. The second
type of broadcast is based on the ˜𝑤max values: if the ˜𝑤max(𝑢)
drops by a (1 + 𝛼) factor, we reassign all edges assigned to 𝑢.
We first state a useful invariant about the approximate
weights, ˜𝑤. Before any merges are performed, the weights
are exact. There may be three things affecting 𝑤(𝑢, 𝑣): the
cut-size changing, or the cluster size of either 𝑢or 𝑣changing.
Changes of the first type are easy to handle, and can update
the weight of the edge to be exact. Changing the cluster size
17

of the endpoint to which the edge is assigned to (say 𝑢) is
also easy, since the weight is implicitly normalized by |𝑢|. So
the only error comes from changes to the neighbor cluster
size, 𝑣, but since we broadcast and make the incident edge
weights exact after a cluster grows in size by a (1 + 𝛼) factor,
the edge weight can be at most a (1 + 𝛼) factor larger than the
true weight. This guarantee for the approximate weights, ˜𝑤,
provided by the broadcasting procedure is summarized by the
next invariant:
Invariant 1. For every edge (𝑢, 𝑣) ∈𝐸,𝑤(𝑢, 𝑣) ≤˜𝑤(𝑢, 𝑣) ≤
𝑤(𝑢, 𝑣)(1 + 𝛼).
In other words, the approximate weights maintained for each
edge are an upper-bound on the true edge weight. Note that
we can compute the exact weight of any edge in 𝑂(1) at any
given time since the cut weight, and cluster sizes are exact;
we need the approximate weights and Invariant 1 only when
computing 𝑤max(𝑢), since we cannot afford to scan over all
incident edges to 𝑢to compute 𝑤max exactly.
Let ¥𝑔(𝑢, 𝑣) be the stored goodness of the edge (𝑢, 𝑣), i.e. the
stored goodness value in the priority queue 𝐴(𝑢) if (𝑢, 𝑣) is
assigned to 𝑢. At initialization, ¥𝑔= ˜𝑔= 𝑔. However, as merges
start to occur in the graph, the stored goodness values, ¥𝑔may
become approximate. Understanding the stored goodness val-
ues is necessary, since when scanning the assigned edges of a
vertex to find a potentially mergeable edge, we use the stored
goodness values, ¥𝑔. The next invariant summarizes the rela-
tionship between the ¥𝑔values and the exact goodness at any
point in time in the algorithm (we provide the proof that the
invariant is maintained in Lemma 12).
Invariant 2. For every edge (𝑢, 𝑣) ∈𝐸, ¥𝑔(𝑢, 𝑣) ≤𝑔(𝑢, 𝑣)(1+𝛼)2
So at any point in time during the algorithm, if the goodness
value of an edge is small, then the stored goodness value of the
edge is also small. Again, we emphasize the difference between
the three types of goodness values:𝑔is the true goodness value
of an edge; ˜𝑔is a (1+𝛼) over-approximation of the 𝑔value that
we can compute at any instant; finally, ¥𝑔is the goodness value
that is stored in the priority queue of the assigned endpoint.
We further note that the invariant says nothing about a lower
bound on ¥𝑔(𝑢, 𝑣) in terms of 𝑔(𝑢, 𝑣). For example, it could be
the case that 𝑔(𝑢, 𝑣) ≫(1 + 𝜖) while ¥𝑔(𝑢, 𝑣) ≤(1 + 𝜖), but this
is not an issue, as we can amortize the cost of checking the
edge against the increase in goodness for the edge.
Initially, Invariant 1 is satisfied since the weights are ex-
act. Lastly, Invariant 2 is satisfied initially since the goodness
values are exact.
Useful Lemmas. Next, we state some useful lemmas about
the approximation error of computing ˜𝑤max and ˜𝑔at a given
point in time, based on the invariants above.
Lemma 10. ∀𝑢∈𝑉, 𝑤max(𝑢) ≤˜𝑤max(𝑢) ≤𝑤max(𝑢)(1 + 𝛼).
In other words, the approximate ˜𝑤max values s a (1 + 𝛼) over-
approximation of the true 𝑤max values.
Proof. Since we store a map indexed on the approximate
partial weights of the neighbors and sorted by the approxi-
mate edge weights ˜𝑤, we can simply use the largest ˜𝑤of an
incident edge to be ˜𝑤max(𝑢). The returned weight is a (1 + 𝛼)
approximation to 𝑤max by Invariant 1.
□
Using Lemma 10, we obtain guarantees on calculating a
particular edge’s goodness value. For the calculation, we use
the exact value of the edge weight, but suffer an approximation
in the numerator; when computing max( ˜𝑤max(𝑢), ˜𝑤max(𝑣)),
the value can be at most a multiplicative (1 + 𝛼) factor larger
than the true value. This is summarized in the next lemma:
Lemma 11. ∀(𝑢, 𝑣) ∈𝐸, 𝑔(𝑢, 𝑣) ≤˜𝑔(𝑢, 𝑣) ≤𝑔(𝑢, 𝑣)(1 + 𝛼).
Proof. The approximate goodness of an edge is𝑈/𝐿where
𝑈= max( ˜𝑤max(𝑢), ˜𝑤max(𝑣)) and 𝐿= min(M(𝑢), M(𝑣),𝑤(𝑢, 𝑣)).
Since 𝑈is a (1 + 𝛼) approximation of the true numerator and
cannot be smaller, and 𝐿is exact, the worst case is when 𝑈is a
(1 + 𝛼) approximation. Therefore, we have 𝑔(𝑢, 𝑣) ≤˜𝑔(𝑢, 𝑣) ≤
(1 + 𝛼)𝑔(𝑢, 𝑣).
□
We describe how ˜𝑤and ˜𝑔are calculated, and why the lem-
mas are true in more detail below. Lemma 11 is crucial since
by only merging edges where ˜𝑔(𝑢, 𝑣) ≤(1 + 𝜖), the edge must
truly be (1 + 𝜖)-good.
Graph Representation Operations
Merge(𝑢, 𝑣). We merge the vertex with smaller cluster size
into the vertex with larger cluster size. Suppose we are merg-
ing 𝑢into 𝑣(thus 𝑣will still be active after the merge). Both
𝑢and 𝑣are initially active. Merging the weights can be done
in 𝑂(|𝑁(𝑢)|) time by iterating over edges in 𝑁(𝑢) and updat-
ing weights in 𝑁(𝑣) using the average-linkage formula. After
merging the neighborhoods, we push all edges in 𝐴(𝑢) (edges
assigned to 𝑢) to 𝑅to be fixed at the end of the merge. Finally,
we see if we need to perform a broadcast operation due to ei-
ther the cluster size increasing sufficiently, or the 𝑤max value
decreasing sufficiently:
(1) If 𝑣’s cluster size grows larger by a (1 + 𝛼) factor (from
the last broadcast value), we iterate over all edges in
𝑁(𝑣) and update their weights in both 𝑣and 𝑣’s neigh-
bor’s queues.
(2) If ˜𝑤max(𝑣) either grows or decreases by a (1 +𝛼) factor
from the last value of ˜𝑤max(𝑣) that a broadcast occurred
at, we iterate over all of 𝑣’s edges and reassign them to
the endpoint with larger ˜𝑤max value.
This concludes the description of the merge procedure. We de-
scribe the correctness details (i.e., showing that the invariants
are preserved after this operation) shortly.
BestAssignedNeighbor(𝑢). This operation is used to com-
pute the min goodness assigned neighbor of 𝑢in Algorithm 4.
Similar to 𝑤max, getting the true best assigned neighbor would
require scanning all edges assigned to 𝑢. Instead, we iterate
over every edge in 𝐴(𝑢) in increasing order of the stored good-
ness value, ¥𝑔(𝑢, 𝑣) until we either find an edge whose approxi-
mate goodness ˜𝑔(𝑢, 𝑣) is smaller than (1 + 𝜖), or we exhaust
all edges with ¥𝑔(𝑢, 𝑣) in the range [1, (1 + 𝜖)/(1 + 𝛼)].
Each edge unsuccessfully considered in this range has ¥𝑔(𝑢, 𝑣) ≤
(1+𝜖)/(1+𝛼), but ˜𝑔(𝑢, 𝑣) > 1+𝜖. When analyzing the running
time, this gap helps show that an edge can be unsuccessfully
checked only 𝑂(log𝑛) times.
18

Correctness
Lemma 12. The Merge(𝑢, 𝑣) operation preserves Invariants 1
and 2.
Proof. Suppose that the invariants are true before the
merge operation, and suppose 𝑢merges into 𝑣(i.e. 𝑣remains
active at the end of the merge).
Invariant 1. The only edges whose weights are affected by
the merge are those incident to 𝑣. Let (𝑣,𝑥) be such an edge. If
(𝑢,𝑥) was in 𝐺previously, then the weight of the (𝑣,𝑥) edge
will be exact. Otherwise, Invariant 1 previously held for (𝑣,𝑥).
It is clear that Invariant 1 continues to hold if the cluster size
of 𝑣has not yet grown by a factor of (1+𝛼); on the other hand
if it grows by more than this factor, 𝑣broadcasts, and the (𝑣,𝑥)
weight will be exact.
Invariant 2. Consider an arbitrary (𝑥,𝑦) edge. When the
edge was last assigned, say to 𝑥, ¥𝑔(𝑥,𝑦) = ˜𝑔(𝑥,𝑦) ≤(1 +
𝛼)𝑔(𝑥,𝑦). The invariant can be violated if 𝑔(𝑥,𝑦) decreases,
which can only occur by max(𝑤max(𝑥),𝑤max(𝑦)) decreas-
ing. The algorithm will update the goodness of (𝑥,𝑦) once
max( ˜𝑤max(𝑥), ˜𝑤max(𝑦)) decreases by a (1 + 𝛼) factor. Since
max( ˜𝑤max(𝑥), ˜𝑤max(𝑦)) ≤(1 + 𝛼) max(𝑤max(𝑥),𝑤max(𝑦)),
after max(𝑤max(𝑥),𝑤max(𝑦)) decreases by a (1 + 𝛼) factor,
max( ˜𝑤max(𝑥), ˜𝑤max(𝑦)) must also decrease by a (1 + 𝛼) fac-
tor. At this point, we perform a broadcast and update the
goodness of this edge to ˜𝑔(𝑥,𝑦). The ratio of ¥𝑔(𝑥,𝑦)/𝑔(𝑥,𝑦)
is always upper bounded by (1 + 𝛼)2, and is largest when
initially 𝑔(𝑥,𝑦) = ¥𝑔(𝑥,𝑦)/(1 + 𝛼) and 𝑔(𝑢, 𝑣) decreases by a
(1 + 𝛼) factor.
□
Lemma 13. Once the algorithm terminates, no edges with
goodness (𝑔) in the range [1,𝑇] with 𝑇= (1 + 𝜖)/(1 + 𝛼)3
remain.
Proof. Suppose otherwise. Let this edge be (𝑢, 𝑣) and its
goodness value be 𝑔𝑢𝑣. Suppose (𝑢, 𝑣) is assigned to 𝑢. By
Invariant 2, ¥𝑔(𝑢, 𝑣) ≤𝑔(𝑢, 𝑣)(1 + 𝛼)2. Since 𝑔𝑢𝑣≤𝑇= (1 +
𝜖)/(1+𝛼)3, by Invariant 2, ¥𝑔(𝑢, 𝑣) ≤𝑔𝑢𝑣(1+𝛼) ≤(1+𝜖)/(1+𝛼).
However, this means that the algorithm incorrectly missed an
edge in the range [0, (1 +𝜖)/(1 +𝛼)] for node 𝑢, contradicting
the definition of BestAssignedNeighbor(𝑢).
□
Running Time Analysis
We will show that 𝑂(log𝑛) total broadcasts and reassignment
operations are performed for any vertex over the course of
the algorithm.
Let 𝑎1, . . . ,𝑎𝑘be the sizes of the cluster containing a vertex
𝑎. We have 𝑎1 = 1 and 𝑎𝑘≤𝑛. When a cluster of size 𝑝
merges with a cluster of size 𝑞(𝑝≥𝑞), the ˜𝑤max(𝑝) value can
increase from 𝑤to at most 𝑤((1 + 𝜖)𝑞+ 𝑝)/(𝑝+𝑞), which is a
multiplicative increase of ((1 + 𝜖)𝑞+ 𝑝)/(𝑝+ 𝑞). The increase
is maximized when 𝑝= 𝑞, for an increase of at most (2 + 𝜖)/2.
We would like to show that the overall increase in ˜𝑤max over
any sequence of merges cannot be too large. We will use the
following lemma for the analysis:
Lemma 14. Let 𝑎1 < 𝑎2 < . . . < 𝑎𝑘. Then, Î𝑘−1
𝑖=1 (1 +
𝑡(𝑎𝑖+1−𝑎𝑖)
𝑎𝑖+1
) = 𝑂(𝑎𝑡
𝑘).
Proof. Since 1 + 𝑥≤𝑒𝑥, for any 𝑥1, . . . ,𝑥𝑘we have
𝑘
Ö
𝑖=1
(1 + 𝑥𝑖) ≤𝑒
Í𝑘
𝑖=1 𝑥𝑖.
Thus, to complete the proof it suffices to show that
𝑘−1
∑︁
𝑖=1
𝑡(𝑎𝑖+1 −𝑎𝑖)
𝑎𝑖+1
≤𝑡ln𝑎𝑘+ 𝑂(1),
(1)
and apply Equation B. Observe that
𝑎𝑖+1 −𝑎𝑖
𝑎𝑖+1
=
𝑎𝑖+1−𝑎𝑖−1
∑︁
𝑗=0
1
𝑎𝑖+1
≤
𝑎𝑖+1−𝑎𝑖−1
∑︁
𝑗=0
1
𝑎𝑖+1 −𝑗=
𝑎𝑖+1
∑︁
𝑗=𝑎𝑖+1
1
𝑗,
which implies
𝑘−1
∑︁
𝑖=1
𝑡(𝑎𝑖+1 −𝑎𝑖)
𝑎𝑖+1
≤𝑡
𝑘−1
∑︁
𝑖=1
𝑎𝑖+1
∑︁
𝑗=𝑎𝑖+1
1
𝑗= 𝑡
𝑎𝑘
∑︁
𝑗=1
1
𝑗= 𝑡ln𝑎𝑘+ 𝑂(1).
□
By setting 𝑡= 𝜖, and 𝑎𝑖+1 = 𝑝+𝑞and 𝑎𝑖= 𝑝, 1+ 𝑡(𝑎𝑖+1−𝑎𝑖)
𝑎𝑖+1
=
((1 + 𝜖)𝑞+ 𝑝)/(𝑝+ 𝑞). Therefore, if 𝑎1 < 𝑎2 < . . . < 𝑎𝑘are
the cluster sizes of a vertex 𝑣, then, Î𝑘−1
𝑖=1 (1 + 𝜖(𝑎𝑖+1−𝑎𝑖)
𝑎𝑖+1
) =
𝑂(𝑎𝜖
𝑘) = 𝑂(𝑛𝜖). Therefore, the total increase in ˜𝑤max(𝑣) over
the course of the algorithm is at most 𝑂(𝑛𝜖) Assuming a poly-
nomially bounded aspect ratio, i.e., letting Wmax be the largest
edge weight initially and Wmin be the smallest edge weight
initially, we have that Wmax/Wmin ∈𝑂(poly(𝑛)). Therefore
the overall range for the best values of a vertex over a sequence
of merges is also 𝑂(poly(𝑛)). For any constant 𝛼> 0, the total
number of broadcasts performed by the vertex over its lifetime
is 𝑂(log𝑛). This is summarized in the following lemma:
Lemma 15. For any constants 𝜖> 0, 𝛼> 0, and a graph
with polynomially bounded aspect ratio, the total number of
broadcast operations performed by any vertex over the sequence
of the algorithm is 𝑂(log𝑛).
Next, we show that the total number of times a given edge
(𝑢, 𝑣) in unsuccessfully checked is low. We use the same con-
stants and setting (bounded aspect ratio):
Lemma 16. Each edge (𝑢, 𝑣) is checked unsuccessfully over all
calls to BestAssignedNeighbor(𝑢) at most 𝑂(log𝑛) times.
Proof. Recall that a check of an edge (𝑢, 𝑣) is unsuccess-
ful if ¥𝑔(𝑢, 𝑣) ≤(1 + 𝜖)/(1 + 𝛼) but ˜𝑔(𝑢, 𝑣) > (1 + 𝜖). Since
¥𝑔(𝑢, 𝑣) = ˜𝑔(𝑢, 𝑣) when this edge was assigned to 𝑢, in the
meantime, ˜𝑔(𝑢, 𝑣) grew by at least a (1 + 𝛼) factor. This could
occur by 𝑈= ˜𝑤max(𝑢) growing by a (1 + 𝛼) factor, or 𝐷=
min(M(𝑢), M(𝑣),𝑤(𝑢, 𝑣)) decreasing by a (1 + 𝛼) factor, or
some combination of the two. In either case, either 𝑈must
grow or 𝐷must shrink by at least a
√
1 + 𝛼factor i.e., a con-
stant > 1. Using the same idea as the proof of Lemma 15, since
𝐺has bounded aspect ratio, the total amount 𝑈can grow is
log1+𝛼𝑛1+𝜖= 𝑂(log𝑛), and so the number of unsuccessful
checks to due 𝑈is 𝑂(log𝑛). Similarly, since the three terms
in 𝐷can be at most Wmax, and can become no smaller than
Wmin/𝑛2, the overall number of unsuccessful checks due to
𝐷is also 𝑂(log𝑛), completing the proof.
□
19

Theorem 2. The overall running time of the algorithm is
𝑂((𝑚+ 𝑛) log2 𝑛).
Proof. Each broadcast requires updating the values of all
neighbors of a node 𝑢in a heap at a cost of 𝑂(log𝑛) per edge.
Since each node requires at most 𝑂(log𝑛) broadcasts in to-
tal, the overall time is 𝑂(𝑚log2 𝑛). Similarly, the cost over
all calls to BestAssignedNeighbor is 𝑂(𝑚log2 𝑛) since each
unsuccessful edge must be reinserted into a heap. Lastly, the
extract operations over the min heap indexed on the nodes
costs 𝑂(𝑛log𝑛) in total. Although the queue may be updated
more times than this due to cases where all edges in a call to
BestAssignedNeighbor are unsuccessful, but this cost can be
charged to one of the unsuccessful edges.
□
20

