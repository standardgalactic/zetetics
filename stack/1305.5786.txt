Graphic lambda calculus
Marius Buliga
Institute of Mathematics, Romanian Academy
P.O. BOX 1-764, RO 014700
Bucure¸sti, Romania
Marius.Buliga@imar.ro
This version: 23.05.2013
Abstract
We introduce and study graphic lambda calculus, a visual language which can be
used for representing untyped lambda calculus, but it can also be used for computations
in emergent algebras or for representing Reidemeister moves of locally planar tangle
diagrams.
1
Introduction
Graphic lambda calculus consists of a class of graphs endowed with moves between them. It
might be considered a visual language in the sense of Erwig [9]. The name ”graphic lambda
calculus” comes from the fact that it can be used for representing terms and reductions from
untyped lambda calculus. It’s main move is called ”graphic beta move” for it’s relation to
the beta reduction in lambda calculus. However, the graphic beta move can be applied
outside the ”sector” of untyped lambda calculus, and the graphic lambda calculus can be
used for other purposes than the one of visual representing lambda calculus.
For other visual, diagrammatic representation of lambda calculus see the VEX language
[8], or David Keenan’s [15].
The motivation for introducing graphic lambda calculus comes from the study of emer-
gent algebras. In fact, my goal is to build eventually a logic system which can be used for
the formalization of certain ”computations” in emergent algebras, which can be applied then
for a discrete diﬀerential calculus which exists for metric spaces with dilations, comprising
riemannian manifolds and sub-riemannian spaces with very low regularity.
Emergent algebras are a generalization of quandles, namely an emergent algebra is a
family of idempotent right quasigroups indexed by the elements of an abelian group, while
quandles are self-distributive idempotent right quasigroups. Tangle diagrams decorated by
quandles or racks are a well known tool in knot theory [10] [13].
It is notable to mention the work of Kauﬀman [14], where the author uses knot dia-
grams for representing combinatory logic, thus untyped lambda calculus. Also Meredith
and Snyder[17] associate to any knot diagram a process in pi-calculus,
Is there any common ground between these three apparently separated ﬁeld, namely
diﬀerential calculus, logic and tangle diagrams? As a ﬁrst attempt for understanding this,
I proposed λ-Scale calculus [5], which is a formalism which contains both untyped lambda
calculus and emergent algebras. Also, in the paper [6] I proposed a formalism of decorated
tangle diagrams for emergent algebras and I called ”computing with space” the various
manipulations of these diagrams with geometric content. Nevertheless, in that paper I was
not able to give a precise sense of the use of the word ”computing”.
I speculated, by
using analogies from studies of the visual system, especially the ”Brain a geometry engine”
paradigm of Koenderink [16], that, in order for the visual front end of the brain to reconstruct
the visual space in the brain, there should be a kind of ”geometrical computation” in the
1
arXiv:1305.5786v2  [cs.LO]  24 Sep 2013

neural network of the brain akin to the manipulation of decorated tangle diagrams described
in our paper.
I hope to convince the reader that graphic lambda calculus gives a rigorous answer to this
question, being a formalism which contains, in a sense, lambda calculus, emergent algebras
and tangle diagrams formalisms.
Acknowledgement.
This work was supported by a grant of the Romanian National Au-
thority for Scientiﬁc Research, CNCS UEFISCDI, project number PN-II-ID-PCE-2011-3-
0383.
2
Graphs and moves
An oriented graph is a pair (V, E), with V the set of nodes and E ⊂V × V the set of
edges. Let us denote by α : V →2E the map which associates to any node N ∈V the set
of adjacent edges α(N). In this paper we work with locally planar graphs with decorated
nodes, i.e. we shall attach to a graph (V, E) supplementary information:
- a function f : V →A which associates to any node N ∈V an element of the ”graphical
alphabet” A (see deﬁnition 2.1),
- a cyclic order of α(N) for any N ∈V , which is equivalent to giving a local embedding
of the node N and edges adjacent to it into the plane.
We shall construct a set of locally planar graphs with decorated nodes, starting from
a graphical alphabet of elementary graphs.
On the set of graphs we shall deﬁne local
transformations, or moves. Global moves or conditions will be then introduced.
Deﬁnition 2.1 The graphical alphabet contains the elementary graphs, or gates, denoted by
λ, Υ, ⋏, ⊤, and for any element ε of the commutative group Γ, a graph denoted by ¯ε. Here
are the elements of the graphical alphabet:
λ graph
λ
,
Υ graph
,
⋏graph
,
¯ε graph
ε
,
⊤graph
.
With the exception of the ⊤, all other elementary graphs have three edges. The graph ⊤has
only one edge.
There are two types of ”fork” graphs, the λ graph and the Υ graph, and two types of
”join” graphs, the ⋏graph and the ¯ε graph. Further I brieﬂy explain what are they supposed
to represent and why they are needed in this graphic formalism.
The λ gate corresponds to the lambda abstraction operation from untyped lambda calcu-
lus. This gate has one input (the entry arrow) and two outputs (the exit arrows), therefore,
at ﬁrst view, it cannot be a graphical representation of an operation. In untyped lambda
calculus the λ abstraction operation has two inputs, namely a variable name x and a term
A, and one output, the term λx.A. There is an algorithm, presented in section 3, which
2

transforms a lambda calculus term into a graph made by elementary gates, such that to any
lambda abstraction which appears in the term corresponds a λ gate.
The Υ gate corresponds to a FAN-OUT gate. It is needed because the graphic lambda
calculus described in this article does not have variable names. Υ gates appear in the process
of elimination of variable names from lambda terms, in the algorithm previously mentioned.
Another justiﬁcation for the existence of two fork graphs is that they are subjected to
diﬀerent moves: the λ gate appears in the graphic beta move, together with the ⋏gate, while
the Υ gate appears in the FAN-OUT moves. Thus, the λ and Υ gates, even if they have
the same topology, they are subjected to diﬀerent moves, which in fact characterize their
”lambda abstraction”-ness and the ”fan-out”-ness of the respective gates. The alternative,
which consists into using only one, generic, fork gate, leads to the identiﬁcation, in a sense,
of lambda abstraction with fan-out, which would be confusing.
The ⋏gate corresponds to the application operation from lambda calculus. The algo-
rithm from section 3 associates a ⋏gate to any application operation used in a lambda
calculus term.
The ¯ε gate corresponds to an idempotent right quasigroup operation, which appears in
emergent algebras, as an abstractization of the geometrical operation of taking a dilation
(of coeﬃcient ε), based at a point and applied to another point.
As previously, the existence of two join gates, with the same topology, is justiﬁed by the
fact that they appear in diﬀerent moves.
1. The set GRAPH.
We construct the set of graphs GRAPH over the graphical alpha-
bet by grafting edges of a ﬁnite number of copies of the elements of the graphical alphabet.
Deﬁnition 2.2 GRAPH is the set of graphs obtained by grafting edges of a ﬁnite number
of copies of the elements of the graphical alphabet. During the grafting procedure, we start
from a set of gates and we add, one by one, a ﬁnite number of gates, such that, at any step,
any edge of any elementary graph is grafted on any other free edge (i.e. not already grafted
to other edge) of the graph, with the condition that they have the same orientation.
For any node of the graph, the local embedding into the plane is given by the element of
the graphical alphabet which decorates it.
The set of free edges of a graph G ∈GRAPH is named the set of leaves L(G). Tech-
nically, one may imagine that we complete the graph G ∈GRAPH by adding to the free
extremity of any free edge a decorated node, called ”leaf”, with decoration ”IN” or ”OUT”,
depending on the orientation of the respective free edge. The set of leaves L(G) thus decom-
poses into a disjoint union L(G) = IN(G) ∪OUT(G) of in or out leaves.
Moreover, we admit into GRAPH arrows without nodes,
, called wires or
lines, and loops (without nodes from the elementary graphs, nor leaves)
Graphs in GRAPH can be disconnected. Any graph which is a ﬁnite reunion of lines,
loops and assemblies of the elementary graphs is in GRAPH.
2. Local moves.
These are transformations of graphs in GRAPH which are local, in the
sense that any of the moves apply to a limited part of a graph, keeping the rest of the graph
unchanged.
We may deﬁne a local move as a rule of transformation of a graph into another of the
following form.
First, a subgraph of a graph G in GRAPH is any collection of nodes and/or edges of
G. It is not supposed that the mentioned subgraph must be in GRAPH. Also, a collection
3

of some edges of G, without any node, count as a subgraph of G. Thus, a subgraph of G
might be imagined as a subset of the reunion of nodes and edges of G.
For any natural number N and any graph G in GRAPH, let P(G, N) be the collection
of subgraphs P of the graph G which have the sum of the number of edges and nodes less
than or equal to N.
Deﬁnition 2.3 A local move has the following form: there is a number N and a condition C
which is formulated in terms of graphs which have the sum of the number of edges and nodes
less than or equal to N, such that for any graph G in GRAPH and for any P ∈P(G, N),
if C is true for P then transform P into P ′, where P ′ is also a graph which have the sum
of the number of edges and nodes less than or equal to N.
Graphically we may group the elements of the subgraph, subjected to the application
of the local rule, into a region encircled with a dashed closed, simple curve. The edges
which cross the curve (thus connecting the subgraph P with the rest of the graph) will be
numbered clockwise. The transformation will aﬀect only the part of the graph which is
inside the dashed curve (inside meaning the bounded connected part of the plane which is
bounded by the dashed curve) and, after the transformation is performed, the edges of the
transformed graph will connect to the graph outside the dashed curve by respecting the
numbering of the edges which cross the dashed line.
However, the grouping of the elements of the subgraph has no intrinsic meaning in graphic
lambda calculus. It is just a visual help and it is not a part of the formalism. As a visual
help, I shall use sometimes colors in the ﬁgures. The colors, as well, don’t have any intrinsic
meaning in the graphic lambda calculus.
2.1. Graphic β move.
This is the most important move, inspired by the β-reduction
from lambda calculus, see theorem 3.1, part (d).
The labels ”1, 2, 3, 4” are used only as guides for gluing correctly the new pattern, after
removing the old one. As with the encircling dashed curve, they have no intrinsic meaning
in graphic lambda calculus.
This ”sewing braids” move will be used also in contexts outside of lambda calculus! It is
the most powerful move in this graphic calculus. A primitive form of this move appears as
the re-wiring move (W1) (section 3.3, p. 20 and the last paragraph and ﬁgure from section
3.4, p. 21 in [6]).
An alternative notation for this move is the following:
A move which looks very much alike the graphic beta move is the UNZIP operation from
the formalism of knotted trivalent graphs, see for example the paper [21] section 3. In order
to see this, let’s draw again the graphic beta move, this time without labeling the arrows:
4

The unzip operation acts only from left to right in the following ﬁgure. Remarkably, it
acts on trivalent graphs (but not oriented).
Let us go back to the graphic beta move and remark that it does not depend on the
particular embedding in the plane. For example, the intersection of the ”1,3” arrow with
the ”4,2” arrow is an artifact of the embedding, there is no node there. Intersections of
arrows have no meaning, remember that we work with graphs which are locally planar, not
globally planar.
The graphic beta move goes into both directions. In order to apply the move, we may
pick a pair of arrows and label them with ”1,2,3,4”, such that, according to the orientation
of the arrows, ”1” points to ”3” and ”4” points to ”2”, without any node or label between
”1” and ”3” and between ”4” and ”2” respectively. Then, by a graphic beta move, we may
replace the portions of the two arrows which are between ”1” and ”3”, respectively between
”4” and ”2”, by the pattern from the LHS of the ﬁgure.
The graphic beta move may be applied even to a single arrow, or to a loop. In the
next ﬁgure we see three applications of the graphic beta move. They illustrate the need for
considering loops and wires as members of GRAPH.
Also, we can apply in diﬀerent ways a graphic beta move, to the same graph and in the
5

same place, simply by using diﬀerent labels ”1”, ... ”4” (here A, B, C, D are graphs in
GRAPH):
A particular case of the previous ﬁgure is yet another justiﬁcation for having loops as
elements in GRAPH.
These two applications of the graphic beta move may be represented alternatively like
this:
6

2.2. (CO-ASSOC) move.
This is the ”co-associativity” move involving the Υ graphs.
We think about the Υ graph as corresponding to a FAN-OUT gate.
By using CO-ASSOC moves, we can move between any two binary trees formed only
with Υ gates, with the same number of output leaves.
2.3. (CO-COMM) move.
This is the ”co-commutativity” move involving the Υ gate.
It will be not used until the section 6 concerning knot diagrams.
2.3.a (R1a) move.
This move is imported from emergent algebras. Explanations are
given in section 5. It involves an Υ graph and a ¯ε graph, with ε ∈Γ.
7

2.3.b (R1b) move.
The move R1b (also related to emergent algebras) is this:
2.4. (R2) move.
This corresponds to the Reidemeister II move for emergent algebras. It
involves an Υ graph and two other: a ¯ε and a ¯µ graph, with ε, µ ∈Γ.
This move appears in section 3.4, p.
21 [6], with the supplementary name ”triangle
move”.
2.5. (ext2) move.
This corresponds to the rule (ext2) from λ-Scale calculus, it expresses
the fact that in emergent algebras the operation indexed with the neutral element 1 of the
group Γ has the property x ◦1 y = y.
8

2.6. Local pruning.
Local pruning moves are local moves which eliminate ”dead” edges.
Notice that, unlike the previous moves, these are one-way (you can eliminate dead edges,
but not add them to graphs).
9

Global moves or conditions.
Global moves are those which are not local, either because
the condition C applies to parts of the graph which may have an arbitrary large sum or
edges plus nodes, or because after the move the graph P ′ which replaces the graph P has
an arbitrary large sum or edges plus nodes.
2.7. (ext1) move.
This corresponds to the rule (ext1) from λ-Scale calculus, or to η-
reduction in lambda calculus (see theorem 3.1, part (e) for details). It involves a λ graph
(think about the λ abstraction operation in lambda calculus) and a ⋏graph (think about
the application operation in lambda calculus).
The rule is: if there is no oriented path from ”2” to ”1”, then the following move can be
performed.
10

2.8. (Global FAN-OUT) move.
This is a global move, because it consists in replacing
(under certain circumstances) a graph by two copies of that graph.
The rule is: if a graph in G ∈GRAPH has a Υ bottleneck, that is if we can ﬁnd a
sub-graph A ∈GRAPH connected to the rest of the graph G only through a Υ gate, then
we can perform the move explained in the next ﬁgure, from the left to the right.
Conversely, if in the graph G we can ﬁnd two identical subgraphs (denoted by A), which
are in GRAPH, which have no edge connecting one with another and which are connected
to the rest of G only through one edge, as in the RHS of the ﬁgure, then we can perform
the move from the right to the left.
Remark that (global FAN-OUT) trivially implies (CO-COMM). ( As an local rule al-
ternative to the global FAN-OUT, we might consider the following. Fix a number N and
consider only graphs A which have at most N (nodes + arrows). The N LOCAL FAN-OUT
move is the same as the GLOBAL FAN-OUT move, only it applies only to such graphs A.
This local FAN-OUT move does not imply CO-COMM.)
2.9. Global pruning.
This a global move which eliminates ”dead” edges.
The rule is: if a graph in G ∈GRAPH has a ⊤ending, that is if we can ﬁnd a sub-graph
A ∈GRAPH connected only to a ⊤gate, with no edges connecting to the rest of G, then
we can erase this graph and the respective ⊤gate.
11

The global pruning may be needed because of the λ gates, which cannot be removed
only by local pruning.
2.10. Elimination of loops.
It is possible that, after using a local or global move, we
obtain a graph with an arrow which closes itself, without being connected to any node. Here
is an example, concerning the application of the graphic β move. We may erase any such
loop, or add one.
λGRAPHS.
The edges of an elementary graph λ can be numbered unambiguously, clock-
wise, by 1, 2, 3, such that 1 is the number of the entrant edge.
Deﬁnition 2.4 A graph G ∈GRAPH is a λ-graph, notation G ∈λGRAPH, if:
- it does not have ¯ε gates,
- for any node λ any oriented path in G starting at the edge 2 of this node can be
completed to a path which either terminates in a graph ⊤, or else terminates at the
edge 1 of this node.
The condition G ∈λGRAPH is global, in the sense that in order to decide if G ∈λGRAPH
we have to examine parts of the graph which may have an arbitrary large sum or edges plus
nodes.
3
Conversion of lambda terms into GRAPH
Here I show how to associate to a lambda term a graph in GRAPH, then I use this to show
that β-reduction in lambda calculus transforms into the β rule for GRAPH. (Thanks to
Morita Yasuaki for some corrections.)
Indeed, to any term A ∈T(X) (where T(X) is the set of lambda terms over the variable
set X) we associate its syntactic tree. The syntactic tree of any lambda term is constructed
by using two gates, one corresponding to the λ abstraction and the other corresponding to
the application. We draw syntactic trees with the leaves (elements of X) at the bottom and
the root at the top. We shall use the following notation for the two gates: at the left is the
gate for the λ abstraction and at the right is the gate for the application.
λ
x
A
λx.A
A
B
AB
12

Remark that these two gates are from the graphical alphabet of GRAPH, but the
syntactic tree is decorated: at the bottom we have leaves from X. Also, remark the peculiar
orientation of the edge from the left (in tree notation convention) of the λ gate. For the
moment, this orientation is in contradiction with the implicit orientation (from down-up) of
edges of the syntactic tree, but soon this matter will become clear.
We shall remove all leaves decorations, with the price of introducing new gates, namely
Υ and ⊤gates.
This will be done in a sequence of steps, detailed further.
Take the
syntactic tree of A ∈T(X), drawn with the mentioned conventions (concerning gates and
the positioning of leaves and root respectively).
We take as examples the following ﬁve lambda terms: I = λx.x, K = λx.(λy.x), S =
λx.(λy.(λz.((xz)(yz)))), Ω= (λx.(xx))(λx.(xx)) and T = (λx.(xy))(λx.(xy)).
Step 1.
Elimination of bound variables, part I. Any leaf of the tree is connected to the
root by an unique path.
Start from the leftmost leaf, perform the algorithm explained further, then go to the
right and repeat until all leaves are exhausted. We initialize also a list B = ∅of bound
variables.
Take a leaf, say decorated with x ∈X. To this leaf is associated a word (a list) which is
formed by the symbols of gates which are on the path which connects (from the bottom-up)
the leaf with the root, together with information about which way, left (L) or right (R), the
path passes through the gates. Such a word is formed by the letters λL, λR, ⋏L, ⋏R.
If the ﬁrst letter is λL then add to the list B the pair (x, w(x)) formed by the variable
name x, and the associated word (describing the path to follow from the respective leaf to
the root). Then pass to a new leaf.
Else continue along the path to the roof. If we arrive at a λ gate, this can happen only
coming from the right leg of the λ gate, thus we can ﬁnd only the letter λR. In such a case
look at the variable y which decorates the left leg of the same λ gate. If x = y then add to
the syntactic tree a new edge, from y to x and proceed further along the path, else proceed
further. If the root is attained then pass to next leaf.
Examples: the graphs associated to the mentioned lambda terms, together with the list
of bound variables, are the following.
- I = λx.x has B =

(x, λL)
	
, K = λx.(λy.x) has B =

(x, λL), (y, λLλR)
	
, S =
λx.(λy.(λz.((xz)(yz)))) has B =

(x, λL), (y, λLλR), (z, λLλRλR)
	
.
λ
x
x
λ
λ
x
y
x
λ
λ
λ
x
x
z
z
y
y
z
- Ω= (λx.(xx))(λx.(xx)) has B =

(x, λL⋏L), (x, λL⋏R)
	
, T = (λx.(xy))(λx.(xy)) has
B =

(x, λL⋏L), (x, λL⋏R)
	
.
13

λ
λ
λ
λ
x
x
x
x
x
x
x
x
x
y
y
x
Step 2.
Elimination of bound variables, part II. We have now a list B of bound variables.
If the list is empty then go to the next step. Else, do the following, starting from the ﬁrst
element of the list, until the list is ﬁnished.
An element, say (x, w(x)), of the list, is either connected to other leaves by one or more
edges added at step 1, or not. If is not connected then erase the variable name with the
associated path w(x) and replace it by a ⊤gate. If it is connected then erase it, replace it
by a tree formed by Υ gates, which starts at the place where the element of the list were
before the erasure and stops at the leaves which were connected to x. Erase all decorations
which were joined to x and also erase all edges which were added at step 1 to the leave x
from the list.
Examples: after the step 2, the graphs associated to the mentioned lambda terms are
the following.
- the graphs of I = λx.x, K = λx.(λy.x), S = λx.(λy.(λz.((xz)(yz)))) are
λ
λ
λ
λ
λ
λ
- the graphs of Ω= (λx.(xx))(λx.(xx)), T = (λx.(xy))(λx.(xy)) are
λ
λ
λ
λ
y
y
14

Remark that at this step the necessity of having the peculiar orientation of the left leg
of the λ gate becomes clear.
Remark also that there may be more than one possible tree of gates Υ, at each elimination
of a bound variable (in case a bound variable has at least tree occurrences). One may use
any tree of Υ which is ﬁt. The problem of multiple possibilities is the reason of introducing
the (CO-ASSOC) move.
Step 3.
We may still have leaves decorated by free variables. Starting from the left to
the right, group them together in case some of them occur in multiple places, then replace
the multiple occurrences of a free variable by a tree of Υ gates with a free root, which ends
exactly where the occurrences of the respective variable are. Again, there are multiple ways
of doing this, but we may pass from one to another by a sequence of (CO-ASSOC) moves.
Examples: after the step 3, all the graphs associated to the mentioned lambda terms,
excepting the last one, are left unchanged. The graph of the last term, changes.
- as an illustration, I ﬁgure the graphs of Ω= (λx.(xx))(λx.(xx)), left unchanged by
step 3, and the graph of T = (λx.(xy))(λx.(xy)):
λ
λ
λ
λ
Theorem 3.1 Let A 7→[A] be a transformation of a lambda term A into a graph [A] as
described previously (multiple transformations are possible because of the choice of Υ trees).
Then:
(a) for any term A the graph [A] is in λGRAPH,
(b) if [A]′ and [A]” are transformations of the term A then we may pass from [A]′ to [A]”
by using a ﬁnite number (exponential in the number of leaves of the syntactic tree of
A) of (CO-ASSOC) moves,
(c) if B is obtained from A by α-conversion then we may pass from [A] to [B] by a ﬁnite
sequence of (CO-ASSOC) moves,
(d) let A, B ∈T(X) be two terms and x ∈X be a variable. Consider the terms λx.A
and A[x := B], where A[x := B] is the term obtained by substituting in A the free
occurrences of x by B.
We know that β reduction in lambda calculus consists in
passing from (λx.A)B to A[x := B]. Then, by one β move in GRAPH applied to
[(λx.A)B] we pass to a graph which can be further transformed into one of A[x := B],
via (global FAN-OUT) moves, (CO-ASSOC) moves and pruning moves,
(e) with the notations from (d), consider the terms A and λx.Ax with x ̸∈FV (A); then
the η reduction, consisting in passing from λx.Ax to A, corresponds to the ext1 move
applied to the graphs [λx.Ax] and [A].
15

Proof.
(a) we have to prove that for any node λ any oriented path in [A] starting at the
left exiting edge of this node can be completed to a path which either terminates in a graph
⊤, or else terminates at the entry peg of this node, but this is clear. Indeed, either the
bound variable (of this λ node in the syntactic tree of A) is fresh, then the bound variable
is replaced by a ⊤gate, or else, the bound variable is replaced by a tree of Υ gates. No
matter which path we choose, we may complete it to a cycle passing by the said λ node.
(b) Clear also, because the (CO-ASSOC) move is designed for passing from a tree of Υ
gates to another tree with the same number of leaves.
(c) Indeed, the names of bound variables of A do not aﬀect the construction of [A],
therefore if B is obtained by α-conversion of A, then [B] diﬀers from [A] only by the particular
choice of trees of Υ gates. But this is solved by (CO-ASSOC) moves.
(d) This is the surprising, maybe, part of the theorem. There are two cases: x is fresh
for A or not. If x is fresh for A then in the graph [(λx.A)B] the name variable x is replaced
by a ⊤gate. If not, then all the occurrences of x in A are connected by a Υ tree with root
at the left peg of the λ gate where x appears as a bound variable.
In the case when x is not fresh for A, we see in the LHS of the ﬁgure the graph [(λx.A)B]
(with a remanent decoration of ”x”). We perform a graphic (β) move and we obtain the
graph from the right.
This graph can be transformed into a graph of A[x := B] via (global FAN-OUT) and
(CO-ASSOC) moves. The case when x is fresh for A is ﬁgured next.
We see that the graph obtained by performing the graphic (β) move is the union of the
graph of A and the graph of B with a ⊤gate added at the root. By pruning we are left
with the graph of A, which is consistent to the fact that when x is fresh for A then (λx.A)B
transforms by β reduction into A.
(e) In the next ﬁgure we see at the LHS the graph [λx.Ax] and at the RHS the graph
[A].
16

The red asterisk marks the arrow which appears in the construction [λx.Ax] from the
variable x, taking into account the hypothesis x ̸∈FV (A). We have a pattern where we can
apply the ext1 move and we obtain [A], as claimed.
□
As an example, let us manipulate the graph of Ω= (λx.(xx))(λx.(xx)):
We can pass from the LHS ﬁgure to the RHS ﬁgure by using a graphic (β) move. Con-
versely, we can pass from the RHS ﬁgure to the LHS ﬁgure by using a (global FAN-OUT)
move. These manipulations correspond to the well known fact that Ωis left unchanged after
β reduction: let U = λx.(xx), then Ω= UU = (λx.(xx))U ↔UU = Ω.
3.1
Example: combinatory logic
S, K and I combinators in GRAPH.
The combinators I = λx.x, K = λx.(λy.(xy))
and S = λx.(λy.(λz.((xz)(yz)))) have the following correspondents in GRAPH, denoted by
the same letters:
λ
λ
λ
λ
λ
λ
S
K
I
17

Proposition 3.2 (a) By one graphic (β) move I ⋏A transforms into A, for any A ∈
GRAPH with one output.
(b) By two graphic (β) moves, followed by a global pruning, for any A, B ∈GRAPH
with one output, the graph (K ⋏A) ⋏B transforms into A.
(c) By ﬁve graphic (β) moves, followed by one local pruning move, the graph (S ⋏K)⋏K
transforms into I.
(d) By three graphic (β) moves followed by a (global FAN-OUT) move, for any A, B, C ∈
GRAPH with one output, the graph ((S ⋏A) ⋏B) ⋏C transforms into the graph (A ⋏C) ⋏
(B ⋏C).
Proof.
The proof of (b) is given in the next ﬁgure.
The proof of (c) is given in the following ﬁgure.
18

(a) and (d) are left to the interested reader.
□
4
Using graphic lambda calculus
The manipulations of graphs presented in this section can be applied for graphs which
represent lambda terms. However, they can also be applied for graphs which do not represent
lambda terms.
Fixed points.
Let’s start with a graph A ∈GRAPH, which has one distinguished input
and one distinguished output.I represent this as follows.
For any graph B with one output, we denote by A(B) the graph obtained by grafting
the output of B to the input of A.
I want to ﬁnd B such that A(B) ↔B, where ↔means any ﬁnite sequence of moves in
graphic lambda calculus. I call such a graph B a ﬁxed point of A.
The solution of this problem is the same as in usual lambda calculus. We start from the
following succession of moves:
19

This is very close to the solution, we only need a small modiﬁcation:
Grafting, application or abstraction?
If the A, B from the previous paragraph were
representing lambda terms, then the natural operation between them is not grafting, but the
application. Or, in graphic lambda calculus the application it’s represented by an elementary
graph, therefore AB (seen as the term in lambda calculus which is obtained as the application
of A to B) is not represented as a grafting of the output of B to the input of A.
We can easily transform grafting into the application operation.
20

Suppose that A and B are graphs representing lambda terms, more precisely suppose
that A is representing a term (denoted by A too) and it’s input represents a free variable x
of the term A. Then the grafting of B to A is the term A[x := B] and the graph from the
right is representing (λx.A)B, therefore both graphs are representing terms from lambda
calculus.
We can transform grafting into something else:
This has no meaning in lambda calculus, but excepting the orientation of one of the
arrows of the graph from the right, it looks like if the abstraction gate (the λ gate) plays
the role of an application operation.
Zippers and combinators as half-zippers.
Let’s take n ≥1 a natural number and let’s
consider the following graph in GRAPH, called the n-zipper:
At the left is the n-zipper graph; at the right is a notation for it, or a ”macro”. The
zipper graph is interesting because it allows to perform (nontrivial) graphic beta moves in a
ﬁxed order. In the following picture is ﬁgured in red the place where the ﬁrst graphic beta
move is applied.
In terms of zipper notation this graphic beta move has the following appearance:
21

We see that a n-zipper transforms into a (n-1)-zipper plus an arrow. We may repeat this
move, as long as we can. This procedure deﬁnes a ”zipper move”:
We may see the 1-zipper move as the graphic beta move, which transforms the 1-zipper
into two arrows.
The combinator I = λx.x satisﬁes the relation IA = A. In the next ﬁgure it is shown
that I (ﬁgured in green), when applied to A, is just a half of the 1-zipper, with an arrow
added (ﬁgured in blue).
By opening the zipper we obtain A, as it should.
The combinator K = λxy.x satisﬁes KAB = (KA)B = A.
In the next ﬁgure the
combinator K (in green) appears as half of the 2-zipper, with one arrow and one termination
gate added (in blue).
After opening the zipper we obtain a pair made by A and B which gets the termination
gate on top of it. A global pruning move sends B to the trash bin.
Finally, the combinator S = λxyz.((xz)(yz)) satisﬁes SABC = ((SA)B)C = (AC)(BC).
The combinator S (in green) appears to be made by half of the 3-zipper, with some arrows
and also with a ”diamond” added (all in blue). Interestingly, the diamond looks alike the
ones from the emergent algebra sector, deﬁnition 5.4.
22

Expressed with the help of zippers, the relation SKK = I appears like this.
Lists and currying.
With the help of zippers, we may enhance the procedure of turning
grafting into the application operation.
We have a graph A ∈GRAPH which has one
output and several inputs.
23

We use an n-zipper in order to clip the inputs with the output.
This graph is, in fact, the following one.
We may interpret the graph inside the green dotted rectangle as the currying of A, let’s
call him Curry(A). This graph has only one output and no inputs. The graph inside the
red dotted rectangle is almost a list. We shall transform it into a list by using again a zipper
and one graphic beta move.
24

Packing arrows.
We may pack several arrows into one. I describe ﬁrst the case of two
arrows. We start from the following sequence of three graphic beta moves.
With words, this ﬁgure means: we pack the 1, 2, entries into a list, we pass it trough one
25

arrow then we unpack the list into the outputs 3, 4. This packing-unpacking trick may be
used of course for more than a pair of arrows, in obvious ways, therefore it is not a restriction
of generality to write only about two arrows.
We may apply the trick to a pair of graphs A and B, which are connected by a pair of
arrows, like in the following ﬁgure.
With the added packing and unpacking triples of gates, the graphs A, B are interacting
only by the intermediary of one arrow.
In particular, we may use this trick for the elementary gates of abstraction and applica-
tion, transforming them into graphs with one input and one output, like this:
If we use the elementary gates transformed into graphs with one input and one output,
the graphic beta move becomes this almost algebraic, 1D rule:
With such procedures, we may transform any graph in GRAPH into a 1D string of
graphs, consisting of transformed elementary graphs and packers and un-packers of arrows,
which could be used, in principle, for transforming graphic lambda calculus into a text
programming language.
26

5
Emergent algebras
Emergent algebras [3] [4] are a distillation of diﬀerential calculus in metric spaces with di-
lations [2].
This class of metric spaces contain the ”classical” riemannian manifolds, as
well as fractal like spaces as Carnot groups or, more general, sub-riemannian or Carnot-
Carath´eodory spaces, Bella¨ıche [1], Gromov [11], endowed with an intrinsic diﬀerential cal-
culus based on some variant of the Pansu derivative [18].
In [2] section 4 Binary decorated trees and dilatations, I propose a formalism for making
easy various calculations with dilation structures. This formalism works with moves acting
on binary decorated trees, with the leaves decorated with elements of a metric space.
Here is an example of the formalism. The moves are (with same names as those used in
graphic lambda calculus, see the explanation further):
Deﬁne the following graph (and think about it as being the graphical representation of
an operation u + v with respect to the basepoint x):
Then, in the binary trees formalism I can prove, by using the moves R1a, R2a, the
following ”approximate” associativity relation (it is approximate because there appear a
basepoint which is diﬀerent from x, but which, in the geometric context of spaces with
dilations, is close to x):
It was puzzling that in fact the formalism worked without needing to know which metric
space is used. Moreover, reasoning with moves acting on binary trees gave proofs of gener-
alizations of results from sub-riemannian geometry, while classical proofs involve elaborate
calculations with pseudo-diﬀerential operators. At a close inspection it looked like some-
27

where in the background there is an abstract nonsense machine which is just applied to the
particular case of sub-riemannian spaces.
In this paper I shall take the following pure algebraic deﬁnition of an emergent algebra
(compare with deﬁnition 5.1 [3]), which is a stronger version of the deﬁnition 4.2 [4] of a Γ
idempotent right quasigroup, in the sense that here I deﬁne a Γ idempotent quasigroup.
Deﬁnition 5.1 Let Γ be a commutative group with neutral element denoted by 1 and oper-
ation denoted multiplicatively. A Γ idempotent quasigroup is a set X endowed with a family
of operations ◦ε : X × X →X, indexed by ε ∈Γ, such that:
- For any ε ∈Γ \ {1} the pair (X, ◦ε) is an idempotent quasigroup, i.e. for any a, b ∈X
the equations x ◦ε a = b and a ◦ε x = b have unique solutions and moreover x ◦ε x = x
for any x ∈X,
- The operation ◦1 is trivial: for any x, y ∈X we have x ◦1 y = y,
- For any x, y ∈X and any ε, µ ∈Γ we have: x ◦ε (x ◦µ y) = x ◦εµ y.
Here are some examples of Γ idempotent quasigroups.
Example 1.
Real (or complex) vector spaces: let X be a real (complex) vector space,
Γ = (0, +∞) (or Γ = C∗), with multiplication as operation. We deﬁne, for any ε ∈Γ the
following quasigroup operation: x ◦ε y = (1 −ε)x + εy. These operations give to X the
structure of a Γ idempotent quasigroup. Notice that x ◦ε y is the dilation based at x, of
coeﬃcient ε, applied to y.
Example 2.
Contractible groups: let G be a group endowed with a group morphism
φ : G →G. Let Γ = Z with the operation of addition of integers (thus we may adapt
deﬁnition 5.1 to this example by using ”ε + µ” instead of ”εµ” and ”0” instead of ”1” as
the name of the neutral element of Γ). For any ε ∈Z let x ◦ε y = xφε(x−1y). This a
Z idempotent quasigroup. The most interesting case is the one when φ is an uniformly
contractive automorphism of a topological group G. The structure of these groups is an
active exploration area, see for example [12] and the bibliography therein. A fundamental
result here is Siebert [20], which gives a characterization of topological connected contractive
locally compact groups as being nilpotent Lie groups endowed with a one parameter family
of dilations, i.e. almost Carnot groups.
Example 3.
A group with an invertible self-mapping φ : G →G such that φ(e) = e,
where e is the identity of the group G. It looks like Example 2 but it shows that there is no
need for φ to be a group morphism.
Local versions.
We may accept that there is a way (deﬁnitely needing care to well for-
mulate, but intuitively clear) to deﬁne a local version of the notion of a Γ idempotent
quasigroup. With such a deﬁnition, for example, a convex subset of a real vector space
gives a local (0, +∞) idempotent quasigroup (as in Example 1) and a neighbourhood of the
identity of a topological group G, with an identity preserving, locally deﬁned invertible self
map (as in Example 3) gives a Z local idempotent quasigroup.
Example 4.
A particular case of Example 3 is a Lie group G with the operations deﬁned
for any ε ∈(0, +∞) by x ◦ε y = x exp(ε log(x−1y)).
Example 5.
A less symmetric example is the one of X being a riemannian manifold, with
associated operations deﬁned for any ε ∈(0, +∞) by x ◦ε y = expx(ε logx(y)), where exp is
the metric exponential.
Example 6.
More generally, any metric space with dilations is a local idempotent (right)
quasigroup.
28

Example 7.
One parameter deformations of quandles. A quandle is a self-distributive
quasigroup. Take now a one-parameter family of quandles (indexed by ε ∈Γ) which satisﬁes
moreover points 2. and 3. from deﬁnition 5.1. What is interesting about this example is
that quandles appear as decorations of knot diagrams [10] [13], which are preserved by the
Reidemeister moves (more on this in the section 6). At closer examination, examples 1, 2
are particular cases of one parameter quandle deformations!
I deﬁne now the operations of approximate sum and approximate diﬀerence associated
to a Γ idempotent quasigroup.
Deﬁnition 5.2 For any ε ∈Γ we give the following names to several combinations of
operations of emergent algebras:
- the approximate sum operation is Σx
ε(u, v) = x •ε ((x ◦ε u) ◦ε v),
- the approximate diﬀerence operation is ∆x
ε(u, v) = (x ◦ε u) •ε (x ◦ε v),
- the approximate inverse operation is invx
ε u = (x ◦ε u) •ε x.
Let’s see what the approximate sum operation is, for example 1.
Σx
ε(u, v) = u(1 −ε) −x + v
It is clear that, as ε converges to 0, this becomes the operation of addition in the vector space
with x as neutral element, so it might be said that is the operation of addition of vectors in
the tangent space at x, where x is seen as an element of the aﬃne space constructed over
the vector space from example 1.
This is a general phenomenon, which becomes really interesting in non-commutative
situations, i.e. when applied to examples from the end of the provided list.
These approximate operations have many algebraic properties which can be found by
the abstract nonsense of manipulating binary trees.
Another construction which can be done in emergent algebras is the one of taking ﬁnite
diﬀerences (at a high level of generality, not bonded to vector spaces).
Deﬁnition 5.3 Let A : X →X be a function (from X to itself, for simplicity). The ﬁnite
diﬀerence function associated to A, with respect to the emergent algebra over X, at a point
x ∈X is the following.
T x
ε A : X →X
,
T x
ε A(u) = A(x) •ε (A (x ◦ε u))
For example 1, the ﬁnite diﬀerence has the expression:
T x
ε A(u −x) = A(x) + 1
ε (A(x + εu) −A(x))
which is a ﬁnite diﬀerence indeed. In more generality, for example 2 this deﬁnition leads to
the Pansu derivative [18].
Finite diﬀerences as deﬁned here behave like discrete versions of derivatives. Again, the
proofs consist in manipulating well chosen binary trees.
All this can be formalized in graphic lambda calculus, thus transforming the proofs into
computations inside graphic lambda calculus.
I shall not insist more on this, with the exception of describing the emergent algebra
sector of graphic lambda calculus.
Deﬁnition 5.4 For any ε ∈Γ , the following graphs in GRAPH are introduced:
- the approximate sum graph Σε
29

- the approximate diﬀerence graph ∆ε
- the approximate inverse graph invε
Let A be a set of symbols a, b, c, .... (These symbols will play the role of scale parameters
going to 0.) With A and with the abelian group Γ we construct a larger abelian group, call
it ¯Γ, which is generated by A and by Γ.
Now we introduce the emergent algebra sector (over the set A).
Deﬁnition 5.5 EMER(A) is the subset of GRAPH (over the group ¯Γ) which is generated
by the following list of gates:
- arrows and loops,
- Υ gate and the gates ¯ε for any ε ∈Γ,
- the approximate sum gate Σa and the approximate diﬀerence gate ∆a, for any a ∈A,
with the operations of linking output to input arrows and with the following list of moves:
- FAN-OUT moves
- emergent algebra moves for the group ¯Γ,
- ¡pruning moves.
The set EMER(A) with the given list of moves is called the emergent algebra sector over
the set A.
30

The approximate inverse is not included into the list of generating gates. That is because
we can prove easily that for any a ∈A we have inva ∈EMER(A). (If ε ∈Γ then we trivially
have invε ∈EMER(A) because it is constructed from emergent algebra gates decorated by
elements in Γ, which are on the list of generating gates.) Here is the proof: we start with
the approximate diﬀerence ∆a and with an Υ gate and we arrive to the approximate inverse
inva by a sequence of moves, as follows.
We proved the following relation for emergent algebras: ∆x
a(u, x) = invx
au. This relation
appears as a computation in graphic lambda calculus.
As for the ﬁnite diﬀerences, we may proceed as this.
Deﬁnition 5.6 A graph A ∈GRAPH, with one input and one output distinguished, is
computable with respect to the group
¯
Gamma if the following graph
can be transformed by the moves from graphic lambda calculus into a graph which is made
by assembling:
- graphs from EMER(A),
- gates λ, ⋏and ⊤.
It would be interesting to mix the emergent algebra sector with the lambda calculus
sector (in a sense this is already suggested in deﬁnition 5.6). At ﬁrst view, it seems that the
emergent algebra gates ¯ε are operations which are added to the lambda calculus operations,
the latter being more basic than the former. I think this is not the case. In [5] theorem 3.4, in
the formalism of lambda-scale calculus (graphic lambda calculus is a visual variant of this), I
31

show on the contrary that the emergent algebra gates could be applied to lambda terms and
the result is a collection, or hierarchy of lambda calculi, organized into an emergent algebra
structure. This is surprising, at least for the author, because the initial goal of introducing
lambda-scale calculus was to mimic lambda calculus with emergent algebra operations.
6
Crossings
In this section we discuss about tangle diagrams and graphic lambda calculus.
An oriented tangle is a collection of wired in 3D space, more precisely it is an embedding
of a oriented one dimensional manifold in 3D space.
Two tangles are the same up to
topological deformation of the 3D space. An oriented tangle diagram is, visually, a projection
of a tangle, in general position, on a plane. More speciﬁcally, an oriented tangle diagram is
a globally planar oriented graph with 4-valent nodes which represent crossings of wires (as
seen in the projection), along with supplementary information about which wire passes over
the respective crossing. A locally planar tangle diagram is an oriented graph which satisﬁes
the previous description, with the exception that it is only locally planar. Visually, a locally
planar tangle diagram looks like an ordinary one, excepting that there may be crossings of
edges of the graph which are not tangle crossings (i.e. nodes of the graph).
The purpose of this section is to show that we can ”simulate” tangle diagrams with
graphic lambda calculus. This can be expressed more precisely in two ways. The ﬁrst way is
that we can deﬁne ”crossing macros” in graphic lambda calculus, which are certain graphs
which play the role of crossings in a tangle diagram (i.e. we can express the Reidemeister
moves, described further, as compositions of moves from graphic lambda calculus between
such graphs). The second way is to say that to any tangle diagram we can associate a graph
in GRAPH such that to any Reidemeister move is associated a certain composition of moves
from graphic lambda calculus.
Meredith ad Snyder [17] achieve this goal with the pi-calculus instead of graphic lambda
calculus. Kauﬀman, in the second part of [14], associates tangle diagrams to combinators
and writes about ”knotlogic”.
Oriented Reidemeister moves.
Two tangles are the same, up to topological equivalence,
if and only if any tangle diagram of one tangle can be transformed by a ﬁnite sequence of
Reidemeister moves into a tangle diagram of the second tangle. The oriented Reidemeister
moves are the following (I shall use the same names as Polyak [19], but with the letter Ω
replaced by the letter R):
- four oriented Reidemeister moves of type 1:
- four oriented Reidemeister moves of type 2:
32

- eight oriented Reidemeister moves of type 3:
Crossings from emergent algebras.
In section 5, example 7, it is mentioned that there
is a connection between tangle diagrams and emergent algebras, via the notion of a quandle.
Quandles are self-distributive idempotent quasigroups, which were invented as decorations
of the arrows of a tangle diagram, which are invariant with respect to the Reidemeister
moves.
Let us deﬁne the emergent algebra crossing macros. (We can choose to neglect the ε
decorations of the crossings, or, on the contrary, we can choose to do like in deﬁnition 5.5
of the emergent algebra sector, namely to add a set A to the group Γ and use even more
nuanced decorations for the crossings.)
33

In [6], sections 3-6 are dedicated to the use of these crossings for exploring emergent
algebras and spaces with dilations. All constructions and reasonings from there can be put
into the graphic lambda calculus formalism. Here I shall explain only some introductory
facts.
Let us associate to any locally planar tangle diagram T a graph in [T] ∈GRAPH,
called the translation of T, which is obtained by replacing the crossings with the emergent
crossing macros (for a ﬁxed ε). Also, to any Reidemeister move we associate it’s translation
in graphic lambda calculus, consisting in a local move between the translations of the LHS
and RHS tangles which appear in the respective move. (Note: these translations are not
added to the moves which deﬁne graphic lambda calculus.)
Theorem 6.1 The translations of all oriented Reidemeister moves of type 1 and 2 can be
realized as sequences of the following moves from graphic lambda calculus: emergent algebra
moves (R1a, R1b, R2, ext2), fan-out moves (i.e. CO-COMM, CO-ASSOC, global FAN-
OUT) and pruning moves. More precisely the translations of the Reidemeister moves R1a,
R1b are, respectively, the graphic lambda calculus moves R1a, R1b, modulo fan-out moves.
Moreover, all translations of Reidemeister moves of type 2 can be expressed in graphic lambda
calculus with the move R2, fan-out and pruning moves.
The proof is left to the interested reader, see however section 3.4 [6].
The fact that the Reidemeister moves of type 3 are not true for (the algebraic version of)
the emergent algebras, i.e. that the translations of those cannot be expressed as a sequence
of moves from graphic lambda calculus, is a feature of the formalism and not a weakness.
This is explained in detail in sections 5, 6 [6], but unfortunately at the moment of the
writing that article the graphic lambda calculus was not available. It is an interesting goal
the one of expressing the constructions from the mentioned sections as statements about the
computability in the sense of deﬁnition 5.6 of the translations of certain tangle diagrams.
As a justiﬁcation for this point of view, let us remark that all tangle diagrams which
appear in the Reidemeister moves of type 3 have translations which are related to the
approximate diﬀerence or approximate sum graphs from deﬁnition 5.4. For example, let’s
take the translation of the graph from the RHS of the move R3d and call it D. This graph
has three inputs and three outputs. Let’s then consider a graph formed by grafting three
graphs A, B, C at the inputs of D, such that A, B, C are not otherwise connected. Then
we can perform the following sequence of moves.
34

The graph from the left lower side is formed by an approximate diﬀerence, a ¯ε gate and
several Υ gates. Therefore, if A, B, C are computable in the sense of deﬁnition 5.4 then
the initial graph (the translation of the LHS of R3d with A, B, C grafted at the inputs) is
computable too.
Graphic beta move as braiding.
Let us now construct crossings, in the sense previously
explained, from gates coming from lambda calculus.
As previously, we deﬁne translations of (locally planar) tangle diagrams into graphs in
GRAPH. The class of locally planar tangle diagrams is out in a one-to one correspondence
with a class of graphs in GRAPH, let us call this class λ −TANGLE.
We could proceed in the inverse direction, namely consider the class of graphs λ −
TANGLE, along with the moves: graphic beta move and elimination of loops. Then we
make the (inverse) translation of graphs in λ−TANGLE into locally planar tangle diagrams
and the (inverse) translation of the graphic beta move and the elimination of loops. The
following proposition explains what we obtain.
Proposition 6.2 The class of graphs λ−TANGLE is closed with respect to the application
of the graphic beta move and of the elimination of loops. The translations of the graphic beta
and elimination of loops moves are the following SPLICE 1, 2 (translation of the graphic
beta move) and LOOP 1, 2 (translation of the elimination of loops) moves.
35

Proof.
The proposition becomes obvious if we ﬁnd the translation of the graphic beta
move. There is one translation for each crossing. (Likewise, there are two translations for
elimination of loops, depending on the orientation of the loop which is added/erased.)
□
36

The following theorem clariﬁes which are the oriented Reidemeister moves which can be
expressed as sequences of graphic lambda calculus moves applied to graphs in λ−TANGLE.
Among these moves, some are more powerful than others, as witnessed by the following
Theorem 6.3 All the translations of the oriented Reidemeister move into moves between
graphs in λ −TANGLE, excepting R2c, R2d, R3a, R3h, can be realized as sequences of
graphic beta moves and elimination of loops. Moreover, the translations of moves R2c, R2d,
R3a, R3h are equivalent up to graphic beta moves and elimination of loops (i.e. any of these
moves, together with the graphic beta move and elimination of loops, generates the other
moves from this list).
Proof.
It is easy, but tedious, to verify that all the mentioned moves can be realized as
sequences of SPLICE and LOOP moves. It is as well easy to verify that the moves R2c,
R2d, R3a, R3h are equivalent up to SPLICE and LOOP moves. It is not obvious that the
moves R2c, R2d, R3a, R3h can’t be realized as a sequence of SPLICE and LOOP moves. In
order to do this, we prove that R2d can’t be generated by SPLICE and LOOP. Thanks are
due to Peter Kravchuk for the idea of the proof, given in an answer to a question I asked on
mathoverﬂow [7], where I described the moves SPLICE and LOOP.
To any locally planar tangle diagram A associate it’s reduced diagram R(A), which is
obtained by the following procedure: ﬁrst use SPLICE 1,2 from left to right for all crossings,
then use LOOP 1,2 from right to left in order to eliminate all loops which are present at
this stage. Notice that:
- the order of application of the SPLICE moves does not matter, because they are applied
37

only once per crossing. There is a ﬁnite number of splices, equal to the number of
crossings. Deﬁne the bag of splices SPLICE(A) to be the set of SPLICE moves applied.
- The same is true for the order of eliminations of loops by LOOP 1, 2. There is a
ﬁnite number of loop eliminations, because the number of loops (at this stage) cannot
be bigger than the number of edges of the initial diagram. Deﬁne the bag of loops
LOOP(A) to be the set of all loops which are present after all splices are done.
Let us now check that the reduced diagram does not change if one of the 4 moves is
applied to the initial diagram.
Apply a SPLICE 1,2 move to the initial diagram A, from left to right, and get B. Then
SPLICE(B) is what is left in the bag SPLICE(A) after taking out the respective splice. Also
LOOP(B) = LOOP(A) because of the deﬁnition of bags of loops. Therefore R(A) = R(B).
Apply a SPLICE 1, 2 from right to left to A and get B. Then R(A) = R(B) by the same
proof, with A, B switching places.
Apply a LOOP1, 2 from left to right to A and get B. The new loop introduced in the
diagram does not participate to any crossing (therefore SPLICE(A) = SPLICE(B)), so we
ﬁnd it in the bag of loops of B, which is made by all the elements of LOOP(A) and this new
loop. Therefore R(A) = R(B). Same goes for LOOP1, 2 applied from right to left.
Finally, remark that the reduced diagram of the LHS of the move R2d is diﬀerent than
the reduced diagram of the RHS of the move R2d, therefore the move R2d cannot be achieved
with a sequence of splices and loops addition/elimination.
□
References
[1] A. Bella¨ıche, The tangent space in sub-Riemannian geometry, in: Sub-Riemannian
Geometry, A. Bella¨ıche, J.-J. Risler eds., Progress in Mathematics, 144, Birkh¨auser,
(1996), 4-78
[2] M. Buliga, Dilatation structures I. Fundamentals, J. Gen. Lie Theory Appl., 1 (2007),
2, 65-95. arXiv:math/0608536
[3] M. Buliga, Emergent algebras, arXiv:0907.1520
[4] M. Buliga, Braided spaces with dilations and sub-riemannian symmetric spaces, in:
Geometry. Exploratory Workshop on Diﬀerential Geometry and its Applications, eds.
D. Andrica, S. Moroianu, Cluj-Napoca 2011, 21-35, arXiv:1005.5031
[5] M. Buliga, λ-Scale, a lambda calculus for spaces with dilations, arXiv:1205.0139
[6] M. Buliga, Computing with space:
a tangle formalism for chora and diﬀerence ,
arXiv:1103.6007
[7] M Buliga, Oriented Reidemeister move R2d by splices and loop adding/erasing?, math-
overﬂow question (version: 2013-04-23)
[8] W. Citrin, R. Hall, B. Zorn. Programming with visual expressions. Proceedings of the
11th International IEEE Symposium on Visual Languages, 1995, pp 294-301
[9] M. Erwig, Abstract syntax and semantics of visual languages, Journal of Visual Lan-
guages and Computing 9 (1998), No. 5, 461-483.
[10] R. Fenn, C. Rourke, Racks and Links in codimension two, J. Knot Theory Ramiﬁcations,
1 (1992), no. 4, 343–406
[11] M. Gromov, Carnot-Carath´eodory spaces seen from within, in the book:
Sub-
Riemannian Geometry, A. Bella¨ıche, J.-J. Risler eds., Progress in Mathematics, 144,
Birkh¨auser, (1996), 79-323.
[12] H. Gl¨ockner, Contractible Lie groups over local ﬁelds, Math. Z. 260 (2008), 889-904,
arXiv:0704.3737
38

[13] D. Joyce, A classifying invariant of knots; the knot quandle, J. Pure Appl. Alg., 23
(1982), 37-65
[14] L. Kauﬀman, Knot logic. Knots and applications, 1110, Ser. Knots Everything, 6, World
Sci. Publ., River Edge, NJ, 1995
[15] David C Keenan. To Dissect a Mockingbird: A Graphical Notation for the Lambda
Calculus with Animated Reduction, 2001. http://dkeenan.com/Lambda/
[16] J.J. Koenderink, The brain a geometry engine, Psychological Research, 52 (1990), no.
2-3, 122-127
[17] L.G. Meredith,
D.F. Snyder,
Knots as processes:
a new kind of invariant,
arXiv:1009.2107
[18] P.
Pansu,
M´etriques
de
Carnot-Carath´eodory
et
quasi-isom´etries
des
espaces
sym´etriques de rang un, Ann. of Math., (2) 129, (1989), 1-60.
[19] M. Polyak, Minimal generating sets of Reidemeister moves, arXiv:0908.3127
[20] E. Siebert, Contractive automorphisms on locally compact groups, Math. Z. 191 1986,
Issue 1, pp 73-90.
[21] D.P. Thurston, The algebra of knotted trivalent graphs and Turaevs shadow world
arXiv:math/0311458
39

