Preprint, accepted as a conference paper at COLM 2024
Automata-based constraints for language model decoding
Terry Koo∗, Frederick Liu∗, and Luheng He
Google DeepMind
{terrykoo,frederickliu,luheng}@google.com
Abstract
Language models (LMs) are often expected to generate strings in some
formal language; for example, structured data, API calls, or code snip-
pets. Although LMs can be tuned to improve their adherence to formal
syntax, this does not guarantee conformance, especially with smaller LMs
suitable for large-scale deployment. In addition, tuning requires significant
resources, making it impractical for uncommon or task-specific formats. To
prevent downstream parsing errors we would ideally constrain the LM to
only produce valid output, but this is severely complicated by tokeniza-
tion, which is typically both ambiguous and misaligned with the formal
grammar. We solve these issues through the application of automata theory,
deriving an efficient closed-form solution for the regular languages, a broad
class of formal languages with many practical applications, including API
calls or schema-guided JSON and YAML. We also discuss pragmatic exten-
sions for coping with the issue of high branching factor. Finally, we extend
our techniques to deterministic context-free languages, which similarly admit
an efficient closed-form solution. In spite of its flexibility and representative
power, our approach only requires access to per-token decoding logits and
lowers into simple calculations that are independent of LM size, making it
both efficient and easy to apply to almost any LM architecture.
1
Introduction
A common use case for LMs is generating output in some formal language; for example,
structured data, API calls (Yao et al., 2023), or code snippets (Li et al., 2022). While powerful
LMs often produce syntactically well-formed output, this is not guaranteed. This paper
describes methods for constraining an LM to generate broad classes of formal languages.
One general recipe for applying constraints to any LM is to mask the decoder logits:
1. Based on the current state of the constraint, build a mask of valid next tokens.
2. Penalize the sampling logits using the mask, so only valid tokens are considered.
3. Feed the selected token back to the constraint, to update its state for the next step.
Tokenization is a major problem in this approach. Popular LMs use data-driven sub-
word tokenizers (Sennrich et al., 2016; Kudo & Richardson, 2018) whose segmentations are
generally both ambiguous and misaligned with formal tokens. For example, consider an
API call like foo(x="bar"), which should be lexed as foo ( x = "bar" ). An LM might
tokenize this as foo( x=" ba r "), merging some lexer tokens and splitting others.
Naively, we could force the tokenization to align with the formal syntax, but this can harm
quality (Lundberg, 2023). On the other hand, accepting a token like x=" involves recognizing
a variable name, separator, and the start of a string literal in one step. Mapping a formal
language constraint onto an arbitrary LM tokenizer involves a long tail of such special cases.
We find elegant solutions to these issues in automata theory (Hopcroft et al., 2001; Riley
et al., 2009). Our main contributions are primarily conceptual rather than empirical:
1. Identify an as-yet unnoticed connection between detokenization and transduction.
∗Equal contribution, alphabetical.
1
arXiv:2407.08103v1  [cs.CL]  11 Jul 2024

Preprint, accepted as a conference paper at COLM 2024
0
1
a
2
b
0
1
a
a
0
1
ϵ
2
ϵ
a
3
ϵ
b
ϵ
Figure 1: FSAs that accept ab (left), odd numbers of as (center), and runs of as or bs (right).
States are depicted as circles, with the start state in bold and final states doubled. Edges are
depicted as directed arcs, labeled with the relevant input symbol, or ϵ if none.
0
2
ϵ
4
a
3
a
5
b
1
ϵ
ϵ
ϵ
0
1
a
2
a
3
b
a
Figure 2:
The FSA constructed from the regular expression /a+|ab/ is initially non-
deterministic (left), but can be determinized (right).
2. Solve the tokenization issues using this connection and operations on automata.
3. Define extensions that address practical problems of efficiency and convenience.
2
Finite-state constraints
In this section, we provide some background and then describe our main set of contributions.
2.1
Finite-state automata (FSAs)
A finite-state automaton A is a tuple (Σ, Q, I, F, E) where Σ is a set of input symbols, Q is a
finite set of states, I ∈Q and F ⊆Q are initial and final states, and E ⊆Q × Σϵ × Q, where
Σϵ = Σ ∪{ϵ}, is a set of edges (Hopcroft et al., 2001; Riley et al., 2009). Each edge e ∈E is a
tuple (es, eσ, et) where es and et are its source and target states and eσ is its input label, or ϵ
if none. See Figure 1.
An FSA A accepts w ∈Σ∗if there exist e1, ..., en ∈E such that w = eσ
1...eσ
n, es
1 = I, et
n ∈F, and
es
i = et
i−1 for i > 1; note that n > |w| iff eσ
i = ϵ. To express the functional behavior of FSAs,
we overload notation and define A(w) as a predicate that is true iff A accepts w. For any
FSA A, we define its language LA = {w ∈Σ∗: A(w)} as the set of strings it accepts. More
generally, the regular languages can be defined as {LA : A is an FSA} (Chomsky, 1956).
Conveniently, the regular languages can also be defined by regular expressions1, which are
equivalent to FSAs (Kleene, 1951). Common tools like UNIX grep compile regular expres-
sions into FSAs with Σ = Unicode, which are then used as predicates on text (McNaughton
& Yamada, 1960; Thompson, 1968). See Figure 2 (left).
An FSA is deterministic if ∀e ∈E, eσ ̸= ϵ and ∀q ∈Q, a ∈Σ, |{e ∈E : es = q ∧eσ = a}| ≤1.
Intuitively, outbound edges have unique inputs so executing the FSA is trivial: track the
current state and traverse the edge matching the next input. Surprisingly, non-deterministic
and deterministic FSAs are equivalent. In particular, given an arbitrary FSA A one can build
a deterministic FSA A′ such that LA = LA′ (Rabin & Scott, 1959). See Figure 2.
1In this paper we use the mathematical definition of regular expressions. Many tools extend regular
expressions with non-regular features like backreferences—at the cost of exponential runtime.
2

Preprint, accepted as a conference paper at COLM 2024
0
1
a:x
2
b:ϵ
0
1
a:x
a:o
0
1
ϵ:(
2
ϵ:[
a:a
3
ϵ:)
b:b
ϵ:]
Figure 3: FSTs that transduce ab into x (left), odd numbers of as into xoxo...x (center), and
runs of as or bs into bracketed versions of themselves (right). Edge labels are eσ:eδ.
2.2
Finite-state transducers (FSTs)
A finite-state transducer is an FSA that generates output. Formally, an FST T is a tuple
(Σ, ∆, Q, I, F, E) where Σ, Q, I, and F are as defined for FSAs, ∆is a set of output symbols,
and E ⊆Q × Σϵ × ∆ϵ × Q is a set of edges (Mohri, 1997; 2004; Riley et al., 2009). Each edge
e ∈E is a tuple (es, eσ, eδ, et) where es, eσ, and et are as defined for FSAs and eδ is its output
label, or ϵ if none. See Figure 3.
An FST T transduces w ∈Σ∗into v ∈∆∗if there exist e1, ..., en ∈E such that w = eσ
1...eσ
n,
v = eδ
1...eδ
n, es
1 = I, et
n ∈F, and es
i = et
i−1 for i > 1. Similar to FSAs, we write2 v = T (w) if T
transduces w to v. Critically, the output of one FST can be fed as the input of another FST or
FSA (Riley et al., 2009). Given FSTs T1 and T2 where ∆1 = Σ2, we can compose them into a
new FST T ′ = T2 ◦T1 where Σ′ = Σ1, ∆′ = ∆2, and T ′(w) = T2(T1(w)). Similarly, given
an FST T1 and FSA A2 where ∆1 = Σ2, we can compose them into a new FSA A′ = A2 ◦T1
where Σ′ = Σ1 and A′(w) = A2(T1(w)).
2.3
Detokenization as transduction
Our first contribution is a reformulation of detokenization (i.e., the process of converting
token sequences back into text) as an FST. Given a vocabulary of tokens V, we construct a
detokenizing FST TV as follows:
1. Let ΣV = V, and let ∆V = {vi : v ∈V, 1 ≤i ≤|v|} be the characters of tokens in V.
2. Add a single root state qr, with IV = qr and FV = {qr}.
3. For each token v ∈V:
(a) Let n = |v| and build a chain of n −1 states and edges starting from qr, where
eσ
i = ϵ and eδ
i = vi, reusing existing edges to form a structure isomorphic to a
prefix trie.
(b) Add another edge en that loops back to qr with eσ
n = v and eδ
n = vn.
See Figure 4. TV has one cycle for each v ∈V that accepts v and generates its characters.
Thus, for any token sequence w ∈V∗, TV(w) is precisely the detokenized characters of w.
2.4
Adapting regular expressions to tokens
Our next contribution is a generic method for adapting any FSA from characters to tokens.
Specifically, given a token vocabulary V and an FSA A that accepts character sequences,
A′ = A ◦TV accepts essentially the same language as A, but in token form. More precisely,
for each token sequence w ∈LA′, the detokenization of w is in LA.
Note that the converse does not hold: w ∈LA has a counterpart in LA′ only if w can be
segmented into tokens from V. For example, suppose A accepts any number in hexadecimal
format, but the tokens in V only cover the digits 0-9. Nevertheless, to the extent that LA
can be tokenized by V, strings in LA are represented in LA′. Indeed, due to tokenization
ambiguity, there may be multiple token sequences in LA′ that are equivalent to the same
string in LA. See Figure 5.
2T (w) should be a set because a non-deterministic FST may have multiple paths transducing the
same input to different outputs. In this paper, the FSTs we define satisfy |T (w)| = 1 so we simply
write v = T (w).
3

Preprint, accepted as a conference paper at COLM 2024
Tokens
f
oo
foo
for
food
0
f:f
2
ϵ:f
1
ϵ:o
3
ϵ:o
oo:o
foo:o
for:r
4
ϵ:o
food:d
Figure 4:
A simple vocabulary of tokens (left), and a detokenizing FST that transduces
sequences of those tokens into sequences of characters (right).
0
1
f
2
o
3
o
f
4
d
1
foo
0
f
2
food
oo
Figure 5:
The character-based FSA equivalent to /(foo)+d/ (left) and its composition
with the detokenizing FST from Figure 4 (right). Note that the same text can have many
tokenizations (e.g., foo vs f oo), and tokens are allowed to cross sub-expression boundaries
(e.g., food merges the last repeat of /(foo)+/ with /d/).
We now describe our method for constraining an LM to a regular language. Before decoding:
1. Convert the LM vocabulary V into a detokenizing FST TV.
2. Define a regular expression R that matches the desired output.
3. Compile R into an FSA AR that accepts characters.
4. Adapt AR into an FSA AR◦V = AR ◦TV that accepts tokens.
5. Determinize AR◦V for efficient execution.
Note that AR◦V is a closed-form solution: it expresses R using all relevant tokens from V
and can be executed independently from both. When decoding, initialize q = IR◦V and then
on each decoding step:
1. Read the valid next tokens off the outbound edges: {eσ : ∃e ∈E, es = q}.
2. If q ∈FR◦V, then also treat the end token </s> as valid.
3. Penalize the decoding logits of invalid tokens.
4. After the LM selects a token, update q by traversing the matching edge.
The required operations are indexing, slicing, and basic arithmetic, which are efficient and
simple enough to execute directly on an accelerator with minimal latency overhead. One
pleasing aspect of this solution is how neatly it separates concerns amongst the two halves:
• TV is vocabulary-specific and, while large, can easily be pre-computed for each LM.
• AR is vocabulary-agnostic, easily specified, and portable across different LMs.
This clean decomposition is only possible because FST-FSA composition provides a fast,
automatic, and general method for joining the two halves.
2.5
Extensions
Our last contribution in this section is a set of regular expression extensions, written as
specially-named capturing groups, that greatly increase the efficiency and expressiveness of
the system. We describe some illustrative examples below and list others in Table 1.
2.5.1
Wildcard matching
One problem in our approach occurs with ”wildcard” matches like /./ or /[^0-9]/ that
match nearly any character. After composition with TV, some states in the resulting FSA will
4

Preprint, accepted as a conference paper at COLM 2024
Name
Description
QUOTED TEXT
Matches a quoted string with backslash escapes.
UNQUOTED TEXT
Matches a YAML-style non-quoted string.
IMAGE
Matches an image generated by a multi-modal LM.
TEXT TOKEN
Matches a single text token.
PARAGRAPH TOKEN
Matches a single text token with no newlines.
TEXT UNTIL
Matches text tokens until a stop phrase appears.
SUBSTRING OF
Matches any substring of a given string.
DELIMITED LIST
Matches a delimited list of homogeneous items.
DELIMITED SUBSEQUENCE OF
Matches a delimited subset of a heterogeneous list.
Table 1: A sampling of available extensions. Those above the line are wildcard matchers
(see Section 2.5.1), while those below are syntactic sugar (see Section 2.5.2)
.
have almost |V| outbound edges, making it expensive to use—keep in mind that |V| > 100k
for commonly-used LMs (Willard & Louf, 2023, Section 3 describes a similar issue).
We mitigate this issue by defining terminal labels, which are token IDs disjoint from V that
map to pre-computed masks of valid tokens. For example, the specially-named captur-
ing group /(?P<PARAGRAPH TOKEN>)/ parses into a terminal edge whose mask indicates
newline-free tokens. This is useful for structuring free text: for example,
/Summary:(\n\* (?P<PARAGRAPH TOKEN>)+){3,5}/
would match the heading ”Summary:” followed by three to five bullets. When penalizing
logits, terminal masks are applied en masse. When updating state, if the selected token
matches a terminal mask we traverse that terminal edge.
Note that a normal token edge and terminal edge can both match the sampled token,
leading to ambiguity about which to traverse. Similarly, two terminal edges can match the
sampled token. We address this with a simple heuristic: prefer normal token edges when
available, and otherwise follow a semi-arbitrary precedence ordering over terminal labels.
This heuristic has worked well because terminals are generally used for wildcard matches,
and in practice wildcards are typically abutted by disjoint delimiting expressions. For
example, in the bulleted list constraint above, the /(?P<PARAGRAPH TOKEN>)+/ expression is
bounded by a newline.
2.5.2
Syntactic sugar
Sometimes, a constraint is inefficient to define as a regular expression. For example, suppose
we wish to reduce hallucination by forcing the LM to generate a substring of some reference
u, such as a user utterance. Unfortunately, the regular expression for substrings of u is
O(|u|2), even though the corresponding FSA is O(|u|)—this can be viewed as a defect of
the specification language.
We therefore define the syntactic sugar /(?P<SUBSTRING OF>abc)/ to match any substring
of ”abc”, resolving the defect by providing a O(|u|) specification syntax. Besides improving
usability, these extensions mitigate the explosion in regular expression size that can occur in
complex applications like JSON constraints. In some cases, growth in the regular expression
reflects growth in the resulting FSA, but we have found that they diverge in some important
practical use cases.
3
Push-down constraints
This section briefly presents our second set of contributions, describing how our system is
easily extended to grammar-based constraints using push-down automata. We begin with
some background on PDAs and then describe our contributions.
5

Preprint, accepted as a conference paper at COLM 2024
Rules
S -> /ab/
S -> /a/ S /b/
3
0
1
a
[•
a
(S → /a/ S • /b/
2
b
•]
b
S → /a/ S • /b/)
Figure 6: A grammar for the canonical context-free language anbn (left), and an equivalent
PDA (right). Edge labels have eσ on top, then e↑marked with ) and e↓marked with (.
The stack symbols are Earley-style dotted rules (Earley, 1970) denoting ”return addresses”
(Allauzen & Riley, 2012, Section 4.5). The initial stack symbol S is written as a dot marked
with square brackets [ and ].
Tokens
a
b
bb
aaab
0
a:a
b:b
2
ϵ:a
1
ϵ:b
3
ϵ:a
bb:b
4
ϵ:a
aaab:b
3
0
1
[a
2
[a(a(ab
(a
b
b)b
(a(a(ab
]
 )b
)b)b
Figure 7: A simple vocabulary of tokens (left), the detokenizing FST built from it (center),
and its composition with the PDA from Figure 6 (right). Note that edges are allowed to
cross the boundaries of terminals and non-terminals, allowing them to push or pop multiple
stack symbols. For legibility, the stack symbols have been simplified into single brackets
that are interleaved with the characters of the token.
3.1
Push-down automata (PDAs)
A push-down automaton can be viewed as an FSA equipped with a stack. Formally, a PDA P
is a tuple (Σ, Π, S, Q, I, F, E) where Σ, Q, I, and F are as defined for FSAs, Π is a set of stack
symbols, S ∈Π is a unique initial stack symbol, and E ⊆Q × Σϵ × Π∗× Q × Π∗is a set of
edges (Hopcroft et al., 2001). Each edge e ∈E is a tuple (es, eσ, e↑, et, e↓) where es, eσ, and et
are as defined for FSAs, and e↑and e↓are popped and pushed stack symbols. See Figure 6.
P accepts w ∈Σ∗if there exist e1, ..., en ∈E such that w = eσ
1...eσ
n, es
1 = I, et
n ∈F, es
i = et
i−1
for i > 1, and e↑
i and e↓
i form a coherent sequence of stack edits. Formally, let si be the stack
before ei where s1 = S, ni = |si|, mi = ni −|e↑
i |, and si+1 = si
1:mie↓
i, then e↑
i = si
mi+1:ni must
hold. In other words, at each step e↑
i must match the suffix of si, and we update the stack by
popping e↑
i off and pushing e↓
i on. As above, we write P(w) to indicate that P accepts w.
Unlike FSAs, non-deterministic and deterministic PDAs are not equivalent: the former
accept the context-free languages, which are also defined by context-free grammars (Chomsky,
1956; Hopcroft et al., 2001), while the latter accept a strict subset, the deterministic context-
free languages (Ginsburg & Greibach, 1966). This subset nevertheless includes all regular
languages and the LL(k) and LR(k) languages (Knuth, 1965), covering the syntax of most
programming languages.
As a result of this non-equivalence, there is no general algorithm to determinize an arbitrary
PDA, as we had for FSAs. Instead, as LL(k) and LR(k) parsers do, our system detects
non-deterministic grammars and signals an error, leaving it to the author of the grammar to
rewrite the grammar in a deterministic manner.
3.2
Adapting grammars to tokens
Although PDAs are more expressive than FSAs, they behave similarly in many ways and,
crucically, FSTs and PDAs are composable (Allauzen & Riley, 2012). Formally, given an FST
T1 and PDA P2 where ∆1 = Σ2, we can compose them into a new PDA P′ = P2 ◦T1 where
Σ′ = Σ1 and P′(w) = P2(T1(w)). As with FSAs, composition with the detokenizing FST
TV adapts a character-based PDA into one that accepts tokens in V. See Figure 7.
6

Preprint, accepted as a conference paper at COLM 2024
This allows us to reuse the method from Section 2.4 with minimal modification:
1. Convert the LM vocabulary V into a detokenizing FST TV.
2. Define the constraints as a deterministic grammar G.
3. Compile G into a character-based PDA PG (Allauzen & Riley, 2012, Section 4.5).
4. Adapt PG into a PDA PG◦V = PG ◦TV that accepts tokens.
When decoding, we apply PG◦V similarly to AR◦V, except we maintain a stack s alongside
q, filter edges by compatibility between e↑and s, and update s according to e↑and e↓.
It’s worth re-emphasizing the benefits of formulating detokenization as an FST. By repre-
senting the entire task in terms of automata, our approach easily generalizes from regular
expressions to grammars while preserving many desirable features. For example, just as
FST-FSA composition enables tokens to cross sub-expression boundaries in the regular
expression (see Figure 5), FST-PDA composition enables tokens to cross (non-)terminal
boundaries in the grammar (see Figure 7).
4
Related work
The most relevant work is Outlines (Willard & Louf, 2023), which also constrains using FSAs
and PDAs—our approach was independently developed in early 2023. Their approach
is based on a bespoke ”indexing” operation that must be manually extended to new use
cases3. In contrast, our core innovation recasts the entire process in automata-theoretic
terms, making it much easier to apply FSAs and generalize to PDAs. Another difference is
that we define extensions to support wildcard matching and improve usability.
Relatedly, Yin et al. (2024) extended Outlines with the ability to ”compress” runs of text
into prefills. This technique is of significant practical interest and could be adapted to our
approach as our techniques appear to be largely orthogonal.
SynCode (Ugare et al., 2024) is a more recent system that also exploits FSAs, but handles
grammars using LALR(1) and LR(1) parsers rather than PDAs. Like Outlines, their approach
relies on a bespoke algorithm; in this case, they speculatively unroll future lexer tokens
to allow LM tokens to cross multiple lexer tokens. This introduces significant complexity
relative to our purely automata-theoretic approach, and deciding how many lexer tokens to
unroll is a trade-off between computational cost and completeness.
Some less closely-related approaches still constrain by masking logits, but dynamically
match the vocabulary on each step instead of devising a method to statically pre-compute
these matches, as we and the systems above do. For example, Synchromesh (Poesia et al.,
2022) iterates the entire LM vocabulary on every decoding step, making it impractical
with current LMs. Guidance (Microsoft, 2023) improves upon this with a cached trie, and
grammar-constrained decoding (Geng et al., 2023) adds expressive power beyond context-
free. While these latter two are more flexible than our approach, they are less efficient and
harder to deploy at scale.
Finally, some other approaches no longer predictively mask logits, but instead sample un-
constrained continuations and reject invalid ones post-hoc. For example, PICARD (Scholak
et al., 2021) converts the top-k tokens to text and performs various levels of validation.
Lew et al. (2023) do sequential Monte-Carlo sampling that, in the case of hard constraints,
reduces to something like rejection sampling with rescoring. These approaches are quite
flexible, but may have issues when the constraints require the LM to select a low-probability
token. In addition, note that post-hoc filtering is orthogonal to predictive masking, so our
approaches could be merged.
5
Applications
The clean design and efficiency of our approach (see Section 2.4 and Section 3.2) enable a
number of different applications. We present several illustrative examples here.
3See, e.g., https://github.com/outlines-dev/outlines/issues/684
7

Preprint, accepted as a conference paper at COLM 2024
5.1
JSON expressions
JSON (Pezoa et al., 2016) is a widely-used structured data format that LMs are often expected
to generate. We study the problem of generating JSON that conforms to a schema, which is a
mapping from field name to type. Field types range from simple primitives like booleans,
numbers, and strings, to sub-objects based on sub-schemas, and arrays or unions of other
types. Field values can be nullable, meaning that they can be substituted with null, and
fields can be marked optional, meaning that both the field name and value may be omitted.
Although the set of all JSON expressions of any kind is a context-free language, somewhat
surprisingly the set of JSON expressions conforming to a particular schema is a regular
language, as the schema limits recursion. It is difficult to manually write the regular expres-
sion corresponding to a particular schema: while leaf-level matchers like /(true|false)/
are simple enough, complexity grows quickly as they are composed into objects, arrays,
and unions. For user convenience, we have implemented tools that automatically translate
schemas into regular expressions.
We have also explored schema-free JSON, which as mentioned above is context-free. A
simple approach is to impose constant limits kO and kA on object and array nesting, which
reduces from context-free to regular, and we have implemented tools to generate regular
expressions for this form of depth-truncated JSON. Naturally, one can also directly convert
the JSON grammar into a PDA-based constraint.
5.2
Python dataclasses
Another case study is constraining LMs to output Python dataclasses (Van Rossum & Drake,
2009). Like the JSON schemas described above, a Python dataclass defines a mapping from
field names to types, and we have implemented similar tooling that reflects on a dataclass
to automatically build a regular expression matching constructor calls. This enables a
particularly convenient API where one can use a dataclass type T to build a constraint, and
then parse the constrained LM response back into an instance of T.
5.3
Speculative decoding
Speculative decoding (Leviathan et al., 2023) is a method for speeding up LM inference
via speculative execution. Short runs of tokens are sampled from a faster approximation
model, and then verified with a more accurate target model that accepts some prefix of
the sampled tokens. Although the system now runs two LMs instead of one, significant
latency reductions can still be achieved due to batching and parallelization of the target
model. The speedup is largely determined by the acceptance rate, which is the proportion of
speculatively-sampled tokens that pass verification.
The approximation model is typically a smaller LM that has disproportionately greater
difficulties following formal syntax, so a natural application is to constrain the approxima-
tion model and increase its acceptance rate on formal language outputs. There are several
complications to address. For one, the constraint must be ”rewindable” because the target
model can reject some of the speculated tokens, forcing the approximation model to restart
at an earlier point. This can be accomplished by tracking a full history of states instead of
just one. In addition, the logits of the target model must also be penalized, or the divergence
between the two will reduce the acceptance rate and may even allow the target model to
resample invalid tokens. To avoid re-evaluating the constraints, we devised a method for
sharing the penalties with the target model.
6
Conclusion and future work
In this paper, we described a system for constraining LM decoding. Our key contribution
is a reformulation of detokenization as an FST, which enables our other contributions by
bringing the entire task of constrained decoding into the domain of automata. Although the
8

Preprint, accepted as a conference paper at COLM 2024
problems raised by ambiguous and misaligned tokenizations are quite thorny, we derive
simple and elegant solutions by leveraging the considerable toolkit of automata theory.
In future work, we would like to explore PDAs more deeply, as thus far we have focused
on FSAs because they appeared to be the more robust and scalable option. One particular
issue is that the grammar must be written carefully to avoid yielding a non-deterministic
PDA. This is a well-studied problem, and there are similar limitations regarding grammar
specification in parsers for determinstic context-free languages: as a simple example, an
LL(k) parser cannot parse a left-recursive grammar. There is a deep literature characterizing
deterministic grammars and PDAs that seems like it would have many useful insights for
avoiding unnecessary non-determinism (Greibach, 1965; Koch & Blum, 1997; Harrison &
Havel, 1973; Havel & Harrison, 1974; Geller et al., 1976, among others).
Other avenues of exploration for PDAs include use cases and efficiency. When decoding
from an LM, one generally specifies a finite maximum budget of decoded tokens, and
that limit shrinks the gap between context-free and regular languages. For example, if we
decode at most 100 characters then the language anbn is no longer context-free, since we can
bound n ≤50. It may be interesting to investigate longer-running, possibly multi-phase
constraint use cases where PDAs can provide more benefit. On the other hand, even for
shorter outputs where FSAs are competitive, PDAs might still provide value because they
are generally more compact than equivalent FSAs.
References
Cyril Allauzen and Michael Riley. A pushdown transducer extension for the openfst
library. In Proceedings of the 17th International Conference on Implementation and Applica-
tion of Automata, CIAA’12, pp. 66–77, Berlin, Heidelberg, 2012. Springer-Verlag. ISBN
9783642316050. doi: 10.1007/978-3-642-31606-7 6. URL https://doi.org/10.1007/
978-3-642-31606-7_6.
Noam Chomsky. Three models for the description of language. IRE Transactions on Informa-
tion Theory, 2(3):113–124, 1956. doi: 10.1109/TIT.1956.1056813.
Jay Earley. An efficient context-free parsing algorithm. Commun. ACM, 13(2):94–102, feb
1970. ISSN 0001-0782. doi: 10.1145/362007.362035. URL https://doi.org/10.1145/
362007.362035.
Matthew M. Geller, Michael A. Harrison, and Ivan M. Havel. Normal forms of deterministic
grammars. Discrete Mathematics, 16(4):313–321, 1976. ISSN 0012-365X. doi: https://doi.
org/10.1016/S0012-365X(76)80004-0. URL https://www.sciencedirect.com/science/
article/pii/S0012365X76800040.
Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. Grammar-constrained
decoding for structured NLP tasks without finetuning. In Houda Bouamor, Juan Pino, and
Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, pp. 10932–10952, Singapore, December 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.emnlp-main.674. URL https://aclanthology.org/
2023.emnlp-main.674.
Seymour Ginsburg and Sheila Greibach. Deterministic context free languages. Informa-
tion and Control, 9(6):620–648, 1966. ISSN 0019-9958. doi: https://doi.org/10.1016/
S0019-9958(66)80019-0. URL https://www.sciencedirect.com/science/article/pii/
S0019995866800190.
Sheila A. Greibach. A new normal-form theorem for context-free phrase structure grammars.
J. ACM, 12(1):42–52, jan 1965. ISSN 0004-5411. doi: 10.1145/321250.321254. URL https:
//doi.org/10.1145/321250.321254.
Michael A. Harrison and Ivan M. Havel.
Strict deterministic grammars.
Journal of
Computer and System Sciences, 7(3):237–277, 1973. ISSN 0022-0000. doi: https://doi.
org/10.1016/S0022-0000(73)80008-X. URL https://www.sciencedirect.com/science/
article/pii/S002200007380008X.
9

Preprint, accepted as a conference paper at COLM 2024
Ivan M. Havel and Michael A. Harrison. On the parsing of deterministic languages. J.
ACM, 21(4):525–548, oct 1974. ISSN 0004-5411. doi: 10.1145/321850.321851. URL https:
//doi.org/10.1145/321850.321851.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. Introduction to automata theory,
languages, and computation, 2nd edition. ACM Press, New York, NY, USA, 2001. doi:
http://doi.acm.org/10.1145/568438.568455.
Stephen Cole Kleene. Representation of events in nerve nets and finite automata. Technical
Report RAND-RM-704, The RAND Corporation, 1951. URL https://www.rand.org/
content/dam/rand/pubs/research_memoranda/2008/RM704.pdf.
Donald E. Knuth. On the translation of languages from left to right. Information and Control,
8(6):607–639, 1965. ISSN 0019-9958. doi: https://doi.org/10.1016/S0019-9958(65)90426-2.
URL https://www.sciencedirect.com/science/article/pii/S0019995865904262.
Robert Koch and Norbert Blum. Greibach normal form transformation, revisited. In
R¨udiger Reischuk and Michel Morvan (eds.), STACS 97, pp. 47–54, Berlin, Heidelberg,
1997. Springer Berlin Heidelberg. ISBN 978-3-540-68342-1.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent
subword tokenizer and detokenizer for neural text processing. CoRR, abs/1808.06226,
2018. URL http://arxiv.org/abs/1808.06226.
Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via
speculative decoding. In International Conference on Machine Learning, pp. 19274–19286.
PMLR, 2023.
Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash K. Mansinghka. Sequential
monte carlo steering of large language models using probabilistic programs, 2023.
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter
Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang,
Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,
Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and
Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):
1092–1097, 2022. doi: 10.1126/science.abq1158. URL https://www.science.org/doi/
abs/10.1126/science.abq1158.
Scott Lundberg.
The art of prompt design:
Prompt boundaries and token heal-
ing.
Towards
Data
Science,
2023.
URL
https://towardsdatascience.com/
the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38.
R. McNaughton and H. Yamada. Regular expressions and state graphs for automata. IRE
Transactions on Electronic Computers, EC-9(1):39–47, 1960. doi: 10.1109/TEC.1960.5221603.
Microsoft. Guidance, 2023. URL https://github.com/guidance-ai/guidance.
Mehryar Mohri. Finite-state transducers in language and speech processing. Computational
Linguistics, 23(2):269–311, 1997. URL https://aclanthology.org/J97-2003.
Mehryar Mohri. Weighted Finite-State Transducer Algorithms. An Overview, pp. 551–563.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-39886-8. doi: 10.
1007/978-3-540-39886-8 29. URL https://doi.org/10.1007/978-3-540-39886-8_29.
Felipe Pezoa, Juan L. Reutter, Fernando Suarez, Mart´ın Ugarte, and Domagoj Vrgoˇc. Foun-
dations of json schema. In Proceedings of the 25th International Conference on World Wide
Web, WWW ’16, pp. 263–273, Republic and Canton of Geneva, CHE, 2016. Interna-
tional World Wide Web Conferences Steering Committee. ISBN 9781450341431. doi:
10.1145/2872427.2883029. URL https://doi.org/10.1145/2872427.2883029.
10

Preprint, accepted as a conference paper at COLM 2024
Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek,
and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language
models. In International Conference on Learning Representations, 2022. URL https://
openreview.net/forum?id=KmtVD97J43e.
M. O. Rabin and D. Scott. Finite automata and their decision problems. IBM Journal of
Research and Development, 3(2):114–125, 1959. doi: 10.1147/rd.32.0114.
Michael Riley, Cyril Allauzen, and Martin Jansche. OpenFst: An open-source, weighted
finite-state transducer library and its applications to speech and language. In Ciprian
Chelba, Paul Kantor, and Brian Roark (eds.), Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American Chapter of the Association for Computational
Linguistics, Companion Volume: Tutorial Abstracts, pp. 9–10, Boulder, Colorado, May 2009.
Association for Computational Linguistics. URL https://aclanthology.org/N09-4005.
Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incrementally
for constrained auto-regressive decoding from language models. In Marie-Francine
Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing, pp. 9895–9901, Online
and Punta Cana, Dominican Republic, November 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.emnlp-main.779. URL https://aclanthology.org/
2021.emnlp-main.779.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare
words with subword units. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
1715–1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:
10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162.
Ken Thompson. Programming techniques: Regular expression search algorithm. Commun.
ACM, 11(6):419–422, jun 1968. ISSN 0001-0782. doi: 10.1145/363347.363387. URL https:
//doi.org/10.1145/363347.363387.
Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh.
Improving llm code generation with grammar augmentation, 2024.
Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley,
CA, 2009. ISBN 1441412697.
Brandon T. Willard and R´emi Louf. Efficient guided generation for large language models,
2023.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and
Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh
International Conference on Learning Representations, 2023. URL https://openreview.net/
forum?id=WE_vluYUL-X.
Liangsheng Yin, Ying Sheng, and Lianmin Zheng.
Fast JSON Decoding for Local
LLMs with Compressed Finite State Machine, 2024. URL https://lmsys.org/blog/
2024-02-05-compressed-fsm/.
11

