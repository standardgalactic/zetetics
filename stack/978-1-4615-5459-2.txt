
THE INFORMATIONAL COMPLEXITY OF 
LEARNING 
Perspectives on Neural Networks 
and Generative Grammar 

THE INFORMATIONAL COMPLEXITY OF 
LEARNING 
Perspectives on Neural Networks 
and Generative Grammar 
PARTHA NIYOGI 
Massachusetts Institute of Technology 
Cambridge. MA 
Springer Science+Business Media, LLC 

Library of Congress Cataloging-in-Publication Data 
A C.I.P. Catalogue record for this book is avaiJable 
from the Library of Congress. 
ISBN 978-1-4613-7493-0 
ISBN 978-1-4615-5459-2 (eBook) 
DOI 10.1007/978-1-4615-5459-2 
Copyright © 1998 Springer Science+Business Media New York 
Originally published by Kluwer Academic Publishersin 1998 
Softcover reprint ofthe hardcover Ist edition 1998 
AII rights reserved. No part ofthis publication may be reproduced, stored in a retrieval 
system or transmitted in any form or by any means, mechanical, photocopying, 
recording, or otherwise, without the prior written permission ofthe publisher, 
Springer Science+Business Media, LLC. 
Printed on acid-free pa per. 

Contents 
List of Figures 
Foreword 
Preface 
Acknowledgments 
1. INTRODUCTION 
1.1 
The Components of a Learning Paradigm 
1.1.1 
Concepts, Hypotheses, and Learners 
1.1.2 
Generalization, Learnability, Successful learning 
1.1.3 
Informational Com plexity 
1.2 
Parametric Hypothesis Spaces 
1.3 
Technical Contents and Major Contributions 
1.3.1 
A Final Word 
2. GENERALIZATION ERROR FOR NEURAL NETS 
2.1 
Introduction 
2.2 
Definitions and Statement of the Problem 
2.2.1 
Random Variables and Probability Distributions 
2.2.2 
Learning from Examples and Estimators 
2.2.3 
The Expected Risk and the Regression Function 
2.2.4 
The Em pirical Risk 
2.2.5 
The Problem 
2.2.6 
Bounding the Generalization Error 
2.2.7 
A Note on Models and Model Complexity 
2.3 
Stating the Problem for Radial Basis Functions 
2.4 
Main Result 
2.5 
Remarks 
2.5.1 
Observations on the Main Result 
2.5.2 
Extensions 
2.5.3 
Connections with Other Results 
2.6 
Implications of the Theorem in Practice: Putting In the Numbers 
2.6.1 
Rate of Growth of n for Guaranteed Convergence 
2.6.2 
Optimal Choice of n 
2.6.3 
Experiments 
ix 
xv 
xix 
xxi 
1 
3 
3 
6 
7 
11 
13 
19 
21 
21 
24 
24 
25 
26 
28 
28 
30 
33 
34 
36 
36 
36 
37 
39 
40 
40 
41 
45 
v 

VI 
INFORMATIONAL COMPLEXITY OF LEARNING 
2.7 
Conclusion 
2-A Notations 
2-B A Useful Decomposition of the Expected Risk 
2-C A Useful Inequality 
2-D Proof of the Main Theorem 
2-0.1 Bounding the approximation error 
2-0.2 Bounding the estimation error 
2-0.3 Bounding the generalization error 
3. ACTIVE LEARNING 
50 
50 
56 
56 
57 
58 
60 
72 
75 
3.1 
A General Framework For Active Approximation 
77 
3.1.1 
Preliminaries 
77 
3.1.2 
The Problem of Collecting Examples 
80 
3.1.3 
In Context 
83 
3.2 
Example 1: A Class of Monotonically Increasing Bounded Functions 
86 
3.2.1 
Lower Bound for Passive Learning 
87 
3.2.2 
Active Learning Algorithms 
88 
3.2.2.1 
Derivation of an optimal sampling strategy 
88 
3.2.3 
Empirical Simulations, and other Investigations 
94 
3.2.3.1 
Distribution of Points Selected 
94 
3.2.3.2 
Classical Optimal Recovery 
95 
3.2.3.3 
Error Rates and Sample Complexities for some Arbitrary 
Functions: Some Simulations 
97 
3.3 
Exam pie 2: A Class of Functions with Bounded First Derivative 
100 
3.3.1 
Lower Bounds 
102 
3.3.2 
Active Learning Algorithms 
3.3.2.1 
Derivation of an optimal sampling strategy 
3.3.3 
Some Simulations 
3.3.3.1 
Distribution of points selected 
3.3.3.2 
Error Rates: 
3.4 
Conclusions, Extensions, and Open Problems 
3.5 
A Simple Example 
3.6 
Generalizations 
3.6.1 
Localized Function Classes 
3.6.2 
The General (-focusing strategy; 
3.6.3 
Generalizations and Open Problems 
4. LANGUAGE LEARNING 
105 
105 
110 
110 
113 
115 
117 
119 
119 
120 
122 
125 
4.1 
Language Learning and The Poverty of Stim ulus 
126 
4.2 
Constrained Gram mars-Principles and Parameters 
128 
4.2.1 
Example: A 3-parameter System from Syntax 
129 
4.2.2 
Example: Parameterized Metrical Stress in Phonology 
132 
4.3 
Learning in the Principles and Parameters Framework 
134 
4.4 
Formal Analysis of the Triggering Learning Algorithm 
137 
4.4.1 
Background 
138 
4.4.2 
The Markov formulation 
139 
4.4.2.1 
Parameterized Grammars and their Corresponding Markov 
Chains 
139 

4.5 
4.6 
4.7 
Contents 
Vll 
4.4.2.2 
Markov Chain Criteria for Learnability 
140 
4.4.2.3 
The Markov chain for the 3-parameter Example 
143 
4.4.3 
Derivation of the transition probabilities for the Markov TLA structure 
145 
4.4.3.1 
4.4.3.2 
Form alization 
Additional Properties of the Learning System 
Characterizing Convergence Times for the Markov Chain Model 
4.5.1 
Some Transition Matrices and Their Convergence Curves 
4.5.2 
Absorption Times 
4.5.3 
Eigenvalue Rates of Convergence 
4.5.3.1 
Eigenvalues and Eigenvectors 
4.5.3.2 
Representation of Tk 
4.5.3.3 
Initial Conditions and Limiting Distributions 
4.5.3.4 
Rate of Convergence 
4.5.3.5 
Transition Matrix Recipes: 
Exploring Other Points 
4.6.1 
Changing the Algorithm 
4.6.2 
Distributional Assum ptions 
4.6.3 
Natural Distributions-CHILDES CORPUS 
Batch Learning Upper and Lower Bounds: An Aside 
145 
147 
148 
148 
152 
153 
153 
154 
155 
156 
156 
157 
157 
159 
160 
4.8 
Conclusions, Open Questions, and Future Directions 
4-A Unem bedded Sentences For Parametric Gram mars 
4-B Memoryless Algorithms and Markov Chains 
162 
164 
167 
167 
168 
168 
169 
4-C Proof of Learnability Theorem 
4-C.1 
Markov state terminology 
4-C.2 
Canonical Decomposition 
4-D Formal Proof 
5. LANGUAGE CHANGE 
5.1 
Introduction 
5.2 
Language Change in Parametric Systems 
170 
173 
173 
181 
5.3 
Exam pie 1: A Three Parameter System 
182 
5.3.1 
Starting with Homogeneous Populations: 
183 
5.3.1.1 
A = TLA; Pi = Uniform; Finite Sample = 128 
183 
5.3.1.2 
A = Greedy, No S.v.; Pi = Uniform; Finite Sample = 128 186 
5.3.1.3 
A = a) R.W. b) S. V. only; Pi = Uniform; Finite Sample 
= 128 
187 
5.3.1.4 
Rates of Change 
188 
5.3.2 
Non-homogeneous Populations: Phase-Space Plots 
192 
5.3.2.1 
Phase-Space Plots: Grammatical Trajectories 
193 
5.3.2.2 
Issues of Stability 
194 
5.4 
Example 2: The Case of Modern French: 
196 
5.4.1 
The Parametric Subspace and Data 
197 
5.4.2 
The Case of Diachronic Syntax Change in French 
198 
5.4.3 
Some Dynamical System Simulations 
199 
5.4.3.1 
Homogeneous Populations [Initial-Old French] 
199 
5.4.3.2 
Heterogeneous Populations (Mixtures) 
201 
5.5 
Conclusions 
203 

Vlll 
INFORMATIONAL COMPLEXITY OF LEARNING 
6. CONCLUSIONS 
6.1 
Emergent Themes 
6.2 
Extensions 
6.3 
A Concluding Note 
References 
207 
208 
210 
212 
213 

List of Figures 
1.1 
The space of possibilities. The various factors which affect the 
informational complexity of learning from examples. 
10 
1.2 
The structure of a Hyper Basis Function Network (same as 
regularization network). 
12 
1.3 
Parametric difference in phrase structure between English and 
Bengali on the basis of the parameter P2. 
14 
1.4 
Analysis of the English sentence "with one hand" according 
to its parameterized X-bar grammar. 
15 
1.5 
Analysis of the Bengali sentence "ek haath diye" a literal 
translation of "with one hand" according to its parameterized 
X-bar grammar. Notice the difference in word order. 
16 
2.1 
This figure shows a picture of the problem. The outermost 
circle represents the set F. Embedded in this are the nested 
subsets, the Hn's. 10 is an arbit~ary target function in :F, In is 
the closest element of Hn and In,1 is the element of Hn which 
the learner hypothesizes on the basis of data. 
33 
2.2 
Bound on the generalization error as a function of the number 
of basis functions n keeping the sample size 1 fixed. This has 
been plotted for a few different choices of sample size. Notice 
how the generalization error goes through a minimum for a 
certain value of n. This would be an appropriate choice for the 
given (constant) data complexity. Note also that the minimum 
is broader for larger 1, that is, an accurate choice of n is less 
critical when plenty of data is available. 
42 
2.3 
The bound on the generalization error as a function of the 
number of examples for different choices of the rate at which 
network size n increases with sample size 1. Notice that if 
n = 1, then the estimator is not guaranteed to converge, i.e., 
the bound on the generalization error diverges. While this is a 
distribution free-upper bound, we need distribution-free lower 
bounds as well to make the stronger claim that n = 1 will 
never converge. 
44 
IX 

x 
INFORMATIONAL COMPLEXITY OF LEARNING 
2.4 
This figures shows various choices of (I, n) which give the same 
generalization error. The x-axis has been plotted on a log 
scale. The interesting observation is that there are an infinite 
number of choices for number of basis functions and number 
of data points all of which would guarantee the same general-
ization error (in terms of its worst case bound). 
45 
2.5 
The generalization error as a function of number of examples 
keeping the number of basis functions (n) fixed. This has 
been done for several choices of n. As the number of examples 
increases to infinity the generalization error asymptotes to a 
minimum which is not the Bayes error rate because of finite 
hypothesis complexity (finite n). 
46 
2.6 
The generalization error, the number of examples (I) and the 
number of basis functions (n) as a function of each other. 
47 
2.7 
The generalization error is plotted as a function of the number 
of nodes of an RBF network (10) trained on 100 data points of 
a function of the type (16) in 2 dimensions. For each number 
of parameters 10 results, corresponding to 10 different local 
minima, are reported. The continuous lines above the experi-
mental data represents the bound ~ + b[( nk In( nl) -In 6) / ~ 1/2 
of eq. (14), in which the parameters a and b have been esti-
mated empirically, and 6 = 10-6 • 
48 
2.8 
Everything is as in figure (6), but here the dimensionality is 6 
and the number of data points is 150. As before, the param-
eters a and b have been estimated empirically and 6 = 10-6 . 
Notice that this time the curve passes through some of the 
data points. However, we recall that the bound indicated by 
the curve holds under the assumption that the global minimum 
has been found, and that the data points represent different 
local minima. Clearly in the figure the curve bounds the best 
of the local minima. 
49 
2.9 
If the distance between 1[ln] and [[in,z] is larger than 2(, the 
condition [emp[in,,] ::; [emp[ln] is violated. 
57 
3.1 
An arbitrary data set fitted with cubic splines 
78 
3.2 
A depiction of the situation for an arbitrary data set. The 
set :F'D consists of all functions lying in the boxes and passing 
through the datapoints (for example, the dotted lines). The 
approximating function h is a linear interpolant shown by a 
solid line. 
89 
3.3 
Zoomed version of interval. The maximum error the approx-
imation scheme could have is indicated by the shaded region. 
This happens when the adversary claims the target function 
had the value Y' throughout the interval. 
89 

LIST OF FIGURES 
xi 
3.4 
The situation when the interval Ci is sampled yielding a new 
data point. This subdivides the interval into two subintervals 
and the two shaded boxes indicate the new constraints on the 
function. 
91 
3.5 
How the CLA chooses its examples. Vertical lines have been 
drawn to mark the x-coordinates of the points at which the 
algorithm asks for the value of the function. 
95 
3.6 
The dotted line shows the density of the samples along the 
x-axis when the target was the monotone-function of the pre-
vious example. The bold line is a plot of the derivative of the 
function. Notice the correlation between the two. 
96 
3.7 
The situation when a function f E :F is picked, n sample 
points (the z's) are chosen and the corresponding y values are 
obtained. Each choice of sample points corresponds to a choice 
of the a's. Each choice of a function corresponds to a choice 
of the b's. 
97 
3.8 
Error rates as a function of the number of examples for the 
arbitrary monotone function shown in a previous figure. 
98 
3.9 
Four other monotonic functions on which simulations have 
been run comparing random, uniform, and active sampling 
strategies. 
99 
3.10 
This figure plots the log of the error (L1 error) against N 
the number of examples for each of the 4 monotonic functions 
shown in fig. 
101 
3.11 
Construction of a function satisying Lemma 2. 
103 
3.12 
An arbitrary data set for the case offunctions with a bounded 
derivative. The functions in 1=1) are constrained to lie in the 
parallelograms as shown. The slopes of the lines making up 
the parallelogram are d and -d appropriately. 
105 
3.13 
A zoomed version of the ith interval. 
106 
3.14 
Subdivision of the ith interval when a new data point is obtained. 107 
3.15 
A figure to help the visualization of Lemma 4. For the z 
shown, the set :F1) is the set of all values which lie within the 
parallelogram corresponding to this z, i.e., on the vertical line 
drawn at x but within the parallelogram. 
108 
3.16 
Four functions with bounded derivative considered in the sim-
ulations. The uniform bound on the derivative was chosen to 
be d = 10. 
111 
3.17 
How CLA-2 chooses to sample its points. Vertical lines have 
been drawn at the x values where the CLA queried the oracle 
for the corresponding function value. 
112 
3.18 
How CLA-2 chooses to sample its points. The solid line is 
a plot of II'(x)1 where f is Function-l of our simulation set. 
The dotted line shows the density of sample points (queried 
by CLA-2) on the domain. 
113 

Xll 
INFORMATIONAL COMPLEXITY OF LEARNING 
3.19 
Results of Simulation B. Notice how the sampling strategy of 
the active learner causes better approximation (lower rates) 
for the same number of examples. 
114 
3.20 
Variation with epsilons. 
115 
4.1 
Analysis of an English sentence. The parameter settings for 
English are spec-first, and comp-final. 
130 
4.2 
nalysis of the Bengali translation of the English sentence of 
the earlier figure. The parameter settings for Bengali are spec-
first, and comp-first. 
131 
4.3 
Depiction of stress pattern assignment to words of different 
syllable length under the parameterized bracketing scheme de-
scribed in the text. 
134 
4.4 
The space of possible learning problems associated with pa-
rameterized linguistic theories. Each axis represents an impor-
tant dimension along which specific learning problems might 
differ. Each point in this space specifies a particular learn-
ing problem. The entire space represents a class of learning 
problems which are interesting. 
136 
4.5 
The 8 parameter settings in the GW example, shown as a Markov 
structure. Directed arrows between circles (states, parameter set-
tings, grammars) represent possible nonzero (possible learner) tran-
sitions. The target grammar (in this case, number 5, setting [0 1 
0]), lies at dead center. Around it are the three settings that differ 
from the target by exactly one binary digit; surrounding those are 
the 3 hypotheses two binary digits away from the target; the third 
ring out contains the single hypothesis that differs from the target 
by 3 binary digits. Note that the learner can either stay in the 
same state or step in or out one ring (binary digit) at a time, ac-
cording to the single-step learning hypothesis; but some transitions 
are not possible because there is no data to drive the learner from 
one state to the other under the TLA. Numbers on the arcs de-
note transition probabilities between grammar states; these values 
are not computed by the original GW algorithm. The next section 
shows how to compute these values, essentially by taking language 
set intersections. 
4.6 
Convergence as function of number of examples. The horizon-
tal axis denotes the number of examples received and the ver-
tical axis represents the probability of converging to the target 
state. The data from the target is assumed to be distributed 
uniformly over degree-O sentences. The solid line represents 
TLA convergence times and the dotted line is a random walk 
learning algorithm (RWA). Note that random walk actually 
144 
converges faster than the TLA in this case. 
151 

LIST OF FIGURES 
Xlll 
4.7 
Convergence rates for different learning algorithms when L1 
is the target language. The curve with the slowest rate (large 
dashes) represents the TLA. The curve with the fastest rate 
(small dashes) is the Random Walk (RWA) with no greediness 
or single value constraints. Random walks with exactly one of 
the greediness and single value constraints have performances 
in between these two and are very close to each other. 
158 
4.8 
Rates of convergence for TLA with L1 as the target language 
for different distributions. The y-axis plots the probability of 
converging to the target after m samples and the x-axis is on 
a log scale, i.e., it shows log(m) as m varies. The solid line de-
notes the choice of an "unfavorable" distribution characterized 
by a = 0.9999; b = c = d = 0.000001. The dotted line denotes 
the choice of a = 0.99; b = c = d = 0.0001 and the dashed line 
is the convergence curve for a uniform distribution, the same 
curve as plotted in the earlier figure. 
161 
5.1 
A simple illustration of the state space for the 3-parameter 
syntactic case. There are 8 grammars, a probability distribu-
tion on these 8 grammars, as shown above, can be interpreted 
as the linguistic composition of the population. Thus, a frac-
tion P1 of the population have internalized grammar, gl, and 
so on. 
5.2 
Percentage of the population speaking languages L1 and L2 
as it evolves over the number of generations. The plot has 
been shown only upto 20 generations, as the proportions of 
L1 and L2 speakers do not vary significantly thereafter. No-
tice the "S" shaped nature of the curve (Kroch, 1989, imposes 
such a shape using models from population biology, while we 
obtain this as an emergent property of our dynamical model 
from different starting assumptions). Also notice the region 
of maximum change as the V2 parameter is slowly set by in-
creasing proportion of the population. L1 and L2 differ only 
179 
in the V2 parameter setting. 
185 
5.3 
Percentage of the population speaking languages L5 and L2 
as it evolves over the number of generations. Notice how the 
shift occurs over a space of 4 generations. 
186 
5.4 
Time evolution of grammars· using greedy algorithm with no 
single value. 
188 

xiv 
INFORMATIONAL COMPLEXITY OF LEARNING 
5.5 
Time evolution of linguistic composition for the situations 
where the learning algorithm used is the TLA (with greediness 
dropped, corresponding to the dotted line) , and the Random 
Walk (solid line). Only the percentage of people speaking 
L1 (-V2) and L2 (+ V2) are shown. The initial population is 
homogeneous and speaks L1. The percentage of L1 speakers 
gradually decreases to about 11 percent. The percentage of 
L2 speakers rises to about 16 percent from 0 percent. The two 
dynamical systems (corresponding to S. V. and R. W.) converge 
to the same population mix. However, the trajectory is not 
the same-the rates of change are different, as shown in this 
plot. 
189 
5.6 
Time evolution of lingui~tic composition for the situations 
where the learning algorithm used is the TLA (with single-
value dropped). Only the percentage of people speaking L2 
( + V2) is shown. The initial population is homogeneous and 
speaks L 1• The maturational time (number,N, of sentences the 
child hears before internalizing a grammar) is varied through 
8, 16, 32, 64, 128, 256, giving rise to the six curves shown in the 
figure. The curve which has the highest initial rate of change 
corresponds to the situation where 8 examples were allowed to 
the learner to develop its mature hypothesis. The initial rate 
of change decreases as the maturation time N increases. The 
value at which these curves asymptote also seems to vary with 
the maturation time, and increases monotonically with it. 
191 
5.7 
The evolution of L2 speakers in the community for various 
values of p (a parameter related to the sentence distributions 
Pi, see text). The algorithm used was the TLA, the inital 
population was homogeneous, speaking only L 1 . The curves 
for p = 0.05,0.75, and 0.95 have been plotted as solid lines. 
193 
5.8 
Subspace of a Phase-space plot. The plot shows (71'1 (t), 71'2(t» 
as t varies, i.e., the proportion of speakers speaking languages 
L1 and L2 in the population. The initial state of the popula-
tion was homogeneous (speaking language LI). The algorithm 
used was the TLA with the single-value constraint dropped. 
194 
5.9 
Subspace of a Phase-space plot. The plot shows (7I'1(t) , 71'2(t» 
as t varies for different initial conditions (non-homogeneous 
populations). The algorithm used by the learner is the TLA 
with single-value constraint dropped. 
195 
5.10 
Evolution of speakers of different languages in a population 
starting off with speakers only of Old French. 
200 
5.11 
Tendency to lose V2 as a result of new word orders introduced 
by Modern French source in our Markov Model. 
202 

Foreword 
From Talmudic times to today, whenever people have pondered the nature of 
intelligence, two topics arise again and again: learning and language. These 
t.wo abilities perhaps constitute the very essence of what it means to be human. 
As Chomsky not.es, these two abilities can also be cast generally as a puzzle 
dubbed "Plato's Problem": How do we come to know so much about the world, 
given t.hat we are provided so lit.tle information about it? -
what modern 
linguists and psychologists call "the poverty of the stimulus." So for example, 
children come into this world not knowing whether t.hey will be born in Beijing 
or New Delhi, yet, on the briefist exposure to the local languages -
literally, 
perhaps, just. hundreds of example sentences -
t.hey come int.o full possession 
of "knowledge" of Chinese or Hindi. How is this possible? 
In this book, Partha Niyogi provides a modern solution to Plato's problem-
one of the first. formal, computational answers, yielding fresh insights int.o both 
how learning works (Part 1) and how human language is learned (Part 2). The 
result.s are important. not. just in themselves -
in the field known as computa-
tionallearning theory -
but range well beyond, to questions particular about. 
human language learning and how many sent.ences it. t.akes t.o learn German or 
Japanese; what neural nets can and cannot. do depending on t.heir size; and, 
more generally, the trade-off between the complexity of one's theory and the 
data required by a machine (or a child) to learn it.. 
Such generality does not come as a surprise: An answer to Plato's problem 
presupposes not only that we understand what we know about the world, but 
also how much data we need to learn it. This is Niyogi's cont.ribut.ion: con-
structing a theory of t.he informational complexity of learning. He sharpens 
Plato's quest.ion by introducing the modern armamentarium of c.omput.at.ional 
learning t.heory -- it.self building on t.he insights of computat.ional c.omplexity, 
and the work of Solomonoff, Chaitin, Kolmogorov, and Vapnik: How can we 
c.ome t.o know so much about the world gi1,en that we have so few data samples. 
time and computational energy? 
In Part 1 of this book Niyogi tackles this question from an abstract mathe-
matical viewpoint, laying the foundations for an application to language learn-
ing in Part 2. Just as it was impossible before the advent of recursive function 
xv 

XVI 
INFORMATIONAL COMPLEXITY OF LEARNING 
theory to even properly talk about how language could "make infinite use of 
finite means" , there's a strong sense in which only recently have we found 
the right tools to talk about the informational complexity of learning. One 
might mark the beginning of the modern era from Kolmogorov, Chaitin, and 
Solomonoff's work on so-called "program size" complexity -
how many lines 
of code does it take to compute some algorithm -
but can be more precisely 
pinpointed within the last. 10-15 years, with t.he rise ofVa.Iiant.'s comput.ational 
learning theory, and, especially, Vapnik's earlier and far-reaching generalizat.ion 
of that approach. As Vapnik notes, t.his paradigm shift it.self can be regarded 
from a statistical point of view and efforts t.o overcome the limitat.ions of Fisher's 
dassical stat.istical met.hods from the 1930s. Fisher too engaged in att.acking 
Plato's problem: for Fisher, a central quest.ion was how to find the "best." 
function fit.ting some set of dat.a -
and how many dat.a samples it. would take 
-
given that one knew almost everything about the function, up t.o a wry 
few parameters, like its mean, standard deviation, and so fort.h -
what. Fisher 
dubbed discriminant analysis. 
Vapnik -
and Niyogi in this book -
goes far beyond Fisher's dassical 
paradigm to answer the question in modern computational dress: how many 
data examples will it take to find the "best" function to approximate a target. 
function, given that we know almost nothing about the function, except general 
properties of the model class from which it is drawn -
for instance, that. t.hey 
are all st.raight. lines. This too is a learning problem, indeed, a learning problem 
par excellance. To address it, Niyogi focuses on the thorny issue that. Fisher, 
and every learning theorist since, has wrestled with: the so-called bias-variance 
dilemma, t.he familiar problem of balancing the number of variables one uses 
to fit data versus the amount of data one needs t.o properly estimate all those 
parameters. If one picks too narrow a dass of models, t.hen one runs the risk 
of not get.ting to the right answer at. all -
this is one type of error, bia8. So, 
a common response to modeling the world -
be it. int.erest rat.es, speech, or 
the weather -
is simply to dump more paramet.ers int.o one's t.heory, add more 
variables to an econometric model, or add more nodes t.o a neural net.. But. 
then one runs a second, just as dangerous risk, and encount.ers a second sort of 
error: so many variables that. it t.akes too much data to est.imat.e t.hem all with 
some degree of confidence, variance. 
Put another way, there are two important sources of learning error; recogni-
tion and proper treatment of this fact is a key difference t.hat. separat.es Fisher's 
past from Niyogi's present.. First, because dat.a is limited, one might. not pick 
the best. function -
a familiar kind of estimation error. But second, even if 
we pick the best function, it might not. be the correct one, because t.he dass 
of functions we use might not let us get dose to the right. answer. If we are 
trying to learn Chinese, then we could go wrong in t.wo ways: we might not. get. 
enough Chinese example sentences, or the dass of languages we t.ry t.o use t.o 
approximat.e Chinese might include only Japanese and German. 
Is there a way to resolve this dilemma, and avoid being gored by the vari-
ance/bias horns? Niyogi shows how. Part 1 develops an explicit. mat.hematical 
(and computational) way to trade-off extra model complexity against. the nUl11-

LIST OF FIGURES 
XVII 
ber of examples one needs to learn a model. Often there's an optimal balance 
between the two -
like the fairy-tale ahout Goldilocks and the Three Bears, 
a model that's neither too big, nor too small, but 'just right." Without. the 
tools that Niyogi has provided, we might never understand how t.o balance 
model complexity against sample complexity. Importantly, though Niyogi uses 
a particular model class, radial basis functions (RBFs) to "learn" functions, 
the details are not crucial to his results. They apply more generally to a large 
class of approximation methods -
including multilayer perceptrons. Niyogi's 
findings (obtained in collaboration with F. Girosi) should therefore be read by 
those in the neural net community, more generally, by any who build learning 
machines. 
In Part 2, Niyogi turns his conceptual insights about sample complexity to 
the specific case of language learning -
demonstrating the generality of his ap-
proach. Here, the class of models to learn are possible grammars (languages), 
parameterized along one of a small number of (discontinuous) dimensions: En-
glish sentences come along in the form verb-object, as in ate an apple, while 
Japanese sentences come in the form object-verb, as in "apple ate" (ringo 
tabeta). We may imagine a single binary "switch" setting this range of varia-
tion. By combining twenty or so such spaces, we can fill the space of possible 
syntactic variation in human language -
this is the so-called "principles and 
parameters" approach recently developed by Chomsky. On current linguistic 
accounts, this picture simplifies learning, because a child can simply decide how 
to flip its language switches on the basis of the sentences it hears. But does t.his 
really help? Niyogi again shows that one requires a formal, informational rom-
plexityanalysis. The language space can be modeled as a Markov chain, wit.h 
the states denot.ing various paramet.er combinations. Importantly, OlW inherit.s 
the results known for Markov chains, where t.ransitions between states model 
t.he child moving from language hypothesis to hypothesis. In particular, one can 
analyt.ically determine when a language learning system of this t.ype is learn-
able, but, more importantly, whether it is feasibly learnable -
whether it will 
take 100 or 1 million examples to learn. In this way, Niyogi has added an im-
portant new criterion to that of learnability tout court, and so a new constraint 
on linguistic t.heories. Not only should (human) languages be learnahle, they 
should be learnable from just a few examples, as seems to be t.he case. Niyogi's 
formalization makes explicit predictions about this informat.ional complexity, 
and shows that, indeed, some otherwise reasonable linguistic parameterizations 
fail this new learn ability litmus test. In this sense, the mathematical theory 
developed in Part 1 does real work for us: by adding the criterion of sample 
complexity, we add a new tool for empirical investigation into the nature of 
language, and so a way to answer concretely Plato's problem. One could not 
ask for more. 
Robert C. Berwick 
Tomaso Poggio 
Massachusetts Institute of Technology 

Preface 
In many ways, the ability to learn is recognized as a critical, almost central 
component of intelligence. This monograph is a treatment of the problem of 
learning from examples. As with any scientific enterprise, we have a certain 
point of view. First, we limit ourselves to inductive learning where the learner 
has access to finite data and has to generalize to the infinite set. We ignore, 
therefore, transductive or deductive forms of learning. The investigations are 
in a statistical setting with a primary focus on the informational complexity of 
learning. 
In any work ofthis sort, it is necessary to make certain choices. A natural one 
perhaps is the adoption of the statistical theory of learning as a framework for 
analysis. This allows us to pose basic questions of learnability and informational 
complexity that potentially cut across different domains. Less obvious choices 
are the domains that we have chosen to investigate -
function learning with 
neural networks and language learning with generative grammars. They present 
an interesting contrast. Neural networks are real-valued, infinite-dimensional, 
continuous mappings. Grammars are boolean-valued, finite-dimensional, dis-
crete mappings. Furthermore, researchers in neural computation and genera-
tive linguistics rarely communicate. They typically go to different conferences, 
publish in different journals, and it is often believed that the theoretical un-
derpinnings and technical contents of the two fields are fundamentally at odds 
with each other. 
This monograph is an attempt to bridge the gap. By asking the same ques-
tion -
how much information do we need to learn -
of both kinds of learning 
problems, we highlight their similarities and differences. Function learning re-
quires neural networks with bounded capacity (finite VC dimension); language 
learning requires grammars with constrained rules (universal grammar). There 
is a learn ability argument at the heart of the modern approach to linguistics, 
and fundamentally, as we shall see, parameter setting in the principles and 
parameters framework of modern linguistic theory is little different from pa-
rameter estimation in neural networks. 
The structure of the monograph is as follows. In chapter 1, the basic frame-
work for analysis is developed. This is a non-technical and accessible chapter 
where the problem of learning from examples is discussed, the notion of in-
formational complexity is introduced and intuitions about it are developed. 
Thereafter, we have four very technical chapters, each dealing with a specific 
learning problem. 
Chapter 2 considers the problem of learning functions in a Sobolev space 
using radial basis function networks (a kind of neural network). It is shown 
XIX 

xx 
INFORMATIONAL COMPLEXITY OF LEARNING 
how the generalization error (a measure of how well the learner has learned) 
depends upon the size of the network and the number of examples that the 
learner has access to. Formal bounds on generalization error are developed. 
Chapter 3 considers the situation where the learner is no longer a passive re-
cipient of data but can actively select data of its own choosing. The effect of 
this on the informational complexity of learning is studied for monotonic func-
tions and functions with a bounded derivative. Chapter 4 considers language 
learning in the principles and parameters framework of Chomsky. It is shown 
how language acquisition reduces to parameter estimation and the number of 
examples needed to estimate such parameters is calculated. Chapter 5 consid-
ers a population oflearners each of whom is trying to attain a target grammar. 
A model of language change is derived. It is shown how such a model charac-
terizes the evolutionary consequences of language learning. By comparing with 
historically observed language change phenomena, it allows us to pose an evo-
lutionary criterion for the adequacy of learning theories. This is the first formal 
model of language change and its derivation rests crucially on an analysis of 
the informational complexity of language learning developed in the previous 
chapter. I have deliberately included an abstract with each chapter to allow 
the reader the possibility of skipping the details if he or she wishes to read the 
chapters non-sequentially. 
As with all interdisciplinary pieces of work, I have to acknowledge an intel-
lectual debt to many fields. The areas of approximation theory and statistics, 
particularly the part of empirical process theory beautifully worked out by Vap-
nik and Chervonenkis, play an important role in chapter 2. Ideas from adaptive 
integration and numerical analysis play in important role in chapter 3. Chap-
ters 4 and 5 have evolved from an application of the computational perspective 
to the analysis of learning paradigms that are considered worthwhile in linguis-
tic theory. My decision of what is linguistically worthwhile has been greatly 
influenced by scholarly works in the Chomskyan tradition. Here, there is some 
use of Markov chain theory and dynamical systems theory. 
In all of this, I have brought to bear well known results and techniques from 
different areas of mathematics to formally pose and answer questions of interest 
in human and machine learning; questions previous unposed or unanswered or 
both. In this strict sense, there is little new mathematics here; though an 
abundant demonstration of its usefulness as a research tool to gain insight in 
the cognitive or computer sciences. This reflects my purpose and intended 
audience for this book -
all people interested in computational aspects of 
human or machine learning and its interaction with natural language 
Partha Niyogi 
Hoboken, NJ 

Acknowledgments 
This book arose out of a doctoral dissertation submitted to the Electrical En-
gineering and Computer Science department at MIT. Thanks go first and fore-
most to my thesis committee. Tommy Poggio supported me throughout, pro-
vided the kind of intellectual freedom that is rare in these times and constantly 
reassured me of the usefulness of theoretical analyses. Bob Berwick introduced 
me to linguistic theory, widened the scope of this work considerably, and has 
been a friend at all times. Vladimir Vapnik followed the learning-theoretic part 
of this work quite closely. We have had several long discussions and his ad-
vice, encouragement and influence on this work is profound. Ron Rivest first 
taught me formal machine learning and was wonderfully supportive through-
out. I cannot thank Federico Girosi enough. He has spent countless selfless 
hours discussing a variety of subjects and has always helped me retain my 
perspective. 
Numerous people, at MIT and elsewhere, have touched my life in various 
ways. I especially wish to thank Sanjoy Mitter, Victor Zue, Ken Stevens, Morris 
Halle, Patrick Winston, David Lightfoot, Amy Weinberg, Noam Chomsky, and 
Kah Kay Sung. 
All of this research was conducted at the Artificial Intelligence Lab and the 
Center for Biological and Computational Learning at MIT. This was sponsored 
by a grant from the National Science Foundation under contract ASC-9217041 
(this award includes funds from ARPA provided under the HPCC program); 
and by a grant from ARPAjONR under contract N00014-92-J-1879. Additional 
support has been provided by Siemens Corporate Research Inc., Mitsubishi 
Electric Corporation and Sumitomo Metal Industries. Support for the A.I. 
Laboratory's artificial intelligence research was provided by ARPA contract 
N00014-91-J-4038. 
XXI 

To my family 
and 
to Parvati Krishnamurty 

1 INTRODUCTION 
We introduce the framework in which leanLing from examples is to be studied. We develop 
a precise notion of informational complexity and discuss the factors upon which this de-
pends. Finally, we provide an outline of the four problems discussed in this book, our major 
contributions, and their implications. 
Learning is the centerpiece of human intelligence. Consequently any attempt to 
understand intelligence in the human being or to replicate it in a machine (as 
the field of artificial intelligence is committed to doing) must of necessity explain 
this remarkable ability. Indeed a significant amount of effort and initiative has 
gone into this enterprise and a collective wisdom has emerged regarding the 
paradigms in which this study is to be conducted. 
Needless to say, learning can mean a variety of things. The ability to learn 
a language, to recognize objects, to manipulate them and navigate through 
them, to learn to play chess or to learn the theorems of geometry all touch 
upon different sectors of this multifaceted activity. They require different skills, 
operate on different spaces and use different procedures. This has naturally led 
to a spate of learning paradigms; but most share one thing in common, i.e., 
learning as opposed to "preprogrammed" or memorized behavior involves the 
updating of hypotheses on the basis of some kind of experience: an adaptation 
if you will to the environment on the basis of stimuli from it. The connection 
to complex adaptive systems springs to mind and later in this book we will 
make this connection more explicit in a specific context. 
1 

2 
INFORMATIONAL COMPLEXITY OF LEARNING 
How then does one begin to study such a multifaceted problem? In order to 
meaningfully define the scope of our investigations, let us begin by considering 
a formulation by Osherson et al (1986). They believe (as do we) that learning 
typically involves 
1. A learner. 
2. A thing to be learned. 
3. An environment in which the thing to be learned is presented to the learner. 
4. The hypotheses that occur to the learner about the thing to be learned on 
the basis of the environment. 
Language acquisition by children is a classic example which fits well into 
this framework. "Children are the learners; a natural language is the thing to 
be learned; the corpus of sentences available to the child is the relevant envi-
ronment; grammars serve as hypotheses." (from Systems that Learn; Osherson 
et al 1986). In contrast, consider an example from machine learning; the task 
of object recognition by the computer. Here the computer (or the correspond-
ing algorithm) is the learner, the identity of objects (like chairs or tables, for 
example) are the things to be learned, examples of these objects in the form 
of images are the relevant environment, and the hypotheses might be decision 
boundaries which can be computed by a neural network. 
In this book we will concern ourselves with learning input-output mappings 
from examples of these mappings; in other words, learning target functions 
which are assumed to belong to some class of functions. The view of the brain 
as an information processor (see Marr, 1982) suggests that in solving certain 
problems (like object recognition, for example) the brain develops a series of 
internal representations starting with the sensory (external) input; in other 
words, it computes a function. In some cases, this function is hardwired (like 
detecting the orientations of edges in an image, for example), in others the 
function is learned like learning to recognize individual faces. 1 As another 
example of an input-output function the brain has to compute, consider the 
problem of speech recognition. The listener is provided with an acoustic signal 
which corresponds to some underlying sentence, i.e., a sequence of phonetic 
(or something quite like it) categories. Clearly the listener is able to uncover 
the transformation from this acoustic space to the lexical space. Note also 
that this transformation appears to be different for different languages, i.e., 
different languages have different inventories of phonetic symbols. Further, 
they carve up the acoustic space in different ways; this accounts for why the 
1 FUnctions mapping images of faces to the identity of the person possessing them may of 
course themselves be composed of more primitive functions, like edge detectors, which are 
hardwired. There is a considerable body of literature devoted to identifying the hardwired and 
learned components of this entire process from a neurobiological perspective. The purpose 
of this example was merely to observe that the brain appears to learn functions of various 
kinds; consequently studying the complexity of learning functions is of some value. 

INTRODUCTION 
3 
same acoustic stimuli might be perceived differently as belonging to different 
phonetic categories by a native speaker of Bengali and a native speaker of 
English. Since children are not genetically predisposed to learn Bengali as 
opposed to English (or vice versa) one might conclude that the precise nature 
of this transformation is learned. 
Not all the functions we consider in this book can be psychologically well-
motivated; while some chapters of this book deal with languages and grammars 
which are linguistically well motivated, Chapter 2, which concentrates in large 
part on Sobolev spaces, can hardly seem to be interesting psychologically. How-
ever, the central strand running through this book is the informational com-
plexity of learning from examples. In other words, if information is provided to 
the learner about the target function in some fashioll, how much information 
is needed for the learner to learn the target well? In the task of learning from 
examples, (examples, as we shall see later are really often nothing more than 
(x, y = lex)) pairs where (x, y) E X x Y and / : X -...,. Y) how many examples 
does the learner need to see? This same question is asked of strikingly different 
classes of functions: Sobolev spaces and context free languages. Certain broad 
patterns emerge. Clearly the number of examples depend upon the algorit.hm 
used by the learner to choose its hypotheses, the complexity of the class from 
which these hypotheses are chosen, the amount and type of noise and so on. 
We will try in this book to tease apart the relative contributions of each in 
specific settings in order to uncover fundamental constraints and relationships 
between oracle and learner; constraints which have to be obeyed by nature and 
human in the process of living.2 
This then is our point of view. Let us now discuss some of the relevant 
issues in turn, briefly evaluate their importance in a learning paradigm, and 
the conceptual role they have to play in this book. 
1.1 
THE COMPONENTS OF A LEARNING PARADIGM 
1.1.1 
Concepts, Hypotheses, and Learners 
Concept Classes We need to define the "things" to be learned. In order 
to do this, we typically assume the existence of identifiable entities (concepts) 
which are to be learned and which belong perhaps to some set or class of entities 
(the concept class). Notationally, we can refer to the concept class by C which 
is a set of concepts c E C. These concepts need to be described somehow 
and various representation. schemes can be used. 
For example, researchers 
have investigated concept classes which can be expressed as predicates in some 
logical system (Mitchell, Carbonell, and Michalski; 1986). For our purposes we 
concentrate on classes of functions, i.e., our concept classes are collections of 
2 Even iC we are totally unconcerned wit.h human learning and are int.erested only in designing 
machines or algorithms which can learn Cunctions Crom examples, a hotly pursued subject. in 
machine learning, the issue of number of examples is obviously of considerable import.ance. 

4 
INFORMATIONAL COMPLEXITY OF LEARNING 
functions from X to Y where X and Yare sets. We will define the specific 
nature of these functions over the course of this book. 
Information Sources Information is presented to the learner about a target 
concept c E C in some fashion. There is a huge space of possibilities ranging 
from a "divine" oracle simply enlightening the learner with the true target 
concept in one fell sweep to adversarial oracles which provide information in a 
miserly, deliberately malicious fashion. We have already restricted our inquiry 
to studying the acquisition of function classes. A natural and well studied 
form of information transmission is to allow the learner access to an oracle 
which provides (x, y) pairs or "labelled examples" perhaps tinged with noise. 
In a variant of the face recognition problem (Brunelli and Poggio, 1992; where 
one is required to identify the gender of some unknown person), for example, 
labelled examples might simply be (image,gender) pairs. On the basis of these 
examples then, the learner attempts to infer the target function. 
We consider several variants to this theme. For example, in Chapter 2, 
we allow the learner access to (x, y) pairs drawn according to a fixed unknown 
arbitrary probability distribution on some space X x Y. This represents a passive 
learner who is at the mercy of the unknown probability distribution, which 
could, in principle provide unrepresentative data with high probability. In 
Chapter 3 we explore the possibility of reconstructing functions by allowing the 
learner to choose his or her own examples, i.e., an active collector rather than 
a passive recipient of examples. This is studied in the context of trying to learn 
functional mappings of various sorts. Mathematically, there are connections to 
adaptive approximation, a somewhat poorly studied problem. Active learning 
(as we choose to call it) is inspired by various strategies of selective attention 
that the human brain develops to solve some cognitive tasks. In Chapters 4 and 
5 which concentrate on learning the class of natural languages, the examples 
are sentences spoken by speakers of the target language. We assume again that 
these sentences are spoken according to a probability distribution on all the 
possible sentences; there are two further twists: 1) no negative examples occur 
and 2) typically a bound on the length of the sentences is observed. In all these 
cases, the underlying question of interest is: given the scheme of presenting 
examples to the learner, how many examples does the learner need to see to 
learn well? This question will be sharpened as we progress. 
The Learner and Its Hypotheses The learner operates with a set of hy-
potheses about reality. As information is presented to it, it updates its hy-
pothesis, or chooses3 among a set of alternate hypotheses on the basis of the 
experience (evidence, data depending upon your paradigm of thinking). Clearly 
then, the learner is mapping its data onto a "best" hypothesis which it chooses 
in some sense from a set of hypotheses (which we can now call the hypothesis 
31n artificial intelligence, this task of "searching" the hypothesis space has been given a 
lot of attention resulting in a profusion of searching heuristics and characterizations of the 
computational difficulty of this problem. In this book, we ignore this issue for the most part. 

INTRODUCTION 
5 
class,1i). This broad principle has found instantiations in many differing forms 
in diverse disciplines. 
Consider an example chosen from the world of finance. A stockbroker might 
wish to invest a certain amount of money on stock. Given the variation of share 
values over the past few years (a time series) and given his or her knowledge or 
understanding of the way the market and its players operate, he or she might 
choose to invest in a particular company. As the market and the share prices 
unfold, he (or she) might vary the investments (buying and selling stock) or 
updating the hypotheses. Cumulative experience then might "teach" him/her 
(or in other words, he/she might "learn") to play this game well. 
Or consider another mini-example from speech recognition (specifically pho-
netic recognition) mapping data to hypotheses. Among other things, the human 
learner has to discriminate between the sounds /s/ and /sh/. He or she learns 
to to do this by being exposed to examples (instances) of each phoneme. Over 
the course of time, after exposure to several examples, the learner develops a 
perceptual decision boundary to separate /s/ sounds from /sh/ sounds in the 
acoustic domain. Such a decision boundary is clearly learned; it marginally 
differs from person to person as evidenced by differing responses humans might 
have when asked to classify a particular sound into one of the two categories. 
This decision boundary, h, can be considered to be the learner's hypothesis of 
the s/sh distinction (which he or she might in principle pick from a class of 
possible decision boundaries 1i on the basis of the data). 
As a matter of fact, the scientific enterprise itself consists of the development 
of hypotheses about underlying reality. These hypotheses are developed by 
observing patterns in the physical world and represented as models, schema or 
theories which describe these patterns concisely. 
If indeed the learner is performing the task of mapping data to hypotheses, 
it becomes of interest to study the space of algorithms which can perform this 
task. Needless to say, the operating assumption is that the human learner is 
also following some algorithm; insights from biology or psychology might help 
the computer scientist to narrow the space of algorithms and a biologically 
plausible computational theory (Marr, 1982) might emerge. For our purposes 
then the learner is an algorithm (or a partial recursive function) from data sets 
to hypothesis classes. 
There is a further important connection between concepts and hypotheses 
which should be highlighted here. In our scheme of things, concepts are assumed 
to be the underlying reality; hypotheses are models of this reality. Clearly for 
successful learning (we discuss learnability in the next section) to occur, the 
elements of1i should be able to approximate the elements of C, in other words, 
1i should have sufficient power or complexity to express C. For learnability in 
the limit (Gold, 1967) or PAC-style (Probably Approximately Correct; Valiant, 
1984) models for learnability, this notion can be made more precise. For exam-
ple, if C is some class of real valued functions, 1l should probably be dense in 
C. 

6 
INFORMATIONAL COMPLEXITY OF LEARNING 
1.1.2 Generalization, Learnability, Successful learning 
In addition to the four points noted earlier, another crucial component of learn-
ing is a criterion for success. Formally speaking, one needs to define a metric 
on the space of hypotheses in order to measure the distance between differing 
hypotheses, as also between the target concept and the learner's hypothesis. 
It is only when such a metric is imposed, that one can meaningfully decide 
whether a learner has "learned" the target concept. There are a number of 
related notions which might be worthwhile to introduce here. 
First, there is the issue of generalization. It can be argued, that a key 
component of learning is not just the development of hypotheses on the basis 
of finite experience (as experience must be), but the use of those hypotheses to 
generalize to unseen experience. Clearly successful generalization necessitates 
the closeness (in some sense) of the learner's hypothesis and the target concept, 
for it is only then that unseen data (consistent with the target concept) can 
be successfully modeled by the learner's hypothesis. Thus successful learning 
would involve successful generalization; this book deals with the informational 
complexity of successful generalization. The learnability of concepts implies the 
existence of algorithms (learners) which can develop hypotheses which would 
eventually converge to the target. This convergence "in the limit" is analogous 
to the notion of consistency in statistical estimators and was introduced to 
the learning community by Gold (1967) and remains popular to this day as a 
criterion for language learning. 
In our case, when learning function classes, 11. and C contain functions from 
some space X to some space Y, examples are (x, y) pairs consistent with some 
target function c E C. Let the learner's hypothesis after m such examples 
be hm E 11.. According to some pre-decided criterion, we can put a distance 
metric d on the space of functions to measure the distance between concept 
and hypothesis (this is our generalization error) d( hm , c). Learnability in the 
limit would require d(hm, c) to go to zero as the number of examples, m, goes 
to infinity. The sense in which this convergence occurs might depend upon 
several other assumptions; one might require this convergence to hold for every 
learning sequence, i.e., for every sequence of examples, or one might want this 
to be satisfied for almost every sequence in which case one needs to assume some 
kind of measure on the space according to which one might get convergence in 
measure (probability). 
Convergence in the limit measures only the asymptotic behavior of learning 
algorithms; they do not characterize behavior with finite data sets. In order to 
correct for this it is required to characterize the rates of the above-mentioned 
convergence; roughly speaking how many examples does the learner need to 
collect so that the generalization error will be small. Again depending upon 
individual assumptions, there are several ways to formally pose this question. 
The most popular approach has been to provide a probabilistic formulation; 
Valiant (1984) does this in his PAC model which has come to play an increas-
ingly important role in computational learning theory. In PAC learning, one 
typically assumes that examples are drawn according to some unknown prob-

INTRODUCTION 
7 
ability distribution on X x Y and presented to the learner. If there exists an 
algorithm A which computes hypotheses from data such that for every f. > 0 
and 0 $ 6 $ 1, A collects m(f., 6) examples and outputs a hypothesis hm sat-
isfying d(hm, c) $ f. with probability greater than 1 - 6, then the algorithm is 
said to PAC-learn the concept c. If the algorithm can PAC-learn every concept 
in C then the concept class is said to be PAC-learnable. Looking closely, it 
can be realized that PAC learnability is essentially the same as weak conver-
gence in probability of hypotheses (estimators) to their target functions with 
polynomial rates of convergence. As a matter of fact, most PAC theories are 
subsumed under the more general statistical theory of learning as developed 
by Vapnik (1982,1995). In any event, PAC like formulations playa powerful 
role in characterizing the informational complexity of learning; we have a great 
intellectual debt to this body of literature and its influence in this book cannot 
be overemphasized. 
Remark Sometimes, an obsession with proving the convergence of learning al-
gorithms might be counterproductive. A very good example of that considered 
in this book is the problem of language learning and language change. We 
need to be able to explain how children learn the language of their social en-
vironment on the basis of example sentences. In particular, researchers have 
postulated algorithms by means of which they can do this; considerable effort 
has gone into showing that these algorithms successfully converge to the target. 
However, this does not explain the simultaneously confounding fact that lan-
guages change with time. If generation after generation, children successfully 
converge to the language of their parental generation, then languages would 
never change. The challenge lies in constructing learning paradigms which can 
explain both. In our book, we demonstrate this by moving into a model for lan-
guage change by starting out with a model for language learning. The language 
change model is a dynamical system characterizing the historical evolution of 
linguistic systems; a formalization of ideas in Lightfoot (1991) and Hawkins 
and Gell-Mann (1992). 
1.1.3 Informational Complexity 
We have discussed how the learner chooses hypotheses from 1i on the basis of 
data and how one needs to measure the relative "goodness" of each hypothesis 
to set a precise criterion for learning. We have also introduced the spirit of the 
Gold and Valiant formulations of learning and their relationship to the issues 
of the number of examples and successful generalization. We pause now to 
comment on some other aspects of this relationship. 
First, note that for a particular concept c E C, given a distance metric d, 
there exists a best hypothesis in 1i given by 
hoo = arg min d(c, h) 
hE1t 
Clearly, if 1i has sufficient expressive power, then d( hoo, c) will be small (precise 
learn ability would actually require it to be 0). If 1i is a small class, then 
d(c, hoo) might be large for some c E C and even in the case of infinite data, 

8 
INFORMATIONAL COi\-IPLEXITY OF LEARNING 
poor generalization will result. This is thus a function of the complexit.y of the 
model class 1i and how well matched. it is to C, a matter discussed earlier as 
well. 
Having established that hoo is the best hypothesis the learner can possibly 
postulate; it is consequently of interest to be able to characterize the conver-
gence of the learner's hypot.hesis hm to this best hypothesis as the number of 
data, In, goes to infinity. The number of examples the learner needs to see 
before it can choose with high confidence a hypothesis close enough to the best 
will be our notion of informational complexity. A crucial observation we would 
like to make is that the number of examples depends (among other things, and 
we will discuss this soon) upon the size of the class 1i. To intuitively appreciate 
this, consider the pathological case of 1{ consisting of just one hypothesis. In 
that case, hm E 1{ is always equal to hoo E 1{ and the learner needs to see 
no data at all. Of course, the expressive power of such a class 1{ would be 
extremely limited. If on the other hand, the class 1{ is very complex and for a 
finite data set has a large number of competing hypotheses which fit the data 
but extend in very different ways to the complete space, then considerably 
more data would be needed to disambiguate between these hypotheses. For 
certain probabilistic models (where function learning is essentially equivalent 
to statistical regression) Vapnik and Chervonenkis studied this problem closely 
and developed the notion of VC-dimension: a combinatorial measure of the 
complexity of the class 1{ which is related to its sample complexity (see also 
Blumer et al (1986) for applications to computational learning theory). 
Thus broadly speaking, the more constrained the hypothesis class 1{. the 
smaller is the sample complexity (i.e. the easier it is to choose from finite 
experience the best hypothesis) but then again, the poorer is the expressive 
power and consequently even hoo might be far away from the reality c. On 
the other hand, increasing the expressive power of 1i might decrease d(hoo, c) 
but increase the sample complexity. There is thus an inherent tension between 
the complexity of 1{ and the number of examples; finding the class 1i of the 
right complexity is the challenge of science. Part of the understanding of bio-
logical phenomena involves deciding where on the tightrope bet.ween extremely 
complex and extremely simple models the true phenomena lie. In this respect, 
informational complexity is a powerful tool to help discriminate between models 
of different complexities to describe natural phenomena. 
One sterling example where the information-complexity approach has startlingly 
revised the kinds of models used can be found in the Chomskyan revolution 
in linguistics. Humans develop a mature knowledge of language which is both 
rich and subtle on the basis of example sentences spoken to them by parents 
and guardians during childhood. On observing the child language acquisition 
process, it is remarkable how few examples they need to be able to general-
ize in very sophisticated ways. Further it is observed that children generalize 
in roughly the same way; too striking a coincidence to be attributed purely 
to chance. Languages are infinite sets of sentences; yet on the basis of expo-
sure to finite linguistic experience (sentences) children generalize to the infinite 
set. If it were the case that children operated with completely unconstrained 

INTRODUCTION 
9 
hypotheses about languages, i.e., if they were willing to consider all possible 
infinite extensions to the finite data set they had, then they would never be 
able to generalize correctly or generalize in the same manner. They received 
far too few examples for that. This "poverty of stimulus" in the child language 
acquisition process motivated Chomsky to suggest that children operate with 
hypotheses about language which are constrained in some fashion. In other 
words, we are genetically predisposed as human beings to choose certain gener-
alizations and not others; we operate with a set of restricted hypotheses. The 
goal of linguistics then shifted to finding the class 1l with the right complex-
ity; something which had large enough expressive power to capture the natural 
languages, and low enough to be learned by children. In this book we spend 
some time on models for learning languages. 
Thus we see that an investigation of the informational complexity of learning 
has implications for model building; something which is at the core of the 
scientific enterprise. Particularly when studying cognitive behavior, it might 
potentially allow us to choose the right complexity, i.e., how much processing 
is already built into the brain (the analog of Bubel and Wiesel's orientation-
specific neurons or Chomsky's universal grammar) and how much is acquired by 
exposure to the environment. At this point, it would be worthwhile to point out 
that the complexity of1l is only one of the factors influencing the informational 
complexity. Recall that we have already sharpened our notion of informational 
complexity to mean the number of examples needed by the learner so that 
d(hm, hoc) is small. There are several factors which could in principle affect it 
and Figure 1.1 shows them as decomposed along several different dimensions 
in the space of possibilities. 
Clearly, informational complexity might depend upon upon the manner in 
which examples are obtained. If one were learning to discriminate between the 
sounds lsi and Ishl, for example, one could potentially learn more effectively if 
one were presented with examples drawn from near the decision boundary, i.e., 
examples of sounds which were likely to be confused. Such a presentation might 
conceivably help the learner acquire a sharper idea of the distinction between 
the two sounds rather than if it were simply presented with canonical examples 
of each phoneme. Of course, it might well be the case that our intuition is false 
in this case, but we will never know unless the issue is formally addressed. In 
similar fashion, the presence and nature of the noise corrupting the examples 
could affect sample complexity. In the case of slsh classification, a lot of noise 
in high frequency bands of the signal could affect our perception of frication and 
might delay learning; on the other hand noise which only affects volume of the 
signal might have less effect. The algorithm used to compute a best hypothesis 
hm from the data might affect both learnability and sample complexity. A 
muddle-headed poorly motivated algorithm might choose hypotheses at random 
or it might choose hypotheses according to some criterion which has nothing 
to do with the metric d by which success is to be measured. In such cases, it 
is possible that hm might not converge to hoo at all, or it might take a very 
long time. Finally the metric d according to which success is to be measured 
is clearly a factor. 

10 
INFORMATIONAL COMPLEXITY OF LEARNING 
concept Class 
Hypothesis Class 
Roi •• 
Algorithm used b7 learner 
to choose best hypothesis 
on the basis of examples 
Distance lIetric 
to decide goodness 
of hypothesis 
Mechanism of Gathering 
Examples 
Figure 1.1. 
The space of possibilities. The various factors which affect the informational 
complexity of learning from examples. 

INTRODUCTION 
11 
These different factors interact with each other; our central goal in this book 
is to explore this possibility-space at many different points. We will return to 
this space and our points of exploration later. It is our hope that after seeing the 
interaction between the different dimensions and their relation to informational 
complexity, our intuitions about the analysis of learning paradigms will be 
sharpened. 
1.2 
PARAMETRIC HYPOTHESIS SPACES 
We have already introduced the notion of hypotheses and hypothesis classes 
from which these hypotheses are chosen. We have also remarked that the 
number of examples needed to choose a "best" hypothesis (or at any rate, one 
close enough to the best according to our distance metric) depends inherently 
upon the complexity of these classes. Another related question of some interest 
is: how do we represent these hypotheses? One approach pervasive in science 
is to capture the degree of variability amongst the hypotheses in a parametric 
fashion. The greater the flexibility of the parameterization, the greater the 
allowed variability and the less is the inbuilt constraints, i.e., the larger the 
domain and consequently the larger the search space. One can consider several 
other examples from the sciences where parametric models have been developed 
for some task or other. 
In our book, we spend a considerable amount of time and energy on two 
parametric models which are remarkably different in their structural properties 
and analyze issues of informational complexity in each. It is worthwhile perhaps 
to say a few words about each. 
Neural Networks 
Feed-forward "neural networks" (Lippman, 1987) are becoming increasingly 
popular in science and engineering as a modelling technique. We consider 
a class of feed-forward networks known as Gaussian regularization networks 
(Poggio and Girosi, 1990). Essentially, such a network performs a mapping 
from !Rk to !R given by the following expression 
Fig. 1.2 shows a diagrammatic (it is particularly popular in the neural net 
communities to show the diagrams or architecture and we see no need to break 
with tradition here) representation of the network. The Ci'S are real-valued, G 
is a Gaussian function (activation function), the ti's are the centers, and the 
(1'; 's are the spreads of the Gaussian functions. 
Clearly then, one can consider H n to be the class of all functions which can 
be represented in the form above. This class would consist of functions parame-
terized by n(k+2) parameters; corresponding to the free variables C;, ti, and (1'i. 
One can make several alterations to the architecture; changing for example the 
number of layers, changing the activation functions, putting constraints on the 

12 
INFORMATIONAL COMPLEXITY OF LEARNING 
Linear 
Weights 
Un" 
Weights 
Output Node 
Basis Function 
Nodes 
Input Nodes 
Figure 1.2. The structure of a Hyper Basis Function Network (same as regularization 
network). 
weights and so on thereby arriving at different kinds of parameterized families, 
e.g., the multilayer perceptrons with sigmoidal units, hierarchical mixture of 
experts (Jacobs et aI, 1991) etc. Such feed forward networks have been used 
for tasks as diverse as discriminating between virgin and non-virgin olive oil, 
speech recognition, predicting the stock market, robotic control and so forth. 
Given the prevalence of such neural networks, we have chosen in this book to 
investigate issues pertaining to informational complexity of networks. 
N aturaJ Languages 
Natural languages can be described by their grammars which are essentially 
functional mappings from strings to the set {O, I}. According to conventional 
notation, there is an alphabet set E which is a finite set of symbols. In the 
case of a particular natural language, like English, for example, this set is the 
vocabulary: a finite set of words. These symbols or words are the basic building 
blocks of sentences which are just strings of words. E* denotes the set of all 
finite sentences and a language L is a subset of E*, i.e., some collection of 
sentences which belong to the language. For example, in English,! eat bananas 
is a sentence (an element of E*), being as it is a string of the three words 
(elements of E), I, eat, and bananas. Further, this sentence belongs to the set 
of valid English sentences. On the other other hand, the sentence I bananas eat, 
though a member of E* is not a member of the set of valid English sentences. 
The grammar G Lassociated with the language L then is a functional de-
scription ofthe mapping from E* to {O, I}, all sentences belonging to E* which 
belong to L are mapped onto 1 by G L, the rest are assigned to 0. According 

INTRODUCTION 
13 
to current theories of linguistics which we will consider in this book, it is prof-
itable for analysis to let the set E consist of syntactic categories like verbs, 
adverbs, prepositions, nouns, and so on. A sentence could now be considered 
to be a string of such syntactic categories; each category then maps onto words 
of the vocabulary. Thus the string of syntactic categories Noun Verb Noun 
maps onto I eat bananas; the string Noun Noun Verb maps onto I bananas 
eat. A grammar is a systematic system of rules and principles which pick out 
some strings of syntactic categories as valid, others as not. Most of linguistic 
theory concentrates on generative grammars; grammars which are able to build 
the valid sentences out of the syntactic components according to certain r.ules. 
Phrase structure grammars build sentences out of phrases; and phrases out of 
other phrases or syntactic categories. 
Over the last decade, a parametric theory of grammars (Chomsky, 1981) has 
begun to evolve. According to this, a grammar G(Pl. ... ,Pn) is parameterized 
by a finite (in this case, n) number of typically boolean-valued parameters PI 
through Pn. If these parameters are set to one set of values, one would obtain 
the grammar of a specific language, say, German. Setting them to another set 
of values would define the grammar of another language, say English. To get 
a feel for what parameters are like, consider an example from X-bar theory; a 
subcomponent of grammars. According to X-bar theory, the structure of an 
X P or X -phrase (where X could stand for adjective, noun, verb, etc.) is given 
by the following context-free production rules which are parameterized by two 
parameters PI and P2. 
X P ---+ Spec X' (PI = 0) or X' Spec (PI = 1) 
X' ---+ Comp X'(p2 = 0) or X' Comp (P2 = 1) 
X' ---+ Comp X(P2 = 0) or X Comp (P2 = 1) 
Comp ---+ YP 
For example, English is a comp-final language (P2 = 1) while Bengali is a 
comp-first language(p2 = 0). Notice how all the phrases (irrespective of whether 
it is a noun phrase, verb phrase etc.) in English have their complement at the 
end, while Bengali is the exact reverse. This is one example of a parameterized 
difference between the two languages. 
Also shown in figures 1.4, and 1.5, we have the tree diagrams corresponding 
to the sentence "with one hand" in English and Bengali. English is spec-first 
and comp-final (i.e., PI = 0 and P2 = 1); Bengali on the other hand is spec-first 
and comp-first (PI = 0 and P2 = 0). 
1.3 
TECHNICAL CONTENTS AND MAJOR CONTRIBUTIONS 
So far we have discussed in very general terms, the various components of a 
learning paradigm and their relationship to each other. We have stated our 
intention of analyzing the informational complexity of learning from examples; 
we have thus defined for ourselves the possibility space of Figure 1.1 that needs 
to be explored. In this book, we look at a few specific points in this space; 

14 
INFORMATIONAL COMPLEXITY OF LEARNING 
VP 
eat (V) 
PP 
with (P) 
NP 
house (N) 
AdjP 
bananas 
(Comp) 
one hand 
(Comp) 
on the hill 
(Comp) 
A 
red (AdD 
with anger 
(Comp) 
English (Comp-final) 
VP 
A 
kola 
khay (eat) 
(bananas) 
PP 
A 
ek haath 
dlye (with) 
(one hand) 
AdjP 
A 
rag-er chote 
lal (red) 
(with anger) 
Bengali (Comp-first) 
Figure 1.3. 
Parametric difference in phrase structure between English and Bengali on the 
basis of the parameter P2. 

Spec 
PP 
P 
I 
with 
P' 
Spec 
I 
one 
NP 
N 
I 
hand 
INTRODUCTION 
15 
N' 
Figure 1.4. 
Analysis of the English sentence "with one hand" according to its parameter-
ized X-bar grammar. 

16 
INFORMATIONAL COMPLEXITY OF LEARNING 
Spec 
Spec 
I 
PP 
NP 
ek (one) 
P' 
N' 
P 
I 
dlye (with) 
N 
I 
haath (hand) 
Figure 1.5. 
Analysis of the Bengali sentence "ek haath diye" a literal translation of "with 
one hand" according to its parameterized X-bar grammar. Notice the difference in word 
order. 

INTRODUCTION 
17 
in doing so, the issues involved in informational complexity can be precisely 
formalized and sharper results obtained. Chapters 2 and 3 of this book are on 
real valued functions and are completely self contained. Chapters 4 and 5 on 
natural languages should be read as a unit; together they form another stand-
alone part of this book. We hope the reader will notice the common thread of 
informational complexity running through both types of investigations. 
Chapter 2 of this book examines the use of neural networks of a certain 
kind (the so called regularization networks) in solving pattern classification 
and regression problems. This corresponds to a point in the space of Figure 1.1 
where the concept class is a Sobolev space of functions, the hypothesis class is 
the class of all feed forward regularization networks (with certain restrictions on 
their weights), the examples are drawn according to a fixed, unknown, arbitrary 
probability distribution, the distance metric is a L2(P) norm on the space of 
functions, the algorithm used to choose the best hypothesis is by training a 
finite sized network on labelled examples according to a least-squares criterion. 
The concept class is infinite-dimensional; on using a finite network and finite 
amount of data, a certain amount of generalization error is made. We observe 
that the generalization error can be decomposed into an approximation error 
due to the finite number of parameters of the network and an estimation error 
due to the finite number of data points. Using techniques from approximation 
theory and VC theory, we obtain a bound on the generalization error in terms 
of the number of parameters and number of examples. Our main contributions 
in this chapter include: 
• Formulation of the trade-off between hypothesis complexity and sample com-
plexity when using Gaussian regularization networks. 
• Combining results from approximation theory and the theory of empirical 
processes to obtain a specific bound on the total generalization error as a 
function of the number of examples and number of parameters. 
• Using the bound above to provide guidelines for choosing an optimal network 
architecture to solve certain regression problems. 
Chapter 3 explores the issue of active learning. We are specifically inter-
ested in investigating whether allowing the learner to choose examples helps in 
learning with fewer examples. This chapter consists of two parts which include 
several forays into this question. The first part explores this issue in a func-
tion approximation setting. It is not immediately clear that even if the learner 
were allowed to choose his/her own examples, there exist principled ways of do-
ing this. We develop a framework within which meaningful adaptive sampling 
strategies can be obtained for arbitrary function classes. As specific examples 
we consider cases where the concept classes are real-valued classes like mono-
tonic functions and functions with bounded first derivative, hypothesis classes 
are spline functions, there is no noise, the learner chooses an interpolating spline 
as a best hypothesis and examples are obtained passively (by random draw) or 
adaptively (according our strategy) by the active learner. We obtain theoreti-
cal and empirical bounds on the sample complexity and generalization error for 

18 
INFORMATIONAL COMPLEXITY OF LEARNING 
this task. In the second part, we discuss the idea of epsilon-focusing; a strategy 
whereby the learner can adaptively focus on smaller and smaller regions of the 
domain to solve certain pattern classification problems. We derive conditions 
on function classes where epsilon-focusing would result in faster learning. Our 
main contributions here include: 
• A formulation of active learning in approximation theoretic terms as an 
adaptive approximation problem. 
• Development of active strategies for learning classes of real valued functions. 
These active strategies differ from traditional adaptive approximation strate-
gies in optimal sampling theory in that examples are adaptively selected on 
the basis of previous examples as opposed to preselected on the basis of 
knowledge about the concept class. 
• Explicit computation of theoretical upper and lower bounds on the sample 
complexity of PAC learning real classes using passive and active strategies. 
Simulations with some test target functions allows us to compare the empir-
ical performance against the theoretical worst case bounds. 
• Introduction of the idea of epsilon-focusing which provides a theoretical mo-
tivation for pattern classification schemes where more data is collected near 
the estimated class boundary. The computation of explicit sample complex-
ity bounds for algorithms motivated by epsilon-focusing. 
Chapters 4 and 5 of this book concentrate on a very different region of 
the possibility space of Figure 1.1. Here the concept class is a restricted 
subclass of natural languages, the hypothesis class consists of parameterized 
grammars including X-bar theory, verb movement and case theory, examples 
are assumed to be drawn according to some distribution on the sentences of the 
target, there might or might not be noise, there is a discrete distance metric 
which requires exact identification of the target, the algorithm used to choose 
the best hypothesis is the Triggering Learning Algorithm (Gibson and Wexler, 
1994). 
The TLA was proposed recently by Gibson and Wexler as a possible mech-
anism by which children set parameters and learned the language to which 
they were exposed. Chapter 4 originated as an attempt to analyze the TLA 
from the perspective of informational complexity and to derive conditions for 
convergence and rates of convergence of the TLA to the target. We explore 
the TLA and its variants under the diverse influence of noise, distributional 
assumptions on the data, and explore the linguistic consequences of this. In 
Chapter 5, we study another important facet of the language learning puzzle. 
Starting with a set of grammars and a learning algorithm, we are able to derive 
a dynamical system whose states correspond to the the linguistic composition 
of the population, i.e., the relative percentage of people in a community speak-
ing a particular language. For the TLA, we give the precise update rules for 
the states of this system, analyze conditions for stability and carry out several 
simulations in linguistically plausible systems. This serves as a formal model 

INTRODUCTION 
19 
for describing the historical evolution of languages and formalizes ideas inher-
ent in Lightfoot (1991) and and Hawkins and Gell-Mann (1992) for the first 
time. These two chapters make several important contributions including: 
• The development of a mathematical framework (a Markov structure) to for-
mally study the issues relating to the learnability and sample complexity of 
the TLA. 
• The investigation of variants of TLA, the effect of noise, distributional as-
sumptions and parameterization of the space in a systematic manner on 
linguistically natural spaces. 
• The derivation of algorithm-independent bounds on the sample complexity 
using results from computational learning theory. 
• The derivation of a linguistic dynamical system starting from the TLA op-
erating on parameterized grammars. 
• Utilizing the dynamical system as a model for language change, running 
simulations on linguistically natural spaces and comparison of the results 
against historically observed patterns. 
• Introduction of the diachronic criterion for deciding the plausibility of any 
learning algorithm. 
1.3.1 
A Final Word 
Over the last decade, there has been an explosion of interest in formal learn-
ing theory (see the Proceedings of ACM COLT for a whiff of this). This has 
brought in its wake a perspective on learning paradigms which we greatly share 
and this book reflects that perspective strongly. In addition, as with all inter-
disciplinary pieces of work, we have an intellectual debt to many different fields. 
The areas of approximation theory and statistics, particularly the part of empir-
ical process theory beautifully worked out by Vapnik and Chervonenkis, model 
selection, pattern recognition, decision theory, and nonparametric regression 
play an important role in Chapter 2. Ideas from adaptive integration and nu-
merical analysis play an important role in chapter 3. Chapters 4 and 5 have 
evolved from the application of our computational perspective to the analy-
sis of learning paradigms which are considered worthwhile in linguistic theory 
(our decision of what is linguistically worthwhile has been influenced greatly by 
scholarly works in the Chomskyan tradition). Here, there is some use of Markov 
chain theory and dynamical systems theory. In all of this, we have brought to 
bear well known results and techniques from different areas of mathematics to 
formally pose and answer questions of interest in human and machine learn-
ing; questions previously unposed or unanswered or both. In this strict sense, 
there is little new mathematics here; though an abundant demonstration of 
its usefulness as a research tool in the cognitive and computer sciences. This 
reflects our purpose and our intended audience for this book, namely, all people 
interested in human or machine learning from a computational perspective. 

2 ON THE RELATIONSHIP BETWEEN 
HYOTHESIS COMPLEXITY, SAMPLE 
COMPLEXITY AND GENERALIZATION 
ERROR FOR NEURAL NETWORKS 
Feedforward networks are a class of approximation techniques that can be used to learil to 
perform some tasks from a finite set of examples. The question of the capability of a network 
to generalize from a finite training set to unseen data is clearly of crucial importance. In this 
chapter, we bound the generalization error of a class of Radial Basis Functions, for certain well 
defined function learning tasks, in terms ofthe number of parameters and number of examples. 
We show that the total generalization error is partly due to the insufficient representational 
capacity of the network (because of the finite size of the network being used) and partly 
due to insufficient information about the target function because of the finite number of 
samples. Prior research has looked at representational capacity or sample complexity in 
isolation. In the spirit of A. Barron, H. White and S. Geman we develop a framework to look 
at both. While the bound that we derive is specific for Radial Basis Functions, a number of 
observations deriving from it apply to any approximation technique. Our result also sheds 
light on ways to choose an appropriate network architecture for a particular problem and the 
kinds of problems that can be effectively solved with finite resources, i.e., with finite number 
of parameters and finite amounts of data. 
2.1 
INTRODUCTION 
Many problems in learning theory can be effectively modelled as learning an 
input output mapping on the basis of limited evidence of what this mapping 
might be. The mapping usually takes the form of some unknown function 
between two spaces and the evidence is often a set of labelled, noisy, examples 
21 

22 
INFORMATIONAL COMPLEXITY OF LEARNING 
i.e., (x, y) pairs which are consistent with this function. On the basis of this 
data set, the learner tries to infer the true function. 
We have discussed in Chapter 1, several examples from speech recognition, 
object recognition, and finance where such a scenario exists. At the risk of be-
laboring this point consider two more examples which illustrate this approach. 
In economics, it is sometimes of interest to predict the future foreign currency 
rates on the basis of the past time series. There might be a function which cap-
tures the dynamical relation between past and future currency rates and one 
typically tries to uncover this relation from data which has been appropriately 
processed. Similarly in medicine, one might be interested in predicting whether 
or not breast cancer will recur in a patient within five years after her treat-
ment. The input space might involve dimensions like the age of the patient, 
whether she has been through menopause, the radiation treatment previously 
used etc. The output space would be single dimensional boolean taking on 
values depending upon whether breast cancer recurs or not. One might collect 
data from case histories of patients and try to uncover the underlying function. 
The unknown target function is assumed to belong to some class :F which 
using the terminology of computational learning theory we call the concept 
class. Typical examples of concept classes are classes of indicator functions, 
boolean functions, Sobolev spaces etc. The learner is provided with a finite 
data set. One can make many assumptions about how this data set is collected 
but a common assumption which would suffice for our purposes is that the 
data is drawn by sampling independently the input output space (X x Y) 
according to some unknown probability distribution. On the basis of this data, 
the learner then develops a hypothesis (another function) about the identity of 
the target function i.e., it comes up with a function chosen from some class, 
say H (the hypothesis class) which best fits the data and postulates this to be 
the target. Hypothesis classes could also be of different kinds. For example, 
they could be classes of boolean functions, polynomials, linear functions, spline 
functions and so on. One such class which is being increasingly used for learning 
problems is the class offeedforward networks «Lippmann, 1987; Hertz, Krogh, 
and Palmer, 1991; Girosi, Jones, and Poggio, 1993). A typical feedforward 
network is a parameterized function of the form 
n 
f(x) = L CiH(X; wd 
i=l 
where {ci}i=l and {wdi=l are free parameters and H(·;·) is a given, fixed 
function (the "activation function"). Depending on the choice of the activa-
tion function one gets different network models, such as the most common 
form of "neural networks", the Multilayer Perceptron (Rumelhart, Hinton, and 
Williams, 1986; Cybenko, 1989; Lapedes, and Farmer, 1988; Hertz, Krogh, 
and Palmer, 1991; Hornik, Stinchcombe, and White, 1989; Funahashi, 1989; 
Mhaskar, and Micchelli, 1992; Mhaskar, 1993; Irie, and Miyake, 1988) , or 
the Radial Basis Functions network (Broomhead, and Lowe, 1988; Dyn, 1987; 

GENERALIZATION ERROR FOR NEURAL NETS 
23 
Hardy, 1971,1990; Micchelli, 1986; Powell, 1990; Moody, and Darken, 1989; 
Poggio, and Girosi, 1990; Girosi, 1992; Girosi, Jones, and Poggio, 1993). 
If, as more and more data becomes available, the learner's hypothesis be-
comes closer and closer to the target and converges to it in the limit, the target 
is said to be learnable. The error between the learner's hypothesis and the 
target function is defined to be the generalization error and for the target to be 
learnable the generalization error should go to zero as the data goes to infinity. 
While learnability is certainly a very desirable quality, it requires the fulfillment 
of two important criteria. 
First, there is the issue of the representational capacity (or hypothesis com-
plexity) of the hypothesis class. This must have sufficient power to represent 
or closely approximate the concept class. Otherwise for some target function 
f, the best hypothesis h in H might be far away from it. The error that this 
best hypothesis makes is formalized later as the approximation error. In this 
case, all the learner can hope to do is to converge to h in the limit of infinite 
data and so it will never recover the target. Second, we do not have infinite 
data but only some finite random sample set from which we construct a hy-
pothesis. This hypothesis constructed from the finite data might be far from 
the best possible hypothesis, h, resulting in a further error. This additional 
error (caused by finiteness of data) is formalized later as the estimation error. 
The amount of data needed to ensure a small estimation error is referred to as 
the sample complexity of the problem. The hypothesis complexity, the sample 
complexity and the generalization error are related. If the class H is very large 
or in other words has high complexity, then for the same estimation error, the 
sample complexity increases. If the hypothesis complexity is small, the sample 
complexity is also small but now for the same estimation error, the approxima-
tion error is high. This point has been developed in terms of the Bias-Variance 
trade-off in (Geman, Bienenstock, and Doursat, 1992) in the context of neural 
networks, and others (Rissanen, 1983; Grenander, 1951; Vapnik, 1982; Stone, 
1974) in statistics in general. 
The purpose of this chapter is two-fold. First, we formalize the problem of 
learning from examples so as to highlight the relationship between hypothesis 
complexity, sample complexity and total error. Second, we explore this rela-
tionship in the specific context of a particular hypothesis class. This is the 
class of Radial Basis function networks which can be considered to belong to 
the broader class of feed-forward networks. Specifically, we are interested in 
asking the following questions about radial basis functions. 
Imagine you were interested in solving a particular problem (regression or 
pattern classification) using Radial Basis Function networks. Then, how large 
must the network be and how many examples do you need to draw so that you 
are guaranteed with high confidence to do very well? Conversely, if you had a 
finite network and a finite amount of data, what are the kinds of problems you 
could solve effectively? 
Clearly, if one were using a network with a finite number of parameters, then 
its representational capacity would be limited and therefore even in the best 
case we would make an approximation error. Drawing upon results in approx-

24 
INFORMATIONAL COMPLEXITY OF LEARNING 
imation theory (Lorentz, 1986) several researchers (Cybenko, 1989; Hartman, 
Keeler, and Kowalski, 1989; Barron, 1993; Hornik, Stinchcombe, and White, 
1989; Chui, and Li, 1990; Arai, 1989; Mhaskar, and Micchelli, 1992; Mhaskar, 
1993; Irie, and Miyake, 1988; Chen, Chen, and Liu, 1990) have investigated the 
approximating power of feedforward networks showing how as the number of 
parameters goes to infinity, the network can approximate any continuous func-
tion. These results assume infinite data and questions of learn ability from finite 
data are ignored. For a finite network, due to finiteness of the data, we make an 
error in estimating the parameters and consequently have an estimation error in 
addition to the approximation error mentioned earlier. Using results from Vap-
nik and Chervonenkis (Vapnik, 1982; Vapnik, and Chervonenkis, 1971, 1981, 
1991) and Pollard (Pollard, 1984) , work has also been done (Haussler, 1992; 
Baum, and Haussler, 1988) on the sample complexity of finite networks show-
ing how as the data goes to infinity, the estimation error goes to zero i.e., the 
empirically optimized parameter settings converge to the optimal ones for that 
class. However, since the number of parameters are fixed and finite, even the 
optimal parameter setting might yield a function which is far from the target. 
This issue is left unexplored by Haussler (1992) in an excellent investigation of 
the sample complexity question. 
In this chapter, we explore the errors due to both finite parameters and 
finite data in a common setting. In order for the total generalization error to 
go to zero, both the number of parameters and the number of data have to go 
to infinity, and we provide rates at which they grow for learnability to result. 
Further, as a corollary, we are able to provide a principled way of choosing the 
optimal number of parameters so as to minimize expected errors. It should be 
mentioned here that White (1990) and Barron (1994) have provided excellent 
treatments of this problem for different hypothesis classes. We will mention 
their work at appropriate points in this chapter. 
The plan of the chapter is as follows: in section 2.2 we will formalize the 
problem and comment on issues of a general nature. We then provide in section 
2.3 a precise statement of a specific problem. In section 2.4 we present our main 
result, whose proof is postponed to appendix 2-D for continuity ofreading. The 
main result is qualified by several remarks in section 2.5. In section 2.6 we will 
discuss possible implications of our result in practice and finally we conclude 
in section 2.7 with a reiteration of our essential points. 
2.2 
DEFINITIONS AND STATEMENT OF THE PROBLEM 
In order to make a precise statement of the problem we first need to introduce 
some terminology and to define a number of mathematical objects. A summary 
of the most common notations and definitions used in this chapter can be found 
in appendix 2-A. 
2.2.1 
Random Variables and Probability Distributions 
Let X and Y be two arbitrary sets. We will call x and y the independent variable 
and response respectively, where x and y range over the generic elements of 

GENERALIZATION ERROR FOR NEURAL NETS 
25 
X and Y. In most cases X will be a subset of a k-dimensional Euclidean 
space and Y a subset of the real line, so that the independent variable will 
be a k-dimensional vector and the response a real number. We assume that a 
probability distribution P(x, y) is defined on X x Y. P is unknown, although 
certain assumptions on it will be made later in this section. 
The probability distribution P(x, y) can also be written asl : 
P(x, y) = P(x)P(yJx) , 
(2.1) 
where P(yJx) is the conditional probability of the response y given the inde-
pendent variable x, and P(x) is the marginal probability of the independent 
variable given by: 
P(x) = l dy P(x, y) . 
Expected values with respect to P(x, y) or P(x) will be always indicated by 
E[l Therefore, we will write: 
E[g(x, y)] == [ 
dxdy P(x, y)g(x, y) 
JXXY 
and 
E[h(x)] == Ix dx P(x)h(x) 
for any arbitrary function 9 or h. 
2.2.2 Learning from Examples and Estimators 
The framework described above can be used to model the fact that in the 
real world we often have to deal with sets of variables that are related by a 
probabilistic relationship. For example, y could be the measured torque at 
a particular joint of a robot arm, and x the set of angular position, velocity 
and acceleration of the joints of the arm in a particular configuration. The 
relationship between x and y is probabilistic because there is noise affecting the 
measurement process, so that two different torques could be measured given 
the same configuration. 
In many cases we are provided with examples of this probabilistic relation-
ship, that is with a data set D/, obtained by sampling I times the set X x Y 
according to P(x, y): 
D/ == {(Xi, Yi) E X x Y}l=l . 
1 Note that we are assuming that the conditional distribution exists, but this is not a very 
restrictive assumption. 

26 
INFORMATIONAL COMPLEXITY OF LEARNING 
From eq. (2.1) we see that we can think of an element (Xi, Yi) of the data set DI 
as obtained by sampling X according to P(x), and then sampling Y according 
to P(ylx). In the robot arm example described above, it would mean that 
one could move the robot arm into a random configuration Xl, measure the 
corresponding torque Yl, and iterate this process I times. 
The interesting problem is, given an instance of X that does not appear in 
the data set DI, to give an estimate of what we expect Y to be. For example, 
given a certain configuration of the robot arm, we would like to estimate the 
corresponding torque. 
Formally, we define an estimator to be any function 1 : X -+ Y. Clearly, 
since the independent variable X need not determine uniquely the response y, 
any estimator will make a certain amount of error. However, it is interesting to 
study the problem of finding the best possible estimator, given the knowledge 
of the data set DI, and this problem will be defined as the problem of learning 
Irom examples, where the examples are represented by the data set DI. Thus 
we have a probabilistic relation between x and y. One can think of this as 
an underlying deterministic relation corrupted with noise. Hopefully a good 
estimator will be able to recover this relation. 
2.2.3 The Expected Risk and the Regression Function 
In the previous section we explained the problem of learning from examples 
and stated that this is the same as the problem of finding the best estimator. 
To make sense of this statement, we now need to define a measure of how good 
an estimator is. Suppose we sample X x Y according to P(x, y), obtaining the 
pair (x, y). A measure2 of the error of the estimator 1 at the point x is: 
(y - l(x))2 . 
In the example of the robot arm, I(x) is our estimate ofthe torque correspond-
ing to the configuration x, and y is the measured torque of that configuration. 
The average error of the estimator 1 is now given by the functional 
1[/] == E[(y - I(X))2] = f 
dxdy P(x, y)(y - l(x))2 , 
lxxy 
that is usually called the expected risk of 1 for the specific choice of the error 
measure. 
Given this particular measure as our yardstick to evaluate different estima-
tors, we are now interested in finding the estimator that minimizes the expected 
risk. In order to proceed we need to specify its domain of definition F. Then 
using the expected risk as a criterion, we could obtain the best element of F. 
Depending on the properties of the unknown probability distribution P(x, y) 
2Note that this is the familiar squared-error and when averaged over its domain yields the 
mean squared error for a particular estimator, a very common choice. However, it is useful 
to remember that there could be other choices as well. 

GENERALIZATION ERROR FOR NEURAL NETS 
27 
one could make different choices for:F. We will assume in the following that 
:F is some space of differentiable functions. For example, :F could be a space 
off unctions with a certain number of bounded derivatives (the spaces Am (Rd ) 
defined in appendix 2-A), or a Sobolev space offunctions with a certain number 
of derivatives in Lp (the spaces Hm,P(Rd ) defined in appendix 2-A). 
Assuming that the problem of minimizing I[J] in :F is well posed, it is easy 
to obtain its solution. In fact, the expected risk can be decomposed in the 
following way (see appendix 2-B): 
I[!] = E[(fo(x) - l(x))2] + E[(y - 10(X))2] 
(2.2) 
where lo(x) is the so called regression lunction, that is the conditional mean 
of the response given the independent variable: 
lo(x) == [ 
dy yP(ylx) . 
(2.3) 
From eq. (2.2) it is clear that the regression function is the function that 
minimizes the expected risk in :F, and is therefore the best possible estimator. 
Hence, 
lo(x) = arg min 1[/] . 
IE :F 
However, it is also clear that even the regression function will make an error 
equal to E[(y - 10(x))2], that is the variance of the response given a certain 
value for the independent variable, averaged over the values the independent 
variable can take. While the first term in eq. (2.2) depends on the choice of 
the estimator I, the second term is an intrinsic limitation that comes from the 
fact that the independent variable x does not determine uniquely the response 
y. 
The problem of learning from examples can now be reformulated as the 
problem of reconstructing the regression function 10, given the example set D,. 
Thus we have some large class of functions :F to which the target function 10 
belongs. We obtain noisy data of the form (x, y) where x has the distribution 
P(x) and for each x, y is a random variable with mean lo(x) and distribution 
P(ylx). We note that y can be viewed as a deterministic function ofx corrupted 
by noise. If one assumes the noise is additive, we can write y = lo(x) + 1/:c 
where 1/x3 is zero-mean with distribution P(ylx). We choose an estimator on 
the basis of the data set and we hope that it is close to the regression (target) 
function. It should also be pointed out that this framework includes pattern 
classification and in this case the regression (target) function corresponds to 
the Bayes discriminant function (Gish, 1990; Hampshire and Pearlmutter, 1990; 
Richard and Lippman, 1991) . 
3Note that the standard regression problem often assumes 7/3: is independent of x. Our case 
is distribution free because we make no assumptions about the nature of 7/3:' 

28 
INFORMATIONAL COMPLEXITY OF LEARNING 
2.2.4 The Empirical Risk 
If the expected risk functional I[f] were known, one could compute the regres-
sion function by simply finding its minimum in :1', that would make the whole 
learning problem considerably easier. What makes the problem difficult and 
interesting is that in practice I[f] is unknown because P(x, y) is unknown. Our 
only source of information is the data set DI which consists of I independent 
random samples of X x Y drawn according to P(x, y). Using this data set, the 
expected risk can be approximated by the empirical risk Iemp: 
For each given estimator I, the empirical risk is a random variable, and under 
fairly general assumptions4, by the law of large numbers (Dudley, 1989) it 
converges in probability to the expected risk as the number of data points goes 
to infinity: 
lim P{II[f] - Iemp[/lI > ~} = 0 
'r/~ > 0 . 
1-00 
(2.4) 
Therefore a common strategy consists in estimating the regression function as 
the function that minimizes the empirical risk, since it is "close" to the expected 
risk if the number of data is high enough. For the error metric we have used, 
this yields the least-squares error estimator. However, eq. (2.4) states only that 
the expected risk is "close" to the empirical risk for each given I, and not for 
all I simultaneously. Consequently the fact that the empirical risk converges 
in probability to the expected risk when the number, I, of data points goes to 
infinity does not guarantee that the minimum of the empirical risk will converge 
to the minimum of the expected risk (the regression function). As pointed out 
and analyzed in the fundamental work of Vapnik and Chervonenkis the notion 
of uniform convergence in probability has to be introduced, and it will be 
discussed in other parts of this chapter. 
2.2.5 The Problem 
The argument of the previous section suggests that an approximate solution 
of the learning problem consists in finding the minimum of the empirical risk, 
that is solving 
min Iemp [f] . 
Je:F 
However this problem is clearly ill-posed, because, for most choices of :1', it 
will have an infinite number of solutions. In fact, all the functions in :1' that 
interpolate the data points (Xi, Yi), that is with the property 
'For example, assuming the data is independently drawn and I[Jj is finite. 

GENERALIZATION ERROR FOR NEURAL NETS 
29 
f(xd = Yi 1, .. . ,1 
will give a zero value for Iemp. This problem is very common in approximation 
theory and statistics and can be approached in several ways. A common tech-
nique consists in restricting the search for the minimum to a smaller set than 
:F. We consider the case in which this smaller set is a family of parametric 
/unctions, that is a family of functions defined by a certain number of real pa-
rameters. The choice of a parametric representation also provides a convenient 
way to store and manipulate the hypothesis function on a computer. 
We will denote a generic subset of :F whose elements are parametrized by 
a number of parameters proportional to n, by Hn. Moreover, we will assume 
that the sets Hn form a nested family, that is 
HI ~ H2 ~ ... ~ Hn ~ ... ~ H. 
For example, Hn could be the set of polynomials in one variable of degree 
n - 1, Radial Basis Functions with n centers, multilayer perceptrons with n 
sigmoidal hidden units, multilayer perceptrons with n threshold units and so on. 
Therefore, we choose as approximation to the regression function the function 
in" defined as: 5 
in / == arg min Iemp [t] . 
, 
/EH" 
(2.5) 
Thus, for example, if Hn is the class of functions which can be represented as 
f = E:=l CeiH(x; Wei) then eq. (2.5) can be written as 
in" == arg min Iemp[t] . 
CO, •
or 
A number of observations need to be made here. First, if the class :F is small 
(typically in the sense of bounded VC-dimension or bounded metric entropy 
(Pollard, 1984) ), then the problem is not necessarily ill-posed and we do not 
have to go through the process of using the sets H n. However, as has been 
mentioned already, for most interesting choices of:F (e.g. classes of functions 
in Sobolev spaces, continuous functions etc.) the problem might be ill posed. 
However, this might not be the only reason for using the classes H n. It might 
be the case that that is all we have or for some reason it is something we would 
like to use. For example, one might want to use a particular class of feed-
forward networks because of ease of implementation in VLSI. Also, if we were 
5Notice that we are implicitly assuming that the problem of minizing lemp[f] over Hn has a 
solution, which might not be the case. However the quantity 
En 1== inf lemp[J] 
, 
/EH" 
is always well defined, and we can always find a function jn,1 for which lemp[jn,z] is arbitrarily 
close to En,l. It will turn out that this is sufficient for our purposes, and therefore we will 
continue, assuming that jn,1 is well defined by eq. (2.5) 

30 
INFORMATIONAL COMPLEXITY OF LEARNING 
to solve the function learning problem on a computer as is typically done in 
practice, then the functions in :F have to be represented somehow. We might 
consequently use Hn as a representation scheme. It should be pointed out that 
the sets H n and :F have to be matched with each other. For example, we 
would hardly use polynomials as an approximation scheme when the class :F 
consists of indicator functions or for that matter use threshold units when the 
class :F contains continuous functions. In particular, if we are to recover the 
regression function, H must be dense in:F. One could look at this matching 
from both directions. For a class :F, one might be interested in an appropriate 
choice of Hn. Conversely, for a particular choice of Hn, one might ask what 
classes :F can be effectively solved with this scheme. Thus, if we were to use 
multilayer perceptrons, this line of questioning would lead "..IS to identify the 
class of problems which can be effectively solved by them. 
Thus, we see that in principle we would like to minimize 1[/] over the large 
class :F obtaining thereby the regression function 10. What we do in practice is 
to minimize the empirical risk Iemp[f] over the smaller class Hn obtaining the 
function in". Assuming we have solved all the computational problems related 
to the actual computation of the estimator in", the main problem is now: 
how good is fn,l? 
Independently of the measure of performance that we choose when answering 
this question, we expect in" to become a better and better estimator as n 
and I go to infinity. In fact, when I increases, our estimate of the expected 
risk improves and our estimator improves. The case of n is trickier. As n 
increases, we have more parameters to model the regression function, and our 
estimator should improve. However, at the same time, because we have more 
parameters to estimate with the same amount of data, our estimate of the 
expected risk deteriorates. Thus we now need more data and n and I have to 
grow as a function of each other for convergence to occur. At what rate and 
under what conditions the estimator in" improves depends on the properties 
of the regression function, that is on F, and on the approximation scheme we 
are using, that is on H n. 
2.2.6 Bounding the Generalization Error 
At this stage it might be worthwhile to review and remark on some general 
features of the problem of learning from examples. Let us remember that our 
goal is to minimize the expected risk I[f] over the set :F. If we were to use a 
finite number of parameters, then we have already seen that the best we could 
possibly do is to minimize our functional over the set H n , yielding the estimator 
In: 
In == arg min 1[/] . 
JEH .. 
However, not only is the parametrization limited, but the data is also finite, and 
we can only minimize the empirical risk Iemp , obtaining as our final estimate the 

GENERALIZATION ERROR FOR NEURAL NETS 
31 
function in". Our goal is to bound the distance from in" that is our solution, 
from 10, that is the "optimal" solution. If we choose to measure the distance 
in the L2(p) metric (see appendix 2-A), the quantity that we need to bound, 
that we will call generalization error, is: 
, 
2 
= 11/0 - In,dl£2(p) 
There are 2 main factors that contribute to the generalization error, and we 
are going to analyze them separately for the moment. 
1. A first cause of error comes from the fact that we are trying to approximate 
an infinite dimensional object, the regression function 10 E :F, with a finite 
number of parameters. We call this error the approximation error, and we 
measure it by the quantity E[(fo - 1'1)2], that is the L2(P) distance between 
the best function in H n and the regression function. The approximation 
error can be expressed in terms of the expected risk using the decomposition 
(2.2) as 
(2.6) 
Notice that the approximation error does not depend on the data set D/, 
but depends only on the approximating power of the class Hn. The natural 
framework to study it is approximation theory, that abound with bounds on 
the approximation error for a variety of choices of H nand :F. In the following 
we will always assume that it is possible to bound the approximation error 
as follows: 
where c(n) is a function that goes to zero as n goes to infinity if H is dense in 
:F. In other words, as shown in figure (2.1), as the number n of parameters 
gets larger the representation capacity of Hn increases, and allows a better 
and better approximation of the regression function 10. This issue has been 
studied by a number of researchers (Cybenko, 1989; Hornik, Stinchcombe, 
and White, 1989; Barron, 1993; Funahashi, 1989; Mhaskar, and Micchelli, 
1992; Mhaskar, 1993) in the neural networks community. 
2. Another source of error comes from the fact that, due to finite data, we 
minimize the empirical risk Iemp[fJ, and obtain in", rather than minimizing 
the expected risk I[J], and obtaining In. As the number of data goes to 
infinity we hope that in" will converge to In, and convergence will take place 
if the empirical risk converges to the expected risk uniformly in probability 
(Vapnik, 1982) . The quantity 

32 
INFORMATIONAL COMPLEXITY OF LEARNING 
IIemp[fl- I[fll 
is called estimation error, and conditions for the estimation error to converge 
to zero uniformly in probability have been investigated by Vapnik and Cher-
vonenkis Pollard, Dudley (1987) , and Haussler (1992) . Under a variety 
of different hypothesis it is possible to prove that, with probability 1 - 6, a 
bound of this form is valid: 
IIemp[J] - I[fll ~ w(l, n, 6) "If E Hn 
(2.7) 
The specific form of w depends on the setting ofthe problem, but, in general, 
we expect w(/, n, 6) to be a decreasing function of I. However, we also expect 
it to be an increasing function of n. The reason is that, if the number of 
parameters is large then the expected risk is a very complex object, and 
then more data will be needed to estimate it. Therefore, keeping fixed the 
number of data and increasing the number of parameters will result, on the 
average, in a larger distance between the expected risk and the empirical 
risk. 
The approximation and estimation error are clearly two components of the 
generalization error, and it is interesting to notice, as shown in the next state-
ment, the generalization error can be bounded by the sum of the two: 
Statement 2.2.1 The following inequality holds: 
• 
2 
lifo -
fn,dIL~(p) ~ e(n) + 2w(1, n, 6) . 
(2.8) 
Proof: using the decomposition of the expected risk (2.2), the generalization 
error can be written as: 
A natural way of bounding the generalization error is as follows: 
(2.10) 
In the first term of the right hand side of the previous inequality we recognize 
the approximation error (2.6). If a bound of the form (2.7) is known for the 
generalization error, it is simple to show (see appendix (2-C) that the second 
term can be bounded as 
II[fn] - I[in,']1 ~ 2w(/, n, 6) 
and statement (2.2.1) follows D. 

GENERALIZATION ERROR FOR NEURAL NETS 
33 
Thus we see that the generalization error has two components: one, bounded 
by c( n), is related to the approximation power of the class of functions {H n}, 
and is studied in the framework of approximation theory. The second, bounded 
by w(l, n, 6), is related to the difficulty of estimating the parameters given finite 
data, and is studied in the framework of statistics. Consequently, results from 
both these fields are needed in order to provide an understanding of the problem 
of learning from examples. Figure (2.1) also shows a picture of the problem. 
F 
", fo 
? / 
II 
f 
/ 
n 
Figure 2.1. 
This figure shows a picture of the problem. The outermost circle represents 
the set F. Em bedded in this are the nested subsets, the H n 'so 10 is an arbitrary target 
function in :1", In is the closest element of Hn and in,/ is the element of Hn which the 
learner hypothesizes on the basis of data. 
2.2.7 A Note on Models and Model Complexity 
From the form of eq. (2.8) the reader will quickly realize that there is a trade-off 
between n and I for a certain generalization error. For a fixed I, as n increases, 
the approximation error c(n) decreases but the estimation error w(/, n, 6) in-
creases. Consequently, there is a certain n which might optimally balance this 
trade-off. Note that the classes Hn can be looked upon as models of increas-
ing complexity and the search for an optimal n amounts to a search for the 
right model complexity. One typically wishes to match the model complexity 
with the sample complexity (measured by how much data we have on hand) 
and this problem is well studied (Eubank, 1988; Stone, 1974; Linehart, and 

34 
INFORMATIONAL COMPLEXITY OF LEARNING 
Zucchini, 1986, Rissanen, 1989; Barron, and Cover, 1989; Efron, 1982; Craven, 
and Wahba, 1979) in statistics. 
Broadly speaking, simple models would have high approximation errors but 
small estimation errors while complex models would have low approximation 
errors but high estimation errors. This might be true even when considering 
qualitatively different models and as an illustrative example let us consider 
two kinds of models we might use to learn regression functions in the space 
of bounded continuous functions. The class of linear models, i.e., the class 
of functions which can be expressed as f = w . x + (), do not have much 
approximating power and consequently their approximation error is rather high. 
However, their estimation error is quite low. The class of models which can be 
expressed in the form H = E~=1 Ci sin (Wi . x + ()i) have higher approximating 
power (Jones, 1990) resulting in low approximation errors. However this class 
has an infinite VC-dimension and its estimation error can not therefore be 
bounded. 
So far we have provided a very general characterization of this problem, 
without stating what the sets :F and Hn are. As we have already mentioned 
before, the set :F could be a set of bounded differentiable or integrable func-
tions, and Hn could be polynomials of degree n, spline functions with n knots, 
multilayer perceptrons with n hidden units or any other parametric approxima-
tion scheme with n parameters. In the next section we will consider a specific 
choice for these sets, and we will provide a bound on the generalization error 
of the form of eq. (2.8). 
2.3 
STATING THE PROBLEM FOR RADIAL BASIS FUNCTIONS 
As mentioned before, the problem of learning from examples reduces to esti-
mating some target function from a set X to a set Y. In most practical cases, 
such as character recognition, motor control, time series prediction, the set X is 
the k-dimensional Euclidean space Rk, and the set Y is some subset of the real 
line, that for our purposes we will assume to be the interval [-M, MJ, where 
M is some positive number. In fact, there is a probability distribution P(x, y) 
defined on the space Rk x [-M, M] according to which the labelled examples 
are drawn independently at random, and from which we try to estimate the 
regression (target) function. It is clear that the regression function is a real 
function of k variables. 
In this chapter we focus our attention on the Radial Basis Functions approx-
imation scheme (also called Hyper-Basis Functions; Poggio and Girosi, 1990 ). 
This is the class of approximating functions that can be written as: 
f(x) = t f3iG( IIx ~i till) 
i=1 
(2.11) 
where G is some given basis function (in our case Gaussian, specifically G( a) = 
Ve- a') and the f3i, ti, and Ui are free parameters. We would like to understand 
what classes of problems can be solved "well" by this technique, where "well" 

GENERALIZATION ERROR FOR NEURAL NETS 
35 
means that both approximation and estimation bounds need to be favorable. It 
is possible to show that a favorable approximation bound can be obtained if we 
assume that the class of functions :F to which the regression function belongs 
is defined as follows: 
:F == {tlf = A * Gm , m > kj2, IAIRk ::; M} . 
(2.12) 
Here A is a signed Radon measure on the Borel sets of Rk, Gm is the Bessel-
Macdonald kernel, i.e., the inverse fourier transform of 
-
1 
Gm(s) = (1 + 41r21IsI12)m/2 
The symbol * stands for the convolution operation, IAIRk is the total variation6 
of the measure A and M is a positive real number. The space :F as defined in 
eq. 2.12 is the so-called Liouville Space of order m. If m is even, this contains the 
Sobolev Space H m ,l of functions whose derivatives up to order m are integrable. 
We point out that the class F is non-trivial to learn in the sense that it has 
infinite pseudo-dimension (Pollard, 1984). 
In order to obtain an estimation bound we need the approximating class to 
have bounded variation, and the following constraint will be imposed: 
n 
This constraint does not affect the approximation bound, and the two pieces fit 
together nicely. Thus the set Hn is defined now as the set offunctions belonging 
to L2 such that 
f(x) = t f3iG( IIx ~i till), t lf3i I ::; M, ti E Rk , Ui E R 
i=l 
i=l 
(2.13) 
Having defined the sets H nand :F we remind the reader that our goal is to 
recover the regression function, that is the minimum of the expected risk over 
F. What we end up doing is to draw a set of I examples and to minimize the 
empirical risk Iemp over the set H n , that is to solve the following non-convex 
minimization problem: 
(2.14) 
6 A signed measure A can be decomposed by the Hahn-Jordan decomposition into A = A + -
A -. Then I AI = A + + A -
is called the total variation of A. See Dudley (1989) for more 
information. 

36 
INFORMATIONAL COMPLEXITY OF LEARNING 
Notice that assumption that the regression function 
fo(x) == E[Ylx] 
belongs to the class :F correspondingly implies an assumption on the probability 
distribution P(ylx), viz., that P must be such that E[ylx] belongs to :F. Notice 
also that since we assumed that Y is a closed interval, we are implicitly assuming 
that P(ylx) has compact support. 
Assuming now that we have been able to solve the minimization problem of 
eq. (2.14), the main question we are interested in is "how far is in" from fo?". 
We give an answer in the next section. 
2.4 
MAIN RESULT 
The main theorem is: 
Theorem 2.1 For any 0 < 6 < 1, for n nodes, I data points, input dimension-
ality of k, and Hn,:F'/o,in" also as defined in the statement of the problem 
above, with probability greater than 1 - 6, 
II ' 
f,A 
112 
0 (1) 0 ([nk In(nl) - In 6] 1/2) 
JO -
n,/ Ll(P) :::; 
;; + 
I 
Proof: The proof requires us to go through a series of propositions and lemmas 
which have been relegated to appendix (2-D) for continuity of ideas. 0 
2.5 
REMARKS 
There are a number of comments we would like to make on the formulation of 
our problem and the result we have obtained. There is a vast body of literature 
on approximation theory and the theory of empirical risk minimization. In re-
cent times, some of the results in these areas have been applied by the computer 
science and neural network community to study formal learning models. Here 
we would like to make certain observations about our result, suggest extensions 
and future work, and to make connections with other work done in related 
areas. 
2.5.1 
Observations on the Main Result 
• The theorem has a PAC (Valiant, 1984) like setting. It tells us that if we 
draw enough data points (labelled examples) and have enough nodes in our 
Radial Basis Functions network, we can drive our error arbitrarily close to 
zero with arbitrarily high probability. Note however that our result is not 
entirely distribution-free. Although no assumptions are made on the form 
of the underlying distribution, we do have certain constraints on the kinds 
of distributions for which this result holds. In particular, the distribution 

GENERALIZATION ERROR FOR NEURAL NETS 
37 
is such that its conditional mean E[ylx] (this is also the regression function 
fo(x)) must belong to a the class of functions F defined by eq. (2.12). 
Further the distribution P(ylx) must have compact support 7. 
• The error bound consists of two parts, one (O(1/n)) coming from approxi-
mation theory, and the other O(((nk In(nl) + In(1/6))/1)1/2) from statistics. 
It is noteworthy that for a given approximation scheme (corresponding to 
{Hn}), a certain class of functions (corresponding to:F) suggests itself. So 
we have gone from the class of networks to the class of problems they can 
perform as opposed to the other way around, i.e., from a class of problems 
to an optimal class of networks. 
• This sort of a result implies that if we have the prior knowledge that fa 
belongs to class F, then by choosing the number of data points, 1, and the 
number of basis functions, n, appropriately, we can drive the misclassifica-
tion error arbitrarily close to Bayes rate. In fact, for a fixed amount of data, 
even before we have started looking at the data, we can pick a starting archi-
tecture, i.e., the number of nodes, n, for optimal performance. After looking 
at the data, we might be able to do some structural risk minimization (Vap-
nik, 1982) to further improve architecture selection. For a fixed architecture, 
this result sheds light on how much data is required for a certain error per-
formance. Moreover, it allows us to choose the number of data points and 
number of nodes simultaneously for guaranteed error performances. Section 
2.6 explores this question in greater detail. 
2.5.2 Extensions 
• There are certain natural extensions to this work. We have essentially proved 
the consistency of the estimated network function in". In particular we have 
shown that in" converges to fa with probability 1 as I and n grow to in-
finity. It is also possible to derive conditions for almost sure convergence. 
Further, we have looked at a specific class of networks ({Hn}) which consist 
of weighted sums of Gaussian basis functions with moving centers variance. 
This kind of an approximation scheme suggests a class of functions :F which 
can be approximated with guaranteed rates of convergence as mentioned 
earlier. We could prove similar theorems for other kinds of basis functions 
which would have stronger approximation properties than the class of func-
tions considered here. As a matter of fact, the general principle on which the 
proof is based has been extended by us to a variety of approximation schemes 
including multilayer perceptrons and other kinds of feedforward networks. 
• We have used notions of metric entropy and covering number (Dudley, 1987; 
Pollard, 1984) in obtaining our uniform convergence results. Haussler (1992) 
uses the results of Pollard and Dudley to obtain uniform convergence results 
7This condition, that is related to the problem of large deviations, could be relaxed, and 
will be subject of further investigations. 

38 
INFORMATIONAL COMPLEXITY OF LEARNING 
and our techniques closely follow his approach. It should be noted here that 
Vapnik deals with exactly the same question and uses the VC-dimension 
instead. It would be interesting to compute the VC-dimension of the class 
of networks and use it to obtain our results. 
• While we have obtained an upper bound on the error in terms of the number 
of nodes and examples, it would be worthwhile to obtain lower bounds on 
the same. Such lower bounds do not seem to exist in the neural network 
literature to the best of our knowledge. 
• We have considered here a situation where the estimated network i.e., in" 
is obtained by minimizing the empirical risk over the class of functions H n. 
Very often, the estimated network is obtained by minimizing a somewhat 
different objective function which consists of two parts. One is the fit to 
the data and the other is some complexity term which favors less complex 
(according to the defined notion of complexity) functions over more complex 
ones. For example the regularization approach (Tikhonov, 1963; Poggio and 
Girosi, 1992; Wahba, 1990) minimizes a cost function of the form 
N 
H[fl = L(Yi - f(xd + Acl>[fl 
i=l 
over the class H = Un> 1 H n. Here A is the so called "regularization param-
eter" and ~[fl is a fun~tional which measures smoothness of the functions 
involved. It would be interesting to obtain convergence conditions and rates 
for such schemes. Choice of an optimal A is an interesting question in regular-
ization techniques and typically cross-validation or other heuristic schemes 
are used. A result on convergence rate potentially offers a principled way to 
choose A. 
• Structural risk minimization is another method to achieve a trade-off be-
tween network complexity (corresponding to n in our case) and fit to data. 
However it does not guarantee that the architecture selected will be the one 
with minimal parameterization8 . In fact, it would be of some interest to 
develop a sequential growing scheme. Such a technique would at any stage 
perform a sequential hypothesis test (Govindarajulu, 1975) . It would then 
decide whether to ask for more data, add one more node or simply stop and 
output the function it has as its f-good hypothesis. In such a process, one 
might even incorporate active learning (Angluin, 1988) so that if the algo-
rithm asks for more data, then it might even specify a region in the input 
domain from where it would like to see this data. It is conceivable that such 
a scheme would grow to minimal parameterization (or closer to it at any 
rate) and require less data than classical structural risk minimization. See 
8Neither does regularization for that matter. The question of minimal parameterization is 
related to that of order determination of systems, a very difficult problem! 

GENERALIZATION ERROR FOR NEURAL NETS 
39 
the next chapter for a detailed treatment of active learning for some limited 
function learning tasks. 
• It should be noted here that we have assumed that the empirical risk E!=l (Yi-
I(Zi))2 can be minimized over the class Hn and the function in,/ be effec-
tively computed. While this might be fine in principle, in practice only 
a locally optimal solution to the minimization problem is found (typically 
using some gradient descent schemes). The computational complexity of 
obtaining even an approximate solution to the minimization problem is an 
interesting one and results from computer science (Judd, 1988; Blum and 
Rivest, 1988) suggest that it might in general be N P-hard. 
2.5.3 Connections with Other Results 
• In the neural network and computational learning theory communities re-
sults have been obtained pertaining to the issues of generalization and learn-
ability. Some theoretical work has been done (Baum and Haussler, 1989; 
Haussler, 1989; Ji and Psaltis, 1992) in characterizing the sample complexity 
of finite sized networks. Of these, it is worthwhile to mention again the work 
of Haussler from which this chapter derives much inspiration. He obtains 
bounds for a fixed hypothesis space i.e. a fixed finite network architecture. 
Here we deal with families of hypothesis spaces using richer and richer hy-
pothesis spaces as more and more data becomes available. Later we will char-
acterize the trade-off between hypothesis complexity and error rate. Others 
(Levin, Tishby, and Solla, 1990; Opper, and Haussler, 1991) attempt to 
characterize the generalization abilities of feed-forward networks using the-
oretical formalizations from statistical mechanics. Yet others (Botros, and 
Atkeson, 1991; Moody, 1992; Cohn and Tesauro, 1991; Weigand, Rumelhart, 
and Huberman, 1991) attempt to obtain empirical bounds on generalization 
abilities. 
• This is an attempt to obtain rate-of-convergence bounds in the spirit of Bar-
ron's work, but using a different approach. We have chosen to combine 
theorems from approximation theory (which gives us the O(1/n) term in 
the rate, and uniform convergence theory (which gives us the other part). 
Note that at this moment, our rate of convergence is worse than Barron's. In 
particular, he obtains a rate of convergence of O(l/n+(nk In(l))/l). Further, 
he has a different set of assumptions on the class of functions (corresponding 
to our F). Finally, the approximation scheme is a class of networks with 
sigmoidal units as opposed to radial-basis units and a different proof tech-
nique is used. Our proof technique is closest in spirit to the idea of structural 
risk minimization of Vapnik (1982) while Barron is closer to the idea of the 
minimum description length principle of Rissanen (1989). 
• It would be worthwhile to make a reference to (Geman, Bienenstock, and 
Doursat, 1992) which talks of the Bias-Variance dilemma. This is another 
way of formulating the trade-off between the approximation error and the 

40 
INFORMATIONAL COMPLEXITY OF LEARNING 
estimation error. As the number of parameters (proportional to n) increases, 
the bias (which can be thought of as analogous to the approximation error) 
of the estimator decreases and its variance (which can be thought of as 
analogous to the estimation error) increases for a fixed size of the data set. 
Finding the right bias-variance trade-off is very similar in spirit to finding 
the trade-off between network complexity and data complexity. 
• Given the class of radial basis functions we are using, a natural comparison 
arises with kernel regression (Krzyzak, 1986; Devroye, 1981) and results on 
the convergence of kernel estimators. It should be pointed out that, unlike 
our scheme, Gaussian-kernel regressors require the variance of the Gaussian 
to go to zero as a function of the data. Further the number of kernels is 
always equal to the number of data points and the issue of trade-off between 
the two is not explored to the same degree. 
• In our statement of the problem, we discussed how pattern classification 
could be treated as a special case of regression. 
In this case the func-
tion fa corresponds to the Bayes a-posteriori decision function. Researchers 
(Richard and Lippman, 1991; Hampshire and Pearlmutter, 1990; Gish, 1990) 
in the neural network community have observed that a network trained on 
a least square error criterion and used for pattern classification was in effect 
computing the Bayes decision function. This chapter provides a rigorous 
proof of the conditions under which this is the case. 
2.6 
IMPLICATIONS OF THE THEOREM IN PRACTICE: PUTTING IN 
THE NUMBERS 
We have stated our main result in a particular form. We have provided a prov-
able upper bound on the error (in the II . IIL2(p) metric) in terms of the number 
of examples and the number of basis functions used. Further we have provided 
the order of the convergence and have not stated the constants involved. The 
same result could be stated in other forms and has certain implications. It 
provides us rates at which the number of basis functions (n) should increase 
as a function of the number of examples (I) in order to guarantee convergence 
(Section 2.6.1). It also provides us with the trade-offs between the two as 
explored in Section 2.6.2. 
2.6.1 
Rate of Growth ofn for Guaranteed Convergence 
From our theorem (2.1) we see that the generalization error converges to zero 
only if n goes to infinity more slowly than I. In fact, if n grows too quickly the 
estimation error w(/, n, 0) will diverge, because it is proportional to n. In fact, 
setting n = tr, we obtain 

GENERALIZATION ERROR FOR NEURAL NETS 
41 
liml ...... +oo w(l, n, 8) = 
-1' 
0 ([l r k1n(l"+1)+ln(1/6)] 1/2) _ 
-
1IIli ...... +oo 
I 
-
= liIIli ...... +oo r- 1 Inl. 
Therefore the condition r < 1 should hold in order to guarantee convergence 
to zero. 
2.6.2 Optimal Choice oEn 
In the previous section we made the point that the number of parameters n 
should grow more slowly than the number of data points I, in order to guarantee 
the consistency of the estimator in,l. It is quite clear that there is an optimal 
rate of growth of the number of parameters, that, for any fixed amount of data 
points I, gives the best possible performance with the least number of parame-
ters. In other words, for any fixed I there is an optimal number of parameters 
n*(l) that minimizes the generalization error. That such a number should exist 
is quite intuitive: for a fixed number of data, a small number of parameters will 
give a low estimation error w(l, n, 8), but very high approximation error €(n), 
and therefore the generalization error will be high. If the number of parameters 
is very high the approximation error €( n) will be very small, but the estima-
tion error w(l, n, 8) will be high, leading to a large generalization error again. 
Therefore, somewhere in between there should be a number of parameters high 
enough to make the approximation error small, but not too high, so that these 
parameters can be estimated reliably, with a small estimation error. This phe-
nomenon is evident from figure (2.2), where we plotted the generalization error 
as a function of the number of parameters n for various choices of sample size 
l. Notice that for a fixed sample size, the error passes through a minimum. 
Notice that the location of the minimum shifts to the right when the sample 
size is increased. 
In order to find out exactly what is the optimal rate of growth of the network 
size we simply find the minimum of the generalization error as a function of n 
keeping the sample size I fixed. Therefore we have to solve the equation: 
a 
A 
2 
On E[(fo - In,l) ] = 0 
for n as a function of I. Substituting the bound given in theorem (2.1) in the 
previous equation, and setting all the constants to 1 for simplicity, we obtain: 
~ [.!. + ( nk In( nl) - In( 8) )~] = 0 . 
on n 
1 
Performing the derivative the expression above can be written as 
1 
...!.- = ! [kn In( nl) - In 0] - 'i ~ [1 (I) 
] 
n 2 
2 
/ 
Inn + 1 . 

42 
INFORMATIONAL COMPLEXITY OF LEARNING 
aBound on the generalization error 
.. a 
a 
.. 
0 .. .. 
Qj 
1:1 
0 
'"111 
ojola 
.C! 
1=50 
Na 
." 
1"4 • 
.. 
Qj 
1:1 
Qj 
~ 
a a a 
a 
0 
25 
50 
75 
n 
Figure 2.2. 
Bound on the generalization error as a function of the number of basis func-
tions n keeping the sample size 1 fixed. This has been plotted for a few different choices 
of sample size. Notice how the generalization error goes through a minimum for a certain 
value of n. This would be an appropriate choice for the given (constant) data complexity. 
Note also that the minimum is broader for larger I, that is, an accurate choice of n is less 
critical when plenty of data is available. 

GENERALIZATION ERROR FOR NEURAL NETS 
43 
We now make the assumption that I is big enough to let us perform the ap-
proximation In(nl) + 1 ~ In(nl). Moreover, we assume that 
1 _ « (n/)nk 
6 
in such a way that the term including 6 in the equation above is negligible. After 
some algebra we therefore conclude that the optimal number of parameters 
n*(I) satisfies, for large I, the equation: 
41 
t 
n*(/) = [k In(n*(I)/)] 
From this equation is clear that n* is roughly proportional to a power of I, and 
therefore we can neglect the factor n* in the denominator of the previous equa-
tion, since it will only affect the result by a multiplicative constant. Therefore 
we conclude that the optimal number of parameters n*(/) for a given number 
of examples behaves as 
n*(I) ex [k 1~/]* . 
(2.15) 
In order to show that this is indeed the optimal rate of growth we reported 
in figure (2.3) the generalization error as function of the number of examples I 
for different rate of growth of n, that is setting n = Ir for different values of r. 
Notice that the exponent r = ~, that is very similar to the optimal rate of eq. 
(2.15), performs better than larger (r = ~) and smaller (r = 110 ) exponents. 
While a fixed sample size suggests the scheme above for choosing an optimal 
network size, it is important to note that for a certain confidence rate (6) and for 
a fixed error rate (€), there are various choices of n and I which are satisfactory. 
Fig. 2.4 shows n as a function of I, in other words (I, n) pairs which yield the 
same error rate with the same confidence. 
If data are expensive for us, we could operate in region A of the curve. If 
network size is expensive we could operate in region B of the curve. In particular 
the economics of trading off network and data complexity would yield a suitable 
point on this curve and thus would allow us to choose the right combination 
of n and I to solve our regression problem with the required accuracy and 
confidence. 
Of course we could also plot the error as a function of data size I for a fixed 
network size (n) and this has been done for various choices of n in Fig. 2.5. 
We see as expected that the error monotonically decreases as a function of I. 
However it asymptotically decreases not to the Bayes error rate but to some 
value above it (the approximation error) which depends upon the the network 
complexity. 
Finally figure (2.6) shows the result of theorem (2.1) in a 3-dimensional plot. 
The generalization error, the network size, and the sample size are all plotted 
as a function of each other. 

44 
INFORMATIONAL COMPLEXITY OF LEARNING 
\"0 
n= 
\ (number of examples) 
\'ND 
n= 
\ '11 
n= 
Figure 2.3. 
The bound on the generalization error as a function of the number of ex-
am pies for different choices of the rate at which network size n increases with sam pie size 
I. Notice that if n = /, then the estimator is not guaranteed to converge, i.e., the bound 
on the generalization error diverges. While this is a distribution free-upper bound, we need 
distribution-free lower bounds as well to make the stronger claim that n = I will never 
converge. 

GENERALIZATION ERROR FOR NEURAL NETS 
45 
A 
~ 
r--
B 
101'12 
101'13 
101'14 
101'15 
101'10 
Number of Examples (I) 
Figure 2.4. 
This figures shows various choices of (I, n) which give the same generalization 
error. The z-axis has been plotted on a log scale. The interesting observation is that there 
are an infinite num ber of choices for num ber of basis functions and num ber of data points all 
of which would guarantee the same generalization error (in terms of its worst case bound). 
2.6.3 Experiments 
The main thrust of this chapter is to provide some insight into how overfitting 
(poor generalization) can be studied in classes of feedforward networks and 
the general laws that govern overfitting phenomena in such networks. How 
closely do "real" function learning problems obey the the general principles 
embodied in the theorem described earlier? We do not attempt to provide an 
extensive answer to this question-but just to satisfy the reader's curiosity, we 
now describe some empirical results. 
The experiment: The target function, a k-dimensional function, was assumed 
to have the following form, which ensures that the assumptions of theorem (2.1) 
are satisfied: 
(2.16) 
Here E is a diagonal matrix (E)a,8 = k (j aOa,8. The parameters, {(j a, Wi, cd 
were chosen at random in the following ranges: (ji E [1.7,2.3], Wi E [-2,2]k, 
Ci E [-1,1], I{)i E [0,271'], N E [3,20]. Training sets of different sizes, ranging 
from I = 30 to I = 500 were randomly generated in the k dimensional cube 
[-71', 71']k, and an independent test set of 2000 examples was chosen to estimate 
the generalization error. Gaussian RBF networks (as in theorem 2.1) with 
different number of hidden units, ranging from n = 1 to n = 300, were trained 

46 
INFORMATIONAL COMPLEXITY OF LEARNING 
I. o 
I. 
I. 
Qj 
Figure 2.5. 
The generalization error as a function of number of examples keeping the 
number of basis functions (n) fixed. This has been done for several choices of n. As the 
number of examples increases to infinity the generalization error asymptotes to a minimum 
which is not the Bayes error rate because of finite hypothesis com plexity (finite n). 

L o 
L 
L 
QI 
GENERALIZATION ERROR FOR NEURAL NETS 
47 
Figure 2.6. 
The generalization error, the number of examples (I) and the number of basis 
functions (n) as a function of each other. 

48 
INFORMATIONAL COMPLEXITY OF LEARNING 
using a gradient descent scheme. Each training session was repeated 10 times 
with random initialization, because of the problem of local minima. We did 
experiments in 2, 4, 6 and 8 dimensions. In all cases the qualitative behavior 
of the experimental results followed the theoretical predictions. In figures 2.7 
and 2.8 we report the experimental results for a 2 and 6 dimensional case 
respectively. 
o 
. 
,.,. . .. •• 
1fil.J! 
~ 
.. 
20 
. 
i 
40 
60 
Number 01 Nodes 
80 
100 
Figure 2.1. 
The generalization error is plotted as a function of the number of nodes 
of an RBF network (10) trained on 100 data points of a function of the type (16) in 2 
dimensions. For each number of parameters 10 results, corresponding to 10 different local 
minima, are reported. The continuous lines above the experimental data represents the 
bound ~ + b[(nk In(nl) -ln6)/lj1/2 of eq. (14), in which the parameters a and b have 
been estimated empirically, and 6 = 10-6. 
We found, in general, that although overfitting occurs as expected, it has 
a tendency to occurr at a larger number of parameters than predicted. We 
attribute that to the presence of local minima, that have the effect of restrict-
ing the hypothesis, and suggesting that the "effective" number of parameters 
(Moody, 1992) is much smaller than the total number of parameters. 
Extensive experimentation is needed to compare the deviation between the-
ory and practice, and the problem of local minima should be seriously ad-
dressed. This is well beyond the scope of the current chapter, and further 
research on the matter needs to be done. 

GENERALIZATION ERROR FOR NEURAL NETS 
49 
~ 
0 
~ 
0 
~ 
cO 
jd 
i~ 
Clco 
0 
0 "! 
0 
0 
20 
40 
60 
80 
Number of Nodes 
Figure 2.8. 
Everything is as in figure (6), but here the dimensionality is 6 and the number 
of data points is 150. As before, the parameters a and b have been estimated empirically 
and 6 = 10-6 . Notice that this time the curve passes through some of the data points. 
However, we recall that the bound indicated by the curve holds under the assum ption that 
the global minimum has been found, and that the data points represent different local 
minima. Clearly in the figure the curve bounds the best of the local minima. 

50 
INFORMATIONAL COMPLEXITY OF LEARNING 
2.7 
CONCLUSION 
For the task of learning some unknown function from labelled examples where 
we have multiple hypothesis classes of varying complexity, choosing the class of 
right complexity and the appropriate hypothesis within that class poses an in-
teresting problem. We have provided an analysis of the situation and the issues 
involved and in particular have tried to show how the hypothesis complexity, 
the sample complexity and the generalization error are related. We proved a 
theorem for a special set of hypothesis classes, the radial basis function net-
works and we bound the generalization error for certain function learning tasks 
in terms of the number of parameters and the number of examples. This is 
equivalent to obtaining a bound on the rate at which the number of param-
eters must grow with respect to the number of examples for convergence to 
take place. Thus we use richer and richer hypothesis spaces as more and more 
data become available. We also see that there is a tradeoff between hypothe-
sis complexity and generalization error for a certain fixed amount of data and 
our result allows us a principled way of choosing an appropriate hypothesis 
complexity (network architecture). The choice of an appropriate model for em-
pirical data is a problem of long-standing interest in statistics and we provided 
connections between our work and other work in the field. 
Most crucially, the results of this chapter provide us with an appreciation 
of the trade-offs involved in choosing the right model -
a trade-off that has 
to be satisfied in the design of engineering systems as well as the development 
of scientific theories. In this larger philosophical sense, the scope of the re-
sults extend well beyond neural networks and we will return to this theme 
after excursions into some other learning problems discussed over the next few 
chapters. 
2-A 
NOTATIONS 
• A: a set of functions defined on S such that, for any a E A, 
• A[: the restriction of A to the data set, see eq. (2.24). 
• B: it will usually indicate the set of all possible I-dimensional Boolean vec-
tors. 
• B: a generic !-separated set in S. 
• C(!,A,d£1): the metric capacity ofa set A endowed with the metric d£1(P). 
• d(·, .): a metric on a generic metric space S. 
• dL l(" .), d£1(P)(', .): L1 metrics in vector spaces. The definition depends 
on the space on which the metric is defined (k-th dimensional vectors, real 
valued functions, vector valued functions). 

GENERALIZATION ERROR FOR NEURAL NETS 
51 
1. In a vector space Rk we have 
1 ' 
dLl(X,y) = y L Ix/J - y/JI 
/J=l 
where x, y E Rk, x/J and y/J denote their Jl-th components. 
2. In an infinite dimensional space :F of real valued functions in k variables 
we have 
dLl(P)(f, g) = r I/(x) - g(x)ldP(x) 
iRk 
where I, g E F and dP(x) is a probability measure on Rk. 
3. In an infinite dimensional space F offunctions in k variables with values 
in Rn we have 
1 n 1 
dLl(P)(f, g) = - L 
n i=1 Rk 
I/i(X) - gi(x)ldP(x) 
where f(x) = (ft (x), ... li(X), ... In (x)), g(x) = (gl (x), ... ,gn(X)) 
are elements of F and dP(x) is a. probability measure on Rk. 
• D,: it will always indicate a data set of I points: 
D, == {(Xi, Yi) E X x y}l=l . 
The points are drawn according to the probability distribution P(x, y). 
• E[l it denotes the expected value with respect to the probability distribu-
tion P(x, y). For example 
1[/] = E[(y - l(x))2] , 
and 
11/0 - IIIL2(p) = E[(fo(x) - I(X))2] . 
• I: a generic estimator, that is any function from X to Y: 
I:X=>Y. 
• lo(x): the regression function, it is the conditional mean of the response 
given the predictor: 
lo(x) == [ 
dy yP(ylx) . 

52 
INFORMATIONAL COMPLEXITY OF LEARNING 
It can also be defined as the function that minimizes the expected risk I[fl 
in U, that is 
fa (x) == arg inf I[fl . 
JE U 
Whenever the response is obtained sampling a function h in presence of zero 
mean noise the regression function coincides with the sampled function h. 
• fn: it is the function that minimizes the expected risk I[fl in Hn: 
Since 
fn == arg inf I[fl 
JEH" 
I[fl = lifo - fIl12(p) + I(fo] 
fn it is also the best L2(P) approximation to the regression function in Hn 
(see figure 2.1). 
• in,,: is the function that minimizes the empirical risk Iemp[f] in Hn: 
in , == arg inf Iemp[fl 
, 
JEH" 
In the neural network language it is the output of the network after training 
has occurred. 
• :F: the space of functions to which the regression function belongs, that is 
the space of functions we want to approximate. 
:F:X:::>Y 
where X E Rd and Y E R. :F could be for example a set of differentiable 
functions, or some Sobolev space Hm,P(Rk ) 
• g: it is a class of functions of k variables 
g : Rk -+ [0, V) 
defined as 
g == {g : g(x) = G(lIx - tID, t E Rk}. 
where Gis .the gaussian function. 
• Gl: it is a k + 2-dimensional vector space of functions from Rk to R defined 
as 

GENERALIZATION ERROR FOR NEURAL NETS 
53 
where x E Rk and zll> is the J.t-th component of the vector x. 
• G2: it is a set of real valued functions in k variables defined as 
_ 
1 
G2 = {ae J : f E Gl, a = rn= } 
V 211'0' 
where 0' is the standard deviation of the Gaussian G. 
• HI: it is a class of vector valued functions 
of the form 
g(x) = (G(llx - tlll), G(lIx - t211),.··, G(lIx - tnll)) 
where G is the gaussian function and the ti are arbitrary k-dimensional 
vectors. 
• HF: it is a class of real valued functions in n variables: 
f: [0, V]n -+ R 
of the form 
f(x) = {3. x 
where {3 == (f31,"" f3n) is an arbitrary n-dimensional vector that satisfies 
the constraint 
n 
EIf3il $ M. 
i=l 
• Hn: a subset of :F, whose elements are parametrized by a number of pa-
rameters proportional to n. We will assume that the sets Hn form a nested 
family, that is 
For example Hn could be the set of polynomials in one variable of degree 
n - 1, Radial Basis Functions with n centers or multilayer perceptrons with 
n hidden units. Notice that for Radial Basis Functions with moving centers 
and Multilayer perceptrons the number of parameters of an element of Hn is 

54 
INFORMATIONAL COMPLEXITY OF LEARNING 
not n, but it is proportional to n (respectively n(k + 1) and n(k + 2), where 
k is the number of variables). 
• H: it is defined as H = U::'=I Hn, and it is identified with the approximation 
scheme. If Hn is the set of polynomials in one variable of degree n - 1, H is 
the set of polynomials of any degree. 
• Hm,p(Rk): the Sobolev space of functions in k variables whose derivatives 
up to order m are in U(Rk). 
• I[!]: the expected risk, defined as 
I[J] == f 
dxdy P(x, y)(y - l(x))2 
lxxy 
where I is any function for which this expression is well defined. It is a 
measure of how well the function I predicts the response y. 
• Iemp[J): the empirical risk. It is a functional on U defined as 
where {(Xi, Yi)}~=1 is a set of data randomly drawn from X x Y according 
to the probability distribution P(x, y). It is an approximate measure of the 
expected risk, since it converges to I[J) in probability when the number of 
data points 1 tends to infinity. 
• k: it will always indicate the number of independent variables, and therefore 
the dimensionality of the set X. 
• 1: it will always indicate the number of data points drawn from X according 
to the probability distribution P(x). 
• £2(p): the set of function whose square is integrable with respect to the 
measure defined by the probability distribution P. The norm in £2(p) is 
therefore defined by 
• Am(Rk)(Mo, MI , M2 , •.. , Mm): the space of functions in k variables whose 
derivatives up to order m are bounded: 
IDal1 ~ Mlal lal = 1,2, ... , m 
where a is a multi-index. 

GENERALIZATION ERROR FOR NEURAL NETS 
55 
• M: a bound on the coefficients of the gaussian Radial Basis Functions tech-
nique considered in this paper, see eq. (2.13). 
• M(£, S, d): the packing number of the set S, with metric d. 
• N(£,S,d): the covering number ofthe set S, with metric d. 
• 
n: a positive number proportional to the number of parameters of the ap-
proximating function. Usually will be the number of basis functions for the 
RBF technique or the number of hidden units for a multilayer perceptron. 
• P(x): a probability distribution defined on X. It is the probability distri-
bution according to which the data are drawn from X. 
• P(ylx): the conditional probability of the response y given the predictor x. 
It represents the probabilistic dependence of y from x. If there is no noise 
in the system it has the form P(ylx) = 6(y - h(x», for some function h, 
indicating that the predictor x uniquely determines the response y. 
• P(x, y): the joint distribution of the predictors and the response. It is a 
probability distribution on X x Y and has the form 
P(x, y) == P(x)P(ylx) . 
• S: it will usually denote a metric space, endowed with a metric d. 
• S: a generic subset of a metric space S. 
• 7: a generic £-cover of a subset S s; S. 
• U: it gives a bound on the elements of the class A. In the specific case of 
the class A considere in the proof we have U = 1 + MV. 
• U: the set of all the functions from X to Y for which the expected risk is 
well defined. 
• V: a bound on the Gaussian basis function G: 
o ~ G(x) ~ V, "Ix E Ric . 
• X: a subset of Ric, not necessarily proper. It is the set of the indepen-
dent variables, or predictors, or, in the language of neural networks, input 
variables. 
• x: a generic element of X, and therefore a k-dimensional vector (in the 
neural network language is the input vector). 
• Y: a subset of R, whose elements represent the response variable, that in 
the neural networks language is the output of the network. Unless otherwise 
stated it will be assumed to be compact, implying that :F is a set of bounded 
functions. In pattern recognition problem it is simply the set {O, I}. 
• y: a generic element of Y, it denotes the response variable. 

56 
INFORMATIONAL COMPLEXITY OF LEARNING 
2-B 
A USEFUL DECOMPOSITION OF THE EXPECTED RISK 
We now show that the function that minimizes the expected risk 
1[J] = f 
P(x, y)dxdy(y - I(X))2 . 
lxxy 
is the regression function defined in eq. (2.3). It is sufficient to add and subtract 
the regression function in the definition of expected risk: 
1[/1 = fxxy dxdyP(x, y)(y - lo(x) + lo(x) - l(x))2 
= fxxy dxdyP(x, y)(y - 10(x))2+ 
+ fxxy dxdyP(x, y)(fo(x) - l(x))2 + 
+ 2 fxxy dxdyP(x, y)(y - lo(x))(fo(x) - I(x)) 
By definition of the regression function lo(x), the cross product in the last 
equation is easily seen to be zero, and therefore 
1[J] = Ix dxP(x)(fo(x) - l(x))2 + 1[/01 . 
Since the last term of [[J] does not depend on I, the minimum is achieved 
when the first term is minimum, that is when I(x) = lo(x). 
In the case in which the data come from randomly sampling a function 1 in 
presence of additive noise, €, with probability distribution P(€) and zero mean, 
we have P(ylx) = P(y - I(x)) and then 
1[/01 = f 
dxdyP(x, y)(y - 10(X))2 = 
lxxy 
= Ix dxP(x) i (y - I(X))2p(y - I(x)) = 
= 1 
dxP(x) i €2P(€)d€ 
= u2 
(2.17) 
(2.18) 
(2.19) 
where u2 is the variance of the noise. When data are noisy, therefore, even in 
the most favourable case we cannot expect the expected risk to be smaller than 
the variance of the noise. 
2-C 
A USEFUL INEQUALITY 
Let us assume that, with probability 1-6 a uniform bound has been established: 
11emp[J]- [[J]I ~ w(l, n, 6) '</1 E Hn . 
We want to prove that the following inequality also holds: 

GENERALIZATION ERROR FOR NEURAL NETS 
57 
I[ fn] 
I[t n,ll 
c:::::: 
7 
~ 7 
2e 
I 
2e 
I 
iemp [fn ] 
iemp [rn,l ] 
Figure 2.9. 
If the distance between 1[Jn] and 1[in,l] is larger than 2£, the condition 
1emp[in,d ~ 1emp[In] is violated. 
11[Jn] - 1[in,dl ~ 2w(i, n, 8) . 
(2.20) 
This fact is easily established by noting that since the bound above is uniform, 
then it holds for both In and in,/, and therefore the following inequalities hold: 
1emp [In] ~ 1[In] + w 
Moreover, by definition, the two following inequalities also hold: 
1emp[in,d ~ 1emp[Jn] 
Therefore tha following chain of inequalities hold, proving inequality (2.20): 
1[In] ~ 1[in,d ~ 1emp[in,d + w ~ 1emp[Jn] + w ~ 1[In] + 2w . 
An intutitive explanation of these inequalities is also explained in figure (2.9). 
2-D 
PROOF OF THE MAIN THEOREM 
The theorem will be proved in a series of steps. Conceptually, there are four 
major steps in the proof outlined in the proof structure below. 
Structure of Proof 
Step 1 
The total generalization error is decomposed into its approximation and es-
timation components. Using the derivations outlined in appendices 2-B, and 

58 
INFORMATIONAL COMPLEXITY OF LEARNING 
2-C, we are able to show that the decomposition has the form of statement 2.2.1 
of section 2.2, viz., with probability 1 - 6, 
11/0 - in.dli2(p) ~ c(n) + 2w(/, n, 6) . 
(2.21) 
We now need to compute c(n) and w(l, n, 6) and these constitute steps 2 and 3 
of the proof structure. 
Step 2 
We obtain a bound on c(n) (the approximation error) in section 2-D.l. The 
fundamental lemma used here is the Maurey-Jones-Barron lemma (Lemma 2-
D.1) and the approximation bound is obtained. 
Step 3 
We obtain a bound on the estimation error w(/, n, 6) in section 2-D.2. Recall 
that we need to be able to prove a uniform law of large numbers of the form: 
VI E Hn, II[!] - lemp[/]1 ~ w(l, n, 6) 
with probability greater than 1 - 6. 
Starting with a uniform law of the form stated in Claim 2-D.l and refining 
it further we arrive at Claim 2-D.3. In doing this, we introduce notions of 
covering numbers and metric entropy. The form of this refined uniform law of 
large numbers is: 
1 
2/ 
In order to let 1-4C(t-j16,A, dLl)]e- mu4 f 
be greater than 1- 6, we need to 
obtain an expression for C( f/16, A, dp)] in terms of the number of parameters. 
Claims 2-D.4 through 2-D.9 go through this computation. 
Finally, in claim 2-D. 10, we show how to use this result to compute an 
expression for w(l, n, 6) which is what we originally set out to do. 
Step 4 
Putting together the approximation and estimation bounds of steps 2 and 3, 
we obtain in section 2-D.3 how the expression for the total generalization error 
in the appropriate form in order to prove the main theorem. 
2-D. 1 Bounding the approximation error 
In this part we attempt to bound the approximation error. In section 2.3 we 
assumed that the class of functions to which the regression function belongs, 
that is the class of functions that we want to approximate, is 
F == {III = A * Gm , m > k/2, IAIRk ~ M} . 
where A is a signed Radon measure on the Borel sets of Rk, Gm is the Bessel-
Macdonald kernel as defined in section 2.3 and M is a positive real number. 
Our approximating family is the class: 

GENERALIZATION ERROR FOR NEURAL NETS 
59 
It has been shown in [49, 50] that the class Hn uniformly approximate elements 
of F, and that the following bound is valid: 
(2.22) 
This result is based on a lemma by Jones [73] on the convergence rate of an 
iterative approximation scheme in Hilbert spaces. A formally similar lemma, 
brought to our attention by R. Dudley [38] is due to Maurey and was published 
by Pisier [109]. Here we report a version of the lemma due to Barron [7, 8] that 
contains a slight refinement of Jones' result: 
Lemma 2-0.1 (Maurey-Jones-Barron) If f is in the closure of the convex 
hull of a set g in a Hilbert space H with Ilgll :5 b for each g E g, then for every 
n ~ 1 and for c > b2 - 11/112 there is a In in the convex hull of n points in 9 
such that 
In order to exploit this result one needs to define suitable classes of functions 
which are the closure of the convex hull of some subset g of a Hilbert space 
H. One way to approach the problem consists in utilizing the integral repre-
sentation of functions. Suppose that the functions in a Hilbert space H can be 
represented by the integral 
I(x) = 1M Gm(x; t)do:(t) 
(2.23) 
where 0: is some measure on the parameter set M, and Gm(x; t) is a function of 
H parametrized by the parameter t, whose norm IIGm(x; t)1I is bounded by the 
same number for any value of t. In particular, if we let Gm(x; t) be translates 
of Gm by t, i.e., Gm(x - t), and 0: be a finite measure, the integral (2.23) can 
be seen as an infinite convex combination of translates of Gm . 
We now make the following two observations. First, it is clear that elements 
of F have an integral representation of the type (2.23) and are members of the 
Hilbert space H. Second, since A is a finite measure (bounded by M) elements 
of F are infinite convex combinations of translates of Gm . We now make use 
of the important fact that convex combinations of translates of Gm can be 
represented as convex combinations of translates and dilates of Gaussians (in 
other words sets of the form of H n for some n). 
This allows us to define g of lemma 2-D.1 to be the parametrized set g = 
{glg(x) = G( IIx~tll n. Clearly, elements of :F lie in the convex hull of 9 as 

60 
INFORMATIONAL COMPLEXITY OF LEARNING 
defined above and therefore, applying lemma (2-D.1) one can prove ([49, 50]) 
that there exist n coefficients co, n parameter vectors ti, and n choices for CT, 
such that 
n 
1 
Ilf - E c.G(x; ti; CT,)1I2 < 0(-) 
,=1 
-
n 
Notice that the bound (2.22), that is similar in spirit to the result of A. Bar-
ron on multilayer perceptrons [7, 8], is interesting because the rate of conver-
gence does not depend on the dimension d of the input space. This is apparently 
unusual in approximation theory, because it is known, from the theory of linear 
and nonlinear widths [126, 108, 91, 92, 32, 31, 33, 96], that, if the function that 
has to be approximated has d variables and a degree of smoothness s, we should 
not expect to find an approximation technique whose approximation error goes 
to zero faster than O( n - iT). Here "degree of smoothness" is a measure of how 
constrained the class of functions we consider is, for example the number of 
derivatives that are uniformly bounded, or the number of derivatives that are 
integrable or square integrable. Therefore, from classical approximation theory, 
we expect that, unless certain constraints are imposed on the class of functions 
to be approximated, the rate of convergence will dramatically slow down as the 
number of dimensions increases, showing the phenomenon known as "the curse 
of dimensionality" [11]. 
In the case of class F we consider here, the constraint of considering functions 
that are convolutions of Radon measures with Gaussians seems to impose on 
this class of functions an amount of smoothness that is sufficient to guarantee 
that the rate of convergence does not become slower and slower as the dimension 
increases. A longer discussion of the "curse of dimensionality" can be found in 
[50]. 
We notice also that, since the rate (2.22) is independent of the dimension, the 
class F, together with the approximating class H n , defines a class of problems 
that are "tractable" even in a high number of dimensions. 
2-D.2 Bounding the estimation error 
In this part we attempt to bound the estimation error IJ[J] - Jemp(f]l. In order 
to do that we first need to introduce some basic concepts and notations. 
Let S be a subset of a metric space S with metric d. We say that an €-cover 
with respect to the metric d is a set T E S such that for every s E S, there 
exists some t E T satisfying d(s, t) ~ f. The size of the smallest f-cover is 
N(f, S, d) and is called the covering number of S. In other words 
N(f,S,d) = minlTI , 
T!;S 
where T runs over all the possible f-cover of Sand ITI denotes the cardinality 
ofT. 

GENERALIZATION ERROR FOR NEURAL NETS 
61 
A set B belonging to the metric space S is said to be e-separated if for all 
x, y E B, d(:c, y) > f. We define the the packing number M(£, S, d) as the size 
of the largest £-separated subset of S. Thus 
M(£,S,d) = maxlBI , 
B~S 
where B runs over all the £-separated subsets of S. It is easy to show that the 
covering number is always less than the packing number, that is N(£, S, d) ~ 
M(£,S,d). 
Let now pee) be a probability distribution defined on S, and A be a set of 
real-valued functions defined on S such that, for any a E A, 
° 
~ ace) ~ u 2 "Ie E S . 
Let also e = (e1, .. ,el) be a sequence of 1 examples drawn independently from 
S according to pee). For any function a E A we define the empirical and true 
expectations of a as follows: 
E[a] = Is dep(e)a(e) 
The difference between the empirical and true expectation can be bounded by 
the following inequality, whose proof can be found in [114] and [63], that will 
be crucial in order to prove our main theorem. 
Claim 2-D.1 ([114], [63]) Let A and e be as defined above. Then, for all 
£ > 0, 
P (3a E A: IE[a]- E[aJi > £) ~ 
In the above result, A( is the restriction of A to the data set, that is: 
A( == {(a(6), ... , a(el» : a E A} . 
(2.24) 
The set A( is a collection of points belonging to the subset [0, U]I of the 1-
dimensional euclidean space. Each function a in A is represented by a point 
in A(, while every point in A( represents all the functions that have the same 
values at the points e1, ... ,el. The distance metric dLl in the inequality above 
is the standard L1 metric in RI, that is 

62 
INFORMATIONAL COMPLEXITY OF LEARNING 
where x and yare points in the I-dimensional euclidean space and zl-' and yl-' 
are their /J-th components respectively. 
The above inequality is a result in the theory of uniform convergence of empir-
ical measures to their underlying probabilities, that has been studied in great 
detail by Pollard and Vapnik, and similar inequalities can be found in the work 
of Vapnik [131, 132, 130], although they usually involve the VC dimension of 
the set A, rather than its covering numbers. 
Suppose now we choose S = X x Y, where X is an arbitrary subset of Ric 
and Y = [-M, M] as in the formulation of our original problem. The generic 
element of S will be written as e = (x, y) E X x Y. We now consider the class 
of functions A defined as: 
A = {a : X x Y -+ R I a( x, y) = (y - h( x»2, h E H n (Ric)) 
where Hn{RIc ) is the class of k-dimensional Radial Basis Functions with n basis 
functions defined in eq. 2.13 in section 2.3. Clearly, 
Iy - h(x)1 ~ Iyl + Ih(x)1 ~ M + MV, 
and therefore 
where we have defined 
U:=M+MV. 
We notice that, by definition of E(a) and E(a) we have 
and 
E(a) = f 
dxdy P(x, y)(y - h(X»2 = I[h] . 
lxxy 
Therefore, applying the inequality of claim 2-D.1 to the set A, and noticing 
that the elements of A are essentially defined by the elements of H n, we obtain 
the following result: 
P('t/h E Hn, IIemp[h] - I[h]1 ~ f) ~ 
(2.25) 
so that the inequality of claim 2-D.1 gives us a bound on the estimation error. 
However, this bound depends on the specific choice of the probability distri-
bution P(x, y), while we are interested in bounds that do not depend on P. 

GENERALIZATION ERROR FOR NEURAL NETS 
63 
Therefore it is useful to define some quantity that does not depend on P, and 
give bounds in terms of that. 
We then introduce the concept of metric capacity of A, that is defined as 
C( f, A, dLl) = sup{N( f, A, dLl(P»} 
P 
where the supremum is taken over all the probability distributions P defined 
over S, and dLl(P) is standard L 1(P) distance9 
induced by the probability distribution P: 
dLl(p)(al. a2) = is dep(e)lal(e) - a2(e)1 al. a2 EA. 
The relationship between the covering number and the metric capacity is showed 
in the following 
Claim 2-D.2 
Proof: For any sequence of points e in S, there is a trivial isometry between 
(A(, dLl) and (A, dLl(p(» where p( is the empirical distribution on the space 
S given by t E!=l 6(e -ei). Here 6 is the Dirac delta function, e E S, and ei is 
the i-th element of the data set. To see that this isometry exists, first note that 
for every element a E A, there exists a unique point (a(ed, ... , a(e,» E A(. 
Thus a simple bijective mapping exists between the two spaces. Now consider 
any two elements 9 and h of A. The distance between them is given by 
This is exactly what the distance between the two points (g(6), .. , g(e,» and 
(h(ed, .. , h(e,», which are elements of A(, is according to the dLl distance. 
Thus there is a one-to-one correspondence between elements of A and A( and 
the distance between two elements in A is the same as the distance between 
their corresponding points in Af Given this isometry, for every f-cover in A, 
there exists an f-cover of the same size in A(, so that 
9Note that here .A is a class of real-valued functions defined on a general metric space S. 
If we consider an arbitrary.A defined on S and taking values in Rn, the dLl(P)' norm is 
appropriately adjusted to be 
dLl(P)(f, g) = ;; t llJ;(X) - 9;(x)IP(x)dx 
;=1 
S 
where f(x) = (11 (x), ... /;(x), ... fn(X», g(x) = (91 (x), ... 9;(X), ... 9n(X» are elements 
of.A and P(X) is a probability distribution on S. Thus dLl and dLl(P) should be interpreted 
according to the context. 

64 
INFORMATIONAL COMPLEXITY OF LEARNING 
N(f,A{,d£l) =N(f,A,d£l(Pd) ~ C(f,A,d£l). 
and consequently E[N(f,A{,d£l)] ~ C(f,A,d£1). 0 
The result above, together with eq. (2.25) shows that the following proposition 
holds: 
Claim 2-D.3 
P(Vh E Hn, IIemp[h]- I[h]1 ~ f) ~ 
( 
/ 
] 
1 
f'/ 
~ 1-4C f 16,A,d£l) e-128U· 
. 
(2.26) 
Thus in order to obtain a uniform bound w on IIemp[h] - I[h]l, our task is 
reduced to computing the metric capacity of the functional class A which we 
have just defined. We will do this in several steps. In Claim 2-D.4, we first 
relate the metric capacity of A to that of the class of radial basis functions 
Hn. Then Claims 2-D.5 through 2-D.9 go through a computation of the metric 
capacity of Hn. 
Claim 2-D.4 
C(f, A, d£l) ~ C(f/4U, Hn, d£l) 
Proof: Fix a distribution P on S = X x Y. Let Px be the marginal distribution 
with respect to X. Suppose K is an f/4U-cover for Hn with respect to this 
probability distribution Px, i.e. with respect to the distance metric d£l(Px ) on 
Hn. Further let the size of K be N(f/4U, Hn, d£l(Px»)' This means that for 
any h E Hn, there exists a function h* belonging to K, such that: 
J 
Ih(x) - h*(x)IPx(x)dx ~ f/4U 
Now we claim the set H(K) = {(y - h(X))2 : h E K} is an f cover for A with 
respect to the distance metric d£l(P)' To see this, it is sufficient to show that 
J I(y - h(x))2 - (y - h*(X))2IP(x, y)dxdy ~ 
~ J 21(2y - h - h*)II(h - h*)IP(x, y)dxdy ~ 
~ J 2(2M + 2MV)lh - h* IP(x, y)dxdy ~ f 
which is clearly true. Now 
N(f,A, d£l(P») ~ IH(K)I = 

GENERALIZATION ERROR FOR NEURAL NETS 
65 
Taking the supremum over all probability distributions, the result follows. 0 
So the problem reduces to finding C(£, H n , dLl), i.e. the metric capacity of the 
class of appropriately defined Radial Basis Functions networks with n centers. 
To do this we will decompose the class Hn to be the composition of two classes 
defined as follows. 
Definitions IN otations 
HI is a class offunctions defined from the metric space (Ric, d £1) to the metric 
space (Rn , d £1 ). In particular, 
HI = {g(x) = (G(lIx - t111), G( IIx - t 211 ), ... , G( Ilx - t nll))} 
0"1 
0"2 
O"n 
where G is a Gaussian and ti are k-dimensional vectors. 
Note here that G is the same Gaussian that we have been using to build our 
Radial-Basis-Function Network. Thus HI is parametrized by the n centers ti 
and the n variances of the Gaussians 0";, in other words n( k + 1) parameters 
in all. 
H F is a class defined from the metric space ([0, V]n, dLl) to the metric space 
(R, dLl). In particular, 
n 
HF = {h(x) =,8. x, x E [0, V]n and L l.8il ~ M} 
i=1 
where ,8 = (.81, ... , .8n) is an arbitrary n-dimensional vector. 
Thus we see that 
Hn = {hF 0 hI : hF E HF and hI E Hd 
where 0 stands for the composition operation, i.e., for any two functions f and 
g, fog = f(g(x)). It should be pointed out that Hn as defined above is defined 
from Ric to R. 
Claim 2-D.5 
Proof: Fix a probability distribution P on Ric. Consider the class 
9 = {g : g(x) = G( Ilx - til), t ERic; 0" E R}. 
0" 
Let K be an N(£,9,dLl(p»)-sized £ cover for this class. We first claim that 

66 
INFORMATIONAL COMPLEXITY OF LEARNING 
is an €-cover for HI with respect to the d£l(P) metric. 
Remember that the d£l(P) distance between two vector-valued functions 
g(x) = (g1(X), .. ,gn(X)) and gOO(x) = (gi(x), .. ,g~(x)) is defined as 
dLl(P)(g,gOO) = ~ ~ f Igi(X) - g;(x)IP(x)dx 
To see this, pick an arbitrary g = (g1, ... , gn) E HI. For each gi, there exists a 
g; E K which is €-close in the appropriate sense for real-valued functions, i.e. 
d£l(P)(9" gt) ::; €. The function g = (gi, .. , g~) is an element of T. Also, the 
distance between (g1, .. , gn) and (gi, .. , g~) in the dLl(P) metric is 
Thus we obtain that 
1 n 
dLl(P)(g, gOO) ::; - L € = € • 
n ,=1 
N(€, HI, dLl(P») ::; [N(€, g, dLl(p»)]n 
and taking the supremum over all probability distributions as usual, we get 
C(€,HI,dLl)::; (C(€,g,dLl)t . 
Now we need to find the capacity ofg. This is done in the Claim 2-0.6. From 
this the result follows. 0 
Definitions IN otations 
Before we proceed to the next step in our proof, some more notation needs to be 
defined. Let A be a family of functions from a set S into R. For any sequence 
e = (6, .. , ed) of points in S, let A( be the restriction of:F to the data set, as per 
our previously introduced notation. Thus A( = {(a(ed, ... , a(ed)) : a E A}. If 
there exists some translation of the set A(, such that it intersects all 2d orthants 
of the space Rd , then e is said to be shattered by A. Expressing this a little 
more formally, let 8 be the set of all possible I-dimensional boolean vectors. If 
there exists a translation t E 'Rd such that for every b E 8, there exists some 
function ab E A satisfying ab(ei) - ti ~ bi ¢> b, = 1 for all i = 1 to d, then 
the set (e1, .. , ed) is shattered by A. Note that the inequality could easily have 
been defined to be strict and would not ha.ve made a difference. The largest d 
such that there exists a sequence of d points which are shattered by A is said 
to be the pseudo-dimension of A denoted by pdimA. 0 
In this context, there are two important theorems which we will need to use. 
We give these theorems without proof. 
Theorem 2.2 (Dudley) Let F be a k-dimensional vector space of functions 
from a set S into R. Then pdim( F) = k. 

GENERALIZATION ERROR FOR NEURAL NETS 
67 
The following theorem is stated and proved in a somewhat more general form 
by Pollard. Haussler, using techniques from Pollard has proved the specific 
form shown here. 
Theorem 2.3 (Pollard, Haussler) Let F be a family of functions from a 
set S into [Ml, M 2], where pdim(F) = d for some 1 $ d < 00. Let P be a 
probability distribution on S. Then for all 0< f $ M2 - M l , 
M(f,F,dLl(P» < 2 (~2e(M2 _ Ml)IOg~2e(M2 _ Mt}) d 
Here M(f, F, dLl(P» is the packing number of F according to the distance met-
ric dLl(P). 
Claim 2-D.6 
Proof: Consider the k + 2-dimensional vector space of functions from Rk to R 
defined as 
Gl == span{l, Zl, x2,., xk, IIx1I2} 
where x E Rk and z/J is the Jt-th component of the vector x. Now consider the 
class 
G2 = {Ve-J : f E Gd 
We claim that the pseudo-dimension of g denoted by pdim(g) fulfills the fol-
lowing inequality, 
pdim (g) $ pdim (G2 ) = pdim (Gt) = (k + 2). 
To see this consider the fact that g ~ G2 • Consequently, for every sequence of 
points x = (Xl, ... ,Xd), gx ~ (G2)x. Thus if (Xl, ... ,Xd) is shattered by g, it 
will be shattered by G2 • This establishes the first inequality. 
We now show that pdim(G2) $ pdim(Gl). It is enough to show that every 
set shattered by G2 is also shattered by Gl . Suppose there exists a sequence 
(Xl, X2, ... , Xd) which is shattered by G2. This means that by our definition 
of shattering, there exists a translation t E Rd such that for every boolean 
vector b E {O, l}d there is some function gb = Ve- Jb where fb E Gl satisfying 
gb(Xi) ~ ti ifand only if bi = 1, where ti and bi are the i-th components oft and 
b respectively. First notice that every function in G2 is positive. Consequently, 
we see that every ti has to be greater than 0, for otherwise, gb(Xi) could never be 
less than ti which it is required to be if bi = 0. Having established that every ti 
is greater than 0, we now show that the set (Xl, X2, ... , Xd) is shattered by Gl . 

68 
INFORMATIONAL COMPLEXITY OF LEARNING 
We let the translation in this case be t' = (log(tdV), log(t2/V), ... , log(td/V)). 
We can take the log since the tdV's are greater than O. Now for every boolean 
vector b, we take the function -/bE G1 and we see that since 
if follows that 
-/b ~ log(t;/V) = t'i <=> bi = l. 
Thus we see that the set (X1,X2, ... ,Xd) can be shattered by G1. By a similar 
argument, it is also possible to show that pdim(Gt) ~ pdim(G2). 
Since G1 is a vector space of dimensionality k + 2, an application of Dudley's 
Theorem [37] yields the value k+2 for its pseudo-dimension. Further, functions 
in the class g are in the range [0, V]. Now we see (by an application of Pollard's 
theorem) that 
N(f., g, d£1(P» ~ M(f., g, dL l(P» 
~ 
~ 2 eefV In e~v))pdim(g) ~ 
~ 2 e~v In (2~V))(k+2) 
Taking the supremum over all probability distributions, the result follows.D 
Claim 2-D.7 
Proof: The proof of this runs in very similar fashion. First note that 
HF~ {,B·x:x, ,BERn}. 
The latter set is a vector space of dimensionality n and by Dudley's theorem 
[37], we see that its pseudo-dimension pdim is n. Also, clearly by the same 
argument as in the previous proposition, we have that pdim(HF) ~ n. To get 
bounds on the functions in H F, notice that 
n 
n 
n 
I I:,8izil ~ I: l,8illzil ~ VI: l,8il ~ MV. 
i=l 
i=l 
i=l 
Thus functions in HF are bounded in the range [-MV, MV]. Now using Pol-
lard's result [63], [114], we have that 
N(f., HF, d£1(P» 
~ M(f., HF, dL l(P» ~ 
~ 2 (4~eV In (4~eV)t . 

GENERALIZATION ERROR FOR NEURAL NETS 
69 
Taking supremums over all probability distributions, the result follows. 0 
Claim 2-D.8 A uniform first-order Lipschitz bound of HF is Mn. 
Proof: Suppose we have x, y E Rn such that 
dLl(x, y) $ f. 
The quantity M n is a uniform first-order Lipschitz bound for H F if, for any 
element of HF, parametrized by a vector {3, the following inequality holds: 
Ix . {3 - y . {31 $ M nf 
Now clearly, 
$ L:?=ll.8ill(Zi - Yi)1 $ 
$ M L:?=1 I(Zi - Yi)1 $ Mnf 
The result is proved. 0 
Claim 2-D.9 
Proof: Fix a distribution P on Rk. Assume we have an f/(2Mn)-cover for HI 
with respect to the probability distribution P and metric dLl(P). Let it be K 
where 
IKI = N(f/2Mn, HI, d£l(P». 
Now each function f E K maps the space Rk into Rn, thus inducing a proba-
bility distribution PJ on the space Rn. Specifically, PJ can be defined as the 
distribution obtained from the measure Jl. J defined so that any measurable set 
A ~ Rn will have measure 
Jl.j(A) = j 
P(x)dx. 
J-l(A) 
Further, there exists a cover Kj which is an f/2-cover for HF with respect to 
the probability distribution PJ . In other words 
We claim that 
H(K) = {f 0 9 : 9 E K and f E Kg} 

70 
INFORMATIONAL COMPLEXITY OF LEARNING 
is an € cover for H n. Further we note that 
IH(K)I = L-JEK IKJI ~ L-JEK C(€/2,HF,d L l) ~ 
~ N(€/(2Mn), HI, d£1(P»C(€/2, HF, dL l) 
To see that H(K) is an €-cover, suppose we are given an arbitrary function 
h J 0 hi E H n. There clearly exists a function hi E K such that 
f dL l(h;(x), h;{x»P(x)dx ~ €/(2Mn) 
iRk 
Now there also exists a function hi E Kh; such that 
IRk Ihl 0 ht(x) - hj 0 ht(x)IP(x)dx = 
= I R" Ihl(Y) - hj(y)IPh;(y)dy ~ €/2 . 
To show that H(K) is an €-cover it is sufficient to show that 
Now 
IRk Ihl 0 hi(X) - hj 0 ht(x)IP(x)dx ~ 
~ IRk{lhJ 0 hi(x) - hi 0 ht(x)l+ 
+Ihl 0 ht(x) - hj 0 ht(x)IP(x)dx} 
by the triangle inequality. Further, since hi is Lipschitz bounded, 
Also, 
IRk Ihl 0 h;(x) - hi 0 hi(x)IP(x)dx ~ 
~ IRk Mnd£1(h;(x), ht(x»P(x)dx ~ Mn(€/2Mn) ~ €/2 . 
IRk Ihl 0 ht(x) - hj 0 ht(x)IP(x)dx = 
= IR" Ihl(Y) - hj(y)IPh;(y)dy ~ €/2 . 
Consequently both sums are less than €/2 and the total integral is less than (. 
N ow we see that 
N(€, Hn, dL l(P» ~ N (€/(2Mn), HI, dL l(P») C(€/2, HF, dL l). 
Taking supremums over all probability distributions, the result follows. 0 
Having obtained the crucial bound on the metric capacity of the class Hn, we 
can now prove the following 

GENERALIZATION ERROR FOR NEURAL NETS 
71 
Claim 2-D.10 With probability 1-6, and'Vh E Hn , the following bound holds: 
Proof: We know from the previous claim that 
~ [SM:Vn In(SM:vn )rCk+3) . 
From claim (2-D.3), we see that 
(2.27) 
~1-6 
as long as 
1 
f2/ 
6 
C(f/16,A,dLl)e-12su4 
~4 
which in turn is satisfied as long as (by Claim 2-DA) 
which implies 
U256MeVUn In U256MeVUn)r Ck+3) 
In other words, 
( ~n In ( Afn) ) n(k+3) e- f2 //B ~ ~ 
for constants A, B. The latter inequality is satisfied as long as 
which implies 
2n(k + 3)(ln(An) -In(f)) - f21/ B ~ In(6/4) 
and in turn implies 

72 
INFORMATIONAL COMPLEXITY OF LEARNING 
f2[ > B In(4/6) + 2Bn(k + 3)(ln(An) -In(f)). 
We now show that the above inequality is satisfied for 
f = ( B [In( 4/6) + 2n(k + 3) ~n(An) + n(k + 3) In(l)]) 1/2 
Putting the above value of f in the inequality of interest, we get 
f2(1/ B) = In(4/8) + 2n(k + 3) In(An) + n(k + 3) In(l) ~ 
~ In(4/8) + 2n(k + 3) In(An)+ 
+2n(k + 3H In (B[ln(4/o)+2n(k+3)in(An)+n(k+3) In(l)]) 
In other words, 
n(k + 3) In(l) ~ 
Since 
B [In(4/8) + 2n(k + 3) In(An) + n(k + 3) In(l)] ~ 1 
the inequality is obviously true for this value of f. Taking this value of f then 
proves our claim. 0 
2-D.3 Bounding the generalization error 
Finally we are able to take our results in Parts II and III to prove our main 
result: 
Theorem 2.4 With probability greater than 1 - 8 the following inequality is 
valid: 
II ' 
fA 
112 
0(1) 
0([nkln(nl)-ln8]1/2) 
)0 -
n,l PCP) ~ 
;; + 
I 
Proof: We have seen in statement (2.2.1) that the generalization error is 
bounded as follows: 
A 
2 
lifo - fn,zlIL2(p) ~ €(n) + 2w(l, n, 8) . 
In section (??) we showed that 

GENERALIZATION ERROR FOR NEURAL NETS 
73 
and in claim (2-D.l0) we showed that 
w(l, n, 6) = 0 ([ nk In(n:) -In 6] 1/2) 
Therefore the theorem is proved putting these results together. 0 

3 
INVESTIGATING THE SAMPLE 
COMPLEXITY OF ACTIVE LEARNING 
SCHEMES 
In the classical learning framework of the previous chapter (akin to PAC) examples were 
randomly drawn and presented to the learner. In this chapter, we consider the possibility of a 
more active learner who is allowed to choose his/her own examples. Our investigations can be 
divided into two natural parts. The first, is in a function approximation setting, and develops 
an adaptive sampling strategy (equivalent to adaptive approximation) motivated from the 
standpoint of optimal recovery (Micchelli and Rivlin, 1976). We provide a general formulation 
of the problem. This can be regarded as sequential optimal recovery. We demonstrate the 
application of this general formulation to two special cases of functions on the real line 1) 
monotonically increasing functions and 2) functions with bounded derivative. An extensive 
investigation of the sample complexity of approximating these functions is conducted yielding 
both theoretical and empirical results on test functions. Our theoretical results (stated in 
PAC-style), along with the simulations demonstrate the superiority of our active scheme over 
both passive learning as well as classical optimal recovery. The second part of this chapter 
is in a concept learning framework and discusses the idea of e-focusing: a scheme where the 
active learner can iteratively draw examples from smaller and smaller regions of the input 
space thereby gaining vast improvements in sample complexity. 
In Chapter 2, we considered a learning paradigm where the learner's hypoth-
esis was constrained to belong to a class of functions which can be represented 
by a sum of radial basis functions. It was assumed that the examples ((x, y) 
pairs) were drawn according to some fixed, unknown, arbitrary, probability dis-
tribution. In this important sense, the learner was merely a passive recipient 
75 

76 
INFORMATIONAL COMPLEXITY OF LEARNING 
of information about the target function. In this chapter, we consider the pos-
sibility of a more active learner. There are of course a myriad ways in which a 
learner could be more active. Consider, for example, the extreme pathological 
case where the learner simply asks for the true target function which is duly 
provided by an obliging oracle. This, the reader will quickly realize is hardly 
interesting. Such pathological cases aside, this theme of activity on the part 
of the learner has been explored (though it is not always conceived as such) in 
a number of different settings (PAC-style concept learning, boundary-hunting 
pattern recognition schemes, adaptive integration, optimal sampling etc.) in 
more principled ways and we will comment on these in due course. 
For our purposes, we restrict our attention in this chapter to the situation 
where the learner is allowed to choose its own examples! , in other words, decide 
where in the domain D (for functions defined from D to Y) it would like to 
sample the target function. Note that this is in direct contrast to the passive 
case where the learner is presented with randomly drawn examples. Keeping 
other factors in the learning paradigm unchanged, we then compare in this 
chapter, the active and passive learners who differ only in their method of 
collecting examples. At the outset, we are particularly interested in whether 
there exist principled ways of collecting examples in the first place. A second 
important consideration is whether these ways allow the learner to learn with 
a fewer number of examples. This latter question is particularly in keeping 
with the spirit of this book, viz., the informational complexity of learning from 
examples. 
This chapter can be divided into two parts which are roughly self-contained. 
In Part I, we consider active learning in an approximation-theoretic setting. We 
develop a general framework for collecting examples for approximating (learn-
ing) real-valued functions. We then demonstrate the application of these to 
some specific classes of functions. We obtain theoretical bounds on the sample 
complexity of the active and passive learners, and perform some empirical sim-
ulations to demonstrate the superiority of the active learner. Part II discusses 
the idea of f-focusing-a paradigm in which the learner iteratively focuses in on 
specific "interesting" regions of the input space to collect its examples. This 
is largely in a concept learning (alternatively, pattern classification) setting2 • 
We are able to show how using this idea, one can get large gains in sample 
complexity for some concept classes. 
lThis can be regarded as a computational instantiation of the psychological practice of 3e-
lective attention where a human might choose to selectively concentrate on interesting or 
confusing regions of the feature space in order to better grasp the underlying concept. Con-
sider, for example, the situation when one encounters a speaker with a foreign accent. One 
cues in to this foreign speech by focusing on and then adapting to its distinguishing proper-
ties. This is often accomplished by asking the speaker to repeat words which are confusing 
to us. 
2Concept learning is being used here in the more restrictive sense of learning {a, l}-valued 
functions. This terminology comes from the PAC literature which often focuses on learning 
boolean functions, sets, or other kinds of discrete mappings. Note that pattern classification 
can also be viewed as a concept learning problem in view of the discrete nature of the 
dependent variable involved. 

ACTIVE LEARNING 
77 
Part I: Active Learning for Approximation of 
Real Valued Functions 
3.1 
A GENERAL FRAMEWORK FOR ACTIVE APPROXIMATION 
3.1.1 
Preliminaries 
We need to develop the following notions: 
F: Let F denote a class of functions from some domain D to Y where Y is 
a subset of the real line. The domain D is typically a subset of Rk though 
it could be more general than that. There is some unknown target function 
f E F which has to be approximated by an approximation scheme. 
1): This is a data set obtained by sampling the target f E F at a number of 
points in its domain. Thus, 
1) = {(Xi,Yi)lxi E D,Yi = f(xt), i = 1 .. . n} 
Notice that the data is uncorrupted by noise. 
11.: This is a class offunctions (also from D to Y) from which the learner will 
choose one in an attempt to approximate the target f. Notationally, we will use 
1t to refer not merely to the class of functions (hypothesis class) but also the 
algorithm by means of which the learner picks an approximating function h E 11. 
on the basis of the data set 1). In other words, 11. denotes an approximation 
scheme which is really a tuple < 11., A > . A is an algorithm that takes as its 
input the data set 1), and outputs an hE 11.. 
Examples: If we consider real-valued functions from Rk to R, some typical 
examples of 11. are the class of polynomials of a fixed order (say q), splines of 
some fixed order, radial basis functions with some bound on the number of 
nodes, etc. As a concrete example, consider functions from [0, 1] to R. Imagine 
a data set is collected which consists of examples, i.e., (Xi, Yi) pairs as per our 
notation. Without loss of generality, one could assume that Xi ;::; Xi+l for each 
i. Then a cubic (degree-3) spline is obtained by interpolating the data points by 
polynomial pieces (with the pieces tied together at the data points or "knots") 
such that the overall function is twice-differentiable at the knots. Fig. 3.1 shows 
an example of an arbitrary data set fitted by cubic splines. 
de : We need a metric to determine how good the approximation learner's 
approximation is. Specifically, the metric de measures the approximation error 
on the region C of the domain D. In other words, de, takes as its input any 
two functions (say !1 and h) from D to R and outputs a real number. It is 
assumed that de satisfies all the requisites for being a real distance metric on 
the appropriate space of functions. Since the approximation error on a larger 
domain is obviously going to be greater than that on the smaller domain, we can 
make the following two observations: 1) for any two sets Cl and C2 such that 
Cl ~ C2 , de1(!1,I2);::; dc2(!1, h), 2) dD (!1, 12) is the total approximation on 

78 
INFORMATIONAL COMPLEXITY OF LEARNING 
0.2 
0.4 
0.6 
0.8 
1.0 
x 
Figure 3.1. 
An arbitrary data set fitted with cubic splines 
the entire domain; this is our basic criterion for judging the "goodness" of the 
learner's hypothesis. 
Examples: For real-valued functions from R" to R, the L~ metric defined as 
dc(ft, 12) = (fc 1ft - 12IPd2: )1/p serves as a natural example of an error metric. 
c: This is a collection of subsets C of the domain. We are assuming that points 
in the domain where the function is sampled, divide (partition) the domain into 
a collection of disjoint sets Ci E C such that U~=1 C; = D. 
Examples: For the case of functions from [0,1] to R, and a data set V, a 
natural way in which to partition the domain [0, 1] is into the intervals [2:;, 2:i+1), 
(here again, without loss of generality we have assumed that 2:; ~ 2:i+d. The 
set C could be the set of all (closed, open, or half-open and half-closed) intervals 
[a, b] ~ [0,1]. 
The goal of the learner (operating with an approximation scheme 1i) is to 
provide a hypothesis h E 1£ (which it chooses on the basis of its example 
set V) as an approximator of the unknown target function f E :F. We now 
need to formally lay down a criterion for assessing the competence of a learner 
(approximation scheme). In recent times, there has been much use of PAC 
(Valiant 1984) like criteria to assess learning algorithms. Such a criterion has 
been used largely for concept learning but some extensions to the case of real 
valued functions exist (Haussler, 1992). We adapt here for our purposes a 

ACTIVE LEARNING 
79 
PAC like criterion to judge the efficacy of approximation schemes of the kind 
described earlier. 
Definition 1 An approximation scheme is said to P-PAC learn the function 
f E :F if for every i > 0 and 1 > 6 > 0, and for an arbitrary distribution 
P on D, it collects a data set V, and computes a hypothesis h E 1t such that 
dD(h, f) < i with probability greater than 1- 6. The function class:F is P-PAC 
learnable if the approximation scheme can P-PAC learn every function in :F. 
The class :F is PAC learnable if the approximation scheme can P-PAC learn 
the class for every distribution P. 
There is an important clarification to be made about our definition above. 
Note that the distance metric d is arbitrary. It need not be naturally related 
to the distribution P according to which the data is drawn. Recall that this 
is not so in typical distance metrics used in classical PAC formulations. For 
example, in concept learning, where the set :F consists of indicator functions, 
the metric used is the L1(P) metric given by d(lA' 1B) = JD 11A -lBIP(x)dx. 
Similarly, extensions to real-valued functions typically use an L2(P) metric. 
The use of such metrics imply that the training error is an empirical average of 
the true underlying error. One can then make use of convergence of empirical 
means to true means (Vapnik, 1982) and prove learnability. In our case, this 
is not necessarily the case. For example, one could always come up with a 
distribution P which would never allow a passive learner to see examples in 
a certain region of the domain. However, the arbitrary metric d might weigh 
this region heavily. Thus the learner would never be able to learn such a 
function class for this metric. In this sense, our model is more demanding 
than classical PAC. To make matters easy, we will consider here the case of 
P - PAC learnability alone, where P is a known distribution (uniform in the 
example cases studied). However, there is a sense in which our notion of PAC 
is easier -the learner knows the true metric d and given any two functions, 
can compute their relative distance. This is not so in classical PAC, where the 
learner cannot compute the distance between two functions since it does not 
know the underlying distribution. 
We have left the mechanism of data collection undefined. Our goal here is the 
investigation of different methods of data collection. A baseline against which 
we will compare all such schemes is the passive method of data collection where 
the learner collects its data set by sampling D according to P and receiving the 
point (x,f(x». If the learner were allowed to draw its own examples, are there 
principled ways in which it could do this? Further, as a consequence of this 
flexibility accorded to the learner in its data gathering scheme, could it learn 
the class :F with fewer examples? These are the questions we attempt to resolve 
in this chapter, and we begin by motivating and deriving in the next section, 
a general framework for active selection of data for arbitrary approximation 
schemes. 

80 
INFORMATIONAL COMPLEXITY OF LEARNING 
3.1.2 The Problem of Collecting Examples 
We have introduced in the earlier section, our baseline algorithm for collecting 
examples. This corresponds to a passive learner that draws examples according 
to the probability distribution P on the domain D. If such a passive learner 
collects examples and produces an output h such that dD(h, f) is less than f 
with probability greater than 1-6, it P-PAC learns the function. The number of 
examples that a learner needs before it produces such an (f-good,6-confidence) 
hypothesis is called its sample complexity. 
Against this baseline passive data collection scheme, lies the possibility of 
allowing the learner to choose its own examples. At the outset it might seem 
reasonable to believe that a data set would provide the learner with some in-
formation about the target function; in particular, it would probably inform it 
about the "interesting" regions of the function, or regions where the approx-
imation error is high and need further sampling. On the basis of this kind 
of information (along with other information about the class of functions in 
general) one might be able to decide where to sample next. We formalize this 
notion as follows: 
Let V = {(Xi, Yi); i = 1 ... n} be a data set (containing n data points) which 
the learner has access to. The approximation scheme acts upon this data set 
and picks an h E 'H. (which best fits the data according to the specifics of 
the algorithm A inherent in the approximation scheme). Further, let Ci ; i = 
1, ... , K (n)3 be a partition of the domain D into different regions on the basis 
of this data set. Finally let 
This is the set of all functions in F which are consistent with the data seen so 
far. The target function could be anyone of the functions in F7). 
We first define an error criterion ec (where C is any subset of the domain) 
as follows: 
ec('H., V, F) = sup dc(h, f) 
JE:Fv 
Essentially, ec is a measure of the maximum possible error the approxima-
tion scheme could have (over the region C) given the data it has seen so far. 
It clearly depends on the data, the approximation scheme, and the class of 
functions being learned. It does not depend upon the target function (except 
indirectly in the sense that the data is generated by the target function after 
all, and this dependence is already captured in the expression). We thus have 
a scheme to measure uncertainty (maximum possible error) over the different 
regions of the input space D. One possible strategy to select a new point might 
3The number of regions K(n) into which the domain D is partitioned by n data points 
depends upon the geometry of D and the partition scheme used. For the real line partitioned 
into intervals as in our example, K(n) = n + 1. For k-cubes, one might obtain Voronoi 
partitions and compute K(n) accordingly. 

ACTIVE LEARNING 
81 
simply be to sample the function in the region Gi where the error bound is the 
highest. Let us assume we have a procedure P to do this. P could be to sample 
the region G at the centroid of G, or sampling G according to some distribution 
on it, or any other method one might fancy. This can be described as follows: 
Active Algorithm A 
1. [Initialize] Collect one example (Xl, Y1) by sampling the domain Donee 
according to procedure P. 
2. [Obtain New Partitions] Divide the domain D into regions G1, ... , GK (l) 
on the basis of this data point. 
3. [Compute Uncertainties] Compute ec; for each i. 
4. [General Update and Stopping Rule] In general, at the jth stage, sup-
pose that our partition of the domain D is into Gj, i = 1 ... K(j). One can 
compute ec. for each i and sample the region with maximum uncertainty 
(say G,) according to procedure P. This would provide a new data point 
(XH1, YHt)· The new data point would re-partition the domain D into new 
regions. At any stage, if the maximum uncertainty over the entire domain 
e D is less than £ stop. 
The above algorithm is one possible active strategy. However, one can carry 
the argument a little further and obtain an optimal sampling strategy which 
would give us a precise location for the next sample point. Imagine for a 
moment, that the learner asks for the value of the function at a point xED. 
The value returned obviously belongs to the set 
Assume that the value observed was y E .1"1)(x). In effect, the learner now has 
one more example, the pair (x, y), which it can add to its data set to obtain a 
new, larger data set V' where 
Vi = V U (x, y) 
Once again, the approximation scheme 1f would map the new data set V' 
into a new hypothesis hi. One can compute 
ec{ll., V ' ,.1") = sup d(h', f) 
/E:FVI 
Clearly, eD(1f, V', F) now measures the maximum possible error after seeing 
this new data point. This depends upon (x, y) (in addition to the usual1f, V, 
and .1"). For a fixed x, we don't know the value of y we would observe if we 
had chosen to sample at that point. Consequently, a natural thing to do at 
this stage is to again take a worst case bound, i.e., assume we would get the 

82 
INFORMATIONAL COMPLEXITY OF LEARNING 
most unfavorable y and proceed. This would provide the maximum possible 
error we could make if we had chosen to sample at x. This error (over the entire 
domain) is 
sup eD(1t,V',F)= 
sup eD(1t,VU(x,y),F) 
YE.r1)(~) 
YE.r1)(~) 
Naturally, we would like to sample the point x for which this maximum error 
is minimized. Thus, the optimal point to sample by this argument is 
Xnew = argmin sup eD(1t,VU(x,y),F) 
~ED YE.r1)(~) 
(3.1) 
This provides us with a principled strategy to choose our next point. The 
following optimal active learning algorithm follows: 
Active Algorithm B (Optimal) 
1. [Initialize] Collect one example (;Vi, yd by sampling the domain Donee 
according to procedure P. We do this because without any data, the ap-
proximation scheme would not be able to produce any hypothesis. 
2. [Compute Next Point to Sample] Apply eq. 3.1 and obtain ;V2. Sampling 
the function at this point yields the next data point (;V2, Y2) which is added 
to the data set. 
3. [General Update and Stopping Rule] In general, at the jth stage, as-
sume we have in place a data set Vj (consisting of j data). One can compute 
;Vj+! according to eq. 3.1 and sampling the function here one can obtain a 
new hypothesis and a new data set V j +1 • In general, as in Algorithm A, stop 
whenever the total error eD(1t, Vb F) is less than f. 
By the process of derivation, it should be clear that if we chose to sample 
at some point other than that obtained by eq. 3.1, an adversary could provide 
a y value and a function consistent with all the data provided (including the 
new data point), that would force the learner to make a larger error than if 
the learner chose to sample at ;Vnew' In this sense, algorithm B is optimal. It 
also differs from algorithm A, in that it does not require a partition scheme, 
or a procedure P to choose a point in some region. However, the computation 
of Xnew inherent in algorithm B is typically more intensive than computations 
required by algorithm A. Finally, it is worthwhile to observe that crucial to our 
formulation is the derivation of the error bound eD(1t, V, F). As we have noted 
earlier, this is a measure of the maximum possible error the approximation 
scheme 1t could be forced to make in approximating functions of F using the 
data set V. Now, if one wanted an approximation scheme independent bound, 
this would be obtained by minimizing eD over all possible schemes, i.e., 
inf eD(1t, V, F) 
1£ 

ACTIVE LEARNING 
83 
Any approximation scheme can be forced to make at least as much error as 
the above expression denotes. Another bound of some interest is obtained by 
removing the dependence of eD on the data. Thus given an approximation 
scheme 1l, if data 'D is drawn randomly, one could compute 
P{eD(1l, 'D,:F) > f} 
or in an approximation scheme-independent setting, one computes 
P{inf eD(1l, 'D,:F) > f} 
1t 
The above expressions would provide us PAC-like bounds which we will make 
use of later in this chapter. 
3.1.3 In Context 
Having motivated and derived two possible active strategies, it is worthwhile 
at this stage to comment on the formulation and its place in the context of 
previous work in similar vein executed across a number of disciplines. 
1) Optimal Recovery: The question of choosing the location of points where 
the unknown function will be sampled has been studied within the framework 
of optimal recovery (Micchelli and Rivlin, 1976; Micchelli and Wahba, 1981; 
Athavale and Wahba, 1979). While work of this nature has strong connections 
to our formulation, there remains a crucial difference. Sampling schemes mo-
tivated by optimal recovery are not adaptive. In other words, given a class of 
functions :F (from which the target I is selected), optimal sampling chooses 
the points Zi ED, i = 1, ... , n by optimizing over the entire function space 
:F. Once these points are obtained, then they remain fixed irrespective of the 
target (and correspondingly the data set 'D). Thus, if we wanted to sample 
the function at n points, and had an approximation scheme 1l with which we 
wished to recover the true target, a typical optimal recovery formulation would 
involve sampling the function at the points obtained as a result of optimizing 
the following objective function: 
(3.2) 
where h('D = {(Z;, I(Z;»i=l...n}) E 1l is the learner's hypothesis when the 
target is I and the function is sampled at the Zi'S. Given no knowledge of the 
target, these points are the optimal to sample. 
In contrast, our scheme of sampling can be conceived as an iterative applica-
tion of optimal recovery (one point at a time) by conditioning on the data seen 
so far. Making this absolutely explicit, we start out by asking for one point 
using optimal recovery. We obtain this point by 
arg min sup d(f, h('D1 = {( Zl, I( zt))})) 
x, fEr 
Having sampled at this point (and obtained Y1 from the true target), we can 
now reduce the class of candidate target functions to :F1, the elements of :F 

84 
INFORMATIONAL COMPLEXITY OF LEARNING 
which are consistent with the data seen so far. Now we obtain our second point 
by 
Note that the supremum is done over a restricted set :1"1 the second time. In 
this fashion, we perform optimal recovery at each stage, reducing the class of 
functions over which the supremum is performed. It should be made clear 
that this sequential optimal recovery is not a greedy technique to arrive at the 
solution of eq. 3.2. It will give us a different set of points. Further, this set 
of points will depend upon the target function. In other words, the sampling 
strategy adapts itself to the unknown target f as it gains more information 
about that target through the data. We know of no similar sequential sampling 
scheme in the literature. 
While classical optimal recovery has the formulation of eq. 3.2, imagine a 
situation where a "teacher" who knows the target function and the learner, 
wishes to communicate to the learner the best set of points to minimize the 
error made by the learner. Thus given a function g, this best set of points can 
be obtained by the following optimization 
arg min d(g,h({(Zi,9(Zi»h=l...n» 
,1;'1,···,X,., 
(3.3) 
Eq. 3.2 and eq. 3.3 provide two bounds on the performance of the active 
learner following the strategy of Algorithm B in the previous section. While 
eq. 3.2 chooses optimal points without knowing anything about the target, and, 
eq. 3.3 chooses optimal points knowing the target completely, the active learner 
chooses points optimally on the basis of partial information about the target 
(information provided by the data set). 
2) Concept Learning: The PAC learning community (which has tradition-
ally focused on concept learning) typically incorporates activity on the part of 
the learner by means of queries, the learner can make of an oracle. Queries 
(Angluin, 1988) range from membership queries (is Z an element of the target 
concept c) to statistical queries (Kearns, 1993 ; where the learner can not ask 
for data but can ask for estimates of functionals of the function class) to ar-
bitrary boolean valued queries (see Kulkarni etal for an investigation of query 
complexity). Our form of activity can be considered as a natural adaptation of 
membership queries to the case of learning real-valued functions in our modi-
fied PAC model. It is worthwhile to mention relevant work which touches the 
contents of this chapter at some points. The most significant of these is an 
investigation of the sample complexity of active versus passive learning con-
ducted by Eisenberg and Rivest (1990) for a simple class of unit step functions. 
It was found that a binary search algorithm could vastly outperform a pas-
sive learner in terms of the number of examples it needed to (f,6) learn the 
target function. This chapter is very much in the spirit of that work focusing 
as it does on the sample complexity question. Another interesting direction is 
the transformation of PAC-learning algorithms from a batch to online mode. 

ACTIVE LEARNING 
85 
While Littlestone etal (1991) consider online learning of linear functions, Kim-
ber and Long (1992) consider functions with bounded derivatives which we 
examine later in this chapter. However the question of choosing one's data 
is not addressed at all. Kearns and Schapire (1990) consider the learning of 
p-concepts (which are essentially equivalent to learning classes of real-valued 
functions with noise) and address the learning of monotone functions in this 
context. Again, there is no active component on the part of the learner. 
3)Adaptive Integration: The novelty of our formulation lies in its adaptive 
nature. There are some similarities to work in adaptive numerical integration 
which are worth mentioning. Roughly speaking, an adaptive integration tech-
nique (Berntsen et al 1991) divides the domain of integration into regions over 
which the integration is done. Estimates are then obtained of the error on each 
of these regions. The region with maximum error is subdivided. Though the 
spirit of such an adaptive approach is close to ours, specific results in the field 
naturally differ because of differences between the integration problem (and its 
error bounds) and the approximation problem. 
4) Bayesian and other formulations: It should be noted that we have a 
worst-case formulation (the supremum in our formulation represents the maxi-
mum possible error the scheme might have). Alternate bayesian schemes have 
been devised (Mackay, 1991; Cohn, 1994) from the perspective of optimal ex-
periment design (Fedorov). Apart from the inherently different philosophical 
positions of the two schemes, an indepth treatment of the sample complexity 
question is not done. We will soon give two examples where we address this sam-
ple complexity question closely. In a separate piece of work (Sung and Niyogi, 
1994) , the author has also investigated such bayesian formulations from such an 
information-theoretic perspective. Yet another average-case formulation comes 
from the information-complexity viewpoint of Traub and Wozniakovski (see 
Traub etal (1988) for details). Various interesting sampling strategies are sug-
gested by research in that spirit. We do not attempt to compare them due to 
the difficulty in comparing worst-case and average-case bounds. 
5) Generating Examples and "Hints": Rather than choosing its new ex-
amples, the learner might generate them by virtue of having some prior knowl-
edge of the learning task. For example, prior knowledge that the target function 
is odd would allow the learner to generate a new (symmetric) example: for every 
(:c,l(:c» pair, the learner could add the example (-:c, - I(:c» to the training 
set. For vision tasks, Poggio and Vetter (1992) use similarity transformations 
like rotation, translation and the like to generate new images from old ones. 
More generally, Abu-Mostafa (1993) has formalized the approach as learning 
from hints showing how arbitrary hints can be incorporated in the learning 
process. Hints induce activity on the part of the learner and the connection 
between the two is worth investigating further. 
Thus, we have motivated and derived in this section, two possible active 
strategies. The formulation is general. We now demonstrate the usefulness 
of such a formulation by considering two classes of real-valued functions as 
examples and deriving specific active algorithms from this perspective. At this 
stage, the important question of sample complexity of active versus passive 

86 
INFORMATIONAL COMPLEXITY OF LEARNING 
learning still remains unresolved. We investigate this more closely by deriving 
theoretical bounds and performing empirical simulation studies in the case of 
the specific classes we consider. 
3.2 
EXAMPLE 1: A CLASS OF MONOTONICALLY INCREASING 
BOUNDED FUNCTIONS 
Consider the following class of functions from the interval [0, 1] ~ 1R to 1R : 
:F = {f : 0 ~ 1 ~ M, and I(x) ~ l(y)'Vx ~ y} 
Note that the functions belonging to this class need not be continuous though 
they do need to be measurable. This class is PAC- learnable (with an L 1(P) 
norm, in which case our notion of PAC reduces to the classical notion) though 
it has infinite pseudo-dimension4(in the sense of Pollard (1984)). Thus, we 
observe: 
Observation 1 The class :F has infinite pseudo-dimension (in the sense 01 
Pollard {l984); Haussler (1992),). 
Proof: To have infinite pseudo-dimension, it must be the case that for every 
n > 0, there exists a set of points {Xl, ... , xn} which is shattered by the class:F. 
In other words, there must exist a fixed translation vector t = (t1' ... , tn) such 
that for every boolean vector b = (b 1, ••• , bn ), there exists a function 1 E :F 
which satisfies I(Xi) - ti > 0 {:} bi = 1. To see that this is indeed the case, let 
the n points be Xi = i/(n+ 1) for i going from 1 to n. Let the translation vector 
then be given by ti = Xi. For an arbitrary boolean vector b we can always come 
up with a monotonic function such that I(Xi) = i/(n + 1) -1/3(n + 1) if bi = 0 
and I(Xi) = i/(n + 1) + 1/3(n + 1) if bi = 1. 0 
We also need to specify the terms 'H, dc, the procedure P for partition-
ing the domain D = [0,1] and so on. For our purposes, we assume that 
the approximation scheme 'H is first order splines. This is simply finding the 
monotonic function which interpolates the data in a piece-wise linear fash-
ion. A natural way to partition the domain is to divide it into the intervals 
[0, Xl)' [Xl. X2), ... , [Xi, Xi+d,· .. , [xn, 1]. The metric de is an Lp metric given 
by dC(l1, h) = (fo1/11- h/pdx)l/p • 
Note that we are specifically interested in comparing the sample complexities 
of passive and active learning. We will do this under a uniform distributional 
assumption, i.e., the passive learner draws its examples by sampling the target 
function uniformly at random on its domain [0,1]. In contrast, we will show how 
our general formulation in the earlier section translates into a specific active 
algorithm for choosing points, and we derive bounds on its sample complexity. 
We begin by first providing a lower bound for the number of examples a passive 
PAC learner would need to draw to learn this class :F. 
4Finite pseudo-dimension is only a sufficient and not necessary condition for PAC learnability 
as this example demonstrates. 

ACTIVE LEARNING 
87 
3.2.1 
Lower Bound for Passive Learning 
Theorem 3.1 Any passive learning algorithm (more specifically, any approxi-
mation scheme which draws data uniformly at random and interpolates the data 
by any arbitrary bounded function) will have to draw at least ~(M/2e)P In(1/6) 
examples to P-PAC learn the class where P is a uniform distribution. 
Proof: Consider the uniform distribution on [0,1] and a subclass offunctions 
which have value 0 on the region A = [0, 1-(2f)P] and belong to F. Suppose the 
passive learner draws I examples uniformly at random. Then with probability 
(1- (210/ M)P)', all these examples will be drawn from region A. It only remains 
to show that for the subclass considered, whatever be the function hypothesized 
by the learner, an adversary can force it to make a large error. 
Suppose the learner hypothesizes that the function is h. Let the value of 
(~I-(2f/M)P,I) Ih(x)IPdx)l/p be X· Obviously 0 ~ X ~ (MP(2e/M)P)I/p = 2e. If 
X < 10, then the adversary can claim that the target function was really 
g(x) = { ~ for x E [0,1 - (2e/ M)P] 
for x E (1- (2e/M)P, 1] 
If, on the other hand X ~ 10, then the adversary can claim the function was 
really 9 = O. 
In the first case, by the triangle inequality, 
d(h, g) = (Iro,I]lg - hIPdx)l/p ~ (IrI-(2f/M)P,I]lg - hIPdx)l/p 
~ U(1-(2</M)P,I) MPdx)l/p - U(I-(2f/M)P,I) IhIPdx)I/P = 210 - X> f 
In the second case, 
d(h, g) = ([ 
Ig - hIPdx)I/P ~ (1 
10 - hIPdx)I/P = X > e 
l[o, I] 
(1-(2</M)p,l) 
Now we need to find out how large I must be so that this particular event 
of drawing all examples in A is not very likely, in particular, it has probability 
less than 6. 
For (1- (2f/ M)P)' to be greater than 6, we need I < 
In(1 t2f/M)P) In( t). It 
is a fact that for a < 1/2, 2~ ~ _ In(\ _ a)' Making use of this fact (and setting 
a = (2f/M)P, we see that for f < (~)(~)I/p, we have ~(M/2f)Pln(1/6) < 
-In(l-t2f/M)P) In(t)· So unless I is greater than HM/2e)Pln(1/6), the prob-
ability that all examples are chosen from A is greater than 6. Consequently, 
with probability greater than 6, the passive learner is forced to make an error 
of atleast e, and PAC learning cannot take place. 0 

88 
INFORMATIONAL COMPLEXITY OF LEARNING 
3.2.2 
Active Learning Algorithms 
In the previous section we computed a lower bound for passively PAC learning 
this class for a uniform distribution5 . Here we derive an active learning strategy 
(the CLA algorithm) which would meaningfully choose new examples on the 
basis of information gathered about the target from previous examples. This 
is a specific instantiation of the general formulation, and interestingly yields 
a "divide and conquer" binary searching algorithm starting from a different 
philosophical standpoint. We formally prove an upper bound on the number 
of examples it requires to PAC learn the class. While this upper bound is a 
worst case bound and holds for all functions in the class, the actual number of 
queries (examples) this strategy takes differs widely depending upon the target 
function. We demonstrate empirically the performance of this strategy for 
different kinds of functions in the class in order to get a feel for this difference. 
We derive a classical non-sequential optimal sampling strategy and show that 
this is equivalent to uniformly sampling the target function. Finally, we are 
able to empirically demonstrate that the active algorithm outperforms both the 
passive and uniform methods of data collection. 
3.2.2.1 
Derivation of an optimal sampling strategy. Consider an ap-
proximation scheme of the sort described earlier attempting to approximate a 
target function f E F on the basis of a data set D. Shown in fig. 3.2 is a picture 
of the situation. We can assume without loss of generality that we start out by 
knowing the value of the function at the points x = 0 and x = 1. The points 
{Xi; i = 1, ... , n} divide the domain into n + 1 intervals Ci (i going from 0 
to n) where C; = [Xi, Xi+l](XO = 0, Xn+l = 1).The monotonicity constraint on 
:F permits us to obtain rectangular boxes showing the values that the target 
function could take at the points on its domain. The set of all functions which 
lie within these boxes as shown is :FT). 
Let us first compute ec,(1i,D,F) for some interval Ci. On this interval, the 
function is constrained to lie in the appropriate box. We can zoom in on this 
box as shown in fig. 3.3. 
The maximum error the approximation scheme could have (indicated by the 
shaded region) is clearly given by 
(i
1h - f(Xi)IPdx)l/P = ( [B (A x)Pdx)l/P = AB1/p I(p + l)l/p 
C. 
10 
B 
where A = f(xi+d - f(xd and B = (Xi+! - xd. 
Clearly the error over the entire domain eD is given by 
n 
e~ = 2::e~. 
(3.4) 
i=O 
5Naturally, this is a distribution-free lower bound as well. In other words, we have demon-
strated the existence of a distribution for which the passive learner would have to draw at 
least as many examples as the theorem suggests. 

ACTIVE LEARNING 
89 
~-
I 
I 
/ 
---
o 
Figure 3.2. 
A depiction of the situation for an arbitrary data set. The set F'D consists of 
all functions lying in the boxes and passing through the datapoints (for example, the dotted 
lines). The approximating function h is a linear interpolant shown by a solid line. 
Figure 3.3. 
Zoomed version of interval. The maximum error the approximation scheme 
could have is indicated by the shaded region. This happens when the adversary claims the 
target function had the value Yi throughout the interval. 

90 
INFORMATIONAL COMPLEXITY OF LEARNING 
The computation of ec is all we need to implement an active strategy moti-
vated by Algorithm A in section 3.1. All we need to do is sample the function in 
the interval with largest error; recall that we need a procedure P to determine 
how to sample this interval to obtain a new data point. We choose (arbitrar-
ily) to sample the midpoint of the interval with the largest error yielding the 
following algorithm. 
The Choose and Learn Algorithm (CLA) 
1. [Initial Step] Ask for values of the function at points x = 0 and x = 1. 
At this stage, the domain [0,1] is composed of one interval only, viz., [0,1]. 
Compute E1 = (p+i)l/P (1- O)l/PI(f(l) - 1(0»1 and T1 = E 1. If T1 < f, stop 
and output the linear interpolant of the samples as the hypothesis, otherwise 
query the midpoint of the interval to get a partition of the domain into two 
subintervals [0,1/2) and [1/2,1]. 
2. [General Update and Stopping Rule] In general, at the kth stage, sup-
pose that our partition of the interval [0,1] is [xo = 0, xd,[xl, X2), ... , 
[Xk-1, Xk = 1]. 
We compute the normalized error Ei = (p+h 1/p (Xi -
xi-d 1/ PI(f(Xi) - l(xi-1»1 for all i = 1, .. , k. The midpoint of the inter-
val with maximum Ei is queried for the next sample. The total normalized 
error Tk = (E~=l Ef)l/P is computed at each stage and the process is termi-
nated when Tk $ f. Our hypothesis h at every stage is a linear interpolation 
of all the points sampled so far and our final hypothesis is obtained upon 
the termination of the whole process. 
Now imagine that we chose to sample at a point x E Ci = [Xi, Xi+1] and 
received the value Y E .r'l)(x) (i.e., y in the box) as shown in the fig. 3.4. This 
adds one more interval and divides Ci into two subintervals Cil and Ci2 where 
Cil = [Xi, X] and Ci2 = [x, Xi+1]. We also correspondingly obtain two smaller 
boxes inside the larger box within which the function is now constrained to lie. 
The uncertainty measure ec can be recomputed taking this into account. 
Observation 2 The addition of the new data point (x,y) does not change the 
uncertainty value on any of the other intervals. It only affects the interval Ci 
which got subdivided. The total uncertainty over this interval is now given by 
(pi1)1/P «x - Xi)(Y - l(xi»P + (Xi+1 - x»«(f(Xi+d - I(xi» _ y)p)l/P = 
= G (zrP + (B - z)(A - r)p)l/P 
where for convenience we have used the substitution z = X - Xi, r = y- f(xi), 
and A and B are I(Xi+d - f(x;) and Xi+1 - Xi as above. Clearly z ranges from 
o to B while r ranges from 0 to A. 
We first prove the following lemma: 

ACTIVE LEARNING 
91 
--""""""-""-"""" .. ~ 
c, 
Figure 3.4. 
The situation when the interval Ci is sam pled yielding a new data point. 
This subdivides the interval into two subintervals and the two shaded boxes indicate the 
new constraints on the function. 
Lemma 3.2.1 
B/2=arg min 
sup G(zrP+(B-z)(A-r)p)l/p 
ze[O,B] re[o,A] 
Proof: Consider any z E [0, B]. There are three cases to consider: 
Case I z > B/2 : let z = B/2 + a where a > 0. We find 
( 
)
lh 
sup G (zrP + (B - z)(A - r)p)l/P = 
sup G (zrP + (B - z)(A - r)P) 
re[O,A] 
re[O,A] 
Now, 
sUPre[O,A] G (zrP + (B - z)(A - r)P) = 
sUPre[O,A] G «B/2 + a)rP + (B/2 - a)(A - r)P) 
= GsuPre[O,A] B/2(rP + (A - r)P) + a(rP - (A - r)P) 
Now for r = A, the expression within the supremum B/2(rP + (A - r)P) + 
a(rP - (A - r)P) is equal to (B/2 + a)AP. For any other r E [0, A], we need to 
show that 
B/2(rP + (A - r)P) + a(rP - (A - r)P) ~ (B/2 + a)AP 
or 
B/2«r/A)P + (1- (r/AW) + a«r/A)P - (1- r/A)P) ~ B/2 + a 

92 
INFORMATIONAL COMPLEXITY OF LEARNING 
Putting f3 = riA (clearly f3 E [0,1], and noticing that (1 - (3)P $ 1 -
f3P and 
f3P - (1 - (3)P $ 1 the inequality above is established. Consequently, we are 
able to see that 
sup G (zrP + (B - z)(A - r)p)l/P = G(B/2 + o:)l/P A 
rE[O,Aj 
Case II Let z = B /2 - 0: for 0: > 0. In this case, by a similar argument as 
above, it is possible to show that again, 
sup G (zrP + (B - z)(A - r)p)l/P = G(B/2 + o:)l/P A 
rE[O,Aj 
Case III Finally, let z = B/2. Here 
sup G (zrP + (B - z)(A - r)p)l/P = G(B/2)1/P sup (rP + (A - r)p)l/P 
rE[O,Aj 
rE[O,Aj 
Clearly, then for this case, the above expression is reduced to GA(B/2)1/p • 
Considering the three cases, the lemma is proved.D 
The above lemma in conjunction with eq. 3.4 and observation 2 proves that 
if we choose to sample a particular interval Ci then sampling the midpoint is 
the optimal thing to do. In particular, we see that 
minxE[Xi,Xi+lj sUPYE(J(Xi),/(Xi+l)j eCi('H, 'D U (x, y),:F) = 
In other words, if the learner were constrained to pick its next sample in the 
interval Ci, then by sampling the midpoint of this interval Ci, the learner 
ensures that the maximum error it could be forced to make by a malicious 
adversary is minimized. In particular, if the uncertainty over the interval Ci 
with its current data set 'D, is eCi' the uncertainty over this region will be 
reduced after sampling its midpoint and can have a maximum value of ecJ21/ p • 
Now which interval must the learner sample to minimize the maximum pos-
sible uncertainty over the entire domain D = [0,1]. Noting that if the learner 
chose to sample the interval Ci then 
min 
sup eD=[O,lj(1t, 'D U (x, y),:F) = 
XECi=[Xi,Xi+d yE:Fv 
From the decomposition above, it is clear that the optimal point to sample 
according to the principle embodied in Algorithm B is the midpoint of the 
interval Cj which has the maximum uncertainty eCj(1t, 'D,:F) on the basis of 
the data seen so far, i.e., the data set 'D. Thus we can state the following 
theorem 

ACTIVE LEARNING 
93 
Theorem 3.2 The CLA is the optimal algorithm for the class of monotonic 
functions 
Having thus established that our binary searching algorithm (CLA) is opti-
mal, we now turn our efforts to determining the number of examples the CLA 
would need in order to learn the unknown target function to f accuracy with 0 
confidence. In particular, we can prove the following theorem. 
Theorem 3.3 The CLA converges in at most (M/f)P steps. Specifically, after 
collecting at most (M/f)P examples, its hypothesis is f close to the target with 
probability 1. 
Proof Sketch: The proof of convergence for this algorithm is a little tedious. 
However, to convince the reader, we provide the proof of convergence for a 
slight variant of the active algorithm. It is possible to show (not shown here) 
that convergence times for the active algorithm described earlier is bounded by 
the convergence time for the variant. First, consider a uniform grid of points 
(f/M)P apart on the domain [0,1]. Now imagine that the active learner works 
just as described earlier but with a slight twist, viz., it can only query points 
on this grid. Thus at the kth stage, instead of querying the true midpoint of 
the interval with largest uncertainty, it will query the gridpoint closest to this 
midpoint. Obviously the intervals at the kth stage are also separated by points 
on the grid (i.e. previous queries). If it is the case that the learner has queried 
all the points on the grid, then the maximum possible error it could make is 
less than f. To see this, let a = f/ M and let us first look at a specific small 
interval [ka, (k + 1)a]. We know the following to be true for this subinterval: 
f(ka) = h(ka) $ f(z), h(z) $ f«k + 1)a) = h«k + 1)a) 
Thus 
If(z) - h(z)1 $ f«k + 1)a) - f(ka) 
and so over the interval [ka, (k + 1)a] 
J1!+1)a If(z) - h(z)IPdz $ J1!+1)a(l«k + 1)a) - f(ka»Pdz 
$ (I«k + 1)a) - f(ka»pa 
It follows that 
j[0,1]If - hlPdz = j[o,a) If - hlPdz + ... + j[1-a,1] If - hlPdz $ 
a «(I(a) - f(O»P + (I(2a) - f(a»P + ... + (1(1) - f(1 - a»P) $ 
a(l(a) - f(O) + f(2a) - f(a) + ... + f(1) - f(1 - a»P $ 
$ a(l(1) - f(O»P $ aMP 

94 
INFORMATIONAL COMPLEXITY OF LEARNING 
So if a = (f./M)P, we see that the Lp error would be at most: 
[ 
If - hlPdz 
~ f. 
( 
) 
IIp 
1[0,1] 
Thus the active learner moves from stage to stage collecting examples at the 
grid points. It could converge at any stage, but clearly after it has seen the 
value of the unknown target at all the gridpoints, its error is provably less than 
f. and consequently it must stop by this time. 0 
3.2.3 Empirical Simulations, and other Investigations 
Our aim here is to characterize the performance of CLA as an active learning 
strategy. Remember that CLA is an adaptive example choosing strategy and 
the number of samples it would take to converge depends upon the specific 
nature of the target function. We have already computed an upper bound on 
the number of samples it would take to converge in the worst case. In this sec-
tion we try to provide some intuition as to how this sampling strategy differs 
from random draw of points (equivalent to passive learning) or drawing points 
on a uniform grid (equivalent to optimal recovery following eq. 3.2 as we shall 
see shortly). We perform simulations on arbitrary monotonic increasing func-
tions to better characterize conditions under which the active strategy could 
outperform both a passive learner as well as a uniform learner. 
3.2.3.1 
Distribution of Points Selected. As has been mentioned earlier, 
the points selected by CLA depend upon the specific target function.Shown 
in fig. 3-5 is the performance of the algorithm for an arbitrarily constructed 
monotonically increasing function. Notice the manner in which it chooses its 
examples. Informally speaking, in regions where the function changes a lot 
(such regions can be considered to have high information density and con-
sequently more "interesting"), CLA samples densely. In regions where the 
function doesn't change much (correspondingly low information density), it 
samples sparsely. As a matter of fact, the density of the points seems to follow 
the derivative of the target function as shown in fig. 3.6. 
Consequently, we conjecture that 
Conjecture 1 The density of points sampled by the active learning algorithm 
is proportional to the derivative of the function at that point for differentiable 
functions. 
Remarks: 
1. The CLA seems to sample functions according to its rate of change over the 
different regions. We have remarked earlier, that the best possible sampling 
strategy would be obtained by eq. 3.3 earlier. This corresponds to a teacher 
(who knows the target function and the learner) selecting points for the 
learner. How does the CLA sampling strategy differ from the best possible 
one? Does the sampling strategy converge to the best possible one as the data 

ACTIVE LEARNING 
95 
Figure 3.5. 
How the CLA chooses its examples. Vertical lines have been drawn to mark 
the x-coordinates of the points at which the algorithm asks for the value of the function. 
goes to infinity? In other words, does the CLA discover the best strategy? 
These are interesting questions. We do not know the answer. 
2. We remarked earlier that another bound on the performance of the active 
strategy was that provided by the classical optimal recovery formulation of 
eq. 3.2. This, as we shall show in the next section, is equivalent to uniform 
sampling. We remind the reader that a crucial difference between uniform 
sampling and CLA lies in the fact that CLA is an adaptive strategy and for 
some functions might actually learn with very few examples. We will explore 
this difference soon. 
3.2.3.2 
Classical Optimal Recovery. For an L1 error criterion, classical 
optimal recovery as given by eq. 3.2 yields a uniform sampling strategy. To see 
this, imagine that we chose to sample the function at points Xi; i = 1, ... , n. 
Pick a possible target function! and let Yi = !(Xi) for each i. We then get 
the situation depicted in fig. 3.7. The n points divide the domain into n + 1 
intervals. Let these intervals have length ai each as shown. Further, if [Xi-l, Xi] 
corresponds to the interval of length ai, then let Yi - Yi -1 = bi . In other words 
we would get n + 1 rectangles with sides ai and bi as shown in the figure. 
It is clear that choosing a vector b = (b1 , ... , bn+d' with the constraint that 
E~ll b; = M and b; ~ 0 is equivalent to defining a set of Y values (in other 

96 
INFORMATIONAL COMPLEXITY OF LEARNING 
CII ... 
CD 
o 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
x 
Figure 3.6. 
The dotted line shows the density of the sam pies along the x-axis when the 
target was the monotone-function of the previous example. The bold line is a plot of the 
derivative of the function. Notice the correlation between the two. 
words, a data set) which can be generated by some function in the class :F. 
Specifically, the data values at the respective sample points would be given by 
Y1 = b1, Y2 = b1 + b2 and so on. We can define :Fb to be the set of monotonic 
functions in :F which are consistent with these data points. In fact, every f E :F 
would map onto some b, and thus belong to some :Fb. Consequently, 
Given a target function f E :Fb, and a choice of n points Xi, one can construct 
the data set 1) = {(Xi, f(Xi»h=l...n and the approximation scheme generates 
an approximating function h(1). It should be clear that for an L1 distance 
metric (d(J, h) = J; If - hldx), the following is true: 
1 n+1 
1 
sup d(J, h) = 2" L aibi = 2"a.b 
JE:Fb 
;=1 
Thus, taking the supremum over the entire class of functions is equivalent 
to 
1 
sup d(J, h(1)) = 
sup 
2"a.b 
JE:F 
{b:b;~o,E b;=M} 

ACTIVE LEARNING 
97 
Figure 3.7. 
The situation when a function! E :F is picked, n sample points (the z's) 
are chosen and the corresponding y values are obtained. Each choice of sample points 
corresponds to a choice of the a's. Each choice of a function corresponds to a choice of the 
b/S. 
The above is a straight forward linear programming problem and yields as its 
solution the result ~Mmax{ai' i = 1, ... , (n + I)}. 
Finally, every choice of n points Zi, i = 1, ... , n results in a corresponding 
vector a where ai ~ 0 and L ai = 1. Thus minimizing the maximum error over 
all the choice of sample points (according to eq. 3.2) is equivalent to 
arg min sup dU, h(V = {(Zi, !(Zi»};=1...n) = 
:1:1,···,:1:" JEF 
= arg 
min 
max{ai; i = 1 ... n + I} 
{a:ai~O'Lai=l} 
Clearly the solution of the above problem is ai = n~l for each i. 
In other words, classical optimal recovery suggests that one should sample 
the function uniformly. Note that this is not an adaptive scheme. In the next 
section, we compare empirically the performance of three different schemes to 
sample. The passive, where one samples randomly, the non-sequential "opti-
mal" , where one samples uniformly, and the active which follows our sequen-
tially optimal strategy. 
3.2.3.3 
Error Rates and Sample Complexities for some Arbitrary 
Functions: Some Simulations. In this section, we attempt to relate the 
number of examples drawn and error made by the learner for a variety of arbi-
trary monotone increasing functions. We begin with the following simulation: 
Simulation A: 
1. Pick an arbitrary monotone-increasing function. 

98 
INFORMATIONAL COMPLEXITY OF LEARNING 
~ 
'~ 
, "-
\~--... --
------
'------~~ 
-----.. --
~...... 
-----
...... _-----------
---.. -... ----
..,. 
'i' 
C!' 
; 
II: 
gS! 
w' 
~ 
... _---
• 
'";" 
CI) 
'";" 
0 
500 
1000 
1500 
N 
Figure 3.S. 
Error rates as a function of the number of examples for the arbitrary monotone 
function shown in a previous figure. 
2. Decide (N), the number of samples to be collected. There are three methods 
of collection of samples. The first is by randomly drawing N examples 
according to a uniform distribution on [0, 1] (corresponding to the passive 
case). The second is by asking for function values on a uniform grid on [0,1] 
of grid spacing liN. The third is the eLA. 
3. The three learning algorithms differ only in their method of obtaining sam-
ples. Once the samples are obtained, all three algorithms attempt to approx-
imate the target by the monotone function which is the linear interpolant of 
the samples. 
4. This entire process is now repeated for various values of N for the same 
target function and then repeated again for different target functions. 
Results: Let us first consider performance on the arbitrarily selected mono-
tonic function of the earlier section. Shown in fig. 3.8 are performance for the 
three different algorithms. Notice that the active learning strategy (eLA) has 
the lowest error rate. On an average, the error rate of random sampling is 8 
times the rate of eLA and uniform sampling is 1.5 times the rate of eLA. 
Figure 3.9 shows four other monotonic functions on which we ran the same 
simulations comparing the three sampling strategies. The results of the simu-
lations are shown in Fig. 3.10 and Table 3.10. Notice that the active strategy 

ACTIVE LEARNING 
99 
~O 
xC! 
.?s.,....: 
f 
___ r" 
C}/<X! 
C'lCX) 
.§o 
Co 
-<0 
.g <0 
o 
. 
o 
. 
co 
:J 
co 
:J 
-..,. 
CD 
• 
cO 
-;~ 
cO 
~"! 
.9N 
° . 
cO 
) 
cO 
°0 
°0 
::::!. 
::::!c:i 
0 
0.0 
0.2 
0.4 
0.6 
O.S 
1.0 
0.0 
0.2 
0.4 
0.6 
O.S 
1.0 
x 
x 
xC! 
___ r" 
~O 
)( . 
___ r" 
~<X! 
SO 
U?<X! 
SO 
..... <0 
o 
. 
co 
g~ 
.2..,. 
CD 
• 
cO 
.2..,. 
CD 
• 
cO 
.9N 
.9N 
° . 
° . 
cO 
cO 
°0 
::::!c:i 
°0 
::::!c:i 
0.0 
0.2 
0.4 
0.6 
O.S 
1.0 
0.0 
0.2 
0.4 
0.6 
O.S 
1.0 
x 
x 
Figure 3.9. 
Four other monotonic functions on which sim ulations have been run com paring 
random, uniform, and active sampling strategies. 

100 
INFORMATIONAL COMPLEXITY OF LEARNING 
Function No. 
Average Random/CLA 
Average Uniform/CLA 
1 
7.23 
1.66 
2 
61.37 
10.91 
3 
6.67 
1.10 
4 
8.07 
1.62 
5 
6.62 
1.56 
Table 3.1. 
Shown in this table is the average error rate of the random sampling and the 
uniform sampling strategies when as a multiple of the error rates due to CLA. Thus for the 
function 3 for example, uniform error rates are on an average 1.1 times CLA error rates. 
The averages are taken over the different values of N (number of examples) for which 
the simulations have been done. Note that this is not a very meaningful average as the 
difference in the error rates between the various strategies grow with N (as can be seen 
from the curves)if there is a difference in the order of the sample com plexity. However they 
have been provided just to give a feel for the numbers. 
(CLA) far outperforms the passive strategy and clearly has the best error per-
formance. The comparison between uniform sampling and active sampling is 
more interesting. For functions like function-2 (which is a smooth approxi-
mation of a step function), where most of the "information" is located in a 
small region of the domain, CLA outperforms the uniform learner by a large 
amount. Functions like function-3 which don't have any clearly identified re-
gion of greater information have the least difference between CLA and the 
uniform learner (as also between the passive and active learner). Finally on 
functions which lie in between these two extremes (like functions 4 and 5) we 
see decreased error-rates due to CLA which are in between the two extremes. 
In conclusion, the active learner outperforms the passive learner. Further, it 
is even better than classical optimal recovery. The significant advantage of the 
active learner lies in its adaptive nature. Thus, for certain "easy" functions, 
it might converge very rapidly. For others, it might take as long as classical 
optimal recovery, though never more. 
3.3 
EXAMPLE 2: A CLASS OF FUNCTIONS WITH BOUNDED FIRST 
DERIVATIVE 
Here the class of functions we consider are from [0,1] to R and of the form 
:F = {fIJ( z) is differentiable and I :~ I ::-::; d} 
Notice a few things about this class. First, there is no direct bound on the 
values that functions in :F can take. In other words, for every M > 0, there 
exists some function J E :F such that J(z) > M for some z E [0,1]. However, 

~ 
.CO 
r:: ' 
u. 
SOl» 
~~ 
... ' 
g~ 
w~ 
.... , 
i .. 
I' .\ \ \, 
, ... 
", ~.: ... ~.~: .. ~ ............................................... . 
--- ....... ""------------
o 500 1 000 1500 2000 2500 3000 
N 
o 500 1 000 1500 2000 2500 3000 
N 
co 
C')' 
ceo 
u.' 
al° -.... 
(!J 
, 
a:C\I 
2:;: 
....... 
w' 
co 
.... , 
ACTIVE LEARNING 
101 
o 500 1000 1500 2000 2500 3000 
N 
Figure 3.10. 
This figure plots the log of the error (L1 error) against N the number of 
examples for each of the 4 monotonic functions shown in fig. 3.9. The solid line represents 
error rates for random sampling, the line with small dashes represents uniform sampling and 
the line with long dashes represents results for CLA. Notice how CLA beats random sampling 
by large amounts and does slightly better than uniform sam piing. 

102 
INFORMATIONAL COMPLEXITY OF LEARNING 
there is a bound on the first derivative, which means that a particular function 
belonging to :F cannot itself change very sharply. Knowing the value of the 
function at any point, we can bound the value of the function at all other 
points. So for example, for every f E:F, we see that If(z)1 ::; dxf(O) ::; df(O). 
We observe that this class too has infinite pseudo-dimension. We state this 
without proof. 
Observation 3 The class :F has infinite pseudo-dimension in the sense of 
Pollard. 
As in the previous example we would like to investigate the possibility of 
devising active learning strategies for this class. We first provide a lower bound 
on the number of examples a learner (whether passive or active) would need in 
order to t identify this class. We then derive in the next section, an optimal 
active learning strategy (that is, an instantiation of the Active Algorithm B 
earlier). We also provide an upper bound on the number of examples this 
active algorithm would take. 
We also need to specify some other terms for this class of functions. The 
approximation scheme 1f. is a first order spline as before, the domain D = [0,1] 
is partitioned into intervals by the data [Xi, zi+d (again as before) and the 
metric de is an L1 metric given by dC(!1, h) = Ie 1!1(z) - h(z)ldz. The 
results in this section can be extended to an Lp norm but we confine ourselves 
to an L1 metric for simplicity of presentation. 
3.3.1 
Lower Bounds 
Theorem 3.4 Any learning algorithm (whether passive or active) has to draw 
at least O«d/t» examples (whether randomly or by choosing) in order to PAC 
learn the class :F. 
Proof Sketch: Let us assume that the learner collects m examples (pas-
sively by drawing according to some distribution, or actively by any other 
means). Now we show that an adversary can force the learner to make an error 
of at least t if it draws less than O«d/t» examples. This is how the adversary 
functions. 
At each of the m points which are collected by the learner, the adversary 
claims the function has value 0. Thus the learner is reduced to coming up with 
a hypothesis that belongs to :F and which it claims will be within an t of the 
target function. Now we need to show that whatever the function hypothesized 
by the learner, the adversary can always come up with some other function, 
also belonging to :F, and agreeing with all the data points, which is more than 
an t distance away from the learner's hypothesis. In this way, the learner will 
be forced to make an error greater than to 
The m points drawn by the learner, divides the region [0,1] into (at most) 
m + 1 different intervals. Let the length of these intervals be b1 , b2 , b3 , ••. , bm+ 1. 
The "true" function, or in other words, the function the adversary will present, 
should have value ° 
at the endpoints of each of the above intervals. We first 
state the following lemma. 

ACTIVE LEARNING 
103 
Lemma 3.3.1 There exists a function I E :F such that I interpolates the data 
and 
1 
kd 
I/ldx> 4( 
1) 
[0,1] 
m + 
where k is a constant arbitrarily close to 1. 
Proof: Consider fig. 3.11. The function I is indicated by the dark line. As is 
shown, I changes sign at each x = x;. Without loss of generality, we consider 
an interval [x;, xi+d of length bi. Let the midpoint of this interval be z = 
(Xi + xi+d/2. The function here has the values 
{ 
d(x - Xi) 
I(x) = 
-d(x - xi+d 
d(X-z)2 + d(b.-a) 
2", 
2 
Simple algebra shows that 
Clearly, a can be chosen small, so that 
for x E [x;, z - a] 
for x E [z + a,xi+d 
for x E [z - a, z + a] 
Figure 3.11. 
Construction of a function satisying Lemma 2. 
where k is as close to 1 as we want. By combining the different pieces of the 
function we see that 
{1 
kd m+1 
io I/ldx > - E bl 
o 
4 
i 

104 
INFOR~IATION'-\L COMPLEXITY OF LEARNING 
Now we make use of the following lemma, 
Lemma 3.3.2 For a set of numbers bl , .. , bm such that bl + b'J + .. + bm = 1, 
the following inequality is true 
Proof: By induction. 0 
Now it is easy to see how the adversary functions. Suppose the learner pos-
tulates that the true function is h. Let Iro.1) Ihldx = y. If X > f, t.he adversary 
claims that the true funct.ion was f = O. In that case fo1 Ih - fld.t· = y > f. If 
on the ot.her hand, X < f, then the adversary claims that the true function was 
f (as above). In that case, 
11 
11 
11 
kd 
If - hldx ~ 
Ifldx -
Ihldx = 
1) - X 
o 
0 
0 
4(m+ 
Clearly, if m is less than ~~ - 1, the learner is forced again to make an error 
greater t.han f. Thus in either case, the learner is forced to make an error greater 
than or equal to f if less than O(d/f} examples are collected (howsoever these 
examples are collected). 0 
The previous result holds for all learning algorithms. It is possible t.o show 
the following result for a passive learner. 
Theorem 3.5 A Passive learner must draw at least 
max(O«d/f), Ad/f} In(1/6))) to learn this class. 
Proof Sketch: The d/f term in the lower bound follows directly from the 
previous theorem. We show how t.he second term is obt.ained. 
Consider the uniform distribution on [0,1] and a subclass offunctions which 
have value 0 on the region A = [0,1- a] and belong t.o F. Suppose the passive 
learner draws I examples uniformly at random. Then with probabilit.y (1 - a)l, 
all these examples will be drawn from region A. It only remains to show that for 
this event, and the subclass considered, whatever be the function hypothesized 
by the learner, an adversary can force it to make a large error. 
It is easy to show (using the arguments of the earlier theorem) t.hat there 
exists a function f E F such that f is 0 on A and fLa Ifldx = ~Q2d. This 
is equal to 2f if a = A4f/d). Now let the learner's hypothesis be h. Let 
f11_a Ihldx = y. If X is greater than f, the adversary claims the target was 
g = O. Otherwise, t.he adversary claims the target was g = f. In either case, 
fig - hldx > f. 
It is possible to show (by an identical argument to the proof of theorem 1), 
that unless I ~ tAd/f) In( 1/6), all examples will be drawn from A. with prob-
ability greater than 6 and the learner will be forced to make an error greater 
than f. Thus the second term appears indicating t.he dependence on 6 in the 
lower bound.D 

ACTIVE LEARNING 
105 
3.3.2 
Active Learning Algorithms 
We now derive in this section an algorithm which actively selects new examples 
on the basis of information gathered from previous examples. This illustrates 
how our formulation of section 3.1.1 can be used in this case to effectively obtain 
an optimal adaptive sampling strategy. 
3.3.2.1 
Derivation of an optimal sampling strategy. Fig. 3.12 shows 
an arbitrary data set containing information about some unknown target func-
tion. Since the target is known to have a first derivative bounded by d, it is 
I .. 
o 
Figure 3.12. 
An arbitrary data set for the case of functions with a bounded derivative. 
The functions in F'f) are constrained to lie in the parallelograms as shown. The slopes of 
the lines making up the parallelogram are d and -d appropriately. 
clear that the target is constrained to lie within the parallelograms shown in 
the figure. The slopes of the lines making up the parallelogram are d and -d 
appropriately. Thus, F'f) consists of all functions which lie within the parallel-
ograms and interpolate the data set. We can now compute the uncertainty of 
the approximation scheme over any interval,G, (given by ec(1i, 1), F)), for this 
case. Recall that the approximation scheme 1i is a first order spline, and the 
data 1) consists of (x, y) pairs. Fig. 3.13 shows the situation for a particular in-
terval (G; = [Xi, Xi+l])' Here i ranges from ° to n. As in the previous example, 
we let Xo = 0, and Xn+1 = 1. 
The maximum error the approximation scheme 1£ could have on this interval 
is given by (half the area of the parallelogram). 
1 
(d2Bl-A?) 
ec;(1i,1),F) = sup 
Ih-fldx= 
4d 
t 
JE:FD 
C; 
where Ai = If(Xi+l) - f(x;) I and Bi = Xi+l - Xi. Clearly, the maximum error 
the approximation scheme could have over the entire domain is given by 
n f 
n 
eD=[O,l](1i, 1), F) = sup L J( If - hldx = L eCj 
JE:FD j=O Cj 
j=O 
(3.5) 

106 
INFORMATIONAL COMPLEXITY OF LEARNING 
The computation of ec is crucial to the derivation of the active sampling 
strategy. Now imagine that we chose to sample at a point x in the interval Ci 
and received a value y (belonging to .1"1)(x». This adds one more interval and 
divides Ci into two intervals Cil and Ci2 as shown in fig. 3.14 .. We also obtain 
two correspondingly smaller parallelograms within which the target function is 
now constrained to lie. 
Figure 3.13. 
A zoomed version of the ith interval. 
The addition of this new data point to the data set (V' = V U (x, y» re-
quires us to recompute the learner's hypothesis (denoted by hi in the fig. 3.14). 
Correspondingly, it also requires us to update ec, i.e., we now need to compute 
ec(H, V', .1"). First we observe that the addition of the new data point does not 
affect the uncertainty measure on any interval other than the divided interval 
Ci. This is clear when we notice that the parallelograms (whose area denotes 
the uncertainty on each interval) for all the other intervals are unaffected by 
the new data point. 
Thus, 
ec; (H, V',:/") = ec;(H, V,:/") = 41d( d2 BJ - AJ) for j i- i 
For the ith interval Ci, the total uncertainty is now recomputed as (half the 
sum of the two parallelograms in fig. 3.14) 
ec;(H, V /,.1") = 41d (d2u2 - v2) + (d2(B; - u)2 - (Ai - v)2») 
(3.6) 
= ~d (d2u2 + d2(Bi - u)2) - (v2 + (A - v)2») 
where u = x - Xi, v = Y - Yi, and Ai and Bi are as before. Note that u 
ranges from 0 to Bi, for Xi ::; X ::; Xi+!. However, given a particular choice 

ACTIVE LEARNING 
107 
of x (this fixes a value of u), the possible values v can take are constrained 
by the geometry of the parallelogram. In particular, v can only lie within the 
parallelogram. For a particular x, we know that .1"v(x) represents the set of all 
possible Y values we can receive. Since v = Y-Yi, it is clear that v E .1"V(X)-Yi. 
Naturally, if Y < Yi, we find that v < 0, and Ai - v> Ai. Similarly, if Y > Yi+1, 
we find that v > Ai. 
Figure 3.14. 
Subdivision of the ith interval when a new data point is obtained. 
We now prove the following lemma: 
Lemma 3.3.3 The following two identities are valid for the appropriate mini-
max problem. 
(1)'f = argmiIluE[o,B] sUPvE{,rV(X)-Yi} (d2u2 + d2(B - u)2) - (v2 + (A - v)2») 
(2) 1(d2 B2 - A2) = 
min:E[o,B] sUPvE{,rV(X)-Yi} (d2u2 + d2(B - u)2) - (v2 + (A - V)2») 
Proof: The expression on the right is a difference of two quadratic expressions 
and can be expressed as Ql(U) - Q2(V). For a particular u, the expression is 
maximized when the quadratic Q2(V) = (v2 + (A - v)2) is minimized. Observe 
that this quadratic is globally minimized at v = A/2. We need to perform this 
minimization over the set v E .1"v(x) - Yi (this is the set of values which lie 
within the upper and lower boundaries of the parallelogram shown in fig. 3.15). 
There are three cases to consider. 
Case I: u E [A/2d, B - A/2d] 
First, notice that for u in this range, it is easy to verify that the upper boundary 
of the parallelogram is greater than A/2 while the lower boundary is less than 
A/2. Thus we can find a value of v (viz. v = A/2) which globally minimizes this 
quadratic because A/2 E .1"v(x) - Yi. The expression thus reduces to d2u2 + 

108 
INFORMATIONAL COMPLEXITY OF LEARNING 
d2(B-u)2 _A2 /2. Over the interval for u considered in this case, it is minimized 
at u = B /2 resulting in the value 
Case II: u E [0, A/2d] 
In this case, the upper boundary of the parallelogram (which is the maximum 
value v can take) is less than A/2 and hence the Q2(V) is minimized when 
v = duo The total expression then reduces to 
= d2(B - u)2 - (A - du)2 = (d2 B2 - A2) - 2ud(dB - A) 
Since, dB > A, the above is minimized on this interval by choosing u = A/2d 
resulting in the value 
dB(dB - A) 
Case III: By symmetry, this reduces to case II. 
A 
Figure 3.15. 
A figure to help the visualization of Lemma 4. For the x shown, the set F1) 
is the set of all values which lie within the parallelogram corresponding to this x, i.e., on 
the vertical line drawn at x but within the parallelogram. 
Since (d2 B2_A2)/2 ~ dB(dB-A) (this is easily seen by completing squares), 
it follows that u = B /2 is the global solution of the mini-max problem above. 
Further, we have shown that for this value of u, the sup term reduces to (d2 B2_ 
A2)/2 and the lemma is proved.O 

ACTIVE LEARNING 
109 
Using t.he above lemma along with eq. 3.6, we see that 
. 
«)) 1.,., 
., 
1 
mID 
sup ec; 11., V U x, Y ,F = -d(d~ Bi - An = -2ec; (11., V, F) 
xEC, YE:Fp(x) 
8 
In other words, by sampling the midpoint of the interval Ci , we are guaranteed 
to reduce the uncertainty by 1/2. As in the case of monotonic funct.ions now, 
we see that using eq. 3.5, we should sample the midpoint of the interval with 
largest uncertainty ec; (H, V, F) to obtain the global solution in accordance 
with the principle of Algorithm B of section 3.1. 
This allows us to formally state an active learning algorithm which is optimal 
in the sense implied in our formulation. 
The Choose and Learn Algorithm -
2 (CLA-2) 
1. [Initial Step] Ask for values of the function at points x = ° and x = 1. 
At this stage, the domain D = [0,1] is composed of one interval only, viz., 
C1 = [0,1]. Compute eC, = }d (d2 - If(l) - f(o)l2) and eD = ee,. If eD < f, 
stop and output. the linear interpolant of the samples as the hypothesis, 
otherwise query the midpoint of t.he interval to get a partition of the domain 
into two subintervals [0,1/2) and [1/2,1]. 
2. [General Update and Stopping Rule] In general, at the kth stage, let 
the partition of the interval [0,1] be [xo = 0, Xt},[Xl, X2), ... , 
[Xk-l, Xk = 1]. Compute ec; = 41d (d2(Xi - xi_d2 -
IYi - Yi_d 2 ) for each 
i = 1, ... , k. The midpoint of the interval with maximum ec, is queried 
for the next sample. The total error eD = L:~=l eCj is computed at each 
stage and the process is terminated when eD :::; f. Our hypothesis h at every 
stage is a linear interpolation of all the points sampled so far and our final 
hypothesis is obtained upon the termination of the whole process. 
It is possible to show that the following upperbound exists on the number 
of examples CLA would take to learn the class of functions in consideration 
Theorem 3.6 The CLA-2 would PAC learn the class in at most 1£ + 1 exam-
ples. 
Proof Sketch: Following a strategy similar to the proof of Theorem 3, we 
show how a slight variant of CLA-2 would converge in at most (d/4f + 1) 
examples. Imagine a grid of n points placed 1/(n - 1) apart on the domain 
D = [0,1] where the kth point is k/(n - 1) (for k going from ° to n - 1). 
The variant of the CLA-2 operates by confining its queries to points on this 
grid. Thus at the kth stage, instead of querying the midpoint of the interval 
with ma..ximum uncertainty, it will query the grid point closest to this midpoint. 
Suppose it uses up all the gridpoints in this fashion, then there will be n - 1 
intervals and by our arguments above, we have seen that the maximum error 
on each interval is bounded by 
1 
., 
1 
., 
., 
1., 
1 
., 
-(d~(-)~ -IYi -
Yi-ll~) :::; -d~(-)~ 
4d 
n - 1 
4d 
n - 1 

110 
INFORMATIONAL COMPLEXITY OF LEARNING 
Since there are n - 1 such intervals, the total error it could make is bounded 
by 
(n _1)~d2(_I_)2 = ~(_1_) 
4d 
n-l 
4d n-l 
It is easy to show that for n > d/4€ + 1, this maximum error is less than €. 
Thus the learner need not collect any more than d/4€ + 1 examples to learn 
the target function to within an € accuracy. Note that the learner will have 
identified the target to € accuracy with probability 1 (always) by following the 
strategy outlined in this variant of CLA-2. 0 
We now have both an upper and lower bound for PAC-learning the class (un-
der a uniform distribution) with queries. Notice that here as well, the sample 
complexity of active learning does not depend upon the confidence parameter 6. 
Thus for 6 arbitrarily small, the difference in sample complexities between pas-
sive and active learning becomes arbitrarily large with active learning requiring 
much fewer examples. 
3.3.3 Some Simulations 
We now provide some simulations conducted on arbitrary functions of the class 
offunctions with bounded derivative (the class :F). Fig. 3.16 shows 4 arbitrary 
selected functions which were chosen to be the target function for the approx-
imation scheme considered. In particular, we are interested in observing how 
the active strategy samples the target function for each case. Further, we are 
interested in comparing the active and passive techniques with respect to er-
ror rates for the same number of examples drawn. In this case, we have been 
unable to derive an analytical solution to the classical optimal recovery prob-
lem. Hence, we do not compare it as an alternative sampling strategy in our 
simulations. 
3.3.3.1 
Distribution of points selected. The active algorithm CLA-2 
selects points adaptively on the basis of previous examples received. Thus the 
distribution of the sample points in the domain D of the function depends 
inherently upon the arbitrary target function. Consider for example, the dis-
tribution of points when the target function is chosen to be Function-l of the 
set shown in fig. 3.16. 
Notice (as shown in fig. 3.17) that the algorithm chooses to sample densely 
in places where the target is flat, and less densely where the function has a 
steep slope. As our mathematical analysis of the earlier section showed, this 
is well founded. Roughly speaking, if the function has the same value at Xi 
and Xi+! , then it could have a variety of values (wiggle a lot) within. However, 
if, f(Xi+d is much greater (or less) than f(Xi), then, in view of the bound, d, 
on how fast it can change, it would have had to increase (or decrease) steadily 
over the interval. In the second case, the rate of change of the function over 
the interval is high, there is less uncertainty in the values of the function within 
the interval, and consequently fewer samples are needed in between. 

ACTIVE LEARNING 
111 
C') 
~ 
, 
";"~ 
~q 
,gil) 
6 .... 
() , 
g~ 
c 
i!~ 
:::JO 
LL q 
"" 
0 
, 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
X 
X 
C\I 
Il) 
ci 
N 
0 
C')"": 
~N 
,0 
~q 
6~ 
13 
cO 
c q 
:::J 
:::J .... 
LL"": 
LLIl) 
0 , 
ci 
C\I 
0 
ci 
ci 
, 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
X 
X 
Figure 3.16. 
Four functions with bounded derivative considered in the sim ulations. The 
uniform bound on the derivative was chosen to be d = 10. 

112 
INFORMATIONAL COMPLEXITY OF LEARNING 
Figure 3.17. 
How CLA-2 chooses to sam pie its points. Vertical lines have been drawn at 
the x values where the CLA queried the oracle for the corresponding function value. 

ACTIVE LEARNING 
113 
In example 1, for the case of monotone functions, we saw that the density 
of sample points was proportional to the first derivative of the target function. 
By contrast, in this example, the optimal strategy chooses to sample points in 
a way which is inversely proportional to the magnitude of the first derivative 
of the target function. Fig. 3.18 exemplifies this. 
o 
N 
It) 
c:i 
C! 
/ / 
O~ 
________ ~ 
______ -r ______ ~ 
________ ~ 
______ 
-r~ 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
x 
Figure 3.18. 
How CLA-2 chooses to sam pie its points. The solid line is a plot of II' (z) I 
where I is Function-l of our simulation set. The dotted line shows the density of sample 
points (queried by CLA-2) on the domain. 
3.3.3.2 
Error Rates:. In an attempt to relate the number of examples 
drawn and the error made by the learner, we performed the following simula-
tion. 
Simulation B: 
1. Pick an arbitrary function from class F. 
2. Decide N, the number of samples to be collected. There are two methods of 
collection of samples. The first (passive) is by randomly drawing N examples 
according to a uniform distribution on [0,1]. The second (active) is the CLA-
2. 
3. The two learning algorithms differ only in their method of obtaining samples. 
Once the samples are obtained, both algorithms attempt to approximate the 
target by the linear interpolant of the samples (first order splines). 

114 
INFORMATIONAL COMPLEXITY OF LEARNING 
4. This entire process is now repeated for various values of N for the same target 
function and then repeated again for the four different target functions of 
fig. 3.16 
The results are shown in fig. 3.19. Notice how the active learner outperforms 
the passive learner. For the same number of examples, the active scheme having 
chosen its examples optimally by our algorithm makes less error. 
.... , 
o 
o 
\ 
\,,~,---.-----.-~-
\ 
\ 
100 
200 
300 
400 
500 
N 
\",~--------------
1 00 
200 
300 
400 
500 
N 
"'t 
~<o 
c 
' 
LL. 
~QJ 
2 ' 
"'0 
w .... 
\ 
\ ..... , 
..... -......... ... 
0;' 
.9~ 
, 
~4---__ ---.-... ~.'.-... -.--.--... ~-.--.-... -... -.. ~ 
... -.--... -.... _ ... ~ 
.. ~ 
"70 
"'t 
~ 
!!:<o 
LL.' 
, 
-':::-00 
2 ' 
WO 
g;~ 
-I~ 
, 
~ 
"7 
0 
100 
200 
300 
400 
500 
N 
1 00 
200 
300 
400 
500 
N 
Figure 3.19. 
Results of Simulation B. Notice how the sampling strategy of the active 
learner causes better approximation (lower rates) for the same number of examples. 
We have obtained in theorem 6, an upper bound on the performance of the 
active learner. However, as we have already remarked earlier, the number of 
examples the active algorithm takes before stopping (i.e., outputting an €-good 
approximation) varies and depends upon the nature of the target function. 
"Simple" functions are learned quickly, "difficult" functions are learned slowly. 
As a point of interest, we have shown in fig. 3.20, how the actual number of 
examples drawn varies with €. In order to learn a target function to €-accuracy, 
CLA-2 needs at most nmax(€) = dj4€ + 1 examples. However, for a particular 
target function, f, let the number of examples it actually requires be n f (€). We 

ACTIVE LEARNING 
115 
plot ~ 
as a function of e. Notice, first, that this ratio is always much less 
~ 
than 1. In other words, the active learner stops before the worst case upper 
bound with a guaranteed e-good hypothesis. This is the significant advantage 
of an adaptive sampling scheme. Recall that for uniform sampling (or classical 
optimal recovery even) we would have no choice but to ask for d/4e examples 
to be sure of having an e-good hypothesis. Further, notice that that as e gets 
smaller, the ratio gets smaller. This suggests that for these functions, the 
sample complexity of the active learner is of a different order (smaller) than 
the worst case bound. Of course, there always exists some function in :F which 
would force the active learner to perform at its worst case sample complexity 
level. 
LO 
c:i 
0.0 
Fn-3 
Fn-2 
----
---
----
----
----
----
Fn-4 
-
Fn-1_-------
---
0.02 
0.04 
0.06 
0.08 
0.10 
e 
Figure 3.20. 
Variation with epsilons. 
3.4 CONCLUSIONS, EXTENSIONS, AND OPEN PROBLEMS 
This part of the chapter focused on the possibility of devising active strategies 
to collect data for the problem of approximating real-valued function classes. 
We were able to derive a sequential version of optimal recovery. This sequen-

116 
INFORMATIONAL COMPLEXITY OF LEARNING 
tial version, by virtue of using partial information about the target function is 
superior to classical optimal recovery. This provided us with a general formu-
lation of an adaptive sampling strategy, which we then demonstrated on two 
example cases. Theoretical and empirical bounds on the sample complexity of 
passive and active learning for these cases suggest the superiority of the active 
scheme as far as the number of examples needed is concerned. It is worthwhile 
to observe that the same general framework gave rise to completely different 
sampling schemes in the two examples we considered. In one, the learner sam-
pled densely in regions of high change. In the other, the learner did the precise 
reverse. This should lead us to further appreciate the fact that active sampling 
strategies are very task-dependent. 
Using the same general formulation, we were also able to devise active strate-
gies (again with superior sample complexity gain) for the following concept 
classes. 1) For the class of indicator functions {1[a,b] : 0 < a < b < I} on 
the interval [0,1], the sample complexity is reduced from l/dn(I/6) for passive 
learning to In(l/f) by adding membership queries. 2) For the class of half-spaces 
on a regular n-simplex, the sample complexity is reduced from n/dn(I/6) to 
n2 ln(s/f) by adding membership queries. Note that similar gains have been 
obtained for this class by Eisenberg (1992) using a different framework. 
There are several directions for further research. First, one could consider the 
possibility of adding noise to our formulation of the problem. Noisy versions 
of optimal recovery exist and this might not be conceptually a very difficult 
problem. Although the general formulation (at least in the noise-free case) 
is complete, it might not be possible to compute the uncertainty bounds ec 
for a variety of function classes. Without this, one could not actually use 
this paradigm to obtain a specific algorithm. A natural direction to pursue 
would be to investigate other classes (especially in more dimensions than 1) 
and other distance metrics to obtain further specific results. We observed that 
the active learning algorithm lay between classical optimal recovery and the 
optimal teacher. It would be interesting to compare the exact differences in a 
more principled way. In particular, an interesting open question is whether the 
sampling strategy of the active learner converges to that of the optimal teacher 
as more and more information becomes available. It would not be unreasonable 
to expect this, though precise results are lacking. In general, on the theme of 
better characterizing the conditions under which active learning would vastly 
outperform passive learning for function approximation, much work remains 
to be done. While active learning might require fewer examples to learn the 
target function, its computational burden is significantly larger. It is necessary 
to explore the information/computation trade-off with active learning schemes. 
Finally, we should note, that we have adopted in this part, a model of learning 
motivated by PAC but with a crucial difference. The distance metric, d, is 
not necessarily related to the distribution according to which data is drawn (in 
the passive case). This prevents us from using traditional uniform convergence 
(Vapnik, 1982) type arguments to prove learnability. The problem of learning 
under a different metric is an interesting one and merits further investigation 
in its own right. 

ACTIVE LEARNING 
117 
Part II: Epsilon Focusing: A Strategy for Active 
Learning 
In Part I, we discussed a principled strategy by means of which an active 
learner could choose its own examples, thereby potentially reducing the in-
formational complexity of learning real-valued functions. The formalization 
adopted ideas from optimal recovery, and active learning reduced to a sequen-
tial version of the optimal recovery problem. In this part of the chapter, we 
discuss another possible scheme for choosing examples. 
Recall that according to the PAC criterion for learning, we need to learn the 
target function to f accuracy (according to some distance metric d on the space 
of functions, :F), with confidence greater than 1 - 6. Sometimes, knowledge 
that the function lies within some f-ball (in function space) might directly 
translate (due to locality properties) into knowledge about the regions of the 
domain X over which the target function values are uncertain. The learner can 
then zoom (epsilon-focus) in on this region of uncertainty, and sample there. 
As a motivating real, world example, one could imagine that in a pattern 
classification task, the knowledge that the learner is within f of the optimal 
discriminant boundary, might inform the learner about which regions of the 
feature space are worth sampling to a greater degree. Intuitively, one might 
think that regions close to the decision boundary are such worthwhile regions. 
We formally illustrate this idea with a simple example in the next section. 
In all the cases we consider, the concept class (class of indicator functions) have 
bounded VC dimension. Consequently, they are learnable, and upper and lower 
bounds on the sample complexity of passive learning exist for these function 
classes. Roughly speaking, instead of learning to (f, 6) accuracy at one shot by 
collecting the requisite number of examples, the learner attempts to obtain a 
loose estimate of the target. Making use of locality properties, then, the learner 
obtains a loose estimate of the regions of the domain to sample more closely. 
On the basis of these fresh samples, the learner tightens its estimate of the 
target, thereby reducing the region of uncertainty. It then freshly samples this 
new, reduced, region of uncertainty and carries on in this fashion. The learner 
can arbitrarily reduce the sample complexity of learning by this scheme. 
After our motivating example, we provide some generalizations, and finally 
end with some open questions. 
3.5 A SIMPLE EXAMPLE 
Suppose we want to PAC-learn (with (f,6) accuracy) the following class of 
indicator functions from [0,1] to {O, 1}. 
:F = {l[a,lj : 0 $ a1} 
Further suppose the distribution P on [0,1] according to which data is drawn 
is known and is uniform. It is known that a passive learner would take atleast 
O«l/f) In(1/6)) examples to do so. We suggest the following k-step strategy 

118 
INFORMATIONAL COMPLEXITY OF LEARNING 
which seeks examples from successively smaller well-focused regions of the do-
main to learn this class in O«klf2A:) In(kle) examples. 
The f.-focusing Algorithm (1) 
The learning occurs over k (k can be arbitrarily chosen) stages. 
1. Draw enough examples to learn the target with f1/k accuracy with elk con-
fidence. Obtain hypothesis 1[a1' 1]. 
2. Now ask for examples drawn uniformly at random from the region [a1 -
f1/A:, a1 + fllk] and try to learn the target function with fllk 12 accuracy 
with e / k confidence (with respect to this new distribution over the smaller 
region). Obtain hypothesis 1[a'2,1]' 
3. Repeat like step 2, i.e., ask for enough examples drawn uniformly at random 
from the region [a2 - f2/k, a'2 + f2/k] in order to learn the target function to 
fllk 12 accuracy with elk confidence. Obtain hypothesis 1[a'3,1]' In general at 
the jth step, ask for examples drawn uniformly at random from the region 
[a{-l - fU -l)lk, a{.-l +fCi -l)lk] to learn the target to within fllk 12 accuracy 
with elk confidence. Obtain hypothesis 1[a'i,1]' 
4. Stop with hypothesis 1[a'k,1]' 
Proof of Correctness: Let the target be 1[a,,1]' At the end of the first step, the 
target is within fllk of the hypothesis with probability greater than 1-elk. This 
means that with high probability lat - all ~ f11 k or in other words al _ fll k ~ 
at < a1 + f1/k. 
We now draw examples only from the region [al - f1/k, a1 + f1 /k]. Let this 
distribution be P2 . By a theorem of Vapnik and Chervonenkis, we need to draw 
4/f2/k In(kle) examples to learn the target to within f1/k 12 with elk confidence 
(for an arbitrary distribution) at this stage. This means that 
In other words, 
lat - a21 ~ f2/k 
Thus after two steps, the above inequality is true. We now draw examples 
only from the region [a2 - f2/k, a2 + f2/k]. 
In general, at the jth step, if we draw 4/f2/k In(kle) examples, we would have 
learnt the target to f1/k 12 accuracy with elk confidence. The distribution (Pj) 
according to which examples are drawn at this stage is uniform over [a{-l -
f(i-1)lk, aj"-l + f(i-1)lk]. Thus, 
So we have, 

ACTIVE LEARNING 
119 
This happens with probability greater than I-olk. Thus with high probability, 
from the (j - I)th stage to the jth stage, we have ''focused'' more closely onto 
at. Ifthis is true at every stage, we would eventually have after k steps ensured 
that 
la'k - atl :$ ( 
which would mean that we have learnt the target to within an ( width. 
If we fail at any stage, the eventual hypothesis ak is not necessarily within 
an ( width of the target. The probability of failing at each stage is less than 
than 01 k so the probability of failing in at least one stage is less than k.o I k = 6. 
Thus the probability of failing is less than 6 or in other words with greater than 
1 - 6 probability, we would have learnt the target to within an ( width which 
was our goal. 
The total number of examples drawn at each stage is 41(2/k In(kI6) and since 
there are k stages in all, the total number of examples in the whole process is 
4kl( (2/k) In(kI6).0 
3.6 
GENERALIZATIONS 
This general strategy can be extended to several other scenarios. We introduce 
the notion of localized function classes. These classes which have a local focusing 
property can be learned faster by the method of (-focusing. We mention some 
concrete results obtained by using this scheme for n-dimensional cases, and for 
the case of noisy examples. No proofs or formal arguments are provided for 
these extensions. We hope, though, that the reader will appreciate the spirit 
of this idea. 
3.6.1 Localized Function Classes 
The previous sections showed how to use the (-focusing strategy to obtain 
superior sample complexity results for some simple concept classes. It is of 
interest to characterize general conditions on function classes for which the 
(-focusing strategy would yield such a superior performance. It is noteworthy 
that the previous function class had the property that knowledge of the distance 
between any two functions f and g in:F (in the dp metric) allowed us to focus 
in on a region of interest in the domain X = [0,1] where f and 9 differ. We 
formalize this notion to derive a general bound on sample complexity for the 
(-focusing strategy. 
Let :F be a concept class (i.e. class of indicator functions) on some compact 
domain X. Let P be the uniform distribution on this domain, i.e., the distribu-
tion which corresponds to the normalized Lebesgue measure on it. We define 
the usual Ll (Jl) distance metric on the space functions by 
d/J(f, g) = [If - gldJl 
(where Jl is a probability measure on the set x.) 
We define the local focusing property of such an arbitrarily defined concept 
class as follows: 

120 
INFORMATIONAL COMPLEXITY OF LEARNING 
Definition 2 For a given I belonging to some concept class F on X, and lor 
any given f > 0, its f-region of interest, R€(f) is given by 
{x E XI/(x) =f. g(x) for some g E F such that dp(f,g) ~ f} 
Definition 3 The concept class F is said to be locally focused with focusing 
bound 9 (g is a real valued function taking values on [0,1]) if for every f > 0, 
sup Volume(R€(f)) ~ g(f} 
le:F 
Here, V olume( s) for any set s ~ X, is simply the volume6 of that set. We 
assume that Volume(X} = 1. 
Clearly, locally focused classes are those with bounded f regions of interest 
into which we can focus in the iterative manner of Algorithm 1. 
3.6.2 The General f.-focusing strategy; 
The general algorithm to learn such f-focused classes is as follows: 
Algorithm 2 
1. Begin with the entire class F, draw examples according to the uniform dis-
tribution P on X, (call this Pt) and attempt to learn the target (ft E F) to 
fl/k with probability at least 1-6/k. Obtain hypothesis Jr. Also obtain the 
reduced set of candidate target functions (version space), 
Finally, also obtain the f-region of interest: 
Rl = R€l/k(/t}. 
2. Draw examples according to a uniform distribution on Rl (call this distri-
bution P2) and learn the target to f2/k /g(fl/k) (according to P2) with prob-
ability greater than 1 - 8/ k. Now obtain hypothesis /2 E F 1 , the reduced 
version space: 
A 
f2/k 
:F2 = {f E Ft!dp2(f, h) ~ 9(fl / k }}' 
and R2 = R€2/k (/2)' 
"/k 
3. Repeat step 2. In general, at the jth step, learn the target to g(€t I)/k) 
(according to distribution Pj), and obtain /j, Fj, and Rj in the obvious way. 
6From a more formal perspective, one should really replace V olume( s) by the measure on 
the set s, i.e., P(s). Clearly, P(X) = 1. In our case, we assume that Volume(X) = 1. Since 
P is a uniform distribution, i.e., any point in this set is as likely as any other point, it follows 
that P(s) is simply Volume(s). We will continue to use this notation, but the reader will 
easily see that P can be used in general, and in fact, need not even be uniform. 

4. Stop at the kth step and output hypothesis A. 
Proof of Learnability: 
ACTIVE LEARNING 
121 
Recall that our eventual goal is to learn the unknown target It within f 
accuracy (according to the distance metric dp ) with probability greater than 
1- 8. 
Consider the first step. The target has been learned to f1/k accuracy with 
high confidence. The learner's hypothesis is A. Clearly, with high probability 
(greater that 1 - 8), the target lies within in an f1/ k ball around A (this is 
denoted by .1"1). According to our definition, all functions in .1"1 agree on the 
region outside of R1. So we only need to sample the region R1 which is what 
we do in the second step. 
In the second step, we learn the target to f2/k jg(f1/k). This is according to 
a distribution P2 (uniform on the region Rd. Again, the target, is within an 
f2/k jg(f1/k) ball of the hypothesis at this stage (12). Thus, 
d (f' 1)- Volume({xER1Ii2(X)=/:lt(x)}) 
2/kj (l/k) 
Po 
2, t 
-
Volume(Rd 
::; f 
9 f 
But, Volume(R!) = g(f1/k). Therefore, 
Clearly, then, 
Thus, after the second step, we see that the target It is within f2/k accuracy 
(with respect to our original distribution P). By our definition of the local 
focusing property, we know that It E .1"2, and the points on which It and i2 
disagree must lie within R2 . 
In general, before the jth step, the points on which the target and the 
(j - 1 )th hypothesis disagree must lie within Rj -1. Since, we sample according 
to a uniform distribution on this (Pj), and attempt to learn the target to an 
accuracy of fj /k jg(fCi - 1)/k), by a similar argument, 
d (f' f) -
Volume({x E Rj-1Iij (x) =/: It(x)}) 
jfkj ( U-1)/k) 
Pj 
2, t 
-
V I 
(R) 
::; f 
9 f 
o ume 
j-1 
But, Volume(Rj_d = g(fU-1)/k). Therefore, 
and, 

122 
INFORMATIONAL COMPLEXITY OF LEARNING 
Thus, after the jth step, the learner has learned the target to eitk accuracy. 
Further, according to our definition of the local focusing property, the points on 
which the learner and target disagree must lie within the set Rj = 'R,fi/k <lj). 
Clearly, after the kth step, the learner will have learned the target to e 
accuracy. The only way, in which the learner could have made a mistake, is 
if it made a mistake on anyone of the steps. The probability of making a 
mistake in each step is C / k. The probability of making a mistake in anyone is 
bounded by c. Thus, the learner would have identified the target to e accuracy 
with confidence greater than 1 - c. 
Sample Complexity: By the standard Vapnik Chervonenkis theorem, we see 
2( (j-i)/.) 
that at the jth stage, the learner will have to draw at most O( 9 
ff 2i/. 
In( k / c» 
examples to satisfy the learn ability requirement of that stage. The total number 
of examples the learner needs would be 
3.6.3 Generalizations and Open Problems 
Now we are in a position to re-evaluate our simple example from this general 
perspective. It is easy to see that 
1. Opening Example: For an arbitrary fa = 1[a,1], we see that 
'R,f(fa) = [a - e, a + e] 
Clearly, g(e) = 2e. The sample complexity is O«k/e2/ k )ln(k/c». 
2. Box Functions: Consider the following class of indicator functions on [0, 1]. 
:F = {l[a, b] : ° 
~ a ~ b ~ 1} 
For an arbitrary fa,b = l[a,b], we see that 
'R,f(fa,b) = [a - e, a + e] U [b - e, b + e] 
Clearly, g(e) = 4L The sample complexity O«k/e2/ k ) In(k/c)) follows. 
Some other generalizations should be noted. We do not attempt to provide 
any formal arguments. 
1. Extensions to n-dimensions: It is possible to extend the e focusing strat-
egy of our opening example to an n-dimensional situation. A concrete example 
includes the PAC learning of a concept class of hyperplanes dividing an n-
simplex into two regions. Essentially, the hyperplane cuts the simplex at its 
edges. Consequently, along each edge, the points on one side of the cut are la-
belled 0, while the points on the other side are labelled 1. Thus, if one confines 
oneself to finding the intersection of the hyperplane with the simplex edge, the 

ACTIVE LEARNING 
123 
problem reduces to a single dimensional case exactly like our opening example. 
If n such edge-intersection problems are solved, then the total n-dimensional 
problem can be solved. 
In view of the fact that we have an effective f-focusing strategy for box func-
tions, we can even address concept classes represented by multilayer perceptrons 
with two hidden layers. In such a case, there are at most two hyperplanes inter-
secting each edge. The single-dimensional problem associated with each edge 
is like a box function. 
2. Handling misclassification noise: The f-focusing strategy in this part has 
been developed for a noise-free case. Extensions to cover a situation with a 
bound on the misclassification noise (the label of the example can be flipped 
with probability at most ,,) can easily be considered as well. 
Finally, some natural questions arise at this stage. First, what kinds of 
concept classes have the locally focusing property? Second, given the existence 
of the locally focusing property, how easy is it to compute the f-region of interest 
'fl. for such concept classes. Further research on these questions is awaited. 

4 
LANGUAGE LEARNING PROBLEMS 
IN THE PRINCIPLES AND PARAMETERS 
FRAMEWORK 
This chapter considers a learning problem in which the hypothesis class is a class of param-
eterized grammars. After a brief introduction to the "principles and parameters" framework 
of modern linguistic theory, we consider a specific learning problem previously analyzed in 
a seminal work by Gibson and Wexler (1994). With our informational-complexity point of 
view developed in this book, we reanalyze their learning problem. This puts particular em-
phasis on the sample complexity of learning, in contrast to previous research in the inductive 
inference, or Gold frameworks (see Osherson and Weinstein, 1986). We show how to formally 
characterize this problem in particular, and a class of learning problems in finite parameter 
spaces in general, as a Markov structure. Important new language learning results follow 
directly: we explicitly compute sample complexity bounds under different distributional as-
sumptions, learning regimes, and grammatical parameterizations. Briefly, we may view this 
as a precise way to model the "poverty of stimulus" children face in language acquisition. 
Our reanalysis alters several conclusions made by Gibson and Wexler. We therefore consider 
this chapter as a useful application of learning-theoretic notions to natural languages, and 
their acquisition. Finally, we describe several directions for further research. 
In Chapters 2 and 3, we considered the problem of learning target functions 
(belonging to certain classes) from examples. Particular emphasis was given 
to the sample complexity of learning such functions, and we have seen how it 
depends upon the complexity of the hypothesis classes concerned. The classes 
of functions we have investigated, have arguably, very little cognitive relevance. 
However, the investigations have helped us to develop a point of view crucial to 
125 

126 
INFORMATIONAL COMPLEXITY OF LEARNING 
the analysis of learning systems-a point of view which allows us to appreciate 
the inherent tension between the approximation error, and the estimation er-
ror, in learning from examples. In particular we have seen how the hypothesis 
classes used by the learner must be large to reduce the approximation error, 
and small to reduce the estimation error. In the rest of the book (Chapters 4 
and 5), we remedy our cognitive irrelevance by considering some classes of 
functions which linguists and cognitive scientists believe the brain must com-
pute. As we shall soon see, there is a learning-theoretic argument at the heart 
of the modern approach to linguistics-hence our choice of linguistic structures 
for analysis. The origin of the research presented in this chapter lies in the 
paper "Triggers" (Gibson and Wexler, 1994; henceforth GW) which marks a 
seminal attempt to formally investigate language learning within the "princi-
ples and parameters" framework (Chomsky, 1981). The results presented in 
this chapter emerged out of a reanalysis of "Triggers" using more sophisticated 
mathematical techniques, than had previously been used in this context. One 
can, thus, regard this as a demonstration, of how our information-theoretic 
point of view, and the arguments and tools of current learning theory, can help 
us to sharpen certain important questions, and lead to insightful analysis of 
relevant linguistic theories. 
In the next section, we provide a brief account of the learning-theoretic con-
siderations inherent in the modern approach to linguistics. We then give a brief 
account of the principles and parameters framework, and the issues involved in 
learning within this framework. This sets the stage for our investigations, and 
we use as a starting point the Triggering Learning Algorithm (TLA) working 
on a three-parameter syntactic subsystem first analyzed by Gibson and Wexler. 
The rest of the chapter analyzes the TLA from the perspective of learn ability 
and sample complexity. Issues pertaining to parameter learning in general, and 
the TLA in particular, are discussed at appropriate points. Finally, we suggest 
various directions for further research-this chapter marks only the opening of 
our research on this theme. Very little work has been done on the formal, com-
putational, aspects of parameter setting, and we attempt here to pose questions 
which we think are of importance in the field. 
4.1 
LANGUAGE LEARNING AND THE POVERTY OF STIMULUS 
The inherent tension between having large hypothesis classes, for greater ex-
pressive power, and small ones, for better learnability, is beautifully instanti-
ated in the human language system. Humans develop a mature knowledge of 
language that is both rich and subtle, on exposure to fairly limited number 
(the so called "poverty of stimulus") of example sentences spoken by parents 
and guardians in childhood. Languages are infinite sets of sentences1 . Yet on 
IThere are an infinite number of sentences in the English language. You haven't heard all of 
them, yet you can judge the grammaticality of sentences you have not heard before. In the 
view of many linguists, you have internalized a grammar-a set of rules, a theory, or schema, 
by means of which you are able to generalize to unseen sentences (examples). 

LANGUAGE LEARNING 
127 
exposure to a finite number of them (during the language acquisition phase 
in childhood) children correctly generalize to the infinite set. Further, they 
generalize in exactly the same way: too striking a coincidence to be attributed 
to chance. This motivated Chomsky (1965) to argue that children must op-
erate with constrained hypotheses about language--constraints which restrict 
the sorts of generalizations that they can make. These constrained hypothe-
sis classes which children operate with, in the language context, are classes of 
grammars. Children choose one particular grammar2 from this class, on the 
basis of the examples they have seen. Thus, a child born in a Spanish speak-
ing environment would choose the grammar which appropriately describes the 
data it has seen (Spanish sentences), and, similarly, a child born in a Chinese 
speaking environment chooses a different grammar, and so on. Of course, chil-
dren might make mistakes, and they do. These mistakes are often resolved 
as more data becomes available to the child. Sometimes (when this happens, 
is undoubtedly, of great interest), these mistakes might never be resolved-a 
possibility which we explore in the next chapter. 
Thus, we see, that if we were totally unconstrained in the kinds of hypotheses 
we could make, then, on the basis of a finite data set, we would all generalize in 
wildly different ways, implying, thereby, that we would never be able to learn 
languages. Yet, we learn languages, apparently with effortless ease as chil-
dren. This realization is crucial to linguistics. Humans, thus, are predisposed 
to choose certain generalizations over others, they are predisposed to choose 
hypotheses belonging to a constrained class of grammars-this predisposition 
is the essence of the innatist view of language; the universal constraints on 
the class of grammars belong to universal grammar. Furthermore, such a class 
of grammars must be large enough to capture the richness of language, yet 
small enough to be learned- exemplifying the tension discussed previously. 
The thrust thus shifted to finding the right constraints incorporated in such a 
class of grammars, in other words, finding the class of grammars of the right 
complexity. Notice, here, the similarity in spirit to the problem of finding a 
regularization network of the right complexity. Consequently, we see that an 
analysis of the complexity of language learning coupled with a computational 
view of the language acquisition device is crucial to the theoretical underpin-
2It should be pointed out that there are various components of a language. There is its 
syntax, that concerns itself with syntactic units like verbs, noun phrases, etc. and their ap-
propriate combinations. Further, there is its phonology that deals with its sound structure, 
its morphology that deals with word structure, and finally, the vocabulary or "words" which 
are the building blocks out of which sentences are ultimately composed. Acquisition of a lan-
guage involves the acquisition of all of this. We have been using the term grammar in a loose 
sort of way-it is a system of rules and principles which govern the production of acceptable 
sentences of the language. The grammar too could be broken into its syntactic parts, its 
phonological parts and so on. Some readers, recalling vivid memories of stuffy English school 
teachers, might have a natural resistance to the idea of rigid rules of grammaticality. For 
such people, we note, that while there is undoubtedly greater flexibility in word order than 
such teachers would suggest, it is a fact, that no one speaks "word salad"-with absolutely 
no attention to word order combinations at all. 

128 
INFORMATIONAL COMPLEXITY OF LEARNING 
nings of modern linguistics (see Wexler and Culicover (1980) for an excellent 
formal exposition of this idea). 
4.2 
CONSTRAINED GRAMMARS-PRINCIPLES AND PARAMETERS 
Having recognized the need for constraints on the class of grammars (this can 
be regarded as an attempt to build a hypothesis class with finite learn ability 
dimension3) researchers have investigated several possible ways of incorporating 
such constraints in the classes of grammars to describe the natural languages 
of the world. Examples of this range from linguistically motivated grammars 
such as Head-driven Phrase Structure Grammars (HPSG), Lexical-Functional 
grammars, Optimality theory for phonological systems, to bigrams, trigrams 
and connectionist schemes suggested from an engineering consideration of the 
design of spoken language systems. Note that every such grammar suggests 
a very specific model for human language, with its own constraints and its 
own complexity. Model-free, unconstrained, tabula rasa learning schemes cor-
respond to hypothesis classes with infinite dimension, and these can never be 
learned in finite time. An important program of research consists of computing 
the sample complexity of learning each of these diverse classes of grammars. 
In this chapter, we conduct our investigations within the purview of the prin-
ciples and parameters framework (Chomsky, 1981). Such a framework attempts 
to capture the "universal" principles common to all the natural languages of 
the world, (part of our biological endowment as human beings possessed of the 
unique language faculty) and the parameters of variation across languages of 
the world. Roughly speaking, there are a finite number of principles governing 
the production of human languages. These abstract principles, can take one 
of several (finite) specific forms-this specific form manifests itself as a rule, 
peculiar to a particular language (or classes of languages). The specific forms 
that such an abstract principle can take is governed by setting an associated 
parameter to one of several values. In typical versions of theories constructed 
within such a framework, one ends up with a parameterized class of grammars. 
The parameters are boolean valued-setting them to one set of values, defines 
the grammar of German (say), setting them to another set of values, defines 
the grammar, perhaps, of Chinese. Specific examples of theories within such a 
framework could include Government and Binding, Head-driven Phrase Struc-
ture Grammar, Optimality Theory, varieties of lexical-functional grammars and 
so forth. The idea is best illustrated in the form of examples. We provide, now, 
two examples, drawn from syntax, and phonology, respectively. 
3In previous chapters, we have utilized the notion of VC-dimension, and pseudo-dimension 
to characterize the complexity of learning real-valued function classes. It is not immediately 
clear, what complexity measure should be used for characterizing classes of grammars-the 
development of a suitable measure, in tune with the demands of the language acquisition 
process, is an open question. 

LANGUAGE LEARNING 
129 
4.2.1 Example: A 3-parameter System from Syntax 
Two X-bar parameters: A classic example of a parametric grammar for syn-
tax comes from X-bar theory (Chomsky, 1981; Haegeman, 1991). This describes 
a parameterized phrase structure grammar, which defines the production rules 
for phrases, and ultimately sentences in the language. The general format 
for phrase structure is summarized by the following parameterized production 
rules: 
x P -+ SpecX' (P1 = 0) or X'Spec(P1 = 1) 
X' -+ CompX'(P2 = 0) or X'Comp(P2 = 1) 
X'-+X 
XP refers to an X-phrase, where X, or the "head", is a lexical category 
like N (Noun), V (Verb), A (Adjective), P (Preposition), and so on. Thus, 
one could generate N P, or Noun Phrases, V P, or Verb Phrases, and other 
phrases in this fashion. Spec refers to specifier, in other words, that part of the 
phrase that "specifies" it, roughly like the old in the old book. Comp refers to 
the complement, roughly a phrase's arguments, like an ice-cream in the Verb 
Phrase ate an ice-cream, or with envy in the Adjective Phrase green with envy. 
Both Spec and Comp can themselves be phrases with their own specifiers and 
complements. Furthermore, in a particular phrase, the spec-position, or the 
comp-position might be blank (in these cases, Spec -+ 0, or Comp -+ 0 re-
spectively). Applying these rules recursively, one can thus generate embedded 
phrases of arbitrary length in the language. Further, these rules are parame-
terized. Languages can be spec-first (P1 = 0) or spec-final (P1 = 1). Similarly, 
they can be comp-first, or comp-final. For example, the parameter settings of 
English are (spec-first,comp-final). Shown in fig. 4.1 is an embedded phrase 
which demonstrates the use of the X-bar production rules (with the English 
parameter settings) to generate an arbitrary English phrase. 
In contrast, the parameter settings of Bengali are (spec-first,comp-first). The 
translation of the same sentence is provided in fig. 4.2. Notice, how a difference 
in the comp-parameter setting causes a difference in word orders. It is claimed 
that as far as basic, underlying word order is concerned, X-bar theory covers all 
the possibilities for natural languages4. Languages of the world simply differ 
in their parameter settings. 
One transformational parameter (V2): The two parameters described 
above define generative rules to obtain basic word-order combinations permit-
ted in the world's languages. As mentioned before, there are many other aspects 
which govern the formation of sentences. For example, there are transforma-
tional rules which determine the production of surface word order from the un-
derlying (base) word-order structure obtained from the production rules above. 
4There are a variety of other formalisms developed to take care of finer details of sentence 
structure. This has to do with case theory, movement, government, binding and so on. See 
Haegeman (1991). 

130 
INFORMATIONAL COMPLEXITY OF LEARNING 
VP 
11 
I 
V' 
(emply) 
XI'->SpeeX' 
X' --> x' Comp 
PP (Comp) 
A 
V' 
Spec 
P' 
r-/p\P(comp) 
(Jpty) A 
l / 
P' 
P' 
NP (Comp) 
v~ 
P' 
I A 
ren 
(ompty) A 
P 
Spec 
N' 
r i\ 
l 
lrom 
Spec 
I 
(emply) 
N' 
I 
N 
I 
there 
with 
his 
money 
Figure 4.1. 
Analysis of an English sentence. The parameter settings for English are spec-
first, and com p-final. 

PP(COIl1) -
A 
Spec 
P' A 
I 
(emptll 
NP(Comp) 
P' 
/\N' 
I 
P 
I 
N 
or 
paisa 
nlye 
(his) 
(money) 
(wIth) 
LANGUAGE LEARNING 
131 
VP 
~-I 
(empty) 
v' 
l 
v' 
/\t 
Spec 
P' 
I 
A 
(omptll 
/ 
\ 
NP (COIl1) 
P' 
A 
Spec 
N' 
(em~tIl 
I 
N 
I 
I 
P 
V 
XP->SpoeX' 
X'->CompX' 
shekh.n theke 
douralo 
(lhere) 
(from) 
(ran) 
Figure 4.2. 
Analysis of the Bengali translation of the English sentence of the earlier figure. 
The parameter settings for Bengali are spec-first, and com p-first. 

132 
INFORMATIONAL COMPLEXITY OF LEARNING 
One such parameterized transformational rule that governs the movement of 
words within a sentence is associated with the V2 parameter. It is observed 
that in German and Dutch declarative sentences, the relative order of verbs and 
their complements seem to vary depending upon whether the clause in which 
they appear is a root clause or subordinate clause. Consider, the following 
German sentences: 
(1) ... dass (that) Karl das (the) Buch (book) kauft (buys) . 
... that Karl buys the book. 
(2) ... Karl kauft das Buch . 
... Karl buys the book. 
This seems to present a complication in that from these sentences it is not 
clear whether German is comp-first (as example 1 seems to suggest) or comp-
final (as example 2 seems to suggest). It is believed (Haegeman, 1991) that 
the underlying word-order form is comp-first (like Bengali, and unlike English, 
in this respect); however, the V2 parameter is set for German (P3 = 1). This 
implies that finite verbs must appear in the exact second position in root declar-
ative clauses (P3 = 0 would mean that this need not be the case). This is a 
specific application of a transformational rule Move-a. For details and analysis, 
see (Haegeman, 1991). 
Each of these three parameters can take one of two values. There are, thus, 
8 possible grammars, and correspondingly 8 languages by extension, generated 
in this fashion. At this stage, the languages are defined over a vocabulary of 
syntactic categories, like N, V etc. Applying the three parameterized rules, one 
would obtain different ways of combining these syntactic categories to obtain 
sentences. Appendix A is a list of the set of unembedded (degree-O) sentences 
obtained for each of the languages, Ll through Ls in this parametric system. 
The vocabulary has been modified so that sentences are now defined over more 
abstract units than syntactic categories. 
4.2.2 Example: Parameterized Metrical Stress in Phonology 
The previous example dealt with a parameterized family for syntax. As we 
mentioned before, syntax is only one component of language. Here we consider 
an example from phonology; in particular, our example deals with metrical 
stress which describes the possible ways in which words in a language can be 
stressed. 
Consider the English word, "candidate". This is a three syllable word, com-
posed ofthe three syllables, I can/,j dil ,and, I date/. A native speaker of Ameri-
can English typically pronounces this word by stressing the first syllable of this 
word. Similarly, such a native speaker would also stress the first syllable of 
the tri-syllabic word, "/al/-/pha/-/bet/" so that it almost rhymes with "can-
didate". In contrast, a French speaker would stress the final syllable of both 
these words-a contrast which is perceived as a "French" accent by the English 
ear. 

LANGUAGE LEARNING 
133 
For simplicity, assume that stress has two levels, i.e., each syllable in each 
word can be either stressed, or unstressed5 . Thus, an n-syllable long word could 
have, in principle, as many as 2n different possible ways of being stressed. For 
a particular language, however, only one of these ways is phonologically well-
formed. Other stress patterns sound accented, or awkward. Words could poten-
tially be of arbitrary length6 . Thus one could write phonological grammars-a 
functional mapping from these words to their correct stress pattern. Clearly, 
this is another example of a functional mapping the brain must compute. Fur-
ther, different languages correspond to different such functions,i.e., they corre-
spond to different phonological grammars. Within the principles and parame-
ters framework, these grammars are parameterized as well. 
Let us consider a simplified version of two principles associated with 3 
boolean valued parameters which playa role in the Halle and Idsardi metrical 
stress system. These principles describe how a multisyllable word can be broken 
into its constituents (recall how sentences were composed of constituent phrases 
in syntax) before stress assignment takes place. This is done by a bracketing 
schema which places brackets at different points in the word, thereby marking 
(bracketing) off different sections as constituents. A constituent is then defined 
as a syllable sequence between consecutive brackets. In particular, a constituent 
must be bounded by a right bracket on its right edge, or, a left bracket on its 
left edge (both these conditions need not be satisfied simultaneously). Further, 
it cannot have any brackets in the middle. Finally, note that not all syllables 
of the word need be part of a constituent. A sequence of syllables might not 
be bracketed by either an appropriate left, or right bracket-such a sequence, 
cannot have a stress-bearing head, and might be regarded as an extra-metrical 
sequence. 
1) the edge parameters: there are two such parameters. 
a) put a left (Pi = 0) or right (Pl = 1) bracket 
b) put the above mentioned bracket exactly one syllable afterthe left (P2 = 0) 
edge or before the right (P2 = 1) edge of the word. 
2) the head parameter: each constituent (made up of one or more syllables) 
has a "head". This is the stress bearing syllable of the constituent, and is in 
some sense, the primary, or most important syllable of that constituent (recall 
how syntactic constituents, the phrases, had a lexical head). This phonological 
5While we have not provided a formal definition of either stress, or syllable, it is hoped, that 
at some level, the concepts are intuitive to the reader. It should, however, be pointed out that 
linguists differ on their characterization of both these objects. For example, how many levels 
can stress have? Typically, (Halle and Idsardi, 1991) three levels are assumed. Similarly, 
syllables are classified into heavy and light syllables. We have discounted such niceties for 
ease of presentation. 
6 One shouldn't be misled by the fact that that a particular language has only a finite number 
of words. When presented with a foreign word, or a "non-sense" word one hasn't heard 
before, one can still attempt to pronounce it. Thus, the system of stress assignment rules 
in our native language probably dictates the manner in which we choose to pronounce it. 
Speakers of different languages would accent these non-sense words differently. 

134 
INFORMATIONAL COMPLEXITY OF LEARNING 
head could be the leftmost (P3 = 0), or, the rightmost (P3 = 1) syllable in the 
constituent. 
Suppose, the parameters are set to the following set of values: [Pi = 0, P2 = 
0, P3 = 0]. Fig. 4.3 shows how some multisyllable words would have stress 
assigned to them. In this case, any n-syllable word would have stress in exactly 
the second position (if such a position exists) and no other. In contrast, if 
[Pl = 0, P2 = 0, P3 = 1], the corresponding language would stress the final 
syllable of all multi-syllable words. Monosyllabic words are unstressed in both 
languages. 
+ 
x(Xxxxx 
x(Xxxxx 
; 
; 
x(X x x x 
XIX xx x 
X( 
X( 
Figure 4.3. 
Depiction of stress pattern assignment to words of different syllable length 
under the parameterized bracketing scheme described in the text. 
These 3 parameters represent a very small (almost trivial) component of 
stress pattern assignment. There are many more parameters which describe in 
more complete fashion, metrical stress assignment. At this level of analysis, 
for example, the language Koya has P3 = 0, while Turkish has P3 = 1; see 
Kenstowicz (1994) for more details. The point of this example was to provide 
a flavor or how the problem of stress-assignment can be described formally by 
a parametric family of functions. The analysis of parametric spaces developed 
in this chapter can be equally well applied to such stress systems. 
4.3 
LEARNING IN THE PRINCIPLES AND PARAMETERS 
FRAMEWORK 
Language acquisition in the principles and parameters framework reduces to 
the setting of the parameters corresponding to the "target" language. A child 
is born in an arbitrary linguistic environment. It receives examples in the form 
of sentences it hears in its linguistic environment. On the basis of example sen-
tences it hears, it presumably learns to set the parameters appropriately. Thus, 
referring to our 3-parameter system for syntax, if the child is born in a German 
speaking environment, and hears German sentences, it should learn to set the 
V2 parameter, and the spec-parameter to spec-first. Similarly, a child hearing 
English sentences, should learn to set the comp-parameter to comp-final. In 
principle, the child is thus solving a parameter estimation problem-an un-

LANGUAGE LEARNING 
135 
usual class of parameter estimation problems, no doubt, but in spirit, little 
different from the parameter estimation problem associated with the regular-
ization networks of Chapter 2. One can thus ask a number of questions about 
such problems. What sort of data does the child need in order to set the target 
parameters? Is such data readily available to the child? How often is such 
data made available to the child? What sort of algorithms does the child use 
in order to set the parameters? How efficient are these algorithms? How much 
data does the child need? Will the child always converge to the target "in the 
limit" ?? 
Language acquisition, in the context of parameterized linguistic theories, 
thus, gives rise to a class of learning problems associated with finite parameter 
spaces. Furthermore, as emphasized particularly by Wexler in a series of works 
(Hamburger and Wexler, 1975; Culicover and Wexler, 1980; and Gibson and 
Wexler, 1994), the finite character of these hypothesis spaces does not solve 
the language acquisition problem. As Chomsky noted in Aspects of the Theory 
of Syntax (1965), the key point is how the space of possible grammars- even 
if finite-is "scattered" with respect to the primary language input data. It is 
logically possible for just two grammars (or languages) to be so near each other 
that they are not separable by psychologically realistic input data. This was the 
thrust of Wexler and Hamburger, and Wexler and Culicover's earlier work on 
the learn ability of transformational grammars from simple data (with at most 
2 embeddings). More recently, a significant analysis of specific parameterized 
theories has come from Gibson and Wexler (1994). They propose the Trigger-
ing Learning Algorithm-a simple, psychologically plausible algorithm which 
children might conceivably use to set parameters in finite parameter spaces. 
Investigating the performance of the TLA on the 3-parameter syntax subsys-
tem shown in the example yields the surprising result, that the TLA cannot 
achieve the target parameter setting for every possible target grammar in the 
system. Specifically, there are certain target parameter settings, for which the 
TLA could get stuck in local maxima from which it would never be able to 
leave, and consequently, learn ability would never result. 
We are interested, both in the learnability, and the sample complexity of the 
finite hypothesis classes suggested by the principles and parameters theory. An 
investigation of this sort requires us to define the important dimensions of the 
learning problem-the issues which need to be systematically addressed. The 
following figure provides a schematic representation of the space of possibilities 
which need to be explored in order to completely understand and evaluate a 
parameterized linguistic theory from a learning perspective. The important 
dimensions are as follows: 
1) the parameterization of the language space itself: a particular linguistic 
theory would give rise to a particular choice of universal principles, and asso-
ciated parameters. Thus, one could vary along this dimension of analysis, the 
parameterization hypothesis classes which need to be investigated. The para-
metric system for metrical stress (Example 2) is due to Halle and Idsardi. A 
variant, investigated by Dresher and Kaye (1990), can equally well be subjected 
to analysis. 

136 
INFORMATIONAL COMPLEXITY OF LEARNING 
Distribution of 
Data 
Parametrization 
Memory Requirements 
Noise 
Learning Algorithm 
Figure 4.4. 
The space of possible learning problems associated with parameterized lin-
guistic theories. Each axis represents an im portant dimension along which specific learning 
problems might differ. Each point in this space specifies a particular learning problem. The 
entire space represents a class of learning problems which are interesting. 

LANGUAGE LEARNING 
137 
2)the distribution of the input data: once a parametric system is decided 
upon, one must, then, decide the distribution according to which data (i.e., 
sentences generated by some target grammar belonging to the parameterized 
family of grammars) is presented to the learner. Clearly, not all sentences 
occur with equal likelihood. Some are more likely than others. How does 
this affect learn ability? How does this affect sample complexity? One could, 
of course, attempt to come up with distribution-independent bounds on the 
sample complexity. This, as we shall soon see, is not possible. 
3) the presence, and nature, of noise, or extraneous examples: in practice, 
children are exposed to noise (sentences, which are inconsistent with the target 
grammar) due to the presence of foreign, or idiosyncratic speakers, disfluencies 
in speech, or a variety of other reasons. How does one model noise? How does 
it affect sample complexity or learnability or both? 
4) the type oflearning algorithm involved: a learning algorithm is an effective 
procedure mapping data to hypotheses (parameter values). Given that the 
brain has to solve this mapping problem, it then becomes of interest, to study 
the space of algorithms which can solve it. How many of them converge to the 
target? What is their sample complexity? Are they psychologically plausible? 
5) the use of memory: this is not really an independent dimension, in the 
sense, that it is related to the kind of algorithms used. The TLA, and variants, 
as we shall soon see, are memoryless algorithms. These can be modeled by a 
Markov chain. 
This is the space which needs to be explored. By making a specific choice 
along each of the five dimensions discussed (corresponding to a single point 
in the 5-dimensional space of fig. 4.4, we arrive at a specific learning prob-
lem. Varying the choices along each dimension (thereby traversing the entire 
space of fig. 4.4) gives rise to the class of learning problems associated with 
parameterized linguistic theories. For our analysis, we choose as a concrete 
starting point the Gibson and Wexler Triggering Learning Algorithm (TLA) 
working on the 3-parameter syntactic subsystem in the example shown. In 
our space of language learning problems, this corresponds to (1) a 3-way pa-
rameterization, using mostly X-bar theory; (2) a uniform sentence distribution 
over unembedded (degree-D) sentences; (3) no noise; (4) a local gradient ascent 
search algorithm; and (5) memoryless (online) learning. Following our analysis 
of this learning system, we consider variations in learning algorithms, sentence 
distribution, noise, and language/grammar parameterizations. 
4.4 
FORMAL ANALYSIS OF THE TRIGGERING LEARNING 
ALGORITHM 
Let us start with the TLA. We first show that this algorithm and others like it 
are completely modeled by a Markov chain. We explore the basic computational 
consequences of this fundamental fact, including some surprising results about 
sample complexity and convergence time, the dominance of random walk over 
hill climbing, and the applicability of these results to actual child language 
acquisition and possibly language change. 

138 
INFORMATIONAL COMPLEXITY OF LEARNING 
4.4.1 
Background 
Following Gold (1967) and Gibson and Wexler (1994) the basic framework is 
that of identification in the limit. We assume some familiarity with Gold's 
assumptions. The learner receives an (infinite) sequence of (positive) example 
sentences from some target language. After each example presentation, the 
learner either (i) stays in the same state; or (ii) moves to a new state (changes 
its parameter settings). If after some finite number of examples the learner 
converges to the correct target language and never changes its guess, then it 
has correctly identified the target language in the limit; otherwise, it fails. 
In the Gibson and Wexler model (and others) the learner obeys two addi-
tional fundamental constraints: (1) the single-value constraint-the learner can 
change only 1 parameter value each step; and (2) the greediness constraint-if 
the learner is given a positive example it cannot recognize and changes one pa-
rameter value, finding that it can accept the example, then the learner retains 
that new value. The TLA can then be precisely stated as follows. See Gibson 
and Wexler (1994) for further details. 
• [Initialize] Step 1. Start at some random point in the (finite) space of pos-
sible parameter settings, specifying a single hypothesized grammar with its 
resulting extension as a language; 
• [Process input sentence] Step 2. Receive a positive example sentence Si 
at time ti (examples drawn from the language of a single target grammar, 
L(Gt )), from a uniform distribution on the degree-O sentences of the language 
(we relax this distributional constraint later on); 
• [Learn ability on error detection] Step 3. If the current grammar parses 
(generates) S;, then go to Step 2; otherwise, continue. 
• [Single-step hill climbing] Step 4. Select a single parameter uniformly at 
random, to flip from its current setting, and change it (0 mapped to 1, 1 to 
0) iff that change allows the current sentence to be analyzed; 
Of course, this algorithm never halts in the usual sense. Gibson and Wexler 
aim to show under what conditions this algorithm converges "in the limit"-
that is, after some number, m, of steps, where m is unknown, the correct target 
parameter settings will be selected and never changed. They investigate the 
behavior of the TLA on a linguistically natural, 3-parameter subspace (of the 
complete linguistic parametric space which involves many more parameters). 
We review their subspace immediately below. Note that a grammar in this 
space is simply a particular n-Iength array of O's and l's; hence there are 2n 
possible grammars (languages). Gibson and Wexler's surprising result is that 
the simple 3-parameter space they consider is unlearnable in the sense that 
positive-only examples can lead to local maxima-incorrect hypotheses from 
which a learner can never escape. More broadly, they show that learnability 
in such spaces is still an interesting problem, in that there is a substantive 
learning theory concerning feasibility, convergence time, and the like, that must 

LANGUAGE LEARNING 
139 
be addressed beyond traditional linguistic theory and that might even choose 
between otherwise adequate linguistic theories. 
Remark. Various researchers (Clark & Roberts 1993; Frank & Kapur, 1992; 
Gibson & Wexler, 1994; Lightfoot, 1991) have explored the notion of triggers 
as a way to model parameter space language learning. For these researchers, 
triggers are essentially sentences from the target that cannot be analyzed by 
the learner's current grammatical hypothesis and thereby indirectly inform it 
about the correct hypothesis. Gibson and Wexler suggest that the existence 
of triggers for every (hypothesis, target) pair in the space suffices for TLA 
learnability to hold. As we shall see later, one important corollary of our 
stochastic formulation shows that this condition does not suffice. In other 
words, even if a triggered path exists from the learner's hypothesis language 
to the target, the learner might, with high probability, not take this path, 
resulting in nonlearnability. A further consequence is that many of Gibson 
and Wexler's proposed cures for nonlearnability in their example system, such 
as a "maturational" ordering imposed on parameter settings, simply do not 
apply. On the other hand, this result reinforces Gibson and Wexler's basic 
point that apparently simple parameter-based language learning models can be 
quite subtle-so subtle that even a seemingly complete computer simulation 
can fail to uncover learn ability problems. 
4.4.2 
Tbe Markov formulation 
Given this background, we turn directly to the formalization of parameter space 
learning in terms of Markov chains. This formalization is in fact suggested but 
left unpursued in a footnote of Gibson and Wexler (1994). 
4.4.2.1 
Parameterized Grammars and their Corresponding Markov 
Chains. Consider a parameterized grammar (language) family with n param-
eters. We picture the 2n-size hypothesis space as a set of points; see figure 4.5 
for the 3-parameter case. Each point corresponds to one particular vector of 
parameter settings (languages, grammars). Call each point a hypothesis state 
or simply state of this space. As is conventional, we define these languages over 
some alphabet7 . One state is the target language (grammar). Without loss of 
generality, we may place the (single) target language at the center of this space. 
Since by the TLA the learner is restricted to moving at most 1 binary value 
in a single step, the theoretically possible transitions between states can be 
drawn as (directed) lines connecting parameter arrays (hypotheses) that differ 
by at most 1 binary digit (a 0 or a 1 in some corresponding position in their 
arrays). (Recall that the distance between the grammars in parameter space is 
the so-called Hamming distance.) 
We may further place weights, b, on the transitions from state i to state j. 
These correspond to the probabilities that the learner will move from hypoth-
7Following standard notation. E denotes a finite alphabet and E* denotes the set of all finite 
strings (sentences) obtained by concatenating elements of E. 

140 
INFORMATIONAL COMPLEXITY OF LEARNING 
esis state i to state j. In fact, given a distribution over the target languages 
L(G), we can carry out an exact calculation of these transition probabilities 
themselves. Thus, we can picture the TLA learning space as a directed, labeled 
graph V with 2n vertices.s 
As mentioned, not all these transitions will be possible in general. For exam-
ple, by the single value hypothesis, the system can only move 1 bit at a time. 
Also, by assumption, only differences in surface strings can force the learner 
from one hypothesis state to another. For instance, if state i corresponds to a 
grammar that generates a language that is a proper subset of another grammar 
hypothesis j, there can never be a transition from j to i, and there might be 
one from i to j. Further, it is clear that once we reach the target grammar 
there is nothing that can move the learner from this state, since no positive 
evidence will cause the learner to change its hypothesis. Thus, there must be 
a loop from the target state to itself, and no exit arcs. In the Markov chain 
literature, this is known as an Absorbing State (A). Obviously, a state that 
leads only to an absorbing state will also drive the learner to that absorbing 
state. If a state corresponds to a grammar that generates some sentences of 
the target there is always a loop from that state to itself, with some nonzero 
probability. Finally, let us introduce the notion of a closed set of states C 
to be any proper subset of states in the Markov chain such that there is no arc 
from any of the states in C to any state outside C in the Markov chain (see 
Isaacson & Madsen, 1976; Resnick, 1992, and later in this chapter for further 
details). In other words, it is a set of states from which there is no way out to 
other states lying ouside this set. Clearly, a closed set with only one element 
(state) is an absorbing state. 
Note that in the absence of noise, the target state is always an Absorbing 
State in the systems under discussion. This is because once the learner is at 
the target grammar, all examples it receives are analyzable and it will never 
exit this state. Consequently, the Markov chains we will consider always have 
at least one A. Given this formulation, one can immediately give a very simple 
learnability theorem stated in terms of the Markov chains corresponding to 
finite parameter spaces and learning algorithms9. We do this below. 
4.4.2.2 
Markov Chain Criteria for Learnability. We argued how the 
behavior of the Triggering Learning Algorithm can be formalized by a Markov 
Chain. This argument will be formally completed by providing details of the 
transition probabilities in a little while. While the formalization is provided 
for the TLA, every memoryless learning algorithm A for identifying a target 
grammar g, from a family of grammars g via positive examples can be formal-
8Gibson and Wexler construct an identical transition diagram in the description of their 
computer program for calculating local maxima. However, this diagram is not explicitly 
presented as a Markov structure; it does not include transition probabilities, which we shall 
see lead to crucial differences in learnability results. Of course, topologically, both structures 
must be identical. 
9Note that learnability requires that the learner converge to the target state from any initial 
state in the system. 

LANGUAGE LEARNING 
141 
ized as a Markov chain M (see appendix B of this chapter). In particular, M 
has as many states as there are grammars in g with the states in M being in 
1-1 correspondence with grammars 9 E g. The target grammar gj corresponds 
to a target state Sj of M. We call M the Markov chain associated with the 
triple (A, g, gj), and the triple itself a memoryless learning system, or learning 
system for short. The triple decides completely the topology of the chain. The 
transition probabilities of the chain are related to the probability P with which 
sentences are presented to the learner. 
An important question of interest is whether or not the learning algorithm A 
identifies the target grammar in the limit. The following theorem shows how to 
translate this conventional Gold-Iearnability criterion for identifiability in the 
limit into a corresponding Markov chain criterion for such memoryless learning 
systems. 
We first recall the familiar definition for Gold-Iearnability: 
Definition 4 Consider a family of grammars g, a target grammar gj E g, and 
a learning algorithm A that is exposed to sentences from the the target according 
to some arbitrary distribution P. Then 9 j is said to be Gold learnable by A 
for the distribution P if and only if A identifies gj in the limit with probability 
1. 
A family of grammars g is Gold-learnable if and only if each member of g 
is Gold-learnable. 
The learn ability theorem below says that if a target grammar gj Egis to 
be Gold-learnable by A, then the Markov chain associated with the particular 
learning system must be restricted in a certain way. To understand the state-
ment of the theorem, we first recall the related notions of absorbing state and 
closed set of states. Intuitively, these terms refer to Markov chain connectivity 
and associated probabilities: an absorbing state has no exit link to any other 
state, while a closed set of states is the extension of the absorbing state notion 
to a set of states. They have already introduced informally in the earlier sec-
tion for pedagogical reasons. They are reproduced here again for completeness 
of the current formal account. 
Definition 5 Given a Markov chain M, an absorbing state of M is a state 
s E M that has no exit arcs to any other states of M. 
Since by the definition of a Markov chain the sum of the transition probabilities 
exiting a state must equal one, it follows that an absorbing state must have a 
self-loop with transition probability 1. In a learning system that makes transi-
tions based on error detection, the target grammar will be an absorbing state, 
because once the learner reaches the target state, all examples are analyzable 
and the learner will never exit that state. 
Definition 6 Given a Markov chain M, a closed set of states (C) is any 
proper subset of states in M such that there is no arc from any of the states in 
C to any state not in C. 

142 
INFORMATIONAL COMPLEXITY OF LEARNING 
If two states belong to the same closed set C then there may be transitions 
from one to the other. Further, there can be transitions from states outside C 
to states within C. However, there cannot be transitions from states within C 
to states outside C. Clearly, an absorbing state represents the special case of 
a closed set of states consisting of exactly one element, namely, the absorbing 
state itself. 
We can now state the learnability theorem. 
Theorem 4.1 Let < A, g, g, E 9 > be a memoryless learning system. Let 
sentences from the target be presented to the learner according to the distribution 
P and let M be the Markov chain associated with this learning system. Then 
the target g, is Gold-learnable by A for the distribution P if and only if M is 
such that every closed set of states in it includes the target state corresponding 
to g,. 
Proof This has been relegated to the appendix for continuity of reading. I 
Thus, if we are interested in the Gold-Iearnability of a memory less learning 
system, one could first construct the Markov chain corresponding to such a 
system and then check to see if the closed sets of the chain satisfy the conditions 
of the above theorem. If and only if they do, the system is Gold-learnable. 
We now provide an informal example of how to construct a Markov chain 
for a parametric family of languages. This is followed by a formal account 
of how to compute the transition probabilities of the Markov chain. Finally, 
we note some additional properties of the learning system that fall out as a 
consequence of our analysis. For example, our analysis is consistent with the 
subset principle, it can handle a variety of algorithms, and even noise. 
Example. 
Consider the following 3-parameter system studied by Gibson and Wexler 
(1994). Its binary parameters are: (1) Spec(ifier) first (0) or last (1); (2) 
Comp(lement) first (0) or last (1); and Verb Second constraint (V2) does not 
exist (0) or does exist (1). Following standard linguistic convention, by Speci-
fier we mean the part of a phrase that "specifies" that phrase, roughly, like the 
old in the old book; by Complement we mean roughly a phrase's arguments, 
like an ice-cream in John ate an ice-cream or with envy in green with envy. 
There are also 7 possible "words" in this language: S, V, 0, 01, 02, Adv, and 
Aux, corresponding to Subject, Verb (Main), Object, Direct Object, Indirect 
Object, Adverb, and Auxiliary Verb. There are 12 possible surface strings for 
each (-V2) grammar and 18 possible surface strings for each (+ V2) grammar 
if we restrict ourselves to unembedded or "degree-O" examples for reasons of 
psychological plausibility (see Wexler & Culicover, 1980; Lightfoot, 1991; and 
Gibson & Wexler, 1994 for discussion). Note that the "surface strings" ofthese 
languages are actually phrases such as [Subject, Verb, Object] as in John ate 
an ice-cream. Figure (3) of Gibson and Wexler summarizes the possible binary 
parameter settings in this system. For instance, parameter setting #5 corre-
sponds to the array [0 1 0]= Specifier first, Comp last, and - V2, which works 
out to the possible basic English surface phrase order of Subject-Verb-Object 
(SVO). As shown in Gibson and Wexler's figure (3), the other possible arrange-

LANGUAGE LEARNING 
143 
ments of surface strings corresponding to this parameter setting include S V; 
S V 01 02 (two objects, as in give John an ice-cream); S Aux V (as in John 
will eat); S Aux V 0; S Aux V 01 02; Adv S V (where Adv is an Adverb, like 
quickly); Adv S V 0; Adv S V 01 02; Adv S Aux V; Adv S Aux V 0; and 
Adv S Aux V 01 02. 
4.4.2.3 
The Markov chain for the 3-parameter Example. Suppose 
the target language is SVO (Subject Verb Object, or "English" setting #5=[0 
1 0)). Within the Gibson and Wexler 3-parameter system, there are 23 = 8 
possible hypotheses, so we can draw this as an 8-point Markov configuration 
space, as shown in Figure 4.5. The shaded rings represent increasing distance 
in parameter space (Hamming distances) from the target. Each labeled circle 
is a Markov state, a possible array of parameter settings or grammar, hence 
specifies a possible target language. Each state is exactly 1 binary digit away 
from its possible transition neighbors. Each labeled, directed arc between the 
points is a possible transition from state i to state j, where the labels are the 
transition probabilities; note that the probabilities from all arcs exiting a state 
sum to 1. We shall show how to compute these probabilities immediately below. 
The target grammar, a double circle, lies at the center. This corresponds to 
the (English) SVO language. Surrounding the bulls-eye target are the three 
other parameter arrays that differ from [0 1 0] by one binary digit each; we 
picture these as a ring 1 Hamming distance away from the target: [0, 1, 1], 
corresponding to Gibson and Wexler's parameter setting #6 in their figure 3 
(Spec-first, Comp-final, + V2, basic order SVO+ V2); [0 0 0], corresponding 
to Gibson and Wexler's setting #7 (Spec-first, Comp-first, - V2), basic order 
SOY; and [1 1 0], Gibson and Wexler's setting #1 (Spec-final, Comp-final, 
- V2), basic order VOS. 
Around this inner ring lie three parameter setting hypotheses, all 2 binary 
digits away from the target: [00 1], [1 0 0], and [1 1 1] (grammars #2,3, and 
8 in Gibson and Wexler's figure 3). Finally, one more ring out, three binary 
digits different from the target, is the hypothesis [10 1], corresponding to target 
grammar 4. 
It is easy to see from inspection of the figure that there are exactly two 
Absorbing States in this Markov chain, that is, states that have no exit arcs 
with non-zero probability. One absorbing state is the target grammar (by 
definition). The other absorbing state is state 2 (corresponding to language 
VOS+V2, i.e., [1 1 1]). Finally, state 4 (parameter setting [1 0 1)), while not 
an absorbing state in itself, has no path to the target. It has arcs that lead 
only to itself or to state 2 (an absorbing state which is not the target). These 
two states correspond to the local maxima at the head of Gibson and Wexler's 
figure 4. Hence this target language is not learnable. In addition to these local 
maxima, the next section below shows that there are in fact other states from 
which the learner will, with high probability, never reach the correct target. 

144 
INFORMATIONAL COMPLEXITY OF LEARNING 
Figure 4.5. The 8 parameter settings in the GW example, shown as a Markov struc-
ture. Directed arrows between circles (states, parameter settings, grammars) repre-
sent possible nonzero (possible learner) transitions. The target grammar (in this 
case, number 5, setting [0 1 0]), lies at dead center. Around it are the three settings 
that differ from the target by exactly one binary digit; surrounding those are the 3 
hypotheses two binary digits away from the target; the third ring out contains the 
single hypothesis that differs from the target by 3 binary digits. Note that the learner 
can either stay in the same state or step in or out one ring (binary digit) at a time, 
according to the single-step learning hypothesis; but some transitions are not possible 
because there is no data to drive the learner from one state to the other under the 
TLA. Numbers on the arcs denote transition probabilities between grammar states; 
these values are not computed by the original GW algorithm. The next section shows 
how to compute these values, essentially by taking language set intersections. 

LANGUAGE LEARNING 
145 
4.4.3 Derivation of the transition probabilities for the Markov TLA structure 
We have discussed in the previous section how the behavior of the TLA can be 
modeled as a Markov chain. The argument is incomplete without a character-
ization of the transition probabilities of the associated Markov chain. We first 
provide an example and follow it with a formal exposition. 
Example. Consider again the 3-parameter system in figure 4.5 with target 
language 5. What is the probability that the learner will move from state 8 to 
state 6 ? The learner will make such a transition if it receives a sentence that 
is analyzable according to the parameter settings of state 6, but not according 
to the parameter settings of state 8. For example, a sentence of the form (S V 
01 02) as in Peter gave John an ice-cream could drive the learner to change 
its parameter settings from 8 to 6. If one assumes a probability distribution 
with which sentences from the target are presented to the learner, one could 
find the total probability measure of all such sentences and use it to calculate 
the appropriate transition probability. 
4.4.3.1 
Formalization. The computation of the transition probabilities 
from the language family can be done by a direct extension of the procedure 
given in Gibson and Wexler (1994). Let the target language Lt consist of the 
strings 81,82, ... , i.e., 
Let there be a probability distribution P on these strings. Suppose the learner 
is in a state s corresponding to the language L8 • Consider some other state k 
corresponding to the language Lk. What is the probability that the TLA will 
update its hypothesis from L8 to Lk after receiving the next example sentence? 
First, observe that due to the single valued constraint, if k and 8 differ by 
more than one parameter setting, then the probability of this transition is zero. 
In fact, the TLA will move from s to k only if the following two conditions 
are met: (1) the next sentence it receives (say, w occurring with probability 
P(w)) is analyzable by the parameter settings corresponding to k and not by 
the parameter setting corresponding to 8; and (2) the TLA has a choice of n 
parameters to flip on not being able to analyze wand it happens to pick the 
one which would move it to state k. 
Event 1 occurs with probability I:wE(Lk\L 8 )nL, P(w). This is simply the 
probability measure associated with all strings w that are both in the target 
Lt and Lk but not in the language L8 (the learner's currently hypothesized 
language). Event 2 occurs with probability lin, since the parameter to flip 
is chosen uniformly at random out of the n possible choices. Thus the co-
occurrence of both these events yields the following expression for the total 
probability of transition from 8 to k after one step: 
P[s -> k] = 

146 
INFORMATIONAL COMPLEXITY OF LEARNING 
Since the total probability over all the arcs out of s (including the self-loop) 
must be 1, we obtain the probability of remaining in state s after one step as: 
P[s -+ s] = 1-
P[s -+ k] 
k is a neighboring state of • 
In other words, the probability of remaining in state s is 1 minus the probability 
of moving to any of the other (neighboring) states. 
Finally, given any parameter space with n parameters, we have 2n languages. 
Fixing one of them as the target language Lt we obtain the following procedure 
for constructing the corresponding Markov chain. Note that this is simply the 
Gibson and Wexler procedure for finding local maxima, with the addition of a 
probability measure on the language family. 
• [Assign distribution] Fix a probability measure P on the strings of the target 
language Lt. 
• [Enumerate states] Assign a state to each language i.e., each Li. 
• [Normalize by the target language] Intersect all languages with the target 
language to obtain for each i, the language Li = L; n Lt. Thus with state i 
associated with language L i , we now associate the language Li. 
• [Take set differences] For any two states i and k, i i= k, if they are more than 
1 Hamming distance apart, then the transition P[i -+ k] = O. If they are 1 
Hamming distance apart then P[i -+ k] = ~P(Lk \ Lt). For i = k, we have 
P[i -+ i] = 1 - Lj;<!i P[i -+ j]. 
Remark. This model captures the dynamics of the TLA completely. We note 
that the learner's movement from one language hypothesis to another is driven 
by purely extensional considerations-that is, it is determined by set differences 
between language pairs. A detailed investigation of this point is beyond the 
scope of this chapter. We simply note here that if this extensional calculation is 
the basis of the learning algorithm, then it is unclear what the notion "trigger" 
means, because the calculation simply refers to string-language set differences. 
We shall therefore henceforth place the term "trigger" in quotes. (The same 
point has been made by Frank & Kapur, 1992). 
Example (continued): For our three parameter system, we can follow the above 
procedure to calculate set differences and build the Markov figure straightfor-
wardly. For example, consider P[8 -+ 6]; we compute (L6 \ L8 ) n L5 = { S V 
01 02, S Aux V 0, S Aux V 01 02}. This set has three degree-O sentences. 
Assuming a uniform distribution on the 12 degree-O strings of the target L 5 , we 
obtain the value of the transition from state 8 to state 6 to be H3/12) = 112 , 
Further, since the normalized language Li for state 1 is the empty set, the set 
difference between states 1 and 5 (L~ \ Li) yields the entire target language, so 
there is a (high) transition probability from state 1 to state 5. Similarly, since 
states 7 and 8 share some target language strings in common, such as S V, and 
do not share others, such as Adv Sand S V 0, the learner can move from state 
7 to 8 and back again. 

LANGUAGE LEARNING 
147 
4.4.3.2 
Additional Properties of the Learning System. Once the math-
ematical formalization has been given many additional properties of this par-
ticular learning system now become evident. For example, an issue that is 
amenable to analysis in the current formalization has to do with the existence 
of subset/superset pairs oflanguages. The existence of such pairs does not alter 
the procedure by which the Markov chain is computed, nor does it alter the 
validity of our main learn ability theorem. However, it is clear by our analysis, 
that if the target happens to be a subset language, the superset language will 
correspond to an absorbing state. This is because all target sentences are ana-
lyzable by the superset language and if the error-driven learner happens to be 
at the state corresponding to it, it will never exit. This additional absorbing 
state automatically implies non-Iearnability by our theorem. Consequently the 
classic results on subset/superset non-Iearnability all fall out as special cases 
of our framework. However, following Gibson and Wexler, we will assume that 
such complications do not arise in the parametric systems under discussion in 
the current chapter. 
It is now easy to imagine other alternatives to the TLA that will avoid the 
local maxima problem: we can vary any of the five aspects of the language 
learning models we described at the beginning of this chapter. To take just 
one example, as it stands the learner is allowed to change only one parameter 
setting at a time. If we relax this condition so that in this situation the learner 
can change more than one parameter at a time, i.e., the learner can conjecture 
hypotheses far from its current one (in parameter space), then the problem with 
local maxima disappears. It is easy to see that in this case, there can be only 
one Absorbing State, namely the target grammar. All other states have exit 
arcs (under the previous assumption of no subset/superset relations). Thus, by 
our main theorem, such a system is learnable. 
As another variant, consider the possibility of noise-that is, occasionally 
the learner gets strings that are not in the target language. Gibson and Wexler 
state (fn. 4) that this is not a problem: the learner need only pay attention to 
frequent data. But this is of course a serious problem for the model; how is 
the learner to "pay attention" to frequent data? Unless some kind of memory 
or frequency-counting device is added, the learner cannot know whether the 
examples it receives are noise or not. If the learner is memory less , then there 
is always some finite probability, however small, of escaping a local maximum. 
Clearly, the memory window has to be large enough to ensure that sufficient 
statistics are computable to distinguish noise from relevant data. A serious 
investigation of this issue is beyond the scope of this chapter. 
To explore these and other possible variations systematically, let us return to 
the 5-way classification scheme for learning models introduced at the beginning 
of this chapter. We consider first details about sample complexity. Next, 
we turn to questions about the distribution of the input data, and ask how 
this changes the sample complexity results. We also consider realistic input 
distributions, namely, some drawn from the CHILDES corpus (MacWhinney, 
1990). Finally, we briefly consider sample complexity issues if the learning 
algorithms operate in batch rather than on-line mode. In the next chapter, 

148 
INFORMATIONAL COMPLEXITY OF LEARNING 
we will consider the effect of noise, and how that can potentially bring about 
diachronic syntax change, as well as some alternate parameterizations of the 
grammatical space. 
4.5 
CHARACTERIZING CONVERGENCE TIMES FOR THE MARKOV 
CHAIN MODEL 
The Markov chain formulation gives us some distinct advantages in theoretically 
characterizing the language acquisition problem. First, we have already seen 
how given a Markov Chain one could investigate whether or not it has exactly 
one absorbing state corresponding to the target grammar. This is equivalent to 
the question of whether any local maxima exist. One could also look at other 
issues (like stationarity or ergodicity assumptions) that might potentially affect 
convergence. Later we will consider several variants to TLA and analyze them 
formally within the Markov framework. We will also see that these variants do 
not suffer from the local maxima problem associated with GW's TLA. 
Perhaps the significant advantage of the Markov chain formulation is that 
it allows us to also analyze convergence times. Given the transition matrix of 
a Markov chain, the problem of how long it takes to converge has been well 
studied. This question is of crucial importance in learnability. Following GW, 
we believe that it is not enough to show that the learning problem is consistent 
i.e., that the learner will converge to the target in the limit. We also need to 
show, as GW point out, that the learning problem is feasible, i.e., the learner 
will converge in "reasonable" time. This is particularly true in the case of finite 
parameter spaces where consistency might not be as much of a problem as 
feasibility. The Markov formulation allows us to attack the feasibility question. 
It also allows us to clarify the assumptions about the behavior of data and 
learner inherent in such an attack. We begin by considering a few ways in 
which one could formulate the question of convergence times. 
4.5.1 Some Transition Matrices and Their Convergence Curves 
Let us begin by following the procedure detailed in the previous section to ac-
tually obtain a few transition matrices. Consider the example which we looked 
at informally in the previous section. Here the target grammar was grammar 5 
(according to our numbering of the languages in Appendix A). For simplicity, 
let us first assume a uniform distribution on the degree-O strings in L5, i.e., the 
probability the learner sees a particular string Sj in L5 is 1/12 because there 
are 12 (degree-O) strings in L5 • We can now compute the transition matrix as 
the following, where O's occupy matrix entries if not otherwise specified: 

LANGUAGE LEARNING 
149 
To 
L1 
L2 
L3 L4 
L5 
L6 
L7 
L8 
L1 
1 
1 
1 
2" 
"6 
3" 
L2 
1 
L3 
3 
1 
1 
4" 
it 
"6 
From 
L4 
1 
12 
12 
L5 
1 
L6 
1 
5 
g 
6" 
L7 
2 
1 
18 
¥ 
~8 
L8 
1 
12 
36 
"9 
Notice that both 2 and 5 correspond to absorbing states; thus this chain 
suffers from the local maxima problem. Note also (following the previous figure 
as well) that state 4 only exits to either itself or to state 2, hence is also a local 
maximum. For a given transition matrix T, it is possible to compute 
Too = lim Tm. 
m_oo 
If T is the transition probability matrix of a chain, then tij, i.e. the element 
of T in the ith row and jth column is the probability that the learner moves 
from state i to state j in one step. It is a well-known fact that if one considers 
the corresponding i, j element ofTm then this is the probability that the learner 
moves from state i to state j in m steps. Correspondingly, the i, jth element 
of Too is the probability of going from initial state i to state j "in the limit" as 
the number of examples goes to infinity. For learnability to hold irrespective of 
which state the learner starts in, the probability that the learner reaches state 5 
should tend to 1 as m goes to infinity. This means that column 5 of Too should 
consist of 1 's, and the matrix should contain O's everywhere else. Actually we 
find that Tm converges to the following matrix as m goes to infinity: 
To 
L1 
L2 
L3 
L4 
L5 
L6 
L7 L8 
L1 
0 
2 
0 
0 
3 
0 
0 
0 
"5 
"5 
L2 
1 
L3 
2 
3 
Too = 
"5 
"5 
From 
L4 
1 
L5 
1 
L6 
1 
L7 
1 
L8 
1 
Examining this matrix we see that if the learner starts out in states 2 or 4, 
it will certainly end up in state 2 in the limit. These two states correspond to 
local maxima grammars in the GW framework. If the learner starts in either 
of these two states, it will never reach the target. From the matrix we also see 
that if the learner starts in states 5 through 8, it will certainly converge in the 
limit to the target grammar. 

150 
INFORMATIONAL COMPLEXITY OF LEARNING 
The situation regarding states 1 and 3 is more interesting, and not covered 
in Gibson and Wexler (1994). If the learner starts in either of these states, 
it will reach the target grammar with probability 3/5 and reach state 2, the 
other absorbing state with probability 2/5. Thus we see that local maxima 
(states unconnected to the target) are not the only problem for learnability. 
As a consequence of our stochastic formulation, we see that there are initial 
hypotheses from which triggered paths exist to the target, however the learner 
will not take these paths with probability one. In our case, because of the 
uniform distribution assumption, we see that the path to the target will only 
be taken with probability 3/5. By making the distribution more favorable, this 
probability can be made larger, but it can never be made one. 
This analysis, motivated as it was by our information-theoretic perspective, 
considerably increases the number of problematic initial states from that pre-
sented in Gibson and Wexler. While the broader implications of this is not 
clear, it certainly renders moot some of the linguisticlO implications of GW's 
analysis. 
Obviously one can examine other details of this particular system. However, 
let us now look at a case where there is no local maxima problem. This is the 
case when the target languages have verb-second (V2) movement in GW's 3-
parameter case. Consider the transition matrix (shown below) obtained when 
the target language is L 1 . Again we assume a uniform distribution on strings 
of the target. 
To 
Ll 
L2 
L3 
L4 
L5 
L6 
L7 L8 
Ll 
1 
L2 
1 
5 
g 
6' 
L3 
2 
1 
18 , 11 
From 
L4 
3 
36 
36 
'9 
L5 
1 
23 
1 
3' 
36 
~~ 
L6 
5 
36 
36 
L7 
1 
11 
1 
18 
12 
r~ 
L8 
1 
18 
18 
Here we find that T"' does indeed converge to a matrix with 1's in the first 
column and O's elsewhere. Consider the first column of 1'"'. It is of the form: 
(P1 (m), P2( m), P3( m), P4( m), P5( m), P6( m), P7( m), P8( m»' 
leFor example, GW rely on "connectedness" to obtain their list of local maxima. From this 
(incorrect) list, noticing that all local maxima were +Verb Second (+V2), they argued for 
ordered parameter acquisition or ''maturation''. In other words, they claimed that the V2 
parameter was more crucial, and had to be set earlier in the child's language acquisition 
process. Our analysis shows that this is incorrect, an example of how computational analysis 
can aid the search for adequate linguistic theories. 

'EC! 
'15:-
S I 
I~ 
::J 
~ 
'E 
j~ 
~. 
~o , 
8", 
'0 0 
f 
LANGUAGE LEARNING 
151 
~---
I 
~C! 
Q. 0 L....r-----....-------.------r-------.---' 
o 
100 
200 
300 
400 
Number of examples (m) 
Figure 4.6. 
Convergence as function of number of examples. The horizontal axis denotes 
the num ber of exam pies received and the vertical axis represents the probability of converging 
to the target state. The data from the target is assumed to be distributed uniformly over 
degree-O sentences. The solid line represents TLA convergence times and the dotted line is 
a random walk learning algorithm (RWA). Note that random walk actually converges faster 
than the TLA in this case. 
Here Pi(m) denotes the probability of being in state 1 at the end of m 
examples in the case where the learner started instate i. Naturally we want 
lim Pi(m) = 1 
m_oo 
and for this example this is indeed the case. Fig. 4.6 shows a plot of the 
following quantity as a function of m, the number of examples. 
p(m) = m.in{pi(m)} 
, 
The quantity p(m) is easy to interpret. Thus p(m) = 0.95 means that for 
every initial state of the learner the probability that it is in the target state 
after m examples is at least 0.95. Further there is one initial state (the worst 
initial state with respect to the target, which in our example is Ls) for which 
this probability is exactly 0.95. We find on looking at the curve that the 
learner converges with high probability within 100 to 200 (degree-D) example 
sentences, a psychologically plausible number. (One can now of course proceed 

152 
INFORMATIONAL COMPLEXITY OF LEARNING 
to examine actual transcripts of child input to calculate convergence times for 
"actual" distributions of examples, and we are currently engaged in this effort.) 
Now that we have made a first attempt to quantify the convergence time, 
several other questions can be raised. How does convergence time depend upon 
the distribution of the data? How does it compare with other kinds of Markov 
structures with the same number of states? How will the convergence time 
be affected if the number of states increases, i.e the number of parameters 
increases? How does it depend upon the way in which the parameters relate to 
the surface strings? Are there other ways to characterize convergence times? 
We now proceed to answer some of these questions. 
4.5.2 Absorption Times 
In the previous section, we computed the transition matrix for a fixed (in prin-
ciple, this could be arbitrary) distribution and showed the rate of convergence 
in a certain way. In particular, we plotted p(m), (the probability of converging 
from the most unfavorable initial state) against m (the number of samples). 
However, this is not the only way to characterize convergence times. Given an 
initial state, the time taken to reach the absorption state (known as the ab-
sorption time) is a random variable. One can compute the mean and variance 
of this random variable. For the case when the target language is L 1 , we have 
seen that the transition matrix has the form: 
T= (~ ~) 
Here Q is a 7 -dimensional square matrix. The mean absorption times from 
states 2 through 8 is given by the vector (see Isaacson and Madsen (1976) ) 
J.t=(I-Q)-11 
where 1 is a 7-dimensional column vector of ones. The vector of second mo-
ments is given by 
1" = (I - Q)-1(2J.t -1). 
Using this result, we can now compute the mean and standard deviation of the 
absorption time from the most unfavorable initial state of the learner. (We note 
that the second moment is fairly skewed in such cases and so is not symmetric 
about the mean, as may be seen from the previous curves.) The four learning 
scenarios considered are the TLA with uniform, and increasingly malicious 
distributions (discussed later), and the random walk (also discussed later). 
Learning 
scenariO 
TLA (uniform) 
TLA (a = 0.99) 
TLA (a = 0.9999) 
RW 
Mean abs. 
time 
34.8 
45000 
4.5 x 106 
9.6 
I St. Dev. I 
of abs. time 
22.3 
33000 
3.3 X 106 
10.1 

LANGUAGE LEARNING 
153 
4.5.3 Eigenvalue Rates of Convergence 
We have shown how to characterize learnability by Markov chains. Recall 
that Markov chains corresponding to memoryless learning algorithms have an 
associated transition matrix T. We saw that Tk was the transition matrix after 
k examples, and in the limiting case, 
lim Tk = Too. 
k-+oo 
In general, the structure of Too, as discussed earlier, determined whether 
the target grammar was Gold-learnable. The rate at which T converges to Too 
determines the rate at which the learner converges to the target "in the limit" . 
This rate allows us to bound the sample complexity in a formal sense, i.e., 
it allows us to bound the number of examples needed before the learner will 
be at the target with high confidence. In this section, we develop some formal 
machinery borrowed from classical Markov chain theory that is useful to bound 
the rate of convergence of the learner to the target grammar for learnable target 
grammars. We first develop the notion of an eigenvalue of a transition matrix 
and show how this can be used to construct an alternative representation of 
Tk. We then discuss the limiting distributions of Markov chains from various 
initial conditions, and finally combine all these notions to formally state some 
results for the rate at which the learner converges to the target. 
4.5.3.1 
Eigenvalues and Eigenvectors. Many properties of a transition 
matrix can be characterized by its eigenvalues and eigenvectors. 
Definition 7 A number A is said to be an eigenvalue of a matrix T if there 
exists some nonzero vector x satisfying 
xT = Ax. 
Such a row vector x is called a left eigenvector ofT corresponding to the eigen-
value A. Similarly, a nonzero column vector y satisfying Ty = Ay is called a 
right eigenvector of T. 
It can be shown that the eigenvalues of a matrix T can be obtained by solving 
IM-TI = 0 
( 4.1) 
where I is the identity matrix and IMI denotes the determinant of the matrix 
M. 
Example: Consider the matrix 
T=[t tJ 
Such a matrix could, for example, be the transition matrix for a learner in 
a parametric space with two grammars, i.e., a space defined by one boolean 

154 
INFORMATIONAL COMPLEXITY OF LEARNING 
valued parameter. In order to solve for the eigenvalues of the matrix, we need 
to solve 
1M - TI = 1 [ ~ 0] [£ 1]1 
A 
- t: =0 
This reduces to the quadratic equation 
which can be solved to yield A = 1 and A = ~ as its two solutions. It can be 
easily seen that the row vector, x = (1,1) is a left eigenvector corresponding to 
the eigenvalue A = 1. As a matter of fact, all multiples of (1, 1) are eigenvectors 
for this particular eigenvalue. Similarly, it can also be seen that x = (1, -1) is 
a left eigenvector for the eigenvalue A = ~. I 
In general, for an m x m matrix T, eq. 4.1 is an mth order equation and can 
be solved to yield m solutions (complex-valued) for A. Two other facts about 
eigenvalue solutions of such transition matrices are worth noting here. 
1. For transition matrices corresponding to finite Markov chains, it is possible to 
show that A = 1 is always an eigenvalue. Further, it is the largest eigenvalue' 
in that any other eigenvalue, A, is less than one in absolute value, i.e., IAI < 1. 
2. For transition matrices corresponding to finite Markov chains, the multi-
plicity of the eigenvalue A = 1 is equal to the number of closed classes (see 
appendix) in the chain. 
In our example above, we do see that A = 1 is an eigenvalue. It has multi-
plicity of 1, indicating that there is only one closed class in the chain; in the 
example, the class consists of the two states of the chain. 
4.5.3.2 
Representation of Tk. The eigenvalues and associated eigenvec-
tors can be used to represent Tk in a form that is convenient for bounding 
the rate of its convergence to Too. This representation is only true for matrices 
that are offull rank, i.e., m x m matrices that have m linearly independent left 
eigenvectors. 
Let T be an m x m transition matrix. Let it have m linearly independent left 
eigenvectors Xl, ... ,Xm corresponding to eigenvalues A1, A2, ... , Am. One could 
then define the matrix L whose rows are the left eigenvectors of the matrix T. 
Thus 
L= [~l 
Clearly, since the rows of L are linearly independent, its inverse, L -1 exists. 
It turns out that the columns of L -1 are the right eigenvectors of T. Let the 
ith column of L-1 be Yi; i.e., 

LANGUAGE LEARNING 
155 
£-1 = [Y1 
Y2 
Ym 1 
Now we can represent Tk in a convenient form stated in the following lemma: 
Lemma 4.5.1 Let T be an m x m transition matrix having m linearly inde-
pendent left eigenvectors, Xl, ... , xm corresponding to eigenvalues A1, ... , Am. 
Further let £ be the matrix whose rows are the left eigenvectors and let the 
columns of £ -1 be Yi 'so Then 
m 
Tk = EAfy;x; 
;=1 
Thus, according to the lemma above, Tk can be represented as the linear 
combination of m fixed matrices (Y;Xi). The coefficients of this linear combina-
tion are Af. Clearly, we see that the rate of convergence of Tk is now bounded 
by the rate of convergence of terms like Af. 
Example (contd.) Continuing our previous example, we can construct the 
matrices, Land £-1 out of the left eigenvectors. In fact using our solutions 
from before, we see that 
£ = [~ !1] 
and £-1 = [t }~] 
The rows of £ are the Xi'S and the columns of L- 1 are the y;'s.1 
4.5.3.3 
Initial Conditions and Limiting Distributions. Recall that 
the learner could start in any initial state. One could quantify the initial 
condition of the learner by putting a distribution on the states of the Markov 
chain according to which the learner picks its initial state. Let this be denoted 
by IIo = (11"1(0),11"2(0), ... , 1I"m(0)). Thus, 11";(0) is the probability with which 
the learner picks the ith state as the initial state. For example, if the learner 
were equally likely to start in any state, then 11";(0) = ;k for all i. 
The above characterizes the probability with which the learner is in each 
of the states before having seen any examples. The learner would then move 
from state to state according to the transition matrix T. After k examples, the 
probability with which the learner would be in each of the states is given by: 
IIk = IIoTk 
Finally, one could characterize the limiting distribution as 
II = lim IIk = IIoToo 
k-oo 
(4.2) 
Clearly, II characterizes the probability with which the learner is in each 
of the states "in the limit". Suppose the target were £1, and it were Gold-
learnable; then the first element of the vector II would be 1 and all other 
elements would be O. In other words, the probability that the learner is at the 
target in the limit is 1 and the probability that the learner is at some other 
state (non-target) in the limit is correspondingly O. 

156 
INFORl\L\TIONAL COMPLEXITY OF LEARNING 
4.5.3.4 
Rate of Convergence. \Ve are interested in bounding the rate 
at which TIk converges to TI. \Ve see that this rate depends on the rate at 
which Tk converges to Teo (eq. 4.2) which in turn depends upon the rate at 
which the A2' converges to 0 by Lemma 4.5.1(for i > 1). As we have discussed, 
A1 = 1. Consequently, we can bound the rate of convergence by the rate at 
which the second largest eigenvalue converges to O. Thus we can state the 
following theorem. 
Theorem 4.2 Let the transition matrix characteri::ing the behavior of the mem-
oryless learner be T. Further, let T have the eigenvalues, A1,"" Am, and m 
linearly independent left eigenvectors, Xl, ... , Xm and m right eigenrfciors 
Yl, ... , Ym; A1 = 1 Then, the distance between the learner's state after k e;z;a m-
pIes and its state in the limit is given by: 
n 
n 
1/ TIk - TI 1/=11 L A~TIoYixi II::; l!!P<~ {IAd k } L 1/ TIoYjXj II 
i=2 
-
-
j=2 
Let us first apply this theorem to the illustrative example of this section. 
Example (contd.) We have already solved for the eigenvalues of T and con-
structed the matrices Land L -1. The rows of L are the row vectors Xi and the 
columns of L -1 are the column vectors Yi. Assuming that the learner is three 
times as likely to start in state 1 as compared to state 2, i.e., TID = (~, t), we 
can show that : 
Thus the rate at which the learner converges to the limiting distribution over the 
state space is of the order of (~ )k. Note that ~ is the second largest eigenvalue 
of the transition matrix. I 
4.5.3.5 
Transition Matrix Recipes:. The above discussion allows us to 
see how one could extract useful learnability properties of the memory less 
learner from the transition matrix characterizing the behavior of that learner 
on the finite parameter space. As a matter of fact, we can now outline a pro-
cedure whereby one could check for the learnability and sample complexity of 
learning in such parameter spaces. 
1. Construct the transition matrix T for the memory less learner according to 
the arguments developed earlier. Such a matrix has 2n states if there are n 
boolean valued parameters in the grammatical theory. 
2. Compute the eigenvalues of the matrix T. 
3. If the multiplicity of the eigenvalue A = 1 is more than one, then there are 
additional closed classes and by the learnability theorem, the target grammar 
is not Gold-learnable. 

LANGUAGE LEARNING 
157 
4. If the target is Gold-learnable, and the eigenvectors are linearly independent, 
then use Theorem 4.2 to bound the rate of convergence. 
Using such a procedure, we can bound the rate of convergence of each of 
the following learning scenarios for the three parameter syntactic subsystem 
we have examined in some detail in previous examples. In each case, the target 
is the language L 1• The learning algorithm is the TLA with different sentence 
distributions (parameterized by a with b, c, d chosen to make sentences outside 
of A equally likely; see next section). We also considered the Random Walk 
Algorithm (no greediness, no single value; see next section) with a uniform 
sentence distribution. The rate of convergence is denoted as a function of the 
number of examples. 
Learning scenario 
TLA (uniform) 
TLA(a = 0.99) 
TLA(a = 0.9999) 
Random Walk 
Rate of Convergence 
0(0.94k) 
0«1 - 1O-4)k) 
0«1 - lO-S)k) 
0(0.89k) 
4.6 
EXPLORING OTHER POINTS 
We have developed, by now, a complete set of tools to characterize learnability 
and sample complexity of memoryless algorithms working on finite parameter 
spaces. We applied these tools to a specific learning problem which corre-
sponded to a point in our 5-dimensional space previously investigated by Gib-
son and Wexler. We also provided an account of how our new analysis revised 
some of their conclusions and had possible applications to linguistic theory. 
Here we now explore some other points in the space. In the next section, we 
consider varying the learning algorithm, while keeping other assumptions about 
the learning problem identical to that before. Later, we vary the distribution 
of the data. 
4.6.1 
Changing the Algorithm 
As one example of the power of this approach, we can compare the convergence 
time of TLA to other algorithms. TLA observes the single value and greediness 
constraints. We consider the following three simple variants by dropping either 
or both of the Single Value and Greediness constraints: 
Random walk with neither greediness nor single value constraints:. 
We have already seen this example before. The learner is in a particular state. 
Upon receiving a new sentence, it remains in that state ifthe sentence is analyz-
able. If not, the learner moves uniformly at random to any of the other states 
and stays there waiting for the next sentence. This is done without regard to 
whether the new state allows the sentence to be analyzed. 

158 
INFORMATIONAL COMPLEXITY OF LEARNING 
C! 
--------------
G) .... 
~ .. ---
_---
i 
.. " 
---.... 
------
w 
""... 
~,..-:;."" 
,..-
.!R 
" 
./':.',. 
~,..",. 
.Q 
,l 
./ 
,. 
I!!= ~ 
/~ 
,./ 
o 
'/ 
, 
o 
/ 
/. 
' 
is 
, 
.I 
,," 
~ 
" / 
,,/ 
111 
' 
! 
, 
2~ 
" / 
,/ 
I: 
" ! 
E 
' ! 
I 
_e 
' i 
" 
" i 
~"'" 
,,:/'/ 
" 
"Elc:i 
, 
l 
:1 
" 
8 
,1 
" 
'i 
'ON:f 
' 
~c:i 
q 
" 
i 
f 
/ 
~ 
/ 
a.. 0 
,.'" 
c:i~--------~------~--------~--------r_------_r~ 
o 
20 
40 
60 
80 
100 
Nurmer of samples 
Figure 4.7. 
Convergence rates for different learning algorithms when Ll is the target 
language. The curve with the slowest rate (large dashes) represents the TLA. The curve 
with the fastest rate (small dashes) is the Random Walk (RWA) with no greediness or 
single value constraints. Random walks with exactly one of the greediness and single value 
constraints have performances in between these two and are very close to each other. 
Random walk with no greediness but with single value constraint:. 
The learner remains in its original state if the new sentence is analyzable. 
Otherwise, the learner chooses one of the parameters uniformly at random and 
flips it thereby moving to an adjacent state in the Markov structure. Again 
this is done without regard to whether the new state allows the sentence to be 
analyzed. However since only one parameter is changed at a time, the learner 
can only move to neighboring states at any given time. 
Random walk with no single value constraint but with greediness:. 
The learner remains in its original state if the new sentence is analyzable. 
Otherwise the learner moves uniformly at random to any of the other states 
and stays there iff the sentence can be analyzed. If the sentence cannot be 
analyzed in the new state the learner remains in its original state. 
Fig. 4.7 shows the convergence times for these three algorithms when Ll 
is the target language. Interestingly, all three perform better than the TLA 
for this task (learning the language L!). More generally, it is found that the 
variants converge faster than the TLA for every target language. Further, 
they do not suffer from local maxima problems. In other words, the class of 

LANGUAGE LEARNING 
159 
languages is not learnable by the TLA, but is by its variants. This is another 
striking consequence of our analysis. The TLA seems to be the "most preferred 
algorithm" by psychologists. The failure of the TLA to learn the 3-parameter 
space was used to argue for maturational theories, alternate parameterizations, 
and parameter ordering. 
In view of the fact that the failure of the TLA can be corrected by fairly 
simple alterationsll , one should examine the conceptual support (from psychol-
ogists) for the TLA more closely before drawing any serious linguistic implica-
tions. This remains yet another example of how the computational perspective 
can allow us to rethink cognitive assumptions. Of course, it may be that the 
TLA has empirical support, in the sense of independent evidence that chil-
dren do use this procedure (given by the pattern of their errors, etc.), but this 
evidence is lacking, as far as we know. 
4.6.2 Distributional Assumptions 
In an earlier section we assumed that the data was uniformly distributed. We 
computed the transition matrix for a particular target language and showed 
that convergence times were of the order of 100-200 samples. In this section 
we show that the convergence times depend crucially upon the distribution. In 
particular we can choose a distribution that will make the convergence time 
as large as we want. Thus the distribution-free convergence time for the 3-
parameter system is infinite. 
As before, we consider the situation where the target language is L 1 . There 
are no local maxima problems for this choice. We begin by letting the distri-
bution be parameterized by the variables a, b, c, d where 
a = P(A={AdvVS}) 
b 
P(B = {Adv V 0 S, Adv Aux V S}) 
c 
P(C = {Adv V 01 02 S, Adv Aux V 0 S, 
Adv Aux V 01 02 S}) 
d 
P(D = {V S}) 
Thus each of the sets A, B, C and D contain different degree-O sentences of L1 . 
Clearly the probability of the set L1 \ {A U B U CUD} is 1 - (a + b + c + d). 
The elements of each defined subset of L1 are equally likely with respect to 
each other. Setting positive values for a, b, c, d such that a + b + c + d < 1 now 
defines a unique probability for each degree(O) sentence in L 1 • For example, 
the probability of (Adv V 0 S) is b/2, the probability of (Adv Aux V 0 S) is 
c/3, that of (V 0 S) is (1 - (a + b + c + d))/6 and so on. 
We can now obtain the transition matrix corresponding to this distribution. 
This is shown in Table 4.1. 
Compare this matrix with that obtained with a uniform distribution on the 
sentences of L1 in the earlier section. This matrix has non-zero elements (tran-
11 Note that we have barely scraped the tip of the iceberg as far as exploring the space of 
possible algorithms is concerned. 

160 
INFORMATIONAL COMPLEXITY OF LEARNING 
Ll 
L2 
L3 
L. 
L5 
L6 
L7 
L8 
Ll 
1 
L2 
l-o-b-c 
2±0±b±c 
3 
3 
L3 
l-o-d 
2±0±d-b 
b 
-3-
~ 
3-Ld 
L. 
c 3 
3 
-3-
L5 
1 
2-0 
0 
3 
-3-
3-Lc 
L6 
~ 
3 
-3-
L7 
l!±!! 
3-20-d 
0 
3 
--3-
3!b 
L8 
b 
3 
-3-
Table 4.1. 
Transition matrix corresponding to a parameterized choice for the distribution 
on the target strings. In this case the target is L1 and the distribution is parameterized 
according to Section 4.7.2 
sition probabilities) exactly where the earlier matrix had non-zero elements. 
However, the value of each transition probability now depends upon a, b, c, and 
d. In particular if we choose a = 1/12, b = 2/12, c = 3/12, d = 1/12 (this is 
equivalent to assuming a uniform distribution) we obtain the appropriate tran-
sition matrix as before. Looking more closely at the general transition matrix, 
we see that the transition probability from state 2 to state 1 is (1-(a+b+c»/3. 
Clearly if we make a arbitrarily close to 1, then this transition probability is 
arbitrarily close to 0 so that the number of samples needed to converge can be 
made arbitrarily large. Thus choosing large values for a and small values for b 
will result in large convergence times. 
This means that the sample complexity cannot be bounded in a distribution-
free sense, because by choosing a highly unfavorable distribution the sample 
complexity can be made as high as possible. For example, we now give the 
convergence curves calculated for different choices of a, b, c, d. We see that for a 
uniform distribution the convergence occurs within 200 samples. By choosing 
a distribution with a = 0.9999 and b = c = d = 0.000001, the convergence time 
can be pushed up to as much as 50 million samples. (Of course, this distribution 
is presumably not psychologically realistic.) For a = 0.99, b = c = d = 0.0001, 
the sample complexity is on the order of 100,000 positive examples. 
4.6.3 Natural Distributions-CHILDES CORPUS 
It is of interest to examine the fidelity of the model using real language distribu-
tions, namely, the CHILDES database. We have carried out preliminary direct 
e~periments using the CHILDES caretaker English input to "Nina" and Ger-
man input to "Katrin" j these consist of 43,612 and 632 sentences each, respec-
tively. We note, following well-known results by psycholinguists, that both cor-
puses contain a much higher percentage of aux-inversion and wh-questions than 
"ordinary" text (e.g., the LOB): 25,890 questions, and 11, 775 wh-questionsj 

C! .... 
i 
I~ 1
0 
'" 
I~ 
E 
,g 
f~ 
8 
'3N 
1;0 
I 
11.0 
" 
0 
0 
LANGUAGE LEARNING 
161 
,--------------;r---
[' 
I 
; 
i 
: 
I 
I 
, 
f 
I 
I 
I 
f 
I 
f 
I 
: 
I 
: 
/ 
i 
I 
I , 
I 
I 
I 
I 
I 
I 
I , 
10 
20 
30 
40 
Log(Number of Samples) 
Figure 4.8. 
Rates of convergence for TLA with Ll as the target language for different 
distributions. The y-axis plots the probability of converging to the target after m samples 
and the x-axis is on a log scale, i.e., it shows log( m) as m varies. The solid line denotes 
the choice of an "unfavorable" distribution characterized by a = 0.9999; b = c = d = 
0.000001. The dotted line denotes the choice of a = 0.99; b = c = d = 0.0001 and the 
dashed line is the convergence curve for a uniform distribution, the same curve as plotted 
in the earlier figure. 

162 
INFORMATIONAL COMPLEXITY OF LEARNING 
201 and 99 in the German corpus; but only 2,506 questions or 3.7% out of 
53,495 LOB sentences. 
To test convergence, an implemented system using a newer version of deMar-
cken's partial parser (see deMarcken, 1990) analyzed each degree-O or degree-1 
sentence as falling into one of the input patterns SVO, S Aux V, etc., as ap-
propriate for the target language. Sentences not parsable into these patterns 
were discarded (presumably "too complex" in some sense following a tradition 
established by many other researchers; see Wexler and Culicover (1980) for 
details). Some examples of caretaker inputs follow: 
this is a book ? what do you see in the book ? 
how many rabbits? 
what is the rabbit doing? ( ... ) 
is he hopping ? oh . and what is he playing with ? 
red mir doch nicht alles nach ! 
ja , die schwatzen auch immer alles nach ( ... ) 
When run through the TLA, we discover that convergence falls roughly 
along the TLA convergence time displayed in figure 4.6-roughly 100 examples 
to asymptote. Thus, the feasibility of the basic model is confirmed by actual 
caretaker input, at least in this simple case, for both English and German. We 
are continuing to explore this model with other languages and distributional 
assumptions. However, there is one very important new complication that must 
be taken into account: we have found that one must (obviously) add patterns 
to cover the predominance of auxiliary inversions and wh-questions. However, 
that largely begs the question of whether the language is verb-second or not. 
Thus, as far as we can tell, we have not yet arrived at a satisfactory parameter-
setting account for V2 acquisition. 
4.7 
BATCH LEARNING UPPER AND LOWER BOUNDS: AN ASIDE 
So far we have discussed a memory less learner moving from state to state 
in parameter space and hopefully converging to the correct target in finite 
time. As we saw this was well-modeled by our Markov formulation. In this 
section however we step back and consider upper and lower bounds for learning 
finite language families if the learner was allowed to remember all the strings 
encountered and optimize over them. Needless to say this might not be a 
psychologically plausible assumption, but it can shed light on the information-
theoretic complexity of the learning problem. 
Consider a situation where there are n languages L 1 , L 2 , .•• Ln over an al-
phabet E. Each language can be represented as a subset of E* i.e. 

LANGUAGE LEARNING 
163 
The learner is provided with positive data (strings that belong to the language) 
drawn according to distribution P on the strings of a particular target language. 
The learner is to identify the target. It is quite possible that the learner receives 
strings that are in more than one language. In such a case the learner will not 
be able to uniquely identify the target. However, as more and more data 
becomes available, the probability of having received only ambigious strings 
becomes smaller and smaller and eventually the learner will be able to identify 
the target uniquely. An interesting question to ask then is how many samples 
does the learner need to see so that with high confidence it is able to identify the 
target, i.e. the probability that after seeing that many samples, the learner is 
still ambigious about the target is less than c. The following theorem provides 
a lower bound. 
Theorem 4.3 The learner needs to draw at least M = maxj;tt In(lip;) In(l/c) 
samples (where Pj = P( Lt n Lj)) in order to be able to identify the target with 
confidence greater than 1 - c. 
Proof 
Suppose the learner draws m (less than M) samples. 
Let k = 
argmaxj;ttpj. This means 1) M = In(l/Pk)ln(l/c) and 2) that with proba-
bility Pk the learner receives a string which is in both Lk and Lt. Hence it 
will be unable to discriminate between the target and the kth language. After 
drawing m samples, the probability that all of them belong to the set Lt n Lk 
is (Pk)m. In such a case even after seeing m samples, the learner will be in 
an ambiguous state. Now (Pk)m > (Pk)M since m < M and Pk < 1. Finally 
since Mln(l/Pk) = In«(1/Pk)M) = In(l/c), we see that (Pk)m > c. Thus the 
probability of being ambiguous after m examples is greater than C which means 
that the confidence of being able to identify the target is less than 1 - c. I 
This simple result allows us to assess the number of samples we need to 
draw in order to be confident of correctly identifying the target. Note that 
if the distribution of the data is very unfavorable, that is, the probability of 
receiving ambiguous strings is quite high, then the number of samples needed 
can actually be quite large. While the previous theorem provides the number 
of samples necessary to identify the target, the following theorem provides 
an upper bound for the number of samples that are sufficient to guarantee 
identification with high confidence. 
Theorem 4.4 If the learner draws more than M = In(l/CLbi» In(l/c) sam-
ples, then it will identify the target with confidence greater than 1 - c. ( Here 
bt = peLt \ Uj;ttLj)). 
Proof 
Consider the set L = Lt \ Uj;ttLj. Any element of this set is present 
in the target language Lt but not in any other language. Consequently upon 
receiving such a string, the learner will be able to instantly identify the target. 
After m > M samples, the probability that the learner has not received any 
member of this set is (1 - p(L))m = (1 - bt)m < (1 - bt)M = c. Hence the 
probability of seeing some member of L in those m samples is greater than 
1 - c. But seeing such a member enables the learner to identify the target so 

164 
INFORMATIONAL COMPLEXITY OF LEARNING 
the probability that the learner is able to identify the target is greater than 
1 -
{) if it draws more than M samples. I 
To summarize, this section provides a simple upper and lower bound on the 
sample complexity of exact identification of the target language from positive 
data. The {) parameter that measures the confidence of the learner of being 
able to identify the target is suggestive of a PAC (Valiant, 1984) formulation. 
However there is a crucial difference. In the PAC formulation, one is interested 
in an {-approximation to the target language with at least 1 -
{) confidence. 
In our case, this is not so. Since we are not allowed to approximate the tar-
get, the sample complexity shoots up with choice of unfavorable distributions. 
There are some interesting directions one could follow within this batch learn-
ing framework. One could try to get true PAC-style distribution-free bounds 
for various kinds of language families. Alternatively one could use the ex-
act identification results here for linguistically plausible language families with 
"reasonable" probability distributions on the data. It might be an interesting 
exercise to recompute the bounds for cases where the learner receives both pos-
itive and negative data. Finally the bounds obtained here could be sharpened 
further. We intend to look into some of these questions in the future. 
4.8 
CONCLUSIONS, OPEN QUESTIONS, AND FUTURE DIRECTIONS 
The problem of learning parameterized families of grammars has several dif-
ferent dimensions as we have emphasized earlier. One needs to investigate the 
learnability for a variety of algorithms, distributional assumptions, parameter-
izations, and so on. In this chapter, we have emphasized that it is not enough 
to merely check for learnability in the limit (as previous research within an in-
ductive inference Gold framework has tended to do; see, for example, Osherson 
and Weinstein, 1986); one also needs to quantify the sample complexity of the 
learning problem, i.e., how many examples does the learning algorithm need to 
see in order to be able to identify the target grammar with high confidence. To 
illustrate the importance of this, we re-analyzed a particular learning problem 
previously studied by Gibson and Wexler. 
Our reanalysis, shows that on finite parameter spaces, the Triggering Learn-
ing Algorithm in particular, and memoryless algorithms in general, can be 
completely modeled by a Markov process. This Markov model then allows us 
to check for learn ability in a very simple fashion, rather than the more com-
plicated procedures previously used in the linguistics community. Further, it 
also allows us to characterize the sample complexity of learning with such algo-
rithms. On studying the performance of the TLA on the specific 3-parameter 
subspace from this perspective, we found several new results. First, the exis-
tence of new problematic initial hypotheses was discovered-leading to revisions 
of certain aspects of maturation and parameter ordering suggested by Gibson 
and Wexler. Second, we showed that the existence of local triggers (in other 
words, a triggered path from the initial hypothesis to the target) is not suffi-
cient to guarantee learn ability. Third, we found that the TLA was suboptimal; 

LANGUAGE LEARNING 
165 
for example the random walk algorithm on this space had no local maxima and 
converged faster. 
This analysis on a simple, previously studied, example demonstrates the 
usefulness of our perspective. It should be reiterated that any finite parameter-
ization, and a class of memory less algorithms can be studied by this approach. 
There are several important questions which need to be pursued further. For 
example, one could turn to other natural parametric systems suggested (the ex-
ample of metrical phonology given in this chapter, a variant studied by Dresher 
and Kaye (1990), a parameterization chosen by Clark and Roberts (1993)) and 
so on. One could then establish the complexity of learning these other para-
metric schemes, possibly with useful results again. 
Another crucial direction relates to the learning algorithm used. What hap-
pens when the learner is allowed the use of memory? An interesting investi-
gation of this issue has been done by Kapur (1992). However some questions 
remain unresolved. For example, is it true that any algorithm with a finite 
memory size (n examples, say) can be modeled as a finite order Markov chain 
(presumably, the order would be related to n, in some sense)? Is this a useful 
way to characterize such algorithms? A complete characterization of human 
language requires us to describe the linguistic knowledge (equivalent to param-
eterization), and the algorithm children use to acquire this knowledge. Insights 
about the kinds of algorithms available, and their psychological feasibility, could 
often direct the search for the right kind of linguistic knowledge. 
It is also of interest to study the relationship between the expressive power 
of the parameterized family of grammars and the number of parameters. One 
needs to reiterate, here, the importance of our point of view in this book. Recall 
how in Chapter 2, we investigated regularization networks from an approxima-
tion and estimation point of view. Grammars, are no different from regulariza-
tion networks in this sense. Thus, one could pose the following general problem. 
Assume a class of grammars G as the concept class, and a parameterized class 
of grammars H n as the hypothesis class. Now, for a target grammar 9 E G, how 
many example sentences need to be drawn, and how large must the number of 
parameters, n, be, so that the learner's hypothesis will be close to the target 
with high confidence? 
Yet another issue has to do with the "smoothness" relation between the pa-
rameter settings and the resulting surface strings. In principles-and-parameters 
theory, it has often been suggested that a small parameter change could lead to 
a large deductive change in the grammar, hence a large change in the surface 
language generated. In all the examples considered so far there is a smooth 
relation between surface sentences and parameters, in that switching from a 
V2 to a non-V2 system, for instance, leads us to a Markov state that is not 
too far away from the previous one. If this is not so, it is not so clear that 
the TLA will work as before. In fact, the whole question of how to formulate 
the notion of "smoothness" in a language-grammar framework is unclear. We 
know in the case of continuous functions, as discussed in Chapter 3, that if 
the learner is allowed to choose examples (which can be simulated by selective 
attention), then such an "active" learner can approximate such functions much 

166 
INFORMATIONAL COMPLEXITY OF LEARNING 
more quickly than a "passive" learner, like the one presented in GW. Is there 
an analog to this in the discrete, digital domain oflanguage? Further, how can 
one approximate a language? Here too mathematics may playa helpful role. 
Recall that there is an analog to a functional analysis of languages-namely, the 
algebraic approach advanced by Chomsky and Schutzenberger (1963). In this 
model, a language is described by an (infinite) polynomial generating function, 
where the coefficients on the polynomial term x gives the number of ways of 
deriving the string x. A (weak, string) approximation to a language can then 
be defined in terms of an approximation to the generating function. If this 
method can be deployed, then one might be able to carryover the results of 
functional analysis and approximation for active vs. passive learners into the 
"digital" domain of language. If this is possible, we would then have a very 
powerful set of previously underutilized mathematical tools to analyze language 
learnability. 

LANGUAGE LEARNING 
167 
Appendix 
4-A 
UNEMBEDDED SENTENCES FOR PARAMETRIC GRAMMARS 
The following table provides the unembedded (degree-O) sentences from each of 
the 8 grammars (languages) obtained by setting the 3 parameters of example 1 
to different values. The languages are referred to as L1 through L8. 
J.anauaIe 
Spec C ... p V2 
u.p.o l1IIIII!bedded smdeDcu 
1., 
1 
1 
0 
"v." "v 0 8" BV 01 02 8" M AU V 8" ".A.UX V 0 a" 
"A,1IX V 01 02 I" n ADV V I" IIt.AIJV v 0 In ".A.DV V 01 02 ." 
• AllV AlIl[ V S· • A.DV AlIl[ V 0 S· "AllV AlIX vol 02 S· 
Lt 
1 
1 
1 
·s v· "s V 0" "0 V s" "s V 01 02" 
·01 V 02 s" "02 V 01 rI' ·S AUl[ v" "s AlIl[ V o· 
"0 4'DX V ." "s 4'DX V 01 02" "01 4'DX V 02 ." "02 411l[ V 01 s" 
n ADV V s" N ADV V 0 sit "AllV V 01 02 aft "ADV AUK V 8" 
II JtDV AUX V 0 8" "1e.DV A:UX v 01 02 a" 
bo 
1 
0 
0 
MV s· "'0 V 8" "0201 v gtt "v A,1Jxs" "0 V AlIX a" 
"0201 v AU In ".I\llV V a" ".ADV 0 v." ".A.DV 02 01 v In 
-AllV V AUXS" "'A.DY 0 V AUXS· "AllV 02 01 V AUX an 
L,. 
1 
0 
1 
tIs yH "0 V s" "s YO" "s v 02 01" "01 v 02 an 
"02 V ols" ". AUX v" "I "''OX 0 v" "0 AU V I" 
"s 411l[ 02 01 v" ·01 411X 02 V s· ·02 4'DX 01 V s" "ADV V rI' 
"A.DVV 0 ,n "ADV V 02 olsl1 "ADV AU V'" 
• ADV 411XO V S· ·4DV 4U 02 01 V s· 
L. 
0 
1 
0 
SV 
IVO 
IV 0102 
14l1X Y 
S4l1X VO 
(~j 
"9 A'UX vol 02" ".ADV I v" " .AllVS V 0" "A.DV S vol 02" 
"AllV S 411l[ v" "ADV 8 411X V O· "AllV S 411l[ V 0102" 
J.,. 
0 
1 
1 
IV 
IVO' ova 
11 VOl 02 
01 v. 02 
"02 v sol" liS AUX v" Its 4UX V 0'" "0 .A.'UX S v" 
"s 411X V 0102" "01411X. V 02" ·02 411X. vOl" "ADV V rI' 
II A,DV V & o· n IUJV V II 01 02" ., A1JV AUX 8 v" "A.DV AUX 8 V 0" 
»A.DV 4l1X S V 01 02" 
b, 
0 
0 
0 
-II v" "II 0 v'" "'1 02 01 v" "s v AUX'" 
(""=~ 
".0201 v AVX" "ADVI v" "ADVS 0 v" -ADVI 02 01 v" 
• AllV a V 4l1X" • ADV & 0 V 411X· "AllV II 02 01 V 411X· 
J.,. 
0 
0 
1 
,.,. v" "s V 0" "0 V 8~~ "8 vo2 01" "01 v 8 02" 
(O.mall, 
"02 V sol" ", 411X V· 's 411X 0 v" "0 4'DX. yo 
DakIa) 
"01 AUX 802 v· '02 411X S 01 yO "4DV V s" "AllV V S o· 
"AllV V 8 0201" "4DV 411l[. v" "AllV 411l[8 0 v· 
·s 4l1X 0201 v· "80 V 4UX· "A.DV A1IX 80201 v" 
4-B 
MEMORYLESS ALGORITHMS AND MARKOV CHAINS 
Memoryless algorithms can be regarded as those which have no recollection 
of previous data, or previous inferences made about the target function. At 
any point in time, the only information upon which such an algorithm acts is 
the current data, and the current hypothesis (state). A memoryless algorithm 
can then be regarded as an effective procedure mapping this information to a 
new hypothesis. In general, given a particular hypothesis state (h in 11., the 
hypothesis space), and a new datum (sentence, s in E*), such a memoryless 
algorithm will map onto a new hypothesis (g E 11.). Of course , g could be 
the same as h or it could be different depending upon the specifics of the 

168 
INFORMATIONAL COMPLEXITY OF LEARNING 
algorithm and the datum. If one includes the possibility of randomization, 
then the mapping need not be deterministic. In other words, given a state h, 
and sentence s, the algorithm maps onto a distribution P1t over the hypothesis 
space, according to which the new state is selected. Clearly, 
L P1t(h) = 1 
hE1t 
Let P be the set of all possible probability distributions over the (finite) 
hypothesis space. For any P1t E P, thus, P1t[h] is the probability measure on 
the hypothesis (state) h. 
A memory less algorithm can then be regarded as a computable function (I) 
from (1i, E*) to P as follows: 
Thus, for any h E 1i, and s E E*, the quantity f(h, s) is a distribution 
over the hypothesis space according to which the learner would pick the next 
hypothesis. Consequently, a learner following such an algorithm, would update 
its hypothesis with each new sentence, and move from state to state in our 
finite parameter space of hypotheses. Suppose, at a point in time, the learner 
is in a state hI. What is the probability that it will move to state h2 after the 
next example? It will do so only if the following two conditions are met. First, 
it receives a sentence (example), s, for which f(hl' s) has a non-zero proba-
bility measure on the state h2 . Let this probability measure be f(hl' s)[h2 ]. 
Second, given the probability over the hypothesis space according to which it 
chooses the next hypothesis, the learner actually ends up choosing h2 as the 
next hypothesis. 
Given a distribution P on E*, according to which sentences are drawn, and 
presented to the learner, the transition probability from hI to h2 is now given 
by: 
{.Ij(h". )[h2J>O} 
Having obtained the transition probabilities, it is clear that the memory less 
algorithm is a Markov chain. 
4-C 
PROOF OF LEARNABILITY THEOREM 
To establish the theorem, we recall three additional standard terms associated 
with the Markov chain states: (1) equivalent states; (2) recurrent states; and 
(3) transient states. We then present another standard result about the form of 
any Markov chain: its canonical decomposition in terms of closed, equivalent, 
recurrent, and transient states. 
4-C.1 
Markov state terminology 
Definition 8 Given a Markov chain M, and any pair of states s, t E M, we 
say that s is equivalent to t if and only if s is reachable from t and t is 

LANGUAGE LEARNING 
169 
reachable from s, where by reachable we mean that there is a path from one 
state to another. 
Two states sand t are equivalent if and only if there is a path from s to t 
and a path from t to s. Using the equivalence relation defined above, we can 
divide any M into equivalence classes of states. All the states in one class are 
reachable (from and to) the states in that class. 
Definition 9 Given a Markov chain M, a state s E M is recurrent if the 
chain returns to s in a finite number of steps with probability 1. 
Definition 10 Given a Markov chain M, and a state s E M, if s is not 
recurrent, then s is transient. 
We will need later the following simple property about transient states: 
Lemma 4-C.1 Given a Markov chain M, ift is a transient state of M, then, 
for any state s E M 
lim p,t(n) = 0 
n--+oo 
where p,t( n) denotes the probability of going from state s to state t in exactly 
n steps. 
Proof Sketch: Proposition 2.6.3 (page 88) of Resnick (1992) states that 
Therefore, LPst(n) is a convergent series. Thus p,t(n)n--+oo -+ 0.1 
4-C.2 Canonical Decomposition 
A particular Markov chain might have many closed states (Definition 3 of text), 
and these need not be disjoint; they might also be subsets of each other. How-
ever, even though there can be many closed states in a particular Markov chain, 
the following standard result shows that there is a canonical decomposition of 
the chain (Lemma 4-C.2) that will be useful to us in proving the learnability 
theorem. 
Lemma 4-C.2 Given a Markov chain M, we may decompose M into disjoint 
sets of states as follows: 
M = TUC1 UC2 •.. 
where (i) T is a collection of transient states and (ii) the Ci 's are closed, 
equivalence classes of recurrent states. 
Proof Sketch: This is a standard Markov chain result; see Corollary 2.10.2 
of page 99 of Resnick (1992).1 We can now proceed to a proof of the main 
learnability theorem. 

170 
INFORMATIONAL COMPLEXITY OF LEARNING 
4-0 
FORMAL PROOF 
::::}. We need to show that if the target grammar is learnable, then every closed 
set in the chain must contain the target state. By assumption, target grammar 
gJ is learnable. Now assume for sake of contradiction that there is some closed 
set C that does not include the target state associated with the target grammar. 
If the learner starts in some sEC, by the definition of a closed set of states, it 
can never reach the target state. This contradicts the assumption that gJ was 
learnable. 
{=: Assume that every closed set of the Markov chain associated with the 
learning system includes the target state. We now need to show that the target 
grammar is Gold-learnable. First, we make use of some properties of the target 
state in conjunction with the canonical decomposition of Lemma 4-C.2 to show 
that every non-target state must be transient. Then we make use of Lemma 4-
C.1 about transient states to show that the learner must converge to the target 
grammar in the limit with probability l. 
First, note the following properties of the target state: 
(i) by construction, the target state is an absorbing state, i.e., no other state 
is reachable from the target state; 
(ii) therefore, no other state can be in an equivalence relation with the target 
state and the target state is in an equivalence class by itself; 
(iii) the target state is recurrent since the chain returns to it with probability 
1 in one step (the target state is an absorbing state). 
These facts about the target state show that the target state consititutes 
a closed class (say Ci) in the canonical decomposition of M. However, there 
cannot be any other closed class Cj , j #- i in the canonical decomposition of 
M. This is because by the definition of the canonical decomposition any other 
such Cj must be disjoint from Ci, and by the hypothesis of the theorem, such 
Cj must contain the target state, leading to a contradiction. Therefore, by the 
canonical decomposition lemma, every other state in M must belong to T, and 
must therefore be a transient state. 
Now denote the target state by sJ. The canonical decomposition of M must 
therefore be in the form: 
TU{sJ}. 
Without loss of generality, let the learner start at some arbitrary state s. 
After any integer number n of positive examples, we know that, 
E P8t(n) = 1 
tEM 
because the learner has to be in one of the states of the chain M after n 
examples with probability 1. But by the decomposition lemma and our previous 
arguments M = T U sJ. Therefore we can rewrite this sum as two parts, one 
corresponding to the transient states and the other corresponding to the final 
state: 

LANGUAGE LEARNING 
171 
LP6t(n) + pu/(n) = 1 
teT 
Now take the limit as n goes to infinity. By the transient state lemma, 
every P6t(n) goes to zero for t E T. There are only a finite (known) number of 
states in T. Therefore, LteTP6t(n) goes to zero. Consequently, Pu/ goes to l. 
But that means that the learner converges to the target state in the limit (with 
probability 1). Since this is true irrespective of the starting state of the learner, 
the learner converges to the target with probability 1, and the associated target 
grammar g, is Gold-learnable. I 

5 THE LOGICAL PROBLEM OF 
LANGUAGE CHANGE 
In this chapter, we consider the problem of language change. Linguists have to explain not 
only how languages are learned (a problem we investigated in the previous chapter), but 
also how and why they have evolved in certain trajectories. While the language learning 
problem has concentrated on the behavior of the individual child, and how it acquires a 
particular grammar (from a class of grammars a), we consider, in this chapter, a population 
of such child learners, and investigate the emergent, global, population characteristics of the 
linguistic community over several generations. We argue that language change is the logical 
consequence of specific assumptions about grammatical theories, and learning paradigms. In 
particular, we are able to transform the parameterized theories, and memoryless algorithms 
of the previous chapter into grammatical dynamical systems, whose evolution depicts the 
evolving linguistic composition of the population. We investigate the linguistic, and compu-
tational consequences of this fact. From a more programmatic perspective, we lay a possible 
logical framework for the scientific study of historical linguistics, and introduce thereby, a 
formal diachronic criterion for adequacy of linguistic theories. 
5.1 
INTRODUCTION 
As is well known, languages change over time. Language scientists have long 
been occupied with describing language changes in phonology, syntax, and 
semantics. There have been many descriptive and a few explanatory accounts of 
language change, including some explicit computational models. Many authors 
appeal naturally to the analogy between language change and another familiar 
model of change, namely, biological evolution. There is also a notion that 
173 

174 
INFORMATIONAL COMPLEXITY OF LEARNING 
language systems are adaptive (dynamical) ones. For instance, Lightfoot (1991, 
chapter 7, pages 163-651£.) talks about language change in this way: 
Some general properties of language change are shared by other dynamic sys-
tems in the natural world ... 
Indeed, entire books have been devoted to the description of language change 
using the terminology of population biology: genetic drift, clines, etc. How-
ever, these analogies have rarely been pursued beyond casual and descriptive 
accounts. l In this chapter we would like to formalize these linguists' intuitive 
notions in a specific way as a concrete computational model, and investigate 
the consequences of this formalization. In particular, we show that a model of 
language change emerges as a logical consequence of language learnability, a 
point made by Lightfoot (1991). We shall see that Lightfoot's intuition that 
languages could behave just as though they were dynamical systems is essen-
tially correct, and we can provide concrete examples of both "gradual" and 
"sudden" syntactic changes occuring over time periods of many generations to 
just a single generation.2 
Not surprisingly, many other interesting points emerge from the formaliza-
tion, some programmatic in nature: 
• We provide a general procedure for deriving a dynamical systems model from 
grammatical theories and learning paradigms. 
• Learnability is a well-known criterion for testing the adequacy of grammatical 
theories. With our new model, we can now give an evolutionary criterion. 
By this we mean that by comparing the evolutionary trajectories of derived 
dynamical linguistic systems to historically observed trajectories, one can 
determine the adequacy of linguistic theories or learning algorithms. 
• We explicitly derive dynamical systems corresponding to parameterized lin-
guistic theories (e.g. Head First/Final parameter in HPSG or GB grammars) 
and memory less language learning algorithms (e.g. gradient ascent in pa-
rameter space). 
• Concretely, we illustrate the use of dynamical systems as a research tool 
by considering the loss of Verb Second position in Old French as compared 
to Modern French. We demonstrate that, when mathematically modeled 
by our system, one grammatical parameterization in the literature does not 
seem to permit this historical change, while another does. We are also able 
to more accurately model the time course of language change. In particu-
lar, in contrast to Kroch (1989) and others, who mimic population biology 
models by imposing an S-shaped logistic change by assumption, we show 
that the time course of language change need not be S-shaped. Rather, 
1 Some notable exceptions are Kroch (1990), Clark and Roberts (1993). 
2Lightfoot 1991 refers to these sudden changes, acting over 1 generation, as "catastrophic" 
but in fact this term usually has a different sense in the dynamical systems literature. 

LANGUAGE CHANGE 
175 
language-change envelopes are derivable from more fundamental properties 
of dynamical systems; sometimes they are S-shaped, but they can also have 
a non monotonic shape, or even non-smooth, "catastrophic" properties. 
• We formally examine the "diachronic envelopes" possible under varying 
conditions of alternative language distributions, language acquisition al-
gorithms, parameterizations, input noise, and sentence distributions-that 
is, what language changes are possible by varying these dimensions. This 
involves the simulation of these dynamical systems under different initial 
conditions, and characterizations of the resulting evolutionary trajectories, 
phase-space plots, issues of stability, and the like. 
• The formal diachronic model as a dynamical system provides a novel possible 
source for explaining several linguistic changes including (a) the evolution of 
modern Greek phonology from proto-Indo-European (b) Bickerton's (1990) 
creole hypothesis (concerning the striking fact that all creoles, irrespective 
of linguistic origin, have exactly the same grammar) as the condensation 
point of a dynamical system (though we have not tested these possibilities 
explicitly). 
The Acquisition-Based Model of Language Change: The Logical Problem 
of Language Change 
How does the combination of a grammatical theory and learning algorithm 
lead to a model of language change? We first note that, just as with language 
acquisition, there is a seeming paradox in language change: it is generally as-
sumed that children acquired their caretaker (target) grammars without error. 
However, if this were always true, at first glance grammatical changes within a 
population could seemingly never occur, since generation after generation, the 
children would have successfully acquired the grammar of their parents. 
Of course, Lightfoot and others have pointed out the obvious solution to this 
paradox: the possibility of slight misconvergence to target grammars could, 
over time (generations), drive language change, much as speciation occurs in 
the population biology sense. We pursue this point in detail below. Similarly, 
just as in the biological case, some of the most commonly observed changes in 
languages seem to occur as the result of the effects of surrounding populations, 
whose features infiltrate the original language. 
We begin our treatment of this subject by arguing that the problem of 
language acquisition at the individual level leads logically to the problem of 
language change at the group (or population) level. Consider a population 
speaking a particular language3 . This is the target language-children are ex-
posed to primary linguistic data from this source (language); typically in the 
form of sentences uttered by caretakers (adults). The logical problem of lan-
guage acquisition is how children acquire this target language from the primary 
3In our framework of analysis, this implies that all the adult members of this population have 
internalized the same grammar (corresponding to the language they speak). 

176 
INFORMATIONAL COMPLEXITY OF LEARNING 
linguistic data-in other words to come up with an adequate learning theory. 
Such a learning algorithm is simply a mapping from primary linguistic data to 
the class of grammars. For example, in a typical inductive inference model (as 
we saw in the previous chapter), given a stream of sentences (primary linguis-
tic data), the algorithm would simply update its grammatical hypothesis with 
each new sentence according to some preprogrammed procedure. An important 
criterion for learn ability (as we saw in the previous chapter) is to require that 
the algorithm converge to the target as the data goes to infinity. 
Now, suppose that the primary linguistic data presented to the child is 
altered (due, perhaps, to presence of foreign speakers, contact with another 
population, disfluencies etc.). In other words, the sentences presented to the 
learner (child) are no longer consistent with a single target grammar. In the 
face of this input, the learning algorithm might no longer converge to the tar-
get grammar. Indeed, it might converge to some other grammar (g2); or it 
might converge to g2 with some probability, g3 with some other probability, 
and so on. In either case, children attempting to solve the acquisition problem 
by means of the learning algorithm, would have internalized grammars differ-
ent from the parental (target) grammar. Consequently, in one generation, the 
linguistic composition of the population would have changed4 . Furthermore, 
this change is driven by 1) the primary linguistic data (composed in this case 
of sentences from the original target language, and sentences from the foreign 
speakers) 2) the language acquisition device: which acting upon the primary 
evidence, causes the acquisition of a different grammar by the children. Finally, 
the change is limited by the hypothesis space of possible grammars; after all, 
the children can never converge to a grammar which lies outside this space of 
grammars. 
In short, on this view, language change is a logical consequence of specific 
assumptions about 
1. the hypothesis space of grammars-in a parametric theory, like the ones we 
examine in this book, this corresponds to a particular choice of parameteri-
zation 
2. the language acquisition device-in other words, the learning algorithm the 
child uses to develop hypotheses on the basis of data 
3. the primary linguistic data-the sentences which are presented to the chil-
dren of anyone generation 
If we specify 1) through 3) for a particular generation, we should, in principle, 
be able to compute the linguistic composition for the next generation. In this 
manner, we can compute the evolving linguistic composition of the population 
from generation to generation; we arrive at a dynamical system. We can be a bit 
4Sociological factors affecting language change, affect language acquisition in exactly the 
same way, yet are abstracted away from the formalization of the logical problem of language 
acquisition. In this same sense, we similarly abstract away such causes. 

LANGUAGE CHANGE 
177 
more precise about this. First, let us recall our framework for language learning. 
Then we will show how to derive a dynamical system from this framework. 
The Language Learning Framework: 
Denote by g, a family of possible (target) grammars. Each grammar 0 E g 
defines a language L(O) ~ E* over some alphabet E in the usual fashion. Let 
there be a distribution P on E* according to which sentences are drawn and 
presented to the learner. Note that if there is well defined target, Ot, and 
only positive examples from this target are presented to the learner, then P 
will have all its measure on L(Ot), and zero measure on sentences outside of 
this. Suppose n examples are drawn in this fashion, one can then let 1)n = 
(E*)n be the set of all n-example data sets the learner might potentially be 
presented with. A learning algorithm A can then be regarded as a mapping 
from 1)n to g. Thus, acting upon a particular presentation sequence dn E 1)n, 
the learner posits a hypothesis A(dn ) = hn E g. Allowing for the possibility of 
randomization, the learner could, in general, posit hi E g with probability Pi for 
such a presentation sequence dn . The standard (stochastic version) learnability 
criterion (after Gold, 1967) can then be stated as follows: 
For every target grammar, Ot E g, with positive-only examples presented ac-
cording to P as above, the learner must converge to the target with probability 
1, i.e., 
In the previous chapter, we concerned ourselves with this learnability issue 
for memory less algorithms in finite parameter spaces. 
From Language Learning to Population Dynamics: 
The framework for language learning has learners (children) attempting to in-
fer grammars on the basis of linguistic data (sentences). At any point in time, 
n, (i.e., after hearing n examples) the child learner has a current hypothesis, 
h, with probability Pn(h). What happens when there is a population of child 
learners? Since an arbitrary child learner, has a probability Pn(h) of developing 
hypothesis h (for every h E g), it follows that a fraction Pn (h) of the population 
of children would have internalized the grammar h after n examples. We there-
fore have a current state of the population after n examples. This state of the 
population (of children) might well be different from the state of the parental 
population. Pretend for a moment that after n examples, maturation occurs, 
i.e., the child retains for the rest of its life, the grammatical hypothesis after 
n examples, then we would have arrived at the state of the mature population 
for the next generation5 • This new generation now produces sentences for the 
5 Maturation is a reasonable hypothesis. After all, it seems even more unreasonable to imagine 
that children are forever wandering around in hypothesis space. After a certain point, and 
there is evidence from developmental psychology to suggest that this is the case, the child 
matures and retains its current grammatical hypothesis for the rest of its life. 

178 
INFORMATIONAL COMPLEXITY OF LEARNING 
following generation of children according to the distribution of grammars in 
the population. The same process repeats itself and the linguistic composition 
of the population evolves from generation to generation. 
Formalizing the Argument Further: 
This formulation leads naturally to a discrete-time dynamical systems model 
for language change. In order to define such a dynamical system formally, one 
needs to specify 
1. the state space, S- a set of states the system can be in. At any given point 
in time, t, the system is in exactly one state s E S; 
2. an update rule defining, the manner in which the state of the system changes 
from one time to the next. Typically, this involves the specification of a 
function, I, which maps St, (the state at time t) to St+! (the state at time 
t + 1).6 
For example, a typical linear dynamical system might consist of state vari-
ables x (where x is a k-dimensional state vector) and a system of differential 
equations x' = Ax (A is a matrix operator) which characterize the evolution 
of the states with time. RC circuits are a simple example of linear dynami-
cal systems. The state (current) evolves as the capacitor discharges through 
the resistor. Population growth models (for example, using logistic equations) 
provide other examples. 
The State Space: 
In our case, the state space is the space of possible linguistic compositions 
of the population. More specifically, it is a distribution Ppop on the space 
of grammars, g7. For example, consider the three parameter syntactic space 
described in Gibson and Wexler (1994) and analyzed in the previous chapter. 
This defines 8 possible "natural" grammars. Thus g has 8 elements. We can 
picture a distribution on this space as shown in fig. 5.1. In this particular case, 
the state space is 
8 
S = {P E R812:Pi = I} 
i=l 
We interpret the state as the linguistic composition of the population. For 
example, a distribution which puts all its weight on grammar 91 and 0 ev-
erywhere else, indicates a homogeneous population which speaks the language 
corresponding to grammar 91. Similarly, a distribution which puts a probability 
mass of 1/2 on 91 and 1/2 on 92 indicates a population (non-homogeneous) with 
half its speakers speaking a language corresponding to 91 and half speaking a 
language corresponding to 92. 
SIn general, this mapping could be fairly complicated. For example, it could depend on 
previous states, future states etc. For reference, see Strogatz (1993). 
7 Obviously one needs to be able to define a u-algebra on the space of grammars, and so on. 
For the cases we look at, this is not a problem because the set of grammars is finite. 

LANGUAGE CHANGE 
179 
R 
G 
Filure 5.1. 
A simple illustration of the state space for the 3-parameter syntactic case. 
There are 8 grammars, a probability distribution on these 8 grammars, as shown above, 
can be interpreted as the linguistic composition of the population. Thus, a fraction Pl of 
the population have internalized grammar, gl, and so on. 
The Update Rule: The update rule is obtained by considering the learning 
algorithm, A, involved. For example, given the state at time t, (Ppop,t), i.e., 
the distribution of speakers in the parental population (they are the generators 
of the primary linguistic data for the next generation), one can obtain the 
distribution with which sentences from E* will be presented to the learner. 
To do this, imagine that the ith linguistic group in the population (speaking 
language Li) produces sentences with distribution Pi (on the sentences of Li, 
i.e., sentences not in L, are produced with probability 0). Then for any w E E*, 
the probability with which it is presented to the learner is given by 
pew) = E Pi(w)Ppop,t(i) 
i 
Now that the distribution with which sentences are presented to the learner 
is determined, the algorithm operates on the linguistic data, dn , (this is a 
dataset of n example sentences drawn according to distribution P) and develops 
hypotheses (A(dn ) E Q). Furthermore, one can, in principle, compute the 
probability with which the learner will develop hypothesis hi after n examples: 
(5.1) 
This finite sample situation is always well defined. In other words, the proba-
bility Pn exists8 . 
Learnability requires Pn(gt) to go to 1, for the unique target grammar, gt, if 
such a grammar exists. In general, however, there is no unique target grammar 
8 This is easy to see for deterministic algorithms, Adet. Such an algorithm would have a precise 
behavior for every data set of n examples drawn. In our case, the examples are drawn in i.i.d. 
fashion according to a distribution P on I:*. It is clear that Pn(hi) = P[{dnIAdet(dn ) = hi}]. 
For randomized algorithms, the case is trickier, but the probability still exists. We saw in 
the previous chapter, how to compute Pn for randomized memoryless algorithms. 

180 
INFORMATIONAL COMPLEXITY OF LEARNING 
since we have non-homogeneous linguistic populations. However, the following 
limiting behavior might still exist: 
Limiting Sample: lim Prob[A(dn ) = hi] = Pi 
(5.2) 
n_oo 
Thus, the child, according to the arguments described earlier, internalizes 
grammar hi E g with probability Pn(hd (for a finite sample analysis) and with 
probability Pi "in the limit". We can find Pi for every i, and the next generation 
would then have a proportion Pi (or Pn(hi), if one wanted to do a finite sample 
analysis) of people who have internalized the grammar hi. Consequently, the 
linguistic composition of the next generation is given by Ppop,t+l(hd = Pi(or 
Pn(hi)). In this fashion, 
Remarks 
1. The finite sample case probability always exists. Suppose, we have solved 
the maturation problem, i.e., we know the rough amount of time, the learner 
takes to develop its mature (adult) hypothesis. This is tantamount to knowing 
(roughly, if not exactly) the number of examples, N, the child would have 
heard by then. In that case pN(h) is the probability that the child internalizes 
the grammar h. This (PN(h)) is the percentage of speakers of Lh in the next 
generation. Note that under this finite sample analysis, for a homogeneous 
population, with all adults speaking a particular language (corresponding to 
grammar, g, say), PN(g) will not be I-that is, there will be a small percentage 
who have misconverged. This percentage might blow up over generations; and 
we potentially have unstable languages. This is in contrast to the limiting 
analysis of homogeneous populations which is trivial for learnable families of 
grammars. 
2. The limiting case analysis is more problematic, though more consistent 
with learnability theories "in the limit." First, the limit in question need not 
always exist. In such a case, of course, no limiting analysis is possible. If 
however, the limit does exist, then Pi is the probability that a child learner 
attains the grammar Pi in the limit-and this is the proportion of the population 
with this internal grammar in the next generation. 
3. In general, the linguistic composition for the (t + l)th generation is given 
in similar fashion from the linguistic composition for the tth generation. Such 
a dynamical system exists for every assumption of a)A, and b)g and C)Pi'S the 
probability with which sentences are produced by speakers of the ith grammar9• 
Thus we see that , 
(g, A, {Pi}) -+ V( dynamical system) 
4. The formulation is completely general so far. It does not assume any 
particular linguistic theory, or learning algorithm, or distribution with which 
9Note that this probability could evolve with generations as well. That will complete all the 
logical possibilities. However, for simplicity, we assume that this does not happen. 

LANGUAGE CHANGE 
181 
sentences are drawn. Of course, we have implicitly assumed a learning model, 
i.e., positive examples are drawn in i.i.d. fashion and presented to the learner 
(algorithm). Our formalization of the grammatical dynamical systems follows 
as a logical consequence of this learning framework. One can conceivably imag-
ine other learning frameworks- these would potentially give rise to other kinds 
of dynamical systems; but we don't formalize them here. 
At this stage, we have developed our case in abstraction. The next obvi-
ous step is to choose specific linguistic theories, and learning paradigms, and 
compute our dynamical system. The important questions are: can we really 
compute all the relevant quantities to specify the dynamical system?? Can we 
evaluate the behavior (the phase-space characteristics) of the resulting dynam-
ical system?? Does this allow us to shed light on linguistic theories?? We show 
some concrete examples of this in this chapter. Our examples are conducted 
within the principles and parameters theory of modern linguistics. 
5.2 
LANGUAGE CHANGE IN PARAMETRIC SYSTEMS 
The previous section led us through the important steps in formalizing the 
process of language change, leading ultimately to a computational paradigm 
within which such change can be meaningfully studied. We carry out our in-
vestigations within the principles and parameters framework introduced in the 
previous chapter. In Chapter 4, we investigated the problem of learnability 
within this framework. In particular, we saw that the behavior of any memo-
ryless algorithm can be modeled as a Markov chain. This analysis will allow us 
to solve equations 1 and 2, and obtain the update equations of our dynamical 
system. We now proceed to do this. 
1) the grammatical theory: Assume there are n parameters-this leads to a 
space g with 2n different grammars in it. 
2) the distribution with which data is produced: If there are speakers of 
the ith language, L" in the population, let them produce sentences according 
to the distribution, Pi, on the sentences of this language. For the most part, 
we will assume, in our simulations, that this is uniform on degree-O sentences 
(exactly as we did in our analysis of the learn ability problem). 
3) the learning algorithm: Let us imagine that the child learner follows some 
memoryless (incremental) algorithm to set parameters. For the most part, we 
will assume that the algorithm is the TLA or one of the variants discussed in 
the previous chapter. 
From One Generation to the Next: The Update Rule 
Suppose the state of the parental population is Ppop,n on g. Then one can 
obtain the distribution P on the sentences of ~* according to which sentences 
will be presented to the learner. Once such a distribution is obtained, we can 
compute the transition matrix T according to which the learner updates its 
hypotheses with each new sentence (as shown in the previous chapter). From 
T, one can finally compute the following quantities: 

182 
INFORMATIONAL COMPLEXITY OF LEARNING 
Prob[ Learner's hypothesis = hi E g after m examples] == {2: (1, ... , 1)'rm}[~1 
Similarly, making use of limiting distributions of Markov chains (see Resnick, 
1992) one can obtain the following (where ONE is a 21" x 21" matrix with all 
ones). 
Prob[ Learner's hypothesis = hi "in the limit"] = (1, ... ,1)'(1 -T+ONE)-1 
These expressions allow us to compute the linguistic composition of the popu-
lation according to our analysis of the previous section. 
Remarks: 
1. The limiting distribution needs to be interpreted. Markov chains corre-
sponding to population mixes do not have an absorbing state. Instead they 
have recurrent states. These states will be visited infinitely often. There might 
be more than one state that will be visited infinitely often. However, the per-
centage of time, the learner will be in a particular state might vary. This is 
provided by the equation above. Since, we know the fraction of the time the 
learner spends in each grammatical state in the limit, we assume that this is 
the probability with which it internalize the grammar corresponding to that 
state in the Markov chain. 
2. The finite case analysis always works. The limiting analysis need not 
work. However, the limiting analysis works only when there is more than one 
target. That is, if there is only one target grammar, for learnable algorithms, 
all children would converge to that target in the limit, and the population 
characteristics would not change with generations. 
We provide now the basic computational framework for modeling language 
change. 
1. Let 'lr1 be the initial population mix, i.e., the percentage of different language 
speakers in the community. Assuming, then, that the ith group of speakers 
produce sentences with probability Pi, we can obtain P with which sentences 
in E* occur for the next generation of children. 
2. From P, we can obtain the transition probabilities for the child learners and 
the limiting distribution 'lr2 for the next generation. 
3. The second generation produce sentences with 'lr2. We can repeat step 1 and 
obtain 'lr3 j in general a population mix 'lri will over a generation change to a 
mix of 'lri+!. 
5.3 
EXAMPLE 1: A THREE PARAMETER SYSTEM 
The previous section developed the necessary mathematical and computational 
tools to completely specify the dynamical systems corresponding to memory less 
algorithms operating on finite parameter spaces. In this example, we investi-
gate the behavior of these dynamical systems. Recall that every choice of 
(9,A, {Pi}) gives rise to a unique dynamical system. We start by assuming: 

Initial Language 
(-V2) 1 
(+V2) 2 
(-V2) 3 
(+V2) 4 
(-V2) 5 
(+V2) 6 
(-V2) 7 
(+V2) 8 
Change to Language? 
2 (0.85), 6 (0.1) 
2 (0.98); stable 
6 (0.48), 8(0.38) 
4 (0.86); stable 
2 (0.97) 
6 (0.92); stable 
2 (0.54), 4(0.35) 
8 (0.97); stable 
LANGUAGE CHANGE 
183 
Table 5.1. 
Language change driven by misconvergence. A finite-sam pie analysis was 
conducted allowing each child learner 128 examples to internalize its grammar. Initial pop-
ulations were linguistically homogeneous, and they drifted (or not) to different linguistic 
compositions. The major language groups after 30 generations have been listed in this 
table. 
1) g : This is a 3-parameter syntactic subsystem described in the previous 
chapter (Gibson and Wexler, 1994). Thus g has exactly 8 grammars. 
2) A : The memory less algorithms we consider are the TLA, and variants 
by dropping either or both of the single-valued and greediness constraints. 
3) {P;} : For the most part, we assume sentences are produced according to 
a uniform distribution on the degree-O sentences of the relevant language, i.e., 
Pi is uniform on (degree-O sentences of) Li. 
5.3.1 
Starting with Homogeneous Populations: 
Here we investigate how stable the languages in the parametric system are in 
the absence of noise or other confounding factors like foreign speech. Thus 
we start off with a linguistically homogeneous population producing sentences 
according to a uniform distribution on the degree-O sentences of the target 
language (parental language). We compute the the distribution of the children 
in the parameter space after 128 example sentences (recall, by the analysis of 
the previous chapter, the learners converge to the target with high probability 
after hearing these many sentences). Some small proportion of the children will 
have misconverged; the goal is to see whether this small proportion can drive 
language change-and if so, in what direction. 
5.3.1.1 
A = TLA; Pi = Uniform; Finite Sample = 128. The table 
below shows the result after 30 generations. Languages are numbered from 1 
to 8 according to the scheme in the appendix of chapter 4. 
Observations: Some striking patterns are observed. 
1. First, all the +V2 languages are relatively stable, i.e., the linguistic com-
position did not vary significantly over 30 generations. This means that every 

184 
INFORMATIONAL COMPLEXITY OF LEARNING 
succeeding generation acquired the target parameter settings and no parameter 
drifts were observed over time. 
2. Populations speaking -V2 languages all drift to speaking + V2 languages. 
Thus a population speaking L1 starts speaking mostly L2. A population speak-
ing language L7 gradually shifts to a population with 54 percent speaking L2 
and 35 percent speaking L4 (with a smattering of other speakers) and seems 
(?) to remain basically stable in this mix thereafter. Note that this relative 
stability of + V2, and the tendency of -V2 languages to drift to + V2 ones, are 
contrary to assertions in the linguistic literature. Lightfoot (1991), for example, 
claims the tendency to lose V2 dominates the reverse tendency in the world's 
languages. Certainly, both English and French lost the V2 parameter setting-
an empirically observed phenomenon that needs to be explained. Right away, 
we see that our dynamical system does not evolve in the expected pattern. The 
problem could be due to incorrect assumptions about the parameter space, the 
algorithm, initial conditions, or distributional assumptions about the sentences. 
This needs to be examined, no doubt, but we have just seen a concrete exam-
ple of how assumptions about grammatical theory, and learning theory, have 
made evolutionary predictions-in this case the predictions are incorrect, and 
our model is falsified. 
3. The rates at which the linguistic composition changes varies significantly. 
Consider for example the change of L1 to L2. Fig. 5.2 below shows the gradual 
decrease in speakers of L1 over successive generations along with the increase in 
L2 speakers. We see that over the first 6 or seven generations very little change 
occurs, thereafter over the next 6 or seven generations the population switches 
at a much faster rate. Note that in this particular case, the two languages 
differ only in the V2 parameter; so the curves essentially plot the gain of V2. 
In contrast, consider fig. 5.3 which shows the decrease of L5 speakers and the 
shift to L 2 • Here we notice a sudden change; over a space of 4 generations, 
the population has shifted completely. The time course of language change has 
been given some attention in linguistic analyses of diachronic syntax change, 
and we return to this in a later section. 
4. We see that in many cases, the homogeneous population splits up into 
different linguistic groups, and seem to remain stable in that mix. In other 
words, certain combinations of language speakers seem to asymptote towards 
equilibrium (atleast by examining the 30 generations simulated so far). For 
example, a population of L7 speakers shifts (over 5-6 generations) to one with 
54 percent speaking L2 and 35 percent speaking L4 and remains that way with 
no shifts in the distribution of speakers. Is this really a stable mix? Or will the 
population shift suddenly after another 100 generations? Can we characterize 
the stable points ("limit cycles")? Other linguistic mixes are inherently unsta-
ble mixes. They might drift systematically to stable situations, or might shift 
dramatically. 
In table 5.1, why are some languages stable while others are unstable? It 
seems that the instability and the drifts observed are to a large extent an 
artifact of the learning algorithm used. Remember that TLA suffers from the 
problem of local maxima. We notice that those languages whose acquisition 

C! ... 
N o 
C! 
LANGUAGE CHANGE 
185 
.V2 
o~ 
________ -r __________ T-________ 
~~ 
________ 
~~ 
5 
10 
Generations 
15 
20 
Figure 5.2. 
Percentage of the population speaking languages L1 and L2 as it evolves 
over the number of generations. The plot has been shown only upto 20 generations, as 
the proportions of L1 and L2 speakers do not vary significantly thereafter. Notice the 
"S" shaped nature of the curve (Kroch, 1989, imposes such a shape using models from 
population biology, while we obtain this as an emergent property of our dynamical model 
from different starting assumptions). Also notice the region of maximum change as the V2 
parameter is slowly set by increasing proportion of the population. L1 and L2 differ only in 
the V2 parameter setting. 
is not impeded by local maxima (the +V2 languages) are stable. Languages 
which have local maxima are unstable; in particular they drift to the local 
maxima over time. Consider L7. If this is the target, then there are two local 
maxima (L2 and L4) and these are precisely the states to which the system 
drifts over time. The same is true for languages L5 and L3 . In this respect, the 
behavior of L1 is unusual since it actually does not have any local maxima; yet 
it tends to flip the V2 parameter over time. 
Remark. We regard local maxima of a language Li to be alternative absorbing 
states (sinks) in the Markov chain for that target language. This differs slightly 
from the conception of local maxima in Gibson and Wexler (1994), a matter 
discussed at some length in Niyogi and Berwick (1993,1996), and in the previous 
chapter. Thus according to our definition L4 is not a local maxima for L5 and 
consequently no shift is observed. 

186 
INFORMATIONAL COMPLEXITY OF LEARNING 
r---··------·-·--·------·--·----·-·-·-
N ci 
o 
~ 
VOS+V2 
I 
I 
SVo-V2 
ci~ ________ _r----------~--------~r_--------~~ 
5 
10 
Generations 
15 
20 
Filure 5.3. 
Percentage of the population speaking languages L5 and L2 as it evolves 
over the number of generations. Notice how the shift occurs over a space of 4 generations. 
5.3.1.2 A = Greedy, No S.V.; Pi = Uniform; Finite Sample = 128. 
The previous discussion of grammatical evolution with TLA begs the question: 
what if the learner used some alternative learning algorithm which did not 
suffer from the problem of local maxima? To investigate this, we consider a 
simple variant of the TLA obtained by dropping the single valued constraint. 
This implies that the learner is no longer constrained to flip one parameter 
at a time. On being presented with a sentence it cannot analyze, it chooses 
any of the alternative grammars and attempts to analyze the sentence with it. 
Greediness is retained; thus the learner retains its original hypothesis if the new 
one is also not able to analyze the sentence. Table 5.2 shows the distribution 
of speakers after 30 generations. 
Observations: In this situation there are no local maxima, and the pattern of 
evolution takes on a very different nature. There are two distinct observations 
to be made. 
1. All homogeneous populations (irrespective of what language they speak) 
eventually drift to a strikingly similar population mix. What is unique about 
this mix? Is it a stable point (or attractor)? Further simulations, and theoret-
ical analysis is needed to resolve this question. 
2. All homogeneous populations drift to a population mix of only + V2 
languages. Thus, the V2 parameter is gradually set over succeeding generations 
by all people in the community (irrespective of which language they speak). In 

Initial Language 
-V21 
+V22 
-V23 
+V24 
-V25 
+V26 
-V27 
+V28 
Change to Language? 
2 (0.41), 4 (0.19), 6 (0.18), 8 (0.13) 
2 (0.42), 4 (0.19), 6 (0.17), 8 (0.12) 
2 (0.40), 4 (0.19), 6 (0.18), 8 (0.13) 
2 (0.41), 4 (0.19), 6 (0.18), 8 (0.13) 
2 (0.40), 4 (0.19), 6 (0.18), 8 (0.13) 
2 (0.40), 4 (0.19), 6 (0.18), 8 (0.13) 
2 (0.40), 4 (0.19), 6 (0.18), 8 (0.13) 
2 (0.40), 4 (0.19), 6 (0.18), 8 (0.13) 
LANGUAGE CHANGE 
187 
Table 5.2. 
Language change driven by misconvergence. A finite-sample analysis was 
conducted allowing each child learner (following the TLA with single-value dropped) 128 
examples to internalize its grammar. Initial populations were linguistically homogeneous. 
and they drifted to different linguistic compositions. The major language groups after 30 
generations have been listed in this table. Notice how all initially homogeneous populations 
tend to the same composition. 
other words, there is as before a tendency to gain V2 rather than lose it (we 
emphasize again, that this is contrary to linguistic intuition). 
Fig. 5.4 shows the changing percentage of the population speaking the dif-
ferent languages starting off from a homogeneous population speaking L5. As 
before, learners who have not converged to the target in 128 examples are the 
driving force for change here. Note again the time evolution of the grammars. 
For about 5 generations there is only a slight decrease in the percentage of 
speakers of L 5 • Then the linguistic patterns switch over the next 7 generations 
to a relatively stable mix. 
5.3.1.3 A = a) R.W. b) S. V. only; Pi = Uniform; Finite Sample = 
128. Here we simulated the evolution of the dynamical systems correspond-
ing to two algorithms, both of which have no greediness constraint. The two 
algorithms are 1) the random walk described in the previous chapter and 2) 
TLA with single-value retained but no greediness constraint. 
In both cases, the population mix after 30 generations is the same, irrespec-
tive of the initial language of the homogeneous population. This is shown in 
table 5.3. 
Observations: 
1. The first striking observation is that both algorithms yield dynamical 
systems which arrive at the same population mix after 30 generations. The 
path by which they arrive at this mix is, however, not the same (see fig. 5.5). 
2. It is also noteworthy, that in these cases, all initially homogeneous pop-
ulations converge to a single population mix. Further, this population mix 
contains all languages in significant proportion. This is in contrast to the pre-
vious situations, where we saw that non-V2 languages were eliminated. 

188 
INFORMATIONAL COMPLEXITY OF LEARNING 
SVo-V2 
co o 
VOS+V2 
......... --_ ..... _ .. _ ...... _----_ ...... --_.-
/ _ --____ 
OVS+V2 
/:,-.{// 
_ .. --_-:.:-.:::-_-:.=----=--....::'=--..:= 
.1"/ 
_-
./ ------
N o 
SVO+V2 
--"'-----
C! 
_____ 
...,--::":-:.----
o~ 
_______ ~------~-------~--__ -----r~ 
5 
10 
Generations 
15 
20 
Figure 5.4. 
Time evolution of gram mars using greedy algorithm with no single value. 
Initial Language 
Any Language 
(Homogeneous) 
Change to Language? 
1 (0.11), 2 (0.16), 3 (0.10), 4 (0.14) 
5 (0.12), 6 (0.14), 7 (0.10), 8 (0.13) 
Table 5.3. 
Language change driven by misconvergence. A finite-sample analysis was con-
ducted allowing each child learner (following 1) random walk and 2) the TLA with greediness 
dropped) 128 examples to internalize its grammar. Initial populations were linguistically ho-
mogeneous, and they drifted to different linguistic com positions. The major language groups 
after 30 generations have been listed in this table. Notice, again, how all initially homoge-
neous populations tend to the sa m e com position. 
5.3.1.4 
Rates of Change. The figures depicting the evolutionary trajec-
tories of the dynamical systems often have an S-shaped behavior (though the 
"smoothness" of this trajectory varied). In this section, we examine a few 
factors which affect the time-course (more generally, trajectories) of our gram-
matical dynamical systems. We begin by noting that linguists have, in the 
past, pursued this question. 
Some Linguistic thoughts on the time-course of language variation: 

LANGUAGE CHANGE 
189 
~ 
... 
CI! 
0 
I!! !.., 
!o 
'0 
8, 
@~ 
~o 
:. 
'" 
0 
+V2 
~ 
0 
5 
10 
15 
20 
25 
30 
Generations 
Figure 5.5. 
Time evolution of linguistic composition for the situations where the learning 
algorithm used is the TLA (with greediness dropped, corresponding to the dotted line) , and 
the Random Walk (solid line). Only the percentage of people speaking L1 (-V2) and L2 
(+V2) are shown. The initial population is homogeneous and speaks L 1 . The percentage 
of L1 speakers gradually decreases to about 11 percent. The percentage of L2 speakers 
rises to about 16 percent from 0 percent. The two dynamical systems (corresponding to 
S.V. and R.W.) converge to the same population mix. However, the trajectory is not the 
same-the rates of change are different, as shown in this plot. 
Bailey (1973) proposed a "wave" model of linguistic change. Among other 
things, this proposes that linguistic replacements follow an S-shaped curve in 
time. In Bailey's own words (taken from Kroch, 1990) 
A given change begins quite gradually; after reaching a certain point (say, 
twenty percent), it picks up momentum and proceeds at a much faster rate; 
and finally tails off slowly before reaching completion. The result is an S-curve: 
the statistical differences among isolects in the middle relative times of the 
change will be greater than the statistical differences among the early and late 
isolects. 
The idea that linguistic changes follow an S-curve has been proposed by 
Osgood and Sebeok (1954), Weinreich, Labov, and Herzog (1968). More specific 
logistic forms were proposed by Altmann (1983), and Kroch (1982,1989). The 
idea of the logistic functional form is borrowed from population biology where 
it is demonstrable that the logistic governs the replacement of organisms and 
of genetic alleles that differ in Darwinian fitness. However Kroch concedes 

190 
INFORMATIONAL COMPLEXITY OF LEARNING 
that "unlike in the population biology case, no mechanism of change has been 
proposed from which the logistic form can be deduced" . 
Crucially, in our case, we suggest a specific acquisition-based model of lan-
guage change. The combination of grammatical theory, learning algorithms, 
and distributional assumptions on sentences drive change-the specific form of 
the change (which might or might not be S-shaped, and might have varying 
rates) is thus a derivative of more fundamental assumptions. This is in contrast 
with the above-mentioned theories of change. 
The effect of maturational time 
One obvious factor influencing the evolutionary trajectories is the matura-
tional time, i.e., the number (N) of sentences the child is allowed to hear before 
forming its mature hypothesis. This was kept at 128 in all the systems shown 
so far. Fig. 5.6 shows the effect of N on the evolutionary trajectories. As usual, 
we plot only a subspace of the population. In particular, we plot the percentage 
of L2 speakers in the population with each succeeding generation. The initial 
composition of the population was homogeneous (with people speaking Ld. It 
is worthwhile to make a few observations: 
1. The initial rate of change of the population is highest for the situation where 
the maturational time is the least, i.e., the learner is allowed the least amount 
of time to develop its mature hypothesis. This is hardly surprising. If 
the learner were allowed access to a lot of examples to make its mature 
hypothesis, most of the learners would have reached the target grammar. 
Very few would have misconverged, and the linguistic composition would 
have changed little over the next generation. On the other hand, if the 
learner were allowed very few examples to develop its hypothesis, many 
would misconverge, causing great change over one generation. 
2. The "stable" linguistic compositions seem to depend upon maturational 
time. For example, if the learner is allowed only 8 examples, the number of 
L2 speakers rises quickly to about 0.26. On the other hand, if the learner is 
allowed 128 examples, the number of L2 speakers eventually rises to about 
0.41. 
3. Note that the trajectories do not have an S-shaped curve. 
4. The maturational time is related to the order of the dynamical system. 
The effect of sentence distributions Pi. 
Another important factor influencing the evolutionary trajectories is the 
distribution Pi with which sentences of the ith language, Li, are presented to the 
learner. In a certain sense, the grammatical space and the learning algorithm 
determine the order of the dynamical system. The sentence distributions on the 
other hand, are like the parameters of the dynamical system (we comment on 
this point later). Clearly the sentence distributions affect rates of convergence 
within one generation as we saw in the previous chapter. Further, by putting 
greater weight on certain word forms rather than others, they might influence 
the systemic evolution in certain directions. 

C! -
co 
ci 
2! 
j as", 
c§.ci 
'0 
& 
~~ 
~o 
8? 
N 
ci 
C! 
0 
0 
5 
10 
15 
Generations 
LANGUAGE CHANGE 
191 
20 
25 
30 
Figure 5.6. 
Time evolution of linguistic composition for the situations where the learning 
algorithm used is the TLA (with single-value dropped). Only the percentage of people 
speaking £2 (+V2) is shown. The initial population is homogeneous and speaks £1. The 
maturational time (number,N, of sentences the child hears before internalizing a grammar) 
is varied through 8, 16, 32, 64, 128, 256, giving rise to the six curves shown in the figure. 
The curve which has the highest initial rate of change corresponds to the situation where 
8 examples were allowed to the learner to develop its mature hypothesis. The initial rate 
of change decreases as the maturation time N increases. The value at which these curves 
asymptote also seems to vary with the maturation time, and increases monotonically with 
it. 
To illustrate this idea, we consider an example. We study the interaction 
between £1 and £2 speakers in the community as the sentence distributions 
with which these speakers produce sentences changes. Recall that so far, we 
have assumed that all speakers produce sentences with uniform distributions on 
degree-O sentences ofthe language. Now, we consider an alternative distribution 
as below: 
2. PI : Speakers of £1 produce sentences so that all degree-O sentences of £1,2 
are equally likely and their total probability is p. Further, sentences of £1 \ 
£1,2 are also equally likely, but their total probability is 1 - p. 

192 
INFORMATIONAL COMPLEXITY OF LEARNING 
3. P2: Speakers of L2 produce sentences so that all degree-O sentences of L1,2 
are equally likely and their total probability is p. Further, sentences of L2 \ 
L 1,2 are also equally likely, but their total probability is 1 - p. 
4. Other Pi'S are all uniform in degree-O sentences. 
Thus, the distributions P;'s are parameterized by a single parameter, p, 
which determines the amount of measure on the sentence patterns in common 
between the languages L1 and L2. Fig. 5.7 shows the evolution of the L2 speak-
ers as p varies. The learning algorithm used was the TLA, and the initial 
population was homogeneous (speaking language Ld. Thus, the initial per-
centage of L2 speakers in the community was O. Notice how the system moves 
in different ways as p varies. When p is very small (0.05), i.e., strings common 
to L1 and L2 occur infrequently, the long term implication is that L2 speakers 
do not grow in the community. As p increases, more strings of L2 occur, and 
the system is driven to increase the number of L2 speakers until p = 0.75 when 
the population evolves into a completely L2 speaking community. After this, 
as p increases further, we notice (see p = 0.95) that the L2 speakers increase 
but can never rise to 100 percent of the population, there is still a residual 
L1 speaking component. This is natural, because for such high values of p, a 
lot of strings common to L1 and L2 occur all the time. This means that the 
learner could converge to L1 just as well, and some learners indeed begin to do 
so increasing the number of the L1 speakers. 
This example shows us that if we wanted a homogeneous L1 speaking pop-
ulation to move to a homogeneous L2 speaking population, by choosing our 
distributions appropriately, we could drive the grammatical dynamical system 
in the appropriate direction. This suggests another important application of 
our dynamical system approach. We can work backwards, and examine the 
conditions needed to generate a change of a certain kind. By checking whether 
such conditions could possibly have existed in history, we can falsify a gram-
matical theory, or a learning paradigm. Note that this example showed the 
effect of sentence distributions, and how to tinker with them to obtain desired 
evolution. One could, in principle, tinker with the grammatical theory, or the 
learning algorithm in the same fashion-leading to a powerful new tool to aid 
the search for an adequate linguistic theory. 
5.3.2 Non-homogeneous Populations: Phase-Space Plots 
For our three-parameter system, we have been able to characterize the update 
rules for the dynamical systems corresponding to a variety of learning algo-
rithms. Each such dynamical system has a specific update procedure according 
to which the states evolve, from some initial state. In the earlier section, we 
examined the evolutionary trajectories when the population was homogeneous. 
A more complete characterization of the dynamical system would be achieved 
by obtaining phase-space plots of this system. Such phase-space plots are pic-
tures of the state-space S filled with trajectories obtained by letting the system 
evolve from various initial points (states) in the state space. 

LANGUAGE CHANGE 
193 
q ,.. 
~ 
0 
e I 
&~ 
~ 
J~ 
('<j 
0 
q 
0 
(} 
10 
Figure 5.7. 
The evolution of L2 speakers in the community for various values of p (a 
parameter related to the sentence distributions Pi, see text). The algorithm used was 
the TLA, the inital population was homogeneous, speaking only L1• The curves for p = 
0.05,0.75, and 0.95 have been plotted as solid lines. 
5.3.2.1 
Phase-Space Plots: Grammatical Trajectories. We have de-
scribed earlier, the relationship between the state of the population in one 
generation and the next. In our case, let II denote an 8-dimensional vector 
variable (state variable). Specifically, II = (11"1, ... ,11"8)' (with L~=111"i) as we 
discussed before. The following schema reiterates the chain of dependencies 
involved in the update rule governing system evolution. The state of the popu-
lation at time t (in generations), allows us to compute the transition matrix T 
for the Markov chain associated with the memoryless learner. Now, depending 
upon whether we want 1) an asymptotic analysis or 2) a finite sample analysis, 
we compute 1) the limiting behavior of yom as m (the number of examples) 
goes to infinity (for an asymptotic analysis), or 2) the value of TN (where N 
is the number of examples after which maturation occurs). This allows us to 
compute the new state of the population. Thus lI(t + 1) = g(lI(t» where 9 is 
a complex non-linear relation. 
lI(t) => P on E* => T => yom => lI(t + 1) 
If we choose a certain initial condition Ill, the system will evolve according to 
the above relation and one can obtain a trajectory of II in the 8 dimensional 
space over time. Each initial condition yields a unique trajectory and one 

194 
INFORMATIONAL COMPLEXITY OF LEARNING 
o 
O~------~------~~-------r--------r--------r~ 
0.2 
0.4 
0.6 
0.8 
1.0 
Percentage 01 Speakers VOS-V2 
Figure 5.8. 
Subspace of a Phase-space plot. The plot shows (11'1 (t), 1r2(t» as t varies, 
i.e., the proportion of speakers speaking languages L1 and L2 in the population. The initial 
state ofthe population was homogeneous (speaking language Lt). The algorithm used was 
the TLA with the single-value constraint dropped. 
can then plot these trajectories obtaining a phase-space plot. Now, each such 
trajectory corresponds to a line in the 8-dimensional plane given by L~=l lri = 
1. It is obviously not possible to display such a high dimensional object but we 
plot in fig. 5.8 the projection of a particular trajectory onto a two dimensional 
subspace given by (1r1(t), 1r2(t» (in other words, the proportion of speakers of 
L1 and L2 ) at different points in time. 
As mentioned earlier, with a different initial condition, we get a different 
grammatical trajectory. The state space is thus filled with all the different 
trajectories corresponding to different initial conditions. Fig. 5.9 shows this. 
5.3.2.2 
Issues of Stability. We notice from the phase-space plots that 
many of the initial conditions yield trajectories which seem to converge to a 
point in the state space. In the dynamical systems terminology, this would 
correspond to a fixed point of the system. In other words, this is a population 
mix which would remain that way. Some natural questions arise at this stage. 
What are the conditions for stability? How many fixed points are there in 
the system? How do we solve for them? These are interesting questions but 
detailed answers are not within the scope of this book. We would like to state 

C\I o 
~ 
LANGUAGE CHANGE 
195 
~--------r-------~------~-------r------~~ 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
VOS-V2 
Figure 5.9. 
Subspace of a Phase-space plot. The plot shows (11"1 (t), 1I"2(t)) as t varies 
for different initial conditions (non-homogeneous populations). The algorithm used by the 
learner is the TLA with single-value constraint dropped. 
here a fixed point theorem which allows us to characterize the stable population 
mIxes. 
First, some notational preliminaries. As before, let Pi be the distribution on 
the sentences of the ith language Li. From Pi, we can construct 11, the transi-
tion matrix whose elements are given by the explicit procedure documented in 
the previous chapter. This matrix, 11, models the behavior of the TLA learner 
if the target language was Li (with sentences from the target produced with 
Pi). Similarly, one can obtain the matrices for variants of the TLA. Note that 
fixing the Pi'S fixes the 11 's and these can be considered to be the parametersIO 
of the dynamical system. If the state of the (parental) population at time t is 
II(t), then it is possible to show that the (true) transition matrix of the TLA 
(or TLA-like) learner is T = L~=111"i(t)11. For the finite case analysis, the 
following theorem holds: 
lOThere might be some confusion at the two different notions of parameters fioating around. 
Just to clarify further; we have n linguistic parameters which define the 2n languages and 
define the state-space of the system. We also have the Pi'S which characterize the way in 
which the system evolves and are therefore the parameters of the complete grammatical 
dynamical system. 

196 
INFORMATIONAL COMPLEXITY OF LEARNING 
Theorem 5.1 (Finite Case) A fixed point (stable point) of the grammatical 
dynamical system (obtained by a TLA like learner operating on the 8 parameter 
space with k examples to choose its mature hypothesis) is a solution of the 
following equation: 
8 
II' = (11"1, ... ,11"8) = (1, ... , l)'(L 1I"iTd 
;=1 
Proof (Sketch): This equation is obtained simply by setting II(t + 1) = II(t). 
Note however, that this is an example of a non-linear multi-dimensional iterated 
function map. The analysis of such a dynamical system is quite non-trivial, and 
our theorem by no means captures all the possibilities. I 
We can similarly state a theorem for the limiting case analysis. 
Theorem 5.2 (Limiting Analysis) A fixed point (stable point) of the gram-
matical dynamical system (obtained by a TLA like learner operating on the 8 
parameter space (given infinite examples to choose its mature hypothesis) is a 
solution of the following equation: 
8 
II' = (11"1, ... ,11"8) = (1, ... , 1)'(I - L 11";1'; + ON E)-l 
i=l 
where ONE is the 8 x 8 matrix with all its entries equal to 1. 
Proof: Again this is trivially obtained by setting II(t + 1) = II(t). The expres-
sion on the right provides an analytical expression for the update equation in 
the asymptotic case. See Resnick (1992) for details. All the caveats mentioned 
in the proof section of the previous theorem apply here as well. I 
Remark: We have just scratched the surface as far as the theoretical charac-
terization of these grammatical dynamical systems are concerned. The main 
purpose of this chapter is to show that these dynamical systems exist as a log-
ical consequence of assumptions about the grammatical space, and a learning 
theory. We have demonstrated some preliminary simulations with these sys-
tems. From a theoretical perspective, it would be very interesting to better 
understand such systems. Strogatz (1993) suggests that non-linear multidi-
mensional (more than 3 dimensions) mappings are likely to be chaotic. Such 
investigations are beyond the scope of this book, and might be a fruitful area 
for further research. 
5.4 
EXAMPLE 2: THE CASE OF MODERN FRENCH: 
The previous example considered a 3-parameter system for which we derived 
several different dynamical systems. Our goal was to concretely instantiate our 
philosophical arguments in sections 2 and 3, and provide a flavor of the many 
different factors which influence the evolution of these grammatical dynamical 
systems. In this section, we briefly consider a different parametric system 
(studied by Clark and Roberts, 1993). The historical context in which we 
study this is the evolution of Modern French from Old French. 

LANGUAGE CHANGE 
197 
Extensive simulations in the earlier section reveal that while the learn ability 
problem of the 3-parameter space can be solved by stochastic hill climbing algo-
rithms, the long term evolution of these algorithms have a behavior which is at 
variance with the diachronic change actually observed in historical linguistics. 
In particular, we saw how there was a tendency to gain rather than lose the 
V2 parameter setting. While this could be an artifact of the class of learning 
algorithms considered, a more likely explanation is that loss of V2 (observed 
in many of the world's languages like French etc.) is due to an interaction of 
parameters and triggers other than that considered in the previous section. We 
investigate this possibility and begin, by first providing the parametric theory. 
5.4.1 
The Parametric Subspace and Data 
We now consider a syntactic space involving the following 5 (boolean-valued) 
parameters. We do not attempt to describe these parameters. The interested 
reader should consult Haegeman (1991) for details. 
1. Pl: Case assignment under agreement (Pl = 1) or not (Pl = 0). 
2. P2: Case assignment under government (P2 = 1) or not «P2 = 0). Relevant 
triggers for this parameter include "Adv V S", "S V 0" . 
3. P3: Nominative clitics. 
4. P4: Null Subject. Here relevant triggers would include "wh V S 0". 
5. P5: Verb-second V2. Triggers include "Adv V S" , and "S V 0". 
These 5 parameters now define a space of 32 parameterized grammars. Each 
grammar in this parameterized system can be represented by a string of 5 bits 
depending upon the values of Pl, ... , P5. We need obviously to look at the 
surface strings (sentences) generated by each such grammar. For the purpose 
of explaining how Old French changed to Modern French over time, Clark and 
Roberts consider the following sentences. We provide these sentences below. 
The parameters settings which need to be made in order to generate each 
sentence is provided in brackets. 
The Relevant Data; 
adv V S [*1**1]; SVO [*1**1] or [1***0]; wh V S 0 [*1***]; wh V sO [**1**] 
; X (pro)V 0 [*1*11] or [1**10]; X V s [**1*1]; X s V [**1*0]; X S V [1***0]; 
(s)VY [*1*11] 
The parameter settings provided in brackets determine the grammars which 
generate the sentence. Thus the sentence (adv V S; quickly ran lohn- incorrect 
word order in English) is generated by all grammars which have case assignment 
under government (P2 = 1) and verb second movement (P5 = 1). The other 
parameters can be set to any value. Clearly there are 8 different grammars 
which can generate (parse) this sentence. Similarly there are 16 (8 correspond-
ing to parameter settings of [* 1 ** 1] and 8 corresponding to parameter settings 
of [1 ***0]) grammars which generate (S V 0) and 4 grammars which generate 
«s) V V). 

198 
INFORMATIONAL COMPLEXITY OF LEARNING 
Remark Note that the set of sentences considered here is only a subset of the 
the total number of (degree-O) sentences generated by the 32 grammars in 
question. Clark and Roberts have only considered this subset and attempted 
to construct learning algorithms and models of diachronic change using genetic 
algorithms. In order to facilitate direct comparison with their results, we have 
not attempted to expand the data set or fill out the space any further. As a 
result, all the grammars do not have unique extensional properties, i.e. some 
generate the same sentences and are thus equivalent. 
5.4.2 The Case of Diachronic Syntax Change in French 
Within this parameter space, it is historically observed that the language spo-
ken in France underwent a parametric change from the twelfth century to mod-
ern times. In particular, a loss of V2 and prodrop is observed. We provide two 
examples of this. In keeping with standard practice, the asterisk denotes an 
ungrammatical sentence. 
Loss of null subjects: pro-drop 
a. 
* Ainsi s'amusaient bien cette nuit. (Modern French) 
thus (they) had fun that night. 
b. 
Si firent (pro) grant joie la nuit. (Old French) 
thus (they) made great joy the night. 
Loss of V2 
a. 
*Puis entendirent-ils un coup de tonerre. (Modern French) 
then they heard a clap of thunder. 
b. 
Lors oirent ils venir un escoiz de tonoire. (Old French) 
then they heard come a clap of thunder 
It has been argued that this transition was brought about by introduction 
of new word orders during the fifteenth and sixteenth centuries resulting in 
generations of children acquiring slightly different grammars and eventually 
culminating in the grammar of modern French. A brief reconstruction of the 
historical process (after Clark and Roberts, 1993) is provided. 
Old French [11011] The language spoken in the twelfth and thirteenth cen-
turies had verb-second movement and null subjects, both of which were dropped 
by the twentieth century. The set of sentences generated by the parameter set-
tings corresponding to Old French are: 
adv V S - [*1**1]; SVO - [*1**1] or [1***0]; wh V S 0 [*1***]; X (pro)V 0 
[*1*11] or [1**10] 
Note that from the data set, it appears that the Case agreement and nom-
inative clitics parameters remain ambiguous. In particular, Old French is in 
a subset-superset relation with another language (generated by the parameter 
settings of 11111). Clearly some kind of subset principle (Berwick, 1985) has 
to be used by the learner for otherwise it is not clear how the data would allow 
the learner to converge to the Old French grammar in the first place. Note that 
TLA or TLA like schemes would not converge uniquely to the grammar of Old 
French. 

LANGUAGE CHANGE 
199 
The string (X)VS occurs with 58% and SV(X) occurs with 34% in Old French 
texts. It is argued that this frequency of (X)VS is high enough to cause the V2 
parameter to trigger to +V2. 
Middle French In Middle French, the data is not consistent. with any of the 
32 target grammars (equivalent to a heterogeneous population). Analysis of 
texts from that period reveal that some old forms (like Adv V S) decreased 
in frequency and new forms (like Adv S V) increased. It is argued in Clark 
and Roberts that such a frequency shift causes "erosion" of V2, brings about 
parameter instability and ultimately convergence to the grammar of Modern 
French. In this t.ransition period (i.e. when Middle French was spoken/written) 
the data is of the following form: 
adv V S [*1**1]; SVO [*1**1] or [1***0]; wh V S 0 [*1***]; wh V s 0 
[**1**]; X (pro)V 0 [*1*11] or [1**10]; X V s [**1*1]; X s V [**1*0]; X S V 
[1 ***0]; (s)VY [*1*11] 
Thus, we have old sentence patterns like Adv V S (though it decreases in 
frequency and becomes only 10%), 5VO, X (pro)V 0 and whV50. The new 
sentence patterns which emerge at this stage are adv 5 V (increases in frequency 
to become 60%), X subjclitic V, V subjclitic (pro)V Y (null subjects) , whY 
subjclitic O. 
Modern French [10100] By the eighteenth century, French had lost both 
the V2 parameter setting as well as the null subject parameter setting. The 
sentence patt.erns consistent with Modern French parameter settings are SVO 
[* 1 ** 1] or [1 ***0], X S V [1 ***0], V s 0 [** 1 **]. Note that this data, though 
consistent with Modern French, will not trigger all the parameter settings. In 
this sense, Modern French (just like Old French) is not uniquely learnable from 
data. However, as before, we shall not concern ourselves overly with this, for 
the relevant parameters (V2 and null subject) are uniquely set by the data here. 
5.4.3 Some Dynamical System Simulations 
We can obtain dynamical systems for this parametric space, for a TLA (or 
TLA-like) algorithm in a straightforward fashion. We show the results of two 
simulations conducted with such dynamical systems. 
5.4.3.1 
Homogeneous Populations [Initial-Old French]. We did a 
simulation on this new parameter space using the Triggering Learning Algo-
rithm. Recall that the relevant Markov chain in this case has 32 states. We 
start the simulation with a homogeneous population speaking Old French (pa-
rameter setting = 11011). Our goal was to see if misconvergence alone, could 
drive Old French to Modern French. 
Just as before, we can observe the linguistic composition of the population 
over several generations. It is observed that in one generation, 15 percent 
of the children converge to grammar 01011; 18 percent to grammar 01111; 33 
percent to grammar 11011 (target) and 26 percent to grammar 11111 with very 
few having converged to other grammars. Thereafter, the population consists 
mostly of speakers of these 4 languages, with one important difference: 15 

200 
INFORMATIONAL COMPLEXITY OF LEARNING 
:: \ 
co o 
(\I o 
p=11011 
p=11111 
( ... ~--.... -.-.... --.-.. -.. -.......... -.-...•.. --.----........ -._._. __ ..... _ 
...... -.. _._ ..... . 
! 
p=01111 
/,'------------------------------------
i I 
~~'\, 
~101\-------------------------- ---------------------------
o 
5 
10 
15 
20 
Number of Generations 
Figure 5.10. 
Evolution of speakers of different languages in a population starting off with 
speakers only of Old French. 
percent of the speakers eventually lose V2. In particular, they have acquired 
the grammar 11110. Shown in fig. 5.10 are the percentage of the population 
speaking the 4 languages mentioned above as they evolve over 20 generations. 
Notice that in the space of a few generations, the speakers of 11011, and 01011 
have dropped out altogether. Most of the population now speaks language 1111 
(46 percent) and 01111 (27 percent). Fifteen percent of the population speaks 
11110 and there is a smattering of other speakers. The population remains 
roughly stable in this configuration thereafter. 
Observations: 
1. On examining the four languages to which the system converges after one 
generation, we notice that they share the same settings for the principles [Case 
assignment under government], [pro drop], and [V2]. These correspond to the 
three parameters which are uniquely set by data from Old French. The other 
two parameters can take on any value. Consequently 4 languages are generated 
all of which satisfy the data from Old French. 
2. Recall our earlier remark that due to insufficient data, there were equiv-
alent grammars in the parameter system. It turns out that in this particular 
case, the grammars (01011) and (11011) are identical as far as their extensional 
properties are concerned; as are the grammars (11111) and (01111). 
3. There is subset relation between the two sets described in (2). The 
grammar (11011) is in a subset relation with (11111). This explains why after 

LANGUAGE CHANGE 
201 
a few generations most of the population switches to either (11111) or (01111) 
(the superset grammars). 
4. An interesting feature of the simulation is that 15 percent of the pop-
ulation eventually acquires the grammar (11110), i.e., they have lost the V2 
parameter setting. This is the first sign of instability of V2 that we have seen 
in our simulations so far (for greedy algorithms which are psychologically pre-
ferred). Recall that for such algorithms, the V2 parameter was very stable in 
our previous example. 
5.4.3.2 
Heterogeneous Populations (Mixtures). The earlier section 
showed that with no new (foreign) sentence patterns the grammatical system 
starting out with only Old French speakers showed some tendency to lose V2. 
However, the grammatical trajectory did not terminate in Modern French. In 
order to more closely duplicate this historically observed trajectory, we examine 
alternative inital conditions. We start our simulations with an initial condition 
which is a mixture of two sources; data from Old French and data from New 
French (reproducing in this sense, data similar to that obtained from the Mid-
dle French period). Thus children in the next generation observe new surface 
forms. Most of the surface forms observed in Middle French are covered by this 
mixture. 
Observations: 
1. On performing the simulations using the TLA as a learning algorithm on 
this parameter space, an interesting pattern is observed. Suppose the learner is 
exposed to sentences with 90 percent generated by Old French grammar (11011) 
and 10 percent by Modern French grammar (10100), within one generation 22 
percent of the learners have converged to the grammar (11110) and 78 percent 
to the grammar (11111). Thus the learners set each of the parameter values to 
1 except the V2 parameter setting. Now Modern French is a non-V2 language; 
and 10 percent of data from Modern French is sufficient to cause 22 percent of 
the speakers to lose V2. This is the behavior over one generation. The new 
population (consisting of 78 percent speaking grammar (11111) and 22 percent 
speaking grammar (11110)) remains stable for ever. 
2. Fig. 5.11 shows the proportion of speakers who have lost V2 after one 
generation, as a function of the proportion of sentences from the Modern French 
Source. The shape of the curve is interesting. For small values of the proportion 
of the Modern French source, the slope of the curve is greater than 1. Thus 
there is a greater tendency of speakers to lose V2 than to retain it. Thus 10 
percent of novel sentences from the Modern French source causes 20 percent 
of the population to lose V2; similarly 20 percent of novel sentences from the 
Modern French source causes 40 percent of the speakers to lose V2. This effect 
wears off later. This seems to capture computationally the intuitive notion of 
many linguists that a small change in inputs provided to children could drive 
the system towards larger change. 
3. Unfortunately, there are several shortcomings of this particular simula-
tion. First, we notice that mixing Old and Modern French sources does not 
cause the desired (historically observed) grammatical trajectory from Old to 

202 
INFORMATIONAL COMPLEXITY OF LEARNING 
C! ... 
N >CD 
1Il0 
.S! 
II) 
~ 
I~ 
r! ! 
§.~ 
'5 
c 
~ 
l~ 
C! 
0 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
Proportton ot Modem French Source 
Figure 5.11. 
Tendency to lose V2 as a result of new word orders introduced by Modern 
French source in our Markov Model. 
Modern French (corresponding in our system to movement from state (11011) 
to state (10100) in our Markov Chain). Although we find that a small injection 
of sentences from Modern French causes a larger percentage of the population 
to lose V2 and gain subject clitics (which are historically observed phenomena), 
nevertheless, the entire population retains the null subject setting and case as-
signment under government. It should be mentioned that Clark and Roberts 
argue that the change in case assignment under government is the driving force 
which allows alternate parse-trees to be formed and causes the parametric loss 
of V2 and null subject. In this sense, it is a more fundamental change. 
4. If the dynamical system is allowed to evolve, it ends up in either of the 
two states (11111) or (11110). This is essentially due to the subset relations 
these states (languages) have with other languages in the system. Another com-
plication in the system is the equivalence of several different grammars (with 
respect to their surface extensions) e.g. given the data we are considering, the 
grammars (01011) and (11011) (Old French) generate the same sentences. This 
leads to multiplicity of paths, convergence to more than one target grammar 
and general inelegance of the state-space description. 
Future Directions: There are several possibilities to consider here. 
1. Using more data and filling out the state-space might yield greater insight. 
Note that we can also study the development of other languages like Italian or 
Spanish within this framework and that might be useful. 

LANGUAGE CHANGE 
203 
2. TLA-like hill climbing algorithms do not pay attention to the subset 
principle explicitly. It would be interesting to explicitly program this into the 
learning algorithm and observe the evolution thereafter. 
3. There are often cases when several different grammars generate the same 
sentences or atleast equally well fit the data. Algorithms which look only 
at surface strings are unable then to distinguish between them resulting in 
convergence to all of them with different probabilities in our stochastic setting. 
We saw an example of this for convergence to four states earlier. Clark and 
Roberts suggest an elegance criterion by looking at the parse-trees to decide 
between these grammars. This difference between strong generative capacity 
and weak generative capacity can easily be incorporated into the Markov model 
as well. The transition probabilities, now, will not depend upon the surface 
properties of the grammars alone, but also upon the elegance of derivation for 
each surface string. 
4. Rather than the evolution of the population, one could look at the evo-
lution of the distribution of words. One can also obtain bounds on frequencies 
with which the new data in the Middle French Period must occur so that the 
correct drift is observed. 
5.5 
CONCLUSIONS 
In this chapter, we have argued that any combination of (grammatical theory, 
learning paradigm) leads to a model of grammatical evolution and diachronic 
change. A learning theory (paradigm) attempts to account for how children 
(the individual child) solve the problem of language acquisition. By consider-
ing a population of such "child learners", we have arrived at a model of the 
emergent, global, population behavior. The key point is that such a model 
is a logical consequence of grammatical, and learning theories. Consequently, 
whenever a linguist suggests a new grammatical, or learning theory, they are 
also suggesting a particular evolutionary theory-and the consequences of this 
need to be examined. 
Historical Linguistics and Diachronic Criteria From a programmatic 
perspective, this chapter has two important consequences. First, it allows us to 
take a formal, analytic view of historical linguistics. Most accounts of language 
change have tended to be descriptive in nature (though significant exceptions 
are the work of Lightfoot, Kroch, Clark and Roberts, among others). In con-
trast, we place the study of historical linguistics (diachronic phenomena) on 
a scientificll platform. In this sense, our conception of historical linguistics 
is closest in spirit to evolutionary theory and population biology12 (which at-
tempts to describe the origin and changing patterns of life) and cosmology 
11 By scientific, we mean, the construction of models with explanatory, and predictive powers-
models which can be falsified in the sense of Popper. 
12lndeed, most previous attempts to model language change, like that of Clark and Roberts 
(1993), and Kroch (1990) have been influenced by the evolutionary models. 

204 
INFORMATIONAL COMPLEXITY OF LEARNING 
(which attempts to describe the origin and evolution of the physical universe). 
As a matter of fact, a pioneering attempt has been made to quantify models 
of cultural transmission by Cavalli-Sforza and Feldman (1981). There are deep 
connections between the evolutionary models developed in this chapter and 
those developed by them -
a matter worth pursuing further. 
Second, it allows us to formally pose a diachronic criterion for the adequacy 
of grammatical theories. A significant body of work in learning theory, has 
already sharpened the learnability criterion for grammatical theories-in other 
words, the class of grammars g must be learnable by some psychologically 
plausible algorithm from primary linguistic data. Now we can go one step 
further. The class of grammars g (along with a proposed learning algorithm 
A) can be reduced to a dynamical system whose evolution must match that of 
the true evolution of human languages (as reconstructed from historical data). 
In This Chapter In this chapter, we have attempted to lay the framework 
for the development of research tools to study historical phenomena. To con-
cretely demonstrate that the grammatical dynamical systems need not be im-
possibly difficult to compute (or simulate), we explicitly showed how to trans-
form parameterized theories, and memory less learning algorithms to dynamical 
systems. The specific simulations of this chapter are far too incomplete to have 
any long term linguistic implications, though, we hope, it certainly forms a 
starting point for research in this direction. Nevertheless, there were certain 
interesting results obtained in this chapter. 
1. We saw that the V2 parameter was more stable in the 3-parameter case, 
than it was in the 5 parameter case. This suggests that the loss of V2 (actually 
observed in history) might have more to do with the choice ofparameterizations 
than learning algorithms, or primary linguistic data (though, we suggest great 
caution, before drawing strong conclusions on the basis of this study). 
2. We were able to shed some light on the time course of evolution. In partic-
ular, we saw how this was a derivative of more fundamental assumptions about 
initial population conditions, sentence distributions, and learning algorithms. 
3. We were able to formally develop notions of system stability. Thus, certain 
parameters could change with time, others might remain stable. This can now 
be measured, and the conditions for stability or change can be investigated. 
4. We were able to demonstrate how one could tinker with the system (by 
changing the algorithm, or the sentence distributions, or maturational time) to 
allow evolution in certain directions. This would suggest the kinds of changes 
needed in linguistics for greater explanatory adequacy. 
Further Research This has been our first attempt to define the boundaries 
of the problem. There are several directions of further research. 
1. From a linguistic perspective, the most interesting thing to do, would 
perhaps be the examination of alternative parameterized theories, and to track 
the change of certain languages in the context of these theories (much like 
our attempt to track the change of French in this chapter). Some worthwhile 
attempts would include a) the study of parametric stress systems (Halle and 

LANGUAGE CHANGE 
205 
Idsardi, 1992)-and in particular, the evolution of modern Greek stress pat-
terns from proto-Indo European; b) the investigation of the possibility that 
creoles correspond to fixed points in parametric dynamical systems, a possi-
bility which might explain the striking fact that all creoles (irrespective of the 
linguistic origin, i.e., initial linguistic composition of the population) have the 
same grammar; c) the evolution of modern Urdu, with Hindi syntax, and Per-
sian vocabulary. 
2. From a mathematical perspective, one could take this research in many 
directions including a) the formalization of the update rule for other grammati-
cal theories and learning algorithms, and the characterization of the dynamical 
systems implied therein b) the investigation of stability issues more closely, 
and characterizing better the phase-space plots c) recall that our dynamical 
systems are multi-dimensional non-linear iterated function mappings-a recipe 
for chaotic behavior, and a possibility to investigate further. 
It is our hope that research in this line will mature to make useful contribu-
tions, both to linguistics, and in view of the unusual nature of the dynamical 
systems involved, to the study of such systems from a mathematical perspec-
tive. 

6 
CONCLUSIONS 
This chapter concludes our discussion by articulating the perspective that emerges over the 
investigations of the previous chapters. We discuss the implications of some of our specific 
results, their role in illuminating our point of view, and directions for future research. 
In this book, we investigated the problem of learning from examples. Implicit 
in any scientific investigation is a certain point of view- crucial to our point 
of view were: 
1. the belief (and recognition) that the brain computes functions (input- output 
maps). Consequently, a function approximation framework is relevant, and 
in the context of learning, it is of some value to understand the complexity 
of learning to approximate (or identify) functions from examples. 
2. a focus on the informational complexity of learning such functions. Roughly 
speaking , if one wishes to learn from examples, then how many examples 
does one need? 
From this starting point, we proceeded to examine the informational com-
plexity of learning from examples in a number of different contexts. Several 
themes have emerged over the course of this book. 
207 

208 
INFORMATIONAL COMPLEXITY OF LEARNING 
6.1 
EMERGENT THEMES 
Hypothesis Complexity: The number of examples needed depends upon the 
complexity of the hypothesis class used. In view of Vapnik and Chervonenkis' 
work (and numerous other works in statistics), this is reasonably well recog-
nized (though people continue to flout this in the design of underconstrained 
models for learning systems). More crucially, there is an inherent tension be-
tween the approximation error and estimation error, and a tradeoff between 
the two is involved whenever one chooses a model of a certain complexity. We 
demonstrated this explicitly in the case of feedforward regularization networks, 
but the point is general. In language learning, this tension plays a crucial role, 
and guided our choice of the kinds of linguistic theories worth examining from 
a scientific perspective. We later investigated the sample complexity of learn-
ing within the principles and parameters framework of modern linguistics. It is 
worthwhile to observe that within this framework as well, the model complexity 
can be measured in some fashion, e.g. the number, and nature of the princi-
ples (parameters), the Kolmogorov complexity of the grammatical class, and so 
on. The exact nature of the relationship between this model complexity, and 
the number of examples needed to learn was not investigated explicitly, and 
remains an important area for further research. 
Manner and Nature of Examples: The informational complexity of learn-
ing from examples clearly depends upon the nature of the examples, and the 
manner in which they are provided to the learner. In every case we have treated 
in this book, examples were (x, y) pairs consistent with some target function. 
There were slight differences, however, between the specific instances examined 
in the different chapters. For the case of regularization networks, these exam-
ples were contaminated with noise. For the case of languages investigated later, 
only positive examples were presented ( i.e., all examples had the y-value of 1). 
A more interesting observation to make on the question of examples is our 
inherently stochastic formulation of the problem. Examples were typically ran-
domly drawn. This was according to some unknown distribution for regu-
larization networks, and the language learner; and according to some known 
distribution in the case of the active function approximator (learner) of chap-
ter 3. Such a stochastic formulation is very much in keeping with the spirit of 
PAC learning, which has influenced much of this work. Furthermore, it allows 
us to to take recourse to laws of large numbers, and better characterize rates 
of convergence, and thereby sample complexity. 
A few further observations need to be made. First, a stochastic formulation 
is not always utilized for investigation of learning paradigms. For example, in 
typical language learning research in an inductive inference setting, the learner 
is required to converge on every training sequence. The rates of convergence 
of such a learner are hard to characterize unless one puts a measure on the 
training sequences. This brings us back to a probabilistic framework, and 
indeed, such extensions have been considered in the past. Second, we observed 
that if examples are chosen by the learner (rather than passively drawn), they 
could potentially learn faster. Of course, this need not always be the case, 

CONCLUSIONS 
209 
and explicit formal studies are required to decide one way or the other. Third, 
the actual number of examples required (in a passive setting) depends upon 
the distribution with which data is presented to the learner. In the case of 
regularization networks, we were able to obtain distribution-free bounds, but 
these are only bounds, and as various researchers have noted, are often weak. 
For language learning in finite parameter cases, we see this dependence on 
distributions explicitly. We notice here that no distribution-free bound exists. 
The Learning Algorithm Used: The informational complexity of learning 
also depends upon the kind of algorithm the learner uses in making its hypothe-
ses about the target. A poorly motivated algorithm might not even converge 
to the target (as the data goes to infinity), let alone do this in reasonable 
time. Again, this is not particularly surprising, and the point becomes vacuous 
without explicit characterization of the relationship (between algorithm and 
sample complexity) in some form. The degree of constraints on the learning 
algorithms we examined, varied from chapter to chapter. For the case of regu-
larization networks, our results were valid for any algorithm which minimized 
a mean-square error term. Though, we did not prove it in the book, it turns 
out that algorithms minimizing cross entropy terms (for pattern classification) 
are covered in the analysis as well. In our investigation of active learning, we 
considered the approximation scheme (a component of the learning algorithm) 
explicitly. As a matter of fact, all comparisons between passive and active 
methods of data collection were made between learners using the same approx-
imation scheme (thereby eliminating the influence of the approximation scheme 
on sample complexity). Active and passive learners represent two significantly 
different kinds of learning algorithms. We saw how to derive an active scheme 
from a passive one in a function approximation setting, and how such a scheme 
could then potentially reduce the informational complexity of learning. For 
language learning, we were able to show that all memoryless algorithms could 
be modeled as a Markov chain. However, the transition probabilities of these 
chains depended on the specific nature of the algorithms. We explicitly com-
puted these transition matrices for a number of variants of the TLA (single 
step, greedy ascent) and showed how the sample complexity seemed to vary for 
the same task. It should also be noted that our analysis scheme in language 
learning (Markov chains) were derived from the learning algorithms used. In 
this sense, the sample complexity results were sharp (exact). This is in contrast 
to bounds on the sample complexity which can be obtained by using uniform 
convergence type arguments, or other techniques. 
Learnability and Evolution: An important connection between learning 
systems and evolutionary systems emerged towards the end of this book. Both 
kinds of systems are adaptive ones. However, according to our analysis here, 
learning occurs at the level of the individual, evolution at the level of the 
population. Clearly, the two interact- and an information-theoretic point of 
view is important for an understanding of such interactions. The manner, 
nature, and number, of examples, the complexity of the hypothesis spaces, the 
learning algorithms used have implications for global evolutionary trajectories 
of populations of learners. In this sense, a theory of learning which attempts 

210 
INFORMATIONAL COMPLEXITY OF LEARNING 
to explain individual behavior logically implies a certain group behavior. We 
demonstrated this connection explicitly for the human language system. This 
kind of evolutionary analysis of learning systems could serve as an important 
research tool in a number of different contexts. Certainly, in economic systems, 
one could examine the evolution (adaptation) of the global (macro) economy 
as a result of the behavior (also adaptive) of the individual economic agents. 
6.2 
EXTENSIONS 
The previous section described the broad results, and the emergent perspective 
of this book. With this perspective, one could proceed in several directions. 
1. Model Selection: At a fundamental level, one could examine the question of 
model selection in general. In the cases we considered, the models (family of 
functions, or hypothesis classes) were homogeneous (in fact, often parameter-
ized) , i.e., all functions in our hypothesis class had the same representation 
(as regularization networks, parameterized grammars, or spline functions 
etc.). The task of learning reduced primarily to the task of estimating the 
values of the parameters. What if we have qualitatively different kinds of 
models? Instead of choosing the best hypothesis h E 1{ as all our learning 
problems were posed, what if we were interested in choosing the best class 
1{ E 1{uJper 7 To make matters a little concrete, suppose one were inter-
ested not in choosing the best regularization network for a certain problem, 
but in deciding whether regularization networks were best for that problem 
(other candidate models might include multi-layer perceptrons, or polyno-
mials, etc .. )7 In the case of languages, one might be interested in choosing 
between bigrams, context-free grammars, parameterized theories, etc. How 
does one characterize the complexity of the super class 1{.uper whose in-
dividual elements are not functions but classes of functions (models)? For 
that matter, how does one measure the distance between two models, i.e., 
d(1{1,1{2)7 This matter is of some interest in recent times as increased 
computational power has made it possible for researchers to literally "throw 
models at the data" in their frantic search for "good" ones? This is also a 
subject of interest to researchers in the field of data mining. 
2. Informational Complexity of Grammars: Another fruitful area of research 
is the informational complexity of learning grammars. As has been men-
tioned earlier, most language learning research tends to focus on the Gold 
paradigm of identification in the limit without due attention to the rates at 
which the learner attains the target. Given the arguments of "poverty of 
stimulus" invoked in the modern approach to linguistics, an informational 
perspective is bound to be of some value in choosing between alternate the-
ories. For example, what is the sample complexity of learning bigrams, 
trigrams, lexical-functional grammars, metrical stress patterns, optimality 
theory etc.? How does it depend upon the algorithms used, noise, sentence 
distributions? What are psychologically plausible algorithms? What are 
"real" sentence distributions like? Quantitative answers to these questions 

CONCLUSIONS 
211 
would considerably aid the search for the right linguistic theory. One could 
also potentially decompose the language learning problem into approxima-
tion and estimation parts. For example, by analogy with our analysis of 
regularization networks, we can pose the following simple problem. Let Mn 
be class of all finite state grammars with at most n states (analogous to 
Hn: networks with at most n hidden units). Let M = U~=lMn' Let M 
(analogous to :1') be some class which can be approximated by M (it could 
simply be M itself). Let examples be sentences drawn from some target 
grammar m EM. Then how many states (n) must we have, and how many 
examples must we draw so that with high confidence, the learner's grammar 
(extensionally) is f close to m with high confidence? 
3. Evolutionary Systems: We argued, in chapter 5, that specific assumptions 
about linguistic theories, and learning paradigms, leads automatically to a 
model of language change. We were able to transform memoryless algo-
rithms operating on finite parameter spaces into explicit dynamical systems. 
There are several interesting directions to pursue within this area of research. 
First, one could attempt to obtain similar dynamical systems correspond-
ing to other assumptions about linguistic theories, and learning algorithms. 
Second, from a purely mathematical perspective, it would be interesting to 
study the classes of dynamical systems motivated by linguistics. For exam-
ple, we saw that the the systems for finite parameter spaces were non-linear, 
and multi-dimensional. The mathematical characterization of such systems 
are far from trivial. Finally, of course, one must attempt to put such evo-
lutionary models to good scientific use, by validating against real cases of 
language change. Such an enterprise, will hopefully result in a mathemati-
cally productive, and scientifically fruitful study of historical linguistics. In 
general, the connection between individual learning, and group evolution is 
an interesting one. It can be studied in other contexts, and formal connec-
tions between learning theory, and evolutionary theory need to be developed 
further. 
4. Computational Complexity: This book focused almost exclusively on the 
number of examples needed so that the learner's hypothesis is close to the 
target. The computational complexity of choosing a hypothesis (once the 
requisite number of examples have been provided) is a matter of great impor-
tance, and largely ignored in this book. For example, our main theorem in 
Chapter 2 assumes that the learner will be able to find the global minimum 
of the mean-square error term. In general, this problem, as we have noted, 
is likely to be N P-hard. Similarly, in Chapter 3, the active learner reduces 
the informational complexity at the cost of increasing the computational 
burden. For the cases we examined, an analytical solution to the sequen-
tial optimal recovery problem allowed us to obtain tractable solutions. In 
general, however, the complexity of solving the optimal recovery equations 
(and recovering the optimal point to sample at each stage) could well be 
intractably high. Further, in the case of language learning, we obtained 
sample complexity bounds which were tuned to specific algorithms known 

212 
INFORMATIONAL COMPLEXITY OF LEARNING 
to be feasible, and psychologically plausible. These algorithms, of course, 
don't learn every possible parameterized space. The complexity of learning 
a parameterized space, in general, could well be N P-hard (scalability with 
respect to number of parameters, and examples). These are directions worth 
pursuing. After all, a truly realistic cognitively plausible theory of human 
learning should require not only a feasible number of examples, but should 
also have low computational (cognitive) burden. 
6.3 A CONCLUDING NOTE 
We started out trying to investigate the informational complexity of learning. 
The problem was studied in several different contexts for widely varying func-
tion classes. Interestingly enough, in each such context, we witnessed a tension 
between competing forces. In chapter 2, we saw the approximation-estimation 
trade-off. In chapter 3, the active learner reduced informational complexity, 
but potentially increased computational complexity since it now had to per-
form elaborate computations to decide where to sample next. In language, 
grammatical hypotheses had to be rich enough to describe all the languages of 
the world, yet constrained enough to be learnable. Finally, in chapter 5, we 
saw that a theory of language had to be tight enough to allow learn ability, yet 
loose enough to allow change. 
This book is not just about informational complexity -
it is also about 
trade-offs. The investigations of the previous chapters highlight the tensions 
that need to be compromised and the balance that has to be struck in order to 
understand human learning and design better machine learning systems. 

References 
[1] 
Y.S. Abu-Mostafa. Hints and the VC-dimension. Neural Computation, 
5:278-288, 1993. 
[2] 
D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 
1988. 
[3] 
W. Arai. Mapping abilities of three-layer networks. In Proceedings of the 
International Joint Conference on Neural Networks, pages 1-419-1-423, 
Washington D.C., June 1989. IEEE TAB Neural Network Committee. 
[4] 
M. L. Athavale and G. Wahba. Determination of an optimal mesh for 
a collocation-projection method for solving two-point boundary value 
problems. Journal of Approximation Theory, 25:38-49, 1979. 
[5] 
R.L. Rivest B. Eisenberg. On the sample complexity of pac-learning using 
random and chosen examples. In Proceedings of the 1990 Workshop on 
Computational LearningTheory, pages 154-162, San Mateo, CA, 1990. 
Morgan Kaufmann. 
[6] 
A. Barron and T. Cover. Minimum complexity density estimation. IEEE 
Transactions on Information Theory, 37(4), 1991. 
[7] 
A.R. Barron. Universal approximation bounds for superpositions of a 
sigmoidal function. IEEE Transaction on Information Theory, 39(3):930-
945, May 1993. 
[8] 
A.R. Barron. Approximation and estimation bounds for artificial neural 
networks. Machine Learning, 14:115-133,1994. 
[9] 
E. B. Baum and D. Haussler. What size net gives valid generalization? 
In IEEE Int. Symp. Inform. Theory, Kobe, Japan, June 1988. 
[10] 
E. B. Baum and D. Haussler. What size net gives valid generalization? 
In D. S. Touretzky, editor, Advances in Neural Information Processing 
Systems I, pages 81-90. Morgan Kaufmann Publishers, Carnegie Mellon 
University, 1989. 
213 

214 
INFORMATIONAL COMPLEXITY OF LEARNING 
[11] 
R.E. Bellman. Adaptive Control Processes. Princeton University Press, 
Princeton, NJ, 1961. 
[12] 
J. Berntsen, T. O. Espelid, and A. Genz. An adaptive algorithm for 
the approximate calculation of multiple integrals. ACM Transactions on 
Mathematical Software, 17(4):437-451, December,1991. 
[13] 
R. C. Berwick. The Acquisition of Syntactic Knowledge. MIT Press, 
1985. 
[14] 
Derek Bickerton. Language and Species. Chicago University Press, 1990. 
[15] 
A. Blum and R. L. Rivest. Training a three-neuron neural net is NP-
complete. In Proceedings of the 1988 Workshop on Computational Learn-
ingTheory, pages 9-18, San Mateo, CA, 1988. Morgan Kaufma. 
[16] 
A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Classifying 
learnable geometric concepts with the vapnik-chervonenkis dimension. In 
Proc. of the 18th ACM STOC, Berkeley, CA, pages 273-282,1986. 
[17] 
S. Botros and C. Atkeson. Generalization properties of Radial Basis Func-
tion. In R. Lippmann, J. Moody, and D. Touretzky, editors, Advances in 
Neural information processings systems 9, San Mateo, CA, 1991. Morgan 
Kaufmann Publishers. 
[18] 
D.S. Broomhead and D. Lowe. Multivariable functional interpolation and 
adaptive networks. Complex Systems, 2:321-355, 1988. 
[19] 
R. Brunelli and T. Poggio. HyperBF Networks for Gender Classification. 
In Proceedings of the Image Understanding Workshop, pages 311-314, 
San Mateo, CA, 1992. Morgan Kaufmann. 
[20] 
L. Cavalli-Sforza and M. Feldman. Cultural Transmission and Evolution: 
A Quantitative Approach. Princeton University Press, Princeton, NJ, 
1981. 
[21] 
N. Chomsky. Aspects of the Theory of Syntax. Cambridge, MA: MIT 
Press, 1965. 
[22] 
N. Chomsky. Lectures on Government and Binding. Dordecht: Reidel, 
1981. 
[23] 
N. Chomsky and M Schutzenberger. The algebraic theory of context-free 
languages. Computer Programming and Formal Systems, pages 53-77, 
1963. 
[24] 
C. K. Chui and X. Li. Approximation by ridge functions and neural net-
works with one hidden layer. CAT Report 222, Texas A and M University, 
1990. 

REFERENCES 
215 
[25] 
R. Clark and I. Roberts. A computational model of language learnability 
and language change. Linguistic Inquiry, 24(2):299-345, 1993. 
[26] 
D. Cohn. Neural network exploration using optimal experiment design. 
AI memo 1491, Massachusetts Institute of Technology, 1994. 
[27] 
D. Cohn and G. Tesauro. Can neural networks do better than the VC 
bounds. In R. Lippmann, J. Moody, and D. Touretzky, editors, Advances 
in Neural information processings systems 3, pages 911-917, San Mateo, 
CA, 1991. Morgan Kaufmann Publishers. 
[28] 
P. Craven and G. Wahba. Smoothing noisy data with spline functions: 
estimating the correct degree of smoothing by the method of generalized 
cross validation. Numer. Math, 31:377-403, 1979. 
[29] 
G. Cybenko. Approximation by superposition of a sigmoidal function. 
Math. Control Systems Signals, 2(4):303-314, 1989. 
[30] 
C. deMarcken. Parsing the lob corpus. In Proceedings of the 25th Annual 
Meeting of the Association for Computational Linguistics., Pittsburgh, 
PA: ACL, 243-251, 1990. 
[31] 
R. DeVore, R. Howard, and C. Micchelli. Optimal nonlinear approxima-
tion. Manuskripta Mathematika, 1989. 
[32] 
R.A. DeVore. Degree of nonlinear approximation. In C.K. Chui, L.L. 
Schumaker, and D.J. Ward, editors, Approximation Theory, VI, pages 
175-201. Academic Press, New York, 1991. 
[33] 
R.A. DeVore and X.M. Yu. Nonlinear n-widths in Besov spaces. In C.K. 
Chui, L.L. Schumaker, and D.J. Ward, editors, Approximation Theory, 
VI, pages 203-206. Academic Press, New York, 1991. 
[34] 
L. Devroye. On the almost everywhere convergence of non parametric 
regression function estimate. The Annals of Statistics, 9:1310-1319, 1981. 
[35] 
B. E. Dresher and J. Kaye. A computational learning model for metrical 
phonology. Cognition, pages 137-195, 1990. 
[36] 
R. M. Dudley. 
Real analysis and probability. 
Mathematics Series. 
Wadsworth and Brooks/Cole, Pacific Grove, CA, 1989. 
[37] 
R.M. Dudley. Universal Donsker classes and metric entropy. Ann. Prob., 
14(4):1306-1326, 1987. 
[38] 
R.M. Dudley. Comments on two preprints: Barron (1991), Jones (1991). 
Personal communication, March 1991. 
[39] 
N. Dyn. Interpolation of scattered data by radial functions. In C.K. 
Chui, L.L. Schumaker, and F.I. Utreras, editors, Topics in multivariate 
approximation. Academic Press, New York, 1987. 

216 
INFORMATIONAL COMPLEXITY OF LEARNING 
[40] 
B. Efron. The jacknife, the bootstrap, and other resampling plans. SIAM, 
Philadelphia, 1982. 
[41] 
B. Eisenberg. Sample complexity of active learning. Master's thesis, 
Massachusetts Institute of Technology, Cambridge,MA, 1992. 
[42] 
G. Altmann et al. A law of change in language. In B. Brainard, editor, 
Historical Linguistics, pages 104-115, Studienverlag Dr. N. Brockmeyer., 
1982. Bochum, FRG. 
[43] 
R.L. Eubank. 
Spline Smoothing and Nonparametric Regression, vol-
ume 90 of Statistics, textbooks and monographs. Marcel Dekker, Basel, 
1988. 
[44] 
R. Frank and S. Kapur. On the use of triggers in parameter setting. Tech. 
Report 92-52, IRCS, University of Pennsylvania, 1992. 
[45] 
K. Funahashi. On the approximate realization of continuous mappings 
by neural networks. Neural Networks, 2:183-192, 1989. 
[46] 
S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the 
bias/variance dilemma. Neural Computation, 4:1-58, 1992. 
[47] 
E. Gibson and K. Wexler. Triggers. Linguistic Inquiry, 25, 1994. 
[48] 
F. Girosi. On some extensions of radial basis functions and their appli-
cations in artificial intelligence. Computers Math. Applic., 24(12):61-80, 
1992. 
[49] 
F. Girosi and G. Anzellotti. Rates of convergence of approximation 
by translates. A.1. Memo 1288, Artificial Intelligence Laboratory, Mas-
sachusetts Institute of Technology, 1992. 
[50] 
F. Girosi and G. Anzellotti. Rates of convergence for radial basis func-
tions and neural networks. In R.J. Mammone, editor, Artificial Neural 
Networks for Speech and Vision, pages 97-113, London, 1993. Chapman 
& Hall. 
[51] 
F. Girosi, M. Jones, and T. Poggio. Priors, stabilizers and basis functions: 
From regularization to radial, tensor and additive splines. A.1. Memo 
No. 1430, Artificial Intelligence Laboratory, Massachusetts Institute of 
Technology, 1993. 
[52] 
H. Gish. A probabilistic approach to the understanding and training 
of neural network classifiers. In Proceedings of the ICASSP-90, pages 
1361-1365, Albuquerque, New Mexico, 1990. 
[53] 
E. M. Gold. Language identification in the limit. Information and Con-
trol, 10:447-474, 1967. 

REFERENCES 
217 
[54] 
Z. Govindarajulu. Sequential Statistical Procedures. Academic Press, 
1975. 
[55] 
U. Grenander. On empirical spectral analysis of empirical processes. Ark. 
Matemat., 1:503-531, 1951. 
[56] 
L. Haegeman. Introduction to Government and Binding Theory. Black-
well: Cambridge, USA, 1991. 
[57] 
M. Halle and W. Idsardi. General properties ofstress and metrical struc-
ture. In DIMACS Workshop on Human Language, Princeton, NJ, 1991. 
[58] 
H. Hamburger and Kenneth Wexler. A mathematical theory of learning 
transformational grammar. Journal of Mathematical Psychology, pages 
137-177, 12. 
[59] 
R.L. Hardy. Multiquadric equations of topography and other irregular 
surfaces. J. Geophys. Res, 76:1905-1915,1971. 
[60] 
R.L. Hardy. Theory and applications of the multiquadric-biharmonic 
method. Computers Math. Applic., 19(8/9):163-208, 1990. 
[61] 
E. Hartman, K. Keeler, and J.M. Kowalski. Layered neural networks 
with gaussian hidden units as universal approximators. (submitted for 
publication), 1989. 
[62] 
D. Haussler. Decision theoretic generalizations of the PAC model for 
neural net and other learning applications. Technical Report UCSC-
CRL-91-02, University of California, Santa Cruz, 1989. 
[63] 
D. Haussler. Decision theoretic generalizations of the PAC model for neu-
ral net and other learning applications. Information and Computation, 
100(1):78-150,1992. 
[64] 
J.A. Hawkins and M. Gell-Mann. Evolution of Human Languages. Santa 
Fe Series, Addison-Wesley, Reading, MA, 1992. 
[65] 
J .A. Hertz, A. Krogh, and R. Palmer. Introduction to the theory of neural 
computation. Addison-Wesley, Redwood City, CA, 1991. 
[66] 
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward net-
works are universal approximators. Neural Networks, 2:359-366, 1989. 
[67] 
J. B. Hampshire II and B. A. Pearlmutter. Equivalence proofs for mul-
tilayer perceptron classifiers and the bayesian discriminant function. In 
J. Elman D. Touretzky and G. Hinton, editors, Proceedings of the 1990 
Connectionist Models Summer School, San Mateo, CA, 1990. Morgan 
Kaufman. 
[68] 
B. Irie and S. Miyake. Capabilities of three-layered Perceptrons. IEEE 
International Conference on Neural Networks, 1:641-648, 1988. 

218 
INFORMATIONAL COMPLEXITY OF LEARNING 
[69] 
D. Isaacson and J. Masden. Markov Chains. John Wiley, New York, 
1976. 
[70] 
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. F. Hinton. Adaptive 
mixtures of local experts. Neural Computation, 3:79-87, 1991. 
[71] 
C. Ji and D. Psaltis. The VC dimension versus the statistical capacity of 
multilayer networks. In S. J. Hanson J. Moody and R. P. Lippman, ed-
itors, Advances in Neural Information Processing Systems 4, pages 928-
935, San Mateo, CA, 1992. Morgan Kauffmann. 
[72] 
L.K. Jones. A simple lemma on greedy approximation in Hilbert space 
and convergence rates for projection pursuit regression and neural net-
work training. The Annals of Statistic, 1990. (to appear). 
[73] 
L.K. Jones. A simple lemma on greedy approximation in Hilbert space 
and convergence rates for Projection Pursuit Regression and neural net-
work training. The Annals of Statistics, 20(1):608-613, March 1992. 
[74] 
S. Judd. Neural Network Design and the Complexity of Learning. PhD 
thesis, University of Massachusetts, Amherst, Amherst, MA, 1988. 
[75] 
S. Kapur. Computational Language Learning. PhD thesis, Cornell Uni-
versity, Ithaca, NY, 1992. 
[76] 
M. Kearns. Efficient noise-tolerant learning from statistical queries. In 
Proceedings of the 1993 STOC, pages 392-401, 1993. 
[77] 
M. J. Kearns and R. E. Schapire. Efficient distribution-free learning of 
probabilistic concepts. In Proceedings of the 1990 FOCS, pages 382-391, 
1990. 
[78] 
M. Kenstowicz. Phonology in Generative Grammar. Blackwell Publish-
ers, Cambridge, MA, 1994. 
[79] 
D. Kimber and P. M. Long. The learning complexity of smooth functions 
of a single variable. In Proceedings of the 1992 Workshop on Computa-
tional Learning Theory, pages 153-159, San Mateo, CA, 1992. Morgan 
Kaufmann. 
[80] 
A. S. Kroch. Grammatical theory and the quantitative study of syntactic 
change. In Paper presented at NWAVE 11, Georgetown Universtiy, 1982. 
[81] 
A. S. Kroch. Function and gramar in the history of english: Periphrastic 
"do.". In Ralph Fasold, editor, Language change and variation. Amster-
dam:Benjamins. 133-172, 1989. 
[82] 
Anthony S. Kroch. Reflexes of grammar in patterns of language change. 
Language Variation and Change, pages 199-243, 1990. 

REFERENCES 
219 
[83] 
A. Krzyzak. The rates of convergence of kernel regression estimates 
and classification rules. IEEE Transactions on Information Theory, IT-
32(5):668-679, September 1986. 
[84] 
S. Kulkarni, S.K. Mitter, and J.N. Tsitsiklis. Active learning using arbi-
trary binary valued queries. Machine Learning, 11:23-36, 1993. 
[85] 
A. Lapedes and R. Farber. How neural nets work. In Dana Z. Anderson, 
editor, Neural Information Processing Systems, pages 442-456. Am. Inst. 
Physics, NY, 1988. Proceedings of the Denver, 1987 Conference. 
[86] 
E. Levin, N. Tishby, and S. A. Solla. A statistical approach to learning 
and generalization in layered neural networks. Proceedings of the IEEE, 
78(10):1568-1574, October 1990. 
[87] 
D. Lightfoot. How to Set Parameters. MIT Press, Cambridge, MA, 1991. 
[88] 
H. Linhart and W. Zucchini. Model Selection. John Wiley and Sons" 
1986. 
[89] 
R. P. Lippmann. An introduction to computing with neural nets. IEEE 
ASSP Magazine, pages 4-22, April 1987. 
[90] 
N. Littlestone, P. M. Long, and M. K. Warmuth. On-line learning of 
linear functions. In Proceedings of the 1991 STOC, pages 465-475, 1991. 
[91] 
G. G. Lorentz. Metric entropy, widths, and superposition of functions. 
Amer. Math. Monthly, 69:469-485, 1962. 
[92] 
G. G. Lorentz. Approximation of Functions. Chelsea Publishing Co., 
New York, 1986. 
[93] 
D. Mackay. Bayesian Methods for Adaptive Models. PhD thesis, Califor-
nia Institute of Technology, Pasadena, CA, 1991. 
[94] 
B. MacWhinney. CHILDES: A toolforstudying language input. Erlbaum, 
Hillsdale, NJ, 1990. 
[95] 
David Marr. 
Vision: A Computational Investigation into the Human 
Representation and Processing of Visual Information. W.H. Freeman 
and Company, San Francisco, 1982. 
[96] 
H.N. Mhaskar. Approximation properties of a multilayered feedforward 
artificial neural network. Advances in Computational Mathematics, 1:61-
80, 1993. 
[97] 
H.N. Mhaskar and C.A. Micchelli. Approximation by superposition of a 
sigmoidal function. Advances in Applied Mathematics, 13:350-373, 1992. 
[98] 
C. A. Micchelli. Interpolation of scattered data: distance matrices and 
conditionally positive definite functions. 
Constructive Approximation, 
2:11-22, 1986. 

220 
INFORMATIONAL COMPLEXITY OF LEARNING 
[99] 
C. A. Micchelli and G. Wahba. Design problems for optimal surface in-
terpolation. In Z. Ziegler, editor, Approximation theory and applications, 
pages 329-348. Academic Press, New York, 1981. 
[100] C.A. Micchelli and T.J. Rivlin. A survey of optimal recovery. In C.A. 
Micchelli and T.J. Rivlin, editors, Optimal Estimation in Approximation 
Theory, pages 1-54. Plenum Press, New York, 1976. 
[101] T. M. Mitchell, J. G. Carbonell, and R. S. Michalski. Machine Learning: 
A Guide to Current Research. Kluwer Academic Publishers, 1986. 
[102] J. Moody. The effective number of parameters: An analysis of generaliza-
tion and regularization in non-linear learning systems. In S. J. Hanson 
J. Moody and R. P. Lippman, editors, Advances in Neural information 
processings systems 4, pages 847-854, San Mateo, CA, 1992. Morgan 
Kaufman. 
[103] J. Moody and C. Darken. Fast learning in networks of locally-tuned 
processing units. Neural Computation, 1(2):281-294, 1989. 
[104] P. Niyogi and R. C. Berwick. Formalizing triggers: A learning model for 
finite parameter spaces. Tech. Report 1449, AI Lab., M.LT., 1993. 
[105] P. Niyogi and R. C. Berwick. A language learning model for finite pa-
rameter spaces. Cognition, 61(1):161-193,1996. 
[106] M. Opper and D. Haussler. Calculation of the learning curve ofbayes op-
timal class algorithm for learning a perceptron with noise. In Proceedings 
of COLT, Santa Cruz, CA, pages 75-87, San Mateo, CA, 1991. Morgan 
Kaufmann Publishers. 
[107] D. Osherson, M. Stob, and S. Weinstein. Systems That Learn. MIT 
Press, Cambridge, MA, 1986. 
[108] A. Pinkus. N-widths in Approximation Theory. Springer-Verlag, New 
York, 1986. 
[109] G. Pisier. Remarques sur un resultat non publie de B. Maurey. In Centre 
de Mathematique, editor, Seminarie d'analyse fonctionelle 1980-1981, 
Palaiseau, 1981. 
[110] T. Poggio and F. Girosi. Networks for approximation and learning. Pro-
ceedings of the IEEE, 78(9), September 1990. 
[111] T. Poggio and F. Girosi. Regularization algorithms for learning that are 
equivalent to multilayer networks. Science, 247:978-982, 1990. 
[112] T. Poggio and F. Girosi. Networks for Approximation and Learning. 
In C. Lau, editor, Foundations of Neural Networks, pages 91-106. IEEE 
Press, Piscataway, NJ, 1992. 

REFERENCES 
221 
[113] T. Poggio and T. Vetter. Recognition and structure from one 2D model 
view: observations on prototypes, object classes and symmetries. A.1. 
Memo No. 1347, Artificial Intelligence Laboratory, Massachusetts Insti-
tute of Technology, 1992. 
[114] D. Pollard. Convergence of stochastic processes. Springer-Verlag, Berlin, 
1984. 
[115] M.J .D. Powell. The theory of radial basis functions approximation in 
1990. Technical Report NA11, Department of Applied Mathematics and 
Theoretical Physics, Cambridge, England, December 1990. 
[116] S. Resnick. Adventures in Stochastic Processes. Birkhauser, 1992. 
[117] M. D. Richard and R. P. Lippman. Neural network classifier estimates 
bayesian a-posteriori probabilities. Neural Computation, 3:461-483, 1991. 
[118] J. Rissanen. A universal prior for integers and estimation by minimum 
description length. The Annals of Statistics, 11:416-431, 1983. 
[119] J. Rissanen. Stochastic Complexity in Statistical Inquiry. World Scien-
tific, 1989. 
[120] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning repre-
sentations by back-propagating errors. Nature, 323(9):533-536, October 
1986. 
[121] M. Stone. Cross-validatory choice and assessment of statistical predic-
tors(with discussion). J. R. Statist. Soc., B36:111-147, 1974. 
[122] S. Strogatz. Nonlinear Dynamics and Chaos. Addison-Wesley, 1993. 
[123] K. K. Sung and P. Niyogi. An active formulation for approximation of 
real valued functions. In Advances in Neural information processings 
systems 7, 1994. 
[124] R. W. Liu T. P. Chen, H. Chen. A constructive proof of approximation 
by superposition of sigmoidal functions for neural networks. Preprint, 
1990. 
[125] A. N. Tikhonov. Solution of incorrectly formulated problems and the 
regularization method. Soviet Math. DoH., 4:1035-1038, 1963. 
[126] A.F. Timan. Theory of approximation of functions of a real variable. 
Macmillan, New York, 1963. 
[127] J. F. Traub, G. W. Wasilkowski, and H. Wozniakovski. Information Based 
Complexity. Academic Press, New York, 1988. 
[128] L.G. Valiant. A theory of learnable. Proc. of the 1984 STOC, pages 
436-445, 1984. 

222 
INFORMATIONAL COMPLEXITY OF LEARNING 
[129] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New 
York, 1995. 
[130] V. N. Vapnik. Estimation of Dependences Based on Empirical Data. 
Springer-Verlag, Berlin, 1982. 
[131] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of 
relative frequences of events to their probabilities. Th. Prob. and its 
Applications, 17(2):264-280, 1971. 
[132] V.N. Vapnik and A. Ya. Chervonenkis. The necessary and sufficient con-
ditions for the uniform convergence of averages to their expected values. 
Teoriya Veroyatnostei i Ee Primeneniya, 26(3):543-564, 1981. 
[133] V.N. Vapnik and A. Va. Chervonenkis. The necessary and sufficient 
conditions for consistency in the empirical risk minimization method. 
Pattern Recognition and Image Analysis, 1(3):283-305, 1991. 
[134] G. Wahba. Splines Models for Observational Data. Series in Applied 
Mathematics, Vol. 59, SIAM, Philadelphia, 1990. 
[135] A. S. Weigand, D. E. Rumelhart, and B. A. Huberman. Generalization 
by weight elimination with applications to forecasting. In R. Lippmann, 
J. Moody, and D. Touretzky, editors, Advances in Neural information 
processings systems 3, San Mateo, CA, 1991. Morgan Kaufmann Pub-
lishers. 
[136] K. Wexler and P. Culicover. Formal Principles of Language Acquisition. 
MIT Press, Cambridge, MA, 1980. 
[137] H. White. Connectionist nonparametric regression: Multilayer percep-
trons can learn arbitrary mappings. Neural Networks, 3(535-549), 1990. 

Index 
Absorbing state, 140, 141 
Acquisition Based Model of Change, 175 
Active Approximation, 77 
Active learner, 76, 88 
Adaptive integration, 85 
Approximation error, 23, 31 
Approximation-Estimation Decomposition, 
32, 56 
Batch learning bounds, 162 
Bengali parameters, 129 
Bias-Variance trade-off, 39 
Bickerton, 175 
Bounded Derivative Functions, 100 
Change in 3 parameter grammatical sys-
tern, 183 
Child language sentence types, 160 
Childes corpus, 160 
Choose and Learn Algorithm (CLA), 90 
Closed set of states, 140, 142 
Computational framework for language 
change, 182 
Concept class, 22 
Concept classes, 3 
Concept learning, 84 
Convergence of Markov language model, 
153 
Covering Number, 60 
Creole Hypothesis, 175 
Diachronic criteria, 204 
Distributional effects on language learn-
ing,159 
Dynamical system model for language 
change, 178 
Empirical Risk, 28, 30 
English parameters, 129 
Epsilon Focusing, 117 
Epsilon region of interest, 120 
Estimation error, 23,32 
Estimator, 26 
Evolution of French, 196 
Evolutionary criterion, 174 
Evolutionary Trajectories, 190, 193 
Expected Risk, 26, 28 
Function learning, 22 
Generalization, 6 
Generalization error, 30-32, 72 
Gold learnability, 6 
Gold learning, 138 
Historical Data for French, 198 
Historical Linguistics, 203 
Homogeneous Populations, 183 
Hypothesis class, 5, 22 
Hypothesis complexity, 23 
Identification in the limit, 138 
Information source, 4 
Informational Complexity, 7 
Kernel regression, 40 
Language Acquisition, 2 
Language change in parametric systems, 
181 
Language learning convergence times, 148 
Learnability, 6 
Learning from Hints, 85 
Learning paradigm, 3 
Loss of null subjects in French, 198 
Loss of V2 in French, 198 
Markov chain convergence, 148 
Markov chain formulation, 137, 139, 140 
Maturational Time, 190 
Memoryless algorithms and Markov chains, 
167 
223 

224 
INFORMATIONAL COMPLEXITY OF LEARNING 
Metric capacity, 50 
Metric entropy, 58 
Metrical Phonology, 132 
Monotonic Function Learning, 86 
Neural Networks, 11 
Non-homogeneous Populations, 192 
Optimal Network Size, 41 
Optimal Recovery, 83 
Osherson etal, 2 
P-PAC learning, 19 
PAC, 6 
PAC language learning, 162 
Parameter setting, 126 
Parametric Grammars, 13 
Parametric Hypotheses, 11 
Passive learner, 15, 80 
Phase-Space Plots, 192 
Population Dynamics, 111 
Poverty of Stimulus, 126 
Poverty of stimulus, 9 
Primary Linguistic Data, 116 
Principles and Parameters, 128 
Proof of language learning, 168 
Radial Basis Functions, 29, 34 
Recurrent states, 169 
Regression function, 21, 28 
Regularization, 38 
S-shaped curve, 184 
Sample complexity, 23 
Sentence types in parametric space, 161 
Sequential Optimal Recovery, 84 
Simulations for change in French, 199 
Sobolev Space, 35, 52, 54 
Stability in grammaticaipopulations, 195 
State space of language change model, 118 
Structural Risk Minimization, 31 
Time course of language change, 189 
Transformational rules, 129 
Transient states, 169 
Triggering Learning Algorithm (TLA), 
138 
Triggers, 139 
Uniform convergence, 28, 31, 62 
Uniform Law of Large Numbers, 58 
Update rule for language change, 119 
Update rule for parametric change, 181 
V2 Movement, 132 
Vapnik, 28, 32, 62 
VC-dimension, 62 
X-bar theory, 129 

