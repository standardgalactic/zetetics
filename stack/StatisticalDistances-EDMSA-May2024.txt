Some recent results on 
statistical distances
Principles and geometry 
Frank Nielsen
Sony Computer Science Laboratories Inc
Tokyo, Japan
May 2024

Outline
I. Background:
‚Ä¢ Divergences are everywhere in information sciences for different purposes! 
‚Ä¢ Principles of 
‚ë†f-divergences
‚ë°Bregman divergences
‚ë¢Rao distances
II. In this talk, three recent advances and concepts:
1.
Fisher-Rao distances and projective distances
2.
Bregman divergences and comparative convexity
3.
f-divergence analysis via maximal invariant

Why statistical distances in information sciences?
‚Ä¢ Probability theory: convergence theorems wrt probability metrics
‚Ä¢ Statistics:
‚Ä¢ Divergence-based estimators: dissimilarities between empirical distributions and models
‚Ä¢ Scoring rules: evaluates forecasts, probabilistic predictions 
‚Ä¢ Information theory: Mutual information of random variables
‚Ä¢ Signal processing: Decompositions, approximate matrix factorization: NMF        
via Œ≤-divergence in sound processing, etc.
‚Ä¢ Machine learning, pattern recognition: Loss functions for training models, 
optimal transport, Integral probability metrics (MMDs)
‚Ä¢ Information geometry: canonical divergences of geometric structures, geometry 
of divergences

Statistical distances in information sciences
Probability theory
Statistics
Information theory
Signal processing
Machine learning
Pattern recognition
Optimal transport
Information geometry

Historically, many distances with purposes‚Ä¶
https://franknielsen.github.io/Divergence/index.html

Seeking principles and properties of distances
‚Ä¢
Csisz√°r,  "On information-type measure of difference of probability distributions and indirect observations," Studia Sci. Math. Hungar. 2 (1967).
‚Ä¢
Bregman, "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", USSR computational 
mathematics and mathematical physics 7.3 (1967)
‚Ä¢
Kantorovich, "Mathematical methods of organizing and planning production," Management science 6.4 (1960) 
‚Ä¢
Nielsen, "An elementary introduction to information geometry", Entropy 22.10 (2020)
‚Ä¢ f-divergences (Ali-Silvey & Csisz√°r)
‚Ä¢ Only separable distances which are monotone under Markov kernels
‚Ä¢ Invariant under ``sufficiency‚Äô‚Äô
‚Ä¢ Bregman divergences
‚Ä¢ Only distances with right-sided centroids = centers of masses
‚Ä¢ Canonical divergences of dually flat spaces in information geometry
‚Ä¢ Transport distances including Wasserstein distances
‚Ä¢ Minimize transport cost wrt a ground distance
‚Ä¢ Fast regularized OT with dense plan, fast sparse reg. OT
f convex, strictly convex at 1
F strictly convex and smooth

Fisher-Rao geodesic metric distances
‚Ä¢ Statistical model {pŒª: Œª‚ààŒõ} with model dimension dim(Œõ)=m
‚Ä¢ Use Riemannian geodesic distance for the Fisher metric expressed in 
chart (Œª,Œõ) using Fisher information matrix:
‚Ä¢ Pro: 
‚Ä¢ Invariant to reparamerization: a geometric distance!
‚Ä¢ Role of Riemann-Christoffel, Ricci, sectional curvatures in statistics
‚Ä¢ Con: Can be difficult to calculate: eg, no formula for multivariate Normals, autodiff
Fisher length of a curve: 
Geodesic is locally length minimizing curve:
Fisher-Rao distance = Fisher length of geodesic
Geodesic wrt Fisher Levi-Civita connection:
Solve ODE with:
‚Ä¢
Initial value problem or
‚Ä¢
Boundary value problem

Only this band can be 
embedded as a 2D surface of 3D
Œ∫=-1/2
U

Precursors of statistical manifolds/information geometry 
[Hotelling 1930]
Space of statistical parameter
[Mahalanobis 1936]
Statistical field
[Rao 1945]
Population space

Fisher-Rao geometry: multivariate normals (MVNs)
Fisher information matrix  (vector, matrix):
Fisher metric tensor:
v= vector space Rd
V=Symmetric matrix 
vector space
Length element:
Non-constant sectional curvatures which can also be positive, not a NPC space (d>1)
[Skovgaard 1984]

Fisher-Rao geodesic equation for MVNs:
‚Ä¢ Consider either initial value conditions or boundary value conditions of 
ODE
‚Ä¢ Yet, once Fisher-Rao geodesics are known, integrate Fisher-Rao length 
elements to get Rao distance.
Using (vector, Matrix) parameterization:
[Initial values, 1987]
Source+tangent vector
Second-order ODE:
[Boundary values, 2023]
Source+target
MVN: No closed form yet‚Ä¶

Demos: MVN Fisher-Rao geodesics on bivariate 
normal Fisher-Rao manifold
Geodesics with initial values 
Geodesics with boundary values 

Strategy to get lower bounds on Fisher-Rao distances
‚Ä¢ Nash embedding theorem: A Riemannian manifold (M,g) of dimension m 
can always be embedded in Euclidean space (E,gEuclidean) of dimension 
O(m3). 
‚Ä¢ Let S={f(p), p ‚àà M} be the submanifold. If geodesics between f(p) and f(q) 
stay in S wrt gEuclidean then the manifold is totally geodesic
‚Ä¢ Riemannian distance in (E,gEuclidean) is ||f(p)-f(q)||2 
‚Ä¢ Rao distance in (M,g) lower bounded by ||f(p)-f(q)||2 :
‚Ä¢ We may embed (M,g) into high-dimensional non-Euclidean manifolds 
(M‚Äô,g‚Äô) too (next slide!) and get
œÅ(p,q) ‚â• ||f(p)-f(q)||2
œÅM(p,q) ‚â• œÅM‚Äô(f(p),f(q))
Figure from
[Zhong et al‚Äô 2018]
Intrinsic
2D manifold
2D manifold embedded in 3D

Diffeomorphic embeddings of MVN(d) onto SPD(d+1)
[Calvo & Oller 1990]
The diffeomorphisms {fŒ≤} foliates the SPD cone P(d+1)
Using half trace metric in P(d+1), get the following metrics on MVN(d):
When  Œ≤=1 (constant), we thus get  a Fisher isometric embedding
of MVN(d) into SPD(d+1):
Œ≤=1

Fisher-Rao MVN distance: A lower bound
[Calvo & Oller 1990]
‚Ä¢
Embed isometrically the Gaussian manifold N(d) into a submanifold 
of codimension 1 into the SPD cone of dimension d+1:
non-totally geodesic submanifold {f(N)}:
‚Ä¢
Use closed-form SPD geodesic in the (d+1)-dimensional cone:
‚Ä¢
SPD path is of length necessarily smaller than the MVN geodesic 
in submanifold f(N). Thus get a lower bound on Rao distance:
Œ≤=1

New fast distances between MV Normals
SPD(d+1) cone
Gaussian(d) manifold
‚ÄúFisher-Rao and pullback Hilbert cone distances on the multivariate Gaussian manifold with 
applications to simplification and quantization of mixtures", ICML TAG 2023
Projective Hilbert SPD distance:

‚Ä¢
MVN Fisher-Rao distance needs approximations 
by sampling geodesics, require all eigenspectrum of SPD matrices.
‚Ä¢
Hilbert SPD distance only requires to calculate extreme eigenvalues
(eg., power method iterations), + geodesics are in simple closed form
Comparison of geodesics  for two bivariate normal distributions shown by ellipses centered at means
A simple approximation method for the Fisher‚ÄìRao distance between multivariate normal distributions, 
Entropy 25.4 (2023): 654
Fisher-Rao and pullback Hilbert cone distances on the multivariate Gaussian manifold with applications to 
simplification and quantization of mixtures,  ICML TAG 2023.

Fisher-Rao distances for MultiVariate Normals
‚Ä¢ Geodesic equation solved for boundary values recently [Kobayashi 
2023]
‚Ä¢ Guaranteed 1+Œµ approximation algorithms [N1]
‚Ä¢ New fast distances based on Hilbert projective geometry of the 
symmetric positive-definite cone [N2]
‚Ä¢ General principles for approximating and bounding Fisher-Rao 
distances, specially when Fisher metric is Hessian:                      
‚Ä¢
[K] Kobayashi, Shimpei. "Geodesics of multivariate normal distributions and a Toda lattice type Lax pair", Physica Scripta 98.11 (2023)
‚Ä¢
[N1] "A simple approximation method for the Fisher‚ÄìRao distance between multivariate normal distributions", Entropy 25.4 (2023): 654  
‚Ä¢
[N2] ‚ÄúFisher-Rao and pullback Hilbert cone distances on the multivariate Gaussian manifold with applications to simplification and 
quantization of mixtures", Topological, Algebraic and Geometric Learning Workshops 2023. PMLR, 2023.
‚Ä¢
[N3] "Approximation and bounding techniques for the Fisher-Rao distances", arXiv:2403.10089 (2024)
[N3] 

Projective divergences beyond Hilbert/Birkhoff
Projective divergences are pseudo-divergences ÔÅä
which are invariant under rescaling of their arguments. 
= Divergences between rays on the positive measure 
orthant cone
For example, Cauchy-Schwarz divergence:
and H√∂lder divergences:
N, Sun, and Marchand-Maillet, "On H√∂lder projective divergences." Entropy 19.3 (2017)

Projective divergences for statistical inference
‚Ä¢ Half-sided projective divergences are useful for estimating 
parameters of densities which have intractable normalizers:
For example, Hyvarinen/Fisher divergence in score matching
Intractable model
because of Z
Empirical distribution
Since we have:
Estimate unnormalized models

What is a ‚ÄúStatistical manifold‚Äù?
Two meanings in the literature: 
‚ë†Typed statistical model manifold versus 
‚ë°manifold with a dualistic structure
‚Ä¢ ‚ë† Manifold of statistical models (points are typed as statistical 
models): 
For example, the Fisher-Rao manifold of Gaussian distributions
‚Ä¢ ‚ë°Manifold with a ``statistical structure‚Äô‚Äô (differential geometry): 
Lauritzen coined the term ‚Äústatistical manifold‚Äù for manifolds equipped 
with structure (g,C), a Riemannian metric tensor g and a totally 
symmetric cubic tensor C  (next slide!)
Lee  et al., "A statistical manifold framework for point cloud data", Intl Conf on Machine Learning, ICML 2022. 
Lauritzen, "Statistical manifolds", Differential geometry in statistical inference 10 (1987): 163-216
‚ë†
Figure from
[Cheng et al. 2017]

Dualistic structures of statistical manifolds:
Pure geometric structures
"The many faces of information geometry." Not. Am. Math. Soc 69.1 (2022): 36-45.
https://www.ams.org/journals/notices/202201/rnoti-p36.pdf
Œ±-manifold
0-manifold = Riemannian manifold
Œ± -geometry from (g,C) structure

Dually flat spaces (M,g, ‚àá , ‚àá *): Bregman manifolds
‚Ä¢ A connection ‚àáis flat 
if there exists a 
coordinate systemŒ∏
such that all 
Christoffel symbols 
vanish: Œì (Œ∏) =0.
‚Ä¢ Œ∏ is called ‚àá‚Äìaffine 
coordinate system 
‚Ä¢ ‚àá-geodesic solves as 
line segments
Also called Hessian manifolds
"On geodesic triangles with right angles in a dually flat space." Progress in Information Geometry: 
Theory and Applications, 2021 

Bregman manifolds in Statistics/ML
‚Ä¢ Whenever you have a strictly convex smooth function, you can build a dually 
flat space (M,g, ‚àá , ‚àá *).
‚Ä¢ Whenever  you have a dually flat space (M,g, ‚àá , ‚àá *), you can reconstruct
the convex conjugates F(Œ∏),  F*(Œ∑) , and the dual Bregman divergences
‚Ä¢ Thm: Bregman divergences = canonical divergences of dually flat spaces
‚Ä¢ In Stats/ML, we often consider exponential families with densities 
‚Ä¢ It turns out that the normalizer Z(Œ∏) partition function AND the log-
normalizer F(Œ∏)=log Z(Œ∏) cumulant function are both convex!
‚Ä¢ So we get two dually flat spaces built from Z or F yielding Bregman 
divergences. Classically DFS from F was considered‚Ä¶

Two Bregman manifolds from partition/cumulants 
functions of exponential families, two pairs of BDs
F(Œ∏)=log Z(Œ∏)   Z(Œ∏)=exp(F(Œ∏)=)    
And furthermore, we can define skewed Jensen divergences from the 
convex generators:
Including the symmetric Jensen divergence when ùõº=1/2: 
‚ë† 
‚ë° 
‚ë† 
‚ë° 

Statistical divergences corresponding to 
BDs wrt cumulant functions F of EFs 
Scaled Bhattacharyya/R√©nyi distances
Scaled skewed Jensen divergence 
for cumulant function F

Statistical divergences corresponding to 
BDs wrt partition functions Z of EFs 
Amari Œ±-divergences
Scaled skewed Jensen divergence 
for partition function Z

KLD between normalized and unnormalized densities
With generalized BDs to duo Bregman pseudo-divergences:
with
with
Kullback-Leibler divergence between two truncated densities of a 
same exponential family amount to a duo Bregman pseudo-divergence
Statistical divergences between densities of truncated exponential families with nested supports: Duo 
Bregman and duo Jensen divergences, Entropy 24.3 (2022) 

Comparative convexity: (M,N)-convexity
‚Ä¢ Definition: A function Z is (M,N)-convex iff for  in Œ± in [0,1]:
‚Ä¢ Ordinary convexity: (A,A)-convexity wrt to arithmetic weighted mean
‚Ä¢ Log-convexity: (A,G)-convexity wrt to A/geometric weighted means:
Ordinary convexity of a function:
for all t in [0,1]
for all t in [0,1]
for all t in [0,1]

Comparative convexity wrt quasi-arithmetic means
‚Ä¢ Kolmogorov-Nagumo-De Finitti quasi-arithmetic mean for a 
strictly monotone generator h(u):
‚Ä¢ Includes power means which are homogeneous means:
Include the geometric mean when p‚Üí0

Generalizing Bregman divergences with 
(M,N)-convexity
‚Ä¢ Skew Jensen divergence from (M,N) comparative convexity:
Non-negative for (M,N)-convex generators F, provided regular 
means M and N (e.g. power means)
Definition:
By analogy to limit of skewed Jensen divergences amount to 
forward/reverse Bregman divergences.

Generalizing Bregman divergences with 
quasi-arithmetic mean convexity
Amounts to a conformal Bregman divergence on monotonic representations:
With generator:
Remark: Conformal Bregman divergences may yield robustness in applications
Conformal factor
Shape retrieval using hierarchical total Bregman soft clustering, IEEE Transactions on pattern analysis and machine intelligence (2012) 

Maximal invariant: Definitions and Eaton theorem
‚Ä¢ Function f(x) is invariant under group action (G,‚àò) when f(g ‚àòx)=f(x)
‚Ä¢ A maximal invariant is a function h such that all orbits 
Ox={h(g ‚àòx) | g  G} for g in G have distinct values
‚Ä¢ Theorem [Eaton]: 
Any invariant function is a function of a maximal invariant
Eaton, Morris L. "Group invariance applications in statistics." IMS, 1989.

Maximal invariance:  A toy example
‚Ä¢ Consider function slope(x,y)=y/x and group of positive reals G=(R+,*,1)
‚Ä¢ Invariant under rescaling: slope(s ‚àò(x,y))=slope(s*x,s*y)=y/x
‚Ä¢ Slope = maximal invariant: group orbits have different values (=slopes)
‚Ä¢ Itakura-Saito divergence is a distance used in sound processing which is 
also invariant to rescaling:
‚Ä¢ Therefore Itakura-Saito divergence can be expressed as a function of 
max invariant slope: D(y:x)=h(slope(x,y)) with h(u)=u-log u-1

Maximal invariance: f-divergences between scale models
‚Ä¢ Compute relative entropy (Kullback-Leibler divergence), Hellinger, chi^2, 
etc between location-scale (elliptical) distributions
‚Ä¢ Consider scale families of probabilities:
‚Ä¢ Those f-divergences are all invariant under rescaling:
‚Ä¢ Thus, we express them using the maximal invariant: 
‚Ä¢ Calculate hp,f symbolic regression from data obtained by Monte Carlo 
integrations of the f-divergences. Useful in practice!
Centered normal distributions
Cauchy distributions
t-Students
N and Okamura, "On f-divergences between Cauchy distributions, " IEEE Transactions on Information Theory 69.5 (2022) 

Computing with maximal invariants
‚Ä¢ The Mahalanobis distance and Jensen-Shannon divergence (JSD) between 
two isotropic Gaussian distributions are invariant under translation: that is, 
the translation = action of group G=(R,+,0)
‚Ä¢ Mahalanobis distance is maximal invariant for the group action (after dD to 
1d reparameterization‚Ä¶)
‚Ä¢ Thus, we have JSD(N(Œºùüè, Œ£),N(Œº2 ,Œ£))=h(ŒîŒ£(Œºùüè, Œº2)) for a strictly 
monotone function h
‚Ä¢ JSD between Gaussians does not admit closed-form. Costly numerical 
approximations in practice! 
‚Ä¢ This formula structure result allows to compare exactly JSDs:
JSD(N1,N2)<JSD(N3,N4) ‚ÜîŒîŒ£(Œºùüè, Œº2)< ŒîŒ£(Œºùüë, Œº4)
Since Mahalanobis distanceŒîŒ£ can be calculated exactly!
N and Okamura, "On the f-divergences between densities of a multivariate location or scale family", Statistics and Computing 34.1 (2024) 

Maximal invariance and hyperbolic ML
‚Ä¢ Hot topic in ML: Discrete hierarchical graph structures can be 
embedded in continuous hyperbolic spaces for downstream tasks
‚Ä¢ We need statistical analysis in hyperbolic spaces
‚Ä¢ How to define probability distributions in hyperbolic spaces? and 
statistical distances between them?
N and  Okamura, "On the f-divergences between hyperboloid and Poincar√© distributions,"  GSI (2023)
Klein disk model
of hyperbolic geometry
Probability density

Poincar√© distributions in the upper plane 
Triplet = Maximal invariant
N and  Okamura, "On the f-divergences between hyperboloid and Poincar√© distributions,"  GSI (2023)
Tojo and Yoshino, Harmonic exponential families on homogeneous spaces, Information Geometry  (2021)
Poincar√© upper plane


Summary: Three concepts for distances/divergences
‚Ä¢ Divergences are everywhere in information sciences!
‚Ä¢ Fisher-Rao distances: lower bounds by submanifold embeddings.
For MV normals: fast Hilbert ‚ë† projective distance using only extreme eigenvalues
‚Ä¢ ‚ë° Maximal invariants for f-divergences and distances
‚Ä¢ Bregman divergences: 
‚Ä¢ canonical divergences of dually flat manifolds,
‚Ä¢ duo Bregman pseudo-divergences for calculating KLD between truncated EF densities
‚Ä¢ Generarized BDs from ‚ë¢ comparative convexity: conformal Bregman divergences

Some references covering concepts in this talk
‚Ä¢ Comparative convexity:
‚Ä¢ Divergences Induced by the Cumulant and Partition Functions of Exponential Families and Their 
Deformations Induced by Comparative Convexity, Entropy 26.3 (2024): 193
‚Ä¢ Generalizing skew Jensen divergences and Bregman divergences with comparative convexity, 
IEEE Signal Processing Letters 24.8 (2017): 1123-1127
‚Ä¢ Maximal invariant:
‚Ä¢ On the f-divergences between densities of a multivariate location or scale family, Statistics and 
Computing 34.1 (2024): 60
‚Ä¢ On f-divergences between Cauchy distributions, IEEE Transactions on Information Theory 69.5 
(2022): 3150-3171
‚Ä¢ Projective distance:
‚Ä¢ Fisher-Rao and pullback Hilbert cone distances on the multivariate Gaussian manifold with 
applications to simplification and quantization of mixtures,  ICML TAG 2023.
‚Ä¢ Clustering in Hilbert's projective geometry: The case studies of the probability simplex and the 
elliptope of correlation matrices, Geometric structures of information (2019): 297-331
‚Ä¢ On H√∂lder projective divergences, Entropy 19.3 (2017): 122

Thank you!

