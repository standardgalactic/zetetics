Resource-Eﬃcient Invariant Networks: Exponential Gains by
Unrolled Optimization
Sam Buchanan∗† ‡
Jingkai Yan† ‡
Ellie Haber§
John Wright† ‡ ¶
March 11, 2022
Abstract
Achieving invariance to nuisance transformations is a fundamental challenge in the construction of ro-
bust and reliable vision systems. Existing approaches to invariance scale exponentially with the dimension
of the family of transformations, making them unable to cope with natural variabilities in visual data such
as changes in pose and perspective. We identify a common limitation of these approaches—they rely on
sampling to traverse the high-dimensional space of transformations—and propose a new computational
primitive for building invariant networks based instead on optimization, which in many scenarios provides
a provably more eﬃcient method for high-dimensional exploration than sampling. We provide empirical
and theoretical corroboration of the eﬃciency gains and soundness of our proposed method, and demon-
strate its utility in constructing an eﬃcient invariant network for a simple hierarchical object detection
task when combined with unrolled optimization. Code for our networks and experiments is available at
https://github.com/sdbuch/refine.
1
Introduction
In computing with any kind of realistic visual data, one must contend with a dizzying array of complex
variabilities: statistical variations due to appearance and shape, geometric variations due to pose and
perspective, photometric variations due to illumination and cast shadows, and more. Practical systems cope
with these variations by a data-driven approach, with deep neural network architectures trained on massive
datasets.
This approach is especially successful at coping with variations in texture and appearance.
For invariance to geometric transformations of the input (e.g., translations, rotations, and scaling, as in
Figure 1(a-d)), the predominant approach in practice is also data-driven: the ‘standard pipeline’ is to deploy
an architecture that is structurally invariant to translations (say, by virtue of convolution and pooling),
and improve its stability with respect to other types of transformations by data augmentation.
Data
augmentation generates large numbers of synthetic training samples by applying various transformations
to the available training data, and demonstrably contributes to the performance of state-of-the-art systems
[CZMV+19; CKNH20; HMCZ+20]. However, it runs into a basic resource eﬃciency barrier associated with
the dimensionality of the set of nuisances: learning over a 푑-dimensional group of transformations requires both
data and architectural resources that are exponential in 푑[BJ17; CJLZ19; Sch19; NI20; CK21]. This is a major
obstacle to achieving invariance to large, structured deformations such as 3D rigid body motion (푑= 6),
homography (푑= 8), and linked rigid body motion [KZFM19] and even nonrigid deformations [ZMH15]
(푑≫8).
It is no surprise, then, that systems trained in this fashion remain vulnerable to adversarial
transformations of domain [FF15; KMF18; XZLH+18; AAG19; ALGW+19; AW19; ETTS+19]—it simply is
not possible to generate enough artiﬁcial training data to learn transformation manifolds of even moderate
dimension. Moreover, this approach is fundamentally wasteful: learning nuisances known to be present in
∗Corresponding author: s.buchanan@columbia.edu
†Department of Electrical Engineering, Columbia University
‡Data Science Institute, Columbia University
§NYU Tandon School of Engineering
¶Department of Applied Physics and Applied Mathematics, Columbia University
1
arXiv:2203.05006v1  [cs.CV]  9 Mar 2022

휀
★
휀COVER
휀
★
휀OPT ≫휀COVER
Complexity(OPT)
Complexity(COVER) ∼#iter ×

휀COVER
휀OPT
푑
≪1
2
3
4
5
6
101
103
105
107
Dimension 푑of 핋
Complexity
Covering
Optimization
(a) Translation (푑= 2)
(b) Euclidean (푑= 3)
(c) Similarity (푑= 4)
(d) Aﬃne (푑= 6)
(e)
(f)
Figure 1: Comparing the complexity of covering-based and optimization-based methods for invariant recognition
of a template embedded in visual clutter.
(a-d): We consider four diﬀerent classes of deformations that generate
the observation of the template, ranging across shifts, rotations, scale, and skew. The dimension 푑of the family of
transformations increases from left to right. (e): A geometric illustration of the covering and optimization approaches
to global invariance: in certifying that a query (labeled with a star) is a transformed instance of the template (at the base
point of the solid red/blue lines), optimization can be vastly more eﬃcient than covering, because it eﬀectively covers the
space at the scale of the basin of attraction of the optimization problem, which is always larger than the template’s associated
휀COVER. (f): Plotting the average number of convolution-like operations necessary to reach a zero-normalized cross-
correlation (ZNCC) of 0.9 between the template and a randomly-transformed query across the diﬀerent deformation
classes. Optimization leads to an eﬃciency gain of several orders of magnitude as the dimensionality of the family of
transformations grows. Precise experimental details are recorded in Appendix A.3.
the input data wastes architectural capacity that would be better spent coping with statistical variability in
the input, or learning to perform complex tasks.
These limitations of the standard pipeline are well-established, and they have inspired a range of
alternative architectural approaches to achieving invariance, where each layer of the network incorporates
computational operations that reﬂect the variabilities present in the data. Nevertheless, as we will survey
in detail in Section 2, all known approaches are subject to some form of exponential complexity barrier:
the computational primitives demand either a ﬁlter count that grows as exp(푑) or integration over a 푑-
dimensional space, again incurring complexity exponential in 푑. Like data augmentation, these approaches
can be seen as obtaining invariance by exhaustively sampling transformations from the 푑-dimensional space
of nuisances, which seems fundamentally ineﬃcient: in many concrete high-dimensional signal recovery
problems, optimization provides a signiﬁcant advantage over naive grid searching when exploring a high-
dimensional space [SER17; GBW19; CDHS21], as in Figure 1(e). This motivates us to ask:
Can we break the barrier between resource-eﬃciency and invariance using optimization as
the architectural primitive, rather than sampling?
In Figure 1, we conduct a simple experiment that suggests a promising avenue to answer this question in the
aﬃrmative. Given a known synthetic textured motif subject to an unknown structured transformation and
embedded in a background, we calculate the number of computations (convolutions and interpolations)
required to certify with high conﬁdence that the motif appears in the image. Our baseline approach is
2

template matching, which enumerates as many transformations of the input as are necessary to certify
the motif’s occurrence (analogous to existing architectural approaches with sampling/integration as the
computational primitive)—each enumeration requires one interpolation and one convolution. We compare
to a gradient-based optimization approach that attempts to match the appearance of the test image to
the motif, which uses three interpolations and several convolutions per iteration (and on the order of 102
iterations). As the dimensionality of the space of transformations grows, the optimization-based approach
demonstrates an increasingly-signiﬁcant eﬃciency advantage over brute-force enumeration of templates—
at aﬃne transformations, for which 푑= 6, it becomes challenging to even obtain a suitable transformation
of the template by sampling.
To build from the promising optimization-based approach to local invariance in this experiment to a full
invariant neural network architecture capable of computing with realistic visual data, one needs a general
method to incorporate prior information about the speciﬁc visual data, observable transformations, and tar-
get task into the design of the network. We take the ﬁrst steps towards realizing this goal: inspired by classical
methods for image registration in the computer vision literature, we propose an optimization formulation
for seeking a structured transformation of an input image that matches previously-observed images, and
we show how combining this formulation with unrolled optimization [GL10; OJMB+20; CCCH+21], which
converts an iterative solver for an optimization problem into a neural network, implies resource-eﬃcient and
principled invariant neural architectural primitives. In addition to providing network architectures incor-
porating ‘invariance by design’, this is a principled approach that leads to networks amenable to theoretical
analysis, and in particular we provide convergence guarantees for speciﬁc instances of our optimization
formulations that transfer to the corresponding unrolled networks. On the practical side, we illustrate how
these architectural primitives can be combined into a task-speciﬁc neural network by designing and evalu-
ating an invariant network architecture for an idealized single-template hierarchical object detection task,
and present an experimental corroboration of the soundness of the formulation for invariant visual motif
recognition used in the experiment in Figure 1. Taken altogether, these results demonstrate a promising
new direction to obtain theoretically-principled, resource-eﬃcient neural networks that achieve guaranteed
invariance to structured deformations of image data.
The remainder of the paper is organized as follows: Section 2 surveys the broad range of architectural
approaches to invariance that have appeared in the literature; Section 3 describes our proposed optimization
formulations and the unrolling approach to network design; Section 4 describes the hierarchical invariant
object detection task and a corresponding invariant network architecture; Section 5 establishes convergence
guarantees for our optimization approach under a data model inspired by the hierarchical invariant object
detection task; and Section 6 provides a more detailed look at the invariance capabilities of the formulation
used in Figure 1.
2
Related Work
Augmentation-based invariance approaches in deep learning.
The ‘standard pipeline’ to achieving in-
variance in deep learning described in Section 1 occupies, in a certain sense, a minimal point on the tradeoﬀ
curve between a purely data-driven approach and incorporating prior knowledge about the data into the
architecture: by using a convolutional neural network with pooling, invariance to translations of the input
image (a two-dimensional group of nuisances) is (in principle) conferred, and invariance to more complex
transformations is left up to a combination of learning from large datasets and data augmentation. A num-
ber of architectural proposals in the literature build from a similar perspective, but occupy diﬀerent points
on this tradeoﬀcurve. Parallel channel networks [CMS12] generate all possible transformations of the input
and process them in parallel, and have been applied for invariance to rotations [DWD15; LSBP16] and scale
[JL21]. Other architectures confer invariance by pooling over transformations at the feature level [SL12],
similarly for rotation [WGTB17] and scale [KSJ14]. Evidently these approaches become impracticable for
moderate-dimensional families of transformations, as they suﬀer from the same sampling-based bottleneck
as the standard pipeline.
To avoid explicitly covering the space of transformations, one can instead incorporate learned defor-
mation oﬀsets into the network, as in deformable CNNs [DQXL+17] and spatial transformer networks
[JSZK15]. At a further level of generality, capsule networks [HKW11; SFH17; Hin21; STDS+21] allow more
3

ﬂexible deformations among distinct parts of an object to be modeled. The improved empirical performance
observed with these architectures in certain tasks illustrates the value of explicitly modeling deformations
in the network architecture. At the same time, when it comes to guaranteed invariance to speciﬁc families
of structured deformations, they suﬀer from the same exponential ineﬃciencies as the aforementioned
approaches.
Invariance-by-construction architectures in deep learning.
The fundamental eﬃciency bottleneck en-
countered by the preceding approaches has motivated the development of alternate networks that are
invariant simply by virtue of their constituent computational building blocks. Scattering networks [BM13]
are an especially principled and elegant approach: they repeatedly iterate layers that convolve an input
signal with wavelet ﬁlters, take the modulus, and pool spatially. These networks provably obtain transla-
tion invariance in the limit of large depth, with feature representations that are Lipschitz-stable to general
deformations [Mal12]; moreover, the construction and provable invariance/stability guarantees generalize
to feature extractors beyond wavelet scattering networks [WB18]. Nevertheless, these networks suﬀer from
a similar exponential resource ineﬃciency to those that plague the augmentation-based approaches: each
layer takes a wavelet transform of every feature map at the previous layer, resulting in a network of width
growing exponentially with depth. Numerous mitigation strategies have been proposed for this limita-
tion [BM13; ZTAM20; ZGM21], and combinations of relatively-shallow scattering networks with standard
learning machines have demonstrated competitive empirical performance on certain benchmark datasets
[OBZ17]. However, the resulting hybrid networks still suﬀer from an inability to handle large, structured
transformations of domain such as pose and perspective changes.
Group scattering networks attempt to remedy this deﬁciency by replacing the spatial convolution oper-
ation with a group convolution 푤, 푥↦→[푤∗푥](푔) =
∫
픾푥(푔′)푤(푔−1푔′) d휇(푔′) [Mal12; CW16; KT18; BBCV21].
In this formula, 픾is a group with suﬃcient topological structure, 휇is Haar measure on 픾, and 푤and 푥are
the ﬁlter and signal (resp.), deﬁned on 픾(or a homogeneous space for 픾, as in spherical CNNs [CGKW18]).
Spatial convolution of natural images coincides with the special case 픾= ℤ2 in this construction; for more
general groups such as 3D rotation, networks constructed by iterated group convolutions yield feature rep-
resentations equivariant to the group action, and intermixing pooling operations yields invariance, just as
with 2D convolutional neural networks. At a conceptual level, this basic construction implies invariant net-
work architectures for an extremely broad class of groups and spaces admitting group actions [WFVW21],
and has been especially successful in graph-structured tasks such as molecular prediction where there is an
advantage to enforcing symmetries [BBCV21]. However, its application to visual data has been hindered
by exponential ineﬃciencies in computing the group convolution—integration over a 푑-dimensional group
픾costs resources exponential in 푑—and more fundamentally by the fact that discrete images are deﬁned
on the image plane ℤ2, whereas group convolutions require the signal to be deﬁned over the group 픾one
seeks invariance to. In this sense, the ‘reﬂexivity’ of spatial convolution and discrete images seems to be the
exception rather than the rule, and there remains a need for resource-eﬃcient architectural primitives for
invariance with visual data.
“Unrolling” iterative optimization algorithms.
First introduced by Gregor and LeCun in the context of
sparse coding [GL10], unrolled optimization provides a general method to convert an iterative algorithm
for solving an optimization problem into a neural network (we will provide a concrete demonstration in
the present context in Section 3), oﬀering the possibility to combine the statistical learning capabilities of
modern neural networks with very speciﬁc prior information about the problem at hand [CCCH+21]. It
has found broad use in scientiﬁc imaging and engineering applications [KKHP17; BHKW18; KBCW19;
KBRW19; OJMB+20], and most state-of-the-art methods for learned MRI reconstruction are based on this
approach [MRRK+21]. In many cases, the resulting networks are amenable to theoretical analysis [CLWY18;
LCWY19], leading to a mathematically-principled neural network construction.
3
Invariant Architecture Primitives: Optimization and Unrolling
Notation.
We write ℝfor the reals, ℤfor the integers, and ℕfor the positive integers. For positive integers
푚, 푛, and 푐, we let ℝ푚, ℝ푚×푛, and ℝ푚×푛×푐denote the spaces of real-valued 푚-dimensional vectors, 푚-by-푛
4

matrices, and 푐-channel 푚-by-푛images (resp.). We write 풆푖, 풆푖푗, etc. to denote the elements of the canonical
basis of these spaces, and 1푚and 0푚,푛(etc.) to denote their all-ones and all-zeros elements (resp.). We write
⟨· , · ⟩and ∥· ∥퐹to denote the euclidean inner product and associated norm of these spaces. We identify
푚by 푛images 풙with functions on the integer grid {0, 1, . . . , 푚−1} × {0, 1, . . . , 푛−1}, and therefore index
images starting from 0; when applying operations such as ﬁltering, we will assume that an implementation
takes the necessary zero padding, shifting, and truncation steps to avoid boundary eﬀects. For a subset
Ω ⊂ℤ2, we write 풫Ω for the orthogonal projection onto the space of images with support Ω.
Given a deformation vector ﬁeld 흉∈ℝ푚′×푛′×2 and an image 풙∈ℝ푚×푛×푐, we deﬁne the transformed
image 풙◦흉by (풙◦흉)푖푗= Í
(푘,푙)∈ℤ2 풙푘푙휙(휏푖푗0 −푘)휙(휏푖푗1 −푙), where 휙: ℝ→ℝis the cubic convolution
interpolation kernel [Key81].1 For parametric transformations of the image plane, we write 흉푨,풃to denote
the vector ﬁeld representation of the transformation parameterized by (푨, 풃), where 푨∈ℝ2×2 is nonsingular
and 풃∈ℝ2 (see Appendix A.1 for speciﬁc ‘implementation’ details). For two grayscale images 풙∈ℝ푚×푛
and 풖∈ℝ푚′×푛′, we write their linear convolution as (풙∗풖)푖푗= Í
(푘,푙)∈ℤ2 푥푘푙푢푖−푘,푗−푙. We write 품휎2 ∈ℝℤ×ℤ
to denote a (sampled) gaussian with zero mean and variance 휎2. When 풙∈ℝ푚×푛and 풖∈ℝ푐, we write
풙⊗풖∈ℝ푚×푛×푐to denote the ‘tensor product’ of these elements, with (풙⊗풖)푖푗푘= 푥푖푗푢푘. We use 풙⊙풖to
denote elementwise multiplication of images.
3.1
Conceptual Framework
Given an input image 풚∈ℝ푚×푛×푐(e.g., 푐= 3 for RGB images), we consider the following general opti-
mization formulation for seeking a structured transformation of the input that explains it in terms of prior
observations:
min
흉
휑(풚◦흉) + 휆휌(흉).
(1)
Here, 흉∈ℝ푚′×푛′×2 gives a vector ﬁeld representation of transformations of the image plane, and 휆> 0 is a
regularization tradeoﬀparameter. Minimization of the function 휑encourages the transformed input image
풚◦흉to be similar to previously-observed images, whereas minimization of 휌regularizes the complexity of
the learned transformation 흉. Both terms allow to incorporate signiﬁcant prior information about the visual
data and task at hand, and an optimal solution 흉to (1) furnishes an invariant representation of the input 풚.
3.2
Computational Primitive: Optimization for Domain Transformations
We illustrate the ﬂexibility of the general formulation (1) by instantiating it for a variety of classes of visual
data. In the most basic setting, we may consider the registration of the input image 풚to a known motif
풙표assumed to be present in the image, and constrain the transformation 흉to lie in a parametric family of
transformations 핋, which yields the optimization formulation
min
흉
1
2
풫Ω

품휎2 ∗(풚◦흉−풙표)
2
퐹+ 휒핋(흉).
(2)
Here, Ω denotes a subset of the image plane corresponding to the pixels on which the motif 풙표is supported,
품휎2 is a gaussian ﬁlter with variance 휎2 applied individually to each channel, and 휒핋(흉) denotes the
characteristic function for the set 핋(zero if 흉∈핋, +∞otherwise). The parameters in (2) are illustrated in
Figure 2(a-d). We do not directly implement the basic formulation (2) in our experiments, but as a simple
model for the more elaborate instantiations of (1) that follow later it furnishes several useful intuitions. For
instance, although (2) is a nonconvex optimization problem with a ‘rough’ global landscape, well-known
results suggest that under idealized conditions (e.g., when 풚= 풙표◦흉표for some 흉표∈핋), multiscale solvers that
repeatedly solve (2) with a smoothing level 휎2
푘then re-solve initialized at the previous optimal solution with
a ﬁner level of smoothing 휎2
푘+1 < 휎2
푘converge in a neighborhood of the true transformation [LC01; MZM12;
KF14; VF14]. This basic fact underpins many classical computer vision methods for image registration
and stitching [Bro92; MV98; BM04; Sze07], active appearance models for objects [CET98], and optical ﬂow
estimation [HS81; LK81; Ana89], and suggests that (2) is a suitable base for constructing invariant networks.
For our experiments on textured visual data in Figure 1 and Section 4, we will need two elaborations of
(2). The ﬁrst arises due to the problem of obtaining invariant representations for images containing motifs
1The function 휙is compactly supported on the interval [−2, 2], and diﬀerentiable with absolutely continuous derivative.
5

흉(0)
3
푁
resample_bicubic
3
푁
3
푁
풙표
−
3
N
conv_lpf
6
N
conv 1x1
6
N
concatenate
mul
2
N
conv
1x1
−
2
N
conv_prox
3
푁
resample_bicubic
3
푁
3
푁
풙표
−
3
N
conv_lpf
6
N
conv 1x1
6
N
concatenate
mul
2
N
conv
1x1
−
2
N
conv_prox
흉(2)
(e) Unrolled network architecture
(a) Motif 풙표
(b) Support mask Ω
(c) Observation 풚
(d) Registered scene
Figure 2: Motif registration with the formulation (1), and an unrolled solver. (a-d): Visualization of components of
a registration problem, such as (2). We model observations 풚as comprising an object involving the motif of interest
(here, the body of the crab template we experiment with in Section 4) on a black background, as in (a), embedded in
visual clutter (here, the beach background) and subject to a deformation, which leads to a novel pose. A mask Ω for the
nonzero pixels of the motif, as in (b), is used to avoid having pixels corresponding to clutter enter the registration cost.
After solving this optimization problem, we obtain a transformation 흉that registers the observation to the motif, as in
(d). In (c-d), we set the red and blue pixels corresponding to the mask Ω to 1 in order to visualize the relative location
of the motif. (e): Optimization formulations imply network architectures, via unrolled optimization. Here we show
two iterations of an unrolled solver for (2), as we detail in Section 3.3; parameters that could be learned from data, à la
unrolled optimization, are highlighted with red text. The operations comprising this unrolled network consist of linear
maps, convolutions, and interpolations, leading to eﬃcient implementation on standard hardware accelerators.
풙표appearing in general backgrounds: in such a scenario, the input image 풚may contain the motif 풙표in a
completely novel scene (as in Figure 2(c-d)), which makes it inappropriate to smooth the entire motif with
the ﬁlter 품휎2. In these scenarios, we consider instead a cost-smoothed formulation of (2):
min
흉
1
2
Õ
횫∈ℤ×ℤ
(품휎2)횫
풫Ω

풚◦(흉+ 흉0,횫) −풙표
2
퐹+ 휒핋(흉).
(3)
In practice, we take the sum over a ﬁnite subset of shifts 횫on which most of the mass of the gaussian ﬁlter
lies. This formulation is inspired by more general cost-smoothing registration proposals in the literature
[MZM12], and it guarantees that pixels of 풚◦흉corresponding to the background Ω푐are never compared
to pixels of 풙표while incorporating the basin-expanding beneﬁts of smoothing. Second, we consider a more
general formulation which also incorporates a low-frequency model for the image background:
min
흉, 휷
1
2
풫eΩ

품휎2 ∗(풚◦흉−풙표−풫Ω푐[품퐶휎2 ∗휷])
2
퐹+ 휒핋(흉).
(4)
Here, 휷∈ℝ푚×푛×푐acts as a learnable model for the image background, and 퐶> 1 is a ﬁxed constant that
guarantees that the background model is at a coarser scale than the motif and image content. The set eΩ
represents a dilation by 휎of the motif support Ω, and penalizing pixels in this dilated support ensures that
an optimal 흉accounts for both foreground and background agreement. We ﬁnd background modeling
essential in computing with scale-changing transformations, such as aﬃne transforms in Figure 1.
3.3
Invariant Networks from Unrolled Optimization
The technique of unrolled optimization allows us to obtain principled network architectures from the
optimization formulations developed in Section 3.2. We describe the basic approach using the abstract
formulation (1). For a broad class of regularizers 휌, the proximal gradient method [PB14] can be used to
attempt to solve the nonconvex problem (1): it deﬁnes a sequence of iterates
흉(푡+1) = prox휆휈푡휌

흉(푡) −휈푡∇흉휑(풚◦흉(푡))

(5)
6

from a ﬁxed initialization 흉(0), where 휈푡> 0 is a step size sequence and prox휌(흉) = arg min흉′ 1
2 ∥흉−흉′∥2
퐹+휌(흉′)
is well-deﬁned if 휌is a proper convex function. Unrolled optimization suggests to truncate this iteration
after 푇steps, and treat the iterate 흉(푇) at that iteration as the output of a neural network. One can then
learn certain parameters of the neural network from datasets, as a principled approach to combining the
structural priors of the original optimization problem with the beneﬁts of a data-driven approach.
In Figure 2(e), we show an architectural diagram for a neural network unrolled from a proximal gradient
descent solver for the registration formulation (2).
We always initialize our networks with 흉(0) as the
identity transformation ﬁeld, and in this context we have prox휆휈푡휌(흉) = proj핋(흉) as the nearest point in 핋to
흉, which can be computed eﬃciently (computational details are provided in Appendix A.1). The cost (2) is
diﬀerentiable; calculating its gradient as in Appendix A.2, (5) becomes
흉(푡+1) = proj핋
 
흉(푡) −휈푡
푐−1
Õ
푘=0

품휎2 ∗풫Ω
h
품휎2 ∗

풚◦흉(푡) −풙표

푘
i
⊗12

⊙

d풚푘◦흉(푡)!
,
(6)
as we represent visually in Figure 2(e), where a subscript of 푘denotes the 푘-th channel of the image and d풚∈
ℝ푚×푛×푐×2 is the Jacobian matrix of 풚. The constituent operations in this network are convolutions, pointwise
nonlinearities and linear maps, which lend themselves ideally to implementation in standard deep learning
software packages and on hardware accelerators; and because the cubic convolution interpolation kernel 휙
is twice continuously diﬀerentiable except at four points of ℝ, these networks are end-to-end diﬀerentiable
and can be backpropagated through eﬃciently. The calculations necessary to instantiate unrolled network
architectures for other optimization formulations used in our experiments are deferred to Appendix A.2.
A further advantage of the unrolled approach to network design is that hyperparameter selection becomes
directly connected to convergence properties of the optimization formulation (1): we demonstrate how
theory inﬂuences these selections in Section 5, and provide practical guidance for registration and detection
problems through our experiments in Sections 4, 5.2 and 6.
4
Invariant Networks for Hierarchical Object Detection
The unrolled networks in Section 3 represent architectural primitives for building deformation-invariant
neural networks: they are eﬀective at producing invariant representations for input images containing local
motifs. In this section, we illustrate how these local modules can be combined into a network that performs
invariant processing of nonlocally-structured visual data, via an invariant hierarchical object detection task
with a ﬁxed template. For simplicity, in this section we will focus on the setting where 핋is the set of rigid
motions of the image plane (i.e., translations and rotations), which we will write as SE(2).
4.1
Data Model and Problem Formulation
We consider an object detection task, where the objective is to locate a ﬁxed template with independently-
articulating parts (according to a SE(2) motion model) in visual clutter. More precisely, we assume the
template is decomposable into a hierarchy of deformable parts, as in Figure 3(a): at the top level of the
hierarchy is the template itself, with concrete visual motifs at the lowest levels that correspond to speciﬁc
pixel subsets of the template, which constitute independent parts. Because these constituent parts deform
independently of one another, detecting this template eﬃciently demands an approach to detection that
captures the speciﬁc hierarchical structure of the template.2 Compared to existing methods for parts-based
object detection that are formulated to work with objects subject to complicated variations in appearance
[FMR08; FGMR10; GIDM15; PVGR15], focusing on the simpler setting of template detection allows us
to develop a network that guarantees invariant detection under the motion model, and can incorporate
large-scale statistical learning techniques by virtue of its unrolled construction (although we leave this latter
2Reasoning as in Section 1, the eﬀective dimension of the space of all observable transformations of the object is the product
of the dimension of the motion model and the number of articulating parts.
A detector that exploits the hierarchical structure
of the object eﬀectively reduces the dimensionality to dim(motion model) + log(number of parts), yielding a serious advantage for
moderate-dimensional families of deformations.
7

direction for future work). We note that other approaches are possible, such as hierarchical sparse modeling
[BS10; JMOB10] or learning a graphical model [SM12].
More formally, we write 풚표∈ℝ푚표×푛표×3 for the RGB image corresponding to a canonized view of the
template to be detected (e.g., the crab at the top of the hierarchy in Figure 3(a) left) embedded on a black
background. For a 퐾-motif object (e.g., 퐾= 4 for the crab template), we let 풙푘∈ℝ푚푘×푛푘×3 denote the 푘distinct
(canonized, black-background-embedded) transforming motifs in the object, each with non-overlapping
occurrence coordinates (푖푘, 푗푘) ∈{0, . . . , 푚표} × {0, . . . , 푛표}. The template 풚표decomposes as
풚표=
퐾
Õ
푘=1
풙푘∗풆푖푘푗푘
|         {z         }
transforming motifs
+ 풚표−
퐾
Õ
푘=1
풙푘∗풆푖푘푗푘
|               {z               }
static body
.
(7)
For example, the four transforming motifs for the crab template in Figure 3(a) are the two claws and two
eyes. In our experiments with the crab template, we will consider detection of transformed templates 풚obs
of the following form:
풚obs =
" 퐾
Õ
푘=1
(풙푘∗풆푖푘푗푘) ◦흉푘+
 
풚표−
퐾
Õ
푘=1
풙푘∗풆푖푘푗푘
!#
◦흉0,
(8)
where 흉0 ∈SE(2), and 흉푘∈SO(2) is suﬃciently close to the identity transformation (which represents the
physical constraints of the template). The detection task is then to decide, given an input scene 풚∈ℝ푚×푛×3
containing visual clutter (and, in practice, 푚≫푚표and 푛≫푛표), whether or not a transformed instance 풚obs
appears in 풚or not, and to output estimates of its transformation parameters 흉푘.
Although our experiments will pertain to the observation model (8), as it agrees with our decomposition
of the crab template in Figure 3(a), the networks we construct in Section 4.3 will be amenable to more
complex observation models where parts at intermediate levels of the hierarchy also transform.3 To this
end, we introduce additional notation that captures the hierarchical structure of the template 풚표. Concretely,
we identify a hierarchically-structured template with a directed rooted tree 퐺= (푉, 퐸), with 0 denoting the
root node, and 1, . . . , 퐾denoting the 퐾leaf nodes. Our networks will treat observations of the form
풚obs =
퐾
Õ
푘=1
((· · · (((풙푘∗풆푖푘푗푘) ◦흉푘) ◦흉푣푑(푘)−1) ◦· · · ) ◦흉푣1) ◦흉0 +
 
풚표−
퐾
Õ
푘=1
풙푘∗풆푖푘푗푘
!
◦흉0,
(9)
where 푑(푘) is the depth of node 푘, and 푣1, . . . , 푣푑(푘)−1 ∈푉with 0 →푣1 →· · · →푣푑(푘)−1 →푘specifying
the path from the root node to node 푘in 퐺. To motivate the observation model (9), consider the crab
example of Figure 3(a), where in addition we imagine the coordinate frame of the eye pair motif transforms
independently with a transformation 흉5: in this case, the observation model (9) can be written in an
equivalent ‘hierarchical’ form
풚obs =

(풙1 ∗풆푖1푗1) ◦흉1 + (풙2 ∗풆푖2푗2) ◦흉2 +

(풙3 ∗풆푖3푗3) ◦흉3 + (풙4 ∗풆푖4푗4) ◦흉4

◦흉5

◦흉0 + 풚body ◦흉0,
by linearity of the interpolation operation 풙↦→풙◦흉(with 풚body = 풚표−Í
푘풙푘∗풆푖푘푗푘).
4.2
Aside: Optimization Formulations for Registration of “Spiky” Motifs
To eﬃciently perform hierarchical invariant detection of templates following the model (9), the networks
we design will build from the following basic paradigm, given an input scene 풚:
1. Visual motif detection: First, perform detection of all of the lowest-level motifs 풙1, . . . , 풙퐾in 풚. The
output of this process is an occurrence map for each of the 퐾transforming motifs, i.e. an 푚× 푛image
taking (ideally) value 1 at the coordinates where detections occur and 0 elsewhere.
3For example, consider a simple extension of the crab template in Figure 3(a), where the left and right claw motifs are further
decomposed into two pairs of pincers plus the left and right arms, with opening and closing motions for the pincers, and the same
SO(2) articulation model for the arms (which naturally moves the pincers in accordance with the rotational motion).
8

2. Spiky motif detection for hierarchical motifs: Detect intermediate-level abstractions using the oc-
currence maps in 풚obtained in the previous step. For example, if 푘= 3 and 푘= 4 index the left
and right eye motifs in the crab template of Figure 3(a), detection of the eye pair motif corresponds to
registration of the canonized eye pair’s occurrence map against the two-channel image corresponding
to the concatenation of 풙3 and 풙4’s occurrence maps in 풚.
3. Continue until the top of the hierarchy: This occurrence map detection process is iterated until the
top level of the hierarchy. For example, in Figure 3(a), a detection of the crab template occurs when
the multichannel image corresponding to the occurrence maps for the left and right claws and the eye
pair motif is matched.
To instantiate this paradigm, we ﬁnd it necessary to develop a separate registration formulation for reg-
istration of occurrence maps, beyond the formulations we have introduced in Section 3. Indeed, occurrence
maps contain no texture information and are maximally localized, motivating a formulation that spreads
out gradient information and avoids interpolation artifacts—and although there is still a need to cope with
clutter in general, the fact that the occurrence maps are generated on a black background obviates the
need for extensive background modeling, as in (4). We therefore consider the following “complementary
smoothing” formulation for spike registration: for a 푐-channel occurrence map 풚and canonized occurrence
map 풙표, we optimize over the aﬃne group Aﬀ(2) = GL(2) ⋊ℝ2 as
min
푨,풃
1
2푐
품휎2푰−휎2
0푨푨∗∗

det−1/2(푨푨∗)

품휎2
0푰∗풚

◦흉푨−1,−푨−1풃

−품휎2푰∗풙표

2
퐹+ 휒Aﬀ(2)(푨, 풃),
(10)
where 품푴denotes a single-channel centered gaussian ﬁlter with positive deﬁnite covariance matrix 푴≻0,
and correlations are broadcast across channels. Here, 휎> 0 is the main smoothing parameter to propagate
gradient information, and 휎0 > 0 is an additional smoothing hyperparameter to mitigate interpolation
artifacts.
In essence, the key modiﬁcations in (10) that make it amenable to registration of occurrence maps are the
compensatory eﬀects for scaling that it introduces: transformations that scale the image correspondingly
reduce the amplitude of the (smoothed) spikes, which is essential given the discrete, single-pixel spike
images we will register. Of course, since we consider only euclidean transformations in our experiments in
this section, we always have 푨푨∗= 푰, and the problem (10) can be implemented in a simpler form. However,
these modiﬁcations lead the problem (10) to work excellently for scale-changing transformations as well:
we explore the reasons behind this from both theoretical and practical perspectives in Section 5.
4.3
Invariant Network Architecture
The networks we design to detect under the observation model (9) consist of a conﬁguration of unrolled
motif registration networks, as in Figure 2(e), arranged in a ‘bottom-up’ hierarchical fashion, following the
hierarchical structure in the example shown in Figure 3(a). The conﬁguration for each motif registration sub-
network is a ‘GLOM-style’ [Hin21] collection of the networks sketched in Figure 2(e), oriented at diﬀerent
pixel locations in the input scene 풚; the transformation parameters predicted of each of these conﬁgurations
are aggregated across the image, weighted by the ﬁnal optimization cost (as a measure of quality of the
ﬁnal solution), in order to determine detections. These detections are then used as feature maps for the next
level of occurrence motifs, which in turn undergo the same registration-detection process until reaching the
top-level object’s occurrence map, which we use as a solution to the detection problem. A suitable unrolled
implementation of the registration and detection process leads to a network that is end-to-end diﬀerentiable
and amenable to implementation on standard hardware acceleration platforms (see Section 4.4).
We now describe this construction formally, following notation introduced in Section 4.1. The network
input is an RGB image 풚∈ℝ푚×푛×3. We shall assume that the canonized template 풚표is given, as are as the
canonized visual motifs 풙1, . . . , 풙퐾and their masks Ω1, . . . , Ω퐾; we also assume that for every 푣∈푉with
푣∉{1, . . . , 퐾}, we are given the canonized occurrence map 풙푣∈ℝ푚푣×푛푣×푐푣of the hierarchical feature 푣in
풚표. In practice, one obtains these occurrence maps through a process of “extraction”, using 풚표as an input
to the network, which we describe in Appendix A.4. The network construction can be separated into three
distinct steps:
9

crab
eye
pair
left claw
right claw
left eye
right eye
풚
흎1 ⊗풆0 + 흎2 ⊗풆1 + 풚5
풚0
흎0
풙3 풙4
풙1
풙2
풙5
풙0
(a) Hierarchical detection of the crab template
(b) 0 degrees
(c) 7.5 degrees
(d) 15 degrees
(e) 22.5 degrees
Figure 3: An example of a hierarchically-structured template, and the results of an implementation of our detection
network. (a): Structure of the crab template, described in Section 4.1, and its interaction with our network architecture
for detection, described in Section 4.3. Left: top-down decomposition of the template into motifs. A template of interest
풚표(here, the crab at top left) is decomposed into a hierarchy of abstractions. The hierarchical structure is captured
by a tree 퐺= (푉, 퐸): nodes represent parts or aggregations of parts, and edges represent their relationships. Right:
bottom-up detection of the template in a novel scene. To detect the template in a novel scene and pose, the network described
in Section 4.3 ﬁrst localizes each of the lowest-level visual motifs at left and their transformation parameters in the
input scene 풚(bottom right). Motifs and the derived occurrence maps are labeled in agreement with the notation we
introduce in Section 4.3. The output of each round of optimization is an occurrence map 흎푣for nodes 푣∈푉; these
occurrence maps then become the inputs for detection of the next level of concepts, following the connectivity structure
of 퐺, until the top-level template is reached (top right). (b-e): Concrete results for the hierarchical invariant object
detection network implemented in Section 4.4: the learned transformation at the minimum-error stride for each motif
is used to draw the motifs’ transformed bounding boxes. Insets at the bottom right corner of each result panel visualize
the quality of the ﬁnal detection trace 흎0 for the template, with a value of 1 at the top of the inset.
Traversal.
The network topology is determined by a simple traversal of the graph 퐺. For each 푣∈푉, let
푑(푣) denote the shortest-path distance from 0 to 푣, with unit weights for edges in 퐸(the “depth” of 푣in 퐺). We
will process motifs in a deepest-ﬁrst order for convenience, although this is not strictly necessary in all cases
(e.g. for eﬃciency, it might be preferable to process all leaf nodes 1, . . . , 퐾ﬁrst). Let diam(퐺) = max푣∈푉푑(푣),
and for an integer ℓno larger than diam(퐺), we let 퐷(ℓ) ∈ℕdenote the number of nodes in 푉that are at
depth ℓ.
Motif detection at one depth.
Take any integer 0 ≤ℓ≤diam(퐺), and let 푣1, . . . , 푣퐷(ℓ) denote the nodes in
퐺at depth ℓ, enumerated in increasing order (say). For each such vertex 푣푘, perform the following steps:
1. Is this a leaf?
If the neighborhood {푣′ | (푣푘, 푣′) ∈퐸} is empty, this node is a leaf; otherwise it is not.
Subsequent steps depend on this distinction. We phrase the condition more generally, although we have
deﬁned 1, . . . , 퐾as the leaf vertices here, to facilitate some implementation-independence.
2. Occurrence map aggregation for non-leaves:
If 푣푘is not a leaf, construct its detection feature map
from lower-level detections: concretely, let
풚푣푘=
Õ
푣′ : (푣푘,푣′)∈퐸
흎푣′ ⊗풆휋푣푘(푣′),
(11)
where 휋푣푘(푣′) denotes a vertex-increasing-order enumeration of the set {푣′ : (푣푘, 푣′) ∈퐸} starting from 0.
By construction (see the fourth step below), 풚푣푘has the same width and height as the input scene 풚, but
푐푣푘= |{푣′ : (푣푘, 푣′) ∈퐸}| channels (one for each child node) instead of 3 RGB channels.
10

3. Perform strided registration:
Because the motif 풙푣푘is in general much smaller in size than the
scene 풚푣푘, and because the optimization formulation (1) is generally nonconvex with a ﬁnite-radius basin of
attraction around the true transformation parameters in the model (9), the detection process consists of a
search for 풙푣푘anchored at a grid of points in 풚푣푘. Concretely, let
Λ푣푘= {(푖Δ퐻,푣푘, 푗Δ푊,푣푘) | (푖, 푗) ∈{0, . . . , 푚−1} × {0, . . . , 푛−1}} ∩({0, . . . , 푚−1} × {0, . . . , 푛−1})
denote the grid for the 푣푘-th motif; here Δ퐻,푣푘and Δ푊,푣푘deﬁne the vertical and horizontal stride lengths of
the grid (we discuss choices of these and other hyperparameters introduced below in Appendix A.4). When
푣푘is a leaf, for each 흀∈Λ푣푘, we let (푼(푣푘, 흀), 풃(푣푘, 흀)) ∈SE(2) denote the parameters obtained after running
an unrolled solver for the cost-smoothed visual motif registration problem
min
흉
1
2
Õ
횫∈ℤ×ℤ
(품휎2푣푘)횫
풫Ω푣푘
h
품휎2
in ∗풚

◦(흉+ 흉0,횫+흀) −풙푣푘
i
2
퐹+ 휒SE(2)(흉),
(12)
for 푇푣푘iterations, with step size 휈푣푘. In addition, we employ a two-step multiscale smoothing strategy,
which involves initializing an unrolled solver for (12) with a much smaller smoothing parameter (휎′
푣푘)2 at
(푼(푣푘, 흀), 풃(푣푘, 흀)) and running it for an additional ﬁxed number of iterations; we let loss(푣푘, 흀) denote the
ﬁnal objective function value after this multiscale process, and abusing notation, we let (푼(푣푘, 흀), 풃(푣푘, 흀))
denote the updated ﬁnal parameters . Precise implementation details are discussed in Appendix A.4. When
푣푘is not a leaf, we instead deﬁne the same ﬁelds on the grid Λ푣푘via a solver for the spike registration
problem
min
흉
1
2푐푣푘
풫Ω푣푘

품휎2푣푘−휎2
0,푣푘∗

풚푣푘◦(흉+ 흉0,흀) −풙푣푘

2
퐹
+ 휒SE(2)(흉),
(13)
with Ω푣푘denoting a dilated bounding box for 풙푣푘, and otherwise the same notation and hyperparameters.
We do not use multiscale smoothing for non-leaf motifs.
4. Aggregate registration outputs into detections (occurrence maps):
We convert the registration
ﬁelds into detection maps, by computing
흎푣푘=
Õ
흀∈Λ푣푘

품휎2
0,푣푘∗풆흀+풃(푣푘,흀)

exp

−훼푣푘max

0, loss(푣푘, 흀) −훾푣푘
	
,
(14)
where each summand 품휎2
0,푣푘∗풆흀+풃(푣푘,흀) is truncated to be size 푚× 푛.4 The scale and threshold parameters
훼푣푘and 훾푣푘appearing in this formula are calibrated to achieve a speciﬁed level of performance under
an assumed maximum level of visual clutter and transformation for the observations (9), as discussed in
Appendix A.4.
We prefer to embed detections as occurrence maps and use these as inputs for higher-level detections
using optimization, rather than a possible alternate approach (e.g. extracting landmarks and processing
these using group synchronization), in order to have each occurrence map 흎푣for 푣∈푉be diﬀerentiable
with respect to the various ﬁlters and hyperparameters.
Template detection.
To perform detection given an input 풚, we repeat the four steps in the previous section
for each motif depth, starting from depth ℓ= diam(퐺), and each motif at each depth. After processing depth
ℓ= 0, the output occurrence map 흎0 can be thresholded to achieve a desired level of detection performance
for observations of the form (9). The detection process is summarized as Algorithm 1.
By construction, this output 흎0 can be diﬀerentiated with respect to each node 푣∈푉’s hyperparameters
or ﬁlters, and the unrolled structure of the sub-networks and 퐺’s topology can be used to eﬃciently calculate
such gradients via backpropagation. In addition, although we do not use the full transformation parameters
푼(흀, 푣) calculated in the registration operations (12) and (13), these can be leveraged for various purposes
(e.g. drawing detection bounding boxes, as in our experimental evaluations in Section 4.4).
4This convolutional notation is of course an abuse of notation, to avoid having to deﬁne a gaussian ﬁlter with a general mean
parameter. In practice, this latter technique is both more eﬃcient to implement and leads to a stably-diﬀerentiable occurrence map.
11

Algorithm 1 Invariant Hierarchical Motif Detection Network, Summarizing Section 4.3
input scene 풚, graph 퐺= (푉, 퐸), motifs (풙푣, Ω푣)푣∈푉, hyperparameters (휈푣, 푇푣, Δ퐻,푣, Δ푊,푣, 휎2
푣, 휎2
0,푣, 훼푣, 훾푣)푣∈푉
set diam(퐺) and node enumerations by depth-ﬁrst traversal of 퐺
for all depths ℓ= diam(퐺), diam(퐺) −1, . . . , 0 do
for all nodes 푣at depth ℓdo
set 푁푣= {푣′ | (푣, 푣′) ∈퐸} and 푐푣= |푁푣|
if 푐푣> 0 then
concatenate occurrence maps into 풚푣= Í
푣′∈푁푣흎푣′ ⊗풆휋푣(푣′)
for all 흀∈Λ푣(Δ퐻,푣, Δ푊,푣) do
if 푐푣> 0 then
set 푼(푣, 흀), 풃(푣, 흀) = arg min흉
1
2푐푣∥품휎2푣−휎2
0,푣∗(풚푣◦(흉+ 흉0,흀) −풙푣)∥2
퐹+ 휒SE(2)(흉)
set loss(푣, 흀) = min흉
1
2푐푣∥품휎2푣−휎2
0,푣∗(풚푣◦(흉+ 흉0,흀) −풙푣)∥2
퐹+ 휒SE(2)(흉)
(both with a 푇푣-layer unrolled solver)
else
set 푼(푣, 흀), 풃(푣, 흀) = arg min흉
1
2
Í
횫(품휎2푣)횫∥풫Ω푣[(품휎2
in ∗풚) ◦(흉+ 흉0,횫+흀) −풙푣]∥2
퐹+ 휒SE(2)(흉)
set loss(푣, 흀) = min흉1
2
Í
횫(품휎2푣)횫∥풫Ω푣[(품휎2
in ∗풚) ◦(흉+ 흉0,횫+흀) −풙푣]∥2
퐹+ 휒SE(2)(흉)
(both with a 푇푣-layer unrolled solver, with two-round multiscale smoothing)
construct the occurrence map 흎푣= Í
흀∈Λ푣(품휎2
0,푣∗풆흀+풃(푣,흀)) exp(−훼푣max{0, loss(푣, 흀) −훾푣})
output template occurrence map 흎0
4.4
Implementation and Evaluation
We implement the hierarchical invariant object detection network described in Section 4.3 in PyTorch
[PGML+19], and test it for detection of the crab template from Figure 3(a) subject to a global rotation (i.e.,
흉0 in the model (9)) of varying size (Figure 3(b-e)). In 512 × 384 pixel scenes on a “beach” background, a
calibrated detector perfectly detects the crab from its constituent parts up to rotations of 휋/8 radians—at
rotations around 휋/6, a multiple-instance issue due to similarity between the two eye motifs begins to hinder
the detection performance. Traces in each panel of Figure 3, right demonstrate the precise localization of
the template.
For hyperparameters, we set 푇푣= 1024 and Δ퐻,푣= Δ푊,푣= 20 for all 푣∈푉, and calibrate detection
parameters as described in Appendix A.4; for visual motifs, we calibrate the remaining registration hyper-
parameters as described in Appendix A.4 on a per-motif basis, and for spike motifs, we ﬁnd the prescriptions
for 휎2
푣and the step sizes 휈푣implied by theory (Section 5) to work excellently without any ﬁne-tuning. We
also implement selective ﬁltering of strides for spiky motif alignment that are unlikely to succeed: due
to the common background, this type of screening is particularly eﬀective here. The strided registration
formulations (12) and (13) aﬀord eﬃcient batched implementation on a hardware accelerator, given that
the motifs 풙푣for 푣∈푉are signiﬁcantly smaller than the full input scene 풚, and the costs only depend on
pixels near to the motifs 풙푣. On a single NVIDIA TITAN X Pascal GPU accelerator (12 GB memory), it takes
approximately ﬁve minutes to complete a full detection. We expect throughput to be further improvable
without sacriﬁcing detection performance by decreasing the maximum iterations for each unrolled network
푇푣even further—the setting of 푇푣= 1024 is conservative, with convergence typically much more rapid. Our
implementation is available at https://github.com/sdbuch/refine.
5
Guaranteed, Eﬃcient Detection of Occurrence Maps
In Section 4, we described how invariant processing of hierarchically-structured visual data naturally leads
to problems of registering ‘spiky’ occurrence maps, and we introduced the formulation (10) for this purpose.
In this section, we provide a theoretical analysis of a continuum model for the proximal gradient descent
method applied to the optimization formula (10). A byproduct of our analysis is a concrete prescription for
the step size and rate of smoothing—in Section 5.2, we demonstrate experimentally that these prescriptions
work excellently for the discrete formulation (10), leading to rapid registration of the input scene.
12

0
10
20
30
10−10
10−8
Iteration
Objective value
0
10
20
30
0
0.5
1
Iteration
NCC
0
10
20
30
100
102
Iteration
Parameter error
Empirical
Estimate
(a) Motif
(b) Scene
(c) Recovered scene
(d)
(e)
(f)
Figure 4: Numerical veriﬁcation of Theorem 5.1. (a): A multichannel spike motif containing 5 spikes. (b): A scene
generated by applying a random aﬃne transformation to the motif. (c): The solution to (10) with these data. The skewing
apparent here is undone by the compensated external gaussian ﬁlter, which enables accurate localization in spite of
these artifacts. (d): Change in objective value of (10) across iterations of proximal gradient descent. Convergence occurs
in tens of iterations. (e): Change in normalized cross correlation across iterations (see Appendix A.3). We observe that
the method successfully registers the multichannel spike scene. (f): Comparison between the left and right-hand side of
equation (18) with gradient descent iterates from (10) (labeled as 휑here). After an initial faster-than-predicted linear rate,
the discretized solver saturates at a sub-optimal level. This is because because accurate estimation of the transformation
parameters (푨, 풃) requires subpixel-level preciseness, which is aﬀected by discretization and interpolation artifacts. It
does not hinder correct localization of the scene, as (e) shows.
5.1
Multichannel Spike Model
We consider continuous signals deﬁned on ℝ2 in this section, as an ‘inﬁnite resolution’ idealization of
discrete images, free of interpolation artifacts. We refer to Appendix B for full technical details. Consider a
target signal
푿표=
푐Õ
푖=1
휹풗푖⊗풆푖,
where 휹풗푖is a Dirac distribution centered at the point 풗푖, and an observation
푿=
푐Õ
푖=1
휹풖푖⊗풆푖,
satisfying
풗푖= 푨★풖푖+ 풃★.
In words, the observed signal is an aﬃne transformation of the spike signal 푿표, as in Figure 4(a, b). This
model is directly motivated by the occurrence maps (11) arising in our hierarchical detection networks.
Following (10), consider the objective function
휑퐿2,휎(푨, 풃) ≡1
2푐
푐Õ
푖=1
품휎2푰−휎2
0(푨∗푨)−1 ∗

det1/2(푨∗푨)

품휎2
0푰∗푿푖

◦흉푨,풃

−품0,휎2푰∗(푿표)푖

2
퐿2 .
13

We study the following “inverse parameterization” of this function:
휑inv
퐿2,휎(푨, 풃) ≡휑퐿2,휎(푨−1, −푨−1풃).
(15)
We analyze the performance of gradient descent for solving the optimization problem
min
푨,풃휑inv
퐿2,휎(푨, 풃).
Under mild conditions, local minimizers of this problem are global. Moreover, if 휎is set appropriately, the
method exhibits linear convergence to the truth:
Theorem 5.1 (Multichannel Spike Model, Aﬃne Transforms, 퐿2). Consider an instance of the multichannel spike
model, with 푼= [풖1, . . . , 풖푐] ∈ℝ2×푐. Assume that the spikes 푼are centered and nondegenerate, so that 푼1 = 0 and
rank(푼) = 2. Then gradient descent
푨푘+1 = 푨푘−푡푨∇푨휑inv
퐿2,휎(푨푘, 풃푘),
풃푘+1 = 풃푘−푡풃∇풃휑inv
퐿2,휎(푨푘, 풃푘)
with smoothing
휎2 ≥2
max푖∥풖푖∥2
2
푠min(푼)2
 푠max(푼)2∥푨★−푰∥2
퐹+ 푐∥풃★∥2
2

(16)
and step sizes
푡푨=
8휋푐휎4
푠max(푼)2 ,
푡풃= 8휋휎4,
(17)
from initialization 푨0 = 푰, 풃0 = 0 satisﬁes
8휋휎4
푡푨
∥푨푘−푨★∥2
퐹+ ∥풃푘−풃★∥2
2 ≤

1 −1
2휅
2푘8휋휎4
푡푨
∥푰−푨★∥2
퐹+ ∥풃★∥2
2

,
(18)
where
휅= 푠max(푼)2
푠min(푼)2 ,
with, 푠min(푼) and 푠max(푼) denoting the minimum and maximum singular values of the matrix 푼.
Theorem 5.1 establishes a global linear rate of convergence for the continuum occurrence map registration
formulation (15) in the relevant product norm, where the rate depends on the condition number of the
matrix of observed spike locations 푼.
This dependence arises from the intuitive fact that recovery of
the transformation parameters (푨★, 풃★) is a more challenging problem than registering the observation
to the motif—in practice, we do not observe signiﬁcant degradation of the ability to rapidly register the
observed scene as the condition number increases. The proof of Theorem 5.1 reveals that the use of inverse
parameterization in (15) dramatically improves the landscape of optimization: the problem becomes strongly
convex around the true parameters when the smoothing level is set appropriately. In particular, (16) suggests
a level of smoothing commensurate with the maximum distance the spikes need to travel for a successful
registration, and (17) suggests larger step sizes for larger smoothing levels, with appropriate scaling of the
step size on the 푨parameters to account for the larger motions experienced by objects further from the
origin. In the proof, the ‘centered locations’ assumption 푼1 = 0 allows us to obtain a global linear rate of
convergence in both the 푨and 풃parameters. This is not a restrictive assumption, as in practice it is always
possible to center the spike scene (e.g., by computing its center of mass and subtracting), and we also ﬁnd
it to accelerate convergence empirically when it is applied.
14

5.2
Experimental Veriﬁcation
To verify the practical implications of Theorem 5.1, which is formulated in the continuum, we conduct
numerical experiments on registering aﬃne-transformed multichannel spike images using the discrete
formulation (10). We implement a proximal gradient descent solver for (10), and use it to register randomly-
transformed occurrence maps, as visualized in Figure 4(a-b). We set the step sizes and level of smoothing
in accordance with (16) and (17), with a complementary smoothing value of 휎0 = 3.
Figure 4 shows
representative results taken from one such run: the objective value rapidly converges to near working
precision, and the normalized cross-correlation between the transformed scene and the motif rapidly reaches
a value of 0.972. This rapid convergence implies the formulation (10) is a suitable base for an unrolled
architecture with mild depth, and is a direct consequence of the robust step size prescription oﬀered
by Theorem 5.1. Figure 4(f) plots the left-hand and right-hand sides of the parameter error bound (18)
to evaluate its applicability to the discretized formulation: we observe an initial faster-than-predicted
linear rate, followed by saturation at a suboptimal value. This gap is due to the diﬀerence between the
continuum theory of Theorem 5.1 and practice: in the discretized setting, interpolation errors and ﬁnite-
resolution artifacts prevent subpixel-perfect registration of the parameters, and hence exact recovery of
the transformation (푨★, 풃★). In practice, successful registration of the spike scene, as demonstrated by
Figure 4(e), is suﬃcient for applications, as in the networks we develop for hierarchical detection in Section 4.
6
Basin of Attraction for Textured Motif Registration with (4)
The theory and experiments we have presented in Section 5 justify the use of local optimization for alignment
of spiky motifs. In this section, we provide additional corroboration beyond the experiment of Figure 1
of the eﬃcacy of our textured motif registration formulation (4), under euclidean and similarity motion
models. To this end, in Figure 5 we empirically evaluate the basin of attraction of a suitably-conﬁgured
solver for registration of the crab body motif from Figure 2 with this formulation. Two-dimensional search
grids are generated for each of the two setups as shown in the ﬁgure. For each given pair of transformation
parameters, a similar multi-scale scheme over 휎as in the above complexity experiment is used, starting
at 휎= 10 and step size 0.05, and halved every 50 iterations. The process terminates after a total of 250
iterations. The ﬁnal ZNCC calculated over the motif support is reported, and the ﬁgure plots the average
over 10 independent runs, where the background image is randomly generated for each pair of parameters in
each run. The ZNCC ranges from 0 to 1, with a value of 1 implying equality of the channel-mean-subtracted
motif and transformed image content over the corresponding support (up to scale).
Panels (a) and (b) of Figure 5 show that the optimization method tends to succeed unconditionally up to
moderate amounts of transformation. For larger sets of transformations, it is important to ﬁrst appropriately
center the image, which will signiﬁcantly improve the optimization performance. In practice, one may use
a combination of optimization and a small number of covering, so that the entire transformation space is
covered by the union of the basins of attractions. We note that irregularity near the edges, especially in
panel (a), can be attributed in part due to the randomness in the background embedding, and in this sense
the size of the basin in these results conveys a level of performance across a range of simulated operating
conditions. In general, these basins are also motif-dependent: we would expect these results to change if
we were testing with the eye motif from Figure 3(a), for example. A notable phenomenon in Figure 5(b),
where translation is varied against scale, is the lack of a clear-cut boundary of the basin at small scales.
This is due to the eﬀect illustrated in Figure 5(c-d), where interpolation artifacts corrupt the motif when
it is ‘zoomed out’ by optimization over deformations, and hence registration can never achieve a ZNCC
close to 1. For applications where perfect reconstruction is not required, such as the hierarchical detection
task studied in Section 4, these interpolation artifacts will not hinder the ability to localize the motif in the
scene at intermediate scales, and if the basin were generated with a success metric other than ZNCC, a
better-deﬁned boundary to the basin would emerge.
15

Translation (pixel)
Rotation (deg)
−30
−20
−10
0
10
20
30
−50
0
50
0
0.2
0.4
0.6
0.8
1
Translation (pixel)
log2( Scale )
−30
−20
−10
0
10
20
30
−2
−1
0
1
0
0.2
0.4
0.6
0.8
1
(a) Translation vs rotation
(b) Translation vs scale
(c) Example of zoomed-out object
(d) Interpolation artifacts
Figure 5: Plotting a basin of attraction for the textured motif registration formulation (4). (a): Heatmap of the ZNCC at
convergence (see Appendix A.3), for translation versus rotation. Optimization conducted with SE(2) motion model. (b):
Heatmap of the ZNCC at convergence, for translation versus scale. Optimization conducted in ‘similarity mode’, a SE(2)
motion model with an extra global scale parameter. In both experiments, each reported data point is averaged over 10
independent runs. (c-d): Notably, when the registration target 풚is zoomed out relative to the motif 풙표, resolution is lost
in the detection target, so recovering it will cause interpolation artifacts and blur the image. This prevents the ZNCC
value from converging to 1 despite correct alignment with the motif, and accounts for the results shown in (b) at small
scales.
7
Discussion
In this paper, we have taken initial steps towards realizing the potential of optimization over transformations
of domain as an approach to achieve resource-eﬃcient invariance with visual data. Below, we discuss several
important future directions for the basic framework we have developed.
Statistical variability and complex tasks.
To build invariant networks for complex visual tasks and real-
world data beyond matching against ﬁxed templates 풙표, it will be necessary to incorporate more reﬁned
appearance models for objects, such as a sparse dictionary model or a deep generative model [BDS19;
DTLW+21; SSKK+21], and train the resulting hybrid networks in an end-to-end fashion. The invariant
architectures we have designed in this work naturally plug into such a framework, and will allow for
investigations similar to what we have developed in Section 4 into challenging tasks with additional structure
(e.g., temporal or 3D data). Coping with the more complex motion models in these applications will demand
regularizers 휌for our general optimization formulation (1) that go beyond parametric constraints.
Theory for registration of textured motifs in visual clutter.
Our experiments in Section 5 have demon-
strated the value that theoretical studies of optimization formulations have with respect to the design of
the corresponding unrolled networks. Extending our theory for spiky motif registration to more general
textured motifs will enable similar insights into the roles played by the various problem parameters in a
formulation like (4) with respect to texture and shape properties of visual data and the clutter present, and
16

allow for similarly resource-eﬃcient architectures to be derived in applications like the hierarchical template
detection task we have developed in Section 4.3.
Hierarchical detection networks in real-time.
The above directions will enable the networks we have
demoed for hierarchical detection in Section 4 to scale to more general data models. At the same time,
there are promising directions to improve the eﬃciency of the networks we design for a task like this
one at the modeling level. For example, the networks we design in Section 4.3 essentially operate in a
‘sliding window’ fashion, without sharing information across the strides 흀, and they perform registration
and detection separately. An architecture developed around an integrated approach to registration and
detection, possibly building oﬀadvances in convolutional sparse modeling [QLZ19; KZLW20; LQKZ+20],
may lead to further eﬃciency gains and push the resulting network closer to real-time operation capability.
Acknowledgments
This work was supported by the National Science Foundation through grants NSF 1733857, NSF 1838061,
NSF 1740833, and NSF 2112085, and by a fellowship award (SB) through the National Defense Science
and Engineering Graduate (NDSEG) Fellowship Program, sponsored by the Air Force Research Laboratory
(AFRL), the Oﬃce of Naval Research (ONR) and the Army Research Oﬃce (ARO). The authors thank
Mariam Avagyan, Ben Haeﬀele, and Yi Ma for helpful discussions and feedback.
References
[SW71]
Elias M Stein and Guido Weiss. Introduction to Fourier Analysis on Euclidean Spaces. Princeton
University Press, 1971.
[HS81]
Berthold K P Horn and Brian G Schunck. “Determining optical ﬂow”. Artif. Intell. 17.1 (Aug.
1981), pp. 185–203.
[Key81]
R Keys. “Cubic convolution interpolation for digital image processing”. IEEE Trans. Acoust.
29.6 (Dec. 1981), pp. 1153–1160.
[LK81]
Bruce D Lucas and Takeo Kanade. “An iterative image registration technique with an applica-
tion to stereo vision”. Proceedings of the 7th international joint conference on Artiﬁcial intelligence
- Volume 2. ĲCAI’81. Vancouver, BC, Canada: Morgan Kaufmann Publishers Inc., Aug. 1981,
pp. 674–679.
[Ana89]
P Anandan. “A computational framework and an algorithm for the measurement of visual
motion”. Int. J. Comput. Vis. 2.3 (Jan. 1989), pp. 283–310.
[Bro92]
Lisa Gottesfeld Brown. “A survey of image registration techniques”. ACM Comput. Surv. 24.4
(Dec. 1992), pp. 325–376.
[CET98]
T F Cootes, G J Edwards, and C J Taylor. “Active appearance models”. Computer Vision —
ECCV’98. Springer Berlin Heidelberg, 1998, pp. 484–498.
[MV98]
J B Antoine Maintz and Max A Viergever. “A survey of medical image registration”. Med.
Image Anal. 2.1 (Mar. 1998), pp. 1–36.
[LC01]
MartinLefébureandLaurentDCohen. “ImageRegistration,OpticalFlowand LocalRigidity”.
J. Math. Imaging Vis. 14.2 (Mar. 2001), pp. 131–147.
[BM04]
Simon Baker and Iain Matthews. “Lucas-Kanade 20 Years On: A Unifying Framework”. Int.
J. Comput. Vis. 56.3 (Feb. 2004), pp. 221–255.
[Sze07]
Richard Szeliski. “Image Alignment and Stitching: A Tutorial”. Foundations and Trends® in
Computer Graphics and Vision 2.1 (2007), pp. 1–104.
[FMR08]
Pedro Felzenszwalb, David McAllester, and Deva Ramanan. “A discriminatively trained,
multiscale, deformable part model”. 2008 IEEE Conference on Computer Vision and Pattern
Recognition. June 2008, pp. 1–8.
17

[HSS08]
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. “Exploring Network Structure, Dy-
namics, and Function using NetworkX”. Proceedings of the 7th Python in Science Conference.
Pasadena, CA USA, 2008, pp. 11–15.
[BS10]
Leah Bar and Guillermo Sapiro. “Hierarchical dictionary learning for invariant classiﬁcation”.
2010 IEEE International Conference on Acoustics, Speech and Signal Processing. ieeexplore.ieee.org,
Mar. 2010, pp. 3578–3581.
[FGMR10]
Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. “Object detec-
tion with discriminatively trained part-based models”. IEEE Trans. Pattern Anal. Mach. Intell.
32.9 (Sept. 2010), pp. 1627–1645.
[GL10]
Karol Gregor and Yann LeCun. “Learning fast approximations of sparse coding”. Proceedings
of the 27th International Conference on Machine Learning (ICML-10). 2010, pp. 399–406.
[JMOB10]
Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, and Francis R Bach. “Proximal Meth-
ods for Sparse Hierarchical Dictionary Learning”. ICML. 2010, pp. 487–494.
[HKW11]
Geoﬀrey E Hinton, Alex Krizhevsky, and Sida D Wang. “Transforming Auto-Encoders”.
Artiﬁcial Neural Networks and Machine Learning – ICANN 2011. Springer Berlin Heidelberg,
2011, pp. 44–51.
[CMS12]
Dan Cireşan, Ueli Meier, and Juergen Schmidhuber. “Multi-column deep neural networks for
image classiﬁcation”. 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Los Alamitos, CA, USA: IEEE Computer Society, June 2012, pp. 3642–3649.
[Mal12]
Stéphane Mallat. “Group Invariant Scattering”. Commun. Pure Appl. Math. 65.10 (Oct. 2012),
pp. 1331–1398.
[MZM12]
Hossein Mobahi, C Lawrence Zitnick, and Yi Ma. “Seeing through the blur”. 2012 IEEE
Conference on Computer Vision and Pattern Recognition. June 2012, pp. 1736–1743.
[SL12]
Kihyuk Sohn and Honglak Lee. “Learning invariant representations with local transforma-
tions”. Proceedings of the 29th International Coference on International Conference on Machine
Learning. ICML’12. Edinburgh, Scotland: Omnipress, June 2012, pp. 1339–1346.
[SM12]
Charles Sutton and Andrew McCallum. “An Introduction to Conditional Random Fields”.
Foundations and Trends® in Machine Learning 4.4 (2012), pp. 267–373.
[BM13]
Joan Bruna and Stéphane Mallat. “Invariant scattering convolution networks”. IEEE Trans.
Pattern Anal. Mach. Intell. 35.8 (Aug. 2013), pp. 1872–1886.
[KSJ14]
Angjoo Kanazawa, Abhishek Sharma, and David Jacobs. “Locally Scale-Invariant Convolu-
tional Neural Networks” (Dec. 2014). arXiv: 1412.5104 [cs.CV].
[KF14]
Soﬁa Karygianni and Pascal Frossard. “Tangent-based manifold approximation with locally
linear models”. Signal Processing 104 (2014), pp. 232–247.
[PB14]
Neal Parikh and Stephen Boyd. “Proximal Algorithms”. Foundations and Trends® in Optimiza-
tion 1.3 (2014), pp. 127–239.
[VF14]
Elif Vural and Pascal Frossard. “Analysis of Image Registration with Tangent Distance”. SIAM
J. Imaging Sci. 7.4 (Jan. 2014), pp. 2860–2915.
[DWD15]
Sander Dieleman, Kyle W. Willett, and Joni Dambre. “Rotation-invariant convolutional neural
networks for galaxy morphology prediction”. Monthly Notices of the Royal Astronomical Society
450.2 (Apr. 2015), pp. 1441–1459.
[FF15]
Alhussein Fawzi and Pascal Frossard. “Manitest: Are classiﬁers really invariant?” British
Machine Vision Conference (BVMC). 2015.
[GIDM15]
Ross Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik. “Deformable part models
are convolutional neural networks”. Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition. openaccess.thecvf.com, 2015, pp. 437–446.
[JSZK15]
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. “Spatial
Transformer Networks”. Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 2. NIPS’15. Montreal, Canada: MIT Press, 2015, pp. 2017–2025.
18

[PVGR15]
Marco Pedersoli, Andrea Vedaldi, Jordi Gonzàlez, and Xavier Roca. “A coarse-to-ﬁne ap-
proach for fast deformable object detection”. Pattern Recognition 48.5 (May 2015), pp. 1844–
1853.
[ZMH15]
Maxim Zaitsev, Julian Maclaren, and Michael Herbst. “Motion artifacts in MRI: A complex
problem with many partial solutions”. J. Magn. Reson. Imaging 42.4 (Oct. 2015), pp. 887–901.
[CW16]
Taco Cohen and Max Welling. “Group Equivariant Convolutional Networks”. Proceedings of
The 33rd International Conference on Machine Learning. Vol. 48. Proceedings of Machine Learning
Research. New York, New York, USA: PMLR, 2016, pp. 2990–2999.
[LSBP16]
Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, and Marc Pollefeys. “TI-Pooling:
Transformation-Invariant Pooling for Feature Learning in Convolutional Neural Networks”.
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2016.
[BJ17]
Ronen Basri and David W Jacobs. “Eﬃcient Representation of Low-Dimensional Manifolds
using Deep Networks”. 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
[DQXL+17]
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. “De-
formable convolutional networks”. Proceedings of the IEEE international conference on computer
vision. 2017, pp. 764–773.
[KKHP17]
Erich Kobler, Teresa Klatzer, Kerstin Hammernik, and Thomas Pock. “Variational Networks:
Connecting Variational Methods and Deep Learning”. Pattern Recognition. Lecture Notes in
Computer Science. Springer, Cham, Sept. 2017, pp. 281–293.
[OBZ17]
Edouard Oyallon, Eugene Belilovsky, and Sergey Zagoruyko. “Scaling the Scattering Trans-
form: Deep Hybrid Networks”. Proceedings of the IEEE International Conference on Computer
Vision (ICCV). Oct. 2017.
[SFH17]
Sara Sabour, Nicholas Frosst, and Geoﬀrey E. Hinton. “Dynamic Routing Between Capsules”.
NIPS. 2017, pp. 3859–3869.
[SER17]
Max Simchowitz, Ahmed El Alaoui, and Benjamin Recht. “On the Gap Between Strict-Saddles
and True Convexity: An Omega(log d) Lower Bound for Eigenvector Approximation” (Apr.
2017). arXiv: 1704.04548 [cs.LG].
[WGTB17]
Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow.
“Harmonic Networks: Deep Translation and Rotation Equivariance”. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). July 2017.
[BHKW18]
S Buchanan, T Haque, P Kinget, and J Wright. “Eﬃcient Model-Free Learning to Overcome
Hardware Nonidealities in Analog-to-Information Converters”. 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP). Apr. 2018, pp. 3574–3578.
[CLWY18]
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. “Theoretical Linear Convergence
of Unfolded ISTA and Its Practical Weights and Thresholds”. Proceedings of the 32nd Interna-
tional Conference on Neural Information Processing Systems. NIPS’18. Montréal, Canada, 2018,
pp. 9079–9089.
[CGKW18]
Taco S Cohen, Mario Geiger, Jonas Köhler, and Max Welling. “Spherical CNNs”. International
Conference on Learning Representations. 2018.
[KMF18]
C Kanbak, S Moosavi-Dezfooli, and P Frossard. “Geometric Robustness of Deep Networks:
Analysis and Improvement”. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. June 2018, pp. 4441–4449.
[KT18]
Risi Kondor and Shubhendu Trivedi. “On the Generalization of Equivariance and Convolu-
tion in Neural Networks to the Action of Compact Groups”. Proceedings of the 35th International
Conference on Machine Learning. Vol. 80. Proceedings of Machine Learning Research. PMLR,
Oct. 2018, pp. 2747–2755.
[WB18]
Thomas Wiatowski and Helmut Bölcskei. “A Mathematical Theory of Deep Convolutional
Neural Networks for Feature Extraction”. IEEE Trans. Inf. Theory 64.3 (Mar. 2018), pp. 1845–
1866.
19

[XZLH+18]
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. “Spatially
Transformed Adversarial Examples”. International Conference on Learning Representations. 2018.
[AAG19]
Rima Alaifari, Giovanni S Alberti, and Tandri Gauksson. “ADef: an Iterative Algorithm
to Construct Adversarial Deformations”. International Conference on Learning Representations.
2019.
[ALGW+19]
Michael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh
Nguyen. “Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar
objects”. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Long
Beach, CA, USA: IEEE, June 2019.
[AW19]
Aharon Azulay and Yair Weiss. “Why do deep convolutional networks generalize so poorly
to small image transformations?” Journal of Machine Learning Research 20.184 (2019), pp. 1–25.
[BDS19]
Andrew Brock, JeﬀDonahue, and Karen Simonyan. “Large Scale GAN Training for High
Fidelity Natural Image Synthesis”. International Conference on Learning Representations. 2019.
[CJLZ19]
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. “Nonparametric Regression on
Low-Dimensional Manifolds using Deep ReLU Networks” (Aug. 2019). arXiv: 1908.01842
[cs.LG].
[CZMV+19]
Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vĳay Vasudevan, and Quoc V. Le. “AutoAug-
ment: Learning Augmentation Strategies From Data”. Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). June 2019.
[ETTS+19]
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry.
“Exploring the Landscape of Spatial Robustness”. International Conference on Machine Learning.
2019, pp. 1802–1811.
[GBW19]
Dar Gilboa, Sam Buchanan, and John Wright. “Eﬃcient Dictionary Learning with Gradient
Descent”. Proceedings of the 36th International Conference on Machine Learning. Vol. 97. Proceed-
ings of Machine Learning Research. Long Beach, California, USA: PMLR, 2019, pp. 2252–
2259.
[KZFM19]
Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. “Learning 3D Human
Dynamics From Video”. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). June 2019.
[KBCW19]
Michael Kellman, Emrah Bostan, Michael Chen, and Laura Waller. “Data-Driven Design
for Fourier Ptychographic Microscopy”. 2019 IEEE International Conference on Computational
Photography (ICCP). 2019, pp. 1–8.
[KBRW19]
Michael R. Kellman, Emrah Bostan, Nicole A. Repina, and Laura Waller. “Physics-Based
Learned Design: Optimized Coded-Illumination for Quantitative Phase Imaging”. IEEE Trans-
actions on Computational Imaging 5.3 (2019), pp. 344–353.
[LCWY19]
Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. “ALISTA: Analytic Weights Are
As Good As Learned Weights in LISTA”. International Conference on Learning Representations.
2019.
[PGML+19]
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. “PyTorch: An Imperative Style,
High-Performance Deep Learning Library”. Advances in Neural Information Processing Systems
32. 2019, pp. 8024–8035.
[QLZ19]
Qing Qu, Xiao Li, and Zhihui Zhu. “A Nonconvex Approach for Exact and Eﬃcient Mul-
tichannel Sparse Blind Deconvolution”. Advances in Neural Information Processing Systems.
Vol. 32. 2019.
[Sch19]
Johannes Schmidt-Hieber. “Deep ReLU network approximation of functions on a manifold”
(Aug. 2019). arXiv: 1908.00695 [stat.ML].
20

[CKNH20]
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. “A Simple Frame-
work for Contrastive Learning of Visual Representations”. Proceedings of the 37th International
Conference on Machine Learning. Vol. 119. Proceedings of Machine Learning Research. PMLR,
2020, pp. 1597–1607.
[HMCZ+20]
Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji
Lakshminarayanan. “Augmix: A simple method to improve robustness and uncertainty under
data shift”. International conference on learning representations. Vol. 1. 2020, p. 6.
[KZLW20]
Han-Wen Kuo, Yuqian Zhang, Yenson Lau, and John Wright. “Geometry and Symmetry in
Short-and-Sparse Deconvolution”. SIAM Journal on Mathematics of Data Science 2.1 (Jan. 2020),
pp. 216–245.
[LQKZ+20]
Yenson Lau, Qing Qu, Han-Wen Kuo, Pengcheng Zhou, Yuqian Zhang, and John Wright.
“Short and Sparse Deconvolution — A Geometric Approach”. International Conference on
Learning Representations. 2020.
[NI20]
Ryumei Nakada and Masaaki Imaizumi. “Adaptive Approximation and Generalization of
Deep Neural Network with Intrinsic Dimensionality”. J. Mach. Learn. Res. 21.174 (2020),
pp. 1–38.
[OJMB+20]
Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis,
and Rebecca Willett. “Deep Learning Techniques for Inverse Problems in Imaging”. IEEE
Journal on Selected Areas in Information Theory 1.1 (May 2020), pp. 39–56.
[ZTAM20]
John Zarka, Louis Thiry, Tomas Angles, and Stephane Mallat. “Deep Network Classiﬁca-
tion by Scattering and Homotopy Dictionary Learning”. International Conference on Learning
Representations. 2020.
[BBCV21]
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. “Geometric Deep Learn-
ing: Grids, Groups, Graphs, Geodesics, and Gauges” (Apr. 2021). arXiv: 2104.13478 [cs.LG].
[CDHS21]
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. “Lower bounds for ﬁnding
stationary points II: ﬁrst-order methods”. Math. Program. 185.1 (Jan. 2021), pp. 315–355.
[CCCH+21]
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang,
and Wotao Yin. “Learning to Optimize: A Primer and A Benchmark” (Mar. 2021). arXiv:
2103.12828 [math.OC].
[CK21]
Alexander Cloninger and Timo Klock. “A deep network construction that adapts to intrinsic
dimensionality beyond the domain”. Neural Networks 141 (2021), pp. 404–419.
[DTLW+21]
Xili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Kwan Ho Ryan Chan, Pengyuan Zhai,
Yaodong Yu, Michael Psenka, Xiaojun Yuan, Heung Yeung Shum, and Yi Ma. “Closed-Loop
Data Transcription to an LDR via Minimaxing Rate Reduction” (Nov. 2021). arXiv: 2111.06636
[cs.CV].
[Hin21]
Geoﬀrey Hinton. “How to represent part-whole hierarchies in a neural network” (Feb. 2021).
arXiv: 2102.12627 [cs.CV].
[JL21]
Ylva Jansson and Tony Lindeberg. “Scale-invariant scale-channel networks: Deep networks
that generalise to previously unseen scales”. CoRR abs/2106.06418 (2021). arXiv: 2106.06418.
[MRRK+21]
Matthew J. Muckley, Bruno Riemenschneider, Alireza Radmanesh, Sunwoo Kim, Geunu
Jeong, Jingyu Ko, Yohan Jun, Hyungseob Shin, Dosik Hwang, Mahmoud Mostapha, Simon
Arberet, Dominik Nickel, Zaccharie Ramzi, Philippe Ciuciu, Jean-Luc Starck, Jonas Teuwen,
DimitriosKarkalousos,ChaopingZhang, AnuroopSriram,ZhengnanHuang,NaﬁssaYakubova,
Yvonne W. Lui, and Florian Knoll. “Results of the 2020 fastMRI Challenge for Machine Learn-
ing MR Image Reconstruction”. IEEE Transactions on Medical Imaging 40.9 (2021), pp. 2306–
2317.
[SSKK+21]
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. “Score-Based Generative Modeling through Stochastic Diﬀerential Equations”.
International Conference on Learning Representations. 2021.
21

[STDS+21]
Weiwei Sun, Andrea Tagliasacchi, Boyang Deng, Sara Sabour, Soroosh Yazdani, Geoﬀrey E
Hinton, and Kwang Moo Yi. “Canonical Capsules: Self-Supervised Capsules in Canonical
Pose”. Advances in Neural Information Processing Systems 34 (2021).
[WFVW21]
Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling. “Coordinate Independent
Convolutional Networks – Isometry and Gauge Equivariant Convolutions on Riemannian
Manifolds” (June 2021). arXiv: 2106.06020 [cs.LG].
[ZGM21]
John Zarka, Florentin Guth, and Stéphane Mallat. “Separation and Concentration in Deep
Networks”. International Conference on Learning Representations. 2021.
22

A
Implementation and Experimental Details
A.1
Implementation Details for Parametric Transformations of the Image Plane
Our implementation of parametric image deformations revolves around the speciﬁc deﬁnition of interpola-
tion we have made:
풚◦흉=
Õ
(푘,푙)∈ℤ2
푦푘푙휙(흉0 −푘1) ⊙휙(흉1 −푙1),
and the identiﬁcation of the image 풚∈ℝ푚×푛with a function on ℤ2 with support in {0, . . . , 푚−1}×{0, . . . , 푛−
1}.5 Although we use the notation ◦for interpolation in analogy with the usual notation for composition of
functions, this operation is signiﬁcantly less well-structured: although we can deﬁne interpolation of motion
ﬁelds 흉0 ◦흉1, it is impossible in general to even have associativity of ◦(let alone inverses), so that in general
(풙◦흉0) ◦흉1 ≠풙◦(흉0 ◦흉1). This failure is intimately linked to the existence of parasitic interpolation artifacts
when computing and optimizing with interpolated images, which we go to great lengths to avoid in our
experiments. On the other hand, there does exist a well-deﬁned identity vector ﬁeld: from our deﬁnitions,
we can read oﬀthe canonical deﬁnition of the identity transformation, from which deﬁnitions for other
parametric transformations we consider here follow. Deﬁning (with a slight abuse of notation)
풎=

0
1
...
푚−1

;
풏=

0
1
...
푛−1

,
we have from the deﬁnition of the cubic convolution interpolation kernel 휙that
풚◦(풎1∗⊗풆0 + 1풏∗⊗풆1) = 풚.
One can then check that the following linear embedding of the aﬃne transformations, which we will write
as Aﬀ(2) = GL(2)×ℝ2, leads to the natural vector ﬁeld analogue of aﬃne transformations on the continuum
ℝ2 (c.f. Appendix B):
Aﬀ(2)  span{풎1∗⊗풆0, 1풏∗⊗풆0, 풎1∗⊗풆1, 1풏∗⊗풆1, 1푚,푛⊗풆0, 1푚,푛⊗풆1}.
(19)
Of course, these vector ﬁelds can be any size—they need not match the size of the image. As we mention in
Section 3.3, we always initialize our networks with the identity transform; in the basis above, this corresponds
to the vector (1, 0, 0, 1, 0, 0) (i.e., this is like a row-wise ﬂattening of the aﬃne transform’s matrix 푨∈GL(2),
concatenated with 풃).
Next we turn to computation of the proximal operator, which we need for unrolling (see Section 3.3).
Given (6) and the fact that (19) is a subspace, we can compute the proximal operator for Aﬀ(2) given an
orthonormal basis for Aﬀ(2). It is then unfortunate that the natural basis vectors that we have used in the
expression (19) are not orthogonal: we have ⟨풎1∗, 1풏∗⟩= ⟨풎, 1⟩⟨풏, 1⟩≫0, for example. To get around
this, in practice we apply a technique we refer to as centering of transformations. Indeed, notice that for any
풄∈ℝ2, we have
Aﬀ(2)  span{(풎−푐01)1∗⊗풆0, 1(풏−푐11)∗⊗풆0, (풎−푐01)1∗⊗풆1, 1(풏−푐11)∗⊗풆1, 1푚,푛⊗풆0, 1푚,푛⊗풆1}
+ 1푚,푛⊗풄.
(20)
In the continuum, applying an aﬃne transform in this way corresponds to the mapping 풙↦→푨(풙−풄)+풃+풄,
hence the name: the image plane is shifted to have its origin at 풄for the purposes of applying the transform.
When we implement aﬃne transforms as suggested by (20), we choose 풄to make the basis vectors orthogonal
5These conventions are not universal, although they seem most natural from a mathematical standpoint—for example, PyTorch
thinks of its images as lying on a grid in the square [−1, +1] × [−1, +1] instead, with spacing and oﬀsets depending on the image
resolution and other implementation-speciﬁc options. In our released code, we handle conversion from our notation to this notation.
23

this necessitates that 풄= ((푚−1)/2, (푛−1)/2). Then we are able to write down a concrete expression for
the projection operator in these coordinates:6
projAﬀ(2)(흉) =  (풎−푚−1
2 1)∗흉01, 1∗흉0(풏−푛−1
2 1), (풎−푚−1
2 1)∗흉11, 1∗흉1(풏−푛−1
2 1), 1∗흉01, 1∗흉11
.
(21)
The low-rank structure of the basis vectors implies that this transformation can be computed quite rapidly.
Although it may seem we have undertaken this discussion for the sake of mathematical rigor, in our
experiments we observe signiﬁcant computational beneﬁts to centering by the prescription above.
For
example, when computing with (10), using a non-orthogonal basis for the aﬃne transforms (or a center that
is not at the center of the region being transformed) often leads to skewing artifacts in the ﬁnal transformation
recovered. We also notice slower convergence.
Finally, for our experiments in Section 4 with the rigid motion model SE(2), some additional discussion
is required. This is because the orthogonal transformations SO(2) are not a linear subspace, like the aﬃne
transforms (19), but a smooth manifold (diﬀeomorphic to a circle). For these transformations, we modify
the formula (1) by diﬀerentiating in a parameterization of SE(2): concretely, we use
SO(2) 

cos 휃
−sin 휃
sin 휃
cos 휃
  휃∈[0, 2휋]

.
Writing 퐹: ℝ→ℝ푚×푛×2 for this parameterization composed with our usual vector ﬁeld representation
(20) for subgroups of the aﬃne transforms, we modify the objective (1) to be min휃휑(풚◦퐹(휃)). A simple
calculation then shows that gradients in this parameterization are obtainable from gradients with respect
to the aﬃne parameterization as
∇휃[휑(풚◦퐹)](휃) =

∇푨[휑(풚◦흉· ,풃)]

cos 휃
−sin 휃
sin 휃
cos 휃

,

−sin 휃
−cos 휃
cos 휃
−sin 휃

.
This is a minor extra nonlinearity that replaces the proximal operation when we unroll networks as in
(5) with this motion model. Gradients and projections with respect to the translation parameters are no
diﬀerent from the aﬃne case.
A.2
Gradient Calculations for Unrolled Network Architectures
We collect in this section several computations relevant to gradients of the function 휑(following the structure
of (1)) in the optimization formulations (2), (3), (4) and (10).
흉gradients.
All of the costs we consider use the ℓ2 error ∥· ∥퐹, so their gradient calculations with respect
to 흉are very similar. We will demonstrate the gradient calculation for (2) to show how (6) is derived; the
calculations for other costs follow the same type of argument. To be concise, we will write ∇흉휑for the
gradient with respect to 흉of the relevant costs 휑(풚◦흉).
Proposition A.1. Let 휑denote the ∥· ∥퐹cost in (2). One has
∇흉휑(흉) =
푐−1
Õ
푘=0
 품휎2 ∗풫Ω

품휎2 ∗ 풚◦흉−풙표

푘

⊗12

⊙ d풚푘◦흉
.
Proof. The cost separates over channels, so by linearity of the gradient it suﬃces to assume 푐= 1. We
proceed by calculating the diﬀerential of 휑(풚◦흉) with respect to 흉. We have for 횫of the same shape as 흉
and 푡∈ℝ
휕
휕푡

푡=0
휑(풚◦(흉+ 푡횫)) =

풫Ω

품휎2 ∗(풚◦흉−풙표)

⊗12, 풫Ω

품휎2 ∗ (d풚◦흉) ⊙횫
,
6In practice, our choice of step size is made to scale each element in this basis to be orthonormal (in particular, applying diﬀerent
steps to the matrix and translation parameters of the transformation)—strictly speaking the projection in (21) is not the orthogonal
projection because this extra scaling has not been applied. We do not specify this scaling here because its optimal value often depends
on the image content: for example, see the step size prescriptions in Theorem 5.1.
24

where d풚∈ℝ푚×푛×2 is the Jacobian matrix of 풚, deﬁned as (here ¤휙denotes the derivative of the cubic
convolution interpolation kernel 휙)
d풚0 =
Õ
(푘,푙)∈ℤ2
푦푘푙¤휙(풎1∗−푘1) ⊙휙(1풏∗−푙1),
d풚1 =
Õ
(푘,푙)∈ℤ2
푦푘푙휙(풎1∗−푘1) ⊙¤휙(1풏∗−푙1),
and where for concision we are writing 품휎2 ∗d풚to denote the ﬁltering of each of the two individual channels
of d풚by 품휎2. Using three adjoint relations (풫Ω is an orthogonal projection, hence self-adjoint; the adjoint of
convolution by 품휎2 is cross-correlation with 품휎2; elementwise multiplication is self-adjoint) and a property
of the tensor product, the claim follows.
□
Convolutional representation of cost-smoothed formulation (3).
The cost-smoothed formulation (3) can
be directly expressed as a certain convolution with 품휎2, leading to very fast convolution-free inner loops in
gradient descent implementation. To see this, write
풫Ω

풚◦(흉+ 흉0,횫) −풙표
2
퐹=
풫Ω

풚◦(흉+ 흉0,횫)
2
퐹+ ∥풫Ω [풙표]∥2
퐹+ 2

풫Ω

풚◦(흉+ 흉0,횫)

, 풫Ω [풙표]

=
D
풚◦(흉+ 흉0,횫)
 ⊙2 , 풫Ω [1]
E
+ ∥풫Ω [풙표]∥2
퐹+ 2

풚◦(흉+ 흉0,횫), 풫Ω [풙표]

,
using self-adjointness of 풫Ω and the fact that it can be represented as an elementwise multiplication, and
writing [ · ]⊙2 for elementwise squaring. Thus, denoting the ∥· ∥퐹cost in (3) by 휑(흉), 휑can be written as
2휑(흉) =
*Õ
횫
(품휎2)횫

풚◦(흉+ 흉0,횫)
 ⊙2 , 풫Ω [1]
+
+

품휎2, 1

∥풫Ω [풙표]∥2
퐹
+ 2
*Õ
횫
(품휎2)횫· 풚◦(흉+ 흉0,횫), 풫Ω [풙표]
+
.
This can be expressed as a cross-correlation with 품휎2:
2휑(흉) =
D
품휎2 ∗

풚◦흉
 ⊙2 , 풫Ω [1]
E
+

품휎2, 1

∥풫Ω [풙표]∥2
퐹+ 2

품휎2 ∗(풚◦흉), 풫Ω [풙표]

,
and taking adjoints gives ﬁnally
2휑(흉) =
D
풚◦흉
 ⊙2 , 품휎2 ∗풫Ω [1]
E
+

품휎2, 1

∥풫Ω [풙표]∥2
퐹+ 2

풚◦흉, 품휎2 ∗풫Ω [풙표]

.
This gives a convolution-free gradient step implementation for this cost (aside from pre-computing the ﬁxed
convolutions in the cost), and also yields a useful interpretation of the cost-smoothed formulation (3), and
its disadvantages relative to the background-modeled formulation (4).
Filter gradient for complementary smoothing formulation (10).
Relative to the standard registration
model formulation (2), the complementary smoothing spike registration formulation (10) contains an extra
complicated transformation-dependent gaussian ﬁlter. We provide a key lemma below for the calculation
of the gradient with respect to the parameters of the complementary smoothing cost in “standard param-
eterization” (see the next paragraph below). The full calculation follows the proof of Proposition A.1 with
an extra “product rule” step and extra adjoint calculations.
Proposition A.2. Given ﬁxed 휎2 > 휎2
0 > 0, deﬁne 횺(푨) = 휎2푰−휎2
0(푨∗푨)−1, and deﬁne
품(푨) =
p
det(푨∗푨)품횺(푨),
where the ﬁlter is 푚by 푛and the domain is the open set {푨| 흈푰−흈2
0(푨∗푨)−1 ≻0}. Then for any ﬁxed 푽∈ℝ푚×푛,
one has
∇푨[⟨푽, 품⟩](푨) = 휎2
0푨−∗©­
«
횺(푨)−1 ©­
«
Õ
푖,푗
푉푖푗푔(푨)푖푗풘푖푗풘∗
푖푗
ª®
¬
횺(푨)−1 −⟨품(푨), 푽⟩횺(푨)−1ª®
¬
(푨∗푨)−1 + ⟨품(푨), 푽⟩푨−∗,
where 푨−∗= (푨−1)∗.
25

Proof. For (푖, 푗) ∈{0, . . . , 푚−1} × {0, . . . , 푛−1}, let 풘푖푗= (푖−⌊푚/2⌋, 푗−⌊푛/2⌋). Then we have
품(푨) = 1
2휋
Õ
푖,푗
풆푖푗exp

−1
2풘∗
푖푗횺(푨)−1풘푖푗−1
2 log det Σ(푨) + 1
2 log det 푨∗푨

.
Let d품denote the diﬀerential of 푨↦→품(푨). By the chain rule, we have for any 횫∈ℝ2×2

푽, d품푨(횫)

= 1
2
Õ
푖,푗
푉푖푗푔(푨)푖푗
휕
휕푡

푡=0
h
−풘∗
푖푗횺(푨+ 푡횫)−1풘푖푗−log det Σ(푨+ 푡횫) + log det(푨+ 푡횫)∗(푨+ 푡횫)
i
.
We need the diﬀerential of several mappings here. We will use repeatedly that if 푿∈GL(2) and 푾∈ℝ2×2,
one has
d[푿↦→

푾, 푿−1
]푿(횫) = −⟨횫, 푿−∗푾푿−∗⟩.
(22)
Applying (22) and the chain rule, we get
d[⟨푾, 횺⟩]푨(횫) = 휎2
0

(푨∗푨)−1푾(푨∗푨)−1, 횫∗푨+ 푨∗횫

= 휎2
0

푨−∗(푾+ 푾∗)(푨∗푨)−1, 횫

.
(23)
In particular, using the chain rule and (23) and (22) gives
휕
휕푡

푡=0
h
풘∗
푖푗횺(푨+ 푡횫)−1풘푖푗
i
= −2휎2
0
D
푨−∗횺(푨)−1풘푖푗풘∗
푖푗횺(푨)−1(푨∗푨)−1, 횫
E
.
(24)
Next, using the Leibniz formula for the determinant, we obtain
d[log det]푿(횫) = ⟨푿−∗, 횫⟩.
(25)
The chain rule and (23) and (25) thus give
휕
휕푡

푡=0

log det Σ(푨+ 푡횫)

= 2휎2
0

푨−∗횺(푨)−1(푨∗푨)−1, 횫

,
(26)
and similarly
휕
휕푡

푡=0

log det(푨+ 푡횫)∗(푨+ 푡횫)

= 2⟨푨−∗, 횫⟩.
(27)
Combining (24), (26) and (27), we have

푽, d품푨(횫)

=
Õ
푖,푗
푉푖푗푔(푨)푖푗
D
휎2
0푨−∗
횺(푨)−1풘푖푗풘∗
푖푗횺(푨)−1 −횺(푨)−1
(푨∗푨)−1 + 푨−∗, 횫
E
,
and the claim follows by distributing and reading oﬀthe gradient.7
□
Diﬀerentiating costs in “inverse parameterization”.
Our theoretical study of spike alignment in Ap-
pendix B and our experiments on the discretized objective (10) in Section 5.2 suggest strongly to prefer
“inverse parameterization” relative to standard parameterization of aﬃne transformations for optimiza-
tion. By this, we mean the following: given a cost 휑(흉푨,풃) optimized over aﬃne transformations (푨, 풃),
one optimizes instead 휑(흉푨−1,−푨−1풃). This nomenclature is motivated by, in the continuum, the inverse of
the aﬃne transformation 풙↦→푨풙+ 풃being 풙↦→푨−1(풙−풃). Below, we show the chain rule calculation
that allows one to easily obtain gradients for inverse-parameterized objectives as linear corrections of the
standard-parameterized gradients.
7After distributing, the sum over 푖, 푗in the ﬁrst factor can be computed relatively eﬃciently using a Kronecker product.
26

Proposition A.3. Let 휑: ℝ2×2 × ℝ2 →ℝ, and let 퐹(푨, 풃) = (푨−1, −푨−1풃) denote the inverse parameterization
mapping, deﬁned on GL(2) × ℝ2. One has
∇푨[휑◦퐹](푨, 풃) = −푨−∗ ∇푨[휑] ◦퐹(푨, 풃)
푨−∗+ 푨−∗ ∇풃[휑] ◦퐹(푨, 풃)
(푨−1풃)∗,
∇풃[휑◦퐹](푨, 풃) = −푨−∗ ∇풃[휑] ◦퐹(푨, 풃)
,
where 푨−∗= (푨−1)∗.
Proof. Let d[휑◦퐹] denote the diﬀerential of 휑◦퐹(and so on). We have for 횫푨and 횫풃the same shape as 푨
and 풃
d퐹푨,풃(횫푨, 횫풃) = 휕
휕푡

푡=0
 (푨+ 푡횫푨)−1, −(푨+ 푡횫푨)−1(풃+ 푡횫풃)
=  −푨−1횫푨푨−1, − 푨−1횫풃−푨−1횫푨푨−1풃
where the asserted expression for the derivative through the matrix inverse follows from, say, the Neumann
series. Now, the chain rule and the deﬁnition of the gradient imply
d[휑◦퐹]푨,풃(횫푨, 횫풃) =

 ∇푨[휑◦퐹](푨, 풃), ∇풃[휑◦퐹](푨, 풃)
,  −푨−1횫푨푨−1, − 푨−1횫풃−푨−1횫푨푨−1풃
,
and the claim follows by distributing and taking adjoints in order to read oﬀthe gradients from the previous
expression.
□
We remark that centering, as discussed in Appendix A.1, can be implemented identically to the standard
parameterization case when using inverse parameterization.
A.3
Additional Experiments and Experimental Details
General details for experiments.
We use normalized cross correlation (NCC) and zero-normalized cross
correlation (ZNCC) for measuring the performance of registration on textured and spike data respec-
tively. Speciﬁcally, for two multichannel images 푿, 풀∈ℝ푚×푛×푐, let ˜푿and ˜풀be the channel-wise mean-
subtracted images from 푿and 풀. The quantities NCC and ZNCC are deﬁned as NCC(푿, 풀) =
⟨푿,풀⟩
∥푿∥퐹∥풀∥퐹and
ZNCC(푿, 풀) =
⟨˜푿, ˜풀⟩
∥˜푿∥퐹∥˜풀∥퐹.
A.3.1
Figure 1 Experimental Details
In the experiment comparing the complexity of optimization and covering-based methods for textured motif
detection shown in Figure 1, the raw background image used has dimension 2048×1536. The crab template
is ﬁrst placed at the center, then a random transformation is applied to generate the scene 풚. Translation
consists of random amounts on both 푥and 푦directions uniformly in [−5, 5] pixels. Euclidean transforms
in addition apply a rotation with angle uniformly from [−휋
4 , 휋
4 ]. Similarity transforms in addition applies a
scaling uniformly from [0.8, 1.25]. Generic aﬃne transforms are parameterized by a transformation matrix
푨∈ℝ2×2 and oﬀset vector 풃∈ℝ2, with the singular values of 푨uniformly from [0.8, 1.25] and the left and
right orthogonal matrices being rotation matrices with angle uniformly in [−휋
4 , 휋
4 ]. For each of the 4 modes
of transform, 10 random images are generated. The optimization formulation used is (4), with 풙표the crab
body motif shown in Figure 2(a). The optimization-based method uses a multi-scale scheme, which uses a
sequence of decreasing values of 휎and step sizes, starting at 휎= 5 and step size 0.005휎(except for aﬃne
mode which starts at 휎= 10), with 휎halved every 50 iterations until stopping criteria over the ZNCC is
met, where ZNCC is calculated over the motif support Ω only. For each value of 휎, a dilated support ˜Ω is
used, which is the dilation of Ω two 휎away from the support of the motif. The background model covers
the region up to 5휎away from the motif. The background 휷is initialized as a gaussian-smoothed version
of the diﬀerence between the initialized image and the ground truth motif, and then continuously updated
in the optimization. For the ﬁrst 5 iterations of every new scale 휎, only the background is updated while
the transformation parameters are held constant. The covering-based method samples a random transform
from the corresponding set of transforms used in each try.
27

A.3.2
Figure 4 Experimental Details
In the experiment of verifying the convergence of multichannel spike registration as shown in Figure 4,
the motif consists of 5 spikes placed at uniformly random positions in a 61 × 81 image. To allow the spike
locations to take non-integer values, we represent each spike as a gaussian density with standard deviation
휎0 = 3 centered at the spike location, and evaluated on the grid. A random aﬃne transformation of the
motif is generated as the scene. As a result, we are able to use this 휎0-smoothed input in (10) without extra
smoothing, and we can compensate the variance of the ﬁlter applied to 풙표in the formulation to account for
the fact that we already smoothed by 휎0 when generating the data. The smoothing level in the registration
is chosen according to equation (16) in Theorem 5.1. Due to the discretization eﬀect and various artifacts,
the step sizes prescribed in Theorem 5.1 will lead to divergence, so we reduce the step sizes by multiplying
a factor of 0.2.
A.3.3
Further Experimental Details
The beach background used for embedding the crab template throughout the experiments is CC0-licensed
and available online: https://www.flickr.com/photos/scotnelson/28315592012. Our code and data are
available at https://github.com/sdbuch/refine.
A.4
Canonized Object Preprocessing and Calibration for Hierarchical Detection
The hierarchical detection network implementation prescription in Section 4.3 assumes the occurrence maps
풙푣for 푣∈푉\ {1, . . . , 퐾} are given; in practice, these are ﬁrst calculated using the template 풚표and its motifs,
by a process we refer to as extraction. Simultaneously, to extract these occurrence maps and have them be
useful for subsequent detections it is necessary to have appropriate choices for the various hyperparameters
involved in the network: we classify these as ‘registration’ hyperparameters (for each 푣∈푉, the step
size 휈푣; the image, scene, and input smoothing parameters 휎2
푣, 휎2
0,푣, and 휎2
in; the number of registration
iterations 푇푣; and the vertical (“height”) and horizontal (“width”) stride sizes Δ퐻,푣and Δ푊,푣) or ‘detection’
hyperparameters (for each 푣∈푉, the suppression parameter 훼푣and the threshold parameter 훾푣). We
describe these issues below, as well as other relevant implementation issues.
Hyperparameter selection.
We discuss this point ﬁrst, because it is necessary to process the ‘leaf’ motifs
before any occurrence maps can be extracted. In practice, we ‘calibrate’ these hyperparameters by testing
whether detections succeed or fail given the canonized template 풚표as input to the (partial) network. Below,
we ﬁrst discuss hyperparameters related to visual motifs (i.e., the formulation (12)), then hyperparameters
for spiky motifs (i.e., the formulation (13)).
Stride density and convergence speed:
The choice of these parameters encompasses a basic compu-
tational tradeoﬀ: setting 푇푣larger allows to leverage the entire basin of attraction of the formulations (12)
and (13), enabling more reliable values of min흀∈Λ푣loss(푣, 흀) and the use of larger values of Δ퐻,푣and Δ푊,푣;
however, it requires more numerical operations (convolutions and interpolations) for each stride 흀∈Λ푣. In
our experiments we err on the side of setting 푇푣large, and tune the stride sizes Δ푊,푣and Δ퐻,푣over multiples
of 4 (setting them as large as possible while being able to successfully detect motifs). The choice of the
step sizes 휈푣is additionally complicated by the smoothing and motif-dependence of this parameter. As we
describe in Section 3.3, we treat the step sizes taken on each component of (푨, 풃) independently, and in our
experiments use a small multiple (i.e., 1/10) of 푡푨
푣= 4휎/max{푚2
푣, 푛2
푣} and 푡풃
푣= 2휎/max{푚푣, 푛푣} for all visual
motifs. This prescription is a heuristic that we ﬁnd works well for the motifs and smoothing parameters
(see the next point below) we test, inspired by the theoretical prescriptions in Section 5 for spike alignment
that we discuss later in this section.
Smoothing parameters:
The smoothing level 휎2
푣in (12) increases the size of the basin of attraction
when set larger. For this speciﬁc formulation, we ﬁnd it more eﬃcient to expand the basin by striding, and
enforce a relatively small value of 휎2
푣= 9 for all visual motifs. Without input smoothing, we empirically
observe that the ﬁrst-round-multiscale cost-smoothed formulation (12) is slightly unstable with respect to
28

high-frequency content in 풚: this motivates us to introduce this extra smoothing with variance 휎2
in = 9/4,
which removes interpolation artifacts that hinder convergence. We ﬁnd the multiscale smoothing mode of
operation described in Section 4.3 to be essential for distinguishing between strides 흀which have “failed” to
register the motif 풙푣and those that have succeeded, through the error loss(흀, 푣): in all experiments, we run
the second-phase multiscale round for (12) as described in Section 4.3, for 256 iterations and with 휎2 = 10−2
and 휎2
in = 10−12. We describe the choice of 휎2
0,푣below, as it is more of a spike registration hyperparameter
(c.f. (14)).
Detection parameters:
The scale parameters 훼푣are set based on the size of the basin of attraction around
the true transformation of 풙푣, and in particular on the scale of loss(흀, 푣) at “successes” and ”failures” to
register. In our experiments, we simply set 훼푣= 1 for visual motifs. The choice of the threshold parameter 훾푣
is signiﬁcantly more important: it accounts for the fact that the ﬁnal cost loss(흀, 푣) at a successful registration
is sensitive to both the motif 풙푣and the background/visual clutter present in the input 풚. In our experiments
in Section 4.4, we tune the parameters 훾푣on a per-motif basis by calculating loss(흀, 푣) for embeddings 풚표◦흉0
for 흉0 ∈SO(2) up to some maximum rotation angle in visual clutter, classifying each 흀as either a successful
registration or a failure, and then picking 휸푣to separate the successful runs for all rotation angles from
the failing runs. For the motifs and range of rotation angles we consider, we ﬁnd that such a threshold
always exists. However, at larger rotation angles we run into issues with the left and right eye motifs being
too similar to each other, leading to spurious registrations and the non-existence of a separating threshold.
In practice, this calibration scheme also requires a method of generating visual clutter that matches the
environments one intends to evaluate in. The calibrated threshold parameters used for our experiments in
Section 4.4 are available in our released implementation.
Hyperparameters for spiky motifs:
The same considerations apply to hyperparameter selection for
spiky motifs (i.e., the formulation (13)). However, the extra structure in such data facilitates a theoretical
analysis that corroborates the intuitive justiﬁcations for hyperparameter tradeoﬀs we give above and leads
to speciﬁc prescriptions for most non-detection hyperparameters, allowing them to be set in a completely
tuning-free fashion. We present these results in Section 5. For detection hyperparameters, we follow the
same iterated calibration process as for visual motifs, with scale parameters 훼푣= 2.5 · 105 (typical values of
the cost (13) are much smaller than those of the cost (12), due to the fact that the gaussian density has a small
퐿2 norm). For the occurrence map smoothing parameters 휎2
0,푣, our network construction above necessitates
setting these parameters to be the same for all 푣∈푉; we ﬁnd empirically that a setting 휎2
0,푣= 9 is suﬃcient to
avoid interpolation artifacts. Finally, the bounding box masks Ω푣are set during the extraction process (see
below), and are dilated by twice the total size of the ﬁlters 품휎2푣. In practice, when implementing gaussian
ﬁlters, we make the image size square, with side lengths 6휎(rounded to the next largest odd integer).
Occurrence map extraction.
Although the criteria above (together with the theoretical guidance from Sec-
tion 5) are suﬃcient to develop a completely automatic calibration process for the various hyperparameters
above, in practice we perform calibration and occurrence map extraction in a ‘human-in-the-loop’ fashion.
The extraction process can be summarized as follows (it is almost identical to the detection process described
in Section 4.3, with a few extra steps implicitly interspersed with calibration of the various hyperparameters):
1. Use the canonized template as input: We set 풚표as the network’s input.
2. Process leaf motifs: Given suitable calibrated settings of the hyperparameters for leaf motifs 푣∈푉,
perform detection and generate all occurrence maps 흎푣via (14).
3. Extract occurrence motifs at depth diam(퐺) −1: For each 푣with 푑(푣) = diam(퐺) −1, we follow the
assumptions made in Section 4.1 (in particular, that each visual motif occurs only once in 풚표and 퐺is
a tree) and after aggregating the occurrence map from 푣’s child nodes via (11), we extract 풙푣from 풚푣
by cropping to the bounding box for the support of 풚푣. Technically, since (14) uses a gaussian ﬁlter,
the support will be nonzero everywhere, and instead we threshold at a small nonzero value (e.g. 1/20
in our experiments) to determine the “support”.
29

4. Continue to the root of 퐺: Perform registration to generate the occurrence maps for nodes at depth
diam(퐺) −1, then continue to iterate the above steps until the root node is reached and processed.
Note that the extracted occurrence motifs 풙푣for 푣∈푉\ {1, . . . , 퐾} depend on proper settings of the
registration and detection hyperparameters: if these parameters are set imprecisely, the extracted occurrence
maps will not represent ideal detections (e.g. they may not be close to a full-amplitude gaussian at the
locations of the motifs in 풚표as they should, or they may not suppress failed detections enough).
Other implementation issues.
The implementation issue of centering, discussed in Appendix A.1, is
relevant to the implementation of the unrolled solvers for (13) and (14). We ﬁnd that a useful heuristic is
to center the transformation 흉at the location of the center pixel of the embedded motif 풙푣(i.e., for a stride
흀∈Λ푣, at the coordinates 흀+ ((푚푣−1)/2, (푛푣−1)/2)). To implement this centering, the locations of the
detections in the spike map deﬁnition (14) need to have the oﬀsets ((푚푣−1)/2, (푛푣−1)/2) added.
The network construction in Section 4.3 relies on the extraction process described above to employ an
identical enumeration strategy in the traversal of the graph 퐺as the detection process (i.e., assuming that
nodes are ordered in increasing order above). In our implementation described in Section 4.4, we instead
label nodes arbitrarily when preparing the network’s input, and leave consistent enumeration of nodes
during traversal to the NetworkX graph processing library [HSS08].
B
Proof of Theorem 5.1
We consider a continuum model for multichannel spike alignment, motivated by the higher-level features
arising in the hierarchical detection network developed in Section 4: signals 푿are represented as elements
of ℝℝ×ℝ×퐶, and are identiﬁable with 퐶-element real-valued vector ﬁelds on the (continuous, inﬁnite) image
plane ℝ2. In this setting, we write ∥푿∥2
퐿2 = Í퐶
푖=1∥푿푖∥2
퐿2 for the natural product norm (in words, the ℓ2 norm
of the vector of channelwise 퐿2 norms of 푿). For 풑∈ℝ2, let 휹풑∈ℝℝ×ℝdenote a Dirac distribution centered
at 풑, deﬁned via
∫
ℝ2 휹풑(풙)푓(풙) d풙= 푓(풑)
for all Schwartz functions 푓[SW71, §I.3]. This models a ‘perfect’ spike signal. For 풑∈ℝ2 and 푴∈ℝ2×2
positive semideﬁnite, let 품풑,푴denote the gaussian density on ℝ2 with mean 풑and covariance matrix 푴.
Consider a target signal
푿표=
푐Õ
푖=1
휹풗푖⊗풆푖,
(28)
and an observation
푿=
푐Õ
푖=1
휹풖푖⊗풆푖
(29)
satisfying
풗푖= 푨★풖푖+ 풃★
(30)
for some (푨★, 풃★) ∈GL(2)×ℝ2. These represent the unknown ground-truth aﬃne transform to be recovered.
Consider the objective function
휑퐿2,휎(푨, 풃) ≡1
2푐
품0,휎2푰−휎2
0(푨∗푨)−1 ∗

det1/2(푨∗푨)

품0,휎2
0푰∗푿

◦흉푨,풃

−품0,휎2푰∗푿표

2
퐿2 ,
where 푨∗denotes the transpose, convolutions are applied channelwise, and for a signal 푺∈ℝℝ×ℝ×푐,
푺◦흉푨,풃(푢, 푣) = 푺(푎11푢+ 푎12푣+ 푏1, 푎21푢+ 푎22푣+ 푏2). We study the following “inverse parameterization” of
this function:
휑inv
퐿2,휎(푨, 풃) ≡휑퐿2,휎(푨−1, −푨−1풃).
30

We analyze the performance of gradient descent for solving the optimization problem
min
푨,풃휑inv
퐿2,휎(푨, 풃).
Under mild conditions, local minimizers of this problem are global. Moreover, if 휎is set appropriately, the
method exhibits linear convergence to the truth:
Theorem B.1 (Multichannel Spike Model, Aﬃne Transforms, 퐿2). Consider an instance of the multichannel
spike model (28)-(29)-(30), with 푼= [풖1, . . . , 풖푐] ∈ℝ2×푐. Assume that the spikes 푼are centered and nondegenerate,
so that 푼1 = 0 and rank(푼) = 2. Then gradient descent
푨푘+1
=
푨푘−휈푡푨∇푨휑inv
퐿2,휎(푨푘, 풃푘),
풃푘+1
=
풃푘−휈푡풃∇풃휑inv
퐿2,휎(푨푘, 풃푘)
with smoothing
휎2 ≥2
max푖∥풖푖∥2
2
푠min(푼)2
 푠max(푼)2∥푨★−푰∥2
퐹+ 푐∥풃★∥2
2

and step sizes
푡푨
=
푐
푠max(푼)2 ,
푡풃
=
1,
휈
=
8휋휎4,
from initialization 푨0 = 푰, 풃0 = 0 satisﬁes
푡−1
푨∥푨푘−푨★∥2
퐹+ ∥풃푘−풃★∥2
2
≤

1 −1
2휅
2푘
푡−1
푨∥푰−푨★∥2
퐹+ ∥풃★∥2
2

,
(31)
where
휅= 푠max(푼)2
푠min(푼)2 ,
with 푠min(푼) and 푠max(푼) denoting the minimum and maximum singular values of the matrix 푼.
Proof. Below, we use the notation ∥푴∥ℓ푝→ℓ푞= sup∥풙∥푝≤1∥푴풙∥푞.
We begin by rephrasing the objective
function in a simpler form: by properties of the gaussian density,
휑퐿2,휎(푨, 풃)
=
1
2푐
푐Õ
푖=1
품푨−1(풖푖−풃),휎2푰−품풗푖,휎2푰

2
퐿2 ,
whence by an orthogonal change of coordinates
휑퐿2,휎(푨, 풃)
=
1
푐
푐Õ
푖=1
휓  1
2 ∥푨−1(풖푖−풃) −풗푖∥2
2

,
where
휓(푡2/2)
=
1
2
품푡풆1,휎2푰−품0,휎2푰
2
퐿2
=
1
4휋휎2 −

품푡풆1,휎2푰, 품0,휎2푰

=
1
4휋휎2 −
1
(2휋휎2)2
∫
ℝ
푒−푠2/휎2푑푠
 ∫
ℝ
푒−(푠−푡)2/2휎2푒−푠2/2휎2푑푠

31

=
1
4휋휎2 −
2−1/2
(2휋휎2)3/2
∫
ℝ
푒−(푠−푡/2)2
휎2
푒−푡2
4휎2 푑푠
=
1
4휋휎2

1 −exp

−푡2/2
2휎2

.
So
휑inv
퐿2,휎(푨, 풃) = 휑퐿2,휎(푨−1, −푨−1풃) = 1
푐
푐Õ
푖=1
휓( 1
2 ∥푨풖푖+ 풃−풗푖∥2
2).
Diﬀerentiating, we obtain
∇푨휑inv
퐿2,휎(푨, 풃)
=
1
푐
푐Õ
푖=1
¤휓( 1
2 ∥휹푖∥2
2)휹푖풖∗
푖
∇풃휑inv
퐿2,휎(푨, 풃)
=
1
푐
푐Õ
푖=1
¤휓( 1
2 ∥휹푖∥2
2)휹푖,
where for concision
휹푖= 푨풖푖+ 풃−풗푖
= (푨−푨★)풖푖+ 풃−풃★.
In these terms, we have the following expression for a single iteration of gradient descent:
푨+
=
푨−푡푨
푐
푐Õ
푖=1
휈¤휓( 1
2 ∥휹푖∥2
2)휹푖풖∗
푖
=
푨−푡푨
푐
푐Õ
푖=1
휈¤휓( 1
2 ∥휹푖∥2
2)(푨−푨★)풖푖풖∗
푖−푡푨
푐
푐Õ
푖=1
휈¤휓( 1
2 ∥휹푖∥2
2)(풃−풃★)풖∗
푖
=
푨−휈푡푨
푐(푨−푨★)푼¤횿푼∗−휈푡푨
푐(풃−풃★) ¤흍
∗푼∗
and
풃+
=
풃−푡풃
푐
푐Õ
푖=1
휈¤휓( 1
2 ∥휹푖∥2
2)휹푖
=
풃−푡풃
푐(풃−풃★)

1, 휈¤흍

−휈푡풃
푐(푨−푨★)푼¤흍,
where above, we have set
¤횿=

¤휓( 1
2 ∥휹1∥2
2)
...
¤휓( 1
2 ∥휹푐∥2
2)

∈ℝ푐×푐,
¤흍=

¤휓( 1
2 ∥휹1∥2
2)
...
¤휓( 1
2 ∥휹푐∥2
2)

∈ℝ푐.
(32)
Writing 횫푨= 푨−푨★, 횫풃= 풃−풃★, we have
횫+
푨
=
횫푨

푰−휈푡푨
푐푼¤횿푼∗
−휈푡푨
푐횫풃¤흍
∗푼∗
횫+
풃
=

1 −푡풃
푐

1, 휈¤흍

횫풃−횫푨
휈푡풃
푐푼¤흍.
To facilitate a convergence proof, we modify this equation to pertain to scaled versions of 횫푨, 횫풃:
푡−1/2
푨
횫+
푨
=
(푡−1/2
푨
횫푨)

푰−푡푨
푐푼(휈¤횿)푼∗
−
푡1/2
푨푡1/2
풃
푐
(푡−1/2
풃
횫풃)(휈¤흍)∗푼∗
32

푡−1/2
풃
횫+
풃
=

1 −푡풃
푐

1, 휈¤흍

(푡−1/2
풃
횫풃) −(푡−1/2
푨
횫푨)
푡1/2
풃
푡1/2
푨
푐
푼(휈¤흍).
In matrix-vector form, and writing ¯횫푨= 푡−1/2
푨
횫푨and ¯횫풃= 푡−1/2
풃
횫풃, we have

vec( ¯횫푨)
¯횫풃
+
=
©­
«
푰6 −

푡푨
푐푼(휈¤횿)푼∗⊗푰2
푡1/2
푨푡1/2
풃
푐
(푼(휈¤흍)) ⊗푰2
푡1/2
푨푡1/2
풃
푐
((휈¤흍)∗푼∗) ⊗푰2
푡풃
푐

1, 휈¤흍

⊗푰2

ª®
¬

vec( ¯횫푨)
¯횫풃

=
©­
«
푰6 −

푡푨
푐푼(휈¤횿)푼∗
푡1/2
푨푡1/2
풃
푐
(푼(휈¤흍))
푡1/2
푨푡1/2
풃
푐
((휈¤흍)∗푼∗)
푡풃
푐

1, 휈¤흍


⊗푰2ª®
¬

vec( ¯횫푨)
¯횫풃


푴

vec( ¯횫푨)
¯횫풃

,
(33)
where in this context ⊗denotes the Kronecker product of matrices. Since 푰6 = 푰4 ⊗푰2, and because the
eigenvalues of a Kronecker product of symmetric matrices are the pairwise products of the eigenvalues of
each factor, we have
∥푴∥ℓ2→ℓ2 =

푰−

푡푨
푐푼(휈¤횿)푼∗
푡1/2
푨푡1/2
풃
푐
(푼(휈¤흍))
푡1/2
푨푡1/2
풃
푐
((휈¤흍)∗푼∗)
푡풃
푐

1, 휈¤흍



ℓ2→ℓ2
.
By our choice of 푡푨and 푡풃, and the assumption 푼1 = 0, we can write

푡푨
푐푼(휈¤횿)푼∗
푡1/2
푨푡1/2
풃
푐
(푼(휈¤흍))
푡1/2
푨푡1/2
풃
푐
((휈¤흍)∗푼∗)
푡풃
푐

1, 휈¤흍


=

푡푨
푐푼(휈¤횿−푰)푼∗+
푼푼∗
∥푼∥2
ℓ2→ℓ2
푡1/2
푨푡1/2
풃
푐
(푼(휈¤흍−1))
푡1/2
푨푡1/2
풃
푐
((휈¤흍−1)∗푼∗)
푡풃
푐

1, 휈¤흍−1

+ 1

and so by the triangle inequality for the operator norm
∥푴∥ℓ2→ℓ2
≤
푰−
"
푼푼∗
∥푼∥2
ℓ2→ℓ2
1
#
ℓ2→ℓ2
+

"
푼
∥푼∥ℓ2→ℓ2 (휈¤횿−푰)
푼∗
∥푼∥ℓ2→ℓ2
1
√푐
푼
∥푼∥ℓ2→ℓ2 (휈¤흍−1)
1
√푐(휈¤흍−1)∗
푼∗
∥푼∥ℓ2→ℓ2

 1
푐, 휈¤흍−1

#
ℓ2→ℓ2
≤
1 −1
휅+ 2∥휈¤흍−1∥ℓ∞,
(34)
since

"
푼
∥푼∥ℓ2→ℓ2 (휈¤횿−푰)
푼∗
∥푼∥ℓ2→ℓ2
1
√푐
푼
∥푼∥ℓ2→ℓ2 (휈¤흍−1)
1
√푐(휈¤흍−1)∗
푼∗
∥푼∥ℓ2→ℓ2

 1
푐, 휈¤흍−1

#
ℓ2→ℓ2
≤

"
푼diag(휈¤흍−1)푼∗
∥푼∥2
ℓ2→ℓ2
0
0

 1
푐, 휈¤흍−1

#
ℓ2→ℓ2
+


0
1
√푐
푼(휈¤흍−1)
∥푼∥ℓ2→ℓ2
1
√푐
(휈¤흍−1)∗푼∗
∥푼∥ℓ2→ℓ2
0


ℓ2→ℓ2
and by Hölder’s inequality
|⟨1
푐, 휈¤흍−1⟩| ≤∥휈¤흍−1∥ℓ∞,
1
√푐∥휈¤흍−1∥ℓ2 ≤∥휈¤흍−1∥ℓ∞.
Inductive argument for (31).
We begin by noting that since 푨0 = 푰, 풃0 = 0, (31) holds for 푘= 0. Now
assume that it is true for 0, 1, . . . , 푘−1. If we can verify that
2∥휈¤흍−1∥ℓ∞≤1
2휅,
(35)
33

then by (33) and (34) together with 푡풃= 1, we have


푡−1/2
푨
vec(푨푘−푨★)
풃푘−풃★

2
퐹
≤

1 −1
2휅
2 

푡−1/2
푨
vec(푨푘−1 −푨★)
풃푘−1 −풃★

2
퐹
.
Applying the inductive hypothesis, we obtain (31) for iteration 푘. So, once we can show that under the
inductive hypothesis, (35) holds, the result will be established.
We begin by showing that under the inductive hypothesis, the errors 휹푖are all bounded. Indeed, by the
parallelogram law
∥휹푖∥2
2
=
∥푨푘−1풖푖+ 풃푘−1 −풗푖∥2
2
=
∥(푨푘−1 −푨★)풖푖+ (풃푘−1 −풃★)∥2
2
≤
2
∥푨푘−1 −푨★∥2
퐹
푡푨
푡푨∥풖푖∥2
2 + 2∥풃푘−1 −풃★∥2
2,
and so applying the inductive hypothesis to bound
푡−1
푨∥푨푘−1 −푨★∥2
퐹+ ∥풃푘−1 −풃★∥2
2 ≤푡−1
푨∥푰−푨★∥2
퐹+ ∥풃★∥2
2,
we obtain for all 푖
∥휹푖∥2
≤
√
2 ×
q
푡−1
푨∥푨★−푰∥2
퐹+ ∥풃★∥2
2 × max
n
푡1/2
푨∥풖푖∥2, 1
o
≤
√
2 ×
q
푡−1
푨∥푨★−푰∥2
퐹+ ∥풃★∥2
2 ×
√푐∥푼∥ℓ1→ℓ2
∥푼∥ℓ2→ℓ2
.
(36)
Since
휓(푠) =
1
4휋휎2

1 −exp

−푠
2휎2

,
we have
¤휓(푠) =
1
8휋휎4 exp

−푠
2휎2

,
and for all 푠≥0
1 −휈¤휓(푠)
 =
1 −8휋휎4 ¤휓(푠)
 ≤
푠
2휎2
by the standard exponential convexity estimate. Plugging in our bound (36), we obtain for all 푖
1 −휈¤휓( 1
2 ∥휹푖∥2
2)

≤
푡−1
푨∥푨★−푰∥2
퐹+ ∥풃★∥2
2
2휎2
×
푐∥푼∥2
ℓ1→ℓ2
∥푼∥2
ℓ2→ℓ2
.
Under our choice of 푡푨and hypotheses on 휎, this is bounded by
1
4휅.
□
34

