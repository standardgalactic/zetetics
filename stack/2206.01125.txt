Preﬁx Conditioning Uniﬁes Language and Label Supervision
Kuniaki Saito1,2*, Kihyuk Sohn3 , Xiang Zhang2 , Chun-Liang Li2 ,
Chen-Yu Lee2 , Kate Saenko1,4 , Tomas Pﬁster2
{keisaito, saenko}@bu.edu
{kihyuks,fancyzhx,chunliang,chenyulee,tpfister}@google.com
1Boston University, 2Google Cloud AI Research, 3Google Research, 4MIT-IBM Watson AI Lab
Abstract
Pretraining visual models on web-scale image-caption
datasets has recently emerged as a powerful alternative
to traditional pretraining on image classiﬁcation data.
Image-caption datasets are more “open-domain”, contain-
ing broader scene types and vocabulary words, and re-
sult in models that have strong performance in few- and
zero-shot recognition tasks.
However large-scale classi-
ﬁcation datasets can provide ﬁne-grained categories with
a balanced label distribution.
In this work, we study a
pretraining strategy that uses both classiﬁcation and cap-
tion datasets to unite their complementary beneﬁts. First,
we show that naively unifying the datasets results in sub-
optimal performance in downstream zero-shot recognition
tasks, as the model is affected by dataset bias: the coverage
of image domains and vocabulary words is different in each
dataset. We address this problem with novel Preﬁx Condi-
tioning, a simple yet effective method that helps disentangle
dataset biases from visual concepts. This is done by intro-
ducing preﬁx tokens that inform the language encoder of the
input data type (e.g., classiﬁcation vs caption) at training
time. Our approach allows the language encoder to learn
from both datasets while also tailoring feature extraction to
each dataset. Preﬁx conditioning is generic and can be eas-
ily integrated into existing VL pretraining objectives, such
as CLIP or UniCL. In experiments, we show that it improves
zero-shot image recognition and robustness to image-level
distribution shift.
1. Introduction
Supervised classiﬁcation datasets (e.g., ImageNet [7])
have traditionally been used to pretrain image representa-
tions for use in downstream tasks.
However, web-scale
image-caption datasets have recently emerged as a pow-
erful pretraining alternative [13, 20, 31].
Such datasets
*Work done during internship at Google Cloud AI Research.
Training Dataset
IN21K
CC12M
IN21K + CC12M
IN21K + CC12M
CLIP
UniCL
CLIP+
Ours
UniCL+
Ours
Accuracy (%) on 
11 standard zero-shot benchmark
Accuracy (%) on 
11 standard zero-shot benchmark
Training Dataset
Figure 1. We propose Preﬁx Conditioning to unify image-caption
(e.g., CC12M [5]) and image classiﬁcation datasets (e.g., Ima-
geNet21K (IN21K) [7]) for training better zero-shot models. Pre-
ﬁx conditioning improves zero-shot recognition performance by
more than 6% on average when training on ImageNet21K and
CC12M.
are more “open-domain”, containing a wider variety of
scene types and vocabularies than traditional classiﬁcation
datasets, which are biased towards speciﬁc categories in
their ﬁxed label sets. Consequently, models trained on web-
scale image-caption datasets have shown stronger gener-
alization in novel tasks [4, 31] and demonstrated remark-
able performance on few and zero-shot image classiﬁcation
tasks [31]. Nevertheless, classiﬁcation datasets are still use-
ful for pre-training as they have a more balanced coverage
of categories, including rare and ﬁne-grained categories,
and a better focus on the labeled objects in each image.
Recent works [43,45] therefore propose to combine cap-
tion and classiﬁcation datasets for pre-training. [43] convert
classiﬁcation labels to “label-prompts” by inserting the la-
bel into a template sentence, e.g., “a photo of a <label>.”1
1We use the term prompt to indicate a template sentence ﬁlled with a
class name.
arXiv:2206.01125v2  [cs.CV]  15 May 2023

Although training on the caption and label-prompt data
achieves promising results, it does not fully resolve distribu-
tion differences between the open-domain caption data and
the classiﬁcation data. In particular, it produces a language
embedding entangled with the classiﬁcation dataset “bias”.
We note that classiﬁcation datasets tend to be biased in at
least two ways: 1) the images mostly contain single ob-
jects from restricted domains, and 2) the vocabulary is lim-
ited and lacks the linguistic ﬂexibility required for zero-shot
learning. Therefore, the class embedding of “a photo of a
dog” optimized for ImageNet may really mean a photo of
a dog from ImageNet instead, which is biased to ImageNet
and does not generalize well to other datasets. We empiri-
cally show that such dataset biases negatively affect uniﬁed
pretraining by reducing the generalization of learned repre-
sentations and thus jeopardizing zero-shot performance.
To recognize diverse concepts in the open domain, the
language model needs to disentangle the dataset bias from
the visual concepts and extract language embeddings gen-
eralizable to the open domain, e.g., the language embed-
ding representing a photo of a dog from an open-domain
dataset, such as image-caption dataset, instead of a photo
of a dog from ImageNet. Given this intuition, we propose to
learn dataset-speciﬁc language embeddings, while sharing
knowledge from both datasets during training. We achieve
this by a simple yet effective approach we call Preﬁx Con-
ditioning. The idea is to learn a dataset-speciﬁc text token
(preﬁx) for each dataset so that the bias of the dataset can
be absorbed into this token, and in return the remaining text
tokens can focus on learning visual concepts. Speciﬁcally,
we prepend a different token for each dataset (e.g., image
classiﬁcation or caption dataset) to the text input token se-
quence during pre-training.
The idea is in part inspired by preﬁx or prompt tun-
ing [18, 21, 46], which showed that learnable tokens
prepended to the input token sequences of the pre-trained
language models are able to learn task-speciﬁc knowledge
and thus can be used to solve downstream tasks by com-
bining the knowledge of pre-trained large language models
and task-speciﬁc preﬁx tokens. Our approach differs from
prompt tuning in two ways: 1) the proposed preﬁx condi-
tioning is designed to unify image-caption and classiﬁcation
datasets by disentangling the dataset bias, which is a unique
distinction to prompt-tuning works, 2) our approach is ap-
plied for VL pre-training while the standard prompt tuning
is used in ﬁne-tuning.
In experiments, the proposed simple technique achieves
superior performance on zero-shot evaluation if we use the
preﬁx of the caption dataset to get the language embedding
at test time as shown in Fig. 1. Meanwhile, inserting the pre-
ﬁx of the classiﬁcation dataset leads to better performance
on classiﬁcation data.
We also observe a drastic perfor-
mance improvement when combining our preﬁx condition-
ing with the UniCL [43] objective because of their comple-
mentarity. Our contributions are summarized as follows:
• We propose novel Preﬁx Conditioning at pre-training
time to unify image-label and image-caption supervision.
It is the ﬁrst mechanism to use preﬁxes to condition the
source of the dataset during vision language contrastive
pre-training, rather than post pre-training.
• This simple approach improves zero-shot recognition per-
formance by more than 6% on average in experiments on
ImageNet21K [7] and CC12M [5].
• Our comprehensive ablation study shows that preﬁx con-
ditioning enables the model to switch its approach to ex-
tracting language features, e.g., attend to different words.
2. Related Work
Vision-Language Contrastive Learning.
Zero-shot
recognition is conventionally solved by learning the rela-
tionship between visual representations and word embed-
dings of the class names [1, 9, 12, 26, 38, 40, 41]. Vision-
language contrastive learning models, such as CLIP [31],
pre-train a model with a large-scale image-caption data
(400M) and achieve a remarkable improvement in zero-shot
recognition. ALIGN [13] demonstrated the effect of scaling
up the size of image-caption data. Various techniques have
been proposed to improve the data efﬁciency given a rela-
tively small amount of image-caption data (order of 10M).
ALBEF [20] employs model distillation and masked lan-
guage modeling. DeCLIP [22], SLIP [29] and TCL [42]
harness self-supervised contrastive learning.
FILIP [44]
uses token-to-token contrastive learning rather than the
global contrastive learning used in CLIP. BLIP [19] gener-
ates pseudo captions to diversify the language modality for
each image. Unlike these works that handle only caption-
style supervision, we focus on the use of label supervision
in vision-language pre-training. Our approach brings or-
thogonal improvement to the aforementioned works as they
seek to improve training on image-caption data.
UniCL [43] and K-Lite [34] unite the image-caption
and image-label supervision by converting labels into text
with pre-deﬁned template sentences.
UniCL leverages a
supervised contrastive loss [15] for image-label pairs. K-
Lite [34] utilizes external knowledge from WordNet [28]
and Wikitionary [27]. The input noun is augmented with
the class hierarchy and deﬁnition to enrich the supervision.
Our method is complementary to these approaches since
both UniCL and K-Lite do not consider the domain shift
between datasets. In experiments, we observe a signiﬁcant
performance boost when UniCL is combined with the preﬁx
conditioning.
Learning with Prompts.
Prompt tuning is a popu-
lar technique to adapt a large language model to a spe-
ciﬁc task with few training data and low computational

Figure 2. Left: Preﬁx conditioning at training time. Dataset-speciﬁc token is added to the input tokens with a contrastive learning objective
applied. Right: Preﬁx conditioning at test time. Given a class name, we construct a class prompt with pre-deﬁned templates and add a
token used to condition real caption during training considering that image-caption dataset covers much wider range of image domains and
vocabulary words than image classiﬁcation dataset.
cost [10,18,21,23,30]. To avoid tuning all parameters of the
model and using hand-crafted prompts, preﬁx embeddings
are added to the training input and are the only parame-
ters optimized during ﬁne-tuning. The preﬁx embedding
can be viewed as the knowledge of the downstream task.
In this paper, since the target task is the zero-shot classi-
ﬁcation, the bias of the language embedding needs to be
from the dataset covering a wide range of domains rather
than a speciﬁc domain. Therefore, we choose to use the
preﬁx embedding learned for image-caption dataset during
test time. This technique is also effective in adapting a pre-
trained vision-language model [46, 47] to few-shot classi-
ﬁcation by tuning the prompts of the language encoder to
adapt to a downstream task. Additionally, prompt-tuning is
effective in adapting a pre-trained vision model to a target
task [14]. While these works aim to tailor a large pre-trained
model to a speciﬁc downstream task with a small amount of
data or low computational cost, our goal here is to condi-
tion a model with the preﬁx during the pre-training stage by
distinguishing between the image label and image caption
data. This allows a model to effectively share the knowl-
edge obtained from two different types of data sources.
Dataset bias in image recognition. A large-scale image
recognition dataset such as ImageNet [7] is known to be bi-
ased towards a speciﬁc image domain. Therefore, a model
trained on such a dataset shows vulnerability to the distri-
bution shift, e.g., shift in object pose [3] and style of the
images [37]. Nevertheless, [16,39] show that adapting only
a linear layer on the pre-trained models can improve per-
formance on the downstream tasks with distribution shifts.
This indicates the importance of having a good classiﬁer on
top of image encoders, such as linear classiﬁers generated
by language encoders with preconditioning in our work. [8]
propose a method for domain generalization. They condi-
tion image recognition models with the domain embedding,
which discriminates the input image domains, and demon-
strate the importance of the domain-speciﬁc image classi-
ﬁer. Our preﬁx conditioning can be seen as an attempt to
de-bias the linear classiﬁer to obtain a domain-speciﬁc clas-
siﬁer and adapt it from the classiﬁcation to the captioning
domain. Also, [2, 17] approach the dataset bias in image
classiﬁcation by de-biasing image representations. By con-
trast, we tackle the problem in the framework of vision-
language learning, disentangle the dataset bias in the lan-
guage embedding and utilize the classiﬁer obtained by the
caption domain. We note that while captioning datasets can
also have data biases, they tend to be more open-domain
than existing classiﬁcation datasets.
3. Method
In this section, we introduce the Preﬁx Conditioning
technique for pretraining a deep learning model on both
image-caption and image-label (classiﬁcation) data.
In
Sec. 3.1, we discuss our problem setting and the back-
ground of contrastive learning with image-caption data. In
Sec. 3.2, we explain the details of our training approach,
and in Sec. 3.3 our inference procedure.
3.1. Preliminaries
Setup. Suppose we have access to two datasets: (i) an
image label dataset SL = {(xn, tP
n , yn)}NL
n=1, where x ∈X
is the image and tP ∈P is a prompt-style language de-
scription based on its class label y ∈Y, and (ii) a dataset of
image-caption pairs SC = {(xn, tC
n )}NC
n=1, where tC ∈T
is a caption. We assume that t is the tokenized language de-
scription. For each image x, an image encoder model fθ pa-
rameterized by θ extracts a visual representation ˜v ∈Rd×1:

˜v = fθ(x). For each caption or prompt t ∈T , a text en-
coder fφ parameterized by φ extracts a language represen-
tation ˜u ∈Rd×1 : ˜u = fφ(t).
Contrastive Loss. CLIP [31] is designed to ﬁnd repre-
sentations that match an image to its paired caption while
separating unpaired ones. For i-th image xi and j-th lan-
guage description tj in a batch B, their features are nor-
malized using vi =
˜vi
∥˜vi∥and uj =
˜uj
∥˜uj∥. Finally, CLIP op-
timizes the symmetric multi-class N-pair loss [35]:
min
{θ,φ} Lcon =Lt2i + Li2t,
(1)
which includes two contrastive terms (a temperature hyper-
parameter τ controls the strength of penalties on hard nega-
tive samples):
Lt2i = −1
|B|
X
i∈B
log
exp(τuT
i vi)
P
j∈B exp(τuT
i vj),
(2)
Li2t = −1
|B|
X
i∈B
log
exp(τvT
i ui)
P
j∈B exp(τvT
i uj).
(3)
UniCL [43] composes each mini-batch with samples from
both SL and SC. Then, for pairs from SL, they regard all
samples from the same class as positive pairs while a sam-
ple from SC has a unique pair. Except for the number of
positive pairs, no special treatment is given to differentiate
between the image-caption and image-label data.
3.2. Preﬁx Conditioned Contrastive Learning
Fig. 2 describes the overview of our approach. We aim to
enable the language encoder to learn embedding strategies
conditioned on the type of input dataset. The conditioning
can then be used to manipulate the bias at inference time.
Preﬁx-tuning [10,18,21,23,30] shares the intuition that
the preﬁx tokens are responsible for switching the context
of a language model from the pre-trained task to the down-
stream task. These approaches leverage the preﬁx to tailor a
model to a single task during ﬁne-tuning and construct dif-
ferent preﬁxes for different natural language tasks [18]. In
our problem setting, there is no task distinction between the
image-caption and image-prompt matching since both are
formulated as contrastive learning. However, we focus on
the fact that the two datasets have different biases in the im-
age distributions and vocabulary words. The label-prompt
sentences are embedded closer to the image classiﬁcation
data, even though we may want to use them to match a new
label to an image from the open-domain image distribution
during zero-shot classiﬁcation.
To solve this problem, we propose to inform the model
of the type of dataset at the input level to switch the fea-
ture extraction. Speciﬁcally, to make the model aware of
the dataset type, preﬁx-conditioning prepends a preﬁx to-
ken to an input sentence to obtain ¯tP = [PREFIXP ; tP ],
¯tC
= [PREFIXC; tC].
The brackets indicate the con-
catenation of two lists of discrete tokens; PREFIXP and
PREFIXC denote a prompt-style and caption-style token re-
spectively. In this way, we prepend the token to learn the
dataset-speciﬁc bias, which enables us to disentangle the
bias in language representations and utilize the embedding
learned on the image-caption dataset at test time even with-
out an input caption.
In prompt-tuning, the number of preﬁx tokens can af-
fect the performance of the model [18, 21, 46]. However,
we do not see the performance difference by the number of
preﬁx tokens. This is probably because adding one token
is enough to distinguish the domain of input sentences. To
avoid signiﬁcantly increasing the training cost, we set the
number of preﬁxes to one in all experiments. Then, the lan-
guage representations for each data source are extracted as
˜uP , ˜uC = fφ(¯tP ), fφ(¯tC). This input design is indepen-
dent from the training objectives, and therefore we can eas-
ily apply the technique to optimize Eq. 1 or UniCL’s loss.
Data Sampling. [6] argue that the data sampling matters
when learning from multiple data sources in a contrastive
learning framework, as the model may learn to distinguish
the samples by exploiting the dataset bias. As such, we need
to take data sampling into consideration in our problem set-
ting as we learn from two different data sources. One op-
tion is a debiased sampling [6], which constructs each mini
batch to contain samples from a single data source. Alterna-
tively, as done in UniCL [43], we can compose each mini-
batch with samples from both data sources (image-caption
and image-label) with equal probability. In experiments,
we choose the debiased sampling, but ﬁnd that the choice
of sampling does not signiﬁcantly affect the performance.
3.3. Inference with Preﬁx Conditioning
During inference (the right side of Fig. 2), an input im-
age is classiﬁed as one of K classes by embedding the cor-
responding label-prompts and choosing the one most sim-
ilar to the image embedding. Following [31], we obtain
class prompts by ﬁlling the default prompt templates with
class names, and add a preﬁx. Considering the wider cov-
erage of domains in the image-caption dataset, the caption-
style preﬁx conditioning may work better to classify novel
downstream data. In our experiments, we empirically ﬁnd
that the caption-style preﬁx indeed outperforms the prompt-
style preﬁx with a large margin in zero-shot recognition
while prompt preﬁx performs better on the image classiﬁca-
tion dataset used to train the model. We provide a detailed
analysis of different conditioning in Section 4.3.
4. Experiments
The goal of experiments is twofold: comparing our ap-
proach with baselines in zero-shot recognition, and analyz-
ing the behavior of preﬁx conditioning. We describe the
experimental setup in Sec. 4.1, show the main results in

Training Data
Objective
Preﬁx
Conditioning
Metric
Classiﬁcation
Caption
Size
IN-1K
Zero-shot
11 datasets
–
CC-3M
3M
CLIP
18.1
28.7
–
CC-12M
12M
CLIP
33.4
41.2
ImageNet-1K
–
1M
CLIP
72.1
20.2
ImageNet-21K
–
12M
CLIP
47.1
39.6
ImageNet-1K
CC-12M
13M
CLIP
68.7
43.3
ImageNet-1K
CC-12M
13M
CLIP
✓
71.5
45.5
ImageNet-1K
CC-12M
13M
UniCL
68.8
43.1
ImageNet-1K
CC-12M
13M
UniCL
✓
71.7
44.5
ImageNet-21K
CC-12M
25M
CLIP
56.8
49.5
ImageNet-21K
CC-12M
25M
CLIP
✓
67.3
57.8
ImageNet-21K
CC-12M
25M
UniCL
58.2
51.7
ImageNet-21K
CC-12M
25M
UniCL
✓
66.5
58.4
ImageNet-21K w/o IN-1K
CC-12M
24M
CLIP
29.1
46.9
ImageNet-21K w/o IN-1K
CC-12M
24M
CLIP
✓
47.8
56.4
Table 1. Performance comparison among different training datasets and training objectives. Note that we use caption preﬁx to obtain
these results. The proposed preﬁx conditioning shows improved zero-shot recognition accuracy across models trained with different
combinations of image-classiﬁcation and image-caption datasets and training objectives.
Sec. 4.2, and analyze the properties of preﬁx-conditioning
in Sec. 4.3.
4.1. Setup
Training Datasets. We conduct experiments on the set-
ting where we have a large source of image-caption and
image-label datasets.
Following UniCL [43], we utilize
CC3M [33] and CC12M [5] as image-caption data. For
the image classiﬁcation dataset, we utilize ImageNet21K
and ImageNet1K [7]. While ImageNet1k contains 1,000
classes, ImageNet21K has more than 20,000 categories that
include ﬁne-grained and general objects. To observe the
behavior in diverse image classiﬁcation data, we also run
experiments on ImageNet21K while excluding the classes
of ImageNet1K. Details are explained in each section.
Training.
We use the same prompt strategy and 80
prompt templates as used in CLIP [31]. During training, we
randomly sample one prompt template and ﬁll it with the
class names, followed by a tokenization step before feeding
into the text encoder. We average language embeddings ex-
tracted from all 80 templates in validation. We use the same
language encoder as CLIP [31] and Swin-Tiny transformer
[24] as the vision encoder following UniCL [43]. All mod-
els are optimized with AdamW [25] where the learning rate
is set to 0.001, and weight decay to 0.1. All models are
trained with a batch size of 1024. Considering the amount
of training data, we train the models for 15 and 50 epochs
in the experiments on ImageNet21K and ImageNet1K re-
spectively.2 For all training, we used a cosine learning rate
2When training a model on two different datasets, e.g., IN21K and
schedule with a warm-up of 10,000 iterations.
Baselines. We train CLIP [31] and UniCL [43] as our
baselines.
For comparison, we present results on CLIP
trained only on image-caption or image classiﬁcation data,
as well as CLIP and UniCL trained on both image-caption
and IN21K data. Unless otherwise stated, CLIP and UniCL
are trained with equal sampling (ES) strategy as in [43],
while our preﬁx conditioning model is trained with debiased
sampling (DS) [6]. We provide an analysis of the sampling
in Sec. 4.2 and ﬁnd that DS itself does not have a noticeable
advantage over ES.
Evaluation.
We evaluate the learned representations
on supervised and zero-shot image classiﬁcation on Ima-
geNet1K3 and on 11 datasets chosen from the ones used in
CLIP [31] including object classiﬁcation (e.g., CIFAR10,
CIFAR100), ﬁne-grained classiﬁcation (e.g., Oxford-IIIT
Pets, Oxford Flowers 102, and Food-101), and aerial images
(e.g., EuroSAT and Resisc45). Although our main focus is
at the zero-shot generalization, we also provide an analysis
of a linear-probe evaluation of the image encoder.
CC12M, we count the epochs based on how many samples are used
from the image classiﬁcation dataset.
For instance, in UniCL, each
mini-batch consists of approximately the same number of samples from
IN21K and CC12M. Then, to train a model for 15 epochs, we train for
N/1024 × 2 × 15 iterations, where N indicates the number of samples in
IN21K.
3While we follow the same zero-shot evaluation protocol when evalu-
ating on ImageNet1K, we note that it is zero-shot only where we explicitly
exclude ImageNet1K from the training, last two rows of Table 1

Train
Preﬁx
Sampling
IN-1K
Cal
CF100
CF10
ESTAT
Food
Flower
Pets
Patch
R45
VOC
DTD
AVG
ES
56.8
70.2
55.0
79.4
21.1
46.0
60.3
57.2
51.2
24.8
57.7
21.4
49.5
✓
ES
65.4
81.2
62.6
88.9
30.4
51.7
61.8
71.9
50.0
28.2
78.1
27.7
57.5
DS
58.7
65.9
55.0
85.7
22.8
40.8
55.7
60.2
50.0
20.6
45.2
23.8
47.8
✓
DS
67.3
79.7
63.8
87.9
31.5
53.4
58.8
69.6
50.6
31.5
80.5
28.4
57.8
Table 2. Ablation study for sampling in IN21K + CC12M. Equal sampling (ES) composes a mini-batch with roughly equal number of
samples from two datasets. Debiased sampling (DS) samples a mini-batch of either IN21K or CC12M with equal probability.
Train Data
Preﬁx
Conditioning IN-1K Cifar10 Cifar100 Caltech Food Pet Patch VOC DTD
ImageNet-21K
71.5
94.3
79.1
83.5
79.1 86.3 82.3
88.9
61.3
ImageNet-21K + CC12M
69.2
93.0
76.4
82.4
78.4 82.2 81.4
88.7
61.4
ImageNet-21K + CC12M
✓
69.4
93.5
77.3
83.2
78.8 83.6 82.0
88.8
62.5
Table 3. Linear evaluation accuracy on models trained with and without preﬁx conditioning. Preﬁx conditioning slightly improves the
performance upon a model without it (second row vs. last row).
4.2. Main Results
We describe our main results in Table 1, followed by the
analysis of preﬁx conditioning in Sec. 4.3.
There are three observations. First, the improvements
upon a model trained only with image-caption or image-
label data are obvious in almost all cases. As the previ-
ous work indicates [43], the effectiveness of combining two
types of supervision is clear from these results.
Second, in all cases, our preﬁx conditioning signiﬁcantly
improves performance on both ImageNet-1K (supervised
recognition) and 11 zero-shot recognition tasks.
When
training on ImageNet-21K, the conditioning improves the
baseline by more than 8% in ImageNet-1K and more than
6% in zero-shot recognition on average. In training with
ImageNet-1K, the margin from the baseline is smaller than
training with ImageNet-21K, probably because the size of
ImageNet-1K (1M) is much smaller than that of ImageNet-
21K (12M). Also, preﬁx conditioning is effective in both
UniCL and CLIP objectives. Due to its simplicity, our ap-
proach can be easily integrated with various objectives.
Finally, our method is less affected by ablating a part
of categories. The classes of ImageNet-1K are excluded
from ImageNet-21K in the last two rows of Table 1. There-
fore, both approaches signiﬁcantly drop performance on
ImageNet-1K, whose task now becomes true zero-shot
recognition, compared to other settings. Even in this set-
ting, preﬁx conditioning maintains high accuracy and out-
performs a CLIP baseline model by a large margin.
Sampling Method.
We analyze the data sampling
scheme to construct a mini-batch in Table 2.
We apply
debiased sampling (DS) in our method, namely, sampling
one data source with equal probability and getting a mini-
batch of it. The other option is mixing two data sources
with equal probability (ES). The table indicates that pre-
ﬁx conditioning works well with ES sampling and the sam-
pling strategy itself is not advantageous. Ablating preﬁx
conditioning during training clearly drops the performance
in both sampling strategies, and the performance is worse
than ES on average in zero-shot results (49.5 vs. 47.8).
ES sampling should allow the model to differentiate sen-
tences by using the prepended preﬁx. Interestingly, this re-
sult implies that differentiating sentences by preﬁx infor-
mation does not much degrade the performance. The dis-
tinguished sentences enable the model to associate images
from different datasets. Since images of two datasets are
different with respect to the categories and the locations of
objects in images, distinguishing the two kinds of images
may not harm generalizability of the representations.
Linear-probe Evaluation. We evaluate the linear-probe
performance in Table 3 to see the quality of learned image
representations. Although the accuracy is better than the
model trained without preﬁx conditioning (second line), the
improvements are not substantial. This result indicates that
the zero-shot performance gain obtained by our method is
not due to the image representations. We investigate the
learned language and image features in the next subsection.
4.3. Analysis of Preﬁx-Conditioning
We present a detailed analysis of preﬁx conditioning.
We ﬁrst study how different preﬁxes impact the zero-shot
recognition performance and analyze their behaviors by
looking into the attention weights of the language trans-
former encoder. We also demonstrate improved robustness
with respect to the image-level domain shift. Unless other-
wise stated, we employ a model trained with CLIP objec-
tive on ImageNet-21K and CC12M in this analysis. Finally,
this section concludes that preﬁx conditioning enables the
language encoder to switch its role during training, which
eases learning from different types of datasets, e.g., image
classiﬁcation and image caption dataset.

Data
Test-time
Preﬁx
IN-1K
Cal
C100
C10
ESTAT
Food
Flower
Pets
Patch
R45
VOC
DTD
AVG
IN-1K
+ CC12M
N/A
68.7
68.7
38.4
69.5
24.4
31.9
13.3
66.6
50.2
25.4
65.6
22.3
43.3
Prompt
75.4
71.7
35.5
63.9
24.2
20.0
8.1
72.2
50.4
24.2
61.1
15.3
40.6
Caption
71.5
75.1
39.4
70.5
26.7
33.9
13.9
72.3
50.5
25.8
67.8
25.4
45.5
IN-21K
+ CC12M
N/A
56.8
70.2
55.0
79.4
21.1
46.0
60.3
57.2
51.2
24.8
57.7
21.4
49.5
Prompt
71.4
76.5
59.0
86.0
20.1
45.7
62.3
69.1
52.4
26.3
76.8
21.4
54.1
Caption
67.3
79.7
63.8
87.9
31.5
53.4
58.8
69.6
50.6
31.5
80.5
28.4
57.8
IN-21K w/o 1K
+ CC12M
N/A
29.1
67.4
45.9
80.0
28.6
40.8
56.9
39.2
50.2
21.9
64.9
19.8
46.9
Prompt
40.8
74.9
61.0
84.6
31.2
48.1
58.7
45.2
51.2
23.5
67.5
21.4
51.6
Caption
47.8
81.9
63.3
87.3
32.4
52.9
62.8
57.0
50.6
25.6
80.1
26.2
56.4
Table 4. Ablation study for test-time preﬁx conditioning. Note that the difference between two results come from the preﬁx used in test
time and we use the same model for this evaluation. A model trained without conditioning is shown at the top of each block.
Caption Prefix
Prompt Prefix
Unconditional Model
Figure 3. An example of attention weights for an end token. Best viewed in color. The sentence shown here is one of class prompts in the
VOC 2007 dataset. Different rows show the weights of different transformer layers. With a prompt preﬁx (leftmost), the model focuses on
a class name (airplane) while caption preﬁx (middle) allows a model to pay attention to another noun, sculpture. By preﬁx conditioning,
the attention of the model changes as intended.
Test Time Preﬁx. We analyze the role of the preﬁx to-
ken in Table 4, where the table describes the comparison in
the choice of test time preﬁx conditioning. As explained in
Sec. 3, the choice of preﬁx during test time should change
the behavior of the model since the preﬁx should tailor the
language encoder for classiﬁcation-style or caption-style
feature extraction. Except for the IN-1K results of a model
trained with the entire IN21K or IN-1K, conditioning with
the caption preﬁx shows much better results. The superi-
ority of the caption preﬁx is noticeable in several datasets.
This means caption preﬁx works better if the target comes
from outside the image classiﬁcation data, indicating that
the class-prompt preﬁx conditioning makes the model tai-
lored for the image classiﬁcation dataset. Class-prompts
preﬁx works better to categorize IN-1K data because the
preﬁx is trained to specialize in classifying it. Note that
caption-style preﬁx performs better than prompt-style preﬁx
in IN-1K for a model trained with IN21K excluding IN1k
classes. This indicates that the caption-style preﬁx works
better when the vocabulary of the class name comes from
outside the image classiﬁcation data since the caption data
covers much more diverse words.
Preﬁx controls attention. Fig. 3 visualizes the atten-
tion weights for an end token in different preﬁx conditions
and models. The input sentence, a sculpture of an airplane,
is one of the class-prompts. When a prompt preﬁx (left-
most) is employed, the language model pays attention to the
class name at the ﬁrst layer, it does not focus on the noun
in other layers. The only noun the encoder focuses on is
airplane. By contrast, the model attends to both sculpture
and airplane in the case of the caption preﬁx and uncon-
ditional model. Note that this behavior does not mean that
the prompt-preﬁx performs better in zero-shot recognition
as shown in experiments due to the effect of the bias in im-
age classiﬁcation dataset.
While we visualize only one example in the main text
due to the space limit and defer more examples to the ap-
pendix, this highlights a general trend that the prompt preﬁx
guides the language encoder to focus on a single word (e.g.,
class name), whereas the caption preﬁx makes the model at-
tend to multiple words. In other words, preﬁx conditioning
allows the language encoder to “switch gears” to represent
sentences from different datasets (i.e., image-classiﬁcation
vs image-caption). On the other hand, the baseline model
without preﬁx conditioning attends to multiple words (e.g.,
Fig. 3 rightmost) even though the input sentence is a class
prompt. This indicates that it is hard to switch the gears
without explicitly informing of the type of dataset.
Language Feature Visualization. Fig. 4 visualizes ex-
tracted language features conditioned with different pre-

Train Data
Preﬁx
Conditioning
Test-Time
Preﬁx
IN
IN-V2
IN-R
IN-S
ImageNet-1K
N/A
72.1
59.3
19.9
17.8
ImageNet-1K + CC12M
N/A
68.7
57.4
27.7
27.8
ImageNet-1K + CC12M
✓
Caption
71.5
60.2
31.8
30.7
ImageNet-1K + CC12M
✓
Prompt
75.4
63.3
29.2
27.9
ImageNet-21K
N/A
47.1
41.1
20.1
16.1
ImageNet-21K + CC12M
N/A
56.8
48.6
29.4
30.6
ImageNet-21K + CC12M
✓
Caption
67.3
57.5
35.2
34.6
ImageNet-21K + CC12M
✓
Prompt
71.4
61.1
32.1
32.2
Table 5. Evaluation on the robustness to the image-level domain shift. Preﬁx conditioned training achieves better robustness, and caption-
preﬁx outperforms prompt-preﬁx in the images distinct from those used in training (IN-R and IN-S).
(a) Different conditions
(b) Prompt condition
(c) Caption condition
(d) No condition
Figure 4. T-SNE [36] visualization of the class-prompt features
of 20 classes of VOC 2007 with different preﬁx conditions. (a):
Language embeddings with prompt (red) and caption (blue) pre-
ﬁxes, respectively. (b)(c)(d): Different colors indicate language
embeddings of different classes.
ﬁxes. As seen in Fig. 4a, language features extracted with
caption-preﬁx (blue) and prompt-preﬁx (red) are clearly
separated. In addition, prompt-preﬁx (Fig. 4b) has lower
intra-class and higher inter-class variance, whereas caption-
preﬁx (Fig. 4c) shows higher intra-class variance across
prompts. Interestingly, results in Table 4 suggest that the
caption-preﬁx conditioned language features result in a bet-
ter zero-shot recognition performance than those condi-
tioned on the prompt-preﬁx. Although the prompt-preﬁx
mode extracts discriminative language embeddings, the em-
beddings do not perform well on the zero-shot recogni-
tion because the embeddings contain signiﬁcant bias from
image-classiﬁcation dataset.
Robustness in image domain shift. Test samples can
be unseen with respect to image classiﬁcation data in two
ways (or combinations of two): 1) The image is similar to
the training distribution, but the class name is different from
the seen image classiﬁcation labels. 2) Although the class
label is the same, the image data comes from a different
distribution. Datasets evaluated in the zero-shot recognition
include both two cases since the vocabularies and image are
from different domains. To understand them, we analyze
the test-time preﬁx by using ImageNet-1K and evaluate the
performance on image-level domain shift using variants of
ImageNet, i.e., ImageNet-V2 [32], ImageNet-R [11], and
ImageNet-S [37]. Table 5 describes the results of ablat-
ing preﬁx-conditioned training and the test-time preﬁx. The
preﬁx-conditioned training outperforms all baselines. This
reveals that the preﬁx-conditioned training achieves class
embeddings that are generalizable across image domains.
The prompt-style preﬁx performs the best in IN, IN-V2,
both of which have image styles similar to ImageNet. By
contrast, the caption-style preﬁx performs the best in IN-R
and IN-S, which has art-style and sketch-style images re-
spectively. Thus, the caption-style preﬁx generates more
generalizable class embeddings for the domain dissimilar
from the ImageNet training data. This observation is con-
sistent with the results in the paragraph Test time Preﬁx.
5. Conclusion
In this paper, we explore a simple yet effective mecha-
nism for uniﬁed pre-training on image-caption and image
classiﬁcation data. We propose to learn preﬁx tokens at
training time to condition the language encoder to switch
the input source. Specifying the preﬁx allows the model
to switch the manner of feature extraction and can control
which visual domain the embedding is projected to. This
approach boosts the performance of zero-shot recognition
accuracy of the contrastive learning models. Our analysis
suggests that the trained language encoder provides robust-
ness to the image-level domain shift. Although we limit our
scope to unifying image-caption and image-label supervi-
sion, incorporating other supervision such as object detec-
tion or semantic segmentation is an interesting next step.
Acknowledgment. We thank Zizhao Zhang for their help-
ful feedback on the manuscript. This work was supported
in part by DARPA LwLL.

Abbreviation
Dataset
#Concepts
Train size
Test size
Source link
Food
Food-101
102
75,750
25,250
Tensorﬂow
CF10
CIFAR-10
10
50,000
10,000
Tensorﬂow
CF100
CIFAR-100
100
50,000
10,000
Tensorﬂow
VOC
VOC2007 classiﬁcation
20
5,011
4,952
Tensorﬂow
DTD
Describable Textures
47
3,760
1,880
Tensorﬂow
Pets
Oxford-IIIT Pets
37
3,680
3,669
Tensorﬂow
Cal
Caltech-101
102
3,060
6084
Tensorﬂow
Flower
Oxford Flowers 102
102
1,020
6,149
Tensorﬂow
Patch
PatchCamelyon
2
294,912
32,768
Tensorﬂow
ESTAT
EuroSAT
10
N/A
27,000
Tensorﬂow
R45
Resisc45
45
N/A
31,500
Tensorﬂow
Table 6. Statistics of datasets used in zero-shot and linear probe.
A. Experimental Details
Dataset.
Table 6 describes the statistics of dataset
used for evaluation.
We pick the test datasets based on
UniCL [43] and availability in Tensorﬂow dataset.
We
use the test set to evaluate zero-shot recognition and linear
probe while the train set is used to train a linear classiﬁer.
Note that since EuroSAT and Resisc45 utilize the training
split for evaluation, we exclude the two datasets from linear
probe evaluation. Also, since Oxford Flowers do not have
many training samples (10 samples per class), we exclude
the dataset from the evaluation too.
Data Augmentation. Following UniCL [43], only ran-
dom cropping is applied to train all models for a fair com-
parison.
Computation. We use 32 Nvidia Tesla V100 GPUs to
train all models. 4 nodes, where each node has 8 GPUs, are
used to run experiments.
B. Additional Results
Attention Visualization.
Fig. 5 visualizes attention
weights for the class forest area, where a prompt tem-
plate, a tatto of, is employed.
The model focuses on a
word, forest when prompt preﬁx is employed. In other two
cases, the model also pays much attention to tatoo proba-
bly because the word should provide useful information to
distinguish a sentence from others for image-caption con-
trastive learning. Fig. 6 represents attention for a real cap-
tion from CC3M. While the model conditioned with cap-
tion preﬁx and unconditional model attend to several words
through many layers, the model conditioned with prompt
preﬁx shows clear attention only in the ﬁrst layer. Since the
prompt-conditioned model has never seen the real caption
during training, it fails in attending to discriminative words.
Class Name Shift. Test samples can be unseen with re-
spect to image classiﬁcation data in two ways (or combina-
tions of two): 1) The image is similar to training distribu-
tion, but the class name used for testing is different from
the image classiﬁcation label. 2) Although the class label
is the same, the image data comes from the different distri-
butions. Datasets evaluated in the zeros-shot recognition in-
clude both two cases since class names and images are from
different domains. 2) is analyzed in Subsection 4.3 of the
main paper, Robustness in image domain shift. We analyze
1) by evaluating the recognition performance of ImageNet-
1K by changing its class name from the one used during
training.
We ﬁnd a synonym for each class with Word-
Net [28], where we exclude synonyms substantially simi-
lar to the original class name and obtain synonyms for 525
classes. Then, we use the synonym to classify images dur-
ing evaluation. Since the input image distribution does not
vary, we can evaluate the performance on the class name
shift. If the model is robust to the change in the class name,
the degrade in the performance should be small.
The ﬁrst 6 rows of Table 7 describe the models trained
with the original class names and evaluated on both original
ones and synonyms, and the last two rows represent a model
trained with synonyms, where the original class names are
replaced with synonyms. Prompt preﬁx outperforms cap-
tion preﬁx with a large margin in testing with class names
used in training time. Generally, caption preﬁx performs
better when tested with the class names different from the
ones used during training. Prompt preﬁx is tailored to han-
dle class names employed during training time while cap-
tion preﬁx enables the language encoder to extract more
general representations.
Interestingly, the choice of class names seems to signif-
icantly change the generalization as shown in the compar-
ison between a model trained with synonyms and original
class names. The original model decreases the accuracy
more than 30% by changing the class name while the model
trained with synonym decreases less than 20%.
Image-Caption Retrieval.
In Table 8, we evaluate
the performance of image-caption retrieval using the sub-

Caption Prefix
Prompt Prefix
Unconditional Model
Figure 5. Attention visualization for a class prompt. Note that the attention weights are for and end token. Best viewed in color. The class
name shown here is one of class prompts in the EUROSAT dataset. Different rows show the weights of different transformer layers. With
a prompt preﬁx (leftmost), the model focuses on a class name (forest area) while caption preﬁx (middle) allows a model to pay attention
to another noun, tattoo. By preﬁx conditioning, the attention of the model changes as intended.
Caption Prefix
Prompt Prefix
Unconditional Model
Figure 6. Attention visualization for a real caption. Note that the attention weights are for and end token. Best viewed in color. The
sentence shown here is from CC3M. Different rows show the weights of different transformer layers. Caption preﬁx conditioning helps to
attend to many words while prompt conditioning fails to do that.
set of CC3M (12288 pairs of image and caption) and
COCO validation set (5000 pairs of image and caption),
where all models are trained with CC12M and ImageNet-
21K. First, our model (last row) slightly performs better
than the model without conditioning (ﬁrst row). Second,
prompt preﬁx conditioning (second row) signiﬁcantly per-
forms worse than caption preﬁx conditioning (last row).
Since the prompt preﬁx conditioning specializes a model
for the class name prompts of ImageNet21K, the condition-
ing does not generalize well to real captions.
Larger Batch-size and Training Epochs. We exam-
ine the effect of increasing batch-size and training epochs
in Table 9. In CLIP, increasing the batch-size and train-
ing epochs improves the performance of both ImageNet-1K
and zero-shot recognition. On the other hand, the zero-shot
performance of UniCL is not beneﬁted from training with
longer epochs (compare last and second to last row). UniCL
attempts to ensure the invariance of images from the same
classes by supervised contrastive loss while CLIP does not
consider it. However, such invariance is not necessarily re-
quired in zero-shot recognition, which leads to the degraded
performance.
Comparison to Reported UniCL’s Results.
In the
main paper, we provide our reproduced results of UniCL,

Train Data
Train on
Synonym
Preﬁx
Training
Test-Time
Preﬁx
Original
Synonym
top-1
top-5
top-1
top-5
IN1K + CC12M
N/A
69.3
89.3
31.2
49.5
IN1K + CC12M
✓
Prompt
75.0
92.9
38.3
54.8
IN1K + CC12M
✓
Caption
71.4
91.6
36.6
56.7
IN21K + CC12M
N/A
54.5
83.2
23.1
43.9
IN21K + CC12M
✓
Prompt
69.9
92.4
32.1
53.7
IN21K + CC12M
✓
Caption
65.3
90.6
33.5
56.9
IN21K + CC12M
✓
✓
Prompt
54.4
78.6
70.8
92.8
IN21K + CC12M
✓
✓
Caption
54.5
82.6
59.0
86.1
Table 7. Evaluation on the robustness to the class name shift using ImageNet-1K. Original refers to the subset of ImageNet-1K classes
while synonym refers to their synonyms taken from Wordnet. The last two rows indicate the models trained with the synonyms, thus
showing superior performance on synonym whereas degrading performance on Original.
Preﬁx
Training
Test-time
Preﬁx
CC3M
COCO
I2T@1 I2T@5 T2I@1 T2I@5 I2T@1 I2T@5 T2I@1 T2I@5
N/A
21.8
47.4
21.0
45.7
23.9
49.5
18.7
43.2
✓
Prompt
13.1
31.3
8.1
21.8
17.2
38.1
16.8
37.7
✓
Caption
22.6
47.5
21.6
46.1
24.7
49.7
19.7
43.9
Table 8. Image-text retrieval results on CC3M and COCO. The performance is evaluated on the subset of CC3M and validation set of
COCO. All models are trained on CC12M and ImageNet-21K. Caption conditioning (last row) slightly improves retrieval performance
compared to the unconditional model (ﬁrst row). Since prompt conditioning (middle) tailors a model for class-prompt, it fails to extract
discriminative information from real captions.
which is based on our implementation, since the authors
have not released the code and did not report the numer-
ical accuracy of each zero-shot recognition. In this para-
graph, we compare our approach and the reported perfor-
mance of UniCL [43] and K-Lite [34] by aligning sev-
eral hyper-parameters, e.g., batch-size and training epochs,
using ImageNet-1K. When using ImageNet-22K and CC-
15M for training, our method (batch-size:4096, training
epochs: 30) shows 73.9 while UniCL (batch-size:4096,
training epochs 32) reports 71.5. When using ImageNet-
21K excluding ImageNet-1K and CC-15M, our method
(batch-size:1024, training epochs 30) shows 49.7 whereas
UniCL (batch-size: 4096, training epochs: 32) and K-
Lite (batch-size: 4096, training epochs: 32) perform 46.6
and 48.7 respectively according to K-Lite results (See last
two rows of Table 3 in [34]). These results suggest that
our method performs better than the reported numbers of
UniCL and K-Lite in ImageNet-1K. Also, the knowledge
augmentation technique proposed by K-Lite can be comple-
mentary to our approach, thus combining two approaches is
an interesting research direction.
T-SNE visualization for language features. Fig. 7 vi-
sualizes extracted language features (ImageNet-1K) condi-
tioned with different preﬁxes. The prompt-preﬁx (left) has
lower intra-class and higher inter-class variance, whereas
caption-preﬁx (right) shows higher intra-class variance
across prompts.
T-SNE visualization for image features. Fig. 8 visu-
alizes image features from ImageNet-1K (blue) and CC3M
(red). Since ImageNet-1K is object-centered while CC3M
covers more diverse scenes, the distributions are separated.
This is consistent across baseline (w/o conditioning) and
our method (with conditioning).
Comparison between unconditioned and conditioned
model by language features. Fig. 9 visualizes language
features of ImageNet-1K class prompts (Blue) and CC3M
captions (Red) for unconditioned (left) and conditioned
(right) respectively. Note that the conditioned model uti-
lizes prompt preﬁx for class prompts and caption preﬁx
for real captions respectively. As seen from the visualiza-
tion, unconditioned model cannot distinguish some prompts
from captions of CC3M. This is probably because some
captions are similar to class prompts of ImageNet.
By
contrast, the conditioned model differentiate class prompts
from captions better than unconditioned model due to the
preﬁx conditioning.
References
[1] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee,
and Bernt Schiele.
Evaluation of output embeddings for
ﬁne-grained image classiﬁcation.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2927–2936, 2015. 2

Training Data
Objective
Batch-size
Epochs
Metric
Classiﬁcation
Caption
IN-1K
Zero-shot
11 datasets
ImageNet-21K
CC-12M
CLIP
1024
15
67.3
57.8
ImageNet-21K
CC-12M
CLIP
1024
30
69.1
58.3
ImageNet-22K
CC-15M
CLIP
1024
15
69.3
58.5
ImageNet-22K
CC-15M
CLIP
4096
15
71.1
59.5
ImageNet-22K
CC-15M
CLIP
4096
30
72.2
59.8
ImageNet-22K
CC-15M
UniCL
1024
15
69.7
58.5
ImageNet-22K
CC-15M
UniCL
4096
15
70.3
60.4
ImageNet-22K
CC-15M
UniCL
4096
30
73.9
58.9
Table 9. Performance comparison among different batch-size and training epochs. ImageNet-22K denotes the combination of ImageNet-
21K and ImageNet-1K, CC-15M indicates that of CC-12M and CC-3M.
(a) Prompt conditioned
(b) Caption conditioned
Figure 7. T-SNE [36] visualization of the class-prompt features
of ImageNet-1K with different preﬁx conditions. Different colors
indicate language embeddings of different classes. Prompt con-
ditioning extracts more class discriminative representations than
caption conditioning.
(a) W/O conditioning
(b) With conditioning
Figure 8.
T-SNE [36] visualization of the image features of
ImageNet-1K (blue) and CC3M (red).
Since ImageNet-1K is
object-centered while CC3M covers more diverse scenes, the dis-
tributions are separated. This is consistent across baseline (w/o
conditioning) and our method (with conditioning).

(a) Unconditioned model
(b) Conditioned model
Figure 9.
T-SNE [36] visualization of language features of
ImageNet-1K class prompts (Blue) and CC3M captions (Red) for
unconditioned (left) and conditioned (right) respectively. Our pro-
posed condition better differentiates prompts from real captions.
[2] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo,
and Seong Joon Oh. Learning de-biased representations with
biased representations. In International Conference on Ma-
chine Learning, pages 528–539. PMLR, 2020. 3
[3] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and
Boris Katz. Objectnet: A large-scale bias-controlled dataset
for pushing the limits of object recognition models.
Ad-
vances in neural information processing systems, 32, 2019.
3
[4] Stephanie CY Chan, Adam Santoro, Andrew K Lampinen,
Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay Mc-
Clelland, and Felix Hill. Data distributional properties drive
emergent few-shot learning in transformers. arXiv preprint
arXiv:2205.05055, 2022. 1
[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut.
Conceptual 12M: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In CVPR,
2021. 1, 2, 5
[6] Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu,
and Osamu Yoshie.
Zerovl: A strong baseline for align-
ing vision-language representations with limited resources.
arXiv preprint arXiv:2112.09331, 2021. 4, 5
[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 1, 2, 3, 5
[8] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland,
and Dhruv Mahajan. Adaptive methods for real-world do-
main generalization. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
14340–14349, 2021. 3
[9] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,
Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. De-
vise: A deep visual-semantic embedding model. Advances
in neural information processing systems, 26, 2013. 2
[10] Tianyu Gao, Adam Fisch, and Danqi Chen.
Making pre-
trained language models better few-shot learners.
arXiv
preprint arXiv:2012.15723, 2020. 3, 4
[11] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of robust-
ness: A critical analysis of out-of-distribution generalization.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 8340–8349, 2021. 8
[12] Dinesh Jayaraman and Kristen Grauman. Zero-shot recogni-
tion with unreliable attributes. Advances in neural informa-
tion processing systems, 27, 2014. 2
[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
Conference on Machine Learning, pages 4904–4916. PMLR,
2021. 1, 2
[14] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.
3
[15] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. Advances
in Neural Information Processing Systems, 33:18661–18673,
2020. 2
[16] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu
Ma, and Percy Liang. Fine-tuning can distort pretrained fea-
tures and underperform out-of-distribution. arXiv preprint
arXiv:2202.10054, 2022. 3
[17] Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and
Jaegul Choo. Learning debiased representation via disentan-
gled feature augmentation. Advances in Neural Information
Processing Systems, 34:25123–25133, 2021. 3
[18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efﬁcient prompt tuning. arXiv preprint
arXiv:2104.08691, 2021. 2, 3, 4
[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
ﬁed vision-language understanding and generation.
arXiv
preprint arXiv:2201.12086, 2022. 2
[20] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi.

Align before fuse: Vision and language representation learn-
ing with momentum distillation. Advances in Neural Infor-
mation Processing Systems, 34, 2021. 1, 2
[21] Xiang Lisa Li and Percy Liang.
Preﬁx-tuning: Optimiz-
ing continuous prompts for generation.
arXiv preprint
arXiv:2101.00190, 2021. 2, 3, 4
[22] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli
Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan.
Su-
pervision exists everywhere: A data efﬁcient contrastive
language-image pre-training paradigm.
arXiv preprint
arXiv:2110.05208, 2021. 2
[23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in nat-
ural language processing. arXiv preprint arXiv:2107.13586,
2021. 3, 4
[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), 2021. 5
[25] Ilya Loshchilov and Frank Hutter.
Decoupled weight de-
cay regularization. In International Conference on Learning
Representations, 2019. 5
[26] Thomas Mensink, Efstratios Gavves, and Cees GM Snoek.
Costa: Co-occurrence statistics for zero-shot classiﬁcation.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 2441–2448, 2014. 2
[27] Christian M Meyer and Iryna Gurevych. Wiktionary: A new
rival for expert-built lexicons? Exploring the possibilities of
collaborative lexicography. na, 2012. 2
[28] George A Miller. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39–41, 1995. 2, 9
[29] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
ing Xie. Slip: Self-supervision meets language-image pre-
training. arXiv preprint arXiv:2112.12750, 2021. 2
[30] Guanghui Qin and Jason Eisner.
Learning how to ask:
Querying lms with mixtures of soft prompts. arXiv preprint
arXiv:2104.06599, 2021. 3, 4
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021. 1, 2, 4, 5
[32] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classiﬁers generalize to im-
agenet? In International Conference on Machine Learning,
pages 5389–5400. PMLR, 2019. 8
[33] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages
2556–2565, 2018. 5
[34] Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jian-
wei Yang, Pengchuan Zhang, Anna Rohrbach, Zhe Gan,
Lijuan Wang, Lu Yuan, et al.
K-lite: Learning transfer-
able visual models with external knowledge. arXiv preprint
arXiv:2204.09222, 2022. 2, 11
[35] Kihyuk Sohn. Improved deep metric learning with multi-
class n-pair loss objective. Advances in neural information
processing systems, 29, 2016. 4
[36] Laurens Van der Maaten and Geoffrey Hinton.
Visualiz-
ing data using t-sne. Journal of machine learning research,
9(11), 2008. 8, 12, 13
[37] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Xing. Learning robust global representations by penalizing
local predictive power. In Advances in Neural Information
Processing Systems, pages 10506–10518, 2019. 3, 8
[38] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot
recognition via semantic embeddings and knowledge graphs.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 6857–6866, 2018. 2
[39] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, et al. Robust ﬁne-tuning of zero-shot models.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 7959–7971, 2022. 3
[40] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh
Nguyen, Matthias Hein, and Bernt Schiele. Latent embed-
dings for zero-shot classiﬁcation.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 69–77, 2016. 2
[41] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot
learning-the good, the bad and the ugly. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4582–4591, 2017. 2
[42] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda,
Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou
Huang. Vision-language pre-training with triple contrastive
learning. arXiv preprint arXiv:2202.10401, 2022. 2
[43] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce
Liu, Lu Yuan, and Jianfeng Gao. Uniﬁed contrastive learning
in image-text-label space. arXiv preprint arXiv:2204.03610,
2022. 1, 2, 4, 5, 6, 9, 11
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 2
[45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917, 2022. 1
[46] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. arXiv
preprint arXiv:2109.01134, 2021. 2, 3, 4
[47] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-
wei Liu. Conditional prompt learning for vision-language
models. arXiv preprint arXiv:2203.05557, 2022. 3

