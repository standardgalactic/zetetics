JHEP07(2023)188
Published for SISSA by
Springer
Received: January 5, 2023
Revised: May 8, 2023
Accepted: July 9, 2023
Published: July 25, 2023
Resonant anomaly detection with multiple reference
datasets
Mayee F. Chen,a Benjamin Nachmanb,c and Frederic Salad
aComputer Science Department, Stanford University,
353 Jane Stanford Way, Stanford, CA 94305, U.S.A.
bPhysics Division, Lawrence Berkeley National Laboratory,
1 Cyclotron Road, Berkeley, CA 94720, U.S.A.
cBerkeley Institute for Data Science, University of California,
190 Doe Library, Berkeley, CA 94720, U.S.A.
dDepartment of Computer Sciences, University of Wisconsin,
1210 W. Dayton Street, Madison, WI 53706, U.S.A.
E-mail: mfchen@stanford.edu, bpnachman@lbl.gov, fredsala@cs.wisc.edu
Abstract: An important class of techniques for resonant anomaly detection in high energy
physics builds models that can distinguish between reference and target datasets, where
only the latter has appreciable signal. Such techniques, including Classiﬁcation Without
Labels (CWoLa) and Simulation Assisted Likelihood-free Anomaly Detection (SALAD)
rely on a single reference dataset.
They cannot take advantage of commonly-available
multiple datasets and thus cannot fully exploit available information. In this work, we
propose generalizations of CWoLa and SALAD for settings where multiple reference
datasets are available, building on weak supervision techniques. We demonstrate improved
performance in a number of settings with realistic and synthetic data. As an added beneﬁt,
our generalizations enable us to provide ﬁnite-sample guarantees, improving on existing
asymptotic analyses.
Keywords: Jets and Jet Substructure, Other Weak Scale BSM Models, Speciﬁc BSM
Phenomenology
ArXiv ePrint: 2212.10579
Open Access, c⃝The Authors.
Article funded by SCOAP3.
https://doi.org/10.1007/JHEP07(2023)188

JHEP07(2023)188
Contents
1
Introduction
2
2
Problem setup
3
3
Multi-CWoLa: learning from multiple resonant features
3
3.1
Multi-CWoLa method
4
3.1.1
Model
4
3.1.2
Parameter estimation
5
3.1.3
Inference and training
6
3.2
Theoretical results
6
3.3
Empirical results
8
4
Multi-SALAD: learning from multiple simulations
10
4.1
Multi-SALAD method
10
4.2
Theoretical results
11
4.3
Empirical results
12
5
Conclusions and outlook
14
A Appendix organization
16
B Glossary
17
C Additional algorithmic details
17
C.1
Multi-SALAD algorithm
17
D Additional theoretical results
19
D.1 The need for 3 resonant features
19
D.2 Rademacher complexity bounds
20
D.3 Bound on CWoLa’s generalization error
20
D.4 Asymptotic behavior of SALAD’s ˆLS(h, w)
22
E Proofs
22
E.1
Proof of Theorem 1
22
E.2
Proof of Theorem 2
23
F Experiment details
25
F.1
Multi-CWoLa experiments
25
F.2
Multi-SALAD experiments
26
– 1 –

JHEP07(2023)188
1
Introduction
Due to the vast parameter space of Standard Model extensions and to the lack of signiﬁcant
evidence for new particles or forces of nature, a new model-agnostic search paradigm has
emerged. Many of these anomaly detection (AD) strategies are enabled by machine learning
(see e.g. refs. [1–4]) and the ﬁrst results with collision data are now available [5, 6]. One way
to characterize AD methods is based on their physics assumption of the new phenomena [2].
Strategies that assume the new physics is “rare” [7] estimate (explicitly or implicitly) the
data probability density and focus on events with low density. In contrast, techniques that
assume the new physics will manifest as an overdensity in phase space use likelihood ratio
methods to compare a reference dataset to a target dataset. The latter approach has been
extensively studied in the context of resonant anomaly detection [8], where one resonant
feature (usually a mass) is used to create a sideband region (reference dataset) nearly devoid
of any anomalous events and a signal region (target dataset) that may contain anomalies.
The reference dataset is used to estimate the presence of anomalies in the target dataset
via interpolation.
Many existing approaches are deﬁned using one reference dataset and one target
dataset [9–18, 18–24].
However, in practice one can have access to or construct mul-
tiple references.
First, there may exist multiple resonant features that can be used to
construct sideband and signal regions. For instance, when a particle decays into two new
particles, the decay products can be used to construct all three intermediate resonances,
a setting present in the LHC Olympics Dataset [3]. Second, there may also exist multiple
independent Standard Model simulators available for producing a dataset (e.g. Pythia [25],
Herwig [26], or Sherpa [27]). Using multiple reference datasets may improve performance,
but it is not clear how to incorporate all of their information when using existing methods
designed for a single set.
We explore two generalizations of resonant AD to multiple reference datasets. First,
we consider Classiﬁcation Without Labels (CWoLa) [9, 10, 28], in which the reference is
simply the sideband region — a form of weak supervision where the noisy label of “signal”
is assigned to events in the signal region and the noisy label of ‘background’ to events in
the sideband region. We propose a new method, Multi-CWoLa, that builds multiple
reference datasets by constructing signal and sideband regions along diﬀerent resonant
features. We consider a point’s membership in each feature’s signal region as a noisy vote for
anomaly, learn weights on each vote, and aggregate them to produce a higher-quality noisy
label. We demonstrate Multi-CWoLa’s performance on the LHC Olympics Dataset [3].
Next, we study Simulation Assisted Likelihood-free Anomaly Detection (SALAD) [14].
In this method, a reweighting function between a reference simulation dataset and a tar-
get dataset is learned in the sideband conditioned on the resonant feature.
The simu-
lated events in the signal region are reweighted by interpolating this function and then
are used to distinguish anomalies in the target dataset. We extend this to the case of
multiple simulated datasets, each of which may make diﬀerent approximation choices and
thus provide complementary accuracy when using SALAD. We introduce Multi-SALAD,
which combines the simulated datasets accordingly and then reweights, with the key ﬁnd-
– 2 –

JHEP07(2023)188
ing that combining data helps when each simulator approximates diﬀerent components of
the background well. We demonstrate Multi-SALAD’s performance on synthetic data.
Reweighting simulations is a widely-used procedure in high energy physics, well beyond
anomaly detection. While we focus on anomaly detection here because of the need for
model-independent bounds, the approach here is generally applicable.
Finally, we study the ﬁnite sample guarantees of our proposed methods. Many res-
onant AD methods have optimality guarantees in some asymptotic limit, but there is no
ﬁrst-principles understanding of the methods’ performance with ﬁnite samples. In partic-
ular, approaches like the ones described above that use classiﬁers to distinguish a refer-
ence dataset from a target dataset approximate the signal-to-background likelihood ratio.
When the reference (physics) model is correct, this approach will converge to the optimal1
Neyman-Pearson likelihood ratio test in the limit of inﬁnite statistics, complex enough clas-
siﬁer architecture, and ﬂexible enough training procedure [15, 29]. However, a ﬁnite sample
understanding of these approaches is lacking. We draw on results from statistical theory to
begin a formal study of resonant AD methods with limited data. Our results lay a founda-
tion for future investigations into the ﬁnite sample properties of AD and related methods.
This paper is organized as follows. Section 2 brieﬂy set up the resonant AD setting and
then Multi-CWoLa and Multi-SALAD are introduced in sections 3 and 4, respectively.
The paper ends with conclusions and outlook in section 5.
2
Problem setup
We have an input space of discriminating features x ∈X and k resonant features m =
[m1, . . . , mk] ∈Rk. Associated with a point (x, m) is an unknown label y ∈Y for Y = {0, 1}
(background vs. signal). Points (x, m, y) are drawn from a distribution P with density
p(·). For a resonant feature mi ∈R, an interval Imi ⊂R is used to deﬁne a signal region
SRi = {(x, m) : mi ∈Imi} and a sideband region SBi = {(x, m) : mi /∈Imi} (when the
resonant feature is obvious, the i is dropped and we use SR and SB). We assume that the
sideband region contains little to no signal, i.e., p(y = 1|(x, m) ∈SB) ≈0. Our goal is to
construct a predictor f : X →Y to perform anomaly detection.
3
Multi-CWoLa: learning from multiple resonant features
We introduce Multi-CWoLa, an approach to anomaly detection that uses multiple ref-
erence datasets and is built using principles from the area of weak supervision [30, 31].
Standard CWoLa.
We have one unlabeled dataset D = {(xi, mi)}n
i=1 with one resonant
feature (k = 1) that we want to use to learn f. We use m to construct the signal and side-
band regions, DSR, DSB ⊂D where DSR = D ∩SR and DSB = D ∩SB, with distributions
pSR and pSB respectively. With the intuition that there are more anomalies in the signal
1Optimal for a given model is not achievable as we do not posit a particular signal hypothesis. Instead,
we are optimal for the following hypothesis test where the null hypothesis is that data is drawn from the
reference dataset and the alternative hypothesis is that the data is drawn from the target dataset. See
appendix A in ref. [15] for details.
– 3 –

JHEP07(2023)188
region, we express each distribution as a mixture of signal and background components
with weight 0 ≤ηSR, ηSB ≤1:
pSR(x) = ηSRp(x|y = 1) + (1 −ηSR)p(x|y = 0)
(3.1)
pSB(x) = ηSBp(x|y = 1) + (1 −ηSB)p(x|y = 0)
(3.2)
Under this construction, the density ratio of the mixtures pSR(x)
pSB(x) can be written in
terms of the ratio of the signal and background components, r(x) = p(x|y=1)
p(x|y=0), as pSR(x)
pSB(x) =
ηSRr(x)+1−ηSR
ηSBr(x)+1−ηSB . Assuming ηSR > ηSB (e.g. more signal in the signal region), the mixture
ratio is monotonically increasing in r(x). Therefore, we train a classiﬁer f to learn pSR(x)
pSB(x) by
distinguishing between DSR and DSB, and this f provides information about r(x) and can
be used for anomaly detection. Note that CWoLa requires that x and m are independent
of each other.
3.1
Multi-CWoLa method
Intuitively, CWoLa uses the resonant feature m as a noisy label that identiﬁes the signal
versus sideband region and then trains a classiﬁer using these. This idea leads to a simple
question — if more than one such feature is available (k > 1), how can the multiple noisy
labels best be utilized? We tackle this question using principles from weak supervision [30–
33]. The idea is that some features that were used to train the weakly supervised classiﬁer
could instead be used to improve the weak labels. In many cases, it is possible to know how
to pick labeling functions with these ‘resonant’ features,2 such as when a known particle
mass is reconstructed. If the additional resonant features correspond to unknown particle
masses, it may not be possible to know how to use the features to inform accurate weak
labels. Scanning over possible intervals could result in a large trails factor. In the numerical
examples below, we include both cases where the features are useful (we know the right
interval) and not very useful (we do not know the right interval) for the weak labeling.
3.1.1
Model
In our approach, we split D along each resonant feature mi to produce pairs of datasets
DSBi and DSRi for each i ∈[k] based on membership in Imi. A straightforward way to
use all datasets (DSB1, DSR1), . . . , (DSBk, DSRk) is to apply standard CWoLa k times by
training k classiﬁers that we can then ensemble or average. Instead, in Multi-CWoLa,
we construct a binary vector per x consisting of k noisy membership labels, M(m) =
{M1(m), . . . , Mk(m)} ∈{0, 1}k, where Mi(m) = 1 if (x, m) ∈DSRi and Mi(m) = 0 if
(x, m) ∈DSBi. We propose to directly aggregate these labels M(m) into an estimate of y,
ˆy, and train a classiﬁer on the aggregated ˆy along with the discriminative features x. Since
each Mi(m)’s “vote” can have diﬀerent correlation with the true y, we aim to combine the
votes in a weighted fashion. We cannot directly measure each membership label’s accuracy
since the true y is unknown, so we draw on methods from weak supervision.
2By resonant, we really mean that we can isolate a region of high signal-to-background — it does not
have to be a closed interval.
– 4 –

JHEP07(2023)188
We model the distribution p(y, M(m)) as a probabilistic graphical model with the
following parametrization:
p(y, M(m); θ) = 1
Z exp

θy ey +
k
X
i=1
θi f
Mi(m)ey

,
(3.3)
where θ = {θy, θi ∀i ∈[k]} are the canonical parameters of the distribution, Z is for
normalization, and ey and f
Mi(m) are y and Mi(m) scaled from {0, 1} to {−1, 1}. Intuitively,
θi represents the (unobserved) strength of the correlation between Mi(m) and y and thus
captures a notion of Mi’s accuracy. This model also implies, for simplicity, that Mi(m) ⊥
Mj(m)|y; that is, the resonant features are conditionally independent given y in addition
to m and x being independent as in the standard CWoLa case.3
Our goal is to estimate the parameters of the graphical model and use them to perform
inference, producing aggregated weak labels ˆy from the distribution p(y = 1|M(m); θ) given
a vector of noisy labels M(m).
3.1.2
Parameter estimation
We ﬁrst learn the parameters of p(y, M(m); θ) as deﬁned in (3.3).
Of key interest is
the accuracy parameter αi = p(Mi(m) = 1|y = 1) = p(Mi(m) = 0|y = 0) of the ith
resonant feature, which corresponds to the canonical parameter θi (see [35] for more back-
ground on probabilistic graphical models). We estimate the accuracy parameters by adapt-
ing the triplet approach from [31]. First, we draw triplets of resonant features a, b, c ∈
[k].4
If the distribution on y, M(m) follows the graphical model in (3.3), it holds that
yMa(m) ⊥yMb(m) if Ma(m) ⊥Mb(m)|y. Then, we have that E[ey f
Ma(m)]E[ey f
Mb(m)] =
E[ f
Ma(m) f
Mb(m)] since ey2 = 1.
Writing one such equation for each pair in the triplet
(a, b, c), we have that
E[ey f
Ma(m)]E[ey f
Mb(m)] = E[ f
Ma(m) f
Mb(m)]
E[ey f
Ma(m)]E[ey f
Mc(m)] = E[ f
Ma(m) f
Mc(m)]
E[ey f
Mb(m)]E[ey f
Mc(m)] = E[ f
Mb(m) f
Mc(m)].
Solving this system, we obtain
|E[ey f
Ma(m)]| =
v
u
u
t

E[ f
Ma(m) f
Mb(m)]E[ f
Ma(m) f
Mc(m)]
E[ f
Mb(m) f
Mc(m)]
,
and similarly for b and c. We assume that each signal region is positively correlated with
the true signal, which allows for us to ignore the absolute value and uniquely recover
E[ey f
Ma(m)].
Next, we can use E[ey f
Ma(m)] = 2p(ey = f
Ma(m)) −1 to obtain αi using
properties of the graphical model in (3.3). Note that in practice, all of these quantities are
empirical estimates, with terms such as ˆE[ f
Ma(m) f
Mb(m)] = 1
n
Pn
i=1 f
Ma(mi) f
Mb(mi).
3We can model some dependencies among resonant features if desired (see [31] for a method and see [34]
for how to learn if resonant features are not conditionally independent). However, we need at least three
conditionally independent subsets of resonant features in M(m) in order for the estimation method from [31]
to recover the correct parameters.
4We assume that k ≥3. In Lemma 1, we discuss why having k = 1 or k = 2 resonant features does not
recover a unique model.
– 5 –

JHEP07(2023)188
Algorithm 1 Multi-CWoLa.
1: Input: dataset D = {(xi, mi)}n
i=1; thresholds Imi that split D into signal and sideband
regions, DSRi and DSBi respectively, for each mi; class balance probability of anomaly
p(y = 1)
2: Construct noisy label Mi(m) =



1
(x, m) ∈DSRi
0
(x, m) ∈DSBi
for each resonant feature mi.
3: for each triplet a, b, c ∈[k] do
4:
βa :=
qˆE[ f
Ma(m) f
Mb(m)]ˆE[ f
Ma(m) f
Mc(m)]/ˆE[ f
Mb(m) f
Mc(m)]

(3.4)
βb :=
qˆE[ f
Ma(m) f
Mb(m)]ˆE[ f
Mb(m) f
Mc(m)]/ˆE[ f
Ma(m) f
Mc(m)]

(3.5)
βc :=
qˆE[ f
Ma(m) f
Mc(m)]ˆE[ f
Mb(m) f
Mc(m)]/ˆE[ f
Ma(m) f
Mb(m)]
,
(3.6)
where ˆE is an empirical estimate of the expectation over D, and f
M(m) indicates
M(m) scaled to {−1, 1}.
5: end for
6: Set accuracy parameter αi = ˆp(Mi(m) = 1|y = 1) = ˆp(Mi(m) = 0|y = 0) = ˆp(Mi(m) =
y) = βi+1
2 .
7: Compute estimate ˆp(y = 1|M(m)) ∝Qm
i=1 ˆp(Mi(m)|y = 1)p(y = 1).
8: Construct ˆy ∼ˆp(y = 1|M(m)) for each (x, m) ∈D.
9: Output: classiﬁer ˆf for anomaly detection trained on {(xi, mi, ˆyi)}n
i=1.
3.1.3
Inference and training
After we learn the accuracy parameters, we use them to estimate p(y = 1|M(m)) for a
given M(m). We use Bayes’ rule and the conditional independence among M(m) to write
p(y|M(m)) =
Qm
i=1 p(Mi(m)|y=1)p(y=1)
p(M(m))
. We assume that the class balance p(y = 1) is known;
otherwise, it can be estimated via tensor decomposition [33]. p(Mi(m)|y = 1) is either equal
to αi if Mi(m) = 1 or 1 −αi if Mi(m) = 0, and the denominator p(M(m)) can be either
directly estimated since all quantities are observable or computed as Qm
i=1 p(Mi(m)|y =
1)p(y = 1)+Qm
i=1 p(Mi(m)|y = 0)p(y = 0) using the estimated accuracies and class balance.
Once p(y = 1|M(m)) is estimated for all M(m) ∈{0, 1}k, the aggregated weak label ˆy
is drawn from such distribution. With labels ˆy for each (x, m) ∈D, we train a classiﬁer ˆf
on the weakly labeled dataset {(x, ˆy)}n
i=1. This procedure is summarized in Algorithm 1.
3.2
Theoretical results
Under (3.3), Multi-CWoLa oﬀers ﬁnite-sample generalization guarantees. Suppose the
downstream model ˆf trained on ˆy belongs to class F. Deﬁne a loss function ℓC : Y ×Y →R
and let the expected loss of f be LC(f) := E [ℓC(f(x), y)] on true labels. Then, the optimal
classiﬁer is f⋆= argminf∈F LC(f), which is achieved with unlimited labeled data. Let
the empirical loss of f on ˆy be ˆLC(f) :=
1
n
Pn
i=1 ℓC(f(xi), ˆyi).
Then, the ˆf we learn
is constructed from ˆf = argminf∈F ˆLC(f), which is learned on ﬁnite and noisily labeled
– 6 –

JHEP07(2023)188
data. Note that this construction is diﬀerent from the standard empirical risk minimization
(ERM) loss on labeled data, and thus ˆLC(f) does not asymptotically equal LC(f). We
aim to minimize the generalization error LC( ˆf) −LC(f⋆).
We now present our result on an upper bound for LC( ˆf) −LC(f⋆).
Deﬁne the
Rademacher complexity of F as Rn(ℓ◦F) = E
h
supf∈F
1
n
Pn
i=1 εiℓ(f(xi), yi)
i
with ran-
dom variables Pr(ε = 1) = Pr(ε = −1) = 1
2. Deﬁne emin as the minimum eigenvalue of
the covariance matrix on [y, M1(m), . . . , Mk(m)], and let amin be the minimum value of
E[ f
Mi(m)y] over all i.
Theorem 1. Assume that p(y, M(m)) can be parametrized according to (3.3) and that ℓ
is scaled to be bounded in [0, 1]. Assume that the class balance p(y) is known (if not, there
are ways to estimate it [33]), and that k ≥3. Then, with probability at least 1 −δ, the
generalization error of Multi-CWoLa on D is at most
LC( ˆf) −LC(f⋆) ≤4Rn(ℓ◦F) + 2
s
log 2/δ
2n
+
c1
emina5
min
 s
k
n + c2k
√n
!
,
where c1, c2 are positive constants.
The proof of Theorem 1 is provided in appendix E. We observe that there are several
quantities controlling the above bound:
• The Rademacher complexity of F: this term describes the model’s expressivity. Smaller
Rademacher complexity means that the model is easier to learn and that our ˆf will
be closer to the best model in F. This quantity can be readily computed for a variety
of function classes F, such as decision trees, linear models, and two-layer feedforward
networks, which makes our bound in Theorem 1 tractable. See appendix D.2 for exact
values.
• Using n ﬁnite samples: as the amount of data increases, the error decreases in O(n−1/2).
• Using noisy labels ˆy instead of y: for our weak supervision algorithm and graphical
model, using ˆy rather than y contributes an additional O(n−1/2) error. Asymptotically,
our approach thus does no worse than training with labeled data.
• The number of resonant features k: as k increases, the expression increases. This is due
to the fact that larger k results in more accuracy parameters to estimate.
By contrast, the standard CWoLa approach with k = 1 does not utilize any aggrega-
tion or weak supervision, which requires k ≥3. For standard CWoLa, the second term
in the generalization error is irreducible due to the fact that using any single resonant
feature in place of y is biased; see Theorem 3 in appendix D for the exact generalization
bound. On the other hand, Multi-CWoLa corrects for some of this bias; the second term
asymptotically approaches 0 with more data.
– 7 –

JHEP07(2023)188
3.3
Empirical results
In ﬁgure 1, we compare Multi-CWoLa with standard CWoLa as well as three other base-
lines. For this study, we use datasets from the LHC Olympics [3]. This data challenge was
created to develop and test resonant anomaly detection methods. The setting is events with
a pair of hadronic jets whose invariant mass is in excess of 1 TeV. Inclusive bump hunts are
broadly sensitive to localized excess in the dijet invariant mass spectrum, but are not partic-
ularly sensitive when the jets have complex substructure. For example, if the jets are formed
from a new resonance A that decays to two other resonances B and C with mB, mC ≪mA
and B, C →qq′, then B and C particles will form two-prong jets that are readily distin-
guishable from the one-prong quark and gluon jets in the background. This signal scenario
is exactly what is present in the LHC Olympics dataset. All events are simulated with
Pythia 8 [25] and for simplicity, are summarized by ﬁve features: the invariant mass of the
two jets (k = 1), the masses of the two jets, and a measure of the two-pronginess of the two
jets (via the n-subjettiness ratio τ21 [36, 37]). In the standard CWoLa setup, we use one
thresholded resonant feature (k = 1) and use 4 discriminative features as x. For Multi-
CWoLa, we have generated k = 3 mixtures by varying how the 3 resonant features (the
jet masses in addition to the dijet mass) are thresholded and use 2 discriminative features
as x. Since we do not know the values of the masses, we pick windows in the middle of the
spectrum for the individual jet masses (correct for one jet and not the other), as shown in
ﬁgure 7. The proportion of anomalies in the training dataset is 0.149, while the proportion
in the test dataset is 0.289. We have three other baselines that utilize 3 resonant features:
• CWoLa + intersect deﬁnes the signal region as the intersection of the resonant fea-
tures’ signal regions, e.g. SR = SR1∩SR2∩SR3, but this can be overly conservative.
• CWoLa +x thresholding has one resonant feature as the noisy label ˆy = M1(m), and
includes the remaining thresholded features as discriminative {M2(m), M3(m), x}.
• CWoLa + average runs standard CWoLa three times, once per resonant feature and
with the 2 discriminative features. The three model scores are averaged to produce
the ﬁnal output.
We vary the number of samples available on a logarithmic scale from n = 59 to 6003
and plot the AUC averaged over 5 runs per sample size in 1. We ﬁnd that Multi-CWoLa
oﬀers a higher AUC and lower variance, especially when there is limited data. We also
plot the (Signiﬁcance Improvement) SI curves averaged over 5 runs for n = 59, 530, 6003
in 2. The SI corresponds to a multiplicative factor by which the signiﬁcance of the anomaly
increases with a corresponding threshold set by the x-axis. Numerically, the SI is the true
positive rate divided by the square root of the false positive rate. The anomaly detection
is useful only if the max SI is above 1. In practice, we do not know what threshold to make
(which is signal-model dependent), so a set of cuts could be applied. Scanning over the
cut would introduce a trials factor, but it is non-trivial to calculate given the correlation
between diﬀerent thresholds. Recent experimental results have used a couple of thresholds
that are widely diﬀerent so that these correlations and the trials factor are both small [5].
More experimental details and results are in appendix F.
– 8 –

JHEP07(2023)188
Figure 1. Comparison between CWoLa and Multi-CWoLa. Using multiple mixed samples
helps performance across a range of dataset sizes. Access to multiple weak sources enables better
AUC and lower variance compared to the single-feature version.
Figure 2. Signiﬁcance Improvement (SI) curve for Multi-CWoLa at sizes n = 59, 530, and 6003.
– 9 –

JHEP07(2023)188
4
Multi-SALAD: learning from multiple simulations
We often have access to a(n approximate) simulation of the background process. We ﬁrst
provide an overview of SALAD, which reweighs samples from the simulation to better
assist with classiﬁcation on the real dataset. Then, we present Multi-SALAD, a variant
of SALAD that uses multiple simulations.
Standard SALAD.
We have a background simulation dataset Dsim = {(xi, mi)}nsim
i=1
with yi = 0 for all i in addition to one true dataset D = {(xi, mi)}n
i=1. Dsim is drawn from
some distribution Psim with density psim. While CWoLA learns the likelihood ratio between
the signal and sideband regions of D alone, SALAD utilizes Dsim as well. Note that if psim
is equal to p(·|y = 0), we could directly train a model to distinguish between D and Dsim in
the signal region to get a classiﬁer that could detect anomalies. However, since Dsim may
not match the true background data, we instead ﬁrst need to learn a reweighting function
that captures the diﬀerences between Dsim and D’s background data, and then we train a
model to distinguish between D and the reweighted Dsim in the signal region. Formally,
given ﬁxed SR and SB for both datasets, the method can be broken into two steps:
1. Reweighting: a classiﬁer ˆg is trained to distinguish between Dsim
SB = Dsim ∩SB and
DSB. Assuming that the sideband region has no anomalies, this ˆg is able to produce
an estimate of the weight ratio5 w(x, m) =
p(x,m|y=0)
psim(x,m|y=0) ≈
ˆg(x,m)
1−ˆg(x,m), assuming that
the datasets are the same size (|Dsim
SB| = |DSB|).
2. Detection: using a loss function LS with estimated ˆw(x, m) applied to Dsim
SR =
Dsim ∩SR, a classiﬁer ˆh is trained to distinguish between DSR and Dsim
SR.
If the estimate ˆw(x, m) is exactly equal to w(x, m) (e.g. ˆg is Bayes-optimal), then the
second step will be equivalent in expectation to learning the ratio
p(x)
p(x|y=0) (see Lemma 2
in appendix D.4), from which one can detect anomalies.
4.1
Multi-SALAD method
Now, we have multiple simulation datasets Dsim
1
, . . . , Dsim
k
.
One approach would be to
maintain distinctions among simulations by reweighing each pair to learn k weight func-
tions wi(x, m), and then using one overall loss function that weights points from each Dsim
SR,i
with wi. However, it has been shown that importance reweighting, despite working in ex-
pectation, can be highly unstable and result in poor performance of tasks on the target data
D [42]. To understand why, ref. [43] showed that the generalization error of an empirical
loss function with importance weights w depends on the magnitude of w. Applied to our
setting, it suggests that the more inaccurate the simulation is, the less the reweighted loss
recovers the true
p(x)
p(x|y=0), and the model may instead pick up on diﬀerences between DSR
and the reweighted Dsim
SR that are noise rather than the anomaly. As a result, aggregating
individual SALAD outputs can be equivalent to ensembling many poor classiﬁers.
5This is with the binary cross entropy loss function (also works for other functions [38]). This likelihood-
ratio trick is well-known (see e.g. refs. [39, 40]), also in high-energy physics (see e.g. ref. [41]).
– 10 –

JHEP07(2023)188
Given these observations, Multi-SALAD uses multiple simulation datasets in a very
simple yet theoretically principled way: control the magnitude of the overall w by com-
bining all the Dsim
i
to produce one large simulation dataset eDsim whose distribution best
approximates the true background p(x|y = 0), and then use standard SALAD with eDsim
and D. Note that this approach both improves sample complexity and can “suppress” a
simulation that on its own has high w, while the approach of learning k weight functions
would not oﬀer such improvements. In Algorithm 2 and appendix C.1, we write this proce-
dure out where we simply concatenate all Dsim
i
together. However, with domain knowledge
on the strengths and weaknesses of each simulation across features, one could produce eDsim
by sampling accordingly from each. We leave this direction for future work.
4.2
Theoretical results
We now present a ﬁnite sample generalization error bound on Multi-SALAD that also
applies to SALAD. To measure the generalization error, recall w(x, m) =
p(x,m|y=0)
psim(x,m|y=0)
and let ˆw be the classiﬁer g’s estimate. We denote h as the reweighted classiﬁer. Let
h⋆= argminh∈H LS(h, w) and let ˆh = argminh∈H ˆLS(h, ˆw). We aim to bound LS(ˆh, ˆw) −
LS(h⋆, w).
We ﬁrst set up some deﬁnitions. Deﬁne nSR as the number of points from D and
eDsim belonging to the signal region, and nSB as the number of points belonging to the
sideband. Let nSR
sim be the number of points in eDsim belonging to the signal region. Let
ˆg(x) ∈[ˆgmin, ˆgmax] and g⋆(x) ∈[g⋆
min, g⋆
max], where g⋆is the optimal classiﬁer. Let RnSR(ℓS◦
{H, G}) be the Rademacher complexity of the overall loss LS(h, w) across function classes
h ∈H, g ∈G. Deﬁne W = maxx,m w(x, m) as the maximum ratio between the simulation
and true background.
Let B1 = max{−log h⋆(x, m), −log(1 −h⋆(x, m))} be based on
the most extreme value of h⋆(i.e. how far apart p and p(·|y = 0) can be).
Let η =
max(−log(1 −h⋆(x, m))) for x, m ∈Dsim
SR. Let RnSB(ℓ◦G) is the Rademacher complexity
of the loss function class used for learning the reweighting, where ℓis point-wise cross-
entropy. Finally, let B2 = −log(min{ˆgmin, g⋆
min}).
Theorem 2. With probability at least 1 −δ, there exists a constant c > 0 such that the
generalization error of Multi-SALAD on eDsim and D is at most
LS(ˆh, ˆw) −LS(h⋆, w) ≤2RnSR(ℓS ◦{H, G}) + (1 + WB1)
s
log 8/δ
2nSR
(4.1)
+
ηnSR
sim
(1 −ˆgmax)(1 −g⋆max)nSR
 
4cRnSB(ℓ◦G) + 2c
s
log 4/δ
2nSB + B2
s
log 8/δ
2nSR
sim
!
.
We make several observations about this bound:
• The bound scales in (nSB)−1/2 and (nSR
sim)−1/2, where the former comes from the
initial reweighting step while the latter comes from the weighted classiﬁcation step.
• The bound is also dependent on the Rademacher complexities of both classiﬁers g
and h used.
– 11 –

JHEP07(2023)188
• The bound depends on the diﬀerence between the simulation and data distributions
through quantities W, B1, B2, η, ˆgmax, gmax. If the distributions have very diﬀerent
densities, these quantities will all be large, increasing the generalization error.
We comment how this bound is diﬀerent when instantiated for SALAD versus Multi-
SALAD. The following example shows how SALAD with one simulation can result in a
large W (and other large constants), while Multi-SALAD with two simulations combined
can reduce W in the bound.
Example 1. Let P1
sim(x|y = 0) = N(µ, σ2), P2
sim(x|y = 0) = N(−µ, σ2) be Gaussian
distributions on x with µ, σ2 ∈R, and let the true background distribution P(·|y = 0) be a
mixture of the Gaussians on x, P(x|y = 0) = 1
2P1
sim+ 1
2P2
sim. Let P1
sim, P2
sim, and P have the
same marginal distribution over m with x ⊥⊥m|y. Then, if we only use one simulation P1
sim,
w(x, m) =
p(x, m|y = 0)
p1
sim(x, m|y = 0) =
p(x|y = 0)
p1
sim(x|y = 0)
=
1
2σ
√
2π exp

−(x−µ)2
2σ2

+
1
2σ
√
2π exp

−(x+µ)2
2σ2

1
σ
√
2π exp

−(x−µ)2
2σ2

= 1
2 + 1
2 exp
(x −µ)2
2σ2
−(x + µ)2
2σ2

= 1
2 + 1
2 exp
−2xµ
σ2

.
Therefore, as x →−∞, W →∞. However, if we deﬁne Psim as the distribution of
the two simulation datasets concatenated, we have that psim(x|y = 0) = p(x|y = 0), and as
a result, W →1, making the generalization error bound smaller.
From this example, we can see that signiﬁcantly diﬀering simulation and data dis-
tributions can result in large, unbounded weight ratios, which are correlated with poor
performance.6
This concretely motivates our algorithmic objective to combine multiple
simulation datasets as to closely approximate the true data.
4.3
Empirical results
To demonstrate how Multi-SALAD can improve over using only one simulation and
over using simulations separately, we consider a synthetic experiment with two simulation
datasets.7 The true background is P(·|y = 0) = 1
2N(−1, 0.2)+ 1
2N(1, 0.2), and the anomaly
is P(·|y = 1) = 1
2N(−1.5, 0.2)+ 1
2N(1.5, 0.2). Simulation 1 is P1
sim = 1
2N(1, 0.2)+ 1
2N(0, 1),
and simulation 2 is P2
sim = 1
2N(−1, 0.2)+ 1
2N(0, 1). We generate 2000 points from the true
background and 100 points that are anomalies to form D, and 2000 points each from P1
sim
and P2
sim to form Dsim
1
and Dsim
2
. We construct signal and sideband regions from these
6The bound in Theorem 2 is meant to provide a general understanding of SALAD’s performance. It
can be made tighter by replacing terms that are maxima like M and B2 with terms that are based on
the overall data distributions (e.g. variance, as in ref. [43]). Variance-based bounds are less likely to be
vacuous, but will still demonstrate how performance is dependent on the intrinsic diﬀerences between the
two distributions.
7We ﬁnd that the diﬀerences between the simulations in the LHC Olympics are not enough to see a
noticeable gain from Multi-SALAD over SALAD.
– 12 –

JHEP07(2023)188
4
2
0
2
4
x
0
200
400
600
800
counts
Data (no anomaly)
Anomaly
Simulation 1
Simulation 2
Figure 3. Synthetic data for evaluating Multi-SALAD.
by splitting datasets in half randomly, assuming they follow the same distribution over x
(i.e., m is independent of x) except that there is no anomaly in the sideband regions. A
visualization is shown in ﬁgure 3.
Intuitively, the anomaly is only slightly diﬀerent from the background data, which
makes it important to learn a good reweighting function from the simulations. Because each
simulation alone diverges greatly from the data for one mode, each individual reweighting
may not approximate the true P(·|y = 1) well. On the other hand, if we combine both
simulation datasets together, the aggregate distribution has smaller weights with lower
variance, which can allow for more accurate reweighting. This is demonstrated in ﬁgure 4,
which depicts the reweighting in the sideband region. We have plotted the true density of
the data distribution in the sideband region, as well as the simulation’s distribution and
the reweighted simulation’s distribution. In red, we plot the absolute value of the diﬀerence
in the densities between the true data and the reweighted simulation. We ﬁnd that the
mean diﬀerence when using simulation 1 only is 0.1150, when using simulation 2 only is
0.1027, and when using simulation 1 and 2 is 0.0640.
Figure 5 depicts the reweighting’s interpolation into the signal region, where we in-
troduce an additional baseline SALAD-Switch, which uses k separate weight functions
wi(x, m) and switches among them in the reweighted loss function LS. We have plotted the
true density of the data distribution in the signal region, which consists of both background
and anomaly, and the reweighted estimate of the background data. In red, we plot the
absolute value of the diﬀerence in the densities between the true data and the reweighted
simulation.
We ﬁnd that the absolute diﬀerence in the densities is lower in regions of
no anomaly (e.g., away from N(−1.5, 0.2) and N(1.5, 0.2)) when using Multi-SALAD.
Note that the reweighting does not remove discrepancies caused by the signal. Therefore,
– 13 –

JHEP07(2023)188
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Density
Data (no anomaly)
Simulation 1
Simulation 1 + SALAD
1.5
1.0
0.5
0.0
0.5
1.0
1.5
x (SB)
0.0
0.5
1.0
|true prob - reweighted|
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Density
Data (no anomaly)
Simulation 2
Simulation 2 + SALAD
1.5
1.0
0.5
0.0
0.5
1.0
1.5
x (SB)
0.0
0.5
1.0
|true prob - reweighted|
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Density
Data (no anomaly)
Simulation [1, 2]
Simulation [1, 2] + SALAD
1.5
1.0
0.5
0.0
0.5
1.0
1.5
x (SB)
0.0
0.5
1.0
|true prob - reweighted|
Figure 4. Top left: SALAD reweighting using simulation 1 on sideband region. Top right: reweight-
ing using simulation 2. Bottom: reweighting using simulation 1 and 2 combined.
Multi-SALAD approximates the background data well, and so a classiﬁer trained on
this reweighted simulation will be able to distinguish diﬀerences between background and
anomaly; on the other hand, a classiﬁer trained on a high-variance reweighted simulation
will more likely learn distinctions coming from poor approximation, rather than anomaly.
With these observations, we present the signal eﬃciency to rejection rate of each
method in ﬁgure 6, where we compare Multi-SALAD against SALAD using simulation
1 only, SALAD using simulation 2 only, and SALAD-Switch. Table 1 contains the accu-
racy and AUC scores for each method. Averaged over 10 random seeds, Multi-SALAD
outperforms other methods. The signal eﬃciency to rejection rate for each of the 10 runs
is available in appendix F.
5
Conclusions and outlook
We extend two resonant AD approaches to incorporate multiple reference datasets. For
Multi-CWoLa, we draw from weak supervision models to handle multiple resonant fea-
tures. For Multi-SALAD, we combine multiple simulation datasets to best approximate
the background process. Future work includes 1) exploring Multi-SALAD’s applicability
– 14 –

JHEP07(2023)188
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
Data (with anomaly)
Reweighted Sim 1
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
x (SR)
0.0
0.5
1.0
|true prob - reweighted|
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density
Data (with anomaly)
Reweighted Sim 2
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
|true prob - reweighted|
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
Data (with anomaly)
Reweighted Sim [1, 2] (SALAD-switch)
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
x (SR)
0.0
0.5
1.0
|true prob - reweighted|
0.0
0.2
0.4
0.6
0.8
1.0
Density
Data (with anomaly)
Reweighted Sim [1, 2] (Multi-SALAD)
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
x (SR)
0.0
0.5
1.0
|true prob - reweighted|
Figure 5. Top left: SALAD reweighting using simulation 1 on signal region. Top right: reweighting
using simulation 2. Bottom left: using both simulation 1 and 2 weights separately. Bottom right:
reweighting using simulation 1 and 2 combined.
0.0
0.2
0.4
0.6
0.8
1.0
Signal Efficiency (True Positive Rate)
100
101
102
103
104
105
Rejection (1/False Positive Rate)
Sim 1 SALAD
Sim 2 SALAD
Multi-SALAD
SALAD-Switch
Figure 6.
Signal eﬃciency to rejection of Multi-SALAD versus other baselines (weighted and
unweighted).
– 15 –

JHEP07(2023)188
Simulation 1
Simulation 2
Simulation 1 and 2
Method
None
SALAD
None
SALAD
None
SALAD-Switch
Multi-SALAD
Accuracy
45.6±1.9
58.3±5.7
44.7±3.1
62.1±9.8
50.0±0.0
54.8±4.7
65.7±8.2
AUC
30.2±3.8
82.8±12.0
29.3±3.2
80.9±13.1
12.6±5.1
76.5±15.2
90.2±8.3
Table 1.
Accuracy and AUC scores (%) for Multi-SALAD on two simulation datasets.
We
compare to SALAD-Switch (diﬀerent reweighting), as well as standard SALAD on individual
simulations and no reweighting. Performance is averaged over 20 random runs with one standard
deviation reported.
on real data and algorithms for sampling from simulation datasets 2) extending Multi-
CWoLa to model more complex relationships among resonant features and 3) using such
approaches together over multiple simulations and resonant features, eﬀectively utilizing
as much information as possible.
Acknowledgments
We thank David Shih and Jesse Thaler for useful discussions and comments about the
manuscript.
BN was supported by the Department of Energy, Oﬃce of Science under
contract number DE-AC02-05CH11231. FS is grateful for the support of the NSF under
CCF2106707 and the Wisconsin Alumni Research Foundation (WARF). We gratefully
acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML);
ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No.
N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and
Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE);
NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi,
BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total,
the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative
(SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare.
The U.S. Government is authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon.
Any opinions, ﬁndings, and
conclusions or recommendations expressed in this material are those of the authors and do
not necessarily reﬂect the views, policies, or endorsements, either expressed or implied, of
NIH, ONR, or the U.S. Government.
A
Appendix organization
We provide a glossary of notation in B. We provide algorithmic details for Multi-SALAD
in section C. We present additional theoretical results on Rademacher complexities and
the asymptotic behavior of SALAD in section D. In section E, we provide proofs for our
theoretical results. In section F, we provide additional experimental details.
– 16 –

JHEP07(2023)188
B
Glossary
The glossary is given in table 2.
C
Additional algorithmic details
C.1
Multi-SALAD algorithm
Multi-SALAD is described in Algorithm 2. We have simulation datasets Dsim
1
, . . . Dsim
k
,
where Dsim
i
= {(xj, mj)}nsim
j=1 and all points belong to the background (y = 0). As discussed
in section 4, we propose using these simulation datasets by aggregating them into a single
simulation dataset Dsim, whether it be by concatenating the datasets, stratiﬁed sampling
— taking a ﬁxed number of samples from each dataset — or something more advanced like
importance sampling or weighting. Then the rest of this section proceeds as follows and is
a review of the standard SALAD method.
Reweighting.
First, we learn weights to correct for the bias of the simulated background
data. We split the both simulation and true data along m to produce sets Dsim
SR, Dsim
SB and
DSR and DSB. We train a classiﬁer over Dsim
SB and DSB to distinguish between simulation
and real data in the sideband region. That is, we train a binary classiﬁer ˆg over points
(x, m, z) in the sideband where x, m is either from psim(·|y = 0) (z = 0) or p(·|y = 0)
(z = 1), where we recall that simulation data only contains y = 0, and no anomalies are
present in the sideband. Denote q as the joint density of (x, m, z). We deﬁne the weight
as the estimated likelihood ratio
ˆw(x, m) =
ˆg(x, m)
1 −ˆg(x, m) ≈q(z = 1|x, m)
q(z = 0|x, m) = q(x, m|z = 1)
q(x, m|z = 0) · q(z = 1)
q(z = 0)
= q(x, m|z = 1)
q(x, m|z = 0) =
p(x, m|y = 0)
psim(x, m|y = 0).
(C.1)
Here, we assume that q(z = 1) = q(z = 0) (i.e. balanced simulation and real dataset,
which we can always ensure by generating more or less simulation data).
Equality is
obtained in the expression above when ˆg is Bayes-optimal.
Training.
The above ˆw(x, m) is deﬁned on the sideband region. Next, we interpolate
and correct the bias of the simulation in the signal region. Let Dsim
SR be the set of simulation
data in the signal region of size nSR
sim, and let DSR be the set of true data in the signal region
of size nSR
data, for a total of nSR points. We train a classiﬁer h to distinguish between the
reweighted simulated data, which approximates true background data, and the true data.
In particular, the loss function used is
ˆLS(h, ˆw) = −1
nSR
 
X
x∈DSR
log h(x, m) +
X
x∈Dsim
SR
ˆw(x, m) log(1 −h(x, m))
!
.
(C.2)
In expectation with an optimal w, we can see that minimizing this loss is equivalent
to minimizing the cross-entropy loss on a task that distinguishes between points drawn
from p and points drawn from p(·|y = 0) in the signal region. Therefore, h can be used for
anomaly detection. The procedure is summarized in Algorithm 2.
– 17 –

JHEP07(2023)188
Symbol
Used for
x
Discriminative feature x ∈X.
m
Resonant feature vector of length k, m = [m1,...,mk] ∈Rk.
y
True unknown label y ∈Y = {0,+1}, where 0 is background and 1 is signal.
P,p
Distribution and density of data (x,m,y).
Imi
Interval along which ith resonant feature mi is thresholded to produce
signal region and sideband.
SR,SB
Signal region and sideband. For an interval Imi, SRi = {(x,m) : mi ∈Imi}
and SBi = {(x,m) : mi /∈Imi}.
f
Classiﬁer f : X →Y used for anomaly detection.
D
Unlabeled dataset D = {(xi,mi)}n
i=1 of discriminative and resonant features.
DSR,DSB
Signal region and sideband of D, DSR = D∩SR, DSB = D∩SB.
ηSR,ηSB
Mixture weights corresponding to p(y = 1|x ∈SR) and p(y = 1|x ∈SB).
It is assumed that ηSR > ηSB.
Mi(m)
Noisy membership label for the ith resonant feature, equal to 0 if x ∈DSBi
and 1 if x ∈DSRi. M(m) = M1(m),...,Mk(m).
ˆy
Weak label drawn from estimated distribution on p(y|M(m)).
θy,θi
Canonical parameters of graphical model on y,M(m) in (3.3).
θy scales with the class balance of y and θi scales with the accuracy of Mi(m).
Z
Partition function used for normalizing distribution p(y,M(m)) in (3.3).
ey, f
M(m)
y and M(m) scaled from {0,1} to {−1,1}.
αi
Accuracy parameter αi = p(Mi(m) = 1|y = 1) for the membership label
of the ith resonant feature.
ℓC
Loss function ℓC : Y ×Y →R for training classiﬁer f.
LC(f)
Expected loss on labeled data using f, LC(f) = E [ℓC(f(x),y)].
f ⋆
Optimal classiﬁer trained on inﬁnite labeled data, f ⋆= argminf∈F LC(f).
ˆLC(f)
Empirical loss on D with weak labels using f, ˆLC(f) = 1
n
Pn
i=1 ℓC(f(xi), ˆyi).
ˆf
Classiﬁer learned using Multi-CWoLa, ˆf = argminf∈F ˆLC(f).
Dsim
Simulation dataset used in standard SALAD, Dsim = {(xi,mi)}nsim
i=1 .
Has distribution Psim and density psim(·).
Dsim
SB, Dsim
SR
Dsim
SB = Dsim ∩SB, Dsim
SR = Dsim ∩SR.
w(x,m)
Density ratio between Dsim
SB and DSB used for reweighting,
w(x,m) =
p(x,m|y=0)
psim(x,m|y=0).
ˆg
Classiﬁer trained to classify Dsim
SB vs DSB, used for approximating w(x,m)
when |Dsim
SB| = |DSB|.
LS(h,w)
Cross-entropy loss function used to classify Dsim
SR reweighted with w vs DSR.
ˆh
Classiﬁer trained using LS.
Dsim
1
,...,Dsim
k
k multiple simulation datasets used in Multi-SALAD.
eDsim
Dataset aggregated from Dsim
1
,...,Dsim
k
.
nSR
nSR = |DSR|.
nSB
nSB = |DSB|.
nSR
sim
nSR
sim = |Dsim
SR|.
h⋆
The optimal classiﬁer h⋆= argminh∈H LS(h,w).
W
The maximum ratio between the simulation and true background,
W = maxx,m w(x,m).
Table 2. Glossary of variables and symbols used in this paper.
– 18 –

JHEP07(2023)188
Algorithm 2 Multi-SALAD.
1: Input: simulation datasets Dsim
1
, . . . , Dsim
k
and real dataset D.
2: Construct overall simulation dataset Dsim = Sk
i=1 Dsim
i
.
3: Split each dataset into signal region and sideband region using resonant feature m to
get {Dsim
SR, Dsim
SB} and {DSR, DSB}.
4: Learn weight ˆw(x, m) =
ˆg(x,m)
1−ˆg(x,m), where ˆg is a classiﬁer that distinguishes data DSB
from simulation Dsim
SB in the sideband region.
5: Train a new classiﬁer ˆh on the signal region to distinguish between points in DSR and
points in Dsim
SR reweighted by ˆw, using the following loss:
ˆLS(h, ˆw) = −1
nSR
 
X
x∈DSR
log h(x, m) +
X
x∈Dsim
SR
ˆw(x, m) log(1 −h(x, m))
!
.
(C.3)
6: Output: classiﬁer output ˆh(x, m), which yields a score that is thresholded for anomaly
detection.
SALAD-Switch.
Here, we formally describe the baseline SALAD-Switch.
Rather
than combining the reference datasets, we keep them separate and learning a reweighting
function on each. That is, the ith weight ratio function is wi(x, m) =
p(x,m|y=0)
psimi(x,m|y=0) for
x, m ∈Dsim
i
and i ∈[k]. The SALAD-Switch objective function is
ˆLswitch(h, ˆw) = −1
nSR
 
X
x∈DSR
log h(x, m) +
k
X
i=1
X
x∈Dsim
SR,i
ˆwi(x, m) log(1 −h(x, m))
!
. (C.4)
These two steps, learning wi and the new objective function, correspond to lines 4 and 5
in Algorithm 2.
D
Additional theoretical results
D.1
The need for 3 resonant features
We show that to identify the model (3.3), we need at least k = 3 resonant features.
Lemma 1. If k = 1 or k = 2 in model (3.3), the parameters θ1 and θ2 cannot be recovered
from the observable quantities.
Proof. The strategy we use to show that the model cannot be identiﬁed for k = 1 or k = 2
is to prove that the observable distributions P( f
M1(m), . . . , f
Mk(m)) are consistent with
multiple values of θ. We do so by direct calculation.
First, consider the case of k = 1.
Set θy = 0 for simplicity.
Then, the model is
1
Z exp(θ f
M1(m)ey). Then Z = 2 exp(θ) + 2 exp(−θ), and
P( f
M1(m) = 1) =
exp(θ) + exp(−θ)
2 exp(θ) + 2 exp(−θ) = 1
2.
Thus, any θ value produces the same observable distribution, so that we cannot identify θ.
– 19 –

JHEP07(2023)188
Next, we consider k = 2. Again, set θy = 0. The model is now
1
Z exp(θ1 f
M1(m)ey +
θ2 f
M2(m)ey). We similarly compute
Z = 2(exp(θ1 + θ2) + exp(−θ1 + θ2) + exp(θ1 −θ2) + exp(−θ1 −θ2)).
The observable distribution is now P( f
M1(m), f
M2(m)). We have that
P( f
M1(m) = 1, f
M2(m) = 1) = 1
Z (exp(θ1 + θ2) + exp(−θ1 −θ2)),
and
P( f
M1(m) = 1, f
M2(m) = −1) = 1
Z (exp(θ1 −θ2) + exp(−θ1 + θ2)).
Note that we have P( f
M1(m) = −1, f
M2(m) = −1) = P( f
M1(m) = 1, f
M2(m) = 1) and
P( f
M1(m) = −1, f
M2(m) = 1) = P( f
M1(m) = 1, f
M2(m) = −1).
As a result, we have the same distribution P( f
M1(m), f
M2(m)) for the parameters
θ1, θ2 = a, b and for θ1, θ2 = b, a, where a, b are some non-negative values. If a ̸= b, we end
up with at least two solutions that cannot be distinguished, completing the proof.
D.2
Rademacher complexity bounds
We present bounds on the Rademacher complexity Rn(F) of various models F. For all of
the F below, we obtain Rn(ℓ◦F) by computing Rn(F). These two Rademacher complex-
ities are equal when we assume that ℓis 1-Lipschitz and apply Talagrand’s lemma.
• Linear models: we deﬁne fθ(x) = θ⊤x with ∥θ∥2 ≤B and E[∥x∥2
2] ≤C2, Rn(F) ≤
BC
√n [44, Theorem 5.5].
• Two-layer feed-forward neural networks (MLPs): we deﬁne fθ(x) where θ =
(U, w) are the parameters for the weights for the two layers of an MLP. Here U ∈Rm×d
and w ∈Rm. Suppose ReLU is the activation function, ∥w∥2 ≤Bw, ∥ui∥2 ≤Bu for
all 1 ≤i ≤m, and that E[∥x∥2
2 ≤C2. Then, Rn(F) ≤2BwBuC
q
m
n [44, Theorem
5.9].
• Kernels: let k : X × X →R be a continuous symmetric function so that for
x1, . . . , xn, the matrix given by Kij = k(xi, xj) is positive semideﬁnite. The class
of kernel estimators consists of functions f(x) = Pn
i=1 αik(Xi, x).
Suppose that
P
i,j αiαjk(Xi, Xj) ≤B2; then, from [45], Rn(F) ≤2B
q
E[k(X,X)]
n
. For particular
kernels it is easy to bound the term in the numerator above. For example, we consider
the RBF kernel which has maximum one, yielding Rn(F) ≤2B
√n.
D.3
Bound on CWoLa’s generalization error
We present a result on CWoLa’s generalization error showing that for k = 1, there exists
an irreducible error due to the noise in using the resonant feature as the label.
For CWoLa, we train a classiﬁer on noisy labels. We describe this objective func-
tion as ˆLnoisy(f) =
1
n
Pn
i=1 ℓC(f(xi), ˆyi), where ˆyi is the resonant feature of the ith
sample, M1(xi), and has error p := p(M1(x) ̸= y).
The CWoLa classiﬁer is equal
– 20 –

JHEP07(2023)188
to ˆf = argminf∈F ˆLnoisy(f).
We distinguish this from the clean objective function,
ˆLclean(f) =
1
n
Pn
i=1 ℓC(f(xi), yi) over true labels, and its population-level equivalent,
Lclean(f) = E [ℓC(f(x), y)], which is equal to the LC(f) used in the main text.
Let
¯y be the ﬂipped binary value of y, so that ¯y = 1 when y = 0 (and vice-versa).
Let
ˆLﬂipped(f) = 1
n
Pn
i=1 ℓC(f(xi), ¯yi) be the empirical loss of f on ﬂipped labels.
Theorem 3. For all f ∈F, assume that |ˆLclean(f)−ˆLﬂipped(f)| ≤∆, the penalty incurred
from trying to predict the ﬂipped label ¯y rather than the true label y. Then, with probability
at least 1 −δ, the generalization error of Multi-CWoLa on D is at most
Lclean( ˆf) −Lclean(f⋆) ≤2p∆+ 4Rn(ℓ◦F) + 2
s
log 2/δ
2n
.
Proof. We can write ˆLnoisy(f) as the mixture of ˆLclean and ˆLﬂipped; for all f, we have that
ˆLnoisy(f) = pˆLﬂipped(f) + (1 −p)ˆLclean(f),
(D.1)
so that
ˆLclean(f) = ˆLnoisy(f) + p(ˆLclean(f) −ˆLﬂipped(f))
(D.2)
We can think of the expression ˆLclean(f)−ˆLﬂipped(f) as the average penalty from trying
to predict the ﬂipped label ¯y rather than the true label y. This penalty varies with the
function f; let us consider an upper bound so that for all f ∈F,
|ˆLclean(f) −ˆLﬂipped(f)| ≤∆.
Now we can apply this idea to the generalization bound. We aim to bound Lclean( ˆf)−
Lclean(f⋆). Using the exact same decomposition as in the proof of Theorem 1, we have
Lclean( ˆf)−Lclean(f⋆)
= (Lclean( ˆf) −ˆLclean( ˆf)) + (ˆLclean( ˆf) −ˆLclean(f⋆)) + (ˆLclean(f⋆) −Lclean(f⋆))
We can apply a standard Rademacher complexity bound to the ﬁrst and third terms
in parentheses as was done in Theorem 1. The middle term ˆLclean( ˆf) −ˆLclean(f⋆) can be
written as
ˆLclean( ˆf) −ˆLclean(f⋆) ≤ˆLnoisy(f) −ˆLnoisy(f⋆) + 2p∆≤2p∆.
(D.3)
and so our overall generalization bound is
Lclean( ˆf) −Lclean(f⋆) ≤2p∆+ 4Rn(ℓ◦F) + 2
s
log(2/δ)
2n
(D.4)
Note that unlike in the k ≥3 case, there is no way to reduce the 2p∆term.
– 21 –

JHEP07(2023)188
D.4
Asymptotic behavior of SALAD’s ˆLS(h, w)
Lemma
2. Assume that the reweighting function is Bayes-optimal,
meaning that
ˆw(x, m) = w(x, m). Then,
lim
nSR→∞
ˆL(h, ˆw) ∝LCE(h),
where LCE(h) = Ex,m,z′=1 [−log h(x, m)] + Ex,m,z′=0 [−log(1 −h(x, m))] is the cross en-
tropy loss on label z′ =



1
x, m ∼P
0
x, m ∼p(·|y = 0)
.
Proof. Let nSR
data be the number of points from D that belong to the signal region. Under
our assumptions, the empirical loss function can be written as
ˆL(h, ˆw) ∝−nSR
data
nSR ·
1
nSR
data
X
x∈DSR
log h(x, m)
−nSR
sim
nSR ·
1
nSR
sim
X
x∈Dsim
SR
p(x, m|y = 0)
psim(x, m|y = 0) log(1 −h(x, m)).
As nSR →∞, the ﬁrst term approaches −Pr(z′ = 1)·Ex,m∼P [log h(x, m)] = −Pr(z′ =
1)·Ex,m|z′=1 [log h(x, m)]. For the second term, we can construct nSR,0
data , the amount of data
where x is from p(·|y = 0), to be equal to nSR
sim such that the expression asymptotically
approaches −Pr(z′ = 0) · Ex,m∼Psim
h
p(x,m|y=0)
psim(x,m|y=0) log(1 −h(x, m))
i
. Performing a change
of expectation, this is equal to −Pr(z′ = 0) · Ex,m|z′=0 [log(1 −h(x, m))].
Putting this
together, we have that
lim
nSR→∞
ˆL(h, ˆw) ∝−Pr(z′ = 1)Ex,m|z′=1 [log h(x, m)] −Pr(z′ = 0)Ex,m|z′=0 [log(1 −h(x, m))]
= LCE(h).
E
Proofs
E.1
Proof of Theorem 1
Proof. From Theorem 3 of [31], we have that LC( ˆf)−LC(f⋆) is bounded by the traditional
ERM generalization gap of LC( ¯f) −LC(f⋆), where ¯f = argminf∈F
1
n
Pn
i=1ℓ(f(xi, mi), yi)
is the classiﬁer learned on labeled data, plus the term
c1
emina5
min
q
k
n + c2k
√n

.
We can apply standard learning theory bounds on LC( ¯f) −LC(f⋆). In particular, this
quantity is equal to
LC( ¯f) −LC(f⋆) = (LC( ¯f) −ˆLC( ¯f)) + (ˆLC( ¯f) −ˆLC(f⋆)) + (ˆLC(f⋆) −LC(f⋆))
≤LC( ¯f) −ˆLC( ¯f) + ˆLC(f⋆) −LC(f⋆)
≤2 sup
f∈F
|LC(f) −ˆLC(f)|,
– 22 –

JHEP07(2023)188
where we have used the fact that ˆLC( ¯f) ≤ˆLC(f⋆). Then, using Theorem 3.3 of [46], we
have that with probability 1 −δ,
LC( ¯f) −LC(f⋆) ≤2
 
2Rn(ℓ◦F) +
s
log 2/δ
2n
!
.
Combining these two terms gives us our desired result.
E.2
Proof of Theorem 2
Proof. We deﬁne the true (cross-entropy) loss as
LS(h, w)= −Pr(z′ = 1)Ez′=1 [log h(x, m)] −Pr(z′ = 0)Ex,m∈PSR
sim [w(x, m) log(1 −h(x, m))] ,
where z′ = 1 for x, m ∼P and 0 for x, m ∼P(·|y = 0). Next, deﬁne w(x, m) = q(x,m|z=1)
q(x,m|z=0)
and let ˆw be the weight ratio learned by our model. Let ˆh = argminh∈H ˆLS(h, ˆw), and let
h⋆= argminh∈H L(h, w⋆). Intuitively, h⋆corresponds to the true diﬀerence between PSR
data
and PSR
data(·|y = 0). We can ﬁrst decompose the generalization error as
LS(ˆh, ˆw) −LS(h⋆, w) = [LS(ˆh, ˆw) −ˆLS(ˆh, ˆw)] + [ˆLS(ˆh, ˆw) −ˆLS(h⋆, ˆw)]
(E.1)
+ [ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)] + [ˆLS(h⋆, w) −LS(h⋆, w)].
(E.2)
We know that ˆLS(ˆh, ˆw) ≤ˆLS(h⋆, ˆw), so
LS(ˆh, ˆw) −LS(h⋆, w) ≤|LS(ˆh, ˆw) −ˆLS(ˆh, ˆw)| + |ˆLS(h⋆, w) −LS(h⋆, w)|
+ ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)
≤sup
h,w
|LS(h, w) −ˆLS(h, w)| + |ˆLS(h⋆, w) −LS(h⋆, w)|
+ ˆLS(h⋆, ˆw) −ˆLS(h⋆, w).
We ﬁrst bound suph,w |LS(h, w) −ˆLS(h, w)|.
For notation, we rewrite LS(h, w) as
LS(h, g), where w(x, m) =
g(x,m)
1−g(x,m) and g belongs to some function class G.
Then,
using Theorem 3.3 from [46], we get that suph,w |LS(h, w) −ˆLS(h, w)| ≤2RnSR(ℓS ◦
{H, G}) +
q
log 1/δ
2nSR with probability at least 1 −δ, where ℓS ◦{H, G} is deﬁned as satisfying
ℓS(h(x, m), g(x, m), y) = −y log h(x, m) −(1 −y)
g(x,m)
1−g(x,m) log(1 −h(x, m)) for h ∈H, g ∈G.
Next, we bound |ˆLS(h⋆, w)−LS(h⋆, w)|. Let W = max w(x, m) < ∞be the maximum
density ratio, and let B1 = maxx,m{−log h⋆(x, m), −log(1 −h⋆(x, m))}.
Assume that
B1 < ∞. We can apply standard concentration inequalities here (Hoeﬀding) to get that
|ˆLS(h⋆, w) −LS(h⋆, w)| ≤WB1
q
log 2/δ
2nSR with probability at least 1 −δ.
Finally, we bound ˆLS(h⋆, ˆw) −ˆLS(h⋆, w). We can write ˆLS(h⋆, ˆw) −ˆLS(h⋆, w) as
ˆLS(h⋆, ˆw) −ˆLS(h⋆, w) =
1
nSR
X
x∈Dsim
SR
( ˆw(x, m) −w(x, m)) · (−log(1 −h⋆(x, m))).
(E.3)
– 23 –

JHEP07(2023)188
Deﬁne η = max(−log(1 −h⋆(x, m))) ≥0 for x, m ∈Dsim
SR, which is small as long as
h⋆(x, m) suﬃciently classiﬁes x and is hence a property of how separated the reweighted
simulation and true data is. Then,
|ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)| ≤
η
nSR
X
x,m∈Dsim
SR
| ˆw(x, m) −w(x, m)|.
(E.4)
Recall that ˆw(x, m) =
ˆg(x,m)
1−ˆg(x,m) and w(x, m) =
g⋆(x,m)
1−g⋆(x,m) where g⋆(x, m) = Pr(z =
1|x, m), so | ˆw(x, m) −w(x, m)| =
|ˆg(x,m)−g⋆(x,m)|
(1−ˆg(x,m))(1−g⋆(x,m)). This denominator is greater than
(1 −ˆgmax)(1 −g⋆
max). Then,
|ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)| ≤
η
(1 −ˆgmax)(1 −g⋆max)nSR
X
x,m∈Dsim
SR
|ˆg(x, m) −g⋆(x, m)|.
(E.5)
We now look at the classiﬁer for training g.
The per-point cross entropy loss for
(x, m, z) is ℓ(g(x, m), z) = −log g(x, m) for z = 1 and −log(1−g(x, m)) for z = 0. WLOG,
assume for some x and m, g⋆(x, m) > ˆg(x, m).
Then |ℓ(g⋆(x, m), 1) −ℓ(ˆg(x, m), 1)| =
log g⋆(x,m)
ˆg(x,m) = log

1 +
 g⋆(x,m)
ˆg(x,m) −1

≥
g⋆(x,m)/ˆg(x,m)−1
g⋆(x,m)/ˆg(x,m)
=
g⋆(x,m)−ˆg(x,m)
g⋆(x,m)
≥|g⋆(x, m) −
ˆg(x, m)| and |ℓ(g⋆(x, m), 0) −ℓ(ˆg(x, m), 0)| = log 1−ˆg(x,m)
1−g⋆(x,m) = log

1 +
 1−ˆg(x,m)
1−g⋆(x,m) −1

≥
(1−ˆg(x,m))/(1−g⋆(x,m))−1
(1−ˆg(x,m))/(1−g⋆(x,m))
= g⋆(x,m)−ˆg(x,m)
1−ˆg(x,m)
≥|g⋆(x, m)−ˆg(x, m)|, where we use the inequality
log(1 + x) ≥
x
1+x for x > −1. Therefore, with probability 1 −δ,
|ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)| ≤
η
(1 −ˆgmax)(1 −g⋆max)nSR
X
x,m∈SR
|ℓ(ˆg(x, m), z) −ℓ(g⋆(x, m), z)|
≤
ηnSR
sim
(1 −ˆgmax)(1 −g⋆max)nSR
 
E [|ℓ(ˆg(x, m), z) −ℓ(g⋆(x, m), z)|] + B2
s
log 2/δ
2nSR
sim
!
,
where B2 = maxx,y{ℓ(ˆg(x, m), z), ℓ(g⋆(x, m), z)} = −log(min{ˆgmin, g⋆
min}).
We assume
that B2 is ﬁnite, so there exists a constant c such that
|ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)| ≤
ηnSR
sim
(1 −ˆgmax)(1 −g⋆max)nSR
 
c|L(ˆg) −L(g⋆)| + B2
s
log 2/δ
2nSR
sim
!
,
where L(g) = Ex,m∈SR [ℓ(g(x, m), z)]. Since g⋆(x, m) is Bayes optimal, |L(ˆg) −L(g⋆)| =
L(ˆg) −L(g⋆) = L(ˆg) −ˆL(ˆg) + ˆL(ˆg) −ˆL(g⋆) + ˆL(g⋆) −L(g⋆) ≤2 supg∈G |L(g) −ˆL(g)|. From
Theorem 3.3 in [46], this is bounded by 2RnSB(ℓ◦G) +
q
log 1/δ
2nSB with probability at least
1 −δ. Then, applying a union bound, with probability 1 −δ, we have
|ˆLS(h⋆, ˆw) −ˆLS(h⋆, w)|
≤
ηnSR
sim
(1 −ˆgmax)(1 −g⋆max)nSR
 
4cRnSB(ℓ◦G) + 2c
s
log 2/δ
2nSB + B2
s
log 4/δ
2nSR
sim
!
.
– 24 –

JHEP07(2023)188
3.00
3.25
3.50
3.75
4.00
TeV
0
1000
2000
3000
4000
5000
6000
Num. events
Invariant mass of two jets
anomaly
background
0.0
0.2
0.4
0.6
TeV
0
2500
5000
7500
10000
12500
15000
17500
Jet mass of lighter jet
anomaly
background
0.00
0.25
0.50
0.75
1.00
TeV
0
2500
5000
7500
10000
12500
15000
Difference in jet mass
anomaly
background
0.00
0.25
0.50
0.75
1.00
Ratio t2/t1
0
2000
4000
6000
8000
Num. events
N-subjettiness ratio of lighter jet
anomaly
background
0.00
0.25
0.50
0.75
1.00
Ratio t2/t1
0
1000
2000
3000
4000
5000
6000
N-subjettiness ratio of heavier jet
anomaly
background
Figure 7. Distribution of each feature (background vs anomaly) for the Multi-CWoLa experi-
ments. The ﬁrst three features are used in Multi-CWoLa as resonant features while the latter
two are used as discriminative features. The ﬁrst feature (invariant mass of the dijet) is used as the
resonant feature in CWoLa.
Putting everything together with another union bound, with probability 1 −δ, the
generalization error is at most
LS(ˆh, ˆw)−LS(h⋆,w) ≤2RnSR(ℓS ◦{H,G})+(1+WB1)
s
log8/δ
2nSR
(E.6)
+
ηnSR
sim
(1−ˆgmax)(1−g⋆max)nSR
 
4cRnSB(ℓ◦G)+2c
s
log4/δ
2nSB +B2
s
log8/δ
2nSR
sim
!
,
(E.7)
where we combine like terms
q
log(4/δ)
2nSR
and WB1
q
log(8/δ)
2nSR
into being upper bounded by
(1 + WB1)
q
log(8/δ)
2nSR .
F
Experiment details
F.1
Multi-CWoLa experiments
For the Multi-CWoLa experiment, we used the anomaly and simulation data from the
Pythia 8 simulations in the LHC Olympics Dataset to create an unlabeled dataset we
want to perform anomaly detection on [3]. We have k = 3, and construct Mi(m) based
on the thresholds [[3.3, 3.7], [0.09, 0.13], [0.3, 0.35]] on the ﬁrst three features. For standard
CWoLa, only the ﬁrst feature is regarded as the resonant feature, and it is thresholded
with the interval [3.3, 3.7] (see ﬁgure 7). We constructed training datasets of varying sizes
– 25 –

JHEP07(2023)188
Method
SALAD 1
SALAD 2
SALAD-Switch
Multi-SALAD
AUC
87.5 ± 9.7
72.4 ± 18.9
93.8 ± 2.1
94.6 ± 0.9
Table 3. AUC for imbalanced classiﬁer, averaged over 5 runs.
with class balance Pr(y = 1) = 0.149. We used one test dataset with 65755 randomly
sampled anomaly points and 161658 randomly sampled background points.
All methods were trained using scikit-learn’s MLPClassiﬁer with max_iter=5000. For
Multi-CWoLa’s weak supervision step, we learn the parameters of the graphical model
using SGD and PyTorch [47] with 30000 steps and learning rate = 1e −6. We do not
assume the class balance Pr(y = 1) = 0.149 is known, and instead set a prior estimate
ˆp(y = 1) = 0.25 to be used in the algorithm.
F.2
Multi-SALAD experiments
Setup.
We use MLPs from Keras [48], each with 3 hidden layers of dimension 32, ReLu
activation, and trained with cross-entropy loss and the Adam optimizer. We train for 50
epochs, batch size 200, and default parameters otherwise. Finally, we evaluate our approach
on a new test set containing 200000 background points and 200000 anomaly points. This
test set is used to produce the signal eﬃciency to rejection rate. All experiments were run
on a personal laptop.
Additional results.
In ﬁgure 8, we show our results on individual runs. This is because
computing the conﬁdence intervals of these curves averaged across the 10 random runs is
too noisy due to the magnitude of the reciprocal 1/FPR.
Unbalanced data.
When the simulation and true dataset are imbalanced, we adjust
our reweighting by q(z=1)
q(z=0) in (C.1), the ratio of real to simulated data. To examine our
approach in this setting, we use our synthetic experiment with 1000 points from the true
background, 50 points that are anomalies, and 5000 points each for Dsim
1
and Dsim
2
. Note
that the ratio of anomalies in the true data is the same as our original setting, but we
have increased the amount of simulation data by 5 times and adjust weights by 1
5. Table 3
reports results over 5 random runs.
– 26 –

JHEP07(2023)188
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
0.0
0.2
0.4
0.6
0.8
1.0
TPR (Signal Efficiency)
100
101
102
103
104
105
1/FPR (Background Rejection)
Sim 1 SALAD
Sim 2 SALAD
SALAD-Switch
Multi-SALAD
Figure 8. Results on individual runs.
– 27 –

JHEP07(2023)188
Open Access.
This article is distributed under the terms of the Creative Commons
Attribution License (CC-BY 4.0), which permits any use, distribution and reproduction in
any medium, provided the original author(s) and source are credited.
References
[1] H.E.P.M.L. community, A living review of machine learning for particle physics,
https://iml-wg.github.io/HEPML-LivingReview/.
[2] G. Karagiorgi et al., Machine learning in the search for new fundamental physics,
arXiv:2112.03769 [INSPIRE].
[3] G. Kasieczka et al., The LHC olympics 2020 a community challenge for anomaly detection in
high energy physics, Rept. Prog. Phys. 84 (2021) 124201 [arXiv:2101.08320] [INSPIRE].
[4] T. Aarrestad et al., The dark machines anomaly score challenge: benchmark data and model
independent event classiﬁcation for the Large Hadron Collider, SciPost Phys. 12 (2022) 043
[arXiv:2105.14027] [INSPIRE].
[5] ATLAS collaboration, Dijet resonance search with weak supervision using √s = 13 TeV pp
collisions in the ATLAS detector, Phys. Rev. Lett. 125 (2020) 131801 [arXiv:2005.02983]
[INSPIRE].
[6] ATLAS Collaboration collaboration, Anomaly detection search for new resonances
decaying into a Higgs boson and a generic new particle X in hadronic ﬁnal states using
√s = 13 TeV pp collisions with the ATLAS detector, ATLAS-CONF-2022-045, CERN,
Geneva, Switzerland (2022).
[7] G. Kasieczka et al., Anomaly detection under coordinate transformations, Phys. Rev. D 107
(2023) 015009 [arXiv:2209.06225] [INSPIRE].
[8] G. Kasieczka, B. Nachman and D. Shih, New methods and datasets for group anomaly
detection from fundamental physics, in the proceedings of the Conference on knowledge
discovery and data mining, (2021) [arXiv:2107.02821] [INSPIRE].
[9] J.H. Collins, K. Howe and B. Nachman, Anomaly detection for resonant new physics with
machine learning, Phys. Rev. Lett. 121 (2018) 241803 [arXiv:1805.02664] [INSPIRE].
[10] J.H. Collins, K. Howe and B. Nachman, Extending the search for new resonances with
machine learning, Phys. Rev. D 99 (2019) 014038 [arXiv:1902.02634] [INSPIRE].
[11] R.T. D’Agnolo and A. Wulzer, Learning new physics from a machine, Phys. Rev. D 99
(2019) 015014 [arXiv:1806.02350] [INSPIRE].
[12] R.T. D’Agnolo et al., Learning multivariate new physics, Eur. Phys. J. C 81 (2021) 89
[arXiv:1912.12155] [INSPIRE].
[13] K. Benkendorfer, L.L. Pottier and B. Nachman, Simulation-assisted decorrelation for
resonant anomaly detection, Phys. Rev. D 104 (2021) 035003 [arXiv:2009.02205] [INSPIRE].
[14] A. Andreassen, B. Nachman and D. Shih, Simulation assisted likelihood-free anomaly
detection, Phys. Rev. D 101 (2020) 095004 [arXiv:2001.05001] [INSPIRE].
[15] B. Nachman and D. Shih, Anomaly detection with density estimation, Phys. Rev. D 101
(2020) 075042 [arXiv:2001.04990] [INSPIRE].
[16] O. Amram and C.M. Suarez, Tag N’ Train: a technique to train improved classiﬁers on
unlabeled data, JHEP 01 (2021) 153 [arXiv:2002.12376] [INSPIRE].
– 28 –

JHEP07(2023)188
[17] A. Hallin et al., Classifying anomalies through outer density estimation, Phys. Rev. D 106
(2022) 055006 [arXiv:2109.00546] [INSPIRE].
[18] J.A. Raine, S. Klein, D. Sengupta and T. Golling, CURTAINs for your sliding window:
constructing unobserved regions by transforming adjacent intervals, Front. Big Data 6 (2023)
899345 [arXiv:2203.09470] [INSPIRE].
[19] R.T. d’Agnolo et al., Learning new physics from an imperfect machine, Eur. Phys. J. C 82
(2022) 275 [arXiv:2111.13633] [INSPIRE].
[20] P. Chakravarti, M. Kuusela, J. Lei and L. Wasserman, Model-independent detection of new
physics signals using interpretable semi-supervised classiﬁer tests, arXiv:2102.07679
[INSPIRE].
[21] B.M. Dillon, R. Mastandrea and B. Nachman, Self-supervised anomaly detection for new
physics, Phys. Rev. D 106 (2022) 056005 [arXiv:2205.10380] [INSPIRE].
[22] M. Letizia et al., Learning new physics eﬃciently with nonparametric methods, Eur. Phys. J.
C 82 (2022) 879 [arXiv:2204.02317] [INSPIRE].
[23] K. Krzyżańska and B. Nachman, Simulation-based anomaly detection for multileptons at the
LHC, JHEP 01 (2023) 061 [arXiv:2203.09601] [INSPIRE].
[24] S. Alvi, C.W. Bauer and B. Nachman, Quantum anomaly detection for collider physics,
JHEP 02 (2023) 220 [arXiv:2206.08391] [INSPIRE].
[25] T. Sjostrand, S. Mrenna and P.Z. Skands, A brief introduction to PYTHIA 8.1, Comput.
Phys. Commun. 178 (2008) 852 [arXiv:0710.3820] [INSPIRE].
[26] J. Bellm et al., Herwig 7.0/Herwig++ 3.0 release note, Eur. Phys. J. C 76 (2016) 196
[arXiv:1512.01178] [INSPIRE].
[27] Sherpa collaboration, Event generation with Sherpa 2.2, SciPost Phys. 7 (2019) 034
[arXiv:1905.09127] [INSPIRE].
[28] E.M. Metodiev, B. Nachman and J. Thaler, Classiﬁcation without labels: learning from
mixed samples in high energy physics, JHEP 10 (2017) 174 [arXiv:1708.02949] [INSPIRE].
[29] J. Neyman and E.S. Pearson, On the problem of the most eﬃcient tests of statistical
hypotheses, Phil. Trans. Roy. Soc. Lond. A 231 (1933) 289 [INSPIRE].
[30] A. Ratner et al., Snorkel: rapid training data creation with weak supervision, in the
Proceedings of the of the 44th international conference on Very Large Data Bases (VLDB),
Rio de Janeiro, Brazil (2018).
[31] D.Y. Fu et al., Fast and three-rious: speeding up weak supervision with triplet methods, in
International conference on machine learning, (2020) [arXiv:2002.11955].
[32] A. Ratner et al., Data programming: creating large training sets, quickly, in the Proceedings
of the of the 30th International Conference on Neural Information Processing Systems, Red
Hook, NY, U.S.A. (2016), p. 3574.
[33] A. Ratner et al., Training complex models with multi-task weak supervision, in the
Proceedings of the of the AAAI Conference on Artiﬁcial Intelligence, (2019).
[34] P. Varma et al., Learning dependency structures for weak supervision models, in the
Proceedings of the of the 36th International Conference on Machine Learning, (2019).
[35] M.J. Wainwright and M.I. Jordan, Graphical models, exponential families, and variational
inference, Foundations and Trends® in Machine Learning 1 (2007) 1.
– 29 –

JHEP07(2023)188
[36] J. Thaler and K. Van Tilburg, Identifying boosted objects with N-subjettiness, JHEP 03
(2011) 015 [arXiv:1011.2268] [INSPIRE].
[37] J. Thaler and K. Van Tilburg, Maximizing boosted top identiﬁcation by minimizing
N-subjettiness, JHEP 02 (2012) 093 [arXiv:1108.2701] [INSPIRE].
[38] B. Nachman and J. Thaler, Learning from many collider events at once, Phys. Rev. D 103
(2021) 116013 [arXiv:2101.07263] [INSPIRE].
[39] T. Hastie, R. Tibshirani and J. Friedman, The elements of statistical learning, Springer, New
York, NY, U.S.A. (2009) [DOI:10.1007/978-0-387-84858-7].
[40] M. Sugiyama, T. Suzuki and T. Kanamori, Density ratio estimation in machine learning,
Cambridge University Press, Cambridge, U.K. (2012) [DOI:10.1017/cbo9781139035613].
[41] K. Cranmer, J. Pavez and G. Louppe, Approximating likelihood ratios with calibrated
discriminative classiﬁers, arXiv:1506.02169 [INSPIRE].
[42] S. Dasgupta and P.M. Long, Boosting with diverse base classiﬁers, in the Proceedings of the
Learning Theory and Kernel Machines, Berlin, Heidelberg, Germany (2003), p. 273.
[43] C. Cortes, Y. Mansour and M. Mohri, Learning bounds for importance weighting, in the
Proceedings of the Advances in Neural Information Processing Systems, Curran Associates
Inc. (2010).
[44] T. Ma, Lecture notes for machine learning theory (CS229M/STATS214),
https://web.stanford.edu/class/stats214/ June 2022
[45] P.L. Bartlett and S. Mendelson, Rademacher and Gaussian complexities: risk bounds and
structural results, J. Mach. Learn. Res. 3 (2002) 463.
[46] M. Mohri, A. Rostamizadeh and A. Talwalkar, Foundations of machine learning, MIT Press,
Cambridge, MA, U.S.A. (2018).
[47] A. Paszke et al., PyTorch: an imperative style, high-performance deep learning library, in
Advances in neural information processing systems 32, Curran Associates Inc. (2019),
p. 8024.
[48] F. Chollet et al., Keras, https://github.com/fchollet/keras.
– 30 –

