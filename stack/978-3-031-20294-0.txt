Synthese Library
Studies in Epistemology, Logic, Methodology,
and Philosophy of Science
469
Antonio Piccolomini d’Aragona
Prawitz’s 
Epistemic 
Grounding
An Investigation into the Power 
of Deduction

Synthese Library
Studies in Epistemology, Logic, Methodology,
and Philosophy of Science
Volume 469
Editor-in-Chief
Otávio Bueno, Department of Philosophy, University of Miami, Coral Gables, USA
Editorial Board Members
Berit Brogaard, University of Miami, Coral Gables, USA
Anjan Chakravartty, Department of Philosophy, University of Miami, Coral Gables,
FL, USA
Steven French, University of Leeds, Leeds, UK
Catarina Dutilh Novaes, VU Amsterdam, Amsterdam, The Netherlands
Darrell P. Rowbottom, Department of Philosophy, Lingnan University, Tuen Mun,
Hong Kong
Emma Ruttkamp, Department of Philosophy, University of South Africa, Pretoria,
South Africa
Kristie Miller, Department of Philosophy, Centre for Time, University of Sydney,
Sydney, Australia

The aim of Synthese Library is to provide a forum for the best current work in
the methodology and philosophy of science and in epistemology, all broadly under-
stood. A wide variety of different approaches have traditionally been represented in
the Library, and every effort is made to maintain this variety, not for its own sake,
but because we believe that there are many fruitful and illuminating approaches to
the philosophy of science and related disciplines.
Special attention is paid to methodological studies which illustrate the interplay
of empirical and philosophical viewpoints and to contributions to the formal
(logical, set-theoretical, mathematical, information-theoretical,decision-theoretical,
etc.) methodology of empirical sciences. Likewise, the applications of logical
methods to epistemology as well as philosophically and methodologically relevant
studies in logic are strongly encouraged. The emphasis on logic will be tempered
by interest in the psychological, historical, and sociological aspects of science. In
addition to monographs Synthese Library publishes thematically uniﬁed anthologies
and edited volumes with a well-deﬁned topical focus inside the aim and scope of
the book series. The contributions in the volumes are expected to be focused and
structurally organized in accordance with the central theme(s), and should be tied
together by an extensive editorial introduction or set of introductions if the volume
is divided into parts. An extensive bibliography and index are mandatory.

Antonio Piccolomini d’Aragona
Prawitz’s Epistemic
Grounding
An Investigation into the Power of Deduction

Antonio Piccolomini d’Aragona
Institute of Philosophy
Czech Academy of Sciences
Prague, Czech
Aix-Marseille University
CNRS, Centre Granger
Aix-en-Provence, France
ISSN 0166-6991
ISSN 2542-8292
(electronic)
Synthese Library
ISBN 978-3-031-20293-3
ISBN 978-3-031-20294-0
(eBook)
https://doi.org/10.1007/978-3-031-20294-0
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland
AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To Barbara

Contents
1
Introduction ..................................................................
1
Part I
The Idea of Epistemic Grounding
2
From Models to Evidence ...................................................
11
2.1
Nature of Inferences ....................................................
11
2.2
Valid Inferences .........................................................
14
2.2.1
Epistemic Compulsion .........................................
14
2.2.2
Justiﬁcation .....................................................
17
2.2.3
Proofs............................................................
19
2.3
The Fundamental Task ..................................................
21
2.4
Inference and Consequence.............................................
22
2.5
Model Theory ...........................................................
24
2.6
Meaning: From Truth to Evidence .....................................
28
3
Valid Arguments and Proofs ................................................
33
3.1
Prawitz’s Normalization Theory .......................................
33
3.2
Semantics Through Arguments and Proofs ............................
40
3.2.1
Valid Arguments (in 1973) .....................................
42
3.2.2
Proofs (in 1977 and 2005)......................................
46
3.3
Three Problems..........................................................
51
3.3.1
Proofs-as-Chains................................................
51
3.3.2
The Recognizability Problem ..................................
56
3.3.3
Validity as Independent from Inferences ......................
59
3.4
Building and Computing................................................
61
4
Prawitz’s Theory of Grounds ...............................................
67
4.1
From Inferences to Proofs, via Grounds ...............................
67
4.1.1
Inferences in the Theory of Grounds ..........................
68
4.1.2
Evidence as the Aim of Reﬂective Inferences .................
74
4.1.3
Prawitz’s Notion of Ground ....................................
85
4.1.4
Inference Acts and Validity .................................... 100
vii

viii
Contents
4.1.5
Advancements and Open Issues................................ 104
4.2
Towards a Formal Approach to Grounds .............................. 117
4.2.1
The Curry-Howard Isomorphism .............................. 118
4.2.2
Constructions and Translations ................................ 121
Part II
Formal Epistemic Grounding
5
Languages of Grounding .................................................... 129
5.1
General Overview ....................................................... 129
5.1.1
From Grounds to Terms, Through Denotation ................ 130
5.1.2
Basic Notions for Languages................................... 142
5.2
A Class of Languages................................................... 150
5.2.1
Background Language and Bases .............................. 150
5.2.2
A Universe of (Operations on) Grounds ....................... 153
5.2.3
Languages, Expansions and Some Examples ................. 167
5.2.4
Denotation....................................................... 173
5.2.5
Primitiveness and Conservativity .............................. 185
6
Systems of Grounding ....................................................... 191
6.1
General Overview ....................................................... 191
6.1.1
Denotation and Identity......................................... 191
6.1.2
Aims and Outcomes of a Deductive Approach................ 197
6.1.3
Three Kinds of Theorems ...................................... 202
6.2
Deduction Over the Gentzen-Language................................ 205
6.2.1
An Enriched Gentzen-Language ............................... 205
6.2.2
A System for the Enriched Gentzen-Language................ 213
6.2.3
Some Theorems Within the System............................ 219
6.2.4
A Theorem About the System.................................. 224
6.3
A Class of Systems ..................................................... 226
6.3.1
Invariant and Characteristic Rules ............................. 226
6.3.2
Normalization ................................................... 228
7
Completeness and Recognizability ......................................... 247
7.1
About Completeness .................................................... 247
7.1.1
From Validity to Universal Validity............................ 247
7.1.2
Correctness of First-Order Intuitionistic Logic................ 250
7.1.3
Accounts of Ground-Theoretic Completeness ................ 250
7.1.4
Incompleteness of Intuitionistic Logic......................... 255
7.2
Recognizability and Equations ......................................... 261
7.2.1
Local and Global Recognizability ............................. 262
7.2.2
Parameters and Structure of Equations ........................ 264
8
Conclusion .................................................................... 271
Bibliography ...................................................................... 277

Chapter 1
Introduction
According to a rather widespread interpretation, logic is to be understood as the
science of correct reasoning. Far from being a deﬁnition, however, this expression
is of a mere indicative nature, raising more questions than it answers. Even leaving
out the problematic issue on what we can and should consider as science, it is far
from clear what a reasoning is, and hence what a correct reasoning is.
From a general point of view, we can follow a long and well-established tradition
according to which a reasoning is a concatenation of passages from certain premises
to certain conclusions, called inferences. This position is, for example, present in
Descartes who, in a well-known passage of his Rules for the direction of the mind,
equates reasoning to
a continuous and uninterrupted movement of thought in which each individual proposition
is clearly intuited. This is similar to the way in which we know that the last link in a long
chain is connected to the ﬁrst: even if we cannot take in at one glance all the intermediate
links on which the connection depends, we can have knowledge of the connection provided
we survey the links one after the other, and keep in mind that each link from the ﬁrst to the
last is attached to its neighbour. (Descartes, 1985, 15)
It is also obvious, however, that this idea can be further understood in many different
alternative ways; a more speciﬁc determination will vary depending on the point of
view adopted on the nature of premises and conclusions, and on the passage itself.
In relation to the narrower notion of correct reasoning, we are fortunate to have a
basic intuition to which we can hold onto, although also such intuition is, in the ﬁnal
analysis, partial and liable to many different ramiﬁcations. It can be illustrated with
the famous words that Aristotle binds to the key notion of the system developed in
the Organon:
the syllogism is a discourse in which, certain things being laid down, something follows of
necessity from them. (Aristotle, 1949, 287)
Syllogisms therefore have a feature that Aristotle emphasizes: necessity. The
question “what did Aristotle mean by necessity?” concerns the history of logic
or the history of philosophy. Much more important for what concerns us is the
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_1
1

2
1
Introduction
general question “what kind of necessity do we refer to when we talk about correct
reasoning?”. We could think that logic, when dressed as a science, has a uniform
vision on the nature of necessity. But this is by no means the case.
It would obviously be impossible, in the restricted framework of this introduc-
tion, to review the multiple reﬂections which, over the centuries, have concerned the
notion of necessity. Nor would such review be of any use with respect to the theme
of this book: Dag Prawitz’s theory of grounds. But necessity plays in Prawitz, and
it will play for us, a decisive role to say the least. The theory of grounds, in fact,
takes the form of a simultaneously philosophical and formal attempt to respond,
with sufﬁcient and satisfactory articulation, to what is perhaps the most original
among the questions of logic: how and why do some inferences, commonly called
deductively correct, have the epistemic power to force us to accept their conclusion,
assuming we have accepted their premises? If we adopt the aforementioned point
of view, that an argument is a chain of inferences, an answer to this question also
amounts to explaining how and why deductively correct reasoning can exert the
force that, since very distant times, has made it the main source of conclusive
knowledge, and a source of irrefutable epistemic certainty for human beings.
In focusing on the power of epistemic binding of deductively correct inferences
and reasoning, Prawitz emphasizes a very particular kind of necessity. In a paper
that is in many ways a watershed between his previous research and the theory of
grounds, the Swedish logician uses the ﬁtting expression of necessity of thought,
specifying that with it he means the circumstance in which
one is committed to holding α true, having accepted the truth of the sentences of ; one
is compelled to hold α true, given that one holds all the sentences of  true; on pain of
irrationality, one must accept the truth of α, having accepted the truth of the sentences of .
(Prawitz, 2005, 677)
This kind of necessity is diametrically different from another, equally well known
and perhaps more practiced in logic, which is based on the notion of possible
world, according to which necessity means truth in all possible worlds. The fact
that something is true in all possible worlds is obviously alien to issues relating to
knowledge: it could be a circumstance of which we are simply unaware or, even if
we are aware of it, we might not see why it occurs.
However problematic, the notion of possible world is often used to substantiate
the idea that model theory, the child of the pioneer works of Bolzano and
Tarski, actually captures the notion of necessity involved in validity and logical
consequence. But many criticisms have been raised against the thesis that model
theory contains modal ingredients of any kind and, given in such general terms, the
question is still widely debated. What we can say with certainty, is that the modality
captured by model theory, although present, is undoubtedly not of the type Prawitz
is interested in, that is, not of an epistemic type.
Starting from some important results in proof theory, Prawitz has thus developed
a formal semantics alternative to model theory, inscribed in the ﬁeld today known as
proof-theoretic semantics. Prawitz offers deﬁnitions of the concepts of validity and
logical consequence that, in accordance with necessity of thought, replace the notion

1
Introduction
3
of truth, as a semantic core, with those of proof or valid argument. The idea would
seem obvious: the epistemic binding that one experiences in necessity of thought
can occur only when we are in possession of a proof or of a valid argument.
Prawitz therefore provides mathematically rigorous characterizations of the
concepts of proof and valid argument, following essentially three lines of research:
the BHK semantics for ﬁrst-order intuitionist logic, originated with Heyting; some
observations by Gentzen on the relationship between introduction and elimination
rules for ﬁrst-order logical constants in natural deduction systems; and Dummett’s
investigations in theory of meaning, with a particular focus on a veriﬁcationist
theory of meaning. By setting itself at the crossroads of so many suggestions, and in
a sense harmonizing them all, Prawitz’s semantics is therefore of an interest more
widely philosophical, which goes beyond the formal results, albeit fundamental, that
it allows us to reach.
However, the approach with valid arguments and proofs is not entirely free of
problems, of which the main one, related to the issues mentioned above, concerns
precisely the possibility of accounting for the epistemic power of deductively
correct inferences and reasoning. If we intend to explain how and why a correct
reasoning can epistemically force someone to accept its conclusions, thus providing
justiﬁcation towards them, there seems to be no other choice but to make this force
depend on the analogous force enjoyed by the inferential passages occurring in the
reasoning itself. To make this explanation work, the notion of correct inference
must conceptually take priority over that of correct reasoning, thus following the
same explanatory order that makes reasoning a chain of inferences. In Prawitz’s
semantics of valid arguments and proofs, however, the notion of correct inference is
deﬁned in terms of valid arguments or proofs, by stating that an inference is correct
when it preserves provability, or the validity of the structure in which it occurs. It
is in this sense not surprising that, with the theory of grounds, Prawitz renounces a
characterization of this type, returning to correct inferences their pivotal role.
This inversion of the natural relationship among the notions of correct inference,
proof and valid argument, refers to another point on which the theory of the grounds
indubitably offers progress. To be forced to validate the correctness of judgments or
assertions that convey or express knowledge is something we experience when, for
example, we follow the steps made by someone who is proving something, or when
we personally perform a proof-act. The epistemic binding is something we “feel”,
and of which we are aware. In this “feeling”, we are not in a condition of passivity;
we must do something, carry out appropriate acts. In the words of Cozzo (2015),
necessity of thought has a phenomenal character, and assumes for us the form of an
active experience.
Strictly speaking, therefore, correct reasoning leads to a state of epistemic
justiﬁcation, but it is not itself, as such, an epistemic justiﬁcation; the proof-act
is the means by which the binding manifests itself, but it is the result of this act,
what the act leads to, that qualiﬁes as the condition of epistemic success. Subtle but
crucial, this distinction can be summed up as a dichotomy between proof-objects,
on the one hand, and proof-acts, on the other (see mainly Sundholm, 1998). In this
regard, Prawitz’s semantics seems ambiguous, since it simultaneously deals with

4
1
Introduction
proofs and valid arguments both as objects and acts, while the theory of grounds is
much more precise: here, Prawitz distinguishes between states of justiﬁcation—he
actually speaks of objects, called precisely grounds, of which we are in possession
when we are justiﬁed in judging or asserting—and acts that enable us to enter a state
of justiﬁcation—namely, proofs that produce grounds.
In the theory of grounds, as just mentioned, “ground” is the expression used by
Prawitz to indicate what we have when we are epistemically justiﬁed. Grounds are
obtained by performing inferential acts, either single, or concatenated so as to form
a proof. The most original trait of the theory of grounds is perhaps the reconstruction
that Prawitz offers of how an inferential act can produce grounds and, therefore, of
what an inferential act actually is. In the commonly accepted meaning, which we
have also referred to at the beginning of this introduction, an inference is simply
identiﬁed by certain premises and certain conclusions, since it appears as a passage
from the ones to the others. In the light of convincing arguments, however, Prawitz
believes that this reconstruction is far too poor to allow an adequate explanation of
the phenomenon of epistemic compulsion. He adds to it the crucial idea that to make
an inference means applying an operation that constructively transforms grounds
for the premises into grounds for the conclusion, and that therefore an inference, in
addition to premises and conclusions, must also involve an operation of this kind.
A deductively correct inference is therefore an inference whose operation actually
returns grounds for the conclusion, when applied to grounds for the premises.
Joining the aforementioned distinction between proof-objects and proof-acts,
and combining it with the notion of deductively correct inference, this innovative
way of characterizing inferential acts makes the theory of grounds a solid, non-
circular, and philosophically meaningful apparatus for a rigorous explanation of
the relationship between correct inferences and correct reasoning, as well as of
the epistemic power of both. Both Prawitz’s semantics of valid arguments and
proofs, and the theory of grounds, attribute a crucial role to primitive canonical
cases, in contrast to the non-canonical ones, that stand in need of justiﬁcation.
However, while in Prawitz’s semantics of valid arguments and proofs the objects
themselves, as not distinguished from the acts, can be canonical or non-canonical,
in the theory of grounds the distinction applies only to the acts; grounds are speciﬁed
solely by virtue of primitive operations, by simple induction on the complexity of
the formulas for which they are grounds. From this perspective, the fact that the
deductive correctness of the inferential acts is explained on the basis of objects of
this type, and not in relation to the acts in which these inferences occur, allows
Prawitz to overcome some problems of circularity.
Understanding the inferential acts as applications of operations on grounds also
plays an important role in relation to a problem that, this time, the theory of grounds
shares in its entirety with Prawitz’s semantics of valid arguments and proofs. If a
proof or valid argument must give justiﬁcation, it seems we cannot help but request
that it be recognizable that they have this power. In the same way, if the possession
of a ground coincides with being in a state of justiﬁcation, we need to be able
to recognize that the inference that gives possession of that ground is deductively
correct. Otherwise, the fact that we have made correct deductions would correspond

1
Introduction
5
to the possession of abstract objects, so that we could be totally unaware of their
epistemic weight; no justiﬁcation, let alone any epistemic binding, seems to be
possible under such circumstances. Similar issues are often raised in relation to the
clauses for the implication and the universal quantiﬁcation in BHK semantics. As
regards the possibility itself of arguing that certain constructs, which are ultimately
formal, are capable of respecting the wishes concerning epistemic justiﬁcation and
binding, the point is obviously crucial also and above all in Prawitz. Unfortunately,
a precise delineation of how this recognizability can occur, assuming that it actually
can, remains decidedly elusive. A good starting point could be to clarify what the
recognition in question is, but already here positions are disparate and discordant,
going from “strong” readings, in terms of decidability, to more “weak” readings,
which involve pragmatic elements.
In the two frameworks—Prawitz’s semantics of valid arguments and proofs, and
theory of grounds—the problem of recognizability arises for the non-canonical
cases; since they are not primitive in the explanation of meaning—namely, not
primitive in the determination of what counts as justiﬁcation for judgments or
assertions about propositions or sentences of different logical form—they must
be justiﬁed. And we need to be able to recognize that the justiﬁcation given
works, and fulﬁlls the task requested. From this perspective, the ground-theoretic
idea of inferences as applications of operations on grounds allows us a minimal,
albeit limited, progress. In certain speciﬁc circumstances—when the inference is
performed starting from premises for which we already have grounds—what an
agent is in possession of at the end of the inference is not something that can be
canonical or non-canonical, but an object deﬁned only by primitive operations. This
is because to perform the inference means the agent is applying an operation—that
is, they are computing it on the grounds of which they are already in possession so
as to get a ground for the conclusion. The same cannot be said of proofs or valid
arguments in the previous approach, where to make an inference only means to
ensure that a conclusion follows certain premises, and where the structure resulting
from the deductive activity is therefore susceptible to the canonical/non-canonical
distinction; when the structure is non-canonical,the fact that it is valid will be visible
only after the subsequent application of justiﬁcations that show how to reduce it to
a canonical structure. Nor, due to structural reasons, can the application of such
justiﬁcations be understood as occurring simultaneously to the completion of the
inferential passages, as instead happens in the theory of grounds.
However, the theory of grounds still suffers from a problem of recognizing
whether certain equations are well-given. Since inferences are intended as appli-
cations of operations on grounds, non-canonical inferences are to be understood
as applications of non-primitive operations on grounds. The latter are in turn
deﬁned, not only by a domain and a codomain, but also by an equation that shows
how the operation behaves. The equation provides a method of “computation”, or
“transformation”, through which, when executed, it is constructively possible to
pass from grounds for the premises—arguments of the operations—to grounds for
the conclusion—the value of the operation on those arguments. The equations are
a “functional” version of the reduction or justiﬁcation procedures of non-canonical

6
1
Introduction
inferences in Prawitz’s semantics of valid arguments and proofs. In this sense, we
could say that the theory of grounds proposes a sort of “internalization” of such
procedures to the process of construction of the argument structure. In appropriate
circumstances, to prove means “computing” reductions of non-canonical structures.
Obviously, in his writings on the theory of grounds, Prawitz does not limit
himself to just stating the ideas that, in a very general sense, we have listed so
far. On the contrary, he presents their formal articulation, which in turn seems to
go in two distinct directions, albeit connected. The ﬁrst consists of a more accurate
characterization of grounds and operations on grounds as objects typed on formulas
of a background language. The typing establishes a link between the object and the
judgment or assertion for which that object constitutes justiﬁcation. The second
aims at the development of a formal language, which includes terms denoting
the aforementioned objects, and formulas that indicate their main properties. The
resulting picture seems to come close to approaches of similar intuitionistic or,
more generally, constructivist inspiration, such as the Kreisel-Goodman theory of
constructions (Kreisel, 1962, 1965; Goodman, 1968, 1970, 1973, 1968) or Martin-
Löf’s intuitionistic type theory (Martin-Löf, 1984). It is not surprising, then, that
both the typing of objects, and the formal languages whose terms denote such
objects, come near the analogues in the typed λ-calculus; the general directives of
the theory of grounds can therefore be considered also, and perhaps mainly, in the
perspective of the formulas-as-types conception, a cornerstone of the Curry-Howard
isomorphism (Howard, 1980).
Despite these suggestions, however, within the writings that Prawitz has ded-
icated to the subject so far, the more formal side is only in an embryonic state.
That said, the importance of the basic objectives of the theory of grounds; the
wide range of theories, traditions and hints to which it is connected, both in the
philosophical ﬁeld and with respect to mathematical logic; and, last but not least,
the progress it allows in many respects, are more than enough reasons to support an
in-depth development of the “technical” area of the theory. Of course, this in-depth
analysis does not only serve the purpose of systematization, as it also permits a better
understanding of some of the basic assumptions of the theory itself, illumination
of some of its non-secondary philosophical characteristics, drawing of non-trivial
consequences, and indications of further possible theorizations
In this context, our proposal will move along two main directives: ﬁrst, the
delineation of a class of formal languages of grounding, and the related deﬁnition of
a denotation function that allows us to associate terms with grounds and operations
on grounds; and second, the development of a class of formal systems of grounding,
which allow us to establish relevant properties, expressed by means of appropriate
formulas, of the terms and of some of the symbols of the languages of grounding. In
either case, these primary intentions will be accompanied by a certain reﬁnement
of the analysis, with the aim of perfecting general deﬁnitions, by introducing
concepts and proving results that allow a more speciﬁc application of the deﬁnitions
themselves. Far from claiming to be exhaustive, our contribution is to be understood
as the ﬁrst draft of a fully “mathematized” theory of grounds, so as to highlight
fundamental characteristics which, in our opinion, should apply in a complete

1
Introduction
7
formalization. Starting from this core, it will also become clearer how and where
further advancements could differ, depending on the different needs, in speciﬁc
choices or alternative characterizations.
The work is divided into two parts. The ﬁrst part is in turn divided into
three chapters. The ﬁrst raises the fundamental problem of the theory of grounds,
namely the explanation of the power of epistemic compulsion of deductively correct
inferences and reasoning, providing a clariﬁcation of the fundamental concepts
involved—i.e. inference, proof, premises/conclusion, and so on. It also explains the
main reasons for the inadequacy of model theory in capturing a notion of necessity
epistemically understood. After that, we will introduce the alternative Prawitz’s
semantics of valid arguments and proofs, following the different conﬁgurations it
has taken over the years, and focus on some of the weak points from which it seems
to suffer compared to a satisfactory explanation of the epistemic force of deduction.
The third chapter, ﬁnally, aims at a reconstruction of the theory of grounds as it has
been presented so far by Prawitz, showing the progresses it allows, the problems
shared with its predecessors, and the points liable to further reﬁnement.
The second part is again divided into three chapters. In the ﬁrst, we introduce a
class of languages of grounding that include only terms; the class is developed by
expansions of a core language which contains operational symbols corresponding
to primitive operations on grounds, and the expansions are classiﬁed, according
to different properties, in relation to a denotation function that associates grounds
or operations on grounds to the terms of the language. In the following chapter,
languages of grounding are enriched with predicates that allow us to construct
formulas which in turn allow to express properties of the terms and of some of
the symbols of the alphabet. The provability of these properties is reached then by
means of a class of formal systems of grounding, each equipped with a set of rules
inspired by the same principles that had led to the characterization of the denotation
functions. Finally, in the third chapter, two questions are discussed: ﬁrst, the
completeness of ﬁrst-order intuitionist logic with respect to the apparatus developed
in the two previous chapters, and this in the form of a conjecture—in a “weak”
and “strong” version—similar to the one elaborated by Prawitz (1973) for his
semantics of valid arguments and proofs; the second discusses the aforementioned
recognizability problem, as it appears in the theory of grounds, in relation to the
theme of the general form of the equations that deﬁne non-primitive operations on
grounds.
Part of the content of this book has already appeared in some articles I have
published elsewhere. In particular: Sects. 3.3.1, 3.3.2, 3.3.3 and 4.1.5 expand
upon my Proofs, grounds and empty functions. Epistemic compulsion in Prawitz’s
semantics, in Journal on philosophical logic, 51, pp. 249–281, 2022; Sect. 5.2 and
its sub-sections expand upon my Denotational semantics for languages of epistemic
grounding based on Prawitz’s theory of grounds, in Studia Logica, 110, pp. 355–
403, 2022; Sects. 6.2 and 6.3 and their sub-sections expand upon my Calculi of
epistemic grounding based on Prawitz’s theory of grounds, in Studia Logica, 110,
pp. 819–877, 2022.

8
1
Introduction
This book is drawn from my doctoral dissertion, titled Dag Prawitz’s theory of
grounds, which I prepared at the University of Aix-Marseille, under the supervision
of Gabriella Crocco, and at the “Sapienza” University of Rome, under the super-
vision of Cesare Cozzo. Both Gabriella and Cesare have been fundamental to my
formation, to say the least. Their constant presence, their readiness to listen, and
above all the criticisms and corrections that they have given me in long and frequent
interviews, are as important as a doctoral student may ask.
I am also grateful to Dag Prawitz, as a researcher in logic, for the fundamental
contributions he has given to this discipline, but also for the kindness with which he
has always welcomed my questions, for the attention he has devoted to them, and
above all for the rigour of his answers, with which I could grow up. All this has been
so important to explain alone the sense of the entire doctorate.
I cannot fail to give a special thanks to Carlo Cellucci, Ansten Klev, Enrico
Moriconi, Peter Schroeder-Heister, Göran Sundholm, Luca Tranchini and Gabriele
Usberti, for the patience with which they have listened to my many questions,
and for the kindness and warmth with which they were able to answer. A special
thanks goes to Myriam Quatrini for the valuable advices and the fruitful remarks
that have guided, not only part of my survey on the theory of grounds, but also a
parallel project, concerning a possible link between this theory and Girard’s Ludics.
A project that I have had the pleasure and honor of conceiving and developing with
Davide Catta, a friend as well as a colleague.
I owe a lot to many other people, so that I dedicate to each one of them the most
heartfelt and dear of my thoughts; these people are Miloš Adžic, Claudio Bernardi,
Constantin Brîncu¸s, Paola Cantù, Emiliano Ippoliti, Francesco Montesi, Peter Pagin
and Paolo Pistone.
Antonio Tuzzi has remarkably helped me for the translation of the thesis from
which this book is drawn, originally written in Italian. He did it with his usual
kindness, and with the affection he has always shown to me—totally returned. For
the same reasons, I am also indebted to Simona De Leoni, one of the most passionate
and enthusiastic persons I have ever met, who is for this reason so precious to me.
The ﬁnal English version of the book is mainly due to Giulia Vivaldi, a good old
friend I met during my studies at the Department of Mathematics of the “Sapienza”
University of Rome. I have many nice memories of those days, and Giulia is literally
an essential part of these memories.
Let me also thank the members of the Center Gilles Gaston Granger (formerly
CEPERC). Each of them in his/her own way has meant that I created a bond by now
unbreakable with Aix-en-Provence and Marseille, and more generally with France.
I cannot fail to thank also the A*MIDEX foundation, that gave me a beautiful
opportunity to carry out my doctoral research, providing me with all the means
and comfort that a student may dream of. I think I must point out my debt to Nicola
Grana; he led my ﬁrst steps in the world of logic at the “Federico II” University of
Naples, making me passionate with his engaging lectures, and accompanying me
little by little to my bachelor degree in philosophy.
The memory that shows no sign of dying out, and that burns more and more every
day alive, is ﬁnally my very deep thanks to Kosta Došen.

Part I
The Idea of Epistemic Grounding

Chapter 2
From Models to Evidence
2.1
Nature of Inferences
Our mental activity is characterized by a series of processes and acts through which,
by elaborating information, knowledge, thoughts and beliefs, we pass to other
information, other knowledge, other thoughts and other beliefs. These operations,
as well as their purposes, can concern several levels of awareness and voluntariness,
varying degrees of complexity, a different use of time and different memory
resources, thus implying a greater or lesser force in the results obtained. Frequently,
an absolute unconsciousness is accompanied by unintentionality and automatism,
and immediacy, rapidity and relative simplicity produce or generate uncertain or
fallible acquisitions. At the opposite extreme, the completion of operations can be
totally conscious and voluntary, often complex, long and tiring, and lead to a state
whose epistemic content seems conclusive and not refutable. Obviously, between
these two poles there are many intermediate stages, in which various elements are
combined in a partial and heterogeneous way.
Linguistic practices, on the other hand, are largely, if not essentially, aimed at the
exchange of information and knowledge. The linguistic heritage which a community
uses for communication purposes is closely linked to the mental activity of each
speaker; the two levels, only apparently distinct in public and private, intersect each
other in such a narrow way as to be, from certain points of view, hardly separable,
or at least not signiﬁcantly so. When we are willing and able to do it, we express
what we think, sometimes remarking that what has been said depends on something
else, from previous elements in our possession, and maybe that we have arrived
there via a certain path. Our interlocutors can agree, take with themselves what we
have made known, and in turn communicate to us; but they can also criticize us, ask
for further explanations, and indicate errors we had not noticed. This urges us—or
should urge us—to retrace our steps and review our line of thought, modifying it,
by expanding our intellectual baggage or rejecting its elements that are no longer
sustainable. Dialectics, then, can continue, in an overall activity of which it is
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_2
11

12
2
From Models to Evidence
probably impossible (and in any case not required here) to give a precise picture. In
this context, a role of primary importance is played by those that, using a widespread
term of technical literature, we call inferences.
The notion of inference seems to rest on at least three indisputable points. First,
an inference involves a passage from certain data, generally called premises, to
another datum (or more), the conclusion (or the conclusions). Second, inferences
must be connected to reasoning. In fact, they can be described as minimum units
which can be reached by breaking a reasoning down into progressively simpler
parts; in turn, a reasoning can be understood as a chain of inferences. Finally,
inferences are important to logic. According to a fairly widespread reading, the latter
is to be understood as the science of correct reasoning. It must therefore deal with
inferences and reasoning in general, providing tools by which to establish which
inferences are valid and which pieces of reasoning are correct, as well as a further
analysis of the notions of validity and correctness themselves—that is to say, when
and why an inference can be said to be valid, and a reasoning correct.
Of course, their minimal character clearly does not exclude that the inferential
units can be further analyzed. However, this circumstance could collide with the
intention of ﬁnding a notion of inference that does not change depending on the
historical and scientiﬁc context. Similarly, if the idea that a reasoning is a chain
is not accompanied by the requirement that its steps have a minimal complexity,
an enormously complicated reasoning can be reduced to an inference having as
premises its hypothesis, and as conclusion its conclusion. This, for its part, could
not go together with the epistemic desiderata of an analysis that looks at inferences
as acts carried out consciously and voluntarily by agents with limited time and
resources. Some transitions could be too difﬁcult, so that agents of the type
described may never be willing to carry them out.
In both cases, the description of the nature of inferences will inﬂuence the
description of the nature of reasoning and, if we are too generous in our way of
looking at inferences, the link could be affected. If we accept that some inferences
may be unconscious, involuntary and even automatic steps, we should also be
willing to accept that steps of this type do not occur in reasoning, or be equally
generous in the treatment of reasoning, so as to authorize unconscious, involuntary
and automatic components in it. More generally, the interdependence between
inferences and reasoning makes it impossible to adopt the above description as a
deﬁnition of the concepts involved: one cannot, on pain of vicious circles, postulate
that an inference is the minimum unit of a reasoning and at the same time that a
reasoning is a chain of inferences. The aforementioned intertwinement is therefore
a result to aspire to, rather than a given from which to start.
Cesare Cozzo has argued that different conceptions about the nature of inferences
are not necessarily incompatible, since they can rather serve different purposes. In
particular,
the question “what conception of inference ought we to adopt?” thus leads to the question
“what is the problem?”. (Cozzo, 2014, 165)

2.1
Nature of Inferences
13
Given the three previous points, the answer to the question on the nature of
inferences, therefore, cannot and should not be univocal. How to articulate it, then?
Cozzo himself indicates, in a commendable way, seven relevant factors.
The ﬁrst one concerns the nature of premises and conclusion. If some authors
maintain that the description given above, according to which premises and
conclusion are mere data, is satisfying, others consider it too permissive. Premises
and conclusion are truth-bearers—that is, entities that are liable to be true or
false. And in turn, this can be understood in at least three ways; truth-bearers can
have an abstract nature, in which case one usually speaks of propositions, or a
linguistic nature, being therefore sentences, or, ﬁnally, they can assume the form
of mental states, or beliefs. However, the range of possible answers does not end
here. Following yet another approach, we could in fact require that
premises and conclusions are not objects or states in which we happen to ﬁnd ourselves, but
responsible acts or actions, which we do. (Cozzo, 2014, 162)
Such acts or actions can, once again, stand out on a mental level, as judgments of
various kinds, or on a linguistic level, as in the case of assertions (but also, perhaps,
of questions or commands). But, more precisely, what are propositions, sentences,
beliefs, judgments and assertions? Here too, the answers are heterogeneous, and
so we have numerous further subramiﬁcations. The second factor is given by the
inferential agent. As in the previous case, we can reason in a more or less exclusive
way:
if you think that only a person can make an inference, you have a narrow conception of
the subject of inference. If you believe that not only a person, but also a machine, or a
non-personal biological entity can infer, then you have a broad conception of the subject.
(Cozzo, 2014, 162–163)
The third ingredient involves the relation between the agent and the set of premises
and conclusion, and depends or is expected to depend, on the way in which the ﬁrst
and the second factor have been settled. Thus, we could say that
the data X are stored in S; S is in the representational state X; S is in the neural state X; S
performs the act X, etc. (Cozzo, 2014, 163)
Probably the most important factor, no doubt central to the logical inquiry,
is what Cozzo labels as the fourth, and concerns the relation between premises
and conclusion. Here, we move from an extremely inclusive response, according
to which an inference is simply a pair where the ﬁrst element is the set of
premises and the second the conclusion, to responses instead insisting that, in an
inference, premises and conclusion cannot be completely untied, for some sort of
connection must occur among them. What this connection is, however, is anything
but unquestionable. Is it an abstract relation, or maybe a causal relation dependent
on a psychological, possibly unaware, involuntary and automatic transition? Perhaps
neither of these, but in a more epistemic sense,
a conscious and deliberate act on the part of the subject. (Cozzo, 2014, 163–164)

14
2
From Models to Evidence
The ﬁfth element is the stability of the premises-conclusion relation. This relation
in fact can be completely aleatory, or substantial but refutable according to future
circumstances. However, some inferences, commonly called deductive, seem to be
such that
the connection between premises and conclusion is stable and can never be subverted by a
new piece of information. (Cozzo, 2014, 164)
At the sixth point, we ﬁnd the public character of inferences. If inferences are
conceived as subpersonal psychological transitions, it will be very difﬁcult to think
of them as something publicly communicable. According to many, however, this
view must be rejected: inferences, and the pieces of reasoning in which they are
involved, must be able to materialize in practices accessible to all members of the
community. Finally, the seventh factor concerns what Cozzo deﬁnes
the context in which premises and conclusion are placed. (Cozzo, 2014, 165)
When we describe an inference, shall we limit ourselves to describing only its
premises, conclusion, agent and their relations? Should we not, perhaps, take into
account also the broader context in which the inference is accomplished? Perhaps
other inferences, or pieces of reasoning, to which it is connected? Or the whole
information or knowledge in the agent’s possession? Or even the whole set of co-
agents able to perform and accept inferences?
2.2
Valid Inferences
The seven factors outlined by Cozzo offer a general, neutral grid in which to
frame different conceptions of the nature of inferences. However, if the adoption
of a determinate conception depends on the problem we intend to solve, what we
have just afﬁrmed must be reported to the object of this investigation, namely,
Dag Prawitz’s theory of grounds, and to the fundamental question the latter aims
to answer: in what sense and why do some inferences—often called valid—seem
endowed with a power of epistemic compulsion?
2.2.1
Epistemic Compulsion
Epistemic compulsion is something we often experience in everyday life. The bill
for a dinner at the restaurant amounts to $ 45: we have given the waiter a banknote of
$ 50 and expect to receive $ 5 change, and so the waiter will have to do. An inference
has been made, the premises of which concern the following circumstances: the
bill is for $ 45, we have given the waiter a banknote of $ 50, and 5 is what we
get by subtracting 45 from 50. Of course, we could feel magnanimous, or could
have made a mistake, and ask for a less change. In the same way, the waiter

2.2
Valid Inferences
15
might be confused, or could refuse to give us what we are due, perhaps claiming
to follow a strange arithmetic in which the numbers 45 and 50 indicate the same
quantity. Ignoring details and secondary aspects, however, the inference compels us
to accept its conclusion: we feel authorized to claim $ 5, and the waiter should feel
obliged to give us exactly that sum. Again, if we are reasonably convinced that A
implies B, and we have ascertained A, we have to conclude B. Without this having
consequences of any kind, we are obviously free to refuse the conclusion. There is,
however, a clear sense in which anyone would maintain that such a behavior would
be wrong, or even irrational. Being free to do what one wants seems to fail if the
correctness of reasoning or the sustainability of its conclusions are at stake: we are
forced because we feel forced.
We are, in other words, in the presence of a very particular phenomenon, which
originates from and depends on
a special force, which is neither the threat of violence, nor the charm of a seductive
persuader: it is simply the sober force of reasoning. (Cozzo, 2014, 165)
Reasons and modalities of epistemic compulsion have often been at the center of the
reﬂection of philosophers and logicians. Precisely to this issue Prawitz has mainly
and explicitly dedicated some of his latest works. In them, the Swedish logician
proposes and develops the aforementioned theory of grounds, an answer—both
time philosophical and formal—that passes through an innovative characterization
of the nature of inferences and of their validity, of proofs, and of the conceptual
content of such interconnected notions. From this perspective, the theory of grounds
is intrinsically worthy of interest. However, its importance derives also from the
solutions and advances that it is able to offer with respect to similar approaches,
including the one that Prawitz himself previously developed. After all, the epistemic
relevance of valid inferences and proofs has always been one of the pivotal points of
Prawitz’s semantic investigations; and the theory of grounds constitutes a signiﬁcant
change of perspective with respect to these previous conceptions.
Focusing on the problem of epistemic compulsion seems to impose some forced
choices on the type of inferences to be considered, a quite binding selection with
respect to the general framework provided by the seven factors identiﬁed by Cozzo.
First, as regards the stability of the premises-conclusion relation, we are only
interested in deductive inferences. Of course, we could also feel epistemically
forced towards conclusions drawn on the basis of inferences whose strength might
decrease, or even vanish, in view of future occurrences. However, in this case, the
inference accomplished does not provide a truly conclusive support, and inferences
of this type may be of interest for logic only as cases that do not respect the
conditions an inference has to satisfy to be valid.
As for the nature of premises and conclusions, it seems reasonable to argue that
their description in terms of data is decidedly unsatisfactory, or at the very least too
general. There are certainly data for which it makes sense to speak of epistemic
compulsion. It is a datum for those who accept the rules of usual arithmetic that
every multiple of 3 is divisible by 3, and that 6 = 2 × 3, and it is therefore a datum
to which one is epistemically compelled that 6 is divisible by 3. However, there

16
2
From Models to Evidence
are also data for which this discourse does not seem to be valid. Let us assume
we are monitoring the different conﬁgurations that the iris of an observer O takes
according to different colors projected on a screen; when the color red, the datum-
premise, appears on the screen, the iris of O will assume a certain conﬁguration, the
datum-conclusion. From a certain point of view, O is forced, so to speak, to draw
a certain conclusion, but it would be strained to speak of an epistemic compulsion.
The constraint, in fact, is not generated by the sober force of reasoning, nor is O
free to oppose it. Premises and conclusion should therefore be, at the very least,
truth-bearers, that is to say, propositions, sentences or beliefs. Of course, even the
stronger requirement that premises and conclusion are judgments or assertions is
compatible with our purposes.
In light of the above, it does not even seem promising to claim that the agent of an
epistemically compelling inference can be a generic biological entity; propositions,
sentences, beliefs, judgments and assertions are objects or acts concerning an
abstract sphere, conceptual or linguistic, which we would struggle to attribute to,
for example, a jellyﬁsh. A human being is certainly more suitable, but what about
the famous Chrysippus’ dog? Following its master, who is far and not visible, it
arrives at a crossroads, sniffs one of the possible paths and, not smelling its master,
conﬁdently and without snifﬁng takes the other path. Similarly, can the agent of an
epistemically compelling inference be a machine? As Prawitz remarks, the central
point here is that epistemic compulsion involves a reﬂective activity, seemingly
lacking in animals and machines. In fact, it seems to go with a reﬂection through
which the performed activity can be appropriately generalized, and understood as
deductively reliable:
when we deliberate over an issue or are epistemically vigilant in general, we are conscious
about our assumptions and are careful about the inference steps that we take, anxious to get
good reasons for the conclusions we draw. [. . . ] taking for granted the truth of a disjunction
‘A or B’, and getting evidence for the truth of not-A, we start to behave as if we held B
true without noticing that we have made an inference [. . . ] the Babylonian mathematicians
were quite advanced, for instance knowing Pythagora’s theorem in some way, but, as far as
we know, they never tried to prove theorems deductively. (Prawitz, 2015, 67)
Essentially the same reasons that have guided our previous choices should at this
point lead us to demand that, between the agent of an epistemically compelling
inference and the set of premises and conclusion of such an inference, there is
something more than having stored certain information, or ﬁnding oneself in a
neural state. The agent must be in some way conscious of the content of premises
and conclusions, if we consider these as propositions or sentences; in the case of a
reading in terms of beliefs, the agent must occupy an intentional state focusing on
such beliefs. Finally, when premises and conclusions are conceived as judgments
or assertions, the agent must perform the acts to which they correspond. This,
one would say, also favors a reading of the premises-conclusion relation in terms
of a conscious and deliberate act on the part of the agent. The conclusion is
(or binds to) the preﬁxed goal, which the agent aims to achieve on the basis of
some sort of support provided by the premises. In any case, such an act could
be accomplished at a later stage, after the achieved awareness of a link between

2.2
Valid Inferences
17
premises and conclusion. Therefore, it is not possible to exclude a priori that the
premises-conclusion relation can be of an abstract type.
Finally, the question relating to epistemic compulsion is undoubtedly compatible
with a public view of inferences, and of the reasoning in which they occur. In
fact, the reﬂective character of the inferences in question seems to imply that we
can give a testimony of them, and of the reasoning in which they are used, and
this can be done without denying that the phenomenon can also—or mainly—
involve mental dynamics. Certainly, there is a long tradition, dating back to at least
Descartes (1985), according to which some acts, essentially intuitive and private,
induce an epistemic compulsion. A discussion on the plausibility of this thesis, or
on the way it can be articulated, would obviously take us too far; therefore, we will
limit ourselves here to emphasizing that the compelling character of such acts must
in any case refer to certain propositions or sentences, and ultimately depends on
the meaning attributed to them. But then, the analytical character of such supposed
intuitive knowledge is something that can manifest itself in practices that testify the
acceptance and sharing of meanings. This observation leads us to the ﬁnal case of
the context in which inferences are accomplished. In this case, however, the possible
alternatives are all mutually compatible.
2.2.2
Justiﬁcation
That some inferences are commonly held to have a power of epistemic compulsion
is certainly a fact. However, the existence of valid inferences can be a matter
of doubt. The deductive ability to support conclusively propositions, sentences,
beliefs, judgments or assertions might be nothing but an illusion, and some forms of
skepticism seem to be oriented exactly towards this thesis. Prawitz himself observes,
however, that
to justify deduction in order to dispel skeptical doubts about the phenomenon is not likely
to succeed, since such an attempt can hardly avoid using deductive inference, the very thing
that is to be justiﬁed. This should not prevent us from trying to explain why and how
deductive inference is able to attain its aims. Deduction should thus be explained rather
than justiﬁed. (Prawitz, 2015, 65–66)
If we can successfully explain how and why some inferences force epistemically, we
will also be in possession of a weapon against the skeptic. In order to express more
than a mere opinion, the skeptic would need to show that no plausible conception on
the nature of inferences and of their validity is such as to attribute to valid inferences
something that can count as a power of epistemic compulsion. Even assuming that
this does not rely on the use of inferences that the skeptic treats as epistemically
compelling, it should still result in something very similar to what Prawitz suggests
doing, albeit with opposite objectives. Therefore, the basic question of the theory of
grounds is, it seems, important even for the skeptic. Be that as it may, Prawitz does
not give too much credit to possible skeptical positions, arguing rather that

18
2
From Models to Evidence
we take for granted that some inferences have such a power, and there is no reason to doubt
that they have. But what gives them this power? This should be explained. (Prawitz, 2015,
73)
Far more important, especially for the development of our reasoning, is another
consideration. The questions related to what we are here calling epistemic compul-
sion, in fact, have often been formulated as questions related to the ability that valid
inferences have to justify who performs them with respect to their conclusion. It is
often said that valid inferences preserve, or transmit, justiﬁcation, in the sense that if
one is justiﬁed with regard to the premises, the same will be true for the conclusion.
What “to be justiﬁed with regard to ...” means obviously depends on the adopted
conception of inference, but it is clear that the justiﬁcation we are talking about is of
the strongest possible type: a conclusive and non-refutable justiﬁcation, a deﬁnitive
reason for the truth of propositions, sentences or beliefs, or for the correctness of
judgments or assertions. The idea seems to be implicit, or at the very least relevant,
even when valid inferences are conceived in the most abstract sense of necessary
truth-preservation: it cannot occur that the premises are true, and the conclusion is
false. The manner in which one passes from the necessary preservation of truth to
the preservation or transmission of justiﬁcation then becomes, in general, a central
problem in the frameworks, both philosophical and formal, which adopt such an
approach.
As Cozzo further points out (Cozzo, 2014), epistemic compulsion and preserva-
tion or transmission of justiﬁcation are, in any case, two sides of the same coin. The
general argument in favor of this position is based on a
plausible assumption concerning the relation between the notion of justiﬁcation and the
notion of argumentative context with rational disputants, the idea that justiﬁcation is
something publicly acknowledgeable by rational subjects: if a person is justiﬁed in asserting
a sentence, or an inference has the power to transmit justiﬁcation, then both facts must be
acknowledged by the disputants involved, if they are rational. (Cozzo, 2014, 165)
From this assumption, we can derive the equivalence between the circumstance that
someone is forced to or justiﬁed in accepting something, and the circumstance that
each of their interlocutors is equally forced on pain of irrationality, or justiﬁed to
do the same. Given two interlocutors A and B, let us suppose that A is justiﬁed
in accepting the premises in a set  of an inference I from  to the conclusion
α that preserves or transmits justiﬁcation, and let us suppose furthermore that A
performs I. According to the public nature of justiﬁcation, B recognizes that A is
justiﬁed in accepting the premises in  and, according to the public nature of the
preservation or transmission of justiﬁcation, B also recognizes that A is justiﬁed in
accepting α. Therefore, on pain of irrationality, B is forced to accept α. Vice versa,
let us suppose that I is epistemically compelling and, again, that A is justiﬁed in
accepting the premises in , and that they perform I. According to the public nature
of justiﬁcation, B recognizes that A is justiﬁed in accepting the premises in , and
therefore they too are obliged to accept them; but then, B will be forced, on pain
of irrationality, to accept α. It follows that, for the above equivalence, A will be
justiﬁed in accepting α.

2.2
Valid Inferences
19
Acquisition of knowledge is the ultimate goal, as well as the main result, of
deductive activity. The means by which this end is achieved is, of course, that
particular form of reasoning called proof. But what is a proof? And what is the
connection between proofs and valid inferences?
2.2.3
Proofs
Inferences can be valid, but they can also fail. They can indeed force, in the
epistemic way previously described, the acceptance of a certain conclusion, and
preserve or transmit justiﬁcation towards it. But they can also only give the illusion
of doing so. This happens frequently, and becomes particularly evident within the
framework of disputes among subjects who aim to achieve truth or knowledge.
Some perform premises-conclusion steps of which they are ﬁrmly convinced, and
therefore feel epistemically forced, or justiﬁed, to accept the conclusions they have
reached. However, they may have made some mistakes, and if the other interlocutors
are careful enough to realize this, they will point out that something has gone wrong
and that for this reason the conclusions must be withdrawn. Therefore, those who,
at ﬁrst, had made demands on the cogency of their results will be willing to admit
they were wrong.
The same argument applies to the reasoning to which the inferences are linked;
it can be wrong or, similarly to valid inferences, can force epistemically, and
conclusively justify the acceptance of its conclusions. The term commonly used in
the latter case is that of proof. A proof is a reasoning crowned by epistemic success,
and therefore it does not make sense to speak, for it, of error. From a conceptual
point of view, a wrong proof is a contradictio in terminis.
Proofs play a major role in the deductive activity that human beings have
partaken since the dawn of the exact sciences. They are the most secure source,
the most reliable instrument in the pursuit of truth and the knowledge of what one
is intellectually committed to, or intends to ﬁnd out. Therefore, as early as its birth
with Aristotle, logic has already been questioning the nature and reasons of the
mysterious force that proofs exert on us, and the conditions that a reasoning must
satisfy to be deemed correct. Over the centuries, the dual objective of justifying
and explaining this link has been understood in very different ways. However,
contemporary logic is the child of a tradition dating back to the late nineteenth
century, and to the ﬁrst half of the twentieth, when the research of Gottlob Frege,
Bertrand Russell, David Hilbert and Alfred Tarski, in particular, marked a point of
no return compared to previous authors.
Through a reﬂection on the rules and universal principles of deductive practice,
and an axiomatization of the same, the formalization process has allowed a structural
deﬁnition of proof as a derivation in a system. The adequacy of purely syntactic
conﬁgurations was then connected to, and made to depend on, a rigorous delineation
of semantic concepts. From this point of view, the completeness theorem for
predicative logic and the incompleteness theorem for Peano’s ﬁrst-order arithmetic,

20
2
From Models to Evidence
both proven by Kurt Gödel, were the watershed for all the subsequent work. In
particular, they inﬂuenced a reﬂection on the notion of proof as such, which led to
what we now call proof-theory. The origins of this theory can be traced back to
Hilbert, who
hoped to establish the consistency of mathematics or, more generally, to obtain a reduction
of mathematics to a certain constructive part of it. Hence, the study of proofs was here only
a tool to obtain this reduction, and it could thus not use principles that were more advanced
than those contained in the constructive part of mathematics to which all mathematics was
to be reduced. We may call such a study reductive proof theory. (Prawitz, 1973, 225)
However, a disciple of Hilbert’s, Gerhard Gentzen, had already taken his ﬁrst steps
towards a more comprehensive approach, with the so-called sequent calculi and
its fundamental Cut-elimination theorem, and with the natural deduction calculi.
Gentzen’s legacy was largely taken up by Prawitz among others, who, ﬁrst with the
normalization theorems for the natural deduction calculi and then with his semantics
of valid arguments and proofs, laid the foundations for what he himself deﬁnes as
general proof-theory. Here,
we are - in contrast - interested in understanding the very proofs in themselves, i.e. in
understanding not only what deductive connections hold but also how they are established,
and we do not impose any special restrictions on the means that may be used in the study
of these phenomena. (Prawitz, 1973, 225)
Over the years, and in a constant comparison with suggestions such as Michael
Dummett’s research into the theory of meaning and with the intuitionistic tradition
stemming from Luitzen Brouwer and Arend Heyting, Prawitz’s theories have
evolved. Thus, the importance of the underlying question of the current theory of
grounds is mainly due to the link between valid inferences and proofs: to explain
how and why valid inferences can be epistemically compelling means also to shed
light on proofs, on their strength and reliability. But how to connect valid inferences
with proofs?
As we have already afﬁrmed, some inferences are involved in reasoning. We can
look at the former as minimum units of the latter, or give the latter the meaning
of inferential chains. In both cases the relation between the two notions becomes
relevant for the type of inferences we must take into account, if we intend to
deal with epistemic compulsion. Deduction, in particular, does not seem to be
generic data processing; it is a conscious activity, in which one deliberately aims to
highlight dependency relations among propositions or sentences, to validate beliefs,
to support judgments or assertions. It is accomplished by intelligent agents, who
reﬂect on what they are doing, and can recognize the content of the information in
their possession, have access to intentional states, make moves in thought and in
language, and communicate with one another within the framework of a context of
varying breadth.
As a correct reasoning, a proof has a particular link with valid inferences. From
an informal point of view, the common idea seems to be quite precise here: the
notion of valid inference is prior compared to the notion of proof. And this is similar,
for example, to the concept Descartes illustrates in a famous passage:

2.3
The Fundamental Task
21
a continuous and uninterrupted movement of thought in which each individual proposition
is clearly intuited. This is similar to the way in which we know that the last link in a long
chain is connected to the ﬁrst: even if we cannot take in at one glance all the intermediate
links on which the connection depends, we can have knowledge of the connection provided
we survey the links one after the other, and keep in mind that each link from the ﬁrst to the
last is attached to its neighbour. (Descartes, 1985, 15)
This line of reasoning contains two important intuitions: (1) all the premises-
conclusion steps of a proof are epistemically compelling, and (2) proofs are
something we do consciously, acts carried out deliberately and voluntarily. As we
will see, not all formal descriptions of the notion of proof have followed these
guidelines. On the contrary, the theory of grounds seems to view this way of
proceeding as the only promising strategy towards an adequate description of the
epistemic power of valid inferences, as well as of the epistemic power of proofs.
2.3
The Fundamental Task
How should the underlying question of the theory of grounds be more precisely
formulated? Is it possible to identify a programmatic setting of a satisfactory
explanation for the strength of epistemic compulsion of which valid inferences
are endowed? In the articles Inference and knowledge (Prawitz, 2009) and The
epistemic signiﬁcance of valid inference (Prawitz, 2012a), Prawitz outlines a general
framework that may perhaps serve that purpose. Given the premises
(a) I is a valid inference from the premises  to the conclusion α;
(b) A is in possession of a ground for each of the premises in ,
which further condition (c) must be added to (a) and (b) in order to get
(d) A has a ground for α?
First of all, it should be noted that Prawitz, in this formulation, uses the
expression “ground”. As is easy to imagine, the notion of ground will receive
a particular connotation as well as a rigorous formal expression. However, the
question is sufﬁciently clear to be understood without knowing the theory of
grounds. “Ground” can, for the moment, be understood in its ordinary sense, as
the proof of the truth of something, or the reason to believe in it, or the guarantee
for the correctness of a judgment or an assertion.
It is manifestly clear, however, that the identiﬁcation of the further condition (c),
and the derivation of (d) from it and from (a) and (b), will require an extension,
a deepening and a more precise characterization of the notion of ground. But
even before doing this, two other goals must be achieved: to provide an adequate
deﬁnition of the notion of valid inference and, then, to identify an appropriate
relation between A and I.

22
2
From Models to Evidence
2.4
Inference and Consequence
According to a rather widespread conception, the notion of valid inference must
be deﬁned through the notion of consequence: an inference is valid if, and only
if, its conclusion is a consequence of its premises. The fact that the notions of
valid inference and consequence are closely related clearly emerges both from
a conceptual examination of what, informally, we mean by them, and from the
historical development of mathematical logic.
When we say that a thing, x, is a consequence of another thing, y, generally
we mean that x has foundation, reason, and guarantee in y, and this is by virtue
of a bond that, with absolute cogency, applies on the basis of unquestionable and
universal properties or laws. If the bond can be captured by thought, what is obtained
is, or is expected to be, a proof of what we get on the basis of the hypothesis from
which we started. Sometimes, the proof consists in a single, simple passage, but
it is possible we might need to perform many complex intermediate transitions. In
both cases, we will have to deal with valid inferences, which exert a power, from
an epistemic point of view, by preserving or transmitting justiﬁcation. However, if
we have achieved some conclusions by proof starting from certain hypotheses, we
are also led to believe that between hypotheses and conclusion there is a special
bond, and that the two poles are therefore united with the same force that the
corresponding reasoning is endowed with.
Thus, valid inference and consequence refer to each other and, since the famous
Aristotelian deﬁnition of syllogisms, they have been approaching to the point
of often being indistinguishable. Since proofs highlight universal connections of
consequentiality, in dealing with valid inferences, logic will have also (and perhaps
mainly) to deal with consequence. However, there is a substantial and often ignored
difference between the two notions, the relevance of which is evident when the
analysis of the notion of valid inference favours an epistemic point of view.
To deal with the power of epistemic compulsion of deductively valid inferences
requires looking at the latter as acts through which intelligent agents pass, in
a conscious and voluntary way, from certain premises to a certain conclusion.
Premises and conclusion can be truth-bearers—that is propositions, sentences or
beliefs—or consist, in turn, of acts, namely judgments or assertions; moreover,
between premises and conclusions there must be a link, which, however abstract,
is in some way known to the agent. Consequence is, on the contrary, a relation
between truth-bearers, and speciﬁcally between propositions or sentences. Logical
consequence can be unanimously understood as a particular instance of a more
general relation of deductive consequence:
(CD) α is a deductive consequence of  if, and only if, necessarily, if all the
elements in  are true, α is true;
(CL) α is a logical consequence of  if, and only if, α is a deductive consequence
of , and this holds only by virtue of the logical form of α and of the logical
form of the elements in .

2.4
Inference and Consequence
23
Deductive consequence is, as indicated by the expression “necessarily” occurring
in (CD), a modal relation; as a restriction of deductive consequence, logical
consequence inherits this modality, but (CL) informs us that it, in this case, depends
solely on the logical form of propositions or sentences under consideration. What
kind of modality is that? And what is logical form?
The second question can be answered in a way that, though often criticized and
subjected to major or minor revisions, is nevertheless shared, or at least assumed,
by most scholars in this area. It is a line of thought dating back to Bernard Bolzano,
corrected and expanded by Tarski, and ﬁnally generalized by model theory. The
underlying idea is that, within a formalized language, one must distinguish between
constant symbols and variable symbols, where the former include what are usually
called logical symbols: “not”, “and”, “or” (in the inclusive sense), “if ...then”, “all”,
and “some” (which will later be formally indicated with the respective symbols ¬,
∧, ∨, →, ∀and ∃). The fact that the logical consequence relation depends solely
on the logical form of α and on the logical form of the elements in  can now be
understood in the most rigorous sense that it depends solely on logical symbols,
taken as constants, regardless of the speciﬁc content of the non-logical ones, which
are instead considered as variables. The question on modality is far more difﬁcult:
how should the expression “necessarily” be understood in (CD)?
An initial, famous reading concerns the so-called possible worlds, and postulates
that α is declared as a deductive consequence of  if, and only if, α is true in
every possible world where all the elements in  are true. This setting, however,
faces multiple difﬁculties: it is not clear how the notion of possible world can be
articulated without giving rise to circular explanations (Cozzo, 2015) and, even if
one appeals to model theory by saying that a possible world is nothing more than
one of the models described in this semantics, there is some doubt about the actual
plausibility of the equation between models and possible worlds (Etchemendy,
1990; Prawitz, 2005). Nonetheless, there are some authors who defend such
approach. Stewart Shapiro, for example, endorses a combined vision of (CD) and
(CL), which leads to a limitation from the analytically possible worlds to the sole
logically possible worlds (Shapiro, 2005). It is therefore a controversial issue, and
probably far from a deﬁnitive answer. What we can certainly say, however, is that
the interpretation in terms of possible worlds is opposed to the epistemic interests
of the kind of analysis we intend to carry out.
On the contrary, the idea proposed by Prawitz in his Logical consequence from
a constructivist point of view (Prawitz, 2005) is much more suitable. Here, the fact
that α is a deductive consequence of  is delineated through what Prawitz himself
calls necessity of thought:
(NT) α is deductive consequence of  if, and only if, the truth of α follows by
necessity of thought from the truth of all the elements in ,
where, with necessity of thought, we mean that
one is committed to holding α true, having accepted the truth of the sentences of ; one
is compelled to hold α true, given that one holds all the sentences of  true; on pain of

24
2
From Models to Evidence
irrationality, one must accept the truth of α, having accepted the truth of the sentences of .
(Prawitz, 2005, 677)
Anticipated in Remarks on some approach to the concept of logical consequence
(Prawitz, 1985), Logical consequence from a constructivist point of view marks, in a
sense, a turning point in Prawitz’s semantic investigation. The reference to necessity
of thought means that attention is focused primarily on inferential aspects, which
leads to a reversal in the order of explanation of the notions of consequence and valid
inference: the ﬁrst is to be explained in terms of the second, rather than vice versa.
In addition, the notion of necessity of thought anticipates many of the aspects of
what becomes, in the theory of grounds, epistemic compulsion. It is no coincidence
that Cozzo emphasizes how
the key feature of the relation between  and α is the compulsion of the inference from 
to α. One cannot be compelled if one does not feel compelled. Inferential compulsion is a
power that acts upon us only in so far as we are aware of its force. Therefore, the necessity
of though is an epistemic necessity: by making the inference a person recognizes a guarantee
of the truth of α given a recognition of the truth of the members of . (Cozzo, 2015, 104)
An epistemic understanding of the modality involved in (CD) seems, therefore, a
promising way to achieve our target. However, we certainly cannot be content with
the informal framework proposed by (NT). The question, then, is how to give a
precise content to Prawitz’s idea of necessity of thought.
2.5
Model Theory
In some articles from the ﬁrst half of the twentieth century, Tarski laid the
foundations for the formal semantics, known today as model theory. Generalizing
or expanding the suggestions of the Polish logician, and often incorporating them
with approaches and results from different backgrounds, model theory soon became
the standard in the deﬁnition of semantic notions. Even today, correctness and
completeness are generally evaluated with regard to model-theoretic procedures
and, when reached, considered as a guarantee of adequacy for the deductive
apparatus under examination. This demonstrates how model theory is not considered
as a mere approach among many others, but as the most correct and meaningful
one. The reasons for this situation are many, and probably too many to be reviewed.
Undoubtedly, model theory had, and still has, much merit of a technical as well as
conceptual nature, since it offers a theoretical framework in which to demonstrate
results of profound relevance, and articulate in a clear and elegant way a conception
of meaning based on assumptions of considerable philosophical depth. However, the
success of the model-theoretic notion of consequence does not imply that it suits to
the epistemic purposes we intend to pursue.
In the famous The concept of truth in formalized languages (Tarski, 1956b),
Tarski aims at a materially adequate and formally correct deﬁnition of the notion
of truth in a language. Formal correctness requires limiting the analysis to formal

2.5
Model Theory
25
languages and to the development of the discourse in formal meta-languages in
which to deﬁne a truth-predicate T applicable to names ⌜α⌝for formulas α of object
languages. Besides offering evident advantages in terms of clarity and precision,
formal languages
possess no terms belonging to the theory of language, i.e. no expressions which denote
signs and expressions of the same or another language or which describe the structural
connections between them; (Tarski, 1956b, 167)
therefore, they are distinguished, in a radical way, from natural languages, since they
avoid the semantic closure that Tarski identiﬁes as the main culprit in the occurrence
of antinomies. The condition of material adequacy, instead, will be satisﬁed if the
deﬁnition of T is such that, for each formula α of the reference language, it can be
derived as a theorem the schema
T (⌜α⌝) if, and only if, p,
where p is a structural description of α. The techniques and the results of The con-
cept of truth in formalized languages will converge later in On the concept of logical
consequence (Tarski, 1956a). Here, starting from the problem of an appropriate
characterization of the notion of consequence, Tarski enunciates his well-known
deﬁnition of logical consequence. Except for some negligible differences, model
theory faithfully adopts the Tarskian line of thought.
Model theory involves interpretations AD of some elements of the alphabet of
a ﬁrst-order language L on sets D: elements k ∈D for individual symbols c;
functions f : Dh →D for functional symbols f h; and relations Rm ⊆Dm for
predicative symbols P m. Therefore, AD interprets closed terms on D, and closed
formulas on a binary set {1, 0}. Finally, an open formula will be interpreted by
AD on 1 or 0, depending on whether or not it is interpreted by AD on 1 or 0 its
universal closure. AD is a model of α if, and only if, AD interprets α on 1. α is
logical consequence of a set of formulas —indicated with  |	 α—if, and only
if, supposing that the individual variables overall occurring free in  and α are
x1, . . . , xn, for every AD, for every k1, . . . , kn, if AD is a model of all the elements
of [k1, . . . , kn/x1, . . . , xn], then it is also a model of α[k1, . . . , kn/x1, . . . , xn]; for
 = ∅, α is said to be logically valid—indicated with |	 α.
The fact that such a characterization concerns the notion of logical consequence
depends essentially on its taking into account a totality of interpretations over a
totality of sets and, above all, on the way in which such interpretations are deﬁned
on such sets. In particular, the non-logical symbols are variable, insofar as they are
from time to time associated with elements, functions and relations in or on the
reference domain. On the contrary, the role of logical symbols is intended to remain
constant.
According to the informal framework offered by (CD) and (CL) above, logical
consequence is a particular instance of a more general notion of deductive con-
sequence and, since the latter is a modal relation, so must the relation of logical
consequence be. Can we say this requirement is respected by the model-theoretic

26
2
From Models to Evidence
deﬁnition? Even before concentrating, as suggested, on an epistemic understanding
of modality in terms of necessity of thought, the answer seems to be in the negative:
in model-theoretic semantics holds what Etchemendy (1990) called the reduction
principle. Indeed, Prawitz had already pointed out this problem in Remarks on some
approaches to the concept of logical consequence (Prawitz, 1985). Here, he makes
a distinction between logical and factual sentences, the latter containing non-logical
constants that are not present in the former:
the analysis makes no distinction between logical sentences [. . .] and factual sentences. The
effect is that a logical sentences is understood as logically true just in case it is true in the
same sense as factual sentences are true. (Prawitz, 1985, 154–155)
The reduction principle seems to offer a serious argument against the presence, in
the model-theoretic notion of logical consequence, of modal ingredients whatever
the type of modality one intends to capture. A detailed treatment of this subject
matter, in particular of the possible objections to the reduction principle, is outside
the scope of this work. Much closer to our goal is the question related to the
possibility of attributing to the model-theoretic approach the epistemic type of
modality involved in the necessity of thought. In fact, as early as 1985, Prawitz
emphasizes that what one should ask oneself is
what is the ground for a universal truth [. . .] or how can we come to know, even with
certainty, that a logical sentence is true in all domains. (Prawitz, 1985, 155)
In order to approach the problem, it is worth reﬂecting on the fact that logical
consequence is a modal relation, since it is a particular instance of a more general
relation of deductive consequence. Modality, understood as necessity of thought,
must therefore refer primarily to deductive consequence. In his On the concept of
logical consequence, although starting from general considerations on the notion
of consequence as such, Tarski actually deals exclusively with the logical case of
this notion. But what is deductive consequence in the Bolzano, Tarski and model-
theoretic tradition?
Taken as a whole, (CD) and (CL) suggest, in a certain, plausible interpretation,
to regard logical consequence in terms of invariance of deductive consequentiality
under variation of the content of the non-logical symbols. In fact, this seems to be the
manner in which Bolzano, Tarski, and model theory conceive the idea of dependence
on the sole logical form. This would mean that α being a logical consequence of
 simply boils down to the fact that, for each AD, the interpretation of α on D
under AD is a deductive consequence of the interpretation of  on D under AD. The
question is, therefore, what is consequence on a set under an interpretation. Upon
closer inspection, however, neither Bolzano, nor Tarski, nor model theory really
answer the question. The only available information is that, if all the elements in 
are true, also α is true, but this is far too little to obtain modal links of an epistemic
type. Thus, Tarski’s thesis (Tarski, 1956a), according to which, on the basis of his
deﬁnition, every consequence of true sentences must be true, may perhaps be valid
when understanding modality in terms of possible worlds—namely, with reference
to objective structures independent of our knowledge—but it is certainly wrong if

2.5
Model Theory
27
we are engaged in the characterization of the necessity of thought. As Prawitz puts
it
if we stay within the framework of Bolzano and Tarski, [the distinction between deductive
and logical consequence] becomes pointless, because the notion of [deductive] consequence
will then collapse into that of truth of the corresponding material implication. (Prawitz,
2013, 185)
At this point, it becomes important to observe how, with the distinction between
deductive consequence and logical consequence, it is possible to match an analo-
gous distinction between valid inferences and logically valid inferences, the former
being those in which the conclusion is a deductive consequence of the premises,
whereas in the latter this relation persists under every variation of the content of
the non-logical symbols—that is to say, in the presence of logical consequentiality
between premises and conclusion. But then the approach in question is also
incapable of attributing an epistemic power to valid inference. If, on the one hand,
we are not able to distinguish signiﬁcantly between deductive consequence and
logical consequence, we will not even be able to distinguish signiﬁcantly between
valid inferences and logically valid inferences, and this is problematic insofar as the
survey focuses on a notion of validity that is independent from the role that logical
constants play in the link between premises and conclusion. On the other hand, the
fact that deductive consequence collapses on the truth of correspondent material
implications has a clear generalization to the inferential case, so that
it would only remain to say that an inference is valid if either one of the premises is false or
the conclusion is true, but clearly no one is interested in equating the validity of an inference
with such a relation between the truth-values of the involved sentences. [. . .] Although [this]
property is certainly relevant for the question whether the inference has the power to justify
a belief in the conclusion (being a necessary condition for that), it is clearly not sufﬁcient
for the inference to have this power. (Prawitz, 2013, 185–186)
In other words, it remains essentially unexplained how, from the assertion of certain
circumstances in certain objective structures, a bond of epistemic modality can
be generated for those who intend to establish that a given truth-bearer, possibly
believed or judged to be true, or asserted, is valid on the basis of a support provided
by other truth-bearers, beliefs, judgments or assertions related to them. From this
perspective, we can also add the observation advanced by Cozzo:
there are inﬁnitely many pairs ⟨, α⟩such that all models of  are models of α but we fully
ignore that they are. If one is fully unaware that this relation obtains, one will not (or in any
case not legitimately) take any responsibility for a support that the premises  provide for
the conclusion α. (Cozzo, 2015, 104)
The notion of consequence so far discussed does not therefore capture necessity
of thought, and hence is unsatisfactory with regard to the problem of validity of
inferences. Is there another, more appropriate way of articulating the idea that an
inference is valid if, and only if, its conclusion follows from its premises?

28
2
From Models to Evidence
2.6
Meaning: From Truth to Evidence
As we have already said, the notions of epistemic compulsion and of necessity
of thought have many things in common. Both of them involve the idea of a link
that exerts its strength on agents engaged in the activity of deducing, in a justiﬁed
manner, conclusions from premises. Therefore, a link resulting from necessity of
thought must be something that the agent performing the corresponding inferential
passage is aware of, something that can be experienced. However, it is precisely by
virtue of an accomplished awareness that the passage must be carried out. As Cozzo
afﬁrms, necessity of thought has
a phenomenal character [. . .]. We have an experience of necessity. But this experience is at
the same time the experience of performing the act of making an inference. Therefore it is
an active experience. (Cozzo, 2015, 108)
The central question then becomes how a bond that induces necessity of thought
can become known, manifesting itself in the deductive activity. And the most
natural answer seems to be just the one Prawitz gives, unsurprisingly, in Logical
consequence from a constructivist point of view, namely, through proofs or valid
arguments.
In the 2005 article we are referring to, Prawitz hints that a valid argument for
α from  is the linguistic expression of a reasoning which takes as hypothesis the
elements in  and as a conclusion α, whereas a proof of α from  can instead be
seen as what a valid argument for α from  stands for. In both cases, however, we
are in the presence of something
such that, when we know of it, we are compelled to hold α true, given that we hold the
sentences of  true. (Prawitz, 2015, 678)
The idea then is to further develop (NT), by translating it as
(PT)
α is a deductive consequence of  if, and only if, there is a proof or a valid
argument for α from .
Without a doubt, (PT) constitutes a remarkable reﬁnement of the intuition contained
in (NT). However, the reference to proofs and valid arguments does not seem
satisfactory if we are not also able to say, more precisely, what valid proofs and
arguments are, and, moreover, if we are not able to show that, on the basis of their
description, proofs and valid arguments are such as to make those who perform them
experience the phenomenon of necessity of thought. However, even before turning
to a formal deﬁnition of the notions of proof and valid argument, a correct analysis
requires clarifying on what basis proofs and valid arguments can have an epistemic
value. Again, the wisest path seems to be the most obvious, since
it is difﬁcult to think of any answer that does not bring in the meaning of the sentences
[occurring in proofs or valid arguments]. In the end it must be because of the meaning of
the expressions involved that we get committed to holding one sentence true, given the truth
of some other sentences. (Prawitz, 2015, 678)

2.6
Meaning: From Truth to Evidence
29
The meaning of propositions or sentences will depend, plausibly, on the meaning
of the simplest non-propositional or non-sentential components of which they are
made. But what is the meaning of these components? How should this meaning
be determined, and on which notions is it based? We could begin by focusing on
the sole logical constant, and pass, at a later time, from such logically relevant
expressions to those having a more particular nature.
As a matter of fact, the question of the meaning of the logical constants has
always been at the center of mathematical logic and of philosophy of logic. A
leading intuition goes back to the Fregean context principle (Frege, 1884), and
can be understood here in the terms of Dummett’s interpretation (Dummett, 1973,
1978a, 1996a,b): the meaning of a non-propositional or non-sentential expression
E is given by the contribution of E to the determination of the meaning of the
propositions or of the sentences in which E occurs. Another inﬂuential thesis, again
due to Frege (1893–1903) and to Ludwig Wittgenstein (1921), is the principle
of truth-conditionality: the meaning of a proposition or sentence is given by the
necessary and sufﬁcient conditions under which it is true. Therefore, the two
intuitions so combined correspond to the idea that the meaning of a logical constant
C is given by the contribution of C to the determination of the necessary and
sufﬁcient conditions for the truth of the propositions or sentences in which C
occurs as the main logical constant. From this point of view, the Tarskian-model-
theoretic deﬁnition of the truth-predicate ﬁts perfectly with this setting, and permits
the derivation of the following clauses:
(∧T )
α ∧β is true if, and only if, α is true and β is true;
(∨T )
α ∨β is true if, and only if, α is true or β is true;
(→T )
α →β is true if, and only if, if α is true, β is true;
(∀T )
∀xα(x) is true if, and only if, for every k, α(k) is true;
(∃T )
∃xα(x) is true if, and only if, it exists k such that α(k) is true.
At this point, what we should ask ourselves is whether the mere ruling of (∧T )—
(∃T ) is sufﬁcient. As a reply, we could reasonably expect that a satisfactory truth-
conditional theory also clariﬁes which notion of truth is really at stake. Already
proposed by Dummett (1973, 1978c), the point is after all raised by Prawitz himself:
questions about the meaning of the logical consequence thus seem to have a straightforward
answer in terms of [truth-conditions]. However, the substance of this answer depends on
what we take truth to be. (Prawitz, 2005, 679–680)
In a sense, it could be argued that Tarski’s formal semantics and the resulting
model theory aim exactly at a delineation of the notion of truth. This can be admitted,
as long as we observe that the deﬁnition of the truth-predicate does not exhaust, but
rather implies, the corresponding notion. As Gabriele Usberti (2016) points out,
for example, Tarski (1956b) relates explicitly to the Aristotelian theory of truth as
correspondence, or even, and perhaps more signiﬁcantly, proves that every closed
formula is true or false by resorting in an essential way to a generalized bivalence
principle, according to which each truth-bearer is determinately either true or false.
The central point, however, is that such a description implies that the meaning of

30
2
From Models to Evidence
the logical constants is known. Tarski (1956b), not without reason, takes the latter
as given, by proving the material adequacy of his approach through examples based
on the possibility of matching names ⌜α⌝of the meta-language with meta-linguistic
translations of formulas α of the object-language. If the meaning of α is unknown,
these examples have no probative value.
Therefore, the abovementioned clauses identify truth only on the condition of
a prior understanding of the meaning of the logical constants; hence, they alone
will not be able to offer a notion on which to base the complete clariﬁcation of this
meaning. But even assuming that the notion of truth is completely independent from
understanding of meaning, there are many reasons for doubting that the clauses are
able to explain simultaneously both truth and meaning. From a general point of
view, in fact, formalization is conﬁned to circumscribing the extension of a meta-
linguistic predicate for names of formulas of a given object language. The fact that
the predicate at stake is actually a truth-predicate can be recognized only if one has
already some underlying idea of truth. Once again, in line with Dummett (1973,
1978c) Prawitz observes in this regard that
if we have deﬁned a set S of sentences by saying that it is the least set of sentences
containing certain atomic formulas and satisfying certain equivalences, such as A ∧B
belongs to S if and only if both A and B belong to S, then obviously we get no information
about the meaning of the logical constants by being told again that these equivalences hold.
[. . .] We must conclude that truth conditions can serve as meaning explanations only if we
already have a grasp of truth. (Prawitz, 2005, 680)
Therefore, the survey must go deeper than (∧T )—(∃T ) permit. In other words,
it is necessary to question the most fundamental notion of truth by which the
mathematical apparatus itself is inspired. A formal semantics, after all, is the
rigorous report of the overall articulation of a language, the structural counterpart of
a larger theory of meaning based on general, and often implicit, concepts.
In this connection, the aforementioned principle of bivalence is adopted, more or
less tacitly, by the overwhelming majority of the proponents of truth-conditionality.
Is this a plausible option? The idea that every truth-bearer is always determinately
true or false is at the core of Dummett’s criticism. In fact, he has dedicated celebrated
arguments to this perspective, which he described as realistic, aimed at showing its
unsustainability (Dummett, 1978b,a,c, 1996a,b). The discussion on these themes
goes beyond the aims of the present work. It will sufﬁce to note how bivalence
implies that truth or falsity are given by virtue of facts totally unrelated to our
epistemic abilities. The semantic status of propositions or sentences will then be
independent of the possibility to know the circumstances that make them true or
false, and the conditions of truth will transcend the conditions of correctness for
judgments or assertions (see Cozzo, 1994b, 2008). Thus, even if we put aside the
reasons for which Dummett refused such a setting, it is easy to see that this setting
does not even seem, as a matter of principle, to conform to the objective we are
pursuing. If the meaning of propositions or sentences has nothing to do with what
it means for us to use them appropriately in the deductive practice, we are not able
to see how, on account of this meaning, they can grant an epistemic strength to the
proofs or valid arguments in which they are involved.

2.6
Meaning: From Truth to Evidence
31
Therefore, because of these difﬁculties, Prawitz proposes to turning to an
epistemic conception in which
truth is instead determined in terms of what it is for us to acquire knowledge, and sentences
are true in virtue of the potential existence of evidence for them. (Prawitz, 2005, 681)
This move obviously makes the notion of evidence the core of the discussion, to the
detriment of the notion of truth, transforming the original truth-conditional theory
into a theory based on conditions of correct judicability or assertability—it is only
when in possession of an appropriate evidence, that judgments and assertions can
be said to be correct. The old principle of Frege and Wittgenstein can still be reused,
because now
we may go back to the idea that meaning is determined by truth conditions. A more
profound way of accounting for the meaning of a sentence is now opened up, namely, in
terms of what counts as evidence for the sentence. Knowing what counts as evidence for
the sentence, one also knows the truth conditions of the sentence. (Prawitz, 2005, 681)
Through these reﬂections, we have brought into focus some important topics. But
the term “evidence” is obviously too vague. How can we make it more precise?
Prawitz’s semantics aims to answer this question.

Chapter 3
Valid Arguments and Proofs
In this chapter, we take into account Prawitz’s semantics of valid arguments and
proofs. Together with the more recent theory of grounds, these semantics belong
to the ﬁeld of proof-theoretic semantics (the expression is due to Schroeder-Heister
(1991); see also Schroeder-Heister (2006, 2018) and Francez (2015), for a general
historical and conceptual introduction),and have two main sources of inspiration: on
the one hand, some pioneering research by Gentzen; on the other, the fundamental
results that Prawitz himself has obtained with reference to some of Gentzen’s
theories. Along with other ideas stemming from the related intuitionistic tradition,
these two suggestions merge in almost all Prawitz’s semantic investigations. Thus,
the next section is dedicated to the introduction of some technical—and arguably
indispensable—concepts and results.
3.1
Prawitz’s Normalization Theory
In the well-known Untersuchungen über das logische Schließen (Gentzen,
1934–1935),Gentzen outlines two types of formal systems for a ﬁrst-order language
L: a calculus of sequents and a calculus of natural deduction. The latter, to which
we will devote our exclusive attention, is based on the idea of associating to every
logical constant C a rule (of inference) of introduction, in which C occurs as
principal logical constant in the conclusion, and a rule (of inference) of elimination,
in which C occurs as principal logical constant in one of the premises (called major
premise, whereas the others, if any, are called minor premises).
α
β (∧I)
α ∧β
α1 ∧α2 (∧E,i), i = 1, 2
αi
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_3
33

34
3
Valid Arguments and Proofs
αi
(∨I), i = 1, 2
α1 ∨α2
α ∨β
[α]
...
γ
[β]
...
γ
(∨E)
γ
[α]
...
β
(→I)
α →β
α →β
α (→E)
β
α(x)
(∀I)
∀yα(y/x)
∀xα(x) (∀E)
α(t/x)
α(t/x)
(∃I)
∃xα(x)
∃yα(y/x)
[α(x)]
...
β (∃E)
β
In the rules (∨E), (→I) and (∃E), the vertical dots between two formulas indicate
that the second one is understood as obtained in dependence of the ﬁrst; the square
brackets instead indicate that the formula put into them can be discharged, in the
sense that the conclusion no longer depends on this formula. In (∀I), x must not
occur free in any undischarged formula on which α(x) depends; in the same way,
in (∃E) x must not occur free either in β, or in any undischarged formula on which
β depends, other than α(x). In both cases, x is called the proper variable of the
inference and, if y ̸= x, y must not occur free in α(x) and must be free for x in
α(x). In the rules (∀E) and (∃I), t must be free for x in α(x).
On the whole, we have introduced a formal system ML called minimal logic. If
we add a constant symbol ⊥for absurdum to the atomic formulas, and put
¬α
def
= α →⊥,
by adding to ML the rule
⊥(⊥)
α

3.1
Prawitz’s Normalization Theory
35
we obtain a formal system IL for intuitionistic logic. Finally, by adding to IL rules
like
EM
α ∨¬α
¬¬α DN
α
[¬α]
...
⊥RAA
α
PRC
((α →β) →α) →α
we obtain a formal system CL for classical logic. The rules permit us to deﬁne a set
of derivations, which are represented as tree-structures whose nodes are formulas;
the initial nodes are the assumptions of the derivation, while the ﬁnal node is the
conclusion. Here, we will take into account only IL, on a ﬁrst-order reference
language L with a set of terms TERML and a set formulas FORML (ATOML for the
atomic formulas).
Deﬁnition 1 The set DERIL of the derivations of IL is the smallest set X deﬁned
by induction as indicated by the following cases:
•
the single node α ∈X for every α ∈FORML
•
1
α
and
2
β
∈X ⇒
1
α
2
β (∧I)
α ∧β
∈X
•
1
α ∨β ,
α
2
γ
and
β
3
γ
∈X ⇒
1
α ∨β
[α]
2
γ
[β]
3
γ
(∨E)
γ
∈X
•
α

β
∈X ⇒
[α]

β
(→I)
α →β
∈X

36
3
Valid Arguments and Proofs
•

⊥∈X ⇒

⊥(⊥)
α
∈X
•

α(x) ∈X in compliance with the restriction on (∀I) ⇒

α(x)
(∀I)
∀yα(y/x)
∈X
•
1
∃yα(y/x) and
α(x)
2
β
∈X in compliance with the restriction on (∃E) ⇒
1
∃yα(y/x)
[α(x)]
2
β (∃E)
β
∈X
α is derivable from  in IL, indicated  ⊢IL α, if, and only if, there is  ∈DERIL
with set of undischarged assumptions  and conclusion α.
Prawitz (1971, 2006) has obtained a fundamental normalization theorem for IL,
and hence, trivially, for its subsystem ML (as well as for other systems, including
CL itself). Just like the Cut-elimination, proven by Gentzen himself (Gentzen,
1934–1935) for sequents calculi, this theorem permits elimination of the detours
inside the derivations, and reduces the latter to structures with interesting properties
and far-reaching consequences. Leaving aside inessential details, we will limit
ourselves here to introducing simple deﬁnitions and enunciating only the most
relevant results.
Deﬁnition 2 A maximal formula of  is an occurrence of a formula in  which is
a consequence of an application of an introduction rule or of (⊥), and major premise
of an application of an elimination rule.
 is said to be in normal form if, and only if, it does not contain maximal
formulas.
For rules other than (⊥), maximal formulas can be eliminated through appropri-
ate reductions.
1
α1
2
α2 (∧I)
α1 ∧α2 (∧E,i), i = 1, 2
αi
∧i-rid
i
αi

3.1
Prawitz’s Normalization Theory
37

αi
(∨I), i = 1, 2
α1 ∨α2
[α1]
1
β
[α2]
2
β (∨E)
β
∨-rid

[αi]
i
β
[α]
1
β
(→I)
α →β
2
α (→E)
β
→-rid
2
[α]
1
β
(x)
α(x)
(∀I)
∀yα(y/x) (∀E)
α(t/x)
∀-rid
(t/x)
α(t/x)

α(t/x)
(∃I)
∃yα(y/x)
[α(x)]
1(x)
β (∃E)
β
∃-rid

[α(t/x)]
1(t/x)
β
where (t/x) indicates the substitution in (x) with t of every free occurrence
of x in occurrences of formulas in (x). One could reasonably expect that
the application of ∧i-rid, ∨-rid, →-rid, ∀-rid or ∃-rid to  with undischarged
assumptions  and conclusion α generates a new derivation with undischarged
assumptions ∗⊆ and conclusion α. We can achieve this by adopting a simple
convention that, by virtue of some theorems (see, for example, VanDalen, 1994),
does not cause any loss of generality.
Convention 3 In each , (1) free and bound variables are all distinct—property
(FB)—and (2) proper and non-proper variables are all distinct, and each proper
variable is used in at most only one application of (∀I) or (∃E)—property (PN).
As regards (⊥), it will be sufﬁcient to recall the result according to which, if
 ⊢IL α, there is  with set of undischarged assumptions  and conclusion α such
that, for each occurrence of β in , if the occurrence of β is the conclusion of an
application of (⊥), then β ∈ATOML. So we can leave aside the maximal formulas
that are consequences of applications of (⊥), and focus only on those to which an
appropriate reduction is associated.
Theorem 4 (Prawitz) If  ⊢IL α, there is  in normal form with undischarged
assumptions ∗⊆ and conclusion α.
Since ML is a subsystem of IL, theorem 4 extends trivially to minimal logic. In
both cases, the basic strategy for the proof consists in progressively eliminating all

38
3
Valid Arguments and Proofs
the maximal formulas of a derivation (and any new maximal formulas resulting from
the elimination itself) through reiterated applications of ∧i-rid, ∨-rid, →-rid, ∀-rid
and ∃-rid. The latter induce for their part a reducibility relation among derivations,
by virtue of which theorem 4 can be expressed more vigorously by saying that each
derivation of IL (and therefore of ML) reduces to a derivation in normal form.
Beyond an intrinsic technical interest, the normalization result has considerable
philosophical importance and lends itself naturally to semantic discussions. This is
consistent with what Gentzen already seems to suggest, in the following famous
passage:
the introductions represent, as it were, the “deﬁnitions” of the symbol concerned, and the
eliminations are no more, in the ﬁnal analysis, than the consequences of these deﬁnitions.
This fact may be expressed as follows: in eliminating a symbol, we may use the formula
with whose terminal symbol we are dealing only “in the sense afforded it by the introduction
of that symbol”. [. . . ] By making these ideas more precise it should be possible to
display the [elimination inferences] as unique functions of their corresponding [introduction
inferences]. (Gentzen, 1934–1935, 192)
Gentzen’s words can be interpreted in two different but closely linked ways. When
focusing on the idea that elimination inferences are nothing but univocal functions
of the corresponding introduction inferences, with the latter “deﬁning” the symbols
occurring as principal in their conclusions, we arrive at an inversion principle which,
introduced for the ﬁrst time by Lorenzen (1950, 1955), was formulated by Prawitz
as follows:
let a be an application of an elimination rule that has β as consequence. Then, deductions
that satisfy the sufﬁcient condition [. . . ] for deriving the major premise of a, when combined
with deductions of the minor premises of a (if any), already “contain” a deduction of β; the
deduction of β is thus obtainable directly from the given deductions without the addition of
a. (Prawitz, 2006, 33)
As can easily be seen, ∧i-rid, ∨-rid, →-rid, ∀-rid and ∃-rid instantiate the inversion
principle on (∧E,i), (∨E), (→E), (∀E) and (∃E), respectively. Instead, if focusing
on the request that the elimination of a symbol occurring in the major premise
be consistent with the way in which this symbol is “deﬁned” by its introduction,
Gentzen’s thesis can be viewed from a semantic perspective. The introduction
rules ﬁx the meaning of the symbols that occur as principal in the conclusion,
and the elimination rules must be, so to speak, in harmony with this meaning.
From a semantic point of view, ∧i-rid, ∨-rid, →-rid, ∀-rid and ∃-rid can be seen
as justiﬁcations of, respectively, (∧E,i), (∨E), (→E), (∀E) and (∃E). Since the
introduction rules ﬁx the meaning, any derivation that ends with the application of an
introduction rule to “acceptable” derivations will also be “acceptable”. Reductions,
then, by showing how to transform derivations with detours into derivations without
detours, also show the “acceptability” of a derivation that ends with the application
of an elimination rule to “acceptable” derivations. The semantic reading is thus a
faithful generalization of the normalization theorem, a generalization in which the
inversion principle becomes a requisite that every legitimate elimination rule must
fulﬁll. Not without reason, Peter Schroeder-Heister emphasized what he calls the
fundamental corollary of theorem 4: by repeatedly applying ∧i-rid, ∨-rid, →-rid,

3.1
Prawitz’s Normalization Theory
39
∀-rid or ∃-rid to  with assumptions  = ∅and no unbound variables, we arrive at
a derivation that ends with the application of an introduction rule. This result is
philosophically interpreted by requiring that a valid closed derivation be reducible to one
using an introduction inference in the last step. (Schroeder-Heister, 2006, 531)
To conclude, we introduce the notion of ﬁrst-order atomic system. As we will see,
Prawitz’s semantics of valid arguments and proofs relativizes its key notions to bases
consisting of derivations in systems of this type, and an analogous approach can be
adopted for the theory of grounds too—and indeed, we will adopt it explicitly in this
case. More speciﬁcally, we will characterize ﬁrst-order atomic systems through the
so-called Post systems. This is the modus operandi adopted by Prawitz himself.
Given a ﬁrst-order language L, a Post system is a pair ⟨L, ℜ⟩, where ℜis a ﬁnite
set of rules
α1
. . .
αn
β
relative to predicative, functional or individual symbols of L and such that:
•
n ≥0 (when n = 0, the rule is an axiom);
•
for every i ≤n, αi ∈ATOML and αi ̸= ⊥;
•
β ∈ATOML or β = ⊥and, if x occurs free in β and n > 0, there is i ≤n such
that x occurs free in αi.
As an example for what will follow, let N be the set of natural numbers, = the usual
equality relation in N2, s the successor function N →N, and + and · the functions
of addition and multiplication N2 →N. Suppose that the predicative, functional
and individual symbols of L are identical to the correspondent ones on N. Let us
take into account the following rules:
(=R)
t = t
t = u (=S)
u = t
t = u
u = z (=T )
t = z
0 = s(t) (s1)
⊥
s(t) = s(u) (s2)
t = u
(+1)
t + 0 = t
(+2)
t + s(u) = s(t + u)
(·1)
t · 0 = 0
(·2)
t · s(u) = (t · u) + t
As can be seen, (=R), (=S) and (=T ) express the usual properties of reﬂexivity,
symmetry and transitivity of the equality—we will indicate the set of these rules
with EQ; (s1), (s2), (+1), (+2), (·1) and (·2), together with the ﬁrst-order induction
rule

40
3
Valid Arguments and Proofs
α(0)
[α(x)]
...
α(s(x)/x) IND
α(t/x)
express instead the so-called Peano axioms for ﬁrst-order arithmetic—we will
indicate the set of these rules with SAM ∪{IND}. Taking into account also
intuitionistic logic, the system EQ ∪SAM ∪{IND} ∪IL is called Heyting ﬁrst-order
arithmetic—we will indicate this system with HA. Obviously, EQ ∪SAM is a Post
system.
3.2
Semantics Through Arguments and Proofs
Since the 1970s, Prawitz has focused on semantic investigations. His semantics of
valid arguments and proofs were for the ﬁrst time systematized, along varying lines
of thought, in the 1971 article Ideas and results in proof theory, in the 1973 article
Towards a foundation of a general proof theory and in the 1977 article Meaning and
proofs: on the conﬂict between classical and intuitionistic logic.
From the beginning, Prawitz is clearly aware of the existence of at least two
possible approaches. In the 1973 article, for example, he afﬁrms that
one could try to give a direct characterization of different kinds of proofs, where a proof
is understood as the abstract process by which a proposition is established, and then study
how proofs are represented syntactically by derivation. Or alternatively, one could start on
a more concrete level and study the verbal arguments that are intended to convince us of
some state of affairs and then attempt to single out those arguments that are valid and which
thus represent proofs. (Prawitz, 1973, 227)
In the context of a critical and widening discussion of Dummett’s theories, Meaning
and proof: on the conﬂict between classical and intuitionistic logic presents general
considerations on proofs. Ideas and results in proof theory and Towards a foundation
of a general proof theory, by contrast, introduce and articulate the notion of valid
argument. Although not always with equal intensity (see, among others, Tranchini
(2014), where it is rightly said that the setting in terms of valid argument is initially
predominant, and that the situation is reversed precisely with the theory of grounds),
both paths remain, over the years, at the center of Prawitz’s attention, until they end
up merging in Logical consequence from a constructivist point of view.
In the following paragraphs, valid arguments will be deﬁned as in Towards
a foundation of a general proof theory, which differs from Ideas and results in
proof theory essentially in the role attributed to Post systems. We will discuss
proofs starting from Meaning and proofs: on the conﬂict between classical and
intuitionistic logic, by integrating the reﬂections contained therein with other
innovative reﬂections, stemming from Logical consequence from a constructivist
point of view.

3.2
Semantics Through Arguments and Proofs
41
At the end of Chap.2, our problem was to explain by virtue of what valid
arguments and proofs were such as to induce, on those in possession of it, the
epistemic constraint of necessity of thought. Since any answer to such a question
cannot ignore what propositions or sentences mean, we have therefore examined
the nature of this meaning. After detecting the inadequacy of an explanation in
terms of bivalent truth-conditions, we have then appealed to Prawitz’s proposal to
focus on the notion of evidence. What we are going to do now is to deﬁne the
notion of evidence through rigorous deﬁnitions of the notions of valid argument
and proof. Could we not afﬁrm, with good reason, that we have fallen into a circular
explanation? In considering this plausible objection, Prawitz suggests also an escape
route:
there seem to be two clashing intuitions at work here, which also occur in more general
discussion concerning the relation between the meaning and the use of a term [. . . ] what
counts as a proof of a sentence is one feature of the use of the sentence. (Prawitz, 2005,
682)
In this sense, while it is true that Prawitz’s semantics of valid arguments and proofs
originates from Gentzen’s research, and from the normalization theory related to
it, it is equally true that this semantics ﬁts perfectly with the thesis, dating back to
Wittgenstein (1953), according to which meaning is use—or rather, with a certain
interpretation of this thesis.
Wittgenstein’s slogan has indeed received all kinds of interpretations. However,
among the most inﬂuential—and moreover essential for the identiﬁcation of the
desiderata that an adequate theory of meaning must satisfy—there is without doubt
Dummett’s well-known work. Against bivalent truth-conditionality, and in favor
of a veriﬁcationist framework, Dummett (1978b, 1996a,b) has in fact proposed,
among others, arguments based on a manifestability requirement, according to
which knowledge or understanding of meaning must be able to manifest itself
through the use of propositions or sentences within the assertive practice. And
the way in which Prawitz conceives the idea that meaning is use is affected by
Dummett’s inﬂuence. This is for example evident in the aforementioned Meaning
and proofs: on the conﬂict between classical and intuitionistic logic, where the point
is discussed in relation to and framed within the theories of meaning of the English
philosopher:
use is to be taken in its broadest possible sense, i.e. as total use in all its aspects. This is not
to say that the meaning is causally determined by the use, because, conversely, it is equally
reasonable to hold that use is determined by meaning; nor should it be concluded from this
that meaning is identical with use. What is claimed is only that if two expressions are used
in the same way, then they have the same meaning, or if two persons agree completely
about the use of an expression, then they should also agree about its meaning. The principle
could be expressed in another way by saying that the meaning of a sentence must be fully
manifest in some way in its use. [. . . ] The principle that meaning is determined by use does
not preclude the possibility that there is some central feature of sentences other than their
total use that determines or constitutes their meaning; it only demands that a feature that
constitutes meaning is determined by use. (Prawitz, 1977, 3–5)

42
3
Valid Arguments and Proofs
In line with these observations, Prawitz therefore bypasses the difﬁculty previously
highlighted by afﬁrming that
if someone asks why 3 + 1 = 4, a natural answer is that this is what “4” means, or that
this is how “4” is deﬁned and used. Similarly, what can we answer someone who questions
the drawing of the conclusion α →β, given a proof of β from α, except that this is how
α →β is used, it is a part of what α →β means? But a similar answer to the question why
2 + 2 = 4 or why we infer α →β from ¬β →¬α seems inadequate. That 2 + 2 = 4
or that we infer α →β from ¬β →¬α is not reasonably looked upon as a usage that
can be equated with the meaning of the expressions involved, but rather is something that is
to be justiﬁed in terms of what the expressions mean. To answer all doubts about a certain
usage of language by saying that this is how the terms are used, or that this is a part of their
meaning, would be a ludicrously conservative way of meeting demands for justiﬁcation.
But for some such doubts the reference to common usage is very reasonable and may be the
only thing to resort to. (Prawitz, 2005, 682)
What are, then, valid arguments and proofs? A prima facie satisfactory answer
could be formulated in purely syntactic terms: valid arguments and proofs are
nothing but derivations in an appropriate, sufﬁciently powerful, formal system.
But Gödel’s incompleteness theorems reveal the inadequacy of such a conception.
In addition, an arbitrary choice of axioms and rules would prevent the required
linking of the notions of valid argument and proof to the meaning of propositions
or sentences, and from justifying valid arguments and derivations based on this
meaning:
a deductive system is [. . . ] an attempt to codify proofs within a given language, but when
setting up such a system, one does not ordinarily try to analyze what makes something a
proof. Nor does proof theory ordinarily try to justify a deductive system except for trying to
prove its consistency. (Prawitz, 2005, 683)
3.2.1
Valid Arguments (in 1973)
The derivations of IL—as in Deﬁnition 1—are tree-structures; the nodes correspond
to occurrences of formulas of a ﬁrst-order language, and the branches reﬂect the
application of (∧I), (∧E,i), (∨I), (∨E), (→I), (→E), (∀I), (∀E), (∃I), (∃E) or
(⊥). However, the argumentative practice is not obviously limited to any ﬁxed set
of rules. For this reason, when developing his notion of valid argument, Prawitz
places no restriction on the type of inferences used in deduction.
He therefore starts from a generic notion of argument. An argument will still
be a tree-structure having as its nodes the occurrences of formulas of a ﬁrst-order
language L; inferences, however, should be now understood as any ﬁgures of the
type
1
. . .
n α1, ..., αm, x1, ..., xs
β
where 1, ..., n are in turn arguments, α1, ..., αm occurrences of assumptions
discharged by the inference, and x1, ..., xs occurrences of free variables bound

3.2
Semantics Through Arguments and Proofs
43
by the inference—otherwise known as proper variables. As a convention, we will
admit that an inference that binds variables is always such that its proper variables do
not occur free either in the conclusion of the inference, or in any of the undischarged
assumptions on which such a conclusion depends. By rule of inference we mean
simply a set of inferences, called applications of the rule; two rules of inference are
said to be disjoint if, and only if, they have an empty intersection.
The initial nodes of an argument are its assumptions, with the ﬁnal node
being instead its conclusion; an argument will be called open when it contains
undischarged assumptions or occurrences of free variables not later bound, and
closed otherwise. As a further request, we will take for granted that all arguments
enjoy the properties (FB) and (PN) according to convention 3. An example of an
open argument  is an argument obtained from  through a substitution σ of free
variables with terms, and of undischarged assumptions with arguments for such
assumptions. Given an argument  ending in an inference of the kind shown above,
we will say that each i (i ≤n) is an immediate subargument of ; a subargument
of  is an initial segment of —namely an element of the reﬂexive and transitive
closure with respect to  of the relation “1 is an immediate subargument of 2”.
As previously mentioned, the notion of valid argument is relativized by Prawitz
to atomic bases and justiﬁcations. An atomic base B is a pair ⟨{K, F, R}, S⟩with K
a set of individual symbols, F a set of functional symbols, and R a set of predicative
symbols—all three such as to contain the individual, functional and predicative
symbols of L—and S a Post system with rules relative to the elements of K, F and R.
It should be noted that S determines also a corresponding set of atomic derivations.
With B-term and B-derivation we will indicate, respectively, a term consisting of
elements of K, F and R, and a derivation in S. A base ⟨{K, F, R}, S⟩is said to be
consistent if, and only if, ⊬S ⊥.
As we have seen, the introduction rules for IL can be understood as determining
or, in a weaker sense, mirroring the meaning of the logical constants to which they
refer. In light of this, an argument ending with an application of an introduction rule
to valid arguments of an appropriate type can be said to be
valid by the very meaning of the logical constants when understood constructively [. . . ].
And conversely, it must be possible to bring a valid argument for a compound formula into
one of these forms. (Prawitz, 1973, 232)
What shall we say, then, of the (applications of) non-introductory rules and
of arguments ending with them, or in which they occur? The idea behind the
relativization of the notion of valid argument to justiﬁcations is that of considering
the latter as a sort of generalization of the reductions associated with (∧E,i), (∨E),
(→E), (∀E) and (∃E). More speciﬁcally, a justiﬁcation J of a set of inference rules
ℜ, each one disjoint from every introduction rule, is a set of constructive functions
f , each of which is associated with a single element R of ℜand such that (1) f
is deﬁned on some set of arguments ending with applications of R; (2) if f is
deﬁned on  with assumptions  and conclusion α, f () is an argument with
assumptions ∗⊆ and conclusion α; and (3) if f is deﬁned on , and σ() is
an example of , f is deﬁned on σ() and is linear with respect to σ—namely,

44
3
Valid Arguments and Proofs
f (σ()) = σ(f ()). A consistent extension J+ of J is a justiﬁcation of a set of
rules of inference ℜ+, such that: (1) ℜ⊆ℜ+; and (2) (ℜ+ −ℜ) ∩ℜ= ∅.
We can assume as sufﬁciently clear a notion of substitution of a subargument 1
of  with an argument 2, in symbols [2/1]. Given a justiﬁcation J of a set of
inference rules ℜ, 1 is said to immediately reduce to 2 with respect to J if, and
only if, there exists a subargument  of 1 such that, for some f ∈J, f is deﬁned
on  and 2 = 1[f ()/]. 1, 2, . . . is said to be a reduction sequence with
respect to J if, and only if, for every i ≤n where n is the length of the sequence if
the latter ends, and for every i ∈N otherwise, i reduces immediately to i+1 with
respect to J. 1 reduces to 2 with respect to J if, and only if, there is a reduction
sequence with respect to J starting with 1 and ending with 2.
Deﬁnition 5  is valid with respect to J and B—or, more simply, (, J) is valid
with respect to B—if, and only if,
•
 is a closed argument the conclusion of which is an atomic formula ⇒
reduces with respect to J to a B-derivation, or
•
 is a closed argument the conclusion of which is not an atomic formula
⇒ reduces with respect to J to an argument ending with the application of
an introduction rule, and the immediate subarguments of which are valid with
respect to J and B, or
•
 is an open argument ⇒every (σ, J+) is valid with respect to B, where
J+ is a consistent extension of J, and σ a closed example of  obtained by
ﬁrst substituting all the occurrences of free variables not later bound in  with
closed B-terms, so as to get an argument 1, and then by substituting all the
assumptions not discharged in 1 with closed arguments for such assumptions,
valid with respect to J+ and B.
(, J) is valid if, and only if, for every B, (, J) is valid with respect to B.1
As we can see, Deﬁnition 5 proceeds by simultaneous recursion. The notion of
closed valid argument for non-atomic formulas of complexity k presupposes the
notion of open valid argument for formulas of complexity h < k, the latter in turn
obtained, at the same complexity, in the terms of the notion of valid closed argument.
Therefore, the recursive character depends, in a substantial way, on the fact that in
1 In the 1971 article Ideas and results in proof theory, Prawitz takes into account extensions of
atomic bases: given an open argument , (, J) is valid with respect to B if and only if each
(σ , J+) is valid with respect to B+, where J+ is a consistent extension of J, B+ an extension
of B, and σ a closed example of  obtained by substituting ﬁrst all the occurrences of free
variables not later bound in  with closed B+-terms, so as to get an argument 1, and then all
the assumptions not discharged into 1 with closed arguments for such assumptions valid with
respect to J+ and B+. This kind of deﬁnition has the advantage of producing a monotonic notion
of validity with respect to extensions of the atomic base (see, among the others, Schroeder-Heister,
2006 and Tranchini, 2014). However, to authorize a change of an atomic base obviously means
to give up the idea that the latter determines the meaning of terms and formulas of the reference
language. The contemporary works on valid arguments refer alternately both to the 1971 deﬁnition,
and to that of 1973. In any case, in later works Prawitz seems to have deﬁnitively opted for the latter.

3.2
Semantics Through Arguments and Proofs
45
an (application of an) introduction rule, the premises are immediate subformulas
of the conclusion, so as to have a lower complexity than the latter. As a further
observation, Prawitz points out how
the deﬁnition of validity is not a reductive explanation of the logical constants. Rather, it
deﬁnes a property of justiﬁed arguments that must apply when the argument represents a
proof. (Prawitz, 1973, 236)
This does not mean, however, that it is not possible to use Deﬁnition 5 to derive
clauses that show the meaning of logical constants. In particular, a closed argument
 valid with respect to a justiﬁcation J and to an atomic base B will be such that, if
its conclusion is atomic, it reduces to a B-derivation, and
(∧A)
the conclusion of  is α ∧β ⇒ reduces with respect to J to a
closed argument ending with an application (∧I) and whose immediate
subarguments are closed arguments for α and β respectively, valid with
respect to J and B;
(∨A)
the conclusion of  is α1 ∨α2 ⇒ reduces with respect to J to a
closed argument ending with an application of (∨I) and whose immediate
subargument is a closed argument for αi (i = 1, 2), valid with respect to J
and B;
(→A)
the conclusion of  is α →β ⇒ reduces with respect to J to a
closed argument ending with an application of (→I) and whose immediate
subargument is an open argument 1 with undischarged assumption α and
conclusion β such that, for every consistent extension J+ of J, for every
closed argument 2 for α valid with respect to J+ and B, the closed
argument 1[2/α] for β is valid with respect to J+ and B;
(∀A)
the conclusion of  is ∀yα(y/x) ⇒ reduces with respect to J to a
closed argument ending with an application of (∀I) and whose immediate
subargument is an open argument (x) with x unbound in (x) and
conclusion α(x) such that, for every closed B-term t, the closed argument
(t) for α(t/x) is valid with respect to J and B;
(∃A)
the conclusion of  is ∃xα(x) ⇒ reduces with respect to J to a
closed argument ending with an application of (∃I) and whose immediate
subargument is, for some closed B-term t, a closed argument α(t/x) valid
with respect to J and B.
Here it should also be noted that, if B is consistent, there cannot be a closed
argument for ⊥valid with respect to some J and to B. In fact, if there were such
an argument, it should reduce with respect to J to a B-derivation of ⊥, which
contradicts the assumed consistency of B. Therefore, if we assume that atomic bases
are always consistent, we can introduce a further clause that ﬁxes the meaning of
the atomic constant ⊥: there is no closed valid argument for ⊥on any base.
As is evident from the way they are represented, arguments are chains of
inferences. The notion of argument, in other words, depends on that of inference.
But when examining the link between validity of arguments and validity of

46
3
Valid Arguments and Proofs
inferences, Prawitz reverses the situation; the notion of valid inference is deﬁned
in the terms of the notion of valid argument.
Deﬁnition 6 A set of inference rules ℜis valid with respect to J and B if, and only
if, for every consistent extension J+ of J, for every application  of the type
1
. . .
n α1, ..., αm, x1, ..., xs
β
of an element of ℜ, if, for every i ≤n, i is valid with respect to J+ and B,  is
valid with respect to J+ and B. A set of inference rules ℜis valid with respect to J
if, and only if, for every B, ℜis valid with respect to J and B.
In light of Deﬁnition 6, we can therefore say that an inference is valid with respect
to a justiﬁcation J and an atomic base B in the case that it instantiates an inference
rule belonging to a set of inference rules valid with respect to J and B. Similarly,
the inference is valid with respect to J when it is valid with respect to J and to every
B. Both the validity of the inference rules and that of the inferences themselves
are, so to say, deﬁned in a “global” way: they depend on the validity of the entire
arguments, which rules and inferences, respectively, are applied to or occur in.
3.2.2
Proofs (in 1977 and 2005)
As we have already said, the article Meaning and proofs: on the conﬂict between
classical and intuitionistic logic deals with the notion of proof starting from an
analysis of the arguments that Dummett puts forward in support of the thesis
according to which
intuitionistic rather than classical logic describes the correct forms of reasoning within
mathematics. (Prawitz, 1977, 2)
Although this is not the place for dealing, if only in a generic way, with Dummett’s
arguments (for which, see Cozzo, 1994b, 2008), we have to remember, however,
that they are framed in general semantic considerations
according to which the meaning of a sentence must be understood in terms of the use of the
sentence. [. . . ] the meaning of a sentence cannot be treated in isolation from the question of
how the truth of the sentence may be established, and in the case of mathematics especially,
this means that meaning has somehow to be understood in terms of proofs. (Prawitz, 1977,
2–3)
As stated above, the connection between meaning and use does not preclude, but
rather suggests, the identiﬁcation of a central aspect of propositions or sentences on
which the explanation can be based. More speciﬁcally, Dummett makes a distinction
between
two aspects of the use of an assertive sentence: (1) the conditions under which it can be
correctly asserted and (2) the commitments made by asserting it. In the case of mathematics,

3.2
Semantics Through Arguments and Proofs
47
aspect (1) is expressed in the rules for inferring the sentence, and aspect (2) in the rules for
drawing consequences from the sentence. (Prawitz, 1977, 7)
On these bases, and dissatisﬁed with the semantics centered on the notion of bivalent
truth, Dummett (1978b,c, 1996a,b) develops a theory that explains meaning in
terms of conditions of correct assertability. Thus, in order to capture the necessity
of thought involved in proofs and valid arguments, Prawitz proposes adopting an
epistemic conception of truth, and this implies, as its ﬁnal result, the attribution of
a central role to the notion of evidence. Now, if in this framework we reason more
speciﬁcally in terms of valid arguments, we can appeal to the clauses (∧A)—(∃A).
But how to behave in the case of proofs?
An answer to this question is given by the intuitionistic tradition, and in particular
by the idea, inspired by Brouwer’s theories and independently developed by Heyting
(1956) and Andrej Nikolaevic Kolmogorov (1932), to ﬁx the meaning of the
logical constants through a speciﬁcation of the notion of proof by induction on
the complexity of the formulas of a ﬁrst-order logical language—the clauses so
obtained are known by the initialism BHK, corresponding to the names of their
authors. We may assume to once again have an atomic base B with individual,
functional and relational symbols, and a Post system relative to them. Therefore, a
proof with respect to B of an atomic formula is again a B-derivation and, under the
usual assumption that there is no proof of ⊥,
(∧P )
a proof with respect to B of α ∧β is a proof π1 with respect to B of α and
a proof π2 with respect to B of β;
(∨P )
a proof with respect to B of α ∨β is a proof π with respect to B of α or of
β, with an indication of which of the disjoints π proves;
(→P )
a proof with respect to B of α →β is a constructive procedure f such
that, for every proof π with respect to B of α, f (π) is a proof with respect
to B of β;
(∀P )
a proof with respect to B of ∀xα(x) is a constructive procedure f such that,
for every closed B-term t, f (t) is a proof with respect to B of α(t);
(∃P )
a proof with respect to B of ∃xα(x) is a proof π with respect to B of α(t)
for some B-term t.
As noted by Prawitz himself, and by Rosza Peter (1959) before him, the notion of
constructive procedure involved in the clauses (→P ) and (∀P ) must be assumed
here as primitive, since it is not possible to
deﬁne it as a Turing machine that always yields a value when applied to an argument; the
quantiﬁer preﬁx [. . . ] in this deﬁnition must then be understood intuitionistically, and this
means that to understand the deﬁnition we must already know what such a constructive
procedure is. (Prawitz, 1977, 27)
The ﬁrst problem with the BHK clauses, however, is that they do not offer an
exhaustive description of the conditions of correct assertability. In other words, some
structures prove formulas with main logical constant c, although they do not have
the form expected with (cP ):

48
3
Valid Arguments and Proofs
it is not true even intuitionistically that the condition for asserting a sentence is that we know
a proof of it in [the sense of the BHK clauses]. This can be seen even for atomic sentences
such as 768 + 859 = 859 + 768, when the given proofs of atomic sentences consist of
proofs following the usual rules of computation. The proof of the mentioned equality would
then proceed via the calculation of the sums in question. But we are perfectly justiﬁed in
asserting the equality without knowing these sums and hence without knowing the proof in
question; it is sufﬁcient that we know, for instance, a proof of ∀x∀y(x + y = y + x) and
then infer the equality by instantiation. [. . . ] Similarly we may assert even intuitionistically
that α(n) ∨β(n) for some numeral n without knowing a proof of α(n) or of β(n); it would
be sufﬁcient, e.g., if we know a proof of α(0) ∨β(0) and of ∀x(α(x) ∨β(x) →α(x + 1) ∨
β(x + 1)). (Prawitz, 1977, 21)
Hence, BHK semantics can be a good starting point, although it is not adequate
by itself for the desired objectives. In fact, it deﬁnes sufﬁcient but not necessary
proof-conditions, and
this would be quite insufﬁcient for a meaning theory as proposed above, e.g., we would then
never be in the position to say that a sentence was incorrectly asserted on a given occasion.
(Prawitz, 1977, 22)
In order to unravel the difﬁculty, Prawitz follows Dummett in introducing a
distinction between direct and indirect—or, using a widespread terminology, canon-
ical and non-canonical—forms of proofs, perfectly symmetrical to the distinction
between valid arguments ending with applications of introduction rules, and valid
arguments which, ending with applications of non-introductory rules, must be
reduced to valid arguments ending with applications of introduction rules:
the condition for asserting a sentence is that we either know a proof of the sentence of the
kind mentioned in [the BHK clauses] or know a procedure for obtaining such a proof. This
procedure may also be called a proof [. . . ] but it is a proof in a secondary sense. (Prawitz,
1977, 22)
In this way, we can reuse the BHK characterization by saying that it outlines the
notion of canonical proof with respect to the chosen atomic base B, and postulate
that a non-canonical proof with respect to B is a constructive procedure to obtain
a canonical proof with respect to B. (∧P )—(∃P ) are now sufﬁcient to explain the
meaning of propositions or sentences, and consequently of the logical constants, in
terms of conditions of canonical assertability. Non-canonical assertability is then
deﬁned in terms of the canonical assertability, making this latter the sole central
notion of our theory.
At this point, the proposed framework suggests a natural criterion of acceptability
of inferences with respect to B:
what we should demand is that we know a [. . . ] procedure which, applied to the way in
which the conditions for asserting the premises are satisﬁed, brings about a situation in
which the conclusion can be asserted appropriately. (Prawitz, 1977, 23)
As Prawitz himself points out, such a condition will be satisﬁed if, and only if, we
know a constructive procedure f such that, given that α1, . . . , αn are the premises
and β the conclusion of the inference, for each canonical proof πi with respect to B
of αi, f (π1, . . . , πn) produces a canonical proof with respect to B of β. Necessity

3.2
Semantics Through Arguments and Proofs
49
can be proved in the following way: if we know canonical proofs π1, . . . , πn with
respect to B of α1, . . . , αn, respectively, the conditions to correctly assert α1, . . . , αn
are satisﬁed. Therefore, given a procedure f that makes it possible to correctly
assert β when the conditions to correctly assert α1, . . . , αn are met, f (π1, . . . , πn)
will produce a canonical proof with respect to B of β. As for sufﬁciency, suppose
that the conditions to assert α1, . . . , αn are satisﬁed by virtue of the knowledge of
proofs f1, . . . , fn with respect to B of α1, . . . , αn respectively; each fi must be a
constructive procedure for obtaining a canonical proof with respect to B of α, so that
we can deﬁne a constructive procedure ex of execution of each fi such that ex(fi)
is a canonical proof with respect to B of α. By hypothesis, f (ex(f1), . . . , ex(fn))
produces a canonical proof with respect to B of β, which justiﬁes the assertion of
β. We can ﬁnally say that an inference (rule) is valid with respect to an atomic base
B if, and only if, (for each inference that is an application of this rule) there is a
situation in which the inference is acceptable with respect to B. An inference (rule)
is valid if, and only if, for each B, it is valid with respect to B. As in the case of
valid arguments, the notions of acceptability of an inference and of validity of an
inference (rule) are globally deﬁned, so that priority is given to proofs.
The notion of proof we have dealt with so far is updated and expanded in Logical
consequence from a constructivist point of view. In 2005, Prawitz proposes ﬁrst of
all rewriting the BHK clauses, following an intuition already present but not made
explicit in 1977:
we should not say simply that a canonical proof of, e.g., α ∧β consists of a canonical proof
of α and a canonical proof of β. It is not enough that we have just constructed these two
canonical proofs separately to be in the position to assert α ∧β—they entitle us only to
assert α and to assert β. [. . . ] we must also be aware of the fact that these two proofs form
a sufﬁcient ground to go one step further and assert α ∧β. Or, more precisely, one should
grant the existence of an operation which yields a canonical proof of α ∧β when performed
on canonical proofs of α and β. (Prawitz, 1977, 26)
the general form of a canonical proof for a compound sentence α with the logical constant c
as main sign can be written Oc(π), where Oc is an operation that stands for the recognition
that we have obtained direct evidence for α because of π. (Prawitz, 2005, 686)
The clauses, however, are now derivable from a broader framework, similar to
that of valid arguments. Let us start with a general notion of proof-structure on a
ﬁrst-order language L, understood as a concatenation of constructive procedures,
applied to appropriate arguments, which represent inference rules and inferences
on the formulas of L. A proof-structure will have some assumptions, given by the
basic arguments of the composite procedure resulting from the concatenation of
procedures to which it corresponds, and a conclusion, given by the range of the
composite procedure resulting from the concatenation of procedures to which it
corresponds. Some procedures can discharge assumptions or bind occurrences of
free variables, so that a proof-structure will be open when it contains undischarged
assumptions or occurrences of free variables not subsequently bound, or otherwise
closed. An immediate substructure of a proof-structure π is a proof-structure that
appears as an argument of the last procedure of π; a substructure of π is an initial

50
3
Valid Arguments and Proofs
segment of π, i.e. an element of the reﬂexive and transitive closure with respect
to π of the relation “π1 is an immediate substructure of π2”. A proof-structure is
canonical when it ends with the application of a procedure corresponding to an
introduction rule, and non-canonical otherwise.
In compliance with the idea that the introduction rules ﬁx or, more weakly, reﬂect
the meaning of the logical constant they refer to, a canonical proof-structure will
be a canonical proof assuming that its immediate substructures are such, while a
non-canonical proof-structure will be a categorical proof (equivalent to the non-
canonical proofs of 1977) in the case that it amounts to a constructive procedure
to obtain a canonical proof. The notions of canonical proof and of categorical
proof, however, must be deﬁned by simultaneous recursion, since, in general, the
arguments of the last procedure of a canonical proof could take a non-canonical
form. This appears to be possible because the premises of (an application) of an
inference rule are immediate subformulas of the conclusion, and therefore they have
a lower complexity than the latter. However, we also need the additional deﬁnitions
of hypothetical proof and general proof. As usual, the whole of these deﬁnitions
refers to an atomic base B with individual, functional and relational symbols, and to
a Post system relative to the latter (the derivations of which will be, by assumption,
canonical proofs), and ﬁnally with the usual request that there should not be any
canonical proof of ⊥.
Deﬁnition 7 A closed canonical proof-structure is a canonical proof on B if, and
only if, all its immediate substructures are categorical, hypotheticalor general proofs
on B.
A closed non-canonical proof-structure is a categorical proof on B if, and only
if, it is a constructive procedure to obtain a canonical proof on B.
An open proof-structure with assumptions α1, . . . , αn and conclusion β is a
hypothetical proof on B if, and only if, it is a constructive procedure f such that, for
each πi categorical proof on B of αi (i ≤n), f (π1, . . . , πn) is a categorical proof
on B of β.
An open proof-structure with occurrences of free variables x1, . . . , xn that are
not subsequently bound and conclusion α(x1, . . . , xn) is a general proof on B if,
and only if, it is a constructive procedure f such that, for each closed B-term ti
(i ≤n), f (t1, . . . , tn) is a categorical proof on B of α(t1, . . . , tn).
A proof-structure is a proof if, and only if, for every B, it is a proof on B.
We can now easily obtain the clauses by introducing primitive procedures O∧,
O∨, O→, O∀and O∃for the rules (∧I), (∨I), (→I), (∀I) and (∃I) respectively—
with the implicit indication of appropriate discharge of assumptions, binding of
occurrences of free variables, and restrictions on proper variables.
(∧P
P )
a canonical proof of α ∧β is O∧(π1, π2) with π1 categorical proof of α and
π2 categorical proof of β;
(∨P
P )
a canonical proof of α ∨β is either O∨(π1) with π1 categorical proof of α,
or O∨(π2) with π2 categorical proof of β;

3.3
Three Problems
51
(→P
P )
a canonical proof of α →β is O→(πα
β ) with πα
β hypothetical proof with
assumption α and conclusion β;
(∀P
P )
a canonical proof of ∀xα(x) is O∀(π(x)) with π(x) general proof with
occurrence of free variable not subsequently bound x and conclusion α(x);
(∃P
P )
a canonical proof of ∃xα(x) is O∃(t, π) with t term on the reference base
and π categorical proof of α(t).
3.3
Three Problems
At the end of Chap. 2, we outlined two goals that an analysis of the notion of α being
a consequence of  by necessity of thought must achieve. First, to deﬁne rigorously
the notions of valid argument or proof, so as to translate with any precision the link
of epistemic modality existing between α and . Thanks to the formal apparatus
of 1973, 1977 and 2005, we can consider this point satisﬁed. Now, a question of
adequacy arises: given deﬁnitions 5 and 7, can we say that the notions of valid
argument and proof that they characterize are such that, if we know a valid open
argument or a hypothetical proof for α from , we are compelled to consider α true,
if we have accepted the truth of all elements in ?
3.3.1
Proofs-as-Chains
With this question, Prawitz concludes Logical consequence from a constructivist
point of view. He ﬁrst of all notes that
some inferences - namely, the inferences by introduction - become valid by the very
meaning of the conclusion of the inference. Because of this meaning, we are compelled
to hold the conclusion true when holding the premises true. [. . . ] An inference in general is
compelling when we know a hypothetical proof of its conclusion α from its set of premises
 or, alternatively, an open valid argument for α from . [. . . ] knowing such a proof is to be
in possession of an effective method which, applied to categorical proofs of the sentences
of , yields a categorical proof of α. [. . . ] knowing a valid open argument  for α from
, we get a valid argument for α by replacing the open assumptions in  with closed valid
argument for them. (Prawitz, 2005, 693)
He then emphasizes signiﬁcantly how, since the compelling character of open
valid arguments and hypothetical proofs is based on their reduction to closed valid
arguments and categorical proofs, we should more generally expect our analysis to
imply that
by knowing a proof or valid argument, one gets committed in the way discussed above.
(Prawitz, 2005, 693)
One could reasonably argue that valid arguments and proofs induce necessity of
thought on an agent who has knowledge of them because, by carrying them out, the

52
3
Valid Arguments and Proofs
agent makes a series of valid inferences. With the transmission of the justiﬁcation
from premises to conclusions, these inferences compel one to accept the latter if one
accepts the former. This is a dynamic vision, in which the epistemic constriction
comes from the carrying out of certain acts; and it is in agreement with a plausible
intuitive point of view, but also with the phenomenal character of active experience
that Cozzo (2015), as we have said, properly attributes to necessity of thought.
However, such an explanation
puts the burden on the notion of valid inference. How is it to be analyzed? If we do not
simply say that an inference is valid when its conclusion is a logical consequence of the
premises, which would bring us back to the beginning of this investigation, we have to try
to develop some concept of “gapfree” inference. The “gapfree” inferences must then be
shown to have a compelling force. (Prawitz, 2005, 693)
At a ﬁrst view, a correct strategy could consist in afﬁrming that an inference is valid
if, and only if, it can be reduced to valid “gapfree” inferences. But
to deﬁne validity in that way would of course have made the whole analysis circular.
(Prawitz, 2005, 693)
Since the circularity depends on the attribution of validity—what we intend to
deﬁne—to “gapfree” inferences, an obvious alternative could consist in starting
from a deﬁnition of valid “gapfree” inference, and then require that a non-“gapfree”
inference be valid if, and only if, it can be reduced to valid “gapfree” inferences. The
introductory inferences are involved in the explanation of the meaning of the logical
constants, so they can be understood as being valid “gapfree” by default. Naturally,
if they were the only valid “gapfree” inferences, the fact that their premises always
have a lower complexity than the conclusion would allow an inductive, and for
this reason satisfactory, deﬁnition of the notion of valid inference. Obviously,
the problem is that we cannot assume every valid inference as being reducible
only to introduction inferences: there will be valid “gapfree” inferences in a non-
introductory form—for example, Gentzen’s eliminations. How can we deﬁne the
validity of the latter? At the end of Logical consequence from a constructivist point
of view, Prawitz says that
our basic intuition is that of canonical proof or argument [. . . ] this amounts to making
inferences by introduction valid - valid by deﬁnition, so to say [. . . ] there are inferences
other than introductions that are gapfree in the sense that they cannot be reasonably be
broken down into simpler inferences. An essential ingredient in the analysis proposed here
is a way of demonstrating the validity of such inferences [. . . ] by applying reductions that
are seen to transform arguments into ones that are known to be valid. (Prawitz, 2005, 693–
694)
In this passage, Prawitz preserves the explanatory primacy of valid arguments and
proofs. This, however, could pose a new problem. Since valid arguments and proofs
are composed of epistemically compelling valid inferences, how can we attribute an
epistemic power to inferences the validity of which is explained by using the notions
of valid argument and proof? This point already holds, in an important way, in the
case of inferences in an introductory form. Suppose the epistemic force of a closed
valid canonical argument

3.3
Three Problems
53
1
α
2
β (∧I)
α ∧β
or of a canonical proof O∧(π1, π2) are explained in terms of their being composed
solely of epistemically compelling valid inferences. If we say now that the last
inference is epistemically compelling since it is valid—where valid means that it
produces a valid closed canonical argument when applied to 1 and to 2, or
a canonical proof when applied to π1 and to π2—we could conclude that this
characterization is satisfactory only if we have already accepted as epistemically
compelling the starting argument and the starting proof. Our explanation seems to
be patently circular.
What we have just said is clearly not intended to deny that the respective
applications of (∧I) and O∧are epistemically compelling; they produce something
that the reference semantics treats as evidence—a closed valid canonical argument
or a canonical proof, indeed. But the problem is rather to show that closed valid
canonical arguments and canonical proofs can quite rightly be understood as
producing evidence by virtue of their being made up of valid inferences. In order
to achieve an adequate explanation of the intended relation between the notions of
valid argument or proof and of valid inference, there thus seems to be no other
possibility than to deﬁne the latter on a basis independent from the former. And
this, of course, requires a local analysis of valid inferences, as opposed to the global
character of that available in Prawitz’s semantics of valid arguments and proofs.
The transition from global to local is indeed one of the basic insights of the
theory of grounds. However, in the framework under examination, it does not seem
to be sufﬁcient. The most immediate difﬁculty already comes from what, following
Usberti (2015), we may call the proofs-as-chains conception: the epistemic power of
valid arguments and proofs can reasonably be considered as dependent on the force
of the valid inferences involved in them, only to the extent that valid arguments and
proofs are composed exclusively of valid inferences.
A ﬁrst problem here is that the statement according to which Prawitz’s valid
arguments and proofs are chains of valid inferences is simply false. It is indeed very
easy to ﬁnd valid arguments and proofs that contain invalid inferences. The point is
remarked by Prawitz himself, with reference to valid arguments:
a justiﬁed argument schema (, J) is clearly [. . . ] valid [. . . ] if the inferences in  that are
not instances of introduction rules [. . . ] are instances of rules in a set ℜwhich is [. . . ] valid
with respect to J [. . . ]. The converse of this does not hold, however. (Prawitz, 1973, 241)
So, suppose we have a closed argument 1 for α valid with respect to a justiﬁcation
J such that ∧1-rid ∈J, and a closed argument 2 for β which contains an
application of an inference rule R such that R is not valid with respect to J: the
argument
1
α1
2
α2 (∧I)
α1 ∧α2 (∧E,1)
α1

54
3
Valid Arguments and Proofs
is valid with respect to J. In fact, by applying ∧1-rid, it reduces with respect to J to
the argument 1, valid with respect to J by assumption. In the case of proofs, the
reasoning amounts to the same. Let us assume that d1 is a proof for α1, d2 a proof-
structure with conclusion α2 containing constructive procedures that represent non-
valid inferences, and f a non-primitive constructive procedure deﬁned on elements
of the type O∧(π1, π2) by the equation
f (O∧(π1, π2)) = π1.
As a consequence f (O∧(d1, d2)) is a non-canonical or categorical proof for α1. By
deﬁnition, f (O∧(d1, d2)) = d1; by assumption, d1 is a canonical proof of α1, or
a constructive procedure to get a canonical proof of α1. But then, f (O∧(d1, d2))
too is a constructive procedure to obtain a canonical proof of α1. Therefore, a proof-
structure π can be a proof even if some of the constructive procedures it is composed
of represent non-valid inferences.
There is, however, a second, deeper shortcoming, stemming from the overall
intertwinement of the concepts of valid inference, valid argument and proof. The
circularities highlighted above, from which we have started our discussion, holds
after all also when the valid arguments and proofs we take into account are made
up of only valid inferences. And the problem now becomes that any attempt to
deﬁne valid inferences based on notions other than those of valid argument or proof,
as we required, seems to encounter difﬁculties because of the distinction between
canonical and non-canonical cases. Let us see why.
It is certainly right to consider introduction inferences as being valid “gapfree”
by default, and then to deﬁne as valid the “gapfree” ones that are non-divisible in
valid “gapfree” inferences. Since inferential validity must be deﬁned independently
of the notions of valid argument and proof, we could then think of dealing with
the case of the “gapfree” inferences in a non-introductory form by saying that they
are valid if, and only if, whenever they are applied to chains of valid “gapfree”
inferences, the resulting inferential concatenation can be reduced to one ending with
an introduction inference applied to a chain of valid “gapfree” inferences. If we
adopt this strategy, however, our deﬁnition of validity for inferences
[α]

β
(→I)
α →β
or represented by proof-structures O→(πα
β ), must require that  and πα
β be chains
of valid “gapfree” inferences. But we are making a distinction between inferences in
an introductory form and inferences in a non-introductory form, between canonical
cases and non-canonical cases, so we cannot exclude that  and πα
β contain valid
“gapfree” inferences in a non-introductory form, and, consequently, inferences of
the same type, having an identical or even greater complexity than that we are
deﬁning as valid. Our deﬁnition would be circular once again.

3.3
Three Problems
55
What we are saying is emphasized by Usberti (2015), who maintains that the
problem we are dealing with here does not apply to the notion of BHK proof, so
that the latter could be suitable for playing the role of central notion in a theory of
meaning based on conditions of correct assertability. In particular, the BHK clauses
proceed obviously by induction on the complexity of the formulas and, in the case of
→and ∀, they abstract from the intrinsic complexity of the constructive procedures
involved. But we have also said that these clauses set sufﬁcient but not necessary
conditions of provability. This means in turn that, although potentially suitable for
the explanation of the meaning,
Heyting’s proofs cannot be seen [. . . ] as chains of valid inferential acts. (Usberti, 2015, 417)
However, the idea of distinguishing between canonical and non-canonical proofs,
and considering the BHK clauses as a description of the sole notion of canonical
proof, transforms the initial induction into a simultaneous recursion. Here, we can
no longer abstract from the intrinsic complexity of the constructive procedures, so
that the immediate subproofs of proofs for implications and universal quantiﬁcations
are to be conceived as chains of valid “gapfree” inferences. Then, we could now
deﬁne as valid introductions of →and ∀in connection with hypothetical or general
proofs only consisting of valid “gapfree” inferences, but
this is just what cannot be done, on pain of being exposed to an objection of circularity.
(Usberti, 2015, 418)
Naturally, Usberti takes care to remember a passage from Gentzen’s Untersuchun-
gen, in which this theme is perhaps for the ﬁrst time focalized:
in interpreting α →β in this way, I have presupposed that the available proof of β from
the assumption α contains merely inferences already recognized as permissible. On the
other hand, such a proof may contain other →-inferences and then our interpretation breaks
down. For it is circular to justify the →-inferences on the basis of a →-interpretation which
itself already involves the presupposition of the admissibility of the same form of inference.
(Gentzen, 1934–1935, 167; but see also Negri & von Plato, 2015)
The fact that the BHK clauses manage to provide an inductive deﬁnition insofar
as they abstract from the intrinsic complexity of the constructive procedures seems
to suggest that a solution to our problem may be the following: the notion on
which to base the characterization of inferential validity will have to do without
the distinction between canonical and non-canonical cases.2 That said, we cannot
give up the distinction between inferences in an introductory form and inferences in
a non-introductory form, and since inferences are to be understood as acts, we can
2 Alternatively, we can keep the distinction, but renounce asking that the explicans amount to a
chain of valid inferential acts. This line of thought is investigated by Prawitz in The concepts of
proof and ground (Prawitz, 2019a), through a notion of “ontological” grounding-tree as distinct
from the act of inferentially building such a tree. It should be noted, however, that Prawitz’s
discussion here mainly concerns Bolzano’s 1837 grounding, and somehow connects to Sundholm’s
2011 own grounding-trees. There are nonetheless reasons for preferring the option without the
canonical/non-canonical distinction, and we shall not explore this possibility here.

56
3
Valid Arguments and Proofs
legitimately expect that the notion on which inferential validity will be based will
refer to objects produced by such acts.
Thus, the explanation of the epistemic power of valid arguments, proofs and valid
inferences is made problematic by the interaction between the two roles that valid
arguments and proofs play in this framework: on the one hand, as mathematical
objects aimed at a formal description of the notion of evidence, they act as explicans
of meaning; on the other, as acts aimed at achieving justiﬁcation, all of their
inferences are expected to be valid. In fact, the distinction between objects and
acts plays a central role, in particular in the theory of grounds. More speciﬁcally,
through the latter Prawitz seems to validate a distinction ascribable to Martin-Löf
(1984, 1985) and developed by Göran Sundholm (Sundholm, 1998, but see also
Sundholm, 1983, 1993) between proof-objects and proof-acts.
3.3.2
The Recognizability Problem
The distinction between canonical and non-canonical cases is also the source of a
recognizability problem, highlighted again by Prawitz himself. With reference to
valid arguments, he says that
given an argument , we may of course not be able to see whether there is a justiﬁcation J
such that (, J) is valid. But even when we are given a valid justiﬁed argument (, J), we
may not be able to see that it is valid. (Prawitz, 1973, 237)
The relevant case here is that of a closed non-canonical valid argument (, J),
being in possession of which means knowing how to obtain a closed valid argument
(1, J), where 1 ends with the application of an introduction rule and has
immediate subarguments valid with respect to J—the effectiveness of the available
method is granted by the constructiveness of the functions in J, therefore it is
sufﬁcient to apply these functions in some order. However, possession is not the
same as knowing that (, J) is a closed valid argument. Such an understanding is
in fact given by the additional information obtained by carrying out the method
that (, J) provides. Likewise, being in possession of an open valid argument
(, J) means knowing how to obtain closed valid arguments: it is sufﬁcient to ﬁrst
replace the occurrences of the free variables not bound in  with closed terms
of the atomic base, and then the undischarged assumptions of the argument so
obtained with closed arguments for such assumptions, valid with respect to some
consistent extension of J and to the atomic base. Again, this however is not the
same as knowing that (, J) is an open valid argument; such an understanding
could be achieved by carrying out all the (usually inﬁnite) required substitutions.
This situation is clearly uncomfortable: if valid arguments must really represent
proofs, they must justify the conclusion under the hypothesis that the assumptions
are justiﬁed, and for this to be possible, obviously it is not enough to have a
method that permits achievement of the goal, since we also require recognition
that the method has this property. However, it cannot be taken for granted that

3.3
Three Problems
57
recognition can always be obtained immediately, or at least easily. In the case of
closed valid arguments, there is no upper limit for the complexity of the reduction
procedures, so that agents with limited time and memory resources may not be able
to thoroughly perform the method in their possession. As a matter of principle,
whenever the resources of time and memory are inﬁnite, its effective character in any
case guarantees the possibility of carrying out the method. In this ideal condition,
the problem remains for open valid arguments, since the substitution process of
occurrences of free variables and assumptions might never end. Therefore, Prawitz
concludes that the valid arguments he deﬁnes are not necessarily conclusive, and
that
by a conclusive argument, one may perhaps understand a justiﬁed argument (, J) together
with an argument showing that (, J) is valid. This second argument must then again be
conclusive, and we would thus be led to an impredicative theory. (Prawitz, 1973, 237)
An obvious alternative could be requiring that validity be decidable or, in a weaker
sense, recognizable. However, even this solution, assuming it is a solution, proves
problematic, as can be seen from what Prawitz says with the respect to the
recognizability problem as relative to proofs:
in the case when α is atomic or is built up of atomic sentences by ∧, ∨, and ∃, the knowledge
of a canonical proof [. . . ] can be taken to consist of just the construction of the proof. [. . . ]
In the cases when α is an implication or a universal sentence and in the case when we know
only a procedure for obtaining a canonical proof, we must require not only a construction
or description of an appropriate procedure but also an understanding of this procedure.
(Prawitz, 1977, 27)
So again, the request that a constructive procedure generating proofs of the
expected type, or so operating on a class of proofs of a certain type, is not only
possessed but also understood as such, derives from the plausible thesis according
to which knowing a proof means not only having a mathematical object, but rather
recognising that this object enjoys signiﬁcant epistemic properties. And it is obvious
that the mere possession of a proof as understood here is not, from this point of
view, sufﬁcient; to know how to get a canonical proof does not mean we also know
that, once carried out, the method will produce the desired result. Likewise, to have
a constructive procedure such as the one provided by the clause (→P ) or by the
clause (∀P ) is equivalent to knowing how to have a certain output on a given input,
but not to knowing that this will happen whatever the possible values to be computed
may be. Again, the absence of an upper limit on the complexity of the computation
of the constructive procedures, and the inﬁnity of the substitutions to be made,
make it implausible to claim that such procedures contain information sufﬁcient
to guarantee the recognition required. At variance with what Prawitz wrote in 1973,
however, the author now seems to reject the idea that recognition can come from
a description of the procedure together with a proof that the procedure has the property
required [. . . ] this would lead to an inﬁnite regress and would defeat the whole project of a
theory of meaning as discussed here. (Prawitz, 1977, 27)

58
3
Valid Arguments and Proofs
The alternative to the idea of a meta-proof on the properties of the constructive
procedures consists of asking whether the relation “being a proof of” can be said to
be decidable:
the sentences α →β and ∀xα(x) can be asserted when we have described and understood
certain kinds of procedures, but it is doubtful in what sense, if any, one could decide the
question whether this condition obtains in a certain situation. (Prawitz, 1977, 29)
Decidability, one could argue, would solve our problem, by implying the existence
of a uniform and general constructive procedure to establish if a construction
amounts to a proof for a formula of the reference language or, if the construction
is open, to a constructive procedure to transform proofs for any formulas of the
reference language into proofs for a formula of the reference language. Be that as it
may, Prawitz is skeptical as concerns this possibility, and proposes speciﬁc counter-
examples in order to show that, in a given interpretation, such a strategy seems to be
untenable. This depends on Gödel’s incompleteness results, as is evident from the
following passage:
in the cases of implications and universal statements, it is not clear that we can decide
whether something is a canonical proof or not. [. . . ] The presence of →and ∀have the
effect that in general the conditions for asserting a sentence cannot be exhausted by any
formal system [. . . ] there is no formal system generating all the procedures that transform
canonical proofs of α to canonical proofs of β. (Prawitz, 1977, 28–29)
So, even if we introduce “idealized” agents with no limitation of memory and time,
they may not be able to decide whether a valid closed non-canonical argument or a
categorical proof are such, nor that valid open arguments or general, hypothetical, or
general-hypotheticalproofs are such. Neither the totality of the reduction procedures
or of the deﬁning equations, nor the totality of their expansions, can be generated
in an axiomatizable theory. None of these totalities can be regimented. Gödel’s
incompleteness theorems imply that the notion of proof, as well as that of valid
argument, cannot coincide with the notion of derivation in a given formal system
(on this point, see also Usberti, 1995).
While the recognizability problem mainly refers to the non-canonical case,
introductions are involved in the explanation of meaning, and thereby produce
structures that can be recognized as valid closed canonical arguments or canonical
proofs—provided we have recognized that their immediate substructures are so,
which can be granted by induction. For example, in the critical cases of canonical
proofs for implications and universal quantiﬁcations, these proofs are required to
consist of applications of procedures which denote the recognition of the related
substructures as, respectively, hypothetical and general proofs. Therefore, if we
assume that such a recognition has taken place, namely, that O→or O∀has been
applied, and since these operations settle the meaning of, respectively, →and ∀, the
obtained result will automatically be recognized as a canonical proof for α →β
or for ∀xα(x). This compels us, of course, to distinguish the primitive procedures
from others which, on the contrary, are not primitive. Prawitz had already explained
the difference in 1977 as follows:

3.3
Three Problems
59
an (or the) essential element of an understanding of [a primitive] operation consists in the
ability to carry out the operation so that the respective results are obtained. [. . . ] a primitive
operation [is] one whose result has to be conceived in terms of the operation, while the
result of a deﬁned operation can be understood independently of the operation. (Prawitz,
1977, 28)
But this is not to say that the canonical case, although less urgent, is entirely
unproblematic. Indeed,
also in the conditions for asserting conjunctions, disjunctions, and existential statements,
there is thus, strictly speaking, a question of understanding certain primitive operations, but
these cases are much less complicated. In the case of implications and universal statements,
it is not clear that we can decide whether something is a canonical proof or not. (Prawitz,
1977, 28–29)
In other words, the fact that in the clauses (→P
P ) and (∀P
P ), the primitive operations
are applied to procedures for which the recognizability problem holds, makes it
difﬁcult to establish what it means to understand O→and O∀. In fact, in order to
make it clear what the use of the latter consists of, we should make it clear what
it means to recognize that a procedure always generates determinate proofs when
applied to certain arguments. Therefore, even if the primitive procedures have the
effect of making the canonical proofs recognizable under the assumption that they
have been applied, we are not allowed to assume that a canonical proof is always
recognizable as such.
3.3.3
Validity as Independent from Inferences
In the recent The fundamental problem of general proof theory, Prawitz highlights
a further weak point of the notion of valid argument:
the validity of an argument relative to a base B and a set of reductions J may depend
essentially on the reductions in J and not at all on the inferences that make up the argument.
In contrast, the justiﬁcation of Gentzen’s elimination rules discussed in [Chapter 2] depends
on reductions of a much more restricted kind. (Prawitz, 2018, 8)
In this context, the Swedish logician proposes a new notion of validity. We will say
that an argument 1 is immediately extracted from a set of arguments 
 if, and only
if, (1) 1 ∈
 or 1 is a subargument of some 2 ∈
, or (2) 1 results from the
substitution with a term t of each free occurrence of x in occurrences of formulas in
some 2 ∈
, or (3) 1 is of the type
2
[α2]
. . .
n
[αn]
n+1
for i ∈
 with conclusion αi (i ≤n) and n+1 ∈
 with undischarged
assumptions {α2, . . . , αn}. An argument  is contained in a set of arguments 
if, and only if, there is a sequence of arguments 1, . . . , n such that n = 

60
3
Valid Arguments and Proofs
and for every i ≤n, i is immediately extracted from 
 ∪{1, . . . , i−1}. Let
us assume as usual that the atomic derivations in a base B are analytically valid
arguments, and that there is no analytically valid argument for ⊥.
Deﬁnition 8  is analytically valid (with respect to B) if, and only if,
1.  is closed ⇒ contains a closed analytically valid (with respect to B)
argument in canonical form;
2.  is open ⇒every σ is analytically valid (with respect to B), where σ is
a closed example of  obtained by ﬁrst replacing all the occurrences of free
variables not bound in  with closed (B-)terms, so to get an argument 1, and
then all the undischarged assumptions in 1 with closed analytically valid (with
respect to B) arguments for such assumptions.
Of course, we will again have a global deﬁnition of analytical validity for
inferences and inference rules.
Deﬁnition 9 A set of inference rules ℜis analytically valid (with respect to B) if,
and only if, for every application  of the type
1
. . .
n α1, ..., αm, x1, ..., xs
β
of an element of ℜ, if, for every i ≤n, i is analytically valid (with respect to B),
 is analytically valid with respect to B.
However, and this is the central point, unlike the notion of validity of 1973,
when we now for analytical validity demand that the non canonical closed argument
contain a closed, canonical, and analytically valid argument [. . . ], we get a condition whose
satisfaction depends on what the major premiss means. (Prawitz, 2018, 13)
Nevertheless, although the analytical validity of an argument now depends on the
type of inferences occurring in it, it is easy to see that there still exist analytically
valid arguments with inferences that instantiate inference rules in sets of inference
rules that are not analytically valid. Additionally, the distinction between arguments
in a canonical form and arguments in a non-canonical form, and the consequent
necessity to deﬁne the notion of analytical validity by simultaneous recursion,
impedes the possibility of characterizing the analytically valid arguments as chains
of analytically valid inferences.
The reasoning is the same for the recognizability problem. To possess a closed
analytically valid argument means knowing how to obtain a closed analytically
valid argument in canonical form: in this case, the effectiveness of the method at
our disposal depends on the constructive character of the operation of extraction,
applied in a given order on the starting argument. But possession is not the same as
knowing that the closed argument is analytically valid. For this purpose, in fact, it is
necessary to complete the extraction. Likewise, to possess an open analytically valid
argument means knowing how to get closed analytically valid arguments, but not
also knowing that the open argument is analytically valid. Such an understanding
could be achieved only by carrying out all the (generally inﬁnite) substitutions

3.4
Building and Computing
61
required. However, we cannot assume that an analytically valid argument contains
information necessary for the desired recognition; there is no upper limit on the
complexity of the extractions, and the domain of analytically valid arguments on
which to make the substitutions is inﬁnite and not regimented. Again, we could
require a proof of the fact that the argument is analytically valid, which would give
rise to a regressive explanation, or ask whether the relation “being an analytically
valid argument for” is decidable.
This ﬂaw can also be referred to the proofs of 1977 and 2005: a proof-structure π
could be a proof simply by virtue of the way in which the non-primitive procedures
occurring in it are deﬁned. Following Deﬁnitions 8 and 9, we could then solve
this problem by introducing a notion of analytical proof, and a related notion of
analytically acceptable inference; in any case, we would again come up against
with the proofs-as-chains and the recognizability problems.
3.4
Building and Computing
So far, valid arguments and proofs have been dealt with almost on a par. We may
even envisage a constructive bijection ι such that, for every SAP valid argument
⟨, J⟩, ι(⟨, J⟩) effectively yields a proof for the same conclusion and on at most
the same variables and assumptions as , and back for ι−1. This could be a ﬁrst step
towards a kind of “extended” Curry-Howard isomorphism (Howard, 1980), though
it is likely that we would here encounter negative results similar to those emerging in
Prawitz’s On the relation between Heyting’s and Gentzen’s approaches to meaning
(Prawitz, 2016).
However, in An approach to general proof theory and a conjecture of a kind
of completeness of intuitionistic logic revisited (Prawitz, 2014) Prawitz focuses on a
property of SAP valid arguments that seems to make them diverge from SAP proofs.
He remarks that the justiﬁcation procedures involved in SAP valid arguments are
deﬁned on argument skeletons rather than on arguments, i.e. skeletons together with
justiﬁcations, and [their values] consist of just argument skeletons instead of skeletons
with justiﬁcations. It is the skeletons with justiﬁcations that represent arguments, valid or
invalid ones, and when an argument step is to be justiﬁed it is conceivable that one wants
the justiﬁcation to depend on the entire arguments for the premises and not only on their
skeletons. As for the value of a justifying operation, it is a shortcoming that it consists of
just a skeleton if one wants it to contain new inferences that were not present in the skeleton
to which the operation is applied. (Prawitz, 2014, 275)
In the case of proofs, instead, we saw that proof-structures involve function
parameters f applied to proof-structures made of function parameters ﬁxed by
equations of the very same kind as those that determine f . When well-deﬁned,
therefore, these function parameters can be looked at as functions that produce, not
proof-structures out of proof-structures, but proofs out of proofs. Of course, this is
a much more reasonable picture, for we—and the standard BHK tradition—usually
conceive of proofs as built up of functions from and to proofs. We may say, in

62
3
Valid Arguments and Proofs
Prawitz’s 2014 terminology, that proof-structures are interpreted proof-terms, while
argument skeletons are uninterpreted proof-terms.
Now, this also becomes important in our present framework as soon as we ask
what it is, from the standpoint of Prawitz’s semantics of valid arguments and proofs,
to carry out a valid closed non-canonical argument or to carry out a categorical
proof.
To carry out a valid closed non-canonical argument ⟨, J⟩means ﬁrst of all to
build the argument skeleton , i.e. to perform a chain of inferential passages from
certain premises to a certain conclusion, in a way that makes it evident that we take
the former to epistemically support the latter. Non-canonical steps are justiﬁed by J
but, since reduction procedures go from argument skeletons to argument skeletons,
the act of reducing  to canonical form must be disjoint and, so to speak, external
to, or separated from, the act of building . First we have to build , and then we
can start its reduction to canonical form. For, if not, some of the steps performed
while building  would correspond not only to simple inferential transitions, but
more strongly to applications of appropriate justifying procedures. And this is in
contrast with the claim that what we have built is a mere argument skeleton.
For a concrete example, suppose that an agent P has carried out a closed
canonical argument skeleton, the ﬁnal step of which is an application of (∧I), i.e.
1
α1
2
α2 (∧I)
α1 ∧α2
Suppose that this skeleton is valid with respect to some J containing ∧i-rid. And
ﬁnally suppose that P applies (∧E,i) to this argument skeleton, obtaining
1
α1
2
α2 (∧I)
α1 ∧α2 (∧E,i)
αi
which is valid with respect to J. The application of (∧E,i) is clearly not sufﬁcient for
P to have a valid closed canonical argument for αi. Even if i is in canonical form,
P has to further apply ∧i-rid to obtain it. By applying (∧E,i), P has therefore just
built a non-canonical argument skeleton; nor can P apply ∧i-rid, before or while
building such skeleton.
If we say that to carry out a categorical proof π merely means to build π, we
are of course in the very same situation as we were with valid arguments: the act of
constructing π is disjoint from the act of computing π to canonical form. However,
an alternative solution seems to be at hand here. A proof is built up of function
parameters that can be looked at as functions going from proofs to proofs, and hence
we might not be forced to conclude that the act of performing a SAP categorical
proof is disjoint from the act of computing a certain structure to canonical form.

3.4
Building and Computing
63
To see the point, let us consider the same situation as above, but now with
reference to SAP proofs. Suppose, in other words, that an agent P has carried out a
canonical proof
O∧(π1, π2)
and that he then applies ψ∧
E,i to such proof, deﬁned by the equation
ψ∧
E,i(O∧(π1, π2)) = πi.
If we take this to mean simply that P builds the structure ψ∧
E,i(O∧(π1, π2)), we
have again that P does not yet have a canonical proof for αi; to obtain the latter,
he must still apply the equation deﬁning ψ∧
E,i. But now ψ∧
E,i can be understood
as a function going from proofs to proofs, so we may be entitled to understand an
application of it as an appropriate computation. We may, so to speak, internalize
conjunction reduction to conjunction elimination, so that the application of ψ∧
E,i
does not provide P with the non-canonical structure, but with the value of its
computation, i.e., according to the equation that deﬁnes ψ∧
E,i, with πi. Thereby,
P comes into possession of a canonical proof, if πi is such.
From this point of view, the implicational case, which is certainly more complex,
should be similarly understood as follows. Suppose that P is in possession of a
canonical proof O→(πα
β ) for α →β and of a proof π for α. Suppose now that P
applies a non-primitive procedure ψ→deﬁned by the equation
ψ→(O→(πα
β ), π) = πα
β (π).
If by application of ψ→, we mean that P computes ψ→on
O→(πα
β ) and π,
namely, P computes the value of ψ→(O→(πα
β ), π), the carrying out of the second
proof, i.e. the application of ψ→to the ﬁrst, will put P in possession of πα
β (π).
Since there is no guarantee that πα
β (π) is a canonical proof for β, we could deduce
that the carrying out of the proof could not give P a canonical proof for β. But
this, according to the interpretation we are proposing, would be wrong; to say that
the carrying out of the intended proof gives P the possession of πα
β (π) means to
say that, through such carrying out, P applies the constructive procedure πα
β to π,
and since by application we here mean the computation of a procedure on certain
values, what the carrying out of the intended proof provides P with is the value of
the computation of πα
β on π. Since O→(πα
β ) is a canonical proof for α →β, πα
β
will be a hypothetical proof for β from α, and since π is a proof for α, πα
β (π) will be
a constructive procedure generating a canonical proof π∗for β. Hence, the carrying
out of the intended proof will give P the possession of such π∗.

64
3
Valid Arguments and Proofs
Thus, the idea that justifying procedures are functions from proofs to proofs,
opens the possibility3 that to perform a proof is not merely to build a certain
structure, standing in need of further justiﬁcation but, more strongly, to carry out a
computation that, under circumstances such as those shown in our examples above,
yields a canonical proof.4 So, in such circumstances, to grant that what we do is
correct, we may just focus on what we have; we may explain the validity of our acts
in terms of the objects that they produce as result. But now observe that, while the
acts can be canonical or not, the only objects we really need can be taken to begin
with primitive operations.
Therefore, in proposing the suggested reading, the ﬁrst obvious consequence is
that we can no longer consider the carrying out of an inferential act as a simple
passage from premises to conclusion. We must more strongly afﬁrm that it consists
in computing the value of the procedure. Then, as the result of carrying out of a proof
is no longer possession of a method, but possession of the result of this method,
so the carrying out of an inference does not give possession of a procedure, but
the result of this procedure. Acceptable inferences can still be deﬁned in terms of
procedures that transform canonical proofs of the premises into canonical proofs of
the conclusion. But it is the inferential act as such—not a subsequent reduction—
that gives as a result the canonical proof.
The adoption of this strategy is of particular interest if we turn again to proofs-
as-chains and recognizability. Inferential validity can be given in terms of objects
that the theory considers as evidence, so that the acts involving such inferences,
or to which the same inferences correspond, have an epistemic value exactly by
virtue of their production of objects of this type. In addition, since such objects
are always canonical, in deﬁning them—and just as in the BHK clauses—we can
abstract from the intrinsic of any constructive procedures that form part of them. As
3 That is, an inferential act involved in a proof is, not the mere appending of a conclusion under
some premises, but the application of a function yielding evidence for the conclusion out of
evidence for the premises, only if justifying procedures are functions from proofs to proofs—
observe that, for valid arguments, we have claimed above that a negation of the consequent implies
a negation of the antecedent. The possibility is thus open because, for proofs, the negation of the
consequent does not hold. But Prawitz seems to also suggest the inverse implication: justifying
procedures go from proofs to proofs only if inferential acts are applications of them. E.g., he says
that “in order to achieve that the justiﬁcations operate not just on argument skeletons, but, as it
were, on skeletons with justiﬁcations, we must make a more radical change in the approach. We
need then to conceive of the valid arguments, i.e. proofs, as built up of operations deﬁned on proofs
and yielding proofs as values” (Prawitz, 2014, 275). If this reconstruction is correct, the idea that
inferential acts are applications of justifying procedures is tantamount to the idea that justifying
procedures are functions from proofs to proofs—i.e., the ﬁrst idea is not a mere possibility, but
something we are compelled to endorse, if we undertake the second.
4 As the example above shows, this line of thought is viable with the closed structures, but it
is not available with the open ones. If P has carried out the hypothetical proof O∧(ξα, ξα), he
cannot compute ψ∧
E,i(O∧(ξα, ξα)) to a canonical form, since in the deﬁning equation the argument
is required to be a canonical proof. In such circumstances, there is no great difference between
carrying out a valid argument and carrying out a proof; in both cases, the agent is just in possession
of a method that, when adequately closed, yields canonical forms.

3.4
Building and Computing
65
a consequence, what we have to recognize is not that the objects we possess enjoy
certain properties, but that the fulﬁlment of certain acts allows us to possess such
objects. And since these objects are always in canonical form, in order to achieve
the required understanding it will be sufﬁcient to recognize that the immediate
subproofs of introduction inferences are categorical proofs of the premises of the
introduction inference. This is exactly the road Prawitz takes with the theory of
grounds.

Chapter 4
Prawitz’s Theory of Grounds
4.1
From Inferences to Proofs, via Grounds
Prawitz’s theory of grounds aims to explain how and why some inferences have
the epistemic power to confer evidence to the conclusion starting from justiﬁed
premises. For this question to be answered, in the previous chapters we have
suggested the necessity of an approach that formalized the notion of evidence
independently from more basic concepts such as truth, valid argument or proof.
In this chapter, we will start our discussion of the theory of grounds. Before that,
however, it seems to us necessary to indicate the articles to which Prawitz has
entrusted the development of his new approach, and that will therefore constitute
our sources. The theory of grounds is announced for the ﬁrst time in an article
of 2006 entitled Validity of inferences (now Prawitz, 2013), further developed in
Inference and knowledge (Prawitz, 2009) and The epistemic signiﬁcance of valid
inference (Prawitz, 2012a), and more systematized in the long and dense Explaining
deductive inference (Prawitz, 2015). Many others articles are linked to it in a
substantial way: from Truth and proof in intuitionism (Prawitz, 2012b) to Truth
as an epistemic notion (Prawitz, 2012c), from An approach to general proof-
theory and a conjecture of a kind of completeness of intuitionistic logic revisited
(Prawitz, 2014) to On the relation between Heyting’s and Gentzen’s approaches to
meaning (Prawitz, 2016), ending with the latest The fundamental problem of general
proof theory (Prawitz, 2018), The seeming interdependence between the concepts of
valid inference and proof (Prawitz, 2019b), The validity of inference and argument
(Prawitz, 2022a), Validity of inferences (Prawitz, 2022b) and Validity of inferences
reconsidered (Prawitz, 2022c). Our guiding article will be Explaining deductive
inference. Obviously, however, we will occasionally refer to other writings, as much
to complete as to integrate some points of our scrutiny.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_4
67

68
4
Prawitz’s Theory of Grounds
4.1.1
Inferences in the Theory of Grounds
When introducing the “fundamental task” that the theory of grounds must fulﬁll,
it has been said that a rigorous discussion of the notion of ground presupposes a
clariﬁcation of the notion of valid inference. The search for a suitable deﬁnition
of validity, however, has led us to an analysis of the notion of evidence. On the one
hand, thus, in order to approach the description of valid inferences, it is fundamental
to understand what evidence they confer; on the other, the reverse is also true, since
the nature of such evidence will depend signiﬁcantly on how it is obtained.
In Chap. 2, following Cozzo (2015), we put forward the argument that what
inferences are depends on the context. In particular, it is possible to identify seven
factors (summarized in Table 4.1).
However, we have also observed that an analysis of the epistemic power of valid
inferences requires, for each of the seven factors above, a rather stringent selection.
Although in some cases there seems to be only one possible option, in others there
remains a quite broad choice. What we have to ask now is what notion of inference
Table 4.1 The seven factors of Cozzo (2015)
Factors
Examples of possible options
Nature of premises and conclusion
Data
Truth-bearers
Acts
Agent of inference
Machine
Impersonal biological entity
Person
Relation between S and premises and
conclusion (, α)
Data (, α) are stored in S
S is in the neural state (, α)
S is in the representational state (, α)
S performs the act (, α)
Relation between premises and conclusion
Pair
Causal link
Abstract relation
Conscious and voluntary act of the agent
Stability
Aleatory
Refutable
Conclusive
Character
Private
Publicly manifestable
Context
Premises, conclusion, relations, agent
+ other inferences and reasonings
+ information and knowledge
+ co-agents and their activity

4.1
From Inferences to Proofs, via Grounds
69
Prawitz refers to in approaching the basic question of his theory of grounds. And it
is indeed the same Swedish logician who provides us with the relevant indications.
In Explaining deductive inference, when introducing his investigation, he points out
that
philosophers and logicians distinguish among other things between inductive, abductive
and deductive inferences. Philosophers and psychologist make a quite different distinction
between what they call intuitive and reﬂective inference. (Prawitz, 2015, 66)
The distinction between inductive, abductive or deductive inferences concerns
exclusively the factor related to the stability of the relationship between premises
and conclusion. Deductive inferences are those of maximum force, establishing
the conclusion in a not refutable way—under the assumption that the premises
are justiﬁed; this does not happen, instead, with the inductive (see, for example,
Hawthorne, 2018) and abductive (see, for example, Douven, 2017) inferences,
in which the conclusion, even assuming that the premises are correct, might be
questioned by future occurrences. The distinction between intuitive and reﬂective
inferences, instead, does not concern an individual factor, but a multiplicity of
factors, of which the most important seem to be those of the relationship between
agent and premises-conclusion, on the one hand, and between premises and
conclusion as such on the other:
to make a reﬂective inference is to be aware of passing to a conclusion from a number of
premises that are explicitly taken to support the conclusion. Most inferences that we make
are not reﬂective but intuitive, that is, we are not aware of making them. (Prawitz, 2015, 66)
Therefore, according to Prawitz, in reﬂective inferences the relationship between
agent S and premises and conclusion (, α) consists at least in the passage that S
performs in an aware and voluntary way from  to α. However, awareness and vol-
untariness are here inextricably linked to an additional, fundamental circumstance:
believing to have found in  a valid basis for α (where the expression “basis for” is
deliberately generic, since it can be speciﬁed only after having clariﬁed the nature
of premises and conclusion), S performs the inferential act for the evident purpose
of substantiating, making known or using the link he/she has identiﬁed. The relation
between  and α, therefore, arises from the fact that S, by inferring, explicitly
assumes the responsibility of a support that the elements of  have on α.
The description of reﬂective inferences as conscious acts seems to be compatible
also with other readings of the relationship between agent and premises-conclusion,
or between premises and conclusion in themselves. It could indeed be required
that the agent be aware to possess certain data, or to be in some neural or
representational state; similarly, it could be that the agent makes the inference by
virtue of the awareness of causal links or abstract relationships between premises
and conclusion. If this is true, it should also be noted that a certain understanding of
the above relationships is compatible only with reﬂective inferences. Both being in
a representational state and performing an act in which some premises are taken
to support a conclusion imply a conscious and voluntary participation; on the
contrary, in order to possess certain data, or to occupy a neural state, awareness
and voluntariness are not necessarily required, as they are not in acts triggered by

70
4
Prawitz’s Theory of Grounds
unconscious or automatic causal links. Be that as it may, there are no doubt two
elements that seem to unite all the possible options to which reﬂective inferences
give rise: ﬁrst,
reﬂective inferences are presumably conﬁned to humans; (Prawitz, 2015, 67)
secondly, the character of awareness has as a result that, although they can be per-
formed at a primarily mental level, reﬂective inferences are publicly—for instance,
linguistically—expressible and communicable in the context of fulﬁllment.
Much of what has been said with regard to reﬂective inferences is consistent with
what, in Chap. 2, we have argued in relation to those endowed with an epistemically
compelling force. In fact, the idea that an inference can deﬁnitely establish its own
conclusion does not necessarily require that the act through which this occurs has a
reﬂective nature—think of Chrysippus’ dog. But if we concentrate on epistemically
compelling deductive inferences, it is safe to assume that they are also reﬂective—
even though the opposite, obviously, may not be the case. It would certainly be
interesting, but alien to the purposes of our survey, to research the precise links that
induction, abduction and deduction have with intuitiveness and reﬂectivity; here, we
can only limit ourselves to emphasizing that Prawitz, with the theory of grounds,
takes exclusively into account reﬂective deductive inferences, with the following
remark:
the logical relevance of the distinction between intuitive and reﬂective inferences may be
doubted, but the distinction turns out to be relevant also for logic. (Prawitz, 2015, 67)
Finally, it remains to understand Prawitz’s outlook of the nature of premises and
conclusion and of the context of an inference. And the best way to do it is, very
simply, to recall a passage of our author—in which, among other things, we ﬁnd
again clearly summarized what has been said so far about reﬂective inferences to
which the theory of grounds is addressed:
a reﬂective inference contains at least a number of assertions or judgments made in the
belief that one of them, the conclusion, say β, is supported by the other ones, the premises,
say α1, . . . , αn. An inference in the course of an argument or a proof is not an assertion or
judgment to the effect that β follows from α1, . . . , αn, but is ﬁrst of all a transition from
some assertions (or judgments) to another one. In other words, it is a compound act that
contains the n + 1 assertions α1, . . . , αn and β, and in addition, the claim that the latter
is supported by the former, a claim commonly indicated by words like “then”, “hence”, or
“therefore”. (Prawitz, 2015, 67)
In the inferences which the theory of grounds deals with premises and conclusion
should therefore be understood as judgments or assertions; Prawitz is careful in
specifying that an inference is not, in itself, the further judgment or assertion
according to which the conclusion follows, or is supported by, the premises. On
the contrary, it just takes the form of a mere act of passage from the former to latter.
But what are, more speciﬁcally, judgments and assertions?
Following (a certain interpretation of) Frege (1879), a judgment can be under-
stood as the mental act by which it is claimed that a certain proposition is true; and
just like a sentence can be considered as the linguistic equivalent of a proposition,

4.1
From Inferences to Proofs, via Grounds
71
an assertion can be considered as the linguistic equivalent of a judgment, i.e., the
linguistic act through which it is claimed (mostly implicitly) that a certain sentence
is true. Regarding premises, conclusion, and the inference itself, however, Prawitz
does not seem to consider the distinction between the mental and the linguistic plan
to be really substantial:
it does not matter for what I am interested in here whether we speak of judgments or
assertions in this context and hence whether we take an inferential transition to be mental
or linguistic. (Prawitz, 2015, 68)
From Frege (1879) Prawitz also draws the symbol of judgment or assertion ⊢, but
extending its use so as to account for what he calls open judgments and assertions,
and judgments and assertions under assumptions
unlike Frege, I want to account for the fact that an inference may occur in a context where
assumptions have been made, and a qualiﬁcation of what has been said is therefore needed.
It should be understood that the assertions in an inferential transition need not be categorical,
but may be made under a number of assumptions. [ . . . ] I write α1, . . . , αn ⊢β to indicate
that the sentence β is asserted under the assumptions α1, . . . , αn. To make an assumption
is a speech act of its own, and one may allow an inference to start with premises that
are assumptions, not assertions. It is convenient however not to have to reckon with this
additional category of premises, but to cover it by the case that arises when α is asserted
under itself as assumption, that is, an assertion of the form α ⊢α. [ . . . ] Furthermore, we
have to take into account that a sentence that is asserted by a premiss or conclusion need
not be closed but may be open. For instance, to take a classical example, we may assume
that
√
2 equals a rational number n/m and infer that then 2 equals n2/m2. (Prawitz, 2015,
68)1
1 It is perhaps worthwhile remarking that premises and conclusion of an inference are understood
as judgments also in Per Martin-Löf’s intuitionistic type theory (Martin-Löf, 1984, but see also
Sundholm, 2012; Dybjer and Palmgren, 2016). The latter is closely connected to Prawitz’s theory
of grounds, from a philosophical as well as from a formal point of view. However, Prawitz and
Martin-Löf have quite different conceptions of what judgments are. For Martin-Löf, “what we
combine by means of the logical operations [ . . . ] and hold to be true are propositions. When we
hold a proposition to be true, we make a judgment:
A

proposition
is true



judgment
In particular, the premises and conclusion of a logical inference are judgments” (Martin-Löf, 1984,
3). Martin-Löf’s fundamental forms of judgment are:
•
α is a set (written α set)
•
α and β are equal sets (written α = β)
•
a is an element of the set α (written a ∈α)
•
a and b are equal elements of the set α (written a = b ∈α)
In intuitionistic type theory, each of these forms receives an explanation aimed at ﬁxing the nature
of the deﬁned object, the epistemic conditions for asserting the intended judgment or, ﬁnally, the
meaning of the latter. One of the distinctive traits of Martin-Löf’s approach is the generalization
of the above forms of judgment to hypothetical judgments, that is, judgments dependent on

72
4
Prawitz’s Theory of Grounds
As regards the context, we have now to underline that the description of
the inferential acts as transitions is, in Prawitz’s view, only approximate at this
stage of the analysis. Therefore the reﬁnement will show how the activity in
question actually includes something more—something essential—than premises,
conclusions, relationships and agent. First of all, our author observes that
when we characterize an inference as an act, we may do this on different levels of
abstractions. If we pay attention to the agent who performs an inference and the occasion at
which it is performed, we are considering an individual inference act. By abstracting from
the agent and the occasion [ . . . ] we get a generic inference act. We may further abstract
from particular premises and conclusion of a generic inference and consider only how the
logical forms of the sentences involved are related [ . . . ]. I shall call what we then get an
inference form. (Prawitz, 2015, 69–70)
assumptions. Conﬁning ourselves to the case of a single variable, under the assumption α set,
and indicating with [x ∈α] the assumption (discharged) “for x ∈α”, we will then have that
•
β(x) set [x ∈α]
•
β(x) = δ(x) [x ∈α]
•
b(x) ∈β(x) [x ∈α]
•
b(x) = d(x) ∈β(x) [x ∈α]
After presenting the meaning of judgments and hypothetical judgments, Martin-Löf deals with
the meaning of propositions and sentences, adhering to the intuitionistic idea that “a proposition
is deﬁned by laying down what counts as a proof of the proposition” (Martin-Löf, 1984, 11).
Therefore, he adopts the BHK clauses for propositions and sentences. Proofs of propositions occurs
in judgments that intuitionistic type theory aims at proving, in terms of four primitive kinds of
rules: “the formation rule says that we can form a certain set (proposition) from certain other
sets (propositions) or families of sets (propositional functions). The introduction rules say what
are the canonical elements (and equal canonical elements) of the set, thus giving its meaning.
The elimination rule shows how we may deﬁne functions on the set deﬁned by the introduction
rules. The equality rules relate the introduction and elimination rules by showing how a function
deﬁned by means of the elimination rule operates on the canonical elements of the set which
are generated by the introduction rules” (Martin-Löf, 1984, 24). Martin-Löf equates sets and
propositions, thereby following the formulas-as-types conception that underlies the Curry-Howard
isomorphism (Howard, 1980), with which we deal below. He also introduces a distinction between
judgments of the form “a ∈α”—that he calls analytic—and judgments of the form “α is true”—
that he calls synthetic, the idea being that α is true in the case where there is a proof a of it
(Martin-Löf, 1985, 1994, 1998). With respect to Martin-Löf’s judgments, Prawitz speciﬁes that
“like Frege, and unlike Martin-Löf, I do not take an expression of the form “it is true that . . . ”,
where the dots stand for a declarative sentence, to be the form of a judgment. To assert a sentence
of the form “ . . . is true”, where the dots stand for the name of a sentence α, is to make a semantic
ascent, as I see it, and is thus not the same as to assert α” (Prawitz, 2015, 68), and that “Martin-Löf
type theory contains rules for how to prove [assertions or judgments of the form a ∈α]. There are
also assertions of propositions in the type theory, but they have the form [α is true] and are inferred
from judgments of the form a ∈α [ . . . ]. Thus, the assertions in the type theory are, as I see it, on
a meta-level as compared to the object level to which the assertions that I am discussing belong”
(Prawitz, 2015, 97).

4.1
From Inferences to Proofs, via Grounds
73
A distinction analogous to the tripartition into individual inferences, generic
inferences and inferential forms may also relate to transitions; more in particular,
a generic inferential transition may be indicated by an inferential ﬁgure
1 ⊢α1
. . .
n ⊢αn x1, . . . , xs
 ⊢γ
with i (i ≤n) and  sets (possibly empty) of propositions or sentences (or
possibly open formulas) for  ⊆
i≤n i, αi (i ≤n) and γ propositions or
sentences (or possibly open formulas) for xh (h ≤s) bound by the transition. The
transition contained in an inferential form can instead be indicated by an inference
scheme of the same type, where the elements in i (i ≤n) and , as well as αi
(i ≤n) and β, are, for example, meta-variable parameters. Now, Prawitz emphasizes
how
we usually take for granted that everything logically relevant about inference acts can be
dealt with when an inference is identiﬁed with a set of premises and a conclusion, in
other words, with what individuates a generic inferential transition or the form of such
a transition. We can then direct our attention solely to inference ﬁgures or schemata and
disregards the acts that they represent. (Prawitz, 2015, 70)
In the framework described so far, however, this move is not harmless, having on
the contrary at least two signiﬁcant consequences; in fact, the exclusive focus on
inferential ﬁgures and schemes, leaving out transitions and the acts they describe,
implies that
[1] we can see premises and conclusions as sentences instead of assertions; the Fregean
assertion sign may be taken just as a punctuation sign that separates a sentence in an
argument from the hypotheses on which it depends. [ . . . ] [2] the distinction between
intuitive and reﬂective inferences is of no interest, nor is there any room for it. (Prawitz,
2015, 70)
Limiting oneself to inferential ﬁgures and schemes means to endorse the point of
view according to which premises and conclusion are abstract entities, between
which an equally abstract tie occurs. In Chap. 2, we have already seen how some
proposals of this type—validity of inferences in terms of (logical) consequence
between premises and conclusion—do not account in a satisfactory way for the
presence of an epistemic bond. Soon, we will see that this line of thought seems,
according to a certain reading, totally impracticable with regard to the basic question
of the theory of grounds.
The risks of a reductionist reading on the nature of inferences seem already
entailed in a description of these inferences as mere transitions. If all that is
involved in an inferential act is the passage—however conscious and voluntary—
from premises to conclusion, what else can count but an abstract relationship
between the former and the latter? If all that we do by making an inference is to
move from judgments to judgments, and if the whole thing can satisfactorily be
exhausted in the linguistic practice of inserting a “therefore”—or other equivalent
expressions—between assertions and assertions, what can we appeal to in order to
explain the epistemic constraint if not to a link between the content of judgments

74
4
Prawitz’s Theory of Grounds
Table 4.2 Inferences in the theory of grounds
Factors
Typology
Nature of premises and conclusion
Acts (judgments or assertions)
Agent of inference
Person
Relation between agent S and
premises and conclusion (, α)
S performs the transition (, α) in a conscious and
voluntary way
Relation between premises and
conclusion
S assumes responsibility for a support of  on α
Stability
Conclusive
Character
Publicly manifestable
Context
Premises, conclusion, (bound) assumptions and
variables, relations, agent, mental operation
and assertions, and to the possibility that it exerts its force on us in some way? In
fact, Prawitz wonders,
does an inference contain something more than an inferential transition, and if so, what?
[ . . . ] On the one hand, when we make inferences in the course of a proof, all that we
announce is normally a number of inferential transitions. On the other hand, there seems to
be something more that goes on at an inference, some kind of mental operation that makes
us believe, correctly or incorrectly, that we have a good reason to make the assertion that
appears as conclusion. (Prawitz, 2015, 68–69)
The idea of a mental operation on the basis of which we feel authorized, more or
less licitly, to judge as true the conclusion, or to assert it, is the focal point of the
theory of grounds. But before getting to the heart of the question, it is necessary to
clear the ﬁeld of some positions related to what is really at stake when we infer. In
fact, through this further reﬂection on the nature of inferences, we will be able to go
back to the notion of evidence.
For the moment, to conclude this section, we brieﬂy summarize the ground-
theoretic notion of inference by showing how the seven factors of Cozzo (2015)
are articulated in Prawitz’s framework. This is done in Table 4.2 below, by a direct
and quick reference to all the results we have obtained so far.
4.1.2
Evidence as the Aim of Reﬂective Inferences
What has been said on the notion of evidence, as well as on the nature of inferences
to which the theory of grounds is dedicated, provides a sufﬁciently clear framework
for a renewed approach to valid inferences. In turn, this will allow both a critical
evaluation, with an essentially negative outcome, of a certain understanding of
inferential validity, and a deeper and more reasoned articulation of the “fundamental
task” referred to in Chap.2. Nonetheless, in seeking an adequate description of
valid inferences we cannot simply take for granted approaches that the logico-
mathematical tradition has strengthened. As seen, many proposals of respectable

4.1
From Inferences to Proofs, via Grounds
75
paternity prove to be unsatisfactory. It is thus plausible that a philosophically
“honest” treatment requires instead a comparison between certain formalizations
and pre-formal desiderata, in order to consider their appropriateness.
Therefore, it will not come as a surprise that, when understanding the question of
valid inferences or, equivalently, when undertaking the “fundamental task”, Prawitz
starts from what could be considered as a sort of non-formal “phenomenological”
analysis of inferential activity. A key-role in this perspective is played by the
three points that, in the previous section, we have witnessed being obscured by an
understanding of inferences as simple premises-conclusion pairs, reducible to the
graphic representation of generic transitions as inferential ﬁgures, and of inferential
forms as inferential schemes: the idea that an inference is an act, that this act
is intuitive or reﬂective, and that also premises and conclusion are acts, namely,
judgments or assertions. Indeed, it is only when an inference is understood as
an act, that the distinction between intuitive and reﬂective inferences makes any
sense; the acts can be unconscious, involuntary and automatic (i.e., intuitive) or, on
the contrary, conscious and deliberate (i.e., reﬂective). Thus, it is only because of
this distinction between intuitive and reﬂective acts that the following observation
gains importance: namely, that conscious and voluntary acts, unlike others, can
be accomplished with a purpose. And this seems to be the case with reﬂective
inferences, in which the transition from premises to conclusion is oriented towards
the latter, focusing on a support that the premises guarantee, or seem to guarantee,
for the conclusion. In the words of Prawitz,
we must say something about the point of inferences, why we are interested in making and
studying them, and now the inferences must be seen as acts. [ . . . ] However, it is the point
of reﬂective inferences that I want to discuss especially, and in this context we can speak of
aims. The personal aims of subjects who make inferences may of course differ, but we can
speak of an aim that should be present in order that an inference is to count as reﬂective. As
already said, it is an ingredient in what it is to make such an inference that the conclusion
is held, correctly or incorrectly, to be supported by the premises. In view of this, reﬂective
inferences must be understood as aiming at getting support for the conclusion. (Prawitz,
2015, 71)
But what is this purpose? What is meant by the support that the premises
secure, or seem to secure, for the conclusion? If an inference involves judgments
or assertions, the support will have to be of an epistemic type, in the speciﬁc sense
that the purpose the agent aims at when moving from premises to conclusion is that
of judging or asserting correctly the proposition or sentence involved in the latter
by virtue of the (possibly hypothetical) correctness of the judgments or assertions
to which the former amount. In other words, the support for a judgment or assertion
cannot but be a guarantee of correctness, and the purpose of the inferential transition
is precisely to achieve this guarantee. In particular, Prawitz points out how this idea
can be declined in different, and, one would say, equivalent ways:
we may say that the function of inferences in general is to arrive at new beliefs with a
sufﬁcient degree of veracity. [ . . . ] We may say that the primary aim is to get a good reason
for the assertion that occurs as conclusion. Since the term reason also stands for cause or
motive, another and better way to express the same point is to say that the aim is to get

76
4
Prawitz’s Theory of Grounds
adequate grounds for assertions or sufﬁcient evidence for the truth of asserted sentences.
Since assertions are evaluated among other things with respect to the grounds or evidence
the speakers have for making them, we may also say that the aim of reﬂective inferences is
to make assertions justiﬁed or warranted. (Prawitz, 2015, 71)
As already remarked, validity of inferences is usually explained by requiring that
the conclusion be a (logical) consequence of the premises; in turn, the explanatory
standard of the relation of (logical) consequence requires some notion of truth,
either by following the general and pre-formal intuition according to which the truth
of the premises necessarily implies the truth of the conclusion, or, with reference
to the more precise proposals of Bolzano, Tarski and model theory, by requiring
the preservation of a formally understood notion of truth under interpretation.
However, it is not difﬁcult to guess how, in relation to an explanation à la Prawitz
of the purpose of inferences, this line of thought turns out to be unsatisfactory; to
determine the validity of a passage from judgments to judgments or from assertions
to assertions through an abstract relationship between propositions or statements, in
turn based on an equally abstract notion such as that of truth, obscures the essentially
epistemic nature of the purpose of the passage, and its result in case of legitimacy.
In this regard, it is useful to note that what has just been said is independent of
which kind of truth is chosen; indeed, an epistemic explanation of the notion, which
contrasts the realistic one of Bolzano, Tarski and model theory, does not seem
beﬁtting, since, more generally,
the aim of a reﬂective inference cannot be described just in terms of truth. The aim has not
been attained when the sentences that is asserted by the conclusion just happens to be true.
(Prawitz, 2015, 72)
At the end of this passage, Prawitz highlights the obvious observation that, in
order to have reasons, grounds, evidence, justiﬁcation or guarantee for judgments or
assertions, it is naturally not enough that the propositions judged or the sentences
asserted are de facto true. We could be totally unaware of this circumstance, and in
that case we would obviously not be authorized (nor, probably, willing) to judge as
true or to assert (possibly under appropriate assumptions) the inferred propositions
or sentences. Besides that, in the ﬁrst part of the same quotation, the Swedish
logician asserts in a stronger, and perhaps less obvious way that the explanation
in terms of simple truth already has the effect of blocking an adequate elucidation
of the purpose itself of reﬂective inferences. In fact, if we limited ourselves to
saying that by inferring we aim to pass from propositions or sentences (possibly
assumed as) true to true propositions or sentences (possibly under assumptions),
we would omit the essential trait of this activity—namely taking responsibility
for the epistemic support that the truth of the premises provides to the truth of
the conclusion, i.e. of the fact that the conclusion is true by virtue of the truth of
the premises; here we can note that, when Prawitz speaks in terms of truth, he
always uses the word “evidence” for this truth. It is precisely (the identiﬁcation
of) this dependence that is our reason, ground, evidence, justiﬁcation or guarantee
for (possibly illicit) conclusive judgments or assertions; in other words,

4.1
From Inferences to Proofs, via Grounds
77
we expect an inference to afford us with knowledge in a Platonian sense, which is again to
say that it should give us not only a true belief, but also a ground for the belief. (Prawitz,
2015, 72)
In any case, the failure of an explanation of the purpose of inferences in terms of
simple truth does not necessarily imply the inadequacy of a description of inferential
validity in terms of truth-preservation (under interpretation) with respect to the
“fundamental task” of the theory of grounds—and indeed, the idea counts some
supporters (among the most recent, see Brîncu¸s, 2015). Prawitz himself points out
that
although this does not seem likely in view of the fact that the deﬁnition of validity refers to
truth and not to any epistemic notion concerned with how truths are established, this should
not be excluded off hand. (Prawitz, 2015, 74)
However, there is reason to believe that the situation will not be so different from that
observed for the epistemic understanding of the modality involved in the relation of
(logical) consequence. In fact, what we have shown there is that the Bolzanian,
Tarskian and model-theoretic approaches do not capture the notion of necessity of
thought (Prawitz, 2005), which, as observed by Cozzo (2015), informs the relation
of (logical) consequence of strong inferential traits; it is therefore presumable that
the evaluation of these proposals with respect to the “fundamental task” is doomed
to fail. Moreover, Cozzo himself proposes a reformulation of the “task” which
emphasizes in particular the connection that we are highlighting:
from the fact that an inference J is valid it should be possible to derive that J is endowed
with necessity of thought. Let us say that “the fundamental task” is to devise an analysis of
deductive validity which satisﬁes this condition. (Cozzo, 2015, 106)
From the insufﬁciency of a characterization of the inferential purpose in terms of
simple truth, we have thus passed to the following question: can we account for
the epistemic power of valid inferences by deﬁning them as those in which the
conclusion is a (logical) consequence of the premises in the sense of Bolzano,
Tarski and model theory? Obviously, the answer will be afﬁrmative only if the
adoption of the proposed deﬁnition takes us on the right direction with respect to
the “fundamental task”—that is to say, it allows us to fulﬁll this task through the
identiﬁcation of an adequate relation between agents and inferences, which would
be the last remaining step to be made at that point. But then, we need to turn to a
second question: can we go beyond the simple observation that this strategy seems
to fail in light of its incapacity to capture an epistemic reading of the relation of
(logical) consequence? There seems to be no other possibility here than to confront
directly with the notion of valid inference, i.e. without passing through (logical)
consequence.
For the purpose of a better understanding of this phase of the analysis (in which
we will essentially use Prawitz, 2009, 2012a and 2013), it is appropriate now to
explicitly repeat that “fundamental task” which, from the beginning of this chapter,
we have so widely called into question—until now without a real need for precision:
given the two premises

78
4
Prawitz’s Theory of Grounds
(a) I is a valid inference from the set of premises  to the conclusion α;
(b) A is in possession of a ground for each of the premises in ,
which additional condition should be added to (a) and (b) to obtain
(c) A has a ground for α?
From this point of view, the ﬁrst and most obvious observation concerns the actual
need to accompany (a) and (b) with an additional premise (c); in fact, in conjunction
with the derivation of (d), it is not sufﬁcient that I is valid, and that the agent is
in possession of grounds for each of the premises in . Obviously, here applies
what has already been said regarding the insufﬁciency of the simple truth of the
conclusion with respect to the possibility that the corresponding inference achieves
the purpose; on several occasions Prawitz insists on this point, arguing for example
that
given a valid argument or a valid inference from a judgment α to a judgment β, it may be
possible for an agent who is already in possession of a ground for α to use this inference
to get a ground for β, too. But the agent is not ensured a ground for β, just because of the
inference from α to β being valid and the agent being in possession of a ground for α. The
agent may simply be ignorant of the existence of this valid inference, in which case its mere
existence does not make her justiﬁed in making the judgment β; (Prawitz, 2009, 183)
it should be clear that the mere existence of a valid argument with conclusion α and a
set  of premises that express already received knowledge does not provide us with the
knowledge expressed by α: we may be unaware of the existence of this valid argument and
hence may be unable to use it to infer α from . (Prawitz, 2012a, 888)
In Validity of inferences, the situation is clariﬁed by a reference to Andrew Wiles
who, as is well known, proved the so-called “Fermat’s last theorem” in 1994, but
previously had to withdraw a ﬁrst, incorrect, proof-attempt. Therefore,
we may grant that Andrew Wiles was justiﬁed in holding true all the facts from which
he started when proving Fermat’s last theorem and that the one step inference from these
starting points to Fermat’s last theorem is valid. But these two assumptions are clearly not
enough to make Wiles or anybody else justiﬁed in holding Fermat’s last theorem true; they
were, we may assume, satisﬁed long before Wiles gave his proof. [ . . . ] Wiles would have
been justiﬁed in asserting Fermat’s last theorem as soon as he gave his ﬁrst incomplete
proof, contrarily to the fact that in reality he soon afterwards had to withdraw it because of
the discovered gap. (Prawitz, 2013, 186–187)
Of course, it is one thing to point out the need for a third premise, and another is to
give one in an adequate form. It is plausible, however, to expect that the choice will
be inﬂuenced by what is meant by valid inference. Above, we have contemplated
the idea of deﬁning as valid those inferences the conclusion of which is a (logical)
consequence of the premises in the sense of Bolzano, Tarki and model theory.
Therefore, the question is: if adopting this framework, what the desired condition
(c) might be? At ﬁrst glance, we may require that A is aware of the validity of I,
i.e.
(c1) A knows that  |	 α

4.1
From Inferences to Proofs, via Grounds
79
Whether this framework is satisfactory or not is a question that Prawitz, again in
Validity of inferences, takes explicitly into account. However, he additionally—and
essentially, for what we are concerned with—observes that (a), (b) and (c)—the
latter in whatever declination, therefore a fortiori in the form (c1)—should be
understood not only as merely sufﬁcient, but also and above all necessary conditions
of (d). More speciﬁcally, given the implication
1) if a person knows the inference J from a sentence α to a sentence β to be valid, and
is justiﬁed in holding α true, then she is also justiﬁed in holding β true [ . . . ] we should
ask whether the antecedent of this implication describes a way to acquire new knowledge;
otherwise, the fact that the implication holds is of little interest. [ . . . ] 1) was formulated
as a possible response to the problem that arose when noting that the mere validity of
an inference whose premises are known to be true does not justify a person in asserting
the conclusion, it then being suggested that it is only when a person sees the validity
of the inference that she is so justiﬁed. Identifying “seeing the validity of an inference”
with “knowing the inference to be valid”, such knowledge was suggested as a necessary
condition for an inference to justify a belief, which we may formulate as follows: 2) it is
only when a person knows an inference to be valid and its premises to be true that the
inference justiﬁes her in holding the conclusion true. (Prawitz, 2013, 187–188)
At this point, Prawitz remarks that the joint action of 1) and 2) generates regressive
explanations similar to those already described by Bolzano (1837) and Lewis Carroll
(1895), thus making the overall approach decidedly unsatisfactory. As a matter of
fact, completing a “fundamental task” of which the third hypothesis has the form
c1), means trying to
establish the truth of the implication 1). Assume that a person, call her A, knows a sentence
α to be true and let J be an inference from α to a sentence β. Assume further not only that
J is valid but that A knows this, as required in 1), if she is to use the inference J to justify
her in believing that β. Why should A now be justiﬁed in holding β true? Suppose that we
argue that, given A’s knowledge of the validity of J, A knows that if α is true then β is
true [ . . . ] and that therefore A may just apply modus ponens and conclude that β is true.
But to be an argument showing that A is justiﬁed in holding β true, it must also be assumed
because of 2) that A knows modus ponens to be a valid inference, (Prawitz, 2013, 190)
which, as is evident, requires the addition to the starting assumptions of the further
hypothesis that A knows that modus ponens is a valid inference, thus resulting in a
regress. Strictly speaking, the regress does not depend in this case on the fact that
the validity of the inferences is understood in the terms of the notion of (logical)
consequence à la Bolzano, Tarski and model theory; this is clearly emphasized by
Prawitz, who writes
the failure to support 1) does not depend on the assumed fact that the Bolzano-Tarski notion
of logical consequence lacks a genuine modal ingredient, because the same regress seems
to arise if we take the validity of the inference to mean that the conclusion is a necessary
consequence of the premises [ . . . ]. What has been shown so far is only that a regress
arises when we combine the Bolzano-Tarski notion of validity with the idea expressed in
2). (Prawitz, 2013, 191–192)
Nonetheless, it is easy to understand how the fact that the approach of Bolzano,
Tarski and model theory lacks any modal traits plays a decisive role in the discourse
we are carrying out. In fact, even ascribing to (c) a minimum force with respect

80
4
Prawitz’s Theory of Grounds
to that of the relation of explicit knowledge postulated by (c1), or in other words
requiring generically and simply that
(c2) A acknowledges that I is valid
is the right characterization of (c), it seems inevitable to fall back into invalidating
regressive forms:
when “valid” means that the implication “if α then β” is true for all variations of the content
of the non-logical terms of α and β, then this is what the person recognizes, and what seems
relevant here is just that she recognizes the truth of this implication (without any variation of
the content). From this we want to conclude that she is justiﬁed in holding β true. [ . . . ] we
may assume that she recognizes the validity of modus ponens [ . . . ]. But there we are again
with this kind of argument that leads to a regress of the Bolzano-Carroll kind. (Prawitz,
2013, 195)
The approach in question thus seems unsatisfactory on principle, and this is
because of a problem that undermines its very foundations. The notion of logical
consequence is a special case of a more general notion of deductive consequence;
the modal character of the former is transmitted to the latter, since the difference
between the two concerns only the invariance of the relation under interpretation of
the non-logical terms. An analogous distinction seems also to concern the notion of
inferential validity:
(ID)
an inference I with set of premises  and conclusion α is deductively valid
if, and only if, an agent A who is justiﬁed for all the premises in  is, through I,
epistemically compelled or justiﬁed to accept α—where it should be noted that
the expression “through I” is deliberately inaccurate, being in fact the object of
the current phase of investigation;
(IL)
an inference I with set of premises  and conclusion α is logically valid if,
and only if, I is deductively valid solely on the basis of the logical form of the
elements in  and of α.
As usual, the idea expressed in (IL) can be translated introducing interpretations
from the language into (one or more) appropriate sets:
(IL∗)
an inference I with set of premises  and conclusion α is logically valid
if, and only if, for every interpretation AD, the inference AD(I) with a set of
premises AD() and conclusion AD(α) is deductively valid.
(IL∗) is based on a prior notion of deductively valid inference, and it is precisely this
notion that Prawitz intends to deﬁne adequately. On the other hand, Bolzano, Tarski
and model theory do not seem to go further than the relation of logical consequence,
and therefore they are not able to deal with the notion of deductively valid inference,
to which (IL) refers; even when we wanted to ﬁnd in them a description of deductive
validity, we have already seen that what results is a trivial collapse of the relation of
deductive consequence on material implication, and of the corresponding inference

4.1
From Inferences to Proofs, via Grounds
81
on the circumstance that either one of the premises is false or the conclusion is true.
Therefore, Prawitz notably remarks how
to say that the conclusion of an inference is a logical consequence of its premises in the
sense of Bolzano-Tarski amounts just to saying two things: 1) that the inference preserves
truth (which only means here that if the premises are true then so is the conclusion) and 2)
that all inferences of the same logical form do the same. Now, although the ﬁrst property
is certainly relevant for the question whether the inference has the power to justify a belief
in the conclusion (being a necessary condition for that), it is obviously not sufﬁcient for the
inference to have this power. The second property seems not even relevant to this question.
(Prawitz, 2013, 185)
Once the feasibility of an approach based on the notion of (logical) consequence
has been discarded, all that remains is the other option explored in Chap. 3: to
describe as valid those inferences for which there exists a valid argument or a
proof—in the sense of Prawitz’s semantics of valid arguments and proofs—from
the premises to the conclusion, and to weigh the adequacy of this proposal in
connection with the “fundamental task”. It would indeed seem promising that the
notions of valid argument and proof allow as a matter of principle to distinguish
between deductive validity strictu sensu and, more generally, logical validity. In
addition, they should be understood as having epistemic connotations that make the
respective inferences capable of forcing, or justifying. Be that as it may, we can from
the beginning point out that even in this case (c) cannot consist in the requirement
that the agent has an explicit knowledge of inferential validity. In other words, when
added to (a) and (b), the condition
(c3)
A knows that there is a valid argument or proof from  to α
blocks any possibility of explanation, just like (c1). In fact, there seems to be a
wider problem already with the relation of explicit knowledge, the excessive force
of which is by itself able to induce regresses:
there are other objections, not connected with any particular view on the validity of
inferences, against the requirement expressed in 2) that one has to know the validity of
an inference if one is to be justiﬁed by the inference in holding the conclusion true. Such
a requirement seems directly to give rise to a circle or a new regress, perhaps a more
straightforward one than the regress noted by Bolzano and Carroll. It appears as soon
as we assume that the knowledge of the validity of the inference has to be explicit and
ask how we know the validity. If the validity is not immediately evident, it must come
about by a demonstration, whose inferences must again be known to be valid according to
the requirement expressed in 2). These inferences must either be of the same kind as the
inferences whose validity we try to demonstrate or be of another kind, and so we get either
into a circle or regress. It hardly seems reasonable to think that this circle or regress can
be avoided by saying that, for sufﬁciently many inferences, their validity is immediately
evident. (Prawitz, 2013, 192)
Therefore, whatever the selected deﬁnition of inferential validity, we are forced
to abandon the idea that the knowledge of the validity of an inference may serve
the purpose of obtaining epistemic constraint or justiﬁcation through the latter; in
conclusion,

82
4
Prawitz’s Theory of Grounds
we need to ﬁnd a relation R between a person P and an inference J in terms of which we
can state a condition that satisﬁes the following demands. On one hand, it is to be substantial
enough so that [ . . . ] it implies that the person is justiﬁed in holding true the conclusion of
the inference [ . . . ] on the other hand, it is not to be so strong that [ . . . ] it cannot be satisﬁed
when taken as a necessary condition for an inference to justify a belief. (Prawitz, 2013, 197)
But what can this relationship be? In answering, it becomes essential to remember
that, against a frequent reductive understanding of them as simple sets of premises
and conclusion, inferences are to be understood as oriented acts, as transitions
between judgments or assertions serving the purpose of getting reason, ground,
evidence, justiﬁcation or guarantee by virtue of a support provided by knowledge
(assumed as) acquired.
From an intuitive point of view, it is the inferential act as such, without further
additions, to force or give justiﬁcation. If it is true that correct deductive practices
exert an epistemic force on us, the correct inferential acts of which they are
ultimately composed must be the means through which this force is carried on. By
performing these acts, we are eventually induced to accept or take for granted the
conclusion. Therefore, Prawitz’s natural and plausible proposal is that the desired
condition (c) simply has the form
(c4)
A performs I.
Here, we must be careful. What does it mean to perform an inference? We already
warned against the risks of a reductionist reading, according to which performing
an inference means only to pass from the judgments expressed by the premises
to the judgments expressed by the conclusion or, at a linguistic level, to perform
a complex act consisting in asserting the premises, in saying “therefore” or other
equivalent expressions, and ﬁnally in asserting the conclusion. On that occasion, it
was said that such a reconstruction seems to force us to abstract relations between
premises and conclusion. Related to this, Prawitz pinpoints that:
a person announces an inference in the way described, say as a step in a proof, but is not able
to defend the inference when it is challenged. Such cases occur actually, and the person may
then have to withdraw the inference, although no counter example may have been given.
If it later turns out that the inference is in fact valid, perhaps by a long and complicated
argument, the person will still not be considered to have had a ground for the conclusion at
the time when she asserted it; (Prawitz, 2009, 186–187)
Fermat had of course some arguments in mind; his problem was only that the margin where
he announced his theorem was too small. Suppose that we found these arguments in some
other notes by Fermat and that they could now be shown to be valid. One would still suspect
him to be unjustiﬁed, if the validity was shown by advanced mathematics that Fermat had no
access to: that the argument was stated and happened to be valid do not change the matter.
(Prawitz, 2012a, 891)
It is therefore necessary to make the inferential acts something more than mere
transitions. Actually, this seems to be independent of, and indeed to be a necessary
condition for, a deﬁnition of the notion of valid inference that allows us to fulﬁll
the “fundamental task”. Just as with the notions of evidence and inference, an
indissoluble bond unites, in the theory of grounds, the most appropriate description

4.1
From Inferences to Proofs, via Grounds
83
of what to perform an inference means and the adequate deﬁnition of valid
inference. The main suggestion regarding this problem comes, not surprisingly, from
Prawitz himself:
having arrived at a contradiction under an assumption α, we conclude that the assumption
is false, saying “hence, by reduction not-α”. [ . . . ] the assertion of the conclusion is
accompanied by an indication of some kind of operation that is taken to justify it. This is
consonant with an intuitive understanding of an inference as consisting of something more
than just a conclusion and some premises. Although the conclusion and the premises may
be all that we make explicit, there is also some kind of operation involved thanks to which
we see that the conclusion is true given that the premises are. [ . . . ] My suggestion is that
in analysing the validity of inference, we should make this operation explicit, and regard
an inference as an act by which we acquire a justiﬁcation or ground for the conclusion by
somehow operating on the already available grounds for the premises. (Prawitz, 2013, 199)
At the conclusion of this section, it is perhaps appropriate to make a ﬁnal point.
Suppose we deﬁne the validity of inferences as existence of valid arguments or
proofs from premises to conclusion, and attribute to the condition (c) the form
(c4), where “making an inference” is understood in merely transitional terms. Then,
the impossibility of fulﬁlling in this way the “fundamental task” becomes even
more evident if we recall the three problems that we have seen to afﬂict Prawitz’s
semantics of valid arguments and proofs—the recognizability problem, the proofs-
as-chains problem, and the problem of independence of validity from inferences of
which arguments and proofs are made up. Let I be a valid inference (represented by
a constructive procedure f ), and suppose that agent A is in possession of evidence
for (the types in the domain of f representative of) the premises of I; in the approach
under examination, such an evidence will amount to valid arguments or proofs in
canonical form. Applying to this evidence (the procedure f which represents) I, A
obtains, respectively, a valid argument  or a proof π:
Recognizability if  is a non-canonical argument or π a categorical or non-
canonical proof (depending on our preferred terminology), on account of the
problem of recognizability we cannot assume that being in possession of  or
π, i.e. knowing how to get a valid argument or a canonical proof, means also to
know that  or π reduce to such valid argument or canonical proof. Thus we
cannot say that A is in possession, albeit indirectly, of evidence for the related
conclusion;2
2 In the case of proofs, the constructive procedures representing inferences are deﬁned on proofs
and give proofs as output; this allows us to redeﬁne the inferential act as a computation of a
procedure of the intended type. When the constructive procedure stands for a valid inference,
it produces canonical proofs if applied to canonical proofs; this instead means that what we
actually need at the level of objects is only the notion of canonical proof, and the problem of
recognizability, which is less urgent in the canonical case, can thus be referred in the full sense
only to the acts performed in the deductive process. The general setup is now, in a sense, less
problematic (while still remaining problematic) since, although the act accomplished might not
recognizably be such as to produce evidence, we are in possession of the result of this act. The
distinction between objects and acts makes it possible to distinguish between what the agent has
and what the agent does, and the agent themselves would occupy mental states reiﬁed by only

84
4
Prawitz’s Theory of Grounds
Proofs-as-chains on the other hand, if we really want to maintain that the mere
possession of  or π is sufﬁcient to compel or to justify, we need to be able to
look at  or π as a chain of inferences that satisfy the property requested by
the “fundamental task”-which we aim to prove for (f representative of) I. But
it is not clear how this can happen, since, on account of the distinction between
canonical and non-canonical cases,  or π might contain inferences of the same
type as (f representative of) I with an identical or greater complexity3;
canonical objects that the theory treats as evidence. A similar strategy does not seem possible in
the case of valid arguments; here, the fact that justiﬁcations are deﬁned on argument structures and
give argument structures as output, implies that the reduction—what de facto forces or justiﬁes—
is external to the construction of the argument, and therefore to the individual inferential steps
which this construction amounts to. Nonetheless, one might argue, when we are in possession
of a valid non-canonical argument, we are factually in possession of a method to obtain a valid
canonical argument—in other words, a valid non-canonical argument denotes a valid canonical
argument (see, for example, Tranchini, 2016). Then, when in possession of a non-canonical valid
argument, the inferential agent occupies a mental state reiﬁed by an object that is the result of the
method which the valid non-canonical argument amounts to—namely, its denotation. However,
this objection is problematic; an argument is a linguistic structure, and as such it is already in itself
an object. If making a valid non-introductory inference is reductionistically understood as the mere
passing from premises to conclusion, by applying the inference the agent comes to occupy a mental
state which will be, as an object, the correspondent valid non-canonical argument, and not the
result of the method which it amounts to, or its denotation. Otherwise, we need more strongly—
and against the hypotheses of the issue we are dealing with here—conceive the performing of
a valid inference in a non-introductory form as the application of the related justiﬁcation. Even
assuming the plausibility of such a move—which is doubtful, since justiﬁcations are deﬁned on
argument structures and give argument structures as output—it is clear that the resulting framework
is de facto identical to that of proofs; to make an inference means to apply a certain constructive
procedure, and it also becomes necessary to distinguish between exclusively canonical objects
that the theory treats as evidence—the valid canonical arguments—and the acts that produce such
objects—the valid non-canonical arguments.
3 In a reductionist view of inferences as mere passages, the proofs-as-chains problem seems to
block the way even to an explanation that sets validity of inferences in terms of existence of valid
arguments or proofs from premises to conclusion, and that attributes to (c) the form (c2). In fact,
the identiﬁcation of the validity of (f representative of) I can either take place by performing
I (as a construction of the functional given by the constructive procedure f which represents
I applied to appropriate values), or can be external to such performance. In the ﬁrst case, we
obtain a picture similar to that suggested for proofs—and, in a purely hypothetical way, for
the valid arguments of note 1 above: inferences cannot be simple transitions from premises to
conclusion, since they consist rather in the computation of constructive procedures of a certain
type; the objects of the theory are always canonical, and the distinction between canonical and
non-canonical cases concerns the acts by which these objects are obtained. In the second case, what
the agent sees by performing I (as construction of the function given by the constructive procedure
f which represents I applied to appropriate values) is that, whenever there are valid closed
arguments (possibly non-canonical) or proofs (possibly non-canonical or categorical, depending on
the preferred terminology) for the premises, through (f representative of) I we get a valid (possibly
non-canonical) argument or a proof (possibly categorical or non-canonical) for the conclusion;
this means that the agent recognizes that  or π reduce to a valid canonical argument ∗or to
a canonical proof π∗(possibly identical to the ﬁrst one). In order that this be sufﬁcient for the
agent to be forced or justiﬁed towards the conclusion, however, we must be able to look at ∗or
π∗as a chain of inferences that satisfy—what we aim to prove for (f representative of) I—the

4.1
From Inferences to Proofs, via Grounds
85
Independent validity if  is a non-canonical argument or π a categorical or non-
canonical proof (depending on the preferred terminology), this may not depend
at all on (f representative of) I, but simply on the justiﬁcation associated
with I (or, in the other case, on the way f is deﬁned). However,  and π
can have an epistemic weight only by virtue of the fact that they reduce to
what the theory treats as evidence—a valid canonical argument or a canonical
proof, respectively. In this regard, it is the process of reducing  (that involves
the reduction associated with I) or the computation of π (which involves an
application of the deﬁning equation of f ) that proves that  and π reduce in the
manner described. Therefore, strictly speaking, it is not the fulﬁllment as such
of I (as a construction of the functional given by the constructive procedure f
which represents I applied to appropriate values) to give the constraint or the
justiﬁcation required.
4.1.3
Prawitz’s Notion of Ground
The “fundamental task” requires a deﬁnition of the notion of deductively valid
inference that depends on that of evidence. In this perspective, the fact that proofs
are intended as what we have when we are epistemically compelled or justiﬁed in
judging or in asserting could lead to deﬁning evidence in terms of valid arguments
and proofs. However, we have seen how this way is not practicable; the problems
met by Prawitz’s semantics of valid arguments and proofs reveal the need for a
deﬁnition of the notion of deductively valid inference, and therefore of evidence,
independent of the notions of valid argument and proof, in such a way that the latter
are deﬁnable in terms of the former. This is also consistent with the usual way in
which proofs themselves are characterized; by deﬁning deductively valid inferences
in terms of valid arguments and proofs, Prawitz’s semantics of valid arguments and
proofs reverses the Cartesian idea of proofs-as-chains.
However, proofs are clearly closely linked with both evidence and deductively
valid inferences. Therefore, the levelling of the notion of evidence on that of proof
may not be wrong after all; what we have when we are epistemically successful
should perhaps be distinguished from the activity through which this success is
achieved. According to this line of thinking, there are two different notions of proof,
or at least two different points of view, summarized in the distinction, introduced by
Martin-Löf (1984) and developed by Sundholm (1998, but see also 1983, 1993),
between proof-objects and proof-acts. According to these authors, judgments are
linked to an activity that consists in catching a certain object of knowledge; if this is
property required by the “fundamental task”. And as soon as ∗or π∗end by introducing →, the
corresponding immediate subargument or substructure could, on account of the distinction between
canonical and non-canonical cases, contain inferences of the same type as (f representative of) I
with equal or greater complexity.

86
4
Prawitz’s Theory of Grounds
true, proofs for judgments will ultimately have to take the shape of acts—proofs-
acts. Proofs for propositions, however, especially when understood as abstract
mathematical entities, and devoid of a real epistemic connotation, will be pure and
simple objectual constructions—proof-objects. Proof-objects and proof-acts are, of
course, closely linked:
the proofs occurring in the meaning explanations are constructions [ . . . ], whereas what is
required for an assertion that a certain proposition is true is that a construction [ . . . ] has
been carried out. (Sundholm, 1998, 187)
The proof-object is the construction-object, that is, the object of the construction-act that
forms part of the proof-act. (Sundholm, 1998, 193)
Through the deductively valid inferences of which they consist, proof-acts allow us
to obtain proof-objects. But then, although it has to be independent, the notion of
evidence cannot be totally detached from that of proof and therefore, again, from
that of deductive inference.
The two knots highlighted here—the independence of the notions of deductively
valid inference and evidence from that of proof, and the shared articulation of these
three notions—offer as many interconnected points of access to a characterization
of the notion of ground.
4.1.3.1
Evidence States and Primitive Operations
Although the term “ground” often occurs in the writings of Prawitz, the notion
associated with it receives, in the theory of grounds, a radically different declination,
as well as a more speciﬁc scope:
in some previous works [ . . . ] I have identiﬁed a ground for a judgment with a proof of
the judgment, or I have spoken of grounds for sentences and have taken them to be valid
arguments. I prefer not to use that terminology now, because I want to take proofs to be
built up by inferences, and I do not want to say that an inference constitutes a ground
for its conclusion - the question is instead how an inference can deliver a ground for the
conclusion. (Prawitz, 2009, 191–192).
In this new meaning, the concept of ground appears for the ﬁrst time in the 2006
article Validity of inferences—published in 2013:
To have a ground for a sentence α, as I use the term, will always mean to be justiﬁed in
holding α true and to know that α. (Prawitz, 2013, 175)
I here use the term ground for a sentence to denote what a person needs to be in possession
of in order to be justiﬁed in holding the sentence true. (Prawitz, 2013, 193–194)
However clear, this characterization is rather lapidarian. It keeps at least two
fundamental questions open: what kind of objects are grounds, and what does it
mean to possess them? In Inference and knowledge Prawitz points out signiﬁcantly
how rational judgments and sincere assertions must be understood
to be made on good grounds. It is not that an assertion is usually accompanied by the
statement of a ground for it; in other words, the speaker often keeps her ground for herself.

4.1
From Inferences to Proofs, via Grounds
87
But if the assertion is challenged, the speaker is expected to be able to state a ground for it.
To have a ground is thus to be in a state of mind that can manifest itself verbally. (Prawitz,
2009, 190–191)
This passage contains two very important pieces of information. First, to possess
grounds means occupying a mental state in which the subject has justiﬁcation for
judgments or assertions; second, there is often no trace of the available grounds
in the linguistic practice, with which the acquisition is associated—although
possession could be made explicit.
The idea that the possession of grounds can be expressed in terms of mental states
of justiﬁcation is again found in The epistemic signiﬁcance of valid inference, but
it is in Explaining deductive inference that this idea investigated in detail. Prawitz
introduces here the discussion of the concept of ground, and consequently the theory
that he articulates around it, with the following words:
one ﬁnds something to be evident by performing a mental act [. . . ]. After having made such
an act, one is in an epistemic state, a state of mind, where the truth of a certain sentence is
evident, or as I have usually put it, one is in possession of evidence for a certain assertion.
(Prawitz, 2015, 88)
This last step highlights an aspect of fundamental importance. It introduces a link
between what, according to the perspective in question, one is in possession of when
in a mental state of justiﬁcation, and who is in this state: the former can be obtained
by the latter by performing some kind of operations. It then becomes possible to
articulate the analysis, both in the sense of characterizing the nature of the grounds
through those operations of which they are the outcome and, in the other direction,
to show more precisely in what sense the notion of ground should be linked to the
states in which we have evidence for judgments or assertions:
rather than trying to analyse phenomenologically the states of mind where we experience
evidence - let us call them evidence states - we have to say what evidence states are possible
and what operations are possible for transforming one evidence state to another. [. . . ] To
state principles like this, it is convenient to think of evidence states as states where the
subject is in possession of certain objects. I shall call these objects grounds [. . . ]. I am so to
say reifying evidence and am replacing evidence states with states where the subject is in
possession of grounds. (Prawitz, 2015, 88–89)
As can be expected, there is a parallelism between the operations that allow us
to have grounds and the inferences which Prawitz intends to deal with: by virtue of
the “fundamental task” we have frequently referred to, in fact, valid inferences too
are such that, by fulﬁlling them, one then ﬁnds oneself in a state of evidence. This
similarity will be one of the key points of the theory of grounds; without anticipating
what we will say more broadly thereafter, the observation nevertheless allows us to
touch a critical point. Prawitz draws attention to the possible circularity resulting
from the interplay of two circumstances:
for logically compound sentences there seems to be no alternative to saying that evidence
comes from inference. On the other hand, since not any inference gives evidence, one cannot
account for evidence by referring to inferences without saying which inferences one has in
mind. (Prawitz, 2015, 77)

88
4
Prawitz’s Theory of Grounds
The solution to this impasse comes again from the adoption of an approach that
recalls the Wittgensteinian motto (Wittgenstein, 1953) of meaning as use in the anti-
holistic articulation of Dummett’s interpretation (Dummett, 1978b, 1996a,b):
it is in the nature of the meaning of some types of sentences that evidence for them
can only be explained in terms of certain kinds of inferences. The legitimacy of these
inferences is then a datum that has to be accepted as somehow constitutive for the meaning
of these sentences. [. . . ] But not all cases of accepted inferences can reasonably be seen as
constitutive of the meaning of the involved sentences [. . . ]. For them, it remains to explain
why they are legitimate. (Prawitz, 2015, 77–78)
These words refer exclusively to inferences, although, by virtue of the link with
the operations involved in the concept of ground, we can expect that the idea that
meaning should be expressed in terms of certain types of inferences is transformed,
or can be transformed, into the idea according to which meaning should be
expressed in terms of certain types of operations and, therefore, of grounds. In fact,
the reference to the problem of meaning is often explicit, and immediately involves
the introduction of operations for the production of grounds:
we may want to say that anyone who knows the meaning of the sentence t = t has access
to a state in which she has evidence for asserting it. Similarly, we may want to say that
anyone who knows the meaning of conjunction and is in a state in which she has evidence
for asserting a sentence α as well as for asserting a sentence β, can put herself in a state in
which she has evidence for asserting α∧β. [. . . ] We can make the latter even more articulate
by saying that there is an operation ∧I that applied to grounds g1 and g2 for asserting α and
β, respectively, produces a ground ∧I(g1, g2) for asserting α ∧β. (Prawitz, 2015, 88–89)
Aiming to explain the epistemic power of deductively valid inferences, the theory
of grounds therefore provides also a characterization of the meaning of propositions
or sentences in terms of what counts as a ground for them, calling into question
primitive and, as such, meaning-constitutive operations. Although Prawitz often
proposed this idea, his clearest and most direct formulation can be found in The
epistemic signiﬁcance of valid inference, where we require a
meaning theory that explains the meaning of our sentences directly in terms of what
constitutes grounds for the corresponding assertions [. . . ]. To have a name for how the
grounds for asserting α ∧β is formed, let us call it conjunction grounding, for short ∧I,
a primitive operation that we introduce in this connection. [. . . ] a ground for the assertion
of a numerical identity would be obtained by making a certain calculation, and outside of
mathematics, a ground for asserting an observation sentence would be got by making an
adequate observation. (Prawitz, 2012a, 893)
This determination of meaning obviously has many consequences; as rightly pointed
out by Cozzo, in fact, an operation as ∧I is also
the core of a semantic and ontological explanation. It serves to explain the content of the
judgment. Its content is that a proposition α ∧β is true. So to explain the content of the
judgment is to explain what that proposition is. This is done by Prawitz in terms of the
operation of conjunction-grounding: it is “a proposition such that a ground for judging it to
be true is formed by bringing together two grounds for afﬁrming the two propositions α and
β”. (Cozzo, 2015, 110; the internal quote is from Prawitz, 2009, 194)

4.1
From Inferences to Proofs, via Grounds
89
It must however be pointed out that, in relation to empirical, or more generally
atomic, propositions or sentences, Prawitz does not go much beyond what we
have already mentioned here—a ground for an empirical proposition or sentence
is obtained by making an appropriate observation, while a ground for a numerical
identity is, for example, obtained by performing a certain computation. This, one
could argue, is testimony of the fact that the interest of the Swedish logician is
primarily aimed at a determination of the notion of ground, and therefore, as we
shall see, of inferential validity, for ﬁrst-order languages, so that the determination
of the meaning of the logical constants becomes a priority. However, we cannot fail
to emphasize how the notion of ground on the empirical, and more generally atomic,
case seems fraught with conceptual difﬁculties. We will not enter such a complex
issue—the interested reader may refer to the bibliography dedicated to it (see, for
example, Brîncu¸s, 2015 and Usberti, 2015).
Since the ﬁrst articles dedicated to the theory of grounds—Validity of inferences
(Prawitz, 2013), Inference and knowledge (Prawitz, 2009) and The epistemic signif-
icance of valid inference (Prawitz, 2012a)—the notion of ground for propositions or
sentences of different logical form has been ﬁxed by inductive clauses in which the
transition from grounds from propositions or sentences of lower logical complexity
to grounds for propositions or sentences of greater logical complexity occurs by
virtue of primitive operations, which are thus constitutive of the meaning itself.
However, in Explaining deductive inference this setting is signiﬁcantly enriched by
some important speciﬁcations:
the grounds will be seen as abstract entities. As such we get to know them via descriptions.
To form a ground for an assertion is thus to form a term that denotes the ground, and it is
in this way that one comes in possession of the ground. Simultaneously with saying what
grounds and operations on grounds there are, I shall therefore indicate a language in which
the grounds can be denoted. [. . . ] the grounds that come out of this enterprise will be like
intuitionistic constructions and the language in which they will be described will be like
the extended lambda calculus. The type structure will however be made more ﬁne-grained
by using sentences as types following Howard (1980), so that the question whether a term
denotes a ground for an assertion of a sentence α coincides with the question of the type of
the term. (Prawitz, 2015, 89)
First of all, Prawitz takes up the basic idea of the formulas-as-types conception
involved in the so-called Curry-Howard isomorphism (Howard, 1980): grounds are
typed abstract objects or, in other words, a ground for ⊢α is an abstract object of
type α. In addition, Prawitz now suggests the idea of developing a formal language
of grounds, of which the terms, also typed, denote grounds. The general lines along
which such formal language is to be developed is determined by assuming in the
ﬁrst place to dispose of
a ﬁrst order language and with that individual terms and what counts as grounds for asserting
atomic sentences. (Prawitz, 2015, 90)
This ﬁrst-order language, which we will call background language, is what the
language of grounds speaks of. Therefore, in the language of grounds we will
have, in addition, of course, to the individual variables x, y, . . ., ground-variables
ξα, ξβ, . . . intended to range over grounds for α, β, and primitive operations related

90
4
Prawitz’s Theory of Grounds
to each of the logical constants of the background language, indicated respectively
with ∧I, ∨I1, ∨I2, →I, ∀I, ∃I. Among the atomic formulas of the background
language, Prawitz usually adds an atomic constant for the absurd ⊥, and considers
¬α as an abbreviation of α →⊥.
If the typing of ground-variables is obvious—a ground-variable ξα obviously
having type α—Prawitz is keen to specify that also
the ground constants [. . . ] are given with their types. The primitive operations are also to
be understood as coming with types, although this may be left implicit. This is harmless,
except for ∨I1, ∨I2 and ∃I (Prawitz, 2015, 90)
The typing of complex terms will therefore be determined in the following way:
(∧τ)
if T has type α and U has type β, ∧I(T, U) has type α ∧β
(∨τ)
if T has type αi, ∨Ii[αi ▷α1 ∨α2](T ) has type α1 ∨α2 (i = 1, 2)
(→τ)
if T has type β, →Iξα(T ) has type α →β
(∀τ)
if T has type α, ∀Ix(T ) has type ∀xα
(∃τ)
if T has type α(t/x), ∃I[α(t/x) ▷∃xα(x)](T ) has type ∃xα(x)
where it should be noted that: if in (∨τ) the type of ∨Ii were not speciﬁed, we
could not know the type of the term that ∨Ii produces, since we could not know the
second element of the corresponding disjunction; and if, in (∃τ), the type of ∃I were
not speciﬁed, we could not know the type of the term that ∃I produces, since we
could not know either what term of the starting formula is existentially quantiﬁed,
or which individual variable is actually used for the existential quantiﬁcation. The
primitive operation →I is followed by a ground-variable, which indicates that this
operation binds that ground-variable in the immediate subterm—and also allows
the determination of the resulting type; the primitive operation ∀I is followed by an
individual variable, which indicates that this operation binds that individual variable
in the immediate subterm—and also allows the determination of the resulting type.
In the latter case, we have a restriction similar to the one in the rule of introduction
of universal quantiﬁcation in a Gentzen system; x must not occur free in δ for ξδ
free in the immediate subterm.
The determination of the typing provides a useful ﬁrst indication of the terms
that, within the formal language under examination, will denote grounds. However,
we can do more, namely
we may also say directly what grounds there are for different sentence forms. (Prawitz,
2015, 91)
The indication of the grounds for propositions or sentences of different logical
form is entrusted to inductive clauses that involve the primitive operations ∧I, ∨I1,
∨I2, →I, ∀I, ∃I. Assuming that the individual ground constants available in the
language of grounds denote grounds for the atomic formulas that constitute their
type, and that there are no grounds for ⊥, in the case of ∧, ∨and ∃, given α, β,
∃xα(x) closed, and t closed, the clauses will be
•
if T denotes a ground for ⊢α and U denotes a ground for ⊢β, ∧I(T, U) denotes
a ground for ⊢α ∧β

4.1
From Inferences to Proofs, via Grounds
91
•
if T denotes a ground for ⊢αi, ∨Ii[αi ▷α1 ∨α2](T ) denotes a ground for
⊢α1 ∨α2 (i = 1, 2)
•
if T denotes a ground for ⊢α(t/x), ∃I[α(t/x) ▷∃xα(x)](T ) denotes a ground
for ⊢∃xα(x)
Prawitz introduces this ﬁrst set of clauses separately from those for →and ∀, and
this because, in the latter case, it is necessary to go beyond the primitive operations
discussed so far, and take into account a general notion of operation on grounds.
4.1.3.2
Operations on Grounds and Open Languages
When we presented the BHK semantics for the ﬁrst-order logical constants, we
also observed how the clauses related to →and ∀require a primitive notion of
constructive procedure; a BHK proof of α →β is a constructive procedure that
transforms BHK proofs of α into BHK proofs of β, while a BHK proof of ∀xα(x)
is a constructive procedure that, for each individual k, produces a BHK proof of
α(k). In the theory of grounds the discourse is very similar: a ground for ⊢α →β
will involve a constructive procedure that turns grounds for ⊢α into grounds for
⊢β, whereas a ground for ⊢∀xα(x) will involve a constructive procedure which,
given an individual k, produces a ground for ⊢α(k). How can we understand now
the constructive procedures which the theory of grounds refers to?
The only operations which we have dealt with so far are primitive operations
for the speciﬁcation of meaning. When applied to grounds of the appropriate type,
primitive operations produce, by deﬁnition, grounds of an appropriate type. In this
regard, however, Prawitz observes that
this simple way of getting grounds [. . . ] does not go very far, of course, and in general, one
has to deﬁne new operations to this end. (Prawitz, 2015, 92)
The content of the last quote can be illustrated with a simple example. Given
grounds g1 for ⊢α1 and g2 for ⊢α2, we can apply to them the primitive
operation ∧I obtaining, by virtue of the clause that ﬁxes what counts as ground
for a conjunction, a ground ∧I(g1, g2) for ⊢α1 ∧α2. Conversely, given a ground
∧I(g1, g2) for ⊢α1 ∧α2, with g1 ground for α1 and g2 ground for ⊢α2, it seems
reasonable to afﬁrm that we can apply to ∧I(g1, g2) a “projection” operation, which
selects g2 and thereby returns a ground for ⊢α2. This “projection” is undoubtedly
constructive, but it is equally clear that it cannot be any of the primitive operations;
primitive operations, in fact, only authorize passages from grounds for propositions
or sentences of lower logical complexity to grounds for propositions or sentences of
greater logical complexity.
Given the possibility of contemplating non-primitive operations, it is obvious
that the production of grounds by means of these operations will have to agree with
the way the notion of ground is determined. A non-primitive operation must be

92
4
Prawitz’s Theory of Grounds
ﬁxed by equations that establish its behavior with regard to the primitive operations.
Therefore, with this aim in view, Prawitz states that
for any closed sentence α1 ∧α2 we can deﬁne two operations, which I call ∧E,1 and ∧E,2,
both of which are to have as domain grounds for α1 ∧α2. The intention is that the operation
∧E,i is always to produce grounds for αi (i = 1, 2) when applied to grounds for α1 ∧α2
[. . . ]. This is attained by letting the two operations be deﬁned by the equations
∧E,1(∧I(g1, g2)) = g1 and ∧E,2 (∧I(g1, g2)) = g2.
The fact the operation ∧E,i produces a ground for αi when applied to a ground g for α1 ∧α2
[. . . ] depends on what ∧means, and has to be established by an argument. (Prawitz, 2015,
92)
The argument to which Prawitz refers is rather simple—our reconstruction will
temporarily disregard certain identity conditions on the primitive operations: given
a ground g for ⊢α1 ∧α2, by virtue of the clause ﬁxing what counts as ground for a
conjunction, g must be of the form ∧I(g1, g2), with gi ground for ⊢αi; by virtue of
the equations that deﬁne ∧E,i, we will then have ∧E,i(g) = ∧E,i(∧I(g1, g2)) = gi;
therefore, for every g ground for ⊢α1∧α2, ∧E,i(g) produces a ground for ⊢αi (i =
1, 2). Other examples of non-primitive operations, provided by Prawitz himself, are:
a binary operation →E for the elimination of implication that produces grounds
for ⊢β when applied to grounds for ⊢α →β and for ⊢α, ﬁxed by the equation
→E(→Iξα(T (ξα)), U) = T (U);
a binary operation Barb for the syllogism in Barbara, that produces grounds for
⊢∀x(P(x) →R(x)) when applied to grounds for ⊢∀x(P(x) →Q(x)) and for
⊢∀x(Q(x) →R(x)), ﬁxed by the equation
Barb(∀Ix(T ), ∀Ix(U)) = ∀Ix(→IξP(x)(→E(U, →E(T, ξP(x)))));
and a binary operation Mtp for disjunctive syllogism, that produces grounds for ⊢β
when applied to grounds for ⊢α ∨β and for ⊢¬α, ﬁxed by the equation
Mtp(∨I2(g1), g2) = g1.
Obviously, in order to give a precise deﬁnition of the notion of grounds for
implications or universal quantiﬁcations, it is necessary to pass from examples of
speciﬁc operations (be they primitive or non-primitive) to a discussion of a more
general nature. From this perspective, a ﬁrst signiﬁcant observation concerns the
distinction between grounds, understood as reiﬁcations of mental states in which
we possess a conclusive justiﬁcation for categorical judgments or assertions, and
operations on grounds; the latter, producing grounds of the appropriate type when
applied to grounds of the appropriate type or, as we will see below, to individuals,
can be understood as grounds for hypothetical-open judgments or assertions. It then

4.1
From Inferences to Proofs, via Grounds
93
becomes important to ask oneself how an operation can be identiﬁed, and here we
can with Prawitz afﬁrm that
an operation is given by stating the types of its domain and range and, for each argument in
the domain, the value it produces for that argument. (Prawitz, 2015, 92)
Thus, for example, the primitive operation ∧I has as domain grounds for ⊢α and
grounds for ⊢β and as codomain grounds for ⊢α ∧β, which can be also expressed
by saying that ∧I has operational type
α, β ▷α ∧β.
Given a ground g1 for ⊢α and a ground g2 for ⊢β, ∧I(g1, g2) is a ground for
⊢α ∧β; of course, being a primitive operation, the production of grounds takes
place, as it were, automatically, so that there is no need—nor would it be possible—
to further specify the behavior of such operation. The non-primitive operation ∧E,i
will have operational type
α1 ∧α2 ▷αi
(i = 1, 2). This is guaranteed by the deﬁning equations for ∧E,i illustrated above.
Again by virtue of the corresponding deﬁning equations, we can say that the non-
primitive operations →E, Barb and Mtp have, respectively, operational types
α →β, α ▷β
∀x(P(x) →Q(x)), ∀x(Q(x) →R(x)) ▷∀x(P(x) →R(x))
α ∨β, ¬α ▷β
The speciﬁcation of operations on grounds of different nature is conducted by
Prawitz by degrees. First of all, the Swedish logician says that an operation is of
operational type
α1, . . . , αn▷β, where α1, . . . , αn, β are closed sentences, if it is an n-ary effective operation
that is deﬁned whenever its ith argument place is ﬁlled with a ground of type αi and then
always produces a ground of type β. Such an operation is a ground for the hypothetical
assertion α1, . . . , αn ⊢β. (Prawitz, 2015, 92)
The limitation to closed formulas in the passage above is only a workaround.Shortly
afterwards Prawitz states that a ground for α(x1, . . . , xm)
is an effective m-ary operation deﬁned for individual terms that always produces a ground
for asserting α(t1/x1, . . . , tm/xm) when applied to t1, . . . , tm. (Prawitz, 2015, 92)
As an obvious generalization, we have that, if an operational type
α1, . . . , αn ▷β

94
4
Prawitz’s Theory of Grounds
involves both open and closed formulas, an operation of this operational type will
be
an n-ary effective operation from operations to operations [. . . ]. It is again a ground for
α1, . . . , αn ⊢β. (Prawitz, 2015, 92)
The last and most complex case discussed by Prawitz is that of an operation of
operational type
(1 ▷α1, . . . , n ▷αn) ▷( ▷β)
where i,  are sets of (open or closed) formulas, with  ⊆
i≤n i, and which
will be
an n-ary effective operation from operations [. . . ] to operations. (Prawitz, 2015, 93)
As a whole, therefore, the theory of grounds uses, like BHK semantics, a primitive
notion of constructive procedure; what Prawitz importantly adds, however, is a
speciﬁcation of the different operational types by which these procedures are
labelled. It is perhaps worthy remembering that, unlike what happens in Explaining
deductive inference (Prawitz, 2015), in Inference and knowledge (Prawitz, 2009)
and in The epistemic signiﬁcance of valid inference (Prawitz, 2012a) Prawitz does
not refer to constructive procedures but, in a more Fregean sense (Frege, 1891,
2001, but see also Kenny, 2003; Tranchini, 2019), to unsaturated grounds, as well
as saturated grounds, involving a substitutional approach of ground-variables with
saturated grounds and of individual variables with closed terms:
an unsaturated ground is like a function and is given with a number of open argument places
that have to be ﬁlled in or saturated by closed grounds so as to become a closed ground.
Something is a ground for an assertion β under the assumptions α1, . . . , αn if and only if
it is an n-ary unsaturated ground that becomes a closed ground for β when saturated by
closed grounds for α1, . . . , αn. [. . . ] We must [. . . ] consider unsaturated grounds that are
unsaturated not only with respect to grounds but also with respect to individuals that can
appear as arguments in propositional functions. (Prawitz, 2009, 193)
a ground for an assertion β under assumptions α1, . . . , αn is to be an unsaturated ground
which when saturated by grounds for the assertions α1, . . . , αn becomes a ground for
the assertion β. For terms that stand for grounds there is a corresponding distinction
between closed and open terms representing saturated and unsaturated grounds [. . . ]. A
ground for [. . . ] an open assertion, say α(x1, . . . , xn), is again an unsaturated ground, say
f (x1, . . . , xn), such that for individuals a1, . . . , an in the domain in question, f (a1, . . . , an)
is a ground for the assertion α(t1, . . . , tn), where ti denotes ai. (Prawitz, 2012a, 893–894)
Once the nature of the operations involved in the theory of grounds has been
clariﬁed, it becomes particularly easy to provide the clauses that ﬁx what counts as
ground for formulas having →and ∀as their main logical constant. Given α, β and
∀xα(x) closed,
•
if T denotes an operation on grounds of operational type α ▷β, or equivalently
a ground for α ⊢β, →Iξα(T ) denotes a ground for ⊢α →β

4.1
From Inferences to Proofs, via Grounds
95
•
if T denotes an operation on grounds of operational type α(x), or equivalently a
ground for ⊢α(x), ∀Ix(T ) denotes a ground for ⊢∀xα(x)
As we can easily see, the clauses for →and ∀place no limitation on the operations
that can be denoted by T in →Iξα(T ) or ∀Ix(T ); such operations can be primitive
and non-primitive—or, more frequently, result from the composition of primitive
and non-primitive operations—as well as responding to operational types of any
complexity. This leads, quite naturally, to two questions. Why not limit ourselves
only to primitive operations? Is it possible that the class of operations which the
theory of grounds refers to is captured by a closed language of grounds? (Where by
closed we mean a language equipped with a ﬁnite, or ﬁnitely axiomatizable, number
of operational symbols for operations from which all the others can be generated,
or, better, in terms of which all the others can be deﬁned).
Regarding the ﬁrst question, non-primitive operations are needed de facto
and de iure. As we shall see shortly, Prawitz understands the primitive opera-
tions as those that are applied in inferential passages in an introductory form,
and the non-primitive operations as those that are applied in non-introductory
inferential passages. Now, it is a commonly experienced fact that inferences in non-
introductory form are often performed in deductive practice. If we limited ourselves
only to primitive operations, therefore, we could not account for this aspect of
deductive practice as it is. However, a theory that seeks answer the question about
the power of epistemic compulsion of deductively valid inferences cannot fail to
distinguish, in the words of Dummett, between
the conditions for the utterance and the consequences of it. (Dummett, 1973, 11)
Therefore, the primitive operations, when applied in inferential passages in intro-
ductory form, capture only the conditions under which judgments and assertions can
be reasonably accomplished, whereas the non-primitive ones, when applied in non-
introductory inferential passages, capture the correct consequences of judgments
and assertions. If we limited ourselves only to primitive operations, therefore, we
could not account for this aspect of deductive practice as it has to be.
As for the second question, it is Prawitz himself who suggests a negative answer,
when he says that
the language of grounds is to be understood as open in the sense that symbols for deﬁned
operations of a speciﬁc type can always be added. (Prawitz, 2015, 92)
The fact that a formal language of grounds should be understood as open in the
speciﬁed sense, obviously depends on what we have just said regarding the need
to take into account both primitive and non-primitive operations; as a matter of
principle, the class of valid inferences in non-introductory form can be generated
by no ﬁnitely axiomatizable formal system. But there is another reason by virtue
of which a formal language of grounds cannot but be open to the addition of
primitive operations; according to Gödel’s incompleteness theorems, in fact, no
closed language of grounds, at least as powerful as a formal system for intuitionistic
ﬁrst-order arithmetic, can express, through its terms, all possible grounds on the

96
4
Prawitz’s Theory of Grounds
background reference language. Although inserted in a different context, which we
will deal with below, this observation is made by Prawitz himself:
we know because of Gödel’s incompleteness result that already for ﬁrst order arithmetical
assertions there is no closed language of grounds in which all grounds for them can be
deﬁned; for any such intuitively acceptable closed language of grounds, we can ﬁnd an
assertion and a ground for it that we ﬁnd intuitively acceptable but that cannot be expressed
within that language. (Prawitz, 2015, 98)
With these observations, we conclude our discussion of the notion of ground as it
appears in Prawitz’s writings. To these observations, we will add only a discussion
about what it means to be in possession of grounds, a point that will be of crucial
importance in some of the issues we will later face.
4.1.3.3
Possession of Grounds
Grounds are abstract entities. Since they are what we have when we are in a mental
state of justiﬁcation for judgments or assertions, it seems furthermore reasonable to
expect that they can have an epistemic character. Last but not least, as a result of
the fulﬁllment of certain acts, they can be constructed by appropriate agents. The
question we now want to ask ourselves is: in what does the possession of a ground
consist?
Following an interpretative line outlined by Cozzo (2015), and in part also
suggested by Usberti (2015), grounds are nothing more than objects that the
philosopher introduces to describe mental states of justiﬁcation, thus reifying them;
expressing oneself in terms of being in possession of grounds is only a formally
convenient way of representing the occupation of the mental states of justiﬁcation.
In particular, Cozzo draws our attention to the need to distinguish
the three levels of the picture outlined by Prawitz: a mental level, a linguistic level and a
mathematical level. [. . . ] to the mental level belong acts of judgments and other mental
epistemic acts like observation, calculation, reasoning. [. . . ] The genuine active experience
of making an inference also belongs to this level: it is the conscious act of moving mentally
from judgments-premises to a judgment-conclusion in such a way that the agent cannot
help but recognize that this movement leads the mind from correct premises to a correct
conclusion. [. . . ] to the linguistic level belong acts of assertions, and linguistic practices of
argumentation in support of assertions [. . . ] to the mathematical level belong mathematical
representations of the mental level in terms of grounds and operations on grounds. This is
the level of abstract entities. The logician resorts to abstract entities in order to construct a
theory that makes the mental phenomenon of deduction intelligible. (Cozzo, 2015, 108)
This point of view has the immediate effect of clarifying what we already have
pointed out: of the possession of grounds, through which the acquisition of evidence
is manifested, there is generally no trace in linguistic practice. The agent keeps,
according to Prawitz’s words, the ground for themselves, since the objects that the
theory introduces are something of which the agent is not necessarily aware. An
evidence of this would be, in a sense, the fact that

4.1
From Inferences to Proofs, via Grounds
97
when an assertion is justiﬁed by way of an inference, it is common to indicate this by
simply stating the inference [. . . ] and the premises of the inferences are then often called
the ground for the assertion. This way of speaking may be acceptable in an everyday context,
but it conceals the problem that we are dealing with, which is probably one reason why the
problem has been so neglected. (Prawitz, 2009, 191)
As further support, we recall that the simple announcing an inferential transition
prevents the fulﬁllment of the “fundamental task”. To this end, Prawitz warns,
it would not do to simply indicate either the premises, or the fact that they are
(possibly) true, or ﬁnally the grounds that (possibly) justify the corresponding
judgment or assertion:
the premises are judgments or assertions afﬁrming propositions, and the fact that one has
judged or asserted them as true cannot constitute a ground for the conclusion, nor can the
truth of the propositions afﬁrmed constitute such a ground. [. . . ] It is rather the fact, if it is
a fact, that the agent has grounds for the premises that is relevant for her having a ground
for the assertion made in the conclusion. But the grounds for the premises are grounds for
them, not for the conclusion. (Prawitz, 2009, 191)
In conclusion, it seems fair to say that, according to Prawitz’s intention, to speak of
grounds and of the possession of grounds is not a
realistic description of the mental act, but is suggested as a theoretical reconstruction of
what goes on when we pass from one evidence state to another. (Prawitz, 2015, 89)
However, the Swedish logician often expresses himself in terms of having
constructed, and of being mentally in possession of something on the basis of which
to have justiﬁcation. It would be precisely in this circumstance that the possession of
grounds may be made explicit—and, in addition, that the grounds may be “named”.
For example:
to be in possession of a ground [. . . ] means basically to have made a certain construction
in the mind of which the agent is aware, and which she can manifest by naming the
construction. (Prawitz, 2009, 199)
Put this way, it becomes possible to connect the notion of ground, and in particular
the notion of possession of grounds, to the BHK tradition. To do this, we cannot fail
to take into account one of Prawitz’s latest articles on the topic, entitled The seeming
interdependencebetween the concepts of valid inference and proof, in which Prawitz
asserts that
in the case of having a ground for the assertion of an arithmetical identity because of having
made the relevant calculation, one may have recorded the steps of the calculation, and one
has then a ground for the assertion also in the concrete sense of a protocol of the justifying
act open for inspection. The process by which one ﬁnds a construction g for a proposition
α can be recorded too. The resulting protocol can be seen as a description of not only the
process but also of the obtained object g. (Prawitz, 2019b, 5–6)
The idea of possession is accompanied here by another important suggestion. The
system of acts through which a ground is acquired can be recorded; the resulting
protocol can be seen, not only as an encoding of that process, but also as a
description of the ground itself. Now, these observations are inserted by Prawitz
in the context of a discussion and a dialogue with the intuitionistic tradition, in

98
4
Prawitz’s Theory of Grounds
particular as this tradition appears in Heyting’s writings. The original BHK clauses
do not ﬁx the concept of proof for judgments or assertions; on the contrary, they
inductively specify the notion of proof for propositions or sentences. This also
applies to the modiﬁcation that Prawitz introduces when developing his own notion
of proof in his 1977 and 2005 articles, which we have dealt with in Chap. 3; in
the latter case, however, the introduction of primitive recognition procedures seems
to indicate that the real question concerns proofs for judgments or assertions or,
at the very least, the performance of proofs for propositions or sentences in the
deductive practice. A proof of a proposition or sentence α ∧β can be perfectly
understood as a simple pair, without any recognition of the fact that the formation
of this pair authorizes us to assert α ∧β, or to judge it as true. The subsequent
request becomes relevant only when we pass from the level of abstraction—in
which certain mathematical objects are structurally related to certain propositions
or sentences as their proofs—to the necessity of identifying conditions of correct
judgmentability or assertability. The tension between these two ways of looking
at proofs is already present in Heyting (1931, 1934), as a distinction between
constructions for propositions or sentences and realizations of such constructions.
Heyting’s notion of proof is linked to his way of understanding propositions
and assertions. The former express the intention to ﬁnd a construction, the latter
the realization of the intention expressed by the asserted proposition. In this
framework, a proof of a proposition would consist in the process of realization of the
construction. The way Heyting uses the term “proof” ﬂuctuates essentially between
construction and realization of this construction; in his 2019 article, Prawitz says
that
the term “construction” occurs frequently in Heyting’s writing, and [. . . ] one should
distinguish between different senses of that word, among them “process of construction”
and “object obtained as the result of a process of construction”. Heyting uses the term in
at least both these two senses, but I think that it is usually clear which one is intended.
When he explains what a proof is it seems clear that he has in mind a construction in the
sense of a process: to realize the intention expressed by a proposition α is to perform a
construction act, which may proceed in steps [. . . ] and which results in the construction
of a mathematical object, namely the construction intended by α (Prawitz, 2019b, 4; the
internal quote is from Sundholm, 1983, 164)
Therefore, The seeming interdependence between the concepts of valid inference
and proof—as indeed do other recent works (see, for example, Prawitz, 2012b,
2018)—looks with renewed interest at the BHK semantics; it suggests, more
speciﬁcally, that the notion of ground is partly inspired by BHK semantics or, more
weakly, that the two approaches look similar, and that they may be connected to
each other in some way—as indeed already suggested by Tranchini (2014). Prawitz
concentrates more directly on the possible circularity of an attempt to deﬁne valid
inferences and proofs, so as to capture their epistemic power:
a deﬁnition of proof in terms of valid inference requires that the latter concept be explained
in terms of evidence or related notions such as ground, justiﬁcation, or knowledge; since
a proof is understood in terms of such epistemic concepts, a valid inference must also be
related to these concepts if it is to serve in an explanation of what a proof is. [. . . ] However,

4.1
From Inferences to Proofs, via Grounds
99
if we ﬁrst explain the concept of proof saying that a proof is a chain of valid inferences, and
then explain the validity of an inference by referring among other things to proofs [. . . ] we
are moving in a plain circle. (Prawitz, 2019b, 2)
Besides the difﬁculties inherent in a deﬁnition of valid inferences in terms of proofs,
Prawitz seems to believe that such an approach is already misleading in itself, on
account of the inversion of the natural Cartesian idea of proofs-as-chains of valid
inferences:
there may be in principle two ways of breaking up the apparent interdependency between
the concepts of proof and valid inference, thereby making explanatory progress with respect
to the two concepts: either explaining the validity of inferences without referring to proofs
or explaining the concept of proof without referring to the validity of inferences. The second
alternative is in my view to put the natural conceptual order upside down. So the ﬁrst
alternative seems to me preferable. (Prawitz, 2019b, 2)
From this point of view, Heyting’s proposal would be of such importance as to
induce the Swedish logician to argue that
neither Heyting nor his intuitionistic successors explain the concept of proof that I am
discussing here but his notion of construction may be developed so that it gives us what
we are looking for, namely a concept of justiﬁcation or ground not based on the concept of
proof. (Prawitz, 2019b, 4)
Nonetheless, there is also a clear distance between Heyting and Prawitz. For
Heyting, both the notion of construction and that of proof as realization of the
intended construction are essentially non-inferential, and this clashes with the
objective Prawitz aims towards with his theory of grounds. Yet, this distance itself is
eventually shown to be a winning feature. In light of the perceived need for a notion
of deductively valid inference that, on the one hand, is ﬁxed in terms of evidence
and, on the other, allows the deﬁnition of the notion of proof, as well as by virtue
of the obvious circularity that would result from understanding evidence in terms
of proofs, Heyting’s constructions offer a suitable way of accessing the notion of
evidence:
his view does not lead to a general account of the concept of proof, but it offers an
explanation of what it is to justify the assertion of a proposition without using the concept
of proof. (Prawitz, 2019b, 5)
The constructions of which Heyting speaks, later encompassed by the BHK
clauses, have been understood in various ways.4 It goes without saying, however,
4 Some of these setups come conceptually close to Prawitz’s standpoint: the λ-calculus originally
developed by Church (1932); the Kreisel-Goodman theory of constructions (Kreisel, 1962, 1965;
Goodman, 1968, 1970, 1973; see Dean and Kurokawa (2016) for a simpliﬁed version functional to
a comparison with Prawitz’s semantics, as well as Gödel (1933) and Weinstein (1983), mentioned
by Dean and Kurokawa; see also Sundholm (1983) for a philosophical discussion of Kreisel’s
and Goodman’s aims to be compared with Prawitz’s ones); Martin-Löf’s intuitionistic type theory
(Martin-Löf, 1975a,b, 1984; see also Martin-Löf’s more philosophical papers, i.e., Martin-Löf
(1985, 1994, 1998), and Sundholm (1993, 1998, 2012) for a discussion of the consequences of
Martin-Löf’s standpoint).

100
4
Prawitz’s Theory of Grounds
that the idea that the possession of grounds is equivalent to the actual possession
of a construction does not necessarily imply that what one is in possession of is an
object of one of the theories that formalize the BHK clauses. The vast majority of
agents engaged in proof activities, including “professional” mathematicians, are not
acquainted with these theoretical setups. Nevertheless, a ground for a proposition or
sentence α is an object that results from the reiﬁcation of an act through which we
enter a state of evidence to judge the proposition α true, or to assert the sentence
α, an act that is also constitutive of the meaning of α. Proof-agents know the
meaning of the propositions and of the sentences used, and are aware of the acts
performed in deductive practice; as a consequence of that, they can make explicit
the grounds as we have understood them above, in a certain form and, possibly,
within a detailed theoretical framework. This is what Prawitz says about his way of
writing the operation underlying the acquisition of grounds through mathematical
induction—indicated with Ind:
an ordinary mathematician would of course not formulate a ground for the conclusion of
an induction inference explicitly in the form of Ind. But if she reﬂects on the inference,
she may very well think that she can form a ground for the conclusion from the grounds for
the premises, namely by taking the ground for the induction base and successively applying
the ground for the induction step as many times as needed. Ind simply represents such a
mental operation. (Prawitz, 2012a, 897)
Reﬂecting on what has been done, the agent can then realize that they have
accomplished certain operations, and possibly describe them.5
4.1.4
Inference Acts and Validity
Fulﬁllment of the “fundamental task” presupposes two steps: identiﬁcation of an
adequate relation between agents and inferences and, obviously, an appropriate
deﬁnition of the notion of valid inference. We have remarked how Prawitz, after
having detected the insufﬁciency of different options, maintains that the most
appropriate relationship between an agent A and an inference I is “A performs I”,
where it remains to be determined what is meant by “performing an inference”.
To perform an inference cannot simply mean announcing it, i.e., to establish
its premises, to say “therefore” or other equivalent expressions, and to state
5 We should probably anticipate that Ind is not, for Prawitz, a primitive operation; apparently,
this could contradict what we have argued about grounds as a reiﬁcation of acts constitutive of
the meaning. Indeed, a term constructed through Ind denotes a ground to the extent that it can be
transformed into, or rather reduced to, a term which begins with a primitive operation, and which
denotes a ground only by virtue of the meaning of the proposition or sentence considered. Thus,
grounds must be distinguished from the terms that describe them; they are denoted directly by a
certain class of terms, which those that, instead, indirectly denote must reduce to. The description
of a ground, which will be more speciﬁcally the description of the act performed to obtain it, will
therefore be subject to the distinction between direct and indirect denotation.

4.1
From Inferences to Proofs, via Grounds
101
their conclusion. This reductionist view, according to which inferences are mere
transitions from judgments or assertions to other judgments or other assertions,
exposes itself to a clear criticism even when the inferences in question are really
deductively valid:
an argument that happens to be valid but where, without further arguments, the conclusion
cannot be seen to follow from the premises is not considered to constitute a piece of correct
reasoning; it is rather viewed as a gap in reasoning until it has been supplemented. (Prawitz,
2012a, 890)
Inspired by the inferences in which operations such as discharge of assumptions or
binding of variables are performed, Prawitz then proposes to conceive inferential
acts as something that involves more than mere linguistic acts, or transitions:
to infer a conclusion α from a set  of premises may be experienced not just as making
the assertion α giving a set  of premises as one’s reason but rather as seeing that the
proposition asserted by α is true given that the propositions asserted by the premises of 
are true. (Prawitz, 2012a, 890)
Thus the idea is that of generalizing the completion of operations to all the
inferential acts, and this because
to characterize correct reasoning we may need to give substance to this metaphorical use of
seeing. (Prawitz, 2012a, 890)
Seeing is often regarded as something passive; but in a passage previously high-
lighted, Cozzo (2015) points out that to see that a certain conclusion follows from
certain premises is an active experience of necessity of thought. In making a valid
inference, the agent does something; through this act, they have the opportunity to
consciously experience the epistemic support that the premises guarantee for the
conclusion. Our current discussion allows us to specify more precisely what the
agent does, and under what conditions the inferential act thus understood can be
said to be valid.
Since Validity of inferences, and then gradually in all his articles on the theory of
grounds, Prawitz has proposed a new conception of inferential act, intended as an
application of one of the previously described operations on grounds. In particular,
in Explaining deductive inference he says that to perform an inference means
in addition to making an inferential transition, to apply an operation to the grounds that one
considers oneself to have for the premises with the intention to get thereby a ground for the
conclusion. (Prawitz, 2015, 94)
This general characterization is accompanied by further speciﬁcations, since, as
mentioned, an act of inference can be understood at different levels of abstraction:
I take an individual or generic inference to be individuated by what individuates an
individual or generic inferential transition from premises α1, . . . , αn to a conclusion β,
and, in addition, by alleged grounds g1, . . . , gn for the premises and an operation φ.
Conforming to the usual terminology according to which an inference may be unsuccessful,
no requirement is put on the alleged grounds and the operation; in other words, g1, . . . , gn
may be any kind of entities, and φ may be any kind of operation. [. . . ] Similarly, an inference

102
4
Prawitz’s Theory of Grounds
form is individuated by the form of an inference transition and an operation. (Prawitz, 2015,
94)6
Individual inferential acts had been initially described by Prawitz, as including
speciﬁc premises and conclusions, plus an agent and a space-time situation in
which the agent makes the inference. The generic inferential acts were instead an
abstraction from the agent and from the occasion, whereas inferential forms added a
further abstraction from speciﬁc premises and conclusions, taking into account only
parameters for the latter. It is therefore natural to consider a further level, of which
we ﬁnd trace in Validity of inferences:
ﬁnally, we may also abstract from the operation left in an inference form, and may then
speak about an inference ﬁgure or schema. (Prawitz, 2013, 198)
The understanding of inferential acts, at the different levels of abstraction, as
applications of operations on the grounds that we believe to have for the premises,
suggests at this point a natural deﬁnition of the notion of valid inference:
an individual or generic inference [. . . ] is (deductively) valid, if g1, . . . , gn are grounds for
α1, . . . , αn and φ is an operation such that φ(α1, . . . , αn) is a ground for the conclusion
β. [. . . ] [An inference form] is deﬁned as valid if the operation when applied to grounds
for the premises produces a ground for the conclusion. [. . . ] for each instance of the form,
the operation is of a speciﬁc type and is denoted by the corresponding instance of the term.
An inference schema is deﬁned as valid when there is an operation such that the inference
schema together with that operation is a valid inference form. (Prawitz, 2015, 94–95)
6 The fact that the grounds in an individual or generic inferential act are only alleged, and that there
is no restriction on such alleged grounds or on the operation involved in the act, is an important
change that Prawitz makes in Explaining deductive inference, compared to the framework proposed
in previous articles. It is essentially due to four objections raised by Cozzo to the original setting,
in which Prawitz required that an inferential act involved grounds for the premises—hence, actual
grounds—and left open the possibility that the operations involved in these acts were always
such as to produce grounds from grounds: (1) only valid inferential acts are inferences, while “it
seems reasonable to say that our activity includes acts of inference that can be valid or invalid”
(Cozzo, 2015, 113); (2) inferential acts for which the agent has no grounds for the premises
are not inferential acts, while “it seems reasonable to say that our deductive activity includes
inferences (valid or invalid) with mistaken premises” (Cozzo, 2015, 114); (3) the theory of grounds
would not be able to distinguish invalid inferential acts from valid inferential acts with unjustiﬁed
premises, while “it seems reasonable to say that an inference can be valid even if its premises
are mistaken” (Cozzo, 2015, 114); (4) Prawitz would not be able to account for the power of
epistemic compulsion experienced in valid inferential acts with unjustiﬁed premises, while “it
seems reasonable to say that the experience of necessity of thought also characterizes the transition
from mistaken premises” (Cozzo, 2015, 114). In light of the new deﬁnition, however, Prawitz
argues that “an individual or generic inference can err in two ways: the alleged grounds for the
premises may not be such grounds, or the operation may not produce a ground for the conclusion”
(Prawitz, 2015, 95). Usberti, however, argues that Cozzo’s fourth objection remains to be settled,
since “an assertion based on an entity of any kind may be true or false, but it is difﬁcult to see
how it can be rational at all” (Usberti, 2019b, 525). Cozzo also introduces the interesting notion
of ground-candidate, “a mathematical representation of the results of epistemic acts underlying
mistaken premises” (Cozzo, 2015, 114; for a discussion of Cozzo’s ground-candidates in Prawitz—
and in Girard’s Ludics—see Catta and d’Aragona, 2021).

4.1
From Inferences to Proofs, via Grounds
103
Analogously, given a ﬁgure
1 ⊢α1
. . .
n ⊢αn
 ⊢β
in order that the represented transition be valid, the operations it involves must be
an operation of type (1 ▷α1, . . . , n ▷αn) ▷( ▷β). [. . . ] the ground for the assertion
β under the assumptions , produced when the operation φ is applied to grounds for the
premises, can then be denoted by a ground term. (Prawitz, 2015, 94–95)
We have thus outlined the two remaining factors for the fulﬁllment of the
repeatedly mentioned “fundamental task”. Given an individual or generic inferential
act I, the relation between an agent A and I will be “A performs I”. By virtue of the
reconstruction that Prawitz offers of the notion of individual or generic inferential
act, I will involve the alleged grounds g1, . . . , gn for the premises, and an operation
φ applicable to g1, . . . , gn. Therefore, the fact that A performs I means for A to
apply φ to g1, . . . , gn, which can be indicated by φ(g1, . . . , gn). If we make the
further hypothesis that I is a valid individual or generic inferential act, according to
the deﬁnition of validity just given g1, . . . , gn are grounds for the premises of I, and
φ is an operation that, when applied to the grounds for the premises of I, produces a
ground for the conclusion of I. Hence, performing I, that is computing, as it were,
φ(g1, . . . , gn), A obtains the ground searched for.
The “fundamental task” is introduced by Prawitz in order to offer a clear concep-
tual grid in which to articulate in a rigorous and precise manner the underlying issue
that the theory of grounds intends to address and resolve: to explain how deductively
valid inferences are able to force epistemically acceptance of the conclusion, given
an epistemic acceptance of the premises. However, the task is formulated in terms
of (possession of) grounds, and thus presupposes that to possess grounds means
having an epistemic compulsion, to be in a state of justiﬁcation. Grounds are abstract
objects expressed in terms of primitive operations, essential to the determination
of meaning, and are obtained either by performing such operations, or performing
non-primitive operations deﬁned through equations by virtue of which grounds for
the elements in the codomain are generated from grounds for the elements in the
domain. We can now ask ourselves: is it fair to say that, in the terms of this notion of
(possession of) grounds, the “fundamental task” is such that its fulﬁllment actually
satisﬁes the basic question of the theory of grounds? If a ground is to be intended
as a description or reiﬁcation of a mental state of justiﬁcation, can we say that the
grounds so as described by Prawitz adequately respect this basic intuition? Does
having a ground, conceived by Prawitz as having applied one of the operations
described above, actually mean to be in possession of evidence for any judgment
or any assertion? In connection with these questions, it is important to point out
how, in Explaining deductive inference, the “fundamental task” is given by Prawitz
not in the terms of the notion of ground, but in the terms of the notion of evidence:
the task to show that a condition c on generic inferences is sufﬁcient for legitimacy can be
spelled out as the task of establishing for any generic inference I and subject A that from
the three facts (1) the inference I satisﬁes the condition c, (2) the subject A has evidence

104
4
Prawitz’s Theory of Grounds
for the premises of I, (3) A performs I, it can be derived that (4) A gets evidence for the
conclusion of I. (Prawitz, 2015, 74)
We will return to this question, but for the moment we can observe, with Prawitz,
that:
an inference that is valid is so in virtue of the meaning of the sentences involved in the
inference. (Prawitz, 2015, 95)
The fact that the inferences of the theory of grounds are valid by virtue of their
meaning is immediate, for example, in the case of Gentzen’s introductions; in the
case of other inferences, such as Gentzen’s eliminations, the guarantee will come
from the fact that the equations that deﬁne the non-primitive operations applied,
produce grounds from grounds. The limitation to grounds and operations for ﬁrst-
order logical constants is not necessary, but it becomes essential when we intend to
pass from a notion of deductive validity to a notion of logical validity:
the notion of validity deﬁned above may be called deductive validity to differentiate it from
[. . . ] a narrower notion of logical validity, which is now easily deﬁned by using the same
strategy as used by Bolzano and Tarski; inferences on various level of abstractions are
logically valid if they are deductively valid and remain deductively valid for all variations
of the meaning of the non-logical vocabulary. (Prawitz, 2015, 95)
4.1.5
Advancements and Open Issues
So far, we have reconstructed the theory of grounds as it appears in Prawitz’s
articles up to Explaining deductive inference—with the integration of The seeming
interdependence of the concepts of valid inference and proof. We can now return to
the problems that Prawitz’s semantics of valid arguments and proofs suffered from,
to check whether the theory of grounds can make any headway against them.
4.1.5.1
Proofs-as-Chains Reconsidered
If proofs are intended as chains of valid inferences, to explain the power of epistemic
compulsion of valid inferences also means explaining the power of epistemic
compulsion of proofs. Nonetheless, Prawitz warns against the possible circularity
such an explanation is at risk of becoming bogged down in, essentially due to
the interdependence between the notions of valid inference and proof. Prawitz’s
semantics of valid arguments and proofs, for example, suffered exactly from this
defect; in it, the notion of valid inference was deﬁned in the terms of the notions
of valid argument and proof, so that it proved impossible to appeal to the idea
that valid arguments and proofs were chains of valid inferences. In the theory of
grounds, however, the notion of valid inference is deﬁned in terms of the notion of
ground, and the notion of ground does not in turn presuppose the notion of proof. It

4.1
From Inferences to Proofs, via Grounds
105
is therefore signiﬁcant that Explaining deductive inference ends with a deﬁnition of
the notion of proof:
an (individual or generic) proof may be now deﬁned as a chain of valid (individual or
generic) inferences. [. . . ] a proof of an assertion does not constitute a ground for the
assertion, but produces such a ground. (Prawitz, 2015, 93)
The theory of grounds therefore offers two substantial advancements. The ﬁrst, that
stems immediately from what we have just said, is that it provides a notion of valid
inference independent of the notions of valid argument and proof, in such a way
that, on the contrary, the former serves as a basis for the latter. The second, instead,
emerges as soon as we turn to the ﬁnal part of the last quote. Here, Prawitz speciﬁes
that a proof is not a ground, but produces a ground; it is not what the justiﬁcation of a
judgment or an assertion is based on, but through what the evidence for a judgment
or an assertion is obtained. In our previous terminology, the proof-objects of the
theory of the grounds are the grounds themselves; proofs, on the other hand, are not
proof-objects, but proof-acts.
If the power of epistemic compulsion of valid arguments and proofs must be
explained by saying that they are made only of valid inferences, and if the latter are
explained in terms of valid arguments and proofs, we already have an impasse with,
for example, a valid closed canonical argument
1
α
2
β (∧I)
α ∧β
or a canonical proof O∧(π1, π2); it would obviously be circular to say that this latter
inference is epistemically compelling since, as valid, it produces the same valid
closed canonical argument or the same canonical proof whose ability to compel
epistemically we wish to explain. We could of course argue that this is a badly
posed example, and that the impasse is only apparent; the valid closed canonical
argument or the canonical proof are epistemically compelling when understood
as acts, and this because the last inference is precisely an act that produces the
canonical closed valid argument or the canonical proof which, understood this time
as objects, constitute evidence for the conclusion. But this requires that the theory
clearly distinguish between proof-objects and proof-acts.
In Prawitz’s previous approach valid arguments and proofs actually play a
double-role: objects that, as formalizations of the notion of evidence, serve to
determine the meaning, and acts that, being aimed at achieving evidence, are
required to be made of only valid inferences. We have said that the inability of the
semantics of valid arguments and proofs to offer an adequate conception of proofs as
chains was ultimately attributable precisely to this twofold status. Moreover, even
if the notion of valid inference is not based on those of valid argument or proof,
but still on a notion that singles out structures then required to be chains of valid
inferences, the distinction between inferences in introductory form and inferences in
non-introductory form could induce circular explanations; we cannot, for example,
characterize as valid an implication introduction by requiring that its immediate

106
4
Prawitz’s Theory of Grounds
subchain be a chain of valid inferences since, on account of the distinction between
canonical and non-canonical cases, we cannot exclude that the immediate subchain
contains inferences of the same type as that we are deﬁning as valid (see again
Gentzen, 1934–1935; Negri & von Plato, 2015; Usberti, 2015).
In this regard, and following an interpretative line outlined by Usberti (2015), we
have already suggested a way out inspired by the BHK clauses. The fact that these
clauses proceed by induction allows for abstraction from the intrinsic complexity
of the functions involved in the problematic clauses for →and ∀and, therefore,
from the distinction between inferences in introductory form and inferences in non-
introductory form, between canonical and non-canonical cases. This shows that the
proofs-as-chains problem can be addressed with the following strategy: the notion
of valid inference is deﬁned relative to canonical objects only, ﬁxed by simple
induction and by abstracting from the distinction between introductory/canonical
and non-introductory/non-canonical cases, a distinction which can then be applied
exclusively to the acts in which inferences occur, characterized instead as chains of
valid inferences.
Valid inferences are deﬁned in terms of a notion of ground which, exactly as for
the BHK proofs, proceeds by simple induction on the logical form. In the case of
the constants →and ∀, a notion of constructive procedure is assumed as primitive,
so as to disregard the complexity inherent in the procedures themselves. The clauses
that establish what counts as ground use only meaning-constitutive operations, and
the resulting grounds are, as it were, always canonical. As Usberti says,
we can see the reasons why Prawitz makes now reference to two different notions, assigning
them different roles: to the notion of ground the role of the key notion of the theory of
meaning, to the notion of deduction the role of (linguistic presentation of) proof [. . . ]. The
crucial difference between the two notions is that grounds for α are deﬁned by recursion on
the complexity of α, while a deduction of α is deﬁned by induction on the number of its
steps. So, in the case of grounds for α →β, it is legitimate to abstract from the intrinsic
complexity of the function that transforms each ground for α into a ground for β. (Usberti,
2015, 418)
If the objects of the theory of grounds are always canonical, the acts in which (valid)
inferences occur are not; valid arguments and proofs, in fact, may involve inferences
of any kind, introductory or not. The terms of the languages of grounds that, in
Explaining deductive inference, Prawitz suggests to develop, must denote grounds;
but, as the Swedish logician claims in The seeming interdependence between the
concepts of valid inference and proof, they can be understood also as protocols—
we could say encodings—of the process by which the denoted ground can be
obtained. Not surprisingly, therefore, Prawitz distinguishes between canonical and
non-canonical terms, saying that
a closed ground term whose ﬁrst symbol is one of the primitive operations is said to be
in canonical form - the form used to specify the grounds there are for different assertions
(Prawitz, 2015, 93)

4.1
From Inferences to Proofs, via Grounds
107
—it goes without saying that if a closed term has as its outermost symbol a non-
primitive operation, it is to be considered in non-canonical form. However, this
distinction has no parallel at the level of the objects of the theory.
The relation between grounds and proofs in Prawitz’s theory of grounds is
analogous to that between constructions/proofs and realizations of such construc-
tions/proofs in Heyting’s approach. However, it also retraces the distinction that
occurs in Martin-Löf’s intuitionistic type theory between proofs for propositions
and proofs for judgments. The notion of ground for judgments or assertions, used to
explain meaning of propositions or sentences, corresponds to Martin-Löf’s notion
of proof for proposition:
the grounds will thereby be among the objects that one comes across within intuitionistic
type theory developed by Martin-Löf. (Prawitz, 2015, 89)
An important difference, however, is that according to Prawitz, a proof-object
cannot be considered as an abstract truth-maker, as in Martin-Löf (1998) and
Sundholm (1998); on the contrary, it must have an epistemic import, which is
relevant for the justiﬁcation of judgments or assertions (on this point see also
Moriconi, 2023). The proofs of the theory of grounds would correspond instead to
the proofs of judgments in Martin-Löf—althoughfor Prawitz the form of a judgment
is not to be understood as “a is a proof-object for α” or, if we pass from the analytic
to the synthetic form, as “α is true”, since expressions of this type should be placed
on a meta-level.
Nonetheless, among Prawitz’s, Heyting’s and Martin-Löf’s frameworks, there
are also profound differences. The most immediate one concerns the distinction
between canonical and non-canonical cases, which Prawitz attributes not to the
objects of the theory of grounds, but to the acts through which these objects are
produced, and to the terms that codify such acts—proofs, terms for grounds. In
Heyting’s writings there seems to be no trace of this theme, while it is clearly
present in Martin-Löf, who oscillates from an exclusive attribution of the distinction
to proofs of propositions, namely to objects—and not therefore to proofs for
judgments, which are acts (Martin-Löf, 1984)—to an introduction of the distinction
also at the level of proofs for judgments, with a simultaneous explanation of this
distinction in terms of the analogous distinction for proofs of propositions (Martin-
Löf, 1985; Prawitz, 2012b).
But the most original trait that the theory of grounds presents with respect to
similar approaches is arguably the explicit and programmatical introduction of
a deep connection between objects and acts, between grounds on the one hand
and terms/proofs on the other (a similar point is raised, although in a different
perspective, by Moriconi (2023)). A proof, i.e., a process that allows us to be
in possession of a ground, can be described by a term of an appropriate formal
language, which in turn will denote the ground obtained; the term, which encodes in
the formal language the corresponding proof, is a name of the denoted ground, i.e. of
the ground which could be obtained if the proof were carried out. The name provides
a set of computation instructions; computing the term means performing the proof,
thereby obtaining the ground to which it leads. A very strong bond is therefore

108
4
Prawitz’s Theory of Grounds
established between the objectual aspect and the operational aspect of the theory,
a bond that is more implicit in the respective proposals of Heyting and Martin-
Löf. In Heyting, the individual steps performed in the realization process of the
proof/construction show what the realized construction is, in correspondence with
the operations of which it is composed (Heyting, 1931, 1934; Prawitz, 2012b); in
Martin-Löf, likewise, the proof of a judgment consists of passages that correspond to
the operations involved in the proof of the proposition and in the logical constants of
the proposition itself—the latter intended as a type—so as to allow a type-checking
that makes judgments of the type “a ∈α” decidable (Martin-Löf, 1984, 1994;
Prawitz, 2012b; Sundholm, 1983, 1998, 2011). However, it lacks any idea that a
proof-act can be described by a term denoting the proof-object it names, and which
can be computed in order to actually obtain such a proof-object.
This idea is, on the contrary, introduced by Prawitz in an explicit and pro-
grammatical way; explicit because it is openly suggested as an essential feature
of the development, both philosophical and formal, of the theory of grounds, and
programmatical because, most notably in his latest writings, Prawitz seems to
conceive the theory of grounds as including a language of terms for which we
can deﬁne a rigorous and precise notion of denotation with respect to a “universe”
of grounds and operations on grounds. This has the immediate consequence of
substantiating the criticism that Prawitz makes to Martin-Löf’s and Sundholm’s idea
that proof-objects would be lacking epistemic import (Martin-Löf, 1998; Sundholm,
1998):
even if proofs are primarily acts, there are objects that relate in various ways to these acts
and are also called proofs or, at least, are considered to have epistemic import. Furthermore,
proof acts can be noted down, and what we then have are linguistic objects that can be seen
as records of the acts. [. . . ] Such a record of an act [. . . ] can be seen as an instruction for how
to perform a proof act. It has clearly epistemic import, and we may call it a representation
of a proof (act) when there is need to distinguish it form the act itself. (Prawitz, 2012b,
63–64)
4.1.5.2
Recognizability Reconsidered
The solution that the theory of grounds offers of the “fundamental task” presupposes
a question of adequacy of the notion of ground with respect to a pre-formal notion
of evidence—and hence, the notion of possession of grounds with respect to a pre-
formal notion of possession of evidence. In the concluding remarks of Explaining
deductive inference, the question is explicitly raised by Prawitz himself:
does a ground as now deﬁned really amount to evidence? When the assertion is a categorical
one, the ground is a truth-maker of the asserted sentence; since the meanings of the
sentences are given by laying down what counts as grounds for asserting them, the truth
of a sentence does not amount to anything more than the existence of such a ground.
Nevertheless one may have doubts about whether to be in possession of a truth-maker of
a sentence as understood here really amounts to being justiﬁed in asserting the sentence.
(Prawitz, 2015, 96)

4.1
From Inferences to Proofs, via Grounds
109
It is important to note that the Swedish logician does not refer here to a notion
of (possession of) ground understood as, so to speak, a synonym of (possession
of) evidence; the questions concern rather the grounds “as now deﬁned” and “as
understood here”. What one wonders is whether the grounds as described in the
theory of grounds appropriately capture the pre-formal notion of ground which the
theory itself should comply with.
What could be problematic here? The answer is: the fact that inferential acts
are intended as applications of primitive or non-primitive operations, hence as
corresponding to terms which, despite always denoting canonical objects, can be
in canonical or non-canonical form. More speciﬁcally, Prawitz argues that
when a subject performs a valid inference and applies an operation φ to what she holds to
be grounds for the premises, she forms a term T that in fact denotes a ground for the drawn
conclusion α, but it is not guaranteed in general that she knows that T denotes a ground for
α. (Prawitz, 2015, 96)
In the semantics of valid arguments and proofs, there was a problem of recog-
nizability related to valid closed non-canonical arguments, and non-canonical or
categorical proofs: to have a valid closed non-canonicalargument or a non-canonical
or categorical proof means knowing how to get a valid closed canonical argument
or a canonical proof for the same conclusion, but not also knowing that the non-
canonical argument or the non-canonical or categorical proof are indeed such as to
produce the expected result. A similar difﬁculty arises in connection with valid open
arguments, and hypothetical or general proofs, for which inﬁnite substitutions from
non-regimentabledomains are required. In the theory of grounds we have something
similar; the difference is that, as the non-canonical cases employ non-primitive
operations ﬁxed by equations of a certain type, the problem can be reformulated
by asking whether it is recognizable that the equations offer a good deﬁnition with
respect to the operational type, so that the operation actually produces grounds of
the type indicated in the codomain starting from grounds of the type indicated in the
domain. In the words of Prawitz,
since the meanings of closed atomic sentences are given by what counts as grounds for
asserting them, [the agent] should thus know that T denotes a ground for asserting an atomic
sentence α when this is how the meaning of α is given. Such knowledge is preserved by
introduction inferences, given again that the meanings of the involved sentences are known:
the term T obtained by an introduction is in normal form, that is, it has the form φ(U) of
φ(U, V ), where φ is a primitive operation and the term U or the terms U and V denote
grounds for the premises - knowing that these terms do so, the agent also knows that T
denotes a ground for the conclusion [. . . ]. However, when φ is a deﬁned operation, the
subject needs to reason from how φ is deﬁned in order to see that T denotes a ground for
the conclusion. If T is a closed term, she can in fact carry out the operations that T is built
up of and bring T to normal form in this way, but she may not know this fact. Furthermore,
when T is an open term, it denotes a ground for an open assertion or an assertion under
assumption, and it is ﬁrst after appropriate substitutions for the free variables that one of the
previous two cases arises. (Prawitz, 2015, 97)
Despite having made a valid inference, therefore, the agent may not recognize it.
And in light of the way in which the notion of valid inferential act is reconstructed by
Prawitz, the agent might therefore not recognize that they made use of an operation

110
4
Prawitz’s Theory of Grounds
that produces grounds for the conclusion when applied to grounds for the premises;
if they actually have grounds for the premises, the agent could thus not recognize
that they have a ground for the conclusion. Compared to the “fundamental task” that
the theory of grounds aimed to accomplish, this could be problematic because
one may ask if to make a valid inference really gives the evidence that one should expect.
(Prawitz, 2015, 97)
Before turning to the way in which Prawitz deals with the problem of recog-
nizability in the framework of the theory of grounds, and to the answers that he
contextually puts forward, it is perhaps appropriate to explain in more detail why
this problem exists. A ﬁrst suggestion obviously comes from what has already been
said with reference to valid arguments and proofs: the computation of a closed term
to its canonical form could be de facto impossible for an agent with limited resources
of time and memory and, in the case of an open term, the process of substitution
of individual variables and ground-variables with, respectively, individuals and
grounds from non-regimentable expansions of a given language of grounds could
turn out to be impossible even if no limitation is put on the agent’s time and
memory. In Explaining deductive inference Prawitz puts forward another argument,
in the light of which recognition of a term as denoting a ground would barely be
plausible—an argument already anticipated in part for proofs (Prawitz, 1977). This
is an observation which refers back to Gödel’s incompleteness theorems—which
have already been mentioned to endorse the character of openness of a language of
grounds.
As one usually does in the framework of the Curry-Howard isomorphism—of
which we shall speak in more detail below—a closed language of grounds could be
understood as a “translation” of a formal system and, in this regard, Prawitz notes
that
it is of course an essential feature of a formal system that it is decidable whether something
is a proof in that system. For a closed language of grounds where the term formation is
restricted by specifying what operations may be used, it may similarly be decidable whether
an expression in the language denotes a ground. (Prawitz, 2015, 98)
However, as we have said, the phenomenon of incompleteness implies that the
languages of grounds must enjoy a character of openness to the introduction of
new operations. Already at the level of ﬁrst-order arithmetic, a closed language
of grounding  would be extensionally inadequate, since it cannot express all
the possible grounds for judgments or assertions that involve the formulas of the
background language. It is therefore necessary to take into account a (inﬁnite)
class of expansions + of  and, although the terms of  can be recognizable as
denoting grounds, we do not have any guarantee that this assumption of recognition
is preserved in all the +.
It is worth noting that while we are talking in terms of recognizability, in Explain-
ing deductive inference Prawitz expresses himself ﬁrst in terms of luminosity of
having evidence for a certain judgment or for a certain assertion and, only later, in
terms of decidability:

4.1
From Inferences to Proofs, via Grounds
111
it may be argued that the condition for having evidence for an assertion is luminous [. . . ].
The crucial question is therefore if it is decidable for an arbitrary deﬁnition of an operation,
which we may contemplate to add to a given closed language of grounds, whether it always
produces a ground of a particular type when applied to grounds in its domain. This is what
must hold if we are to say that the property of being a ground is decidable, and it seems
to me that we must be skeptical of such an idea, and therefore also of the idea that the
condition for something to be a proof or to constitute evidence is luminous. (Prawitz, 2015,
97–98)
Referring to a concept of contemporary mathematical logic, the notion of decidabil-
ity is certainly more precise, and therefore more stringent than those of luminosity
or recognizability. The question is thus in what sense does Prawitz believe that
undecidability implies non-luminosity, and why does he pass from a discourse
of recognizability (as in Prawitz, 1973 or Prawitz, 1977) to one of decidability?
That the issue under consideration is to be formulated in the strict terms of the
notion of decidability is a thesis put forward by some authors (see, for example,
Pagin, 1998); nevertheless, one could ask whether there can be a plausible way
of understanding the terms “luminous” or “recognizable” such that, although not
strictly decidable, the fact that an arbitrary term denotes a ground can indeed
be luminous or recognizable. This gap between decidability and luminosity or
recognizability is addressed, for example, by the following criticism from Usberti:
suppose that the correct answer to the crucial question is negative, namely that there is an
operation O on grounds represented in the formal theory of grounds T by a term K such
that if t1, . . . , tn are terms denoting grounds for α1, . . . , αn respectively, then K(t1, . . . , tn)
denotes a ground for β, but neither the sentence of T translating “K(t1, . . . , tn) is a ground
for β”, nor the sentence translating “K(t1, . . . , tn) is not a ground for β”, is a theorem of
T . There is no reason to conclude that it is not intuitively evident that K(t1, . . . , tn) is a
ground for β, and therefore that intuitive evidence is not epistemically transparent. [. . . ]
Concededly, formal provability is intended to catch intuitive evidence, but sometimes it
does not succeed, as just Gödel’s theorem shows; when this happens, we don’t infer that
intuitive evidence is different from what it appears to be (for instance, that Gödel’s sentence
is not intuitively true/evident), but that formal provability is incomplete. (Usberti, 2019b, 4)
As can be seen, Usberti refers to notions of epistemic transparency and of
intuitive evidence that somehow resemble, respectively, the notions of luminosity
or recognizability which we are dealing with, on the one hand, and the notion of
evidence in the pre-formal sense we have used so far, on the other.7
7 As mentioned several times, Gödel’s theorems are used by Prawitz in support of the openness of
the languages of grounds, and of the impossibility of ensuring that the recognizability/decidability
of the fact that a term denotes a ground, which may also be plausibly assumed for the terms of a
closed language, be preserved in expansions of the given language. However, incompleteness is
also key for replying to a possible objection to Prawitz’s argument: recognizability/decidability is
ensured by the fact that, for every closed languages that expand the given language of grounds, for
each of its terms, we can recognize/decide that such term denotes a ground. For this objection
to achieve its objective, it is necessary to assume that the expansions of a given language of
grounds are effectively generable: the algorithm suggested could, in this case, use a sort of diagonal
procedure to cover the terms of all the possible expansions, establishing at each step that the term

112
4
Prawitz’s Theory of Grounds
For the recognizability problem related to the semantics of valid arguments and
proofs, we saw that a possible solution, similar to that adopted by Kreisel (1962) for
the BHK clauses, consisted in requiring that a valid argument or proof for α be not
only a valid argument  or proof π according to Prawitz’s deﬁnitions, but also an
additional valid argument ∗or proof π∗for “ is a valid argument for α” or “π is
a proof for α”. Similarly, in the case of the recognizability problem of the theory of
grounds, the idea would be to require that a valid inference be not only describable
by a term T that actually denotes a ground for ⊢α, but also by a further term T ∗
denoting a ground for “T denotes a ground for ⊢α”. However, Prawitz does not
consider this way viable, and in particular argues that to perform an inference
is not to assert that the inference is valid. Nor is it to make an assertion about the grounds
that one has found for the conclusion of the inference. One may of course reﬂect over the
inference that one has made, and, if successful, one may be able to demonstrate that a ground
for the conclusion has been obtained and that the inference is valid. But a requirement to
the effect that one must have performed such a successful reﬂection in order that one’s
conclusion is to be held to be justiﬁed would be vulnerable to [. . . ] vicious regresses.
(Prawitz, 2015, 97)
Prawitz therefore reafﬁrms his reconstruction of the inferential act: when we
infer, what we do is make judgments/assertions-premises for which we believe
to be in possession of grounds, and then pass to a judgment/assertion-conclusion
applying an operation that, when the grounds we thought we had for the
judgments/assertions-premises are actually such, and when the inferential act is
valid, actually produces a ground for the judgment/assertion-conclusion. This
activity, however complex, does not involve and is not expected to involve a further
judgment or assertion to establish that the performed passage actually produces
the expected result. If we adopted this point of view, in fact, a valid inference
should be able to produce not one, but two grounds: one for the judgment/assertion-
conclusion, and the other for the judgment or assertion “the inference is valid”—i.e.
“the term which this inference corresponds to denotes a ground for the conclusion”.
Indeed, this second judgment-assertion is not, according to Prawitz, equivalent to
the judgment/assertion-conclusion of the inference. And this could in turn, quite
clearly, generate regressive explanations, since the second judgment or assertion
involves a proposition or sentence that
is on a meta-level as compared to the former one [. . . ]. To be justiﬁed in asserting it, it is
of course not sufﬁcient only to produce a truth-maker of α. One must also have a ground
for the assertion that what is produced is a truth-maker of α, which has to be delivered by a
proof on the meta-level of an assertion of the form “. . . . is a ground for asserting . . . ”. This
proof will in turn depend on its inferences giving evidence for their conclusions. To avoid an
inﬁnite regress it seems again to be essential that there are inferences that give evidence for
considered denotes a ground. But if the starting language of grounds is a language of grounds
for ﬁrst-order arithmetic, the effective generability of all its expansions would be equivalent to the
recursive enumerability of all the “truths” of ﬁrst-order arithmetic—it is sufﬁcient to enumerate the
types of the terms in each of these languages; this, by virtue of Gödel’s incompleteness theorems,
is impossible. An argument similar to that just proposed is also found in Usberti (1995, 2015).

4.1
From Inferences to Proofs, via Grounds
113
their conclusions without it necessarily being known that they give such evidence. (Prawitz,
2015, 97)
Prawitz’s reasoning is obviously based on a particular conception of judgments or
assertions; to judge that α is true does not also mean to judge that it is true that α
is true and, analogously, to assert α does not also mean to assert that α is true. With
speciﬁc reference to assertions, the centrality of this point is most clearly stated in
The seeming interdependence of the concepts of valid inference and proof:
to assert that α is true is from a constructive point of view to assert that there is a construction
for α, which of course requires that one can specify a construction a for α. To assert a
proposition α, I take to be the same as uttering in assertive mood a sentence that expresses
α. It certainly requires the speaker to have a construction for α since, as always, she needs
a ground for the assertion. But it does not require, one may argue, a proof that so and so is a
construction for α, because that there is a construction of α is not what she asserts; it is not
a part of the content of the proposition α. (Prawitz, 2019b, 9–10)
Although the theory of grounds still suffers from a recognizability problem, this
now seems to have less urgency. We can appeal to some of Prawitz’s statements,
according to which it could already be satisfying, to fulﬁll the “fundamental task”
in the manner outlined above, making it
a conceptual truth that a person who performs a valid inference is aware of making an
operation that produces what she takes to be a ground for the conclusion. (Prawitz, 2015,
98)
The recognizability problem does not concern grounds as such, as abstract objects
liable to some formalization; the problem concerns grounds as a clariﬁcation of the
notion of evidence, and therefore, and above all, our being in possession of grounds
as being in possession of evidence. To have a ground means having constructed a
term that denotes that ground, or better, to have performed acts that can be described
as terms that denote the ground to which these acts lead. Two key-points come into
play here: ﬁrst, the idea that an inferential act is the application of an operation on
the grounds that we think we have for the premises in order to obtain a ground
for the conclusion; and, second, the clear distinction between always canonical
objects, which the theory treats as evidence, and acts that allow us to come into
possession of these objects. What an agent possesses after having made a valid
inference is a ground, an always canonical object of evidence, not a term; the term,
which certainly denotes the ground obtained, has only the role of describing the
act performed, and can be canonical or not depending on whether the last inference
made by the agent is in an introductory form. When the inferential act is valid, the
agent has grounds for the premises and, by applying to these grounds the operation
associated with the inference, they get—as result of a computation—a ground for
the conclusion; this activity can be codiﬁed by a term, which hypostatizes the agent’s
activity—and which provides the instructions for the computation. Therefore, a
valid inference objectively gives the agent possession of something that the theory
treats as evidence.
Such a solution does not seem possible in the case of the notion of valid
argument. Above, we have suggested that this impossibility comes from the fact

114
4
Prawitz’s Theory of Grounds
that the justiﬁcation procedures associated with valid arguments go from argument
structures to argument structures, and not from valid arguments to valid arguments.
We have also suggested that, when considering the notion of proof, a solution
analogous to that adopted in the theory of grounds seemed possible, but only
provided that the fulﬁllment of an inference is understood as the computation of
a procedure, and the canonical objects of evidence are always distinguished from
the acts that produce such objects—and this because the actual procedures involved
in the proofs go from proofs to proofs. It is easy to observe that also the operations
on grounds, denoted by the operational symbols in terms of languages of grounds,
are to be understood as operations from grounds to grounds.
4.1.5.3
Independent Validity Reconsidered
In the semantics of valid arguments and proofs, the validity of a structure could
depend, not on the (parameters standing for the) inferences involved, but on how
they are justiﬁed. Prawitz’s solution in this case was a notion of analytical validity,
based on a relation of containment between structures.
In the theory of grounds, we seem to have the same situation. An inference
might be valid, not because of its premises and its conclusion, or of their grounds,
but simply because of how the corresponding operation is deﬁned. Unless we put
restrictions on such operations—apart from their being total and constructive—the
results they produce may have little to do with the arguments to which they are
applied. Also in this case, however, we can appeal to a notion of validity based on
some kind of containment. We may limit ourselves to a ﬁxed stock of primitive
extraction operations f1, . . . , fn, and require that the deﬁning equations of our
theory be always given in terms of combinations of f1, . . . , fn.
However, thanks to Prawitz’s reconstruction of an inference as the application
of an operation, there are reasons to maintain that the problem is now less urgent.
Although the operation associated to the inference may produce values that have
nothing to do with the arguments to which the operation is applied, it is the
performing of the inference that provides us with these values. To illustrate this
point, consider the following example. Let c2 be a computation of 2 + 2 = 4, and
let f be an operation of type 1 + 1 = 2 ⊢2 + 2 = 4 acting as a kind of “pointer”
over c2, i.e., for every ground g for 1 + 1 = 2,
f (g) = c2.
Finally, let c1 be a computation of 1 + 1 = 2. Suppose that an agent has carried
out c1, and that they then apply to this computation an inference J from premise
1 + 1 = 2 to conclusion 2 + 2 = 4, i.e.
c1
1 + 1 = 2 I
2 + 2 = 4

4.1
From Inferences to Proofs, via Grounds
115
As soon as I is associated to f , it becomes a valid inference, since
f (c1) = c2.
Clearly, this does not depend on c1, or on c2 being somehow contained in c1, but
merely on the fact that f takes c1, and then “points” to c2. Nonetheless, to the extent
that to perform I means for the agent to apply f to c1, it is the performing of J as
such, without further reductions, that provides the ground for the conclusion. It is
as if to perform I meant exactly to ﬁnd a computation that justiﬁes the conclusion,
and to use it to justify the transition.
4.1.5.4
The Empty-Function Problem
We have seen that, when trying to explain the compulsion experienced in correct
deduction, Prawitz’s semantics of valid arguments and proofs suffers from at least
three problems. With respect to these problems, the theory of grounds allows
for some advancements. Therefore, if these ﬂaws were the only obstacles to a
satisfactory description of the epistemic force of valid inferences and proofs, we
would be entitled to claim that the theory of grounds permits some advancements
with respect to its own fundamental task too. Unfortunately, this is not the case.
Prawitz has recently described a further problem, in light of which we must
conclude that any improvement provided by the theory of grounds is only relative to
the three problems discussed so far, and not in the absolute sense of accounting for
epistemic compulsion. As this new problem holds equally well both in the semantics
of valid arguments and proofs and in the theory of grounds, we must, in other words,
admit that the two approaches are equally unable to explain how and why correct
deduction exerts its power.
In two recent—and still unpublished—papers, Validity of inferences (Prawitz,
2022b) and Validity of inferences reconsidered (Prawitz, 2022c), this fatal ﬂaw
is directed mainly at the valid arguments setup. Inspired by Aristotle’s (1949)
distinction between perfect and imperfect syllogism, Prawitz distinguishes between
immediately and mediately valid inferences. In a mediately valid inference, the
conclusion follows from the premises. An immediately valid inference is much more
epistemic in nature, in that its conclusion is in addition evident, given evidence for its
premises. In other words, immediately valid inferences are those endowed with the
power of compulsion, since in them the justiﬁcation for the premises is a sufﬁcient
condition for the conclusion to be justiﬁed.
The problem affecting the deﬁnition of the notion of valid inference in terms
of valid arguments is that what one obtains with it is a notion of mediately valid
inference, rather than immediately valid. Of course, this may be deduced already
from the recognizability problem. However, even if everything were recognizable,
the deﬁnition itself would still be unable to account for epistemic compulsion. To

116
4
Prawitz’s Theory of Grounds
illustrate this point, let us provide a simpliﬁed example. Given α and β closed (on a
base B), we say that the inference
α I
β
is valid (on B) if, and only if, there is a reduction procedure J that, when associated
to I, is such that, for every consistent J+, for every closed valid argument (, J+)
for α, (′, J+) is a valid argument (on B) for β, where ′ is the argument skeleton

α I
β
If there is no closed valid argument (on B) for α, I is vacuously valid (on B). This
can be also expressed by saying that the reduction procedure J to be associated to I
for I to be valid (on B) can be simply the empty function. Consider now the sentence
(on a base B for arithmetic)
∃n x y z (n > 2 ∧xn + yn = zn)
for which, after Wiles’ proof of Fermat’s last theorem, we know we have no proof
(on B). The inference
∃n x y z (n > 2 ∧xn + yn = zn)
⊥
is hence valid (on B) according to the semantics of valid arguments and proofs, but
of course it is not an immediately valid inference: the assumption that the premise
is justiﬁed is clearly insufﬁcient for the conclusion to become evident to us. One
should know that the premise has no proof, but that would amount to knowing a
proof of Fermat’s theorem; and this proof cannot be the following argument, trivially
valid (on B), but patently devoid of epistemic traits:
1
[∃n x y z (n > 2 ∧xn + yn = zn)]
⊥
(→I), 1
¬∃n x y z (n > 2 ∧xn + yn = zn)
This shows that the ﬁrst inference involved in this argument is only mediately valid,
as well as what the problem is with inferences that are only mediately valid.
It is easy to realize that the same situation is lurking out in the theory of grounds.
An operation on grounds of the type
∃n x y z (n > 2 ∧xn + yn = zn) ⊢⊥
trivially exists (on a base B for arithmetic): it is the empty function f∅. Thus, when
our inference above is associated to f∅, it is trivially valid (on B) for the theory of
grounds, but clearly not immediately valid for us. We may again require a proof of

4.2
Towards a Formal Approach to Grounds
117
the fact that the domain of f∅is empty, which would amount to having a ground
for Fermat’s last theorem; this is a ground that we cannot expect to have the simple
form
→Iξ∃n x y z (n>2∧xn+yn=zn)(f∅(ξ∃n x y z (n>2∧xn+yn=zn)))
and which thus forces us to conclude that the inference is in the end only mediately
valid, and therefore epistemically void.
Of course, the empty functions we have been discussing so far can be taken to
objectively—even if trivially—transform proofs in their domain into proofs in their
co-domain. But they lack a sufﬁcient epistemic import. This is perfectly expressed
by Prawitz when, in observing that the problem holds also for the BHK clause for
implication (as well as for Martin-Löf, 1985) notion of hypothetical proof), he says
that
there is no objection [. . . ] if by proof is here meant the same as was later called proof-object
in Martin-Löf’s type-theory. The empty construction is a construction of Fermat’s theorem,
and to prove that it is, one has to prove Fermat’s theorem. But by a proof Heyting clearly
means proof in the usual epistemic sense, and then of course this empty construction is
again not what we mean by a proof of Fermat’s theorem. (Prawitz, 2022c, 7)
Given the similarities between BHK proof-objects and Prawitz’s grounds, we should
hence not be surprised if the theory of grounds suffers from the empty-function
problem. However, we have also seen that the idea of explaining validity of
inferences in terms of proof-objects is a crucial step for the solution of the proofs-
as-chains problem. It therefore seems we cannot have both things, and it is probably
for this reason that, in the two papers mentioned at the beginning of this section,
Prawitz tries to develop a “mixed” approach:
it is not certain that the concept of valid inference can be explained without the use of the
concept of proof; the two notions may have to be explained simultaneously by a number of
principles that relate them to each other. (Prawitz, 2022c, 8)
4.2
Towards a Formal Approach to Grounds
Starting from the next section, we shall be concerned with a formal development
of Prawitz’s ground-theoretic ideas. A full understanding of the apparatus we
are going to propose requires the introduction of some technical tools which the
theory of grounds refers to more or less directly, as well as a quick exposition of
formalizations that Prawitz has developed over the years, that come conceptually
close to the ground-theoretic approach. As for the ﬁrst point, we will refer to the
so-called Curry-Howard isomorphism—for the exposition of which we will use,
in addition to The formulae-as-types notion of construction. by William Alvin
Howard (1980), Proofs and types by Jean-Yves Girard et al. (1993) and, for some
details, Lectures on the Curry-Howard isomorphism, by Morten Heine Sørensen and

118
4
Prawitz’s Theory of Grounds
Pawel Urzyczy (2006); Prawitz’s aforementioned proposals are instead contained in
Constructive semantics (Prawitz, 1970) and On the relationship between the Heyting
and Gentzen approaches meaning (Prawitz, 2016).
4.2.1
The Curry-Howard Isomorphism
Described for the ﬁrst time in The formula-as-type notion of construction (Howard,
1980) with reference to the constants ∧, →and ∀, and subsequently extended to the
constants ∨, ∃and ⊥, the Curry-Howard isomorphism—named after its creators,
Haskell Curry and Howard—identiﬁes a deep connection between Gentzen’s
systems of natural deduction for ﬁrst-order intuitionistic logic IL and what we will
call ﬁrst-order typed λ-calculus. The interest of the relation is essentially twofold:
ﬁrst, the isomorphism deﬁnes a bi-univocal correspondence between derivations
of IL and terms of the typed λ-calculus, offering an operational interpretation of
derivations; to such a bijection, which alone of course is not naturally sufﬁcient
to guarantee isomorphism, an invariance of normalization, modulo the bijection, is
added, together with the derived properties—which we have seen proven by Prawitz
(2006) for IL.
The Curry-Howard isomorphism is closely connected with the so-called
formulas-as-types conception. Martin-Löf summarizes this conception as follows:
if we take seriously the idea that a proposition is deﬁned by laying down how its canonical
proofs are formed [. . . ] and accept that a set is deﬁned by prescribing how its canonical
elements formed, then it is clear that it would only lead to an unnecessary duplication to
keep the notions of proposition and set (and the associated notions of proof of a proposition
and element of a set) apart. Instead, we simply identify them, that is, treat them as one and
the same notion. (Martin-Löf, 1984, 13)
When referred to the speciﬁc case of the derivation in IL, this standpoint becomes
the tenet according to which
the rules of natural deduction then appear as a special way of constructing functions: a
deduction of β on the hypotheses α1, . . . , αn can be seen as a function t[x1, . . . , xn] which
associates to elements ai ∈αi a result t[a1, . . . , an] ∈β. (Girard et al., 1993, 11)
In order to substantiate the idea of translating IL derivations into functional
expressions, it is necessary to provide a formal language in the which functional
expressions can be given rigorously. This language will be referred to a ﬁrst-order
logic language L, and its types will be the elements of the set FORML ∪{0} with
0 constant type for terms of L. The language will have individual typed variables,
and functional symbols: D, for pair-formation; Di (i = 1, 2), for pair-projections;
injβ
i (i = 1, 2), for formation of a term of type α ∨β (i = 1) or β ∨α (i = 2)
from a term of type α; case [α, β] t1 of t2, for proof by cases on a term of type
α ∨β; λ, for λ-abstraction on (possibly typed) individual variables; application of
a λ-abstraction; and let [yj, α(yj)] t1 in t2, for proof on a speciﬁc t1 on a term t2
of type ∃xα(x).

4.2
Towards a Formal Approach to Grounds
119
The analogue of the maximal formulas in the derivations of IL is, in the typed
λ-calculus, the concept of redex. A redex is a term in this calculus of one of the
following forms:
•
Di(D(t1, t2)) (i = 1, 2)
•
case [α, β] injβ
i (t1) of t2 or t3 (i = 1, 2)
•
(λξα.t1)t2
•
(λx.t)k
•
let [x, α(x)] D(k, t1) in t2
t is said to be in normal form if, and only if, it does not contain redexes. Some
equations ﬁx the computation of terms in non-normal form:
Di(D(t1, t2)) = ti(i = 1, 2)
case [α, β] injβ
1(t1) of t2 or t3 = t2[t1/ξα]
case [α, β] injβ
2(t1) of t2 or t3 = t3[t1/ξα]
(λξα.t1)t2 = t1[t2/ξα]
(λx.t)k = t[k/x]
let [x, α(x)] D(k, t1) in t2 = t2[t1/ξα(x)]
As can be seen, these equations reﬂect Prawitz’s reductions for IL. And similarly
to what happened there, it is possible to ensure that the result of the computation
of a redex of type α with a set of free typed variables  is still a term of type α
with a set of free typed variables ∗⊆; it is sufﬁcient to adopt an analogue
of Convention 3 in Chap. 3, after having introduced a distinction between free and
bound variables, and having established that x in λx.t and let [x, α(x)] t1 in t2 is
the proper variable of, respectively, λ-abstraction and let/in.
Convention 10 In every t (1) free and bound variables are all distinct—property
(FB)—and (2) proper and non-proper variables are all distinct, and each proper
variable is used in at most one application of λ-abstraction or let/in—property
(PN).
Strictly speaking, the typed calculus we have outlined is a functional equivalent
of ML; in order to extend it to IL, we must ﬁnd the way to express (⊥). Therefore
we add ⊥α (α ∈FORML) to the operational symbols, in such a way that, for each t
of type ⊥, ⊥α(t) has type α. With this modiﬁcation, we could also have redexes of
the type
•
Di(⊥α1∧α2(t)) (i = 1, 2)
•
case [α, β] ⊥α∨β(t1) of t2 or t3
•
⊥α→β(t1)t2
•
⊥∀xα(x)(t)k
•
let [x, α(x)] ⊥∃xα(x)(t1) in t2

120
4
Prawitz’s Theory of Grounds
which, however, we avoid worrying about by resorting to an analogue of the theorem
quoted in Sect. 3.2, regarding the corresponding case of the derivations of IL: for
every t1 of type α with set of free typed variables , there is t2 of type α with
set of free typed variables ∗⊆ such that, for every application of ⊥β in t2,
β ∈ATOML. At this point, we can enunciate a theorem of normal form.
Theorem 11 For every t1 of type α with set of free typed variables , there is t2 in
normal form of type α with set of free typed variables ∗⊆.
More precisely, we can talk about normalizability. t1 immediately reduces to
t2 if, and only if, t2 can be obtained by eliminating a redex from t1 through the
aforementioned equations. t1, . . . , tn is said to be a reduction sequence if, and only
if, for every i ≤n, ti immediately reduces to ti+1. t1 reduces to t2 if, and only
if, there is a reduction sequence that begins with t1 and ends with t2. t1 is called
normalizable if, and only if, there is t2 in normal form such that t1 reduces to t2.
Hence, Theorem 11 can be reformulated saying that each t is normalizable.
The frequent analogies so far referred to between IL derivations and terms of the
typed λ-calculus are no coincidence: the Curry-Howard isomorphism, in fact, allows
us to obtain the aforementioned properties of the terms—including normalization—
as derived properties of a bijection with corresponding derivations of IL, and vice
versa. The bijection can be deﬁned by induction on the complexity of  ∈IL:
α
ι
	⇒
ξα
1
α
2
β (∧I)
α ∧β
ι
	⇒
D(ι(1), ι(2))

α1 ∧α2 (∧E,i), i = 1, 2
αi
ι
	⇒
Di(ι()), i = 1, 2

αi
(∨I), i = 1, 2
α1 ∨α2
ι
	⇒
inj
αj
i (ι()), i, j = 1, 2, i ̸= j
1
α ∨β
[α]
2
γ
[β]
3
γ
(∨E)
γ
ι
	⇒
case [α, β] ι(1) of ι(2) or ι(3)
[α]

β
(→I)
α →β
ι
	⇒
λξα.ι()

4.2
Towards a Formal Approach to Grounds
121
1
α →β
2
α (→E)
β
ι
	⇒
ι(1)ι(2)
(x)
α(x)
(∀I)
∀yα(y/x)
ι
	⇒
λy.ι((y/x))

∀xα(x) (∀E)
α(k)
ι
	⇒
ι()k

α(k/x) (∃I)
∃xα(x)
ι
	⇒
D(k, ι())
1
∃yα(y/x)
[α(x)]
2(x)
β (∃E)
β
ι
	⇒
let [x, α(x)] ι(1) in ι(2(x))

⊥(⊥)
α
ι
	⇒
⊥α(ι())
The untyped variables in terms of the typed λ-calculus should be understood as
indexed, and doing the same thing (as is usual, after all) for the assumptions in the
derivations of IL, it is easy to prove that ι is injective and surjective and, therefore,
that it is a bijection. Isomorphism is guaranteed by the fact that 1 immediately
reduces to 2 if, and only if, ι(1) reduces immediately to ι(2).
4.2.2
Constructions and Translations
The article Constructive semantics, in which Prawitz anticipates—somewhat
surprisingly—the argument that he will only resume many years later with the
theory of grounds, dates back to 1970.
Constructive semantics revolves around two notions. The ﬁrst, that of typed
construction object, determines a set that has among its elements both the formulas
of a ﬁrst-order logical language L—on a base B deﬁned as usual, with relative
Post system S and a set DERS of atomic derivations—and what Prawitz calls
constructions for these formulas, which are used to ﬁx the meaning of the logical
constants. The construction terms, instead, belong to a formal language in itself,

122
4
Prawitz’s Theory of Grounds
and denote the above constructions under a speciﬁc interpretation of the symbols
occurring in them. However, the initial deﬁnition is that of type: the types will be
0 for terms and formulas of L and for derivations of DERS, and ⟨τ1, τ2⟩or τ1(τ2),
with the already acquired types τ1 and τ2, for, respectively, pairs and functions from
objects of type τ1 to objects of type τ2. Terms, formulas, atomic derivations, pairs
and constructive procedures determine a set of typed construction objects; the latter
is initially useful for establishing a typing of the formulas of L—with the aim of
establishing a correspondence between each formula of L and each construction
(term) for this formula.
•
α : 0 for α ∈ATOML
•
α1 : τ1, α2 : τ2 ⇒α1 ∧α2 : ⟨τ1, τ2⟩
•
α1 : τ1, α2 : τ2 ⇒α1 ∨α2 : ⟨⟨τ1, τ2⟩, 0⟩
•
α1 : τ1, α2 : τ2 ⇒α1 →α2 : τ1(τ2)
•
α : τ ⇒∃xα(x) : ⟨0, τ⟩
•
α : τ ⇒∀xα(x) : 0(τ)
Moreover, it becomes possible to establish inductively the notion ω is a construction
for α on B, indicated with C(ω, α, B):
•
α ∈ATOML ⇒ω ∈DERS
•
α is of the form α1 ∧α2 ⇒ω is of the form ⟨ω1, ω2⟩with C(ω1, α1, B) and
C(ω2, α2, B)
•
α is of the form α1∨α2 ⇒ω is of the form ⟨⟨ω1, ω2⟩, αi⟩with i, j = 1, 2, i ̸= j,
C(ωi, αi, B), and ωj an arbitrary object of type αj
•
α is of the form α1 →α2 ⇒ω is an object of type of α such that, for every
ω1 such that C(ω1, α1, B), C(ω(ω1), α2, B) [Prawitz takes into account here
also extensions B+ of B, but we will leave out this detail to comply with the
subsequent developments]
•
α is of the form ∃xα(x) ⇒ω is of the form ⟨t, ω1⟩with t ∈TERML and
C(ω1, α(t), B)
•
α is of the form ∀xα(x) ⇒ω is an object of type of α such that, for every
t ∈TERML, C(ω(t), α(t), B)
Given the previous deﬁnitions, it is easy to verify that C(ω, α, B) if, and only if,
ω and α are objects of the same type. We will say then that α closed is constructively
true on B if, and only if, there is ω such that C(ω, α, B); the same will apply for
α open if, and only if, it exists ω such that C(ω, CL(α), B). Therefore, generally
speaking, α is constructively valid if, and only if, α is constructively true on every B.
We will say that α is intuitionistically true on B if, and only if, α is constructively
true on a consistent B; then α will be intuitionistically valid if, and only if, α is
intuitionistically true on each consistent B.
The construction terms are introduced by Prawitz in this framework in order to
prove that ML and IL are correct in terms of constructive and intuitionistic validity.
To do this, we need a formal language with typed individual variables and functional
symbols D, for pair formation; Di (i = 1, 2), for projection on pair; λ, for λ-
abstraction on typed individual variables; application of a λ-abstraction; c, for a sort

4.2
Towards a Formal Approach to Grounds
123
of “choice function” such that c(t1, t2, t3, t4) = t1 if t3 = t4 and c(t1, t2, t3, t4) = t2
if t3 ̸= t4; S, for a substitution function such that S(t1, . . . , tn, u1, . . . , un, z) =
z[u1, . . . , un/t1, . . . , tn]. With value of a term t we mean the term t∗such that
t = t∗; it is easy to prove that, for each t, there exists a unique t∗, and furthermore
that t and t∗always have the same type. Therefore, given α1, . . . , αn, β formulas of
L with FV (α1) ∪. . . ∪FV (αn) ∪FV (β) = {y1, . . . , ym}, we will say that a term
t is (intuitionistically) appropriate for α1, . . . , αn ⊢β if, and only if
(1) the free variables of t are a subset of {x0
1, . . . , x0
m, xτ1, . . . , xτn}, with αi : τi
(i ≤n) and moreover
(2) for every (consistent) base B on L, for every t1, . . . , tn, u1, . . . , um objects
on B, if z is the term obtained from t by replacing xτi with ti and x0
i
with ui, then C(ti, S(y1, . . . , ym, u1, . . . , um, αi), B) (i
≤
n) implies
C(z, S(y1, . . . , ym, u1, . . . , um, β), B).
Thus, if n = 0, t is intuitionistically appropriate for α1, . . . , αn ⊢β if, and
only if, for every (consistent) base B, C(t, CL(β), B). We can now prove that,
if α1, . . . , αn ⊢ML[IL] β, there is a term of the just described formal language
which is (intuitionistically) appropriate for α1, . . . , αn ⊢β. As an easy corollary,
if ⊢ML[IL] α, then α is, respectively, constructively or intuitionistically valid: it is
indeed ensured the existence of a term t appropriate for ⊢α—and hence such that
C(t, CL(α), B) for every (consistent) B. Two observations have now to be made.
The ﬁrst concerns the relation between the program elaborated by Prawitz in
Constructive semantics and the one we ﬁnd in the semantics of valid arguments
and proofs. Undoubtedly, the above terms of the formal language can be extended,
generalizing the Curry-Howards isomorphism, to encodings of a certain class
of valid arguments—in particular, valid arguments whose valid inferences (and
inference rules) in non-introductory form are justiﬁed by resorting exclusively to
selection or composition of, and to substitution in, subarguments obtained with
limitations similar to the previous ones. It is also true that constructions can be
viewed as encodings of valid arguments, or rewriting of proofs, in canonical form.
However, in Constructive semantics Prawitz clearly separates the two notions: a
construction is a typed object always in canonical form, whereas a term is a typed
expression of a speciﬁc formal language that can be in canonical or non-canonical
form. There is nothing like that in the semantics of valid arguments and proofs
and, from this point of view, Constructive semantics is particularly interesting with
respect to the proofs-as-chains and recognizability problems, and with respect to
their possible solution. It should also be noted that in Constructive semantics there
is no reference to the problem of (valid) inferences; it becomes therefore important
to emphasize how the theory of grounds picks up, with particular reference to this
problem, many of the ideas of Constructive semantics.
The last point concerns the formal language that we have deﬁned. It is designed
for the deﬁnition of terms suitable to denote constructions, and this in view of
a correctness proof for ML and IL. To do this, a closed language, in a certain
sense equivalent to the typed λ-calculus from the previous section, is sufﬁcient.
In contrast, the theory of grounds requires taking into account open languages. With

124
4
Prawitz’s Theory of Grounds
respect to this issue, of particular signiﬁcance is an article in which Prawitz carries
out a translation of valid arguments into BHK proofs, and vice versa.
As we have seen, Prawitz’s semantics of valid arguments and proofs has, among
others, two important sources of inspiration: BHK semantics, and Gentzen’s sug-
gestion that the introduction rules establish—or, more weakly, mirror—the meaning
of logical constants—whereas the elimination rules are univocal functions, and as
such justiﬁable, of the corresponding introductions. This dual matrix corresponds
to the parallel plans on which, in Prawitz’s framework, the notion of proof can
be investigated: as an abstract object by virtue of which propositions or sentences
are correctly judged as true or asserted, or as a linguistic structure, essentially
inferential, dedicated to establishing the correctness of judgments or assertions.
We have also said that Prawitz, in some sense, enriches the picture of the BHK
clauses, introducing the same distinction for proofs between canonical and non-
canonical cases which, in a natural way, applies in the case of valid arguments; this,
for its part, could give hope that the two approaches are not only parallel, but even
equi-extensional. In more detail, a non-canonical proof could be made to correspond
to a valid non-canonical argument, understanding the constructive functions that
occur in the former as translations of the rules that occur in the latter; therefore, to
inferences—or inference rules—in a non-introductory form there will correspond
applications of constructive procedures—or constructive procedures—the behavior
of which is ﬁxed by equations that translate into operational terms the reductions of
the starting rules.
This issue is dealt with in the article On the relation between Heyting’s and
Gentzen’s approaches to meaning (Prawitz, 2016), in which Prawitz notably
observes how
Gentzen was concerned with what justiﬁes inferences and thereby with what makes some-
thing a valid form of reasoning. These concerns were absent from Heyting’s explanations
of mathematical propositions and assertions. The constructions that Heyting refers to in
his meaning explanations [. . . ] are mathematical objects [. . . ]. They are not proofs built up
from inferences. (Prawitz, 2016, 5–6)
This conceptual difference, however, may not be so profound as to prevent the
translation of one approach into the other, and vice versa. According to Prawitz
himself, moreover, the differences
do not rule out the possibility that the existence of such proofs nevertheless comes materially
to the same. For instance, a BHK-proof of an implication α →β is deﬁned as an operation
that takes a BHK-proof of α into one of β, and a closed Gentzen proof of α →β affords
similarly a construction that takes a Gentzen proof of α into one of β [. . . ]. Such similarities
may make one expect that one can construct a BHK-proof given a Gentzen proof and vice
versa. (Prawitz, 2016, 6)
Prawitz then proposes to deﬁne a mapping from closed valid arguments to BHK
proofs and, vice versa, a mapping from BHK proofs to closed valid arguments.
The intention is that this mapping be, more strongly, a constructive bijection: given
a BHK proof, it is possible to construct a closed valid argument that corresponds
to it and, vice versa, from a closed valid argument we can extract an appropriate

4.2
Towards a Formal Approach to Grounds
125
BHK proof. It should be pointed out, however, that Prawitz’s translations concern
a notion of “constructive validity”, obtained by understanding intuitionistically the
existential involved in the deﬁnition of a closed valid argument, and a notion of
“strong validity” that we have omitted in this work, but that Prawitz had already
introduced, although in a different form, in Towards a foundation of a general
proof theory (Prawitz, 1973). Without going into detail, we limit ourselves here to
summarizing the results achieved by the Swedish logician: (1) the translation from
closed valid arguments to BHK proofs only works when “valid” means “strongly
valid” or “valid in a constructive sense”; and (2) the translation from BHK proofs to
closed valid arguments only works when “valid” does not mean “strongly valid”. In
conclusion, Prawitz is unable to achieve a full equi-extensionality, showing on the
contrary how the latter depends in an essential way on the type of validity attributed
to the argumental structures.

Part II
Formal Epistemic Grounding

Chapter 5
Languages of Grounding
5.1
General Overview
In the ﬁrst part, we discussed the reasons that have led Prawitz to the adoption of the
theory of grounds, as well as how this theory appears in the writings that Prawitz
has so far devoted to his ground-theoretic approach. We have said that the aim of the
theory of grounds is that of accounting for the epistemic compulsion experienced
in correct deduction, and we have shown that both model theory and Prawitz’s
semantics of valid arguments and proofs are unable to describe this phenomenon in a
satisfactory way. The semantics of valid arguments and proofs, in particular, suffers
from three problems: the proofs-as-chains problem, the recognizability problem,
and the problem of validity as independent from inferences. With respect to these
problems, the theory of grounds offers partial or total solutions but, in the end, it is
affected by a further problem, already detectable in the semantics of valid arguments
and proofs: the problem of vacuous validity, that is, the problem of justiﬁcation of
inferences through empty functions. Of course, this may cast doubts also on the
capability of the theory of grounds to capture epistemic compulsion. Nonetheless, it
is undoubted that the theory of grounds provides a very ﬁne-grained reconstruction
of deductive activity, as well as of the power of this activity. This happens through
a harmonious implementation of many inﬂuential traditions in the constructivist
ﬁeld of contemporary logic: BHK semantics, proof-theoryoriginating with Gentzen,
Dummett’s investigations into theory of meaning, and Prawitz’s own normalization
theory. Therefore, although the ground-theoretic conception might stand in need of
further reﬁnements, it may still be worth trying to develop its formal aspects in more
detail. Formalisation, after all, may help to clarify the weak aspects of the theory,
and suggest the direction along which possible improvements could be introduced.
Unfortunately, the formal part of the theory of grounds is, in Prawitz’s writings, still
at an embryonic stage. Apart from a few precious examples and hints, we do not ﬁnd
in these writings a uniform formal framework, nor more speciﬁc concepts than the
very general ones of (operation on) grounds and of language of grounding. It is for
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_5
129

130
5
Languages of Grounding
this reason that, building upon Prawitz’s suggestions, the second part of our work is
devoted to the development of a formal theory of epistemic grounding. Our proposal
is not meant to be exhaustive, and indeed will remain to further developments. The
proposal is thus intended to serve as a secure basis for future, Prawitz-inspired
research on epistemic grounding.
In this chapter we introduce a class of languages of grounding—namely formal
languages the terms of which express grounds or operations on grounds. The
link between terms, on the one hand, and grounds or operations on grounds that
they express, on the other, is speciﬁed through a notion of denotation, deﬁned
as a function from languages of grounding to grounds or operations on grounds.
Since some limitative results show that, for each language of grounding at least
as powerful as Heyting ﬁrst-order arithmetic, there are grounds or operations on
grounds denoted by no term, no language of grounding, with the relative denotation
function, can be said to be “deﬁnitive”. Therefore, it is also necessary to deﬁne a
notion of expansion of language of grounding.
5.1.1
From Grounds to Terms, Through Denotation
When we talk about grounds, we should keep in mind two basic ideas. The ﬁrst
illuminates and substantiates the understanding of the grounds as abstract objects.
A ground is what we must be in possession of when we judge or assert correctly; it
is the reiﬁcation of a mental state of justiﬁcation for judgments or assertions. The
second concerns the understanding of the grounds as epistemic objects: a ground
is obtained by performing certain operations. This suggests that, strictly speaking,
grounds are not the only objects of the correspondent theory; other, and equally
important, objects are the operations on grounds. But grounds and operations on
grounds are notions inseparably related to each other.
5.1.1.1
Grounds, Operations, Judgments/Assertions
A ground justiﬁes a certain judgment or a certain assertion; it is the reiﬁcation
of a mental state related to a well-determined judgment, or to a well-determined
assertion. In the same way, an operation on grounds allows us to pass from grounds
that justify certain judgments or certain assertions to a certain judgment or to a
certain assertion; the passage takes place from a mental state of justiﬁcation for a
well-determined group of judgments or assertions, to a well-determined judgment
or to a well-determined assertion. It therefore makes sense to talk about grounds for
a speciﬁc judgment or a speciﬁc assertion, and about an operation on grounds for
a speciﬁc judgment or a speciﬁc assertion on the basis of other speciﬁc judgments
or other speciﬁc assertions. The link that is so established between grounds and
operations on grounds, on the one hand, and judgments or assertions on the other,

5.1
General Overview
131
allows us to set more precisely the different types of grounds and operations on
grounds, and to clarify the link between the former and the latter.
In the perspective of what Schroeder-Heister (Schroeder-Heister, 2008, 2012),
and with him Kosta Došen (Došen, 2015), have called dogma of the primacy of the
categorical over the hypothetical, the initial notion is that of ground for a categorical
judgment or assertion. Categorical judgments and assertions involve propositions or
sentences, which we will understand here as being expressed by closed formulas. In
this case, the grounds will be ﬁxed by the clauses we have illustrated in Chap. 4—
and which will be recalled below; in particular, on par with the proposition or
sentence involved in the judgment or assertion for which the ground is a ground,
our object will be a closed one. Obviously, the fact that the closed character of
a categorical judgment or assertion stems from the closed character of its ground
depends on the fact that the justiﬁcation of a categorical judgment or assertion
cannot be based on reference to arbitrary individuals or on assumptions; if the
ground must act as a justiﬁcation, it must not have individual variables or free
ground-variables.
After having acquired the notion of ground for a categorical judgment or
assertion, we can introduce a ﬁrst, elementary class of operation on grounds, which
we will call of ﬁrst level. While the grounds for categorical judgments or assertions
are closed, a ﬁrst level operation on grounds is an object that applies to individuals
in an appropriate domain, or to grounds. In describing ﬁrst level operations, we will
proceed gradually, starting with those that only apply to individuals, then those that
only apply to grounds, and ﬁnally, as a combination of these ﬁrst two cases, we will
discuss those that apply both to individuals and to grounds. The values produced by
these operations will always be grounds.
In what follows, we will understand general judgments and assertions as
involving open formulas, and hence as having the form ⊢α(x1, . . . , xn). A ground
for ⊢α(x1, . . . , xn) is to be understood as a function f that associates individuals
from a reference domain D to grounds for ⊢α(k1, . . . , kn), with ki ∈D (i ≤n).
If we indicate with Grα(k1,...,kn) the class of the grounds for ⊢α(k1, . . . , kn), in a
standard, set-theoretic notation for functions, we will thus have
f : Dn →

k1,...,kn∈D Grα(k1,...,kn)
and
f (k1, . . . , kn) = g ⇔g ∈Grα(k1,...,kn)
The function will therefore have a linguistic expression with individual variables
f (x1, . . . , xn)
such that, for every ki ∈D (i ≤n),
f (k1, . . . , kn) ∈Grα(k1,...,kn)

132
5
Languages of Grounding
i.e., is a ground for ⊢α(k1, . . . , kn). In this case, we will speak of an operation of
operational type
α(x1, . . . , xn).
In the case of general judgments or assertions, we have correctness if what has
been judged or afﬁrmed is valid, depending on no assumption, for arbitrary individ-
uals corresponding to the free individual variables involved in the propositions or
sentences concerned; grounds for general judgments or assertions can be understood
as functions whose linguistic expressions involve exclusively these individual free
variables and which, when applied to individuals in the reference domain, produce
grounds for the categorical judgments or assertions obtained by performing the same
replacement in the propositions or sentences at issue.
Then, we have hypothetical judgments or assertions, involving a proposition or
sentence depending on other propositions or sentences, and hence having the form
α1, . . . , αn ⊢β, for α1, . . . , αn, β closed. A ground for α1, . . . , αn ⊢β is to be
understood as a function f that associates grounds for ⊢αi (i ≤n) to grounds for
⊢β. If we indicate with Grαi, Grβ the class of the grounds for ⊢αi (i ≤n) and
⊢β, respectively, we will thus have—again in a standard, set-theoretic notation—
f : Grα1 × · · · × Grαn →Grβ.
The function will therefore have a linguistic expression with ground-variables
f (ξα1, . . . , ξαn)
such that, for every gi ∈Grαi (i ≤n),
f (g1, . . . , gn) ∈Grβ
i.e., is a ground for ⊢β. In this case, we will speak of an operation on grounds of
operational type
α1, . . . αn ▷β.
We have here correctness if what has been judged or afﬁrmed is valid, with
no reference to arbitrary individuals, depending on a certain number of closed
assumptions; grounds for hypothetical judgments or assertions can be understood
as functions whose linguistic expressions involve exclusively free ground-variables,
each of an appropriate closed type, and which, when appropriately applied to
grounds, produce grounds for the dependent categorical judgment or assertion.
By combining hypothetical judgments or assertions with general judgments
or assertions, we obtain open-hypothetical judgments or assertions. They involve
a possibly open formula depending on other possibly open formulas, and thus
have the form α1, . . . , αm
⊢β, for α1, . . . , αm, β possibly open. A ground

5.1
General Overview
133
for α1, . . . , αm ⊢β is to be understood as a function f that operates in the
following way. Let D be our reference domain, and let {x1, . . . , xn} be the
set of the individual variables occurring free in α1, . . . , αm, β. Given a n-tuple
⟨k1, . . . , kn⟩of elements of D, and given a m-tuple ⟨g1, . . . , gm⟩of grounds such
that gi is a ground for ⊢αi[k1, . . . , kn/x1, . . . , xn] (i ≤m), f associates to
the pair ⟨⟨k1, . . . , kn⟩, ⟨g1, . . . , gm⟩⟩a ground for ⊢β[k1, . . . , kn/x1, . . . , xn].
So, let us indicate with Grαi[k1,...,kn/x1,...,xn] the class of the grounds for ⊢
αi[k1, . . . , kn/x1, . . . , xn] (i ≤m), and given
Dn ×

k1,...,kn∈D(Grα1[k1,...,kn/x1,...,xn] × · · · × Grαm[k1,...,kn/x1,...,xn])
let us consider its subclass
B = {⟨⟨k1, . . . , kn⟩, ⟨g1, . . . , gm⟩⟩| gi ∈Grαi[k1,...,kn/x1,...,xn], i ≤m}.
We will therefore have—again in a standard, set-theoretic notation—
f : B →

k1,...,kn∈D Grβ[k1,...,kn/x1,...,xn]
and
f (⟨⟨k1, . . . , kn⟩, ⟨g1, . . . , gm⟩⟩) = g ⇔g ∈Grβ[k1,...,kn/x1,...,xn].
Observe that this case would have been sufﬁcient to describe all the ﬁrst level
operations. Starting from it, indeed, we can obtain the other two by setting
{x1, . . . , xn} = ∅or {ξα1, . . . , ξαm} = ∅.
The function will therefore have a linguistic expression with individual and ground-
variables
f (x1, . . . , xn, ξα1, . . . , ξαm)
such that, for every ki ∈D (i ≤m),
f (k1, . . . , kn, ξα1[k1,...,kn/x1,...,xn], . . . , ξαm[k1,...,kn/x1,...,xn])
is an operation of operational type
α1[k1, . . . , kn/x1, . . . , xn], . . . , αm[k1, . . . , kn/x1, . . . , xn]
▷β[k1, . . . , kn/x1, . . . , xn]

134
5
Languages of Grounding
that is, for every gj ground for ⊢αj[k1, . . . , kn/x1, . . . , xn] (j ≤m),
f (k1, . . . , kn, g1, . . . , gm)
is a ground for ⊢β[k1, . . . , kn/x1, . . . , xn]. In this case, we will speak of an
operation of operational type
α1, . . . , αm ▷β.
We have here correctness if what has been judged or afﬁrmed is valid for arbitrary
individuals, corresponding to the free individual variables in the propositions or
sentences of reference, and dependent on a certain number of possibly open assump-
tions; grounds for hypothetical-general judgments or assertions can be understood
as functions whose linguistic expressions involve the free individual variables of
the propositions or sentences at issue, and free ground-variables of the appropriate
type, and which, when appropriately applied to grounds (possibly on formulas in
which the free individual variables have been replaced with names of individuals of
the reference domain), produce a ground for the dependent (categorical or general)
judgment or assertion (with possible replacements of free individual variables with
individuals from the reference domain).
Starting from the notion of operation on grounds of ﬁrst level, we can outline a
more complex class of operations on grounds, which we will call of second level.
In addition to individuals and grounds, these objects can also apply to ﬁrst level
operations on grounds.
Here, we have to take into account hypothetical, or general-hypothetical judg-
ments or assertions, where at least one of the assumptions, and possibly also the
depending judgment or assertion, is again a hypothetical, or general-hypothetical
judgment or assertion. The general form will hence be τ1, . . . , τm ⊢τm+1, where
τi (i ≤m + 1) is either a formula, or a hypothetical, or general-hypothetical
judgment or assertion (we have the restriction that, if τm+1 is a hypothetical, or
general-hypothetical judgment or assertion, the set of its assumptions is contained
in the union set of the assumptions of all the τi, for i ≤m). A ground for
τ1, . . . , τm ⊢τm+1 is to be understood as a function f that operates in the following
way. Let D be our reference domain, and let {x1, . . . , xn} be the set of the individual
variables occurring free in the codomains of the τ1, . . . , τm, τm+1. Given an n-
tuple ⟨k1, . . . , kn⟩of elements of D, and given an m-tuple ⟨g1, . . . , gm⟩of grounds
or operations on grounds such that gi is a ground or an operation on grounds
of operational type τi[k1, . . . , kn/x1, . . . , xn] (i ≤m), f associates to the pair
⟨⟨k1, . . . , kn⟩, ⟨g1, . . . , gm⟩⟩a ground or operation on grounds of operational type
τm+1[k1, . . . , kn/x1, . . . , xn]. Thus, if we indicate with Grτi the class of the grounds
or of the operations on grounds of operational type τi (i ≤m), we proceed in the
same way as above, by singling out a corresponding subclass B of
Dn ×

k1,...,kn∈D(Grτ1[k1,...,kn/x1,...,xn] × · · · × Grτm[k1,...,kn/x1,...,xn]).

5.1
General Overview
135
Only the individual variables in the codomains are mentioned because, in operations
of this kind, the arguments may be operations on grounds with any domain—hence,
the variables of the domains may not be actually there. In conclusion, we will have
a linguistic expression
f (x1, . . . , xn, ξτ1, . . . , ξτm)
and we will speak of an operation of type
τ1, . . . , τm ▷τm+1.
Some operations on grounds may bind individual variables. In this case the
linguistic expression of the operation will not involve the individual variables that
it binds and which are contained in the judgment or assertion of which it is ground.
In turn, the dependent judgment or assertion will not contain any of the bound
individual variables, or it will contain them as individual variables universally
quantiﬁed. An operation on grounds that binds individual variables will be subject
to a similar restriction as that on the introduction rule for ∀or the elimination
rule for ∃in Gentzen’s natural deduction systems. More generally, operations on
grounds are subject to restrictions similar to those on the rules for the quantiﬁers in
Gentzen’s natural deduction systems, related to the proper variables (introduction of
∀and elimination of ∃) and to the being free of a term for a variable in a formula
(introduction of ∃and elimination of ∀). Some operations on grounds may also bind
ground-variables, in which case the judgment or dependent assertion will involve a
judgment or assertion that is no longer dependent on the assumptions corresponding
to the type of the bound ground-variables.
In our description of the various kinds of operations, we have so far only
mentioned their domain and codomain. However, the operations that we will deal
with, are not to be understood as functions in a standard set-theoretic sense—
i.e., as laws that associate, without further speciﬁcations, elements of the domain
to elements of the codomain. A ground-theoretic operation is also determined
by another parameter—namely, by equations that deﬁnes it, by showing how the
arguments in the domain are “computed” by the function, and hence how the value
of the function on those arguments can be obtained, and what this value is. In
addition, the “computation” instruction exhibited by the equation must be such that
the function it deﬁnes is constructive. Through the equation, it must therefore be
possible to effectively “compute” the function on every argument of the domain, to
get thereby the corresponding argument in the codomain.
The notion of second-level operation on grounds requires that of ﬁrst level
operation on grounds, and the latter in turn requires the notion of ground for
categorical judgments or assertions. However, as we have seen in Chap. 4, the
deﬁnition of ground for categorical judgments or assertions of the form ⊢α →β
or ⊢∀xα(x) requires the notion of ﬁrst-level operation on ground as ground for,
respectively, hypothetical judgments or assertions of the form α ⊢β, and general
judgments or assertions of the form α(x). Nonetheless, the overall deﬁnition is not

136
5
Languages of Grounding
circular, and this is because the clauses proceed via simultaneous recursion—taking
advantage of the fact that primitive operations concern the passage from judgments
or assertions which involve formulas of lower logical complexity to judgments or
assertions that involve formulas of greater logical complexity. Therefore, in order to
know what a ground is for ⊢α →β we need to know what a ground is for α ⊢β,
and to know this we need to know what is a ground for ⊢α and what is a ground
⊢β. Similarly, in order to know what a ground is for ⊢∀xα(x), we need to know
what a ground is for α(x), and to know this we need to know what a ground is for
⊢α(k), for k (name of) individual in the reference domain.
5.1.1.2
Denotation of Terms
The languages of grounding that we will be developing will consist of terms to
be understood as “names” of some of the “inhabitants” of our “universe”. When
deﬁning the denotation, we will stick to the following general scheme: a closed term
of type α denotes a ground for the categorical judgment or assertion ⊢α; an open
term with only free individual variables x1, . . . , xn of type α(x1, . . . , xn) denotes a
ground for the general judgment or assertion α(x1, . . . , xn) - namely an operation
f (x1, . . . , xn)
of operational type
α(x1, . . . , xn);
an open term with only ground-variables ξα1, . . . , ξαn of type β denotes a ground
for the hypothetical judgment or assertion α1, . . . , αn ⊢β—namely an operation
on grounds
f (ξα1, . . . , ξαn)
of operational type
α1, . . . , αn ▷β;
and an open term with free individual variables x1, . . . , xn and free ground-variables
ξα1, . . . , ξαm of type β denotes a ground for the general-hypothetical judgment or
assertion α1, . . . , αm ⊢β—namely an operation on grounds
f (x1, . . . , xn, ξα1, . . . , ξαm)
of operational type
α1, . . . , αm ▷β.

5.1
General Overview
137
The second level operation on grounds are denoted by no term, since the languages
of grounding do not include variables typed on ﬁrst-level operations on grounds (see
Prawitz, 2015).
5.1.1.3
A Summary Scheme
The content of the two previous sections is summarised and clariﬁed in Table 5.1.
Two observations are now in order. First, we anticipate that the impossibility
to express second-level operations will be overcome by attributing denotation to
the operational symbols for the construction of terms. Indeed, denotation of terms
is deﬁned inductively, on the basis of a denotation function for the elements of
the alphabet of the language of grounding. In the alphabet, in turn, we will have
operational symbols typed on operational types for ﬁrst-level operations, depending
on the operations that such symbols are meant to express. If the symbol binds
ground-variables on the index i, the denotation function for the alphabet will assign
to it a second-level operation. The i-th assumption of the operational type of this
operation is a hypothetical or general-hypothetical judgment or assertion that has,
as assumption, the assumption typing the bound ground-variable, and as depending
judgment or assertion, the formula with index i in the operational type of the symbol.
In other words, if the type of the operational symbol has β as i-th assumption, and
if the symbol binds the variable ξα on the index i, the denotation of this symbol will
be a second-level operation on grounds, the operational type of which has, as i-th
assumption, the hypothetical or general-hypothetical judgment or assertion α ⊢β.
Table 5.1 Acts, objects and denoting terms
Act
Object
Term
Categorical ⊢α
Closed ground g
Closed of type α
General ⊢α(x1, . . . , xn)
Operation on grounds
f (x1, . . . , xn) of operational
type α(x1, . . . , xn)
Open of type α(x1, . . . , xn)
with free variables x1, . . . , xn
Hypothetical
α1, . . . , αn ⊢β
Operation on grounds
f (ξα1, . . . , ξαn) of
operational type
α1, . . . , αn ▷β
Open of type β with free
variables ξα1, . . . , ξαn
General-hypothetical
α1, . . . , αm ⊢β
Operation on grounds
f (x1, . . . , xn, ξα1, . . . , ξαm)
of operational type
α1, . . . , αm ▷β
Open of type β with free
variables x1, . . . , xn and
ξα1, . . . , ξαm
General hypothetical
τ1, . . . , τm ⊢τm+1 for
some τi
general-hypothetical
(i ≤m, possibly also
i = m + 1)
Operation on grounds
f (x1, . . . , xn, ξτ1, . . . , ξτm)
of operational type
τ1, . . . , τm ▷τm+1
None

138
5
Languages of Grounding
Alternatively, we could have let the languages of grounding contain variables typed
on ﬁrst-level operations on grounds. We shall actually pursue this strategy, but only
starting from Chap. 6, with reference to the formal systems that we will develop
therein. For the moment, we shall leave it aside, to avoid an excessive degree of
complexity in languages of grounding and in the deﬁnition of denotation.
The second observation concerns the general framework that we are in the
process of outlining. Our “universe” is “inhabited” by: saturated grounds, for
categorical judgments or assertions; and unsaturated grounds, that is, functions
deﬁned on individuals or grounds, for general, hypothetical, or general-hypothetical
judgments or assertions of ﬁrst level, and functions deﬁned on individuals, grounds
and ﬁrst-level operations on grounds for general, or general-hypothetical judgments
or assertions of second level. To this dichotomy there corresponds, at the linguistic
level, the one between closed terms, with no free individual or ground-variables,
and open terms, where we instead have free occurrences of individual or ground-
variables. The relevant notions for the unsaturated cases are deﬁned by essentially
resorting to the saturated cases. That a ﬁrst-level operation has a certain operational
type is explained by requiring that the operation gives the right saturated value
when applied to saturated arguments, and that a second-level operation has a
certain operational type is explained by requiring that the operation gives the right
unsaturated value of lower level when applied to unsaturated arguments of lower
level. As we will see when deﬁning the denotation of the elements of the alphabet
and of the terms of the languages of grounding, this line of thought applies also
to the linguistic level. Given an open term, its denotation will be established by
referring to the saturated denotata of its closed instances.
Thus, our point of view is strongly Fregean (see in particular Frege, 1884,
1891, 2001). Here, Frege’s objects are our saturated grounds, whereas Frege’s
functions/concepts are our unsaturated grounds—that is, our operations. However,
this standpoint is also in line with the one that Prawitz himself adopts—above
all in his ﬁrst ground-theoretic papers (Prawitz, 2009, 2012a, 2013). As we said
in Chap. 4, the expressions “saturated” and “unsaturated”, as well as the idea of
grounds and their expressions being closed/complete or open/incomplete, is present
also in the earliest ground-theoretic writings of the Swedish logician (the same
conclusion seems to be upheld by Tranchini, 2014, 2019).
This remark has an important and deep consequence, concerning the issue about
the identity of the “inhabitants” of our universe. In what follows, we shall employ
two distinct notions of identity. The ﬁrst, more intensional in nature, will be called
identity in the strict sense, while the second, more extensional, will be labelled as
equivalence. Identity will amount to “sameness”, in the sense that two (operations
on) grounds will be said to be identical when they have the “same” structure,
and when they involve the “same” computational instructions for the operations
they are made up of. In this case, “sameness” of deﬁning equations amounts
to “sameness” of computations instructions, where the latter notion is taken as
primitive. By contrast, (operations on) grounds can be said to be simply equivalent
when they have the same domain, and they produce the same saturated values when
applied to the same saturated arguments or, at the second level, when this property

5.1
General Overview
139
holds for the unsaturated values generated from equal unsaturated values. As for
deﬁning equations, they will not ﬁx an operation by showing to which operation, or
combination of operations, the ﬁrst is equivalent. Rather, the equations show which
values the operations produce when they are applied to saturated values—to put it
with Frege, we pass through the courses-of-values.
The idea that equivalence cannot be established, or used in deﬁnitions, without
passing through the saturated/closed case, is reminiscent of the distinction between
open derivations and open valid arguments, in the respective frameworks of
normalization theory and proof-theoretic semantics—the former being a theory of
the structural properties of the derivations of a formal system, obtained by means of
reduction procedures, and the latter an investigation about the semantic properties
of given argument structures, obtained by means of appropriate justiﬁcations for
non-introductory rules. The normalizability of an open derivation is a property of
the derivation as such, and it can be obtained by applying syntactic operations to the
derivation itself, without referring to its closed instances. Instead, the validity of an
open argument structure requires looking at the closed instances of the structure,
and to perform on them adequate operations, that respect semantic desiderata
concerning the whole class of valid arguments. Thanks to what Schroeder-Heister
(Schroeder-Heister, 2006) calls the fundamental corollary of normalization theory,
the two notions are clearly linked, and we could even say that the second is a kind of
“semantization” of the ﬁrst. However, they are also conceptually distinct, and refer
to two different levels. Both in this chapter and in the next, we will again deal with
questions concerning identity. The similarity between how this notion is framed
within the theory of grounds, and the distinction between open derivations and
open valid arguments, will then be sharper—in particular, when discussing identity
axioms occurring in the formal systems of Chap. 6.
5.1.1.4
Proper and Improper Grounds
It may have been noted that the table above was too “simpliﬁed”, as it implies
symmetries that do not necessarily hold between operational types and arities of
the respective operations. In general, an operation on grounds may be such that
its linguistic expression has to involve more free individual variables than those
occurring in its operational type; a term denoting this operation may contain more
free individual variables than those occurring in the corresponding operational type.
In spite of this difﬁculty, however, throughout our investigation we will ensure
that an operational type for a ﬁrst-level operation on grounds involves a set of
free individual variables that is contained in the set of the free individual variables
occurring in the expression of an operation on grounds with the same operational
type—apart from bindings. In other words, if x occurs free in an operational type τ,
then the expression of any operation on grounds φ having type τ involves x, and x
occurs free in all terms (if any) denoting φ - apart from bindings. If the set of the
individual variables occurring free in τ coincides with that of the expression of φ,

140
5
Languages of Grounding
we will say that φ is a proper ground for the judgment or assertion corresponding
to τ. Otherwise, we shall speak of an improper ground.
The setup we have adopted can be justiﬁed as follows: an operation on grounds
may correspond to a reasoning which is not in normal form. This reasoning can
be reduced to a normal form, thereby corresponding to an operation on grounds
which involves all and only the individual variables occurring free in its type.
Likewise, the term denoting the operation that can be associated to the non-normal
reasoning can be reduced to a term denoting the operation that can be associated to
the reduced reasoning, and that contains only x as a free individual variable. If we
generalize this observation, it seems plausible to assume that operations on grounds
relative to more individual variables than those occurring in their operational type,
and corresponding terms (if any), contain some “detours”, and can be reduced to
operations on grounds and terms that require all and only the individual variables
occurring free in the corresponding operational type.
5.1.1.5
Total Constructive Functions
We have said that operations on grounds are total constructive functions. But what
do we mean by this expression?
To say that an operation on grounds is a total function, means that the operation
is a function that converges on all the values in the deﬁnition domain. Let us give
just two examples: an operation on grounds
f (x1, . . . , xn)
of operational type
α(x1, . . . , xm)
(m ≤n) is a total function such that, for every sequence k1, . . . , kn of individuals in
the reference domain,
f (k1, . . . , kn)
is a ground for ⊢α(k1, . . . , km). Likewise, an operation on grounds
f (ξα1, . . . , ξαn)
of operational type
ξα1, . . . , ξαn ▷β
is a total function such that, for every gi ground for ⊢αi (i ≤n),

5.1
General Overview
141
f (g1, . . . , gn)
is a ground for ⊢β. The reasoning proceeds analogously for ﬁrst-level operations
on grounds of other kinds, as well as for second-level operations on grounds. The
case of totality is therefore quite easily managed. How about with the expression
“constructive”?
From a general point of view, it would appear that, in the approach adopted by
Prawitz, constructiveness is to be understood as a sort of effective computability.
An operation on grounds is a constructive function in the sense that, whenever
applied to speciﬁc values in the deﬁnition domain, it is actually possible to compute
it so as to obtain the output to which those speciﬁc values correspond. After all,
it is no coincidence that, both in Prawitz, and in frameworks philosophically akin
to the theory of grounds, the expression “effective procedure” is often preferred
to “constructive function”—and in the present work, we have often used these
expressions as synonyms; an effective procedure is a set of computation instructions
such that, by using those instructions, the computation can actually be carried out.
Contemporary mathematical logic offers a series of devices that translate, in
formally rigorous terms, the concept of effectively computable function, although
often exclusively in the context of an investigation into the foundations of mathemat-
ics: recursive functions developed by Gödel (Gödel, 1931), Peter (Peter, 1951) and
Stephen Cole Kleene (Kleene, 1936), Turing machines (Turing, 1937) and, ﬁnally,
λ-calculus, introduced by Alonzo Church (Church, 1932), which we have already
dealt with. These theories are provably equivalent, in the sense that they declare
as effectively computable the same functions; on this basis, the so-called Church-
Turing thesis (Copeland, 2017) afﬁrms that a function is effectively computable
if, and only if, it is recursive, or equivalently if it is implementable in a Turing
machine, or equivalently if it can be translated into a λ-term. It is often said that the
Church-Turing thesis is not subject to a proof or ﬁnal confutation, since it links a
pre-formal or intuitive concept, such as that of an effectively computable function,
to mathematically precise notions, such as those of a recursive function, Turing
machine or λ-term. However, we might be tempted to exploit it here, in order to
deﬁne more precisely the functions related to operations on grounds.
We have said that the idea that an operation on grounds is a constructive
function can be equated with the idea that an operation on grounds is an effectively
computable function. Therefore, we could be led to understand operations on
grounds as, more speciﬁcally, recursive functions, or Turing machines, or λ-terms;
even if the latter are mainly related to a study of the foundations of mathematics,
the ideas underlying their deﬁnitions could be adapted to the theory of grounds, so
as to have a well-deﬁned notion of operation on grounds, a well-describable class
of operations on grounds. This strategy, however, seems problematic, on account of
a circularity pointed out by Peter in Rekursivitat und Konstruktivitat (Peter, 1959).
Although Peter primarily refers to the BHK semantics, on the one hand, and
to recursive functions, on the other, the central point of her argument is that
the deﬁnition of a recursive function, and therefore deﬁnitions of equivalent
notions, such as Turing machines or λ-terms, cannot be employed in a constructive
determination of the meaning of the logical constants without falling into vicious

142
5
Languages of Grounding
circles. The deﬁnitions, in fact, involve an existential quantiﬁer—in the case of
recursive functions, as a condition of existence for a scheme of equations that
produces expected values, and as the existence condition for the zeros of the μ-
operator. Now, this existential quantiﬁer clearly cannot be read in non-constructive
terms, since the characterization of the meaning of the logical constants would in
that case be inadequate; but if the existential must be read in necessarily constructive
terms, the characterization would clearly be circular.
As part of the exposition of BHK semantics in Meaning and proofs: on the
conﬂict between classical and intuitionistic logic (Prawitz, 1977), Prawitz refers
explicitly to Peter’s argument (quoting Peter, 1959); the Swedish logician concludes
that the notion of constructive function employed in the BHK clauses for →
and ∀must be assumed to be primitive and not further analyzable. However, the
clauses that ﬁx what counts as a ground for categorical judgments or assertions are
also intended as a constructive determination of the meaning of logical constants,
meaning they too are subject to the argument by which it would be circular to
specify operations on grounds in terms of recursive functions, Turing machines
and λ-terms. In light of this, in the course of our investigation we will assume as
primitive the notion of constructive function. We will limit ourselves just to saying
that an operation on grounds must be constructive in the sense of being actually
computable, on the basis of the above consideration.
Be that as it may, the character of computability of operations on grounds
will be guaranteed by conceiving the latter not just as set-theoretic objects but
as deﬁned by groups of equations that render them transformation methods; the
equations have to be such as to actually provide a way to transform the grounds or
the operations on grounds in the deﬁnition domain into grounds or operations on
grounds in the codomain. In this sense, the operations on grounds can be conceived
as a generalization of the reduction procedures used by Prawitz in the proof of the
normalization theorems (Prawitz, 2006), or of the equations for the elimination of
redexes in typed λ-calculus.
5.1.2
Basic Notions for Languages
The development of a formal framework for Prawitz’s epistemic grounding, based
on the main lines indicated earlier in this chapter, will proceed following the points
illustrated in the sections below.
5.1.2.1
Atomic Bases
Grounds and operations on grounds concern well-deﬁned judgments or well-
deﬁned assertions, and therefore speciﬁc formulas. A ground or an operation on
grounds will always be for a certain judgment or assertion, which can also be
expressed by saying that grounds and operations on grounds have as type certain

5.1
General Overview
143
(combinations of) formulas. Consequently, also the terms that denote grounds and
operations on grounds will be typed. We call the language to which types belong
a background language. Here, a background language will always be a ﬁrst-order
logical language.
A ﬁrst-order logical language comprises ﬁrst of all a well-speciﬁed alphabet,
starting from which we deﬁne recursively a set of terms and a set of formulas.
Beyond the usual logical constants ∧, ∨, →, ∀and ∃, and an atomic constant ⊥
for the absurd, the alphabets are characterized by a set of individual constants, a
set of relational symbols and a set of functional symbols—of which the ﬁrst and the
third can be empty. For such sets it is possible to deﬁne an atomic formal system,
which we will understand later as a Post-system of the type of that for ﬁrst-order
arithmetic described in Chap. 3. Individual constants, relational symbols, functional
symbols, and, ﬁnally the Post system related to them, constitute an atomic base for
a language of grounding on the ﬁrst-order logical language in question. The atomic
bases play a twofold role.
The ﬁrst is to provide, through the atomic system, a set of individual constants
for the language of grounding; each closed derivation  in the atomic system will
correspond, in the language of grounding, to one and only one individual constant δ ,
“canonical name” of . The individual constants are some of the terms with minimal
complexity, and therefore act as a base for the construction of more complex terms.
Also, they denote by deﬁnition grounds for the atomic formulas on which they
are typed. Secondly, through the atomic bases, it should be possible to determine
the intended meaning of individual constants, relational symbols and functional
symbols, and consequently, of the terms and formulas of the background language.
Obviously this is essential when we want to know what the grounds or the operations
on grounds denoted by the terms of the language of grounding are grounds or
operations on grounds for. The question is now how this determination of meaning
is established.
Once again, the atomic system is central. In fact, by providing rules that involve
individual constants, relational symbols and functional symbols, it sets the behavior
of these symbols and, depending on the role they play in the atomic derivations,
permits us to consider them as well-deﬁned objects. Let us take into account the
following rules of a Post system for ﬁrst-order arithmetic:
0 = s(t) (s1)
⊥
(+1)
t + 0 = t
(·1)
t · 0 = 0
Assuming we know the meaning of the functional symbols s (successor function),
+ (addition) and · (multiplication), they tell us that the individual constant 0 of a
ﬁrst-order arithmetic language behaves in such a way that: 0 is the successor of no
term, the result of the addition of a term to 0 is the term itself, and ﬁnally the result
of the product of each term for 0 is 0. The rules allow us to consider the individual
constant 0 as the zero of natural numbers.

144
5
Languages of Grounding
It is however clear that such a determination of meaning requires that the rules of
the atomic system enjoy some structural properties. The meaning of a symbol will
be ﬁxed by a certain set of rules, and each of these rules must plausibly concern
this symbol. This remark raises numerous and difﬁcult questions. What is meant by
“concern”? Under what conditions and in what way does a group of rules concerning
a symbol determine its meaning? Is it possible to say that some rules concern a
symbol in a more direct way than others, so as to contribute more signiﬁcantly to
the determination of the meaning of the symbol? Will a rule concerning a certain
symbol be able to concern others, and if so—as seems to be the case, observing
that the rules for 0 set out above also concern the symbols s, + and · - what is the
relationships between the rules concerning the symbol and those that concern the
other symbols involved? An answer to questions like these, even if only partially
exhaustive, would need a discussion to itself, and in fact there is a whole theory in
literature designed to clarify, resolve and articulate on such issues; it is the theory of
meaning based on the notion of an immediate argumental role developed by Cozzo
ﬁrst in Teoria del signiﬁcato and ﬁlosoﬁa della logica (Cozzo, 1994b) and then, with
signiﬁcant differences, in Meaning and argument. A theory of meaning centered on
immediate argumental role (Cozzo, 1994a). In what follows, we will put aside these
problems, assuming as primitive the thorny notion of rule for a symbol; it will be
tacitly understood to be as proposed by Cozzo (especially in Cozzo, 1994a).
5.1.2.2
Expanding a Language
A language of grounding involves terms built on an alphabet containing—in addi-
tion to ground-variables and individual constants—a limited group of operational
symbols designed to denote operations on grounds, of which the operational type
has a well-deﬁned structure. However by explicit indication of Prawitz (Prawitz,
2015), a language of grounding must be conceived as open, where “open” means
that new operational symbols, for new operations on grounds, can be indeﬁnitely
added to it. As we have already argued, this request has a double motivation.
Operations on grounds are associated with inferential acts. Obviously, inferential
activity is not limited to a set of ﬁnitely axiomatizable rules; if one of the aspects
of the deductive practice—the conditions for judging or asserting correctly—can
be fully described in terms of a speciﬁable number of introduction rules, the other
pole—the correct consequences of judgments or assertions—concerns rules in a
non-introductory form, and is on the contrary totally open. Therefore, there will
be no ﬁnitely axiomatizable set operations on grounds—and operational symbols
expressing them—able to capture the deductive practice as a whole. An obvious
objection could be that a ﬁnitely axiomatizable set of operations on grounds—and of
symbols expressing them—could be complete, in the sense of allowing the deﬁnition
of all the possible operations on grounds on a given domain. However, even if it is
true that some languages of grounding could be complete with respect to certain
domains, from Gödel’s incompleteness theorems we know that this can never hold
for any domain. Therefore, once again, if the target of the theory of grounds must

5.1
General Overview
145
be the explanation of the deduction in its entirety, no closed language of grounding
can be said satisfactory.
To account for the character of openness, we will adopt the following strategy:
ﬁrst we will deﬁne a general notion of language of grounding, and then a notion
of expansion of a language of grounding, so as to identify a class of languages of
grounding. Although each element of the class is a closed language of grounding,
the whole class can in a sense be intended as an open language of grounding.
A leading idea for the discussion of languages of grounding and their expansions
concerns the conditions that need to be met for a certain formal apparatus to count as
a language of grounding. Our principle will simply be the following: in a language
of grounding, each operational symbol F must be associated with an operational
type
α1, . . . , αn ▷β
and bind individual and ground-variables such that, if i is the set of the ground-
variables bound by F on index i, there is an operation on grounds f of operational
type
(1 ▷α1, . . . , n ▷αn) ▷β
such that x is bound by F on index j if, and only if, x is bound by f on index j,
and where i ▷αi is simply αi if i = ∅(i, j ≤n).
The obvious consequence of this requirement is that it becomes possible to
prove that all terms of a language of grounding denote a ground or an operation
on grounds. One may wonder whether this is desirable, or even plausible.
A possible answer is simply that a language of grounding is, in fact, a language
of grounding, and it would no longer be such if some of its terms did not denote.
The interest of a language of grounding, in the general framework proposed by the
theory of grounds, is to have a formal tool for denoting grounds and describing the
ways in which they can be obtained.
Nonetheless, the alternative—languages of grounding in which some operational
symbols cannot denote any operation on grounds and thus where there are terms that
do not denote either grounds or operations on grounds—would still carry theoretical
insight. In languages like these, one could study under what conditions a term does
not denote, and under what conditions it denotes despite involving non-denoting
subterms. Indeed, the very deﬁnition of the notion of denotation would take a
signiﬁcant speciﬁc weight: it must take into account the general intuition according
to which a term does not denote when its operational symbols are combined in such
a way that the computation of the operations to which these symbols correspond
does not end in a canonical form. But if the type of an operational symbol is
not inhabited by any operation on grounds, the operation to which the symbol

146
5
Languages of Grounding
corresponds must be set by a “defective” equation; consequently, if the computation
of the combination does not end in a canonical form, it diverges into an inﬁnite chain
f1(x) = f2(y) = . . . = fn(z) = . . .
or into a loop
f1(x) = f2(y) = . . . = fn(z) = f1(x) = f2(y) = . . .
This opens the way to many interesting questions—for instance, which oper-
ational types admit “non-defective” equations for the respective operations on
grounds? According to what criteria and how can we determine if an equation for
the deﬁnition of an operation is “defective” or not? Is it possible that an equation
is “defective”, if it uses in its deﬁniens only operations whose equations, on the
contrary, are not? If an operational type is not inhabited by any operation, is it
possible to link it with a contextual equation in which deﬁniendum and deﬁniens
have the same type, and the deﬁniendum does not involve more variables than the
deﬁniens? Or must we resort to explicit deﬁnitions, which involve other operations
whose equations are “defective”? More in general, what does it mean for an equation
to be “defective”? And what is the semantic domain of a language of grounding that
contains non-denoting operational symbols and terms?
An approach that authorizes the introduction of non-denoting expressions might
be fruitfully linked to some research sectors in contemporary mathematical logic.
Think of Dana Scott’s computation theory which, as denotational semantics for
programming languages, considers interpretations on partially ordered domains,
so as to account for divergent programs, or whose denotation is only approximate
(Scott, 1970; Cardone, 2017); or of the research ﬁeld that Neil Tennant inaugurated
with the proof-theoretical treatment of paradoxes, focusing on the links between
paradoxical rules and looping reductions (Tennant (1982, 1995, 2016), although
the suggestion can be already found in Prawitz (2006); Tennant’s approach and
conclusions have been expanded or developed by several scholars—among them,
for example, Tranchini (2016, 2019)—and often criticized or denounced as partial
by many others—see, for example, Petrolo and Pistone (2019)). However, in the
course of our work, we will not deal with such an approach.
Before moving on to the next section, we would like to make a ﬁnal remark. So
far, we have only talked about terms of a language of grounding but, if a language
of grounding must be actually conﬁgured as a language, it seems natural to require
that it also involves formulas, and therefore predicates for the formation of atomic
formulas and logical constants. In this chapter, we will consider only the terms of a
language of grounding;the complete version of these languages, with an explanation
of the properties their formulas express, will be discussed in Chap. 6.

5.1
General Overview
147
5.1.2.3
Primitiveness and Conservativity
In light of the deﬁnitions we will provide, there will essentially be two ways to
expand a language of grounding on a base B1:
•
by adding new individual constants for atomic derivations in a proper expansion
B2 of B1, or
•
by adding new operational symbols for non-primitive operations on ground.
Obviously, the two ways can be combined, in the sense that an expansion of a
grounding language may contain new individual constants and new operational
symbols.
As regards the ﬁrst kind of expansion, it consists de facto in an expansion of the
atomic base. Since an atomic base is nothing but a quadruple relative to a ﬁrst-order
language, a proper expansion of an atomic base will be an atomic base in which
one of the elements is a superset of the corresponding element of the unexpanded
base. Thus, the Post system of the base B2 contains the Post system of the base B1
as its own subsystem. If the other elements of B2 are unchanged compared to the
corresponding elements of B1, the Post system of B2 does nothing but add new rules
for the elements of the same alphabet on which the Post system of B1 already act;
then, the language of grounding and its expansion will have the same background
language. However, it is not obviously necessary that this is the case. B2, in fact,
could contain new individual constants, new relational symbols or new functional
symbols compared to those available in B1, and its Post system could provide rules
for the new symbols; in this case, the background language of the expansion will be
an expansion of the background language of the unexpanded language of grounding.
As we have said, an atomic base is intended to ﬁx the meaning of some of the
elements of the background language alphabet. A change of base therefore implies
a change of meaning. Hence, when we expand a language of grounding according to
the ﬁrst of the aforementioned modes, we add new primitive elements with respect
to the denotation of grounds and operation on grounds; an individual constant, as
said, will by deﬁnition denote a ground for the (atomic) formula on which it is typed.
Accordingly, we will call primitive an expansion thus obtained.
At this point, we might consider another type of primitive expansion: adding to
a language of grounding operational symbols for primitive operations on grounds
different from those related to ∧, ∨, →, ∀and ∃- namely, operational symbols
for operations involved in the deﬁnition of the concept of ground for categorical
judgments or assertions of formulas the main logical constant of which is not any
of the constants ∧, ∨, →, ∀and ∃. It is not difﬁcult to realize, however, that
contemplating such an expansion requires changing the set of logical constants
of the background language. Apart from a quick example, we will not deal with
expansions of this kind; in other words, our analysis will be limited to ﬁrst-order
logic, which implies the aforementioned restriction that a background language is
always only a ﬁrst-order logical language. There are essentially two reasons for this
choice.

148
5
Languages of Grounding
First, there is little of interest the addition of non-modal logical constants that
do not involve a passage to orders higher than the ﬁrst—think of the constant ↔,
or of Sheffer’s constant (Sheffer, 1913) | and of its dual ↓; Prawitz himself, in
fact, proved that the ﬁrst-order logical constants are functionally complete (Prawitz,
1979; see also Schroeder-Heister, 1984). Second, we know that the addition of
second-order quantiﬁers leads to many difﬁculties in the semantic frameworks of
constructivist, or more generically veriﬁcationist, inspiration; the breach of the
Fregean principle of compositionality goes together with reliable phenomena of
unpredicativity that require ad hoc treatments, or substantial changes that avoid the
emergence of paradoxes (see, among others, Pistone, 2015). The problem already
concerns a formulation of the clauses for determining what counts as ground
for categorical judgments or assertions of second-order quantiﬁed formulas, and
therefore the primitive operations on grounds for second-order quantiﬁers; actually,
it is reasonable to expect that we ﬁnd here the same phenomenon we have with the
introduction rules for these quantiﬁers. We can borrow Cozzo’s words, who notes
how
the introduction rule for the second-order existential quantiﬁer has the form:
α(T )
∃Xα(X)
where T is a predicative (second-order) term that may contain the formula ∃Xα(X) as a
part [. . . ] the premise α(T ) could be much more complex than the conclusion ∃Xα(X) and
the rule violates the molecularity requirement. (Cozzo, 1994b, 113)
Likewise, the clause ﬁxing what counts as ground for ∃Xα(X) should appeal to a
primitive operation, that we can indicate with ∃2I, saying that ∃2I(g) is a ground
for ⊢∃Xα(X) if, and only if, g is a ground for α(T ); but T could contain ∃Xα(X)
as its subformula, and this could make the expected inductive nature of the clauses
problematic.1
If an expansion obtained by adding individual constants is primitive, an expan-
sion obtained by adding operational symbols for non-primitive operations on
grounds can obviously be called non-primitive. A non-primitive operation on
grounds, in fact, will have to be ﬁxed by one or more equations by virtue of which,
whenever applied to grounds for the types of its domain, the operation returns a
ground for the type of its codomain; but the notion of ground is ﬁxed in terms of
primitive operations, so a non-primitive operation is “harmonic” with respect to the
determination of the meaning that these primitive operations convey. Similarly, the
1 The restriction to the ﬁrst-order does not disregard the great interest of an approach that includes
expansions of languages of grounding with operational symbols for primitive operations related to
logical constants of higher order than the ﬁrst. This is a point that we will leave out for mere
reasons of space, and that could be fruitfully developed later. In a sense, our discussion will
be equivalent to a formal framework for the analysis of ﬁrst-order inferential validity; hence,
to authorize expansions with operational symbols for primitive operations on grounds would be
equivalent to a formal framework for the analysis of inferential validity of n-th order—namely of
inferential validity as such.

5.1
General Overview
149
terms of an expansion obtained by adding operational symbols for non-primitive
operations on grounds will not contain new primitive concepts compared to the
unexpanded language of grounding, but they will denote on the basis of a notion
of ground—and therefore of a determination of meaning—already available before
the expansion.
Together with the distinction between primitive and non-primitive expansions,
we introduce another classiﬁcation: conservative and non-conservative expansions.
The basic idea is the following. Let 1 be a grounding language on a base B on L,
and let
Gr1
B = {g | g is a ground over B denoted by some term of 1}
Gr2
L = {g | g is a ground with type in L denoted by some term of 2}
Since 2 is an expansion of 1, we will have that Gr1
B ⊆Gr2
L . If the inclusion is
strict, that is in the case of Gr1
B ⊂Gr2
L , we will say that 2 is a non-conservative
expansion of 1; when the two sets are equal, namely in the case of Gr1
B = Gr2
L ,
we will say that 2 is a conservative expansion of 1.
In other words, an expansion is non-conservative if, and only if, it contains terms
that denote grounds based on the unexpanded language of grounding that were not
denoted by any term in the unexpanded language of grounding; an expansion is non-
conservative if, and only if, it allows us to express new grounds on the same base
as the language of grounding that it enhances. The expansion, on the contrary, is
conservative if, and only if, all the grounds that it allows us to express on the old
base were already expressible in the unexpanded language of grounding—i.e. if all
the objects that its terms “name”, were already “nameable” in the old language of
grounding. Obviously, the notion is relativized to a ﬁxed denotation function.
It should be noted that the kind of conservativeness we are dealing with here
is signiﬁcantly different from the usual notion of conservativity of formal systems.
Given a formal system  on a language L, and given an expansion + of , we
say that + is conservative on  if, and only if, for each ﬁnite set  of formulas
of L, for each formula α of L, if  ⊢+ α, then  ⊢ α. In terms of languages of
grounding, this would amount to requiring that, given a language of grounding 1
on a base B, and given an expansion 2 of 1, 2 is conservative on 1 if, and only
if, for each α1, . . . , αn ⊢β on B, if there exists a term of 2 denoting a ground g1
on B for α1, . . . , αn ⊢β, then there exists a term of 1 denoting a ground g2 on B
for α1, . . . , αn ⊢β. This type of conservativity does not clearly imply that g1 = g2;
the kind of conservativity we outlined, however, requires exactly that the class of
grounds on B denoted by the terms of 2 be identical to the class of grounds on B
denoted by the terms of 1. The second type of conservativeness clearly implies the
ﬁrst, but the opposite does not always apply.
In other words, it is not a conservativity of provability, but a conservativity,
so to speak, of denotation. If, as a mere way of example, we understand a term
as a proof, and a ground as the canonical form to which this proof reduces—

150
5
Languages of Grounding
if the term is closed—or as the method that this proof denotes—if the term is
open—the conservativity proposed here corresponds to the idea that a conservative
expansion of a language of grounding adds names for proof-methods that were
already available in the unexpanded language, but does not involve a substantial
increase in the deductive power of the language itself. On the contrary, in a non-
conservative expansion, we dispose of new proof-methods.
5.2
A Class of Languages
Let us now begin the delineation of our class of grounding language. We will
proceed in the following order: ﬁrst, we will deﬁne a background language and
a notion of base on a background language; then we will discuss the notion
of operational type and different notions of operations on grounds of different
operational types; ﬁnally, we will provide the clauses which ﬁx the notion of ground
for categorical judgments or assertions for formulas of different logical kinds. Then,
we will deﬁne the notions of language of grounding, and of expansion of a language
of grounding, followed by the deﬁnition of the notion of denotation, and the proof
of some related results. We will conclude with the distinction between primitive and
non-primitive, conservative and non-conservative expansions.
5.2.1
Background Language and Bases
The background languages we consider here will always be ﬁrst-order logical
languages of the kind previously used in our investigation.
Deﬁnition 12 A ﬁrst-order logical language L is characterised as usual by an
alphabet, and by sets of terms and formulas deﬁned recursively starting from that
alphabet. The alphabet ALL consists of:
•
individual variables xi (i ∈N)
•
relational symbols P n
j (n, j ∈N)
•
functional symbols φm
h (m, h ∈N)
•
individual constants ∥ci∥(i ∈N)
•
logical constants ∧, ∨, →, ∀, ∃, ⊥
•
brackets and commas as auxiliary symbols
The set TERML of the terms of L is the smallest set X such that
•
xi ∈X (i ∈N)
•
∥ci∥∈X (i ∈I)
•
t1, . . . , tm ∈X ⇒φm
h (t1, . . . , tm) ∈X

5.2
A Class of Languages
151
The set FORML of the formulas of L is the smallest set X such that
•
⊥∈X
•
t1, . . . , tn ∈TERML ⇒P n
j (t1, . . . , tn) ∈X
[These ﬁrst two clauses deﬁne the set of the atomic formulas of L, indicated with
ATOML]
•
α, β ∈X ⇒α ⋆β ∈X (where ⋆is one of the symbols ∧, ∨, →, which we will
indicate more brieﬂy with ⋆= ∧, ∨, →)
•
α ∈X ⇒⋆xi α ∈X (⋆= ∀, ∃, i ∈N)
Negation is deﬁned by setting
¬α
def
= α →⊥
so it is not a primitive symbol of the language.
When this does not generate ambiguity, we will omit indices and subscripts to
lighten up the notation.
We consider as deﬁned in a usual way the following standard notions:
•
set FV (t) of the free variables of t;
•
set FV (α) of the free variables of α - we set FV () = {FV (β) | β ∈};
•
substitution t[s/x] of x with s in t;
•
substitution α[t/x] of x with t in α;
•
substitution α[γ/β] of atomic β with γ in α;
•
t free for x in α;
•
γ free for atomic β in α.
From this point on, we will assume, as usual, that individual substitutions can be
generalised to simultaneous n-ary substitutions, and that all substitutional items are
free in terms and formulas for the items they replace.
Deﬁnition 13 Given a ﬁrst-order logical language L1, an expansion of L1 is a ﬁrst-
order logical language L2 such that TERML1 ⊆TERML2 and FORML1 ⊆FORML2.
To explain the notion of base, we ﬁrst need a preliminary notion of atomic
system, and a clariﬁcation of the relationship between an atomic system and the
language on which it acts. The atomic systems of interest here are always Post
systems—namely pairs ⟨L, ℜ⟩, with L a ﬁrst-order logical language and ℜa ﬁnitely
axiomatizable set of rules
α1
. . .
αn
β

152
5
Languages of Grounding
over individual constants, relational symbols or functional symbols of L and such
that:
•
n ≥0 (if n = 0 the rule is an axiom);
•
for every i ≤n, αi ∈ATOML and αi ̸= ⊥;
•
β ∈ATOML or β = ⊥and, if x ∈FV (β), there is i ≤n such that x ∈FV (αi).
Note that the possibility of the rules of a Post system binding variables or
assumptions is left open. Now, as previously mentioned, the idea is that the rules
of a Post system determine the meaning of the elements of the alphabet of the
background language. For instance, the Post system for ﬁrst-order arithmetic, which
we outlined in Chap. 3, consists of rules by virtue of which we can interpret: the
constant 0 as the zero of the natural numbers; the symbol s as the successor function
on natural numbers; the symbol + as addition on natural numbers; the symbol ·
as multiplication on natural numbers; the symbol = as the relation of equality on
natural numbers. With this in mind, we require the following deﬁnition.
Deﬁnition 14 An atomic system S = ⟨L1, ℜ⟩is an atomic system for L2 if, and
only if, L2 is an expansion of L1. S totally interprets L2 if, and only if, L1 = L2.
Otherwise, S partially interprets L2.
If S is a system for L, its rules are expected to determine the meaning of the elements
of the alphabet of L. In other words, they are rules for such symbols. As mentioned
previously, the notion of “rule for a symbol” will be considered as primitive; a
detailed development of this can be found in Cozzo’s theory of meaning (Cozzo,
1994a,b). As a limit case, we have the system ⟨∅, ∅⟩, which we name the empty
system. Whatever L, the empty system is a system for L.
Deﬁnition 15 Given an atomic system S1 = ⟨L1, ℜ1⟩, an expansion of S1 is an
atomic system S2 = ⟨L2, ℜ2⟩with L2 = L1 and ℜ1 ⊆ℜ2, or with L2 proper
expansion of L1 and ℜ1 ⊂ℜ2.
Deﬁnition 16 Given a ﬁrst-order logical language L, we indicate: with R the set of
the relational symbols of L; with F the set of the functional symbols of L; and with
C the set of the individual constants of L. Finally, let S be an atomic system for L.
We will say that the quadruple
⟨R, F, C, S⟩
is an atomic base on L. We will call L the background language of the base. A base
whose atomic system is empty will be called logical.
For simplicity, we will assume the following convention.
Convention 17 Given a base B on L, if B is not logical, S totally interprets L.
Deﬁnition 18 Let B be a base on L1 with atomic system S1. We will say that a
base on L2, for L2 expansion of L1, with atomic system S2 expansion of S1, is an
expansion of B.

5.2
A Class of Languages
153
By Convention 17, given a real expansion of R, F, or C, and given the atomic system
S of B is not empty, also S is expanded.
5.2.2
A Universe of (Operations on) Grounds
In this section, we introduce the notion of operation on grounds and, subsequently,
the deﬁnition of the notion of ground for categorical judgments or assertions. To
make this possible, we need a preliminary notion of operational type, and we need
to specify some further concepts such as those of deﬁning equations, of composition
of operations on grounds, and of equivalence and identity between (operations on)
grounds.
5.2.2.1
Operational Types
As previously mentioned, an operation on grounds is a total constructive function
with a certain domain and a certain codomain; domain and codomain are nothing
other than classes of individuals or (operations on) grounds for certain judgments or
for certain assertions on a certain background language. When applied to an element
of the domain, then, the operation on grounds produces a ground for the judgment
or assertion that constitutes the codomain. An operation on grounds will therefore
be identiﬁed by two parameters: domain and codomain, and an equation indicating
how the operation transforms each element of the domain into an element of the
codomain. For the ﬁrst parameter, we shall employ the expression operational type.
The main purpose of the notion of operational type will be to classify the possible
operations on grounds according to the lines just indicated, but we will also use it
for another purpose. When we introduce the languages of grounding, in fact, all
elements of the alphabet, and all terms built starting from these elements will have
to have a certain type. Now, for the ground-variables,the individual constants and all
terms, a type is nothing but a formula of the background language. But the alphabet
of a language of grounding also includes operational symbols, the type of which
will be, more speciﬁcally, an operational type. The reason for this is simple: an
operational symbol allows the creation of a term of a certain type starting from terms
of another type, and the operational type assigned to it has exactly the purpose of
making this circumstance explicit.
Deﬁnition 19 Let L be a ﬁrst-order logical language. A pre-type on L is any α ∈
FORML, or an expression
 ▷α
for ﬁnite  ⊂FORML and α ∈FORML. We call  the domain of the pre-type, α the
co-domain of the pre-type. Given a sequence of pre-types, its domain is the union

154
5
Languages of Grounding
set of the domains of its elements. An operational type on L is either a pre-type on
L - for closed α ∈FORML we call it simply a type on L - or an expression of the
form
τ1, . . . , τn ▷τn+1
where τi (i ≤n + 1) is a pre-type and the domain of τn+1 is a subset of the domain
of τ1, . . . , τn. We say that τ1, . . . , τn is the domain of the operational type, and that
τn+1 is the co-domain of the operational type. We say that each τi (i ≤n) is an entry
of the domain operational type.
As a notational convention, if the domain of an operational type is non-empty, and
some entry of the domain or the co-domain have non-empty domain, we put domain
and co-domain between parentheses. We ﬁnally remark that, in light of deﬁnition 9,
expressions such as the following are not operational types: (▷α)▷β, (▷α)▷(▷β),
( ▷α) ▷(▷β), or ▷β. As generic examples, the following are all operational
types:
•
α(x1, . . . , xn)
•
(α1(x1, . . . , xn) ▷β1(y1, . . . , ym), α2 ▷β2) ▷δ(z1, . . . , zp)
•
(α1(x1, . . . , xn) ▷β1(y1, . . . , ym), α2 ▷β2) ▷(α2 ▷δ(z1, . . . , zp))
as are, once again, with reference to a language for ﬁrst-order arithmetic, the
following:
•
∀x(0 ≥x)
•
∀x(x ≤y ∧y ≤z), 94 = s(s(0)) ▷∀w(w = s(s(s(
√
64)))
•
(∃x(x ≤y), 0 ≥25, 0 = s(s(0)) ▷0 = 1) ▷(0 = s(s(x)) ▷∀y(y ̸= 7!))
5.2.2.2
Forms of Judgments and Assertions
A judgment can be understood as a mental act where one claims a proposition to
be true. Just as sentences can be seen as the linguistic counterpart of propositions,
assertions can be taken to be the linguistic counterpart of judgments. Thus, an
assertion is a linguistic act where one claims a sentence to be true. However, this
does not mean that judgments or assertions are to be of the form “. . . is true”,
where the dots are replaced by (a name of) a proposition or sentence. The claim of
truth is implicit; judgments or assertions just employ propositions or sentences in a
certain mood, or with a certain force.
Judgments or assertions of propositions or sentences are said by Prawitz to be
categorical. To indicate them, we can use the Fregean (meta-linguistic) turnstile—
i.e., ⊢α, for α closed. Prawitz also contemplates other forms of judgments or
assertions: general judgments or assertions, i.e., judgments or assertions involving
open formulas, noted
⊢α(x1, . . . , xn);

5.2
A Class of Languages
155
hypothetical judgments or assertions, i.e., judgments or assertions involving a
proposition or sentence depending on other propositions or sentences, noted
α1, . . . , αn ⊢β;
and general-hypothetical judgments or assertions, i.e., judgments or assertions
involving a possibly open formula that depends on other possibly open formulas,
noted
α1(x1), . . . , αn(xn) ⊢β(x),
with xi and x sequences of the variables occurring free in αi and β, respectively
(i ≤n), and where at least one of these sequences is not empty.
We call of ﬁrst level the judgments or assertions illustrated thus far. Then, we can
move to judgments or assertions of the second level. They can again be hypothetical
or general-hypothetical, and involve a possibly open formula, a hypothetical or a
general-hypothetical judgment or assertion of ﬁrst level depending on at least one
hypothetical or general-hypotheticaljudgment or assertion of ﬁrst level—and maybe
other possibly open formulas. The notation is in this case
τ1, . . . , τn ⊢τn+1
where τi and τn+1 are possibly open formulas, or general or general-hypothetical
judgments or assertions of ﬁrst level, and where we have the following restrictions:
there is at least one τi hypothetical or general-hypothetical of ﬁrst level; if τn+1 is
hypothetical or general-hypothetical of ﬁrst level, what is on the left of its turnstile
is contained in or equal to the union set of what is on the left of the turnstile of the
hypothetical or general-hypothetical τi-s of ﬁrst level (i ≤n).
Apart from categorical and general judgments or assertions, it easy to see that the
notations for the different forms of judgments or assertions run parallel to those for
the operational types—replacing ▷with ⊢. That is indeed no coincidence for, as we
shall see in the next section, the operational type of an object depends on the form
of the judgment or assertion it is a ground for.
5.2.2.3
First- and Second-Level Operations
Operations on grounds can be of ﬁrst or of second level. In operations on grounds
of ﬁrst level, the domain of the operational type only consists of possibly open
formulas, or else it is empty; in operations on grounds of second level, the domain
of the operational type contains at least one entry of the form  ▷α. The deﬁnition
of operations on grounds of second level presupposes the notion of ground for
categorical judgments or assertions, as well as that of operations on grounds of
ﬁrst level. In turn, operations on grounds of ﬁrst level are distinguished according
to whether their domain is empty or not. The deﬁnition of operations on grounds

156
5
Languages of Grounding
of ﬁrst level with operational type with non-empty domain presupposes the notion
of ground for categorical judgments or assertions, as well as that of operations on
grounds of ﬁrst level with operational type with empty domain. So, we will start
from this latter kind of operations. But ﬁrst, a preliminary remark. Operations on
grounds will be indicated by a notation of the form
f (x1, . . . , xn, ξτ1, . . . , ξτm)
for n, m ≥0, where τi is a possibly open formula, or a hypothetical or general-
hypothetical judgment or assertion (i ≤m). So, the operation at issue is deﬁned on n
individuals k1, . . . , kn, and on grounds for τi[k1, . . . , kn/x1, . . . , xn]—plus possibly
other replacements with arbitrary individuals of unbound variables occurring in τi
other than x1, . . . , xn (i ≤m).
Given an atomic base B on a ﬁrst-order language L, assume that B identiﬁes a
domain DB. A B-operation on grounds of operational type
α(x1, . . . , xn)
with n ≥0, is a total constructive function
f (x1, . . . , xm)
with m ≥n and m > 0 such that, for every k1, . . . , km ∈DB,
f (k1, . . . , km)
is a ground for
⊢α(k1, . . . , km/x1, . . . , xn)
What is replaced in the formula must be a name of an individual, but we will not
indicate this neither here nor in what follows. The operation is a ground on B for
the corresponding general judgment or assertion, proper if m = n, and improper
otherwise. As an example, think of the following reasoning:
1. x > y, assumption
2. y > z, assumption
3. hence x > z, from points 1 and 2, > being transitive
4. hence, y > z →x > z, from 3, introducing implication on 1
5. hence, x > y →(y > z →x > z), from 4, introducing implication on 2
6. w = w →(x > y →(y > z →x > z)), from 5, introducing implication
7. w = w, = being reﬂexive
8. hence, x > y →(y > z →x > z), from 6 and 7, by modus ponens

5.2
A Class of Languages
157
If in this reasoning we replace x, y, z, w with four natural numbers n, m, p, q, we
obtain a correct reasoning proving n > m →(m > p →n > p). Observe that w
only occurs in a detour.
Before moving on, we need the following notation. x1, . . . , xn+1, x, z and y
indicate sequences of distinct individual variables, where z and y are assumed to
share no individual variables with any xi (i ≤n + 1). The elements of xi are meant
to have an index i, so they are of the form xi (i ≤n + 1). x is a (possibly empty)
sub-sequence of x1, . . . , xn in the sense that all of its elements are of the form xi
for some xi element of xi (i ≤n). xi −x indicates a sequence we obtain from xi
by deleting from it all elements that also occur in x. [x1, . . . , xn+1 −x] y indicates
a sequence we obtain from x1, . . . , xn+1 and y by bringing their elements together
and then deleting: all the elements of x, followed by the same individual variables
occurring with different indices except one; and ﬁnally the indices themselves. k
indicates a sequence of individuals on DB of the same length as [x1, . . . , xn+1 −
x] y. With this established, we say that a B-operation on grounds of operational type
α1(x1), . . . , αn(xn) ▷β(xn+1)
that binds the elements of x and z, is a total constructive function
f ([x1, . . . , xn+1 −x] y, ξα1(x1), . . . , ξαn(xn))
such that, for every k, for every gi ground on B for ⊢αi(xi)[k/xi −x] (i ≤n),
f (k, g1, . . . , gn)
is a ground on B for ⊢β(k/xn+1). If the operation binds xi, we say that it binds
x on i. x may occur in some xj with i ̸= j, without being bound on j (i, j ≤
n). The operation is a ground on B for the corresponding hypothetical or general-
hypothetical judgment or assertion, proper if y is empty, and improper otherwise.
The sequences z and y stand for, respectively, vacuous bindings and individual
variables not occurring in the operational type. On y we have the following
restriction: if, for every i ≤n, the operation binds some x on i, then y is empty. The
reasons for this restriction will be clear after the examples. Consider the following
reasoning:
1. 2 + 2 = 4, assumption
2. 4 = x, assumption
3. hence 2 + 2 = x, from points 1 and 2, = being transitive
If, in this reasoning, we replace x with
√
16, and we replace the assumptions with
computations of 2 + 2 = 4 and of
√
16 = 4, we obtain a correct reasoning proving
2 + 2 =
√
16. Observe that no variable is bound in this case. Now consider ∀-

158
5
Languages of Grounding
introduction in two variables in Gentzen’s ﬁrst-order natural deduction
α(x, y)
(∀I)
∀xα(x, y)
Clearly, we cannot consider this rule as an argument, for otherwise α(x, y) would be
an assumption, and it would violate the restriction on the so-called proper parameter.
But still we can consider it as an operation f (y, ξα(x,y)) that, binding x, yields
grounds for ⊢∀xα(x, k) from grounds for ⊢α(x, k/y), i.e., from operations
on grounds of operational type α(x, k/y). Since α(x, y) cannot be considered an
assumption, no individual variables other than y can be taken into account. This
explains the restriction on y above.
We can now describe B-operations on grounds of second level, although again
we require some notation. ξ 1, . . . , ξ n and ξ indicates sequences of distinct ground-
variables, for ξ sharing no ground-variable with ξi (i ≤n). The elements of ξi have
an index i, so they are of the form ξα
i (i ≤n), for some α formula of the background
language of B. As for sequences of distinct individual variables and of individuals,
we have the same conventional notations as above, except that now we take xi to
consist of two sub-sequences x↑
i and x↓
i - so the elements will be x↑
i and x↓
i (i ≤n).
This is to take into account the fact that the individual variables occurring free in
the i-th entry of the operational type of a B-operation on grounds of second level,
may be either in the domain—indicated with ↑—or in the co-domain of the entry—
indicated with ↓—or in both. Thus, a B-operation on grounds of operational type
τ1, . . . , τn ▷τn+1
that binds the elements of x, z, ξ1, . . . , ξ n and ξ (note that the elements of ξi are
typed on formulas in the domain of τi (i ≤n)), is a total constructive function
f ([x↓
1, . . . , x↓
n −x] y, ξτ1, . . . , ξτn)
such that:
1. for every k, for every gi ground on B for ⊢τi[k/xi −x],
f (k, g1, . . . , gn)
is a ground on B for ⊢τn+1[k/xn+1];
2. let ⊢αi(x↓
i ) be the co-domain of the i-th entry and β(x↓
n+1) the co-domain of
the co-domain, for every k, for every gi ground on B for ⊢αi(x↓
i )[k/x↓
i −x],
f (k, g1, . . . , gn)
is a ground on B for ⊢β(k/x↓
n+1).

5.2
A Class of Languages
159
Ground-variables are bound under replacement of individual variables with k in
the types of the original bound ground-variables. If the operation binds ξα
i , we say
it binds ξα on i. α may occur in a j-th entry with i ̸= j, without ξα bound on
j (i, j ≤n). As for bindings of individual variables, we have the same wording as
above. The operation is a ground on B for the correspondinghypothetical or general-
hypothetical judgment or assertion, proper for y empty, and improper otherwise.
The sequence ξ stand for vacuous bindings of ground-variables. The sequences
z and y account again for, respectively, vacuous bindings of individual variables
and individual variables not occurring in the operational type. On y we have the
same restriction as above. Observe that the operation is deﬁned on individuals that
correspond only to individual variables in the co-domains of the entries. The reasons
for this will be soon clear. A typical example of operation on grounds of second
level is →-introduction in Gentzen’s natural deduction, which we consider here in
the following form:
[α(x)]
...
β(y)
(→I)
α(x) →β(y)
This can be taken to be an operation on grounds
f (x, y, ξα(x)▷β(y))
of operational type
(α(x) ▷β(y)) ▷α(x) →β(y)
binding ξα(x): for every t1, t2, for every ground g for α(t1) ⊢β(t2), or simply for
⊢β(t2), f (t1, t2, g) is a ground for ⊢α(t1) →β(t2), with ξα(t1) bound.
x is mentioned only because α(x) occurs in the co-domain: as we shall see below,
the operation can be applied to operations on grounds h(x, y, ξα(x)) of operational
type α(x) ▷β(y), so that the application will still be deﬁned on individuals
corresponding to x, i.e. f (y, h(x, y, ξα(x))). Moreover, as in case 2, and as we shall
see again below, the operation may be applied simply to grounds for ⊢β(t), for
every t, or to operations on grounds h(y) of operational type β(y), or ﬁnally to
operations on grounds whose operational type involves neither α(x) nor x. So, x
should not be mentioned as referred to the occurrence of α(x) in the domain of the
domain.

160
5
Languages of Grounding
5.2.2.4
Deﬁning Equations
Given that operations on grounds are expected to be constructive functions, they
must be deﬁned by one or more equations, that can explain how operations on
grounds produce given results on given arguments.
The only restriction we will impose on the form of a deﬁning equation is that the
instructions it provides must render total and effective the operation it is intended to
deﬁne. Here, we will limit ourselves to closed formulas. The equations we introduce
for operations on grounds will correspond to Gentzen’s eliminations in ﬁrst-order
natural deduction and to induction in ﬁrst-order arithmetic. For operations that
correspond to Gentzen’s eliminations, we will need to rely upon the deﬁnition of
grounds for closed formulas of different logical kinds that we give below in the
Section on ground-clauses.
•
f∧(ξα1∧α2) of operational type α1 ∧α2 ▷αi deﬁned by the equation
f∧(∧I(g1, g2)) = gi
(i = 1, 2);
•
f∨(ξα1∨α2, ξα1▷β, ξα2▷β) of operational type α1 ∨α2, (α1 ▷β), (α2 ▷β) ▷β
binding ξαi on the i + 1th entry deﬁned by the equation
f∨(∨I[αi ▷α1 ∨α2](g), h1(ξα1), h2(ξα2)) = hi(g)
(i = 1, 2);
•
f→(ξα→β, ξα) of operational type α →β, α ▷β deﬁned by the equation
f→(→Iξα(h(ξα)), g) = h(g);
•
f∀(ξ∀xα(x)) of operational type ∀xα(x) ▷α(k) deﬁned by the equation
f∀(∀Ix(h(x)) = h(k);
•
f∃(ξ∃xα(x), ξα(x)▷β) of operational type ∃xα(x), (α(x) ▷β) ▷β binding x and
ξα(x) on the second entry deﬁned by the equation
f∃(∃I[α(t) ▷∃xα(x)](g), h(x, ξα(x))) = h(t, g);
•
f n
Ind(ξα(0), ξα(x)▷α(s(x))) of operational type α(0), (α(x) ▷α(s(x))) ▷α(n)
binding x and ξα(x) on the second entry deﬁned by the equation
f n
Ind(g, h(x, ξα(x))) =

g
if n = 0
h(n −1, f n−1
Ind (g, h(x, ξα(x)))
if n > 0

5.2
A Class of Languages
161
In the open case, the behavior of the operation is speciﬁed after we have replaced
individual variables with (names of) individuals, and after application to grounds
for formulas that are closed by replacement.
Fundamentally, the symbol = does not indicate a relation between objects in this
semantic universe, e.g. f∧(∧I(g1, g2)) is not an object in this universe. Indeed, the
symbol = could be said to be on a meta-level, simply showing the result obtained
by applying functions to arguments.
It can be shown that, for every atomic base B, the operations f∧—f∃are B-
operations on grounds of the indicated operational type. Let us consider here only
the cases of f∨and a f∃. Beginning with f∨, suppose that G is a proper ground on
B for ⊢α1 ∨α2. Given the ground-clause for ∨later described, G must be of the
form
∨I[αi ▷α1 ∨α2](Gi)
with Gi proper ground on B for ⊢αi (i = 1, 2). Now suppose that hi(ξαi) is a
proper ground on B for αi ⊢β (i = 1, 2). This means that, for every gi that is a
proper ground on B for ⊢αi, hi(gi) is a proper ground on B for ⊢β. But then,
hi(Gi) is a proper ground on B for ⊢β, and since
f∨(∨I[αi ▷α1 ∨α2](Gi), h1(ξα1), h2(ξα2)) = hi(Gi)
our proof is complete. Similarly, for f∃suppose that G is a proper ground on B for
⊢∃xα(x). Given the ground-clause for ∃later described, G must be of the form
∃I(G1)
with G1 a proper ground on B for ⊢α(t), for some term t on the background
language of B. Now, suppose that h(x, ξα(x)) is a proper ground on B for α(x) ⊢β.
This means that for every term u on the background language of B, for every g that
is a proper ground on B for ⊢α(u), h(u, g) is a proper ground on B for ⊢β. But
then, h(t, G1) is a proper ground on B for ⊢β, and since
f∃(∃E(G1), h(x, ξα(x))) = h(t, G1)
we have again shown the result.
We can prove that f n
Ind is a B-operation on grounds of the indicated operational
type, where B is a base for a logical language for ﬁrst-order arithmetic that has
the Post system of Chapter 3 as its atomic system. Let G be a proper ground on B
for ⊢α(0), and let h(x, ξα(x)) be a proper ground on B for α(x) ⊢α(s(x)). The
second supposition implies that, for every term u on the background language of B,
for every proper ground g on B for ⊢α(u), h(u, g) is a proper ground on B for
⊢α(s(u)). We can now consider two separate cases: if n = 0,
f 0
Ind(G, h(x, ξα(x))) = G

162
5
Languages of Grounding
and we have ﬁnished; if n > 0, by (meta)inductive hypothesis, we know that
f n−1
Ind (G, h(x, ξα(x)))
is a proper ground G1 on B for ⊢α(n −1), and thus h(n −1, G1) will be a proper
ground on B for ⊢α(n).
5.2.2.5
Composition and Restrictions
Let us now introduce the concept of composition of operations on grounds, as
well as some restrictions on these operations. The restrictions we will discuss are
a generalization—mutatis mutandis—of Gentzen’s restrictions for the quantiﬁer
rules.
Regarding composition, an operation on grounds can be plugged into another
operation on grounds under the condition that the operational type of the former
has (co-domain of the) co-domain identical to (the co-domain of) one of the
entries of the operational type of the latter. To be precise, given a base B, and
hi(xi, ξτ i
1, . . . , ξτ i
mi ) ground on B for τ 1
i , . . . , τ i
mi ⊢τ i
mi+1, where τ i
mi+1 is αi if
τ i
m+1 has empty domain, or has co-domain αi otherwise, and given a B-operation
on grounds f (x, ξτ1, . . . , ξτn) of operational type τ1, . . . , τn ▷τn+1, where τi is αi
if τi has empty domain, or it has co-domain αi otherwise, we say that
f (x, . . . , hi(xi, ξτ i
1, . . . , ξτ imi ), . . . )
is a composition of
f (x, ξτ1, . . . , ξτn) with hi(xi, ξτ i
1, . . . , ξτ i
mi )
—we will henceforth speak of composition of f with hi, or of hi being plugged
into f on index i (i ≤n). Naturally, if f binds x on index i, and if x is an element
of xi, x is considered to be bound on i in the composition of f with hi; similarly,
if f binds ξα on index i, then every ξτ i
j = ξα (j ≤mi) should be considered to be
bound on index i in the composition of f with hi.
It is in general not immediate that the composition of grounds yields grounds.
To ensure this, some restrictions need to be introduced, on operations on grounds
and on the composition of these operations. Thus, for every base B and every B-
operation on grounds f of operational type τ1, . . . , τn ▷τn+1, the following two
restrictions must hold:
restriction 1 —if, for some i ≤n, τi has empty domain and is of the form
. . . ∀x . . . α(x, t/y)

5.2
A Class of Languages
163
or it has non-empty domain, and its co-domain has such form, and if τn+1 has empty
domain and is of the form
. . . ∃y . . . β(u/x, y)
or it has non-empty domain, and its co-domain has such form, then
•
t is free for y in . . . ∀x . . . α(x, y) and
•
u is free for x in . . . ∃y . . . β(x, y);
restriction 2 —if f binds x on index i, then
•
every ground hi on B for τ i
1, . . . τ i
m+1 ⊢τ i
m+1 plugged into f on index i must
be such that, for every j ≤mi, if τ i
j has non-empty domain  then, for every
β ∈, x occurs free in β if, and only if, hi binds ξβ on index i
j, or hi is in turn
a composed operation and x is bound on index i
j by some operation which hi is
composed of, or f binds ξβ on index i—naturally, if τ i
j ha empty domain, then
it may taken to be simply β - and
•
x does not occur free in τn+1, if τn+1 has empty domain, or in the co-domain of
τn+1, if τn+1 has non-empty domain.
The following proposition establishes a type of closure property of operations under
plugging. The proof, which is simple but tedious, is omitted.
Proposition 20 Let B be an atomic base, and let f (x, ξτ1, . . . , ξτn) be a B-
operation on grounds of operational type τ1, . . . , τn ▷τn+1. Then, for every
hi(xi, ξτ i
1, . . . , ξτ i
mi ) ground on B for τ i
1, . . . τ i
m+1 ⊢τ i
m+1 (i ≤n), the composition
of f with hi is a ground on B for
. . . {τ i
1, . . . , τ i
mi} −νi · · · ⊢(. . . (({σ i
s , . . . , σ i
t } −{νi
s, . . . , νi
t }) −νi) · · · ⊢β)
where
•
νi is the set of the types of the ground-variables bound by f on index i;
•
{νi
s, . . . , νi
t } is the set of the types of the ground-variables bound by hi on indices
i
s, . . . ,i
t (s, . . . , t ≤mi)
•
{σ i
s , . . . , σ i
t } is the the set of the domains of the entries of the operational type of
hi with non-empty domain (s, . . . , t ≤mi);
•
β is τn+1, if τn+1 has empty domain, or it is the co-domain of τn+1, if τn+1 has
non-empty domain.
Moreover, if the composition of f with hi is deﬁned on v ≥0 individuals, for any
sequence k of individuals of length w ≤v, the composition of f with hi applied to
k is a ground on B for the same judgment or assertion as above, with appropriate
replacements.

164
5
Languages of Grounding
5.2.2.6
Grounds
We can now introduce (proper) grounds for categorical judgments or assertions. To
do this, we use clauses where primitive operations are assumed, one for each ﬁrst-
order logical constant. Primitive operations do not require deﬁnition, because the
clauses ensure they are meaning-constitutive. Their operational types shall be left
implicit; where it is necessary to indicate them, we will do so in square brackets
after the symbol for the operation. Furthermore, we will indicate any primitive
operations that bind individual or ground-variables by accompanying the symbol for
the operation with the variable that is bound. Every formula in the clauses is to be
understood as closed, or as open but then closed with adequate bindings. Therefore,
given an atomic base B on a ﬁrst-order language L with atomic system S, we say
that:
(At)
a ground on B for ⊢α with α ∈ATOML is any derivation in S with no
undischarged assumption or unbound individual variables;
(∧)
g1 is a ground on B for ⊢α and g2 is a ground on B for ⊢β if, and only if,
∧I(g1, g2) is a ground on B for ⊢α ∧β;
(∨)
g is a ground on B for ⊢αi if, and only if, ∨I[αi ▷α1 ∨α2](g) is a ground
on B for ⊢α1 ∨α2 (i = 1, 2);
(→)
f (ξα) is a ground on B for α ⊢β if, and only if, →Iξα(f (ξα)) is a ground
on B for ⊢α →β;
(∃)
g is a ground on B for ⊢α(t) if, and only if, ∃I[α(t)▷∃xα(x)](g) is a ground
on B for ⊢∃xα(x);
(∀)
f (x) is a ground on B for α(x) if, and only if, ∀Ixα(x) is a ground on B for
⊢∀xα(x).
The closure clause states that “nothing else is a closed ground on B”. In other words,
the only possible forms of closed grounds on B are those indicated in the clauses
above. As for ⊥, we have:
(⊥)
there is no ground on B for ⊢⊥.
(⊥) means that, given an atomic base B on L, we can introduce a B-operation on
grounds of operational type ⊥▷α for every α ∈FORML deﬁned by the empty
equation—i.e. by a null number of computation instructions. It will simply be the
empty function of operational type indicated above.
5.2.2.7
Equivalence and Identity
Last, but not least, we consider the concepts of equivalence and identity between
grounds on an atomic base B with atomic system S, to which a domain of
individuals DB is associated.
We deﬁne equivalence on B by simultaneous recursion, based on the “categori-
cal” case. Given two grounds g1, g2 on B, g1 ≡B g2 if, and only if, they have the
same operational type, are deﬁned on the same number of individuals of DB and the
following conditions hold:

5.2
A Class of Languages
165
•
the domain of their operational type is empty and they are deﬁned on a null
number of individuals ⇒
– g1 and g2 are the same atomic derivation in S;
– g1 is ∧I(g3, g4) and g2 is ∧I(g5, g6) with g3 ≡B g5 and g4 ≡B g6;
– g1 is ∨I[αi ▷α1 ∨α2](g3) and g2 is ∨I[αi ▷α1 ∨α2](g4) with g3 ≡B g4
(i = 1, 2);
– g1 is →Iξα(g3) and g2 is →Iξα(g4) with g3 ≡B g4;
– g1 is ∃I[α(t) ▷∃xα(x)](g1) and g3 is ∃I[α(t) ▷∃xα(x)](g4) with g3 ≡B g4;
– g1 is ∀Ix(g3) and g2 is ∀Ix(g4) with g3 ≡B g4.
•
the domain of their operational type is non-empty or they are deﬁned on a non-
null number of individuals ⇒when applied to identical individuals of DB, and
then to identical grounds on B for elements in their domain with appropriate
substitutions of individual variables with (names of) individuals, in such a way
as to obtain respective grounds g∗
1 and g∗
2 on B the operational type of which has
empty domain and which are deﬁned on a null number of individuals, it holds
that g∗
1 and g∗
2 are identical (see below for identity).
Equivalence is a form of extensional identity, e.g., two operations on grounds
are equivalent when they produce identical values from identical arguments. To
characterise this accurately, we need a deﬁnition of identity, which is given
below. First, however, we note that we consider the following to hold of oper-
ations on grounds precisely because they are operations: any B-operation on
grounds f (x1, . . . , xn, ξτ1(x1), . . . , ξτm(xm)) is deﬁned in such a way that, for every
k1, . . . , kn ∈DB, for every g1
i , g2
i grounds on B for elements in the domain of the
operational type with appropriate substitutions of individual variables with (names
of) individuals, and such that g1
i ≡B g2
i (i ≤m),
f (k1, . . . , kn, g1
1, . . . , g1
m) ≡B f (k1, . . . , kn, g2
1, . . . , g2
m)
That is to say, an operation on grounds must be deﬁned in such a way to yield
equivalent values from equivalent arguments.
We deﬁne identity on B, indicated with ≈B, by distinguishing between two
cases: either the objects are atomic derivations of S, or they are made of (possibly
composite) operations on grounds. Identity involves the notion of operations being
deﬁned by the same equation, which we will interpret intuitively as follows: given
that a deﬁning equation provides instructions for computing an operation, two
operations are deﬁned by the same equation when their computation instructions
are the same. Given two grounds g1, g2 on B, g1 ≈B g2 if, and only if, they have
the same operational type, are deﬁned on the same number of individuals of DB,
and
•
g1 and g2 are the same atomic derivation in S, or
•
g1 is f (x1, . . . , xn, σ1, . . . , σm) and g2 is h(x1, . . . , xn, σ ∗
1 , . . . , σ ∗
m), where
f (x1, . . . , xn, ξτ1, . . . , ξτm) and h(x1, . . . , xn, ξτ1∗, . . . , ξτm∗)

166
5
Languages of Grounding
are deﬁned by the same equation and σi is a ground on B if, and only if, σ ∗
i is a
ground on B, with σi ≈B σ ∗
i (i ≤m).
Therefore, identity is “sameness”, and clearly g1 ≈B g2 ⇒g1 ≡B g2—however,
the vice versa may not hold.
To illustrate equivalent operations on grounds that are not identical, consider ﬁrst
of all the operation
DS(ξα∨β, ξ¬α)
of operational type α ∨β, ¬α ▷β, which is deﬁned by requiring that, for every
∨I(g1) ground for ⊢α ∨β with g1 ground for ⊢β, for every g2 ground for ⊢¬α,
DS(∨I(g1), g2) = g1.
It can easily be shown that this operation is equivalent to the composite operation
f∨(ξα∨β, ⊥β(f→(ξα, ξ¬α)), ξβ), where f∨and f→are the operations deﬁned
above, and where ⊥β is the empty function of operational type ⊥▷β. However,
as the two operations consist of different computational instructions, they are not
identical.
Finally, we consider identity on two bases. Let there be two atomic bases B1, B2
on the same background language and with respective atomic systems S1, S2, and
given a ground g1 on B1 and a ground g2 on B2, identity on bases B1, B2, indicated
with g1 ≈B1/B2 g2, can be deﬁned similarly to identity on one base, except that
atomic derivations belong to S1 and S2 respectively, and we replace ≈B with ≈B1/B2
in the second clause, and the two occurrences of B with B1 followed by B2. With
this deﬁnition, we close our discussion on our universe of grounds and operations
on grounds. We now turn our attention to languages of grounding.2
2 We have omitted a few details to avoid lengthening our discussion. These details are not essential
to a general understanding of the formal development proposed here, but for completeness, we
outline them in this footnote.
First, consider the case of operations on grounds of second level f that vacuously bind ξα, for α
occurring in the domain of some entry of their operational type. This requires the ground-variables
of the linguistic expression of an operation on grounds of ﬁrst-level h to be considered as having
an index (other than that concerning the entry of the operational type where the ground-variable
occurs), and that f bind ground-variables of a speciﬁc index i (other than that concerning the entry
of the operational type where the ground-variable occurs). The binding is then vacuous when ξα
has in h an index j ̸= i. The deﬁnition of identity requires a slight modiﬁcation to take into account
vacuous binding of ground-variables. For example, →Iξα
2 (→Iξα
1 (ξα
1 )) must not be identical—
nor equivalent—to →Iξα
1 (→Iξα
2 (ξα
1 )), although both are grounds for ⊢α →(α →α). This can
be satisﬁed by requiring that for two operations on grounds of second level to be identical, there
must be a “structure-preserving” re-indexing of the ground-variables such that all ground-variables
have the same indices in the two cases—e.g. →Iξα
2 (→Iξα
1 (ξα
1 )) and →Iξα
3 (→Iξα
1 (ξα
1 )) can
be re-indexed to →Iξα
4 (→Iξα
1 (ξα
1 )).
We have also not deﬁned what we can call variables-variants, which are important when
considering nested compositions of operations that bind individual variables. The deﬁnition is as

5.2
A Class of Languages
167
5.2.3
Languages, Expansions and Some Examples
The grounds and operations on grounds described thus far, make up some of the
objects denoted by the terms of the languages of grounding that we deﬁne in this
Section.
5.2.3.1
Languages of Grounding and Expansions
A language of grounding is relative to an atomic base. It includes an alphabet and
a set of terms. The alphabet contains names for certain derivations in the atomic
system of the base, ground-variables, and operational symbols. These symbols
are made up of labels attached to operational types. Since we want languages
of grounding to be “recursive”—i.e. it must be decidable whether an operational
symbol belongs to the language or not—the set of the operational symbols is either
ﬁnite, with a ﬁnite number of labels associated to an operational type τ in an equally
ﬁnite set of operational types, or non-ﬁnite but partitioned according to schemes of
operational types σ, where each scheme is associated to a ﬁnite number of labels.
As with inference rules and inferences instantiating them, a scheme of operational
types can be understood as an expression with meta-variables, indicating a structure
that all its instances must have. Finally, it should be noted that we permit only
operational symbols with operational types such that operations on grounds of those
types exist on the reference base. The operational types of the operational symbols
are always of ﬁrst level, but when the operational symbols bind ground-variables,
the operations we require to exist on the reference base are of second level - namely,
follows in the ﬁrst-level case (the second-level case is analogous): given a B-operation on grounds
f ([x1, . . . , xn+1 −x] y, ξα1(x1), . . . , ξαn(xn)) of operational type α1(x1), . . . , αn(xn) ▷β(xn+1),
let w be a sub-sequence of x1, . . . , xn. A w-variant of this operation on grounds is a total
constructive function
h([[x1, . . . , xn+1 −x] −w] y, ξα1(x1), . . . , ξαm(xn))
such that, for every k of the same length as [[x1, . . . , xn+1 −x] −w] y, for every gi ground on B
for ⊢αi(xi)[k/[xi −x] −w] (i ≤n),
f (k, g1, . . . , gm)
is a ground on B for ⊢β(xn+1)[k/xn+1 −w]. The w-variant is again an operation on grounds of
operational type α1(x1), . . . , αm(xn) ▷β(xn+1), except instead of grounds for the entries of the
operational type after replacement of all the free individual variables with (names of) individuals,
it can take as values operations on grounds for these very entries where some individual variables
may remain free, depending on whether w contains some of the individual variables that occur free
in this entry. When deﬁning equations of variable-variants, we perform the same steps as in the
case of deﬁning equations for open grounds, except that (names of) individuals are replaced in the
arguments that the operation to be deﬁned takes as values.

168
5
Languages of Grounding
their operational type is of second level, with operational types of ﬁrst level on the
entries where the bindings take place.
Deﬁnition 21 Let B be an atomic base with atomic system S, over a background
language L, and to which a domain of individuals DB is associated. A language of
grounding  over B is speciﬁed by
•
an alphabet Al containing
– an individual constant δi naming the i-th atomic derivation in S with no
undischarged assumptions and unbound individual variables;
– ground-variables ξα
i (α ∈FORML, i ∈N);
– operational symbols
F1[τi], . . . , Fn[τi] (n ≥1)
for τi ∈{τ1, . . . , τm} (i ≤m, m ≥0) set of operational types of ﬁrst level
over L, or
F i
1[τσi], . . . , F i
n[τσi] (n ≥1)
for every τσi instance of σi ∈{σ1, . . . , σn} (i ≤n, n ≥0) set of schemes
of operational types of ﬁrst level over L. Each operational symbol may bind a
sequence of individual variables x or a sequence of ground-variablesξ - where
x and ξ, as well as bindings, have to be understood with the same notational
conventions and wording as in Sect. 5.2.2.4.
We implement the following restriction. If an operational type occurring
in an operational symbol φ is α1(x1), . . . , αn(xn) ▷β(xn+1), then there is a
B-operation on grounds f such that
∗f is deﬁned on the same number of individuals as the length of
x1, . . . , xn −x;
∗the operational type of f has co-domain β(xn+1) and, for every i ≤n, the
entry of index i has co-domain αi(xi) and, as domain, the set of types of
the ground-variables that φ binds on i, if any; otherwise the entry of index
i is αi(xi);
∗φ binds x and ξα on indexes i and j respectively iff f binds x and ξα on
indexes i and j respectively (i, j ≤n);
– a set of typed terms TERM given by induction. If the operational type
occurring in an operational symbol φ is α1(x1), . . . , αn(xn) ▷β(xn+1), and
if φ binds a sequence of individual variables x and a sequence of ground-
variables ξ, the inductive clause says
Ui : αi(xi) ∈TERM (i ≤n) ⇒φ x ξ (U1, . . . , Un) : β(xn+1) ∈TERM
where the colon indicates that a certain term has a certain type. The inductive
clause puts restrictions 1 and 2 as in Sect. 5.2.2.5 on the formation of the term.

5.2
A Class of Languages
169
Over every language of grounding captured by this deﬁnition, we need to deﬁne
some standard notions. These are:
•
set S(T ) of the subterms of T ;
•
sets FV I(T ) and BV I(T ) of, respectively, the free and bound individual
variables of T and
•
sets FV T (U) and BV T (U) of, respectively, the free and bound ground-variables
of U.
We say that U is closed if, and only if, FV I(U) = FV T (U) = ∅. These notions
have to be speciﬁed inductively, so that, given a language of grounding , for every
F x ξ (. . . U . . . ) : α ∈TERM, two circumstances hold:
(a) FV I(F x ξ (. . . Ui . . . )) is equal to the union of (1) FV (α) and (2) of the free
individual variables of Ui other than those in x bound on index i and
(b) FV T (F x ξ (. . . Ui . . . )) is equal to the union of the free ground-variables of
Ui other than those in ξ bound on index i.
Finally, for the sake of greater simplicity, we adopt a rather standard convention
concerning free and bound variables, and proper variables. A similar convention
has already been mentioned with regard to the derivations in Gentzen’s natural
deduction, as well as to typed λ-calculus, and is intended to apply to all languages
of grounding.
Convention 22 For every T , two properties hold: ﬁrst, FV I(T ) ∩BV I(T ) = ∅—
property (FB)—and, second, proper and non-proper variables are all distinct from
each other, and each proper variables occurs in at most one ∀Ix(U)—property (PN).
Given a language of grounding  on B, let us indicate with F the set of its
operational symbols.
Deﬁnition 23 Let 1 be a language of grounding over an atomic base B1. An
expansion of 1 is language of grounding 2 on an atomic base B2 with B2
expansion of B1, or with F1 ⊆F2.
5.2.3.2
Examples
In this section, we exemplify two languages of grounding. First, we present a
language of grounding that we call “Gentzen-language”, a kind of functional trans-
lation of ﬁrst-order intuitionistic logic in a Gentzen natural deduction system. In
the second example, we present a language that can be understood as the functional
translation of Heyting’s ﬁrst-order arithmetic in Gentzen’s natural deduction.
Gentzen-Language
Let B be any atomic base with atomic system S on a background language L. We
call core language on B - indicated with G—a language of grounding that, beyond

170
5
Languages of Grounding
names δ of derivations in S with no undischarged assumptions and unbound indi-
vidual variables, and ground-variables ξα, has the following operational symbols:
•
∧I[α, β ▷α ∧β] (α, β ∈FORML);
•
∨I[αi ▷α1 ∨α2] (i = 1, 2, α1, α2 ∈FORML);
•
→I[α ▷α →β] binding ξα (α, β ∈FORML);
•
∃I[α(t) ▷∃xiα(xi)] (t ∈TERML, α(t/xi) ∈FORML, i ∈N)
•
∀I[α(xi) ▷∀xjα(xj/xi)] binding xi (i, j ∈N, α(xi) ∈FORML) and
•
⊥α[⊥▷α] (α ∈FORML).
We qualify these operational symbols as primitive. The labels of primitive opera-
tional symbols are the same as those used for primitive operations in the ground-
clauses. Although they ideally would be kept distinct, we shall not make this
distinction explicit here; when attributing a denotation to these symbols, we shall
simply make them correspond to the primitive operations. As an example of term
formation, we remark that TERMG is speciﬁed as follows (omitting type, subscripts
and superscripts whenever possible);
•
δ : α ∈TERMG—where α is the conclusion of the derivation of which δ is a
name;
•
ξα : α ∈TERMG;
•
T : α, U : β ∈TERMG ⇒∧I(T, U) : α ∧β ∈TERMG;
•
T : αi ∈TERMG ⇒∨I[αi ▷α1 ∨α2](T ) : α1 ∨α2 ∈TERMG;
•
T : β ∈TERMG ⇒→Iξα(T ) : α →β ∈TERMG;
•
T : α(t/x) ∈TERMG ⇒∃I[α(t/x) ▷∃xα(x)](T ) : ∃xα(x) ∈TERMG - with the
restriction that t is free for x in α(x);
•
U : α(x) ∈TERMG ⇒∀I[α(x) ▷∀yα(y/x)]x(U) : ∀yα(y/x) ∈TERMG - with
the restriction that x /∈FV (β) for ξβ ∈FV T (U) and
•
T : ⊥∈TERMG ⇒⊥α(T ) : α ∈TERMG.
From now on, we assume for simplicity that the following conventions holds.
Convention 24 Every language of grounding on an atomic base B is an expansion
of a core-language on B.
We now call Gentzen-language—indicated with Gen—an expansion of a core-
language obtained by adding the following non-primitive operational symbols:
•
∧E,i[α1 ∧α2 ▷αi] (i = 1, 2, α1, α2 ∈FORML);
•
∨E[α ∨β, γ, γ ▷γ ]—binding ξα on the second entry and ξβ on the third
(α, β, γ ∈FORML);
•
→E[α →β, α ▷β] (α, β ∈FORML);
•
∃E[∃xα(x), β ▷β]—binding x and ξα(x) on the second entry (α(x), β ∈
FORML);
•
∀E[∀xα(x) ▷α(k)] (α(x) ∈FORML, k ∈TERML name of individual in DB).
Next, let us establish a kind of Curry-Howard isomorphism between IL of Chap. 3
and Gen. First of all, consider the following bijective function

5.2
A Class of Languages
171
i-th derivation S
ι
	⇒
δi
α
ι
	⇒
ξα
1
α
2
β (∧I)
α ∧β
ι
	⇒
∧I(ι(1), ι(2))

α1 ∧α2 (∧E,i), i = 1, 2
αi
ι
	⇒
∧E,i(ι()), i = 1, 2

αi
(∨I), i = 1, 2
α1 ∨α2
ι
	⇒
∨I[αi ▷α1 ∨α2]ι())
1
α ∨β
[α]
2
γ
[β]
3
γ
(∨E)
γ
ι
	⇒
∨E ξα ξβ(ι(1), ι(2), ι(3))
[α]

β
(→I)
α →β
ι
	⇒
→Iξα(ι())
1
α →β
2
α (→E)
β
ι
	⇒
→E(ι(1), ι(2))
(x)
α(x)
(∀I)
∀yα(y/x)
ι
	⇒
∀Ix(ι((x)))

∀xα(x) (∀E)
α(t)
ι
	⇒
∀E(ι())

α(t/x)
(∃I)
∃xα(x)
ι
	⇒
∃I(ι())

172
5
Languages of Grounding
1
∃yα(y/x)
[α(x)]
2(x)
β (∃E)
β
ι
	⇒
∃E x ξα(x)(ι(1), ι(2(x))

⊥(⊥)
α
ι
	⇒
⊥α(ι())
Second, as we shall see in the next Section, it is possible to associate to the non-
primitive operational symbols of Gen equations that establish the behavior of (the
operations on grounds denoted by) such symbols. These equations are similar
to those for the elimination of the redexes in the typed λ-calculus. Hence, we
can introduce, for the terms of Gen, a notion of reduction that the bijection ι
preserves isomorphically, with respect to the corresponding reduction relation for
the derivations of IL. Therefore, ι is an isomorphism with respect to this reducibility
relation, and hence also with respect to reduction to normal form. Of course, it is
easily seen that our ι here is in all ways similar to the bijection between IL and
typed λ-calculus in Chap. 4.
Heyting’s First-Order Arithmetic
Let B be an atomic base with atomic system the Post system on a ﬁrst-order
language L for arithmetic as deﬁned in Chap. 3, to which the domain of individuals
N is associated. A language of grounding for Heyting’s arithmetic—indicated with
HA—is an expansion of Gen over B obtained by adding the following non-primitive
operational symbols:
•
Ind[α(0), α(s(x)) ▷α(n)]—binding x
and ξα(x) on the second entry
(α(0), α(s(x)) ∈FORML, n name of individual in N)
Here, too, we have a bijection that is easily seen to induce an isomorphism between
terms and derivations. This is achieved by adding to the previous bijection ι the
clause
1
α(0)
[α(x)]
2(x)
α(s(x)/x) IND
α(t)
ι
	⇒
Ind x ξα(x)(ι(1), ι(2(x))).
We have, of course, Gödel’s incompleteness: there exists closed G ∈FORML such
that there is no closed T : G ∈TERMHA, and no closed T : ¬G ∈TERMHA. It is
often said that, however undecidable, G is a true formula on the intended model N.
Thus, if by truth of G we mean the existence of a ground for ⊢G, this also means
that there is a ground for ⊢G inexpressible in HA. The observation provides us with
a decisive reason to consider a language of grounding as open to the introduction
of new linguistic resources: in order to express grounds on languages at least as

5.2
A Class of Languages
173
rich as those of ﬁrst-order arithmetic, we should be able to expand any language of
grounding on these languages. In other words, there is no language that allows us to
express all the grounds we need.
5.2.4
Denotation
As with model theory, deﬁning the notion of denotation is done in two steps: ﬁrst we
ﬁx the denotation of the elements of the alphabet, and then on this basis we establish
the denotation of the terms of the language by induction on the complexity.
5.2.4.1
Denotation for Alphabet and Terms
In what follows, we use the notion of scheme of (systems of) equations, which
should be understood in the same way as the notion of scheme of operational types,
i.e., as a meta-linguistic characterisation of a class of (systems of) equations sharing
a certain structure.
Deﬁnition 25 Let  be a language of grounding over an atomic base B with atomic
system S. A denotation for the alphabet of  is a function den∗speciﬁed as
follows:
•
den∗(δi) = i-th derivation of S of which δi is a name;
•
den∗(ξα) = Id(FV (α), ξα), where Id(FV (α), ξα) is the identity B-operation
on grounds of type α ▷α;3
•
given an operational symbol φ,
– if φ is primitive, den∗(φ) = φ—i.e. a primitive operational symbol corre-
sponds to the primitive operation with the same label in the ground-clauses;
– if φ is non-primitive, den∗(φ) is one of the B-operations on grounds assumed
as existing in connection with φ in Deﬁnition 11.
We have the following restriction: for every two operational symbols F[τ 1
σ]
and F[τ 2
σ] where τ 1
σ and τ 2
σ are instances of the same scheme of operational
types, den∗(F[τ 1
σ]) and den∗(F[τ 2
σ]) are B-operations on grounds whose
deﬁning (systems of) equations are instances of the same scheme of (systems
of) equations.
Deﬁnition 26 Let  be a language of grounding, and let den∗be a denotation for
the elements of its alphabet. A denotation for the terms of  associated to den∗is
a function den speciﬁed as follows:
3 We have therefore a distinction between the ground-variables used to speak about operations
on grounds, and those that occur in languages of grounding. The former should be expressed
with a distinct notation, such as Id(εα), or simply a typed “hole” Id(−α). However, to avoid
overburdening the notation, we have omitted this distinction.

174
5
Languages of Grounding
•
den(δ) = den∗(δ);
•
den(ξα) = den∗(ξα) and
•
den(φ x ξ (U1, . . . , Un)) = den∗(φ)(y, den(U1), . . . , den(Un)).4
For the last clause of this deﬁnition to be well-posed, den(Ui) (i ≤n) must denote
a ground on B in the domain of den∗(φ), or a ground which can be composed with
den∗(φ). The denotation theorem presented in the following section ensures this.
As an example of denotation functions, let us take the languages of grounding
Gen and HA of the previous section, and let us consider the deﬁning equa-
tions of Sect. 5.2.2.4. Let den∗(kE) = fE for each logical constant k, and
den∗(Ind[α(0), α(s(x)) ▷α(n)]) = f n
Ind. For ⊥α, we can instead require that
den∗(⊥α) be the empty function of operational type ⊥▷α.
Building upon what we have just observed, and to illustrate the connection
between denotation functions and expansions of languages of grounding, consider
the following situation. Given an atomic base B and an operational type on the
language of B, e.g. τ1, . . . , τn ▷τn+1, if there exists τi (i ≤n) such that there are
no grounds for τi on B, a B-operation on grounds of this operational type exists
trivially and is unique; it is the empty function. Then, a non-primitive operational
symbol that corresponds to an operation on grounds of the operational type just
indicated, is automatically authorized in a language of grounding on B; as in the
case of ⊥α, the denotation of the operational symbol in question will be the empty
function associated with the intended operational type. If there is no change of base,
the denotation can remain stable throughout the expansions. In the case of a change
of base—say from B to B+—there are two possible situations, which we illustrate
with the following examples. First, let B be a logical base on a language L for ﬁrst-
order arithmetic and, for every α ∈ATOML closed, let us consider the operational
type α ▷¬∃x(0 = s(x)). Clearly, there is no ground on B for ⊢α, whatever α is,
or for ⊢¬∃x(0 = s(x)). Hence, B-operations on grounds having such operational
types exist, and they are unique: they are the empty functions, which we indicate
with f α
∅. Therefore, we can build a language of grounding  on B that has a non-
primitive operational symbol F of operational type α ▷¬∃x(0 = s(x)) for every
α ∈ATOML closed, and we can require that
den∗(F[α ▷¬∃x(0 = s(x))]) = f α
∅.
Naturally, the identity symbol used for arithmetical formulas should not be inter-
preted as the same one used to deﬁne denotation but, given this is clear from
the context, the distinction does not need to be made explicitly. Now, consider
an expansion B+ of B that has as atomic system a Post system S for ﬁrst-order
arithmetic as in Chap. 3. This gives us a ground on B+ for ⊢¬∃x(0 = s(x)),
4 y is the sequence of the individual variables of the operational type of den∗(φ) in the case when
den∗(φ) is an operation on grounds of ﬁrst level, and of the individual variables of the co-domains
of the operational type of den∗(φ) in the case when den∗(φ) is an operation on grounds of second
level.

5.2
A Class of Languages
175
namely
→Iξ∃x(0=s(x))(∃E x ξ0=s(x)(ξ∃x(0=s(x)), δ))
where δ is the derivation in S
0 = s(x) (s1)
⊥
Similarly, there will be grounds on B+ for some ⊢α if, and only if, ⊢S α. Now, let
+ be a language of grounding on B+, which expands . For α such that ⊬S α,
the operational symbols F of  of operational type α ▷¬∃x(0 = s(x)) can still
be associated with the empty function f α
∅. But once ⊢S α, this denotation function
stops working. In these cases, we can therefore require that den∗(F[α ▷¬∃x(0 =
s(x))]) be the operation f (ξα) such that, for every g ground on B+ for ⊢α,
f (g) = →Iξ∃x(0=s(x))(∃E x ξ0=s(x)(ξ∃x(0=s(x)), δ)).
This allows us to pass from  to +, but a denotation function deﬁned on  cannot
remain invariant on +.
As a further example, let once again B be a logical base on a language L for
ﬁrst-order arithmetic as in Chap. 3 and, for every α ∈ATOML closed, consider the
operational type α ▷0 = s(0). It is clear that there is no ground on B neither for
⊢α, for any α, nor for ⊢0 = s(0). Therefore, B-operations on grounds with such
operational types exist and are unique: they are the empty functions f α
∅. A language
of grounding1 on B can therefore be built with a non-primitive operational symbol
F of operational type α ▷0 = s(0) for every α ∈ATOML closed, and requiring that
den∗(F[α ▷0 = s(0)]) = f α
∅.
Finally, we consider an expansion B+ of B with atomic system S for ﬁrst-order
arithmetic. As before, there is no ground on B+ for ⊢0 = s(0), since ⊬S 0 =
s(0). However, there will be grounds on B+ for some ⊢α, precisely when ⊢S α.
Now, suppose 2 is a language of grounding on B+. For α such that ⊬S α, we can
continue to associate the operational symbols F of 1 of operational type α ▷0 =
s(0) with the empty function f α
∅. But when ⊢S α, there can be no B+-operation on
grounds of the intended operational type. Hence, some of the operational symbols F
previously considered are not admissible, and thus 1 cannot be expanded towards
2.
5.2.4.2
Denotation Theorem
The denotation theorem introduced here can be considered as establishing “correct-
ness” for languages of grounding, because it states that any term of any language
of grounding denotes a ground on the reference base. This result clearly depends on

176
5
Languages of Grounding
the restriction we applied to operational symbols in Deﬁnition 21—namely that only
operational symbols whose operational type is inhabited by an adequate operation
on grounds are permitted. Note that the proof of this theorem is of considerable
length, as many cases and sub-cases have to be considered—although here we focus
only on those most relevant to our discussion.
Theorem 27 Let  be a language of grounding on an atomic base B to which
a domain of individuals DB is associated, den∗be a denotation function for the
elements of , and den be a denotation function for the terms of  associated to
den∗. Let U : β ∈TERM be such that
FV I(U) = {x1, . . . , xn} and FV T (U) = {ξα1, . . . , ξαm},
and let {αs, . . . , αt} be the set of the formulas that occur as types of the elements
of FV T (U). Then, den(U) is a ground on B for αs, . . . , αt ⊢β deﬁned on n
individuals on DB.
Proof We proceed by induction on the complexity of U : β ∈TERM.
•
For U constant or ground-variable, the result holds trivially;
•
Now, let us assume the result is proved for every term less complex than U. For
the sake of greater clarity, we distinguish two cases.
(Case 1) U is closed. By virtue of observation (a) in Sect. 5.2.3.1, it must hold
that FV (β) ⊆FV I(U), and hence, since we have assumed FV I(U) = ∅, it
must hold FV (β) = ∅. We have therefore to prove that U is a proper ground on
B for ⊢β, with β closed. We focus only on a U of the form F x ξγ (Z), with
Z : α ∈TERM. Then, F must have operational type α ▷β. Again by virtue
of observation (a) in Sect. 5.2.3.1, it must hold that FV (α) ⊆FV I(Z), and
that FV I(Z) ⊆{x}. Hence, den∗(F) is a B-operation on grounds f (ξγ ▷α)
of operational type (γ ▷α) ▷β, binding x and ξγ , for closed or open α.
As for Z, after what has already been established with respect to FV I(Z), by
observation (b) in Sect. 5.2.3.1, it must apply that FV T (Z) ⊆{ξγ } and, again
by observations (a) and (b) in Sect. 5.2.3.1, it will hold that
FV T (Z) ̸= ∅⇒for every y ̸= x ∈FV (γ ), y ∈BV I(Z).
To lighten the proof, we reduce this circumstance to the assumption that
FV (γ ) ⊆FV I(Z) - noting that this is an imprecise, although useful, simpli-
ﬁcation. We now have a series of subcases. The ﬁrst is that in which Z is itself
closed; by induction hypothesis, den(Z) is a ground g on B for ⊢α, with α
closed. But then,
den(F x ξγ (Z)) = den∗(F)(den(Z)) = f (g)
is a ground on B of the required type. The other subcase is where Z is open,
which in turn has the three subcases of the disjunction
FV I(Z) = {x} or FV T (Z) = {ξγ }.

5.2
A Class of Languages
177
Let us consider only the case where FV I(Z) = {x} and FV T (Z) = {ξγ }.
By induction hypothesis, den(Z) is a B-operation on grounds h(x, ξγ ) of
operational type γ ▷α, and is hence a ground on B for γ ⊢α, proper if γ
or α are open, and improper otherwise. But then,
den(F x ξγ (Z)) = den∗(F)(den(Z)) = f (h(x, ξγ ))
is a ground on B of the required kind;
(Case 2) U is open, FV I(U) = {x1, . . . , xn} and FV T (U) = {ξα1, . . . , ξαm}
(for n > 0 or m > 0). By observations (a) and (b) of Section 5.2.3.1, it
holds that 
i≤m FV (αi) ∪FV (β) ⊆FV I(U). We must prove that den(U)
is a B-operation on grounds f (x1, . . . , xn, ξα1, . . . , ξαm) of operational type
α1, . . . , αm ▷β, which is a ground on B for α1, . . . , αm
⊢β, proper if

i≤m FV (αi) ∪FV (β) = FV I(U), improper if 
i≤m FV (αi) ∪FV (β) ⊂
FV I(U). For the sake of simplicity, let us assume that U has the form
F x1 ξγ1(Z), with Z
: α
∈TERM, FV I(F x1 ξγ1(Z)) = {x2} and
FV T (F x1 ξγ1(Z)) = {ξγ2}. Then, F has operational type α ▷β. By virtue
of observation (a) in Sect. 5.2.3.1, and of restriction (2) in Sect. 5.2.2.5 applied
to Deﬁnition 21, it must hold that FV (β) ⊆{x2}. Furthermore, again by virtue
of observation (a) in Sect. 5.2.3.1, it must hold that FV (α) ⊆FV I(Z), and that
FV I(Z) ⊆{x1, x2}. Hence, den∗(F) can be: either a B-operation on grounds
f (ξγ1▷α) of operational type, (γ1 ▷α) ▷β, if FV (α) ∪FV (β) ⊆{x1}; or
a B-operation on grounds f (x2, ξγ1▷α) of operational type (γ1 ▷α) ▷β, if
x2 ∈FV (α) ∪FV (β). In all cases, den∗(F) binds x1 and ξγ1. As for Z, after
what has already been established for FV I(Z), we observe that, by observation
(b) in Sect. 5.2.3.1, we have FV T (Z) ⊆{ξγ1, ξγ2}. Hence, again by observations
(a) and (b) in Sect. 5.2.3.1, we will have
FV T (Z) ̸= ∅⇒for every y ̸= xj ∈FV (γi) (i, j = 1, 2), y ∈BV I(Z).
To lighten the proof, we reduce this circumstance to the assumption that FV (γi)
(i = 1, 2) ⊆FV I(Z)—again noting that this is an imprecise, although useful,
simpliﬁcation. However, by observation (b) in Sect. 5.2.3.1, it cannot be that
FV T (Z) = ∅, as ξγ2 ∈FV T (F x ξ(Z)). Hence, ξγ2 ∈FV T (Z), and FV (γ2) ⊆
FV I(Z). We now have a series of cases. First of all, that in which FV I(Z) = ∅.
Here, we have in turn the two subcases of the disjunction
FV T (Z) = {ξγ2} or FV T (Z) = {ξγ1, ξγ2}.
Focusing on the latter, suppose that FV T (Z) = {ξγ1, ξγ2}. By induction
hypothesis, den(Z) is a B-operation on grounds h(ξγ1, ξγ2) of operational type
γ1, γ2 ▷α with γ1, γ2 and α closed, and is hence a proper ground on B for
γ1, γ2 ⊢α. Since x2 ∈FV I(F x ξ(Z)), it must hold that FV (β) = {x2}, and
hence den∗(F) will be a B-operation on grounds f (x2, ξγ1▷α) of operational

178
5
Languages of Grounding
type (γ1 ▷α) ▷β. But then
den(F x ξ(Z)) = den∗(F)(den(Z)) = f (x2, h(ξγ1,γ2))
is a ground on B of the required kind. Now let FV I(Z) ̸= ∅. We have the three
subcases of the disjunction
FV I(Z) = {x1} or FV I(Z) = {x2} or FV I(Z) = {x1, x2}.
each articulating with respect to the two subcases of the disjunction
FV T (Z) = {ξγ2} or FV T (Z) = {ξγ1, ξγ2}.
We consider only one of these cases. Suppose that FV I(Z) = {x2} and
FV T (Z) = {ξγ1, ξγ2}. We discuss only one of the four possibilities:
FV (γ1) ∪FV (γ2) ∪FV (α) = {x2} and FV (β) = {x2}
By induction, den(Z) is B-operation on grounds h(x2, ξγ1, ξγ2) of operational
type γ1, γ2 ▷α, i.e. a proper ground on B for γ1, γ2 ⊢α. den∗(F) is a B-
operation on grounds f (x2, ξγ1▷β) of operational type (γ1 ▷α) ▷β. But then,
den(F x ξ(Z)) = den∗(F)(den(Z)) = f (x2, h(x2, ξγ1, ξγ2))
is a ground on B of the required kind.
For other cases, and the more complex ones, the reasoning is similar.
5.2.4.3
Closure Under Canonical Form
We now introduce a notion with partial links to the denotation theorem, that allow
us to identify a ﬁrst property of languages of grounding. For this purpose, we ﬁrst
require a convention and an additional deﬁnition. From this point on, there will be
extensive use of the notions of equivalence and identity between (operations on)
grounds as deﬁned in Sect. 5.2.2.7—whose notations are, respectively, ≡B, ≈B (for
identity on one base) and ≈B1,B2 (for identity on two bases).
The converse of Theorem 27 could be considered a type of “completeness” result,
stating that every ground on an atomic base is denoted by some term in some
language of grounding on that base. Our convention corresponds to assuming this
“completeness”.
Convention 28 For every atomic base B and g ground on B for α1, . . . , αn ⊢β
(n ≥0), there is a language of grounding  on B such that, for some denotation
function den∗for the elements of the alphabet of , and called den the denotation

5.2
A Class of Languages
179
function for the terms of  associated to den∗, there is U : β ∈TERM such that
den(U) ≈B g.
Deﬁnition 29 Let  be a language of grounding, and let T ∈TERM. We say that
T is in canonical form if, and only if, it is an individual constant of , or a ground-
variable, or the outermost operational symbol of T is a primitive operational symbol.
Now, the denotation theorem asserts that, whatever the denotation on  is, if T is
closed, it denotes a ground for a categorical judgment or assertion—i.e., an object
whose outermost operation is primitive. We may therefore expect it is possible to
ﬁnd a canonical term in  with a denotation equivalent to that of T , or even the
same denotation as T —depending on whether we are considering equivalence or
identity.
We ﬁrst focus on equivalence, but identity is discussed immediately afterwards.
Note that, from this point forwards, it is assumed that we are working on a language
of grounding  (possibly with indices) over an atomic base B (with atomic system
S and over a background language L), with den∗(possibly with indices) denotation
function for the alphabet of  and den (possibly with indices) denotation function
for the terms of  associated to den∗. In doing so, we can avoid too burdensome a
notation.
Deﬁnition 30  is closed under canonical form relative to den iff, for every closed
T ∈TERM, there is a closed canonical U ∈TERM such that den(T ) ≡B den(U).
We have already come across examples of languages of grounding closed under
canonical form relative to a denotation deﬁned on their terms—typically, the
language Gen of section “Gentzen-Language”, via the denotation for its alphabet
suggested in Sect. 5.2.4.1, and thanks to Prawitz’s normalization theorems (Prawitz,
2006). In general, however, this characteristic does not apply to every language of
grounding, as illustrated in the following example.
For any atomic base B, take the B-operation on grounds f∧,i (i = 1, 2)
of Sect. 5.2.2.4. Consider also the B-operation on grounds hi(ξαi→α1∨α2) of
operational type αi →α1 ∨α2 ▷α1 ∧α2 →αi deﬁned by requiring that, for
every g ground on B for ⊢αi →α1 ∨α2,
hi(g) = →Iξα1∧α2(f∧,i(ξα1∧α2)).
Let us now take a core-language on B, and expand it to  by adding operational
symbols
F[αi →α1 ∨α2 ▷α1 ∧α2 →αi].
Finally, let us deﬁne a denotation den∗for the alphabet of  such that
den∗(F[αi →α1 ∨α2 ▷α1 ∧α2 →αi]) = hi(ξαi→α1∨α2).

180
5
Languages of Grounding
Let den be the denotation for the terms of  associated to den∗. In  we surely have
a term denoting via den a proper ground g on B for categorical ⊢αi →α1 ∨α2,
i.e.,
→Iξαi (∨I[αi ▷α1 ∨α2](ξαi)).
It follows that the term of 
F(→Iξαi (∨I[αi ▷α1 ∨α2](ξαi)))
denotes via den a ground on B for the categorical ⊢α1 ∧α2 →αi, i.e.,
den(F(→Iξαi(∨I[αi ▷α1 ∨α2](ξαi)))) =
= den∗(F)(den(→Iξαi (∨I[αi ▷α1 ∨α2](ξαi))))) =
= hi(g) = →Iξα1∧α2(f∧,i(ξα1∧α2)).
Suppose there is a canonical U ∈TERM such that
den(F(→Iξαi (∨I[αi ▷α1 ∨α2](ξαi)))) ≡B den(U).
U must have the form →Iξα1∧α2(Z) for some Z : αi ∈TERM. Now, by
assumption we have
den(U) ≡B →Iξα1∧α2(f∧,i(ξα1∧α2))
and since we have
den(U) = den(→Iξα1∧α2(Z)) = den∗(→I)(den(Z)) = →Iξα1∧α2(den(Z))
this gives us
den(Z) ≡B f∧,i(ξα1∧α2).
By the denotation theorem and how equivalence is deﬁned, this is possible if, and
only if, FV T (Z) = {ξα1∧α2}.
The language of grounding we just built is a kind of Curry-Howard linear
translation of a Gentzen natural deduction system, say , with all the introduction
rules, plus
αi →α1 ∨α2 F
α1 ∧α2 →αi
Therefore, there can be Z : αi ∈TERM with FV T (Z) = {ξα1∧α2} if, and only if,
α1 ∧α2 ⊢ αi. And, clearly, α1 ∧α2 ⊬ αi.

5.2
A Class of Languages
181
Nonetheless, we might wonder whether we can expand any language of ground-
ing to one that that is closed under canonical form relative to some denotation. Here,
we will prove the weaker result that any language of grounding can be expanded to
one that is closed under canonical form with respect to a given closed term, leaving
aside whether an expansion can be also found for the whole set of closed terms. The
proof involves several subcases, with one requiring a strategy that could be deﬁned
as duplication of operational symbols. We will later explore whether or not this
strategy is plausible.
Deﬁnition 31 Let T ∈TERM be closed. We say that  is closed under canonical
form relative to den with respect to T if, and only if, there is a closed canonical
U ∈TERM such that den(T ) ≡B den(U).
Theorem 32 Given 1 over B with atomic system S, and T ∈TERM1 closed,
there is an expansion 2 of 1 on B such that, for some den∗
2, 2 is closed under
canonical form relative to den2 with respect to T .
Proof By cases on the type α of T - for α of course closed. If α is atomic, then
den1(T ) is an atomic derivation  in S named by some δ in 1. We can hence put
2 = 1 and den2 = den1, since
den1(T ) =  = den1(δ)
and  ≡B . For α logically complex, we consider the case α = β1 ∨β2 - for
βi of course closed (i = 1, 2). den1(T ) is in this case ∨I[βi ▷β1 ∨β2](g) for g
ground on B for ⊢βi. By convention 28, there is ∗on B such that, for some closed
U : βi ∈TERM∗, and for some den∗, den(U) ≈B g. The only difﬁcult case is the
following. Consider the situation where Al1 ∩Al∗̸= ∅and, indicating with
F1,T and F∗,U the sets of the non-primitive operational symbols of 1 and ∗
respectively occurring in T and U respectively, F1,T ∩F∗,U ̸= ∅and, for some
x ∈F1,T ∩F∗,U, it does not hold that den∗
1(x) ≈B den∗(x). For every operational
symbol F[τ] ∈F1,T ∩F∗,U for which it does not hold that den∗
1(F[τ]) ≈B
den∗(F[τ]), let us take an operational symbol F†[τ] such that F†[τ] /∈F1 ∪F∗.
Let us call F† the set of the operational symbols obtained in this way. We let 2 be
such that Al2 = (Al1 ∪Al∗) ∪F†, and den∗
2 be such that
den∗
2(x) =
⎧
⎪⎪⎨
⎪⎪⎩
den∗
1(x)
if x ∈Al1
den∗(x)
if x ∈(Al∗−(Al1 ∩Al∗))
den∗(F[τ])
if x = F†[τ]
Let U† be the term of TERM2 obtained from U by replacing with F†[τ] ∈F† any
F[τ] ∈F1,T ∩F∗,U for which it does not hold that den∗
1(F[τ]) ≈B den∗(F[τ]).
We have that den2(T ) ≈B den1(T ) and den2(U†) ≈B den(U), so that
den1(T ) = ∨I[βi ▷β1 ∨β2](g) =
= den∗(∨I[βi ▷β1 ∨β2])(den∗(U)) = den(∨I[βi ▷β1 ∨β2](U))
and clearly ∨I[βi ▷β1 ∨β2](g) ≡B ∨I[βi ▷β1 ∨β2](g).

182
5
Languages of Grounding
Theorem 32 also holds in a weaker form, i.e., by replacing ≈B with ≡B in
Convention 28. Similarly, everything said thus far for closure under canonical form
can also be expressed in terms of identity on B; all we need is to replace ≡B with
≈B in Deﬁnition 30. This gives us a more stringent notion than that identiﬁed
by Deﬁnition 30, so we will speak in this case of  being strictly closed under
canonical form. Then, if we replace ≡B with ≈B in an example analogous to that
provided for simple closure under canonical form, we ﬁnd that not every language
of grounding is strictly closed under canonical form. We can obtain an analogous
notion of strictly closed under canonical form with respect to a closed term by
replacing ≡B with ≈B in Deﬁnition 31. Finally, Theorem 32 still holds—indeed,
the proof provided here also applies to the stricter notion.
There is a crucial step of the proof: the strategy we deﬁned as duplication.
Duplication involves adding, for each operational symbol which is part of at least
two of the terms T , U1 and U2, an operational symbol that has the same operational
type, but is indicated with a different “label”, which is not attributed to any of
the operational symbols already present. It is unclear whether duplication is a
plausible, and thus viable, strategy. Doubts arise owing to the fact that a language of
grounding can be understood as a kind of linear translation of a formal system in a
Gentzen format. Therefore, a language of grounding which has duplicate symbols,
would be like a formal system with rules where premises, conclusions, discharge
of assumptions and binding of variables are equal, and where the only thing that
changes is the “name” of the rule. As an example, we could have
α1
. . .
αn F
β
α1
. . .
αn F ∗
β
Now, if an inference rule is considered to be nothing but the set of its instances, it is
easy to conclude that F and F ∗are the same rule. From this perspective, duplication
is a senseless strategy.
There is, however, a possible answer to this objection. A language of grounding
can obviously be understood as a formal system; but that said, this is only really true
when a denotation has been deﬁned on it. Denotation is what makes it possible to
understand the purely syntactic symbols of the language as actual operations on
grounds. In other words, a language of grounding is such when its terms name
grounds - i.e., not when they are understood as mere proof-terms but, more strongly,
when a speciﬁc denotation function renders them interpreted proof-terms (see
Prawitz, 2014). Thus, if a language of grounding is comparable to a formal system,
a language of grounding on which a denotation has been deﬁned is comparable to a
formal system where each rule is identiﬁed, not only through premises, conclusion,
discharge of assumptions and binding of variables, but also by a reduction procedure
associated to it. Therefore, even if the rules F and F ∗have the same set of instances,
they will be different if they are associated with different reduction procedures. This
is similar to taking two inferences (or inference rules) to be distinct when they have
different justiﬁcation procedures.

5.2
A Class of Languages
183
5.2.4.4
Universal Grounds and Universal Terms
Thus far, we have considered grounds relative to an atomic base. Similarly, we have
deﬁned languages of grounding over an atomic base. We might wonder whether it
is possible speak of grounds over an atomic base that remain such regardless of the
atomic base, and what kind of terms in languages of grounding would denote these
“universal” grounds. As the meaning of the non-logical constants of the background
language is ﬁxed by an atomic base, (denotation of) an object which is a ground over
all bases would amount to a kind of logical validity.
In the case of grounds, the steps are as follows. Given an atomic base B1 on a
background language L1, and a ground g1 on B1, g1 is universal if, and only if, for
every L2 expansion of L1, for every atomic base B2 on L2, there is a ground g2 on
B2 such that g1 ≈B1,B2 g2—using the notion of identity over two bases introduced
in Sect. 5.2.2.7. Note that operations deﬁned in Sect. 5.2.2.4, except for induction,
are universal grounds. In the case of terms, we give the following deﬁnition.
Deﬁnition 33 Given  and den∗, T ∈TERM is universal with respect to den∗
if, and only if, for every element x ∈Al occurring in T , den∗(x) is a universal
ground.
Theorem 34 Given  over B and den∗, T ∈TERM is universal with respect to
den∗if, and only if,
(a) S(T ) does not contain individual constants and
(b) for every non-primitive operational symbol φ of  occurring in T , den∗(φ) is
a universal ground.
Proof (	⇒) Let T be universal with respect to den∗. Then, given a logical base B2
on L, every element x ∈Al occurring in T denotes a ground on B2. If S(T ) ∩
DERS ̸= ∅, this cannot happen; since B2 logical, no individual constant occurring in
T could denote a ground on B2. Point b) is trivial. If, for every x ∈Al occurring in
T , den∗(x) is a universal ground, then, for every non-primitive operational symbol
F of T , den∗(F) is a universal ground.
(⇐	) Let S(T ) ∩DERS = ∅and, for every non-primitive operational symbol F of
1 occurring in T , let den∗(F) be a universal ground. Thus T contains only ground-
variables, primitive operational symbols and non-primitive operational symbols.
And clearly, the images of ground-variables and primitive operational symbols
through den∗are universal grounds.
Corollary 35 T ∈TERM is universal with respect to den∗if, and only if, for every
U ∈S(T ), U is universal with respect to den∗.
Some might consider it preferable that universality of terms be relative to a
denotation function for terms. By assuming a plausible convention, Deﬁnition 33
can be sufﬁcient to ensure this.
Convention 36 Let B be an atomic base and let f (x, ξτ1, . . . , ξτn) be a B-
operation on grounds that is, more speciﬁcally, a universal ground. Then, the

184
5
Languages of Grounding
operation is deﬁned in such a way that, for every sequence of individuals k the
length of which is not greater than that of x, for every gi ground on B for τi[k/x]
(i ≤n, but not necessarily for all such i-s) that is more in particular a universal
ground, f (k/x, . . . , gi, . . . ) is a universal ground.
Proposition 37 Given  and den∗, and T ∈TERM universal with respect to
den∗, den(T ) is a universal ground.
Proof By induction on the complexity of T . The case of ground-variables is
obvious, since the identity function for the given operational type is clearly a
universal ground. If T is a constant, then it cannot be universal with respect to den∗,
since for example a constant will not count as a ground on a logical base. If T is
F x ξ (U1, . . . , Un)
then, by corollary 35, every Ui is universal with respect to den∗and hence,
by induction hypothesis, den(Ui) is a universal ground. By hypothesis on the
universality of T with respect to den∗, moreover, deﬁnition 33 tells us that den∗(F)
is a universal ground. Therefore, since
den(F x ξ (U1, . . . , Un)) = den∗(F)(y, den(U1), . . . , den(Un)),
the result follows immediately from convention 36.
The converse of Proposition 37, however, does not hold, as a non-canonical term can
“reduce” to a universal ground, while involving a sub-term which is not universal.
We illustrate this with two examples.
First, let B1 be a non-logical base, and let  be an appropriate language of
grounding on B1. Then, ∧E,1(∧I(→Iξα(ξα), δ)) with δ individual constant in
the atomic system of B1 is such that, with respect to the den∗where den∗(∧E,1) is
as indicated in Sect. 5.2.2.4, den(∧E,1(∧I(→Iξα(ξα), δ)) is a universal ground—
where den is the denotation function for terms of  associated with den∗. Indeed,
den(∧E,1(∧I(→Iξα(ξα), δ))) = →Iξα(Id(ξα)).
However, the term is not universal with respect to den∗. The constant δ involved in
it will not denote a ground on a logical base B2.
Now, let there be a logical base B1 on a language for ﬁrst-order arithmetic as
in Chap. 3, and an appropriate language of grounding 1 on B1 that has in its
alphabet a non-primitive operational symbol F of operational type τ of the form
s(s(0)) = s(0) + s(0) ▷¬∃x(0 = s(x)). Then, the term ∧E,1(∧I(→Iξα(ξα), →
Iξτ (F(ξτ))) is such that, with respect to the den∗
1 where den∗
1(∧E,1) is as indicated
in Sect. 5.2.2.4, and den∗
1(F) is the empty function of the intended operational type,
den1(∧E,1(∧I(→Iξα(ξα), →Iξτ (F(ξτ )))) is a universal ground—where den1
is the denotation function for terms of 1 associated with den∗
1. Once again, we

5.2
A Class of Languages
185
have the following situation:
den1(∧E,1(∧I(→Iξα(ξα), →Iξτ (F(ξτ )))) = →Iξα(Id(ξα)).
However, the term is not universal with respect to den∗
1. Given a base B2 on the same
background language, with Post system S for ﬁrst-order arithmetic as in Chapter 3,
the empty function on the intended operational type is not admissible on B2, since
⊢S s(s(0)) = s(0) + s(0).
5.2.5
Primitiveness and Conservativity
Finally, there are two interesting classiﬁcation criteria of languages of grounding
for us to explore; a language can be the expansion of another, in a primitive or non-
primitive, and conservative or non-conservative sense. We begin with the concept of
primitiveness.
5.2.5.1
Primitive Expansions
Deﬁnition 38 Given 1 over B1 with atomic system ⟨L1, ℜ1⟩, let 2 be an
expansion of 1 over B2 with atomic system ⟨L2, ℜ2⟩. We call 2 a primitive
expansion of 1 iff ℜ1 ⊂ℜ2.
Thus, a language of grounding can be considered a primitive expansion of another
when the former is relative to an atomic base that is a proper expansion of the latter.
For the expansion to be non-primitive, the only new linguistic resources must be
non-primitive operational symbols. We now make two observations.
First, why would an expansion on an atomic base containing strictly the starting
language be called primitive? Recall convention 17, which states that the atomic
system of an atomic base must interpret completely the background language. This
means that when an atomic base on the expanded language of grounding is the
proper expansion of the atomic base of a starting language of grounding, the new
Post system will be a proper expansion of the starting Post system. In fact, given a
base B1
⟨R, F, C, S⟩,
suppose that a proper expansion B2 of B1 does not expand S properly. B2 must
therefore involve proper super-sets of at least one of the components of the
background language of B1. In other words, B2 must be
⟨R+, F+, C+, S⟩,

186
5
Languages of Grounding
with R ⊂R+ or F ⊂F+ or C ⊂C+. This, however, means the background language
of B2 is a proper expansion of the background language of B1. As S is an atomic
system for the background language of B1, the background language of B2 is a
proper expansion of the language of S, which means that the background language
of B2 is only partially interpreted by S.
We therefore ﬁnd that the expanded language of grounding is relative to an
atomic base that, besides acting on a potentially enriched background language,
also involves a more powerful atomic system. Therefore, this atomic base contains
new atomic derivations, and thus the expanded language of grounding contains
new individual constants which, by deﬁnition, constitute grounds for judgments or
assertions involving atomic formulas. These constants are considered to be primitive
elements, given that they cannot be reduced to elements of the starting language of
grounding, nor deﬁned in such terms. Note how this differs to the non-primitive
operational symbols, which must be deﬁned in terms of equations that ﬁx their
behavior with respect to how meaning is established by the clauses (AtG)—(∃G).
Our deﬁnition of primitive expansion might be viewed as being too restricted.
Indeed, if primitiveness of an expansion depends on the addition of non-reducible
elements to a language of grounding, why not take into account those expansions
whose primitiveness depends on the addition of new primitive operational symbols
that are related to new logical constants? To do this, we would need to take into
account not only an atomic base on the background language, but also the set ℑof
the logical constants of the background language, i.e.
⟨R, F, C, S⟩and ℑ.
Therefore, a proper expansion of a language of grounding on such a base would be a
language of grounding on a new atomic base or, in addition, on a new set of logical
constants
⟨R+, F+, C+, S+⟩or ℑ+.
with S ⊂S+, and R ⊂R+ or F ⊂F+ or C ⊂C+ or ℑ⊂ℑ+.
Although such a strategy is both feasible and correct, it will not be dealt with
here, beyond an example at the end of this Section. There are two key reasons for
its omission, other than the necessity of brevity. If the new primitive operational
symbols are related to logical non-modal constants that do not imply a passage to
orders higher than the ﬁrst, we conclude that while the primitive expansions are
certainly possible, they are not particularly signiﬁcant. As has been shown (Prawitz,
1979; Schroeder-Heister, 1984), the usual intuitionistic ﬁrst-order logical constants
are functionally complete with respect to all the possible ﬁrst-order intuitionistic
logical constants. That said, if the new primitive operational symbols are relative
to logical constants of orders higher than the ﬁrst—for example, second-order
quantiﬁers—the ground-theoretic approach discussed here is likely to encounter
problems related to Dummett’s molecularity requirement (see Cozzo, 1994b), or
to phenomena of loss of compositionality or paradoxicality (see Pistone, 2015).

5.2
A Class of Languages
187
5.2.5.2
Conservative Expansions
As we have already seen, Gödel’s incompleteness implies that there are some
grounds for judgments or assertions on a language for ﬁrst-order arithmetic that
cannot be expressed in a language of grounding for Heyting’s ﬁrst-order arithmetic.
We can therefore consider expansions in which such grounds can be constructed as
being non-conservative. Conservativeness can thus be deﬁned as a mere generaliza-
tion of this idea.
Deﬁnition 39 Given 1 over an atomic base with background language L, let 2
be an expansion of 1 over an atomic base B, and let den∗be relative to 2. 2 is
conservative over 1 with respect to den∗if, and only if, for every U : β ∈TERM2
with β ∈FORML, and such that, for every ξα ∈FV T (U), α ∈FORML, there is
Z ∈TERM1 such that den(U) ≡B den(Z).
Therefore, 1 is conservatively expanded by 2 when, for every ground on B for
a judgment or assertion τ1, . . . , τn ⊢τn+1 on L denoted via den by some term of
2, there is a term in 1 that denotes via den an equivalent ground. Notably, this
means that, for every operation the operational type of which is on L, and which
is denoted by some term of 2, there is an operation denoted by some term of 1
such that the two operations are “extensionally” equal—although they might differ
computationally. Thus, everything that can be expressed through 2 on B, can be
expressed through 1 in an “extensionally” equal way.
The type of conservativity being dealt with here differs from the usual notion
of conservativity of theories. One option would be to speak of conservativity “of
denotation”, to distinguish it from conservativity of derivability. There is, however,
a clear link between these two notions, as established by the following theorem.
Theorem 40 Given 1 with background language L, let 2 be an expansion of it
over B, and let den∗be relative to 2. Let us ﬁnally suppose that, for every non-
primitive operational symbol φ of 2 the operational type of which is on L, there is
a composition of B-operations on grounds f1, . . . , fn resulting in a ground h on B
such that:
•
for every i ≤n, there is an operational symbol φi of 1 such that den∗(φi) ≡B
fi, and
•
den∗(φ) ≡B h.
Then, 2 is conservative over 1 with respect to den∗.
Proof By induction on the complexity of T ∈TERM2:
•
the atomic case is trivial by non-primitivity of 2 with respect to 1;
•
for simplicity, let us consider the case of the term F ξα x (U). By denotation
theorem, den(U) is a ground on B for some appropriate judgment or assertion.
By induction hypothesis, there is Z ∈TERM1 of the same type as U (not
necessarily distinct from U) such that den(U) ≡B den(Z). If F is an operational
symbol of 1, the required term is F ξα x (Z) which, as can be easily seen, has

188
5
Languages of Grounding
the same type as the starting term. Indeed—possibly under substitution (∗/⋆)
of free individual variables with individuals, and of free ground-variables with
grounds on B for the closed formulas thereby obtained—we have
den(F ξα x (U))(∗/⋆) = den∗(F)(den(U)(∗/⋆)) ≡B
den∗(F)(den(Z)(∗/⋆)) = den(F ξα x (Z))(∗/⋆)
from which, for the arbitrariness of the eventual substitution (∗/⋆), we conclude
that
den(F ξα x (U)) ≡B den(F ξα x (Z)).
If instead F is a primitive operational symbol, or if it is not a non-primitive oper-
ational symbol of 1, by the hypotheses of the theorem we know that there exists
a B-operation on grounds h resulting from the composition of B-operations
on grounds f1, . . . , fn such that, for every i ≤n, fi ≡B den∗(Fi), with Fi
operational symbol of 1, and den∗(F) ≡B h. Let us apply the composite h to
den(Z). By replacing each fi with the corresponding Fi, we will have a term
1 with Z as a subterm, which we indicate with F1, . . . , Fn(Z). Now—possibly
under substitution (∗/⋆) of free individual variables with individuals, and of free
ground-variables with grounds on B for the closed formulas thereby obtained,
and under appropriate substitutions of den(U) and den(Z) for variables in the
composition of f1, . . . , fn—we have
den(F ξα x (U))(∗/⋆) = den∗(F)(den(U)(∗/⋆)) = h(den(U)(∗/⋆)) =
f1 ◦· · · ◦fn(den(U)(∗/⋆)) ≡B f1 ◦· · · ◦fn(den(Z)(∗/⋆)) =
den∗(F1), . . . , den∗(Fn)(den(Z)(∗/⋆)) = den(F1, . . . , Fn(Z))(∗/⋆)
from which, for the arbitrariness of the eventual substitution (∗/⋆), we again
conclude that
den(F ξα x (U)) ≡B den(F1, . . . , Fn(Z))
and the result is proven.5
5 Theorem 40 leads to a seemingly interesting observation. Given any language of grounding  on
an atomic base B on a background language L, suppose that it is always possible to associate to ,
via Curry-Howard isomorphism, a formal system  on L which includes the atomic system of
B. We indicate the usual conservativeness of theories—extended to admissibility of rules under
substitution—as conservativeness on provability, and the conservativeness of Deﬁnition 39 as
conservativeness on denotation. The following applies. Given a language of grounding 1 on an
atomic base B, and given a non-primitive expansion 2 of 1, there exists a denotation function
den∗of the elements of the alphabet of 2 such that 2 is a conservative expansion on the

5.2
A Class of Languages
189
Thus far, we have dealt with conservativity in terms of equivalence. We therefore
close this discussion by considering its relation to identity.
Deﬁnition 41 Let 1 and 2 be as in Deﬁnition 39 on B1 and B2 respectively.
2 is strictly conservative over 1 with respect to den∗if, and only if, for every
U : β ∈TERM2 with β ∈FORML and such that, for every ξα ∈FV T (U),
α ∈FORML, there is Z ∈TERM1 such that den(U) ≡B2 den(Z) and, for some
ground g on B1, den(Z) ≈B1,B2 g.
5.2.5.3
Primitiveness vs Conservativity
We conclude with some examples showing that primitiveness and conservativity are
not equivalent notions.
Non-primitive Non-conservative Gen is a non-primitive expansion of a core-
language G when it has the same base as G. However, there is no denotation
function such that Gen is conservative over G, as in G we have no term of type,
e.g., α1 ∧α2 →αi (i = 1, 2).
Non-primitive Conservative A non-primitive expansion obtained by adding to
Gen an operational symbol φ that denotes, via den∗, DS in Sect. 5.2.2.7 is
conservative over Gen with respect to den∗when the latter assigns the operations
on grounds of Sect. 5.2.2.4.
Primitive Non-conservative Any expansion with enlargement of the atomic base.
Alternatively, one obtained by adding a reﬂection principle to a language of
grounding for Heyting’s ﬁrst-order arithmetic as in section “Heyting’s First-Order
Arithmetic”.
Primitive Conservative Consider 2 obtained by adding to 1 new logical con-
stants. Suppose that 1 and 2 are, respectively, a kind of Curry-Howard translation
of Gentzen’s natural deduction systems 1 and 2. Additionally, suppose that we
can associate to the elimination rules of 2 reduction procedures permitting proof
of a sub-formula property. Therefore, any derivation in 2 with assumptions and
conclusion in the language of 1 can be reduced to a derivation in 1. And as we
denotation of 1 with respect to den∗if, and only if, 2 is provability conservative with respect to
1. The left-right direction of the equivalence is a rather immediate consequence of Theorem 40;
it is sufﬁcient to choose as den∗the function that associates to each non-primitive operational
symbol F of 2 a coding of the derivation of the operational type of F in 1. The left-right
direction is obvious in the case of closed terms, and in the case of open terms that correspond to
the denotation of primitive operational symbols that do not bind individual or ground-variables.
In the case instead of non-primitive operational symbols that bind individual or ground-variables,
we know that each of their applications corresponds to a derivation in 1, which guarantees
admissibility under substitutions in 1.

190
5
Languages of Grounding
can assume that reduction procedures can be Curry-Howard translated into deﬁning
(systems of) equations for non-primitive operations to be associated to the non-
primitive operational symbols of 2, we can deﬁne on 2 a den∗in such a way that
2 is conservative over 1 with respect to den∗.

Chapter 6
Systems of Grounding
6.1
General Overview
In this chapter we propose and develop a class of formal systems of grounding.
Each element of the class refers to one of the languages of grounding identiﬁed in
the previous Chapter. The aim of the systems of grounding is to provide rules that
allow us to deduce relevant properties of the terms of languages of grounding. These
properties will be essentially two: the fact that a term denotes a ground for a certain
judgment or a certain assertion, and the fact that two terms denote the same ground.
Since the class of languages to which these systems refer is inﬁnite, the class
of the systems of grounding is also inﬁnite. However, each system has a standard
structure. Firstly, there are rules for the predicate which indicates that a certain term
is a ground for a certain judgment or assertion; then, rules for an identity predicate;
and ﬁnally rules for the usual constants of ﬁrst-order intuitionist logic. The systems
differ by only a restricted subgroup of some, but not all, of the aforementioned
groups of rules.
We will start from a speciﬁc example - namely, a system of grounding for the
Gentzen-language as indicated in the previous Chapter. Based on this example, we
will then generalize our argument, and will show how the properties enjoyed by
the Gentzen-language apply for each element of the class. First of all, however,
we carry out some preliminary observations, aimed at facilitating the reading and
understanding of what we will later discuss, as well as at indicating the reasons for
our formal choices.
6.1.1
Denotation and Identity
The languages of grounding we have presented in the previous Chapter consist,
strictly speaking, only of terms. It is therefore natural to wonder whether it is
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_6
191

192
6
Systems of Grounding
possible to enrich these languages with formulas and, if so, what these formulas
should express. Obviously, the formulas will have to indicate, by means of suitable
predicates, properties of the terms, meaning that the question of which formulas to
include is reduced to what the most relevant properties are of the terms in question.
A ﬁrst property is indicated by the denotation theorem: all the terms of a language
of grounding denote grounds. The general scheme is that, if the term is closed,
it denotes a ground for a categorical judgment or assertion, whereas if the term
is open, it denotes a ground for a general, hypothetical, or general-hypothetical
judgment or assertion - namely, an operation on grounds that produces grounds for
categorical judgments or assertions, when applied to appropriate arguments. In the
case of closed terms, this property will be indicated with
Gr(T, β)
for closed T and β. In developing a system of grounding, we will enrich the
language of grounding to which it refers with a binary predicate Gr(. . . , −−−).
As for the open case, let us suppose that T contains a sequence x of free
individual variables, and no free ground-variables. Then
Gr(T (x), β)
indicates that T (x) is an operation on grounds of operational type β, for FV (β) ⊆
FV I(T (x)). For every appropriate sequence t of closed terms, the closed term
T (t/x) is a ground for ⊢β(t)—namely
Gr(T (t/x), β(t)).
If instead T contains free ground-variables ξα1, . . . , ξαn, and no free individual
variables,
Gr(T (ξα1, . . . , ξαn), β)
for closed α1, . . . , αn, β, indicates that T (ξα1, . . . , ξαn) is an operation on grounds
of operational type α1, . . . , αn ▷β. For every closed term Ui : αi (i ≤n) in
expansions of the language of grounding to which T belongs, the closed term
T (U1, . . . , Un) is a ground for ⊢β—namely
Gr(T (U1, . . . , Un), β).
Finally, if T contains a sequence of free individual variables x1, . . . , xn, and of free
ground-variables ξα1, . . . , ξαm,
Gr(T (x1, . . . , xn, ξα1, . . . , ξαm), β)

6.1
General Overview
193
indicates as before that T
is an operation on grounds of operational type
α1, . . . , αm ▷β for FV (α1) ∪. . . ∪FV (αm) ∪FV (β) ⊆{x1, . . . , xn}. For
every appropriate sequence of closed terms t1, . . . , tn, for every closed term
Ui : αi(t1, . . . , tn) (i ≤m) in expansions of the language of grounding to which
T belongs, the closed term T (t1, . . . , tn/x1, . . . , xn, U1, . . . , Um) is a ground for
⊢β(t1, . . . , tn), namely
Gr(T (t1, . . . , tn/x1, . . . , xn, U1, . . . , Um), β(t1, . . . , tn)).
In the case of a term T (x, ξα1, . . . , ξαn) denoting an operation on grounds of
operational type α1, . . . , αn ▷β, expressed by the formula
Gr(T (x, ξα1, . . . , ξαn), β),
we have noted that this means requiring that, after appropriate substitutions of
ground-variables, which might follow the substitution of individual variables, the
term in question denotes a ground for a categorical judgment or assertion, which is
expressed by the formula
Gr(T (t/x, U1, . . . , Un), β).
Of course, the legitimacy of this statement depends on the fact that the terms
used to replace the ground-variables—possibly after substitution of the individual
variables—are such as to denote grounds for categorical judgments or assertions
involving the type of the ground-variable substituted—possibly after replacing
individual variables. If the type is αi(t), and if the term denoting a ground for ⊢αi(t)
is Ui (i ≤n), then the fact that the starting term denotes a ground after the indicated
substitutions depends on
Gr(Ui, αi(t)),
a dependence that can be expressed with
. . . Gr(Ui, αi(t)) . . .
...
Gr(T(t/x, . . . Ui . . . ), β)
However, since this must apply to every possible Ui that enjoys this property, we can
say that the fact that the starting term denotes an operation on grounds of a given
operational type, depends on the assumption
Gr(ξαi, αi)

194
6
Systems of Grounding
where ξαi is the ground-variable occurring in the starting term, a dependence that
can be expressed once again with
. . . Gr(ξαi, αi) . . .
...
Gr(T(x, . . . ξα
i . . . ), β)
In developing the systems of grounding, we will follow the intuition contained in
this observation. More precisely, since a term T without ground-variables must
denote a ground for a categorical or general judgment or assertion—namely, an
operation on grounds with empty domain—we will ensure that the fact that this
term denotes that ground is a provable theorem of the system, i.e.
⊢Gr(T (x), α).
By contrast, since a term T in which free ground-variables occur ξα1, . . . , ξαn
denotes a ground for a hypothetical, or general-hypotheticaljudgment or assertion—
namely, an operation on grounds with a non-empty domain and codomain β—we
will ensure that the fact that this term denotes that ground is provable in dependence
on assumptions which require that each of the free ground-variables denote a ground
for a judgment or assertion involving the type of the ground-variable, i.e.
Gr(ξα1, α1), . . . , Gr(ξαn, αn) ⊢Gr(T (x, ξα1, . . . , ξαn), β).
When introducing the binary predicate Gr(. . . , −−−), we will impose the
following restriction: given  the language of grounding to which T belongs,
Gr(T, α) is a formula of the enriched grounding language if, and only if, T is a
term of type α in . There is a fairly “practical” reason for this restriction. In light
of the close connection between the syntactic typing and the formulas involved in
the judgment or assertion that terms denote grounds for, it would make no sense to
permit, for example, a formula such as Gr(T, β) if T has type α instead. Indeed,
from the denotation theorem we know that T must be a ground for α, or an operation
on grounds with codomain α. If, on the contrary, we wanted to be less restrictive,
we would encounter some unnecessary complications. First of all, we would end up
authorizing formulas in which there is no connection between the syntactic typing
and the semantic value of a term. As a result, since a system of grounding, while
having to comply with certain basic semantic assumptions, is conﬁgured as a mere
deductive apparatus, it would become possible to prove results such as
1 ⊢Gr(T, β).

6.1
General Overview
195
These theorems determine a sort of “deductive over-generation”. The system proves
what we expect—that is
2 ⊢Gr(T, α)
as well as other results that, in semantic terms, are incorrect. The whole system
would be incorrect with respect to the reference semantics, but in a very peculiar
sense—this is why we talked about “deductive over-generation”, and not about
incorrectness strictu sensu. For each of the derivations that lead to incorrect results,
in fact, it is possible to ﬁnd a substitution of formulas of the background language
that returns the derivation of a correct result; and this substitution of formulas is
exactly what is obtained by introducing the aforementioned restriction.
The second property we intend to deal with is identity, which will be understood
here in the terms of the equivalence relation indicated in the previous chapter. As
known, if a closed term T denotes a ground, it will denote the same ground of a
canonical closed term U. Of course, T and U do not necessarily belong to the same
language; but, if the language is closed under canonical form, this is guaranteed for
an appropriate choice of the denotation function. Within these languages, therefore,
we can pass from an identity of the denotation of T and U to an identity between T
and U expressible in the enriched grounding language—namely
T ≡U.
To this end, we will therefore introduce into languages of grounding a binary
predicate . . . ≡−−−, which enjoys the usual properties of reﬂexivity, symmetry,
transitivity, and substitution of identicals. For this reason, although neither T nor U
are canonical, the identity between them will apply if both of them reduce to the
same canonical closed term Z.
If this is what holds for identity between closed terms, in the open case we follow
an extensional criterion. If T and U contain free individual or ground-variables,
each of them denotes a ground for a general, or general-hypothetical judgment or
assertion—namely, an operation on grounds having a certain operational type. They
are identical if, and only if, the operations they denote return the same values on the
same arguments. In other words,
T (x, ξ) ≡U(x, ξ)
if, and only if, for every sequence of closed individual terms t, and of terms for
closed grounds Z,
T (t/x, Z/ξ) ≡U(t/x, Z/ξ).
Operations on grounds denoted by non-primitive operational symbols of a
language of grounding are to be intended as deﬁned by certain equations. When
developing systems of grounding, we will use this idea, even if internalizing, so

196
6
Systems of Grounding
to speak, the deﬁning equations in the system itself. In other words, instead of
mapping the non-primitive operational symbols on operations deﬁned by certain
equations, through denotation functions that connect the language of grounding
to our “universe” of grounds and operations on grounds, we will provide identity
axioms that directly concern non-primitive operational symbols - namely, equations
that can be used in the derivations of the system to eliminate the non-primitive
operational symbols, transforming some of the terms constructed with such symbols
into others that the system considers to be identical to them. Now, even if the
axioms faithfully reﬂect the equations that, in the denotational approach, deﬁne
the operation denoted by the non-primitive operational symbol in question, the two
ways of proceeding differ on an essential point: while an operation on grounds is
deﬁned by an equation that shows how the operation behaves on closed objects,
the axiom that, in the system, ﬁxes the behavior of a non-primitive operational
symbol provides a syntactic method of transformation, connected only with the
type of its arguments, regardless of the presence or absence of free variables in
these arguments. By way of example, let us consider the non-primitive operational
symbol ∧E,i⟨α1 ∧α2 ▷αi⟩; according to the standard interpretation, this symbol is
associated to an operation on grounds f∧,i(ξα1∧α2) of operational type α1 ∧α2 ▷αi
ﬁxed by requiring that, for every ∧I(g1, g2) ground for ⊢α1 ∧α2, with gi ground
for ⊢αi,
f∧,i(∧I(g1, g2)) = gi.
The equivalent in a system of grounding will be an axiom for the predicate . . . ≡
−−−of the type
R∧
E,i( I(T1, T2))
Ti
where Ti is any term of type αi. Now, let α2 be a closed atomic formula, and let δ be
(the name of) a closed derivation of α2 in an atomic system that occurs in the base
B of a Gentzen-language on which we have deﬁned a system of grounding. From
a denotational point of view, the term ∧E,2(∧I(ξα1, δ)) denotes a B-operation on
grounds f (ξα1) of operational type α1 ▷α2; indeed
den(∧E,2(∧I(ξα1, δ)) = den∗(∧E,2)(∧I(den∗(ξα1), den∗(δ)) = f (∧I(Id, δ))
and, for every g ground on B for ⊢α1,
f∧,2(∧I(Id, δ))(g) = f∧,2(∧I(Id(g), δ)) = f∧,2(∧I(g, δ)) = δ.
Obviously, the operation will give δ on any ground g for α1, but in the approach
in question, we cannot conclude that the operation is equal to δ - namely, that the
denotation of the starting term is the same as for δ. An operation on grounds, being
open, is an object different from a ground for a categorical judgment or assertion,

6.1
General Overview
197
which is instead closed. In the system, however, it will be possible to prove that the
starting term and δ are identical, and therefore that this term is a ground for α2.
Thus, the identity ﬁxed by the predicate . . . ≡−−−is an equivalence
relation among terms that reduce or expand from/to each other via syntactic
transformations that generalize the reduction or expansion procedures in Prawitz’s
theory of normalization, or the β—reduction or η-expansion in the theory of λ-
conversion. Despite this asymmetry,in a system of grounding it will be possible to
demonstrate identities that faithfully reﬂect identity in the denotational approach. In
other words, if den(T ) = den(U), then
⊢T ≡U.
A second clariﬁcation is similar to that already made for the predicate
Gr(. . . , −−−). Namely, we will adopt the following restriction: called  the
language of grounding to which T and U belong, T ≡U is a formula of the
enriched language of grounding if, and only if, T and U have identical type α in .
Here too, the restriction is aimed at avoiding the incorrectness of a grounding system
which is, in the peculiar sense indicated above, a “deductive super-generation”.
6.1.2
Aims and Outcomes of a Deductive Approach
The theoretical apparatus presented in the previous chapter, in terms of denotation
functions from languages of grounding to a “universe” of grounds and operations
on grounds, actually allows, in itself, to prove that a term denotes a ground, which
ground this is, and if and when two terms share the same denotation. The advantage
of a formal-system approach is that it allows for the same results through a clear and
well-deﬁned set of rules.
The systems we are going to propose in this Chapter share the following group
of rules:
•
rules for the predicate Gr(. . . , −−−);
•
rules for the predicate . . . ≡−−−and
•
introduction and elimination rules of a Gentzen natural deduction system for ﬁrst-
order intuitionistic logic.
The rules for the predicate Gr(. . ., −−−) are further divided into two subgroups:
•
rules of type introduction and
•
rules of type elimination.
Now, the rules of type introduction are nothing but a translation, in deductive terms,
of the clauses (AtG) - (∃G). For example, for each closed individual constant δ of
the language of grounding to which the system refers, and that “names” a closed
derivation of an atomic formula α in the atomic base of reference, we will have an

198
6
Systems of Grounding
axiom
C
Gr(δ, α)
while, in the case of for example ∧, we will have the rule
Gr(T, α)
Gr(U, β)
I
Gr( I(T, U), α
β)
The rules of type introduction can also be seen as introduction rules of the
appropriate primitive operational symbol.
By contrast, the rules of type elimination express, relative to the main logical
constant of the type, Dummett’s fundamental assumption (Dummett, 1991): if T
denotes a ground for a formula with main logical constant k, then it must be possible
to reduce T to a canonical term starting with kI that denotes a ground for the
same formula. These rules will take the form of generalized elimination rules. For
example, in the case of ∧, we will have
Gr(T, α ∧β)
[T ≡∧I(ξα, ξβ)]
[Gr(ξα, α)]
[Gr(ξβ, β)]
...
A D∧
A
The rule discharges the assumptions T ≡∧I(ξα, ξβ), Gr(ξα, α) and Gr(ξβ, β),
and it can be applied if the following holds true: in the derivation of A as a minor
premise, ξα does not occur free in A, nor in assumptions on which A depends other
than T ≡∧I(ξα, ξβ) and Gr(ξα, α); furthermore, ξβ does not occur free in A, nor
in assumptions on which A depends other than T ≡∧I(ξα, ξβ) and Gr(ξβ, β). We
can additionally require that ξα and ξβ are fresh variables, not previously used in
the derivation of Gr(T, α ∧β).
In the margins of what has been said about the rules of introduction and
elimination of type, it may be useful to specify that the enriched languages of
grounding will also have two new linguistic resources, whose introduction depends
on “technical” reasons. The ﬁrst is variables for terms of type α(x) deﬁned on
individual variables x, intended to represent operations on grounds of operational
type α(x), or operations on grounds with codomain α(x) for x not free in any of
the entries of the domain. These variables will be represented with the notation
hα(x)(x), where x can be substituted by any term t in the background language.
These variables are needed to express the rule of type elimination for terms T of
type ∀xα(x); in one of the assumptions on which the minor premise of the rule
depends, in fact, we will ﬁnd the dischargeable formula
T ≡∀Ix(hα(x)(x))

6.1
General Overview
199
which, in this speciﬁc case, indicates that T reduces to a term in canonical form of
the same type, and which is a ground for the formula of which it is type - in fact, the
other dischargeable assumption will be
for all x(Gr(hα(x)(x), α(x)))
—that is to say, T is identical to a term in canonical form the immediate subterm of
which is an operation on grounds of operational type
α(x)
or an operation on grounds with codomain α(x) for x not free in any of the entries
of the domain.
We will also have a variable for terms of type β deﬁned on ground-variables of
arbitrary type α, intended to represent operations on grounds with codomain β with
α arbitrary in the domain. These variables will be represented with the notation
fβ(ξα), for any α in the set of formulas of the background language, where ξα
can be substituted by any term of type α in the language of grounding. As for the
requirements that determine this second addition, we limit ourselves only to point
out that, without it, the system would prove a series of incorrect results, above all
in connection with operational symbols that bind ground-variables. To understand
this point, it is perhaps sufﬁcient to reﬂect on the formulation of the clause that sets
what counts as ground for formulas with →as main logical constant, which, with
the variables for operations on grounds with domain β, can be written - and, as we
will see, proven in the system—in the form
Gr(→Iξα(fβ(ξα)), α →β) ⇔for all ξα(if Gr(ξα, α) then Gr(fβ(ξα), β))
while, without these variables, we can only ask for a problematic vacuous quantiﬁ-
cation
Gr(→Iξα(ξβ), α →β) ⇔for all ξα(if Gr(ξα, α) then Gr(ξβ, β)).
Now we come to the rules for the predicate . . . ≡−−−. They are further divided
into four subgroups:
•
rules that determine an equivalence relation that preserves the denotation;
•
substitution rules with primitive operational symbols;
•
equations for non-primitive operational symbols and
•
substitution rules with non-primitive operational symbols;
The ﬁrst of the four subgroups includes the usual rules of reﬂexivity, symmetry
and transitivity. To these we add a rule that states preservation of the denotation

200
6
Systems of Grounding
among identical terms, i.e.
T ≡U
Gr(U, α) ≡P
Gr(T, α)
The substitution rules with primitive operational symbols express the substitution
principle of identicals in relation to the primitive operational symbols of a language
of grounding. These rules are bidirectional, allowing to pass from identity between
terms of minor complexity to identity between terms of greater complexity, and vice
versa. In the case of ∧, for example, we will have
T1 ≡U1
T2 ≡U2
≡∧
1
I(T1, T2)
I(U1, U2)
∧I(T1, T2) ≡∧I(U1, U2) ≡∧
2i
Ti
Ui
Analogously, the substitution rules with non-primitive operational symbols express
the substitution principle of identicals on non-primitive operational symbols of a
language of grounding. In this case, however, the rules are unidirectional—that is,
they go only from the identity of less complex terms to that of more complex terms.
In the case of ∧, we will have
T ≡U
≡∧
3i
E,i(T)
E,i(U)
—obviously for T and U of appropriate type. The reason for this unidirectionality
is obvious. Some non-primitive operational symbols, such as ∧E,i, are deﬁned
in such a way that, when applied to certain terms, they reduce these terms to
others, “deleting” some subterms in the application arguments; while identity can
be guaranteed in the case of terms which result at the end of the reduction, it may
not apply to those that are “deleted”, and therefore on the application arguments
themselves.
Finally, the equations for non-primitive operational symbols were brieﬂy dis-
cussed in the previous section. They are a sort of “internalization” of the deﬁning
equations of the operations on grounds which, in the denotational approach, non-
primitive operational symbols are meant to denote. While a denotation function
on a language of grounding connects the syntactic components of a language of
grounding to a “universe” of grounds and operations on grounds, within a formal
system we will have a series of axioms that ﬁx the deductive behavior of the
syntax. The deﬁning equations which, via denotation, determine which function is
represented by the given non-primitive operational symbol, become, in the formal
system, equations that tell us how, in well-deﬁned contexts, the non-primitive
operational symbols can be eliminated, and how the terms in which they occur are
transformed into others that are provably identical. Using ∧as a case study, for the

6.1
General Overview
201
non-primitive operational symbol ∧E,i, we will have the axiom
R∧
E,i( I(T1, T2))
Ti
for T1 and T2 arbitrary terms of the language of grounding. We have said that
operations on grounds, as operations on grounds, are to be understood as functions
deﬁned on closed objects; however, the equations of a formal system are instead
conﬁgured as methods of syntactic transformation, and as such they concern only
the type of the term to which the symbol in question applies, regardless of whether
this term is closed or open. Now this has consequences on the functioning of the
identity predicate, and also on the type of restrictions we must explicitly request of
an equation for it to be included among the axioms of the calculus. More speciﬁcally,
since the operations on grounds are to be intended as functions, we can assume
that the equations that deﬁne them respect by default conditions such as identity of
domain and codomain between deﬁniendum and deﬁniens, and linearity with respect
to substitution—namely, (f (g))(∗/⋆) = f (g(∗/⋆)); however, since the equations of
a system ﬁx the deductive behavior of syntactic symbols, we must request explicitly
that deﬁniendum and deﬁniens have the same type, that the deﬁniens is not deﬁned
on more variables than the deﬁniendum, and that all instances of the equation make
it possible to move from the substitution on the whole term to the substitution on its
arguments.
The last set of rules includes the usual rules of a Gentzen’s natural deduction
system for standard constants in ﬁrst-order intuitionistic logic. The propositional
constants link formulas in the usual way, while the quantiﬁers act on three different
types of variables: individual variables, ground-variablesand functional variables,—
where it has to be noted that, while the ground-variables and the functional variables
explicitly occur in a term, the individual variables occur in the term when they occur
in the type of its ground variables.
If, thanks to the predicates Gr(. . . , −−−) and . . . ≡−−−, we can demonstrate
denotation properties of individual terms, or identities between/among two or more
terms, the fact that with logical constants it is possible to pass from atomic formulas
to logically complex formulas allows us to prove general properties of entire classes
of terms, or general properties of components of the alphabet of the enriched
language of grounding. In particular, it becomes possible to prove general relations
of denotation and identity, which in turn allows us to derive theorems that express
the conditions for denoting grounds of a certain structure—namely, an analogue of
the clauses (AtG)—(∃G) - and theorems that guarantee a valid deﬁnition of non-
primitive operational symbols.
In the theory of grounds, a pivotal role is played by the principle according to
which closed non-canonical terms, have the same denotation of—or, more generally,
can be reduced to—closed canonical terms. Let us take into account, for example,
the following words of Prawitz:
the fact that the operation ∧E,i always produces a ground for α1 when applied to a ground g
for α1 ∧α2 is not an expression of what ∧means [. . . ]. Instead it depends on what ∧means,

202
6
Systems of Grounding
and has to be established by an argument: ﬁrstly, in view of [the] clause [. . . ] specifying
the grounds for α1 ∧α2 exhaustively, g must have the form ∧I(g1, g2) [. . . ]. Secondly,
because of the identity condition, g1 and g2 are unique. Hence, according to the equations
that deﬁne the operations, ∧E,i(∧I(g1, g2)) = gi, where gi is a ground for asserting αi.
(Prawitz, 2015, 92)
Prawitz intends to prove that the non-primitive operation (denoted by the non-
primitive operational symbol) ∧E,i is well deﬁned; to do it, in addition to the
structural properties of primitive operations such as the identity conditions to which
they are subject, the Swedish logician makes use of the principle according to which
a ground for ⊢α1 ∧α2 has a certain form—and therefore a term that denotes such
ground has the same denotation of, or more generally is reducible to a canonical
term.
The proof of the fact that non-primitive operations (and, as a consequence, the
non-primitive operational symbols that denote them) are well deﬁned is one of the
key-points of the theory of grounds. It is this that leads Prawitz to the recognizabil-
ity/decidability problem that we have widely discussed. The circumstance depends
primarily on the aforementioned “fundamental task” that the theory of grounds
intends to fulﬁll—namely, to explain how and why deductively valid inferences can
force us epistemically. In doing so, we certainly cannot avoid making sure that the
operations on grounds the theory associates to the inferential passages are, in the
concrete examples, well-posed, and guaranteeing that this circumstance is actually
ascertainable - otherwise, no inferential agent would be willing to use that inference
or, even if it were used, the agent could never become aware of the correctness
of its deductive acts, which seems contrary to the idea that the agent could be
epistemically forced by such acts. As previously mentioned, this is a point which
profoundly distinguishes Prawitz’s theory of grounds from similar theories, such as
the Kreisel-Goodman theory of constructions and Martin-Löf’s intuitionistic type
theory. To borrow from Prawitz,
it is often said that the concept of proof should be deﬁned in such a way that it becomes
decidable whether something is a proof, but how this is to be achieved is seldom indicated,
except of course in the case of formal proofs. The terms that denote constructions are
supposed to be typed, and whether an expression has a type is decidable, but the rules for
typing already assume that the demands put on the deﬁned operations are fulﬁlled. (Prawitz,
2019b, 10)
As a result, it seems necessary to explicitly adopt Dummett’s fundamental assump-
tion (Dummett, 1991) among the rules of the class of formal systems for the theory
of grounds.
6.1.3
Three Kinds of Theorems
In proposing systems of grounding, we will focus mainly on three types of results
that these systems allow us to obtain. The three typologies can in turn be grouped

6.1
General Overview
203
into two groups: theorems of the system, and meta-theorems—that is, theorems
about the system.
Among the theorems of the system, we ﬁnd the analogues of the clauses (AtG)—
(∃G). In other words, we will show how the translations of these clauses in enriched
languages of grounding are derivable in systems of grounding on these languages.
The clause for ∧will become, for example the formula
Gr(ξα, α) and Gr(ξβ, β) ⇔Gr(∧I(ξα, ξβ), α ∧β)
derivable from the empty set of assumptions—and therefore universally quantiﬁ-
able.
Along with these results, we will have next the derivability of the well-
deﬁnedness of non-primitive operational symbols, with respect to the operational
type of the operation on grounds they are intended to represent. Again in the case of
∧, the formula
for all ξα1∧α2(if Gr(ξα1∧α2, α1 ∧α2) then Gr(∧E,i(ξα1∧α2), αi))
is a theorem of a system for an enriched Gentzen-language of grounding. It says
that, for each ground for ⊢α1 ∧α2, the application of ∧E,i to such ground produces
a ground for ⊢αi. The derivation, which makes an essential appeal both to the
deﬁning equation of ∧E,i and to Dummett’s fundamental assumption, follows that
of Prawitz, mentioned in the previous section.
These two types of theorems are essential to obtain the result of the other
typology, the theorem about systems. The meta-theorem is a deductive analogue
of the denotation theorem and consists in ensuring that, given
FV I(U) = {x1, . . . , xn} and FV T (U) = {ξα1, . . . , ξαm}
with U : β, we have
Gr(ξα1, α1), . . . , Gr(ξαm, αm) ⊢Gr(U(x1, . . . , xn, ξα1, . . . , ξαm), β).
In other words, in systems of grounding it is possible to prove that all terms denote,
in dependence on the assumption that the free ground-variables denote grounds for
the respective types. This also means that a term denotes a ground for a categorical
or general judgment or assertion, if it does not contain free occurrences of ground-
variables, and a ground for a hypothetical or general-hypothetical judgment or
assertion if it contains them. The meta-theorem just mentioned is therefore a sort
of correctness result for systems of grounding, comparable to the denotational
approach developed in the previous Chapter - recall that we had interpreted the
denotation theorem itself as a kind of correctness result of the languages of
grounding with respect to our “universe” of grounds and operations on grounds.
Finally, a last type of theorem internal to the system concerns the rewritability
of non-primitive operational symbols of a given language of grounding in terms of

204
6
Systems of Grounding
operational symbols of its sublanguage of grounding. In this case, we will provide
as an example the rewriting of disjunctive syllogism in the Gentzen-language in
Chap. 5—appropriately enriched.
Obviously, since the class of languages of grounding identiﬁed in the previous
chapter is inﬁnite, the class of systems of grounding on such languages will also be
inﬁnite. This is why we cannot actually show for each of the systems in our class that
they prove the three types of results mentioned above. The actual illustration will be
carried out inside of the only system of grounding that we will exemplify, which acts
on a Gentzen-language of grounding—appropriately enriched. Otherwise, we will
limit ourselves just to indicating how and where systems of grounding differ from
each other. In this regard, we have to bear in mind that languages of grounding may
differ with respect to the bases on which they operate, in which case they involve
different individual constants, or with respect to the non-primitive operational
symbols they are endowed with—or both. It follows that systems of grounding
are distinguishable from each other by the axioms concerning different individual
constants and also, primarily, for the rules concerning identity in connection with
non-primitives operational symbols—namely, the equations for such symbols and
the contextual substitution of identical.1
1 There remains another point to discuss, but an in-depth discussion of it would lead us too far
astray. This is the possibility of deriving, in systems of grounding, formulas of which the main
logical sign is negation. Since, as mentioned, in introducing the predicate Gr(. . . , −−−), we
require that it be applicable to a term T and to a formula α of the background language if, and only
if, T actually has type α in the language of grounding of the system, and since it is possible to prove
of each term—depending or not on assumptions—that it denotes a ground for that formula, or an
operation on grounds having that formula as codomain, then no negated atom will be derivable in
the system. Actually, the derivability of such formulas would be possible only in one of the two
following circumstances: (1) T does not have type α or (2) T has type α, but does not denote. Now,
considering (1), as we have seen, the violation of syntactic typing has unpleasant consequences; in
any case, even if we wanted to renounce the restriction, the derivation should be based on axioms
that tell us that a term cannot denote a ground or an operation on grounds if it does not have as type
the formula for which it denotes a ground, or which appears in its codomain. As for (2), instead, it
should in some way be possible to derive in the system that there is no succession of equations that
“reduce” the term to a canonical form U for each immediate subargument Z of which it holds that
Gr(Z, β), for some appropriate β; if, on the contrary, the term is open, we should prove that there
is a substitution that returns an instance of the term for which the previous circumstance holds.
Such a derivation should be based on rules that reason on “reductions”, rather than on terms; we
should verify whether these rules stand out necessarily on a higher order than that of the systems
of grounding we are here considering, or if it is somehow possible to “internalize” them, placing
them on the same level of rules for denotation and identity. Following this road would require the
introduction of non-denoting terms, and therefore the authorization of languages of grounding of
the type discussed in Sect. 5.1.2.2. Regarding the predicate . . . ≡−−−, similar requirements
apply as with the predicate Gr(. . . , −−−): if T and U have different types, the derivation of the
negations of T ≡U requires what was shown at point (1) above, while if T and U have the same
type, but denote different grounds, or different operations on grounds, or one of the two does not
denote, it could be indispensable to reason about the “reductions” of T and U, as in (2) above,
showing that there are none that go from one to the other. In any case, some negative formulas are
actually derivable, as we will see when discussing the derivability of the clause (⊥G). In general,

6.2
Deduction Over the Gentzen-Language
205
6.2
Deduction Over the Gentzen-Language
Our initial focus will be on a system of grounding for a Gentzen-language, as seen
in Chap. 5. We enrich this language as outlined above.
6.2.1
An Enriched Gentzen-Language
For the sake of greater clarity, we provide a precise deﬁnition of the Gentzen-
language over which we are going to work.
Deﬁnition 42 Given a background language L and an atomic base B over it, a
Gentzen-language of grounding Gen over B is determined by an alphabet AlGen
and a set of typed terms TERMGen. AlGen contains the following elements:
•
an individual constant δi naming the i-th  ∈DERS with no undischarged
assumptions and unbound variables (i ∈N);
•
typed-variables ξα
i (α ∈FORML, i ∈N);
•
operational symbols of the form F[ω] possibly binding individual and typed-
variables. We call primitive the following symbols:
– ∧I[α, β ▷α ∧β] (α, β ∈FORML)
– ∨I[αi ▷α1 ∨α2] (αi ∈FORML, i = 1, 2);
– →I[β ▷α →β] binding ξα
i (α, β ∈FORML, i ∈N);
– ∀I[α(xi) ▷∀yj(α(yj/xi)] binding xi (i, j ∈N, α ∈FORML) and
– ∃I[α(t) ▷∃xiα(xi)] (t ∈TERML, α ∈FORML, i ∈N)
We call non-primitive the following symbols:
– ∧E,i[α1 ∧α2 ▷αi] (αi ∈FORML, i = 1, 2);
– ∨I[α1 ∨α2, γ, γ ▷γ ] binding ξαi on the i + 1-th entry (αi, γ ∈FORML,
i = 1, 2, j ∈N);
– →E[α →β, α ▷β] (α, β ∈FORML);
– ∀E[∀xiα(xi) ▷α(t/xi)] (α ∈FORML, t ∈TERML, i ∈N);
– ∃E[∃xiα(xi), β ▷β] binding yj and ξα(yj)
k
on the second entry (α, β ∈
FORML, i, j, k ∈N) and
– ⊥α[⊥▷α] (α ∈FORML).
We omit operational types, subscripts and superscripts whenever possible. Below,
we indicate typing by a colon and binding of variables by writing the bound
variables soon after the operational symbol. TERMGen is the smallest set X such
that:
however, the derivation depends essentially on the use of the logical part, so that the formulas will
always be logically complex.

206
6
Systems of Grounding
•
δ : α ∈X where α is the conclusion of the  ∈DERS named by δ;
•
ξα : α ∈X;
•
T : α, U : β ⇒∧I(T, U) : α ∧β ∈X;
•
T : α1 ∧α2 ∈X ⇒∧E,i(T ) : αi ∈X;
•
T : αi ∈X ⇒∨I[αi ▷α1 ∨α2](T ) : α1 ∨α2 ∈X;
•
T : α ∨β, U : γ, Z : γ ∈X ⇒∨I ξα ξβ(T, U, Z) : γ ∈X;
•
T : β ∈X ⇒→Iξα(T ) : α →β ∈X;
•
T : α →β, U : α ∈X ⇒→E(T, U) : β ∈X;
•
U : α(x) ∈X ⇒∀I[α(x) ▷∀yα(y)]x(U) : ∀yα(y) ∈X;
•
T : ∀xα(x) ∈X →∀I[∀xα(x) ▷α(t)](T ) : α(t) ∈X;
•
T : α(t) ∈X ⇒∃I[α(t) ▷∃xα(x)](T ) : ∃xα(x) ∈X;
•
T : ∃xα(x), U : β ∈X ⇒∃Ey ξα(y)(T, U) : β ∈X and
•
T : ⊥∈X ⇒⊥α(T ) : α ∈X.
In the clause for ∀I, it must not hold that x ∈FV (β) for ξβ ∈FV T (U). In the
clause for ∃E, it must not hold that y ∈FV (γ ) for γ ̸= α(y) and ξγ ∈FV T (U).
In the clauses for ∀E and ∃I, t must be free for x in α(x). For the notation FV T ,
see below.
As previously said, we can take some notions as being deﬁned in a standard
inductive way on Gen. These are: the set S(T ) of the sub-terms of T ; the sets
FV I(T ) and BV I(T ) of, respectively, the free and bound individual variables of
T ; and the sets FV T (U) and BV T (U) of, respectively, the free and bound typed-
variables of U. Recall that U is said to be closed if, and only if, FV I(U) =
FV T (U) = ∅, and is open otherwise. Further to these, we must take into account
other common notions.
A substitution of x with t in T and a substitution of ξα with W : α in T are
functions TERMGen →TERMGen deﬁned in a usual inductive way. The inductive
basis is, respectively,
δ[t/x] = δ and ξα[t/x] = ξα[t/x]
δ[W/ξα] = δ and ξβ[W/ξα] =

ξβ
if α ̸= β
W
if α = β
But we must take into account possible clashes of bound variables, as well as
terms required to be free for individual variables in formulas. This can be achieved
by ensuring that individual variables are replaced in the types of bound typed-
variables, that bound variables are never replaced, that no variable becomes bound
after substitution, and that substitution respects the restrictions on terms free for
individual variables in formulas. So, for example,
→Iξα(U)[t/x] = →Iξα[t/x](U[t/x])

6.2
Deduction Over the Gentzen-Language
207
∀Iy(U)[t/x] =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∀Iy(U)
if x = y
∀Iy(U[t/x])
if x ̸= y, y /∈FV (t)
∀Iz((U[z/y])[t/x])
if x ̸= y, y ∈FV (t), for z /∈FV (t), z /∈FV I (U)
∃I (U)[t/x] =
⎧
⎨
⎩
∃I (U)
if U : α(s/y), ∃I (U) : ∃yα(y), x ∈FV (s), t not free for y in α(y)
∃I (U[t/x])
otherwise
∨E ξβ
i ξγ
j (U, Z1, Z2)[W/ξα
h ] =
•
∨E ξβ
i ξγ
j (U[W/ξα
h ], Z1, Z2)
if ξα
h = ξβ
i = ξγ
j ;
•
∨E ξβ
i ξγ
j (U[W/ξα
h ], Z1[W/ξα
h ], Z2)
if ξα
h ̸= ξβ
i , ξβ
i
/∈FV T (W), ξα
h = ξγ
j ;
•
∨E ξβ
s ξγ
j (U[W/ξα
h ], (Z1[ξβ
s /ξβ
i ])[W/ξα
h ], Z2)
if ξα
h ̸= ξβ
i , ξβ
i ∈FV T (W), ξα
h = ξγ
j , for ξβ
s /∈FV T (Z1), ξβ
s /∈FV T (W);
•
∨E ξβ
i ξγ
j (U[W/ξα
h ], Z1, Z2[W/ξα
h ])
if ξα
h = ξβ
i , ξα
h ̸= ξγ
j , ξγ
j /∈FV T (W);
•
∨E ξβ
i ξγ
s (U[W/ξα
h ], Z1, (Z2[ξγ
s /ξγ
j ])[W/ξα
h ])
if ξα
h = ξβ
i , ξα
h ̸= ξγ
j , ξγ
j ∈FV T (W), for ξγ
s /∈FV T (Z2), ξγ
s /∈FV T (W);
•
∨E ξβ
i ξγ
j (U[W/ξα
h ], Z1[W/ξα
h ], Z2[W/ξα
h ])
if ξα
h ̸= ξβ
i , ξβ
i
/∈FV T (W), ξα
h ̸= ξγ
j , ξγ
j /∈FV T (W);
•
∨E ξβ
s ξγ
j (U[W/ξα
h ], (Z1[ξβ
s /ξβ
i ])[W/ξα
h ], Z2[W/ξα
h ])
if ξα
h ̸= ξβ
i , ξβ
i
∈FV T (W), ξα
h ̸= ξγ
j , ξγ
j
/∈FV T (W), for ξβ
s
/∈FV T (Z1),
ξβ
s /∈FV T (W);
•
∨E ξβ
i ξγ
s (U[W/ξα
h ], Z1[W/ξα
h ], (Z2[ξγ
s /ξγ
j ])[W/ξα
h ])
if ξα
h ̸= ξβ
i , ξβ
i
/∈FV T (W), ξα
h ̸= ξγ
j , ξγ
j ∈FV T (W), for ξγ
s
/∈FV T (Z2),
ξγ
s /∈FV T (W);
•
∨E ξβ
s ξγ
t (U[W/ξα
h ], (Z1[ξβ
s /ξβ
i ])[W/ξα
h ], (Z2[ξγ
t /ξγ
j ])[W/ξα
h ])
if ξα
h ̸= ξβ
i , ξβ
i
∈FV T (W), ξα
h ̸= ξγ
j , ξγ
j ∈FV T (W), for ξβ
s
/∈FV T (Z1),
ξβ
s /∈FV T (W), ξγ
t
/∈FV T (Z2), ξγ
t
/∈FV T (W).
At this point we have two languages: a background language L and the Gentzen-
language Gen on a base B over L. Our next step is to enrich this latter, by adding
to it two binary predicates and the ﬁrst-order logical constants.
To avoid confusion between formulas of L and formulas of the enriched Gen,
we indicate the latter with capital letters of the Latin alphabet A, B, C, . . .. To

208
6
Systems of Grounding
avoid confusion between the logical constants of L and the logical constants of
the enriched Gen, we indicate the latter as follows: conjunction is ×, disjunction is
+, implication is ⊃, universal quantiﬁcation is  and existential quantiﬁcation is E.
Deﬁnition 43 Given Gen over B, an enriched Gentzen-language over B—
indicated with Gen+—is determined by an alphabet AlGen+, a set of typed terms
TERMGen+ and a set of formulas FORMGen+ - whose set of atomic formulas is
indicated with ATOMGen+. AlGen+ is such that AlGen ⊂AlGen+ and moreover it
contains
•
functional variables hα
i , fα
j (α ∈FORML, i, j ∈N);
•
binary relation symbols Gr and ≡;
•
logical constants ×, +, ⊃, , E and ⊥G.
As before, we will omit subscripts and superscripts whenever possible. TERMGen+
is the smallest set X such that TERMGen ⊂X and
•
hα
x(t) : α(t/x) ∈X (t ∈TERML);
•
fα(T ) : α ∈X (T ∈TERMGen).
plus recursive clauses for forming complex terms through the operational symbols
of Gen as indicated in deﬁnition 1. FORMGen+ is the smallest set X such that:
•
T : α ∈TERMGen ⇒Gr(T, α) ∈X;
•
T : α, U : α ∈X ⇒T ≡U ∈X;
•
⊥G ∈X;
•
A, B ∈X ⇒A□B ∈X (for □= ×, +, ⊃) and
•
A ∈X ⇒□ν A ∈X (for □= , E and ν = xi, ξα
i for i ∈N and α ∈FORML).
As usual, we set
¬GA
def
= A ⊃⊥G
where ¬G indicates the negation for formulas of Gen+. As previously discussed,
formulas of the kind Gr(T, α) are meant to express that T is a ground for α, or
an operation on grounds with co-domain α when T contains individual or typed
unbound variables. To avoid complex notation, such atomic formulas will simply
be written as T : α—note, however, that this notation must not be confused with
the meta-linguistic expression of equal form that has been used so far to indicate
that T is a term of type α in Gen. Formulas of the kind T
≡U are meant
to express that T and U denote equivalent grounds or equivalent operations on
grounds, with equivalence deﬁned as in Chap. 5—i.e., extensional identity, such
that two operations on grounds are equivalent when they produce identical values
from identical arguments, where identity is instead intensional.
Let us dig a little deeper into the multi-layered linguistic structure being dealt
with here. First, note that this is a three-level structure, made up of: (a) a background
language L that provides types for the terms of (b) the language Gen, which contains
terms whose properties are expressed by the formulas of (c) the language Gen+.

6.2
Deduction Over the Gentzen-Language
209
The three levels are meant to refer to two domains: (1) the objects that the terms
of L stand for and (2) the grounds and operations on grounds, named by terms of
Gen, that justify assertions about (1). The individual variables of Gen (and thus of
Gen+) range over the elements of (1), whereas the typed-variables of Gen (and thus
of Gen+) range over some of the (terms standing for) elements of (2). Additionally,
Gen+ contains typed functional variables. A functional variable of the form hα
x is
meant to range over terms that are to stand for operations on grounds of operational
type  ▷α involving x, for x not occurring free in any element of —thus, hα
x(t)
will be any such operation applied to the individual that t stands for. Instead, a
functional variable of the form fα is meant to range over terms that are to stand
for operations on grounds of operational type  ▷α—so fα(Z) will be any such
operation applied to the ground or operation on grounds that Z stands for. Therefore,
typed functional variables are meant to account for (properties of) operations that
are suitable for the binding of individual and of typed-variables—notions that are
otherwise inexpressible in Gen.
Notably, all kinds of variables - individual, typed- or functional—can be bound,
indifferently,by the quantiﬁers of Gen+. The reason for this is that, when expressing
properties of (terms standing for) grounds and operations on grounds, we might ﬁnd
ourselves needing to quantify both over individuals denoted by terms of L, and over
(terms standing for) grounds and operations on grounds denoted by terms of Gen
and Gen+. For example, it must be possible to express the fact that
•
an operation on grounds of operational type α(x), whenever applied to an
individual k, produces a ground for α(k) (and that, given a term that denotes
this operation, whenever we replace x with a term denoting k, we obtain a term
that denotes a ground for α(k));
•
an operation on grounds of operational type α ▷β, whenever applied to a
ground g for α, produces a ground for β (and that, given a term that denotes
this operation, whenever we replace a typed-variable ξα with a term denoting a
ground for α, we obtain a term that denotes a ground for β);
•
an operation on grounds of operational type (α ▷β) ▷γ , whenever applied to
an operation on grounds of operational type α ▷β, produces a ground for γ (and
that, given a term that denotes this operation, whenever we replace a functional
variable fβ applied to a typed-variable ξα—i.e. fβ(ξα)—with a term denoting
an operation on grounds of operational type α ▷β, we obtain a term that denotes
a ground for γ ).
With this established, we can provide some examples of reading of formulas of
Gen+. All the formulas we exemplify here are provable in the system developed in
the next Section. The formula
ξα∨β(ξα∨β : α∨β ⇔Eξαξβ((ξα∨β ≡∨I(ξα)×ξα : α)+(ξα∨β ≡∨I(ξβ)×ξβ : β)))

210
6
Systems of Grounding
is to be read as follows (where ⇔is the standard abbreviation of (. . . ⊃−−−) ×
(−−−⊃. . .) for bi-implication):
for every term ξα∨β (i.e. for every term of type α ∨β), ξα∨β denotes a ground for α ∨β
if, and only if, for some term ξα (i.e. for some term of type α), for some term ξβ (i.e. for
some term of type β), ξα∨β is equivalent to ∨I(ξα) and ξα denotes a ground for α, or ξα∨β
is equivalent to ∨I(ξβ) and ξβ denotes a ground for β.
In other words, the formula says that every (term denoting a) ground for α ∨β (is
equivalent to a term that) begins with the primitive operation ∨I applied either to a
(term denoting a) ground for α or to a (term denoting a) ground for β. The formula
ξ∀xα(x)(ξ∀xα(x) : ∀xα(x) ⇔Ehα(x)(ξ∀xα(x) ≡∀Ix(hα(x)(x)) × x(hα(x)(x) : α(x))))
is to be read as follows (again, where ⇔is again the standard abbreviation of (. . . ⊃
−−−) × (−−−⊃. . .) for bi-implication):
for every ξ∀xα(x) (i.e. for every term of type ∀xα(x)), ξ∀xα(x) denotes a ground for ∀xα(x)
if, and only if, there is term hα(x) where x does not occur free in free typed-variables (i.e.
there is term of type α(x) where x does not occur free in free typed-variables) such that
ξ∀xα(x) is equivalent to ∀Ix(hα(x)(x)) and, for every x, hα(x)(x) denotes a ground for α(x).
Thus, the formula states that every (term denoting a) ground for ∀xα(x) (is
equivalent to a term that) begin with the primitive operation ∀I applied to a (term
denoting an) operation on grounds of operational type α(x)—with x bound. Finally,
the formula
ξ∃xα(x)fβ(ξ∃xα(x) : ∃xα(x) × xξα(x)(ξα(x) : α(x) ⊃fβ(ξα(x)) : β) ⊃
∃E x ξα(x)(ξ∃xα(x), fβ(ξα(x))) : β)
is to be read as follows:
for every term ξ∃xα(x) (i.e. for every term of type ∃xα(x)), for every term fβ possibly
involving free typed-variables (i.e. for every term of type β possibly involving free typed-
variables), if ξ∃xα(x) denotes a ground for ∃xα(x) and if, for every x, for every term ξα(x)
(i.e. for every term of type α(x)), if ξα(x) denotes a ground for α(x) then fβ(ξα(x)) denotes
a ground for β, then ∃E x ξα(x)(ξ∃xα(x), fβ(ξα(x))) denotes a ground for β.
Thus the formula says that the operation ∃E yields a (term denoting a) ground for β
whenever applied to a (term denoting a) ground for ∃xα(x) and to a (term denoting
an) operation on grounds of operational type α(x) ▷β—binding x and ξα(x) on the
second entry.
Finally, we close this Section considering some standard notions around free and
bound variables in terms and formulas, as well as substitution of variables with
terms in formulas. First, the set S(T ) of the sub-terms of T is deﬁned in a usual
inductive way on the “traditional” cases. For the functional variables we have
S(hα(t)) = {hα(t)} and S(fα(T )) = S(T ) ∪{fα(T )}.

6.2
Deduction Over the Gentzen-Language
211
The sets FV I(U) and BV I(U) of respectively the free and bound individual
variables of U are deﬁned in a usual inductive way on the “traditional” cases. For
the functional variables we have
⋆I(hα
x(t)) = ⋆(t) ∪⋆(α(t/x)) and ⋆I (fα(T )) = ⋆I(T ) ∪⋆(α)
where ⋆= FV, BV . The sets FV T (U) and BV T (U) of respectively the free
and bound typed-variables of U are deﬁned as usual in an inductive way on the
“traditional” cases. For the functional variables we have
⋆T (hα(t)) = ∅and ⋆T (fα(U)) = ⋆T (U)
where ⋆= FV, BV . Finally, the sets FV F (U) and BV F (U) of respectively the
free and bound functional variables of U are deﬁned inductively beginning from
the base
⋆F (hα(t)) = {hα} and ⋆F (fα(T )) = {fα}
where ⋆= FV, BV . A term U is closed if, and only if, FV I(U) = FV T (U) =
FV F(U) = ∅, and open otherwise.
A substitution of x with u in T is a function TERMGen+ →TERMGen+ deﬁned
in a usual inductive way in the “traditional cases”. For the functional variables we
have
hα(t)[u/x] = hα[u/x](t[u/x]) and fα(T )[u/x] = fα[u/x](T [u/x]).
A substitution of ξα with U : α in T is a function TERMGen+ →TERMGen+ deﬁned
in a usual inductive way for the “traditional cases”. For the functional variables we
have
hα(t)[U/ξα] = hα(t) and fα(Z)[U/ξα] = fα(Z[U/ξα]).
A substitution of hβ
x with U : β in Z is a function TERMGen+ →TERMGen+ that is
deﬁned only when x /∈FV (α) for ξα ∈FV T (U). It works in the usual inductive
way on the “traditional cases” while, for the functional variables, we have
hβ
x(t)[U/hβ
x] = U[t/x] and fα(T )[U/hβ
x] = fα(T )
A substitution of fα with U : α in T is an always deﬁned function TERMGen+ →
TERMGen+. It runs in a usual inductive way in the “traditional cases”, while for the
functional variables we have
hα(t)[U/fα] = hα(t) and fα(Z)[U/fα] = U

212
6
Systems of Grounding
In the last three cases, the following restriction is applied: the substitution is deﬁned
only when U ∈TERMGen. This is to avoid cases like
fα(ξα)[fα(ξα)/ξα] = fα(ξα[fα(ξα)/ξα]) = fα(fα(ξα))
which would produce ill-formed terms.
The sets FV I(A) and BV I(A) of respectively the free and bound individual
variables of A can be deﬁned in the usual inductive way, starting from
⋆I(T : α) = FV I(T ) and ⋆I (T ≡U) = ⋆I(T ) ∪⋆I(U)
where ⋆= FV, BV . Note that, if T : α, then ⋆(α) ⊆⋆I(T ) with ⋆= FV, BV .
Bound individual variables in quantiﬁed formulas give
BV I(□ν A) =

BV I(A) ∪{ν}
if ν is an individual variable
BV I(A)
otherwise
The sets FV T (A) and BV T (A) of respectively the free and bound typed-variables
of A, and the sets FV F (A) and BV F(A) of respectively the free and bound
functional variables are deﬁned analogously. We say that A is closed if, and only if,
FV I(A) = FV T (A) = FV F (A) = ∅, and that it is open otherwise.
A substitution of x with t in A is a function FORMGen+ →FORMGen+ deﬁned in
the usual inductive way. We specify solely that, in the case of quantiﬁed formulas,
we have
(□ν A)[t/x] =
⎧
⎪⎪⎨
⎪⎪⎩
□ν[t/x] A[t/x]
if ν is a typed- or functional variable
□ν A[t/x]
if ν is an individual variable and ν ̸= x
□ν A
if ν is an individual variable and ν = x
A substitution of ξα with T : α in A is a function FORMGen+ →FORMGen+ deﬁned
in a usual inductive way. We specify solely that, in the case of quantiﬁed formulas,
we have
(□ν A)[T/ξα
i ] =

□ν A[T/ξα
i ]
if ν ̸= ξα
i
□ν A
if ν = ξα
i
A substitution of hβ
x with U : β in A is a function FORMGen+ →FORMGen+ that
is deﬁned only when x /∈FV (α) for ξα ∈FV T (U). It is deﬁned in the usual
inductive way. In the case of quantiﬁed formulas, it behaves like the substitutions of
typed-variables in formulas. The same holds for a substitution of fα with T : α in
A, except that this substitution is deﬁned for every T : α.
The notion of t free for x in A can be speciﬁed in the usual inductive way. As
for quantiﬁed formulas □ν A, we must have that either ν = x or that ν ̸= x,

6.2
Deduction Over the Gentzen-Language
213
ν /∈FV (t) and t free for x in A. Similarly, the notion of U : α free for ξα or hα or
fα in A is deﬁned in the usual inductive way; for quantiﬁed formulas □ν A, either
ν = ξα, hα, fα or ν ̸= ξα, hα, fα, ν /∈FV I(U), ν /∈FV T (U), ν /∈FV F (U) and
U free for ξα, hα, fα in A.
6.2.2
A System for the Enriched Gentzen-Language
We are now in the position to introduce a formal system of grounding for Gen+.
There are three kinds of rules in the system:
•
type introductions and type eliminations;
•
equivalence rules, i.e.
– standard rules for reﬂexivity, symmetry and transitivity, plus preservation of
denotation;
– rules saying that operational symbols applied to equivalent terms yield
equivalent terms (and vice versa in the canonical cases);
– equations for computing non-canonical terms whose outermost operations are
applied to (relevant) canonical terms;
•
ﬁrst-order logical rules.
We will show that three types of results are provable within the system:
•
theorems translating the ground-clauses of Sect. 6.2 in Gen+;
•
theorems showing that the non-primitive operational symbols of Gen+ are well-
deﬁned;
•
theorems showing that if we add a non-primitive operational symbol φ to Gen+,
thereby obtaining an expansion Gen+
1 of Gen+, and if we expand our system 
by adding to it an equation associated to φ that allows a proof that φ stands for a
function of type  ▷α (alongside other appropriate equivalence rules), thereby
obtaining an expansion 1 of , and ﬁnally if  ⊢α is derivable in ﬁrst-order
intuitionistic logic, then for every term T of Gen+
1 we can ﬁnd a term U of Gen+
such that T ≡U is provable in 1.
Our ﬁnal step will be to prove a theorem about the system—namely, if U is a
term of Gen of type β and FV T (U) = {ξα1, . . . , ξαn}, then U : β is provable
in the system under the assumptions ξαi : αi (1 ≤i ≤n) only. Recall that
U : α is meant to express that U is a ground for β if U is closed, and an
operation on grounds of operational type α1, . . . , αn ▷β deﬁned on m individuals if
FV I(U) = {x1, . . . , xm} and FV T (U) = {ξα1, . . . , ξαn}. Then, the meta-theorem
can be interpreted as essentially being a correctness result for our system restricted
to terms of Gen—similar to the denotation theorem of Chap. 5.

214
6
Systems of Grounding
6.2.2.1
Typing Rules
Typing rules can be divided into introductions and eliminations. Type eliminations
come in the form of generalized elimination rules (see Schroeder-Heister, 1984a,
b). They are in line with Dummett’s so-called fundamental assumption—i.e. every
closed proof of α must reduce to a closed canonical proof of α (see Dummett, 1991).
The rules are given in a Gentzen’s natural deduction format.
Type Introductions
C, for every individual constantδ naming a Δ ∈DERS with conclusion α
δ : α
T : α
U : β
∧I
∧I(T, U) : α ∧β
T : αi
∨I, (i = 1, 2)
∨I[αi ▷α1 ∨α2](T) : α1 ∨α2
[ξα
i : α]
...
T(ξα
i ) : β
→I
→Iξα
i (T(ξα
i )) : α →β
T(xi) : α(xi)
∀I
∀Iy(T(y/xi)) : ∀yα(y/xi)
T : α(t/x)
∃I
∃I[α(t/x) ▷∃xα(x)](T)
We have the following restrictions: in →I, ξα
i must not occur free in undischarged
assumption other than ξα
i : α; and in ∀I, xi must not occur free in any undischarged
assumption.2
2 Although seemingly non-standard, the restriction on →I actually turns out to be necessary.
Without it, we could prove in the system such results as Efβ(→Iξα(∧I(ξα, fβ(ξα))) : α →
α ∧β) from the assumption EfβEξα(fβ(ξα) : β)—i.e., morally, from the fact that there is an
operation fβ that for some ground ξα for α yields a ground fβ(ξα) for β, we can conclude that
there is an operation fβ such that →Iξα(∧I(ξα, fβ(ξα))) is a ground for α →α ∧β, which
intuitively does not hold because we have to grant that fβ yields a ground for β for every ground ξα
for α. Also, note the difference between the restriction on term formation for ∀I, and the restriction
on type introduction for ∀I. In particular, we can have ∀Ix(hα(x)(x)) as a term, but we cannot
prove ∀Ix(hα(x)(x)) : ∀xα(x) from the assumption hα(x)(x) : α(x). Without this restriction, e.g.,
x(hα(x)(x) : α(x)) would be provable from Ex(hα(x)(x) : α(x))—i.e. from the fact that, for
some x, hα(x)(x) denotes a ground for α(x) we derive that this holds for all x. In general, that a

6.2
Deduction Over the Gentzen-Language
215
Type Eliminations
T : α ∧β
[T ≡∧I(ξα, ξβ)]
[ξα : α]
[ξβ : β]
...
A D∧
A
T : α ∨β
[T ≡∨I(ξα)]
[ξα : α]
...
A
[T ≡∨I(ξβ)]
[ξβ : β]
...
A D∨
A
The two minor premises A are the conclusions of two distinct sub-derivations:
the leftmost (possibly) depends on the assumptions T ≡∨I(ξα) and ξα : α—
discharged only in this sub-derivation—whereas the rightmost (possibly) depends
on the assumptions T ≡∨I(ξβ) and ξβ : β - discharged only in this sub-derivation.
T : α →β
[T ≡→Iξα(f β(ξα))]
[Πξα(ξα : α ⊃f β(ξα)) : β))]
...
A D→
A
term enjoying given properties denotes a ground for a given formula is something that the system
has to prove, although what one proves is exactly the intended meaning of the term—in this case,
an arbitrary term with no free individual variable x occurring free in typed-variables that occur free
within it is required to stand for an operation on grounds of operational type  ▷α for x not free
in any element of , but this requirement must be proved by the system. The individual variable
x is free in hα(x)(x) : α(x), even if hα(x)(x) should stand for operations on grounds—applied
to x—of operational type  ▷α for x not free in any element of . There is thus a difference
between an individual variable being free in a term or in a formula, and an individual variable
being suitable for quantiﬁcation through ∀I. The same difference applies, in Prawitz’s semantics
of valid arguments, to closed valid arguments for α(x), to which we can apply licitly Gentzen’s
introduction for ∀, but where x may occur free—and eventually be replaced by other individual
terms before quantiﬁcation. This observations also apply to -introduction displayed below—
labelled with (I )—which undergoes the same restrictions as those required for the ∀I-rule.

216
6
Systems of Grounding
T : ∀xα(x)
[T ≡∀Ix(hα(x)(x))]
[Πx(hα(x)(x) : α(x))]
...
A D∀
A
T : ∃xα(x)
[T ≡∃I(ξα(x))]
[ξα(x) : α(x)]
...
A D∃
A
There are usual restrictions on proper variables, e.g.: in D→, fβ must not occur free
in any undischarged assumption on which A depends, other than those discharged
by the rule; in D∀, for every t ∈TERML, hα(t) must not occur free in any
undischarged assumption on which A depends (in what we obtain by replacing x
with t in the derivation of the minor premise), other than those discharged by the
rule (with substitution of x with t); in D∃, x and ξα(x) must not occur free in any
undischarged assumption on which A depends other than those discharged by the
rule. Additionally, there is a rule for eliminating the type ⊥:
T :
G
Finally, it is worth noting that the term T mentioned in the major premise of
the type elimination rules does not need to begin with a primitive operation—for
example, it might be merely a typed-variable. Nonetheless, the leftmost assumption
of the minor premises equates T with a term that begins with a primitive operation.
The type elimination rules are inspired by Dummett’s fundamental assumption. The
idea behind the rules is as follows: if T denotes a ground for a formula α with main
logical sign s—what the major premise expresses—then T must be equivalent to a
term that directly denotes a ground for α, i.e. a term where the primitive operation sI
is applied to grounds for the sub-formula(s) of α—what the assumptions the minor
premise depends on express.
6.2.2.2
Equivalence Rules
We can divide equivalence rules into standard rules for an equivalence relation,
plus one for preservation of denotation; rules for applying operational symbols
to equivalent terms; and equations ruling the computation of the non-primitive
operational symbols on (relevant) canonical arguments.

6.2
Deduction Over the Gentzen-Language
217
Standard Rules
≡R
T ≡T
T ≡U ≡S
U ≡T
U ≡Z
Z ≡W ≡T
U ≡W
T ≡U
U : α ≡P
T : α
Applications to Equivalent Terms
T ≡U
V ≡W
≡∧
1
∧I(T, V ) ≡∧I(U, W)
∧I(T1, T2) ≡∧I(U1, U2) ≡∧
2,i, i = 1, 2
Ti ≡Ui
T ≡U
≡∧
3,i, i = 1, 2
∧E,i(T) ≡∧E,i(U)
T ≡U
≡∨
1
∨I(T) ≡∨I(U)
∨I[αi ▷α1 ∨α2](T) ≡∨I[αi ▷α1 ∨α2](U) ≡∨
2
T ≡U
T ≡U
Z1 ≡Z2
W1 ≡W2
≡∨
3
∨Eξα
i ξβ
i (T, Z1, W1) ≡∨Eξα
i ξβ
i (U, Z2, W2)
T ≡U
≡→
1
→Iξα
i (T) ≡→Iξα
i (U)
→Iξα(T) ≡→Iξα(U) ≡→
2
T ≡U
T1 ≡T2
U1 ≡U2
≡→
3
→E(T1, U1) ≡→E(T2, U2)
T ≡U
≡∀
1
∀Ixi(T) ≡∀Ixi(U)
∀Ix(T) ≡∀Ix(U) ≡∀
2
T ≡U
T ≡U
≡∀
3
∀E(T) ≡∀E(U)
T ≡U
≡∃
1
∃I(T) ≡∃I(U)
∃I(T) ≡∃I(U) ≡∃
2
T ≡U
T ≡U
Z ≡V
≡∃
3
∃E xi ξα(xi)
j
(T, Z) ≡∃E xi ξα(xi)
j
(U, V )

218
6
Systems of Grounding
Equations
R∧,i, i = 1, 2
∧E,i(∧I(T1, T2)) ≡Ti
R∨,i, i = 1, 2
∨Eξα2ξα2(∨I[αi ▷α1 ∨α2)(T), U1(ξα1), U2(ξα2)) ≡Ui(T)
R→
→E(→Iξα(T(ξα)), U) ≡T(U)
R∀
∀E[∀xα(x) ▷α(t/x)](∀Ix(T(x))) ≡T(t)
R∃
∃E x ξα(x)(∃I[α(t/x) ▷∃xα(x)](T), U(x, ξα(x))) = U(t, T)
6.2.2.3
Logic
Logical rules are reminiscent of the standard Gentzen natural deduction rules for
ﬁrst-order intuitionistic logic. Here, we provide only those for the quantiﬁers and
the explosion principle, as the remaining rules are straightforward.
A(ν)
(ΠI)
ΠμA(μ/ν)
ΠνA(ν) (ΠE)
A(τ/ν)
A(τ/ν) (
I)
νA(ν)
νA(μ/ν)
[A(μ)]
...
B (
E)
B
⊥G
(⊥G)
A
In these rules, ν, μ are individual, typed- or functional variables, whereas τ
represents a term of L or a term of Gen. We have standard restrictions on proper
variables, plus the following restriction for (E) and (EI): if ν is ξα or fα, τ is of
type α; if ν is hβ
x, τ is of type β and such that x /∈FV (α) for ξα ∈FV T (τ).

6.2
Deduction Over the Gentzen-Language
219
6.2.2.4
Derivations and Remarks
Our system will be referred to as GG, i.e. Gentzen-grounding. As usual, we indicate
with G ⊢GG A the fact that A is derivable in GG from a set of assumptions G.
Occasionally, we use DERGG to indicate the set of the derivations of GG. We brieﬂy
highlight the following two points.
First, it is worth noting that the type elimination rules are inter-derivable with
elimination rules of a more straightforward structure. For example, D∃is inter-
derivable with the following rule
T :
xα(x)
x ξα(x)(T
I[α(x)
xα(x)](ξα(x))
ξα(x) : α(x))
Our choice to adopt generalized elimination rules was to make the proof of
normalization in a Section below a little easier.
Second, it may be asked why we do not have the inverses of the equivalence rules
for equivalent applications of non-primitive operational symbols. Indeed, such rules
are not plausible; as an example, with such rules we could derive the following
R∧,1
∧E,1(∧I(ξα, ξβ)) ≡ξα
R∧,1
∧E,1(∧I(ξα, ξγ)) ≡ξα
≡S
ξα ≡∧E,1(∧I(ξα, ξγ)) ≡T
∧E,1(∧I(ξα, ξβ)) ≡∧E,1(∧I(ξα, ξγ))
I(ξα, ξβ)
I(ξα, ξγ)
This is clearly undesirable when β ̸= γ —and also violates the restriction for the
well-forming of formulas with ≡as main sign.
6.2.3
Some Theorems Within the System
The following theorems can be proved within the system.
6.2.3.1
Theorems Expressing the Ground-Clauses
We indicate bi-conditional with ⇔, and as usual we put A ⇔B
def
= (A ⊃B)×(B ⊃
A). The following results hold:
(∧)* ⊢GG ξα∧β(ξα∧β : α ∧β ⇔EξαEξβ(ξα∧β ≡∧I(ξα, ξβ) × (ξα : α × ξβ :
β)))
(∨)* ⊢GG ξα∨β(ξα∨β : α ∨β ⇔Eξαξβ((ξα∨β ≡∨I(ξα) × ξα : α)+
(ξα∨β ≡∨I(ξβ) × ξβ : β)))

220
6
Systems of Grounding
(→)* ⊢GG ξα→β(ξα→β : α →β ⇔Efβ(ξα→β ≡→Iξα(fβ(ξα))×
ξα(ξα : α ⊃fβ(ξα) : β)))
(∀)*
⊢GG ξ∀xα(x)(ξ∀xα(x) : ∀xα(x) ⇔Ehα(x)(ξ∀xα(x) ≡∀Ix(hα(x)(x))×
x(hα(x)(x) : α(x))))
(∃)*
⊢GG ξ∃xα(x)(ξ∃xα(x) : ∃xα(x) ⇔ExEξα(x)(ξ∃xα(x) ≡
∃I(ξα(x)) × ξα(x) : α(x)))
Clearly, (k)* can be considered as a translation in Gen+ of the ground-clause (k) of
Chap. 4. Given type introductions, it is of no surprise that the clauses are provable.
Now we consider the derivation of (∃)*, as an example of the derivations in GG
that establish the above theorems. The left-to-right implication can be proved by
applying (⊃I) to the following derivation:
ξ∃xα(x) : ∃xα(x)
1
[ξ∃xα(x) ≡∃I(ξα(x))]
2
[ξα(x) : α(x)] (×I)
ξ∃xα(x) ≡∃I(ξα(x)) × ξα(x) : α(x)
(
I)
ξα(x)(ξ∃xα(x) ≡∃I(ξα(x)) × ξα(x) : α(x))
(
I)
x ξα(x)(ξ∃xα(x) ≡∃I(ξα(x)) × ξα(x) : α(x)) D∃, 1, 2
x ξα(x)(ξ∃xα(x) ≡∃I(ξα(x)) × ξα(x) : α(x))
The right-to-left implication can be proved by applying (EE) twice and (⊃I) once
to the following derivation from the assumption ExEξα(x)(ξ∃xα(x) ≡∃I(ξα(x)) ×
ξα(x) : α(x))
ξ∃xα(x) ≡∃I(ξα(x)) × ξα(x) : α(x) (×E,2)
ξα(x) : α(x)
∃I
∃I(ξα(x)) : ∃xα(x)
ξ∃xα(x) ≡∃I(ξα(x)) × ξα(x) : α(x) (×E,1)
ξ∃xα(x) ≡∃I(ξα(x)) ≡P
ξ∃xα(x) : ∃xα(x)
Then we take the two derivations above and introduce ×. Finally, we universally
quantify over ξ∃xα(x) by (I).
The ground-clauses of Chap. 4 are usually accompanied by two additional
clauses: one stating that every closed derivation in the atomic system S of an atomic
base B with conclusion α is a ground for α on B, and the other stating that every
B is consistent, i.e. that ⊬S ⊥- or, as is more commonly said, that there is no
ground on B for ⊥. The ﬁrst of these clauses is clearly provable in GG because
we have assumed as an axiom that, for every individual constant δ naming a closed
 ∈DERS with conclusion α, δ : α holds. The second is also provable - i.e. we have
⊢GG ¬Eξ⊥(ξ⊥: ⊥). Finally, it is worth noting that a kind of introduction for ⊥α is
easily provable, i.e. ⊢GG ξ⊥(ξ⊥: ⊥⊃⊥α(ξ⊥) : α).

6.2
Deduction Over the Gentzen-Language
221
6.2.3.2
Checking the Equations
The purpose of an operational symbol’s type is twofold: not only does it provide a
“syntactic” typing, but it also indicates that the symbol itself denotes an operation
on grounds of that type. Thus, if T , for example, stands for a ground for α1 ∧α2, we
expect ∧E,i(T ) to denote a ground for αi (i = 1, 2). That the operational symbols
of Gen actually behave in this way should be granted by the equations associated to
them in GG. Therefore, we might expect GG to prove that the operational symbols are
well-deﬁned, which is indeed the case. In other words, the following results hold:
(∧w) ⊢GG ξα1∧α2(ξα1∧α2 : α1 ∧α2 ⊃∧E,i(ξα1∧α2) : αi)
(∨w) ⊢GG ξα1∨α2fβ
1fβ
2(((ξα1∨α2 : α1 ∨α2 × ξα1(ξα1 : α1 ⊃fβ
1(ξα1) :
β))× ξα2(ξα2 : α2 ⊃fβ
2(ξα2) : β)) ⊃∨Eξα1ξα2(ξα1∨α2, fβ
1(ξα1),
fβ
2(ξα2)) : β)
(→w) ⊢GG ξα→βξα(ξα→β : α →β × ξα : α ⊃→E(ξα→β, ξα) : β)
(∀w)
⊢GG ξ∀xα(x)(ξ∀xα(x) : ∀xα(x) ⊃∀E(ξ∀xα(x)) : α(t))
(∃w)
⊢GG ξ∃xα(x)fβ(ξ∃xα(x) : ∃xα(x)×xξα(x)(ξα(x) : α(x) ⊃fβ(ξα(x)) :
β) ⊃∃E x ξα(x)(ξ∃xα(x), fβ(ξα(x))) : β)
Clearly, (kw) proves that kE yields something that is a ground for the co-domain of
kE, under the assumption that the arguments to which kE is applied are grounds for
the domain of kE—recall that T : α is just an abbreviation for Gr(T, α).
As an example, we prove (∃w), that we abbreviate with ξ∃xα(x)fβT h∃. First,
we eliminate conjunction on an assumption that corresponds to the antecedent of
T h∃- we indicate this derivation with 0 -
1
[ξ∃xα(x) : ∃xα(x) × ΠxΠξα(x)(ξα(x) : α(x) ⊃f β(ξα(x)) : β)] (×E,1)
ξ∃xα(x) :
xα(x)
We now want to apply type elimination to the conclusion of 0, so to obtain the
consequent of T h∃. So we prove that the application of ∃I to ξ∃xα(x) and to an
arbitrary term in the free individual variable x and in the free typed-variable ξα(x) is
equal to an application of ∃I to an arbitrary canonical term and to the same arbitrary
term as above—we call this derivation 1
1—
2
[ξ∃xα(x) ≡∃I(ξα(x))]
≡R
f β(ξα(x)) ≡f β(ξα(x))
≡∃
3
E x ξα(x)(ξ∃xα(x), f β(ξα(x)))
E x ξα(x)( I(ξα(x)), f β(ξα(x)))
Then we apply the equation for ∃I to prove via transitivity that the application
of ∃I to ξ∃xα(x) and to an arbitrary term in the free individual variable x and in
the free typed-variable ξα(x) is equivalent to something of type β, i.e. we have the

222
6
Systems of Grounding
derivation 2
1
Δ1
1
R∃
∃E x ξα(x)(∃I(ξα(x)), f β(ξα(x)))) ≡f β(ξα(x)) ≡T
E x ξα(x)(ξ∃xα(x), f β(ξα(x))
f β(ξα(x))
and the derivation 2
1
[ξ∃xα(x) : ∃xα(x) × ΠxΠξα(x)(ξα(x) : α(x) ⊃f β(ξα(x)) : β)]
(×E,2)
ΠxΠξα(x)(ξα(x) : α(x) ⊃f β(ξα(x)) : β)
(ΠE)
Πξα(x)(ξα(x) : α(x) ⊃f β(ξα(x)) : β)
(ΠE)
ξα(x) : α(x) ⊃f β(ξα(x)) : β
3
[ξα(x) : α(x)]
(⊃E)
f β(ξα(x)) : β
Finally, via preservation of denotation, we can apply type elimination
Δ0
Δ2
1
Δ2
≡P
∃E x ξα(x)(ξ∃xα(x), f β(ξα(x))) : β D∃, 2, 3
∃E x ξα(x)(ξ∃xα(x), f β(ξα(x))) : β (
I), 1
Th
Then one applies (I) twice on fβ and on ξ∃xα(x) to obtain ξ∃xα(x)fβT h∃.
6.2.3.3
Derived Types
Consider an expansion of Gen+, which we term Gen++, obtained by adding a non-
primitive operational symbol DS[α ∨β, ¬α ▷β]. An expansion GG+ of GG can be
then constructed by adding the following equivalence rules
T ≡U
V ≡W
≡DS
DS(T, V )
DS(U, W)
RDS
DS( I[β ▷α
β](T), U)
T
In GG+, we are able to prove that every term of Gen++ can be rewritten as a term
of Gen+, owing to the following result:
⊢GG+ ξα∨β ξ¬α (ξα∨β : α ∨β × ξ¬α : ¬α ⊃DS(ξα∨β, ξ¬α) ≡
∨Eξαξβ(ξα∨β, ⊥β(→E(ξα, ξ¬α)), ξβ)).
Similar results constitute the “deductive” version of the conservativity property
between languages of grounds deﬁned in Chap. 5—see Theorem 40. The derivation

6.2
Deduction Over the Gentzen-Language
223
is as follows (using the theorems discussed in the previous Section, and abbrevi-
ating the theorem to be proved as ξα∨βξ¬αT hDS). Let 0
1 be the following
derivation:
2
[ξα∨β : α ∨β × ξ¬α : α] (×E,2)
ξ¬α : ¬α
3
[ξα : α] (
I)
ξ¬α :
α
ξα : α
The conclusion of 0
1 can be used as minor premise in an application of (⊃E),
whose major premise is obtained from (→w), in such a way as to obtain a term of
type ⊥, so that we can apply the explosion principle to derive T hF —i.e. we have
the following derivation 1
Δ0
1
Πξ¬αΠξα(ξ¬α : ¬α × ξα : α ⊃→E(ξ¬α, ξα) : ⊥) (ΠE)
Πξα(ξ¬α : ¬α × ξα : α ⊃→E(ξ¬α, ξα) : ⊥) (ΠE)
ξ¬α : ¬α × ξα : α ⊃→E(ξ¬α, ξα) : ⊥(⊃E)
→E(ξ¬α, ξα) : ⊥⊥
⊥G
(⊥G)
F(ξα∨β, ξ¬α)
Eξαξβ(ξα∨β,
β(
E(ξα, ξ¬α)), ξβ)
This constitutes a derivation from the left side-assumptions of an application of
type elimination to ξα∨β : α ∨β, which is the ﬁnal goal of our proof. The derivation
from the right side-assumptions is instead obtained by applying the equations for F
and for ∨E on an arbitrary canonical term of type α ∨β and on an arbitrary term
of type β, and then by showing that what one obtains is transitively equivalent to
an application of ∨E to ξα∨β, →E(ξ¬α, ξα) and ξβ, which is in turn provably
equivalent to F(ξα∨β, ξ¬α). This reasoning amounts to four sub-derivations. First,
we have the derivation 0
2
4
[ξα∨β ≡∨I(ξβ)]
≡R
ξ¬α ≡ξ¬α
≡F
F(ξα∨β, ξ¬α) ≡F(∨I(ξβ), ξ¬α)
RF
F(∨I(ξβ), ξ¬α) ≡ξβ
≡T
F(ξα∨β, ξ¬α)
ξβ
Then we have the derivation 1,1
2
5
[ξα∨β ≡∨I(ξβ)]
≡R
⊥β(→E(ξ¬α, ξα)) ≡⊥β(→E(ξ¬α, ξα))
≡R
ξβ ≡ξβ
≡∨
3
Eξαξβ(ξα∨β,
β(
E(ξ¬α, ξα)), ξβ)
Eξαξβ( I(ξβ),
β(
E(ξ¬α, ξα)), ξβ)

224
6
Systems of Grounding
Then the derivation 1
2
Δ1,1
2
R∨,2
∨Eξαξβ(∨I(ξβ), ⊥β(→E(ξ¬α, ξα)), ξβ) ≡ξβ
≡T
∨Eξαξβ(ξα∨β, ⊥β(→E(ξ¬α, ξα)), ξβ) ≡ξβ
≡S
ξβ
Eξαξβ(ξα∨β,
β(
E(ξ¬α, ξα)), ξβ)
And ﬁnally we have the derivation 2
Δ0
2
Δ1
2
≡T
F(ξα∨β, ξ¬α)
Eξαξβ(ξα∨β,
β(
E(ξ¬α, ξα)), ξβ)
We can now apply type elimination to ξα∨β : α ∨β to obtain the consequent of
T hF , to which we can apply (⊃I) to discharge the assumption ξα∨β × ξ¬α : ¬α,
and then we can apply twice (I), respectively binding ξ¬α and ξα∨β, i.e.
1
[ξα∨β : α ∨β × ξ¬α : ¬α] (×E,1)
ξα∨β : α ∨β
Δ1
Δ2
D∨, 3, 4, 5
F(ξα∨β, ξ¬α) ≡∨Eξαξβ(ξα∨β, ⊥β(→E(ξ¬α, ξα)), ξβ) (⊃I), 1, 2
ThF
(ΠI)
Πξ¬α ThF
(ΠI)
Πξα∨β Πξ¬α ThF
The property proved here also relates to a completeness issue, discussed in the next
chapter.
6.2.4
A Theorem About the System
We close this section with the proof of a meta-theorem that can be viewed as a kind
of correctness result for GG—relative to terms of Gen. The theorem represents a
“deductive” version of the denotation theorem seen in Chap. 5. A more detailed
version of the theorem can be obtained by taking into account type-variables with
equal type but different indices. For simplicity, we omit these details in what
follows.
Theorem 44 Let U : β ∈TERMGen with FV T (U) = {ξα1, . . . , ξαn}. Then ξα1 :
α1, . . . , ξαn : αn ⊢GG U : β.

6.2
Deduction Over the Gentzen-Language
225
Proof By induction on the complexity of U, using the results proved in the previous
Section. The induction basis is trivial. For the induction step, we prove a few relevant
examples.
•
∨I(Z) : α1 ∨α2 with Z : αi (i = 1, 2) and FV T (∨I(Z)) = FV T (Z) ⇒by
induction hypothesis, there is  ∈DERGG that satisﬁes the required properties,
so that
Δ
Z : αi
∨I
∨I(Z) : α1 ∨α2
satisﬁes the required properties;
•
∨Eξβ1ξβ2(Z1, Z2, Z3) : β3 with Z1 : β1 ∨β2, Z2 : β3, Z3 : β3 and
FV T (∨Eξβ1ξβ2(Z1, Z2, Z3)) = FV T (Z1)∪(FV T (Z2)−{ξβ1})∪(FV T (Z3)−{ξβ2})
⇒by induction hypothesis, there are 1, 2, 3 ∈DERGG that satisfy the
required properties, so that, given the following derivation 
Δ1
Z1 : β1 ∨β2
[ξβ1 : β1]
Δ2
Z2 : β3
(⊃I)
ξβ1 : β1 ⊃Z2 : β3
(ΠI)
Πξβ1(ξβ1 : β1 ⊃Z2 : β3) (×I)
Z : β1 ∨β2 × Πξβ1(ξβ1 : β1 ⊃Z2 : β3)
[ξβ2 : β2]
Δ3
Z : β3
(⊃I)
ξβ2 : β2 ⊃Z3 : β3
(ΠI)
Πξβ2(ξβ2 : β2 ⊃Z3 : β3) (×I)
(Z : β1
β2
Πξβ1(ξβ1 : β1
Z2 : β3))
Πξβ2(ξβ2 : β2
Z3 : β3)
we have that
Δ
theorem of the previous Section
Πξβ1∨β2Πξβ3
1 Πξβ3
2
Th∨
(ΠE)
Πξβ3
1 Πξβ3
2
Th∨[Z1/ξβ1∨β2]
(ΠE)
Πξβ3
2
(Th∨[Z1/ξβ1∨β2])[Z2/ξβ3
1 ]
(ΠE)
((Th∨[Z1/ξβ1∨β2])[Z2/ξβ
1 ])[Z3/ξβ3
2 ] (⊃E)
Eξβ1ξβ2(Z1, Z2, Z3) : β3
satisﬁes the required properties;
•
∀Ix(Z) : ∀xβ(x) with Z : β(x) and FV T (∀Ix(Z)) = FV T (Z) ⇒by induction
hypothesis, there is  ∈DERGG that satisﬁes the required properties (recall that,
for the hypotheses of the theorem, ∀Ix(Z) ∈TERMGen, therefore x cannot occur
free in γ for ξγ ∈FV T (Z) and hence, again by induction hypothesis, x does not

226
6
Systems of Grounding
occur free in any undischarged assumption of ), so that
Δ
Z : β(x)
I
xI(Z) :
xβ(x)
satisﬁes the required properties;
•
∀E(Z) : β(t) with Z : ∀xβ(x) and FV T (∀E(Z)) = FV T (Z) ⇒by induction
hypothesis, there is  ∈DERGG that satisﬁes the required properties, so that
Δ
Z : ∀xβ(x)
theorem of the previous Section
Πξ∀xβ(x)(ξ∀xβ(x) : ∀xβ(x) ⊃∀E(ξ∀xβ(x)) : β(t)) (ΠE)
Z : ∀xβ(x) ⊃∀E(Z) : β(t) (⊃E)
E(Z) : β(t)
satisﬁes the required properties.
The other cases are in all ways analogous.
⊓⊔
Clearly, the converse of Theorem 44 does not hold—e.g. ∧E,i(∧I(δ, ξβ)) : α
is provable for δ : α, but FV T (∧E,i(∧I(δ, ξβ))) ̸= ∅. This is because the
equations that rule the non-primitive operational symbols provide “syntactic” means
of transformations between terms. Indeed, it should come as no surprise that, in the
previous Chapter, denotation functions of open terms were deﬁned by appealing to
closed instances of the operation they denote. In such cases it is usually said that
terms denote, not their normal form, but their full-evaluated form, i.e. their form
reduced up to open sub-terms (see e.g. Tranchini, 2019).
6.3
A Class of Systems
In this Section, we generalize from the system of grounding presented for the
enriched Gentzen-language to a class of systems of grounding relative to the
languages of grounding developed in the previous chapter—adequately enriched.
6.3.1
Invariant and Characteristic Rules
Given a language of grounding , we build an enriched version + of it as we did
for Gen with Gen+, and then a formal system  over + as we did for GG over
Gen+. Therefore  contains:
(τ) type introductions and type eliminations;

6.3
A Class of Systems
227
(ϵ) equivalence rules, i.e.
(ϵ1) standard rules for reﬂexivity, simmetry and transitivity, plus preservation
of denotation;
(ϵ2) rules saying that operational symbols applied to equivalent terms
yield equivalent terms (and vice versa in the canonical cases);
(ϵ3) equations for computing non-canonical terms whose outermost oper-
ations apply to (relevant) canonical terms;
(λ) ﬁrst-order logical rules.
The bold font used for the groups of rules (ϵ2) and (ϵ3) is not unintentional. While
(τ), (ϵ1) and (λ) contain the same schematic rules for any system of grounding, (ϵ2)
and (ϵ3) depend on the set of non-primitive operational symbols of , and can thus
be called characteristic rules of . This also means that the only way in which an
expansion + of  from  only with respect to (ϵ2) and (ϵ3). + is a system of
grounding over ∗+, where ∗+ is the enriched version of ∗, for ∗expansion of
; observe however that + differs from  also with respect to the axioms for the
individual constants, when the base of ∗properly contains the base of .
To better characterise this class of systems of grounding, we need to clarify
the general form of the characteristic rules of a system of grounding  over .
First, observe that not all of the rules in group (ϵ2) really depend on the non-
primitive operational symbols of . Since the primitive operational symbols are the
same in any language of grounding, the rules concerned are only those saying that,
whenever a non-primitive operational symbol is applied to equivalent terms, it yields
equivalent terms. Therefore, the general form of the characteristic rule is in this case
very straightforward. Given F[α1, . . . , αn ▷β] ∈Al, binding (possibly empty)
sequences of individual and typed-variables x, ξ, and given Ti, Ui : αi ∈TERM+
(1 ≤i ≤n),
T1 ≡U1
. . .
Tn ≡Un
≡F
F x ξ (T1, ..., Tn)
F x ξ (U1, ..., Un)
Second, the equations for the computation of the non-primitive operational symbols
need to satisfy certain properties (similar to those required by Prawitz, 1973 for the
justiﬁcation procedures on arguments). Given F[α1, . . . , αn ▷β] ∈Al, binding
(possibly empty) sequences of individual and typed-variables x, ξ, and given Ui :
αi ∈TERM+ (1 ≤i ≤n), the following three points holds: (1) each rule for
computing F has the form
RF
F x ξ (U1, ..., Un)
Z
for some Z : β ∈TERM+; (2) for each RF of the kind indicated in (1), it holds
that ⋆(F x ξ (U1, . . . , Un)) ⊆⋆(Z), for ⋆= FV I, FV T ; and (3) for each RF of
the kind indicated in (1), for every substitution (∗/◦) of variables with terms, there

228
6
Systems of Grounding
is a rule for computing F of the form
R′
F
(F x ξ (U1, ..., Un))( / )
W
such that ⊢
W
≡Z(∗/◦). Here, (3) is a linearity condition for F under
substitutions of variables. If we indicate a substitution with sub, the application
of F to given arguments with F(a), and a computation of F(a) with comp(F(a)),
we have ⊢ sub(comp(F(a))) ≡comp(sub(F(a))) = comp(F(sub(a))). So,
taking R∧,1 as an example, we have the two instances
R∧,1
E,1( I(ξα, ξβ))
ξα
R∧,1
E,1( I(T, ξβ))
T
and—with a slight but intuitive abuse of notation -
(∧E,i(∧I(ξα, ξβ)))[T/ξα] ≡ξα[T/ξα] = T ≡
∧E,1(∧I(T, ξβ)) = ∧E,i((∧I(ξα, ξβ)[T/ξα]).
We may assume that all the systems of grounding described in this section allow
to prove results analogous to those proved within GG in Sects. 6.2.3.1, 6.2.3.2
and 6.2.3.2. In particular, this means that we may assume that the equations for
computing the non-primitive operational symbols are always well-given. On this
basis, a meta-theorem analogous to the one proved for GG in Sect. 6.2.4 becomes
provable for each system in our class.
6.3.2
Normalization
We now prove a normalization result that abstracts from characteristic rules, and
hence holds for each of the systems of grounding outlined in the previous section.
Normalization is based, as usual, on the possibility of transforming a derivation
with maximal formulas into one without maximal formulas. The idea is clearly
that of progressively lowering the complexity of the maximal formulas through
appropriate reduction and permutation functions. We deal with reduction functions
ﬁrst and then outline permutation functions. Finally, we provide a measure for
maximal formulas, and we prove the theorem itself.
6.3.2.1
Reductions
Unlike a standard logical system where only logically maximal formulas (LMF) are
at issue, in systems of grounding we also have typing maximal formulas (TMF)—

6.3
A Class of Systems
229
namely, formulas which occur both as conclusions of a type introduction, and as
major premises of a type elimination. When there is no need to distinguish, we shall
refer to both LMF and TMF as maximal points.
Standard reduction functions for LMFs can be found in Prawitz (2006). Reduc-
tion functions for TMFs are instead deﬁned below, but essentially follow the
same intuition as reduction functions for LMFs. It is redundant to prove by type
introduction (possibly under assumptions) that a term T denotes a ground for α, and
then to apply type elimination to T : α to obtain A, since the conditions required by
the assumptions of the minor premises A in the type elimination—namely, that T is
equivalent to some canonical form whose immediate sub-terms denote (operations
on) grounds for the immediate sub-formulas of α—are already satisﬁed by T itself
and by the derivations (possibly under assumptions) of the premises of the type-
introduction.
However, while standard reduction functions for LMFs remove logically complex
formulas of Gen+, those associated to the TMFs remove atomic formulas of Gen+.
The redundant passage in a TMF, therefore, does not concern the introduction-
elimination of a logical constant of Gen+, but the introduction-elimination of a
primitive operational symbol sI in a term, and of a logical constant s in the formula
of the background language for which that term is said to denote a ground. In a
sense, the reduction functions for the TMFs can be considered as an “embedding”
in GG of the so-called inversion principle (Prawitz, 2006) relative to a proof-system,
say S, for the background language of Gen+. The principle says that, given a
derivation in S of α from  whose last inference is an elimination rule the major
premise of which is inferred by an introduction, the immediate sub-derivations
already “contain” a derivation of α from . Now, if we take the derivations of S to be
Curry-Howard translated by the terms of Gen, what the “embedding” shows is that
the “denotational” properties of the derivations of S, expressed by the formulas of
Gen+, remain stable modulo normalization in S. As an example, the “denotational”
properties of a non-normal derivation  in S, say
Δ1
α1
Δ2
α2 (
I)
α1
α2 (
I)
αi
are the same as those enjoyed by
Δi
αi
can be seen in GG by showing that the consequences we can draw by applying a
type elimination after a type-introduction on a term T : α1 ∧α2 that translates the
immediate sub-derivation of , and whose immediate sub-terms translate 1 and
2 respectively, are the same as those we would obtain from the premises of the
type-introduction alone - plus an application of reﬂexivity to T . This is indeed what

230
6
Systems of Grounding
the TMF reduction for ∧does, i.e.
Δ1
T : α
Δ2
U : β
∧I
∧I(T, U) : α ∧β
[∧I(T, U) ≡∧I(ξα, ξβ)]
[ξα : α]
[ξβ : β]
Δ3(ξα, ξβ)
A D∧
A
reduces to
≡R
∧I(T, U) ≡∧I(T, U)
Δ1
T : α
Δ2
U : β
Δ3(T/ξα, U/ξβ)
A
To spell out in detail, we have replaced the passage giving rise to the TMF
∧I(T, U) : α ∧β with a derivation obtained from the derivation 3(ξα, ξβ) of
the minor premise A of D∧where: (1) the assumption ∧I(T, U) ≡∧I(ξα, ξβ) is
replaced by an application to ∧I(T, U) of the reﬂexivity axiom for equivalence; (2)
the assumption ξα : α is replaced by the derivation 1 of T : α, and ξα is replaced
with T throughout 3(ξα, ξβ); and (3) the assumption ξβ : β is replaced by the
derivation 2 of U : β, and ξβ is replaced with U throughout 3(ξα, ξβ) of the
minor premise A of D∧.
The TMF reductions for ∨and ∃are carried out in a similar way, so we can omit
the details. As for ∨,
Δ1
T : αi
∨I
∨I[αi
α1 ∨α2](T ) : α1 ∨α2
[∨I(T ) ≡∨I(ξα1 )]
[ξα1 : α1]
Δ2(ξα1 )
A
[∨I(T ) ≡∨I(ξα2 )]
[ξα2 : α2]
Δ3(ξα2 )
A
D∨
A
reduces to
≡R
∨I(T) ≡∨I(T)
Δ1
T : αi
Δi+1(T/ξαi)
A

6.3
A Class of Systems
231
And for ∃,
Δ1
T : α(t/x)
∃I
∃I[α(t/x) ▷∃xα(x)](T) : ∃xα(x)
[∃I(T) ≡∃I(ξα(x))]
[ξα(x) : α(x)]
Δ2(x, ξα(x))
A D∃
A
reduces to
≡R
∃I(T) ≡∃I(T)
Δ1
T : α(t/x)
Δ2(t/x, T/ξα(x))
A
The TMF reduction for →and ∀are more complex, since the derivation produced
by the reduction involves some new inferences - but no new assumptions. As for →,
[ξα : α]
Δ1(ξα)
T(ξα) : β
→I
→Iξα(T(ξα)) : α →β
[→Iξα(T(ξα)) ≡→Iξα(fβ(ξα))]
[Πξα(ξα : α ⊃fβ(ξα) : β)]
Δ2(fβ(ξα))
A
D→
A
reduces to
≡R
→Iξα(T(ξα)) ≡→Iξα(T(ξα))
[ξα : α]
Δ1(ξα)
T(ξα) : β
(⊃I)
ξα : α ⊃T(ξα) : β
(ΠI)
Πξα(ξα : α ⊃T(ξα) : β)
Δ2(T(ξα)/fβ(ξα))
A
The passage giving rise to the TMF →Iξα(T (ξα)) : α →β is replaced with
a derivation obtained from the derivation 2(fβ(ξα)) of the minor premise A of
D→where: (1) the assumption →Iξα(T (ξα)) ≡→Iξα(fβ(ξα)) is replaced
by an application to →Iξα(T (ξα)) of the reﬂexivity axiom for equivalence; and
(2) the assumption ξα(ξα : α ⊃fβ(ξα) : β) is replaced with the derivation
of ξα(ξα : α ⊃T (ξα) : β) obtained by ﬁrst applying (⊃I) to the immediate
sub-derivation 1(ξα) of the premise of the TMF discharging ξα : α, and thereby
obtaining ξα : α ⊃T (ξα) : β, and then applying (I) to ξα : α ⊃T (ξα) :

232
6
Systems of Grounding
β, binding ξα. The term fβ(ξα) is replaced with T (ξα) throughout 2(fβ(ξα)).
Observe that the application of (I) in the new derivation is licit, thanks to the
restriction on →I. The reduction for ∀essentially follows the same steps, i.e.
Δ1(x)
T(x) : α(x)
∀I
∀Ix(T(x)) : ∀xα(x)
[∀Ix(T(x)) ≡∀Ix(hα(x)(x))]
[Πx(hα(x)(x) : α(x))]
Δ2(hα(x)(x))
A D∀
A
reduces to
≡R
∀Ix(T(x)) ≡∀Ix(T(x))
Δ1(x)
T(x) : α(x)
(ΠI)
Πx(T(x) : α(x))
Δ2(T/hα(x)(x))
A
Observe that the application of (I) in the new derivation is allowed, because the
type-introduction ∀I for ∀and -introduction have the same restrictions.
6.3.2.2
Permutations
When proving his normalization theorems, Prawitz uses, not just the reductions
shown in Chap. 3, but also the so-called permutations. Although we have not yet
made use of any of these operations so far, they are required in the systems of
grounding we have been developing, so that we will now deal with them in more
detail.
The necessity for permutations depends, in ﬁrst-order intuitionistic logic, on the
presence of the rules (∨E) and (∃E). During a derivation, we might ﬁnd there is a
concatenation of applications of such rules in which one or both the minor premises
of the ﬁrst application are obtained by introduction, and the conclusion of the latter
is a major premise of an elimination. This also applies to our systems of grounding,
although in this case also with reference to type elimination rules. In other words,
we may ﬁnd ourselves in the following situation:

6.3
A Class of Systems
233
Δn
An
Δ2
A2
Δ1
A1
Δa
... intro
intro
S
Δb
... ⋆1
S
Δc
... ⋆2
S
...
S
Δd
... ⋆n
S
Δe
...
Δf
... elim
elim
B
where: b, c and d could in turn end with an introduction; b, c, d, e
and f could be “empty”—that is, the rules to which they correspond could have
a number of premises lower than that indicated in our general representation; and
⋆i (i ≤n) is (∨G
E), or (∃G
E), or a type elimination rule. In the presence of such
cases, it would make sense to consider S a maximal point of the derivation, even
if its occurrence as a conclusion of an introduction could be very “far” from its
occurrence as a major premise of elimination. In order “to shorten” this distance, and
apply the required reduction, we can use a series of permutations, which consist in
“bringing up” the last elimination rule within the derivations of the minor premises
of the various ⋆i (i ≤n). The procedure is applied, in our general representation, n
times, and within it the ﬁrst step would thus be
Δn
An
Δ2
A2
Δ1
A1
Δa
... intro
intro
S
Δb
... ⋆1
S
Δc
... ⋆2
S
...
S
Δe
...
Δf
... elim
elim
B
Δd∗
... ⋆n
B
where d∗is obtained by applying
S
Δe
...
Δf
... elim
elim
B
to the conclusion of d. After n −1 passages of this kind, we get to

234
6
Systems of Grounding
Δn
An
Δ2
A2
Δ1
A1
Δa
... intro
intro
S
Δb
... ⋆1
S
Δe
...
Δf
... elim
elim
B
Δc∗
... ⋆2
B
...
B
Δd∗
... ⋆n
B
where c∗is obtained from c as indicated for d∗. The last passage ﬁnally returns
Δn
An
Δ2
A2
Δ1
A1
Δa
... intro
intro
S
Δe
...
Δf
... elim
elim
B
Δb∗
... ⋆1
B
Δc∗
... ⋆2
B
...
B
Δd∗
... ⋆n
B
where the same holds for b∗as what has been said for d∗and c∗. For a concrete
example,
Δ1
T : α1 ∨α2
[T ≈∨I(ξα1)]
[ξα1 : α1]
Δ2
A
[T ≈∨I(ξα2)]
[ξα2 : α2]
Δ3
A
D∨
A
Δ4
Δ5
B
Δ6
permutes into
Δ1
T : α1 ∨α2
Δ7
Δ8 D∨
B
Δ6

6.3
A Class of Systems
235
where j (j = 7, 8) is
[T ≈∨I(ξαi)]
[ξαi : αi]
Δi+1
A
Δ4
Δ5
B
for i = 1, 2.
6.3.2.3
Cut-Segments and Conventions
Deﬁnition 45 A cut-segment in  is a sequence σ of n occurrences of a formula A
in  such that:
•
the ﬁrst occurrence of A is the conclusion of an application of a (type)
introduction rule;
•
for every 1 ≤i ≤n, the (i + 1)-th occurrence of A is the conclusion of an
application of (+E), or of (EE), or of a type elimination rule, and
•
the n-th occurrence of A is the major premise of a (type) elimination rule.
Given a cut-segment σ = A1, . . . , An, we say that σ has length n. We also say that
A is the formula of σ, and that Ai (1 ≤i ≤n) is an occurrence of σ - of course,
an occurrence of σ the formula of which is A is an occurrence of A in . Clearly,
if n = 1, σ is a maximal point. Given two cut-segments σ1, σ2, we say that σ1 is
disjoint from σ2 if, and only if, none of the occurrences of σ1 is also an occurrence
of σ2.
The following conventions cause no loss of generality thanks to well-know
results (Prawitz 2006, Van Dalen 1994).
Convention 46 For every application of (⊥G) in  with conclusion A, A ∈
ATOM+.
Convention 47 (1) Free and bound individual and typed-variables are all distinct in
; (2) proper and non-proper variables are all distinct in , and each proper variable
occurs at most once.
6.3.2.4
Measure
Maximal points undergo a double measure. We now provide this measure, and then
explain why, unlike what happens in normalization for simply logical systems, the
measure has to be double. In what follows, we use max: N2 →N, which is the

236
6
Systems of Grounding
standard function
max(n, m) =

n
if m < n
m
if n ≤m
and, given F = {S | S ⊂N and S ﬁnite}, we use the function MAX: F →N such
that
MAX(S) =

0
if S = ∅
n ∈S
such that, for every m ∈S, m ≤n
The measure of α ∈FORML is a function k1 : FORML →N deﬁned in a standard
inductive way. Given A ∈FORM+, we set
TA = {k1(α) | U : α sub-formula A}
Deﬁnition 48 The type-measure of A ∈FORM+ is a function τ : FORM+ →N
such that
τ(A) = MAX(TA).
Deﬁnition 49 The logical measure of A ∈FORM+ is a function k2 : FORM+ →
N inductively deﬁned as follows:
•
A ∈ATOM+ ⇒k2(A) = 0
•
A = B□C →k2(A) = max(k2(B), k2(C)) + 1 (□= ×, +, ⊃)
•
A = □ν B ⇒k2(A) = k2(B) + 1 (□= , E)
Deﬁnition 50 The measure of A ∈FORM+ is a function μ: FORM →N2 such
that
μ(A) = (τ(A), k2(A)).
Thus, the measure of A ∈FORM+ is a pair whose ﬁrst element is the measure of the
most complex α ∈FORML occurring in sub-formulas of A of the form . . . : −−−,
and whose second element is the standard measure of the logical complexity of A.
As a concrete example, let us compute the complexity of
A = ξα∨β(ξα∨β : α ∨β ⊃Eξαξβ((ξα∨β ≡∨I(ξα) × ξα : α) + (ξα∨β ≡
∨I(ξβ) × ξβ : β))).
Assume k1(α) = 3 and k1(β) = 5. Thus, k1(α ∨β) = max(k1(α), k1(β)) + 1 =
max(3, 5) + 1 = 5 + 1 = 6. Our A contains three atomic sub-formulas of the form

6.3
A Class of Systems
237
. . . : −−−, i.e.
ξα∨β : α ∨β, ξα : α and ξβ : β.
Thus, TA = {3, 5, 6}, so that τ(A) = MAX(TA) = 6. We now have to compute the
logical complexity of A, that is
k2(ξα∨β(ξα∨β : α∨β ⊃Eξαξβ((ξα∨β ≡∨I(ξα)×ξα : α)+(ξα∨β ≡∨I(ξβ)×
ξβ : β)))) =
= max(k2(ξα∨β : α ∨β), k2(Eξαξβ((ξα∨β ≡∨I(ξα) × ξα : α) + (ξα∨β ≡
∨I(ξβ) × ξβ : β)))) + 1 =
= max(0, k2((ξα∨β ≡∨I(ξα) × ξα : α) + (ξα∨β ≡∨I(ξβ) × ξβ : β)) + 2) + 1 =
= max(0, max(1, 1) + 1) + 2) + 1 = 5.
where not all steps are shown. So in conclusion we have μ(A) = (τ(A), k2(A)) =
(6, 5). Once the measure has been set, it is possible to deﬁne a strict order relation
on the formulas of +. Since the measure is double, it is possible to deﬁne on
the formulas of + an order of an “alphabetical” kind - that is, for every A, B ∈
FORM+,
μ(A) < μ(B) ⇔

τ(A) < τ(B)
or
k2(A) < k2(B)
for τ(A) = τ(B)
The reasons for adopting the double measure are essentially two. First, observe that
the measure of an atomic formula of the form T : α is not 0, but (k1(α), 0). This is
important because, as previously said, our strategy for proving normalization will be
based on the idea of progressively lowering the complexity of the maximal points.
Since maximal points may well be TMFs, i.e. they may have the form T : α, we
must hence be able to set a non-null measure of them for the lowering process to go
through - observe that α cannot be atomic, so the measure is neither (0, 0) in these
cases. Second, reductions for typing rules may generate new TMFs or LMFs. In the
speciﬁc case of new LMFs, we must ensure that they have a lower measure than
the eliminated TMFs—that the measure is lower in the case of new TMFs depends
on the fact that the new TMFs have as type formulas which are sub-formulas of the
type of the starting TMFs. Suppose we are in the following situation:
[ξα : α]
Δ1
T (ξα) : β
→I
→Iξα(T (ξα)) : α →β
[→Iξα(T (ξα)) ≡→Iξα(fβ(ξα))]
[Πξα(ξα : α ⊃fβ(ξα) : β)]
(ΠE)
U : α ⊃fβ(U) : β
Δ2(fβ)
A
D→
A
Δ3

238
6
Systems of Grounding
The reduction for →gives
≡R
→Iξα(T(ξα)) ≡→Iξα(T(ξα))
[ξα : α]
Δ1
T(ξα) : β
(⊃I)
ξα : α ⊃T(ξα) : β
(ΠI)
Πξα(ξα : α ⊃T(ξα) : β) (ΠE)
U : α ⊃T(U) : β
Δ2(T/fβ)
A
Δ3
with a new LMF, namely, ξα(ξα : α ⊃T (ξα) : β). Now note that μ(→
Iξα(T (ξα)) : α →β) = (k1(α →β), 0), and μ(ξα(ξα : α ⊃T (ξα) :
β)) = (max(k1(α), k1(β)), 2). Since k1(α →β) = max(k1(α), k1(β)) + 1, we
have max(k1(α), k1(β)) < k1(α →β), and hence μ(ξα(ξα : α ⊃T (ξα) : β)) <
μ(→Iξα(T (ξα)) : α →β) by the alphabetic order.
As in Prawitz’s standard normalization theory, also the reductions associated with
logical rules could produce new TMFs or new LMFs. In these cases, however, the
following applies. Let A be an LMF, and let B be a new TMF or LMF produced by
the reduction on A; since B is a sub-formula of A, we have that τ(B) ≤τ(A), and
k2(B) < k2(A). Therefore, μ(B) < μ(A).
6.3.2.5
Normalization Theorem
Deﬁnition 51 Given , let σ be a cut-segment in  the formula of which is A. The
measure of σ—written μ(σ)—is μ(A).
To every  we associate the set
M = {μ(σ) | σ cut-segment of }.
Deﬁnition 52 The degree of  is a function δ : DER →N2 such that
δ() = (MAX(M), n)
where n is the sum of the lengths of the cut-segments σ of  such that μ(σ) =
MAX(M).
Also on derivations it is possible to deﬁne a strict order relation. It is again of an
“alphabetical” kind, that is, for every 1, 2 such that δ(i) = (MAX(Mi), ni)

6.3
A Class of Systems
239
(i = 1, 2),
δ(1) < δ(2) ⇔

MAX(M1) < MAX(M2)
or
n1 < n2
for MAX(M1) = MAX(M2)
Deﬁnition 53  is normal if, and only if, δ() = ((0, 0), 0). Otherwise,  is non-
normal.
Deﬁnition 54 a immediately reduces to b—written a ⪰b—if, and only
if, a = b, or b can be obtained from a by applying one of the reduction
functions for TMFs, or one of the reduction functions for LMFs, or a permutation
function. Moreover, a reduces to b—written a ≻b—if, and only if, there
is a sequence 1, . . . , n with 1 = a, n = b, and i ⪰i+1 for every
1 ≤i ≤n.
We take as given the notion of a formula occurrence standing (immediately) above
(respectively, below) another formula occurrence, and the notion of side-connected
occurrences of formulas—basically, formulas that occur as premises of the same
inference. The upper edge of a cut-segment σ whose formula is A is the occurrence
of A in σ that stands above all the other occurrences of A in σ—the lower edge of σ
is accordingly the occurrence of A in σ that stands below all the other occurrences
of A in σ.
As previously said, the normalization strategy is based on the idea of progres-
sively lowering the degree of a non-normal derivation. A standard way of doing
this—which we shall also follow here—is that of ﬁnding a topmost rightmost
maximal cut-segment σ in the derivation. This means that: (1) σ has degree equal
to the degree of the derivation; (2) there is no cut-segment of equal degree (2.1)
whose lower edge stands above the upper edge of σ or (2.2) in which there is an
occurrence side-connected to the lower edge of σ or (2.3) whose lower edge stands
above a formula side-connected with the lower edge of σ. We can thereby grant
that the application of a reduction or permutation function on the lower edge of σ
actually decreases the degree of the derivation, i.e. either all the cut-segments of the
resulting derivation are of a lower degree, or the overall length of the cut-segments
of highest degree in the resulting derivation is smaller. The existence of a topmost
cut-segment in a non-normal derivation can be easily proved by induction on the
length of the derivation.
Proposition 55 If  is a non-normal derivation, there is a cut-segment σ in  such
that μ(σ) = MAX(M) and such that, for no cut-segment σ ∗, does the lower edge
of σ ∗stand above the upper edge of σ and μ(σ ∗) = μ(σ).
Next, we show that, if a topmost maximal cut-segment (TMCS) σ is not also a
rightmost maximal cut-segment, there is a TMCS disjoint from σ, and that, given a
sequence σ1, . . . , σn of TMCSs which are not also rightmost maximal cut-segments,
there is a TMCS disjoint from each σi (i ≤n).

240
6
Systems of Grounding
Proposition 56 Given a non-normal derivation , and given a TMCS σ 1 of ,
suppose there is a cut-segment σ ∗in  such that μ(σ ∗) = μ(σ 1) and such that
the lower edge of σ 1 is side-connected either with an occurrence in σ ∗or with a
formula occurrence in  that stands below the lower edge of σ ∗. Then, there is a
TMCS σ 2 of  disjoint from σ 1. Moreover, let σ 1, σ 2, σ 3, . . . , σ n be a sequence of
pairwise disjoint TMCS of  such that, for every i < n, σ i+1 stands with σ i in the
same relation as the one occurring between σ ∗and σ 1 above, and suppose there is a
cut-segment σ ∗∗in  that stands with σ n in the same relation as the one occurring
between σ ∗and σ 1 above. Then, there is a TMCS σ n+1 of  such that, for every σ i
(1 ≤i ≤n), σ n+1 is disjoint from σ i.
Proof If σ ∗is a TMCS of , we can put σ 2 = σ ∗since σ ∗is disjoint from σ 1. If
σ ∗is not a TMCS of , then the sub-derivation ∗of  whose conclusion is the
upper edge of σ ∗is non-normal, and we can apply Proposition 55 to ﬁnd a TMCS
σ 2 of ∗. Since μ(σ 2) = μ(σ ∗) = μ(σ 1), σ 2 is also a TMCS of . Finally, σ 2
is disjoint from σ 1. This proves the ﬁrst part of the proposition. As for the second,
since σ ∗∗stands with σ n in the same relation as the one one occurring between σ ∗
and σ 1 above, with the same reasoning as above we can prove that there is a TMCS
σ n+1 of  disjoint from σ n. But now observe that also σ n+1 stands with σ n in the
same relation as σ ∗and σ 1 above, and since we have assumed that this holds for
all the pairs σ i, σ i+1 of our sequence of pairwise disjoint TMCSs of , we can
conclude that
(∗) for every i < n + 1, the lower edge of σ i is side-connected either with an
occurrence in σ i+1 or with a formula occurrence in  that stands below the
lower edge of σ i+1.
Suppose now that there is σ i not disjoint from σ n+1 for some i < n −1. Then, the
lower edge of σ i and the lower edge of σ n+1 are the same formula occurrence in .
So the lower edge of σ n+1 is side-connected either with an occurrence in σ i+1 or
with a formula occurrence in  that stands below the lower edge of σ i+1. But this
is clearly impossible because of (∗).
⊓⊔
Finally, we prove the existence of a topmost rightmost cut-segment in a non-normal
derivation.
Proposition 57 If  is a non-normal derivation, then there is a TMCS σ of  such
that, for no cut-segment σ ∗in , μ(σ ∗) = μ(σ) and the lower edge of σ is side-
connected with an occurrence in σ ∗or with a formula occurrence in  that stands
below the lower edge of σ ∗.
Proof The existence of a TMCS in  is granted by Proposition 55. Moreover, if no
TMCS satisﬁed the required property, by Proposition 56  would contain an inﬁnite
number of TMCS, which is clearly impossible.
⊓⊔
Now, by choosing a topmost rightmost maximal cut-segment—abbreviated with
TRMCS—we can prove the following fundamental theorem.

6.3
A Class of Systems
241
Theorem 58 Let  be a non-normal derivation. Then, there is a derivation ∗
such that  ⪰∗and δ(∗) < δ().
Proof Let us choose a TRMCS σ of  whose formula is A. Suppose that σ has
length greater than 1. Then the lower edge of σ is the conclusion either of an
elimination rule for a logical constant of the (enriched) language of grounding, or
of a type elimination rule. To give an example of each case, we may have either
Δ1
B + C
Δ2 intro
intro
A
Δ3
A
Δ4
A (+E)
A
Δ5
Δ6 elim
elim
D
—where the bold character indicates the chosen TRMCS, and where 5 or 6 may
be empty—or we may have
Δ1
T : ∃xα(x)
Δ2 intro
intro
A
Δ3
A D∃
A
Δ4
Δ5 elim
elim
B
—where 4 or 5 may be empty. In all cases, by applying a permutation, we obtain
a derivation ∗with δ(∗) = (MAX(M∗), n∗) for MAX(M∗) = MAX(M) and
n∗< n, whence δ(∗) < δ(). In the ﬁrst example, we have
Δ1
B + C
Δ2 intro
intro
A
Δ3
A
Δ4
Δ5 elim
elim
D
Δ4
A
Δ4
Δ5 elim
elim
D (+E)
D
while in the second example we have
Δ1
T : ∃xα(x)
Δ2 intro
intro
A
Δ3
A
Δ4
Δ5 elim
elim
B D∃
B

242
6
Systems of Grounding
If σ has length 1, then σ is either an LMF or a TMF. As an example of each case,
we may have
[A]
Δ1
B
(⊃I)
A ⊃B
Δ2
A (⊃E)
B
or
Δ1
T(x) : α(x)
∀I
∀Ix(T(x)) : ∀xα(x)
Δ2
A D ∀
A
By applying a reduction, we obtain a derivation ∗such that:
•
∗does not contain cut-segments not already occurring in . In this case: either
all the cut-segments of ∗have lower measure than the cut-segments of  of
maximal measure, so that MAX(M∗) < MAX(M), and hence δ(∗) < δ();
or ∗contains cut-segments of the same measure of the cut-segments of 
of maximal measure, so that δ(∗) = (MAX(M∗), n∗) for MAX(M∗) =
MAX(M) and n∗< n, from which δ(∗) < δ() or
•
∗contains cut-segments which were not already in . If B is the formula
of a new cut-segment obtained by eliminating A, then μ(B) < μ(A). This
means that, either MAX(M∗) < MAX(M), and thus δ(∗) < δ(), or
δ(∗) = (MAX(M∗), n∗) for MAX(M∗) = MAX(M) and n∗< n, giving
us again δ(∗) < δ().
In the ﬁrst case, the reduction yields
Δ2
A
Δ1
B
The new cut-segment σ ∗produced by the reduction can have A as formula.
Independently of whether A is atomic or logically complex, we have that τ(A) ≤
τ(A ⊃B) and k2(A) < k2(A ⊃B), from which μ(A) < μ(A ⊃B), and hence
μ(σ ∗) < μ(σ). In the second case, if 2 does not depend on the assumptions
∀Ix(T (x)) ≡∀Ix(hα(x)(x)) and x(hα(x)(x) : α(x)), the reduction yields simply
Δ2
A

6.3
A Class of Systems
243
and no new cut-segment can be produced. If instead 2 depends on the assumption
∀Ix(T (x)) ≡∀Ix(hα(x)(x)) but not on the assumption x(hα(x)(x) : α(x)), the
reduction produces the derivation
Δ3
≡R
∀Ix(T(x)) ≡∀Ix(T(x))
Δ4
Δ5
A
obtained from 2 by replacing the assumption ∀Ix(T (x)) ≡∀Ix(hα(x)(x)) with
an application of the reﬂexivity axiom for equivalence to ∀Ix(T (x)), and then
replacing hα(x)(x) with T (x) everywhere. Once again, no new cut-segments can be
produced. If ﬁnally 2 depends on the assumption x(hα(x)(x) : α(x)) but not on
the assumption ∀Ix(T (x)) ≡∀Ix(hα(x)(x)), or if it depends on both assumptions,
the reduction produces the derivation
Δ3
Δ1
T(x) : α(x)
(ΠI)
Πx(T(x) : α(x))
Δ4
Δ5
A
which is obtained from 2 by replacing the assumption x(hα(x)(x) : α(x)) with
the derivation
Δ1
T(x) : α(x)
(ΠI)
Πx(T(x) : α(x))
and then replacing hα(x)(x) with T (x) everywhere—and replacing the assumption
∀Ix(T (x)) ≡∀Ix(hα(x)(x)) with an application of the reﬂexivity axiom for
equivalence to ∀Ix(T (x)). The new cut-segment σ ∗produced by the reduction
can have x(T (x) : α(x)) as its formula. Clearly, τ(x(T (x) : α(x))) <
τ(∀Ix(T (x)) : ∀xα(x)), so μ(x(T (x) : α(x))) < μ(∀Ix(T (x)) : ∀xα(x))),
and hence μ(σ ∗) < μ(σ).
⊓⊔
Corollary 59 For every , there is a normal ∗such that  ≻∗.
The normalization theorem is thus a corollary of Theorem 58, obtained by repeat-
edly applying reduction and permutation functions. We should add that the normal
derivation ∗to which  reduces depends on a set of assumptions ∗and involves
unbound individual variables x1, . . . , xn such that, given  the set of assumptions on
which  depends, and y1, . . . , ym the unbound individual variables of , ∗⊆
and {x1, . . . , xn} ⊆{y1, . . . , ym}. That this is the case is easily seen from how
reduction and permutation functions are deﬁned.

244
6
Systems of Grounding
Normalization results are usually accompanied by derived theorems concerning
the structure of normal derivations, the sub-formula property, and so on. We
conclude this section by brieﬂy discussing some of these topics, although we remark
that this is not meant to be exhaustive.
As is known, in Gentzen’s natural deduction system for, say, intuitionistic or
classical ﬁrst-order logic (respectively, IL and CL) one can prove that paths of
normal derivations enjoy some most interesting structural properties (the results
we are going to mention are due to Prawitz, 2006). A path in a derivation  in
IL or CL is a sequence a1, . . . , an of formula occurrences in  such that: (1) a1
is a top-formula in  not discharged by a disjunction or existential elimination; (2)
for every i < n, ai is not the minor premise of an implication elimination and,
either ai is not the major premise of a disjunction or existential elimination and
ai+1 is the formula occurrence immediately below ai, or ai is the major premise of
a disjunction or existential elimination and ai+1 is an assumption discharged by this
elimination; and (3) an is the minor premise of an implication elimination, or the
end formula of , or the major premise of a disjunction or existential elimination
that does not discharge any assumption. When  is normal, each path in  can
be divided into a sequence s1, . . . , sn of segments (i.e. of sequences of occurrences
of one and the same formula in , as for cut-segments but without requiring that
the upper edge is the conclusion of an introduction and the lower edge the major
premise of an elimination) where, for some i < n, a minimal segment si splits the
path into two parts: the so-called E-part where, given j < i, the formula of each sj+1
is an immediate sub-formula of the formula of sj, and the so-called I-part where,
given i < j < n, the formula of sj is an immediate sub-formula of the formula of
sj+1. So morally, the minimal segment splits the path into two parts: one where only
elimination rules are applied, and another where only introduction rules are applied;
the minimal segment itself only consist of the application of a ⊥-rule. From this one
can ﬁnally prove that, if  has assumptions  and conclusion α, then every formula
occurring in  is either a sub-formula of α or a sub-formula of some elements of .
The question is now whether similar results also hold for normal derivations
in the systems of grounding we have been discussing so far. These systems can
be understood as Gentzen’s ﬁrst-order natural deduction systems with underlying
systems given by the equivalence and typing rules. Although the latter provide
means for proving atomic formulas and for drawing consequences from them, the
underlying systems are not atomic. Clearly, this depends on the type elimination
rules, which either mention non-atomic formulas—D→and D∀—or allow for non-
atomic derivations from discharged assumptions to the minor premise(s). However,
we know that it may be possible to prove results about the form of normal
derivations similar to those mentioned above, when IL or CL have a suitable
underlying atomic system (see Cellucci, 1978 for a survey). The E-part and the
I-part are now separated by a minimal segment where only atomic rules are applied,
so that every formula occurrence in the E-part contains its immediate successor as
sub-formula, and every formula in the I-part contains its immediate predecessor as
sub-formula; it also follows a sub-formula principle limited to the E-part and to the
I-part, i.e. every formula occurrence either in the E-part or in the I-part is a sub-
formula of the conclusion or of one of the undischarged assumptions. We might

6.3
A Class of Systems
245
therefore wish to have something similar in our systems of grounding, i.e. that
each path in a normal derivation in such systems splits into an E-part where only
elimination rules for the logical constants of the language of grounding are applied,
a minimal segment where only equivalence or typing rules are applied, and ﬁnally
an I-part where only introduction rules for the logical constants of the language of
grounding are applied.
Whether this holds or not depends on how we deﬁne paths. If we want to stick to
a notion of path like the one introduced above, we should say what follows. Given
a derivation  in a system of grounding, a path in  is a sequence a1, . . . , an of
formula occurrences in  such that: (1) a1 is a top-formula in  not discharged
by (+E), (EE), or by a type elimination; (2) for every i < n, ai is not the minor
premise of (⊃E) and, either ai is not the major premise of (+E), (EE) or of a type
elimination rule and ai+1 is the formula occurrence immediately below ai, or ai
is the major premise of (+E), (EE) or of a type elimination rule and ai+1 is an
assumption discharged by this elimination; and (3) an is the minor premise of (⊃E),
or the end formula of , or the major premise of (+E), (EE) or of a type elimination
that does not discharge any assumption. But now consider the following normal
derivation:
1
ξα→β : α →β
2
ξα : α
3
[Πξα(ξα : α ⊃ffβ(ξα) : β)] (ΠE)
ξα : α ⊃fβ(ξα) : β (⊃E)
fβ(ξα) : β
(
I)
fβ(fβ(ξα) : β)) D→, 3
fβ(fβ(ξα) : β))
Here, we have a path where the major premise of D→is followed by the major
premise of (E). Of course, this depends on the fact that D→—like D∀—has a
side-assumption which, unlike its atomic major premise, is logically complex.
One may, however, expect that, when the normal derivation does not involve
applications of D→or of D∀, situations similar to the one outlined above do not
obtain. In such cases, each path in a normal derivation may split into an E-part with
only logical eliminations, a central part with only equivalence and typing rules, and
an I-part with only logical introductions. One may thus ask whether, similarly to
what happens in the E-part and the I-part, in the central part we never have a type-
introduction followed by a type elimination. The answer would be negative, though,
and this mainly depends on the rule of preservation of denotation. For example,
given the derivation 1
1
ξα : α
2
ξβ : β
∧I
∧I(ξα, ξβ) : α ∧β
3
∧I(ξα, ξβ) ≡ξα∧β
≡P
ξα∧β : α ∧β

246
6
Systems of Grounding
consider the following normal derivation
Δ1
4
[ξα
1 : α]
≡∧
∧E,1(∧I(ξα
1 , ξβ
1 )) ≡ξα
1
≡P
∧E,1(∧I(ξα
1 , ξβ
1 )) : α
5
[ξα∧β ≡∧I(ξα
1 , ξβ
1 )]
≡∧
3,1
∧E,1(ξα∧β) ≡∧E,1(∧I(ξα
1 , ξβ
1 )) ≡P
∧E,1(ξα∧β) : α D∧, 4, 5
∧E,1(ξα∧β) : α
Here, we have two paths where a premise of ∧I is followed by the major premise
of D∧, without creating a TMF because of the intermediate leftmost application of
≡P .
One may envisage different ways to deal with the problems just discussed, and
explore whether it is possible to solve them in such a way as to obtain results like
those mentioned above about the form and the sub-formulas of normal derivations.
For example, one may require that a path can pass through an assumption discharged
by a type elimination only when this assumption is atomic—and hence allowing a
path to start from a non-atomic assumption discharged by a type elimination. As
for ≡P , one may require that passages where a type-introduction ends in one of
the premises of ≡P do give rise to maximal points, and then try to deﬁne suitable
reduction functions for removing the application of ≡P .3 A discussion of whether
and how these strategies are feasible would, however, lead us too far astray—but
may constitute the topic of future works.
3 One may also expect that a similar situation may occur because of an application of (⊥G), but
in a normal derivation this cannot happen, for it generally holds what follows. Let π be a path
in a normal derivation, and let s1, . . . , sn be the segments of π whose formulas are respectively
A1, . . . , An. If, for some i < j ≤n there are Ai, Aj such that Ai = T : α and Aj = U : β with
k1(α) > k2(β), then there is i < h < j such that Ah is the major premise of a type elimination
rule. This suggests in turn that an order in normal derivations may be found not with respects to
formulas, but with respect to types, i.e. the type of each term occurring in a normal derivation is a
sub-formula of the type of some term which occurs either in some sub-formula of the conclusion,
or in some sub-formula of some element of the set of undischarged assumptions. In particular,
given a path π in a normal derivation without applications of D→or D∀(or deﬁned in such a
way that each path splits into an E-part, an I-part and a central part with only equivalence and
typing rules), and given the sequence of segments s1, . . . , sn in its central part whose formulas are
respectively A1, . . . , An, for every i ≤n there is j ≤n such that either the type of the term(s)
occurring in Ai is an immediate sub-formula of the type of the term(s) occurring in Aj, or vice
versa. We may then call maximum type-point any Ai such that the type of the term(s) occurring in
Ai is not the immediate sub-formula of the type of the term(s) of any Aj. The problem of ﬁnding
an order in the central part of paths of normal derivations becomes then the problem of whether
these paths can have a single maximum type-point.

Chapter 7
Completeness and Recognizability
7.1
About Completeness
In this chapter we deal with two topics: completeness of intuitionistic ﬁrst-order
logic with respect to the formal ground-theoretic framework outlined in Chaps. 4, 5
and 6, and the recognizability problem, discussed in Chaps. 3 and 4, read through
the lens of the formal framework of the last two chapters. Let us begin with
completeness.
7.1.1
From Validity to Universal Validity
In Chap. 4 we saw that an inference is to be understood as valid on B if, and only
if, there is a B-operation on grounds associated with it, that is, a B-operation that
has as domain the premises, and as (codomain of the) codomain the conclusion
of the inference—as well as appropriate discharges of assumptions and bindings
of variables. On the other hand, an inference rule is valid on B if, and only if,
there is a “structural” B-operation on grounds associable to it, where the notion of
structurality is to be understood in the sense indicated by the following words of
Prawitz:
this operation has no speciﬁc type, but is speciﬁed by a term with ambiguous types; for each
instance of the form, the operation is of a speciﬁc type and is denoted by the corresponding
instance of that term. (Prawitz, 2015, 95)
A proof on B is a ﬁnite chain of inferences valid on B. In the following, we indicate
with
 |	B α
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_7
247

248
7
Completeness and Recognizability
the fact that there is a proof on B with  as a set of assumptions, α as a conclusion,
and a set of free individual variables equal to that of the individual variables
occurring free in  and α.
It is of course possible to obtain a universal notion of validity. We could say that
an inference and an inference rule are valid if, and only if, for every base B, they
are valid on B. In turn, this can be understood in the following two ways:
(|	1)
for every atomic base B, the inference can be associated to a B-operation
on grounds, and the inference rule can be associated to a “structural” B-
operation on grounds, and
(|	2)
the inference can be associated to a universal operation on grounds, and
the inference rule can be associated to a “structural” universal operation on
grounds.
The notion of universal operation on grounds is here to be understood as deﬁned in
Chap. 5. In what follows, we shall adopt (|	2)—which obviously implies (|	1).1
An inference is valid if, and only if, there is a universal operation on grounds
1 When adapting to our framework a proof by Piecha and Schroeder-Heister (Piecha and Schroeder-
Heister, 2018), we will instead use (|	1). However, observe that there could be reasons to maintain
that also (|	1) implies (|	2), and hence that the two formulations above are equivalent. Indeed,
suppose that, for every atomic base B, there is a B-operation on grounds fB - which, for the sake
of simplicity, we suppose to be not deﬁned on individuals—of an appropriate operational type
τ1, . . . , τn ▷τn+1—which, by the assumption on fB, will have no free individual variables—by
virtue of which an inference J can be said to be valid on B. Let us now deﬁne an operation f of
the same operational type as fB by requiring that, for every B, for every gi ground on B for τi
(i ≤n),
f (g1, . . . , gn) = fB(g1, . . . , gn).
Thanks to this deﬁnition, the operation f could be understood as a universal operation on grounds
of the indicated operational type. However, in the deﬁnition of f it is essential to quantify over all
bases; on the other hand, a universal operation on grounds as deﬁned in Chap. 5 is to be understood
as an operation that is deﬁned “in the same way” on all bases. Here, we can perhaps reason as
follows. Given an atomic base B on a ﬁrst-order logical language L, for every α ∈FORML let us
consider the set
Grα
B = {g | g is a ground on B for ⊢α}.
Then, called B the set of all the atomic bases on L, let us consider the class
Grα =

B∈B
Grα
B
Let us suppose that, for every B, there is a constructive function
fB : Grα
B →Grβ
B.
Now, an operation on grounds which is universal in the sense we have indicated in Chapter 5 will
be a constructive function
f ∗: Grα →Grβ

7.1
About Completeness
249
associated to it, having as domain the premises, and as (codomain of the) codomain
the conclusion of the inference—as well as appropriate discharges of assumptions
and bindings of variables. An inference rule is valid if, and only if, there is a
universal “structural” operation on grounds associated to it. A proof is a ﬁnite chain
of valid inferences. Finally, we denote with
 |	2 α
the fact that there is a proof with  as a set of assumptions, α as a conclusion, and
a set of free individual variables equal to that of the individual variables occurring
free in  and α.
Observation 60  |	B α if, and only if, there is a proper ground on B for  ⊢α -
that is, a B-operation on grounds of operational type  ▷α relative to all and only
the individual variables occurring free in  and α. Likewise,  |	2 α if, and only
if, there is a proper universal ground for  ⊢α—that is, a universal operation on
grounds of operational type  ▷α relative to all and only the individual variables
occurring free in  and α.
The correctness of the observation can be shown as follows. According to the
deﬁnition of  |	B α, there exists a proof π on B with set of assumptions 
and conclusion α, and of which the free individual variables are all and only the
individual variables occurring overall free in  and α. A proof on B is a chain of
inferences valid on B. Let J1, . . . , Jn be the inferences valid on B involved in π.
For each i ≤n, Ji can be associated to a B-operation on grounds fi. But then, π
can be conceived as a composite B-operation on grounds f1 ◦· · · ◦fn, and, more
speciﬁcally, for the assumption on free individual variables of π, as a proper ground
on B for  ⊢α. The latter will be, by deﬁnition, a B-operation on grounds of the
required type. By deﬁnition of  |	2 α, instead, there exists a proof π with set of
assumptions  and conclusion α, and of which the free individual variables are all
and only the individual variables occurring overall free in  and α. A proof is a
chain of valid inferences. Let J1, . . . , Jn be the valid inferences involved in π. For
each i ≤n, Ji can be associated to a universal operation on grounds fi. But then,
π can be conceived as a composite universal operation on grounds f1 ◦· · · ◦fn,
and, more speciﬁcally, for the assumption on the free individual variables of π, as
such that, for every g ∈Grα, the value f ∗(g) can be speciﬁed by the same deﬁning equation
regardless of the class Grα
B to which g belongs. Instead, an operation on grounds which is universal
in the sense required to move from (|	1) to (|	2), will be a function
f ∗∗: Grα →Grβ
such that, for every g ∈Grα, f ∗∗(g) = fB(g), for g ∈Grα
B. In order to move from f ∗∗to f ∗,
we need the axiom of choice, which however is known to be valid (and even provable) in certain
constructivist setups. A similar argument obviously applies if, from the speciﬁc inferences, we turn
to inference rules.

250
7
Completeness and Recognizability
a proper universal ground for  ⊢α. The latter will be, by deﬁnition, a universal
operation on grounds of the required type.
7.1.2
Correctness of First-Order Intuitionistic Logic
It is now easy to establish that all the inference rules of Gentzen’s intuitionistic ﬁrst-
order logic, constituting the system IL of Chap. 3, are valid in the intended sense.
For the introduction rules, there exist clearly universal operations on grounds—
the primitive ones involved in the clauses (AtG)—(⊥G)—that, when appropriately
associated with each instance of the rules, make these instances valid. The oper-
ations we deﬁned in Chap. 5, associated to the non-primitive operational symbols
of a Gentzen-language, are instead universal operations on grounds which, when
appropriately associated to instances of the elimination rules, make them similarly
valid. From this we obtain, almost immediately, a result of correctness of IL with
respect to the notion of logical consequence deﬁned above.
Theorem 61  ⊢IL α ⇒ |	2 α.
Proof Let  be a derivation of α from  in DERIL. Since all the inference rules of
IL are valid, all the instances of such rules occurring in  will be valid as well. So,
 is a ﬁnite chain of valid inferences in the intended sense, and hence it is a proof
of α from the set of assumptions .
⊓⊔
7.1.3
Accounts of Ground-Theoretic Completeness
At this point, it would be natural to question whether, in addition to being correct
with respect to the ground-theoretical notion of logical consequence, Gentzen’s
natural deduction for ﬁrst-order intuitionist logic is also in this sense complete:
 |	2 α ⇒ ⊢IL α.
With reference to his semantics of valid arguments, Prawitz raises the completeness
issue in terms of a famous conjecture. The formulation we mention here is the one
from the 1973 Towards a foundation of a general proof-theory (Prawitz, 1973). The
notion of derivability of a rule in IL is the standard one (see, for example, von
Plato, 2014), while the notion of inference rule valid according to proof-theoretic
semantics is the one we provided in Chap. 3.
Conjecture 62 (Prawitz’s Conjecture) If an inference rule R is valid, then R is
derivable in IL.
Since we also have a result of proof-theoretic correctness of IL, if Conjecture 62
were correct we would have that the set of proof-theoretically valid inference rules

7.1
About Completeness
251
is identical to the set of rules derivable in IL (for an overview of the surveys related
to Prawitz’s conjecture, see Piecha (2016); the refutation for the semantics of valid
arguments is found in Piecha and Schroeder-Heister (2018)).
In the theory of grounds, an inference rule is valid if, and only if, there is
a universal “structural” operation on grounds associated to it. In light of this
deﬁnition, the conjecture can be reformulated as follows.
Conjecture 63 (Prawitz’s Conjecture in Ground-Theoretic Form) Given an opera-
tional type τ1, . . . , τn ▷τn+1, if there is a universal operation on grounds f of such
type, then the inference
is derivable in IL, where:
•
σi is τi if τi has empty domain, and
i
...
αi
if τi has domain i and co-domain αi (i ≤n);
•
f binds x and ξγ on index i if, and only if, R binds x and discharges γ on σi
(i ≤n), and
•
β is the co-domain of τn+1.
Proposition 64 If Conjecture 63 is correct,  |	2 α ⇒ ⊢IL α.
Proof Suppose that  |	2 α—namely that there is a proof π with a set of
assumptions  and a conclusion α, and of which the individual variables are all
and only those occurring free in  and α. π is a chain of valid inferences J1, . . . , Jn
and, from observation 60, it can be understood as a composite universal operation
on grounds f1 ◦· · · ◦fn, where each fi is associable to an inference Ji (i ≤n). We
can now proceed in two possible ways.
(1) Given
 = {β1, . . . , βn}
and taken any atomic base B, we can deﬁne an operation
h(ξβ1, . . . , ξβn)
by requiring that, for every gi ground on B for ⊢βi (i ≤n),
h(g1, . . . , gn) = f1 ◦· · · ◦fn(g1, . . . , gn)

252
7
Completeness and Recognizability
Since f1 ◦· · · ◦fn is a composite universal operation on grounds, f1 ◦· · · ◦fn is,
more speciﬁcally, a universal operation on grounds, and hence also a B-operation
on grounds. Therefore
h(ξβ1, . . . , ξβn)
is a B-operation on grounds. However, due to the arbitrariness of B, it is more
generally a universal operation on grounds. According to Conjecture 63, then, the
rule
is derivable in IL. By deﬁnition of derivability, we conclude that  ⊢IL α.
(2) Conjecture 63 implies that each fi corresponds to a rule, say Ri, derivable in
IL. By replacing each Ji in π with the derivation of Ri, we obtain a derivation of α
from  in IL, i.e.  ⊢IL α.
⊓⊔
Conjecture 63 requires that, should a scheme of operational types be “inhabited”
by some universal operation on grounds with certain bindings, the operational type
be derivable in IL with appropriate bindings of individual variables and discharging
of assumptions. But the type could be “inhabited” by “extensionally” different
universal operations on grounds. However, derivations in IL correspond to terms of
Gen. Therefore, we may ask whether each of the universal operations “inhabiting”
the type is extensionally equal to the denotation of some term of Gen. This would
be a stronger conjecture, which we may call of full-completeness.
Conjecture 65 (Full-Completeness Conjecture) Given  over B1, let den∗
1 be
relative to  and such that, for some operational symbol φ of , den∗
1(φ) is a
universal operation on grounds f . Then, for some den∗
2 relative to Gen over B1,
for some φ1, . . . , φn operational symbols of Gen, there is a universal operation on
grounds h composed of den∗
2(φ1), . . . , den∗
2(φn) such that, for every B2, called g1
the ground on B2 such that den∗
1(φ) ≈B1,B2 g1, and g2 the ground on B2 such that
h ≈B1,B2 g2, g1 ≡B2 g2.
Proposition 66 Let Gen+ be a non-primitive expansion of Gen over an atomic
base B, obtained by adding to Gen a non-primitive operational symbol F, and let
den∗
1 be a denotation function for the elements of the alphabet of Gen+ such that
den∗
1(F) is a universal operation on grounds. Then, if Conjecture 65 is correct,
there is a denotation function den∗
2 for the elements of the alphabet of Gen+ such
that Gen+ is conservative with respect to den∗
2 over Gen.
Proof Let Gen+ and den∗
1 be as deﬁned in the hypotheses. Since we are supposing
that Conjecture 65 is correct, there will be a denotation function den∗
3 for the
elements of the alphabet of Gen such that
den∗
1(F) ≡B den∗
3(F1) ◦· · · ◦den∗
3(Fn)

7.1
About Completeness
253
where, for every i ≤n, Fi is an operational symbol of Gen. Hence, let us deﬁne the
denotation function den∗
2 for the elements of the alphabet of Gen+ such that, for
every operational symbol x of Gen+,
den∗
2(x) =

den∗
1(x)
if x ∈AlGen+ −AlGen
den∗
3(x)
if x ∈AlGen
On account of how den∗
2 has been deﬁned, we will have
den∗
2(F) ≡B den∗
2(F1) ◦· · · ◦den∗
2(Fn)
where, for every i
≤
n, Fi is an operational symbol of Gen. Hence, the
hypotheses of theorem 40 in Chapter 5 (rewritability of operational symbols implies
conservativity) are satisﬁed. It follows by this theorem that Gen+ is conservative
with respect to den∗
2 over Gen.2
⊓⊔
Another consequence of Conjecture 65 concerns the systems of grounding
outlined in Chap. 6. First of all, recall that the scheme of equations that, in a
system of grounding, ﬁx the deductive behavior of the non-primitive operational
symbols can be understood as “internalizations” of the schemes of equations that
set the behavior of the operations which, via denotation functions, these symbols are
intended to represent. Therefore, given an expansion Gen+ of Gen over an atomic
base B, obtained by adding a non-primitive operational symbol F, let den∗
1 be a
denotation function for the elements of the alphabet of Gen+ such that den∗
1(F)
is a universal operation on grounds f . The latter, will be deﬁned by m equations
(m ≥1), possibly involving other operations on grounds f i
j (i ∈N, j ≤m)
2 Proposition 66 shows that, given the stated hypotheses, Conjecture 65 implies the existence of
a denotation function for the elements of the alphabet of Gen+ with respect to which Gen+ is
conservative over Gen. Again under the same hypotheses, it would be possible to obtain also
the inverse implication by enriching the languages of grounding as indicated in Chap. 6, i.e., by
introducing variables of the type hα(x) and fβ. With this approach, ﬁrst of all, also the inverse
of Theorem 40 would hold. Let + be a non-primitive expansion of a language of grounding 
on an atomic base B, and let F be a non-primitive operational symbol properly in +. Let us
suppose that F binds an individual variable x on index i, where the i-th entry of the operational
type of F is αi(x), and a ground-variable ξγ on index j, where the j-th entry of the operational
type of F is αj . Finally, let den∗be a denotation function for the elements of the alphabet of
+, suitably deﬁned also on hα(x) and fα, and let den be the denotation function for the terms
of  associated with den∗. Then, den∗(F) ≡B den∗(F1) ◦· · · ◦den∗(Fn)—where, for every
i ≤n, Fi is an operational symbol of —if, and only if, there is T
∈TERM such that
den(F x ξ(. . . , hαi(x), fαj (ξγ ), . . . )) ≡B den(T ). In other words, in enriched languages of
grounding, it becomes possible to build terms whose denotation is identical to that of operational
symbols binding individual and ground-variables, just as occurs in non-enriched ones for the
operational symbols that do not bind any individual or ground-variables. Hence, the existence
of a denotation function den∗with respect to which Gen+ is conservative over Gen, implies the
rewritability via den∗of the operational symbols of Gen+ in Gen.

254
7
Completeness and Recognizability
according to the scheme
⎧
⎪⎪⎨
⎪⎪⎩
f = f 1
1 ◦· · · ◦f n1
1
if . . .
...
...
f = f 1
m ◦· · · ◦f nm
m
if . . .
In general, it is not guaranteed that, for every f i
j (i ≤nj, j ≤m) in the deﬁning
equation of an operational symbol x1 of Gen+, there exists an operational symbol
x2 of Gen+ such that den∗
1(x2) = f i
j . However, let us suppose that there is an
expansion Gen++ of Gen+ with non-primitive operational symbols such that it
is possible to deﬁne a denotation function den∗
2 for the elements of the alphabet
of Gen++ that complies with the condition above. We say that Gen++ is self-
contained with respect to den∗
2 - clearly, den∗
1(F) ≈B den∗
2(F) = f .3
We now enrich Gen++ appropriately, and call this enrichment Gen++
1
, in such
a way that it is possible to deﬁne on Gen++
1
an appropriate system of grounding
 which “internalizes” the schemes of equations that ﬁx den∗
2(x), for every non-
primitive operational symbol x of Gen++. We say that  totally interprets Gen++
1
with respect to den∗
2. If  complies with appropriate provability conditions—i.e.,
if it checks that non-primitive operational symbols are well-deﬁned—the following
proposition becomes provable.
Proposition 67 Let Gen+ be a non-primitive expansion of Gen, obtained by
adding to Gen a non-primitive operational symbol F, and let den∗
1 be a denotation
function for the elements of the alphabet of Gen+ such that den∗
1(F) is a universal
operation on grounds. Then, Conjecture 65 is equivalent to the following condition.
Let
•
Gen++ be an expansion of Gen+, and den∗
2 a denotation function for the
elements of the alphabet of Gen++ such that Gen++ is self-contained with
respect to den∗
2, and
•
 be a system of grounding on an appropriate enrichment Gen++
1
of Gen++
which totally interprets Gen++
1
with respect to den∗
2.
Then, there is a denotation function den∗
3 for the elements of the alphabet of Gen
with respect to which Gen is self-contained, and such that, given an appropriate
enrichment Gen∗of Gen, and a system of grounding ∗which totally interprets
Gen∗with respect to den∗
3, for every T ∈TERMGen++
1 , there is U ∈TERMGen∗such
3 Observe that a self-contained expansion may not be available. For example, the deﬁning equations
that can be associated to the various operational symbols may involve an inﬁnite chain: f1 requires
f2 requires . . . requires fn requires fn+1 requires . . . . Here, we may not ﬁnd a recursive language
of grounding, which takes into account all the fi-s (i ∈N).

7.1
About Completeness
255
that
⊢∪∗T ≡U.
We limit ourselves to just a sketch of the proof. For the left-to-right direction, from
Conjecture 65 we know that den∗
2(F) is an operation that is equivalent to a combi-
nation of operations denoted, via an appropriate denotation function, say den∗
3, by
certain operational symbols of the Gentzen-language. den∗
2 is “internalized” in ,
while den∗
3 is “internalized” in ∗, so that  ∪∗totally interprets both Gen++
1
and Gen∗. It is sufﬁcient at this point to ensure that  ∪∗allows us to derive
the rewritability of F in terms of operational symbols of Gen, in compliance with
the identity between the operation denoted by F via den∗
2 and the combination of
operations denoted by operational symbols of Gen via den∗
3. If this obtains, we
can replace each occurrence of F with its rewriting in Gen salva identitate. But
provability of rewriting is something that, if  and ∗are “sound” systems of
grounding, we can guarantee.
The right-to-left direction is even more straightforward. If the systems of
grounding “internalize” the deﬁning equations denoted by the operational symbols
via den∗
2 and den∗
3, we obtain the rewritability of the operation denoted by F via
den∗
1—recalling that den∗
2(F) “preserves” den∗
1(F)—in terms of an appropriate
combination of operations denoted by the operational symbols of Gen via den∗
3.
For example, if F binds ξγ on index i, and x and ξδ on index j, we ﬁnd that there
exists U ∈TERMGen∗such that
⊢∪∗F . . . x ξγ ξδ . . . (. . . fαi(ξγ ), fαj (ξδ) . . . ) ≡U
so that we can then quantify universally.
7.1.4
Incompleteness of Intuitionistic Logic
Conjecture 62, in a weak form, has recently been refuted by Thomas Piecha
and Schroeder-Heister in Incompleteness of intuitionistic propositional logic with
respect to proof-theoretic semantic (Piecha and Schroeder-Heister, 2018). In this
Section, we show how the proof by Piecha and Schroeder-Heister applies also to
the ground-theoretic framework. First of all, given a ﬁrst-order logical language L,
ﬁnite  ⊂FORML, and an atomic base B on L, we indicate with
|	B 
that |	B β for every β ∈.
Proposition 68 For every atomic base B,  |	B α ⇔(|	B  ⇒|	B α).

256
7
Completeness and Recognizability
Proof (	⇒) For arbitrary B, let us suppose that  |	B α—namely that there exists
a proof π on B with a set of assumptions  and a conclusion α, and the individual
variables of which are all and only those occurring free in  and α. As a chain of
inferences valid on B, say J1, . . . , Jn, from Observation 60 π can be understood as a
composite B-operation on grounds f1◦· · ·◦fn, where each fi can be associated with
an inference Ji (i ≤n) and, more speciﬁcally, for the restriction on free individual
variables, as a proper ground on B for  ⊢α. Let us now suppose that |	B  and,
setting
 = {β1, . . . , βn},
let πi be a proof on B of βi the individual variables of which are all and only those
occurring free in βi (i ≤n). As a chain of inferences valid on B, say J i
1, . . . , J i
mi,
from Observation 60 πi can be understood as a composite B-operation on grounds
f i
1 ◦· · · ◦f i
mi, where each f i
j can be associated with an inference J i
j (j ≤mi)
and, more speciﬁcally, for the restriction on free individual variables, πi can be
understood as a proper ground on B for ⊢βi. So, the application of f1 ◦· · · ◦fn
to the various f i
1 ◦· · · ◦f i
mi returns a proper ground on B for ⊢α. Each of the
B-operations on grounds involved in this ground, can be associated to an inference,
which will hence turn out to be valid on B. Moreover, depending on the way in
which the B-operations are combined, the inferences valid on B can be combined
so as to obtain a proof on B of α. Hence, |	B α.
(⇐	) Again, setting
 = {β1, . . . , βn},
since, according to Observation 60, each proof on B for βi or α can be understood
as a proper ground on B, say gi for ⊢βi or g for ⊢α (i ≤n), it is sufﬁcient to
deﬁne an operation
h(x, ξβ1, . . . , ξβn)
which, for every proper ground gi on B for ⊢βi as indicated above, points on a
speciﬁc g as indicated above, namely
h(x, g1, . . . , gn) = g.
We then have a B-operation on grounds of operational type  ▷α which, when
associated to the inference

7.1
About Completeness
257
makes it valid on B, so that  |	B α. Observe that the required operation can be
understood simply as the empty function if it is not the case that, for every i ≤n,
there is a proof on B for βi.
⊓⊔
Proposition 69 For every atomic base B:
(→)
|	B α →β if, and only if, α |	B β
(∨)
|	B α ∨β if, and only if, |	B α or |	B β
Proof The proof is trivial, in the light of Observation 60, and of the clauses (→G)
and (∨G).
⊓⊔
In order to reconstruct the reasoning of Piecha and Schroeder-Heister, it will be
sufﬁcient to limit ourselves to propositional logic and, as Piecha and Schroeder-
Heister do, to rely on some preliminary results, which are stated in the following
proposition (proof omitted).
Proposition 70 In IL the following circumstances hold:
1. Harrop’s rule
is not derivable (Harrop, 1960);
2. disjunctions can be eliminated from negated formulas, so as to obtain inter-
derivable formulas without ∨, by applying the following results
⎧
⎪⎪⎨
⎪⎪⎩
¬(α ∨β) ⊢IL ¬α ∧¬β and ¬α ∧¬β ⊢IL ¬(α ∨β)
¬(α ∧β) ⊢IL ¬(¬¬α ∧¬¬β) and ¬(¬¬α ∧¬¬β) ⊢IL ¬(α ∧β)
¬(α →β) ⊢IL ¬¬α ∧¬β and ¬¬α ∧¬β ⊢IL ¬(α →β)
We now adapt to the relation |	B a generalized disjunction property—that we
indicate with (GDP|	B ):
∨does not occur in  ⇒( |	B α ∨β ⇒ |	B α or  |	B β).
For the last step, we no longer understand validity as (|	2), but as (|	1)—both of
them as in Sect. 6.1.1. We have the following two results.
Proposition 71  |	2 α ⇒ |	1 α
Proof Suppose  |	2 α - namely, that a proof π exists with  as set of assumptions
and α as a conclusion, and a set of free individual variables equal to that of the
individual variables occurring free in  and α. Then, for every atomic base B, π
is a proof on B with  as set of assumptions and α as a conclusion, and a set of
free individual variables equal to that of the individual variables occurring free in

258
7
Completeness and Recognizability
 and α. Hence, for every atomic base B, there is a proof on B with  as set of
assumptions and α as a conclusion, and a set of free individual variables equal to
that of the individual variables occurring free in  and α. Therefore,  |	1 α.
⊓⊔
Proposition 72  ⊢IL α ⇒ |	1 α
Proof It follows immediately from the correctness theorem and from the previous
proposition.
⊓⊔
Theorem 73 (Piecha and Schroeder-Heister) If, for every atomic base B,
(GDP|	B) holds, then Harrop’s rule is valid.
Proof Since we are now adopting (|	1), we must prove that, for every atomic base
B, there is a B-operation on grounds of operational type
¬α →(β1 ∨β2) ▷(¬α →β1) ∨(¬α →β2).
By virtue of Observation 60, this can be done by showing that, for every atomic base
B,
¬α →(β1 ∨β2) |	B (¬α →β1) ∨(¬α →β2).
Therefore, let B be an arbitrary base, and let us suppose that
|	B ¬α →(β1 ∨β2).
By Proposition 69 we will have that
¬α |	B β1 ∨β2.
By Proposition 70, there exists a formula α∗without ∨such that
¬α ⊢IL α∗and α∗⊢IL ¬α.
By Proposition 72, we have in particular that
α∗|	1 ¬α
and hence, for our speciﬁc base B,
α∗|	B ¬α.
Again by virtue of Observation 60 (and of Proposition 20 in Sect. 5.2.2.5 about the
composition of B-operations on grounds), we therefore have that
α∗|	B β1 ∨β2.

7.1
About Completeness
259
Since we are assuming (GDP|	B), we have
α∗|	B βi (i = 1, 2).
Again by Proposition 72, we have that
¬α |	B α∗
and hence, by virtue again of Observation 60 (and of Proposition 20 in Sect. 5.2.2.5
about the composition of B-operations on grounds),
¬α |	B βi.
By Proposition 69, hence,
|	B ¬α →βi
and, again by Proposition 69,
|	B (¬α →β1) ∨(¬α →β2).
In conclusion, we have the following:
|	B ¬α →(β1 ∨β2) ⇒|	B (¬α →β1) ∨(¬α →β2).
By Proposition 68, we can thus conclude that
¬α →(β1 ∨β2) |	B (¬α →β1) ∨(¬α →β2).
The result now follows immediately from the arbitrariness of B.
⊓⊔
Theorem 74 (Piecha and Schroeder-Heister) (GDP|	B ) holds for every atomic
base B.
Proof Let B be an arbitrary base, and let us suppose that  |	B α ∨β. Then, by
Proposition 68, we have that
|	B  ⇒|	B α ∨β
and, by Proposition 69,
|	B  ⇒(|	B α or |	B β).
By adopting classical logic in the meta-language, we have
(|	B  ⇒|	B α) or (|	B  ⇒|	B β)

260
7
Completeness and Recognizability
and, by Proposition 68,
 |	B α or  |	B β.
The result now follows immediately from the arbitrariness of B.
⊓⊔
Corollary 75 (Piecha and Schroeder-Heister) Harrop’s rule is valid.
Proof It follows immediately from Theorems 73 and 74—bearing in mind that we
are adopting (|	1).
⊓⊔
Corollary 76 (Piecha and Schroeder-Heister) For some  and α,  |	1 α and
 ⊬IL α.
Proof Setting
 = ¬α →(β1 ∨β2) and α = (¬α →β1) ∨(¬α →β2)
—and again bearing in mind that we are adopting (|	1)—we know from corollary 75
that Harrop’s rule is valid, and hence that  |	1 α. By contrast, from Proposition 70,
we know that Harrop’s rule is not derivable in IL, and hence that  ⊬IL α.4
⊓⊔
Therefore, if we understand the notion of validity in the terms of (|	1), the
proof of Piecha and Schroeder-Heister shows that IL is incomplete with respect
to the theory of grounds. Note that it is not difﬁcult to outline an analogue of
Conjecture 63 that implies the completeness of IL also with respect to (|	1)—just
as Conjecture 63, as shown by Proposition 64, implies the completeness of IL with
respect to (|	2). Thus, the result of Piecha and Schroeder-Heister also implies a
refutation of the conjecture. In the concluding remarks of their paper, Piecha and
Schroeder-Heister notably observe that
the incompleteness of intuitionistic logic with respect to such a semantics therefore raises
the question of whether there is an intermediate logic between intuitionistic and classical
logic which is complete with respect to it. (Piecha and Schroeder-Heister 2018, 13)
It should be observed that the proof of Piecha and Schroeder-Heister applies to (|	1),
but it is not clear whether it also applies to (|	2). Although (|	2) certainly implies
(|	1), the reverse could not hold—see, however, note 1 in this chapter. In addition,
Proposition 68 is seemingly false when formulated on (|	2). In particular, there are
4 It should be observed that, if we adopt classical logic in the meta-language, one can easily prove
even the |	1 validity of the so-called Split-rule, which looks the same as Harrop’s rule, except that
the antecedent of the premise is not negated, nor is it required to be ∨-free. Piecha and Schroeder-
Heister prove their result with respect to Harrop’s rule because the latter allows for a stronger
result, which holds irrespective of whether the meta-theory is or not classical. This stronger result
relies upon principles like those that Piecha and Schroeder-Heister call Export and Import, and
may require a different treatment of atomic bases. Here, I preferred to stick to Harrop’s rule, in
such a way as to give at least a sketch Piecha and Schroeder-Heister’s broader strategy in their
2018 paper.

7.2
Recognizability and Equations
261
 and α such that |	2  ⇒|	2 α, but it does not hold that  |	2 α. Take  = {p}
and α = q. Obviously, neither|	2 p, nor |	2 q, but we can construct an atomic base
that has p as unique axiom, and on which therefore no operation on grounds can
exist of operational type p ▷q.5
Be that as it may, we can adapt the aforementioned observation of Piecha and
Schroeder-Heister to our case. If intuitionistic logic is not complete with respect
to the theory of grounds, then there could be some intermediate logic for which
completeness applies. With regard to this logic, it would remain to verify whether,
besides being complete in the sense indicated by Conjecture 63, it is also full-
complete in the sense indicated by Conjecture 65. If this were the case, after
drawing up a language of grounding that, on a par with Gen for intuitionistic logic,
constitutes a functional isomorphic translation of this intermediate logic, we will
have the results indicated in Propositions 66 and 67. However, observe that there
may simply be no intermediate logic which is complete with respect to the theory
of grounds - whether in terms of (|	1) or in the stronger terms of (|	2). In other
words, the class of rules which are valid according to Prawitz’s semantics of valid
arguments (and hence to the theory of grounds) could, after all, be non-recursive.
7.2
Recognizability and Equations
In this section, we deal more extensively with the recognizability problem that, as
seen in Chap. 4, the theory of grounds inherits from the semantics of valid arguments
and proofs—and that it shares with BHK semantics. This will be done along two
lines.
First, we show that the recognizability thesis has both a weak and a strong
reading. If some background assumptions are accepted—in particular, the adoption
of classical logic in the meta-language—the weak reading, unlike the strong one,
would seem plausible. If, however, these assumptions are rejected—in particular,
if intuitionistic logic is used also in meta-language - the weak reading becomes
problematic too.
5 Piecha and Schroeder-Heister build their proof of incompleteness on the basis of some general
semantic principles. Among these there is also the following:  |	 α ⇔for every atomic base B,
(|	B  ⇒|	B α). As said, in our frameworks it nonetheless holds that: for every atomic base B,
 |	B α ⇔(|	B  ⇒|	B α). Therefore, if Piecha and Schroeder-Heister’s semantic principle
held also in our framework, we would have that:  |	 α ⇔for every atomic base B,  |	B α.
Now, this is exactly how the relation (|	1) is characterised, i.e.,  |	1 α if, and only if, for every
atomic base B, there is an operation f such that f is a B-operation on grounds of operational
type  ▷α. But the equivalence seems not to work in the case of (|	2), where instead we have an
inversion of the quantiﬁers:  |	2 α if, and only if, there is an operation f such that, for every
atomic base B, f is a B-operation on grounds of operational type  ▷α. It should be noted that
the latter seems to be the way in which Prawitz also characterizes his notion of valid inference in
Towards a foundation of a general proof theory (Prawitz, 1973) and in An approach to general
proof theory and to conjecture of a kind of completeness of intuitionistic logic revisited (Prawitz,
2014).

262
7
Completeness and Recognizability
Second, we show that, from a ground-theoretic point of view, the recognizability
problem can be understood as the issue of a general theory of deﬁning equations
for non-primitive operations on grounds. We provide general guidelines for a ﬁrst
classiﬁcation of these equations, investigating which kinds of equation validate the
desired recognizability, and which, by contrast, lose it.
7.2.1
Local and Global Recognizability
In Chap. 3, we have extensively discussed a recognizability problem that Prawitz’s
proof-theoretic semantics inherits from the BHK clauses. As for the latter, epistemic
needs could lead one to request that a proof of α →β is not simply a constructive
function which, when applied to any BHK proof of α, turns it into a BHK proof
of β, and that a proof of ∀xα(x) is not simply a constructive function which, when
applied to any individual k in a reference domain, produces a BHK proof of α(k).
Since proofs have an epistemic relevance for us only when we are able to recognize
that they prove what we intend to prove, we might be right to postulate that the
above functions must be accompanied by the recognition of the fact that they have
an appropriate behavior. Likewise, we might require, for valid arguments and proofs
in Prawitz’s proof-theoretic semantics, that it be recognizable that a valid closed
non-canonical argument or a non-canonical/categorical proof reduces, respectively,
to a valid closed canonical argument and to a canonical proof, and that a valid open
argument or a hypothetical-general proof is such that all its closed instances are
valid arguments and proofs.
In the theory of grounds, the problem becomes that of recognizing that a term
of an appropriate language of grounding denotes a ground for a certain judgment or
assertion. If the term is closed, it must be possible to recognize that it reduces to a
canonical form which denotes a ground according to the clauses (∧G)—(∃G) and, if
the term is instead open, it must be possible to recognize that it denotes an operation
on grounds of the appropriate operational type. From the problem of recognizing the
denotation of a term, we can then pass to that of recognizing the operational type
of an operation on grounds, and hence to that of recognizing whether the deﬁning
equation of this operation is well-given. As we have seen in Chap. 6, in many
cases it is possible to prove that speciﬁc equations deﬁne appropriately speciﬁc
operations on grounds. To raise the recognizability issue means asking whether the
recognition guaranteed by proofs of that type is possible for arbitrary operations,
ﬁxed by equally arbitrary equations.
The recognizability problem can be understood along two different degrees of
generality. In the weaker degree, which we call local recognition, we ask whether
it is possible to recognize, for every equation ε that deﬁnes an operation on
grounds f of operational type τ, that ε deﬁnes f so that the latter has truly
an operational type τ, but without requiring additionally that the recognition be
achieved through a method that works also for other equations and operations. In
the stronger degree, which we call global recognizability, we instead ask whether it

7.2
Recognizability and Equations
263
is possible to recognize, for every equation ε that deﬁnes an operation on grounds
f of operational type τ, that ε deﬁnes f so that the latter truly has an operational
type τ, with such recognition achieved through a universal method that allows us
to reach homogeneously the same type of recognition for all possible equations
and all possible operations. In local recognition, therefore, recognition is achieved
by applying different methods in a “case by case” way. By contrast, in global
recognizability there is one method that is uniformly applicable to all cases. We
therefore have two different theses of recognizability, which differ from one another
in the order of the universal and existential quantiﬁers:
(L)
for every equation ε that deﬁnes an operation on grounds f of operational
type τ, there is a procedure ℘such that ℘allows recognition that ε deﬁnes an
operation on grounds f of operational type τ;
(G)
there is a procedure ℘such that, for every equation ε that deﬁnes an operation
on grounds f of operational type τ, ℘allows recognition that ε deﬁnes an
operation on grounds f of operational type τ.
At ﬁrst glance, one would say that (L) and (G) are completely different claims. In
particular, (L), unlike (G), seems plausible. An operation on grounds, in fact, is an
epistemic object and, as such, it should always be in principle possible to recognize
its properties, included that of having a certain operational type. The equation
that deﬁnes an operation of this type, therefore, must be such as to attribute, in a
recognizable way, the operational type required. In addition, if we accept classical
logic, the negation of (L) would correspond to the existence of an equation ε that
deﬁnes an operation on grounds f of operational type τ, such that, whatever the
procedure ℘is, ℘does not permit recognition that ε deﬁnes an operation on grounds
f of operational type τ; namely, we would have an epistemic object with absolutely
unknowable properties—a sort of contradictio in terminis. By contrast, (G) requires
the existence of a universal procedure, homogeneous and uniform, which works on
all equations. If by “recognizability” we mean “decidability”, the existence of such a
procedure is clearly impossible. But even in a less restricted reading, (G) would still
seem to put forward an excessive claim, for it would not make much sense to invoke
the existence of a universal, homogeneous and uniform recognition method, unless
we are able to provide, at the very least, a generic description of the instructions it
contains. However, it is far from obvious how—and indeed very unlikely that—such
a description can ever be found.
In substantiating the plausibility of (L), we referred to the use of classical logic.
However, if the logic we intend to use in dealing with the sustainability of (L) is not
classical but intuitionistic, then also the plausibility of (L) is called into question.
The problem is that, in this case, it would seem possible to argue that (L) implies (G),
and therefore that, if we accept (L), we are consequently forced to also accept (G).
The way to this conclusion is reminiscent of that with which the intuitionists justify
the validity of the axiom of choice. According to Martin-Löf—whose intuitionistic
type theory actually proves the axiom of choice:
the usual argument in intuitionistic mathematics, based on the intuitionistic interpretation
of the logical constants, is roughly as follows: to prove ∀x∃yC(x, y) →∃f ∀xC(x, f (x)),

264
7
Completeness and Recognizability
assume that we have a proof of the antecedent. This means that we have a method which,
applied to an arbitrary x, yields a proof of ∃yC(x, y), that is, a pair consisting of an element
y and a proof of C(x, y). Let f be the method which, to an arbitrarily given x, assigns the
ﬁrst component of this pair. Then C(x, f (x)) holds for an arbitrary x, and hence so does
the consequent. (Martin-Löf, 1984, 50)
By adapting this reasoning to our case, we can therefore take into account the
implication
∀ε ∃℘ε R(ε, ℘ε) →∃℘∀ε R(ε, ℘(ε))
where R is the binary predicate “the procedure . . . allows the recognition on the
equation −−−”. If we suppose we have a proof of the antecedent, we will have a
constructive function F1 such that, for every equation ε,
F1(ε) = (℘ε, πε)
where ℘ε is a recognition procedure, and πε is a proof of R(ε, ℘ε). We can now
deﬁne two operations, F2 and F3, such that, for every equation ε,
F2(ε) = d1(F1(ε)) and F3(ε) = d2(F1(ε))
where d1 and d2 are, respectively, left and right projections. By using λ-abstraction,
we will then have
F2 = λε.d1(F1(ε)) and F3 = λε.d2(F1(ε))
and therefore the sought proof of the implication will be
λF1.(F2, F3).
As regards the implication, it is perhaps appropriate to observe that both ℘ε and
℘are functions of ε, but in a signiﬁcantly different way. ℘ε is a procedure which,
when applied to ε, allows us to recognize in a direct way that ε deﬁnes an operation
on grounds having a certain operational type; ℘guarantees the same recognition,
but only through the choice of an appropriate ℘ε.
7.2.2
Parameters and Structure of Equations
If the recognizability problem is read in such a way that “recognizable” means
“decidable”, its solution is certainly negative in nature. Since we have no limitation
on the class of the operations of the theory, this class will be “too large”, thereby
subjected to all the limitations imposed by Gödel’s theorems.

7.2
Recognizability and Equations
265
If this is true, however, it is also true that it is precisely the absence of more
speciﬁc indications on the operations and on the structure of their deﬁning equations
that makes it difﬁcult to understand the recognizability problem as a problem of
decidability. Decidability is a precise mathematical notion, and to raise a question
of decidability makes sense only within a well speciﬁed formal context. Thus, a
more precise answer—even if not necessarily univocal—to our problem can come
by imposing some restrictions on the classes of operations and of deﬁning equations.
We could indicate a certain number of basic properties that operations must comply
with, alongside a certain number of parameters that the equations that deﬁne them
must satisfy.
A fruitful proposal in this sense—which we will limit ourselves to suggesting
in very broad lines—could be inspired, on the one hand, by the reﬂections that
led Prawitz (Prawitz, 2018) to the notion of analytically valid argument, discussed
in Chap. 3, and to Constructive semantics (Prawitz, 1970), whose content was
illustrated in Chap. 4. The notion of analytically valid argument is based on a
notion of containment such that a closed non-canonical argument can be said to be
analytically valid if it contains an analytically valid closed canonical one. In order to
determine whether containment holds or not, Prawitz authorizes two operations:
1. extraction of a sub-argument;
2. substitution of free individual variables with terms, and of assumptions with
closed analytically valid arguments for such assumptions.
In Constructive semantics, instead, Prawitz aims at a notion of construction, and to
a class of terms that denote constructions. Terms are constructed using operational
symbols with intended interpretation as follows:
(a) pair formation;
(b) left and right projection on pair;
(c) λ-abstraction;
(d) application for a λ-abstraction;
(e) 4-ary choice function, that selects the ﬁrst or second element of its arguments
depending on whether the third and the fourth are or not equal.
There would also be a function replacing individual and typed variables, but here
we can leave it to one side. The operations of points (a) and (c) are primitive: λ-
abstraction allows the construction of canonical objects for α →β or ∀xα(x),
when applied, respectively, to an operation that transforms constructions of α into
constructions of β binding ξα, or to an operation that transforms individuals k
from a reference domain into constructions of α(k) binding x; pair formation
obviously allows the construction of canonical objects for α ∧β—when applied
to constructions of α and β—but also for α1 ∨α2—a pair whose ﬁrst element
is a construction g of αi (i = 1, 2) and whose second element is an indication
of what disjunct g is a construction—as well as for ∃xα(x)—a pair whose ﬁrst
element is a construction of α(t) and whose second element is the term t. The
operations at points (b), (d) and (e) are instead non-primitive, and correspond quite
well to operations 1 and 2 above; application for the λ-abstraction recalls operation

266
7
Completeness and Recognizability
2; projection on pair is, obviously, an example of extraction from an argument that
contains at least two immediate subarguments,while the choice function could allow
us to choose which of these two subarguments should be picked up.
With his notion of analytically valid argument, Prawitz seems to suggest that
the reduction or justiﬁcation procedure associated with a rule in non-introductory
form should be such that the argument it generates as output is obtained from the
input argument by applying only operations 1 and 2; in Constructive semantics,
Prawitz shows instead the adequacy of Gentzen’s elimination rules by offering
a functional interpretation of the latter that contains only the operations (a)—
(e). We can generalize what Prawitz does in Constructive semantics and, relying
on the similarities highlighted between Constructive semantics and the notion
of analytically valid argument, request that non-canonical cases be justiﬁed by
resorting to a well-deﬁned stock of operations. More speciﬁcally, we could request
that the deﬁning equation of a given operation on ground F be such as to express
F as a combination of some or all the operations of formation of pair, projection
on pair, λ-abstraction, application of λ-abstraction, and choice function, and of no
other operation.6
Delving into the technical details of this proposal would take us far beyond the
scope of this work. Thus, we will be satisﬁed with the suggestion made, turning now
to a ﬁnal question.
Are there general properties of the deﬁning equations that inﬂuence recogniz-
ability? Given that deﬁning equations can be of many different types, have more or
less complex structures, and enjoy a variety of characteristics, we could instead ask
which of these types, structures and characteristics allow recognizability, and which,
on the contrary, cause its loss. In this way, we could link the theory of grounds to a
kind of theory of deﬁnitions. The survey on the different typologies of deﬁnitions,
and on the criteria of their acceptability, has a long history and is very general, but
6 Let us indicate pair formation with D, projection on a pair with Di (i = 1, 2), and application of
λ-abstraction with App, and let us suppose that a ground for ⊢α1 ∨α2 is of the form (g, i), with
g ground for ⊢αi (i = 1, 2)—or, alternatively, ((g, i), αj ) (j = 1, 2), if we wish to indicate the
other disjunct—and that a ground for ∃xα(x) is of the form (g, t), with g ground for ⊢α(t). We
can deﬁne the operational symbols ∧E,i, ∨E, →E, ∀E and ∃E as follows:
∧E,i(ξα1∧α2) = Di(ξα1∧α2)
∨E ξ α1 ξ α2(ξ α1∨α2, h1(ξ α1), h2(ξ α2)) = App(λξ αD2(ξα1∨α2 ).hD2(ξα1∨α2 )(ξ αD2(ξα1∨α2 )), D1(ξ α1∨α2))
→E(ξα→β, ξα) = App(ξα→β, ξα)
∀E(ξ∀xα(x), y) = App(ξ∀xα(x), y)
∃E x ξα(x)(ξ∃xα(x), h(x, ξα(x))) = App(App(λxλξα(x).(h(x, ξα(x)), D2(ξ∃xα(x))), D1(ξ∃xα(x)))
Note that, strictly speaking, we did not make use of the function of choice; the latter serves only
in the case of the elimination of the disjunction and we have preferred, instead, to link the index
of the argument to be chosen with the projection on the second element of the possible ground for
⊢α1 ∨α2 - we should have written D2(D1(ξα1∨α2)) if the ground for ⊢α1 ∨α2 were of the form
((g, i), αj ).

7.2
Recognizability and Equations
267
it has often received a targeted and in-depth analysis (among the various works that
can be mentioned in this regard, consider for example Padoa (1901), Carnap (1928),
and Suppes (1999); in what they call revision theory of truth, and with particular
reference to paradoxes and to truth, also Gupta and Belnap (1993) deal with the
theory of deﬁnitions, as does Kramer (2016)). Here, we intend to propose an initial
classiﬁcation of the possible deﬁning equations for operations on grounds into three
macro-groups. As a preliminary step, we ﬁrst highlight some points.
So far, an operation on grounds f has been understood as being ﬁxed by an
equation ε, but this is actually inaccurate. In general, it could be necessary to resort,
not to a single equation scheme, but to a system of equation schemes 1, . . . , m.
The scheme will moreover involve conditional clauses c1, . . . , cm such that, in the
case ci, computation of f is achieved by applying scheme i (i ≤m). The general
form of the equational deﬁnition of an n-ary operation can therefore be represented
graphically as follows:
f (a1, . . . , an) =
⎧
⎪⎪⎨
⎪⎪⎩
1
if c1
......
m
if cm
Each i (i ≤m) could in turn mention other operations f i
1, . . . , f i
p, which we
indicate by writing i as
f i
1 ◦· · · ◦f i
pi(a1, . . . , an).
In the following, we will call
f (a1, . . . , an)
the deﬁniendum, the system of equational schemes the deﬁniens, and a1, . . . , an the
equational context. Obviously, we take into account the case of m = 1, and also that
of m = 0, which covers the empty function.
So let f be an operation on grounds, and let S be the system that deﬁnes it. We
say that f is recursively deﬁned if, and only if, there exists a scheme of equations
 of S in which f appears. The graphical representation is, in this case,
f (a1, . . . , an) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
1
if c1
...
...
f p
1 ◦· · · ◦f ◦· · · ◦f p
qp(a1, . . . , an)
if cp
...
...
m
if cm

268
7
Completeness and Recognizability
Obviously, we are assuming that the deﬁnition is non-circular—namely that it does
not create inﬁnite chains or loops. A recursive deﬁnition will then proceed, in
general, by setting the value of f on arguments of a certain complexity, in terms
of the value of f on arguments of lower complexity.
In this context, it is difﬁcult to imagine a positive solution to the recognizability
problem, and this even independently of well-known limitative results for analogous
deﬁnitions in arithmetic. The fact that the deﬁniendum occurs in the deﬁniens
makes recursive deﬁnitions very complex, and the task of establishing, say, that this
circumstance does not cause circularity or loops will be complex as well. Since one
has to reason on the degrees of the arguments which the operation applies to, one
must reason on the form of such arguments, and on the reciprocal relation between
arguments of different complexity. It may also be essential to reason on the nature of
the intended domain of application—the deﬁnition could work on certain domains,
but fail on others.
As an example, let us consider the recursively deﬁned operation Ind of Chap. 5.
In order to verify whether the system that deﬁnes it is well-given, we ﬁrst have
to establish that the value of the operation is well-deﬁned with respect to a case
of minimal complexity. Then, we have to prove that the reduction from a case of
complexity δ to the case of complexity δ −1 works via reducibility of the case of
complexity δ−1 to gradually less complex cases, up to the minimum degree, and that
this applies whatever the degree is. We must know that the application domain can
be canonically represented in the atomic system as 0, or s(s(. . . (s(0)) . . .), where s
is the successor function. This serves to establish that the reduction from complexity
δ to complexity δ −1 deﬁnes the function on all possible relevant arguments. But
this also means that some understanding is required of the fact that the steps just
described ﬁt with the “structure” of the intended domain, which in turn requires
some kind of meta-induction.
Recursive deﬁnitions represent a ﬁrst macro-group. A second group is obtained
simply by denying that the system is recursive—namely, f is deﬁned in a non-
recursive way if, and only if, in its deﬁning system S there is no  in which f
appears. Non-recursively deﬁned operations can in turn be divided into two classes.
Let f be an operation on grounds, and let S be the system that deﬁnes it. We say
that f is deﬁned contextually if, and only if, there is a scheme of equations  of S
such that each component of the expression of  already occurs in the equational
context. Examples of contextual deﬁnitions are those for the operations on grounds
associated to the operational symbols ∧E,i (i = 1, 2), ∨E, →E, ∀E and ∃E, as
well as the deﬁnition of the operation DS, i.e.
DS(∨I(g1), g2) = g1
—note that all of these deﬁnitions are given in Chap. 4.
Although proving that an operation deﬁned contextually is well-given can be
much easier than obtaining the same for a recursively deﬁned operation, recogniz-
ability nonetheless remains problematic. Here too, recognition can be obtained only
by reasoning on the equational context, and generally a contextual deﬁnition shows

7.2
Recognizability and Equations
269
that the arguments of the context have a speciﬁc structure. For example, in the case
of the operations associated to the operational symbols ∧E,i (i = 1, 2), ∨E, →E,
∀E and ∃E, recognition will depend essentially on the form of the grounds for
different logical forms. In addition, in many cases it is often necessary to reﬂect
on the general structure of the expected operational type. For example, in the case
of the operation DS, we must note that if g2 is a ground for ⊢¬α, then g1 must
necessarily be a ground for ⊢β, because if it were a ground for ⊢α we could, by
combining it with g2, get a ground for ⊢⊥. Such reﬂections usually take place at
a meta-level and, once again, there seems to be no upper bound to their potential
complexity.
Now we come to the last type of non-recursive operations. Let f be an operation
on grounds, and let S be the system that deﬁnes it. We say that f is reductively
deﬁned if, and only if: (1) in no component of the expression of the equational
context do there occur other operations on grounds and (2) each scheme of
the equations  of S is a composition of a non-null number of operations on
grounds (obviously different from f ). The deﬁnition may therefore be represented
graphically as
f (a1, . . . , an) =
⎧
⎪⎪⎨
⎪⎪⎩
f 1
1 ◦· · · ◦f 1
p1(a1, . . . , an)
if c1
...
...
f m
1 ◦· · · ◦f m
pm(a1, . . . , an)
if cm
with pi ≥1 (i ≤m), and where no aj (j ≤n) mentions operations on grounds
of any sort - in other words, aj can be understood as a meta-variable for grounds
or operations on grounds. S does nothing but “reduce” the behavior of f to the
behavior (of the combination) of other operations. Thus, we do not take into account
in the strict sense the application context of f , but only the application context of
other operations in the terms of which f can be deﬁned. An example of reductive
deﬁnition is that of the operation f corresponding to the disjunctive syllogism,
whenever it is deﬁned by the equation scheme
f (g1, g2) = f∨ξα ξβ(g1, ⊥β(f→(ξα, g2)), ξβ)
where, as noted, there is no further speciﬁcation on the structure of g1 and g2—
it is only necessary to suppose that they are grounds for, respectively, ⊢α ∨β and
⊢¬α—and where the deﬁniens is a composition of a non-null number of operations
on grounds other than f .
Reductive deﬁnitions are a kind of “rewriting” of the operation to deﬁne
through a ﬁnite number of other operations. Hence, in this case we can indicate
a criterion that, when respected, guarantees recognizability: if we know that all
of the operations involved in the deﬁnition of f are well given, we can conclude
that also f is well-given—or better, we just have to verify that the combination
of operations in the system deﬁning f respects the typing, but this can be done

270
7
Completeness and Recognizability
through a simple type-checking algorithm. Obviously, we may not know that the
operations used in the deﬁnition of f are well-given. But we have to bear in mind
that when we handle operations on grounds, we can do this within a language or
a system of grounding as described in chaps. 5 and 6. And since these languages
and systems are obtained through expansions of less rich languages and systems,
we could guarantee—inductively,so to speak - the recognizability on languages and
systems in which the deﬁnitions (of the denotation) of non-primitive symbols are
given reductively, by appealing only (to the operations denoted by) the operational
symbols of the languages and systems prior to expansion.

Chapter 8
Conclusion
Undoubtedly, the theory of grounds share many points with the whole corpus of
Prawitz’s semantic investigations. The questions that Prawitz aims to answer are,
after all, the same that had already inspired the semantics of valid arguments and
proofs: what are correct inferences and reasoning, and why are they able to convey
knowledge, justiﬁcation, and epistemic constraint? However, radically different—
indeed, we could say almost diametrically opposite—is the perspective from which
these questions are understood and addressed.
The notions of deductively correct inference and reasoning are closely related,
and the treatment of one cannot but lead to the characterization of the other. That
is all the more true in the context of an explanation of the epistemic power they
both enjoy; the reasoning works thanks to the “goodness” of the inferences that
compose it, and the latter are acceptable if they can be satisfactorily used to construct
a convincing reasoning. But this leads to the question of where we should start the
analysis from: reasoning or inferences?
The semantics of valid arguments and proofs reconstructs the notion of inferen-
tial validity in the global terms of the structures in which these inferences occur.
The theory of grounds, instead, reverses the order and, by adopting a local point
of view, uses valid inferences as its bases, deﬁning valid arguments and proofs
as concatenations of valid inferences. To use a slogan, we pass from validity as
transmission of justiﬁcation, to validity as production of justiﬁcation. The change
of direction, while seemingly simple, requires a series of measures, and is fraught
with interesting consequences.
On what basis can we deﬁne inferential validity, once it is no longer described
in terms of the structures in which inferences occur? Prawitz’s answer cannot avoid
traveling along two paths. First of all, Prawitz needed to focus on the acts in which
inferences appear. But the act leads to a justiﬁcation state. Justiﬁcation states are
in turn reiﬁed as grounds, and hence considered as objects. In order for the bind to
work, we must ensure that the act is per se able to generate the object. Therefore, an
inference cannot be a mere transition to a certain conclusion from certain premises.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0_8
271

272
8
Conclusion
Otherwise, what it produces could not be distinguishable from (a description of) the
act constructed by performing it and, in non-canonical cases, it would be necessary
to go through a further justiﬁcation, which cannot but be “external” to the inferential
process itself. Inferences must then be applications of operations on grounds, and
grounds must be objects speciﬁable by simple induction, obtained by applying
only primitive, meaning-constitutive operations. In addition, operations must range
from grounds to grounds and not from non-interpreted structures to non-interpreted
structures, as happens instead for reductions of valid arguments in the previous
approach.
The characterization of inferential validity on the basis of objects deﬁned
inductively starting from primitive operations, and the consequent shift of the
canonical/non-canonical distinction only at the level of (descriptions of) acts, leads
to a ﬁrst, fundamental result: a non-circular and satisfactory explanation of the
reciprocal link between inferences and correct reasoning. In order to clarify the
epistemic force, the theory of grounds seems to be far more convincing than any
path offered by the semantics of valid arguments and proofs.
The ﬁnal obstacles towards a successful explanation of deductive compulsion
are then the recognizability problem and the problem of vacuous validity. For the
former, the fact that inferential operations go from grounds to grounds, and the
fact that grounds result from the application of primitive operations, suggests a
minimum progress, although limited to cases of inferences from “closed” premises
for which we already have grounds. In all other cases, the problem remains,
both at the semantic level, as the problem of recognizing the good position of
equations that deﬁne operations on grounds, and at the syntactic level, as the
problem of recognizing the denotation of operational symbols or terms in languages
of grounding, and the adequacy of the identity axioms that rule their behaviour in
systems of grounding.
There are various ways to deal with this issue. The most immediate perhaps orig-
inates from a reﬂection on the notion of recognizability. What kind of recognition
should we request? Is it decidability, or any pragmatically describable operation?
Is it to be independent of the context, so as to be performed individually by the
inferential agent, or does it have to go through a cooperation with other co-agents, in
a shared and maybe dialogic epistemic context? Does it have a conclusive character,
or does it lead to a temporary acceptance, in principle revisable, of the inference we
have recognized as valid?
If we want to pursue the “hard line”—for example, by requiring recognizability
to be decidability—there is little hope of succeeding. Some ways out could be
opened by putting strong restrictions on the class of operations and deﬁning
equations, with the aim of giving this class less blurred boundaries. The adoption
of the “soft line” seems instead to offer more options, although it exposes itself to
the risk of making the concept of recognizability even more vague. In this case,
however, many interesting developments are looming, as well as connections with
recent lines of research. A contextual approach, such as that suggested by Cozzo
(2015, 2019), could lead to dialectical and dialogical perspectives, an integral part
in contemporary argumentation theory (see for example Cantù & Testa, 2006). A

8
Conclusion
273
further alternative is Usberti’s internalist perspective (Usberti, 2015, 2019b and,
for a systematic and independent discussion, 2019a). Here, the recognizability
problem is linked to the question of the grounding of empirical judgments or
assertions, as well as to a characterization in terms of cognitive states, equipped by
default checking algorithms that guarantee recognition and accessibility of relevant
epistemic properties.
From a more formal point of view, the question of recognizability leads to a
general theory of admissible deﬁning equations. Structure, typology and general
form of these equations, and consequently properties of the operations they deﬁne,
could constitute stand-alone subjects of studies, regardless of the restrictions that
can be made for a more precisely deﬁned framework. However, the approach
illustrated in Chaps. 5 and 6 also suggests a link with another line of research.
The languages and systems of grounding we presented constitute a hierarchy.
Climbing this hierarchy means adding to the given languages of grounding new
individual constants, which represent new derivations in a new atomic base, and
new non-primitive operational symbols, or adding to the given systems of grounding
new axioms for new individual constants, and new axioms of identity for new non-
primitive operational symbols. Each language of grounding can be intended as a
translation, through an appropriate extension of the Curry-Howard isomorphism, of
a (ﬁnitely) axiomatizable (ﬁrst-order) system, so that for each of these languages
we could assume the existence of decision algorithms that verify the denotation
of the terms, and for each of the formal systems of grounding related to these
languages we could assume the recursive enumerability of derivations. From this
point of view, the following question would certainly be of importance, on account
of the recognizability problem: is it possible to ﬁnd a method to recursively generate
all the possible expansions of a given language of grounding, with all the possible
deﬁning equations for the operations denoted by the operational symbols of these
languages? And if so, to what extent?
Certainly, it applies here what has already been said about the recognizability
problem in the most stringent terms of the notion of decidability. Unless the class of
equations and operations they deﬁne is speciﬁed according to stricter parameters, it
makes little sense to wonder about the existence of a recursive method of generation.
Moreover, we must bear in mind the limitative results that come into play as soon
as Prawitz’s project is intended with the generality to which it aspires. Among
the bases of the languages of grounding, we have to take into account those with
an expressive power greater than or equal to the expressive power of a base for
Heyting’s ﬁrst-order arithmetic, so that Gödel’s theorems imply the impossibility
of recursive generation, and the contemporary internal decidability of each of the
languages generated.
Once these considerations have been made, however, the study of speciﬁc
classes of expansions, of their properties and of their obtainability starting from
elements already known, may have a certain interest, and give interesting results.
We limit ourselves to two examples: the class of the non-primitive expansions of a
Gentzen-language (on a certain base), which would correspond to the class of the
expansions of ﬁrst-order intuitionistic logic (on a certain base) obtained by adding

274
8
Conclusion
constructively valid non-introductoryrules; and the class of the primitive expansions
of a language of grounding for Heyting’s ﬁrst-order arithmetic, obtained by adding
ground-theoretical equivalents of the reﬂection principle.
The ﬁrst example singles out a class of extensions that seem relevant for
completeness issues. After proving the falsity of Prawitz’s conjecture (Piecha &
Schroeder-Heister, 2018), Piecha and Schroeder-Heister envisage that completeness
with respect to Prawitz’s semantics could apply for some intermediate logic. They
also suggest that Prawitz’s semantics may turn out to be a semantics of intuitionisti-
cally admissible, rather than derivable rules (for the notion of admissibility, see for
example von Plato, 2014). Of course—at least in a non-substitutional understanding
of the notion of admissibility—some rules are admissible in IL, but not valid in
Prawitz’s semantics. If we limit ourselves to propositional intuitionist logic, this is
for example the case of the rule
p
α
with p propositional variable and α arbitrary formula. Be that as it may, once a
complete system has been found, and once we have veriﬁed that—given possible
restrictions on the notion of admissibility—Prawitz’s semantics accounts for the
intuitionistically admissible rules, we could study the issue of the recursive genera-
bility of the expansions under examination in connection with that of the recursive
enumerability of the derivations of rules, or of the proofs of admissibility in such
systems.
As for the second example, a study of the expansions could perhaps be fruitfully
linked to Solomon Feferman’s research (see for example Feferman, 1958, 1988,
1991, 2013, 2016) on the generation of formal arithmetic systems through transﬁnite
iteration of the reﬂection principle or the truth-operator.
The problem of vacuous validity, instead, is not a problem about whether the
formal rendering of some essentially semantic intuitions is adequate with respect
to given informal desiderata. Rather, it concerns the structure of the overall
development of Prawitz’s semantics, independently of whether the core-notion is
that of valid argument or proof, or that of ground. What is problematic in both the
approaches is that evidence is understood as a proof-object. This implies that the
justiﬁcation of the non-canonical cases, be it a procedure for reducing arguments or
proofs, or an equation for deﬁning a non-primitive operation, may be epistemically
void. But if we renounce the idea of using proof-objects for accounting for evidence,
we must also renounce, it seems, the idea of ﬁnding a non-circular deﬁnition of the
concepts of valid inference and proof. It may be, on the contrary, that proof-objects
and proof-acts must be deﬁned simultaneously, as Prawitz himself does in some of
his most recent papers on this topic (Prawitz, 2022a,b,c).
Within this framework, well-founded deﬁnitions are replaced by heuristic prin-
ciples which shed light upon the intertwinement of the notions of valid inferences
and proofs, and which are assumed as starting points, rather than as goals of the
analysis. Notably, one of these principles is precisely the one we have tried to obtain
as a main result of our investigation in this book, i.e. that an argument is valid if,

8
Conclusion
275
and only if, all its inferences are valid. In Prawitz’s new approach, this is no longer
a deﬁnition to be achieved, but a leading tenet from which consequences are drawn.
Of course, this also implies that inferential validity cannot rely upon notions which
are prior to those of valid argument and proof, and vice versa. To avoid vacuous
validity, thus, Prawitz provides a new notion of valid inference, through a kind of
“semantic generalisation” of the inversion principle (Prawitz, 2006), and in line with
the idea of containment between arguments as in his deﬁnition of analytically valid
argument (Prawitz, 2018). This leads Prawitz to introduce an interesting notion of
non-creative inference, i.e., roughly, an inference where evidence for the conclusion
is (and can be seen to be) “contained” in evidence for the premises, and to the
principle that all non-creative inferences are valid. The resulting picture is seemingly
promising and fruitful, and raises questions which are partly similar to those we
have dealt with in Chap.7: besides a standard problem of completeness—namely,
whether non-creative inferences in the language of intuitionistic logic are derivable
in this logic—we have a more “philosophical” completeness issue about whether all
valid inferences that respect Prawitz’s heuristic principles are non-creative.
The theory of grounds provides fundamental insights into the philosophy of
deduction. It solves several problems in Prawitz’s previous semantics of valid
arguments and proofs, one of the most important approaches within the lively ﬁeld
of constructive proof-theoretic semantics. More generally, it constitutes an original
and rich framework in which to explain, or at least try to explain, why and how
valid inferences, valid arguments and proofs exert a power of epistemic compulsion
upon us. This is one of the deepest and most difﬁcult questions that logicians have
been faced with since ancient times, and Prawitz’s proposal is undoubtedly worth
investigating for those who seek answers in this ﬁeld. Finally, the theory shows that
grounding, a widespread topic in contemporary logic (see Poggiolesi, 2016, 2020
for an overview), can be addressed from an epistemic standpoint alien to most other
approaches—although we remark that, in Prawitz, the word “grounding” does not
share the meaning used in metaphysical literature, for Prawitz does not focus on
propositions or sentences grounding another proposition or sentence, but rather on
how such links are obtained. In this book, we have tried to reconstruct the reasons
that led Prawitz to develop his notion of epistemic grounding, and have proposed
a formal framework where this notion can be rigorously addressed. Undoubtedly,
many questions remain unanswered. But this, after all, is one of the merits of any
good theory: not only providing satisfactory answers, but also stimulating questions
and suggesting future ﬁelds of research from which to harvest new ideas.

Bibliography
Prawitz’s Writings
Prawitz, D. (1970). Constructive semantics, in Proceedings of the First Scandinavian Logic
Symposium, Filosoﬁska Studier, No. 8, Uppsala (pp. 96–114).
Prawitz, D. (1971). Ideas and results in proof theory, in J. Fenstad (Ed.), Proceedings of the Second
Scandinavian Logic Symposium. Amsterdam: North-Holland (pp. 235–308)
Prawitz, D. (1973). Towards a foundation of a general proof-theory, in P. Suppes (Ed.), Logic
methodology and philosophy of science IV (pp. 225–250). Amsterdam: North-Holland Pub-
lishing Company
Prawitz, D. (1977). Meaning and proofs: on the conﬂict between classical and intuitionistic logic.
Theoria, 43(1), 2–40.
Prawitz, D. (1979). Proofs and the meaning and completeness of the logical constants, in J.
Hintikka et al. (Eds.), Essays on mathematical and philosophical Logic, Reidel, Dordrecht
(pp. 25–40).
Prawitz, D. (1985). Remarks on some approaches to the concept of logical consequence. Synthese,
62, 153–171.
Prawitz, D. (2005). Logical consequence from a constructivist point of view, in S. Shapiro (Ed.),
The Oxford handbook of philosophy of mathematics and logic (pp. 671–695). Oxford: Oxford
University Press.
Prawitz, D. (2006). Natural deduction. A proof theoretical study. New York: Dover Publications.
Prawitz, D. (2009). Inference and knowledge, in M. Pelis (Ed.), The Logica Yearbook 2008 (pp.
175–192). London: College Publications.
Prawitz, D. (2011). Proofs and perfect syllogism, in E. Grosholz, E. Ippoliti, & C. Cellucci (Eds.),
Logic and knowledge (pp. 385–402). London: College Publications.
Prawitz, D. (2012a). The epistemic signiﬁcance of valid inference. Synthese, 187, 887–898.
Prawitz, D. (2012b). Truth and proof in intuitionism, in P. Dybier, S. Lindström, E. Palmgren, &
G. Sundholm (Eds.), Epistemology versus ontology (pp. 45–67). Dordrecht: Springer.
Prawitz, D. (2012c). Truth as an epistemic notion, in L. Tranchini (Ed.), Anti-realistic Notions of
Truth, special issue of Topoi, 31, pp. 9–16.
Prawitz, D. (2013). Validity of inferences, in M. Frauchiger (Ed.), Reference, rationality, and
phenomenology: themes from Føllesdal (pp. 179–204). Dordrecht: Ontos Verlag.
Prawitz, D. (2014). An approach to general proof theory and a conjecture of a kind of completeness
of intuitionistic logic revisited, in L. C. Pereira, E. H. Haeusler, & V. de Paiva (Eds.), Advances
in natural deduction (pp. 269–279). Dordrecht: Springer.
Prawitz, D. (2015). Explaining deductive inference, in H. Wansing (Ed.), Dag Prawitz on proofs
and meaning (pp. 65–100). Cham: Springer.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
A. Piccolomini d’Aragona, Prawitz’s Epistemic Grounding, Synthese Library 469,
https://doi.org/10.1007/978-3-031-20294-0
277

278
Bibliography
Prawitz, D. (2016). On the relation between Heyting’s and Gentzen’s approaches to meaning, in T.
Piecha & P. Schroeder-Heister (Eds.), Advances in proof-theoretic semantics (pp. 5–25). Cham:
Springer.
Prawitz, D. (2018). The fundamental problem of general proof theory. Studia Logica, 107, 11–29.
Prawitz, D. (2019a). The concepts of proof and ground, in S. Centrone, S. Negri, D. Sarikaya, &
P. M. Schuster (Eds.), Mathesis Universalis, computability and proof (pp. 291–309). Cham:
Springer.
Prawitz, D. (2019b). The seeming interdependence between the concepts of valid inference and
proof, in G. Crocco & A. Piccolomini d’Aragona (Eds.), Inferences and proofs, special issue of
Topoi, 38, pp. 493–503.
Prawitz, D. (2022a). The validity of inference and argument, forthcoming.
Prawitz, D. (2022b). Validity of inferences, forthcoming.
Prawitz, D. (2022c). Validity of inferences reconsidered, forthcoming.
Other Sources
Aristotle. (1949). Aristotle’s prior and posterior analytics, W. D. Ross (Ed.). Oxford: Oxford
University Press.
Bolzano, B. (1837). Wissenschaftlehre. Sulzbach: Seidel.
Brîncu¸s, C. (2015). The epistemic signiﬁcance of valid inference - a model-theoretic approach, in
S. Costreie & M. Dumitru (Eds.), Meaning and truth (pp. 11–37). Bucharest: PRO Universitaria
Publishing.
Cantù, P., & Testa, I. (2006). Teorie dell’argomentazione. Un’introduzione alle logiche del dialogo.
Milano: Mondadori.
Cardone, F. (2017). Games, full abstraction and full completeness, in The Stanford encyclopedia
of philosophy (winter 2017 edition).
Carnap, R. (1928). Der logische Aufbau der Welte. Hamburg: Felix Meiner.
Carroll, L. (1895). What the tortoise said to Achilles. Mind, 4(14), 278–280.
Catta, D., & Piccolomini d’Aragona, A. (2021). Game of grounds, in G. Oliveri, S. Boscolo, & C.
Ternullo (Eds.), Objects, structures and logics (pp. 259–286). Cham: Springer.
Cellucci, C. (1978). Teoria della dimostrazione. Normalizzazioni and assegnazioni di numeri
ordinali. Torino: Bollati Boringhieri.
Church, A. (1932). A set of postulates for the foundation of logic. Annals of mathematics (2nd
Series), 33(2), 353–354.
Copeland, B. J. (2017). The Church-Turing thesis, in The Stanford encyclopedia of philosophy
(winter 2017 edition).
Cozzo, C. (1994a). Meaning and argument. A theory of meaning centred on immediate argumental
role. Uppsala: Almqvist & Wiksell.
Cozzo, C. (1994b). Teoria del signiﬁcato and ﬁlosoﬁa della logica. Bologna: CLUEB.
Cozzo, C. (2008). Introduzione a Dummett. Roma Bari: Laterza.
Cozzo, C. (2014). Inference and compulsion, in E. Moriconi (Ed.), Second Pisa colloquium in
logic, language and epistemology (pp. 162–180). Pisa: ETS.
Cozzo, C. (2015). Necessity of thought, in H. Wansing (Ed.), Dag Prawitz on proofs and meaning
(pp. 101–120). Cham: Springer.
Cozzo, C. (2019). Cogency and context, in G. Crocco & A. Piccolomini d’Aragona (Eds.),
Inferences and proofs, special issue of Topoi, 38, pp. 505–516.
Dean, W., & Kurokawa, H. (2016). Kreisel’s theory of constructions, the Kreisel - Goodman
paradox, and the second clause, in T. Piecha & P. Schroeder-Heister (Eds.), Advances in proof-
theoretic semantics (pp. 27–63). Cham: Springer.
Descartes, R. (1985). Rules for the direction of the mind, in J. Cottingham, R. Stoothoff, & D.
Murdoch, The philosophical writings of Descartes vol. I. Cambridge: Cambridge University
Press.

Bibliography
279
Došen, K. (2015). Inferential semantics, in H. Wansing (Ed.), Dag Prawitz on proofs and meaning
(pp. 147–162). Cham: Springer.
Douven, I. (2017). Abduction, in The Stanford encyclopedia of philosophy (summer 2017 edition).
Dummett, M. (1973). Frege: philosophy of language. London: Duckworth.
Dummett, M. (1978a). Nominalism, in M. Dummett (Ed.), Truth and other enigmas (pp. 38–49).
Cambridge: Harvard University Press.
Dummett, M. (1978b). The philosophical basis of intuitionistic logic, in M. Dummett (Ed.), Truth
and other enigmas (pp. 215–247). Cambridge: Harvard University Press.
Dummett, M. (1978c). Truth, in M. Dummett (Ed.), Truth and other enigmas (pp. 1–24).
Cambridge: Harvard University Press.
Dummett, M. (1991). The logical basis of metaphysics. Cambridge: Harvard University Press.
Dummett, M. (1996a). What is a theory of meaning (I), in M. Dummett (Ed.), The seas of language
(pp. 1–33). Oxford: Oxford University Press.
Dummett, M. (1996b). What is a theory of meaning (II), in M. Dummett (Ed.), The seas of
language (pp. 34–93). Oxford: Oxford University Press.
Dybjer, P., & Palmgren, E. (2016). Intuitionistic type theory, in The Stanford encyclopedia of
philosophy (winter 2016 edition).
Etchemendy, J. (1990). The concept of logical consequence. Cambridge: CSLI Publications.
Feferman, S. (1958). Ordinal logic re-examined, and on the strength of ordinal logics. Journal of
symbolic logic, 23, 105–106.
Feferman, S. (1988). Turing in the land of 0(z), in R. Herken (Ed.), The universal Turing machine.
A half-century survey (pp. 113–147). Oxford: Oxford University Press.
Feferman, S. (1991). Reﬂecting on incompleteness. Journal of Symbolic Logic, 56(1), 1–49.
Feferman, S. (2013). Turing’s thesis: ordinal logics and oracle computability, in S. B. Cooper & J.
van Leeuwen (Eds.), Alan Turing: his work and impact (pp. 145–150). Amsterdam: Elsevier.
Feferman, S. (2016). The operational perspective: three routes, in R. Kahle, T. Strahm, & T. Studer
(Eds.), Advances in proof theory. Progress in computer science and applied logic (Vol. 28, pp.
269–289). Cham: Birkhäuser.
Francez, N. (2015). Proof-theoretic semantics. London: College Publications.
Frege, G. (1879). Begriffsschrift: eine der arithmetischen nachgebildete Formelsprache des reinen
Denkens, Halle.
Frege, G. (1884). Die Grundlagen der Arithmetik. Breslau: Koebner.
Frege, G. (1891). Function und Begriff. Vortrag gehalten in der Sitzung vom 9. Januar 1891 der
Jenaischen Gesellschaft für Medicin und Naturwissenschaft. Jena: Verlag Hermann Pohle.
Frege, G. (1893–1903). Die Grundgesetze der Arithmetik. Jena: Pohle.
Frege, G. (2001). Scritti ﬁlosoﬁci, C. Penco & E. Picardi (Eds.). Roma Bari: Laterza.
Gentzen, G. (1934–1935). Untersuchungen über das logische Schließen, in Matematische
Zeitschrift, XXXIX (pp. 179–210, 405–431).
Girard, J. Y., Taylor, P., & Lafont, Y. (1993). Proofs and types. Cambridge: Cambridge University
Press.
Gödel, K. (1931). Über formal unentscheidbare Sätze der Principia Mathematica und verwandter
Systeme I, in Monatshefte für Mathematik und Physik (Vol. 38, pp. 173–198).
Gödel, K. (1933). The present situation in the foundations of mathematics, in S. Feferman et al.
(1995), Kurt Gödel collected works. Vol. III. Oxford: Oxford University Press.
Goodman, N. (1968). Intuitionistic arithmetic as a theory of constructions, Ph.D. dissertation,
Stanford.
Goodman, N. (1970). A theory of constructions equivalent to arithmetic, in J. M. A. Kino & R.
Vesley (Eds.), Intuitionism and proof theory (pp. 101–120). Amsterdam: Elsevier.
Goodman, N. (1973). The arithmetic theory of constructions, Cambridge summer school in
mathematical logic (pp. 274–298). Berlin, Heidelberg: Springer.
Gupta, A., & Belnap, N. (1993). The revision theory of truth. Cambridge: MIT Press.
Harrop, R. (1960). Concerning formulas of the types α →β ∨γ, α →∃xβ(x) in intuitionistic
formal systems. Journal of symbolic logic, 25(1), 27–32.

280
Bibliography
Hawthorne, J. (2018). Inductive logic, in The Stanford encyclopedia of philosophy (spring 2018
edition).
Heyting, A. (1931). Die intuitionistiche Grundlegung der Mathematik. Erkenntnis, 2, 106–115.
Heyting, A. (1934). Mathematische Grundlagenforschung, Intuitionismus, Beweistheorie. Berlin,
Heidelberg: Springer.
Heyting, A. (1956). Intuitionism. An introduction. Amsterdam: North-Holland Publishing Com-
pany.
Howard, W. (1980). The formula-as-types notion of construction, in J. R. Hindley & J. P. Seldin
(Eds.), To H. B. Curry: essays on combinatoriy logic, lambda calculus and formalism (pp.
479–490). London: Academic Press.
Kenny, A. (2003). Frege: un’introduzione. Torino: Einaudi.
Kleene, S. C. (1936). λ-deﬁnability and recursiveness. Duke Mathematics Journal, 2(2), 340–353.
Kolmogorov, A. N. (1932). Zur Deutung der intuitionistischen Logik, in Mathematische Zeitschrift
(Vol. XXXV, pp. 58–65).
Kramer, P. (2016). The revision theory of truth, in The Stanford encyclopedia of philosophy (winter
2016 edition).
Kreisel, G. (1962) Foundations of intuitionistic logic, in E. Nagel, P. Suppes, & A. Tarski (Eds.),
Logic methodology and philosophy of science (pp. 198–210). Stanford: University Press.
Kreisel, G. (1965). Mathematical logic, in T. Saaty (Ed.), Lectures on modern mathematics (Vol.
III). New York: Wiley.
Lorenzen, P. (1950). Konstruktive Begründung der Mathematik. Mathematische Zeitschrift, LIII,
162–202.
Lorenzen, P. (1955). Einführung in die operative Logik und Mathematik. Berlin, Heidelberg:
Springer.
Martin-Löf, P. (1975a). About models for intuitionistic type theories and the notion of deﬁnitional
equality, in S. Kanger (Ed.), Proceedings of the Third Scandinavian Logic Symposium (pp.
81–109). Amsterdam: Elsevier.
Martin-Löf, P. (1975b). An intuitionistic theory of types. Predicative part, in H. E. Rose & J. C.
Sheperdson (Eds.), Logic Colloquium ’73 (pp. 73–118). Amsterdam: Elsevier.
Martin-Löf, P. (1984). Intuitionistic type theory. Napoli: Bibliopolis.
Martin-Löf, P. (1985). On the meaning of the logical constants and the justiﬁcations of the logical
laws, in Atti degli incontri di logica matematica (Vol. 2), Siena.
Martin-Löf, P. (1994). Analytic and synthetic judgments in type theory, in P. Parrini (Ed.), Kant
and contemporary epistemology (pp. 87–99). Dordrecht: Kluwer Academic Publishers.
Martin-Löf, P. (1998). Truth and knowability: on the principles C and K of Michael Dummett,
in H. G. Dales & G. Olivieri (Eds.), Truth in mathematics (pp. 105–114). Oxford: Clarendon
Press.
Moriconi, E. (2023). From proof-objects to grounds, in A. Piccolomini d’Aragona (Ed.), Perspec-
tives on deduction. Cham: Springer.
Negri, S., & von Plato, J. (2015). Meaning in use, in H. Wansing (Ed.), Dag Prawitz on proofs and
meaning (pp. 239–257). Cham: Springer.
Padoa, A. (1901). Essai d’une théorie algébrique des nombres entiers, précédé d’une introduction
logique à une théorie déductive quelconque, in Bibliothèque du Congrès International de
Philosophie (Vol. 3, pp. 309–365). Paris: Librairie Armand Colin.
Pagin, P. (1998). Bivalence: meaning theory vs metaphysics. Theoria, 64(2–3), 157–186.
Peter, R. (1951). Recursive Functionen. Budapest: Akademiai Kiado.
Peter, R. (1959). Rekursivitat und Konstruktivitat, in A. Heyting (Ed.), Constructivity in mathe-
matics (pp. 226–233). Amsterdam: North-Holland Publishing Company.
Petrolo, M., & Pistone, P. (2019). On paradoxes in normal form, in G. Crocco & A. Piccolomini
d’Aragona (Eds.), Inferences and proofs, special issue of Topoi, 38, pp. 605–617.
Piecha, T. (2016). Completeness in proof-theoretic semantics, in T. Piecha & P. Schroeder-Heister
(Eds.), Advances in proof-theoretic semantics (pp. 231–251). Cham: Springer.
Piecha, T., & Schroeder-Heister, P. (2018). Incompleteness of intuitionistic propositional logic with
respect to proof-theoretic semantics. Studia Logica, 107, 233–246.

Bibliography
281
Pistone, P. (2015). On proofs and types in second order logic, PhD dissertation, Roma 3 Univerisity,
Aix-Marseille University.
Poggiolesi, F. (2016). A critical overview of the most recent logics of grounding, in F. Boccuni &
A. Sereni (Eds.), Objectivity, realism and proof, Boston Studies in the Philosophy and History
of Science (pp. 291–309). Cham: Springer.
Poggiolesi, F. (2020). Logics of grounding, in M. Raven (Ed.), Routledge handbook for metaphys-
ical grounding (pp. 213–227). London: Routledge.
Schroeder-Heister, P. (1984). Generalized rules for quantiﬁers and the completeness of the
intuitionistic operators ∧, ∨, →, ⊥, ∀, ∃, in M. Richter, E. Börger, W. Oberschelp, B. Schinzel,
& W. Thomas (Eds.), Computation and proof theory (pp. 399–426). Berlin, Heidelberg:
Springer.
Schroeder-Heister, P. (1991). Uniform proof-theoretic semantics for logical constants. Abstract.
Journal of Symbolic Logic, 56, 1142.
Schroeder-Heister, P. (2006). Validity concepts in proof-theoretic semantics. Synthese, 148, 525–
571.
Schroeder-Heister, P. (2008). Proof-theoretic versus model-theoretic consequence, in M. Peliš
(Ed.), The Logica Yearbook 2008 (pp. 187–200). Prague: Filosoﬁa.
Schroeder-Heister, P. (2012). The categorical and the hypothetical: a critique of some fundamental
assumptions of standard semantics. Synthese, 187, 925–942.
Schroeder-Heister, P. (2018). Proof-theoretic semantics, in E. N. Zalta (Ed.), The Stanford
Encyclopedia of Philosophy (Spring 2018 Edition).
Scott, D. (1970). Outline of a mathematical theory of computation, Oxford Mono. PRG-2. Oxford:
Oxford University Press.
Shapiro, S. (2005). Logical consequence: proof theory and model theory, in S. Shapiro (Ed.),
The Oxford handbook of philosophy of mathematics and logic (pp. 651–670). Oxford: Oxford
University Press.
Sheffer, H. M. (1913). A set of ﬁve independent postulates for Boolean algebras, with application
to logical constants. Transactions of the American Mathematical Society, 14(4), 481–488.
Sørensen, M. H., & Urzyczyn, P. (2006). Lectures on the Curry-Howard isomorphism. New York:
Elsevier.
Sundholm, G. (1983). Constructions, proofs and the meaning of the logical constants. Journal of
Philosophical Logic, 12, 151–172.
Sundholm, G. (1993). Questions of proof. Manuscrito (Campinas), 16, 47–70.
Sundholm, G. (1998). Proofs as acts and proofs as objects. Theoria, 54(2–3), 187–216.
Sundholm, G. (2011). A garden of grounding trees, in E. Grosholz, E. Ippoliti, & C. Cellucci
(Eds.), Logic and knowledge (pp. 57–74). London: College Publications.
Sundholm, G. (2012). On the philosophical work of Per Martin-Löf, in P. Dybjer, S. Lindström, E.
Palmgren, & G. Sundholm (Eds.), Epistemology versus ontology: essays on the philosophy and
foundations of mathematics in honour of Per Martin-Löf (pp. xxiii-xxiv). Cham: Springer.
Suppes, P. (1999). Introduction to logic. New York: Dover.
Tarski, A. (1956a). On the concept of logical consequence, in A. Tarski (Ed.), Logic, semantics,
metamathematics. Oxford: Oxford University Press.
Tarski, A. (1956b). The concept of truth in formalized languages, in A. Tarski (Ed.), Logic,
semantics, metamathematics. Oxford: Oxford University Press.
Tennant, N. (1982). Proof and paradox. Dialectica, 36(2/3), 265–296.
Tennant, N. (1995). On paradox without self-reference. Analysis, 55(3), 199–207.
Tennant, N. (2016). Normalizability, cut eliminability and paradox. Synthese, 199, 597–616.
Tranchini, L. (2014). Dag Prawitz, APhEx 9.
Tranchini, L. (2016). Proof-theoretic semantics, paradoxes and the distinction between sense and
denotation. Journal of Logic and Computation, 26(2), 495–512.
Tranchini, L. (2019). Proof, meaning and paradox. Some remarks, in in G. Crocco & A.
Piccolomini d’Aragona (Eds.), Inferences and proofs, special issue of Topoi, 38, pp. 591–603.
Turing, A. M. (1937). On computable numbers, with an application to the Entscheidungsproblem.
Proceedings of the London Mathematical Society, 42(s2), 230–265.

282
Bibliography
Usberti, G. (1995). Signiﬁcato and conoscenza. Per una critica del neoveriﬁcazionismo. Milano:
Guerini and Associati.
Usberti, G. (2015). A notion of C-justiﬁcation for empirical statements, in H. Wansing (Ed.), Dag
Prawitz on proofs and meaning (pp. 415–450). Cham: Springer.
Usberti, G. (2016). The paradox of knowability from an intuitionistic standpoint, in T. Piecha
& P. Schroeder-Heister (Eds.), Advances in proof-theoretic semantics (pp. 115–137). Cham:
Springer.
Usberti, G. (2019a). A notion of internalistic logical validity, in L. Bellotti, L. Gili, E. Moriconi,
& G. Turbanti (Eds.), III Pisa colloquium on logic, language and epistemology (pp. 389–406).
Pisa: ETS.
Usberti, G. (2019b). Inference and epistemic transparency, in G. Crocco & A. Piccolomini
d’Aragona (Eds.), Inferences and proofs, special issue of Topoi, 38, pp. 517–530.
Van Dalen, D. (1994). Logic and structure. Berlin Heidelberg: Springer.
von Plato, J. (2014). Elements of logical reasoning. Cambridge: Cambridge University Press.
Weinstein, S. (1983). The intended interpretation of intuitionistic logic. Journal of Philosophical
Logic, 12(2), 261–270.
Wittgenstein, L. (1921). Logisch-philosophische Abhandlung. Annalen der Naturphilosophie, XIV,
185–262.
Wittgenstein, L. (1953). Philosophical investigations. Oxford: Basil Blackwell.

