Evaluating natural language processing models with generalization
metrics that do not need access to any training or testing data
Yaoqing Yang1, Ryan Theisen1, Liam Hodgkinson1,2, Joseph E. Gonzalez1,
Kannan Ramchandran1, Charles H. Martin4, Michael W. Mahoney1,2,3
1 University of California, Berkeley
2 International Computer Science Institute
3 Lawrence Berkeley National Laboratory
4 Calculation Consulting
Abstract
The search for eﬀective and robust metrics has been the focus of recent theoretical and empirical
work on generalization of deep neural networks (NNs). In this paper, we discuss the performance of
natural language processing (NLP) models, and we evaluate various existing and novel generalization
metrics. Compared to prior studies, we (i) focus on NLP instead of computer vision (CV), (ii) focus on
generalization metrics that predict test error instead of the generalization gap, (iii) focus on generalization
metrics that do not need the access to data, and (iv) focus on the heavy-tail (HT) phenomenon that has
received comparatively less attention in the study of deep neural networks. We extend recent HT-based
work which focuses on power law (PL) distributions, and we study exponential (EXP) and exponentially
truncated power law (E-TPL) ﬁtting to the empirical spectral densities (ESDs) of weight matrices. Our
empirical studies are carried on (i) hundreds of Transformers trained in diﬀerent settings, in which we
systematically vary the amount of data, the model size and the optimization hyperparameters, (ii) a
total of 51 pretrained Transformers from eight families of Huggingface NLP models, including BERT,
GPT2, ALBERT, etc., and (iii) a total of 28 existing and novel generalization metrics. From our detailed
empirical analyses, we show that shape metrics, or the metrics obtained from ﬁtting the shape of the ESDs,
perform uniformly better at predicting generalization performance than scale metrics commonly studied
in the literature, as measured by the average rank correlations with the generalization performance for all
of our experiments. We also show that among the three HT distributions considered in our paper, the
E-TPL ﬁtting of ESDs performs the most robustly when the models are trained in experimental settings,
while the PL ﬁtting achieves the best performance on well-trained Huggingface models, and that both
E-TPL and PL metrics (which are both shape metrics) outperform scale metrics.
1
Introduction
Recent years have seen a wide array of large-scale empirical studies on the various metrics used to quantify
generalization (Dziugaite et al., 2020; Jiang et al., 2019; Martin & Mahoney, 2021a; Martin et al., 2021). On
the one hand, theory-driven metrics have the potential to reveal more information than test error, bringing
us one step closer to unpacking the black box of deep NNs (Frankle & Carbin, 2018; Nakkiran et al., 2019;
Zhang et al., 2021). On the other hand, a wide variety of generalization metrics have been applied to predict
the quality of pretrained models (Martin & Mahoney, 2019; Martin et al., 2021), design eﬀective training
procedures (Foret et al., 2020; Izmailov et al., 2018), improve network eﬃciency (Chen et al., 2020; Dong
et al., 2019), quantify network robustness (Tanay & Griﬃn, 2016; Yang et al., 2020), improve ensemble
learning techniques (Fort et al., 2019; Garipov et al., 2018), analyze and improve large-scale machine learning
contests (Martin & Mahoney, 2021a), and so on.
Despite advances in the study of generalization, however, several recent papers point out the deﬁciencies
of many of these “fantastic” generalization metrics. These include a lack of “robustness” to the changes of
1
arXiv:2202.02842v2  [cs.CL]  9 Oct 2022

environmental hyperparameters (Dziugaite et al., 2020; Jiang et al., 2019) (such as data, network architecture
and training schemes), or the Simpson’s paradox that generalization metrics perform diﬀerently (i.e., predict
opposite trends) when applied to each sub-part of a collection of learning models or to the holistic study
(Martin & Mahoney, 2021a). Another drawback is the over-reliance on experiments with CV models, which
are relatively well-explored, and which are not representative of many other application areas. Despite a
few counterexamples (Martin et al., 2021; Nakkiran et al., 2019; Yang et al., 2021), systematic studies of
generalization in other ﬁelds, such as NLP, are largely missing.
Generalization metrics for NLP. The objective of this paper is to provide a systematic study of
generalization metrics in NLP, addressing several deﬁciencies in prior studies (Dziugaite et al., 2020; Jiang
et al., 2019; Martin et al., 2021). Compared to CV, predicting generalization in NLP has several important
diﬀerences that require careful consideration. The training data from standard CV benchmarks can often
be easily obtained, while NLP pretraining datasets are typically web-scale and are challenging to access.
Therefore, generalization metrics that can measure the quality of learning models without access to data
are ideal for NLP. In this paper, we focus on generalization metrics that do not need access to data, which
is useful for evaluating pretrained NLP models (Wolf et al., 2020). Indeed, recent work has demonstrated
that access to training or testing data is not necessary for assessing the model quality of learning models
(Martin et al., 2021), though these have yet to be evaluated at scale in the NLP domain. Furthermore, it is
typically infeasible to train NLP models to interpolate the (frequently large) training set. This becomes an
issue when applying most existing generalization metrics as they often estimate the generalization gap (i.e.,
the diﬀerence between training and test performance) rather than the test error itself. Metrics that focus
on predicting the generalization gap include most of the well-known metrics in CV, such as those based on
the PAC-Bayesian framework (McAllester, 1999; Neyshabur et al., 2018) and margins (Bartlett et al., 2017;
Jiang et al., 2018; Pitas et al., 2017).
To illustrate the issue, consider the problem of model selection between two models (Jiang et al., 2020;
Martin & Mahoney, 2021a).1 Suppose we are given two classiﬁcation models. Then even if we have i) access
to both models’ training errors, and ii) a metric which is guaranteed to perfectly rank correlate with the
generalization gap, then we still cannot determine which model as smaller test error. This means that, if our
objective is to construct a metric that correctly predicts which model has lower test error, rank correlation
with the generalization gap is not suﬃcient. In this paper, we aim to study how generalization metrics
correlate with model quality, for which we use test error as a close approximation. As we will demonstrate (in
Figure 4), rank correlation with the generalization gap indeed does not imply rank correlation with model
quality in practice, and in fact often orders models in the opposite order of their test errors. From a practical
point of view, for NLP tasks, we prefer generalization metrics that can directly predict trends in test error
(or similar evaluation metrics in NLP, such as the test BLEU score (Papineni et al., 2002)) rather than trends
in the generalization gap.
Naturally, we cannot expect a metric to be universally correlated with test error if evaluating the metric
does not need data. However, within certain classes of models (e.g., stages of training in one model or across
pre-trained models), they may be eﬀective at diagnosing model quality. With these objectives in mind, among
the generalization metrics in the literature, we take particular interest in those derived from the heavy-tail
self regularization (HT-SR) theory (Martin & Mahoney, 2019, 2021b), which (i) predicts test error directly
instead of the generalization gap and (ii) does not require access to training (or testing) data. In addition to
these two advantages, actual data often follow heavy-tail distributions (Feldman, 2020; Martin & Mahoney,
2021b; Martin et al., 2021), which can be even more evident in NLP than the more well-behaved datasets in
CV (Li, 2017) that are often used to study generalization.
HT-SR theory. The core principle of HT-SR theory is that HT structures arise naturally in the ESDs
of the weight matrices of well-trained models, as the result of extracting various correlations in data during
optimization (Martin & Mahoney, 2019, 2021a,b; Martin et al., 2021). A primary practical consequence is
that by estimating the PL parameters from the ESDs (requiring only weights), one can predict model quality,
1As the report of the NeurIPS 2020 Competition on Predicting Generalization in Deep Learning (Jiang et al., 2020) points
out, the generalization metric “should” be able to order models’ performance in a way similar to the generalization gap, and
thus one hopes that it can be used for model selections or neural architecture search. However, see (Martin & Mahoney, 2021a)
for a detailed exposition of issues and problems with this.
2

as smaller parameters are reported to correspond to higher test accuracy. However, these estimators can be
unstable, and so one must be careful not to rely on them alone. The quality of the PL ﬁt itself should also
point to similar conclusions (Martin & Mahoney, 2021b), which can serve as a sanity check. Fortunately, for
large Transformer models (Kenton & Toutanova, 2019; Vaswani et al., 2017) typically used in modern NLP
tasks, there are many large linear layers, which allows for greater accuracy in the PL estimators.
The principles of HT-SR theory extend beyond ﬁtting the PL coeﬃcient, however, as ESDs can take many
forms. To this end, we study three diﬀerent types of distributions to ﬁt to the ESDs of weight matrices,
including power laws (PL) in Eqn. (1), exponentially truncated power laws (E-TPL) in Eqn. (2), and
exponential laws (EXP) in Eqn. (3). These are all commonly considered families of distributions in classical
studies of PL (Clauset et al., 2009), and it is often hard in practice to predict which family ﬁts data the
best (as we show in this paper, this is true for deep NNs especially). Figure 1 shows examples of comparing
diﬀerent HT ﬁttings on the same ESD. Following Martin & Mahoney (2021a), we refer to the various metrics
derived from HT-SR as shape metrics.
100
101
102
Eigenvalues of correlation matrix
10
4
10
3
10
2
10
1
100
101
ESD
alpha=1.76, ks_distance=0.05
xmin
(a) Small ks distance.
10
2
10
1
100
101
102
Eigenvalues of correlation matrix
10
4
10
3
10
2
10
1
100
101
ESD
alpha=3.46, ks_distance=0.07
xmin
(b) Mediocre ks distance.
10
2
10
1
100
101
102
Eigenvalues of correlation matrix
10
4
10
3
10
2
10
1
100
101
ESD
alpha=4.84, ks_distance=0.11
xmin
(c) Large ks distance.
100
101
102
Eigenvalues of correlation matrix
10
4
10
3
10
2
10
1
100
101
ESD
E-TPL beta=1.50, ks_distance=0.02
 E-TPL lambda=0.01
xmin
(d) E-TPL ﬁtting of the ESD.
10
2
10
1
100
101
102
Eigenvalues of correlation matrix
10
4
10
3
10
2
10
1
100
101
ESD
E-TPL beta=1.19, ks_distance=0.02
 E-TPL lambda=0.04
xmin
(e) E-TPL ﬁtting of the ESD.
10
2
10
1
100
101
102
Eigenvalues of correlation matrix
10
4
10
3
10
2
10
1
100
101
ESD
E-TPL beta=1.00, ks_distance=0.06
 E-TPL lambda=0.09
xmin
(f) E-TPL ﬁtting of the ESD.
Figure 1: Comparing PL and E-TPL ﬁtting. (First row). Good, mediocre, and bad PL ﬁttings measured
by the ks distance. (Second row). E-TPL ﬁtting of the ESD on the same column. Blue histograms
represent the ESDs. Solid vertical lines represent the lower threshold xmin of the PL distribution found by
the ﬁtting procedure. Solid curves represent ESDs truncated using xmin, and dashed curves represent the
ﬁtted HT distributions.
Contributions. The following summarizes our main contributions.
• Deviating from prior work examining generalization metrics in CV (Dziugaite et al., 2020; Jiang et al.,
2019), we provide the ﬁrst systematic empirical study on various generalization metrics in NLP. Our
detailed studies include the following:
– We consider 360 transformers trained with varying hyperparameters, and eight families of pretrained
SOTA transformers downloaded from Huggingface (Wolf et al., 2020), including BERT (Kenton &
Toutanova, 2019), GPT2 (Radford et al., 2019), ALBERT (both v1 and v2) (Lan et al., 2019), etc.
3

– We measure the correlation between 28 generalization metrics and the model quality (measured
by test-time performance) over three diﬀerent model classes: (i) models trained with the optimal
hyperparameters; (ii) a single model at diﬀerent stages of training; and (iii) a model trained with
diﬀerent hyperparameters (similar to Jiang et al. (2019); Martin & Mahoney (2021a)).
• We extend prior studies on HT-SR theory and investigate alternative models to ﬁt heavy-tail/light-tail
distributions. Our results show that E-TPL ﬁts are comparatively robust alternatives to PL ﬁts for
predicting trends in test error on suboptimally-trained models.
• We ﬁnd that, applied appropriately, HT-based shape metrics consistently perform better than scale metrics
(or norm-based metrics) for predicting model quality.
• We provide results for data-dependent metrics motivated by margins and PAC-Bayesian bounds (Dziugaite
et al., 2020; Jiang et al., 2019). While these metrics perform well in predicting the generalization gap, we
show that none of them satisfactorily predicts test error directly.
In order that our results can be reproduced and extended, we have open-sourced our code.2
Preliminary of ESDs of weight matrices.
Consider a NN with d layers and corresponding weight
matrices W1, W2,..., Wd. For each weight matrix Wi with shape N × M, assume without loss of generality
that N ≥M (otherwise, consider W⊤
i ). We deﬁne the correlation matrix as Xi = W⊤
i Wi, and denote the
eigenvalues of Xi as {λj}M
j=1, so that λj = σ2
j , where {σj}M
j=1 are the singular values of Wi. Furthermore, we
use λi,max to denote the maximum eigenvalue of the correlation matrix Xi. The ESD of the weight matrix
Wi refers to the empirical density of the eigenvalues of Xi, typically represented through a histogram. We
let p(x) denote the density function to ﬁt the ESD taking values in the interval (xmin, xmax). For a power
law, p satisﬁes
p(x) ∝x−α,
xmin < x < xmax.
(1)
From Martin & Mahoney (2021a), xmax is chosen to be the maximum eigenvalue of the empirical correlation
matrix. However, xmin is a variable to be optimized to improve the quality of PL ﬁtting, and it is not equal
to the minimum eigenvalue in general.
2
Heavy-tail self-regularization theory
Here, we provide a brief overview of the HT-SR theory, and discuss several metrics that can be derived from
it. According to HT-SR theory, the ESDs of the weight matrices become more heavy-tailed during training as
they become increasingly correlated. One can quantify the extent of these correlations by ﬁtting a PL to the
ESD of a weight matrix, for example, by using the open-source WeightWatcher tool 3(Martin et al., 2021).
After computing the ESD of a weight matrix, we use the maximum likelihood estimate from Alstott et al.
(2014) to ﬁt the PL distribution, the speciﬁc form of which has been deﬁned in (1). Let PL alpha denote the
PL coeﬃcient averaged over layers; eﬀectively the slope of the tail of the ESD of the pooled weights, on a
log-log scale.
Correctly identifying and ﬁtting PL distributions is well-known to be a challenge in practice. For example,
a density that appears as a straight line on a log-log scale plot need not follow a power law, as there are
many other distributions that could show a similar behavior, including lognormal and exponential-type
distributions (Clauset et al., 2009). Nested distributions such as E-TPL, which combine the pure PL and
other distributional assumptions, can often improve the quality of ﬁtting (Alstott et al., 2014; Clauset et al.,
2009). Therefore, in addition to the PL distribution deﬁned in (1), we consider several other distribution
classes from the literature.
• (E TPL lambda and E TPL beta) The ESDs are assumed to take a “nested” form.
p(x) ∝x−β exp(−λx), xmin < x < xmax.
(2)
2https://github.com/nsfzyzz/Generalization_metrics_for_NLP
3https://github.com/CalculatedContent/WeightWatcher
4

After ﬁtting the E-TPL, we call the exponential truncation coeﬃcient λ the E TPL lambda metric, and
we call the PL coeﬃcient the E TPL beta metric.
• (EXP lambda). The ESDs are assumed to take the following form.
p(x) ∝exp(−λx), xmin < x < xmax.
(3)
After ﬁtting the EXP, we call the exponential coeﬃcient λ the EXP lambda metric.
For more details of the various metrics considered in this paper, see Table 1. All of the metrics derived
from HT-SR do not require access to data, nor do they require model training/retraining, and thus they are
relatively cheap to compute. Our primary comparisons are between shape metrics (derived from HT-SR),
and scale metrics (mostly norm-based). For the precise deﬁnitions of these metrics, see Appendix A.
Name
Eqn
Ref
Need initial
weights?
Scale or
shape
Need
data?
Need
gpu?
Predicting model quality
or generalization gap?
param norm
(4)
Jiang et al. (2019)
No
Scale
No
No
Generalization gap
fro dist
(5)
Jiang et al. (2019)
Yes
Scale
No
No
Generalization gap
log norm
(6)
Martin & Mahoney (2021b)
No
Scale
No
No
Generalization gap
log spectral norm
(7)
Martin & Mahoney (2021a)
No
Scale
No
No
Generalization gap
dist spec int
(8)
Jiang et al. (2019)
Yes
Scale
No
No
Generalization gap
path norm
(9)
Neyshabur et al. (2015)
No
Scale
No
No
Generalization gap
mp softrank
(10) Martin & Mahoney (2021b)
No
Scale/Shape
No
No
Model quality
stable rank
(11) Martin & Mahoney (2021b)
No
Scale/Shape
No
No
Model quality
PL alpha
(1)
Martin & Mahoney (2021b)
No
Shape
No
No
Model quality
E TPL beta
(2)
This paper
WeightWatcher
No
Shape
No
No
Model quality
E TPL lambda
(2)
This paper
WeightWatcher
No
Shape
No
No
Model quality
EXP lambda
(3)
This paper
WeightWatcher
No
Shape
No
No
Model quality
PL ks distance
(12) Martin & Mahoney (2021b)
No
Shape
No
No
Model quality
E TPL ks distance
(12)
This paper
Martin & Mahoney (2021b)
No
Shape
No
No
Model quality
alpha weighted
(13) Martin & Mahoney (2021b)
No
Hybrid
No
No
Model quality
log alpha norm
(14) Martin & Mahoney (2021a)
No
Hybrid
No
No
Model quality
inverse margin
(17)
Jiang et al. (2019)
No
Scale
Yes
Maybe
Generalization gap
log prod of spec over margin (18)
Bartlett et al. (2017)
Pitas et al. (2017)
No
Scale
Yes
Maybe
Generalization gap
log sum of spec over margin
(19)
Bartlett et al. (2017)
Pitas et al. (2017)
No
Scale
Yes
Maybe
Generalization gap
log prod of fro over margin
(20)
Bartlett et al. (2017)
Pitas et al. (2017)
No
Scale
Yes
Maybe
Generalization gap
log sum of fro over margin
(21)
Bartlett et al. (2017)
Pitas et al. (2017)
No
Scale
Yes
Maybe
Generalization gap
path norm over margin
(22)
Neyshabur et al. (2015)
No
Scale
Yes
Maybe
Generalization gap
pacbayes init
(25)
Neyshabur et al. (2017)
Yes
Scale
Yes
Yes
Generalization gap
pacbayes orig
(26)
Neyshabur et al. (2017)
No
Scale
Yes
Yes
Generalization gap
pacbayes flatness
(27)
Neyshabur et al. (2017)
No
Scale
Yes
Yes
Generalization gap
pacbayes mag init
(28)
Jiang et al. (2019)
Yes
Scale
Yes
Yes
Generalization gap
pacbayes mag orig
(29)
Jiang et al. (2019)
No
Scale
Yes
Yes
Generalization gap
pacbayes mag flatness
(30)
Jiang et al. (2019)
No
Scale
Yes
Yes
Generalization gap
Table 1: Overview of the generalization metrics considered. We focus on the shape metrics derived from the
ESDs of weight matrices. See Appendix A for the details of these metrics.
Issues of PL ﬁtting. It is well-known that subtle issues can arise when ﬁtting the ESDs (Alstott et al.,
2014; Clauset et al., 2009; Martin & Mahoney, 2017, 2021a). To best mitigate these issues in PL ﬁts, we
adopt the ﬁtting strategies used in WeightWatcher (Martin & Mahoney, 2017). For example, as in Clauset
et al. (2009), it is common to choose the lower threshold xmin which coincides with the best quality ﬁt under
the Kolmogorov–Smirnoﬀstatistic (referred to as PL ks distance for PL and E TPL ks distance for E-TPL
in the sequel; see Eqn. (12)). However, this method is time-consuming, especially for E-TPL as there are two
parameters to ﬁt. Instead, we adopt the ﬁx-ﬁnger method (see WeightWatcher) which selects xmin as the
peak of the ESD when ﬁtting E-TPLs. More than a simple speed improvement, we ﬁnd this method also
yields more stable results.
5

Comparing PL and E-TPL ﬁtting. Referring to Figure 1, we now discuss how E-TPL could partially
address these ﬁtting issues. On the ﬁrst row of Figure 1, we show three typical cases of PL ﬁtting. In
Figure 1a, the log-log scale reveals a “linear region” of the histogram, which the PL ﬁtting correctly locates.
The quality of ﬁt, measured by the ks distance, is within a typical range, as reported in Table 5 of Martin
& Mahoney (2021b). In Figure 1b and Figure 1c, the ESDs do not exhibit a clear linear region on the log-log
scale. Following Martin & Mahoney (2021b), it is ill-advised to consider metrics derived from a PL ﬁt in
these scenarios. In practice, this typically occurs when PL alpha > 4 (e.g., see Figure 1c). On the other hand,
in these two cases, the corresponding E-TPL ﬁts (shown on the second row in Figure 1) still closely match
the empirical density function (see Figure 1e and Figure 1f), and the ks distance on the second row using a
E-TPL ﬁt is smaller than that for the PL ﬁt on the ﬁrst row, even when the ﬁt on the second row clearly
covers a larger part of the ESD. 4 In these two cases, the E TPL lambda plays a similar role as the PL alpha
in PL ﬁtting, and provides an eﬀective alternative when the ESD does not exhibit a proper PL.
Between these three PL and E-TPL ﬁttings, we would like to point out that the important thing in
HT-SR is not the PL ﬁtting per se but that the spectral distributions exhibit HT or other non-standard
shapes. The particular forms of the distributions ﬁt here simply constitute diﬀerent ways to quantify this
property in practice. These details, such as selecting the most appropriate distributional assumptions,
clearly matter if we would like to engineer the tools of HT analysis to eﬀectively measure the ground truth.
However, the primary concern in predicting generalization is to measure the shape information, and the shape
information is independent of the ﬁtting procedure, although better ﬁtting procedures may capture the shape
information better.
3
Empirical results
In this section, we ﬁrst give full details of the experimental setup, in Section 3.1. Then, we provide the
analyses of the empirical results: in Section 3.2, we study Transformers trained with diﬀerent hyperparameters;
and in Section 3.3, we focus on pretrained Transformers from the Huggingface website.
3.1
Experimental setup
Dataset. We consider the WMT14 German to English (DE-EN) dataset (Bojar et al., 2014), commonly
used as a benchmark for neural machine translation (Edunov et al., 2018; Ott et al., 2018; Shen et al., 2020;
Vaswani et al., 2017). WMT14 consists of 4.5 million sentence pairs for training.
Hyperparameters. To capture the relationship between the generalization metrics and model quality
in a number of diﬀerent settings, we vary several hyperparameters: the number of samples (either 160K,
320K, 640K, 1.28M, 2.56M samples), the initial learning rate during training (across eight diﬀerent rates),
the model width (embedding dimension either 256, 384, 512, 768, or 1024), and the model depth ({4,
5, 6, 7, 8}-layer transformers). We also construct a high-dimensional grid of diﬀerent hyperparameters
Θ = {(θ1, . . . , θK) : θ1 ∈Θ1, . . . , θK ∈ΘK}, so that we can compare models when one of the hyperparameters
is varied. Two separate high-dimensional grids with dimension K = 3 are considered: (1) sample×learning
rate×width; (2) sample×learning rate×depth. Each grid contains 5×8×5=200 of these training settings. In
total, there are 360 trained models because the two high-dimensional grids overlap each other, and 40 models
belong to both grids. We will consider three subtasks to evaluate the considered generalization metrics.
Task one, correlation evaluated on optimally trained models.
In the ﬁrst task (Section 3.2.1),
we study the relationship between model quality and generalization metrics on models trained with the
optimal choice of hyperparameters. This task mimics the grid-search method often employed in large-scale
(pre)training tasks.
Task two, correlation in time. In the second task (Section 3.2.2), we track BLEU score and generalization
metrics during training, assessing time-wise correlation to model quality. This task has been considered in
4We note that the value of ks distance can be eﬀectively made smaller if one restricts to a smaller part of the distribution,
as is often done in practice by optimizing the xmin in the (truncated) PL distribution (1). This potential bias is alleviated by
using the ﬁx-ﬁnger method.
6

Figure 2: BLEU-score vs. six shape metrics for 200 Transformers trained on WMT14 with varying hyperpa-
rameters. HT-SR theory applies for optimally-tuned models (black stars), that is, models that have better
BLEU scores exhibit heavier-tailed ESDs. HT-SR theory is a predictive theory designed to provide predictions
for state-of-the-art-models (Martin & Mahoney, 2021b; Martin et al., 2021); thus, for suboptimal models, the
HT-SR metrics can be anti-correlated with model quality, see e.g. the grey dotted line in the ﬁrst subﬁgure.
the literature (Bartlett et al., 2017), and from a practical point of view, capturing the time-wise dependence
during training could potentially lead to better ways of early stopping or regularizing the model.
Task three, correlation when hyperparameters are varied. In the third task (Section 3.2.3), we study
the relationship between the model quality and the generalization metrics when a single hyperparameter is
varied. Metrics that achieve a high (rank) correlation for all the hyperparameters are good candidates for
model selection.
Training and model setup. For the details of the training settings, see Appendix B.
3.2
Evaluating the metrics on Transformers trained in diﬀerent settings
In this subsection, we study 28 generalization metrics (with details provided in Table 1), and we examine their
correlations with BLEU score (Papineni et al., 2002), the most commonly used metric to evaluate machine
translation. We also consider correlation between these metrics and the generalization gap, deﬁned as the
BLEU score for training data subtracted by the BLEU score for test data.
3.2.1
Task one: Evaluating correlations on optimally trained models only
Here, we group models using the number of training samples, and select the best model from each group
when the model depth and the learning rate are varied. In Figure 2, each curve represents a group of models
trained with a certain number of training samples. The black star on each curve represents the model trained
with the optimal choice of hyperparameters (learning rate and depth in our setting), obtained by searching
for the optimum on a third-order polynomial ﬁt of each curve. From Figure 2, we see that the shape metrics
always correctly predict the model quality, i.e., the BLEU scores should be higher when the metric values are
smaller. Since all six shape metrics show similar trends, a pairing of these metrics can be considered as a
sanity check.
7

0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 160000
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 320000
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 640000
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 1280000
0
10
20
30
40
50
BLEU score
0
10
20
30
40
50
BLEU score
0
10
20
30
40
50
BLEU score
0
10
20
30
40
50
BLEU score
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 160000
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 320000
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 640000
0
5
10
15
20
Epoch number
0.00
0.02
0.04
0.06
0.08
0.10
E-TPL lambda
Num samples = 1280000
0
10
20
30
40
50
BLEU score
0
10
20
30
40
50
BLEU score
0
10
20
30
40
50
BLEU score
0
10
20
30
40
50
BLEU score
Figure 3: E TPL lambda closely tracks the BLEU score, i.e., BLEU score increases when the E TPL lambda
drops. Results are shown for Transformers trained on WMT14 with diﬀerent number of samples. (First
row). Training with dropout 0.1. (Second row). Training without dropout.
Remark. Figure 2 points out an important but subtle issue in empirically evaluating the HT-SR theory. In
Figure 2, one can make a model less well-trained—and artiﬁcially anti-correlate the generalization metric
with the task accuracy. For example, see the gray dotted line in the ﬁrst subﬁgure in Figure 2. In Section 3.3,
we study models from the Huggingface website, and therefore, our results will strongly depend on how
well-trained these models are.
3.2.2
Task two: Time-wise correlations and rank correlation Results
In this subsection, we study time-wise correlation between our chosen metrics and the BLEU scores. In other
words, we see if a generalization metric can track the test curve during training.
E TPL lambda tracks the BLEU score. As a warm-up, we consider how well the E TPL lambda metric
deﬁned in (2) tracks the BLEU score (recalling that E TPL lambda assumes the ESDs follow E-TPLs). We use
training with and without dropout to study the eﬀect of training schemes, and we consider diﬀerent quantities
of data to test robustness in the dataset. In Figure 3, the ﬁrst row considers models trained with dropout,
while the second row considers models trained without dropout. The multiple columns track E TPL lambda
and the BLEU score throughout training for diﬀerent amounts of data. We can see that E TPL lambda
not only successfully tracks BLEU scores but also diﬀerentiates underﬁtting (ﬁrst row, with dropout) from
overﬁtting (second row, without dropout) in this experiment.
Shape metrics predict model quality, while scale metrics predict the generalization gap.
Now we consider the rank correlations between our chosen metrics and the test BLEU score. The rank
correlations are evaluated across training, i.e., for each of the 360 settings of the hyperparameters, we calculate
the Spearman’s rank correlation between BLEU scores and the values of each generalization metric over
all epochs. The summarized results are presented in Figure 4a. A positive Spearman’s rank correlation
(with BLEU) suggests that the generalization metric is useful in tracking BLEU during training. A negative
Spearman’s rank correlation, on the other hand, implies that the metric often gives the incorrect prediction.
In Figure 4a, we use the average rank correlations for all settings to study the eﬀectiveness of each metric,
and present 25% quantile rank correlations to indicate robustness across runs.
In Figure 4a, we ﬁnd shape metrics, such as EXP lambda, E TPL lambda, E TPL ks distance, and
E TPL beta, exhibit some of the highest rank correlations with BLEU score.
The EXP lambda metric,
which assumes a EXP distribution on the ESDs, achieves the highest median rank correlation, while the
E TPL lambda metric, which assumes a E-TPL distribution on the ESDs, achieves the second highest. We
discuss the inverse margin metric in Appendix C.
8

Correlations with model quality
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
exp_lambda
e_tpl_lambda
e_tpl_ks_distance
inverse_margin
e_tpl_beta
mp_softrank
stable_rank
pl_alpha
path_norm_over_margin
pl_ks_distance
alpha_weighted
log_alpha_norm
pacbayes_mag_orig
path_norm
dist_spec_init
pacbayes_mag_init
pacbayes_flatness
param_norm
pacbayes_orig
fro_dist
pacbayes_init
pacbayes_mag_flatness
log_sum_of_fro_over_margin
log_prod_of_fro_over_margin
log_sum_of_spec_over_margin
log_prod_of_spec_over_margin
log_norm
log_spectral_norm
Shape
Hybrid
Scale
(a) Correlations with model quality.
Spearman’s
rank correlation between various generalization metrics
and BLEU.
Correlations with generalization gap
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
log_prod_of_spec_over_margin
log_sum_of_spec_over_margin
log_spectral_norm
log_norm
log_prod_of_fro_over_margin
log_sum_of_fro_over_margin
pacbayes_init
fro_dist
pacbayes_orig
pacbayes_mag_flatness
pacbayes_flatness
param_norm
dist_spec_init
pacbayes_mag_init
path_norm
alpha_weighted
pacbayes_mag_orig
log_alpha_norm
pl_ks_distance
path_norm_over_margin
pl_alpha
stable_rank
mp_softrank
e_tpl_beta
inverse_margin
e_tpl_lambda
e_tpl_ks_distance
exp_lambda
Shape
Hybrid
Scale
(b) Correlations with generalization gap.
Spear-
man’s rank correlation between various generalization
metrics and the generalization gap.
Figure 4: Comparing multiple generalization metrics for predicting BLEU score (on the left) or the general-
ization gap (on the right). Lines on each box delineate the 25/50/75 percentiles of the rank correlations in
360 diﬀerent settings (including diﬀerent amount of data, diﬀerent network depths, diﬀerent network widths,
and diﬀerent initial learning rates).
In Figure 4b, we plot the rank correlations to the generalization gap across our chosen metrics. While it is
encouraging that most existing generalization metrics yield correct predictions, as previously discussed, correct
predictions of the generalization gap do not imply accurate predictions on the best-performing models here.
3.2.3
Task three: Rich correlational structures and the Simpson’s paradox when data size,
model size and training hyperparameters are varied
For our ﬁnal task, we vary each of the hyperparameters and study the trends of the generalization metrics.
We ﬁrst focus on the sample×learning rate×depth hyperparameter grid— see Figures 5 and 6 for plots of
BLEU score against shape and scale metrics, respectively. Since we vary the number of samples, the learning
rate, and the model depth to obtain diﬀerent models, we group these models to visualize trends over each
hyperparameter. In each subﬁgure, we color-code the models by either learning rate or the number of samples.
We discuss grouping models by depth later in Figure 8c. Note that Figure 5 partially overlaps with Figure 2,
except for diﬀerent ﬁtting methods.
For each curve, we expect the generalization metrics to be negatively correlated with the models’ quality
measured using the BLEU score, i.e., the regression lines should have negative slopes. Comparing Figure 5
and 6, one can see that the shape metrics tend to show the correct trends (which are more negatively correlated)
than the scale metrics.
Remark. In Figure 5, constrained by the least-squares ﬁtting, some regression lines are not aligned well
with data, e.g., the second ﬁgure on the second row. In Appendix D, we ﬁt the data using the orthogonal
distance regression to mitigate this issue.
9

0.0
0.2
0.4
0.6
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
3
4
5
6
7
8
9
PL_alpha
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
PL_alpha vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.04
0.06
0.08
0.10
0.12
E_TPL_ks_distance
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_ks_distance vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.0
0.2
0.4
0.6
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
3
4
5
6
7
8
9
PL_alpha
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
PL_alpha vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
0.04
0.06
0.08
0.10
0.12
E_TPL_ks_distance
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_ks_distance vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
Figure 5: BLEU-score versus shape metrics for 200 Transformers trained on WMT14 with diﬀerent hyper-
parameters. (First row). Trained models grouped by the learning rate. (Second row). Trained models
grouped by the number of samples. The BLEU scores and the evaluated shape metrics display the correct
(downward) trend.
Abnormal hyperparameters lead to the “Simpson’s paradox”. From Figure 5, we can see that
the prediction of trends degrades for relatively large learning rates. For a ﬁxed number of samples, when the
learning rate becomes larger, the trends deviate from a perfectly linear ﬁt. Now, we include more models
trained with particularly large learning rates, and we show the results in Figure 7. We see that the results
potentially display a “Simpson’s paradox” (similar results for a diﬀerent corpus of models have been reported
previously by Martin & Mahoney (2021a)), i.e., the overall correlation trends are opposite to the trends in
individual groups. In these ﬁgures, the regression lines are strongly inﬂuenced by the models trained with
large learning rates and are biased towards the models with low BLEU scores. This phenomenon is known in
the HT-SR literature (Martin & Mahoney, 2021a; Martin et al., 2021), and one often has to avoid biasing the
results with these poorly trained models 5.
BLEU scores are not signiﬁcantly inﬂuenced by network depth. Unlike learning rates and
number of samples, we ﬁnd that the BLEU scores are almost identical when we vary the number of layers
from 4 to 8. In Figure 8a and 8b, we show E TPL lambda vs BLEU for models grouped by diﬀerent learning
rates and number of samples. These two subﬁgures are repeated from the ﬁrst column of Figure 7. From
these two ﬁgures, we see that the BLEU scores vary signiﬁcantly when these two hyperparameters are varied.
In Figure 8c, we show the same set of models color-coded by the network depth. We can see that the BLEU
scores almost remain identical. This is because, from Figure 8a and 8b, we see that these models are roughly
divided into “vertical” groups when the learning rate is varied, and they are roughly divided into “horizontal”
groups when the number of samples is varied. Therefore, each small “cluster” in Figure 8c corresponds to a
group of models trained with the same learning rate and the number of samples but diﬀerent depths, and
these clusters show that the BLEU scores almost remain ﬁxed when the depth is varied. This phenomenon
suggests that the rank correlations calculated for varying depths may not be informative.
Rank correlations. To systematically evaluate the various metrics considered in this paper, we study the
5From a theoretical point of view, this phenomenon is caused by the change of the HT random matrix universality class at
the point of PL alpha = 2. For more details, see Table 1 of Martin & Mahoney (2021b).
10

0.75
1.00
1.25
1.50
1.75
2.00
2.25
log_spectral_norm
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_spectral_norm vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
7.5
8.0
8.5
9.0
9.5
10.0
10.5
log_sum_of_spec_over_margin
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_sum_of_spec_over_margin vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
60
80
100
120
140
160
pacbayes_flatness
10
15
20
25
30
BLEU score
pacbayes_flatness vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.75
1.00
1.25
1.50
1.75
2.00
2.25
log_spectral_norm
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_spectral_norm vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
7.5
8.0
8.5
9.0
9.5
10.0
10.5
log_sum_of_spec_over_margin
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_sum_of_spec_over_margin vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
60
80
100
120
140
160
pacbayes_flatness
10
15
20
25
30
BLEU score
pacbayes_flatness vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
Figure 6: BLEU-score versus shape metrics for 200 Transformers trained on WMT14 with diﬀerent hyper-
parameters. (First row). Trained models grouped by the learning rate. (Second row). Trained models
grouped by the number of samples. The BLEU scores and the evaluated shape metrics display the wrong
(upward) trend.
0.0
0.2
0.4
0.6
E_TPL_lambda
0
5
10
15
20
25
30
BLEU score
E_TPL_lambda vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
2
4
6
8
PL_alpha
0
5
10
15
20
25
30
35
BLEU score
PL_alpha vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.04
0.06
0.08
0.10
0.12
E_TPL_ks_distance
0
5
10
15
20
25
30
BLEU score
E_TPL_ks_distance vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.0
0.2
0.4
0.6
E_TPL_lambda
0
5
10
15
20
25
BLEU score
E_TPL_lambda vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
1.5
2.0
2
4
6
8
PL_alpha
0
5
10
15
20
25
BLEU score
PL_alpha vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
1.5
2.0
0.04
0.06
0.08
0.10
0.12
E_TPL_ks_distance
0
5
10
15
20
25
BLEU score
E_TPL_ks_distance vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
1.5
2.0
Figure 7: BLEU-score versus shape metrics with particularly large learning rates. (First row). Trained
models grouped by the learning rate. (Second row). Trained models grouped by the number of samples.
The BLEU scores and the evaluated shape metrics display Simpson’s paradox when there are models trained
with particularly large learning rates (1.5 and 2.0).
11

0.0
0.2
0.4
0.6
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
(a) Grouping by learning rates.
0.0
0.2
0.4
0.6
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
(b) Grouping by number of samples.
0.0
0.2
0.4
0.6
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
depth
4
5
6
7
8
(c) Grouping by network depths.
Figure 8: E TPL lambda vs BLEU for the same set of trained models grouped by diﬀerent hyperparameters.
While BLEU scores change signiﬁcantly when the initial learning rates and the number of samples are varied,
they almost remain ﬁxed for varying network depths.
rank correlation between these metrics and the BLEU score. For Task three, we consider each one-dimensional
slice of the hyperparameter space Θ = {(θ1, . . . , θK) : θ1 ∈Θ1, . . . , θK ∈ΘK}, i.e., slices of the form
{(θ1, . . . , θK) : θi ∈Θi while other parameters θj, j ̸= i are ﬁxed},
and we calculate the rank correlation using the models in each such slice. Then, we aggregate the rank
correlations from all the one-dimensional slices and plot the distributions of the rank correlations. See
Figure 9. As we have shown in Figure 8c, the rank correlations calculated with varying network depths may
not be informative due to the insigniﬁcant change in the BLEU score. Therefore, we focus on the other three
hyperparameters, namely learning rate, network width and number of samples. Also, similar to Task two, we
provide the rank correlation results on both the test BLEU scores and the generalization gap. Results on the
generalization gap are shown in Figure 10.
Before we analyze the results of Figure 9 and 10, we discuss a subtle issue in calculating the generalization
metrics.
We note that, in Jiang et al. (2019), generalization metrics are “normalized” by dividing by
the (square root of the) number of training samples, in correspondence with how they appear in uniform
generalization bounds. However, normalizing the generalization metrics from Jiang et al. (2019) by the
number of samples unavoidably complicates the correlation when varying the number of samples. This makes
the comparison between scale metrics from Jiang et al. (2019) and the shape metrics challenging, as there is
no natural way to normalize the shape metrics with respect to the number of training samples. Therefore, in
Figure 9, we include the results for both with and without dividing the generalization metrics from Jiang
et al. (2019) by the (square root of the) number of samples.
Here are our observations from Figure 9 and 10.
• From Figure 9a and 9c, shape metrics perform particularly well when varying the learning rate and the
number of samples. Speciﬁcally, in Figure 9c, several shape metrics achieve perfect rank correlations,
which are close to 1. From Figure 9b, shape metrics also perform well for varying network widths except
for stable rank and E TPL beta6.
• From Figure 9d, normalizing the scale metrics from Jiang et al. (2019) by the number of samples
signiﬁcantly improves their correlational predictions. However, shape metrics can achieve a similar
performance without the help of normalizing the number of samples.
6The insuﬃciency of stable rank is caused by the inﬂuence of the matrix size when the model width is varied, i.e., wider
models tend to have a larger stable rank simply because of the increased matrix size instead of the model quality. The
insuﬃciency of E TPL beta is likely due to the ﬁx-ﬁnger method in E-TPL ﬁttings, which ﬁxes xmin as the peak of the ESD
without searching for the optimal value. However, optimal E-TPL ﬁtting requires simultaneous searching of xmin, E TPL beta
and E TPL lambda, which is computationally demanding. Thus, further investigation is necessary for achieving a balance between
the quality of ﬁtting and the computational cost.
12

• By comparing Figure 9 and 10, one can see that the scale metrics are much better correlated with the
generalization gap than the test BLEU scores. For Figure 10a and 10b, this conclusion is obvious from
the plots. For Figure 10c and 10d, the scale metrics need to be divided by (the square root of) the
number of samples to achieve a good correlation when the number of samples is varied.
Details of the rank correlation calculations. When calculating the rank correlation with the test
accuracy, we associate a negative sign to all the generalization metrics, i.e., a positive rank correlation in
Figure 4a means that a generalization metric is negatively correlated with the BLEU score. We use this
procedure to follow the conventional wisdom that a smaller value of the complexity metric leads to better
generalization. On the other hand, for Figure 4b, a positive rank correlation means that the metric is
positively correlated with the generalization gap.
3.3
Evaluating the generalization metrics using Huggingface Transformers
Finally, we evaluate the data-free generalization metrics on pretrained Transformers. Eight series of models
downloaded from Huggingface (Wolf et al., 2020) are considered—see Table 2 for details. We also include 24
BERT models from the “Smaller BERT” series (Turc et al., 2019) produced from a “pretrained distillation”
pipeline that combines masked language modeling pretraining (Kenton & Toutanova, 2019) and knowledge
distillation from a single BERT teacher model. In total, there are 51 pretrained Transformers.
Model series
Models
BERT (Kenton & Toutanova, 2019)
BERT-Tiny, BERT-Mini, BERT-Small, BERT-Medium, BERT-Base, BERT-Large
Smaller BERT (Turc et al., 2019)
24 smaller BERT models (English only, uncased, trained with WordPiece masking)
GPT2 (Radford et al., 2019)
GPT2, GPT2-medium, GPT2-large, GPT2-xl
ALBERTv1 (Lan et al., 2019)
ALBERT-base-v1, ALBERT-large-v1, ALBERT-xlarge-v1, ALBERT-xxlarge-v1
ALBERTv2 (Lan et al., 2019)
ALBERT-base-v2, ALBERT-large-v2, ALBERT-xlarge-v2, ALBERT-xxlarge-v2
T5 (Raﬀel et al., 2020)
T5-small, T5-base, T5-large
DialoGPT (Zhang et al., 2020)
DialoGPT-small, DialoGPT-medium, DialoGPT-large
FlauBERT (Le et al., 2020)
FlauBERT small cased, FlauBERT base cased, FlauBERT large cased
Funnel Transformer (Dai et al., 2020)
FunnelModel-small, FunnelModel-medium, FunnelModel-intermediate
FunnelModel-large, FunnelModel-xlarge
Table 2: Pretrained Transformers considered in this paper.
We report rank-correlations averaged over these 8 model series in Figure 11a (left subplot), i.e.,
larger/deeper models should have smaller generalization metric values. Again, we ﬁnd that the shape
metrics outperform scale metrics (except for stable rank, which is strongly inﬂuenced by the size of the
weight matrix). The hybrid models achieve performance in-between the shape and scale metrics. In Figure 11a
(right subplot), we compare diﬀerent metrics in their ability to select the best model. That is, we report for
each metric the proportion that the best model is selected from one model series when this metric is used as
the model selection criterion. Note that the rankings of metrics on the two subplots in Figure 11a are the
same.
From Figure 11a, we can see that, while the shape metrics perform better than scale metrics, none show a
particularly strong rank correlation. To understand this, we examine the “Smaller BERT” series (Turc et al.,
2019), which contains a more ﬁne-grained structure of diﬀerent model sizes. Speciﬁcally, these models are
arranged in a 4-by-6 grid, where 6 represents {2,4,6,8,10,12} transformer layers and 4 means diﬀerent hidden
embedding sizes {128,256,512,768}. From Figure 11b, we see that the E TPL ks distance correctly predicts
the trend that wider and deeper models perform better. On the other hand, from Figure 11c, E TPL lambda
correctly predicts that wider models are better, but incorrectly predicts that shallower models are better
(yet another form of Simpson’s paradox in a data set of neural network model quality; see also Martin &
Mahoney (2021a)). We note that the BERT series {BERT-Tiny, BERT-Mini, BERT-Small, BERT-Medium,
BERT-Base, BERT-Large} overlaps with the 2D grid (as shown in Figure 11b and 11c). Consequently, the
rank correlations for the BERT series (which we include as one of the eight series in making Figure 11a, is a
“noisy subsample” of the results in Figure 11b and 11c.
Another curious observation from Figure 11a is that, for the pretrained transformers, PL metrics,
such as PL alpha and PL ks distance, outperform E-TPL metrics, such as E TPL lambda, E TPL beta and
13

Correlations with model quality when diﬀerent hyperparameters are varied.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
pl_ks_distance
e_tpl_ks_distance
inverse_margin
exp_lambda
e_tpl_lambda
pl_alpha
mp_softrank
stable_rank
pacbayes_mag_orig
path_norm_over_margin
alpha_weighted
log_alpha_norm
path_norm
e_tpl_beta
pacbayes_flatness
param_norm
log_prod_of_spec_over_margin
log_spectral_norm
fro_dist
dist_spec_init
log_prod_of_fro_over_margin
log_norm
log_sum_of_fro_over_margin
log_sum_of_spec_over_margin
pacbayes_init
pacbayes_orig
pacbayes_mag_init
pacbayes_mag_flatness
Shape
Hybrid
Scale
(a) Varying learning rate.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
dist_spec_init
e_tpl_ks_distance
mp_softrank
pacbayes_mag_flatness
pl_ks_distance
e_tpl_lambda
exp_lambda
path_norm
path_norm_over_margin
pacbayes_flatness
inverse_margin
pl_alpha
alpha_weighted
log_alpha_norm
fro_dist
pacbayes_init
pacbayes_orig
param_norm
e_tpl_beta
log_spectral_norm
stable_rank
log_prod_of_spec_over_margin
log_prod_of_fro_over_margin
pacbayes_mag_orig
pacbayes_mag_init
log_sum_of_fro_over_margin
log_norm
log_sum_of_spec_over_margin
Shape
Hybrid
Scale
(b) Varying network width.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
exp_lambda
e_tpl_lambda
e_tpl_ks_distance
mp_softrank
stable_rank
pl_ks_distance
e_tpl_beta
inverse_margin
pl_alpha
path_norm_over_margin
log_alpha_norm
path_norm
alpha_weighted
param_norm
pacbayes_mag_init
log_sum_of_spec_over_margin
pacbayes_init
log_sum_of_fro_over_margin
log_norm
pacbayes_mag_orig
fro_dist
pacbayes_flatness
log_prod_of_fro_over_margin
dist_spec_init
pacbayes_mag_flatness
log_spectral_norm
log_prod_of_spec_over_margin
pacbayes_orig
Shape
Hybrid
Scale
(c) Varying number of training samples. In this subplot,
metrics from Jiang et al. (2019) are not normalized by the
number of training samples.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
exp_lambda
stable_rank
e_tpl_lambda
log_sum_of_spec_over_margin
e_tpl_ks_distance
log_sum_of_fro_over_margin
mp_softrank
pacbayes_mag_init
pacbayes_mag_orig
pacbayes_flatness
param_norm
inverse_margin
pacbayes_orig
pl_ks_distance
fro_dist
e_tpl_beta
pacbayes_mag_flatness
pacbayes_init
pl_alpha
dist_spec_init
path_norm_over_margin
path_norm
log_alpha_norm
alpha_weighted
log_prod_of_spec_over_margin
log_spectral_norm
log_prod_of_fro_over_margin
log_norm
Shape
Hybrid
Scale
(d) Varying number of training samples. In this subplot,
metrics from Jiang et al. (2019) are normalized by the
number of training samples.
Figure 9: Comparing multiple generalization metrics in terms of the rank correlations with the test BLEU
score when multiple hyperparameters are varied. Metrics are ranked by the median rank correlations.
14

Correlations with generalization gap when diﬀerent hyperparameters are varied.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
pacbayes_mag_flatness
log_spectral_norm
log_sum_of_spec_over_margin
pacbayes_init
log_norm
log_sum_of_fro_over_margin
fro_dist
param_norm
log_prod_of_fro_over_margin
log_prod_of_spec_over_margin
pacbayes_orig
dist_spec_init
e_tpl_beta
pacbayes_mag_init
log_alpha_norm
alpha_weighted
pacbayes_flatness
path_norm
path_norm_over_margin
inverse_margin
stable_rank
mp_softrank
e_tpl_lambda
exp_lambda
pacbayes_mag_orig
pl_alpha
e_tpl_ks_distance
pl_ks_distance
Shape
Hybrid
Scale
(a) Varying learning rate.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
pacbayes_orig
stable_rank
e_tpl_beta
log_sum_of_spec_over_margin
pacbayes_init
log_norm
log_sum_of_fro_over_margin
pacbayes_mag_init
pacbayes_mag_orig
fro_dist
param_norm
log_prod_of_fro_over_margin
log_prod_of_spec_over_margin
log_spectral_norm
pacbayes_flatness
log_alpha_norm
alpha_weighted
inverse_margin
path_norm_over_margin
pl_alpha
path_norm
dist_spec_init
e_tpl_lambda
exp_lambda
e_tpl_ks_distance
pl_ks_distance
pacbayes_mag_flatness
mp_softrank
Shape
Hybrid
Scale
(b) Varying network width.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
exp_lambda
e_tpl_lambda
e_tpl_ks_distance
mp_softrank
stable_rank
inverse_margin
e_tpl_beta
pl_ks_distance
pl_alpha
path_norm_over_margin
alpha_weighted
log_alpha_norm
path_norm
param_norm
log_prod_of_spec_over_margin
log_spectral_norm
fro_dist
dist_spec_init
log_prod_of_fro_over_margin
pacbayes_flatness
pacbayes_mag_orig
pacbayes_mag_init
pacbayes_mag_flatness
log_sum_of_fro_over_margin
log_norm
pacbayes_init
log_sum_of_spec_over_margin
pacbayes_orig
Shape
Hybrid
Scale
(c) Varying number of training samples. In this subplot,
metrics from Jiang et al. (2019) are not normalized by the
number of training samples.
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Spearman s rank correlation
exp_lambda
e_tpl_lambda
log_sum_of_spec_over_margin
log_sum_of_fro_over_margin
pacbayes_mag_init
inverse_margin
pacbayes_mag_orig
pacbayes_flatness
stable_rank
e_tpl_ks_distance
mp_softrank
param_norm
e_tpl_beta
fro_dist
pacbayes_orig
pl_ks_distance
pacbayes_mag_flatness
pacbayes_init
pl_alpha
dist_spec_init
path_norm_over_margin
path_norm
alpha_weighted
log_alpha_norm
log_spectral_norm
log_prod_of_fro_over_margin
log_prod_of_spec_over_margin
log_norm
Shape
Hybrid
Scale
(d) Varying number of training samples. In this subplot,
metrics from Jiang et al. (2019) are normalized by the
number of training samples.
Figure 10: Comparing multiple generalization metrics in terms of the rank correlations with the generalization
gap when multiple hyperparameters are varied. Metrics are ranked by the median rank correlations.
15

0.5
0.0
0.5
Spearman s rank correlation
Shape
Hybrid
Scale
0.00
0.25
0.50
0.75
1.00
Proportion of models correctly selected
stable_rank
log_norm
log_spectral_norm
log_alpha_norm
alpha_weighted
e_tpl_lambda
exp_lambda
e_tpl_beta
pl_alpha
mp_softrank
e_tpl_ks_distance
pl_ks_distance
Shape
Hybrid
Scale
(a) Model selection on Huggingface Transformers. Metrics on the left
and right are aligned.
2
4
6
8
10
12
Number of layers
128
256
512
768
Hidden dimmension size
BERT
Tiny
BERT
Mini
BERT
Small
BERT
Medium
BERT
Base
e_tpl_ks_distance
0.06
0.07
0.08
(b) E TPL ks distance evaluated on BERT models of
diﬀerent size.
2
4
6
8
10
12
Number of layers
128
256
512
768
Hidden dimmension size
BERT
Tiny
BERT
Mini
BERT
Small
BERT
Medium
BERT
Base
e_tpl_lambda
0.1
0.2
0.3
0.4
0.5
0.6
(c) E TPL lambda evaluated on BERT models of dif-
ferent size.
Figure 11: Generalization metrics evaluated on pretrained Transformers. (a) Model selection results on eight
Huggingface Transformer model series: BERT, GPT2, ALBERTv1, ALBERTv2, T5, DialoGPT, FlauBERT,
Funnel Transformer. Left shows the rank correlation averaged over diﬀerent Transformers. Right shows the
proportion of the best Transformers correctly selected using diﬀerent metrics. Shape metrics outperform scale
metric only except stable rank which is strongly inﬂuenced by the matrix size. (b and c) Evaluating two
metrics on the “Smaller BERT” series. While E TPL ks distance predicts the correct trends, E TPL lambda
shows the reversed trends with depth.
16

E TPL ks distance. This phenomenon may seem surprising as one may expect E-TPL ﬁts to be more ﬂexible
than PL ﬁts. These pretrained models are likely trained with much larger datasets and over many more
epochs than the models we have otherwise considered. Here, PLs appear to provide a more natural ﬁt.
This is further evidence that HT-SR theory is particularly well-suited for evaluating the quality of relatively
high-quality models.
4
Conclusion
After conducting large-scale empirical studies on a variety of metrics, we ﬁnd that shape metrics derived from
HT-SR theory are more eﬀective for model selection and model evaluation, in particular for evaluating models
even without access to training or testing data. Poor correlations between existing generalization metrics and
test-time performance have been reported in prior work (Dziugaite et al., 2020; Jiang et al., 2019; Nagarajan
& Kolter, 2019). Rather than providing a “lump sum” to rank existing and novel generalization metrics
(Figure 4), we evaluated these metrics in several ways: quantifying correlations only on optimally-trained
models (Figure 2); examining the time-wise correlation during training (Figure 3); diﬀerentiating between
the correlation with test accuracy versus generalization gap (Figure 4); thoroughly investigating the rich
correlational structures when diﬀerent hyperparameters are varied (Figures 5 to 10); and evaluating these
metrics on pretrained Transformer models where we do not have any control over the training process
(Figure 11). By thorough empirical investigations, we show that shape metrics perform consistently better
than scale metrics in model selection—they correlate primarily with test accuracy instead of generalization
gap, and they display better correlations with models’ test performance. These metrics from HT-SR theory
provide value to practitioners, allowing one to assess pretrained NLP models without training or testing data,
even when their corresponding training loss is not small. We expect these studies to be relevant and useful
for improving existing generalization metrics in NLP moving forward.
Acknowledgements. MM would like to acknowledge the IARPA (contract W911NF20C0035), NSF,
and ONR for providing partial support of this work. KR would like to acknowledge support from NSF
CIF-2007669, NSF CIF-1703678 and ARO fund 051242-002. JG would like to acknowledge support from NSF
CISE Expeditions Award CCF-1730628, NSF CAREER Award and gifts from Alibaba Group, Amazon Web
Services, Ant Group, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk
and VMware. WeightWatcher is a publicly-available tool distributed under Apache License 2.0 with copyright
held by Calculation Consulting. Our conclusions do not necessarily reﬂect the position or the policy of our
sponsors, and no oﬃcial endorsement should be inferred.
References
JeﬀAlstott, Ed Bullmore, and Dietmar Plenz. Powerlaw: a python package for analysis of heavy-tailed
distributions. PloS one, 9(1):e85777, 2014.
Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks.
Advances in Neural Information Processing Systems, 30:6241–6250, 2017.
Paul T Boggs and Janet E Rogers. Orthogonal distance regression. Contemporary Mathematics, 112:183–194,
1990.
Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on
statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp.
12–58, 2014.
17

Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours:
A theoretically inspired perspective. In International Conference on Learning Representations, 2020.
Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical data.
SIAM review, 51(4):661–703, 2009.
Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy
for eﬃcient language processing. Advances in neural information processing systems, 33:4271–4282, 2020.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. HAWQ: Hessian aware
quantization of neural networks with mixed-precision. In IEEE/CVF International Conference on Computer
Vision, pp. 293–302, 2019.
Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo
Wang, Ioannis Mitliagkas, and Daniel M Roy. In search of robust measures of generalization. Advances in
Neural Information Processing Systems, 33, 2020.
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 489–500,
2018.
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large margin deep
networks for classiﬁcation. Advances in Neural Information Processing Systems, 31:842–852, 2018.
Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of the
52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954–959, 2020.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
eﬃciently improving generalization. In International Conference on Learning Representations, 2020.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective.
Technical Report Preprint: arXiv:1912.02757, 2019.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In Conference on Neural Information Processing
Systems, pp. 8803–8812, 2018.
P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider optima
and better generalization. In Conference on Uncertainty in Artiﬁcial Intelligence, pp. 876–885, 2018.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap in deep
networks with margin distributions. In International Conference on Learning Representations, 2018.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization
measures and where to ﬁnd them. In International Conference on Learning Representations, 2019.
Yiding Jiang, Pierre Foret, Scott Yak, Daniel M Roy, Hossein Mobahi, Gintare Karolina Dziugaite, Samy
Bengio, Suriya Gunasekar, Isabelle Guyon, and Behnam Neyshabur. Neurips 2020 competition: Predicting
generalization in deep learning. arXiv preprint arXiv:2012.07976, 2020.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the Annual Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186,
2019.
18

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference
on Learning Representations, 2019.
Hang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre
Allauzen, Benoit Crabb´e, Laurent Besacier, and Didier Schwab. FlauBERT: Unsupervised language model
pre-training for French. In Proceedings of the 12th Language Resources and Evaluation Conference, pp.
2479–2490, 2020.
Hang Li. Deep learning for natural language processing: advantages and challenges. National Science Review,
2017.
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the diﬃculty of
training transformers. arXiv preprint arXiv:2004.08249, 2020.
Charles H Martin and Michael W Mahoney. Rethinking generalization requires revisiting old ideas: statistical
mechanics approaches and complex learning behavior. Technical Report Preprint: arXiv:1710.09553, 2017.
Charles H Martin and Michael W Mahoney. Traditional and heavy tailed self regularization in neural network
models. In International Conference on Machine Learning, pp. 4284–4293, 2019.
Charles H Martin and Michael W Mahoney. Post-mortem on a deep learning contest: a Simpson’s paradox and
the complementary roles of scale metrics versus shape metrics. Technical Report Preprint: arXiv:2106.00734,
2021a.
Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):
1–73, 2021b.
Charles H Martin, Tongsu Serena Peng, and Michael W Mahoney. Predicting trends in the quality of
state-of-the-art neural networks without access to training or testing data. Nature Communications, 12(1):
1–13, 2021.
David A McAllester. PAC-Bayesian model averaging. In Annual Conference on Computational Learning
Theory, pp. 164–170, 1999.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization in
deep learning. Advances in Neural Information Processing Systems, 32, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
In Conference on Learning Theory, pp. 1376–1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in
deep learning. Advances in Neural Information Processing Systems, 30:5947–5956, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations,
2018.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In
Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, 2018.
19

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation
of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics, pp. 311–318, 2002.
Konstantinos Pitas, Mike Davies, and Pierre Vandergheynst. Pac-bayesian margin bounds for convolutional
neural networks. Technical Report Preprint: arXiv:1801.00171, 2017.
Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
Journal of Machine Learning Research, 21(140):1–67, 2020.
Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. A simple but tough-to-beat data
augmentation approach for natural language understanding and generation. Technical Report Preprint:
arXiv:2009.13818, 2020.
Thomas Tanay and Lewis Griﬃn. A boundary tilting persepective on the phenomenon of adversarial examples.
Technical Report Preprint: arXiv:1608.07690, 2016.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the
importance of pre-training compact models. Technical Report Preprint: arXiv:1908.08962, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp.
5998–6008, 2017.
Colin Wei and Tengyu Ma. Improved sample complexities for deep neural networks and robust classiﬁcation
via an all-layer margin. In International Conference on Learning Representations, 2019.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.
38–45. Association for Computational Linguistics, October 2020.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,
Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International
Conference on Machine Learning, pp. 10524–10533. PMLR, 2020.
Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez, Kannan
Ramchandran, and Michael W Mahoney. Boundary thickness and robustness in learning models. Advances
in Neural Information Processing Systems, 33, 2020.
Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, Kannan Ramchandran, and
Michael W Mahoney. Taxonomizing local versus global structure in neural network loss landscapes. In
Thirty-Fifth Conference on Neural Information Processing Systems, 2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and William B Dolan. DIALOGPT: Large-scale generative pre-training for conversational response
generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:
System Demonstrations, pp. 270–278, 2020.
20

A
Generalization metrics
In this section, we provide deﬁnitions and details on the various metrics considered in our analysis. We begin
with scale metrics, and then consider shape metrics obtained from the ESDs of the weight matrices. Although
our focus is on generalization metrics that do not need data to evaluate, we also deﬁne generalization metrics
based on margin (Bartlett et al., 2017; Pitas et al., 2017) and PAC-Bayesian bounds (McAllester, 1999;
Neyshabur et al., 2018).
A.1
Notation and preliminaries
General notation. As before, we consider a NN with d layers and corresponding weight matrices W1,
W2,..., Wd. We use W to denote the collection of all the weights and denote the vector that consists of
all the model weights as vect(W). The neural network (as a function) is denoted by fW, taking a single
input sample x and outputs a vector fW(x). The superscript init on a weight matrix, e.g. Winit
1
, denotes the
initial weights from which the model is trained. The notation 1 means an all-one vector, and I means the
identity matrix.
Norms and distances. We use diﬀerent types of norms deﬁned on vectors and matrices. ∥· ∥2 and
∥· ∥1 used on vectors respectively means the ℓ2 norm and the ℓ1 norm. ∥· ∥F and ∥· ∥2 used on matrices
respectively denotes the Frobenius norm and the spectral norm (which is the induced ℓ2 norm).
A.2
Scale metrics
Norm-based and distance-based metrics. In the following, we discuss multiple metrics obtained from
the norms of the weights or the distance between the ﬁnal weights and those at initialization. While some
metrics are averaged over layers, and others are not, this inconsistency is not in error. We follow deﬁnitions of
metrics from several prior papers verbatim. Results in the second task (comparing model performance across
a single training run) are independent of these factors. However, to compare networks with diﬀerent sizes,
proper normalization is necessary. Some metrics across the literature are also linearly dependent on others,
and are therefore redundant for comparison. For example, log prod of spec and log sum of spec from
Jiang et al. (2019) overlap with log spectral norm from Martin & Mahoney (2021a), and log sum of fro
and log prod of fro from Jiang et al. (2019) overlap with log norm from Martin & Mahoney (2021b). These
metrics are not considered.
• (param norm). The squared Frobenius norm summed over all weight matrices.
µparam norm =
d
X
i=1
∥Wi∥2
F .
(4)
• (fro dist). The distance between a weight matrix and its initilized value, calculated using the Frobenius
norm and summed over all layers.
µfro dist =
d
X
i=1
∥Wi −Winit
i
∥2
F .
(5)
• (log norm).
µlog norm = 1
d
d
X
i=1
log ∥Wi∥2
F .
(6)
• (log spectral norm).
µlog spectral norm = 1
d
d
X
i=1
log ∥Wi∥2
2.
(7)
21

• (dist spec int).
µdist spec int =
d
X
i=1
∥Wi −Winit
i
∥2
2.
(8)
• (path norm). The metric is introduced in Neyshabur et al. (2015). To calculate the metric, we square the
parameters of the network, perform a forward pass on an all-ones input and then compute the sum of the
network outputs.
µpath norm = ∥fW2(1)∥1 .
(9)
Scale metrics that require more shape information from the ESDs. The following metrics
require more than just a single type of norm, instead involving a combination of a norm with other factors.
• (mp softrank). This metric is introduced in Martin & Mahoney (2021b). To calculate this metric, we
ﬁt the MP distribution on the ESD, obtain the bulk max of the MP distribution and then divide by the
maximum eigenvalue.
µmp softrank = 1
d
d
X
i=1
λi,MP/λi,max.
(10)
• (stable rank). The metric is a norm-adjusted measure of the scale of the ESD.
µstable rank = 1
d
d
X
i=1
∥Wi∥2
F /∥Wi∥2
2
(11)
A.3
Shape metrics
Tail-exponent ﬁtting. The following metrics are derived from heavy or light-tailed ﬁts to the ESD.
• (PL alpha). The slope of the tail of the ESD, on a log-log scale. We use MLE from Alstott et al. (2014) to
estimate PL alpha. The distribution of eigenvalues is assumed to have the form of (1).
• (E TPL lambda). The tail exponent of the E-TPL ﬁt to the ESD. This is a novel generalization metric
introduced in this work.
• (EXP lambda). The tail exponent of the EXP ﬁt to the ESD, under the assumption that the ESD follows
an exponential distribution (3). This is also a novel generalization metric introduced in this work.
• (PL ks distance). The Kolmogorov-Smirnoﬀ(KS) goodness-of-ﬁt test statistic for the PL ﬁt:
µks distance = 1
d
d
X
i=1
sup
x |F ∗
i (x) −Si(x)|,
(12)
where F ∗
i (x) is the distribution of the estimated PL ﬁt to the ESD, and Si(x) is the ESD itself.
• (E TPL ks distance). The KS test statistic for the E-TPL ﬁt, deﬁned in the same way as (12), except
that F ∗
i (x) is the distribution of the estimated E-TPL ﬁt to the ESD.
A.4
Hybrid metrics
The following metrics are scaled versions of PL alpha, involving both shape information from PL alpha and
scale information from other weighted norms. Let αi denote the estimated PL coeﬃcient of the ESD of the
i-th weight matrix Wi. Recall that λi,max is the largest eigenvalue of Wi.
• (alpha weighted). A scale-adjusted form of PL alpha. This metric is denoted as ˆα in Martin & Mahoney
(2021a,b); Martin et al. (2021).
µalpha weighted = 1
d
d
X
i=1
αi log λi,max.
(13)
22

• (log alpha norm). This metric is another scale-adjusted PL alpha metric in the form of a Schatten norm.
Recall that we let {λj}M
j=1 denote the set of eigenvalues of the correlation matrix Xi = W⊤
i Wi, where Wi
is the N-by-M weight matrix that satisﬁes N ≥M. Then, we can deﬁne the Schatten p-norm as
∥Xi∥p =


M
X
j=1
λp
j


1
p
.
(14)
The metric log alpha norm is given by
µlog alpha norm = 1
d
d
X
i=1
log ∥Xi∥αi
αi.
(15)
A.5
Margin-based metrics
Finally, we discuss generalization metrics derived from margins. Recalling that fW denotes a neural network
with weights W, for a multi-class classiﬁcation problem with sample-label pair (x, y), we deﬁne the margin as
γ(x, y, fW) = (fW(x))[y] −max
i̸=y fW(x)i.
(16)
For machine translation, we consider the margin of each output token. We note that the number of classes, or
the number of possible tokens, is often particularly large (in the order of thousands) for machine translation.
Note that margins can be deﬁned in any layer (Elsayed et al., 2018; Wei & Ma, 2019; Yang et al., 2020).
Following Jiang et al. (2019), we consider output margins only, and use the 10th percentile of the margin
distribution calculated from the entire training set as a robust surrogate for the minimum margin. Using the
margin γ deﬁned as the 10th percentile, we deﬁne several generalization metrics.
• (inverse margin).
µinverse margin = 1
γ2 .
(17)
• (log prod of spec over margin).
µlog prod of spec over margin = log
Qd
i=1 ∥Wi∥2
2
γ2
= µlog prod of spec −2 log γ.
(18)
Note that log prod of spec is not used in this paper due to overlap with log spectral norm.
• (log sum of spec over margin).
µlog sum of spec over margin
=
log d
 Qd
i=1 ∥Wi∥2
2
γ2
!1/d
=
log d + 1
d
 µlog prod of spec −2 log γ

.
(19)
• (log prod of fro over margin).
µlog prod of fro over margin = log
Qd
i=1 ∥Wi∥2
F
γ2
= µlog prod of fro −2 log γ.
(20)
Note that log prod of fro is not used in this paper due to overlap with log norm.
• (log sum of fro over margin).
µlog sum of fro over margin
=
log d
 Qd
i=1 ∥Wi∥2
F
γ2
!1/d
=
log d + 1
d
 µlog prod of fro −2 log γ

.
(21)
• (path norm over margin).
µpath norm over margin = µpath norm
γ2
.
(22)
23

A.6
Metrics derived from PAC-Bayesian bounds
Several well-known generalization bounds are derived using the PAC-Bayesian framework, which bounds
the generalization gap using the KL-divergence between a predeﬁned prior distribution (usually chosen as
Gaussian) and the posterior distribution of the trained models. A key component of the PAC-Bayesian bounds
used in most existing implementations is the procedure of searching for the largest magnitude of Gaussian
perturbation, denoted as σ, such that the perturbed weights of the neural network achieve a bounded increase
in the training loss. More speciﬁcally, σ is deﬁned such that
EU∼N(0,σ2I)[TrainLoss(fW+U)] ≤TrainLoss(fW) + δ,
(23)
where δ is a predetermined threshold, and is chosen as δ = 1
2 in our machine translation experiments. Similarly,
one can deﬁne a “magnitude-aware” perturbation σ′ satisfying
EU[TrainLoss(fW+U)] ≤TrainLoss(fW) + δ,
(24)
where each weight entry ui in U is distributed as N(0, σ′2|wi|2 + ϵ2), and ϵ is chosen as 1e-3 (Dziugaite et al.,
2020). Given the perturbation magnitude σ, the magnitude-aware perturbation σ′ and the number of samples
m, one can deﬁne the following generalization metrics.
• (pacbayes init).
µpacbayes init = µ2
l2 dist
4σ2
+ log m
σ + 10.
(25)
• (pacbayes orig).
µpacbayes orig = µ2
l2
4σ2 + log m
σ + 10.
(26)
• (pacbayes flatness).
µpacbayes flatness = 1
σ2 .
(27)
• (pacbayes mag init).
µpacbayes mag init = 1
4
ω
X
i=1
log
ϵ2 + (σ′2 + 1)µ2
l2 dist/ω
ϵ2 + σ′2|wi −winit
i
|2

+ log m
σ + 10.
(28)
• (pacbayes mag orig).
µpacbayes mag orig = 1
4
ω
X
i=1
log
 ϵ2 + (σ′2 + 1)µ2
l2/ω
ϵ2 + σ′2|wi −winit
i
|2

+ log m
σ + 10.
(29)
• (pacbayes mag flatness).
µpacbayes mag flatness = 1
σ′2 .
(30)
B
Additional details on the experiment setup
For training Transformers, we follow exactly the setup in Vaswani et al. (2017), and we develop our
implementation based on an online repository7 which reproduces the results from Vaswani et al. (2017) with
more easily conﬁgurable Transformer architectures. The architecture puts the LayerNorm before the residual
connection, which has been shown to provide more stabilized training (Liu et al., 2020; Xiong et al., 2020).
7https://github.com/gordicaleksa/pytorch-original-transformer
24

As we have mentioned earlier, we vary the hyperparameters of training to evaluate the correlations between
the generalization metrics and model quality. In the “standard setting”, we train with Transformer-base,
which has six layers, eight attention heads and embedding dimension 512. Then, we vary the number of
Transformer layers from 4 to 8, and we vary the embedding dimension from 256 to 1024. When varying the
embedding dimension, we let the number of attention heads vary proportionally.
We train with dropout 0.1 and 10% label smoothing. Note that in one experiment shown in Figure 3,
we remove dropout to observe the eﬀect of overﬁtting. For all of our experiments, we train with the inverse
square-root learning rate. Given the embedding dimension de, step number t, number of warm-up steps tw,
the formula for the inverse square-root learning rate schedule (Vaswani et al., 2017) is the following.
Learning Rate = d−0.5
e
· min(t−0.5, t · t−1.5
w
).
(31)
For results trained with a particular learning rate lr, such as the results shown in Figure 8a, lr is the constant
factor multiplied with the standard learning rate schedule (31). For each experiment, we train the model for
20 epochs. When calculating the ESDs of the weight matrices, we treat the query, key and value matrices as
separate weight matrices.
C
Additional analysis on scale metrics
In this section, we discuss an issue of computing margin-based generalization metrics. Generically, these
bounds are of the form
L(f) ≤ˆLγ(f) + C/γ
where L(f) is the population error, ˆLγ is the training margin loss at margin γ, typically
X
(x,y)∈S
1{max
j
f(x)j ≤γ + f(x)y},
and C is some complexity term. First, note that this construction requires the margin γ to be positive.
Moreover, the training margin loss is an increasing function of γ, while the complexity term C/γ is decreasing
in γ, thus the conventional way of using the margin bound is to optimize over the margin to balance two terms
in the margin bound (Bartlett et al., 2017), rather than pre-specifying the value of the margin dependent
on the data. However, we choose to follow the related papers Dziugaite et al. (2020); Jiang et al. (2019),
and we use the 10th percentile margin as a robust estimate of the minimum margin in the dataset. We use
this margin in all of the margin-normalized generalization metrics. However, in all of the experiments on
machine translation, the 10th percentile margin remains negative throughout the whole training, violating
the requirement that the bound is evaluated at a positive value of margin. See Figure 12. This problem
results from the large Alphabet for machine translation, which makes it diﬃcult to fully interpolate the data,
and hence makes the margin-normalized generalization metrics in Dziugaite et al. (2020); Jiang et al. (2019)
hard to be applicable to the present setting.
D
Fitting regression lines using orthogonal distance regression
In this section, we ﬁt the regression plots from Figure 5 to 6 using the orthogonal distance regression (Boggs
& Rogers, 1990). See Figure 13 to Figure 14.
25

0
5
10
15
20
Epochs
30
20
10
0
10 percentile margin
Margins over training
Figure 12: The margins remain negative in the experiments on machine translation due to the large
alphabet size.
0.0
0.2
0.4
0.6
0.8
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
4
6
8
10
PL_alpha
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
PL_alpha vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.04
0.06
0.08
0.10
0.12
E_TPL_ks_distance
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_ks_distance vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.0
0.2
0.4
0.6
0.8
E_TPL_lambda
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_lambda vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
4
6
8
PL_alpha
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
PL_alpha vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
0.04
0.06
0.08
0.10
0.12
E_TPL_ks_distance
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
E_TPL_ks_distance vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
Figure 13: BLEU-score versus shape metrics. (First row). Trained models grouped by the learning rate.
(Second row). Trained models grouped by the number of samples. The regression lines are ﬁtted using
orthogonal distance regression.
26

0.75
1.00
1.25
1.50
1.75
2.00
2.25
log_spectral_norm
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_spectral_norm vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
8
9
10
log_sum_of_spec_over_margin
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_sum_of_spec_over_margin vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
60
80
100
120
140
160
pacbayes_flatness
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
pacbayes_flatness vs. BLEU score
Num samples
160000
320000
640000
1280000
2560000
0.75
1.00
1.25
1.50
1.75
2.00
2.25
log_spectral_norm
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_spectral_norm vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
8
9
10
log_sum_of_spec_over_margin
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
log_sum_of_spec_over_margin vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
60
80
100
120
140
160
pacbayes_flatness
10.0
12.5
15.0
17.5
20.0
22.5
25.0
27.5
BLEU score
pacbayes_flatness vs. BLEU score
Learning rate
0.0625
0.125
0.25
0.375
0.5
0.625
0.75
1.0
Figure 14: BLEU-score versus scale metrics. (First row). Trained models grouped by the learning rate.
(Second row). Trained models grouped by the number of samples. The regression lines are ﬁtted using
orthogonal distance regression.
27

