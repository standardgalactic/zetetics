A Survey of Question Answering for Math and Science Problem
Arindam Bhattacharya
Indian Institute of Technology, Delhi
arindam@cse.iitd.ac.in
Abstract
Turing test was long considered the mea-
sure for artiﬁcial intelligence. But with the
advances in AI, it has proved to be insuf-
ﬁcient measure. We can now aim to mea-
sure machine intelligence like we measure
human intelligence. One of the widely ac-
cepted measure of intelligence is standard-
ized math and science test.
In this paper, we explore the progress we
have made towards the goal of making a
machine smart enough to pass the stan-
dardized test. We see the challenges and
opportunities posed by the domain, and
note that we are quite some ways from ac-
tually making a system as smart as a even
a middle school scholar.
1
Introduction
Humans intelligence is often measured by ask-
ing them questions and evaluating their answers.
Starting from infancy, through school, college and
career, humans are judged by evaluating answers
that they give to questions posed to them. Such
an approach is so widespread that it is possible for
someone to believe that it is the only way to mea-
sure intelligence. Yet, for long we have tried to
quantify artiﬁcial intelligence in a very different
way.
The Turing test, proposed by Alan Turing in
1950 states that: if a system can exhibit conver-
sational behaviour that is indistinguishable from
that of a human during a conversation, that system
could be considered intelligence (Turing, 1950).
Since then it has been often critisized of being a
poor measure for the purpose (Hayes and Ford,
1995). It has also since been proposed that stan-
dardized tests in mathemetics and science could
be a suitable measure for judging machine intelli-
gence (Clark and Etzioni, 2016). To this end, there
has been attempts to develop Question Answering
systems aimed at the task of solving standardized
tests of math and science problems.
In this paper, we will brieﬂy explore the various
dimesions along which attempts have been made
to tackle both math and science problems.
For
mathematics, we will explore systems that tries
to solve algebraic word problems and geometry
problems. For science, we will explore systems
that solves questions ranging from basic questions
(e.g. deﬁnitional knowledge) to questions that re-
quire complex world knowledge, and even dia-
grams.
2
Question Answering and the
Math/Science domain
Question Answering (QA) is the task of generating
or extracting an answer to a question posed in nat-
ural language. Modern QA systems can be divided
into two broad paradigms (Jurafsky and Martin,
2016). The ﬁrst paradigm, called text-based (or IR
based) QA relies on large volumes of text. Given
a question, it uses information retrieval methods
on the available corpus and retrieves documents
which contains the answer. The candidate answers
are then extracted from the text and ranked. The
second paradigm is knowledge based QA, where
we create a semantic representation of the ques-
tion and use it to query databases of facts.
Focusing on science and mathematics as do-
main for QA presents certain unique challanges.
Solving these problems require not only under-
standing question for which we levarage language
processing, but also to maintain an internal repre-
sentation of the problem and often carry out sym-
bolic computation (Clark and Etzioni, 2016). They
cannot be easily answered by information retrieval
arXiv:1705.04530v1  [cs.AI]  10 May 2017

or knowledge based methods.
Along with the challanges, the domain also pro-
vides us with certain unique oppotunities. Using
standardized test provides us with questions that
are graduated by difﬁculty and multifaceted in na-
ture: different questions explore different types of
knowledge (Schoenick et al., 2016).
3
Question Answering for Science
This section will focus on Science questions of
standardized tests. The types of quesions include
basic fact retrieval, inference and world knowlege,
and diagrams. We also explore the state of the art
in these types of quesions.
3.1
Dataset
The standardized test for Science used for the
QA tasks is the New York Regents Science Ex-
ams (NYSED). Following are examples of types
of questions in the test:
Basic Questions
These questions are factoid type questions requir-
ing deﬁnitional knowledge. Example:
1. Which object is the best conductor of elec-
tricity? (A) a wax crayon (B) a plastic spoon
(C) a rubber eraser (D) an iron nail
2. The movement of soil by wind or water is
called (A) condensation (B) evaporation (C)
erosion (D) friction
An IR based QA system can address these ques-
tions.
Simple Inference
These are questions which require simple infer-
ence over known facts to arrive at the answer. Ex-
ample:
1. Which example describes an organism taking
in nutrients? (A) dog burying a bone (B) A
girl eating an apple (C) An insect crawling
on a leaf (D) A boy planting tomatoes in the
garden
Answering this question requires knowledge that
eating involves taking in nutrients, and that an ap-
ple contains nutrients.
More Complex World Knowledge
These questions require deeper world knowledge
and more advanced liguistic capabilities for the
system to be able to understand questions and pro-
duce an answer. Example:
1. A student riding a bicycle observes that it
moves faster on a smooth road than on a
rough road. This happens because the smooth
road has (A) less gravity (B) more gravity (C)
less friction (D) more friction
To answer this question the system must be aware
of the fact that riding a bicycle implies moving it,
and infer logically the path [smooth -¿ less friction
-¿ faster movement].
Diagram
Questions with diagrams are quite common in
these test. The diagrams includes sketchs, maps,
graphs, tables etc. These often proves to be quite
difﬁcult for the QA systems. Example:
1. Which letter in the diagram 3.1 points to the
plant structure that takes in water and nutri-
ents?
Figure 1: Parts of plant.
3.2
Models
Various approaches, ranging from a combination
of Information Retrieval, Statistics and Inference
to Integer Programming are employed to tackle the
aforementioned challanges.
(Daniel Khashabi et al., 2016) proposes a
method for solving QA using Integer Linear Pro-
gramming. Given semi-structured knowledge as

tables, the QA problem is formulated as that of
ﬁnding a desirable Support Graph (Fig. 2), which
in turn is formulated as ILP.
Figure 2: TableILP searches for the best support
graph (chains of reasoning) connecting the ques-
tion to an answer, in this case June.
The state of the art system, ARISTO (Clark
et al., 2016) employs an ensemble of solvers that
tackle the problem at various layers.
The lay-
ers are: Information Retrieval solver, Pointwise
Mutual Information solver, Support Vector Ma-
chine solver, RULE solver that contains hand
coded rules, and an Integer Linear Programming
solver.
The system is shown in Fig.
3 On
non-diagram, multiple choice science questions
(NDMC), Aristo system currently scores on aver-
age 75% (4th grade), 63% (8th grade), and 41%
(12th grade) on (previously unseen) NY Regents
Science Exams (NDMC questions only, typically
4-way multiple choice). As can be seen, questions
become considerably more challenging at higher
grade levels.
On a broader multi-state collec-
tion of 4th grade NDMC questions, Aristo scores
65% (unseen questions). Note that these are the
”easier” questions (no diagrams, multiple choice);
other question types pose additional challenges as
we have described. There are no good systems that
can tackle the questions out of NDMC and no sys-
tem to date comes even close to passing a full 4th
grade science exam.
4
Question Answering for Mathematics
Math questions cannot be solved by IR systems.
The basic strategy in mathematics, especially
arithmatic quesions, is to understand the problem
and formulate an equation that can be calculated.
Geometry questions poses difﬁculties because of
their reliance on diagrams.
Figure 3:
Aristo uses ﬁve solvers, each using
different types of knowledge, to answer multiple
choice questions.
4.1
Dataset
For algebra questions, the standardized tests of
(NYSED) is used. For geometry, questions from
SATs are used.
Algebraic Problems
Algebraic problems are posed as stories that re-
quire language processions. Example:
1. Molly owns the Wafting Pie Company. This
morning, her employees used 816 eggs to
bake pumpkin pies. If her employees used a
total of 1339 eggs today, how many eggs did
they use in the afternoon?
Some of them requires world modelling as well,
for example:
1. Saras high school won 5 basketball games
this year.
They lost 3 games.
How many
games did they play in all?
2. John has 8 orange balloons, but lost 2 of
them. How many orange balloons does John
have now?
Here, for the ﬁrst, the knowledge that game is ei-
ther won or lost is needed. The second example
also has “lost” but results in a subtraction problem
rather the addition like the ﬁrst.
Geometry Problems
Geometry problems combine arithmatic and dia-
grammatic reasoning. Example: Fig.4
4.2
Models
One of the earliest attempt at solving algebraic
word problems employed simple verb categoriza-
tion (Hosseini et al., 2014). The model extracted

Figure 4: In the diagram, AB intersects circle O at
D, AC intersects circle O at E, AE = 4, AC = 24,
and AB = 16. Find AD.
the verbs from the question and tried to formulate
equations based on the verb category. The model
is presented in Fig. 6.
A more sophisticated (and current state of the
art) system, ALGES (Koncel-Kedziorski et al.,
2015) uses Integer Linear Programming to map
the word problems into equation trees. In con-
trast to (Hosseini et al., 2014), which covered only
+, −, ALGES covers +, −, ∗, /.
They are also
able to solve multi-sentence problem, unlike pre-
vious models. ALGES proceeds by extracting sets
of ground truths from the text, and using ILP to
form trees using them. Overview of the system is
presented in Fig. 5.
ALGES produces an accu-
Figure 5: An overview of the process of learning
for a word problem and its Qsets.
racy of 72%, which is a 53% error reduction over
the previous best methods (verb categorization).
Geomertry quesions poses much greater difﬁ-
Figure 6: Verb Categorization
culty, as expected, and the state of art systems can
only achieve a score of 49% in standard SAT. The
initial work in this area aimed at aligning text with
geometric diagrams (Seo et al., 2014). It achieved
the goal in three steps: identifying the primitives
in the ﬁgure by picking elements that maximized
the pixel coverage, agreement between the primi-
tives and textual elements, and maximizing the co-
herence of the elements. Fig. 7 shows the align-
ment achieved by the system.
Figure 7: Diagram understanding: identifying vi-
sual elements in the diagram and aligning them
with their textual mentions. Visual elements and
their corresponding textual mentions are color
coded. This Figure is best viewed in color.
While (Seo et al., 2014) understands the di-
agram, it could not solve geometric problems.
GeoS (Seo et al., 2015) builds on the previous
model and can answer geometric questions.
It
works in two steps: ﬁrst it uses GAligner and lan-
guage processing to convert the diagram and ques-
tion into logical expressions, and then, it uses sat-
isﬁability solver to deduce the answer. Fig. 8 sum-
marizes the model.

Figure 8: Overview of our method for solving ge-
ometry questions.
5
Conclusion
We have seen that the current state of the art is not
good at solving standardized tests (the smartest AI
could not pass high school). There is a long way
to go for AI. The ﬁeld of QA systems on standard-
ized math and science questions is fairly nascent
with room for improvment with e.g. neural mod-
els, especially for diagram based questions. But
the lack of results may also indicate that the cur-
rent trend of data driven AI may not be the one
stop solution to all problems.
We started the discussion with how Turing test
is no longer the best measure for intelligence and
agreed with (Clark and Etzioni, 2016) that stan-
dardized tests would be a better judge of intelli-
gence for machines, that is more in line with how
we measure human intelligence. But if the system
could pass the standardized test, would it be intel-
ligent? (Weston et al., 2015) argues that standard-
ized math and science tests can not test common
sense, and hence not a true test for intelligence.
Nonetheless, passing the test would be a landmark
in the history of AI, as was the moment of passing
Turing test.
References
Peter Clark and Oren Etzioni. 2016. My Computer is
an Honor Student - but how Intelligent is it? Stan-
dardized Tests as a Measure of AI. AI Magazine .
Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sabhar-
wal, Oyvind Tafjord, and Peter Turney. 2016. Com-
bining Retrieval, Statistics, and Inference to Answer
Elementary Science Questions. Association for the
Advancement of Artiﬁcial Intelligence .
Tushar Khot Daniel Khashabi, Ashish Sabharwal, Pe-
ter Clark, Oren Etzioni, and Dan Roth. 2016. Ques-
tion Answering via Integer Programming over Semi-
Structured Knowledge. IJCAI .
P. Hayes and K. Ford. 1995. Turing Test Considered
Harmful. IJCAI .
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to Solve Arithmetic Word Problems with Verb Cate-
gorization. ACL .
Daniel Jurafsky and James H. Martin. 2016. Speech
and Language Processing.
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
Sabharwal, Oren Etzioni, and Siena Dumas Ang.
2015. Parsing Algebraic Word Problems into Equa-
tions. ACL .
NYSED. 2014.
New York Regents Science Exams.
http://www. nysedregents.org/.
Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter
Turney, and Oren Etzioni. 2016. Moving Beyond the
Turing Test with the Allen AI Science Challenge.
Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and
Oren Etzioni. 2014. Diagram Understanding in Ge-
ometry Questions. AAAI .
Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren
Etzioni, and Clint Malcolm. 2015. Solving Geom-
etry Problems: Combining Text and Diagram Inter-
pretation. EMNLP .
Alan M. Turing. 1950. Computing Machinery and In-
telligence..
Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merri¨enboer, Armand Joulin,
and Tomas Mikolov. 2015.
Towards AI-complete
Question Answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698 .

