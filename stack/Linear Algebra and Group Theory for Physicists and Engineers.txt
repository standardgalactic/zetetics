Linear Algebra 
and Group 
Theory for 
Physicists and 
Engineers
Yair Shapira


Yair Shapira
Linear Algebra and Group
Theory for Physicists
and Engineers

Yair Shapira
Department of Computer Science
Technion, Israel Institute of Technology
Haifa, Israel
ISBN 978-3-030-17855-0
ISBN 978-3-030-17856-7
(eBook)
https://doi.org/10.1007/978-3-030-17856-7
Mathematics Subject Classiﬁcation (2010): 15-xx, 15Axx, 39B22, 15A60, 60J10
© Springer Nature Switzerland AG 2019
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afﬁliations.
This book is published under the imprint Birkhäuser, www.birkhauser-science.com by the registered
company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This book introduces two closely related subjects: linear algebra and group theory
for undergrads in engineering, physics, chemistry, and (applied) math. This is
indeed an interdisciplinary point of view: math and its applications go hand in hand.
The linear algebra part introduces both vectors and matrices, with a lot of
examples in two and three dimensions: small 2  2 Lorentz matrix, and 3  3
rotation matrix. This prepares the reader quite well to the group theory stuff: 2  2
Moebius and Pauli matrices, 3  3 projective matrices, and so on. This way, the
reader gets ready to higher dimensions as well: big Fourier and Markov matrices,
operators in quantum mechanics, and stiffness and mass matrices in (high-order)
ﬁnite elements.
It makes sense to place the matrices in a new group. This may help mirror (or
represent) many other groups as well. This is how linear algebra paves the way to
group theory. Thanks to the language of matrices, groups become much more
concrete, and easy to store on the computer.
Thus, the book offers a unique approach: it introduces both linear and modern
algebra at the same time. This shows quite clearly how related these topics really
are, and how they can beneﬁt from each other, and complete each other. Indeed, at
the end of the book, we work the other way around: group theory paves the way to
linear algebra to uncover the electronic structure in the atom.
How to Use the Book in Academic Courses?
The book could be used as a textbook in undergraduate courses:
• linear algebra for physicists and engineers (Chaps. 1–4 and 7),
• group theory and its geometrical applications (Chaps. 5–6),
• special relativity—algebraic point of view (Chaps. 1, 4, and 15),
• quantum mechanics—algebraic point of view (Chaps. 1, 7, and 14),
• high-order ﬁnite elements in 3-D (Chaps. 8–13).
v

Indeed, Chaps. 1–4 introduce linear algebra, with applications in mechanics and
statistics. Chapters 5–6, on the other hand, introduce group theory, with applica-
tions in projective geometry. Furthermore, Chaps. 8–13 introduce high-order ﬁnite
elements to design a regular mesh (and an optimal spline) in a complicated 3-D
domain. Finally, Chaps. 14–15 assemble the stiffness and mass matrices in
advanced applications in quantum chemistry and general relativity.
The book is nearly self-contained: the only prerequisite is elementary calculus,
which could be attended concurrently with these courses. There are plenty of
examples and ﬁgures to make the material more visual and friendly. All ﬁgures are
referenced in the text.
Each chapter ends with a lot of relevant exercises with hints or even solutions.
This may help the reader follow the theory and develop new results on his/her own.
Roadmaps: How to Read the Book?
Here are a few different ways to read the book. They are illustrated in a few
roadmaps (Figs. 1–3):
• Physicists, chemists, and engineers might want to
– read Chaps. 1–2 about linear algebra, with applications in mechanics.
– Then, proceed to Chap. 4, where matrices are used to introduce special
relativity.
– Finally, conclude with more advanced applications: Chaps. 7 and 14 about
quantum mechanics, and Chap. 15 about general relativity.
• Computer scientists, on the other hand, might want to
– start from Chap. 1 about linear algebra.
– Then, proceed to Chap. 3, which uses a Markov matrix to design a search
engine.
physicist/chemist/engineer
Chapters 1–2: linear algebra
Chapter 4: special relativity
Chapters 7 and 14: quantum mechanics
Chapter 15: general relativity
Fig. 1 How could a physicist/chemist/engineer read the book?
vi
Preface

– Finally, conclude with Chap. 5 about group theory, and Chap. 6 that uses it
in computer graphics.
• Finally, numerical analysts and engineers could also
– start from Chaps. 1–2 to get introduced to linear algebra.
– Then, skip to Chaps. 8–13 about ﬁnite elements, meshes, and splines.
– Finally, conclude with Chaps. 14–15 that assemble the stiffness and mass
matrices in advanced physical systems.
Haifa, Israel
Yair Shapira
computer scientist
Chapter 1: linear algebra
Chapter 3: search engines
Chapter 5: group theory
Chapter 6: applications
in computer graphics
Fig. 2 How could a computer scientist read the book?
numerical analyst/engineer
Chapters 1–2: linear algebra
Chapters 8–12: ﬁnite elements in 3-D
Chapter 13: Splines in 3-D
Chapters 14–15: advanced
applications in physics
Fig. 3 How could a numerical analyst or an engineer read the book?
Preface
vii

Contents
Part I
Introduction to Linear Algebra
1
Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Vectors in Two and Three Dimensions . . . . . . . . . . . . . . . . . .
4
1.1.1
Two-Dimensional Vectors . . . . . . . . . . . . . . . . . . . .
4
1.1.2
Adding Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.1.3
Scalar Times Vector . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.4
Three-Dimensional Vectors . . . . . . . . . . . . . . . . . . .
6
1.2
Vectors in Higher Dimensions . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
Multidimensional Vectors . . . . . . . . . . . . . . . . . . . .
7
1.2.2
Associative Law . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.3
The Origin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.4
Multiplication and Its Laws . . . . . . . . . . . . . . . . . . .
8
1.2.5
Distributive Laws . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.3
Complex Numbers and Vectors . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.1
Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.2
Complex Vectors . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4
Rectangular Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.1
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.2
Adding Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.4.3
Scalar-Times-Matrix . . . . . . . . . . . . . . . . . . . . . . . .
13
1.4.4
Matrix-Times Vector . . . . . . . . . . . . . . . . . . . . . . . .
13
1.4.5
Matrix-Times-Matrix . . . . . . . . . . . . . . . . . . . . . . . .
15
1.4.6
Distributive and Associative Laws . . . . . . . . . . . . . .
15
1.4.7
The Transpose Matrix . . . . . . . . . . . . . . . . . . . . . . .
17
1.5
Square Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.5.1
Symmetric Square Matrix . . . . . . . . . . . . . . . . . . . .
18
1.5.2
The Identity Matrix . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.5.3
The Inverse Matrix as a Mapping . . . . . . . . . . . . . .
19
1.5.4
Inverse and Transpose . . . . . . . . . . . . . . . . . . . . . . .
20
ix

1.6
The Hermitian Adjoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.6.1
Complex Matrix and Its Hermitian Adjoint . . . . . . .
21
1.6.2
Hermitian (Self-Adjoint) Matrix . . . . . . . . . . . . . . . .
22
1.7
Inner Product and Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.7.1
Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.7.2
Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.7.3
Inner Product and the Hermitian Adjoint . . . . . . . . .
24
1.7.4
Inner Product and Hermitian Matrix. . . . . . . . . . . . .
25
1.8
Orthogonal and Unitary Matrix . . . . . . . . . . . . . . . . . . . . . . .
26
1.8.1
Inner Product of Column Vectors . . . . . . . . . . . . . .
26
1.8.2
Orthogonal and Orthonormal Column Vectors . . . . .
26
1.8.3
Projection Matrix . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.8.4
Unitary and Orthogonal Matrix . . . . . . . . . . . . . . . .
28
1.9
Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . .
29
1.9.1
Eigenvectors and Eigenvalues . . . . . . . . . . . . . . . . .
29
1.9.2
Singular Matrix and Its Null Space . . . . . . . . . . . . .
29
1.9.3
Eigenvalues of the Hermitian Adjoint . . . . . . . . . . .
30
1.9.4
Eigenvalues of a Hermitian Matrix . . . . . . . . . . . . .
30
1.9.5
Eigenvectors of a Hermitian Matrix . . . . . . . . . . . . .
31
1.10
The Sine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
1.10.1
Discrete Sine Wavesx . . . . . . . . . . . . . . . . . . . . . . .
31
1.10.2
Orthogonality of the Discrete Sine Waves . . . . . . . .
33
1.10.3
The Sine Transform . . . . . . . . . . . . . . . . . . . . . . . .
34
1.10.4
Diagonalization. . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
1.10.5
Sine Decomposition . . . . . . . . . . . . . . . . . . . . . . . .
35
1.10.6
Multiscale Decomposition . . . . . . . . . . . . . . . . . . . .
35
1.11
The Cosine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
1.11.1
Discrete Cosine Waves . . . . . . . . . . . . . . . . . . . . . .
36
1.11.2
Orthogonality of the Discrete Cosine Waves . . . . . .
36
1.11.3
The Cosine Transform . . . . . . . . . . . . . . . . . . . . . .
38
1.11.4
Diagonalization. . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
1.11.5
Cosine Decomposition . . . . . . . . . . . . . . . . . . . . . .
38
1.12
Positive (Semi)Deﬁnite Matrix . . . . . . . . . . . . . . . . . . . . . . . .
39
1.12.1
Positive Semideﬁnite Matrix . . . . . . . . . . . . . . . . . .
39
1.12.2
Positive Deﬁnite Matrix . . . . . . . . . . . . . . . . . . . . .
39
1.13
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
1.13.1
The Cauchy–Schwarz Inequality . . . . . . . . . . . . . . .
40
1.13.2
Generalized Eigenvalues and Eigenvectors . . . . . . . .
41
1.13.3
Root of Unity and Fourier Transform. . . . . . . . . . . .
42
x
Contents

2
Vector Product with Applications in Geometrical Mechanics . . . . .
51
2.1
The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.1.1
Minors and the Determinant . . . . . . . . . . . . . . . . . .
51
2.1.2
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2.1.3
Algebraic Properties . . . . . . . . . . . . . . . . . . . . . . . .
53
2.1.4
The Inverse Matrix in Its Explicit Form . . . . . . . . . .
54
2.1.5
Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.2
Vector Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.2.1
Standard Unit Vectors in 3-D . . . . . . . . . . . . . . . . .
56
2.2.2
Inner Product—Orthogonal Projection . . . . . . . . . . .
57
2.2.3
Vector Product . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.2.4
The Right-Hand Rule . . . . . . . . . . . . . . . . . . . . . . .
59
2.3
Orthogonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.3.1
Invariance Under Orthogonal Transformation . . . . . .
62
2.3.2
Relative Axis System: Gram–Schmidt Process . . . . .
63
2.3.3
Angle Between Vectors . . . . . . . . . . . . . . . . . . . . . .
65
2.4
Linear and Angular Momentum . . . . . . . . . . . . . . . . . . . . . . .
66
2.4.1
Linear Momentum . . . . . . . . . . . . . . . . . . . . . . . . .
66
2.4.2
Angular Momentum . . . . . . . . . . . . . . . . . . . . . . . .
67
2.5
Angular Velocity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.5.1
Angular Velocity . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.5.2
The Rotating Axis System . . . . . . . . . . . . . . . . . . . .
70
2.5.3
Velocity and Its Decomposition . . . . . . . . . . . . . . . .
70
2.6
Real and Fictitious Forces . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.6.1
The Centrifugal Force . . . . . . . . . . . . . . . . . . . . . . .
71
2.6.2
The Centripetal Force . . . . . . . . . . . . . . . . . . . . . . .
72
2.6.3
Euler Force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
2.6.4
The Earth and Its Rotation . . . . . . . . . . . . . . . . . . .
74
2.6.5
Coriolis Force. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
2.7.1
Rotation and Euler Angles . . . . . . . . . . . . . . . . . . .
77
2.7.2
Principal Axes . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
2.7.3
The Inertia Matrix . . . . . . . . . . . . . . . . . . . . . . . . .
82
2.7.4
Triple Vector Product . . . . . . . . . . . . . . . . . . . . . . .
83
2.7.5
Conservation of Angular Momentum . . . . . . . . . . . .
85
3
Markov Chain in a Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
3.1
Characteristic Polynomial and Spectrum . . . . . . . . . . . . . . . . .
89
3.1.1
Null Space and Characteristic Polynomial . . . . . . . .
89
3.1.2
Spectrum and Spectral Radius . . . . . . . . . . . . . . . . .
90
Contents
xi

3.2
Graph and Its Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3.2.1
Weighted Graph . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3.2.2
Markov Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3.2.3
Example: Uniform Probability . . . . . . . . . . . . . . . . .
92
3.3
Flow and Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
3.3.1
Stochastic Flow: From State to State . . . . . . . . . . . .
92
3.3.2
Mass Conservation . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.4
The Steady State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.4.1
The Spectrum of Markov Matrix . . . . . . . . . . . . . . .
94
3.4.2
Converging Markov Chain . . . . . . . . . . . . . . . . . . .
95
3.4.3
The Steady State. . . . . . . . . . . . . . . . . . . . . . . . . . .
96
3.4.4
Search Engine in the Internet. . . . . . . . . . . . . . . . . .
97
3.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
3.5.1
Gersgorin’s Theorem . . . . . . . . . . . . . . . . . . . . . . .
98
4
Special Relativity: Algebraic Point of View . . . . . . . . . . . . . . . . . . .
103
4.1
Systems and Their Time . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
4.1.1
How to Add Velocities? . . . . . . . . . . . . . . . . . . . . .
104
4.1.2
Never Exceed the Speed of Light! . . . . . . . . . . . . . .
105
4.1.3
How to Measure Time? . . . . . . . . . . . . . . . . . . . . . .
106
4.1.4
The Self-System . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
4.2
Lorentz Transformation and Matrix . . . . . . . . . . . . . . . . . . . .
107
4.2.1
Lorentz Transformation . . . . . . . . . . . . . . . . . . . . . .
107
4.2.2
Lorentz Matrix and the Inﬁnity Point . . . . . . . . . . . .
108
4.2.3
Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
4.2.4
Composition of Lorentz Transformations . . . . . . . . .
110
4.2.5
The Inverse Transformation . . . . . . . . . . . . . . . . . . .
111
4.3
Proper Time in the Self-System . . . . . . . . . . . . . . . . . . . . . . .
112
4.3.1
Proper Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
4.3.2
The Twin Paradox . . . . . . . . . . . . . . . . . . . . . . . . .
114
4.3.3
Hyperbolic Geometry: Minkowski Space . . . . . . . . .
114
4.3.4
Length Contraction . . . . . . . . . . . . . . . . . . . . . . . . .
115
4.3.5
Simultaneous Events . . . . . . . . . . . . . . . . . . . . . . . .
117
4.3.6
Time Dilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
4.4
Velocity and Slope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
4.4.1
Doppler’s Effect . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
4.4.2
Slope: Moebius Transformation . . . . . . . . . . . . . . . .
119
4.4.3
Perpendicular Velocity . . . . . . . . . . . . . . . . . . . . . .
120
4.5
Momentum and Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
4.5.1
Conservation of Momentum . . . . . . . . . . . . . . . . . .
122
4.5.2
Relative Energy . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
4.5.3
Energy Is Conserved—Mass Is Not . . . . . . . . . . . . .
126
4.5.4
Lorentz Transformation on Momentum–Energy . . . .
127
xii
Contents

4.6
Energy and Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
4.6.1
Absolute Nuclear Energy . . . . . . . . . . . . . . . . . . . .
129
4.6.2
Invariant Rest Mass . . . . . . . . . . . . . . . . . . . . . . . .
130
4.7
Center of Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
4.7.1
Collection of Subparticles . . . . . . . . . . . . . . . . . . . .
130
4.7.2
Center of Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
4.7.3
Rest Mass of the Collection . . . . . . . . . . . . . . . . . .
132
4.8
Force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
4.8.1
Passive System—Strong Perpendicular Force . . . . . .
132
4.8.2
Photon: A New Universe? . . . . . . . . . . . . . . . . . . . .
137
4.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
4.9.1
Motion in Three Dimensions . . . . . . . . . . . . . . . . . .
137
Part II
Introduction to Group Theory
5
Group Representation and Isomorphism Theorems . . . . . . . . . . . .
145
5.1
Moebius Transformation and Matrix. . . . . . . . . . . . . . . . . . . .
146
5.1.1
Riemann Sphere—Extended Complex Plane . . . . . . .
146
5.1.2
Moebius Transformation and the Inﬁnity Point . . . . .
146
5.1.3
The Inverse Transformation . . . . . . . . . . . . . . . . . . .
147
5.1.4
Moebius Transformation as a Matrix . . . . . . . . . . . .
148
5.1.5
Product of Moebius Transformations . . . . . . . . . . . .
149
5.2
Matrix: A Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
5.2.1
Matrix as a Vector Function . . . . . . . . . . . . . . . . . .
150
5.2.2
Matrix Multiplication as Composition . . . . . . . . . . .
150
5.3
Group and Its Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
5.3.1
Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
5.3.2
The Unit Element . . . . . . . . . . . . . . . . . . . . . . . . . .
151
5.3.3
Inverse Element . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.4
Mapping and Homomorphism . . . . . . . . . . . . . . . . . . . . . . . .
153
5.4.1
Mapping and Its Origin . . . . . . . . . . . . . . . . . . . . . .
153
5.4.2
Homomorphism . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
5.4.3
Mapping the Unit Element . . . . . . . . . . . . . . . . . . .
155
5.4.4
Preserving the Inverse Operation . . . . . . . . . . . . . . .
155
5.4.5
Kernel of a Mapping. . . . . . . . . . . . . . . . . . . . . . . .
156
5.5
The Center and Kernel Subgroups . . . . . . . . . . . . . . . . . . . . .
157
5.5.1
Subgroup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
5.5.2
The Center Subgroup . . . . . . . . . . . . . . . . . . . . . . .
158
5.5.3
The Kernel Subgroup . . . . . . . . . . . . . . . . . . . . . . .
158
5.6
Equivalence Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
5.6.1
Equivalence Relation in a Set . . . . . . . . . . . . . . . . .
159
5.6.2
Decomposition into Equivalence Classes . . . . . . . . .
160
5.6.3
Family of Equivalence Classes . . . . . . . . . . . . . . . .
160
Contents
xiii

5.6.4
Equivalence Relation Induced by a Subgroup . . . . . .
161
5.6.5
Equivalence Classes Induced by a Subgroup . . . . . .
162
5.7
The Factor Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
5.7.1
The New Set G=S . . . . . . . . . . . . . . . . . . . . . . . . . .
162
5.7.2
Normal Subgroup . . . . . . . . . . . . . . . . . . . . . . . . . .
163
5.7.3
The Factor (Quotient) Group . . . . . . . . . . . . . . . . . .
164
5.7.4
Isomorphism. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
5.7.5
The Fundamental Theorem of Homomorphism . . . . .
166
5.8
Geometrical Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
5.8.1
Application in Moebius Transformations . . . . . . . . .
167
5.8.2
Two-Dimensional Vector Set . . . . . . . . . . . . . . . . . .
168
5.8.3
Geometrical Decomposition into Planes . . . . . . . . . .
169
5.8.4
Family of Planes . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
5.8.5
Action of Factor Group . . . . . . . . . . . . . . . . . . . . . .
170
5.8.6
Composition of Functions . . . . . . . . . . . . . . . . . . . .
170
5.8.7
Oblique Projection: Extended Cotangent . . . . . . . . .
171
5.8.8
Homomorphism Onto Moebius Transformations . . . .
172
5.8.9
The Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
5.8.10
Eigenvectors and Fixed Points . . . . . . . . . . . . . . . . .
175
5.8.11
Isomorphism Onto Moebius Transformations . . . . . .
175
5.9
Application in Continued Fractions . . . . . . . . . . . . . . . . . . . .
176
5.9.1
Continued Fractions . . . . . . . . . . . . . . . . . . . . . . . .
176
5.9.2
Algebraic Formulation . . . . . . . . . . . . . . . . . . . . . .
177
5.9.3
The Approximants . . . . . . . . . . . . . . . . . . . . . . . . .
177
5.9.4
Algebraic Convergence . . . . . . . . . . . . . . . . . . . . . .
178
5.10
Isomorphism Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.10.1
The Second Isomorphism Theorem . . . . . . . . . . . . .
179
5.10.2
The Third Isomorphism Theorem. . . . . . . . . . . . . . .
180
5.11
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
6
Projective Geometry with Applications in Computer Graphics . . . .
187
6.1
Circles and Spheres . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
6.1.1
Degenerate “Circle” . . . . . . . . . . . . . . . . . . . . . . . .
188
6.1.2
Antipodal Points in the Unit Circle . . . . . . . . . . . . .
189
6.1.3
More Circles . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
6.1.4
Antipodal Points in the Unit Sphere . . . . . . . . . . . . .
190
6.1.5
General Multidimensional Hypersphere . . . . . . . . . .
191
6.1.6
Complex Coordinates . . . . . . . . . . . . . . . . . . . . . . .
191
6.2
The Complex Projective Plane . . . . . . . . . . . . . . . . . . . . . . . .
192
6.2.1
The Complex Projective Plane . . . . . . . . . . . . . . . . .
192
6.2.2
Topological Homeomorphism onto the Sphere . . . . .
193
6.2.3
The Center and Its Subgroups . . . . . . . . . . . . . . . . .
194
6.2.4
Group Product . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
xiv
Contents

6.2.5
The Center—A Group Product . . . . . . . . . . . . . . . .
195
6.2.6
How to Divide by a Product? . . . . . . . . . . . . . . . . .
195
6.2.7
How to Divide by a Circle? . . . . . . . . . . . . . . . . . .
196
6.2.8
Second and Third Isomorphism Theorems . . . . . . . .
197
6.3
The Real Projective Line . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
6.3.1
The Real Projective Line . . . . . . . . . . . . . . . . . . . . .
199
6.3.2
The Divided Circle . . . . . . . . . . . . . . . . . . . . . . . . .
200
6.4
The Real Projective Plane . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
6.4.1
The Real Projective Plane . . . . . . . . . . . . . . . . . . . .
201
6.4.2
Oblique Projection . . . . . . . . . . . . . . . . . . . . . . . . .
202
6.4.3
Radial Projection . . . . . . . . . . . . . . . . . . . . . . . . . .
203
6.4.4
The Divided Sphere . . . . . . . . . . . . . . . . . . . . . . . .
204
6.4.5
Inﬁnity Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
6.4.6
The Inﬁnity Circle . . . . . . . . . . . . . . . . . . . . . . . . .
205
6.4.7
Lines as Level Sets . . . . . . . . . . . . . . . . . . . . . . . . .
205
6.5
Inﬁnity Points and Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
6.5.1
Inﬁnity Points and Their Projection . . . . . . . . . . . . .
207
6.5.2
Riemannian Geometry . . . . . . . . . . . . . . . . . . . . . . .
207
6.5.3
A Joint Inﬁnity Point . . . . . . . . . . . . . . . . . . . . . . .
208
6.5.4
Two Lines Share a Unique Point . . . . . . . . . . . . . . .
208
6.5.5
Parallel Lines Do Meet . . . . . . . . . . . . . . . . . . . . . .
209
6.5.6
The Inﬁnity Line . . . . . . . . . . . . . . . . . . . . . . . . . .
209
6.5.7
Duality: Two Points Make a Unique Line . . . . . . . .
210
6.6
Conics and Envelopes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
6.6.1
Conic as a Level Set . . . . . . . . . . . . . . . . . . . . . . . .
211
6.6.2
New Axis System . . . . . . . . . . . . . . . . . . . . . . . . . .
211
6.6.3
The Projected Conic . . . . . . . . . . . . . . . . . . . . . . . .
212
6.6.4
Ellipse, Hyperbola, or Parabola . . . . . . . . . . . . . . . .
212
6.6.5
Tangent Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
6.6.6
Envelope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
6.6.7
The Inverse Mapping . . . . . . . . . . . . . . . . . . . . . . .
214
6.7
Duality: Conic–Envelope . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
6.7.1
Conic and Its Envelope . . . . . . . . . . . . . . . . . . . . . .
215
6.7.2
Hyperboloid and Its Projection . . . . . . . . . . . . . . . .
215
6.7.3
Projective Mappings . . . . . . . . . . . . . . . . . . . . . . . .
218
6.8
Applications in Computer Graphics . . . . . . . . . . . . . . . . . . . .
218
6.8.1
Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
6.8.2
Motion in a Curved Trajectory . . . . . . . . . . . . . . . .
219
6.8.3
The Translation Matrix . . . . . . . . . . . . . . . . . . . . . .
219
6.8.4
General Translation of a Planar Object . . . . . . . . . . .
220
6.8.5
Unavailable Tangent . . . . . . . . . . . . . . . . . . . . . . . .
220
Contents
xv

6.8.6
Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
6.8.7
Relation to the Complex Projective Plane . . . . . . . . .
223
6.9
The Real Projective Space . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
6.9.1
The Real Projective Space . . . . . . . . . . . . . . . . . . . .
224
6.9.2
Oblique Projection . . . . . . . . . . . . . . . . . . . . . . . . .
224
6.9.3
Radial Projection . . . . . . . . . . . . . . . . . . . . . . . . . .
225
6.10
Duality: Point–Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
6.10.1
Points and Planes . . . . . . . . . . . . . . . . . . . . . . . . . .
225
6.10.2
The Extended Vector Product . . . . . . . . . . . . . . . . .
226
6.10.3
Three Points Make a Unique Plane . . . . . . . . . . . . .
227
6.10.4
Three Planes Share a Unique Point . . . . . . . . . . . . .
227
6.11
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
7
Quantum Mechanics: Algebraic Point of View . . . . . . . . . . . . . . . .
231
7.1
Nondeterminism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
7.1.1
Relative Observation . . . . . . . . . . . . . . . . . . . . . . . .
231
7.1.2
Determinism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
7.1.3
Nondeterminism: Observables . . . . . . . . . . . . . . . . .
233
7.2
State—Wave Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
7.2.1
Physical State . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
7.2.2
The Position Matrix . . . . . . . . . . . . . . . . . . . . . . . .
234
7.2.3
Dynamics: Schrodinger Picture . . . . . . . . . . . . . . . .
234
7.2.4
Wave Function and Phase . . . . . . . . . . . . . . . . . . . .
235
7.2.5
Superposition and Interference . . . . . . . . . . . . . . . . .
235
7.3
Observables Don’t Commute! . . . . . . . . . . . . . . . . . . . . . . . .
236
7.3.1
Don’t Look! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
7.3.2
The Momentum Matrix and Its Eigenvalues . . . . . . .
236
7.3.3
Order Matters! . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
7.3.4
Commutator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
7.3.5
Planck Constant . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
7.4
Observable and Its Expectation . . . . . . . . . . . . . . . . . . . . . . .
239
7.4.1
Observable or Measurable . . . . . . . . . . . . . . . . . . . .
239
7.4.2
Symmetrization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
7.4.3
Observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
7.4.4
Random Variable and Its Expectation . . . . . . . . . . .
240
7.5
Heisenberg’s Uncertainty Principle . . . . . . . . . . . . . . . . . . . . .
241
7.5.1
Variance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
7.5.2
Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
7.5.3
Heisenberg’s Uncertainty Principle . . . . . . . . . . . . .
242
7.6
Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
7.6.1
Shifting an Eigenvalue . . . . . . . . . . . . . . . . . . . . . .
242
7.6.2
Shifting an Eigenvalue of a Product . . . . . . . . . . . . .
243
7.6.3
A Number Operator . . . . . . . . . . . . . . . . . . . . . . . .
244
xvi
Contents

7.6.4
Eigenvalue—Expectation . . . . . . . . . . . . . . . . . . . . .
244
7.6.5
Ladder Operator: Lowering an Eigenvalue . . . . . . . .
245
7.6.6
Null Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
7.6.7
Raising an Eigenvalue. . . . . . . . . . . . . . . . . . . . . . .
246
7.7
Hamiltonian and Its Eigenvalues . . . . . . . . . . . . . . . . . . . . . .
246
7.7.1
Hamiltonian of the Harmonic Oscillator . . . . . . . . . .
246
7.7.2
Concrete Number Operator . . . . . . . . . . . . . . . . . . .
247
7.7.3
Energy Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . .
248
7.7.4
Ground State . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
248
7.7.5
Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . . .
249
7.8
Coherent States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
7.8.1
Dynamic State . . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
7.8.2
Coherent State . . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
7.8.3
Coherent State—Nondeterministic Energy . . . . . . . .
251
7.8.4
Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . .
252
7.9
Particle in Three Dimensions . . . . . . . . . . . . . . . . . . . . . . . . .
252
7.9.1
Tensor Product . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
7.9.2
Commutativity . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
7.9.3
Three-Dimensional Grid . . . . . . . . . . . . . . . . . . . . .
255
7.10
Angular Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
7.10.1
Angular Momentum Component . . . . . . . . . . . . . . .
256
7.10.2
Using the Commutator . . . . . . . . . . . . . . . . . . . . . .
256
7.10.3
Ladder Operator: Raising an Eigenvalue . . . . . . . . .
257
7.10.4
Lowering an Eigenvalue . . . . . . . . . . . . . . . . . . . . .
257
7.10.5
Angular Momentum . . . . . . . . . . . . . . . . . . . . . . . .
258
7.11
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
7.11.1
Eigenvalues and Energy Levels . . . . . . . . . . . . . . . .
258
7.11.2
Spin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
260
7.11.3
Pauli Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
7.11.4
Polarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
Part III
Polynomials and Basis Functions
8
Polynomials and Their Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
8.1
Polynomial of One Variable . . . . . . . . . . . . . . . . . . . . . . . . .
267
8.1.1
Polynomial of One Variable . . . . . . . . . . . . . . . . . .
267
8.1.2
Adding Polynomials . . . . . . . . . . . . . . . . . . . . . . . .
268
8.1.3
Multiplying a Polynomial by a Scalar . . . . . . . . . . .
269
8.1.4
Multiplying Polynomials . . . . . . . . . . . . . . . . . . . . .
269
8.2
Horner’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
8.2.1
Computing the Value of a Polynomial . . . . . . . . . . .
271
8.2.2
Introducing Parentheses . . . . . . . . . . . . . . . . . . . . . .
272
8.2.3
Horner’s Algorithm. . . . . . . . . . . . . . . . . . . . . . . . .
272
Contents
xvii

8.2.4
Efﬁciency of Horner’s Algorithm . . . . . . . . . . . . . . .
273
8.2.5
Composition of Polynomials . . . . . . . . . . . . . . . . . .
273
8.3
Decimal and Binary Numbers . . . . . . . . . . . . . . . . . . . . . . . .
274
8.3.1
Natural Number as a Polynomial . . . . . . . . . . . . . . .
274
8.3.2
Binary Polynomial . . . . . . . . . . . . . . . . . . . . . . . . .
274
8.4
Implicit Horner Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
8.4.1
Monomial: Individual Power . . . . . . . . . . . . . . . . . .
275
8.5
Differentiation and Integration . . . . . . . . . . . . . . . . . . . . . . . .
276
8.5.1
Derivative of a Polynomial . . . . . . . . . . . . . . . . . . .
276
8.5.2
Indeﬁnite Integral . . . . . . . . . . . . . . . . . . . . . . . . . .
277
8.5.3
Deﬁnite Integral over an Interval . . . . . . . . . . . . . . .
278
8.6
Polynomial of Two Variables . . . . . . . . . . . . . . . . . . . . . . . .
279
8.6.1
Polynomial of Two Independent Variables . . . . . . . .
279
8.6.2
Arithmetic Operations . . . . . . . . . . . . . . . . . . . . . . .
279
8.7
Differentiation and Integration . . . . . . . . . . . . . . . . . . . . . . . .
280
8.7.1
Partial Derivatives. . . . . . . . . . . . . . . . . . . . . . . . . .
280
8.7.2
The Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
8.7.3
Integral over the Unit Triangle . . . . . . . . . . . . . . . .
281
8.7.4
Second Partial Derivatives . . . . . . . . . . . . . . . . . . . .
283
8.7.5
Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
8.8
Polynomial of Three Variables . . . . . . . . . . . . . . . . . . . . . . . .
285
8.8.1
Polynomial of Three Independent Variables . . . . . . .
285
8.9
Differentiation and Integration . . . . . . . . . . . . . . . . . . . . . . . .
286
8.9.1
Partial Derivatives. . . . . . . . . . . . . . . . . . . . . . . . . .
286
8.9.2
The Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
8.9.3
Vector Field (or Function) . . . . . . . . . . . . . . . . . . . .
287
8.9.4
The Jacobian . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
8.9.5
Integral over the Unit Tetrahedron . . . . . . . . . . . . . .
288
8.10
Normal and Tangential Derivatives . . . . . . . . . . . . . . . . . . . .
289
8.10.1
Directional Derivative . . . . . . . . . . . . . . . . . . . . . . .
289
8.10.2
Normal Derivative . . . . . . . . . . . . . . . . . . . . . . . . .
290
8.10.3
Differential Operator . . . . . . . . . . . . . . . . . . . . . . . .
291
8.10.4
High-Order Normal Derivatives . . . . . . . . . . . . . . . .
291
8.10.5
Tangential Derivative . . . . . . . . . . . . . . . . . . . . . . .
292
8.11
High-Order Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . .
292
8.11.1
High-Order Partial Derivatives . . . . . . . . . . . . . . . . .
292
8.11.2
The Hessian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
8.11.3
Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
8.12
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
8.12.1
Taylor Series of Sine and Cosine . . . . . . . . . . . . . . .
295
xviii
Contents

9
Basis Functions: Barycentric Coordinates in 3-D . . . . . . . . . . . . . .
299
9.1
Tetrahedron and Its Mapping . . . . . . . . . . . . . . . . . . . . . . . . .
299
9.1.1
General Tetrahedron . . . . . . . . . . . . . . . . . . . . . . . .
299
9.1.2
Integral over a Tetrahedron . . . . . . . . . . . . . . . . . . .
301
9.1.3
The Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
9.1.4
Degrees of Freedom . . . . . . . . . . . . . . . . . . . . . . . .
302
9.2
Barycentric Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304
9.2.1
Barycentric Coordinates . . . . . . . . . . . . . . . . . . . . .
304
9.2.2
The Inverse Mapping . . . . . . . . . . . . . . . . . . . . . . .
305
9.2.3
Geometrical Interpretation . . . . . . . . . . . . . . . . . . . .
305
9.2.4
The Chain Rule and Leibnitz Rule . . . . . . . . . . . . . .
307
9.2.5
Integration in Barycentric Coordinates . . . . . . . . . . .
308
9.3
How to Match Two Tetrahedra? . . . . . . . . . . . . . . . . . . . . . .
309
9.3.1
Continuity Across an Edge . . . . . . . . . . . . . . . . . . .
309
9.3.2
Smoothness Across an Edge . . . . . . . . . . . . . . . . . .
310
9.3.3
Continuity Across a Side . . . . . . . . . . . . . . . . . . . . .
311
9.4
Piecewise-Polynomial Function . . . . . . . . . . . . . . . . . . . . . . .
312
9.4.1
Independent Degrees of Freedom . . . . . . . . . . . . . . .
312
9.4.2
Smooth Piecewise-Polynomial Function . . . . . . . . . .
314
9.4.3
Continuous Piecewise-Polynomial Function . . . . . . .
315
9.5
Basis Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
9.5.1
Side-Midpoint Basis Function . . . . . . . . . . . . . . . . .
315
9.5.2
Edge-Midpoint Basis Function . . . . . . . . . . . . . . . . .
317
9.5.3
Hessian-Related Corner Basis Function . . . . . . . . . .
319
9.5.4
Gradient-Related Corner Basis Function . . . . . . . . . .
321
9.5.5
Corner Basis Function. . . . . . . . . . . . . . . . . . . . . . .
322
9.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
Part IV
Finite Elements in 3-D
10
Automatic Mesh Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
10.1
The Reﬁnement Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
10.1.1
Iterative Multilevel Reﬁnement . . . . . . . . . . . . . . . .
328
10.1.2
Conformity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
10.1.3
Regular Mesh . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
10.1.4
How to Preserve Regularity? . . . . . . . . . . . . . . . . . .
329
10.2
Approximating a 3-D Domain . . . . . . . . . . . . . . . . . . . . . . . .
329
10.2.1
Implicit Domain . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
10.2.2
Example: A Nonconvex Domain . . . . . . . . . . . . . . .
330
10.2.3
How to Find a Boundary Point? . . . . . . . . . . . . . . .
333
10.3
Approximating a Convex Boundary . . . . . . . . . . . . . . . . . . . .
334
10.3.1
Boundary Reﬁnement . . . . . . . . . . . . . . . . . . . . . . .
334
10.3.2
Boundary Edge and Triangle . . . . . . . . . . . . . . . . . .
335
Contents
xix

10.3.3
How to Fill a Valley? . . . . . . . . . . . . . . . . . . . . . . .
336
10.3.4
How to Find a Boundary Edge? . . . . . . . . . . . . . . .
338
10.3.5
Locally Convex Boundary: Gram–Schmidt
Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339
10.4
Approximating a Nonconvex Domain. . . . . . . . . . . . . . . . . . .
342
10.4.1
Locally Concave Boundary . . . . . . . . . . . . . . . . . . .
342
10.4.2
Convex Meshes . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
10.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
11
Mesh Regularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347
11.1
Angle and Sine in 3-D . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347
11.1.1
Sine in a Tetrahedron . . . . . . . . . . . . . . . . . . . . . . .
347
11.1.2
Minimal Angle . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
11.1.3
Proportional Sine . . . . . . . . . . . . . . . . . . . . . . . . . .
350
11.1.4
Minimal Sine . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
11.2
Adequate Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
11.2.1
Equivalent Regularity Estimates. . . . . . . . . . . . . . . .
351
11.2.2
Inadequate Equivalence . . . . . . . . . . . . . . . . . . . . . .
352
11.2.3
Ball Ratio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
11.3
Numerical Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
11.3.1
Mesh Regularity . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
11.3.2
Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . .
355
11.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
358
12
Numerical Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
12.1
Integration in 3-D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
12.1.1
Volume of a Tetrahedron . . . . . . . . . . . . . . . . . . . .
359
12.1.2
Integral in 3-D . . . . . . . . . . . . . . . . . . . . . . . . . . . .
360
12.1.3
Singularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
12.2
Changing Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
12.2.1
Spherical Coordinates . . . . . . . . . . . . . . . . . . . . . . .
362
12.2.2
Partial Derivatives. . . . . . . . . . . . . . . . . . . . . . . . . .
363
12.2.3
The Jacobian . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
12.2.4
Determinant of Jacobian . . . . . . . . . . . . . . . . . . . . .
363
12.2.5
Integrating a Composite Function . . . . . . . . . . . . . .
364
12.3
Integration in the Meshes . . . . . . . . . . . . . . . . . . . . . . . . . . .
364
12.3.1
Integrating in a Ball . . . . . . . . . . . . . . . . . . . . . . . .
364
12.3.2
Stopping Criterion . . . . . . . . . . . . . . . . . . . . . . . . .
365
12.3.3
Richardson Extrapolation . . . . . . . . . . . . . . . . . . . .
366
12.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
xx
Contents

13
Spline: Variational Model in Three Spatial Dimensions . . . . . . . . .
369
13.1
Expansion in Basis Functions . . . . . . . . . . . . . . . . . . . . . . . .
370
13.1.1
Degrees of Freedom . . . . . . . . . . . . . . . . . . . . . . . .
370
13.1.2
The Function Space and Its Basis . . . . . . . . . . . . . .
370
13.2
The Stiffness Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
372
13.2.1
Assemble the Stiffness Matrix . . . . . . . . . . . . . . . . .
372
13.2.2
How to Order the Basis Functions? . . . . . . . . . . . . .
373
13.3
Finding the Optimal Spline . . . . . . . . . . . . . . . . . . . . . . . . . .
374
13.3.1
Minimum Energy . . . . . . . . . . . . . . . . . . . . . . . . . .
374
13.3.2
The Schur Complement. . . . . . . . . . . . . . . . . . . . . .
374
13.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Part V
Advanced Applications in Physics and Chemistry
14
Quantum Chemistry: Electronic Structure . . . . . . . . . . . . . . . . . . .
379
14.1
Wave Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
380
14.1.1
Particle and Its Wave Function . . . . . . . . . . . . . . . .
380
14.1.2
Two Particles . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
380
14.2
Electrons in Their Orbitals . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
14.2.1
Atom: Electrons in Orbitals . . . . . . . . . . . . . . . . . . .
381
14.2.2
Potential Energy and Its Expectation . . . . . . . . . . . .
382
14.3
Distinguishable Electrons . . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
14.3.1
Hartree Product . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
14.3.2
Potential Energy of Hartree Product . . . . . . . . . . . . .
383
14.4
Indistinguishable Electrons . . . . . . . . . . . . . . . . . . . . . . . . . .
383
14.4.1
Indistinguishable Electrons . . . . . . . . . . . . . . . . . . .
383
14.4.2
Pauli’s Exclusion Principle: Slater Determinant . . . .
384
14.5
The Permutation Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
14.5.1
Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
14.5.2
Switch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
14.5.3
Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
385
14.5.4
Permutation Group . . . . . . . . . . . . . . . . . . . . . . . . .
386
14.5.5
Number of Permutations . . . . . . . . . . . . . . . . . . . . .
387
14.6
Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
388
14.6.1
Determinant: A New Deﬁnition . . . . . . . . . . . . . . . .
388
14.6.2
Determinant of the Transpose . . . . . . . . . . . . . . . . .
388
14.6.3
Determinant of a Product . . . . . . . . . . . . . . . . . . . .
389
14.6.4
Orthogonal and Unitary Matrix . . . . . . . . . . . . . . . .
390
14.6.5
The Characteristic Polynomial . . . . . . . . . . . . . . . . .
391
14.6.6
Eigenvalues and Trace . . . . . . . . . . . . . . . . . . . . . .
392
14.7
Orbitals and Their Canonical From . . . . . . . . . . . . . . . . . . . .
393
14.7.1
The Overlap Matrix and Its Diagonal From . . . . . . .
393
14.7.2
Unitary Transformation . . . . . . . . . . . . . . . . . . . . . .
394
Contents
xxi

14.7.3
Slater Determinant and Its Overlap . . . . . . . . . . . . .
394
14.7.4
The Canonical From . . . . . . . . . . . . . . . . . . . . . . . .
395
14.8
Expected Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
14.8.1
Coulomb and Exchange Integrals . . . . . . . . . . . . . . .
395
14.8.2
Effective Potential Energy . . . . . . . . . . . . . . . . . . . .
397
14.8.3
Kinetic Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
14.9
The Hartree–Fock System . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
14.9.1
Basis Functions—The Coefﬁcient Matrix . . . . . . . . .
398
14.9.2
The Mass Matrix . . . . . . . . . . . . . . . . . . . . . . . . . .
399
14.9.3
Pseudo-eigenvalue Problem . . . . . . . . . . . . . . . . . . .
399
14.10
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
14.10.1
Permutation—Product of Switches . . . . . . . . . . . . . .
400
14.10.2
How to Have the Canonical Form? . . . . . . . . . . . . .
401
15
General Relativity: Einstein Equations . . . . . . . . . . . . . . . . . . . . . .
403
15.1
General Relativity—Some Background . . . . . . . . . . . . . . . . . .
403
15.1.1
Flat Versus Curved Geometry . . . . . . . . . . . . . . . . .
403
15.1.2
Gravitational Time Dilation . . . . . . . . . . . . . . . . . . .
404
15.1.3
Gravitational Redshift . . . . . . . . . . . . . . . . . . . . . . .
405
15.1.4
“Straight” Line . . . . . . . . . . . . . . . . . . . . . . . . . . . .
405
15.2
Metric in Spacetime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
15.2.1
Spacetime. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
15.2.2
The Unknown Metric . . . . . . . . . . . . . . . . . . . . . . .
406
15.2.3
Minkowski Metric and Riemann Normal
Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
15.2.4
Gravity Waves . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
15.3
Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
15.3.1
The Gradient Symbol . . . . . . . . . . . . . . . . . . . . . . .
407
15.4
Einstein Summation Convention . . . . . . . . . . . . . . . . . . . . . .
408
15.4.1
Lower and Upper Indices . . . . . . . . . . . . . . . . . . . .
408
15.4.2
The Inverse Metric . . . . . . . . . . . . . . . . . . . . . . . . .
408
15.5
The Riemann Tensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
15.5.1
A New Convention . . . . . . . . . . . . . . . . . . . . . . . . .
409
15.5.2
The Christoffel Symbol . . . . . . . . . . . . . . . . . . . . . .
409
15.5.3
The Riemann Tensor. . . . . . . . . . . . . . . . . . . . . . . .
410
15.6
Einstein Equations in Vaccum . . . . . . . . . . . . . . . . . . . . . . . .
410
15.6.1
Vacuum and Curvature . . . . . . . . . . . . . . . . . . . . . .
410
15.6.2
The Ricci Tensor . . . . . . . . . . . . . . . . . . . . . . . . . .
411
15.6.3
Einstein Equations in Vacuum . . . . . . . . . . . . . . . . .
411
15.7
Einstein Equations—General Form . . . . . . . . . . . . . . . . . . . . .
412
15.7.1
The Stress (Energy Momentum) Tensor . . . . . . . . . .
412
15.7.2
The Stress Tensor and Its Trace. . . . . . . . . . . . . . . .
414
15.7.3
Ricci Scalar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
414
xxii
Contents

15.7.4
Einstein Tensor. . . . . . . . . . . . . . . . . . . . . . . . . . . .
414
15.7.5
Einstein Equations—General Form . . . . . . . . . . . . .
415
15.7.6
The Trace-Subtracted Form . . . . . . . . . . . . . . . . . . .
415
15.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
416
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
419
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
423
Contents
xxiii

List of Figures
Fig. 1.1
The vector ðx; yÞ is drawn as an arrow, issuing from the origin
ð0; 0Þ, and leading to the point ðx; yÞ in the Cartesian plane . . . .
4
Fig. 1.2
How to add ðx; yÞ to ð^x;^yÞ? Use them as sides in a new
parallelogram, and let their sum be the diagonal of this
parallelogram. This is the parallelogram rule . . . . . . . . . . . . . . .
4
Fig. 1.3
How to multiply (or stretch) the original vector ðx; yÞ by factor
2? Well, multiply coordinate by coordinate, to produce
the new vector 2ðx; yÞ ¼ ð2x; 2yÞ, which is twice as long . . . . . .
5
Fig. 1.4
The three-dimensional vector ðx; y; zÞ is an arrow, issuing
from the origin ð0; 0; 0Þ, and leading to the point ðx; y; zÞ
in the Cartesian space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
Fig. 1.5
The imaginary number i. The arrow leading from the origin
to i makes a right angle with the real axis. In i2 ¼ 1,
on the other hand, this angle doubles, to make a ﬂat angle
with the positive part of the real axis . . . . . . . . . . . . . . . . . . . . .
9
Fig. 1.6
The complex plane C. The imaginary number i 
ﬃﬃﬃﬃﬃﬃﬃ
1
p
is at ð0; 1Þ. A complex number a þ bi is at ða; bÞ . . . . . . . . . . . .
10
Fig. 1.7
A rectangular m  n matrix: there are m rows, and n
columns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
Fig. 1.8
In a square matrix A of order n, the main diagonal contains
a1;1, a2;2, . . ., an;n. If A is symmetric, then the lower triangular
part mirrors the upper triangular part: aj;i ¼ ai; j . . . . . . . . . . . . .
18
Fig. 1.9
In a Hermitian matrix, the lower triangular part is the complex
conjugate of the upper triangular part . . . . . . . . . . . . . . . . . . . . .
22
Fig. 1.10
The smoothest sine wave: sinð…xÞ. To obtain the discrete
sine mode, just sample at n discrete points: x ¼ 1=ðn þ 1Þ,
x ¼ 2=ðn þ 1Þ, x ¼ 3=ðn þ 1Þ, . . ., x ¼ n=ðn þ 1Þ . . . . . . . . . . . .
32
Fig. 1.11
The smoothest (nonconstant) cosine wave: cosð…xÞ. Sample
it at n discrete points: x ¼ 1=ð2nÞ, x ¼ 3=ð2nÞ, x ¼ 5=ð2nÞ,
. . ., x ¼ ð2n  1Þ=ð2nÞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
xxv

Fig. 1.12
The nth root of unity, and its powers in the complex plane:
w, w2, w3, . . ., wn ¼ 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
Fig. 2.1
The right-hand rule: the horizontal unit vectors, i and j,
produce the vertical unit vector i  j ¼ k . . . . . . . . . . . . . . . . . .
56
Fig. 2.2
The vector v makes angle g with the positive part of the x-axis:
cosðgÞ ¼ v1=kvk
57
Fig. 2.3
The vector ^v makes angle g with the unit vector ^i. Once
^v projects onto the ^i-axis, we have cosðgÞ ¼ ð^v;^iÞ=k^vk. . . . . . . .
58
Fig. 2.4
The right-hand rule: take your right hand, and match your
thumb to ðu1; u2Þ, and your index ﬁnger to ðv1; v2Þ. Then, your
middle ﬁnger will point upward, toward your own eyes,
as indicated by the “” at the origin . . . . . . . . . . . . . . . . . . . . .
61
Fig. 2.5
Up to a sign, ordering doesn’t matter. You could either
apply the orthogonal transformation and then the vector
product, or work the other way around: apply the vector
product ﬁrst, and then the orthogonal transformation . . . . . . . . .
62
Fig. 2.6
At time t, the particle is at r  rðtÞ 2 R3, with a linear
momentum p  pðtÞ 2 R3. This momentum could split into
two parts: the radial part is proportional to r, whereas the other
part is perpendicular to r. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
Fig. 2.7
For the sake of better visualization, we assume that the angular
velocity ! is perpendicular to r. This way, it points from the
page toward your eyes, as indicated by the “” at the origin.
(Don’t confuse ! with the other vector !!). . . . . . . . . . . . . . . . .
70
Fig. 2.8
The ﬁctitious centrifugal force: m!  ð!  rÞ. If ! and r
are orthogonal to each other, then it is also radial: mk!k2r . . . .
72
Fig. 2.9
Here, the particle is connected to the origin by a wire.
This supplies the centripetal force required to cancel the
original centrifugal force, and keep the particle at the constant
distance krk from the origin, rotating at the constant angular
velocity ! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
Fig. 2.10
Here, the particle rotates counterclockwise faster and faster,
so !0 points in the same direction as !. In this case, Euler force
pulls the particle clockwise, opposing the original rotation,
and slowing it down . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
Fig. 2.11
A horizontal cross section of the northern hemisphere
of the Earth at latitude 0  g\…=2 (a view from above). Here,
we place the origin not at the center of the Earth but at the
center of the cross section. As a result, r is horizontal as well: it
lies in the cross section. But ! (the direction of the spaceship)
is not: it makes angle g with the horizontal r-axis. The Coriolis
force pulls the entire spaceship westward . . . . . . . . . . . . . . . . . .
75
Fig. 2.12
The orthogonal matrix U rotates the entire x-y plane by angle h
counterclockwise and maps it to the new ~x-~y plane . . . . . . . . . .
78
xxvi
List of Figures

Fig. 2.13
In the v-w plane, p is orthogonal to v, but not to w . . . . . . . . . .
83
Fig. 2.14
The more general case, in which r is not necessarily
perpendicular to !. Let ^r be the part of r that is perpendicular
to !. Then, the centrifugal force is mk!k2^r rightward . . . . . . . .
84
Fig. 2.15
Let ^! be the part of ! that is perpendicular to r. If ! is radial,
then the angular momentum is mkrk2^!. This is nonconstant:
it must change direction to point not only upward but also
inward. Why isn’t angular momentum conserved? Because
the system is not closed or isolated: a horizontal centripetal
force must be supplied from the outside . . . . . . . . . . . . . . . . . . .
86
Fig. 2.16
The particles at r and r attract each other just enough to
supply the horizontal centripetal force required to make them
rotate together about the vertical !-axis. This is a closed
isolated system: no external force acts upon it. This is why
the total angular momentum is now conserved: ^! þ ^! keeps
pointing straight upward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
Fig. 4.1
In our lab, the ﬁrst particle moves rightwards at velocity v,
while the second particle moves leftwards at velocity u . . . . . . .
104
Fig. 4.2
Away from the second particle, the ﬁrst particle moves
at velocity ðu þ vÞ=ð1 þ uv=c2Þ, not u þ v . . . . . . . . . . . . . . . . . .
104
Fig. 4.3
The proper time of the lab is just t. In the lab, it can be read
from a static clock: t1; t2; . . .. This is the maximal proper time.
A clock moving at the constant speed of x=t ¼ v, on the other
hand, has a shorter proper time: t0
1 ¼ s1\t1, then t0
2 ¼ s2\t2,
and so on . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Fig. 4.4
The twin paradox: I live in the lab. My twin, on the other hand,
lives inside a particle, getting away at speed v. I say: “my time
ticks faster, so I’m older!” My twin, on the other hand, sees
things the other way around, and says: “my time ticks faster, so
I’m older!” Who is right? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
Fig. 4.5
A level set of s – a hyperbola in the original x-t lab coordinates.
ðx; tÞ is on the hyperbola if x could be reached at time t
by a particle moving at speed v ¼ x=t with respect to the lab.
In the self-system of the particle, on the other hand,
this will happen at proper time s0 . . . . . . . . . . . . . . . . . . . . . . . .
115
Fig. 4.6
View from the lab: the ﬁrst particle travels at velocity ðvx0; vy0Þ,
while the second particle travels at velocity ðu; 0Þ . . . . . . . . . .
121
Fig. 4.7
View from the second particle: the ﬁrst particle gets away
at a new velocity: ðdx=dt; dy=dtÞ in the x-y-t system . . . . . . . . .
122
Fig. 4.8
Conservation of momentum: after the original particle
(top picture) explodes and splits into two subparticles
(bottom picture), the total momentum is still muðﬂuÞ,
as before . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
List of Figures
xxvii

Fig. 4.9
View from the lab: initially, at time t0 ¼ 0, the collection
is still at rest at ðx0; y0Þ ¼ ð0; 0Þ. At t0 ¼ 0, an oblique external
force F0 ¼ ðF0
x0; F0
y0Þ starts to act upon it, to increase
its momentum and kinetic energy, while not changing
its mass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
Fig. 4.10
View from the second particle: the force that acts on the
collection remains the same in the x-direction, but seems
weaker in the perpendicular y-direction. . . . . . . . . . . . . . . . . . . .
133
Fig. 5.1
The homomorphism n from the original group G onto the
group M is not necessarily one-to-one. Still, it preserves
(or mirrors) the algebraic operation, denoted by the vertical
arrows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Fig. 5.2
Thanks to the homomorphism n, the inverse operation
in the original group G is mirrored or preserved in the group
M as well . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
Fig. 5.3
The homomorphism n maps its entire kernel (on the left)
to the unit element i 2 M (on the right) . . . . . . . . . . . . . . . . . . .
156
Fig. 5.4
The original set G is decomposed (or split) into disjoint lines,
or equivalence classes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
Fig. 5.5
Disjoint equivalence classes are considered as individual
elements in G=S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
Fig. 5.6
The new mapping N maps disjoint equivalence classes
(or distinct elements in the factor group) to distinct elements
in M . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
Fig. 5.7
The new isomorphism N from the factor group onto M
(the horizontal arrows) preserves (or mirrors) the algebraic
operations (the vertical arrows) . . . . . . . . . . . . . . . . . . . . . . . . . .
167
Fig. 5.8
A picture of C2: the two-dimensional complex vector ðc1; c2Þt
spans an oblique plane—the equivalence class Cðc1; c2Þt . . . . . .
169
Fig. 5.9
The oblique projection P projects the oblique plane Cðc1; c2Þt
to c1=c2. In particular, the horizontal complex plane fðz; 0Þ j
z 2 Cg projects to 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
Fig. 6.1
Antipodal points on the unit circle . . . . . . . . . . . . . . . . . . . . . . .
189
Fig. 6.2
A circle centered at O  ðxo; yoÞ. . . . . . . . . . . . . . . . . . . . . . . . .
190
Fig. 6.3
The ﬁrst complex coordinate c1  x þ y
ﬃﬃﬃﬃﬃﬃﬃ
1
p
. The circle
contains complex numbers c1 with jc1j2 ¼ x2 þ y2 ¼ r2,
for a ﬁxed radius r  0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
Fig. 6.4
The second complex coordinate c2  z þ w
ﬃﬃﬃﬃﬃﬃﬃ
1
p
. The circle
contains complex numbers c2 with jc2j2 ¼ z2 þ w2 ¼ 1  r2,
where r  1 is the radius of the former circle: the c1-circle
above . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
xxviii
List of Figures

Fig. 6.5
The new jc1j-jc2j plane, where c1  x þ y
ﬃﬃﬃﬃﬃﬃﬃ
1
p
and
c2  z þ w
ﬃﬃﬃﬃﬃﬃﬃ
1
p
are formed from the original ðx; y; z; wÞ 2 R4.
The arc contains those points for which jc1j2 þ jc2j2 ¼ 1,
including those points in the c1- and c2-circles above. . . . . . . . .
192
Fig. 6.6
The line Cv meets the unit circle at two antipodal points:
v=kvk. Fortunately, in the divided circle, they coincide with
each other. For example, the horizontal x-axis, Cð1; 0Þt, is
represented by the pair ð1; 0Þ . . . . . . . . . . . . . . . . . . . . . . . . . .
200
Fig. 6.7
The top semicircle is enough: each line of the form Cv
is represented by the unique point v=kvk. There is just one
exception: the horizontal x-axis Cð1; 0Þt is still represented
by the pair ð1; 0Þ. In the divided circle, these points are
considered as one and the same. Topologically, this “closes”
the semicircle from below, producing a circle. . . . . . . . . . . . . . .
201
Fig. 6.8
Oblique projection onto the horizontal plane z  1. Each line
of the form Cðx; y; zÞt (z 6¼ 0) projects onto ðx=z; y=z; 1Þ. . . . . . .
202
Fig. 6.9
Radial projection: each line of the form Cv projects onto
the pair of antipodal unit vectors v=kvk in the sphere S2 . . . . .
203
Fig. 6.10
What is an inﬁnity point in the real projective plane? It is
a horizontal line of the form Cðx; y; 0Þt, projected onto the
antipodal points ðx; y; 0Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 þ y2
p
in the inﬁnity circle. . . . . .
204
Fig. 6.11
The hyperboloid projects onto a circle in the horizontal plane
z  1. Each plane tangent to the original hyperboloid projects
to a line tangent to the circle and perpendicular to its radius . . .
216
Fig. 6.12
The route of the moon in the solar system, projected on the
horizontal plane z  1. It is assumed that the moon rotates
around the Earth in an oblique plane, whose normal vector
is n ¼ ð1; 1; 1Þt=
ﬃﬃﬃ
3
p
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
Fig. 7.1
The position-momentum grid. At each individual time t, solve
for the n-dimensional state v. This way, v contains the entire
physical information at time t, in terms of probability. To be at
x ¼ Xj; j, the particle has probability jvjj2. To have momentum
p ¼ ‚k, the particle has probability jðuðkÞ; vÞj2 (1  j; k  n) . . . .
237
Fig. 7.2
Gaussian distribution: w lies in the null spaces of both A and
AhA, with zero expectation: ðAw; AwÞ ¼ ðw; AhAwÞ ¼ 0. To
be at x ¼ Xk;k, the particle has probability jwkj2. Thus, it is
highly likely to be at x ¼ 0—the expectation . . . . . . . . . . . . . . .
249
Fig. 7.3
A coherent state: a Gaussian distribution, shifted by a complex
number ‚. For each k, jvkj2 is the probability to be
at x ¼ Xk;k. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
List of Figures
xxix

Fig. 7.4
The Poisson distribution. To have energy h!ðk þ 1=2Þ,
the probability is expðj‚j2Þj‚j2k=k!, where ‚ is the
eigenvalue of the coherent state with respect to A. The
maximal probability is at k¼: j‚j2. . . . . . . . . . . . . . . . . . . . . . . . .
253
Fig. 7.5
The discrete two-dimensional grid: m horizontal rows,
of m points each. Since n ¼ m2, our general state v makes
a (complex) grid function, deﬁned at each grid point.
Furthermore, in each individual row, X acts in the same way:
it couples grid points in the same row . . . . . . . . . . . . . . . . . . . .
253
Fig. 7.6
How likely is the particle to have spin-up? Take the
eigenvector ð1; iÞt=
ﬃﬃﬃ
2
p
or ð1; i; 0Þt=
ﬃﬃﬃ
2
p
. Thanks to the
right-hand rule, it points from the page towards your eye,
as indicated by the ‘’ at the origin. Then, calculate its inner
product with the (normalized) state v. Finally, take the absolute
value of this inner product, and square it up. . . . . . . . . . . . . . . .
262
Fig. 7.7
How likely is the particle to have spin-down? Take the
eigenvector ð1; iÞt=
ﬃﬃﬃ
2
p
or ð1; i; 0Þt=
ﬃﬃﬃ
2
p
. Thanks to the
right-hand rule, it points deep into the page, as indicated
by the ‘	’ at the origin. Then, calculate its inner product
with the (normalized) state v. Finally, take the absolute
value of this inner product, and square it up. . . . . . . . . . . . . . . .
262
Fig. 8.1
How to multiply pðxÞ ¼ a0 þ a1x þ a2x2 þ a3x3 by
qðxÞ ¼ b0 þ b1x þ b2x2? Sum the terms diagonal by diagonal:
in the kth diagonal, sum those terms with the power
xk (0  k  5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270
Fig. 8.2
The unit triangle t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
Fig. 8.3
Integration over the unit triangle: for each ﬁxed x, integrate
over the vertical 0  y  1  x. . . . . . . . . . . . . . . . . . . . . . . . . . .
282
Fig. 8.4
To deﬁne the (i; j)th partial derivative, March diagonal by
diagonal: use mathematical induction on i þ j ¼ 0; 1; 2; 3; . . . . . . .
284
Fig. 8.5
The unit tetrahedron T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
Fig. 9.1
A general tetrahedron t, vertexed at k, l, m, and n. . . . . . . . . . .
300
Fig. 10.1
The good case: the arrow leading from a to a þ d indeed
contains a boundary point in @X. In this case, F indeed
changes sign over the arrow: FðaÞ\0\Fða þ dÞ or
FðaÞ [ 0 [ Fða þ dÞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
Fig. 10.2
The bad case: the arrow leading from a to a þ d is too short,
so the boundary @X remains ahead of it. The arrow must ﬁrst
shift or stretch forward, until its head passes @X, as in the
previous ﬁgure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
334
Fig. 10.3
A boundary edge with both its endpoints on @X.
Unfortunately, its midpoint may still lie outside the nonconvex
domain, so no boundary reﬁnement is carried out there.
xxx
List of Figures

Furthermore, the subedge is no longer a boundary edge:
it has an endpoint off @X. Therefore, no boundary reﬁnement
will take place ever. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
Fig. 10.4
A boundary edge of M, although not of X. In this sense, the
left subedge is a legitimate boundary edge, with both endpoints
on @M, although not on @X. Its own midpoint lies well inside
X, so boundary reﬁnement will indeed take place there
in the next reﬁnement step. Although the mesh gets slightly
nonconvex, this should produce no overlapping tetrahedra . . . . .
337
Fig. 10.5
The coarse mesh: a view from above. In the reﬁnement step,
the oblique edge splits, and a normal vector issues from its
midpoint towards your eyes to hit the boundary above it . . . . . .
337
Fig. 10.6
The next ﬁner mesh: two pyramids, with a concave valley
in between . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
Fig. 10.7
The next reﬁnement step: the vertical edge along the valley,
although submaximal, splits, and a normal vector issues from
its midpoint ‘’ towards your eyes to hit the boundary above
it, and ﬁll the valley with four new tetrahedra . . . . . . . . . . . . . .
338
Fig. 10.8
Projection onto the plane perpendicular to the boundary edge
e  ðm; nÞ. The direction vector d points from a  ðm þ nÞ=2
towards @X, as required . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
Fig. 11.1
The tetrahedron t: a view from above. It sits on its horizontal
base: Mðk; l; mÞ. Its left edge ðk; lÞ and its top corner n make
a nonhorizontal face: Mðk; l; nÞ. Between these two faces,
there is a vertical angle: d. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
Fig. 11.2
Strong versus weak regularity estimates. The weak estimates
at the top are equivalent to each other, but inferior to the robust
estimates at the bottom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
Fig. 11.3
A ﬂat tetrahedron: a view from above. All regularity estimates
(weak and strong alike) are as good . . . . . . . . . . . . . . . . . . . . . .
354
Fig. 11.4
A thin tetrahedron: a view from above. The weak estimates lie:
they are as small as "2, but the true estimate is only as small
as ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
Fig. 12.1
The vector ðx; y; zÞ in its spherical coordinates: r
(the magnitude of the vector), / (the angle between the
original vector and its projection onto the x-y plane), and h
(the angle between this projection and the x-axis). . . . . . . . . . . .
362
List of Figures
xxxi

Part I
Introduction to Linear Algebra
We’re already familiar with elementary algebraic objects: numbers (or scalars), along
with the arithmetic operations between them. In this part, on the other hand, we look
at more complicated structures: vector and matrix. Fortunately, it is also possible to
deﬁne arithmetic (or algebraic) operations between them.
With these new operations, the vectors make a new linear space. The (nonsingular)
matrices, on the other hand, make yet another important structure: a group. In a
group, the associative law must hold. The commutative law, on the other hand, not
necessarily holds.
The matrix is not just an algebraic object. It may also have a geometrical meaning:
a mapping or transformation. This is most useful in many applications.
In special relativity, for example, the Lorentz transformation can be written as a
small 2 × 2 matrix. In geometrical mechanics, on the other hand, 3 × 3 matrices
are more useful. Finally, a yet bigger matrix is often used in stochastic analysis,
to model a Markov chain in a graph. This has an interesting application in modern
search engines in the internet.

Chapter 1
Vectors and Matrices
Here is what we are going to do in this chapter. What is a vector? It is a ﬁnite list of
(real) numbers: scalars, or components.
In a geometrical context, the components have yet another name: coordinates. In
the two-dimensional Cartesian plane, for example, a vector contains two coordinates:
the x- and y-coordinates. This is why the vector is often denoted by the pair (x, y).
Geometrically, this can also be viewed as an arrow, leading from the origin (0, 0) to
the point (x, y) ∈R2. Here, R is the real axis, R2 is the Cartesian plane, and “∈”
means “belongs to”.
In the three-dimensional Cartesian space, on the other hand, the vector also con-
tains a third coordinate: the z-coordinate. This is why the vector is often denoted by
the triplet (x, y, z) ∈R3.
Still, vectors are more than just lists of numbers. They also have linear arithmetic
operations: addition, multiplication by a scalar, and more. With these operations, the
vectors form a complete linear space.
What is a matrix? It is a rectangular frame, full of numbers: scalars or elements,
ordered row by row in the matrix.
Unlike the vector, the matrix has a new arithmetic operation: multiplication. A
matrix could multiply a vector, or be applied to a vector. This is done from the left:
ﬁrst write the matrix, then the vector. Likewise, a matrix could multiply another
matrix.
In geometrical terms, the matrix can also be viewed as a linear mapping (or
transformation) from one vector space to another. To map a vector, the matrix should
be applied to it. This produces the new image (or target) vector. With this new
interpretation, the matrix is now more active: it acts upon a complete vector space.
For example, the matrix could simply rotate the original vector (see exercises at the
end of Chap.2).
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_1
3

4
1
Vectors and Matrices
1.1
Vectors in Two and Three Dimensions
1.1.1
Two-Dimensional Vectors
What is a vector? It is a ﬁnite list or sequence: a ﬁnite set of numbers, ordered one
by one in a row. In this list, each number is also called a scalar or a component. The
total number of components is often denoted by the natural number n.
In geometrical terms, on the other hand, the components are also viewed as coor-
dinates. This way, the vector also takes a new interpretation: an n-dimensional vector,
in a new n-dimensional linear space.
In the trivial case of n = 1, for example, the vector contains just one component
or coordinate. In this degenerate case, the one-dimensional vector (x) mirrors the
scalar x: both can be interpreted geometrically as the point x on the real axis.
In the more interesting case of n = 2, on the other hand, the two-dimensional
vector is a pair of two numbers: x and y. Here, the ﬁrst component x serves as
the horizontal coordinate, whereas the second component y serves as the vertical
coordinate.
This way, the original vector (x, y) takes its geometrical meaning as well: the new
point (x, y) in the Cartesian plane. To illustrate the vector, draw an arrow from the
origin (0, 0) to the point (x, y) ∈R2 (Fig.1.1).
1.1.2
Adding Vectors
Consider two vectors: (x, y) and (ˆx, ˆy) that lie somewhere in the Cartesian plane. How
toaddthemtoeachother?Forthispurpose,usetheparallelogramrule(Fig.1.2).After
Fig. 1.1 The vector (x, y) is
drawn as an arrow, issuing
from the origin (0, 0), and
leading to the point (x, y) in
the Cartesian plane
Fig. 1.2 How to add (x, y)
to (ˆx, ˆy)? Use them as sides
in a new parallelogram, and
let their sum be the diagonal
of this parallelogram. This is
the parallelogram rule

1.1 Vectors in Two and Three Dimensions
5
all, we already have three points: (0, 0), (x, y), and (˜x, ˜y). To make a parallelogram,
we need just one more point. This new point will indeed be the required sum (x, y)+
(ˆx, ˆy).
Unfortunately, this is still too geometrical. After all, we can never trust our own
human eye or hand to draw this accurately. Instead, we better have a more algebraic
method, independent of geometry.
Fortunately, the above geometrical rule also has an algebraic face: add component
by component:
(x, y) + (ˆx, ˆy) ≡(x + ˆx, y + ˆy).
This way, in the required sum, each individual coordinate is easy to calculate: it is
just the sum of the corresponding coordinates in the original vectors. This algebraic
formulation is much more practical: it is easy to implement on the computer, and to
extend to higher dimensions as well.
1.1.3
Scalar Times Vector
A vector can also be multiplied by a number (or scalar, or factor), either from the left
(scalar times vector) or from the right (vector times scalar). What is the result? It is
a new vector that could be either shorter or longer, but must still point in the same
direction as before. After all, the ratio between the coordinates remains the same.
The only thing that has changed is the length (or magnitude) (Fig.1.3).
Unfortunately, this is still too geometrical: it gives no practical algorithm. After
all, we can never trust our human eye or hand to draw the new vector accurately. How
to do this algebraically? Easy: just multiply coordinate by coordinate. For example,
to multiply the vector (x, y) by the scalar a from the left, deﬁne
a(x, y) ≡(ax, ay).
Likewise, to multiply from the right, deﬁne
(x, y)a ≡(xa, ya),
which is just the same as before. In this sense, the multiplication is indeed commu-
tative.
Fig. 1.3 How to multiply
(or stretch) the original
vector (x, y) by factor 2?
Well, multiply coordinate by
coordinate, to produce the
new vector
2(x, y) = (2x, 2y), which is
twice as long

6
1
Vectors and Matrices
Later on, we’ll see a lot of examples with two-dimensional vectors. In Sect.1.3.1,
we’ll see that a complex number is actually a two-dimensional vector. Furthermore,
in the exercises at the end of Chap.2, we’ll see how to rotate a vector. Moreover, in
Chaps.4 and 5, we’ll see Lorentz and Moebius transformations. Here, however, we
have no time for examples. To see how algebra and geometry go hand in hand, we
better go ahead and extend the above to three spatial dimensions as well.
1.1.4
Three-Dimensional Vectors
So far, we’ve considered the two-dimensional case n = 2. In the three-dimensional
case n = 3, on the other hand, we introduce one more dimension: the z dimen-
sion. This way, a vector is now a triplet of three (rather than two) components or
coordinates: (x, y, z).
Ingeometricalterms,thevector(x, y, z)representsapointinthethree-dimensional
Cartesian space, with the horizontal coordinates x and y, and the height coordinate
z. This is why the vector is often illustrated as an arrow, issuing from the origin
(0, 0, 0), and leading to the point (x, y, z) ∈R3 (Fig.1.4).
As in Sect.1.1.2, addition is still made coordinate by coordinate:
(x, y, z) + (ˆx, ˆy, ˆz) ≡(x + ˆx, y + ˆy, z + ˆz).
Furthermore, as in Sect.1.1.3, multiplication by a scalar is still made coordinate by
coordinate as well. This could be done either from the left:
a(x, y, z) ≡(ax, ay, az),
or from the right:
(x, y, z)a ≡(xa, ya, za).
In both cases, the result is the same. In this sense, the commutative law indeed applies.
Later on, we’ll see a lot of examples. In Chap.2, in particular, we’ll rotate a three-
dimensional vector. Here, however, we have no time for this. After all, the complete
algebraic picture is far more general. To realize this, we better proceed to a yet higher
dimension, with no apparent geometrical meaning anymore.
Fig. 1.4 The
three-dimensional vector
(x, y, z) is an arrow, issuing
from the origin (0, 0, 0), and
leading to the point (x, y, z)
in the Cartesian space

1.2 Vectors in Higher Dimensions
7
1.2
Vectors in Higher Dimensions
1.2.1
Multidimensional Vectors
So far, our vectors had a concrete geometrical meaning. For n > 3, on the other hand,
they are no longer geometrical, but only algebraic.
The n-dimensional vector is ﬁrst of all a set: a ﬁnite list or sequence of n individual
numbers (components):
v ≡(v1, v2, v3, . . . , vn) ∈Rn,
where R is the real axis and Rn is the n-dimensional space. Still, the vector is more
than that: it is also an algebraic object, with all sorts of arithmetic operations. To see
this, consider yet another vector:
u ≡(u1, u2, . . . , un) ∈Rn.
This vector can now be added to v, component by component:
u + v ≡(u1 + v1, u2 + v2, . . . , un + vn).
1.2.2
Associative Law
Fortunately, this operation is associative. Indeed, let w ≡(w1, w2, . . . , wn) be yet
another vector in Rn. Now, to sum these three vectors, ordering doesn’t matter: either
add the ﬁrst two to each other and then add the third one as well, or start from the
second and third vectors and add them to each other, and then add the ﬁrst as well:
u + (v + w) = (u1 + (v1 + w1), u2 + (v2 + w2), . . . , un + (vn + wn))
= ((u1 + v1) + w1, (u2 + v2) + w2, . . . , uvn + vn) + wn)
= (u + v) + w.
This is indeed the associative law for addition.
Still, the associative law applies not only to addition but also to multiplication by
a scalar.
1.2.3
The Origin
In Rn, there is one special vector: the zero vector (or the origin), with n zeroes:
0 ≡(0, 0, 0, . . . , 0)
(n zeroes).

8
1
Vectors and Matrices
In what way is this vector special? Well, it is the only vector that can be added to
any n-dimensional vector v, with no effect whatsoever:
0 + v = v + 0 = v.
1.2.4
Multiplication and Its Laws
Now, how to multiply a vector by a scalar a ∈R, either from the left or from the
right? Well, as before, this is done component by component:
av ≡va ≡(av1, av2, . . . , avn).
In this sense, multiplication is indeed commutative. Fortunately, it is associative as
well. To see this, consider yet another scalar b:
b(av) = b(av1, av2, . . . , avn) = (bav1, bav2, . . . , bavn) = (ba)v.
1.2.5
Distributive Laws
Furthermore, the above arithmetic operations are also distributive in two senses. On
one hand, you can push the same vector v into parentheses:
(a + b)v = ((a + b)v1, (a + b)v2, . . . , (a + b)vn)
= (av1 + bv1, av2 + bv2, . . . , avn + bvn)
= av + bv.
On the other hand, you can push the same scalar a into parentheses:
a(u + v) = (a(u1 + v1), a(u2 + v2), . . . , a(un + vn))
= (au1 + av1, au2 + av2, . . . , aun + avn)
= au + av.
This completes the deﬁnition of the new vector space Rn, and the linear algebraic
operations in it.

1.3 Complex Numbers and Vectors
9
1.3
Complex Numbers and Vectors
1.3.1
Complex Numbers
The two-dimensional vectors deﬁned above may also help to model complex num-
bers. As a matter of fact, complex numbers have just one more algebraic operation:
multiplication. Let’s explain this brieﬂy.
The negative number −1 has no square root: there is no real number whose square
is −1. Fortunately, it is still possible to introduce a new auxiliary (not real) number—
the imaginary number i. This way, i is the only number whose square is −1:
i2 = −1,
or
i ≡
√
−1.
Because it lies outside the real axis, i may now span a new vertical axis, perpendicular
to the original real axis (Fig.1.5).
For this purpose, place i at (0, 1), above the origin. This way, i spans the entire
imaginary axis:
{bi = (0, b) | −∞< b < ∞} .
Here, b is some real number. In the above, the algebraic multiple bi also takes the
geometrical form (0, b): a new point on the vertical imaginary axis. In particular, if
b = 1, then we obtain the original imaginary number i = (0, 1) once again.
Together, the real and imaginary axes span the entire complex plane:
C ≡{a + bi ≡(a, b) | −∞< a, b < ∞} .
Here, a and b are some real numbers. The new complex number a + bi also takes its
geometrical place (a, b) ∈C (Fig.1.6).
So far, the complex plane is deﬁned geometrically only. What is its algebraic
meaning? Well, since complex numbers are two-dimensional vectors, we already
know how to add them to each other and multiply them by a real scalar. How to
Fig. 1.5 The imaginary number i. The arrow leading from the origin to i makes a right angle with
the real axis. In i2 = −1, on the other hand, this angle doubles, to make a ﬂat angle with the positive
part of the real axis

10
1
Vectors and Matrices
Fig. 1.6 The complex plane
C. The imaginary number
i ≡√−1 is at (0, 1). A
complex number a + bi is at
(a, b)
multiply them by each other? Well, we already know that
i2 = −1.
After all, this is how i has been deﬁned in the ﬁrst plane.
What is the geometrical meaning of this algebraic equation? Well, look at the
negative number −1. What angle does it make with the positive part of the real axis?
A ﬂat angle of 180◦. The imaginary number i, on the other hand, makes a right angle
of 90◦with the real axis (Fig.1.5). Thus, multiplying i by itself means adding yet
another right angle, to make a ﬂat angle together.
Let’s go ahead and extend this linearly to the entire complex plane. In other words,
let’s use the distributive and commutative laws to multiply a+bi times c+di (where
a, b, c, and d are some real numbers):
(a + bi)(c + di) = a(c + di) + bi(c + di) = ac + adi + bci + bdi2 = ac −bd + (ad + bc)i.
For example, look at the special case
c = a and d = −b.
This way, c + di is the complex conjugate of a + bi, denoted by a small bar on top:
c + di ≡a −bi ≡
¯
(a + bi).
In this case, the above product is a new real number: the squared absolute value of
a + bi:
|a + bi|2 ≡(a + bi)(a −bi) = a2 + b2.
Thanks to this deﬁnition, the absolute value is the same as the length of the vector
(a, b), obtained from Pythagoras theorem. If this is nonzero, then we could divide
by it

1.3 Complex Numbers and Vectors
11
(a + bi) a −bi
a2 + b2 = 1.
So, we now also have the reciprocal (or inverse) of a + bi:
(a + bi)−1 = a −bi
a2 + b2 .
1.3.2
Complex Vectors
In our n-dimensional vector, each individual component can now be a complex num-
ber. This yields a new vector space: Cn. The only difference is that the components
(and the scalar that may multiply them) can now be not only real but also complex.
Because the arithmetic operations are deﬁned in the same way, the same associa-
tive and distributive laws still hold. In other words, the algebraic operations remain
linear. For this reason, Cn could be viewed as a natural extension of the original
vector space Rn.
1.4
Rectangular Matrix
1.4.1
Matrices
A matrix is a frame, full of numbers. In an m by n (or m × n) rectangular matrix,
there are mn numbers: m rows, and n columns (Fig.1.7).
The original m by n matrix is often denoted by
A ≡

ai,j

1≤i≤m, 1≤j≤n .
Here, the individual element ai,j is some number, placed in the ith row, and in the
jth column. This way, A contains n columns, ordered one by one, left to right. Each
column contains m individual elements (numbers), ordered top to bottom.
Fig. 1.7 A rectangular
m × n matrix: there are m
rows, and n columns

12
1
Vectors and Matrices
Let’s look at each column as an individual object, which could serve as a new com-
ponent in a new row vector. From this point of view, A is actually an n-dimensional
vector, with n “components” in a row: each “component” is not just a scalar, but
a complete vector in its own right: an m-dimensional column vector, containing m
numbers:
A ≡

v(1) | v(2) | v(3) | · · · | v(n)
,
where v(1), v(2), . . . , v(n) are column vectors in Rm. This way, for 1 ≤j ≤n, v(j) is
the jth column in A:
v(j) ≡
⎛
⎜⎜⎜⎜⎜⎜⎝
v(j)
1
v(j)
2
v(j)
3...
v(j)
m
⎞
⎟⎟⎟⎟⎟⎟⎠
≡
⎛
⎜⎜⎜⎜⎜⎝
a1,j
a2,j
a3,j
...
am,j
⎞
⎟⎟⎟⎟⎟⎠
.
In this column vector, for 1 ≤i ≤m, the ith component is the matrix element
v(j)
i
≡ai,j.
For example, if m = 3 and n = 4, then A is a 3 × 4 matrix:
A =
⎛
⎝
a1,1 a1,2 a1,3 a1,4
a2,1 a2,2 a2,3 a2,4
a3,1 a3,2 a3,3 a3,4
⎞
⎠.
In this form, A could also be viewed as a list of three rows. Each row contains four
numbers, ordered left to right. In total, A contains 12 elements.
1.4.2
Adding Matrices
Let
B ≡

bi,j

1≤i≤m ,1≤j≤n
be yet another m×n matrix. This way, B can now be added to A, element by element:
A + B ≡

ai,j + bi,j

1≤i≤m, 1≤j≤n .
Alternatively, B could also be written in terms of its columns:
B ≡

u(1) | u(2) | u(3) | · · · | u(n)
,

1.4 Rectangular Matrix
13
where u(1), u(2), . . . , u(n) are column vectors in Rm. This way, B could also be added
column by column:
A + B ≡

v(1) + u(1) | v(2) + u(2) | v(3) + u(3) | · · · | v(n) + u(n)
.
It is easy to see that this operation is associative:
(A + B) + C = A + (B + C),
where C is yet another m × n matrix.
1.4.3
Scalar-Times-Matrix
To multiply A by a real number r ∈R (either from the left or from the right), just
multiply element by element:
rA ≡Ar ≡

rai,j

1≤i≤m, 1≤j≤n .
Clearly, this operation is associative as well: if q ∈R is yet another scalar, then
q(rA) ≡q

rai,j

1≤i≤m, 1≤j≤n =

qrai,j

1≤i≤m, 1≤j≤n = (qr)A.
Furthermore, the above operations are also distributive in two senses. On one hand,
you can push the same matrix A into parentheses:
(r + q)A = rA + qA.
On the other hand, you can push the same scalar r into parentheses:
r(A + B) = rA + rB.
1.4.4
Matrix-Times Vector
Recall that A has already been written column by column:
A =

v(1) | v(2) | · · · | v(n)
.
This way, each column is in Rm. Consider now a different column vector, not in Rm
but rather in Rn:

14
1
Vectors and Matrices
w =
⎛
⎜⎜⎜⎝
w1
w2
...
wn
⎞
⎟⎟⎟⎠.
How many components are here? As many as the number of columns in A. This is
just enough! We can now scan A column by column, multiply each column by the
corresponding component from w, and sum up:
Aw ≡w1v(1) + w2v(2) + · · · + wnv(n) =
n

j=1
wjv(j).
This is indeed A times w: a new linear combination of the columns of A, with
coefﬁcients taken from w.
Thus, w and Aw may have different dimensions: w is n-dimensional, whereas Aw
is m-dimensional. Later on, we’ll use this property to interpret A geometrically, as a
mapping:
A : Rn →Rm.
This means that A maps each n-dimensional vector to an m-dimensional vector.
In other words, A is a function: it takes an n-dimensional vector, and returns an
m-dimensional vector. Here, “→” stands for mapping, not for a limit.
In Aw, what is the ith component? Well, it is just
(Aw)i =
n

j=1
wjv(j)
i
=
n

j=1
ai,jwj,
1 ≤i ≤m.
To calculate this, scan the ith row in A element by element. Multiply each element
by the corresponding component from w, and sum up.
This is the new matrix-times-vector operation. What are its algebraic properties?
Well, ﬁrst of all, it is associative for scalars: for any scalar r ∈R,
A(rw) = r(Aw) = (rA)w.
Furthermore, it is also distributive in two senses. On one hand, you can push the
same vector into parentheses:
(A + B)w = Aw + Bw,
where B has the same dimensions as A. On the other hand, you can push the same
matrix into parentheses:
A(w + u) = Aw + Au,
where u has the same dimension as w.

1.4 Rectangular Matrix
15
In summary, w →Aw is indeed a linear transformation. This will be useful later.
In Chap.2, for example, we’ll see how a matrix could rotate a vector. Here, however,
we have no time for examples. To have the complete algebraic picture, we better
extend the above to a yet more complicated operation: matrix-times- matrix.
1.4.5
Matrix-Times-Matrix
The above can now be extended to deﬁne yet another kind of multiplication: matrix-
times-matrix. For this purpose, however, we must be careful to pick a matrix B with
proper dimensions: an l × m matrix, where l is some natural number as well.
Why must B have these dimensions? Because, this way, the number of columns
in B is the same as the number of rows in A. This is a must: it will help multiply B
times A soon.
Fortunately, A has already been written column by column. Now, apply B to each
individual column:
BA ≡

Bv(1) | Bv(2) | · · · | Bv(n)
.
Why is this legitimate? Because A has m rows, and B has m columns. This way, the
product BA is a new l × n matrix: it has as many rows as in B, and as many columns
as in A.
In BA, what is the (i, k)th element? Well, for this purpose, focus on the kth column:
Bv(k). In it, look at the ith component. It comes from the ith row in B: scan it element
by element, multiply each element by the corresponding component in v(k), and sum
up:
(BA)i,k =

Bv(k)
i =
m

j=1
bi,jv(k)
j
=
m

j=1
bi,jaj,k,
1 ≤i ≤l, 1 ≤k ≤n.
Thus, at the same time, two things are scanned element by element: the ith row in B,
and the kth column in A. This makes a loop of m steps. In each step, pick an element
from the ith row in B, pick the corresponding element from the kth column in A,
multiply, and sum up:
(BA)i,k =
m

j=1
bi,jaj,k.
1.4.6
Distributive and Associative Laws
Fortunately, the new matrix-times-matrix operation is distributive in two senses. On
one hand, you can push the same matrix into parentheses from the right:

16
1
Vectors and Matrices

B + ˆB

A = BA + ˆBA
(where ˆB has the same dimensions as B). On the other hand, you can push the same
matrix into parentheses from the left:
B

A + ˆA

= BA + BˆA
(where ˆA has the same dimensions as A).
Moreover, matrix-times-matrix is also an associative operation. To see this, let
C =

ci,j

1≤i≤k, 1≤j≤l
be a k × l matrix, where k is some natural number as well.
Why are the dimensions of C picked in this way? Well, this guarantees that the
number of columns in C is the same as the number of rows in B (and also in BA).
This way, we can now apply C from the left, and produce the new matrices C(BA)
and (CB)A.
Are they the same? To check on this, let’s calculate the (s, t)th element, for some
1 ≤s ≤k and 1 ≤t ≤n:
(C(BA))s,t =
l
i=1
cs,i(BA)i,t
=
l
i=1
cs,i
m

j=1
bi,jaj,t
=
l
i=1
m

j=1
cs,ibi,jaj,t
=
m

j=1
l
i=1
cs,ibi,jaj,t
=
m

j=1

l
i=1
cs,ibi,j

aj,t
=
m

j=1
(CB)s,jaj,t
= ((CB)A)s,t.
This can be done for every pair (s, t). Therefore,
C(BA) = (CB)A,

1.4 Rectangular Matrix
17
as asserted. This proves that matrix multiplication is not only distributive but also
associative. In summary, it is a linear operation. This will be useful later.
1.4.7
The Transpose Matrix
Consider again our matrix A. Look at it the other way around: view the rows as
columns, and the columns as rows. This yields a new n × m matrix— the transpose
matrix At:

At
j,i ≡ai,j,
1 ≤i ≤m, 1 ≤j ≤n.
For example, if A is the 3 × 4 matrix
A =
⎛
⎝
a1,1 a1,2 a1,3 a1,4
a2,1 a2,2 a2,3 a2,4
a3,1 a3,2 a3,3 a3,4
⎞
⎠,
then At is the 4 × 3 matrix
At =
⎛
⎜⎜⎝
a1,1 a2,1 a3,1
a1,2 a2,2 a3,2
a1,3 a2,3 a3,3
a1,4 a2,4 a3,4
⎞
⎟⎟⎠.
From the above deﬁnition, it clearly follows that
(At)t = A.
In Sect.1.4.5, we’ve deﬁned the product BA, where B is an l × m matrix. Why is this
product well deﬁned? Because the number of rows in A is the same as the number
of columns in B.
How to multiply the transpose matrices? Well, the number of rows in Bt is the
same as the number of columns in At. So, we can construct a new n × l matrix: the
product AtBt.
Is it really new? Not quite. After all, we’ve already seen it before, at least in its
transpose form:
(BA)t = AtBt.
To prove this, pick some 1 ≤i ≤l and 1 ≤k ≤n. Consider the (k, i)th element in
(BA)t:
(BA)t
k,i = (BA)i,k =
m

j=1
bi,jaj,k =
m

j=1
At
k,jBt
j,i =

AtBt
k,i ,
as asserted.

18
1
Vectors and Matrices
1.5
Square Matrix
1.5.1
Symmetric Square Matrix
So far, we’ve assumed that A was rectangular: the number of rows was not necessarily
the same as the number of columns. In this section, on the other hand, we focus on
a square matrix of order m = n. Since A is square, it has a main diagonal—from the
upper left corner to the lower right corner:
a1,1, a2,2, a3,3, . . . , an,n
(Fig.1.8). This diagonal splits A into two triangular parts: its upper right part, and its
lower left part. If they mirror each other, then we say that A is symmetric.
This means that one could place a mirror on the main diagonal: the (i, j)th element
is the same as the (j, i)th element:
ai,j = aj,i,
1 ≤i, j ≤n.
In other words, A remains unchanged under interchanging the roles of rows and
columns:
A = At.
1.5.2
The Identity Matrix
Here is an example of a symmetric matrix: the identity matrix of order n, denoted by
I. On the main diagonal, it is 1:
Ii,i ≡1,
1 ≤i ≤n.
Off the main diagonal, on the other hand, it is 0:
Fig. 1.8 In a square matrix
A of order n, the main
diagonal contains a1,1, a2,2,
. . ., an,n. If A is symmetric,
then the lower triangular part
mirrors the upper triangular
part: aj,i = ai,j

1.5 Square Matrix
19
Ii,j ≡0,
1 ≤i ̸= j ≤n.
In summary,
I ≡
⎛
⎜⎜⎜⎝
1
0
1
...
0
1
⎞
⎟⎟⎟⎠.
Off the main diagonal, all elements vanish. No need to write all these zeroes. A blank
space stands for a zero element.
Why is the identity matrix so special? Well, apply it to just any n-dimensional
vector v, and you’d see no effect whatsoever:
Iv = v.
Likewise, apply I to just any square matrix A of order n, and you’d see no effect
whatsoever:
IA = AI = A.
This is why I is also called the unit matrix.
1.5.3
The Inverse Matrix as a Mapping
The square matrix A may also have an inverse matrix: a new matrix A−1, satisfying
A−1A = I.
In this case, we say that A is nonsingular or invertible.
Thus, in the world of matrices, a nonsingular matrix plays the role of a nonzero
number, and its inverse plays the role of the reciprocal. Later on, we’ll see this in a
yet wider context: group theory.
Recall that A could also be viewed as a mapping of vectors:
v →Av.
How to map Av back to v? Easy: just apply A−1 to it:
Av →A−1(Av) =

A−1A

v = Iv = v.
This way, thanks to associativity, A−1 maps the vector Av back to v.

20
1
Vectors and Matrices
So, both A and A−1 could actually be viewed as mappings. This could be quite
useful. Indeed, let’s look at them the other way around: A−1 is now the original
mapping that maps
v →A−1v,
and A is its mirror: the inverse mapping that maps A−1v back to v:
A−1v →A

A−1v

= v.
In the language of matrices, this could be written simply as
AA−1 = I.
In summary, although matrices not necessarily commute, A and A−1 do commute
with each other.
1.5.4
Inverse and Transpose
The inverse matrix could be quite hard to calculate. Still, once calculated, it is useful
for many purposes.
WhatistheinverseofAt?Noneedtocalculateit!Afterall,A−1 isalreadyavailable.
To have the answer, just take its transpose. Indeed, from Sect.1.4.7,

A−1t At =

AA−1t = It = I.
In other words,

At−1 =

A−1t ,
as asserted. In summary, the inverse of the transpose is just the transpose of the
inverse. Thus, no parentheses are needed: one could simply write
A−t ≡

A−1t =

At−1 .
Finally, let A and B be two square matrices of order n. What is the inverse of the
product? It is just the product of the inverses, in the reverse order:
(AB)−1 = B−1A−1.
Indeed, thanks to associativity,
(AB)

B−1A−1
= A

B

B−1A−1
= A

BB−1
A−1
= A

IA−1
= AA−1 = I.

1.6 The Hermitian Adjoint
21
1.6
The Hermitian Adjoint
1.6.1
Complex Matrix and Its Hermitian Adjoint
Let’s go back to a rectangular matrix, which is not necessarily square. So far, we
have considered a real matrix, with real elements in R. Let’s extend the discussion
to an m × n complex matrix, with complex elements in C.
Fortunately, the same properties still hold, including the distributive and associa-
tive laws. The transpose, however, must be replaced by a more general notion: the
Hermitian adjoint.
Recall that the complex number
c ≡a + b
√
−1
has the complex conjugate
¯c ≡a −b
√
−1
(Sect.1.3.1). For a given complex matrix A, its Hermitian adjoint is a new matrix—the
conjugate transpose matrix. It can be obtained in two stages:
• take its transpose,
• and then take the complex conjugate of each individual element.
This makes a new n × m matrix, denoted by Ah (or A∗):
Ah
j,i ≡¯ai,j,
1 ≤i ≤m, 1 ≤j ≤n.
In particular, if A happens to be a real matrix, then the complex conjugate has no
effect whatsoever. In this case, the Hermitian adjoint is just the same as the transpose:
Ah = At.
This way, the Hermitian adjoint is indeed a natural extension of the notion of the
transpose to the wider set of complex matrices.
As in Sect.1.4.7, it is easy to see that
(Ah)h = A,
and that
(BA)h = AhBh,
provided that the number of columns in B is the same as the number of rows in A.

22
1
Vectors and Matrices
Fig. 1.9 In a Hermitian
matrix, the lower triangular
part is the complex conjugate
of the upper triangular part
1.6.2
Hermitian (Self-Adjoint) Matrix
Consider now a complex square matrix A, of order m = n. We say that A is Hermitian
(or self-adjoint) if it is the same as its Hermitian adjoint:
A = Ah.
In this case, the (i, j)th element is the complex conjugate of the (j, i)th element:
ai,j = Ah
i,j = ¯aj,i,
1 ≤i, j ≤n.
In this case, the lower triangular part is the complex conjugate of the upper triangular
part (Fig.1.9). This way, the main-diagonal elements must be real:
ai,i = ¯ai,i,
1 ≤i ≤n.
1.7
Inner Product and Norm
1.7.1
Inner Product
Let
u ≡
⎛
⎜⎜⎜⎝
u1
u2
...
un
⎞
⎟⎟⎟⎠and v ≡
⎛
⎜⎜⎜⎝
v1
v2
...
vn
⎞
⎟⎟⎟⎠
be two column vectors in Cn. They can also be viewed as narrow n × 1 “matrices”,
with just one column. This way, they also have their own Hermitian adjoint:
uh = (¯u1, ¯u2, . . . , ¯un) and vh = (¯v1, ¯v2, . . . , ¯vn) ,
which are 1 × n “matrices”, or n-dimensional row vectors.

1.7 Inner Product and Norm
23
Thus, uh has n “columns”, and v has n “rows”. Fortunately, this is the same number.
So, we can now go ahead and multiply uh times v, as in Sect.1.4.5. The result is a
new 1 × 1 “matrix”, or just a new (complex) scalar:
(u, v) ≡uhv =
n

j=1
¯ujvj.
This is called the scalar product, or the inner product of u and v.
And what would happen if we dropped the bar, and didn’t use the complex con-
jugate at all? This would produce the so-called “real” inner product:
utv =
n

j=1
ujvj.
In general, this is not necessarily a real number: it could be complex as well. Still, if
u and v are real vectors in Rn, then this number must be real: the same as the original
inner product. This is why it is called “real” inner product.
Let’s return now to the original inner product deﬁned above. Let c ∈C be some
complex number. Then, we could “pull” c out of parentheses, possibly with a bar on
top:
(cu, v) = ¯c(u, v),
and
(u, cv) = c(u, v).
Finally, the inner product is a skew-symmetric operation: interchanging u and v
introduces a bar on top. Indeed,
(v, u) =
n

j=1
¯vjuj
is the complex conjugate of (u, v).
1.7.2
Norm
What is the inner product of v with itself? Well, this is a real nonnegative number:
(v, v) =
n

j=1
¯vjvj =
n

j=1
|vj|2 ≥0.
Could it be zero? Only if v was the zero vector:

24
1
Vectors and Matrices
(v, v) = 0 ⇔v = 0.
Thus, (v, v) has a square root. Let’s use it to deﬁne the norm (or length, or magnitude)
of v:
∥v∥≡

(v, v).
This way,
∥v∥≥0,
and
∥v∥= 0 ⇔v = 0,
as expected from magnitude. Furthermore, every complex number c ∈C could be
“pulled” out of the ∥· ∥sign:
∥cv∥=

(cv, cv) =

¯cc(v, v) =

|c|2(v, v) = |c|

(v, v) = |c| · ∥v∥.
This way, if v is a nonzero vector, then it could be normalized. Indeed, since ∥v∥> 0,
one could pick c = 1/∥v∥, to produce the normalized unit vector v/∥v∥: the unique
vector of norm 1 that is proportional to v.
The norm ∥v∥deﬁned above is also called the l2-norm, and is also denoted by
∥v∥2, to distinguish it from other norms: the l1-norm
∥v∥1 ≡
n

i=1
|vi|,
and the l∞- norm (or the maximum norm)
∥v∥∞≡max
1≤i≤n |vi|.
Like the l2-norm deﬁned above, these norms also make sense: they vanish if and only
if v is the zero vector. Furthermore, every complex number c ∈C could be pulled
out:
∥cv∥1 = |c| · ∥v∥1, and ∥cv∥∞= |c| · ∥v∥∞.
In what follows, however, we mostly use the l2-norm. This is why we denote it simply
by ∥v∥for short, rather than ∥v∥2.
1.7.3
Inner Product and the Hermitian Adjoint
Assume again that A is an m×n rectangular complex matrix. Let u and v be complex
vectors of different dimensions: u is m-dimensional, and v is n-dimensional. This

1.7 Inner Product and Norm
25
way, Av is m-dimensional as well (Sect.1.4.4). We can now take the inner product
of two m-dimensional vectors:
(u, Av) ≡uhAv.
This is a well-deﬁned complex scalar (Sect.1.7.1).
Furthermore, Ah is an n × m rectangular complex matrix. This way, Ah can ba
applied to u: the result Ahu is n-dimensional. For this reason, the inner product
(Ahu, v) is a well-deﬁned complex scalar.
Recall that both u and v could also be viewed as narrow matrices. Thanks to
associativity (Sect.1.4.6), we now have
(u, Av) = uh(Av) = (uhA)v = (Ahu)hv = (Ahu, v).
Let’s use this result in a special case: a square matrix.
1.7.4
Inner Product and Hermitian Matrix
Let’s look at an interesting special case: m = n, so A is now a square matrix. Assume
that A is also Hermitian:
A = Ah
(Sect.1.6.2). In this case, the above formula reduces to
(u, Av) = (Au, v),
for every two n-dimensional complex vectors u and v.
Let us now go the other way around. Assume that we didn’t know yet that A
was Herminian. Instead we only knew that A was a square complex matrix of order
m = n, satisfying
(u, Av) = (Au, v)
for every two n-dimensional complex vectors u and v. Could we then conclude that
A was Hermitian?
Fortunately, yes. For this purpose, let’s use the above formula in a special case.
For each pair of natural numbers 1 ≤i, j ≤n, let u and v be standard unit vectors,
with one nonzero component only:
uk =
 1 if k = i
0 if k ̸= i
(1 ≤k ≤n)
vk =
 1 if k = j
0 if k ̸= j
(1 ≤k ≤n).

26
1
Vectors and Matrices
With this choice,
ai,j = (u, Av) = (Au, v) = ¯aj,i,
implying that A is indeed Hermitian (Fig.1.9).
1.8
Orthogonal and Unitary Matrix
1.8.1
Inner Product of Column Vectors
Assume again that A is an m × n rectangular complex matrix. For 1 ≤i ≤n, let v(i)
denote the ith column in A: an m-dimensional column vector. Let us show that, for
1 ≤i, j ≤n, the inner product of the ith column with the jth column is the same as
the (i, j)th element in AhA. Indeed,
(AhA)i,j =
m

k=1

Ah
i,k ak,j
=
m

k=1
¯ak,iak,j
=
m

k=1
¯v(i)
k v(j)
k
=

v(i), v(j)
.
This formula will be useful below.
1.8.2
Orthogonal and Orthonormal Column Vectors
Consider two complex vectors u and v, of the same dimension. We say that they are
orthogonal to each other if their inner product vanishes:
(u, v) = 0.
Furthermore, we also say that u and v are orthonormal if they are not only orthogonal
to each other but also have norm 1:
(u, v) = 0, and ∥u∥= ∥v∥= 1.

1.8 Orthogonal and Unitary Matrix
27
Consider again our m × n rectangular complex matrix, written column by column:
A ≡

v(1) | v(2) | · · · | v(n)
,
for some m ≥n. Assume that its columns are orthonormal:

v(i), v(j)
= 0,
1 ≤i, j ≤n, i ̸= j,
and
∥v(i)∥= 1,
1 ≤i ≤n.
In this case, what is the (i, j)th element in AhA? Well, it is either zero or one:
(AhA)i,j =

v(i), v(j)
=
 1 if i = j
0 if i ̸= j
(Sect.1.8.1), In other words, AhA is just the identity matrix of order n:
AhA = I.
As a result, A preserves the inner product of any two n-dimensional vectors u and v:
(Au, Av) = (AhAu, v = (Iu, v)) = (u, v)
(Sect.1.7.3). In particular, by picking u = v, A preserves norm as well:
∥Av∥2 = (Av, Av) = (v, v) = ∥v∥2.
Let us now go the other way around. Assume that we didn’t know yet that A had
orthogonal columns. Instead, we only knew that AhA was the identity matrix:
AhA = I.
Could we then conclude that A must also have orthonormal columns? Fortunately,
yes. Indeed, from Sect.1.8.1,

v(i), v(j)
= (AhA)i,j = Ii,j =
 1 if i = j
0 if i ̸= j,
as asserted. In summary, A has orthonormal columns if and only if
AhA = I
is the identity matrix of order n ≤m.

28
1
Vectors and Matrices
1.8.3
Projection Matrix
So far, we’ve studied the product AhA. Now, let’s multiply the other way around: AAh.
This could be a completely different matrix: after all, the commutative law doesn’t
apply to matrices. Still, is AAh special in any way?
Well, with the above assumptions (m ≥n and A has orthonormal columns), it
indeed is. To see how, multiply the above equation by A from the left:
A

AhA

= AI = A.
Next, multiply this new equation by Ah from the right:

A

AhA

Ah = AAh.
Thanks to the associative law (Sect.1.4.6), this equation can also be written as

AAh 
AAh
= AAh.
Thus, AAh is a projection matrix: if you square it, it still remains the same.
1.8.4
Unitary and Orthogonal Matrix
Assume now that A is a square matrix of order m = n. If A also has orthonormal
columns, then A is called a unitary matrix. In this case, the above formulas tell us
that
AhA = AAh = I
is the identity matrix of order n. This could also be written in terms of the inverse
matrix:
Ah = A−1
and A =

Ah−1 .
Thanks to Sect.1.5.4, the inverse of the transpose is just the transpose of the inverse.
Therefore, one could drop these parentheses, and simply write
A = A−h.
If A is also real, then it is also called an orthogonal matrix. In this case,
AtA = AAt = I,
so
At = A−1 and A = (At)−1 = A−t.

1.9 Eigenvalues and Eigenvectors
29
1.9
Eigenvalues and Eigenvectors
1.9.1
Eigenvectors and Eigenvalues
Let A be a square complex matrix of order n. An eigenvector of A is a nonzero vector
v ∈Cn satisfying
Av = λv,
for some scalar λ ∈C. In other words, applying A to v has the same effect as
multiplying v by λ. The number λ is then called the eigenvalue of A associated with
the eigenvector v.
Note that, for every nonzero number c ∈C, cv is a legitimate eigenvector as well:
A(cv) = cAv = cλv = λ(cv)
(Sect.1.4.4). Thus, the eigenvector associated with λ is not deﬁned uniquely, but
only up to a (nonzero) scalar multiple.
What is the best way to pick c? Well, since v ̸= 0, ∥v∥> 0 (Sect.1.7.2). Thus, best
pick c ≡1/∥v∥. This would “normalize” v, and produce a “new” unit eigenvector:
A
 v
∥v∥

= λ
 v
∥v∥

,

v
∥v∥
 = 1.
This is the unique unit eigenvector proportional to v.
1.9.2
Singular Matrix and Its Null Space
What are the algebraic properties of the eigenvector v? First of all, it is nonzero:
v ̸= 0,
where 0 is the n-dimensional zero vector. Furthermore, v also satisﬁes
(A −λI) v = λv −λv = 0,
where I is the identity matrix of order n. Thus, the matrix A −λI maps v to 0. In
other words, v is in the null space of A −λI.
For this reason, A−λI must be singular (not invertible). Indeed, by contradiction:
if there were an inverse matrix (A−λI)−1, then we could apply it to the zero vector, to
map it back to v. On the other hand, from the very deﬁnition of matrix-times-vector,
this must be the zero vector:

30
1
Vectors and Matrices
v = (A −λI)−10 = 0,
in violation of the very deﬁnition of v as a nonzero eigenvector. Thus, A −λI must
indeed be singular, as asserted. Let’s use this to design an eigenvalue for Ah as well.
1.9.3
Eigenvalues of the Hermitian Adjoint
So far, we looked at the matrix A −λI. What about its Hermitian adjoint
(A −λI)h = Ah −¯λI?
Well,itissingularaswell.Indeed,bycontradiction:ifitwerenonsingular(invertible),
then there would exist some nonzero n-dimensional vector u, mapped to v:

Ah −¯λI

u = v.
This would lead to a contradiction:
0 = (0, u) = ((A −λI) v, u) =

v,

Ah −¯λI

u

= (v, v) > 0.
So, Ah −¯λI must be singular as well. As such, it must map some nonzero vector
w ̸= 0 to the zero vector:

Ah −¯λI

w = 0,
or
Ahw = ¯λw.
Thus, ¯λ must be an eigenvalue of Ah. In summary, the complex conjugate of any
eigenvalue of A is an eigenvalue of Ah.
1.9.4
Eigenvalues of a Hermitian Matrix
Assume now that A is also Hermitian:
A = Ah
(Sect.1.6.2). From Sect.1.7.4, it then follows that
λ(v, v) = (v, λv) = (v, Av) = (Av, v) = (λv, v) = ¯λ(v, v).

1.9 Eigenvalues and Eigenvectors
31
Now, because v is a nonzero vector, (v, v) > 0 (Sect.1.7.2). By dividing by (v, v),
we then obtain
λ = ¯λ,
so λ is real:
λ ∈R.
In summary, a Hermitian matrix has real eigenvalues only. As a matter of fact, in the
world of matrices, a Hermitian matrix plays the role of a real number.
1.9.5
Eigenvectors of a Hermitian Matrix
So far, we’ve discussed the eigenvalues of a Hermitian matrix, and saw that they
must be real. What about the eigenvectors? What are their algebraic properties?
To answer this, let u and v be two eigenvectors of the Hermitian matrix A:
Av = λv and Au = μu,
where μ ̸= λ are two distinct eigenvalues. What is the relation between u and v?
Well, it turns out that they are orthogonal to each other. Indeed, thanks to Sects.1.7.4
and 1.9.4,
μ(u, v) = ¯μ(u, v) = (μu, v) = (Au, v) = (u, Av) = (u, λv) = λ(u, v).
Now, since μ ̸= λ, we must have
(u, v) = 0,
as asserted. Furthermore, we can normalize both u and v, to obtain the orthonormal
eigenvectors u/∥u∥and v/∥v∥. This will be useful later. Let’s see some interesting
examples.
1.10
The Sine Transform
1.10.1
Discrete Sine Wavesx
An interesting example is the discrete sine wave. To obtain it, just sample the sine
function (Fig.1.10).

32
1
Vectors and Matrices
Fig. 1.10 The smoothest sine wave: sin(πx). To obtain the discrete sine mode, just sample at n
discrete points: x = 1/(n + 1), x = 2/(n + 1), x = 3/(n + 1), . . ., x = n/(n + 1)
For a ﬁxed 1 ≤j ≤n, consider the sine mode (or wave, or oscillation)
sin(jπx).
What is the role of j? Well, j is the wave number: it tells us how fast the wave oscillates.
In fact, as j increases, the above function oscillates more and more rapidly. For a small
j, the function is smooth, and oscillates just a little. For a greater j, on the other hand,
the wave oscillates more rapidly: move x just a little, and you may already see an
oscillation. In other words, if x measures the time, then the wave oscillates more
frequently. This is why j is called the wave number, or the frequency.
The above sine mode is continuous: it is a function of every x ∈R. How to obtain
the discrete sine mode? For this purpose, do two things:
• Discretize: sample the original sine mode at n equidistant points between 0 and 1:
x =
1
n + 1,
2
n + 1,
3
n + 1, . . . ,
n
n + 1.
This produces the n-dimensional column vector
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
sin

jπ
n+1

sin

2jπ
n+1

sin

3jπ
n+1

...
sin

njπ
n+1

⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.

1.10 The Sine Transform
33
• Normalize: multiply by √2/(n + 1), to produce the new column vector
v(j) ≡

2
n + 1
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
sin

jπ
n+1

sin

2jπ
n+1

sin

3jπ
n+1

...
sin

njπ
n+1

⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
This is the discrete sine mode. Thanks to the above normalization, it has norm 1:
∥v(j)∥= 1
(see exercises below). Furthermore, as we’ll see next, the discrete sine modes are
orthogonal to each other.
1.10.2
Orthogonality of the Discrete Sine Waves
Are the discrete sine modes orthogonal to each other? Yes! Indeed, they are the
eigenvectors of a new symmetric matrix T. T is tridiagonal: it has nonzero elements
on three diagonals only— the main diagonal, the diagonal just above it, and the
diagonal just below it:
T ≡tridiag(−1, 2, −1) ≡
⎛
⎜⎜⎜⎜⎜⎝
2 −1
−1 2 −1
... ... ...
−1 2 −1
−1 2
⎞
⎟⎟⎟⎟⎟⎠
.
The rest of the elements, on the other hand, vanish. No need to write them explicitly
– the blank spaces stand for zeroes.
As a real symmetric matrix, T is also Hermitian:
T h = T t = T.
What are its eigenvectors? The discrete sine modes! To see this, recall the formula:
sin(θ + φ) + sin(θ −φ) = 2 sin(θ) cos(φ).
To study the ith component in v(j), set in this formula
θ =
ijπ
n + 1 and φ =
jπ
n + 1.

34
1
Vectors and Matrices
This gives
sin
(i + 1)jπ
n + 1

+ sin
(i −1)jπ
n + 1

= 2 sin
 ijπ
n + 1

cos
 jπ
n + 1

.
By doing this for all components 1 ≤i ≤n, we have
Tv(j) = λjv(j),
where
λj ≡2 −2 cos
 jπ
n + 1

= 4 sin2

jπ
2(n + 1)

.
In summary, T is a Hermitian matrix, with n distinct eigenvalues. From Sect.1.9.5,
its eigenvectors are indeed orthogonal to each other.
1.10.3
The Sine Transform
Now, let’s place the discrete sine modes in a new matrix, column by column:
W ≡

v(1) | v(2) | · · · | v(n)
.
By now, we already know that W is orthogonal. From Sect.1.8.4, we therefore have
W −1 = W h = W t = W.
W is called the sine transform: it transforms each n-dimensional vector u to a new
vector of the same norm:
∥Wu∥= ∥u∥.
To uncover the original vector u, just apply W once again:
u = W(Wu).
1.10.4
Diagonalization
Now, let’s place the eigenvalues of T in a new diagonal matrix:
Λ ≡diag(λi)n
i=1 ≡diag(λ1, λ2, . . . , λn) ≡
⎛
⎜⎜⎜⎝
λ1
λ2
...
λn
⎞
⎟⎟⎟⎠.

1.10 The Sine Transform
35
Recall that the columns of W are the eigenvectors of T:
TW = WΛ,
or
T = WΛW −1 = WΛW.
This is called the diagonal form (or the diagonalization, or the spectral decomposi-
tion) of T. This is how the sine transform helps diagonalize the original matrix T.
1.10.5
Sine Decomposition
The sine transform W is a real, symmetric, and orthogonal matrix:
W 2 = W tW = W hW = W −1W = I.
For this reason, every vector u ∈Cn can be written uniquely as a linear combination
of the columns of W, with coefﬁcients that are just the components of Wu:
u = Iu = W 2u = W(Wu) =
n

j=1
(Wu)jv(j).
This way, u is decomposed in terms of more and more frequent (or oscillatory) waves,
each multiplied by the corresponding amplitude (Wu)j.
For example, what is the smoothest part of u? It is now ﬁltered out, in terms of
the ﬁrst discrete wave, times the ﬁrst amplitude:
(Wu)1v(1).
What is the next (more oscillatory) part? It is
(Wu)2v(2),
and so on, until the most oscillatory term
(Wu)nv(n).
1.10.6
Multiscale Decomposition
The above can be viewed as a multiscale decomposition. The ﬁrst discrete wave can
be viewed as the coarsest scale, in which the original vector u is approximated rather
poorly. The remainder (or the error) is

36
1
Vectors and Matrices
u −(Wu)1v(1).
This is approximated by the second discrete wave, on the next ﬁner scale. This
contributes a ﬁner term, to produce a better approximation of u. The up-to-date
remainder is then
u −(Wu)1v(1) −(Wu)2v(2).
This is approximated by the third discrete wave, on the next ﬁner scale, and so on.
In the end, the most oscillatory (ﬁnest) term is added as well, to complete the entire
multiscale decomposition. This is no longer an approximation: it is exactly u.
1.11
The Cosine Transform
1.11.1
Discrete Cosine Waves
Likewise, one could also deﬁne cosine waves. For a ﬁxed 1 ≤j ≤n, consider the
function
cos((j −1)πx).
To discretize, just sample the above cosine function at n equidistant points between
0 and 1:
x = 1
2n, 3
2n, 5
2n, . . . , 2n −1
2n
.
This produces a new v(j): the discrete cosine mode, whose components are now
v(j)
i
≡cos

i −1
2

(j −1)π
n

,
1 ≤i ≤n
(Fig.1.11). The normalization is left to the exercises below.
1.11.2
Orthogonality of the Discrete Cosine Waves
To show orthogonality, redeﬁne T at its upper left and lower right corners:
T1,1 ≡Tn,n ≡1
rather than 2. This way, T takes the new form

1.11 The Cosine Transform
37
Fig. 1.11 The smoothest (nonconstant) cosine wave: cos(πx). Sample it at n discrete points: x =
1/(2n), x = 3/(2n), x = 5/(2n), . . ., x = (2n −1)/(2n)
T ≡
⎛
⎜⎜⎜⎜⎜⎝
1 −1
−1 2 −1
... ... ...
−1 2 −1
−1 1
⎞
⎟⎟⎟⎟⎟⎠
.
Although the new matrix T is still real, tridiagonal, and symmetric, its eigenvectors
are completely different from those in Sect.1.10.2: they are now the discrete cosine
waves. To see this, recall the formula:
cos(θ + φ) + cos(θ −φ) = 2 cos(θ) cos(φ).
In particular, set
θ =

i −1
2

(j −1)π
n and φ = (j −1)π
n .
With these θ and φ, the above formula takes the form:
cos

i + 1
2

(j −1)π
n

+ cos

i −3
2

(j −1)π
n

= 2 cos

i −1
2

(j −1)π
n

cos

(j −1)π
n

.
Thus, the discrete cosine wave deﬁned in Sect.1.11.1 satisﬁes
Tv(j) = λjv(j),

38
1
Vectors and Matrices
with the new eigenvalue
λj = 2 −2 cos

(j −1)π
n

= 4 sin2 
(j −1) π
2n

.
Because T is symmetric, its eigenvectors are orthogonal to each other (Sect.1.9.5).
This proves orthogonality of the discrete cosine modes, as asserted.
1.11.3
The Cosine Transform
We are now ready to normalize:
v(j) ←
v(j)
v(j)
(see exercises below). Let’s place these (normalized) cosine modes in a new matrix,
column by column:
W ≡

v(1) | v(2) | · · · | v(n)
.
This new W, known as the cosine transform, is still real and orthogonal, but no longer
symmetric:
W −1 = W h = W t ̸= W.
1.11.4
Diagonalization
For this reason, our new T is diagonalized in a slightly different way:
T = WΛW −1 = WΛW t ̸= WΛW,
where Λ is now a new diagonal matrix, containing the new eigenvalues:
Λ ≡diag (λ1, λ2, . . . , λn) .
1.11.5
Cosine Decomposition
Furthermore, every vector u ∈Cn can now be decomposed uniquely in terms of
discrete cosine waves as well:

1.11 The Cosine Transform
39
u = Iu = (W W t)u = W(W tu) =
n

j=1
(W tu)jv(j).
This is called the cosine decomposition of u.
1.12
Positive (Semi)Deﬁnite Matrix
1.12.1
Positive Semideﬁnite Matrix
Consider again a general Hermitian matrix A. Assume that, for every (complex)
vector v,
(v, Av) ≡vhAv ≥0.
We then say that A is positive semideﬁnite.
In particular, we could pick v to be an eigenvector of A, with the eigenvalue λ.
This would give
∥v∥2λ ≥0.
Thus, all eigenvalues of A must be nonnegative.
This also works the other way around: if all eigenvalues of a Hermitian matrix
are nonnegative, then it must be positive semideﬁnite. Indeed, given a complex vec-
tor v, just decompose it as a linear combination of the (orthogonal) eigenvectors.
Fortunately, we’ve already seen an example of a positive semideﬁnite matrix: T in
Sect.1.11.2, whose all eigenvalues are nonnegative.
1.12.2
Positive Deﬁnite Matrix
Assume now that A is also nonsingular. This means that 0 is no eigenvalue. In this
case, for every nonzero vector v ̸= 0, not only
vhAv ≥0
but also
vhAv > 0.
We then say that A is not only positive semideﬁnite but also positive deﬁnite. In
particular, pick v as an eigenvector of A, with the eigenvalue λ. In this case,
∥v∥2λ > 0.

40
1
Vectors and Matrices
This shows that all eigenvalues of A are positive.
This also works the other way around: if all eigenvalues of a Hermitian matrix
are positive, then it must be positive deﬁnite. Indeed, given a nonzero vector v, just
decompose it as a linear combination of the (orthogonal) eigenvectors. Fortunately,
we’ve already seen an example of a positive deﬁnite matrix: T in Sect.1.10.2, whose
all eigenvalues are positive.
1.13
Exercises
1.13.1
The Cauchy–Schwarz Inequality
1. Let u and v be n-dimensional (complex) vectors:
u ≡(u1, u2, . . . , un)t and v ≡(v1, v2, . . . , vn)t.
Prove the Cauchy–Schwarz inequality:
|(u, v)| ≤∥u∥· ∥u∥,
or

n

i=1
¯uivi

2
= |(u, v)|2 ≤(u, u)(v, v) =
 n

i=1
|ui|2
  n

i=1
|vi|2

.
Hint: pick two different indices between 1 and n: 0 ≤i ̸= j ≤n. Note that
0 ≤|uivj −ujvi|2
=

¯ui ¯vj −¯uj ¯vi
 
uivj −ujvi

= ¯ui ¯vjuivj −¯ui ¯vjujvi −¯uj ¯viuivj + ¯uj ¯viujvi
= |ui|2|vj|2 −(¯uivi)(uj ¯vj)) −(¯ujvj)(ui ¯vi) + |uj|2|vi|2,
so
(¯uivi)(uj ¯vj)) + (¯ujvj)(ui ¯vi) ≤|ui|2|vj|2 + |uj|2|vi|2.
Do the same with the plus sign, to obtain
(¯uivi)(uj ¯vj)) + (¯ujvj)(ui ¯vi)
 ≤|ui|2|vj|2 + |uj|2|vi|2.
2. Could the Cauchy–Schwarz inequality be an exact equality as well? Hint: only if
u is proportional to v (or is a scalar multiple of v).
3. Conclude the triangle inequality:

1.13 Exercises
41
∥u + v∥≤∥u∥+ ∥v∥.
Hint:
∥u + v∥2 = (u + v, u + v)
= (u, u) + (u, v) + (v, u) + (v, v)
= ∥u∥2 + (u, v) + (v, u) + ∥v∥2
≤∥u∥2 + 2|(u, v)| + ∥v∥2
≤∥u∥2 + 2∥u∥· ∥v∥+ ∥v∥2
= (∥u∥+ ∥v∥)2 .
1.13.2
Generalized Eigenvalues and Eigenvectors
1. Let A be a square complex matrix of order n. Let B be a Hermitian matrix of
order n. An n-dimensional vector v is called a generalized eigenvector of A if
Av = λBv and
(v, Bv) ̸= 0.
The scalar λ is then called a generalized eigenvalue of A.
2. Could v be the zero vector?
3. Conclude that A −λB maps v to the zero vector.
4. Conclude that v is in the null space of A −λB.
5. Is A −λB singular? Hint: otherwise,
v = (A −λB)−1 0 = 0,
in violation of (v, Bv) ̸= 0.
6. Is Ah −¯λB singular? Hint: otherwise, there would be a nonzero vector u mapped
to v:

Ah −¯λB

u = v,
which would lead to a contradiction:
0 = (0, u) = ((A −λB) v, u) =

v,

Ah −¯λB

u

= (v, v) > 0.
7. Conclude that Ah −¯λB has a nontrivial null space.
8. Conclude that ¯λ is a generalized eigenvalue of Ah.
9. Assume now that A is Hermitian as well. Must the generalized eigenvalue λ be
real? Hint:
λ(v, Bv) = (v, λBv) = (v, Av) = (Av, v) = (λBv, v) = ¯λ(Bv, v) = ¯λ(v, Bv).

42
1
Vectors and Matrices
Finally, divide this by (v, Bv) ̸= 0.
10. Let u and v be two generalized eigenvectors:
Av = λBv and Au = μBu,
where λ ̸= μ are two distinct generalized eigenvalues. Show that
(u, Bv) = 0.
Hint:
μ(u, Bv) = μ(Bu, v)
= ¯μ(Bu, v)
= (μBu, v)
= (Au, v)
= (u, Av)
= (u, λBv)
= λ(u, Bv).
1.13.3
Root of Unity and Fourier Transform
1. Let n be some natural number. Deﬁne the complex number
w ≡exp

2π√−1
n

.
Show that w can also be written as
w = cos
2π
n

+
√
−1 sin
2π
n

.
2. Show that w is the nth root of unity:
wn = expn

2π√−1
n

= exp

2π√−1
n
n

= exp(2π
√
−1) = 1.
3. Look at Fig.1.12. How many roots of unity are there in it? Hint: for each 1 ≤
j < n,

wjn = wjn = wnj =

wnj = 1j = 1.
4. Use w and its powers to design a new n × n complex matrix:

1.13 Exercises
43
Fig. 1.12 The nth root of unity, and its powers in the complex plane: w, w2, w3, . . ., wn = 1
W ≡n−1/2 
w(i−1)(j−1)
1≤i,j≤n .
5. Let 1 ≤j ≤n be ﬁxed. Show that the jth column in W could also be obtained
from the exponent wave
exp(2π
√
−1(j −1)x),
sampled the at n equidistant points
x = 0, 1
n, 2
n, 3
n, . . . , n −1
n
,
and normalized by n1/2.
6. Show that the Hermitian adjoint of W is just the complex conjugate:
W h = ¯W.
7. Show that the ﬁrst column in W is the constant column vector
n−1/2(1, 1, 1, . . . , 1)t.
8. Show that this is a unit vector of norm 1.
9. Show that this is indeed the ﬁrst discrete cosine wave in Sect.1.11.1 (in its
normalized form).

44
1
Vectors and Matrices
10. Show that this eigenvector has the zero eigenvalue in Sect.1.11.2.
11. Conclude that the tridiagonal matrix T deﬁned in Sect.1.11.2 is singular (not
invertible).
12. Show that, in the new matrix W deﬁned above, every column is a unit vector of
norm 1 as well.
13. Show that, for 1 < j ≤n, the jth column in W sums to zero:
n

i=1
w(i−1)(j−1) =
n−1

i=0
wi(j−1) = 1 −wn(j−1)
1 −wj−1
=
1 −1
1 −wj−1 = 0.
14. Multiply the above equation by w(j−1)/2, to obtain
n

i=1
w(i−1/2)(j−1) = 0
(1 < j ≤n).
15. Look at the real part of this equation, to conclude that
 n

i=1
cos2

i −1
2

(j −1)π
n

−
 n

i=1
sin2

i −1
2

(j −1)π
n

=
n

i=1

cos2

i −1
2

(j −1)π
n

−sin2

i −1
2

(j −1)π
n

=
n

i=1
cos

i −1
2

(j −1)2π
n

= 0.
(1 < j ≤n).
16. In the uniform grid in Sect.1.11.1, sum the squares of sines and cosines:
 n

i=1
cos2

i −1
2

(j −1)π
n

+
 n

i=1
sin2

i −1
2

(j −1)π
n

=
n

i=1

cos2

i −1
2

(j −1)π
n

+ sin2

i −1
2

(j −1)π
n

=
n

i=1
1
= n
(1 < j ≤n).

1.13 Exercises
45
17. Add these two formulas to each other, to conclude that the discrete cosine waves
in Sect.1.11.1 have norm
v(j) =
 √n
if j = 1
 n
2
if j > 1.
18. Show that the eigenvalues in Sect.1.11.2 are distinct.
19. Conclude that the discrete cosine waves are indeed orthogonal to each other.
20. Rewrite the zero column sums in W
n

i=1
w(i−1)(j−1) = 0
(1 < j ≤n)
in the simpler form
n−1

i=0
wij = 0
(1 ≤j < n),
or
n−1

i=1
wij = −1
(1 ≤j < n).
21. Look at the real part of this equation, to conclude that
−1 =
n−1

i=1
cos
2ijπ
n

=
n−1

i=1

cos2
ijπ
n

−sin2
ijπ
n

=
n−1

i=1
cos2
ijπ
n

−
n−1

i=1
sin2
ijπ
n

,
(1 ≤j < n).
22. Substitute n + 1 for n in the above equation, to read
 n

i=1
cos2
 ijπ
n + 1

−
 n

i=1
sin2
 ijπ
n + 1

= −1
(1 ≤j ≤n).
23. In the uniform grid in Sect.1.10.1, sum the squares of sines and cosines:

46
1
Vectors and Matrices
 n

i=1
cos2
 ijπ
n + 1

+
 n

i=1
sin2
 ijπ
n + 1

=
n

i=1

cos2
 ijπ
n + 1

+ sin2
 ijπ
n + 1

=
n

i=1
1
= n
(1 ≤j ≤n).
24. Subtract these two formulas from each other, to conﬁrm that the discrete sine
waves introduced in Sect.1.10.1 are indeed unit vectors of norm 1.
25. Show that the eigenvalues in Sect.1.10.2 are distinct.
26. Conclude that the discrete sine waves are indeed orthonormal.
27. In Sect.1.10.2, in the n × n matrix T, redeﬁne the elements in the upper right
and lower left corners as
T1,n ≡Tn,1 ≡−1
rather than the original deﬁnition
T1,n ≡Tn,1 ≡0.
This way, T is no longer tridiagonal, but periodic:
T ≡
⎛
⎜⎜⎜⎜⎜⎝
2 −1
−1
−1 2 −1
... ... ...
−1 2 −1
−1
−1 2
⎞
⎟⎟⎟⎟⎟⎠
,
or
Ti,j =
⎧
⎨
⎩
2
if i = j
−1
if |i −j| = 1 or |i −j| = n −1
0
otherwise
(1 ≤i, j ≤n). Show that this new T is no longer tridiagonal.
28. Show that, in this new T, the rows sum to zero.
29. Conclude that the ﬁrst column of W (the constant n-dimensional unit vector) is
an eigenvector of this new T, with the zero eigenvalue.
30. Conclude that this new T is singular.
31. More generally, show that the jth column of W (1 ≤j ≤n) is an eigenvector of
this new T, with the new eigenvalue

1.13 Exercises
47
λj = 2 −

wj−1 + w−(j−1)
= 2 −2 cos
(j −1)2π
n

= 4 sin2
(j −1)π
n

.
32. Use the above to form the matrix equation
TW = WΛ,
where Λ is now the new n × n diagonal matrix
Λ ≡diag(λ1, λ2, . . . , λn).
33. So far, we’ve seen that T has complex eigenvectors: the columns of W. Does it
have real eigenvectors as well?
34. Design them! Hint: follow the exercises below, one by one.
35. For this purpose, look at the jth column of W. Look at its real part. Is it an
eigenvector of T in its own right? Hint: Yes! After all, T and λj are real.
36. What is its eigenvalue? Hint: λj.
37. Look again at the jth column of W. Look at its imaginary part. Is it an eigenvector
of T in its own right? Hint: Yes! After all, T and λj are real.
38. What is its eigenvalue? Hint: λj.
39. Are the above eigenvalues different from each other? Hint: most of them are.
Only for 1 ≤j < n/2 is the (j + 1)st eigenvalue the same as the (n −j + 1)st
one:
λn−j+1 = 4 sin2
(n −j)π
n

= 4 sin2

π −jπ
n

= 4 sin2
jπ
n

= λj+1.
40. Conclude that the (j + 1)st and the (n −j + 1)st columns of W are eigenvectors
of T, with the same eigenvalue.
41. Show that these column vectors are the complex conjugate of each other.
42. Conclude that their sum is twice their real part, which is an eigenvector of T as
well, with the same eigenvalue: λj+1.
43. Conclude also that the difference between them is 2√−1 times their imaginary
part, which is an eigenvector of T as well, with the same eigenvalue: λj+1.
44. Show that those columns of W that correspond to different eigenvalues are indeed
orthogonal to each other. Hint: T is symmetric (Sect.1.9.5).
45. Show that the (j + 1)st and the (n −j + 1)st columns of W, although having the
same eigenvalue λj+1, are orthogonal to each other as well:
n

i=1
¯w(n−j)(i−1)wj(i−1) =
n−1

i=0
¯w(n−j)iwji
=
n−1

i=0
w(j−n)iwji

48
1
Vectors and Matrices
=
n−1

i=0
w(j−n+j)i
=
n−1

i=0
w2ji
= 1 −w2jn
1 −w2j
=
1 −1
1 −w2j
= 0
(1 ≤j < n/2).
46. Use a similar calculation to verify directly that every two columns in W are
indeed orthogonal to each other. Hint: for every 1 ≤k ̸= j ≤n,
n

i=1
¯w(k−1)(i−1)w(j−1)(i−1) =
n−1

i=0
¯w(k−1)iw(j−1)i
=
n−1

i=0
w(1−k)iw(j−1)i
=
n−1

i=0
w(j−k)i
= 1 −w(j−k)n
1 −wj−k
=
1 −1
1 −wj−k
= 0.
47. Show that the columns of W are also unit vectors of norm 1.
48. Conclude that the columns of W are orthonormal.
49. Conclude that W is a unitary matrix. (W is known as the discrete Fourier trans-
form.)
50. Verify that W indeed satisﬁes
W hW = ¯W tW = ¯W W = I
and
W W h = W ¯W t = W ¯W = I,
where I is the n × n identity matrix.

1.13 Exercises
49
51. Multiply the matrix equation
TW = WΛ
by W −1 from the right, to obtain our new T in its diagonal form:
T = WΛ ¯W.
52. Write an efﬁcient algorithm to calculate Wu, for any given vector u ∈Cn. The
solution can be found in Chap.5 in [61].
53. Let K ≡(Ki,j)1≤i,j≤n be the n × n matrix with 1’s on the secondary diagonal
(from the upper right to the lower left corner), and 0’s elsewhere:
Ki,j =
 1 if i + j = n + 1
0
otherwise.
Show that K is both symmetric and orthogonal.
54. Conclude that
K2 = KtK = I.
55. Verify that K2 = I by a direct calculation.
56. Conclude that K is a projection matrix.

Chapter 2
Vector Product with Applications
in Geometrical Mechanics
How to use vectors and matrices? Well, we’ve already seen a few important
applications: the sine, cosine, and Fourier transforms. Here, on the other hand, we use
matrices and their determinant to introduce yet another practical operation: vector
product in 3-D. This will help deﬁne angular momentum and velocity, and establish
the relation between them.
2.1
The Determinant
2.1.1
Minors and the Determinant
For a real square matrix, the determinant is a real function: it maps the original
matrix to a real number. For a complex matrix, on the other hand, the determinant
is a complex function: it maps the original matrix to a new complex number: its
determinant. To deﬁne the determinant, we must ﬁrst deﬁne the minor.
Let A be a square matrix of order n > 1. Let 1 ≤i, j ≤n be two ﬁxed indices.
Deﬁne a slightly smaller (n −1) × (n −1) matrix: just drop from A its ith row and
jth column. The result is indeed a smaller matrix: just n −1 rows and n −1 columns.
This is the (i, j)th minor of A, denoted by A(i, j).
Thanks to the minors, we can now go ahead and deﬁne the determinant recursively.
If A is very small and contains one entry only, then its determinant is just this entry. If,
on the other hand, A is bigger than that, then its determinant is a linear combination
of the determinants of those minors obtained by dropping the ﬁrst row:
det(A) ≡

a1,1
if n = 1
n
j=1(−1) j+1a1, j det

A(1, j)
if n > 1.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_2
51

52
2
Vector Product with Applications in Geometrical Mechanics
This kind of recursion could also be viewed as mathematical induction on n =
1, 2, 3, . . .. Indeed, for n = 1, det(A) is just the only element in A: det(A) ≡a1,1.
For n > 1, on the other hand, the minors are smaller matrices of order n −1, whose
determinant has already been deﬁned in the induction hypothesis, and can be used
to deﬁne det(A) as well. This completes the induction step, and indeed the entire
deﬁnition, as required. Later on, we’ll see yet another (equivalent) deﬁnition.
2.1.2
Examples
For example, let I be the identity matrix of order n. Then,
det(I) = 1.
This could be proved easily by mathematical induction. After all, in the above for-
mula, all minors vanish, except for the (1, 1)st one: the (n −1) × (n −1) identity
matrix.
For yet another example, let α be some scalar. Then,
det(αI) = αn.
This could be proved by mathematical induction as well. After all, in the above
formula, all minors vanish, except for the (1, 1)st one.
Another interesting example is the so-called switch matrix:
det
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1
1
1
1
...
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
= −1.
Why is it called a switch matrix? Because once applied to a vector, it interchanges
its ﬁrst and second components. Again, most of its minors vanish: only the (1, 2)nd
minor is nonzero—the (n −1) × (n −1) identity matrix. To contribute to the deter-
minant, it must pick a minus sign.
As a ﬁnal example, look at the special case of n = 2. In this case, we have a small
2 × 2 matrix. Its determinant is
det
a b
c d

= ad −bc.

2.1 The Determinant
53
Indeed, there are here two nonzero minors: the (1, 1)st minor is just the lower right
element d. To contribute to the determinant, it must be multiplied y a. The (1, 2)nd
minor, on the other hand, is the lower left element c. To contribute to the determinant,
it must be multiplied by b, and pick a minus sign.
Later on, we’ll interpret this determinant geometrically: the area of the parallel-
ogram that the columns (or the rows) of the 2 × 2 matrix make in the Cartesian
plane (this chapter, Sect.2.3.3). This will be quite useful in special relativity later on
(Chap.4).
2.1.3
Algebraic Properties
Let’s discuss some general properties of the determinant, to be proved later in the
book. Let A and B be square matrices of order n. The determinant of the product is
the product of the individual determinants:
det(AB) = det(A) det(B).
(This will be proved in Chap.14, Sect.14.6.3.) This is quite useful. For example,
what happens when two rows in A interchange? The determinant just picks a minus
sign. For instance, to interchange the ﬁrst and second rows in A, just apply the above
switch matrix:
det
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1
1
1
1
...
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
A
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
= det
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1
1
1
1
...
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
det(A) = −det(A).
For this reason, if both rows are the same, then the determinant must vanish:
det(A) = −det(A) = 0.
In this case, A must be singular: it has no inverse. Fortunately, this is a rather rare
case. More often, A has a nonzero determinant and is therefore invertible, as we’ll
see below.
Another useful property is that the transpose has the same determinant:
det

At
= det(A).
(This will be proved in Chap.14, Sect.14.6.2.) Thus, to calculate the determinant,
we could equally well work with columns rather than rows.

54
2
Vector Product with Applications in Geometrical Mechanics
det(A) =

a1,1
if n = 1
n
i=1(−1)i+1ai,1 det

A(i,1)
if n > 1.
This will be useful below.
Thanks to these two algebraic properties, we also have a third one: if Q is a real
orthogonal matrix, then
2
det(Q) = det

Qt
det(Q) = det

Qt Q

= det(I) = 1,
so
det(Q) = ±1.
Later on in this chapter, we’ll pick the correct sign. Before going into this, let’s use
the determinant of a nonsingular matrix to design its inverse (Chap.1, Sect.1.5.3).
2.1.4
The Inverse Matrix in Its Explicit Form
If det(A) ̸= 0, then A is nonsingular: it has an inverse matrix. Indeed, in this case,
det(A) could be used to deﬁne A−1 explicitly. In fact, each individual element in
A−1 is given in terms of the transpose minor:

A−1
i, j = (−1)i+ j det

A( j,i)
det(A)
,
1 ≤i, j ≤n.
To check on this formula, let’s use it to calculate a few elements in A−1A = I. Let’s
start from the upper left element:

A−1 A

1,1 =
n

j=1

A−1
1, j a j,1 =
1
det(A)
n

j=1
(−1)1+ j det

A( j,1)
a j,1 = det(A)
det(A) = 1
(Sect.2.1.3). This is indeed as required.
Next, let’s check an off-diagonal element as well. For this purpose, let’s design a
new matrix B as follows. B is nearly the same as A. Only its ﬁrst column is different:
it is the same as the second one. This way, both the ﬁrst and second columns in
B are the same as the second column in A. Thus, B must have a zero determinant
(Sect.2.1.3). Moreover, in B, most minors are the same as in A. So, we could work
with B rather than A:

A−1A

1,2 =
n

j=1

A−1
1, j a j,2

2.1 The Determinant
55
=
1
det(A)
n

j=1
(−1)1+ j det

A( j,1)
a j,2
=
1
det(A)
n

j=1
(−1)1+ j det

B( j,1)
b j,1
= det(B)
det(A)
= 0,
as required. The explicit formula for A−1 is called Cramer’s rule.
For example, consider the case n = 2. In this case, we have a small 2 × 2 matrix.
Now, if its determinant is nonzero:
ad −bc ̸= 0,
then its inverse is
a b
c d
−1
=
1
ad −bc
 d −b
−c a

.
(Check!) Next, we use Cramer’s rule yet more efﬁciently.
2.1.5
Cramer’s Rule
It is too expensive to calculate all minors and their determinant. Fortunately, we
often don’t need the entire inverse matrix in its explicit form. Usually, we are given
a speciﬁc vector
v ≡(11, v2, v3, . . . , vn)t ,
and we only need to apply A−1 to it.
For example, let’s calculate the ﬁrst component of A−1v. For this purpose, let’s
redeﬁne B as a new matrix that is nearly the same as A. Only its ﬁrst column is
different: it is the same as v. This way, we now have

A−1v

1 =
n

j=1

A−1
1, j v j =
1
det(A)
n

j=1
(−1)1+ j det

A( j,1)
v j = det(B)
det(A).
Now, there is nothing special about the ﬁrst component. In fact, let 1 ≤i ≤n be a
ﬁxed index. To calculate the ith component of A−1v, use a similar approach: redeﬁne
B as a new matrix that is nearly the same as A. Only its ith column is now different:
it is the same as v. With this new B, the ith component in A−1v is

56
2
Vector Product with Applications in Geometrical Mechanics

A−1v

i = det(B)
det(A).
This formula will be useful in applied geometry: barycentric coordinates in 3-D
(Chap.9). In this chapter, on the other hand, we use the determinant for yet another
geometrical purpose: vector product in 3-D, with its physical applications.
2.2
Vector Product
2.2.1
Standard Unit Vectors in 3-D
So far, the determinant was deﬁned algebraically. Still, does it have a geometrical
meaning as well? To see this, let’s look at two vectors of the same dimension n.
Can they multiply each other, to form a new vector? In general, they can’t: we
could calculate their inner product, but this would be just a scalar, not a vector
(Chap.1, Sect.1.7.1). Still, there is one exception: the three-dimensional Cartesian
space, obtained by setting n = 3. In this space, a vector product could indeed be
deﬁned.
For this purpose, deﬁne three standard unit vectors:
i = (1, 0, 0)t
j = (0, 1, 0)t
k = (0, 0, 1)t.
This way, i points in the positive x-direction, j points in the positive y-direction, and
k points in the positive z-direction (Fig.2.1).
These are orthonormal vectors: orthogonal unit vectors. In what way are they
standard? Well, they align with the x-y-z axes in the three-dimensional space. In
this sense, they actually make the standard coordinate system that spans the entire
Fig. 2.1 The right-hand
rule: the horizontal unit
vectors, i and j, produce the
vertical unit vector i × j = k

2.2 Vector Product
57
Cartesian space. Indeed, let
u ≡(u1, u2, u3)t and v ≡(v1, v2, v3)t
be some three-dimensional real vectors in R3. Thanks to the standard unit vectors,
they could also be written as
u ≡u1i + u2j + u3k and v ≡v1i + v2j + v3k.
This will be useful below.
2.2.2
Inner Product—Orthogonal Projection
In the above, we’ve considered two real three-dimensional vectors: u and v. Their
inner product is just the real scalar
(u, v) = uhv = utv = u1v1 + u2v2 + u3v3.
This is actually a bilinear form: it takes two inputs (or arguments), u and v, to produce
one new real number: their inner product.
Whatisthegeometricalmeaningofthis?Well,thisisjustanorthogonalprojection.
Consider, for instance, the x-axis, spanned by the unit vector i. What is the inner
product of v with i? It is just
(v, i) = v1 · 1 + v2 · 0 + v3 · 0 = v1.
This is just the x-coordinate of v: the projection of v on the x-axis. In fact, if v makes
angle η with the positive part of the x-axis, then
cos(η) = v1
∥v∥= (v, i)
∥v∥
(Fig.2.2)
Fig. 2.2 The vector v makes
angle η with the positive part
of the x-axis:
cos(η) = v1/∥v∥

58
2
Vector Product with Applications in Geometrical Mechanics
Fig. 2.3 The vector ˆv makes
angle η with the unit vector ˆi.
Once ˆv projects onto the
ˆi-axis, we have
cos(η) = (ˆv,ˆi)/∥ˆv∥
Now, there is nothing special about the i-direction. Indeed, let’s rotate both v and
i by a ﬁxed angle, to obtain the new vector ˆv, and the new unit vector ˆi (Fig.2.3).
This kind of rotation is an orthogonal transformation (see exercises below). As such,
it preserves inner product (Chap.1, Sect.1.8.2). Furthermore, the angle η is still the
same as before. Thus,
cos(η) = (v, i)
∥v∥=

ˆv,ˆi

∥ˆv∥.
You could also look at things the other way around. After all, the axis system is
picked arbitrarily. Why not pick the x-axis to align with the ˆi-axis in Fig.2.3? This
way, we’d have the same picture as in Fig.2.2. Just rotate yourself, and look at it
from a slightly different angle!
In either point of view, the inner product remains the same: the orthogonal pro-
jection of v (or ˆv) onto the unit vector i (or ˆi). This is good: the inner product can
now help calculate the cosine of the angle between the vectors. This angle will then
give a geometrical meaning to the vector product deﬁned below.
2.2.3
Vector Product
What do we want from a vector product?
• Itshouldtaketwoinputs(orarguments):theoriginalrealthree-dimensionalvectors
u and v.
• Likewise, its output should be a real three-dimensional vector, not just a scalar. In
other words, once the symbol “×” is placed in between u and v, u × v should be
a new three-dimensional vector: their vector product.
• We already know that the inner product (u, v) is a bilinear form: it is linear in both
u and v. Likewise, the vector product u × v should be a bilinear operator: linear
in u and linear in v as well.
• Interchanging the arguments should just change sign:
v × u = −(u × v).
• If both arguments are the same, then the vector product should vanish:

2.2 Vector Product
59
u × u = 0
(the origin).
• Take the triplet i, j, and k, and copy it periodically in a row:
i, j, k, i, j, k, i, j, k, . . . .
In this list, each pair should produce the next vector:
i × j = k
j × k = i
k × i = j
(Fig.2.1). This is called the right-hand rule. Later on, we’ll motivate it geometri-
cally as well.
How to deﬁne a good vector product, with all these properties? Fortunately, we can
use the determinant. After all, thanks to its original deﬁnition, the determinant is just
a linear combination of the items in the ﬁrst row, which might be vectors in their own
right:
u × v ≡det
⎛
⎝
⎛
⎝
i
j
k
u1 u2 u3
v1 v2 v3
⎞
⎠
⎞
⎠
= i(u2v3 −u3v2) −j(u1v3 −u3v1) + k(u1v2 −u2v1)
= (u2v3 −u3v2, u3v1 −u1v3, u1v2 −u2v1)t.
What’s so good about this new deﬁnition? Well, let’s see.
2.2.4
The Right-Hand Rule
In its new deﬁnition, does the vector product have the desirable properties listed
above? Well, let’s check:
• It is indeed bilinear: if w is yet another real three-dimensional vector, and α and
β are some real numbers, then
(αu + βw) × v = α(u × v) + β(w × v)
u × (αv + βw) = α(u × v) + β(u × w).
(Check!)
• Interchanging rows in a matrix changes the sign of its determinant. Therefore,

60
2
Vector Product with Applications in Geometrical Mechanics
v × u ≡det
⎛
⎝
⎛
⎝
i
j
k
v1 v2 v3
u1 u2 u3
⎞
⎠
⎞
⎠= −det
⎛
⎝
⎛
⎝
i
j
k
u1 u2 u3
v1 v2 v3
⎞
⎠
⎞
⎠= −(u × v).
(Check!)
• As a result, a matrix with two identical rows must have a zero determinant. There-
fore,
u × u = det
⎛
⎝
⎛
⎝
i
j
k
u1 u2 u3
u1 u2 u3
⎞
⎠
⎞
⎠= (0, 0, 0)t = 0.
(Check!)
• The right-hand rule indeed holds. Let’s verify this for the standard unit vectors.
(Later on, we’ll verify this for more general vectors as well.) For this purpose,
take your right hand, with your thumb pointing in the positive x-direction, and
your index ﬁnger pointing in the positive y-direction. Then, your middle ﬁnger
will point in the positive z-direction:
i × j ≡det
⎛
⎝
⎛
⎝
i j k
1 0 0
0 1 0
⎞
⎠
⎞
⎠= k.
Now, let your thumb point in the positive y-direction, and your index ﬁnger in the
positive z-direction. Then, your middle ﬁnger will point in the positive x-direction:
j × k ≡det
⎛
⎝
⎛
⎝
i j k
0 1 0
0 0 1
⎞
⎠
⎞
⎠= i.
Finally, let your thumb point in the positive z-direction, and your index ﬁnger
in the positive x-direction. Then, your middle ﬁnger will point in the positive
y-direction:
k × i ≡det
⎛
⎝
⎛
⎝
i j k
0 0 1
1 0 0
⎞
⎠
⎞
⎠= j,
as required.
So far, we’ve considered standard unit vectors only. But what about more general
vectors, like u and v above?
Well, let’s focus on one component in u×v. For instance, what is the z-coordinate?
It is just
(u × v)3 = det
⎛
⎝
⎛
⎝
1
u1 u2
v1 v2
⎞
⎠
⎞
⎠= u1v2 −u2v1.

2.2 Vector Product
61
Fig. 2.4 The right-hand
rule: take your right hand,
and match your thumb to
(u1, u2), and your index
ﬁnger to (v1, v2). Then, your
middle ﬁnger will point
upward, toward your own
eyes, as indicated by the “⊙”
at the origin
Is this positive? To check on this, let’s look at the two-dimensional subvector
(u1, u2)t ∈R2. Assume that it lies in the upper half of the Cartesian plane, where
u2 > 0. (Otherwise, just switch to −u) Likewise, look at the two-dimensional sub-
vector (v1, v2)t ∈R2, and assume that it lies in the upper half of the Cartesian plane,
where v2 > 0 (Fig.2.4).
This way, to check whether u1v2 −u2v1 is positive or not, one could divide it by
u2v2 > 0. When would this be positive? Only when
cotan(φ) = u1
u2
> v1
v2
= cotan(θ),
or
φ < θ,
as in Fig.2.4. (After all, the cotangent function is monotonically decreasing.) In this
case, since its z-coordinate is positive, u × v points from the page outward, toward
your eyes. This is in agreement with the right-hand rule: take your right hand, and
match your thumb to u, and your index ﬁnger to v. Then, your middle ﬁnger will
point toward your own eyes, as required.
Finally, the vector product has yet another interesting property: u×v is orthogonal
to both u and v. Indeed, the inner product of u × v with either u or v produces a
matrix with two identical rows, with a zero determinant:
(u × v)tu = det
⎛
⎝
⎛
⎝
u1 u2 u3
u1 u2 u3
v1 v2 v3
⎞
⎠
⎞
⎠= 0
(u × v)tv = det
⎛
⎝
⎛
⎝
v1 v2 v3
u1 u2 u3
v1 v2 v3
⎞
⎠
⎞
⎠= 0.
This property will be useful in orthogonal transformations.

62
2
Vector Product with Applications in Geometrical Mechanics
2.3
Orthogonalization
2.3.1
Invariance Under Orthogonal Transformation
The vector product is (nearly) invariant under orthogonal transformation: up to a
sign, ordering doesn’t matter: you could either apply the vector product and then
transform, or do things the other way around: ﬁrst transform, and then apply the
vector product (Fig.2.5).
To see this, let Q be a 3 × 3 real orthogonal matrix. Let us show that, up to a sign,
Q preserves vector product:
Q(u × v) = ±(Qu) × (Qv).
(The proper sign will be speciﬁed later.)
Let us ﬁrst show that Q(u × v) is proportional to (Qu) × (Qv), or orthogonal to
both Qu and Qv. Fortunately, as discussed in Chap.1, Sect.1.8.2, Q preserves inner
product, so
(Q(u × v), Qu) = (u × v, u) = 0
(Q(u × v), Qv) = (u × v, v) = 0.
This proves that Q(u × v) is indeed proportional to (Qu) × (Qv), as asserted. But
what about their magnitude? Is it the same? To see this, just take their inner product:
((Qu) × (Qv), Q(u × v)) = det
⎛
⎝
⎛
⎝
(Q(u × v))t
(Qu)t
(Qv)t
⎞
⎠
⎞
⎠
= det
⎛
⎝
⎛
⎝
(u × v)t Qt
ut Qt
vt Qt
⎞
⎠
⎞
⎠
Fig. 2.5
Up to a sign,
ordering doesn’t matter. You
could either apply the
orthogonal transformation
and then the vector product,
or work the other way
around: apply the vector
product ﬁrst, and then the
orthogonal transformation

2.3 Orthogonalization
63
= det
⎛
⎝
⎛
⎝
(u × v)t
ut
vt
⎞
⎠Qt
⎞
⎠
= det
⎛
⎝
⎛
⎝
(u × v)t
ut
vt
⎞
⎠
⎞
⎠det

Qt
= (u × v, u × v) det (Q)
= (Q(u × v), Q(u × v)) det (Q)
=
 ∥Q(u × v)∥2
if
det(Q) = 1
−∥Q(u × v)∥2
if det(Q) = −1.
So,
(Qu) × (Qv) =
 Q(u × v)
if
det(Q) = 1
−Q(u × v) if det(Q) = −1.
In summary, ordering is (nearly) immaterial: making the vector product and then
the orthogonal transformation is the same (up to a sign) as making the orthogonal
transformation and then the vector product.
In particular, vector product is invariant under an orthogonal transformation with
determinant 1 (rotation). This means that vector product is purely geometrical: it
is independent of the coordinate system that happens to be used. For this reason,
the vector product must have a pure geometrical interpretation, free of any tedious
algebraic detail. To see this, let’s switch to a more convenient axis system, which is
no longer absolute, but relative to the original vectors u and v.
2.3.2
Relative Axis System: Gram–Schmidt Process
Let us use the above properties to design a new coordinate system in R3. For this
purpose, assume that u and v are linearly independent of each other: they are not
a scalar multiple of each other. In this case, we can form a new 3 × 3 orthogonal
matrix:
O ≡

v(1) | v(2) | v(3)
.
What are these columns Well, they should orthonormalize the original vectors u and
v:
1. First, normalize u:
v(1) ≡
u
∥u∥.
This way, v(1) is the unit vector proportional to u.
2. Then, as in Sect.2.2.2, project v on v(1), and subtract:

64
2
Vector Product with Applications in Geometrical Mechanics
v(2) ≡v −

v(1), v

v(1).
This is called a Gram–Schmidt process. Because u and v are linearly independent,
v(2) ̸= 0.
By now, we’ve orthogonalized v with respect to u: v(2) is now orthogonal to v(1).
Indeed, since v(1) is a unit vector,

v(1), v(2)
=

v(1), v −

v(1), v

v(1)
=

v(1), v

−

v(1), v

= 0.
3. Next, normalize v(2) as well:
v(2) ←
v(2)
∥v(2)∥.
This way, v(1) and v(2) are now orthonormal and span the same plane as the
original vectors u and v.
4. Next, deﬁne a new vector, orthogonal to both u and v:
v(3) ≡v(1) × v(2).
5. Finally, normalize v(3) as well:
v(3) ←
v(3)
∥v(3)∥.
We’ll soon realize that this normalization is actually unnecessary: v(3) is already a
unit vector. Still, by now, we don’t know this as yet: we only know that v(3) is a unit
vector that points in the same direction as v(1) × v(2).
In summary, O has real orthonormal columns. As such, O is an orthogonal matrix:
Ot O = OOt =
⎛
⎝
1 0 0
0 1 0
0 0 1
⎞
⎠.
Thus, the vector product is invariant under O. Indeed, in Sect.2.3.1, the relevant sign
is the plus sign, and O must have determinant 1:
v(1) × v(2) = (Oi) × (Oj) = O(i × j) = Ok = v(3).
So, there was never any need to normalize v(3): it was a unit vector all along.

2.3 Orthogonalization
65
What are v(1), v(2), and v(3) geometrically? A new axis system! v(1) points in the
positive direction of the new x-axis, v(2) points in the positive direction of the new
y-axis, and v(3) points in the positive direction of the new z-axis.
Each three-dimensional vector can now be written in terms of these new coordi-
nates. As an exercise, let’s reconstruct u. Fortunately, u is orthogonal to both v(2)
and v(3). Thus, the general expansion in Chap.1, Sect.1.11.5, reduces to
u = Iu = (OOt)u = O(Otu) =
3

j=1
(Otu) jv( j) = (Otu)1v(1) =

v(1), u

v(1) = ∥u∥v(1).
What does this mean geometrically? It means that u is conﬁned to the new x-axis,
as required. In fact, in the new coordinates, u takes the simple form of
Otu =
⎛
⎝
∥u∥
0
0
⎞
⎠.
v, on the other hand, is expanded in two terms: its orthogonal projections on v(1) and
on v(2). After all, v is orthogonal to v(3), but not necessarily to v(1), let alone to v(2):
v =
3

j=1
(Otv) jv( j)
= (Otv)1v(1) + (Otv)2v(2)
=

v(1), v

v(1) +

v(2), v

v(2)
=

v(1), v

v(1) +

v(2), v −

v(1), v

v(1)
v(2)
=

v(1), v

v(1) +
v −

v(1), v

v(1) v(2).
What does this mean geometrically? It means that v is conﬁned to the new x-y plane.
In fact, in the new coordinates, v is actually two-dimensional—it has just two nonzero
coordinates:
Otv =
⎛
⎝

v(1), v

v −

v(1), v

v(1)
0
⎞
⎠.
Thankstothisform,wecannowredeﬁneu×v geometricallyratherthanalgebraically.
2.3.3
Angle Between Vectors
Like O, Ot is a rotation matrix: a real orthogonal matrix, with determinant 1. There-
fore, the vector product is invariant under Ot as well, and can be calculated in the
new coordinates, using the above form.

66
2
Vector Product with Applications in Geometrical Mechanics
In the new coordinates, the original (algebraic) deﬁnition gets very simple. This
is also apparent geometrically. After all, v contains only two nonzero coordinates:
the v(1)-coordinate (the new x-coordinate), and the v(2)-coordinate (the new y-
coordinate). In u × v, only the latter coordinate is relevant. The former (which
is proportional to u) drops, contributing nothing to u × v:
u × v = ∥u∥·
v −

v(1), v

v(1) v(1) × v(2)
= ∥u∥· ∥v∥
v −

v(1), v

v(1)
∥v∥
v(3)
= ∥u∥· ∥v∥sin(η)v(3),
where η is the angle between u and v in the u-v plane (the new x-y plane):
cos(η) = (Otv)1
∥v∥
=

v(1), v

∥v∥
=
(u, v)
∥u∥· ∥v∥
(Fig.2.3).
So, what is the norm ∥u × v∥? It is just the area of the parallelogram that u and
v make in the plane that they span: the new x-y plane. This is a pure geometrical
interpretation, independent of any coordinate system, and free of any algebraic detail.
No wonder it is so useful in physics.
2.4
Linear and Angular Momentum
2.4.1
Linear Momentum
The vector product introduced above is particularly useful in geometrical physics. To
see this, consider a particle of mass m, traveling in the three-dimensional Cartesian
space. At time t, it is at position
r ≡r(t) ≡
⎛
⎝
x(t)
y(t)
z(t)
⎞
⎠∈R3.
Later on, in quantum mechanics, we’ll see that this is not so simple. Still, for the
time being, let’s accept this.
To obtain the velocity of the particle, differentiate with respect to time:
r′ ≡r′(t) ≡
⎛
⎝
x′(t)
y′(t)
z′(t)
⎞
⎠∈R3.

2.4 Linear and Angular Momentum
67
Fig. 2.6 At time t, the particle is at r ≡r(t) ∈R3, with a linear momentum p ≡p(t) ∈R3. This
momentum could split into two parts: the radial part is proportional to r, whereas the other part is
perpendicular to r
To obtain the linear momentum, multiply by m, the mass of the particle:
p ≡p(t) ≡mr′(t) ∈R3.
Later on, in special relativity, we’ll redeﬁne the linear momentum more carefully.
Still, for the time being, let’s accept this.
Finally, to have the force, differentiate p, to obtain the new vector p′(t). In sum-
mary, at each particular time t, the linear momentum p describes the full motion of
the particle, telling us how it moves in each and every spatial direction. Still, we’ll
soon see that only two spatial directions are relevant: the third one vanishes.
Indeed, p will soon split into two orthogonal components. The ﬁrst will tell us how
fast the particle gets farther and farther away from the origin, or how fast ∥r∥grows.
This component must be radial: proportional to r, pointing in the same direction as
r. In fact, it is just the orthogonal projection of p onto the unit vector r/∥r∥:
(r, p)
∥r∥2 r =
 r
∥r∥, p
 r
∥r∥= cos(η)∥p∥r
∥r∥,
where η is the angle between p and r (Fig.2.6 and Sect.2.2.2). This radial component
indeed tells us how fast the particle gets away from the origin, or how fast ∥r∥grows
in time, as required.
2.4.2
Angular Momentum
Still, this is not the whole story. After all, as time goes by, r may not only change
magnitude but also change direction. To understand this part of the motion, let’s look

68
2
Vector Product with Applications in Geometrical Mechanics
at the other component of p, which will tell us how fast the particle rotates about the
origin.
Where does this rotation take place? Well, inﬁnitesimally, it takes place in the r-p
plane: the plane spanned by r and p. In other words, this is the plane orthogonal to
the vector product r × p.
So, the second component of p must be nonradial: orthogonal (or perpendicular)
to the former component. This way, it will indeed tell us how fast the particle rotates
about a new vector: the angular momentum r × p.
What is the norm of this vector? We already know what it is:
∥r × p∥= ∥r∥· ∥p∥sin(η)
(Sect.2.3.3). To rotate about r × p, the particle must make a small (inﬁnitesimal)
arc in the r-p plane. For this purpose, the second component of p must be tangent to
this arc, or orthogonal not only to r × p but also to r (Fig.2.6). In summary, it must
be proportional to
(r × p) × r.
Fortunately, we already know the norm of this vector:
∥(r × p) × r∥= ∥r × p∥· ∥r∥sin
π
2

= ∥r × p∥· ∥r∥= ∥p∥sin(η)∥r∥2.
So, to have the second (nonradial) component of p in its properly scaled form, we
must divide by ∥r∥2:
r × p
∥r∥2 × r = r × p
∥r∥×
r
∥r∥.
Indeed, the norm of this vector is

r × p
∥r∥2 × r
 = sin(η)∥p∥,
as required. In summary, we now have the complete orthogonal decomposition of
the original linear momentum:
p = (r, p)
∥r∥2 r + r × p
∥r∥2 × r = (r, p)
∥r∥·
r
∥r∥+ r × p
∥r∥×
r
∥r∥.
What is so nice about this decomposition? Well, it is uniform: both terms are written
in the same style. The only difference is that the former term uses inner product,
whereas the latter term uses vector product.
Furthermore, both terms are orthogonal (perpendicular) to each other. After all,
this is how they have been designed in the ﬁrst place: the former is proportional to
r, whereas the latter is perpendicular to r. This is why they also satisfy Pythagoras’
theorem: the sum of squares of their norms is

2.4 Linear and Angular Momentum
69

(r, p)
∥r∥·
r
∥r∥

2
+

r × p
∥r∥×
r
∥r∥

2
=
(r, p)
∥r∥
2
+

r × p
∥r∥

2
= cos2(η)∥p∥2 + sin2(η)∥p∥2
=

cos2(η) + sin2(η)

∥p∥2
= ∥p∥2,
as required.
2.5
Angular Velocity
2.5.1
Angular Velocity
What is momentum? It is mass times velocity. This is true not only in one but also
in three spatial dimensions. So, to obtain the velocity vector, just divide by the mass
of the particle:
v = p
m = r × p
m∥r∥2 × r + (r, p)
m∥r∥2 r.
This is just a special case of a more general (not necessarily orthogonal) decompo-
sition:
v = u + w,
where
u ≡ω × r,
and
ω ≡ω(t) ≡
⎛
⎝
ω1(t)
ω2(t)
ω3(t)
⎞
⎠∈R3
is a new vector: the angular velocity. (Don’t confuse it with the other vector w.) This
way, the particle rotates about ω. The norm ∥ω∥tells us how fast: by what angle per
second. By deﬁnition, u must be perpendicular to both ω and r (Fig.2.7).
The angular velocity is time-dependent: ω may change in time, not only in mag-
nitude but also in direction. For simplicity, however, we often look at some ﬁxed
time. This way, the argument “(t)” may drop.

70
2
Vector Product with Applications in Geometrical Mechanics
Fig. 2.7 For the sake of
better visualization, we
assume that the angular
velocity ω is perpendicular
to r. This way, it points from
the page toward your eyes, as
indicated by the “⊙” at the
origin. (Don’t confuse ω
with the other vector w!)
2.5.2
The Rotating Axis System
In general, ω may point in just any direction, not necessarily perpendicular to r
or v. (See exercises below.) For simplicity, however, we often assume that ω is
perpendicular to r:
(ω,r) = 0.
Otherwise, just redeﬁne the origin, and shift it along the ω-axis, until obtaining new
orthogonal ω and r. This way, ω-r-u make a new right-hand system, rotating around
the ω-axis. In terms of these new coordinates, the particle may “feel” a few new
forces.
2.5.3
Velocity and Its Decomposition
As we’ve seen so far, at time t, the particle rotates (inﬁnitesimally) around the ω-axis
(at a rate of angle ∥ω∥per second), making an inﬁnitesimal arc. In our velocity
v = u + w,
the former term
u ≡ω × r
is tangent to this arc. The remainder
w ≡v −u,
on the other hand, maybe nontangential, and even perpendicular to the arc.
For the sake of better visualization, we often assume that ω andr are perpendicular
to each other, as in Fig.2.7. This way, ω-r-u form a new right-hand system: the
rotating axis system.

2.5 Angular Velocity
71
Only in our ﬁnal example does w contain a component parallel to ω. In most of
our discussion, on the other hand, ω is orthogonal to w too, and indeed to v too, as
in Fig.2.7. In this case, it makes sense to deﬁne
ω ≡r × p
m∥r∥2 ,
in agreement with the formula at the beginning of Sect.2.5.1. This way, w is radial,
so ω-w-u make the same right-hand system as ω-r-u: the rotating coordinate system
(Sect.2.5.2).
2.6
Real and Fictitious Forces
2.6.1
The Centrifugal Force
What is the centrifugal force? This is the force that the particle “feels” in its own
ideal “world”: the rotating coordinate system.
In general (even if ω is not orthogonal to r), the centrifugal force is
−mω × (ω × r).
To illustrate, it is convenient to assume that ω and r are orthogonal to each other:
(ω,r) = 0.
This way,
∥ω × r∥= ∥ω∥· ∥r∥,
and the centrifugal force is radial:
−mω × (ω × r) = m∥ω∥2r.
Still, this is “felt” in the rotating axis system only. In reality, on the other hand, there
is no centrifugal force at all. Indeed, in the static axis system used in Fig.2.8, the
real force is just p′ = mv′. So long as v′ = 0, there is no force at all: Newton’s ﬁrst
law holds, and the linear momentum is conserved. As a result, v can never change
physically: what could change is just its writing style in rotating coordinates. This
nonphysical “change” is due to the ﬁctitious centrifugal “force”.
Still, the rotating coordinates are legitimate too, and we might want to work in
them. In fact, if the particle stayed at the same rotating coordinates (0, ∥r∥, 0) all
the time, then it would rotate physically round and round forever. For this purpose,
a new counterforce must be applied, to cancel the centrifugal force out.

72
2
Vector Product with Applications in Geometrical Mechanics
Fig. 2.8 The ﬁctitious centrifugal force: −mω × (ω × r). If ω and r are orthogonal to each other,
then it is also radial: m∥ω∥2r
2.6.2
The Centripetal Force
How to balance (or cancel) the centrifugal force? For this purpose, let’s go ahead
and “connect” the particle to the origin by a wire. In rotating coordinates, this reacts
to the centrifugal force. Indeed, thanks to Newton’s second law, this supplies the
required counterforce—the centripetal force:
mω × (ω × r).
If ω and r are still orthogonal to each other, then this force is radial as well:
mω × (ω × r) = −m∥ω∥2r.
This is indeed how the wire must react. After all, Newton’s second law must work
in just any coordinate system, rotating or not.
In the rotating axis system, this helps cancel the original centrifugal force, leaving
the particle at the same (rotating) coordinates (0, ∥r∥, 0) forever. In the static coor-
dinates, on the other hand, the centripetal force has another physical job: to make u
turn.
In fact, the centripetal force pulls the particle toward the origin in just the correct
amount: it keeps the particle at a constant distance ∥r∥from the origin, rotating at a
constant angular velocity ω. This way, the particle makes not only inﬁnitesimal but
also global arc around the ω-axis (Fig.2.9). In fact, this arc is as big as a complete
circle of radius ∥r∥. As a result, the original velocity vector is always tangent to this
circle: there is no nontangential component anymore:
v = u
and w = 0.

2.6 Real and Fictitious Forces
73
Fig. 2.9 Here, the particle is connected to the origin by a wire. This supplies the centripetal force
required to cancel the original centrifugal force, and keep the particle at the constant distance ∥r∥
from the origin, rotating at the constant angular velocity ω
2.6.3
Euler Force
In the above example, the particle rotates at the constant angular velocity ω. But what
if ω changed in time? In this case, ω might have a nonzero time derivative:
ω′ ≡ω′(t) ≡
⎛
⎝
ω′
1(t)
ω′
2(t)
ω′
3(t)
⎞
⎠̸= 0.
In the rotating coordinate system, this introduces a new force—Euler force:
−mω′ × r.
What is the direction of this new force? In general, we can’t tell. After all, ω′ could
point in just any direction. Indeed, as time goes by, the angular velocity may change
not only magnitude but also direction, making the particle rotate in all sorts of new
r-u planes.
Still, for simplicity, assume that ω′ keeps pointing in the same direction as the
original ω (Fig.2.10). This way, ω keeps pointing in the same direction all the time:
it only gets bigger and bigger in magnitude:
Fig. 2.10 Here, the particle rotates counterclockwise faster and faster, so ω′ points in the same
direction as ω. In this case, Euler force pulls the particle clockwise, opposing the original rotation,
and slowing it down

74
2
Vector Product with Applications in Geometrical Mechanics
∥ω∥′ ≡d∥ω∥
dt
> 0.
What happens physically? The particle rotates counterclockwise faster and faster.
What could possibly supply the energy required for this? Well, assume that there is
some angular accelerator that keeps increasing the angle that the particle makes per
second. This way, the particle keeps rotating counterclockwise in the same r-u plane,
at a bigger and bigger angle ∥ω(t)∥per second. Unfortunately, the Euler force pulls
the particle back clockwise, in an attempt to oppose this motion and slow it down.
So, not all the energy of the accelerator can go to increasing ∥ω∥: some of it must go
to canceling Euler force. Indeed, to balance Euler force, the accelerator must waste
a force in the amount of
m∥ω′ × r∥= m∥ω′∥· ∥r∥
counterclockwise. Only the rest could be invested in accelerating the particle angu-
larly.
As a result, in the rotating axis system, the particle remains at rest. It always keeps
the same (rotating) coordinates: (0, ∥r∥, 0). This way, in its own subjective (rotating)
“world”, it remains effortless, allowing the rotating axis system to carry it round and
round, faster and faster.
2.6.4
The Earth and Its Rotation
The rotating coordinates are not just theoretical. They may also be quite real, and
easy to work with. Let’s go ahead and use them in practice.
In Figs.2.9 and 2.10, the particle makes a closed circle around the ω-axis. In fact,
the velocity is tangent to this circle, with no nontangential component at all:
v = u
and w = 0.
Let us now consider a more complicated case, in which v does contain a nontangential
component as well:
v = u + w,
where w ̸= 0.
As a matter of fact, we’ve already seen such a case. In Fig.2.7, however, w might
seem radial. Here, on the other hand, w makes angle η with the horizontal r-axis,
and angle π/2 −η with the vertical ω-axis:
(ω, w) ≥0
(Fig.2.11). Consider, for example, a spaceship standing somewhere in the northern
hemisphere of the Earth, at latitude 0 ≤η < π/2. Its nose points upward: straight
toward the sky.

2.6 Real and Fictitious Forces
75
Fig. 2.11 A horizontal cross section of the northern hemisphere of the Earth at latitude 0 ≤η < π/2
(a view from above). Here, we place the origin not at the center of the Earth but at the center of the
cross section. As a result, r is horizontal as well: it lies in the cross section. But w (the direction of
the spaceship) is not: it makes angle η with the horizontal r-axis. The Coriolis force pulls the entire
spaceship westward
The entire Earth rotates eastward: this is why we see the sun rising from the east.
For this reason, in Fig.2.11, the angular velocity ω points northward: from the page
upward, straight toward your eyes.
In reality, ω is not quite constant: it changes direction, although very slowly. In
fact, as the Earth rotates, ω rotates too. Why? Because the Earth is not a perfect
sphere. For this reason, the north pole is not constant: it loops clockwise. This is very
slow: the loop takes 27, 000 years to complete. This is called precession.
Besides, the north pole makes yet another (small) loop clockwise. This loop is
quicker: it takes 1.3 years to complete. Still, it is very small: just ten meters in radius.
So, both loops can be ignored.
In Fig.2.11, we can see a horizontal cross section of the Earth at latitude 0 ≤
η < π/2 (a view from above, say from the North Star). Recall that the origin can be
picked arbitrarily. Here, we place it not at the center of the Earth but at the center of
the cross section. This way, ∥r∥is not the radius of the Earth but the radius of the
cross section. Indeed, r is horizontal: it lies in the cross section in its entirety.
Initially, at t = 0, the spaceship stands on the face of the Earth, in this cross
section. As a matter of fact, r is just the location of the spaceship in the cross section.
This way, the body of the spaceship points obliquely away from the cross section,
making angle η with the horizontal r-axis.
Later on, at t > 0, on the other hand, the spaceship will ﬂy away from the Earth.
Fortunately, the cross section can be extended into an inﬁnite horizontal plane. In
this plane, r will denote the orthogonal projection (or “shadow”) that the spaceship
will make on this plane. This way, r will always be horizontal: it will keep being in
this plane at all times.
If η = 0, then the situation is simple. The cross section meets the equator, and
its center coincides with the center of the Earth. Since r lies in this cross section,
it coincides with the radius of the Earth. This way, r is perpendicular to the face

76
2
Vector Product with Applications in Geometrical Mechanics
of the Earth: it points straight into the sky. Fortunately, the centrifugal force in this
direction is well balanced by gravity, which supplies the required centripetal force.
Why isn’t the Earth a perfect sphere? Well, if it were, then some gravity would
have been lost at the equator to balance the centrifugal force there, as discussed
above. So, at the equator, gravity would have been a little weaker. This is probably
how the Earth had evolved in the ﬁrst place: at the equator, due to weaker gravity, it
got a bit wide and “fat”.
If, on the other hand, η > 0, then the situation is more complicated. The cross
section passes not through the center of the Earth but above it. Since r is still hori-
zontal, it is now shorter than before:
∥r∥= cos(η) (the radius of the Earth) .
Furthermore, r is no longer perpendicular to the face of the Earth: it also has a new
component that points southward.
What is the norm of this new component? Clearly, it is sin(η)∥r∥. This produces
a new centrifugal force in the amount of m∥ω∥2 sin(η)∥r∥southward, which is not
balanced by gravity. After all, gravity pulls downward, toward the ground, not north-
ward.
Recall that here we work in the rotating axis system: we stand on the face of the
Earth, unaware of any rotation. Therefore, to us, the above force is real and is truly
felt. Likewise, the spaceship feels it as well, and its route could be affected, including
the shadow r it makes on the (extended) cross section.
Fortunately, the above force can never affect ω, which produced it in the ﬁrst
place. Can it affect the “shadow” r? Not much: it needs time to act. Therefore, for
small t, its effect on r is as small as t2. (See exercises at the end of the chapter about
Newtonian mechanics in [60].) For this reason, it hardly affects the original motion,
illustrated in Fig.2.11 at the initial time of t = 0. Still, after a while, the effect may
accumulate and grow, and should be taken into account. To balance this, the nose of
the spaceship could point a little obliquely northward, from the start.
2.6.5
Coriolis Force
Together with the entire Earth, the spaceship rotates eastward, at a rate of angle ∥ω∥
per second. This produces one component of its velocity—the tangential part:
u ≡ω × r.
Here, though, we work in the rotating axes, spanned by ω, r, and u. In rotating
coordinates, there is no tangential motion at all.
Now, at time t = 0, the spaceship also obtains initial velocity w upward, straight
toward the sky. This is liftoff: the spaceship really feels it. Still, at the same time, it
also feels a new force, which mustn’t be ignored:

2.6 Real and Fictitious Forces
77
−2mω × w.
This is the Coriolis force (Fig.2.11).
What is its direction? Well, it must be perpendicular to the entire ω-w plane.
Thanks to the right-hand rule, this must be westward.
What is the norm of the Coriolis force? Well, this depends on the angle π/2 −η
between ω and w:
2m ∥ω × w∥= 2m ∥ω ∥·∥w∥sin
π
2 −η

= 2m ∥ω ∥·∥w∥cos(η).
Fortunately, this force doesn’t have much time to act: for small t, its effect on w is
as small as t. (See exercises at the end of the chapter about Newtonian mechanics in
[60].) Thus, it hardly affects the motion, illustrated in Fig.2.11 at the initial time of
t = 0. Still, as time goes by, it may accumulate, and mustn’t be ignored. To balance
it, the spaceship should point a little obliquely eastward, from the start.
2.7
Exercises
2.7.1
Rotation and Euler Angles
1. Let Q be an n × n orthogonal matrix. Show that it preserves norm: for every
n-dimensional vector v,
∥Qv∥2 = (Qv, Qv) = (v, Qt Qv) = (v, Iv) = (v, v) = ∥v∥2.
2. Let O and Q be two orthogonal matrices of the same order. Show that their
product OQ is an orthogonal matrix as well. Hint: thanks to associativity,
(OQ)t(OQ) =

Qt Ot
(OQ) = Qt 
Ot O

Q = Qt I Q = Qt Q = I.
3. Let 0 ≤θ < 2π be some (ﬁxed) angle. Let U be the following 2 × 2 matrix:
U ≡U(θ) ≡
cos(θ) −sin(θ)
sin(θ) cos(θ)

.
Show that U rotates the x-axis by angle θ counterclockwise. Hint: apply U to
the standard unit vector that points rightward:
U
 1
0

=
cos(θ)
sin(θ)

.
The result is the ˜x-unit vector in Fig.2.12.

78
2
Vector Product with Applications in Geometrical Mechanics
Fig. 2.12 The orthogonal matrix U rotates the entire x-y plane by angle θ counterclockwise and
maps it to the new ˜x- ˜y plane
4. Show that U rotates the y-axis by angle θ counterclockwise as well. Hint: apply
U to the standard unit vector (0, 1)t.
5. Conclude that U rotates the entire x-y plane by angle θ counterclockwise. Hint:
extend the above linearly. For this purpose, write a general two-dimensional
vector as a linear combination of the standard unit vectors (1, 0)t and (0, 1)t.
6. Show that the columns of U are orthogonal to each other.
7. Show that the columns of U are unit vectors of norm 1.
8. Conclude that the columns of U are orthonormal.
9. Conclude that the columns of U span new axes: the ˜x- and ˜y-axes in Fig.2.12.
10. Conclude also that U is an orthogonal matrix.
11. Verify that U indeed satisﬁes
U tU = UU t = I,
where I is the 2 × 2 identity matrix.
12. Conclude that U t is an orthogonal matrix as well.
13. Verify that the columns of U t (the rows of U) are indeed orthogonal to each
other as well.
14. Verify that the columns of U t (the rows of U) are indeed unit vectors of norm 1.
15. Again, interpretU geometrically as a rotation: once applied to a two-dimensional
vector, it rotates it by angle θ counterclockwise. Hint: check this for the standard
unit vectors (1, 0)t and (0, 1)t. Then, extend this linearly.
16. Interpret U t geometrically as the inverse rotation: once applied to a vector, it
rotates it by angle θ clockwise.
17. Interpret the equation U tU = I geometrically. For this purpose, make sure that
the composition of U t on top of U is just the identity mapping that changes
nothing. Hint: rotating clockwise cancels rotating counterclockwise.
18. Show that
det(U) = det

U t
= 1.

2.7 Exercises
79
19. Introduce a third spatial dimension: the z-axis. This makes the new x–y–z-axis
system in R3.
20. Assume that yet another (right-hand) axis system is also given: the ˜x– ˜y–˜z-axis
system. How to map the original x–y–z-axis system to the new ˜x– ˜y–˜z-axis
system?
21. To do this, use three stages:
• Rotate the entire ˜x- ˜y plane by a suitable angle ψ clockwise, until the ˜x-axis
hits the x-y plane.
• Then, rotate the entire x-y plane by a suitable angle φ counterclockwise, until
the x-axis matches the up-to-date ˜x-axis.
• By now, the up-to-date x- and ˜x-axes align with each other. So, all that is left to
do is to rotate the up-to-date y-z plane by a suitable angle θ counterclockwise,
until the y- and z-axes match the up-to-date ˜y- and ˜z-axes, respectively.
22. Conclude that to map the original x–y–z-axis system to the original ˜x– ˜y–˜z-axis
system, one could use three stages:
• Rotate the entire x-y plane by angle φ counterclockwise.
• Then, rotate the up-to-date y-z plane by angle θ counterclockwise.
• Finally, rotate the up-to-date x-y plane by angle ψ counterclockwise.
The angles φ, θ, and ψ are called Euler angles.
23. Show that this is a triple product of three orthogonal matrices.
24. Conclude that this triple product is an orthogonal matrix as well.
25. Write it in its explicit form:
U(φ)
1
 1
U(θ)
 U(ψ)
1

.
26. Does this matrix have determinant 1? Hint: it transfers a right-hand system to
a right-hand system. Furthermore, it is the product of three rotation matrices of
determinant 1.
27. Say this in the terminology of group theory: the special matrices on the right
(that rotate a particular plane) generate the entire group of general rotations in
three spatial dimensions.
28. Consider a 3 × 3 matrix. Show that interchanging two rows in it changes the
sign of the determinant.
29. Conclude that if the matrix has two identical rows, then its determinant must
vanish.
30. Let u, v, and w be real three-dimensional vectors in R3. Show that
u × u = 0.
31. Show that
u × v = −(v × u).

80
2
Vector Product with Applications in Geometrical Mechanics
32. Show that
(u, v × w) = det
⎛
⎝
⎛
⎝
ut
vt
wt
⎞
⎠
⎞
⎠.
33. Assume also that u, v, and w are linearly independent of each other: they don’t
belong to a plane that passes through the origin. Take the original triplet u, v, w,
and copy it time and again in a row:
u, v, w, u, v, w, u, v, w, . . . .
In this list, start from some vector and look ahead to the next two. Show that this
produces the same determinant, no matter whether you started from u or v or w:
det
⎛
⎝
⎛
⎝
ut
vt
wt
⎞
⎠
⎞
⎠= det
⎛
⎝
⎛
⎝
vt
wt
ut
⎞
⎠
⎞
⎠= det
⎛
⎝
⎛
⎝
wt
ut
vt
⎞
⎠
⎞
⎠.
Hint: interchange the ﬁrst and second rows, and then the second and third rows:
det
⎛
⎝
⎛
⎝
ut
vt
wt
⎞
⎠
⎞
⎠= −det
⎛
⎝
⎛
⎝
vt
ut
wt
⎞
⎠
⎞
⎠= det
⎛
⎝
⎛
⎝
vt
wt
ut
⎞
⎠
⎞
⎠.
34. Conclude that
(u, v × w) = (v, w × u) = (w, u × v).
35. Assume that u, v, and w satisfy the right-hand rule. Show that, in this case,
(u, v × w) > 0.
Hint: recall that u, v, and u × v satisfy the right-hand rule (Fig.2.4). So, to
complete u and v into a right-hand system, one could add either w or u × v.
Thus, both w and u ×v must lie in the same side of the u-v plane. As in Fig.2.3,
they must therefore have a positive inner product:
(w, u × v) > 0.
36. Could this serve as a new (algebraic) version of the right-hand rule?

2.7 Exercises
81
2.7.2
Principal Axes
1. Recall that
r ≡
⎛
⎝
x
y
z
⎞
⎠
is the position of the particle in the Cartesian space. Assume that r is ﬁxed. Show
that rrt is a 3 × 3 matrix.
2. Show that rrt is symmetric.
3. What are the eigenvalues and eigenvectors of rrt?
4. Show that r is an eigenvector of rrt, with the eigenvalue ∥r∥2. Hint: thanks to
associativity,

rrt
r = r

rtr

= (r,r)r = ∥r∥2r.
5. Let q be a vector that is orthogonal to r. Show that q is an eigenvector of rrt,
with the zero eigenvalue. Hint: thanks to associativity,

rrt
q = r

rtq

= (r, q)r = 0.
6. Design two different q’s that are both orthogonal to r and are also orthogonal to
each other.
7. Show that this could be done in many different ways. Still, pick arbitrarily one
particular pair of orthogonal q’s.
8. Conclude that rrt is positive semideﬁnite: its eigenvalues are greater than or
equal to zero.
9. Conclude that rrt has the following diagonal form:
rrt = O
⎛
⎝
∥r∥2
0
0
⎞
⎠Ot,
where O is an orthogonal matrix, with columns that are proportional to r and
the above q’s. These columns are called principal axes. They span the entire
Cartesian space, using new principal coordinates.
10. In terms of principal coordinates, where is the particle? Hint: it is always at the
same principal coordinates: (∥r∥, 0, 0).
11. Show that there are two principal axes that could be deﬁned in many different
ways. Still, pick arbitrarily one particular choice.

82
2
Vector Product with Applications in Geometrical Mechanics
2.7.3
The Inertia Matrix
1. Let I be the 3 × 3 identity matrix. Deﬁne the 3 × 3 inertia matrix of the particle:
A ≡A(r) ≡∥r∥2I −rrt.
2. Show that A is symmetric.
3. Show that r is an eigenvector of A, with the zero eigenvalue.
4. Show that the above q’s are eigenvectors of A, with the eigenvalue ∥r∥2.
5. Conclude that A is positive semideﬁnite: its eigenvalues are greater than or equal
to zero. These eigenvalues are called moments of inertia.
6. Show that A has the diagonal form
A = O
⎛
⎝
0
∥r∥2
∥r∥2
⎞
⎠Ot.
7. Recall that
ω ≡
⎛
⎝
ω1
ω2
ω3
⎞
⎠∈R3
is the angular velocity. To help visualize things better, we’ve assumed so far that
ω was perpendicular to r. This, however, is not a must: from now on, let’s drop
this assumption. Show that, even if ω is no longer perpendicular to r, Aω still is
(Aω,r) = 0.
8. Show that
∥ω × r∥2 = ∥ω∥2∥r∥2 −(ω,r)2.
Hint: see Sect.2.3.3.
9. Conclude that
∥ω × r∥2 = ωt Aω.
Hint: thanks to associativity,
∥ω × r∥2 = ∥ω∥2∥r∥2 −(ω,r)2
= ∥r∥2ωtω −

ωtr
 
rtω

= ∥r∥2ωt Iω −ωt 
rrt
ω
= ωt Aω.
10. Show that the entire principal axis system rotates about the ω-axis, carrying the
“passive” particle at angle ∥ω∥per second.

2.7 Exercises
83
2.7.4
Triple Vector Product
1. Let v and w be linearly independent vectors in R3: v is not a scalar multiple
of w, but points in a different direction. Let η be the angle between v and w
(0 < η < π). Let p be yet another vector, perpendicular to v in the v-w plane
(Fig.2.13). Show that
p × (v × w) = (p, w)v.
Hint: use the fact that
cos
π
2 + η

= −sin(η).
2. Conclude that
p × (w × v) = −(p, w)v.
3. In the latter formula, interchange the roles of v and w.
4. Conclude that
q × (v × w) = −(q, v)w,
where q is perpendicular to w in the v-w plane.
5. Add these formulas to each other:
(p + q) × (v × w) = (p, w)v −(q, v)w = (p + q, w)v −(p + q, v)w,
where p is perpendicular to v and q is perpendicular to w in the v-w plane.
6. Show that every vector u in the v-w plane could be written as u = p + q,
where p is perpendicular to v and q is perpendicular to w in the v-w plane. Hint:
because v and w are linearly independent of each other, so are also p and q.
Geometrically, this exercise is just the parallelogram rule (Fig.1.2).
7. Conclude that
u × (v × w) = (u, w)v −(u, v)w,
where u is just any vector in the v-w plane.
v
p
w
η
Fig. 2.13 In the v-w plane, p is orthogonal to v, but not to w

84
2
Vector Product with Applications in Geometrical Mechanics
angular velocity: ω
r
m∥ω∥2ˆr – centrifugal force
origin
Fig. 2.14 The more general case, in which r is not necessarily perpendicular to ω. Let ˆr be the part
of r that is perpendicular to ω. Then, the centrifugal force is m∥ω∥2ˆr rightward
8. Extend the above formula to a yet more general u that may lie outside the v-w
plane as well. Hint: the component of u that is perpendicular to the v-w plane
contributes nothing to either side of the above formula.
9. Use the above formula to obtain once again the orthogonal decomposition of the
linear momentum at the end of Sect.2.4.2. Hint: set u = v = r (the position of
the particle) and w = p (the linear momentum).
10. Use the above formula to write the centrifugal force (Fig.2.8) in a more general
case, in which ω and r are not necessarily perpendicular to each other, so the
rotating axes do not necessarily align with them anymore. Hint:
−ω × (ω × r) = −(ω,r)ω + (ω, ω)r
= ∥ω∥2

r −(ω,r)
∥ω∥2 ω

= ∥ω∥2ˆr,
where ˆr is the part of r that is perpendicular to ω (Fig.2.14).
11. Prove the above result in a more geometrical way. Hint: note that ω ×r = ω × ˆr.
Therefore, ω × (ω × r) = ω × (ω × ˆr).
12. Does the centrifugal force really exist? Hint: only in the rotating axis system! In
the static axis system, on the other hand, it has no business to exist. Indeed, in
Figs.2.6 and 2.7, the velocity is often constant, meaning equilibrium: no force
at all.
13. Does the centripetal force exist? Hint: only if supplied by some source, such as
gravity.
14. What is the role of the centripetal force? Hint: in the rotating axis system, it
balances the centrifugal force, and cancels it out. This way, there is no force
at all, so the particle always has the same (rotating) coordinates, and is carried
“passively” by the rotating axis system round and round forever. In the static
axis system, on the other hand, the centripetal force has a more “active” role: to
make u turn. This indeed makes the particle go round and round, as required.
15. At the end of Sect.2.5.3, there is a relation between the angular velocity and
the angular momentum. Extend it to the more general case, in which ω is no
longer perpendicular to r, so the rotating axes no longer align with it. For this

2.7 Exercises
85
purpose, assume that the angular velocity is given. How to uncover the angular
momentum? Show that, if w in Fig.2.7 is radial, then the angular momentum
could be obtained from the angular velocity:
r × p = m · r × v
= m · r × (u + w)
= m · r × u
= m · r × (ω × r)
= m ((r,r)ω −(r, ω)r)
= m∥r∥2

ω −(r, ω)
∥r∥2 r

= m∥r∥2 ˆω,
where ˆω is the part of ω that is perpendicular to r (Fig.2.15).
16. Prove the above result in a more geometrical way. Hint: note that ω ×r = ˆω ×r.
Therefore, r × (ω × r) = r × ( ˆω × r).
17. Conclude that, if w in Fig.2.7 is radial, then the angular momentum has a yet
simpler form:
r × p = m Aω,
where A ≡A(r) is the inertia matrix introduced above.
18. Look at things the other way around. Assume that the angular momentum is now
available. How to deﬁne the angular velocity? This could be done as follows: let
ω have some radial component. Deﬁne its other component by
ˆω ≡r × p
m∥r∥2 .
Show that, this way, w in Fig.2.7 must be radial.
2.7.5
Conservation of Angular Momentum
1. In Fig.2.15, the angular momentum is not conserved! After all, it is proportional
to ˆω, which changes direction to keep pointing obliquely (upward and inward).
Why isn’t angular momentum conserved? Hint: only in a closed (isolated) system
is angular momentum conserved. The particle, however, is not isolated: there is
an external force acting upon it—a horizontal centripetal force, which makes it
rotate about the vertical ω-axis.
2. To supply this centripetal force, introduce a second particle at position

86
2
Vector Product with Applications in Geometrical Mechanics
Fig. 2.15 Let ˆω be the part of ω that is perpendicular to r. If w is radial, then the angular momentum
is m∥r∥2 ˆω. This is nonconstant: it must change direction to point not only upward but also inward.
Why isn’t angular momentum conserved? Because the system is not closed or isolated: a horizontal
centripetal force must be supplied from the outside
Fig. 2.16 The particles atr andr−attract each other just enough to supply the horizontal centripetal
force requiredtomake themrotate togetherabout the verticalω-axis.Thisisa closedisolatedsystem:
no external force acts upon it. This is why the total angular momentum is now conserved: ˆω + ˆω−
keeps pointing straight upward
r−≡
⎛
⎝
−x
−y
z
⎞
⎠,
on the other side of the vertical ω-axis (Fig.2.16). Its inertia matrix is A(r−).
What are its eigenvectors? Could they be the same as those of A(r)? Hint: only
if r−is proportional or perpendicular to r.

2.7 Exercises
87
3. What are the principal axes of the second particle? Are they the same as those
of the ﬁrst particle?
4. In terms of its own principal coordinates, where is the second particle? Hint: it
is always at the same principal coordinates: (∥r∥, 0, 0).
5. Let ˆω−be the part of ω perpendicular to r−. Show that the angular momentum
of the second particle is
m∥r−∥2 ˆω−= m A(r−)ω.
6. Together, these particles make a closed system: for a suitable ∥r∥, they attract
each other just enough to supply the horizontal centripetal force required to
make them rotate together about the vertical ω-axis. This is called the two-body
problem. What is the total angular momentum? Where does it point? Must it be
vertical?
7. Show that the total angular momentum is now conserved. Hint: the sum ˆω + ˆω−
always points straight upward, with no inward component anymore.
8. Deﬁne the inertia matrix of the two-body system:
B ≡B(r) ≡A(r) + A(r−).
Write the total angular momentum as
m∥r∥2 
f ˆω + ˆω−
= m

A(r)ω + A(r−)ω

= mBω.
9. Without calculating B explicitly, show that it is symmetric.
10. Conclude that B has three orthogonal eigenvectors. Hint: see Chap.1, Sect.1.9.5.
11. Could these new eigenvectors be the same as those of A(r)? Hint: only if r−is
proportional or perpendicular to r.
12. Show that B is still positive semideﬁnite: its eigenvalues are greater than or equal
to zero. Hint: every three-dimensional vector v could be decomposed as a linear
combination of eigenvectors of A(r) or A(r−). Therefore,
(v, Bv) = (v, A(r)v) + (v, A(r−)v) ≥0.
13. Show that if r−̸= −r, then B is also positive deﬁnite: its eigenvalues are strictly
positive. These are the moments of inertia of the two-body system.
14. Design an eigenvector of B that is perpendicular to both r and r−. Hint: if
r−̸= −r, then take r × r−.
15. What is its eigenvalue? Hint: 2∥r∥2.
16. Still, this eigenvector depends on r. Design yet another eigenvector that doesn’t
depend onr, and remains the same throughout the rotation. Hint: take the vertical
vector (0, 0, 1)t.
17. What is its eigenvalue? Hint: 2(∥r∥2 −z2) = 2(x2 + y2).
18. Could it possibly be negative or zero? Hint: no! If it were zero, then r−= r,
which is impossible.

88
2
Vector Product with Applications in Geometrical Mechanics
19. Conclude that our vertical ω remains an eigenvector of B all the time.
20. Conclude that the total angular momentum mBω remains always vertical.
21. Conclude once again that the total angular momentum is indeed conserved.
22. Note that this is just a special case of a more general theorem: in a general closed
system, if (and only if) the angular velocity is an eigenvector of the inertia matrix
of the system, then it remains constant, unchanged throughout the entire rotation
around it, and always proportional to the total angular momentum of the system,
which remains constant as well.
23. Use the orthogonal eigenvectors of B to design principal axes for the two-body
system.
24. Could they be the same as those of the ﬁrst particle alone? Hint: only if r−is
proportional or perpendicular to r.
25. Which principal axis is independent of r, and remains the same throughout the
entire rotation? Hint: the vertical z-axis.
26. In Fig.2.8, even if ω is not perpendicular to r, show that the centrifugal force
could be written simply as
−mω × (ω × r) = m A(ω)r.

Chapter 3
Markov Chain in a Graph
So far, we’ve mostly used small matrices, with a clear geometrical meaning: 2 × 2
matrices transform the Cartesian plane, and 3 × 3 matrices transform the entire
Cartesian space. What about yet bigger matrices? Fortunately, they may still have
a geometrical meaning of their own. Indeed, in graph theory, they may help design
a weighted graph, and model a stochastic ﬂow in it. This makes a Markov chain,
converging to a unique steady state. This has a practical application in modern search
engines on the Internet [44].
3.1
Characteristic Polynomial and Spectrum
3.1.1
Null Space and Characteristic Polynomial
In this chapter, we’ll see how useful matrices are in graph theory. This will help
design a practical ranking algorithm for search engines on the Internet. Before going
into this, let’s see some more background in linear algebra.
Let A be a square (real or complex) matrix of order n. In many cases, one might
want to focus on the eigenvalues alone. After all, they tell us how A acts on impor-
tant nonzero vectors: the eigenvectors. How to characterize the eigenvalues, without
solving for the eigenvectors?
For this purpose, let I be the identity matrix of order n. Let λ be some (unknown)
eigenvalue. Then A −λI maps the (unknown) eigenvector to zero. In other words,
the eigenvector belongs to the null space of A −λI (Chap.1, Sect.1.9.2).
So, there is no need to know the eigenvector explicitly: it is sufﬁcient to know
that A −λI maps it to the zero vector. This means that A −λI has no inverse. After
all, no matrix in the world could possibly map the zero vector back to the original
nonzero eigenvector. Thus, A −λI must have zero determinant:
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_3
89

90
3
Markov Chain in a Graph
det(A −λI) = 0.
This is called the characteristic equation. It characterizes the eigenvalue λ in a simple
algebraicway,asrequired.Evenbeforeλisknown,wealreadyknowsomethingabout
it: it solves the characteristic equation
det(A −μI) = 0,
whereμstands for ageneral (unspeciﬁed) complexnumber: theindependent variable.
Once the characteristic equation is solved, one could go ahead and solve for the
eigenvector too, if necessary.
The left-hand side det(A −μI) is called the characteristic polynomial in the
independent variable μ. So, the original eigenvalue λ is a root of the characteristic
polynomial: a special argument, for which the characteristic polynomial vanishes,
and the characteristic equation is solved. There is at least one root, and at most n
distinct roots, each with its own private eigenvector.
3.1.2
Spectrum and Spectral Radius
Let’s place the eigenvalues in a new set:
spectrum(A) ≡{μ ∈C | det(A −μI) = 0} .
This is the spectrum of A: the set of eigenvalues.
How large could an eigenvalue be in magnitude? This is called the spectral radius:
ρ(A) ≡
max
μ∈spectrum(A) |μ|.
Clearly, the spectral radius is a nonnegative real number. How large could it be? Well,
it can’t exceed the maximal row-sum:
ρ(A) ≤max
1≤i≤n
n

j=1
|ai, j|,
or the maximal column-sum:
ρ(A) ≤max
1≤j≤n
n

i=1
|ai, j|.
This is proved in the exercises below.
Note that ρ(A) is not necessarily an eigenvalue on its own right. After all, even
if A is a real matrix, its eigenvalues (and eigenvectors) are not necessarily real: they

3.1 Characteristic Polynomial and Spectrum
91
could be complex as well. For this reason, ρ(A) is not necessarily an eigenvalue: it
is just the absolute value of a (complex) eigenvalue.
We already know that, if A is Hermitian, then its eigenvalues are real:
A = Ah ⇒spectrum(A) ⊂R.
(Here, ‘⊂’ means “contained in”.) In this case, either ρ(A) or −ρ(A) must be an
eigenvalue. Nevertheless, even in this case, the eigenvectors may still be complex.
Only if A is a real symmetric matrix must it have not only real eigenvalues but also
real eigenvectors. Of course, the real eigenvector is deﬁned up to a scalar multiple, so
it could always be multiplied by a complex scalar to produce a comlex eigenvector
as well. Still, to make your life easy, better design a real eigenvector, and stick to
it. For this purpose, given a complex eigenvector, just look at its real part (or its
imaginary part): this is indeed a real eigenvector, easy to use. (See exercises at the
end of Chap.1.).
3.2
Graph and Its Matrix
3.2.1
Weighted Graph
What is a graph? It is modeled in terms of two sets: N and E. N contains the nodes,
and E contains the edges. Each edge is a pair of two nodes. Geometrically, the edge
leads from one node to the other.
In a weighted graph, each edge is assigned a nonnegative number: its weigh, or
the amount it could carry. This way, the edge ( j, i) ∈E (leading from node j to
node i) has the weight ai, j ≥0. This lets the amount ai, j to ﬂow from node j to node
i. If, on the other hand, ( j, i) /∈E, then no edge leads from node j to node i, and
ai, j ≡0.
3.2.2
Markov Matrix
Let’s index the nodes by the index
i = 1, 2, 3, . . . , |N|,
where |N| is the total number of nodes. Consider the jth node (1 ≤j ≤|N|). How
much weight ﬂows from it to all nodes in N? Well, assume that this weight sums to 1:
|N|

i=1
ai, j = 1.
This way, ai, j can be viewed as the probability that a particle based at node j would
pick edge ( j, i) to move to node i. In particular, if ( j, j) ∈E, then there is a small

92
3
Markov Chain in a Graph
circle: an edge leading from node j to itself. In this case, the particle could stay at
node j. The probability for this is a j, j ≥0.
The weights (or probabilities) can now be placed in a new |N| × |N| matrix:
A ≡

ai, j

1≤i, j≤|N| .
This is the probability matrix, or the Markov matrix: its columns sum to 1. Let’s use
it to describe a discrete stochastic ﬂow.
3.2.3
Example: Uniform Probability
Consider those edges issuing from node j:
outgoing( j) ≡{( j, i) ∈E | i ∈N} ⊂E.
This way, |outgoing( j)| is the total number of those edges issuing from j. Consider
a simple example, with a uniform probability: the particle has no preference—it is
equally likely to move to any neighbor node:
ai, j ≡

1
|outgoing( j)|
if ( j, i) ∈E
0
if ( j, i) /∈E.
Why is this a legitimate probability? Because the columns sum to 1:
|N|

i=1
ai, j =

{i∈N | ( j,i)∈E}
1
|outgoing( j)|
=

( j,i)∈outgoing( j)
1
|outgoing( j)|
= |outgoing( j)|
1
|outgoing( j)|
= 1.
3.3
Flow and Mass
3.3.1
Stochastic Flow: From State to State
Let’s use the weights (or probabilities) to form a new discrete ﬂow, step by step. The
ﬂow is stochastic: we can never tell for sure what the result of a particular step is,
but only how likely it is to happen.

3.3 Flow and Mass
93
So far, we’ve assigned weights to the edges. These weights are permanent: they
are assigned once and for all, and never change any more.
Assume now that each node contains a nonnegative mass. These masses are dif-
ferent from the above weights: they may change dynamically, step by step.
At the beginning, node j contains the initial mass u j ≥0. These masses can be
placed in a new |N|-dimensional column vector:
u ≡

u1, u2, u3, . . . , u|N|
t .
This is the initial state: the mass distribution among the nodes in N.
Next, what happens to the mass in the jth node? Well, in the ﬁrst step, u j may
split into tiny bits, each ﬂowing to a different neighbor node: each edge of the form
( j, i) transfers the amount ai, ju j from node j to node i. This way, the original mass
is never lost, but only redistributed among the neighbor nodes:
|N|

i=1
ai, ju j = u j
|N|

i=1
ai, j = u j · 1 = u j.
This is indeed mass conservation. This will be discussed further below.
In the ﬁrst step, node j may lose its original mass through outgoing edges. Fortu-
nately, at the same time, it may also gain some new mass through incoming edges.
In fact, through each incoming edge of the form (i, j) ∈E, it gains the new mass of
a j,iui Thus, at node j, the mass has changed from
u j →
|N|

i=1
a j,iui = (Au) j.
This is true for each and every node j ∈N. In summary, the mass distribution has
changed from the original state u to the new state Au:
u →Au.
This completes the ﬁrst step. The same procedure can now repeat in the next step as
well, to change the state from
Au →A(Au) = A2u,
and so on.
The process may then continue step by step forever. Thanks to mass conservation,
the total mass never changes.

94
3
Markov Chain in a Graph
3.3.2
Mass Conservation
What is mass conservation? It means that the total mass remains unchanged. Indeed,
since the columns of A sum to 1, the total mass after the step is the same as before:
|N|

i=1
(Au)i =
|N|

i=1
|N|

j=1
ai, ju j
=
|N|

j=1
|N|

i=1
ai, ju j
=
|N|

j=1
u j
|N|

i=1
ai, j
=
|N|

j=1
u j
 |N|

i=1
ai, j

=
|N|

j=1
u j · 1
=
|N|

j=1
u j.
The same is true in subsequent steps as well. By mathematical induction, mass is
preserved throughout the entire process.
This is indeed an inﬁnite process that may go on and on forever. Does it converge
to a steady state? To answer this, we must study the spectrum of A.
3.4
The Steady State
3.4.1
The Spectrum of Markov Matrix
Fortunately, the spectral radius of A is as small as
ρ(A) ≤max
1≤j≤n
n

i=1
|ai, j| = 1
(see exercises below). Is it exactly
ρ(A) = 1?

3.4 The Steady State
95
Well, to check on this, let’s look at the transpose matrix At. We already know that its
eigenvalues are the complex conjugate of those of A (Chap.1, Sect.1.9.3). Therefore,
both have the same spectral radius:
ρ

At
= ρ(A) ≤1.
What else do we know about At? Well, its rows sum to 1. Therefore, we already have
one eigenvector: this is the constant |N|-dimensional vector
c ≡(1, 1, 1, . . . , 1)t ,
satisfying
Atc = c.
So, for At, 1 is indeed an eigenvalue. As a result,
ρ

At
= 1.
Is 1 an eigenvalue of A as well? It sure is. After all,
¯1 = 1
is an eigenvalue of A as well. Thus,
ρ(A) = 1
as well.
In summary, both A and At share a common eigenvalue: 1. Still, they don’t
necessarily share the same eigenvector. For At, this is the constant vector. For A, on
the other hand, it could be completely different. Fortunately, we can still tell how it
looks like. For this purpose, we need a new assumption.
3.4.2
Converging Markov Chain
Let’s make a new assumption: our graph is well-connected—no node could drop
from it. In other words, all nodes in N are important—no node is redundant. Every
node is valuable—it may be used to receive some mass at some step. Dropping it
may, therefore, spoil the entire ﬂow.
Because all nodes may be used to take some mass, N has no invariant subset, from
which no mass ﬂows away. This means that the ﬂow is global: there is no autonomous
subgraph, in which the original mass circulates forever, never leaking to the rest of
the graph.

96
3
Markov Chain in a Graph
In this case, we say that A is irreducible. This is a most desirable property: it
guarantees that the inﬁnite ﬂow makes a Markov chain that converges to a unique
steady state.
Why is this true? Well, we already know that A has eigenvalue 1. Thanks to the
above assumption, we can now tell how the corresponding eigenvector looks like:
• A has a unique eigenvector v of eigenvalue 1:
Av = v
(up to a scalar multiple).
• All its components are positive:
v j > 0,
1 ≤j ≤|N|.
• To have this property, v is deﬁned up to multiplication by a positive number.
• 1 is maximal—all other eigenvalues are strictly smaller than 1 in magnitude:
μ ∈spectrum(A) ⇒|μ| < 1.
This is indeed the Peron–Frobenius theory [73].
Thanks to these properties, the inﬁnite ﬂow makes a Markov chain that converges
to a unique steady state. Even if the mass is initially concentrated in just one node,
it will eventually get distributed globally among all nodes.
3.4.3
The Steady State
To design the steady state, let u be the initial state. Let us write
u = v + w,
where v is the unique eigenvector corresponding to 1 (Sect.3.4.2), and the residual
(or remainder, or error) w is a linear combination of the other (pseudo-) eigenvectors,
associated with eigenvalues smaller than 1 in magnitude. This way, as the process
progresses, we have
∥Anw∥→n→∞0,
so
Anu = An(v + w) = Anv + Anw = v + Anw →n→∞v.
Thus, the inﬁnite ﬂow converges to the steady state v.

3.4 The Steady State
97
We’re not done yet. After all, v is not deﬁned uniquely. In fact, although v contains
positive components only, these components are deﬁned up to multiplication by a
positive constant. How to specify v uniquely?
Fortunately, thanks to mass conservation, we can tell the total mass in v:
|N|

j=1
v j =
|N|

j=1
u j.
This determines v uniquely, as required. Moreover, this also shows that the error w
must have zero total mass. After all, unlike u and v, w could contain not only positive
but also negative masses, which cancel each other, and sum to zero.
3.4.4
Search Engine in the Internet
How to use a Markov chain in practice? Well, let’s use it to model a communication
networks, such as the Internet. Each site is considered as an individual node. A link
from one site to another makes an edge. This way, each site may contain a few links:
its outgoing edges. For simplicity, assume that the probability to click on such a link
is uniform, as in Sect.3.2.3.
You, the surfer, play here the role of the particle. Initially, you are at the kth site,
for some 1 ≤k ≤|N|. While surﬁng, you may use a link to move to another site.
Still, if this is a good site, and you are likely to stay there for long, then ak,k could
be nearly 1.
In stochastic terms, you must eventually approach the steady-state: v. How likely
are you to enter the lth site (1 ≤l ≤|N|)? Just look at the positive component vl,
and see how large it is.
Still, you are not the only one. Like you, there are many other surfers. Thus, mass
could stand here for the number of surfers at a particular site. The initial mass u tells
us the initial distribution of surfers. The steady-state v, on the other hand, tells us the
“ﬁnal” distribution: how likely are the surfers to enter a particular site eventually.
Now, suppose that you are interested in some keyword, and want to search the
web for those sites that contain it. Still, there are many correct answers: many sites
may contain the same keyword. How should the search engine order (or rank) the
answers? This is called the ranking problem.
For simplicity, assume that the search engine knows nothing about your surﬁng
habits (which is highly unlikely these days). How should it rank the answers to your
search? Well, it should start from the maximal component in v. If it indeed contains
the keyword, then it should be ranked ﬁrst. After all, this must be a popular site, in
which you must be interested. Next, rank the second maximal component in v, and
so on.
Still, the web is dynamic, not static. Every day, new sites are added, and old
sites drop. For this reason, a good search engine should better update A often, and

98
3
Markov Chain in a Graph
recalculate v often. This is indeed an eigenvector problem. (Later on in the book,
we’ll present it in a more general form.) Fortunately, the positive constant that may
multiply v is immaterial: it has no effect on the ranking.
3.5
Exercises
3.5.1
Gersgorin’s Theorem
1. Let A be an n × n (complex) matrix. Let λ be an eigenvalue, associated with the
eigenvector v:
Av = λv,
v ̸= 0.
Pick a speciﬁc i, for which vi is a maximal component in v (in absolute value):
|v j| ≤|vi|,
1 ≤j ≤n.
Show that
|vi| > 0.
Hint: otherwise, v = 0, which is impossible for an eigenvector.
2. For the above i, show that the eigenvalue is not too far from the main-diagonal
element:
λ −ai,i
 ≤

1≤j≤n, j̸=i
ai, j
 .
This is Gersgorin’s theorem. Hint:
λ −ai,i
 · |vi| =

λ −ai,i

vi

=


1≤j≤n, j̸=i
ai, jv j

≤

1≤j≤n, j̸=i
ai, jv j

=

1≤j≤n, j̸=i
ai, j
 ·
v j

≤

1≤j≤n, j̸=i
ai, j
 · |vi|
= |vi|

1≤j≤n, j̸=i
ai, j
 .

3.5 Exercises
99
Now, divide by |vi| > 0.
3. For the above i, conclude that the eigenvalue is as small as the row-sum (in
absolute value):
|λ| ≤
n

j=1
|ai, j|.
Hint: use Gersgorin’s theorem:
|λ| −|ai,i| ≤
λ −ai,i
 ≤

1≤j≤n, j̸=i
ai, j
 .
4. Conclude that
ρ(A) ≤max
1≤i≤n
n

j=1
|ai, j|.
Hint: the above could be done for each and every eigenvalue.
5. Do the same for the Hermitian adjoint. Conclude that
ρ(A) = ρ

Ah
≤max
1≤j≤n
n

i=1
|ai, j|.
6. Assume now that A is a Markov matrix. What does this mean? Hint: its elements
are positive or zero, and its columns sum to 1.
7. Must A have a real spectrum? Hint: no—A might be nonsymmetric.
8. What about the transpose matrix, At? Must it be a Markov matrix as well? Hint:
the rows of A not necessarily sum to 1.
9. Show that
ρ(At) ≤1.
Hint: use Gersgorin’s theorem above.
10. Does this necessarily mean that
ρ(At) = 1?
Hint: only if you could ﬁnd at least one eigenvalue of modulus 1.
11. Design an eigenvector for At. Hint: the constant vector c.
12. Show that
Atc = c.
13. Conclude that
1 ∈spectrum(At).

100
3
Markov Chain in a Graph
14. Conclude that
ρ(At) = 1.
15. Conclude that
1 ∈spectrum(A)
as well. Hint: see Chap.1, Sect.1.9.3. After all, the complex conjugate of 1 is 1
as well.
16. Conclude that
ρ(A) = 1
as well.
17. Conclude that the spectrum of A lies in the closed unit circle in the complex
plane:
spectrum(A) ⊂{z ∈C | |z| ≤1} .
18. Let v be the eigenvector of A satisfying
Av = v.
Prove that v indeed exists, and that v ̸= 0. Hint: we’ve already seen that 1 is an
eigenvalue of A.
19. Must v be the constant vector? Hint: the constant vector is an eigenvector of At,
but not necessarily of A.
20. Assume that A is also irreducible (Sect.3.4.2). What can you say now about v?
Hint: up to a scalar multiple, v is unique, and has positive components only.
21. What can you say now about the spectrum of A? Hint: 1 is the only eigenvalue of
magnitude 1: all other eigenvalues are strictly smaller than 1 in absolute value:
μ ∈spectrum(A), μ ̸= 1 ⇒|μ| < 1.
22. Could A have an eigenvalue of the form
exp(θ
√
−1) = cos(θ) + sin(θ)
√
−1
for any 0 < θ < 2π? Hint: no—this complex number has magnitude 1, but is
different from 1.
23. Conclude that the spectrum of A lies in the open unit circle, plus the point 1:
spectrum(A) ⊂{z ∈C | |z| < 1} ∪{1}.
(Here, ‘∪’ means a union with the set that contains one element only: 1.)

3.5 Exercises
101
24. Use the above properties to prove that the Markov chain indeed converges to a
steady-state. Hint: see Sect.3.4.3.
25. Why is the steady-state unique? Hint: mass conservation.
26. How could this help design a good search engine on the internet? Hint: see
Sect.3.4.4.
27. How often should the search engine update A, and solve a new eigenvector
problem, to update v as well?

Chapter 4
Special Relativity: Algebraic Point
of View
To model static shapes in the plane, the ancient Greeks introduced Euclidean geome-
try. To model motion, on the other hand, Newton added a new time axis, perpendicular
to the plane. This may help model a force, applied to an object from the outside to
accelerate its original motion.
This ﬁts well in Plato’s philosophy: to think about a general concept, we must
introduce a new word in our language, to represent not only one concrete instance
but also the godly spirit behind all possible instances. Likewise, a Newtonian force
acts from the outside, to give life to a static object.
Einstein, on the other hand, returned the time dimension back into the very heart
of geometry. This way, time is not different from any other spatial dimension. Once
the time axis is united with the original spatial axes, we have a new four-dimensional
manifold: spacetime.
This is more in the spirit of Aristotle’s philosophy. A word takes its meaning not
from the outside but from the very inside: the deep nature of the general concept it
stands for.
To introduce special relativity [18, 78], matrices are most useful. Indeed, to rep-
resent a Lorentz transformation, just use a small 2 × 2 matrix. This may improve on
Newtonian mechanics, and provide a more accurate way to add velocities.
Thanks to this new matrix, we also have a new (relative) deﬁnition of energy and
momentum.Thisishowtruephysicalquantitiesshouldindeedbedeﬁned:completely
independent of the coordinate system that happens to be used. To transform from
system to system, just use the same 2 × 2 Lorentz matrix.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_4
103

104
4
Special Relativity: Algebraic Point of View
4.1
Systems and Their Time
4.1.1
How to Add Velocities?
In Newtonian mechanics, velocities are added linearly. Consider, for example, a
particle that travels at the constant velocity v at some constant direction, while another
particle travels at the constant velocity u at the opposite direction (Fig.4.1). These
velocities are not absolute: they are only relative to our lab.
Still, our lab is not an absolute reference point: it is not static, but dynamic as
well. After all, it is on the earth, which travels around the sun, which travels around
the center of our galaxy: the Milky Way. Fortunately, this underlying motion could
be disregarded. After all, we don’t feel it at all. In fact, we are only interested in the
velocities of the particles with respect to our lab. For this purpose, we may assume
that the lab is at rest.
Better yet, we are even more interested in the relation between the individual
particles: how fast do they get away from each other? For this purpose, we better
eliminate the lab altogether, and look at one particle from the other.
How fast does the ﬁrst particle get away from the second one? In Newton’s theory,
the velocities add up, so the answer is simply u + v. For small velocities, this indeed
makes sense. But what happens when v is as large as the speed of light c? In this case,
the sum c + u would exceed the speed of light, which is impossible, as is evident
from experiment.
Fortunately, velocities are added nonlinearly: the accurate answer is not u +v but
rather
u + v
1 + uv
c2
(Fig.4.2).Clearly,solongasbothu andv aremoderate,thisisnearlythesameasu+v.
This is why Newton’s theory works well in practical engineering problems. Still,
strictly speaking, it is not quite accurate. At high velocities, this subtle inaccuracy
becomes crucial.
Fig. 4.1 In our lab, the ﬁrst particle moves rightwards at velocity v, while the second particle moves
leftwards at velocity u
Fig. 4.2 Away from the second particle, the ﬁrst particle moves at velocity (u + v)/(1 + uv/c2),
not u + v

4.1 Systems and Their Time
105
4.1.2
Never Exceed the Speed of Light!
Thanks to the above formula, we can now be consistent, and keep a universal rule:
never exceed the speed of light! Indeed, assume that, with respect to the lab, both
particles don’t exceed the speed of light:
|u| ≤c and |v| ≤c.
Then, with respect to each other, the particles don’t exceed the speed of light either.
To prove this algebraically, we must ﬁrst scale the velocities properly.
The original velocities are not scaled right yet. To be scaled better, they should
be divided by the speed of light:
βu ≡u
c and βv ≡v
c .
Einstein’s law says that c is the same in all systems. Furthermore, no speed could
ever exceed c:
|βu| ≤1 and |βv| ≤1.
Furthermore, assume that both u and v are strictly smaller (in magnitude) than the
speed of light:
|u| < c and |v| < c,
so
|βu| < 1 and |βv| < 1.
In this case,
1 −βu −βv + βuβv = (1 −βu) (1 −βv) > 0,
so
βu + βv < 1 + βuβv,
or
βu + βv
1 + βuβv
< 1.
Moreover, if both u and v change sign, then the above is still valid. Therefore, we
also have

βu + βv
1 + βuβv
 = |βu + βv|
1 + βuβv
< 1.
By multiplying both sides of this inequality by c, we also have

u + v
1 + uv
c2
 < c.

106
4
Special Relativity: Algebraic Point of View
This is a good result: our new formula for adding velocities is consistent! Indeed,
with respect to each other, the particles never exceed the speed of light, as required.
So far, we’ve assumed that no particle is as fast as light. What happens when one
of them is? In this case, things may get strange. Consider, for instance, the following
extreme case:
v = c and
−c < u ≤c.
In this case,
u + c
1 + uc
c2
= u + c
1 + u
c
= c.
This may still make sense: the ﬁrst particle is so fast that it can no longer distinguish
between the lab and the second particle. It views them as one and the same thing,
left behind at the speed of light.
Still, there is a yet stranger case:
v = c and u = −c.
In this (singular) case, the second particle is as fast: it follows the ﬁrst particle at the
speed of light as well. In these extreme circumstances, our rule is no good: it divides
zero by zero.
Fortunately, this case is degenerate and uninteresting: at the initial time of t = 0,
both particles are at the origin, and have the same speed: c. As such, they are no longer
distinguishable: they must be one and the same particle. In this sense, although the
mathematical model fails, physics is still valid.
4.1.3
How to Measure Time?
In our original lab, consider the Cartesian coordinates x, y, and z. Assume that, at
the initial time t = 0, the ﬁrst particle lies at the origin (0, 0, 0). From there, it starts
moving rightwards, toward (1, 0, 0).
Because there is no external force, the particle never accelerates or changes direc-
tion. This models a motion in some ﬁxed direction. After all, the x-axis could have
been picked to ﬁt the original direction. This is actually a one-dimensional motion:
both the y and z coordinates take no part in the motion, and remain the same in all
systems. Thus, they could be ignored and dropped: we “live” in the x-t plane only.
How should the time t be measured? Well, in a standard clock, a time unit is often
measured in terms of length. For example, in one second, the long hand in the clock
makes an angle of 6◦: 1/60 of a complete circle.
In the present context, on the other hand, we might want to use a linear (rather
than circular) clock. For this purpose, just use a light beam. In each second, the light
advances one more light second. This tells us that one more second has passed.

4.1 Systems and Their Time
107
This way, time has been scaled by c. Instead of the original time variable t, mea-
sured in seconds, we actually use the length variable ct, measured in light seconds.
After all, while t increases by one second, ct advances by one light second.
Thus, the mysterious time variable t is better realized by the new length variable
ct, in a new axis: the ct-axis. This way, instead of the x-t plane, we actually focus
on the new x-ct plane.
4.1.4
The Self-System
The original coordinates x, y, and z tell us the position in the lab. Similarly, t tells
us the time, as measured in the lab. Later on, we’ll see that these measurements are
relevant in the lab only. In other systems, on the other hand, they might be different.
For example, the particle also has a self-system that travels with it in the same
direction and at the same speed: v. This system has new (prime) coordinates: x′
and t′. This prime has nothing to do with differentiation: t′ just tells us how much
time has passed since the initial time t′ = 0, as measured by a clock carried by the
self-system, traveling at speed v with respect to the lab. Likewise, x′ tells us our
position with respect to the particle: how far we are from it. If we are to the right of
the particle, then x′ > 0. If, on the other hand, we are to the left of the particle, then
x′ < 0. After all, at all times t′ ≥0, the particle remains at the same position relative
to itself: x′ = 0. As before y′ ≡y and z′ ≡z remain irrelevant, and can be ignored.
How to measure the time in the self-system? As before, better use not t′ but ct′.
Initially, at t′ = t = 0, the particle lies at the origin in both systems:
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
x′
y′
z′
ct′
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
0
0
0
0
⎞
⎟⎟⎠.
At any later time, on the other hand, the systems may start to differ, not only in terms
of position but also in terms of time: t′ may differ from t. Time is relative: it depends
on the system where it is measured.
4.2
Lorentz Transformation and Matrix
4.2.1
Lorentz Transformation
In the self-system, the particle is at rest at
x′ = 0.
In the lab, on the other hand, the particle moves at speed v, so it is at

108
4
Special Relativity: Algebraic Point of View
x = dx
dt t = vt = v
c ct = βvct,
where
βv ≡v
c ≡
dx
d(ct)
is the scaled velocity, obtained from differentiation with respect to the scaled time
ct.
Consider now a more general point at a ﬁxed distance x′ from the particle. So, in
the self system, it is at rest at position x′. In the lab, on the other hand, it moves at
speed v, so it is at
x = x′ + vt = x′ + βvct.
In other words,
x′ = x −βvct.
Let’s use this to transform the original x-ct lab coordinates to the new x′-ct′ self
coordinates. This transformation must be invariant: insensitive to interchanging x
and ct. After all, both measure distance: x measures the distance from the lab’s
origin, whereas ct measures the distance made by a light beam (Sect.4.1.3). Why
not interchange them?
Once the dummy coordinates y′ ≡y and z′ ≡z are dropped, things get clearer:
we obtain the new Lorentz transformation that transforms x and ct into x′ and ct′:
	 x
ct

→
	 x′
ct′

≡γ (βv)
	 1
−βv
−βv
1

 	 x
ct

,
where
γ ≡γ (βv) ≡
1

1 −β2v
≥1
is picked to make sure that the determinant is 1 (Chap.2, Sect.2.1.2). This is the
Lorentz transformation that gives the self coordinates in terms of the lab coordinates.
The Lorentz transformation is given in terms of a symmetric 2 × 2 matrix. This
way, time and space indeed relate symmetrically to each other. In principle, time is
not different from any spatial dimension.
4.2.2
Lorentz Matrix and the Inﬁnity Point
Lorentz transformation preserves area: thanks to the coefﬁcient γ, the Lorentz matrix
has determinant 1.

4.2 Lorentz Transformation and Matrix
109
What happens when |v| is as large as c? In this case, γ is no longer a number: it
is the inﬁnity point ∞. Still, it is assumed that
0 · ∞= 0.
We’ll return to this point later.
Let’s look at another extreme case: v = 0. In this case, Lorentz matrix is just the
2 × 2 identity matrix:
I ≡
	 1 0
0 1

.
This looks rather boring. Still, it may help formulate the Lorentz matrix in the general
case as well.
For this purpose, deﬁne also the 2 × 2 matrix
J ≡
	 0 1
1 0

.
The original Lorentz matrix can now be written as
γ (βv) (I −βv J) .
This will be very helpful later on.
4.2.3
Invariance
Fortunately, the Lorentz matrix commutes with J:
Jγ (βv) (I −βv J) = γ (βv) (I −βv J) J.
Therefore, the Lorentz transformation is invariant under interchanging x and ct:
	ct
x

=
J
	 x
ct

→γ (βv) (I −βv J) J
	 x
ct

=
Jγ (βv) (I −βv J)
	 x
ct

=
J
	 x′
ct′

=
	ct′
x′

.

110
4
Special Relativity: Algebraic Point of View
In other words, the Lorentz transformation is blind (completely insensitive) to inter-
changing x and ct, as required.
4.2.4
Composition of Lorentz Transformations
The Lorentz transformation may also help explain the rule of adding velocities
(Sect.4.1.1). For this purpose, note that every two Lorentz matrices commute with
each other.
Consider the composition of two Lorentz transformations. What does this mean
geometrically? Well, let’s look at our ﬁrst particle, moving rightwards at speed v.
The second particle, on the other hand, moves leftwards at speed u, or rightwards
at speed −u. These velocities are with respect to our original lab, which is assumed
to be at rest.
Fortunately, we can also look at things the other way around: from the perspective
of the second particle, the entire lab “moves” rightwards at speed u. This way, the
second particle is now assumed to be at rest. How does the ﬁrst particle travel away
from it?
To calculate this, we need to compose two motions: the motion of the ﬁrst particle
with respect to the lab, on top of the “motion” of the entire lab away from the second
particle. In other words, we need to compose two Lorentz transformations, or just
multiply two Lorentz matrices. Since
J 2 = I,
the product is
γ (βu) (I −βu J) γ (βv) (I −βv J) = γ (βu) γ (βv) (I −βu J) (I −βv J)
= γ (βu) γ (βv)

I −βu J −βv J + βuβv J 2
= γ (βu) γ (βv) ((1 + βuβv) I −(βu + βv) J)
= γ (βu) γ (βv) (1 + βuβv)
	
I −βu + βv
1 + βuβv
J

= γ
	 βu + βv
1 + βuβv

 	
I −βu + βv
1 + βuβv
J

.
After all, each Lorentz matrix has determinant 1, so the product must have determi-
nant 1 as well (Chap.2, Sect.2.1.3).
This composition describes the total motion of the ﬁrst particle away from the
second one. The total velocity is, thus, not u + v but rather

4.2 Lorentz Transformation and Matrix
111
c βu + βv
1 + βuβv
= u + v
1 + uv
c2
,
as asserted in Sect.4.1.1.
4.2.5
The Inverse Transformation
Let’s look at the special case in which u = −v: the second particle coincides with
the ﬁrst one. In this case, the above composition takes the form
γ (β−v) (I −β−v J) γ (βv) (I −βv J) = γ
	 β−v + βv
1 + β−vβv

 	
I −β−v + βv
1 + β−vβv
J

= γ(0)I
= I.
Thus, the inverse transformation is represented by the inverse matrix:
γ (β−v) (I −β−v J) = γ (βv) (I + βv J) .
This is a legitimate Lorentz matrix as well: it has determinant 1, as required.
What is this geometrically? Well, with respect to the ﬁrst particle, the entire
lab “moves” leftwards at speed v, or rightwards at speed −v. Thus, the inverse
transformation may differ from the original one in sign only: replace v by −v. This
way, the inverse transformation indeed transforms the self coordinates back to the
original lab coordinates, as required.
In other words, the inverse transformation considers the particle to be at rest,
and the entire lab as moving at speed −v away from it. This is why the inverse
transformation only picks a minus sign: it uses −v rather than v.
The inverse Lorentz matrix could also be obtained from Cramer’s formula in
Chap.2, Sect. 2.1.4:
	 1
−βv
−βv
1

−1
=
1
1 −β2v
	 1 βv
βv 1

= γ2 (βv)
	 1 βv
βv 1

.
Indeed, just divide both sides by γ(βv), and you obtain the same inverse matrix as
before.
Later on, in the book, we’ll see a new theory: group theory. We’ll then realize that
the subgroup of Lorentz transformations is represented by (and indeed isomorphic
to) the subgroup of Lorentz matrices: they mirror each other, and are algebraically
the same (Chap.5, Sect.5.1.4). Furthermore, both subgroups are also homeomorphic
to the open interval (−1, 1):
γ (βv) (I −βv J) ↔βv ∈(−1, 1).

112
4
Special Relativity: Algebraic Point of View
4.3
Proper Time in the Self-System
4.3.1
Proper Time
In its self system, the particle is always at x′ = 0. Imagine a tiny clock embedded
inside the particle. In the self-system, what would be the time read from this clock?
This is the proper time of the particle:
s ≡t′.
Fortunately, this time could be calculated not only from the self system but also from
any other system, such as our lab.
In the x′-t′ self-system, the tiny clock is at
(x′, t′) = (0, s).
Let’s use this to form the matrix
	ct′ x′
x′ ct′

=
	cs 0
0 cs

= csI.
Clearly, this matrix has determinant c2s2. Let’s apply the inverse Lorentz matrix to
it. Let’s do this column by column. Let’s start with the second column: the inverse
Lorentz matrix transforms it back to the x-t lab coordinates:
	 x′
ct′

→γ (βv) (I + βv J)
	 x′
ct′

=
	 x
ct

.
Fortunately, the inverse Lorentz matrix commutes with J (Sect.4.2.3). Therefore,
the ﬁrst column transforms in a similar way:
	ct′
x′

= J
	 x′
ct′

→Jγ (βv) (I + βv J)
	 x′
ct′

= J
	 x
ct

=
	ct
x

.
In summary, the entire matrix transforms to
	ct x
x ct

= γ (βv) (I + βv J)
	ct′ x′
x′ ct′

= csγ (βv) (I + βv J) .
Since the inverse Lorentz matrix has determinant 1, the above matrix still has deter-
minant c2s2:
c2t2 −x2 = c2s2.

4.3 Proper Time in the Self-System
113
Fig. 4.3 The proper time of the lab is just t. In the lab, it can be read from a static clock: t1, t2, . . ..
This is the maximal proper time. A clock moving at the constant speed of x/t = v, on the other
hand, has a shorter proper time: t′
1 = s1 < t1, then t′
2 = s2 < t2, and so on
So, we’ve managed to calculate the proper time not only from the self-system but
also from the lab:
s ≡

t2 −x2
c2 .
In these new terms, what is t? It is the proper time of the lab: the time read from
a static clock in the lab. After all, this is how t was deﬁned in the ﬁrst place. Still,
it has yet another (mathematical) meaning: the maximal proper time of any particle
(Fig.4.3). After all, every moving particle would have a shorter proper time. Indeed,
at any time t, the particle would be at x = vt, so its proper time would be
s ≡

t2 −x2
c2 =

t2 −β2vt2 =
t
γ (βv) ≤t.
In summary, if you want to think that a lot of time has passed, then you should better
look at your own static clock. Just hold it in your hand, and look at it. This way, it
will tick fast, telling you that many seconds have passed. If, on the other hand, it
moved towards or away from you, then it would tick more slowly, telling you that
less seconds have passed. Later on, we’ll refer to this as time dilation. It leads to the
twin paradox.

114
4
Special Relativity: Algebraic Point of View


w
my twin
clock
particle
myself
in the lab
Fig. 4.4 The twin paradox: I live in the lab. My twin, on the other hand, lives inside a particle,
getting away at speed v. I say: “my time ticks faster, so I’m older!” My twin, on the other hand,
sees things the other way around, and says: “my time ticks faster, so I’m older!” Who is right?
4.3.2
The Twin Paradox
Suppose that I live in the lab. With me, I have a static clock to show me my proper
time: t. I also have a twin brother, who lives inside the particle, moving at the constant
velocity v away from me (Fig.4.4). With him, he carries his own clock that shows
him his own proper time: s.
As discussed above, I think that my own proper time goes faster, so I’m older.
My twin, on the other hand, views things the other way around. He thinks that he is
static and that I travel at velocity −v away from him. Therefore, he believes that his
own proper time goes faster, and that he is older than me. Who is right?
Here is the answer. Later on, we’ll also discuss yet another effect: length con-
traction. From my own point of view, distances in the moving system are shorter.
Therefore, I’m bigger than my twin: the veins in my body are longer, and the blood
in my body has a longer distance to ﬂow to my heart. This requires more time. For-
tunately, thanks to time dilation, I indeed have more time. In summary, both twins
have the same metabolism, and age in the same rate.
Why did we have a paradox in the beginning? Because we looked at a single
position: x = 0 (or x′ = 0). Through x′ = 0, a complete line passes: the entire
t′-axis. Once transformed from system to system, this axis scales differently, leading
to the twin paradox. To avoid this paradox, better transform a more substantial area,
with some thickness in the x-dimension as well. After all, as a transformation from
R2 to R2, the Lorentz transformation preserves area, as required.
4.3.3
Hyperbolic Geometry: Minkowski Space
In our original lab, let’s look at a ﬁxed time t = t0 > 0. At t0, where could the
particle be? Well, this depends on its velocity: to reach x, the velocity must have
been v = x/t0. Let’s look at all those x’s that could have been reached by any
particle, traveling at any possible speed v, not exceeding the speed of light: ||v ≤c.
This makes a horizontal line segment in the x-t plane:
{(x, t0) | |x| ≤ct0} .

4.3 Proper Time in the Self-System
115
Fig. 4.5 A level set of s – a hyperbola in the original x-t lab coordinates. (x, t) is on the hyperbola
if x could be reached at time t by a particle moving at speed v = x/t with respect to the lab. In the
self-system of the particle, on the other hand, this will happen at proper time s0
This is a level set of t. Indeed, in it, t is constant: t = t0. Still, now we know better:
because it moves at its own speed v, the particle also has its own proper time s. Thus,
we should actually look at a level set of s: the hyperbola

(x, t) | c2t2 −x2 = c2s2
0

,
where s0 is constant.
The motion of such a particle is modeled by the arrow in Fig.4.5. Once the arrow
hits the hyperbola, the particle arrives at x. The tiny clock embedded inside it will
then show its proper time: s0.
Finally, let’s look at all possible s0’s. Together, all these level sets make a new
manifold: the two-dimensional x-s manifold.
4.3.4
Length Contraction
In its self system, the particle is always at x′ = 0. In the lab, on the other hand, it is
at x. Where is this? To answer this, you must know where 0 is in the lab. From 0,
measure x, and you arrive at the correct location.
Thus, the location has no meaning on its own, but only relative to a reference
point: 0. What is meaningful is the distance between two different locations.

116
4
Special Relativity: Algebraic Point of View
Consider, for example, a stick that moves at velocity v with respect to our lab. In
its self-system, the stick is at rest: one endpoint at x′
1, and the other at x′
2. Thus, in
its own system, its length is
△x′ ≡x′
2 −x′
1.
What is the view from the lab? Well, let’s use Lorentz transformation:
	 x′
1
ct′
1

= γ (βv)
	 1
−βv
−βv
1

 	 x1
ct1

and
	 x′
2
ct′
2

= γ (βv)
	 1
−βv
−βv
1

 	 x2
ct2

.
Now, let’s subtract the former equation from the latter:
	 △x′
c△t′

= γ (βv)
	 1
−βv
−βv
1

 	 △x
c△t

.
To measure the length of the moving stick, a viewer who sits in the lab has no access
to the self-system: he/she must use the x-t lab coordinates. For this purpose, he/she
must have both endpoints x1 and x2 at the same time t1 = t2:
	 △x′
c△t′

= γ (βv)
	 1
−βv
−βv
1

 	△x
0

= γ (βv)
	
△x
−βv△x

.
In this equation, the top tells us that
△x′ = γ (βv) △x,
or
△x =
△x′
γ (βv).
Since γ ≥1, |△x| ≤|△x′|. This is called length contraction. From its own self-
system, the stick looks longer than any other system (such as our lab). We’ve already
used this effect to “solve” the twin paradox (Sect.4.3.2).
Moreover, the above length △x (observed from the lab) decreases monotonically
as |v| increases. In the extreme case of |v| = c and γ = ∞, for example, △x = 0.
This means that, from the lab, the stick travels so fast that it seems to shrink to one
point.
This conﬁrms what was already said at the end of Sect.4.1.2: two particles that
follow each other at the speed of light are indistinguishable—they could be consid-

4.3 Proper Time in the Self-System
117
ered as one and the same. This is also why, in a particle that travels at the speed of
light, no change could ever be observed.
4.3.5
Simultaneous Events
In the above, in the lab, both endpoints are measured at the same time t1 = t2. These
are indeed simultaneous events. Still, in the x-t plane, they are not identical. After
all, they take place in two different locations: x1 ̸= x2.
In the self-system, on the other hand, these events are no longer simultaneous.
Indeed, in the equation in Sect.4.3.4, look now at the bottom:
c△t′ = −γ (βv) βv△x = −βv△x′ ̸= 0.
Thus, the events are simultaneous in the lab only. In every other system, on the other
hand, they are no longer simultaneous.
4.3.6
Time Dilation
Thus, in spacetime, two events could differ in time or location or both. So far, we
discussed simultaneous events that happen at the same time. Next, let’s consider
events that take place at the same location, but at different times.
Consider again a tiny clock that moves at velocity v with respect to the lab
(Fig.4.3). In this clock, two different times are measured: t′
2 > t′
1. This is done
at the same place x′
2 = x′
1 in the self-system of the clock.
Thus, in the self-system, the time difference is
△t′ ≡t′
2 −t′
1.
This is indeed the proper time: the time in a clock that is at rest (Sect.4.3.1).
A viewer who looks at the moving clock from the lab, on the other hand, measures
the time difference
△t ≡t2 −t1
Is this the same? Well, from the inverse Lorentz transformation, we have
	 x1
ct1

= γ (βv)
	 1 βv
βv 1

 	 x′
1
ct′
1

and
	 x2
ct2

= γ (βv)
	 1 βv
βv 1

 	 x′
2
ct′
2

.

118
4
Special Relativity: Algebraic Point of View
Let’s subtract the former equation from the latter:
	 △x
c△t

= γ (βv)
	 1 βv
βv 1

 	 △x′
c△t′

= γ (βv)
	 1 βv
βv 1

 	
0
c△t′

= γ (βv)
	 βvc△t′
c△t′

.
In this equation, the bottom tells us that
△t = γ (βv) △t′.
Since γ ≥1, △t ≥△t′. This is time dilation: to read the shortest possible time from
your clock, better read it from the self-system of the clock (where it is static), rather
than from any lab that may travel toward or away from it.
This is also the slowest time: it ticks more slowly, giving a smaller time difference:
△t′ ≤△t. In the beginning, this had led to the twin paradox. Fortunately, together
with length contraction, this makes perfect sense, and “solves” the twin paradox
(Sect.4.3.2).
In the lab, the observed time difference △t increases monotonically with |v|. In
the extreme case of |v| = c and γ = ∞, △t = ∞as well. For this reason, in a
particle that travels as fast as light, no change could ever be observed: every tiny
change would seem to last forever.
4.4
Velocity and Slope
4.4.1
Doppler’s Effect
In the above, the particle gets away from the lab at speed v. Inside the particle, there
is a tiny clock. We assume that a viewer who sits in the lab could still read the time
from this clock. This is still quite theoretical: how could this be done in practice?
After all, this information must travel from the clock back to the lab, at a ﬁnite speed,
not exceeding the speed of light!
In its own self-system, the tiny clock shows time t′. Once read from the lab, on
the other hand, this time transforms to t. For example, as things are observed from
the earth, at time t1 > 0, the particle gets as far as x1 = vt1. At this time, a signal as
fast as light issues from the particle, to carry the news back to the lab. To arrive, it
needs some more time: x1/c = vt1/c. (Here, we assume for simplicity that v > 0,
as in Fig.4.4.) Denote the arrival time by T1. Later on, at time t2 > t1, the next signal
will issue as well, to arrive at T2 > T1.

4.4 Velocity and Slope
119
How to write the arrival time difference in terms of the real-time difference? In
other words, how to write T2 −T1 in terms of the original time difference t′
2 −t′
1,
read in the self-system itself? Well, thanks to time dilation (Sect.4.3.6) and the above
discussion,
△T ≡T2 −T1
= t2 + x2
c −

t1 + x1
c

= t2 + vt2
c −
	
t1 + vt1
c

= t2 −t1 + v
c (t2 −t1)
= (△t) (1 + βv)
= (△t′)γ (βv) (1 + βv)
= (△t′) 1 + βv

1 −β2v
= (△t′)
1 + βv
√1 −βv
√1 + βv
= (△t′)

1 + βv
1 −βv
.
Thus, since v > 0, a movie taken inside the particle would arrive to the lab in slow
motion: an original activity that takes △t′ seconds inside the particle would seem
to take as many as √(1 + βv)/(1 −βv)△t′ seconds upon being watched here in the
lab. This is indeed Doppler’s effect.
4.4.2
Slope: Moebius Transformation
How do things look like from the second particle? Recall that this particle travels
at speed −u with respect to the lab. To describe its self-system, let’s use now the
x-t coordinates. After all, our convention is to use these coordinates in the system
we’re interested in. This system is now not the lab but the self-system of the second
particle.
Let the lab system use now the x′-t′ coordinates. In these coordinates, how does
the ﬁrst particle move? Well, it moves at speed v, making a linear path or trajectory:
x′ = vt′,
or
dx′
dt′ = v,

120
4
Special Relativity: Algebraic Point of View
or
dx′
d (ct′) = dx′
cdt′ = v
c = βv.
In other words, in the two-dimensional vector (x′, ct′)t, the components have a
constant ratio: βv. To transform this vector back to the x-t self-system of the second
particle, apply an inverse Lorentz matrix to it:
	 x′
ct′

→
	 x
ct

= γ (βu)
	 1 βu
βu 1

 	 x′
ct′

.
Consider, for example, the vector (βv, 1)t. It lies in the above path: its components
indeed have ratio βv. Let’s transform it as above:
	 1 βu
βu 1

 	βv
1

=
	 βu + βv
1 + βuβv

.
This is indeed how the ﬁrst particle looks like from the x-t self-system of the second
particle. In this system, to have the new slope, just divide the new top component by
the new bottom component:
dx
cdt = βu + βv
1 + βuβv
.
This is indeed the correct way to add velocities (Sect.4.1.1). After all, this new slope
is also the velocity of the ﬁrst particle, as observed from the second particle.
The Lorentz transformation transforms two-dimensional vector to two-
dimensionalvector.TheMoebiustransformation,ontheotherhand,transformsscalar
to scalar— the original slope to the new slope:
βv = dx′
cdt′ →dx
cdt = βu + βv
1 + βuβv
(Chap.5). Let’s use it to calculate the perpendicular velocity as well.
4.4.3
Perpendicular Velocity
Let us now consider a two-dimensional motion: not only in the x′ but also in the
y′ spatial direction. For this purpose, our lab still uses primes in its coordinates:
x′, y′, and t′. Assume now that the ﬁrst particle moves at velocity (vx′, vy′) with
respect to the lab: velocity vx′ in the positive x′ direction, and also velocity vy′ in
the perpendicular y′ direction. (Note that these subscripts are not partial derivatives,
but just coordinates.) The second particle, on the other hand, still moves at velocity
(−u, 0) in the x′ direction only. This is how things look like from the lab (Fig.4.6).

4.4 Velocity and Slope
121
Fig. 4.6 View from the lab:
the ﬁrst particle travels at
velocity (vx′, vy′), while the
second particle travels at
velocity (−u, 0)
How do things look like from the second particle? Well, to describe the self-system
of the second particle, let’s use the x-y-t coordinates. After all, our convention is to
use these coordinates in the system we’re interested in. The lab, on the other hand,
is less interesting: this is why it is described by the x′-y′-t′ coordinates.
Now, in its own self-system, the second particle is at rest, while the entire lab
moves at velocity (u, 0). In these terms, how does the ﬁrst particle move?
Fortunately, the Moebius transformation in Sect.4.4.2 can now extend, and trans-
form not only dx′/dt′ but also dy′/dt′. For this purpose, we must also use an extended
3 × 3 Lorentz matrix, which leaves the second component unchanged:
⎛
⎝
x′
y′
ct′
⎞
⎠→
⎛
⎝
x
y
ct
⎞
⎠=
⎛
⎝
γ (βu)
1
γ (βu)
⎞
⎠
⎛
⎝
1
βu
1
βu
1
⎞
⎠
⎛
⎝
x′
y′
ct′
⎞
⎠.
(As usual, blank spaces stand for zero matrix elements.)
Still, we are mainly interested in slopes, or ratios between different components.
After all, the slopes tell us in what direction the ﬁrst particle gets farther and farther
away from the second one (Fig.4.7). So, the above three-dimensional vector could
be multiplied by just any (nonzero) scalar, with no effect. (Compare with Chap.6,
Sect.6.4.1.) In fact, we’re only interested in the differential form:
⎛
⎝
dx
dy
cdt
⎞
⎠=
⎛
⎝
γ (βu)
1
γ (βu)
⎞
⎠
⎛
⎝
1
βu
1
βu
1
⎞
⎠
⎛
⎝
dx′
dy′
cdt′
⎞
⎠
=
⎛
⎝
γ (βu)
1
γ (βu)
⎞
⎠
⎛
⎝
1
βu
1
βu
1
⎞
⎠
⎛
⎝
βvx′
βvy′
1
⎞
⎠.
We can now go ahead and divide by dt, to obtain the new slopes (or velocities) dx/dt
and dy/dt, as observed from the second particle. (This way, we actually eliminate
the x′-y′-t′ lab coordinates, and drop them.) As observed from the second particle,

122
4
Special Relativity: Algebraic Point of View
Fig.4.7 Viewfromthe secondparticle: the ﬁrst particle getsawayat a newvelocity: (dx/dt, dy/dt)
in the x-y-t system
the x-velocity of the ﬁrst particle is still
dx
dt = c βu + βvx′
1 + βuβvx′
=
u + vx′
1 + βuβvx′
,
as in Sect.4.4.2. The y-velocity, on the other hand, is
dy
dt = c
βvy′
γ (βu)

1 + βuβvx′
. =
vy′
γ (βu)

1 + βuβvx′
.
Thus, as observed from the second particle, the ﬁrst particle indeed makes the path
in Fig.4.7. To draw it, we now have the new slopes dx/dt and dy/dt in terms of
three known parameters: u, vx′, and vy′.
4.5
Momentum and Energy
4.5.1
Conservation of Momentum
Consider a particle of mass m, moving rightwards at velocity u with respect to the
lab. Suddenly, the particle explodes, and splits into two new subparticles of mass
m/2 each. Thanks to symmetry, with respect to the original particle, one subparticle
ﬂies rightwards at the extra velocity of v, while the other ﬂies leftwards at the extra
velocity of v (Fig.4.8).
In Newtonian mechanics, the momentum is deﬁned as the mass times the velocity
(Chap.2, Sect.2.4.1). This is the linear momentum in the x-spatial dimension. In
these terms, the momentum indeed remains the same:

4.5 Momentum and Energy
123
Fig. 4.8 Conservation of
momentum: after the original
particle (top picture)
explodes and splits into two
subparticles (bottom
picture), the total momentum
is still muγ(βu), as before
m
2 (u + v) + m
2 (u −v) = m
2 (u + v + u −v) = mu.
Thanks to special relativity, however, we already know that this is not the correct
way to add velocities (Sect.4.1.1). To ﬁx this, let’s redeﬁne momentum in a more
accurate way:
muγ (βu)
rather than just mu. This is indeed a relative deﬁnition: it deﬁnes the momentum of
the original particle not absolutely but only relative to the lab.
This new deﬁnition is indeed a natural extension of the old one. After all, for a
small velocity u ≪c, little has changed: since βu ≪1, we also have γ(βu) ∼1, so
muγ(βu) ∼mu.
For a large velocity u, on the other hand, the new deﬁnition is an important improve-
ment. Indeed, to make the explosion happen, some energy is required, which must
come from somewhere. Now, our system is isolated: no force or energy could come
from the outside. Therefore, the energy for the explosion must come from mass: the
subparticles must lose some of their original mass. More precisely, each subparticle
has mass
m
2γ (βv) <
m
2γ (0) = m
2
(Sect.4.5.3 below). What is the physical meaning of this inequality? Well, it says
that the mass after the explosion must be less than before the explosion, when the
subparticle was still inside the particle, and had velocity v = 0 with respect to it.
Still, no mass was lost for nothing: it supplied the extra energy required to make the
explosion happen.
Let’s assemble the so-called momentum matrix: mass times Lorentz matrix,
summed over both subparticles. (Later on, we’ll focus on just one element in it,
to obtain the desired momentum.) Thanks to the composition in Sect.4.2.4,

124
4
Special Relativity: Algebraic Point of View
mass · Lorentz matrix
=
m
2γ (βv)γ
	 βu + βv
1 + βuβv

 	
I + βu + βv
1 + βuβv
J

+
m
2γ (βv)γ
	 βu −βv
1 −βuβv

 	
I + βu −βv
1 −βuβv
J

=
m
2γ (βv)γ (βv) (I + βv J) γ (βu) (I + βu J) +
m
2γ (βv)γ (βv) (I −βv J) γ (βu) (I + βu J)
= m
2 (I + βv J + I −βv J) γ (βu) (I + βu J)
= mγ (βu) (I + βu J) .
Thus, after the explosion, the momentum matrix remains the same (with respect to
the lab). This is indeed conservation of momentum in matrix sense.
The above is a matrix equation: it actually contains four scalar equations. Let’s
look at just one of them, say the upper right one:
m
2γ (βv)γ
	 βu + βv
1 + βuβv

 βu + βv
1 + βuβv
+
m
2γ (βv)γ
	 βu −βv
1 −βuβv

 βu −βv
1 −βuβv
= mγ (βu) βu.
To simplify, let’s multiply this by c:
m
2γ (βv)
u + v
1 + βuβv
γ
	 βu + βv
1 + βuβv

+
m
2γ (βv)
u −v
1 −βuβv
γ
	 βu −βv
1 −βuβv

= muγ (βu) .
This is indeed conservation of momentum: after the explosion, relative to the lab,
mass times the velocity times the relevant γ still sums to the same: muγ(βu).
4.5.2
Relative Energy
So far, we’ve deﬁned the relative momentum, and made sure that it is indeed con-
served. What about relative energy? To help deﬁne it too, we’ll need to differentiate
γ(βv):
γ′ (βv) ≡

1 −β2
v
−1/2′
= −1
2

1 −β2
v
−3/2 (−2βv)
= βv

1 −β2
v
−3/2 .
Let’s use this to differentiate γ(βv) as a composite function of v:
d
dv γ (βv) = γ′ (βv) dβv
dv
= 1
c γ′ (βv)
= 1
c βv

1 −β2
v
−3/2 .

4.5 Momentum and Energy
125
Let’s use this to differentiate the product vγ(βv):
d
dv (vγ (βv)) = γ (βv) + v d
dv γ (βv)
= γ (βv) + v
c βv

1 −β2
v
−3/2
= γ (βv) + β2
v

1 −β2
v
−3/2
= γ−2 (βv) γ3 (βv) + β2
v

1 −β2
v
−3/2
=

1 −β2
v
 
1 −β2
v
−3/2 + β2
v

1 −β2
v
−3/2
=

1 −β2
v
−3/2 .
We are now ready to deﬁne relative energy. This deﬁnition will be accurate not only
for small but also for large velocities.
Consider a particle of mass m that is initially at rest in the x-t coordinates in
the lab. Then, an external force F is applied to it from time 0 until time q > 0,
to increase both its momentum and energy. Thanks to the increase in its energy,
the particle doesn’t have to lose any mass: it may remain with the same mass m
throughout the entire time interval [0, q].
To have the force, we need to differentiate the relative momentum in Sect.4.5.1
with respect to time. This will help deﬁne relative energy:
 x(q)
x(0)
F(x)dx =
 q
0
F(x(t))dx(t)
=
 q
0
F(x(t))dx(t)
dt
dt
=
 q
0
F(x(t))v(t)dt
= m
 q
0
d
dt (vγ (βv)) v(t)dt
= m
 q
0
d
dv (vγ (βv)) dv
dt v(t)dt
= m
 v(q)
v(0)
d
dv (vγ (βv)) vdv
= m
 v(q)
v(0)

1 −β2
v
−3/2 vdv
= mc2
 v(q)
v(0)

1 −β2
v
−3/2 1
c βvdv
= mc2 
γ

βv(q)

−γ

βv(0)

= mc2 
γ

βv(q)

−1

.

126
4
Special Relativity: Algebraic Point of View
This is indeed the new kinetic energy that the external force has introduced into the
particle from time 0 until time q.
The potential (nuclear) energy stored in the particle at rest, on the other hand, is
not relative, but absolute. This is the amount subtracted in the above formula:
Epotential ≡E(0) ≡mc2.
With this original energy, the total energy is available as a smooth function of v:
E(v) ≡Epotential + Ekinetic(v) = mc2 + mc2 (γ (βv) −1) = mc2γ (βv) .
This new deﬁnition is closely related to the momentum matrix
mγ (βv) (I + βv J) .
Indeed, just look at the lower right corner, and multiply by c2.
The new deﬁnition improves on the old one: it is accurate not only for small but
also for large velocities. In fact, for a small velocity v ≪c, it is nearly the same as
the well-known (inaccurate) formula. Indeed, from the Taylor expansion around 0,
we have that, for β2
v ≪1,
γ (βv) =

1 −β2
v
−1/2 ∼1 + 1
2β2
v.
Therefore, for v ≪c, the new deﬁnition nearly agrees with the classical one:
E(v) = mc2γ (βv) ∼mc2
	
1 + 1
2β2
v

= mc2 + m
2 v2.
If, however, no external force has been applied to it, could the particle still have a
nonzero velocity v ̸= 0? Well, it could, but, in this case, its kinetic energy must have
come from somewhere: from its original potential energy. For this, there is a price
to pay: the particle must have lost some mass to start moving. Only if it gave up
motion altogether and remained at a complete rest (v = 0) could it keep its original
(maximal) mass.
4.5.3
Energy Is Conserved—Mass Is Not
In Sect.4.5.2, we’ve assumed that an external force is applied to the particle, to
increase both its momentum and energy. This is why its mass remains m at all times.
In Sect.4.5.1, on the other hand, the explosion takes place in a closed (isolated)
system: no external force is applied. For this reason, not only the total momentum
but also the total energy remains unchanged.

4.5 Momentum and Energy
127
During the explosion, where do the subparticles get their extra kinetic energy
from? Well, it must come from the original potential (nuclear) energy, stored in the
original particle. As a price, during the explosion, mass must decrease.
As a matter of fact, this is true not only for an exploding particle but also for any
particle that starts moving at velocity v, while preserving its total energy. Its new
kinetic energy must come from somewhere: from its potential nuclear energy. As a
price, the original mass m that the particle had at rest must decrease from m to
m(v) ≡
m
γ (βv) < m.
This way, its total energy remains the same as at rest:
E(v) = m(v)c2γ (βv) =
m
γ (βv)c2γ (βv) = mc2 = E(0).
This is why the absolute quantity m is also called the rest mass: the maximal possible
mass. At motion, on the other hand, the new mass m(v) gets smaller:
m(v) =
m
γ (βv) < m =
m
γ(0).
The above happens in an isolated system only: no external force is welcome, so the
total energy remains constant, while the mass m(v) decreases as |v| increases. This
means that energy is never lost, but can only convert from potential to kinetic energy.
In this process, the total energy remains the same:
E = Epotential(v) + Ekinetic(v) =
m
γ (βv)c2 +
m
γ (βv)c2 (γ (βv) −1) = mc2.
This is quite different from the situation considered in Sect.4.5.2, in which the
system is not isolated, and welcomes an external force from the outside. In that case,
the kinetic energy increases with |v|, while the mass remains constant. This was
necessary to help deﬁne the kinetic energy obtained from the work that the external
force does.
Mass, on the other hand, may change even in a closed system. In fact, as |v|
increases, the mass m(v) decreases. In the extreme case of |v| = c, for instance, we
have γ = ∞, so the particle has no mass at all: all its potential energy has already
been exploited, and converted into kinetic energy.
4.5.4
Lorentz Transformation on Momentum–Energy
In their new deﬁnitions in Sects.4.5.1–4.5.2, both energy and momentum are relative:
they depend on the velocity v, which may change from system to system. Consider,

128
4
Special Relativity: Algebraic Point of View
for instance, a particle of mass m, moving rightwards at velocity v with respect to the
lab. To describe the lab, use again the x′-t′ coordinates (as in Sect.4.4.2). This prime
means no differentiation—it just reminds us that both x′ and t′ are measured in the
lab. Thus, in the lab, the momentum of the particle is denoted with a prime as well:
p′ ≡mvγ (βv) .
Likewise, in the lab, the total energy of the particle is denoted with a prime as well:
E′ ≡mc2γ (βv) .
Why are we using primes here? Because we are not really interested in the lab. We are
more interested in yet another particle, moving leftwards at velocity u with respect
to the lab. To describe its own self-system, use the standard x-t coordinates, with no
prime (as in Sect.4.4.2). After all, our convention is to use no prime in the system
we’re interested in. So, in the self-system of the second particle, what are the energy
E and the momentum p of the ﬁrst particle?
Of course, we could take a naive approach: use the rule of adding velocities to
calculate the velocity of the ﬁrst particle away from the second one. Then, use it to
calculate the relative momentum and energy as well. Still, this would require a lot of
calculations. Is there a more direct way?
Fortunately, there is. For this purpose, observe that p′ and E′/c have a familiar
ratio:
p′
E′/c = mvγ (βv)
mcγ (βv) = v
c = βv.
So, let’s put them in one column vector, proportional to the column (βv, 1)t in
Sect.4.4.2. More precisely, this is just the second column in the momentum matrix,
multiplied by c:
	 p′
E′/c

≡cmγ (βv) (I + βv J)
	0
1

.
From the perspective of the second particle, on the other hand, the entire lab moves
rightwards at velocity u. On top of that, the ﬁrst particle also travels in the lab at
velocity v. Thus, the momentum matrix is now
mγ (βu) (I + βu J) γ (βv) (I + βv J) .
To have the energy and momentum of the ﬁrst particle from the perspective of the
second one, just look at the second column of this new momentum matrix, and
multiply by c:
	 p
E/c

= cmγ (βu) (I + βu J) γ (βv) (I + βv J)
	0
1

= γ (βu) (I + βu J)
	 p′
E′/c

.

4.5 Momentum and Energy
129
So, to drop the primes and have both p and E for free, just apply the inverse Lorentz
transformation in Sect.4.2.5. This way, you work with energy and momentum only,
avoiding the explicit transformation of the entire lab back to the self-system of the
second particle. As a result, both the energy and the momentum of the ﬁrst particle
are now available not only with respect to the lab but also with respect to the second
particle, with no need to add the velocities u and v explicitly any more.
4.6
Energy and Mass
4.6.1
Absolute Nuclear Energy
In the above, we put the energy and the momentum in the second column of the
momentum matrix. Let’s go ahead and do this in the ﬁrst column as well:
	 E′/c
p′
p′
E′/c

= cmγ (βv) (I + βv J) .
Fortunately, a Lorentz matrix must have determinant 1:
E′2
c2 −p′2 = det
		 E′/c
p′
p′
E′/c


= det (cmγ (βv) (I + βv J))
= c2m2 det (γ (βv) (I + βv J))
= m2c2.
As in Sect.4.5.4, to drop the primes, we have to apply yet another Lorentz matrix.
This has no effect on the determinant:
E2
c2 −p2 = det
		 E/c
p
p
E/c


= det
	
γ (βu) (I + βu J)
	 E′/c
p′
p′
E′/c


= det
		 E′/c
p′
p′
E′/c


= m2c2.
Thus, the determinant is invariant and absolute: it doesn’t depend on u, and doesn’t
change from system to system. To simplify, multiply by c2:
E2 −c2 p2 = m2c4.

130
4
Special Relativity: Algebraic Point of View
This is indeed the squared nuclear energy stored in the particle at rest (Sects.4.5.2–
4.5.3). This energy is not relative, but absolute: it is completely independent of the
velocity, or the system used to measure it.
In general, the momentum p might be a three-dimensional vector rather than just
a scalar. In this case, p2 should be replaced by the inner product ∥p∥2 = (p, p).
4.6.2
Invariant Rest Mass
What is the physical meaning of this formula? It tells us that the rest mass m is
invariant: it remains the same in all systems, and is never changed under any Lorentz
transformation. For this reason, m could be calculated not only in the original lab
but also in any other system.
The above discussion is thus most practical. Thanks to it, m could be calculated
not only from p′ and E′ (the momentum and energy in the lab) but also from p
and E (the momentum and energy in the self-system of the second particle). This
observation will be most useful below.
In particular, why not calculate m in the self-system of the ﬁrst particle itself?
After all, in this system, there is no velocity or momentum or kinetic energy at all,
so the above formula simpliﬁes to read
E2
potential = m2c4,
or
Epotential = mc2,
Einstein’s famous formula.
4.7
Center of Mass
4.7.1
Collection of Subparticles
In the lab, if the velocity v of the ﬁrst particle is unknown, then it could still be
obtained in terms of the momentum p′ and the energy E′:
v = mvγ (βv)
mc2γ (βv)c2 = p′
E′ c2.
When is this useful? When the momentum and energy are available, but the velocity
is not. This is quite practical: p′ and E′ are more fundamental than v, which is often
missing.

4.7 Center of Mass
131
Throughout this chapter, the second particle could be just theoretical, and have
no size or mass at all. After all, it only serves as a reference point for the ﬁrst particle
and its motion.
The ﬁrst particle, on the other hand, is more real and physical. To emphasize this,
let’s replace it by a collection of k ≥1 subparticles, each with velocity vi, momentum
p′
i, and energy E′
i with respect to the lab (1 ≤i ≤k).
What are the total momentum and energy? Well, as fundamental (and conserved)
quantities, they sum up:
p′ ≡
k

i=1
p′
i
and
E′ ≡
k

i=1
E′
i.
The velocity of the entire collection, on the other hand, is not necessarily the sum of
the vi’s. After all, the subparticles may have different masses, which are not always
available. As a matter of fact, some of them may even have no mass at all (those that
are as fast as light, and have |vi| = c). To deﬁne the total velocity properly, better
use the fundamental relative quantities: momentum and energy.
4.7.2
Center of Mass
We are now ready to deﬁne the velocity of the entire collection:
v ≡p′
E′ c2 = c2
k
i=1 p′
i
k
i=1 E′
i
.
This new velocity describes the motion of no concrete physical object, but only a
theoretical object: the center of mass of the entire collection. Where is this? To tell
this, let’s use the second particle.
The above velocity is in terms of the lab. Next, let’s look at things from the second
particle, which travels in the lab at velocity −u. In its self-system, the momentum
and energy of the collection are
	 p
E/c

= γ (βu) (I + βu J)
	 p′
E′/c

(Sect.4.5.4).
In this equation, look at the top. Assume also that the second particle follows the
center of mass at the same speed:
u ≡−v,
so βu = −βv = −p′
E′/c.

132
4
Special Relativity: Algebraic Point of View
In this case, in the previous equation, the top simpliﬁes to read
p = 0.
Thus, with respect to the second particle, the entire collection has no momentum at
all: it is at a complete rest. This is why the second particle marks the center of mass
itself.
4.7.3
Rest Mass of the Collection
What is the rest mass m of the entire collection? Again, we can’t just sum the
individual masses of the subparticles. Instead, we better work with more fundamental
quantities: momentum and energy.
In the self-system of the second particle, the collection has no momentum at all.
Therefore, the formula in Sect.4.6.1 tells us that
E2 = m2c4,
or
m ≡E
c2 .
This is indeed a proper deﬁnition of the total mass of the entire collection.
What is the physical meaning of this? Well, in its own self-system (or the self-
system of the center of mass), the collection has no momentum at all: p = 0. Thus,
the above actually deﬁnes its rest mass m in terms of its total energy E. Fortunately,
mass is invariant, so m is the same in the original lab as well (Sect.4.6.2). Still, in
the lab, the collection moves, so its true mass is no longer m but only m/γ(βv) < m
(Sect.4.5.3).
4.8
Force
4.8.1
Passive System—Strong Perpendicular Force
As in Sect.4.4.3, assume now that the lab is described by the x′-y′-t′ coordinates. In
the lab, the entire collection moves obliquely, at the new velocity v ≡(vx′, vy′)t: vx′
in the positive x′-direction, and vy′ in the perpendicular y′-direction. In this case, the
momentum is oblique as well: p′ ≡(p′
x′, p′
y′)t, proportional to v. This is the view
from the lab (Fig.4.9). (From the second particle, on the other hand, things look
different—Fig.4.10.)

4.8 Force
133
Fig. 4.9 View from the lab:
initially, at time t′ = 0, the
collection is still at rest at
(x′, y′) = (0, 0). At t′ = 0,
an oblique external force
F′ = (F′
x′, F′
y′) starts to act
upon it, to increase its
momentum and kinetic
energy, while not changing
its mass
Fig. 4.10 View from the
second particle: the force
that acts on the collection
remains the same in the
x-direction, but seems
weaker in the perpendicular
y-direction
Note that there is no differentiation here: the prime means no derivative, but only
reminds us that we are in the lab system. Also, the subscripts x′ and y′ mean no partial
derivative, but only spatial coordinates.
Thus, in the formula in Sect.4.6.1, p′2 should be replaced by the inner product
∥p′∥2 = (p′, p′). After all, in theory, we could always redeﬁne x to align with v and
p′ (see exercises below). Fortunately, there is no need to do this explicitly.
The second particle, on the other hand, still moves in the x′-direction only: at
velocity (−u, 0) ̸= (0, 0) with respect to the lab. To transform from the lab to the
self-system of the second particle, we must now use an extended 3 × 3 Lorentz
matrix:
⎛
⎝
px
py
E/c
⎞
⎠=
⎛
⎝
γ (βu)
1
γ (βu)
⎞
⎠
⎛
⎝
1
βu
1
βu
1
⎞
⎠
⎛
⎝
p′
x′
p′
y′
E′/c
⎞
⎠.
Here,wenolongerassumethatu = −v.Thus,thesecondparticlenolongercoincides
with the center of mass. Instead, this job is left to the lab itself.
Indeed, assume now that the lab system is initially the same as the self-system of
the collection: at time t′ = 0, the collection is at rest in the lab:

134
4
Special Relativity: Algebraic Point of View
p′ ≡
	 p′
x′
p′
y′

=
	 0
0

,
E′ = mc2 > 0,
and v ≡
	vx′
vy′

≡p′
E′ c2 =
	0
0

.
This is true at time t′ = 0 only. At t′ > 0, on the other hand, things may change, due
to an external force.
Unlike before, assume now that the lab is no longer closed or isolated. On the
contrary: from time t = t′ = 0 onward, an external force is applied to the entire
collection, to increase its momentum and (kinetic) energy in the lab, while preserving
mass. This is why the lab is called here the passive system: after all, the original force
is applied directly to the collection that was initially at rest in it.
In the passive system, this force could be measured, and is often available: the
derivative of the momentum in the original x′-y′-t′ coordinates:
F′ ≡
	 F′
x′
F′
y′

≡
⎛
⎜⎝
dp′
x′
dt′
dp′
y′
dt′
⎞
⎟⎠.
Let’s focus on the force at the initial time t′ = 0:
F′ ≡
	 F′
x′
F′
y′

≡
⎛
⎜⎝
dp′
x′
dt′ (0)
dp′
y′
dt′ (0)
⎞
⎟⎠.
(After all, every time t′ > 0 could in theory be shifted back to zero.) How does this
force look like from the second particle? In other words, how to transform the force
to the x-y-t coordinates in the self-system of the second particle?
Of course, we could take a naive approach: transform the momentum and energy to
the self-system of the second particle, and deﬁne the force F there by differentiating
the momentum with respect to t. Still, this could require a lot of calculations. Is there
a more direct way?
Fortunately, there is. To differentiate the momentum with respect to time, let’s
use the trick in Sect.4.4.3.
To start, let’s differentiate t with respect to t′. This seems easy: after all, in the
lab, t′ is the proper time, isn’t it? So, as in Sect.4.3.1, it should satisfy
t′ =
t
γ (βu),
shouldn’t it? Furthermore, as in Sect.4.3.6, it should satisfy time dilation:
△t′ =
△t
γ (βu),
shouldn’t it?

4.8 Force
135
Unfortunately not. After all, t′ might be proper only in an isolated lab, which
welcomes no external force. In our lab, on the other hand, t′ is only nearly proper:
only at t′ = 0, before the force had time to act, does t′ behave like a proper time.
To see this, let’s use the inverse Lorentz transformation:
ct = γ (βu)

βux′ + ct′
,
or
t = γ (βu)
	βu
c x′ + t′

.
Let’s differentiate this with respect to t′:
dt
dt′ = γ (βu)
	βu
c · dx′
dt′ + 1

= γ (βu)
	βu
c vx′ + 1

.
At t′ = 0, in particular, vx′ = 0, so this simpliﬁes to read
dt
dt′ = γ (βu) .
Thus, at t′ = 0, t′ is indeed nearly proper: it behaves just like a proper time. Let’s
use this to look at the force from the second particle as well (Fig.4.10).
Let’s start with the perpendicular component: Fy. From the above 3 × 3 matrix,
in the y-direction, the momentum is still the same:
py = p′
y′.
Thus, the differentiation is simple:
Fy ≡dpy
dt
=
dp′
y′
dt
=
dp′
y′
γ (βu) dt′
=
1
γ (βu) ·
dp′
y′
dt′
=
1
γ (βu) F′
y′.
Thus, in the perpendicular y-direction, the passive system feels the maximal force.
From any other system, on the other hand, the force feels weaker. In particular, the
self-system of the second particle feels a force that is γ(βu) times as weak.

136
4
Special Relativity: Algebraic Point of View
What about the force in the x-direction? Does it also feel weaker? Well, let’s use
the same trick:
Fx = dpx
dt
=
d

γ (βu)

p′
x′ + βu E′
c

γ (βu) dt′
=
d

p′
x′ + βu E′
c

dt′
= dp′
x′
dt′ + βu
dE′
cdt′
= F′
x′ + βu
c · dE′
dt′ .
To simplify this, let’s look at the latter term, and show that it contributes nothing.
For this purpose, let’s look at the original equation
E′2 = c2 
p′, p′
+ m2c4.
This equation comes from the original deﬁnition of E′ and p′ in the lab. Therefore,
it holds for every time t′ ≥0, although possibly with a different E′, p′, and v. Later
on, we’ll focus on the initial time t′ = 0 once again.
In this equation, the latter term remains constant. After all, thanks to the external
force, the potential energy (and the mass) remain unchanged. So, once differentiating
both sides with respect to t′, the latter term drops:
2E′ dE′
dt′ = 2c2(p′)t dp′
dt′ = 2c2(p′)t F′,
where (p′)t is the row vector that contains the momentum in the lab.
Recall again that we’re particularly interested in the initial time of t′ = 0. At
t′ = 0, the momentum is still zero:
2E′ dE′
dt′ = 2c2(0, 0)F′ = 0.
Since E′ > 0, we must therefore have
dE′
dt′ = 0.
In summary, at t = t′ = 0,

4.8 Force
137
Fx = F′
x′ + βu
c · dE′
dt′ = F′
x′.
Thus, unlike Fy, Fx remains the same at all systems.
4.8.2
Photon: A New Universe?
A light ray may be viewed in two different ways. On one hand, it is a wave. On the
other hand, it is also a particle: a photon, traveling at speed c with respect to us.
From our perspective, a particle as fast as light may have no mass at all (end
of Sect.4.5.3). Furthermore, due to length contraction, it may have no size either
(Sect.4.3.4). Still, this is only from our own (subjective) point of view. The photon
may disagree: in its own self-system, it is at rest, so it may well have both mass
and size. In fact, in its own self-system, the photon may even contain a whole new
universe, with many other mankinds in it! Only because the photon is so fast don’t
we get to see this interesting universe!
On the contrary: from the photon’s perspective, the entire universe travels at the
speed of light in the opposite direction. Thanks to length contraction, in the photon’s
eyes, the entire universe is as small as a single point, with no size or mass at all. So,
we don’t even exist!
Yet worse, the photon is not the only one who says so. In fact, all photon in the
entire universe agree on just one thing: the universe doesn’t exist at all! So many
witnesses can’t be wrong, can they?
4.9
Exercises
4.9.1
Motion in Three Dimensions
1. Show that the determinant of a 2 × 2 matrix is the same as the area of the
parallelogram made by its column vectors. Hint: see Chap.2, Sect.2.3.3.
2. Show that the determinant of a 2 × 2 matrix is the same as the area of the
parallelogram made by its row vectors. Hint: see Chap.2, Sect.2.1.3.
3. Show that the Lorentz matrix has determinant 1.
4. Consider a 2 × 2 matrix. Multiply it by a Lorentz matrix. Show that the original
determinant hasn’t changed. Hint: see Chap.2, Sect.2.1.3.
5. Conclude that the Lorentz transformation preserves area in the two-dimensional
Cartesian plane.
6. Conclude also that the inverse Lorentz matrix has determinant 1 as well.
7. Conclude that the inverse transformation preserves area as well.

138
4
Special Relativity: Algebraic Point of View
8. Use Cramer’s formula (Chap.2, Sect.2.1.4) to calculate the inverse Lorentz
matrix directly.
9. Does this agree with the trick in Sect.4.2.5: to have the inverse, just change the
sign of the velocity—replace v by −v?
10. What is the physical meaning of this? Hint: from the moving system, the lab
seems to move in the opposite direction.
11. Let
v ≡
⎛
⎝
v1
v2
v3
⎞
⎠∈R3
be some nonzero three-dimensional real vector. Deﬁne the 3 × 3 matrix Ov,
whose columns are v (normalized), a vector that is orthogonal to v (normalized
as well), and their vector product:
Ov ≡
	 v
∥v∥

v⊥
∥v⊥∥

v × v⊥
∥v∥· ∥v⊥∥

.
Show that Ov is an orthogonal matrix. Hint: see Chap.2, Sects.2.2.4–2.3.2.
12. Conclude that Ov has determinant 1.
13. Consider a particle that moves at velocity v ∈R3 with respect to the lab. In other
words, the particle moves at direction v/∥v∥at speed ∥v∥. Let (x′, y′, z′, ct′)
denote the lab coordinates, and (x′′, y′′, z′′, ct′′) the self coordinates of the par-
ticle. We are now ready to deﬁne the more general Lorentz transformation
⎛
⎜⎜⎝
x′
y′
z′
ct′
⎞
⎟⎟⎠→
⎛
⎜⎜⎝
x′′
y′′
z′′
ct′′
⎞
⎟⎟⎠= Lv
⎛
⎜⎜⎝
x′
y′
z′
ct′
⎞
⎟⎟⎠,
where Lv is the following 4 × 4 Lorentz matrix:
Lv ≡
	 Ov
1

⎛
⎜⎜⎝
γ

β∥v∥

1
1
γ

β∥v∥

⎞
⎟⎟⎠
⎛
⎜⎜⎝
1
−β∥v∥
1
1
−β∥v∥
1
⎞
⎟⎟⎠
	 Ot
v
1

.
(As usual, blank spaces stand for zero matrix elements.) Show that this indeed
transforms the lab system to the self-system of the particle.
14. Show that Lv has determinant 1.
15. Consider also a second particle, moving at velocity −u ∈R3 with respect to
the lab. Denote its self coordinates by (x, y, z, ct). With respect to this system,
the entire lab moves at velocity u ∈R3. Show that the transformation from this
system to the lab system is

4.9 Exercises
139
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠→
⎛
⎜⎜⎝
x′
y′
z′
ct′
⎞
⎟⎟⎠= Lu
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠.
16. Consider the composed Lorentz transformation
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠→
⎛
⎜⎜⎝
x′′
y′′
z′′
ct′′
⎞
⎟⎟⎠
from the self-system of the second particle to the self-system of the ﬁrst particle.
Show that it is represented by the matrix product LvLu:
⎛
⎜⎜⎝
x′′
y′′
z′′
ct′′
⎞
⎟⎟⎠= Lv
⎛
⎜⎜⎝
x′
y′
z′
ct′
⎞
⎟⎟⎠= LvLu
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠.
17. Show that LvLu has determinant 1 as well. Hint: see Chap.2, Sect. 2.1.3.
18. Does Lu commute with Lv? Hint: only if u is a scalar multiple of v.
19. Consider the inverse Lorentz transformation
⎛
⎜⎜⎝
x′′
y′′
z′′
ct′′
⎞
⎟⎟⎠→
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠
from the self-system of the ﬁrst particle back to the self-system of the second
particle. Show that it is represented by the inverse matrix
(LvLu)−1 = L−1
u L−1
v
= L−uL−v.
20. Conclude that the last column in L−uL−v describes the motion of the ﬁrst particle
away from the second one. Hint: see the exercises below.
21. Show that, in its self-system, the ﬁrst particle is at rest:
x′′ = y′′ = z′′ = 0.
22. Conclude that t′′ is a proper time. Hint: see Sect.4.3.1.
23. Conclude also that
⎛
⎜⎜⎝
0
0
0
ct′′
⎞
⎟⎟⎠= LvLu
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠.

140
4
Special Relativity: Algebraic Point of View
24. Show that it is sufﬁcient to solve this up to a scalar multiple. Hint: we’re only
interested in the slopes (or ratios) x/t, y/t, and z/t.
25. Conclude that it is sufﬁcient to solve
LvLuw =
⎛
⎜⎜⎝
0
0
0
1
⎞
⎟⎟⎠,
where
w ≡
⎛
⎜⎜⎝
x
y
z
ct
⎞
⎟⎟⎠
(up to a scalar multiple).
26. Simplify this even more to read
Luw = L−v
⎛
⎜⎜⎝
0
0
0
1
⎞
⎟⎟⎠=
	v
c

(up to a scalar multiple).
27. Simplify this even more, to read
w ≡L−u
	 v
c

(up to a scalar multiple).
28. Consider a special case, in which u aligns with the x-axis:
u ≡
⎛
⎝
∥u∥
0
0
⎞
⎠.
Show that, in this case, one could design
Ou ≡I
(the 3 × 3 identity matrix).
29. Show that, in this case,

4.9 Exercises
141
Lu ≡
⎛
⎜⎜⎝
γ

β∥u∥

1
1
γ

β∥u∥

⎞
⎟⎟⎠
⎛
⎜⎜⎝
1
−β∥u∥
1
1
−β∥u∥
1
⎞
⎟⎟⎠.
30. Show that, in this case,
L−u ≡
⎛
⎜⎜⎝
γ

β∥u∥

1
1
γ

β∥u∥

⎞
⎟⎟⎠
⎛
⎜⎜⎝
1
β∥u∥
1
1
β∥u∥
1
⎞
⎟⎟⎠.
31. Show that this is not just a special case, but a most general case. Hint: for a
general u, pick the x-axis to align with u in the ﬁrst place.
32. Use w (as deﬁned above) to uncover the slopes (or the velocity) along which the
ﬁrst particle travels away from the second particle:
⎛
⎝
dx/dt
dy/dt
dz/dt
⎞
⎠= c
w4
⎛
⎝
w1
w2
w3
⎞
⎠.
33. Interpret Lu as a projective mapping in the real projective space (Chap.6,
Sects.6.7.3 and 6.9.1).
34. Interpret the above method as the three-dimensional extension of the methods
in Sects.4.4.2–4.4.3.
35. Likewise, in Sect.4.4.3, interpret the inverse Lorentz transformation back to
the x-y-t self-system of the second particle as a projective mapping in the real
projective plane (Chap.6, Sects.6.4.1 and 6.7.3).
36. What does this mapping do? Hint: it maps the original velocity (dx′/dt′, dy′/dt′)
of the ﬁrst particle in the lab (Fig.4.6) to the new velocity (dx/dt, dy/dt) of the
ﬁrst particle away from the second one (Fig.4.7).
37. In Figs.4.6 and 4.7, where is the time variable? Why is it missing? Hint: these
ﬁgures are static, not dynamic: they tell us the position, not the time. Time is just
a parameter, telling us how a particle moves in the direction pointed at by the
arrow. Still, we can’t see this: only in a movie could we see this dynamics—not
in a static picture. How did we get rid of time? We just divided by t (or t′). This
way, the time variable was eliminated. Now, it is only used implicitly to push
the particle along the arrow—the velocity vector.

Part II
Introduction to Group Theory
What have we done so far? Well, the vectors introduced above make a linear space.
Indeed, the algebraic operations between them are linear. The (nonsingular) matrices,
on the other hand, makes a new mathematical structure: a group.
In a group, although the commutative law not necessarily holds, the associative
law does hold. In what follows, we introduce group theory, including the ﬁrst, second,
and third isomorphism theorems, and their geometrical applications.
Matrices are particularly useful to represent all sorts of practical transformations
in geometry and physics. In special relativity, for example, Lorentz transformations
are written as 2 × 2 matrices. Here, we’ll put this in a much wider context: group
representation. To show how useful this is, we’ll represent projective mappings as
3 × 3 matrices. This is particularly useful in computer graphics. Finally, we’ll also
use matrices to introduce yet another important ﬁeld: quantum mechanics.

Chapter 5
Group Representation and Isomorphism
Theorems
What is the most elementary algebraic object? This could be the individual number. In
the previous part, we also introduced more complicated algebraic structures: vectors
and matrices.
Furthermore, elementary algebraic objects like numbers, once used as input and
output, form a yet more advanced mathematical object: a function. The polynomial,
for example, is just a special kind of function, enjoying many algebraic operations:
addition, multiplication, and composition.
Functions are indeed studied in a few major mathematical ﬁelds. In set theory, a
set of functions is often studied just like any other set, and its cardinality is estimated.
In algebra, on the other hand, functions are also viewed as algebraic objects that can
be composed with each other. Finally, in calculus, functions are also considered as
analytic objects that can be differentiated and integrated.
In this chapter, we consider a special kind of function: a mapping or transforma-
tion. Together, the transformations form a new mathematical structure: a group, with
a lot of interesting properties.
To help study a mapping, we mirror it by a matrix. This way, algebraic operations
are mirrored as well: composition of two mappings is mirrored by multiplication of
two matrices. This is indeed group representation.
This point of view is most useful in the practical implementation. After all, a
mapping could hardly be stored on the computer. A matrix, on the other hand, can.
Furthermore, the representation could help understand the deep nature of the original
mapping as an algebraic and geometrical object.
We have already seen an example of a useful transformation: in special relativity
(Chap.4), the Lorentz transformation has been represented as a 2 × 2 matrix. Here,
on the other hand, we put this in a much wider context: group theory. In particular,
we prove the ﬁrst, second, and third isomorphism theorems, used later in projective
geometry.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_5
145

146
5
Group Representation and Isomorphism Theorems
5.1
Moebius Transformation and Matrix
5.1.1
Riemann Sphere—Extended Complex Plane
To make the discussion more concrete, we need a new geometrical concept: the
inﬁnity “point.” As a matter of fact, this is not really a point. Still, it can be added to
the complex plane, to form a complete “sphere.”
The extended complex plane (or the Riemann sphere)
C ∪{∞}
is obtained from the original complex plane C by adding one more object: the inﬁnity
point ∞. This new “point” is not really a point, but a new artiﬁcial object, to help
model a complex number with an arbitrarily large absolute value.
The inﬁnity point is unique. In fact, in the complex plane, one could draw a ray
issuing from the origin in just any angle. The complex number z could then “slide”
along this ray, and approach the same point: inﬁnity. Later on, we’ll also meet more
complicated spaces, with many inﬁnity points.
5.1.2
Moebius Transformation and the Inﬁnity Point
Thanks to the inﬁnity point, we can now deﬁne the Moebius transformation.
We’ve already met this transformation in the context of special relativity (Chap.4,
Sect.4.4.2). Here, however, we introduce it in much more detail and depth, and in a
much wider context [4, 59].
A Moebius transformation (or mapping) from the extended complex plane onto
itself is deﬁned by
z →az + b
cz + d ,
where a, b, c, and d are some ﬁxed complex parameters. Here, ‘→’ stands for
transformation, not for a limit.
Unfortunately, the deﬁnition is still incomplete. After all, the mapping is not yet
deﬁned at the inﬁnity point z = ∞. Let us complete this gap in such a way that the
mapping remains continuous:
∞→a
c .
This way, z could approach inﬁnity on just any ray in the complex plane. After all,
in every direction, the transformed values converge to a/c, as required.
Still, the deﬁnition is not yet complete. What happens at the pole z = −d/c, at
which the denominator vanishes? Well, to preserve continuity, deﬁne

5.1 Moebius Transformation and Matrix
147
−d
c →∞.
This way, z could approach the pole from just any direction. In either case, the
transformed values would approach a unique point: the inﬁnity point.
There is one case in which these formulas coincide, and agree with each other.
This happens if c = 0. In this case, both formulas read
∞→∞.
Still, this makes no problem: c could safely vanish. There is something else that must
never vanish.
Indeed, to make sure that the mapping is invertible, the parameters a, b, c, and d
must satisfy the condition
ad −bc ̸= 0.
Otherwise, we’d have ad = bc, which means that
• either c ̸= 0, so the entire complex plane is mapped to
az + b
cz + d = 1
c · acz + bc
cz + d
= 1
c · acz + ad
cz + d
= a
c ,
• or c = a = 0, so the entire complex plane is mapped to b/d,
• or c = d = 0, so the entire complex plane is mapped to ∞.
In either case, the transformation would be constant, and not invertible. This is why
the above condition is necessary to make sure that the original Moebius transforma-
tion is indeed nontrivial and invertible.
5.1.3
The Inverse Transformation
Fortunately, the condition ad −bc ̸= 0 is not only necessary but also sufﬁcient to
guarantee that the original Moebius transformation is invertible. Indeed, assume that
the original complex number z is mapped to the new complex number
u ≡az + b
cz + d .
To have the inverse mapping in its explicit form, let us write z in terms of u. For this
purpose, let us multiply the above equation by the denominator:
u(cz + d) = az + b.

148
5
Group Representation and Isomorphism Theorems
Now, let us throw those terms that contain z to the left-hand side, and the other terms
to the right-hand side:
z(cu −a) = −du + b.
This implies that
z = −du + b
cu −a
= du −b
−cu + a .
Thus, the required inverse transformation is
z →dz −b
−cz + a .
To make sure that this transformation is indeed continuous, we must also deﬁne it
properly at ∞and at its pole, a/c:
∞→−d
c
a
c →∞.
These are just the reverse of the original deﬁnitions in Sect.5.1.2. This way, the
inverse transformation indeed maps the inﬁnity point back to the pole of the original
transformation, as required.
5.1.4
Moebius Transformation as a Matrix
The parameters in the original Moebius transformation are deﬁned up to a scalar
multiple. After all, for any nonzero complex number q ̸= 0, the same transformation
could also be deﬁned by
z →qaz + qb
qcz + qd .
Thus, the original Moebius transformation is associated with the 2 × 2 matrix
a b
c d

,
or just any nonzero scalar multiple of it.
Similarly, the inverse mapping is associated with the matrix
 d −b
−c a

,

5.1 Moebius Transformation and Matrix
149
or just any nonzero scalar multiple of it. As can be seen in Chap.2, Sect.2.1.4, this
matrix is just a scalar multiple of the inverse of the original matrix.
Later on, we’ll see that this is not just an association, but much more: the matrix
actually represents and mirrors the original transformation. In terms of a matrix, the
original condition takes the form
det
a b
c d

= ad −bc ̸= 0.
Why does this condition make sense? Because it guarantees that the original matrix
is nonsingular (invertible), as required.
Thus, the set of invertible Moebius transformations is mirrored by the set of
(complex) nonsingular 2 ×2 matrices, deﬁned up to a nonzero scalar multiple. Later
on, we’ll refer to this as isomorphism or group representation [21, 25, 34, 35].
Indeed, it preserves the same algebraic structure: the composition of two Moebius
transformations is mirrored by the product of the 2 × 2 matrices associated with
them.
5.1.5
Product of Moebius Transformations
The product of the Moebius transformations m′ and m is deﬁned as their composition:
m′m ≡m′ ◦m.
This means that for every z ∈C ∪{∞},
(m′m)(z) ≡m′(m(z)).
Note that this algebraic operation is associative. Indeed, for every three Moebius
transformations m, m′, and m′′,
((m′′m′)m)(z) = (m′′m′)(m(z))
= m′′(m′(m(z)))
= m′′((m′m)(z))
= (m′′(m′m))(z).
Since this applies to each and every z ∈C ∪{∞}, it can be written more concisely
as
(m′′m′)m = m′′(m′m),
which makes an associative law. This is no surprise: after all, as discussed below,
this kind of composition is mirrored by matrix multiplication.

150
5
Group Representation and Isomorphism Theorems
5.2
Matrix: A Function
5.2.1
Matrix as a Vector Function
As discussed above, each invertible Moebius transformation is associated with a non-
singular 2×2 complex matrix (deﬁned up to a nonzero scalar multiple). Fortunately,
matrix-times-matrix multiplication could be viewed as a composition as well.
For this purpose, consider a 2 × 2 matrix g. Let’s interpret it as a special vector
function, rather than just a matrix:
g : C2 →C2,
with the explicit deﬁnition
g(v) ≡gv,
(v ∈C2).
Here, when g is followed by v, with no parentheses, then this is a matrix-vector
product, as in Chap.1, Sect.1.4.4. This is then used to deﬁne the new vector function,
which uses round parentheses.
Why is this new interpretation equivalent to the original one? Because it charac-
terizes g uniquely! Indeed, given a matrix g, we’ve already seen how it is used to
deﬁne a unique vector function. Conversely, given a vector function of the above
form, we can easily reconstruct the unique matrix that deﬁnes it. For this purpose,
just apply the vector function g() to the standard unit vectors (1, 0)t and (0, 1)t, to
uncover the matrix g column by column:
g =

g(1) | g(2)
, where g(1) = g
 1
0

and g(2) = g
0
1

.
5.2.2
Matrix Multiplication as Composition
Let g and g′ be 2 × 2 matrices. With their new interpretation as vector functions,
their product can now be viewed as a composition:
g′g ≡g′ ◦g.
This means that for every two-dimensional vector v ∈C2,
(g′g)(v) ≡g′(g(v)).
Does this agree with the original deﬁnition—the matrix-times-matrix product in
Chap.1, Sect.1.4.5? Well, let’s look at the matrix g′g. Thanks to associativity, it

5.2 Matrix: A Function
151
deﬁnes the vector function
(g′g)(v) = (g′g)v = g′(gv) = g′(g(v)),
which is the same as the above composition.
Furthermore, we’ve already seen that this matrix is unique: g′g is the only matrix
that could be used to deﬁne the above composition.
So, matrices could mirror these special vector functions: matrix-times-matrix
could mirror composition. How does this help to mirror Moebius transformations?
To get to the bottom of this, we better use the principle of induction: study not only
one special case, but also a much wider ﬁeld: groups.
5.3
Group and Its Properties
5.3.1
Group
The above g is just a special case. In general, g could be not only a 2 × 2 matrix but
also any element in a group.
A group G is a set of elements or objects, with some algebraic operation between
them. This operation is called multiplication or product. Thus, the group is closed
under this kind of multiplication: for every two elements g and g′ in G, their products
gg′ and g′g (which are not necessarily the same) are legitimate elements in G as well.
This kind of multiplication might be rather strange and nonstandard. Fortunately,
it mustn’t be too nonstandard: although it doesn’t have to be commutative, it must
still be associative: for every three elements g, g′, and g′′ in G,
g(g′g′′) = (gg′)g′′.
5.3.2
The Unit Element
Furthermore, it is also assumed that G contains a unit element I that satisﬁes
gI = Ig = g
for every element g ∈G. Here, I has nothing to do with the identity matrix in
Chap.1, Sect.1.5.2. It is just some special element in G.
Fortunately, this unit element is unique in G. Indeed, assume that I ′ ∈G was a
unit element as well:
gI ′ = I ′g = g

152
5
Group Representation and Isomorphism Theorems
for every element g ∈G. In particular, this would be true for g = I:
I ′I = I.
Since I is the original unit element, we also have
I ′I = I ′.
In summary,
I ′ = I ′I = I,
so I ′ is not really new: it is the same as the original unit element I.
5.3.3
Inverse Element
Finally, it is also assumed that every element g ∈G has an inverse element g′ ∈G
(dependent on g), for which
gg′ = I.
Even if the commutative law doesn’t hold in G in general, g′ does commute with g:
g′g = I.
Indeed, thanks to the associative law,
g′g = g′(Ig) = g′((gg′)g) = g′(g(g′g)) = (g′g)(g′g).
Now, let’s multiplying this equation (from the right) by an inverse of g′g, denoted
by (g′g)′. Thanks again to the associative law, we have
I = ((g′g)(g′g))(g′g)′ = (g′g)

(g′g)(g′g)′
= (g′g)I = g′g,
as asserted.
Furthermore, the inverse of g is unique. Indeed, assume that g had yet another
inverse g′′, satisfying
gg′′ = I
as well. Thanks to the associative law, we’d then have
g′′ = Ig′′ = (g′g)g′′ = g′(gg′′) = g′I = g′.

5.3 Group and Its Properties
153
Thus, g′′ isn’t really new: it is the same as g′. The unique inverse of g can now be
denoted by g−1 rather than g′. Once we have this new notation, we can use g′ once
again to stand for a general element in G, independent of g.
Let us show that the inverse of the inverse is the original element itself:

g−1−1 = g.
Indeed, we already know that g−1 is the inverse of g, not only from the right but also
from the left:
g−1g = I.
Fortunately, this equation could also be interpreted the other way around: g behaves
as expected from an inverse to g−1. But g−1 has only one inverse, so it must indeed
be g:

g−1−1 = g,
as asserted. Thus, the inverse operation is symmetric: not only g−1 is the inverse of
g, but also g is the inverse of g−1.
Finally, consider two general elements g′, g ∈G. How to invert their product g′g?
Take the individual inverses, in the reverse order:
(g′g)−1 = g−1g′−1.
Indeed, look at the right-hand side. Does it behave as expected from the inverse of
g′g? Well, thanks to the associative law,
(g′g)

g−1g′−1
=

(g′g)g−1
g′−1 =

g′ 
gg−1
g′−1 = (g′I)g′−1 = g′g′−1 = I.
The assertion follows now from the uniqueness property.
5.4
Mapping and Homomorphism
5.4.1
Mapping and Its Origin
Here we recall some properties of sets and mappings, which are particularly relevant
to the present discussion. Let G and M be some sets (not necessarily groups). A
mapping ξ from G to M is a function
ξ : G →M
that maps each element g ∈G to an element

154
5
Group Representation and Isomorphism Theorems
ξ(g) ∈M.
Consider now some element m ∈M. What elements are mapped to it from G? There
could be many. Let’s place them in one subset, called the origin of m under ξ:
ξ−1(m) ≡{g ∈G | ξ(g) = m} ⊂G.
This is just a notation for a subset of G. It has nothing to do with inverse. In fact, ξ
may have no inverse mapping at all. After all, ξ−1(m) might contain more than one
element in G. On the other hand, ξ−1(m) might also be completely empty. In either
case, no inverse mapping could possibly be deﬁned.
We say that ξ is onto M if every element m ∈M has a nonempty origin, with at
least one element in it:
ξ−1(m)
 = |{g ∈G | ξ(g) = m}| ≥1,
m ∈M.
Furthermore, we say that ξ is one-to-one if every element m ∈M has at most one
element in its origin:
ξ−1(m)
 = |{g ∈G | ξ(g) = m}| ≤1,
m ∈M.
Clearly, we can now combine these properties: ξ is a one-to-one mapping from G
onto M if every element m ∈M has exactly one element in its origin:
ξ−1(m)
 = |{g ∈G | ξ(g) = m}| = 1,
m ∈M.
This element is the one to which m is mapped by the inverse mapping ξ−1. Only in
this case is ξ invertible.
5.4.2
Homomorphism
Unlike in a mere set, the elements in the group are algebraic: they can multiply each
other. Furthermore, they can also be mapped to yet another group, to have a better
idea about their nature and properties. For this purpose, however, the mapping must
preserve or mirror the original algebraic structure.
Let G and M be two groups. A (not necessarily one-to-one) mapping ξ from G
onto M is called a homomorphism if it preserves algebraic operations: a product in
G is mapped to the product in M (Fig.5.1).
More precisely, for every two elements g, g′ ∈G, order doesn’t matter: multiply-
ing in G and then transferring the product to M is the same as transferring to M and
then multiplying in M:
ξ(gg′) = ξ(g)ξ(g′).

5.4 Mapping and Homomorphism
155
Fig. 5.1 The
homomorphism ξ from the
original group G onto the
group M is not necessarily
one-to-one. Still, it preserves
(or mirrors) the algebraic
operation, denoted by the
vertical arrows
product in G
product in M
G
M
ξ
Why is this convenient? Because, when ξ is not one-to-one, M is “smaller,” and easier
to sort out. It is partially “blind:” it doesn’t distinguish between different elements
in G that are of the same kind.
5.4.3
Mapping the Unit Element
Since the homomorphism preserves the algebraic operation, it must map the original
unit element I ∈G to the unit element i ∈M:
ξ(I) = i.
Here, i has nothing to do with the notation i = √−1, used often in complex analysis.
This is just a coincidence that both notations use the same letter i.
Indeed,
ξ(I) = ξ(I I) = ξ(I)ξ(I).
Now, in M, ξ(I) must have an inverse. Let’s use it to multiply this equation (say
from the right). Thanks to associativity, we then have
i = ξ(I)(ξ(I))−1 = (ξ(I)ξ(I))(ξ(I))−1 = ξ(I)

ξ(I)(ξ(I))−1
= ξ(I)i = ξ(I),
as asserted.
5.4.4
Preserving the Inverse Operation
Thanks to the above properties, the homomorphism also preserves the inverse oper-
ation (Fig.5.2): while g maps to ξ(g), the inverse of g in G must map to the inverse
of ξ(g) in M. Indeed,
i = ξ(I) = ξ(gg−1) = ξ(g)ξ(g−1),
so

156
5
Group Representation and Isomorphism Theorems
Fig. 5.2 Thanks to the
homomorphism ξ, the
inverse operation in the
original group G is mirrored
or preserved in the group M
as well
g
g−1
I
i
m
m−1
G
M
ξ
(ξ(g))−1 = ξ

g−1
in M, as asserted.
Still, recall that the homomorphism is not necessarily one-to-one. Thus, I may be
not the only element that maps to i. The elements that map to i, including I, form a
special subset: the kernel.
5.4.5
Kernel of a Mapping
The kernel of a mapping ξ (not necessarily a homomorphism) contains those elements
that are mapped to the unit element i in M:
ξ−1(i) ≡{g ∈G | ξ(g) = i}
(Fig.5.3). Recall that this is just a notation for a subset of G. It means no inverse:
after all, ξ is not necessarily invertible.
If ξ is onto M, then the kernel must be nonempty (Sect.5.4.1). For example, in
the present context, in which ξ is a homomorphism, the kernel must contain at least
one element: the unit element I (Sect.5.4.3).
Unfortunately, the original homomorphism ξ, although it maps G onto M while
preserving algebraic operations, is not necessarily one-to-one, so it is not necessarily
invertible. For example, the kernel may contain more elements but I. This means
that G and M do not exactly mirror each other. Fortunately, ξ can still be modiﬁed
to form an invertible mapping. For this purpose, we need a new concept: subgroup.
Fig. 5.3 The
homomorphism ξ maps its
entire kernel (on the left) to
the unit element i ∈M (on
the right)
i
ξ−1(i)
M
ξ

5.5 The Center and Kernel Subgroups
157
5.5
The Center and Kernel Subgroups
5.5.1
Subgroup
A subgroup is a subset that is a group in its own right: a subset S ⊂G is a subgroup
if
1. S is closed under multiplication:
s, s′ ∈S ⇒ss′ ∈S.
2. S contains the unit element:
I ∈S.
3. S is closed under the inverse operation:
s ∈S ⇒s−1 ∈S.
Fortunately, S also inherits the associative law:
s, s′, s′′ ∈S ⇒s, s′, s′′ ∈G ⇒(ss′)s′′ = s(s′s′′).
Thus, S is indeed a legitimate group in its own right.
As a matter of fact, to make sure that a subset S is also a subgroup, it is sufﬁcient
to check just two conditions:
1. S is closed under division:
s, s′ ∈S ⇒ss′−1 ∈S.
2. S contains the unit element:
I ∈S.
Indeed, under these conditions, S is also closed under the inverse operation:
s ∈S ⇒s−1 = Is−1 ∈S.
As a result, S is also closed under multiplication:
s, s′ ∈S ⇒s, s−1 ∈S ⇒ss′ = s

s′−1−1 ∈S.
Thus, the original three conditions hold, so S is indeed a legitimate subgroup of G.

158
5
Group Representation and Isomorphism Theorems
5.5.2
The Center Subgroup
Recall that our original group G is not necessarily commutative: it may contain
“bad” elements that don’t commute with each other. Still, it may also contain “good”
elements that do commute with every element. Let’s place them in a new subset: the
center C:
C ≡{c ∈G | cg = gc for every g ∈G} .
By now, we only know that C is a subset. Is it also a subgroup? Let’s see: is it
closed under multiplication? Well, let c and c′ be two elements in C. Thanks to
the associative law inherited from G, the product cc′ commutes with every element
g ∈G:
(cc′)g = c(c′g) = c(gc′) = (cg)c′ = (gc)c′ = g(cc′).
Thus, cc′ is in C as well, as required.
Next, is the unit element I in C? Well, it does commute with every element g ∈G:
Ig = g = gI.
Finally, is C closed under the inverse operation? In other words, for every element
c ∈C, does its inverse c−1 commute with every element g ∈G? Well, we already
know that c does:
cg = gc.
Now, let’s multiply this equation by c−1 from the right. Thanks to associativity,
(cg)c−1 = (gc)c−1 = g(cc−1) = gI = g.
Now, let’s multiply this equation by c−1 from the left:
c−1 
(cg)c−1
= c−1g.
Thanks again to associativity,
gc−1 = I

gc−1
=

c−1c
 
gc−1
= c−1 
c

gc−1
= c−1 
(cg)c−1
= c−1g.
Thus, c−1 does commute with every g ∈G, so it does belong to C as well, as required.
This proves that the center C is not only a subset but also a subgroup of G.
5.5.3
The Kernel Subgroup
G has another interesting subset: the kernel ξ−1(i) of the homomorphism ξ : G →M
(Sect.5.4.5). Is it also a subgroup?
Well,isitclosedundermultiplication?Well,foreverytwoelementsg, g′ ∈ξ−1(i),

5.5 The Center and Kernel Subgroups
159
ξ(gg′) = ξ(g)ξ(g′) = ii = i,
so their product is in the kernel as well:
gg′ ∈ξ−1(i).
Next, is the unit element I ∈G in ξ−1(i) as well? Yes, it is (Sect.5.4.3).
Finally, is the kernel also closed under the inverse operation? Well, for each
element g ∈ξ−1(i), its inverse g−1 is in ξ−1(i) as well:
ξ(g−1) = (ξ(g))−1 = i−1 = i
(Sect.5.4.4). This proves that the kernel of ξ is not only a subset but also a subgroup
of G.
5.6
Equivalence Classes
5.6.1
Equivalence Relation in a Set
In a set G (not necessarily a group), what is a relation? Well, a relation is actually a
subset of G2: it may contain an ordered pairs of the form (g, g′). We then say that g
is related to g′:
g ∼g′.
What is an equivalence relation? Well, this is a special kind of relation: it has three
properties:
1. Reﬂexivity: every element g ∈G is related to itself:
g ∼g.
2. Symmetry: for every two elements g, g′ ∈G, if g is related to g′, then g′ is
related to g as well:
g ∼g′ ⇒g′ ∼g.
3. Transitivity: for every three elements g, g′, g′′ ∈G, if g is related to g′ and g′ is
related to g′′, then g is related to g′′ as well:
g ∼g′, g′ ∼g′′ ⇒g ∼g′′.
Let’s use this to decompose the original set G.

160
5
Group Representation and Isomorphism Theorems
Fig. 5.4 The original set G
is decomposed (or split) into
disjoint lines, or equivalence
classes
G
5.6.2
Decomposition into Equivalence Classes
Thanks to the equivalence relation, we can now decompose the original set G (which
may be a group or not) in terms of disjoint equivalence classes (Fig.5.4). In this
decomposition, each equivalence class contains those elements that are related (or
equivalent) to each other. In particular, each element g ∈G belongs to one equiva-
lence class only:
g ∈ψg ≡

g′ ∈G | g′ ∼g

.
Indeed, thanks to reﬂexivity, g ∼g, so g ∈ψg. Now, could g belong to yet another
equivalence class of the form ψg′, for any other g′ ∈G? Well, if it did, then this
would mean that g ∼g′. Thanks to transitivity, we’d then have
g′′ ∼g ⇒g′′ ∼g′,
so
ψg ⊂ψg′.
On the other hand, thanks to symmetry, we’d also have g′ ∼g. Thanks again to
transitivity, we’d then have
g′′ ∼g′ ⇒g′′ ∼g,
so
ψg′ ⊂ψg.
In summary, we’d have
ψg′ = ψg,
so ψg is actually the only equivalence class containing g, as asserted.
5.6.3
Family of Equivalence Classes
Let’s look at the family (or set) of these disjoint equivalence classes:

ψg | g ∈G

.

5.6 Equivalence Classes
161
In this family, it is assumed that there is no duplication: each equivalence class of
the form ψg appears only once, with g being some representative picked arbitrarily
from it. Furthermore, in this family, each equivalence class is an individual element,
not a subset. To pick its inner elements and obtain G one again, one must apply the
union operation:
G = ∪g∈Gψg.
This is the union of all the ψg’s: it contains all their elements.
5.6.4
Equivalence Relation Induced by a Subgroup
So far, the discussion was rather theoretical. After all, we never speciﬁed what the
equivalence relation was. Now, let’s go back to business. Assume again that G is not
just a set but actually a group, as before, with a subgroup S ⊂G. This way, S may
help deﬁne (or induce) a new relation: for every two elements g′ and g in G, g′ is
related to g if their “ratio” is in S:
g′ ∼g if g′g−1 ∈S.
In other words, there is an element s ∈S that can multiply g and produce g′:
g′ = sg.
In this case, s = g′g−1 is unique.
Is this an equivalence relation? Well, let’s check: is it reﬂexive? In other words,
given a g ∈G, is it related to itself? Fortunately, it is:
gg−1 = I ∈S,
as required.
Next, is it symmetric? Well, consider two elements g′, g ∈G. Assume that g′ ∼g,
or g′g−1 ∈S. Since S is a subgroup, it also contains the inverse element:
gg′−1 =

g′g−1−1 ∈S,
so g ∼g′ as well, as required.
Finally, is it transitive? Well, consider three elements g, g′, g′′ ∈G. Recall that S
is closed under multiplication: if g′g−1 ∈S and g′′g′−1 ∈S, then their product is in
S as well. For this reason, thanks to associativity,
g′′g−1 = g′′(Ig−1)
= g′′ 
g′−1g′
g−1

162
5
Group Representation and Isomorphism Theorems
= g′′ 
g′−1 
g′g−1
=

g′′g′−1 
g′g−1
∈S
as well, as required. In summary, this is indeed an equivalence relation in the original
group G.
5.6.5
Equivalence Classes Induced by a Subgroup
With this new equivalence relation, how does an equivalence class look like? Well,
consider a particular element g ∈G. As discussed in Sect.5.6.2, it is contained in
one equivalence class only:
ψg ≡

g′ ∈G | g′ ∼g

=

g′ ∈G | g′g−1 ∈S

=

g′ ∈G | g′g−1 = s for some s ∈S

=

g′ ∈G | g′ = sg for some s ∈S

.
Thus, the equivalence class takes the special form
ψg = Sg ≡{sg | s ∈S} .
5.7
The Factor Group
5.7.1
The New Set G/S
Let’s place these equivalence classes as individual elements in a new set (or family):
{Sg | g ∈G} .
Unfortunately, in this family, there is some duplication: each equivalence class of
the form Sg may appear many times. In fact, every two equivalent elements g′ ∼g
introduce the same equivalence class Sg′ = Sg into the above family.
To avoid this, one might want to drop all the duplicate copies of the form Sg′.
This way, Sg appears only once, with g being some representative picked arbitrarily
from it. The resulting family is called G/S (Fig.5.5).

5.7 The Factor Group
163
Fig. 5.5 Disjoint
equivalence classes are
considered as individual
elements in G/S
G/S
G
Why is this name suitable? Because the original elements in G are regarded only
up to multiplication (from the left) by just any element from S. This way, equivalent
elements in G are united into one and the same element in G/S.
Thus, G/S is completely blind to any difference between equivalent elements
g′ ∼g, and can never distinguish between them. After all, in G/S, such elements
coincide to form the same element Sg′ = Sg.
5.7.2
Normal Subgroup
So far, G/S was just a set. To make it a group, we must deﬁne a proper multiplication
between its elements. This is not always possible.
To guarantee that it is, we must also assume that S is normal in the sense that it
“commutes” with every element g ∈G:
Sg = gS ≡{gs | s ∈S} .
In other words, for every s ∈S, there is an s′ ∈S (dependent on both g and s) for
which
gs′ = sg.
In this case, s′ = g−1sg is unique.
For example, S could be a subgroup of the center of G, deﬁned in Sect.5.5.2:
S ⊂C ⊂G.
In this case, s′ = s in the above equation.
What’s so good about S being normal? Well, consider two elements g, h ∈G. In
G, their product is just gh. Now, let s and s′ be two elements from S. What happens
when g is replaced by the equivalent element sg, and h is replaced by the equivalent
element s′h? Well, since S is normal, there is an s′′ ∈S such that
s′′g = gs′.
Thanks to associativity,

164
5
Group Representation and Isomorphism Theorems
(sg)(s′h) = s(g(s′h)) = s((gs′)h) = s((s′′g)h) = s(s′′(gh)) = (ss′′)(gh)).
Thus, the product gh hasn’t changed much: it was just replaced by an equivalent
element.
In summary, thanks to normality, multiplication is invariant under the equivalence
relation. In other words, order doesn’t matter: switching to an equivalent element
and then multiplying is the same as multiplying and then switching to the relevant
equivalent element. In terms of equivalent classes, this could be written as
(Sg)(Sh) = S(gh).
In G, this means that the left-hand side is the same equivalence class as the right-hand
side. In g/S, on the other hand, this could be used as a new deﬁnition.
5.7.3
The Factor (Quotient) Group
In G/S, the original equivalence classes are considered as elements. How to multiply
them with each other? Well, consider two elements of the form
Sg, Sh ∈G/S,
for some g, h ∈G. Their product in G/S is now deﬁned as
(Sg)(Sh) ≡S(gh).
To be well-deﬁned, this product mustn’t depend on the particular representatives g
or h: replacing each of them by an equivalent element mustn’t affect the result. Since
S is normal, this is indeed the case.
Still, this is not the end of it. To be a group, G/S must also be associative. To
check on this, consider three elements of the form
Sg, Sh, Sk ∈G/S,
for some g, h, k ∈G. Since G is associative and S is normal,
(SgSh)Sk = S(gh)Sk = S((gh)k) = S(g(hk)) = SgS(hk) = Sg(ShSk),
so G/S is associative as well.
Still, this is not the end of it. To be a group, G/S must also contain a unit element.
For this job, let’s choose
S = SI ∈G/S.
Indeed, for any element Sg ∈G/S,

5.7 The Factor Group
165
SgSI = S(gI) = Sg = S(Ig) = (SI)Sg.
Still, this is not the end. To be a group, G/S must also be closed under the inverse
operation. Fortunately, for every element of the form Sg ∈G/S, the inverse is just
Sg−1 ∈G/S. Indeed,
SgSg−1 = S

gg−1
= SI = S.
This is the end of it: G/S is indeed a legitimate group. Let’s use it to modify
the original homomorphism ξ deﬁned in Sect.5.4.2, and make a new one-to-one
isomorphism.
5.7.4
Isomorphism
Consider again the homomorphism
ξ : G →M.
As pointed out in Sect.5.4.2, ξ is not necessarily one-to-one, so it is not necessarily
invertible: there may be some element m ∈M with more than one element in its
origin: |ξ−1(m)| > 1.
Fortunately, ξ can still be modiﬁed to produce a one-to-one homomorphism:
an isomorphism. To do this, consider the kernel ξ−1(i), deﬁned in Sect.5.4.5. As
discussed in Sect.5.5.3, this is a legitimate subgroup of G. Therefore, it can be
substituted for S above, and induce a new equivalence relation in G. The equivalence
classes can then be placed in the new set
G/ξ−1(i).
Moreover, if the kernel is normal in the sense that it commutes with every element
in G:
ξ−1(i)g = gξ−1(i),
g ∈G,
then this is not just a set but actually a new group: the factor group (Sects.5.7.2–5.7.3).
Fortunately, ξ doesn’t distinguish between equivalent elements in G. For example,
if g and g′ are equivalent to each other, then there must be an element s ∈ξ−1(i) for
which
g′ = sg
(Sect.5.6.5). Therefore, we must have
ξ(g′) = ξ(sg) = ξ(s)ξ(g) = iξ(g) = ξ(g).

166
5
Group Representation and Isomorphism Theorems
Fig. 5.6 The new mapping
Ξ maps disjoint equivalence
classes (or distinct elements
in the factor group) to
distinct elements in M
M
G
Ξ
Thus, ξ maps the entire equivalence class to one and the same element in M. This
observation can now be used to form a new one-to-one mapping from the factor group
G/ξ−1(i) onto M. In this new mapping, the entire equivalence class is mapped as a
whole to its image: some element in M. This is indeed invertible: this image element
could simply map back to the original equivalence class.
More precisely, the new isomorphism
Ξ : G/ξ−1(i) →M
is deﬁned by
Ξ

ξ−1(i)g

≡ξ(g),
g ∈G
(Fig.5.6). Why is this well-deﬁned? Because it doesn’t depend on the particular
representative g picked arbitrarily from the equivalence class. After all, one could
pick any equivalent element g′ ∼g, and still have
Ξ

ξ−1(i)g′
= ξ(g′) = ξ(g),
as shown above.
5.7.5
The Fundamental Theorem of Homomorphism
Like the original homomorphism ξ, Ξ is onto M. Indeed, for every element m ∈M,
there is an element g ∈G for which ξ(g) = m. Therefore,
Ξ

ξ−1(i)g

= ξ(g) = m,
as required.
Furthermore, Ξ is one-to-one. Indeed, consider two distinct elements ξ−1(i)g and
ξ−1(i)g′ in G/ξ−1(i). Clearly, g′ ≁g, so g′g−1 /∈ξ−1(i), so
ξ(g′)(ξ(g))−1 = ξ(g′)ξ

g−1
= ξ(g′g−1) ̸= i
(Sect.5.4.4). As a result,
Ξ

ξ−1(i)g

= ξ(g) ̸= ξ(g′) = Ξ

ξ−1(i)g′
,

5.7 The Factor Group
167
Fig. 5.7 The new
isomorphism Ξ from the
factor group onto M (the
horizontal arrows) preserves
(or mirrors) the algebraic
operations (the vertical
arrows)
product in G/ξ−1(i)
product in M
G/ξ−1(i)
M
Ξ
as asserted. In summary, Ξ is indeed invertible.
Fortunately, Ξ also preserves algebraic operations (Fig.5.7). Indeed, for every
two elements g, g′ ∈G, we have
Ξ

ξ−1(i)g
 
ξ−1(i)g′
= Ξ

ξ−1(i)(gg′)

= ξ(gg′)
= ξ(g)ξ(g′)
= Ξ

ξ−1(i)g

Ξ

ξ−1(i)g′
.
In summary, the factor group G/ξ−1(i) is isomorphic to M:
G/ξ−1(i) ≃M.
Thus, these groups are exactly mirrored by each other, and have the same algebraic
structure.
This is the fundamental theorem of homomorphism, or the ﬁrst isomorphism
theorem. Later on, we’ll use it to prove two other important theorems: the second
and third isomorphism theorems. Before doing this, however, we use it in our original
application: Moebius transformations.
5.8
Geometrical Applications
5.8.1
Application in Moebius Transformations
Let’s apply the above theory to the special case in which G is the group of 2 × 2
nonsingular complex matrices, and M is the group of invertible Moebius transfor-
mations (Sect.5.1.4). In this case, the unit element I ∈G is the 2×2 identity matrix
I =
 1 0
0 1

,
and the unit element i ∈M is the identity mapping
z →z and ∞→∞,

168
5
Group Representation and Isomorphism Theorems
for which a = d ̸= 0 and b = c = 0 in Sect.5.1.2.
In this case, what is the center of G? Well, it contains the nonzero scalar multiples
of the 2 × 2 identity matrix:
C ≡{c ∈G | cg = gc for all g ∈G} = {zI | z ∈C, z ̸= 0}
(see exercises below). From Sect.5.5.2, C is indeed a subgroup of G.
Our job is to design a suitable homomorphism
ξ : G →M,
with the kernel
ξ−1(i) = C.
For this purpose, we need some geometrical preliminaries.
5.8.2
Two-Dimensional Vector Set
Let us use the above center subgroup C ⊂G to deﬁne an equivalence relation in the
set
V ≡C2 \ {(0, 0)}.
Here, ‘\’ means “minus” the set that contains the origin. Thus, V contains the nonzero
two-dimensional complex vector. Although V is not a group but a mere set, it can
still be decomposed in terms of disjoint equivalence classes, as in Sects.5.6.1–5.6.3.
For every two vectors v, v′ ∈V , let
v′ ∼v if v′ = cv for some c ∈C.
Since C is deﬁned as in Sect.5.8.1, this means that v′ is just a nonzero scalar multiple
of v. Still, in principle, the same could be done with other normal subgroups as well.
It is easy to see that this is an equivalence relation in the original set V . Indeed,
• for every v ∈V , v = Iv. This shows reﬂexivity.
• Furthermore, for every v, v′ ∈V , if v′ = cv (for some c ∈C), then v = c−1v′.
This shows symmetry.
• Finally, for every v, v′, v′′ ∈V , if v′ = cv and v′′ = c′v′ (for some c, c′ ∈C),
then v′′ = c′(cv) = (c′c)v. This shows transitivity.
This proves that the aboverelation is indeed a legitimate equivalence relation in V .

5.8 Geometrical Applications
169
the vertical complex plane {(0, z) | z ∈C}
the horizontal
complex plane {(z, 0) | z ∈C}
the oblique plane {z(c1, c2) | z ∈C}
(c1, c2)
Fig. 5.8 A picture of C2: the two-dimensional complex vector (c1, c2)t spans an oblique plane—the
equivalence class C(c1, c2)t
5.8.3
Geometrical Decomposition into Planes
Consider the nonzero two-dimensional complex vector
v ≡
 c1
c2

∈V.
What is its equivalence class? Well, it takes the form
Cv ≡{cv | c ∈C} = {zv | z ∈C, z ̸= 0} =
	
z
 c1
c2

| z ∈C, z ̸= 0

.
In geometrical terms, this is just the oblique plane spanned by the vector v = (c1, c2)t
(Fig.5.8).
5.8.4
Family of Planes
Together, all such planes make the family
V/C ≡{Cv | v ∈V } .
To avoid duplication, each individual plane of the form Cv appears only once, with v
being some representative picked arbitrarily from it. Note that, unlike in Sect.5.7.3,
here V/C is just a set, not a group. This is because the original set V is not a group
in the ﬁrst place.
In V , Cv is a subset: an oblique plane (Fig.5.8). In the new set V/C, on the
other hand, Cv is just an element. To obtain the original set V once again, one must,

170
5
Group Representation and Isomorphism Theorems
therefore, apply the union operation, to pick the inner vectors from each plane:
V = ∪Cv∈V/CCv
(Sect.5.6.3).
5.8.5
Action of Factor Group
The original group G acts on the set V : each element g ∈G acts on each v ∈V ,
transforming it into the new vector gv. Thanks to the above decomposition, this also
applies to complete planes: the factor group G/C acts on v/C.
Indeed, an element of the form Cg ∈G/C acts not only on individual vectors of
the form v ∈V but also on complete planes of the form Cv:
Cg(Cv) ≡C(gv).
Why is this a legitimate deﬁnition? Because it is independent of the particular repre-
sentative g or v. Indeed, since C is normal, replacing g by cg and v by c′v (for some
c, c′ ∈C) changes nothing:
C(cg)(C(c′v)) ≡C(cgc′v) = C(cc′gv) = C(gv).
5.8.6
Composition of Functions
Thanks to the above action, each element of the form Cg ∈G/C can also be
interpreted as a function
Cg : V/C →V/C.
After all, the original algebraic operation in G/C, deﬁned in Sect.5.7.3 as
(Cg′)(Cg) = C(g′g) (g, g′ ∈G),
is mirrored well by function composition:
(Cg′ ◦Cg)(Cv) = Cg′(Cg(Cv)) = Cg′(C(gv)) = C(g′gv) = C(g′g)(Cv),
for every v ∈V .

5.8 Geometrical Applications
171
c1/c2
0
1
horizontal plane: {(z, 0) | z ∈C}
horizontal plane: {(z, 1) | z ∈C}
oblique plane: z(c1, c2)
∞
(c1, c2)
Fig. 5.9 The oblique projection P projects the oblique plane C(c1, c2)t to c1/c2. In particular, the
horizontal complex plane {(z, 0) | z ∈C} projects to ∞
5.8.7
Oblique Projection: Extended Cotangent
Let us deﬁne the oblique projection
P : V/C →C ∪{∞}
by
P

C
 c1
c2

≡
	c1/c2 if c2 ̸= 0
∞
if c2 = 0.
Fortunately, this deﬁnition is independent of the particular representative (c1, c2)t.
After all, for every nonzero complex number z, one may replace c1 by zc1 and c2 by
zc2, and still have the same projection.
In geometrical terms, P can be viewed as an oblique projection on the horizontal
plane
{(z, 1) | z ∈C}
(Fig.5.9). This way, P actually extends the standard cotangent projection, to apply
not only to real numbers but also to complex numbers.
The inverse mapping
P−1 : C ∪{∞} →V/C
can now be deﬁned simply by
P−1(z) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
C
 z
1

if z ∈C
C
 1
0

if z = ∞.
In what sense is this the inverse? Well, in two senses: on one hand,

172
5
Group Representation and Isomorphism Theorems
P−1P = C I = C
is the unit element in G/C that leaves V/C unchanged: each oblique plane projects,
and then unprojects. On the other hand,
P P−1 = i
is the identity transformation in Sect.5.8.1: each complex number unprojects, and
then projects back.
Let’s use P and P−1 to associate the original Moebius transformation with the
relevant 2 × 2 matrix.
5.8.8
Homomorphism Onto Moebius Transformations
The association made in Sect.5.1.4 takes now the form of a new homomorphism
ξ : G →M,
from the group of 2 × 2 nonsingular complex matrices, onto the group of invertible
Moebius transformations:
ξ(g) ≡PCgP−1
(g ∈G).
Why is this a Moebius transformation? Well, look what happens to a complex number
z: it transforms to
z →PCgP−1z,
or, in three stages,
z →P−1z →CgP−1z →PCgP−1z.
In other words, z ﬁrst unprojects to an oblique plane:
z →C
 z
1

,
which is then multiplied from the left by
Cg ≡C
a b
c d

,
and projects back:

5.8 Geometrical Applications
173
C
a b
c d

C
 z
1

= C
a b
c d
  z
1

= C
az + b
cz + d

→az + b
cz + d ,
as required.
Furthermore, what happens to the inﬁnity point? Well, it ﬁrst unprojects to a
horizontal plane:
∞→C
 1
0

,
which then transforms by
C
 1
0

→C
a b
c d

C
 1
0

= C
a b
c d
  1
0

= C
a
c

,
which then projects back:
C
a
c

→a
c ,
as required.
Let us show that ξ is indeed a legitimate homomorphism. First, is it onto M? Well,
as discussed in Sect.5.1.4, every invertible Moebius transformation m ∈M has the
explicit form
z →az + b
cz + d ,
for some complex parameters a, b, c, and d, satisfying
ad −bc ̸= 0.
As discussed above, this transformation could also be decomposed as
m = ξ(g) = PCgP−1,
where
g =
a b
c d

is a nonsingular matrix, with a nonzero determinant. So, ξ maps g to m, as required.
Finally, does ξ preserve algebraic operations? Well, thanks to the associativity of
function composition (Sects.5.8.5–5.8.6),
ξ(g′)ξ(g) =

PCg′P−1 
PCgP−1
= (PCg′)

P−1P

CgP−1
= P(Cg′)(C I)(Cg)P−1
= P(Cg′)(Cg)P−1

174
5
Group Representation and Isomorphism Theorems
= PC(g′g)P−1
= ξ(g′g).
This proves that ξ is indeed a legitimate homomorphism, as asserted.
5.8.9
The Kernel
To design a proper isomorphism as well, we must also have the kernel of ξ. Fortu-
nately, this is just the center of G:
ξ−1(i) = C = {zI | z ∈C, z ̸= 0}
(deﬁned in Sect.5.8.1). Let’s prove this in two stages. First, let’s show that
C ⊂ξ−1(i).
Indeed, for every c ∈C, ξ(c) is just the identity transformation that leaves every
complex number unchanged:
ξ(c)(z) = PCcP−1(z) = PCcC
 z
1

= PC
 z
1

= z,
and also leaves the inﬁnity point unchanged:
ξ(c)(∞) = PCcP−1(∞) = PCcC
 1
0

= PC
 1
0

= ∞.
Thus, ξ(c) is indeed the identity mapping, or the unit element in M:
ξ(c) = i ∈M,
so
C ⊂ξ−1(i),
as asserted.
Conversely, let us also show that
ξ−1(i) ⊂C.
Indeed, if ξ(g) is the identity transformation z →z, then g must be a nonzero scalar
multiple of the 2 × 2 identity matrix (see exercises below).
In summary,
ξ−1(i) = C,

5.8 Geometrical Applications
175
as asserted. This means that ξ doesn’t distinguish between matrices that are a nonzero
scalar multiple of each other. In view of the discussion in Sect.5.1.4, ξ is indeed a
good candidate to represent invertible Moebius transformations in terms of 2 × 2
nonsingular complex matrices.
5.8.10
Eigenvectors and Fixed Points
If v ∈V is an eigenvector of g ∈G with the eigenvalue λ ∈C:
gv = λv,
then Cv contains eigenvectors only. After all, each element in Cv is a nonzero scalar
multiple of v, so it must be an eigenvector as well, with the same eigenvalue λ.
Furthermore, since g is nonsingular, λ must be nonzero.
In this case, Cv is a ﬁxed point that remains unchanged under the action of Cg.
Indeed, from the deﬁnitions in Sect.5.8.5,
Cg(Cv) = C(gv) = C(λv) = Cv.
Furthermore, in this case, PCv is a ﬁxed point that remains unchanged under the
Moebius transformation ξ(g):
ξ(g)(PCv) = PCgP−1PCv = PCgCv = PCv.
5.8.11
Isomorphism Onto Moebius Transformations
Let us now use the fundamental theorem of homomorphism (Sects.5.7.4–5.7.5) to
design a new isomorphism
Ξ : G/C →M.
Naturally, it is deﬁned by
Ξ(Cg) ≡ξ(g) = PCgP−1,
Cg ∈G/C.
This way, Ξ doesn’t distinguish between matrices that are a nonzero scalar multiple
of each other: it views them as one element in G/C, and maps them as a whole to
the same Moebius transformation.
This is indeed a proper group representation. To see this, look at things the other
way around. A Moebius transformation is not easy to store on the computer. To do
this, use Ξ −1. In fact, each element m ∈M is mirrored (or represented) by the unique
element

176
5
Group Representation and Isomorphism Theorems
ξ−1(m) = P−1mP = C
a b
c d

∈G/C.
After all, a matrix is easy to store on the computer. Furthermore, Ξ −1 also preserves
the algebraic operation: it mirrors it by matrix product, easy to calculate on the
computer.
5.9
Application in Continued Fractions
5.9.1
Continued Fractions
Let us use the above theory to deﬁne a continued fraction. For k = 1, 2, 3, . . .,
consider the Moebius transformations
mk(z) ≡
ak
z + bk
,
where ak and bk are some nonzero complex numbers (known as the coefﬁcients).
For n = 1, 2, 3, . . ., consider the compositions
f1 ≡m1
f2 ≡m1 ◦m2
f3 ≡m1 ◦m2 ◦m3
fn ≡m1 ◦m2 ◦m3 ◦· · · ◦mn.
As a matter of fact, this is just a mathematical induction:
fn ≡
	
m1
if n = 1
fn−1 ◦mn
if n > 1.
Now, let’s apply these functions to z = 0:
f1(0), f2(0), f3(0), . . . .
These are the approximants. We say that they converge (in the wide sense) to the
continued fraction f if
fn(0) →n→∞f ∈C ∪{∞}
[27, 28]. In particular, if f ∈C is a concrete number, then the convergence is in the
strict sense as well. If, on the other hand, f = ∞is no number, then the convergence
is in the wide sense only.

5.9 Application in Continued Fractions
177
5.9.2
Algebraic Formulation
To study the convergence, let’s use the factor group
G/C ≃M,
with the isomorphism Ξ in Sect.5.8.11. For this purpose, let’s deﬁne the new matrices
in Sect.5.1.4:
gk ≡
0 ak
1 bk

.
This way,
mk = Ξ(Cgk) = PCgk P−1.
For example, both sides of this equation could be applied to the complex number
0 ∈C:
mk(0) = PCgk P−1(0) = PCgkC
 0
1

= PCgk
 0
1

= PC
ak
bk

= ak
bk
,
in agreement with the original deﬁnition of mk.
5.9.3
The Approximants
Let us use the isomorphism Ξ to obtain the composition fn as well:
Ξ(Cg1g2 · · · gn) = Ξ(Cg1Cg2 · · · Cgn)
= Ξ(Cg1)Ξ(Cg2) · · · Ξ(Cgn)
= m1 ◦m2 ◦· · · ◦mn
= fn.
Both sides of this equation could now be applied to 0 ∈C:
PCg1g2 · · · gn
 0
1

= PCg1g2 · · · gn P−1(0)
= Ξ(Cg1g2 · · · gn)(0)
= fn(0).
In other words, the approximant fn(0) is just the ratio between the upper right and
lower right elements in the matrix product g1g2 · · · gn:

178
5
Group Representation and Isomorphism Theorems
fn(0) = (g1g2 · · · gn)1,2
(g1g2 · · · gn)2,2
.
This observation will be useful below.
5.9.4
Algebraic Convergence
Actually,thismatrixproductisdeﬁnedbymathematicalinductiononn = 1, 2, 3, . . .:
g1g2 · · · gn ≡
	
g1
if n = 1
((g1g2 · · · gn−1) gn
if n > 1.
Recall that gn is a special 2 × 2 matrix: its ﬁrst column is just the standard unit
vector (0, 1)t. For this reason, the above products also have a special property: the
ﬁrst column in g1g2 · · · gn is the same as the second column in g1g2 · · · gn−1:
g1g2 · · · gn
 1
0

= (g1g2 · · · gn−1)

gn
 1
0

= g1g2 · · · gn−1
0
1

.
Thus, if convergence indeed takes place as n →∞, then both columns in g1g2 · · · gn
must be nearly proportional to each other: the ratio between the upper and lower
components must approach the same limit f .
Note that these ratios remain unchanged upon multiplying the original product
g1g2 · · · gn by a nonsingular diagonal matrix from the right. Thus, the continued
fraction f exists if and only if there exist diagonal matrices Dn ∈G for which
g1g2 · · · gn Dn →n→∞(v | v) ,
for some
v =
 c1
c2

∈V.
What is the meaning of this convergence? Well, it is interpreted elementwise: there
are actually two independent limit processes here—one for the upper right element,
and another one for the lower right element. Thanks to this convergence, the required
continued fraction f can now be obtained by
f ≡lim
n→∞fn(0) = lim
n→∞PCg1g2 · · · gn
0
1

= PCv = c1
c2
∈C ∪{∞}.
Thus, to guarantee convergence, one only needs to design suitable diagonal matrices
Dn ∈G, in such a way that g1g2 · · · gn Dn converge elementwise to a singular matrix
of the form (v | v) /∈G, for some v ∈V . This can also be written more concisely

5.9 Application in Continued Fractions
179
as
Cg1g2 · · · gn →n→∞(Cv | Cv) .
This means that G/C is not closed: elements from it could converge to a limit outside
it [62]. In this case, if the second component in v is nonzero:
c2 ̸= 0,
then the convergence is also in the strict sense. Otherwise, it is in the wide sense
only.
5.10
Isomorphism Theorems
5.10.1
The Second Isomorphism Theorem
By now, we are rather experienced in “playing” with groups. Let’s use the funda-
mental theorem of homomorphism (Sect.5.7.5) to prove another important theorem
in group theory: the second isomorphism theorem, used later in projective geometry.
For this purpose, let G be a group. Let T ⊂G be a subgroup (normal or not). Let
S ⊂G be a normal subgroup.
Consider T ∩S. Is it a legitimate group? Well, it certainly contains the unit element.
After all, I ∈T , and I ∈S. Now, is it closed under multiplication? Well, to check
on this, let s, s′ ∈T ∩S. In this case, ss′ ∈T , and ss′ ∈S, as required. Finally, is
it closed under the inverse operation? Well, to check on this, let s ∈T ∩S. In this
case, s−1 ∈T , and s−1 ∈S, as required. In summary, T ∩S is indeed a legitimate
subgroup of T .
Still, is it normal? Well, to check on this, let s ∈T ∩S, and t ∈T . Since S ⊂G
is normal, there is an s′ ∈S such that st = ts′. Fortunately, s′ = t−1st ∈T , so
s′ ∈T ∩S, as required.
What is the product of T times S? Well, it contains those products of an element
from T with an element from S:
T S ≡{ts | t ∈T, s ∈S} ⊂G.
Is this a legitimate group? Well, it certainly contains the unit element. After all,
I ∈T , and I ∈S. Now, is it closed under multiplication? Well, to check on this, let
ts, t′s′ ∈T S. Since S is normal, st′ = t′s′′, for some s′′ ∈S. Thus,
(ts)(t′s′) = t(st′)s′ = t(t′s′′)s′ = (tt′)(s′′s′) ∈T S,
as required.

180
5
Group Representation and Isomorphism Theorems
Finally, is it closed under the inverse operation? Well, to check on this, let ts ∈T S.
Now, since S is normal, s−1t−1 = t−1s′′′, for some s′′′ ∈S. Therefore,
(ts)(t−1s′′′) = (ts)(s−1t−1) = t(ss−1)t−1 = tt−1 = I,
as required. So, T S is indeed a legitimate subgroup, although not necessarily normal.
If T was also normal, then T S would have been normal as well. Indeed, in this
case, for each g ∈G and ts ∈T S, there would be t′ ∈T and s′ ∈S for which
(ts)g = t(sg) = t(gs′) = (tg)s′ = (gt′)s′ = g(t′s′),
as required. For our purpose, however, we don’t need this, so T could be either
normal or not.
S, on the other hand, is normal not only in G but also in T S. To see this, let
ts ∈T S, and s′ ∈S. Since S ⊂G is normal, and since ts ∈G, there is an s′′ ∈S
such that (ts)s′ = s′′(ts), as required.
The second isomorphism theorem says that
T
T ∩S ≃T S
S .
To prove this, let’s use the fundamental theorem of homomorphism. For this purpose,
deﬁne the new homomorphism
ξ : T →T S
S
by ξ(t) ≡St.
Is this a legitimate homomorphism? Well, is it onto? Fortunately, it certainly is: after
all, every element Sts ∈T S/S can also be written as Sts = Ss′t = St.
Furthermore, ξ certainly preserves the original algebraic operation in T . So, it is
indeed a legitimate homomorphism.
Moreover, its kernel is T ∩S. So, we can now use the fundamental theorem of
homomorphism to obtain
T
T ∩S ≃T S
S ,
as asserted.
5.10.2
The Third Isomorphism Theorem
Finally, let’s use the fundamental theorem of homomorphism to prove yet another
important theorem in group theory: the third isomorphism theorem. Thanks to this
theorem, groups may sometimes behave just like simple fractions: a common factor
can be “canceled out.”

5.10 Isomorphism Theorems
181
Let G be a group. Let S, T ⊂G be two normal subgroups. Assume also that
S ⊂T .
Note that S is normal not only in G but also in T . Indeed, let t ∈T , and s ∈S.
Since t ∈G, there is an s′ ∈S such that st = ts′.
Now, consider T/S ⊂G/S. Is it a legitimate subgroup? Well, it certainly contains
the unit element: S. Still, is it closed under multiplication? To check on this, let
St, St′ ∈T/S. Fortunately, StSt′ = S(tt′) is in T/S as well, as required.
Furthermore, is it closed under the inverse operation? Fortunately, it is: the inverse
element St−1 is in T/S as well.
So, T/S ⊂G/S is a legitimate subgroup. Is it normal? Well, to check on this, let
g ∈G, and t ∈T . Since T ⊂G is normal, there is a t′ ∈T such that
StSg = S(tg) = S(gt′) = SgSt′,
as required.
The third isomorphism theorem tells us that, as in simple fractions, S could be
“canceled out:”
G/T ≃G/S
T/S .
To prove this, let’s use the fundamental theorem of homomorphism. For this purpose,
deﬁne the new homomorphism
ξ : G →G/S by ξ(g) ≡Sg,
g ∈G.
On top of this, deﬁne yet another homomorphism:
ξ′ : G/S →G/S
T/S
by ξ′(Sg) ≡(T/S)Sg,
Sg ∈G/S.
Now, consider the composite homomorphism
ξ′ξ : G →G/S
T/S .
What is its kernel? Well, it is just T ! Indeed, on one hand, it includes T . After all, in
T/S, a typical element is of the form St (for some t ∈T ). Therefore,
ξ′ξ(t) = ξ′(St) = (T/S)St = T/S,
which is just the unit element in (G/S)/(T/S). On the other hand, the kernel of ξ′ξ
is also included in T . After all, if g ∈G \ T , then
ξ′ξ(g) = ξ′(Sg) = (T/S)Sg ̸= T/S,
because Sg /∈T/S.

182
5
Group Representation and Isomorphism Theorems
Thanks to the fundamental theorem of homomorphism, we therefore have
G/T ≃G/S
T/S ,
as asserted. In the next chapter, we’ll use the isomorphism theorems in projective
geometry.
5.11
Exercises
1. Recall that G is the group of 2 × 2 nonsingular complex matrices. Let A and B
be two matrices in G, denoted by
A ≡
a1,1 a1,2
a2,1 a2,2

and B ≡
 b1,1 b1,2
b2,1 b2,2

.
Assume that B is also a diagonal matrix:
b1,2 = b2,1 = 0.
Show that the upper right element in the product AB is
(AB)1,2 = b2,2a1,2.
2. Show that the upper right element in the product B A is
(B A)1,2 = b1,1a1,2.
3. Assume also that A and B commute with each other:
AB = B A.
Conclude that
b2,2a1,2 = (AB)1,2 = (B A)1,2 = b1,1a1,2.
4. Assume also that B is nonconstant:
b1,1 ̸= b2,2.
Conclude that A must be lower triangular:
a1,2 = 0.
5. Similarly, show that the lower left element in the product AB is

5.11 Exercises
183
(AB)2,1 = b1,1a2,1.
6. Similarly, show that the lower left element in the product B A is
(B A)2,1 = b2,2a2,1.
7. Conclude that if A and B commute with each other, then
b1,1a2,1(AB)2,1 = (B A)2,1 = b2,2a2,1.
8. Conclude that if B is also nonconstant, then A must be upper triangular:
a2,1 = 0.
9. Conclude that, if A commutes with the nonconstant diagonal matrix B, then A
must be diagonal as well:
a1,2 = a2,1 = 0.
10. Conclude that if A commutes with every matrix B ∈G, then A must be diagonal.
11. Conclude that the center of G may contain diagonal matrices only.
12. Show that the diagonal matrices make a subgroup in G.
13. Conclude that the center must be a subgroup of that subgroup.
14. Assume now that A is diagonal:
a1,2 = a2,1 = 0,
and B is not necessarily diagonal. Show that the upper right element in the
product AB is
(AB)1,2 = a1,1b1,2.
15. Similarly, show that the upper right element in the product B A is
(B A)1,2 = a2,2b1,2.
16. Conclude that if A commutes with B, then
a1,1b1,2 = (AB)1,2 = (B A)1,2 = a2,2b1,2.
17. Conclude that if B is not lower triangular:
b1,2 ̸= 0,
then A must be constant:
a1,1 = a2,2.

184
5
Group Representation and Isomorphism Theorems
18. Conclude that if A commutes with every matrix B ∈G, then A must be a
constant diagonal matrix.
19. Conclude that the center of G may contain constant diagonal matrices only.
20. Conclude that the center of G may contain only nonzero scalar multiples of the
identity matrix I ∈G.
21. Show that the center of G contains all nonzero scalar multiples of I ∈G.
22. Show that this is indeed a subgroup of G.
23. Recall that M is the group of invertible Moebius transformations, with compo-
sition as the algebraic operation. Show that the identity mapping
i(z) ≡az + b
cz + d ≡z
is indeed the unique unit element in M:
im = mi = m,
m ∈M.
(Recall that i has nothing to do with the imaginary number √−1, denoted often
by the same letter i.)
24. Show that, in the above formulation of i, in the numerator, we must have a ̸= 0.
25. Show also that, in the above formulation of i, in the numerator, we must have
b = 0. Hint:
i
−b
a

= a −b
a + b
c −b
a + d = −b + b
ad−bc
a
= a(−b + b)
ad −bc
= 0.
Now, since i is the identity mapping, we must also have −b/a = 0, or b = 0.
26. Show also that, in the above formulation of i, in the denominator, we must have
c = 0. Hint:
i
−d
c

= a −d
c + b
c −d
c + d = −ad−bc
c
−d + d = ad −bc
c(d −d) = ∞.
Now, since i is the identity mapping, we must also have −d/c = ∞, or d = 0.
27. Conclude that, in the above formulation of i, we must also have d = a. Hint:
thanks to the previous exercises, i(1) = a/d. Now, since i is the identity map-
ping, we must also have a/d = 1, or d = a.
28. Recall that ξ is deﬁned by
ξ(g) ≡PCgP−1
(g ∈G)
(Sect.5.8.8). Show that ξ maps G onto M.
29. Show that ξ preserves algebraic operations:
ξ(g)ξ(g′) = ξ(gg′),
g, g′ ∈G.

5.11 Exercises
185
30. Conclude that ξ is indeed a homomorphism.
31. From the above exercises about the identity mapping i ∈M and its algebraic
formulation, conclude that the kernel of ξ, ξ−1(i), contains only nonzero scalar
multiples of the 2 × 2 identity matrix I ∈G.
32. Furthermore, show that ξ−1(i) contains all nonzero scalar multiples of I ∈G.
33. Conclude that ξ−1(i) is the same as the center of G, calculated above.
34. Show that the upper triangular matrices make a subgroup in Go.
35. Similarly, show that the lower triangular matrices make a subgroup in Go.
36. Let n be some natural number. Consider the set Gn, containing the nonsingular
n × n complex matrices. Show that In, the identity matrix of order n, is the unit
element in Gn.
37. Show that Gn is indeed a group.
38. Show that the diagonal matrices make a subgroup in Gn.
39. Show that the center of Gn is
Cn ≡{zIn | z ∈C, z ̸= 0} .
40. Verify that Cn is indeed a group in its own right.
41. Conclude that Cn is indeed a legitimate subgroup of Gn.
42. Show that Gn acts on the set Cn \ {0} (the n-dimensional space, without the
origin).
43. Conclude that Gn can be interpreted as a group of vector functions deﬁned on
Cn \ {0}, with composition playing the role of the algebraic operation.
44. Show that the factor group Gn/Cn is indeed a group.
45. Show that Gn/Cn acts on (Cn \ {0}) /Cn.
46. Conclude that Gn/Cn can be interpreted as a group of functions deﬁned in
(Cn \ {0}) /Cn, with composition playing the role of the algebraic operation.
47. Show that the upper triangular matrices make a subgroup in Gn.
48. Similarly, show that the lower triangular matrices make a subgroup in Gn.
49. Use the discussions in Sects.5.9.1–5.9.4 to study the convergence of periodic
continued fractions. Let j be a ﬁxed natural number (the period). Assume that
the coefﬁcients in the original continued fraction are periodic:
ak+ j = ak and bk+ j = bk,
k ≥1.
Find algebraic conditions on the eigenvalues of the matrix product
g1g2 · · · g j
that guarantee convergence to the continued fraction f . The solution can be
found in [62].

Chapter 6
Projective Geometry with Applications
in Computer Graphics
What is a geometrical object? It is something that we humans could imagine and
visualize. Still, in Euclidean geometry, a geometrical object is never deﬁned explic-
itly, but only implicitly, in terms of relations, axioms, and logic ([22] and Chap.6 in
[63]).
For example, a point may lie on a line, and a line may pass through a point. Still,
a line is not just a collection of points. It is much more than that: an independent
object, which may contain a given point, or not.
This way, Euclidean geometry uses no geometrical intuition, but only logic. After
all, logic is far more reliable than the human eye.
Still, logic doesn’t give us sufﬁcient order or method. For this purpose, linear
algebra is far better suited. How to use it in geometry?
The answer is in analytic geometry: the missing link between geometry and alge-
bra. For this purpose, we introduce a new axis system. This way, a line is no longer
an independent object, but a set of points that satisfy a linear equation.
This way, points are the only low-level bricks. Lines, angles, and circles, on
the other hand, are high-level objects, built of points. Since these points satisfy an
algebraic equation, it is much easier to prove theorems.
In projective geometry, on the other hand, we move another step forward: we use
not only analytic geometry but also group representation and topology [15, 79]. For
this purpose, we use the isomorphism theorems proved above. This way, we have a
complete symmetry between points and lines, viewed as algebraic (nongeometrical)
objects: a line may now be interpreted as a point, and a point may be interpreted as
a line.
In the projective plane, the original axioms in Euclidean geometry take a much
more symmetric form. Just as every two distinct points make a unique line, every
two distinct lines meet at a unique point. (In particular, two parallel lines meet at an
inﬁnity point.) After all, as pure algebraic objects, points and lines mirror each other,
so their roles may interchange.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_6
187

188
6
Projective Geometry with Applications in Computer Graphics
Similarly, in the projective space, there is a complete symmetry between points
and planes. Just as every three independent points make a unique plane, every three
independent planes meet at a unique point. As a result, the roles of point and plane
may interchange: a plane may be viewed as a single point, whereas a point may
be viewed as a complete plane. After all, both can be interpreted as pure algebraic
objects, free of any geometrical intuition.
In this chapter, we use group theory to introduce the ﬁeld of projective geometry.
In particular, we use matrix–vector and matrix–matrix multiplication to form a group
of mappings of the original projective plane or space.
To introduce group theory, we focused on an individual transformation, or on the
matrix that represents it. Here, on the other hand, we focus on the original geometrical
object (points, lines, and vectors), rather than the mapping that acts upon it. This
approach is particularly useful in computer graphics [54, 74].
6.1
Circles and Spheres
6.1.1
Degenerate “Circle”
We start with some preliminary deﬁnitions, which will be useful later. In particular,
we deﬁne circles, spheres, and hyperspheres in higher dimensions. For this purpose,
we must start from a degenerate zero-dimensional “circle”.
Consider the one-dimensional real axis. Consider two points on it: −1 and 1.
They are antipodal (or opposite) points: placed symmetrically at opposite sides of 0.
Together, they make a new set:
S0 ≡{−1, 1}.
Note that this notation has nothing to do with the subgroup S in Chap.5.
In what sense is S0 a “circle?” Well, it contains those real numbers of absolute
value 1. On the real axis, there are just two such points: −1 and 1. In this sense, the
diameter of S0 is just the line segment leading from −1 to 1.
Now, let’s also introduce an orthogonal axis: the y-axis. This forms the two-
dimensional Cartesian plane. In this plane, the original one-dimensional real axis is
embedded into the horizontal x-axis (Fig.1.6). In particular, the original antipodal
points −1 and 1 embed into a new pair of antipodal points: (−1, 0) and (1, 0), on
the new x-axis. This is the embedded S0.
The original diameter also embeds just the new line segment leading from (−1, 0)
to (1, 0) on the new x-axis. Next, we’ll use this embedded diameter to produce a
more genuine circle.

6.1 Circles and Spheres
189
Fig. 6.1
Antipodal points
on the unit circle
x
y
6.1.2
Antipodal Points in the Unit Circle
In the Cartesian plane, the embedded diameter can now rotate counterclockwise,
making a larger and larger angle θ with the positive part of the x-axis. For each
angle 0 < θ < π, this makes a new pair of antipodal points, placed symmetrically
at opposite sides of (0, 0):
(cos(θ), sin(θ)) from above, and (−cos(θ), −sin(θ)) from below.
(Fig.6.1). This way, the original point (1, 0) draws the upper semicircle. Its antipodal
counterpart (−1, 0), on the other hand, draws the lower semicircle. Together, they
draw the entire unit circle:
S1 ≡

(x, y) ∈R2 | x2 + y2 = 1

.
6.1.3
More Circles
The unit circle S1 can now shift to just any place in the Cartesian plane, to form a
new circle.
In analytic geometry, a circle is characterized by two parameters: a point O ≡
(xo, yo) to mark its center, and a positive number r > 0 to stand for its radius
(Fig.6.2). From Pythagoras’ theorem, the circle contains those points of distance r
from O:

(x, y) ∈R2 | (x −xo)2 + (y −yo)2 = r2
.
This is indeed a link to algebra: the circle is no longer a low-level abstract object as
in Euclidean geometry, but rather a set of points that satisfy an algebraic equation.

190
6
Projective Geometry with Applications in Computer Graphics
Fig. 6.2
A circle centered
at O ≡(xo, yo)
x
y
xo
yo
6.1.4
Antipodal Points in the Unit Sphere
What is a diameter in S1? It is a line segment connecting two antipodal points:
[(−cos(θ), −sin(θ)), (cos(θ), sin(θ))].
Now,let’sintroduceyetanotheraxis:thevertical z-axis,toformthethree-dimensional
Cartesian space. This way, the original unit circle embeds right into the horizontal
x-y-plane, for which z = 0:
S1 × {0} =

(x, y, 0) ∈R3 | x2 + y2 = 1

.
In particular, the above diameter also embeds into
[(−cos(θ), −sin(θ), 0), (cos(θ), sin(θ), 0)].
Let’s go ahead and rotate it at angle φ, upwards into the new z dimension. For each
angle 0 < φ < π, this makes a new pair of antipodal points:
(cos(θ) cos(φ), sin(θ) cos(φ), sin(φ))
(which draws the upper semicircle) and
(−cos(θ) cos(φ), −sin(θ) cos(φ), −sin(φ))
(which draws the lower semicircle).
This can be done for each and every pair of antipodal points in the embedded S1,
characterized by some θ. Once this is done for all 0 ≤θ < π, the upper semicircles
make the upper hemisphere, and the lower semicircles make the lower hemisphere.
Together, we have the entire unit sphere:
S2 ≡

(x, y, z) ∈R3 | x2 + y2 + z2 = 1

.

6.1 Circles and Spheres
191
6.1.5
General Multidimensional Hypersphere
The above procedure may repeat for higher and higher dimensions as well. By math-
ematical induction on n = 1, 2, 3, . . ., we obtain the general hypersphere:
Sn−1 ≡

(v1, v2, . . . , vn) ∈Rn | v2
1 + v2
2 + · · · + v2
n = 1

.
Note that this is just a notation: the superscript n −1 is not a power, although it has
something to do with power: it reﬂects the fact that the above procedure has been
iterated n −1 times, to form S1, S2, . . . , Sn−1.
For instance, by setting n = 4, we obtain the hypersphere
S3 ≡

(x, y, z, w) ∈R4 | x2 + y2 + z2 + w2 = 1

.
6.1.6
Complex Coordinates
To visualize the above hypersphere geometrically, let’s introduce the new complex
coordinates c1 and c2. The original real coordinates x and y will then serve as real
and imaginary parts in c1. The third and fourth real coordinates, z and w, on the other
hand, will serve as real and imaginary parts in c2.
The original hypersphere S3 can now be deﬁned in terms of the new complex
coordinates:
S3 ≡

(c1, c2) ∈C2 | |c1|2 + |c2|2 = 1

.
To illustrate, let’s deﬁne r ≡|c1|: the radius of some circle in the c1-plane, around
the origin x = y = 0 (Fig.6.3). The complementary circle of radius |c2| = √1−r2,
on the other hand, can then be drawn in the c2-plane, around the origin z = w = 0
(Fig.6.4).
Now, let’s pick one point from the former c1-circle, and another point from the
latter c2-circle (Fig.6.5). Together, they form a new four-dimensional point (c1, c2) =
(x, y, z, w) ∈S3:
Fig. 6.3 The ﬁrst complex
coordinate c1 ≡x + y√−1.
The circle contains complex
numbers c1 with
|c1|2 = x2 + y2 = r2, for a
ﬁxed radius r ≥0
x
y

192
6
Projective Geometry with Applications in Computer Graphics
Fig. 6.4 The second
complex coordinate
c2 ≡z + w√−1. The circle
contains complex numbers
c2 with
|c2|2 = z2 + w2 = 1 −r2,
where r ≤1 is the radius of
the former circle: the
c1-circle above
z
w
Fig. 6.5 The new |c1|-|c2|
plane, where
c1 ≡x + y√−1 and
c2 ≡z + w√−1 are formed
from the original
(x, y, z, w) ∈R4. The arc
contains those points for
which |c1|2 + |c2|2 = 1,
including those points in the
c1- and c2-circles above
|c1|
|c2|
r
x2 + y2 + z2 + w2 = |c1|2 + |c2|2 = r2 + 1 −r2 = 1.
How does the c1-circle in Fig.6.3 relate geometrically to the c2-circle in Figs.6.4?
This is illustrated in the two-dimensional (|c1|, |c2|)-plane (Fig.6.5). This completes
the missing link between c1 and c2: one just needs to pick a point from the arc
|c1|2 + |c2|2 = 1.
6.2
The Complex Projective Plane
6.2.1
The Complex Projective Plane
We start from the easy case: the complex projective plane. Actually, we’ve already
met it in Chap.5, Sect.5.8.7. Here, however, we put it in a wider geometrical context.
Recall the set of nonzero two-dimensional complex vectors:
V ≡C2 \ {(0, 0)}

6.2 The Complex Projective Plane
193
(Chap.5, Sects.5.8.2–5.8.4). Recall that it splits into disjoint planes of the form
C
 c1
c2

≡

z
 c1
c2

| z ∈C, z ̸= 0

,
where c1 and c2 are some complex parameters that do not both vanish at the same
time (Fig.5.8).
How to visualize such a complex plane like? Well, for instance, by setting c2 = 0,
we have the horizontal complex plane in Fig.5.9. By setting c1 = 0, on the other
hand, we have the vertical complex plane
{(0, z) | z ∈C, z ̸= 0} .
Thus, the complex projective plane is just the family of such planes, each considered
as an individual element:
V/C ≡{Cv | v ∈V } .
In this set, there is no duplication: each element of the form Cv appears only once,
with v being some representative picked arbitrarily from it.
This is no group: there is no algebraic operation. After all, V was never a group
in the ﬁrst place. So, there is no point to talk about homomorphism. Still, there is a
point to talk about homeomorphism, to visualize how V/C looks like topologically,
or how continuous it may be.
6.2.2
Topological Homeomorphism onto the Sphere
In Chap.5, Sect.5.8.7, we have already seen that the complex projective plane is
topologically homeomorphic to the extended complex plane: both have the same
continuity properties. Furthermore, the extended complex plane is topologically
homeomorphic to the sphere. Thus, in summary, the complex projective plane is
topologically homeomorphic to the sphere:
V/C ≃C ∪{∞} ≃S2.
Here, “≃” means topological homeomorphism (an invertible mapping that preserves
continuity), not algebraic isomorphism. After all, these are just sets, not groups,
so there is no algebraic operation to preserve. Below, we’ll extend this to higher
dimensions as well.

194
6
Projective Geometry with Applications in Computer Graphics
6.2.3
The Center and Its Subgroups
In Sect.6.1.5, we’ve deﬁned the general hypersphere Sn−1 ⊂Rn. Note that this
notation has nothing to do with the subgroup S in Chap.5.
Now, let’s use G: the group of 2 × 2 nonsingular complex matrices (Chap.5,
Sect.5.8.1). In G, the unit element is just the 2 × 2 identity matrix I. Furthermore,
the center C ⊂G contains the nonzero scalar multiples of I. (See exercises at the
end of Chap.5).
Let’s write C as the product of two subgroups. For this purpose, let H ⊂C
contain the positive multiples of I:
H ≡{r I | r ∈R, r > 0} .
It is easy to see that H is indeed a group in its own right.
Now, let us deﬁne yet another subgroup U ⊂C:
U ≡{zI | z ∈C, |z| = 1} .
Using the unit circle S1 (Sect.6.1.2), this can be written more concisely as
U = S1I.
It is easy to see that U is indeed a group in its own right.
6.2.4
Group Product
What is the product of U and H? Well, it contains those products of an element from
U with an element from H:
U H ≡{uh | u ∈U, h ∈H}
(Chap.5, Sect.5.10.1).
The original group G isn’t commutative: two matrices not necessarily commute
with each other. Fortunately, its center C is. For this reason,
uh = hu,
uh ∈U H.
This implies that
U H = HU.
Moreover, it also implies that U H is indeed a group in its own right.

6.2 The Complex Projective Plane
195
6.2.5
The Center—A Group Product
So, U H is a subgroup of G. Is it a subgroup of C as well? Well, to check on this,
let’s pick an element from U H:
wIr I = (wr)I ∈C
(w ∈C, |w| = 1, r ∈R, r > 0).
This shows that
U H ⊂C.
Conversely, is C a subgroup of U H? Well, to check on this, let’s pick an element
from C: a nonzero complex multiple of I. Fortunately, each nonzero complex number
z ∈C has the polar decomposition
z = |z| exp(θ
√
−1),
where
θ ≡arg(z)
is the angle that z makes with the positive part of the real axis. (See exercises at the
end of Chap.8.) Thus,
zI = exp(θ
√
−1)I|z|I ∈U H
(z ∈C, z ̸= 0),
as required. This implies that
C ⊂U H.
In summary, we have
C = U H.
Let’s use this decomposition to visualize the complex projective plane geometrically.
6.2.6
How to Divide by a Product?
Thanks to the above factorization, each individual vector v ∈V spans the complex
plane
Cv = (U H)v = U(Hv),
where Hv is just a “ray”:
Hv ≡{hv | h ∈H} = {rv | r ∈R, r > 0} ,

196
6
Projective Geometry with Applications in Computer Graphics
and U(Hv) is not just one ray but a complete fan of rays, making a complete complex
plane:
U(Hv) ≡{u(hv) | h ∈H, u ∈U}
=

exp(θ
√
−1)rv | r, θ ∈R, r > 0, 0 ≤θ < 2π
	
.
Together, all such planes make the complex projective plane:
V/C =
V
U H ≡{(U H)v | v ∈V } = {U(Hv) | Hv ∈V/H} = V/H
U
.
In the above, there is no duplication: each equivalence class in V/H is represented,
say, by a unique unit vector in S3:
Hv = H
 v
∥v∥

.
Thus, V/H is mirrored by the hypersphere S3 in Sect.6.1.5. Since U is also mirrored
by S1, we have
V/C ≃S3/S1.
Here, “≃” stands for topological homeomorphism (an invertible mapping that pre-
serves continuity), and S3/S1 contains equivalence circles in the hypersphere S3.
Let’s see how this looks like geometrically.
6.2.7
How to Divide by a Circle?
How to “divide” by S1? Well, shrink each equivalence circle into a single point that
lies in it. In Chap.5, Sect.5.8.7, this is done algebraically: divide by the second
complex coordinate, c2. Here, on the other hand, this is done geometrically.
Fortunately, the equivalence circle has already shrunk with respect to |c2|. All that
is left to do is to shrink it with respect to the angle arg(c2) as well. For this purpose,
the circle in Fig.6.4 has to shrink to just one point on it. This way, the second complex
coordinate, c2, reduces to the nonnegative real coordinate |c2|. This produces the top
hemisphere in the three-dimensional (x, y, |c2|)-space:

(x, y, |c2|) ∈R3 | |c2| ≥0, x2 + y2 + |c2|2 = 1

.
What do we have at the bottom of this hemisphere? Well, this is the equator:
x2 + y2 = 1,
or c2 = 0.

6.2 The Complex Projective Plane
197
In the arc in Fig.6.5, this is the lower endpoint: r = |c1| = 1.
At the equator, we can no longer divide by c2 = 0. Instead, we must divide by
c1. More precisely, it is only left to divide by arg(c1). This shrinks the entire equator
into the single point C(1, 0)t: the inﬁnity point in the complex projective plane. This
“closes” the hemisphere from below, at the unique inﬁnity point that contains the
entire (shrunk) equator.
What is this topologically? We already know it well: this is just the sphere in
Sect.6.1.4! In summary, we have the topological homeomorphism
V/C ≃S3/S1 ≃S2,
in agreement with Sect.6.2.2.
6.2.8
Second and Third Isomorphism Theorems
In the above, we dealt with sets, not groups. This is why “≃” meant just topological
homeomorphism, not algebraic isomorphism. After all, there is no algebraic opera-
tion to preserve. In this section, on the other hand, we deal with groups once again.
In this context, “≃” means not only topological homeomorphism but also algebraic
isomorphism.
To understand its inner structure, we better reconstruct the original factor group
G/C more patiently, in two stages. First, deﬁne G/H, whose elements are of the
form
Hg ≡{rg | r ∈R, r > 0} .
What is Hg? Well, it has two possible interpretations. In G/H, it is an individ-
ual element. In G, on the other hand, it is a subset: an equivalence class (induced
by the subgroup H ⊂G), which may contain many elements. Fortunately, these
interpretations mirror each other.
What is the center of G/H? Well, it is just
C/H ⊂G/H
(see exercises below). Its elements are of the form
H exp(θ
√
−1),
θ ∈R, 0 ≤θ < 2π.
Now, let’s go ahead and divide by this center, to have the factor group of factor groups:
(G/H)/(C/H). What is a typical element in it? Well, take the factor group C/H in
the denominator, and use it to multiply a representative from the factor group G/H
in the numerator. This produces an individual element in (G/H)/(C/H), which can
also be viewed as a subset of G/H: a complete equivalence class in G/H, induced
by C/H:

198
6
Projective Geometry with Applications in Computer Graphics
(C/H)Hg =

H exp(θ
√
−1)Hg | θ ∈R, 0 ≤θ < 2π
	
=

H

exp(θ
√
−1)g

| θ ∈R, 0 ≤θ < 2π
	
⊂G/H
(Chap.5, Sects.5.7.3 and 5.8.5).
So, we also have a mirroring between an individual element in (G/H)/(C/H)
and a complete equivalence class in G:
(C/H)Hg ↔

exp(θ
√
−1)rg | r, θ ∈R, r > 0, 0 ≤θ < 2π
	
= Cg ∈G/C.
This makes a new isomorphism:
G/H
C/H ≃G/C.
But we already know this formula well: this is just the third isomorphism theorem
(Chap.5, Sect.5.10.2).
Furthermore, in our case, we know quite well how C/H looks like
C/H ≃U.
As a matter of fact, this is just a special case of the second isomorphism theorem.
Indeed, in Chap.5, Sect.5.10.1, just substitute T ←U and S ←H, and note that
they have just one element in common: the unit element.
Combining these results, we can write the above less formally:
G/C ≃G/H
U
.
Thus, we got what we wanted. To divide by C, one could use two stages: ﬁrst, divide
by H; then, divide by U as well. After all, C factorizes as C = U H. This way, a
typical element Cg ∈G/C is mirrored by U(Hg).
How does Cg act on the plane Cv ∈V/C? Well, this action can now factorize as
well:
Cg(Cv) = U(Hg)U(Hv) = U(HgHv) = U(H(gv)) = (U H)gv = Cgv,
as required. This is a rather informal writing style. After all, as a subgroup of G,
U could act on an individual element in V or G, but not in V/H or G/H. On the
latter, what should act is C/H, not U. Still, as we’ve seen above, this is essentially
the same.

6.2 The Complex Projective Plane
199
All these algebraic games are very nice, but give little geometrical intuition. For
this purpose, it is sometimes better to drop complex numbers altogether, and stick to
good old real numbers. Let’s start from the simplest case.
6.3
The Real Projective Line
6.3.1
The Real Projective Line
What is the real projective line? First, redeﬁne V to contain real vectors only:
V ≡R2 \ {(0, 0)}.
Furthermore, redeﬁne G to contain real matrices only, This way, its center C contains
only real multiples of I:
C ≡{x I | x ∈R, x ̸= 0} = (R \ {0}) I.
This way, we have
V/C = R2 \ {(0, 0)}
(R \ {0}) I .
This is the real projective line. Why line? Because, in the Cartesian plane, it can be
modeled by the horizontal line y ≡1: each (oblique) line of the form Cv ∈V/C
meets this horizontal line at one point exactly. This is indeed the oblique cotangent
projection (Fig.5.9).
There is just one exception: the x-axis doesn’t meet the above horizontal line at
all, so it must map to ∞. Fortunately, there is a more uniform way to model the
real projective line geometrically. For this purpose, we must use some algebra once
again.
What are the subgroups of the new center C? Well, once conﬁned to real numbers
only, U is redeﬁned to contain two elements only: I and −I:
U ≡{x I | x ∈R, |x| = 1} = ±I = S0I
(Sect.6.1.1). The second subgroup H, on the other hand, remains the same as before.
Thus, each ray of the form Hv ∈V/H is spanned by a unique unit vector in S1:
Hv = H
 v
∥v∥

.
This can be done for every v ∈V . Together, we have a fan of rays, each represented
by a unique point, at which it meets the circle:

200
6
Projective Geometry with Applications in Computer Graphics
V/H =

H
 x
y

| x, y ∈R, x2 + y2 = 1

≃S1
(Sect.6.1.2). Hereafter, “≃” means topological homeomorphism only, not algebraic
isomorphism. After all, on the left, we have just a set, not a group, so there is no
algebraic operation to preserve.
In summary, the real projective line takes the form
V/C =
V
U H = V/H
U
≃S1/S0.
This is the divided circle.
6.3.2
The Divided Circle
As discussed above, the real projective line is associated with the divided circle:
V/C ≃S1/S0.
How to visualize this geometrically? Well, this is illustrated in Fig.6.6. Each line
of the form Cv (for some v = (x, y)t ∈V ) meets the unit circle at two antipodal
points: v/∥v∥and −v/∥v∥. Fortunately, in the divided circle, they are just one and
the same point.
What happens in the horizontal line v = (1, 0)t? Well, in this case, Cv is just
the x-axis: the inﬁnity object in the real projective line. Indeed, in the cotangent
projection in Fig.5.9, this line maps to ∞. Fortunately, in the divided circle, this line
is mirrored well by the pair (±1, 0) (Fig.6.6).
How to visualize the divided circle geometrically? Well, take the original unit
circle in Fig.6.1, and consider each pair of antipodal points as just one point. This
is like taking just the upper semicircle (Fig.6.7). The lower semicircle, on the other
Fig. 6.6 The line Cv meets
the unit circle at two
antipodal points: ±v/∥v∥.
Fortunately, in the divided
circle, they coincide with
each other. For example, the
horizontal x-axis, C(1, 0)t,
is represented by the pair
(±1, 0)
Cv
1
−1
−v/∥v∥
v/∥v∥
the x-axis
v

6.3 The Real Projective Line
201
Cv
1
−1
v/∥v∥
the x-axis
v
Fig. 6.7 The top semicircle is enough: each line of the form Cv is represented by the unique point
v/∥v∥. There is just one exception: the horizontal x-axis C(1, 0)t is still represented by the pair
(±1, 0). In the divided circle, these points are considered as one and the same. Topologically, this
“closes” the semicircle from below, producing a circle
hand, could drop. After all, each point on it is no longer necessary: it is already
mirrored by its upper counterpart.
Or is it? Well, there is just one exception: the points (±1, 0) are both needed,
and shouldn’t drop. Instead, they should unite into just one point. Topologically, this
“closes” the semicircle at the bottom, producing a closed circle:
V/C ≃S1/S0 ≃S1.
We are now ready to move on to higher dimensions as well.
6.4
The Real Projective Plane
6.4.1
The Real Projective Plane
Let’s move on to a yet higher dimension. For this purpose, redeﬁne V as a three-
dimensional vector set:
V ≡R3 \ {(0, 0, 0)}.
This way, a vector in V is speciﬁed by three real degrees of freedom: its ﬁrst, second,
and third coordinates. The projective plane V/C, on the other hand, gives away one
degree of freedom: the unspeciﬁed scalar multiple. This is why V/C depends on two
degree of freedom only, and is referred to as a plane.
Furthermore, G is also redeﬁned as the group of 3 × 3 nonsingular real matrices.
The unit element in G is now the 3 × 3 identity matrix:
I ≡
⎛
⎝
1 0 0
0 1 0
0 0 1
⎞
⎠.

202
6
Projective Geometry with Applications in Computer Graphics
The subgroups C, U, and H are also redeﬁned to use this new I:
C ≡{x I | x ∈R, x ̸= 0} = (R \ {0}) I
U ≡{x I | x ∈R, |x| = 1} = ±I = S0I
H ≡{x I | x ∈R, x > 0} .
With these new deﬁnitions, V/C is called the real projective plane. Why plane?
Because, in the Cartesian space, it can be modeled by the horizontal plane z ≡1:
each oblique line of the form Cv ∈V/C meets this plane at one point exactly.
The real projective plane has an important advantage: it has not just one inﬁnity
point, but many inﬁnity points from all directions.
6.4.2
Oblique Projection
To get a better idea about V/C, project it onto the horizontal plane
{z ≡1} = {(x, y, 1) | x, y ∈R} .
(Note that, unlike in the complex case, here z stands for a real coordinate.) More
precisely, each line of the form C(x, y, z)t (with z ̸= 0) projects onto the unique
point at which it meets the above horizontal plane:
C
⎛
⎝
x
y
z
⎞
⎠→
x
z , y
z , 1

.
In Fig.6.8, this horizontal plane is viewed from an eye or a “camera” placed at the
origin (0, 0, 0), faced upwards. Through this camera, one could see the semispace

(x, y, z) ∈R3 | z > 0

.
Fig. 6.8 Oblique projection
onto the horizontal plane
z ≡1. Each line of the form
C(x, y, z)t (z ̸= 0) projects
onto (x/z, y/z, 1)
z
v
y
y
x
x
z ≡1

6.4 The Real Projective Plane
203
More precisely, because one could only see a two-dimensional image, one sees the
oblique projection onto the horizontal plane

(x, y, z) ∈R3 | z = 1

.
And what about z = 0? Well, in this case, the projection must be radial:
C
⎛
⎝
x
y
0
⎞
⎠→±
1

x2 + y2 (x, y, 0).
In summary, the entire projection is deﬁned by
C
⎛
⎝
x
y
z
⎞
⎠→
⎧
⎪⎨
⎪⎩
 x
z , y
z , 1

if z ̸= 0
± (x,y,0)
√
x2+y2 if z = 0.
Next, let’s introduce a more uniform projection.
6.4.3
Radial Projection
Alternatively, one might want to use a more uniform approach: always use a radial
projection, regardless of whether z = 0 or not:
C
⎛
⎝
x
y
z
⎞
⎠→±
1

x2 + y2 + z2 (x, y, z)
(Fig.6.9). Let’s see what we obtain.
Fig. 6.9 Radial projection:
each line of the form Cv
projects onto the pair of
antipodal unit vectors
±v/∥v∥in the sphere S2
z
v
y
x

204
6
Projective Geometry with Applications in Computer Graphics
6.4.4
The Divided Sphere
Fortunately, we already have the decomposition
C = U H.
Now, each ray of the form Hv ∈V/H can also be represented by the unique vector
v/∥v∥in the unit sphere S2. Thus,
V/C =
V
U H = V/H
U
≃S2/S0.
This is the divided sphere: the family of pairs of antipodal points in the original unit
sphere. In this family, each pair of antipodal points is viewed as an individual object
in its own right.
6.4.5
Inﬁnity Points
Let’s consider once again the oblique projection in Sect.6.4.2. It is not quite uniform:
it distinguishes between ordinary “points” and inﬁnity “points”, where a “point”
means a complete line of the form Cv ∈V/C.
What is an inﬁnity point in the real projective plane? It is just a horizontal line of
the form C(x, y, 0)t, for some real numbers x and y that do not both vanish at the
same time (Fig.6.10). Unfortunately, the zero z-coordinate can no longer be used to
divide, or normalize, or project to the horizontal plane
{z ≡1} = {(x, y, 1) | x, y ∈R}
as in Fig.6.8.
Fortunately, the unit sphere S2 is much more symmetric than the above plane.
Therefore, the horizontal line C(x, y, 0)t can still project to S2, as in Fig.6.10. In
both the oblique and the radial projections, this makes a pair of antipodal inﬁnity
points of the form
Fig. 6.10
What is an
inﬁnity point in the real
projective plane? It is a
horizontal line of the form
C(x, y, 0)t, projected onto
the antipodal points
±(x, y, 0)/

x2 + y2 in the
inﬁnity circle
z
(x, y, 0)
y
x

6.4 The Real Projective Plane
205
± (x, y, 0)

x2 + y2 .
Together, they make the inﬁnity circle.
6.4.6
The Inﬁnity Circle
Together, these inﬁnity points make the inﬁnity circle:

(x, y, 0) | x, y ∈R, x2 + y2 = 1

= S2 ∩{(x, y, 0) | x, y ∈R} ≃S1.
This circle is just the equator in the original unit sphere. Fortunately, unlike in the
complex projective plane in Sect.6.2.7, it no longer shrinks into a single inﬁnity point.
On the contrary: it contains many useful inﬁnity points, in all horizontal directions.
6.4.7
Lines as Level Sets
So far, we’ve used a vector of the form
v ≡
⎛
⎝
v1
v2
v3
⎞
⎠∈V
to stand for a particular point in V . Fortunately, this is not the only option: v could
also make a complete plane in V .
Indeed, each real linear function
f : V →R
could also be deﬁned in terms of real inner product with a ﬁxed vector v ∈V :
f (x, y, z) ≡fv(x, y, z) ≡(x, y, z)v = xv1 + yv2 + zv3.
The vector v is also called the gradient of f , denoted by
∇fv =
⎛
⎝
v1
v2
v3
⎞
⎠= v.
Because fv is linear, it must indeed have a constant gradient (Chap.8, Sect.8.9.2).

206
6
Projective Geometry with Applications in Computer Graphics
Now, let r be a ﬁxed real number. What is the rth level set of fv? Well, it is the
“origin” of r under fv: it contains those vectors (x, y, z)t ∈V that fv maps to r:
lv,r ≡f −1
v (r) =

(x, y, z)t ∈V | fv(x, y, z) = r

.
This notation has nothing to do with inverse. In fact, fv may have no inverse at all.
After all, a level set may contain a few points (Chap.5, Sect.5.4.1).
In particular, what is the zero level set? Well, it contains those vectors that are
orthogonal to v. Together, they make a complete plane, orthogonal to v:
lv,0 ≡f −1
v (0) =

(x, y, z)t ∈V | fv(x, y, z) = (x, y, z)v = 0

.
In a linear function as above, the gradient is a constant vector. In a more general
function, on the other hand, the gradient may change from point to point, and the
level set may be curved. Fortunately, at each point in it, the gradient is still normal
(perpendicular) to it in a new sense: normal to the plane tangent to it.
Note that, if some vector is in lv,0, then so is every nonzero scalar multiple of it.
In other words, lv,0 is invariant under C:
Clv,0 = lv,0.
This is how things look like in V . In V/C, on the other hand, this makes a complete
line. To see this, let the plane lv,0 cut the horizontal plane
{z ≡1} = {(x, y, 1) | x, y ∈R} .
This produces a line: the “shadow” of lv,0 on this horizontal plane.
Note also that, for every c ∈C,
lcv,0 = lv,0.
Thus, lv,0 could be deﬁned not only by the original vector v ∈V but also by every
nonzero scalar multiple of v.
Thus, in V/C, lv,0 could be deﬁned in terms of the entire line Cv ∈V/C rather
than the concrete vector v ∈V . Geometrically, this (oblique) line is represented by a
unique point: the point at which it meets the horizontal plane z ≡1. This is the start
of duality: in the real projective plane, a point is also a line, and a line is also a point.

6.5 Inﬁnity Points and Line
207
6.5
Inﬁnity Points and Line
6.5.1
Inﬁnity Points and Their Projection
In particular, lv,0 contains the point
(−v2, v1, 0),
and every nonzero scalar multiple of it. As in Fig.6.10, this point can project radially
onto a pair of antipodal points in the inﬁnity circle:
±
1

v2
1 + v2
2
(−v2, v1, 0).
As discussed above, lv,0 also makes a line: its shadow on (or intersection with) the
horizontal plane z ≡1. Thus, in the real projective plane, the original plane lv,0 ⊂V
is interpreted geometrically as a new extended line: the shadow on the horizontal
plane z ≡1, plus its “endpoints:” the above pair of antipodal points on the inﬁnity
circle.
By doing this for every v ∈V , the entire real projective plane is represented
geometrically as a fan of inﬁnite lines on the horizontal plane z ≡1, each extended
by a pair of “endpoints”. In summary, the entire real projective plane has projected
onto the horizontal plane z ≡1, surrounded by the inﬁnity circle. This is in agreement
with the original oblique projection in Sect.6.4.2.
6.5.2
Riemannian Geometry
In the radial projection in Sect.6.4.3, on the other hand, the entire zero-level set lv,0
projects radially only, to cut the original unit sphere at a great circle, centered at the
origin (0, 0, 0). Fortunately, in Riemannian geometry, such a circle is considered as
a line. This way, lines are no longer linear in the usual sense, but rather circular.
Furthermore, each point on a great circle coincides with its antipodal counterpart on
the other side.
This way, the divided sphere S2/S0S mirrors the horizontal plane z ≡1. Each
line on the latter can now extend to a complete plane that passes through the origin,
and cuts the sphere at a great circle. By doing this for two lines that cross each other
at a unique point, we obtain two great circle that meet each other at two antipodal
points, considered as one. Moreover, by doing this for two parallel lines that “meet”
each other at an inﬁnity point, we obtain two great circles that meet each other at
two antipodal points on the equator.

208
6
Projective Geometry with Applications in Computer Graphics
6.5.3
A Joint Inﬁnity Point
For example, consider the new vector
v′ ≡
⎛
⎝
v1
v2
v′
3
⎞
⎠∈V
that differs from the original vector v in the z-coordinate only:
v′
3 ̸= v3.
Clearly, the oblique projections of the zero-level sets lv,0 and lv′,0 make two parallel
lines on the horizontal plane z ≡1. Where do they “meet” each other? To ﬁnd out,
we must employ the radial projection, to obtain two great circles, which meet each
other at two antipodal points on the equator:
±
1

v2
1 + v2
2
(−v2, v1, 0),
which are considered as one.
Assume now that we’re given two lines in the real projective plane. Where do
they meet? To ﬁnd out, let’s introduce an easy algebraic method. This will show
once again that the joint point is indeed unique.
6.5.4
Two Lines Share a Unique Point
Unlike in Euclidean or analytic geometry, here, in projective geometry, every two
distinct lines meet each other at a unique point. This is true not only in Riemannian
geometry, but also in the original oblique projection (Sect.6.4.2).
What is this joint point? To ﬁnd out, consider two independent vectors v, v′ ∈V ,
which are not a scalar multiple of each other. The corresponding zero-level sets, lv,0
and lv′,0, make two distinct planes in V , In the real projective plane V/C, on the
other hand, they are considered as two distinct lines. After all, in the horizontal plane
z ≡1, they cut two lines: their shadow.
What is their intersection in V ? Fortunately, this is available in terms of vector
product:
lv,0 ∩lv′,0 = C(v × v′).
After all, v × v′ is orthogonal to both v and v′ (Chap.2, Sect.2.2.4).

6.5 Inﬁnity Points and Line
209
6.5.5
Parallel Lines Do Meet
Let us study the z-coordinate in v × v′:
(v × v′)3 = v1v′
2 −v2v′
1.
This is zero if and only if (v′
1, v′
2) is a scalar multiple of (v1, v2), as in the example
in Sect.6.5.3. In this case, lv,0 and lv′,0 cut not only the horizontal plane z ≡1 (at
two parallel lines) but also the inﬁnity circle, at two inﬁnity points:
±
1

v2
1 + v2
2
(−v2, v1, 0).
Thus, two parallel lines on the horizontal plane z ≡1 do meet each other at two
antipodal inﬁnity points, making one and the same point on the equator in the divided
sphere.
Thus, in both Riemannian geometry and the real projective plane, there are no
parallel lines any more: every two distinct lines meet each other at a unique point. Is
this true even when one of the lines is the inﬁnity line?
6.5.6
The Inﬁnity Line
How does the inﬁnity line look like? Well, we’ve already seen that an inﬁnity point
has a zero z-coordinate. What vector is orthogonal to all such points? This is just the
standard unit vector
e ≡
⎛
⎝
0
0
1
⎞
⎠∈V.
Indeed, every inﬁnity point must lie in the plane orthogonal to e—the horizontal
plane z ≡0:
le,0 =

(x, y, z)t ∈V | fe(x, y, z) = (x, y, z)e = z = 0

=

(x, y, 0)t ∈V

.
So, in the real projective plane V/C, the inﬁnity line is just the horizontal plane
z ≡0. After all, once projected onto the unit sphere as in Fig.6.10, it makes the
entire inﬁnity circle.
The inﬁnity line also meets every other line at a unique inﬁnity point. Indeed,
every vector v ∈V that is not a scalar multiple of e must have a nonzero component
v1 ̸= 0 or v2 ̸= 0. Therefore, the zero-level sets of v and e intersect each other at the
line

210
6
Projective Geometry with Applications in Computer Graphics
lv,0 ∩le,0 = C(v × e) = C
⎛
⎝
−v2
v1
0
⎞
⎠.
Once projected on the unit sphere, this line makes the inﬁnity point
±
1

v2
1 + v2
2
(−v2, v1, 0).
This is indeed the unique joint point of the original lines le,0 and lv,0 in the real
projective plane.
We’re now ready to see that, in projective geometry, there is a complete symmetry
between points and lines: just as every two distinct lines meet each other at a unique
point, every two distinct points make a unique line. This is proved algebraically: there
is no longer any need to assume a speciﬁc axiom for this, as is done in Euclidean
geometry.
6.5.7
Duality: Two Points Make a Unique Line
In projective geometry, a vector v ∈V may have two different interpretations: either
as the point Cv, or as the line lv,0. Let’s use this duality to form a complete symmetry
between points and lines.
Indeed, as discussed in Sect.6.5.4, the vector product v × v′ produces the unique
joint point of the distinct lines lv,0 and lv′,0. Fortunately, this also works the other
way around: once v and v′ are interpreted as the points Cv and Cv′ in V/C, their
vector product makes the unique line that passes through both of them.
Indeed, since both v and v′ are orthogonal to v × v′, both belong to the zero-level
set of fv×v′:
v, v′ ∈lv×v′,0,
or
Cv, Cv′ ⊂lv×v′,0.
Thus, in projective geometry, vector product can be applied to two distinct objects of
the same kind, to form a new object (of a new kind) that lies in both of them. If the
original objects are interpreted as points, then the new object is the line that passes
through them. If, on the other hand, the original objects are interpreted as lines, then
the new object is the point they share.

6.6 Conics and Envelopes
211
6.6
Conics and Envelopes
6.6.1
Conic as a Level Set
So far, we’ve only studied the sets V and V/C. Now, let us also study the groups G
and G/C that act upon them.
A conic (ellipsoid, hyperboloid, or paraboloid) in V is deﬁned by some symmetric
matrix g ∈G [48]. For this purpose, consider the quadratic function
qg : V →R,
deﬁned by
qg(v) ≡vtgv.
Here, we assume that g is indeﬁnite, so qg may return either positive or negative or
zero value.
For each real number r ∈R, the rth level set of qg (the origin of r under qg) is
denoted by
mg,r ≡q−1
g (r) =

v ∈V | qg(v) = r

.
This notation has nothing to do with inverse. In fact, qg may have no inverse at all.
After all, the level set may contain a few vectors (Chap.5, Sect.5.4.1).
In particular, the zero-level set of qg is
mg,0 = q−1
g (0) =

v ∈V | qg(v) = 0

.
This zero-level set is called a conic in V .
6.6.2
New Axis System
As a real symmetric matrix, g has real eigenvalues and real orthonormal eigenvectors.
(See Chap.1, Sects.1.9.4 and 1.10.4, and exercises therein.) These eigenvectors form
a new (real) axis system in V , which may differ from the standard x-y-z system. In
the new axis system, g is in its diagonal form, with its (real) eigenvalues on the main
diagonal. This is indeed the axis system in which the original conic visualizes best.
Thanks to the algebraic properties of the original matrix g, we have a rather good
geometrical picture. Thanks to symmetry, we have the new axis system. Furthermore,
thanks to indeﬁniteness, the eigenvalues are not of the same sign, so the zero-level
set mg,0 is nonempty. For this reason, in terms of the new axis system, the original
conic must be a hyperboloid.

212
6
Projective Geometry with Applications in Computer Graphics
Fortunately, for every c ∈C,
mcg,0 = mg,0.
Thus, mg,0 can be deﬁned not only by the original matrix g ∈G but also by the
element Cg in the factor group G/C.
6.6.3
The Projected Conic
Clearly, if v ∈mg,0, then every nonzero scalar multiple of v is in mg,0 as well:
Cv ⊂mg,0. For this reason, mg,0 is invariant under C:
Cmg,0 = mg,0.
Thus, mg,0 can be interpreted not only as a conic in V , but also as a lower dimensional
conic in V/C. Once projected on the horizontal plane z ≡1 as in Fig.6.8, the original
conic indeed produces a one-dimensional conic: ellipse, hyperbola, or parabola.
To make this more concrete, assume that a camera is placed at the origin (0, 0, 0),
faced upwards. Through the camera, one could only see the upper part of the original
conic, with z > 0. More precisely, one only sees a two-dimensional image: the
horizontal plane z ≡1, with the conic’s shadow in it: a curve of the form
⎧
⎨
⎩
⎛
⎝
x
y
1
⎞
⎠∈V

(x, y, 1)g
⎛
⎝
x
y
1
⎞
⎠= 0
⎫
⎬
⎭.
This is the projected one-dimensional conic in the horizontal plane z ≡1.
6.6.4
Ellipse, Hyperbola, or Parabola
How does the projected conic look like? Well, this depends on the leading quadratic
terms: x2, xy, and y2 in the original function qg. The coefﬁcients of these terms can
be found in the minor g(3,3): the 2 × 2 upper left block in g (Chap.2, Sect.2.1.1).
Like g, g(3,3) is a real symmetric matrix, with a diagonal form: its real eigenvectors
make a new two-dimensional axis system, which may differ from the standard x-y
system. In this new axis system, the projected conic may indeed visualize best. In
fact, if g(3,3) has a positive determinant:
det

g(3,3)
> 0,

6.6 Conics and Envelopes
213
then its (real) eigenvalues must have the same sign, so the projected conic must be
an ellipse. If, on the other hand,
det

g(3,3)
< 0,
then the eigenvalues must have different signs, so the projected conic must be a hyper-
bola (in terms of the new two-dimensional axis system). Finally, if the determinant
vanishes:
det

g(3,3)
= 0,
then one of the eigenvalues must vanish, so the projected conic must be a parabola
(in terms of the new axis system) in the horizontal plane z ≡1.
6.6.5
Tangent Planes
Let v be some vector in the original conic mg,0. Let us apply g to v, to produce
the new vector gv ∈V . As discussed in Sect.6.4.7, gv deﬁnes the plane lgv,0 that is
orthogonal to gv. Furthermore, becauselgv,0 is invariant under C, it can be interpreted
not only as a plane in V but also as a line in the real projective plane V/C. Moreover,
it can project to a yet more concrete line: its shadow on the horizontal plane z ≡1.
Let us now return to the original conic mg,0 ⊂V . Fortunately, the plane lgv,0 is
tangent to it at v. Indeed, since v ∈mg,0, vtgv = 0, so v ∈lgv,0 as well. Furthermore,
both the conic and the plane have the same normal vector at v. After all, both are
level sets of functions with proportional gradients at v:
∇qg(v) = 2gv = 2∇fgv
(Sect.6.4.7, and Chap.8, Sect.8.9.2). Thus, the mapping
v →gv
maps the original point v ∈mg,0 to a vector that is normal (or perpendicular, or
orthogonal) to both the original conic and the tangent plane at v.
Fortunately, once projected onto the horizontal plane z ≡1, the tangent plane also
produces the line (or shadow) that is tangent to the projected conic at the projected
v.
6.6.6
Envelope
The new vector gv studied above has yet another attractive property: it belongs to
the zero-level set of the quadratic function associated with the inverse matrix g−1:

214
6
Projective Geometry with Applications in Computer Graphics
qg−1(gv) = (gv)tg−1gv = (gv)tv = vtgtv = vtgv = qg(v) = 0,
or
gv ∈mg−1,0.
This can be written more compactly as
gmg,0 ⊂mg−1,0.
Now, let’s substitute g−1 for g:
g−1mg−1,0 ⊂mg,0.
By applying g to both sides, we have
mg−1,0 ⊂gmg,0.
In summary,
gmg,0 = mg−1,0.
In the dual interpretation in Sect.6.5.7, the original tangent plane lgv,0 is viewed as a
mere point: Cgv ∈mg−1,0. In this interpretation, the new conic mg−1,0 makes a new
envelope: a family of planes, all tangent to the original conic.
6.6.7
The Inverse Mapping
What is the envelope of the new conic mg−1,0? To ﬁnd out, we just need to use g−1:
g−1mg−1,0 = mg,0,
which is just the original conic once again.
More speciﬁcally, consider an individual vector of the form gv ∈mg−1,0. The
inverse mapping
gv →g−1gv = v
maps it to the vector v that is normal to the new conic at gv:
∇qg−1(gv) = 2g−1gv = 2v = 2∇fv.

6.7 Duality: Conic–Envelope
215
6.7
Duality: Conic–Envelope
6.7.1
Conic and Its Envelope
Thus, the original roles have interchanged: v is no longer interpreted as a mere point
in the original conic, but rather as a complete plane: lv,0, tangent to the new conic at
gv. The original tangent plane lgv,0, on the other hand, is now interpreted as a mere
point: gv, in the new conic. This is a geometrical observation. Algebraically, it has
already been written most compactly as
g−1mg−1,0 = mg,0.
In summary, projective geometry supports two kinds of duality. In the elementary
level, a line takes the role of a point, whereas a point is viewed as a complete line
(Sect.6.5.7). In the higher level, on the other hand, the original conic is viewed as an
envelope, whereas the original envelope is viewed as a conic.
6.7.2
Hyperboloid and Its Projection
Consider, for example, the special case
g =
⎛
⎝
−1 0 0
0 −1 0
0
0 1
⎞
⎠.
This way, the original conic is the hyperboloid
mg,0 =

v ∈V | vtgv = 0

=

(x, y, z)t ∈V | z2 = x2 + y2
.
In this simple example,
g2 = I,
or
g−1 = g,
so the new conic is the same.
Now, let’s pick some vector v in the conic, say
v ≡
⎛
⎝
v1
v2
v3
⎞
⎠≡
⎛
⎝
−1
1
√
2
⎞
⎠∈mg,0

216
6
Projective Geometry with Applications in Computer Graphics
Fig. 6.11 The hyperboloid
projects onto a circle in the
horizontal plane z ≡1. Each
plane tangent to the original
hyperboloid projects to a line
tangent to the circle and
perpendicular to its radius
projected v
projected lgv,0
projected gv
projected lggv,0 = lv,0
(Fig.6.11). The tangent plane at v is perpendicular to
gv =
⎛
⎝
−v1
−v2
v3
⎞
⎠=
⎛
⎝
1
−1
√
2
⎞
⎠.
More explicitly, the tangent plane at v is
lgv,0 =

(x, y, z)t ∈V | x −y = −z
√
2
	
.
In particular, v itself belongs not only to the conic but also to this plane, as required.
To have a better geometrical understanding, let’s project obliquely, to make a
shadow on the horizontal plane z ≡1. The projected conic is the circle

(x, y, 1)t ∈V | x2 + y2 = 1

.
Furthermore, the projected lgv,0 is the line

(x, y, 1)t ∈V | x −y = −
√
2
	
.
As can be seen in the upper left part of Fig.6.11, this line is indeed tangent to the
circle at the projected v:
v
v3
=
v
√
2
=
⎛
⎝
−1/
√
2
1/
√
2
1
⎞
⎠.
So far, gv has been used only to form the tangent plane lgv,0. Thanks to duality, gv can
also be viewed as a mere point on the new conic, which is just the same hyperboloid:
mg−1,0 = mg,0.

6.7 Duality: Conic–Envelope
217
More explicitly, the vector
gv =
⎛
⎝
1
−1
√
2
⎞
⎠
projects onto the vector
gv
(gv)3
= gv
√
2
=
⎛
⎝
1/
√
2
−1/
√
2
1
⎞
⎠.
The tangent plane at gv, which is just
lggv,0 = lv,0 =

(x, y, z)t ∈V | x −y = z
√
2
	
,
projects onto the tangent line

(x, y, 1)t ∈V | x −y =
√
2
	
,
as in the lower right part of Fig.6.11.
Thanks to duality, lv,0 can also be interpreted as a mere point: the original vector
v ∈V . This is indeed duality: the tangent to the tangent is just the original vector
itself, and the envelope of the envelope is just the original conic itself.
There is nothing special about the above choice of v: every two points v and gv
on the original hyperboloid project onto two antipodal points:
±
v1
v3
, v2
v3

.
Furthermore, lgv,0 and lv,0 project onto two parallel lines that share the same normal
vector:
1

v2
1 + v2
2
 v1
v2

(Sect.6.5.5). For this reason, the projected lv,0 and the projected lgv,0 are both per-
pendicular to the projected v and gv. This is indeed as expected from a circle in
Euclidean geometry: the tangent should be perpendicular to the radius ([22] and
Chap.6 in [63]).
In summary, duality is relevant not only in the original real projective plane but
also in the horizontal plane z ≡1: just as the projected lgv,0 is tangent to the circle at
the projected v, the projected lv,0 is tangent to the circle at the projected gv, on the
other side.

218
6
Projective Geometry with Applications in Computer Graphics
6.7.3
Projective Mappings
A projective mapping (or transformation) acts in the real projective plane V/C. This
way, it can model a three-dimensional motion. Once projected onto the horizontal
plane z ≡1, the original three-dimensional trajectory produces a two-dimensional
shadow, easy to illustrate and visualize geometrically.
Let g ∈G be a real nonsingular 3 × 3 matrix. Clearly, g can be interpreted as a
linear mapping:
v →gv,
v ∈R3.
As discussed in Chap.5, Sect.5.8.5, Cg ∈G/C can also act on the real projective
plane V/C:
Cv →Cg(Cv) ≡C(gv),
Cv ∈V/C.
In other words, if v is a representative from the equivalence class Cv ∈V/C, and
g is a representative from the equivalence class Cg ∈G/C, then gv may represent
the new equivalence class Cg(Cv) ∈V/C.
This is how the projective mapping looks like in V/C. How does it look like in
the horizontal plane z ≡1? Well, it can break into three stages: ﬁrst unproject, then
apply Cg, then project. Together, this makes PCgP−1:
v →PCgP−1v ≡PCgCv ≡PC(gv) ≡
 gv/(gv)3
if
(gv)3 ̸= 0
±gv/∥gv∥
if (gv)3 = 0.
Let’s look at a few examples.
6.8
Applications in Computer Graphics
6.8.1
Translation
Translation is an important example of a projective mapping that is often used in
computer graphics. Let α and β be some real parameters. Consider the matrix
g ≡
⎛
⎝
1 0 α
0 1 β
0 0 1
⎞
⎠.
In the horizontal plane z ≡1, in particular, g translates by (α, β)t:
⎛
⎝
x
y
1
⎞
⎠→g
⎛
⎝
x
y
1
⎞
⎠=
⎛
⎝
x + α
y + β
1
⎞
⎠.

6.8 Applications in Computer Graphics
219
Thus, the horizontal plane z ≡1 remains invariant.
This kind of translation, however, is too simple and naive to simulate or visualize
a real three-dimensional motion. In computer graphics, one might want to simulate
the original motion well, before projecting to two dimensions.
6.8.2
Motion in a Curved Trajectory
For this purpose, consider a planar object in the original space V , with the unit normal
vector n. Suppose that it moves along a given curve or trajectory t ⊂V . How to
simulate or visualize this motion best?
The original three-dimensional motion can be approximated by a composition of
many tiny linear translations, each advances the object by a small step in the direction
tangent to t ⊂V . Let’s focus on the ﬁrst step: the next steps can model in the same
way.
Initially, the object is placed at the beginning of t. At this point, let p be the unit
vector tangent (or parallel) to t.
6.8.3
The Translation Matrix
Deﬁne the translation matrix g by
g ≡I + γ · p · nt,
where γ is a real parameter to be speciﬁed later, and nt is the row vector transpose
to n [71].
Consider, for example, the simple case in which both the planar object and the
tangent vector p lie in the horizontal plane z ≡1:
n =
⎛
⎝
0
0
1
⎞
⎠,
and
p =
⎛
⎝
α
β
0
⎞
⎠.
In this case, the motion is actually two-dimensional:

220
6
Projective Geometry with Applications in Computer Graphics
g = I + γ
⎛
⎝
α
β
0
⎞
⎠(0, 0, 1) =
⎛
⎝
1 0 γ · α
0 1 γ · β
0 0
1
⎞
⎠.
With γ = 1, this is just the same as in Sect.6.8.1. More general (genuinely three-
dimensional) motion, on the other hand, requires a more general translation matrix.
6.8.4
General Translation of a Planar Object
What does a general translation do? Well, it translates the entire planar object in the
direction pointed at by p. To see this, let r be the (real) inner product that n makes
with the initial point in the trajectory. Initially, the planar object lies in its entirety in
the plane (or level set, or shifted zero-level set)
ln,r =

v ∈V | ntv = r

= r · n + ln,0.
Then, each point v in the planar object translates by the same amount:
v →gv = v + γ · p · ntv = v + γ · r · p,
as required. This completes the ﬁrst step in the discrete path that approximates the
original motion.
6.8.5
Unavailable Tangent
In practice, however, the tangent p is not always available. Fortunately, it can still
be approximated by the difference between two given points on the trajectory—the
next point minus this point:
p .= u2 −u1.
For instance, using the parameter
γ ≡
1
ntu1
= 1
r
we have a new translation matrix that never uses p:
g ≡I + 1
r (u2 −u1)nt.
This g indeed translates u1 to u2:

6.8 Applications in Computer Graphics
221
u1 →gu1 = u1 + 1
r (u2 −u1)ntu1 = u1 + (u2 −u1) = u2,
as required. The object is then ready to advance from u2 to the next point on the
trajectory.
In the next step, on the other hand, up-to-date values of u1, u2, r, and γ should be
used, to design a new translation matrix g, and advance the object to the next point
on the trajectory, and so on.
Finally, to visualize the discrete path well, project it on the horizontal plane z ≡1,
asabove.Thismaygiveacompleteanimationmovieoftheoriginalthree-dimensional
motion.
6.8.6
Rotation
How to visualize the motion of the moon in the solar system? It contains two inner
rotations: the Moon around the Earth, and the Earth around the Sun. The latter may
take place in the horizontal plane z ≡1. For instance, the sun could be at (0, 0, 1),
and the Earth could start from (1, 0, 1). In this case, the Earth’s motion is governed
by the matrix
g(θ) ≡
⎛
⎝
cos(θ) −sin(θ) 0
sin(θ) cos(θ) 0
0
0
1
⎞
⎠.
This way, the Earth’s orbit is
⎧
⎨
⎩g(θ)
⎛
⎝
1
0
1
⎞
⎠

0 < θ ≤2π
⎫
⎬
⎭=
⎧
⎨
⎩
⎛
⎝
cos(θ)
sin(θ)
1
⎞
⎠

0 < θ ≤2π
⎫
⎬
⎭≃S1.
The rotation of the moon around the Earth, on the other hand, is not necessarily
conﬁned to any horizontal plane. On the contrary, it may take place in an oblique
plane, with the normal vector
n ≡
⎛
⎝
n1
n2
n3
⎞
⎠,
with some nonzero real components n1, n2, and n3. Let’s deﬁne two more (real)
orthonormal vectors:
m ≡
1

n2
1 + n2
2
⎛
⎝
−n2
n1
0
⎞
⎠
and k ≡n × m.

222
6
Projective Geometry with Applications in Computer Graphics
This way, n, m, and k form a new axis system in R3 (Chap.2, Sects.2.2.4 –2.3.2).
Let’s use them as columns in the new 3 × 3 (real) orthogonal matrix
O ≡(n | m | k) .
We are now ready to deﬁne the matrix that rotates the moon by angle θ in the oblique
m-k plane:
ˆg(θ) ≡O
⎛
⎝
1
0
0
0 cos(θ) −sin(θ)
0 sin(θ) cos(θ)
⎞
⎠Ot.
This way, if the moon is initially at m (relative to the Earth), then it will later be at
ˆg(θ)m = O
⎛
⎝
1
0
0
0 cos(θ) −sin(θ)
0 sin(θ) cos(θ)
⎞
⎠Otm
= O
⎛
⎝
1
0
0
0 cos(θ) −sin(θ)
0 sin(θ) cos(θ)
⎞
⎠
⎛
⎝
0
1
0
⎞
⎠
= O
⎛
⎝
0
cos(θ)
sin(θ)
⎞
⎠
= cos(θ)m + sin(θ)k
(relative to the Earth). For this reason, if the Earth was at the origin (0, 0, 0), and the
moon was initially at m, then the moon’s orbit would be

ˆg(θ)m | 0 < θ ≤2π

= {cos(θ)m + sin(θ)k | 0 < θ ≤2π} ≃S1.
The Earth, however, is not static, but dynamic: it orbits the Sun at the same time.
Therefore, the true route of the moon in the solar system is the sum of these two
routes: the Earth around the Sun, plus the moon around the Earth, at a frequency 12
times as high:
⎧
⎨
⎩g(θ)
⎛
⎝
1
0
1
⎞
⎠+ ˆg(12θ)m

0 < θ ≤2π
⎫
⎬
⎭.
To visualize, let’s use the discrete angles
0 < θ1 < θ2 < · · · < θN = 2π,
for some large natural number N. These N distinct angles produce the discrete path

6.8 Applications in Computer Graphics
223
Fig. 6.12
The route of the moon in the solar system, projected on the horizontal plane z ≡1.
It is assumed that the moon rotates around the Earth in an oblique plane, whose normal vector is
n = (1, 1, 1)t/
√
3
⎧
⎨
⎩g(θi)
⎛
⎝
1
0
1
⎞
⎠+ ˆg(12θi)m

1 ≤i ≤N
⎫
⎬
⎭.
This discrete path can now project onto the horizontal plane z ≡1, by just dividing
each vector by its third component. This may produce a two-dimensional animation
movie to visualize the original three-dimensional motion of the moon in the solar
system (Fig.6.12).
6.8.7
Relation to the Complex Projective Plane
How does the real projective plane relate to the complex projective plane in
Sect.6.2.1? Well, recall that the latter was ﬁrst reduced to a hemisphere. For this
purpose, we divided by arg(c2) (Sect.6.2.7). At the equator at the bottom of the
hemisphere, however, it is impossible to divide by c2 = 0. Instead, we must divide
by arg(c1), shrinking the entire equator into just one inﬁnity point, thus losing a lot
of valuable information about the original direction of each individual inﬁnity point
on this equator.

224
6
Projective Geometry with Applications in Computer Graphics
Fortunately, the real projective plane improves on this. The equator no longer
shrinks, so the original hemisphere no longer reduces to a standard sphere. This way,
each pair of antipodal inﬁnity points still point in the original direction, storing this
valuable information for future use.
6.9
The Real Projective Space
6.9.1
The Real Projective Space
Let us now go ahead to a yet higher dimension: redeﬁne V as
V ≡R4 \ {(0, 0, 0, 0)}.
Furthermore, redeﬁne G as the group of 4 × 4 real nonsingular matrices. In this
group, the unit element is the 4 × 4 identity matrix
I ≡
⎛
⎜⎜⎝
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
⎞
⎟⎟⎠.
This new I is now used to redeﬁne the subgroups C, U, and H:
C ≡{x I | x ∈R, x ̸= 0} = (R \ {0}) I
U ≡{x I | x ∈R, |x| = 1} = ±I = S0I
H ≡{x I | x ∈R, x > 0} .
We are now ready to project.
6.9.2
Oblique Projection
For each vector
v =
⎛
⎜⎜⎝
v1
v2
v3
v4
⎞
⎟⎟⎠∈V,
In V , Cv is a complete equivalence class: a three-dimensional hyperplane in V . Still,
it also has another face: an individual element (or “point”) in the real projective space
V/C. Let’s go ahead and project it “obliquely”.

6.9 The Real Projective Space
225
If v4 ̸= 0, then Cv could indeed project on the hyperplane
{(x, y, z, 1) ∈V } ⊂V
simply by dividing by v4:
v →1
v4
v =
⎛
⎜⎜⎝
v1/v4
v2/v4
v3/v4
1
⎞
⎟⎟⎠.
If, on the other hand, v4 = 0, then v represents an inﬁnity point that must project
radially by
v →± 1
∥v∥v.
This completes our “oblique” projection.
6.9.3
Radial Projection
Alternatively, one could also use a more uniform approach: always project radially,
regardless of whether v1 vanishes or not:
v →± 1
∥v∥v.
Fortunately, in S3/S0, these antipodal points are considered as one and the same.
As in Sect.6.4.1, this new projection could also be written algebraically as
V/C =
V
U H = V/H
U
≃S3/S0,
where “≃” stands for topological homeomorphism.
6.10
Duality: Point–Plane
6.10.1
Points and Planes
As discussed above, the point v ∈V represents the point Cv in the real projective
space V/C. Furthermore, as in Sect.6.5.7, v also has a dual interpretation: the three-
dimensional hyperplane orthogonal to v:

226
6
Projective Geometry with Applications in Computer Graphics
{(x, y, z, w) ∈V | xv1 + yv2 + zv3 + wv4 = 0} .
Fortunately, this hyperplane is invariant under C. Therefore, it might get rid of one
redundant degree of freedom, and be viewed as a two-dimensional plane in V/C.
In summary, in the real projective space, Cv may take two possible meanings:
either the point Cv, or the plane orthogonal to v.
We’ve already seen duality in the context of the real projective plane: we’ve used
vector product (deﬁned in Chap.2, Sect.2.2.3) to show that two distinct lines meet
at a unique point, and two distinct points make a unique line (Sects.6.5.4 and 6.5.7).
To extend this, we must ﬁrst extend vector product to four spatial dimensions as
well. This will help establish duality in the real projective space as well: every three
independent points make a unique plane, and every three independent planes meet
at a unique point.
In a more uniform language, in V/C, three independent objects of one kind make
a unique object of another kind. Thus, the three original objects could be interpreted
in terms of either kind: there is a complete symmetry between both kinds.
6.10.2
The Extended Vector Product
To deﬁne the required vector product in four spatial dimensions as well, consider a
row made of four column vectors—the standard unit vectors in V :
E ≡
⎛
⎜⎜⎝
⎛
⎜⎜⎝
1
0
0
0
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
0
1
0
0
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
0
0
1
0
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
0
0
0
1
⎞
⎟⎟⎠
⎞
⎟⎟⎠.
This row will serve as the ﬁrst row in the 4 × 4 matrix used in the vector product.
The so-called triple vector product can now be deﬁned as a vector function of the
form
× :

R43 →R4.
What does it do? Well, it takes the column vectors u, v, w ∈R4, and places them as
rows in a new 4×4 matrix (whose ﬁrst row is E). Finally, it returns the determinant:
× (u, v, w) ≡det
⎛
⎜⎜⎝
⎛
⎜⎜⎝
E
ut
vt
wt
⎞
⎟⎟⎠
⎞
⎟⎟⎠.
This is a new vector in R4, as required. Indeed, thanks to the original deﬁnition of a
determinant (Chap.2, Sect.2.1.1), this is just a linear combination of the items in the
ﬁrst row—the standard unit vectors in R4. Let’s use it in the real projective space.

6.10 Duality: Point–Plane
227
6.10.3
Three Points Make a Unique Plane
Fortunately, a matrix with two identical rows has a zero determinant (Chap.2,
Sect.2.2.4). This is why the new triple vector product is so attractive: it produces a
new vector, orthogonal to u, v, and w:
(×(u, v, w), u) = det
⎛
⎜⎜⎝
⎛
⎜⎜⎝
ut
ut
vt
wt
⎞
⎟⎟⎠
⎞
⎟⎟⎠= 0
(×(u, v, w), v) = det
⎛
⎜⎜⎝
⎛
⎜⎜⎝
vt
ut
vt
wt
⎞
⎟⎟⎠
⎞
⎟⎟⎠= 0
(×(u, v, w), w) = det
⎛
⎜⎜⎝
⎛
⎜⎜⎝
wt
ut
vt
wt
⎞
⎟⎟⎠
⎞
⎟⎟⎠= 0.
For this reason, if u, v, and w are linearly independent vectors that represent three
independent points in V/C, then their triple vector product represents the required
“plane” in V/C: the unique plane that contains all three points—Cu, Cv, and Cw:
Cu, Cv, Cw ∈{Cv ∈V/C | (×(u, v, w), v) = 0} ⊂V/C.
Let us now look at things the other way around.
6.10.4
Three Planes Share a Unique Point
In the dual interpretation, on the other hand, u is no longer a mere vector in V , but
rather a complete hyperplane in V : the hyperplane orthogonal to u. Likewise, v is
now viewed as the hyperplane orthogonal to v, and w is now viewed as the hyperplane
orthogonal to w. What point do they share? This is just
×(u, v, w).
After all, in V , this point belongs to all three hyperplanes. Therefore, in V/C,
C (×(u, v, w))
is indeed the unique point shared by all three planes, as required.

228
6
Projective Geometry with Applications in Computer Graphics
Conics can now be deﬁned in the spirit in Sect.6.6.1. Tangent hyperplanes can
also be deﬁned in the spirit in Sect.6.6.5. The details are left as an exercise.
6.11
Exercises
1. In Sect.6.2.8, show that C/H is a legitimate subgroup of G/H. Hint: see Chap.5,
Sect.5.10.2.
2. Furthermore, show that C/H is normal. Hint: see Chap.5, Sect.5.10.2.
3. In Sect.6.2.8, what is the center of G/H?
4. Show that this center must include C/H. Hint: each element Hc ∈C/H com-
mutes with every element Hg ∈G/H:
HcHg = H(cg) = H(gc) = HgHc.
5. Show that C/H must include the center of G/H. Hint: in the exercises at the end
of Chap.5, assume that the matrices A and B commute up to a scalar multiple.
Fortunately, this scalar must be 1. After all, when either A or B is diagonal, both
AB and B A have the same diagonal.
6. Conclude that C/H is indeed the center of G/H.
7. How can the formula U ≃C/H (end of Sect.6.2.8) be deduced from the second
isomorphism theorem in Chap.5, Sect.5.10.1? Hint: assume that T and S have
just one joint element: the unit element. Then, substitute T ←U and S ←H.
8. Show that the set of real nonsingular 3 × 3 matrices is indeed a group.
9. Show that I, the identity matrix of order 3, is indeed the unit element in this
group.
10. Show that the center of this group is the set of real nonzero scalar multiples of
I. Hint: see exercises at the end of Chap.5.
11. Show that U, deﬁned in Sect.6.4.1, is indeed a group in its own right.
12. Conclude that U is indeed a subgroup of the above center.
13. Show that H, deﬁned in Sect.6.4.1, is indeed a group in its own right.
14. Conclude that H is indeed a subgroup of the above center.
15. Conclude that U H is indeed a group in its own right.
16. Conclude that U H is a subgroup of the above center.
17. Show that U H is exactly the same as the above center.
18. Show that a “point” in the real projective plane could be viewed an object in R3:
a real nonzero three-dimensional vector, deﬁned up to a nonzero scalar multiple.
19. Show that a “line” in the real projective plane could be interpreted in terms of
its normal vector, which is a real nonzero three-dimensional vector. This way,
the “line” is still in R3: it contains those three-dimensional vectors orthogonal
to that normal vector.
20. Show that multiplying that normal vector by a nonzero real scalar would still
produce the same “line” as above.
21. Show that the above “line” is invariant under nonzero scalar multiplication.

6.11 Exercises
229
22. Conclude that the above “line” is invariant under the above center.
23. Conclude that the above “line” is indeed a legitimate line in the real projective
plane.
24. Conclude that the line can be rightly called a projective line.
25. Show that, in the real projective plane, two distinct points make a unique line.
Hint: use the vector product of the original three-dimensional vectors as a normal
to the required line.
26. Show that, in the real projective plane, two distinct lines share a unique point
(possibly an inﬁnity point). Hint: take the vector product of the original normal
vectors.
27. Give an algebraic condition to guarantee that two such projective lines are “par-
allel” to each other, or meet each other at a unique inﬁnity point on the inﬁnity
circle. Hint: the original normal vectors must have a vector product with a van-
ishing z-coordinate. To guarantee this, from each normal vector, drop the third
component. The resulting two-dimensional subvectors should be proportional to
each other.
28. Show that the inﬁnity line meets every other projective line at a unique inﬁnity
point on the inﬁnity circle. Hint: its normal vector is (0, 0, 1)t, so the above
condition indeed holds.
29. Show that a “point” in the real projective space could be viewed as an object
in R4: a real nonzero four-dimensional vector, deﬁned up to a nonzero scalar
multiple.
30. Showthata“plane”intherealprojectivespacecouldbeinterpretedasanobjectin
R4, in terms of its four-dimensional normal vector. This way, the “plane” contains
those four-dimensional vectors that are orthogonal to that normal vector.
31. Show that multiplying that normal vector by a nonzero real scalar still produces
the same “plane” as above.
32. Show that the above “plane” is invariant under nonzero scalar multiplication.
33. Conclude that the above “plane” is indeed a legitimate plane in the real projective
space.
34. Conclude that the above plane can be rightly called a projective plane.
35. Show that, in the real projective space, three independent points make a unique
plane. Hint: use the triple vector product (Sect.6.10.2) of the original linearly
independent four-dimensional vectors, to produce the required four-dimensional
normal vector.
36. Show that, in the real projective space, three independent planes share a unique
point. Hint: take the triple vector product of the original linearly independent
four-dimensional normal vectors. The resulting four-dimensional vector should
be interpreted up to a nonzero scalar multiple.
37. Extend the deﬁnition of conics in Sect.6.6.1 to the real projective space in
Sect.6.9.1 as well.
38. Deﬁne tangent planes in the real projective space, analogous to tangent lines in
Sect.6.6.5.
39. For n = 1, 2, 3, . . ., deﬁne the 2n-dimensional complex projective space.
40. Show that it is topologically homeomorphic to S2n+1/S1.

230
6
Projective Geometry with Applications in Computer Graphics
41. Show that this is just the top half of S2n, with a rather strange “equator” at the
bottom: not S2n−1 but rather S2n−1/S1— the inﬁnity hyperplane, which is a lower
dimensional complex projective space in its own right, deﬁned inductively.
42. Extend the duality established in Sects.6.5.4–6.5.7 to the complex projective
space as well. (Be sure to use the complex conjugate in your new kind of vector
products.)
43. Produce an animation movie of a planar object traveling along a curved trajectory
in the three-dimensional Cartesian space (Sect.6.8.1).
44. Produce an animation movie of the Earth and the Moon, traveling in the solar
system (Sect.6.8.6).
45. Show that the set of real nonsingular 4 × 4 matrices is indeed a group.
46. Show that I, the identity matrix of order 4, is indeed the unit element in this
group.
47. Show that the center of this group is the set of real nonzero scalar multiples of
I. Hint: see exercises at the end of Chap.5.
48. Show that U, deﬁned in Sect.6.9.1, is indeed a group in its own right.
49. Conclude that U is indeed a subgroup of this center.
50. Show that H, deﬁned in Sect.6.9.1, is indeed a group in its own right.
51. Conclude that H is indeed a subgroup of this center.
52. Conclude that U H is indeed a group in its own right.
53. Conclude that U H is a subgroup of the above center.
54. Show that U H is exactly the same as the above center.
55. Show that, in the method in Chap.4, Sect.4.4.3, the inverse Lorentz transforma-
tion back to the x-y-t self system of the second particle is actually interpreted as
a projective mapping in the real projective plane (Sects.6.4.1 and 6.7.3). In this
mapping, the original velocity (dx′/dt′, dy′/dt′) of the ﬁrst particle in the lab
(Fig.4.6) transforms to the new velocity (dx/dt, dy/dt) of the ﬁrst particle away
from the second one (Fig.4.7). Because we divide by t or t′, the time variable
is eliminated, and is only used implicitly to advance the particle in the direction
pointed at by the velocity vector.

Chapter 7
Quantum Mechanics: Algebraic Point
of View
The matrices introduced above have two algebraic operations. Thanks to addition,
they make a new linear space. Thanks to multiplication, they also form a group.
In this group, the commutative law doesn’t hold anymore. Indeed, multiplying
from the left is not the same as multiplying from the right. How different could these
operations be? To measure this, we need a new algebraic operation: the commutator.
Thanks to the commutator, we can now introduce yet another important ﬁeld:
quantum mechanics. Indeed, thanks to the above algebraic tools, this can be done in
a straightforward and transparent way. For this purpose, we redeﬁne momentum and
energy in their stochastic (probabilistic) face.
In Chap.2, we’ve already introduced angular momentum in classical mechanics.
In the exercises below, on the other hand, we use quantum mechanics to redeﬁne
angular momentum, and highlight it from an algebraic point of view. This may help
study a few elementary particles like electrons and photons, with their new property:
spin. This property is not well understood physically. Fortunately, thanks to groups
and matrices, it can still be modeled and understood mathematically.
7.1
Nondeterminism
7.1.1
Relative Observation
In Newtonian mechanics, we often consider a particle, or just any physical object.
Such an object must lie somewhere in space: this is its position.
At each individual time, the object may have a different position. This is deter-
ministic: we can measure the position, and tell it for sure. This way, we can also
calculate how fast it changes: the velocity. From the velocity, we can then calculate
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_7
231

232
7
Quantum Mechanics: Algebraic Point of View
the momentum and the kinetic energy at each individual time. This gives a complete
picture of the object, and its physical motion.
In Chap.4, on the other hand, we’ve seen that things are not so absolute, but more
relative. The position that I measure in my lab may differ from the position that you
measure in your own system. In fact, position is meaningless on its own, unless it
has some ﬁxed reference point: the origin. What is meaningful is just the difference
between two positions.
The same is true for another important quantity: time. In fact, time is only relative
and nonphysical. Indeed, the time that I see in my clock may differ from the time
that you see in your own clock, particularly if you get away from me fast.
Don’t worry: no clock is bugged—both work well. Time is not absolute, but only
relative: it depends on the perspective from which it is measured. This is why time
is never deﬁned absolutely.
Even the universe has no absolute beginning. Indeed, you can never time travel
back to the big bang, but only approach it as a limit. Indeed, the big bang itself is
singular: the universe was so dense that time was so slow and heavy that it hardly
moved at all!
Thus, in special relativity, time is just an observation. To know it, one must look
and observe. This measurement is relative—it depends on the perspective from which
it is made. For this reason, two observers may see a different time in their different
systems. For example, the time that I see in my own clock here on the Earth may
differ from the time in some other clock, placed on a satellite.
7.1.2
Determinism
So, special relativity teaches us to be more modest, and not trust our own eyes. What
you see in your system is not necessarily the absolute truth: it is often different from
what I see in my own (moving) system. This applies not only to time but also to other
important observations. Just like there is no absolute time, there is also no absolute
position. What exists physically is just the original object itself.
Together, time and position make a new pair of observations, which depend on
the system where they are measured. Likewise, momentum and energy make a new
pair as well. This is why momentum is more fundamental than velocity.
Fortunately, this is still deterministic. You know what you see, with no doubt.
This way, the physical quantities are still well deﬁned, uniquely and unambigu-
ously. Unfortunately, this is true only in the macroscale, but not in the microscale or
nanoscale, used often in molecules, atoms, and subatomic particles.

7.1 Nondeterminism
233
7.1.3
Nondeterminism: Observables
In quantum mechanics, things get yet worse: nothing is certain any more. After all,
a particle could be so small that its position has no physical meaning whatsoever.
In this context, the position is no longer an observation, but just an observable: you
could observe and measure it, but better not.
Instead of a physical position, the particle may only have a probability to be
somewhere: a number that tells us how likely it is to be there. Perhaps it is there, and
perhaps not. We’ll never know, unless we’re ready to take the risk and change our
physical state forever.
Indeed, to know the position for sure, you must take a measurement. But this is
not advisable: the particle is often so small that detecting its position is too hard, and
requires a complicated experiment, which may change the entire physical state. This
way, we may lose a lot of information about other important observables, such as
momentum.
This also works the other way around: the particle is so small that measuring its
momentum is too hard, and may require a complicated experiment. As a result, vital
information is gone, and the original position may never be discovered any more!
Fortunately, thanks to our advanced algebraic tools, we can now model even a
highly nondeterministic state like this. Indeed, to model an observable, we can now
use a matrix. After all, matrices enjoy all sorts of useful algebraic properties.
This way, there is no need to look or observe as yet: this could wait until later.
In the meantime, we can still “play” with our matrices, and design more and more
observables as well.
7.2
State—Wave Function
7.2.1
Physical State
In Newtonian mechanics, we often consider a particle, traveling along the x-axis. At
time t, its position is x(t). By differentiating, one could also calculate the velocity
x′(t), and the momentum mx′(t) (where m is the mass). This is the linear momentum
in the x-spatial dimension. This could be done at each individual time t.
In quantum mechanics, on the other hand, this is not so easy any more. Indeed,
there is no determinism any more. In the “true” physics, the particle is nowhere (or
everywhere...). The physical state only tells us where it could be. Perhaps it is there,
and perhaps not...
The physical state is no longer a function x(t), but a nonzero n-dimensional
(complex) vector v. This vector contains every information that nature tells us about
the particle, including where it might be at time t. For this reason, v isn’t ﬁxed, but
may change in time: v ≡v(t).

234
7
Quantum Mechanics: Algebraic Point of View
7.2.2
The Position Matrix
So, where might the particle be? For this purpose, we have a new n × n diagonal
matrix: X. On its main diagonal, you can ﬁnd possible positions that the particle
might take.
Consider, for example, some element on the main diagonal: Xk,k (for some 1 ≤
k ≤n). How likely is the particle to be at position x = Xk,k? Well, the probability
for this can be deduced from v: it is just |vk|2.
Clearly, the probabilities must sum to 1. For this purpose, we must assume that
v has already been normalized to have norm 1. So, what is important is only the
direction of v, not its norm. For every practical purpose, one may assume that v is
deﬁned up to a scalar multiple only:
v ∈

Cn \ {0}

/ (C \ {0})
(Chap.5, Sects.5.8.3–5.8.5).
7.2.3
Dynamics: Schrodinger Picture
Unfortunately, n may be too small. After all, the particle could get farther and farther
away from the origin, and reach inﬁnitely many positions. To allow this, X must be
an inﬁnite matrix, with an inﬁnite order.
For example, a particle could “jump” from number to number along the real axis.
To model this, X must be as big as
X ≡
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
...
−3
−2
−1
0
1
2
3
...
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
This way, on its main diagonal, X has all possible positions: all integer numbers. In
a yet more realistic model, on the other hand, X should be even bigger: on its main
diagonal, it should contain not only integer but all real numbers. In this case, X is
not just a matrix, but actually an operator.
This kind of dynamics is called Schrodinger’s picture: X remains constant at
all times, whereas v changes from time to time. This setting is more common than

7.2 State—Wave Function
235
Heisenberg’spicture,whichworkstheotherwayaround:v remainsconstant,whereas
X changes in time.
For simplicity, however, we try to avoid inﬁnite dimension. Instead, we mostly
stick to our ﬁnite dimension n, and our original n-dimensional vector and n × n
matrix.
7.2.4
Wave Function and Phase
How does the state v look like? Well, it may look like a discrete sine or cosine wave,
as in Figs.1.10 or 1.11. This is why v is often called a wave function.
Still, v is not necessarily real: it may well be a complex vector in Cn. For example,
it may be the discrete Fourier mode, as in Fig.1.12.
Thus, in general, each component vk is a complex number in its own right. As
such, it has its own polar decomposition—amplitude times exponent:
vk = |vk| exp (iθk) ,
where i ≡√−1 is the imaginary number, and θk is the phase: the angle that vk makes
with the positive part of the real axis.
The amplitude |vk| tells us how likely the particle is to be at position Xk,k. More
precisely, the probability for this is |vk|2. The exponent, on the other hand, is a
complex number of magnitude1. As such, it has no effect on this probability. Still, it
may encapsulate important information about other physical properties.
Once each component has been written in its polar decomposition, v takes a famil-
iar face: wave function. This way, the original particle also has a new mathematical
face: wave. As such, it enjoys an interesting physical phenomenon: interference.
7.2.5
Superposition and Interference
Two electrons cannot be in exactly the same state at the same time. This is Pauli’s
exclusion principle. (See exercises below.) Two photons, on the other hand, can. In
this case, they have the same wave function.
Two wave functions may sum up, and produce a new wave function: their super-
position. In this process, the original states sum up, component by component. To
add two corresponding components to each other, their phases are most important.
If they match, then they enhance each other. If, on the other hand, they don’t match,
then they may even cancel (or annihilate) each other. This is the phenomenon of
interference.
Once the wave functions have been summed up and normalized, we have the
new (joint) state: the superposition of the two original states. In summary, a particle

236
7
Quantum Mechanics: Algebraic Point of View
actually has two mathematical faces: on one hand, it is a particle. On the other
hand, it is also a wave. Each face is useful to analyze and explain different physical
phenomena.
7.3
Observables Don’t Commute!
7.3.1
Don’t Look!
Let’s return to our original particle. It only has a nondeterministic position, where it
might be. But where is it located in fact?
Don’t ask! Because, to ﬁnd out, you must carry out an experiment. At probability
|vk|2, you’d then discover a position x = Xk,k (for some 1 ≤k ≤n).
What happens mathematically? Well, we now know for sure that x = Xk,k. In
other words, the probability for this is now as large as 1: there is no doubt at all. So,
our v has changed forever. After all, we now know that |vk|2 = 1. Since v has norm
1, all other components must now vanish. Indeed, the particle can no longer lie at
any other position but Xk,k.
In summary, in your experiment, you spoiled the original v completely, with all
the valuable information that was in it! Instead of the original interesting v, you now
have a boring deterministic v: a standard unit vector.
Here one may ask: why do we still need v? After all, we already got what we
wanted: we discovered the true position! Still, v contained information not only about
the position but also about many other physical properties.
To appreciate better the information we may lose, let’s study our original v once
again. The probability that x = Xk,k is
|vk|2 =



e(k), v


2 ,
where e(k) is a standard unit vector: an eigenvector of X, with eigenvalue Xk,k. Still,
position is not the only observable we’re interested in. We might also want to know
the momentum p of the particle, in the x-spatial direction.
7.3.2
The Momentum Matrix and Its Eigenvalues
To have the momentum, we are given yet another n ×n Hermitian matrix: P. Again,
the physical state doesn’t tell us the momentum for sure: it just tells us how likely
the particle is to have a certain momentum. For example, let λk be an eigenvalue of
P. How likely is the momentum to be p = λk? The probability for this is



u(k), v


2 ,

7.3 Observables Don’t Commute!
237
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
λk
x
Xj,j
Fig. 7.1 The position-momentum grid. At each individual time t, solve for the n-dimensional state
v. This way, v contains the entire physical information at time t, in terms of probability. To be at
x = X j, j, the particle has probability |v j|2. To have momentum p = λk, the particle has probability
|(u(k), v)|2 (1 ≤j, k ≤n)
where u(k) is the (normalized) eigenvector of P, corresponding to λk.
Together, X and P give us all possible position–momentum pairs (X j, j, λk) (1 ≤
j, k ≤n). This makes a new two-dimensional grid of such possible pairs (Fig.7.1).
The above grid mirrors the phase plane in classical mechanics, which contains all
possible position–momentum pairs. Ideally, the grid should have been inﬁnite and
continuous: a two-dimensional Cartesian plane. After all, in reality, both position
and momentum could take any value, not just n discrete values. To model this, n
should be inﬁnite. In this case, an inner product should be interpreted as an inﬁnite
sum, or even an integral. For simplicity, however, we stick to our ﬁnite dimension n,
and our original n-dimensional vectors and n × n matrices.
7.3.3
Order Matters!
Although you might be curious to know the exact position of the particle, better
restrain yourself, and not look! Because, if you looked, then you’d spoil v forever,
and lose the valuable probabilities of the form |(u(k), v)|2 that gave you an idea about
what the momentum was.
This also works the other way around. Better not measure the momentum, because
then you’d damage v, and lose the valuable probabilities |vk|2 about the original
position.
This means that order matters: measuring x and then measuring p may give
different results from measuring p and then measuring x. In other words, applying
X and then applying P is not the same as applying P and then applying X: they
don’t commute with each other.

238
7
Quantum Mechanics: Algebraic Point of View
7.3.4
Commutator
So, it was indeed a good idea to use matrices to model physical observables. After
all, matrices often don’t commute with each other. In our case, X and P indeed have
a nonzero commutator:
[X, P] ≡X P −P X ̸= (0),
where ‘(0)’ stands for the zero n × n matrix.
Recall that X and P are both Hermitian. As a result, [X, P] is an anti-Hermitian
matrix:
[X, P]h = (X P −P X)h = Ph X h −X h Ph = P X −X P = [P, X] = −[X, P].
This will be useful later.
7.3.5
Planck Constant
To have this commutator in its explicit form, we have a new law of nature:
[X, P] = i ¯hI,
where i = √−1 is the imaginary number, I is the n × n identity matrix, and ¯h is
called Planck constant: a universal constant, positive, and very small.
Why is this law plausible? Well, the error due to measuring in two different orders
mustn’t depend on the original particle. After all, we might have the same error even
if there was no particle at all. In fact, even with no particle at all, we might still
measure a nonzero momentum or position. (This is called the ground state.)
Thus, [X, P] should better be a constant matrix, independent of the particle under
consideration. This is why it must be of the form i ¯hI: thanks to the imaginary number
i, it is indeed anti-Hermitian, as required. Furthermore, thanks to the small constant
¯h, it is very small, and has an effect only in microscale or nanoscale, used in quantum
mechanics. In macroscale, on the other hand, it has no practical effect whatsoever.
This is why it was ignored in both geometrical mechanics (Chap.2) and special
relativity (Chap.4).

7.4 Observable and Its Expectation
239
7.4
Observable and Its Expectation
7.4.1
Observable or Measurable
The matrices introduced above are called observables (or measurables, or experi-
ments). After all, they let us observe. For example, by applying X to v, we get some
idea about the nondeterministic position: its expectation at state v.
The actual observation, on the other hand, should better wait until later. After all,
it requires an experiment, which may spoil the original state, with all the valuable
probabilities that could have been deduced from it about other observables, such as
momentum. In the meantime, we can still “play” with our matrices, and apply all
sorts of algebraic operations to them.
Consider an n × n matrix A, not necessarily Hermitian. Let’s write it as the sum
of two matrices:
A = A + Ah
2
+ A −Ah
2
.
The former term is called the Hermitian part of A. It is indeed Hermitian:
 A + Ah
2
h
= Ah + A
2
= A + Ah
2
.
The latter term, on the other hand, is called the anti-Hermitian part of A. It is indeed
anti-Hermitian:
 A −Ah
2
h
= Ah −A
2
= −A −Ah
2
.
7.4.2
Symmetrization
AproperobservableshouldbetterbeHermitian.Thisway,itseigenvectorsareorthog-
onal to each other (and could be used to decompose any other vector), and its eigen-
values are real (Chap.1, Sects.1.9.4–1.9.5). This is indeed a good property: after all,
the eigenvalues stand for possible observed values, which are always real, with no
imaginary part.
Fortunately, the anti-Hermitian part of A is often as small as ¯h, and can be disre-
garded. This is called symmetrization: replacing A by its Hermitian part. Fortunately,
this could wait until later. In the meantime, we can still stick to our original A, Her-
mitian or not.

240
7
Quantum Mechanics: Algebraic Point of View
7.4.3
Observation
So far, we’ve seen two observables: the position X, and the momentum P. In the
context of special relativity, the time t could be viewed as an observable as well
(Chap.4). After all, to have the time, you must observe: either look at your own
clock, to see the proper time, or at least look at someone else’s clock, to see a new
observable: a different time (Figs.4.3–4.5).
Of course, in special relativity, the scale is so large that randomness has no effect.
In every practical sense, the observables commute with each other, so everything is
deterministic.
In very small scales, on the other hand, randomness can no longer be ignored.
On the contrary: the “true” physical state is no longer deterministic. It only gives us
the probability to observe something, not the actual observation. This is the “true”
nature: just probability. Of course, we humans will never get to see this “truth.” After
all, we must make a decision: what to measure ﬁrst, and what to measure later. Each
choice may give us different results: the true original nature remains a mystery.
7.4.4
Random Variable and Its Expectation
Thus, an observable is more mathematical than physical. It makes a random variable:
we can’t tell for sure what its value is. Fortunately, we can still tell what its value
might be. For example, its value could be λ: some eigenvalue of the observable. The
probability for this depends on the physical state v: it is |(u, v)|2, where u is the
corresponding (normalized) eigenvector.
An observable like X or P must be Hermitian. A more general random variable,
on the other hand, may be represented by a more general matrix A, not necessarily
Hermitian. What is its expectation (or average) at state v? To ﬁnd out, just apply A
to v, and take the inner product with v. This gives a new complex number:
(v, Av) =

v, A + Ah
2
v

+

v, A −Ah
2
v

.
In this sum, the former term is real, and the latter term is imaginary. Thus, in terms
of absolute value, each term is smaller than (or equal to) the entire sum:
|(v, Av)| =





v, A + Ah
2
v

+

v, A −Ah
2
v




 ≥





v, A −Ah
2
v




 .
This gives us a useful lower bound for the expectation.

7.5 Heisenberg’s Uncertainty Principle
241
7.5
Heisenberg’s Uncertainty Principle
7.5.1
Variance
The original random variable might take all sorts of possible values. How likely are
they to spread out, and differ from the average? To get some idea about this, we
deﬁne the variance at state v:
∥(A −(v, Av) I) v∥2 .
Let’s estimate the variances of our original random variables X and P. Fortunately,
their variances have a lower bound: their covariance.
7.5.2
Covariance
At state v, the expectation of X is (v, Xv), and the expectation of P is (v, Pv). Since
these matrices are Hermitian, these expectations are real.
Now, at state v, consider the product of the variances of X and P. How to estimate
this from below? Well, thanks to the Cauchy–Schwarz inequality (exercises at the
end of Chap.1),
∥(X −(v, Xv) I) v∥· ∥(P −(v, Pv) I) v∥≥|((X −(v, Xv) I) v, (P −(v, Pv) I) v)| .
What do we have on the right-hand side? This is the covariance of X and P at state
v. To estimate it from below, recall that, although X and P are both Hermitian, their
product X P is not. Fortunately, we still have the estimate in Sect.7.4.4:
∥(X −(v, Xv) I) v∥· ∥(P −(v, Pv) I) v∥≥|((X −(v, Xv) I) v, (P −(v, Pv) I) v)|
= |(v, (X −(v, Xv) I) (P −(v, Pv) I) v)|
≥1
2 |(v, [X −(v, Xv) I, P −(v, Pv) I] v)|
= 1
2 |(v, [X, P] v)|
= 1
2



v, i ¯hIv


=
¯h
2
(assuming that v has already been normalized in advance).

242
7
Quantum Mechanics: Algebraic Point of View
7.5.3
Heisenberg’s Uncertainty Principle
Finally, take the square of the above inequality. This gives a lower bound for the
product of the variances of X and P:
∥(X −(v, Xv) I) v∥2 ∥(P −(v, Pv) I) v∥2 ≥
¯h2
4 .
This is Heisenberg’s uncertainty principle. It tells you that you can’t enjoy both
worlds. If you measured the precise position, then the variance of X becomes zero.
Unfortunately, there is a price to pay: the variance of P becomes huge. As a result,
there is no hope to measure the original momentum any more.
This also works the other way around: if you measured the precise momentum,
then the variance of P would vanish. Unfortunately, in this case, there is yet another
price to pay: the variance of X would become inﬁnite. As a result, there is no hope
to measure the original position any more!
This is why we better wait with the actual observation until later. In the meantime,
we can still “play” with our original observables algebraically, to design all sorts of
new interesting observables.
7.6
Eigenvalues
7.6.1
Shifting an Eigenvalue
Let’s use the power of linear algebra to study the commutator. Let C and T be n × n
matrices, not necessarily Hermitian. Assume also that they don’t commute with each
other. On the contrary: they have a nonzero commutator, proportional to T :
[C, T ] ≡CT −TC = αT,
for some complex number α ̸= 0.
Let u be an eigenvector of C, with the eigenvalue λ:
Cu = λu.
How to ﬁnd more eigenvectors? Just apply T to u. Indeed, if T u is a nonzero vector,
then it is an eigenvector of C as well:
CT u = (TC + [C, T ]) u = (TC + αT ) u = (λ + α) T u.
In summary, we’ve managed to “shift” λ by α, obtaining a new eigenvalue of C:
λ + α. Moreover, so long as we don’t hit the zero vector, we can now repeat this

7.6 Eigenvalues
243
procedure time and again, obtaining more and more eigenvectors of C, with new
eigenvalues, shifted more and more.
λ, λ + α, λ + 2α, λ + 3α, . . . .
Of course, because n is ﬁnite, this process must stop somewhere. Still, if n were
inﬁnite, then it could proceed indeﬁnitely.
7.6.2
Shifting an Eigenvalue of a Product
Let’s use the above in a special case. For this purpose, let A and B be n × n matrices
(not necessarily Hermitian). Assume that they don’t commute with each other. On
the contrary: they have a nonzero commutator, proportional to the identity matrix:
[A, B] ≡AB −B A = βI,
for some complex number β ̸= 0.
Now, let’s look at the product B A. What is its commutator with B? We already
know what it is:
[B A, B] ≡B AB −BB A = B(AB −B A) = B[A, B] = βB.
We can now use the result in Sect.7.6.1, to shift an eigenvalue of B A. For this
purpose, let u be an eigenvector of B A with the eigenvalue λ:
B Au = λu.
If Bu is a nonzero vector, then it is an eigenvector of B A as well:
B A(Bu) = (λ + β)Bu.
Likewise, one could also shift in the opposite direction. For this purpose, look again
at the product B A. What is its commutator with A? It is just
[B A, A] = B AA −AB A = (B A −AB)A = −[A, B]A = −β A.
We can now use the result in Sect.7.6.1 once again, to design a new eigenvector of
B A. For this purpose, take the original eigenvector u, and apply A to it:
B A(Au) = (λ −β)Au.
This way, if Au is a nonzero vector, then it is indeed an eigenvector of BA as well,
with the new eigenvalue λ −β. So long as we don’t hit the zero vector, we can now

244
7
Quantum Mechanics: Algebraic Point of View
repeat this procedure time and again, and design more and more eigenvectors of B A,
with new eigenvalues:
λ, λ ± β, λ ± 2β, λ ± 3β, . . . .
Of course, since n is ﬁnite, this process must stop somewhere. Still, if n were inﬁnite,
then it might continue forever.
7.6.3
A Number Operator
In the above, consider a special case, in which
B ≡Ah
and β ≡1.
This way, the assumption in the beginning of Sect.7.6.2 takes the form
[A, Ah] = I.
Furthermore, the product studied above is now
B A = Ah A.
As a Hermitian matrix, Ah A has real eigenvalues only. Furthermore, thanks to the
above discussion, we can now lower or raise an eigenvalue of Ah A: from λ to
λ + 1, λ + 2, λ + 3, . . .
(until hitting the zero vector), and also to
λ −1, λ −2, λ −3, . . .
(until hitting the zero vector, which will be very soon).
Ah A is an important matrix: it is called a number operator. Why? Because its
eigenvalues are 0, 1, 2, 3, 4, . . ..
7.6.4
Eigenvalue—Expectation
Indeed, let u be an eigenvector of Ah A, with the eigenvalue λ:
Ah Au = λu.

7.6 Eigenvalues
245
With some effort, both λ and u could have been calculated (Chap.3, Sect.3.1.1).
Better yet, there is no need to calculate them explicitly at all!
Since Ah A is Hermitian, λ must be real (Chap.1, Sect.1.9.4). Could λ be negative?
No! Indeed, look at the expectation of Ah A at u:
λ(u, u) =

u, Ah Au

= (Au, Au) ≥0.
Let’s use u to design more eigenvectors.
7.6.5
Ladder Operator: Lowering an Eigenvalue
Recall that we consider now a special case: in Sect.7.6.2, set B ≡Ah, and β ≡1.
This way, the above eigenvector u could be used to design a new eigenvector: Au.
Indeed, if Au is a nonzero vector, then it is an eigenvector of Ah A in its own right,
with a smaller eigenvalue: λ −1.
In this context, the matrix A serves as a “ladder” operator. By applying it to u, we
go down the ladder, to a smaller eigenvalue.
What is the norm of Au? To ﬁnd out, look at the expectation of Ah A at u:
∥Au∥2 = (Au, Au) =

u, Ah Au

= λ(u, u) = λ∥u∥2.
In other words,
∥Au∥=
√
λ∥u∥.
Later on, we’ll make sure to normalize the eigenvectors.
7.6.6
Null Space
This lowering procedure can’t continue forever, or we’d hit a negative eigenvalue,
which is impossible (Sect.7.6.4). It must stop upon reaching some eigenvector w
whose eigenvalue is zero. At this stage, lowering is no longer possible. So, we see in
retrospect that our λ must have been a nonnegative integer number: this is the only
way to make sure that the lowering procedure eventually hits zero, and stops.
In summary, we must eventually reach a new vector w, for which
Ah Aw = 0.
How does w look like? Well, at w, Ah A must have zero expectation:
(Aw, Aw) =

w, Ah Aw

= (w, 0) = 0.

246
7
Quantum Mechanics: Algebraic Point of View
Therefore,
Aw = 0
as well. Thus, w lies in the null spaces of both A and Ah A (Chap.1, Sect.1.9.2, and
Chap.3, Sect.3.1.1).
Fortunately, we can now go ahead and apply the reverse procedure to w, to obtain
bigger eigenvalues back again.
7.6.7
Raising an Eigenvalue
For this purpose, let’s use Sect.7.6.2 once again (again, with B ≡Ah and β ≡1).
This way, from our original eigenvector u, we can now form Ahu: a new eigenvector
of Ah A, with a bigger eigenvalue: λ + 1. In this context, Ah serves as a new ladder
operator, to help “climb” up the ladder.
What is the norm of Ahu? Well, since [A, Ah] = I,
∥Ahu∥2 =

Ahu, Ahu

=

u, AAhu

=

u,

Ah A + [A, Ah]

u

=

u,

Ah A + I

u

= (u, (λ + 1) u)
= (λ + 1) (u, u) .
In other words,
∥Ahu∥=
√
λ + 1∥u∥.
Later on, we’ll make sure to normalize these eigenvectors, as required.
7.7
Hamiltonian and Its Eigenvalues
7.7.1
Hamiltonian of the Harmonic Oscillator
So far, we’ve studied the number operator Ah A from an algebraic point of view: its
eigenvalues and eigenvectors. Still, what is its physical meaning? To see this, let’s
model a harmonic oscillator: a spring.
To model position and momentum, we already have our Hermitian matrices X
and P. Let’s use them to deﬁne a new matrix—the Hamiltonian:

7.7 Hamiltonian and Its Eigenvalues
247
H ≡mω2
2

X2 +
1
m2ω2 P2

,
where the mass m and the frequency ω are given parameters. (Don’t confuse ω with
the vector w in Sect.7.6.6, or with the angular velocity in geometrical mechanics.)
This is the Hamiltonian observable. It will be used to observe the total energy in the
harmonic oscillator—kinetic and potential alike.
What could the total energy be? Well, it must be an eigenvalue of H. At what
probability? This already depends on the current state v: normalize it, calculate its
inner product with a (normalized) eigenvector, take the absolute value, and square it
up.
7.7.2
Concrete Number Operator
How do the eigenvalues of H look like? To see this, let’s deﬁne A in Sect.7.6.3 more
concretely:
A ≡=
mω
2¯h

X +
i
mω P

.
This way, its Hermitian adjoint is
Ah =
mω
2¯h

X −
i
mω P

.
Thus, the commutator is still

A, Ah
= mω
2¯h

X +
i
mω P, X −
i
mω P

= −mω
2¯h ·
i
mω ([X, P] −[P, X]) = I,
as in Sect.7.6.3. For this reason, the above properties still hold, including raising and
lowering eigenvalues. To construct normalized eigenvectors of Ah A, just start from
some w in the null space of A, normalize it to have norm 1, apply Ah time and again,
and normalize:
w, Ahw,
1
√
2

Ah2 w,
1
√
3!

Ah3 w, . . .
1
√
k!

Ahk w, . . . .
Since Ah A is Hermitian, these are orthonormal eigenvectors of Ah A, which can help
decompose just any vector (Chap.1, Sects.1.9.5 and 1.10.5).

248
7
Quantum Mechanics: Algebraic Point of View
7.7.3
Energy Levels
So, our concrete number operator still has the same algebraic properties as before.
Still, what is its physical meaning? To see this, let’s calculate it explicitly:
Ah A = mω
2¯h

X −
i
mω P
 
X +
i
mω P

= mω
2¯h

X2 +
1
m2ω2 P2 +
i
mω [X, P]

= mω
2¯h

X2 +
1
m2ω2 P2

+ i
2¯h i ¯hI
= 1
¯hω H −1
2 I.
Thus, the Hamiltonian matrix is strongly related to the concrete number operator:
H = ¯hω

Ah A + 1
2 I.

.
Thus, the total energy in the harmonic oscillator has a very simple form. After all, it
is just an eigenvalue of H:
¯hω
2 , 3¯hω
2 , 5¯hω
2 , 7¯hω
2 , . . . .
Inunitsassmallas ¯h,thisisjustthefrequencyω,timesa(nonnegative)integernumber
plus one half. This is indeed quantum mechanics: energy is no longer continuous,
but comes in discrete levels.
Furthermore, the eigenvectors of H are the same as those of Ah A, designed
above. Thanks to conservation of energy, each of them makes a constant state, with
no dynamics at all: if your initial physical state is an eigenvector, then it must remain
so forever. After all, it must preserve the same energy level—its eigenvalue. For this
reason, the wave function must be a standing wave that never moves. Let’s see how
this looks like.
7.7.4
Ground State
First, let’s look at w—the eigenvector that lies in the null space of A (Fig.7.2). What
is its physical meaning? Well, it represents a very strange case: no momentum, and
no motion at all!
This is the minimal energy level. Indeed, w is in the null space of Ah A as well:

7.7 Hamiltonian and Its Eigenvalues
249
q q
q
q
q
q
q
q
q
q q q
q
q
q
q
q
q
q
q q
Xk,k
0
x
wk
component
Fig. 7.2
Gaussian distribution: w lies in the null spaces of both A and Ah A, with zero expectation:
(Aw, Aw) = (w, Ah Aw) = 0. To be at x = Xk,k, the particle has probability |wk|2. Thus, it is
highly likely to be at x = 0—the expectation
Ah Aw = 0.
For this reason, w is an eigenvector of H as well:
Hw = ¯hω

Ah A + 1
2 I

w =
¯hω
2 w.
Thus, w is indeed the ground state: even with no particle at all, there is still some
minimal energy. This energy is very small: in units as small as ¯h, it has just ω/2
units, where ω is the frequency.
7.7.5
Gaussian Distribution
How does w look like? Well, it makes a Gaussian distribution, with zero expectation.
In Fig.7.2, we illustrate the components wk as a function of x. To lie at x = Xk,k,
the particle has probability |wk|2 (provided that ∥w∥= 1).

250
7
Quantum Mechanics: Algebraic Point of View
7.8
Coherent States
7.8.1
Dynamic State
In Schrodinger’s picture, the matrices X, P, and H never change (Sect.7.2.3). The
dynamics is in the state, which may change in time, along with the physical infor-
mation it carries: the probabilities encapsulated in it.
How can such a moving state look like? Well, let’s try a vector we already know:
a (normalized) eigenvector of H:
1
√
k!

Ahk w,
k ≥0.
But this is no good: there is no dynamics here. After all, can this state ever change?
No, it can’t! Indeed, the total energy must remain the same eigenvalue of H. So,
to introduce a time dependence, the best you can do is to multiply this state by the
(complex) number exp(iωt). But this introduces no dynamics. After all, this has no
effect on the probabilities. Besides, the state is deﬁned up to a scalar multiple only.
Thus, this is still a standing wave function that travels nowhere.
A standing wave is rather rare: most waves travel in time. For this purpose, let’s
turn to a more general state v. Let’s expand it in terms of the orthonormal eigenvectors
of H:
v =

k≥0
 1
√
k!

Ahk w, v

1
√
k!

Ahk w.
In this expansion, look at the kth coefﬁcient: take its absolute value, and square it up:




 1
√
k!

Ahk w, v




2
= 1
k!




Ahk w, v



2
.
Assuming that ∥v∥= 1, this is the probability to have energy level ¯hω(k + 1/2).
This probability is not necessarily constant: it may change in time, along with the
entire state v. This is how the wave function can indeed travel.
7.8.2
Coherent State
For example, assume that v is a coherent state: an eigenvector of A, not of Ah A:
Av = λv,

7.8 Coherent States
251
q q
q
q
q
q
q
q
q
q q q
q
q
q
q
q
q
q
q q
λ
x
|vk|
amplitude
Fig. 7.3
A coherent state: a Gaussian distribution, shifted by a complex number λ. For each k,
|vk|2 is the probability to be at x = Xk,k
where λ is now not necessarily real: it may have a nonzero imaginary part. After all,
A is not Hermitian.
How does v look like? Well, it makes a Gaussian distribution, shifted by the
complex number λ. This is illustrated in Fig.7.3: to lie at position x = Xk,k, the
particle has probability |vk|2.
7.8.3
Coherent State—Nondeterministic Energy
In a coherent state, we’ve just seen the probability to be at position x = Xk,k. This
is illustrated in Fig.7.3. Still, there is yet another interesting question: what is the
probability to have a certain amount of energy? Well, not every amount is allowed,
but only our discrete energy levels. Fortunately, the probability to be at the kth energy
level is already available in Sect.7.8.1. In a coherent state, it is even simpler:

252
7
Quantum Mechanics: Algebraic Point of View
1
k!




Ahk w, v



2
= 1
k!



w, Akv


2
= 1
k!



w, λkv


2
= 1
k!|λ|2k |(w, v)|2
= |(w, v)|2 |λ|2k
k! .
Here, we have the factor |(w, v)|2. How to estimate it? For this purpose, note that
the probabilities must sum to 1:
1 = |(w, v)|2 
k≥0
|λ|2k
k!
.= |(w, v)|2 exp

|λ|2
.
As a result,
|(w, v)|2 .= exp

−|λ|2
.
7.8.4
Poisson Distribution
This is the Poisson distribution (Fig.7.4). Unlike the Gaussian distribution, it contains
no geometrical information: it doesn’t tell us where the particle might lie on the
x-axis, but only what energy it may have. In fact, the probability to have energy
¯hω(k + 1/2) is
exp

−|λ|2 |λ|2k
k! ,
where λ is the eigenvalue of the coherent state with respect to A.
Thus, our coherent state is essentially different from the ground state, or from any
other eigenvector of Ah A. Indeed, in a coherent state, energy is no longer known or
conserved. On the contrary: it is still nondeterministic: it is not yet known for sure,
and its probabilities can even change in time.
7.9
Particle in Three Dimensions
7.9.1
Tensor Product
So far , we’ve seen three important observables: position, momentum, and energy.
Let’s design a new observable: angular momentum. For this purpose, we need to
introduce a new (discrete) spatial dimension.

7.9 Particle in Three Dimensions
253
q
q
q
q q
q
q
q
q
q q q q q q q q q q q q
k .= λ 2
k
exp −|λ|2
|λ|2k
k!
Fig. 7.4
The Poisson distribution. To have energy
¯hω(k + 1/2), the probability is
exp(−|λ|2)|λ|2k/k!, where λ is the eigenvalue of the coherent state with respect to A. The maximal
probability is at k .= |λ|2
Fig. 7.5
The discrete
two-dimensional grid: m
horizontal rows, of m points
each. Since n = m2, our
general state v makes a
(complex) grid function,
deﬁned at each grid point.
Furthermore, in each
individual row, X acts in the
same way: it couples grid
points in the same row
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
p
Assume now that n = m2, for some integer m. This way, our general state v can
be viewed not only as a vector but also as a grid function:
v ∈Cn ∼= Cm×m
(Fig.7.5). Let’s redeﬁne our observables in the new m × m grid. To represent
x-position, for example, our new position matrix should act in each horizontal row
individually: it should couple grid points in the same row, not mix different rows.

254
7
Quantum Mechanics: Algebraic Point of View
Let x and p denote the x-position and x-momentum in one horizontal row in the
grid. Let X and P be the corresponding m ×m matrices, acting in just one horizontal
row in the grid. How to extend them to the entire grid?
For this purpose, let I be the m×m identity matrix. Let’s use X, P, and I to deﬁne
extended n × n block diagonal matrices that act in the x-spatial direction, x-row by
x-row:
X ⊗I ≡
⎛
⎜⎜⎜⎝
X
X
...
X
⎞
⎟⎟⎟⎠and P ⊗I ≡
⎛
⎜⎜⎜⎝
P
P
...
P
⎞
⎟⎟⎟⎠.
Here, the new symbol ‘⊗’ produces a bigger matrix: the tensor product of two smaller
matrices.
Likewise, to act in the y-spatial direction in the grid (y-column by y-column),
deﬁne new n × n (Hermitian) matrices:
I ⊗X ≡
⎛
⎜⎜⎜⎝
X1,1I
X2,2I
...
Xm,m I
⎞
⎟⎟⎟⎠and I ⊗P ≡
⎛
⎜⎜⎜⎜⎝
P1,1I · · · · · · P1,m I
...
... · · ·
...
...
· · · ...
...
Pm,1I · · · · · · Pm,m I
⎞
⎟⎟⎟⎟⎠
.
These new deﬁnitions are quite useful. For example, how likely is the particle to be
at position (Xi,i, X j, j)? Well, the probability for this is just |vi, j|2. After all, the state
v is now interpreted as a grid function, with two indices.
7.9.2
Commutativity
Thus, X ⊗I is completely different from I ⊗X: the former acts on the individual
x-rows, whereas the latter acts on the individual y-columns in the grid. Because they
act in different spatial directions, these matrices commute with each other.
Furthermore, although X and P don’t commute, X ⊗I and I ⊗P do:
(X ⊗I) (I ⊗P) =≡
⎛
⎜⎜⎜⎜⎝
P1,1X · · · · · · P1,m X
...
... · · ·
...
...
· · · ...
...
Pm,1X · · · · · · Pm,m X
⎞
⎟⎟⎟⎟⎠
= (I ⊗P) (X ⊗I) .

7.9 Particle in Three Dimensions
255
Likewise, P ⊗I and I ⊗X do commute with each other:
(P ⊗I) (I ⊗X) = (I ⊗X) (P ⊗I) .
Let’s extend this to a yet higher dimension.
7.9.3
Three-Dimensional Grid
So far, we’ve only considered scalar position and momentum: x and p. Let’s move
on to a three-dimensional position:
r ≡
⎛
⎝
x
y
z
⎞
⎠
(Chap.2,Sect.2.4.1).Thisway,r encapsulatesthreedegreesoffreedom:thepositions
in the x-, y-, and z-coordinates. Likewise, p is now a three-dimensional vector:
the linear momentum, containing the x-, y-, and z-scalar momenta. In quantum
mechanics, each component should be mirrored by a matrix.
For this purpose, assume now that n = m3, for some integer m. This way, our
general state v can be interpreted not only as a vector but also as a grid function:
v ∈Cn ∼= Cm×m×m.
Let’s take our m × m matrices X and P and place them in suitable tensor products.
This way, we obtain extended n × n matrices, which may help observe position and
momentum in the x-, y-, and z-coordinates:
Rx ≡X ⊗I ⊗I
Ry ≡I ⊗X ⊗I
Rz ≡I ⊗I ⊗X
Px ≡P ⊗I ⊗I
Py ≡I ⊗P ⊗I
Pz ≡I ⊗I ⊗P.
These new deﬁnitions are quite useful. For example, how likely is the particle to be
at position (Xi,i, X j, j, Xk,k)? Well, the probability for this is just |vi, j,k|2. After all,
the state v is now interpreted as a grid function, with three indices.
Do these new matrices commute with each other? Well, it depends: if they act in
different spatial directions, then they do (Sect.7.9.2). For example,

256
7
Quantum Mechanics: Algebraic Point of View

Rx, Py

= (0).
If, on the other hand, they act in the same spatial direction, then they don’t. For
example,
[Rx, Px] = i ¯hI ⊗I ⊗I.
This is used next.
7.10
Angular Momentum
7.10.1
Angular Momentum Component
Thanks to these new matrices, we can now deﬁne a new kind of observable: angular
momentumcomponent.Thismirrorstheoriginal(deterministic)angularmomentum:
Lx ≡Ry Pz −Rz Py
L y ≡Rz Px −Rx Pz
Lz ≡Rx Py −Ry Px.
Are these indeed legitimate observables? Well, thanks to Sects.7.9.2–7.9.3, they are
indeed Hermitian. For instance,
Lh
x =

Ry Pz −Rz Py
h = Ph
z Rh
y −Ph
y Rh
z = Pz Ry −Py Rz = Ry Pz −Rz Py = Lx.
Thismatrixmirrorsthe x-componentofthevectorproductr×p (Chap.2,Sect.2.4.2).
Here, however, we have an algebraic advantage: two matrices can combine to produce
the third one. This shows once again how clever it is to use matrices to model physical
observables.
7.10.2
Using the Commutator
The above matrices don’t commute with each other. On the contrary: thanks to
Sects.7.3.4 and 7.9.2–7.9.3, they have a nonzero commutator:

Lx, L y

=

Ry Pz −Rz Py, Rz Px −Rx Pz

=

Ry Pz, Rz Px

−

Ry Pz, Rx Pz

−

Rz Py, Rz Px

+

Rz Py, Rx Pz

= Ry Px

Pz, Rz

−Ry Rx

Pz, Pz

−Py Px

Rz, Rz

+ Py Rx

Rz, Pz

= −i ¯hRy Px −(0) −(0) + i ¯hPy Rx
= i ¯hLz.

7.10 Angular Momentum
257
The same works in the other components as well:

Lx, L y

= i ¯hLz

L y, Lz

= i ¯hLx

Lz, Lx

= i ¯hL y.
In summary, Lx, L y, and Lz don’t commute. On the contrary: each two have a nonzero
commutator—the third one (times i ¯h).
7.10.3
Ladder Operator: Raising an Eigenvalue
Let’s use the above to raise an eigenvalue. For instance, let u be an eigenvector of
Lz with the eigenvalue λ:
Lzu = λu.
Since Lz is Hermitian, λ must be real. How to raise it? For this purpose, deﬁne a new
matrix:
T ≡Lx + i L y.
It doesn’t commute with Lz. On the contrary: they have a nonzero commutator:

Lz, T

=

Lz, Lx + i L y

=

Lz, Lx

+ i

Lz, L y

= i ¯hL y + ¯hLx
= ¯hT.
Thanks to Sect.7.6.1, we can now raise λ: if T u is a nonzero vector, then it is an
eigenvector of Lz as well, with the bigger eigenvalue λ + ¯h. This way, T serves as a
ladder operator, to help “climb” up the ladder. Furthermore, this procedure can now
repeat time and again, producing bigger and bigger eigenvalues, until hitting the zero
vector, and the maximal eigenvalue of Lz.
These eigenvalues are the only values that the z-angular momentum might take.
This is indeed quantum mechanics: angular momentum is no longer continuous. On
the contrary: it may take certain discrete values only.
7.10.4
Lowering an Eigenvalue
How to lower λ? For this purpose, redeﬁne T as
T ≡Lx −i L y.

258
7
Quantum Mechanics: Algebraic Point of View
It doesn’t commute with Lz. On the contrary: they have a nonzero commutator:

Lz, T

=

Lz, Lx −i L y

=

Lz, Lx

−i

Lz, L y

= i ¯hL y −¯hLx
= −¯hT.
Thanks to Sect.7.6.1, we can now lower λ: if T u is a nonzero vector, then it is an
eigenvector of Lz as well, with a new eigenvalue: λ −¯h. This way, our new T serves
as a new ladder operator, to help go down the ladder. Furthermore, this procedure can
now repeat time and again, until hitting the zero vector, and the minimal eigenvalue
of Lz.
So far, we’ve studied the eigenvalues and eigenvectors of Lz. The same can now be
done for Lx and L y as well. Let’s combine these matrices to form the entire angular
momentum.
7.10.5
Angular Momentum
Let’s place the above matrices as blocks in a new rectangular 3n × n matrix:
L ≡
⎛
⎝
Lx
L y
Lz
⎞
⎠.
This is the nondeterministic angular momentum. It mirrors the deterministic angular
momentum r × p (Chap.2, Sect.2.4.2). In the following exercises, we’ll see a few
interesting applications.
7.11
Exercises
7.11.1
Eigenvalues and Energy Levels
1. What is an observable? Hint: a Hermitian n × n matrix.
2. May an observable have an eigenvalue with a nonzero imaginary part? Hint: a
Hermitian matrix may have real eigenvalues only (Chap.1, Sect.1.9.4).
3. How many (linearly independent) eigenvectors does an observable have? Hint:
n.
4. How many (distinct) eigenvalues may an observable have? Hint: at most n.

7.11 Exercises
259
5. A degenerate observable has at least two (linearly independent) eigenvectors
that share the same eigenvalue. How many distinct eigenvalues may such an
observable have? Hint: at most n −1.
6. Can the position matrix X be degenerate? Hint: X must have distinct elements
on its main diagonal, to stand for distinct positions that the particle may take.
7. ConsidertheHamiltonianoftheharmonicoscillator(Sect.7.7.1).IsitHermitian?
8. Is it a legitimate observable?
9. May it have an eigenvalue with a nonzero imaginary part?
10. May it have a negative eigenvalue? Hint: the number operator must have a non-
negative expectation (Sects.7.6.3–7.6.4).
11. What is the minimal eigenvalue of the Hamiltonian?
12. May it be zero? Hint: it must be bigger than the minimal eigenvalue of the number
operator, which is zero.
13. May the harmonic oscillator have no energy at all?
14. What is the minimal energy of the harmonic oscillator? Hint: this is the minimal
eigenvalue of the Hamiltonian.
15. What is the eigenvector corresponding to this eigenvalue? Hint: this is the ground
state—the state of minimal energy.
16. How does the ground state look like geometrically? Hint: a Gaussian (Fig.7.2).
17. What is the probability to have a certain amount of energy? Hint: at probabil-
ity 1, it has the minimal energy. It can’t have any other energy level. This is
deterministic.
18. Can the ground state change dynamically in time? Hint: no! Energy must remain
at its minimum.
19. Consider some other eigenvector of the Hamiltonian. Can it change dynamically
in time? Hint: no! Energy must remain the same eigenvalue of the Hamiltonian.
20. How much energy may the harmonic oscillator have? Hint: the allowed energy
levels are the eigenvalues of the Hamiltonian.
21. To model the Hamiltonian well, what must the dimension n be? Hint: n should
better be inﬁnite. Indeed, one can start from the ground state, and raise eigenval-
ues time and again, designing inﬁnitely many new states, with more and more
energy.
22. In the harmonic oscillator, is there a maximal energy? Hint: The above process
is unstoppable. Indeed, the zero vector is never hit (Sect.7.6.7).
23. Consider an angular momentum component like Lz (Sect.7.10.1). Is it Hermi-
tian?
24. Is it a legitimate observable?
25. Are its eigenvalues real? Hint: see Chap.1, Sect.1.9.4.
26. Are its eigenvectors orthogonal to each other? Hint: see Chap.1, Sect.1.9.5.
27. Normalize them to have norm 1, and be not only orthogonal but also orthonormal.
28. Consider some eigenvalue of Lz. Consider an n-dimensional state v ∈Cn.
How likely is the z-angular momentum to be the same as the above eigenvalue?
Hint: normalize v to have norm 1. Then, take its inner product with the relevant
(orthonormal) eigenvector. Finally, take the absolute value of this inner product,
and square it up.

260
7
Quantum Mechanics: Algebraic Point of View
29. Must zero be an eigenvalue of Lz? Hint: yes—look at the constant eigenvec-
tor, or any other grid function that is invariant under interchanging the x- and
y-coordinates: x ↔y (Sect.7.9.3).
30. Conclude that Lz must have a nontrivial null space.
31. Given a positive eigenvalue of Lz, show that its negative counterpart must be an
eigenvalue as well. Hint: interpret the eigenvector as a grid function. Interchange
the x- and y-spatial coordinates: x ↔y. This makes a new eigenvector, with
the negative eigenvalue.
32. Show that Lz has a few eigenvalues of the form
0, ±¯h, ±2¯h, ±3¯h, . . . .
Hint: see Sects.7.10.3–7.10.4.
33. Show that Lz may in theory have a few more eigenvalues of the form
±
¯h
2 , ±3¯h
2 , ±5¯h
2 , ±7¯h
2 , . . . .
Hint: use symmetry considerations to make sure that, in this (ﬁnite) list, the
minimal and maximal eigenvalues have the same absolute value.
7.11.2
Spin
1. Deﬁne new 3 × 3 matrices:
Sx ≡¯h
⎛
⎝
0 0 0
0 0 −i
0 i 0
⎞
⎠
Sy ≡¯h
⎛
⎝
0 0 i
0 0 0
−i 0 0
⎞
⎠
Sz ≡¯h
⎛
⎝
0 −i 0
i 0 0
0 0 0
⎞
⎠,
where i ≡√−1 is the imaginary number.
2. Are these matrices Hermitian?
3. Are they legitimate observables?
4. Show that

7.11 Exercises
261

Sx, Sy

= i ¯hSz

Sy, Sz

= i ¯hSx

Sz, Sx

= i ¯hSy.
5. Conclude that these matrices mirror the angular momentum components in
Sect.7.10.2.
6. Focus, for instance, on Sz. What are its eigenvectors and eigenvalues?
7. Show that (1, i, 0)t is an eigenvector of Sz, with the eigenvalue ¯h.
8. Interpret this eigenvector to point in the positive z-direction. Hint: its positive
y-direction has a larger phase—it is at angle π/2 ahead of the positive x-direction
(Fig.7.6 and Sects.7.2.4–7.2.5). Follow the right-hand rule: place your right
hand with your thumb pointing in the positive x-direction, and your index ﬁnger
pointing in the positive y-direction. This way, your middle ﬁnger will point in
the positive z-direction (Chap.2, Sect.2.2.4).
9. Show that (1, −i, 0)t is an eigenvector as well, with the eigenvalue −¯h.
10. Interpret this eigenvector to point in the negative z-direction (Fig.7.7).
11. Normalize these eigenvectors to have norm 1. Hint: divide by
√
2.
12. Show that (0, 0, 1)t is an eigenvector as well, with the eigenvalue 0.
13. Conclude that this eigenvector is in the null space of Sz.
14. Show that these eigenvectors are orthogonal to each other. Hint: calculate their
inner product, and don’t forget the complex conjugate.
15. Conclude that they are not only orthogonal but also orthonormal.
16. Is this as expected from a Hermitian matrix? Hint: yes—a Hermitian matrix must
have real eigenvalues and orthonormal eigenvectors.
17. Is this also as expected from Sects.7.10.3–7.10.4? Hint: yes—an eigenvalue may
be raised or lowered by ¯h.
18. Consider a new physical system: a boson. This is an elementary particle with a
new physical property: spin. This is a degenerate kind of angular momentum: it
has no value, but just direction. As such, it is more mathematical than physical.
19. Interpret Sz as a new observable, telling us the spin around the z-axis. This is
a degenerate kind of angular momentum: the boson “spins” around the z-axis
with no speciﬁc rate, but just with a speciﬁc direction: either counterclockwise
(spin-up), or clockwise (spin-down).
20. How likely is the boson to have spin-up? Hint: take the current state v ∈C3,
normalize it to have norm 1, calculate its inner product with (1, i, 0)t/
√
2, take
the absolute value of this inner product, and square it up.
21. How likely is the boson to have spin-down? Hint: do the same with (1, −i, 0)t/
√
2.
22. How likely is the boson to have spin-zero? Hint: |v3|2.
23. Repeat the above exercises for Sx as well. This makes a new observable: left- or
right-spin, around the x-axis.
24. Repeat the above exercises for Sy as well. This makes a new observable: in- or
out-spin, around the y-axis, pointing deep into the page (denoted by ‘⊗’).

262
7
Quantum Mechanics: Algebraic Point of View
y is at phase π/2
ahead of x
spin-up ⊙
imaginary axis: i
y
real axis
x
pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp
Fig.7.6
Howlikelyistheparticletohavespin-up?Taketheeigenvector(1, i)t/
√
2or(1, i, 0)t/
√
2.
Thanks to the right-hand rule, it points from the page towards your eye, as indicated by the ‘⊙’ at
the origin. Then, calculate its inner product with the (normalized) state v. Finally, take the absolute
value of this inner product, and square it up
@
y is at phase π/2 behind x
spin-down ⊗
negative imaginary axis: −i
y
real axis
x
pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp
Fig. 7.7
How likely is the particle to have spin-down? Take the eigenvector (1, −i)t/
√
2 or
(1, −i, 0)t/
√
2. Thanks to the right-hand rule, it points deep into the page, as indicated by the
‘⊗’ at the origin. Then, calculate its inner product with the (normalized) state v. Finally, take the
absolute value of this inner product, and square it up

7.11 Exercises
263
7.11.3
Pauli Matrices
1. The above spin is also called spin-one, because the maximal eigenvalue is 1
(times ¯h). Consider now a new physical system: a fermeon. (For example, an
electron or a proton or a neutron.)
2. This is a yet simpler kind of particle. It has a yet simpler kind of spin, called
spin-one-half, because its maximal observation is 1/2 (times ¯h).
3. For this purpose, consider a state of a yet smaller dimension: v ∈C2. Deﬁne the
2 × 2 Pauli matrices:
σx ≡
¯h
2
 1 0
0 −1

σy ≡
¯h
2
0 1
1 0

σz ≡
¯h
2
0 −i
i 0

.
4. Are these matrices Hermitian?
5. Are they legitimate observables?
6. Show that these matrices satisfy

σx, σy

= i ¯hσz

σy, σz

= i ¯hσx

σz, σx

= i ¯hσy.
7. Conclude that these matrices mirror the angular momentum components in
Sect.7.10.2.
8. What are the eigenvectors of σx? Hint: the standard unit vectors (1, 0)t and
(0, 1)t.
9. Are they orthonormal?
10. What are their eigenvalues?
11. What are the eigenvectors of σy? Hint: (1, 1)t and (1, −1)t.
12. Are they orthogonal to each other?
13. Normalize them to have norm 1.
14. What are their eigenvalues? Hint: ±¯h/2.
15. Is this as expected from a Hermitian matrix? Hint: yes—a Hermitian matrix must
have real eigenvalues and orthonormal eigenvectors.
16. Is this also as expected from Sects.7.10.3–7.10.4? Hint: yes—an eigenvalue may
be raised or lowered by ¯h.
17. What are the eigenvectors of σz? Hint: (1, i)t and (1, −i)t.
18. Are they orthogonal to each other? Hint: calculate their inner product, and don’t
forget the complex conjugate.
19. Normalize them to have norm 1. Hint: divide by
√
2.

264
7
Quantum Mechanics: Algebraic Point of View
20. What are their eigenvalues?
21. As in spin-one above, interpret these eigenvectors to indicate spin-up or spin-
down. Hint: see Figs.7.6–7.7.
22. Show that the Pauli matrices have the same determinant:
det (σx) = det

σy

= det (σz) = −
¯h2
4 .
23. Show that the Pauli matrices have the same square:
σ2
x = σ2
y = σ2
z =
¯h2
4 I,
where I is the 2 × 2 identity matrix.
24. Does this agree with the eigenvalues calculated above? Hint: take an eigenvector,
and apply the Pauli matrix twice.
25. Consider now a new physical system: two fermeons (say, two electrons). Could
their states be exactly the same? Hint: no! This is Pauli’s exclusion principle.
7.11.4
Polarization
1. The Pauli matrices could be used to observe not only spin-one-half but also a
completely different physical property, in a completely different physical system.
2. For this purpose, consider now a new physical system: a photon.
3. The photon is a boson. As such, it has a state in C3, to help specify its spin-one.
Still, it also has yet another state in C2, to help specify yet another physical
property: its polarization.
4. Indeed, the photon is not only a particle but also a light ray, or an electromag-
netic wave. As such, it travels in some direction: say upwards, in the positive
z-direction. At the same time, it also oscillates in the x-y plane.
5. To help observe this, the Pauli matrices could also serve as new observables. In
particular, σx tells us how likely the photon is to oscillate in the x- or y-direction:
in a (normalized) state v ≡(v1, v2)t, the probability to oscillate in the x-direction
is |v1|2, whereas the probability to oscillate in the y-direction is |v2|2.
6. At the same time, σy tells us how likely the photon is to oscillate obliquely in the
x-y plane, at an angle of ±45◦from the positive part of the x-axis. To calculate
the probability to oscillate at angle ±45◦, just take the eigenvector (1, ±1)t/
√
2
calculate its inner product with v, take the absolute value, and square it up. What
do you get in terms of v1 and v2?
7. Finally, σz tells us how likely the photon is to make circles in the x-y plane
(Figs.7.6–7.7). To calculate the probability to make circles (counter)clockwise,
take the eigenvector (1, ±i)t/
√
2, calculate its inner product with v, take the
absolute value, and square it up. What do you get in terms of v1 and v2?

Part III
Polynomials and Basis Functions
The polynomial is an algebraic object as well. Indeed, two polynomials can be
added to or multiplied by each other. Still, the polynomial is also an analytic object:
it can be differentiated and integrated, as in calculus. These two aspects can now
combine to design a special kind of function: basis function, useful in many practical
applications.
To study the polynomial, we can use tools from linear algebra. Indeed, a vector
could be used to model a polynomial and store its coefﬁcients. Still, the polyno-
mial has more algebraic operations: multiplication and composition. This way, the
polynomials make a new mathematical structure: a ring.
In turn, polynomials of a certain degree make a new vector space. Basis functions
can then be designed carefully to extend this space further. This way, they can help
designasmoothsplineinthreespatialdimensions.Thisisthekeytotheﬁnite-element
method.
Basis functions could be viewed as a special kind of vectors. Indeed, they span a
new linear space, with all sorts of interesting properties. Later on, we’ll use them in a
geometrical application: designing a new spline to approximate a given function on
a mesh of tetrahedra. Furthermore, we’ll also use them in the ﬁnite-element method,
to help solve complex models in quantum mechanics and general relativity.

Chapter 8
Polynomials and Their Gradient
The polynomial is a special kind of function, easy to deal with. We start with a
polynomial of just one independent variable: x. We discuss a few algebraic operations
that can be applied to it. In particular, we introduce a few algorithms to calculate the
value of the polynomial at a given argument.
Then, we move on to a more complicated case: polynomials of two (or even three)
independent variables: x and y (and even z). Geometrically, we are now in a higher
dimension: not only the one-dimensional axis R, but also the two-dimensional plane
R2, and even the three-dimensional space R3.
Our polynomials are then differentiated in these new domains. This way, we
obtain more advanced analytical objects: partial and directional derivatives. Although
we focus on polynomials, the discussion could be easily extended to more general
functions as well. This will be used later in advanced applications in physics and
chemistry.
8.1
Polynomial of One Variable
8.1.1
Polynomial of One Variable
What is a polynomial? Well, a real polynomial is a real function p : R →R, deﬁned
by
p(x) ≡a0 + a1x + a2x2 + · · · + anxn =
n

i=0
aixi,
where n is a nonnegative integer number (the degree), and a0, a1, a2, . . . , an are real
numbers (the coefﬁcients). Usually, it is assumed that an ̸= 0. Otherwise, it could
drop.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_8
267

268
8
Polynomials and Their Gradient
Thus, to deﬁne a concrete polynomial of degree n, it is sufﬁcient to specify its
coefﬁcients: a0, a1, a2, . . . , an. Thus, the polynomial is mirrored by the (n + 1)-
dimensional vector
(a0, a1, a2, . . . , an) ∈Rn+1.
What is a complex polynomial? Well, it is different from the above real polynomial
in one aspect only: the coefﬁcients a0, a1, a2, . . . , an (as well as the independent
variable x) can now be not only real but also complex numbers. This makes the
polynomial a complex function p : C →C, rather than a mere real function p :
R →R.
In this chapter, polynomials are studied as algebraic objects, with the arithmetic
operations that can be applied to them: addition, multiplication, composition, etc.
Furthermore, polynomials are also viewed as analytic objects that can be differenti-
ated and integrated. This will be quite useful in many applications later on.
Lateron,theoriginalpolynomialofoneindependentvariablewillalsobeextended
to more complicated polynomials of two or even three independent variables, along
with the relevant algebraic and analytic operations: partial, normal, and tangential
differentiation.
8.1.2
Adding Polynomials
The above interpretation of the polynomial p(x) as an (n + 1)-dimensional vector
is particularly useful in arithmetic operations. To see this, let q(x) be yet another
polynomial of degree m:
q(x) ≡
m

i=0
bixi.
Without loss of generality, assume that m ≤n. (Otherwise, just interchange the roles
of p and q.) This leaves two possibilities: if m = n, then we can go ahead and add
p and q. If, on the other hand, m < n, then we must ﬁrst deﬁne n −m ﬁctitious zero
coefﬁcients
bm+1 ≡bm+2 ≡· · · ≡bn ≡0,
to let q have n + 1 coefﬁcients as well.
We are now ready to deﬁne the new polynomial p + q:
(p + q)(x) ≡p(x) + q(x) =
n

i=0
(ai + bi)xi.
Thus, the original vectors have been added to each other coefﬁcient by coefﬁ-
cient. This means that vectors indeed mirror polynomials in terms of addition and
subtraction:

8.1 Polynomial of One Variable
269
(p −q)(x) ≡p(x) −q(x) =
n

i=0
(ai −bi)xi.
8.1.3
Multiplying a Polynomial by a Scalar
How to multiply the original polynomial by a given scalar c? Again, this is done
coefﬁcient by coefﬁcient:
(cp)(x) ≡c · p(x) = c
n

i=0
aixi =
n

i=0
(cai)xi.
This way, the original vector of coefﬁcients has been multiplied by cc component
by component. Again, vector mirror polynomials: not only in terms of addition but
also in terms of scalar multiplication.
8.1.4
Multiplying Polynomials
In the above, we’ve seen that a polynomial of degree n is mirrored by the (n + 1)-
dimensional vector of its coefﬁcients in terms of addition, subtraction, and multipli-
cation by a scalar. Still, the polynomial is more than that: it has yet another algebraic
operation—multiplication by another polynomial.
In Chap.1, Sect.1.7.1, and Chap.2, Sects. 2.2.2, 2.2.3, we have seen that two
vectors can be multiplied by each other in two different ways: inner product, which
produces a mere scalar, or vector product, in which two three-dimensional vectors
produce a new three-dimensional vector. Still, general vectors of higher dimension
cannot be multiplied by each other to produce a new vector. In the context of poly-
nomials, on the other hand, this is possible:
(pq)(x) ≡p(x)q(x).
After all, polynomials are just functions, which can be multiplied by each other.
Still, we’re not done yet. Indeed, we are not only interested in the value of pq
for a given argument x. After all, this is easily available: just calculate p(x) and
q(x), and multiply. Here, however, we want much more than that: the entire vector
of coefﬁcients of the new polynomial pq. This vector is useful in many applications.
The product of the two polynomials
p(x) ≡
n

i=0
aixi and q(x) ≡
m

j=0
b jx j

270
8
Polynomials and Their Gradient
Fig. 8.1 How to multiply
p(x) = a0 + a1x + a2x2 +
a3x3 by q(x) = b0 + b1x +
b2x2? Sum the terms
diagonal by diagonal: in the
kth diagonal, sum those
terms with the power xk
(0 ≤k ≤5)
is deﬁned by
(pq)(x) ≡p(x)q(x) =
n

i=0
aixi
m

j=0
b jx j =
n

i=0
m

j=0
aib jxi+ j.
Note that this double sum scans the (n + 1) × (m + 1) grid
{(i, j) | 0 ≤i ≤n, 0 ≤j ≤m} .
In this rectangular grid, each point of the form (i, j) contributes a term of the form
aib jxi+ j. Still, it makes sense to sum the contributions diagonal by diagonal, as in
Fig.8.1. After all, on the kth diagonal, each point of the form (i, j) satisﬁes i + j = k,
and contributes aib jxi+ j = aibk−ixk.
This way, the above grid is scanned diagonal by diagonal, rather than row by row.
The diagonals are indexed by the new index k = i + j = 0, 1, 2, . . . , n + m. The
inner index, the row-index i, must never exceed the original grid:
(pq)(x) =
n

i=0
m

j=0
aib jxi+ j =
n+m

k=0
min(k,n)

i=max(0,k−m)
aibk−ixk.
Thus, the new polynomial pq is associated with the vector
(c0, c1, c2, c3, . . . , cn+m) = (ck)n+m
k=0 ,
where the individual coefﬁcient ck is deﬁned by
ck ≡
min(k,n)

i=max(0,k−m)
aibk−i.

8.1 Polynomial of One Variable
271
Let’s look at a simple example, in which q is a trivial polynomial of degree m = 0:
q(x) ≡b0.
In this case, the coefﬁcients ck are simply
ck =
k

i=k
aibk−i = akb0
(0 ≤k ≤n). Thus, in this case,
(pq)(x) =
n

k=0
ckxk =
n

k=0
b0akxk = b0
n

i=0
aixi = b0 · p(x).
This agrees with the original deﬁnition of scalar times polynomial: the scalar b0 times
the original polynomial p(x) (Sect.8.1.3).
8.2
Horner’s Algorithm
8.2.1
Computing the Value of a Polynomial
Consider now a new task: for a given argument x, how to compute the value of p(x)?
The naive algorithm contains three stages:
• First, calculate the individual monomials (powers of x):
x2, x3, x4, . . . , xn.
Fortunately, this can be done recursively:
xi = x · xi−1
(i = 2, 3, 4, . . . , n).
This costs n −1 scalar multiplications.
• Then, multiply each monomial by the corresponding coefﬁcient, to obtain
a1x, a2x2, a3x3, . . . , anxn.
This costs n more multiplications.
• Finally, sum up:
a0 + a1x + a2x2 + · · · + anxn = p(x).
This costs n scalar additions.

272
8
Polynomials and Their Gradient
Thus, the total cost is 2n −1 multiplications and n additions. Could this cost be
reduced? Fortunately, it could. For this purpose, one must introduce parentheses,
and take a common factor out of them.
8.2.2
Introducing Parentheses
Consider the problem of computing
ab + ac,
where a, b, and c are some given numbers. The direct calculation requires two
multiplications to calculate the individual products ab and ac, and one addition to
sum up. Could this be done more efﬁciently? Yes—just use the distributive law:
introduce parentheses, and take the common factor a out of them:
ab + ac = a(b + c).
To calculate the right-hand side, the cost is smaller: one addition to calculate b + c,
and one multiplication to calculate a(b + c).
8.2.3
Horner’s Algorithm
The same idea can also help calculate
p(x) =
n

i=0
aixi
at a given argument x. Here, however, the task is a little more complicated: sum not
only two but also n + 1 terms. Although these terms share no common factor, the n
latter terms
a1x, a2x2, a3x3, . . . , anxn
do: the common factor x, which can be taken out of parentheses:
p(x) = a0 + xp1(x),
where the new polynomial p1(x) is deﬁned by
p1(x) = a1 + a2x + a3x2 + · · · + anxn−1 =
n

i=1
aixi−1.

8.2 Horner’s Algorithm
273
Better yet, if a1 = 0, then we could take not only x but also x2 out of parentheses. In
general, we could take factor xk out of parentheses, for some (ﬁxed maximal) k ≥1:
p(x) = a0 + xk pk(x),
where the new polynomial pk(x) is of degree n −k:
pk(x) = ak + ak+1x + ak+2x2 + · · · + anxn−k =
n

i=k
aixi−k.
This way, the value of pk(x) can be calculated recursively by the same algorithm
itself. This is Horner’s algorithm.
8.2.4
Efﬁciency of Horner’s Algorithm
Fortunately, Horner’s algorithm costs less: at most n multiplications and n additions.
To see this, use mathematical induction on the degree n. Indeed, for n = 0, p(x) is
just the constant function: p(x) ≡a0, so there are n = 0 multiplications and n = 0
additions. Now, for n = 1, 2, 3, . . ., assume that the induction hypothesis holds, so
the calculation of p1(x) costs as little as n −1 multiplications and n −1 additions.
Thus, calculating
p(x) = a0 + xp1(x)
requires just one more multiplication to calculate xp1(x), and one more addition to
calculate a0 + xp1(x), which totals n multiplications and n additions at most, as
asserted. This completes the induction step, and indeed the entire proof.
8.2.5
Composition of Polynomials
To compose two given polynomials, just mirror Horner’s algorithm. The composition
of the polynomials p and q is deﬁned by
(p ◦q)(x) ≡p(q(x)).
This formula is good enough to compute the value of p ◦q at any given argument x:
ﬁrst, compute q(x), then use it as an argument in p to compute p(q(x)). Still, here
we want more than that: we want to have the entire vector of coefﬁcients of the new
polynomial p ◦q. After all, this vector is most useful in many applications.
To deﬁne the required vector of coefﬁcients, use mathematical induction on the
degree of p: n. Indeed, for n = 0, p is just the constant function p(x) ≡a0, so

274
8
Polynomials and Their Gradient
(p ◦q)(x) = p(q(x)) = a0
as well. Now, for n = 1, 2, 3, . . ., assume that we already know how to obtain
the entire vector of coefﬁcients of pk ◦q, where pk is deﬁned in Sect.8.2.3. (This
is the induction hypothesis.) From Sect.8.1.4, we already know how to multiply
polynomials. This helps compute qk (if k > 1). (Later on, we’ll see a yet better way
to compute qk.) Furthermore, this also helps multiply qk times pk ◦q. Finally, just
add a0 to the ﬁrst coefﬁcient. This completes the induction step. This completes the
inductive (or recursive) Horner algorithm for composing two polynomials.
8.3
Decimal and Binary Numbers
8.3.1
Natural Number as a Polynomial
Every natural number k is actually a polynomial. After all, it must satisfy
10n ≤k < 10n+1
for some nonnegative integer number n. This leads to the decimal representation of
k as a ﬁnite list of digits:
anan−1an−2 · · · a1a0,
which actually stands for the polynomial
k = a0 + a1 · 10 + a2 · 102 + · · · + an · 10n =
n

i=0
ai · 10i = p(10).
In other words, the original natural number k is nothing but a polynomial, evaluated
at the argument x = 10 (the decimal base).
8.3.2
Binary Polynomial
In the above, we’ve used base 10. Still, there is nothing special about this base: we
could use base 2 as well. For this purpose, note that our natural number k must also
satisfy
2m ≤k < 2m+1
for some nonnegative integer number m. This leads to the binary representation of
k, which can actually be viewed as a binary polynomial q, evaluated at the argument
x = 2 (the binary base). The coefﬁcients of q are the binary digits

8.3 Decimal and Binary Numbers
275
b0, b1, b2, . . . , bm,
which are either 0 or 1. In its binary form, k is often written as a ﬁnite list of binary
digits (or bits):
bmbm−1bm−2 · · · b1b0.
This means that k is just the value of the polynomial q at 2:
k = b0 + b1 · 2 + b2 · 22 + · · · + bm · 2m =
m

j=0
b j2 j = q(2).
Why is this useful? Well, in this form, even a very large k could be stored most
efﬁciently on the computer. This is particularly important in coding–decoding algo-
rithms in cryptography (Chap.5 in [61]).
8.4
Implicit Horner Algorithm
8.4.1
Monomial: Individual Power
Consider now a new task: how to calculate the value of a monomial? More speciﬁ-
cally, for a given x, how to calculate xk efﬁciently? For this purpose, the binary form
k = q(2) comes handy, although implicitly.
The naive algorithm to compute xk is rather expensive: it requires k −1 multipli-
cations to compute
xi = x · xi−1, i = 2, 3, 4, . . . , k.
Still, this algorithm has an advantage: as a by-product, we have not only xk but also
x2, x3, x4, . . ., xk−1. This might be worthwhile if we need all these new monomials.
But what if we don’t? Is there a more efﬁcient algorithm?
Fortunately, there is: Horner’s algorithm, in its implicit form. This way, xk can be
calculated in at most 2m multiplications, where 2m ≤k < 2m+1. Fortunately, 2m is
as small as 2 log2 k, which is usually far smaller than k −1.
Indeed, from Horner’s algorithm, we have
k = q(2) = b0 + 2q1(2),
where the degree of q1 is smaller than the degree of q. This way,
xk = xq(2) = xb0+2q1(2) = xb0x2q1(2) = xb0 
x2q1(2) =

x ·

x2q1(2) if b0 = 1

x2q1(2)
if b0 = 0.

276
8
Polynomials and Their Gradient
Why is this efﬁcient? Because to calculate the right-hand side, one needs just two
more multiplications:
• one to calculate x2 to help calculate (x2)q1(2) recursively,
• and another one to multiply (x2)q1(2) by x (if b0 = 1).
It is easy to prove by mathematical induction that the total cost is as small as 2m
multiplications, where m is the degree of q.
To simplify the above algorithm, let’s eliminate q, and leave it implicit. After all,
b0 is just the unit binary digit in
k = q(2) = b0 + 2q1(2).
Thus, if k is even, then
b0 = 0 and q1(2) = k
2.
If, on the other hand, k is odd, then
b0 = 1 and q1(2) = k −1
2
.
Thus, the algorithm takes the more explicit form
xk =
 x · (x2)(k−1)/2
if k is odd
(x2)k/2
if k is even.
In this form, the algorithm uses k only, as required. q, on the other hand, is never
mentioned any more.
For every polynomial p, we can now use the same idea to compute pk efﬁciently:
just mirror the above algorithm.
8.5
Differentiation and Integration
8.5.1
Derivative of a Polynomial
So far, we have treated the polynomial as an algebraic object, with arithmetic opera-
tions like addition and multiplication. Furthermore, we also looked at it as a function,
and calculated its value at a given argument x. Next, let’s treat it as an analytic func-
tion, which can be differentiated and integrated.
Consider again a polynomial of degree n:
p(x) =
n

i=0
aixi.

8.5 Differentiation and Integration
277
Its derivative is a new polynomial of degree max(0, n −1):
p′(x) =
0
if n = 0
n
i=1 aiixi−1 = n−1
i=0 (i + 1)ai+1xi if n > 0.
This is a new polynomial in x. As such, it can be differentiated as well to produce
the second derivative of p:
p′′(x) = d2 p
dx2 (x) = (p′(x))′.
This is a new polynomial of degree max(0, n −2). As such, it can be differentiated
as well to produce the third derivative of p, and so on. In general, for i = 0, 1, 2, . . .,
the ith derivative of p is deﬁned recursively by
p(i) ≡
 p
if i = 0

p(i−1)′ if i > 0.
In this notation, the zeroth derivative is just the function itself:
p(0) ≡p,
and its derivative is
p(1) ≡p′.
Furthermore, the nth derivative of p is just a constant, or a polynomial of degree 0:
p(n) = ann!.
For this reason, every higher derivative must vanish:
p(i) ≡0,
i > n.
8.5.2
Indeﬁnite Integral
Could you guess a new function, whose derivative is p? This is called the primitive
function of p. It is also called indeﬁnite integral (not to be confused with deﬁnite
integral, deﬁned later). For our polynomial
p(x) =
n

i=0
aixi,

278
8
Polynomials and Their Gradient
the primitive function is a new polynomial of degree n + 1:
P(x) =
n

i=0
ai
i + 1xi+1 =
n+1

i=1
ai−1
i
xi.
Indeed, the derivative of P is just the original polynomial p:
P′(x) = p(x),
as required. This is why P is also called the antiderivative of p.
8.5.3
Deﬁnite Integral over an Interval
The indeﬁnite integral can now be used for a geometrical purpose: calculating an area
in the Cartesian plane. For this purpose, the original polynomial p is also interpreted
as a geometrical object. After all, it makes a graph in the x-y Cartesian plane. What
is the area underneath this graph?
More precisely, let’s bound the area from all four sides. For this purpose, assume
that the graph of p is above the x-axis. This way, our area is already bounded from
two sides: from below by the x-axis, and from above by the graph itself. To bound it
from the left and the right as well, just issue two verticals from the x-axis upwards:
at x = a on the left, and at x = b on the right (where a < b are some ﬁxed real
numbers).
What is the area of this region? It is just
 b
a
p(x)dx = P(b) −P(a).
This is the fundamental theorem of calculus. Note that this area may well be negative,
if p is mostly negative, and its graph is mostly underneath the x-axis.
Here are some elementary examples. If p(x) = x (a linear polynomial), then
P(x) = x2/2, so
 b
a
xdx = b2 −a2
2
= (b −a)a + b
2
.
This is just the length of the interval, times the value of p at its midpoint. This is
called the trapezoidal (or the trapezoid, or the trapezium) rule.
If, on the other hand, p(x) = x2 (a quadratic polynomial), then P(x) = x3/3, so
 b
a
x2dx = b3 −a3
3
.

8.5 Differentiation and Integration
279
Finally, if p(x) ≡1 is just the constant function, then P(x) = x, so
 b
a
dx = b −a,
which is just the length of the original interval [a, b].
When a = 0 and b = 1, we have the unit interval [0, 1]. In this case, the above
formula simpliﬁes to read
 1
0
p(x)dx = P(1) −P(0) = P(1) =
n+1

i=1
ai−1
i
.
8.6
Polynomial of Two Variables
8.6.1
Polynomial of Two Independent Variables
So far, we’ve considered a polynomial of one independent variable: x. Next, let’s
consider a polynomial of two independent variables: x and y.
What is this? Well, a real polynomial of two variables is a function of the form
p : R2 →R
that can be written as
p(x, y) =
n

i=0
ai(x)yi,
where x and y are real arguments, and ai(x) (0 ≤i ≤n) is a real polynomial in one
independent variable: x only.
Likewise, a complex polynomial in two independent variables is a function
p : C2 →C
with the same structure as above, except that x and y can now be not only real but
also complex numbers, and the polynomials ai(x) can now be complex polynomials
of one variable.
8.6.2
Arithmetic Operations
How to carry out arithmetic operations between polynomials of two variables? The
same as before (Sects.8.1.2–8.1.4). The only difference is that the ai’s are no longer

280
8
Polynomials and Their Gradient
scalars but polynomials (in x) in their own right. Fortunately, we already know how
to add or multiply them by each other.
In summary, consider two polynomials of two variables:
p(x, y) =
n

i=0
ai(x)yi
and q(x, y) =
m

j=0
b j(x)y j
(for some natural numbers m ≤n). How to add them to each other? Well, if m < n,
then deﬁne a few dummy zero polynomials:
bm+1 ≡bm+2 ≡· · · ≡bn ≡0.
We are now ready to add:
(p + q)(x, y) = p(x, y) + q(x, y) =
n

i=0
(ai + bi)(x)yi,
where
(ai + bi)(x) = ai(x) + bi(x)
is just the sum of polynomials of one variable,
Furthermore, how to multiply p times q? Like this:
(pq)(x, y) = p(x, y)q(x, y)
=
	 n

i=0
ai(x)yi

 ⎛
⎝
m

j=0
b j(x)y j
⎞
⎠
=
n

i=0
⎛
⎝
m

j=0
(aib j)(x)yi+ j
⎞
⎠.
This is just the sum of n polynomials of two variables, which we already know how
to do.
8.7
Differentiation and Integration
8.7.1
Partial Derivatives
So far, we’ve looked at the polynomial

8.7 Differentiation and Integration
281
p(x, y) =
n

i=0
ai(x)yi
as an algebraic object, with two arithmetic operations: addition and multiplication.
Fortunately, it can also be viewed as an analytic object, with a new analytic operation:
partial differentiation.
For this purpose, let’s view y as a ﬁxed parameter, and differentiate p(x, y) as a
function of x only. The result is called the partial derivative of p with respect to x:
px(x, y) ≡
n

i=0
a′
i(x)yi,
where a′
i(x) is the derivative of ai(x).
Now,let’sworktheotherwayaround:view x asaﬁxedparameter,anddifferentiate
p as a function of y only. This is the partial derivative of p with respect to y:
py(x, y) ≡
0
if n = 0
n
i=1 ai(x)iyi−1 = n−1
i=0 (i + 1)ai+1(x)yi if n > 0.
Note that both partial derivatives are polynomials of two variables in their own right.
Together, they make a pair, or a two-dimensional vector: the gradient.
8.7.2
The Gradient
Let px serve as the ﬁrst component, and py as the second component in a new
two-dimensional vector. This makes the gradient of p at the point (x, y):
∇p(x, y) ≡
 px(x, y)
py(x, y)

.
Thus, the gradient of p is actually a vector function that not only takes but also returns
a two-dimensional vector:
∇p : R2 →R2.
8.7.3
Integral over the Unit Triangle
As an analytic object, p(x, y) can be not only differentiated but also integrated. This
could be viewed as an extension of the fundamental theorem of calculus (Sect.8.5.3).
Where is the integration carried out? For this purpose, consider the so-called unit
triangle

282
8
Polynomials and Their Gradient
Fig. 8.2 The unit triangle t
@
@
@
@
@
0
1
0
1
Fig. 8.3 Integration over the
unit triangle: for each ﬁxed
x, integrate over the vertical
0 ≤y ≤1 −x
@
@
@
@
@
1 −x
t ≡{(x, y) | 0 ≤x, y, x + y ≤1}
(Fig.8.2). This way, the unit triangle sits on its base: the unit interval [0, 1]. From
this interval, issue many verticals upwards, in the y-direction. To integrate on t, just
integrate on each and every individual vertical.
Our aim is to calculate the volume under the surface that p makes in the Cartesian
space: the two-dimensional surface (or manifold) z = p(x, y) in the x-y-z space.
For simplicity, assume that p is positive, so this surface lies above the x-y plane.
What is the volume underneath it? More precisely, what is the volume between the
surface and the horizontal x-y plane below it?
Still, to calculate a volume, we must be yet more precise. Our three-dimensional
region must be bounded not only from above and below but also from all other
sides. For this purpose, from the unit triangle t, issue three “walls” upwards, in the
z-direction. This way, we got what we wanted: a closed three-dimensional region.
What is its volume? It is just the integral of p over t:
 
t
p(x, y)dxdy.
Note that this volume may well be negative, if p is mostly negative. For simplicity,
however, we assume that p is positive.
To calculate this integral, let 0 ≤x < 1 be a ﬁxed parameter, as in Fig.8.3.
Furthermore, let P(x, y) be the indeﬁnite integral of p(x, y) with respect to y:
P(x, y) ≡
n

i=0
ai(x)
i + 1 yi+1 =
n+1

i=1
ai−1(x)
i
yi.
This way, P(x, y) is characterized by the property that its partial derivative with
respect to y is the original polynomial p(x, y):

8.7 Differentiation and Integration
283
Py(x, y) = p(x, y).
Fortunately, we’ve already split t into many verticals, issuing from its base upwards,
in the y-direction. To integrate over t, just integrate on each and every vertical:
 
t
p(x, y)dxdy =
 1
0
 1−x
0
p(x, y)dy

dx =
 1
0
P(x, 1 −x)dx.
Thanks to the fundamental theorem of calculus, we already know how to calculate
this. This is indeed the volume of our three-dimensional region, as required.
8.7.4
Second Partial Derivatives
Let us now return to differentiation. We’ve already differentiated p with respect to x
and y to produce the partial derivatives px and py. Fortunately, these are polynomials
of two variables in their own right. As such, they can be differentiated as well, to
produce the second partial derivatives of p. For example, the mixed partial derivative
of p is
pxy(x, y) ≡(px(x, y))y .
From Sect.8.7.1, partial differentiation (or derivation) is insensitive to the order in
which it is carried out:
pxy(x, y) =
n−1

i=0
(i + 1)a′
i+1(x)yi = pyx(x, y).
This is also called the (1, 1)st partial derivative of p. After all, x1 = x and y1 = y,
so
pxy(x, y) = px1y1(x, y).
With this notation, the (0, 0)th partial derivative of p is nothing but p itself:
px0y0(x, y) = p(x, y).
The process may now continue yet more: differentiate a second partial derivative,
and obtain a new partial derivative of order three. For example, the (2, 1)st partial
derivative of p is
px2y1(x, y) ≡pxxy(x, y) ≡(pxx(x, y))y .
In general, the (i, j)th partial derivative of p is deﬁned diagonal by diagonal, using
mathematical induction on its order: i + j = 0, 1, 2, 3, . . . (Fig.8.4):

284
8
Polynomials and Their Gradient
6
-
s
s
s
s
s
s
s
s
s
s
j + j =
0
1
2
3
...
j
· · ·
i
(i, j) = (0, 3)
Fig. 8.4 To deﬁne the (i, j)th partial derivative, March diagonal by diagonal: use mathematical
induction on i + j = 0, 1, 2, 3, . . .
pxi y j ≡
⎧
⎨
⎩
p
if i = j = 0

pxi−1y j
x if
i > 0

pxi y j−1
y if
j > 0.
Fortunately, if both i > 0 and j > 0, then these formulas agree with each other.
Indeed, in the same mathematical induction, we could also prove that reordering
doesn’t matter:

pxi y j−1
y =

pxi−1y j−1
xy =

pxi−1y j−1
yx =

pxi−1y j
x .
This completes the induction step, and indeed the entire deﬁnition, as required.
To count the partial derivatives, let’s use some results from discrete math. How
many distinct partial derivatives of order up to (and including) m are there? Well,
from Sect.10.15 in [60, 63], the answer is
 m + 2
2

= (m + 2)!
2! · m!
= (m + 1)(m + 2)
2
.
How many distinct partial derivatives of order m exactly are there? The answer to
this is
m + 2 −1
2 −1

=
m + 1
1

= m + 1.

8.7 Differentiation and Integration
285
Indeed, here they are:
px0ym, px1ym−1, px2ym−2, . . . , pxm y0.
8.7.5
Degree
The original form
p(x, y) =
n

i=0
ai(x)yi
is somewhat incomplete: the degree is not necessarily n. To uncover the degree, a
yet more explicit form is needed.
For this purpose, one must write each polynomial ai(x) more explicitly:
ai(x) =

j≥0
ai, jx j,
where the ai, j’s are some scalars. This way, p can now be written as
p(x, y) =

i≥0
ai(x)yi =

i≥0

j≥0
ai, jx j yi.
The degree of p is the maximal sum i + j for which there is here a nontrivial
monomial of the form ai, jx j yi (with ai, j ̸= 0). Note that, unlike in a polynomial of
one variable, here the degree may be greater than n.
In a polynomial p(x, y) of degree m, what is the maximal number of distinct
monomials? Well, this is the same as the total number of distinct pairs of the form
(i, j), with i + j ≤m. From Sect.10.15 in [60, 63], this number is just
 m + 2
2

= (m + 2)!
m! · 2!
= (m + 1)(m + 2)
2
.
8.8
Polynomial of Three Variables
8.8.1
Polynomial of Three Independent Variables
A polynomial of three independent variables is deﬁned by

286
8
Polynomials and Their Gradient
p(x, y, z) =
n

i=0
ai(x, y)zi,
where the coefﬁcients ai are now polynomials of two independent variables: x and
y.
How to add, subtract, or multiply polynomials of three variables? Fortunately,
this is done in the same way as in Sects.8.1.2–8.1.4. There is just one change: the
ai’s are now polynomials of two variables in their own right. Fortunately, we already
know how to “play” with them algebraically.
8.9
Differentiation and Integration
8.9.1
Partial Derivatives
Let us view both y and z as ﬁxed parameters, and differentiate p(x, y, z) as a function
of x only. This produces the partial derivative with respect to x:
px(x, y, z) ≡
n

i=0
(ai)x(x, y)zi.
In this sum, the coefﬁcients ai are differentiated with respect to x as well.
Similarly, let us view both x and z as ﬁxed parameters, and differentiate p(x, y, z)
as a function of y only. This produces the partial derivative with respect to y:
py(x, y, z) ≡
n

i=0
(ai)y(x, y)zi.
Finally, let us view both x and y as ﬁxed parameters, and differentiate p(x, y, z) as
a function of z only. This produces the partial derivative with respect to z:
pz(x, y, z) ≡
0
if n = 0
n
i=1 ai(x, y)izi−1 = n−1
i=0 (i + 1)ai+1(x, y)zi if n > 0.
Together, these partial derivatives make a new three-dimensional vector: the gradient.
8.9.2
The Gradient
Once placed in a three-dimensional vector, these partial derivatives form the gradient
of p:

8.9 Differentiation and Integration
287
∇p(x, y, z) ≡
⎛
⎝
px(x, y, z)
py(x, y, z)
pz(x, y, z)
⎞
⎠.
Often, the gradient is nonconstant: it may change from point to point. Only when p
is linear is its gradient constant.
Thus, the gradient of p is actually a vector function (or ﬁeld):
∇p : R3 →R3.
8.9.3
Vector Field (or Function)
Avectorﬁeldcouldactuallybeevenmoregeneralthanthat.Forthispurpose,consider
three real functions (not necessarily polynomials):
f ≡f (x, y, z)
g ≡g(x, y, z)
h ≡h(x, y, z).
Let’splacetheminanewthree-dimensionalvector.Thismakesanewvectorfunction:
⎛
⎝
x
y
z
⎞
⎠→
⎛
⎝
f
g
h
⎞
⎠≡
⎛
⎝
f (x, y, z)
g(x, y, z)
h(x, y, z)
⎞
⎠.
This is also called a vector ﬁeld. In what follows, we consider a differentiable vector
ﬁeld, in which f , g, and h are differentiable functions.
8.9.4
The Jacobian
So far, we’ve deﬁned the gradient of the polynomial p. This is a column vector. The
transpose gradient, on the other hand, is a row vector:
∇t p(x, y, z) ≡

px(x, y, z), py(x, y, z), pz(x, y, z)

.
Let us now apply the transpose gradient to a vector ﬁeld, row by row:
∇t
⎛
⎝
f
g
h
⎞
⎠≡
⎛
⎝
∇t f
∇tg
∇th
⎞
⎠=
⎛
⎝
fx fy fz
gx gy gz
hx hy hz
⎞
⎠.

288
8
Polynomials and Their Gradient
This 3 × 3 matrix is the Jacobian of the original vector ﬁeld.
In summary, the original mapping
⎛
⎝
x
y
z
⎞
⎠→
⎛
⎝
f
g
h
⎞
⎠
has the Jacobian matrix
∂( f, g, h)
∂(x, y, z) ≡∇t
⎛
⎝
f
g
h
⎞
⎠=
⎛
⎝
fx fy fz
gx gy gz
hx hy hz
⎞
⎠.
(The dependence on the spatial variables x, y, and z is often omitted for short.) The
Jacobian matrix will be particularly useful in integration.
8.9.5
Integral over the Unit Tetrahedron
The unit tetrahedron T is a three-dimensional region, with four corners (or vertices):
(0, 0, 0), (1, 0, 0), (0, 1, 0),
and (0, 0, 1)
(Fig.8.5). Furthermore, T is bounded by four triangles (faces or sides). In particular,
T sits on its base: the unit triangle t in Fig.8.2. In terms of analytic geometry, T is
deﬁned by
T ≡{(x, y, z) | 0 ≤x, y, z, x + y + z ≤1} .
The original polynomial
p(x, y, z) ≡
n

i=0
ai(x, y)zi
Fig. 8.5 The unit
tetrahedron T

8.9 Differentiation and Integration
289
can now be integrated in T . For this purpose, from each point on the base of T ,
just issue a vertical upwards, in the z-direction, until it hits the upper face of T . To
integrate in T , just integrate on each and every individual vertical.
To carry out this plan, let P(x, y, z) be the indeﬁnite integral of p(x, y, z) with
respect to z:
P(x, y, z) ≡
n

i=0
ai(x, y)
i + 1 zi+1 =
n+1

i=1
ai−1(x, y)
i
zi.
With this new deﬁnition, we are now ready to integrate:
  
T
p(x, y, z)dxdydz =
 
t
 1−x−y
0
p(x, y, z)dz

dxdy
=
 
t
(P(x, y, 1 −x −y) −P(x, y, 0)) dxdy
=
 
t
P(x, y, 1 −x −y)dxdy.
Fortunately, this is just a two-dimensional integration in t, which we already know
how to do (Sect.8.7.3).
8.10
Normal and Tangential Derivatives
8.10.1
Directional Derivative
Let us now return to the subject of differentiation. So far, we’ve differentiated
p(x, y, z) in a Cartesian direction: x, y, or z. This produces the three partial deriva-
tives. Let us now generalize this, and differentiate p in just any spatial direction as
well.
For this purpose, let n be a ﬁxed three-dimensional vector in R3:
n ≡
⎛
⎝
n1
n2
n3
⎞
⎠= (n1, n2, n3)t ∈R3.
Assume also that n is a unit vector:
∥n∥2 ≡

n2
1 + n2
2 + n2
3 = 1.
Deﬁne pn(x, y, z) as the directional derivative of p(x, y, z) in the direction pointed
at by n. This is the inner product of n with the gradient of p at (x, y, z):

290
8
Polynomials and Their Gradient
pn(x, y, z) ≡(n, ∇p(x, y, z)) = nt∇p(x, y, z) = n1 px(x, y, z) + n2 py(x, y, z) + n3 pz(x, y, z).
For short, we often omit the dependence on the speciﬁc point (x, y, z):
pn ≡(n, ∇p) = nt∇p = n1 px + n2 py + n3 pz.
This still depends on (x, y, z) implicitly, and may still change from point to point.
Only when p is linear is the directional derivative constant.
Next, let’s look at an interesting special case.
8.10.2
Normal Derivative
Assume now that
n ≡(n1, n2, n3)t
is normal (or orthogonal, or perpendicular) to a particular plane in R3. In other words,
n makes a zero inner product with the difference between any two points that lie on
the plane. In the case, the above directional derivative is also called normal derivative.
As a matter of fact, n could be normal to a mere line in R3. For example, consider
the line
{(x, y, 0) | x + y = 1} ⊂R3.
(This line contains one of the edges in the unit tetrahedron in Fig.8.5.) Consider two
distinct points on this line:
(x, 1 −x, 0) and (ˆx, 1 −ˆx, 0),
where 0 ≤x, ˆx ≤1, and x ̸= ˆx. The difference between these two points is just
(x, 1 −x, 0) −(ˆx, 1 −ˆx, 0) = (x −ˆx, 1 −x −(1 −ˆx), 0) = (x −ˆx, ˆx −x, 0).
This difference is orthogonal to two constant vectors:
(1, 1, 0)t and (0, 0, 1)t.
Thus, to be normal to the above line, n could be either
n =
⎛
⎝
0
0
1
⎞
⎠= (0, 0, 1)t or n =
1
√
2
⎛
⎝
1
1
0
⎞
⎠=
1
√
2
(1, 1, 0)t
or just any (normalized) linear combination of these two vectors.

8.10 Normal and Tangential Derivatives
291
8.10.3
Differential Operator
It is sometimes convenient to use differential operators: ∂/∂x means partial differ-
entiation with respect to x, ∂/∂y means partial differentiation with respect to y, and
∂/∂z means partial differentiation with respect to z. With these new notations, the
operator of normal differentiation can be written as
∂
∂n ≡n1
∂
∂x + n2
∂
∂y + n3
∂
∂z .
For example, if
n =
1
√
2
(1, 1, 0)t,
then the operator of normal differentiation takes the form
∂
∂n =
1
√
2
 ∂
∂x + ∂
∂y

.
For yet another example, consider the plane
{(x, y, z) | x + y + z = 1} ⊂R3.
(This plane contains the upper face of the unit tetrahedron in Fig.8.5.) The normal
vector to this plane is
n =
1
√
3
⎛
⎝
1
1
1
⎞
⎠.
Thus, in this case, the operator of normal differentiation is just
∂
∂n =
1
√
3
 ∂
∂x + ∂
∂y + ∂
∂z

.
8.10.4
High-Order Normal Derivatives
Because the normal derivative of a polynomial is a polynomial in its own right, it has
a normal derivative as well. The normal derivative of the normal derivative is called
the second normal derivative.
This can be extended to a yet higher order. Indeed, by mathematical induction on
i = 1, 2, 3, . . ., the (i + 1)st normal derivative is just the normal derivative of the ith
normal derivative.

292
8
Polynomials and Their Gradient
8.10.5
Tangential Derivative
So far, we have assumed that n was normal to a given line or plane in the three-
dimensional Cartesian space. Assume now that n is no longer normal but rather
parallel to the line or the plane. This way, n is orthogonal to every vector that is
normal to the original line or plane. In fact, if n was shifted to issue from the original
line or plane rather than from the origin, then it would be contained in that line or
plane, and would indeed be tangent to it as well.
The directional derivative in the direction pointed at by n is then called the tangen-
tial derivative. Furthermore, we also have a yet higher order: the tangential derivative
of the tangential derivative is called the second tangential derivative, or the tangential
derivative of order 2.
Again, this is just mathematical induction: the zeroth tangential derivative is the
original function itself. Now, for i = 0, 1, 2, . . ., the (i + 1)st tangential derivative
(or the tangential derivative of order i + 1) is deﬁned as the tangential derivative of
the ith tangential derivative. This will be useful later in the book.
8.11
High-Order Partial Derivatives
8.11.1
High-Order Partial Derivatives
For polynomials of two variables, high-order partial derivatives have already been
deﬁned in Sect.8.7.4. For polynomials of three variables, on the other hand, high-
order partial derivatives have also been used implicitly in Sect.8.10.4. Here, however,
we deﬁne them more explicitly, including mixed partial derivatives.
For this purpose, recall that the partial derivative of a polynomial of three variables
is a polynomial of three variables in its own right. As such, it can be differentiated
once again. For example, px can be differentiated with respect to z to produce
pxz(x, y, z) ≡(px(x, y, z))z.
From Sect.8.9.1, the order in which the partial differentiation takes place is imma-
terial:
pxz(x, y, z) =
n−1

i=0
(i + 1) (ai+1)x (x, y)zi = pzx(x, y, z).
Furthermore, let’s use differential operators (Sect.8.10.3) to deﬁne the (i, j, k)th
partial derivative:
pxi y j zk(x, y, z) =
	 ∂
∂x
i  ∂
∂y
 j  ∂
∂z
k
p

(x, y, z).

8.11 High-Order Partial Derivatives
293
For example, the (2, 1, 0)th partial derivative of p is just
px2y1z0(x, y, z) = pxxy(x, y, z).
In particular, the (0, 0, 0)th partial derivative of p is just p itself:
px0y0z0(x, y, z) ≡p(x, y, z).
The order of the (i, j, k)th partial derivative is the sum i + j + k. With this termi-
nology, the (i, j, k)th partial derivative could have been deﬁned more explicitly by
mathematical induction on the order i + j + k = 0, 1, 2, 3, . . .:
pxi y j zk ≡
⎧
⎪⎪⎨
⎪⎪⎩
p
if i = j = k = 0

pxi−1y j zk
x if
i > 0

pxi y j−1zk
y if
j > 0

pxi y j zk−1
z if
k > 0.
As discussed in Sect.8.7.4, the same mathematical induction could also have been
used to prove that these deﬁnitions always agree with each other.
To count the partial derivatives, let’s use some results from discrete math. Thanks
to Sect.10.15 in [60, 63], the total number of partial derivatives of order up to (and
including) m is
m + 3
3

= (m + 3)!
3! · m!
= (m + 1)(m + 2)(m + 3)
6
.
Furthermore, the total number of partial derivatives of order m exactly is
m + 3 −1
3 −1

=
m + 2
2

= (m + 2)!
2! · m!
= (m + 1)(m + 2)
2
.
8.11.2
The Hessian
The second partial derivatives deﬁned above can now be placed in a new 3×3 matrix.
This is the Hessian.
For this purpose, recall that the transpose gradient of p is the row vector containing
the partial derivatives of p:
∇t p ≡(∇p)t = (px, py, pz)
(Sect.8.9.4). The dependence on the spatial variables x, y, and z is omitted here for
short.

294
8
Polynomials and Their Gradient
Now, to each individual component in this row vector, apply the gradient operator
∇:
∇∇t p = (∇px | ∇py | ∇pz) =
⎛
⎝
pxx pyx pzx
pxy pyy pzy
pxz pyz pzz
⎞
⎠.
This is the Hessian of p: the 3 × 3 matrix that contains the second partial derivatives
of p.
Fortunately, a mixed partial derivative is insensitive to the order in which the
differentiation is carried out:
pxy = pyx
pxz = pzx
pyz = pzy.
Thus, the Hessian is a symmetric matrix, equal to its transpose:
∇∇t p =
⎛
⎝
pxx pyx pzx
pxy pyy pzy
pxz pyz pzz
⎞
⎠=
⎛
⎝
pxx pxy pxz
pyx pyy pyz
pzx pzy pzz
⎞
⎠= ∇t∇p.
In other words, the Hessian is the Jacobian of the gradient.
8.11.3
Degree
As discussed in Sect.8.7.5, the polynomials of two variables ai(x, y) could be written
more explicitly:
ai(x, y) =

j≥0

k≥0
ai, j,kxk y j,
where the ai, j,k’s are some scalars. Using this formulation, the original polynomial
of three variables could be written as
p(x, y, z) =
n

i=0

j≥0

k≥0
ai, j,kxk y jzi.
Thus, the degree of p could be much greater than n: it is the maximal sum i + j + k
for which there is a nontrivial monomial of the form ai, j,kxk y jzi (with ai, j,k ̸= 0).
How many monomials are there? Well, thanks to Sect.10.15 in [60, 63], a poly-
nomial of degree m may contain at most

8.11 High-Order Partial Derivatives
295
 m + 3
3

= (m + 3)!
m! · 3!
= (m + 1)(m + 2)(m + 3)
6
distinct monomials.
8.12
Exercises
8.12.1
Taylor Series of Sine and Cosine
1. Let u = (ui)0≤i≤n be an (n + 1)-dimensional vector, and v = (vi)0≤i≤m be an
(m+1)-dimensional vector. Complete both u and v into (n+m+1)-dimensional
vectors by adding zero dummy components:
un+1 = un+2 = · · · = un+m = 0
and
vm+1 = vm+2 = · · · = vn+m = 0.
2. Deﬁne the convolution of u and v, denoted by u ∗v. This is a new (n + m + 1)-
dimensional vector, whose components are
(u ∗v)k =
k

i=0
uivk−i,
0 ≤k ≤n + m.
3. Show symmetry:
u ∗v = v ∗u.
4. Use u to deﬁne the new polynomial
p(x) =
n

i=0
uixi =
n+m

i=0
uixi.
5. Likewise, use v to deﬁne the new polynomial
q(x) =
m

i=0
vixi =
n+m

i=0
vixi.
6. Write the product pq in terms of the convolution vector u ∗v:

296
8
Polynomials and Their Gradient
(pq)(x) =
n+m

k=0
(u ∗v)kxk.
7. The inﬁnite Taylor series of the exponent function exp(x) = ex around zero is
exp(x) = 1 + x + x2
2! + x3
3! + x4
4! + · · · =
∞

n=0
xn
n! .
For a moderate |x|, for a sufﬁciently large k, this could be truncated after k + 1
terms:
exp(x) .=
k

n=0
xn
n! .
Write an efﬁcient version of Horner’s algorithm to compute this. Hint: avoid
dividing by a factorial.
8. Do the same for the sine function:
sin(x) = x −x3
3! + x5
5! −x7
7! + · · · =
∞

n=0
(−1)n
x2n+1
(2n + 1)!.
9. Do the same for the cosine function:
cos(x) = 1 −x2
2! + x4
4! −x6
6! + · · · =
∞

n=0
(−1)n x2n
(2n)!.
10. Use the above Taylor series to show that, for a given imaginary number ix,
exp(ix) = cos(x) + i · sin(x).
11. Use the above to write the polar decomposition of a a complex number.
12. Use the above Taylor series to show that the derivative of exp(x) is exp(x) itself:
exp′(x) = exp(x).
13. Use the above Taylor series to show that the derivative of sin(x) is
sin′(x) = cos(x).
14. Use the above Taylor series to show that the derivative of cos(x) is
cos′(x) = −sin(x).
15. Conclude that

8.12 Exercises
297
cos′′(x) = −cos(x).
16. Conclude also that
sin′′(x) = −sin(x).
17. Conclude that both sin(x) and cos(x) solve the differential equation
u′′(x) = −u(x)
for the unknown function u(x).
18. Conclude that every linear combination of sin(x) and cos(x) solves this differ-
ential equation as well.
19. Find the unique linear combination that not only solves the above differential
equation but also satisﬁes the boundary conditions
u′(0) = u(π) = 0.

Chapter 9
Basis Functions: Barycentric
Coordinates in 3-D
Thanks to the above background, we are now ready to design a special kind of
function: basis function (or B-spline). This will be the key to the ﬁnite-element
method, with advanced applications in modern physics and chemistry.
We start from a simple case: just one tetrahedron. In it, the basis function is deﬁned
as a polynomial. To each adjacent tetrahedron, the basis function is then extended,
and deﬁned as a different polynomial. This way, the basis function is piecewise
polynomial.
Still, at the interface between two adjacent tetrahedra, these polynomials must
agree smoothly with each other. In other words, at a face shared by two adjacent
tetrahedra, the basis function must be continuous. Moreover, across an edge shared
by two adjacent tetrahedra, the basis function must be smooth: have a continuous
gradient. These properties will be used later to design a smooth 3-D. This is the
basis for the ﬁnite-element method, used in advanced applications in physics and
chemistry later on.
9.1
Tetrahedron and Its Mapping
9.1.1
General Tetrahedron
So far, we’ve considered a special tetrahedron: the unit tetrahedron T , vertexed at
(0, 0, 0), (1, 0, 0), (0, 1, 0),
and (0, 0, 1)
(Fig.8.5). Consider now a more general tetrahedron t, with more general corners (or
vertices): k, l, m, n ∈R3. Each corner is a point (or a three-dimensional column
vector) in R3. This way, our new tetrahedron t is denoted by
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_9
299

300
9
Basis Functions: Barycentric Coordinates in 3-D
Fig. 9.1 A general
tetrahedron t, vertexed at k,
l, m, and n
t ≡(k, l, m, n) ⊂R3
(Fig.9.1). Here, the order of the corners is determined arbitrarily in advance.
Let us now map T onto t. For this purpose, consider the vectors leading from k to
the three other corners in t. Together, these column vectors form a new 3× 3 matrix:
St ≡(l −k | m −k | n −k) .
Later on, we’ll introduce the notion of regularity: t must be nondegenerate. In other
words, its corners mustn’t lie on the same plane. This way, St is nonsingular: it has
a nonzero determinant.
More than that: t is rather thick, and not too thin. This means that St is far from
being singular: its determinant is far away from zero.
To map T onto t, deﬁne the new mapping
Et
⎛
⎝
⎛
⎝
x
y
z
⎞
⎠
⎞
⎠≡k + St
⎛
⎝
x
y
z
⎞
⎠.
This way, the corners of T map to the corresponding corners of t:
Et
⎛
⎝
⎛
⎝
0
0
0
⎞
⎠
⎞
⎠= k
Et
⎛
⎝
⎛
⎝
1
0
0
⎞
⎠
⎞
⎠= l
Et
⎛
⎝
⎛
⎝
0
1
0
⎞
⎠
⎞
⎠= m
Et
⎛
⎝
⎛
⎝
0
0
1
⎞
⎠
⎞
⎠= n.
Furthermore, in the terminology in Chap.8, Sect.8.9.4, St is the Jacobian of Et.

9.1 Tetrahedron and Its Mapping
301
Clearly, the inverse mapping maps t back onto T :
E−1
t
⎛
⎝
⎛
⎝
x
y
z
⎞
⎠
⎞
⎠= S−1
t
⎛
⎝
⎛
⎝
x
y
z
⎞
⎠−k
⎞
⎠.
For this reason, S−1
t
is the Jacobian of E−1
t
.
9.1.2
Integral over a Tetrahedron
Let F be an integrable function in t. How to integrate in t? This could be rather
difﬁcult. After all, t is a general tetrahedron. Better integrate in T —the unit tetrahe-
dron. Still, F was never deﬁned in T . What is deﬁned in T is the composite function
F ◦Et. Let’s go ahead and integrate it in T :
  
t
F(x, y, z)dxdydz = | det(St)|
  
T
(F ◦Et) (x, y, z)dxdydz.
Let’s consider a common example that arises often in practice. Assume that F is a
product of two integrable functions:
F(x, y, z) = f (x, y, z)g(x, y, z),
(x, y, z) ∈t.
To mirror these functions, deﬁne new composite functions in T :
˜f = f ◦Et and ˜g = g ◦Et.
In other words,
f = ˜f ◦E−1
t
and g = ˜g ◦E−1
t
in t. In this case, the above formula takes the form
  
t
f gdxdydz = | det(St)|
  
T
( f ◦Et)(g ◦Et)dxdydz
= | det(St)|
  
T
˜f ˜gdxdydz.
This will be useful below.

302
9
Basis Functions: Barycentric Coordinates in 3-D
9.1.3
The Chain Rule
The chain rule tells us how to differentiate a composite function [14]. Thanks to it,
the gradient of f can be written in terms of the gradient of ˜f .
Still, f and its gradient are evaluated in t. ˜f and its gradient, on the other hand,
are evaluated in T , not t. Therefore, they must be composed with E−1
t
. This way, we
can now take the transpose gradient ∇t, and obtain a row vector:
∇t f (x, y, z) = ∇t ˜f

E−1
t
(x, y, z)

S−1
t
,
(x, y, z) ∈t.
The dependence on x, y, and z is often dropped, for short:
∇t f =
		
∇t ˜f

◦E−1
t

S−1
t
in t. Likewise, we can also take the gradient of g:
∇g(x, y, z) = S−t
t ∇˜g

E−1
t
(x, y, z)

,
(x, y, z) ∈t.
Once the dependence on x, y, and z is dropped, this takes the shorter form
∇g = S−t
t (∇˜g) ◦E−1
t
in t. We can now take the inner product of these two gradients, and integrate in t:
  
t
∇t f ∇gdxdydz = | det(St)|
  
T
∇t ˜f S−1
t
S−t
t ∇˜gdxdydz.
This formula will be most useful later. We are now ready to design special polyno-
mials in t.
9.1.4
Degrees of Freedom
Consider an unspeciﬁed polynomial p(x, y, z) of degree m. How to specify it? Well,
one way is to specify its coefﬁcients ai, j,k (0 ≤i+ j+k ≤m). How many coefﬁcients
are there to specify?
Well, thanks to discrete math, we already know the answer:
 m + 3
3

= (m + 3)!
m! · 3!
= (m + 1)(m + 2)(m + 3)
6
(Chap.8, Sect.8.11.3). For m = 5, for example, there is a need to specify

9.1 Tetrahedron and Its Mapping
303
5 + 3
3

= 6 · 7 · 8
6
= 56
coefﬁcients of the form ai, j,k (0 ≤i + j + k ≤5). After all, even a zero coefﬁcient
must be speciﬁed as such.
This is a rather explicit way to specify a polynomial. Is there a more implicit
way? After all, one could also specify any 56 independent pieces of information, or
degrees of freedom.
For this purpose, a suitable piece of information is not necessarily a coefﬁcient:
it could also be the value of p (or any partial or directional derivative of p) at any
point in the Cartesian space.
To specify a polynomial p of degree ﬁve, for example, let’s look at the original
tetrahedron t, and pick degrees of freedom symmetrically in it. At each corner of t,
let’s specify the partial derivatives of order 0, 1, and 2.
How many such partial derivatives are there? From Chap.8, Sect.8.11.1, the
answer is
2 + 3
3

= 4 · 5
2
= 10.
So far, we have already speciﬁed a total of 40 degrees of freedom: ten at each corner.
These, however, are not enough: to characterize p uniquely, we must specify 16
more. For this purpose, let’s look at the edges of t. In each edge, let’s look at the
midpoint, and pick two nontangential derivatives, in a direction that is not parallel
to the edge.
For example, consider the edge (k, l), leasing from k to l. Its midpoint is (k+l)/2.
First, let’s look at the difference: l −k. Without loss of generality, assume that
it is “nearly” in the z-direction. This means that, its maximal coordinate is the third
coordinate:
|(l −k)3| ≥max (|(l −k)1| , |(l −k)2|) .
In this case, at the midpoint (k + l)/2, it wouldn’t make sense to pick the z-partial
derivative: it is nearly tangent, and has no chance to be normal. Instead, it would
make more sense to pick the x- and y-partial derivatives, and specify
px
k + l
2

and
py
k + l
2

.
Since there are six edges, this speciﬁes 12 more degrees of freedom.
So far, we’ve speciﬁed 52 degrees of freedom. Still, these are not enough: we
need four more. What to pick? Well, at each side midpoint, let’s pick a nontangential
derivative.
For example, look at the face △(k, l, m), vertexed at k, l, and m. Consider its
normal vector: the vector product (l−k)×(m−k). Without loss of generality, assume
that it is “nearly” in the z-direction. In other words, its z-coordinate is maximal:

304
9
Basis Functions: Barycentric Coordinates in 3-D
|((l −k) × (m −k))3| ≥max (|((l −k) × (m −k))1| , |((l −k) × (m −k))2|) .
In this case, at the midpoint (k + l + m)/3, it wouldn’t make sense to pick the
x- or y-partial derivative: they are nearly tangent, and have no chance to be normal.
Instead, it would make more sense to pick the z-partial derivative, and specify
pz
k + l + m
3

.
Since there are four sides, this speciﬁes four more degrees of freedom. This makes
a total of 56 degrees of freedom, as required.
Below, we’ll see that these degrees of freedom are indeed independent of each
other: they specify p uniquely, as required. To do this, we need some more geometry
and algebra.
9.2
Barycentric Coordinates
9.2.1
Barycentric Coordinates
Our original tetrahedron t could also be represented in a different way. For this
purpose, let d = (x, y, z)t be some point in t. Usually, d is in the interior of t. Still,
this is not a must: d could also lie on the boundary of t: its faces, edges, or even
corners. Anyway, d is a convex combination of the corners:
d = λkk + λll + λmm + λnn,
where λk, λl, λm, and λn are nonnegative real numbers that sum to 1:
λk + λl + λm + λn = 1.
The coefﬁcients λk, λl, λm, and λn are the barycentric coordinates of d
[10, 43, 52, 57]. Together, they make a new four-dimensional vector:
λ ≡(λk, λl, λm, λn)t.
Note that all the above vectors are column vectors, although not quite of the same
dimension: k, l, m, n, and d are three-dimensional column vectors, whereas λ is a
four-dimensional column vector. Thus, the above convex combination can also be
written as a four-dimensional system:
d
1

=
k
1

l
1

m
1

n
1

λ.

9.2 Barycentric Coordinates
305
This way, d is written in terms of λ. In fact, this is just a projective mapping: the
“oblique” real projective space
{λ | λk + λl + λm + λn = 1}
is mapped to the “horizontal” real projective space in Chap.6, Sects.6.9.1 and 6.9.2.
9.2.2
The Inverse Mapping
Fortunately, this mapping could also be inverted, to give λ in terms of d. Indeed, the
above 4 × 4 matrix is nonsingular: it has a nonzero determinant. To see this, let’s
multiply it by a new matrix U—a 4 × 4 upper triangular matrix:
U ≡
⎛
⎜⎜⎝
1 −1 −1 −1
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠.
Clearly,
det(U) = 1.
Therefore,
det
k l m n
1 1 1 1

= det
k l m n
1 1 1 1

det(U)
= det
k l m n
1 1 1 1

U

= det
k l −k m −k n −k
1
0
0
0

= det
k
St
1 0 0 0

= −det(St)
̸= 0.
9.2.3
Geometrical Interpretation
From Cramer’s rule (Chap.2, Sect.2.1.5), we can now have the barycentric coordi-
nates in their explicit form. The ﬁrst one, for instance, is

306
9
Basis Functions: Barycentric Coordinates in 3-D
λk(d) =
det
d l m n
1 1 1 1

det
k l m n
1 1 1 1

=
det
d l m n
1 1 1 1

U

det
k l m n
1 1 1 1

U

=
det
 d l −d m −d n −d
1
0
0
0

det
k l −k m −k n −k
1
0
0
0

=
det
d
S(d,l,m,n)
1 0
0
0

det
k
St
1 0 0 0

= −det

S(d,l,m,n)

−det(St)
= det

S(d,l,m,n)

det(St)
.
This gives λk an interesting geometrical meaning. To see this, just draw four new
edges, leading from d to the corners of t. This splits t into four disjoint subtetrahedra,
each vertexed at d and three corners of t.
Now, look at one particular subtetrahedron, vertexed at d, l, m, and n. Calculate
its volume, and divide it by the volume of t. This is indeed λk.
In summary, λk is the relative volume of (d, l, m, n): the subtetrahedron that lies
across from k in t. Similar formulas can also be written for the three other barycentric
coordinates:
λl(d) = det

S(k,d,m,n)

det(St)
λm(d) = det

S(k,l,d,n)

det(St)
λn(d) = det

S(k,l,m,d)

det(St)
.
Why do the barycentric coordinate sum to 1? We can now interpret this not only
algebraically but also geometrically: the four subtetrahedra sum to the original tetra-
hedron t.

9.2 Barycentric Coordinates
307
Furthermore, it is now easy to see that, at the corners, the barycentric coordinates
are either 0 or 1:
λi(j) =
 1
if
i = j
0
if i ̸= j
i, j ∈{k, l, m, n}.
After all, in the special case in which d is a corner, one subtetrahedron is t, whereas
the three others are degenerate. This nice result will be useful later.
9.2.4
The Chain Rule and Leibnitz Rule
As discussed above, we now have λ in terms of d:
λ =
k l m n
1 1 1 1
−1  d
1

.
Thus, λ ≡λ(d) is a function of d. More speciﬁcally, the individual barycentric
coordinates are functions of d as well:
λk ≡λk(d), λl ≡λl(d), λm ≡λm(d),
and λn ≡λn(d).
We can now differentiate these functions with respect to x, y, and z. For this purpose,
let’s look at the above inverse matrix:
k l m n
1 1 1 1
−1
.
Look at its ﬁrst, second, and third columns. Together, they make a 4 × 3 rectangular
matrix: the Jacobian of λ with respect to d:
∂λ
∂d = ∂(λk, λl, λm, λn)
∂(x, y, z)
.
Fortunately, we have Cramer’s formula to help write the individual matrix elements
explicitly (Chap.2, Sect.2.1.4). Assume that this has already been done. This way,
∂λ/∂d can now be used in the chain rule. This will be quite useful: it will help
differentiate a composite function of λ ≡λ(d).
To see this, let
f (λ) and g(λ)
be two differentiable functions. This means that they have a gradient with respect to
λ:

308
9
Basis Functions: Barycentric Coordinates in 3-D
∇λ f ≡
⎛
⎜⎜⎝
∂f/∂λk
∂f/∂λl
∂f/∂λm
∂f/∂λn
⎞
⎟⎟⎠
and ∇λg ≡
⎛
⎜⎜⎝
∂g/∂λk
∂g/∂λl
∂g/∂λm
∂g/∂λn
⎞
⎟⎟⎠.
Later on, we’d like to differentiate a product like f g with respect to λ. This is done
as in Leibnitz rule:
∇λ( f g) = (∇λ f ) g + f ∇λg = g∇λ f + f ∇λg.
Likewise, the transpose gradient (with respect to λ) is
∇t
λ( f g) = g∇t
λ f + f ∇t
λg.
To both sides of this equation, apply ∇λ from the left:
∇λ∇t
λ( f g) = ∇λ

g∇t
λ f + f ∇t
λg

= ∇λg∇t
λ f + g∇λ∇t
λ f + ∇λ f ∇t
λg + f ∇λ∇t
λg.
Now, because λ ≡λ(d), both f and g are actually composite functions of x, y,
and z. As such, they can be differentiated with respect to x, y, and z, to form the
gradient. Fortunately, we have the chain rule to help do this:
∇t f = ∇t
λ f
∂λ
∂d

and ∇g =
∂λ
∂d
t
∇λg.
Thus, the Hessian of f (with respect to x, y, and z) is
∇∇t f = ∇∇t
λ f
∂λ
∂d

=
∂λ
∂d
t
∇λ∇t
λ f
∂λ
∂d

.
9.2.5
Integration in Barycentric Coordinates
Now, look at the oblique real projective space, introduced at the end of Sect.9.2.1:
λn = 1 −λk −λl −λm.
This is easy to integrate in. Indeed, as in Sect.9.1.2, take the inner product of the
above gradients, and integrate in t:

9.2 Barycentric Coordinates
309
  
t
∇t f ∇gdxdydz
= | det(St)|
 1
0
dλk
 1−λk
0
dλl
 1−λk−λl
0
dλm∇t
λ f
∂λ
∂d
 ∂λ
∂d
t
∇λg,
where the gradients on the right, ∇t
λ f and ∇λg, are evaluated at four-dimensional
points of the form
(λk, λl, λm, 1 −λk −λl −λm) .
Thus, this is just an integral over the unit tetrahedron T :
(λk, λl, λm) ∈T.
Fortunately, we already know how to do this (Chap.8, Sect.8.9.5). This will be useful
later. Our functions f and g will need to be smooth not only in t but also outside it.
Let’s see how such a function should indeed be extended smoothly to a neighbor
tetrahedron as well.
9.3
How to Match Two Tetrahedra?
9.3.1
Continuity Across an Edge
In Sect.9.1.4, we’ve already introduced 56 independent degrees of freedom. In what
sense are they independent? In the following sense:
A polynomial of degree ﬁve with 56 vanishing degrees of freedom must be
identically zero.
Let’s go ahead and prove this.
For this purpose, consider a polynomial p(x, y, z) of degree ﬁve (or less).
Consider the edge
(k, l),
leading from corner k to corner l in t. In this edge, λm = λn = 0. Thus, unless p
vanishes throughout the entire edge, p mustn’t contain a factor λm or λn.
For a start, assume that p has 20 vanishing degrees of freedom: ten vanishing
partial derivatives (of order 0, 1, and 2) at k, and the same at l. So, how does p look
like in the edge? Well, along the edge, look at p, and at its ﬁrst and second tangential
derivatives. Both vanish at both endpoints: k and l. Thus, once restricted to the edge,
p must contain cubic factors of the form λ3
k and λ3
l :
p |(k,l)= λ3
kλ3
l · · · = λ3
k (1 −λk)3 · · ·

310
9
Basis Functions: Barycentric Coordinates in 3-D
(times an unknown factor, possibly zero). But p is of degree ﬁve at most, and mustn’t
contain such a big factor. Therefore, p must vanish throughout the entire edge:
p |(k,l)≡0.
As a bonus, p also has a zero tangential derivative along the entire edge. Furthermore,
in the face △(k, l, m), p must contain (at least a linear) factor λm:
p |△(k,l,m)= λm · · ·
(times an unknown factor, possibly zero). Later on, we’ll use this to redeﬁne p
continuously outside t as well.
9.3.2
Smoothness Across an Edge
Furthermore, under some more conditions, the above factor is not only linear but
also quadratic:
p |△(k,l,m)= λ2
m · · ·
(times an unknown factor, possibly zero). To have this, let’s look at the gradient of
p: ∇p. How does it look like in the edge (k, l)? Well, at the endpoints, we already
know that ∇p vanishes. So, at both k and l, p has zero tangential derivative. Thus,
once restricted to the edge, ∇p must contain quadratic factors of the form λ2
k and λ2
l :
∇p |(k,l)= βλ2
kλ2
l = βλ2
k (1 −λk)2 .
Here, since ∇p is a polynomial of degree four (or less), β must be a constant three-
dimensional vector.
Assume now that p has two more vanishing degrees of freedom: at the midpoint
(k + l)/2, it also has two vanishing nontangential derivatives. Thus, thanks to the
bonus at the end of Sect.9.3.1, its gradient vanishes there:
∇p
k + l
2

= (0, 0, 0)t.
Therefore, we must have β = (0, 0, 0)t, so ∇p vanishes throughout the entire edge:
∇p |(k,l)≡(0, 0, 0)t.

9.3 How to Match Two Tetrahedra?
311
Therefore, in the face △(k, l, m), the original polynomial p must contain (not only
linear but also) quadratic factor:
p |△(k,l,m)= λ2
m · · ·
(times an unknown factor, possibly zero).
9.3.3
Continuity Across a Side
Look now at the face △(k, l, m), and the edges in it. Assume now that p has vanishing
degrees of freedom not only in (k, l) but also in the two other edges: (l, m) and (m, k).
This way, p has a total of 36 vanishing degrees of freedom: ten vanishing partial
derivatives (of orders 0, 1, and 2) at k, l, and m, and two vanishing nontangential
derivatives at (k + l)/2, (l + m)/2, and (m + k)/2.
Thus, the discussion in Sect.9.3.2 applies to all three edges: not only (k, l) but
also (l, m) and (m, k). Therefore, p must contain a factor as big as
p |△(k,l,m)= λ2
mλ2
kλ2
l · · · .
But p is of degree ﬁve at most, so it must vanish throughout the entire face:
p |△(k,l,m)≡0.
As a bonus, p also has two vanishing tangential derivative along the entire face. This
will be useful later.
Unfortunately, the gradient of p, although vanishes in the edges, not necessarily
vanishes throughout the entire face. For example, ∇p may still take there the nonzero
form
(∇p) |△(k,l,m)≡γλkλlλm,
where γ is a constant nonzero three-dimensional vector.
Asamatteroffact,evenif∇p happenedtovanishatthesidemidpoint(k+l+m)/3
as well, it might not vanish throughout the entire side. After all, as a polynomial of
degree four, it might still take the nonzero form
(∇p) |△(k,l,m) ≡γλkλlλm
1
3 −λk

or (∇p) |△(k,l,m) ≡γλkλlλm
1
3 −λl

or (∇p) |△(k,l,m) ≡γλkλlλm
1
3 −λm

.

312
9
Basis Functions: Barycentric Coordinates in 3-D
So, we are stuck: we can say no more about ∇p. Fortunately, we can still say more
about p itself. In fact, since p vanishes throughout △(k, l, m), it must contain a linear
factor of the form
p = λn · · · .
Assume now that p has vanishing degrees of freedom in the three other faces as well.
This way, in total, p has 52 vanishing degrees of freedom: ten at each corner, and
two at each edge midpoint. We can then do the same in each face. As a result, p must
contain the factor
p = λkλlλmλn · · · .
More precisely, since p is of degree ﬁve at most, it must be of the form
p = λkλlλmλn (αkλk + αlλl + αmλm + αnλn) ,
where αk, αl, αm, and αn are some scalars.
9.4
Piecewise-Polynomial Function
9.4.1
Independent Degrees of Freedom
Assumenowthat p alsohasfourmorevanishingdegreesoffreedom:ateachsidemid-
point, it has a vanishing nontangential derivative. Thanks to the bonus in Sect.9.3.3,
at each side midpoint, p actually has a zero gradient. So, in total, p indeed has 56
vanishing degrees of freedom in t. Is p identically zero?
Well, to ﬁnd out, let’s calculate its explicit gradient at the side midpoint
(k + l + m)/3. Clearly, at this point, the barycentric coordinates are
λk = λl = λm = 1
3
and λn = 0.
Thanks to the chain rule (Sect.9.2.4), we can calculate the gradient indirectly: apply
∇λ rather than ∇. This way, we differentiate with respect to λk, λl, λm, and λn rather
than x, y, or z.
What happens when such a differentiation is carried out? Well, upon evaluating
at λn = 0, the term that contains λ2
n drops. The three other terms, on the other hand,
must be differentiated with respect to λn, or they’d vanish as well. In summary, at
(k + l + m)/3, we have

9.4 Piecewise-Polynomial Function
313
(0, 0, 0) = ∇t p
= ∇t
λ p
∂λ
∂d

= ∇t
λ (λkλlλmλn (αkλk + αlλl + αmλm + αnλn))
∂λ
∂d

= λkλlλm∇t
λλn (αkλk + αlλl + αmλm)
∂λ
∂d

= λkλlλm(0, 0, 0, 1) (αkλk + αlλl + αmλm)
∂λ
∂d

= 3−4(0, 0, 0, 1) (αk + αl + αm)
∂λ
∂d

.
Now, the fourth row in ∂λ/∂d can’t be (0, 0, 0), or λn would be just a constant,
which is impossible. Therefore, we must have
αk + αl + αm = 0.
The same could be done at the three other side midpoints as well. In summary, we
have four linear equations:
αl + αm + αn = 0
αk + αm + αn = 0
αk + αl + αn = 0
αk + αl + αm = 0.
More compactly, this could be written as a four-dimensional linear system:
⎛
⎜⎜⎝
0 1 1 1
1 0 1 1
1 1 0 1
1 1 1 0
⎞
⎟⎟⎠
⎛
⎜⎜⎝
αk
αl
αm
αn
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
0
0
0
0
⎞
⎟⎟⎠.
Look at this 4 × 4 matrix. Is it singular? Fortunately not. Indeed, it has no zero
eigenvalue. In fact, it has only two eigenvalues: 3 and −1. To see this, just write it as
⎛
⎜⎜⎝
0 1 1 1
1 0 1 1
1 1 0 1
1 1 1 0
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
1 1 1 1
1 1 1 1
1 1 1 1
1 1 1 1
⎞
⎟⎟⎠−I,
where I is the 4 × 4 identity matrix.

314
9
Basis Functions: Barycentric Coordinates in 3-D
On the right-hand side, look at the ﬁrst matrix. It can be written as a column vector
times a row vector:
⎛
⎜⎜⎝
1 1 1 1
1 1 1 1
1 1 1 1
1 1 1 1
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
1
1
1
1
⎞
⎟⎟⎠(1, 1, 1, 1).
What are the eigenvectors? Well, (1, 1, 1, 1)t is an eigenvector, with the eigenvalue 4.
Every vector orthogonal to (1, 1, 1, 1)t, on the other hand, has the eigenvalue 0. Thus,
once I is subtracted, we get the eigenvalues 3 and −1, as asserted.
Thus, the only solution to the above linear system is
αk = αl = αm = αn = 0.
Thus, our original polynomial is identically zero:
p ≡0.
This means that our original 56 degrees of freedom are indeed independent of each
other, as asserted. This is the key for designing a basis function.
9.4.2
Smooth Piecewise-Polynomial Function
Let t1 and t2 be two neighbor tetrahedra that share a joint edge: t1∩t2. Let p1 and p2 be
two different polynomials of degree ﬁve (or less), deﬁned in t1 and t2, respectively.
Assume also that, in their joint edge t1 ∩t2, both p1 and p2 share the same 22
degrees of freedom: ten partial derivatives at each endpoint, and two nontangential
derivatives at the midpoint. As discussed above, p1 −p2 and ∇p1 −∇p2 must then
vanish throughout the entire edge.
Now, in the union t1 ∪t2, deﬁne a new piecewise-polynomial function:
u(x, y, z) ≡
 p1(x, y, z)
if (x, y, z) ∈t1
p2(x, y, z)
if (x, y, z) ∈t2.
How do we know that u is smooth? Well, in t1 ∩t2, u is continuous: it is the same
from both sides. Furthermore, ∇u is continuous as well: it is the same from both
sides as well. These properties will be useful later.

9.4 Piecewise-Polynomial Function
315
9.4.3
Continuous Piecewise-Polynomial Function
Assume now that t1 and t2 share not only a mere edge but also a complete face: t1 ∩t2
is now a face, not just an edge. Moreover, assume now that both p1 and p2 share
the same 36 degrees of freedom in t1 ∩t2: ten at each vertex, and two more at each
edge midpoint. As discussed above, p1 −p2 must then vanish throughout the entire
face. Thus, u is continuous throughout the entire face t1 ∩t2: it is the same from both
sides. This property will be useful later: it will help design a piecewise-polynomial
basis function.
9.5
Basis Functions
9.5.1
Side-Midpoint Basis Function
We are now ready to design our ﬁrst basis function. For a start, we deﬁne it in t as a
polynomial of degree ﬁve. Later on, we’ll extend it continuously outside t as well.
Consider, for example, the side midpoint
w ≡k + l + m
3
.
Recall that, at w, the barycentric coordinates are
λk = λl = λm = 1
3
and λn = 0.
Assume that
∂λn
∂z ̸= 0.
After all, in Sect.9.1.4, we’ve already assumed that this partial derivative is far away
from zero. This way, at w, the degree of freedom is just the z-partial derivative. Now,
let’s deﬁne the corresponding basis function ψw in t:
ψw ≡αλkλlλmλn
1
3 −λn

,
where α is a constant scalar, to be speciﬁed later.
Why is this a good candidate for a basis function? Because it has just one nonzero
degree of freedom. Indeed,

316
9
Basis Functions: Barycentric Coordinates in 3-D
• at every corner in t, there is a triple product of vanishing barycentric coordinates.
For this reason, the partial derivatives of order 0, 1, and 2 vanish there, as required.
• Furthermore,ateveryedgemidpoint,therearetwovanishingfactors,sothedegrees
of freedom (the nontangential derivatives) vanish there as well.
• Moreover, at every side midpoint but w, there are two vanishing factors, so the
degree of freedom (the nontangential derivative) vanishes there as well.
• So, our only task is to pick α cleverly, to make sure that the ﬁnal degree of freedom
(the nontangential derivative at w) is correct.
So, what should α be? Pick α to make the z-partial derivative equal to 1 at w. How
to do this? As did in Sect.9.4.1, use the chain rule. This way, at w,
(·, ·, 1) = ∇tψw
= ∇t
λψw
∂λ
∂d

= ∇t
λ

αλkλlλmλn
1
3 −λn
 ∂λ
∂d

= αλkλlλm∇t
λλn
1
3 −λn
 ∂λ
∂d

= 3−4α(0, 0, 0, 1)
∂λ
∂d

.
More explicitly, deﬁne
α ≡34
∂λn
∂z
.
This way, only at w is the degree of freedom equal to 1. All the rest vanish, as
required. Thus, ψw is our ﬁrst basis function.
In practice, we may have not only one tetrahedron t but also a complete mesh. In
particular, t may have a neighbor tetrahedron from the other side of △(k, l, m). How
to deﬁne ψw there? Just use the same approach, and deﬁne ψw as a different polyno-
mial there. Still, at the joint side, both deﬁnitions agree with each other (Sect.9.4.3).
This way, ψw is indeed a continuous piecewise-polynomial function, as required.
In the rest of the mesh, on the other hand, ψw is deﬁned as zero. This way, ψw is
indeed a proper basis function: continuous and piecewise-polynomial, with just one
nonzero degree of freedom: at w only.
The above could be done not only at w but also at any other side midpoint. Next,
we deﬁne yet another kind of basis function.

9.5 Basis Functions
317
9.5.2
Edge-Midpoint Basis Function
Consider now the edge midpoint
h ≡k + l
2
.
Clearly, at h, the barycentric coordinates are
λk = λl = 1
2
and λm = λn = 0.
Assume that, in the Jacobian ∂λ/∂d, the 2 × 2 lower left block is nonsingular:
det
∂(λm, λn)
∂(x, y)

̸= 0.
After all, in Sect.9.1.4, we’ve already assumed that this determinant is far away from
zero. This way, at h, the degrees of freedom are just the x- and y-partial derivatives.
Let’s start with the x-partial derivative. Let’s introduce the corresponding basis
function, ψh,1:
ψh,1 ≡λ2
kλ2
l (αmλm + αnλn) ,
where αm and αn are constant scalars, to be speciﬁed later.
Why is this a good candidate for a basis function? Because it has just one nonzero
degree of freedom. Indeed,
• at every corner of t, there is here a product of at least three vanishing barycentric
coordinates. For this reason, the partial derivatives of order 0, 1, and 2 vanish there,
as required.
• Furthermore, at every edge midpoint but h, the quadratic factor λ2
k or λ2
l vanishes,
so the nontangential derivatives vanish there as well.
• Moreover, at every side midpoint across from h, the quadratic factor λ2
k or λ2
l
vanishes, so the nontangential derivative vanishes there as well.
• Later on, we’ll also make sure that the degrees of freedom vanish at those side
midpoints nearby h.
• Before doing this, we have more urgent business: to make sure that the degrees of
freedom are correct at h itself. For this purpose, pick the above α’s cleverly.
So, what should αm and αn be? Pick them to make the x-partial derivative equal to
1, and the y-partial derivative equal to 0 at h. In other words, at h,

318
9
Basis Functions: Barycentric Coordinates in 3-D
(1, 0, ·) = ∇tψh,1
= ∇t
λψh,1
∂λ
∂d

= ∇t
λ

λ2
kλ2
l (αmλm + αnλn)
 ∂λ
∂d

= λ2
kλ2
l ∇t
λ (αmλm + αnλn)
∂λ
∂d

= λ2
kλ2
l

αm∇t
λλm + αn∇t
λλn
 ∂λ
∂d

= 2−4 (αm(0, 0, 1, 0) + αn(0, 0, 0, 1))
∂λ
∂d

= 2−4 (0, 0, αm, αn)
∂λ
∂d

.
These are two linear equations in two unknowns: αm and αn. What is the coefﬁcient
matrix? It is a familiar block: the 2 × 2 lower left block in the original Jacobian
∂λ/∂d. More precisely, we actually look at the transpose system, so we actually
look at the transpose block. Anyway, it has the same determinant: nonzero. So, it is
nonsingular, as required. Therefore, αm and αn can be solved for uniquely.
So, at h, the degrees of freedom are correct. What about the rest of the degrees
of freedom in t? Well, From Sect.9.2.4, most of them already vanish, as required.
Only at nearby side midpoints may they still be nonzero. How to ﬁx this?
Consider, for example, the side midpoint w, discussed in the previous section.
Fortunately, we already have a basis function: ψw in t. Let’s go ahead and subtract a
multiple of it from ψh,1 in t:
ψh,1 ←ψh,1 −

ψh,1

z (w)ψw.
Once this substitution is made, we have

ψh,1

z (w) = 0,
as required. Fortunately, this substitution doesn’t spoil the rest of the degrees of
freedom, which remain correct.
The same kind of subtraction can also be made for the other nearby side midpoint:
(k + l + n)/3. In t, subtract
ψh,1 ←ψh,1 −

ψh,1

z
k + l + n
3

ψ(k+l+n)/3.

9.5 Basis Functions
319
Once this substitution is made, we also have

ψh,1

z
k + l + n
3

= 0,
as required. Fortunately, this substitution doesn’t spoil any other degree of freedom.
Thus, in its ﬁnal form, ψh,1 makes a proper basis function in t.
So far, we’ve deﬁned ψh,1 in t only. It is now time to extend it to the entire
mesh as well. First, what about those neighbor tetrahedra that share (k, l) as their
joint edge? Repeat the same procedure there as well. This way, in each edge-sharing
tetrahedron, ψh,1 is deﬁned as a different polynomial of degree ﬁve. Still, across
(k, l), ψh,1 remains smooth (Sect.9.4.2).
In the rest of the tetrahedra in the mesh, which don’t use (k, l) as an edge, ψh,1
is deﬁned as zero. This way, ψh,1 is indeed a proper basis function throughout the
entire mesh.
So far, we’ve focused on the x-partial derivative at h. Now, let’s consider the
y-partial derivative. For this purpose, in the beginning of the above development,
just replace (1, 0, ·) by (0, 1, ·). This produces a new function ψh,2, corresponding
to the y-partial derivative at h, as required.
The same could be done not only at h but also at any other edge midpoint. Next,
we move on to yet another kind of basis function: corner basis function.
9.5.3
Hessian-Related Corner Basis Function
Consider the corner n ∈t. Clearly, at n, the barycentric coordinates are
λk = λl = λm = 0 and λn = 1.
Let’s deﬁne the basis function corresponding to the xx-partial derivative at n:
ψn,5 ≡λ3
n

i,j∈{k,l,m}
αi,jλiλj.
Why is this a good candidate for a basis function? Because it has just one nonzero
degree of freedom. Indeed,
• thanks to the cubic factor λ3
n, ψn,5 has vanishing degrees of freedom at k, l, and m,
as required.
• This is also true at the edge and side midpoints, at least those that lie across from n.
• Later on, we’ll make sure that this is also true at those that lie nearby n.
• Before doing this, we have more urgent business: to make sure that the degrees of
freedom at n are correct. For this purpose, we must pick the α’s cleverly.

320
9
Basis Functions: Barycentric Coordinates in 3-D
Thanks to symmetry, we may assume here that
αi,j = αj,i,
so we’re actually looking for six unknown α’s. To ﬁnd them, solve the following six
equations at n:
⎛
⎝
1 0 0
0 0 0
0 0 0
⎞
⎠= ∇∇tψn,5
=
∂λ
∂d
t 
∇λ∇t
λψn,5
 ∂λ
∂d

=
∂λ
∂d
t
∇λ∇t
λ
⎛
⎝λ3
n

i,j∈{k,l,m}
αi,jλiλj
⎞
⎠
∂λ
∂d

=
∂λ
∂d
t
⎛
⎝2λ3
n

i,j∈{k,l,m}
αi,j∇λλi∇t
λλj
⎞
⎠
∂λ
∂d

= 2
∂λ
∂d
t
⎛
⎜⎜⎝
αk,k αk,l αk,m 0
αl,k αl,l αl,m 0
αm,k αm,l αm,m 0
0
0
0
0
⎞
⎟⎟⎠
∂λ
∂d

= 2
∂(λk, λl, λm)
∂(x, y, z)
t
⎛
⎝
αk,k αk,l αk,m
αl,k αl,l αl,m
αm,k αm,l αm,m
⎞
⎠
∂(λk, λl, λm)
∂(x, y, z)

.
More explicitly, deﬁne
⎛
⎝
αk,k αk,l αk,m
αl,k
αl,l
αl,m
αm,k αm,l αm,m
⎞
⎠≡1
2
∂(λk, λl, λm)
∂(x, y, z)
−t
⎛
⎝
1 0 0
0 0 0
0 0 0
⎞
⎠
∂(λk, λl, λm)
∂(x, y, z)
−1
.
This way, ψn,5 has the correct degrees of freedom at n as well.
Still, this is not good enough. To become a proper basis function, this function
must now be modiﬁed. For each nearby side midpoint, subtract a multiple of a
basis function like that deﬁned in Sect.9.5.1 in t. Furthermore, for each nearby edge
midpoint, subtract a multiple of two basis functions like those deﬁned in Sect.9.5.2
in t.
This deﬁnes our new basis function in t. How to extend it to those neighbor
tetrahedra that share n as their joint corner? In each of these, just repeat the above
procedure, and deﬁne ψn,5 as a different polynomial of degree ﬁve. In the rest of
the mesh, on the other hand, deﬁne it as zero. This extends ψn,5 into a continuous
piecewise-polynomial basis function in the entire mesh.

9.5 Basis Functions
321
The same method could also be used to design ﬁve more basis functions, corre-
sponding to the xy-, yy-, xz-, yz-, and zz-partial derivative at n. In fact, to deﬁne
ψn,6, ψn,7, ψn,8, ψn,9, and ψn,10, make just a small change in the above: just replace
⎛
⎝
1 0 0
0 0 0
0 0 0
⎞
⎠by
⎛
⎝
0 1 0
1 0 0
0 0 0
⎞
⎠,
⎛
⎝
0 0 0
0 1 0
0 0 0
⎞
⎠,
⎛
⎝
0 0 1
0 0 0
1 0 0
⎞
⎠,
⎛
⎝
0 0 0
0 0 1
0 1 0
⎞
⎠, or
⎛
⎝
0 0 0
0 0 0
0 0 1
⎞
⎠,
respectively.
9.5.4
Gradient-Related Corner Basis Function
Likewise, let’s go ahead and design a new basis function, corresponding to the x-
partial derivative at n:
ψn,2 ≡λ3
n (αkλk + αlλl + αmλm) .
How to ﬁnd the unknowns αk, αl, and αm? Solve three linear equations at n:
(1, 0, 0) = ∇tψn,2
=

∇t
λψn,2
 ∂λ
∂d

= ∇t
λ

λ3
n (αkλk + αlλl + αmλm)
 ∂λ
∂d

= λ3
n

αk∇t
λλk + αl∇t
λλl + αm∇t
λλm
 ∂λ
∂d

= λ3
n (αk(1, 0, 0, 0) + αl(0, 1, 0, 0) + αm(0, 0, 1, 0))
∂λ
∂d

= (αk, αl, αm, 0)
∂λ
∂d

= (αk, αl, αm)
∂(λk, λl, λm)
∂(x, y, z)

.
More explicitly, deﬁne
⎛
⎝
αk
αl
αm
⎞
⎠≡
∂(λk, λl, λm)
∂(x, y, z)
−t
⎛
⎝
1
0
0
⎞
⎠.

322
9
Basis Functions: Barycentric Coordinates in 3-D
Still, this is not good enough. To become a proper basis function, this function must
now be modiﬁed. For each nearby side midpoint, subtract a multiple of the basis
function deﬁned in Sect.9.5.1 in t. Furthermore, for each nearby edge midpoint,
subtract a multiple of those two basis functions deﬁned in Sect.9.5.2 in t. Finally,
subtract a multiple of those six basis functions deﬁned in Sect.9.5.3:
ψn,2 ←ψn,2 −

ψn,2

xx (n)ψn,5
ψn,2 ←ψn,2 −

ψn,2

xy (n)ψn,6
ψn,2 ←ψn,2 −

ψn,2

yy (n)ψn,7
ψn,2 ←ψn,2 −

ψn,2

xz (n)ψn,8
ψn,2 ←ψn,2 −

ψn,2

yz (n)ψn,9
and ﬁnally ψn,2 ←ψn,2 −

ψn,2

zz (n)ψn,10
in t. As before, ψn,2 is now extended into a continuous piecewise-polynomial basis
function in the entire mesh.
The same approach can also be used to design two more basis functions, corre-
sponding to the y- and z-partial derivative at n. In fact, to deﬁne ψn,3 and ψn,4, just
replace (1, 0, 0)t above by (0, 1, 0)t or (0, 0, 1)t, respectively.
9.5.5
Corner Basis Function
Finally, let’s deﬁne the basis function corresponding to the function itself at n:
ψn,1 ≡λ3
n.
Still, this is not good enough. To become a proper basis function, this function
must now be modiﬁed. As in Sect.9.5.2, for each nearby side midpoint, subtract a
multiple of a side-midpoint basis function in t. Furthermore, for each nearby edge
midpoint, subtract a multiple of two edge-midpoint basis functions in t. Moreover, as
in Sect.9.5.4, subtract a multiple of six Hessian-related basis functions in t. Finally,
subtract a multiple of three gradient-related basis functions:
ψn,1 ←ψn,1 −

ψn,1

x (n)ψn,2
ψn,1 ←ψn,1 −

ψn,1

y (n)ψn,3
and ﬁnally ψn,1 ←ψn,1 −

ψn,1

z (n)ψn,4
in t. As before, ψn,1 should now be extended into a continuous piecewise-polynomial
basis function in the entire mesh.

9.5 Basis Functions
323
This completes the design of ten new basis functions for n. The same can now be
done for the other corners as well.
9.6
Exercises
1. Show that the side-midpoint basis function deﬁned in Sect.9.5.1 is indeed a
proper basis function: it has only one nonzero degree of freedom.
2. Show that, once extended to the entire mesh, it makes a smooth basis function:
continuous, piecewise-polynomial, and with a continuous gradient across edges.
3. Show that the edge-midpoint basis function deﬁned in Sect.9.5.2 is indeed a
proper basis function: it has only one nonzero degree of freedom.
4. Show that, once extended to the entire mesh, it makes a smooth basis function:
continuous, piecewise-polynomial, and with a continuous gradient across edges.
5. Show that the Hessian-related corner basis function deﬁned in Sect.9.5.3 is
indeed a proper basis function: it has only one nonzero degree of freedom.
6. Show that, once extended to the entire mesh, it makes a smooth basis function:
continuous, piecewise-polynomial, and with a continuous gradient across edges.
7. Show that the gradient-related corner basis function deﬁned in Sect.9.5.4 is
indeed a proper basis function: it has only one nonzero degree of freedom.
8. Show that, once extended to the entire mesh, it makes a smooth basis function:
continuous, piecewise-polynomial, and with a continuous gradient across edges.
9. Show that the corner basis function deﬁned in Sect.9.5.5 is indeed a proper basis
function: it has only one nonzero degree of freedom.
10. Show that, once extended to the entire mesh, it makes a smooth basis function:
continuous, piecewise-polynomial, and with a continuous gradient across edges.

Part IV
Finite Elements in 3-D
To deﬁne useful basis functions, one must ﬁrst have a proper mesh. Consider a three-
dimensional domain, convex or nonconvex. To approximate it well, design a mesh
of disjoint (nonoverlapping) tetrahedra. In numerical analysis, these are called ﬁnite
elements [9, 69].
In the mesh, we have nodes: the corners of the tetrahedra. The node is the most
elementary ingredient in the mesh. An individual node may serve as a corner in a
few tetrahedra.
If they belong to the same tetrahedron, then the nodes must be connected to each
other by an edge. An edge is often shared by a few adjacent tetrahedra. There are
two kinds of edges: a boundary edge could be shared by two tetrahedra. An inner
edge, on the other hand, must be shared by more tetrahedra.
Each tetrahedron is bounded by four triangles: its sides or faces. In the mesh,
there are two kinds of faces: an inner face is shared by two adjacent tetrahedra. A
boundary face, on the other hand, belongs to one tetrahedron only.
Howtoconstructthemesh?Startfromacoarsemeshthatapproximatesthedomain
poorly, and improve it step by step. For this purpose, reﬁne: split coarse tetrahedra.
At the same time, introduce new (small) tetrahedra next to the convex parts of the
boundary, to improve the approximation from the inside. This procedure may then
repeat time and again iteratively, producing ﬁner and ﬁner meshes at higher and
higher levels.
This makes a multilevel hierarchy of ﬁner and ﬁner meshes, approximating the
original domain better and better. In the end, at the top level, those tetrahedra that
exceed the domain may drop from the ﬁnal mesh. This completes the automatic
algorithm to approximate the original domain well.
The mesh should be as regular as possible: the tetrahedra should be thick and
nondegenerate. Furthermore, the mesh should be as convex as possible. Only at the
top level may the mesh become concave again. A few tricks are introduced to have
these properties.
To verify accuracy, numerical integration is then carried out on the ﬁne mesh. For
this purpose, we use a simple example, for which the analytic integral is well-known
in advance. The numerical integral is then subtracted from the analytic integral. This
is the error: it turns out to be very small in magnitude. Furthermore, our regularity
estimates show that the mesh is rather regular, as required.

326
Finite Elements in 3-D
Once the basis functions are well-deﬁned in the ﬁne mesh, they can be used to
approximate a given function, deﬁned in the original domain. This is indeed the spline
problem [5, 11, 17, 28, 41, 51, 58, 75, 76]: design a smooth piecewise-polynomial
function to “tie” (or match) the original values of the function at the mesh nodes.
The spline problem could also be formulated as follows: consider a discrete grid
function, deﬁned at the mesh nodes only. Extend it into a complete spline: a smooth
piecewise-polynomial function, deﬁned not only at the nodes but also in between.
The solution must be optimal in terms of minimum “energy.” This is indeed the
smoothest solution possible.
Our (regular) ﬁnite-element mesh could be used not only in the spline problem
but also in many other practical problem. Later on, we’ll see interesting applications
in modern physics and chemistry.

Chapter 10
Automatic Mesh Generation
Consider a complicated domain in three spatial dimensions. How to store it on the
computer? For this purpose, it must be discretized: approximated by a discrete mesh,
ready to be used in practical algorithms.
To approximate the domain well, best use a mesh of tetrahedra. This way, the
tetrahedra may take different shapes and sizes. Next to the curved boundary, many
small tetrahedra should be used to approximate the boundary well. Next to the ﬂat
part of the boundary, on the other hand, a few big tetrahedra may be sufﬁcient. This
is indeed local reﬁnement: small tetrahedra should be used only where absolutely
necessary.
The automatic reﬁnement algorithm starts from a coarse mesh that approximates
the domain rather poorly. Then, this mesh reﬁnes time and again, producing ﬁner and
ﬁner meshes that approximate the domain better and better. Indeed, in a ﬁner mesh,
small tetrahedra may be added next to the curved boundary to approximate it better
from the inside. This produces a multilevel hierarchy of more and more accurate
meshes.
At the intermediate levels, the mesh is often still convex. This is good enough for
a convex domain, but not for a more complicated, nonconvex domain. Fortunately, at
the top level, this gets ﬁxed: those tetrahedra that exceed the domain too much drop.
This way, the ﬁnest mesh at the top level gets nonconvex and ready to approximate
the original nonconvex domain.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_10
327

328
10
Automatic Mesh Generation
10.1
The Reﬁnement Step
10.1.1
Iterative Multilevel Reﬁnement
What is multilevel reﬁnement? It uses a hierarchy of ﬁner and ﬁner meshes to approx-
imate the original domain better and better [45, 47, 61].
At the bottom level, one may place a rather poor mesh, containing only a few big
tetrahedra. Don’t worry: the initial coarse mesh will soon reﬁne and improve. This
is indeed the reﬁnement step, producing the next ﬁner mesh in the next higher level.
How does the reﬁnement step work? Well, each coarse tetrahedron splits into two
subtetrahedra. How is this done? Well, a coarse edge is divided into two subedges.
For this purpose, its original midpoint is connected to those two corners that lie across
from it. This produces two new subtetrahedra. In the ﬁner mesh, they are going to
replace the original coarse tetrahedron, providing a better resolution.
10.1.2
Conformity
What happens to those neighbor tetrahedra that share the same coarse edge? To
preserve conformity, they must split in the same way as well. This way, the above
midpoint is connected to those corners that lie across from it not only in the original
tetrahedron but also in each adjacent (edge-sharing) tetrahedron.
So far, we’ve been busy splitting tetrahedra. This was done in two possible ways.
A coarse tetrahedron may split out of its own initiative, to reﬁne. Still, this is not the
only way: a tetrahedron may also be forced to split not to reﬁne but only to preserve
conformity and ﬁt to a neighbor tetrahedron that has already reﬁned.
Still, the reﬁnement step may not only split existing tetrahedra but also introduce
new ones to improve the approximation next to the convex part of the boundary.
This is called boundary reﬁnement. This completes the reﬁnement step, producing
the next ﬁner mesh at the next higher level. This mesh is now ready for the next
reﬁnement step, and so on.
The boundary of the original domain may contain two parts: the convex part,
and the concave (or nonconvex) part. The reﬁnement step distinguishes between the
two. At the convex part, a few new (small) tetrahedra are introduced to approximate
the curved boundary better from the inside. At the concave part, on the other hand,
no new tetrahedron is introduced. On the contrary: those tetrahedra that exceed the
domain too much even drop in the end. This way, the ﬁnest mesh at the top level will
get accurate again, even for a complicated nonconvex domain.

10.1 The Reﬁnement Step
329
10.1.3
Regular Mesh
What is a regular mesh? Well, in a regular mesh, the tetrahedra are thick and non-
degenerate [7, 39]. As discussed in Chap.9, Sect.9.1.1, this is a most important
property.
Fortunately, at the bottom level, one can often pick a rather regular initial mesh.
How to make sure that the ﬁner mesh in the next higher level remains regular as
well?
10.1.4
How to Preserve Regularity?
Well, here is the trick. Just before the reﬁnement step, order the coarse tetrahedra
one by one by maximal edge: a tetrahedron with a longer maximal edge before a
tetrahedron with a shorter maximal edge. In their new order, scan the tetrahedra one
by one: reﬁne a tetrahedron only if it is indeed a coarse tetrahedron. Otherwise, leave
it. This way, each coarse tetrahedron reﬁnes only once: only its maximal edge splits
into two subedges. This way, the original coarse tetrahedron splits into two new
subtetrahedra, which can no longer split any more in this reﬁnement step.
Or can’t they? Well, there may still be one exception. Once a coarse tetrahedron
splits (at its maximal edge, as above), all those neighbor tetrahedra that share this
edge must split as well to preserve conformity (Sect.10.1.2). In such a neighbor
tetrahedron, however, this edge is not necessarily maximal: it could be submaximal.
Could it really? After all, if the neighbor tetrahedron contained a longer edge,
then it would have already been listed before and would have already split long ago.
Thus, it can’t be a coarse tetrahedron: it could only be a subtetrahedron, obtained
from an earlier split.
This is not so good: splitting a subtetrahedron at its submaximal edge may produce
a rather thin (irregular) subsubtetrahedron. As a result, regularity may decrease a
little. Fortunately, this will soon get ﬁxed: just before the next reﬁnement step, the
tetrahedra are going to be reordered once again in terms of maximal edge, so this
problem should probably get ﬁxed in the next reﬁnement step, increasing regularity
again.
10.2
Approximating a 3-D Domain
10.2.1
Implicit Domain
The original three-dimensional domain Ω ⊂R3 may be rather complicated: its
boundary ∂Ω may be curved, nonstandard, and irregular. In fact, ∂Ω may be available
only implicitly, in terms of a given real function F(x, y, z):

330
10
Automatic Mesh Generation
∂Ω ≡

(x, y, z) ∈R3 | F(x, y, z) = 0

.
This deﬁnes ∂Ω implicitly as the zero level set of F: the set of points at which F
vanishes.
Assume, for example, that we want ∂Ω to be a sphere centered at (1/2, 1/2, 1/2).
Assume also that we want this sphere to conﬁne the unit cube:
[0, 1]3 ⊂Ω.
In this case, F could be deﬁned as
F(x, y, z) ≡

x −1
2
2
+

y −1
2
2
+

z −1
2
2
−3
4.
Indeed, with this deﬁnition, ∂Ω is a sphere of radius
√
3/2 around (1/2, 1/2, 1/2).
In fact, F is negative inside the sphere, positive outside it, and zero on the sphere
itself.
In practice, however, F is rarely available in its closed analytic form. More often,
it is only available as a computer function: for every given x, y, and z, F(x, y, z)
can be calculated on the computer. Let’s see a more interesting example.
10.2.2
Example: A Nonconvex Domain
In the above example, Ω is convex: the interior of the sphere that conﬁnes the unit
cube. Here, on the other hand, we consider a more complicated example, in which
Ω is no longer convex.
Let R+ be the nonnegative part of the real axis:
R+ ≡{x ∈R | x ≥0} ⊂R.
Assume that Ω lies in the nonnegative “octant” of the three-dimensional Cartesian
space:
Ω ⊂

R+3 ≡

(x, y, z) ∈R3 | x ≥0, y ≥0, z ≥0

.
This way, every point in Ω must have three nonnegative coordinates: x ≥0, y ≥0,
and z ≥0.
Still, we are not done yet: we want Ω to be much smaller than (R+)3. For this pur-
pose, let R1 and R2 be some positive parameters to be speciﬁed later (0 < R1 < R2).
Assume also that Ω lies in the sphere of radius R2, centered at (0, 0, 0). This way,
every point in Ω must have a magnitude of R2 or less.
Finally, assume also that Ω lies outside of the sphere of radius R1. This way, every
point in Ω must have a magnitude of R1 or more.

10.2 Approximating a 3-D Domain
331
In summary, Ω contains those points with magnitude between R1 and R2, and
with three nonnegative coordinates:
Ω ≡

(x, y, z) ∈R3 | x ≥0, y ≥0, z ≥0, R2
1 ≤x2 + y2 + z2 ≤R2
2

.
This deﬁnes Ω in a closed analytic form: a nonconvex domain.
It would be more interesting, though, to pretend that we don’t have this closed
form available. After all, this is usually the case in practice. Thus, we should better
train in using F, not Ω. For this purpose, how to deﬁne F?
Well, from the original deﬁnition, the origin is not in Ω. Thus, one could issue a
ray from the origin towards Ω. Clearly, the ray meets ∂Ω at two points: it enters Ω
through a point of magnitude R1 and leaves through a point of magnitude R2. On the
ray, it makes sense to deﬁne F as a parabola that vanishes at these two points, and
has its unique minimum in between. This would indeed deﬁne F in (R+)3.
So far, we’ve “deﬁned” F in (R+)3 only. Still, this is not the end of it. After all, F
must be deﬁned outside Ω as well. In fact, it must be positive there: it must increase
monotonically away from Ω.
To extend F to the rest of the Cartesian space, one must consider negative coor-
dinates as well. Each negative coordinate should contribute its absolute value to F
to increase the value of F linearly as the point leaves (R+)3. This way, F indeed
increases monotonically away from Ω, as required.
So, we only need to specify the above parabolas explicitly. This will indeed make
one factor in F: a ≡a(x, y, z). This way, both a and F will indeed vanish on the
round parts of ∂Ω, where the magnitude is either R1 or R2. Still, this is not good
enough. After all, F must vanish on the ﬂat sides of Ω as well. For this purpose,
F must contain yet another factor: b ≡b(x, y, z). So, we can already view F as a
product of two functions: F = ab.
How should b look like? Well, each nonzero three-dimensional vector (x, y, z)t
makes three cosines with the x-, y-, and z-axes:
x

x2 + y2 + z2 ,
y

x2 + y2 + z2 , and
z

x2 + y2 + z2
(Chap.2, Sect.2.3.3). Consider the minimal cosine (in absolute value):
b ≡b(x, y, z) ≡min(|x|, |y|, |z|)

x2 + y2 + z2 .
This way, b is positive throughout (R+)3, except at the planes x ≡0, y ≡0, and
z ≡0, where it vanishes. Thus, these planes are contained in the zero level set of b:

332
10
Automatic Mesh Generation
∂
	
R+3
=

(x, y, z) ∈R3 | x ≥0, y ≥0, z ≥0, xyz = 0

⊂

(x, y, z) ∈R3 | xyz = 0

=

(x, y, z) ∈R3 | b(x, y, z) = 0

∪{(0, 0, 0)}.
In particular, the zero level set of b contains the ﬂat sides of Ω, as required.
Let us now go ahead and deﬁne a explicitly as well:
a ≡a(x, y, z) ≡
	
R1 −

x2 + y2 + z2

 	
R2 −

x2 + y2 + z2

.
This way, in each ray issuing from the origin towards Ω, a makes a parabola, as
required. Each parabola vanishes at two points only: the point of magnitude R1, and
the point of magnitude R2.
Fortunately, b is constant in the ray, so the product ab still makes a parabola in the
ray, as required. Moreover, ab still has a unique minimum in between these points.
For this reason, ab must also have a unique minimum in the entire domain Ω. As a
matter of fact, this minimum is obtained at
1
√
3
· R1 + R2
2
(1, 1, 1).
We are now ready to deﬁne F in the entire Cartesian space:
F(x, y, z) ≡
⎧
⎪⎪⎨
⎪⎪⎩
ab
if (x, y, z) ∈Ω
|a|
if (x, y, z) ∈

R+3 \ Ω
F(max(x, 0), max(y, 0), max(z, 0))
−min(x, 0) −min(y, 0) −min(z, 0)
if (x, y, z) /∈

R+3 .
This way, outside (R+)3, F is deﬁned recursively from its value at the nearest point
in (R+)3. With this deﬁnition, F indeed increases linearly as either x or y or z gets
more and more negative.
This completes the deﬁnition of F. Indeed, F is negative in the interior of Ω,
zero on the boundary ∂Ω, and positive outside of Ω. Furthermore, F increases
monotonically away from Ω, as required.
We thus have a good example to model a realistic case. In fact, we can just pretend
that Ω has never been disclosed to us explicitly, but only implicitly, in terms of F.
As a matter of fact, we can even pretend that F is available not analytically but
only computationally: given x, y, and z, we have a computer program to calculate
F(x, y, z) for us. As we’ll see below, this is enough to design a proper mesh to
approximate Ω.

10.2 Approximating a 3-D Domain
333
10.2.3
How to Find a Boundary Point?
As discussed above, to model a realistic case, we pretend that ∂Ω is not available
explicitly. What is available is the computer function F: for every given x, y, and z,
F(x, y, z) can be calculated on the computer. Thus, ∂Ω is only available implicitly,
as the zero level set of F:
(x, y, z) ∈∂Ω
if
F(x, y, z) = 0.
Fortunately, this is good enough to ﬁnd a new boundary point on ∂Ω. How to do
this? Well, for this purpose, one must ﬁnd some concrete coordinates x, y, and z, for
which
F(x, y, z) = 0.
Let a ∈R3 be some initial point. Furthermore, let d ∈R3 be some nonzero direction
vector. The task is to ﬁnd a boundary point of the form a + αd, for some unknown
(nonnegative) scalar α.
For this purpose, consider the arrow leading from a to a + d (Fig.10.1). If F
changes sign over the arrow:
F(a)F(a + d) < 0,
then the required boundary point lies in between a and a + d, and can be found by
iterative bisection: split the original arrow into two subarrows. If F changes sign
over the ﬁrst subarrow:
F(a)F

a + d
2

< 0,
then the boundary point must lie on the ﬁrst subarrow, in between a and a + d/2. In
this case, the ﬁrst subarrow is picked to substitute the original arrow:
d ←d
2 .
If, on the other hand, F changes sign over the second subarrow:
∂Ω
a
a + d
Fig. 10.1 The good case: the arrow leading from a to a + d indeed contains a boundary point in ∂Ω.
In this case, F indeed changes sign over the arrow: F(a) < 0 < F(a + d) or F(a) > 0 > F(a + d)

334
10
Automatic Mesh Generation
Fig. 10.2 The bad case: the
arrow leading from a to
a + d is too short, so the
boundary ∂Ω remains ahead
of it. The arrow must ﬁrst
shift or stretch forward, until
its head passes ∂Ω, as in the
previous ﬁgure
r
pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp
∂Ω
a
a + d
F

a + d
2

F(a + d) < 0,
then the boundary point must lie on the second subarrow, in between a + d/2 and
a + d. In this case, the second subarrow is picked:
a ←a + d
2 and d ←d
2 .
This procedure repeats time and again, until d gets sufﬁciently short, so the required
boundary point is found with a sufﬁcient accuracy.
Unfortunately, the situation is not always so benign. Some preparation work may
be needed before iterative bisection can start. To study such a case, let us return to
the original a and d.
In Fig.10.2, for example, the original arrow is too short. It must ﬁrst shift (or
stretch) ahead, until its head passes ∂Ω. Only then can iterative bisection start, as
above.
10.3
Approximating a Convex Boundary
10.3.1
Boundary Reﬁnement
Thanks to the above method, we can now ﬁnd a new boundary point. This can now
be used in the reﬁnement step. This way, the mesh reﬁnes not only in the interior
of the domain but also next to the curved boundary. For this purpose, a few new
(small) tetrahedra are introduced next to the convex part of the boundary to improve
the approximation there from the inside.
For instance, consider a boundary edge that lies next to the convex part of the
boundary, with both its endpoints on the boundary. Such an edge must then be shared
by two boundary triangles that lie next to the boundary as well, with all vertices on
the boundary. In fact, each boundary triangle may serve as the face in one tetrahedron
only.

10.3 Approximating a Convex Boundary
335
Since the boundary is locally convex there, the boundary edge (and the boundary
triangles) should lie (mostly) inside the domain. In the reﬁnement step, the coarse
boundary edge may split. In this case, a normal vector issues from its midpoint
towards the boundary. This normal vector may make a good direction vector that
points towards a new boundary point, as in Sect.10.2.3.
This new boundary point is then connected to ﬁve points: three on the original
boundary edge (two endpoints and one midpoint), and two off it (one vertex in each
boundary triangle). This adds four new tetrahedra to improve the approximation next
to the locally convex boundary from the inside.
10.3.2
Boundary Edge and Triangle
What is a boundary edge? So far, we’ve assumed that a boundary edge must have
both endpoints on ∂Ω. But what happens when Ω is nonconvex? In this case, the
initial coarse mesh might be quite different from Ω: it might exceed it quite a bit
(Fig.10.3).
In this case, a standard boundary edge (with both endpoints on ∂Ω) may not do:
its midpoint may lie outside Ω. After all, Ω may be locally concave there.
In this case, once this edge splits into two subedges, there is no boundary reﬁne-
ment. But what about the subedge, split in the next reﬁnement step? Well, it is no
longer a boundary edge: it has just one endpoint on ∂Ω, and one endpoint off ∂Ω. For
this reason, in the next reﬁnement step too, the subedge may split, but with no bound-
ary reﬁnement. This is indeed unfortunate: the domain will never be approximated
accurately there.
To ﬁx this, we must redeﬁne a boundary edge in terms of the current mesh M,
not the original domain Ω. A boundary edge must have both endpoints on ∂M, not
necessarily on ∂Ω.
How could this help? Well, consider the above situation once again (Fig.10.4).
In the ﬁrst reﬁnement step, there is no change: the original boundary edge is so
coarse that its midpoint lies outside the nonconvex domain, so there is no boundary
reﬁnement there. Fortunately, its subedge is a boundary edge of M, although not of
Ω. As such, since its own midpoint lies well inside Ω, it may indeed split there in
the next reﬁnement step, this time with boundary reﬁnement, as required.
This way, the ﬁne mesh will no longer be convex. Later on, we’ll see that this
may be risky: in yet ﬁner meshes, tetrahedra may overlap, making a complete mess.
Fortunately, here M is nonconvex outside the domain only, so there should be no
risk.
Thus, from now on, a boundary edge will mean a boundary edge of M, not Ω, and a
boundary triangle will mean a boundary triangle of M as well. For this purpose, we’ll
deﬁne a new mechanism to detect such an edge, based on M only, and independent
of Ω or F.

336
10
Automatic Mesh Generation
Fig. 10.3 A boundary edge
with both its endpoints on
∂Ω. Unfortunately, its
midpoint may still lie outside
the nonconvex domain, so no
boundary reﬁnement is
carried out there.
Furthermore, the subedge is
no longer a boundary edge: it
has an endpoint off ∂Ω.
Therefore, no boundary
reﬁnement will take place
ever
10.3.3
How to Fill a Valley?
In Sect.10.1.4, we’ve seen that only the maximal edge should split. Or should it?
Well, sometimes a submaximal edge should split instead to make the ﬁne mesh more
convex. After all, in Fig.10.4, we only see a two-dimensional projection. In reality,
on the other hand, the three-dimensional mesh may suffer from a local concavity: a
“valley.”
Consider, for example, the coarse mesh M, as viewed from above (Fig.10.5).
Assume that M is ﬂat from above, so this is indeed a real view. Which is the maxi-
mal edge? Clearly, the oblique one. This is indeed the edge that should split in the
reﬁnement step.
Assume that ∂Ω lies above M, closer to your eyes. In this case, there is also a
boundary reﬁnement: from the edge midpoint, a normal vector issues towards your
eyes to meet the boundary above it, next to your eye. This is indicated by the ‘⊙’ in
Fig.10.5. This forms a little pyramid, made of four new tetrahedra, to approximate
the boundary above it better.

10.3 Approximating a Convex Boundary
337
Fig. 10.4 A boundary edge
of M, although not of Ω. In
this sense, the left subedge is
a legitimate boundary edge,
with both endpoints on ∂M,
although not on ∂Ω. Its own
midpoint lies well inside Ω,
so boundary reﬁnement will
indeed take place there in the
next reﬁnement step.
Although the mesh gets
slightly nonconvex, this
should produce no
overlapping tetrahedra
Fig. 10.5 The coarse mesh:
a view from above. In the
reﬁnement step, the oblique
edge splits, and a normal
vector issues from its
midpoint towards your eyes
to hit the boundary above it
⊙
⊙
Unfortunately, this also produces a concave valley in between the new pyramids
(Fig.10.6). How to ﬁll it? Well, in the next reﬁnement step, in each pyramid, split
not the maximal edge but rather the vertical edge along the valley, in between the
pyramids (Fig.10.7). This way, a normal vector will issue from the middle of the
valley towards your eyes to help ﬁll the valley with four new tetrahedra, next to the
boundary above it.
Thus, the original order in Sect.10.1.4 must change. First, list those tetrahedra
near a valley: those near a “deep” valley before others. This way, a tetrahedron near
a deep valley splits earlier, as required, and the valley gets ﬁlled sooner, even though
the edge along it is submaximal. Later on, we’ll state more clearly what “deep”
means.

338
10
Automatic Mesh Generation
Fig. 10.6 The next ﬁner
mesh: two pyramids, with a
concave valley in between
@
@
@
@
@
@
@
@
@
@
@
@
?
concave valley
pyramid
pyramid
⊙
⊙
Fig. 10.7 The next
reﬁnement step: the vertical
edge along the valley,
although submaximal, splits,
and a normal vector issues
from its midpoint ‘⊙’
towards your eyes to hit the
boundary above it, and ﬁll
the valley with four new
tetrahedra
@
@
@
@
@
@
@
@
@
@
@
@
@
?
concave valley
pyramid
pyramid
⊙
After these, list those tetrahedra that are near no valley. These are ordered as
before: those with a long edge before others. After all, they should reﬁne at their
maximal edge, as before.
Finally, list those tetrahedra that exceed the (nonconvex) domain too much, regard-
less of the length of their edges. After all, they should have actually been dropped,
so they have little business to reﬁne. Later on, we’ll state more clearly what “too
much” mean.
10.3.4
How to Find a Boundary Edge?
In Sect.10.3.2, we’ve explained why we’re interested in a boundary edge of M, not of
Ω. How to ﬁnd such an edge? More precisely, given an edge, how to check whether
it is indeed a boundary edge, and ﬁnd the boundary triangles that share it? After all,
the endpoints of the edge must now lie on ∂M, which is not available numerically!

10.3 Approximating a Convex Boundary
339
Well, for this purpose, let t1 ⊂M be some tetrahedron in the mesh, and let e ⊂t1
be an edge in it. Then, e is shared by two sides s1, s2 ⊂t1:
e = s1 ∩s2.
To check whether e is a boundary edge or not, let us check whether it belongs to any
boundary triangle. For this purpose, let us search for a neighbor tetrahedron t2 that
shares s2 as its joint face:
s2 = t1 ∩t2.
Now, t2 must have another face, s3, that also shares e as its joint edge:
e = s1 ∩s2 ∩s3.
We can now apply the same procedure iteratively to t2 rather than t1 to ﬁnd yet another
triangle, s4 that shares e as well, and so on. This produces a list of triangles
s1, s2, s3, . . . , sn
that share e as their joint edge:
e = s1 ∩s2 ∩s3 ∩· · · ∩sn = ∩n
i=1si.
Now, if
sn = s1,
then e can’t be a boundary edge: it is surrounded by tetrahedra from all directions,
so it must be away from ∂M. If, on the other hand,
sn ̸= s1,
then sn must be a boundary triangle, and e must indeed be a boundary edge.
In this case, how to ﬁnd the second boundary triangle? Apply the same procedure
once again, only this time interchange the roles of s1 and s2.
10.3.5
Locally Convex Boundary: Gram–Schmidt Process
Let
t ≡(k, l, m, n) ⊂M
be some tetrahedron in the mesh, vertexed at corners k, l, m, and n (Fig.9.1). Let
e ≡(m, n) ⊂t

340
10
Automatic Mesh Generation
Fig. 10.8 Projection onto
the plane perpendicular to
the boundary edge
e ≡(m, n). The direction
vector d points from
a ≡(m + n)/2 towards ∂Ω,
as required
∂Ω
w
a
v
u
l
k
d
o
p
be an edge in t. Assume that, in the reﬁnement step, e should split at its midpoint:
a ≡m + n
2
.
We must then know: is e a boundary edge? After all, if it is, then boundary reﬁnement
may be required at a (Sect.10.3.1). Fortunately, we already know how to ﬁnd out the
answer. (Sect.10.3.4).
Thus, assume that e is indeed a boundary edge:
e ⊂∂M.
In this case, we also get a bonus: two boundary triangles:
△(m, n, u) and △(m, n, v) ⊂∂Ω.
These will be useful below.
Furthermore, assume also that the domain is locally convex at a:
F(a) < 0, so a ∈Ω \ ∂Ω.
In this case, it makes sense to ﬁll the gap between e and ∂Ω with four new tetrahedra.
For this purpose, issue a normal vector d from a towards ∂Ω to hit ∂Ω at the new
boundary point w ∈∂Ω (Sect.10.2.3). Then, connect w to ﬁve points: m, n, a, u,
and v (Fig.10.8). This indeed produces four new tetrahedra, to approximate the
boundary better at a from the inside, as required.

10.3 Approximating a Convex Boundary
341
Now, how to deﬁne the direction vector d? Well, it should point in between u
and v, towards w ∈∂Ω. For this purpose, let
e ≡
n −m
∥n −m∥2
be the unit vector parallel to e. Furthermore, deﬁne the differences
o ≡u −a and p ≡v −a.
Now, project both o and p onto the plane perpendicular to e:
o ←o −(o, e)e
p ←p −(p, e)e.
Indeed, after these substitutions,
(o, e) = (p, e) = 0.
This is actually a Gram–Schmidt process (Chap.2, Sect.2.3.2). Next, normalize both
o and p:
o ←
o
∥o∥2
p ←
p
∥p∥2
.
This produces the picture in Fig.10.8. Now, let d be the vector product
d ≡(p −o) × e.
Next, normalize d as well:
d ←
d
∥d∥2
.
Is d a good direction vector? Well, in Fig.10.8, assume that e points into the page,
away from you. Thanks to the right-hand rule, d indeed points away from M, as
required. (See exercises at the end of Chap.2.)
But what if the orientation is the other way around, and
det ((k −a | e | l −a)) < 0?
In this case, d must reverse:
d ←−d.
(See exercises at the end of Chap.2.)

342
10
Automatic Mesh Generation
By now, in either case, d is a good direction vector: it indeed points away from
M, towards ∂Ω, as required.
We can now also tell what a “deep” valley is: if
(o + p, d)
is large, then the valley along e is indeed deep. In this case, t should be listed early,
and split early at a, ﬁlling the valley with four new tetrahedra:
(a, w, m, u), (a, w, m, v), (a, w, n, u),
and (a, w, n, v),
as required. This helps approximate Ω better from the inside.
Still, there is a condition. To use these four new tetrahedra, w must be away from
a:
∥w −a∥2 ≥10−2∥n −m∥2.
Otherwise, these new tetrahedra are too thin and degenerate and should better be left
out, and not added to M.
10.4
Approximating a Nonconvex Domain
10.4.1
Locally Concave Boundary
Still, there is one more problem. So far, the domain has been approximated from the
inside, at its (locally) convex part only. At its locally concave (nonconvex) part, on
the other hand, no boundary reﬁnement has been carried out. After all, the midpoint
of the boundary edge often lies outside the domain. So, the approximation is still
poor.
How to ﬁx this? Well, a few strategies have been tested. One method is as follows.
Recall that, in the reﬁnement step, the original coarse tetrahedron
t ≡(k, l, m, n)
is replaced by two subtetrahedra:
(k, l, m, a) and (k, l, a, n),
where
a ≡m + n
2
is the edge midpoint. Here, on the other hand, since a /∈Ω, it might make sense to
replace it by the nearest point c ∈∂Ω. This way, in the reﬁnement step, t is replaced
by two new ﬁne tetrahedra:

10.4 Approximating a Nonconvex Domain
343
(k, l, m, c) and (k, l, c, n).
Is this a good ﬁx? Unfortunately not: it might produce overlapping tetrahedra in the
next reﬁnement step, and a complete mess.
A better approach is as follows. Wait until the entire multilevel hierarchy is com-
plete. Then, ﬁx the top level only: from the ﬁnest mesh, drop those tetrahedra that
exceed Ω too much in the sense of having 3–4 corners outside Ω. A tetrahedron
with only 1–2 corners outside Ω, on the other hand, mustn’t drop. This way, at the
top level, Ω gets approximated at its concave part as well: not from the inside, but
from the outside.
10.4.2
Convex Meshes
Why ﬁx only the top level, not the intermediate ones? Well, dropping tetrahedra from
an intermediate mesh might spoil the special structure required to carry out the next
reﬁnement step.
In fact, even in a nonconvex domain, the mesh should better remain as convex
as possible for as long as possible. Dropping a tetrahedron, on the other hand, may
produce a “hole” in the mesh, making it highly nonconvex too early.
What would then happen in the next reﬁnement step? Well, consider a boundary
edge in such a hole. Recall that this is a boundary edge of the mesh, not necessarily of
the domain (Sect.10.3.2). The normal vector issuing from its midpoint (Sect.10.3.5)
may then hit the other bank of the hole, producing overlapping tetrahedra, and a
complete mess.
This is why no tetrahedron should drop from any intermediate mesh. This way, in
most of the multilevel hierarchy, the meshes remain as convex as possible. Only from
the ﬁnest mesh may some tetrahedra drop, leaving it as nonconvex as the original
domain, as required.
10.5
Exercises
1. Consider the closed unit cube
Ω ≡[0, 1]3 ≡

(x, y, z) ∈R3 | 0 ≤x, y, z ≤1

.
2. Show that Ω is convex.
3. Deﬁne the function
F(x, y, z) ≡max
x −1
2
 ,
y −1
2
 ,
z −1
2


−1
2.

344
10
Automatic Mesh Generation
4. Show that F is negative in the interior of Ω, zero on its boundary, positive outside
it, and monotonically increasing away from it.
5. Conclude that ∂Ω is indeed the zero level set of F, as required.
6. Show that F is monotonically increasing on each ray issuing from the middle of
Ω at (1/2, 1/2, 1/2).
7. Write the unit cube as the union of six disjoint tetrahedra.
8. Make sure that this mesh is conformal.
9. Apply a reﬁnement step to this mesh.
10. Make sure that the ﬁne mesh is conformal as well.
11. Assume now that Ω is the interior of the sphere that conﬁnes the unit cube.
12 Show that Ω is convex.
13. Deﬁne F as in Sect.10.2.1.
14. Show that F is negative in the interior of Ω, zero on its boundary, positive outside
it, and monotonically increasing away from it.
15. Conclude that ∂Ω is indeed the zero level set of F, as required.
16. Show that F is monotonically increasing on each ray issuing from the middle of
Ω at (1/2, 1/2, 1/2).
17. Now, let Ω be the nonconvex domain in Sect.10.2.2. Show that Ω is indeed the
intersection of the ball of radius R2, the outside of the ball of radius R1, and
(R+)3.
18. Show that Ω is indeed nonconvex.
19. What is the convex part of the boundary of Ω?
20. What is the concave (nonconvex) part of the boundary of Ω?
21. What is the ﬂat part of the boundary of Ω?
22. Show that the function F deﬁned in Sect.10.2.2 is indeed negative in the interior
of Ω, zero on ∂Ω, positive outside of Ω, and monotonically increasing away
from Ω.
23. Conclude that ∂Ω is indeed the zero level set of F, as required.
24. Show that F has a unique minimum on each ray issuing from the origin towards
Ω.
25. Show that F increases monotonically on each ray issuing from ∂Ω away from
Ω.
26. Show that F has a unique minimum in Ω.
27. Find this minimum explicitly.
28. Consider now some mesh of tetrahedra. Show that a boundary triangle serves as
a face in exactly one tetrahedron.
29. Show that a boundary edge is shared by exactly two faces: the boundary triangles.
30. Show that, if these boundary triangles serve as faces in one and the same tetra-
hedron, then this is the only tetrahedron that uses the above boundary edge.
31. Show that the algorithm in Sect.10.3.4 indeed tells whether a given edge is a
boundary edge or not.
32. Show that, if this is indeed a boundary edge, then this algorithm also ﬁnds the
boundary triangles that share it.
33. In this case, how can the second boundary triangle be found as well?

10.5 Exercises
345
34. Show that the direction vector d deﬁned in Sect.10.3.5 (in its ﬁnal form) indeed
points from the midpoint a towards ∂Ω, in between the boundary triangles.
35. Show that the four new tetrahedra that are added to the mesh at the end of
Sect.10.3.5 indeed improve the approximation at the convex part of the boundary
from the inside.
36. Show that the reﬁnement step preserves conformity.
37. Show that the dropping technique in Sect.10.4.1 indeed improves the approxi-
mation at the concave part of the boundary from the outside.
38. Why should this dropping technique be applied to the ﬁnest mesh only?

Chapter 11
Mesh Regularity
In Chap.10, Sects.10.1.3–10.1.4, we’ve already met the important concept of mesh
regularity, and took it into account in the reﬁnement step. Here, we continue to discuss
it, and introduce a few reliable tests to estimate it. This way, we can make sure that
our multilevel reﬁnement is indeed robust: the meshes are not only more and more
accurate but also fairly regular.
Some regularity tests could be rather misleading and inadequate. Here, we high-
light this problem, and avoid it. We are careful to use regularity tests that are genuine
and adequate.
Why are tetrahedra so suitable to serve as ﬁnite elements in our mesh? Because
they are ﬂexible: can take all sorts of shapes and sizes. This is the key for an efﬁcient
local reﬁnement: use small tetrahedra only where absolutely necessary. Still, there is
a price to pay: regularity must be compromised. After all, to approximate the curved
boundary well, the tetrahedra must be a little thin. Still, thanks to our tricks, regularity
decreases only moderately and linearly from level to level. This is not too bad: it is
unavoidable and indeed worthwhile to compromise some regularity for the sake of
high accuracy.
11.1
Angle and Sine in 3-D
11.1.1
Sine in a Tetrahedron
How to measure mesh regularity? For this purpose, we must ﬁrst ask: how to measure
the regularity of one individual tetrahedron? Or, how to measure how thick it is?
Consider the general tetrahedron
t ≡(k, l, m, n),
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_11
347

348
11
Mesh Regularity
vertexed at its distinct corners k, l, m, and n (Fig.9.1). For example, if
k = (0, 0, 0)t
l = (1, 0, 0)t
m = (0, 1, 0)t
n = (0, 0, 1)t,
then t is just the unit tetrahedron T in Fig.8.5.
Recall the 3 × 3 matrix
St ≡(l −k | m −k | n −k) ,
whose columns are the vectors leading from k to the three other corners. As discussed
in Chap.9, Sect.9.1.1, St is the Jacobian of the mapping that maps T onto t. For
example, if t = T , then St is just the 3 × 3 identity matrix: the Jacobian of the
identity mapping.
In a two-dimensional triangle, we already have the sine function, to help estimate
the individual angles. Could the sine function be extended to a three-dimensional
tetrahedron as well? After all, unlike the triangle, the tetrahedron has no angles in
the usual sense!
For this purpose, let’s deﬁne the “sine” of t at some corner, say k. This new sine
will have a value between 0 and 1, to tell us how straight (or right-angled) t is at k:
sin(t, k) ≡
| det(St)|
∥l −k∥· ∥m −k∥· ∥n −k∥.
In other words, take St, normalize its columns, calculate the determinant, and take
the absolute value.
In the extreme case in which t is degenerate, its sine is as small as 0. In the more
optimistic case in which t is as straight as the unit tetrahedron T , on the other hand,
its sine is as large as 1. Thus, the sine indeed tells us how straight and right-angled t
is at k.
11.1.2
Minimal Angle
To estimate sin(t, k), let us write it in terms of angles between edges or faces in t.
This may help bound sin(t, k) from below, indicating regularity.
Consider the face △(k, l, m) ⊂t. In this face, let α be the angle vertexed at k. In
Fig.11.1, for example, △(k, l, m) is horizontal, so α would be a horizontal angle as
well.
Furthermore, consider the edge (k, n). Consider its orthogonal projection onto
the above face. This produces a new angle between (k, n) and this projection: β.

11.1 Angle and Sine in 3-D
349
Fig. 11.1 The tetrahedron t:
a view from above. It sits on
its horizontal base:
△(k, l, m). Its left edge
(k, l) and its top corner n
make a nonhorizontal face:
△(k, l, n). Between these
two faces, there is a vertical
angle: δ
A
A
A
AU
-
XX
HH








γ
n
m
k
l
p
In Fig.11.1, for example, β would be the vertical angle between (k, n) and the x-y
plane.
Consider now another face: △(k, l, n) ⊂t. In this face, let γ be the angle vertexed
at k. In Fig.11.1, for example, γ is a nonhorizontal angle.
Finally, let δ be the angle between the above faces (or between their normal
vectors). Then, we have
sin(t, k) = sin(α) sin(β) = sin(α) sin(γ) sin(δ).
Now, how to make sure that t is nondegenerate? Well, require that these angles are
nonzero (have a positive sine). Furthermore, how to make sure that t is quite thick?
Well, require that these angles are far from zero: their sine is bounded from below
by a positive constant. This is indeed the minimal-angle criterion.
Still, this is a rather geometrical criterion. Is there a more algebraic, easily calcu-
lated criterion?
Well, let’s try. Assume that t is regular in the sense that
sin(t, k) ≥C
for some positive constant 0 < C ≤1, independent of k. Then, we also have
sin(α) ≥sin(α) sin(γ) sin(δ) = sin(t, k) ≥C
sin(δ) ≥sin(α) sin(γ) sin(δ) = sin(t, k) ≥C,
or
α ≥arcsin(C)
δ ≥arcsin(C),
where 0 <arcsin(C) ≤π/2. In fact, even as C →0, we still have
α ≥arcsin(C) ∼C
δ ≥arcsin(C) ∼C.

350
11
Mesh Regularity
Furthermore, since C is independent of k, the same could be done for every angle in
t, either between two distinct edges or between two distinct faces. All such angles
are indeed bounded from below by arcsin(C) > 0. Thus, t is indeed thick enough.
Is the reverse also true? Well, let’s try the other way around: assume now that all
angles like α and δ are bounded from below by a positive constant 0 < C4 ≤π/2.
Then, we have
sin(t, k) = sin(α) sin(γ) sin(δ) ≥sin3(C4) > 0.
Furthermore, since this is true for any α, γ, and δ, k in the above estimate could also
be replaced by l, m, or n.
Do we have here two equivalent criteria to estimate regularity? Unfortunately not:
as C4 →0, the latter estimate is too weak, and gives us little information:
sin(t, k) ≥sin3(C4) ∼C3
4 ≪C4 ≪1.
As discussed below, this kind of “equivalence” may be misleading and inadequate.
11.1.3
Proportional Sine
Unfortunately, sin(t, k) doesn’t tell us the whole story. After all, even a straight and
right-angled tetrahedron may still be disproportionate and nonsymmetric: the edges
issuing from k may still be different from each other in length. To account for this,
let’s introduce the so-called proportional sine:
Psine(t, k) ≡sin(t, k) min(∥l −k∥, ∥m −k∥, ∥n −k∥)
max(∥l −k∥, ∥m −k∥, ∥n −k∥).
Still, this is not the end of it. To tell how straight and symmetric t is, we might want
to look at it from the best point of view. So far, we’ve looked at it only from k. There
might, however, be a better direction. This motivates the deﬁnition of the maximal
proportional sine:
maxPsine(t) ≡
max
q∈{k,l,m,n} Psine(t, q).
For example, in terms of maximal proportional sine, the unit tetrahedron T has the
maximal possible regularity: 1.

11.1 Angle and Sine in 3-D
351
11.1.4
Minimal Sine
The maximal proportional sine is an algebraic criterion, easy to calculate on the
computer. Later on, we’ll see that it is actually equivalent to the minimal sine:
minSine(t) ≡
min
q∈{k,l,m,n} sin(t, q).
In terms of minimal sine, the most regular tetrahedron is no longer T , but rather the
even equilateral tetrahedron, whose edges have the same length.
In Sect.11.1.2, we have already seen that minimal sine is “equivalent” to minimal
angle: a tetrahedron regular in one sense is also regular in the other sense. Still, this
“equivalence” is inadequate and misleading. The minimal-angle criterion is much
more robust and reliable (Fig.11.2). Unfortunately, it is geometrical in nature, and
not easy to calculate. Later on, we’ll introduce a new regularity estimate that is not
only robust but also easy to calculate: ball ratio.
11.2
Adequate Equivalence
11.2.1
Equivalent Regularity Estimates
How thick is t? In the above, we already gave three possible estimates: its minimal
angle, minimal sine, or maximal proportional sine. Below, we’ll see that these esti-
mates are not completely independent of each other. On the contrary, they may be
related, or even equivalent to each other. In fact, minimal sine and maximal propor-
tional sine are both weak, whereas minimal angle is strong and robust (Fig.11.2).
To see this, let us ﬁrst introduce yet another (weak) regularity estimate— volume
ratio:
| det(St)/6|
maxEdge3(t),
where maxEdge(t) is the maximal edge length in t. Let’s show that this estimate is
not really new: it is actually equivalent to the maximal proportional sine:
maximal proportional sine ≥constant · (volume ratio)
(which is obvious) and
volume ratio ≥constant · (maximal proportional sine)
(which is not so obvious).

352
11
Mesh Regularity
-

-

?
6
?
6
weak estimates:
3
3
maximal
proportional
sine
volume ratio
minimal sine
ball ratio
minimal angle
robust estimates:
Fig. 11.2 Strong versus weak regularity estimates. The weak estimates at the top are equivalent to
each other, but inferior to the robust estimates at the bottom
Let’s prove the “not so obvious” bit. For this purpose, consider the maximal edge
in t. It is shared by two faces in t. Each such face contains at least one more edge
that is also as long as maxEdge(t)/2.
Fortunately, every corner in t belongs to at least one of these faces. Thus, from
every corner in t, there issues at least one edge that is as long as maxEdge(t)/2.
Now, in the maximal edge, look at that endpoint from which two long edges issue:
the maximal edge itself, and another edge that is also as long as maxEdge(2)/2. Look
at the proportional sine at this corner. This completes the proof.
This equivalence is indeed genuine and adequate. The volume ratio is thus not
really a new estimate: it gives us the same information as does the maximal propor-
tional sine. Indeed, if t is thick, then both tell us this. If, on the other hand, t is too
thin, then both tell us this in the same way.
Below, on the other hand, we’ll see that this is not always the case: two different
regularity estimates may seem equivalent, but are not.
11.2.2
Inadequate Equivalence
Unfortunately, the “equivalence” introduced in [7] is not good enough. It uses an
inequality like above, but only for a thick tetrahedron, whose regularity estimate
is bounded from below by a positive constant, not for a thin tetrahedron, whose

11.2 Adequate Equivalence
353
regularity estimate approaches zero. This kind of “equivalence” may be rather mis-
leading and inadequate.
In Sect.11.1.2, we have already seen an example of an inadequate equivalence:
as C4 →0, the minimal angle is bounded from below much better than the minimal
sine.
This is also the case with the ball ratio [the radius of the ball inscribed in t, divided
by maxEdge(t)]. The ball ratio and the volume ratio may seem equivalent to each
other, but are not. After all, in the proof in [7], while the ball ratio is well bounded
from below by C2 > 0, the volume ratio may get as small as C3
2 ≪C2 ≪1.
All this is still too theoretical. To establish that an “equivalence” is inadequate, it is
not enough to study its proof: there is a need to design a concrete counterexample of
a limit process in which the tetrahedron t gets less and less regular, yet its regularity
estimates disagree with each other. This is done next.
Consider, for example, the ﬂat tetrahedron vertexed at
(−1, 0, 0), (1, 0, 0), (0, −1, ε),
and (0, 1, ε),
where ε > 0 is a small parameter (Fig.11.3). Is this tetrahedron regular? Well, for
ε ≪1, it certainly isn’t: its volume is as small as ε, but all its edges are as long as 1.
Therefore, all regularity estimates agree with each other: they are as small as ε ≪1.
So, by now, we have no evidence of any inadequacy. The above is no counterex-
ample: the regularity estimates still agree with each other.
Let’s try and design yet another example: a thin tetrahedron, vertexed at
(−1, 0, 0), (1, 0, 0), (0, −ε, ε),
and (0, ε, ε)
(Fig.11.4). Is this tetrahedron regular? No, it is most certainly not: its volume is
now as small as ε2. Still, its regularity estimates disagree with each other. Indeed,
it has only one edge as short as ε, and ﬁve edges that are as long as 1. For this
reason, its weak regularity estimates lie: its volume ratio, minimal sine, and maximal
proportional sine are as small as ε2, which is too harsh. Its strong regularity estimates,
on the other hand, are more realistic: its minimal angle and ball ratio are only as small
as ε, not ε2.
What might happen if a weak regularity estimate was used in a stopping criterion
in multilevel reﬁnement? Well, we might then believe that a particular tetrahedron is
too thin, even when it is not. This might be too pedant, leading to stopping too early,
and rejecting good legitimate ﬁne meshes.
Picking a smaller stopping threshold is no cure: this might be too loose, leading
to stopping too late, and accepting ﬂat tetrahedra, for which all regularity tests are
as good (Fig.11.3).
A robust regularity test is thus clearly necessary. To be practical, it must also be
easy to calculate. This is done next.

354
11
Mesh Regularity
Fig. 11.3 A ﬂat tetrahedron:
a view from above. All
regularity estimates (weak
and strong alike) are as good
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
x
1
−1
y
Fig. 11.4 A thin
tetrahedron: a view from
above. The weak estimates
lie: they are as small as ε2,
but the true estimate is only
as small as ε
XXXXXXXX









X
X
X
X
X
X
X
X
x
1
−1
y
ε
−ε
11.2.3
Ball Ratio
Like the minimal angle, the ball ratio is a robust regularity estimate. This is the radius
of the ball inscribed in t, divided by maxEdge(t).
In other words, look at the largest ball that can be contained in t. Clearly, this ball
is tangent to the faces of t from the inside. Denote its center by o, and its radius by
r. Now, take r, and divide by the length of the maximal edge in t.
Unfortunately, this is still a geometrical deﬁnition, not easy to calculate. Is there
an algebraic formula, easy to calculate on the computer?

11.2 Adequate Equivalence
355
Fortunately, there is. For this purpose, connect o to the four corners of t: k, l, m,
and n. This splits t into four disjoint subtetrahedra.
Clearly, the volume of t is the sum of the volumes of these subtetrahedra. Fur-
thermore, in each subtetrahedron, the radius issuing from o makes a right angle with
the face that lies across from o. Thus,
| det(St)| =
det(S(o,l,m,n))
 +
det(S(k,o,m,n))
 +
det(S(k,l,o,n))
 +
det(S(k,l,m,o))

= r (∥(m −l) × (n −l)∥+ ∥(m −k) × (n −k)∥
+ ∥(l −k) × (n −k)∥+ ∥(l −k) × (m −k)∥) ,
where “×” stands for vector product.
This formula can now be used to calculate r. The ball ratio is then obtained
immediately as r/maxEdge(t).
11.3
Numerical Experiment
11.3.1
Mesh Regularity
What is the regularity of the entire mesh M? Naturally, it is just the minimum reg-
ularity of the individual tetrahedra in M. Still, not all tetrahedra should be con-
sidered. After all, a tetrahedron with 3–4 corners outside Ω should be disregarded
(Sect.10.4.1):
minimal sine(M) ≡
min
t⊂M, t has 2–4 corners in Ω
minSine(t),
or
minimal maxPsine(M) ≡
min
t⊂M, t has 2–4 corners in Ω
maxPsine(t),
or
minimal ballR(M) ≡
min
t⊂M, t has 2–4 corners in Ω
ballR(t).
11.3.2
Numerical Results
To test the quality of our multilevel reﬁnement, we consider the nonconvex domain
in Chap.10, Sect.10.2.2, with R2 = 1 and R1 = 0.75. The initial (coarse) mesh is
just a hexahedron: three edge-sharing tetrahedra, each two sharing a face as well.

356
11
Mesh Regularity
In our ﬁrst (dummy) test, only inner splitting is used: no boundary reﬁnement
is carried out at all. This way, all meshes at all levels remain conﬁned to the initial
hexahedron. As a result, Ω remains poorly approximated.
Why is this test important? Well, it may help ﬁlter out the effect of inner splitting
only: may this affect regularity?
Fortunately, not much. Indeed, from Table11.1, it turns out that, in 11 levels,
regularity decreases by 50% only. Furthermore, different regularity estimates have
nearly the same value: minimal sine is as large as ball ratio. This tells us that no
tetrahedron is probably as thin as in Fig.11.4.
Since no boundary reﬁnement is used, no new valley is produced. Therefore,
before each reﬁnement step, the tetrahedra are ordered in terms of maximal edge
only, as in Chap.10, Sect.10.1.4. After all, no valley is ﬁlled, so no deep-valley
criterion is relevant. This approach is used in the next test as well.
In the second test, we move on to a more interesting implementation: use boundary
reﬁnement as well, to help approximate Ω better. Still, we don’t use the trick in
Chap.10, Sect.10.3.3, as yet: no valley is ﬁlled as yet. After all, ∂Ω is rather smooth,
so the mesh in Fig.10.6 is only slightly concave, producing no overlapping tetrahedra
in the next reﬁnement steps.
To estimate the accuracy of the mesh, we also report the volume error:

  
Ω
dxdydz −

t⊂M, t has 2–4 corners in Ω
  
t
dxdydz.

.
This is discussed in detail in Chap.12 below.
From Tables11.2–11.3, it turns out that it is indeed a good idea to order the
tetrahedra by maximal edge before each reﬁnement step. Although this may require
more nodes, this is a price worth paying for the sake of better accuracy and regularity.
Indeed, regularity decreases as slowly as linearly, whereas accuracy improves as fast
as exponentially.
In the third and ﬁnal test, on the other hand, we let the deep-valley criterion
dominate the maximal-edge criterion. Before each reﬁnement step, the tetrahedra
are now ordered as in Chap.10, Sect.10.3.3. Then, the tetrahedra split in this order
as well, at the edge of deeper valley, if any. This may help ﬁll the valleys in Fig.10.7.
Unfortunately, in the ﬁrst three levels, regularity drops (Table11.4). After all, the
original hexahedron is slightly concave from below, so a submaximal edge may split
there, leaving a maximal edge coarse and long. Fortunately, in the next higher levels,
things get better: the regularity remains nearly constant, with a very good accuracy
in a moderate number of nodes. Thus, in practice, it may make sense to ignore those
old valleys that already exist in the initial mesh.

11.3 Numerical Experiment
357
Table 11.1 The dummy test: no boundary reﬁnement is carried out, so all meshes are conﬁned to
the original hexahedron, with inner reﬁnement only. Three regularity estimates are reported at the
11th level
Ordering strategy
Minimal ballR
Minimal sine
Minimal maxPsine
Leave disordered
0.012
0.007
0.023
Order by maximal
edge
0.033
0.056
0.129
Table 11.2 The nonconvex domain: R2 = 1, R1 = 0.75. The tetrahedra are left disordered. No
valley is ﬁlled
Level
Nodes
Tetrahedra
Minimal
ballR
Minimal
sine
Minimal
maxPsine
Volume
error
1
5
3
0.0548
0.1301
0.24839
0.2305
2
8
6
0.0758
0.1330
0.24569
0.2305
3
14
24
0.0465
0.1045
0.18739
0.1942
4
32
84
0.0307
0.0371
0.06409
0.1358
5
74
276
0.0164
0.0328
0.07749
0.0579
6
173
720
0.0119
0.0328
0.04239
0.0234
7
447
1938
0.0088
0.0146
0.02999
0.0153
8
1080
5187
0.0090
0.0091
0.01649
0.0084
9
2770
13740
0.0060
0.0071
0.0081
0.0037
10
7058
36651
0.0035
0.0036
0.0093
0.0017
11
18116
92792
0.0018
0.0021
0.0059
0.0003
Table 11.3 Before each reﬁnement step, order the tetrahedra by maximal edge: those with a longer
edge before others. No valley is ﬁlled
Level
Nodes
Tetrahedra
Minimal
ballR
Minimal
sine
Minimal
maxPsine
Volume
error
1
5
3
0.0547
0.1301
0.2483
0.2305
2
8
6
0.0758
0.1330
0.2456
0.2305
3
14
24
0.0465
0.1045
0.1873
0.1942
4
32
84
0.0307
0.0371
0.0640
0.1358
5
83
282
0.0170
0.0371
0.0525
0.0588
6
212
858
0.0233
0.0269
0.0524
0.0279
7
560
2472
0.0103
0.0173
0.0299
0.0145
8
1530
7386
0.0102
0.0148
0.0285
0.0073
9
4297
21516
0.0048
0.0077
0.0134
0.0027
10
11897
61446
0.0048
0.0058
0.0140
0.0006
11
32976
168602
0.0048
0.0062
0.0135
0.00001

358
11
Mesh Regularity
Table 11.4 The deep-valley criterion dominates the maximal-edge criterion. This way, an edge
along a valley splits early, even though it is submaximal
Level
Nodes
Tetrahedra
Minimal
ballR
Minimal
sine
Minimal
maxPsine
Volume
error
1
5
3
0.0547
0.1301
0.2483
0.2305
2
9
16
0.0094
0.0055
0.0130
0.2199
3
19
50
0.0045
0.0026
0.0039
0.2100
4
48
176
0.0045
0.0026
0.0039
0.1522
5
128
523
0.0049
0.0020
0.0053
0.0726
6
324
1479
0.0048
0.0020
0.0053
0.0252
7
879
4263
0.0049
0.0020
0.0053
0.0132
8
2484
12674
0.0040
0.0020
0.0053
0.0030
9
7197
37603
0.0035
0.0013
0.0053
0.00013
11.4
Exercises
1. Show that, in terms of maximal proportional sine, the unit tetrahedron T has the
maximal possible regularity: 1.
2. Write the even equilateral tetrahedron explicitly, and calculate its minimal sine
and its maximal proportional sine.
3. Show that, in terms of minimal sine, the even equilateral tetrahedron has the
maximal possible regularity.
4. Whyarebothminimalsineandmaximalproportionalsinenotasrobustasminimal
angle or ball ratio? Hint: see Fig.11.4.
5. Write (and prove) an explicit formula to calculate the ball ratio. Hint: see
Sect.11.2.3.

Chapter 12
Numerical Integration
Does our multilevel reﬁnement work well? Does it approximate well the original
domain? To check on this, we use numerical integration.
Fortunately, our numerical results are encouraging: as the mesh reﬁnes, the numer-
ical integral gets more and more accurate. This indicates that our original algorithm
is indeed robust and could be used in even more complicated domains.
Of course, there is a price to pay: regularity must decrease. After all, this is
why tetrahedra are so suitable to serve as ﬁnite elements in our mesh: they are
ﬂexible, and may come in all sorts of shapes and sizes. This is indeed the key for
an efﬁcient local reﬁnement: use small tetrahedra only where absolutely necessary.
Still, to approximate the curved boundary well, some tetrahedra must also be a little
thin. Fortunately, in our numerical experiments, regularity decreases only moderately
and linearly from level to level. This is good enough: after all, it is unavoidable and
indeed worthwhile to compromise some regularity for the sake of high accuracy.
12.1
Integration in 3-D
12.1.1
Volume of a Tetrahedron
Consider again a tetrahedron of the form
t ≡(k, l, m, n),
vertexed at k, l, m, and n (Fig.9.1). In Chap.9, Sect.9.1.2, we have already seen how
to integrate in t, using an easier calculation in the unit tetrahedron T :
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_12
359

360
12
Numerical Integration
  
t
F(x, y, z)dxdydz = | det(St)|
  
T
(F ◦Et) (x, y, z)dxdydz,
where Et maps T onto t, and St is its Jacobian. In this chapter, we explain this
formula in some more detail and in a wider context.
The standard coordinates x, y, and z used in t could also be written in terms of
reference coordinates: ˆx, ˆy, and ˆz in T (Fig.8.5). These new coordinates could be
deﬁned implicitly in T by
⎛
⎝
x
y
z
⎞
⎠= Et
⎛
⎝
⎛
⎝
ˆx
ˆy
ˆz
⎞
⎠
⎞
⎠= k + St
⎛
⎝
ˆx
ˆy
ˆz
⎞
⎠,
where Et and St are as in Chap.9, Sect.9.1.1. This way, every point (x, y, z) ∈t is
given uniquely in terms of its own reference point (ˆx, ˆy, ˆz) ∈T .
The reference coordinates can now be used to integrate in t. In particular, the
volume of t is
  
t
dxdydz =
  
T
det
∂(x, y, z)
∂(ˆx, ˆy, ˆz)
	 d ˆxd ˆydˆz
=
  
T
|det (St)| d ˆxd ˆydˆz
= |det (St)|
  
T
d ˆxd ˆydˆz
= |det (St)|
6
.
(See Chap.8, Sect.8.9.5, and verify that the volume of T is indeed 1/6.) This result
is particularly useful in the numerical integration below.
12.1.2
Integral in 3-D
We’ve already seen integration in two spatial dimensions (Chap.8, Sect.8.7.3). Let’s
extend this to three spatial dimensions as well. For this purpose, we can use the mesh
designed in Chap.10. All that is left to do is to let the mesh size approach zero:
meshsize(M) ≡
max
t⊂M, t has 2–4 corners in Ω
maxEdge(t) →0.
Consider a domain Ω ⊂R3, and assume that a function f ≡f (x, y, z) is deﬁned
in it. Assume also that a multilevel hierarchy of meshes is available to approximate
Ω better and better. Let M be some mesh in this hierarchy.

12.1 Integration in 3-D
361
The following limit, if indeed exists, deﬁnes the integral of f in Ω:
  
Ω
f (x, y, z)dxdydz ≡
lim
meshsize(M) →0

t≡(k,l,m,n)⊂M, t has 2–4 corners in Ω
|det (St)|
6
· f (k) + f (l) + f (m) + f (n)
4
.
12.1.3
Singularity
But what if f had a singularity? Well, if f is not well-deﬁned at some corner (say k)
in some tetrahedron t, then, in the contribution from t to the above sum, substitute
f (k) ←f (l) + f (m) + f (n)
3
.
If the singularity is not too sharp, then this might help. For example, assume that, at
a distance r from k, f is as small as
| f | ≤constant · r−2.
Assume also that the mesh is fairly regular, so t is rather thick. In this case, f is as
small as
max (| f (l)| , | f (m)| , | f (n)|) ≤constant · maxEdge−2(t).
Thus, the above substitution indeed helps. After all, the contribution from t is also
multiplied by the volume of t, which dominates: it is as small as
|det (St)|
6
≤maxEdge3(t).
Let’s see some interesting examples. For this purpose, let’s introduce spherical coor-
dinates in three spatial dimensions.

362
12
Numerical Integration
12.2
Changing Variables
12.2.1
Spherical Coordinates
We’ve already used spherical coordinates implicitly to construct the unit sphere in
the ﬁrst place (Chap.6, Sect.6.1.4). Here, however, we introduce them fully and
explicitly, and use them more widely.
A nonzero vector (x, y, z) ∈R3 could be written in terms of its unique spherical
coordinates:
• r ≥0: the magnitude of the vector,
• −π/2 ≤φ ≤π/2: the angle between the original vector and its orthogonal
projection onto the x-y plane,
• and 0 ≤θ < 2π: the angle between this projection and the positive part of the
x-axis.
Our new independent variables are now r, θ, and φ. θ is known as the azimuthal
angle: it is conﬁned to the horizontal x-y plane. φ, on the other hand, measures the
elevation from the x-y plane upwards. Its complementary angle, π/2 −φ, is known
as the polar angle between the original vector and the positive part of the z-axis
(Fig.12.1).
The original Cartesian coordinates x, y, and z can now be viewed as dependent
variables. After all, they now depend on our new independent variables r, θ, and φ:
Fig. 12.1 The vector
(x, y, z) in its spherical
coordinates: r (the
magnitude of the vector), φ
(the angle between the
original vector and its
projection onto the x-y
plane), and θ (the angle
between this projection and
the x-axis)
A
A
A
A
A











r =

x2 + y2 + z2
(x, y, 0)
y
z
π
2 −φ
x
θ
x
pU

12.2 Changing Variables
363
x = r · cos(φ) cos(θ)
y = r · cos(φ) sin(θ)
z = r · sin(φ).
Let’s use the new spherical coordinates in integration.
12.2.2
Partial Derivatives
Since x, y, and z are functions of r, θ, and φ, they also have partial derivatives with
respect to them (Chap.8, Sects.8.9.1–8.9.4). For example, the partial derivative of
x with respect to θ, denoted by ∂x/∂θ, is obtained by keeping r and φ ﬁxed, and
differentiating x as a function of θ only. These partial derivatives form the Jacobian.
12.2.3
The Jacobian
As discussed in Chap.8, Sect.8.9.4, the Jacobian is the matrix of partial derivatives:
∂(x, y, z)
∂(r, θ, φ)
≡
⎛
⎜⎝
∂x
∂r
∂x
∂θ
∂x
∂φ
∂y
∂r
∂y
∂θ
∂y
∂φ
∂z
∂r
∂z
∂θ
∂z
∂φ
⎞
⎟⎠
=
⎛
⎝
cos(φ) cos(θ) −r · cos(φ) sin(θ) −r · sin(φ) cos(θ)
cos(φ) sin(θ)
r · cos(φ) cos(θ)
−r · sin(φ) sin(θ)
sin(φ)
0
r · cos(φ)
⎞
⎠.
12.2.4
Determinant of Jacobian
As in Chap.2, Sect.2.1.1, the determinant of the above Jacobian is
det
∂(x, y, z)
∂(r, θ, φ)
	
= det
⎛
⎝
⎛
⎝
cos(φ) cos(θ) −r · cos(φ) sin(θ) −r · sin(φ) cos(θ)
cos(φ) sin(θ)
r · cos(φ) cos(θ)
−r · sin(φ) sin(θ)
sin(φ)
0
r · cos(φ)
⎞
⎠
⎞
⎠

364
12
Numerical Integration
= r2 cos(φ) det
⎛
⎝
⎛
⎝
cos(φ) cos(θ) −sin(θ) −sin(φ) cos(θ)
cos(φ) sin(θ)
cos(θ)
−sin(φ) sin(θ)
sin(φ)
0
cos(φ)
⎞
⎠
⎞
⎠
= r2 cos(φ)

sin2(φ)

sin2(θ) + cos2(θ)

+ cos2(φ)

cos2(θ) + sin2(θ)

= r2 cos(φ)

sin2(φ) + cos2(φ)

= r2 cos(φ).
Thanks to this determinant, we can now go ahead and integrate in spherical coordi-
nates.
12.2.5
Integrating a Composite Function
Let’s write f as a composite function of the spherical coordinates r, θ, and φ:
f (x, y, z) ≡f (x(r, θ, φ), y(r, θ, φ), z(r, θ, φ)) ≡f (r, θ, φ).
This way, we can now go ahead and integrate in spherical (rather than Cartesian)
coordinates:
  
Ω
f (x, y, z)dxdydz
=
  
Ω
f (r, θ, φ)
det
∂(x, y, z)
∂(r, θ, φ)
	 drdθdφ
=
  
Ω
f (r, θ, φ)r2 cos(φ)drdθdφ.
For this purpose, however, Ω must be written in spherical coordinates as well. In
some symmetric cases, this is easy to do.
12.3
Integration in the Meshes
12.3.1
Integrating in a Ball
Assume that Ω is a ball of radius R > 0, centered at the origin:
Ω ≡

(x, y, z) | x2 + y2 + z2 ≤R2
=

(r, θ, φ) | 0 < r ≤R, 0 ≤θ < 2π, −π
2 < φ < π
2

.

12.3 Integration in the Meshes
365
Note that, in spherical coordinates, the vertical line x = y = 0 (or φ = ±π/2) must
be excluded from Ω, because θ is not deﬁned uniquely there, and the Jacobian is
singular there. Fortunately, this line has no effect on the integral: it has zero volume,
or zero three-dimensional measure.
Assume also that the integrand f depends on r only:
f ≡f (r).
In this case, the integral of f in Ω is
  
Ω
f dxdydz =
  
Ω
f (r)r2 cos(φ)drdθdφ
=
 2π
0
dθ
 π/2
−π/2
cos(φ)dφ
 R
0
f (r)r2dr
= 2π

sin
π
2

−sin
−π
2
		  R
0
f (r)r2dr
= 4π
 R
0
f (r)r2dr.
Here are some straightforward examples. If f (r) = 1/r, then we have
  
Ω
dxdydz

x2 + y2 + z2 = 4π
 R
0
r · dr = 2πR2
(Chap.8, Sect.8.5.3). If, on the other hand, f ≡1 is just the constant integrand, then
we obtain the volume of the ball of radius R:
  
Ω
dxdydz = 4π
 R
0
r2dr = 4π
3 R3.
12.3.2
Stopping Criterion
The above formula may help calculate the volume error in Chap.11, Sect.11.3.2.
This way, our multilevel reﬁnement proves robust: the volume error indeed decreases
rapidly as the mesh reﬁnes.
In more general cases, on the other hand, Ω is not as simple, and its volume is
not available in a closed analytic form. How then could we make sure that our mul-
tilevel reﬁnement works well? Well, on each mesh, calculate the numerical integral.
Then, calculate the difference between each two consecutive numerical integrals: the
numerical integral on this level, minus the numerical integral on the previous level.
Does this difference approach zero (in absolute value) as the mesh reﬁnes? If it does,

366
12
Numerical Integration
then we must be on the right track. If, on some level, it fails to decrease, then some
tetrahedra might overlap, so we should better stop reﬁning.
12.3.3
Richardson Extrapolation
To have a yet better numerical integral, one might also want to use Richardson
extrapolation: take the numerical integral on this level, and multiply it by 4/3. Then,
takethenumericalintegralonthepreviouslevel,andmultiplyitby1/3.Then,subtract
the latter from the former. The result may improve on the standard numerical integral:
it is often twice as accurate. Furthermore, like the standard numerical integral in
Sect.12.3.2, it could be used in a stopping criterion as well.
12.4
Exercises
1. As in Sect.12.3.1, use spherical coordinates to compute the integral
  
Ω
dxdydz

x2 + y2 + z2 ,
where Ω is the ball of radius R2 (centered at the origin), minus a smaller ball of
radius R1.
2. Let R1 →0 What is the limit?
3. Use the above exercises to deﬁne and compute the integral
  
Ω
dxdydz

x2 + y2 + z2 ,
where Ω is the ball of radius R2, centered at the origin.
4. Use spherical coordinates to compute the integral
  
Ω
dxdydz
x2 + y2 + z2 ,
where Ω is the ball of radius R2 (centered at the origin), minus a smaller ball of
radius R1.
5. Let R1 →0. What is the limit?
6. Use the above exercises to deﬁne and compute the integral
  
Ω
dxdydz
x2 + y2 + z2 ,

12.4 Exercises
367
where Ω is the ball of radius R2, centered at the origin.
7. Explain why the numerical integral in Sect.12.1.2 sums contributions only from
those tetrahedra with at least two corners in Ω.
8. Explain why each such tetrahedron contributes the average of f over its four
corners, times its volume.
9. Explain why, at a corner of singularity, the missing value of f should be replaced
by the average of f over the three other corners (in Sect.12.1.3).

Chapter 13
Spline: Variational Model in Three
Spatial Dimensions
Once our mesh is sufﬁciently regular and accurate, basis functions (B-splines) can
be deﬁned in it. What is a basis function? It has the following properties:
• piecewise polynomial,
• continuous throughout the entire mesh,
• has only one nonzero degree of freedom in the mesh,
• its gradient is continuous across all edges and at all side midpoints,
• its Hessian is continuous at all mesh nodes.
Why are they called basis functions? Because they can combine to form a much more
general function. In this sense, they span a wide space of functions.
Indeed, consider a general function, with the following properties only:
• piecewise polynomial,
• its gradient is continuous at all mesh nodes and all edge- and side midpoints,
• its Hessian is continuous at all mesh nodes.
Why is this a more general function? Because it must meet fewer requirements than
before. Still, it can be written as a unique linear combination of the basis functions.
This is why they are called basis functions: they indeed form a new basis for this
function space.
What is a spline? It is a function of the above type, splined (nailed) at the mesh
nodes: at each individual node, it must have some prescribed value. This is called a
spline.
In theory, there are many possible splines. After all, the total number of degrees
of freedom is very big: much bigger than the total number of nodes. Still, we are
looking for one special spline: the minimum-energy spline.
Let’s formulate the problem in a slightly different way. The mesh nodes form a
discrete (nonuniform) grid. The original values in the grid are called Dirichlet data.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_13
369

370
13
Spline: Variational Model in Three Spatial Dimensions
Together, they form a discrete grid function, deﬁned at the nodes only. The challenge
is to extend the original grid function, and deﬁne it not only at the nodes but also in
between. This extension should have as little “energy” as possible.
Fortunately, this problem can be formulated algebraically as a system of linear
equations. The solution to this system gives us the long vector that contains the
unknown degrees of freedom in the entire mesh, as required.
13.1
Expansion in Basis Functions
13.1.1
Degrees of Freedom
Thanks to multilevel reﬁnement (Chaps.10, 11), we now have a fairly regular mesh
M, which approximates the original three-dimensional domain well. Let N be the
set of nodes, E the set of edges, and L the set of sides in M. This way, M contains
|N| nodes, |E| edges, and |L| sides.
What are the degrees of freedom in M? Well, each node has ten degrees of freedom
(Chap.9, Sects.9.5.3–9.5.5), making a total of 10|N| degrees of freedom in M.
Furthermore, each edge midpoint has two degrees of freedom (Chap.9, Sect.9.5.2),
making 2|E| more degrees of freedom in M. Finally, each side midpoint has one
degree of freedom (Chap.9, Sect.9.5.1), making |L| more degrees of freedom in M.
Thus, in total, M has
K ≡10|N| + 2|E| + |L|
degrees of freedom.
For each of these degrees of freedom, we have one basis function: a continuous
piecewise-polynomial function, with only one nonzero degree of freedom in M. Let’s
go ahead and use them.
13.1.2
The Function Space and Its Basis
Why are these functions called basis functions? Because they form a basis! Indeed,
they make a unique linear combination to help write every given function.
More precisely, not quite every function, but only a function f with the following
properties:
1. f is piecewise polynomial: in each individual tetrahedron t ⊂M, f is a (different)
polynomial of degree ﬁve.
2. In each individual node in N, f has continuous gradient and Hessian.

13.1 Expansion in Basis Functions
371
3. In every edge- and side-midpoint in M, f has continuous nontangential partial
derivatives.
As we’ve seen in Chap.9, Sect.9.4.2, f must then be rather smooth: it must have a
continuous gradient throughout the entire edge (for all edges in E). Furthermore, f
must also be continuous throughout M (Chap.9, Sect.9.4.3).
Thus, f can be written as a unique linear combination of the basis functions:
f =

j∈N
cj,1ψj,1 + cj,2ψj,2 + · · · + cj,10ψj,10
+

h=(j+q)/2, (j,q)∈E
ch,1ψh,1 + ch,2ψh,2
+

w=(j+q+u)/3, △(j,q,u)∈L
cwψw.
What are the coefﬁcients in this expansion? Well, they are just the corresponding
degrees of freedom of f . For example, cw is the nontangential derivative of f at the
side midpoint w, say
cw ≡fz(w)
(Chap.9, Sects.9.1.4 and 9.5.2).
Why is the above expansion true? Well, consider some tetrahedron t ⊂M. Now,
in t, both sides of the above expansion are polynomials of degree ﬁve with the
same 56 degrees of freedom. Therefore, they must coincide throughout t (Chap.9,
Sect.9.4.1). This is true for every t ⊂M. Thus, they must also coincide throughout
M, as required.
This means that the basis functions indeed form a basis. After all, every function
f with the above properties can be written as a unique linear combination of them.
In other words, the basis functions span a new function space, containing all these
f ’s.
To represent f yet more easily, let’s reindex the basis functions in a simpler way,
and list them one by one in a row:
ψ1, ψ2, ψ3, . . . , ψK.
This way, f could be written simply as
f =
K

j=1
c jψ j.
This form will be the most useful below.

372
13
Spline: Variational Model in Three Spatial Dimensions
13.2
The Stiffness Matrix
13.2.1
Assemble the Stiffness Matrix
To design the optimal spline, we need a new K × K matrix. This is the coefﬁcient
matrix (or the stiffness matrix), denoted by A. Let’s specify its elements:
ai, j ≡
  
M
∇tψ j∇ψidxdydz
=

t⊂M
  
t
∇tψ j∇ψidxdydz
=

t⊂M
|det (§t)|
  
T
∇t 
ψ j ◦Et

S−1
t
S−t
t ∇(ψi ◦Et) dxdydz
(1 ≤i, j ≤K, Chap.9, Sect.9.1.3). This way, to calculate A, we can now integrate
in T rather than t.
How to calculate A efﬁciently? Fortunately, this could be done iteratively. Initially,
A isjustthezero K×K matrix.Then,scanthetetrahedraonebyone.Eachtetrahedron
t ⊂M may then add a new contribution to each matrix element:
ai, j ←ai, j + |det (§t)|
  
T
∇t 
ψ j ◦Et

S−1
t
S−t
t ∇(ψi ◦Et) dxdydz.
Clearly, this contribution is often zero. Only if both ψ j and ψi are nonzero in t may
the contribution be nonzero. In other words, only if both ψ j and ψi have a nonzero
degree of freedom at a corner or an edge- or side-midpoint in t may the contribution
be nonzero.
Alternatively, one could also use the chain rule in Chap.9, Sect.9.2.5, to integrate
in terms of barycentric coordinates. After all, in practice, this integration is in T as
well. This is particularly relevant if the ψi’s are still available in terms of barycentric
coordinates in each tetrahedron t.
Once all nonzero contributions have been assembled from all tetrahedra, the coef-
ﬁcient matrix A is ready. Below, we explain why this matrix is indeed relevant to
design the best spline.

13.2 The Stiffness Matrix
373
13.2.2
How to Order the Basis Functions?
How to order the basis functions
ψ1, ψ2, ψ3, . . . , ψK
one by one in a row? So far, this wasn’t speciﬁed! After all, this didn’t really matter.
Now, let’s go ahead and order them more explicitly, at least blockwise. More pre-
cisely, let’s assume that those corner basis functions in Chap.9, Sect.9.5.5, are listed
last.
More formally, let’s split our original index set into two disjoint subsets:
{1, 2, 3, . . . , K} = R ∪G,
where
R ≡{1, 2, 3, . . . , K −|N|}
G ≡{K −|N| + 1, K −|N| + 2, K −|N| + 3, . . . , K} .
Assume that G indexes those corner basis functions in Chap.9, Sect.9.5.5:
ψG ≡

ψK−|N|+1, ψK−|N|+2, ψK−|N|+3, . . . , ψK

≡

ψj,1

j∈N .
This way, R indexes the rest of the basis functions, contained in ψR. This also splits
the coefﬁcients in Sect.13.1.2 into two subvectors:
c ≡(c1, c2, c3, . . . , cK)t =
 cR
cG
	
.
Assume that f in Sect.13.1.2 is just a grid function: it is only given at the discrete
nodes, not in between. This means that only those |N| degrees of freedom in cG are
available:
cj,1 = f (j), j ∈N
(Chap.9, Sect.9.5.5). This splines (or nails) f at the nodes only, leaving it still free
and unspeciﬁed in between. To specify f uniquely in between the nodes as well, we
must also specify the rest of the degrees of freedom in cR. This would indeed deﬁne
the desired spline: the best extension that still agrees with the original grid function
at the individual nodes.

374
13
Spline: Variational Model in Three Spatial Dimensions
13.3
Finding the Optimal Spline
13.3.1
Minimum Energy
What does “best” mean? Well, for this purpose, let’s deﬁne the energy of f . In
physics, this is often called kinetic energy (Chap.14, Sect.14.8.3):
energy( f ) ≡
  
M
∥∇f ∥2
2dxdydz
=
  
M
∇t f ∇f dxdydz
=
  
M
K

j=1
c j∇tψ j
K

i=1
ci∇ψidxdydz
=
K

j=1
K

i=1
c jci
  
M
∇tψ j∇ψidxdydz
=
K

j=1
K

i=1
c jai, jci
= ct Ac.
To minimize this kind of energy, let’s decompose A cleverly.
13.3.2
The Schur Complement
Thanks to the above splitting, A has a new block form:
A =
 ARR ARG
AGR AGG
	
.
In practice, no explicit reordering of rows or columns is needed: the above block
form may remain implicit. Still, it is particularly useful to minimize the energy.
Fortunately, A is symmetric. Therefore,
ARG = At
GR.
Furthermore, ARR is symmetric as well. Moreover, ARR is also positive deﬁnite: it
has positive eigenvalues only (see exercises below). Therefore, ARR is nonsingular:
it has a unique inverse matrix A−1
RR. Moreover, A−1
RR is symmetric and positive deﬁnite
as well. Thus, A can be written as a triple product:

13.3 Finding the Optimal Spline
375
A =
 ARR ARG
AGR AGG
	
=
 ARR 0
AGR I
	 
A−1
RR 0
0
S
	  ARR ARG
0
I
	
,
where I is the identity matrix of order |N|, and S is the Schur complement matrix:
S ≡AGG −AGR A−1
RR ARG.
Thanks to this decomposition, the energy can now be written as
ct Ac = ct
 ARR 0
AGR I
	 
A−1
RR 0
0
S
	  ARR ARG
0
I
	
c
=
 ARR ARG
0
I
	
c
	t 
A−1
RR 0
0
S
	  ARR ARG
0
I
	
c
=
 ARRcR + ARGcG
cG
	t 
A−1
RR 0
0
S
	  ARRcR + ARGcG
cG
	
= (ARRcR + ARGcG)t A−1
RR (ARRcR + ARGcG) + ct
GScG.
How to minimize this? Clearly, the latter term is not in our hands to minimize. Indeed,
it is ﬁxed: cG is given in advance. So, we can only “play” with the former term. How
to minimize it? Best make it zero! Indeed, since A−1
RR is symmetric and positive
deﬁnite, it would be best to pick
ARRcR + ARGcG = 0,
or
ARRcR = −ARGcG.
Since ARR is nonsingular, this system indeed has a unique solution cR, which can be
found iteratively, as in Chap.17 in [61]. This produces the entire vector c, including
the new degrees of freedom in cR, required to extend f to the entire mesh. This is
indeed the desired spline.
13.4
Exercises
1. Show that the coefﬁcient (stiffness) matrix A is indeed symmetric.
2. Conclude that the eigenvalues of A are real. Hint: see Chap.1, Sect.1.9.4.
3. Conclude that the eigenvectors of A are orthogonal to each other. Hint: see
Chap.1, Sect.1.9.5.
4. Let v be a K-dimensional vector. Show that vt Av ≥0. Hint: use the very
deﬁnition of A to show that vt Av is just energy.

376
13
Spline: Variational Model in Three Spatial Dimensions
5. Conclude that the eigenvalues of A are nonnegative. Hint: pick v above as an
eigenvector.
6. Show that ARR (the upper left block in A) is symmetric as well.
7. Conclude that the eigenvalues of ARR are real.
8. Conclude also that the eigenvectors of ARR are orthogonal to each other.
9. Let v be a (K −|N|)-dimensional vector. Show that vt ARRv ≥0. Hint: add
dummy zero components. In other words, let 0 be the |N|-dimensional zero
column vector. This way,
vt ARRv =

vt, 0t
A
 v
0
	
≥0.
10. Conclude that the eigenvalues of ARR are nonnegative.
11. Show that, if vt ARRv = 0, then v must be the zero vector. Hint: use the compo-
nents of v as degrees of freedom in a new function
g ≡

j∈R
v jψ j,
with zero energy. In each tetrahedron t ⊂M, pick a corner q ∈t. For each point
(x, y, z) ∈t, use the line integral
g(x, y, z) = g(q) +
 (x,y,z)
q
∇g · dl = 0 +
 (x,y,z)
q
0 · dl = 0.
12. Conclude that, if v is a nonzero vector, then vt ARRv > 0.
13. Conclude that the eigenvalues of ARR are positive.
14. Conclude that ARR is nonsingular: it has a unique inverse matrix A−1
RR.
15. Show that A−1
RR is symmetric as well.
16. Show that A−1
RR has the same eigenvectors as ARR, with the reciprocal eigenvalue.
17. Let v be a nonzero (K −|N|)-dimensional vector. Show that vt A−1
RRv > 0. Hint:
write v as v = ARR A−1
RRv.
18. Conclude that, to minimize the energy of the spline, cR should better solve the
linear system
ARRcR = −ARGcG.

Part V
Advanced Applications in Physics and
Chemistry
Finally, we combine both linear algebra and group theory in practical applications
in quantum chemistry and general relativity. First, we use group theory to introduce
the permutation group and study the determinant of a square (complex) matrix.
Once the determinant is used in our quantum-mechanical model, we can write the
expected energy and obtain the Hartree–Fock system: a pseudo-eigenvalue problem.
Thanks to linear algebra, the (generalized) eigenvectors have a desirable property:
orthogonality. This is indeed how group theory and linear algebra combine to form
a complete theory with practical applications.
We conclude with an interesting application in general relativity: Einstein equa-
tions. To introduce them, we must use new features in linear algebra: tensors and
algebraic operations between them. For this purpose, we must introduce a new prin-
ciple: Einstein’s summation convention. It improves on the standard sums used in
linear algebra. In fact, it tells us how to raise and lower indices and come up with
a coherent summation strategy. Thanks to it, the nonlinear system of equations gets
particularly easy to introduce.

Chapter 14
Quantum Chemistry: Electronic
Structure
Let’s see how linear algebra and group theory can combine in a practical application
in quantum chemistry: the electronic structure in an atom or a molecule. Indeed, the
position of each electron is a random variable: we can never tell it for sure, but only
at some probability. Likewise, energy and momentum are nondeterministic as well:
we can never know what they are precisely, but only with some uncertainty. This is
not because we are ignorant, but because nature is stochastic!
Fortunately, we can still tell where each electron might be, and how likely this is.
For this purpose, we need its wave function (orbital). How to ﬁnd the correct orbital?
For this purpose, we need to gather the kinetic and potential energy of the electron,
which come from its electrostatic attraction to the nucleus and repulsion from other
electrons. Again, this is a random variable: it can never be calculated explicitly, but
only in terms of expectation. Still, this is good enough: thanks to the power of linear
algebra, the orbital can be solved for as an eigenvector, with a physical eigenvalue:
its energy level.
Each electron may have a different orbital, with a different energy. These orbitals
are(generalized)eigenvectorsofthesameHermitianmatrix.Thankstolinearalgebra,
we can make sure that they are indeed orthonormal (with respect to a relevant inner
product). This is indeed their canonical form.
Why is this important? Because electrons are often indistinguishable from each
other. To deﬁne their orbitals properly, we must also use group theory. Indeed, thanks
to the permutation group, the wave function can take the form of a determinant,
leading to a simple formula for the (expected) energy. Thanks to this model, we
can now handle indistinguishable electrons of the same spin as well. This leads to
the Hartree–Fock system [1]. This is indeed how group theory and linear algebra
combine to form a complete theory, with practical applications.
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_14
379

380
14
Quantum Chemistry: Electronic Structure
14.1
Wave Function
14.1.1
Particle and Its Wave Function
Consider a particle in the three-dimensional Cartesian space. In classical mechanics,
it has a deterministic position: (x, y, z). In quantum mechanics, on the other hand,
its position is nondeterministic: a random variable, known at some probability only.
In Chap.7, Sect.7.9.3, we’ve introduced the state v: a grid function, deﬁned on a
uniform m × m × m grid. This tells us the (nondeterministic) position of the particle
in 3-D. How likely is it to be at (Xi,i, X j, j, Xk,k)? The probability for this is stored
in v: it is given by |vi, j,k|2.
Now, let’s extend this, and make it not only discrete but also continuous. Instead
of v, let’s talk about a wave function w(x, y, z), deﬁned on the entire three-
dimensional Cartesian space. This way, the particle could now be everywhere: not
only on a discrete grid but also in just any point in 3-D. How likely is it to be at
(x, y, z) ∈R3? The probability for this is just |w(x, y, z)|2.
This makes sense: after all, the position should be a continuous random variable,
which may take just any value, not necessarily in a discrete grid.
For this purpose, however, the sums and inner products used in Chap.7 should
be replaced by integrals. In particular, to make a legitimate probability, w must be
normalized to satisfy
  
|v(x, y, z)|2dxdydz = 1,
where each integral sign integrates over one spatial coordinate, from −∞to ∞. This
can be viewed as an extension of the vector norm deﬁned in Chap.1, Sect.1.7.2. In
this sense, w has norm 1. Later on, we’ll make sure that this normalization condition
indeed holds.
So far, we’ve mainly talked about one observable: position. Still, there is yet
another important observable, which can never be continuous: energy. Indeed, only
certain energy levels are allowed, and the rest remain nonphysical. This is indeed
quantum mechanics: energy comes in discrete quantities.
How to uncover the wave function w? This will be discussed below. For simplicity,
we use atomic units, in which the particle has mass 1, and Planck constant is 1 as
well.
14.1.2
Two Particles
Consider now two particles that may interact and even collide with each other. In this
case, they don’t have independent wave functions, but just one joint wave function:

14.1 Wave Function
381
w(r1,r2), where r1 = (x1, y1, z1) and r2 = (x2, y2, z2) are their possible positions.
Once w(r1,r2) is solved for, it must also be normalized to satisfy
     
|w(r1,r2)|2dx1dy1dz1dx2dy2dz2 = 1.
This way, |w(r1,r2)|2 may indeed serve as a legitimate probability function, to tell
us how likely the particles are to be at r1 and r2 at the same time.
Unfortunately, there are still a few problems with this model. First, what about
three or four or more particles? The dimension soon gets too high to handle! Besides,
even with just two particles, the joint wave function is not very informative: it doesn’t
give us any information about each individual particle on its own. Therefore, it may
make sense to assume that the particles don’t interact with each other, so their wave
function can be factored as a product of the form
w(r1,r2) = v(1)(r1)v(2)(r2),
where v(1)(r1) and v(2)(r2) are the wave functions of the individual particles. Later
on, we’ll improve on this model yet more, to handle not only distinguishable but also
indistinguishable particles.
14.2
Electrons in Their Orbitals
14.2.1
Atom: Electrons in Orbitals
Consider now a special kind of particle: an electron. More precisely, consider an
atom with M electrons. In particular, look at the nth electron (1 ≤n ≤M). It has
a (nondeterministic) position rn ≡(xn, yn, zn) in the three-dimensional Cartesian
space.
Where is the electron? We’ll never know for sure! After all, measuring the position
is not a good idea—it can change the original wave function forever, with no return.
Without doing this, the best we can tell is that the electron could be at rn. The
probability for this is |v(n)(rn)|2, where v(n) is the wave function of the nth electron:
its orbital.
In general, v(n) is a complex function, deﬁned in the entire three-dimensional
Cartesian space. Like every complex number, v(n) has a polar decomposition. In
it, what matters is the absolute value. The phase, on the other hand, has no effect
on the probability |v(n)|2. Still, it does play an important role in the dynamics of
the system: it tells us the (linear and angular) momentum of the electron, at least
nondeterministically.
The function v(n)(rn) is also known as the nth orbital: it tells us where the nth
electron could be found in the atom. Unfortunately, v(n) is not yet known. To uncover
it, we must solve a (generalized) eigenvalue problem.

382
14
Quantum Chemistry: Electronic Structure
14.2.2
Potential Energy and Its Expectation
What is the potential energy in the atom? This is a random variable, so we can never
tell it for sure. Fortunately, we can still tell its expectation. For this purpose, assume
that
w(r1,r2, . . . ,rM)
is the joint wave function of all electrons together. This way, |w(r1,r2, . . . ,rM)|2 is
the probability to ﬁnd them at r1, r2, . . ., rn at the same time. Later on, we’ll write w
more explicitly.
The potential energy has a few terms, coming from electrostatics. The ﬁrst term
comes from attraction to the nucleus (assumed to lie at the origin). To have its
expectation, take the probability |w|2 to ﬁnd the electrons at a certain position,
multiply by the potential 1/∥r∥, sum, and integrate:
−
  
· · ·
  
M

n=1
|w|2
∥rn∥dx1dy1dz1dx2dy2dz2 · · · dxMdyMdzM.
This is a 3M-dimensional integral: each integral sign integrates over one individual
spatial coordinate, from −∞to ∞. Later on, we’ll simplify it considerably.
On top of this, there is yet more potential energy. This adds more terms, coming
from the electrostatic repulsion of every two electrons from each other:
  
· · ·
  
M

i=1
M

n=i+1
|w|2
∥ri −rn∥dx1dy1dz1dx2dy2dz2 · · · dxMdyMdzM.
These sums scan all pairs of indices 1 ≤i, n ≤M. Still, only if i < n does the pair
appear. After all, a pair must appear just once, not twice.
These are the Coulomb integrals. Let’s go ahead and simplify them.
14.3
Distinguishable Electrons
14.3.1
Hartree Product
Unfortunately, w is not informative enough: it mixes different orbitals with each
other. We might want to separate variables, and write w as a product of orbitals:
w(r1,r2, . . . ,rM) = v(1)(r1)v(2)(r2) · · · v(M)(rM).

14.3 Distinguishable Electrons
383
This is the Hartree product. Thanks to it, we’ll have more information about the nth
individual electron, and how likely it is to be at rn. In fact, the probability for this is
just |v(n)(rn)|2.
Lateron,we’llseethatthiskindoffactorizationispossibleonlyfordistinguishable
electrons. Indistinguishable electrons, on the other hand, must have a more compli-
cated wave function. Still, for the time being, let’s assume that this factorization is
valid.
14.3.2
Potential Energy of Hartree Product
Assume now that each individual orbital indeed makes a legitimate probability
function:
  
|v(i)(r)|2dxdydz = 1,
1 ≤i ≤M.
In this case, the expectation of the potential energy of the Hartree product simpliﬁes
to read
−
M

n=1
   |v(n)|2
∥r∥dxdydz
+
M

i=1
M

n=i+1
     
|v(i)(r)|2
1
∥r −˜r∥|v(n)(˜r)|2dxdydzd ˜xd ˜yd˜z.
Here, both r ≡(x, y, z) and ˜r ≡(˜x, ˜y, ˜z) are dummy variables, integrated upon in
the latter six-dimensional integral.
14.4
Indistinguishable Electrons
14.4.1
Indistinguishable Electrons
Unfortunately, two electrons can be distinguished from each other only if they have
a different spin: one has spin-up, and the other has spin-down. (See exercises at the
end of Chap.7.) If, on the other hand, they have the same spin, then they can never
be distinguished from each other.
Thus, it would make sense to place our electrons in two disjoint subsets. For this
purpose, let 0 ≤L ≤M be a new integer number. Now, assume that the L former
electrons have spin-up, and the M −L latter electrons have spin-down.

384
14
Quantum Chemistry: Electronic Structure
Let’s focus on the L former electrons. What is their joint wave function? Well, it
can no longer be a simple Hartree product. After all, their indistinguishability must
be reﬂected from their wave function.
14.4.2
Pauli’s Exclusion Principle: Slater Determinant
Thus, the L former electrons must have a more complicated wave function—a deter-
minant of a new L × L matrix:
1
√
L!
det

v(n)(ri)

1≤i,n≤L

.
This way, they also satisfy Pauli’s exclusion principle: two electrons can never have
the same state (same spin and also same orbital). Indeed, in this case, the above
matrix would have two identical columns, so its determinant would vanish. This is
called Slater determinant. To study it, we must redeﬁne the determinant from scratch,
and study its algebraic properties. For this purpose, group theory comes handy.
14.5
The Permutation Group
14.5.1
Permutation
Consider the set of n natural numbers:
{1, 2, 3, . . . , n}.
A permutation is a mapping from this set onto itself. This means that the permutation
maps each natural number 1 ≤i ≤n to a distinct natural number 1 ≤p(i) ≤n.
This is denoted by
p ({1, 2, 3, . . . , n}) .
14.5.2
Switch
For example, the switch
(1 →3)
switches 1 with 3: at the same time, 1 maps to 3, 3 maps to 1, and the rest of the
numbers remain ﬁxed: 2 maps to 2, 4 maps to 4, and so on. For this reason, the switch
is symmetric: it can also be written as

14.5 The Permutation Group
385
(1 →3) = (3 →1).
After the switch, the list of n natural numbers takes a new order:
{3, 2, 1, 4, 5, . . . , n}.
We say that the switch is odd: it picks a minus sign. This is denoted by
e((3 →1)) = −1.
14.5.3
Cycle
A cycle, on the other hand, can be more complicated. For example,
(1 →3 →2)
maps 1 to 3, 3 to 2, and 2 to 1 (at the same time). This is why the cycle is indeed
cyclic: it can also be written as
(1 →3 →2) = (2 →1 →3) = (3 →2 →1).
In all these forms, the new order is
{2, 3, 1, 4, 5, . . . , n}.
What does this cycle do? It maps 1–3. To make room, both 3 and 2 must shift one
space leftwards. As a matter of fact, this can be written as the composition (or product)
of two switches:
(3 →2 →1) = (3 →2)(2 →1).
This composition is carried out right to left: 1 switches with 2, producing
{2, 1, 3, 4, 5, . . . , n}.
Then, it switches with 3 as well, producing
{2, 3, 1, 4, 5, . . . , n},
as required.

386
14
Quantum Chemistry: Electronic Structure
Why is this decomposition useful? Because it tells us that the cycle is even, not
odd. Indeed, each switch picks a minus sign, which cancel each other:
e ((3 →2 →1)) = e ((3 →2)) e ((2 →1)) = (−1)(−1) = 1.
Let’s introduce a short notation for this cycle:
[3 →1] ≡(3 →2 →1).
As discussed above, this cycle is even:
e([3 →1]) = 1.
This is also called a 3-cycle. The switch, on the other hand, is also called a 2-cycle.
Likewise, we can also write a yet longer cycle—a 4-cycle:
[4 →1] ≡(4 →3 →2 →1).
This cycle can be decomposed as the composition of three switches:
[4 →1] = (4 →3)(3 →2)(2 →1).
Again, this is read right to left: 1 switches with 2, then with 3, then with 4. The
result is
{2, 3, 4, 1, 5, 6, . . . , n},
as required. This is why this cycle is odd, not even:
e([4 →1]) = −1,
and so on.
14.5.4
Permutation Group
How does a general permutation look like? Well, suppose that 1 maps to some
1 ≤k ≤n. This occupies k: the rest of the numbers (from 2 to n) can no longer
map to k. To meet this condition, they should be mapped in two stages: ﬁrst, mix
them (using a smaller permutation). Then, shift those numbers that lie from 2 to k
one space leftwards. This way, k is not used, as required. In summary, the original
permutation has been decomposed as
p ({1, 2, 3, . . . , n}) = [k →1]q ({2, 3, 4, . . . , n}) ,
for a unique (smaller) permutation q that mirrors p.

14.5 The Permutation Group
387
Let’s place all permutations on {1, 2, 3, . . . , n} in a new group:
P ({1, 2, 3, . . . , n}) .
Why is this a legitimate group? Well, it has an algebraic operation: composition of
mappings. This way, it is indeed associative. Furthermore, it contains the identity
permutation that changes nothing. Once the identity permutation is composed with
any other permutation, it leaves it as is. Finally, every permutation has a unique
inverse permutation that undoes it.
Thanks to the above, the entire permutation group can be written as the union of
smaller groups:
P ({1, 2, 3, . . . , n}) = ∪n
k=1[k →1]P ({2, 3, 4, . . . , n}) .
Note that, in this union, the k-cycles [k →1] have alternating signs: even, odd, even,
odd, and so on. In other words,
e([k →1]) = (−1)k−1.
This will help redeﬁne the determinant.
14.5.5
Number of Permutations
To use the above group more easily, let’s denote it by
P ≡P ({1, 2, 3, . . . , n})
for short. How big is P? In other words, how many permutations are there in P?
Well, let’s count. 1 can map to n possible numbers. On top of these, 2 can map to
n −1 possible numbers. On top of these, 3 can map to n −2 possible numbers, and
so on. In total, there are n! different permutations in P:
|P| = n!.
Half of them are odd, and half are even. To see this, pick some odd permutation
q ∈P, say
q ≡(1 →2).
This way, for every permutation p ∈P,
e(qp) = e(q)e(p) = −e(p).
Therefore, the invertible mapping

388
14
Quantum Chemistry: Electronic Structure
p →qp
maps every odd permutation to an even one, and every even permutation to an odd
one.
14.6
Determinant
14.6.1
Determinant: A New Deﬁnition
How to use the permutation group in practice? Consider an n × n (complex) matrix:
A ≡

ai, j

1≤i, j≤n .
Let’s redeﬁne its determinant as a new (complex) number: sum of products of
elements. Each product multiplies n elements: one element from each row, and one
element from each column:
det(A) ≡

p∈P
e(p)a1,p(1)a2,p(2)a3,p(3) · · · an,p(n).
Why is this the same as the original deﬁnition in Chap.2, Sect.2.1.1? To see this, use
mathematical induction on n, and use the union in Sect.14.5.4 to mirror the minors
in the original deﬁnition.
14.6.2
Determinant of the Transpose
We can now see why the transpose matrix has the same determinant. Indeed, each
permutation is mirrored by its unique inverse. Furthermore, if it is even (odd), then
its inverse is even (odd) as well. Indeed,
1 = e

pp−1
= e(p)e

p−1
.
Thus, instead of scanning all permutations one by one, scan all inverse permutations:
det(A) ≡

p∈P
e(p)a1,p(1)a2,p(2)a3,p(3) · · · an,p(n)
=

p−1∈P
e(p)a1,p(1)a2,p(2)a3,p(3) · · · an,p(n)

14.6 Determinant
389
=

p−1∈P
e

p−1
ap−1(1),1ap−1(2),2ap−1(3),3 · · · ap−1(n),n
=

p∈P
e (p) ap(1),1ap(2),2ap(3),3 · · · ap(n),n
= det

At
.
Let’s use this to calculate the determinant of a product of two matrices.
14.6.3
Determinant of a Product
Consider now two (complex) matrices of order n:
A ≡

ai, j

1≤i, j≤n
and
B ≡

bi, j

1≤i, j≤n .
What is the determinant of AB? In its new deﬁnition, it is just
det(AB) ≡

p∈P
e(p)(AB)1,p(1)(AB)2,p(2) · · · (AB)n,p(n).
Inside the sum, we have a product of n factors of the form
(AB)i,p(i) ≡
⎛
⎝
n

j=1
ai, jb j,p(i)
⎞
⎠.
The above product scans i = 1, 2, 3, . . . , n, and multiplies these factors one by one.
Upon opening parentheses, one must pick one particular j from each such factor.
Which j to pick? Well, there is no point to pick the same j from two different factors,
say the ith and kth factors. After all, the resulting product will be soon canceled with
a similar product, obtained from a permutation of the form
(i →k)p,
which mirrors p: it is nearly the same as p, but also switches i and k on top, picking
an extra minus sign on top.
So, we better focus on a more relevant option: pick a different j from each factor,
say
j = q(i),
for some permutation q ∈P. This way,

390
14
Quantum Chemistry: Electronic Structure
det(AB)
≡

p,q∈P
e(p)a1,q(1)bq(1),p(1)a2,q(2)bq(2),p(2) · · · an,q(n)bq(n),p(n)
=

p,q−1∈P
e(p)aq−1(1),1b1,pq−1(1)aq−1(2),2b2,pq−1(2) · · · aq−1(n),nbn,pq−1(n)
=

p,q∈P
e(p)aq(1),1b1,pq(1)aq(2),2b2,pq(2) · · · aq(n),nbn,pq(n)
=

r,q∈P
e(r)e(q)aq(1),1b1,r(1)aq(2),2b2,r(2) · · · aq(n),nbn,r(n)
= det

At
det(B)
= det(A) det(B).
In summary, the determinant of the product is indeed the product of the determinants:
det(AB) = det(A) det(B).
Let’s use this result further.
14.6.4
Orthogonal and Unitary Matrix
As a result, if O is an orthogonal matrix, then its determinant is either 1 or −1:
1 = det

Ot O

= det

Ot
det(O) = (det(O))2 .
Furthermore, the determinant of the Hermitian adjoint is the complex conjugate of
the original determinant:
det

Ah
= det
 ¯At
=
¯
det (At) =
¯
det(A).
As a result, if U is a unitary matrix, then its determinant is a complex number of
absolute value 1:
1 = det

U hU

= det

U h
det(U) = | det(U)|2.
This will be useful below.

14.6 Determinant
391
14.6.5
The Characteristic Polynomial
Let’s write the characteristic polynomial as
det(A −λI) = q0 + q1λ + q2λ2 + · · · + qn−1λn−1 + qnλn
(Chap.3, Sect.3.1.1). What are these coefﬁcients? Well, let’s start with the leading
term: qnλn. What is the coefﬁcient qn? To tell this, look at this determinant in terms of
the new deﬁnition. Only one permutation contributes to λn: the identity permutation.
Indeed, it produces the product

a1,1 −λ
 
a2,2 −λ
 
a3,3 −λ

· · ·

an,n −λ

.
Upon opening parentheses, pick −λ from each factor. This produces the leading
term:
qnλn = (−1)nλn.
In summary,
qn = (−1)n.
Next, what is qn−1? Again, only the identity permutation contributes to λn−1. (All
others contribute to λn−2 at most.) As discussed above, it produces the product

a1,1 −λ
 
a2,2 −λ
 
a3,3 −λ

· · ·

an,n −λ

.
Upon opening parentheses, pick −λ from most factors. Only from one factor don’t
pick −λ. This can be done in n different ways, producing
qn−1λn−1 = (−1)n−1λn−1
n

i=1
ai,i.
In summary,
qn−1 = (−1)n−1
n

i=1
ai,i = (−1)n−1trace(A),
where the trace of a matrix is the sum of its main-diagonal elements.
By now, we’ve already uncovered two coefﬁcients in the characteristic polyno-
mial. Finally, what is q0? Again, look at det(A −λI) in its new deﬁnition. This time,
however, look at all permutations. Each permutation produces a product of n factors.
From these factors, never pick −λ. After all, this is the only way to contribute to q0.
The result is
q0 = det(A).

392
14
Quantum Chemistry: Electronic Structure
14.6.6
Eigenvalues and Trace
How to use the above in practice? For this purpose, we must write the characteristic
polynomial in a new form: not as a sum but as a product. Fortunately, it has degree
n, so it has n (complex) roots:
λ1, λ2, λ3, . . . , λn.
At these λi’s, the characteristic polynomial vanishes. (Some of them may be the
same, but this doesn’t matter.) These are indeed the eigenvalues of A. Thanks to
them, the characteristic polynomial can also be written as
det(A −λI) = (λ1 −λ)(λ2 −λ)(λ3 −λ) · · · (λn −λ).
This way, by setting λ = λi (1 ≤i ≤n), we indeed obtain zero, as required.
Furthermore, the leading term is indeed (−1)nλn, as required.
In the latter form, let’s open parentheses, and pick −λ from n −1 factors. This
way, we obtain qn−1λn−1 in a new form:
qn−1λn−1 = (−1)n−1λn−1
n

i=1
λi.
Thus, the trace is also the sum of eigenvalues:
trace(A) =
n

i=1
λi.
Finally, upon opening parentheses in the latter formula, never pick −λ from any
factor. This way, we obtain q0 in a new form
q0 = λ1λ2λ3 · · · λn.
In summary, the determinant is also the product of eigenvalues:
det(A) = λ1λ2λ3 · · · λn.

14.7 Orbitals and Their Canonical From
393
14.7
Orbitals and Their Canonical From
14.7.1
The Overlap Matrix and Its Diagonal From
Let’s use the above in the context of functions, deﬁned in the three-dimensional Carte-
sian space. The overlap of two functions is obtained by integrating them against one
another. This can be viewed as an extension of the inner product, deﬁned in Chap.1,
Sect.1.7.2. This way, we can talk about orthogonality, and even orthonormality.
In particular, once this is calculated for every two orbitals, we obtain the L × L
overlap matrix:
O ≡

Oi,n

1≤i,n≤L ≡
  
¯v(i)(r)v(n)(r)dxdydz

1≤i,n≤L
.
(Although we use the notation “O”, this is not necessarily an orthogonal matrix!)
A proper orbital should have norm 1: overlap 1 with itself. This way, it makes
a proper probability function. Still, by now, we don’t have this property as yet: the
main-diagonal elements Oi,i may still be different from 1.
Fortunately, O is Hermitian and positive semideﬁnite (Chap.1, Sect.1.12.1). As
such, it can be diagonalized by a unitary matrix U, independent of r:
O = U h DU,
where
D ≡diag

D1,1, D2,2, . . . , DL,L

is a diagonal matrix, with the eigenvalues of O on its main diagonal:
Dn,n ≥0,
1 ≤n ≤L.
Often, the orbitals are linearly independent of each other: they have no linear combi-
nation that vanishes (almost) everywhere. In this case, O is not only positive semidef-
inite but also positive deﬁnite—its eigenvalues are strictly positive:
Dn,n > 0,
1 ≤n ≤L.
In this case, the overlap matrix has a positive determinant:
det(O) = det(D) = D1,1D2,2 · · · DL,L > 0.

394
14
Quantum Chemistry: Electronic Structure
14.7.2
Unitary Transformation
Let’s use the unitary matrix ¯U to transform the original orbitals to the new orbitals
u(i) ≡
L

j=1
¯Ui, jv( j).
What is the overlap matrix of these new orbitals? Let’s look at its (i, n)th element.
Since U is independent of r, this element takes the form
  
¯u(i)(r)u(n)(r)dxdydz =
  
L

j=1
Ui, j ¯v( j)(r)
L

k=1
¯Un,kv(k)(r)dxdydz
=
L

j=1
L

k=1
Ui, j
  
¯v( j)(r)v(k)(r)dxdydz

¯Un,k
=
L

j=1
L

k=1
Ui, j O j,k ¯U t
k,n
=

U O ¯U t
i,n
= Di,n.
Since D is diagonal, the new orbitals are orthogonal to each other: they have zero
overlap with each other. Let’s go ahead and use this property.
14.7.3
Slater Determinant and Its Overlap
What is the Slater determinant? Well, like every determinant, it is the sum of L!
different products, using L! different permutations of 1, 2, . . . , L. This determinant
is insensitive to the above unitary transformation. Indeed, once written in terms of
the new orbitals, it just picks a complex factor of absolute value 1: det(U). This has
no effect on its overlap with itself:
1
L!
  
· · ·
   det

v(n)(ri)

1≤i,n≤L

2
dx1dy1dz1 · · · dxLdyLdzL
= 1
L!
  
· · ·
   det

v(n)(ri)

1≤i,n≤L
¯Ut

2
dx1dy1dz1 · · · dxLdyLdzL
= 1
L!
  
· · ·
   det

u(n)(ri)

1≤i,n≤L

2
dx1dy1dz1 · · · dxLdyLdzL
= D1,1D2,2 · · · DL,L.

14.7 Orbitals and Their Canonical From
395
Why is the latter formula correct? Well, what do we have in the integrand? Just the
complex conjugate of the determinant, times the determinant itself. In both, the same
permutation must be picked, or there would be no contribution at all (thanks to the
orthogonality of the transformed orbitals). Once the same permutation is picked, it
has no effect: it just interchanges the dummy variables integrated upon. Thanks to
the normalization factor 1/L!, the above formula indeed holds.
14.7.4
The Canonical From
So, it is convenient to work with the transformed orbitals, which have a diagonal
overlap matrix. In other words, they are orthogonal to each other. If they are also
linearly independent of each other, then they can also be normalized:
u(i) ←D−1/2
i,i
u(i).
This is indeed their canonical form. In it, they are also orthonormal. In other words,
their overlap matrix is just the identity matrix.
We now have a simple algorithm to normalize the original Slater determinant.
First, calculate the original overlap matrix. Then, diagonalize it. Then, use the unitary
matrix to transform the orbitals. Finally, normalize the transformed orbitals, to obtain
theircanonicalform.Intermsoftheseﬁnalorbitals,thenewSlaterdeterminantindeed
has overlap 1 with itself, as required. This is why the canonical form is so useful.
Later on, we’ll make sure to have the canonical form automatically for free, with
no need to use this algorithm. Therefore, we can assume that the orbitals are already
in their canonical form. This will help simplify the expected energy considerably.
Later on, we’ll see that this assumption is indeed plausible.
14.8
Expected Energy
14.8.1
Coulomb and Exchange Integrals
Yet another Slater determinant can also be deﬁned for the M −L latter orbitals of
the remaining spin-down electrons. In summary, our up-to-date wave function takes
the form of a product of two Slater determinants:
w(r1,r2, . . . ,rM)
≡
1
√
L!
det

v(n)(ri)

1≤i,n≤L

1
√(M −L)! det

v(n)(ri)

L<i,n≤M

.

396
14
Quantum Chemistry: Electronic Structure
With this new wave function, what is the expected potential energy? Fortunately, it
is much simpler than the general form in Sect.14.2.2. In fact, it is nearly as simple as
in Sect.14.3.2: it contains just one more double sum of new integrals—the exchange
integrals.
To see this, as discussed above, assume that v(1), v(2), . . . , v(L) are already
in their canonical form: orthonormal in terms of overlap. Likewise, assume that
v(L+1), v(L+2), . . . , v(M) are orthonormal as well. Later on, we’ll make sure that this
is indeed the case.
In the expected energy, we often integrate on the probability function |w|2. It is
sometimes more convenient to write it as
|w|2 = ¯w · w.
This way, the potential due to attraction to the nucleus simpliﬁes to read
−
  
· · ·
  
M

n=1
|w|2
∥rn∥dx1dy1dz1dx2dy2dz2 · · · dxMdyMdzM
= −
  
· · ·
  
¯w
M

n=1
1
∥rn∥wdx1dy1dz1dx2dy2dz2 · · · dxMdyMdzM
= −
M

n=1
  
¯v(n) 1
∥r∥v(n)dxdydz.
Why is this correct? Well, let’s focus on the former Slater determinant in w. Like
every determinant, it is just the sum of products, each uses a different permutation
of 1, 2, . . . , L. Now, to have a nonzero integral, one must pick the same permutation
in ¯w as in w. (Otherwise, thanks to orthogonality, there would be no contribution at
all.) Once the same permutation is picked, it has no effect: it just interchanges the
dummy variables integrated upon. Thanks to the normalization factor 1/
√
L!, the
above formula indeed holds.
In the Coulomb integrals, on the other hand, things are not so simple any more. To
have a nonzero contribution, one has two options. In the main option, pick the same
permutation in ¯w as in w. This will produce new Coulomb integrals of the form
M

i=1
M

n=i+1
     
|v(i)(r)|2
1
∥r −˜r∥|v(n)(˜r)|2dxdydzd ˜xd ˜yd˜z.
Still, this is not the only option: in ¯w, one could also pick a slightly different per-
mutation, in which i and n switch on top (1 ≤i, n ≤L, or L < i, n ≤M). For
example, if p is picked in w, then pick
(n →i)p

14.8 Expected Energy
397
in ¯w. If p is even (odd), then (n →i)p is odd (even):
e ((n →i)p) = e ((n →i)) e(p) = −e(p).
This will produce the so-called exchange integrals, with a minus sign:
−
M

i=1

n>i same spin
     
¯v(i)(r)v(i)(˜r)
1
∥r −˜r∥¯v(n)(˜r)v(n)(r)dxdydzd ˜xd ˜yd˜z.
Let’s see what this means for each individual orbital.
14.8.2
Effective Potential Energy
What does this mean for an individual electron? In other words, what is the effective
potential that the nth electron feels? Well, it feels attraction to the nucleus:
−
  
¯v(n) 1
∥r∥v(n)dxdydz.
On top of this, it also feels repulsion from all other electrons:
+
M

i=1
     
|v(i)(r)|2
1
∥r −˜r∥|v(n)(˜r)|2dxdydzd ˜xd ˜yd˜z.
Here, one may ask: does it feel any repulsion from itself? No, it doesn’t. Still, there
is one ﬁctitious term in this sum: the term for which i = n. Don’t worry: it will drop
soon.
On top of this, it also feels the exchange force from all other electrons of the same
spin:
−

i, same spin as n
     
¯v(i)(r)v(i)(˜r)
1
∥r −˜r∥¯v(n)(˜r)v(n)(r)dxdydzd ˜xd ˜yd˜z.
Here, one may ask: does it feel any exchange force from itself? No, it doesn’t. There
is one ﬁctitious term in the above sum: the term for which i = n. Fortunately, it
cancels the ﬁctitious term introduced above.
14.8.3
Kinetic Energy
On top of this, it also has its own kinetic energy:

398
14
Quantum Chemistry: Electronic Structure
1
2
  
∇t ¯v(n) · ∇v(n)dxdydz.
Together, all these terms must sum to its expected energy:
E
  
|v(n)|2dxdydz,
where E is a constant energy level: an eigenvalue of the Hamiltonian. This is indeed
quantum mechanics: energy comes in discrete quantities. Only these energy levels
are allowed.
14.9
The Hartree–Fock System
14.9.1
Basis Functions—The Coefﬁcient Matrix
So far, our orbital has been a function in the three-dimensional Cartesian space. This
is too general. To help uncover the orbital, we must approximate it by piecewise-
polynomial functions. More precisely, as in Chap.13, let’s write it as a linear com-
bination of basis functions:
v(n) .=
K

j=1
c jψ j,
where the ψ j’s are the basis functions in the mesh, and the c j’s are their (unknown)
complex coefﬁcients. Let’s plug this in the effective energy in Sect.14.8.2, term by
term. For this purpose, in each term, replace ¯v(n) by ψl, and v(n) by ψ j. This will
assemble the (l, j)th element in the coefﬁcient matrix A:
al, j
≡−
  
ψl
1
∥r∥ψ jdxdydz
+
M

i=1
     
|v(i)(r)|2
1
∥r −˜r∥ψl(˜r)ψ j(˜r)dxdydzd ˜xd ˜yd˜z
−

i, same spin as n
     
¯v(i)(r)v(i)(˜r)
1
∥r −˜r∥ψl(˜r)ψ j(r)dxdydzd ˜xd ˜yd˜z
+ 1
2
  
∇tψl · ∇ψ jdxdydz
(1 ≤l, j ≤K). This way, A is no longer a constant matrix. On the contrary:
it depends on the orbitals— the unknown v(i)’s in the above sums. Still, for ﬁxed
orbitals, A is Hermitian, as required.

14.9 The Hartree–Fock System
399
14.9.2
The Mass Matrix
Now, deﬁne also the mass matrix B. Its (l, j)th element is just the overlap of ψl with
ψ j:
bl, j ≡
  
ψlψ jdxdydz
(1 ≤l, j ≤K).
How to solve for the unknown c j’s? For this purpose, place them in a new
K-dimensional vector:
c ≡(c1, c2, c3, . . . , cK)t .
This way, we can now plug our discrete approximation in. The effective energy in
Sects.14.8.2–14.8.3 takes now the discrete form
ch Ac = Ech Bc,
where E is the (unknown) energy level. Thus, this is a nonlinear equation, with two
types of unknowns: the vector c, and the scalar E.
14.9.3
Pseudo-eigenvalue Problem
How to make sure that the orbitals are indeed in their canonical form? In other words,
how to make sure that same-spin orbitals are indeed orthonormal in terms of overlap?
Fortunately, same-spin orbitals solve the same (generalized) eigenvalue problem,
with the same (Hermitian) coefﬁcient matrix A. Their (generalized) eigenvalues are
different from each other: their distinct energy levels. Thanks to the exercises at the
end of Chap.1, same-spin orbitals are indeed orthogonal to each other: they have zero
overlap with each other. Once normalized properly, they are indeed in their canonical
form, as assumed all along.
The mass matrix B is the same for all orbitals. The coefﬁcient matrix A, on
the other hand, is not: it depends on the orbitals, and therefore has two forms. For
1 ≤n ≤L (orbital of a spin-up electron), A comes in one form. For L < n ≤M
(orbital of a spin-down electron), on the other hand, A has a different form. Still,
only same-spin orbitals should be orthogonal to each other. Fortunately, they share
the same A, and solve the same pseudo-eigenvalue problem.
What should the energy level E be? Well, it should be minimal. For this purpose,
we need to solve a pseudo-eigenvalue problem:
Ac = E Bc.

400
14
Quantum Chemistry: Electronic Structure
The term “pseudo” reminds us that this is actually a nonlinear system: A depends on
the unknown orbitals. Still, for ﬁxed orbitals (say the solutions), A is Hermitian, as
required.
Furthermore, on the right-hand side, we have yet another symmetric matrix: B.
Thus, this is a generalized eigenvalue problem. Thanks to the exercises at the end
of Chap.1, its (generalized) eigenvectors indeed produce orthogonal orbitals of zero
overlap with each other. Once normalized properly, they are indeed in their canonical
form, as required.
14.10
Exercises
14.10.1
Permutation—Product of Switches
1. For some 1 ≤i < k ≤n, consider a switch of the form (i →k). What does it
do? Hint: at the same time, i maps to k, and k maps back to i.
2. Show that it is symmetric:
(i →k) = (k →i).
What is its inverse? Hint: itself.
3. Consider a cycle of the form
[k →i] ≡(k →k −1 →k −2 →· · · →i + 1 →i).
What does it do? Hint: at the same time, k maps to k −1, k −1 maps to k −2,
. . ., i + 1 maps to i, and i maps back to k.
4. Is it symmetric as well? Hint: only if k = i + 1.
5. Write it as a product (composition) of switches:
[k →i] = (k →k −1)(k −1 →k −2) · · · (i + 1 →i).
6. What is its inverse? Hint:
[k →i]−1 = (i →i + 1)(i + 1 →i + 2) · · · (k −1 →k) = [i →k].
7. Write the original switch (i →k) as a composition of two such cycles. Hint:
(i →k) = [i →k −1] ◦[k →i].
8. Conclude once again that the original switch is odd. Hint:
e ([i →k −1] ◦[k →i]) = e ([i →k −1]) e ([k →i]) = (−1)k−1−i+k−i.

14.10 Exercises
401
9. Consider now a more general 3-cycle:
(l →k →i).
What does it do? Hint: at the same time, l maps to k, k maps to i, and i maps
back to l.
10. Write it as a product of two switches. Hint
(l →k →i) = (l →k)(k →i).
11. Conclude that it is even.
12. Consider now a general permutation
p ∈P.
Write it as a product of general cycles. Hint: start from 1. It must map to some
number, which must map to some other number, and so on, until returning back to
1. This completes one general cycle. The rest is a disjoint (smaller) permutation,
which can beneﬁt from an induction hypothesis.
13. Conclude that every permutation can be written as a product of general cycles,
each written as a product of switches, each written as a product of two more
elementary cycles, each written as a product of most elementary switches, as
above.
14. What can you say about the determinant of the transpose?
15. What can you say about the determinant of the Hermitian adjoint?
16. What can you say about the determinant of an orthogonal matrix?
17. What can you say about the determinant of a unitary matrix?
14.10.2
How to Have the Canonical Form?
1. Consider the unitary transformation in Sect.14.7.2. How does it affect the Slater
determinant? Hint: it multiplies it by det(U h).
2. Does this affect the absolute value of the Slater determinant? Hint: from
Sect.14.6.4,
| det(U)| = | det

U h
| = 1.
3. Consider the original overlap matrix O (Sect.14.7.1). Is it Hermitian?
4. Is it positive semideﬁnite?
5. Is it positive deﬁnite? Hint: only if the original orbitals are linearly independent
of each other: they have no linear combination that vanishes (almost) every-
where.
6. Show that
det(O) = det(D) = D1,1D2,2 · · · Dn,n.

402
14
Quantum Chemistry: Electronic Structure
Hint: O and D share the same characteristic polynomial and determinant.
7. Is this positive? Hint: only if the original orbitals are linearly independent of
each other: they have no linear combination that vanishes (almost) everywhere.
8. How to normalize the original Slater determinant without calculating the unitary
transformation explicitly? Hint: just divide by √det(O).
9. Conclude that there is no need to transform explicitly: the original Slater deter-
minant could have been normalized by the square root of the determinant of the
original overlap matrix.
10. Describe the algorithm to have the canonical form.
11. Is it necessary?
12. Why is the canonical form good? Hint: it helps simplify the integrals in the
expected (potential) energy.
13. Show that same-spin orbitals are eigenfunctions of the same (generalized) eigen-
value problem, with the same (Hermitian) coefﬁcient matrix, and the same mass
matrix on the right-hand side. This way, if they have different (generalized)
eigenvalues (energy levels), then they are indeed orthogonal to each other: have
zero overlap with each other. Once normalized properly, they are indeed in their
canonical form, as assumed all along. Hint: see exercises below.
14. Show that the coefﬁcient matrix A in Sect.14.9.1 is indeed Hermitian, provided
that the orbitals in the integrand are ﬁxed.
15. Show that the mass matrix in Sect.14.9.2 is indeed symmetric.
16. Look at the pseudo-eigenvalue problem in Sect.14.9.3. In what sense is it
“pseudo?”
17. In what sense is it “generalized?”
18. Look at two (generalized) eigenvectors c of different (generalized) eigenvalues
E (energy levels). In what sense are they orthogonal to each other? Hint: see
exercises at the end of Chap.1.
19. Conclude that the orbitals formed from them have zero overlap with each other.
20. Concludethat,oncenormalizedproperly,theseorbitalsareindeedintheircanon-
ical form.
21. Conclude that, in retrospect, it was indeed plausible to simplify the Coulomb
and exchange integrals in Sect.14.8.1.

Chapter 15
General Relativity: Einstein Equations
Here is an interesting application in general relativity: Einstein equations. To intro-
duce them, we must use new features in linear algebra: tensors, and algebraic opera-
tions between them. For this purpose, we must introduce a new principle: Einstein’s
summation convention. It improves on the standard sums used in linear algebra. In
fact, it tells us how to raise and lower indices, and come up with a coherent summa-
tion strategy. Thanks to it, the nonlinear system of equations gets particularly easy
to introduce.
Indeed, thanks to tensors, we can model the curvature in spacetime. The key to
the curvature is the metric that tells us how spacetime stretches, and in what direction
[2, 6, 12, 17, 64] .
Unfortunately, the metric is not yet known. Fortunately, Einstein equations tell us
an important thing about it: it can never come from nothing. On the contrary: it must
be produced from its physical source: the stress (energy momentum) tensor. This
gives us a new system of equations for the unknown metric. To solve it numerically,
expand the metric in terms of basis functions in space, and march in time.
15.1
General Relativity—Some Background
15.1.1
Flat Versus Curved Geometry
In special relativity, we assumed no gravity at all. This is why a particle often ﬂies
undisturbed at a constant speed, and never accelerates or changes direction. Only at
the end of Chap.4 did we see a force that acts on the particle. Still, even there, we
only considered the initial time t = 0, before the force had any time to act.
In real systems such as the solar system, on the other hand, gravity can no longer
be ignored. On the contrary: it must be explained by a new mathematical model,
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7_15
403

404
15
General Relativity: Einstein Equations
independent of the coordinates that happen to be used. This is done in a new ﬁeld:
general relativity. Here is some historical background.
The ancient Greeks introduced Euclidean geometry for one main purpose: to
model static shapes in the two-dimensional plane—triangles, circles, and so on.
Later on, this theory was also extended to the three-dimensional space. This was
quite useful to calculate volume, surface area, and more.
Newton, on the other hand, introduced a new time axis on top, to help model not
only static but also dynamic shapes. This was indeed a breakthrough: a new force
can now be applied to the original shape from the outside, to accelerate its original
velocity, and even change its direction.
This ﬁts well in Plato’s philosophy. To refer to a geometrical shape (or just any
general object), we must introduce a new word in our language, to represent not only
one concrete instance but also the “godly” spirit behind all possible instances. This
way, the word stands behind the concept it describes. Likewise, in physics, force
stands behind the motion, and affects it from the outside.
Newton viewed time as an external parameter, which makes a new (nonphysical)
axis, orthogonal to the (physical) phase space, where the original motion takes place.
Einstein, on the other hand, threw the time dimension back into the very heart of
geometry. This way, time is not different from any other spatial dimension. Once
the time axis is united with the original three-dimensional space, we have a new
four-dimensional manifold: spacetime.
This is more in the spirit of Aristotle’s philosophy. A word in our language takes
its meaning not from the outside but from the very inside: the deep nature of the
general object it stands for.
In particular, energy and momentum are now not only physical but also geomet-
rical: sources of mass, gravity, curvature, and symmetry. They determine the true
metric in spacetime, telling us what a straight line really is: the shortest path between
two events in spacetime.
Still, because spacetime is a curved manifold, such a line may no longer look
straight in the usual sense. After all, the time axis is no longer straight: the time scale
may change from place to place. Near a massive star, for example, time may get
slower.
15.1.2
Gravitational Time Dilation
This is called gravitational time dilation. The light coming from a massive star has
a constant speed: c. Still, near the star, less seconds have passed than here, so the
light made a shorter distance than here. This may also be called gravitational length
contraction: to us, distances near the star measure shorter than they’d measure if
they were here. After all, less seconds have passed there than here. (Compare with
Chap.4, Sects.4.3.4–4.3.6.)

15.1 General Relativity—Some Background
405
15.1.3
Gravitational Redshift
During this time, light not only travels, but also oscillates like a wave, at a constant
frequency. Still, less seconds have passed there than here, so the light wave had
less time to oscillate, and made fewer cycles than here. This is called gravitational
redshift: to us, the light coming from the star seems redder—less frequent. Thus, in
our eyes, the star may look redder than it really is. A very massive star may even
look so red that it could hardly be seen at all.
What happens to a light ray that passes by a massive star? Well, as it gets closer
and closer to the star, its time gets slower, so it makes a shorter distance than before.
So, it must curve a little, and make a C-shape around the star. Around a star as massive
and dense as a black hole, it can even spiral, and eventually “fall” right into the black
hole, so we can never see it anymore!
15.1.4
“Straight” Line
In terms of our curved geometry in spacetime, this is still considered as a “straight”
line. Indeed, in spacetime, to be straight means to follow a “valley” where time is
as slow as possible. In such a valley, the light ray mostly remains in the slow-time
region, saving time, and becoming “short” in spacetime. In terms of its own private
(proper) time, this is indeed the fastest way.
From our perspective, such a light ray may seem curved. From its own perspective,
on the other hand, the light ray is as straight as ever. After all, its own proper time,
measured in its own clock, remains as fast as before. Only our clock measures
different time scales: slower near the star than here.
A black hole is a special kind of star—so massive and dense that even light can’t
escape from its gravitational force. Still, a light ray that approaches it would “feel”
nothing unusual. Why? Because its own proper time still ticks at the same rate as
before.
After all, the light ray remains in a free fall, feeling no new force at all. Although
gravity acts upon it quite strongly, this affects it only in the eyes of an outer observer:
he/she will indeed observe that the light ray accelerates and gains more and more
momentum and kinetic energy on its way to the black hole. From its own self-system,
on the other hand, the light ray feels much calmer: there is no force or acceleration
or any change to its momentum or kinetic energy or time rate.

406
15
General Relativity: Einstein Equations
15.2
Metric in Spacetime
15.2.1
Spacetime
Where does the motion take place? In classical mechanics, it takes place in space. In
general relativity, on the other hand, it takes place in spacetime: together with the time
dimension, this is a four-dimensional manifold. Each individual point in spacetime
is called an event: a four-dimensional vector, specifying not only the spatial location
but also the time.
What coordinates to use in spacetime? For this purpose, we need four coordinates.
In our lab, we already have our own coordinates: t, x, y, and z. For the sake of
uniformity, these are often denoted by x0 ≡t, x1 ≡x, x2 ≡y, and x3 ≡z. These
are often indexed by a small Greek letter, say α = 0, 1, 2, 3.
15.2.2
The Unknown Metric
In each individual event in spacetime, we have a metric g, telling us how spacetime
“stretches” at this event. In fact, g is a 4 × 4 matrix, depending on the event under
consideration. Its eigenvectors tell us in what directions spacetime stretches at each
individual event (Chap.5, Sect.5.8.10).
The metric g is not yet available. In fact, each entry (element) is an unknown func-
tion in spacetime. In total, there are as many as 16 unknown entries. Fortunately, this
number can be reduced: since g is symmetric, there are actually just ten independent
entries: say, those in the upper triangular part. This way, we only seek those entries
gm,n indexed by 0 ≤m ≤n ≤3.
15.2.3
Minkowski Metric and Riemann Normal Coordinates
What do we know in advance about the unknown metric g? Well, Einstein’s equiva-
lence principle says that, at each individual event in spacetime, g could be approxi-
mated locally by a constant (ﬂat) metric: the hyperbolic Minkowski metric. Although
this local approximation remains implicit and theoretical, it still tells us how smooth
g really is.
What does this mean geometrically? Well, it means that, at each particular event
in spacetime, g deﬁnes a smooth three-dimensional manifold that not only contains
the event but also has a tangent hyperboloid at the event (Chap.6, Sects.6.6.1–6.7.2).
The Minkowski metric has a simple form: for some unknown coordinates (not
necessarily our lab coordinates), it can be written as hyperbolic (Fig.4.5). In this
sense, the unknown metric g is locally hyperbolic: at each event in spacetime, there
are some unknown coordinates (known as Riemann’s normal coordinates) for which

15.2 Metric in Spacetime
407
the metric is (nearly) hyperbolic. Unfortunately, Riemann’s normal coordinates have
only a theoretical value: they remain local and implicit, and can never be “tied” to
form a useful global coordinate system.
15.2.4
Gravity Waves
There is, though, one exception. To detect gravity waves, we do assume that the
true metric g is close enough to the Minkowski metric. This way, the Riemann
normal coordinates may indeed take a more practical and global form. For this
purpose, however, we must transform our original lab coordinates to more convenient
coordinates. This is known as gauge transformation. Fortunately, there is no need to
do this explicitly. Instead, it is sufﬁcient to assume that the gauge conditions hold.
This way, the original Einstein equations split into ten decoupled wave equations for
the 10 unknown functions gm,n (0 ≤m ≤n ≤3).
Why not do this here? Because here we seek the true metric g, which (globally)
may be completely different from the Minkowski metric.
15.3
Symbols
15.3.1
The Gradient Symbol
Einstein equations contain all the information about the (yet unknown) metric g.
Once they are solved, g is uncovered, as required.
To introduce them, let’s differentiate the entries in g with respect to our four
coordinates. In fact, each entry has four partial derivatives. This makes the “gradient”
of g, which is still unknown as well. Let’s place it in a new 4×4×4 symbol, indexed
by three (lower) indices—m, n, α = 0, 1, 2, 3:
Υmn,α ≡∂gmn
∂xα
(0 ≤m, n, α ≤3).
Here, mn actually means “m, n”. We often drop the comma. Only in the gradient do
we keep the comma: “, α” means a partial derivative with respect to xα (0 ≤α ≤3).
For example, for α = 0, this is a partial derivative with respect to t = x0.
A symbol may be viewed as an extention of the concept of matrix: it may use more
than two indices. In our context, it also depends on the event under consideration: it
may change from event to event. Unlike the tensor introduced later, a symbol is not
necessarily invariant under changing the coordinate system.

408
15
General Relativity: Einstein Equations
The convention is to use capital Greek letters to denote symbols like this. Let’s
deﬁne yet another 4 × 4 × 4 symbol:
Θαmn ≡1
2

Υαm,n + Υαn,m −Υmn,α

.
This will be useful later.
15.4
Einstein Summation Convention
15.4.1
Lower and Upper Indices
In linear algebra, we use lower indices only. This is good enough to denote vector
components and matrix elements, and sum over any (lower) index in them. Here, on
the other hand, we distinguish between lower and upper indices. This will help sum
(or contract): an upper index could contract with a lower index only.
15.4.2
The Inverse Metric
The inverse metric is often denoted by two upper indices:
gμν ≡

g−1
μν .
Here, we use a small Greek letter as an index: 0 ≤μ, ν ≤3. Also, we often use
Einstein summation convention:
an index that appears twice, once as an upper index and another time as a lower
index, is summed over (contracted).
This way, no ‘’ sign is needed anymore.
For example, in the identity matrix I, denote the individual elements by
δμ
κ ≡
 1
if
μ = κ
0
if μ ̸= κ.
Thanks to Einstein summation convention, the sum of the main-diagonal elements
has a new short form, with no “”:
trace(I) = δμ
μ ≡
3

μ=0
δμ
μ =
3

μ=0
1 = 1 + 1 + 1 + 1 = 4.

15.4 Einstein Summation Convention
409
This way, the “” sign is avoided and dropped, with the same meaning. We then
say that the upper and lower indices have been contracted with each other. We can
now combine the above conventions, and multiply g by its own inverse from the left:
gμνgνκ ≡
3

ν=0
gμνgνκ ≡
3

ν=0

g−1
μν gνκ = δμ
κ.
This way, μ remains an upper index, κ remains a lower index, and ν is gone, as
required. In other words, the second (upper) index in gμν has been contracted with
the ﬁrst (lower) index in gνκ.
An upper index can be contracted only with a lower index, but not with an upper
index. This is why the above contraction is indeed legitimate.
15.5
The Riemann Tensor
15.5.1
A New Convention
What does the above formula say? In terms of linear algebra, it simply says

g−1g

μκ = Iμκ.
Still, this naive style uses lower indices only. Our new style, on the other hand, is
safer: it distinguishes between lower and upper indices, and protects from a wrong
contraction.
Still, in the naive style, let’s introduce a new convention: drop the parentheses!
This way,
g−1gμκ ≡gμνgνκ.
This way, applying g−1 from the left means contracting with ν, which is the ﬁrst
lower index in g. Likewise, g−1 could be applied not only to g but also to any other
matrix, and even to a bigger symbol like Θ.
15.5.2
The Christoffel Symbol
To get used to this new convention, let’s apply g−1 not only to g but also to Θ. For
this purpose, contract with the ﬁrst lower index in Θ. This deﬁnes a new symbol:
Γ α
mn ≡g−1Θαmn ≡gαβΘβmn.

410
15
General Relativity: Einstein Equations
This raises α from a lower index in Θ to an upper index in Γ . Indeed, the dummy
index β is contracted upon and gone.
This is the Christoffel symbol. It is the key to the curvature of spacetime, stored
in the Riemann tensor.
15.5.3
The Riemann Tensor
Let’s deﬁne a yet bigger 4 × 4 × 4 × 4 symbol, with four indices—an upper index,
followed by three lower indices:
Ξ ρ
μσν ≡
∂Γ ρ
νμ
∂xσ + Γ ρ
σλΓ λ
νμ.
Here, there are two terms. The ﬁrst term indeed uses four indices, as required. In
the second term, the dummy index λ is contracted upon and gone, leaving just four
indices, as required.
To deﬁne the Riemann tensor, just antisymmetrize Ξ in terms of σ and ν: subtract
the same, but with σ and ν interchanged:
Rρ
μσν ≡Ξ ρ
μσν −Ξ ρ
μνσ.
This is the key for the curvature in spacetime.
15.6
Einstein Equations in Vaccum
15.6.1
Vacuum and Curvature
The Riemann tensor tells us the curvature: how curved spacetime is at each individual
event in it. This curvature is shaped by the original mass distribution. It actually
deﬁnes the true (nonﬂat) geometry in spacetime, and the true metric in it. This
produces the desired system of equations, whose solution is the (still unknown)
metric.
For a start, let’s model gravity in vacuum: an empty domain, with no matter or
energy or any other source of curvature. In this domain, the curvature should vanish.
Of course, outside the domain, things may be different: there may be massive
stars (like black holes) that may curve spacetime globally, both outside the domain
and inside it. Still, they may affect our domain only indirectly: through boundary
conditions. In the interior of the domain, on the other hand, there should be no
curvature at all.

15.6 Einstein Equations in Vaccum
411
Does this mean that, in our domain, the Riemann tensor must vanish? No! If it
were, then the metric would be constant, and spacetime would be completely ﬂat:
every free fall would follow a straight line, as in Minkowski space.
This may be good enough in special relativity, where no gravity is assumed at
all. In general relativity, on the other hand, gravity curves spacetime. As a result,
a free fall may follow not a straight but a curved geodesic: the shortest path in the
underlying (nonﬂat) geometry.
For instance, this is indeed why the Earth orbits the Sun: in terms of the curved
geometry in the solar system, it “falls” not in a straight line but in an elliptic orbit.
Although this is not straight in the usual sense, it is perfectly straight in the curved
spacetime.
In this example, what is our domain? Well, it doesn’t contain the Sun itself, but
only the space outside it, which is assumed to be completely empty. This is indeed a
perfect vacuum: the Earth itself doesn’t count, because this is the object in motion,
upon which gravity acts.
So, to model gravity in general, the Riemann tensor mustn’t vanish. This would
be too pedant—it would throw us back to special relativity, losing all the interesting
effects of gravity. Surely, we wouldn’t like this to happen. To avoid this, something
else should vanish: not the original Riemann tensor, but a smaller tensor—the Ricci
tensor.
15.6.2
The Ricci Tensor
What is the Ricci tensor? It is just the “trace” of the Riemann tensor, obtained by
contracting the upper index with the second lower index:
Rμν ≡Rρ
μρν.
In the original Riemann tensor, the upper index is followed by three lower indices.
The above deﬁnition actually contracts the ﬁrst and third indices in the original
Riemann tensor. This is indeed legitimate: the ﬁrst index is upper, and the third index
is lower. This way, these indices are now gone: the resulting Ricci tensor takes just
two indices.
15.6.3
Einstein Equations in Vacuum
In vacuum, the Ricci tensor must vanish:
Rμν = 0.

412
15
General Relativity: Einstein Equations
Here, we better keep the indices μ and ν, and not drop them. This may help indicate
that this is the Ricci tensor Rμν, not the original Riemann tensor, which takes four
indices.
Because Rμν depends on g, this indeed makes a system of equations in the
unknown metric g. These are Einstein equations in vacuum. Once they are solved, g
is indeed uncovered, as required.
How to solve them numerically? Well, on the left-hand side, the tensor depends
on g, which is not yet known. To solve for it numerically, expand it in terms of basis
functions:
gm,n ≡
K

j=1
c(m,n)
j
ψ j,
0 ≤m, n ≤3.
(This is a standard linear algebra sum: it uses no summation convention.) Once this
is plugged in, multiply each equation by ψl, and integrate in space. This gives 10K
equations in 10K new unknowns: the c(m,n)
j
’s. Still, they may change in time. To solve
for them, we must therefore march in time, time level by time level. For this purpose,
the t-partial derivative should be discretized by an (implicit) ﬁnite difference: the
current time level, minus the previous one, divided by the time step △t.
So far, we’ve considered Einstein equations in vacuum. This is good enough in
the study of black holes and the solar system. After all, nobody is interested in the
interior of a star, but only in the empty space outside it.
But this is no longer good enough in cosmology. In this ﬁeld, we can no longer
assumethatourdomainisempty.Onthecontrary:weoftenuseaverylargescale—we
average on the stars and galaxies in the entire universe, and consider them as a homo-
geneous dust, with a uniform density, and no pressure at all. Thus, we can’t assume
vacuum anymore: the universe is full of matter and energy. To model this, Einstein
equations must also take a nonzero right-hand side: the stress (energy momentum)
tensor.
15.7
Einstein Equations—General Form
15.7.1
The Stress (Energy Momentum) Tensor
Cosmology is one example in which the vacuum can no longer be assumed. On the
contrary: the right-hand side s now a nonzero tensor: the stress (energy momentum)
tensor Tμν, which often takes the simple form
Tμν ≡Vμν + pgμν
(0 ≤μ, ν ≤3),
where the scalar p and the tensor V may depend on g, g−1, and the ﬁeld itself in a
simple way. For simplicity, we often drop the indices μ and ν:

15.7 Einstein Equations—General Form
413
T ≡pg + V.
This simple form appears in a few important models:
1. In a scalar ﬁeld, p is the Lagrangian: the kinetic energy minus the potential
energy (as in a harmonic oscillator). This way, p depends linearly on g−1.
2. In electromagnetics, on the other hand,
p = 1
4trace

g−1Fg−1F

and V = −Fg−1F,
where F is the antisymmetric tensor that stores the electric ﬁeld (E1, E2, E3)
and the magnetic ﬁeld (B1, B2, B3):
F ≡
⎛
⎜⎜⎝
0
E1
E2
E3
−E1
0
−B3
B2
−E2
B3
0
−B1
−E3 −B2
B1
0
⎞
⎟⎟⎠.
This leads to the Einstein–Maxwell equations.
3. Finally, in cosmology, in a very large scale, the stars and galaxies are averaged,
and viewed as a perfect ﬂuid, with pressure p. (In Friedmann’s theory, in particu-
lar, they are considered as dust, with no pressure at all: p = 0.) In this model, we
also assume spatial isotropy: at each individual point in space, every direction
looks the same. This way, V has just one nonzero entry:
V =
⎛
⎜⎜⎝
ρ + p 0 0 0
0
0 0 0
0
0 0 0
0
0 0 0
⎞
⎟⎟⎠,
where ρ is the energy density. (Don’t confuse it with the index ρ used in the
Riemann tensor above!) Moreover, we also assume symmetry and homogeneity:
in every point in space, everything is the same: p ≡p(t) and ρ ≡ρ(t). This
way, the universe must be a highly symmetric three-dimensional manifold:
• Either a “closed” (compact) hypersphere, with a (constant) positive spatial
curvature,
• Or an “open” (noncompact) hyperboloid , with a negative curvature,
• Or a ﬂat (and open) Euclidean space, with no curvature at all. (From observa-
tions, this option is most likely.)
As the universe expands, the matter gets more and more spread out, and the energy
density decreases: dρ/dt < 0. In reality, however, the universe may contain not
only dust but also other kinds of radiation, with a nonzero pressure, proportional
to their own energy density. For example, even if the universe were completely

414
15
General Relativity: Einstein Equations
empty, it would still contain “vacuum” energy, with p = −ρ, dp/dt = dρ/dt = 0,
and V = 0. Because it has a constant ρ and p, this “dark” energy is also called
the cosmological constant. Still, the above models are too ideal: in a more realistic
cosmological model, the universe should better be a mix of dust, dark energy, and
other kinds of radiation together.
15.7.2
The Stress Tensor and Its Trace
Now, let’s take T , and apply g−1 to it from the left:
g−1T = pI + g−1V.
Here, I is the 4 × 4 identity matrix, so trace(I) = 4. Therefore,
trace

g−1T

= 4p + trace

g−1V

.
This scalar can now be multiplied by gmn from the right, with two new indices—m
and n:
trace

g−1T

gmn = 4pgmn + trace

g−1V

gmn.
15.7.3
Ricci Scalar
So far, we’ve considered a few kinds of tensors. The Riemann tensor takes four
indices. The Ricci tensor, on the other hand, takes just two indices. The Ricci scalar,
on the other hand, has no index at all:
R ≡trace

g−1Rμν

.
15.7.4
Einstein Tensor
We are now ready to deﬁne Einstein tensor: take the Ricci tensor, and subtract half
of the Ricci scalar times g:
Gμν ≡Rμν −1
2 Rgμν.
(Don’t confuse Gμν with Newton’s constant G, which has no index at all!)

15.7 Einstein Equations—General Form
415
15.7.5
Einstein Equations—General Form
We are now ready to write Einstein equations in their general form, with a nonzero
tensor on the right-hand side as well. This way, they are relevant not only in vacuum
but also in matter or radiation, as required:
Gμν = 8πGTμν.
(Distinguish between Gμν on the left, a tensor with two indices, and G on the right,
which stands for Newton’s gravity constant, which takes no index at all.)
What do we have in this system? On the right-hand side, the tensor is available.
On the left-hand side, on the other hand, the tensor depends on g, which is not yet
known. How to solve for it numerically? Again, expand it in space as
gm,n ≡
K

j=1
c(m,n)
j
ψ j,
0 ≤m, n ≤3.
(Recall that this is a mere linear algebra sum, with no summation convention.) Now,
plug this in, multiply each equation by ψl, and integrate in space. This way, we have
a discrete (nonlinear) system for the c(m,n)
j
’s. Still, they may depend on time. Thus,
to solve for them, we must march in time. For this purpose, discretize the t-partial
derivative: say, take the current time level, subtract the previous one, and divide by
△t.
15.7.6
The Trace-Subtracted Form
In the ﬁnal Einstein equations, apply g−1 to both sides:
g−1Gμν = g−1

Rμν −1
2 Rgμν

= g−1Rμν −1
2 RIμν = g−1Tμν.
Now, take the trace of both sides:
trace

g−1Rμν

−1
2 R · trace(I) = R −1
2 R · 4 = −R = 8πG · trace

g−1Tμν

.
In summary, we now have the Ricci scalar in a more explicit form
R = −8πG · trace

g−1Tμν

.
Let’s plug this in Einstein equations, and divide by 8πG:

416
15
General Relativity: Einstein Equations
1
8πG Rμν + 1
2trace

g−1T

gμν = Tμν.
This is called the trace-subtracted form.
What do we have in this system? On the right-hand side, we have a known tensor.
On the left-hand side, on the other hand, the tensor depends on the metric g, which
is still unknown. To solve for it, use the same numerical technique as before.
15.8
Exercises
1. Let A be a 4 × 4 matrix:
A ≡

ai, j

0≤i, j≤3 .
Apply g−1 to it from the left. For this purpose, contract with ν, which is the ﬁrst
lower index in A:
g−1aμκ = gμνaνκ.
Hint: the dummy index ν is contracted upon and gone. This is our new convention
in Sect.15.5.1.
2. Is this a legitimate contraction? Hint: ν indeed appears twice—as an upper index
in gμν, and also as a lower index in gνκ, as required.
3. Show that this new style could actually be obtained from the naive linear algebra
style: just introduce parentheses back again:
g−1aμκ =

g−1A

μκ =
3

ν=0

g−1
μν aνκ =
3

ν=0
gμνaνκ = gμνaνκ.
Hint: use Einstein summation convention, and write the inverse of g with two
upper indices (Sect.15.4.2).
4. What is so good about this new style? Hint: thanks to it, g−1 can now be applied
not only to a matrix like g but also to a bigger symbol like Θ, to help deﬁne the
Christoffel symbol:
Γ α
mn ≡g−1Θαmn ≡gαβΘβmn
(Sect.15.5.2).
5. In the latter formula, what happened to the index β? Hint: it has been contracted
upon and gone. This is why it can no longer be found in Γ .
6. Is this a legitimate contraction? Hint: β is indeed an upper index in gαβ, and a
lower index in Θ, as required.
7. Why is α an upper index in Γ ? Hint: because it is an upper index in gαβ, and
should remain an upper index in Γ as well.

15.8 Exercises
417
8. Give yet another example of applying g−1 in the new style. Hint: apply g−1 to
the Ricci tensor, and take the trace:
R ≡trace

g−1Rμν

.
This deﬁnes the Ricci scalar (Sect.15.7.3).
9. What information is stored in the Riemann tensor? Hint: the entire curvature in
spacetime.
10. Does it make sense to require that it vanishes? Hint: this is too pedant—it would
lead to a ﬂat metric, which allows no gravity at all.
11. Does it make more sense to require that the Ricci tensor vanishes? Hint: yes—this
gives Einstein equations in vacuum.
12. Give an example of a domain that has only vacuum in it. Hint: in the solar system,
consider the space outside the Sun.
13. In this domain, doesn’t the Earth itself violate the vacuum assumption? Hint: it
is considered as a mere point. After all, it is the object in motion, upon which
gravity acts.
14. In this domain, how could the Earth feel any gravity? Hint: it feels the Sun’s
gravity through the boundary conditions at the Sun’s surface.
15. Why does the Earth orbit the Sun? Hint: in the curved geometry in the solar
system, the Earth’s orbit is considered as a “straight” line in spacetime. This
way, the Earth actually “falls” freely along its orbit. In the curved geometry in
spacetime, this is considered as a straight line.
16. Look at a massive star in the sky. Does the time on it tick faster or slower than
your own time here? Hint: slower. This is gravitational time dilation.
17. Does the diameter of the star look shorter or longer than if the star were here and
its diameter had been measured here? Hint: shorter. This is gravitational length
contraction.
18. Is the speed of light the same there as here? Hint: yes—the above effects cancel
eachother.Onthestar,duetogravitationaltimedilation,lesssecondshavepassed
than here. Still, due to gravitational length contraction, distances measure shorter
on the star than they’d measure if they were here.
19. What color does the star have? Hint: due to gravitational redshift, it looks redder
than it would look here.

References
1. Adachi, H., Mukoyama, T., Kawai, J.: Hartree-Fock-Slater Method for Materials Science:
the DV-X Alpha Method for Design and Characterization of Materials. Springer, New York
(2006)
2. Alcubierre, M.: Introduction to 3+1 Numerical Relativity. Oxford University Press, New York
(2012)
3. Anthony, M., Harvey, M.: Linear Algebra: Concepts and Methods. Cambridge University
Press, New York (2012)
4. Arnold, D.N., Rogness, J.: Moebius transformations revealed. Not. AMS 55, 1226–1231
(2008)
5. Averbuch, A.Z., Neittaanmki, P., Zheludev, V.A.: Spline and Spline Wavelet Methods with
Applications to Signal and Image Processing. Vol. I: Periodic Splines. Springer, New York
(2014)
6. Baumgarte, T.W., Shapiro, S.L.: Numerical Relativity: Solving Einstein Equations on the
Computer. Cambridge University Press, New York (2010)
7. Brandts, J., Korotov, S., Krizek, M.: On the equivalence of regularity criteria for triangular
and tetrahedral ﬁnite element partitions. Comput. Math. Appl. 55, 2227–2233 (2008)
8. Bransden, B.H., Joachain, C.J.: Quantum Mechanics. Prentice-Hall, USA (2000)
9. Brenner, S.C., Scott, L.R.: The Mathematical Theory of Finite Element Methods. Texts in
Applied Mathematics, vol. 15. Springer, New York (2002)
10. Cahen, G., Joly, P., Roberts, J.E., Tordjman, N.: High order triangular ﬁnite elements with
mass lumping for the wave equation. SIAM J. Numer. Anal. 38, 2047–2078 (2001)
11. Cheney, E.W.: Introduction to Approximation Theory, 2nd edn. Chelsea (1982)
12. Chirvasa, M.: Finite Difference Methods in Numerical Relativity. VDM Verlag, Riga (2010)
13. Cohen, E., Reisenfeld, R.F., Elber, G.: Geometric Modeling with Splines: An Introduction.
A K Peters Ltd., USA (2001)
14. Courant, R., John, F.: Introduction to Calculus and Analysis, vol. 1–2. Springer, New York
(1998–1999)
15. Coxeter, H.S.M.: Projective Geometry. Springer, New York (2003)
16. Dierckx, P.: Curve and Surface Fitting with Splines. Oxford University Press, New York
(1993)
17. d’Inverno, R.: Approaches to Numerical Relativity. Cambridge University Press, New York
(2008)
18. Einstein, A.: Relativity: The Special and the General Theory. Martino Fine Books (2010)
19. Eves, H.: Foundations and Fundamental Concepts of Mathematics. Dover Publications, New
York (1997)
20. Fischer, C.F.: The Hartree-Fock Method for Atoms: a Numerical Approach. Wiley, New York
(1977)
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7
419

420
References
21. Fulton, W., Harris, J.: Representation Theory: A First Course, 3rd edn. Springer, New York
(1999)
22. Gibson,C.C.:ElementaryEuclideanGeometry:AnIntroduction.CambridgeUniversityPress,
New York (2003)
23. Goeke, K., Reinhard, P.G.: Time-Dependent Hartree-Fock and Beyond: Lecture Notes in
Physics: 171. Springer, New York (1983)
24. Golub, G.H., Van Loan, C.F.: Matrix Computations, 4th edn. Johns Hopkins University Press,
USA (2013)
25. Goodman, R., Wallach, N.R.: Symmetry, Representations, and Invariants. Springer, New York
(2009)
26. Grifﬁths, D.J., Schroeter, D.F.: Introduction to Quantum Mechanics, 3rd edn. Cambridge
University Press, New York (2018)
27. Henrici, P.: Applied and Computational Complex Analysis, vol. 1–2. Wiley-Interscience, New
York (1977)
28. Henrici, P., Kenan, W.R.: Applied and Computational Complex Analysis: Power Series-
Integration-Conformal Mapping-Location of Zeros. Piscataway, Wiley-IEEE (1988)
29. Hollig, K.: Finite Element Methods with B-Splines. SIAM, Philadelphia (2003)
30. Holm, D.D.: Geometric Mechanics Part I: Dynamics and Symmetry. World Scientiﬁc, Singa-
pore (2011)
31. Holm, D.D.: Geometric Mechanics Part II: Rotating, Trnaslating and Rolling, 2nd edn. World
Scientiﬁc, Singapore (2011)
32. Holm, D.D., Schmah, T., Stoica, C.: Geometric Mechanics and Symmetry: from Finite to
Inﬁnite Dimensions. Oxford University Press, New York (2009)
33. Hrabovsky, C.: em Classical Mechanics: The Theoretical Minimum (2014)
34. James, G., Liebeck, M.: Representations and Characters of Groups, 2nd edn. Cambridge
University Press, New York (2001)
35. Jones, H.F.: Groups, Representations and Physics, 2nd edn. CRC Press, New York (1998)
36. Karatzas, I., Shreve, S.E.: Brownian Motion and Stochastic Calculus, 2nd edn. Springer, New
York (1991)
37. Kibble, T.: Classical Mechanics, 5th edn. Imperial College Press, London (2004)
38. Klein, F.: Elementary Mathematics from an Advanced Standpoint: Arithmetic, Algebra. Anal-
ysis. Dover Publications, New York (2004)
39. Krizek, M., Strouboulis, T.: How to generate local reﬁnements of unstructured tetrahedral
meshes satisfying a regularity ball condition. Numer. Methods Part. Differ. Equ. 13, 201–214
(1997)
40. Kuratowski, K., Musielak, J.: Introduction to Calculus. Pergamon Press, New York (1961)
41. Lai, M.J., Schumaker, L.L.: Spline Functions on Triangulations. Cambridge University Press,
New York (2007)
42. Lang, S.: Linear Algebra, 3rd edn. Springer, New York (2010)
43. Langer, T., Seidel, H.P.: Higher order barycentric coordinates. Comput. Graph. Forrum 27,
459–466 (2008)
44. Langville, A.N., Meyer, C.D.: Google’s Page Rank and Beyond: The Science of Search Engine
Rankings. Princeton University Press, New York (2006)
45. Layton, W., Lee, H.K., Peterson, J.: Numerical solution of the stationary Navier-Stokes equa-
tions using a multilevel ﬁnite element method. SIAM J. Sci. Comput. 20, 1–12 (1998)
46. Miller, G.L.: Riemann’s hypothesis and tests for primality. J. Comput. Syst. Sci. 13, 300–317
(1976)
47. Mitchell, W.F.: Optimal multilevel iterative methods for adaptive grids. SIAM J. Sci. Stat.
Comput. 13, 146–167 (1992)
48. Narayanan, P.J.: Computer Graphics Model: Projective Geometry. CS 3500. IIIT, India (2009)
49. Newman, J.R.: The World of Mathematics. Courier Dover Publications, New York (2000)
50. Ortega, J.M.: Introduction to Parallel and Vector Solution of Linear Systems. Plenum Press,
New York (1988)
51. Prenter, P.M.: Splines and Variational Methods. Dover Publications, New York (1975)

References
421
52. Rand, A., Gillette, A., Bajaj, C.: Quadratic serendipity ﬁnite elements on polygons using
generalized barycentric coordinates. Math. Comput
53. Robinett, R.: Quantum Mechanics: Classical Results, Modern Systems, and Visualized Ex-
amples, 2nd edn. Oxford University Press, New York (2006)
54. Rogers, D.F., Adams, J.A.: Mathematical Elements for Computer Graphics, 2nd edn.
McGraw-Hill, New York (1989)
55. Rosenlicht, M.: Introduction to Analysis. Dover Publications, New York (1985)
56. Saad, Y., Schultz, M.H.: GMRES: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM J. Sci. Stat. Comput. 7, 856–869 (1986)
57. Schoberl, J., Zaglmayr, S.: High order Nedelec elements with local complete sequence prop-
erties. COMPEL 24, 374–384 (2005)
58. Schumaker, L.L.: Spline Functions: Basic Theory, 3rd edn. Cambridge University Press, New
York (2007)
59. Schwerdtfeger, H.: Geometry of Complex Numbers: Circle Geometry, Moebius Transforma-
tion. Non-Euclidean Geometry. Dover Publications, New York (1980)
60. Shapira, Y.: Set Theory and Discrete Math for Physicists and Engineers. (Submitted)
61. Shapira, Y.: Solving PDEs in C++: Numerical Methods in a Uniﬁed Object-Oriented Ap-
proach, 2nd edn. SIAM, Philadelphia (2012)
62. Shapira, Y.: Algebraic interpretation of continued fractions. J. Comput. Appl. Math. 78, 3–8
(1997)
63. Shapira, Y.: Mathematical Objects in C++: Computational Tools in a Uniﬁed Object-Oriented
Approach. CRC Press, Boca Raton (2009)
64. Shibata, M.: Numerical Relativity. World Scientiﬁc Publishing, Singapore (2016)
65. Sibley, T.Q.: The Foundations of Mathematics. Wiley, New York (2008)
66. Sobczyk, G.: New Foundations in Mathematics: The Geometric Concept of Number.
Birkhauser, Switzerland (2013)
67. Strang, G.: Introduction to Applied Mathematics. Wellesley-Cambridge Press, New York
(1986)
68. Strang, G.: Introduction to Linear Algebra, 3rd edn. SIAM, Philadelphia (2003)
69. Strang, G., Fix, G.: An Analysis of the Finite Element Method. Prentice-Hall, Englewood
Cliffs (1973)
70. Taylor, J.R.: Classical Mechanics. University Science Books, Herndon (2005)
71. Tolba, O., Dorsey, J., McMillan, L.: A Prospective Drawing System. Cs Laboratory, MIT
Cambridge
72. Vaisman, I.: Analytical Geometry. World Scientiﬁc, Singapore (1997)
73. Varga, R.: Matrix Iterative Analysis. Prentice-Hall, Englewood Cliffs (1962)
74. Vince, J.: Mathematics for Computer Graphics, 3rd edn. Springer, New York (2006)
75. Wahba, G.: Spline Models for Observational Data. Capital City Press, Montpelier (1990)
76. Wang, R.: Multivariate Spline Functions and Their Applications. Springer, New York (2001)
77. Wilder, R.L.: Introduction to the Foundations of Mathematics, 2nd edn. Dover Publications,
New York (2012)
78. Woodhouse, N.M.J.: Special Relativity, 2nd edn. Springer, London (2007)
79. Wylie Jr., C.R.: Introduction to Projective Geometry. Dover Publications, New York (2009)
80. Xiao, H., Gimbutas, Z.: A numerical algorithm for the construction of efﬁcient quadrature
rules in two and higher dimensions. Comput. Math. Appl. 59, 663–676 (2010)

Index
A
Absolute
energy, 129
Abstract
object, 145
in Euclidean geometry, 187
Action, 170
Adding
2-d vectors, 4
matrices, 12
polynomials, 268
velocities, 104
Adequacy
in regularity, 352
Adjoint
Hermitian-
eigenvalues of, 30
matrix, 21
self- (see: Hermitian), 22
Afﬁne
in 3-d, 348
Algebra
linear-, 8, 187
Algebraic
continued fraction, 177, 178
convergence, 178
object, 1, 143, 145, 154, 187, 268
Algorithm
recursive-, 274
Amplitude, 35, 235
Analytic
geometry, 187
object, 145, 268
Angle
azimuthal-, 362
Euler-, 79
in spherical coordinates, 362
minimal-, 348
momentum, 67, 256
conservation of, 85
polar-, 362
velocity, 69
Angular
momentum, 67, 256
conservation of, 85
velocity, 69
Anti
Hermitian, 238, 239
Antiderivative, 277
Antipodal
points, 85, 189, 190, 207, 225
vectors, 200, 203
Antisymmetric, 410, 413
Application
3-d, 325
geometrical-, 103, 325
in Moebius transformation, 167
numerical-, 325
Approximant, 177
Approximation, 325
3-d, 369
numerical-, 369
Aristotle, 404
Assemble, 372
Associative law, 1, 143
in composition, 149, 173
in factor group, 164
in functions, 149, 173
in group, 151
in kernel, 159
in matrices, 13, 14, 16, 25
in transformations, 149
in vectors, 7
Atom, 381
© Springer Nature Switzerland AG 2019
Y. Shapira, Linear Algebra and Group Theory for Physicists
and Engineers, https://doi.org/10.1007/978-3-030-17856-7
423

424
Index
Atomic units, 380
Axis
principal-, 81
system, 211
principal-, 81
relative-, 63
rotating-, 69
time-, 106
Azimuthal angle, 362
B
Ball, 364
Ball ratio, 354
Barycentric, 304, 308
Base
binary-, 274
decimal-, 274
Basis function, 299, 373, 398
Bilinear, 56, 58
Binary
base, 274
digit, 274
polynomial, 274, 276
representation, 274
Bisection, 333
Bit, 275
Black hole, 405, 412
Boson, 261
Boundary, 328
concave-, 342
convex, 339
edge, 333, 335, 338
reﬁnement, 334
B-spline, 299
C
Calculus, 278, 281
Cartesian
coordinate, 364
line, 206, 213
plane, 4, 187, 188
space, 6, 58, 190
Cauchy, 40
Center, 168, 174, 194
mass-, 131
subgroup, 158, 168
Centrifugal force, 71
Centripetal force, 72
Chain
Markov-, 95
rule, 302, 305, 307
Characteristic
equation, 90
polynomial, 90, 391
Chemistry, 379
Christoffel, 409
Circle, 189, 201
degenerate-, 188
divided-, 200
dividing by, 196
equivalence-, 196
great-, 207
in analytic geometry, 189
inﬁnity-, 205, 209
radius of-, 189
semi-, 201
unit-, 189
closed, 100
open, 100
Class
equivalence-, 160, 162
Clockwise, 78
counter-, 78
Closed
cube, 343
system, 85, 126
Coding, 275
Coefﬁcient
in continued fraction, 176
in polynomial, 267
matrix, 372
Coherent state, 250
Column
in matrix, 12, 300
number, 15
orthogonal-, 27
orthonormal-, 27
vector, 12, 26, 300
Combination
convex-, 304
linear-, 14, 35, 96, 226, 290
Commutative law, 1, 143
Commutator, 238, 256
Commute
groups, 194
in center, 158
in matrices, 182
in normal subgroup, 163
inverse element, 152
Compact, 413
Complex
conjugate, 10, 47
coordinate, 168, 191, 192, 196
function, 268

Index
425
matrix, 21, 150, 172, 175, 182
number, 9
plane, 193
extended-, 146
vector, 11
Component
of vector, 3
Composite
function, 299, 364
Composition
associative law, 149, 173
of functions, 170, 299
of polynomials, 273
of transformations, 110, 149, 176
Computational
chemistry, 379
Computer
graphics, 218, 221
Concave
boundary, 342
Conformity, 328, 329
Conic, 211, 215, 227
in projective plane, 211, 215
in projective space, 228
invariant-, 212
Conjugate
complex-, 10, 47
matrix, 21
transpose, 21
Conservation
mass-, 93, 126
of angular momentum, 85
of energy, 124
of momentum, 122
Constant
cosmological-, 414
energy, 248
matrix, 184
Planck-, 238
Continued fraction, 176
algebraic-, 177, 178
approximant, 177
coefﬁcient, 176
convergence, 176, 178
periodic-, 185
Continuous
function, 369
gradient, 369
Moebius transformation, 146
Contraction, 115, 137
gravitational-, 404
Contradiction (see: proof), 30, 41
Convergence
algebraic-, 178
of continued fraction, 176, 178
strict-, 176
to inﬁnity, 176
wide-, 176
Convex
boundary, 339
combination, 304
Convolution, 295
Coordinate, 187
barycentric-, 304, 308
Cartesian-, 364
complex-, 168, 191, 192, 196
horizontal-, 4
in gradient, 281
in group, 168
in projective geometry, 204, 216
in vector, 3, 5
lab-, 108
normal-, 407
of point, 330
polar-, 362
principal-, 81
reference-, 359
rotating-, 71
self-, 108
spherical-, 362
system
relative-, 63
vertical-, 4
Coriolis, 76
Corner, 303, 328
in tetrahedron, 299
Cosine
decomposition, 36
derivative, 296
discrete-, 36
mode, 36
Taylor
polynomial, 296
series, 296
transform, 38
wave, 36
Cosmological constant, 414
Cosmology, 412
Friedmann model, 413
manifold, 413
Cotangent
projection, 171
Coulomb, 382, 396
Counterclockwise, 78
Covariance, 241
Cramer formula, 55, 111, 137, 305

426
Index
Cryptography, 275
Cube
unit-, 330
closed-, 343
Curvature, 410
negative-, 413
positive-, 413
spatial-, 413
Curve, 219
Cycle, 385, 400
D
Data
Dirichlet-, 369
Decimal
base, 274
representation, 274
Decoding, 275
Decomposition, 70
cosine-, 36
error in-, 35
geometrical-, 168
group-, 195
into planes, 168
matrix-, 375
multiscale-, 35
of a set, 160
orthogonal-, 68
polar-, 195, 235, 296
remainder in-, 35
sine-, 35
spectral-, 35, 95
Deﬁnite
integral, 278
positive-, 39, 87, 374
semi-, 39, 81
Degenerate, 259
Degree
of freedom, 201, 255, 302, 370
of polynomial, 285, 294
Density, 412
Derivation (see: differentiation), 283
Derivative, 276
directional-, 289
gradient, 281, 286
normal-, 290
high-order, 291
order of-, 291
second-, 291
partial- (see: partial derivative), 280
tangential-, 292
Determinant, 51, 108, 363, 388
parallelogram, 53
Slater-, 384, 394
transpose, 388
Deterministic, 232
non-, 233, 251, 258
Diagonal
form, 35, 38, 49
index, 270
induction, 283
in parallelogram, 5
in polynomial, 270
main-, 18, 22
matrix, 33, 178, 182
off-, 19, 34
Diagonalization, 35, 38, 49
Diameter, 189, 190
Differentiable, 287
Differential
operator, 291
Differentiation
operator, 124
Digit, 274
binary-, 274
Dilation, 113, 117, 134
gravitational-, 404
Dimension, 7
four-, 103, 191, 229, 304, 404
multi-, 7
of vector, 3
one-, 4
three-
application, 325
approximation, 369
domain, 327
grid, 255
integration, 359
mesh, 327
point, 6
space, 3, 6, 58
vector, 6, 286
two-
grid, 236, 252
vector, 3, 281
vector set, 168
Direction
in derivative, 289
Directional derivative, 289
Dirichlet
data, 369
Discrete
cosine, 36
ﬂow, 92
Fourier transform, 48

Index
427
grid, 222
mode, 36
path, 220, 221
sine, 36
wave, 32, 36
Disjoint, 160
Distinguishable, 382
Distribution
Gaussian-, 249
Poisson-, 252
Distributive law
in matrices, 13–15
in polynomials, 272
in vectors, 8
Domain, 329
complicated-, 327
nonconvex-, 330
Doppler effect, 118
2-d (see: dimension), 168
Duality, 206, 210, 215, 225
Dust, 412, 413
Dynamics, 234, 250
E
Earth
rotation, 74, 221
Edge
boundary-, 333, 335, 338
in tetrahedron, 290
longest-, 336
Effective energy, 397
Eigenvalue, 29, 81, 245, 374, 375, 392
expectation, 244
generalized-, 41, 399
of Hermitian adjoint, 30
of Hermitian matrix, 30
problem, 399
pseudo-, 399
reciprocal-, 376
Eigenvector, 29, 81, 175, 375
generalized-, 41, 399
normalized-, 29
of Hermitian matrix, 31
unit-, 29
Einstein, 103, 130, 404
equations, 414
in vacuum, 411
trace-subtracted, 415
Maxwell, 413
tensor, 414
Electric ﬁeld
tensor, 413
Electromagnetic, 413
ﬁeld, 413
tensor, 413
wave, 264
Electron, 235, 263, 381
distinguishable, 382
indistinguishable, 383
structure, 379
Electrostatic, 382
Element
equivalent-, 160, 163
ﬁnite-, 325
in matrix (see: matrix), 12
Elementary particle, 261
Ellipse, 212
Ellipsoid, 211
Energy, 232, 382, 412
absolute-, 129
conservation of, 124
constant-, 248
dark-, 414
effective-, 397
ground-, 238, 248
in spline, 369, 374
kinetic-, 126, 397, 413
level, 248
minimal-, 369, 374
nondeterministic-, 251
nuclear-, 126, 129
potential-, 126, 129, 382, 397, 413
relative-, 124
tensor, 412
total-, 126, 247
vacuum-, 414
Engine
search-, 97
Envelope, 213, 215
Equation
characteristic-, 90
Einstein, 414
in vacuum, 411
trace-subtracted, 415
matrix-, 47, 49
quadratic-, 189
wave-, 407
Equator, 197, 205
Equilibrium, 84
Equivalence
circle, 196
class, 160, 162
in a group, 162
reﬂexive-, 159, 168
relation, 168

428
Index
in a group, 161
in a set, 159
symmetric-, 159, 168
transitive-, 159, 168
Equivalent (see: equivalence)
element, 160, 163
Error
in sine decomposition, 35
in steady state, 96
Euclidean
geometry, 103, 187, 210
Euclidean space, 413
Euler
angles, 79
force, 73
Event, 117
Exchange integral, 395
Exclusion, 384
Exclusion principle, 235, 264
Expectation, 240, 245, 382
eigenvalue, 244
Experiment, 104, 233, 239
Exponent
derivative of-, 296
Taylor
polynomial, 296
series, 296
wave, 43
Extrapolation, 366
F
Face
in tetrahedron, 291
Factor
common-, 272
group, 164
action, 170
Factorization, 195
Family
of equivalence circles, 196
of equivalence classes, 160
of planes, 169
Fermeon, 263
Field
electric-, 413
electromagnetic-, 413
scalar-, 413
vector-, 287
Fixed point, 175
Flow
discrete-, 92
in graph, 92
stochastic-, 92
Fluid, 413
Fock, 379, 398
Force
centrifugal-, 71
centripetal-, 72
Coriolis-, 76
Euler-, 73
perpendicular-, 132
Form
bilinear-, 56, 58
canonical-, 395
diagonal-, 35, 38, 49
Four-dimensional (see: dimension), 103
Fourier
transform, 48
Fraction
continued-, 176
algebraic-, 177, 178
approximant, 177
coefﬁcient, 176
convergence, 176, 178
periodic-, 185
Freedom
degree of, 201, 255, 302, 370
Frequency, 32
Friedmann, 413
Frobenius, 96
Function, 153
associative law, 149, 173
basis-, 299, 373, 398
bilinear-, 56
complex-, 268
composite-, 299, 364
composition of, 170, 299
differentiable-, 287
graph of-, 278
grid-, 255, 325, 369
invariant-, 259
linear-, 205
nodal-, 369
primitive-, 277
quadratic-, 211
real-, 205, 267
space, 370
vector-, 281, 287
wave-, 235, 380
Fundamental
theorem
of homomorphism, 166
G
Galaxy

Index
429
Milky Way, 104
Gauge transformation, 407
Gaussian, 249
Generalized
eigenvalue, 41, 399
eigenvector, 41, 399
General relativity, 403, 410
Generation
group-, 79
mesh-, 327
Geometrical, 66, 305, 306
application, 103, 325
decomposition, 168
mechanics, 51
object, 145, 187
physics, 66
Geometry
analytic-, 187
curved-, 403
Euclidean-, 103, 187, 210
hyperbolic-, 114
projective-, 187
duality, 210, 215, 226
Riemannian-, 207
Gersgorin theorem, 98
Gradient, 205
in 2-d, 281
in 3-d, 286
transpose-, 287, 293
Gram–Schmidt, 63, 339
Graph
ﬂow in-, 92
matrix of, 91
of function, 278
weighted-, 91
steady state, 96
Graphics, 218, 221
computer-, 218, 221
Gravity, 410
constant, 415
length contraction, 404
redshift, 405
time dilation, 404
waves, 407
Great circle, 207
Grid
2-d, 236, 252
3-d, 255
discrete-, 222
function, 255, 325, 369
invariant-, 259
in polynomial, 270
uniform-, 32, 36
Ground state, 238, 248
Group, 1, 143, 151
action, 170, 198
associative law, 151, 159, 164
center, 158, 168, 174, 194
commute, 194
decomposition, 195
equivalence classes, 162
equivalence relation, 161
factor-, 164, 170
generation, 79
homomorphism (see: homomorphism),
154
inverse element, 152
isomorphism (see: isomorphism), 165
modulus-, 164
permutation-, 386
product, 194
quotient- (see: factor), 164
representation, 145, 149, 175, 187
rotation-, 79
subgroup, 157, 194
center-, 158, 168
kernel-, 158, 165
normal-, 163, 170
theory, 143
unit element (see: unit), 151
H
Half
open, 43
Hamiltonian, 246
Harmonic oscillator, 246
Hartree, 379, 398
product, 382
Heisenberg
picture, 234
uncertainty principle, 242
Hemisphere, 196
Hermitian
adjoint, 21, 24
eigenvalues of, 30
anti-, 238, 239
matrix, 21, 25, 239
eigenvalues of, 30
eigenvectors of, 31
Hessian, 293
matrix, 293
symmetric-, 294
Hierarchy
of meshes, 328
High-level, 187

430
Index
High-order derivative
normal-, 291
partial-, 292
Homeomorphism, 193, 196, 200, 201, 225
Homogeneous
dust, 412
Homomorphism, 154, 172
fundamental theorem, 166
invertible-, 165
one-to-one-, 165
Horizontal
coordinate, 4
line, 200
plane, 169, 170
Horner algorithm, 273
Hyperbola, 115, 212
Hyperbolic, 114
Hyperboloid, 211, 215, 406, 413
Hyperplane, 225, 228
tangent-, 228
Hypersphere, 191, 196
general-, 194
multidimensional-, 191
I
Identity
matrix, 18, 27, 35, 48, 78
in group, 151, 167, 174, 184, 194
in Lorentz, 109
inprojective geometry,194,201,224,
228
in Schur complement, 375
transformation, 109, 174
Indeﬁnite
integral, 277
matrix, 211
Independent
linearly-, 63
variable, 268, 279, 285
Index
diagonal-, 270
row-, 270
Indistinguishable, 383
Induction
diagonal by diagonal, 283
hypothesis
in composing polynomials, 273
in determinant, 52
in Horner algorithm, 273
mathematical-, 291, 293
in composing polynomials, 273
in continued fraction, 176
in determinant, 51
in differentiation, 283, 291, 293
in Horner algorithm, 273
step
in Horner algorithm, 273
in partial derivative, 284
Inequality
Cauchy–Schwarz, 40
triangle-, 40
Inertia
matrix, 82
moment, 82, 87
Inﬁnite
matrix, 234
vector, 234
Inﬁnity
circle, 205, 209
convergence to, 176
line, 209
object, 200
point, 108, 146, 204, 207, 208
Inner product, 22, 24, 57, 290
real-, 23, 205, 305
Integrable, 301
Integral
Coulomb-, 382, 396
deﬁnite-, 278
exchange-, 395
in 3-d, 360, 364
indeﬁnite-, 277
line-, 376
over general tetrahedron, 301
over interval, 278
over unit tetrahedron, 288
over unit triangle, 281
trapezoidal rule, 278
Integration
barycentric-, 308
3-d, 359
in 3-d, 360, 364
in spherical coordinates, 364
numerical-, 325, 359
Interference, 235
Internet, 97
Interval
half-open, 43
integral over, 278, 279
open-, 32, 36
unit-, 32, 36
Invariant, 113, 206
conic, 212
grid function, 259
mass, 130

Index
431
plane, 219
transformation, 62, 109
Inverse
commute, 152
element, 152
mapping, 147, 154, 171, 214, 301
matrix, 54, 78, 213, 307, 374
Moebius transformation, 147
operation, 155, 157
rotation, 78
transformation, 111, 147
Invertible
homomorphism, 165
mapping, 147, 154
matrix (see: nonsingular), 54
not-, 147
transformation, 173
Irreducible, 95
Isomorphism, 149, 165, 175
ﬁrst theorem, 166
in Lorentz, 111
in polynomials, 268
one-to-one-, 165
second theorem, 197
theorem, 166, 197
third theorem, 197
Iterative
bisection-, 333
reﬁnement-, 328, 359
J
Jacobian, 287, 307, 348, 363
determinant, 363
K
Kernel, 156, 168, 174
subgroup, 158, 165
Kinetic energy, 126, 397, 413
L
Lab, 108
coordinate, 108
Ladder operator, 245, 257
Lagrangian, 413
Law
associative-, 7, 25, 149, 159, 173
commutative-, 1, 143
distributive-, 8, 13–15, 272
Newton, 103
of nature, 103
Leibnitz, 307
Length
contraction, 115, 137
gravitational-, 404
Level (see: hierarchy, high-level, multilevel,
two-level), 187
energy-, 248
set, 115, 205, 211, 329, 333
Light, 105, 137
ray, 264
speed, 105
Limit
process, 178
Line
as level set, 205
Cartesian-, 206, 213
horizontal-, 200
inﬁnity-, 209
in projective plane, 208, 210
integral, 376
parallel-, 187, 209, 292
projective-, 199
tangent-, 213
vertical-, 282
Linear
algebra, 8, 187
bi-, 56
combination, 14, 35, 96, 226, 290
function, 205
independent, 63
in vectors, 3, 7
momentum, 66
polynomial, 278
space, 1, 4, 231, 265
Local
reﬁnement, 328
Lorentz, 107
matrix, 108
transformation, 107, 110, 127
Lower
triangular, 182
part, 18
Low-level (see: high-level), 187
M
Magnitude, 5
in spherical coordinates, 362
Main
diagonal, 18, 22
Manifold, 103, 282, 404
in cosmology, 413
Mapping
afﬁne-

432
Index
in 3-d, 348
Mapping (see: set, Moebius), 19, 146, 301
inverse-, 147, 154, 171, 214, 301
invertible-, 147, 154
kernel of-, 156
not invertible-, 147
one-to-one-, 154
onto, 154
origin of-, 154
projective-, 218, 305
Markov
chain, 95
matrix, 91, 94
Mass
center, 131
conservation, 93, 126
distribution, 93
invariant-, 130
matrix, 399
rest-, 127, 130, 132
Mathematical
induction, 293
in composing polynomials, 273
in continued fraction, 176
in determinant, 51
in differentiation, 283, 291, 293
in Horner algorithm, 273
object
in group, 154, 166
Matrix, 3, 11
adding, 12
adjoint-, 21, 24
anti-symmetric, 410, 413
assemble-, 372
coefﬁcient-, 372
commutator, 238, 256
commute, 182
complex-, 21, 150, 172, 175, 182
conjugate transpose of, 21
constant-, 184
decomposition, 375
degenerate-, 259
determinant, 51, 108, 363, 388
diagonal-, 33, 178, 182
eigenvalue, 29, 81, 245, 374, 375, 392
generalized-, 41, 399
eigenvector, 29, 81
generalized, 41, 399
element, 12
equation, 47, 49
Hermitian
anti-, 238, 239
Hermitian adjoint, 21, 24
Hermitian-, 22, 25, 239
eigenvalues of, 30
eigenvectors of, 31
Hessian, 293
identity-, 18, 27, 35, 48, 78
in group, 151, 167, 174, 184, 194
in Lorentz, 109
inprojective geometry,194,201,224,
228
in Schur complement, 375
indeﬁnite-, 211
inertia-, 82
inﬁnite-, 234
inverse-, 213
inverse of, 54, 78, 307, 374
invertible- (see: nonsingular), 54
irreducible-, 95
Lorentz-, 109
Markov-, 91, 94
mass-, 399
minor of-, 51, 212
momentum-, 123, 236
multiplying by scalar, 13, 148
nonconstant-, 182
nonsingular-, 1, 54, 143, 149, 167, 178,
194, 228, 374, 375
null space, 29, 89, 246
of graph, 91
order, 18
orthogonal-, 28, 34, 38, 40, 222
overlap-, 393
Pauli-, 263
periodic-, 46
position-, 234
positive deﬁnite, 39, 87, 374
positive semideﬁnite, 39, 81
probability-, 92, 94
product, 178, 185
projection-, 28
real-, 21, 201, 218, 224, 228
regular- (see: nonsingular), 54
representation (see: group), 145
rotation-, 63, 65, 78
Schur-, 375
self-adjoint (see: Hermitian), 21
singular-, 29, 53, 179
spectrum, 90, 94
square-, 18, 21, 25, 28, 29
stiffness-, 372
symmetric
anti-, 410, 413
symmetric-, 18, 294, 374, 375
times matrix, 15, 150

Index
433
times vector, 13
trace, 392
translation-, 219
transpose of, 17, 307, 388
triangular-
lower-, 182
part, 18
upper-, 183, 305
tridiagonal-, 34
unit-, 19
unitary-, 28, 48, 394
Maxwell, 413
Measurable, 239
Measure
zero-, 365
Mechanics
geometrical-, 51
Newtonian-, 103, 122, 231
quantum-, 231
Mesh
generation, 327
hierarchy, 328
of tetrahedra, 327
reﬁnement
in 3-d, 328, 359
regularity, 329, 347, 355
Metric, 404
Minkowski-, 407
Milky Way, 104
Minimal
angle, 348
energy, 369, 374
Minkowski, 114
metric, 407
space, 410
Minor, 51, 212
transpose-, 54
Mixed partial derivative, 283, 292
Mode
cosine-, 36
discrete-, 36
sine-, 32
Model (see: mathematical)
variational-, 369
Modulus
group, 164
Moebius transformation, 119, 146, 167, 172,
175
as matrix, 148
composition, 149, 176
continuous-, 146
inverse-, 147
invertible-, 147
not invertible-, 147
pole, 147
Moment of inertia, 82, 87
Momentum, 232
angular-, 67, 256
conservation of, 85
conservation of, 122
linear-, 66
matrix, 123, 236
nondeterministic-, 258
relative-, 124
scalar-, 255
tensor, 412
vector-, 255
Monomial
computing, 271, 275
Moon, 221
Motion
curved-, 219
in computer graphics, 219
Multidimensional
hypersphere, 191
vector, 7
Multilevel
hierarchy, 359
reﬁnement, 328
reﬁnement (see reﬁnement), 359
Multiplying
2-d vector by scalar, 5
matrices, 15, 150
matrix by scalar, 13, 148
matrix-vector, 13
polynomial by scalar, 269
polynomials, 269
Multiscale
decomposition, 35
N
Natural
number, 274
Neutron, 263
Newton, 404
constant, 415
gravity constant, 415
law of nature, 103
mechanics, 103, 122, 231
Nodal function, 369
Nonconvex, 330
Nondeterministic, 233
energy, 251
momentum, 258
Nonsingular, 178

434
Index
matrix, 1, 54, 143, 149, 167, 194, 228,
374, 375
Norm
of vector, 23
of vector product, 66
Normal
coordinate, 407
derivative, 290
high-order, 291
order of-, 291
second-, 291
in computer graphics, 221
in projective geometry, 206, 213, 221
in rotation, 221
in translation, 219
of tangent plane, 206, 213
subgroup, 163, 170
vector, 290
in angle, 349
in mesh, 336
Normalized, 24
eigenvector, 29
vector, 29
Nuclear energy, 126, 129
Null space, 29, 89, 246
Number, 187
complex-, 9
natural-, 274
operator, 244, 247
wave-, 32
Numerical
analysis, 325
application, 325
approximation, 369
integration, 325, 359
O
Object
abstract-, 145
in Euclidean geometry, 187
algebraic-, 1, 143, 145, 154, 187, 268
analytic-, 145, 268
geometrical-, 145, 187
inﬁnity-, 200
in group, 154, 166
Oblique
plane, 169–171
projection, 171, 202, 224
Observable, 233, 239
degenerate-, 259
Off-diagonal, 19, 34
One-to-one
homomorphism, 165
isomorphism, 165
mapping, 154
Onto, 154
Open
half-, 43
interval, 32, 36
Operation (see: operator or arithmetic)
inverse-, 155, 157
union-, 170
Operator
bilinear-, 56
differential-, 291
differentiation-, 124
ladder-, 245, 257
number-, 244, 247
Opposite points (see: antipodal), 188
Orbital, 381
canonical form, 395
orthogonal-, 393, 395
orthonormal-, 393, 395
Order
in search engine, 97
in the internet, 97
in the web, 97
of matrix, 18
of normal derivative, 291
of partial derivative, 283, 293
Origin, 7, 364
in 2-d, 4
in 3-d, 6
of mapping, 153
Orthogonal
columns, 27
decomposition, 68
matrix, 28, 34, 38, 40, 222
orbitals, 393, 395
plane, 206
projection, 57
transformation, 62
vectors, 26, 33, 36, 40, 206, 221, 290
Orthonormal, 221
columns, 27
orbitals, 393, 395
vector, 26, 33, 40
Oscillator, 246
Overlap
matrix, 393
Slater determinant, 394
P
Pair, 3, 285

Index
435
Parabola, 212
Paraboloid, 211
Paradox
twin-, 114
Parallel
in computer graphics, 219
in projective geometry, 219
in translation, 219
line and plane, 292
lines, 187, 209, 292
Parallelogram, 137
determinant, 53
diagonal in-, 5
in vector product, 66
rule, 4, 83
Parameter
in analytic geometry, 189
in computer graphics, 218–220
in continued fraction, 176
in derivative, 280, 286
in matrix, 55, 146, 173
in projective plane, 193
in transformation, 146, 173
Partial
derivative, 280, 286, 363
chain rule, 302
high-order, 292
mixed-, 283, 292
order of-, 283, 293
second-, 283, 294
third-, 283
Particle, 91
elementary-, 261
Path
discrete-, 220, 221
in relativity, 119
Pauli, 384
exclusion, 235, 264
matrix, 263
Perfect ﬂuid, 413
Period
in continued fraction, 185
Periodic
continued fraction, 185
matrix, 46
Permutation, 384
cycle, 385, 400
group, 386
number, 387
product, 400
switch, 384, 400
Peron–Frobenius theory, 96
Perpendicular(see:orthogonal,normal),290
force, 132
velocity, 120
Phase, 235
space, 404
Photon, 137, 235, 264
Physics (see: mechanics)
geometrical-, 66
Planck, 238, 380
Plane, 205
Cartesian-, 4, 187, 188
complex-, 193
extended-, 146, 192
decomposition, 168
family, 169
horizontal-, 169, 171
hyper-, 225, 228
tangent-, 228
in projective space, 225, 227
invariant-, 219
oblique-, 169, 171
orthogonal-, 206
parallel-, 292
projective-
real-, 201
set, 169
tangent-, 206, 213, 227
union of-, 170
vertical-, 169
Plato, 404
Point
antipodal-, 85, 189, 190, 207, 225
ﬁxed-, 175
in 3-d, 6
inﬁnity-, 108, 146, 204, 207, 208
in projective plane, 208, 210, 213
in projective space, 225, 227
joint-, 208, 210
opposite- (see: antipodal), 188
reference-, 360
Poisson distribution, 252
Polar
angle, 362
coordinates, 362
decomposition, 195, 235, 296
Polarization, 264
Pole, 147
Polynomial, 267
adding, 268
binary-, 274, 276
characteristic-, 90, 391
composition
Horner algorithm, 273
composition of, 273

436
Index
computing, 271
Horner algorithm, 273
degree of, 285, 294
linear-, 278
multiplying, 269
multiplying by scalar, 269
of 2 variables, 279
of 3 variables, 285
quadratic-, 278
root of, 90, 392
Position, 232
matrix, 234
scalar-, 255
super-, 236
vector-, 255
Positive
deﬁnite, 39, 87, 374
semideﬁnite, 38, 81
Potential
electrostatic-, 382
energy, 382, 397, 413
Potential energy, 126, 129
Precession, 75
Primitive function, 277
Principal
axis system, 81
coordinates, 81
Principle
exclusion-, 235, 264
Heisenberg-, 242
uncertainty, 242
Probability, 91
matrix, 92, 94
uniform, 92
Problem
ranking-, 97
two-body, 85
Process
limit-, 178
Product
group-, 194
Hartree-, 382
inner-, 22, 25, 57, 290
real-, 23, 205, 305
matrix-, 178, 185
matrix-vector, 150
permutation-, 400
scalar-, 23
tensor-, 252
vector-, 58, 66, 210, 226
triple-, 83, 226
Projection, 207, 215
cotangent-, 171
matrix, 28
oblique-, 171, 202, 224
orthogonal-, 57
radial-, 203, 225
Projective
geometry, 187
duality, 210, 215, 226
line, 199
mapping, 218, 305
plane
real-, 201
space, 224, 305
transformation, 218, 305
Proof
by contradiction, 30
Proper time, 112, 134
Proportion
in vector, 5, 24
Proton, 263
Pythagoras
theorem, 10, 68, 189
Q
Quadratic
equation, 189
function, 211
polynomial, 278
Quantum
dynamics, 234, 250
mechanics, 231, 414
Quotient group (see: factor), 164
R
Radius, 189
of ball, 364
projection, 203, 225
spectral-, 90, 94
Random variable, 240, 382
covariance, 241
expectation, 240, 245
variance, 241
Ranking problem, 97
Ratio
ball-, 354
in continued fraction, 178
Real
function, 205, 267
inner product, 23, 205, 305
matrix, 21, 201, 218, 224, 228
projective
line, 199

Index
437
plane, 201
space, 224, 305
vector, 23
Reciprocal, 376
Recursion
in polynomial, 271
Recursive (see: recursion)
algorithm, 274
Redshift, 405
Reference
coordinate, 359
point, 360
Reﬁnement
boundary-, 334
in 3-d, 328, 334, 359
iterative-, 328, 359
local-, 328
multilevel-, 328, 359
step, 328
Reﬂexive, 159, 168
Regularity, 329, 347, 355
adequacy, 352
estimate, 351
mesh-, 329, 347, 355
minimal angle, 348
numerical results, 355
of tetrahedron, 347
Regular (see: nonsingular), 54
Relation (see: equivalence)
in a set, 159
reﬂexive-, 159, 168
symmetric-, 159, 168
transitive-, 159, 168
Relative, 231
axis system, 63
energy, 124
momentum, 124
time, 232
Relativity
general-, 403, 410
special-, 103, 410
Remainder, 70
in sine decomposition, 35
in steady state, 96
Representation, 196
binary-, 274
decimal-, 274
group-, 145, 149, 175, 187
matrix- (see: group), 145
Rest mass, 127, 130, 132
Ricci
scalar, 414
tensor, 411
Richardson, 366
Riemann, 146
coordinates, 407
geometry, 207
normal coordinates, 407
tensor, 410
Right-hand
rule, 59, 341
side, 272
system, 70, 79
Root
of polynomial, 90, 392
of unity, 42
Rotation, 69, 77, 221
axis system, 69
coordinate, 71
Earth, 74, 221
group, 79
inverse-, 78
matrix, 63, 65, 78
Row
index, 270
in matrix, 12, 15
number, 15
vector, 293
Rule
chain-, 302, 305, 307
Cramer-, 55, 111, 137, 305
Leibnitz-, 307
parallelogram-, 4, 83
right-hand-, 59, 341
trapezoidal-, 278
S
Scalar, 4
multiplying a matrix, 13, 148
multiplying a polynomial, 269
multiplying a vector, 5
product, 23
Ricci-, 414
Scale, 35, 238
Schmidt, 63, 339
Schrodinger, 234
Schur
complement, 375
matrix, 375
Schwarz, 40
Search
engine, 97
the internet, 97
the web, 97
Self

438
Index
adjoint matrix (see: Hermitian), 22
coordinate, 108
system, 107
time, 134
Semicircle, 201
Semispace, 202
Sequence, 3
Series
Taylor, 296
truncated-, 296
Set
decomposition, 160
disjoint-, 160
equivalence, 160
in a group, 162
relation, 159
level-, 115, 205, 211, 329, 333
mapping, 153
of equivalence classes, 160
of planes, 169
subset of-, 96, 154, 163
vector-, 168, 201
zero
measure, 365
volume, 365
Side
in tetrahedron, 291
Sine
decomposition, 35
derivative, 296
discrete-, 36
mode, 31
Taylor
polynomial, 296
series, 296
transform, 34
wave, 31
Singular, 365
matrix, 29, 53, 179
Singularity, 361
Skew-symmetric, 23
Slater, 384, 394
Solar system, 412
Space
Cartesian-, 6, 58, 190
3-d, 3, 6, 58
function-, 370
linear-, 1, 4, 231, 265
Minkowski-, 114
null-, 29, 89, 246
phase-, 404
projective-, 224, 305
semi-, 202
time, 103, 117, 404
vector-, 3, 8, 143, 265
Spacetime, 103, 117, 404
metric, 404
Special relativity, 103, 410
Spectral, 35
decomposition, 35, 95
radius, 90, 94
Spectrum, 90, 94
Speed (see: velocity)
of light, 105
Sphere, 193, 231
coordinates, 362
divided-, 204
general-, 191, 194
hemi-, 196
hyper-, 191, 196
multidimensional-, 191
Riemann-, 146
unit-, 190
Spin, 261
Spline, 325, 369
B-, 299
Square
matrix, 18, 22, 25, 28, 29
Standing wave, 248, 250
Star, 404
State, 92, 235
coherent-, 250
dynamic-, 250
ground-, 238, 248
physical-, 233
steady-, 96
stochastic-, 92
Steady state, 96
Stiffness, 372
Stochastic, 92, 231
ﬂow, 92
state, 92
Stress, 412
Subgroup, 157, 194
center-, 158, 168
kernel-, 158, 165
normal-, 163, 170
Subset, 96, 154, 163
Sun, 221
Superposition, 236
Surface, 282
Switch, 384, 400
Symmetric
anti-, 410, 413
Hessian, 294
matrix, 18, 294, 374, 375

Index
439
relation, 159, 168
skew-, 23
Symmetrization, 239
System
axis-, 211
principal-, 81
relative-, 63
rotating-, 69
closed-, 85, 126
coordinate-
relative-, 63
isolated-, 85, 126
lab-, 108
passive-, 132
right-hand, 70, 79
self-, 107
T
Tangent
derivative, 292
hyperplane, 228
in computer graphics, 219, 220
in projective geometry, 219, 227
line, 213
plane, 206, 213, 227
Tangential derivative, 292
Taylor
polynomial, 296
series, 296
Tensor
anti-symmetric, 410, 413
Einstein-, 414
electromagnetic ﬁelds, 413
energy-, 412
momentum-, 412
Ricci-, 411
Riemann-, 410
stress-, 412
symmetric
anti-, 410, 413
Tensor product, 252
Tetrahedron
general-, 299
integral over, 301
mesh of, 327
regularity, 347
side (see: side), 291
unit-
integral over, 288
volume of, 359
Theorem
fundamental-, 166, 278, 281
Gersgorin-, 98
isomorphism-, 166, 197
Pythagoras, 10, 68, 189
Theory
group-, 143
Time, 106
axis, 106
dilation, 113, 117, 134
gravitational-, 404
event, 117
proper-, 112, 134
relative-, 232
self-, 134
space-, 103, 117, 404
Topology, 193, 196, 200, 201, 225
Total
energy, 126, 247
Trace, 392
subtracted, 415
Trajectory, 219
in relativity, 119
Transform
cosine-, 38
Fourier-, 48
sine-, 34
Transformation
afﬁne-
in 3-d, 348
composition, 110, 149, 176
gauge-, 407
identity-, 109, 174
invariant-, 62, 109
inverse-, 111, 147
invertible-, 173
Lorentz-, 107, 110, 127
Moebius- (see: Moebius), 146
orthogonal-, 62
projective-, 218, 305
Transitive, 159, 168
Translation, 218
matrix, 219
Transpose (see: matrix, vector, tensor), 17,
388
conjugate-, 21
gradient, 287, 293
minor, 54
vector, 293
Trapezoidal rule, 278
Triangle
inequality, 40
unit-
integral over, 281
Triangular

440
Index
lower-, 182
part, 18
upper-, 183, 305
part, 18
Tridiagonal
matrix, 34
Twin paradox, 114
Two-body problem, 85
Two-dimensional (see: dimension), 168
U
Uncertainty
principle, 242
Uniform
grid, 32, 36
Union
of planes, 170
operation, 169
Unit
atomic-, 380
circle, 189
closed, 100
open, 100
cube, 330
closed-, 343
eigenvector, 29
element, 151
in factor group, 164
in homomorphism, 155
in Moebius group, 167
interval
integration, 279
open-, 32, 36
matrix, 19
sphere, 190
tetrahedron
integral over, 288
triangle, 281
vector, 24, 29, 34, 221, 289
in 3-d, 56
standard-, 25, 56, 77, 150, 178, 226,
236
Unitary, 394
Unitary matrix, 28, 48
Unity
root of, 42
Universe, 137, 412
closed-, 413
compact-, 413
Euclidean-, 413
ﬂat-, 413
noncompact-, 413
open-, 413
Upper triangular, 18, 183, 305
V
Vacuum
Einstein equations, 411
energy, 414
Variable
independent-, 268, 279, 285
in polynomial, 268, 279, 285
random-, 240, 382
covariance, 241
expectation, 240, 245
variance, 241
three-, 285
two-, 279
Variance, 241
Variational, 369
Vector, 3, 6
antipodal-, 200, 203
column-, 12, 26, 300
complex-, 11
component of-, 3
coordinate of-, 3
cosine of-, 331
2-d, 3
gradient, 281
3-d, 6
gradient, 286
dimension of-, 3
ﬁeld, 287
function, 281, 287
inﬁnite-, 234
inner product, 22, 24, 57, 290
real-, 23, 205, 305
norm, 23
normal-, 206, 221
in angle, 349
in mesh, 336
normalized-, 24, 29
orthogonal-, 26, 33, 36, 40, 206, 221
orthonormal-, 26, 33, 40, 221
product, 58, 66, 210, 226
triple-, 83, 226
projection, 57
real-, 23
row-, 293
set, 168, 201
space, 3, 8, 143, 265
transpose of-, 293
unit- (see: unit vector), 29, 34
Vecuum, 410

Index
441
Velocity, 70
adding-, 104
angular-, 69
perpendicular-, 120
Vertex
in tetrahedron, 299
Vertical
coordinate, 4
line, 282
plane, 169
Visualization, 191, 193, 211, 218
W
Wave, 137
amplitude, 35, 235
cosine-, 36
discrete-, 32, 36
electromagnetic-, 264
equation, 407
exponent-, 43
frequency, 32
function, 235, 380
gravity-, 407
interference, 235
number, 32
phase, 235
sine-, 31
standing-, 248, 250
superposition, 236
Web, 97
Weighted graph, 91
Z
Zero
measure, 365
volume, 365

