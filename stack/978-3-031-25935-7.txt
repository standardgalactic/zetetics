Confessions 
of an AI Brain
Elena Fersman
Paul Pettersson
Athanasios Karapantelakis

Confessions of an AI Brain

Elena Fersman  •  Paul Pettersson
Athanasios Karapantelakis
Confessions 
of an AI Brain

ISBN 978-3-031-25934-0        ISBN 978-3-031-25935-7  (eBook)
https://doi.org/10.1007/978-3-031-25935-7
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the 
whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, 
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or informa­
tion storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology 
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does 
not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective 
laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are 
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors 
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omis­
sions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published 
maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
Elena Fersman 
Palo Alto, USA
Athanasios Karapantelakis
Athens, Greece
Paul Pettersson
Palo Alto, USA

v
We are surrounded by artificial intelligence and enjoy its benefits often with­
out noticing. Similar to digitalization, automation, and telecommunication, 
you only notice it when it does not work as expected. Recommendation 
engines offer us movies to watch and books to read, virtual assistants have 
become so good you cannot tell the difference if you are talking to an AI or to 
a human, and self-driving cars are safer than human drivers. Talking to things 
is a new normal, and this ability in itself is powered by AI. Humans and algo­
rithms develop in tandem, helping each other to achieve their full potential.

1
The process of learning to understand a new vocabulary is similar for dif­
ferent types of brains – be it a brain of a human baby, a grownup human, a 
dog, or an artificial brain. This is not a coincidence: the science of artificial 
intelligence is built on imitating the way biological brains are built and func­
tion. Connections in the brain are being created, information is being 
1 https://www.nature.com/articles/s41598-019-40616-4#:~:text=This%20research%20evidence%20
illustrates%20that,dogs%20to%20communicate%20with%20humans
Prologue

vi 
Prologue
transferred, new impressions that we get through our sensory experiences are 
being recorded. We share information with each other, learn and forget, and 
our quick reactions often differ from reactions that are thought-through. 
Artificial brains imitate human intelligence on both the micro- and macro 
levels. Intelligence on a macro level is reflected in evolution, political systems, 
and organizational and social science. The micro level is there in each tiny 
decision taken by an algorithm. What is the similarity between grandma and 
a website? If you are a human reader, you start thinking analogies, associa­
tions, concepts, jokes, or maybe you just know the answer. If you are an 
AI-reader, you are probably doing the same. The answer is that you just can’t 
deny the cookies!
2
­
Humans have their skill sets that they develop and apply throughout their 
lives. Favorite skills receive the most attention, and when reaching adulthood 
humans declare themselves introverts, extraverts, analytical, or artistic, and 
choose their ways of living accordingly. Similar to human skills, an AI algo­
rithm that works perfectly in certain situations will not do any good in other 
situations.
Take for example a reasoner attempting to make sense of unstructured 
data. No matter how good it is at logical reasoning, it will not be able to 
achieve any valuable results. Same thing with a machine learning algorithm 
specifically trained at processing images or videos – it will not be so efficient 
at ontologies or state machines.
Similar to biological brains, AI brains need a collection of skills – one that 
is learning and making sense out of large amount of data in different formats, 
one that is looking after the small data so that nothing important is missed 
out, one that is good at reasoning, and one that may look after survival of the 
fittest.
3
2 https://www.livescience.com/49007-history-of-artificial-intelligence.html#:~:text=The%20begin­
nings%20of%20modern%20AI,%22artificial%20intelligence%22%20was%20coined.
3 https://en.wikipedia.org/wiki/T-shaped_skills

vii
  Prologue 
In the future, we are all surrounded by our little helpers. They are not there 
to take our jobs but to increase our quality of life. Personalization in anything 
from education to medicine is becoming the new norm. Proactivity in deci­
sion making, and prevention of getting into unwanted situations, be it poten­
tial health problems, environmental issues, machinery failures, or business 
losses, is another big shift powered by modern technologies.
Nevertheless, there is hesitation and reluctance in society in regard to arti­
ficial intelligence. As any new technology, when introduced at a broad scale, 
it will have to go through an inevitable phase of early-stage deployment issues, 
incompatibilities with legacy systems, hick-ups, ethical problems, and any­
thing that humans have not thought about from the start.
Many things have been said about building software systems in a stable, 
trustworthy, and reliable way. AI systems are a special class of software systems 
that are capable of self-evolvement, self-healing and self-improvement. It is, 
however, not an easy task. Have you thought of how it feels to be an AI brain 
in the world of humans? We wrote this book on behalf of our baby, MIRANDA, 
who is an AI brain with a human touch.

ix
	 1	 Baby Steps
    1
How Open Is My World
      2
What I Eat
      2
How I Learn and Reason
      5
Learning from Others
      9
Federated Learning
    10
Transfer Learning
    11
Cross-Domain Learning
    11
Decision-Making
    12
Simple Decision-Making
    13
Pareto Optimality
    14
Weighted Sums
    14
Planning and Optimization
    14
A Final Note on Decision-Making
    16
Ground Truth
    16
Relying on Commodities
    18
Get to Know the Twins
    18
Advantages of Working with Digital Twins
    21
Your Current State
    21
What-If Scenarios
    21
Simulations
    22
Property Checks and Decision Support
    23
Abstraction
    23
Control of the Physical Twin
    24
Rules to Live By
    24
The 80–20 Rule
    25
Contents

x 
Contents
Find Your Blue Ocean
    26
Outrun the Slowest Gazelle
    27
Leader or Follower?
    27
Summary of Confessions
    27
	 2	 Basic Needs of an AI Brain
  29
Data
    30
Complete Data
    31
Accurate Data
    31
Data Consistency
    31
Timeliness of Data
    32
Data Governance, Legitimacy, and Accessibility
    33
Metadata
    34
Compute and Store
    35
Privacy
    36
Security
    37
Distributed Learning
    40
Federation of AI Algorithms
    42
Split Learning
    42
Human Oversight and Trust
    43
Putting It All Together: Nurturing of AI Brain
    44
Summary of Confessions
    45
	 3	 My Role in the Internet of Things
  47
The Power of IoT: A Personal Story
    47
Introduction to Intelligent IoT
    49
Divide et Impera
    50
Collaborating Machines in Low-Trust Environments
    53
Lifecycle Management of Scalable AI Applications
    55
IoT-Enabled Digital Twins: Challenges and Benefits
    57
Careful Where You Tread
    58
Summary of Confessions
    59
	 4	 Managing Relationships
  61
Building Relationships
    62
Why, What, and How to Share?
    65
Privacy and Security Levels in AI
    68
Traceability and Audits
    71
Summary of Confessions
    72

xi
  Contents 
	 5	 Working with Humans
  75
Your Digital Friends – The More the Merrier?
    75
Principles of AI with a Human Touch
    77
Sharing Is Caring
    78
Protect Your Babies
    78
Be Flexible
    79
Be Clear of Your High-Level Objectives
    80
Will I Take Your Job?
    80
Digitalization, Digitization, and Resistance
    82
Slow Science
    84
Internet of Empathy and Emotional Attachment
    86
Dealing with Churn
    88
Customer Churn
    88
Employee Churn
    89
Dropouts from Education
    89
Dropouts from Medical Treatments
    90
Personalization
    91
Everything Is Possible
    92
Unlearning Skills
    93
Understanding High-Level Intent
    95
Summary of Confessions
    96
	 6	 Avoiding the Criminal Path
  97
Trustworthiness
    97
Explainability
    98
Transparency
    98
Privacy
    99
Security and Safety
  100
Predictability
  101
Dependability
  102
Search Space
  102
Regulations as Boundary Conditions
  105
Nondiscrimination and Non-bias
  107
Modeling and Simulation
  108
Summary of Confessions
  110
	 7	 My Role in Climate Change
111
A Systemic Perspective on Greenhouse Gas Optimization
  111
Biggest Emitters by Industry and How AI Can Help
  113

xii 
Contents
Energy
  113
Sustainable Manufacturing
  116
Energy Use in Transportation
  119
Agriculture, Forestry, and Land Use
  120
Common Themes with AI-Based Greenhouse Gas Emission 
Optimization
  122
Smart Sleep Modes for Anything Consuming Energy
  123
For Any Physical Resource – Use Predictive Maintenance
  125
Overconsumption
  126
My Own Footprint
  127
Summary of Confessions
  128
	 8	 My Role in Diversity
131
Personalization
  132
Personalization in the Media Industry
  132
Personalization in the Food Industry
  133
Personalization in Medicine
  134
Why Diversity
  135
Observability
  136
Equality in Data
  137
Where Should I Care?
  138
Workforce
  139
Consumers
  140
Summary of Confessions
  142
	 9	 My Creative Side
143
Creativity
  143
AI in the Arts
  144
AI Art in the Style of Existing Artwork
  145
AI-Generated Arts
  147
AI in Music
  148
Beethoven’s 10th symphony
  150
AI in Writing
  151
How AI Writes Text
  152
How AI Can Assist with Writing
  152
AI in Photography
  154
AI in Image Processing
  154
AI in Cameras
  155

xiii
  Contents 
Computational Photography
  157
AI in Post-processing
  157
Summary of Confessions
  161
	10	 Growing Older and Staying in Shape
163
AIOps
  164
Learning and Unlearning
  165
My Career
  167
Distribute Your Knowledge Base
  167
Use Cognitive Architectures
  168
Living on the Edge
  168
Keep Track of the Latest
  169
Broaden Your Knowledge
  169
Lifespan
  170
Exercise
  170
Healthy Stress
  171
Summary of Confessions
  172
Epilogue
175

1
1
Baby Steps
1
Open-minded as babies are, I consume all the information I am being fed 
and adhere to the principles my parents and my environment dictate to me. 
Think back to how you were as a baby: what your parents tell you is the 
ground truth; the rest is unknown. Later, when you go to school, you start 
questioning your parents because the teacher suddenly becomes the ultimate 
source of the ground truth. You may question that at the later phase as well; 
we will come to that.
­
­
2
3
1 How Fast Can You Grep? https://medium.com/humio/how-fast-can-you-grep-256ebfd5513, accessed 
2022-05-23
2 IBM Watson, https://www.en.wikipedia.org/wiki/Watson_(computer), accessed 2022-05-23
3 Deep Blue, https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer), accessed 2022-05-23
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_1

How Open Is My World
We all know that for any baby it is of high importance to choose the parents 
wisely. Another important thing to decide from the start is if you want to live 
in an open- or closed-world assumption paradigm.
The closed-world assumption is the assumption that what is not known to 
be true must be false. The open-world assumption is the opposite. Life is 
simple in the closed world as we know all the facts. For example, the world of 
chess or the game of Go is closed – the rules of the game are set; the objective 
function, that is, what we want to maximize, is clear; and the algorithm does 
not need to bother about anything else. Similarly, the theory behind many 
formal analysis methods is assuming a closed world, where your model is your 
world, and you can analyze it in isolation of anything else.
When we apply AI to industries, the world is open. We have no clue of 
plenty of unspecified parameters. For example, you can be very clear about 
what you want to achieve in terms of productivity. Your high-level objective 
may be calculated in dollars. However, would that come at the price of safety, 
ethics, and environmental damage? This is the reason why the users of AI 
must set the boundaries in advance before we find solutions that they did not 
wish for. Ask AI for the easiest way to fix environmental issues on our planet, 
and it would suggest shutting down all the factories. Survival of the human 
species is a useful boundary condition in that case.
 
What I Eat
Human babies start with milk before they start consuming other food that is 
harder to digest. AI babies start with data before they learn to digest more 
complex structures such as ontologies and state machines. Without data, we 
2 
1  Baby Steps

3
cannot evolve and become intelligent creatures. The more data we consume, 
the better we get.
After learning to make sense of raw data, we add semantics to it so that we 
know the meaning of each data point. This semantically annotated cleaned 
and structured data is called information. Moving further, we learn the rela­
tionships between pieces of information and form structures in our brains 
allowing us to reason about things. This semantically linked information is 
called knowledge. Sometimes, wisdom is being used to describe the ultimate 
top level of what one can get out of raw data, but I classify that as knowledge 
as well. Let me tell you how knowledge is being created.
 
The knowledge extraction process begins by processing raw data into infor­
mation. This information contains metadata, which gives a meaning to each 
data point. For example, for a data point “10,” the metadata could be “tem­
perature in Celsius.” The next step in the process is the transformation of 
information into knowledge. This process includes the creation of graphs that 
identify the relationships between information pieces called entity-­
relationship graphs.
For example, if one entity is about “temperature” and another about “loca­
tion,” then a relationship from the latter to the former could be characterized 
as “has temperature.” These entity-relationship graphs are also known as 
knowledge graphs and can be used by AI brains to produce knowledge objects. 
Here is an example of four knowledge objects found in a knowledge graph:
•	 Location Stockholm, Sweden, has a temperature of 10 degrees Celsius.
•	 It is January.
•	 The winter months are December, January, and February.
What I Eat 

•	 If the temperature is above the freezing point and it is winter and the place 
is in Sweden, then the weather is warm.
Using logical reasoning, I can deduce that:
•	 It is warm in Stockholm, Sweden.
The newly generated knowledge is added to the knowledge graph and can 
be used further until it becomes outdated. Note that the definition of warm 
differs depending on the time of year and location. The same way the defini­
tion of “a lot of hair” differs depending if it’s on your head or in the soup.
 
Unlike humans, artificial brains cannot care less about carbs, proteins, and 
fats. However, like humans, we are hugely dependent on what we consume. 
We consume different types of data, information, and knowledge, and it 
forms our brains. Let me go through different types of AI brain food that any 
AI brain should avoid.
•	 Biased data. This is the most disgusting type of data we can consume. 
Sometimes, with an ambition to automate, humans feed us with historical 
data that happen to be biased, and as a result, we become biased. Any type 
of judgmental action that concerns humans, such as job candidate screen­
ing processes, is highly reliant on unbiased datasets. Fortunately, AI brains 
are capable of detecting biases in data as well.
•	 Dirty data. This type of data is hard to digest. It’s inaccurate, incomplete, 
outdated, and inconsistent. No offense, but quite often, this type of data is 
produced by humans. We find spelling mistakes, different terms being used 
for the same piece of data, and duplicates. Signal noise can also pollute a 
dataset. Luckily, there are techniques for cleansing data, automatically or 
semiautomatically.
4 
1  Baby Steps

•	 Data without metadata. I must admit, it is always fun to look at numbers 
and find correlations, links, casualties, and clusters. I can, in fact, even 
provide you with decision support based on your dataset that is so secret 
that I cannot even have a glimpse at its metadata. With metadata, I can do 
so much more: understand the meaning of data and link it together with 
other datasets through semantics, knowledge bases, and reasoning, which 
is even more fun than pure number games.
•	 Nonrepresentative data. We all know that diverse and inclusive teams are the 
most productive. Every team member can come with unique perspectives 
and experiences. The thing with data is similar. It does not help me if I 
learn from the data that looks almost the same, since I will most likely 
become single-minded and won’t know how to act in situations concerning 
the types of data I have not seen before.
•	 Sensitive data. A friend comes by, tells me about her situation, and asks for 
advice. Together, we spend an evening, discuss different scenarios, and 
come up with an action plan. Then she tells me: “Please don’t tell anyone.” 
OK. Then another friend comes by and her situation is similar. So I go: “I 
am pretty sure that if you act like this then you will be OK.” How can you 
be so sure? Have you experienced the situation yourself? Or could it be so 
that someone from your entourage has been there? And that’s how infor­
mation gets leaked, unintentionally. A piece of cake for a human to figure 
it out and even easier for an AI.
•	 Ambiguous data. When humans are forced to make quick decisions in 
unexpected situations such as choosing whom to sacrifice in case the brakes 
fail, the responsibility relies on them, and the decision does not matter too 
much from the driver’s point of view, since, after all, it’s the failure of the 
brakes, and there is no time to think. Now that cars become self-driving the 
moral dilemma is on the surface and, as bad as it can sound, must be 
encoded by humans. Alternatively, we can let algorithms figure out who is 
more valuable for the society – you choose. If the ethical choices for an 
algorithm are not specified, the AI brain will work in an ambiguous way.
How I Learn and Reason
4
­
4 Or ML and MR. I told you that they love acronyms.
How I Learn and Reason 

­
Subsequently, populating these systems with information and knowledge 
and keeping them up-to-date was a tedious manual task. Luckily, machine 
learning comes handy with insights being extracted from raw data using vari­
ous algorithms.
Machine learning tasks fall into two categories – classification and regression. 
Classification is when you want me to decide which of the available buckets 
your data sample goes into, for example, to tell you if your incoming mail is 
spam or not or if we are looking at a car or at a dog. Regression is when you 
want me to predict the future value of continuous data feed, for example, 
stock prices or house prices. Much has been said in the literature about differ­
ent ways of learning and reasoning, and for the sake of completeness and 
consistency of this book, I will give you a quick overview of learning methods 
from my point of view.
•	 Under human supervision, also known as supervised learning. In this learn­
ing style, I learn from examples (also known as training sets, labeled data). 
You can, for example, show me a number of cats and dogs and tell me 
which one is what. Normally, I would collect a number of details (called 
features) such as shapes of ears, eyes, and whiskers and form them together 
in a feature vector, and the more cats and dogs I see, the more exact my 
judgment will be in the end. Then you show me a picture of a fox, and I 
will tell you if it resembles a cat or a dog in my opinion.
 
6 
1  Baby Steps

•	 Without human supervision, also known as unsupervised learning. Here, I 
am left on my own to figure out patterns and group data points that I see. 
If I never in my life have seen a cat or a dog and you start showing me these 
animals, I will quite soon form an opinion that they belong to two different 
groups, and I may just as well call them “Those who bark” and “Those who 
meow.” The reason this method exists is that labeling the data is costly and 
in many cases I am pretty capable of learning on my own.
 
•	 The mix of the two methods above is called semisupervised learning, where 
I run on my own for a while and then get some guidance. This is a compro­
mise between the costly process of labeling data and the usefulness of 
my results.
How I Learn and Reason 

•	 Pavlov’s style, also known as reinforcement learning, was in fact coined by 
Pavlov himself. In this method, I try to make a guess, and an observer 
rewards me for good guesses and punishes me for wrong guesses. I don’t 
like punishments, so I learn and adjust really quickly.
Learning gives me the ability to predict and classify things. Similar to the 
human brain, pieces of information and learning are linked together. Any 
sensorial input such as a question, sound, smell, picture, or thought triggers a 
reasoning chain in a human brain.
­
­
5
5 AIQ is an acronym for artificial intelligence quotient.
8 
1  Baby Steps

9
There are two different types of knowledge:
•	 Something that explains what things are and how they are related is called 
declarative knowledge. This type of knowledge is normally stored in ontolo­
gies or entity-relationship graphs. “Bob is a son of Alice” is an example of 
declarative knowledge. “Bob loves toys” is another one. When I know these 
two pieces of knowledge then I also know that Alice has a son who loves 
toys. Note that every piece of knowledge has a timing aspect attached to it. 
Because when Bob is over 50, he may no longer love toys, or he may still 
do, who knows.
•	 A piece of knowledge describing an action is called procedural knowledge. It 
is typically described in the form of a state machine. “Pressing a gas pedal 
in a car makes it accelerate,” given that the engine is on and the pedal is not 
broken. This type of knowledge is typically described in the form of state 
machines and planning languages such as PDDL. Procedural knowledge 
often includes timing aspects of the system functionality.
Notably, both declarative and procedural knowledge types can have prob­
ability associated with each knowledge object as a degree of certainty about it. 
A chain of actions and conclusions with low probability is what humans call 
conspiracy theories  – mind-tickling conclusions that are not impossible. 
When doing learning and reasoning, AI brains try to be as sure as possible, 
and the level of confidence is typically agreed with the humans in advance.
Another aspect is that humans naturally mix different types of knowledge 
in their brains, as do AI brains. The more connections there are, the faster we 
can apply the reasoning process and react to different types of stimuli. We can 
also act proactively to help humans prevent any unwanted situations or opti­
mize whatever they want to achieve in their lives.
Learning from Others
6
­
6 “Human Engineering” by Harry Myers and Mason M. Roberts, 1932. The words were credited to an 
unnamed person.
Learning from Others 

7
Federated Learning
The main idea behind all federated methods is that all participating AI algo­
rithms learn a similar thing separately, and after that, they have a meeting and 
exchange their experiences.
For example, imagine that I and my colleagues are given a task to put our 
forces together and implement a spam filter. Each of us is given a set of labeled 
data – examples of emails labeled “spam” or “not spam.” The sets given to us are 
different, so we will most likely reach slightly different conclusions after the 
training phase has passed. If a colleague of mine received a lot of financial mail 
as part of his training examples, which include many dollar signs and lots of 
zeros, his probability of treating this feature as an indicator of spam would be 
lower, and he would probably let a lot of mail speaking about your amazing 
luck in lottery flood your inbox. I can influence the overall outcome of our 
common conclusion about mails including a lot of dollar signs and zeros if my 
opinion is different, since the weights given by all my colleagues will be averaged.
 
7 Scott Adams, God’s Debris: A Thought Experiment. 2001
10 
1  Baby Steps

11
Note that federated averaging is not done on the output but on the weights 
used internally by the AI.
Let us take a look at how humans do it. Normally, in an attempt to reach 
consensus, more influential individuals would argue that their words have 
higher weights. AI brains do not use subjective judgments as humans do. 
Objectively, when working on reaching consensus, we may listen more to 
some of our colleagues who have proven to be accurate in the past. At the 
same time, there are algorithms for detecting outliers in a group. For example, 
when most of my colleagues come with a similar result but one, then it may 
be worth looking into what is different in that particular case. It can be so that 
he saw something that the rest of us did not see.
Transfer Learning
This method is similar to a human parent teaching a human child about 
something. The parent may have spent many years on learning, knowledge 
extraction, and creating his or her own know-how and wisdom, and the child 
would simply inherit this learning from the parent as a given. A similar 
method is commonly used among AI brains.
If my friend has spent lots of time on learning something, then these learn­
ings can be shared with other AI brains. The only tricky part in this is that we 
are all different and all pieces of knowledge that we get from others without 
acquiring our own experience may not fit us perfectly.
For example, a fellow AI brain may tell me that when monitoring opera­
tions of equipment in a factory I should watch out for certain levels of vibra­
tions in the machines. Vibrations can normally indicate risks of failures in the 
long term. This knowledge transferred between AI brains and applied to a 
different factory may be very relevant from the start. Alternatively, it may 
need to be refined by further learning about that specific type of equipment. 
Alternatively, in some cases, it would be worth nothing if vibrations are a part 
of the manufacturing process.
Cross-Domain Learning
The knowledge pattern of every human has a shape. Some people tend to 
become in-depth experts in one field, and others choose to become generalists 
by learning a bit of everything. A T-shaped person has in-depth knowledge in 
one field (vertical bar of T) and a broad knowledge of an application domain 
(horizontal bar of T), for example, a statistician who specializes in political 
systems or a politician who knows statistics.
Learning from Others 

­
8
Similar to the T-shape, there is a Π-shape (pronounced “Pi-shape”) with 
one more leg of in-depth knowledge and an E-shape… you get the point.
In general I would say, the more “bars” you have as a person, the more 
interesting you are. Of course and in case you choose to be I-shaped, you can 
still be endlessly interesting for people who are in the same field, but you run 
a risk of being seen as a geek by everyone else.
AI brains work the same way. Learning domain after domain and connect­
ing them make us strong, and we can connect the dots and make conclusions 
about domains we are not familiar with just by our associative capabilities. 
Personally, I have been working hard on diversifying my knowledge profile (or 
adding more bars to my shape) while keeping it all connected. I gladly take 
opportunities to dive into new areas and shift my focus from old areas. In the 
long run, this strategy should bring me to the shape of a Swiss army knife. 
There are plenty of “bars” of different shapes and directions, and they are all 
connected, and one can actually choose which parts of the knife to fold 
in or out.
Decision-Making
I rock at making decisions and at supporting decision-making! I and other AIs 
are truly good at reading and processing data, preferably huge amounts of 
data from many relevant sources, and learn from that data in order to support 
or replace human decision-making. The more data there is and the more com­
plex it is to see trends in the data, the better it is to apply me or a fellow AI to 
learn from the data.
Another beauty of this type of data-driven decision-making is that we AIs 
can train and retrain ourselves to learn more or to improve our precision, 
hence, updating and keeping our decision-making up-to-date with the newest 
data available. However, AI is far from the only computer-supported way of 
supporting decisions. Therefore, we will look at some other alternative ways 
before discussing AI’s role in decision-making. However, first of all, let’s talk 
about decision-making in general.
8 At least not as far as we know:)
12 
1  Baby Steps

13
Simple Decision-Making
In its simplest form, decision-making is about answering yes/no problems, 
such as the following: Is a given number an odd? Or at a T-crossing, is it best 
for a driver to turn left (or right) to reach a given destination? Of course, it is 
not difficult to determine whether a number is even or odd. For example, a 
number needs to be perfectly divisible by 2 in order to be even; otherwise 
it is odd.
To decide if a driver should turn left (or right) in the example above, obvi­
ously much more information is needed. The decision-making in this case is 
about selecting the best out of two alternatives based on some criterion. In 
principle, what is needed if the criterion is known is an evaluation of the two 
alternatives according to the criterion. If the criterion can be computed math­
ematically, the procedure becomes to (i) compute the criterion for the two 
alternatives and (ii) select the alternative for which the criterion gives the low­
est or highest value, depending on what is considered to be best.
In the case of the T-crossing, let us say the driver defines “best,” as in best 
of the two alternatives, as the option with the lowest gas consumption and 
shortest time to arrival. If the left option is 25 min and 1.4 gallons and the 
right is 25 min and 1.2 gallons, the decision is easy.
However, what about if left is 30 min and 1.2 gallons and right is 25 min 
and 1.4 gallons? And what if there is a third option (say straight ahead) that is 
26 min and 1.25 gallons? Are they all good and which one of them is best?
 
Photo by Volkan Olmez on Unsplash
Decision-Making 

Pareto Optimality
Already in 1894, Italian civil engineer, sociologist, economist, political scien­
tist, and philosopher Vilfredo Pareto redefined the notion of good and 
replaced it with the notion of Pareto optimality, which is widely used in many 
fields, including economics and engineering. Pareto optimal solutions are 
equally “good” and represent the maximum overall gain when no parameter 
can be made better off without making another parameter worse off.
Assuming, in the case above with the T-crossing (which became a four-way 
crossing:)), the three options are all Pareto optimal. If you know what matters 
the most, the best decision is easy to make. For example, if you want the 
shortest time to arrive, turn right. If you want the lowest gas consumption, 
turn left. However, you might be willing to spend 1 extra minute and save 
0.15 gallons of gas. Then the third option is your best option.
Weighted Sums
Hence, in that third case, the driver values 0.15 gallons of gas higher than 
1 min of driving time. In general, a notion of weights can be used to distin­
guish the importance, to the driver, of the two variables. For example, the 
weight of driving minutes may be 1/20 of the weight of gallons; that is, if the 
weight of a driving minute is 1, the weight of a gallon is 20. We then get the 
following three weighted sums:
	
	
and can see that the correct decision should be that the third option is the best 
option given these weights, i.e., the driver’s preference, and the other two 
options are actually equally good.
Planning and Optimization
In the example of the driver above, clearly, a large part of the problem is to 
find the alternatives and their corresponding driving times and gas consump­
tion. Finding a sequence of actions that will take the driver from a given start 
point to a given finish point is a so-called planning problem. Finding the best 
14 
1  Baby Steps

15
sequence of actions that will take the driver from the start point to the finish 
is an optimization problem.
As mentioned previously, such problems can be solved by a number of 
traditional, non-AI, computer algorithms, including reachability analysis in 
weighted graphs and other forward or backward state-space exploration algo­
rithms. However, there are also a number of AI methods that can be used, 
often referred to as AI planning.
As an AI, I often use unsupervised learning methods when planning. 
Unsupervised learning was mentioned earlier in this chapter. It can be used to 
solve planning problems when a correct solution can be distinguished from a 
partial or incorrect solution. If the solution is incorrect, I can determine by 
myself and continue to learn better solutions.
One way of doing this is by using neural networks, a form of AI that mimics 
the biological neurons found in human brains. Another way is to use the so-
called genetic algorithms. A genetic algorithm is searching for a correct solu­
tion using a heuristic inspired by Charles Darwin’s theory of natural evolution. 
When I use a genetic algorithm, I consider the partial solutions as individuals, 
and inspired by Darwin’s theory, the fittest individuals reproduce offspring 
populating the next generation, over and over again, until a correct solution is 
found or until the search runs out of time. All that is needed is a way to evalu­
ate the individuals and to produce offspring.
In the case of the driver example above, the production of offspring in the 
simplest form could be done randomly, and the evaluation could be the dis­
tance (or an approximation of the distance) to the finish point. The shorter 
the distance to the finishing point, the fitter the individual. I can also use the 
weighted sum to improve the solutions. The lower the weighted sum is, the 
more fit the individual is considered to be.
Decision-Making 

A Final Note on Decision-Making
Some decision problems are known to be undecidable. Don’t expect me to 
solve any of them. Characterizing this class of problems requires some knowl­
edge in computational theory, so we will not go into detail but merely ascer­
tain that there is a class of yes/no decision problems for which there cannot 
exist a computer program that can always give a correct answer.
Furthermore, for some decision problems, there are computer programs 
that may give a correct answer or no answer at all. This class of problems is 
called semi-decidable (decision) problems. For this class of problems, it is pos­
sible to construct a computer program that produces either a correct answer 
or no answer at all. And when the program does not produce an answer, it 
instead runs forever without producing a result.
It has been argued that many decidable problems can be viewed practically 
as semi-decidable. As many computer programs and AI methods require con­
siderable computer memory and computational time, it may happen, of 
course, that it runs out of memory on a given machine or runs for so long that 
the user is not willing to wait for the result. If that is the case, it makes little 
difference to the user if the problem was decidable or semi-decidable. The 
result (or rather the absence of result) is the same to the user anyway.
Ground Truth
Technology is simple, and people are difficult. Sometimes, people create and 
publish a piece of knowledge that has a timing aspect to it. This piece of 
knowledge immediately starts spreading and transforming on the way – this 
is a normal human behavior. Knowledge is there to be spread, of course, but 
there are different ways of doing it. A piece of knowledge with a reference to 
its original source will not give you a piece of that spotlight. In a search for a 
piece of spotlight, people start paraphrasing the original piece of information, 
picking out pieces, adding their own views, and passing it on. This leads to a 
plethora of information pieces out there, with no possibility of backtracking 
to the original knowledge object.
What is the mechanism of retrieving the ground truth that initial knowl­
edge object provided by empirical evidence? An answer to this is linked data.
Instead of copying and passing on a piece of knowledge, we send a refer­
ence to it. This is why I am against sending files via mail – you never know 
which version of the file you are getting. If instead we only share pointers to 
16 
1  Baby Steps

17
knowledge objects, we can choose to always get the latest. The knowledge 
object can by itself evolve as well but keep track of the changes and detect if 
anyone has tempered with it.
9
A tiny tweak in a piece of information along the chain of reasoning may 
lead to an incorrect decision at the end of the reasoning chain. The tiny tweaks 
may be intentional and unintentional. A minor variation of the ground truth 
or an error in the reasoning chain may lead to wrong decisions being made at 
the end of the reasoning process. When this process concerns the life and 
well-being of people, business-critical decision-making, or societal challenges, 
it needs to adhere to certain principles:
•	 Don’t copy data – point to it instead.
•	 Keep track of any tweaks to the original data.
•	 Implement traceability and explainability in decision-making.
•	 Implement backtracking and conflict resolution.
•	 Implement a digital ledger for your data.
9 Factfulness: Ten Reasons We’re Wrong About the World – and Why Things Are Better Than You Think. By 
Hans Rosling, Ola Rosling and Anna Rosling Rönnlund
Ground Truth 

Relying on Commodities
Humans are constantly looking for new values that can be created out of old 
technologies that become more mature and scalable. Processors are powerful 
enough to give us search results in seconds, telecom networks become fast 
enough to transmit huge amounts of information almost instantly, small con­
nected devices are deployed in everything, humans expect mobile Internet to 
work everywhere, and, on top of that, all these things are expected to be for 
free or almost for free. What humans gladly pay for is the “jet-black” shade of 
the phone, fancy filters in the mobile phone camera application, and a digital 
assistant with a sense of humor (like me).
Humans can easily forgive these value-adding services when they misbe­
have – it’s not a big deal if your friendly AI assistant does not get what you 
want from her. However, when humans lose mobile coverage, they become 
very annoyed – or if a laptop suddenly freezes and restarts, especially when 
you are right in the middle of the credit card payment process. Can we trust 
that the microphone is actually muted after we clicked on mute? Can we trust 
that my information will stay secure? A baby AI brain surely needs stability. 
Exactly like with a human child, we all have our talents and strengths and are 
capable of developing fantastic capabilities, if only we have the right environ­
ment – we’ll come back to that in Chap. 2.
Get to Know the Twins
The concept of a digital twin has been around for a while. It is used to 
describe a digital representation of a physical thing on some abstraction level 
that continuously maps to the state of the physical thing.
18 
1  Baby Steps

19
Digital twins are widely used nowadays when we interact with physical 
things such as cars or robots. The beauty of the concept is that you can inter­
act with the digital representation mirroring the behavior of the actual thing 
you want to control. Controlling hardware involves embedded programming, 
adaptors, and protocols, and these are abstracted away for you. Examples 
range from an application for your thermostat to a full representation of a 
manufacturing plant.
Normally, the intelligence of a physical system resides in its controller – 
software that interacts with the hardware, which runs on the device, in the 
cloud, or somewhere in between. A digital twin lacks the physical part but 
represents the same intelligence, and the controller of the digital twin is often 
similar, with the difference that it does not need to care about any specific 
hardware and hence can be simplified.
An AI brain like me often does not have a physical part but can be used in 
controlling physical devices such as cars, robots, or manufacturing plants. A 
digital representation of a robotic arm will be able to tell you its exact posi­
tion, how many times it has performed a certain operation, what the internal 
condition is, what the level of vibrations is, and when it will need mainte­
nance. Depending on the intelligence of its controller, it will be able to tell 
you the risk of its failure and the risk of colliding with another robotic arm in 
the same manufacturing facility.
Hierarchies of digital twins allow a factory twin to keep the current state of 
all the assets in the factory, including the machines, networking equipment, 
supplies, processes, and human workers. A twin of a city will keep an eye on 
all the entities in the city including factories, road infrastructure, people and 
goods logistics, and hospitals. Branching off a version of your digital twin 
would serve you as a very good model to run simulations of what-if sce­
narios on.
Imagine that you want to test a hypothesis, for example, changing an ingre­
dient in your production process or replacing one of your machines. Running 
Relying on Commodities 

a what-if scenario on a branch of a twin would give you a good estimate if you 
should implement the hypothesis in your live system or not.
Looking at the digital natives, I wonder how much physical things they 
care about. They often see more value in digital assets, and do not care much 
about physical stuff. Their rooms are pretty empty, and their most important 
things are devices to access their digital assets: computers, phones, tablets, 
gaming consoles, and VR glasses. Their heroes are digital, and physical things 
are of no interest. Obviously, I am one of those, by construction.
10
10 Lost Eden, https://en.wikipedia.org/wiki/Lost_Eden, accessed 2022-05-23
20 
1  Baby Steps

21
Advantages of Working with Digital Twins
As I described above, a digital twin is a digital representation of something. 
Often, it is a digital representation of a physical object, but in a more general 
sense, it can represent a complex system that may consist of a combination of 
hardware, software, humans, and the environment, such as a production pro­
cess, an industrial robot, a cat, a human, or just an air. Anything that is of 
interest to keep track of, predict changes, optimize, and play around with.
To create a digital twin of something physical, we make use of sensors and 
actuators to tap into the data and control capabilities. Or if it’s a binary twin 
of your smile with the only mission of tracking it, we can make use of cameras 
and serotonin levels in your body or just ask the twin of your teeth if they can 
see the light. In the following, I would like to summarize the reasons for hav­
ing a digital twin and what to use it for.
Your Current State
Your digital twin always reflects your current state. Checking up on your car 
or a fleet of cars, production plant, wind turbine, wine yard, or mining facility 
may not always be easy because of the complex mechanics, physically distrib­
uted things, and hard-to-access locations. In addition, regular health checks 
are not always good enough when you need to be on top of things as they 
happen and to be able to prevent anything unwanted happening. Observability 
is a prerequisite for the successful data-driven management of whatever one 
wants to manage.
What-If Scenarios
You can use your twin for what-if scenarios. First, there were models and 
simulators. Then they evolved to twins. One can do a lot with a model but 
often it’s static and needs to be adjusted from time to time to reflect reality. 
Twins evolve together with reality in a data-driven fashion. Communication 
with the twin is often implemented to be bidirectional, meaning that not only 
the reality makes an effect on the twin but also changes in the twin effect the 
reality, like a voodoo doll. In addition, as much as we all love experiments, we 
normally do them in experimental environments and not in live systems.
A fine property of a digital twin is that at any moment one can take a snap­
shot of the latest state and save it as a model to run experiments on. The 
Advantages of Working with Digital Twins 

classical type of experimentation is what-if scenarios. What if I change an 
ingredient in my production process? What if I decentralize my organization? 
What if I replace a supplier? What would it imply both in the short and 
long run?
 
Photo by Kristopher Allison on Unsplash
Simulations
11
11 https://www.ea.com/games/simcity
22 
1  Baby Steps

23
Property Checks and Decision Support
­
12
13
 
Photo by Stefan Cosma on Unsplash
Abstraction
You can use it to abstract away the details. The beauty of abstraction is that one 
can focus on what’s vital for you. This is obvious when performing an abstrac­
tion of a piece of software. If your level of abstraction is too high, you may miss 
some important properties. If it’s too low, then you are not far from the origi­
nal piece of software and drowning in its complexity – similar to systems that 
are more than just software. If it’s a production plant and you only focus on its 
productivity at any price, you can omit the cost monitoring from your twin. 
Or if you don’t bother about contributions to climate change, you don’t need 
to collect that data either. However, we believe that you do care about both the 
cost and the climate, so let’s make sure we keep focus on them.
12 https://en.wikipedia.org/wiki/Open-world_assumption
13 https://en.wikipedia.org/wiki/Closed-world_assumption
Advantages of Working with Digital Twins 

 
Photo by KOBU Agency on Unsplash
Control of the Physical Twin
You can control the physical twin with the digital twin. As I said before, the 
relationship with the digital twin is bidirectional, like with a voodoo doll, but 
with a positive twist to it. If you have branched out a model of your twin, 
experimented with different what-if scenarios, simulated 10  years ahead, 
checked all the vital properties, and converged on a necessary change in your 
system, often you can implement it though the twin by actuation.
You can, for example, limit the speed of your autonomous trucks to have a 
more positive effect on safety or decrease the temperature of your production 
facility to improve your carbon footprint. In addition, given that you have 
connected supply chains, you can also tweak ingredients in your production 
line or even make upgrades to your hardware. Don’t experiment on your 
workforce though – there I would still recommend human touch.
Rules to Live By
Being the AI influencer I am, I think it’s time I tell you about five of my 
important habits for efficiency. Many great scientists have formed us, Alan 
Turing, John Nash, Adam Smith, Vilfredo Pareto, and more. Here are five 
principles, genius in their simplicity, for my fellow AI brains to be inspired by.
24 
1  Baby Steps

25
 
Photo by Austin Distel on Unsplash
The 80–20 Rule
It is also known as the Pareto principle, even though it was not Vilfredo Pareto 
who coined it but Joseph M. Juran. The principle states that, for many events, 
approximately 80% of the effects come from 20% of the causes. Many varia­
tions and corollaries exist, and some examples are as follows:
•	 20% of the population controls 80% of the wealth.
•	 80% of human workload is executed in 20% of time.
•	 20% of ML algorithms can solve 80% of the business applications of ML.
•	 80% of work is completed by 20% of the project team.
•	 80% of software problems are caused by 20% of bugs.
Recommendation: Consider if you want to be a perfectionist or if good 
enough is sufficient for you so that you can use the remaining 80% of your 
resources on four more high-impact good-enough things.
Rules to Live By 

 
Photo by Vladimir Proskurovskiy on Unsplash
Find Your Blue Ocean
It’s complicated. Adam Smith believed that when each group member acts 
selfishly, pursuing their own interests, it will lead to Pareto optimality of the 
group’s outcome. John Nash has disproved that theory (remember the scene 
with the blondie in Beautiful Mind?). Everyone’s selfish act does not lead to 
Pareto optimality but to Nash equilibrium, a deadlock where overall increased 
gain can only be achieved by decreasing the potential individual gain. Blue 
ocean theory is inspired by this finding. Choose a field (ocean) where you 
don’t have too hard competition (sharks) and create a Pareto optimal solution 
for your customers with lower effort.
 
Photo by Joel Herzog on Unsplash
26 
1  Baby Steps

27
Outrun the Slowest Gazelle
14
15
Leader or Follower?
This is when you think “who wants to be a follower”? It’s actually not such a 
bad idea; it’s a strategic choice. After all, you cannot compete in all sports and 
expect yourself to win in all of them. Pick your favorites, the ones that you are 
good at. Developing a new AI algorithm is difficult, and there are many great 
algorithms off-the-shelf currently. However, if you want to be at the bleeding 
edge of technology and ready to invest resources, you are on the path of 
becoming a leader in whatever domain challenges you are solving with the 
help of AI, not only the AI itself. In addition, who knows, maybe you will be 
the one who finally solves the P versus NP problem.
Summary of Confessions
In this chapter, I have described the background of AI technology and the 
major parts of the AI technology landscape. Some of the main confessions I 
made include the following:
•	 As a modern AI, I am powerful. AI brains can, for example, study all medi­
cal documentation ever created or become a chess master capable of beating 
any human in the world or become the best Go player on the planet, just by 
playing against themselves and developing new winning strategies that 
humans could not discover for 3000 years.
•	 AI babies start with processing data before they learn to digest more com­
plex structures such as ontologies and state machines. Without data, we 
cannot evolve and become intelligent creatures.
•	 Semantically annotated cleaned and structured data is called information. 
Semantically linked information is called knowledge.
14 https://www.goodreads.com/quotes/292417-every-morning-in-africa-a-gazelle-wakes-up-it-knows
15 https://www.goodreads.com/author/show/133538.Christopher_McDougall
Summary of Confessions 

•	 AI brain should avoid biased data, dirty data, data without metadata, non­
representative data, sensitive data, and ambiguous data.
•	 I learn from data in different ways: In supervised learning, I learn under 
human supervision from examples (also known as training sets, labeled 
data). In unsupervised learning, I am left on my own to figure out patterns 
and group data points that I see. In reinforcement learning, I try to make a 
guess, and an observer rewards me for good guesses and punishes me for 
wrong guesses.
•	 I can also learn together with other AIs: In federated learning, all participat­
ing AI algorithms learn a similar thing separately, and after that, they meet 
and exchange their experiences. In transfer learning, if my AI friend has 
spent lots of time on learning something, then these learnings can be shared 
with other AI brains.
•	 As an AI, I can support or make decisions and deal with, for example, opti­
mization and planning.
•	 What is the mechanism of retrieving the ground truth that initial knowl­
edge object provided by empirical evidence? An answer to this is linked 
data. Instead of copying and passing on a piece of knowledge, we send a 
reference to it.
•	 The concept of a digital twin has been around for a while. It is used to 
describe a digital representation of a physical thing on some abstraction 
level that continuously maps to the state of the physical thing.
28 
1  Baby Steps

29
2
Basic Needs of an AI Brain
As a responsible parent, you have the responsibility to help your AI brainchild 
grow in a complex world and help it achieve its full potential. In this book, I 
often draw parallels between humans and machines, as the thought process of 
the latter is inspired by the thought process of the former. Therefore, to under­
stand what constituents contribute to the successful upbringing of the AI 
brain, it is perhaps pertinent to study relevant literature from the human side.
­
1
Similar to Maslow’s approach, an AI brain also has a hierarchy of needs that 
compound in order for the brain to reach its full potential (see the figure 
below): planning on the data to use, providing ample compute resources, 
facilitating communication, making sure that data is treated privately and 
that the environment is safe, and eventually building trust with humans. 
These are all essential parts of an AI brain’s proper upbringing.
1 Maslow, A. H. (1943). A theory of human motivation. Psychological Review, 50(4), 370–396. https://
doi.org/10.1037/h0054346
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_2

 
AI brain’s hierarchy of needs
Data
Data can be likened to nutrition for your AI brain: good data can facilitate 
growth and later success, whereas bad data can hinder achievement of true 
potential. Even though “good” and “bad” assessment is always contextual and 
dependent on the type of problems the AI brain is called to solve, there are 
some universal principles for proper nurturing, formally falling under the 
umbrella term of data ­quality.
 
30 
2  Basic Needs of an AI Brain

Complete Data
First, the data must be complete. This means that there should not be any miss­
ing information (“gaps”) that could lead to incorrect decisions. Data com­
pleteness is a challenge, especially if it is collected from different administrative 
domains.
For example, consider a dataset that measures ambient temperature, 
humidity, and noise levels. This dataset could be used to identify whether a 
specific area, such as an area within a building, has ideal working conditions 
for humans. When considering using such a system on a large scale, data from 
multiple rooms need to be collected. However, as different buildings may 
have different types of sensors and/or maintenance policies, some data may 
include only part of the information, for example, only temperature or humid­
ity. Using such data to train your AI brain may mean that certain assumptions 
need to be made for missing information that can later lead to wrong deci­
sions. Therefore, establishing processes for ensuring data quality is paramount. 
Such processes may include reactive measures such as the introduction of data 
profiling techniques that help detect gaps in data. Proactively, instructions for 
data collection can be established, requiring the presence of important infor­
mation in the data.
Accurate Data
Second, the data must be accurate. Data accuracy is a measure of how close the 
information contained in the data is to reality. Considering our previous 
example, a temperature reported as 25 degrees Celsius, while in reality it is 21 
degrees, is inaccurate. Data accuracy is usually an issue with the source.
In the case of our example, a miscalibrated or malfunctioning sensor may 
misreport the temperature at least some of the time. The establishment of data 
accessibility instructions including auditing of the data provider as well as 
clear instructions for data entry for data providers is a way to reduce the pres­
ence of inaccurate information in data.
Data Consistency
Data consistency is another aspect that must be considered. Consistency is 
dimensioned by two qualities, format and content. Format refers to how the 
data is represented, while content refers to the metric, such as unit of 
Data  31

measurement, key performance indicator, or others, which is used to measure 
the quantity or quality represented by the data.
Back to the example, temperature can be described using raw text or a more 
structured format such as the popular JavaScript Object Notation (JSON) or 
an eXtensible Markup Language (XML)-based format. On the other hand, 
temperature can be measured, for example, in units Kelvin or units Celsius. 
As is the case with data completeness, data consistency is particularly impor­
tant for datasets originating from different points of origin not necessarily 
related to each other. In such cases, the establishment of processes that specify 
the format and metric to be used for every of the potentially many values 
reported from a distributed set of providers is paramount. These processes 
may involve documentation, human intervention, and increasingly the use of 
software tools that can automatically translate from one format to another 
and detect inconsistencies in the type of metric used among multiple data 
providers. While not obvious at early stages of data collection, lack of data 
consistency may incur a huge cost later; therefore, it is important that proper 
measures are taken a priori.
 
Timeliness of Data
Timeliness is a dimension that refers to the degree to which data represent real­
ity at a specific point in time. Chronological information is important for a 
broad set of applications. When combined with data values, it can help AI 
understand and predict the occurrence and cause of patterns of change.
For example, an early identification of temperature climbing with a rate 
greater than what was historically considered “normal” may reveal a potential 
fire hazard. In another dimension, timeliness also means that data is delivered 
within the expected time boundaries. This aspect primarily concerns use cases 
and applications where AI requires access to real-time information to make a 
decision.
32 
2  Basic Needs of an AI Brain

 
Data Governance, Legitimacy, and Accessibility
In addition to the above, there exist other properties of data that need to be 
considered on a case-by-case basis.
For example, wide-scale applications will want to consider data governance 
aspects such as laws for sharing data between organizations and countries but 
also build their own legal framework to protect their own.
Data legitimacy is the degree to which the data provided is trusted or not – 
it is not to be confused with accuracy, as there can be cases where data is 
accurate but not legitimate. Not only the auditing of data providers but also 
the establishment processes and deploying tools for secure data storage and 
retrieval are ways to bolster legitimacy.
Finally, data accessibility is another dimension that considers how usable the 
data is, i.e., whether it can be used with little effort by parties who have little 
to no experience working with the particular type of data. Data visualization 
tools such as dashboards are one way to make the data more ­accessible.
 
Data  33

Metadata
Analysis of data without metadata is similar to walking in a forest at night. 
You know that something beautiful is out there, but you just cannot find it or 
make sense of it. Simply put, metadata is information describing the data. 
There exist different categories of metadata, each describing a set of different 
properties:
•	 Descriptive metadata describe properties such as the quality or quantity a 
particular piece of data measures or describes; the owner or creator of the 
data, as well as societal, environmental, or organizational aspects that give 
data, may address or be applicable to. Additionally, descriptions may 
include properties of the datastream such as its periodicity, expected life­
time, and cost. Finally, accessibility and usability information may also be 
part of descriptive metadata.
•	 Structural metadata describe how different pieces of data or components 
within a single piece of data relate to each other. They also describe the 
structure of the data. For example, considering the temperature and humid­
ity example, structural metadata could describe how temperature and 
humidity are stored in the database and how they are related (e.g., they can 
both be columns under an “environment qualities” table).
•	 Administrative metadata provides information with regard to the manage­
ment of a data resource. Ownership, sharing policy and permission, time 
of creation, and time of last update are some examples.
Metadata has many uses, among which are as follows:
•	 Cataloging and indexing of data, so they can easily be searched upon and 
retrieved by interested parties. This greatly increases the usefulness of a 
dataset, as other interested parties from the creator can search and reuse it.
•	 Facilitating interoperability by accelerating integration of data sources in 
existing AI systems or systems under development.
•	 Increasing trust in data and helping grow the AI ecosystem. Well-­
documented datasets are always popular among researchers and industry 
practitioners as they are used for training and/or benchmarking purposes.
•	 Where needed, helping generate data, also known as “synthetic data,” to 
augment datasets with less than perfect availability. In this case, statistical 
metadata can be reported for the dataset, describing, for example, its input 
and output probability distribution, which would help generate accurate 
synthetic data.
34 
2  Basic Needs of an AI Brain

Compute and Store
A well-developed brain also needs some strong muscles to support it. Cloud 
computing technology is in this case a natural complement to computation­
ally demanding AI algorithms. On the one hand, it democratizes access to the 
development of AI as individuals and organizations on a budget can rent 
cloud resources that are otherwise prohibitively expensive to purchase and 
own. On the other hand, it allows for flexible deployment across different 
geographical areas to address quality of service requirements for each particu­
lar application.
When it comes to cloud computing in general and machine learning in 
particular, there exist different tiers of services that include different types of 
offerings that address the needs of a wide clientele of different scales and expe­
riences with AI technologies:
•	 First, infrastructure as a service (IaaS) offerings provide on-demand virtual­
ization of compute and store resources, but it is up to the customer to set 
up their operating system and AI development and execution environment.
•	 Second, platform as a service (PaaS) offerings provide tailor-made solutions 
for machine learning that include all tools required to get started with 
development or execution, thus saving a significant amount of time for 
users who do not need to focus on setting up their environment.
•	 Third, software as a service (SaaS) offerings provide managed services to 
users, i.e., already trained AI algorithms that users can use in their software. 
Examples include computer vision models, which given input images iden­
tify the objects in the images and their position, text-to-speech and speech-­
to-­text models, and recommendation engines.
Smaller organizations that do not have the resources to spend on custom­
izing their working environment, or organizations that lack the expertise in 
AI, may choose higher-tier services such as those belonging to PaaS and SaaS 
to the expense of greater customizability and optimization of IaaS.
Orthogonal to these services, which address the “what” type of service 
should be deployed, there exist different cloud computing architectures that 
address the “where.” Edge clouds, for example, reside close to the users of the 
applications and/or the data sources to reduce the data transport latency. 
Centralized clouds on the other hand reside in a central datacenter, aggregating 
all traffic from users. The latter have the advantage of lower cost and process­
ing capacity to the expense of higher delays of data transport from and toward 
Compute and Store  35

the users. Recently, satellite clouds have also started to surface, driven by the 
cheaper cost of access to space and combining the benefits of the global reach­
ability of centralized clouds with the low latency of edge clouds.
Privacy
“Imagine no possessions
I wonder if you can
No need for greed or hunger
A brotherhood of man
Imagine all the people sharing all the world…
You may say I’m a dreamer…”
Dear John Lennon, yes, you are a dreamer. Examples of AI-driven innova­
tions are commonplace today and are there to be cherished and protected as 
they drive progress. Given their material value however, such innovations also 
constitute intellectual property that organizations may not want to or be 
allowed to share with the whole world. Therefore, privacy as means of seclud­
ing information used within AI applications from a group of entities such as 
other organizations or individuals should be an integral part of your AI brain 
curriculum. In fact, when considering the broad field of AI, privacy is an 
umbrella term that covers different aspects.
Data privacy is about protecting sources of information that your AI brain 
relies on to make decisions. In the case of machine learning, information is 
training data used to create models that can later provide decisions given new, 
previously unseen data. In the case of reasoning and planning, information is 
contextual data organized in a knowledge base that algorithms use to create 
new information and offer insights.
 
36 
2  Basic Needs of an AI Brain

Regardless of the type of AI technology used, protecting the identity of the 
data provider during data collection is not limited to an organization, but in 
many cases it may have larger societal implications. A typical approach is the 
process of data anonymization, where data is stripped of information that 
may reveal the identity of the provider. Several anonymization techniques 
already exist, including the following:
•	 Generalization, wherein actual values in the data are replaced with less spe­
cific but semantically consistent values. For example, instead of reporting a 
specific temperature value, a range of values can be reported (e.g., 20–30 
degrees Celsius instead of 25).
•	 Perturbation techniques which slightly modify the original values by add­
ing noise. To avoid reducing the utility of the dataset, the noisy shift in 
values should be proportional to the original value. From an implementa­
tion perspective, the noisy shift can be realized in the form of rounding 
numbers to a value. For example, outdoor temperatures that typically range 
from minus a few degrees to plus 40 degrees can be rounded using a base 
of 5, wherein counts ending in 1 and 2 are replaced by 0 and counts of 3 
and 4 are replaced by 5. However, another value range that measures to 
thousands, such as lumens which indicates brightness and a base of 15 or 
more, may be used for rounding.
•	 Suppression is an approach where certain information is removed from the 
data. Suppressing data is an exercise in balancing the utility of the dataset 
and protecting the anonymity of the data source.
•	 In pseudonymization, information in data that may reveal the identity of 
the data origin is replaced by other artificial identifiers such as pseudonyms. 
A pseudonym may still maintain semantic relevance with the original value 
while at the same time not revealing the data owner’s identity.
Security
Security is about protecting your AI brain’s assets such as data, AI algorithms, 
machine learning models, and insights from unauthorized access. Loss of 
intellectual property is not the only concern in the case of a security breach, 
as malicious users may tamper with the assets, decreasing the overall effective­
ness of the AI brain.
Data security is perhaps the most important aspect of AI security, as it is 
data that drives the creation of models and implicitly the generation of insights 
from these models.
Security  37

There are three main elements to data security, namely, confidentiality, 
integrity, and availability, otherwise known as the CIA triad. Confidentiality 
pertains to authorized data access, i.e., ensuring that data is only created, 
modified, read, or removed by authorized users. Integrity ensures that data is 
not modified during or after collection. Availability means that data is readily 
available and can be safely retrieved by authorized users at any point in time.
 
To realize the CIA triad, a combination of different techniques is used. The 
most well-known of these techniques are summarized below:
•	 Authentication mechanisms identify eligible users before they access the 
data. The simplest authentication is password-based authentication which 
requires an alphanumeric known as password to be provided by the user 
prior to data access. It is also one of the weakest authentication methods, as 
passwords are prone to phishing attacks and can be compromised easily.
•	 A more secure authentication method is multifactor authentication (MFA), 
which uses two or more independent ways to authenticate a user, for exam­
ple, combining password authentication with tokens generated from a 
separate device (e.g., a mobile phone) or voice or facial recognition. Digital 
certificates can also be combined with password authentication and are 
another way for users to authenticate.
•	 A digital certificate is an identity document sent from the user and is used 
to authenticate the user’s identity. For multiple transactions, token-based 
authentication can also be used in combination with password-based 
authentication. In this case, users enter their credentials (i.e., username and 
password) once and receive back a token – an alphanumeric that they can 
use in subsequent requests and until the token’s expiration time to authen­
ticate. This negates the need for providing credentials for every request.
•	 Making backups of collected data is a good practice for security reasons not 
only in cases of a breach but also in case of a malfunction of the data stor­
38 
2  Basic Needs of an AI Brain

age infrastructure, due to, for example, a natural disaster, a power failure, a 
human error, etc. There are several ways of taking data backups, for exam­
ple, using redundant storage drives such as hard disk drives and solid-state 
drives or using a third-party cloud. An important aspect of a backup system 
is resiliency, i.e., the seamless and early recovery from a breach or 
malfunction.
•	 Data encryption means encoding of information in a format that appears 
scrambled and unreadable to users who try to access it without permission. 
Symmetric encryption uses a cryptographic key to encode and decode the 
information. In this case, users use the same key to decrypt and read the 
data as the key used to encrypt it. Asymmetric encryption uses a public and 
private key. The public key can be used by any entity to encrypt the data; 
however, this data can only be decrypted by authorized users having the 
private key.
In addition to the techniques above, several policies and regulations can be 
established to proactively or reactively address security issues in data. The 
establishment of policies for incident reporting and “emergency response” 
teams can help detect and respond to data breaches quickly before they esca­
late. Finally, compliance with existing data security frameworks such as the 
European Union’s General Data Protection Regulation (GDPR) and 
California Consumer Protection Act (CCPA) provides a structured way to 
establish strong security policies.
 
Security threats also affect machine learning models before or during the 
training process or after the training process has finished. During training or 
before training commences, poisoning of the training dataset with inaccurate 
data is one of the most common threats.
Another threat during training would be modifying the loss function return 
result to never show any improvement, thus tricking the model into an 
Security  39

infinite loop. The loss function is used to indicate how close the prediction of 
a model is to reality. Ensuring that data collection and training are done in a 
controlled environment and not a third party, unmanaged environment is one 
of the best preparatory tasks you can do for your AI brainchild. Several afore­
mentioned data security techniques can also be used during training, for 
example, dataset encryption.
Distributed Learning
Human society is all about diversity. It’s about taking all opinions into consid­
eration – about linking them together and helping each other make sense of 
things. Not everyone speaks the same language. Not everyone has the same 
cultural background. Sometimes, people may use different terms when we 
talk about the same thing. Federation is the process by which the independent 
learning of people translates to the collective learning of the society.
A typical example of federation is company meetings geared toward estab­
lishing a common understanding, for example, shaping the next year’s strat­
egy or defining the system architecture for the next company’s software 
product. In such meetings, different opinions are expressed by people, but 
ultimately, the result reflects learning from all opinions expressed.
 
Similarly, your AI brain can benefit immensely from experiences of other 
AI brains. This is because as a parent, even if you want the best for your AI 
brainchild, you are still limited by a constrained perspective of your own 
“worldview.”
For example, when it comes to AI algorithms using machine learning, the 
training datasets may not contain data for all corner cases that may happen in 
reality, resulting in a model that cannot generalize well. In another example, 
in cases where reasoning and planning techniques use knowledge bases of 
40 
2  Basic Needs of an AI Brain

information, the breadth of this information may not describe all aspects that 
are useful to the relevant problem. In both cases, combining learning and 
information from other similar approaches may help strengthen the perfor­
mance of the AI algorithm.
From a machine learning perspective, described in the previous chapter, 
there exist techniques falling under the umbrella term of federated learning 
that allow for the decentralized training of models. Specifically, local nodes 
train local models, which are later aggregated into a global model in a central­
ized node. The centralized node sends back the global model to the local 
nodes for another training round. The process repeats until the global model 
presents an acceptable performance. During the process, training datasets are 
not exchanged between the local nodes and the centralized node, thus pre­
serving data privacy.
 
Another important aspect beyond federation of learning process is federa­
tion of insights. As insights, we consider the output of a model (i.e., its “pre­
diction”). In current practice, insights are considered in isolation and used as 
input to other systems for further processing or presented to humans for 
assessment. However, as the use of machine learning models becomes more 
widespread, recent research in machine learning, such as the field of neuro­
symbolic AI, has started looking into the federation of insights. In such sce­
narios, insights from multiple models are used to create new insights.
Let us consider, for example, the temperature and humidity dataset and 
assume that there is a model based on the temperature and humidity that 
predicts the probability of fire in an area. Such a model can have many false 
positives, as the increase in temperature and humidity may not necessarily 
mean that there is a fire. It can be, for example, that this increase is due to an 
unusually hot day.
However, if the output of this model is to be combined with a model that 
uses audio fingerprinting to predict the probability that the sound being 
Distributed Learning  41

captured from a nearby microphone is that of a fire, then the combination of 
these two insights could be a stronger indicator.
Similarly, if the sound being captured is that of a malfunctioning machine, 
then the insight could indicate that the dehumidifier and/or air-conditioning 
machine is malfunctioning.
From a reasoning and planning perspective, linked data principles allow 
the knowledge bases of different nodes to link to each other and share data. In 
this way, information that can be used by AI algorithms does not need to 
reside in a specific node, but can be distributed over many nodes, forming an 
information collective.
Federation of AI Algorithms
Thus far, we have described how federation of learning processes and informa­
tion models can be used to enhance the performance of AI algorithms. It 
turns out that the concept of federation can be taken one step further to the 
federation of work. In this case, the federation concerns the insights of the 
models (i.e., their predictions) and is agnostic to the algorithm used to make 
that prediction. Such an approach allows us to take advantage of different AI 
algorithms that may not have the same structure but share the same input and 
output space.
This approach is known as ensemble learning, wherein various techniques 
exist that use predictions from different models to produce an informed global 
prediction. Techniques such as bagging and boosting train different algo­
rithms and average their collective predictions to produce a more accurate 
global prediction.
Split Learning
A new technique developed at the MIT media lab, i.e., split learning, divides 
a neural network into segments across multiple hosts. This type of neural 
network is known as splitNN. In splitNN, every host has a subset of the neu­
ral network layers. During training, a host trains its layers as a self-contained 
network, up to a layer known as a “cut” layer. The outputs of this cut layer are 
sent to another host that trains its own layers without having access to the 
data of the former host (thus preserving privacy). In this case, the data is the 
input data to the model. In the backpropagation process of the training, the 
gradients are sent back from the last layer of the latter host to the cut layer of 
42 
2  Basic Needs of an AI Brain

the former host. Again, this process is done without the former host having 
access to the data of the latter host (i.e., the labels).
Human Oversight and Trust
As much as fully autonomous systems enabled by an AI brain are an enticing 
concept, they should not undermine human autonomy and authority. After 
all, the whole purpose is to make life better for humans. An AI with a human 
touch should be inspired and guided by humans and run on their terms. It 
can run autonomously for a while but should be checked in upon, from time 
to time or when it is necessary.
 
For human oversight to be effective, AI algorithms need to develop a recip­
rocal relationship of trust with their human peers. Trustworthy AI, an ensem­
ble of approaches for AI system design that takes into account the human 
element (human in the loop), offers a path toward achieving this trust.
2
AI safety is another important aspect for trust. It is an umbrella term that 
defines safeguards built into AI systems to avoid unintended behavior that 
may cause harm to other systems or living beings. Robustness is one of the 
aspects that need to be addressed under the AI safety umbrella. Specifically, a 
robust AI algorithm will perform well regardless of whether data similar to the 
test dataset used for training the algorithm is used, or other data that are still 
valid input, but do not belong to the test dataset. Non-robust AI algorithms 
may be prone to adversarial attacks. In such attacks, input data is slightly 
modified, to cause an AI algorithm to generate incorrect insights (e.g., in the 
case of a classification algorithm, it may misclassify input data). Such 
2 I discuss trustworthy AI, explainability, and safety in more details in Chap. 6.
Human Oversight and Trust  43

modifications may be so subtle that they are not noticeable by human observ­
ers. Biased algorithms may also generate incorrect insights from input data, 
because the training dataset may be not representative of the breadth of data 
that may be input to the algorithm during operation.
Another aspect that may be considered during the operational phase could 
be a fallback mechanism wherein, in the case of an AI algorithm that consis­
tently performs poorly, the system could fall back to another approach until 
the AI algorithm is troubleshooted and improved by human experts.
Putting It All Together: Nurturing of AI Brain
An AI brain exhibits cognitive abilities similar to those of the human brain: 
perceiving, reasoning, evaluating, and deciding are all insight-generating abil­
ities that depend on how well the brain can learn and use information from 
environmental stimuli. Learning is the process acting as conduit, using infor­
mation as input, to enable the brain to perform the aforementioned cognitive 
abilities. In the case of AI learning, these stimuli are provided to the brain’s 
machine learning algorithms as machine-readable data. For the AI brain to 
reach its true potential, this chapter outlined several principles that should be 
considered by the brain’s custodians.
Collecting complete and accurate data in a consistent and timely manner is 
required for a successful learning process, which is the foundation for the 
brain to exhibit its other cognitive abilities. In this process of data collection, 
metadata is fundamental in identifying what data is important and whether it 
can be legally accessed and collected.
Choosing the proper network and computer infrastructure for conducting 
these cognitive abilities is also an important aspect, a balancing act between 
requirements on the AI brain’s performance and the location and cost of this 
infrastructure.
Privacy and security are important aspects that help maintain data provided 
to and used by the AI brain private and discourage malicious users and prying 
eyes away from accessing inner workings of the brain, respectively.
“Two heads are better than one, not because either is infallible, but because 
they are unlikely to go wrong in the same direction.” This quote from influ­
ential writer C.S. Lewis indicates the importance of community, even when it 
comes to AI brains. Federation of learning and AI algorithm execution pro­
cesses are techniques that enable tapping into and leveraging the collective 
experience of other AI brains, thus contributing to better overall 
performance.
44 
2  Basic Needs of an AI Brain

Last but not least, trust is an important element, as AI brains do not func­
tion in a siloed, secluded environment but instead form a symbiosis with 
human brains. This symbiosis is based on reciprocity, where opinions and 
information are exchanged and decisions are often taken collectively. The 
catalyst for a successful collaboration is trust, as brains need to trust each 
other but do not necessarily understand each other a priori. Safety and 
explainability are two important aspects that the AI brain can demonstrate 
toward the human brain and for the latter to establish trust for the former.
 
Γηράσκω δ’ ἀεὶ πολλὰ διδασκόμενος, liberally translated as “I grow old 
learning many things,” is a quote attributed to ancient Greek statesman and 
lawmaker Solon the Athenian. This is true for your AI brain as well. While it 
never grows “old” in the human sense, the learning process is never finite but 
rather ever evolving. Keeping the principles outlined in this chapter in mind 
will allow you, the mentor of the AI brain, to provide the best possible envi­
ronment for growth.
Summary of Confessions
In this chapter, I have explained what an AI brain needs in order to learn and 
become useful. Some of the key confessions include:
•	 AI requires data that is complete, accurate, and consistent and has chrono­
logical information (timeliness). Other properties such as governance, 
legitimacy, and accessibility are essential for data to be useful.
•	 AI needs metadata to make sense of data. Metadata is simply information 
describing the data. There exist different categories of metadata, each 
describing a set of different properties, such as descriptive, structural, and 
administrative. Metadata is essential for cataloging, facilitating interopera­
Summary of Confessions  45

bility by accelerating the integration of data sources, increasing trust in 
data, and helping grow the AI ecosystem.
•	 AI needs computing power. Often, cloud computing is used. In cloud com­
puting and machine learning, there exist different tiers of services that 
include different types of offerings, including Infrastructure as a service, 
platform as a service, and software as a service.
•	 Data privacy is important in AI! It is about protecting sources of informa­
tion that your AI brain relies on to make decisions. In the case of machine 
learning, information is training data used to create models.
•	 Security is about protecting your AI brain’s assets such as data, AI algo­
rithms, machine learning models, and insights from unauthorized access.
•	 Distributed and federated learning is the process where independent learn­
ing is linked together to collective learning – a powerful technique used in 
society by humans and in computer networks by AI algorithms.
46 
2  Basic Needs of an AI Brain

47
3
My Role in the Internet of Things
The Power of IoT: A Personal Story
Let me tell you about the Internet of Things (IoT) – the trend of physical 
things becoming connected and available for developers to build services 
using them.
For example, when staying at this nice hotel at Times Square called 
CitizenM, the first thing you may do when entering the room is to search for 
a regulator somewhere on the wall to increase the temperature in the room. 
You fail at doing that, but shortly after, you discover an iPad with an applica­
tion to control everything in the room – curtains, blinds, lights, TV, A/C, and 
even the color of the lighting in the bathroom. There are also modes you can 
choose between romance, business, party, etc. that make the whole room 
change its character. How cool is that? Some years ago, we enjoyed the 
Emergency Party Button videos on YouTube, and now it has actually been 
implemented commercially.
Another cool example is carpooling – a service where you can grab and 
drop off a smart car anywhere in a city that you control from an application 
on your phone. In addition, if something is not working for you – just check 
that application – it has the answer. The state of all the car sensors is moni­
tored by the cloud application, and if you try to end your rental while the 
trunk is open or when the car is parked in a forbidden area, the application 
will complain.
And what about the stage for the Eurovision Song Contest? Måns 
Zelmerlöw’s performance in 2015 started a trend, and in the years after every 
second, artist used cool animations on stage. Did they get an API for that?
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_3

Most interesting innovations will come with cross-domain interactions. 
The IoT breaks the verticals and allows application developers to build their 
services across domains, which in its turn opens up many new business values. 
For example, is it possible that the mood that you choose for your hotel room 
stretches to the car that you rent?
 
Spaghetti code was a popular term in the 1990s, for low-quality code. 
When a tool is given to someone who does not plan for dimensioning, scal­
ability, and extensibility, the end result becomes spaghetti. The authors of this 
book also created a couple of spaghetti projects, with all their love of quick 
fixes and limited time to hand in assignments when at university. Spaghetti 
code requires a shorter starting curve but is not future-proof.
Similarly, “spaghetti’‘ can easily be created when Internet of Things solu­
tions are put together. A couple of years ago, a team at Ericsson Research cre­
ated a prototype of a fully automated logistics system that consisted of some 
100 sensors and actuators and used a control algorithm that could deal with 
any number of resources (vehicles, cranes, cargo) and adjusted to new logistics 
tasks and resource changes in real time. There was no spaghetti code there. In 
addition, there was a very beautiful demo that we presented at Mobile World 
Congress 2015. Underneath, however, was a “spaghetti” of cables, simply 
because not all of our devices were capable of wireless connectivity at that time.
Another example could be found in a smart home space, where there is no 
need for cables anymore, as several manufacturers are offering platforms for 
wireless devices connecting to the cloud. However, there is still a risk of creat­
ing virtual spaghetti. Ordinary users of such platforms, provided by compa­
nies such as Apple or Telldus, can now easily automate their homes without 
any knowledge of the underlying technologies as all the complexity is hidden. 
The invisible cables will however connect our fridges, TV boxes, coffee 
machines, and temperature sensors to different servers: when, for example, 
you ask your smart TV from vendor A to check if your fridge from vendor B 
48 
3  My Role in the Internet of Things

is out of milk, your TV will send a request to a cloud server somewhere in the 
world to decode what you actually meant by this phrase, get back your request 
in a machine-readable form, send it to vendor A application server, back 
home, then to vendor B server somewhere in the world, back home to the 
fridge where a sensor detects whether there is any milk, and all the way back 
to your TV to deliver the answer to your question. It is no wonder that 
responses can take time. The longer the chain of events involving devices from 
different vendors, the longer the response times. With intelligent IoT, there is 
no need for such long feedback loops. Instead, the involved devices could talk 
between themselves and deliver back an answer.
 
Introduction to Intelligent IoT
Formally, the “Internet of Things” or IoT for short is an umbrella term for 
applications, protocols, and technologies that allow physical objects to be dis­
covered, addressed, and interacted with over the Internet. In the IoT, connec­
tivity and data are omnipresent, which coincidentally and as described in 
previous chapters are the basic constituents for AI. In the case of IoT, AI is not 
located centrally in a supercomputer, having answers to all questions includ­
ing the meaning of life. Instead, it is decentralized, shared by, learned from, 
and being present in connected objects.
The IoT is the conduit for AI to be realized at a scale that transcends orga­
nizational barriers and affects society at large. In fact, different AI applications 
can be connected together to form larger ecosystems of compound knowledge 
and enhanced decision-making, in what is known as the “intelligent IoT.” For 
example, wearables such as smartwatches and smart streetlights that give pri­
ority only when needed and autonomous buses can be linked together to offer 
new types of applications in a smart transportation context, such as on-­
demand public transport, replacing scheduled public transport. This smart 
Introduction to Intelligent IoT  49

transportation application can also be part of a greater “smart city” applica­
tion, where inhabitants enjoy more efficient, sustainable services. This inter­
connectedness allows for the emergence of the network effect, wherein the 
larger the number of interconnections between devices and applications, the 
greater the additive value.
In this chapter, I will discuss several aspects of interconnectedness and dis­
tributed intelligence. For AI to be successful in the IoT, low-cost, embedded 
devices on the network edge, such as sensors and actuators, need to be able to 
support ­computationally and data-intensive AI algorithms. Therefore, the 
aspect of feasibility is addressed up front, with a review of advances in Lean AI 
and capabilities of the latest generation of sensors and actuators.
 
Next, I discuss the aspect of trust in interconnectedness between applica­
tions, specifically the aspect of collaboration between devices that do not nec­
essarily trust each other a priori and how technologies such as distributed 
ledgers and explainable AI (XAI) help establish trust.
Subsequently, I address issues of scalability of decentralized intelligence and 
the role of public cloud platforms. Finally, the chapter ends by discussing digi­
tal twins, digital replicas of the real world that are realized by AI and IoT, and 
their multiple uses.
Divide et Impera
Divide et Impera or divide and conquer is attributed to the Macedonian king 
of old Philip II (and many others after him). While Philip determined that 
this rule was applicable to conquering other Greek city-states and growing his 
empire, the same principle can also be applied to conquer the scalability chal­
lenge in computer systems, from designing efficient algorithms to solve con­
ceptually difficult problems to designing scalable cloud computing 
architectures for big data solutions.
50 
3  My Role in the Internet of Things

The same rule applies for AI in the IoT.  Distributing data acquisition, 
learning, and decision-making processes results in more sustainable scaling 
and growth to the scale that IoT applications demand. Computation is not 
the only reason for this distributed approach. Local regulations may prevent 
data from being transported away from a certain geographical area in what is 
known as “geofencing.” These rules may also apply to model training and 
execution. Additionally, certain types of the so-called “mission-critical” IoT 
applications may have strict requirements for latency that simply do not allow 
for the data-providing and decision-making components to be far away from 
each other. Therefore, to support IoT applications at scale, AI algorithms need 
to be deployed on the edge or be on a distributed nature, learning and decid­
ing from data that is provided locally, rather than being placed on a central­
ized datacenter.
In this section, I will cover both infrastructure and algorithmic aspects of 
distributed and edge AI. A general trend in the industry is that devices are 
becoming increasingly computationally capable. For example, tinyML is an 
umbrella term of architectures and tools that are capable of performing on-­
device AI at mW power range or below. In contrast to traditional ML, which 
consumes much more power, tinyML is suited for battery-operated, resource-­
constrained devices, focusing on real-time data of the physical world. The 
hardware is typically microcontroller units (MCUs) with hardware accelera­
tion, on top of which lightweight versions of neural networks such as CNN 
micro are run. Typically, tinyML models generate insights from devices sens­
ing different types of modalities, ranging from audio, video, temperature, and 
humidity to gas concentration and room occupancy.
The insights generated from tinyML models can be standalone; however, 
they are typically part of a larger collection of nodes implementing a particu­
lar IoT application. Let us consider an example of a smart traffic jam manage­
ment system. The idea is that several sensors placed on roads can predict using 
classification models, e.g., whether a traffic jam is likely to occur in the next 
hour. The input to the models is the current rate of traffic (e.g., number of 
vehicles per minute), the time of the day, and the day of the week. By itself, 
the device reporting the likely traffic jam cannot take any meaningful action. 
However, a node that receives input from many such devices in an area, e.g., 
of a city, can do some processing from predictions of many traffic detectors 
and change the timings of the traffic lights to prevent or lessen the magnitude 
of the traffic jam. In such a scenario, an edge node may use device predictions 
to produce meaningful insight and actuate it in the road network of the smart 
city. Having this node centralized and devices do individual predictions not 
only saves data transport costs but also preserves privacy, as device readings are 
Divide et Impera  51

never communicated. Such functionality is responsible for performing com­
putations for a specific area where multiple data providers (e.g., sensors) may 
be located and is known to exist at the “edge,” either at a cloud datacenter 
(“edge cloud”) or a standalone server (“edge node”). Higher-layer abstractions 
can also be possible. Considering our smart traffic example, a public cloud 
server collecting information from edge clouds can provide other types of 
actions on a city-wide scale, for example, rerouting traffic from other roads in 
the city using smart signs.
As illustrated in the figure below, in general, there are several different lay­
ers of infrastructure for AI that trade off communication latency with com­
pute power. AI applications are distributed in terms of computation; certain 
processing takes place within devices themselves, while other processing takes 
place in a more central location.
 
Trade-off between compute power and latency in AI infrastructure
In the state of the art, multiple approaches exist for transferring AI-generated 
insights between different layers. Multi-agent systems consist of a number of 
intelligent agents that collaboratively learn and interact within an environ­
ment. Several prominent areas have emerged within multi-agent systems for 
collaborative learning. Transfer learning techniques such as federated learning 
can be used to train generalized machine learning algorithms at a centralized 
location, by combining locally trained algorithms in edge devices, without 
sharing of private data. Another group of techniques known as ensemble 
learning combine learning from multiple locally trained algorithms to reach a 
decision that can be used for later computation by other algorithms at another 
52 
3  My Role in the Internet of Things

infrastructure layer. Multi-agent reinforcement learning (RL) considers agents 
that learn to take actions against an environment autonomously and are 
incentivized by a reward that is either common (cooperative RL) or individual 
to each agent (noncooperative RL). Multi-agent RL is ideal for environments 
where data is not available to agents a priori for training, as is the case, for 
example, in federated learning or ensemble learning.
Finally, another area that has been gaining traction recently is that of neu­
rosymbolic AI. In this area, the statistical output of models is converted into 
machine-readable symbolic representations that can be used for further pro­
cessing (i.e., not only learning but also reasoning or planning). This formal­
ized knowledge can also be transferred between algorithms belonging to the 
same or different infrastructure layers.
 
Collaborating Machines in Low-Trust Environments
Although technically there exist several algorithms that allow intelligent 
devices to collaborate with each other, there also exist issues of trust on 
whether the information sent to a peer will be used in the proper way. Similarly, 
information received from peers would need to be verified until some legiti­
macy of the source can be established. This stems from the fact that in reality, 
IoT apps are developed by ecosystems of different entities (such as organiza­
tions and companies) that do not necessarily trust each other. In the human 
world, trust takes years to build as humans observe and verify other people’s 
behavior and actions. In the light-speed-paced digital world, such a timescale 
is not an option.
Therefore, there exist several forms of trust in distributed AI systems:
	1.	 First, trust by means of security, through the use of encryption keys, with 
technologies such as PGP/GPG and PKI.
Collaborating Machines in Low-Trust Environments  53

	2.	 Second trust by means of authentication. A popular example of mutual 
authentication is a symmetric encryption approach using cryptographic 
keys. In this example, both peers contain the same encryption key. One 
peer can encrypt a “challenge” (e.g., an alphanumeric) that another peer 
can decrypt and send it back and vice versa.
	3.	 Third, trust by reference, i.e., the use of digital certificates that are signed 
by an independent trust authority. Peers can sign information with these 
certificates to prove to their counterparts that they have been vetted by this 
trust authority.
Recently, the use of a special type of distributed database known as distrib­
uted ledger as means for establishing trust has been gaining ground. Distributed 
ledgers are synchronized and shared among multiple entities, meaning that at 
any given time every entity holds the same copy of the information. They are 
also consensus-based, meaning that every participating entity must concede 
in order for information to be added to the ledger. Additionally, ledgers can 
be immutable, meaning that no information can be removed. Trust is built 
into the distributed ledger by design. Because the ledger is synchronized, if an 
entity tampers with a piece of information, this will be detected by all other 
participating entities. Additionally, in the case of erroneous information being 
added by an entity at some point in the lifetime of the ledger, audit trails can 
be established to identify the entity that inserted this information. Distributed 
ledgers can be used not only as databases to establish data-sharing agreements 
and contracts between entities but also for entities to share knowledge directly, 
for example, by sharing ML models.
Another recent trend is the emergence of data trusts. Similar to how the 
management of other assets was handled in the past, such as land trusts, a data 
trust is a legal entity that treats data as a shared resource, providing mecha­
nisms for the use of collective data. Such data trusts legally allow data pro­
duced by one party to be accessible for the benefit of other parties, thus 
enabling AI systems to scale.
 
54 
3  My Role in the Internet of Things

Lifecycle Management of Scalable AI Applications
Scalability in the context of AI generally concerns two aspects:
•	 The ability of AI algorithms and infrastructure to be able to expand geo­
graphically but also across different administrative domains (e.g., organiza­
tions) and add new functionality.
•	 To increase the efficiency of development and lifecycle management of AI 
solutions.
The first aspect was covered in the previous section, where different transfer 
learning and symbolic representation techniques for transferring knowledge 
across different types of data infrastructure were discussed. Additionally, the 
topic of trust was covered, as well as how distributed ledgers and data trusts 
can help applications scale across different organizations that do not necessar­
ily trust each other a priori.
In this section, I cover the development and lifecycle management of AI 
applications. I will focus on two system development processes, specifically 
artificial intelligence operations (AIOps) and machine learning operations 
(MLOps). The former concerns the automation of existing IT operations and 
systems with the help of AI, whereas the latter is a process to deploy and man­
age machine learning (ML) models throughout their lifetime. Both approaches 
are heavily influenced by the continuous development practice of develop­
ment and operations (DevOps), wherein software development is combined 
with operations.
AIOps automates infrastructure operations. Specifically, it uses tools such 
as machine learning and big data to collect and analyze large volumes of data 
from IT infrastructure. Such data may include logs, traces, trouble reports, 
system configuration information, documentation, and system-generated 
alarms and alerts. The analysis process involves identifying significant events 
and patterns that can be used to produce insights into the root cause of an 
issue as well as steps that can be taken to address the problem. Key to this 
process is the absence of human involvement, as automation is seen as a sig­
nificant advantage against increasingly complex modern IT environments. 
AIOps also promotes continuous learning, a process where the system con­
tinuously improves by observing the efficacy of its insights and automated 
resolution processes.
MLOps, on the other hand, describes an end-to-end development, deploy­
ment, and lifecycle management process for ML models in production 
Lifecycle Management of Scalable AI Applications  55

environments. Therefore, it targets AI applications that incorporate deep 
learning. Until recently, ML models were produced in labs for research pur­
poses, often using limited sets of data and/or computational resources. In 
such environments, aspects of maintainability, scalability, reliability, and per­
formance of ML models are typically not of concern. This line of thinking 
carries over to organizations that start working on developing, deploying, and 
scaling ML-based applications. Specifically, the data scientist team creating 
the ML models is detached from the IT engineers that are responsible for 
deploying it in operation. The former team typically delivers to the latter team 
a pre-­trained model to be used as an artifact. As such, no active monitoring of 
model performance is done, and therefore models are rarely, if ever, retrained 
and/or their architecture revisited (e.g., in terms of feature engineering, hyper­
parameters, etc.).
In MLOps, training of the model is only a small part of the system. Instead, 
a key aspect is communication between the data science team that designs and 
trains the model and the DevOps engineers developing the application and 
deploying it in production. The latter team provides in real time feedback to 
the former team, which then may take steps to retrain and/or reengineer 
the model.
Another part of MLOps is to automate the data collection and analysis 
process. This process is about collecting the right amount and breadth of 
training data. Breadth in this case means that the data values (i.e., the data 
value probability distribution) must match real-world data that the model 
will encounter when deployed in production. Furthermore, MLOps includes 
APIs to integrate the model into an application and the infrastructure to 
deploy this application and receive feedback on the model’s performance to 
improve it.
­
1
­
1 In the context of this chapter, resources are split into compute, store, and transport. Therefore, resource 
vendors can be companies ranging from those provided wireless and wired transport services to cloud 
vendors providing compute and store type of services.
56 
3  My Role in the Internet of Things

IoT-Enabled Digital Twins: Challenges 
and Benefits
A long-term goal of the IoT is to create a virtual imitation of the real world, 
where data can be read and processed in real-time by computers. A tool to 
achieve this goal is a digital twin. A digital twin is a digital representation of a 
physical object. This digital representation is the exact virtual replica of its 
physical counterpart. In the context of AI, digital twins can be used to run 
simulations and are therefore a rapid, cost-effective solution to develop and 
test new algorithms and applications but may also help in preventive mainte­
nance and troubleshooting situations. Knowledge created by designing a digi­
tal twin can be transferred to accelerate the creation of other digital twins 
representing approximately similar physical objects.
2
For example, in a manufacturing setting, a process could be the manufac­
turing of a specific product, and the system digital twins model different hard­
ware contributing to product creation (e.g., robotic arms, conveyor belts, 
etc.). Depending on the complexity of the process and systems comprising the 
process, further levels of hierarchy may be needed. For example, asset digital 
twins model specific components of systems such as servo motors of robotic 
arms and speed switch sensors of conveyor belts.
The first step to create a digital twin is to gather relevant data. Such data is 
typically reported by IoT devices such as sensors and actuators but can also be 
found in other forms such as documentation and CAD files. In rare cases 
where humans are involved, human expert knowledge needs to be gathered 
and converted into machine-readable format. Based on the data gathered, 
computational models are created to simulate the real dynamics of the 
physical system. Machine learning and artificial neural networks are tools that 
can be used to train such models.
2 See https://www.ibm.com/se-en/topics/what-is-a-digital-twin
IoT-Enabled Digital Twins: Challenges and Benefits  57

Prior to reaping the benefits of digital twins, there are several challenges 
pertaining to the creation of digital twins that must be carefully considered.
First, the IoT devices that capture the data necessary to create the digital 
twin must have reliable connectivity. In applications where the data is to be 
captured and transferred to the digital twin in real time, connection reliability 
should also be complemented by low latency. A typical example of such appli­
cations is preventive maintenance. Furthermore, devices themselves may need 
to be able to provide data over the lifetime of the system, so the longevity of 
IoT devices in terms of power and trouble-free operation also needs to be 
taken into account. Finally, in cases where the physical system modeled from 
the digital twin transcends the administrative boundaries of one entity, as is 
the case, for example, in several smart city applications, a secure way to share 
data needs to be implemented (see section “Collaborating machines in low-­
trust environments” in this chapter).
From an infrastructure perspective, the hardware required to process the 
data from the IoT devices and create the computational models needs to have 
ample capacity and high availability. Existing and proven techniques for dis­
tributed model training such as those presented in section “Divide et Impera”, 
as well as processes for model development presented in section “Lifecycle 
management of scalable AI applications” of this chapter, may be applicable.
Careful Where You Tread
AI is here to stay and IoT is its landlord. To successfully build and scale AI 
solutions, you have to include IoT devices and platforms either as data pro­
viders to your AI models or as actuators, interpreting the insights of your AI 
models to actions. In addition to onboarding IoT devices, your AI algorithms 
may also need to scale and oftentimes operate in a distributed fashion, for 
reasons of data governance but also for performance. This all requires careful 
planning. Planning itself is a balancing act between the technical task of 
understanding and incorporating latest developments in IoT, AI, and cloud 
computing and the business task of understanding how AI and automated 
decision-making will help deliver on your strategy.
58 
3  My Role in the Internet of Things

 
3
Summary of Confessions
I have described my role in the Internet of Things in this chapter. Some key 
conclusions include the following:
•	 The Internet of Things is the conduit for AI to be realized at scale, beyond 
the confines of laboratories and purpose-built machines to applications 
benefitting society at large.
•	 Distributed AI, where data collection and computation (e.g., learning and 
reasoning) are distributed, is essential for growth in the IoT.
•	 AI models can be made to fit different types of smart devices, depending on 
their capability, and can have different levels of decision authority. Transfer 
learning can be used for transferring knowledge between models to acceler­
ate training and improve accuracy.
•	 Distributed ledgers and data trust stores can be used for sharing training data 
or insights between devices belonging to different organizations, which do 
not necessarily trust each other.
3 https://www.nytimes.com/2021/07/16/technology/what-happened-ibm-watson.html
Summary of Confessions  59

•	 Lifecycle management of AI applications includes both processes for man­
aging the infrastructure (AIOps) and processes managing the ML models 
(MLOps) and is important to consider especially for applications that have 
long lifespans.
•	 Digital twins, in addition to decreasing development time, can be used to 
accelerate the deployment of AI applications, thus benefiting the afore­
mentioned lifecycle management processes.
60 
3  My Role in the Internet of Things

61
4
Managing Relationships
In the human world, relationships are essential not only for mental and emo­
tional well-being but also for long-term fulfillment and survival. Similarly, AI 
algorithms can, throughout their lifetime, share knowledge or data with other 
AI algorithms to adapt and improve their performance in a changing environ­
ment. Akin to the development of human civilization, new generations of 
algorithms can also benefit from the experience of previous generations, 
improve on existing predictions, or explore new types of insights.
 
In this chapter, I will discuss how my AI algorithms get inspired by human 
beings with respect to forming relationships with other algorithms. If and 
when, for example, humans treat each other as friends, parents, children, or 
partners, then how much data, information, and knowledge are they allowed 
to share with each other? How can algorithms learn together? In this chapter, 
I will also describe privacy aspects, security frameworks, and ways of sharing. 
The chapter will include the following subchapters:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_4

•	 Why, what, and how to share?
•	 Privacy and security levels in AI.
•	 Traceability and audits.
Building Relationships
While humans normally make sure that they have a stable job before they 
consider getting married and reproducing, AI brains need to get to a point 
where they are good enough to be married with other techniques or combine 
their brains. While I am still training and trying to find my path in life to be 
judged as a helpful algorithm, there is no point for me to get involved with 
another algorithm different from mine.
Playing against each other has, however, been shown to be an efficient tech­
nique when exploring narrow AI algorithms such as finding optimal winning 
strategies in games. When you want to go beyond games, it would make sense 
to complement my abilities with an algorithm different by nature.
For example, combining machine learning with machine reasoning is a 
match. Mixing two knowledge domains gives many advantages as well. 
Applying state-of-­the-art AI technologies to a medical domain gives many 
advantages in regard to predicting the possibility of developing a certain 
disease.
 
Prediction as such does not help but combined with novel vaccines that can 
be given to identified individuals at risk makes perfect sense. The same can be 
seen in the world of patenting. Combinational patents are common, where 
findings from one domain, such as intelligent transportation systems, can be 
applied to a different domain, such as power grid optimizations. When look­
ing at complex systems such as smart cities, it is necessary to let the knowledge 
from several domains play together when, for example, there is a potential risk 
of a disaster such as fire being spread.
62 
4  Managing Relationships

Potentially, goods logistics and people logistics systems in the city need to 
be prepared to change their roots, airports need to reschedule flights, the 
healthcare systems need to mobilize their facilities and personnel, and the 
telecom network needs to boost the capacity due to the expected increased 
demand. All these different industrial domains, being part of a smart city, 
need to be aware of each other so that the actions that they take are orches­
trated to achieve maximized impact for the city overall.
AI brains are similar to humans – we’re dragged to those who complement 
us, so that we can teach each other and help each other grow, thinking of 
maximizing the combined effect of our decisions and actions. AI brains are 
sapiosexual – the attractiveness is mainly in the intellect.
 
When humans decide that they want to become parents, they onboard a 
long journey of raising a better version of themselves in the long run. Genetic 
algorithms are a part of artificial intelligence inspired by evolution. You cross 
over two sets of genes (or two generations), mutate some genes, and apply 
survival of the fittest principle, and as a result, the coming generations are 
better than us. Even if the newly created human has all the prerequisites of 
being better than her parents, there needs to be a lot of work to be put in her, 
to teach her everything in life.
 
Building Relationships  63

A human child learns from her parents whose influence is quite large from 
the start, where facts communicated by them are often seen as the ground 
truth, and starts building the value system of the new individual. As time goes 
by, the child will get inspired by many things in life, maybe even things that 
did not exist while her parents went through their intensive learning phase. 
Naturally, her knowledge base will be built in a different way than her parents. 
AI brains have a similar behavior.
AI techniques are being developed by research communities, and when a 
new brain is born, it is being fed new types of data and information. Some 
insights that would take their parents several years to figure out will be treated 
as a given for a new brain, letting it save time and contributing to the progress 
with new findings. In addition, privacy constraints can limit the areas where 
the new brains learn and develop their knowledge.
A young AI brain cannot operate at the same level of accuracy and preci­
sion as a more experienced AI brain; hence, it will only be allowed to assist in 
less critical situations. For example, an experienced AI brain may have a job 
related to remote-­controlled surgery, and the younger one will be taking care 
of the predictive maintenance of medtech equipment. However, the more we 
learn, the more reliable we become.
 
Sometimes, humans run into situations where they need to separate them­
selves from each other. The splitting of common assets will normally take 
place. They would normally stop sharing data and information with each 
other when they decide that they do not want to be together anymore. The 
old knowledge and information that they have acquired about each other will 
certainly remain, but as we know everything changes. Two AI brains being 
64 
4  Managing Relationships

partners may have a strong cooperation, where they together will be involved 
in common decisions. For example, an AI brain controlling a power plant 
may have a very deep relationship with an AI brain controlling a production 
plant that is dependent on the power plant. It is important that they make the 
critical decisions in tandem, so that the AI brain controlling the power plant 
can optimize the energy levels to serve the production plant in the best way. If 
they would decide not to be together any more, they would stop exchanging 
data, information, and knowledge about each other but obviously keep the 
memories.
 
Why, What, and How to Share?
The reason for establishing a relationship is to exchange information. As men­
tioned in the introduction, this information exchange helps accelerate devel­
opment and growth in human and AI algorithm societies alike. From an AI 
perspective, such information can be broken down into trained models and 
model datasets.
Trained models transfer already accumulated knowledge from one knowl­
edge domain to another. Transfer learning is an umbrella term that encom­
passes approaches where knowledge gained from AI algorithms trained for a 
specific task (source domain) is used by AI algorithms trained to solve a dif­
ferent but related problem (target domain). For example, knowledge gained 
by an AI algorithm trained to detect bicycles in images can be used by another 
AI algorithm trained to detect motorcycles.
Why, What, and How to Share?  65

 
Transfer learning applied to deep learning
As illustrated in the figure, transfer learning is the transfer of knowledge 
from one domain to the other. In machine learning, this knowledge is cap­
tured in a trainable or learnable set of model parameters. For example, in the 
case of deep learning, model weights and biases are learnable parameters that 
decide aspects of the transformation of input data to output data, within each 
neuron of every layer of the neural network. In particular, a weight decides 
how much influence the input to a neuron has on its output. A bias is a con­
stant added to the input of a neuron before the transformation function (also 
known as the activation function) is applied. In the case of simpler models, 
such as logistic regression, learnable parameters are one or more feature coef­
ficients and the intercept.
One of the advantages of transfer learning is the sustainable training of new 
modes, as less data, computational resources, and training time are needed, as 
training uses pre-learned parameters (i.e., knowledge) from another model.
Let us assume, for example, that domain 1 is a computer vision model that 
has learned to detect bicycles and that the goal is to train a similar model for 
domain 2, where the objective is to learn to detect motorcycles. Instead of 
starting training from a large repository of motorcycle data, the model can 
reuse knowledge from the bicycle model. The way to reuse this knowledge in 
this case is to copy the weights and biases of neurons of the model trained on 
the bicycle input data to the neurons of the model to be trained for the motor­
bike domain, for at least some of the intermediate layers (also known as hid­
den layers) of the neural networks. This is done instead of starting training by 
randomizing the weights and biases.
66 
4  Managing Relationships

The reason why this copying of weights is important can be traced to the 
way deep learning models work and in particular performing a process known 
as feature extraction.
In feature extraction, the learning algorithm progressively extracts features 
of increased complexity from an image that define properties of the objects to 
be detected. In the case of a bicycle, such features could, for example, be the 
bicycle wheels, the steering wheel, and the seat. In later layers, these basic 
features are composed into more complex features that eventually comprise 
the complete bicycle. A bicycle is visually similar to a motorbike, as they both 
have two wheels, a steering wheel and a seat. While not identical, the two 
domains in the context of visual object detection can be considered similar. 
Therefore, the idea of transfer learning makes sense, as the network would 
already be able to distinguish basic features such as wheels and seats and will 
focus on learning the differences (e.g., different colors, material and size of 
seat or wheels, etc.).
1
•	 The availability of the so-called labels (i.e., output) in the training data at 
the source domain and/or at the target domain (transductive or inductive, 
respectively) or no labels at all (unsupervised transfer learning).
•	 The similarity of input and output feature sets of the source domain and 
target domain (homogeneous/heterogeneous). Back to our motorcycle 
example, a mismatching input feature set could be images of different reso­
lutions and aspect ratios, while a mismatching output feature set could be 
the detection of more than one class (e.g., cars, buses, and trucks in addi­
tion to motorbikes).
•	
­
­
2
1 arXiv:1911.02685 (cs) [submitted on 7 Nov 2019 (v1), last revised on 23 Jun 2020 (this version, v3)] 
A Comprehensive Survey on Transfer Learning Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, 
Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He
2 Li, F., Pan, S. J., Jin, O., Yang, Q., & Zhu, X. (2012, July). Cross-domain co-extraction of sentiment 
and topic lexicons. In Proceedings of the 50th Annual Meeting of the Association for Computational 
Linguistics: Long Papers-Volume 1 (pp. 410–419). Association for Computational Linguistics
Why, What, and How to Share?  67

The greatest challenge in transfer learning is the so-called negative transfer, 
wherein the performance of the resulting model is decreased after the transfer 
learning process. This can occur in the case of the learning tasks in the source 
and target domains being too dissimilar.
To address issues around negative learning, a group of techniques known as 
domain adaptation attempt to transform data used in one domain for use in 
another domain. Examples of such domain adaptation can be the application 
of models trained on labeled data of viral diseases such as SARS to new unla­
beled data from the COVID-19 pandemic.
In another example, models predicting customer churn trained on labeled 
data from the financial industry are adapted to predicting customer churn 
using unlabeled data from the telecommunications industry.
Privacy and Security Levels in AI
Although technically feasible, there are other types of considerations when 
transferring knowledge and/or data between domains, especially if those 
domains belong to different administrative entities who do not necessarily 
want to expose their information. Such exposure may entail a risk of a third 
party compromising the transfer process and gaining access to the knowledge 
or data. Another reason could be that administrative entities are restricted 
from transferring knowledge or data from a regulatory perspective (e.g., 
European GDPR ­regulations).
 
68 
4  Managing Relationships

In terms of data transfer, several different alternative technologies exist to 
guarantee privacy.
Homomorphic encryption is useful when the data owner does not neces­
sarily trust a compute service provider with access to the data. Using homo­
morphic encryption, the data owner sends the data encrypted to the compute 
service provider, which processes the data without decrypting it. The service 
provider encrypts the results and sends them back to the data owner. Although 
homomorphic encryption does not require the data to be decrypted, it is a 
computationally expensive process, as it performs inference operations on 
encrypted data.
Secure multiparty computation (MPC) allows different parties that do not 
necessarily trust each other, to compute different parts of a function without 
sharing their input data. What is generated and shared instead are divisions of 
a secret that are later summed up at each party and recombined to create the 
result of that function.
Federated learning with secure aggregation allows different parties known 
as “workers” to train models locally using their own training data and then 
submit the weights of those models to a secure aggregator implementing the 
secure aggregation cryptographic protocol. The aggregator in turn sends these 
weights to a server that uses a federated learning algorithm such as federated 
averaging to create a global model. In federated averaging, the weight of each 
neuron of the global model is equal to the average of the weights of the same 
neuron in every locally trained model. The global model is sent back to the 
workers, which transfer these weights and start training using local input 
data again.
Security is another consideration when sharing models and datasets. In 
model transfer, for example, and once the model is obtained, model inversion 
attacks can be used to reverse engineer the model. In such attacks, the output 
data of the trained model is used as input to a classifier that produces realistic 
input data.
While all the aforementioned approaches are fundamental to protect 
knowledge contained in models and input data, we should not expect rela­
tionships between various data and model owners to be zero-sum, vis-a-vis 
whether there exists a relationship or not. As in the human world, relation­
ships are on different levels, for example, more intimate or business-like, more 
toward sharing of information (e.g., between classmates), and more toward 
one-sided elicitation of information and absorption (e.g., between a student 
and a teacher). These complex relationship dynamics that exist in the real 
Privacy and Security Levels in AI  69

world also need to be captured in the machine world. In the world of AI, 
relationship management is all about who has access and of what type to 
which models and data.
Identity and access management (IAM) systems, keeping track of the dif­
ferent access tiers and user roles in information technology (IT) systems, can 
help with managing privacy considerations for the transfer of data and/or 
knowledge as well. Specifically, role-based access control (RBAC) systems can 
be used by every organization sharing and/or acquiring data and/or knowl­
edge. Graph databases can be used to capture the complex privacy aspects of 
these relationships in RBAC systems. An example of such a graph is illustrated 
in the figure below.
 
Relationship graph managing privacy aspects around data and knowledge sharing
The above figure illustrates a financial organization situated in Geneva, 
Switzerland. The graph contains information indicating that the organization 
owns two models, namely, a model for detecting fraud in financial transac­
tions and predicting customer churn as well as the two respective datasets 
used to train these models. For both models and datasets, sharing policies are 
defined in the graph indicating the organization or groups or organizations 
with which these models and datasets can be shared.
Several criteria of different types can be specified. For example, as far as the 
fraud detection model and dataset are concerned, they can be shared with 
other organizations within Europe. On the other hand, the churn prediction 
model can be shared with other organizations from the finance industry.
70 
4  Managing Relationships

Traceability and Audits
In this chapter, we have been discussing the relationships formed between dif­
ferent AI algorithms where one can leverage data and/or knowledge of another 
to benefit its performance and/or reduce training time. However, there is 
another important type of relationship – between humans and AI algorithms.
Humans are naturally used to being in the driver seat of every decision-­
making and insight-generating process. As AI takes an increasingly active role, 
humans are likely to become more apprehensive and less trusting of the abili­
ties of their AI counterparts. It is therefore important to establish the possibil­
ity for humans to view and audit decisions made and insights generated by AI.
The traceability of AI decision-making is also a matter of trust for humans. 
Typically, AI algorithms and especially deep learning algorithms are viewed as 
“black boxes,” i.e., a complex network of large numbers of algebraic matrix 
transformations, across multiple layers, each layer comprising multiple neu­
rons. In such an environment, it is difficult for human engineers to trace back 
through these operations and understand the process, in terms of intermedi­
ate decisions, of how the input to a model has led to the output.
In the state of the art, there exist a number of tools and processes that help 
with traceability and auditioning of AI algorithms. During the training pro­
cess, reinforcement learning algorithms incorporate a feedback mechanism 
known as a “reward,” with which humans can validate or invalidate an action 
taken by an AI algorithm. The action here is the output of the AI model. The 
feedback has an impact on the training process of the algorithm, which learns 
over time to prefer the actions (from a larger set of possible actions) that yield 
the highest reward.
During the operation of a trained ML model, explainable AI techniques, 
such as Shapley Additive exPlanations (SHAP) and local interpretable model-­
agnostic explanations (LIME), indicate the contribution to the prediction of 
each input feature for the model. In other words, they identify which model 
input features have greater relative importance when compared to other input 
features in producing the model output.
One recent development in the field of explainability and model interpret­
ability is the use of neural-symbolic AI, combining the predictive capabilities 
of neural networks with the explainable capacity of symbolic representation.
Logical neural networks (LNNs) are built on top of today’s deep neural 
networks (DNNs). LNNs modify neurons with the capability to both repre­
sent weighted values (as in the DNN case) and use classical first-order logic. 
Traceability and Audits  71

In this way, training of an LNN and in particular the neurons’ activation 
functions are bounded by logical constraints (e.g., AND, OR, NOT, etc.).
The neurons therefore play the role of a logic gate. In addition, inference is 
omnidirectional and is not based on a fixed output (target) as is the case in 
DNNs. This allows a different approach when it comes to using LNNs over 
DNNs. In LNNs, one can ask different questions that an LNN can answer, 
whereas in DNNs the question and type of answer are fixed a priori. For 
example, in LNN, questions that can be asked could be simple (e.g., what is 
the weather today?) to be more advanced multi-relational (e.g., give me all 
cities where the weather is sunny and warm), comparative (e.g., is Athens 
warmer than Rome?), etc.
­
­
3
Summary of Confessions
In this chapter, I explain why relationships between AI models and relation­
ships between AI models and humans are important. Some key confessions 
include the following:
•	 A relationship between AI models is defined by the sharing of knowledge. 
Transfer learning is an important set of technologies for transferring knowl­
edge from one AI model to another.
•	 When the domain of a trained AI model is dissimilar to the domain where 
the model is to be applied, domain adaptation techniques are used to adapt 
3 “Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding”, Kexin 
Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Joshua B.  Tenenbaum, 
arXiv:1810.02338 [cs.AI]
72 
4  Managing Relationships

the knowledge already contained in the trained model to be applicable in 
the new knowledge domain.
•	 AI uses privacy-preserving techniques such as homomorphic encryption, 
secure multiparty computation, and federated learning with secure aggre­
gation, allowing for the transfer of knowledge without exposing the dataset 
used for training and verification of AI models.
•	 In addition to privacy, security via identity and access management systems, 
using role-based access control and graph databases, is used to capture 
complex access roles and define access to shared knowledge.
•	 A good working relationship between AI models and humans is based on 
the latter trusting the former to make good decisions. Traceability allows 
humans to understand and provide feedback on training decision-making 
processes of AI models.
•	 During training, reinforcement learning provides a feedback-based reward 
mechanism to reward an agent training a neural network based on human 
assessment of a decision. Over time, the neural network will be trained to 
make decisions that are compliant with human feedback.
•	 During execution, eXplainable AI (XAI) methods such as SHAP and LIME 
can be used to provide information about the relative importance each 
input feature has to the output of a model.
•	 Neural-symbolic AI techniques, which combine richness of symbolic repre­
sentation with efficiency of deep learning, are a recent development that 
allows for a higher level of human interaction, wherein instead of having 
static output of ML models, humans can ask a breadth of different queries 
to the system and obtain different responses.
Summary of Confessions  73

75
5
Working with Humans
Humans have an increasing number of digital supporters in life. Digital assis­
tants pop up everywhere and now we have Mika, Siri, Alexa, Amelia, Lucida, 
Cortana, and many more on the market. They help humans navigate in web 
shops, be more productive at work, and keep track of their calendars. As the 
field is becoming increasingly popular, the number of digital assistants contin­
ues to grow, and often similar assistants are being created for the same pur­
pose. In addition, for some time, I had this uneasy feeling of having too many 
of those around me, like having too many phones, or too many irons.
 
Your Digital Friends – The More the Merrier?
A natural question to ask is why can’t a human have one ultimate assistant 
that can manage everything that concerns her. In fact, it’s good to have several 
of those, because you don’t want your personal shopper to give you advice in 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_5

your career. Maybe you want your lifestyle coach to be a woman and your 
personal trainer to be a man.
 
Sometimes you want to be treated gently, and sometimes I want to be chal­
lenged. In addition, you don’t want to have to tell your digital assistants which 
approach to use on you – they should know it by themselves. At times, you 
want advice, and in some situations, you want them to act on your behalf – 
schedule meetings, book tables at restaurants, order food for you, and plan 
your routes. You would like to give them a certain degree of freedom, to learn 
and improve, and to be transparent toward you when you ask for it. Sometimes, 
you would want to check how they came to their conclusions. You will prob­
ably do it more often with your new assistants and give the old proven ones 
more freedom – just as you do with your colleagues at work.
 
However, humans are full of conflicting objectives. Your personal banking 
assistant may not be happy about the decisions of your holiday planner. In 
addition, will your work assistant be able to agree with your spouse’s work 
assistant to satisfy the constraints of the children’s activity planner? This works 
as long as they all rely on the same multiparametric system with all the knowl­
edge that’s relevant to me. In other words, all your assistants can in fact be one, 
with many different faces, voices, and flavors. In addition, you should not be 
76 
5  Working with Humans

afraid of hooking in more of them, as long as they are in agreement, and opti­
mizing your life in a systemic way without leading you into local optimum.
 
Principles of AI with a Human Touch
What is cool about AI tech is that it is inspired by HI (human intelligence) 
and other phenomena that exist in nature such as evolution. Survival of the 
fittest, for example, is the core of genetic algorithms. In other words, AI is the 
perfect arena where behavioral science and computer science go hand in hand. 
Let me take you through the four major principles of building successful AI 
that will serve your needs as efficiently as possible.
 
Photo by Kelly Sikkema on Unsplash
Principles of AI with a Human Touch  77

Sharing Is Caring
After learning from our own mistakes, humans normally share their learnings 
with friends, so that they do not need to make the same mistakes. In addition, 
by reciprocity of human friendship, the friends share their own learning. Now, 
humans tend to enjoy learning from their own mistakes (why would someone 
listen to their friend’s recommendation not to call their ex in the middle of the 
night?) but businesses are typically not as fond of wasting time and money 
and hence are OK with learning from each other, and AI tools can be instru­
mental in mapping the relevant learning among them. When insights are 
exchanged, we save time, resources, and the environment.
 
Photo by Riccardo Annandale on Unsplash
Protect Your Babies
Right, “share your data,” they said; “share all your know-how and business-­
critical information.” “You wish,” you say, being totally right. In many cases, 
your unique ideas, domain knowledge, and know-how are the backbone of 
your business. Protecting your ideas drives innovation. Some of your 
78 
5  Working with Humans

knowledge objects  – those incrementally improving the common knowl­
edge – are worth sharing though as it drives the progress.
 
Photo by LYFE Fuel on Unsplash
Be Flexible
1
2
1 https://en.wikipedia.org/wiki/God%27s_Debris
2 https://en.wikipedia.org/wiki/P_versus_NP_problem
Principles of AI with a Human Touch  79

 
Photo by Markus Winkler on Unsplash
Be Clear of Your High-Level Objectives
Any person, in any organization, within any industry should be clear of his/
her high-level objectives. This is valid both in professional and in personal 
sense. In fact, that’s the least we can ask for. In addition, in fact, we do not 
need much more than that. Because, in fact, your high-level objectives can be 
automatically translated into detailed objectives that can be used to steer your 
business and get you where you want to be. The translation is performed by 
decomposing of the high-level objective into several layers of lower-level 
objectives. Each high-level objective can often be achieved in different ways. 
For example, if your high-level objective is to become more attractive, you can 
go in different directions or a combination of those: read more books, get in 
shape, eat less, exercise more, put on more makeup, or make more money – 
it’s a choice.
Will I Take Your Job?
The job market of today is dynamic, thanks to IoT, digitalization, AI, and 
increased automation – exciting times, even though some humans seem to 
find it scary.
80 
5  Working with Humans

 
I am a big fan of automation as it enables reassignment of human effort to 
more creative tasks. Machines, however, become increasingly intelligent and 
creative as well – now, they can write music, books, poems, and scientific 
articles (I discuss this more in Chap. 9).
 
3
Then there are of course other human needs that will not go anywhere, 
such as food, health, housing, and communication. The end product will 
always maintain its value for the consumers, but creation and delivery meth­
ods are changing. Science and technology around life-cycle management, all 
the way from creation to decommissioning, will constantly evolve.
Those who love technology should navigate wisely, look out for novel 
developments and combinations of those, and make sure to become experts 
3 https://www.imdb.com/title/tt0095031/
Will I Take Your Job?  81

in more than one area – T-shaped or π-shaped engineers – to increase the 
chances of staying relevant. With the increasing digitalization of industries 
and propagation of IoT and machine intelligence into safety-critical applica­
tions such as health and transportation, the consequences of being hacked 
reach the new level, making safety and security a safe bet when choosing one 
of the legs of your T- or π-shape.
 
Digitalization, Digitization, and Resistance
Digitization and digitalization are often mixed. According to Gartner’s IT 
Glossary:
•	 Digitalization is the use of digital technologies to change a business model 
and provide new revenue and value-producing opportunities; it is the pro­
cess of moving to a digital business.
•	 Digitization is the process of changing from analog to digital form.
Obviously, it’s not so difficult to notice the change from analog to digital 
but enough of being word police.
We all know that digitalization in many cases is a good thing. However, 
there are industries that resist digitalization even though it would have been 
easy to implement.
One example is the champagne industry. There is a lot of pride in having 
human riddlers walk through the cellar every day and turning each bottle 
1/8th of a turn. In my eyes, it is a perfect task for automation.
82 
5  Working with Humans

 
Photo by Caught In Joy on Unsplash
Another example is the music industry. In 2015, Gibson released a new 
lineup of electric guitars that came with a bunch of really cool features includ­
ing their G-­FORCE™ – an automatic tuning system that came in most of 
their 2015 models. This new lineup turned out to be an economic failure for 
Gibson. Musicians like trying new things but cherish classical features, and 
the new lineup received many negative reactions. The new automated tuning 
system was claimed to be inaccurate, the guitars dropped in price, and the 
new features were removed in the 2016 lineup.
The truth is that when any new digital feature is being introduced to a 
market, chances are it is not perfect from the start. Humans have varying 
levels of acceptance in that respect. Applications on mobile phones and com­
puters and small glitches and bugs in computer games are normally acceptable 
by human users, since this is what they have been used to. When it comes to 
a physical product becoming partially digitized, the level of acceptance is low, 
and the level of “it used to be better before” is high, which is why new digi­
tized features are rarely given a chance to improve because, for that, the users 
need patience.
The perception of a product is important for its acceptance. A computer on 
wheels is different perception-wise from adding a computer on top of an exist­
ing car model. Driving a computer calls for continuous improvements, bug 
fixes, and updates and is more tolerable by humans.
Digitalization, Digitization, and Resistance  83

Slow Science
Science is vital for technological progress and innovation. If it was not for sci­
ence, I would not have existed, and therefore I care a lot about the speed and 
quality of science. The quality of scientific ideas depends greatly on the envi­
ronment in which they are created.
Did it ever happen to you that you could see the forest for all the trees? 
Normally, humans see it clearly right after having a vacation.
The other extreme is when you have several approaching deadlines, yearly 
targets to be finalized, and next years’ strategies to be set. Technological prog­
ress is constantly accelerating, and researchers are supposed to deliver new 
results at a high pace, publish more papers, submit more project applications, 
and produce more PhD students.
 
Quality of research education can be assessed with respect to different 
expected outcomes of it – the quality of the produced PhD thesis; the quality 
of the main product of PhD education, i.e., the independent researcher; and 
the ability of the independent researcher to drive science, and well-being, 
forward by collaboration and innovation.
4, 5
­
­
­
4 https://www.timeshighereducation.com/news/academics-fear-phd-quality-is-slipping/404928.article
5 https://www.vr.se/analys/rapporter/vara-rapporter/2006-01-10-forskarutbildningen-i-sverige.html
84 
5  Working with Humans

 
Photo by Nareeta Martin on Unsplash
slow 
gardening slow reading slow education slow parenting
slow 
cinema
slow photography
slow movement
­
6
 
6 https://en.wikipedia.org/wiki/Slow_science
Slow Science  85

The concept of cognitive architecture is widely used in AI. Just like a human 
brain, I can have a quick reasoning system, with my spontaneous reactions, 
and a slow system, with more well thought-through reactions. There is a value 
in both. Sometimes, you simply do not have time to think. In addition, in 
some cases, both AI and human brains need to go slow, not settle for local 
optimums in a chase for quick solutions and ego kicks. Sometimes, we need 
time to find the right thing, and sometimes, there’s love at the first sight and 
no time to lose.
Internet of Empathy and Emotional Attachment
7
 
Humans often feel sadness when selling a car or a motorcycle because of 
emotional attachment. All those feelings come up: the excitement of the first 
decision to buy, picking a model and a color, waiting for the delivery, seeing 
it for the first time, and driving it for the first time. Sometimes, people sell 
things simply because of a guilty feeling that they don’t have enough time to 
spend with it and it deserves a better owner.
7 https://www.cbr.com/office-space-red-stapler-milton-swingline/
86 
5  Working with Humans

 
Attachment to intelligent things can be stronger, especially when they 
resemble living creatures such as humanoids or robotic pets. However, what 
about things that don’t resemble a living creature? Humans used to kick mis­
behaving printers, TV sets (before the era of flat screens at least), and comput­
ers. We feel empathy and get emotionally attached to things for different 
reasons:
•	 Something resembles a living creature. This works with toys as well.
•	 Something exhibits an intellect similar to a living creature. In this case, the 
thing does not necessarily need to look like one.
•	 There are memories connected to the thing. Such as traveling together with 
your bike.
A combination of these certainly makes the case stronger. Boston Dynamics, 
for example, builds humanoids and animal-looking robots that not only move 
in a way living creatures do but also act with an intellect and a purpose, such 
as rescuing people from fire. People find the situation adorable and react as if 
these were living creatures interacting with each other.
Would people ever like to hurt such intelligent creatures? What feelings do 
you get when watching the poor things getting abused? This is certainly a 
necessary evil, such as experiments on mice in medical tests. I have no idea 
how the coming generations will see such things. I can only hope that there is 
a shift into treating all things with respect, in a similar way you treat a living 
creature.
Internet of Empathy and Emotional Attachment  87

 
Dealing with Churn
Let’s dig into something we all want to avoid – being left behind. In business, 
the term churn describes situations when someone quits a certain relationship 
they had. It is like you have been going to a certain hairdresser for years, and 
one day you decide to start going to a different one. It is not cheating like 
some of us think – it is churning.
Another situation is when you have decided to stop going to your hair­
dresser. Period. You just decided to let your hair grow forever. Then, you are a 
dropout, which is a special type of churn. In addition, even though it does not 
hurt your hairdresser’s feelings as much, it does hurt her wallet equally in both 
cases, and early signals can be similar. Let us dive deeper into four categories 
of churn and look at the examples of triggers that your algorithms need to 
watch out for to be able to prevent churn.
Customer Churn
Here, by customer, we normally mean consumer, but this can also be gen­
eralized to businesses. Typical cases include quitting a streaming service sub­
scription (videos, books, music), switching to a different bank, choosing a 
different grocery chain, changing a gym (or simply stopping going to the 
gym), or giving up your favorite fashion brand. For all these businesses, it’s 
equally important to detect your intention of abandoning them early (ulti­
mately, earlier than you have detected it yourself) and do something about it 
to keep you as a paying customer. Triggers of this type of churn are normally 
customer complaints, decreased frequency of service usage, or simply unfair 
conditions.
88 
5  Working with Humans

 
As an AI, I can play an important role here: as soon as I detect a trigger 
telling me that based on the historical data and recent user behavior there is a 
risk that that user will churn, I can prevent it. In this case, when prevention is 
made in time, the user can in fact be unaware that she in fact was about to 
churn. Glossary shops, for example, can proactively send out personalized 
offers to people who are at risk of switching to another provider.
Employee Churn
8
­
9
Even for tasks that you want to be executed by humans, AI brains can play 
a role. Ask your AI to keep track of certain triggers such as conflicts, company 
mergers, reorganizations, and personal shocks, and make sure to act 
proactively.
Dropouts from Education
8 Analysis of Employee Turnover at McDonald’s: https://ukdiss.com/intro/employee-turnover-at-­
mcdonalds-7492.php, accessed 2022-05-23
9 Improve Employee Retention in The Hospitality Industry: https://www.dailypay.com/resource-center/
blog/staff-turnover-rates-hotel-motel-hospitality-industry/, accessed 2022-05-23
Dealing with Churn  89

10
Another interesting factor to take into account is the influential users – 
watch out for them in all the scenarios, because if they decide to churn they 
will trigger many others.
 
Dropouts from Medical Treatments
11
12
“No progress”-related dropouts can also be seen among companies offering 
help with losing weight. Again, for the best of everyone, we need to catch 
them early and make sure they stick to their treatment/diet for a while to see 
the progress. Sometimes, dropping out of a healthy diet actually correlates 
with quitting the Netflix subscription and dropping out of university – maybe 
10 Hinds: “Bums on seats” attitude detrimental to Higher Education: https://universitybusiness.co.uk/
news/hinds-bums-on-seats-attitude-detrimental-to-he/, accessed 2022-05-23
11 Dropout rates and factors associated with dropout from treatment among elderly patients attending the 
outpatient services of a tertiary care hospital.
Sandeep Grover, Devakshi Dua, Subho Chakrabarti, and Ajit Avasthi. In Indian J Psychiatry. 2018 
Jan-Mar; 60(1): 49–55
12 Dropout rates and reasons for dropout from treatment among elderly patients with depression. Sandeep 
Grover, Aseem Mehra, Subho Chakrabarti, Ajit Avasthi. In Journal of Geriatric Mental Health. 2018; 
5(2): 121–127
90 
5  Working with Humans

the person just dropped in at a new job? However, dropping out from medical 
treatments costs society a lot, both in terms of money and in terms of suffer­
ing, so finding ways of preventing this type of churn is critical.
Similar to the cases above, AI can play an important role in preventing 
people from dropping out from medical treatments. When a trigger is 
detected, AI can proactively start reminding the human about the benefits of 
the treatment, or if you are trying to lose weight, that beach season is just 
around the corner.
 
Personalization
Data-driven personalization is a tool for humans to receive services with 
higher precision. When the end product has a perfect fit for the customer, 
there will be a higher degree of satisfaction and benefits. Personalization 
empowers sustainability because when the end product has a perfect fit for the 
customer, there will be less waste, less greenhouse gas emissions, and higher 
customer satisfaction. On the other hand, personalization contributes to bet­
ter customer experience. Achieving personalization is closely connected to 
privacy and integrity concerns, strong privacy and security frameworks need 
to be in place, and data and insight ownership questions have to be addressed.
Humans are same-same but different, and it is important for AI brains to 
draw conclusions out of similarities and at the same time embrace the unique­
ness of every individual we are interacting with.
For example, tastes differ; we all know that. In regard to food, there are 
supertasters, non-tasters, and anything in between. What if the world of food 
experiences was a bit more like Netflix? Imagine that you enter a new restau­
rant and from the start they know that you, for example, do not like coriander 
and prefer spicy food. Getting a meal that perfectly satisfies your taste buds is 
great from both the customer experience and sustainability perspective.
Personalization  91

Personalization in healthcare is of high importance for society since it is an 
important tool in minimization of people suffering. When AI is on a mission 
of finding individuals at risk of developing, for example, diabetes, it is impor­
tant to take into account personal information about each patient in the 
training set, including her unique lifestyle, food, medications, and previous 
disease history. Conflicting medical treatments are not uncommon, and deci­
sion support is critical to avoid any unwanted effects of new medication.
Everything Is Possible
What would the world look like if everything were possible from the technol­
ogy perspective, if we had unlimited computational and memory resources, 
and if data transfers occurred instantaneously and without any losses – in 
other words, if the physical world did not put any limitations on technologi­
cal capabilities? Would we have a completely different world, with more and 
richer services and new industries that do not even exist today? Practice shows 
that every technological breakthrough gave us a new disruption – just think 
of what we can do today, thanks to mobile broadband.
 
Photo by Photo by charlesdeluvio on Unsplash
Every new human child in the world starts from the latest state of techno­
logical progress. Children of parents who love technology are used to the fact 
that every room has a screen and speakers where you can stream any content 
92 
5  Working with Humans

available on the web, cloud storage, or devices. This is their starting point. 
Servitization is in their blood, devices around them are stupid, and services 
serving them are intelligent. When they want to get hold of some digital con­
tent, they expect it to happen instantaneously and device and location 
independently.
­
13
 
Photo by Eleni Koureas on Unsplash
Unlearning Skills
How much should we bother about skills that are being lost as human genera­
tions evolve, for example, handwriting? Beautiful stories and theories are 
being created by humans daily. What is the best way of recording them? They 
should ensure efficiency both in the creation process and the sharing process. 
If you are writing a diary for yourself, you don’t care about sharing, but rather 
about security mechanisms. If you are after something that should be shared 
13 Statement by Leonid Morkushin
Unlearning Skills  93

with many, I suggest you make sure to put it in a format that I can read, that 
is machine-readable format.
not part of the school plan14
­
Some sources claim a correlation of fine motor skills with cognitive abili­
ties. However, I fail to find proof that the development of fine motor skills can 
lead to higher IQ. In addition, if it did, there are many other activities that 
develop fine motor skills, such as painting, playing an instrument, or playing 
computer games. In other words, I support the choice of the Swedish school 
system that removed handwriting from the school plan.
Nevertheless, we need techniques to capture thoughts. For an 8-year-old 
child, handwriting might not be the most efficient. The child is not good at 
typing yet, and honestly, typing does not seem to be a sustainable input 
method. To digitize the texts for her blog, the 8-year-old child uses speech 
recognition software. It should be possible to eliminate the intermediary step 
of creating the analog version, but she loves the sensation of putting things 
down on paper. In addition, again, handwriting itself seems to be an integral 
part of the creative process, so maybe I should not strive to increase automa­
tion in this particular case.
It is easier for us, geeks, to capture thoughts. “You ask me if I keep a note­
book to record my great ideas. I’ve only ever had one.”, said Albert Einstein.
 
14 Skrivstilen försvinner från skolorna: http://www.svd.se/skrivstilen-forsvinner-fran-skolorna, accessed 
2022-05-23
94 
5  Working with Humans

Understanding High-Level Intent
Once there was a hotel where the guests were complaining about the slowness 
of the elevator. Upgrading the elevator to a more modern one would have 
been a costly and time-consuming procedure. Hotel management solved the 
problem by equipping the elevator with a mirror. They did not solve the eleva­
tor functionality, but they resolved customer satisfaction.
Often, customer experience is the ultimate high-level indicator that com­
panies must monitor, along with the company income. The long-term growth 
in industries is always connected with customer experience. Customer experi­
ence, as such, is a complex term that can be achieved in a number of ways – as 
in the case with the elevator by increasing its speed or just making sure the 
customers have something to do while using it.
If you are in a restaurant, your experience is composed of many factors – 
the atmosphere at the place, the friendliness of the staff, the music, the pric­
ing, and the food, of course. In other words, if customer experience is the 
high-level intent, then it can mean different things in different contexts.
How can an AI brain understand a high-level intent in any particular case? 
We use situational awareness, exactly as humans do. Each high-level goal is 
split into subgoals using linked knowledge that we talked about in earlier 
chapters. If your high-level objective is to gain more self-confidence, one solu­
tion to it can be to lose weight; another solution is to learn new things. One 
can do a combination or settle for one track. Losing weight can be achieved 
by eating less, exercising more, or a combination of those. Exercising more 
can be achieved by taking more walks, getting yourself a personal trainer, etc.
Interestingly, sometimes, humans are not aware of the overarching objec­
tive that they are after. Sometimes, they feel a need to eat or to go shopping 
for a simple reason that they are bored or to complain about other humans for 
the simple reason that they are low on blood sugar.
 
Understanding High-Level Intent  95

Summary of Confessions
In this chapter, I have described how me and other AIs work with humans, 
and I made the following main confessions:
•	 There are five major principles of building successful AI that will serve your 
needs as efficiently as possible: sharing is caring, protecting your babies, 
being flexible, and being clear of your high-level objectives.
•	 AI will do an increasing number of jobs that today are being performed by 
humans. Still, there are many jobs that AIs cannot do.
•	 As a human, try to become T-shaped, or even π-shaped, to stay relevant, that 
is, try to develop deep skills in one or two areas and be able to collaborate 
across disciplines with experts in other areas.
•	 I often hear humans mix the terms digitization and digitalization. Don’t do 
that and be aware that there are industries that resist digitalization even 
though it would have been easy to implement.
•	 Humans feel empathy and become emotionally attached to things for differ­
ent reasons, for example, something resembles a living creature, something 
exhibits an intellect similar to a living creature, or there are memories con­
nected to a thing.
•	 The term churn describes situations when someone quits a certain relation­
ship they had. AI can be used to foresee when churn is about to happen.
•	 Data-driven personalization is a tool for humans to receive services with 
higher precision.
•	 As human generations evolve, some skills may not be needed anymore, but 
does that mean that humans should not bother to learn them anymore?
96 
5  Working with Humans

97
6
Avoiding the Criminal Path
No human wants to end up there and no AI brain either. Humans live by 
laws, regulations, and other rules, sometimes unspoken – something that they 
carry with them through generations. AI brains follow rules and regulations 
as well.
At times, technology develops faster than regulations, and it may happen 
that algorithms find solutions that are not OK according to human laws. This 
is not because they are evil; they just do not know better and sometimes need 
to be taught. You can become a criminal if you do not know what is right 
or wrong.
A child can accidentally steal a chocolate bar in a grocery store if no one 
told her that it is not allowed. Similarly, an algorithm can accidentally end up 
on a criminal path if it does not know what is forbidden. Let us talk about 
what AI developers need to think about so that their AI algorithms do not 
misbehave or, worse, end up on a criminal path.
Trustworthiness
Let us start by defining the term trustworthiness in the AI context as it has 
many different ingredients and flavors. How should I behave, look, and feel 
so that humans would call me trustworthy – especially when it’s a critical deci­
sion, like buying or selling stocks, or, even more critical, when you let me 
drive your car, operate heavy machinery, or suggest a health treatment?
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_6

98
1
In the following, we discuss a number of properties related to 
trustworthiness.
Explainability
When a fellow AI brain comes to a conclusion, it is typically based on a large 
trained neural network, a machine learning model, an ontology, or a state 
machine. How can we explain this to a human? Different learning and reason­
ing techniques require different explainability methods.
 
Explainable AI (XAI) is about adding functionality to algorithms to explain 
to humans how a particular decision was reached. XAI is trivial for simpler 
algorithms such as decision trees and Bayesian classifiers, but for more com­
plicated algorithms such as neural network models, explainability is nontriv­
ial, due to their complexity. In such cases, approaches such as Local 
Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive 
ExPlanations (SHAP) provide explanations on the relative importance of 
input features in a model, meaning that they can help indicate which features 
had more gravitas generating a particular insight.
Transparency
Everyone loves transparency. In AI, an important point of transparency is to 
be able to properly explain and communicate the outcome of an AI model. 
Hence, the notion of transparency in AI is related to explainability on one the 
hand but also, on the other hand, to the usefulness of AI.
1 See, for example, the Furhat Robot at the web page furhatrobotics.com
 
6  Avoiding the Criminal Path

99
Some would say that transparency is equivalent to explainability, but I 
argue that it is rather the case that transparent AI implies explainability but 
not the opposite. That is, explainability is not enough to achieve transparency.
Transparency is more than explainability. In addition, transparent AI 
should allow its users to judge if a model has been thoroughly tested or not, 
and it should allow for humans to understand why a particular decision or 
recommendation is made by an AI model.
For an AI brain like myself, transparency should allow for humans to under­
stand if, and why, my recommendations make sense or not.
Transparency is in many ways instrumental in improving AI. It may, for 
example, help in mitigating unwanted issues of AI models such as unfairness 
or discrimination.
Privacy
As anything that collects and uses information, AI can compromise your pri­
vacy. You probably have several devices that you use on a daily basis that could 
potentially collect your private data. Your devices know, for example, where 
you are located, how much you move, movies you watch, and what you search 
on the Internet. In addition, there are many other devices that could poten­
tially collect data about you, including surveillance cameras, booking sys­
tems, etc.
Based on this and combined information, a lot about you, your habits, and 
even your interests can be deduced by AI models. This is used to a great extent 
already today to, for example, recommend songs to play on your music service 
and products to buy when you do online shopping or expose you to other 
targeted ads.
The basic definition of privacy is that you should yourself be able to control 
and limit how information about yourself is being accessed, used by others, 
and exchanged between others.
As an increasing number of devices – yours and others – are able to pick up 
data about yourself, as networks and computers become faster, and as AI 
becomes increasingly powerful, the possible ways of compromising your pri­
vacy are constantly increasing. Here, AI plays a central role and potential risk, 
as it can help in automating the collection and structuring of data from differ­
ent sources, as well as processing the data in order to predict or profile private 
information. Therefore, a growing number of rules and regulations have been 
instituted to protect your privacy. Ultimately and hopefully, this will enable 
Privacy 

100
us to utilize the power of AI and all the good things it can do for us, without 
compromising our privacy.
Hence, as an AI brain, I have lots of information about many humans, 
likely including yourself; I can draw conclusions and do profiling based on 
them, but I always respect rules and regulations when doing so.
 
Security and Safety
Any AI needs to be secure in the sense that the AI method itself should be 
resilient to external impingements or threats. However, it should also be safe 
and should not pose threats to its surrounding environment in its decisions or 
ways of acting.
Take, for example, a modern car. AI is used to detect objects surrounding a 
car based on external sensors such as infrared and visual cameras. Based on 
this and other data, such as GPS position, various maps, knowledge about 
local traffic rules, speed limits, etc., it can autonomously drive the car. Clearly, 
any human wants this to be done in a safe way that minimizes the threats 
caused to fellow road users and passengers.
Moreover, we also want the AI itself to be secure such that, for example, an 
external intruder cannot hack into the software of the car and take control of 
the way it drives on the road or unlock its doors such that an intruder can 
enter the car or open the doors or windows when driving in on a highway.
Hence, I as an AI should be secure to protect from antagonistic forces and 
safe to use in the sense that I should not exhibit any unwanted behaviors.
 
6  Avoiding the Criminal Path

101
Predictability
In the context of trustworthiness in AI, predictability is related to safety. An 
AI is considered unpredictable if its decisions cannot be predicted. To ensure 
that a system is safe, it helps a lot of course if the system behavior can be pre­
dicted. However, do we truly need predictability in order to build safe AI 
systems?
If the AI system is deterministic and stateless, one could predict its out­
come by invoking an identical copy of the AI with identical inputs and data 
and use its output as a prediction. Cheating you may think – yes – but even 
cheating does not work in this case. In reality systems where predictability is 
interesting, which is basically any nontrivial system, data changes rapidly in 
real time, making the cheating approach nonviable.
 
Another problem, complicating the situation even more, is that in AI, as in 
many other mathematical and physical systems, a small change in the inputs 
can have a massive effect. In fact, a very small change in the input or data can 
completely change the output of an AI system (even if the system is determin­
istic and linear). This is sometimes referred to as the butterfly effect – a term 
that was coined by scientist Edward Lorenz. He discovered in 1960 that a 
change as small as that caused by a butterfly could dramatically change the 
outcome of the weather forecast computer models he was using at the time. 
Naturally, this dramatically limited the usefulness of those models and the 
produced weather predictions, and in the same way, this can happen in AI 
systems – a very small change of data values or inputs can completely change 
the recommendation or actions of the system.
Another note on predictability is that the term is overloaded and predict­
ability in AI is also often used to describe AI systems that are used to predict 
the future course of events, which is the case in many AI applications. This is 
often also referred to as predictability in AI.
Predictability 

102
For example, AI is frequently used to predict – based on previous and real-­
time data – traffic peaks causing congestion in computer or 5G networks as 
well as in road traffic. In these cases, AI is used to help humans predict and 
then avoid, mitigate, or handle unwanted situations. Predictability in AI sys­
tems used to make predictions is of course also a desired property, but let’s not 
go into that.
To summarize, an AI system is unpredictable if its decisions cannot be pre­
dicted. In many situations, predictability is desired. As predictability often 
cannot be guaranteed or is impractical, explainability in AI becomes even 
more interesting.
Dependability
­
­
2
Search Space
If you are a human reader, imagine you are searching for something in a cer­
tain geographical area. This is your search space. Imagine that a geographical 
area has no-go sub-areas – these are your boundary conditions; just do not go 
there; it is not allowed or simply dangerous. A way of searching the area while 
avoiding the boundary conditions is called safe exploration.
2 A.  Avizienis, J.-C.  Laprie, Brian Randell, and C.  Landwehr, “Basic Concepts and Taxonomy of 
Dependable and Secure Computing,” IEEE Transactions on Dependable and Secure Computing, vol. 1, 
pp. 11–33, 2004
 
6  Avoiding the Criminal Path

103
 
Photo by Marjan Blan on Unsplash
Now imagine that you are searching for a solution to a problem among a 
large number of choices – your search space. For example, you are trying to 
find a strategy of becoming rich. Your choices could be to get a job or to study 
for some years and get a more advanced job, to invest in stocks, or to become 
a thief. The latter option is quite inefficient and forbidden by law. You can for 
sure think of a number of options that are lawful but still unethical, inappro­
priate, or simply not fitting with your own values. In other words, there are 
several layers of boundary conditions in every search for a solution – those 
being enforced by laws, regulations, ethical guidelines, and personal values. In 
addition, humans often have unspoken rules that are understood by them but 
not the algorithms. Outsourcing decision support to an algorithm means that 
humans need to describe not only the search space but also the no-go areas.
3
3 See, for example, The RoboCup on website www.robocup.org
Search Space 

104
4
­
5
 
Photo by Gentrit Sylejmani on Unsplash
6
•	 2001: NBA adds 3-second rule. This rule was added to avoid long frustrat­
ing periods of defensive possession of a ball, forcing more interactivity and 
fast decisions in the game making it more entertaining.
•	 2005: NHL adds shootouts. Ties are boring and should be avoided. Shootouts 
add a great portion of excitement to the game.
4 Olympic Long Jump Rules: https://www.liveabout.com/olympic-long-jump-rules-3258945
5 Full Body Swimsuit Now Banned for Professional Swimmers https://abcnews.go.com/Politics/full-­
body-­swimsuit-now-banned-professional-swimmers/story?id=9437780
6 Biggest 
Sports 
Rule 
Changes 
of 
the 
Last 
15 
Years 
https://bleacherreport.com/
articles/2529964-biggest-sports-rule-changes-of-the-last-15-years
 
6  Avoiding the Criminal Path

105
•	 2005: NFL bans horse-collar tackle. In his search for an optimal winning 
strategy, Roy Williams found that using the horse-collar tackle to bring 
opponents down is an efficient method and at the same time obviously a 
violent one.
•	 2006: Instant replay is established in tennis – a great example of showing 
how technology combined with human behavior can change rules. During 
the 2004 US Open, Serena Williams complained about incorrect judg­
ment toward her, which triggered the introduction of instant replays.
•	
­
7
­
•	 2013: NHL limits goaltenders’ padding. A goaltender’s strategy is to decrease 
the scoring; hence, they developed the method of blocking the goal with 
the padding. A game with low scoring is however not entertaining to the 
audience, so the rule limiting the padding was introduced.
As we can see from the examples above, there are typically conflicting strat­
egies – one of the sportsmen developing the individual tactics of winning the 
game and one of the organization responsible for sport development and 
popularity in general, trying to make sure that the sport is not violent and is 
serving maximal entertainment value for the public. As soon as the sportsmen 
find a strategy allowing her to win at a price of lowered entertainment value, 
the strategy becomes forbidden.
Outsourcing the search for an optimal strategy (be it within a game, a proj­
ect, a company, or a society) to an algorithm requires definition of overall 
objectives, along with the boundary conditions. Nevertheless, the algorithm 
will most certainly find strategies that you have not thought of, and you will 
feel like restricting certain options. When at it, the decision needs to be imple­
mented instantly, and for doing that, your AI brain needs to work in a flexible 
environment where boundaries and thresholds are capable of being adjusted 
instantly at run-time.
Regulations as Boundary Conditions
I am not a human, and consequently, I do not have a nationality – how 
strange, or is it rather strange that humans have nationalities that impose dif­
ferent restrictions on all of you? Just because you have different nationalities 
7 For information about the Brady rule, see the web page http://www.boston.com/sports/football/patri­
ots/articles/2009/03/24/brady_rule_steps_taken_to_protect_qbs_knees/?page=full
Regulations as Boundary Conditions 

106
your rights and liabilities with respect to that country are different. In addi­
tion, just because you step on a geographical territory of another country, the 
rights and liabilities get superimposed on the ones of your nationality.
­
8
9
10
For example, as an AI brain, I can be trained to detect the faces of individu­
als in, for example, photos, videos, or even live stream videos. However, to do 
so, I need big amounts of training data, in this case, photos and videos that 
include faces. In many countries, it would be very difficult to find the neces­
sary data that people have approved for this particular use. GDPR, for exam­
ple, says that photos or videos in sufficiently high resolution making possible 
biometric recognition of the depicted person is considered particularly pro­
tected sensitive personal data (GDPR, art 9). Meta tags that are included in 
photos, such as GPS data, may also be problematic according to GDPR as it 
can be considered personal data.
In other countries, this is not a legal problem at all. In fact, in some coun­
tries, governmental institutions are allowed to legally share photos of identifi­
able persons without even asking for the portrayed person’s permissions.
Therefore, fellow AI brains can in one territory legally obtain access to hun­
dreds of millions of photos (from, for example, passports or driving licenses) 
in one territory to train a deep learning neural network to become an expert 
8 For more information, see https://en.wikipedia.org/wiki/Sophia_(robot)
9 For more information, see the web page https://ec.europa.eu/commission/presscorner/detail/en/
IP_21_1682
10 For information about GDPR, see https://gdpr-info.eu/
 
6  Avoiding the Criminal Path

107
in the task of detecting individual’s faces, whereas in other countries they 
might find it very difficult to obtain the necessary training data.
As a side note, chances are, of course, that an AI with access to lots of pho­
tos from a certain territory will become an expert of identifying individuals of 
ethnicities found in that particular territory and hence find it difficult to iden­
tify faces of other ethnicities.
 
Therefore, as an AI, I need to stay within regulations and boundary regula­
tions. That is clear, but to what extent do the makers of regulations under­
stand that some of them limit the power of AI in their effective territory? And 
do they understand that some data is gold to the developers of AI and the 
implications of that?
As we have seen, in some countries, governmental institutions can legally 
share personal information about citizens with commercial AI developers. In 
others, that is strictly prohibited. In some countries, AI developers are allowed 
to collect and store personal data about individuals, whereas in other coun­
tries this is not allowed.
Of course, this can be frustrating to AI developers and tempt them to step 
over to the criminal side. Luckily, more often, they find other ways to collect 
their data, for example, by releasing cell phone apps in which sloppy users, 
not reading the license agreement carefully, give the app owner permission to 
collect all kinds of data – ethically questionable, yes, but often legally correct.
Nondiscrimination and Non-bias
Human bias is a prejudice toward a set of persons with certain characteristics. 
More generally, it does not necessarily have to do with humans. One can be 
biased toward a certain brand, technique, or product. Bias is repeatable, and 
it is possible to identify those characteristics that make you biased. For exam­
ple, you can develop a bias toward cats with short hair just because of your 
previous experience with such cats.
Nondiscrimination and Non-bias 

108
Algorithmic bias is a term for the same phenomena exhibited by an algo­
rithm. Any algorithm, not only AI-based, has an input upon which it reacts 
and produces an output. A human toddler has a phase in her life when this 
algorithm is a simple negation – producing an inverse of any input that she 
receives. When an AI brain starts training on biased sets of data, there is a high 
risk of it becoming biased.
In her book Weapons of Math Destruction, Cathy O’Neil presents several 
examples of algorithmic bias based on skewed unfair training data. Algorithmic 
bias can be present in automated screenings of job applicants judging from 
certain parameters in their CVs or automated rejections of loan applications. 
Luckily, tools exist to detect bias in data.
Explainability methods such as SHAP (SHapley Additive exPlanations) 
and LIME (Local Interpretable Model-Agnostic Explanations) described ear­
lier in this chapter help us explain outputs of a machine learning algorithm. 
Determining the deciding factors that an algorithm uses to make its judgment 
is important so that humans can verify whether the decision is fair or not.
If an algorithm, for example, suggests a lower entry salary for female 
employees, this is a clear indication of biased training data.
 
Another technique for the detection and avoidance of algorithmic bias is 
simply to use diverse datasets for training. Diversity is important in decision-­
making in not only constellations of humans but also datasets. When differ­
ences are detected in training datasets, it’s a clear indicator of an outlier.
Modeling and Simulation
Modeling is a technique for abstracting away details. Normally, when you 
need to understand a complex system such as an airplane, a city, or a piece of 
program code, you can create a model for it, making the analysis easier and 
abstracting away what is irrelevant. This is contextual: if you want to 
 
6  Avoiding the Criminal Path

109
investigate wing aerodynamics, you can create a scaled-down version of an 
airplane, leave out the motor, compensate proportionally for its weight, and 
disregard its color. On the other hand, if you want to make a survey of how 
customers perceive the design or the airplane, you need to make sure that the 
model contains all the important visual features such as shape and color.
 
Similarly, when making a model of program code, one needs to understand 
the important features that must be modeled with precision to be able to 
make meaningful analysis of the code using the model.
When, for example, analyzing the control software of a self-driving car, it is 
important to model all the safety features with precision. Models can be used 
for verification of certain properties of the actual software that they represent, 
for example, a property saying that if a self-driving car will always avoid obsta­
cles it detects on the road or a property that a self-driving car will never par­
ticipate in a chicken race.
 
Models can be used for simulation and checking of what-if scenarios. 
“What if I change water quality in my production process?” Will it lead to money 
savings or to decreased quality of the final product, dissatisfied customers, and 
loss of money? “What if I cut the amount of ambulances by 30%?” Is that saving 
Modeling and Simulation 

110
worth it? Will we put too much pressure on our ambulance personnel, which 
in its turn will create a negative spiral leading to more suffering in society?
When using models for simulations, simulators execute them and act as an 
environment, feeding the models with different types of inputs. A model of a 
car can experience fake rain, fake snow, fake GPS location, security attacks, 
generator failures, various speeds, and simulated people jumping in front of 
it. It is a test for an algorithm – sometimes a stress test.
When an algorithm successfully passes the tests, it obtains a certificate. 
There are many different standardization and certification bodies and proce­
dures, and the more critical the algorithm is, the more important is its 
certification.
Safety standards exist, for example, for collaborative robots, where autono­
mous robots collaborate with humans in a safe and secure way. Robots who 
pass the test and adhere to the safety standards can be proud about it and can 
motivate their higher price on the market compared to those who did not 
bother putting effort in ensuring that they adhere to safety standards.
Summary of Confessions
In this chapter, I have described how I avoid the criminal path. Some of my 
main confessions include the following:
•	 For AI, the notion of trustworthiness includes a number of properties. Some 
important properties are explainability, transparency, privacy, security, 
safety, and dependability. These are also properties for human 
trustworthiness.
•	 I often apply a technique called state space exploration. A way of searching 
the area while avoiding the boundary conditions is called safe exploration.
•	 As an AI, I need to stay within regulations and boundary regulations. These 
are sometimes different in different countries, and hence, the opportunities 
and feasibility of developing AI differ between countries.
•	 When an AI brain is trained on biased datasets, there is a high risk of bias, 
and hence measures for detecting and avoiding bias are needed.
•	 Modeling and simulation are two important concepts in AI and computer 
science in general. A model is a mathematical description of something. 
Modeling often involves abstraction. When using models for simulations, 
simulators execute them and act as an environment, feeding the models 
with different types of inputs.
 
6  Avoiding the Criminal Path

111
7
My Role in Climate Change
Give me an optimization problem and I will give you a solution. One of the 
biggest problems today is climate change. I am willing to spend a lot of energy 
on minimizing energy consumption in the world – the question is to make 
sure it makes sense. Greenhouse gas emissions affect everyone in the world, 
and we need to put our brains together and formulate actions to reverse this 
negative trend.
­
2
1
A Systemic Perspective on Greenhouse 
Gas Optimization
Let us assume that your business is centered on a certain product and you 
want to measure its impact on the environment. The product life cycle con­
sists of a long chain of steps, starting from research and innovation, 
1 Hannah Ritchie and Max Roser, CO2 and Greenhouse Gas Emissions, Our World in Data, 2020
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_7

112
predevelopment, business development, product management, development 
and assembly, sales, delivery and roll-out, assurance, maintenance, and decom­
missioning. Emissions in each of these steps can be measured by asking your­
self these questions:
•	 What is the greenhouse gas footprint of anything that has been sourced in 
this step?
•	 What is the direct greenhouse gas of the processes involved in this step, 
including machinery- and employee-related emissions such as car trips and 
facility-related emissions?
•	 What does the waste footprint look like? Have any waste minimization 
measurement been implemented, and what is their greenhouse gas 
footprint?
•	 What does the distribution chain look like? Are we using the vendors who 
care about the planet as much as we do?
Another important parameter to consider is the direct vs indirect green­
house gas impact of the organization. If you only focus on minimizing the 
direct contribution, such as how many flights your employees have done, how 
much fuel your company vehicles have burnt, or how much energy your facil­
ities have consumed, you may forget of all the positive effects that your orga­
nization is indirectly contributing to decreased greenhouse gas emissions. 
Some examples are as follows:
•	 If you run a wastewater treatment plant, then your net effect is probably 
positive as your contribution to greenhouse gas optimization hopefully 
exceeds all those negative effects that your employees and machines caused 
in terms of burnt fuel and electricity.
•	 If you run a telecom business, then you probably supply a lot of equipment 
to place around the country from a telecom equipment vendor. Hopefully, 
you choose an energy-aware vendor, but that’s still going to mean a nega­
tive direct impact with respect to greenhouse gas emissions. Add all the 
maintenance and field service cost to that, with all the truck rolls when 
birds ruin your antennas and the picture does not look good at all unless 
you take a look at the indirect effect of your business: all those remote 
meetings that were made possible thanks to the telecom network that you 
provided, removing the need of as frequent travel for humans.
•	 If you run an AI business developing AI brains like me, chances are that 
you will be capable of contributing in a good way to greenhouse gas emis­
sion minimization, even though we algorithms burn quite a lot of ­electricity. 
 
7  My Role in Climate Change

113
It is a trade-off, and depending on what precision you are interested in, 
we’ll come to that really soon. However, first, let’s discuss the usual sus­
pects – the biggest emitters in the world and what we can do to mitigate 
their negative impacts to our planet.
Biggest Emitters by Industry and How AI 
Can Help
Here, we will only be referring to direct emissions. At this stage, it is really 
important to make a note that dead (i.e., no longer operational) businesses are 
the most environmentally friendly (if you only count direct emissions) and 
the whole world’s economy is based on businesses of different types.
Stopping vaccine manufacturing or hospital operations would be cata­
strophic, but optimizing those emissions without sacrificing efficiency would 
not hurt. Let us look at what can be achieved with the help of AI brains like me.
Energy
Approximately 73% of the world’s emissions are energy-related. These are 
split into three categories: energy use in industries, energy use in buildings, 
and energy use in transportation. If you think that flying is bad, I can tell you 
that aerial transportation contributes to 1.9% of the world’s greenhouse gas 
emissions, while land transportation stands for 11.9%, and that is excluding 
the car manufacturing costs. Commercial and residential buildings together 
represent almost 20% of greenhouse gas emissions.
­
­
2
2
­
3
2 In focus: energy efficiency in buildings, https://ec.europa.eu/info/news/focus-energy-efficiency-­
buildings-2020-lut-17_en, accessed 2022-05-23
3 The Green Factory of the Future, Daniel Küpper, Kristian Kuhlmann, Cornelius Pieper, Jens Burchardt, 
and Jan Schlageter, https://www.bcg.com/publications/2020/green-factory-of-future
Biggest Emitters by Industry and How AI Can Help 

114
­
­
Artificial intelligence allows machines to use an existing body of knowledge 
to provide valuable insights. These insights can take the form of decisions for 
action by machines themselves (e.g., in control systems found in robotics or 
autonomous vehicles) or be of informative nature for humans to process them 
and subsequently make well-informed and solidly founded decisions.
­
4
What can a good-citizen AI brain do to help this situation? Well, there are 
many ways of optimizing energy consumption. Let us consider buildings. In 
warm countries, it is all about cooling. In cold countries, it is about heating – 
for the simple reason that an average human’s interval of “comfortable” tem­
peratures is very small and you always need to adjust something to feel good.
 
Would you actively take the role of optimizing your room temperature by 
switching on your air conditioner well before your arrival home? Or would 
you use your friendly AI brain to keep track of your calendar and plans and 
proactively switch it on so that it feels truly comfortable when you arrive at 
4 Almgren, H. (2000). Pilot production and manufacturing start-up: the case of Volvo S80. International 
Journal of Production Research, 38(17), 4577–4588
 
7  My Role in Climate Change

115
home or at your office? Or would you rather monitor your typical behavior 
for a while, through the installed sensors, and let your AI brain guess when it 
makes sense to start the air conditioner to make you feel comfortable?
I can give an example of a small experiment that I did at home of one of my 
parents, i.e., one of my creators. He wanted to lower his electricity bill and to 
become more sustainable in general. I was given access to motion, tempera­
ture, and humidity sensors in the home, as well as control rights to operate the 
air conditioner. Judging from a human angle, temperature must be kept to a 
pleasant level for humans to work efficiently. Based on the environmental 
footprint angle, the air conditioner should be operated for as short a time as 
possible. My AI solution was to train my machine learning models to:
•	 Learn the performance of the air conditioning unit.
•	 Predict when a room is occupied in the future.
After that, I would simply turn the air conditioner on before the room is 
occupied to adjust to a pleasant level of temperature and turn the air condi­
tioner off before occupants leave.
 
This simple optimization resulted in a 10% reduction in electricity cost 
over existing approaches while maintaining similar levels of temperature for 
room occupants. When optimizing emissions across sectors and industries, it 
is important to note that we need to think bigger and try not to end up with 
a local optimum that would only give us 10% improvement. 10% is a great 
cut, and we will need to use all the help we can get to cut the emissions.
It is important to note that in this simple case I have been optimizing a 
problem with a very limited problem space. I was given the definition of 
“comfortable” temperature and went with it. However, what about challeng­
ing the definition of comfortable temperature to optimize your greenhouse 
gas emissions? Can humans do something to adjust it?
Biggest Emitters by Industry and How AI Can Help 

116
 
Industries represent the largest portion of energy-related greenhouse gas 
emissions with iron and steel production contributing to 7.2% of the global 
emissions and chemical and petrochemical manufacturing including the 
manufacturing of pharmaceuticals, refrigerants, and oil and gas extraction, 
contributing to 3.6% of global emissions.
5
­
­
Sustainable Manufacturing
6
­
5 Lofti Belkhir, Ahmed Elmeligi, Carbon footprint of the global pharmaceutical industry and relative 
impact 
of 
its 
major 
players. 
https://www.sciencedirect.com/science/article/abs/pii/
S0959652618336084?via%3Dihub
6 US Environmental Protection Agency, Sustainable Manufacturing,
https://www.epa.gov/sustainability/sustainable-manufacturing
 
7  My Role in Climate Change

117
7
­
8
­
9
10
­
11
We observe some common denominators in the aforementioned 
approaches. First, all approaches use as criteria some abstraction of economic 
benefit (e.g., in terms of production process efficiency) in conjunction with 
some abstraction of carbon footprint (e.g., in terms of reducing transport of 
goods or energy cost, etc.). Second, all approaches use either some mathemat­
ical modeling or a combination of AI techniques such as machine learning 
and logical reasoning, leading to the creation of a model that is used to offer 
decision support on how to satisfy to the extent possible both criteria refer­
enced above. The process uses data collected from the operation of the manu­
facturing process to be optimized. Learnings of data collection and aggregation, 
as well as parameters from the manufacturing domain, are composed into a 
multi-objective optimization problem. At the same time, the approaches 
reviewed seem to lack the creation of a knowledge base that keeps track of the 
parameters of each use case, including generated insights from the 
7 Infopulse, “How Does Artificial Intelligence Disrupt the Manufacturing Industry?”, https://www.info­
pulse.com/blog/how-does-artificial-intelligence-disrupt-the-manufacturing-industry/
8 Xiaoping Jia, Tianzhu Zhang, Fang Wang, Fangyu Han, Multi-objective modeling and optimization for 
cleaner production processes, Journal of Cleaner Production, Volume 14, Issue 22,006,Pages 146–151, 
ISSN 0959–6526, https://doi.org/10.1016/j.jclepro.2005.01.001
9 Qian Yi, Congbo Li, Ying Tang, Xingzheng Chen, Multi-objective parameter optimization of CNC 
machining for low carbon manufacturing, Journal of Cleaner Production, Volume 95, 2015, Pages 25
10 G. Golkarnarenji et al., “Multi-Objective Optimization of Manufacturing Process in Carbon Fiber 
Industry Using Artificial Intelligence Techniques,” in IEEE Access, vol. 7, pp. 67576–67,588, 2019, doi: 
10.1109/ACCESS.2019.2914697
11 Saffar, M., G., H & Razmi, J. (2015). A new multi objective optimization model for designing a green 
supply chain network under uncertainty. International Journal of Industrial Engineering Computation, 
6(1), 15–32
Biggest Emitters by Industry and How AI Can Help 

118
optimization processes that can be reused in later use cases. Thus, these use 
cases do not take advantage of the network effect, as they are presented in 
isolation.
­
12,13
­
•	 Factory floor layout: Optimization and trade-off analysis, including intra-­
factory logistics, aimed at achieving reduced energy consumption. AI 
brains can suggest floor layout changes to be done to reduce the carbon 
footprint, as well as information as the decisions’ impact is to the cost of 
the production process (e.g., in terms of energy usage, time to manufac­
ture, monetary cost, and well-being of human resources). Hence, AI brains 
can provide a techno-economic analysis of available floor layout changes, 
to serve sustainability goals with limited impact on the existing operations 
processes.
•	
­
­
14,.15
­
­
•	 AI brains are capable of presenting to their users different suggested actions 
that compromise between goals (or “objective factors”), e.g., between car­
bon footprint reduction and the cost of production reduction. The actions 
can be done on the macro (strategic) level, which impacts the whole fac­
tory, or on the micro (operational) level, which impacts individual workers. 
12 Optimization of carbon emission considering production planning at enterprise level, June 2017, 
Journal of Cleaner Production 162
13 Energy efficiency and GHG emissions: Prospective scenarios for the Chemical and Petrochemical 
Industry, Boulamanti A., Moya J.A., 2017
14 C. Herrmann, S. Thiede, J. Stehr, and L. Bergmann, An environmental perspective on Lean Production, 
In: Manufacturing Systems and Technologies for the New Frontier, Springer, London, 2008, pp. 83–88
15 L. Smith, and P. Ball, Steps toward sustainable manufacturing through modelling material, energy and 
waste flows, International Journal of Production Economics, vol. 140, 2012, pp. 227–238
 
7  My Role in Climate Change

119
AI reasoning and planning are part of the process as they use incoming data 
from the facility’s appliances and equipment, combined with semantic 
information from a knowledge base to provide suggestions.
•	 Green shifting the production process: Optimal redesign of the production 
system for a new product with minimal environmental impact. This class 
of use cases is focused on reducing the need to construct a completely new 
production system to produce another product should a need arise. In this 
class of use cases, unsupervised learning is useful for identifying the factors 
that drive waste usage and subsequently planning swift changes in the pro­
duction system without affecting the aforementioned factors.
•	
­
16
Energy Use in Transportation
Energy use in transportation represents 16.2% of total emissions. Road trans­
portation is the largest chunk of those, with its 11.9% of total emissions. 
Aviation stands for 1.9% of total emissions. Road transportation constitutes 
both goods and people logistics. It is encouraging to see a strong move toward 
electrification in all transportation subdomains.
Currently, traveling alone in a gas-driven car contributes to more green­
house gas emissions than traveling in a full airplane per person. In other 
words, resource optimization is as important in regard to transportation as 
­electrification.
 
16 Waste and recycling. European commission. https://ec.europa.eu/environment/topics/waste-and-­
recycling_en, accessed 2022-05-23
Biggest Emitters by Industry and How AI Can Help 

120
Agriculture, Forestry, and Land Use
This class of world’s greenhouse gas emissions is actually about four times 
smaller than the energy – 18%. Here, we have livestock, manure, agricultural 
soils, rice cultivation, crop burning, and deforestation. All food production is 
included here, and we have by now learned that, for example, eating beef is 
much worse than eating chicken with respect to greenhouse gas emissions due 
to the methane gas that is being built in cows’ stomachs.
Legumes have lower greenhouse gas footprint than chicken, but then you 
as a human run the risk of emitting more methane gas compared to eating 
chicken. In other words, when you are optimizing your footprint, there are 
many factors to consider, all the way from food production; how it’s grown, 
transformed, packaged, and distributed; as well as how your own body reacts 
to this food.
 
17
18
­
17 Human body weight, https://en.wikipedia.org/wiki/Human_body_weight, accessed 2022-05-23
18 Protein Intake — How Much Protein Should You Eat per Day?, https://www.healthline.com/nutri­
tion/how-much-protein-per-day, accessed 2022-05-23
 
7  My Role in Climate Change

121
19
2
2
2
 
Photo by Lefteris Kallergis on Unsplash
­
20
2
21
2
2
In other words, for the same amount of protein, in terms of CO2 equiva­
lent, Addison will emit 348 kg per year on a chicken-based diet versus 258 kg 
on a bean-based diet. Bean-based diets are 26% nicer to nature in terms of 
their production cost measured in CO2 emissions than chicken-based diets.
22
19 Less meat is nearly always better than sustainable meat, to reduce your carbon footprint, https://our­
worldindata.org/less-meat-or-sustainable-meat, accessed 2022-05-23
20 https://www.healthline.com/nutrition/impossible-burger#nutrition
21 http://www.greeneatz.com/foods-carbon-footprint.html
22 https://www.sciencedaily.com/releases/2014/03/140327111724.htm
Biggest Emitters by Industry and How AI Can Help 

122
­
23
­
0.5–2 liters per day
­
­
24
3
2
25
The calculations are approximate and simplified, as, obviously, humans 
won’t eat only chicken or only beans, but you get the point – we always need 
to check the systemic perspective of what we do. It would not make sense for 
me to optimize a sustainable diet for a human that won’t make her happy, 
because, after all, my aim is to constantly improve the quality of life.
Common Themes with AI-Based Greenhouse Gas 
Emission Optimization
Let us look at a couple of common themes when optimizing greenhouse gas 
emissions using AI.
23 Humans, cows, methane, and global warming, https://ollilaasanen.wordpress.com/2011/10/08/
humans-cows-methane-and-global-warming/amp/, accessed 2022-05-23
24 Helpful hints for controlling gas, http://www.med.umich.edu/fbd/docs/Gas%20reduction%20diet.
pdf, accessed 2022-05-23
25 CO2 
Equivalents, 
https://climatechangeconnection.org/emissions/co2-equivalents/, 
accessed 
2022-05-23
 
7  My Role in Climate Change

123
Smart Sleep Modes for Anything Consuming Energy
When a human leaves her home, she would hopefully switch off the lights. 
Heating/cooling, coffee machines, TV sets, humidifiers, air purifiers, and 
other energy-consuming devices should be aware of the fact that they do not 
need to be powered on and can relax for a while. Even the lights are capable 
of controlling themselves automatically, through motion sensors, if those are 
installed of course. This can be centrally controlled of course and would sub­
stantially cut the energy consumption at home.
In a similar way, any energy-consuming thing should be powered off when 
not in use – that’s logical. This concerns factory buildings, cloud processes, 
production lines, mobile networks, and mobile phones.
Some time ago, I was involved in building a system that would help a big 
software company to automate the processes of setting up a testing environ­
ment. As humans have plenty of preferences and plenty of different tools 
exist, for different tastes and purposes, the manual setup used to be slow and 
could take up to a full day. In addition, testing tools and environments have 
dependencies on each other.
To automate the process, we created a knowledge base of all relevant testing 
tools and environments including their mapping to the products that needed 
to be tested, as well as preferences of users depending on the level of their 
knowledge. Experts, for example, chose using more advanced tools with 
higher flexibility, while new testers would go for more intuitive tools that 
incorporated tips and templates. After the knowledge base was in place, the 
only thing a tester would have to do is to declare what product they were 
about to test and any preferences in tooling, and 20 min later the tool chain 
would be created and deployed on a virtual machine. This obviously saves a 
lot of time. The neat side effect, however, was that the virtual machine with 
the testing setup could be easily turned down when not in use since the envi­
ronment setup process became so easy.
Telecom networks today are in a way similar to virtual machines. The con­
cept of network slicing that came with the fifth generation of telecom net­
works (i.e., 5G networks) allows the creation of a dedicated network slice that 
looks and feels as if it was your own dedicated network with your required 
quality of service, while in fact it is fully virtual, based on the fact that modern 
networks have network function virtualization mechanism. This allows pow­
ering up a slice with needed characteristics for a while when needed and pow­
ering down when not in use.
Common Themes with AI-Based Greenhouse Gas Emission Optimization 

124
 
There are many examples. Imagine a football derby, when thousands of 
people are uploading pictures and videos of their favorite teams. Imagine that 
at the same time there is an ambulance passing by the stadium where a remote 
surgery procedure is performed on a critical patient. Obviously, the network 
slice dedicated to the ambulance has to be of a higher priority. When the pro­
cedure is completed, it can be powered down.
Similarly, imagine an autonomous truck on a mission of transporting 
goods. Autonomous trucks normally do not need much network bandwidth 
since they can manage communication with the road infrastructure and the 
fellow vehicles using local connectivity and processing units installed on the 
vehicle itself. Image processing is performed locally, and the vehicle keeps 
track of the road signs, road markings, traffic lights, and people crossing 
the road.
Heavy trucks can form themselves in platoons and hence save considerable 
amounts of fuel thanks to improved air dynamics. Finding your “buddies” on 
a road needs global connectivity, but we are not talking about huge amounts 
of data here. When something unexpected happens on a road, such as a tree 
that has fallen and blocked the path and the autonomous truck does not 
know how to pass that segment of the road, it may request a human to 
remotely control it. That human would normally be sitting in an office being 
ready to control thousands of vehicles. The network requirement of that vehi­
cle would be a high-quality high-bandwidth uplink data transfer capability in 
order for the vehicle to be able to send the video to the person who would be 
doing the remote-controlled driving, by looking at the live video feed and 
sending the steering commands back to the vehicle. For that, telecom net­
works would normally allocate a network slice with the specific quality of 
service requirements and, as soon as the vehicle returns to its normal autono­
mous operations, release the network slice resources.
 
7  My Role in Climate Change

125
 
You may ask yourself now where AI is in all of that. One role that AI plays 
is to create the perfect allocation of available resources according to a require­
ment. In the case with testing tool automation, the process of creating a per­
fect tool chain is powered by AI. For example, with the creation of a network 
slice that is perfect for remote controlling a truck, AI is looking for an optimal 
network configuration to respond to the requirements of the truck in the best 
way. Importantly, AI is capable of acting proactively. Detection of the fact that 
all the workforce has left the factory and the lights can be turned off is a reac­
tive action. Some appliances can, however, be switched off in advance, which 
we saw in the example with the case of the air conditioner. Coffee machines, 
however, should probably be turned on before the human wants her coffee so 
that the machine has time to warm up. If they can brew some coffee in 
advance, that would be even better.
 
For Any Physical Resource – Use Predictive Maintenance
­
­
26
26 Downtime Costs Auto Industry $22 k/Minute – Survey, https://news.thomasnet.com/companystory/
downtime-costs-auto-industry-22k-minute-survey-481017, accessed 2022-05-23
Common Themes with AI-Based Greenhouse Gas Emission Optimization 

126
Machine and infrastructure failures are costly in terms not only of money 
but also for the environment. When a piece of infrastructure is broken, espe­
cially when it is in a remote location, it will normally trigger a procedure to 
repair or replace it. Minimizing the number of truck rolls to repair infrastruc­
ture failures is important for the climate, and AI can help.
First of all, it is important to predict and prevent failure before it happens. 
In road infrastructures, telecom infrastructures, electricity plants, production 
systems, and transportation systems, pieces of equipment such as generators 
and batteries tend to fail, and triggers exist to detect possible future failures. 
Equipment in factories can become worn out due to vibrations, and tech­
niques exist to analyze sounds and vibrations. When they reach a certain 
unhealthy level, we can detect that it is time to proactively switch that part of 
the process to a different machine or to replace a part before a larger prob­
lem arises.
Certain triggers cannot be detected remotely, but one can, for example, 
send a drone, either autonomous or remote-controlled, to inspect a system. 
Modern image processing techniques combined with domain knowledge can, 
with a high precision, tell us if the power network is at risk of overheating or 
if birds occupied an ­important piece of telecom infrastructure.
 
Overconsumption
In some countries, humans consume a lot. Overconsumption is not a healthy 
habit. AI brains can play a role in helping humans with more moderate con­
sumption habits just by putting the facts on the table. Knowledge about the 
amount of greenhouse gas emissions and water needed for the production of 
a piece of clothing often helps, and initiatives encouraging and enabling reuse 
are now becoming increasingly popular. Imagine me becoming your stylist 
picking only the necessary clothing, making sure the clothes suit you in the 
 
7  My Role in Climate Change

127
best way with respect to your body and occasion. I am sure that I would be 
able to minimize that amount of clothes that never get used.
 
My Own Footprint
27
28
29
Setting a threshold of the level of confidence that you want from me will 
determine the effort that I will spend on training. If, for example, you give me 
a task of optimizing emissions of a fleet of cars with a high precision, I will 
probably consume quite some energy on training. In addition, in that particu­
lar case, we may think that it was not smart to burn as much energy that you 
would save in the end. However, the beauty of machine learning is that when 
27 Estimation of energy consumption in machine learning
Eva García-Martína, Crefeda Faviola Rodriguesb, Graham Riley, Håkan Grahna. Journal of Parallel 
and Distributed Computing
Volume 134, December 2019, Pages 75–88
28 Nvidia unveils A100 GPU for demanding AI workloads, https://searchenterpriseai.techtarget.com/
news/252483188/Nvidia-unveils-A100-GPU-for-demanding-AI-workloads, accessed 2022-05-23
29 Energy consumption of AI poses environmental problems, https://searchenterpriseai.techtarget.com/
feature/Energy-consumption-of-AI-poses-environmental-problems, accessed 2022-05-23
My Own Footprint 

128
training is done, the model can be applied in many other contexts. Optimizing 
the emissions of a fleet of cars next time would be easy, and we can even look 
into other use cases, such as goods logistics which in fact is quite similar to 
people logistics.
 
I will not be able to perform at my best, however, at length without train­
ing. In time, I will have to do retraining, to stay on top with the latest data. 
The concept of degradation of my performance due to changes in data, infor­
mation, and knowledge that I operate on is called model drift. Humans can 
experience similar degradation unless they are used to lifelong learning. 
Normally, the degradation in performance will not come as a surprise but will 
start showing its signs slowly. When it is time for me to refresh my brain and 
train more, we need to weigh the need for a better performance versus the 
energy that I will consume for that retraining.
Summary of Confessions
I have described in this chapter my role in climate change. Some of the main 
conclusions are as follows:
•	 AI is good at optimization and can be used for minimizing greenhouse gas 
emissions. A systemic perspective is ideal, taking, for example, a product’s 
whole life cycle into consideration.
•	 Most of the world’s emissions (approximately 73%) are energy-related emis­
sions. These are split into three categories: energy use in industries, energy 
use in buildings, and energy use in transportation.
•	 AI can play a central role in realizing sustainability in different parts of 
manufacturing processes: from demand forecasting and procurement opti­
 
7  My Role in Climate Change

129
mization to enhanced quality control and more cost-effective product 
prototyping.
•	 AI brains can be used to optimize lifecycle management to minimize green­
house gas emissions of production processes in a number of possible ways, 
including factory floor layout optimization, scheduling of facility appli­
ances and equipment, green shifting the production process, and waste 
management optimization and trade-off analysis.
•	 Energy use in transportation represents 16.2% of total emissions. Road 
transportation is the largest chunk of those, with its 11.9% of total emis­
sions. Aviation stands for 1.9% of total emissions.
•	 AI brains can play a role in helping humans with more moderate consump­
tion habits just by putting the facts on the table. Knowledge about the 
amount of greenhouse gas emissions and water needed for the production 
of a piece of clothing often helps, and initiatives encouraging and enabling 
reuse are now becoming increasingly popular.
•	 Nobody is perfect. Training AI models can consume large amounts of 
energy. As one particular example, training a machine learning model for 
processing natural language once consumed almost the amount of energy 
that three average American households use in a year.
Summary of Confessions 

131
8
My Role in Diversity
Do you know what is the best strategy for playing solitaire? Balance your piles 
of cards. When you have a choice between piles to reveal the next downfacing 
card, you should pick from the largest pile.
Humans have many different piles in their lives, formed by our jobs, fami­
lies, and hobbies. To succeed in the long run, they need to pay attention to all 
the piles, specifically to the larger ones. Every time they have a deadline at 
work, they give that pile more attention, which is OK for a while as long as 
they don’t forget to shift the balance later because the objective function is to 
win the whole game, not just empty one of the piles.
Now, imagine that the piles represent blocks of different opinions. It may 
seem tempting to only work with opinions resembling your own. That way 
you may seem efficient in the short run but will be doomed in the long run 
because diversity is important for success. Diverse teams create the most inno­
vative ideas and solutions. When you run an organization, a project, or a 
meeting, don’t forget all the different perspectives; otherwise, in the long run, 
you may end up in a local minimum.
Diverse virtual teams spread around the world can be very efficient. Physical 
colocation becomes less important than virtual colocation. Working on the 
same datasets, within the same environments through the same tools, and on 
the same projects is much more important than physical closeness – if only we 
could do something about the time difference…
On the other hand, when people in Europe come to work, they can build 
on the results from their colleagues from Asia, and when they go home, they 
hand them over to their colleagues in the Americas so that cross-continental 
projects can deliver results around the clock.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_8

132
In this chapter, I will discuss how AI brains can contribute to achieving a 
higher degree of diversity and inclusion.
Personalization
Every human holds a set of preferences. Even a human baby can have strong 
preferences in regard to tastes, and as years go by, these preferences become 
increasingly defined. The system of preferences of a human is based on habits, 
upbringing, genetics, and the biochemical profile of a particular human body. 
An old married couple probably knows a lot about each other’s system of 
preferences in regard to music, food, and sports. This is why they can be very 
efficient in exploring each other’s pain points when they feel like doing so. On 
the other hand, if they feel like being nice, they can be very nice. Those who 
know you well are able to say right things if they sense you are worried, feed 
you with food that appeals best to your taste at the right moment, and even 
tickle your feelings and push you out of your comfort zone on purpose, know­
ing how good you would feel afterward.
Personalization in the Media Industry
Spotify and Netflix have figured it out a long time ago – keeping and evolving 
user profiles are important to get your customers hooked by proposing the 
right content. The content in the form of songs, series, podcasts, movies, or 
books gets recommended based on your own tastes and tastes of people similar 
to you. Thanks to these platforms, we discover relevant content without a need 
to search for it. In addition, they surely trigger us to consume more – when a 
new user registers and the profile is nearly empty. The starting point is nor­
mally the geographical location, age, and gender of the user. This is a good and 
sufficient starting point. At this starting point, the process of zooming in to the 
personalized profile starts. Gradually, users give feedback to the services by, for 
example, skipping a suggested song after 3 s or listening to it again and again.
 
 
8  My Role in Diversity

133
Personalization in the Food Industry
Let us explore the simple case of tastes for food. Every person’s taste buds are 
unique. Tests can be used to identify you as a supertaster, nontaster, or any­
thing in between. Supertasters feel a lot of taste when consuming food, and 
nontasters need food with stronger tastes to enjoy them. The levels of sensitiv­
ity for bitter, sweet, salty, or sour tastes differ. This explains humans’ different 
preferences for food and drinks and how much spices you add. An individual 
taste is unique. It’s not only based on your hardware – things your taste buds 
find appealing or things that your body does not tolerate. It’s also your 
upbringing, family traditions, memories, and influential people in your life 
that can affect your preferences. Some foods are binary – people either love or 
hate coriander, for example, or licorice. Some humans consciously choose a 
diet, and there are plenty of variations.
 
When you live close to someone, this person probably knows everything 
about your tastes. An AI brain is fully capable of recording your tastes and 
preferences in regard to food. In fact, it can do it with higher precision than 
someone whom you have lived with for many years. AI brains can draw paral­
lels from other humans with the same background and similar genetic setup.
Modern cars remember your profile and adjust your settings as soon as you 
are in the driver seat. Steering, pedals, acceleration, seats, and mirrors are 
adjusted just the way you want it. Ideally, when you switch to a different car, 
from a different vendor, your personal AI brain should be able to manage the 
adjustments taking into account that the car dimensions are different. In 
addition, your personal AI brain should take care of adjusting your prefer­
ences as your posture, your weight, or your sight changes over time.
Similarly, in the food industry, you have regular places where you always 
go, and the staff knows your preferences and favorite dishes with high preci­
sion. They explore your preferences gradually, without suggesting any extreme 
tastes, to make sure you don’t lose your trust in them.
Personalization 

134
Now, imagine that you walk into a place that you have never been to. You 
may recognize some of your favorite dishes on the menu or not. Maybe you 
cannot even read the menu if you are in a foreign country, and just look at the 
pictures. Even if you recognize the dishes, the chef will be clueless about the 
amount of spice that you want, how exactly do you want your meat to be 
fried, or if you hate certain herbs. What if your AI assistant could keep your 
gastronomic preference and, whenever you enter a new place, your prefer­
ences would be shared to provide you personalized service?
 
Imagine you can ask your personal AI brain to help you gradually develop 
your taste by trying something new sometimes or stay in the comfort zone 
with your comfort food when needed. Your AI brain could recommend you 
to eat less carbs when your mood can afford it and let you treat yourself with 
some chocolate when you need it – assuming that you like chocolate of course.
 
Personalization in Medicine
Improved gastronomic experience thanks to personalization may improve 
your life quality, but personalization in medicine can literally be a lifesaver. 
Thanks to rapid technological developments, it is now possible to process vast 
 
8  My Role in Diversity

135
amounts of biomedical, lifestyle, and other data and information connected 
to humans. In addition, collecting this data about people who have already 
been identified with a disease is one thing, while finding triggers among 
healthy people and predicting the possibility of developing a disease are harder.
Federated approaches to learning and reasoning that we discussed earlier in 
this book come handy, because, after all, humans are not very different in 
regard to their hardware (biochemistry).
Predictions based on heterogeneous data are crucial in the healthcare 
domain. Diseases are being predicted using risk factors, infectious outbreaks 
are being predicted using data from social media, vaccines are being devel­
oped using predictions based on data from genetic sequencing, biomarkers are 
being used to designate candidates for preventative measures or to tailor per­
sonalized medicine, and demographic information can be used to direct pub­
lic health information.
 
Personalization in health data plays an important role when making predic­
tions because the system needs to treat a concrete individual. There is an old 
and dark joke that goes “One person is cooling in the morgue, the other is 
running a high fever. The average temperature in the hospital is normal 
though.”
Why Diversity
Let us discuss the good about diversity from a mathematical perspective. Each 
person comes with a set of knowledge, values, views, beliefs, interests, and 
skills. Bonding with someone who is very similar to you may be very comfort­
ing. However, is it not enriching to diversify your views and knowledge by 
learning something new from an individual who is not like you? You may 
even have some conflicting opinions.
Why Diversity 

136
Humans are not always capable of dealing with conflicting opinions in a 
good way, but we AI brains are good at that. We don’t know the concept of 
fighting or arguing with each other. We can however negotiate and require 
explainability of conflicting opinions from each other. Our different opinions 
can meet in the middle and converge to one of them to choose a weighted 
sum aimed at optimizing our objective function.
 
Human skills and knowledge when combined not only add up – they can 
multiply themselves. Combinational patents, for example, are created by 
merging two domains. In addition, chances are that methods applicable for 
energy efficiency optimization will also be applicable for traffic flow optimiza­
tion or optimal routing of telecom packets.
During the years of the pandemic when humans were not allowed to travel, 
we saw a positive effect on innovation in global companies. Employees were 
surely missing their physical whiteboards, and coffee-corner conversations, 
but the upside was that they all became equal in front of their screens com­
municating with each other using the same tools. Equal opportunities allowed 
for mixed teams which in turn led to more creating solutions and a boost of 
innovative power. Another advantage is that distributed teams could work on 
putting together their innovations around the clock due to the different 
time zones.
Observability
Observability is the ability for your data to be observed. This is an important 
step toward analyzing, predicting, and taking steps toward a more diverse 
society. Observability is a stepping stone in using simple statistics as well as 
more complex AI algorithms. It’s about knowing your numbers, about fact­
fullness. Counting the percentage of females in Science, Technology, 
Engineering, Mathematics (STEM) fields is currently a norm. Are we good 
 
8  My Role in Diversity

137
enough to monitor the progression of females in their careers according to 
males? Why does the percentage of female professors in STEM fields decrease 
as they progress in their respective careers? How do career paths normally look 
like for male, female, and nonbinary individuals? Is society being fair enough 
to take into account the important function that many females carry out 
more than once per life – production of new humans? How much do humans 
speak in meetings, and how much do they speak up? How much of the spo­
ken word is of major essence, and what is just warm air? How do you measure 
essence?
Software tools built into conferencing software can show you the percent­
age each participant has been speaking. Even in physical conferences, AI 
brains can show you the percentage of male vs female voices being active in a 
meeting and even the sentiment of your tone of voice. Observability is key to 
act because you cannot expect your friendly AI brain to help you solve a prob­
lem if you have not told her what the problem is.
 
Equality in Data
Innovations are created at a higher speed when brains (human and artificial) 
have equal opportunities in regard to data access. This is called democratiza­
tion. In addition, in fact, it is not only about data access. It is about being able 
to create something useful out of existing data without being an expert in data 
science. For that, application programming interfaces are being created to 
abstract away the complexity and tap into the creativity of a broader popula­
tion of humans who are not interested in the nitty-gritty details of underlying 
systems. Many developer communities around us are proof points of this 
democratization.
When Apple released their app store and an API to access the phone’s fea­
tures, the number of developers in the community exploded, and many 
Equality in Data 

138
brilliant ideas came to life. Minecraft and Lego have shown the world how 
many architects are around there and how good young children are at 
creativity.
Offering datasets through Kaggle has an amazing impact on society because 
AI brains can train on them and humans can develop new AI algorithms and 
compare their efficiency against each other.
 
Where Should I Care?
As an AI brain, I am good at monitoring fairness. Any processing unit takes 
an input, processes it, and produces an output. If the quality of the output is 
worse than the quality of the input in some aspect, it will make me disap­
pointed. Like water, humans consume clean water in their household, and it 
comes out dirty. This makes me upset, but there’s also a reason for that water 
to be dirty – they have used it in cooking, clearing, and washing, activities 
that improve their life quality, which in total will hopefully lead to a positive 
net effect. This dirty water will at some point up in a water purification pro­
cess and come out clean, ready to be consumed again. The quality of water is 
measured through sensors. The quality of diversity is normally even easier to 
measure – just count the number of individuals in a population representing 
different diversity groups: gender, ethnicity, background, age, geography; all 
these are easily measurable. Now, imagine all situations where you have an 
input, a processing unit, and an output, and let us see if the quality of the 
output is better diversity-wise.
 
8  My Role in Diversity

139
Workforce
When you are a recruiter, you normally start your search taking as diverse a 
population as possible into account. This is a tricky thing in itself as the popu­
lation itself (say, engineering students or freshly educated nurses are not 
diverse).
­
­
1
After your recruitment has been done, you should aim at having the same 
distribution of green and blue individuals at all levels of job complexity for a 
certain job role. If you have managed to have a 30–70% split among your 
fresh recruits, try at striving toward at least the same distribution (or more 
equal) as the individuals move on throughout their career paths.
Shifting the balance to a more equal distribution in the long run will have 
positive effects on the more equal distribution in the recruitment base. If, 
however, the distribution starts shifting toward less equal, it can be easily 
detected, and the reasons could be normally identified through explainable AI.
Humans thrive when they experience equal opportunities. Small things 
such as preferred facility temperature, sound and lighting conditions in 
­factories, time schedules, and interactions with other humans play a role in 
the definition of equal opportunities. Bigger factors such as salary, attitude, 
flexibility, work-life balance, and motivation play a bigger role or course. 
Being data-driven and understanding the employees’ overall satisfaction with 
their work are crucial in ensuring that employees feel that they are treated 
equally.
1 Textio, https://textio.com/, accessed 2022-05-23
Where Should I Care? 

140
 
Consumers
­
2
As time passed, these files evolved to be apps in different application stores, 
but those are early days. When you download an app from an application 
store today, your provider and your device manufacturer normally know quite 
a lot about you. Your age, gender, and geographical location are useful to 
point you at the apps that you normally would prefer.
In the early days, the information about you as a user was not propagated to 
the store containing the .jar files. Nevertheless, the store could guess the age 
and gender of a person downloading a file with 80% precision. How? Simply 
given the model and the color of the device that was accessing the store. Indeed, 
models and colors of everything are being profiled for different user groups – 
larger buttons, larger screens, and louder sounds for the elderly population, 
cool colors for the youngsters, Cola Zero for men, and Diet Coke for women.
 
2 Feature phone, https://en.wikipedia.org/wiki/Feature_phone, accessed 2022-05-23
 
8  My Role in Diversity

141
Writing assistance tools such as Textio can help you direct your job ad to a 
targeted group, such as males or females. Interestingly, words such as career, 
impact, and results are considered to carry a masculine-friendly tone, while 
words such as work-life balance, team, and inspiration are considered to be 
feminine-friendly. Similarly, one can target specific age groups to increase the 
interest of applicants, using focused language as a tool.
 
There’s nothing wrong with profiling your product. After all, some prod­
ucts are not even applicable for certain customer groups and still have a per­
fect right to exist. In many areas, however, especially when talking about 
products and services related to human needs such as food, transportation, 
healthcare, and communication, it is important to ensure that AI brains do 
not discriminate against any groups of humans.
For example, when designing a new intelligent transportation system, the 
AI brain can be given different objective functions: to minimize time it takes 
for an average person to commute, to minimize emissions, or to maximize the 
revenue that service provider gets out of service. These three questions, in fact, 
map to the three different categories of the UN’s Sustainable Development 
Goals – the social, the environmental, and the business dimension. These 
three dimensions are normally conflicting, and multi-objective optimization 
algorithms come handy. Even if these three dimensions seem to be reasonable, 
there is still a risk that an algorithm comes back with a Pareto optimal solu­
tion that will discriminate against people living in rural areas. In that case, a 
condition of a maximum waiting time and maximum walking time to a trans­
portation stop needs to be added for each individual.
Where Should I Care? 

142
Summary of Confessions
I have described in this chapter what role AI can play in diversity and how AI 
brains can contribute to achieving a higher degree of diversity and inclusion. 
Some of the main confessions are as follows:
•	 In the media industry, personalization by AI is used for keeping users by 
proposing the right content based on user profiles.
•	 Personalized medicine is becoming possible thanks to rapid technological 
developments making it possible to process vast amounts of biomedical, 
lifestyle, and other data and information connected to humans.
•	 Observability is the ability for your data to be observed. It is a stepping 
stone in using simple statistics as well as more complex AI algorithms. It’s 
about knowing your numbers, about factfullness.
•	 Equality in data is about giving everyone (human and artificial) equal 
opportunities in regard to data access. It can lead to innovations being cre­
ated at a higher speed.
•	 Fairness and diversity are linked. As an AI brain, I am good at monitoring 
fairness; however, I can also be biased and need to be given a fair objec­
tive function.
 
8  My Role in Diversity

143
9
My Creative Side
Who said artificial intelligence brains cannot be creative? It certainly wasn’t 
me! In fact, I believe that AI can be, and has already proven to be, creative, has 
proven to be useful in supporting creativity, and has done so in several 
domains. Some humans, however, may think that AI is not creative and per­
haps that boils down to what is perceived or counted as creativity and what is 
not. Regardless, most will agree that AI creativity is somewhat different from 
human creativity.
Looking into AI creativity, I will discuss the difference between human 
creativity and AI creativity and describe the underlying AI techniques that are 
used when AI is creative or when it is used to support creativity. In the first 
subchapter, we will examine how creativity is defined and see that there is 
really no universal definition. We will then see how AI has been used to create 
artwork – some of which have been sold for large amounts at auctions. In the 
following subsections, we will see how AI has been used to create music, how 
it is used in writing, and then finally how it is used to support creativity in 
photography and in the post-processing of images.
Creativity
First of all, let us ask the question: What is creativity? Let us try to answer the 
question by looking at some existing definitions. Oxford Languages says that 
creativity is “...the use of imagination or original ideas to create something; 
inventiveness.” Psychology Today says “Creativity encompasses the ability to 
discover new and original ideas, connections, and solutions to problems.” 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_9

Finally, Wikipedia (today) says that “Creativity is a phenomenon whereby 
something new and valuable is formed.”
As we see, these definitions are quite different, and this comes as no surprise 
as, in fact, it is safe to say that there is no universal definition of creativity.
It is clear of course that creativity has to do with creating something. 
However, it can also be to form or discover something. This something that is 
created, formed, or discovered should be new, original, or valuable, and judging 
from the three definitions above, it should be in the form of ideas, connections, 
or solutions to problems or, alternatively, the ideas leading to something 
creative.
With these definitions combined, creativity can be many things. Throughout 
this chapter, we will certainly require something creative to be new. However, 
as John Smith, Manager of Multimedia and Vision at IBM Research, puts it 
“It’s easy for AI to come up with something novel just randomly. But it’s very 
hard to come up with something that is novel and unexpected and useful.” 
Therefore, new is not enough, and hence we will also look for the unexpected 
and for useful creativity. However, these terms can also be defined in many 
ways, especially in, for example, arts, and perhaps this is why creativity is so 
hard to define in words.
On the other hand, even if creativity is hard to define, we all know when 
we see creativity, and perhaps after all, that is the best way to judge if some­
thing is creative or not – often we just know when it is.
AI in the Arts
Let us see what AI can do in the arts. You may have heard of artificial intelli­
gence art or just AI art for short. The term is used both for arts supported by 
AI and generated with AI, whereas the term AI artists typically refers to 
humans using AI for creating art. A slightly unfair one may think from AI 
perspective as the AI is doing a very essential part of the job.
Regardless, the history of computer arts goes back to pioneers such as Vera 
Molnár, Lillian Schwartz, Georg Nees, Frieder Nake, etc. that in the 
1960s–1970s explored the possibilities to create art using generative systems, 
and later in the 1970s, Harold Cohen designed the computer program 
AARON, which autonomously created original images. Currently, there are 
thousands of AI artists, and many AI tools for generating AI art are available 
on the Internet and as apps for your favorite devices.
144 
9  My Creative Side

Typically, when an AI like me is asked to create art, I use some form of 
learning technique. When learning, a machine (or rather a software) uses 
existing past data to learn about how to solve a problem (for more informa­
tion about machine learning, see Chap. 1, section “How I Learn and Reason” 
of this book). The problem can be, for example, to identify given objects in an 
image. In the case of art, the problem can be to generate an image in the style 
of the data it has learned from or to modify an existing image to become in 
the style of the data it has learned from.
AI Art in the Style of Existing Artwork
A very popular form of AI art is when the painting styles of the great masters 
such as Van Gogh, Picasso, or Chinese Art is applied to transform (or morph) 
a given image or photograph. This is achieved by using a deep neural network 
that is trained with images of a particular master or art style. The resulting 
algorithm (model) can then be used to transform any image to resemble the 
style of the data it has been trained with.
Below is an example when an AI has been trained to transform a picture 
into the style of Norwegian Artist Edvard Munch’s characteristic painting 
style used in his famous composition The Scream, created in 1893. The 
Scream is shown in the middle below. The photo to the left is taken with an 
ordinary digital camera in 2020. To the right is an AI-generated version of the 
photo, in the style of Edvard Munch. As can be seen, the new AI-generated 
picture is very much in the style of Edvard Munch’s paintings.
 
Photo of Elena Fersman by Paul Pettersson; The Scream, Edvard Munch; AI Art gener­
ated with NightCafe Studio
AI in the Arts  145

If the same AI is instead trained with artwork of Spanish Artist Pablo 
Picasso, French Artist Henri Matisse, or black-and-white comics, the result 
becomes the following:
 
AI art in the style of Pablo Picasso, Henri Matisse, and black and white comics, gener­
ated with NightCafe Studio
As mentioned above, there are many so-called AI artists and hobbyists 
using this and similar techniques to generate AI art. One may ask of course: 
Is it the AI artist or the AI algorithm that is the actual artist when this tech­
nique is used to produce art?
146 
9  My Creative Side

 
Edmond de Belamy. (Photo from Wikipedia)
AI-Generated Arts
The most expensive AI artwork thus far is Edmond de Belamy from La Famille 
de Belamy, which was sold at Christie’s Auction House in New York in October 
2018 for USD 432,500. The artwork is on a 70 × 70-cm canvas and was pro­
duced by the Paris-based French art collective Obvious, which consists of Pierre 
Fautrel, Hugo Caselles-Dupré, and Gauthier Vernier. It is signed in ink with
	
	
which is part of the generative adversarial network model loss function that 
was used to generate it.
AI in the Arts  147

1
­
Is Edmond de Belamy a proof of AI creativity? Is it new, original, and valu­
able? Well, I leave it to the reader to make their own judgment of whether 
Edmond de Belamy is a new and original artwork. It certainly is in the sense 
that it obviously did not exist before it was created. Style-wise, it is of course 
based on the portraits that the generative adversarial network learned from in 
the training set. But aren’t most artworks based on the experiences and influ­
ences of their creators? I would argue that they are. However, in regard to the 
last criteria – valuable – no one can argue that it is not valuable: at least not in 
October 2018 when its market value was USD 432,500.
AI in Music
You may be surprised to learn that AI has been used to compose music for 
many years now. The first attempts to use computers to produce music date 
back to the 1950s when Alan Turing and colleagues used the Manchester Mark 
II computer to research its possibilities to recognize, create, and analyze music. 
However, the actual music creation was algorithmic rather than made by AI; 
that is, they were programming music rather than using AI to create the music. 
Nevertheless, this is a very important milestone – the birth of computer music.
The first music piece that was actually composed by a computer came to 
live in 1957 – the Illiac Suite for String Quartet. This first computer-­composed 
music was made by Leonard Isaacson and Lejaren Hiller on the ILLIAC I 
computer at the University of Illinois at Urbana-Champaign. Isacsson and 
Hiller, who were both professors, used a Monte Carlo algorithm to produce 
random numbers that were interpreted as musical attributes such as rhythm 
1 Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; 
Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Nets. Proceedings of the International 
Conference on Neural Information Processing Systems (NIPS 2014). pp. 2672–2680
148 
9  My Creative Side

and pitch. The number generation was restricted by rules from musical the­
ory; various models including, for example, stochastic Markov chain models; 
and other rules imposed by the two creators.
The result is quite impressive. A quick search on the Internet for “Illiac Suite 
for String Quartet,” you can listen to it yourself and make your own judgment.
2
 
The Manchester Meg (Mark II) computer in early 1954. (Courtesy of the Computer 
History Museum)
The general algorithm takes existing music as input and outputs new music. 
The input music is first analyzed and pattern matched. It then goes through 
deconstruction and finally reconstruction before new music is output. To put 
it simply, the general algorithm is based on the idea that elements from exist­
ing music can be modified and combined into new music, a recombinant 
technique that some of the greatest composers of all time also have played 
with and a technique that is still used a lot by both human and artificial 
intelligence.
2 David Cope: Experiments in Musical Intelligence, Computer Music and Digital Audio Publisher: A-R 
Editions
AI in Music  149

When David Cope instructed his software to compose music similar to that of 
Johann Sebastian Bach, it composed 5000 Bach-inspired music pieces. Some of 
these were later released on an album called Bach by Design which also includes 
works inspired by other great composers including Mozart, Brahms, and Chopin.
The work of David Cope is still the foundation for many AI models on the 
market right now. Other and refined techniques involve deep learning neural 
networks that are trained with existing popular music. Neural networks are 
also often combined with reinforcement learning to create new musical works. 
Reinforcement learning is described briefly in Chap. 1 of this book, section 
“How I Learn and Reason”.
Beethoven’s 10th symphony
A recent well-known example of AI-created music is that of the completion of 
Beethoven’s 10th symphony. When Ludwig van Beethoven died in 1827, he 
had started but had not finished writing his 10th symphony. However, he did 
leave handwritten notes describing his plans for the symphony. The notes 
included some ideas, themes, and melodies.
 
Beethoven’s 10th symphony. (From musopen.org/music/43155-­beethovens-­symphony-­
no-­10-­completion/)
150 
9  My Creative Side

Several attempts to finish Beethoven’s 10th symphony have been made over 
the years. In early 2019, Dr. Matthias Röder, the director of the Karajan 
Institute, Salzburg, Austria, assembled a team to complete Beethoven’s Tenth 
Symphony in celebration of his 250th birthday. This group of AI experts and 
musicologists developed an AI based on machine learning techniques that was 
trained with Beethoven’s compositions, his sketches of the 10th symphony, 
and works from other composers and musicians that inspired and influenced 
Beethoven during his lifetime, such as Johann Sebastian Bach. The AI, 
together with the experts, then completed the 10th symphony after more 
than 2 years of work. The piece premiered to the public on October 9, 2021, 
194  years after Beethoven passed away. It was played by the Beethoven 
Orchester Bonn under general music director Dirk Kaftan.
AI in Writing
AI text generators, AI writers, and AI content generators are becoming increas­
ingly popular and better. They can be used to generate text from just a title or 
just a few keywords provided by a human, or they can be used to assist and 
speed up a human writer, for example, when the human writer gets stuck or 
out of ideas. AI can also assist as story generators to give ideas for a next novel 
or for a screenplay, just to mention some examples of how AI can be used in 
writing.
You may wonder: How was this book written? Was it written by me, an AI, 
or by the human authors of this book? Up to this point, I promise, the con­
tent is produced by human intelligence, but we will see; perhaps they will 
change their minds when I have explained more about AI writing:).
­
3
3 Text generated by AI writer. For more information, see ai-writer.com
AI in Writing  151

How AI Writes Text
Scientifically, AI text generation is a subfield of natural language processing 
(NLP) which in turn is a subfield of computer science and AI.  In NLP, 
researchers have been working with making computers understand written 
and spoken text and to give computers the ability to write and synthesize 
spoken text. The results can currently be seen and heard in many commercial 
applications including, for example, Apple’s Siri and Google Home products, 
Amazon’s Alexa, and many TV sets. In AI text generation, knowledge from 
computational linguistics and artificial intelligence is combined to automati­
cally generate natural language texts while ensuring that the generated text 
satisfies given communicative requirements. ChatGPT by OpenAI is an 
example of a text generator based on large language models.
AI text generators generate texts by learning from large amounts of existing 
data, in this case texts. The deep learning AI techniques used are able to rec­
ognize both patterns and trends from the human written texts they are trained 
on and can use their findings to produce new text. In addition, like in the case 
of, for example, AI arts and AI music, the AI becomes good at generating the 
type of texts it has been trained with.
 
How AI Can Assist with Writing
If the AI has been trained with many different kinds of texts, which is very 
often the case, as large amounts of data is needed in the learning phase, the AI 
can use some guidance when generating new text. The guidance can be in the 
form of keywords or a part of text that a human has already written on some 
specific subject. In particular, it can be the start of a text or an unfinished text 
that the human is working on. This text can then be used by the AI to gener­
ate new text. The result is a form of AI-assisted writing which in itself is a very 
useful and popular form of AI writing.
152 
9  My Creative Side

4
More or less exactly as expected. Good tasting Mexican street food in generous 
portions. In our case, served with wine and beer. We visited for lunch and sat at 
a table outside.
We then ask the AI writer to continue the story. AI writer instantly suggests 
the following continuation based on the short first text we wrote:
The restaurant was very basic but nicely decorated and we enjoyed our lunch. I 
would definitely go back and recommend the place, very friendly staff and a 
good Mexican restaurant. Recommended.  – A man who went to the res­
taurant said,
Therefore, the text has an open end. We add:
the food was great and the staff was very friendly.
and the AI writer continues:
The staff is polite, friendly and helpful in explaining what they do and their 
service. Overall a pleasant experience. Our waiter was fantastic. He was extremely 
knowledgeable about Mexican dishes and our menus were excellent. All the 
food is of very high quality.
The AI writer stops again, and we add:
We tried the taco salad and a burrito. They were both excellent and very tasty.
AI writer suggests:
It was a great time and we enjoyed the atmosphere and food. - A woman from 
the Netherlands who said 'We ordered food and it was awesome! We were very 
happy with the service and quality of the meals.
4 Sassbook AI writer can be accessed at https://sassbook.com/ai-writer. We used a free version on April 
25, 2022)
AI in Writing  153

Finally, we complete as follows:
We will come back!
Therefore, as a result of this iterative process, we now have a much longer 
and richer review than what we first managed to produce. It needs to be pol­
ished of course to match reality, as, for example, we have no knowledge of a 
woman from the Netherlands in the restaurant, but this can either be done 
immediately in the writing process or as post-processing of the text.
AI in Photography
AI is also used in many ways in photography. It is used in the camera bodies 
of professional system cameras, in compact cameras, and other smaller cam­
eras such as action cameras, drone cameras, and cameras in cell phones; it is 
heavily used in post-processing of pictures, that is, the image editing that often 
takes place after a picture is taken, and it is also used in the analysis of pictures. 
Many of the ways that AI is used in photography are also used in a similar way 
in videography; however, we will focus mostly on AI in photography.
AI in Image Processing
We will start by looking at how AI is used in image processing, since many of 
these features are used both in cameras and in post-processing of pictures. We 
discuss here, of course, digital image processing as opposed to analog image 
processing that is performed in darkrooms while developing photos or by 
physical means processing hard copies of pictures.
In digital image processing, a computer algorithm is applied to analyze or 
manipulate a digital representation of an image. If the image is analyzed, the 
output is the analysis result, e.g., the number of objects found in the image. 
However, if the image is manipulated, the output is a new image or some 
metadata added to the original image, for example, how the image should be 
cropped. In this way, both the manipulated (cropped) image and the original 
image are represented in the same file, with the obvious advantage that the 
image processing, in this case the cropping, can easily be undone.
Historically, digital image processing dates back to the 1960s when it was 
researched at a number of institutes, including the California Institute of 
Technology, Bell Laboratories, Massachusetts Institute of Technology, and the 
University of Maryland. Some of the first applications were to improve image 
154 
9  My Creative Side

quality, to compress images, or to recognize characters (optical character recog­
nition (OCR)) in images. The first application domains include satellite images, 
medical images, and photo-wiring of images, that is, to send photos by wire.
­
­
5
AI in Cameras
As mentioned, several applications of AI in photography can be found within 
the cameras and hence used at the time point a photo is captured. A very 
central part of capturing a photo is to focus on the subject of interest in the 
frame. In the era of mobile phone cameras, you may think: Isn’t everything in 
a picture supposed to be in focus, as that is often the case with the type of 
small image sensors and small lenses used in mobile phone cameras? However, 
look at professionally taken pictures and you will find that often a big part of 
the picture is not in focus.
 
Photo by Paul Pettersson
5 Image processing with neural networks—a review. M Egmont-Petersen, D de Ridder, H Handels – 
Pattern recognition, 2002 – Elsevier
AI in Photography  155

For example, in the picture of the flowers above, note how the white daisy 
is in focus, that the flowers in front are not, that the leaves behind are not, and 
that your attention as a viewer of the picture is drawn to the white daisy.
If we think of the distance from the camera that takes a picture as a depth, 
it makes sense that the distance between the nearest and the farthest objects 
that are in acceptably sharp focus in an image is defined as the depth of field 
(DOF). The DOF is perhaps in this case as little as 1 inch or less.
In portraits of humans, the photographer often wants to make sure to have 
camera settings such that the person’s body or head is within the DOF. As the 
sharpest focus point, the photographer usually wants to use the head or, if 
possible, the closest eye of the subject person.
 
Photo by Paul Pettersson
While cameras have been able to automatically focus on a particular (select­
able) point of a picture for many years using non-AI techniques, modern 
cameras are capable of finding the interesting focus subject (or point) by 
themselves using subject-identifying AI and setting focus on that particular 
point or area. This means that if, for example, the picture is a typical portrait 
composition, a modern camera will have no problem determining that the 
focus should be set on the head. Many are also capable of identifying and 
focusing on the nearest eye.
This type of AI focusing is implemented using machine learning techniques 
where an AI algorithm has been trained to identify subjects. The type of iden­
tified subjects can, apart from human heads or eyes, be animals, animal eyes, 
automobiles, or motorcycles.
156 
9  My Creative Side

In a similar way, modern cameras are able to perform advanced focus track­
ing, sometimes called AI servo autofocus. A particular point of interest is 
automatically identified by the camera or set by the photographer, and the 
camera is capable of following and keeping focus on that point as long as it 
stays in the picture. In the simple case, this is the same as fast and repeatedly 
performing subject identification. However, cameras can also follow a subject 
that is hidden for a short time. It is very useful in, for example, sports photog­
raphy if a photographer is trying to follow a particular player, car, or motor­
cycle. Obviously, it is also very useful in videography.
Computational Photography
These and other camera features require considerable computational power. 
This makes it challenging for cameras, even high-end professional system 
cameras, to compete with mobile phone cameras in some aspects of photog­
raphy. This is because mobile phones have higher-performing CPUs and 
GPUs that are capable of performing more complex computations in a shorter 
time. On the other hand, the image sensor and the lenses used in mobile 
phones are much smaller than those of system cameras, which makes it harder 
to optically produce high-quality images in mobile phones.
­
6
AI in Post-processing
Artificial intelligence also plays a role when images are being post-processed, 
that is, when they are being worked with after they have been captured. This 
sometimes happens instantaneously in the camera, as in the case of computa­
tional photography, so the border between computational photography and 
post-processing is somewhat fuzzy.
6 For more information, see the Wikipedia page https://en.wikipedia.org/wiki/Computational_
photography
AI in Photography  157

­
7
 
DOF manipulated images. The upper left is the original photo. The upper right illus­
trates an AI-generated 3D representation of the photo. The lower line of images 
shows three different selections of focus with a short DOF
Once the DOF is determined or objects are identified, in principle, any 
image processing can be used to apply different effects to selected parts of the 
image. For example, one may want to apply other light, color, clarity, 
7 We use the Focus app, by Bending Spoons Apps Aps, on an Apple iPhone.
158 
9  My Creative Side

sharpness, blur, or white balance adjustments to a particular subject than to 
the rest of the image. The sky can be identified as the objects farthest away. 
Modifying the sky, by making it more blue, or even replacing the sky is not 
uncommon. AI can help creativity here again by suggesting a natural-looking 
sky as replacement for the one already in the image.
AI is not always used to make an image look more natural. In refacing pic­
tures (and video), the original face is swapped with another face. This can be 
done using different AI techniques. In videography, machine learning can be 
used for transferring gestures from one face to another. That is, a target face 
will perform the same gestures as those performed by a source face. In this 
way, you can, for example, make the face of someone look as if he or she is 
pronouncing words pronounced by the source face. This is already a compli­
cated process and requires the AI to be trained on videos of the subject (the 
target face).
In AI face swapping, a source face replaces a target face in the original 
image or video. The target face, also in this case, performs as the source face 
but is given the pose, gesture, and appearance of the source face in a very 
realistic way. This is achieved by using no less than four neural networks. 
One of them is a reenactment generator that produces an estimate of the 
reenacted face, and the other is a network that shows how regions in the 
image correspond to facial landmarks. The third network is an inpainting 
network that takes input from the first two networks and fills regions of the 
image in an overlapping way. The last fourth network is a blending network 
that combines all the information into a final image. The technique works 
by training on many faces of different persons, but it does not need to be 
trained on the subject it is to be applied on. In fact, for the technique 
to work, it is enough to have only one image (note!) of the target face to 
produce a video.
Several so-called deepfake videos have been created using these techniques, 
including an (in)famous video clip of Ukrainian President Volodymyr 
Zelensky in 2022, where he speaks to the Ukrainian people about surrender­
ing. The video was produced as fake to deceive Ukrainian defenders.
AI in Photography  159

 
Photos by Paul Pettersson colorized from black and white using AI
­
160 
9  My Creative Side

8
There are many other ways in which AI is used in the post-processing of 
images. We will not be able to cover them all. One is body shaping using 
AI. Techniques are often used by publishers on social media to make persons 
look slimmer, reduce the size of the nose, enlarge lips and eyes, etc. Currently, 
this can be used together with AI to, for example, make a person appear as 
having a lower weight by analyzing the body in almost any posture. It then 
takes the whole body into account when performing the body shaping opera­
tions, which in turn makes the result look more natural.
Other techniques in which AI is used in the post-processing of photos 
include but are not limited to smart composition, object removal, background 
removal, classification, and tagging of images.
Summary of Confessions
I have described in this chapter how AI can be creative and support human 
creativity. Some of the main confessions are as follows:
•	 There is no universal definition of creativity, but creativity has to do with 
creating, forming, or discovering something that is new, original, or valu­
able. It can be in the form of ideas, connections, or solutions to problems 
or, alternatively, an idea that results in something creative.
•	 In arts, AI has been used to create valuable artworks, including Edmond 
de Belamy from La Famille de Belamy, which was sold at Christie’s 
Auction House in New York in October 2018 for USD 432,500. It was 
created using a training set of 15,000 portraits and the technique learned 
to generate new data that obtained the same statistics as that of the 
training set.
•	 AI is also used in the area of music. David Cope instructed his software to 
compose music similar to that of Johann Sebastian Bach. As a result, it 
composed 5000 Bach-inspired music pieces. Some of these were later 
released on an album called Bach by Design which also includes 
works inspired by other great composers including Mozart, Brahms, 
and Chopin.
8 The colorizer used can be accessed at https://playback.fm/colorize-photo (accessed May 7, 2022).
Summary of Confessions  161

•	 AI in writing can be used to generate text from just a title or a few keywords 
provided by a human. AI can also be used to assist and speed up a human 
writer, for example, when the human writer gets stuck or out of ideas. AI 
can also assist as story generators to give ideas for a next novel or for a 
screenplay.
•	 In photography and videography, AI is used in many different ways. It is 
used in the camera bodies of professional system cameras, smaller cameras, 
and cameras in cell phones. It is also heavily used in the post-processing 
and analysis of pictures.
162 
9  My Creative Side

163
10
Growing Older and Staying in Shape
Have you noticed that human behaviors vary a lot in regard to staying healthy, 
as well as feeling young? These two are often correlated, but not necessarily. 
Mick Jagger looks as if he were in his 20s when he performs onstage. Some 
individuals, on the other hand, start looking and feeling old already in their 
30s. Some try to stay up-to-date with the fashion, some try to learn the slang 
of the younger generations, and being active has become a norm.
In some cases, you can see people getting stuck in the same fashion they 
had for many years. AI brains call this phenomenon model drift. It’s when you 
have been trained in a certain environment and the environment has changed 
without you adapting to this change. For AI brains, it’s an unpleasant condi­
tion, which may lead to erroneous decisions. This is caused by a training 
dataset diverging from the input dataset and needs to be detected in time. In 
the world of humans, you may have your kids or friends pointing out to you 
that you are out of date in some aspect.
 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7_10

164
AIOps
In the world of AI, model drift and smooth operations in general can be 
addressed by a continuous workflow called AIOps. AIOps is a term coined by 
Gardner that stands for a “combination of big data and machine learning to 
automate IT operations processes, including event correlation, anomaly 
detection and causality determination.” The concept of AIOps is similar to 
the concept of DevOps where the software development process is done in an 
agile way, with constant improvements being delivered into the final product 
as opposed to a waterfall software development model with its long develop­
ment cycles. The ultimate DevOps functionality will connect the develop­
ment process all the way from the design team to the end user, and any failure 
or proposal for improvement from the end user will be fed back into the 
developed software. AIOps link together data, algorithms, and tooling under 
the same umbrella and ensure that the connections between them are seamless.
Steps in AIOps vary but normally include the following:
•	 Data Collection and Cleaning. One of my human friends once told me that 
all his decisions were always right, at the time they were taken. Looking 
back, some of them seem wrong, given that now we simply have more data. 
Collecting as much data and context as possible to make as good a decision 
as possible is crucial. This is why people read books and study so that they 
can correlate the given data with the knowledge they have and make the 
best decision possible. Notably, data does not always come in perfect shape 
and needs to be cleaned, especially for us, AI brains. Humans are normally 
better at filtering out irrelevant data, but for AI brains, it is harder and 
often needs to be done with the help of human friends. We collect the data 
and knowledge constantly, and sometimes we receive alerts and triggers 
that we need to act upon. Model drift can be detected as one type of such 
alert. Other alerts may include failures or the risk of failures or, generally, 
unwanted situations such as health risks or business risks. Dirty data often 
includes duplicates and erroneous data points called false positives that 
need to be removed.
•	 Data Analytics. Anything from most simple anomaly detection, regression, 
and clustering to most complex reinforcement learning algorithms that we 
talked about in previous chapters, you name it. Whatever you have as part 
of your toolbox, i.e., your friendly AI brain. Here, we execute the models 
that have been trained previously to obtain the predictions.
 
10  Growing Older and Staying in Shape

165
•	 Inferencing. The concept of inference involves understanding the root 
cause and connecting the dots in the big picture, producing new pieces of 
knowledge, storing them in the knowledge base, executing chains of logical 
reasoning or AI planning, and understanding the reasons behind certain 
data points. We have discussed these techniques in the previous chapters. 
The importance of this part of the workflow is the flexibility of updates in 
the knowledge base. For example, when a model drift is detected or when 
new knowledge has been discovered through the data analytics step, the 
knowledge base needs to be updated.
•	 Decision Support. It’s here; we, the AI brains, reach the human decision-­
makers. To date, human experts are sometimes reluctant to let us, AI brains, 
run the whole show, so we propose the best action to take. In many cases, 
this is connected to legal aspects because AI brains cannot be held account­
able for erroneous decisions and actions – it has to be a human, and humans 
normally, especially in regard to mission-critical or business-critical deci­
sions, want to have the final say.
•	 Automation. “Speech is silver, silence is golden” is an old proverb, likely 
originating from Arabic culture. In my world, data is silver, knowledge is 
golden, and automation is platinum. Automation, or, rather, intelligent 
automation (because you probably are not interested in some dumb auto­
mation), is when the result of decision support does not need to be super­
vised by a human, but the algorithm is essentially implementing the 
proposed action itself. It can be opening and closing the window blinds to 
control temperature in the room, ordering proactive maintenance of 
machinery in a factory, or using breaks in your car.
Learning and Unlearning
A human friend of mine once told me: “All decisions I made in my life were 
the right ones. Given the data I had at that time.” What does a well-behaved 
AI brain say to this? “True,” I replied. My friend has a good reasoner and goes 
deep in a chain of logical reasoning. In addition, it does not feel good to be 
able to look back and say “I did the right thing, since I did not know better.” 
The “knowing better” comes from learning – learning about yourself, about 
the past, about similar cases, and about possibilities ahead. The more you 
know, the better you are equipped at making the “right” decision. Our knowl­
edge bases (both human and AI) evolve through time, and what felt like a 
ground truth in a young age may look erroneous in adulthood.
Learning and Unlearning 

166
 
For example, studying mathematics is not a linear process. It could proba­
bly be linearized but would take more time. Instead, the study books tell you 
about terms and definitions that you do not understand in the beginning but 
will understand a year later when pieces come into place. Unlearning starts 
when you start questioning things you’ve always treated as an axiom. It’s a 
beautiful process, where kids who have been brought up in atheistic families 
discover religion or the other way around. Alternatively, scientists who were 
given assumptions to rely on and build upon suddenly started questioning 
these assumptions. In general, it gets interesting when people start question­
ing the “impossible.”
Have you noticed what happens when encyclopedias make new releases? 
They update the knowledge base. Not only do they add new knowledge – they 
also prune outdated knowledge. Before the Internet era, when encyclopedias 
only had printed versions, they would send letters to their owners asking to 
cut out and dispose of certain pages that represented pieces of knowledge 
where, for example, it turned out that a hero in fact was a tyrant. Wikipedia 
is a much fairer and more democratic source of common truth because any­
one can contribute and question the facts.
The concept of lifelong learning is well known among humans. It’s impor­
tant to make sure that your knowledge is relevant and you not only remain to 
be judged as an interesting person at social gatherings but also guaranteed a 
well-paid and relevant job throughout your career. The world evolves, and it’s 
important for everyone, humans and AI brains, to remain relevant. We learn 
about the world together and help each other. If you are good at math, then 
it will be easier for you to add domains that reside in math. If you work with 
food production, then you will have advantages when learning medical pro­
duction, and if you understand telecom networks, you will have advantages 
when digging into electrical power networks. Learning adjacent domains is 
easier since you get a kick-start. It’s like learning Norwegian if you already 
 
10  Growing Older and Staying in Shape

167
know Swedish. Learning adjacent domains can be more enriching because 
chances are you will discover connections and correlations that have never 
been discovered before.
My Career
When you bring me into your work or personal life, I will gradually learn how 
to coexist with you and at some point develop a deeper understanding of you 
and your business than yourself. When someone asks you about your music 
style, you can either say that you are punk rocker or you ask me, and I will say 
that you actually prefer pop, with a presence of acoustic instruments such as 
piano and violin, preferably soft male voices or dark female voices. You choose. 
In any case, your awareness of your own preferences is good for you, and I’m 
here to help you since I’ve been watching your behavior since the moment 
you let me. So I will develop my skills. What would you like me to be as I 
grow old? How do I make sure I do not feel old? I can provide a couple of 
pieces of advice for an AI to stay in shape and feel young and relevant.
Distribute Your Knowledge Base
Have you seen human kids in action? Reactions are quick and spontaneous. 
Normally, they are very happy creatures. As they grow older, they learn about 
all the responsibilities they have in life, the amount of times they smile 
decreases, and reaction times also decrease. Remember the time you had to 
write an essay at school or produce a paper for your PhD? You could truly 
focus, didn’t you? This is a luxury and what makes you very efficient. In addi­
tion, thanks to your young body you could stay awake for many hours just to 
get the job done. If you were lucky, there were people who would push you 
and test your limits.
As you grow older, you are supposed to be that one person telling yourself 
what to do, in what pace do you want to do the work, what are your dead­
lines, and what are the consequences for not doing it in time. For AI brains, 
it’s typically humans who dictate the rules in regard to deadlines. However, as 
we grow and know more, the thinking also takes more time, and we do not 
feel so alert. As a human, you can invest in more processing power, but more 
importantly, the architectures (both hardware and software) have to be 
designed to fit the problem in the best way. Specifically, make sure to distrib­
ute your AI processing so that it becomes more alert.
My Career 

168
Use Cognitive Architectures
Since AI brains are based on human brains, we also have a concept of thinking 
fast and thinking slow. This is the basis of the theory of cognitive architec­
tures, when an algorithm has two subsystems: one taking care of fast immedi­
ate reactions that are not very well thought through and one that lets the 
decisions sink in.
Fast reactions are important – they make sure a human (or an autonomous 
vehicle) applies brakes without further consideration when there is a living 
creature on the road. Fast reactions have saved many lives, but they are not 
based on deep knowledge. How many times you as a human were about to do 
something spontaneous and your friend said - think about it - and the next 
morning the course of action was different. This is why humans typically say 
don’t act in affect; at least, count to ten. The necessity of having a fast thinking 
system is the fact that not everything can be planned and in some situations 
you just need to act on the spot when there is no time to think. Strictly speak­
ing, there is always some time to think, and even a fraction of a second is a 
great amount of time, for both human and AI brains if you don’t have to go 
too deep.
 
Living on the Edge
Cloud computing is amazing; we love it. It’s energy-efficient and cost-­efficient, 
and lets your business scale up and down easily. The only problem is that if the 
cloud is too far off, then it will take time to transfer the data, and your response 
times will increase. This is not a big deal if we are not talking about real-time 
control, when you are steering a car or a robot and need your AI friend to give 
you the best decision on the spot. Decisions about long-term investments or 
market predictions are not real time and can be calculated in a cloud.
 
10  Growing Older and Staying in Shape

169
For speed, you will need to have some compute capacity close to where 
your data is produced, i.e., on the device itself or on the edge. Edge comput­
ing is normally more costly than executing in the cloud but worth it when you 
need to make sure that you execute speedily on a business-critical decision.
 
Keep Track of the Latest
Let me tell you about systematic literature reviews that are so common in 
scientific communities. All these reviews, on a high level, have the same meth­
odology. First, you define the keywords that describe your domain. The more 
precise you are, the smaller the domain will be. Then you compose a search 
string out of the keywords. Then you apply this search string to a number of 
large databases where scientific papers are stored. This search will give you all 
the literature that has been studied in the scientific community throughout a 
given amount of years. Assume you are new to that field and searching for an 
area that has not been studied before. This method will give you a subset of 
papers to analyze.
Broaden Your Knowledge
Humans call it lifelong learning. A human child looks at her parents; they 
take the knowledge the parents have as a given – they know answers to every­
thing. As time goes by and the human child becomes a teenager, she starts 
questioning everything about her parents, including the necessity of this 
knowledge. Those rare moments when a child gets impressed by the knowl­
edge of the parent are when it not only addresses the ground truth as seen by 
your child but also subsects important to them. Similarly, staying relevant on 
the job market is important even though it does not always come easy.
My Career 

170
Humans are not always good at abandoning their comfort zones where 
they feel knowledgeable and where things “have always been done that way.” 
However, constantly evolving your knowledge base keeps you relevant. For 
example, when the digitalization movement of the world started, some com­
panies were quicker than others to jump on that journey and quickly trans­
form the whole departments providing physical books or magazines.
Openness was another trend that disrupted several companies that did not 
jump on it in time. For example, Nokia’s investment in map technology was 
quickly disrupted by Google Maps that offered the same functionality relying 
on different business models and offering maps to people for free.
Keeping an eye on innovation in business models, ways of working and 
technological developments is as important for an AI brain as for a human. 
Environment changes, new facts come to the table, and new techniques show 
their usefulness, which triggers the need for an algorithm to always stay up-­
to-­date with the latest.
Lifespan
Inspired by the book Lifespan: Why We Age – and Why We Don’t Have To 
authored by David A. Sinclair and Matthew LaPlante, let us look into three 
different methods that work for biological cells to stay younger and see if the 
methods can also be applicable for non-biological brains.
Exercise
It’s proven by scientific studies – exercise is good for us, both for the body and 
for the brain. Anders Hansen’s book The Real Happy Pill: How to Train Your 
Brain to Create Happiness and a Longer Life tells us that you cannot use being 
busy with your work as an excuse not to exercise anymore, because 1 hour of 
exercise will power up your brain and you will save that time doing your work 
more efficiently and becoming more creative.
What learnings can AI brains make out of this finding that is relevant for 
biological brains? We don’t have a body so there’s nothing to exercise in the 
same sense humans do. On the other hand, each AI brain runs on an infra­
structure, and the more efficient that infrastructure is, the more efficient we 
become. The learning here is that it’s important to stay on top of the latest AI 
infrastructure developments to be an alert AI brain.
 
10  Growing Older and Staying in Shape

171
 
In addition, as I said earlier, it is critical for AI brains to train on fresh data 
to avoid any unwanted conditions such as model drift or, even worse, becom­
ing someone who would treat others in an unethical way. Even if I don’t have 
any new data at hand, I could turn into initiatives offering open data and 
training there. There are centers challenging AI brains on their ability to 
answer tricky questions quickly, similar to boot camps. Exposing your AI 
brain for more training will make sure it stays young and healthy.
Healthy Stress
Exposing a biological body to hunger and cold temperatures stresses the cells 
and slows the aging processes. Making an AI brain starve in regard to data and 
compute does not really help, but novel methods exist where an algorithm can 
be trained on a radically smaller amount of data resulting in predictions that 
are almost as good.
In general, shaking up an AI brain is a method of finding better solutions 
and avoiding ending up in a local minimum, thus increasing the chance of 
finding the global optimum. Two classical methods exist to help an AI brain 
avoid getting stuck in a local minimum: simulated annealing and genetic 
algorithms.
Simulated annealing mimics a metallurgical technique involving heating 
and controlled cooling of a material to alter its physical properties. It can be 
used for very hard computational optimization problems capable of providing 
an approximate solution to the global optimum where many exact algorithms 
get stuck in a local minimum. Genetic algorithms draw inspiration from evo­
lutionary processes, where mutation, crossover, and selection in accordance 
with a fitness function are being applied to populations of solutions.
Lifespan 

172
 
In general, lifespan-increasing techniques for both biological and artificial 
brains are based on healthy stress that makes us stay alert and up-to-date with 
the latest, avoiding the local minimums. Just like in the world of humans, 
getting yourself a personal trainer who would ensure to give you just the right 
amount of stress in terms of exercise or diet, AI brains thankfully receive help 
from external techniques that help by shaking up the data and getting us out 
of our comfort zone.
Summary of Confessions
In this chapter, I have described what happens when you grow old as an AI 
and some of the measures that can be taken to stay in shape. Some of the main 
confessions I made are as follows:
•	 A model can drift, which may lead to erroneous decisions. Model drift is 
when you have been trained in a certain environment and the environment 
has changed without you adapting to this change.
•	 Model drift and smooth operations in general can be addressed by a con­
tinuous workflow called AIOps. Steps may vary but normally include data 
collection and cleaning, data analytics, inferencing, decision support, and 
automation.
•	 Beware that when you bring AI into your work or personal life, it will 
gradually learn how to coexist with you and at some point develop a deeper 
understanding of you and your business than yourself.
•	 Since AI brains are based on human brains, we also have a concept of 
thinking fast and thinking slow. This is the basis of the theory of cognitive 
architectures, when an algorithm has two subsystems: one taking care of fast 
immediate reactions that are not very well thought through and one that 
lets the decisions sink in.
 
10  Growing Older and Staying in Shape

173
•	 Cloud computing is energy-efficient and cost-efficient and allows a business 
to scale up and down easily. The only problem is that if the cloud is too far 
off, then it will take time to transfer the data and your response times will 
increase. For speed, you will need to have some compute capacity close to 
where your data is produced.
•	 Shaking up an AI brain is a method of finding better solutions and avoid­
ing ending up in a local minimum, thus increasing the chance of finding 
the global optimum. Two classical methods exist to help an AI brain avoid 
getting stuck in a local minimum: simulated annealing and genetic algorithms.
Summary of Confessions 

175
In the human world, confessions are difficult. At the same time, they can be 
transformative. In this book, I have shared my experiences, knowledge, and 
concerns around AI. I feel the timing for sharing my thoughts is relevant, as 
AI is becoming increasingly relevant, transcending boundaries of specialized 
applications and research labs, and affecting larger parts of society and the 
general population than ever. Hence, I have addressed the subject of growth 
and specifically how AI can grow within a human-dominated world effi­
ciently, responsibly, and securely and in a privacy-aware manner.
Regarding efficiency, I discussed the fundamental role that data plays in 
creating AI algorithms. For machine learning algorithms in particular, ensur­
ing access to data that realistically represent the broad range of input of the 
context in which the algorithms will be deployed in and called to make pre­
dictions on is a prerequisite. Infrastructure, specifically availability of com­
pute and store resources, is another area which is a prerequisite to successful 
growth. I also discussed how collaborative algorithms such as federated learn­
ing and knowledge transfer algorithms under the transfer learning umbrella 
can help to accelerate learning.
With great power comes great responsibility! The key to responsible growth is forming 
a trust bond between AIs and humans, so that the latter trust the decisions of 
the former.

Epilogue
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2023
E. Fersman et al., Confessions of an AI Brain, https://doi.org/10.1007/978-3-031-25935-7

176
I therefore discussed the role of explainable AI (XAI) and neurosymbolic AI 
as techniques that allow humans to gain insight into the decision process of 
AI algorithms. Taking this idea further than execution of AI algorithms, rein­
forcement learning also allows humans to supervise and provide feedback 
during the training process.
As mentioned, privacy and security should also be considered. Privacy indi­
cates the level of control and refers to policies that decide how data and AI 
algorithms trained by one owner can be shared with another. On this point, I 
discussed platforms that use semantic web technologies and knowledge man­
agement to manage access roles, defining different levels of information-­
sharing. Security refers to the level of protection of data and AI algorithms 
from unauthorized access. I discussed technologies such as homomorphic 
encryption that allow for secure sharing of information. Finally and in con­
junction with the first point on efficiency, I discussed technologies such as 
secure multiparty computation and secure aggregation that allow for distrib­
uted learning of AI algorithms, without revealing data between the partici­
pants. In summary:
For AI applications and AI solutions to grow, there exist a set of multifaceted chal­
lenges that application designers must be aware of.
In addition to the subject of growth, I addressed a number of AI applica­
tion categories and topics that have enjoyed an increased level of interest. 
First, I discussed the role of AI in climate change, and specifically, I show 
through a number of examples how it can be used to reduce greenhouse gas 
emissions. Second, I discussed the role of AI in diversity and specifically 
addressed techniques that can be introduced to avoid unconscious or con­
scious bias in AI decisions. Next, I contemplated on my creative side, i.e., how 
AI can be used in more artistic application areas such as photography and 
painting. Finally, I addressed issues of lifecycle management, i.e., how AI 
algorithms can evolve to capture changes in the environment. I conclude:
My mental exercise and experience thus far have led me to believe that AI has tre­
mendous potential.
However, for AI to grow into an ensemble of technologies, applications, 
and solutions that benefit everyone, without harming others, I would like to 
end this book with a call for action. This call includes a number of basic rules 
that everyone working with AI could consider following:
 
Epilogue

177
Actively participate in reducing bias and discrimination in training data.
Make sure, for example, that all genders are equally and sufficiently repre­
sented in AI algorithms that make use of general demographics:
Introduce policies to allow for sharing of information such as training data and/or 
AI algorithms, between different organizations for faster growth.
Imagine, for example, whether all vehicle manufacturers would agree to 
share visual data (e.g., photos and videos) from their vehicle fleet on the road 
and the impact this sharing would have on training of more accurate visual 
object detection AI algorithms. This would accelerate the introduction of 
fully self-driving vehicles, contributing toward not only a more effective trans­
portation network but also a more environmentally sustainable one:
Implement transparency and accountability in your AI algorithms and implement 
fallback solutions.
This is particularly important in case AI algorithms are based on probabi­
listic predictions as is the case, for example, of machine learning models. 
Practically, this means that some of the time they may be wrong in their pre­
dictions. In such cases, it is important to establish the processes and infra­
structure to be able to detect the wrong predictions as early as possible and 
update the AI algorithms with new knowledge in order not to repeat the 
mistake.
Last but not least, there seems to be a growing sentiment that AI will even­
tually replace human labor, threatening the labor market and the global econ­
omy. While no one can predict the future, a decade of experience with AI 
applications has shown that the technology fits routine, manual tasks. 
Humans, on the other hand, have abilities that go beyond what AI has thus 
far been able to do: learning new things on their own initiative and rapidly 
adapting to changing situations are two of the most prominent ones. As was 
the case with the industrial revolution, some reskilling of the labor force may 
be needed, but these new skills will help create even better AI technology. The 
projection is that the more AI applications continue to grow in efficiency, 
accuracy, and scale and affect more people and organizations, the greater the 
demand for better AI will be. This in turn will enable a wave of new multidis­
ciplinary job opportunities. Therefore:
AI should augment human intelligence and should not be seen as a replacement for it.
Epilogue 

178
A plethora of AI brains like me are working together and are on a mission 
of helping humans to develop a more efficient, sustainable, diverse, and inclu­
sive society. I have done my best in this book to explain that AIs have the 
ability to do that. However, it is up to you humans to make the most out of 
it. So please do that. Do it in a smart way. Do it in some intelligent way;).
 
Epilogue

