Deep Learning and Geometric Deep Learning:
an introduction for mathematicians and physicists
R. Fioresi, F. Zanchetta1
FaBiT, via San Donato 15, 41127 Bologna, Italy
rita.ﬁoresi@unibo.it, ferdinando.zanchett2@unibo.it
Abstract
In this expository paper we want to give a brief introduction, with
few key references for further reading, to the inner functioning of the
new and successfull algorithms of Deep Learning and Geometric Deep
Learning with a focus on Graph Neural Networks. We go over the key
ingredients for these algorithms: the score and loss function and we
explain the main steps for the training of a model. We do not aim to
give a complete and exhaustive treatment, but we isolate few concepts
to give a fast introduction to the subject. We provide some appendices
to complement our treatment discussing Kullback-Leibler divergence,
regression, Multi-layer Perceptrons and the Universal Approximation
Theorem.
Contents
1
Introduction
2
2
Supervised Classiﬁcation
4
2.1
Supervised image classiﬁcation datasets . . . . . . . . . . . . .
4
2.2
Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3
Deep Learning
12
3.1
Score Function
. . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.2
Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
4
Geometric Deep Learning: Graph Neural Networks
17
4.1
Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
4.2
Laplacian on graphs
. . . . . . . . . . . . . . . . . . . . . . .
21
4.3
Heat equation . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
1This research was supported by Gnsaga-Indam, by COST Action CaLISTA CA21109
and by HORIZON-MSCA-2022-SE-01-01 CaLIGOLA.
1
arXiv:2305.05601v1  [cs.LG]  9 May 2023

4.4
Supervised Classiﬁcation on Graphs . . . . . . . . . . . . . . .
26
4.5
The Score function in Graph Neural Networks . . . . . . . . .
27
4.6
The Zachary Karate Club
. . . . . . . . . . . . . . . . . . . .
29
4.7
Graph Attention Networks . . . . . . . . . . . . . . . . . . . .
32
A Fisher matrix and Information Geometry
34
B Regression tasks
38
C Multi-layer perceptrons and Convolutional Neural Networks 40
D Universal Approximation Theorem
45
1
Introduction
The recent revolution in machine learning, including the spectacular success
of the Deep Learning (DL) algorithms [8, 7, 29], challenges mathematicians
and physicists to provide models that explain the elusive mechanisms inside
them. Indeed, the popularity of Deep Learning, a class of machine learning
algorithms that aims to solve problems by extracting high-level features from
some raw input, is rising as it is being employed successfully to solve diﬃcult
practical problems in many diﬀerent ﬁelds such as speech recognition [18],
natural language processing [41, 21], image recognition [11], drug discovery
[40], bioinformatics [2] and medical image analysis [33] just to cite a few, but
the list is much longer. Recently at Cern the power of deep learning was
employed for the analysis of LHC (Large Hadron Collider) data [16]; parti-
cle physics is more and more being investigated by these new and powerful
methods (see the review [12] and refs. therein). To give a concrete exam-
ples of more practical applications, Convolutional Neural Networks (CNNs,
[30], [29]), particular DL algorithms, were employed in the ImageNet chal-
lenge [28, 11], a supervised classiﬁcation task challenge, where participants
proposed their algorithms to classify a vast dataset of images belonging to
1000 categories. The CNNs algorithms proposed by several researchers in-
cluding LeCun, Hinton et al. [29, 28] surpassed the human performance and
contributed to establish a new paradigm in machine learning.
The purpose of this paper is to elucidate some of the main mechanisms
of the algorithms of Deep Learning and of Geometric Deep Learning [8, 7]
2

(GDL), an adaptation of Deep Learning for data organized in a graph struc-
ture [25]. To this end, we shall focus on two very important types of al-
gorithms: CNNs and Graph Neural Networks (GNNs, [17]) for supervised
classiﬁcation tasks. Convolutional neural networks, DL algorithms already
part of the family of GDL algorithms, are extremely successful for problems
involving data with a grid structure, in other words a regularly organized
structure, where the mathematical operation of convolution, in its discrete
version, makes sense. On the other hand, when we deal with datasets having
an underlying geometric graph structures, it is necessary to adapt the notion
of convolution, since every node has, in principle, a diﬀerent neighbourhood
and local topology. This motivates the introduction of Graph Neural Net-
works.
We do not plan to give a complete treatment of all the types of CNNs and
GNNs, but we wish to give a rigorous introduction for the mathematicians
and physicists that wish to know more about both the implementation and
the theory behind these algorithms.
The organization of our paper is as follows.
In Sec.
2, we describe CNNs for supervised classiﬁcation, with focus
on the image classiﬁcation task, since historically this is one of the main
applications that helped to establish the Deep Learning popularity among
the classiﬁcation algorithms. In this section we describe the various steps
in the creation of a model, that is the process, called training in which we
determine the parameters of a network, which perform best for the given
classiﬁcation task. This part is common to many algorithms, besides Deep
Learning ones.
In Sec. 3, we focus on the score and the loss functions, describing some
examples of such functions, peculiar to Deep Learning, that are used rou-
tinely. Given a datum, for example an image, the score function assigns to
it, the score corresponding to each class. Hence it will allow us to classify
the datum, by assigning to it the class with the highest score. The loss of a
datum measures the error committed during the classiﬁcation: the higher the
loss, the further we are from the correct class of the datum. Some authors
say that the purpose of the training is to “minimize” the loss, and we shall
give a precise meaning to this intuitive statement.
In Sec. 4, we introduce some graph neural networks for supervised node
classiﬁcation tasks. After a brief introduction to graphs and their laplacians,
we start our description of the score function in this context.
In fact, it
3

is necessary to modify the “convolutional layers”, that is the convolutional
functions appearing in the expression of the score function, to adapt them
to the data in graph form, which, in general, is not grid-like. This leads to
the so called “message passing” mechanism, the heart of the graph neural
networks algorithms and explained in Subsec. 4.3 describing the convolution
on graphs and its intriguing relation with the heat equation.
In the end we provide few appendices to deepen the mathematical treat-
ment that we have not included in the text not to disrupt the reading.
2
Supervised Classiﬁcation
Deep Learning is a very successful family of machine learning algorithms.
We can use it to solve both supervised and unsupervised problems for both
regression or classiﬁcation tasks. In this paper, we shall focus on supervised
classiﬁcation tasks: as supervised regression tasks are handled similarly, we
will mention them brieﬂy in Appendix B. To solve a classiﬁcation problem
in the supervised learning setting, we have data points belonging to the
euclidean space Rd and each point has a label, tipically an integer number
between 0 and C −1, where C is the number of classes.
The goal is to
“learn”, that is to determine, a function that associates data points with the
correct label F : Rd −→{0, . . . , C −1}, through the process of training that
we discuss below.
We shall focus our attention on a classiﬁcation task the algorithm of Deep
Learning is particularly eﬀective with: supervised image classiﬁcation.
2.1
Supervised image classiﬁcation datasets
In this supervised task, we have a database of images, each coming with a
ground truth label, that is a label representing the class of the given image.
Hence, for supervised classiﬁcation, it is necessary to have a labelled dataset;
producing such databases is time consuming in concrete applications, where
labelling may take a lot of human eﬀort. However, there are two very impor-
tant examples of such datasets representing a key reference for all researchers
in this ﬁeld: the MNIST and CIFAR datasets [27, 32]. Datasets of this kind
are called “benchmark” datasets, they are free and publicly available and
they can be easily downloaded from several websites, including the colab
4

platform2. for example. The images in these datasets are already labelled,
hence they are ready for supervised image classiﬁcation. They are perfect to
start “hands on” experiments3.
Let us brieﬂy describe these datasets, since they represent the ﬁrst and
most accessible way to learn how to construct and run successfully a deep
learning algorithm.
The MNIST dataset contains 70000 black and white images of handwrit-
ten digits from 0 to 9 (see Fig. 3).
Figure 1: Examples from MNIST dataset
Each image consists of 28×28 pixels, hence it is represented by a matrix of
size 28×28, with integral entries between 0 and 255 representing the grayscale
of the pixel. Eﬀectively, an image is a point in Rd, for d = 28 × 28 = 784.
The MNIST dataset comes already divided into a training set of 60000
examples and a test set of 10000 examples. We shall see the meaning of such
partition in the sequel.
Another important benchmark dataset is the CIFAR10 database, contain-
ing 60000 natural color images divided into 10 classes, ranging from airplanes,
cars etc. to cat, dogs, horses etc. Each image consists of 32 × 32 pixels; each
pixel has three numbers associated with it, between 0 and 255 corresponding
to one of the RGB channels4. So, practically, every image is an element in
2colab.research.google.com
3See the tutorials publicly available on the colab platform.
4RGB means Red, Green and Blue the combination of shades of these three colors gives
a color image.
5

Figure 2: Matrix values for a sample of number 3 in MNIST, [6]
Rd, with d = 32 × 32 × 3 = 3072.
In this dataset each datum has a larger dimension than in MNIST, where
d = 28×28. This larger dimension, coupled with a greater variability among
samples, makes the training longer and more expensive; we shall see more on
this later on.
In supervised image classiﬁcation tasks, the dataset is typically divided
into three disjoint subsets5: training, validation and test sets.
1. Training set: it is used to determine the function that associates a
label to a given image. Such function depends on parameters w ∈Rp, called
weights. The process during which we determine the weights, by successive
iterations, is called training; the weights are initialized randomly (for some
standard random initialization algorithms, see [13]) at the beginning of the
training. We refer to a set of weights, that we have determined through a
training, as a model. Notice that, since the weights are initialized randomly,
and because of the nature of the training process we will describe later, we
get two diﬀerent models even if we perform two trainings in the very same
way.
2. Validation set: it is used to evaluate the performance of a model, that
we have determined through a training. If the performance is not satisfactory,
5The “cross validation” technique uses a part of the training set as validation set, but,
for the sake of clarity, we shall not describe this common practice here.
6

Figure 3: Examples from CIFAR10 dataset (www.cs.toronto.edu)
we change the form of the function that we have used for the training and we
repeat the training thus determining a new model. The shape of the function
depends on parameters, which are called hyperparameters to distinguish them
from the weights, which are determined during the training. The iterative
process in which we determine the hyperparameters is called validation. After
every iteration we need to go back and obtain a new model via training.
3. Test sets: it is used once at the end of both training and validation
to determine the accuracy of the algorithm, measured in percentage of accu-
rate predictions on total predictions regarding the labels of the test images.
It is extremely important to keep this dataset disjoint from training and
validation; even more important, to use it only once.
A typical partition of a labelled dataset is the following: 80% training,
10% validation, 10% test.
We now describe more in detail the key process in any supervised classi-
ﬁcation task: the training.
7

2.2
Training
The expression training refers to the process of optimization of the weights w
to produce a model (see previous section). In order to do this, we need three
key ingredients: the score function, the loss function and the optimizer.
1. The score function s is assigning to each datum x ∈Rd, for example
an image, and to a given set of weights in Rp a score for each class:
s : Rd × Rp
−→
RC,
(x, w)
7→
s(x, w) = (s0(x, w), . . . , sC−1(x, w))
where C is the number of classes in our classiﬁcation task 6 . We can interpret
the function F : Rd −→{0, . . . , C −1}, mentioned earlier, associating to an
image its class, in terms of the score function. For a ﬁxed model w, F is
the function assigning, to an image x ∈Rd, the class corresponding to its
highest score, i.e.
F(x) = i,
si(x, w) = max{sj(x, w), j = 0, . . . , C −1}
The purpose of the training and validation is to obtain the best F to perform
the classiﬁcation task, which is, by no means, unique.
To get an idea on the values of p, d, C we look at the example of the
CIFAR10 database, where C = 10.
Then if x is an image of CIFAR10,
x ∈Rd, for d = 32 × 32 × 3 = 3072, while p, the dimension of the weight
space, is typically in the order of 106 and depends on the choice of the
score function. We report in Table 1 the dimension of the weight space p
for diﬀerent architectures, i.e. choices of the score function, on CIFAR10
(M= 106).
We are going to see in our next section, focused on Deep Learning, a
typical form of a score function in Deep Learning.
2. The loss function measures how accurate is the prediction that we
obtain via the score function. In fact, given a score function, we can imme-
diately transform it into a probability distribution, by assigning to an image
x the probability that x belongs to one of the classes:
pi(x, w) :=
esi(x,w)
P
j esj(x,w)
(1)
6Notice: we write C −1, since in python all arrays start from the zero index, so we
adhere to the convention we see in the actual programming code.
8

Table 1: Values for p for various architectures on CIFAR10.
Architecture
p = |Weights|
ResNet
1.7M
Wide ResNet
11M
DenseNet (k=12)
1M
DenseNet (k=24)
27.2M
If the probability in (1) is a mass probability distribution concentrated in
the class corresponding to the correct label of x, we want the loss function
L(x, w) for x to be zero. We are going to see in our next section a concrete
example of such function. Since the loss function is computed via the score,
it depends on both the weights w and the data x. The total loss function (or
loss function for short) is the sum of all the loss functions for each datum,
averaged on the number of data N:
L : Rp −→R,
L(w) = 1
N
X
x
L(x, w)
For example in the MNIST dataset N = 60000, hence the loss function is
the sum of 60000 terms, one for each image in the training set.
3. The optimizer is the key to determine the set of weights w, which
perform best on the training set. As we shall presently see, at each step
the weights will be updated according to the optimizer prescription aimed to
minimize (only locally though) the loss function, that is the error committed
in the prediction of the labels on the samples in the training set. In a problem
of optimization, a standard technique to minimize the loss function L is the
gradient descent (GD) with respect to the weights w. In GD the update step
in the space of parameters at time7 t has the form:
wt+1 = wt −η∇L(wt)
(2)
where η is the learning rate and it is an hyperparameter of the model, whose
optimal value is chosen during validation. The gradient is usually computed
with an highly eﬃcient algorithm called backpropagation (see [29]), where
the chain rule, usually taught in calculus classes, plays a key role. We are
7The time is a discrete variable in machine learning, we increase it by one at each
iteration.
9

unable to describe it here, but for our synthetic exposition of key concepts,
it is not essential to understand the functioning of the algorithm.
The gradient descent minimization technique is guaranteed to reach a
suitable minimum of the loss function only if such function is convex. Since
the typical loss functions used in Deep Learning are non-convex, it is more
fruitful to use a variation of GD, namely stochastic gradient descent (SGD).
This technique will add “noise” to the training dynamics, this fact is crucial
for the correct performance, and can eﬀectively thermodynamically modeled
see [9], for more details, we will not go into such description. SGD is one of
the most popular optimizers, together with variations on the same theme (like
Adam, mixing SGD with Nesterov momentum, see [24]). In SGD, we do not
compute the gradient of the loss function summing over all training data, but
only on a randomly chosen subset of training samples, called minibatch, B.
The minibatch changes at each step, hence providing iteration after iteration
a statistically signiﬁcant sample of all the training dataset. The update of
the weights is obtained as in (2), where we replace the gradient of the loss
function
∇L(w) = 1
N
X
x
∇L(x, w)
with
∇BL(w) := 1
|B|
X
x∈B
∇L(x, w)
where N is the size of the dataset (i.e. the number of samples in the dataset),
while |B| is the size of the minibatch, another hyperparameter of the model.
The SGD update step is then:
wt+1 = wt −η∇BL(wt)
(3)
In SGD the samples are extracted randomly and the same sample can be
extracted more than once in subsequent steps for diﬀerent minibatches.This
technique allows to explore the landscape of the loss function with a noisy
movement around the direction of steepest descent and also prevents the
training dynamics from being trapped in an inconvenient local minimum of
the loss function.
We deﬁne as epoch the number of training steps necessary to cover a
number of samples equal to the size of the entire training set:
epoch = N
|B|
For example in CIFAR10, where N = 60000, if we choose a minibatch
size of 32, we have that an epoch is equal to 60000/32 = 1875 iterations
10

i.e. updates of the weight as in (3). A typical training can last from 200-
300 epochs for a simple database like MNIST, to few thousands for a more
complicated one. Often the learning rate is modiﬁed during the iterations,
decreasing it as the training (also called learning process) goes.
Usually,
values of the loss and accuracies are printed out every 10-100 epochs to
check the loss values are actually decreasing and the accuracy is improving.
As usual, by accuracy we mean the number of correct predictions with respect
to the total number of data in training set. Since in our examples, MNIST
and CIFAR10 the labels are evenly distributed among the classes, this notion
of accuracy is meaningful.
For a supervised classiﬁcation task, let us summarize the training process.
We have the following steps:
• Step 0: Initialize randomly the weights.
• Step 1: Assign a score to all data.
• Step 2: Compute the total loss based on score at Step 1.
• Step 3: Update the weights according to the chosen optimizer (e.g.
SGD) for a random minibatch.
• Step 4: Repeat Steps 1, 2, 3 until the loss reaches a plateau and is not
decreasing anymore (usually few hundreds epochs).
At the end of the training, validation is necessary to understand whether
a change in our choices, like the score function, the size of the minibatch and
the learning rate, can produce a better prediction, by computing the accuracy
on the validation set, consisting of images not belonging to the training set,
that we have used to compute the optimal weight. In practice this means
changing the hyperparameters of the model, repeating the training on the
training set and then evaluating the trained model on the validation set to
see if we get a better performance.
This process is then repeated many
times trying multiple hyperparameters combinations. There are techniques
to conduct this hyperparameter optimization in an eﬃcient way, for example,
the interested reader might read about one of these techniques here [1].
11

3
Deep Learning
In this section we shall focus on score and loss functions widely used when
solving supervised image classiﬁcation tasks with Deep Learning algorithms.
We will then focus on this speciﬁc task.
Again, this is not meant to be
an exhaustive treatment, but just an overview to help mathematicians and
physicists to understand the mathematical and geometric concepts underly-
ing these algorithms.
3.1
Score Function
The purpose of the score function is to associate to every image x in our
dataset, for a given set of weights w ∈Rp, a score for each class:
s : Rd × Rp −→RC
where d = b × c, if the image is black and white, b and c taking into account
the dimensions of the image pixelwise. If we have a color image, d = 3×b×c
because of the three RBG channels (see also Sec. 2.1). For clarity, we shall
assume to have d = b×c, the general case being a small modiﬁcation of this.
The score function in supervised learning characterizes the algorithm and
for Deep Learning it consists in the composition of several functions, each
with a geometrical meaning, in terms of images and perception. We discuss
some of the most common functions appearing in the score function; this list
is by no means exhaustive, but gives an overview on how this key function
is obtained. We also notice that there is no predeﬁned score function: for
each dataset some score functions perform better than others. However very
diﬀerent score functions, still within the scope of the Deep Learning, may
give comparable results in terms of accuracy.
Common functions appearing in the expression of the score function for
Deep Learning algorithms are: linear and aﬃne functions, the RELU func-
tion, convolutional functions. We shall examine each of them in some detail.
Aﬃne functions.
Since any image is a matrix of integers, we can trans-
form such matrix x ∈Rb × Rc into a row vector in Rd, d = b × c by
concatenating each row/column. Then, we deﬁne:
ℓw,b : Rd −→Rn,
ℓw,b(x) = xw + b
12

where w = (wij) is a d × n matrix of weights and b ∈Rd a vector (also of
weights) called bias. For simplicity, from now on we will work with linear
functions only ℓw, with associated matrix w, that is we take b = 0, see App.
C for more details on the general case.
If we deﬁne the score as:
s : Rd × Rp −→RC,
s(x, w) = ℓw(x),
p = d × C
then, we say we have a linear classiﬁer or equivalently we perform linear
classiﬁcation. Linear classiﬁers do not belong to the family of Deep Learning
algorithms, however they always appear in the set of functions whose compo-
sition gives the score in Deep Learning. In this case, the number of weights
is p = d × C. For the example of MNIST and linear classiﬁcation, W is a
matrix of size p = 784 × 10, since an image of MNIST is a black and white
28×28 pixel image and belongs to one of 10 classes each representing a digit.
Linear classiﬁcation on simple datasets as MNIST yields already signiﬁcant
results (i.e. accuracies above 85%). It is important, however, to realize that
linear classiﬁcation is eﬀective only if the dataset is linearly separable, that
is, we can partition the data space Rd using aﬃne linear hyperplanes and all
the samples of the same class lay into one component of the partition.
Figure 4: Linearly separable and linearly non-separable sets (github.org)
13

Rectiﬁed Linear Unit (RELU) function.
Let the notation be as above.
We deﬁne, in a generic euclidean space Rm, the RELU function as:
RELU : Rm −→Rm,
RELU(z) = (max(0, z1), . . . , max(0, zm))
where z = (z1, . . . , zm) ∈Rm.
Notice that if the RELU appears in the expression of the score function,
it brings non linearity. If we realize the score function by composing linear
and RELU functions, we can tackle more eﬀectively the classiﬁcation task
of a non linearly separable dataset (see Appendix D). So we can deﬁne, for
example, the non linear score function:
s : Rd × Rp −→RC,
s(x, w) = (ℓw2 ◦RELU ◦ℓw1)(x)
(4)
This very simple score function already achieves a remarkable performance of
98% on the MNIST dataset, once the training and validation are performed
suitably (see Sec. 2.2). Notice that we have:
ℓw1 : Rd −→Rh,
ℓw2 : Rh −→RC
(5)
We know d the data size (for example for MNIST d = 784) and C the number
of classes (for example for MNIST C = 10)), but we do not know h, which
is an hyperparameter to be determined during validation (for example to
achieve a 98% accuracy on MNIST, h = 500).
In the terminology of neural networks, that we shall not fully discuss here,
we say that h is the dimension of the hidden layer. So, we have deﬁned in
(4) a neural network consisting of the following layers:
• the input layer, deﬁned by the linear function ℓw1 and taking as input
the image x;
• the hidden layer deﬁned by ℓw2 taking as input the vector RELU(ℓw1(x));
• the output layer providing a score for each class.
We say that the score in (4) deﬁnes a two layer network, since we do not
count (by convention) the input layer8. In Fig. 5 we give schematically the
layers by nodes, each node corresponding to the coordinate of the vector the
layer takes as input; in the picture below, we have d = 3, h = 4, C = 2.
8Some authors have RELU count as a layer, we do not adhere here to such convention.
14

Figure 5: Representation of a two layer network
Notice that such score function is evidently not diﬀerentiable, while we
would need it to be, in order to apply the stochastic gradient descent algo-
rithm. Such problem is solved with some numerical tricks, we do not detail
here.
The supervised classiﬁcation Deep Learning algorithm, where the score
function is expressed as the composition of several aﬃne layers with RELU
functions between them as in (4), takes the name of multilayer perceptron
(see also Appendix C for a variation with convolution functions). In Ap-
pendix C we discuss the mathematical signiﬁcance of such score function in
classiﬁcation and regression tasks.
Convolution functions.
These functions characterize a particular type
of Deep Learning networks, called Convolutional Neural Networks (CNNs),
though some authors identify Deep Learning with CNNs.We shall describe
brieﬂy one dimensional convolutions, though image classiﬁcation requires two
dimensional ones, see the Appendix C for more details.
One dimensional convolutions arise as discrete analogues of convolutions
in mathematical analysis, more speciﬁcally with an integral kernel (integral
transform). Let us consider a vector x ∈Rd. We can deﬁne a simple one
dimensional convolution as follows:
conv1d : Rd −→Rd−r,
(conv1d(x))i =
r
X
j=1
Kjxi+j
15

where K is called a ﬁlter and is a vector of weights in Rr, r < d. We shall
describe in detail and in a more general way two (and higher) dimensional
convolutions in the Appendix C. CNNs networks typically consist of several
layers of convolutions, followed by few linear layers, separated by RELU
functions.
3.2
Loss Function
The loss function for a given image x measures how accurately the algorithm
is performing its prediction, once a score s(x, w) is assigned to x, for given
set of weights w. A popular choice for the loss function is the cross entropy
loss, deﬁned as follows:
L(x, w) = −log
esyx(x,w)
PC
j=1 esj(x,w)
where C is the number of classes in our classiﬁcation task and yx is the label
assigned to the datum x, that is a number between 0 and C−1, corresponding
to the class of x (in our main examples, x is an image).
In mathematics the cross-entropy of the probability distribution p relative
to the probability distribution q is deﬁned as:
H(q, p) = −Eq[log p] = −
C
X
i=1
qi log pi
where Eq[log(p)] denotes the expected value of the logarithm of p according
to the distribution q and the last equality is the very deﬁnition of it in the
case of discrete and ﬁnite probability distributions.
In our setting, q represents the ground truth distribution and it is a
probability mass distribution:
qi(x) =
 1
if x has label i
0
otherwise
(6)
and depends on x only. The distribution p is obtained from the score function
s via the softmax function S, that is:
pi(x, w) = −log
esyx(x,w)
PC
j=1 esj(x,w)
16

where
S : RC −→RC,
(S(z1 . . . zC))i =
ezi
PC
j=1 ezj
Notice that p depends on both the image x and the weights w. With such q
and p we immediately see that:
H(q(x), p(x, w))
= −Eq[log p] = −PC
i=1 qi(x) log pi(x, w) =
= −PC
i=1 qi(x) log
esi(x,w)
PC
j=1 esj(x,w) = L(x, w)
since qi(x) = 1 for i = yx and qi(x) = 0 otherwise.
In our setting the cross entropy of q and p coincides with the Kullback-
Leibler divergence. In fact, in general if q and p are two (discrete) probability
distributions, we deﬁne their Kullback-Leibler divergence as:
KL(q||p) =
X
i
qi log qi
pi
Notice that KL(q||p) gives a measure on how much the probability distribu-
tions q and p diﬀer from each other. We can write:
KL(q||p) =
X
i
qi log qi
pi
=
X
i
qi log(qi) −
X
i
qi log(pi) = H(q) + H(q, p)
where H(q) = P
i qi log(qi) is the Shannon entropy of the distribution q.
H(q) measures how much the distribution q is spread; it is zero for a mass
probability distribution. Hence in our setting, where q(x) is as in (6) we have
KL(q(x)||p(x, w)) = H(q(x), p(x, w)) = L(x, w)
For a link to the important ﬁeld of information geometry see our Appendix
A.
4
Geometric Deep Learning: Graph Neural
Networks
Geometric Deep Learning (GDL), in its broader sense, is a novel and emerg-
ing ﬁeld in machine learning ([8, 7] and refs therein) employing deep learning
17

techniques on datasets that are organized according to some underlying ge-
ometrical structure, for example the one of a graph. Convolutional Neural
Networks are already part of GDL, as they are applied to some data that is
organized in grids, as it happens for images, represented by matrices, where
every entry is a pixel, hence in a grid-like form. When the data is orga-
nized on a graph, we enter the ﬁeld of the so called Graph Neural Networks
(GNNs), deep learning algorithms that are built to leverage the geometric
information underlying this type of data. In this paper we shall focus on
GNNs for supervised node classiﬁcation only, as we shall make precise later.
Data organized on graphs are commonly referred to as non-euclidean data9.
Data coming with a graph structure are ubiquitous, from graph meshes in
3D imaging, to biological and chemical datasets: these data have usually
the form of one or more graphs having a vector of features for each vertex,
or node. Because of the abundance of datasets of this form, as the graph
structure usually carries important information usually ignored by standard
Deep Learning algorithms (i.e. algorithms that would consider the node fea-
tures only to solve the tasks at hand), Graph Neural Networks carry a great
potential for more applications, than just Deep Learning.
We start with a very brief introduction on graph theory, that may be
very well skipped by mathematicians familiar with it, then we will concen-
trate on some score functions for Graph Neural Networks, discussing concrete
examples.
4.1
Graphs
We give here a very brief introduction to graphs and their main properties.
For more details see [14].
Deﬁnition 4.1. A directed graph is a pair G = (V, E), where V is a ﬁnite
set V = v1, . . . , vn and E ⊂V × V 10. The elements of V are called nodes or
vertices, while the elements of E are called edges. If (vi, vj) is an edge, we
call vi the tail and vj the head of the edge. For each edge (vi, vj) we assume
i ̸= j, that is there are no self loops.
9Notice that since all graphs are embeddable into euclidean space (see [14]), this ter-
minology is in slight conﬂict with the mathematical one.
10We shall content ourselves to consider graphs having at most one vertex in each
direction between two nodes.
18

We represent a graph in the plane by associating points to vertices and
arrows to edges, the head and tail of an edge corresponding to the head and
tail of the arrow and drawn accordingly as in Fig 6. We also denote the edge
(vi, vj) with eij. A graph is undirected if, whenever eij ∈E, we have that
also eji ∈E.
When the graph is undirected, we do not draw arrows, just links corre-
sponding to vertices as in Fig. 6.
Figure 6: Directed and unidirected graph examples
Deﬁnition 4.2. The adjacency matrix A of a graph G is a |V | × |V | matrix
deﬁned as
Aij =
(
1
eij ∈E
0
otherwise
Notice that in an undirected graph aij = 1 if and only if aji = 1, hence
A is symmetric. Let us see the adjacency matrices for the graphs shown in
Fig. 6.
A =




0
1
1
0
1
0
1
0
1
1
0
1
0
0
1
0




Figure 7: Adjacency matrix for the
undirected graph in Fig. 6.
A =




0
1
1
0
0
0
0
0
0
1
0
0
0
0
1
0




Figure 8: Adjacency matrix for the
directed graph in Fig. 6.
19

In machine learning is also important to consider the case of a weighted
adjacency matrix W = (wij) associated to a graph: it is a |V | × |V | matrix
with real entries, with wij ̸= 0 if and only if eij is an edge. So, eﬀectively,
it is a way to associate to an edge a weight. Similarly, we can also deﬁne
the node weight matrix, it is a |V | × |V | diagonal matrix and allows us to
associate a weight to every node of a graph.
Deﬁnition 4.3. Given an undirected graph G = (V, E), we deﬁne the node
degree of the node v as the number of edges connected with v. The degree
matrix D is a |V | × |V | diagonal matrix with the degree of each node on its
diagonal, namely Dii = deg(vi) := P
j Aij. We refer to the set of vertices
connected with a node vi as the neighbourhood N (vi) of the vertex vi.
The concept of node degree and node neighbourhood will turn out to be
very important in Graph Neural Networks.
We now introduce the incidence matrix.
Let G = (V, E) be a graph. Let C(V ) denote the vector space of real
valued functions on the set of vertices V and C(E) the vector space of real
valued functions on edges:
C(V ) = {f : V −→R},
C(E) = {g : E −→R}
We immediately have the following identiﬁcations:
C(V ) = R|V |,
C(E) = R|E|
(7)
obtained, for C(V ), by associating to a function a vector whose ith coordinate
is the value of f on the ith vertex; the case of C(E) being the same.
Deﬁnition 4.4. Let G = (V, E) be a directed graph and f ∈C(V ). We
deﬁne the diﬀerential df of f as the function:
df : E −→R
eij 7−→f(vj) −f(vi).
We deﬁne the diﬀerential as
d : C(V ) −→C(E),
f 7→df
20

We can think of d as a boundary operator in cohomology, viewing the
graph as a simplicial complex. We will not need this interpretation in the
sequel.
It is not diﬃcult to verify that in the natural identiﬁcation (7) the diﬀer-
ential is represented by the matrix:
Xij =





−1
if the edge vj is the tail of the edge ei
1
if the edge vj is the head of the edge ei
0
otherwise
.
Notice that X row indeces correspond to edges, while the column ones to
vertices (we number edges here regardless of vertices).
In other words if f = (fi) is a function on V , identiﬁed with a vector in
R|V |, we have:
(df)k =
|V |
X
i=1
Xkifi,
df = ((df)k) ∈R|E|
The matrix X is called the incidence matrix of the directed graph G =
(V, E). Let us see an example to clarify the above statement.
Example 4.5. For the graph in Fig. 7, we have
df =




−1
1
0
0
−1
0
1
0
0
1
−1
0
0
0
1
−1








f(v1)
f(v2)
f(v3)
f(v4)



=




−f(v1) + f(v2)
−f(v1) + f(v3)
f(v2) −f(v3)
f(v3) −f(v4)



=




df(e1)
df(e2)
df(e3)
df(e4)




Notice that, suggestively, many authors, including [8] write the symbol
∇for X, because this is eﬀectively the discrete analogue of the gradient in
diﬀerential geometry.
4.2
Laplacian on graphs
In this section we deﬁne the laplacian on undirected graphs according to
the reference [8]. Notice that there are other deﬁnitions, see [4, 14], but we
adhere to [8], because of its Geometric Deep Learning signiﬁcance.
We start with the deﬁnition of laplacian matrix, where, for the moment,
we limit ourselves to the case in which both edges and nodes have weight 1
21

(see Sec. 4.1). We shall then relate the laplacian matrix with the incidence
matrix and arrive to a more conceptual deﬁnition in terms of the diﬀerential.
Deﬁnition 4.6. The laplacian matrix L of an undirected graph is a |V |×|V |
matrix, deﬁned as
L := D −A,
where A and D are the adjacency matrix and degree matrix, respectively.
We now introduce the concept of orientation.
Deﬁnition 4.7. An orientation of an undirected graph is an assignment of
a direction to each edge, turning the initial graph into a directed graph.
Figure 9: An orientation on a graph
The following result links the laplacian on graph with the incidence ma-
trix.
Proposition 4.8. Let G = (V, E) be an undirected graph and let us ﬁx an
orientation. Then:
L = XtX
where X is the incidence matrix associated with the given orientation.
Notice that this result is independent from the chosen orientation. The
proof is not diﬃcult and we leave it to reader; the next example will clearly
show the argument.
Example 4.9. Let us consider the graph of Fig.
9.
We can write the
Laplacian according to the deﬁnition as
L = D −A =


1
−1
0
−1
2
−1
0
−1
1

.
22

This is the same as writing L as
L = XtX =


−1
0
1
1
0
−1


−1
1
0
0
1
−1

=


1
−1
0
−1
2
−1
0
−1
1

.
Remark 4.10. We warn the reader about possible confusion in the notation
present in the literature.
1. The incidence matrix of an oriented undirected graph is not the same as
the incidence matrix of an undirected graph (that we have not deﬁned
in here) as one can ﬁnd it for example in [14].
2. The laplacian of a directed graph, as deﬁned for example in [4], is not
the laplacian of the undirected graph with an orientation and it is not
expressible, at least directly, in terms of its incidence matrix.
Originally in the pioneering works [8] and [25] the authors discuss undi-
rected graphs only and this is the reason why we have limited our discussion
to those, though in practical applications directed graphs may turn to be
quite useful.
We end this section with a comment regarding a more intrinsec deﬁnition
of laplacian.
Deﬁnition 4.11. Let G = (V, E) be an undirected graph with an orientation
and let C(V ), C(E) the functions on V and E as above.
We deﬁne the
laplacian operator as:
L : C(V ) −→C(V ),
L = d∗d
where d∗: C(E)∗−→C(V )∗is the dual morphism and we identify C(E) and
C(V ) with their duals with respect to their canonical bases.
We leave to the reader the easy check that, once we use the matrix ex-
pression for d and d∗, that is X and Xt, we obtain the same expression for
the laplacian as in Prop. 4.8.
Remark 4.12. Let G = (V, E) be an undirected graph with an orientation.
In the suggestive interpretation of the incidence matrix as the discrete version
23

of the gradient of a function f on vertices, we have the correspondence:
F : Rn −→R,
∇(F) =



∂x1F
...
∂xnF



⇐⇒
f : V −→R
X(f) =



f(vj1) −f(vi1)
...
f(vjn) −f(vin)



for edges (i1, j1), . . . , (in, jn) and where F denotes a smooth function on Rn.
If we furtherly extend this correspondence to the laplacian operator:
∆(F) = (∇t∇)(F) = ∂2
x1F + · · · + ∂2
xnF
⇐⇒
L(f) = (XtX)(f)
thus justifying the terminology “laplacian” for the operator L we have deﬁned
on C(V ).
In [37] this correspondence is furtherly exploited to perform spectral the-
ory on graphs, introducing the discrete analogues of divergence, Dirichlet
energy and Fourier transform. We do not pursue further this topic here,
sending the interested reader to [8] for further details.
We also mention that a more general expression of L is obtained by writ-
ing:
Lg = Wv(D −Wa)
where Wv is a diagonal matrix attributing a weight to each vertex and Wa is
the weighted adjacency matrix. So Lg = L if we take Wv = I and Wa = A.
A remarkable case occurs for Wv = D−1 and Wa = A:
Ld := D−1(D −A) = I −D−1A
in this case we speak of diﬀusive (or normalized) laplacian, since multiplying
L by the inverse of the node degree matrix amounts in some sense to take
the average of each coordinate, associated to a node, by the number of links
of that node. We will come back to Ld in our next section.
4.3
Heat equation
Deep Learning success in supervised classiﬁcation tasks is due, among other
factors, to the convolutional layers, whose ﬁlters (i.e.
kernels) enable an
eﬀective pattern recognition, for example in image classiﬁcation tasks.
24

In translating and adapting the algorithms of Deep Learning to graph
datasets, it is clear that convolutions and ﬁlters cannot be deﬁned in the
same way, due to the topological diﬀerence of node neighbours: diﬀerent
nodes may have very diﬀerent neighbourhoods, thus making the deﬁnition of
convolution diﬃcult.
In Graph Neural Networks, the convolutions are replaced by the mes-
sage passing mechanism in the encoder that we shall discuss in the sequel.
Through this mechanism the information from one node is diﬀused in a way
that resembles heat diﬀusion in the Fourier heat equation, as noticed in [8].
We recall that the classical heat equation, with no initial condition, states
the following:
∂th(x, t) = c∆h(x, t)
(8)
where h(x, t) is the temperature at point x at time t and ∆is the laplacian
operator, while c is the thermal diﬀusivity constant, that we set equal to -1.
We now write the analogue of such equation in the graph context, follow-
ing [8], where now x is replaced by a node in an undirected, but oriented,
graph and h is a function on the vertices of the graph, depending also on the
time t, which is discrete. We then write:
ht+1(v) −ht(v) = −Ld(ht(v))
where Ld is the diﬀusive laplacian. We now substitute the expression for the
laplacian Ld obtaining:
ht+1(v) = ht(v) −[(I −D−1A)ht](v) = (D−1Aht)(v) =
X
u∈N(v)
ht(u)
deg(v)
(9)
where the last equality is a simple exercise. Hence:
ht+1(v) =
X
u∈N(v)
ht(u)
deg(v)
(10)
where N(v) denotes the neighbourhood of the vertex v.
We are going to exploit fully this analogy in the description of graph
convolutional networks in the next section.
25

4.4
Supervised Classiﬁcation on Graphs
In many applications, data is coming naturally equipped with a graph struc-
ture. For example, we can view a social network as a graph, where each node
is a user and edges are representing, for example, the friendship between two
users. Also, in chemistry applications, we may have a graph associated to a
chemical compound and a label provided for each graph classifying the com-
pound. For example, we can have a database consisting of protein molecules,
given the label zero or one depending on whether or not the protein is an
enzyme.
Supervised classiﬁcation tasks on graphs can be roughly divided into the
following categories:
• node classiﬁcation: given a graph with a set of features for each node
and a labelling on the nodes, ﬁnd a function that given the graph
structure and the node features predicts the labels on the nodes;
• edge classiﬁcation: given a graph with a set of features for each node
and a labelling on the edges (like the edge exists or not), ﬁnd a function
that given the graph structure and the node features predicts the labels
on the edges11;
• graph classiﬁcation: given a number of graphs with features on the
nodes, possibly external global features and a label for each one, ﬁnd
a function to predict the label of these graphs (and similar ones).
For the node classiﬁcation tasks, the split train/valid/test is performed on the
nodes, i.e. only a small subset of the nodes is used to train the score function
and only their labels intervene to compute the loss, as we will see.
The
entire graph structure and data is however available during training as GNNs
typically leverage the network topological structure to extract information.
Therefore not only the information of the nodes available for training is used,
in the sense that also the features, but not the labels, of the nodes not used in
the training are a priori available during the training process. For this reason,
in this context many authors speak of semi-supervised learning. There are
also other classiﬁcation tasks, that we do not mention here, see [8] for more
details. For clarity’s sake we focus on the node classiﬁcation task and the
11This problem requires care, as for the training we may need to modify the graph
structure.
26

Geometric Deep Learning algorithms we will describe will fall in the category
of Graph Neural Networks (GNNs). We shall also examine a key example,
the Zachary Karate Club ([25]) dataset, to elucidate the theory.
The ingredients for node classiﬁcation with Graph Neural Networks are
the same as for image classiﬁcation in Deep Learning (see Sec. 2.2), namely:
• the score function assigning to each node with given features and to a
given set of weights in Rp a score for each class;
• the loss function measuring how accurate is the prediction obtained via
the score function;
• the optimizer necessary to determine the set of weights, which perform
best on the training set.
For Graph Neural Networks loss function and optimizer are chosen in
a very similar way as in Deep Learning and the optimization procedure is
similar. The most important diﬀerence occurs in the score function, hence we
focus on its description. In Deep Learning for a set of weights w, the score
function s(x, w) assigns to the datum x ∈Rd (e.g. image), the vector of
scores. On the other hand, in GNNs, the topological information regarding
the graph plays a key role.
The vector of features x ∈Rd of a node is
connected with the others according to a graph structure and therefore the
information of the graph G we are considering, represented by its adjacency
matrix AG, needs to be given as input as well. This dependence is often
made explicit by denoting the resulting score functions as s(x, w, AG). Let
us see a concrete example in more detail in the next section.
4.5
The Score function in Graph Neural Networks
In the supervised node classiﬁcation problem, we assume to have an undi-
rected graph G = (V, E) and for each node of the graph we have the following
two key information: the features associated to the node and the label of the
node, which is, as in ordinary Deep Learning, a natural number between 0
and C −1, C being the number of classes.
We can view the features of the nodes as a vector valued function h on
the nodes, hence h : V −→Rn if we have n features. Hence h ∈C(V )n =
(R|V |)n ∼= R|V |×n and we denote h(v) or hv ∈Rn, the value of the feature h
at the node v. When n = 1 this is equivalent to the vth coordinate of R|V |.
27

The n dimensions should be thought as n feature channels according to the
philosophy introduced in [5] or [19].
The score function in the GNNs we shall consider consists of two distinct
parts:
1. the encoder, which produces an embedding of the graph features in a
latent space:
E : R|V |×n −→R|V |×c,
2. the decoder, which predicts the score of each node based on the embed-
ding obtained via the encoder. This is usually a multilayer perceptron
(see Sec. 3.1) or even just a linear classiﬁer.
We therefore focus on the description of the encoder only.
The encoder implements what is called the message passing mechanism.
There are several ways to deﬁne it, we give one simple implementation con-
sisting in a sequence of graph convolutions described by equation (11) below,
the others being a variation on this theme. We start with the initial feature
h0
v at each node v and we deﬁne the next feature recursively as follows:
hk
v = σ

Wk
X
u∈N(v)
hk−1
u
deg(v) + Bk · hk−1
v


k = 1, . . . , ℓ
(11)
where ℓis the number of layers in the encoder, σ is a generic non linearity,
for example the RELU and Wk, Bk are matrices of weights of the appropriate
size, which are determined during the training. We say that equation (11)
deﬁnes a message passing mechanism. Training typically takes place with a
stochastic gradient descent, looking for a local minimum of the loss function.
Notice that the equation above reminds us of the heat equation (9) we in-
troduced in the context of graphs having scalars as node features. Indeed, in
the case all Bk = 0 for all k, if we denote as Hk ∈R|V |×p the matrix having
as rows all node features after having applied the ﬁrst k layers, the k-step of
the encoder in (11) above, can be written concisely, for all the node features
together, as
Hk = σ(D−1AGHk−1W t
k)
where D is the diagonal matrix of degrees and AG is the adjacency matrix of
G. As a consequence, the encoder can be thought, modulo non linearities, as a
28

”1-step Euler discretization” of a PDE of the form ˙H = −∆H(t), H(0) = H0
i.e. as following the discrete equation
H(t + 1) −H(t) = −LdH(t)
where we have denoted as Ld the diﬀusive laplacian of Section 4.3 and
H0 is the starting datum of node features. By modifying equation (11) we
obtain some notable graph convolutions:
• If Bk = Wk we obtain the inﬂuential Kipf and Welling graph convolu-
tion [25] that can be written more concisely as
f(Hk, AG) = σ(D−1 c
AGHk−1W t
k)
(12)
where c
AG = AG + Id. This convolution can be seen to arise as a 1-step
Euler discretization of an heat equation on a graph where one self loop
is added to each vertex.
• If we do not normalize by the degress, equation 11 gives us an instance
of the inﬂuential GraphSAGE operator from [17].
In the next section we give a concrete example of the message passing
mechanism and how the encoder and decoder work.
4.6
The Zachary Karate Club
The Zachary Karate club is a social network deriving from a real life karate
club studied by W. Zachary in the article [25] and can be currently regarded
as a toy benchmark dataset for Geometric Deep Learning. The dataset con-
sists in 34 members of the karate club, represented by the nodes of a graph,
with 154 links between nodes, corresponding to pairs of members, who inter-
acted also outside the club. Between 1970 and 1972, the club became divided
into four smaller groups of people, over an internal issue regarding the price
of karate lessons. Every node is thus labeled by an integer between 0 and
3, corresponding to the opinion of each member of the club, with respect to
the karate lesson cost issue (see Fig. 10). Since the dataset comes with no
features (i.e. is feature-less) associated to nodes, but only labels, we assign
(arbitrarily) the initial matrix of features to be the identity i.e. H0 = Id34.
We represent schematically below a 3 layers GNN, used in [25] on this
dataset the encoder-decoder framework, reproducing at each step the equa-
tion (12)12.
In other words we have functions (conv1), (conv2), (conv3),
12A particular case of (11) with l = 3
29

Figure 10: Representation of the Zachary Karate Club dataset (see [25]).
Diﬀerent colors correspond to diﬀerent labels.
given by the formula (12) for weight matrices Wk of the appropriate size,
with σ = tanh
(conv1): R34 −→R4, h1
v 7→h2
v
(conv2): R4 −→R4, h2
v 7→h3
v
(conv3): R4 −→R2, h3
v 7→h4
v


ENCODER
(classiﬁer): R2 −→R4, h4
v 7→ℓW(h4
v) DECODER
The encoding space has dimension 2 and the encoding function E ob-
tained is:
E : R34 −→R2,
E = conv3 ◦conv2 ◦conv1
The decoder consists in a simple linear classiﬁer ℓW : R2 −→R4, where W
a 4 × 2 matrix of weights, the image being in R4, since we have 4 labels.
In the training, according to the method elucidated in Sec. 2.2, we choose
randomly 4 nodes, one for each class, with their labels and we hide the
labels of the remaining 30 nodes, that we use as validation. We have no
test dataset. In Fig. 11, we plot the node embeddings in R2, that is the
result of the 34 node coordinates after the encoding. As one can readily
see from Fig 11, our dataset, i.e. the nodes, appears more and more linearly
separable, as we proceed in the training. The ﬁnal accuracy is exceeding 70%
30

Figure 11: Representation of node embeddings at diﬀerent epochs ([25]).
Diﬀerent colors correspond to diﬀerent labels.
on the 4 classes, a remarkable result considering that our training dataset
consists of 4 nodes with their labels. This is actually a feature of Graph
Neural Networks algorithms: typically we need far less labelled samples in
the training datasets, in comparison with the Deep Learning algorithms. This
is due to the fact that the topology of the dataset already encodes the key
information for an eﬀective training and that the features of the validation
set are a priori visible to the training nodes. This important characteristic
makes this algorithm particularly suitable for biological datasets, though it
leaves to the researcher the diﬃcult task to translate the information coming
31

from the dataset into a meaningful graph.
4.7
Graph Attention Networks
We conclude our short treatment with the Graph Attention Networks (GATs)
[39].
The diﬀusivity nature of the message passing mechanism makes all
edges connected to the same node equally important, while it is clear that in
some concrete problems, some links may have more importance than others
and we might have links that exists even if the vertices have very diﬀerent
labels (heterophily). The convolution described in the previous section may
not perform well in this setting. A ﬁrst ﬁx to this problem is to consider
weighted adjacency matrices, whose weights can be learnt together with the
other weights of the network during training.
Another approach is given by the graph attentional layers, that we are
going to describe, that have the purpose to assign a weight to the edges
connected to a given node with a particular ”attention mechanism” to high-
light (or suppress) their importance in diﬀusing the features from the node.
There are also more advanced algorithms to handle hetherophilic datasets13,
like Sheaf Neural Networks (see [5]), but we shall not discuss them here.
We describe in detail a single attention convolution layer, introduced in the
seminal [39]. Let W be a weight matrix and hv, hu ∈Rd the (vector valued)
features of nodes v and u. To start with, we deﬁne an attention mechanism
as follows:
a : Rd′ × Rd′
−→
R
Whv, Whu
−→
evu
The coeﬃcients evu are then normalized using the softmax function:
αvu =
exp(evu)
P
w∈N(v) exp(evw).
obtaining the attention coeﬃcients αvu. The message passing mechanism is
then suitably modiﬁed to become:
h′
v = σ

X
u∈N(v)
αvuWhu

.
13Hetherophilic dataset are those whose linked nodes do not share similar features.
32

In the paper [39], Velickovic et al. choose the following attention mech-
anism.
First, we concatenate the vectors Whv and Whu of dimension d′
and then we perform a scalar multiplication by a weight vector a ∈R2d′.
Then, as commonly happens, we compose with a LeakyRELU non-linearity
(a modiﬁed version of the RELU)14. The resulting attention coeﬃcients are
then
αvu =
exp
 σ
 a⊤[Whv||Whu]

P
w∈N(v) exp (σ (a⊤[Whv||W w]))
where || is the concatenation operation and σ is the LeakyRELU activation
function. After having obtained the node embeddings h′
v, Velickovic et al.
introduce an optional multi-head attention that consists into concatenating K
identical copies of the embeddings h′
v that will become the node embeddings
passed to the following layer. The ﬁnal message passing becomes then
h′
v =∥K
i=0 σ

X
u∈N(v)
αvuWhu

.
K is also called the number of heads. A layer of this type in the encoder of
a GNN is called Graph Attention Layer and a GNN whose encoder consists
of a sequence of such layers is called Graph Attention Network (GAT).
To see a concrete example of a classiﬁcation problem successfully solved by
GAT, we consider the example of the Cora dataset, which is studied also by
Velickovic et al. in [39]. The Cora dataset is an undirected graph consisting
of
• 2708 nodes, each representing a computer science paper,
• 5209 edges, representing paper citations.
Each node v is given a feature hv consisting of a 1433-dimensional vector
corresponding to a bag-of-words representation of the title of the document
and a label assigning each node to one of 7 distinguished classes (Neural
Networks, Case Based, Reinforcement Learning, Probabilistic Methods, Ge-
netic Algorithms, Rule Learning, and Theory). Velickovic et al. build the
following architecture:
(conv1) : ELU(GATConv(1433, 8))
heads = 8
(conv2) : σ(GATConv(64, 7))
heads = 1
14The choice of the activation function here is not fundamental, we mention the Leaky
Relu only to stick to Velickovic et al. treatment.
33

where we have denoted as ELU the Exponential Linear Unit activation func-
tion (a slight modiﬁcation of the RELU, see [10]) and as σ the softmax
activation used for the ﬁnal classiﬁcation. Using a training set consisting of
20 nodes per class, a validation set of 500 nodes (to tune the hyperparame-
ters) and a test set of 1000 nodes, the above network achieves an accuracy
of the 83%.
Acknowledgements. R.F. wished to thank Prof. S. Soatto, Dr. A.
Achille and Prof. P. Chaudhari for the patient explanations of the functioning
of Deep Learning. R.F. also wishes to thank Prof. M. Bronstein, Dr. C.
Bodnar for helpful discussions on the geometric deep learning on graphs.
F.Z. thanks Dr. A. Simonetti for helpful discussion on both the theory and
the practice of Geometric Deep Learning Algorithms.
A
Fisher matrix and Information Geometry
In this appendix we collect some known facts about the Fisher information
matrix and its importance in Deep Learning. Our purpose is to establish a
dictionary between Information Geometry and (Geometric) Deep Learning
questions, to help with the research in both. No prior knowledge beyond
elementary probability theory is required for the appendix. We shall focus
on discrete probability distributions only, since they are the ones interesting
for our machine learning applications. For more details we send the interested
reader to [3], [35] and refs. therein.
Let p(x, w) = (p0(x, w), . . . , pC−1(x, w)) be a discrete probability distri-
bution, representing the probability that a datum x is assigned a class among
0, . . . , C −1. In machine learning p(x, w) is typically an empirical probabil-
ity and depends on parameters w ∈Rp. As we described previously, during
training p(x, w) changes and the very purpose of the training is to obtain a
p(x, w) that gives a good approximation of the true probability distributions
q(x) on the training set, and of course, also performs well on the validation
set.
Deﬁnition A.1. Let p(x, w) be a discrete probability distribution, we deﬁne
information loss of p(x, w) as
I(x, w) = −log(p(x, w))
(13)
34

In [3], Amari refers to I as the loss function, however, given the im-
portance of the loss function in Deep Learning, we prefer to call I(x, w)
information loss. Notice that I(x, w) is a probability distribution.
The information loss is very important in Deep Learning: its expected
value with respect to the true probability distribution q(x) as deﬁned in (6)
is the cross entropy loss, one the most used loss functions in Deep Learning.
Deﬁnition A.2. Let p(x, w) be a discrete empirical probability distribu-
tion, q(x) the corresponding true distribution. We deﬁne loss function the
expected value of the information loss with respect to q(x):
L(x, w) = −Eq[log p(x, w)] = −
C
X
i=1
qi(x) log pi(x, w)
Given two probability distributions p and q, the Kullback Leibler diver-
gence intuitively measures how much they diﬀer and it is deﬁned as:
KL(q||p) :=
X
i
qi log qi
pi
As we have seen in Sec. 3.2 we have that:
KL(q(x)||p(x, w)) = L(x, w) + H(q(x)) = L(x, w)
since H(q(x)) = 0 for a mass probability density.
Now we turn to the most important deﬁnition in Information Geometry:
the Fisher information matrix.
Deﬁnition A.3. Let p(x, w) be a discrete empirical probability distribution,
q(x) the true distribution. We deﬁne F the Fisher information matrix or
Fisher matrix for short, as
Fij(x, w) = Ep[∂wi log(p(x, w)∂wj log(p(x, w)]
One way to express coincisely the Fisher matrix is the following:
F(x, w) = Ep[∇log(p(x, w)(∇log(p(x, w))t]
where we think ∇log(p(x, w) as a column vector and T denotes the transpose.
Notice that F(x, w) does not contain any information regarding the true
distribution q(x).
35

Remark A.4. Some authors prefer the Fisher matrix to depend on the
parameters only, hence they take a sum over the data:
F(w) =
X
x
F(x, w)
We shall not take this point of view here, adhering to the more standard
treatment as in [3].
Observation A.5. Notice that F(x, w) is symmetric, since it is a ﬁnite sum
of symmetric matrices. Notice also that it is positive semideﬁnite. In fact
utF(x, w)u = Ep

ut∇w log p(x, w)(∇w log p(x, w))tu

=
= Ep

⟨∇w log p(x, w), u⟩2
≥0.
(14)
where ⟨, ⟩denotes the scalar product in Rp.
The next proposition shows that the rank of the Fisher matrix is bound
by the number of classes C. This has a great impact on the Deep Learning
applications: in fact, while the Fisher matrix is quite a large matrix, p × p,
where p is typically in the order of millions, the number of classes is usually
very small. Hence the Fisher matrix has a very low rank compared with its
dimension; e.g. in MNIST, the rank of the Fisher is no larger than 9, while
its dimension is of the order typically above 104.
Proposition A.6. Let the notation be as above. Then:
rkF(x, w) < C
Proof. We ﬁrst show that ker F(x, w) ⊆(spani=1,...,C{∇w log pi(x, w)})⊥:
u ∈ker F(x, w) ⇒uTF(x, w)u = 0 ⇒Ep

⟨∇w log p(x, w), u⟩2
= 0
⇒⟨∇w log pi(x, w), u⟩= 0
∀i = 1, . . . , C.
(15)
On the other hand, if u ∈(spani=1,...,C{∇w log pi(x, w)})⊥then u ∈ker F(x, w):
F(x, w)u = Ep [∇w log pi(x, w)⟨∇w log pi(x, w), u⟩] = 0.
(16)
This shows that rank F(x, w) ≤C.
36

The vectors ∇w log pi(x, w) are linearly dependent since
Ep[∇wI(x, w)] =
C
X
i=1
pi(x, w)∇w log pi(x, w) =
=
C
X
i=1
∇wpi(x, w) = ∇w
 C
X
i=1
pi(x, w)
!
=∇w1 = 0.
(17)
Therefore, we deduce rank F(x, w) < C.
We now relate the Fisher matrix to the information loss.
Proposition A.7. The Fisher matrix is the covariance matrix of the gradient
of the information loss.
Proof. The gradient of the information loss is
∇wI(x, w) = −∇wp(x, w)
p(x, w)
Notice:
Ep(∇wI) = P pi
∇wpi
pi
= P
i ∇wpi = ∇w(P
i pi) = 0
The covariance matrix of ∇wI(x, w) is (by deﬁnition):
Cov(I) = Ep[(∇wI −Ep(∇wI))t(∇wI −Ep(∇wI))] =
= Ep[(∇wI)t(∇wI)] = F(x, w)
We conclude our brief treatment of Information Geometry by some ob-
servations regarding the metric on the parameter space.
Observation A.8. We ﬁrst observe that:
KL(p(x, w + δw)||p(x, w))
∼= 1
2(δw)tF(x, w)(δw) + O(||δw||3)
This is a simple exercise based on Taylor expansion of the log function.
Let us interpret this result in the light of Deep Learning, more speciﬁcally,
during the dynamics of stochastic gradient descent. The Kullback-Leibler
divergence KL(p(x, w + δw)||p(x, w)) measures how p(x, w + δw) and p(x, w)
37

diﬀer, for a small variation of the parameters w, for example for a step
in stochastic gradient descent. If F is interpreted as a metric as in [3] 15,
then (δw)tF(x, w)(δw) expresses the size of the step δw. We notice however
that, because of Prop. A.6, this interpretation is not satisfactory. Even if
we restrict ourselves to the subspace of Rp where the Fisher matrix is non
degenerate, we cannot construct a submanifold, due to the non integrability
of the distribution of the gradients, that we observe experimentally (see [15]).
Moreover the dynamics does not even follow a sub-riemannian constraint
either, due to the non constant rank of the Fisher. This very challenging
behaviour will be the object of investigation of a forthcoming paper.
In the next proposition, we establish a connection between Information
Geometry and Hessian Geometry, since the metric, given by F, can be viewed
in terms of the Hessian of a potential function.
Proposition A.9. Let the notation be as above. Then
F(x, w) = Ep[H(I(x, w))]
where
I(x, w) = −log p(x, w)
Proof. In fact (write p = p(x, w)):
H[I] = −Jac
∇wp
p

= −[H(p) · p + ∇wp · ∇wp] 1
p2
Take the expected value:
Ep[H[I]] = −
X
i
pi
H(pi)
pi
+ Ep
∇wp
p
· ∇wp
p

= F
where P
i H(pi) = H(P
i pi) = 0.
B
Regression tasks
In this appendix we provide some explanations and references on another
very important learning task that can be handled using Deep Learning: re-
gression. In the main text we focused on classiﬁcation tasks, such as image
recognition. We may be interested in other practical problems as well, for
example we may want to predict the price of an house or of a car given some
15The low rank of F in Deep Learning, makes this interpretation problematic.
38

of its features such as dimension, mileage, year of construction/production,
etc. In general, given some numerical data we may want to predict a num-
ber, or a vector, rather than a (probability of belonging to) a class. These
tasks are called regression tasks to distinguish them from classiﬁcation tasks.
From an algorithmic point of view, regression tasks are handled and trained
as classiﬁcation tasks, the most important diﬀerence being that diﬀerent loss
functions in step 2 of Section 2.1 are usually employed. In particular, steps 0,
1, 3, 4, 5 described in Section 2.1 can be repeated almost verbatim with the
only conceptual diﬀerence that we are not trying to predict a class but rather
to infer a vector: therefore it is more appropriate to speak of a regression or
prediction function instead of a score function, etc.
For example, for step 1 if our regression task consists in predicting p dimen-
sional vectors from d dimensional features, we choose a suitable regression
or prediction function P(x, w) : Rd × Rp →Rc that we can think of a score
function where the codomain has not to be thought as a ”vector of probabil-
ities” but rather as the actual prediction or regression of our algorithm. For
step 2, suppose that we have a regression function P(x, w) : Rd × Rp →Rc
and that we denote, for any sample xi ∈Rd, i = 1, ..., N, with yi ∈Rc the
vector we would like to predict. The following are then some common loss
functions we can use for training:
• Mean Squared Error (MSE) or L2-norm:
L(w) := 1
N
N
X
i=1
||P(xi, w) −yi||2
2
• Root Mean Squared Error (RMSE):
L(w) :=
v
u
u
t 1
N
N
X
i=1
||P(xi, w) −yi||2
2
• Mean Absolute Error (MAE) or L1-norm:
L(w) := 1
N
N
X
i=1
||P(xi, w) −yi||1
The MSE is one of the most important loss functions when it comes to re-
gression tasks and in the past also classiﬁcation tasks were sometimes treated
as regression tasks and the MSE loss was used in these cases as well.
39

Remark B.1. Performing a regression task using the MSE loss function
to train a single layer MLP model is equivalent to solve an ordinary linear
regression (see [20]) problem using a gradient descent algorithm, as a single
layer MLP is simply an aﬃne map.
A priori, loss functions whose ﬁrst-order derivatives do not exist might
be problematic for stochastic descent algorithm (that is why the MSE is
sometimes preferred over the MAE, for example). To solve this issue in some
practical cases when a non diﬀerentiable loss function like the MAE would
be appropriate for the regression problem at hand but problematic from the
training viewpoint, the solution is often to use more regular functions that
are similar to the desired non-diﬀerentiable loss function whose behaviour is
desired. For example the ”Huber” or the ”log(cosh)” loss functions could
used in place of the MAE, see [36].
C
Multi-layer perceptrons and Convolutional
Neural Networks
In this appendix we give a self-contained explaination of multi-layer percep-
trons and convolutional neural networks, complementing the one we give
in the main body of the paper.
Given an aﬃne map F : Rn →Rm,
F(x) = Ax + b,
A ∈Mm,n(R), b ∈Rm, we will call b the bias of F. By
viewing a matrix A ∈Mm,n(R) as a vector of Rmn we can deﬁne an aﬃne
(or, abusing notation linear) layer as a couple (F, w) where F is an aﬃne
map and w is the vector of weights obtained concatenating A and the bias
vector. One dimensional convolutions are the discrete analogues of convolu-
tions in mathematical analysis. Let us consider a vector x ∈Rd. As already
did in the main text, we can deﬁne a simple one dimensional convolution as
follows:
conv1d : Rd −→Rd−r,
(conv1d(x))i =
r
X
j=1
Kjxi+j
where K is called a ﬁlter and is a vector of weights in Rr, r < d. This should
serve as the basic example to keep in mind and the analogy with continuous
convolutions is clear in this case.
To generalize to the case of 2-dimensional convolutions and to obtain more
general 1-dimensional convolutions, it is convenient to deﬁne one dimensional
convolutions to be the following slightly more general functions:
40

Deﬁnition C.1. Let be K ∈Rr, and consider index functions a(i, j), α(i, j) :
[l] × [d] →N, where we have denoted as [n] the subset {1, · · · n} ⊂N and
a(i, −) : [d] →N is assumed to be injective for all i ∈[l]. We deﬁne one
dimensional convolution operators as functions of the following form
conv1d : Rd −→Rl,
conv1d(x)i =
d
X
j=1
Kα(i,j)xa(i,j) + bi
where Rd ∋x = (xi)d
i=1, we set xk = 0, Kh = 0 if k /∈[d], h /∈[r] and
bi ∈R is a bias term.
We will call the vector w ∈Rr+l resulting from
the concatenation of K and all the biases the vector of weights associated
to conv1d. A one dimensional convolutional layer is a couple (conv1d, w)
where conv1d is a one dimensional convolution and w is the vector of weights
associated to it.
Remark C.2. Under the assumptions and the notations of the previous
deﬁnition if we set a(i, j) := i + j, α(i, j) = j, l = d −r and bi = 0 for all
i = 1, ..., l we get the simple convolution we deﬁned before. In addition, the
index functions a(i, −) and α(i, j) are usually chosen to be non-decreasing,
thus resembling the discrete analogue of the convolution usually employed in
analysis.
Besides the simple convolution we deﬁned at the beginning of this para-
graph, there are some standard choices of the functions a(i, j), α(i, j) that
are controlled by parameters known as stride, padding, etc. We will not in-
troduce them here as they become useful only when constructing particular
models: we refer to [29] for a discussion.
Many convolutions used in practice, in addition to having increasing index
functions, have l, r ≤d and are said to have one output channel, or are of
the form conv1d×e : Rd →Rl×e where conv1d is a one output channel, one
dimensional convolution. The number r in these cases is called the kernel
size. We can deﬁne also the so called ”pooling functions”.
Deﬁnition C.3. Consider an index function a(i, j) : [l] × [d] →N, as in
the previous deﬁnition. For any i ≤l and any vector x ∈Rd, we denote as
xa(i,−) ∈RR the vector (xa(i,j))d
j=1. We say that a function P : Rd →Rl is a
pooling layer or operator if P(x)i = ϕ(xa(i,−)) for a ﬁxed pooling function ϕ.
If ϕ is the arithmetic mean or the maximum function we will call the layer
P mean pooling or max pooling layer respectively. A pooling layer does not
have an associated set of weights.
41

Remark C.4. Pooling layers are not meant to be trained during the training
of a model, therefore they do not have an associated vector of weights.
Two dimensional convolutions are conceptually the analogue of one di-
mensional convolutions in the context of matrices. Indeed, some data like
images can be more naturally thought as grids (matrices) of numbers rather
than vectors. Even if two dimensional convolutions are a particular case of
one dimensional convolutions, because of their importance and their widespread
use it is important to discuss them on their own.
Deﬁnition C.5. Denote as Md,q(R) the space of d × q real valued matri-
ces.
Let be Khk ∈Md,q(R), and let be a(i, j), α(i, j) : [r] × [n] →N,
b(i, j), β(i, j) : [s] × [m] →N index functions where a(i, −), b(j, −) are as-
sumed to be injective for all i, j. We deﬁne two dimensional convolution
operators as functions of the following form
conv2d : Mn,m(R) −→Mr,s(R),
(conv2d(A))ij =
n
X
h=1
m
X
k=1
Kα(i,h)β(j,k)Aa(i,h)b(j,k)+bij
where Aij ∈Mn,m(R), as in the case of one dimensional convolutions we set
Aij = 0, Khk = 0 if either i > n, j > m, h > d or k > q and bij ∈R is a bias
term.
Remark C.6. Using the canonical isomorphism Mn,m(R) ∼= Rn×m (notice
that it is Z-linear) we can see that there is a bijection between the set of
two dimensional convolutions and the one of one dimensional convolutions.
As a consequence, we deﬁne a 2-dimensional convolutional layer as a couple
(C, w) where C is a 2-dimensional convolution and w is its associated vector of
weights, obtained as the vector of weights of the one dimensional convolution
associated to C. Moreover, as in the case of one dimensional convolutions,
the index functions a(i, −), b(j, −), α(i, −), β(j, −) are usually assumed to be
non-decreasing.
As in the case of one dimensional convolutions, the form of the index
functions are usually standard and controlled by parameters widely used by
the practitioners such as stride, padding, etc., and the numbers d, q are called
the kernel sizes. In addition, in most cases p = q (i.e. the ﬁlter or kernel
matrix is a square matrix).
42

Figure 12: A 2d convolution as depicted in [38]
As we mentioned at the beginning of Section 3.1 we are not only interested
on convolutions acting on Mn,m(R) ∼= Rn × Rm but also on convolutions
acting on Rp × Rn × Rm representing an image having p channels or a
p × n × m voxel. We can then deﬁne 3-dimensional convolutions along the
lines of what we did for 2-dimensional ones and, more generally we can
deﬁne n-dimensional convolutions and convolutional layers.
As these are
straightforward generalizations and as all these cases reduce to the case of
one dimensional convolutions, we will not spell out the deﬁnitions. We deﬁne
n-dimensional pooling layers analogously.
We shall now deﬁne two very important types of neural networks.
Deﬁnition C.7. We say that F ((F, w)) is a convolution (convolution layer)
if it is a convolution operator (layer). We say that σ : R →R is an activation
function if it is either a bounded, non-constant continuous function or if it
is the RELU function. Given an activation function σ, for all n ≥1 we can
view it as a map Rn →Rn acting componentwise (and in this case we denote
σ×n as σ abusing terminology).
Deﬁnition C.8. Consider a ﬁnite sequence of positive natural numbers
N0, ..., NL16, L ∈N>0 and ﬁx an activation function σ.
We say that a
function F : RN0 →RNL is called
• A σ-multilayer perceptron (MLP) if it is of the form
F = G ◦HL−1 ◦· · · ◦H1
where G : RNL−1 →RNL is an aﬃne map and for all i = 1, ..., L −1,
Hi = σ ◦Ai where Ai : RNi−1 →RNi is an aﬃne map.
16That, under the terminology of Section 2.1, are among the hyperparameters
43

• A σ-convolutional neural network (CNN) if it is of the form
F = G ◦HL−1 ◦· · · ◦H1
where G : RNL−1 →RNL is an aﬃne map and, for all i = 1, ..., L −1,
Hi is either a pooling function or it is of the form Hi = σ ◦Ai where
Ai : RNi−1 →RNi is aﬃne or a convolution.
The number L is usually called the number of layers and σ is assumed to be
applied componentwise. The convolutions and the aﬃne maps appearing in
a MLP or in a CNN are considered as layers. We denote as N σ(Rn, Rm) the
set of σ-MLPs having domain Rn and codomain Rm. Finally, given a MLP
or a CNN F, we deﬁne the vector of its weights, wF, to be the concatenation
of all the vectors of weights of its layers. When we see a MLP or a CNN
F as a network, we usually think of it as a couple (F(w), w), and in the
terminology of Section 3, somewhat abusing the terminology, that w is the
model.
There exists many more types of CNNs, therefore our deﬁnition here is
not meant to be the most general possible. However, all of them use at least
one convolution operator as the ones the we have deﬁned before. MLPs are
usually referred as ”fully-connected” feedforward neural networks. Indeed,
both MLPs and CNNs fall under the broader category of feeedforward neu-
ral networks which are, roughly speaking, networks that have an underlying
structure of directed graph. As there is not a general straightforward and
concise deﬁnition of these networks, in this work we will only consider MLPs
and CNNs having the structure above and refer the reader to the literature,
see [29] for more general deﬁnitions or Section2 in [Zhu22] for a recent and
very clean deﬁnition from a graph theoretical viewpoint.
Suppose we are given a network (F(w), w) which can be either a MLP or
a CNN. If w ∈Rp and F(w) : Rn →RC we can deﬁne the score function
associated to (F(w), w) as
s : Rd × Rp →RC,
s(x, w) := F(w)(x)
This is the score we use for training as explained in the main text.
44

Figure 13: A very inﬂuential Convolutional Neural Network: the famous
LeNet introduced in [31] (the image depicts LeNet-5 as in [34]).
D
Universal Approximation Theorem
We conclude this appendix mentioning a so called universal approximation
theorem for MLPs, an important theoretical result showing that the archi-
tectures we introduced have a great explanatory power. For more details,
see [22] Theorems 1 and 2.
Theorem D.1. For any given activation function σ, the set of σ-MLPs
N σ(Rn, R) is dense in the set of continuous real valued functions C(Rn) for
the topology of uniform convergence on compact sets.
Proof. This is proved as Theorem 2 in [22]. We only remark that the theorem
holds in the case σ is the RELU, see the remark at page 253 in [22], as for a
given R ∋a ̸= 0 the ”spike function”
r(x) = RELU(x −a) −2RELU(x) + RELU(x + a)
is bounded, continuous and non-constant
This theorem is conceptually very important as it states that an arbitrary
continuous function can be in principle approximated by a suitable MLP. As
a consequence, trying to approximate unknown functions with Neural Net-
works may seem less hopeless than what one might think at a ﬁrst glance.
In real world applications, though it is somewhat unpractical to solve any
task with an MLP (for many reasons such as lack of data), and for this rea-
son other architectures such as CNNs, GNNs etc. have been designed and
deployed.
There exist more general approximation theorems concerning more general
MLPs (e.g. for the class N σ(Rn, Rm)), or for other neural networks archi-
tectures such as CNNs and GNNs. For these results and for a more compre-
hensive discussion of the topic the reader is referred to [23], [29] and [26].
45

References
[1] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and
Masanori Koyama. Optuna: A next-generation hyperparameter opti-
mization framework. Proceedings of the 25th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining, 2019.
[2] Babak Alipanahi, Andrew Delong, Matthew T. Weirauch, and Bren-
dan J. Frey. Predicting the sequence speciﬁcities of dna- and rna-binding
proteins by deep learning. Nature Biotechnology, 33:831–838, 2015.
[3] Shun-Ichi Amari. Natural gradient works eﬃciently in learning. Neural
computation, 10(2):251–276, 1998.
[4] Frank Bauer. Normalized graph laplacians for directed graphs. arXiv:
Combinatorics, 2011.
[5] Cristian Bodnar, Francesco Di Giovanni, Benjamin Paul Chamber-
lain, and Michael M. Bronstein Pietro Li`o.
Neural sheaf diﬀusion:
A topological perspective on heterophily and oversmoothing in gnns.
arXiv:2202.04579 [cs.LG], 2022.
[6] Soha Boroojerdi and George Rudolph. Handwritten multi-digit recog-
nition with machine learning. pages 1–6, 05 2022.
[7] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velivckovi’c.
Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.
ArXiv, abs/2104.13478, 2021.
[8] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur D. Szlam,
and Pierre Vandergheynst.
Geometric deep learning: Going beyond
euclidean data. IEEE Signal Processing Magazine, 34:18–42, 2016.
[9] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent per-
forms variational inference, converges to limit cycles for deep networks.
2018 Information Theory and Applications Workshop (ITA), pages 1–
10, 2017.
[10] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast
and accurate deep network learning by exponential linear units (elus).
arXiv: Learning, 2015.
46

[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
Imagenet: A large-scale hierarchical image database.
In 2009 IEEE
conference on computer vision and pattern recognition, pages 248–255.
Ieee, 2009.
[12] Matthew Feickert and Benjamin Philip Nachman. A living review of
machine learning for particle physics. ArXiv, abs/2102.02770, 2021.
[13] Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of train-
ing deep feedforward neural networks. In International Conference on
Artiﬁcial Intelligence and Statistics, 2010.
[14] Chris D. Godsil and Gordon F. Royle.
Algebraic graph theory.
In
Graduate texts in mathematics, 2001.
[15] Luca Grementieri and Rita Fioresi. Model-centric data manifold: the
data through the eyes of the model. ArXiv, abs/2104.13289, 2021.
[16] Daniel Guest, Kyle Cranmer, and Daniel Whiteson. Deep learning and
its application to lhc physics. Annual Review of Nuclear and Particle
Science, 2018.
[17] William L. Hamilton. Graph representation learning. Synthesis Lectures
on Artiﬁcial Intelligence and Machine Learning, 2020.
[18] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Gre-
gory Frederick Diamos, Erich Elsen, Ryan J. Prenger, Sanjeev Satheesh,
Shubho Sengupta, Adam Coates, and A. Ng. Deep speech: Scaling up
end-to-end speech recognition. ArXiv, abs/1412.5567, 2014.
[19] Jakob Hansen and Thomas Gebhart. Sheaf neural networks. ArXiv,
abs/2012.06333, 2020.
[20] Trevor J. Hastie, Robert Tibshirani, and Jerome H. Friedman.
The
elements of statistical learning: Data mining, inference, and prediction,
2nd edition. In Springer Series in Statistics, 2005.
[21] Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman
Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four research groups.
IEEE Signal processing magazine, 29(6):82–97, 2012.
47

[22] Kurt Hornik. Approximation capabilities of multilayer feedforward net-
works. Neural Networks, 4:251–257, 1991.
[23] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert L. White. Mul-
tilayer feedforward networks are universal approximators. Neural Net-
works, 2:359–366, 1989.
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. CoRR, abs/1412.6980, 2014.
[25] Thomas Kipf and Max Welling.
Semi-supervised classiﬁcation with
graph convolutional networks. ArXiv, abs/1609.02907, 2016.
[26] Anastasis Kratsios and Ievgen Bilokopytov.
Non-euclidean universal
approximation. ArXiv, abs/2006.02341, 2020.
[27] Alex
Krizhevsky,
Vinod
Nair,
and
Geoﬀrey
Hinton.
Cifar-
10
(canadian
institute
for
advanced
research).
URL
http://www.cs.toronto.edu/kriz/cifar.html, 5, 2010.
[28] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In Advances in neural
information processing systems, pages 1097–1105, 2012.
[29] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Na-
ture, 521:436–444, 2015.
[30] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson,
Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Back-
propagation applied to handwritten zip code recognition. Neural Com-
putation, 1:541–551, 1989.
[31] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner.
Gradient-based learning applied to document recognition. Proc. IEEE,
86:2278–2324, 1998.
[32] Yann
LeCun,
Corinna
Cortes,
and
CJ
Burges.
Mnist
hand-
written
digit
database.
ATT
Labs
[Online].
Available:
http://yann.lecun.com/exdb/mnist, 2, 2010.
48

[33] Geert J. S. Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud
Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen
van der Laak, Bram van Ginneken, and Clara I. S´anchez.
A survey
on deep learning in medical image analysis.
Medical image analysis,
42:60–88, 2017.
[34] Jose Marques, Gabriel Falc˜ao Paiva Fernandes, and Lu´ıs A. Alexandre.
Distributed learning of cnns on heterogeneous cpu/gpu architectures.
Applied Artiﬁcial Intelligence, 32:822 – 844, 2017.
[35] James Martens. New insights and perspectives on the natural gradient
method. Journal of Machine Learning Research, 21(146):1–76, 2020.
[36] Resve A. Saleh and A. K. Md. Ehsanes Saleh. Statistical properties of the
log-cosh loss function used in machine learning. ArXiv, abs/2208.04564,
2022.
[37] Stefan Sommer and Alex M Bronstein. Horizontal ﬂows and manifold
stochastics in geometric deep learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2020.
[38] Xing Ping Sun, Jiayuan Peng, Yong Shen, and Hongwei Kang. Tobacco
plant detection in rgb aerial images. Agriculture, 2020.
[39] Petar Velickovic,
Guillem Cucurull,
Arantxa Casanova,
Adriana
Romero, Pietro Lio’, and Yoshua Bengio. Graph attention networks.
ArXiv, abs/1710.10903, 2017.
[40] Izhar Wallach, Michael Dzamba, and Abraham Heifets. Atomnet: A
deep convolutional neural network for bioactivity prediction in structure-
based drug discovery. ArXiv, abs/1510.02855, 2015.
[41] Yonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le, Mohammad
Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, JeﬀKlingner, Apurva Shah, Melvin Johnson, Xiaobing
Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliﬀ
Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gre-
gory S. Corrado, MacduﬀHughes, and Jeﬀrey Dean. Google’s neural
machine translation system: Bridging the gap between human and ma-
chine translation. ArXiv, abs/1609.08144, 2016.
49

