
PROBABILITY, MARKOV CHAINS,
QUEUES, AND SIMULATION

This page intentionally left blank 


Copyright c⃝2009 by Princeton University Press
Published by Princeton University Press, 41 William street, Princeton, New Jersey 08540
In the United Kingdom: Princeton University Press, 6 Oxford Street,
Woodstock, Oxfordshire OX20 1TW
All Rights Reserved
Library of Congress Cataloging-in-Publication Data
Stewart, William J., 1946–
Probability, Markov chains, queues and simulation: the mathematical basis of
performance modeling/William J. Stewart. – 1st ed.
p. cm.
ISBN 978-0-691-14062-9 (cloth : alk. paper) 1. Probability–Computer simulation.
2. Markov processes. 3. Queueing theory. I. Title.
QA273.S7532 2009
519.201′13–dc22
2008041122
British Library Cataloging-in-Publication Data is available
This book has been composed in Times
Printed on acid-free paper. ∞
press.princeton.edu
MATLAB is a registered trademark of The MathWorks, Inc.
Typeset by S R Nova Pvt Ltd, Bangalore, India
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

This book is dedicated to all those
whom I love, especially
My dear wife, Kathie,
and my wonderful children
Nicola, Stephanie, Kathryn, and William
My father, William J. Stewart and
the memory of my mother, Mary (Marshall) Stewart

This page intentionally left blank 

Contents
Preface and Acknowledgments
xv
I
PROBABILITY
1
1
Probability
3
1.1
Trials, Sample Spaces, and Events
3
1.2
Probability Axioms and Probability Space
9
1.3
Conditional Probability
12
1.4
Independent Events
15
1.5
Law of Total Probability
18
1.6
Bayes’ Rule
20
1.7
Exercises
21
2
Combinatorics—The Art of Counting
25
2.1
Permutations
25
2.2
Permutations with Replacements
26
2.3
Permutations without Replacement
27
2.4
Combinations without Replacement
29
2.5
Combinations with Replacements
31
2.6
Bernoulli (Independent) Trials
33
2.7
Exercises
36
3
Random Variables and Distribution Functions
40
3.1
Discrete and Continuous Random Variables
40
3.2
The Probability Mass Function for a Discrete Random Variable
43
3.3
The Cumulative Distribution Function
46
3.4
The Probability Density Function for a Continuous Random Variable
51
3.5
Functions of a Random Variable
53
3.6
Conditioned Random Variables
58
3.7
Exercises
60
4
Joint and Conditional Distributions
64
4.1
Joint Distributions
64
4.2
Joint Cumulative Distribution Functions
64
4.3
Joint Probability Mass Functions
68
4.4
Joint Probability Density Functions
71
4.5
Conditional Distributions
77
4.6
Convolutions and the Sum of Two Random Variables
80
4.7
Exercises
82
5
Expectations and More
87
5.1
Deﬁnitions
87
5.2
Expectation of Functions and Joint Random Variables
92
5.3
Probability Generating Functions for Discrete Random Variables
100

viii
Contents
5.4
Moment Generating Functions
103
5.5
Maxima and Minima of Independent Random Variables
108
5.6
Exercises
110
6
Discrete Distribution Functions
115
6.1
The Discrete Uniform Distribution
115
6.2
The Bernoulli Distribution
116
6.3
The Binomial Distribution
117
6.4
Geometric and Negative Binomial Distributions
120
6.5
The Poisson Distribution
124
6.6
The Hypergeometric Distribution
127
6.7
The Multinomial Distribution
128
6.8
Exercises
130
7
Continuous Distribution Functions
134
7.1
The Uniform Distribution
134
7.2
The Exponential Distribution
136
7.3
The Normal or Gaussian Distribution
141
7.4
The Gamma Distribution
145
7.5
Reliability Modeling and the Weibull Distribution
149
7.6
Phase-Type Distributions
155
7.6.1
The Erlang-2 Distribution
155
7.6.2
The Erlang-r Distribution
158
7.6.3
The Hypoexponential Distribution
162
7.6.4
The Hyperexponential Distribution
164
7.6.5
The Coxian Distribution
166
7.6.6
General Phase-Type Distributions
168
7.6.7
Fitting Phase-Type Distributions to Means and Variances
171
7.7
Exercises
176
8
Bounds and Limit Theorems
180
8.1
The Markov Inequality
180
8.2
The Chebychev Inequality
181
8.3
The Chernoff Bound
182
8.4
The Laws of Large Numbers
182
8.5
The Central Limit Theorem
184
8.6
Exercises
187
II
MARKOV CHAINS
191
9
Discrete- and Continuous-Time Markov Chains
193
9.1
Stochastic Processes and Markov Chains
193
9.2
Discrete-Time Markov Chains: Deﬁnitions
195
9.3
The Chapman-Kolmogorov Equations
202
9.4
Classiﬁcation of States
206
9.5
Irreducibility
214
9.6
The Potential, Fundamental, and Reachability Matrices
218
9.6.1
Potential and Fundamental Matrices and Mean Time to Absorption
219
9.6.2
The Reachability Matrix and Absorption Probabilities
223

Contents
ix
9.7
Random Walk Problems
228
9.8
Probability Distributions
235
9.9
Reversibility
248
9.10
Continuous-Time Markov Chains
253
9.10.1 Transition Probabilities and Transition Rates
254
9.10.2 The Chapman-Kolmogorov Equations
257
9.10.3 The Embedded Markov Chain and State Properties
259
9.10.4 Probability Distributions
262
9.10.5 Reversibility
265
9.11
Semi-Markov Processes
265
9.12
Renewal Processes
267
9.13
Exercises
275
10 Numerical Solution of Markov Chains
285
10.1
Introduction
285
10.1.1 Setting the Stage
285
10.1.2 Stochastic Matrices
287
10.1.3 The Effect of Discretization
289
10.2
Direct Methods for Stationary Distributions
290
10.2.1 Iterative versus Direct Solution Methods
290
10.2.2 Gaussian Elimination and LU Factorizations
291
10.3
Basic Iterative Methods for Stationary Distributions
301
10.3.1 The Power Method
301
10.3.2 The Iterative Methods of Jacobi and Gauss–Seidel
305
10.3.3 The Method of Successive Overrelaxation
311
10.3.4 Data Structures for Large Sparse Matrices
313
10.3.5 Initial Approximations, Normalization, and Convergence
316
10.4
Block Iterative Methods
319
10.5
Decomposition and Aggregation Methods
324
10.6
The Matrix Geometric/Analytic Methods for Structured Markov Chains
332
10.6.1 The Quasi-Birth-Death Case
333
10.6.2 Block Lower Hessenberg Markov Chains
340
10.6.3 Block Upper Hessenberg Markov Chains
345
10.7
Transient Distributions
354
10.7.1 Matrix Scaling and Powering Methods for Small State Spaces
357
10.7.2 The Uniformization Method for Large State Spaces
361
10.7.3 Ordinary Differential Equation Solvers
365
10.8
Exercises
375
III
QUEUEING MODELS
383
11 Elementary Queueing Theory
385
11.1
Introduction and Basic Deﬁnitions
385
11.1.1 Arrivals and Service
386
11.1.2 Scheduling Disciplines
395
11.1.3 Kendall’s Notation
396
11.1.4 Graphical Representations of Queues
397
11.1.5 Performance Measures—Measures of Effectiveness
398
11.1.6 Little’s Law
400

x
Contents
11.2
Birth-Death Processes: The M/M/1 Queue
402
11.2.1 Description and Steady-State Solution
402
11.2.2 Performance Measures
406
11.2.3 Transient Behavior
412
11.3
General Birth-Death Processes
413
11.3.1 Derivation of the State Equations
413
11.3.2 Steady-State Solution
415
11.4
Multiserver Systems
419
11.4.1 The M/M/c Queue
419
11.4.2 The M/M/∞Queue
425
11.5
Finite-Capacity Systems—The M/M/1/K Queue
425
11.6
Multiserver, Finite-Capacity Systems—The M/M/c/K Queue
432
11.7
Finite-Source Systems—The M/M/c//M Queue
434
11.8
State-Dependent Service
437
11.9
Exercises
438
12 Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
444
12.1
The Erlang-r Service Model—The M/Er/1 Queue
444
12.2
The Erlang-r Arrival Model—The Er/M/1 Queue
450
12.3
The M/H2/1 and H2/M/1 Queues
454
12.4
Automating the Analysis of Single-Server Phase-Type Queues
458
12.5
The H2/E3/1 Queue and General Ph/Ph/1 Queues
460
12.6
Stability Results for Ph/Ph/1 Queues
466
12.7
Performance Measures for Ph/Ph/1 Queues
468
12.8
Matlab code for Ph/Ph/1 Queues
469
12.9
Exercises
471
13 The z-Transform Approach to Solving Markovian Queues
475
13.1
The z-Transform
475
13.2
The Inversion Process
478
13.3
Solving Markovian Queues using z-Transforms
484
13.3.1 The z-Transform Procedure
484
13.3.2 The M/M/1 Queue Solved using z-Transforms
484
13.3.3 The M/M/1 Queue with Arrivals in Pairs
486
13.3.4 The M/Er/1 Queue Solved using z-Transforms
488
13.3.5 The Er/M/1 Queue Solved using z-Transforms
496
13.3.6 Bulk Queueing Systems
503
13.4
Exercises
506
14 The M/G/1 and G/M/1 Queues
509
14.1
Introduction to the M/G/1 Queue
509
14.2
Solution via an Embedded Markov Chain
510
14.3
Performance Measures for the M/G/1 Queue
515
14.3.1 The Pollaczek-Khintchine Mean Value Formula
515
14.3.2 The Pollaczek-Khintchine Transform Equations
518
14.4
The M/G/1 Residual Time: Remaining Service Time
523
14.5
The M/G/1 Busy Period
526
14.6
Priority Scheduling
531
14.6.1 M/M/1: Priority Queue with Two Customer Classes
531
14.6.2 M/G/1: Nonpreemptive Priority Scheduling
533

Contents
xi
14.6.3 M/G/1: Preempt-Resume Priority Scheduling
536
14.6.4 A Conservation Law and SPTF Scheduling
538
14.7
The M/G/1/K Queue
542
14.8
The G/M/1 Queue
546
14.9
The G/M/1/K Queue
551
14.10 Exercises
553
15 Queueing Networks
559
15.1
Introduction
559
15.1.1 Basic Deﬁnitions
559
15.1.2 The Departure Process—Burke’s Theorem
560
15.1.3 Two M/M/1 Queues in Tandem
562
15.2
Open Queueing Networks
563
15.2.1 Feedforward Networks
563
15.2.2 Jackson Networks
563
15.2.3 Performance Measures for Jackson Networks
567
15.3
Closed Queueing Networks
568
15.3.1 Deﬁnitions
568
15.3.2 Computation of the Normalization Constant: Buzen’s Algorithm
570
15.3.3 Performance Measures
577
15.4
Mean Value Analysis for Closed Queueing Networks
582
15.5
The Flow-Equivalent Server Method
591
15.6
Multiclass Queueing Networks and the BCMP Theorem
594
15.6.1 Product-Form Queueing Networks
595
15.6.2 The BCMP Theorem for Open, Closed, and Mixed Queueing
Networks
598
15.7
Java Code
602
15.8
Exercises
607
IV
SIMULATION
611
16 Some Probabilistic and Deterministic Applications of Random Numbers
613
16.1
Simulating Basic Probability Scenarios
613
16.2
Simulating Conditional Probabilities, Means, and Variances
618
16.3
The Computation of Deﬁnite Integrals
620
16.4
Exercises
623
17 Uniformly Distributed “Random” Numbers
625
17.1
Linear Recurrence Methods
626
17.2
Validating Sequences of Random Numbers
630
17.2.1 The Chi-Square “Goodness-of-Fit” Test
630
17.2.2 The Kolmogorov-Smirnov Test
633
17.2.3 “Run” Tests
634
17.2.4 The “Gap” Test
640
17.2.5 The “Poker” Test
641
17.2.6 Statistical Test Suites
644
17.3
Exercises
644

xii
Contents
18 Nonuniformly Distributed “Random” Numbers
647
18.1
The Inverse Transformation Method
647
18.1.1 The Continuous Uniform Distribution
649
18.1.2 “Wedge-Shaped” Density Functions
649
18.1.3 “Triangular” Density Functions
650
18.1.4 The Exponential Distribution
652
18.1.5 The Bernoulli Distribution
653
18.1.6 An Arbitrary Discrete Distribution
653
18.2
Discrete Random Variates by Mimicry
654
18.2.1 The Binomial Distribution
654
18.2.2 The Geometric Distribution
655
18.2.3 The Poisson Distribution
656
18.3
The Accept-Reject Method
657
18.3.1 The Lognormal Distribution
660
18.4
The Composition Method
662
18.4.1 The Erlang-r Distribution
662
18.4.2 The Hyperexponential Distribution
663
18.4.3 Partitioning of the Density Function
664
18.5
Normally Distributed Random Numbers
670
18.5.1 Normal Variates via the Central Limit Theorem
670
18.5.2 Normal Variates via Accept-Reject and Exponential
Bounding Function
670
18.5.3 Normal Variates via Polar Coordinates
672
18.5.4 Normal Variates via Partitioning of the Density Function
673
18.6
The Ziggurat Method
673
18.7
Exercises
676
19 Implementing Discrete-Event Simulations
680
19.1
The Structure of a Simulation Model
680
19.2
Some Common Simulation Examples
682
19.2.1 Simulating the M/M/1 Queue and Some Extensions
682
19.2.2 Simulating Closed Networks of Queues
686
19.2.3 The Machine Repairman Problem
689
19.2.4 Simulating an Inventory Problem
692
19.3
Programming Projects
695
20 Simulation Measurements and Accuracy
697
20.1
Sampling
697
20.1.1 Point Estimators
698
20.1.2 Interval Estimators/Conﬁdence Intervals
704
20.2
Simulation and the Independence Criteria
707
20.3
Variance Reduction Methods
711
20.3.1 Antithetic Variables
711
20.3.2 Control Variables
713
20.4
Exercises
716
Appendix A: The Greek Alphabet
719
Appendix B: Elements of Linear Algebra
721
B.1
Vectors and Matrices
721
B.2
Arithmetic on Matrices
721

Contents
xiii
B.3
Vector and Matrix Norms
723
B.4
Vector Spaces
724
B.5
Determinants
726
B.6
Systems of Linear Equations
728
B.6.1
Gaussian Elimination and LU Decompositions
730
B.7
Eigenvalues and Eigenvectors
734
B.8
Eigenproperties of Decomposable, Nearly Decomposable, and Cyclic
Stochastic Matrices
738
B.8.1
Normal Form
738
B.8.2
Eigenvalues of Decomposable Stochastic Matrices
739
B.8.3
Eigenvectors of Decomposable Stochastic Matrices
741
B.8.4
Nearly Decomposable Stochastic Matrices
743
B.8.5
Cyclic Stochastic Matrices
744
Bibliography
745
Index
749

This page intentionally left blank 

Preface and Acknowledgments
This book has been written to provide a complete, yet elementary and pedagogic, treatment of the
mathematical basis of systems performance modeling. Performance modeling is of fundamental
importance to many branches of the mathematical sciences and engineering as well as to the
social and economic sciences. Advances in methodology and technology have now provided the
wherewithal to build and solve sophisticated models. The purpose of this book is to provide the
student and teacher with a modern approach for building and solving probability based models with
conﬁdence.
The book is divided into four major parts, namely, “Probability,” “Markov Chains,” “Queueing
Models,” and “Simulation.” The eight chapters of Part I provide the student with a comprehensive
and thorough knowledge of probability theory. Part I is self-contained and complete and should
be accessible to anyone with a basic knowledge of calculus. Newcomers to probability theory as
well as those whose knowledge of probability is rusty should be equally at ease in their progress
through Part I. The ﬁrst chapter provides the fundamental concepts of set-based probability and the
probability axioms. Conditional probability and independence are stressed as are the laws of total
probability and Bayes’ rule. Chapter 2 introduces combinatorics—the art of counting—which is so
important for the correct evaluation of probabilities. Chapter 3 introduces the concepts of random
variables and distribution functions including functions of a random variable and conditioned
random variables. This chapter prepares the ground work for Chapters 4 and 5: Chapter 4 introduces
joint and conditional distributions and Chapter 5 treats expectations and higher moments. Discrete
distribution functions are the subject of Chapter 6 while their continuous counterparts, continuous
distribution functions, are the subject of Chapter 7. Particular attention is paid to phase-type
distributions due to the important role they play in modeling scenarios and the chapter also includes
a section on ﬁtting phase-type distributions to given means and variances. The ﬁnal chapter in Part
I is devoted to bounds and limit theorems, including the laws of large numbers and the central limit
theorem.
Part II contains two rather long chapters on the subject of Markov chains, the ﬁrst on
theoretical aspects of Markov chains, and the second on their numerical solution. In Chapter 9,
the basic concepts of discrete and continuous-time Markov chains and their underlying equations
and properties are discussed. Special attention is paid to irreducible Markov chains and to the
potential, fundamental, and reachability matrices in reducible Markov chains. This chapter also
contains sections on random walk problems and their applications, the property of reversibility in
Markov chains, and renewal processes. Chapter 10 deals with numerical solutions, from Gaussian
elimination and basic iterative-type methods for stationary solutions to ordinary differential equation
solvers for transient solutions. Block methods and iterative aggregation-disaggregation methods for
nearly completely decomposable Markov chains are considered. A section is devoted to matrix
geometric and matrix analytic methods for structured Markov chains. Algorithms and computational
considerations are stressed throughout this chapter.
Queueing models are presented in the ﬁve chapters that constitute Part III. Elementary queueing
theory is presented in Chapter 11. Here an introduction to the basic terminology and deﬁnitions
is followed by an analysis of the simplest of all queueing models, the M/M/1 queue. This is then
generalized to birth-death processes, which are queueing systems in which the underlying Markov
chain matrix is tridiagonal. Chapter 12 deals with queues in which the arrival process need no longer

xvi
Preface and Acknowledgments
be Poisson and the service time need not be exponentially distributed. Instead, interarrival times and
service times can be represented by phase-type distributions and the underlying Markov chain is
now block tridiagonal. The following chapter, Chapter 13, explores the z-transform approach for
solving similar types of queues. The M/G/1 and G/M/1 queues are the subject of Chapter 14. The
approach used is that of the embedded Markov chain. The Pollaczek-Khintchine mean value and
transform equations are derived and a detailed discussion of residual time and busy period follows. A
thorough discussion of nonpreemptive and preempt-resume scheduling policies as well as shortest-
processing-time-ﬁrst scheduling is presented. An analysis is also provided for the case in which only
a limited number of customers can be accommodated in both the M/G/1 and G/M/1 queues. The ﬁnal
chapter of Part III, Chapter 15, treats queueing networks. Open networks are introduced via Burke’s
theorem and Jackson’s extensions to this theorem. Closed queueing networks are treated using both
the convolution algorithm and the mean value approach. The “ﬂow-equivalent server” approach is
also treated and its potential as an approximate solution procedure for more complex networks is
explored. The chapter terminates with a discussion of product form in queueing networks and the
BCMP theorem for open, closed, and mixed networks.
The ﬁnal part of the text, Part IV, deals with simulation. Chapter 16 explores how uniformly
distributed random numbers can be applied to obtain solutions to probabilistic models and other
time-independent problems—the “Monte Carlo” aspect of simulation. Chapter 17 describes the
modern approaches for generating uniformly distributed random numbers and how to test them
to ensure that they are indeed uniformly distributed and independent of each other. The topic of
generating random numbers that are not uniformly distributed, but satisfy some other distribution
such as Erlang or normal, is dealt with in Chapter 18. A large number of possibilities exist and
not all are appropriate for every distribution. The next chapter, Chapter 19, provides guidelines for
writing simulation programs and a number of examples are described in detail. Chapter 20 is the
ﬁnal chapter in the book. It concerns simulation measurement and accuracy and is based on sampling
theory. Special attention is paid to the generation of conﬁdence intervals and to variance reduction
techniques, an important means of keeping the computational costs of simulation to a manageable
level.
The text also includes two appendixes; the ﬁrst is just a simple list of the letters of the Greek
alphabet and their spellings; the second is a succinct, yet complete, overview of the linear algebra
used throughout the book.
Genesis and Intent
This book saw its origins in two ﬁrst-year graduate level courses that I teach, and have taught for
quite some time now, at North Carolina State University. The ﬁrst is entitled “An Introduction to
Performance Evaluation;” it is offered by the Computer Science Department and the Department
of Electrical and Computer Engineering. This course is required for our networking degrees.
The second is a course entitled “Queues and Stochastic Service Systems” and is offered by the
Operations Research Program and the Industrial and Systems Engineering Department. It follows
then that this book has been designed for students from a variety of academic disciplines in which
stochastic processes constitute a fundamental concept, disciplines that include not only computer
science and engineering, industrial engineering, and operations research, but also mathematics,
statistics, economics, and business, the social sciences—in fact all disciplines in which stochastic
performance modeling plays a primary role. A calculus-based probability course is a prerequisite
for both these courses so it is expected that students taking these classes are already familiar with
probability theory. However, many of the students who sign up for these courses are returning
students, and it is often the case that it has been several years and in some cases a decade or more,
since they last studied probability. A quick review of probability is hardly sufﬁcient to bring them

Preface and Acknowledgments
xvii
up to the required level. Part I of the book has been designed with them in mind. It provides the
prerequisite probability background needed to fully understand and appreciate the material in the
remainder of the text. The presentation, with its numerous examples and exercises, is such that
it facilitates an independent review so the returning student in a relatively short period of time,
preferably prior to the beginning of class, will once again have mastered probability theory. Part I
can then be used as a reference source as and when needed.
The entire text has been written at a level that is suitable for upper-level undergraduate students or
ﬁrst-year graduate students and is completely self-contained. The entirety of the text can be covered
in a two-semester sequence, such as the stochastic processes sequence offered by the Industrial
Engineering (IE) Department and the Operations Research (OR) Program at North Carolina State
University. A two-semester sequence is appropriate for classes in which students have limited (or
no) exposure to probability theory. In such cases it is recommended that the ﬁrst semester be devoted
to the Chapters 1–8 on probability theory, the ﬁrst ﬁve sections of Chapter 9, which introduce the
fundamental concepts of discrete-time Markov chains, and the ﬁrst three sections of Chapter 11,
which concern elementary queueing theory. With this background clearly understood, the student
should have no difﬁculty in covering the remaining topics of the text in the second semester.
The complete content of Parts II–IV might prove to be a little too much for some one-semester
classes. In this case, an instructor might wish to omit the later sections of Chapter 10 on the
numerical solution of Markov chains, perhaps covering only the basic direct and iterative methods.
In this case the material of Chapter 12 should also be omitted since it depends on a knowledge of
the matrix geometric method of Chapter 10. Because of the importance of computing numerical
solutions, it would be a mistake to omit Chapter 10 in its entirety. Some of the material in Chapter
18 could also be eliminated: for example, an instructor might include only the ﬁrst three sections of
this chapter. In my own case, when teaching the OR/IE course, I concentrate on covering all of the
Markov chain and queueing theory chapters. These students often take simulation as an individual
course later on. When teaching the computer science and engineering course, I omit some of the
material on the numerical solution of Markov chains so as to leave enough time to cover simulation.
Numerous examples with detailed explanations are provided throughout the text. These examples
are designed to help the student more clearly understand the theoretical and computational aspects
of the material and to be in a position to apply the acquired knowledge to his/her own areas of
interest. A solution manual is available for teachers who adopt this text for their courses. This
manual contains detailed explanations of the solution of all the exercises.
Where appropriate, the text contains program modules written in Matlab or in the Java
programming language. These programs are not meant to be robust production code, but are
presented so that the student may experiment with the mathematical concepts that are discussed.
To free the student from the hassle of copying these code segments from the book, a listing of all of
the code used can be freely downloaded from the web page:
http://press.princeton.edu/titles/8844.html
Acknowledgments
As mentioned just a moment ago, this book arose out of two courses that I teach at North Carolina
State University. It is, therefore, ineluctable that the students who took these courses contributed
immeasurably to its content and form. I would like to express my gratitude to them for their patience
and input. I would like to cite, in particular, Nishit Gandhi, Scott Gerard, Rong Huang, Kathryn
Peding, Amirhosein Norouzi, Robert Shih, Hui Wang, Song Yang, and Shengfan Zhang for their
helpful comments. My own doctoral students, Shep Barge, Tugrul Dayar, Amy Langville, Ning
Liu, and Bin Peng, were subjected to different versions of the text and I owe them a particular

xviii
Preface and Acknowledgments
expression of thanks. One person who deserves special recognition is my daughter Kathryn, who
allowed herself to be badgered by her father into reading over selected probability chapters.
I would like to thank Vickie Kearn and the editorial and production staff at Princeton University
Press for their help and guidance in producing this book. It would be irresponsible of me not to
mention the inﬂuence that my teachers, colleagues, and friends have had on me. I owe them a
considerable debt of gratitude for helping me understand the vital role that mathematics plays, not
only in performance modeling, but in all aspects of life.
Finally, and most of all, I would like to thank my wife Kathie and our four children, Nicola,
Stephanie, Kathryn, and William, for all the love they have shown me over the years.

Part I
PROBABILITY

This page intentionally left blank 

Chapter 1
Probability
1.1 Trials, Sample Spaces, and Events
The notions of trial, sample space, and event are fundamental to the study of probability theory.
Tossing a coin, rolling a die, and choosing a card from a deck of cards are examples that are
frequently used to explain basic concepts of probability. Each toss of the coin, roll of the die,
or choice of a card is called a trial or experiment. We shall use the words trial and experiment
interchangeably. Each execution of a trial is called a realization of the probability experiment.
At the end of any trial involving the examples given above, we are left with a head or a tail,
an integer from one through six, or a particular card, perhaps the queen of hearts. The result of a
trial is called an outcome. The set of all possible outcomes of a probability experiment is called the
sample space and is denoted by . The outcomes that constitute a sample space are also referred to
as sample points or elements. We shall use ω to denote an element of the sample space.
Example 1.1 The sample space for coin tossing has two sample points, a head (H) and a tail (T ).
This gives  = {H, T }, as shown in Figure 1.1.
H
T
Figure 1.1. Sample space for tossing a coin has two elements {H, T }.
Example 1.2 For throwing a die, the sample space is  = {1, 2, 3, 4, 5, 6}, Figure 1.2, which
represents the number of spots on the six faces of the die.
Figure 1.2. Sample space for throwing a die has six elements {1, 2, 3, 4, 5, 6}.
Example 1.3 For choosing a card, the sample space is a set consisting of 52 elements, one for each
of the 52 cards in the deck, from the ace of spades through the king of hearts.
Example 1.4 If an experiment consists of three tosses of a coin, then the sample space is given by
{HHH, HHT, HTH, THH, HT T, THT, T TH, T T T }.
Notice that the element HHT is considered to be different from the elements HTH and THH, even
though all three tosses give two heads and one tail. The position in which the tail occurs is important.
A sample space may be ﬁnite, denumerable (i.e., inﬁnite but countable), or inﬁnite. Its elements
depend on the experiment and how the outcome of the experiment is deﬁned. The four illustrative
examples given above all have a ﬁnite number of elements.

4
Probability
Example 1.5 The sample space derived from an experiment that consists of observing the number
of email messages received at a government ofﬁce in one day may be taken to be denumerable. The
sample space is denumerable since we may tag each arriving email message with a unique integer
n that denotes the number of emails received prior to its arrival. Thus,  = {n|n ∈N}, where N is
the set of nonnegative integers.
Example 1.6 The sample space that arises from an experiment consisting of measuring the time
one waits at a bus stop is inﬁnite. Each outcome is a nonnegative real number x and the sample
space is given by  = {x|x ≥0}.
If a ﬁnite number of trials is performed, then, no matter how large this number may be, there is
no guarantee that every element of its sample space will be realized, even if the sample space itself
is ﬁnite. This is a direct result of the essential probabilistic nature of the experiment. For example,
it is possible, though perhaps not very likely (i.e., not very probable) that after a very large number
of throws of the die, the number 6 has yet to appear.
Notice with emphasis that the sample space is a set, the set consisting of all the elements of
the sample space, i.e., all the possible outcomes, associated with a given experiment. Since the
sample space is a set, all permissible set operations can be performed on it. For example, the notion
of subset is well deﬁned and, for the coin tossing example, four subsets can be deﬁned: the null
subset φ; the subsets {H}, {T }; and the subset that contains all the elements,  = {H, T }. The set
of subsets of  is
φ, {H}, {T }, {H, T }.
Events
The word event by itself conjures up the image of something having happened, and this is no
different in probability theory. We toss a coin and get a head, we throw a die and get a ﬁve, we
choose a card and get the ten of diamonds. Each experiment has an outcome, and in these examples,
the outcome is an element of the sample space. These, the elements of the sample space, are called
the elementary events of the experiment. However, we would like to give a broader meaning to the
term event.
Example 1.7 Consider the event of tossing three coins and getting exactly two heads. There are
three outcomes that allow for this event, namely, {HHT, HTH, THH}. The single tail appears on the
third, second, or ﬁrst toss, respectively.
Example 1.8 Consider the event of throwing a die and getting a prime number. Three outcomes
allow for this event to occur, namely, {2, 3, 5}. This event comes to pass so long as the throw gives
neither one, four, nor six spots.
In these last two examples, we have composed an event as a subset of the sample space, the
subset {HHT, HTH, THH} in the ﬁrst case and the subset {2, 3, 5} in the second. This is how we
deﬁne an event in general. Rather than restricting our concept of an event to just another name for
the elements of the sample space, we think of events as subsets of the sample space. In this case,
the elementary events are the singleton subsets of the sample space, the subsets {H}, {5}, and {10
of diamonds}, for example. More complex events consist of subsets with more than one outcome.
Deﬁning an event as a subset of the sample space and not just as a subset that contains a single
element provides us with much more ﬂexibility and allows us to deﬁne much more general events.
The event is said “to occur” if and only if, the outcome of the experiment is any one of the
elements of the subset that constitute the event. They are assigned names to help identify and
manipulate them.

1.1 Trials, Sample Spaces, and Events
5
Example 1.9 Let A be the event that a throw of the die gives a number greater than 3. This event
consists of the subset {4, 5, 6} and we write A = {4, 5, 6}. Event A occurs if the outcome of the
trial, the number of spots obtained when the die is thrown, is any one of the numbers 4, 5, and 6.
This is illustrated in Figure 1.3.
1
2
3
4
5
6
Ω
A
Figure 1.3. Event A: “Throw a number greater than 3.”
Example 1.10 Let B be the event that the chosen card is a 9. Event B is the subset containing four
elements of the sample space: the 9 of spades, the 9 of clubs, the 9 of diamonds, and the 9 of hearts.
Event B occurs if the card chosen is one of these four.
Example 1.11 The waiting time in minutes at a bus stop can be any nonnegative real number. The
sample space is  = {t ∈ℜ| t ≥0}, and A = {2 ≤t ≤10} is the event that the waiting
time is between 2 and 10 minutes. Event A occurs if the wait is 2.1 minutes or 3.5 minutes or 9.99
minutes, etc.
To summarize, the standard deﬁnition of an event is a subset of the sample space. It consists
of a set of outcomes. The null (or empty) subset, which contains none of the sample points, and
the subset containing the entire sample space are legitimate events—the ﬁrst is called the “null” or
impossible event (it can never occur); the second is called the “universal” or certain event and is
sure to happen no matter what the outcome of the experiments gives. The execution of a trial, or
observation of an experiment, must yield one and only one of the outcomes in the sample space. If
a subset contains none of these outcomes, the event it represents cannot happen; if a subset contains
all of the outcomes, then the event it represents must happen. In general, for each outcome in the
sample space, either the event occurs (if that particular outcome is in the deﬁning subset of the
event) or it does not occur.
Two events A and B deﬁned on the same sample space are said to be equivalent or identical if
A occurs if and only if B occurs. Events A and B may be speciﬁed differently, but the elements in
their deﬁning subsets are identical. In set terminology, two sets A and B are equal (written A = B)
if and only if A ⊂B and B ⊂A.
Example 1.12 Consider an experiment that consists of simultaneously throwing two dice. The
sample space consists of all pairs of the form (i, j) for i = 1, 2, . . . , 6 and j = 1, 2, . . . , 6. Let
A be the event that the sum of the number of spots obtained on the two dice is even, i.e., i + j is an
even number, and let B be the event that both dice show an even number of spots or both dice show
an odd number of spots, i.e., i and j are even or i and j are odd. Although event A has been stated
differently from B, a moment’s reﬂection should convince the reader that the sample points in both
deﬁning subsets must be exactly the same, and hence A = B.
Viewing events as subsets allows us to apply typical set operations to them, operations such as
set union, set intersection, set complementation, and so on.
1. If A is an event, then the complement of A, denoted Ac, is also an event. Ac is the subset of
all sample points of  that are not in A. Event Ac occurs only if A does not occur.
2. The union of two events A and B, denoted A ∪B, is the event consisting of all the sample
points in A and in B. It occurs if either A or B occurs.

6
Probability
3. The intersection of two events A and B, denoted A ∩B, is also an event. It consists of the
sample points that are in both A and B and occurs if both A and B occur.
4. The difference of two events A and B, denoted by A −B, is the event that A occurs and B
does not occur. It consists of the sample points that are in A but not in B. This means that
A −B = A ∩Bc.
It follows that  −B =  ∩Bc = Bc.
5. Finally, notice that if B is a subset of A, i.e., B ⊂A, then the event B implies the event A. In
other words, if B occurs, it must follow that A has also occurred.
Example 1.13 Let event A be “throw a number greater than 3” and let event B be “throw an odd
number.” Event A occurs if a 4, 5, or 6 is thrown, and event B occurs if a 1, 3, or 5 is thrown. Thus
both events occur if a 5 is thrown (this is the event that is the intersection of events A and B) and
neither event occurs if a 2 is thrown (this is the event that is the complement of the union of A and
B). These are represented graphically in Figure 1.4. We have
Ac = {1, 2, 3};
A ∪B = {1, 3, 4, 5, 6};
A ∩B = {5};
A −B = {4, 6}.
1
2
3
4
5
6
Ω
A
B
Figure 1.4. Two events on the die-throwing sample space.
Example 1.14 Or again, consider the card-choosing scenario. The sample space for the deck of
cards contains 52 elements, each of which constitutes an elementary event. Now consider two
events. Let event A be the subset containing the 13 elements corresponding to the diamond cards
in the deck. Event A occurs if any one of these 13 cards is chosen. Let event B be the subset that
contains the elements representing the four queens. This event occurs if one of the four queens is
chosen. The event A ∪B contains 16 elements, the 13 corresponding to the 13 diamonds plus the
queens of spades, clubs, and hearts. The event A ∪B occurs if any one of these 16 cards is chosen:
i.e., if one of the 13 diamond cards is chosen or if one of the four queens is chosen (logical OR).
On the other hand, the event A ∩B has a single element, the element corresponding to the queen of
diamonds. The event A∩B occurs only if a diamond card is chosen and that card is a queen (logical
AND). Finally, the event A −B occurs if any diamond card, other than the queen of diamonds,
occurs.
Thus, as these examples show, the union of two events is also an event. It is the event that consists
of all of the sample points in the two events. Likewise, the intersection of two events is the event
that consists of the sample points that are simultaneously in both events. It follows that the union
of an event and its complement is the universal event , while the intersection of an event and its
complement is the null event φ.
The deﬁnitions of union and intersection may be extended to more than two events. For n events
A1, A2, . . . , An, they are denoted, respectively, by
n
i=1
Ai
and
n
i=1
Ai.


8
Probability
A
C
B
Figure 1.6. Events A, B, and C are mutually exclusive.
A
C
B
Figure 1.7. Events A, B, and C are collectively exhaustive.
Events that are both mutually exclusive and collectively exhaustive, such as those illustrated in
Figure 1.8, are said to form a partition of the sample space. Additionally, the previously deﬁned
four events on the deck of cards, A1, A2, A3, A4, are both mutually exclusive and collectively
exhaustive and constitute a partition of the sample space. Furthermore, since the elementary events
(or outcomes) of a sample space are mutually exclusive and collectively exhaustive they too
constitute a partition of the sample space. Any set of mutually exclusive and collectively exhaustive
events is called an event space.
A
B
C
D
E
F
Figure 1.8. Events A–F constitute a partition.
Example 1.17 Bit sequences are transmitted over a communication channel in groups of ﬁve. Each
bit may be received correctly or else be modiﬁed in transit, which occasions an error. Consider an
experiment that consists in observing the bit values as they arrive and identifying them with the
letter c if the bit is correct and with the letter e if the bit is in error.
The sample space consists of 32 outcomes from ccccc through eeeee, from zero bits transmitted
incorrectly to all ﬁve bits being in error. Let the event Ai, i = 0, 1, . . . , 5, consist of all outcomes
in which i bits are in error. Thus A0 = {ccccc}, A1 = {ecccc, ceccc, ccecc, cccec, cccce}, and so
on up to A5 = {eeeee}. The events Ai, i = 0, 1, . . . , 5, partition the sample space and therefore
constitute an event space. It may be much easier to work in this small event space rather than in the
larger sample space, especially if our only interest is in knowing the number of bits transmitted in
error. Furthermore, when the bits are transmitted in larger groups, the difference becomes even more
important. With 16 bits per group instead of ﬁve, the event space now contains 17 events, whereas
the sample space contains 216 outcomes.

1.2 Probability Axioms and Probability Space
9
1.2 Probability Axioms and Probability Space
Probability Axioms
So far our discussion has been about trials, sample spaces, and events. We now tackle the topic
of probabilities. Our concern will be with assigning probabilities to events, i.e., providing some
measure of the relative likelihood of the occurrence of the event. We realize that when we toss a
fair coin, we have a 50–50 chance that it will give a head. When we throw a fair die, the chance of
getting a 1 is the same as that of getting a 2, or indeed any of the other four possibilities. If a deck of
cards is well shufﬂed and we pick a single card, there is a one in 52 chance that it will be the queen
of hearts. What we have done in these examples is to associate probabilities with the elements of the
sample space; more correctly, we have assigned probabilities to the elementary events, the events
consisting of the singleton subsets of the sample space.
Probabilities are real numbers in the closed interval [0, 1]. The greater the value of the
probability, the more likely the event is to happen. If an event has probability zero, that event cannot
occur; if it has probability one, then it is certain to occur.
Example 1.18 In the coin-tossing example, the probability of getting a head in a single toss is 0.5,
since we are equally likely to get a head as we are to get a tail. This is written as
Prob{H} = 0.5 or Prob{A1} = 0.5,
where A1 is the event {H}.
Similarly, the probability of throwing a 6 with a die is 1/6 and the probability of choosing
the queen of hearts is 1/52. In these cases, the elementary events of each sample space all have
equal probability, or equal likelihood, of being the outcome on any given trial. They are said to
be equiprobable events and the outcome of the experiment is said to be random, since each event
has the same chance of occurring. In a sample space containing n equally likely outcomes, the
probability of any particular outcome occurring is 1/n. Naturally, we can assign probabilities to
events other than elementary events.
Example 1.19 Find the probability that should be associated with the event A2 = {1, 2, 3}, i.e.,
throwing a number smaller than 4 using a fair die. This event occurs if any of the numbers 1, 2,
or 3 is the outcome of the throw. Since each has a probability of 1/6 and there are three of them,
the probability of event A2 is the sum of the probabilities of these three elementary events and is
therefore equal to 0.5.
This holds in general: the probability of any event is simply the sum of the probabilities
associated with the (elementary) elements of the sample space that constitute that event.
Example 1.20 Consider Figure 1.6 once again (reproduced here as Figure 1.9), and assume that
each of the 24 points or elements of the sample space is equiprobable.
A
C
B
Figure 1.9. Sample space with 24 equiprobable elements.

10
Probability
Then event A contains eight elements, and so the probability of this event is
Prob{A} = 1
24 + 1
24 + 1
24 + 1
24 + 1
24 + 1
24 + 1
24 + 1
24 = 8 × 1
24 = 1
3.
Similarly, Prob{B} = 4/24 = 1/6 and Prob{C} = 8/24 = 1/3.
Assigning probabilities to events is an extremely important part of developing probability
models. In some cases, we know in advance the probabilities to associate with elementary events,
while in other cases they must be estimated. If we assume that the coin and the die are fair and the
deck of cards completely shufﬂed, then it is easy to associate probabilities with the elements of the
sample space and subsequently to the events described on these sample spaces. In other cases, the
probabilities must be guessed at or estimated.
Two approaches have been developed for deﬁning probabilities: the relative frequency approach
and the axiomatic approach. The ﬁrst, as its name implies, consists in performing the probability
experiment a great many times, say N, and counting the number of times a certain event occurs, say
n. An estimate of the probability of the event may then be obtained as the relative frequency n/N
with which the event occurs, since we would hope that, in the limit (limit in a probabilistic sense)
as N →∞, the ratio n/N tends to the correct probability of the event. In mathematical terms, this
is stated as follows: Given that the probability of an event is p, then
lim
N→∞Prob
 n
N −p
 > ϵ

= 0
for any small ϵ > 0. In other words, no matter how small we choose ϵ to be, the probability that the
difference between n/N and p is greater than ϵ tends to zero as N →∞. Use of relative frequencies
as estimates of probability can be justiﬁed mathematically, as we shall see later.
The axiomatic approach sets up a small number of laws or axioms on which the entire theory
of probability is based. Fundamental to this concept is the fact that it is possible to manipulate
probabilities using the same logic algebra with which the events themselves are manipulated. The
three basic axioms are as follows.
Axiom 1:
For any event A, 0 ≤Prob{A} ≤1; i.e., probabilities are real numbers
in the interval [0, 1].
Axiom 2:
Prob{} = 1; The universal or certain event is assigned probability 1.
Axiom 3:
For any countable collection of events A1, A2, . . . that are mutually exclusive,
Prob
	 ∞

i=1
Ai

≡Prob{A1 ∪A2 ∪· · · ∪An ∪· · · } =
∞

i=1
Prob{Ai}.
In some elementary texts, the third axiom is replaced with the simpler
Axiom 3†:
For two mutually exclusive events A and B, Prob{A ∪B} = Prob{A} + Prob{B}
and a comment included stating that this extends in a natural sense to any ﬁnite or denumerable
number of mutually exclusive events.
These three axioms are very natural; the ﬁrst two are almost trivial, which essentially means that
all of probability is based on unions of mutually exclusive events. To gain some insight, consider
the following examples.
Example 1.21 If Prob{A} = p1 and Prob{B} = p2 where A and B are two mutually exclusive
events, then the probability of the events A ∪B and A ∩B are given by
Prob{A ∪B} = p1 + p2
and Prob{A ∩B} = 0.

1.2 Probability Axioms and Probability Space
11
Example 1.22 If the sets A and B are not mutually exclusive, then the probability of the event
A ∪B will be less than p1 + p2 since some of the elementary events will be present in both A and
B, but can only be counted once. The probability of the event A∩B will be greater than zero; it will
be the sum of the probabilities of the elementary events found in the intersection of the two subsets.
It follows then that
Prob{A ∪B} = Prob{A} + Prob{B} −Prob{A ∩B}.
Observe that the probability of an event A, formed from the union of a set of mutually exclusive
events, is equal to the sum of the probabilities of those mutually exclusive events, i.e.,
A = Prob
	 n
i=1
Ai

= Prob{A1 ∪A2 ∪· · · ∪An} =
n

i=1
Prob{Ai}.
In particular, the probability of any event is equal to the sum of the probabilities of the outcomes in
the sample space that constitute the event since outcomes are elementary events which are mutually
exclusive.
A number of the most important results that follow from these deﬁnitions are presented below.
The reader should make an effort to prove these independently.
• For any event A, Prob{Ac} = 1 −Prob{A}. Alternatively, Prob{A} + Prob{Ac} = 1.
• For the impossible event φ, Prob{φ} = 0 (since Prob{} = 1).
• If A and B are any events, not necessarily mutually exclusive,
Prob{A ∪B} = Prob{A} + Prob{B} −Prob{A ∩B}.
Thus Prob{A ∪B} ≤Prob{A} + Prob{B}.
• For arbitrary events A and B,
Prob{A −B} = Prob{A} −Prob{A ∩B}.
• For arbitrary events A and B with B ⊂A,
Prob{B} ≤Prob{A}.
It is interesting to observe that an event having probability zero does not necessarily mean that this
event cannot occur. The probability of no heads appearing in an inﬁnite number of throws of a fair
coin is zero, but this event can occur.
Probability Space
The set of subsets of a given set, which includes the empty subset and the complete set itself, is
sometimes referred to as the superset or power set of the given set. The superset of a set of elements
in a sample space is therefore the set of all possible events that may be deﬁned on that space. When
the sample space is ﬁnite, or even when it is countably inﬁnite (denumerable), it is possible to
assign probabilities to each event in such a way that all three axioms are satisﬁed. However, when
the sample space is not denumerable, such as the set of points on a segment of the real line, such
an assignment of probabilities may not be possible. To avoid difﬁculties of this nature, we restrict
the set of events to those to which probabilities satisfying all three axioms can be assigned. This is
the basis of “measure theory:” for a given application, there is a particular family of events (a class
of subsets of ), to which probabilities can be assigned, i.e., given a “measure.” We shall call this
family of subsets F. Since we will wish to apply set operations, we need to insist that F be closed
under countable unions, intersections, and complementation. A collection of subsets of a given set
 that is closed under countable unions and complementation is called a σ-ﬁeld of subsets of .

12
Probability
The term σ-algebra is also used. Using DeMorgan’s law, it may be shown that countable
intersections of subsets of a σ-ﬁeld F also lie in F.
Example 1.23 The set {, φ} is the smallest σ-ﬁeld deﬁned on a sample space. It is sometimes
called the trivial σ-ﬁeld over  and is a subset of every other σ-ﬁeld over . The superset of  is
the largest σ-ﬁeld over .
Example 1.24 If A and B are two events, then the set containing the events , φ, A, Ac,
B, and Bc is a σ-ﬁeld.
Example 1.25 In a die-rolling experiment having sample space {1, 2, 3, 4, 5, 6}, the following are
all σ-ﬁelds:
F = {, φ},
F = {, φ, {2, 4, 6}, {1, 3, 5}},
F = {, φ, {1, 2, 4, 6}, {3, 5}},
but the sets {, φ, {1, 2}, {3, 4}, {5, 6}} and {, φ, {1, 2, 4, 6}, {3, 4, 5}} are not.
We may now deﬁne a probability space or probability system. This is deﬁned as the triplet {,
F, Prob}, where  is a set, F is a σ-ﬁeld of subsets of  that includes , and Prob is a probability
measure on F that satisﬁes the three axioms given above. Thus, Prob{·} is a function with domain
F and range [0, 1] which satisﬁes axioms 1–3. It assigns a number in [0, 1] to events in F.
1.3 Conditional Probability
Before performing a probability experiment, we cannot know precisely the particular outcome that
will occur, nor whether an event A, composed of some subset of the outcomes, will actually happen.
We may know that the event is likely to take place, if Prob{A} is close to one, or unlikely to
take place, if Prob{A} is close to zero, but we cannot be sure until after the experiment has been
conducted. Prob{A} is the prior probability of A. We now ask how this prior probability of an event
A changes if we are informed that some other event, B, has occurred. In other words, a probability
experiment has taken place and one of the outcomes that constitutes an event B observed to have
been the result. We are not told which particular outcome in B occurred, just that this event was
observed to occur. We wish to know, given this additional information, how our knowledge of the
probability of A occurring must be altered.
Example 1.26 Let us return to the example in which we consider the probabilities obtained on three
throws of a fair coin. The elements of the sample space are
{HHH, HHT, HTH, THH, HT T, THT, T TH, T T T }
and the probability of each of these events is 1/8. Suppose we are interested in the probability of
getting three heads, A = {HHH}. The prior probability of this event is Prob{A} = 1/8. Now, how
do the probabilities change if we know the result of the ﬁrst throw?
If the ﬁrst throw gives tails, the event B is constituted as B = {THH, THT, TTH, TTT } and we
know that we are not going to get our three heads! Once we know that the result of the ﬁrst throw is
tails, the event of interest becomes impossible, i.e., has probability zero.
If the ﬁrst throw gives a head, i.e., B = {HHH, HHT, HTH, HTT }, then the event A = {HHH}
is still possible. The question we are now faced with is to determine the probability of getting
{HHH} given that we know that the ﬁrst throw gives a head. Obviously the probability must
now be greater than 1/8. All we need to do is to get heads on the second and third throws, each of
which is obtained with probability 1/2. Thus, given that the ﬁrst throw yields heads, the probability

1.3 Conditional Probability
13
of getting the event HHH is 1/4. Of the original eight elementary events, only four of them can
now be assigned positive probabilities. From a different vantage point, the event B contains four
equiprobable outcomes and is known to have occurred. It follows that the probability of any one of
these four equiprobable outcomes, and in particular that of HHH, is 1/4.
The effect of knowing that a certain event has occurred changes the original probabilities of
other events deﬁned on the sample space. Some of these may become zero; for some others, their
associated probability is increased. For yet others, there may be no change.
Example 1.27 Consider Figure 1.10 which represents a sample space with 24 elements all with
probability 1/24. Suppose that we are told that event B has occurred. As a result the prior
probabilities associated with elementary events outside B must be reset to zero and the sum of
the probabilities of the elementary events inside B must sum to 1. In other words, the probabilities
of the elementary events must be renormalized so that only those that can possibly occur have
strictly positive probability and these probabilities must be coherent, i.e., they must sum to 1.
Since the elementary events in B are equiprobable, after renormalization, they must each have
probability 1/12.
B
Figure 1.10. Sample space with 24 equiprobable elements.
We let Prob{A|B} denote the probability of A given that event B has occurred. Because of the
need to renormalize the probabilities so that they continue to sum to 1 after this given event has
taken place, we must have
Prob{A|B} = Prob{A ∩B}
Prob{B}
.
(1.1)
Since it is known that event B occurred, it must have positive probability, i.e., Prob{B} > 0,
and hence the quotient in Equation (1.1) is well deﬁned. The quantity Prob{A|B} is called the
conditional probability of event A given the hypothesis B. It is deﬁned only when Prob{B} ̸= 0.
Notice that a rearrangement of Equation (1.1) gives
Prob{A ∩B} = Prob{A|B}Prob{B}.
(1.2)
Similarly,
Prob{A ∩B} = Prob{B|A}Prob{A}
provided that Prob{A} > 0.
Since conditional probabilities are probabilities in the strictest sense of the term, they satisfy all
the properties that we have seen so far concerning ordinary probabilities. In addition, the following
hold:
• Let A and B be two mutually exclusive events. Then A ∩B = φ and hence Prob{A|B} = 0.
• If event B implies event A, (i.e., B ⊂A), then Prob{A|B} = 1.

14
Probability
Example 1.28 Let A be the event that a red queen is pulled from a deck of cards and let B be the
event that a red card is pulled. Then Prob{A|B}, the probability that a red queen is pulled given that
a red card is chosen, is
Prob{A|B} = Prob{A ∩B}
Prob{B}
= 2/52
1/2 = 1/13.
Notice in this example that Prob{A ∩B} and Prob{B} are prior probabilities. Thus the event
A ∩B contains two of the 52 possible outcomes and the event B contains 26 of the 52 possible
outcomes.
Example 1.29 If we observe Figure 1.11 we see that Prob{A ∩B} = 1/6, that Prob{B} = 1/2,
and that Prob{A|B} = (1/6)/(1/2) = 1/3 as expected. We know that B has occurred and that event
A will occur if one of the four outcomes in A ∩B is chosen from among the 12 equally probable
outcomes in B.
B
A
Figure 1.11. Prob{A|B} = 1/3.
Equation (1.2) can be generalized to multiple events. Let Ai, i = 1, 2, . . . , k, be k events for
which Prob{A1 ∩A2 ∩· · · ∩Ak} > 0. Then
Prob{A1 ∩A2 ∩· · · ∩Ak} = Prob{A1}Prob{A2|A1}Prob{A3|A1 ∩A2} · · ·
× Prob{Ak|A1 ∩A2 ∩· · · ∩Ak−1}.
The proof is by induction. The base clause (k = 2) follows from Equation (1.2):
Prob{A1 ∩A2} = Prob{A1}Prob{A2|A1}.
Now let A = A1 ∩A2 ∩· · · ∩Ak and assume the relation is true for k, i.e., that
Prob{A} = Prob{A1}Prob{A2|A1}Prob{A3|A1 ∩A2} · · · Prob{Ak|A1 ∩A2 ∩· · · ∩Ak−1}.
That the relation is true for k + 1 follows immediately, since
Prob{A1 ∩A2 ∩· · · ∩Ak ∩Ak+1} = Prob{A ∩Ak+1} = Prob{A}Prob{Ak+1|A}.
Example 1.30 In a ﬁrst-year graduate level class of 60 students, ten students are undergraduates.
Let us compute the probability that three randomly chosen students are all undergraduates. We shall
let A1 be the event that the ﬁrst student chosen is an undergraduate student, A2 be the event that the
second one chosen is an undergraduate, and so on. Recalling that the intersection of two events A
and B is the event that occurs when both A and B occur, and using the relationship
Prob{A1 ∩A2 ∩A3} = Prob{A1}Prob{A2|A1}Prob{A3|A1 ∩A2},
we obtain
Prob{A1 ∩A2 ∩A3} = 10
60 × 9
59 × 8
58 = 0.003507.

1.4 Independent Events
15
1.4 Independent Events
We saw previously that two events are mutually exclusive if and only if the probability of the union
of these two events is equal to the sum of the probabilities of the events, i.e., if and only if
Prob{A ∪B} = Prob{A} + Prob{B}.
Now we investigate the probability associated with the intersection of two events. We shall see that
the probability of the intersection of two events is equal to the product of the probabilities of the
events if and only if the outcome of one event does not inﬂuence the outcome of the other, i.e., if
and only if the two events are independent of each other.
Let B be an event with positive probability, i.e., Prob{B} > 0. Then event A is said to be
independent of event B if
Prob{A|B} = Prob{A}.
(1.3)
Thus the fact that event B occurs with positive probability has no effect on event A. Equation (1.3)
essentially says that the probability of event A occurring, given that B has already occurred, is just
the same as the unconditional probability of event A occurring. It makes no difference at all that
event B has occurred.
Example 1.31 Consider an experiment that consists in rolling two colored (and hence distinguish-
able) dice, one red and one green. Let A be the event that the sum of spots obtained is 7, and let B
be the event that the red die shows 3. There are a total of 36 outcomes, each represented as a pair
(i, j), where i denotes the number of spots on the red die, and j the number of spots on the green
die. Of these 36 outcomes, six, namely, (1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1), result in event A
and hence Prob{A} = 6/36. Also six outcomes result in the occurrence of event B, namely, (3, 1),
(3, 2), (3, 3), (3, 4), (3, 5), (3, 6), but only one of these gives event A. Therefore Prob{A|B} = 1/6.
Events A and B must therefore be independent since
Prob{A|B} = 1/6 = 6/36 = Prob{A}.
If event A is independent of event B, then event B must be independent of event A; i.e.,
independence is a symmetric relationship. Substituting Prob{A∩B}/Prob{B} for Prob{A|B} it must
follow that, for independent events
Prob{A|B} = Prob{A ∩B}
Prob{B}
= Prob{A}
or, rearranging terms, that
Prob{A ∩B} = Prob{A}Prob{B}.
Indeed, this is frequently taken as a deﬁnition of independence. Two events A and B are said to be
independent if and only if
Prob{A ∩B} = Prob{A}Prob{B}.
Pursuing this direction, it then follows that, for two independent events
Prob{A|B} = Prob{A ∩B}
Prob{B}
= Prob{A}Prob{B}
Prob{B}
= Prob{A},
which conveniently brings us back to the starting point.
Example 1.32 Suppose a fair coin is thrown twice. Let A be the event that a head occurs on the ﬁrst
throw, and B the event that a head occurs on the second throw. Are A and B independent events?
Obviously Prob{A} = 1/2 = Prob{B}. The event A ∩B is the event of a head occurring on the
ﬁrst throw and a head occurring on the second throw. Thus, Prob{A ∩B} = 1/2 × 1/2 = 1/4 and

16
Probability
since Prob{A} × Prob{B} = 1/4, the events A and B must be independent, since
Prob{A ∩B} = 1/4 = Prob{A}Prob{B}.
Example 1.33 Let A be the event that a card pulled randomly from a deck of 52 cards is red, and
let B be the event that this card is a queen. Are A and B independent events? What happens if event
B is the event that the card pulled is the queen of hearts?
The probability of pulling a red card is 1/2 and the probability of pulling a queen is 1/13. Thus
Prob{A} = 1/2 and Prob{B} = 1/13. Now let us ﬁnd the probability of the event A ∩B (the
probability of pulling a red queen) and see if it equals the product of these two. Since there are two
red queens, the probability of choosing a red queen is 2/52, which is indeed equal to the product of
Prob{A} and Prob{B} and so the events are independent.
If event B is now the event that the card pulled is the queen of hearts, then Prob{B} = 1/52. But
now the event A ∩B consists of a single outcome: there is only one red card that is the queen of
hearts, and so Prob{A ∩B} = 1/52. Therefore the two events are not independent since
1/52 = Prob{A ∩B} ̸= Prob{A}Prob{B} = 1/2 × 1/52.
We may show that, if A and B are independent events, then the pairs (A, Bc), (Ac, B), and
(Ac, Bc) are also independent. For example, to show that A and Bc are independent, we proceed as
follows. Using the result, Prob{A} = Prob{A ∩B} + Prob{A ∩Bc} we obtain
Prob{A ∩Bc} = Prob{A} −Prob{A ∩B}
= Prob{A} −Prob{A}Prob{B}
= Prob{A}(1 −Prob{B}) = Prob{A}Prob{Bc}.
The fact that, given two independent events A and B, the four events A, B, Ac, and Bc are pairwise
independent, has a number of useful applications.
Example 1.34 Before being loaded onto a distribution truck, packages are subject to two
independent tests, to ensure that the truck driver can safely handle them. The weight of the package
must not exceed 80 lbs and the sum of the three dimensions must be less than 8 feet. It has been
observed that 5% of packages exceed the weight limit and 2% exceed the dimension limit. What is
the probability that a package that meets the weight requirement fails the dimension requirement?
The sample space contains four possible outcomes: (ws, ds), (wu, ds), (ws, du), and (wu, du),
where w and d represent weight and dimension, respectively, and s and u represent satisfactory and
unsatisfactory, respectively. Let A be the event that a package satisﬁes the weight requirement, and
B the event that it satisﬁes the dimension requirement. Then Prob{A} = 0.95 and Prob{B} = 0.98.
We also have Prob{Ac} = 0.05 and Prob{Bc} = 0.02.
The event of interest is the single outcome {(ws, du)}, which is given by Prob{A ∩Bc}. Since A
and B are independent, it follows that A and Bc are independent and hence
Prob{(ws, du)} = Prob{A ∩Bc} = Prob{A}Prob{Bc} = 0.95 × 0.02 = 0.0019.
Multiple Independent Events
Consider now multiple events. Let Z be an arbitrary class of events, i.e.,
Z = A1, A2, . . . , An, . . . .
These events are said to be mutually independent (or simply independent), if, for every ﬁnite
subclass A1, A2, . . . , Ak of Z,
Prob{A1 ∩A2 ∩· · · ∩Ak} = Prob{A1}Prob{A2} · · · Prob{Ak}.

1.4 Independent Events
17
In other words, any pair of events (Ai, A j) must satisfy
Prob{Ai ∩A j} = Prob{Ai}Prob{A j};
any triplet of events (Ai, A j, Ak) must satisfy
Prob{Ai ∩A j ∩Ak} = Prob{Ai}Prob{A j}Prob{Ak};
and so on, for quadruples of events, for quintuples of events, etc.
Example 1.35 The following example shows the need for this deﬁnition. Figure 1.12 shows a
sample space with 16 equiprobable elements and on which three events A, B, and C, each with
probability 1/2, are deﬁned. Also, observe that
Prob{A ∩B} = Prob{A ∩C} = Prob{B ∩C}
= Prob{A}Prob{B} = Prob{A}Prob{C} = Prob{B}Prob{C} = 1/4
and hence A, B, and C are pairwise independent.
C
B
A
Figure 1.12. Sample space with 16 equiprobable elements.
However, they are not mutually independent since
Prob{C|A ∩B} = Prob{A ∩B ∩C}
Prob{A ∩B}
= 1/4
1/4 = 1 ̸= Prob{C}.
Alternatively,
1/4 = Prob{A ∩B ∩C} ̸= Prob{A}Prob{B}Prob{C} = 1/8.
In conclusion, we say that the three events A, B, and C deﬁned above, are not independent;
they are simply pairwise independent. Events A, B, and C are mutually independent only if all the
following conditions hold:
Prob{A ∩B} = Prob{A}Prob{B},
Prob{A ∩C} = Prob{A}Prob{C},
Prob{B ∩C} = Prob{B}Prob{C},
Prob{A ∩B ∩C} = Prob{A}Prob{B}Prob{C}.
Example 1.36 Consider a sample space that contains four equiprobable outcomes denoted a, b, c,
and d. Deﬁne three events on this sample space as follows: A = {a, b}, B = {a, b, c}, and C = φ.
This time
Prob{A ∩B ∩C} = 0 and Prob{A}Prob{B}Prob{C} = 1/2 × 3/4 × 0 = 0
but
1/2 = Prob{A ∩B} ̸= Prob{A}Prob{B} = 1/2 × 3/4.
The events A, B, and C are not independent, nor even pairwise independent.

18
Probability
1.5 Law of Total Probability
If A is any event, then it is known that the intersection of A and the universal event  is A. It is also
known that an event B and its complement Bc constitute a partition. Thus
A = A ∩ and B ∪Bc = .
Substituting the second of these into the ﬁrst and then applying DeMorgan’s law, we ﬁnd
A = A ∩(B ∪Bc) = (A ∩B) ∪(A ∩Bc).
(1.4)
Notice that the events (A∩B) and (A∩Bc) are mutually exclusive. This is illustrated in Figure 1.13
which shows that, since B and Bc cannot have any outcomes in common, the intersection of A and
B cannot have any outcomes in common with the intersection of A and Bc.
C
U
U
B
A
B
B
A
A
Figure 1.13. Events (A ∩B) and (A ∩Bc) are mutually exclusive.
Returning to Equation (1.4), using the fact that (A ∩B) and (A ∩Bc) are mutually exclusive, and
applying Axiom 3, we obtain
Prob{A} = Prob{A ∩B} + Prob{A ∩Bc}.
This means that to evaluate the probability of the event A, it is sufﬁcient to ﬁnd the probabilities of
the intersection of A with B and A with Bc and to add them together. This is frequently easier than
trying to ﬁnd the probability of A by some other method.
The same rule applies for any partition of the sample space and not just a partition deﬁned by an
event and its complement. Recall that a partition is a set of events that are mutually exclusive and
collectively exhaustive. Let the n events Bi, i = 1, 2, . . . , n, be a partition of the sample space .
Then, for any event A, we can write
Prob{A} =
n

i=1
Prob{A ∩Bi},
n ≥1.
This is the law of total probability. To show that this law must hold, observe that the sets A∩Bi, i =
1, 2, . . . , n, are mutually exclusive (since the Bi are) and the fact that Bi, i = 1, 2, . . . , n, is a
partition of  implies that
A =
n
i=1
A ∩Bi, n ≥1.
Hence, using Axiom 3,
Prob{A} = Prob
	 n
i=1
A ∩Bi

=
n

i=1
Prob{A ∩Bi}.
Example 1.37 As an illustration, consider Figure 1.14, which shows a partition of a sample space
containing 24 equiprobable outcomes into six events, B1 through B6.

1.5 Law of Total Probability
19
B
B
B
B
B
1
2
5
3
6
B4
A
Figure 1.14. Law of total probability.
It follows then that the probability of the event A is equal to 1/4, since it contains six of the
sample points. Because the events Bi constitute a partition, each point of A is in one and only one
of the events Bi and the probability of event A can be found by adding the probabilities of the events
A ∩Bi for i = 1, 2, . . . , 6. For this particular example it can be seen that these six probabilities are
given by 0, 1/24, 1/12, 0, 1/24, and 1/12 which when added together gives 1/4.
The law of total probability is frequently presented in a different context, one that explicitly
involves conditional probabilities. We have
Prob{A} =
n

i=1
Prob{A ∩Bi} =
n

i=1
Prob{A|Bi}Prob{Bi},
(1.5)
which means that we can ﬁnd Prob{A} by ﬁrst ﬁnding the probability of A given Bi, for all i, and
then computing their weighted average. This often turns out to be a much more convenient way of
computing the probability of the event A, since in many instances we are provided with information
concerning conditional probabilities of an event and we need to use Equation (1.5) to remove these
conditions to ﬁnd the unconditional probability, Prob{A}.
To show that Equation (1.5) is true, observe that, since the events Bi form a partition, we must
have n
i=1 Bi =  and hence
A =
n
i=1
A ∩Bi.
Thus
Prob{A} = Prob
	 n
i=1
A ∩Bi

=
n

i=1
Prob {A ∩Bi} =
n

i=1
Prob{A ∩Bi}
Prob{Bi}
Prob{Bi}
and the desired result follows.
Example 1.38 Suppose three boxes contain a mixture of white and black balls. The ﬁrst box
contains 12 white and three black balls; the second contains four white and 16 black balls and
the third contains six white and four black balls. A box is selected and a single ball is chosen from
it. The choice of box is made according to a throw of a fair die. If the number of spots on the die
is 1, the ﬁrst box is selected. If the number of spots is 2 or 3, the second box is chosen; otherwise
(the number of spots is equal to 4, 5, or 6) the third box is chosen. Suppose we wish to ﬁnd Prob{A}
where A is the event that a white ball is drawn.
In this case we shall base the partition on the three boxes. Speciﬁcially, let Bi, i = 1, 2, 3, be the
event that box i is chosen. Then Prob{B1} = 1/6, Prob{B2} = 2/6, and Prob{B3} = 3/6. Applying
the law of total probability, we have
Prob{A} = Prob{A|B1}Prob{B1} + Prob{A|B2}Prob{B2} + Prob{A|B3}Prob{B3},
which is easily computed using
Prob{A|B1} = 12/15,
Prob{A|B2} = 4/20,
Prob{A|B3} = 6/10.

20
Probability
We have
Prob{A} = 12/15 × 1/6 + 4/20 × 2/6 + 6/10 × 3/6 = 1/2.
1.6 Bayes’ Rule
It frequently happens that we are told that a certain event A has occurred and we would like to
know which of the mutually exclusive and collectively exhaustive events B j has occurred, at least
probabilistically. In other words, we would like to know Prob{B j|A} for any j. Consider some
oft-discussed examples. In one scenario, we may be told that among a certain population there
are those who carry a speciﬁc disease and those who are disease-free. This provides us with
the partition of the sample space (the population) into two disjoint sets. A certain, not entirely
reliable, test may be performed on patients with the object of detecting the presence of this disease.
If we know the ratio of diseased to disease-free patients and the reliability of the testing procedure,
then given that a patient is declared to be disease-free by the testing procedure, we may wish to know
the probability that the patient in fact actually has the disease (the probability that the patient falls
into the ﬁrst (or second) of the two disjoint sets). The same scenario may be obtained by substituting
integrated circuit chips for the population, and partitioning it into defective and good chips, along
with a tester which may sometimes declare a defective chip to be good and vice versa. Given that
a chip is declared to be defective, we wish to know the probability that it is in fact defective. The
transmission of data over a communication channel subject to noise is yet a third example. In this
case the partition is the information that is sent (usually 0’s and 1s) and the noise on the channel
may or may not alter the data. Scenarios such as these are best answered using Bayes’ rule.
We obtain Bayes’ rule from our previous results on conditional probability and the theorem of
total probability. We have
Prob{B j|A} = Prob{A ∩B j}
Prob{A}
=
Prob{A|B j}Prob{B j}

i Prob{A|Bi}Prob{Bi}.
Although it may seem that this complicates matters, what we are in fact doing is dividing the
problem into simpler pieces. This becomes obvious in the following example where we choose
a sample space partitioned into three events, rather than into two as is the case with the examples
outlined above.
Example 1.39 Consider a university professor who observes the students who come into his ofﬁce
with questions. This professor determines that 60% of the students are BSc students whereas 30%
are MSc and only 10% are PhD students. The professor further notes that he can handle the questions
of 80% of the BSc students in less than ﬁve minutes, whereas only 50% of the MSc students and 40%
of the PhD students can be handled in ﬁve minutes or less. The next student to enter the professor’s
ofﬁce needed only two minutes of the professor time. What is the probability that student was a PhD
student?
To answer this question, we will let Bi, i = 1, 2, 3, be the event “the student is a BSc, MSc,
PhD” student, respectively, and we will let event A be the event “student requires ﬁve minutes or
less.” From the theorem of total probability, we have
Prob{A} = Prob{A|B1}Prob{B1} + Prob{A|B2}Prob{B2} + Prob{A|B3}Prob{B3}
= 0.8 × 0.6 + 0.5 × 0.3 + 0.4 × 0.1 = 0.6700.
This computation gives us the denominator for insertion into Bayes’ rule. It tells us that approxi-
mately two-thirds of all students’ questions can be handled in ﬁve minutes or less. What we would

1.7 Exercises
21
now like to compute is Prob{B3|A}, which from Bayes’ rule, is
Prob{B3|A} = Prob{A|B3}Prob{B3}
Prob{A}
= 0.4 × 0.1
0.6700
= 0.0597,
or about 6% and is the answer that we seek.
The critical point in answering questions such as these is in determining which set of events
constitutes the partition of the sample space. Valuable clues are usually found in the question posed.
If we remember that we are asked to compute Prob{B j|A} and relate this to the words in the
question, then it becomes apparent that the words “student was a PhD student” suggest a partition
based on the status of the student, and the event A, the information we are given, relates to the time
taken by the student. The key is in understanding that we are given Prob{A|B j} and we are asked to
ﬁnd Prob{B j|A}. In its simplest form, Bayes’ law is written as
Prob{B|A} = Prob{A|B}Prob{B}
Prob{A}
.
1.7 Exercises
Exercise 1.1.1 A multiprocessing system contains six processors, each of which may be up and running, or
down and in need of repair. Describe an element of the sample space and ﬁnd the number of elements in the
sample space. List the elements in the event A =“at least ﬁve processors are working.”
Exercise 1.1.2 A gum ball dispenser contains a large number of gum balls in three different colors, red, green,
and yellow. Assuming that the gum balls are dispensed one at a time, describe an appropriate sample space for
this scenario and list all possible events.
A determined child continues to buy gum balls until he gets a yellow one. Describe an appropriate sample
space in this case.
Exercise 1.1.3 A brother and a sister arrive at the gum ball dispenser of the previous question, and each of
them buys a single gum ball. The boy always allows his sister to go ﬁrst. Let A be the event that the girl gets a
yellow gum ball and let B be the event that at least one of them gets a yellow gum ball.
(a) Describe an appropriate sample space in this case.
(b) What outcomes constitute event A?
(c) What outcomes constitute event B?
(d) What outcomes constitute event A ∩B?
(e) What outcomes constitute event A ∩Bc?
(f) What outcomes constitute event B −A?
Exercise 1.1.4 The mail that arrives at our house is for father, mother, or children and may be categorized into
junk mail, bills, or personal letters. The family scrutinizes each piece of incoming mail and observes that it is
one of nine types, from jf ( junk mail for father) through pc (personal letter for children). Thus, in terms of
trials and outcomes, each trial is an examination of a letter and each outcome is a two-letter word.
(a) What is the sample space of this experiment?
(b) Let A1 be the event “junk mail.” What outcomes constitute event A1?
(c) Let A2 be the event “mail for children.” What outcomes constitute event A2?
(d) Let A3 be the event “not personal.” What outcomes constitute event A3?
(e) Let A4 be the event “mail for parents.” What outcomes constitute event A4?
(f) Are events A2 and A4 mutually exclusive?
(g) Are events A1, A2, and A3 collectively exhaustive?
(h) Which events imply another?
Exercise 1.1.5 Consider an experiment in which three different coins (say a penny, a nickel, and a dime in
that order) are tossed and the sequence of heads and tails observed. For each of the following pairs of events,

22
Probability
A and B, give the subset of outcomes that deﬁnes the events and state whether the pair of events are mutually
exclusive, collectively exhaustive, neither or both.
(a)
A: The penny comes up heads.
B: The penny comes up tails.
(b)
A: The penny comes up heads.
B: The dime comes up tails.
(c)
A: At least one of the coins shows heads.
B: At least one of the coins shows tails.
(d)
A: There is exactly one head showing.
B: There is exactly one tail showing.
(e)
A: Two or more heads occur.
B: Two or more tails occur.
Exercise 1.1.6 A brand new light bulb is placed in a socket and the time it takes until it burns out is measured.
Describe an appropriate sample space for this experiment. Use mathematical set notation to describe the
following events:
(a) A = the light bulb lasts at least 100 hours.
(b) B = the light bulb lasts between 120 and 160 hours.
(c) C = the light bulb lasts less than 200 hours.
(d) A ∩Cc.
Exercise 1.2.1 An unbiased die is thrown once. Compute the probability of the following events.
(a) A1 : The number of spots shown is odd.
(b) A2 : The number of spots shown is less than 3.
(c) A3 : The number of spots shown is a prime number.
Exercise 1.2.2 Two unbiased dice are thrown simultaneously. Describe an appropriate sample space and
specify the probability that should be assigned to each. Also, ﬁnd the probability of the following events:
(a) A1 : The number on each die is equal to 1.
(b) A2 : The sum of the spots on the two dice is equal to 3.
(c) A3 : The sum of the spots on the two dice is greater than 10.
Exercise 1.2.3 A card is drawn from a standard pack of 52 well-shufﬂed cards. What is the probability that
it is a king? Without replacing this ﬁrst king card, a second card is drawn. What is the probability that the
second card pulled is a king? What is the probability that the ﬁrst four cards drawn from a standard deck of 52
well-shufﬂed cards are all kings. Once drawn, a card is not replaced in the deck.
Exercise 1.2.4 Prove the following relationships.
(a) Prob{A ∪B} = Prob{A} + Prob{B} −Prob{A ∩B}.
(b) Prob{A ∩Bc} = Prob{A ∪B} −Prob{B}.
Exercise 1.2.5 A card is drawn at random from a standard deck of 52 well-shufﬂed cards. Let A be the event
that the card drawn is a queen and let B be the event that the card pulled is red. Find the probabilities of the
following events and state in words what they represent.
(a) A ∩B.
(b) A ∪B.
(c) B −A.
Exercise 1.2.6 A university professor drives from his home in Cary to his university ofﬁce in Raleigh each
day. His car, which is rather old, fails to start one out of every eight times and he ends up taking his wife’s
car. Furthermore, the rate of growth of Cary is so high that trafﬁc problems are common. The professor ﬁnds
that 70% of the time, trafﬁc is so bad that he is forced to drive fast his preferred exit off the beltline, Western
Boulevard, and take the next exit, Hillsborough street. What is the probability of seeing this professor driving
to his ofﬁce along Hillsborough street, in his wife’s car?
Exercise 1.2.7 A prisoner in a Kafkaesque prison is put in the following situation. A regular deck of 52 cards
is placed in front of him. He must choose cards one at a time to determine their color. Once chosen, the card is
replaced in the deck and the deck is shufﬂed. If the prisoner happens to select three consecutive red cards, he
is executed. If he happens to selects six cards before three consecutive red cards appear, he is granted freedom.
What is the probability that the prisoner is executed.

1.7 Exercises
23
Exercise 1.2.8 Three marksmen ﬁre simultaneously and independently at a target. What is the probability of
the target being hit at least once, given that marksman one hits a target nine times out of ten, marksman two
hits a target eight times out of ten while marksman three only hits a target one out of every two times.
Exercise 1.2.9 Fifty teams compete in a student programming competition. It has been observed that 60% of
the teams use the programming language C while the others use C++, and experience has shown that teams
who program in C are twice as likely to win as those who use C++. Furthermore, ten teams who use C++
include a graduate student, while only four of those who use C include a graduate student.
(a) What is the probability that the winning team programs in C?
(b) What is the probability that the winning team programs in C and includes a graduate student?
(c) What is the probability that the winning team includes a graduate student?
(d) Given that the winning team includes a graduate student, what is the probability that team
programmed in C?
Exercise 1.3.1 Let A be the event that an odd number of spots comes up when a fair die is thrown, and let B
be the event that the number of spots is a prime number. What is Prob{A|B} and Prob{B|A}?
Exercise 1.3.2 A card is drawn from a well-shufﬂed standard deck of 52 cards. Let A be the event that the
chosen card, is a heart, let B be the event that it is a black card, and let C be the event that the chosen card is a
red queen. Find Prob{A|C} and Prob{B|C}. Which of the events A, B, and C are mutually exclusive?
Exercise 1.3.3 A family has three children. What is the probability that all three children are boys? What is
the probability that there are two girls and one boy? Given that at least one of the three is a boy, what is the
probability that all three children are boys. You should assume that Prob{boy} = Prob{girl} = 1/2.
Exercise 1.3.4 Three cards are placed in a box; one is white on both sides, one is black on both sides, and the
third is white on one side and black on the other. One card is chosen at random from the box and placed on a
table. The (uppermost) face that shows is white. Explain why the probability that the hidden face is black is
equal to 1/3 and not 1/2.
Exercise 1.4.1 If Prob{A | B} = Prob{B} = Prob{A ∪B} = 1/2, are A and B independent?
Exercise 1.4.2 A ﬂashlight contains two batteries that sit one on top of the other. These batteries come from
different batches and may be assumed to be independent of one another. Both batteries must work in order for
the ﬂashlight to work. If the probability that the ﬁrst battery is defective is 0.05 and the probability that the
second is defective is 0.15, what is the probability that the ﬂashlight works properly?
Exercise 1.4.3 A spelunker enters a cave with two ﬂashlights, one that contains three batteries in series (one
on top of the other) and another that contains two batteries in series. Assume that all batteries are independent
and that each will work with probability 0.9. Find the probability that the spelunker will have some means of
illumination during his expedition.
Exercise 1.5.1 Six boxes contain white and black balls. Speciﬁcally, each box contains exactly one white ball;
also box i contains i black balls, for i = 1, 2, . . . , 6. A fair die is tossed and a ball is selected from the box
whose number is given by the die. What is the probability that a white ball is selected?
Exercise 1.5.2 A card is chosen at random from a deck of 52 cards and inserted into a second deck of 52
well-shufﬂed cards. A card is now selected at random from this augmented deck of 53 cards. Show that the
probability of this card being a queen is exactly the same as the probability of drawing a queen from the ﬁrst
deck of 52 cards.
Exercise 1.5.3
A factory has three machines that manufacture widgets. The percentages of a total day’s
production manufactured by the machines are 10%, 35%, and 55%, respectively. Furthermore, it is known that
5%, 3%, and 1% of the outputs of the respective three machines are defective. What is the probability that a
randomly selected widget at the end of the day’s production runs will be defective?
Exercise 1.5.4 A computer game requires a player to ﬁnd safe haven in a secure location where her enemies
cannot penetrate. Four doorways appear before the player, from which she must choose to enter one and only
one. The player must then make a second choice from among two, four, one, or ﬁve potholes to descend,
respectively depending on which door she walks through. In each case one pothole leads to the safe haven.

24
Probability
The player is rushed into making a decision and in her haste makes choices randomly. What is the probability
of her safely reaching the haven?
Exercise 1.5.5 The ﬁrst of two boxes contains b1 blue balls and r1 red balls; the second contains b2 blue balls
and r2 red balls. One ball is randomly chosen from the ﬁrst box and put into the second. When this has been
accomplished, a ball is chosen at random from the second box and put into the ﬁrst. A ball is now chosen from
the ﬁrst box. What is the probability that it is blue?
Exercise 1.6.1 Returning to Exercise 1.5.1, given that the selected ball is white, what is the probability that it
came from box 1?
Exercise 1.6.2 In the scenario of Exercise 1.5.3, what is the probability that a defective, randomly selected
widget was produced by the ﬁrst machine? What is the probability that it was produced by the second machine.
And the third?
Exercise 1.6.3 A bag contains two fair coins and one two-headed coin. One coin is randomly selected, tossed
three times, and three heads are obtained. What is the probability that the chosen coin is the two-headed coin?
Exercise 1.6.4 A most unusual Irish pub serves only Guinness and Harp. The owner of this pub observes that
85% of his male customers drink Guinness as opposed to 35% of his female customers. On any given evening,
this pub owner notes that there are three times as many males as females. What is the probability that the person
sitting beside the ﬁreplace drinking Guinness is female?
Exercise 1.6.5 Historically on St. Patrick’s day (March 17), the probability that it rains on the Dublin parade
is 0.75. Two television stations are noted for their weather forecasting abilities. The ﬁrst, which is correct nine
times out of ten, says that it will rain on the upcoming parade; the second, which is correct eleven times out
of twelve, says that it will not rain. What is the probability that it will rain on the upcoming St. Patrick’s day
parade?
Exercise 1.6.6 80% of the murders committed in a certain town are committed by men. A dead body with a
single gunshot wound in the head has just been found. Two detectives examine the evidence. The ﬁrst detective,
who is right seven times out of ten, announces that the murderer was a male but the second detective, who is
right three times out of four, says that the murder was committed by a woman. What is the probability that the
author of the crime was a woman?

Chapter 2
Combinatorics—The Art of Counting
In many of the examples we have seen so far, it has been necessary to determine the number
of ways in which a given event can occur, and hence, knowing the probability of each of these
occurrences and Axiom 3, we can work out the probability of the event. In this section, we shall
consider techniques, based on the law of total probability, for counting all possibilities. It is usual
to describe this problem in terms of how distinguishable balls may be chosen from a box and it
is also usual to assume that the choice is random, i.e., each ball in the box is equally likely to be
chosen. Alternatively, the problem may be phrased in terms of how indistinguishable balls may be
inserted into distinguishable boxes. The former is called the selection problem; the latter is called
the allocation problem. We shall formulate our discussion in terms of the selection problem.
In the context of the selection problem, once one of the distinguishable balls has been chosen
from the box, and prior to the next ball being selected, a decision must be taken as to what should
be done with the ﬁrst ball chosen. If it is put back into the box, it is said that the selection is made
with replacement. In this case, the same ball may be chosen a second time, then a third time, and a
fourth time, and so on. The second possibility is that the ball is set aside and never returned to the
box. In this case, the selection is made without replacement.
A ﬁnal point concerns the order in which the balls are selected. In some cases, this order is
important. For example, it may be necessary to know whether the black ball was chosen before or
after the white ball. When the order is important, the term used is permutation. In other cases, all that
is needed is to know that both a black ball and a white ball were chosen, and not the order in which
they were chosen. This is known as a combination. Combinations do not distinguish selections by
their order: permutations do.
2.1 Permutations
An arrangement of items, also called an ordered sequence of items, is said to be a permutation of
the items. An ordered sequence of n items is called an n-permutation. With n distinct items, there
are n! permutations possible. This is the number of different ways in which the n distinct items can
be arranged.
Example 2.1 Given three different letters A, B, and C there are 3! = 6 permutations (or arrange-
ments), namely,
ABC, ACB, BAC, BCA, CAB, CBA.
Notice that, not only there are three distinct items A, B, and C, but there are also three different
places into which they may be placed, i.e., the ﬁrst, second, and third positions of a three-letter
string. For this reason, ABC is considered to be a different permutation from ACB.
Now consider the number of permutations that may be generated from items that are not all
distinct.

26
Combinatorics—The Art of Counting
Example 2.2 Consider the word OXO. This time not all letters are distinct. There are two O’s and
one X, and only three different arrangements can be found, namely,
OXO, XOO, OOX.
Since the character O appears twice, the number n! = 3! obtained with n = 3 distinct characters
must be divided by 2!. In this case, we have 3!/2! = 3 arrangements.
Example 2.3 Consider the word PUPPY. If all ﬁve characters were distinct, the number of
permutations that would be obtained is 5!. However, since P appears three times in PUPPY, the
5! must be divided by 3! to yield 5!/3! = 5 × 4 = 20. The number of different ways in which the
characters of the word PUPPY can be arranged is therefore 20.
Example 2.4 As a ﬁnal example, consider the word NEEDED. It contains three E’s, two D’s, and
one N. The number of different ways in which the characters in this word may be arranged is
6!
3! × 2! = 60.
Thus, the number of permutations that may be obtained from n items, not all of which are distinct,
may be obtained by ﬁrst assuming them to be distinct (which gives n!), and then, for each multiple
item, dividing by the factorial of its multiplicity. We are now ready to examine the various versions
of the selection problem. We shall ﬁrst consider permutations with and without replacement and
then consider combinations without and with replacements.
2.2 Permutations with Replacements
In the case of permutations with replacement, our goal is to count the number of ways in which
k balls may be selected from among n distinguishable balls. After each ball is chosen and its
characteristics recorded, it is replaced in the box and the next ball is chosen. An alternative way
to state this is to say that after each ball is selected, that ball is set aside and a ball that is identical to
it takes its place in the box. In this way it is possible for the same ball to be chosen many times. If
k = 1, i.e., only one ball is to be chosen, then the number of possible permutations is n, since any
of the n balls may be chosen. If k = 2, then any of the n balls may be chosen as the ﬁrst ball and
then replaced in the box. For each of these n choices for ﬁrst ball, the next ball chosen may also be
any of the n distinguishable balls, and hence the number of permutations when k = 2 is n ×n = n2.
It is evident that this reasoning may be continued to show that the number of permutations obtained
for any k is nk.
Example 2.5 The number of four-digit codes that can be obtained using the decimal number system
(with ten digits from 0 to 9 inclusive) is 104 = 10,000. These are the codes ranging from 0000
through 9999.
Suppose now that there are n1 ways of choosing a ﬁrst item, and n2 ways of choosing a second
item. Then the number of distinct ordered pairs is equal to n1n2. In terms of distinguishable balls
in boxes, this may be viewed as the number of ways in which one ball may be chosen from a ﬁrst
box containing n1 distinguishable balls and a second ball chosen from a second box containing n2
distinguishable balls. The extension to more than two different items is immediate. If there are ni
ways of choosing an ith item, for i = 1, 2, . . . , k, then the number of distinct ordered k-tuples is
equal to k
i=1 ni = n1n2 . . . nk.
Example 2.6 Suppose a shirt may be chosen from among n1 = 12 different shirts and a tie from
among n2 = 20 different ties. Then the number of shirt and tie combinations is n1n2 = 240.

2.3 Permutations without Replacement
27
2.3 Permutations without Replacement
We now move on to the problem of counting the number of different permutations obtained in
selecting k balls from among n distinguishable balls in the case that once a particular ball is chosen,
it is not returned to the box. This is equal to the number of ordered sequences of k distinguishable
objects, i.e., the number of k-permutations, that can be obtained from n distinguishable objects. We
denote this number P(n, k) and assign the values P(n, 0) = 1, n = 0, 1, . . . , by convention. We say
that P(n, k) is the number of permutations of n objects taken k at a time.
If k = 1, then any ball may be chosen and the total number of permutations is just n. If k = 2,
then any of the n balls may be chosen as the ﬁrst. For each of these n possible choices, there remain
n −1 balls in the box, any one of which may be chosen as the second. Thus the total number of
permutations for k = 2 is equal to n(n −1). With k = 3, there are n different possibilities for the
ﬁrst ball, but only n−1 for the second and n−2 for the third, which gives P(n, 3) = n(n−1)(n−2).
We may now generalize this to any arbitrary k ≤n to obtain
P(n, k) = n(n −1)(n −2) · · · (n −k + 1) =
n!
(n −k)!.
A number of relationships exist among the values of P(n, k) for different values of the parameters
n and k. For example,
P(n, k) = nP(n −1, k −1),
k = 1, 2, . . . , n,
n = 1, 2, . . . ,
for all permissible n and k. This formula may be obtained by observing that
P(n, k) =
n!
(n −k)! = n
(n −1)!
[(n −1) −(k −1)]! = nP(n −1, k −1).
It may also be arrived by simply reasoning as follows. The ﬁrst ball chosen can be any of the n
distinguishable balls. This leaves n −1 balls from which k −1 must still be chosen: the number of
ways in which k −1 balls may be chosen from n −1 is equal to P(n −1, k −1).
Example 2.7 Let n = 4, k = 3 and distinguish the four ball by means of the four letters A, B, C,
and D. We have
P(4, 3) = 4 × 3 × 2 = 24.
These 24 different possibilities are given as
ABC,
ABD,
ACB,
ACD,
ADB,
ADC,
BAC,
BAD,
BCA,
BCD,
BDA,
BDC,
CAB,
CAD,
CBA,
CBD,
CDA,
CDB,
DAB,
DAC,
DBA,
DBC,
DCA,
DCB.
Notice that we distinguish between, for example, ABC and ACB, since the order is important, but
we do not include the same letter more than once (e.g., AAB is not present), since letters once used
cannot be used again.
Example 2.8 Suppose we wish to ﬁnd the number of ways that a four-digit code, in which all the
digits are different, can be selected. Given that we work with ten digits (0 through 9), the ﬁrst digit
can be any of the ten, the second any of the remaining nine, and so on. Continuing in this fashion,
we see that the total number of permutations is given as 10 × 9 × 8 × 7, which naturally is the same
value when computed directly from the formula.
P(10, 4) =
10!
(10 −4)! = 5040.

28
Combinatorics—The Art of Counting
Problems such as these are frequently formulated as probability questions, requesting one, for
example, to ﬁnd the probability that a randomly generated four-digit code has all different digits. As
we have just seen, the total number of four-digit codes having all digits different is 5,040, while the
total number of four-digit numbers is 10,000. If it is assumed that all choices of four-digit codes are
equally likely, then the probability of getting one in which all the digits are different is 5,040/10,000
or about 0.5. Such questions require us to ﬁnd the size of the sample space of permutations without
replacement and to compare it to the size of the sample space with replacement. Obviously, this only
works correctly when all the choices are equally likely (random selection). Two further example will
help clarify this.
Example 2.9 Suppose r pigeons returning from a race are equally likely to enter any of n homing
nests, where n > r. We would like to know the probability of all pigeons ending up in different
nests?
Let k1 be the nest entered by the ﬁrst pigeon, k2 that entered by the second pigeon, and so on.
We have 1 ≤ki ≤n, for i = 1, 2, . . . ,r. The total number of possibilities available to the pigeons
is equal to N = nr, since pigeon number 1 can ﬂy into any of the n nests, and the same for pigeon
number 2, and so on. This is the situation of permutations with replacement. We are told that each of
these N possible choices is equally likely. There are thus nr distinct and equiprobable arrangements
of the r pigeons in the n nests. These nr events are the elementary events that constitute the sample
space of the experiment.
Let A denote the event that all pigeons end up in different nests. This event occurs if all the
ki, i = 1, 2, . . . , kr, are distinct: each pigeon ﬂies into a different nest. This is the situation
of permutation without replacement. The ﬁrst pigeon may enter any one of the n nests, the
second may enter any of the remaining n −1 nests, and so on. The number of possible ways
in which the r pigeons can be arranged in the n nests so that none share a nest is therefore
given by n(n −1) · · · (n −r + 1). This is the number of outcomes that result in the event A,
and since there are a total of nr equiprobable outcomes in the sample space, the probability of
A must be
Prob{A} = n(n −1) · · · (n −r + 1)
nr
= n!/(n −r)!
nr
= w/o replacement
w/ replacement .
Example 2.10 Consider the well-known birthday problem, that of determining the likelihood that
among a group of k persons at least two have the same birthday (day and month only). To ﬁnd the
probability that at least two people have the same birthday, it is easier to compute the complement
of this event, the probability that no two individuals have the same birthday. If we assume that a
year has 365 days, then the number of permutations with replacement is 365k. This is the size of the
sample space from which all possible birthday permutations are drawn. The number of permutations
without replacement is 365!/(365 −k)!. This is the number of outcomes in the sample space in
which no two birthdays in the permutation are the same. The probability of ﬁnding among the k
individuals, no two with the same birthday is the ratio of these two, i.e.,
365!/(365 −k)!
365k
= 365
365 × 364
365 × 363
365 × · · · × 365 −k + 1
365
=

1 −
1
365
 
1 −
2
365

· · ·

1 −k −1
365

.
It may be observed that this probability is less than 0.5 for what may appear to be a relatively small
value of k, namely, k = 23. In other words, in a class with 23 students, the likelihood that no two
students have the same birthday is less than 50%.

2.4 Combinations without Replacement
29
2.4 Combinations without Replacement
We now turn to the case in which balls are selected without replacement, but the order in which they
are selected is unimportant. All that matters is that certain balls have been chosen and the point at
which any one was selected is not of interest. For example, we may have chosen a green ball, a red
ball, and a black ball, but we no longer know or care which of these three was chosen ﬁrst, second,
or third. We shall let C(n, k) denote the number of ways in which k balls may be selected without
replacement and irrespective of their order, from a box containing n balls. This is called the number
of combinations of n items taken k at a time and it is understood to be without regard for order.
As we saw previously, any collection of k distinct items may be placed into k! different
permutations. For example, with k = 3 and using the letters ABC, we have the 3! = 6 permutations
ABC, ACB, BAC, BCA, CAB, CBA.
These are all different permutations because the order is important. If the order is not important,
then the only information we need is that there is an A, a B, and a C. All 3! permutations yield only
a single combination. This provides us with a means to relate permutations without replacement
to combinations without replacement. Given that any sequence of k items may be arranged into k!
permutations, it follows that there must be k! times as many permutations (without replacement) of
k distinct items as there are combinations (without replacement), since the actual order matters in
permutations but not in combinations. In other words, we must have
k!C(n, k) = P(n, k) for n = 0, 1, 2, . . . and k = 0, 1, . . . , n.
This leads to
C(n, k) = P(n, k)
k!
=
n!
k!(n −k)!
for n = 0, 1, 2, . . . and k = 0, 1, . . . , n.
Example 2.11 Let A, B, C, D, and E be ﬁve distinguishable items from which two are to be
chosen (which implicitly implies, without replacement, and it matters not which one comes ﬁrst: i.e.,
combination). The number of ways that these two can be chosen is given by C(5, 2) = 5!/(2!3!) =
10. These are
AB
AC
AD
AE
BC
BD
BE
CD
CE
DE.
If the choices are made randomly, then the probability of getting any particular one of these ten
combinations is 1/10. Let us compute the probability of choosing exactly one of (A, B) and one of
(C, D, E). This is the event that contains the outcomes {AC, AD, AE, BC, BD, BE}. The number
of ways in which such a combination can appear is the product of the number of ways in which
we can choose one from two (i.e., C(2, 1)) times the number of ways we can choose one from the
remaining three (i.e., C(3, 1)). The probability of such a choice is then
C(2, 1) × C(3, 1)
C(5, 2)
= 2!/(1!1!) × 3!/(1!2!)
5!/(2!3!)
= 2 × 3
10
= 0.6.
C(n, k) is called a binomial coefﬁcient since, as we shall see in the next section, it is the
coefﬁcient of the term pkqn−k in the expansion of the binomial (p + q)n. Alternative ways for
writing this binomial coefﬁcient are
C(n, k) = Cn
k =

n
k

,
and we shall use all three, depending on the context. It is read as “n choose k.”

30
Combinatorics—The Art of Counting
To compute binomial coefﬁcients, notice that

n
k

=
n!
k!(n −k)! = n
k

(n −1)!
(k −1)!(n −k)!

= n
k

(n −1)!
(k −1)!([n −1] −[k −1])!

= n
k
n −1
k −1

,
which eventually leads to

n
k

= n
k × n −1
k −1 × · · · × n −(k −1)
k −(k −1) =
k
j=1
n −j + 1
j
,
and is computationally more robust than forming factorials and taking a ratio.
The binomial coefﬁcients possess a large number of interesting properties, only four of which
we list below. Their proof and interpretation is left as an exercise. For 0 ≤j ≤k ≤n:
(a)

n
k

=

n
n −k

.
(b)

n
k

= n
k

n −1
k −1

.
(c)

n
k

=

n −1
k

+

n −1
k −1

.
(d)
 n
k
  k
j

=
 n
j
  n −j
k −j

.
Consider now the problem of placing k distinguishable balls into n different boxes in such a way
that the number of balls in box i is ki, for i = 1, 2, . . . , n. We assume that n
i=1 ki = k, so that each
ball is put into one of the boxes and none are left over. The number of combinations obtained in
selecting the ﬁrst k1 balls for box 1 is given as C(k, k1). This leaves k −k1 balls, from which k2 are
chosen and put into box 2. The number of combinations obtained in selecting these k2 from k −k1
balls is C(k −k1, k2), so that the total obtained with the ﬁrst two boxes is C(k, k1) × C(k −k1, k2).
This now leaves k −k1 −k2 from which k3 must be chosen and put into box 3. Continuing in this
fashion, we see that the total number of combinations is given by
C(k, k1) × C(k −k1, k2) × C(k −k1 −k2, k3) × · · · × C

k −
n−1

i=1
ki, kn

,
where C(k −n−1
i=1 ki, kn) = C(kn, kn). Substituting in the formula for the number of combinations,
we have
k!
k1!(k −k1)! ×
(k −k1)!
k2!(k −k1 −k2)! ×
(k −k1 −k2)!
k3!(k −k1 −k2 −k3)! × · · ·
=
k!
k1!k2! · · · kn! ≡

k
k1, k2, . . . , kn

.
(2.1)

2.5 Combinations with Replacements
31
These are called the multinomial coefﬁcients. Observe that, when k = k1 + k2, we have

k
k1, k2

=
 k
k1

=
 k
k2

.
Example 2.12 Let us compute the number of ways in which ﬁve cards can be dealt from a regular
deck of 52 cards. Since it does not matter in which order the cards are drawn, our concern is with
combinations rather than permutations so the answer is C(52, 5) = 2, 598, 960. Let K be the event
that there are exactly three kings among the selected cards. The number of outcomes that result in
event K is the product of C(4, 3) and C(48, 2) since three kings must be drawn from four cards and
two cards from the remaining 48. The probability of obtaining exactly three kings is
C(4, 3) × C(48, 2)
C(52, 5)
= 0.001736.
Example 2.13 During quality control of a batch of 144 widgets, 12 are chosen at random and
inspected. If any of the 12 is defective, the batch is rejected; otherwise it is accepted. We wish to
compute the probability that a batch containing 10 defective widgets is accepted.
The number of ways in which the inspector can choose the 12 widgets for inspection is the
number of combinations of 144 items taken 12 at a time, and is thus equal to
N = C(144, 12) =
144!
12! 132!.
We are told that these are all equiprobable, since the inspector chooses the 12 widgets at random.
Let A be the event that the batch is accepted, i.e., none of the 10 defective widgets appears in
the sample of 12 chosen by the inspector. This means that all 12 selected widgets belong to the 134
good ones. The number of ways in which this can happen, denoted by N(A), is given as
N(A) = C(134, 12) =
134!
12! 122!.
It now follows that
Prob{A} = N(A)
N
= 134! 12! 132!
12! 122! 144! = 0.4066.
2.5 Combinations with Replacements
It only remains to determine the number of combinations possible when k balls are selected with
replacement from a box containing n distinguishable balls. It may be shown that this is identical to
the problem of counting the number of combinations when k balls are selected without replacement
from a box containing a total of n + k −1 distinguishable balls, i.e.,
(n + k −1)!
(n −1)! k! .
It is useful to describe different scenarios that result in the same number of combinations.
Consider, for example, the one we have been using so far, that of counting the number of different
combinations possible in selecting k balls from a box containing n distinguishable balls. As an
example, we shall let n = 4 and differentiate among the balls by calling them A, B, C, and D. If

32
Combinatorics—The Art of Counting
we choose k = 3 then the formula tells us that there are (4 + 3 −1)!/(3! × 3!) = 20 different
combinations. These are
AAA
AAB
AAC
AAD
ABB
ABC
ABD
ACC
ACD
ADD
BBB
BBC
BBD
BCC
BCD
BDD
CCC
CCD
CDD
DDD.
Notice that, although the four balls are distinguishable, the same ball may occur more than once
in a given combination. This is because after a ball is chosen, it is replaced and may be chosen
once again. Notice also that we do not include combinations such as BAA or CBA because with
combinations, the order is unimportant and these are equivalent to AAB and ABC, respectively.
A second equivalent scenario changes things around. It may be shown that counting the
number of combinations in selecting k balls from a box of n distinguishable balls is equivalent
to counting the number of combinations obtained in distributing k indistinguishable balls among n
distinguishable boxes. This is the assignment problem mentioned previously. Consider the example
with n = 4 boxes and k = 3 balls and let the four distinguishable boxes be represented by a vector
of four integer components, distinguished according to their position in the vector. The 20 different
combinations are then given as
(0 0 0 3)
(1 0 0 2)
(2 0 0 1)
(3 0 0 0)
(0 0 1 2)
(1 0 1 1)
(2 0 1 0)
(0 0 2 1)
(1 0 2 0)
(2 1 0 0)
(0 0 3 0)
(1 1 0 1)
(0 1 0 2)
(1 1 1 0)
(0 1 1 1)
(1 2 0 0)
(0 1 2 0)
(0 2 0 1)
(0 2 1 0)
(0 3 0 0)
where, for example, (1 0 0 2) indicates that one ball is in box 1 and two balls are in box 4 and we are
not able to distinguish among the balls. In this case, we need to distinguish between, for example,
(1 0 0 2) and (2 0 0 1) since, in the ﬁrst case, the ﬁrst box contains one ball and the fourth two balls,
while in the second case, the ﬁrst box contains two balls and the fourth only one ball.
This latter scenario is useful in determining the number of states in certain queuing network
models and Markov chains. The problem is to determine the number of ways in which k identical
customers may be distributed among n different queuing centers. This is the same as the number of
integer vectors of length n that satisfy the constraints
(k1, k2, . . . , kn) with 0 ≤ki ≤k and
n

i=1
ki = k
and is given by
(n + k −1)!
(n −1)! k! .

2.6 Bernoulli (Independent) Trials
33
We summarize all four possibilities below, using an example of four distinguishable items, called
A, B, C, and D, taken two at a time under the various possibilities.
AA AB AC AD
BA BB BC BD
CA CB CC CD
DA DB DC DD
⎫
⎪
⎪
⎬
⎪
⎪
⎭
Permutations with replacement
AB AC AD
BA
BC BD
CA CB
CD
DA DB DC
⎫
⎪
⎪
⎬
⎪
⎪
⎭
Permutations without replacement
AA AB AC AD
BB BC BD
CC CD
DD
⎫
⎪
⎪
⎬
⎪
⎪
⎭
Combinations with replacement
AB AC AD
BC BD
CD
⎫
⎬
⎭Combinations without replacement
The formulae for the four different possibilities are collected together and shown below.
With replacement
Without replacement
Permutations
nk
P(n, k) =
n!
(n −k)!
Combinations
 n + k −1
k

= (n + k −1)!
k! (n −1)!
C(n, k) =
 n
k

=
n!
k!(n −k)!
2.6 Bernoulli (Independent) Trials
We saw previously that in a single toss of a fair coin the probability of getting heads is one-half.
Now let us consider what happens when this coin is tossed n times. An element of the sample space
may be written as (s1, s2, . . . , sn), i.e., a string of length n in which each letter si is either si = H
or si = T , i = 1, 2, . . . , n. It follows that the size of the sample space is 2n and all the elementary
events in this space are equiprobable since each toss is assumed to be independent of all previous
tosses. Thus, for example, the probability of getting all heads is 2−n and is the same as that of getting
all tails, or exactly k heads followed by n −k tails, for 0 ≤k ≤n.
Let A be the event that there are exactly k heads among the n tosses. The number of outcomes
that result in event A is C(n, k), since this is the number of ways in which the k positions containing
H can be selected in a string of length n. The probability of obtaining exactly k heads on n tosses

34
Combinatorics—The Art of Counting
of the coin is therefore given as
 n
k

2−n.
We now modify the results for the case in which the coin is not a fair coin. Suppose the probability
of obtaining heads on one toss of the coin is given as p and the probability of getting tails is q, where
p + q = 1. The sample space is the same as before; only the probabilities of the elementary events
change. The probability of tossing n heads is p × p × · · · × p = pn, that of tossing n tails is
q × q × · · · × q = qn, and the probability of tossing k heads followed by n −k tails is now
p × · · · × p × q × · · · × q = pkqn−k. The probability of any single outcome having k heads
and n −k tails in any order is also given by pkqn−k, because the individual p’s and q’s may be
interchanged. The probability of obtaining exactly k heads in n tosses is now given as

n
k

pkqn−k.
Substituting p = q = 1/2 gives the same result as before. Notice that if we sum over all possible
values of k we obtain, from the binomial theorem,
1 = (p + q)n =
n

k=0
 n
k

pkqn−k,
which must be the case for a proper probability assignment.
Sequences of n independent repetitions of a probability experiment such as this are referred to as
Bernoulli sequences. Often, the outcomes of each trial, rather than being termed heads and tails, are
called success and failure, or good and defective, and so on, depending on the particular application
at hand.
Example 2.14 An experiment has probability of success equal to 0.7. Let us ﬁnd the probability of
three successes and two failures in a sequence of ﬁve independent trials of the experiment.
Let the probability of success be denoted by p and the probability of failure be q = 1 −p. The
probability of obtaining three successes and two failures is equal to p3q2 = 0.73 × 0.32 = 0.03087.
The number of ways in which this can happen, i.e., the number of outcomes with exactly three
successes, is C(5, 3) = 10. Thus, the answer we seek is equal to 0.3087.
Example 2.15 Consider a binary communication channel that sends words coded into n bits. The
probability that any single bit is received correctly is p. The probability that it is received in error is
therefore q = 1 −p. Using code correcting capabilities, it is possible to correct as many as e errors
per word sent. Let us ﬁnd the probability that the transmission of any word is successful, assuming
that the transmission of individual bits is independent.
To solve this problem, notice that, as long as the number of bits in error is e or less, the word
will be received correctly. The probability that the word is received having exactly k bits in error is
given as
 n
k

(1 −p)k pn−k.
Therefore, the probability that the word is correctly received is
e

k=0
 n
k

(1 −p)k pn−k.

2.6 Bernoulli (Independent) Trials
35
Notice that, in a sequence of n independent trials having probability of success equal to p, the
probability of n1 successes and n2 failures, with n = n1 + n2, is given by
 n
n1

pn1(1 −p)n−n1 =
 n
n2

(1 −p)n2 pn−n2.
Bernoulli trials often appear in the context of reliability modeling. A system is represented by a
set of independent components each of which has its own probability of success. Components may
be organized into series in which the output from one becomes the input for the next. Success is
achieved if a successful outcome is obtained with each of the components in the series. If any of the
components fail, then the entire system fails. Alternatively, components may be arranged in parallel
and a successful outcome is achieved if one or more of the components succeeds in completing
its task correctly. The purpose of arranging components in parallel is often to increase the overall
reliability of the system: in the event of one component failing, others continue to function and a
successful outcome for the system as a whole remains possible. Combinations of components in
series and in parallel are also possible.
Consider the case of n components in series, in which the probability of success of each
component is p. The probability that the complete operation is successful is equal to the probability
that all n components function correctly and is equal to pn. The probability that the overall operation
is unsuccessful is 1 −pn.
Now consider the case when the n components are placed in parallel and success is achieved if
at least one of the components is successful. In this case, it is easier to ﬁrst compute the probability
of the event that the overall operation is not a success and then to compute the probability of the
complement of this event. In general, in probability related questions, it is often easier to compute
the probability of the intersection of events than it is to compute the union of a set of events.
Use of the techniques of complementation and DeMorgan’s law frequently allows us to choose
the easier option. In the present situation, a system failure occurs if all n components fail (all
implies intersection, whereas at least one implies union). The probability of failure in a system of n
components arranged in parallel, in which the probability of failure of each component is 1 −p, is
equal to (1 −p)n. From this we can compute the probability of success of the overall system to be
1 −(1 −p)n.
Example 2.16 In life-critical environments, it is not uncommon to have multiple computers perform
exactly the same sequence of tasks so that, if one computer fails, the mission is not compromised.
The mission will continue, and will be deemed to be successful, as long as at least one of the
computers executes its tasks correctly. Consider a system in which three computers perform the
same sequence of six tasks one after the other. Consistency tests are built into each task. A computer
is assumed to be functioning correctly if its state satisﬁes the consistency tests. Let the probability
that a computer fails to satisfy the consistency test during any task be equal to 0.04. We wish to
compute the probability that the mission is successful.
This situation may be represented as a reliability model which consists of three parts in parallel
(the three computers) and in which each part contains six components in series. Let p = 0.96 be
the probability that a computer satisﬁes the consistency test on any given task. Then the probability
that a single computer successfully completes all six tasks is equal to p6 = 0.7828; on the other
hand, p′ = 1 −p6 is the probability that it will not. Since there are three computers in parallel,
the probability that all three will fail is given by (1 −p′)3 = 0.01025. Thus the probability of a
successful mission is 1 −(1 −p′)3 = 0.9897.
The concept of a Bernoulli trial may be generalized from two possible outcomes to many possible
outcomes. Suppose that instead of two possibilities (heads and tails, or 0 and 1, or good and bad)
there are m possibilities h1, h2, . . . , hm with probabilities p1, p2, . . . , pm, and m
j=1 p j = 1. On
each trial, one and only one of the m possibilities can occur. This time the sample space consists of

36
Combinatorics—The Art of Counting
n-tuples in which each element is one of the m choices hi, i = 1, 2, . . . , m. Suppose, for example,
that the number of trials is given by n = 6 and that the number of possible outcomes on each trial
is m = 4. Let us denote the different outcomes by a, b, c, and d. Some possible elements of the
sample space are
(a, a, a, a, a, a),
(c, a, d, d, a, b),
(a, b, b, c, c, c),
. . . .
Now let us consider those particular elements of the sample space for which hi occurs ni times, for
i = 1, 2, . . . , m. We must have m
j=1 n j = n. For example, choosing n1 = 1, n2 = 2, n3 = 0 and
n4 = 3, some possibilities are
(a, b, b, d, d, d),
(b, a, d, d, d, b),
(d, a, d, d, b, b),
. . . .
The total number of elementary events that have the property that hi occurs ni times, subject to the
conditions above, is given by the multinomial coefﬁcient

n
n1, n2, . . . , nm

=
n!
n1!n2! . . . nm!.
We leave it as an exercise to show that indeed the number of ways of placing h1 in n1 slots, h2 in
n2 slots, and so on, with m
j=1 n j = n, is given by the multinomial coefﬁcient. The probability of
occurrence of any elementary event is given by pn1
1 pn2
2 . . . pnm
m and hence the probability that h1 will
occur n1 times, h2 will occur n2 times, and so on is given by
p(n1, n2, . . . , nm) =
n!
n1!n2! . . . nm! pn1
1 pn2
2 . . . pnm
m .
Example 2.17 An introductory graduate level class on applied probability has 10 undergraduate
students, 30 masters students and 20 doctoral students. During the professor’s ofﬁce hours, students
arrive independently to seek answers. What is the probability that, of the last eight students to visit
the professor, two were undergraduate, ﬁve were masters, and one was a doctoral student?
We assume that the probability of an undergraduate student seeking the professor during ofﬁce
hours is 10/60, that of a master’s student 30/60, and that of a doctoral student 20/60. In this case,
the required answer is given by

8
2, 5, 1
 10
60
2 30
60
5 20
60
1
=
7
144.
2.7 Exercises
Exercise 2.1.1 For each of the following words, how many different arrangements of the letters can be found:
RECIPE, BOOK, COOKBOOK?
Exercise 2.1.2 Find a word for which the number of different arrangements of its letters is equal to 30.
Exercise 2.2.1 A pizza parlor offers ten different toppings on its individually sized pizzas. In addition, it offers
thin crust and thick crust. William brings his friends Jimmy and Jon with him and each purchases a single one-
topping pizza. How many different possible commands can be sent to the kitchen?
Exercise 2.2.2 Billy and Kathie bring two of their children to a restaurant in France. The menu is composed of
an entree, a main dish, and either dessert or cheese. The entree is chosen from among four different possibilities,
one of which is snails. Five different possibilities, including two seafood dishes, are offered as the main dish.
To the dismay of the children, who always choose dessert over cheese, only two possibilities are offered for
dessert. The cheese plate is a restaurant-speciﬁc selection and no other choice of cheese is possible. Although

2.7 Exercises
37
Billy will eat anything, neither Kathie nor the children like snails and one of the children refuses to eat seafood.
How many different possible commands can be sent to the kitchen?
Exercise 2.2.3 Consider a probability experiment in which four fair dice are thrown simultaneously. How
many different outcomes (permutations) can be found? What is the probability of getting four 6’s?
Exercise 2.2.4 Is it more probable to obtain at least one 6 in four throws of a fair die than it is to obtain at least
one pair of 6’s when two fair dice are thrown simultaneously 24 times?
Exercise 2.3.1 Show that
P(n, k) = P(n −1, k) + kP(n −1, k −1),
k = 1, 2, . . . , n,
n = 1, 2, . . . .
Interpret this result in terms of permutations of n items taken k at a time.
Exercise 2.3.2 William Stewart’s four children come down for breakfast at random times in the morning. What
is the probability that they appear in order from the oldest to the youngest.
Note: William Stewart does not have twins (nor triplets, nor quadruplets!).
Exercise 2.3.3 The Computer Science Department at North Carolina State University wishes to market a new
software product. The name of this package must consist of three letters only.
(a) How many possibilities does the department have to choose from?
(b) How many possibilities are there if exactly one vowel is included?
(c) How many possibilities are there if all three characters must be different and exactly one must be a
vowel?
Exercise 2.4.1 For each of the following properties of the binomial coefﬁcient, prove their correctness, ﬁrst
using an algebraic argument and then using an intuitive “probabilistic” argument.
(a)
 n
k

=

n
n −k

.
(b)
 n
k

= n
k
 n −1
k −1

.
(c)
 n
k

=
 n −1
k

+
 n −1
k −1

.
(d)
 n
k
  k
j

=
 n
j
  n −j
k −j

.
Exercise 2.4.2 Three cards are chosen at random from a full deck. What is the probability that all three are
kings?
Exercise 2.4.3 A box contains two white balls and four black ones.
(a) Two balls are chosen at random. What is the probability that they are of the same color?
(b) Three balls are chosen at random. What is the probability that all three are black?
Exercise 2.4.4 Yves Bouchet, guide de haute montagne, leads three clients on “la traversé de la Meije.” All
four spend the night in the refuge called “La Promotoire” and must set out on the ascension at 3:00 a.m., i.e., in
the dark. Yves is the ﬁrst to arise and, since it is pitch dark, randomly picks out two boots from the four pairs
left by him and his clients.
(a) What is the probability that Yves actually gets his own boots?
(b) If Yves picks two boots, what is the probability that he chooses two left boots?
(c) What is the probability of choosing one left and one right boot?
(d) If, instead of picking just two boots, Yves picks three, what is the probability that he ﬁnds his own boots
among these three?

38
Combinatorics—The Art of Counting
Exercise 2.4.5 A water polo team consists of seven players, one of whom is the goalkeeper, and six reserves
who may be used as substitutes. The Irish water polo selection committee has chosen 13 players from which
to form a team. These 13 include two goalkeepers, three strikers, four “heavyies,” i.e., defensive players, and
four generalists, i.e., players who function anywhere on the team.
(a) How many teams can the coach form, given that he wishes to ﬁeld a team containing a goalkeeper, one
striker, two heavyies, and three generalists?
(b) Toward the end of the last period, Ireland is leading its main rival, England, and, with defense in mind,
the coach would like to ﬁeld a team with all four heavyies (keeping, of course, one goalkeeper, one
striker, and one generalist). From how many teams can he now choose?
Exercise 2.4.6 While in France, William goes to buy baguettes each morning and each morning he takes the
exact change, 2.30 euros, from his mother’s purse. One morning he ﬁnds a 2 euro coin, two 1 euro coins, ﬁve
0.20 euro coins, and four 0.10 euro coins in her purse. William has been collecting coins and observes that
no coin of the same value comes from the same European country (i.e., all the coins are distinct). How many
different possibilities has William from which to choose the exact amount?
Exercise 2.4.7 An apple grower ﬁnds that 5% of his produce goes bad during transportation. A supermarket
receives a batch of 160 apples per day. The manager of the supermarket randomly selects six apples.
(a) What is the probability that none of the six are bad?
(b) What is the probability that the supermarket manager chooses at least two bad apples?
Exercise 2.4.8 A box contains two white balls, two red balls, and a black ball. Balls are chosen without
replacement from the box.
(a) What is the probability that a red ball is chosen before the black ball?
(b) What is the probability of choosing the two white balls before choosing any other ball?
Exercise 2.4.9 Three boxes, one large, one medium, and one small, contain white and red balls. The large box,
which is chosen half the time, contains ﬁfteen white balls and eight red ones; the medium box, which is chosen
three times out of ten, contains nine white balls and three red ones. The small box contains four white balls and
ﬁve red ones.
(a) On choosing two balls at random from the large box, what is the probability of
(i) getting two white balls?
(ii) getting one white and one red ball?
(b) After selecting one of the boxes according to the prescribed probabilities, what is the probability of
getting one white and one red ball from this box?
(c) Given that a box is chosen according to the prescribed probabilities, and that a white ball is randomly
picked from that box, what is the probability that the ball was actually chosen from the large box?
Exercise 2.4.10 A deck of 52 cards is randomly dealt to four players so that each receives 13 cards. What is
the probability that each player holds an ace?
Exercise 2.4.11 Consider a system that consists of p processes that have access to r identical units of a
resource. Suppose each process alternates between using a single unit of the resource and not using any
resource.
(a) Describe a sample space that illustrates the situation of the p processes.
(b) What is the size of this sample space when (a) p = 6, r = 8 and (b) p = 6, r = 4?
(c) In this ﬁrst case (p = 6, r = 8), deﬁne the following events.
• A: Either two or three processes are using the resource.
• B: At least three but not more than ﬁve processes are using the resource.
• C: Either all the processes are using the resource or none are.
Assuming that each process is equally likely as not to be using a unit of resource, compute the
probabilities of each of these three events.
(d) Under the same assumption, what do the following events represent, and what are their probabilities?
• A ∩B.
• A ∪B.
• A ∪B ∪C.

2.7 Exercises
39
Exercise 2.5.1 Show that, as the population size n grows large, the difference between the number of
combinations without replacement for a ﬁxed number of items k becomes equal to the number of combinations
with replacement.
Exercise 2.6.1 The probability that a message is successfully transmitted over a communication channel
is known to be p. A message that is not received correctly is retransmitted until such time as it is
received correctly. Assuming that successive transmissions are independent, what is the probability that no
retransmissions are needed? What is the probability that exactly two retransmissions are needed?
Exercise 2.6.2 The outcome of a Bernoulli trial may be any one of m possibilities which we denote
h1, h2, . . . , hm. Given n positions, show that the number of ways of placing h1 in n1 positions, h2 in n2
positions and so on, with m
j=1 n j = n, is given by the binomial coefﬁcient,

n
n1, n2, . . . , nm

=
n!
n1!n2! · · · nm!.

Chapter 3
Random Variables and Distribution Functions
3.1 Discrete and Continuous Random Variables
Random variables, frequently abbreviated as RV, deﬁne functions, with domain and range, rather
than variables. Simply put, a random variable is a function whose domain is a sample space  and
whose range is a subset of the real numbers, ℜ. Each elementary event ω ∈ is mapped by the
random variable into ℜ. Random variables, usually denoted by uppercase latin letters, assign real
numbers to the outcomes in their sample space.
Example 3.1 Consider a probability experiment that consists of throwing two dice. There are 36
possible outcomes which may be represented by the pairs (i, j); i = 1, 2, . . . , 6, j = 1, 2, . . . , 6.
These are the 36 elements of the sample space. It is possible to associate many random variables
having this domain. One possibility is the random variable X, deﬁned explicitly by enumeration as
X(1, 1) = 2;
X(1, 2) = X(2, 1) = 3;
X(1, 3) = X(2, 2) = X(3, 1) = 4;
X(1, 4) = X(2, 3) = X(3, 2) = X(4, 1) = 5;
X(1, 5) = X(2, 4) = X(3, 3) = X(4, 2) = X(5, 1) = 6;
X(1, 6) = X(2, 5) = X(3, 4) = X(4, 3) = X(5, 2) = X(6, 1) = 7;
X(2, 6) = X(3, 5) = X(4, 4) = X(5, 3) = X(6, 2) = 8;
X(3, 6) = X(4, 5) = X(5, 4) = X(6, 3) = 9;
X(4, 6) = X(5, 5) = X(6, 4) = 10;
X(5, 6) = X(6, 5) = 11;
X(6, 6) = 12.
Thus X maps the outcome (1, 1) into the real number 2, the outcomes (1, 2) and (2, 1) into the
real number 3, and so on. It is apparent that the random variable X represents the sum of the spots
obtained when two dice are thrown simultaneously. As indicated in Figure 3.1, its domain is the 36
elements of the sample space and its range is the set of integers from 2 through 12.
...
...
...
...
...
...
(1,3)
(2,2)
3
4
(2,1)
(1,1)
2
(1,2)
Sample Space
Sum of spots
Domain
   Range
Figure 3.1. Domain and range of X (not all points are shown).

3.1 Discrete and Continuous Random Variables
41
Observe that the random variable X of Example 3.1 partitions the sample space into 11 subsets;
i.e., into 11 events that are mutually exclusive and collectively exhaustive: in other words, these 11
events form an event space. The subsets are distinguished according to the particular value taken
by the function. Thus {(1, 3), (2, 2), (3, 1)} is the subset consisting of elements of the sample space
that are mapped onto the real number 4; {(5, 6), (6, 5)} is the subset consisting of the elements that
are mapped onto the real number 11; and so on. Each and every outcome in the sample space is
associated with a real number; several may be associated with the same real number: for example,
(1, 3), (2, 2), and (3, 1) are all associated with the integer 4, but none is associated with more than
one real number. This is shown graphically in Figure 3.1 and follows because X has been deﬁned
as a function. Different random variables may be deﬁned on the same sample space. They have the
same domain, but their range can be different.
Example 3.2 Consider a different random variable Y whose domain is the same sample space as
that of X but which is deﬁned as the difference in the number of spots on the two dice. In this case
we have
Y(1, 1) = Y(2, 2) = Y(3, 3) = Y(4, 4) = Y(5, 5) = Y(6, 6) = 0;
Y(2, 1) = Y(3, 2) = Y(4, 3) = Y(5, 4) = Y(6, 5) = 1;
Y(1, 2) = Y(2, 3) = Y(3, 4) = Y(4, 5) = Y(5, 6) = −1;
Y(3, 1) = Y(4, 2) = Y(5, 3) = Y(6, 4) = 2;
Y(1, 3) = Y(2, 4) = Y(3, 5) = Y(4, 6) = −2;
Y(4, 1) = Y(5, 2) = Y(6, 3) = 3;
Y(1, 4) = Y(2, 5) = Y(3, 6) = −3;
Y(5, 1) = Y(6, 2) = 4;
Y(1, 5) = Y(2, 6) = −4;
Y(6, 1) = 5;
Y(1, 6) = −5.
Observe that the random variable Y has the same domain as X but the range of Y is the set of
integers between −5 and +5 inclusive. As illustrated in Figure 3.2, Y also partitions the sample
space into 11 subsets, but not the same 11 as those obtained with X.
Yet a third random variable, Z, may be deﬁned as the absolute value of the difference between
the spots obtained on the two dice. Again, Z has the same domain as X and Y, but now its range is
the set of integers between 0 and 5 inclusive. This random variable partitions the sample space into
six subsets: the ﬁrst subset containing elements of the sample space in which the number of spots
differ by 5, irrespective of which is ﬁrst and which is second; the second subset of the partition
contains elements in which the number of spots differ by 4, and so on. This partition is shown in
Figure 3.3.
This is an important property of random variables, that they partition the sample space into
more meaningful representations of the probability experiment. If our concern is only with the sum
obtained in throwing two dice, it makes more sense to work with the 11 events obtained by the
partition generated by the random variable X, than the 36 elementary events of the sample space. In
this way, random variables are used as a means of simpliﬁcation.
Example 3.3 Let R be the random variable that counts the number of heads obtained in tossing
three coins. Then R assigns the value 3 to the outcome HHH; the value 2 to each of the outcomes

42
Random Variables and Distribution Functions
(1,4)
(2,6)
(2,4)
(1,5)
(2,5)
(6,6)
(6,5)
(5,6)
(6,4)
(1,1)
(1,2)
(2,1)
(1,3)
(2,2)
(3,1)
(2,3)
(3,2)
(4,1)
(3,4)
(3,3)
(4,2)
(5,1)
(1,6)
(4,4)
(5,3)
(5,5)
(4,3)
(5,2)
(6,1)
(3,5)
(6,2)
(3,6)
(4,5)
(5,4)
(6,3)
(4,6)
Y=3
Y=4
Y=5
X=4
X=3
X=2
Y=-5
X=12
. . . . . .
. . . . . .
. . . . . .
. . . . . .
Y=-4
Figure 3.2. Partitions according to X (horizontal and dashed) and Y (vertical and dotted).
(6,6)
(2,4)
(4,6)
(3,5)
(5,6)
(4,5)
(3,4)
(2,3)
(1,2)
(2,1)
(3,2)
(4,3)
(5,4)
(6,5)
(1,1)
(2,2)
(3,3)
(4,4)
(5,5)
(6,1)
(5,2)
(4,1)
(1,6)
(1,3)
(3,1)
(4,2)
(5,3)
(6,4)
(6,3)
(6,2)
(1,4)
(2,5)
(3,6)
(2,6)
(1,5)
(5,1)
Z=5
Z=4
Z=0
Z=3
Figure 3.3. Partition according to Z.
HHT, HTH and THH; the value 1 to each of the outcomes HTT, THT, and TTH. Finally, R assigns
the value 0 to the outcome TTT. If our concern is uniquely with the number of heads obtained, then
it is easier to work with R than with the eight outcomes of the probability experiment.
Since a random variable is a function, it is not possible for two or more values in the range to be
assigned to the same outcome in the sample space. For a random variable X, the set of elements of
the sample space that are assigned the value x is denoted by Ax or X−1(x). Notice that this describes
an event, since it consists of a subset of the sample space. We have
Ax ≡[X = x] ≡{ω ∈|X(ω) = x}
which is the set of all outcomes in the sample space (domain) for which the random variable X
has the value x (an element of the range). In the example of two dice thrown simultaneously and
X the random variable describing the sum of spots obtained, we ﬁnd, for example, that A3 is the
set {(1, 2), (2, 1)}, and A5 is the set {(1, 4), (2, 3) (3, 2) (4, 1)}. As we previously observed, the
set of all such events is called an event space and it is frequently more convenient to work in this
event space than in the original sample space. The event space of our random variable X deﬁnes
11 events, each corresponding to a value of x = 2, 3, . . . , 12. For all other values of x, Ax is the
null set.

3.2 The Probability Mass Function for a Discrete Random Variable
43
Random variables which take on values from a discrete set of numbers, (i.e., whose range
consists of isolated points on the real line) are called discrete random variables. The set may be
inﬁnite but countable. The examples considered so far have all been discrete and ﬁnite. Discrete
random variables are used to represent distinct indivisible items, like people, cars, trees, and so on.
Continuous random variables take on values from a continuous interval. For example, a random
variable that represents the time between two successive arrivals to a queueing system, or that
represents the temperature in a nuclear reactor, is an example of a continuous random variable.
Time, in the ﬁrst case, or temperature in the second, are reals belonging to a continuous subset of
the real number system. Of course, random variables that represent quantities like time, temperature,
distance, area, volume, etc. may be discretized into minutes, hours, yards, miles, and so on, in which
case it is appropriate to use discrete random variables to represent them. All random variables
deﬁned on a discrete sample space must be discrete. However, random variables deﬁned on a
continuous sample space may be discrete or continuous.
3.2 The Probability Mass Function for a Discrete Random Variable
The probability mass function (frequently abbreviated to pmf ) for a discrete random variable X,
gives the probability that the value obtained by X on the outcome of a probability experiment is
equal to x. It is denoted by pX(x). Sometimes the term discrete density function is used in place of
probability mass function. We have
pX(x) ≡Prob{[X = x]} = Prob{ω|X(ω) = x} =

X(ω)=x
Prob{ω}.
The notation [X = x] deﬁnes an event: the event that contains all the outcomes that map into the real
number x. If none of the outcomes map into x, then [X = x] is the null event and has probability
zero. Most often we simply write Prob{[X = x]} as Prob{X = x}. Since pX(x) is a probability, we
must have
0 ≤pX(x) ≤1 for all x ∈ℜ.
(3.1)
The probability mass function pX(x) is often written more simply as p(x); i.e., the subscript X is
omitted when there can be no confusion.
Example 3.4 In the case of two dice being thrown simultaneously, and X the random variable
representing the sum of spots obtained, we have pX(2) = 1/36, since only one of the 36
equiprobable outcomes in the sample space gives the sum 2 (when each die tossed yields a single
spot). Similarly, pX(3) = 2/36 since there are two outcomes that give a total number of spots
equal to 3, namely, (1, 2) and (2, 1). We may continue in this fashion. So long as x is equal to
one of the integers in the interval [2, 12], then pX(x) is strictly positive. For all other values of x,
pX(x) = 0.
Discrete random variables are completely characterized by their probability mass functions.
These functions, and hence their associated discrete random variables, are frequently illustrated
either in tabular form or in the form of bar graphs. In bar graph form, the probability mass function
of the random variable X (sum of spots obtained in throwing two dice) is shown in Figure 3.4. The
corresponding tabular form of this bar graph is shown in Table 3.1.
Table 3.1. Probability mass function of X in tabular form.
xi
2
3
4
5
6
7
8
9
10
11
12
pX(xi)
1/36
2/36
3/36
4/36
5/36
6/36
5/36
4/36
3/36
2/36
1/36

44
Random Variables and Distribution Functions
1
2
3
4
5
6
7
8
9
10
11
12
1/36
2/36
3/36
4/36
6/36
5/36
x
p  (x)
X
Figure 3.4. Bar graph representation of X.
Notice that

x∈ℜ
pX(x) = 1
since the random variable X assigns some value x ∈ℜto each and every sample point ω ∈, so
that when we sum over every possible value of x, we sum over the probabilities of all the outcomes
in the sample space, and this must be equal to 1.
When (as is the case here) the random variable is discrete, then the set of values assumed by x is
denumerable, and constitutes a subset of the reals. We shall denote successive elements of this set
by {x1, x2, . . .}. The set itself is called the image of X and it is usual to denote Prob{X = xi} by pi.
It follows that

k
pX(xk) =

k
pk = 1.
(3.2)
It may be shown that any real-valued function deﬁned on ℜand satisfying Equations (3.1) and
(3.2) is the probability mass function for some random variable, X.
Example 3.5 Let us return to the experiment that consists of tossing a fair coin three times. The
sample space is given by
{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT }
and each of these has probability 1/8 = 0.125. Let R be the discrete random variable deﬁned as
the number of heads obtained during this sequence of tosses. Since R counts the number of heads
obtained it can only have the values 0, 1, 2, or 3. Only one of the eight elementary events gives
zero heads, so the probability that R = 0 is 1/8, i.e., pR(0) = Prob{R = 0} = 0.125. Three of the
elementary events yield one head (HTT, THT, and TTH) and hence pR(1) = 0.375, and so on. The
values that R can assume and their corresponding probabilities are shown in Table 3.2.
Table 3.2. Probability mass function of R.
xi
0
1
2
3
pR(xi)
0.125
0.375
0.375
0.125
In probability theory, the same basic formulation of a probability mass function can often be
used in different scenarios. Such probability mass functions are deﬁned with a small number
of parameters and various scenarios captured by simply changing the values of the parameters.

3.2 The Probability Mass Function for a Discrete Random Variable
45
Random variables whose probability mass functions are identical up to the values assigned to these
parameters are called families of random variables. These include Bernoulli random variables,
binomial random variables, and Poisson random variables among the class of discrete random
variables, and the exponential, Erlang, and standard normal random
variables among the class
of continuous random variables. These and others are considered in detail in later chapters. At this
point, and for illustration purposes only, we present two families of random variables, Bernoulli
random variables and Poisson random variables.
A random variable X is said to be a Bernoulli random variable if its probability mass function
has the form
pX(x) =
⎧
⎨
⎩
1 −p,
x = 0,
p,
x = 1,
0
otherwise.
In this case, the parameter is p and must satisfy the condition 0 < p < 1. Taking p = 1/2 satisﬁes
the requirements for probability experiments dealing with tossing a fair coin; other values of p are
appropriate for experiments using a biased coin.
The probability mass function for a Poisson random variable, X, is given by
pX(n) =
αne−α/n!,
n = 0, 1, 2, . . . ,
0
otherwise,
where the parameter α is strictly positive. Poisson random variables are frequently used to model
arrival processes as follows. Let λ be the rate of arrivals to a facility and t an interval of observation.
Then, during the time interval t, the number of arrivals could be taken to be a Poisson random
variable with parameter α = λt. Indeed, we shall frequently write the probability mass function of
a Poisson random variable as
pX(n) = (λt)n
n! e−λt for n = 0, 1, 2, . . . ;
otherwise pX(n) = 0.
Example 3.6 The number of students who visit a professor during ofﬁce hours is a Poisson random
variable with an average of λ = 12 students per hour.
(a) Compute the probability that no students arrive in an interval of 15 minutes.
Using λ = 12 and t = 0.25, λt = 3 and the probability of n = 0, 1, 2, . . . students arriving is
pX(n) = 3n
n!e−3.
Therefore the probability that no students (n = 0) arrive in a 15 minute period is
pX(0) = e−3 = 0.04979.
(b) Compute the probability that not more than three students arrive in an interval of 30 minutes.
In this case, the time interval is 1/2 hour, so λt = 6. To compute the probability that not more
than three students arrive in 30 minutes we need to add the probabilities of zero, one, two,
and three students arriving. These four probabilities are given, respectively, by
pX(0) = e−6 = 0.002479,
pX(1) = 6e−6 = 0.014873,
pX(2) = 62e−6/2 = 0.044618,
pX(3) = 63e−6/6 = 0.089235,
and their sum is given by pX(0) + pX(1) + pX(2) + pX(3) = 0.1512.
For the most part, in this section on the probability mass function of a discrete random variable,
our concern has been with determining the probability that the random variable X has a certain value

46
Random Variables and Distribution Functions
x: pX(x) = Prob{X = x}. In the next section, we consider the probability that the random variable
has any one of the values in some speciﬁed set of values. The last part of the previous example has
already set the stage.
3.3 The Cumulative Distribution Function
For any discrete random variable X, there is a denumerable number of reals that are assigned to X.
These reals are denoted by the set {x1, x2, . . .} and it is natural to arrange these reals in increasing
order of magnitude. For example, these were arranged from 2 through 12 for the random variable
X and from 0 through 3 for the random variable R as has been shown in Tables 3.1 and 3.2,
respectively. Given this situation, we may now wish to ﬁnd the probability that a random variable
X assumes a value that is less than or equal to a given xi, i.e., the probability Prob{X ≤xi}. Notice
that this corresponds to the probability of an event, the event that consists of all elements ω of the
sample space for which X(ω) = xk with xk ≤xi.
Example 3.7 Consider once again, the random variable X, deﬁned as the sum of spots obtained
in simultaneously throwing two fair dice. We previously examined the probability mass function
for X. Perhaps now we are interested in knowing the probability that the random variable X has a
value less than 6. Let us denote this event as A and observe that its probability is just the sum of the
probabilities
Prob{A} = Prob{X = 2} + Prob{X = 3} + Prob{X = 4} + Prob{X = 5},
which can be denoted more simply by Prob{X < 6}. Hence we have
Prob{X < 6} = 1
36 + 2
36 + 3
36 + 4
36 = 10
36.
Observe that the event A is deﬁned as the subset consisting of the ten outcomes:
A = {(1, 1), (1, 2), (2, 1), (1, 3), (2, 2), (3, 1), (1, 4), (2, 3), (3, 2), (4, 1)}.
We can go further. Suppose we would like to know the probability that the random variable X
has a value that lies between 5 and 7 inclusive. In this case, we may write
Prob{A} = Prob{X = 5} + Prob{X = 6} + Prob{X = 7},
which again may be written more succinctly as Prob{5 ≤X ≤7}, and we have
Prob{5 ≤X ≤7} = 4
36 + 5
36 + 6
36 = 15
36.
Now consider a subset S that contains an arbitrary collection of the values that the random
variable X can assume. Then, the set {ω|X(ω) ∈S} contains all the outcomes for which the random
variable X has a value in S. By deﬁnition, this constitutes an event which is denoted by [X ∈S] and
we have
[X ∈S] = {ω|X(ω) ∈S} =

xk∈S
{ω|X(ω) = xk}.
If pX(x) is the probability mass function for the discrete random variable X, then
Prob{X ∈S} =

xk∈S
pX(xk).

3.3 The Cumulative Distribution Function
47
Example 3.8 Let us return to the example of two dice and the random variable X that describes the
sum of spots obtained and let S be the set that contains only the two values x1 = 2 and x10 = 11,
i.e., S = {2, 11}. Then {ω|X(ω) = x1} = (1, 1) and {ω|X(ω) = x10} = {(5, 6), (6, 5)}. Hence
[X ∈S] = {(1, 1), (5, 6), (6, 5)} and
Prob{X ∈S} = pX(2) + pX(11) = 1/36 + 2/36 = 1/12.
When the set S contains all possible values within a range of values that X can assume, say
(a, b], then it is more usual to write Prob{X ∈S} as Prob{a < X ≤b} as illustrated previously.
In the particular case when S contains all values that are less than or equal to some speciﬁed value
x, then Prob{X ∈S} is called the cumulative distribution function (CDF) of the random variable
X. This function is denoted by FX(x) and is also referred to as the probability distribution function
(PDF). It is deﬁned for real values of x in the range (−∞< x < ∞). We have
FX(x) ≡Prob{−∞< X ≤x} = Prob{X ≤x}.
Observe that, if X is a discrete random variable, then x does not have to be one of the values that X
can assume; i.e., x need not be one of the xi. For example, when the xi are integers,
FX(x) =

−∞<xi≤⌊x⌋
pX(xi)
where ⌊x⌋is the ﬂoor of x and denotes the largest integer less than or equal to x.
1/6
2/6
3/6
4/6
5/6
1
1
2
3
4
5
6
7
8
9
10
11
12
x
F (x)
X
10/36
Figure 3.5. Cumulative distribution function of X.
To illustrate this, consider Figure 3.5, which shows the cumulative distribution function of the
random variable X, deﬁned as the sum of spots found on two simultaneously thrown dice. It may
be observed from this ﬁgure that for any value of x in the interval [4, 5), for example, FX(x) = 1/6.
This is the probability that the number of spots obtained is 4 or less. If x lies in the interval [5, 6),
then FX(x) = 10/36, the probability that the number of spots is 5 or less. For the random variable R,
which denotes the number of heads obtained in three tosses of a fair coin, the cumulative distribution
function is shown in Figure 3.6. If x lies somewhere in the interval [1, 2), then FR(x) = 1/2 which

48
Random Variables and Distribution Functions
0
1
2
3
4
1/8
2/8
3/8
4/8
5/8
6/8
7/8
1
x
F (x)
R
Figure 3.6. Cumulative distribution function of R.
is the probability of throwing one or zero heads, i.e., the event A = {TTT, HTT, THT, TTH}. In
both of these ﬁgures, the step “risers” have been included simply to show the steplike feature of the
cumulative distribution function of a discrete random variable; the value assumed by the cumulative
distribution function at xi is the value of the top of the riser.
Observe that, in both ﬁgures, the value of the cumulative distribution function is zero for
sufﬁciently small values of x. The function is nondecreasing with increasing values of x and is
equal to 1 for sufﬁciently large values of x. This is due to the fact that the probability that the
random variable will assume a value smaller than the smallest xk must be zero while the probability
that it will have a value less than or equal to the largest xk must be 1. These properties are inherent
in all cumulative distribution functions.
The disjointed steplike structure of the two functions shown is a distinctive feature of discrete
random variables. These examples illustrate the fact that the distribution function of a discrete
random variable is not continuous. They have at least one point at which the approach of the function
from the left and from the right do not meet: the approach from the right is strictly greater than
the approach from the left. In the examples, these are the points xi that may be assumed by the
random variable. For continuous random variables, the monotonic nondecreasing property of the
cumulative distribution function is maintained, but the increase is continuous and not steplike.
The cumulative distribution function of a continuous random variable is a continuous function of x
for all −∞< x < ∞.
Whereas the probability mass function is deﬁned only for discrete random variables, the
cumulative distribution function applies to both discrete and continuous random variables. The
following deﬁnition covers both cases:
Deﬁnition 3.3.1 (Cumulative distribution function) The cumulative distribution function FX of
a random variable X is deﬁned to be the function
FX(x) = Prob{X ≤x} for −∞< x < ∞.
Simply put, it is just the probability that the random variable X does not exceed a given real
number x.
Example 3.9 The random variable X whose distribution function has the value zero when x < 0,
the value one when x ≥1, and which increases uniformly from the value zero at x = 0 to the value

3.3 The Cumulative Distribution Function
49
1
1
F(x)
x
Figure 3.7. Cumulative distribution function of the uniform continuous random variable.
one at x = 1 is called the uniform continuous random variable on the interval [0, 1]. Its distribution
function is usually written as
F(x) =
⎧
⎨
⎩
0,
x < 0,
x,
0 ≤x < 1,
1,
x ≥1,
and is illustrated in Figure 3.7.
Observe that the cumulative distribution function illustrated in Figure 3.7 has a derivative
everywhere except at x = 0 and x = 1. The distribution functions of continuous random variables
that we shall consider will be absolutely continuous, meaning that they are continuous and their
derivatives exist everywhere, except possibly at a ﬁnite number of points.
Example 3.10 Consider a second continuous random variable, M, deﬁned as
FM(x) =
1 −e−x
if x ≥0,
0
otherwise.
A random variable having this distribution function is said to be an (negative) exponential random
variable with parameter value equal to one. A graph of this function is shown in Figure 3.8.
0
1
2
3
4
5
0
0.5
1
λ = 1.0
Figure 3.8. CDF of an exponential random variable: FM(x) = 1 −e−x for x ≥0.

50
Random Variables and Distribution Functions
Distribution functions have a number of mathematical properties, the most important of which
we denote below. The reader may wish to verify that these properties are indeed evident in Figures
3.5, 3.6, 3.7, and 3.8.
• 0 ≤F(x) ≤1 for −∞< x < ∞, since F(x) is a probability.
• limx→−∞F(x) = 0 and limx→∞F(x) = 1.
• If the random variable X has a ﬁnite image, then
F(x) = 0 for all x sufﬁciently small, and
F(x) = 1 for all x sufﬁciently large.
• F(x) is a nondecreasing function of x. Since (−∞, x1] ∈(−∞, x2] for x1 ≤x2, it follows
that F(x1) ≤F(x2) for x1 ≤x2.
• F(x) is right continuous. This means that for any x and any decreasing sequence xk with
k ≥1 that converges to x, we must have limk→∞F(xk) = F(x).
Notice that for any random variable X, discrete or continuous, we have
Prob{a < X ≤b} = Prob{X ≤b} −Prob{X ≤a} = F(b) −F(a).
Now let a →b and observe what happens. We obtain
Prob{X = b} = F(b) −F(b−)
where F(b−) is the limit of F(x) from the left, at the point b. There are now two possibilities:
• F(b−) = F(b). In this case F(x) is continuous at the point b and thus Prob{X = b} = 0.
The event X = b has zero probability even though that event may occur.
• F(b−) ̸= F(b). Here F(x) has a discontinuity (a jump) at the point b and Prob{X = b} > 0.
As we have seen, the cumulative distribution function of a discrete random variable grows only by
jumps, while the cumulative distribution function of a continuous random variable has no jumps,
but grows continuously. When X is a continuous random variable, the probability that X has any
given value must be zero. All we can do is assign a positive probability that X falls into some ﬁnite
interval, such as [a, b], on the real axis.
For a discrete random variable, F(x) has a staircaselike appearance. At each of the points xi, it
has a positive jump equal to pX(xi). Between these points, i.e., in the interval [xi, xi+1), it has a
constant value. In other words, as shown in Figure 3.9, we must have
F(x) = F(xi) for xi ≤x < xi+1,
F(xi+1) = F(xi) + pX(xi+1).
xi
xi+1
p  (x    )
i+1
X
F  (x  )
X
i
Figure 3.9. Cumulative distribution function of a discrete random variable.
Finally, we point out that any continuous, monotonically nondecreasing function 	X(x) for which
lim
x→−∞	X(x) = 0 and
lim
x→∞	X(x) = 1

3.4 The Probability Density Function for a Continuous Random Variable
51
can serve as the distribution function of a continuous random variable X. We previously noted
that the probability mass function provides a complete speciﬁcation of a discrete random variable.
The same can be said of a cumulative distribution function; that it completely speciﬁes a random
variable. The two distributions are closely related. Given one, the other can be easily determined.
Example 3.11 Let X be a discrete random variable with probability distribution function given by
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
x ∈(−∞, −2),
1/10,
x ∈[ −2, −1),
3/10,
x ∈[ −1, 1),
6/10,
x ∈[ 1, 2),
1,
x ∈[ 2, ∞).
Let us compute the probability mass function of this random variable. Observe ﬁrst that this random
variable has discontinuities at the points x = −2, −1, 1, and 2. These are the values that X can
assume and are the only points at which pX(x) can have a nonzero value. By subtracting the value
of the function just prior to one of these points from the value of the function at the point itself, we
obtain the following probability mass function of X:
PX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1/10,
x = −2,
2/10,
x = −1,
3/10,
x = 1,
4/10,
x = 2,
0
otherwise.
3.4 The Probability Density Function for a Continuous Random Variable
For a continuous random variable X, the function deﬁned as
f (x) = dF(x)/dx
is called the probability density function of X. This function fulﬁlls much the same role for a
continuous random variable as the role played by the probability mass function of a discrete random
variable. In this case, the summation used earlier is replaced by integration. Observe that
f (x) = lim

x→0
F(x + 
x) −F(x)

x
= lim

x→0
Prob{x < X ≤x + 
x}

x
.
Thus, when 
x is small, we have
f (x)
x ≈Prob{x < X ≤x + 
x},
i.e., f (x)
x is approximately equal to the probability that X lies in a small interval, (x, x + 
x].
As 
x tends to the inﬁnitesimally small dx, we may write
Prob{x < X ≤x + dx} = f (x)dx.
It must follow that f (x) ≥0 for all x.
If we know the density function of a continuous random variable, we can obtain its cumulative
distribution function by integration. We have
FX(x) = Prob{X ≤x} =
 x
−∞
fX(t)dt
for −∞< x < ∞.

52
Random Variables and Distribution Functions
Since F(∞) = 1, it follows that
 ∞
−∞
fX(x)dx = 1.
From these considerations, we may state that a function f (x) is the probability density function for
some continuous random variable if and only if it satisﬁes the two properties:
f (x) ≥0 for all x ∈ℜ
and
 ∞
−∞
f (x)dx = 1.
The probability that X lies in the interval (a, b] is obtained from
Prob{X ∈(a, b]} = Prob{a < X ≤b} = Prob{X ≤b} −Prob{X ≤a}
=
 b
−∞
fX(t)dt −
 a
−∞
fX(t)dt =
 b
a
fX(t)dt.
Thus, the probability that the random variable lies in a given interval on the real axis is equal to the
area under the probability density curve on this interval.
Example 3.12 The density function of the continuous random variable M, whose cumulative
distribution function is given by
FM(x) =

1 −e−x
if x ≥0,
0
otherwise,
is
fM(x) =

e−x
if x ≥0,
0
otherwise,
and is shown in Figure 3.10. We have
FM(x) =
 x
−∞
fM(t)dt =
 x
0
e−tdt = −e−tx
0 = −e−x −(−1) = 1 −e−x
and
 ∞
−∞fM(x)dx =
 ∞
0 e−xdx = 1.
0
1
2
3
4
5
0
0.5
1
λ = 1.0
Figure 3.10. Density function for an exponentially distributed random variable.

3.5 Functions of a Random Variable
53
Notice that the values of the density function are not probabilities and may have values greater
than 1 at some point. This is illustrated in the following example.
Example 3.13 The density function of the continuous random variable with cumulative distribution
function
F(x) =
⎧
⎨
⎩
0,
x < 0,
4x,
0 ≤x < 0.25,
1,
x ≥0.25,
is given by
f (x) =
⎧
⎨
⎩
0,
x < 0,
4,
0 ≤x ≤0.25,
0,
x > 0.25.
We mentioned earlier that the probability of a continuous random variable having a given
value, say c, must be zero. This now becomes evident since the probability of the event
[X = c] ={ω|X(ω) = c} is given by
Prob{X = c} = Prob{c ≤X ≤c} =
 c
c
fX(t)dt = 0.
The set {ω|X(ω) = c} is not necessarily empty; it is just that the probability assigned to the event
ω is zero. As a ﬁnal remark, notice that, since the probability assigned to individual points must be
zero, it follows that it makes little difference whether end points are included or not into intervals.
We have
Prob{a ≤x ≤b} = Prob{a < x ≤b} = Prob{a ≤x < b}
= Prob{a < x < b} =
 b
a
fX(x)dx = FX(b) −FX(a).
3.5 Functions of a Random Variable
We have seen that a random variable X is a function that assigns values in ℜto each outcome in a
sample space. Given a random variable X we may deﬁne other random variables that are functions
of X. For example, the random variable Y deﬁned as being equal to 2X takes each outcome in the
sample space and assigns it a real that is equal to twice the value that X assigns to it. The random
variable Y = X2 assigns an outcome that is the square of the value that X assigns to it, and so on.
In general, if a random variable X assigns the value x to an outcome and if g(X) is a function of
X, then Y = g(X) is a random variable and assigns the value g(x) to that outcome. The random
variable Y is said to be a derived random variable.
Consider now the case of a discrete random variable X whose probability mass function is pX(x).
Given some function g(X) of X we would like to create the probability mass function of the random
variable Y = g(X), i.e., we seek to ﬁnd pY(y). We shall proceed by means of some examples.
Example 3.14 Let X be a discrete random variable with probability mass function given by
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1/10,
x = 1,
2/10,
x = 2,
3/10,
x = 3,
4/10,
x = 4,
0
otherwise.

54
Random Variables and Distribution Functions
With this random variable, each outcome in the sample space is mapped into one of the four real
numbers 1, 2, 3, or 4. Consider now the random variable Y = 2X. In this case each outcome is
mapped into one of the integers 2, 4, 6, or 8. But what about the probability mass function of Y? It
is evident that, if the probability that X maps an outcome into 1 is 1/10, then the probability that Y
maps an outcome into 2 must also be 1/10; if X maps an outcome into 2 with probability 2/10, then
this must also be the probability that Y maps an outcome into 4, and so on. In this speciﬁc example,
we have
pY(y) = Prob{Y = g(x)} = Prob{X = x} = pX(x).
However, as the next example shows, this is not always the case.
Example 3.15 Let X be a discrete random variable with probability mass function given by
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1/10,
x = −2,
2/10,
x = −1,
3/10,
x = 1,
4/10,
x = 2,
0
otherwise.
With this random variable X, each outcome in the sample space is mapped into one of the four real
numbers −2, −1, 1, or 2. Now consider the derived random variable Y = X2. In this case, each
outcome is mapped into 1 or 4. If X maps an outcome into either −1 or 1, then Y will map that
outcome into 1; while if X maps an outcome into −2 or 2, then Y will map that outcome into 4.
The random variable Y has the following probability mass function, which is not the same as that
of X:
pY(y) =
⎧
⎨
⎩
2/10 + 3/10,
y = 1,
1/10 + 4/10,
y = 4,
0
otherwise.
These examples illustrate the rule that the probability mass function of a random variable Y,
which is a function of a random variable X, is equal to the probability mass function of X, if
g(x1) ̸= g(x2) when x1 ̸= x2. Otherwise the probability mass function of Y is obtained from the
general relation (general in the sense that it also covers the above case):
pY(y) =

x: g(x)=y
pX(x).
Here the summation is over all x for which g(x) = y. One ﬁnal example of the applicability of this
rule is now given.
Example 3.16 Let X be a discrete random variable that is deﬁned on the integers in the interval
[−3, 4]. Let the probability mass function of X be as follows.
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0.05,
x ∈{−3, 4},
0.10,
x ∈{−2, 3},
0.15,
x ∈{−1, 2},
0.20,
x ∈{0, 1},
0
otherwise.
We wish to ﬁnd the probability mass function of the random variable Y deﬁned as Y = X2 −|X|.
Since the possible values of X are given by [−3, −2, −1, 0, 1, 2, 3, 4], it follows that the
corresponding values of Y = X2 −|X| are [6, 2, 0, 0, 0, 2, 6, 12] and hence the range of Y is

3.5 Functions of a Random Variable
55
[0, 2, 6, 12]. This allows us to compute the probability mass function of Y as
pY(y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0.15 + 0.20 + 0.20 = 0.55
if y = 0
(g(x) = y for x = −1, 0, 1),
0.10 + 0.15 = 0.25
if y = 2
(g(x) = y for x = −2, 2),
0.05 + 0.10 = 0.15
if y = 6
(g(x) = y for x = −3, 3),
0.05 = 0.05
if y = 12
(g(x) = y for x = 4),
0
otherwise.
We now consider functions of a continuous random variable X, and again we shall ﬁrst provide
some illustrative examples.
Example 3.17 Let X be a continuous random variable that is uniformly distributed in the interval
[−1, +1]. Let Y = αX + β, with α > 0, be a derived random variable. Observe that the probability
density function of X is given by
fX(x) =

1/2,
−1 ≤x ≤1,
0
otherwise,
since X is uniformly distributed and the area of the density function between x = −1 and x = 1
must be equal to 1. This density function has the shape of a rectangle with base [−1, +1] and height
1/2. The cumulative distribution function of X is given by
FX(x) =
⎧
⎨
⎩
0,
x ≤−1,
(x + 1)/2,
−1 ≤x ≤1,
1,
x ≥1.
Now consider the random variable Y. The effect of multiplying X by α is to multiply the
interval [−1, +1] by α. This becomes the interval over which Y has nonzero values. Since Y is
also uniformly distributed, its density function will be rectangular in shape, but its height must be
1/α times the height of fX(x) so that the area beneath fY(y) between −α and +α remains equal to
1. The effect of adding β is to shift the interval over which nonzero values of Y occur. Taking these
together, we obtain
fY(y) =

1/2α,
β −α ≤y ≤β + α,
0
otherwise.
The cumulative distribution of Y is given as
FY(y) =
⎧
⎨
⎩
0,
y ≤β −α,
(y + α −β)/2α,
β −α ≤y ≤β + α,
1,
y ≥β + α.
Observing that the interval β −α ≤y ≤β + α may be written as
−1 ≤y −β
α
≤1,
the probability density function and the cumulative distribution of Y may be written in terms of
those same functions of X. We have
fY(y) = 1
α fX
 y −β
α

and
FY(y) = FX
 y −β
α

.
These relations hold in general for any continuous random variable X and a random variable Y
derived from X by multiplication with a positive constant α and shifted by an amount β. In the
absence of a shift β, these relations simplify to
fY(y) = 1
α fX(y/α) and
FY(y) = FX(y/α)
as the following example shows.

56
Random Variables and Distribution Functions
Example 3.18 Let X be a continuous random variable having a wedge-shaped probability density
function on the interval [0, 1]. Speciﬁcally, let
fX(x) =

2x,
0 ≤x ≤1,
0
otherwise,
and
FX(x) =
⎧
⎨
⎩
0,
x ≤0,
x2,
0 ≤x ≤1,
1,
x ≥1,
where FX(x) has been derived from fX(x) by integration. The probability density function and
cumulative probability function of the derived random variable Y = 5X are given by
fY(y) =
2y/25,
0 ≤y ≤5,
0
otherwise,
and
FY(y) =
⎧
⎨
⎩
0,
y ≤0,
y2/25,
0 ≤y ≤5,
1,
y ≥5.
For Y = αX, these functions are given by
fY(y) =

2y/α2,
0 ≤y ≤α,
0
otherwise,
and
FY(y) =
⎧
⎨
⎩
0,
y ≤0,
y2/α2,
0 ≤y ≤α,
1,
y ≥α,
and again
fY(y) = 1
α fX(y/α) and
FY(y) = FX(y/α).
To conclude this section, we consider two examples of derived random variables that are more
complex than those obtained by scalar multiplication and shift.
Example 3.19 The cumulative distribution function of an exponential random variable X with
parameter λ is given by
Prob{X ≤x} =
1 −e−λx,
x ≥0,
0
otherwise.
Let us compute the cumulative distribution function and the density function of a random variable
Y deﬁned as Y = X2. We have
Prob{Y ≤y} = Prob{X2 ≤y} = Prob{X ≤√y} = 1 −e−λ√y
for y ≥0.
We may now compute the density function of Y by differentiation. We obtain
fY(y) = d
dy Prob{Y ≤y} = λe−λy1/2 × 1
2 y−1/2 =
λ
2√y e−λ√y
for y > 0.
In simulation experiments it is frequently the case that one has access to sequences of random
numbers that are uniformly distributed over the interval (0, 1), but what is really required are random
numbers from a nonuniform distribution. In particular, in queuing scenarios, random numbers that
are distributed according to a (negative) exponential distribution are often needed. This last example
shows how such numbers may be obtained.
Example 3.20 Let X be a random variable that is uniformly distributed on (0, 1) and deﬁne a
derived random variable Y as Y = −ln(1 −X)/λ. The cumulative distribution function of Y is
given by
FY(y) = Prob{Y ≤y} = Prob{−ln(1 −X)/λ ≤y}
= Prob{ln(1 −X) ≥−λy} = Prob{1 −X ≥e−λy}
= Prob{X ≤1 −e−λy} = FX(1 −e−λy).
Since X is uniformly distributed on the interval (0, 1), we must have FX(x) = x on this interval.
In particular FX(1 −e−λy) = 1 −e−λy and therefore FY(y) = 1 −e−λy for 0 ≤1 −e−λy ≤1.

3.5 Functions of a Random Variable
57
Since all nonnegative values of y satisfy these bounds, we have
FY(y) =

0,
y ≤0,
1 −e−λy,
y ≥0.
This deﬁnes the exponential distribution with parameter λ.
A function g(x) of x is said to be an increasing function of x if for x1 ≤x2, g(x1) ≤g(x2).
Similarly, it is said to be a decreasing function of x if for x1 ≤x2, g(x1) ≥g(x2). The next example
shows that, when the random variable Y is an increasing or decreasing function of X, and the density
function of X is provided, then it is not necessary to ﬁrst ﬁnd the cumulative distributions of X and
then Y to compute the density function of Y.
Example 3.21 Let the probability density function of a random variable X be given by
fX(x) =
α/x5,
1 ≤x < ∞,
0
otherwise.
Let a new random variable be deﬁned as Y = 1/X. We seek the probability density function of Y.
We shall ﬁrst answer the question using the standard approach. Our ﬁrst task is to compute the
value of α. Since we must have
1 =
 ∞
−∞
fX(x) =
 ∞
1
α
x5 = −α x−4
4

∞
1
= α
4 ,
we must have α = 4. The cumulative distribution function of X may now be computed as
FX(x) =
 x
1
fX(t)dt =
 x
1
4
t5 dt = −t−4x
1 = 1 −1
x4 ,
x ≥1.
Now using probabilities, and observing that the range of Y is (0, 1], we can compute the cumulative
distribution function of Y as
Prob{Y ≤y} = Prob{1/X ≤y} = Prob{X ≥1/y} = 1 −Prob{X < 1/y} = 1 −FX(1/y) = y4
for 0 < Y ≤1. Finally, we may now compute the probability density function of Y by taking
derivatives. We obtain
fY(y) = d
dy FY(y) = d
dy y4 = 4y3,
0 < y ≤1.
In examples like this last one, in which Y is a decreasing function of X (as x gets larger, y = 1/x
gets smaller), an easier approach may be taken. Indeed, it may be shown that if Y = g(X) is an
increasing or a decreasing function of X then
fY(y) = fX(x)

dx
dy
 ,
(3.3)
where, in this expression, x is the value obtained when y = g(x) is solved for x.
To show that Equation (3.3) is true for increasing functions, consider the following. Let Y = g(X)
be an increasing function of X. Then, because g(X) is an increasing function
FY(y) = Prob{Y ≤y} = Prob{g(X) ≤g(x)} = Prob{X ≤x} = FX(x).
It now follows that
fY(y) = d
dy FY(y) = d
dy FX(x) = F′
X(x)dx
dy = fX(x)dx
dy .

58
Random Variables and Distribution Functions
In the example, y = 1/x, so x = 1/y and substituting into Equation (3.3), we have
fY(y) =
4
(1/y)5

−1
y2
 = 4y3
as before. We leave the proof of the case when Y is a decreasing function of X as an exercise.
3.6 Conditioned Random Variables
We saw earlier that an event A may be conditioned by a different event B and that the probability
assigned to A may change, stay the same, or even become zero as a result of knowing that the
event B occurs. We wrote this as Prob{A|B}, the probability of event A given the event B. Since
a random variable X deﬁnes events on a sample space, it follows that the probabilities assigned
to random variables may also change upon knowing that a certain event B has occurred. The event
[X = x] contains all the outcomes which are mapped by the random variable X onto the real number
x and its probability, Prob{X = x}, is equal to the sum of the probabilities of these outcomes.
Knowing that an event B has occurred may alter Prob{X = x} for all possible values of x. The
conditional probabilities Prob{X = x|B} are deﬁned whenever Prob{B} > 0. When the random
variable X is discrete, Prob{X = x|B} is called the conditional probability mass function of X and
is denoted by pX|B(x). From our prior deﬁnition of conditional probability for two events, we may
write
pX|B(x) = Prob{[X = x] ∩B}
Prob{B}
.
In many practical examples, the event [X = x] is contained entirely within the event B, or else the
intersect of [X = x] and B is the null event. In the ﬁrst case we have
pX|B(x) =
pX(x)
Prob{B}
if
[X = x] ⊂B
(3.4)
while in the second [X = x] ∩B = φ and hence pX|B(x) = 0. We now give three examples.
Example 3.22 Let X be the random variable that counts the number of spots obtained when two
fair dice are thrown. The probability mass function for X is
xi
2
3
4
5
6
7
8
9
10
11
12
pX(xi)
1/36
2/36
3/36
4/36
5/36
6/36
5/36
4/36
3/36
2/36
1/36
Let B be the event that the throw gives an even number of spots on one of the dice and an odd
number of spots on the other. It then follows that the event B contains 18 outcomes. Each outcome
is a pair with the property that the ﬁrst can be any of six numbers, but the second must be one of
the three even numbers (if the ﬁrst is odd) or one of three odd numbers (if the ﬁrst is even). Since
all outcomes are equally probable, we have Prob{B} = 1/2. Given this event, we now compute the
conditional probability mass function, pX|B. Since the intersection of B and [X = x] when x is an
even number is empty, it immediately follows that the probability of the sum being an even number
is zero. For permissible odd values of x, the event [X = x] is entirely contained within the event
B and we may use Equation (3.4). In summary, the conditional probability mass function of X is
obtained as
pX|B(x) =
pX(x)/Prob{B},
x = 3, 5, 7, 9, 11,
0
otherwise,
and is presented in tabular form as follows.

3.6 Conditioned Random Variables
59
xi
2
3
4
5
6
7
8
9
10
11
12
pX|B(xi)
0
2/18
0
4/18
0
6/18
0
4/18
0
2/18
0
Example 3.23 Let X be the same random variable as before, but now let B be the event that the
sum obtained is strictly greater than 8, i.e., B = {X > 8}. There are a total of ten outcomes that give
a sum greater than 8, which means that Prob{X > 8} = 10/36. The conditional probability mass
function, now written as pX|X>8(x), may be obtained from Equation (3.4) and we ﬁnd
xi
2
3
4
5
6
7
8
9
10
11
12
pX|X>8(xi)
0
0
0
0
0
0
0
4/10
3/10
2/10
1/10
If Bi, i = 1, 2, . . . , n, is a set of mutually exclusive and collectively exhaustive events, i.e., an
event space, then
pX(x) =
n

i=1
pX|Bi(x)Prob{Bi}.
This result follows immediately from applying the law of total probability to the event [X = x].
Example 3.24 Let X be a random variable that denotes the duration in months that it takes a newly
graduated computer science student to ﬁnd suitable employment. Statistics indicate that the number
of months, n is distributed according to the formula α(1 −α)n−1, with α = 0.75 for students whose
GPA exceeds 3.5, α = 0.6 for students whose GPA is higher that 2.8 but does not exceed 3.5, and
α = 0.2 for all other students. This leads to the following conditional probability mass functions, in
which we indicate student performance by the letters H, M, or L depending on whether their GPA
is high, medium, or low, respectively:
pX|H(n) = 0.75(0.25)n−1 for n = 1, 2, . . . ,
pX|M(n) = 0.60(0.40)n−1 for n = 1, 2, . . . ,
pX|L(n) = 0.20(0.80)n−1 for n = 1, 2, . . . .
If we know that 20% of the students have GPAs in excess of 3.5 and 35% have GPAs less than 2.8,
then the probability mass function of X is given by
pX(n) = 0.2 × 0.75(0.25)n−1 + 0.45 × 0.60(0.40)n−1 + 0.35 × 0.20(0.80)n−1
for n = 1, 2, . . . .
These concepts carry over in a natural fashion to random variables that are continuous. For a
continuous random variable X, and a conditioning event B, we may write
fX|B(x)dx = Prob{x < X ≤x + dx|B} = Prob{[x < X ≤x + dx] ∩B}
Prob{B}
.
Thus
fX|B(x) =
⎧
⎨
⎩
fX(x)/Prob{B}
if [X = x] ⊂B,
0
if [X = x] ∩B = φ.
Example 3.25 Let X be a continuous random variable with probability density function given by
fX(x) =

(1/2)e−x/2
if x ≥0,
0
otherwise.
Let B be the event that X < 1 and we wish to ﬁnd fX|X<1(x). We ﬁrst compute Prob{X < 1} as
Prob{X < 1} =
 1
0
(1/2)e−x/2dx = −e−x/21
0 = 1 −e−1/2.

60
Random Variables and Distribution Functions
The conditional probability density function of X is then given by
fX|X<1(x) =
 fX(x)/Prob{X < 1}
for 0 ≤x < 1,
0
otherwise,
=
(1/2)e−x/2/(1 −e−1/2)
for 0 ≤x < 1,
0
otherwise.
The conditional cumulative distribution function, FX|B(x|B), of a random variable X, given that
the event B occurs, is deﬁned as
FX|B(x|B) = Prob{X ≤x, B}
Prob{B}
,
where (X ≤x, B) is the intersection of the events [X ≤x] and B. Note that we must have
FX|B(−∞|B) = 0 and FX|B(∞|B) = 1.
Also,
Prob{x1 < X ≤x2|B} = FX|B(x2|B) −FX|B(x1|B) = Prob{[x1 < X ≤x2], B}
Prob{B}
.
The conditional density function may be obtained as the derivative:
fX|B(x|B) = d
dx FX|B(x|B),
which must be nonnegative and have area equal to 1.
3.7 Exercises
Exercise 3.1.1 A fair die is thrown once. Alice wins $2.00 if one, two, or three spots appear and $4.00 if four
or ﬁve spots appear, but pays Bob $6.00 if six spots appear. Let W be the random variable that represents the
amount Alice wins or loses. What is the domain and range of W? Give the partition of the state space induced
by this random variable. What is the set A4?
Exercise 3.1.2 A four-sided die (a tetrahedron having the numbers 1, 2, 3, and 4 printed on it, one per side) is
thrown twice. Let M be the random variable that denotes the maximum number obtained on the two throws.
What is the domain and range of M? Give the partition of the state space induced by this random variable.
What is the set A3?
Exercise 3.1.3 A survey reveals that 20% of students at Tobacco Road High School smoke while 15% drink
beer, both considered to be bad habits. Let X be the random variable that denotes the number of these bad
habits indulged by a randomly chosen student. What is the domain and range of X? Describe the partition of
the state space induced by this random variable.
Exercise 3.2.1 Let X be a random variable whose probability mass function is given as
pX(x) =
αx2,
x = 1, 2, 3, 4,
0
otherwise.
(a) What is the value of α?
(b) Compute Prob{X = 4}.
(b) Compute Prob{X ≤2}.

3.7 Exercises
61
Exercise 3.2.2 Let X be a random variable whose probability mass function is given as
pX(x) =
α/x,
x = 1, 2, 3, 4,
0
otherwise.
(a) What is the value of α?
(b) Compute Prob{X is odd}.
(c) Compute Prob{X > 2}.
Exercise 3.2.3 Balls are drawn from an urn containing two white balls and ﬁve black balls until a white ball
appears. Let X be the random variable that denotes the number of black balls drawn before a white ball appears.
What is the domain and range of X and what is the partition induced on the sample space by X? Give a table
of the probability mass function of X.
Exercise 3.2.4 One of the enclosures in a wildlife park contains a herd of sixteen elephants, six rhinos, and ten
hippos. Off in an isolated corner of the enclosure, four of these animals (maybe young males?) have become
agitated and are menacing each other. Construct a table of the probability mass function of the random variable
that counts the number of tusks in this group of four. For the purposes of answering this question, you should
assume that the four in question are equally likely to come from any of the 32 animals in the enclosure.
Assume that elephants have two tusks, rhinos one, and hippos none.
Exercise 3.2.5 An urn contains seven white balls and ﬁve black ones. Suppose n balls are chosen at random.
Let the random variable X denote the number of white balls in the sample. What is the probability mass
function of X if the n balls are chosen (a) without replacement? (b) with replacement?
Exercise 3.3.1 A discrete random variable X takes the value 1 if the number 6 appears on a single throw of a
fair die and the value 0 otherwise. Sketch the probability mass function and the probability distribution function
of X.
Exercise 3.3.2 The cumulative distribution function of a discrete random variable X is given as
FX(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
x < 0,
1/4,
0 ≤x < 1,
1/2,
1 ≤x < 2,
1,
x ≥2.
Find the probability mass function of X.
Exercise 3.3.3 The cumulative distribution function of a discrete random variable N is given as
FN(x) = 1 −1
2n
if x ∈[n, n + 1) for n = 1, 2, . . .
and has the value 0 if x < 1.
(a) What is the probability mass function of N?
(b) Compute Prob{4 ≤N ≤10}.
Exercise 3.3.4 A bag contains two fair coins and a third coin which is biased: the probability of tossing a head
on this third coin is 3/4. A coin is pulled at random and tossed three times. Let X be the random variable
that counts the number of heads obtained in these three tosses. Give the probability mass function and the
cumulative density function of X.
Exercise 3.3.5 Let X be a continuous random variable whose cumulative distribution function is
FX(x) =
⎧
⎨
⎩
0,
x < 0,
x/5,
0 ≤x ≤5,
1,
x > 5.
Compute the following probabilities:
(a) Prob{X ≤1}.
(b) Prob{X > 3}.
(c) Prob{2 < X ≤4}.

62
Random Variables and Distribution Functions
Exercise 3.4.1 The cumulative distribution function of a continuous random variable X is given by
FX(x) =
⎧
⎨
⎩
0,
x < 0,
x2/4,
0 ≤x ≤2,
1,
x > 2.
Find the probability density function of X.
Exercise 3.4.2 Let X be a continuous random variable whose probability density function is given by
fX(x) =
α(1 + x)−3,
x > 0,
0
otherwise.
Find α and Prob{0.25 ≤x ≤0.5}.
Exercise 3.4.3 The cumulative distribution function of a continuous random variable E is given by
FE(x) =

0,
x < 0,
1 −e−μx −μxe−μx,
0 ≤x < ∞,
and μ > 0. Find the probability density function of E.
Exercise 3.4.4 Let the probability density function of a continuous random variable X be given by
fX(x) =
1/4,
−2 ≤x ≤2,
0
otherwise.
Find the cumulative distribution function of X.
Exercise 3.4.5 The probability density function for a continuous “Rayleigh” random variable X is given by
fX(x) =

α2xe−α2x2/2,
x > 0,
0
otherwise.
Find the cumulative distribution of X.
Exercise 3.4.6 Let f1(x), f2(x), . . . , fn(x) be a set of n probability density functions and let p1, p2, . . . , pn
be a set of probabilities for which n
i=1 pi = 1. Prove that n
i=1 pi fi(x) is a probability density function.
Exercise 3.5.1 The cumulative distribution function of a discrete random variable X is given as
FX(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
x < 0,
1/4,
0 ≤x < 1,
1/2,
1 ≤x < 2,
1,
x ≥2.
Find the probability mass function and the cumulative distribution function of Y = X 2.
Exercise 3.5.2 A discrete random variable X assumes each of the values of the set {−10, −9, . . . , 9, 10} with
equal probability. In other words, X is a discrete integer-valued random variable that is uniformly distributed
on the interval [−10, 10]. Compute the following probabilities:
Prob{4X ≤2},
Prob{4X + 4 ≤2},
Prob{X 2 −X ≤3},
Prob{|X −2| ≤2}.
Exercise 3.5.3 Homeowners in whose homes wildlife creatures take refuge call “Critter Control” to rid them
of these “pests.” Critter Control charges $25 for each animal trapped and disposed of as well as a ﬂat fee of
$45 per visit. Experience has shown that the distribution of trapped animals found per visit is as follows.
xi
0
1
2
3
4
pX(xi)
0.5
0.25
0.15
0.05
0.05
Find the probability mass function of the amount of money Critter Control collects per visit.

3.7 Exercises
63
Exercise 3.5.4 Let the probability distribution function of a continuous random variable X be given by
FX(x) =
1 −e−2x,
0 < x < ∞,
0
otherwise.
Find the cumulative distribution function of Y = eX.
Exercise 3.5.5 Show that if the derived random variable Y = g(X) is a decreasing function of a random
variable X, then
fY(y) = fX(x)




dx
dy




 ,
where, in this expression, x is the value obtained when y = g(x) is solved for x.
Exercise 3.6.1 The probability mass function of a discrete integer-valued random variable is
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1/4,
x = −2,
1/8,
x = −1,
1/8,
x = 0,
1/2,
x = 1,
0
otherwise.
Find pX|B(x) where B = [X < 0].
Exercise 3.6.2 The probability mass function of a discrete random variable is given by
pX(x) =
α/x,
x = 1, 2, 3, 4,
0
otherwise.
Find pX|B(x) where B = [X is odd].
Exercise 3.6.3 The density function of a continuous random variable is given by
fX(x) =
1/20,
−10 ≤x ≤10,
0
otherwise.
Find the conditional density functions fX|X<0(x) and fX|X>5(x).
Exercise 3.6.4 Let X be a continuous random variable whose probability density function is given by
fX(x) =
α(1 + x)−3,
x > 0,
0
otherwise.
Find the conditional density function fX|0.25≤X≤0.5(x). Verify that your answer is correct by integrating
fX|0.25≤X≤0.5(x) with respect to x between the limits 0.25 and 0.5 and showing that this gives the value 1.

Chapter 4
Joint and Conditional Distributions
4.1 Joint Distributions
In the previous chapter we discussed individual random variables that were either discrete or
continuous. In this chapter we shall examine the relationships that may exist between two or more
random variables. This is often a more common modeling problem than one that involves a single
random variable. For example, we may be interested in relating the random variable X that measures
the strength of a cell phone signal at some location and the random variable Y that measure the
distance of that location from the nearest coverage tower. Or we may be interested in relating a
random variable X that deﬁnes the arrival process of patients to a doctor’s ofﬁce and the random
variable Y that measures the length of time those customers spend in the waiting room. As a ﬁnal
example, we may be interested in examining a situation in which jobs arrive at a machine shop
in which the machines are subject to failure. A ﬁrst random variable may be used to indicate the
number of jobs present and a second may be used to represent the number of working machines.
Sums, differences, and products of random variables are often of importance, as are relations
such as the maximum and minimum. Consider the execution of a computer program which consists
of different sections whose time to complete depends on the amount and type of data provided. It
follows that the time required to execute the entire program is also data dependent. The execution
time of the ith section may be represented by a random variable Xi. Let the random variable
X represent the time needed to execute the entire program. If the sections are executed one
after the other, then X is given by X = 
i Xi. If the sections are executed in parallel, then
X = max{Xi; i = 1, 2, . . .}.
We begin by deﬁning the joint cumulative distribution function for two random variables and then
consider joint probability mass functions for two discrete random variables and the joint probability
density function for two continuous random variables. These latter two are generally more useful in
actual applications that the joint cumulative distribution function.
4.2 Joint Cumulative Distribution Functions
The joint cumulative distribution function of two random variables X and Y is given by
FX,Y(x, y) = Prob{X ≤x, Y ≤y}; −∞< x < ∞, −∞< y < ∞.
To be a bona ﬁde joint CDF, a function must possess the following properties, similar to those found
in the CDF of a single random variable:
• 0 ≤FX,Y(x, y) ≤1 for −∞< x < ∞; −∞< y < ∞, since FX,Y(x, y) is a probability.
• FX,Y(x, −∞) = FX,Y(−∞, y) = 0,
FX,Y(−∞, −∞) = 0, FX,Y(+∞, +∞) = 1.
• FX,Y must be a nondecreasing function of both x and y, i.e.,
FX,Y(x1, y1) ≤FX,Y(x2, y2) if x1 ≤x2 and y1 ≤y2.
Example 4.1
The cumulative distribution function of a single discrete random variable X, given
in tabular form, is as follows.


66
Joint and Conditional Distributions
and
Prob{X ≤a, c < Y ≤d} = FX,Y(a, d) −FX,Y(a, c).
Use of the joint cumulative distribution function of two random variables to compute the probability
that (X, Y) lies in a rectangle in the plane is not unduly complicated. We may use the result
Prob{a < X ≤b, c < Y ≤d} = FX,Y(b, d) −FX,Y(a, d) −FX,Y(b, c) + FX,Y(a, c),
(4.1)
which requires that the joint cumulative distribution function be evaluated at each of the four points
of the rectangle. Unfortunately, when the required probability is found in a nonrectangular region,
the joint CDF becomes much more difﬁcult to use. This is one of the reasons why joint CDFs are
infrequently used. Nevertheless, Equation (4.1) is a useful special case whose proof constitutes an
enlightening exercise.
We proceed as follows: let A represent the rectangle with corners (a, c), (b, c), (b, d), and
(a, d) (counterclockwise order), i.e., the rectangle within which we seek the probability that X, Y
can be found. Then Prob{A} = Prob{a < X ≤b, c < Y ≤d}. Let B represent the semi-inﬁnite
rectangle that lies to the left of A, and let C represent the semi-inﬁnite rectangle that lies below A,
as illustrated in Figure 4.2. Observe that A, B, and C are events that have no points in common, i.e.,
they are mutually exclusive, and so
Prob{A ∪B ∪C} = Prob{A} + Prob{B} + Prob{C},
and hence
Prob{A} = Prob{A ∪B ∪C} −Prob{B} −Prob{C}.
(b, c)
(b, d)
(a, d)
B
C
A
(a, c)
Figure 4.2. Disjoint areas A, B, and C representing mutually exclusive events.
Furthermore, from our deﬁnition of the joint CDF, we have the following relationships:
Prob{B} = FX,Y(a, d) −FX,Y(a, c),
Prob{C} = FX,Y(b, c) −FX,Y(a, c),
Prob{A ∪B ∪C} = FX,Y(b, d) −FX,Y(a, c),
and we may conclude that
Prob{A} = FX,Y(b, d) −FX,Y(a, c) −[FX,Y(a, d) −FX,Y(a, c) + FX,Y(b, c) −FX,Y(a, c)]
= FX,Y(b, d) −FX,Y(a, d) −FX,Y(b, c) + FX,Y(a, c).

4.2 Joint Cumulative Distribution Functions
67
Example 4.2 The joint CDF of two continuous random variables is given by
FX,Y(x, y) = (1 −e−λx)(1 −e−μy),
0 ≤x < ∞, 0 ≤y < ∞,
and is zero otherwise, where both λ and μ are strictly positive. Let us compute the probability
Prob{0 ≤x ≤1, 0 ≤y ≤1}. From the above relationship, we have
Prob{0 ≤x ≤1, 0 ≤y ≤1} = FX,Y(1, 1) −FX,Y(0, 1) −FX,Y(1, 0) + FX,Y(0, 0)
= (1 −e−λ)(1 −e−μ) −(1 −e−λ)(1 −e0) −(1 −e0)(1 −e−μ)
+ (1 −e0)(1 −e0)
= (1 −e−λ)(1 −e−μ).
We now turn our attention to marginal cumulative distribution functions derived from joint CDFs
by considering what happens as one, but not both, of X and Y goes to inﬁnity. In the limit as
y →∞, the event {X ≤x, Y ≤y} tends to the event {X ≤x, Y < ∞} = {X ≤x}, where we have
used Y < ∞to signify that Y may take on any value. This implies that the only restriction is that
X ≤x, i.e., the event {X ≤x, Y < ∞} reduces to the event {X ≤x}. But now, the probability of
the event {X ≤x} is simply the cumulative distribution of X and hence
lim
y→∞FX,Y(x, y) = FX(x).
Similarly,
lim
x→∞FX,Y(x, y) = FY(y).
These are called the marginal distributions, and may be computed from the joint distribution.
Example 4.3 Returning to Example 4.1, we may compute the marginal distributions as
−2 < x
−2 ≤x < −1
−1 ≤x < 0
0 ≤x < 1
x ≥1
FX(x)
0
1/8
1/4
1/2
1
for X as before, and for Y:
−2 < y
−2 ≤y < 0
0 ≤y < 2
y ≥2
FY(y)
0
1/4
3/4
1
These are just the top row and the rightmost column of the CDF table provided earlier.
Similarly, the marginal CDFs of X and Y, derived from the joint CDF of Example 4.2, are easily
computed, and are respectively given by
FX(x) = (1 −e−λx)(1 −0) = (1 −e−λx),
FY(y) = (1 −0)(1 −e−μy) = (1 −e−μy).
The random variables X and Y are said to be independent if
FX,Y(x, y) = FX(x)FY(y), −∞< x < ∞, −∞< y < ∞.
In other words, if X and Y are independent random variables, their joint cumulative distribution
function factors into the product of their marginal cumulative distribution functions. The reader
may wish to verify that the random variables X and Y of Examples 4.1 and 4.2 are independent. If,
in addition, X and Y have the same distribution function (for example, when λ = μ in Example 4.2),

68
Joint and Conditional Distributions
then the term independent and identically distributed is applied to them. This is usually abbreviated
to iid.
The generalization of the deﬁnition of the joint cumulative distribution function to joint
distribution functions of more than two random variables, referred to as random vectors, is
straightforward. Given n random variables X1, X2, . . . Xn, their joint cumulative distribution
function is deﬁned for all real xi, −∞< xi < ∞, i = 1, 2, . . . , n, as
F(x1, x2, . . . , xn) = Prob{X1 ≤x1, X2 ≤x2, . . . , Xn ≤xn}.
In this case, F must be a nondecreasing function of all the xi, F(−∞, −∞, . . . , −∞) = 0 and
F(∞, ∞, . . . , ∞) = 1. The marginal distribution of Xi is obtained from
Prob{Xi ≤xi} = FXi(xi) = F(∞, . . . , ∞, xi, ∞. . . ∞).
The random variables X1, X2, . . . , Xn are said to be independent if, for all real x1, x2, . . . , xn,
F(x1, x2, . . . , xn) =
n
i=1
FXi(xi).
If in addition, all n random variables, Xi, i = 1, 2, . . . , n, have the same distribution function, they
are said to be independent and identically distributed.
4.3 Joint Probability Mass Functions
The probability mass function for a single discrete random variable X is denoted pX(x) and gives
the probability that X has the value x, i.e., pX(x) is the probability of the event that consists of
all outcomes that are mapped into the value x. If X and Y are two discrete random variables, an
outcome is a point in the (x, y) plane and an event is a subset of these two-dimensional points:
[X = x, Y = y] is the event that consists of all outcomes for which X is mapped into x and Y
is mapped into y. Their joint probability mass function is denoted pX,Y(x, y) and is equal to the
probability of the event [X = x, Y = y]. We write
pX,Y(x, y) = Prob{X = x, Y = y}.
The joint probability mass function of two discrete random variables may be conveniently
represented in tabular form. For example, we may place realizable values of X across the top of
the table and those of Y down the left side. The element of the table corresponding to column
X = x and row Y = y denotes the probability of the event [X = x, Y = y].
Example 4.4 Let X be a random variable that has values in {1, 2, 3, 4, 5} and Y a random variable
with values in {−1, 0, 1, 2} and let their joint probability mass function be given by pX,Y(x, y) = α
for all possible values of x and y. To compute the value of α, we observe that only 20 points in the
(x, y) plane have positive probability, and furthermore the probability of each is the same. This gives
α = 1/20 = 0.05. The following table displays the joint probability mass function of X and Y.
X = 1
X = 2
X = 3
X = 4
X = 5
Y = −1
1/20
1/20
1/20
1/20
1/20
Y = 0
1/20
1/20
1/20
1/20
1/20
Y = 1
1/20
1/20
1/20
1/20
1/20
Y = 2
1/20
1/20
1/20
1/20
1/20


70
Joint and Conditional Distributions
Example 4.6 Let X and Y be two discrete random variables with joint probability mass function:
X = −1
X = 0
X = 1
Y = −1
1/12
3/12
1/12
Y = 0
1/12
0/12
1/12
Y = 1
1/12
3/12
1/12
Thus, both discrete random variables X and Y take values from the set {−1, 0, 1}. The table
indicates that Prob{X = 0; Y = 0} = 0, that Prob{X = 0; Y = 1} = 1/4 and so on. The marginal
probability mass function of X, pX(x), is obtained by adding the entries in each column of the table;
that of Y, pY(y), is found by adding the entries in each row of the table. We have
X = −1
X = 0
X = 1
pY(y)
Y = −1
1/12
3/12
1/12
5/12
Y = 0
1/12
0/12
1/12
1/6
Y = 1
1/12
3/12
1/12
5/12
pX(x)
1/4
1/2
1/4
1
In this example, X and Y are not independent since
1/12 = Prob{X = 1, Y = 1} ̸= Prob{X = 1}Prob{Y = 1} = 3/12 × 5/12.
When arranged in tabular form like this, with the marginal probability mass functions along the
bottom and right-hand side, it is not difﬁcult to see where the name marginals comes from.
Example 4.7 The Town of Cary has a men’s soccer team and a women’s soccer team. The
probability mass functions of the number of goals scored by each team in a typical game are
0
1
2
3
> 3
Men
0.5
0.2
0.1
0.1
0.1
Women
0.3
0.1
0.3
0.2
0.1
We wish to compute the probability that the women’s team scores more goals than the men’s team
during a randomly chosen match, and the probability that both score the same number of goals.
To proceed, we make the assumption that the number of goals scores by one team is independent
of the number scored by the other. This being the case, we form the joint probability mass function
from pX,Y(x, y) = pX(x)pY(y), where X is the number of goals scored by the men’s team and Y
the number scored by the women’s team, and obtain
pX,Y(x, y)
X = 0
X = 1
X = 2
X = 3
X > 3
Y = 0
0.15
0.06
0.03
0.03
0.03
Y = 1
0.05
0.02
0.01
0.01
0.01
Y = 2
0.15
0.06
0.03
0.03
0.03
Y = 3
0.10
0.04
0.02
0.02
0.02
Y > 3
0.05
0.02
0.01
0.01
0.01
The probability that the women’s team scores more goals that the men’s team is given by
Prob{X < Y} = 0.05 + (0.15 + 0.06) + (0.10 + 0.04 + 0.02) + (0.05 + 0.02 + 0.01 + 0.01)
= 0.51.
The probability that both score the same number of goals is
Prob{X = Y} = 0.15 + 0.02 + 0.03 + 0.02 + 0.01 = 0.23.

4.4 Joint Probability Density Functions
71
4.4 Joint Probability Density Functions
Recall that the probability density function fX(x) of a single continuous random variable X is
deﬁned by means of the relation
FX(x) =
 x
−∞
fX(t)dt
for −∞< x < ∞.
We also saw that for an inﬁnitesimally small dx, Prob{x
<
X
≤x + dx} =
fX(x)dx,
 ∞
−∞fX(x)dx = 1, and that Prob{a < X ≤b} =
 b
a fX(x)dx. These same ideas carry over to
two dimensions and higher. If X and Y are continuous random variables, we can usually ﬁnd a joint
probability density function fX,Y(x, y) such that
FX,Y(x, y) =
 y
−∞
 x
−∞
fX,Y(u, v)dudv,
(4.2)
and which satisﬁes the conditions fX,Y(x, y) ≥0 for all (x, y), and
 ∞
−∞
 ∞
−∞
fX,Y(u, v)dudv = 1.
If the partial derivatives of FX,Y(x, y) with respect to x and y exist, then
fX,Y(x, y) = ∂2FX,Y(x, y)
∂x∂y
.
These relations imply that
Prob{a < X ≤b, c < Y ≤d) =
 b
a
 d
c
fX,Y(x, y)dydx.
This means that, whereas the probability density function of a single random variable is measured
per unit length, the joint probability density function of two random variables is measured per unit
area. Furthermore, for inﬁnitesimal dx and dy,
Prob{x < X ≤x + dx, y < Y ≤y + dy} ≈fX,Y(x, y)dxdy.
This property leads us to take fX,Y(x, y) as the continuous analog of Prob{X = x, Y = y}.
With two joint random variables that are continuous, an event corresponds to an area in the (x, y)
plane. The event occurs if the random variable X assigns a value x1 to the outcome, Y assigns a value
y1 to the outcome and the point (x1, y1) lies within the area that deﬁnes the event. The probability
of the occurrence of an event A is given by
Prob{A} =

{x,y | (x,y)∈A}
fX,Y(u, v)dudv =

A
fX,Y(u, v)dudv.
Example 4.8 Let X and Y be two continuous random variables whose joint probability density
function has the form
fX,Y(x, y) = α(x + y),
0 ≤x ≤y ≤1,
and is equal to zero otherwise. Find the value of the constant α and sketch the region of positive
probability, i.e., the joint domain of fX,Y(x, y) for which fX,Y(x, y) > 0. Also, sketch the region
for which X + Y ≤1 and integrate over this region to ﬁnd Prob{X + Y ≤1}.


4.4 Joint Probability Density Functions
73
Example 4.9 Two continuous random variables have joint probability density function given by
fX,Y(x, y) =
αxy2,
0 ≤x ≤1, 0 ≤y ≤1,
0
otherwise.
Let us ﬁnd the value of α and the following probabilities:
(a) Prob{Y < X},
(b) Prob{Y < X2}, and (c) Prob{max(X, Y) ≥1/2}.
The value of α is found by integrating the joint density function over all (x, y) and setting the result
to 1. We have
1 =
 ∞
−∞
 ∞
−∞
fX,Y(x, y) dy dx =
 1
0
 1
0
αxy2 dy dx
=
 1
0

αx y3
3
 
1
0
dx =
 1
0
αx
3 dx = αx2
6

1
0
= α
6 ,
and hence α = 6. We now compute the required probabilities.
(a) Prob{Y < X} =
 1
0
 x
0
6xy2 dy dx =
 1
0
6x y3
3

x
0
dx =
 1
0
2x4dx = 2x5
5

1
0
= 2/5.
(b) Prob{Y < X2} =
 1
0
 x2
0
6xy2 dy dx =
 1
0
6x y3
3

x2
0
dx =
 1
0
2x7dx = 2x8
8

1
0
= 1/4.
(c) Prob{max(X, Y) ≤1/2} =
 1/2
0
 1/2
0
6xy2 dy dx =
 1/2
0
2x dx
 1/2
0
3y2 dy
=

x2 |1/2
0
 
y3 |1/2
0

=
1
2
5
= 1
32.
Uniformly Distributed Joint Random Variables
When the random variables X and Y are uniformly distributed over a region of the plane, it becomes
possible to use geometric arguments to compute certain probabilities. For example, if X and Y are
continuous random variables, uniformly distributed over the unit square, and we seek the probability
X ≤Y, we note that the region 0 ≤x ≤1, 0 ≤y ≤1, and x ≤y constitutes one-half
of the unit square, i.e., the triangle bounded by the y-axis, the lines y = 1 and x = y, and so
Prob{X ≤Y} = 0.5. More generally, if R is a ﬁnite region of the plane into which (X, Y) is certain
to fall, the probability that (X, Y) actually lies in a subregion A of R is proportional to the size of
A, relative to the size of R. In other words,
Prob{(X, Y) ∈A} = Area of A
Area of R .
Example 4.10 The joint probability density function of two continuous random variables, X and
Y, is given by
fX,Y(x, y) =
α,
x ≥0, y ≥0 and x + y ≤1,
0
otherwise.
Thus X and Y are uniformly distributed over the region of positive probability. Describe this region
and ﬁnd the value of the constant α. Also, compute the probability Prob{X + Y ≤0.5}.
The region of positive probability is the right-angled triangle whose base is the unit interval
along the x-axis, having unit height and whose hypotenuse is a segment of the line x + y = 1,

74
Joint and Conditional Distributions
i.e., the right angle of the triangle is at the origin. Such a shape is sometimes referred to as a wedge.
The usual way to ﬁnd the constant α is by integrating over this region and setting the resulting value
to 1. With this approach, we obtain
1 =
 1
0
 1−x
0
α dydx =
 1
0
α y|1−x
0
dx =
 1
0
(α −αx)dx = αx −αx2/2
1
0 = α/2,
and hence α = 2. An alternative way to obtain this result is to observe that X and Y are uniformly
distributed over the region, and since the region occupies one-half of the unit square, the value
of α must be 2. Also, although the required probability may be found as
Prob{X + Y ≤0.5} =
 0.5
0
 0.5−x
0
2 dydx =
 0.5
0
2y
0.5−x
0
dx
=
 0.5
0
(1 −2x)dx = (x −x2)
0.5
0
= 0.25,
a geometric argument shows that the area of the right-angle triangle with both base and height equal
to 1/2 is 1/2 × 1/2 × 1/2 = 1/8 and hence
Prob{(X, Y) ∈A} = Area of A
Area of R = 1/8
1/2 = 1/4,
as before.
We consider one further example of this type.
Example 4.11 Let X and Y be two continuous random variables, uniformly distributed on the unit
square. We wish to ﬁnd Prob{X2 + Y 2 ≤r2} for r ≤1. The joint probability density function of X
and Y is given by
fX,Y(x, y) =
1,
0 ≤x ≤1, 0 ≤y ≤1,
0
otherwise.
The area of the region of positive probability, i.e., the area of R, is equal to 1, Observe that
x2 + y2 = r2 deﬁnes a circle centered on the origin and with radius r. The area of this circle is
known to be πr2. The portion of it that lies in the positive quadrant, where the joint density function
is positive, is πr2/4. Thus, in our terminology, the area of A is πr2/4 and hence
Prob
!
X2 + Y 2 ≤r2"
= πr2/4
1
= πr2
4 .
Derivation of Joint CDFs from Joint Density Functions
Equation (4.2) relates the joint density function and the joint cumulative distribution function of
two random variables, which leads us consider using one to obtain the other. However, as we now
show in an example, deriving the joint CDF from a joint probability density function is frequently
not straightforward. The difﬁculty arises from the fact that, whereas the density function may be
nonzero over a relatively simple region and zero elsewhere, the joint CDF will often be nonzero
over a much larger region.
Example 4.12 Random variables X and Y have joint probability density function given by
fX,Y(x, y) =
2(x + y),
0 ≤x ≤y ≤1,
0
otherwise.


76
Joint and Conditional Distributions
A complete description of the CDF is therefore given by
FX,Y(x, y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
x < 0 or y < 0 or both,
1,
x > 1 and y > 1,
x2y + xy2 −x3,
0 ≤x ≤y ≤1,
x + x2 −x3,
0 < x < 1 and y > 1,
y3,
x > y and 0 < y < 1.
Marginal Probability Density Functions and Independence
As in the discrete case, we may also form marginal distributions. Since
FX(x) =
 x
−∞
 ∞
−∞
fX,Y(u, y)dydu,
the marginal density functions fX and fY may be formed as
fX(x) =
 ∞
−∞
fX,Y(x, y)dy
and
fY(y) =
 ∞
−∞
fX,Y(x, y)dx.
(4.3)
Joint continuous random variables X and Y are independent if fX,Y(x, y) = fX(x) fY(y). When X
is a discrete random variable and Y is a continuous random variable (or vice versa), we may still
form their joint distribution. In this case, the condition for independence is
Prob{X = x, Y ≤y} = pX(x)FY(y), for all x, y.
Example 4.13 Let the joint probability density function of two continuous random variables, X and
Y be given by
fX,Y(x, y) =

(2x + 5y)/100, 0 < x ≤5, 0 < y ≤2,
0
otherwise.
Let us form the marginal probability density function of each random variable and decide whether
they are independent or not. The marginal probability density function of X is given by
fX(x) =
 ∞
−∞
fX,Y(x, y)dy =
 2
0
2x + 5y
100
dy = x
50
 2
0
dy+ 1
20
 2
0
ydy = x
25+ y2
40

2
0
= 2x + 5
50
.
Similarly, for Y, we ﬁnd
fY(y) =
 ∞
−∞
fX,Y(x, y)dx =
 5
0
2x + 5y
100
dx = 1
50
 5
0
xdx+ y
20
 5
0
dx =
x2
100

5
0
+5y
20 = 1 + y
4
.
In this case, X and Y are not independent since fX,Y(x, y) ̸= fX(x) fY(y).
Example 4.14 Consider two continuous random variables X and Y with joint density function
given by
fX,Y(x, y) = αβe−(αx+βy), 0 < x < ∞, 0 < y < ∞.
We seek the marginal densities of the two random variables and their joint cumulative distribution
function. We would also like to compute the probability that X is greater than Y.
The marginal density function of X is computed as
fX(x) =
 ∞
−∞
fX,Y(x, y)dy =
 ∞
0
αβe−(αx+βy)dy = αe−αx, 0 < x < ∞.

4.5 Conditional Distributions
77
In the same way, we ﬁnd the marginal density of Y to be
fY(y) = βe−βy 0 < y < ∞.
Observe that X and Y are independent random variables, since fX,Y(x, y) = fX(x) fY(y).
The joint cumulative distribution function of X and Y is computed from
FX,Y(x, y) =
 x
−∞
 y
−∞
fX,Y(u, v)dudv =
 x
0
 y
0
αβe−(αu+βv)dudv
=
#
1 −e−αx$ #
1 −e−βy$
, 0 < x < ∞, 0 < y < ∞,
and is equal to zero otherwise. Because of the simplicity of the region of positive probability (the
entire positive quadrant), the usual difﬁculties associated with computing joint CDFs from joint
probability density functions are avoided in this example. Also, observe that since X and Y are
independent, we could have derived this result by ﬁrst ﬁnding the CDF of each of the marginals and
multiplying them together. Finally, we have
Prob{X ≥Y} =
 ∞
x=0
 x
y=0
fX,Y(x, y)dydx =
 ∞
x=0
 x
y=0
βe−βydy

αe−αxdx
=
 ∞
x=0
#
1 −e−βx$
αe−αxdx =
 ∞
x=0

αe−αx −αe−(α+β)x 
dx
= −e−αx∞
0 +
α
α + β e−(α+β)x

∞
0
= 1 −
α
α + β =
β
α + β .
4.5 Conditional Distributions
In the previous chapter we considered the effect of knowing that a certain event B had occurred on
the probability distribution of a random variable X. We now extend this to the case of two random
variables X and Y and our concern is with the probability distribution of one of these when the
conditioning event is deﬁned by the value taken by the second.
Discrete Random Variables
Consider ﬁrst the case when X and Y are two discrete random variables with joint probability mass
function pXY(x, y). Then the conditional probability mass function of X given the event {Y = y} is
pX|Y(x|y) = Prob{X = x|Y = y} = Prob{X = x, Y = y}
Prob{Y = y}
= pXY(x, y)
pY(y)
(4.4)
so long as pY(y) > 0. In other words, we choose a speciﬁc value for y and ﬁnd Prob{X = x|Y = y}
as a function of x.
Example 4.15 Let X and Y be two discrete random variables with joint probability mass function:
X = −1
X = 0
X = 1
pY(y)
Y = −1
1/12
3/12
1/12
5/12
Y = 0
1/12
0/12
1/12
2/12
Y = 1
1/12
3/12
1/12
5/12
pX(x)
3/12
6/12
3/12

78
Joint and Conditional Distributions
Let us compute Prob{X = x|Y = 1}. We have
Prob{X = −1|Y = 1} = Prob{X = −1, Y = 1}/Prob{Y = 1} = (1/12)/(5/12) = 1/5,
Prob{X = 0|Y = 1} = Prob{X = 0, Y = 1}/Prob{Y = 1} = (3/12)/(5/12) = 3/5,
Prob{X = 1|Y = 1} = Prob{X = 1, Y = 1}/Prob{Y = 1} = (1/12)/(5/12) = 1/5.
Observe that for each different value Y can assume, we obtain a different probability mass
function pXY(x|y). In Example 4.15, three different probability density function corresponding to
y = −1, 0, 1 may be obtained, only the last (Y = 1) of which was computed.
Example 4.16 In addition to its men’s and women’s soccer teams, the town of Cary also has a youth
team, some of whose players also play on the men’s team. The number of goals scored by the men’s
team in a typical match is not independent of the number of goals scored by the youth team. The
joint probability mass function of the men’s and youth team is given by
pX,Y(x, y)
X = 0
X = 1
X = 2
X = 3
X > 3
pY(y)
Y = 0
0.04
0.02
0.05
0.03
0.01
0.15
Y = 1
0.03
0.05
0.10
0.05
0.02
0.25
Y = 2
0.04
0.03
0.08
0.07
0.02
0.24
Y = 3
0.04
0.04
0.04
0.04
0.04
0.20
Y > 3
0.01
0.02
0.06
0.04
0.03
0.16
pX(x)
0.16
0.16
0.33
0.23
0.12
1.00
where Y now indicates the number of goals scored by the youth team. For convenience, we have
also included the marginals along the right-hand side and bottom. Let us compute the probability
that the men’s team scores two goals, given that the youth team scores one goal, and the proba-
bility that the youth team scores two goals, given that the men’s team scores one goal. These
probabilities are respectively given by
Prob{X = 2|Y = 1} = 0.10/0.25 = 0.40
and
Prob{Y = 2|X = 1} = 0.03/0.16 = 0.19.
From Equation (4.4) we see that
pXY(x, y) = pY(y)pX|Y(x|y) for pY(y) > 0.
Similarly, we may show that
pXY(x, y) = pX(x)pY|X(y|x) for pX(x) > 0.
In this manner, we may compute the joint probability mass function of X and Y, from their marginal
distributions and their conditional probability mass functions. The procedure applies whether they
are independent or not. If they are independent, then
pX|Y(x|y) = pX(x) and pY|X(y|x) = pY(y)
and pXY(x, y) = pX(x)pY(y) as previously seen. Also, observe that the marginal probability may
be formed as
pX(x) =

all y
pXY(x, y) =

all y
pX|Y(x|y)pY(y),

4.5 Conditional Distributions
79
which is another form of the theorem of total probability. Continuing with the case of two discrete
random variables, we may form the conditional cumulative distribution function as
FX|Y(x|y) = Prob{X ≤x | Y = y} = Prob{X ≤x, Y = y}
Prob{Y = y}
for all possible values of x and values of y for which Prob{Y = y} > 0. This can be obtained from
the conditional probability mass function as
FX|Y(x|y) =

u≤x pXY(u, y)
pY(y)
=

u≤x
pX|Y(u|y).
Example 4.17 Returning to the joint random variables X and Y of Example 4.15, let us compute
the conditional cumulative distribution function of X given Y = 1. We have
FX|Y(x|1) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
x < −1,
12/5 × 1/12 = 1/5,
−1 ≤x < 0,
12/5 × (1/12 + 3/12) = 4/5,
0 ≤x < 1,
12/5 × (1/12 + 3/12 + 1/12) = 1,
x ≥1.
Continuous Random Variables
When X and Y are not discrete random variables problems arise since, for a continuous random
variable, the denominator, Prob{Y = y}, is zero. However, we may deﬁne conditional density
functions as
fX|Y(x|y) = fX,Y(x, y)
fY(y)
for
0 < fY(y) < ∞.
(4.5)
This mimics Equation (4.4) where we use fX,Y(x, y) in place of Prob{X = x, Y = y} and fY(y) in
place of Prob{Y = y}.
Example 4.18 Compute the conditional marginal probability density function fX|Y(x|Y = y) from
the following joint density function of two continuous random variables X and Y:
fX,Y(x, y) =
e−y,
0 < x < y < ∞,
0
otherwise.
We ﬁrst compute the marginal probability density functions. We have
fX(x) =
 ∞
x
e−ydy = −e−y∞
x = e−x
since fX(x) > 0 only when y > x. The other marginal is more straightforward. We have
fY(y) =
 y
0
e−ydx = ye−y.
Let us now form the conditional marginal distribution fX|Y(x|Y = y). From Equation (4.5),
fX|Y(x|Y = y) = e−y
ye−y = 1
y
for 0 < x < y.
This conditional density function is uniformly distributed on (0, y).
Corresponding to Equation (4.4), we may rewrite Equation (4.5) as
fX,Y(x, y) = fY(y) fX|Y(x|y) = fX(x) fY|X(y|x) for 0 < fY(y) < ∞, 0 < fX(x) < ∞.
(4.6)

80
Joint and Conditional Distributions
When X and Y are independent random variables, then
fX,Y(x, y) = fX(x) fY(y)
and it must follow that
fY|X(y|x) = fY(y).
(4.7)
Indeed, Equation (4.7) is a necessary and sufﬁcient condition for two jointly distributed random
variables X and Y to be independent. Notice also that Equation (4.5) may be used to compute the
joint probability density function for two continuous random variables, as the next example shows.
Example 4.19
Let X be uniformly distributed on [0, 1] and let Y be uniformly distributed on
[0, X]. We would like to ﬁnd the joint probability density function of X and Y. The statement of the
problem allows us to write
fX(x) = 1, 0 ≤x ≤1,
fY|X(y|x) = 1/x, 0 ≤y ≤x.
Therefore, using Equation (4.6), we obtain the required joint probability density function as
fX,Y(x, y) = fX(x) fY|X(y|x) = 1 × 1/x = 1/x,
0 ≤y ≤x ≤1.
There are a number of other results that may be obtained from the deﬁnition of the joint density
function. For example, we can compute the marginal density of X in terms of the conditional
density as
fX(x) =
 ∞
−∞
f (x, y)dy =
 ∞
−∞
fY(y) fX|Y(x|y)dy,
which is the continuous analog of the theorem of total probability. We may further obtain the
continuous analog of Bayes’ rule as
fY|X(y|x) =
fY(y) fX|Y(x|y)
 ∞
−∞fY(y) fX|Y(x|y)dy .
Finally, from Equation (4.5) we may deﬁne conditional cumulative distribution functions as
FX|Y(x|y) = Prob{X ≤x|Y = y} =
 x
−∞f (u, y)du
fY(y)
=
 x
−∞
fX|Y(u|y)du.
Example 4.20 Let us return to Example 4.19 to compute the marginal distribution of Y and the
conditional probability density function of X given Y. We have
fY(y) =
 ∞
−∞
fX,Y(x, y)dx =
 1
y
1
x dx = −ln y
and
fX|Y(x|y) = fX,Y(x, y)
fY(y)
=
1/x
−ln y ,
y ≤x ≤1.
4.6 Convolutions and the Sum of Two Random Variables
Suppose we are given the probability mass functions of two independent discrete random variables
X and Y which take their values on the nonnegative integers, and we wish to ﬁnd the probability
mass function of their sum, S = X + Y. Let
pi = Prob{X = i} and qi = Prob{Y = i}.

4.6 Convolutions and the Sum of Two Random Variables
81
When S has the value k, neither X nor Y can be greater than k but both must lie in the interval [0, k]
with sum equal to k. For example, if X = i and 0 ≤i ≤k, then we must have Y = k −i. Therefore,
for all k = 0, 1, . . . ,
Prob{S = k} = Prob{X + Y = k} =
k

i=0
Prob{X = i, Y = k −i}.
If X and Y are independent random variables, then
Prob{S = k} =
k

i=0
Prob{X = i}Prob{Y = k −i} =
k

i=0
piqk−i.
This is called the convolution of the series pi and qi.
Example 4.21 Let X and Y be two independent, discrete random variables with probability mass
functions given by
pX(i) = Prob{X = i} = λi
i! e−λ,
pY(i) = Prob{Y = i} = μi
i! e−μ,
for
i = 0, 1, 2, . . . and λ, μ > 0,
respectively. In a later chapter, we shall call such random variables Poisson random variables. Let
us compute the probability mass function of X + Y. Using the convolution result, we obtain
Prob{X + Y = k} =
k

i=0
λi
i! e−λ μk−i
(k −i)!e−μ = e−(λ+μ) 1
k!
k

i=0
k!
i!(k −i)!λiμk−i
= e−(λ+μ) 1
k!
k

i=0
k
i

λiμk−i = (λ + μ)k
k!
e−(λ+μ),
which is also a Poisson distributed random variable with parameter λ + μ.
Similar results hold for continuous random variables. Let X and Y be continuous random
variables with probability density functions given by fX(x) and fY(y), respectively, and let S =
X + Y. Then
FS(s) = Prob{X + Y ≤s} =
 ∞
−∞
 s−x
−∞
fX,Y(x, y)dydx
and
fS(s) = d
ds FS(s) =
 ∞
−∞
d
ds
 s−x
−∞
fX,Y(x, y)dy

dx =
 ∞
−∞
fX,Y(x, s −x)dx,
for values −∞< s < ∞. Similarly, or more simply by substituting y = s −x, we may also obtain
fS(s) =
 ∞
−∞
fX,Y(s −y, y)dy,
−∞< s < ∞.
When X and Y are independent random variables, then
fS(s) =
 ∞
−∞
fX(x) fY(s −x)dx =
 ∞
−∞
fX(s −y) fY(y)dy,
−∞< s < ∞.

82
Joint and Conditional Distributions
This is the convolution formula for continuous random variables and can be viewed as a special case
of the density function of X + Y when, as a result of their independence, fX,Y(x, y) = fX(x) fY(y).
If, in addition, X and Y are nonnegative random variables, then
fS(s) =
 s
0
fX(x) fY(s −x)dx =
 s
0
fX(s −y) fY(y)dy,
0 < s < ∞,
i.e., the lower limit changes from −∞to 0 because fX(x) = 0 for all x < 0 and the upper limit
changes from ∞to s because fY(s −x) = 0 for x > s.
Example 4.22 Let X and Y be two independent and nonnegative, continuous random variables with
probability density functions
fX(x) =
e−x,
x ≥0,
0
otherwise,
fY(y) =
2e−2y,
y ≥0,
0
otherwise.
We would like to ﬁnd the probability density function of S = X + Y. Since both X and Y are
nonnegative, S must also be nonnegative. We have
fS(s) =
 ∞
−∞
fX(s −y) fY(y)dy =
 s
0
e−(s−y)2e−2ydy = 2e−s
 s
0
e−ydy = 2e−s(1−e−s), s > 0.
For s ≤0, fS(s) = 0.
4.7 Exercises
Exercise 4.1.1 Give an example (possibly from your own experience) of an instance in which two discrete
random variables are jointly needed to represent a given scenario.
Exercise 4.1.2 Give an example (possibly from your own experience) of an instance in which two continuous
random variables are jointly needed to represent a given scenario.
Exercise 4.1.3 Give an example (possibly from your own experience) of an instance in which two random
variables, one discrete and one continuous, are jointly needed to represent a given scenario.
Exercise 4.1.4 Give an example (possibly from your own experience) of an instance in which three random
variables are jointly needed to represent a given scenario.
Exercise 4.2.1 Construct the joint cumulative probability distribution function of two discrete random
variables X and Y from their individual cumulative distributions given below, under the assumption that X
and Y are independent.
x < 0
0 ≤x < 1
1 ≤x < 2
2 ≤x < 3
3 ≤x < 4
x ≥4
FX(x)
0.0
0.2
0.5
0.5
0.6
1.0
y < 0
0 ≤y < 1
1 ≤y < 2
2 ≤y < 3
3 ≤y < 4
y ≥4
FY(y)
0.0
0.3
0.5
0.6
0.7
1.0
Exercise 4.2.2 The joint cumulative distribution function of two discrete random variables is given below.
Compute the following probabilities: (a) Prob{X < 1, Y ≤1}, (b) Prob{X ≤1, Y < 1}, (c) Prob{−2 < X ≤2,
1 < Y ≤2}, (d) Prob{−1 < X ≤1, 0 < Y ≤2}. Are X and Y independent?

4.7 Exercises
83
y ≥2
0.000
0.200
0.440
0.720
0.940
1.000
1 ≤y < 2
0.00
0.120
0.264
0.432
0.564
0.600
0 ≤y < 1
0.00
0.040
0.088
0.144
0.188
0.200
y < 0.000
0.000
0.000
0.000
0.000
0.000
0.000
FX,Y(x, y)
x < −3
−3 ≤x < −1
−1 ≤x < 0
0 ≤x < 1
1 ≤x < 3
x ≥3
Exercise 4.2.3 The joint cumulative distribution function of two continuous random variables X and Y is
FX,Y(x, y) =
1 −e−2x −e−3y + e−(2x+3y),
0 ≤x < ∞, 0 ≤y < ∞,
0
otherwise.
(a) Find Prob{X ≤2, Y ≤1}.
(b) Use Equation (4.1) to ﬁnd Prob{0 < X ≤1, 1 < Y ≤2}.
(c) Show that X and Y are independent.
(d) How could the knowledge that X and Y are independent facilitate the computation of the
probability in part (b)?
Exercise 4.3.1 Consider two discrete random variables X and Y whose joint probability mass function is given
in the table below.
x = 0
x = 1
x = 2
x = 3
y = 0
a
2a
2a
a
y = 1
b
2b
2b
b
What restrictions must be placed on the values of a and b so that this table represents a valid joint probability
density function?
Exercise 4.3.2 Two discrete random variables X and Y have joint probability mass function as follows:
X = 1
X = 2
X = 3
X = 4
Y = 0
1/6
1/12
0
1/3
Y = 1
1/6
0
1/4
0
(a) Compute the marginal probability mass functions of X and Y and check for independence.
(b) Compute the probability of each of the following four events:
(i) X < 3;
(ii) X is odd;
(iii) XY is odd;
(iv) Y is odd given that X is odd.
Exercise 4.3.3 Let X and Y be two discrete random variables with joint probability mass function
X = −2
X = 0
X = 2
Y = 1
0
2a
a
Y = 2
2a
0
2a
Y = 4
a
2a
0
Let S and Z be two additional discrete random variables such that S = X + Y and Z = X −Y.
(a) Find a and the marginal probability mass function of X.
(b) Are X and Y independent?
(c) Construct the table of the joint probability mass function of S and Z.
(d) Are S and Z independent?

84
Joint and Conditional Distributions
Exercise 4.3.4 Let X, Y, and Z be three discrete random variables for which
Prob{X = 0, Y = 0, Z = 0} = 6/24,
Prob{X = 0, Y = 1, Z = 0} = 8/24,
Prob{X = 0, Y = 1, Z = 1} = 6/24,
Prob{X = 1, Y = 0, Z = −1} = 1/24,
Prob{X = 1, Y = 0, Z = 1} = 1/24,
Prob{X = 1, Y = 1, Z = 0} = 2/24.
Let S be a new random variable for which S = X + Y + Z.
(a) Find the marginal probability mass function of X.
(b) Are X and Y independent?
(c) Are X and Z independent?
(d) Find the marginal probability mass function of S.
Exercise 4.3.5 Let X, Y, and Z be three discrete random variables for which
Prob{X = 1, Y = 1, Z = 0} = p,
Prob{X = 1, Y = 0, Z = 1} = (1 −p)/2,
Prob{X = 0, Y = 1, Z = 1} = (1 −p)/2,
where 0 < p < 1. What is the joint probability mass function of X and Y?
Exercise 4.3.6 Let X be a random variable that has the value {−1, 0, 1} depending upon whether a child
in school is performing below, at, or above grade level, respectively. Let Y be a random variable that is equal
to zero if a child comes from an impoverished family and equal to one otherwise. In a particular class, it is
observed that 20% of the children come from impoverished families and are performing below grade level,
that 20% are from impoverished families and are performing at grade level, and that 6% are from impoverished
families and are performing above grade level. Of the remaining children, half are performing at grade level and
one-third are performing above grade level. Construct a table of the joint probability mass function of X and Y
and compute the marginal probability density function of both random variables. Are X and Y independent?
Exercise 4.4.1 Two continuous random variables X and Y have joint probability density function given by
fX,Y(x, y) =
2xy + x,
0 ≤x ≤1, 0 ≤y ≤1,
0
otherwise.
Show that fX,Y(x, y) is a bona ﬁde joint density function and ﬁnd (a) Prob{X ≥Y} and (b) Prob{min(X, Y) ≤
1/2}.
Exercise 4.4.2
X and Y are two independent and identically distributed continuous random variables. The
probability density function of X is
fX(x) =
3x2,
0 ≤x ≤1,
0
otherwise.
That of Y is similarly deﬁned. Write down the joint probability density function of X and Y and ﬁnd the
probability that Prob{Y −X ≥1/2}.
Exercise 4.4.3 The roots of a quadratic polynomial ax2 + bx + c = 0 are real when b2 −4ac ≥0. Use this
result to ﬁnd the probability that the roots of r 2 + 2Xr + Y = 0 are real, where X and Y are independent,
continuous random variables with X uniformly distributed over (−1, 1) and Y uniformly distributed over (0, 1).
Exercise 4.4.4 Two friends agree to meet between 6:00 p.m. and 7:00 p.m. for an afterwork drink at the local
pub. Each arrives randomly within this hour, but both being impatient will only wait for at most ten minutes
for the other to arrive. Use geometric arguments to ﬁnd the probability that the two friends actually meet.

4.7 Exercises
85
Exercise 4.4.5 Let X and Y be two continuous random variables whose joint density function is given by
fX,Y(x, y) =
αe−y,
0 < x < y < ∞,
0
otherwise.
Compute the value of α and then ﬁnd FX,Y(2, y).
Exercise 4.4.6 Let X and Y be two continuous random variables whose joint probability density function is
given by
fX,Y(x, y) =
1 −α(x + y),
0 ≤x ≤1, 0 ≤y ≤2,
0
otherwise.
Find the value of the constant α and the probabilities Prob{X ≥1/2, Y ≤1} and Prob{X < Y}.
Exercise 4.4.7 Let the joint probability density function of two continuous random variables X and Y be
given by
fX,Y(x, y) =
α,
a < x ≤b, 0 < y ≤c,
0
otherwise.
Find the value of α and the marginal density functions of X and Y. Are X and Y, independent random
variables?
Exercise 4.4.8 The joint probability density function of two continuous random variables X and Y is given by
fX,Y(x, y) =
α,
x2 + y2 ≤r 2,
0
otherwise.
Compute the value of the constant α and the marginal density functions of X and Y. Are X and Y independent?
Exercise 4.5.1 Let X and Y be two discrete random variables whose joint probability mass function is as
follows.
pX,Y(x, y)
X = −3
X = −1
X = 0
X = 1
X = 3
Y = 0
0.10
0.04
0.03
0.02
0.01
Y = 1
0.10
0.10
0.10
0.10
0.00
Y = 2
0.00
0.10
0.15
0.10
0.05
Find Prob{X = 0|Y = 0} and Prob{Y = 2|X = 1}.
Exercise 4.5.2 Two discrete random variables X and Y, which are independent, have probability mass
functions given by
0
1
2
3
4
X
0.2
0.3
0.0
0.1
0.4
Y
0.3
0.2
0.1
0.1
0.3
Find Prob{X = 1|Y > 2} and Prob{X = 1|Y ̸= 2}.
Exercise 4.5.3 The joint probability density function of two random variables X and Y is given by
fX,Y(x, y) =
2,
0 ≤y ≤x ≤1,
0
otherwise.
Derive the conditional probability density functions fX|Y(x|y) and fY|X(y|x).
Exercise 4.5.4 Random variables X and Y have joint probability density function given by
fX,Y(x, y) =
2(x + y),
0 ≤x ≤y ≤1,
0
otherwise.
Find the conditional probability density functions fX|Y(x|y) and fY|X(y|x).

86
Joint and Conditional Distributions
Exercise 4.5.5 The joint probability density function of two continuous random variables X and Y is given by
fX,Y(x, y) =
αx2y,
0 ≤x ≤1, 0 ≤y ≤1,
0
otherwise.
Find the value of α and Prob{X ≤0.5|Y = 0.2}.
Exercise 4.5.6 Let X and Y be two continuous random variables whose joint probability density function is
given by
fX,Y(x, y) =
1 −(x + y)/3,
0 ≤x ≤1, 0 ≤y ≤2,
0
otherwise.
What are the marginal density functions of X and Y? Compute the probability density function of Y given X
and hence the following probabilities: (a) Prob{Y ≤1 | X = 0.5} and (b) Prob{Y ≤1 | X = 0.25}.
Exercise 4.6.1 The probability mass function of a discrete random variable X, in tabular form, is given below.
Random variable Y is identically distributed to, and independent of, X.
X = 1
X = 2
X = 3
X = 4
pX(x)
0.1
0.2
0.3
0.4
(a) Express the probability mass function of X as a formula.
(b) Use the convolution equation to compute the distribution of X + Y.
(c) Construct a table of the joint probability mass function p(X,Y)(x, y) and from this table ﬁnd
Prob{X + Y = 4}.
(d) Verify your answer to (c) against that computed by inserting the appropriate values into your answer
to part (b).
Exercise 4.6.2 Let X and Y be two discrete random variables that are independent and identically distributed
with probability mass function given by
pX(k) = Prob{X = k} =
p(1 −p)k,
k = 1, 2, . . . , 0 ≤p ≤1,
0
otherwise.
Such random variables are called geometric random variables, and are discussed in Chapter 6. Find the
distribution of X + Y.
Exercise 4.6.3 Let X and Y be two independent random variables that are identically and uniformly distributed
on the unit interval. Use the convolution formula to compute the probability density function of the random
variable S = X + Y.

Chapter 5
Expectations and More
5.1 Deﬁnitions
A random variable is completely characterized by its cumulative distribution function or
equivalently, by its probability mass/density function. However, it is not always possible, and in
many cases not necessary, to have this complete information. In some instances a single number that
captures some average characteristic is sufﬁcient. Such numbers include the mean value, median,
and mode of the random variable. The mean value of a random variable is computed in a similar
manner to that used to compute the average value of a set of numbers. The average value of a set
of n numbers is formed by multiplying each number by 1/n and adding the results together. The
mean value of a discrete random variable which assumes the values {x1, x2, . . . , xn} is obtained by
multiplying each by the proportion of time it occurs, and then adding, i.e., by forming n
i=1 xi pi.
The mean value of a random variable X is more commonly called the expectation of X and we shall
have much more to say about this in just a moment. The median of a set of numbers is that number
(or numbers) which lies in the middle of the set once the numbers are arranged into ascending or
descending order. The median of a random variable X is any number m that satisﬁes
Prob{X < m} ≤1/2 and Prob{X > m} ≤1/2,
or equivalently,
Prob{X < m} = Prob{X > m}.
The mode is the most likely possible value. For a random value, it is the value for which the
probability mass function or probability density function attains its maximum. If m is a mode of
a random variable X, then Prob{X = m} ≥Prob{X = x} for all values of x. Like the median, and
unlike the mean value, a random variable may have multiple modes.
Example 5.1 If X is the random variable that denotes the number of spots obtained in one throw of
a fair die, then there are six modes (since each number of spots is equally likely and all outcomes
attain the same maximum value of 1/6); the median is any number in the interval (3, 4); there is only
one mean value, which is equal to (1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5.
The deﬁnition of the mean and the fact that it is a unique number make it, from a mathematical
point of view, much more attractive to work with than either the median or the mode. As we shall
now see, it may be readily extended to provide a complete characterization of a random variable.
Indeed, a random variable may be completely characterized by a set of values, called moments.
These moments are deﬁned in terms of the distribution function, but generally do not need the
distribution function to be computed. It may be shown that, if two random variables have the same
moments of all orders, then they also have the same distribution.
The ﬁrst moment is the mean or expectation of the random variable X and is given by the
Riemann-Stieltjes integral,
E[X] =
 ∞
−∞
xdF(x).

88
Expectations and More
For a continuous random variable X, this becomes
E[X] =
 ∞
−∞
x f (x)dx,
where f (x) is the probability density function of X. If X is a discrete random variable whose values
{x1, x2, . . .} have corresponding probabilities {p1, p2, . . .}, then the integral becomes a sum and we
have, as previously indicated,
E[X] =
∞

i=1
xi pi.
When a discrete random variable takes its values on the set of nonnegative integers, we may write
its expectation as
E[X] = p1 + 2p2 + 3p3 + 4p4 + · · ·
= (p1 + p2 + p3 + p4 + · · · ) + (p2 + p3 + p4 + · · · ) + (p3 + p4 + · · · ) + · · · ,
which gives the useful formula
E[X] =
∞

i=1
Prob{x ≥i}.
The nth moment of a random variable X is deﬁned similarly. For a continuous random variable,
it is given by
E[Xn] =
 ∞
−∞
xn f (x)dx,
while for a discrete random variable X it is
E[Xn] =

i
xn
i pi.
The ﬁrst two moments are the most important. The ﬁrst, as we have seen, is called the expectation
or mean value of X; the second moment is called the mean-square value of X. It is given by
E[X2] =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
 ∞
−∞
x2 f (x)dx
if X is continuous,

i
x2
i pi
if X is discrete.
Central moments are also important. They are the moments of the difference between a random
variable X and its mean value E[X]. The nth central moment is given by
E[(X −E[X])n] =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
 ∞
−∞
(x −E[X])n f (x)dx
if X is continuous,

i
(xi −E[X])n pi
if X is discrete.

5.1 Deﬁnitions
89
The second central moment is more commonly called the variance of the random variable X and is
written as
σ 2
X ≡Var [X] ≡E[(X −E[X])2] =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
 ∞
−∞
(x −E[X])2 f (x)dx
if X is continuous,

i
(xi −E[X])2 pi
if X is discrete.
A frequently used formula for the variance, and one we shall derive shortly (see Equation 5.5), is
Var [X] = E[X2] −(E[X])2.
(5.1)
Since E[X2] is the mean-square value of X, the variance is equal to the mean-square value of X
minus the square of the mean value of X. The variance, which cannot be negative, characterizes the
dispersion of the random variable X about its mean value. Its square root, denoted by σX, is called
the standard deviation of the random variable X. The standard deviation yields a number whose
units are the same as those of the random variable and for this reason provides a clearer picture
of the dispersion of the random variable about its mean. One ﬁnal characterization of a random
variable that we shall need, is its coefﬁcient of variation, written as CX and deﬁned as
CX =
σX
E[X],
i.e., the standard deviation of X divided by its expected value. Its usefulness lies in the fact that it is
a dimensionless measure of the variability of X. Often, in place of the coefﬁcient of variation, the
squared coefﬁcient of variation is used instead. This is deﬁned as
C2
X = Var [X]
(E[X])2 .
Example 5.2 Consider the random variable X which denotes the total number of spots obtained
when two fair dice are thrown simultaneously. Previously, we saw that the probability mass function
of X is given by
xi
2
3
4
5
6
7
8
9
10
11
12
pX(xi)
1/36
2/36
3/36
4/36
5/36
6/36
5/36
4/36
3/36
2/36
1/36
In this case the expectation of X is given by
E[X] =
∞

i=1
xi pi =
12

i=2
xi pi
= 2 1
36 + 3 2
36 + 4 3
36 + 5 4
36 + 6 5
36 + 7 6
36 + 8 5
36 + 9 4
36 + 10 3
36 + 11 2
36 + 12 1
36
= 252
36 = 7.0.
This is also the value of the median and the mode of X. To compute the variance of X, we proceed
as follows:
Var [X] =
12

i=2
(xi −7)2 pi
= 52 1
36 + 42 2
36 + 32 3
36 + 22 4
36 + 12 5
36 + 02 6
36 + 12 5
36 + 22 4
36 + 32 3
36 + 42 2
36
+52 1
36 = 210
36 = 5.8333.

90
Expectations and More
It follows that the standard deviation of X is
√
5.8333 = 2.4152.
Example 5.3 Let X be a continuous random variable whose probability density function is
fX(x) =
αx2,
−1 ≤x ≤1,
0
otherwise,
for some constant α. We wish to compute the value of α and then ﬁnd the expectation and standard
deviation of X. Since
1 =
 ∞
−∞
fX(x)dx =
 1
−1
αx2 dx = αx3/3
1
−1 = 2α/3,
we must have α = 3/2. We may now compute E[X] as
E[X] =
 ∞
−∞
x fX(x)dx =
 1
−1
3x3/2 dx = 3x4/8
1
−1 = 0.
The variance is computed from
Var [X] =
 ∞
−∞
(x −E[X])2 fX(x)dx =
 ∞
−∞
x2 fX(x)dx,
which is just equal to the second moment! In this case,
Var [X] = E[X2] =
 1
−1
3x4/2 dx = 3x5/10
1
−1 = 3/5
and the standard deviation is given by σX = √3/5.
Example 5.4 Consider the continuous random variable M whose probability density function is
given by
fM(x) =

e−x
if x ≥0,
0
otherwise.
In this case, the expectation is given by
E[M] =
 ∞
−∞
x f (x)dx =
 ∞
0
xe−xdx = −e−x|∞
0 = 1,
and its variance is computed as
Var [M] =
 ∞
0
(x −E[M])2e−xdx =
 ∞
0
(x −1)2e−xdx
=
 ∞
0
x2e−xdx −2
 ∞
0
xe−xdx +
 ∞
0
e−xdx = 2 −2 + 1 = 1,
which is just the same as its mean value!
We stated previously that the standard deviation of a random variable is a number whose units
are the same as those of the random variable and is an appropriate measure of the dispersion of
the random variable about its mean. If a particular value for X, say x, lies in an interval of σX
about E[X], i.e., x ∈[E[X] −σX, E[X] + σX], then it is considered to be close to the expected
value of X. Usual (or typical) values of X lie inside this range. If x lies outside 2σX of E[X], i.e.,
x < E[x] −2σx or x > E[x] + 2σx, then x is considered to be far from the mean.

5.1 Deﬁnitions
91
Chebychev’s inequality, which is proven in a later section, makes these concepts more precise.
Let X be a random variable whose expectation is E[X] = μX and whose standard deviation is given
by σX. Chebychev’s inequality states that
Prob{|X −μX| ≤kσX} ≥1 −1
k2 .
In words, this says that the probability a random variable lies within ±k standard deviations of its
mean value is at least 1 −1/k2. Alternatively, the probability that a random variable lies outside ±k
standard deviations of its mean is at most 1/k2. This means that 75% (= 1 −1/22) of the time, a
random variable is within two standard deviations of its mean, and 89% (= 1 −1/32) of the time,
it falls within three standard deviations. In fact it turns out that in most cases, these numbers are
underestimations of what generally occurs. Indeed, for many distributions, the probability that a
random variable lies within two standard deviations can be as high as 95% or greater.
Example 5.5 In Ms. Wright’s class of 32 eighth graders, the height of her students, rounded to the
nearest inch, is given in the following table.
Height
≤62
63
64
65
66
67
68
69
70
71
≥72
Students
1
2
1
3
4
3
6
5
4
2
1
Are Peter, whose height is 5′4′′ and Paul, whose height is 5′11′′ outside the norm for this class?
We ﬁrst compute the mean and standard deviation of student height. We ﬁnd the expectation
E[H] to be
(62+2×63+64+3×65+4×66+3×67+6×68+5×69+4×70+2×71+1×72)/32 = 67.47
and the variance to be
Var [H] =
#
(62 −67.47)2 + 2(63 −67.47)2 + (64 −67.47)2 + 3(65 −67.47)2
+ 4(66 −67.47)2 + 3(67 −67.47)2 + 6(68 −67.47)2 + 5(69 −67.47)2
+ 4(70 −67.47)2 + 2(71 −67.47)2 + (72 −67.47)2$
/32 = 6.06,
which means that the standard deviation is
√
6.06 = 2.46. It follows that students whose heights
lie between 67.47 −2.46 and 67.47 + 2.46, i.e., in the interval [65.01, 69.93], or since we have
rounded to the nearest inch, students whose height lies between 65 and 70 inches, should be
considered normal for this class. Since Peter’s and Paul’s heights both lie outside this, neither is
“average” for this class. However, neither student’s height lies outside the 2 standard deviation
range of 67.47 ± 2 × 2.46 = [62.55, 72.39].
The random variables that we shall consider all have ﬁnite expectation. The expectation is
said not to exist unless the corresponding integral or summation is absolutely convergent, i.e., the
expectations exist only if
 ∞
−∞
|x| f (x)dx < ∞and
∞

i=1
|xi|pi < ∞.
Example 5.6 The function
fX(x) =
1
π(1 + x2),
−∞< x < ∞,

92
Expectations and More
is a probability density function but does not have an expectation. To see this, observe that
 ∞
−∞
dx
π(1 + x2) = 1
π arctan(x)

∞
−∞
= 1
π
π
2 −−π
2

= 1,
and so fX(x) is a density function. However, this does not converge absolutely since
 ∞
−∞

x
π(1 + x2)
 dx = 1
π
 ∞
0
2x
1 + x2 dx = 1
π ln(1 + x2)

∞
0
= ∞.
5.2 Expectation of Functions and Joint Random Variables
Expectation of Functions of a Random Variable
Given a discrete random variable X, the expectation of a new random variable Y which is a function
of X, i.e., Y = h(X), may be written as
E[Y] = E[h(X)] =
∞

−∞
h(X)pX(x).
(5.2)
Similarly, for a continuous random variable X, the expectation of some function h(X) of X is
given as
E[h(X)] =
 ∞
−∞
h(X) fX(x)dx.
(5.3)
Example 5.7 For the discrete random variable R which denotes the number of heads obtained in
three tosses of a fair coin, we saw that its probability mass function is given by
xk
0
1
2
3
pR(xk)
0.125
0.375
0.375
0.125
Consider now a gambling situation in which a player loses $3 if no heads appear, loses $2 if one
head appears, wins nothing and gains nothing if two heads appear but gains $7 dollars if three heads
appears. Let us compute the mean number of dollars won or lost in one game. To do so, we deﬁne
the derived random variable Y as follows:
Y = h(X) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
−3,
X = 0,
−2,
X = 1,
0,
X = 2,
7,
X = 3,
0
otherwise.
We now compute E[Y] and discover that the player should expect to lose 25 cents each time he
plays, since
E[Y] =

x=0,1,2,3
h(X)pX(x) = −3 × 1/8 −2 × 3/8 + 7 × 1/8 = −1/4.
Example 5.8 Let X be a continuous random variable with probability density function
fX(x) =
2x,
0 ≤x ≤1,
0
otherwise,

5.2 Expectation of Functions and Joint Random Variables
93
and let Y be the derived random variable Y = 4X + 2. The expectation of X is given by
E[X] =
 1
0
2x2 dx = 2x3
3

1
0
= 2/3.
Let us now compute the expectation of Y in two ways: ﬁrst integrating its density function over the
appropriate interval, and second using the easier approach offered by Equation (5.3).
Using the approach of Section 3.5, we ﬁnd the probability density function of Y by ﬁrst ﬁnding its
cumulative distribution function. However, we ﬁrst need to ﬁnd the cumulative distribution function
of X. This is
FX(x) =
 x
0
2u du = x2 for 0 ≤x ≤1;
(5.4)
is equal to zero for x < 0, and is equal to 1 for x > 1. We may now compute FY(y) as
FY(y) = Prob{Y ≤y} = Prob{4X + 2 ≤y} = Prob

X ≤y −2
4
%
= FX
 y −2
4

.
To ﬁnd the correct limits, we observe that FX(x) = 0 for x < 0, which allows us to assert that
FY(y) = 0 for (y −2)/4 < 0, i.e., for y < 2. Also, FX(x) = 1 for x > 1, so that FY(y) = 1 for
(y −2)/4 > 1, i.e., for y > 6. It follows from Equation (5.4) that
FY(y) =
 y −2
4
2
for 2 ≤y ≤6.
Finally, by differentiating FY(y) with respect to y, we may now form the density function of Y as
fY(y) =

2(y −2)/16,
2 ≤y ≤6,
0
otherwise,
and compute its expectation as
E[Y] =
 6
2
2y(y −2)/16 dy = (2y3/3 −2y2)/16|6
2 = 4.6667.
Using Equation (5.3), we obtain
E[Y] = E[h(X)] =
 ∞
−∞
h(X) fX(x) dx =
 1
0
(4x + 2)2x dx =
 1
0
(8x2 + 4x) dx
= (8x3/3 + 2x2)
1
0 = 14/3,
the same as before, but with considerably less computation.
The extension to functions of n random variables is immediate. If a random variable Y is deﬁned
as a function h of the n random variables X1, X2, . . . , Xn, its expectation may be found using the
formula
E[Y] = E[h(X1, X2, . . . , Xn)] =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
h(x1, x2, . . . , xn) f (x1, x2, . . . , xn)dx1dx2 · · · dxn

x1

x2
· · ·

xn
h(x1, x2, . . . , xn)p(x1, x2, . . . , xn)
for the continuous and discrete cases, respectively. If the function h is a linear function, i.e.,
h(x1, x2, . . . , xn) = α1x1 + α2x2 + · · · + αnxn,
where αi, i = 1, 2, . . . , n, are constants, then
E[Y] = E[h(X1, X2, . . . , Xn)] = h(E[X1], E[X2], . . . , E[Xn]).

94
Expectations and More
Notice that the random variables X1, X2, . . . , Xn need not be independent. However, this result does
not generally apply to functions h that are not linear.
Example 5.9 Returning to Example 5.8, and observing that h(x) = 4x + 2 is a linear function, we
may compute E[Y] as
E[Y] = E[h(X)] = h(E[X]) = h(2/3) = 4(2/3) + 2 = 14/3,
as before.
Expectation of Jointly Distributed Random Variables
When X and Y are jointly distributed random variables and Z is a random variable that is a function
of X and Y, i.e., Z = h(X, Y), then the expectation of Z may be found directly from the joint
distribution. When X and Y are discrete random variables, then
E[Z] =

all x

all y
h(x, y)pX,Y(x, y),
and when they are continuous random variables,
E[Z] =
 ∞
−∞
 ∞
−∞
h(x, y) fX,Y (x, y) dx dy.
Example 5.10 Let X and Y be two continuous random variables whose joint probability density
function is
fX,Y(x, y) =

1 −(x + y)/3,
0 ≤x ≤1, 0 ≤y ≤2,
0
otherwise.
We wish to ﬁnd E[XY]. We have
E[XY] =
 2
y=0
 1
x=0
xy[1 −(x + y)/3] dx dy =
 2
y=0
 1
x=0

xy −x2y
3
−xy2
3

dx dy
=
 2
y=0
x2y
2
−x3y
9
−x2y2
6

1
x=0
dy =
 2
y=0
 y
2 −y
9 −y2
6

dy
=
7y2
36 −y3
18

2
0
= 12
36 = 1/3.
Observe that, when X and Y are independent random variables, then E[XY] = E[X]E[Y] since
we have
E[XY] =
 ∞
−∞
 ∞
−∞
xy fXY(x, y)dxdy =
 ∞
−∞
 ∞
−∞
xy fX(x) fY(y)dxdy
=
 ∞
−∞
 ∞
−∞
x fX(x) y fY(y) dxdy =
 ∞
−∞
E[X] y fY(y)dy = E[X]E[Y].

5.2 Expectation of Functions and Joint Random Variables
95
The case in which a random variable is deﬁned as the sum of two other random variables,
warrants special attention. We have
E[Z] = E[X + Y] =
 ∞
−∞
 ∞
−∞
(x + y) fXY(x, y)dxdy
=
 ∞
−∞
 ∞
−∞
x fXY(x, y)dxdy +
 ∞
−∞
 ∞
−∞
y fXY(x, y)dxdy
=
 ∞
−∞
x
& ∞
−∞
fXY(x, y)dy
'
dx +
 ∞
−∞
y
& ∞
−∞
fXY(x, y)dx
'
dy
=
 ∞
−∞
x fX(x)dx +
 ∞
−∞
y fY(y)dy = E[X] + E[Y].
In developing this result, we have used Equation (4.3), which deﬁnes the marginal probabilities
fX(x) and fY(y). Notice that this property, called the linearity property of the expectation, holds
whether the random variables are independent or not. More generally, the expectation of a sum
of random variables is the sum of the expectations of those random variables. That is, let X1,
X2, . . . , Xn be a set of n random variables. Then
E[X1 + X2 + · · · + Xn] = E[X1] + E[X2] + · · · + E[Xn].
The same result does not apply to variances: the variance of a sum of two random variables is
equal to the sum of the variances only if the random variables are independent. To show this we let
μX = E[X] and μY = E[Y]. Then
Var [X + Y] = E[(X + Y −μX −μY)2]
= E[(X −μX)2] + 2E[(X −μX)(Y −μY)] + E[(Y −μY)2]
= E[(X −μX)2] + E[(Y −μY)2] = Var (X) + Var (Y),
where we have used the fact that, when X and Y are independent,
E[(X −μX)(Y −μY)] = E[X −μX]E[Y −μY] = 0,
since, for example, E[X −μX] = E[X] −E[μX] = μX −μX = 0.
To compute the variance of the sum of two random variables X and Y that are not necessarily
independent, we use the relation Var [X] = E[X2] −E[X]2, and proceed as follows:
Var [X + Y] = E[(X + Y)2] −(E[X] + E[Y])2
= E[X2] + 2E[XY] + E[Y 2] −(E[X])2 −2E[X]E[Y] −(E[Y])2
and hence
Var [X + Y] = Var [X] + Var [Y] + 2(E[XY] −E[X]E[Y]).
Thus the variance of the sum of two random variables is equal to the sum of the variances only if
E[XY] −E[X]E[Y] is zero. This is the case, for example, when X and Y are independent. The
quantity E[XY] −E[X]E[Y] is called the covariance of X and Y and is denoted
Cov [X, Y] = E[XY] −E[X]E[Y].
The variance of a sum of random variables, X1, X2, . . . , Xn is given by
Var [X1 + X2 + · · · + Xn] =
n

k=1
Var [Xk] + 2

j<k
Cov [X j, Xk]

96
Expectations and More
Some properties of the covariance of two random variables are
• Cov (αX, βY) = αβ Cov(X, Y),
• Cov (X + α, Y + β) = Cov(X, Y),
• Cov (X, αX + β) = αVar (X),
where α and β are scalars. As a general rule, the covariance is positive if above-average values of
X are associated with above-average values of Y or if below-average values of X are associated
with below-average values of Y. If above-average values of one are associated with below-average
values of the other, then the covariance will be negative. Sometime difﬁculties arise in interpreting
the magnitude of a covariance, and for this reason, a new quantity, called the correlation , is used.
Just as the standard deviation of a random variable often gives a better feel than the variance for
the deviation of a random variable from its expectation, the correlation of two random variables
normalizes the covariance to make it unitless. The correlation of two random variables is deﬁned as
Corr [X, Y] = Cov [X, Y]
σXσY
where σX and σY are the standard deviations of X and Y, respectively. For any two random variables,
we always have −1 ≤Corr [X, Y] ≤1.
Example 5.11 Let us compute the covariance and correlation of the two random variables X and
Y whose joint probability density function is
fX,Y(x, y) =
6x,
0 < x < y < 1,
0
otherwise.
Let us ﬁnd E[XY], Cov [X, Y], Corr [X, Y], E[X + Y], and Var [X + Y].
We begin by computing the marginal density function of both X and Y, and from these we shall
compute the expectation and variance of X and of Y. With this information we can then compute
the requested items.
fX(x) =
 ∞
−∞
fXY(x, y) dy =
 1
x
6x dy = 6xy|1
x = 6x(1 −x),
0 ≤x ≤1,
and is zero otherwise.
fY(y) =
 ∞
−∞
fXY(x, y) dx =
 y
0
6x dx = 3x2|y
0 = 3y2,
0 ≤y ≤1,
and is zero otherwise.
E[X] =
 ∞
−∞
x fX(x)dx =
 1
0
6x2(1 −x) dx =

2x3 −3x4
2

1
0
= 1/2,
E[Y] =
 ∞
−∞
y fY(y)dy =
 1
0
3y3 dy = 3/4,
E[X2] =
 ∞
−∞
x2 fX(x)dx =
 1
0
6x3(1 −x) dx =
3x4
2
−6x5
5

1
0
= 3/10,
E[Y 2] =
 ∞
−∞
y2 fY(y)dy =
 1
0
3y4 dy = 3/5,
Var [X] = E[X2] −E[X]2 = 3/10 −1/4 = 1/20,
Var [Y] = E[Y 2] −E[Y]2 = 3/5 −9/16 = 3/80.

5.2 Expectation of Functions and Joint Random Variables
97
We are now in a position to compute the required quantities. We have
E[XY] =
 1
x=0
 1
y=x
6x2y dy dx =
 1
0
3x2y21
y=x dx =
 1
0
(3x2−3x4) dx =

x3 −3x5
5

1
0
= 2/5,
Cov [X, Y] = E[XY] −E[X]E[Y] = 2/5 −1/2 × 3/4 = 1/40,
Corr [X, Y] = Cov[X, Y]
σXσY
=
1/40
√1/20√3/80 =
1
√
3
=
√
3/3,
E[X + Y] = E[X] + E[Y] = 1/2 + 3/4 = 5/4,
Var [X + Y] = Var [X] + Var [Y] + 2 Cov [X, Y] = 1/20 + 3/80 + 1/20 = 11/80.
The covariance of two random variables X and Y captures the degree to which they are correlated.
X and Y are uncorrelated if E[XY] = E[X]E[Y]. If, for example, X and Y are independent, then
E[XY] = E[X]E[Y] and X and Y are also uncorrelated. However, if X and Y are uncorrelated, it
does not necessarily follow that they are also independent.
Example 5.12 Let X and Y be two discrete random variables with joint probability mass function
given by
X = −1
X = 0
X = 1
pY(y)
Y = −1
1/12
3/12
1/12
5/12
Y = 0
1/12
0/12
1/12
2/12
Y = 1
1/12
3/12
1/12
5/12
pX(x)
3/12
6/12
3/12
1
From this table we have
E[X] = −1 × 3/12 + 1 × 3/12 = 0,
E[Y] = −1 × 5/12 + 1 × 5/12 = 0,
and
E[XY] = (−1)(−1)1/12 + (−1)(1)1/12 + (1)(−1)1/12 + (1)(1)1/12 = 0.
It follows then that X and Y are uncorrelated, since E[XY] = E[X]E[Y] = 0, but they are not
independent since
pX,Y(0, 0) = 0 ̸=
pX(0)pY(0) = 6/12 × 2/12.
In summary, X and Y are uncorrelated if Corr [X, Y] = 0, or if Cov [X, Y] = 0 or if
E[XY] = E[X]E[Y]; independent random variables are uncorrelated, but uncorrelated random
variables are not necessarily independent.
Consider now the variance of the difference of two random variables X and Y:
Var [X −Y] = Var [X] + Var [Y] −2(E[XY] −E[X]E[Y])
which is obtained by replacing X + Y with X −Y in the formula for the variance of a sum. Thus
the variance of the sum of two random variables is also equal to the variance of their difference, if
they are uncorrelated.

98
Expectations and More
Conditional Expectation and Variance
Given a conditional density function fX|Y(x|y) of joint random variables X and Y, we may form the
conditional expectation of X given Y as
E[X|Y = y] ≡E[X|y] =
 ∞
−∞
x fX|Y(x|y)dx =
 ∞
−∞x fX,Y(x, y)dx
fY(y)
, 0 < fY(y) < ∞.
More generally, the conditional expectation of a function h of X and Y is given by
E[h(X, Y)|Y = y] =
 ∞
−∞
h(x, y) fX|Y(x|y)dx.
When X and Y are discrete random variables, we have
E[X|Y = y] =

all x
x Prob{X = x|Y = y) =

all x
xpX|Y(x|y),
and if h is a function of X and Y, then
E[h(X, Y)|Y] =

all x
h(x, y)pX|Y(x|y).
These results lead to the law of total expectation:
E[X] = E[ E[X|Y] ],
i.e., the expectation of the conditional expectation of X given Y is equal to the expectation of X,
which is quite a mouthful. For continuous random variables, we have
E[X] = E[ E[X|Y] ] =
 ∞
−∞
E[X|Y = y] fY(y)dy,
while for discrete random variables
E[X] = E[ E[X|Y] ] =

y
E[X|Y = y]Prob{Y = y}.
We prove this in the discrete case only, the continuous case being analogous. We have
E[X] = E[ E[X|Y] ] =

y
E[X|Y = y] Prob{Y = y}
=

y

x
x Prob{X = x|Y = y)

Prob{Y = y}
=

x
x

y
Prob{X = x|Y = y)Prob{Y = y}

=

x
x Prob{X = x} = E[X],
since, by the law of total probability, 
y Prob{X = x|Y = y)Prob{Y = y} = Prob{X = x}.
For two random variables X and Y, the conditional variance of Y given X = x is deﬁned as
Var [Y|X] = E[( Y −E[Y|X] )2 | X],

5.2 Expectation of Functions and Joint Random Variables
99
or by the following equivalent formulation which is frequently more useful,
Var [Y|X] = E[Y 2|X] −( E[Y|X] )2.
We also have
Var [Y] = E [Var [Y|X]] + Var [E[Y|X]] .
Example 5.13 Let X and Y be two continuous random variables whose joint probability density
function is given by
fX,Y(x, y) =

1 −(x + y)/3,
0 ≤x ≤1, 0 ≤y ≤2,
0
otherwise.
We wish to ﬁnd E[X|Y = 0.5] and E[Y|X = 0.5]. This requires a knowledge of the marginal
distributions of X and Y as well as the conditional marginal distributions so we begin by computing
these quantities. We have
fX(x) =
 ∞
−∞
fX,Y(x, y)dy =
 2
y=0
[1 −(x + y)/3] dy = (y −xy/3 −y2/6)
2
0
= 4/3 −2x/3,
0 ≤x ≤1,
and
fY(y) =
 ∞
−∞
fX,Y(x, y)dx =
 1
x=0
[1 −(x + y)/3] dx = (x −xy/3 −x2/6)
1
0
= 5/6 −y/3,
0 ≤y ≤2.
Also
fX|Y(x|0.5) = fX,Y(x, 0.5)
fY(0.5)
= 1 −x/3 −0.5/3
5/6 −0.5/3
= 5
4 −x
2 ,
fY|X(y|0.5) = fX,Y(0.5, y)
fX(0.5)
= 1 −0.5/3 −y/3
4/3 −1/3
= 5
6 −y
3 .
The required results may now be formed. We have
E[X|Y = 0.5] =
 ∞
−∞
x fX|Y(x|0.5)dx =
 1
0
x
5
4 −x
2

dx =
5x2
8
−x3
6

1
0
= 11
24,
E[Y|X = 0.5] =
 ∞
−∞
y fY|X(y|0.5)dy =
 2
0
y
5
6 −y
3

dy =
5y2
12 −y3
9

2
0
= 7
9.
Example 5.14 Let us return to Example 5.11 and ﬁnd E[Y|x]. For convenience, we rewrite the
joint density function and the previously computed result for the marginal density of X. These were
fX,Y(x, y) =
6x,
0 < x < y < 1,
0
otherwise,
and
fX(x) = 6x(1 −x),
0 ≤x ≤1,
respectively. We may now compute E[Y|x] as
E[Y|x] =
 ∞
−∞
y fY|X(y|x) dy =
 1
y=x
6xy
6x(1 −x) dy =
1
1 −x
 1
y=x
y dy =
1
1 −x
1 −x2
2

= 1 + x
2
.

100
Expectations and More
Properties of the Expectation and Variance
Some of the most important properties of the expectation of random variables X and Y are collected
together and given below.
1. E[1] = 1. Indeed, for any constant c, E[c] = c.
2. If c is a constant, then E[cX] = cE[X].
3. | E[X] | ≤E[ |X| ].
4. If X ≥0, then E[X] ≥0. Furthermore, if X ≤Y, then E[X] ≤E[Y].
5. Linearity: If X and Y are two random variables, not necessarily independent, then
E[X + Y] = E[X] + E[Y].
6. If X and Y are independent random variables, then E[XY] = E[X]E[Y].
7. If X1, X2, . . . is an increasing or decreasing sequence of random variables converging to X,
then E[X] = limi→∞E[Xi].
A number of other important properties can be deduced from these. For example, from
properties 2 and 5, we have
E
( n

i=1
ci Xi
)
=
n

i=1
ci E[Xi].
In particular, for given constants a and b,
E[aX + b] = aE[X] + b.
Taking a = 0 and b = 1 yields property 1 again. Taking a = 1 and b = −E[X] gives
E[X −E[X]] = 0,
and thus the ﬁrst central moment must always be zero. Finally, the frequently used formula for the
variance of a random variable, namely,
Var [X] = E[X2] −(E[X])2,
(5.5)
may be derived from these properties. We have
Var [X] = E[(X −E[X])2] = E[X2 −2X E[X] + (E[X])2]
= E[X2] −2E[X]E[X] + (E[X])2 = E[X2] −(E[X])2.
Some properties of the variance are given below.
1. Var [1] = 0.
2. Var [X] = E[X2] −(E[X])2.
3. Var [cX] = c2Var [X].
4. If X and Y are independent random variables, then
Var [X + Y] = Var [X] + Var [Y].
5. If X and Y are uncorrelated random variables, then
Var [X −Y] = Var [X + Y].
5.3 Probability Generating Functions for Discrete Random Variables
Given a discrete random variable X that assumes only nonnegative integer values {0, 1, 2, . . .} with
pk = Prob{X = k}, we may deﬁne the probability generating function of X by
G X(z) =
∞

i=0
pizi = p0 + p1z + p2z2 + · · · + pkzk + · · · .

5.3 Probability Generating Functions for Discrete Random Variables
101
G X(z) is also called the z-transform of X and may be shown to converge for any complex number z
for which |z| < 1. Since ∞
i=0 pi = 1, it follows that G X(1) = 1.
Example 5.15 The random variable R denoting the number of heads obtained in three tosses of a
fair coin has generating function given by
G R(z) =
3

i=0
pizi = z0 1
8 + z1 3
8 + z2 3
8 + z3 1
8 = 1
8(1 + 3z + 3z2 + z3).
When z = 1 we ﬁnd
G R(1) = 1
8(1 + 3 + 3 + 1) = 1.
The probability distribution of a discrete random variable X is uniquely determined by its
generating function. It follows then that if two discrete random variables X and Y have the same
probability generating function, then they must also have the same probability distribution and
probability mass function. Observe that the kth derivative of a probability generating function
evaluated at z = 0, can be used to obtain the probability pk. We have
1
k!G(k)
X (0) = pX(k), k = 0, 1, 2, . . . .
Example 5.16 (continued from Example 5.15). From the second derivative of G R(z) evaluated at
z = 0 we obtain
p2 = 1
2!
d2
dz2
1
8(1 + 3z + 3z2 + z3)

z=0
= 1
16(6 + 6z)

z=0
= 6
16 = 3
8,
the probability of getting a single head in three tosses of a fair coin.
In addition to selected probabilities, expectations and higher moments of a random variable may
be obtained directly from the probability generating function by differentiation and evaluation at
z = 1. We have
E[X] =
∞

k=0
kpk = lim
z→1
 ∞

k=0
kpkzk−1

= lim
z→1 G′
X(z) = G′
X(1).
Example 5.17 (continued from Example 5.15). For the random variable R, we have
E[R] = d
dz
1
8(1 + 3z + 3z2 + z3)

z=1
=
1
8(3 + 6z + 3z2)

z=1
= 1
8(3+6+3) = 12
8 = 1.5.
Higher moments are obtained from higher derivatives. Observe that
E[X] =
∞

k=0
kpk,
E[X(X −1)] = E[X2] −E[X] =
∞

k=0
k(k −1)pk,
E[X(X −1)(X −2)] = E[X3] −3E[X2] + 2E[X] =
∞

k=0
k(k −1)(k −2)pk,
...
E[X(X −1) · · · (X −n + 1)] =
∞

k=0
k(k −1) · · · (k −n + 1)pk.

102
Expectations and More
These are called the factorial moments of X. The nth derivative of the probability generating function
of X evaluated at z = 1 gives the nth factorial moment. For example,
G′′
X(z) =
∞

k=0
k(k −1)zk−2 pk,
and hence
G′′
X(1) = lim
z→1 G′′
X(z) = lim
z→1
∞

k=0
k(k −1)zk−2 pk =
∞

k=0
k(k −1)pk = E[X(X −1)].
In general, the kth derivative of G X(z) evaluated at z = 1 provides the kth factorial moment, i.e.,
G(k)
X (1) = E[X(X −1) · · · (X −k + 1)].
This provides a rather straightforward procedure for computing regular moments once factorial
moments have been computed. For example, since E[X(X −1)] = E[X2] −E[X], then given the
second factorial moment and the expectation, the second regular moment may be computed as
E[X2] = E[X] + E[X(X −1)].
Example 5.18 Let X be a discrete random variable with probability mass function
pk = pX(k) =
⎧
⎨
⎩

n
k

pkqn−k,
0 ≤k ≤n,
0
otherwise,
where p + q = 1. The corresponding probability generating function is
G X(z) =
n

k=0
zk

n
k

pkqn−k =
n

k=0

n
k

(zp)kqn−k = (zp + q)n.
Taking the ﬁrst and second derivatives with respect to z, we obtain
G′
X(z) = n(zp + q)n−1 p and G′′
X(z) = (n −1)n(zp + q)n−2 p2.
Evaluating these at z = 1 and relating them to the ﬁrst two factorial moments, we obtain
E[X] = G′
X(1) = np and
E[X(X −1)] = G′′
X(1) = (n −1)np2.
It follows that
E[X2] = (n −1)np2 + np = (np)2 −np2 + np.
The variance of X may now be computed as
Var [X] = E[X2] −E[X]2 = (np)2 −np2 + np −(np)2 = np(1 −p) = npq.
Observe that for ﬁxed z, the probability generating function G X(z) is just the mathematical
expectation of the random variable h(X) = zX, i.e.,
G X(z) = E[zX], |z| ≤1,
since from the previous properties of the expectation we have
E[h(X)] =
∞

−∞
h(X)pX(x) =
∞

k=0
zXProb{X = k} =
∞

k=0
zk pk.

5.4 Moment Generating Functions
103
Thus, if X1, X2, . . . , Xn are n independent random variables taking the values 0, 1, 2, . . . , then, for
ﬁxed z, the random variables zXi for i = 1, 2, . . . , n are also independent, and it follows that
E[zX1+X2+···+Xn] = E[zX1zX2 · · · zXn] = E[zX1]E[zX2] · · · E[zXn],
and hence
G X(z) = G X1(z)G X2(z) · · · G Xn(z).
(5.6)
This expresses the generating function of a sum of random variables (X = X1 + X2 + · · · + Xn)
in terms of the generating functions of the individual terms of the summand. To compute the
probability mass function of a sum of discrete random variables that are independent, it sufﬁces
therefore to form the probability generating function for each of them, to compute the product
of these generating functions and then to determine the required probabilities by continuous
differentiation of the product of the generating functions.
Example 5.19 Let X be the random variable that denotes the number of heads obtained when two
coins are simultaneously tossed and let Y be the random variable that denotes the number of spots
obtained when a fair die is thrown. Let us ﬁnd the probability generating function of X + Y. First
we compute the probability generating function of both X and Y. Since pX(0) = pX(2) = 1/4;
pX(1) = 1/2 and pY(k) = 1/6 for k = 1, 2, . . . , 6, we have
G X(z) = 1
4 + z
2 + z2
4
and
GY(z) = z + z2 + z3 + · · · + z6
6
.
Therefore
G X+Y(z) = G X(z)GY(z) = z
24 + z2
8 + z3 + z4 + z5 + z6
6
+ z7
8 + z8
24.
Finally, when X1 and X2 are two independent discrete-valued random variables and the derived
random variable X is equal to X1 with probability r and equal to X2 with probability (1 −r), then
G X(z) = r G X1(z) + (1 −r) G X2(z).
5.4 Moment Generating Functions
The moment generating function is deﬁned for both discrete and continuous random variables. For
a given random variable X, its moment generating function, written as MX(θ), is deﬁned for real
values of θ, as
MX(θ) = E[eθ X].
It exists if this expectation is ﬁnite for all θ in an open interval around zero, i.e., if the expectation
exists for all −c < θ < c, for some positive constant c. The range of values of θ for which MX(θ)
exists is called the region of convergence.
If X is a discrete random variable, then from our previous deﬁnition of expectation,
MX(θ) =

all x
eθx pX(x).
A similar result holds for continuous random variables. Let fX(x) be the density function of a
continuous random variable, X. Then the moment generating function of X is
MX(θ) =
 ∞
−∞
eθx fX(x)dx,
(5.7)
providing the integral exists.

104
Expectations and More
Example 5.20 Let X be a discrete random variable whose probability mass function is
pX(x) =

1/n,
x = 1, 2, . . . , n,
0
otherwise.
The moment generating function of X is given by
MX(θ) =

all x
eθx pX(x) =
n

x=1
eθx
n
= 1
n (eθ + e2θ + · · · + enθ) = 1
n
eθ −e(n+1)θ
1 −eθ

.
Example 5.21 Let X be a continuous random variable whose probability density function is
fX(x) =

λe−λx,
x ≥0,
0
otherwise,
where λ > 0. The moment generating function of X is
MX(θ) =
 ∞
0
eθxλe−λxdx = λ
 ∞
0
e(θ−λ)xdx.
This integral exists only if θ < λ and this deﬁnes the range of convergence of this moment
generating function. For θ < λ we ﬁnd
MX(θ) =
λ
θ −λe(θ−λ)x

∞
0
=
λ
λ −θ .
It should be apparent that there is a connection between the probability generating function,
G X(z), of a discrete random variable X deﬁned in the previous section, and its moment generating
function, MX(θ). Speciﬁcally, we have
MX(θ) = G X(eθ).
Indeed, given the probability generating function of X, we may immediately write its moment
generating function, using the above equation.
Example 5.22 We saw in Example 5.18 that the probability generating function for the random
variable X with probability mass function
pk = pX(k) =
⎧
⎨
⎩
n
k

pkqn−k,
0 ≤k ≤n,
0
otherwise,
where p + q = 1 is given by
G X(z) = (pz + q)n.
It must follow that the moment generating function of X is given by
MX(θ) = (peθ + q)n.
Observe that, if we expand the exponential in Equation (5.7) and take expectations, we ﬁnd
MX(θ) =
 ∞
−∞

1 + θx + (θx)2
2!
+ (θx)3
3!
+ · · ·

fX(x)dx
=
 ∞
−∞
fX(x)dx + θ
 ∞
−∞
x fX(x)dx + θ2
2!
 ∞
−∞
x2 fX(x)dx + · · ·
= 1 + θ E[X] + θ2E[X2]
2!
+ θ3E[X3]
3!
+ · · · .

5.4 Moment Generating Functions
105
Hence
MX(0) = 1
and the kth moment of X is obtained from the kth derivative evaluated at θ = 0,
dk
dθk MX(θ)

θ=0
= E[Xk], k = 1, 2, . . . ,
which justiﬁes the name of moment generating function. From a practical point of view, when
computing moments of a random variable X, it is frequently easier to ﬁrst ﬁnd MX(θ) and then to
compute the moments by differentiation as described above, than the alternative “ﬁrst principles”
approach of integrating xn fX(x).
Example 5.23 Let us return to Example 5.21 and compute the ﬁrst, second, and higher moments of
X. Previously we computed the moment generating function of X to be
MX(θ) =
λ
λ −θ .
Hence
E[X] = dMX(θ)
dθ

θ=0
=
λ
(λ −θ)2

θ=0
= 1
λ
and
E[X2] = d2MX(θ)
dθ2

θ=0
=
2λ
(λ −θ)3

θ=0
= 2
λ2 .
By continuous differentiation with respect to θ,
dk
dθk MX(θ) =
λk!
(λ −θ)k+1
for k = 1, 2, . . . .
Hence, setting θ = 0,
E[Xk] = k!
λk .
The same approach applies to discrete random variables as illustrated in the next example.
Example 5.24 The probability mass function of a discrete random variable X is given by
pX(x) =
⎧
⎪
⎨
⎪
⎩
1 −p,
x = 0,
p,
x = 1,
0
otherwise.
Then
MX(θ) =

all x
eθx pX(x) = e0(1 −p) + eθ p = 1 −p + peθ.
Hence
E[X] = dMX(θ)
dθ

θ=0
= peθ
θ=0 = p
and
E[X2] = d2MX(θ)
dθ2

θ=0
= peθ
θ=0 = p,
and the same for all higher moments.

106
Expectations and More
Moment generating functions turn out to be especially useful in analyzing sums of independent
random variables. If X1 and X2 are two independent random variables and Y = X1 + X2, then
MY(θ) = E[eθ(X1+X2)] = E[eθ X1eθ X2] = E[eθ X1]E[eθ X2] = MX1(θ)MX2(θ).
In words, the moment generating function of a sum of two independent random variables is equal to
the product of their generating functions. The proof follows directly as a consequence of the result
E[X1X2] = E[X1]E[X2] for independent random variables X1 and X2 and the additional fact that
if X1 and X2 are independent, then the derived random variables eθ X1 and eθ X2 are also independent.
This result generalizes directly to the sum of n independent random variables. If X1, X2, . . . , Xn are
n independent random variables and Y = X1 + X2 + · · · + Xn, then
MY(θ) = E[eθ X1eθ X2 · · · eθ Xn] = MX1(θ)MX2(θ) · · · MXn(θ).
(5.8)
If, in addition to being independent, the n random variables are also identically distributed, then
MXi(θ) = MX(θ) for all i = 1, 2, . . . , n and
MX(θ) = [MX(θ)]n.
Example 5.25 Let X1 and X2 deﬁned below be two discrete random variables that are independent.
pX1( j) =
⎧
⎪
⎨
⎪
⎩
0.4,
j = 1,
0.6,
j = 2,
0
otherwise,
pX2(k) =
⎧
⎪
⎨
⎪
⎩
0.2,
k = −1,
0.8,
k = 4,
0
otherwise.
Let Y = X1 + X2. We wish to ﬁnd the expectation of Y and its probability mass function. From the
deﬁnition, MX(θ) = 
all x eθx pX(x), we have
MX1(θ) = 0.4eθ + 0.6e2θ
and MX2(θ) = 0.2e−θ + 0.8e4θ,
and hence
MY(θ) =
#
0.4eθ + 0.6e2θ$
×
#
0.2e−θ + 0.8e4θ$
= 0.08 + 0.12eθ + 0.32e5θ + 0.48e6θ,
The expectation of Y is given by
E[Y] = d
dθ MY(θ)

θ=0
=
#
0.12eθ + 5(0.32)e5θ + 6(0.48)e6θ$
θ=0 = 4.6.
The probability mass function can be found directly from the previously computed value of MY(θ)
and matching coefﬁcients.
MY(θ) =

all y
eθy pY(y) = pY(0) + pY(1)eθ + pY(2)e2θ + · · ·
= 0.08 + 0.12eθ + 0.32e5θ + 0.48e6θ.
We ﬁnd
pY(y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0.08,
y = 0,
0.12,
y = 1,
0.32,
y = 5,
0.48,
y = 6,
0
otherwise.

5.4 Moment Generating Functions
107
Example 5.26 Consider a random variable Y with probability density function
fY(y) =

λ2ye−λy,
y ≥0,
0
otherwise,
and whose parameter λ is strictly greater than zero. Then
MY(θ) =
 ∞
0
eθyλ2ye−λydy = λ2
 ∞
0
ye−(λ−θ)ydy =
λ2
(λ −θ)
 ∞
0
y(λ −θ)e−(λ−θ)ydy.
Using integration by parts with u = y and dv = (λ −θ)e−(λ−θ)ydy, and observing the constraint
λ > θ, we obtain
 ∞
0
y(λ −θ)e−(λ−θ)ydy = uv|∞
0 −
 ∞
0
vdu = ye−(λ−θ)y∞
0 −
 ∞
0
e−(λ−θ)ydy =
1
λ −θ .
It follows then that
MY(θ) =
λ2
(λ −θ)2 =

λ
λ −θ
2
.
However, previously we saw that a random variable having probability density function
fX(x) =
	
λe−λx,
x ≥0,
0
otherwise,
had moment generating function given by
MX(θ) =
λ
λ −θ .
It must follow that the random variable Y is equal to the sum of two independent and identically
distributed random variables X1 and X2 with
fX1(x) = fX2(x) =
	
λe−λx,
x ≥0,
0
otherwise.
Moment generating functions satisfy the linear translation property . If α and β are constants and
Y = αX + β, then
MY(θ) = eβθMX(αθ).
To see this, observe that
MY(θ) = E[eθY] = E[eθ(αX+β)] = E[eβθeαθ X] = eβθ E[eαθ X] = eβθMX(αθ).
The following is a list of the properties of moment generating functions that were presented in
this section:
• MX(θ) = G X(eθ), when X is a discrete random variable.
•
M(k)
X (θ)

θ=0 = E[Xk], for k = 1, 2, . . ..
• MX+Y(θ) = MX(θ)MY(θ), for independent random variables X and Y.
• MY(θ) = eβθMX(αθ), for Y = αX + β.
The moment generating function is closely related to the Laplace transform . The Laplace
transform of a real-valued function h(t), t ≥0, is generally denoted by h∗(s) and is deﬁned as
h∗(s) =
 ∞
0
e−sxh(x)dx,

108
Expectations and More
where s, unlike θ in the moment generating function, may be a complex number. This deﬁnition
applies to functions h(x) that need not be density functions, and indeed the Laplace transform is
found to be useful in many domains other than statistics and probability. In the case that h(t), t > 0,
does in fact correspond to a density function, then the Laplace transform is equal to the moment
generating function of X evaluated at −s. In our current context of random variables, it is usual to
denote the Laplace transform as LX(s) and we have
LX(s) ≡h∗(s) = MX(−s).
It follows then that
LX(s) = E[e−sX]
and
E[Xk] = (−1)k dk
dsk LX(s)

s=0
for k = 1, 2, . . . .
Example 5.27 Making the substitution s = −θ in Example 5.21, it immediately follows that
the Laplace transform of the continuous random variable X with probability density function
fX(x) = λe−λx, x ≥0 and zero otherwise, is given by
LX(s) =
λ
λ + s .
There is one other function of a random variable X that warrants our attention, namely, the
characteristic function, deﬁned as
CX(θ) = E[eiθ X],
where i
= √−1. It is related to the moment generating function since CX(θ) = MX(iθ)
and in contexts outside probability and statistics, is called the Fourier transform. Although the
characteristic function has the disadvantage of involving complex numbers, it has two important
advantages over moment generating functions. First, the characteristic function is ﬁnite for all
random variables and all real values of θ and second, an inversion formula exists which may
facilitate the construction of the probability mass function for a discrete random variable, or the
probability density of a continuous random variable, if it indeed exists. The characteristic function
possesses properties that are similar to those that are listed above for the moment generating
function.
5.5 Maxima and Minima of Independent Random Variables
Let X1, X2, . . . , Xn be n independent random variables whose cumulative distribution functions are
given by FX1(x), FX2(x), . . . , FXn(x), respectively. We wish to compute the distribution functions
that are deﬁned as the maximum and minimum of these n random variables. In other words, we seek
the distribution function of the random variables Xmax and Xmin deﬁned as
Xmax = max(X1, X2, . . . , Xn) and
Xmin = min(X1, X2, . . . , Xn).
Observe ﬁrst that Xmax ≤x if and only if Xi ≤x for all i = 1, 2, . . . , n and therefore
Fmax(x) = Prob{X1 ≤x, X2 ≤x, . . . , Xn ≤x}.
Hence, from our assumption that the n random variables are independent, it follows that
Fmax(x) =
n
i=1
Prob{Xi ≤x} =
n
i=1
FXi(x).

5.5 Maxima and Minima of Independent Random Variables
109
Example 5.28
Let us compare the expected time to perform three data-dependent tasks on a
multiprocessor computer system to the expected time to perform these tasks on single processor.
Let X1, X2, and X3 be the random variables that describe the times needed to perform the three
tasks, and, due to data dependence, let us further assume that each has an identical (exponential)
cumulative distribution given by
FXi = 1 −e−t/4,
i = 1, 2, 3.
Thus, the mean processing time for each of the three tasks is given by E[Xi] = 4. Let X be
the random variable that describes the time needed to complete all three tasks. Since all three
tasks on the multiprocessor will ﬁnish as soon as the last of them has been completed, we have
X = max{X1, X2, X3}. Thus the cumulative distribution function of X is
FX(t) = (1 −e−t/4)3
and its density function, obtained by differentiating FX(t), is given by
fX(t) = 3(1 −e−t/4)2 (1/4)e−t/4 = 3
4e−t/4(1 −e−t/4)2.
As a side note, observe that the distribution of the maximum of a set of exponentially distributed
random variables is not exponential. The expectation of X may now be computed. Keeping in mind
the previously computed result,
 ∞
0
λte−λt = 1
λ,
t ≥0,
we obtain
E[X] = 3
4
 ∞
0
te−t/4(1 −e−t/4)2dt
= 3
 ∞
0
t
4e−t/4(1 −2e−t/4 + e−t/2)dt
= 3
 ∞
0
t
4e−t/4dt −
 ∞
0
t
2e−t/2dt + 1
3
 ∞
0
3t
4 e−3t/4dt

= 3

4 −2 + 4
9

= 7.3333.
Notice that this computed expected value for the last of the three tasks to complete, namely, 7.3333,
is quite a bit longer than the expected time to complete any one of the tasks, namely, 4.
On a single processor, the random variable that describes the total time required is now given by
X = X1 + X2 + X3 and we compute its expectation simply as
E[X] =
3

i=1
E[Xi] = 12.
Now consider the random variable Xmin = min(X1, X2, . . . , Xn). In this case, it is easier to ﬁrst
compute Prob{Xmin > x} and to form Fmin(x) from the relationship
Fmin(x) = 1 −Prob{Xmin > x}.
Notice that Xmin > x if and only if Xi > x for all i = 1, 2, . . . , n and therefore
Prob{Xmin > x} = Prob{X1 > x, X2 > x, . . . , Xn > x}.

110
Expectations and More
Once again, reverting to the independence of the n random variables X1, X2, . . . , Xn, we have
Prob{Xmin > x} =
n
i=1
Prob{Xi > x} =
n
i=1
(1 −Fi(x))
and so
Fmin(x) = 1 −
n
i=1
(1 −Fi(x)) .
Example 5.29 Let
us ﬁnd the distribution of the time until the ﬁrst of the three tasks on the
multiprocessor computer system of the previous example ﬁnishes. The random variable X that
describes this situation is given by X = min{X1, X2, X3} and its cumulative distribution function is
given by
FX(t) = 1 −e−t/4e−t/4e−t/4 = 1 −e−3t/4,
t ≥0.
Thus the mean time for the ﬁrst task to ﬁnish is equal to 4/3 which is considerably less than the
mean of any of the individual tasks!
Notice, in the above example, that the distribution obtained as the minimum of the set of three
exponential distributions is itself an exponential distribution. This holds true in general: unlike the
maximum of a set of exponential distributions, the minimum of a set of exponential distributions is
itself an exponential distribution.
5.6 Exercises
Exercise 5.1.1 Find the median, mode, and expected value of the discrete random variable whose cumulative
distribution function is as follows:
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
x < −1,
1/8,
−1 ≤x < 0,
1/4,
0 ≤x < 1,
1/2,
1 ≤x < 2,
1,
x ≥2.
Exercise 5.1.2 The probability mass function of a discrete random variable X is as follows:
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0.1,
x = −2,
0.2,
x = 0,
0.3,
x = 2,
0.4,
x = 5,
0
otherwise.
Find the expectation, second moment, variance, and standard deviation of X.
Exercise 5.1.3 A random variable X takes values 0, 1, 2, . . . , n, . . . with probabilities
1
2, 1
3, 1
32 , 1
33 , · · · , 1
3n , · · · .
Show that this represents a genuine probability mass function. Find E[X], the expected value of X.
Exercise 5.1.4 Two fair dice are thrown. Let X be the random variable that denotes the number of spots shown
on the ﬁrst die and Y the number of spots that show on the second die. It follows that X and Y are independent
and identically distributed. Compute E[X 2] and E[XY] and observe that they are not the same.
Exercise 5.1.5 Balls are drawn from an urn containing w white balls and b black balls until a white ball
appears. Find the mean value and the variance of the number of balls drawn, assuming that each ball is replaced
after being drawn.

5.6 Exercises
111
Exercise 5.1.6 Find the expectation, second moment, variance, and standard deviation of the continuous
random variable with probability density function given by
fX(x) =
3x2/16,
−2 ≤x ≤2,
0
otherwise.
Exercise 5.1.7 Let X be a continuous random variable with probability density function
fX(x) =
1/10,
2 ≤x ≤12,
0
otherwise.
Find the expectation and variance of X.
Exercise 5.1.8 The probability density function of a random variable X is given as
fX(x) =
8/x3,
x ≥2,
0
otherwise.
Find the expectation, second moment, and variance of X.
Exercise 5.1.9 The joint probability density function of two random variables X and Y is given by
fX,Y(x, y) =
sin x sin y,
0 ≤x ≤π/2, 0 ≤y ≤π/2,
0
otherwise.
Find the mean and variance of the random variable X.
Exercise 5.1.10 A stockbroker is interested in studying the manner in which his clients purchase and sell
stock. He has observed that these clients may be divided into two equal groups according to their trading
habits. Clients belonging to the ﬁrst group buy and sell stock very quickly (day traders and their ilk); those in
the second group buy stocks and hold on to them for a long period. Let X be the random variable that denotes
the length of time that a client holds a given stock. The stockbroker observes that the holding time for clients
of the ﬁrst type has mean m1 and variance λ; for those of the second type, the corresponding numbers are m2
and μ, respectively. Find the variance of X in terms of m1, m2, λ, and μ.
Exercise 5.2.1 The probability mass function of a discrete random variable X is
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0.2,
x = −2,
0.3,
x = −1,
0.4,
x = 1,
0.1,
x = 2,
0
otherwise.
Find the expectation of the following functions of X:
(a) Y = 3X −1,
(b) Z = −X,
and (c) W = |X|.
Exercise 5.2.2 The cumulative distribution function of a continuous random variable X is given as
FX(x) =
1 −e−2x,
0 < x < ∞,
0
otherwise.
Find the probability density function and expectation of the derived random variable Y = eX.
Exercise 5.2.3 Let X and Y be two continuous random variables with probability density functions
fX(x) =
1/5,
−3 ≤x ≤2,
0
otherwise,
and
fY(y) =
8/y3,
x ≥2,
0
otherwise.
Find the expectation of the random variable Z = 4X −3Y.

112
Expectations and More
Exercise 5.2.4 Let X and Y be two discrete random variables with joint probability mass function
X = −1
X = 0
X = 1
Y = −1
0.15
0.05
0.20
Y = 0
0.05
0.00
0.05
Y = 1
0.10
0.05
0.35
Find the expectation of Z = XY and W = X + Y.
Exercise 5.2.5 Let X and Y be two random variables whose joint probability distribution function is given
below.
X = 1
X = 2
X = 3
X = 4
Y = −1
c
0
0
0
Y = 0
a
2a
2a
a
Y = 1
b
2b
2b
b
(a) Under what conditions does this table represent a proper joint distribution function for the random
variables X and Y?
(b) Compute the expectation of the random variables X and Y.
(c) Compute E[Y 2] and E[(Y −E[Y])2].
Exercise 5.2.6 The joint probability density function of two random variables X and Y is given by
fX,Y(x, y) =
2,
0 ≤x ≤y ≤1,
0
otherwise.
Find the following quantities:
(a) E[X] and E[Y], (b) Var [X] and Var [Y], (c) E [XY] and Cov [X, Y], and (d) E[X + Y] and Var [X + Y].
Exercise 5.2.7 Let X and Y be two random variables whose joint probability density function is
fX,Y(x, y) =
x + y,
0 < x < 1, 0 < y < 1,
0
otherwise.
Find E[XY], Cov[X, Y], Corr [X, Y], E[X + Y], and Var [X + Y].
Exercise 5.2.8 Let X, Y, and Z be three discrete random variables for which
Prob{X = 1, Y = 1, Z = 0} = p,
Prob{X = 1, Y = 0, Z = 1} = (1 −p)/2,
Prob{X = 0, Y = 1, Z = 1} = (1 −p)/2,
where 0 < p < 1. Determine the covariance matrix for X and Y.
Exercise 5.2.9 The joint density function of two continuous random variables X and Y is given below.
fX,Y(x, y) =
9x2y2,
0 ≤x ≤1, 0 ≤y ≤1,
0
otherwise.
Find E[Y|x].
Exercise 5.2.10 The joint probability density function of X and Y is given by
fX,Y(x, y) =
1/2,
−1 ≤x ≤y ≤1,
0
otherwise.
Find fX|Y(x|y), fY|X(x|y), E[X|y], and E[Y|x].

5.6 Exercises
113
Exercise 5.2.11 Let X and Y be two random variables. Prove the following:
(a) E[XY] = E[X]E[Y], when X and Y are independent.
(b) E [ E[Y|X] ] = E[Y] for jointly distributed random variables.
(c) Var (αX + β) = α2X for constants α and β.
(d) For jointly distributed random variables X and Y, show that the variance of Y is equal to the sum of the
expectation of the conditional variance of Y given X and the variance of the conditional expectation
of Y given X, i.e., that
Var [Y] = E [Var [Y|X]] + Var [E[Y|X]] .
Exercise 5.3.1 A discrete random variable X takes the value 1 if the number 6 appears on a single throw of a
fair die and takes the value 0 otherwise. Find its probability generating function.
Exercise 5.3.2 Find the probability generating function of a discrete random variable with probability mass
function given by
pX(k) = qk−1 p,
k = 1, 2, . . . ,
where p and q are probabilities such that p + q = 1. We shall see later that this is called the geometric
distribution function.
Exercise 5.3.3 Let X be a discrete random variable with probability mass function given by
pX(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1/10,
x = 1,
2/10,
x = 2,
3/10,
x = 3,
4/10,
x = 4,
0
otherwise.
Find the probability generating function of X and of 2X. Now ﬁnd Prob{2X = 3} and E[2X].
Exercise 5.3.4 Consider a discrete random variable X whose probability generating function is given by
G X(z) = ez −e + 2 −z.
Observe that when z = 1, G X(z) = 1. Find the probabilities pX(0), pX(1), pX(2), pX(3), and pX(4). What do
you suppose is the probability of pX(k)? Find E[X] and Var [X].
Exercise 5.3.5 The probability mass function of a random variable X is given by
pk = Prob{X = k) = pX(k) =
⎧
⎨
⎩
n
k

pkqn−k,
0 ≤k ≤n,
0
otherwise,
where p and q are nonzero probabilities such that p + q = 1. This distribution is referred to as the binomial
distribution with parameters n and p. Find the probability generating function of X. Also ﬁnd the probability
generating function of the sum of m such random variables, Xi, i = 1, 2, . . . , m, with parameters ni and p,
respectively.
Exercise 5.3.6 Consider a discrete random variable X whose probability generating function is given by
pk = pX(k) =
αke−α/k!,
k = 0, 1, 2, . . . , α > 0,
0
otherwise.
This distribution is referred to as the Poisson distribution with parameter α. Find the probability generating
function of X. Also ﬁnd the probability generating function of the sum of m such random variables, Xi,
i = 1, 2, . . . , m, with parameters αi, respectively.
Exercise 5.4.1 Find the moment generating function of the discrete random variable X with probability mass
function given in Exercise 5.3.3. Derive the value of E[X] from this moment generating function.
Exercise 5.4.2 Find the moment generating function of the discrete random variable X with probability mass
function given in Exercise 5.3.2. Also ﬁnd E[X] and Var [X].

114
Expectations and More
Exercise 5.4.3 Consider a discrete random variable X whose probability mass function is given by
pX(k) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
e,
k = 0,
0,
k = 1,
1/k!,
k = 2, 3, . . . ,
0
otherwise.
Find the moment generating function of X and from it compute E[X].
Exercise 5.4.4 The moment generating function of a discrete random variable X is
MX(θ) = eθ
12 + e3θ
3 + e6θ
6 + e9θ
3 + e12θ
12 .
Find the probability mass function of X.
Exercise 5.4.5 The probability density function of a continuous random variable X is given by
fX(x) =
1/(b −a),
0 ≤x ≤b,
0
otherwise.
Find the moment generating function of X and from it the expectation E[X].
Exercise 5.4.6 The probability density function of a continuous random variable X is given by
fX(x) = α
2 e−α|x|,
−∞< x < ∞,
for α > 0. Prove that fX(x) is indeed a density function. Find the moment generating function of X and E[X].
(A random variable having this density function is called a Laplace random variable.)
Exercise 5.5.1 Three individuals of equal ability set out to run a one mile race. Assume that the number of
minutes each needs to run a mile is a random variable whose cumulative distribution function is given by
FX(x) =
⎧
⎨
⎩
0,
x ≤5,
(x −5)/4,
5 ≤x ≤9,
1,
x ≥9.
Find the cumulative distribution of the number of minutes it takes for all three to have passed the ﬁnish line,
and compute the expectation of this distribution. What is the probability that all three runners ﬁnish in less than
six minutes?
Exercise 5.5.2 Using the scenario of Exercise 5.5.1, ﬁnd the distribution of the time it takes for the ﬁrst runner
to arrive, and its expected value. What is the probability that this time exceeds seven minutes?
Exercise 5.5.3 Alice and Bob, working with an architect, have drawn up plans for their new home, and are
ready to send them out to the best builders in town to get estimates on the cost of building this home. At
present they have identiﬁed ﬁve different builders. Their architect has told them that the estimates should be
uniformly distributed between $500,000 and $600,000. What is the cumulative distribution and the expectation
of the lowest estimate, assuming that all builders, and hence their estimates, are independent? Compute the
probability that the lowest estimate does not exceed $525,000. What would this distribution, expectation, and
probability be, if Alice and Bob were able to identify 20 builders? What conclusions could be drawn concerning
the size of the lowest estimate and the number of builders?
Exercise 5.5.4 The shelf lives of three different products all have the same type of cumulative distribution
function, namely,
FX(x) =
1 −e−αx,
x ≥0,
0
otherwise,
where α > 0 is different for each of the three products: more speciﬁcally, the different values of α are given
by 1/2, 1/3, and 1/6. Find the cumulative distribution function and the expectation of the random variable that
characterizes the time until the shelf life of one of these three products expires and that product needs to be
replaced. You should assume that the shelf life of each product is independent of the others.

Chapter 6
Discrete Distribution Functions
On a number of occasions in previous chapters, we alluded to random variables of one sort or
another and to their distribution functions by speciﬁc names, such as exponential, geometric, and
so on. It frequently happens in practice, that the distribution functions of random variables fall into
different families and that one random variable differs from another only in the value assigned to
a single parameter, or in the value of a small number of parameters. For example, the probability
density function of one continuous random variable may be given by f (x) = 2e−2x for x ≥0 and
zero otherwise while for another random variable it may be f (x) = e−x/2/2 for x ≥0 and zero
otherwise. Both random variables belong to the family of (exponential) random variables whose
probability density function may be written f (x) = λe−λx for x ≥0, zero otherwise, and having
λ > 0: they differ only in the value assigned to λ. Being able to associate a given random variable
as a member of a family of random variables gives us a greater understanding of the situation
represented by the given random variable. Furthermore, known results and theorems concerning
that family of random variables can be directly applied to the given random variable.
In this chapter we consider some of the most common distribution functions of discrete random
variables; in the next, we turn our attention to those of continuous random variables.
6.1 The Discrete Uniform Distribution
Let X be a discrete random variable with ﬁnite image {x1, x2, . . . , xn}. The uniform distribution is
obtained when each value in the image has equal probability and, since there are n of them, each
must have probability equal to 1/n. The uniform probability mass function is given as
pX(xi) = Prob{X = xi} =

1/n,
i = 1, 2, . . . , n,
0
otherwise.
The corresponding cumulative distribution function is given by
FX(t) =
⌊t⌋

i=1
pX(xi) = ⌊t⌋
n
for
1 ≤t ≤n,
while Fx(t) = 0 for t < 1 and FX(t) = 1 for t > n.
With the discrete uniform distribution, it often happens that the xi are equally spaced. In this
case, a bar graph of the probability mass function of this distribution will show n equally spaced
bars all of height 1/n, while a plot of the cumulative distribution function will display a perfect
staircase function of n identical steps, each of height 1/n, beginning at zero just prior to the ﬁrst
step at x1 and rising to the value 1 at the last step at xn.
In the speciﬁc case in which xi = i for i = 1, 2, . . . , n, the moments of the discrete uniform
distribution are given by
E[Xk] =
n

i=1
ik/n.

116
Discrete Distribution Functions
In particular, we have
E[X] =
n

i=1
i/n = 1
n
n

i=1
i = 1
n
n(n + 1)
2
= n + 1
2
,
and, using the well-known formula for the sum of the squares of the ﬁrst n integers,
E[X2] =
n

i=1
i2/n = 1
n
n

i=1
i2 = 1
n
n(n + 1)(2n + 1)
6

= (n + 1)(2n + 1)
6
.
The variance is then computed as
Var [X] = E[X2] −(E[X])2 = (n + 1)(2n + 1)
6
−
n + 1
2
2
= n2 −1
12
.
When the xi are n equally spaced points beginning at x1 = a and ending at xn = b = a + n −1, we
ﬁnd the corresponding formulae for the mean and variance to be respectively given by
E[X] = a + b
2
,
Var [X] = (b −a + 2)(b −a)
12
.
The probability generating function for a discrete uniformly distributed random variable X is
G X(z) =
n

i=1
zi/n = 1
n
n

i=1
zi = z(1 −zn)
n(1 −z) .
Its moment generating function MX(θ) is obtained by setting z = eθ, since MX(θ) = G X(eθ).
Example 6.1 An example of a discrete uniform random variable is the random variable that denotes
the number of spots that appear when a fair die is thrown once. Its probability mass function is given
by p(xi) = 1/6 if xi ∈{1, 2, . . . , 6} and is equal to zero otherwise.
6.2 The Bernoulli Distribution
A discrete random variable X that can assume only two values, zero and one (for example,
representing success or failure, heads or tails, etc.) gives rise to a Bernoulli distribution. It arises
from the realization of a single Bernoulli trial. Its probability mass function is given by
pX(0) = Prob{X = 0} = 1 −p,
pX(1) = Prob{X = 1} = p,
where 0 < p < 1. In our discussions on Bernoulli random variables, we shall often set q = 1 −p.
The cumulative distribution function of a Bernoulli random variable is given as
FX(x) =
⎧
⎨
⎩
0,
x < 0,
q,
0 ≤x < 1,
1,
x ≥1.
The probability mass and cumulative distribution functions are shown in Figure 6.1. The moments
are computed from
E[X j] = 0 jq + 1 j p = p for all j = 1, 2, . . . .
Also,
Var [X] = E[X2] −(E[X])2 = p −p2 = p(1 −p) = pq

6.3 The Binomial Distribution
117
x
x
F  (x)
X
X
0
0
1
1
1
1
p
q
q
p  (x)
Figure 6.1. Probability mass function and CDF of the Bernoulli distribution.
and
C2
X = pq
p2 = q
p .
The probability generating function for a Bernoulli random variable is given by
G X(z) = qz0 + pz1 = q + pz.
Its moment generating function is
MX(eθ) = q + peθ.
Example 6.2 A survey reveals that 20% of students at Tobacco Road High School smoke. A student
is chosen at random. Let X = 0 if that student smokes and X = 1 if the student does not smoke.
Then X is a Bernoulli distributed random variable with probability mass function given by
pX(x) =
⎧
⎨
⎩
1/5,
x = 0,
4/5,
x = 1,
0
otherwise.
6.3 The Binomial Distribution
The binomial distribution arises from a sequence of independent Bernoulli trials, with probability
of success equal to p on each trial. It is used whenever a series of trials is made for which
• Each trial has two mutually exclusive outcomes: called success and failure.
• The outcomes of successive trials are mutually independent.
• The probability of success, denoted by p, is the same on each trial.
An elementary event ω in a probability experiment consisting of n Bernoulli trials may be written
as a string of zeros and ones such as
011010100 . . . 010100101111
*
+,
-
n times
where a 0 in position j denotes failure at the jth trial and a 1 denotes success. Since the Bernoulli
trials are independent, the probability that an elementary event ω has exactly k successes (and n −k
failures) is given by pkqn−k.
Now let X be the random variable that describes the number of successes in n trials. The domain
of X, the set of all possible outcomes, is the set of all strings of length n that are composed of
zeros and ones, and its image is the set {0, 1, . . . , n}. For example, with n = 5, an element of
the domain is ω = 00110 and the value assumed by X is given as X(ω) = 2. In other words, the
value assigned by X to any outcome is the number of 1’s (i.e., the number of successes) it contains.

118
Discrete Distribution Functions
Thus, X(ω) = k if there are exactly k ones in ω. It follows then that the number of distinct outcomes
with k successes is equal to the number of distinct sequences that contain k ones and n −k zeros.
This number is given by the binomial coefﬁcient
C(n, k) =

n
k

=
n!
k!(n −k)!.
These C(n, k) elementary events all have probability equal to pkqn−k and hence
Prob{X = k} = C(n, k)pkqn−k.
Thus the probability mass function of a binomial random variable X is given by
b(k; n, p) = Prob{X = k) = pX(k) =
⎧
⎨
⎩
n
k

pkqn−k,
0 ≤k ≤n,
0
otherwise.
A random variable X having this probability mass function is called binomial random variable.
Its probability mass function is denoted by b(k; n, p) and gives the probability of k successes in n
independent Bernoulli trials with probability of success at each trial being equal to p. The binomial
theorem may be used to show that this deﬁnes a proper probability mass function, (and thereby
lends its name to the distribution). We have
n

k=0
pk =
n

k=0

n
k

pk(1 −p)n−k = [p + (1 −p)]n = 1.
The interested reader may wish to draw some plots of the probability mass function of a binomial
random variable. When p is close to 0.5, the distribution tends to be symmetric about the midpoint
(n/2); as the value of p moves further and further away from 1/2, the distribution becomes
more and more skewed. The size of n also has an effect on the shape of the distribution: larger
values of n result in more symmetry. As a rule of thumb, approximately 95% of the distribution
of a binomial random variables falls in an interval of ±2σX (two standard deviations) of its
expectation.
The cumulative distribution function of a binomial random variable is denoted by B(t; n, p). It
has the value 0 for t < 0; the value 1 for t > n, while for 0 ≤t ≤n,
B(t; n, p) =
⌊t⌋

k=0

n
k

pk(1 −p)n−k.
Values of the binomial cumulative distribution function have been tabulated for some values of n
and p and are available in many texts. In using these tables, the following identities may prove
useful:
B(t; n, p) = 1 −B(n −t −1; n, 1 −p),
b(t; n, p) = B(t; n, p) −B(t −1; n, p).
Since a binomial random variable X is the sum of n mutually independent Bernoulli random
variables, Xi, i = 1, 2, . . . , n, i.e.,
X =
n

i=1
Xi,

6.3 The Binomial Distribution
119
we have, using the linear property of expectation and variance,
E[X] =
n

i=1
E[Xi] = np,
Var [X] =
n

i=1
Var [Xi] = npq,
C2
X = npq
n2 p2 = q
np .
Finally, from Equation (5.6), we may compute the probability generating function of the binomial
distribution as
G X(z) =
n
i=1
G Xi(z) = (q + pz)n = [1 −p(1 −z)]n,
since the generating function for each Bernoulli random variable Xi is, as we have just seen, q + pz.
Its moment generating function is found from MX(θ) = G X(eθ).
Example 6.3 A grocery store receives a very large number of apples each day. Since past experience
has shown that 5% of them will be bad, the manager chooses six at random for quality control
purposes. Let X be the random variable that denotes the number of bad apples in the sample. What
values can X assume? For each possible value of k, what is the value of pX(k)? In particular, what
is the probability that the manager ﬁnds no bad apples?
The number of bad apples found by the manager (and hence the value assumed by the random
variable X) can be any number from 0 through 6. The probability associated with any value k is the
same as that of obtaining k “successes” in six trials with a probability of success equal to 0.05, i.e.,
from the binomial probability mass function as
b(k; 6, 0.05) = C(6, k) × 0.05k × 0.956−k.
The following is a table of the values of pX(k).
k
0
1
2
3
4
5
6
pX(k)
0.7351
0.2321
0.03054
0.002143
8.4609 × 10−5
1.78125 × 10−6
1.5625 × 10−8
Thus the probability that the manager ﬁnds no bad apples is 0.7351.
This question may be posed in many different forms, as, for example, in terms of the transmission
of binary digits through a communication channel that is subject to error, i.e., a bit is transmitted
in error with probability p (equal to 0.05 in our bad apple example) and is independent of previous
errors. The probability of an error-free transmission is computed in just the same way as that used
to compute the probability that the manager ﬁnds no bad apples.
The binomial distribution may be used in polling situations to gauge the sentiments of voters prior
to an election. A subset of size n of the population is chosen and their view of a particular candidate
running for ofﬁce, or their opinion of a bond issue, etc., is solicited. Each person providing an
opinion may be viewed as a Bernoulli random variable and since we shall assume that the opinions
of the respondents are independent, the polling experiment may be viewed as a binomial random
variable, X. However, in this situation, the actual number of respondents in favor of the candidate is
less important that the percentage of them who favor the candidate and so it is the induced random
variable Y = X/n that is most useful. Since X is a binomial random variable with expectation

120
Discrete Distribution Functions
E[X] = np and Var [X] = np(1 −p), the expectation and variance of Y are given by
E[Y] = E[X/n] = E[X]/n = p,
Var [Y] = Var [X/n] = Var [X]/n2 = p(1 −p)
n
.
This now begs the question of the accuracy of the polling results. We have formed the random
variable Y and wish to know how well it performs in computing the correct percentage of the
population in favor of the candidate, p. A good indication may be obtained from the 2σ bounds.
Observe that the variance, and hence the standard deviation, is maximized when p = 1/2 (a simple
calculus exercise shows that the maximum of p(1−p) occurs when p = 1/2). The maximum value
of σX is √1/4n and the 2σ bound cannot exceed 1/√n. Therefore, if the sample size is 100, it is
likely that the value obtained from the polling results differ from the true value by no more than
±1/
√
100 = ±0.1, while if the sample size is 900, they differ from the true result by no more than
±0.033. Putting this in terms of our political candidate, if the poll of 900 voters shows that 45% of
the population supports this candidate, then that candidate should expect to receive between 42%
and 48% of the vote. We shall return to this concept of polling in a later chapter, when we shall
examine it in much more detail.
6.4 Geometric and Negative Binomial Distributions
Like a binomial random variable, a geometric random variable can be associated with a sequence of
Bernoulli trials. However, instead of counting the number of successes in a ﬁxed number of trials,
a geometric random variable counts the number of trials up to, and including, the ﬁrst success; a
modiﬁed geometric random variable counts the number of trials before the ﬁrst success. A discrete
random variable that has a negative binomial distribution, counts the number of trials up to and
including the kth success. We begin with the geometric distribution.
The Geometric Distribution
Let X be a geometric random variable with parameter p, 0 < p < 1, the probability of success
in a single Bernoulli trial. If 0 denotes failure and 1 success, then the (inﬁnite) sample space of a
geometric probability experiment consists of all sequences of a string of 0’s followed by a single 1,
i.e., {1, 01, 001, 0001, . . .}. The geometric random variable X assumes the value of the number of
digits in any string. For example, X(0001) = 4 and X(000001) = 6. Its image is the set of integers
greater than or equal to 1, and its probability mass function is given by
pX(n) =

p(1 −p)n−1,
n = 1, 2, . . . ,
0
otherwise.
(see Figure 6.2). The probability pX(n) = p(1 −p)n−1 = qn−1 p is that of obtaining a sequence of
n −1 failures (each with probability q = 1 −p) followed by a single success (with probability p).
Thus the random variable X denotes the index of the ﬁrst success in a random experiment consisting
of a number of independent trials each with probability of success equal to p and probability of
failure equal to q = 1 −p. Notice that, using the formula for the sum of a geometric series (hence
the name), we have
∞

n=1
pX(n) =
∞

n=1
pqn−1 =
p
1 −q = 1.

6.4 Geometric and Negative Binomial Distributions
121
k
k
2
3
4
1
2
4
3
5
6
7
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
1
5
6
7
p = 0.25
p = 0.5
p  (k)
X
p  (k)
X
Figure 6.2. The geometric probability mass function for two values of p.
Since
Prob{X ≤t} =
⌊t⌋

n=1
p(1 −p)n−1 = p 1 −(1 −p)⌊t⌋
1 −(1 −p)
= 1 −(1 −p)⌊t⌋= 1 −q⌊t⌋for t ≥0,
the cumulative distribution function for a geometric random variable is
FX(t) =

0,
t ≤0,
1 −q⌊t⌋,
t ≥0.
The expectation and variance of a geometric random variable are easily found by differentiating
its probability generating function to obtain factorial moments. The ﬁrst (factorial) moment is the
expectation, while the second (factorial) moment and the expectation can be used to form the
variance. However, we shall leave this as an exercise and instead we shall compute the expectation
and variance directly from their deﬁnitions so that we may introduce an effective way to evaluate
series of the form ∞
n=1 n jqn with 0 < q < 1 and j ≥1. We do so by associating n jqn with its
jth derivative with respect to q and interchanging the summation and the derivative. This procedure
will become apparent in the computation of the expectation (with j = 1) and variance (with j = 2)
of a geometric random variable.
In computing the expectation, we use the fact that nqn−1 = d(qn)/dq and proceed as follows:
E[X] =
∞

n=1
npqn−1 = p
∞

n=1
nqn−1 = p
∞

n=0
d
dq qn = p d
dq
∞

n=0
qn = p d
dq
1
1 −q =
p
(1 −q)2 = 1
p .
The variance is computed in a similar manner. This time using n(n −1)qn−2 = d2(qn)/dq2, we ﬁnd
E[X2] =
∞

n=1
n2 pqn−1 =
∞

n=1
n(n −1)pqn−1 +
∞

n=1
npqn−1 = q
∞

n=1
n(n −1)pqn−2 +
∞

n=1
npqn−1
= pq d2
dq2
∞

n=1
qn + p d
dq
∞

n=1
qn = pq d2
dq2
q
1 −q + p d
dq
q
1 −q
= pq
2
(1 −q)3 + p
1
(1 −q)2 = 2pq
p3 + p
p2 = 2q + p
p2
= 1 + q
p2 .
Hence
Var [X] = E[X2] −E[X]2 = 1 + q
p2
−1
p2 = q
p2 = 1 −p
p2
.

122
Discrete Distribution Functions
We may now form the squared coefﬁcient of variation for a geometric random variable. It is given
as
C2
X = 1 −p.
To compute the probability generating function, we have
G X(z) =
∞

n=1
pqn−1zn = p
q
∞

n=1
(qz)n = p
q ×
qz
1 −qz =
pz
1 −(1 −p)z .
Its moment generating function is
MX(eθ) =
peθ
1 −(1 −p)eθ .
The geometric distribution is the only discrete probability distribution that has the Markov or
memoryless property, something that we shall address (and prove) in a later chapter. It implies that
a sequence of n −1 unsuccessful trials has no effect on the probability of getting a success on the
nth attempt.
Example 6.4 Locating a bug in a computer program involves running the program with randomly
selected data sets until the bug manifests itself. The probability that any particular data set isolates
the bug is known to be 0.2. It follows that the expectation and standard deviation of the number of
data sets needed to ﬁnd the bug are given by 1/0.2 = 5 and
.
(1 −0.2)/0.22 = 4.47, respectively.
Example 6.5 Consider a machine that at the start of each day either is in working condition or
has broken down and is waiting to be repaired. If at the start of any day it is working, then with
probability p1 it will break down. On the other hand, if at the start of the day, it is broken, then with
probability p2 it will be repaired. Let X denote the number of consecutive days that the machine is
found to be in working condition. Then X is geometrically distributed since the probability that the
machine is in working order at the beginning of exactly n consecutive days and breaks down during
the nth day is equal to (1−p1)n−1 p1. The mean and variance of the number of consecutive days that
the machine begins the day in working order is given by
E[X] = 1
p1
,
Var [X] = 1 −p1
p2
1
.
Similar results can be found concerning the number of days that the machine spends undergoing
repairs. In this case, the parameter used is p2. The term sojourn time is used to designate the time
spent in a given state and later during our analysis of discrete-time Markov chains we shall see that
the sojourn time in any state of a discrete-time Markov chain is geometrically distributed.
The Modiﬁed Geometric Distribution
Whereas a geometric random variable counts the number of trials up to and including the ﬁrst
success, a modiﬁed geometric random variable counts the number of trials before the ﬁrst success.
Its probability mass function is given by
pX(n) =
p(1 −p)n,
n = 0, 1, 2, . . . ,
0
otherwise.
Observe that the modiﬁed geometric distribution is just a regular geometric distribution, but deﬁned
on the set {0, 1, 2, . . .} instead of the set {1, 2, 3, . . .}. The cumulative distribution function of a
modiﬁed geometric random variable is
FX(t) =
⌊t⌋

n=0
p(1 −p)n = p
1 −(1 −p)⌊t⌋+1
1 −(1 −p)

= 1 −(1 −p)⌊t+1⌋
for t ≥0,

6.4 Geometric and Negative Binomial Distributions
123
and zero otherwise. Various characteristics are given by
E[X] = 1 −p
p
,
Var [X] = 1 −p
p2
,
and
C2
X =
1
1 −p .
Its probability generating function and moment generating functions are, respectively,
G X(z) =
p
1 −(1 −p)z
and
MX(θ) =
p
1 −(1 −p)eθ .
Example 6.6 Professor Don Bitzer keeps a large jar full of jellybeans in his ofﬁce and each day he
eats a certain number of them. It is well known that once you start eating jellybeans, it is difﬁcult
to stop (and Professor Bitzer is no exception to this rule) so some days he does not even start eating
them. So, before eating each jellybean (including the ﬁrst), Professor Bitzer considers the situation
and with probability p decides to eat no more. If X is the number of jellybeans he eats on a typical
day, then X is a modiﬁed geometric random variable whose probability mass function is
pX(n) =
p(1 −p)n,
n = 0, 1, 2, . . . ,
0
otherwise.
If we take p = 0.04, since jellybeans are hard to resist, then the average number he eats on a typical
day is E[X] = (1 −p)/p = 0.96/0.04 = 24, and the probability he eats more than ten on a typical
day is given by
Prob{X > 10} = 1 −Prob{X ≤10} = 1 −

1 −(1 −p)10+1 
= (1 −p)11 = (0.96)11
= 0.6382.
The Negative Binomial Distribution
Let us now turn our attention to random variables that have a negative binomial distribution. Such
random variables count the number of Bernoulli trials, with parameter p, up to and including the
occurrence of the kth success. Thus, a geometric random variable is a negative binomial random
variable with k = 1. The probability mass function of a negative binomial random variable X with
parameters p and k is given by
pX(n) =

n −1
k −1

pk(1 −p)n−k,
k ≥1,
n = k, k + 1, . . . .
(6.1)
If the kth success is to occur on exactly trial n, then there must have been exactly k −1 successes
scattered among the ﬁrst n −1 trials. The probability of obtaining a success on the kth trial is p
and the probability of obtaining k −1 successes in the ﬁrst n −1 trials, obtained from the binomial
formula, is

n −1
k −1

pk−1(1 −p)(n−1)−(k−1).
The product of these two probabilities gives Equation (6.1).
To ﬁnd the expectation and variance of a negative binomial random variable, X, it is convenient
to consider X as the sum of k independent geometric random variables, X1, X2, . . . , Xk, each with
expectation 1/p and variance (1 −p)/p2. In this context X1 represents the number of trials up to
and including the ﬁrst success, X2 represents the number of trials from the ﬁrst success up to and
including the second and so on, up to Xk which represents the number of trial from success k −1

124
Discrete Distribution Functions
up to and including success number k. Then
E[X] =
k

i=1
E[Xi] = k
p ,
Var [X] =
k

i=1
Var [Xi] = k(1 −p)
p2
.
Example 6.7 The chance of winning a prize at a certain booth at the state fair is 0.15 and each
attempt costs $1. What is the probability that I will win a prize for each of my four children with
my last $10 bill?
Let X denote the (negative binomial) random variable which gives the number of attempts up
to and including the kth success. The $10 bill gives me n = 10 attempts to win k = 4 prizes. The
probability of winning the fourth prize on my tenth try can be computed from the negative binomial
distribution as
Prob{X = 10} =

10 −1
4 −1

(0.15)4 (0.85)6 = 0.0160.
The probability that I will need at least $7 may be found by subtracting the probability of taking
four, ﬁve, or six attempts to win four prizes from 1, i.e.,
Prob{X ≥7} = 1 −
6

i=4
Prob{X = i}
= 1 −

3
3

(0.15)4 (0.85)0 −

4
3

(0.15)4 (0.85)1 −

5
3

(0.15)4 (0.85)2
= 1 −0.0005 −0.0017 −0.0073 = 0.9905,
rather large odds, indeed. In fact, the average number of dollars I had need to spend to win four
prizes is given by
E[X] = k
p =
4
0.15 = 26.67.
6.5 The Poisson Distribution
The Poisson probability mass function, denoted as f (k, α), is given by
f (k, α) = pX(k) =

αke−α/k!,
k = 0, 1, 2, . . . , α > 0,
0
otherwise.
Notice that this deﬁnes a genuine probability mass function since
∞

k=0
f (k, α) = e−α
∞

k=0
αk
k! = e−αeα = 1.
The cumulative distribution function is given by
F(k) = Prob{X ≤k} = e−α
k

j=0
α j
j! .
A Poisson process, extensively used in queueing theory and discussed in that context in a later
chapter, is a counting process in which the number of events that occur within a given time period

6.5 The Poisson Distribution
125
has a Poisson distribution. If λ is the rate at which these events occur, and t is the time period
over which we observe these events, then the parameter of interest is λt which, to all intents and
purposes, is the number of events that occurred in time t. In this case we set α = λt.
Example 6.8 Customers arrive at a queueing system according to a Poisson distribution at a rate of
12 customers per hour.
• What is the probability that exactly six customers will arrive in the next 30 minutes?
• What is the probability that three or more customers will arrive in the next 15 minutes?
• What is the probability that two, three, or four customers will arrive in the next 5 minutes?
In problems of this nature, we begin by setting α = λt. For all three parts of the question we have
λ = 12 customers per hour and hence the parameter for the Poisson distribution is 12t. Therefore
Prob{Xt = k} = (12t)k
k!
e−12t.
(a) Since the question is set in units of an hour, we take t = 0.5 for 30 minutes and get
Prob{Exactly 6 customers in next 30 minutes} = Prob{X0.5 = 6} = 66
6! e−6 = 0.1606.
(b) The probability of getting three or more arrivals is equal to 1 minus the probability of having
zero, one, or two arrivals in the next 15 minutes. So, using λt = 12 × (1/4) = 3, we have
1 −Prob{0, 1, 2 arrivals } = 1 −e−3

1 + 3
1 + 32
2!

= 0.5768.
(c) Since ﬁve minutes is equal to 1/12 hours, the parameter α is now equal to 1. We have
Prob{2, 3, 4 customers} =
4

k=2
(1)k
k! e−1 = e−1 17
24 = 0.2606.
We now derive the expectation and variance of a Poisson random variable. Its expectation is
given by
E[X] =
∞

k=0
kpX(k) = e−α
∞

k=0
k αk
k!
= αe−α
∞

k=1
αk−1
(k −1)! = αe−α
∞

j=0
α j
j! = αe−αeα = α.
Therefore the expected number of events that occur in (0, t] is equal to α = λt. For the variance,
we ﬁrst compute the (factorial) moment
E[X(X −1)] =
∞

k=0
k(k −1)pX(k) = e−α
∞

k=0
k(k −1)αk
k!
= e−αα2
∞

k=2
αk−2
(k −2)! = e−αα2
∞

k=0
αk
k! = α2 = (λt)2.
Now we form the variance as follows:
σ 2
X = E[X2] −E2[X] = (E[X(X −1)] + E[X]) −(E[X])2 = α2 + α −α2 = α.

126
Discrete Distribution Functions
Thus, the mean and variance of a Poisson random variable are identical. The expectation and
variance may also be conveniently computed from the probability generating function, which we
now derive. We have
G X(z) = E[zX] =
∞

k=0
zk pX(k) =
∞

k=0
e−α (αz)k
k!
= e−α+αz = eα(z−1).
The moment generating function of a Poisson random variable is given by
MX(eθ) = eα(eθ−1).
The probability mass function of a Poisson random variable may be viewed as the limiting case
of the probability mass function of a binomial random variable b(k; n, p), when n is large and np is
moderately sized. Such is the case when the number of trials, n, is large, the probability of success,
p, is small (so that each success is a rare event), but nevertheless the average number of successes,
np, is moderate. This happens, for example, if as n →∞then p →0 in such a way that their
product np = α remains constant. We proceed as follows. Let X be a discrete binomial random
variable such that np = α. Then
Prob{X = k} =
n!
(n −k)!k! pk(1 −p)n−k =
n!
(n −k)!k!
α
n
k 
1 −α
n
n−k
= n(n −1) · · · (n −k + 1)
nk
αk
k!
 
1 −α
n
n 
1 −α
n
−k
.
Now letting n tend to inﬁnity, we ﬁnd
lim
n→∞
n(n −1) · · · (n −k + 1)
nk
= 1,
lim
n→∞

1 −α
n
n
= e−α
and
lim
n→∞

1 −α
n
−k
= 1.
Hence, assuming that p →0 and np = α is constant as n →∞, we obtain
lim
n→∞Prob{X = k} = e−α αk
k! ,
which is a Poisson probability mass function.
Example 6.9 In a lottery, a total of N = 1,000 tickets are printed of which M = 5 are winning
tickets. How many tickets should I buy to make the probability of winning equal to p = 3/4.
The probability that any given ticket is a winning ticket is given by M/N = 1/200. Thus, each
ticket that I buy can be considered as a separate trial with a probability of success equal to 1/200.
If I buy n tickets, then this corresponds to a series of n independent trials. Since the probability of
winning is rather small (1/200) and the probability that I would like to have for winning is rather
high (3/4), it is clear that a rather large number of tickets must be bought (n must be large). It
follows that the number of winning tickets among those purchased is a random variable with ap-
proximately Poisson distribution. The probability that there are exactly k winning tickets among
the n purchased is
pX(k) = Prob{k winning tickets} = αk
k! e−α
where α = n×(M/N) = n/200. The probability that at least one of the tickets is a winning ticket is
1 −P(0) = 1 −e−α,

6.6 The Hypergeometric Distribution
127
and so the number of tickets that must be bought to have a probability of 3/4 to have a winning
ticket is the smallest integer n satisfying
e−n/200 ≤1 −3/4 = 0.25
Some representative values are provided in the table below.
n
100
200
277
278
300
500
1,000
2,000
e−n/200
0.6065
0.3679
0.2503
0.2491
0.2231
0.0821
0.0067
0.00004540
The answer in this case is given by n = 278. Notice that the approximations we have made result
in the erroneous situation that even if I buy all tickets, I am still not guaranteed to win!
6.6 The Hypergeometric Distribution
The hypergeometric distribution arises in situations in which objects fall into two distinct categories,
such as black and white, good and bad, male and female, etc. Suppose a collection consists of N
such objects, r of which are of type 1 and N −r of type 2. Now let n of the N objects be chosen at
random and without replacement from the set. The random variable X that denotes the number of
type 1 from the selected n, is a hypergeometric random variable and its probability mass function is
given by
pX(k) =
r
k
 N −r
n −k


N
n

,
k = max(0, n −N + r), . . . , min(n,r),
and is equal to zero otherwise.
The expectation and variance of a hypergeometric random variable X, with parameters n, N,
and r, are given by
E[X] = n
 r
N

,
Var [X] = n
 r
N
 
1 −r
N
  N −n
N −1

.
Example 6.10 A box contains twelve white balls and eight black balls. Seven are chosen at random.
Let X denote the number of white balls in this chosen set of seven. The probability that only six of
the chosen seven are white is found by taking the ratio of the number of ways that six white balls
and one black ball can be chosen from the box to the total number of ways of selecting seven balls
from the box. This gives
Prob{X = 6} =
12
6
 8
1


20
7

= 0.09536.
This same result is obtained by substituting the values N = 20, n = 7, r = 12, and k = 6 into the
probability mass function given previously. The probability of getting k white balls is
Prob{X = k} =

12
k
 
8
7 −k

20
7

,
k = 0, 1, . . . , 7.

128
Discrete Distribution Functions
The expected number of white balls chosen and the standard deviation are
E[X] = n
 r
N

= 7 × 12
20 = 4.2,
σX =
/
n
 r
N
 
1 −r
N
  N −n
N −1

=
/
4.2
 8
20
 13
19

=
√
1.1495 = 1.0721.
If n = 14 balls are selected rather than seven, the permissible values of k run from k =
max(0, n −N + r) = 6 to k = min(n,r) = 12 and the distribution of white balls is
Prob{X = k} =
12
k
 
8
14 −k


20
14

,
k = 6, 7, . . . , 12.
The hypergeometric distribution allows us to be more precise in answering certain types of
questions that we previously answered using the binomial distribution: questions that pertain
to determining the probability of ﬁnding a certain number of defective parts from a pool of
manufactured parts, for example, or the probability of ﬁnding bad apples in a consignment of apples.
When faced with such questions, we assumed that the probability of choosing a defective part was
always the same. However, this is not the case. Suppose ten apples in a batch of 1000 are bad. Then
the probability of choosing one bad apple is 1/100; the probability of choosing two bad apples is
1/100 × 9/999 and not (1/100)2, since the selection is made without replacement.
Example 6.11 If a jar contains 1000 jellybeans, 200 of which are red, and if I choose eight at
random, then the probability that I get exactly three red ones among the eight selected is
8
3

×
& 200
1000
199
999
198
998
 800
997
799
996
798
995
797
994
796
993
'
.
The term in the square brackets may be approximated by (0.2)3 (0.8)5 which is what is obtained
from the binomial distribution under the assumption that the probability of choosing a red jellybean
is always p = 2/10.
The key phrase is that, in situations involving a hypergeometric random variable, the selection
is made without replacement, which indeed is what happens frequently in practice. The binomial
distribution can be an extremely useful and easy to use approximation, when the size of the selection
is small compared to the size of the population from which the selection is made. However, it
remains an approximation.
6.7 The Multinomial Distribution
The multinomial distribution arises in probability experiments that involve n independent and
identical trials in which each outcome falls into one and only one of k mutually exclusive classes. We
seek the distribution of the n trials among the k classes. Let pi, i = 1, 2, . . . , k, be the probability
that an outcome falls into class i and let Xi be the random variable that denotes the number of out-
comes that fall into class i, i = 1, 2, . . . , k. The multinomial probability mass function is given by
pX1,X2,...,Xk(n1, n2, . . . , nk) =
n!
n1!n2! · · · nk! pn1
1 pn2
2 · · · pnk
k
subject to k
i=1 ni
= n and k
i=1 pi
= 1. Like the binomial distribution, and unlike the
hypergeometric distribution, the multinomial distribution is associated with the concept of selection

6.7 The Multinomial Distribution
129
with replacement, since the probabilities pi, i = 1, 2, . . . , k, are constant and do not change with
the trial number.
The distributions of the k random variables Xi, i = 1, 2, . . . , k, may be computed from this joint
probability mass function and their expectations and variances subsequently found. In fact each Xi
is a binomial random variable with parameter pi, since Xi simply provides a count of the number
of successes in (i.e., the number of times that an outcome falls into) class i. We have
E[Xi] = npi
and
Var [Xi] = npi(1 −pi).
Example 6.12
Assume that mountain climbing expeditions fail as a result of three mutually
exclusive events. With probability 0.6 an expedition fails due to adverse weather conditions; with
probability 0.25, it fails due to climber injuries, while with probability 0.15 the failure is due to
insufﬁcient or lost equipment. We shall label these failure causes 1, 2, and 3, respectively. On a
recent summer, four expeditions failed in their attempt to climb La Meije. Let N1, N2, and N3 be
three random variables that denote the number of these four failures that were due to causes 1, 2,
and 3, respectively.
The joint distribution of N1, N2, and N3 is the multinomial distribution and is given by
pN1,N2,N3(n1, n2, n3) =
4!
n1!n2!n3! pn1
1 pn2
2 pn3
3 =
4!
n1!n2!n3!0.6n10.25n20.15n3
for 0 ≤ni ≤4, i = 1, 2, 3 with n1 + n2 + n3 = 4, and is equal to zero otherwise.
Thus, the probability that two failures were due to inclement weather and the other two were due
to equipment failures (causes 1 and 3) is given by
pN1,N2,N3(2, 0, 2) =
4!
2! 0! 2!0.620.2500.152 = 0.0486,
while the probability that all four failures were due to bad weather is
pN1,N2,N3(4, 0, 0) =
4!
4! 0! 0!0.640.2500.150 = 0.1296.
Observe that the probability of exactly two failures being caused by adverse weather conditions (the
distribution of the other two causes is now irrelevant) is given by the binomial distribution—since
we are looking for two successes (where here a success actually means a failure to reach the summit)
out of four tries. The probability of exactly n1 failures due to adverse weather conditions is
pN1(n1) =
4!
n1! (4 −n1)!0.6n1(1 −0.6)4−n1
and so
pN1(2) =
4!
2! 2!0.62(0.4)2 = 0.3456.
Indeed, the complete marginal distribution of N1 is
pN1(n1) =
4!
n1! (4 −n1)!0.6n10.44−n1 =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0.44 = 0.0256,
n1 = 0,
4(0.6)0.43 = 0.1536,
n1 = 1,
6(0.6)2(0.4)2 = 0.3456,
n1 = 2,
4(0.6)30.4 = 0.3456,
n1 = 3,
0.64 = 0.1296,
n1 = 4.
Its expectation and variance are
E[X1] = np1 = 2.4,
Var [X1] = np1(1 −p1) = 0.96.

130
Discrete Distribution Functions
6.8 Exercises
Exercise 6.1.1 The number of letters, X, delivered to our home each day is uniformly distributed between 3
and 10. Find
(a) The probability mass function of X.
(b) Prob{X < 8}.
(c) Prob{X > 8}.
(d) Prob{2 ≤X ≤5}.
Exercise 6.2.1 A bag contains six white balls and twelve black balls. A probability experiment consists of
choosing a ball at random from the bag and inspecting its color. Identify this situation with the Bernoulli
random variable and give its probability mass function. What is the expected value of the random variable and
what does this mean in terms of the probability experiment?
Exercise 6.3.1 A school has 800 students. What is the probability that exactly four students were born on July
31? (Assume 365 days in a year.)
Exercise 6.3.2 The probability of winning a lottery is 0.0002. What is the probability of winning at least twice
in 1,000 tries?
Exercise 6.3.3 What is the probability of getting a passing grade of 70% on a ten-question true-false test, if
all answers are guessed? How does this probability change if the number of questions on the test is increased
to twenty?
Exercise 6.3.4 A manufacturer produces widgets which are sold in packets of 144. In the most recent batch of
21,600, it is estimated that 5% are defective. Let X be the random variable that denotes the number of defective
widgets in a packet. Compute the probability mass function of X under the simplifying assumption that for
every widget in the package the probability that it is defective is 5% (which essentially reduces to the case of
selection with replacement). What is the probability that a package contains more than ten defective widgets?
Exercise 6.3.5 A family has eight children. What is the probability of there being seven boys and one girl,
assuming that a boy is as likely to be born as a girl? What is the probability of there being between three and
ﬁve boys? How many children should be born to be 95% sure of having a girl?
Exercise 6.3.6 A scientiﬁc experiment is carried out a number of times in the hope that a later analysis of the
data ﬁnds at least one success. Let n be the number of times that the experiment is conducted and suppose
that the probability of success is p = 0.2. Assuming that the experiments are conducted independently from
one another, what is the number of experiments that must be conducted to be 95% sure of having at least one
success?
Exercise 6.3.7 A coin is tossed 400 times and the number of heads that appear is equal to 225. What is the
likelihood that this coin is biased?
Exercise 6.3.8 A political candidate polls 100 likely voters from a large population and ﬁnds that only 44 have
the intention of voting for him. Should he abandon his run for ofﬁce? What should he do if he polls 2,500 likely
voters and ﬁnds that 1,100 indicate a preference for him?
Exercise 6.4.1 Write down the probability generating function of a geometric random variable with parameter
p, 0 < p < 1, and use this to ﬁnd its expectation and variance.
Exercise 6.4.2 A basketball player at the free-throw line has a 80% chance of making the basket, a statistic
that does not change during the course of the game. What is the average number of free throws he makes before
his ﬁrst miss? What is the probability that his ﬁrst miss comes on his fourth try? On his ﬁfth try?
Exercise 6.4.3 Nicola, Stephanie, Kathryn, and William, in that order, take turns at rolling a fair die until one
of them throws a 6. What is the probability that William is the ﬁrst to throw a 6?
Exercise 6.4.4 Messages relayed over a communication channel have probability p of being received correctly.
A message that is not received correctly is retransmitted until it is. What value should p have so that the
probability of more than one retransmission is less than 0.05?

6.8 Exercises
131
Exercise 6.4.5
A young couple decides to have children until their ﬁrst son is born. Assuming that each
child born is equally likely to be a boy or a girl, what is the probability that this couple will have exactly four
children? What is the most probable range for the number of children this couple will have?
Exercise 6.4.6 A different couple from those of Exercise 6.4.5 intend to continue having children until they
have two boys. What is the probability they will have exactly two children? exactly three children? exactly four
children? What is the most probable range for the number of children this couple will have?
Exercise 6.4.7 Let X be an integer valued random variable whose probability mass function is given by
Prob{X = n} = αtn for all n ≥0,
where 0 < t < 1. Find G X(z), the value of α, the expectation of X, and its variance.
Exercise 6.4.8 The boulevards leading into Paris appear to drivers as an endless string of trafﬁc lights, one
after the other, extending as far as the eye can see. Fortunately, these successive trafﬁc lights are synchronized.
Assume that the effect of the synchronization is that a driver has a 95% chance of not being stopped at any
light, independent of the number of green lights she has already passed. Let X be the random variable that
counts the number of green lights she passes before being stopped by a red light.
(a) What is the probability distribution of X?
(b) How many green lights does she pass on average before having to stop at a red light?
(c) What is the probability that she will get through 20 lights before having to stop for the ﬁrst time?
(d) What is the probability that she will get through 50 lights before stopping for the fourth time?
Exercise 6.4.9 A popular morning radio show offers free entrance tickets to the local boat show to the sixth
caller who rings the station with the correct answer to a question. Assume that all calls are independent and
have probability p = 0.7 of being correct. Let X be the random variable that counts the number of calls needed
to ﬁnd the winner.
(a) What is the probability mass function of X?
(b) What is the probability of ﬁnding a winner on the 12th call?
(c) What is the probability that it will take more than ten calls to ﬁnd a winner?
Exercise 6.4.10
In a best of seven sports series, the ﬁrst person to win four games is declared the overall
winner. One of the players has a 55% chance of winning each game, independent of any other game. What is
the probability that this player wins the series?
Exercise 6.4.11 Returning to Exercise 6.4.10, ﬁnd the probability that the series ends after game 5. Once
again, assume that one of the players has a 55% change of winning each game, independent of any other game.
Exercise 6.4.12 Given the expansion
(1 −q)−k =
∞

i=0
i + k −1
k −1

qi,
show that Equation (6.1) deﬁnes a bona ﬁde probability mass function. The name negative binomial comes
from this expansion with its negative exponent, −k.
Exercise 6.5.1 Write down the probability generating function for a Poisson random variable X, and use it to
compute its expectation and variance.
Exercise 6.5.2 Let X be a Poisson random variable with mean value α = 3 that denotes the number of calls
per minute received by an airline reservation center. What is the probability of having no calls in a minute?
What is the probability of having more than three calls in a minute?
Exercise 6.5.3 During rush hours at a Paris metro station, trains arrive according to a Poisson distribution with
an expected number of ten trains per 60 minute period.
(a) What is the probability mass function of train arrivals in a period of length t minutes?
(b) What is the probability that two trains will arrive in a three minute period?
(c) What is the probability that no trains will arrive in a ten minute period?
(d) Find the probability that at least one train will arrive in a period of length t minutes and use this to
compute how long is needed to be 95% sure that a train will arrive?

132
Discrete Distribution Functions
Exercise 6.5.4 Five percent of the blood samples taken at a doctor’s ofﬁce need to be sent off for additional
testing. From the binomial probability mass function, what is the probability that among a sample of 160, ten
have to be sent off? What answer is obtained when the Poisson approximation to the binomial is used instead?
Exercise 6.5.5 On average, one widget in 100 manufactured at a certain plant is defective. Assuming that
defects occur independently, what is the distribution of defective parts in a batch of 48? Use the Poisson
approximation to the binomial to compute the probability that there is more than one defective part in a batch
of 48? What is the probability that there are more than two defective parts in a batch of 48?
Exercise 6.5.6 Consider a situation in which certain events occur randomly in time, such as arrivals to a
queueing system. Let X(t) be the number of these events that occur during a time interval of length t. We wish
to compute the distribution of the random variable X(t), under the following three assumptions:
(1) The events are independent of each other. This means that X(
t1), X(
t2), . . . are independent if the
intervals 
t1, 
t2, . . . do not overlap.
(2) The system is stationary, i.e., the distribution of X(
t) depends only on the length of 
t and not on
the actual time of occurrence.
(3) We have the following probabilities:
• Prob{at least 1 event in 
t} = λ
t + o(
t),
• Prob{more than 1 event in 
t} = o(
t).
where o(
t) is a quantity that goes to zero faster than 
t, i.e.,
lim

t→0
o(
t)

t
= 0,
and λ is a positive real number that denotes the rate of occurrence of the events. Show that X is a Poisson
random variable.
Exercise 6.5.7 Prove that the sum of n independent Poisson random variables Xi, i = 1, 2, . . . , n, with
parameters λ1, λ2, . . . , λn, respectively, is also Poisson distributed.
Exercise 6.6.1 I have four pennies and three dimes in my pocket. Using the hypergeometric distribution,
compute the probability that, on pulling two coins at random from my pocket, I have enough to purchase a
20-cent newspaper. Compute the answer again, this time using only probabilistic arguments.
Exercise 6.6.2 Let X be a hypergeometric random variable with parameters N = 12, r = 8, and n = 6.
(a) What are the possible values for X?
(b) What is the probability that X is greater than 2?
(c) Compute the expectation and variance of X.
Exercise 6.6.3 Last year, a wildlife monitoring program tagged 12 wolves from a population of 48 wolves.
This year they returned and captured 16.
(a) How many tagged wolves should they expect to ﬁnd among those captured?
(b) What is the probability of capturing exactly four tagged wolves?
(c) Suppose seven tagged wolves are captured. Should the wildlife team assume that the population of
wolves has increased, decreased or stayed the same over the past year?
Exercise 6.6.4 Cardinal Gibbons High School has 1,200 students, 280 of whom are seniors. In a random
sample of ten students, compute the expectation and variance of the number of included seniors using both the
hypergeometric distribution and its binomial approximation.
Exercise 6.7.1 A large jar of jellybeans contains 200 black ones, 300 brown ones, 400 green ones, 500 red
ones, and 600 yellow ones, all mixed randomly together. Use the multinomial distribution to estimate the
probability that a ﬁstful of 15 jellybeans contains 5 green, 5 red, and 5 yellow ones? Write an expression
(without evaluating it) for the probability of obtaining no black, brown, or green jellybeans in a ﬁstful of 15.
Exercise 6.7.2 Each Friday evening, Kathie and Billy go to one of four local pubs. They come to a decision
as to which one to go to on any particular Friday night by throwing a fair die. If the die shows 1, they go to
RiRa’s, if it shows 2, they go to Tir na n’Og, if it shows 3, they go to the Fox and Hound, while if it shows a

6.8 Exercises
133
number greater than 3, they go to their favorite, the Hibernian. What is the probability that during the 14 weeks
of summer, they visit RiRa’s twice, Tir na n’Og three times, the Fox and Hound three times, and the Hibernian
six times? What is the probability that during the four consecutive Fridays in June, they visit each pub exactly
once?
Exercise 6.7.3 Revels Tractor Works observes that riding lawnmowers break down for one of four different
(read mutually exclusive) reasons, which we simply label as type 1, 2, 3, and 4, with probabilities p1 = 0.1,
p2 = 0.2, p3 = 0.3, and p4 = 0.4, respectively. On Monday, ﬁve of these mowers are brought in for repair.
(a) Give an expression for the probability mass function PX1,X2,X3,X4(n1, n2, n3, n4), where Xi is the
number requiring repairs of type i, i = 1, 2, 3, 4.
(b) Use your knowledge of the binomial distribution to compute the probability that three require repairs
of type 4.
(c) What is the probability that three require repairs of type 4 and the other two require repairs of type 1?

Chapter 7
Continuous Distribution Functions
7.1 The Uniform Distribution
A continuous random variable X that is equally likely to take any value in a range of values (a, b),
with a < b, gives rise to the uniform distribution. Such a distribution is uniformly distributed on its
range. The probability density function of X is given as
fX(x) =
c,
a < x < b,
0
otherwise.
Thus the range is given by (a, b) and the value assumed by X on this range is c. This means that
c =
1
b −a
since the sum of probabilities must be 1, i.e.,
1 =
 ∞
−∞
fX(x)dx =
 b
a
c dx = c(b −a).
The cumulative distribution function is given by
FX(x) =
 x
−∞
fX(t)dt =
1
b −a t

x
−∞
=
⎧
⎨
⎩
0,
x ≤a,
(x −a)/(b −a),
a ≤x ≤b,
1,
x ≥b.
Figure 7.1 shows the cumulative distribution function and the probability density function of this
random variable. Observe that, if (r, s) is any subinterval of (a, b), the probability that X takes a
value in this subinterval does not depend on the actual position of the subinterval in (a, b) but only
on the length of the subinterval. In other words, we have
Prob{r < X < s} = FX(s) −FX(r) = s −r
b −a .
The values of fX(x) at the endpoints a and b do not affect the probabilities deﬁned by areas under
the graph.
b
a
a
1
1
b-a
F(x)
f(x)
x
b
x
Figure 7.1. The CDF and the probability density function of the continuous uniform distribution.

7.1 The Uniform Distribution
135
The mean of the uniform distribution is obtained as
E[X] =
 ∞
−∞
x f (x)dx = c
 b
a
xdx = a + b
2
.
Its variance is computed from
Var [X] = E[X2] −(E[X])2 =
 b
a
x2
b −a dx −
a + b
2
2
= (b −a)2
12
.
We may now compute the squared coefﬁcient of variation as
C2
X = (b −a)2
12
×
4
(b + a)2 = (b −a)2
3(b + a)2 .
Example 7.1 A continuous uniformly distributed random variable X has two parameters which are
the endpoints of its range (a, b). If E[X] = 5 and Var [X] = 3, we can generate two equations with
which to obtain a and b. We have
E[X] = a + b
2
= 5,
Var [X] = (b −a)2
12
= 3,
which gives a +b = 10 and (b −a)2 = 36. The second of these gives b −a = ±6 and if we assume
that a < b, we end up with two equations in two unknowns,
a + b = 10,
b −a = 6,
which we solve to obtain a = 2 and b = 8. The probability density function of X is given by
fX(x) =

1/6,
2 < x < 8,
0
otherwise.
The astute reader will have noticed that, although the expectation of a continuous random
variable X uniformly distributed on (a, b) is equal to that of a discrete random variable Y uniformly
distributed over the same interval, the same is not true for the variance. The variances in the
continuous and in the discrete case are respectively given by
Var [X] = (b −a)2
12
and Var [Y] = (b −a + 2)(b −a)
12
.
The variance of X is less than the variance of Y. Discretization is the process of deriving a discrete
random variable from a continuous one. Let a and b be integers with b > a and let j = b −a.
Let X be the continuous uniform distribution on (a, b). The discrete uniform random variable Y,
with probability mass function pY(i) = 1/(b −a) for i = a + 1, a + 2, . . . , a + j = b and zero
otherwise, is obtained by setting Y = ⌈X⌉, i.e., the ceiling of X. This discretizes or lumps the event
{i −1 < x ≤i} into the event {Y = i} with the result that Y is a discrete uniform distribution on
[a + 1, b], since
pY(i) =
 i
i−1
fX(x) dx =
 i
i−1
1
b −a dx =
1
b −a ,
i = a + 1, a + 2, . . . , b,
and zero otherwise. As we have just seen, this process of discretization has a tendency to increase
variability, the variance of Y being greater than the variance of X.
To compute the Laplace transform of the continuous uniform distribution, we assume, without
loss of generality, that 0 ≤a < b. Then
LX(s) =
 b
a
e−sx
1
b −a dx = e−as −e−bs
s(b −a) .

136
Continuous Distribution Functions
In later sections, we shall discuss simulation as a modeling technique. At this point, it is
worthwhile pointing out that the continuous uniform distribution on the unit interval (0 ≤x ≤1) is
widely used in simulation, since other distributions, both continuous and discrete, may be generated
from it. Of all distribution functions, the uniform distribution is considered the most random in the
sense that it offers the least help in predicting the value that will be taken by the random variable X.
Example 7.2 let X be a continuous random variable that is uniformly distributed over [−2, 2]. Then
the probability density function of X is
fX(x) =
1/4,
−2 ≤x ≤2,
0
otherwise.
Its cumulative distribution function is FX(x) = 0 for x ≤−2 and FX(x) = 1 for x ≥2. For values
of x between −2 and +2, it is
FX(x) =
 x
−2
fX(t)dt = 1
4
 x
−2
dt = x + 2
4
.
Let us compute E[X3], the third moment of X:
E[X3] = 1
4
 2
−2
x3dx = x4
16

2
−2
= 0.
It should be obvious that all odd moments, including the expectation, of this particular random
variable are equal to zero. Continuing, let us compute E[eX] and the Laplace transform, LX(s). We
have
E[eX] = 1
4
 2
−2
exdx = ex
4

2
−2
= e2 −e−2
4
= 1.8134
and, by substitution,
LX(s) = e2s −e−2s
4s
.
7.2 The Exponential Distribution
The cumulative distribution function for an exponential random variable, X, with parameter λ > 0,
is given by
F(x) =
1 −e−λx,
x ≥0,
0
otherwise.
Graphs of this distribution for various values of λ are shown in Figure 7.2. The corresponding
probability density function is obtained simply by taking the derivative of F(x) with respect to x:
f (x) =

λe−λx,
x ≥0,
0
otherwise.
Again some plots for various values of λ are given in Figure 7.3. Observe that the probability density
function intercepts the y = f (x) axis at the point λ, the parameter of the distribution.
One of the most important properties of the exponential distribution is that it possesses the
memoryless property. This means that the past history of a random variable that is exponentially

7.2 The Exponential Distribution
137
0
1
2
3
4
5
0
0.5
1
λ = 2.0
λ = 1.0
λ = 0.75
λ = 0.50
Figure 7.2. CDFs of some exponentially distributed random variables.
0
1
2
3
4
5
0
0.5
1
1.5
2
λ = 2.0
λ = 1.0
λ = 0.75
λ = 0.50
Figure 7.3. Some probability density functions for the exponential distribution.
distributed plays no role in predicting its future. For example, let X be the random variable that
denotes the length of time that a customer spends in service (called the service time) at some facility
and let us assume that X is exponentially distributed. Then the probability that the customer in
service ﬁnishes at some future time t is independent of how long that customer has already been in
service. Similarly, if the time between arrivals (the interarrival time) of patients to a doctor’s ofﬁce
is exponentially distributed, then the probability that an arrival occurs by time t is independent of
the length of time that has elapsed from the previous arrival.
To explore this further, let us assume that an arrival occurs at time 0. Let t0 seconds pass during
which no arrivals occur. Now, what is the probability that the next arrival occurs t seconds from
now? We assume that X is the random variable deﬁned as the time between successive arrivals, and

138
Continuous Distribution Functions
that X is exponentially distributed with parameter λ, i.e., F(x) = 1 −e−λx. Then
Prob {X ≤t0 + t|X > t0} = Prob {t0 < X ≤t0 + t}
Prob {X > t0}
= Prob {X ≤t0 + t} −Prob {X ≤t0}
Prob {X > t0}
= 1 −e−λ(t+t0) −(1 −e−λt0)
1 −(1 −e−λt0)
= e−λt0 −e−λt0e−λt
e−λt0
= 1 −e−λt
= Prob {X ≤t},
which shows that the distribution of remaining time until the next arrival, given that t0 seconds has
already elapsed since the last arrival, is identically equal to the unconditional distribution of the
interarrival time.
Example 7.3 Assume that the amount of time a patient spends in a dentist’s ofﬁce is exponentially
distributed with mean equal to 40 minutes (λ = 1/40). We ﬁrst compute the probability that a
patient spends more than 60 minutes in the dentist’s ofﬁce. Let X be the exponentially distributed
random variable that describes the time a patient spends in the ofﬁce. Then the probability that a
patient spends more than one hour there is given by
Prob{X > 60} = e−60λ = e−1.5 = 0.2231.
Second, we ﬁnd the probability that a patient will spend 60 minutes in the dentist’s ofﬁce given that
she has already spent 40 minutes there. In this case, we seek the probability that the patient will
spend a further 20 minutes in the ofﬁce, but since the exponential distributon has the memoryless
property, this is just equal to the unconditional probability that she will spend 20 minutes there,
which is given by
Prob{X > 20} = e−20λ = e−.5 = 0.6065.
The exponential distribution is the only continuous distribution that exhibits this memoryless
property, which is also called the Markov property. Furthermore, it may be shown that, if X is
a nonnegative continuous random variable having this memoryless property, then the distribution
of X must be exponential. For a discrete random variable, the geometric distribution is the only
distribution with this property. Indeed, if X is an exponentially distributed random variable with
parameter λ, then the discrete random variable Y obtained from X by discretization as Y = ⌈X⌉is
a geometric random variable with probability of success given by p = 1 −e−λ. We have
pY(i) = Prob{i −1 < X ≤i} = FX(i) −FX(i −1)
= 1 −e−λi −1 + e−λ(i−1)
= e−λ(i−1) #
1 −e−λ$
= (1 −p)i−1 p.
The mean E[X] of the exponential distribution may be found as
E[X] =
 ∞
−∞
x f (x)dx =
 ∞
0
λxe−λxdx = −λ ∂
∂λ
 ∞
0
e−λxdx
= −λ ∂
∂λ
&
−1
λe−λx

∞
0
'
= −λ ∂
∂λ
&1
λ −1
λe−∞
'
= −λ ∂
∂λ
&1
λ
'
= −λ
&
−1
λ2
'
= 1
λ.

7.2 The Exponential Distribution
139
This same result may also be obtained directly by integration by parts. If an exponentially distributed
random variable represents the interarrival time of customers to a queue, then the average interval
between arrivals is equal to 1/λ, which is a nice result, since the probability of an arrival in an
interval of length 
t is λ
t, which implies that λ is equal to the average rate of arrivals.
To compute the variance, we write
E[X2] =
 ∞
0
x2λe−λxdx = λ ∂2
∂λ2
 ∞
0
e−λxdx = λ ∂2
∂λ2
1
λ

= 2
λ2 .
Thus the variance is computed as
σ 2
X = E[X2] −E2[X] = 2
λ2 −
1
λ
2
= 1
λ2 .
We use an induction proof to show that the nth moment is given by
E[Xn] =
 ∞
0
xnλe−λxdx = n!
λn .
We have already shown that the basis clause, E[X] = 1/λ, is true. Assuming that E[Xn−1] =
(n −1)!/λn−1 we now show that E[Xn] = n!/λn. Using integration by parts, with u = xn and
dv = λe−λxdx, we obtain
E[Xn] =
 ∞
0
xnλe−λxdx = −xne−λx∞
0 +
 ∞
0
nxn−1e−λxdx
= 0 + n
λ
 ∞
0
xn−1λe−λxdx
= n
λ E[Xn−1] = n!
λn .
Alternatively, these same results can be found from the moment generating function of an
exponentially distributed random variable, which is given by
MX(θ) = E[eθ X] =
 ∞
0
eθxλe−λxdx = λ
 ∞
0
e−(λ−θ)xdx =
λ
λ −θ
for θ < λ.
Now, for example, we ﬁnd the second moment as
E[X2] = d2
dθ2 MX(θ)

θ=0
= d2
dθ2
λ
λ −θ

θ=0
=
2λ
(λ −θ)3

θ=0
= 2
λ2 .
Example 7.4 The time spent waiting for a bus on the North Carolina State Campus may be
represented by an exponential random variable with a mean of three minutes. We would like to
know the probability of having to wait more than ﬁve minutes and the probability that the time
spent waiting is within ±2 standard deviations of the mean.
We have E[X] = 1/λ = 3, which gives λ = 1/3. The probability of having to wait more than
ﬁve minutes is
Prob{X > 5} = 1 −Prob{X ≤5} = 1 −FX(5) = 1 −(1 −e−1/3×5) = e−5/3 = 0.1889.
For an exponential random variable, the standard deviation is equal to the mean. Thus we have
σX = 3, and the probability that the time spent waiting is within two standard deviations of the
mean is
Prob{3 −6 ≤X ≤3 + 6} = Prob{X ≤9} = 1 −e−1/3×9 = 0.9502.

140
Continuous Distribution Functions
The Laplace transform of the exponential probability density function is given by
F∗(s) =
 ∞
0
e−sx f (x)dx =
 ∞
0
e−sxλe−λxdx = λ
 ∞
0
e−(s+λ)xdx =
λ
s + λ.
Let X1 and X2 be independent and identically distributed exponential random variables with
mean 1/λ. The probability distribution of their sum is given by
FX1+X2(x) = Prob{X1 + X2 ≤x} =
 x
0
Prob{X1 ≤x −s}λe−λsds
=
 x
0
#
1 −e−λ(x−s)$
λe−λsds
= λ
 x
0
#
e−λs −e−λx$
ds
= λ
 x
0
e−λsds −λe−λx
 x
0
ds
= 1 −e−λx −λxe−λx.
The corresponding density function is obtained by differentiation, which gives
fX1+X2(x) = λ2xe−λx,
x ≥0.
This result may also be obtained using the convolution approach and is left to the exercises.
Let X1, X2, . . . , Xn be n independent exponential random variables having parameters
λ1, λ2, . . . , λn, respectively. Let Xmin = min(X1, X2, . . . , Xn) and Xmax = max(X1, X2, . . . , Xn).
Then, from Section 5.5, the cumulative distribution functions of Xmin and Xmax are given,
respectively, by
Prob{Xmin ≤x} = 1 −
n
i=1
#
1 −(1 −e−λi x)
$
= 1 −
n
i=1
e−λi x = 1 −e−(λ1+λ2+···+λn)x,
Prob{Xmax ≤x} =
n
i=1
#
1 −e−λi x$
.
Observe that the ﬁrst of these is exponentially distributed with parameter λ1 + λ2 + · · · + λn; the
second is not exponentially distributed.
Now let X1 and X2 be independent exponential random variables with means 1/λ1 and 1/λ2,
respectively. The probability that one is smaller than the other is given by
Prob{X1 < X2} =
 ∞
0
Prob{X1 < X2 | X2 = x}λ2e−λ2xdx
=
 ∞
0
Prob{X1 < x}λ2e−λ2xdx
=
 ∞
0
#
1 −e−λ1x$
λ2e−λ2xdx
=
 ∞
0
λ2e−λ2xdx −λ2
 ∞
0
e−(λ1+λ2)xdx
= 1 −
λ2
λ1 + λ2
=
λ1
λ1 + λ2
.

7.3 The Normal or Gaussian Distribution
141
More generally,
Prob{Xi = min(X1, X2, . . . , Xn)} =
 ∞
0

j̸=i
Prob{X j > x}λie−λi xdx
=
 ∞
0
λie−(λ1+λ2+···+λn)xdx
=
λi
(λ1 + λ2 + · · · + λn).
Example 7.5 In order for a ﬂashlight to function correctly, both the bulb and the battery must be
in working order. If the average lifetime of a bulb is 800 hours and that of the battery is 300 hours,
and if the lifetime of both is independent and exponentially distributed, then the probability that the
ﬂashlight fails due to an exhausted battery rather than a burnt out bulb can be obtained from the
above formula as
λ1
λ1 + λ2
=
1/300
1/300 + 1/800 = 0.7273.
The exponential distribution is closely related to the Poisson distribution. Speciﬁcally, if a
random variable describing an arrival process has a Poisson distribution, then the associated random
variable deﬁned as the time between successive arrivals (the interarrival time) has an exponential
distribution. To see this, notice that since the arrival process is Poisson, we have
pn(t) = e−λt (λt)n
n!
(with p0(t) = e−λt),
which describes the number of arrivals that have occurred by time t. Let X be the random variable
that denotes the time between successive events (arrivals). Its probability distribution function A(t)
is given by
A(t) = Prob{X ≤t},
i.e.,
A(t) = 1 −Prob{X > t}.
But Prob{X > t} = Prob{0 arrivals in (0, t]} = p0(t). Thus
A(t) = 1 −p0(t) = 1 −e−λt,
t ≥0.
Differentiating, we obtain the associated density function
a(t) = λe−λt,
t ≥0,
which is none other than the exponential density function. In other words, X is exponentially
distributed with mean 1/λ.
7.3 The Normal or Gaussian Distribution
The normal or Gaussian density function, which has the familiar bell-shaped curve, is omnipresent
throughout the world of statistics and probability. If X is a Gaussian random variable then its
probability density function fX(x), sometimes denoted by N(μ, σ 2), is
fX(x) = N(μ, σ 2) =
1
σ
√
2π
e−(x−μ)2/2σ 2 for −∞< x < ∞.
(7.1)

142
Continuous Distribution Functions
It may be shown that expectation is E[X] = μ and the variance Var [X] = σ 2. Thus the mean
and variance are the actual parameters of the distribution. The normal density function for selected
values of its parameters is shown in Figure 7.4.
−6
−4
−2
0
2
4
6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
σ = 0.5
σ = 1.0
σ = 2.0
Figure 7.4. The normal distribution for different values of σ.
As is apparent from this ﬁgure, the normal distribution is symmetric about its mean value μ. It
is short and ﬂat when σ is large and tall and skinny when σ is small. As we shall see momentarily,
approximately 68% of the area under fX(x) lies within one standard deviation of its mean while
95% lies within two standard deviations.
The cumulative distribution function for a random variable that is normally distributed has no
closed form, i.e., we cannot write FX(x) in the form of an equation as we have been able to do
with other distributions. This means that we must have recourse to precomputed tables. Usually
these tables provide data relating to the standard normal distribution N(0, 1), having mean 0 and
variance 1. The probability density function of a standard normal random variable is obtained by
substituting the values μ = 0 and σ 2 = 1 into Equation (7.1) . Because of its importance, and to
avoid confusion, the probability density function of a standard normal random variable X is often
denoted φX(x) and its cumulative distribution function by 	X(x).
Mathematical tables generally provide the values of 	(x) in increments of 0.01 from x = 0
(where 	(0) = 0.50) to x = 3 (where 	(3) = 0.9987). Some tables include values of x up to
x = 5 (	(5) = 0.9999997). Since a standard normal random variable X has variance and standard
deviation equal to 1, the quantity 	(1) −	(−1) is the probability that X lies within one standard
deviation of its mean, 	(2)−	(−2) is the probability that it lies within two standard deviations of its
mean, and so on. The symmetry of the normal distribution allows us to use the standard tables to ﬁnd
	(x) for negative arguments. For example, the symmetry of the standard normal distribution around
zero implies that Prob{X ≤−1} = Prob{X ≥1} and hence Prob{X ≤−1} = 1 −Prob{X ≤1}.
Obviously, this holds more generally: for any nonnegative value α, we have
Prob{X ≤−α} = 1 −Prob{X ≤α}.
The values provided by tables of the standard normal distribution can be used to ﬁnd the values
of all normal distributions, N(μ, σ 2). Indeed, if a random variable X has the distribution N(μ, σ 2),
then the random variable Z = (X −μ)/σ has a standard normal distribution. To see this, observe

7.3 The Normal or Gaussian Distribution
143
that
Prob{Z ≤z} = Prob{(X −μ)/σ ≤z} = Prob{X ≤σz + μ}
=
 σz+μ
−∞
1
√
2πσ
e−(x−μ)2/2σ 2dx
=
 z
−∞
1
√
2πσ
e−t2/2dt
for t = (x −μ)/σ. Thus, if X has a normal distribution with expectation μ and standard deviation
σ, then, for any constant α,
Prob{X ≤α} = Prob{(X −μ)/σ ≤(α −μ)/σ} = Prob{Z ≤(α −μ)/σ}.
In words, the probability that X, a normally distributed random variable, is less than α is equal to
the probability that the standard normal random variable is less than (α −μ)/σ. Observe also that
if a random variable X has distribution N(μ, σ 2), then the derived random variable Y = aX + b
has a normal distribution with mean μY = aμ + b and standard deviation σY = |a|σ, i.e., Y is
N(aμ + b, (aσ)2).
Example 7.6 Assume that test scores in CSC579 are normally distributed with mean μ = 83 and
standard deviation σ = 8. Since we wish to use the normal distribution, we shall treat the test scores
as real numbers rather than integer values. We wish to compute the probability of scores between
75 and 95.
Prob{75 ≤X ≤95} = Prob{X ≤95} −Prob{X ≤75}
= Prob{Z ≤(95 −83)/8} −Prob{Z ≤(75 −83)/8}
= Prob{Z ≤1.5} −Prob{Z ≤−1}
= 	(1.5) −	(−1)
= 0.9332 −0.1587 = 0.7745.
Observe that if we were to replace 95 with 91 in the above analysis, we would effectively compute
the probability of scores that lies within one standard deviation of the mean (i.e., scores in the range
83 ± 8). This yields
Prob{75 ≤X ≤91} = Prob{Z ≤1} −Prob{Z ≤−1} = 0.8413 −0.1587 = 0.6826.
This result is not restricted to the example given above. Approximately two-thirds of the outcomes
associated with normally distributed random variable lie within one standard deviation of the mean.
Furthermore, since 	(2) −	(−2) = 0.9773 −(1 −0.9773) = 0.9546, approximately 95% of the
outcomes of a normally distributed random variable lie within two standard deviations of the mean.
We shall now compute the moment generating function for the normal distribution. We ﬁrst
consider the standard normal case. Let the random variable Z be normally distributed with mean 0
and variance 1. Then
MZ(θ) = E(eθ Z) =
 ∞
−∞
eθz fZ(z)dz =
 ∞
−∞
1
√
2π
eθz−z2/2dz
=
 ∞
−∞
1
√
2π
e−(z2−2θz+θ2−θ2)/2dz =
 ∞
−∞
1
√
2π
e−(z−θ)2/2+θ2/2dz
= eθ2/2
 ∞
−∞
1
√
2π
e−(z−θ)2/2dz = eθ2/2
 ∞
−∞
fZ(z)dz = eθ2/2.

144
Continuous Distribution Functions
Now let X be a normally distributed random variable with mean μ and variance σ 2 and let Z be
standard normal. Then X = μ + σ Z and
MX(θ) = E[eθ X] = E[eθ(μ+σ Z)] = eμθ E[eσθ Z]
= eμθMZ(σθ) = eμθeσ 2θ2/2 = eμθ+σ 2θ2/2.
Let us now consider a random variable X constructed as a sum of normally distributed random
variables. Recall from Equation (5.8), that if X1, X2, . . . , Xn are n independent random variables
and X = X1 + X2 + · · · + Xn, then
MX(θ) = E[eθ X1eθ X2 · · · eθ Xn] = MX1(θ)MX2(θ) · · · MXn(θ).
If these n random variables are normally distributed, with expectations μi and variances σ 2
i
respectively, then
MX(θ) = eμ1θ+σ 2
1 θ2/2 eμ2θ+σ 2
2 θ2/2 · · · eμnθ+σ 2
n θ2/2
= e(μ1+μ2+···+μn)θ+(σ 2
1 +σ 2
2 +···+σ 2
n )θ2/2.
Setting μ = μ1 + μ2 + · · · + μn and σ 2 = σ 2
1 + σ 2
2 + · · · + σ 2
n , it becomes apparent that X is
normally distributed with mean μ and variance σ 2.
This result concerning sums of normally distributed random variables leads us to an important
application of the normal distribution. It turns out that the normal distribution, in certain cases may
be used as an approximation to the discrete binomial distribution. If the random variable X has a
binomial distribution for which both n and the mean value np are large, then the density function of
X is close to that of the normal distribution. In other words, the binomial distribution becomes more
and more normal as n becomes large. We provide the following theorem, stated without proof.
Theorem 7.3.1 (De Moivre-Laplace)
Let X1, X2, . . . , Xn be n independent and identically
distributed Bernoulli random variables each taking the value 1 with probability p and the value
0 with probability q = 1 −p. Let
X =
n

k=1
Xk
and X∗= X −E[X]
√Var [X] .
Then
lim
n→∞Prob{a ≤X∗≤b} =
1
√
2π
 b
a
e−x2/2dx.
Observe that X is a binomial random variable and denotes the number of successes in n independent
Bernoulli trials with expectation np and standard deviation √npq. The random variable X∗is the
normalized sum that takes the values
ξ = k −np
√npq ,
k = 0, 1, . . . , n,
with probabilities
Prob{X∗= ξ} = Cn
k pkqn−k,
k = 0, 1, . . . , n.
Example 7.7 As an example of the use of the normal distribution in approximating a binomial
distribution, let us compare the results obtained for binomial distribution with n = 10 and p = 0.5
and with the normal distribution with mean μ = np = 5 and σ = √npq =
√
2.5. We shall just

7.4 The Gamma Distribution
145
check a single value, Prob{X ≤8}. For the binomial distribution we have
8

k=0
C(10, k)0.5k0.510−k = 0.9893.
For the normal distribution, we obtain
Prob{X ≤8} = Prob{Z ≤(8 −5)/
√
2.5} = Prob{Z ≤1.8974} = 0.9713.
The approximation is not particularly accurate, but the reason is that n is not sufﬁciently large.
It is possible to add a continuity correction which attempts to adjust for the fact that we are
approximating a discrete distribution with a continuous one. However, for small values of n it is
relatively easy to compute binomial coefﬁcients. This task becomes much harder for large n, but it
is precisely these cases that the approximation by the normal distribution becomes more accurate.
One ﬁnal comment on the normal distribution: the central limit theorem, which is discussed in
Section 8.5, states that the sum of n independent random variables tends to the normal distribution
in the limit as n →∞. This is an important result in many branches of statistics and probability
for it asserts that no matter which distribution the n random variables have, so long as they are
independent, the distribution of their sum can be approximated by the normal distribution. Thus our
previous comments concerning approximating a binomial distribution with a normal distribution
should not be too surprising since a binomial random variable is a sum of n independent Bernoulli
random variables.
7.4 The Gamma Distribution
The gamma distribution gets its name from the fact that the gamma function appears in the
denominator of its density function. The gamma function itself is deﬁned as an integral over the
right half axis as
(α) =
 ∞
0
yα−1e−ydy
and has a number of interesting properties, including
(1) = 1,
(1/2) = √π,
(α) = (α −1)(α −1) for α > 1,
(n) = (n −1)! for n = 1, 2, . . . .
The ﬁrst of these can be found from simple integration, the second by appealing to the standard
normal density function, the third by using integration by parts, and the fourth from the application
of the third.
The probability density function of a random variable X having a gamma distribution is deﬁned
with positive parameters α, β > 0 as
fX(x) ≡f (x; β, α) =
1
βα(α)xα−1e−x/β, x > 0,
(7.2)
and is equal to zero for other values of x. The parameter α is called the shape parameter since the
shape of the density curve assumes different forms for different values of α. The parameter β is
called the scale parameter. It does not change the shape of the curve, but scales it horizontally and

146
Continuous Distribution Functions
vertically. The corresponding cumulative distribution function is
FX(x) ≡F(x; β, α) =
 x
0
1
βα(α)uα−1e−u/β du.
When α = 1, we obtain
fX(x) = 1
β e−x/β, x > 0,
which is the exponential distribution seen earlier in this chapter. Setting α = n/2 and β = 2 gives
the density function for a random variable having a chi-square distribution, namely,
fX(x) = x(n/2)−1e−x/2
2n/2(n/2) , n = 1, 2, . . . , x > 0.
In this case, the parameter n is called the degrees of freedom of the distribution.
The gamma
distribution is also related to the Erlang-r distribution which we shall consider in detail in the next
section. As we shall see, the Erlang-r distribution is deﬁned as
fX(x) = μ(μx)r−1e−μx
(r −1)!
,
x > 0, λ > 0,
where r = 1, 2, . . . and can be derived from Equation (7.2) by making the substitutions α = r, β =
1/μ, and invoking the property (r) = (r −1)!. We shall also see later that the Erlang distribution
may be equated to a sum of independent and identically distributed exponential distributions and
if these exponentials are associated with arrivals subject to a Poisson distribution, then the gamma
distribution may also be associated with the Poisson distribution. The gamma distribution describes
the waiting time until the kth Poisson arrival. In this case, the reciprocal of the shape parameter β is
called the rate parameter.
When α is a positive integer, the cumulative distribution function of a gamma random variable X
may be written in terms of a summation instead of an integral. Letting k = α be a positive integer,
repeated integration by parts of the cumulative distribution function yields
FX(x) ≡F(x; β, k) = 1 −
k−1

i=0
(x/β)i
i!
e−x/β,
which is a Poisson distribution with parameter x/β. It follows that precomputed tables of the
Poisson cumulative distribution function may be used to evaluate the CDF of a gamma random
variable whose α parameter is a positive integer.
Example 7.8 A random variable X has a gamma distribution with parameters α ≡k = 10 and
β = 0.5. Let us ﬁnd Prob{X > 4}.
Prob{X > 4} = 1 −
 4
0
1
(0.5)10 (10)x10−1e−x/0.5 dx
= 1 −F(4; 0.5, 10) =
9

i=0
(4/0.5)i
i!
e−4/0.5 =
9

i=0
8i
i! e−8 = 0.7166.
The expectation and variance of a random variable X having a gamma distribution with
parameters α and β are given by
E[X] = αβ
and
Var [X] = αβ2,

7.4 The Gamma Distribution
147
respectively. Conversely, given an expectation E[X] and variance Var [X], the parameters of the
gamma distribution that ﬁts these characteristics are given by
α = E[X]2
Var [X]
and
β = Var [X]
E[X] .
We now derive the formula for the expectation. To do so, we work to make the value of the
integral equal to 1 by integrating the gamma density function with parameter α + 1 between 0 and
inﬁnity (line 2) and then apply the third property of the gamma function given previously, namely,
α(α) = (α + 1):
E[X] =
 ∞
0
x
1
βα(α)xα−1e−x/β dx =
1
βα(α)
 ∞
0
x(1+α)−1e−x/β dx
= β1+α(1 + α)
βα(α)
 ∞
0
1
β1+α(1 + α)x(1+α)−1e−x/β dx
= β1+α(1 + α)
βα(α)
= β (1 + α)
(α)
= β α(α)
(α) = αβ.
The second moment, and as a result the variance, of X may be found using a similar analysis. We
obtain
E[X2] = α(α + 1)β2
and
Var [X] = E[X2] −E[X]2 = α(α + 1)β2 −α2β2 = αβ2.
When α is a positive integer, tables of the Poisson cumulative distribution function can be used to
obtain values of the cumulative distribution function of a gamma random variable. For other values
of α, other tables are available. These are not tables for the gamma distribution itself, but more
usually tables of the incomplete gamma function. This function is the cumulative distribution of a
gamma distributed random variable for which the parameter β has the value 1. In other words, they
are tables of
 x
0
uα−1e−u
(α)
du,
and provide the value of this function for selected values of x and α. The probabilities of gamma
random variables for which the parameter β = 1 may be read off directly from such tables. For
other values of β we need to work with a derived random variable. We now show that, if X is a
random variable whose distribution is gamma with parameters α and β ̸= 1, then the cumulative
distribution of the derived random variable Y = X/β is an incomplete gamma function. We have
Prob{Y ≤y} = Prob{X/β ≤y} = Prob{X ≤yβ} =
 yβ
0
xα−1e−x/β
βα(α) dx.
Introducing the change of variables u = x/β (i.e., x = βu), the integral between x = 0 and x = yβ
becomes the integral between u = 0 and u = y and we obtain
Prob {Y ≤y} =
 y
0
(uβ)α−1e−u
βα(α)
βdu, =
 y
0
uα−1e−u
(α)
du,
which is the incomplete gamma function. This allows us to compute the probability that a
gamma distributed random variable
X, with parameters α and β, is less than some value x,

148
Continuous Distribution Functions
i.e., Prob{X ≤x}, since
Prob {X ≤x} = Prob
 X
β ≤x
β
%
= Prob

Y ≤x
β
%
.
Example 7.9 Let X be a gamma random variable with expectation E[X] = 12 and variance
Var [X] = 48. Let us compute Prob{X > 15} and Prob{X ≤9}.
We ﬁrst compute the parameters α and β of the gamma distribution as
α = E[X]2
Var [X] = 12 × 12
48
= 3
and
β = Var [X]
E[X] = 48
12 = 4.
Then
Prob{X > 15} = 1 −Prob{X ≤15} = 1 −Prob
 X
β ≤15
4
%
= 1 −Prob{Y ≤3.75}.
However, a small problem now arises for most tables of the incomplete gamma function do not
provide the value of the function at y = 3.75. To overcome this problem we need to interpolate
between the entries of the table. If our tables provide for integer values of y only, then we obtain the
values Prob{Y ≤3} = 0.577 and Prob{Y ≤4} = 0.762, and since 3.75 lies three-quarters of the
way between 3 and 4, we estimate
Prob{Y ≤3.75} ≈0.577 + 3
4(0.762 −0.577) = 0.7158,
which gives Prob{X > 15} ≈1 −0.7158 = 0.2842.
Similarly,
Prob{X ≤9} = Prob
 X
β ≤9
4
%
= Prob{Y ≤2.25}.
From the tables, we ﬁnd Prob{Y ≤2} = 0.323 and Prob{Y ≤3} = 0.577 and we may estimate
Prob{Y ≤2.25} ≈0.323 + 1
4(0.577 −0.323) = 0.3865.
Since α = 3 is a positive integer, we could have answered this question by referring to the associated
Poisson distribution, but here again, we may need to interpolate between values in our tables. Let
us compute Prob{X > 15} in this manner. We have
Prob{X > 15} = 1 −F(15; 4, 3) =
2

i=0
(15/4)i
i!
e−15/4,
a Poisson distribution with parameter 3.75. If our tables are at integer values, we obtain
2

i=0
3i
i! e−3 = 0.4232
and
2

i=0
4i
i! e−4 = 0.2381
and interpolating between them gives
Prob{X > 15} ≈0.4232 + 3
4(0.2381 −0.4232) = 0.2844.
Other tables provide values at intervals of 0.2 in which case we can bound 3.75 between 3.6 and
3.8. Given that
2

i=0
(3.6)i
i!
e−3.6 = 0.303
and
2

i=0
(3.8)i
i!
e−3.8 = 0.269,

7.5 Reliability Modeling and the Weibull Distribution
149
interpolating now gives the more accurate result
Prob{X > 15} ≈0.303 + 3
4(0.269 −0.303) = 0.2775.
7.5 Reliability Modeling and the Weibull Distribution
The Weibull Distribution
The Weibull distribution has the remarkable property that it can be made to assume a wide variety
of shapes, a property that makes it very attractive for reliability modeling. The Weibull probability
density function is described, sometimes as having one, sometimes as having two, and sometimes as
having three parameters. For the three-parameter version, these parameters are the scale parameter
η, the shape parameter β, and the location parameter ξ. The two-parameter version is obtained by
setting the location parameter to zero, ξ = 0. We note that the location parameter simply shifts the
position of the density curve along the x-axis, but does not otherwise alter its shape or scale. Other
density functions could also be annotated with a location parameter, but we have refrained from
doing so. The one-parameter Weibull density function is obtained by setting the location parameter
to zero and the shape parameter to a constant value, i.e., β = c, so that the only remaining variable
is the scale parameter η. The three-parameter probability density function is deﬁned as
fX(x) = β
η
x −ξ
η
β−1
e
−

x−ξ
η
β
with x ≥0, β > 0, η > 0, and −∞< ξ < ∞. For other values of x, the Weibull probability density
function is zero. Notice that when β = 1 the Weibull distribution reduces to the two-parameter
exponential distribution. As may readily be veriﬁed by differentiation, the corresponding Weibull
cumulative distribution function is given by
FX(x) = 1 −e
−

x−ξ
η
β
.
Obviously substitution of ξ = 0 will yield the two-parameter Weibull probability density and
distribution functions, while substitution of ξ = 0 and β = c will yield the one-parameter versions.
In what follows, we shall set ξ = 0 and consider the three-parameter version no further. The
expectation and variance of a two-parameter Weibull random variable are given by
E[X] = η 

1 + 1
β

and
σ 2
X = η2
(


1 + 2
β

−

1 + 1
β
2)
,
respectively, where (1 + 1/β) is the gamma function evaluated at the point 1 + 1/β. We now
derive the expectation of a Weibull random variable, but leave a similar analysis for the variance as
an exercise. We have
E[X] =
 ∞
0
x β
ηβ xβ−1e−(x/η)β dx = β
ηβ
 ∞
0
e−(x/η)β xβ dx.
Making the substitution u = (x/η)β (or x = ηu1/β), we ﬁnd xβ−1dx = (ηβ/β)du and so
xβdx = (ηβ+1/β)u1/βdu. Hence
E[X] = β
ηβ
 ∞
0
e−u ηβ+1
β
u1/βdu = η
 ∞
0
u1/βe−udu = η 

1 + 1
β

.

150
Continuous Distribution Functions
Example 7.10 A random variable X has a Weibull distribution with η = 1/4 and β = 1/2. Then,
its probability density function is given as
fX(x) = 4
2 (4x)−1/2 e−(4x)1/2,
x ≥0,
and is equal to zero otherwise. Its cumulative distribution function is
FX(x) = 1 −e−(4x)1/2,
x ≥0,
and is otherwise equal to zero. Also, using the result (k) = (k −1)! for integer k, we have
E[X] = 0.25 (3) = 0.25 × 2! = 0.5,
Var [X] = 0.252 ((5) −(3)2) = 0.0625(4! −2!2) = 1.25,
Prob{X > 1} = 1 −Prob{X ≤1} = e−(4)1/2 = 0.1353.
Reliability: Components in Series and in Parallel
As was mentioned previously, the versatility of the Weibull distribution has resulted in it being
widely applied in reliability and life-data analysis and so we take this opportunity to consider
reliability modeling in more detail. Let X be a random variable that denotes the time to failure
of some system (component, product, or process). Then the reliability at any time t, denoted by
RX(t), of that system is deﬁned as
RX(t) = Prob{X > t} = 1 −Prob{X ≤t} = 1 −FX(t).
(7.3)
In words, the reliability of the system is simply the probability that the time to failure will exceed t
or equivalently, it is the probability that the system will survive until time t. As can be inferred from
Equation (7.3), RX(t) is a monotone nonincreasing function of t and is equal to zero in the limit
as t →∞: no system lasts forever. In biological applications, the term survivor function is used
in the place of reliability function. It is generally denoted by SX(t) and is deﬁned identically to the
reliability function as SX(t) = Prob{X > t} = 1 −FX(t).
Example 7.11 If the distribution of the time to failure of a system is exponentially distributed with
parameter μ, then the reliability function is
RX(t) = 1 −
#
1 −e−μt$
= e−μt.
For example, if the mean time to failure is ﬁve years, then μ = 1/5 and the reliability after four
years is
RX(4) = e−4/5 = 0.4493.
This is the probability that the system is still functioning after four years. If the distribution of the
time to failure is normally distributed, then we need to consult tables of standard normal distribution.
For example, if the expectation of the time to failure is ﬁve years and the standard deviation is one
year, then the reliability after four years is
RX(4) = Prob{X > 4} = Prob{Z > (4 −5)/1} = Prob{Z > −1} = 0.8413.
In practical situations, a system whose reliability is to be gauged sometimes consists of
components in series or in parallel, and these individual components have their own failure
distributions. In the case of components in series, the failure of any one component results in
a complete system failure. This is sometimes represented graphically as a string (or series) of
components connected together in a linear fashion so that if one component fails, the line is broken
and the overall system fails. Let X be the random variable that represents the time to failure of the
overall system and let Xi be the random variable representing the time to failure of component i,

7.5 Reliability Modeling and the Weibull Distribution
151
for i = 1, 2, . . . , n. We shall take FXi(t) to be the cumulative distribution function of Xi and
RXi(t) = 1 −FXi(t) to be its reliability function. The system will continue to work after time t
if and only if each of the n components continues to function after time t, i.e., if and only if the
minimum of the Xi is greater than t. It follows then that the reliability of the system, denoted by
RX(t) is given by
RX(t) = Prob{X > t} = Prob{min(X1, X2, . . . , Xn) > t}.
From earlier results on the minimum of a set of independent random variables, independent because
we shall now assume that the components fail independently of one another, this leads to
RX(t) =
n
i=1
#
1 −FXi(t)
$
=
k
i=1
RXi(t).
It is apparent that the more components are added to a system in series, the more unreliable that
system becomes. If the probability that component i operates correctly is pi, then the probability
that all n components function correctly is just n
i=1 pi.
Example 7.12 As an example of a system of four components in series (but without the typical
illustration of four in a line), consider the reliability of installing four new, identical, tires on a car.
Let X be a Weibull random variable that denotes the number of miles a tire will last before it fails.
We shall assume that the tires fail independently and that the parameters of the Weibull distribution
are η = 72 and β = 2, given that the mileage is denoted in thousands of miles. Failure of the system
(of four tires) occurs at the moment the ﬁrst tire fails.
The cumulative distribution function and the reliability function of a Weibull random variable
with parameters η = 72 and β = 2 are respectively given by
FXi(x) = 1 −e−(x/72)2
and
RXi(x) = e−(x/72)2.
The expectation and variance are
E[Xi] = 72 (1 + 1/2) = 36(1/2) = 36√π = 63.81,
Var [Xi] = 722 ((2) −π/4) = 722(0.2146) = 1,112.5.
The reliability of four tires each having this reliability function is
RX(x) =
4

i=1
e−(x/72)2 = e−4(x/72)2 = e−(x/36)2.
Observe that this itself is a Weibull reliability function with parameter η = 36 and β = 2 and that
its expectation is E[X] = 36(1 + 1/2) = 18√π = 31.90, considerably less than the expectation
of a single tire. The probability of driving 30,000 miles before a tire failure is
Prob{X > 30} = RX(30) = e−(30/36)2 = e−0.6944 = 0.4994.
For the case of components in parallel, as long as one of the components is working, the system
has not yet failed. This is the idea behind redundancy: additional components are incorporated into
the system to increase its reliability. Usually such systems are drawn as a collection of components,
one above the other, with separate lines through each one, so that if one line disappears, the others
are still present and a system-wide failure is avoided. As before, let Xi be the random variable that
represents the time to failure of component i, but now let X be the random variable that describes
the time to failure for the system when its i components are arranged in parallel. To derive an
equation for RX(t) (= Prob{X > t}, the probability that the overall system will survive until time
t), it is more convenient to ﬁrst ﬁnd Prob{X ≤t} and then to compute RX(t) from the relation
RX(t) = 1 −Prob{X ≤t}. For a system of independent components in parallel, {X ≤t} if and only

152
Continuous Distribution Functions
if max{X1, X2, . . . , Xn} ≤t. Therefore
Prob{X ≤t} = Prob{max(X1, X2, . . . , Xn) ≤t} =
n
i=1
FXi(t) =
n
i=1
(1 −RXi(t)),
and it follows that the probability of the time to failure exceeding t is given by
RX(t) = 1 −
k
i=1
#
1 −RXi(t)
$
.
Example 7.13 Consider a system consisting of n components in parallel. Assume that the time to
failure of the ith component Xi is exponentially distributed with parameter μi, i.e., Prob{Xi ≤t} =
1 −e−μit for i = 1, 2, . . . , n. Then
RX(t) = 1 −
n
i=1
(1 −e−μit).
Suppose n = 3 and the mean number of years to failure of the three components is 1.0, 2.0, and
2.5, respectively; then the reliability functions of the three components are
R1(t) = e−t,
R2(t) = e−0.5t, and
R3(t) = e−0.4t,
respectively. The reliability function of the three-component parallel system is
RX(t) = 1 −(1 −e−1.0t)(1 −e−0.5t)(1 −e−0.4t).
The probability that the system lasts more than two years is
RX(2) = 1 −(1 −e−2.0)(1 −e−1.0)(1 −e−0.8) = 1 −(0.8647)(0.6321)(0.5507) = 0.6990.
Just one ﬁnal point concerning this example. When all n components are identically, exponen-
tially distributed, i.e., μi = μ for all i, then it may be shown that
E[X] =

1 + 1
2 + · · · + 1
n

μ,
which shows that the mean time to failure increases as additional components are added, but with
diminishing returns.
It is possible, and indeed happens often, that some components in a system are arranged in
parallel while elsewhere in the same system, other components are arranged in series. This gives
rise to a mixed system, but provides no additional difﬁculty in its analysis.
Example 7.14 Consider a system consisting of seven components labeled A through G and
arranged into series and parallel sections as shown in Figure 7.5. For the system to function it is
necessary that A and G have not failed and that one of (B and C) or D or (E and F) continues to
work.
F
A
B 
C 
D
G
E
Figure 7.5. Components in series and in parallel.
Assume that the time to failure of the components are exponentially distributed and measured in
thousands of hours with mean times to failure given by 1/μi, i = 1, 2, . . . , 7, for components A
through F, respectively. Components B and C and components E and F both constitute a subsystem

7.5 Reliability Modeling and the Weibull Distribution
153
in series whose reliability functions, denoted RBC(t) and RE F(t) are given by
RBC(t) = RB(t)RC(t) = e−μ2te−μ3t = e−(μ2+μ3)t,
RE F(t) = RE(t)RF(t) = e−μ5te−μ6t = e−(μ5+μ6)t.
Both these are arranged in parallel with component D so that the reliability of the subsystem
consisting of components B, C, D, E, and F is given by
RBC DE F(t) = 1 −[1 −RBC(t)] [1 −RD(t)] [1 −RE F(t)]
= 1 −

1 −e−(μ2+μ3)t 
1 −e−μ4t 
1 −e−(μ5+μ6)t 
.
Finally, this subsystem is arranged in series with component A before it and component G after it.
The reliability of the overall system is therefore given by
RX(t) = RA(t) RBC DE F(t) RG(t)
= e−μ1t #
1 −

1 −e−(μ2+μ3)t 
1 −e−μ4t 
1 −e−(μ5+μ6)t $
e−μ7t.
Given the following mean times to failure, in thousands of hours,
1
μ1
= 4,
1
μ2
= 1,
1
μ3
= 0.5,
1
μ4
= 2,
1
μ5
= 0.25,
1
μ6
= 0.75,
1
μ7
= 5,
we ﬁnd, for example, the probability that the time to system failure exceeds 1,000 hours is
RX(1) = e−1/4 #
1 −

1 −e−3 
1 −e−1/2 
1 −e−16/3 $
e−1/5 = 0.4004.
The Hazard or Instantaneous Failure Rate
Related to the reliability function just discussed, the hazard rate is also used in reliability modeling.
Alternate names for the hazard rate are force of mortality, instantaneous failure rate, failure-rate
function, intensity function, and conditional failure rate. Let X be the random variable that denotes
the time to failure of a system and let fX(t) be its density function and FX(t), its cumulative
distribution function. We have just seen that the reliability function is given by RX(t) = 1 −FX(t).
With these deﬁnitions in hand, we deﬁne the hazard rate to be
h X(t) = fX(t)
RX(t) =
fX(t)
1 −FX(t).
(7.4)
The hazard rate may be viewed as the conditional probability that the system will fail in the next
small interval of time, 
t, given that it has survived to time t. To see this, we have
Prob{X ≤t + 
t | X > t} = Prob{X ≤t + 
t, X > t}
Prob{X > t}
=
 t+
t
t
fX(u)du
RX(t)
≈h X(t)
t,
where we have approximated the integral over the small interval 
t by fX(t)
t. The reader should
clearly distinguish between fX(t)
t, which is (approximately) the unconditional probability that
the system will fail in the interval (t, t + 
t], and h(t)
t, which is (approximately) the conditional
probability that the system will fail in the interval (t, t + 
t] given that it has survived to time t.
It follows that h X(t) must always be greater than fX(t). This may also be seen from Equation (7.4)
since the denominator in the right hand side is always less than 1.
Example 7.15 Let X be the random variable that denotes the time to failure of a system and let
fX(t) = λe−λt, i.e., X is exponentially distributed. Then the hazard rate is constant since
h X(t) =
λe−λt
1 −(1 −e−λt) = λe−λt
e−λt = λ.

154
Continuous Distribution Functions
Example 7.16 Consider a system consisting of two components in parallel and in which the
lifetime of each component is independently and identically distributed with cumulative distribution
functions FX1(t) = FX2(t) = FXc(t), respectively. We wish to ﬁnd the hazard rate of the two-
component system, h X(t). The cumulative distribution function of the time to failure of the system
is given by
FX(t) = Fmax(t) =
n
i=1
FXi(t) = [FXc(t)]2
with corresponding density function obtained by differentiation,
fX(t) = 2 fXc(t)FXc(t).
The hazard rate of the two component system is therefore given by
h X(t) =
fX(t)
1 −FX(t) = 2 fXc(t)FXc(t)
1 −[FXc(t)]2
=
2FXc(t)
1 + FXc(t) ×
fXc(t)
1 −FXc(t)
=
 2FXc(t)
1 + FXc(t)

h Xc(t).
Since the term in parentheses can never exceed 1, the failure rate of the system is always less than
that of the individual components, but since the term in parentheses tends to 1 as t →∞, the failure
rate of the system tends to that of an individual component as t →∞.
Given a reliability function RX(t), the corresponding hazard rate may be found by forming the
cumulative distribution of X from FX(x) = 1 −RX(t), which in turn may be used to obtain the
density function fX(x), and the hazard rate determined as the ratio of fX(t) to RX(t). Similarly,
given the hazard rate h X(t), the corresponding reliability function may be derived from it. We have
d
dt RX(t) = d
dt (1 −FX(t)) = −fX(t)
and hence
−h X(t) = −fX(t)
RX(t) =
1
RX(t)
d
dt RX(t) = d
dt ln RX(t),
by applying the result that if u is any differentiable function of t, then d/dt (ln u) = (1/u) d/dt (u).
Thus ln RX(t) = −
 t
0 h X(u)du, and
RX(t) = e−
 t
0 h X(u)du.
The integral in the above expression is called the cumulative hazard and is written as HX(t), i.e.,
HX(t) =
 t
0
h X(u) du
and
RX(t) = e−HX(t).
Example 7.17 Let the hazard rate of a component be given as h X(t) = eλt. The corresponding
reliability function is obtained as
RX(t) = e−
 t
0 eλudu = e(1−eλt)/λ.
Example 7.18 Assume the hazard rate of a component is given by h X(t) = 2t. We would like
to compute the probability that the component will still be functioning after two years. We ﬁrst

7.6 Phase-Type Distributions
155
compute the reliability function as
RX(t) = e−
 t
0 2udu = e−t2.
Substitution of t = 2 into this expression gives RX(2) = e−4 = 0.0183, the probability that the
component is still functioning after two years. Since the cumulative distribution is FX(t) = 1−e−t2,
we can proceed to compute the density function as
fX(t) = 2te−t2.
If the hazard rate is an increasing function of t, then the system wears out with time. This is an
easily understood concept and applies to very many systems. On the other hand, if the hazard rate is
a decreasing function of t, then the system becomes more reliable with time. This is a less common
occurrence, but it can be observed in the early stages of some processes such as the ageing of some
ﬁne wines. Some electronic components also experience a “burn-in” period—a period over which
the components experience increased reliability with time.
This brings us back to the Weibull distribution. It has the property that the hazard rate of a random
variable with this distribution can be made increasing or decreasing, depending upon whether the
parameter β is greater than or less than 1. We have already seen that, when β = 1, the Weibull
distribution reduces to the exponential distribution which has a constant hazard rate. Substituting
for the density and distribution function of the two-parameter Weibull distribution into Equation
(7.4) we obtain
h X(t) =
β
η

t
η
β−1
e−(t/η)β
e−(t/η)β
= β
η
 t
η
β−1
.
The Weibull distribution allows us to handle situations in which a system experiences an initial
period during which reliability improves (the hazard rate decreases, β < 1), a generally long second
period, the useful life of the system, during which the reliability is constant (β = 1), and then an end
period during which the reliability decreases (the hazard rate increases, β > 1). The typical graph
of the hazard rate of such a scenario exhibits a “bathtub” shape. One well-known example concerns
the human lifespan: an initial period in which the small child is susceptible to childhood illnesses,
a long period from youth to mature adult, and a ﬁnal period as the person progresses into old age.
The advantage of the Weibull distribution is that it allows us to cover all these by simply adjusting
a parameter.
7.6 Phase-Type Distributions
The exponential distribution is very widely used in performance modeling. The reason, of course,
is the exceptional mathematical tractability that ﬂows from the memoryless property of this
distribution. But sometimes mathematical tractability is not sufﬁcient to overcome the need to model
processes for which the exponential distribution is simply not adequate. This leads us to explore
ways in which we can model more general distributions while maintaining some of the tractability
of the exponential. This is precisely what phase-type distributions permit us to do. Additionally, as
we shall see in later sections, phase-type distributions prove to be very useful when it is necessary
to form a distribution having some given expectation and variance.
7.6.1 The Erlang-2 Distribution
Phase-type distributions (the word “stage” is also used) get their name from the fact that they can
be represented as the passage through a succession of exponential phases or stages. We begin by
examining the exponential distribution which consists of a single exponential phase. To make our

156
Continuous Distribution Functions
discussion more concrete, we shall consider a random variable X which denotes the service time of
a customer at a service center. This is the time that the customer spends receiving service and does
not include the time, if any, that customer spends waiting for service. We assume that this service
time is exponentially distributed with parameter μ > 0. This is represented graphically in Figure
7.6 where the single exponential phase is represented by a circle that contains the parameter of
the exponential distribution. Customers are served by entering the phase from the left, spending an
amount of time that is exponentially distributed with parameter μ within the phase and then exiting
to the right. We could equally well have chosen the random variable X to represent the interarrival
time of customers to a service center, or indeed any of the previous examples in which X is
exponentially distributed.
μ
Figure 7.6. An exponential service phase.
Recall that the density function of the exponential distribution is given by
fX(x) ≡dF(x)
dx
= μe−μx,
x ≥0,
and has expectation and variance E[X] = 1/μ,
σ 2
X = 1/μ2.
Now consider what happens when the service provided to a customer can be expressed as
one exponential phase followed by a second exponential phase. We represent this graphically in
Figure 7.7.
μ
μ
1
2
Figure 7.7. Two exponential service phases in tandem.
A customer enters the servicing process (drawn as a box) and immediately begins to receive
service that is exponentially distributed with parameter μ. Upon completion of this phase of its
service, the customer enters the second phase and immediately begins to receive service that is once
again exponentially distributed with parameter μ. At the end of this second phase, the customer
departs and the servicing process is now ready to begin the service of another customer. Notice
that although both service phases are exponentially distributed with the same parameter, they are
completely independent.
The servicing process does not contain two independent servers. Rather, think of it as consisting
of a single service provider that operates in one phase or the other (but not both) at any given instant
of time. With this interpretation it is easy to see that no more than one customer can be receiving
service at any time, since a single server cannot serve two customers at the same time. A second
customer cannot enter phase 1 of the servicing process until after the previous customer has exited
from phase 2, nor can the customer receiving service choose to terminate its service after phase 1
and depart without receiving service at phase 2.

7.6 Phase-Type Distributions
157
To analyze this situation, we shall assume that the probability density function of each of the
phases is given by
fY(y) = μe−μy,
y ≥0,
with expectation and variance E[Y] = 1/μ, σ 2
Y = 1/μ2. A customer ﬁrst spends an amount of
time that is randomly chosen from fY(y). Upon completion, the customer again spends an amount
of time randomly (independently) chosen from fY(y). After this second random interval expires,
the customer departs and only then a new customer can begin to receive service. We now seek the
distribution of the total time spent by a customer in the service facility. Obviously it is a random
variable that is the sum of two independent and identically distributed exponential random variables.
Let Y be an exponentially distributed random variable with parameter μ and let X = Y + Y.
From our previous results (see Section 4.6) concerning the convolution of two independent random
variables, we have
fX(x) =
 ∞
−∞
fY(y) fY(x −y)dy
=
 x
0
μe−μyμe−μ(x−y)dy
= μ2e−μx
 x
0
dy = μ2xe−μx,
x ≥0,
and is equal to zero for x ≤0. This is the probability density function for a random variable that has
an Erlang-2 distribution. We shall use the notation E2 to denote this distribution. The corresponding
cumulative distribution function is given by
FX(x) = 1 −e−μx −μxe−μx = 1 −e−μx (1 + μx) ,
x ≥0.
It is a useful exercise to compute the density function using Laplace transforms. We can form
the Laplace transform of the overall service time probability density function as the product of the
Laplace transform of the independent phases. Let the Laplace transform of the overall service time
distribution be
LX(s) ≡
 ∞
0
e−sx fX(x)dx
and let the Laplace transform of each of the exponential phases be
LY(s) ≡
 ∞
0
e−sy fY(y)dy.
Then
LX(s) = E[e−sx] = E[e−s(y1+y2)] = E[e−sy1]E[e−sy2] = LY(s)LY(s) =

μ
s + μ
2
,
since the Laplace transform of the exponential distribution with mean μ is μ/(s + μ). To obtain
fX(x) we must now invert this transform, i.e., we need to ﬁnd the function of x whose transform
is μ2/(s + μ)2. The easiest way to accomplish this is to look up tables of transform pairs and to
pick off the appropriate answer. One well-known transform pair is the transform 1/(s + a)r+1 and
its associated function xre−ax/r!. It is usual to write such a pair as
1
(s + a)r+1
⇐⇒
xr
r! e−ax.
(7.5)

158
Continuous Distribution Functions
Use of this transform pair with a = μ and r = 1 allows us to invert LX(s) to obtain
fX(x) = μ2xe−μx = μ(μx)e−μx,
x ≥0,
as before.
The expectation and higher moments may be found from the Laplace transform as
E[Xk] = (−1)k dk
dsk LX(s)

s=0
for k = 1, 2, . . . .
This allows us to ﬁnd the mean as
E[X] = −d
ds LX(s)

s=0
= −μ2 d
ds (s + μ)−2

s=0
= μ2 2(s + μ)−3
s=0 = 2
μ.
This should hardly be surprising since the time spent in service is the sum of two independent and
identically distributed random variables and hence the expected time in service is equal to the sum
of the expectations of each, i.e., E[X] = E[Y] + E[Y] = 1/μ + 1/μ = 2/μ. Similarly, we may
show that the variance of an Erlang-2 random variable is given by
σ 2
X = σ 2
Y + σ 2
Y =
 1
μ
2
+
 1
μ
2
= 2
μ2 .
Example 7.19
Let us compare the expectation and variance of an exponential random variable
with parameter μ and a two-phase Erlang-2 random variable, each of whose phases has parameter
2μ. We ﬁnd
Mean
Variance
Exponential
1/μ
1/μ2
Erlang-2
1/μ
1/2μ2
Notice that the mean time in service in the single-phase and the two-phase system is the same (which
is to be expected since we sped up each phase in the two-phase system by a factor of 2). However,
notice the variance of the two-phase system is one-half the variance of the one-phase system. This
shows that an Erlang-2 random variable has less variability than an exponentially distributed random
variable when both have the same mean.
7.6.2 The Erlang-r Distribution
It is easy to generalize the Erlang-2 distribution to the Erlang-r distribution, which we shall
designate by Er. We can visualize an Er distribution as a succession of r identical, but independent,
exponential phases with parameter μ as represented in Figure 7.8. A customer entering a service
facility that consists of a single Erlang-r server must spend r consecutive intervals of time, each
drawn from an exponential distribution with parameter μ, before its service is completed. No other
customer can receive service during this time, nor can the customer in service leave until all r phases
have been completed.
To analyze this situation, let the time a customer spends in phase i be drawn from the density
function fY(y) = μe−μy, y ≥0. Then the expectation and variance per phase are given by
E[Y] = 1/μ and σ 2
Y = 1/μ2,
respectively.
Since the total time spent in the service facility is the sum of r independent and identically
distributed random variables, the mean and variance of the overall service received by a customer

7.6 Phase-Type Distributions
159
μ
μ
μ
μ
1
2
r−1
r
Figure 7.8. r exponential service phases in tandem.
are given by
E[X] = r
 1
μ

= r
μ,
σ 2
X = r
 1
μ
2
= r
μ2 , respectively.
The Laplace transform of the service time is
LX(s) =

μ
s + μ
r
,
which, using the same transform pair as before
1
(s + a)r+1
⇐⇒
xr
r! e−ax,
with a = μ, inverts to give
fX(x) = μ(μx)r−1e−μx
(r −1)!
,
x ≥0.
(7.6)
This is the Erlang-r probability density function. The corresponding cumulative distribution
function is given by
FX(x) = 1 −e−μx
r−1

i=0
(μx)i
i!
,
x ≥0 and r = 1, 2, . . . .
(7.7)
As we mentioned in a previous chapter, an Erlang-r random variable may be viewed as the
time spent waiting until the rth arrival in a scenario in which customers arrive (alternatively
events/successes occur) according to a Poisson distribution. To consider this further, consider a
situation in which customers arrive according to a Poisson distribution whose rate is given by μ.
Let N(t) be the random variable that denotes the number of arrivals during the time period [0, t].
Therefore N(t) is a Poisson random variable with parameter μt and so the probability of r −1 or
fewer customer arrivals is given by the cumulative distribution of N(t) as
Prob{N(t) ≤r −1} =
r−1

k=0
(μt)k
k! e−μt.
We wish to ﬁnd the probability distribution of the random variable Wr, the time we need to wait
until the ﬁrst r customers arrive. However, rather than trying to compute Prob{Wr ≤t} directly, we
shall ﬁrst ﬁnd Prob{Wr > t}. Notice that the arrival of the rth customer will be greater than t if, and
only if, r −1 or fewer customers arrive by time t. In other words,
Prob{Wr > t} = Prob{N(t) ≤r −1}.

160
Continuous Distribution Functions
It now follows that
Prob{Wr ≤t} = 1 −Prob{Wr > t} = 1 −
r−1

k=0
(μt)k
k! e−μt,
(7.8)
which is just the Erlang-r distribution.
Example 7.20 Let N(t) be a Poisson arrival process with rate μ = 0.5 and let W4 be the waiting
time until the fourth arrival. Let us ﬁnd the density function and the cumulative distribution function
of W4, as well as its expectation and standard deviation. We shall also ﬁnd the probability that the
wait is longer than 12 time units.
Replacing μ with 0.5 and r with 4 in Equation (7.8), we immediately obtain the cumulative
distribution as
FW4(t) = Prob{W4 ≤t} = 1 −e−t/2
3

k=0
(t/2)k
k!
,
t ≥0.
The density function may be found from this, or more simply, by substituting directly into the
formula for the density of an Erlang-r random variable. We ﬁnd
fW4(t) = 0.5(t/2)3e−t/2
3!
= 1
96 t3e−t/2,
t ≥0.
In this same way, the expectation and standard deviation may be obtained directly from the
corresponding formulae for an Erlang-r random variable. We have
E[W4] = 4
0.5 = 8
and
σW4 =
0
4
.25 = 4,
Prob{W4 > 12} = 1 −Prob{W4 ≤12} = e−6
3

k=0
6k
k! = e−6

1 + 6 + 36
2 + 216
6

= 0.1512.
By differentiating FX(x) with respect to x we may show that Equation (7.7) is indeed the
distribution function with corresponding density function given by Equation (7.6). We have
fX(x) = d
dx FX(x) = μe−μx
r−1

k=0
(μx)k
k!
−e−μx
r−1

k=0
k(μx)k−1μ
k!
= μe−μx + μe−μx
r−1

k=1
(μx)k
k!
−e−μx
r−1

k=1
k(μx)k−1μ
k!
= μe−μx −μe−μx
r−1

k=1
k(μx)k−1
k!
−(μx)k
k!

= μe−μx
	
1 −
r−1

k=1
(μx)k−1
(k −1)! −(μx)k
k!

= μe−μx

1 −

1 −(μx)r−1
(r −1)!
%
= μ(μx)r−1
(r −1)! e−μx.
To show that the area under this density curve is equal to 1, let
Ir =
 ∞
0
μrxr−1 e−μx
(r −1)!
dx,
r = 1, 2, . . . .


162
Continuous Distribution Functions
7.6.3 The Hypoexponential Distribution
As we have just seen, it is possible to use a sequence of exponential phases, an Erlang-r distribution,
to model random variables that have less variability than an exponentially distributed random
variable while maintaining the desirable mathematical properties of the exponential distribution.
Any given value μ for the mean can be obtained by an appropriate choice for the mean of each
phase: with r phases, each should have mean equal to rμ. However, with a single series of phases,
the choice of variance is limited. Only values that lead to squared coefﬁcients of variation equal to
1/i for integer i, are possible. We saw one approach to overcome this difﬁculty lies in a mixture
of Erlang distributions. An alternative approach is to modify the phase-type representation of the
Erlang-r distribution by permitting each phase to have a different parameter; i.e., we allow phase i
to provide service that is exponentially distributed with parameter μi, which may be different for
each phase. This leads to the hypoexponential distribution and is illustrated in Figure 7.10.
μ
μ
1
2
r−1
r
1
μr−1
r
μ2
Figure 7.10. Hypoexponential distribution.
Consider ﬁrst the case of two exponential phases. Let Y1 and Y2 be two independent random
variables that are exponentially distributed with parameters μ1 and μ2, respectively, and let X =
Y1 + Y2. Then the probability density function of X is obtained from the convolution of the two
exponential distributions. We have
fX(x) =
 ∞
−∞
fY1(y) fY2(x −y)dy
=
 x
0
μ1e−μ1y μ2e−μ2(x−y)dy
= μ1μ2e−μ2x
 x
0
e−(μ1−μ2)ydy
=
μ1μ2
μ1 −μ2
#
e−μ2x −e−μ1x$
for x ≥0 and is 0 otherwise. Its corresponding cumulative distribution function is given by
FX(x) = 1 −
μ2
μ2 −μ1
e−μ1x +
μ1
μ2 −μ1
e−μ2x,
x ≥0.
Its expectation, variance, and squared coefﬁcient of variation are respectively given by
E[X] = 1
μ1
+ 1
μ2
,
Var [X] = 1
μ2
1
+ 1
μ2
2
,
and C2
X =
μ2
1 + μ2
2
(μ1 + μ2)2 < 1,
and its Laplace transform by
LX(s) =

μ1
s + μ1
 
μ2
s + μ2

.

7.6 Phase-Type Distributions
163
For more than two phases, the analysis is more tedious. The Laplace transform for the probability
density function of an r-phase hypoexponential random variable X is
LX(s) =

μ1
s + μ1
 
μ2
s + μ2

· · ·

μr
s + μr

.
The density function fX(x) is the convolution of r exponential densities each with its own parameter
μi and is given by
fX(x) =
r

i=1
αiμie−μi x,
x > 0,
where αi =
r
j=1, j̸=i
μ j
μ j −μi
,
and is equal to zero otherwise. The expectation, variance, and squared coefﬁcient of variation are as
follows:
E[X] =
r

i=1
1
μi
,
Var [X] =
r

i=1
1
μ2
i
,
and C2
X =

i 1/μ2
i
#
i 1/μi
$2 ≤1.
To show that C2
X cannot be greater than 1, we use the fact that for real ai ≥0, 
i a2
i ≤
#
i ai
$2,
since the right-hand side contains the left-hand side plus the sum of all the nonnegative cross-terms.
Now taking ai = 1/μi implies that C2
X ≤1.
Example 7.21 Consider a random variable modeled as three consecutive exponential phases with
parameters μ1 = 1, μ2 = 2, and μ3 = 3. Let us ﬁnd the expectation, variance, and squared
coefﬁcient of variation of X, as well as its probability density function.
The expectation of X is just equal to the sum of the expectations of each phase, and since the
three exponential phase are independent of each other, the variance is also equal to the sum of the
variances of each phase. Hence
E[X] =
3

i=1
1
μi
= 1
1 + 1
2 + 1
3 = 11
6 ,
Var [X] =
3

i=1
1
μ2
i
= 1
1 + 1
4 + 1
9 = 49
36.
The squared coefﬁcient of variation is
C2
X = 49/36
121/36 = 49
121 = 0.4050.
α1 =
r
j=1, j̸=i
μ j
μ j −μ1
=
μ2
μ2 −μ1
×
μ3
μ3 −μ1
= 2
1 × 3
2 = 3,
α2 =
r
j=1, j̸=i
μ j
μ j −μ2
=
μ1
μ1 −μ2
×
μ3
μ3 −μ2
= 1
−1 × 3
1 = −3,
α3 =
r
j=1, j̸=i
μ j
μ j −μ3
=
μ1
μ1 −μ3
×
μ2
μ2 −μ3
= 1
−2 × 2
−1 = 1.
It follows then that
fX(x) =
3

i=1
αiμie−μi x = 3e−x −6e−2x + 3e−3x,
x > 0.

164
Continuous Distribution Functions
Neither the hypoexponential distribution nor a mixture of Erlang distributions can be used if
coefﬁcients of variation greater than 1 are required. To proceed in this direction, we turn to phases
in parallel rather than phases in series as tried to this point.
7.6.4 The Hyperexponential Distribution
Our goal now is to ﬁnd a phase-type arrangement that gives larger coefﬁcients of variation than the
exponential. Consider the conﬁguration presented in Figure 7.11, in which α1, is the probability that
the upper phase is taken and α2 = 1 −α1 the probability that the lower phase is taken. If such a
distribution is used to model a service facility, then a customer entering service will, with probability
α1, receive service that is exponentially distributed with parameter μ1 and then exit the server, or
else, with probability α2 receive service that is exponentially distributed with parameter μ2 and then
exit the server. Once again, only one customer can be in the process of receiving service at any one
time, i.e., both phases cannot be active at the same time.
μ
α
1−α
μ1
2
1
1
Figure 7.11. Two exponential phases in parallel.
The density function of the service time received by a customer is given by
fX(x) = α1μ1e−μ1x + α2μ2e−μ2x,
x ≥0,
and is otherwise equal to zero, while the cumulative distribution function is
FX(x) = α1(1 −e−μ1x) + α2(1 −e−μ2x),
x ≥0.
Its Laplace transform is
LX(s) = α1
μ1
s + μ1
+ α2
μ2
s + μ2
.
The ﬁrst and second moments are respectively given by
E[X] = α1
μ1
+ α2
μ2
and
E[X2] = 2α1
μ2
1
+ 2α2
μ2
2
.
This allows us to compute the variance as E[X2] −(E[X])2 and from it the squared coefﬁcient of
variation as
C2
X = E[X2] −(E[X])2
(E[X])2
= E[X2]
(E[X])2 −1 = 2α1/μ2
1 + 2α2/μ2
2
(α1/μ1 + α2/μ2)2 −1 ≥1.
The proof that the squared coefﬁcient of variation is not less than 1 is given in the more general
context of multiple parallel phases, which is discussed next.
Example 7.22 Let us ﬁnd the expectation, standard deviation, and squared coefﬁcient of variation
of a two-phase hyperexponential random variable X with parameters α1 = 0.4, μ1 = 2, and




168
Continuous Distribution Functions
E[X2] = (−1)2 d2
ds2
(1 −α)μ1
s + μ1
+
αμ1μ2
(s + μ1)(s + μ2)

s=0
= d
ds
−(1 −α)μ1
(s + μ1)2
+ αμ1μ2
&
−1
(s + μ1)(s + μ2)2 +
−1
(s + μ1)2(s + μ2)
'
s=0
=
2(1 −α)μ1
(s + μ1)3 +
2αμ1μ2
(s + μ1)(s + μ2)3 +
αμ1μ2
(s + μ1)2(s + μ2)2
+
2αμ1μ2
(s + μ1)3(s + μ2) +
αμ1μ2
(s + μ1)2(s + μ2)2

s=0
= 2(1 −α)
μ2
1
+ 2α
μ2
2
+
α
μ1μ2
+ 2α
μ2
1
+
α
μ1μ2
= 2
μ2
1
+ 2α
μ2
2
+
2α
μ1μ2
,
Var [X] =
 2
μ2
1
+ 2α
μ2
2
+
2α
μ1μ2

−
 1
μ1
+ α
μ2
2
= 2μ2
2 + 2αμ2
1 + 2αμ1μ2
μ2
1μ2
2
−(μ2 + αμ1)2
μ2
1μ2
2
= μ2
2 + 2αμ2
1 −α2μ2
1
μ2
1μ2
2
= μ2
2 + αμ2
1(2 −α)
μ2
1μ2
2
.
Finally, we obtain the coefﬁcient as C2
X = Var [X]/E[X]2, although we also point out that we could
have formed it without ﬁrst computing Var [X], by using the formulation C2
X = E[X2]/E[X]2 −1,
C2
X = Var [X]
E[X]2 = μ2
2 + αμ2
1(2 −α)
μ2
1μ2
2
×
μ2
1μ2
2
(μ2 + αμ1)2 = μ2
2 + αμ2
1(2 −α)
(μ2 + αμ1)2
.
(7.11)
Example 7.23 The expectation, second moment, variance, and squared coefﬁcient of variation of a
Coxian-2 random variable with parameters μ1 = 2, μ2 = 0.5, and α = 0.25, are
E[X] = 1
μ1
+ α
μ2
= 1
2 + 1/4
1/2 = 1,
E[X2] = 2
μ2
1
+ 2α
μ2
2
+
2α
μ1μ2
= 2
4 + 1/2
1/4 + 1/2
1
= 3,
Var [X] = E[X2] −E[X]2 = 3 −1 = 2,
C2
X = Var [X]/E[X]2 = 2.
7.6.6 General Phase-Type Distributions
Phase-type distributions need not be restricted to linear arrangements, but can be very general
indeed. Consider a collection of k phases such that the random variable that describes the time
spent in phase i, i = 1, 2, . . . , k, is exponentially distributed with parameter μi. It is possible to

7.6 Phase-Type Distributions
169
envisage a general phase type probability distribution deﬁned on these phases as the total time spent
moving in some probabilistic fashion among the k different phases. It sufﬁces to specify:
• The initial probabilities, i.e., the probabilities σi, i = 1, 2, . . . , k that a given phase i is the
ﬁrst phase entered: k
i=1 σi = 1.
• The routing probabilities ri j, i, j = 1, 2, . . . , k, which give the probabilities that, after
spending an exponentially distributed amount of time with mean 1/μi in phase i, the next
phase entered is phase j. For all i, rii < 1 while for at least one value of i, k
j=1 ri j < 1.
• The terminal probabilities, i.e., the probabilities ηi, i
= 1, 2, . . . , k, that the process
terminates on exiting from phase i. At least one must be strictly positive. For all i =
1, 2, . . . , k, we must have ηi + k
j=1 ri j = 1: on exiting from phase i, either another phase j
is entered (with probability ri j) or the process terminates (with probability ηi).
k
μ
μ1
2
α
1−α
1−α
1
1
1
2
α2
μ
Figure 7.15. The Coxian distribution, again.
Example 7.24 Consider the Coxian distribution described earlier and shown again in Figure 7.15.
The initial probabilities are (in vector form) σ = (1, 0, 0, . . . , 0); the terminal probabilities are
η = (1 −α1, 1 −α2, . . . , 1 −αk−1, 1) and the probabilities ri j are the elements of the matrix
R =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0 α1 0 · · ·
0
0 0 α2 · · ·
0
...
...
...
...
0 0 0
αk−1
0 0 0 · · ·
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
The vectors σ and η and the matrix R together with the parameters of the exponential distributions
completely characterize a Coxian distribution.
Example 7.25
A general phase-type distribution having four phases and exponential parameters
μi, i = 1, 2, 3, 4; vectors σ = (0, .4, 0, .6) and η = (0, 0, .1, 0) and routing probability matrix
R =
⎛
⎜
⎜
⎝
0 .5 0 .5
0 0 1 0
.2 0 0 .7
1 0 0 0
⎞
⎟
⎟
⎠
begins life in phase 2 with probability .4 or in phase 4 with probability .6, and moves among the
phases until it eventually departs from phase 3. It has the graphical interpretation of Figure 7.16.
General phase-type distributions frequently have an extra phase appended to represent the
exterior into which the process ﬁnally departs. Once in this phase, called a sink or an absorbing
phase, the process remains there forever. The phase-type probability distribution is that of the
random variable which represents the time to absorption, from some initial phase(s), into this sink.
In this case, it is usual to combine the parameters of the exponential distributions of the phases
and the routing probabilities into a single matrix Q whose elements qi j give the rate of transition

170
Continuous Distribution Functions
0.6
1
2
3
4
0.4
0.7
0.2
0.1
1
0.5
0.5
1
Figure 7.16. A general phase-type distribution.
from phase i to some other phase j, i.e., qi j = μiri j. It is also usual to set the diagonal element
in each row of this matrix equal to the negated sum of the off-diagonal elements in that row, i.e.,
qii = −
j̸=i qi j. Thus the sum of all the elements in any row is zero. As we shall see later, this
matrix together with an initial starting vector describes a continuous-time Markov chain with a
single absorbing state. The matrix is called the transition-rate matrix or inﬁnitesimal generator of
the Markov chain.
Example 7.26 For the Coxian distribution of Example 7.24,
Initial distribution : (1, 0, 0, . . . , 0 | 0) = (σ | 0),
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−μ1 μ1α1
0
· · ·
0
μ1(1 −α1)
0
−μ2 μ2α2 · · ·
0
μ2(1 −α2)
...
...
...
...
0
0
0
μk−1αk−1 μk−1(1 −αk−1)
0
0
0
· · ·
−μk
μk
0
0
0
· · ·
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
For the general phase-type distribution of Example 7.25,
Initial distribution: (0, .4, 0, .6 | 0) = (σ | 0),
Q =
⎛
⎜
⎜
⎜
⎜
⎝
−μ1 .5μ1
0
.5μ1
0
0
−μ2 μ2
0
0
.2μ3
0
−μ3 .7μ3 .1μ3
μ4
0
0
−μ4
0
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
.
To proceed, let us consider an arbitrary phase-type distribution consisting of k phases plus the
absorbing phase. Such a distribution is deﬁned by a transition rate matrix Q and an initial probability
distribution σ ′. Let us partition Q and σ ′ as follows:
Q =

S
S0
0
0

,
σ ′ = (σ | σk+1) = (σ1, σ2, . . . , σk | σk+1) .
Here S is a k × k matrix that contains the transition rates among the nonabsorbing states and with
diagonal elements equal to the negated sum across the rows of Q; S0 is a column vector of length
k whose ith component gives the rate at which the process enters the absorbing state from state i;
σ ′
i , i = 1, 2, . . . , k + 1, is the probability that the starting phase is phase i. Notice that it is possible
to have σk+1 > 0, i.e., for the process to begin in the absorbing phase, phase k + 1.
The probability distribution of the phase-type random variable deﬁned by Q and σ ′ is identical
to the probability distribution of the time to absorption into the sink state, state k + 1. It is given by
FX(x) = 1 −σeSxe,
x ≥0,

7.6 Phase-Type Distributions
171
where e is a column vector of length k whose elements are all equal to 1. Its probability density
function has a jump of magnitude σk+1 at the origin. Its density on (0, ∞) is
fX(x) = F′
X(x) = σeSx S0,
and its moments, which are all ﬁnite, are given by
E[X j] = (−1) j j!σ S−je,
j = 1, 2, . . . .
In particular, the expectation of a phase-type distribution is just the expected time to absorption in
the Markov chain. We have
E[X] = −σ S−1e ≡xe,
where x = −σ S−1 is a row vector of length k. Since the effect of multiplying x with e is to sum
the components of x, we may take xi, i = 1, 2, . . . , k, to be the mean time spent in phase i prior
to absorption. The proof of these results, obtained in the Markov chain context, can be found in
Chapters 9 and 10.
Example 7.27 Consider a Coxian distribution with four phases and rates μ1 = 1, μ2 = 2, μ3 = 4,
μ4 = 8. On completion of phase i = 1, 2, 3, the process proceeds to phase i + 1 with probability .5
or enters the sink phase with probability .5. This allows us to write
σ ′ = (1, 0, 0, 0, | 0) = (σ | 0),
Q =
⎛
⎜
⎜
⎜
⎜
⎝
−μ1 .5μ1
0
0
.5μ1
0
−μ2 .5μ2
0
.5μ2
0
0
−μ3 .5μ3 .5μ3
0
0
0
−μ4 μ4
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎝
−1
.5
0
0 .5
0 −2
1
0 1
0
0 −4
2 2
0
0
0 −8 8
0
0
0
0 0
⎞
⎟
⎟
⎟
⎟
⎠
=
S S0
0 0

.
Given that
S−1 =
⎛
⎜
⎜
⎝
−1 −.25 −.0625 −.015625
0 −.5
−.125
−.03125
0
0
−.25
−.0625
0
0
0
−.125
⎞
⎟
⎟
⎠,
we ﬁnd
−σ S−1 = (1, .25, .0625, .015625).
Therefore the expectation of this Coxian (and the mean time to absorption in the Markov chain) is
−σ S−1e = 1.328125.
Also, the mean time spent in
phase1 :
1/μ1 = 1 = x1,
phase2 :
.5/μ2 = .25 = x2,
phase3 :
(.5 × .5)/μ3 = .0625 = x3,
phase4 :
.53/μ4 = .015625 = x4.
7.6.7 Fitting Phase-Type Distributions to Means and Variances
Phase-type distributions play an important role in modeling scenarios in which only the ﬁrst and
second moments of a distribution are known or indeed when the only information one possesses


7.6 Phase-Type Distributions
173
For the second moment
E[X2] = d2
ds2

(1 −α)
μ
s + μ + α
μr
(s + μ)r

s=0
= d
ds

−(1 −α)
μ
(s + μ)2 −α
μrr
(s + μ)r+1

s=0
=

(1 −α)
2μ
(s + μ)3 + α μrr(r + 1)
(s + μ)r+2

s=0
= (1 −α) 2
μ2 + αr(r + 1)
μ2
.
(7.13)
It follows that the variance is given by
Var [X] = E[X2] −E[X]2 = 2(1 −α) + αr(r + 1) −(1 −α + αr)2
μ2
and the squared coefﬁcient of variation by
C2
X = Var [X]
E[X]2 = 2(1 −α) + αr(r + 1) −(1 −α + αr)2
(1 −α + αr)2
.
(7.14)
We are required to select three parameters, r, α, and μ, that satisfy the two equations (7.12)
and (7.14). Additionally, as the analysis of the Erlang-r distribution suggests, we should choose
r to be greater than 1/C2
X. Since it is advantageous to choose r as small as possible, we shall set
r =
8 1
C2
X
9
.
Having thus chosen a value for r, and with our given value of C2
X, we may use Equation (7.14),
which involves only r, C2
X, and α, to ﬁnd an appropriate value for α. Marie [32] recommends the
choice
α = r −2C2
X +
.
r2 + 4 −4rC2
X
2(C2
X + 1)(r −1)
.
Finally, we may now compute μ from Equation (7.12) and obtain
μ = 1 + α(r −1)
E[X]
.
Example 7.28 Let us construct a phase-type distribution having expectation E[X] = 4 and variance
Var [X] = 5.
With these parameters, we have C2
X = 5/16 = 0.3125 which is less than 1 and we may use
the analysis just developed. We choose parameters for a Coxian distribution as represented in
Figure 7.17:
r =
8 1
C2
X
9
=
8
1
0.3125
9
= ⌈3.2⌉= 4,
α = r −2C2
X +
.
r2 + 4 −4rC2
X
2(C2
X + 1)(r −1)
= 4 −2(0.3125) + √16 + 4 −16(0.3125)
2(0.3125 + 1)(3)
= 0.9204,
μ = 1 + α(r −1)
E[X]
= 1 + 3(0.9204)
4
= 0.9403.

174
Continuous Distribution Functions
Let us check our answers by computing the expectation and variance of this Coxian:
E[X] = (1 −α) 1
μ + α r
μ = (0.0796)
1
0.9403 + (0.9204)
4
0.9403 = 0.0847 + 3.9153 = 4.0,
Var [X] = 2(1 −α) + αr(r + 1) −(1 −α + αr)2
μ2
= 2(0.0796) + (0.9204)20 −[0.0796 + 4(0.9204)]2
(0.9403)2
= 4.4212
0.8841 = 5.0.
For coefﬁcients of variation greater than 1, it sufﬁces to use a two-phase Coxian. This is
represented in Figure 7.18, where it is apparent that we need to ﬁnd three parameters, namely,
μ1, μ2, and α.
μ
μ1
2
α
1−α
1
Figure 7.18. Suggested Coxian for C2
X ≥0.5.
Previously, we found the expectation and squared coefﬁcient of variation of this Coxian to be
given by
E[X] = μ2 + αμ1
μ1μ2
,
C2
X = μ2
2 + αμ2
1(2 −α)
(μ2 + αμ1)2
.
Given E[X] and C2
X, our task is to use these equations to ﬁnd the three parameters μ1, μ2, and α.
From the inﬁnite number of solutions possible, the following is frequently recommended since it
yields particularly simple forms [32]:
μ1 =
2
E[X],
α =
1
2C2
X
,
and μ2 =
1
E[X] C2
X
= αμ1.
This distribution is valid for values of C2
X that satisfy C2
X ≥0.5, and not just for those values greater
than or equal to 1.
Example 7.29 A random variable X having expectation E[X] = 3 and standard deviation equal to
σX = 4 may be modeled as a two-phase Coxian. Given that E[X] = 3 and C2
X = 16/9, we may
take the parameters of the Coxian distribution to be
μ1 =
2
E[X] = 2
3,
α =
1
2C2
X
= 9
32,
and μ2 =
1
E[X] C2
X
= 3
16.

7.6 Phase-Type Distributions
175
To check, we ﬁnd the expectation and standard deviation of this Coxian
μ2 + αμ1
μ1μ2
=
3
16 + 9
32
2
3
2
3
3
16
=
6
16
1
8
= 3
μ2
2 + αμ2
1(2 −α)
(μ2 + αμ1)2
=
9
256 + 9
32
4
9
55
32
# 3
16 + 9
32
2
3
$2 =
0.25
0.1406 = 1.7778 = 16
9 .
μ
α
1−α
μ1
2
1
1
Figure 7.19. Two-phase hyperexponential distribution.
As an alternative to the Coxian-2, a two-phase hyperexponential distribution such as that shown
in Figure 7.19 may also be used to model distributions for which C2
X ≥1. As is the case for a
Coxian-2 distribution, a two-phase hyperexponential distribution is not deﬁned uniquely by its ﬁrst
two moments so that there is a considerable choice of variations possible. One that has attracted
attention is to balance the means by imposing the additional condition
α
μ1
= 1 −α
μ2
.
This leads to the formulae
α = 1
2

1 +
/
C2
X −1
C2
X + 1

,
μ1 =
2α
E[X],
and μ2 = 2(1 −α)
E[X] .
Example 7.30 Let us return to the previous example of a random variable with expectation
E[X] = 3 and squared coefﬁcient of variation C2
X = 16/9 and see what parameters we obtain
for the two-phase hyperexponential:
α = 1
2

1 +
0
C −1
C + 1

= 1
2

1 +
/
7/9
25/9

= 0.7646,
μ1 =
2α
E[X] = 1.5292
3
= 0.5097,
and
μ2 = 2(1 −α)
E[X]
= 0.4709
3
= 0.1570.
We now check these results. The expectation of the two-phase hyperexponential is given by
E[X] = α
μ1
+ 1 −α
μ2
= 0.7646
0.5097 + 0.2354
0.1570 = 1.50 + 1.50 = 3.0.

176
Continuous Distribution Functions
Similarly, the squared coefﬁcient of variation is given by
C2
X = 2α/μ2
1 + 2(1 −α)/μ2
2
(α/μ1 + (1 −α)/μ2)2 −1
=
1.5292/0.2598 + 0.4708/0.0246
(0.7646/0.5097 + 0.2354/0.1570)2 −1
= 25
9 −1 = 16
9 .
Although we considered the possibility of matching only the ﬁrst two moments of a distribution
with phase-type representations, techniques exist for matching higher moments. However, this is
beyond what we seek to accomplish in this text. The interested reader requiring further information
may wish to consult the relevant literature, particularly [41] and the references therein.
7.7 Exercises
Exercise 7.1.1 Let X be a continuous, uniformly distributed random variable with both expectation and
variance equal to 3. Find the probability density function of X and the probability that X > 2.
Exercise 7.1.2 Let X be uniformly distributed on the interval (0, 4). What is the probability that the roots of
z2 + 2Xz −2X + 15 = 0 are real?
Exercise 7.1.3 Let X1 and X2 be two independent random variables with probability density functions given
by pX1(x1) and pX2(x2) respectively. Let Y be the random variable Y = X1 + X2. Find the probability density
function of Y when both X1 and X2 are uniformly distributed on [0, 1]. Illustrate the probability density
function of Y by means of a ﬁgure.
Exercise 7.2.1 Derive the expectation and variance of an exponential random variable from its Laplace
transform.
Exercise 7.2.2 The density function of an exponential random variable is given by
fX(x) =
.5e−x/2,
x > 0,
0
otherwise.
What is the expectation and standard deviation of X? Write down its cumulative distribution function and ﬁnd
the probability that X is greater than 4.
Exercise 7.2.3 What is the probability that the value of an exponential random variable with parameter λ lies
within 2σ of its expectation?
Exercise 7.2.4 Let X be an exponential random variable whose variance is equal to 16. Find Prob{1 ≤X ≤2}.
Exercise 7.2.5 The time spent waiting in the emergency room of a hospital before one is called to see a doctor
often seems to be exponentially distributed with mean equal to 60 minutes. Let X be the random variable that
describes this situation. Find the following probabilities:
(a) Prob{X > 30}, (b) Prob{X > 90}, (c) Prob{X > 90|X > 60}.
Exercise 7.2.6 When Charles enters his local bank, he ﬁnds that the two tellers are busy, one serving Alice
and the other serving Bob. Charles’ service will begin as soon as the ﬁrst of the two tellers becomes free. Find
the probability that Charles will not be the last of Alice, Bob, and himself to leave the bank, assuming that the
time taken to serve any customer is exponentially distributed with mean 1/λ.
Exercise 7.2.7 Let X be an exponentially distributed random variable and let α and β be two constants such
that α ≥0 and β ≥0. Prove that
Prob{X > α + β} = Prob{X > α}Prob{X > β}

7.7 Exercises
177
and from it deduce that
Prob{X > α + β|X > α} = Prob{X > β}.
Exercise 7.2.8 Let X be an exponentially distributed random variable with parameter λ. Find the probability
density function and the cumulative distribution function of (a) Y = X 2 and (b) Z =
√
X.
Exercise 7.2.9 Let X1 and X2 be two independent and identically distributed exponential random variables
with parameter λ. Find the density function of Y = X1 + X2 using the convolution approach.
Exercise 7.2.10 Show that a geometric sum of exponentially distributed random variables is itself exponen-
tially distributed.
Hint: Let N be a geometrically distributed random variable, let X1, X2, . . . be independent and identically
distributed exponential random variables with parameter μ, and let S = N
n=1 Xn. Now evaluate the Laplace
transform of S, E[e−sS], using a conditioning argument.
Exercise 7.3.1 Show, by taking derivatives of the moment generating function of the standard normal
distribution, that this distribution has mean value zero and variance equal to 1.
Exercise 7.3.2 Using tables of the cumulative distribution of a standard normal, random variable, compute the
following probabilities of a normally distributed random variable with μ = 10 and σ = 4. (a) Prob{X < 0} ,
(b) Prob{X > 12}, and (c) Prob{−2 ≤X ≤8}.
Exercise 7.3.3 The time spent waiting for a bus is normally distributed with mean equal to 10 minutes and
standard deviation equal to 10 minutes. Find (a) the probability of waiting less than 12 minutes and (b) the
probability of waiting more than 15 minutes.
Exercise 7.3.4 A normally distributed random variable has mean μ = 4. However, instead of being given the
standard deviation, we are told that Prob{2 ≤X ≤6} = 0.4. Use this to ﬁnd the standard deviation of X.
Exercise 7.3.5 The most recent census population estimates that there are 12.5 million octogenarians in a total
population of 250 million. Assume that the age (in years) of the population is normally distributed with mean
equal to 38 years. Find the standard deviation of this distribution and estimate the total number of individuals
older than 55.
Exercise 7.3.6 The age of a randomly selected person in a certain population is a normally distributed random
variable X. Furthermore, it is known that Prob{X ≤40} = 0.5 and Prob{X ≤30} = 0.25. Find the mean μ
and standard deviation σ of X. Also ﬁnd Prob{X > 65} and the percentage of those over 65 who are older than
85.
Exercise 7.3.7 It is estimated that 15% of trafﬁc accidents are caused by impatient drivers. Of the 400 accidents
that occurred last week, estimate the probability that no more than 50 were due to impatient drivers.
Exercise 7.3.8 Suppose 2,880 fair dice are thrown all at once. Find the probability that the number of 1’s that
show lies between 450 and 470.
Exercise 7.4.1 Prove that the variance of a random variable having a gamma distribution with parameters α
and β is given by Var [X] = αβ2.
Exercise 7.4.2 A random variable X has a gamma distribution with parameters β = 1 and α = 6. Find (a)
Prob{X ≤5}, (b) Prob{X > 4}, and (c) Prob{4 ≤X ≤8}.
Exercise 7.4.3 The length of time that a supermarket can keep milk on its shelves before it goes bad is a gamma
random variable X with parameters β = 3 and α = 4. Find (a) Prob{X ≤12}, (b) Prob{12 ≤X < 15}, and
(c) the expected shelf life of a carton of milk in this supermarket.
Exercise 7.4.4 The time it takes Kathie to cook dinner has a gamma distribution with mean 20 minutes and
variance 40 minutes. Find the probability that it will take more than 30 minutes to cook dinner, (a) using tables
of the Poisson cumulative distribution function, and (b) using tables of the incomplete gamma function.
Exercise 7.4.5 The time it takes to eat breakfast has a gamma distribution with mean ﬁve minutes and standard
deviation two minutes. Find the parameters α and β of the distribution and the probability that it takes more
than eight minutes to eat breakfast.

178
Continuous Distribution Functions
Exercise 7.5.1 Derive the second moment of a Weibull random variable and from it show that
Var [X] = η2



1 + 2
β

−

1 + 1
β
2
.
Exercise 7.5.2 The time in minutes to execute a certain task is represented as a Weibull random variable X
with parameters η = 8 and β = 2. Give the probability density function and the cumulative distribution of
X. Also ﬁnd the expectation and standard deviation of X and the probability that the task takes longer than 16
minutes.
Exercise 7.5.3 Let the lifetime of component i, i = 1, 2, . . . , n, in a series of independent components have a
Weibull distribution with parameters ηi, i = 1, 2, . . . , n, and β. Show that the lifetime of the complete system
has a Weibull distribution and ﬁnd its parameters.
Exercise 7.5.4 A system contains two components arranged in series, so that when one of the components fails,
the whole system fails. The time to failure of each component is a Weibull random variable with parameters
ηc = 8 and βc = 2. What is the expected lifetime of each component and the expected lifetime of the system.
Find the probability that the system survives until time t = 4.
Exercise 7.5.5 The time to failure of each of two embedded computers in a time-critical space mission is an
exponential random variable with mean time to failure equal to 5,000 hours. At least one of the computers
must function for a successful mission outcome. What is the longest time that the mission can last so that the
probability of success is 0.999?
Exercise 7.5.6 Consider a system that contains four components with reliability functions given respec-
tively by
R1(t) = e−αt, R2(t) = e−βt, R1(t) = e−γ t, and R1(t) = e−δt.
What is the reliability function of the system that has these four components arranged (a) in series and (b) in
parallel? A different system arranges these components so that the ﬁrst two are in series, the last two are in
series, but the two groups of two are in parallel with each other. What is the reliability function in this case?
Exercise 7.5.7 Find the hazard rate of a product whose lifetime is uniformly distributed over the interval (a, b),
with a < b.
Exercise 7.5.8 The hazard rate of a certain component is given by
h(t) = et/4
5 ,
t > 0.
What are the cumulative hazard function and the reliability function of this component? What is the probability
that it survives until t = 2.
Exercise 7.5.9 The hazard rate of a component is given by
h X(t) = α
2 t−1/2 + β
4 t−3/4.
Find the reliability function of the component.
Exercise 7.6.1 The number of telephone calls to a central ofﬁce is a Poisson process with rate μ = 4 per ﬁve
minute period. Find the probability that the waiting time for three or more calls to arrive is greater than two
minutes. Give also the mean and standard deviation of the time to observe three calls arrive.
Exercise 7.6.2 Let X be an Erlang-r random variable. Construct phase-type representations of X when its
expectation and standard deviation are as follows: (a) E[X] = 4, σX = 2, (b) E[X] = 6, σX = 2, and (c)
E[X] = 3, σX = 2. In each case, specify the number of exponential phases, r, in the representation and the
parameter of each phase μ. Also in each case give the probability density function of X.
Exercise 7.6.3 It has been observed that the time bank customers spend with a teller has an average value of
four minutes and a variance of nine minutes. This sevice time is to be modeled as a mixed Erlang distribution

7.7 Exercises
179
whose probability density function is
fX(t) = α μe−μt + (1 −α) μ2te−μt,
t ≥0.
What values should be assigned to the parameters α and μ?
Exercise 7.6.4 The manufacturing process for widgets involves three steps. Assume that the time spent in each
of these three steps is an exponential random variable with parameter values equal to 2, 4, and 5 respectively.
Find the expected duration, variance, and squared cefﬁcient of variation of the manufacturing process. Also
ﬁnd the probability density function of the manufacturing time.
Exercise 7.6.5 A person chooses to go to McDonald’s for breakfast in the morning with probability 0.5.
Otherwise, with probability 0.3 he goes to Hardee’s and with probability 0.2 he goes to IHOP. If the mean time
spent eating breakfast is 15 minutes in McDonald’s, 12 minutes in Hardee’s, and 20 minutes in IHOP and if
the distribution is exponential, ﬁnd the expectation and standard deviation of the time spent eating breakfast.
Also compute the squared coefﬁcient of variation and show that it is greater than 1.
Exercise 7.6.6 Prove that the ﬁrst and second moments of a hyperexponential-2 random variable X, with
parameters μ1, μ2, and α1, are respectively given by
E[X] = α1
μ1
+ α2
μ2
and
E[X 2] = 2α1
μ2
1
+ 2α2
μ2
2
,
where α2 = 1 −α1.
Exercise 7.6.7 Professor Nozit has arranged his lectures into modules. The time it takes to present the
theoretical aspects of a module is exponentially distributed with a mean of 45 minutes. Eighty percent of
the time, Professor Nozit also provides an example of the application of the theory, and the time to complete
this is also exponentially distributed with a mean of 20 minutes. Find the mean and standard deviation of the
length of Professor Nozit’s lectures.
Exercise 7.6.8 Derive the expectation of the Coxian-r random variable that is represented graphically in Figure
7.14. Show that the answer you compute using the notation of Figure 7.14 is the same as that derived with
reference to Figure 7.13.
Exercise 7.6.9 Show that a Cox-2 random variable with parameters μ1 = 2/E, μ2 = 1/(EC), and α = 1/2C
has expectation equal to E and squared coefﬁcient of variation equal to C.

Chapter 8
Bounds and Limit Theorems
We shall begin this section with three theorems that allow us to bound probabilities. They are of
interest when bounds are sought on the tail of the distribution, i.e., on the probability that a random
variable X exceeds some given quantity—a quantity that is usually greater than the mean. The ﬁrst,
the Markov inequality, uses only the expectation of a random variable and provides bounds that are
often rather loose. The second, the Chebychev inequality, uses both the expectation and the variance
of a random variable and usually generates tighter bounds. The third, the Chernoff bound, requires a
knowledge of the moment generating function of a random variable. Since a knowledge of a moment
generating function implicitly implies a knowledge of moments of all orders, we should expect the
Chernoff bound to provide tighter bounds than those obtained from either the Markov or Chebychev
inequalities.
8.1 The Markov Inequality
Let X be a random variable and h a nondecreasing, nonnegative function. The expectation of h(X),
assuming it exists, is given by
E[h(X)] =
 ∞
−∞
h(u) fX(u)du,
and we may write
 ∞
−∞
h(u) fX(u)du ≥
 ∞
t
h(u) fX(u)du ≥h(t)
 ∞
t
fX(u)du = h(t)Prob{X ≥t}.
This leads directly to the so-called Markov inequality, given by
Prob{X ≥t} ≤E[h(X)]
h(t)
.
(8.1)
It is frequently applied in the case when X is nonnegative and h(x) = x, when it reduces to
Prob{X ≥t} ≤E[X]
t
,
t > 0.
This inequality is exceedingly simple, requiring only the mean value of the distribution. It is used
primarily when t is large (in bounding the tail of a distribution), when E[X]/t is small and a
relatively tight bound can be obtained. Otherwise the bound can be very loose. Bear in mind also
that this inequality is applicable only to random variables that are nonnegative.
Example 8.1 Let X be the random variable that denotes the age (in years) of a randomly selected
child in Centennial Campus Middle school. If the average child’s age at that school is 12.5 years,
then, using the Markov inequality, the probability that a child is at least 20 years old satisﬁes the
inequality
Prob{X ≥20} ≤12.5/20 = 0.6250,

8.2 The Chebychev Inequality
181
which is a remarkably loose bound. While this bound is obviously correct, the probability that there
is a child aged 20 in a middle school should be very close to zero.
8.2 The Chebychev Inequality
The Markov inequality is a ﬁrst-order inequality in that it requires only knowledge of the mean
value. The Chebychev inequality is second order: it requires both the mean value and the variance
of the distribution. It may be derived from the Markov inequality as follows. Let the variance σ 2
X be
ﬁnite and deﬁne a new random variable Y as
Y ≡(X −E[X])2.
As in the simple form of the Markov inequality, let h(x) = x. Then, from the Markov inequality,
Prob{Y ≥t2} ≤E[Y]
t2
.
Observe that
Prob{Y ≥t2} = Prob{(X −E[X])2 ≥t2} = Prob{|X −E[X]| ≥t}
and that
E[Y] = E[(X −E[X])2] = σ 2
X,
from which we immediately have the Chebychev inequality
Prob{|X −E[X]| ≥t|} ≤σ 2
X
t2 .
(8.2)
From Equation (8.2), it is apparent that the random variable X does not stray far from its mean value
E[X] when its variance σ 2
X is small. If we set t = c σX, for some positive constant c, we obtain
Prob{|X −E[X]| ≥c σX} ≤1
c2 ,
(8.3)
and thus the probability that a random variable is greater than c standard deviations from its mean
is less than 1/c2. Setting c = 2 for example, shows that the probability that a random variable
(indeed any random variable) is more than two standard deviations from its mean is less that 1/4.
An alternative form of Equation (8.3) is
Prob{|X −E[X]| ≤c σX} ≥1 −1
c2 .
This form of the Chebychev inequality is often used to compute conﬁdence intervals in simulation
experiments as we shall see in later chapters.
As we have mentioned, the Chebychev inequality may be applied to any random variable,
unlike the Markov inequality which is applicable only to random variables that are nonnegative.
Furthermore, it generally provides a better bound, since it incorporates the variance as well as the
expected value of the random variable into the computation of the bound.
Example 8.2 Let us return to the same example of middle school children and recompute the bound
using the Chebychev inequality. In this case we also need the variance of the ages of the children,
which we take to be 3. We seek the probability that a child at the school could be as old as 20. We
need to ﬁrst put this into the form needed by the Chebychev equation:
Prob{X ≥20} = Prob{(X −E[X]) ≥(20 −E[X])} = Prob{(X −E[X]) ≥7.5}.

182
Bounds and Limit Theorems
However,
Prob{(X −12.5) ≥7.5} ̸= Prob{|X −12.5| ≥7.5} = Prob{5 ≤X ≥20},
so that we cannot apply the Chebychev inequality to directly compute Prob{X ≥20}. Instead, we
can compute a bound for Prob{5 ≤X ≥20} and obtain
Prob{|X −E[X]| ≥7.5} ≤
3
(7.5)2 = 0.0533,
which is still a much tighter bound than that obtained previously.
8.3 The Chernoff Bound
As is the case the Chebychev inequality, the Chernoff bound may be derived from the Markov
inequality. Setting h(x) = eθx for some θ ≥0 in Equation (8.1), and using the fact that the moment
generating function of a random variable is given by
MX(θ) = E[eθ X],
we obtain
Prob{X ≥t} ≤E[eθ X]
eθt
= e−θtMX(θ) for all θ ≥0.
Since this holds for all θ ≥0 and since it is in our interest to have the smallest lowest bound
possible, we may write
Prob{X ≥t} ≤min
all θ≥0 e−θtMX(θ),
which is known as the Chernoff bound.
Example 8.3
The moment generating function of a normally distributed random variable with
mean value μ = 4 and variance σ 2 = 1 is given by
MX = eμθ+σ 2θ2/2 = e4θ+θ2/2.
The Chernoff bound yields
Prob{X ≥8} ≤min
θ≥0 e−8θe4θ+θ2/2 = min
θ≥0 e(θ2−8θ)/2.
Observe that the upper bound is minimized when θ2 −8θ is minimized. Taking the derivative of this
function and setting it equal to zero gives 2θ −8 = 0 so that the minimum is achieved when θ = 4.
The Chernoff bound then gives
Prob{X ≥8} ≤e(θ2−8θ)/2
θ=4 = e−8 = 0.0003355.
The reader may wish to note that the bound computed from the Markov inequality is 0.5; that
obtained from the Chebychev inequality is 0.0625, while the exact value of this probability is
0.0000317.
8.4 The Laws of Large Numbers
Let X1, X2, . . . , Xn be n independent and identically distributed random variables. We may view
these n random variables as n independent trials of the same probability experiment. In this light
they are sometimes referred to as a random sample of size n from the experimental distribution. We

8.4 The Laws of Large Numbers
183
shall assume that the mean and variance are both ﬁnite and given by E[X] and σ 2
X, respectively.
We would expect that as more experiments are conducted, i.e., as n becomes large, the average
value obtained in the n experiments should approach the expected value, E[X]. This is exactly what
happens. Setting
Sn ≡
n

i=1
Xi,
the average value obtained by these n experiments is given by Sn/n. Other names for this computed
mean value Sn/n are the arithmetic mean and the statistical average. As n becomes large, we expect
Sn/n to be close to E[X]: the weak law of large numbers provides justiﬁcation for this assertion.
Notice that
E

Sn/n
 
= E[X]
and
Var

Sn/n
 
= nσ 2
X
n2
= 1
n σ 2
X,
(8.4)
which shows that the mean is independent of n whereas the variance decreases as 1/n. Thus, as n
becomes large, the variance approaches 0 and the distribution of Sn/n becomes more concentrated
about E[X].
Let us now apply the Chebychev inequality to Sn/n. Substituting from (8.4), we obtain
Prob

Sn
n −E[X]
 ≥ϵ
%
≤σ 2
X/n
ϵ2
= σ 2
X
nϵ2 .
(8.5)
This leads directly to the weak law of large numbers. Taking the limit of both sides of (8.5) as
n →∞and for any ﬁxed value of ϵ, no matter how small, we ﬁnd
lim
n→∞Prob

Sn
n −E[X]
 ≥ϵ
%
= 0.
(8.6)
We may interpret this inequality as follows. Irrespective of how small ϵ may be, it is possible to
select a large enough value of n so that the probability of Sn/n being separated by more than ϵ from
E[X] converges to zero. Alternatively, we may write
lim
n→∞Prob

Sn
n −E[X]
 < ϵ
%
= 1,
i.e., Prob{|Sn/n −E[X]| < ϵ} approaches 1 as n →∞. Thus, as the number of experiments
increases, it becomes less likely that the statistical average differs from E[X].
In the above derivation we assumed that both the expectation and variance of the random
variables X1, X2, . . . were ﬁnite. However, the weak law of large numbers requires only that the
expectation be ﬁnite. Both Equations (8.5) and (8.6) hold when the variance is ﬁnite, but only
Equation (8.6) holds when the variance is inﬁnite. Equation (8.5) is a more precise statement since
it provides a bound in terms of n. We state the theorem as follows:
Theorem 8.4.1 (Weak law of large numbers) Let X1, X2, . . . , Xn be n independent and identi-
cally distributed random variables with ﬁnite mean E[X]. Then, setting Sn = X1 + X2 + · · · + Xn
and given any ϵ > 0, however small,
lim
n→∞Prob

Sn
n −E[X]
 ≥ϵ
%
= 0.

184
Bounds and Limit Theorems
Convergence in this sense is said to be convergence in a probabilistic sense: a sequence of random
variables Xn is said to converge in probability to a random variable X if, for any ϵ > 0,
lim
n→∞Prob{|Xn −X| ≥ϵ} = 0.
In making explicit reference to a weak law of large numbers, we have implicitly implied the
existence of a strong law of large numbers, and indeed such a law exists. The strong law of large
numbers, a proof of which may be found in Feller [15], states that with probability 1, the sequence
of numbers Sn/n converges to E[X] as n →∞. We have
Theorem 8.4.2 (Strong law of large numbers)
Let X1, X2, . . . , Xn be n independent and
identically distributed random variables with ﬁnite mean E[X]. Then, setting Sn = X1 + X2 +
· · · + Xn and given any ϵ > 0, however small,
Prob

lim
n→∞

Sn
n −E[X]
 ≥ϵ
%
= 0.
The difference between the weak and strong laws of large numbers is subtle. Observe that, in
contrast with the weak law of large numbers, the limit in the strong law is taken inside the probability
braces. The weak law concerns the average behavior of many sample paths, some of which might
not converge to E[X], whereas the strong law is very precise and states that each sequence converges
to E[N] for sufﬁciently large n. It follows that the strong law of large numbers implies the weak
law. In practical applications of probability theory, the difference is usually inconsequential.
8.5 The Central Limit Theorem
In our discussion of the normal distribution, we saw that the sum of n normally distributed random
variables was itself normally distributed. Furthermore, we stated that the normal distribution could
be used to approximate the discrete binomial distribution. In this section, we take this one step
further. The central limit theorem, which we state below without proof, asserts that the sum of n
independent random variables may be approximated by a normal distribution, irrespective of the
particular distribution of the n random variables. It is for this reason that the normal distribution
plays such an important role in practical applications.
We consider ﬁrst the case when the n random variables are independent and identically
distributed having mean value E[Xi] = μ and variance Var [Xi] = σ 2 for i = 1, 2, . . . , n. Then the
sum of these n random variables, X1 + X2 + · · · + Xn, for n sufﬁciently large, has an approximate
normal distribution with mean nμ and variance nσ 2. We write this as
Prob
 X −nμ
σ√n
≤x
%
≈Prob{Z ≤x}.
(8.7)
Example 8.4 The tube of toothpaste that sits on our bathroom sink has a mean lifetime of 30 days
and a standard deviation of 5 days. When one tube is ﬁnished it is immediately replaced by a new
tube. Kathie has been to the sales and has returned with 12 new tubes of toothpaste. We shall let
Xi, i = 1, 2, . . . , n, be the random variable that represents the lifetime of the ith tube and it seems
reasonable to assume that these n random variables are independent and identically distributed.
Kathie would like to know the probability that these 12 tubes will last a full year of 365 days or
more.
The random variable X = X1 + X2 + · · · + Xn represents the lifetime of these 12 tubes
of toothpaste. Under the assumption that n
= 12 is sufﬁciently large to apply the normal
approximation, we obtain the mean value and standard deviation of X to be
E[X] = 12 × 30 = 360 days
and
σX = 5 ×
√
12 = 17.3205.

8.5 The Central Limit Theorem
185
Taking these for the mean and standard deviation of the normal distribution, the probability that the
12 tubes will last 365 days or less is
Prob{X ≤365} = Prob

Z ≤365 −360
17.3205
%
= Prob{Z ≤0.2887} = 0.6141.
The probability that they will last more than one year is 0.3859.
Consider next the case of the central limit theorem applied to a sample mean Sn = (X1 +
X2 + · · · + Xn)/n. Once again, the n random variables Xi, i = 1, 2, . . . , n, are independent and
identically distributed with mean values E[Xi] = μ and variances Var [Xi] = σ 2. In this case, for
sufﬁciently large values of n, the sample mean Sn is approximately normally distributed with mean
μ and variance σ 2/n. We write this as
Prob
 Sn −μ
σ/√n ≤x
%
≈Prob{Z ≤x}.
(8.8)
Example 8.5 A survey is being conducted to compute the mean income of North Carolina families.
Let us suppose that this is actually equal to $32,000 and has a standard deviation of $10,000. These
ﬁgures are unknown to those conducting the survey. What is the probability that the mean value
computed by the survey is within 5% of the true mean if (a) 100 families are surveyed, and (b) 225
families are surveyed?
We seek the probability that the survey will yield a mean value between $30,400 and $33,600
(since 5% of $32,000 is $1,600). Using E[Sn] = 32,000 and for the ﬁrst case when only 100
families are surveyed, σSn = 10,000/10 = 1,000, we have
Prob{30,400 < Sn < 33,600} = Prob
30,400 −32,000
1,000
< Z < 33,600 −32,000
1,000
%
= Prob{−1.6 < Z < 1.6} = 0.9452 −0.0548 = 0.8904.
When 225 families are surveyed, we have E[Sn] = 32,000 and σSn = 10, 000/15 = 666.67. This
time we get
Prob{30,400 < Sn < 33,600} = Prob
30,400 −32,000
666.67
< Z < 33,600 −32,000
666.67
%
= Prob{−2.4 < Z < 2.4} = 0.9918 −0.0082 = 0.9836.
This example shows that it is not necessary to sample large numbers of families to get an accurate
estimate of the mean. The difﬁculty with sampling of course lies elsewhere: it lies in ensuring that
those families chosen are randomly selected and representative of the population as a whole.
In the general statement of the central limit theorem, the random variables do not need to be
identically distributed.
Theorem 8.5.1 (Central limit theorem)
Let X1, X2, . . . , Xn be independent random variables
whose expectations E[Xi] = μi and variances Var [Xi] = σ 2
i are both ﬁnite. Let Yn be the
normalized random variable
Yn =
n
i=1 Xi −n
i=1 μi
1n
i=1 σ 2
i
.

186
Bounds and Limit Theorems
Then E[Yn] = 0 and Var [Yn] = 1, and under certain rather broad conditions,
lim
n→∞FYn(t) = Prob{Yn ≤t} =
 t
−∞
1
√
2π
e−u2/2du.
In other words, the limiting distribution of Yn is standard normal, N(0, 1).
When the random variables are all identically distributed, as well as being independent, then Yn
simpliﬁes to
Yn =
n
i=1 Xi −nμ
σ√n
,
where μ = n
i=1 μi/n and σ 2 = n
i=1 σ 2
i /n.
Example 8.6 A fair coin is tossed 400 times. We shall use the central limit theorem to compute an
approximation to the probability of getting at least 205 heads. Let Xi be the random variable that
has the value 1 if a head appears on toss i and the value 0 otherwise. The number of heads obtained
in n tosses is given by Yn = X1 + X2 +· · ·+ Xn. With n = 400, μ = 1/2, and σ 2 = 1/4, we obtain
Prob{Yn ≥205} = Prob
Yn −nμ
σ√n
≥205 −nμ
σ√n
%
= Prob
Yn −200
10
≥0.5
%
= Prob{Z ≥0.5}.
Since Prob{Z ≥0.5} = 1 −0.6915, the computed approximation is 0.3085.
The above example is instructive. Using the same approach, we may compute Prob{Yn ≤204}
and we ﬁnd that this works out to be equal to 0.6554 (see Exercise 8.5.3). We know that Prob{Yn ≤
204} + Prob{Yn ≥205} must be equal to 1, but the sum of our two approximations gives only
0.6554 + 0.3085 = 0.9639! The reason for this error is primarily because we are approximating a
discrete distribution with the continuous normal distribution. This difﬁculty can be circumvented by
replacing each of the integers 204 and 205 with 204.5, in essence dividing the range (204, 205)
between the two of them. This correction is called the continuity correction or the histogram
correction and should be used whenever the central limit theorem approximation is applied to
integer-valued random variables. If we now compute Prob{Yn ≥204.5} (this is all we need since
we have now ensured that Prob{Yn ≤204.5} = 1 −Prob{Yn ≥204.5}), we ﬁnd
Prob{Yn ≥204.5} = Prob
Yn −200
10
≥0.45
%
= Prob{Z ≥0.45} = 1 −0.6736 = 0.3264,
which turns out to be a much better approximation than that obtained previously.
The same approach may be used when we are required to ﬁnd the probability that X lies in some
interval [a, b]. The formula to use in this case is
Prob{a ≤X ≤b} ≈	

b + 1
2 −μ
σ

−	

a −1
2 −μ
σ

.
Example 8.7 Returning to the previous example, let us now compute Prob{201 ≤X ≤204}.
Inserting a = 201 and b = 204 into the approximation given above, we obtain
Prob{201 ≤X ≤204} ≈Prob
Yn −200
10
≤0.45
%
−Prob
Yn −200
10
≤0.05
%
= Prob{Z ≤0.45} −Prob{Z ≤0.05} = 0.6736 −0.5199 = 0.1537.

8.6 Exercises
187
Indeed, even when this interval shrinks down to a single point, the continuity approximation
provides a meaningful approximation, even though for a continuous random variable the probability
of it assuming a single point is zero.
Example 8.8 To use the normal approximation to ﬁnd the probability of getting exactly 204 heads
in 400 tosses of a fair coin, we set both a and b equal to 204 and write
Prob{X = 204} ≈Prob
Yn −200
10
≤0.45
%
−Prob
Yn −200
10
≤0.35
%
= Prob{Z ≤0.45} −Prob{Z ≤0.35} = 0.6736 −0.6368 = 0.0368.
The use of a −1
2 and b + 1
2 rather than a and b is essential when approximating a binomial
distribution for which √np(1 −p) is small. It is also essential when a and b are close together,
irrespective of the value of √np(1 −p). As we noted above, the continuity correction should be
applied when the normal distribution is used to approximate an integer-valued random variable.
In our examples, the range of the discrete random variable has always been the set of integers.
However, the range of a discrete random variable can be the set {0, ±δ, ±2δ, ±3δ, . . .} for any real
number δ, and the continuity correction still conveniently applies.
A ﬁnal word of warning. Not all distributions obey the central limit theorem. One example is the
Cauchy random variable, which has the probability density function
f (x) =
1
π(1 + x2).
The expectation and all higher moments of a Cauchy random variable are not deﬁned. In particular,
it does not have a ﬁnite variance and therefore does not satisfy the conditions of the central limit
theorem.
8.6 Exercises
Exercise 8.1.1 Let X be the random variable that denotes the number of trials until the ﬁrst success in a
probability experiment consisting of a sequence of Bernoulli trials, each having probability of success p = 1/4.
Use the Markov inequality to ﬁnd an upper bound on the probability that X > 5 and compare this with the
exact probability, computed from the geometric distribution.
Exercise 8.1.2 Let ten seconds be the mean time between the arrival of electronic orders to purchase stocks.
The orders are (almost) instantaneously routed by a clerk from his computer to a trader on the stock ﬂoor. The
clerk would like to go next door to buy a sandwich for lunch, but expects that it will take about two minutes.
What can be said about the probability of no orders arriving during the time the clerk is off buying lunch?
Exercise 8.1.3 Construct an example for which the upper bound obtained by the Markov inequality gives a
value in excess of 1.0 (and therefore serves no useful purpose, since all probabilities must be less than or equal
to 1).
Exercise 8.2.1 An unfair coin, which produces heads only once out of every ﬁve attempts, is tossed 100 times.
Let X be the random variable that denotes the number of heads obtained. Show that E[X] = 20 and that
Var [X] = 16. Now ﬁnd an upper bound on the probability that X ≥60, ﬁrst using the Markov inequality and
second using the Chebychev inequality.
Exercise 8.2.2 Complete Example 8.3 by ﬁnding the bounds obtained from the Markov and Chebychev
inequalities as well as the exact value of the probability.

188
Bounds and Limit Theorems
Exercise 8.2.3 Consider an experiment in which two fair dice are repeatedly thrown. Let X be the random
variable that denotes the number of throws needed to observe the second occurrence of two 6’s. Apply both the
Markov inequality and the Chebychev inequality to bound the probability that at least 200 tosses are needed.
Exercise 8.2.4 The probability of an event occurring in one trial is 0.5. Use Chebychev’s inequality to show
that the probability of this event occurring between 450 and 550 times in 1000 independent trials exceeds 0.90.
Exercise 8.2.5 The local garden shop keeps an average of 60 rose bushes in stock with variance equal to
64. A customer arrives and wishes to purchase 36 rose bushes. Use Chebychev’s inequality to determine the
likelihood of this request being met.
Exercise 8.2.6 A farmer agrees to sell batches of 12 dozen apples to grocery stores. He estimates that 2% of
his apples are bad and wishes to provide the grocery stores with guarantees whereby, if more than k apples per
batch are bad, he will refund the entire cost of the batch to the grocery store. How large should k be so that the
farmer will not have to reimburse grocery store owners more than 5% of the time?
Exercise 8.2.7 Let X be an exponentially distributed random variable with parameter λ = 1. Compare the
upper bound on the probability Prob{X > 4} obtained from the Chebychev inequality and the exact value of
this probability.
Exercise 8.3.1 Let Xi, i = 1, 2, 3, 4, 5, be independent exponentially distributed random variables each with
the same parameter μ = 1/4 and let X = 5
i=1 Xi. It follows from earlier remarks that X has an Erlang-5
distribution. Use the Chernoff bound to ﬁnd an upper bound on Prob{X > 40}. Compare this to the exact
probability.
Exercise 8.4.1 Show that the weak law of large numbers provides justiﬁcation for the frequency interpretation
of probability.
Exercise 8.4.2 Apply Equation (8.5) to the case of n independent Bernoulli random variables which take the
value 1 with probability p, to show that
Prob




Sn
n −p




 ≥ϵ

≤p(1 −p)
nϵ2
.
Now show that p(1 −p) ≤1/4 for 0 < p < 1 and hence that
Prob




Sn
n −p




 ≥ϵ

≤
1
4nϵ2 .
Observe that the upper bound in this second case is independent of p.
Compute both these bounds for the two separate cases p = 1/2 and p = 1/10. Now let ϵ = 0.1. What
do these two inequalities tell us about the value of n needed in this case (ϵ = 0.1) to obtain an upper bound
of 0.01.
Exercise 8.5.1 A coded message of 5 million bits is transmitted over the internet. Each bit is independent
and the probability that any bit is 1 is p = 0.6. Use the central limit theorem approximation to estimate the
probability that the number of 1’s transmitted lies between 2,999,000 and 3,001,000.
Exercise 8.5.2 Apply the central limit theorem to compute an approximation to the probability sought in
Exercise 8.3.1. How does this approximation compare to the exact answer and the Chernoff bound?
Exercise 8.5.3 A fair coin is tossed 400 times. Use the central limit theorem to compute an approximation to
the probability of getting at most 204 heads.
Exercise 8.5.4 Consider a system in which parts are routed by conveyor belts from one workstation to the
next. Assume the time spent at each workstation is uniformly distributed on the interval [0, 4] minutes and that
the time to place an object on the conveyor belt, transport it to and then remove it from the next station, is a

8.6 Exercises
189
constant time of 15 seconds. An object begins the process by being placed on the ﬁrst conveyor belt which
directs it to the ﬁrst station and terminates immediately after being served at the eighth workstation. Find
(a) the mean and variance of the time an object spends in the system,
(b) the probability that the total time is greater than 25 minutes, and
(c) the probability that the total time is less than 20 minutes.
Exercise 8.5.5 It is known that a sum of independent Poisson random variables is itself Poisson. In particular,
if X1 is Poisson with parameter λ1 and X2 is Poisson with parameter λ2, then X = X1 + X2 is Poisson with
parameter λ1 + λ2. Use this result to apply the central limit theorem approximation with continuity correction
to ﬁnd the probability that fewer than 380 customers arrive during peak demand hours, when it is known that
the random variable that describes the arrival process during this time is Poisson with mean value λ = 400.

This page intentionally left blank 

Part II
MARKOV CHAINS

This page intentionally left blank 

Chapter 9
Discrete- and Continuous-Time Markov Chains
Figure 9.1. Andrei Andreevich Markov (1856–1922). Photo courtesy of the Markov family.
9.1 Stochastic Processes and Markov Chains
It is often possible to represent the behavior of a system, physical or mathematical, by describing
all the different states it may occupy (possibly an inﬁnite number) and by indicating how it moves
among these states. The system being modeled is assumed to occupy one and only one state at any
moment in time and its evolution is represented by transitions from state to state. These transitions
are assumed to occur instantaneously; in other words, the actual business of moving from one state
to the next consumes zero time. If the future evolution of the system depends only on its current
state and not on its past history, then the system may be represented by a Markov process. Even
when the system does not possess this Markov property explicitly, it is often possible to construct
a corresponding implicit representation. Examples of the use of Markov processes may be found
extensively throughout the biological, physical, and social sciences as well as in business and
engineering.
As an example, consider the behavior of a frog who leaps from lily pad to lily pad on a pond or
lake. The lily pads constitute the states of the system. Where the frog jumps to depends only upon
what information it can deduce from its current lily pad—it has no memory and thus recalls nothing
about the states (lily pads) it visited prior to its current position, nor even the length of time it has
been on the present lily pad. The assumption of instantaneous transitions is justiﬁed by the fact that

194
Discrete- and Continuous-Time Markov Chains
the time the frog spends in the air between two lily pads is negligible compared to the time it spends
sitting on a lily pad. The information we would like to obtain concerning a system is the probability
of being in a given state or set of states at a certain time after the system becomes operational. Often
this time is taken to be sufﬁciently long that all inﬂuence of the initial starting state has been erased.
Other measures of interest include the time taken until a certain state is reached for the ﬁrst time.
For example, in the case of the frog and the lily pond, if one of the lily pads were not really a lily
pad but the nose of an alligator waiting patiently for breakfast, we would be interested in knowing
how long the frog survives before being devoured by the alligator. In Markov chain terminology,
such a state is referred to as an absorbing state, and in such cases we seek to determine the mean
time to absorption.
A Markov process is a special type of stochastic process. A stochastic process is deﬁned as a
family of random variables {X(t), t ∈T }. In other words, each X(t) is a random variable and as
such is deﬁned on some probability space. The parameter t usually represents time, so X(t) denotes
the value assumed by the random variable at time t. T is called the index set or parameter space and
is a subset of (−∞, +∞). If the index set is discrete, e.g., T = {0, 1, 2, . . .}, then we have a discrete-
(time) parameter stochastic process; otherwise, if T is continuous, e.g., T = {t : 0 ≤t < +∞},
we call the process a continuous-(time) parameter stochastic process. The values assumed by the
random variables X(t) are called states. The set of all possible states forms the state space of the
process and this may be discrete or continuous. If the state space is discrete, the process is referred
to as a chain and the states are usually identiﬁed with the set of natural numbers {0, 1, 2, . . .} or a
subset of it. An example of a discrete state space is the number of customers at a service facility.
An example of a continuous state space is the length of time a customer has been waiting for
service. Other examples of a continuous state space could include the level of water in a dam or the
temperature inside a nuclear reactor where the water level/temperature can be any real number in
some continuous interval of the real axis. Thus, a stochastic process can have a discrete state space
or a continuous state space, and may evolve at a discrete set of time points or continuously in time.
In certain models, the statistical characteristics in which we are interested, may dependent on the
time t at which the system is started. The evolution of a certain process that begins in mid-morning
may be very different from the evolution of that same process if it begins in the middle of the night.
A process whose evolution depends on the time at which it is initiated is said to be nonstationary. On
the other hand, a stochastic process is said to be stationary when it is invariant under an arbitrary
shift of the time origin. Mathematically, we say that a stochastic process is stationary if its joint
distribution is invariant to shifts in time, i.e., if for any constant α,
Prob{X(t1) ≤x1, X(t2) ≤x2, . . . , X(tn) ≤xn}
= Prob{X(t1 + α) ≤x1, X(t2 + α) ≤x2, . . . , X(tn + α) ≤xn}
for all n and all ti and xi with i = 1, 2, . . . , n. Stationarity does not imply that transitions are
not allowed to depend on the current situation. Indeed, the transition probabilities can depend
on the amount of time that has elapsed from the moment the process is initiated (which is equal
to t time units if the process begins at time zero). When the transitions in a stochastic process
do in fact depend upon the amount of time that has elapsed, the stochastic process is said to
be nonhomogeneous. When these transitions are independent of the elapsed time, it is said to be
homogeneous. In either case (homogeneous or nonhomogeneous) the stochastic process may, or
may not, be stationary. If it is stationary, its evolution may change over time (nonhomogeneous) but
this evolution will be the same irrespective of when the process was initiated. Henceforth, in this
text, our concern is with stationary processes only.
A Markov process is a stochastic process whose conditional probability distribution function
satisﬁes the Markov or memoryless property. Consequently, we may deﬁne discrete-time Markov
chains (DTMCs) and continuous-time Markov chains (CTMCs) as well as discrete-time Markov

9.2 Discrete-Time Markov Chains: Deﬁnitions
195
processes and continuous-time Markov processes. Our interest lies only in (discrete-state) Markov
chains in both discrete and continuous time and we shall leave (continuous-state) Markov processes
to more advanced texts.
9.2 Discrete-Time Markov Chains: Deﬁnitions
For a discrete-time Markov chain, we observe its state at a discrete, but inﬁnite, set of times.
Transitions from one state to another can only take place, or fail to take place, at these, somewhat
abstract, time instants—time instants that are mostly taken to be one time unit apart. Therefore
we may represent, without loss of generality, the discrete index set T of the underlying stochastic
process by the set of natural numbers {0, 1, 2, . . .}. The successive observations deﬁne the random
variables X0, X1, . . . , Xn, . . . at time steps 0, 1, . . . , n, . . . , respectively. Formally, a discrete-time
Markov chain {Xn, n = 0, 1, 2, . . .} is a stochastic process that satisﬁes the following relationship,
called the Markov property:
For all natural numbers n and all states xn,
Prob{Xn+1 = xn+1|Xn = xn, Xn−1 = xn−1, . . . , X0 = x0}
= Prob{Xn+1 = xn+1|Xn = xn}.
(9.1)
Thus, the fact that the system is in state x0 at time step 0, in state x1 at time step 1, and so on,
up to the fact that it is in state xn−1 at time step n −1 is completely irrelevant. The state in which
the system ﬁnds itself at time step n + 1 depends only on where it is at time step n. The fact that
the Markov chain is in state xn at time step n is the sum total of all the information concerning the
history of the chain that is relevant to its future evolution.
To simplify the notation, rather than using xi to represent the states of a Markov chain, henceforth
we shall use single letters, such as i,
j, and k. The conditional probabilities Prob{Xn+1 =
xn+1|Xn = xn}, now written as Prob{Xn+1 = j|Xn = i}, are called the single-step transition
probabilities, or just the transition probabilities, of the Markov chain. They give the conditional
probability of making a transition from state xn = i to state xn+1 = j when the time parameter
increases from n to n + 1. They are denoted by
pi j(n) = Prob{Xn+1 = j|Xn = i}.
(9.2)
The matrix P(n), formed by placing pi j(n) in row i and column j, for all i and j, is called the
transition probability matrix or chain matrix. We have
P(n) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
p00(n)
p01(n)
p02(n)
· · ·
p0 j(n)
· · ·
p10(n)
p11(n)
p12(n)
· · ·
p1 j(n)
· · ·
p20(n)
p21(n)
p22(n)
· · ·
p2 j(n)
· · ·
...
...
...
...
...
...
pi0(n)
pi1(n)
pi2(n)
· · ·
pi j(n)
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Notice that the elements of the matrix P(n) satisfy the following two properties:
0 ≤pi j(n) ≤1,
and, for all i,

all j
pi j(n) = 1.
A matrix that satisﬁes these properties is called a Markov matrix or stochastic matrix.

196
Discrete- and Continuous-Time Markov Chains
A Markov chain is said to be (time-)homogeneous if for all states i and j
Prob{Xn+1 = j|Xn = i} = Prob{Xn+m+1 = j|Xn+m = i}
for n = 0, 1, 2, . . . and m ≥0. To elaborate on this a little further, we have, for a homogeneous
Markov chain,
pi j = Prob{X1 = j|X0 = i} = Prob{X2 = j|X1 = i} = Prob{X3 = j|X2 = i} = · · ·
and we have replaced pi j(n) with pi j, since transitions no longer depend on n.
For a nonhomogeneous Markov chain,
pi j(0) = Prob{X1 = j|X0 = i} ̸= Prob{X2 = j|X1 = i} = pi j(1).
As we have just seen, in a homogeneous discrete-time Markov chain the transition probabilities
pi j(n) = Prob{Xn+1 = j|Xn = i} are independent of n and are consequently written as
pi j = Prob{Xn+1 = j|Xn = i},
for all n = 0, 1, 2, . . . .
It follows then that the matrix P(n) should be replaced with the matrix P.
The evolution of the Markov chain in time is as follows. Assume that it begins at some initial
time 0 and in some initial state i. Given some (inﬁnite) set of time steps, the chain may change state
at each of these steps but only at these time steps. For example, if at time step n the Markov chain is
in state i, then its next move will be to state j with probability pi j(n). In particular, if pii(n) > 0, a
situation referred to as a self-loop, the chain will, with probability pii(n), remain in its current state.
The probability of being in state j at time step n + 1 and in state k at time step n + 2, given that
the Markov chain is in state i at time step n, is
Prob{Xn+2 = k, Xn+1 = j | Xn = i}
= Prob{Xn+2 = k | Xn+1 = j, Xn = i}Prob{Xn+1 = j | Xn = i}
= Prob{Xn+2 = k | Xn+1 = j}Prob{Xn+1 = j | Xn = i}
= p jk(n + 1)pi j(n),
where we have ﬁrst used properties of conditional probability and second used the Markov property
itself. A sequence of states visited by the chain is called a sample path: p jk(n + 1)pi j(n) is the
probability of the sample path i, j, k that begins in state i at time step n. More generally,
Prob{Xn+m = a, Xn+m−1 = b, . . . , Xn+2 = k, Xn+1 = j | Xn = i}
(9.3)
= Prob{Xn+m = a | Xn+m−1 = b}Prob{Xn+m−1 = b | Xn+m−2 = c} · · ·
· · · Prob{Xn+2 = k|Xn+1 = j}Prob{Xn+1 = j | Xn = i}
= pba(n + m −1)pcb(n + m −2) · · · p jk(n + 1)pi j(n).
When the Markov chain is homogeneous, we ﬁnd
Prob{Xn+m = a, Xn+m−1 = b, . . . , Xn+2 = k, Xn+1 = j|Xn = i} = pi j p jk · · · pcb pba
for all possible values of n.
Markov chains are frequently illustrated graphically. Circles or ovals are used to represent states.
Single-step transition probabilities are represented by directed arrows, which are frequently, but
not always, labeled with the values of the transition probabilities. The absence of a directed arrow
indicates that no single-step transition is possible.
Example 9.1 A weather model. Consider a homogeneous, discrete-time Markov chain that
describes the daily weather pattern in Belfast, Northern Ireland (well known for its prolonged

9.2 Discrete-Time Markov Chains: Deﬁnitions
197
periods of rainy days). We simplify the situation by considering only three types of weather pattern:
rainy, cloudy, and sunny. These three weather conditions describe the three states of our Markov
chain: state 1 (R) represents a (mostly) rainy day; state 2 (C), a (mostly) cloudy day; and state 3
(S), a (mostly) sunny day. The weather is observed daily. On any given rainy day, the probability
that it will rain the next day is estimated at 0.8; the probability that the next day will be cloudy is
0.15, while the probability that tomorrow will be sunny is only 0.05. Similarly, probabilities may be
assigned when a particular day is cloudy or sunny as shown in Figure 9.2.This is the probability of
the sample
R
C
S
0.8
0.2
0.2
0.05
0.3
0.1
0.7
0.5
0.15
Figure 9.2. Transition diagram for weather at Belfast.
The transition probability matrix P for this Markov chain is
P =
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠.
(9.4)
Note that the elements in P represent conditional probabilities. For example, the element p32 tells
us that the probability that tomorrow is cloudy, given that today is sunny, is 0.3. Using Equation
(9.4), we may compute quantities such as the probability that tomorrow is cloudy and the day after
is rainy, given that it is sunny today. In this case we have, for all n = 0, 1, 2, . . . ,
Prob{Xn+2 = R, Xn+1 = C | Xn = S} = pSC pC R = 0.3 × 0.7 = 0.21.
This is the probability of the sample path S →C →R.
Example 9.2 A social mobility model. Sociologists broadly categorize the population of a country
into upper-, middle-, and lower-class brackets. One of their concerns is to monitor the movement of
successive generations among these three classes. This may be modeled by a discrete-time Markov
chain {Xn, n ≥0}, where Xn gives the class of the nth generation of a family. To satisfy the
Markov property, it is assumed that the class of any generation depends only on the class of its
parent generation and not on the class of its grandparent generation or other ancestors. A possible
transition probability matrix for this Markov is given by
P =
⎛
⎝
0.45
0.50
0.05
0.15
0.65
0.20
0.00
0.50
0.50
⎞
⎠,
where states U = 1, M = 2, and L = 3 represent the upper, middle, and lower class respectively.
Thus, with probabilities 0.45, 0.5, and 0.05, a generation born into the upper class will itself remain
in the upper class, move to the middle class, or even to the lower class, respectively. On the other
hand, the generation born into the lower class is equally likely to remain in the lower class or move to
the middle class. According to this modeling scenario, a generation born into the lower class cannot
move, in one generation, into the upper class. To ﬁnd the probability that a second generation will

198
Discrete- and Continuous-Time Markov Chains
end up in the upper class we need to compute
Prob{Xn+2 = U, Xn+1 = M|Xn = L} = pLM pMU = 0.5 × 0.15 = 0.075,
i.e., the probability of the sample path L →M →U.
Example 9.3 The Ehrenfest model. This model arises from considering two volumes of gas that
are connected by a small opening. This opening is so small that only one molecule can pass through
it at any time. The model may also be viewed as two buckets containing balls. At each time instant,
a ball is chosen at random from one of the buckets and moved into the other. A state of the system
is given by the number of balls (or molecules) in the ﬁrst bucket (or volume of gas). Let Xn be the
random variable that gives the number of balls in the ﬁrst bucket after the nth selection (or time
step). Then {Xn, n = 1, 2, . . .} is a Markov chain, since the number of balls in the ﬁrst bucket at
time step n+1 depends only on the number that is present in this bucket at time step n. Suppose that
initially there is a total of N balls partitioned between the two buckets. If at any time step n, there
are k < N balls in bucket 1, the probability that there are k + 1 after the next exchange is given by
Prob{Xn+1 = k + 1 | Xn = k} = N −k
N
.
If the number in the ﬁrst bucket is to increase, the ball that is exchanged must be one of the N −k
that are in the second bucket and, since a ball is chosen at random, the probability of choosing one
of these N −k is equal to (N −k)/N. Similarly, if k ≥1,
Prob{Xn+1 = k −1 | Xn = k} = k
N .
These are the only transitions that are possible. For example, if N = 6, the transition probability
matrix has seven rows and columns corresponding to the states in which there are zero through six
balls in the ﬁrst bucket and we have
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
6/6
0
0
0
0
0
1/6
0
5/6
0
0
0
0
0
2/6
0
4/6
0
0
0
0
0
3/6
0
3/6
0
0
0
0
0
4/6
0
2/6
0
0
0
0
0
5/6
0
1/6
0
0
0
0
0
6/6
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Example 9.4 A nonhomogeneous Markov chain. Let us now take an example of a nonhomogeneous
discrete-time Markov chain. Consider a two-state Markov chain {Xn, n = 1, 2, . . . , } in which we
denote the two states a and b, respectively. At time step n, the probability that the Markov chain
remains in its current state is given by paa(n) = pbb(n) = 1/n, while the probability that it changes
state is given by pab(n) = pba(n) = (n −1)/n—see Figure 9.3. The ﬁrst four transition probability
1/n
1/n
(n−1)/n
(n−1)/n
a
b
Figure 9.3. Two-state nonhomogeneous Markov chain.

9.2 Discrete-Time Markov Chains: Deﬁnitions
199
1
b
b
b
b
b
b
b
a
a
a
a
a
a
a
a
a
1/3
4/5
1/5
4/5
1/5
1/5
1/5
4/5
4/5
1/4
3/4
1/4
3/4
1/4
3/4
1/3
2/3
1/2
1/2
1/4
3/4
2/3
Figure 9.4. Sample paths in a nonhomogeneous Markov chain.
matrices are given by
P(1) =

1
0
0
1

, P(2) =

1/2
1/2
1/2
1/2

, P(3) =

1/3
2/3
2/3
1/3

, P(4) =

1/4
3/4
3/4
1/4

,
while the nth is
P(n) =

1/n
(n −1)/n
(n −1)/n
1/n

.
As a consequence, the probability of changing state at each time step increases with time, while
the probability of remaining in the same state at each time step decreases with time. The important
point to note is that, although these probabilities change with time, the process is still Markovian
and that at any point, the future evolution does not depend on what has happened in the past, but
only on the current state in which the system ﬁnds itself. To see this more clearly, let us examine
some sample paths beginning in state a. These are shown graphically in Figure 9.4.
Starting in state a, then, according to P(1), after a single time step, the Markov chain remains
in state a. In the next time step, according to P(2), the chain either stays in state a with probability
1/2 or moves to state b with the same probability. And so the process unfolds. Figure 9.4 highlights
a particular sample path in bold type. This is the path that begins in state a, stays in state a after the
ﬁrst and second time steps, moves to state b on the third time step, and remains in state b on the
fourth time step. The probability that this path is taken is the product of the probabilities of taking
each segment and, using Equation (9.3), is
Prob{X5 = b, X4 = b, X3 = a, X2 = a|X1 = a}
= paa(1)paa(2)pab(3)pbb(4)
= 1 × 1/2 × 2/3 × 1/4 = 1/12.
Other paths lead to state b after four transitions, and have different probabilities according to the
route they follow. What is important is that, no matter which route is chosen, once the Markov
chain arrives in state b after four steps, the future evolution is speciﬁed by P(5), and not any other
P(i), i ≤4. On the ﬁgure, all transition probabilities leading out of any b in the right most column,
are the same. The future evolution o f the system depends only on the fact that the system is in state
b at time step 4.

200
Discrete- and Continuous-Time Markov Chains
k-Dependent Markov Chains
A stochastic process is not a Markov chain if its evolution depends on more than its current state,
for instance, if transitions at step n +1 depend not only on the state occupied at time step n, but also
on the state occupied by the process at time step n −1. Suppose, in the Belfast weather example,
we are told that, if there are two rainy days in a row, then the probabilities that the following day
is rainy, cloudy, or sunny are given as (0.6, 0.3, 0.1), respectively. On the other hand, if a sunny or
cloudy day is followed by a rainy day, the probabilities that tomorrow is rainy, cloudy, or sunny are
as before, i.e., (0.8, 0.15, 0.05), respectively. Thus the probability of transitions from a rainy day
depend not only on the fact that today it is raining, but also on the weather yesterday; hence the
stochastic process with states R, C, and S is not a Markov chain. However, at the cost of increasing
the number of states, it can be made into a Markov chain. In this example, it sufﬁces to add a single
state, one corresponding to the situation in which it has rained for two consecutive days. Let us
denote this new state RR. The previous rainy state, which we now denote just by R describes the
situation when the day prior to a rainy day was either cloudy or sunny. The transition probability
diagram for this new Markov chain is shown in Figure 9.5 where we have implicitly assumed
that the remaining probabilities (cloudy day followed by rainy, cloudy, or sunny day, i.e., the
previous row 2; and sunny day followed by rainy, cloudy, or sunny day, i.e., the previous row 3) are
unchanged.
C
0.2
0.2
S
R
RR
0.6
0.3
0.1
0.8
0.15
0.05
0.5
0.3
0.7
0.1
Figure 9.5. Modiﬁed transition diagram for Belfast weather.
This technique of converting a non-Markovian process to a Markov chain by incorporating
additional states may be generalized. If a stochastic process has s states and is such that transitions
from each state depend on the history of the process during the two prior steps, then a new process
consisting of s2 states may be deﬁned as a Markov chain. If the weather in Belfast on any particular
day depends on the weather during the preceding two days, then we may deﬁne a nine-state
Markov chain in which each state is a pair, RR, RC, RS, C R, CC, CS, SR, SC, SS, each of which
characterizes the weather on the previous two days. Naturally, we may extend this even further. If a
stochastic process that consists of s states is such that transitions depend on the state of the system
during the prior k time steps, then we may construct a Markov chain with sk states. Referring back
to the deﬁnition of the Markov property, Equation (9.1), we state this more formally as follows:
Let {Xn, n ≥0} be a stochastic process such that there exists an integer k for which
Prob{Xn+1 = xn+1|Xn = xn, . . . , Xn−k+1 = xn−k+1, Xn−k = xn−k, . . . , X0 = x0}
= Prob{Xn+1 = xn+1|Xn = xn, . . . , Xn−k+1 = xn−k+1}
for all n ≥k. Thus, the future of the process depends on the previous k states occupied. Such a
process is said to be a k-dependent process. If k = 1, then Xn is a Markov chain. For k > 1, a new

9.2 Discrete-Time Markov Chains: Deﬁnitions
201
stochastic process {Yn, n ≥0}, with
Yn = (Xn, Xn+1, . . . , Xn+k−1),
is a Markov chain. If the set of states of Xn is denoted by S, then the states of the Markov chain Yn
are the elements of the cross product
S × S × · · · × S
*
+,
-
k terms
.
The Sojourn Time
In Examples 9.1, 9.2, and 9.4, the diagonal elements of the transition probability matrices are all
nonzero and strictly less than 1. This means that at any time step the system may remain in its
current state. The number of consecutive time periods that a Markov chain remains in a given state
is called the sojourn time or holding time of that state. At each time step, the probability of leaving
any state i is independent of what occurred at previous time steps and, for a homogeneous Markov
chain, is equal to

i̸= j
pi j = 1 −pii.
This process may be identiﬁed with a sequence of Bernoulli trials with probability of success
(taken to mean an exit from state i) at each trial equal to 1 −pii. The probability that the sojourn
time is equal to k time steps is that of having k −1 consecutive Bernoulli failures followed by a
single success. It follows immediately that the sojourn time at state i, which we denote by Ri, has
probability mass function
Prob{Ri = k} = (1 −pii)pk−1
ii
,
k = 1, 2, . . . ,
0 otherwise,
which is the geometric distribution with parameter 1 −pii. In other words, the distribution of
the sojourn times in any state of a homogeneous discrete-time Markov chain has a geometric
distribution. Recall that the geometric distribution is the only discrete distribution that has the
memoryless property—a sequence of j −1 unsuccessful trials has no effect on the probability of
success on the jth attempt. The mean and variance of the sojourn time are respectively given by
E[Ri] =
1
1 −pii
and
Var [Ri] =
pii
(1 −pii)2 .
(9.5)
We stress that. this result holds only when the Markov chain is homogeneous. When the Markov
chain is nonhomogeneous, and in state i at time step n, the probability mass function of Ri(n), the
random variable that represents the remaining number of time steps in state i, is
Prob{Ri(n) = k} = pii(n) × pii(n + 1) × · · · × pii(n + k −2) × [1 −pii(n + k −1)].
This is not a geometric distribution—it becomes geometric when pii(n) = pii for all n.
The Embedded Markov Chain
In the last example, that of the extended state space for the weather at Belfast, the state R must be
exited after spending exactly one time unit in that state. In the diagram, there is no self-loop on the
state R, unlike the other states. A rainy day is followed by another rainy day (the state RR), or a
cloudy day C, or a sunny day S. These are the only possibilities. This leads to an alternative way
of viewing a homogeneous Markov chain. Until now, we considered the evolution of the system at
each time step, and this included the possibility of the state being able to transition back to itself
(or simply remain where it is) at any time step. We now view the Markov chain only at those time
steps at which an actual change of state takes place, neglecting (or more correctly, postponing) any
consideration of the sojourn time in the different states. For all rows i for which 0 < pii < 1, we
need to alter that row by removing the self-loop—by setting pii = 0—and replacing the elements pi j

202
Discrete- and Continuous-Time Markov Chains
with probabilities conditioned on the fact that the system transitions out of state i—the probabilities
pi j, i ̸= j, must be replaced with pi j/(1 −pii). Notice that the new matrix remains a stochastic
transition probability matrix since, for all i,

j
pi j
1 −pii
=
pii
1 −pii
+

j̸=i
pi j
1 −pii
=

j̸=i
pi j
1 −pii
=
1
1 −pii

j̸=i
pi j =
1
1 −pii
×(1−pii) = 1.
It is the transition probability matrix of a different discrete-time Markov chain. This new Markov
chain is called the embedded Markov chain, since it is a Markov chain embedded in the original
Markov chain—the embedding points of time being precisely those at which an actual transition out
of a state occurs. An embedded Markov chain has no self-loops and to identify it with the chain from
which it originates, we must also specify the sojourn time in each state. Embedded Markov chains
are more commonly associated with continuous-time processes that may or may not be Markovian,
and we shall return to this in later sections, where we shall see that a study of the embedded chain
together with information concerning the sojourn time in the states can often be used to facilitate
the analysis of continuous-time stochastic processes. Finally, notice that the diagonal elements of
Example 9.3 are all zero and that with this example, we only observe the Markov chain when a
molecule (ball) actually moves.
Example 9.5 The original transition probability matrix P for the example of the weather at Belfast
is given by
P =
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠.
The transition diagram for the corresponding embedded Markov chain is given in Figure 9.6.
R
C
S
0.875
0.25
0.125
0.375
0.75
0.625
Figure 9.6. Embedded Markov chain for weather at Belfast.
Its transition probability matrix is given by
⎛
⎝
0.0
0.75
0.25
0.875
0.0
0.125
0.625
0.375
0.0
⎞
⎠,
and the parameters (probability of success) for the geometric distributions representing the sojourn
in the three states are respectively given by 0.2, 0.8, and 0.8.
9.3 The Chapman-Kolmogorov Equations
In the previous section we saw how, in a discrete-time Markov chain and using Equation 9.3, we
could compute the probability of following a sample path from a given starting state. For example,
in the weather example, we saw that the probability of rain tomorrow, followed by clouds the day
after, given that today is sunny, is pSR pRC = 0.075. We would now like to answer questions such

9.3 The Chapman-Kolmogorov Equations
203
as how to ﬁnd the probability that it is cloudy two days from now, given that it is sunny today. To do
so, we must examine the sample paths that includes both sunny and cloudy days as the intermediate
day (tomorrow) as well as the one we have just computed (intermediate day is rainy)—the weather
on the intermediate day can be any of these three possibilities. Thus, the probability that it is cloudy
two days from now, given that today is sunny, is
pSR pRC + pSC pCC + pSS pSC
=

w=R,C,S
pSw pwC
= 0.5 × 0.15 + 0.3 × 0.2 + 0.2 × 0.3 = 0.195,
as shown graphically in Figure 9.7.
C
S
S
S
S
S
R
R
R
C
C
C
0.5
0.3
0.2
0.15
0.2
0.3
R
Figure 9.7. Sample paths of length 2 beginning on a sunny day.
This result may be obtained directly from the single-step transition probability matrix of the
Markov chain. Recall that this is given by
P =
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠.
The result we seek is obtained when we form the inner product of row 3 of P, i.e., r3 = (.5, .3, .2),
which corresponds to transitions from a sunny day, with column 2 of P, i.e., c2 = (.15, .2, .3)T ,
which corresponds to transitions into a cloudy day. This gives r3c2 = 0.5×0.15+0.3×0.2+0.2×
0.3 = 0.195 as before. Now suppose that the weather today is given by w1, where w1 ∈{R, C, S}.

204
Discrete- and Continuous-Time Markov Chains
The probability that, two days from now, the weather is w2 where w2 ∈{R, C, S}, is obtained by
forming the inner product of row w1 with column w2. All nine different possibilities constitute the
elements of the matrix P2. We have
P2 =
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠×
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠=
⎛
⎝
.770
.165
.065
.750
.175
.075
.710
.195
.095
⎞
⎠.
Thus the (2, 3) element, which is equal to 0.075, gives the probability that it is sunny in two
days time, given that it is cloudy today. In a more general context, the i j element of the square of
a transition probability matrix of a homogeneous discrete-time Markov chain is the probability of
being in state j two time steps from now, given that the current state is state i. In a similar fashion,
the elements of P3 give the conditional probabilities three steps from now, and so on. For this
particular example, it turns out that if we continue to take higher and higher powers of the matrix
P, then this sequence converges to a matrix in which all rows are identical. We have
lim
n→∞Pn =
⎛
⎝
.76250
.16875
.06875
.76250
.16875
.06875
.76250
.16875
.06875
⎞
⎠.
(9.6)
In obtaining these results we have essentially used the theorem of total probability. We are
required to sum over all sample paths of length 2 that lead from state i to state k. For a homogeneous
discrete-time Markov chain, we have, for any n = 0, 1, 2, . . . ,
Prob{Xn+2 = k, Xn+1 = j|Xn = i} = pi j p jk,
and from the theorem of total probability, we obtain
Prob{Xn+2 = k|Xn = i} =

all j
pi j p jk.
Observe that the right-hand side deﬁnes the ikth element of the matrix obtained when P is multiplied
with itself, i.e., the element (P2)ik ≡p(2)
ik . Continuing in this fashion, and observing that
Prob{Xn+3 = l, Xn+2 = k, Xn+1 = j|Xn = i} = pi j p jk pkl,
we obtain, once again with the aid of the theorem of total probability,
Prob{Xn+3 = l|Xn = i} =

all j

all k
pi j p jk pkl =

all j
pi j

all k
p jk pkl =

all j
pi j p(2)
jl ,
which is the ilth element of P3, p(2)
jl being the jl element of P2.
It follows that we may generalize the single-step transition probability matrix of a homogeneous
Markov chain to an m-step transition probability matrix whose elements
p(m)
i j
= Prob{Xn+m = j|Xn = i}
can be obtained from the single-step transition probabilities. Observe that pi j = p(1)
i j . We now show
that p(m)
i j
can be computed from the following recursive formula, called the Chapman-Kolmogorov
equation:
p(m)
i j
=

all k
p(l)
ik p(m−l)
kj
for 0 < l < m.

9.3 The Chapman-Kolmogorov Equations
205
For a homogeneous discrete-time Markov chain, we have
p(m)
i j
= Prob{Xm = j|X0 = i}
=

all k
Prob{Xm = j, Xl = k|X0 = i}
for 0 < l < m
=

all k
Prob{Xm = j|Xl = k, X0 = i}Prob{Xl = k|X0 = i}.
Now applying the Markov property, we ﬁnd
p(m)
i j
=

all k
Prob{Xm = j|Xl = k}Prob{Xl = k|X0 = i}
=

all k
p(m−l)
kj
p(l)
ik
for 0 < l < m.
In matrix notation, the Chapman-Kolmogorov equations are written as
P(m) = P(l)P(m−l),
where, by deﬁnition, P(0) = I, the identity matrix. This relation states that it is possible to write
any m-step homogeneous transition probability matrix as the sum of products of l-step and (m −l)-
step transition probability matrices. To go from i to j in m steps, it is necessary to go from i to an
intermediate state k in l steps, and then from k to j in the remaining m −l steps. By summing over
all possible intermediate states k, we consider all possible distinct paths leading from i to j in m
steps. Note in particular that
P(m) = PP(m−1) = P(m−1)P.
Hence, the matrix of m-step transition probabilities is obtained by multiplying the matrix of one-step
transition probabilities by itself (m −1) times. In other words, P(m) = Pm.
For a nonhomogeneous discrete-time Markov chain, the matrices P(n) may depend on the
particular time step n. In this case the product P2 must be replaced by the product P(n)P(n + 1),
P3 with P(n)P(n + 1)P(n + 2) and so on. It follows that
P(m)(n, n + 1, . . . , n + m −1) = P(n)P(n + 1) · · · P(n + m −1)
is a matrix whose ij element is Prob{Xn+m = j|Xn = i}.
Let π(0)
i
be the probability that the Markov chain begins in state i, and let π(0) be the row vector
whose ith element is π(0)
i . Then the jth element of the vector that results from forming the product
π(0)P(0) gives the probability of being in state j after the ﬁrst time step. We write this as
π(1) = π(0)P(0).
For a homogeneous Markov chain, this becomes
π(1) = π(0)P.
The elements of the vector π(1) provide the probability of being in the various states of the Markov
chain (the probability distribution) after the ﬁrst time step. For example, consider the Belfast weather
example and assume that observations begin at time step 0 with the weather being cloudy. Thus,
π(0) = (0, 1, 0) and
π(1) = π(0)P(0) = (0, 1, 0)
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠= (0.7, 0.2, 0.1),

206
Discrete- and Continuous-Time Markov Chains
which just returns the second row of the matrix P. So, starting on day zero with a cloudy day, the
probability that day one is also cloudy is 0.2, as previously computed. To compute the probability
of being in any state (the probability distribution) after two time steps, we need to form
π(2) = π(1)P(1) = π(0)P(0)P(1).
For a homogeneous Markov chain, this becomes
π(2) = π(1)P = π(0)P2,
and for the Belfast example, we ﬁnd
π(2) = π(1)P = π(0)P2 = (0, 1, 0)
⎛
⎝
.770
.165
.065
.750
.175
.075
.710
.195
.095
⎞
⎠= (0.750, 0.175, 0.075).
Thus the probability that it is cloudy two days after observations begin on a cloudy day is given by
0.175. In computing this quantity, we have summed over all sample paths of length 2 that begin and
end in state 2.
In general, after n steps, the probability distribution is given by
π(n) = π(n−1)P(n −1) = π(0)P(0)P(1) · · · P(n −1)
or, for a homogeneous Markov chain,
π(n) = π(n−1)P = π(0)Pn.
In forming the jth component of this vector, we effectively compute the summation over all sample
paths of length n which start, with probability π(0)
i , in state i at time step 0 and end in state j. Letting
n →∞, for the Belfast example, we ﬁnd
lim
n→∞π(n) = π(0) lim
n→∞Pn = (0, 1, 0)
⎛
⎝
.76250
.16875
.06875
.76250
.16875
.06875
.76250
.16875
.06875
⎞
⎠= (0.76250, 0.16875, 0.06875).
We hasten to point out that the limit, limn→∞π(n), does not necessarily exist for all Markov chains,
nor even for all ﬁnite-state Markov chains. We shall return to this topic in a later section. Over
the course of the remainder of this chapter, we shall consider homogeneous Markov chains only,
the sole exception being certain deﬁnitions in the section devoted to continuous-time Markov
chains. From time to time, however, we shall include the word “homogeneous” just as a gentle
reminder.
9.4 Classiﬁcation of States
We now proceed to present a number of important deﬁnitions concerning the individual states of
(homogeneous) discrete-time Markov chains. Later we shall address the classiﬁcation of groups of
states, and indeed, the classiﬁcation of the Markov chain itself. We distinguish between states that
are recurrent, meaning that the Markov chain is guaranteed to return to these states inﬁnitely often,
and states that are transient, meaning that there is a nonzero probability that the Markov chain will
never return to such a state. This does not mean that a transient state cannot be visited many times,
only that the probability of never returning to it is nonzero. A transient state can be visited only a
ﬁnite number of times.
Figure 9.8 illustrates different possibilities. No probabilities are drawn on the paths in this
ﬁgure: instead, it is implicitly assumed that the sum of the probabilities on paths exiting from any

9.4 Classiﬁcation of States
207
1
2
3
4
5
6
7
8
Figure 9.8. Recurrent and transient states.
state is 1. We may make the following observations:
• States 1 and 2 are transient states. Furthermore, the ﬁgure shows that the Markov chain can
be in state 1 or 2 only at the very ﬁrst time step. Transient states that cannot exist beyond the
initial time step are said to be ephemeral states.
• States 3 and 4 are transient states. The Markov chain can enter either of these states and move
from one to the other for a number of time steps, but eventually it will exit from state 3 to
enter state 6.
• State 5 is also a transient state. The Markov chain will enter this state from state 2 at the ﬁrst
time step, if state 2 is the initial state occupied. Once in state 5, the Markov chain may, for
some ﬁnite number of time steps, remain in state 5, but eventually it will move on to either
state 7 or state 8.
• States 6 and 7 are recurrent states. If the Markov chain reaches one of these states, then all
subsequent transitions will take it from one to the other. Notice that, when the Markov chain
is in state 6, it returns to state 6 every second transition. The same is true of state 7. Returns
to the states in this group must take place at time steps that are multiples of 2. Such states
are said to be periodic of period 2. Furthermore, it is apparent that the mean recurrence time
(which is equal to 2) is ﬁnite. Recurrent states whose mean recurrence time is ﬁnite are said
to be positive recurrent. Recurrent states whose mean recurrence time are inﬁnite are said to
be null recurrent. Inﬁnite mean recurrence times can only occur when the Markov chain has
an inﬁnite number of states.
• Finally, state 8 is also a recurrent state. If the Markov chain ever arrives in this state, it will
remain there forever. A state with this property is said to be an absorbing state. State i is an
absorbing state if and only if pii = 1. A nonabsorbing state i must have pii < 1 and may be
either transient or recurrent.
We now proceed to a more formal description of these characteristics. In the previous section
we saw that p(n)
j j is the probability that the Markov chain is once again in state j, n time steps after
leaving it. In these intervening steps, it is possible that the process visited many different states
as well as state j itself. Based on this deﬁnition, let us deﬁne a new conditional probability, the
probability that on leaving state j the ﬁrst return to state j occurs n steps later. We shall denote this
quantity f (n)
j j and we have
f (n)
j j = Prob {ﬁrst return to state j occurs exactly n steps after leaving it}
= Prob {Xn = j, Xn−1 ̸= j, . . . , X1 ̸= j|X0 = j}
for n = 1, 2, . . . .
This should not be confused with p(n)
j j , which is the probability of returning to state j in n steps,
without excluding the possibility that state j was visited at one or more intermediate steps.
We shall now relate p(n)
j j and f (n)
j j and construct a recurrence relation that permits us to compute
f (n)
j j . We have already seen how to compute p(n)
j j by taking higher powers of the single-step transition
probability matrix P. Notice that f (1)
j j = p(1)
j j = p j j; i.e., the probability that the ﬁrst return to state

208
Discrete- and Continuous-Time Markov Chains
j occurs one step after leaving it, is just the single-step transition probability p j j. Since p(0)
j j = 1,
we may write this as
p(1)
j j = f (1)
j j p(0)
j j .
Now consider p(2)
j j , the probability of being in state j two time steps after leaving it. This can happen
because the Markov chain simply does not move from state j at either time step or else because it
leaves state j on the ﬁrst time step and returns on the second. In order to ﬁt our analysis to the
recursive formulation, we interpret these two possibilities as follows.
1. The Markov chain “leaves” state j and “returns” to it for the ﬁrst time after one step, which
has probability f (1)
j j , and then “returns” again at the second step, which has probability p(1)
j j .
The verbs, “leaves” and “returns” are in quotation marks since in this case the Markov chain
simply stays in state j at each of these two time steps, rather than actually changing state.
2. The Markov chain leaves state j and does not return for the ﬁrst time until two steps later,
which has probability f (2)
j j .
Thus
p(2)
j j = f (1)
j j p(1)
j j + f (2)
j j = f (1)
j j p(1)
j j + f (2)
j j p(0)
j j
and hence f (2)
j j may be computed from
f (2)
j j = p(2)
j j −f (1)
j j p(1)
j j .
In a similar manner, we may write an expression for p(3)
j j , the probability that the process is in state
j three steps after leaving it. This occurs if the ﬁrst return to state j is after one step and in the
remaining two steps the process may have gone elsewhere, but has returned to state j by the end of
these two steps; or if the process returns to state j for the ﬁrst time after two steps and in the third
step remains in state j; or ﬁnally, if the ﬁrst return to state j is three steps after leaving it. These are
the only possibilities and combining them gives
p(3)
j j = f (1)
j j p(2)
j j + f (2)
j j p(1)
j j + f (3)
j j p(0)
j j ,
from which f (3)
j j is easily computed as
f (3)
j j = p(3)
j j −f (1)
j j p(2)
j j −f (2)
j j p(1)
j j .
Continuing in this fashion, essentially by applying the theorem of total probability and using
p(0)
j j = 1, it may be shown that
p(n)
j j =
n

l=1
f (l)
j j p(n−l)
j j
,
n ≥1.
(9.7)
Hence f (n)
j j , n ≥1, may be calculated recursively as
f (n)
j j = p(n)
j j −
n−1

l=1
f (l)
j j p(n−l)
j j
,
n ≥1.
The probability of ever returning to state j is denoted by f j j and is given by
f j j =
∞

n=1
f (n)
j j .
If f j j = 1, then state j is said to be recurrent. In other words, state j is recurrent if and only if,
beginning in state j, the probability of returning to j is 1, i.e., the Markov chain is guaranteed

9.4 Classiﬁcation of States
209
to return to this state in the future. In this case, we have p(n)
j j
> 0 for some n > 0. In fact,
since the Markov chain is guaranteed to return to this state, it must return to it inﬁnitely often.
Thus the expected number of visits that the Markov chain makes to a recurrent state j given that it
starts in state j is inﬁnite. We now show that the expected number of visits that the Markov chain
makes to state j given that it starts in state j is equal to ∞
n=0 p(n)
j j and hence it must follow that
∞
n=0 p(n)
j j = ∞when state j is a recurrent state. Let In = 1 if the Markov chain is in state j at time
step n and In = 0 otherwise. Then ∞
n=0 In is the total number of time steps that state j is occupied.
Conditioning on the fact that the Markov chain starts in state j, the expected number of time steps
it is in state j is
E
( ∞

n=0
In|X0 = j
)
=
∞

n=0
E

In|X0 = j
 
=
∞

n=0
Prob{Xn = j|X0 = j} =
∞

n=0
p(n)
j j .
(9.8)
Thus, when state j is recurrent,
∞

n=0
p(n)
j j = ∞.
If f j j < 1, then state j is said to be transient. There is a nonzero probability that the Markov
chain will never return to this state. The expected number of times the Markov chain returns to state
j is therefore ﬁnite. On each occasion that the Markov chain is in state j, the probability that it will
never return is 1 −f j j. The returns to state j may be identiﬁed with a sequence of Bernoulli trials
which counts the number of trials up to and including the ﬁrst “success,” where “success” is taken
to mean that the Markov chain will not return to state j. The probability that the Markov chain,
upon leaving state j, will return exactly n −1 times (n ≥1) and then never return again, is equal
to (1 −f j j) f n−1
j j
which is none other than the geometric probability mass function. Thus the mean
number of times the Markov chain will return to state j is equal to 1/(1 −f j j) which is ﬁnite. It
follows that, when state j is transient,
∞

n=0
p(n)
j j < ∞.
When state j is recurrent, i.e., when f j j = 1, we deﬁne the mean recurrence time M j j of state
j as
M j j =
∞

n=1
nf (n)
j j .
This is the average number of steps taken to return to state j for the ﬁrst time after leaving it. A
recurrent state j for which M j j is ﬁnite is called a positive recurrent state or a recurrent nonnull
state. If M j j = ∞, we say that state j is a null recurrent state. Theorem 9.4.1 follows immediately
from these deﬁnitions.
Theorem 9.4.1 In a ﬁnite Markov chain
• No state is null recurrent.
• At least one state must be positive recurrent, i.e., not all states can be transient.
If all its states were transient, a Markov chain would spend a ﬁnite amount of time in each of
them, after which it would have nowhere else to go. But this is impossible, and so there must be at
least one positive-recurrent state.
Example 9.6 Consider the discrete-time Markov chain whose transition diagram is shown in
Figure 9.9.

210
Discrete- and Continuous-Time Markov Chains
0.5
0.5
0.5
0.5
1.0
3
1
2
Figure 9.9. Transition diagram for Example 9.6.
Its transition probability matrix is given by
P =
⎛
⎝
0
1/2
1/2
1/2
0
1/2
0
0
1
⎞
⎠.
Observe that successive powers of P are given by
Pk =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
⎛
⎝
0
(1/2)k
1 −(1/2)k
(1/2)k
0
1 −(1/2)k
0
0
1
⎞
⎠
if k = 1, 3, 5, . . . ,
⎛
⎝
(1/2)k
0
1 −(1/2)k
0
(1/2)k
1 −(1/2)k
0
0
1
⎞
⎠
if k = 2, 4, 6, . . . .
It follows then that
f (1)
11 = p(1)
11 = 0,
f (2)
11 = p(2)
11 −f (1)
11 p(1)
11 = (1/2)2 −0 = (1/2)2,
f (3)
11 = p(3)
11 −f (2)
11 p(1)
11 −f (1)
11 p(2)
11 = 0 −(1/2)2 × 0 −0 × (1/2)2 = 0,
f (4)
11 = p(4)
11 −f (3)
11 p(1)
11 −f (2)
11 p(2)
11 −f (1)
11 p(3)
11 = (1/2)4 −0 −(1/2)2 × (1/2)2 −0 = 0,
and in general,
f (k)
11 = 0 for all k ≥3.
In fact, from the transition probability diagram, it is evident that the ﬁrst return to state 1 must occur
after two steps; the ﬁrst return cannot be at any other time. Thus, f11 = ∞
n=1 f (n)
11
= 1/4 < 1
and hence state 1 is transient. A similar result applies to state 2. Since this is a ﬁnite Markov chain
consisting of three states, two of which are transient, it must follow that the third state is positive
recurrent. We may show this explicitly as follows. We have
f (1)
33 = p(1)
33 = 1,
f (2)
33 = p(2)
33 −f (1)
33 p(1)
33 = 1 −1 = 0,
f (3)
33 = p(3)
33 −f (1)
33 p(2)
33 −f (2)
33 p(1)
33 = 1 −1 −0 = 0,
and so on. We have
f (k)
33 = 0 for all k ≥2.

9.4 Classiﬁcation of States
211
which, once again, must evidently be the case, as seen in Figure 9.9. Thus f33 = ∞
n=1 f (n)
33 = 1
and state 3 is recurrent. Its mean recurrence time is
M33 =
∞

n=1
nf (n)
33 = 1,
and since this is ﬁnite, state 3 is positive recurrent.
So far in this section, we have been concerned with transitions from any state back to that same
state again. Now let us consider transitions between two different states. Corresponding to f (n)
j j , let
us deﬁne f (n)
i j for i ̸= j as the probability that, starting from state i, the ﬁrst passage to state j occurs
in exactly n steps. We then have f (1)
i j
= pi j, and as was the case for Equation 9.7, we may derive
p(n)
i j =
n

l=1
f (l)
i j p(n−l)
j j
,
n ≥1.
This equation may be rearranged to obtain
f (n)
i j
= p(n)
i j −
n−1

l=1
f (l)
i j p(n−l)
j j
,
which is more convenient for ﬁnding f (n)
i j . The probability fi j that state j is ever reached from state
i is given by
fi j =
∞

n=1
f (n)
i j .
If fi j < 1, the process starting from state i may never reach state j. When fi j = 1, the expected
value of the sequence f (n)
i j , n = 1, 2, . . . , of ﬁrst passage probabilities for a ﬁxed pair i and j
(i ̸= j) is called the mean ﬁrst passage time and is denoted by Mi j. We have
Mi j =
∞

n=1
nf (n)
i j
for i ̸= j.
The Mi j uniquely satisfy the equation
Mi j = pi j +

k̸= j
pik(1 + Mkj) = 1 +

k̸= j
pik Mkj,
(9.9)
since the process in state i either goes to state j in one step (with probability pi j), or else goes ﬁrst
to some intermediate state k in one step (with probability pik) and then eventually on to j in an
additional Mkj steps. If i = j, then Mi j is the mean recurrence time of state i and Equation 9.9
continues to hold.
We now write Equation 9.9 in matrix form. We shall use the letter e to denote a (column) vector
whose components are all equal to 1 and whose length is determined by its context. Likewise, we
shall use E to denote a square matrix whose elements are all equal to 1. Notice that E = eeT .
Letting diag{M} be the diagonal matrix whose ith diagonal element is Mii, it follows that
Mi j = 1 +

k̸= j
pik Mkj = 1 +

k
pik Mkj −pi j M j j.
Equation (9.9) may be written in matrix form as
M = E + P(M −diag{M}).
(9.10)

212
Discrete- and Continuous-Time Markov Chains
The diagonal elements of M are the mean recurrence times, whereas the off-diagonal elements are
the mean ﬁrst passage times. The matrix M may be obtained iteratively from the equation
M(k+1) = E + P(M(k) −diag{M(k)})
with M(0) = E.
(9.11)
As Example 9.8 will show, not all of the elements of M(k+1) need converge to a ﬁnite limit. The
mean recurrence times of certain states as well as certain mean ﬁrst passage times may be inﬁnite,
and the elements of M(k+1) that correspond to these situations will diverge as k →∞. Divergence
of some elements in this context is an “acceptable” behavior for this iterative approach.
The matrix F, whose ijth component is fi j, is called the reachability matrix. This matrix is
examined in more detail in Section 9.6, where we explore an alternative way to compute the
quantities fi j, the probability of ever visiting state j on leaving state i. In that section, we shall
also describe how the expected number of visits to any state j after leaving some state i may be
found.
A ﬁnal property of the states of a Markov chain that is of interest to us is that of periodicity.
A state j is said to be periodic with period p, or cyclic of index p, if on leaving state j a return
is possible only in a number of transitions that is a multiple of the integer p > 1. In other words,
the period of a state j is deﬁned as the greatest common divisor of the set of integers n for which
p(n)
j j > 0. A state whose period is p = 1 is said to be aperiodic. Observe, in Example 9.6, that
states 1 and 2 are periodic. On leaving either of these states, a return is possible only in a number of
steps that is an integer multiple of 2. On the other hand, state 3 is aperiodic. A state that is positive
recurrent and aperiodic is said to be ergodic. If all the states of a Markov chain are ergodic, then the
Markov chain itself is said to be ergodic.
To conclude this section, we present some initial results concerning the limiting behavior of Pn
as n →∞. In a later section, we shall delve much more deeply into Pn and its limit as n →∞.
Recalling that the ijth element of the nth power of P, i.e., Pn is written as p(n)
i j , we have the following
result.
Theorem 9.4.2 Let j be a state of a discrete-time Markov chain.
• If state j is a null recurrent or transient state, and i is any state of the chain, then
lim
n→∞p(n)
i j = 0.
• If j is positive recurrent and aperiodic (i.e., ergodic), then
lim
n→∞p(n)
j j > 0,
and for any other state i, positive recurrent, transient, or otherwise,
lim
n→∞p(n)
i j = fi j lim
n→∞p(n)
j j .
Example 9.7 The Markov chain with transition probability P given below has two transient states
1 and 2, and two ergodic states 3 and 4. The matrix P and limn→∞Pn are
P =
⎛
⎜
⎜
⎝
.4
.5
.1
0
.3
.7
0
0
0
0
0
1
0
0
.8
.2
⎞
⎟
⎟
⎠,
lim
n→∞Pn =
⎛
⎜
⎜
⎝
0
0
4/9
5/9
0
0
4/9
5/9
0
0
4/9
5/9
0
0
4/9
5/9
⎞
⎟
⎟
⎠.
Since states 1 and 2 are transient, limn→∞p(n)
i j = 0 for i = 1, 2, 3, 4 and j = 1, 2. Since states 3
and 4 are ergodic, limn→∞p(n)
j j > 0 for j = 3, 4 while since fi j = 1 for i = 1, 2, 3, 4, j = 3, 4 and
i ̸= j:
lim
n→∞p(n)
i j = fi j lim
n→∞p(n)
j j = lim
n→∞p(n)
j j > 0,
i = 1, 2, 3, 4,
j = 3, 4,
i ̸= j.

9.4 Classiﬁcation of States
213
Example 9.8 Consider a homogeneous discrete-time Markov chain whose transition probability
matrix is
P =
⎛
⎝
a
b
c
0
0
1
0
1
0
⎞
⎠
(9.12)
with 0 < a < 1. Note that, in this example,
p(n)
11 = an
for n = 1, 2, . . . ,
p(n)
22 = p(n)
33 = 0
for n = 1, 3, 5, . . . ,
p(n)
22 = p(n)
33 = 1
for n = 0, 2, 4, . . . ,
and that
p(n)
12 = ap(n−1)
12
+ b × 1{n is odd} + c × 1{n is even},
where 1{·} is an indicator function, which has the value 1 when the condition inside the braces is
true and the value 0 otherwise. Then
f (1)
11 = p(1)
11 = a,
f (2)
11 = p(2)
11 −f (1)
11 p(1)
11 = a2 −a × a = 0,
f (3)
11 = p(3)
11 −f (1)
11 p(2)
11 −f (2)
11 p(1)
11 = p(3)
11 −f (1)
11 p(2)
11 = 0.
It immediately follows that f (n)
11
= 0 for all n ≥2, and thus the probability of ever returning to
state 1 is given by f11 = a < 1. State 1 is therefore a transient state. Also,
f (1)
22 = p(1)
22 = 0,
f (2)
22 = p(2)
22 −f (1)
22 p(1)
22 = p(2)
22 = 1,
f (3)
22 = p(3)
22 −f (1)
22 p(2)
22 −f (2)
22 p(1)
22 = p(3)
22 = 0,
and again it immediately follows that f (n)
22 = 0 for all n ≥3. We then have f22 = ∞
n=1 f (n)
22 =
f (2)
22
= 1, which means that state 2 is recurrent. Furthermore, it is positive recurrent, since
M22 = ∞
n=1 nf (n)
22 = 2 < ∞. In a similar fashion, it may be shown that state 3 is also positive
recurrent.
Now consider f (n)
12 :
f (1)
12 = b,
f (2)
12 = p(2)
12 −f (1)
12 p(1)
22 = p(2)
12 = ap(1)
12 + c = ab + c,
f (3)
12 = p(3)
12 −f (1)
12 p(2)
22 −f (2)
12 p(1)
22 = p(3)
12 −f (1)
12
= (a2b + ac + b) −b = a2b + ac.
Continuing in this fashion, we ﬁnd
f (4)
12 = a3b + a2c,
f (5)
12 = a4b + a3c,
etc.,
and it is easy to show that in general we have
f (n)
12 = an−1b + an−2c.

214
Discrete- and Continuous-Time Markov Chains
It follows that the probability that state 2 is ever reached from state 1 is
f12 =
∞

n=1
f (n)
12 =
b
1 −a +
c
1 −a = 1.
Similarly, we may show that f13 = 1. Also, it is evident that
f (1)
23 = f (1)
32 = 1
and
f (n)
23 = f (n)
32 = 0
for n ≥2
so that f23 = f32 = 1. However, note that
f (n)
21 = f (n)
31 = 0
for n ≥1,
and so state 1 can never be reached from state 2 or from state 3.
To examine the matrix M of mean ﬁrst passage times (with diagonal elements equal to the mean
recurrence times), we shall give speciﬁc values to the variables a, b, and c. Let
P =
⎛
⎝
0.7
0.2
0.1
0.0
0.0
1.0
0.0
1.0
0.0
⎞
⎠
and take M(0) =
⎛
⎝
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
⎞
⎠.
Then, using the iterative formula (9.11) we ﬁnd
M(1) =
⎛
⎝
1.3
1.8
1.9
2.0
2.0
1.0
2.0
1.0
2.0
⎞
⎠,
M(2) =
⎛
⎝
1.6
2.36
2.53
3.0
2.0
1.0
3.0
1.0
2.0
⎞
⎠,
etc.
The iterative process tends to the matrix
M(∞) =
⎛
⎝
∞
11/3
12/3
∞
2
1
∞
1
2
⎞
⎠,
and it may be readily veriﬁed that this matrix satisﬁes Equation (9.10). Thus, the mean recurrence
time of state 1 is inﬁnite, as are the mean ﬁrst passage times from states 2 and 3 to state 1. The
mean ﬁrst passage time from state 2 to state 3 or vice versa is given as 1, which must obviously
be true since, on leaving either of these states, the process immediately enters the other. The mean
recurrence time of both state 2 and state 3 is 2. States 2 and 3 are each periodic with period 2, since,
on leaving either one of these states, a return to that same state is only possible in a number of steps
that is a multiple of 2. These states are not ergodic.
Part 1 of Theorem 9.4.2 allows us to assert that the ﬁrst column of limn→∞Pn contains only zero
elements, since state 1 is transient. Since states 2 and 3 are both periodic, we are not in a position to
apply the second part of this theorem.
9.5 Irreducibility
Having thus discussed the classiﬁcation of individual states, we now move to classiﬁcations
concerning groups of states. Let S be the set of all states in a Markov chain, and let S1 and S2
be two subsets of states that partition S. The subset of states S1 is said to be closed if no one-step
transition is possible from any state in S1 to any state in S2. This is illustrated in Figure 9.10, where
the subset consisting of states {4, 5, 6} is closed. The subset containing states 1 through 3 is not
closed. Notice also, that the set that contains all six states is closed. More generally, any nonempty

9.5 Irreducibility
215
1
1
2
3
4
5
6
S
S2
Figure 9.10. S1 is a closed subset of states.
subset S1 of S is said to be closed if no state in S1 leads to any state outside S1 (in any number of
steps), i.e.,
p(n)
i j = 0
for i ∈S1, j ̸∈S1, n ≥1.
If the closed subset S1 consists of a single state, then that state is an absorbing state. Naturally
enough, a set of states that is not closed is said to be open. It is apparent that any ﬁnite set of transient
states must constitute an open set. Any individual state that is not an absorbing state constitutes by
itself, an open set. If the set of all states S is closed and does not contain any proper subset that
is closed, then the Markov chain is said to be irreducible. On the other hand, if S contains proper
subsets that are closed, the chain is said to be reducible. The Markov chain drawn in Figure 9.10
is not an irreducible Markov chain. A closed subset of states is said to be an irreducible subset if
it contains no proper subset that is closed. In Figure 9.10, the subset consisting of states 4, 5, and
6 is the unique irreducible subset of this Markov chain. In this example, no other subset of states
constitutes an irreducible subset. Any proper subset of an irreducible subset constitutes a set of
states that is open.
The matrix of transition probabilities of the Markov chain shown in Figure 9.10 has the following
nonzero structure:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
∗
0
0
0
∗
0
0
∗
0
0
0
∗
0
0
∗
0
0
0
0
0
0
0
∗
0
0
0
∗
0
0
0
0
0
0
∗
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=

D11
U12
L21
D22

.
In this matrix, the symbol ∗represents a nonzero probability corresponding to a transition of the
Markov chain. The matrix has been decomposed according to the partition {1, 2, 3}, {4, 5, 6} into
two diagonal blocks D11 and D22, and two off-diagonal blocks, U12 and L21, all of size 3 × 3.
Observe that the lower off-diagonal block L21 is identically equal to zero. This means that no
transition is possible from any state represented by diagonal block D22 to any state represented
by diagonal block D11. On the other hand, the upper off-diagonal block U12 does contain nonzero
elements signifying that transitions do occur from the states of D11 to the states of D22.
An alternative deﬁnition for a Markov chain to be irreducible can be enunciated in terms of the
reachability of the states. State j is said to be reachable or accessible from state i if there exists a
path from state i to state j. We write this as i →j. Thus in the above example, there exists a path
from state 3 through states 4 and 6 to state 5 (with nonzero probability equal to p34 p46 p65 > 0) so
state 5 is accessible from state 3. On the other hand, there is no path from state 5 to state 3 and hence
state 3 is not reachable from state 5. A discrete-time Markov chain is irreducible if every state is
reachable from every other state, i.e., if there exists an integer n for which p(n)
i j > 0 for every pair of
states i and j. In the example of Figure 9.10, it is not possible for any of the states 4, 5, or 6 to reach

216
Discrete- and Continuous-Time Markov Chains
states 1, 2, or 3. The converse is possible, i.e., it is possible to reach states 4, 5, and 6 from any of
states 1, 2, or 3. This Markov chain becomes irreducible if we insert a path from any of states 4, 5,
or 6 to state 1, 2, or 3.
If state j is reachable from state i (i →j) and state i is reachable from state j ( j →i) then
states i and j are said to be communicating states and we write i ↔j. By its very nature, this
communication property is symmetric, transitive, and reﬂexive and thus constitutes an equivalence
relationship. We have, for any states i, j, and k,
i ↔j =⇒j ↔i,
i ↔j and j ↔k =⇒i ↔k,
i ↔j and j ↔i =⇒i ↔i.
The ﬁrst of these must obviously hold from the deﬁnitions. To show that the second holds, consider
the following: i ↔j implies that i →j and thus there exists an n1 > 0 for which p(n1)
i j
> 0.
Similarly, j ↔k implies that j →k and there exists an n2 > 0 for which p(n2)
jk
> 0. Set n = n1+n2.
Then, from the Chapman-Kolmogorov equation, we have
p(n)
ik =

all l
p(n1)
il
p(n2)
lk
≥p(n1)
i j
p(n2)
jk
> 0,
and so i →k. It may similarly be shown that k →i, and the required result follows. The third
(reﬂexive) relationship follows directly from the second (transitive). A state that communicates
with itself in this fashion is called a return state. A nonreturn state is one that does not communicate
with itself. As its name implies, once a Markov chain leaves a nonreturn state, it will never return.
The set of all states that communicate with state i forms a class and is denoted by C(i). This may
be the empty set since it is possible that a state communicates with no other state, not even itself. For
example, this is the case of an ephemeral state, a transient state that can only be occupied initially
and which departs to some other state at the very ﬁrst time step. On the other hand, any state i for
which pii > 0 is a return state. It follows that the states of a Markov chain may be partitioned into
communicating classes and nonreturn states. Furthermore, the communicating classes may, or may
not, be closed. If state i is recurrent, the communicating class to which it belongs is closed. Only
transient states can belong to nonclosed communicating classes.
Notice that, if state i is recurrent and i →j, then state j must communicate with state i, i.e.,
i ↔j. There is a path from i to j and since i is recurrent, after leaving j we must return at some
point to i, which shows that there must be a path from j to i. Furthermore, in this case, state j must
be also be recurrent. Since we return to state i inﬁnitely often, and j is reachable from i, we can
also return to state j inﬁnitely often. Since i and j communicate, we have
p(n1)
i j
> 0 and
p(n2)
ji
> 0
for some n1 > 0, n2 > 0,
and since state i is recurrent, there exists an integer n > 0 such that
p(n2+n+n1)
j j
≥p(n2)
ji
p(n)
ii
p(n1)
i j
> 0.
Since i is recurrent, ∞
n=1 p(n)
ii
= ∞, and it follows that state j is recurrent
∞
m=1 p(m)
j j = ∞

,
since
∞

n=1
p(n2)
ji
p(n)
ii
p(n1)
i j
= p(n2)
ji
p(n1)
i j
∞

n=1
p(n)
ii = ∞.
Thus recurrent states can only reach other recurrent states: no transient state can be reached from a
recurrent state and the set of recurrent states must be closed. If state i is a recurrent state, then C(i)
is an irreducible closed set and contains only recurrent states. Furthermore, all these states must be

9.5 Irreducibility
217
positive recurrent or they all must be null recurrent. A Markov chain in which all the states belong
to the same communicating class is irreducible. The following theorems concerning irreducible
discrete-time Markov chains follow immediately from these deﬁnitions.
Theorem 9.5.1 An irreducible, discrete-time Markov chain is positive recurrent or null recurrent
or transient; i.e.,
• all the states are positive recurrent, or
• all the states are null recurrent, or
• all the states are transient.
Furthermore, all states are periodic with the same period p, or else all states are aperiodic.
Examples of irreducible Markov chains in which all states are null recurrent or all are transient,
are given in Section 9.7.
Theorem 9.5.2 In a ﬁnite, irreducible Markov chain, all states are positive recurrent.
We saw previously that in a ﬁnite Markov chain, no states are null recurrent, and at least one state
must be positive recurrent. Adding the irreducible property means that all states must be positive
recurrent.
Theorem 9.5.3 The states of an aperiodic, ﬁnite, irreducible Markov chain are ergodic.
The conditions given in this last theorem are sufﬁcient conditions only. The theorem must not be
taken as a deﬁnition of ergodicity.
1
2
3
4
5
6
7
8
Figure 9.11. Communicating classes and nonreturn states.
We return to Figure 9.8, reproduced again as Figure 9.11, to illustrate these new deﬁnitions.
In the Markov chain of this ﬁgure, state 1 and state 2 are each nonreturn states, states 3 and
4 constitute a communicating class that is not closed, state 5 is a return state and constitutes a
nonclosed communicating class by itself (if state 5 did not have a self-loop, then it would be a
nonreturn state), states 6 and 7 together form a closed communicating class, and state 8 is a return
state that forms a one-state closed communicating class. State 8 is an absorbing state. Thus, the
states are partitioned as follows:
{1}, {2}, {3, 4}, {5}, {6, 7}, {8}.
The Markov chain is reducible, states 1 through 5 are transient states, and states 6 through 8 are
positive-recurrent states. The set of recurrent states {6, 7, 8} is closed and contains two closed proper
subsets {6, 7} and {8}.
The state space of a Markov chain may be partitioned into two subsets, the ﬁrst containing only
transient states and the second subset containing only recurrent states. This second group may be
further partitioned into irreducible, closed communicating classes. After a possible renumbering of

218
Discrete- and Continuous-Time Markov Chains
the states of a Markov chain, it is always possible to bring the transition probability matrix to the
following “normal” form:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
T11
T12
T13
· · ·
T1N
0
R2
0
· · ·
0
0
0
R3
· · ·
0
...
...
...
...
...
0
0
0
· · ·
RN
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
in which there are N −1 closed communicating classes Rk, k = 2, . . . , N, and a set containing
all transient states T . Once the process enters one of the closed classes, it remains there. The set of
transient states may contain multiple communicating classes, but, at least for the moment, we need
not be concerned about this possibility. If some T1k is not identically equal to zero, then transitions
are possible from at least one of the transient states into the closed set Rk. Evidently, each of the
Rk may be considered as an irreducible Markov chain in its own right. This means that many of the
properties of Rk may be determined independently of other states.
The matrix corresponding to the Markov chain of Figure 9.11 is shown in partitioned form below,
where the nonzero elements are shown with an asterisk.
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
∗
∗
0
0
0
0
0
0
0
0
∗
0
0
0
0
0
0
∗
0
∗
0
0
0
0
∗
0
0
0
0
0
0
0
0
0
∗
0
∗
∗
0
0
0
0
0
0
∗
0
0
0
0
0
0
∗
0
0
0
0
0
0
0
0
0
∗
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
9.6 The Potential, Fundamental, and Reachability Matrices
In Section 9.4, we deﬁned the reachability matrix F as the matrix whose i j element fi j is the
probability of ever reaching state j from state i. Our interest in this matrix F lies chieﬂy in that it
allows us to compute the probability of ending up in a particular recurrent state when the Markov
chain begins in a given transient state. Now, let ri j be the expected number of times that state j is
visited, given that the Markov chain starts in state i, and let R be the matrix whose ijth element is
ri j. The matrix R is called the potential matrix. By means of a conditioning argument similar to
that used in Equation (9.8), we obtain
R =
∞

n=0
Pn.
(9.13)
When the states are arranged in such a way that transient states precede recurrent states, the upper
left-hand corner of the potential matrix (the part that concerns transitions that leave and return to
transient states) is called the fundamental matrix. The fundamental matrix is denoted1 by S and
its ijth element gives the expected number of times the Markov chain is in transient state j, given
1 The reader may have noticed a somewhat unfortunate, but generally accepted, choice of letters to designate these different
matrices: the reachability matrix (which might have been called R) is denoted F, the fundamental matrix (which might have
been F) is denoted S, and the potential matrix is called R.

9.6 The Potential, Fundamental, and Reachability Matrices
219
that it started in transient state i. These matrices allow us to compute some properties of particular
importance including
• The mean and variance of the random variable that describes the number of visits to a
particular transient state starting from a (possibly) different transient state, before being
absorbed into a recurrent state.
• Beginning in a given transient state, the mean and variance of the random variable that
describes the total number of steps the Markov chain makes before being absorbed into a
recurrent state. This mean is often designated MTTA, the mean time to absorption.
In general, it is easier to ﬁrst compute the matrix R and from it the matrix F, and this is what we
now proceed to do. The next section, Section 9.6.1, is concerned with the potential and fundamental
matrices and from them, the computation of the mean number of steps taken until absorption;
Section 9.6.2 is concerned with the reachability matrix and the probabilities of absorption into
different recurrent classes.
9.6.1 Potential and Fundamental Matrices and Mean Time to Absorption
Some elements of the potential matrix R may be inﬁnite, other may be zero, and yet others positive
ﬁnite real numbers. We consider a state i and the elements on row i of R. A number of different
possibilities occur according to the classiﬁcation of the initial state and the ﬁnal state. Recall that ri j
is the expected number of times that state j is visited, given that the Markov chain starts in state i.
State i is recurrent
When state i is recurrent, then we know that the Markov chain returns to state i an inﬁnite number
of times and so the element in column position i of row i (the diagonal element) must be inﬁnite.
We write this as rii = ∞. Further, the elements in column positions j corresponding to states that
communicate with state i, must also be inﬁnite (ri j = ∞). This includes all states j that are in the
same closed communicating class, C(i), as i. All other elements in row i must be zero, since it is not
possible to go from a recurrent state to a transient state, nor to a recurrent state that is in a different
irreducible class. Thus, if i is a recurrent state
ri j =
∞,
j ∈C(i),
0
otherwise.
State i is transient and state j is recurrent
If a transient state i can reach any state in recurrent class C( j), we must have rik = ∞for all
k ∈C( j). In this case, after leaving i, the Markov chain can enter C( j) and stay in it forever. If
transient state i cannot reach any state of recurrent class C( j), then rik = 0 for all k ∈C( j). Thus,
given that i is transient and j is recurrent:
If there is an n for which p(n)
i j > 0
rik = ∞for all k ∈C( j);
otherwise
rik = 0 for all k ∈C( j).
Both states i and j are transient
When both i and j are transient states, we use Equation (9.13). Let us assume that the states are
numbered in such a way that all transient states come before all recurrent states. The transition
probability matrix may then be written as
P =
T
U
0
V

,

220
Discrete- and Continuous-Time Markov Chains
in which the submatrix T represents transitions among transient states only, U represents transitions
from transient states to recurrent states, and V represents transitions among recurrent states. Then
Pn =

T n
¯U(n)
0
V n

for some matrix ¯U(n). Thus
R =
∞

n=0
Pn =
⎛
⎝
∞
n=0 T n
∞
n=0 ¯U(n)
0
∞
n=0 V n
⎞
⎠≡
⎛
⎝
S
∞
n=0 ¯U(n)
0
∞
n=0 V n
⎞
⎠.
The quantities we seek, the expected number of visits to transient state j given that the Markov
chain starts in transient state i, are the elements of S = ∞
n=0 T n. These are the only elements of
the potential matrix R that can be different from zero or inﬁnity. As we mentioned previously, the
matrix S is called the fundamental matrix of the Markov chain.
The i j element of the fundamental matrix, si j, gives the expected number of times the Markov
chain is in transient state j starting from transient state i. The matrix S has a number of interesting
formulations. Since
S = I + T + T 2 + · · ·
we have
S −I = T + T 2 + T 2 + · · · = T S
(9.14)
and hence
S −T S = I
or (I −T )S = I.
It is equally easy to show that S also satisﬁes S(I −T ) = I. Notice that
ST = T S = T + T 2 + · · · ,
which is the original series without the identity matrix. When the number of transient states is ﬁnite,
then, since T n gives the probability of moving from one transient state to another transient state after
n steps, the matrices T n must eventually tend to zero. In the ﬁnal analysis, the Markov chain cannot
avoid leaving the set of transient states and entering a recurrent state. Thus, as n →∞, T n →0
and the series I + T + · · · + T n converges. From
I = (I + T + T 2 + · · · )(I −T )
we have
S =
∞

k=0
T k = I + T + T 2 + · · · = (I −T )−1.
The proof of the nonsingularity of (I −T ) may also be justiﬁed on the basis that T is strictly
substochastic (i.e., it has at least one row whose sum is strictly less than 1). Thus, when the number
of transient states is ﬁnite, the matrix I −T is nonsingular and we may compute the elements of S
as the inverse matrix of I −T :
S = (I −T )−1.
If the number of transient states is not ﬁnite, then it may be shown that S is the minimal nonnegative
solution of
(I −T )X = I, X ≥0.

9.6 The Potential, Fundamental, and Reachability Matrices
221
Example 9.9
Let us compute the potential matrix for the Markov chain whose transition
probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.2
.1
.1 .3
.1
.5
.7
.3
.5
.5
1.0
.9
.1
.1
.9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
In this example, states 1, 2, and 3 are transient, states 4 and 5 constitute an irreducible subset, state
6 is an absorbing state, and states 7 and 8 constitute a different irreducible subset. Rows 4 through 8
of the matrix R are therefore easy to compute. In these ﬁve rows all elements are zero except for the
elements in diagonal blocks, which represent transitions among recurrent states of the same closed
class and which are all equal to ∞. In rows 1 through 3, the elements in column positions 4, 5, 7, and
8 must all be ∞: the Markov chain eventually will move from these states to the irreducible subset
containing states 4 and 5, or to the irreducible subset containing states 7 and 8 and once there will
remain there. The elements in column position 6 must be zero, since there is no path from any of
the transient states to the absorbing state 6. This leaves only the fundamental matrix S = (I −T )−1
to be computed:
T =
⎛
⎝
.4
.2
0
.3
.3
0
0
0
.1
⎞
⎠,
(I −T ) =
⎛
⎝
.6
−.2
0
−.3
.7
0
0
0
.9
⎞
⎠,
(I −T )−1 =
⎛
⎝
1.9444
0.5556
0.0
0.8333
1.6667
0.0
0.0
0.0
1.1111
⎞
⎠.
Therefore, the matrix R, whose ijth element ri j is the expected number of times that state j is visited,
given that the Markov chain begins in state i, is
R =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1.9444
0.5556
0.0
∞
∞
0
∞
∞
0.8333
1.6667
0.0
∞
∞
0
∞
∞
0.0
0.0
1.1111 ∞
∞
0
∞
∞
0
0
0
∞
∞
0
0
0
0
0
0
∞
∞
0
0
0
0
0
0
0
0
∞
0
0
0
0
0
0
0
0
∞
∞
0
0
0
0
0
0
∞
∞
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Let Ni j be a random variable, the total number of visits to transient state j beginning from
transient state i and let N be the matrix whose elements are Ni j. We have just seen that E[Ni j] = si j,
i.e.,
E[N] = S.
For completeness, we give the second moments of the random variables, N, namely,
E[N 2] = S(2 diag{S} −I).
The variance of N may now be computed as
Var [N] = E[N 2] −E[N]2 = S(2 diag{S} −I) −sq{S}

222
Discrete- and Continuous-Time Markov Chains
where sq{S} is the matrix whose i j element is s2
i j (= si j × si j, which is
not the same as
s(2)
i j = (S2)i j).
Since the i j element of the fundamental matrix gives the number of times the Markov chain
is in transient state j starting from transient state i, it follows that the sum of the elements in
row i of S gives the mean number of steps that the Markov chain, beginning in state i, makes
before being absorbed. This is the ith element of the vector Se. It is the mean time to absorption
(MTTA) beginning from state i. The variance of the total time to absorption beginning from state i is
given by
(2S −I)Se −sq{Se}
where sq{Se} is a vector whose ith element is the square of the ith element of Se. We summarize
these results in the following theorem, under the assumption that T is ﬁnite and thus (I −T )−1 is
well deﬁned.
Theorem 9.6.1 Consider a discrete-time Markov chain having a ﬁnite number of transient states
and whose transition probability matrix is written as
P =

T
U
0
V

.
(9.15)
Assume this Markov chain starts in some transient state i. Then
• The mean number of times the Markov chain visits transient state j is given by the ijth element
of
S = (I −T )−1.
• The variance of the number of times the Markov chain visits transient state j is given by the
ijth element of
S(2 diag{S} −I) −sq{S}.
• The mean time to absorption (the mean number of steps among transient states before moving
into a recurrent state) is given by the ith element of
Se = (I −T )−1e.
• The variance of the time to absorption is given by the ith element of
(2S −I)Se −sq{Se}.
If the Markov chain begins in transient state i with probability αi, then the above results must be
modiﬁed accordingly. Let α be the probability vector whose ith component is αi. Then
• αS is a vector whose jth component gives the mean number of visits to state j before
absorption,
• αSe is a real number that gives the expected total number of steps before absorption, and
• αS[2 diag{S} −I] −sq{αS} and α(2S −I)Se −(αSe)2 are the corresponding variances.
In Theorem 9.6.1, the submatrix T usually represents the set of all transient states. However,
it also holds when T represents any open subset of states. Let us use this theorem to investigate
properties of an individual nonabsorbing state, i, by letting T be the submatrix that consists of the
single element pii. If the state of interest is the ﬁrst, then the matrix T in (9.15) reduces to the single
element, p11 which must be strictly less than 1, since this is a nonabsorbing state. In this case, the
matrix S consists of a single element, namely 1/(1 −p11) and from the ﬁrst part of the theorem
is equal to the mean number of steps that the Markov chain remains in this nonabsorbing state.

9.6 The Potential, Fundamental, and Reachability Matrices
223
Additionally the variance may be computed from the second part of Theorem 9.6.1. It is given by
1
(1 −p11)

2
1 −p11
−1

−

1
1 −p11
2
=
2
(1 −p11)2 −
1
(1 −p11) −
1
(1 −p11)2
=
1
(1 −p11)2 −
1
1 −p11
=
p11
(1 −p11)2 .
If the nonabsorbing state is state i, then the mean number of time steps the Markov chain spends
in this singleton subset of transient states before moving to a recurrent state is 1/(1 −pii) and the
variance is given by pii/(1 −pii)2. The reader may recognize that these results were derived earlier
and in a different context, when we identiﬁed the sojourn time in a state of a Markov chain with a
sequence of Bernoulli trials, Equation (9.5).
Example 9.10 Let us return to Example 9.9, where we saw that the fundamental matrix is given by
S = (I −T )−1 =
⎛
⎝
1.9444
0.5556
0.0
0.8333
1.6667
0.0
0.0
0.0
1.1111
⎞
⎠.
The mean time to absorption when the Markov chain begins in state i = 1, 2, 3 are the elements of
Se = (I −T )−1e =
⎛
⎝
1.9444
0.5556
0.0
0.8333
1.6667
0.0
0.0
0.0
1.1111
⎞
⎠
⎛
⎝
1
1
1
⎞
⎠=
⎛
⎝
2.5
2.5
1.1111
⎞
⎠.
The variance of the number of times the Markov chain visits state j beginning from state i is given
by the ijth element of
S(2 diag{S} −I) −sq{S}
=
⎛
⎝
1.9444
0.5556
0.0
0.8333
1.6667
0.0
0.0
0.0
1.1111
⎞
⎠
⎛
⎝
2.8889
0.0
0.0
0.0
2.3333
0.0
0.0
0.0
1.2222
⎞
⎠−
⎛
⎝
3.7809
0.3086
0.0
0.6944
2.7778
0.0
0.0
0.0
1.2346
⎞
⎠
=
⎛
⎝
1.8364
0.9877
0.0
1.7130
1.1111
0.0
0.0
0.0
0.1235
⎞
⎠,
while the variance of the total time to absorption beginning from state i is given by the elements of
[2S −I](Se) −sq{Se}
=
⎛
⎝
2.8889
1.1111
0.0
1.6667
2.3333
0.0
0.0
0.0
1.2222
⎞
⎠
⎛
⎝
2.5000
2.5000
1.1111
⎞
⎠−
⎛
⎝
6.2500
6.2500
1.2346
⎞
⎠=
⎛
⎝
3.7500
3.7500
0.1235
⎞
⎠.
9.6.2 The Reachability Matrix and Absorption Probabilities
We now turn to the computation of the elements of the reachability matrix F. We shall separate
this into different categories depending on the classiﬁcation of the initial and terminal states. Recall
that the element fi j of the reachablility matrix F gives the probability of ever reaching state j upon
leaving state i.
Both states are recurrent and belong to the same closed communicating class
In this case
fi j = 1.

224
Discrete- and Continuous-Time Markov Chains
Both states are recurrent but belong to different closed communicating classes
In this case
fi j = 0.
State i is recurrent and state j is transient
Here
fi j = 0.
Both states are transient
In this case, the probability of ever visiting state j from state i may be nonzero. Recall that si j is the
expected number of visits to transient state j starting from transient state i and fi j is the probability
of ever visiting state j from state i. Therefore, for a ﬁnite number of states:
si j = 1{i= j} + fi js j j,
where 1{i= j} is equal to 1 if i = j and is equal to 0 otherwise. If we let H denote the upper left corner
of F corresponding to transient states only, then, in matrix notation, the above equation becomes
S = I + H

diag{S}
 
,
or alternatively
H = (S −I)[diag{S}]−1.
(9.16)
The inverse must exist since sii ≥1 for all i. Thus we may write the individual elements of S in
terms of those of H as
sii =
1
1 −hii
and
si j = hi js j j
for i ̸= j,
or alternatively, since at this point we require the elements of H,
hii = 1 −1
sii
and hi j = si j
s j j
for i ̸= j.
Two additional results concerning the case when states i and j are both transient are of interest.
The ﬁrst concerns the probability of visiting a particular transient state j a ﬁxed number of times,
assuming the Markov chain starts in transient state i. To visit state j exactly k > 0 times, the Markov
chain must make a transition from i to j at least once (with probability hi j), return from state j to
state j a total of k −1 times (with probability hk−1
j j ) and then never return from state j to state j
again (probability 1 −h j j). Writing this in matrix terms, we obtain
H × diag{H}k−1 × [I −diag{H}].
Using the substitution H = (S −I)[diag{S}]−1, observing that diag{H} = I −[diag{S}]−1, and
using the fact that diagonal matrices commute under matrix multiplication, this becomes
(S −I)[diag{S}]−1 × [diag{S}]−1 × (I −[diag{S}]−1)k−1.
The probability that state j is visited zero times, when the Markov chain begins in state i must be
zero if i = j and equal to 1 −hi j if i ̸= j.
The second result concerns the mean number of different transient states visited before
absorption into a recurrent class, assuming that the Markov chain starts in transient state i. This is
equal to the sum of the probabilities of getting to the different transient states. Since the probability
of ever visiting state i from state i is 1 and hi j is the probability of reaching state j from state i, the
mean number of transient states visited before absorption is 1 + 
j̸=i hi j. In matrix terminology,

9.6 The Potential, Fundamental, and Reachability Matrices
225
this is obtained by summing across row i of (H −diag{H} + I). In terms of the fundamental matrix
S, this becomes
[H −diag{H} + I]e = [H + (I −diag{H})]e
= ((S −I)[diag{S}]−1 + (I −(I −[diag{S}]−1))e
= ((S −I)[diag{S}]−1 + [diag{S}]−1)e
= S[diag{S}]−1e.
State i is transient and state j is recurrent
Observe that if a recurrent state j can be reached from a transient state i, then all the states that are
in C( j), the recurrent class containing j, can be reached from i, and with the same probability, i.e.,
fik = fi j for all k ∈C( j).
It sufﬁces then to determine the probability of entering any state of a recurrent class. These
probabilities are called absorption probabilities. To simplify matters, we combine all states in an
irreducible recurrent set into a single state, an absorbing state, and compute the probability of
entering this state from transient state i. We shall assume that the states are arranged in normal
form, i.e.,
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
T11
T12
T13
· · ·
T1N
0
R2
0
· · ·
0
0
0
R3
· · ·
0
...
...
...
...
...
0
0
0
· · ·
RN
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Each of the matrices Rk is replaced with the value 1 (indicating a single absorbing state), the matrix
T11 is left unaltered, and the matrices T1k are replaced by vectors tk, created by summing across each
row of Tik, i.e., tk = T1ke, This gives
¯P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
T11
t2
t3
· · ·
tN
0
1
0
· · ·
0
0
0
1
· · ·
0
...
...
...
...
...
0
0
0
· · ·
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
A Markov chain whose transition matrix is in this form is referred to as an absorbing chain.
Example 9.11 Consider the discrete-time Markov chain with transition probability matrix P:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.2
.1
.1 .3
.1
.5
.7
.3
.5
.5
1.0
.9
.1
.1
.9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

226
Discrete- and Continuous-Time Markov Chains
The absorbing chain is given by the matrix ¯P:
¯P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.3
.1
.4
.5
1.0
1.0
1.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
To simplify the notation, we write the absorbing chain as
¯P =
 T B
0 I

.
Taking higher powers of ¯P we obtain
¯Pn =
 T n (I + T + · · · + T n−1)B
0
I

=
 T n Bn
0
I

,
where Bn = (I +T +· · ·+T n−1)B. We are now in a position to compute the absorption probabilities.
The probability that, starting from transient state i, the Markov chain enters a state of the jth
irreducible recurrent class by time step n is given by the i j element of Bn. Notice that the states
of the ﬁrst recurrent class are those whose transitions are associated with diagonal block R2, those
of the second by diagonal block R3, and so on. The probability of ever being absorbed from state i
into the jth recurrent class is then given by the ijth element of limn→∞Bn.
The matrix A = limn→∞Bn is called the absorption probability matrix. If the number of transient
states is ﬁnite we may obtain A directly from the fundamental matrix, S. We have
A = lim
n→∞Bn = lim
n→∞(I + T + · · · + T n−1)B =
 ∞

k=0
T k

B = (I −T )−1B = SB.
The ijth element of A gives the probability of ever reaching the jth recurrent class starting from
transient state i. For every state k in this class,
fik = ai j.
If the probabilities of absorption into only a single absorbing state/recurrent class are required,
it sufﬁces to form the product of S and the corresponding column of B, rather than perform
the complete matrix–matrix product. The ith element of the resulting vector gives the absorption
probability when the Markov chain starts in state i.
A ﬁnal property concerning the matrix A is of note. Let
¯A =
 0 A
0 I

.
Then
¯P ¯A =
 T B
0 I
 0 A
0 I

=
 0 A
0 I

= ¯A
since
T A + B = T SB + B = (S −I)B + B = SB = A,
where we have ﬁrst used the fact that A = SB and second, from Equation (9.14), that T S = S −I.
Hence, a column of ¯A may be found as the vector ¯α which satisﬁes
¯P ¯α = ¯α.

9.6 The Potential, Fundamental, and Reachability Matrices
227
In other words ¯α is the right-hand eigenvector corresponding to a unit eigenvalue of the matrix
¯P and may be computed using standard eigenvalue/eigenvector methods with resorting to matrix
inversion.
Example 9.12 Referring back to the running example,
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.2
.1
.1 .3
.1
.5
.7
.3
.5
.5
1.0
.9
.1
.1
.9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
we can immediately ﬁll in the elements on rows 4 through 8 of F. These elements are equal to 1
inside the diagonal blocks and zero elsewhere. To compute the ﬁrst 3 × 3 block, we use Equation
(9.16). We have
H = F3×3 = (S −I)[diag{S}]−1
=
⎛
⎝
0.9444
0.5556
0.0
0.8333
0.6667
0.0
0.0
0.0
0.1111
⎞
⎠
⎛
⎝
1.9444
0.0
0.0
0.0
1.6667
0.0
0.0
0.0
1.1111
⎞
⎠
−1
=
⎛
⎝
0.4857
0.3333
0.0
0.4286
0.4000
0.0
0.0
0.0
0.1000
⎞
⎠.
Only the absorption probabilities still remain to be found. From
A = lim
n→∞Bn = (I −T )−1B = SB,
A =
⎛
⎝
0.6
−0.2
0.0
−0.3
0.7
0.0
0.0
0.0
0.9
⎞
⎠
−1 ⎛
⎝
0.2
0.0
0.2
0.1
0.0
0.3
0.4
0.0
0.5
⎞
⎠=
⎛
⎝
0.4444
0.0
0.5556
0.3333
0.0
0.6667
0.4444
0.0
0.5556
⎞
⎠.
Observe that for each starting state, the sum of the absorption probabilities is 1. The i j element of
A gives the probability of being absorbed into the jth recurrent class, starting from transient state i.
Thus the entire matrix F is written as
F =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.4857
0.3333
0.0
0.4444
0.4444 0.0 0.5556
0.5556
0.4286
0.4000
0.0
0.3333
0.3333 0.0 0.6667
0.6667
0.0
0.0
0.1000 0.4444
0.4444 0.0 0.5556
0.5556
0.0
0.0
0.0
1.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
1.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
1.0
1.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(9.17)
The results we have given so far relate to Markov chains with both transient and recurrent states.
We have been concerned with probabilities of moving from transient states to one or more of the
closed communicating classes. It is interesting to observe that these results may also be used to
extract useful information in the context of Markov chains that have no transient states. For example,
given a ﬁnite irreducible Markov chain, we may wish to compute the probability of reaching some
state i before state j when starting from any third state k (with i ̸= j ̸= k). This can be achieved by
separating out states i and j from the remainder of the states and forming the fundamental matrix

228
Discrete- and Continuous-Time Markov Chains
corresponding to all other states—all states other than i and j become transient states. The set of all
states other than states i and j forms an open subset of states and the results of Theorem 6.1.9 can
be applied. In other words, the problem posed can be solved by turning states i and j into absorbing
states and determining the probability that, starting in some nonabsorbing state k, the Markov chain
enters absorbing state i before it enters absorbing state j.
Assume that there are a total of n states and that these are ordered as follows: the set of (n −2)
states which does not contain i or j, but which does contain state k, appears ﬁrst, followed by state i
and ﬁnally state j. The probability that state i is entered before state j is given by the elements of the
vector formed as the product of S and a vector, denoted vn−1, whose components are the ﬁrst n −2
elements of column n −1 of P, i.e., the elements that deﬁne the conditional probabilities of entering
state i given that the Markov chain is in state k, k = 1, 2, . . . , n −2. Similarly, the probability that
state j is reached before state i is computed from the elements of the vector obtained when S is
multiplied by a vector, denoted vn, whose components are the ﬁrst n −2 elements of the last column
of P.
Example 9.13 Consider the six-state Markov chain whose transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.25
.25
.50
.50
.50
.50
.50
.50
.50
.50
.50
.50
.50
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We have
S =
⎛
⎜
⎜
⎝
.75
−.25
−.50
1.0
−.50
−.50
1.0
−.50
−.50
1.0
⎞
⎟
⎟
⎠
−1
= 2
9
⎛
⎜
⎜
⎝
8.0
3.0
2.0
1.0
6.0
9.0
6.0
3.0
4.0
6.0
10.0
5.0
2.0
3.0
5.0
7.0
⎞
⎟
⎟
⎠.
With
v5 =
#
0.5 0.0 0.0 0.0
$T
and v6 =
#
0.0 0.0 0.0 0.5
$T ,
we obtain
Sv5 =
#
0.8889 0.6667 0.4444 0.2222
$T
and
Sv6 =
#
0.1111 0.3333 0.5556 0.7778
$T .
Thus, for example, the probability that state i = 5 is reached before state j = 6 given state k = 1
as the starting state is 0.8889. The probability that state j is reached before state i is only 0.1111.
Notice that combining v5 and v6 into a single 4×2 matrix gives what we previously referred to as the
matrix B, so what we are actually computing is just SB = A, the matrix of absorption probabilities.
9.7 Random Walk Problems
Picture a drunken man trying to walk along an imagined straight line on his way home from the pub.
Sometimes he steps to the right of his imagined straight line and sometimes he steps to the left of
it. This colorful scenario leads to an important class of Markov chain problems, that of the random
walk. The states of the Markov chain are the integers 0, ±1, ±2, . . . (the drunkard’s straight line)
and the only transitions from any state i are to neighboring states i + 1 (a step to the right) with
probability p and i −1 (a step to the left) with probability q = 1 −p. The state transition diagram
is shown in Figure 9.12 and the process begins in state 0. When p = 1/2, the process is said to be a
symmetric random walk.


230
Discrete- and Continuous-Time Markov Chains
Consider the inﬁnite (denumerable) Markov chain whose transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
· · ·
q
0
p
0
· · ·
0
q
0
p
0
· · ·
0
0
q
0
p
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
where p is a positive probability and q = 1 −p. Observe that every time the Markov chain reaches
state 0, it must leave it again at the next time step. State 0 is said to constitute a reﬂecting barrier.
In other applications it may be more appropriate to set p11 = 1 −p and p12 = p, called a Bernoulli
barrier, or p11 = 1 and p12 = 0, called an absorbing barrier. We pursue our analysis with the
reﬂecting barrier option. As we pointed out previously, since every state can reach every other state,
the Markov chain is irreducible and hence all the states are positive recurrent or all the states are
null recurrent or all the states are transient. Notice also that a return to any state is possible only
in a number of steps that is a multiple of 2. Thus the Markov chain is periodic with period equal
to 2. We now wish to use Theorems 9.7.1 and 9.7.2 to classify the states of this chain. Consider
the system of equations z = zP.
(z0 z1 z2 . . . ) = (z0 z1 z2 . . . )
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
· · ·
q
0
p
0
· · ·
0
q
0
p
0
· · ·
0
0
q
0
p
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Taking these equations one at a time, we ﬁnd
z0 = z1q
=⇒z1 = 1
q z0,
z1 = z0 + qz2
=⇒z2 = 1
q (z1 −z0) = 1
q (1 −q)z1 =
 p
q

z1 = 1
q
 p
q

z0.
All subsequent equations are of the form
z j = pz j−1 + qz j+1
for j ≥2
and have the solution
z j+1 =
 p
q

z j,
which may be proven by induction as follows. Base clause, j = 2:
z2 = pz1 + qz3
=⇒z3 = 1
q (z2 −pz1) = 1
q z2 −z2 =
 p
q

z2.
We now assume this solution to hold for j and prove it true for j + 1. From
z j = pz j−1 + qz j+1
we obtain
z j+1 = 1
q (z j −pz j−1) =
 1
q

z j −
 p
q

z j−1 = (1/q −1)z j =
 p
q

z j

9.7 Random Walk Problems
231
which completes the proof. This solution allows us to write any component in terms of z0. We have
z j =
 p
q

z j−1 =
 p
q
2
z j−2 = · · · =
 p
q
 j−1
z1 = 1
q
 p
q
 j−1
z0,
j ≥1
and summing over all z j, we ﬁnd
∞

j=0
z j = z0
⎡
⎣1 + 1
q
∞

j=1
 p
q
 j−1
⎤
⎦.
Notice that the summation inside the square bracket is ﬁnite if and only if p < q in which case
∞

j=1
 p
q
 j−1
=
∞

j=0
 p
q
 j
=
1
1 −p/q
(iff p < q).
The sum of all z j being equal to 1 implies that
1 = z0
&
1 + 1
q

1
1 −p/q
'
.
Simplifying the term inside the square brackets, we obtain
1 = z0
&
1 +
1
q −p
'
= z0
&q −p + 1
q −p
'
= z0
&
2q
q −p
'
and hence
z0 = q −p
2q
= 1
2

1 −p
q

for p < q.
The remaining z j are obtained as
z j = 1
q
 p
q
 j−1
z0 = 1
2q

1 −p
q
  p
q
 j−1
.
This solution satisﬁes the conditions of Theorem 9.7.1 (it exists and its components sum to 1) and
hence all states are positive recurrent (when p < q). We also see that the second part of this theorem
is true (all z j are strictly positive). Finally, this theorem tells us that there is no other solution.
We now examine the other possibilities, namely, p = q and p > q. Under these conditions,
either all states are null recurrent or else all states are transient (from Theorems 9.5.1 and 9.7.1).
We shall now use Theorem 9.7.2 to determine which case holds. Let the matrix P′ be the matrix
obtained from P when the ﬁrst row and column of P is removed and let us consider the system of
equations y = P′y. We have
⎛
⎜
⎜
⎜
⎝
y1
y2
y3
...
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
0
p
0
0
· · ·
q
0
p
0
· · ·
0
q
0
p
· · ·
...
...
...
...
...
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
y1
y2
y3
...
⎞
⎟
⎟
⎟
⎠.
Again taking these equations one at a time, we have
y1 = py2,
y2 = qy1 + py3,
and in general
y j = qy j−1 + py j+1.

232
Discrete- and Continuous-Time Markov Chains
Writing the left-hand side as py j + qy j, we have
py j + qy j = qy j−1 + py j+1
for j ≥2,
which yields
p(y j+1 −y j) = q(y j −y j−1) for j ≥2.
(9.18)
When j = 1, a similar rearrangement gives
p(y2 −y1) = qy1.
It follows from Equation (9.18) that
y j+1 −y j =
 q
p

(y j −y j−1) = · · · =
 q
p
 j−1
(y2 −y1)
=
 q
p
 j−1  q
p

y1 =
 q
p
 j
y1
for j ≥1.
Thus
y j+1 −y j =
 q
p
 j
y1
for j ≥1
and
y j+1 = (y j+1 −y j) + (y j −y j−1) + · · · + (y2 −y1) + y1
=
 q
p
 j
y1 +
 q
p
 j−1
y1 + · · · +
 q
p
1
y1 + y1
=
(
1 +
 q
p

+
 q
p
2
+ · · · +
 q
p
 j)
y1.
Consider now the case in which p = q. We obtain
y j = jy1
for j ≥1.
In order to satisfy the conditions of Theorem 9.7.2, we need to have 0 ≤y j ≤1 for all j. Therefore
the only possibility in this case (p = q) is that y1 = 0, in which case y j = 0 for all j. Since the
only solution is y = 0, we may conclude from Theorem 9.7.2 that all the states are recurrent states
and since we know that they are not positive recurrent, they must be null recurrent.
Finally, consider the case in which p > q. In this case, q/p is a fraction and the summation
converges. We have
y j =
(
1 +
 q
p

+
 q
p
2
+ · · · +
 q
p
 j−1)
y1 =
j−1

k=0
 q
p
k
y1 = 1 −(q/p) j
1 −(q/p) y1.
It is apparent that if we now set
y1 = 1 −q
p
we obtain a value of y1 that satisﬁes 0 ≤y1 ≤1 and, furthermore, we also obtain
y j = 1 −
 q
p
 j
,
which also satisﬁes 0 ≤y j ≤1 for all j ≥1. From Theorem 9.7.2 we may now conclude that all
the states are transient when p > q.

9.7 Random Walk Problems
233
Let us now consider a different random walk, the gambler’s ruin problem. The gambler begins
with i dollars (in state i) and on each play either wins a dollar with probability p or loses a dollar
with probability q = 1 −p. If Xn is the amount of money he has after playing n times, then
{Xn, n = 0, 1, 2, . . .} is a Markov chain and the transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
0
· · ·
0
q
0
p
0
· · ·
0
0
q
0
p
· · ·
0
...
...
...
...
...
0
· · ·
q
0
p
0
0
· · ·
0
q
0
p
0
· · ·
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We would like to ﬁnd the probability that the gambler will eventually make his fortune by arriving
in state N given that he starts in state i. Let xi be this probability. We immediately have that x0 = 0
and xN = 1, since the gambler cannot start with zero dollars and win N dollars, while if he starts
with N dollars he has already made his fortune. Given that the gambler starts with i dollars, after
the ﬁrst play he has i + 1 dollars with probability p, and i −1 dollars with probability q. It follows
that the probability of ever reaching state N from state i is the same as the probability of reaching
N beginning in i + 1 with probability p plus the probability of reaching state N beginning in state
i −1 with probability q. In other words,
xi = pxi+1 + qxi−1.
This is the equation that will allow us to solve our problem. It holds for all 1 ≤i ≤N −1. Be
aware that it does not result from the multiplication of a vector by the transition probability matrix,
but rather by conditioning on the result of the ﬁrst play. Writing this equation as
xi = pxi + qxi = pxi+1 + qxi−1
allows us to derive the recurrence relation
xi+1 −xi = q
p (xi −xi−1) ,
i = 1, 2, . . . , N −1.
Now, using the fact that x0 = 0, we have
x2 −x1 = q
p (x1 −x0) = q
p x1,
x3 −x2 = q
p (x2 −x1) =
 q
p
2
x1,
...
xi −xi−1 = q
p (xi−1 −xi−2) =
 q
p
i−1
x1,
i = 2, 3, . . . , N.
Adding these equations, we ﬁnd
xi −x1 =
( q
p

+
 q
p
2
+ · · · +
 q
p
i−1)
x1,

234
Discrete- and Continuous-Time Markov Chains
i.e.,
xi =
(
1 +
 q
p

+
 q
p
2
+ · · · +
 q
p
i−1)
x1 =
i−1

k=0
 q
p
k
x1.
We must consider the two possible cases for this summation, p ̸= q and p = q = 1/2. When
p ̸= q, then
xi = 1 −(q/p)i
1 −q/p x1
and in particular
1 −(q/p)N
1 −q/p
x1 = xN = 1.
This allows us to compute x1 as
x1 =
1 −q/p
1 −(q/p)N ,
and from this, all the remaining values of xi, i = 2, 3, . . . , N −1, can be found. Collecting these
results together, we have
xi = 1 −(q/p)i
1 −(q/p) ×
1 −(q/p)
1 −(q/p)N
=
1 −(q/p)i
1 −(q/p)N ,
i = 1, 2, . . . N
and p ̸= q.
Notice the limits as N tends to inﬁnity:
lim
N→∞xi =

1 −(q/p)i ,
p > 1/2,
0,
p < 1/2.
This means that, when the game favors the gambler (p > 1/2), there is a positive probability that
the gambler will make his fortune. However, when the game favors the house, then the gambler is
sure to lose all his money. This same sad result holds when the game favors neither the house nor
the gambler (p = 1/2). We leave it as an exercise for the reader to show that, when p = q = 1/2,
then xi = i/N, for i = 1, 2, . . . , N, and hence xi approaches zero as N →∞.
Example 9.14 Billy and his brother Gerard play marbles. At each game Billy has a 60% chance of
winning (perhaps he cheats) while Gerard has only a 40% chance of winning. If Billy starts with 16
marbles and Gerard with 20, what is the probability that Billy ends up with all the marbles? Suppose
Billy starts with only 4 marbles (and Gerard with 20), what is the probability that Gerard ends up
with all the marbles?
We shall use the following equation generated during the gambler’s ruin problem:
xi = 1 −(q/p)i
1 −(q/p)N , i = 1, 2, . . . , N
and p ̸= q.
Substituting in the values p = 0.6, i = 16, and N = 36, we have
x16 = 1 −(.4/.6)16
1 −(.4/.6)36 = 0.998478.
To answer the second part, the probability that Gerard, in a more advantageous initial setup, wins
all the marbles, we ﬁrst ﬁnd x4, the probability that Billy wins them all. We have
x4 = 1 −(.4/.6)4
1 −(.4/.6)24 = 0.802517.
So the probability that Gerard takes all of Billy’s marbles is only 1 −0.802517 = 0.197483.

9.8 Probability Distributions
235
9.8 Probability Distributions
We now turn our attention to probability distributions deﬁned on the states of a (homogeneous)
discrete-time Markov chain. Of particular interest is the probability that the chain is in a given state
at a particular time step. We shall denote by πi(n) the probability that a Markov chain is in state i at
step n, i.e.,
πi(n) = Prob{Xn = i}.
In vector notation, we let π(n) = (π1(n), π2(n), . . . , πi(n), . . .). Note that the vector π is a row
vector. We adopt the convention that all probability vectors are row vectors. All other vectors are
taken to be column vectors unless speciﬁcally stated otherwise. The state probabilities at any time
step n may be obtained from a knowledge of the initial probability distribution (at time step 0) and
the matrix of transition probabilities. We have, from the theorem of total probability,
πi(n) =

all k
Prob{Xn = i|X0 = k}πk(0).
The probability that the Markov chain is in state i at step n is therefore given by
πi(n) =

all k
p(n)
ki πk(0),
(9.19)
which in matrix notation becomes
π(n) = π(0)P(n) = π(0)Pn,
where π(0) denotes the initial state distribution and P(n) = Pn since we assume the chain to be
homogeneous. The probability distribution π(n) is called a transient distribution, since it gives the
probability of being in the various states of the Markov chain at a particular instant in time, i.e., at
step n. As the Markov chain evolves onto step n + 1, the distribution at time step n is discarded—
hence it is only transient. Transient distributions should not be confused with transient states, which
were discusssed in an earlier section. For a discrete-time Markov chain, transient distributions are
computed directly from the deﬁning equation, Equation (9.19). In the remainder of this section, we
shall be concerned with stationary, limiting, and steady-state distributions, rather than with transient
distributions.
Deﬁnition 9.8.1 (Stationary distribution) Let P be the transition probability matrix of a discrete-
time Markov chain, and let the vector z, whose elements z j denote the probability of being in state
j, be a probability distribution; i.e.,
z j ∈ℜ,
0 ≤z j ≤1,
and

all j
z j = 1.
Then z is said to be a stationary distribution if and only if zP = z.
Thus
z = zP = zP2 = · · · = zPn = · · · .
In other words, if z is chosen as the initial probability distribution, i.e., π j(0) = z j for all j,
then for all n, we have π j(n) = z j. The reader might recognize the similarity of this distribution
and the vector quantity introduced in Theorem 9.7.1 in the context of irreducible Markov chains.
In that theorem, all the components of the vector z must be strictly positive in which case its
existence implies that the states are positive recurrent. In some texts, the term invariant measure
or invariant distribution is used in place of stationary distribution. We prefer to reserve the term
invariant measure for any nonzero vector quantity that satisﬁes zP = z without necessarily forcing

236
Discrete- and Continuous-Time Markov Chains
the elements of z to be probabilities. Thus, an invariant measure is any left-hand eigenvector
corresponding to a unit eigenvalue of P.
Example 9.15 Let
P =
⎛
⎜
⎜
⎝
0.4
0.6
0
0
0.6
0.4
0
0
0
0
0.5
0.5
0
0
0.5
0.5
⎞
⎟
⎟
⎠.
It is easy to verify, by direct substitution, that the following probability distributions all satisfy
zP = z and hence are stationary distributions:
z = (1/2, 1/2, 0, 0),
z = (0, 0, 1/2, 1/2),
z = (α/2, α/2, (1 −α)/2, (1 −α)/2),
0 ≤α ≤1.
The vector (1, 1, −1, −1) is an invariant vector but it is not a stationary distribution.
In many cases of interest, a discrete-time Markov chain has a unique stationary distribution. An
example is the following.
Example 9.16 Let us compute the stationary distribution of the social mobility model discussed
earlier. The transition probability matrix for this Markov is given by
P =
⎛
⎝
0.45
0.50
0.05
0.15
0.65
0.20
0.00
0.50
0.50
⎞
⎠,
where states 1, 2, and 3 represent the upper, middle, and lower class respectively. To ﬁnd the
stationary distribution of this Markov chain, we observe that zP = z gives rise to the homogeneous
system of equations z(P −I) = 0 which must have a singular coefﬁcient matrix (P −I),
for otherwise z = 0. This singular coefﬁcient matrix gives rise to only two (not three) linearly
independent equations. We choose the ﬁrst two, namely,
z1 = 0.45z1 + 0.15z2,
z2 = 0.5z1 + 0.65z2 + 0.5z3.
Temporarily setting z1 = 1, we have, from the ﬁrst of these two equations, z2 = 0.55/0.15 =
3.666667. Now substituting the values of z1 and z2 into the second equation, we ﬁnd z3 = 2[−0.5+
0.35(0.55/0.15)] = 1.566667. Our computed solution at this point is (1, 3.666667, 1.566667).
We must now normalize this solution so that its components sum to 1. By adding z1 + z2 + z3 and
dividing each component by this sum, we obtain z = (0.160428, 0.588235, 0.251337).
The effect of incorporating this normalizing equation has been to convert the system of equations
with singular coefﬁcient matrix, namely, z(P −I) = 0, into a system of equations with nonsingular
coefﬁcient matrix and nonzero right-hand side:
(z1, z2, z3)
⎛
⎝
−0.55
0.50
1.0
0.15
−0.35
1.0
0.00
0.50
1.0
⎞
⎠= (0, 0, 1)
—the third equation has been replaced by the normalizing equation. The resulting system of
equations has a unique, nonzero solution—the unique stationary distribution of the Markov chain.

9.8 Probability Distributions
237
It may be observed that
zP = (0.160428, 0.588235, 0.251337)
⎛
⎝
0.45
0.50
0.05
0.15
0.65
0.20
0.00
0.50
0.50
⎞
⎠
= (0.160428, 0.588235, 0.251337) = z.
What this result tell us is that, in a country where class mobility can be represented by this Markov
chain, approximately 16% of the population is in the upper-class bracket, 59% is in the middle-class
bracket, and 25% is in the lower-class bracket.
A unique stationary distribution exists when the Markov chain is ﬁnite and irreducible. In this
case, when one of the equations in the system of linear equations z(P −I) = 0 is replaced by
the normalizing equation, the resulting system has a nonsingular coefﬁcient matrix and nonzero
right-hand side, meaning that it has a unique solution.
We now turn our attention to the existence or nonexistence of limn→∞π(n) = limn→∞π(0)P(n)
for some initial probability distribution π(0).
Deﬁnition 9.8.2 (Limiting distribution) Let P be the transition probability matrix of a homoge-
neous discrete-time Markov chain and let π(0) be an initial probability distribution. If the limit
lim
n→∞P(n) = lim
n→∞Pn
exists, then the probability distribution
π = lim
n→∞π(n) = lim
n→∞π(0)P(n) = π(0) lim
n→∞P(n) = π(0) lim
n→∞Pn
exists and is called a limiting distribution of the Markov chain.
When the states of the Markov chain are positive recurrent and aperiodic (i.e., ergodic) then
the limiting distribution always exists and indeed is unique. This same result also holds when the
Markov chain is ﬁnite, irreducible and aperiodic, since the ﬁrst two of these properties implies
positive recurrency.
Example 9.17 We have previously seen that the transition probability matrix P and limn→∞Pn for
the Markov chain weather example are given by
P =
⎛
⎝
0.8
0.15
0.05
0.7
0.2
0.1
0.5
0.3
0.2
⎞
⎠,
lim
n→∞Pn =
⎛
⎝
.76250
.16875
.06875
.76250
.16875
.06875
.76250
.16875
.06875
⎞
⎠.
Since the elements in a probability vector lie in the interval [0, 1] and sum to 1, it follows
that multiplication of limn→∞Pn by any probability vector yields the limiting distribution π =
(.76250, .16875, .06875).
Limiting distributions can also exist when the transition probability matrix is reducible, so long
as all the states are positive recurrent.
Example 9.18 Let
P =
⎛
⎜
⎜
⎝
0.4
0.6
0
0
0.6
0.4
0
0
0
0
0.5
0.5
0
0
0.5
0.5
⎞
⎟
⎟
⎠.

238
Discrete- and Continuous-Time Markov Chains
Observe that
lim
n→∞P(n) = lim
n→∞Pn =
⎛
⎜
⎜
⎝
0.5
0.5
0
0
0.5
0.5
0
0
0
0
0.5
0.5
0
0
0.5
0.5
⎞
⎟
⎟
⎠.
It follows that
(1, 0, 0, 0) lim
n→∞P(n) = (.5, .5, 0, 0),
(0, 0, .5, .5) lim
n→∞P(n) = (0, 0, .5, .5),
(α, 1 −α, 0, 0) lim
n→∞P(n) = (.5, .5, 0, 0) for 0 ≤α ≤1,
(.375, .375, .125, .125) lim
n→∞P(n) = (.375, .375, .125, .125)
all satisfy the conditions necessary for a probability distribution to be a limiting distribution.
On the other hand, the aperiodicity property is necessary.
Example 9.19 Given the following transition probability matrix:
P =
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠,
then limn→∞Pn does not exist and P does not have a limiting distribution. In this case, successive
powers of P alternate as follows:
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠−→
⎛
⎝
0
0
1
1
0
0
0
1
0
⎞
⎠−→
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠−→
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠−→. . .
i.e.,
P,
P2,
P3,
P4 = P,
P5 = P2,
P6 = P3,
P7 = P,
. . . .
Hence this Markov chain has no limiting distribution.
Deﬁnition 9.8.3 (Steady-state distribution) A limiting distribution π is a steady-state distribution
if it converges, independently of the initial starting distribution π(0), to a vector whose components
are strictly positive (i.e., πi > 0 for all states i) and sum to 1. If a steady-state distribution exists, it
is unique.
Example 9.20 We have just seen that the Markov chain of Example 9.18 has multiple limiting
distributions, each determined by an initial starting distribution. Consequently, this Markov chain
has no steady-state distribution. The Markov chain of Example 9.19 does not have a limiting
distribution, and hence neither does it possess a steady-state distribution.
Component i of a steady-state distribution is generally thought of as the long-run proportion of
time the Markov chain spends in state i, or, equivalently, the probability that a random observer
sees the Markov chain in state i after the process has evolved over a long period of time. We stress
that the existence of a steady-state distribution implies that both the vector π(n) and the matrix
P(n) converge independently of the initial starting distribution π(0). Steady-state distributions are
also called equilibrium distributions and long-run distributions, to highlight the sense that the effect
of the initial state distribution π(0) has disappeared. When a steady-state distribution exists for a
Markov chain, then that distribution is also the unique stationary distribution of the chain. Observe

9.8 Probability Distributions
239
that a steady-state distribution is the unique vector π that satisﬁes
π = π(0) lim
n→∞P(n) = π(0) lim
n→∞P(n+1) =
>
π(0) lim
n→∞P(n)?
P = π P,
i.e., π = π P, and consequently π is the (unique) stationary probability vector. In the queueing
theory literature, the equations π = π P are called the global balance equations since they equate
the ﬂow into and out of states. To see this, notice that the ith equation, πi = 
all j π j p ji, may be
written as
πi(1 −pii) =

j, j̸=i
π j p ji
or
πi

j, j̸=i
pi j =

j, j̸=i
π j p ji.
The left-hand side represents the total ﬂow from out of state i into states other than i, while the
right-hand side represents the total ﬂow out of all states j ̸= i into state i.
Example 9.21 Steady-state distribution—a two-state Markov chain. Let {Xn, n ≥0} be a two-state
(0 and 1) Markov chain, whose transition probability matrix is given by
P =

1 −p
p
q
1 −q

,
where 0 < p < 1 and 0 < q < 1. Let Prob{X0 = 0} = π0(0) be the probability that the Markov
chain begins in state 0. It follows that Prob{X0 = 1} = 1−π0(0) = π1(0). Let us ﬁnd the probability
distribution at time step n. We shall use the relationship
Prob{Xn+1 = 0} = Prob{Xn+1 = 0 | Xn = 0}Prob{Xn = 0}
+ Prob{Xn+1 = 0 | Xn = 1}Prob{Xn = 1}.
Let n = 0;
Prob{X1 = 0} = Prob{X1 = 0 | X0 = 0}Prob{X0 = 0}
+ Prob{X1 = 0 | X0 = 1}Prob{X0 = 1}
= (1 −p)π0(0) + q[1 −π0(0)]
= (1 −p −q)π0(0) + q.
Let n = 1;
Prob{X2 = 0} = Prob{X2 = 0 | X1 = 0}Prob{X1 = 0}
+ Prob{X2 = 0 | X1 = 1}Prob{X1 = 1}
= (1 −p)[ (1 −p −q)π0(0) + q ] + q[ 1 −(1 −p −q)π0(0) −q ]
= (1 −p −q)[ (1 −p −q)π0(0) + q ] + q
= (1 −p −q)2π0(0) + (1 −p −q)q + q.
This leads us to suspect (the proof, by induction, is left as Exercise 9.8.3) that for arbitrary n we
have
Prob{Xn = 0} = (1 −p −q)nπ0(0) + q
n−1

j=0
(1 −p −q) j,
Prob{Xn = 1} = 1 −Prob{Xn = 0}.

240
Discrete- and Continuous-Time Markov Chains
Since n−1
j=0(1 −p −q) j is the sum of a ﬁnite geometric progression, we have
n−1

j=0
(1 −p −q) j = 1 −(1 −p −q)n
p + q
,
which gives
Prob{Xn = 0} = (1 −p −q)nπ0(0) +
q
p + q −q (1 −p −q)n
p + q
=
q
p + q + (1 −p −q)n
&
π0(0) −
q
p + q
'
.
Also
Prob{Xn = 1} =
p
p + q + (1 −p −q)n
&
π1(0) −
p
p + q
'
.
We now ﬁnd the limiting distribution by examining what happens as n →∞. Since |1−p−q| < 1,
limn→∞(1 −p −q)n = 0 and hence
lim
n→∞Prob{Xn = 0} =
q
p + q
and
lim
n→∞Prob{Xn = 1} =
p
p + q .
Also,
lim
n→∞Pn = lim
n→∞

1 −p
p
q
1 −q
n
=
1
p + q

q
p
q
p

since
1
p + q

q
p
q
p
 
1 −p
p
q
1 −q

=
1
p + q

q
p
q
p

.
Finally, for 0 ≤α ≤1, the product of (α, 1 −α) and limn→∞Pn gives (q/(p + q), p/(p + q)),
i.e.,
(α, 1 −α)
q/(p + q)
p/(p + q)
q/(p + q)
p/(p + q)

= (q/(p + q), p/(p + q)).
Hence (q/(p +q), p/(p +q)) is the unique steady-state distribution of this Markov chain. That this
unique steady-state distribution is also the unique stationary distribution of this Markov chain may
be veriﬁed by checking that

q/(p + q), p/(p + q)
 
1 −p
p
q
1 −q

=

q/(p + q), p/(p + q)
 
.
As we have just seen, in some examples the unique stationary distribution is also the limiting
distribution. In some other cases, a Markov chain may possess a stationary distribution but not
a limiting distribution, and so on. To explore this further we consider various categories of
Markov chains and investigate the existence or nonexistence of stationary, limiting, and steady-state
distributions.
Irreducible Markov Chains that are Null Recurrent or Transient
An example of such a Markov chain is the reﬂective barrier, random walk problem examined in
detail in the previous section where, when p = q, we obtained an irreducible, null-recurrent

9.8 Probability Distributions
241
Markov chain and when p > q, we obtained an irreducible, transient Markov chain. Recall that
p is the probability of moving from any state i > 0 to state i + 1, q is the probability of moving
from any state i > 0 to i −1. In such Markov chains, there is no stationary probability vector.
The only solution to the system of equations z = zP is the vector whose components are all
equal to zero. Furthermore, if a limiting distribution exists, its components must all be equal to
zero.
Irreducible Markov Chains that are Positive Recurrent
An example of an irreducible, positive-recurrent Markov chain is the same random walk problem
but this time with p < q. In an irreducible, positive-recurrent Markov chain, the system of equations
z = zP has a unique and strictly positive solution. This solution is the stationary probability
distribution π, and its elements are given by
π j = 1/M j j,
(9.20)
where M j j is the mean recurrence time of state j (which, for a positive-recurrent state, is ﬁnite).
Equation (9.20) is readily veriﬁed by multiplying both sides of Equation 9.10 by the vector π and
observing that π P = π. We get
π M = π E + π P(M −diag{M}) = eT + π(M −diag{M}) = eT + π M −πdiag{M}
and thus πdiag{M} = eT . Conversely, the states of an irreducible Markov chain which has a unique
stationary probability vector, are positive recurrent. An irreducible, positive-recurrent Markov chain
does not necessarily have a limiting probability distribution. This is the case when the Markov chain
is periodic as the following example shows.
Example 9.22 Consider the four-state irreducible, positive-recurrent Markov chain whose transi-
tion probability matrix is
P =
⎛
⎜
⎜
⎝
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
⎞
⎟
⎟
⎠.
It may readily be veriﬁed that the vector (.25, .25, .25, .25) is the unique stationary distribution of
this Markov chain, but that no matter which starting state is chosen, there is no limiting distribution.
This Markov chain is periodic with period 4 which means that if it is in state 1 at time step n it will
move to state 2 at time step n + 1, to state 3 at time step n + 2, to state 4 at time step n + 3, and
back to state 1 at time step n + 4. It will alternate forever in this fashion and will never settle into a
limiting distribution. As illustrated in the second part of Example, 9.18, the limit limn→∞P(n) does
not exist.
The existence of a unique stationary distribution of a Markov chain does not necessarily mean
that the Markov chain has a limiting distribution.
Irreducible, Aperiodic Markov Chains
An example of an irreducible and aperiodic Markov chain is the semi-inﬁnite random walk problem
with a Bernoulli barrier. From state 0, rather than moving to state 1 with probability 1, the
Markov chain remains in state 0 with probability q or moves to state 1 with probability p. This
introduces a self-loop on state 0 and destroys the periodicity property of the original chain. The
three previous characteristics of the states, transient, null recurrent, and positive recurrent, remain
in effect according to whether p > q, p = q, or p < q, respectively.

242
Discrete- and Continuous-Time Markov Chains
In an irreducible and aperiodic Markov chain, the limiting distribution always exists and is
independent of the initial probability distribution. Moreover, exactly one of the following conditions
must hold:
1. All states are transient or all states are null recurrent, in which case π j = 0 for all j, and
there exists no stationary distribution (even though the limiting distribution exists). The state
space in this case must be inﬁnite.
2. All states are positive recurrent (which, together with the aperiodicity property, makes them
ergodic), in which case π j > 0 for all j, and the probabilities π j constitute a stationary
distribution. The π j are uniquely determined by means of
π j =

all i
πi pi j
and

j
π j = 1.
In matrix terminology, this is written as
π = π P,
πe = 1.
When the Markov chain is irreducible and contains only a ﬁnite number of states, then these
states are all positive recurrent and there exists a unique stationary distribution. If the Markov chain
is also aperiodic, the aperiodicity property allows us to assert that this stationary distribution is also
the unique steady-state distribution. The states of an irreducible, ﬁnite, and aperiodic Markov chain
are ergodic as is the Markov chain itself.
Irreducible, Ergodic Markov Chains
Recall that in an ergodic discrete-time Markov chain all the states are positive recurrent and
aperiodic. This, together with the irreducibility property, implies that in such a Markov chain the
probability distribution π(n), as a function of n, always converges to a limiting distribution π,
which is independent of the initial state distribution. This limiting (steady-state) distribution is also
the unique stationary distribution of the Markov chain. It follows from Equation (9.19) that
π j(n + 1) =

all i
pi jπi(n),
and taking the limit as n →∞of both sides gives
π j =

all i
pi jπi.
Thus, the equilibrium probabilities may be uniquely obtained by solving the matrix equation
π = π P
with π > 0
and
∥π∥1 = 1.
(9.21)
It may be shown that, as n →∞, the rows of the n-step transition matrix P(n) = Pn all become
identical to the vector of stationary probabilities. Letting p(n)
i j denote the ijth element of P(n), we
have
π j = lim
n→∞p(n)
i j
for all i and j,
i.e., the stationary distribution is replicated on each row of Pn in the limit as n →∞. This property
may be observed in the example of the Markov chain that describes the evolution of the weather in
Belfast. The matrix given in Equation (9.6) consists of rows that are all identical and equal to the
steady-state probability vector. We have π = (.76250, .16875, .06875).

9.8 Probability Distributions
243
The following are some performance measurements often deduced from the steady-state
probability vector of irreducible, ergodic Markov chains.
• ν j(τ), the average time spent by the chain in state j in a ﬁxed period of time τ at steady
state, is equal to the product of the steady-state probability of state j and the duration of the
observation period:
ν j(τ) = π jτ.
The steady-state probability π j itself may be interpreted as the proportion of time that the
process spends in state j, averaged over the long run. Returning to the weather example, the
mean number of sunny days per week is only 0.48125, while the average number of rainy
days is 5.33750.
• 1/π j is the average number of steps between successive visits to state j. For example, the
average number of transitions from one sunny day to the next sunny day in the weather
example is 1/.06875 = 14.55. Hence the average number of days between two sunny days is
13.55.
• νi j is the average time spent by the Markov chain in state i at steady state between two
successive visits to state j. It is equal to the ratio of the steady-state probabilities of states
i and j:
νi j = πi/π j.
The quantity νi j is called the visit ratio, since it indicates the average number of visits to state
i between two successive visits to state j. In our example, the mean number of rainy days
between two sunny days is 11.09 while the mean number of cloudy days between two sunny
days is 2.45.2
Irreducible, Periodic Markov Chains
We now investigate the effects that periodicity introduces when we seek limiting distributions and
higher powers of the single-step transition matrix. In an irreducible discrete-time Markov chain,
when the number of single-step transitions required on leaving any state to return to that same state
(by any path) is a multiple of some integer p > 1, the Markov chain is said to be periodic of period
p, or cyclic of index p. One of the fundamental properties of such a Markov chain, is that it is
possible by a permutation of its rows and columns to transform it to the form, called the normal
form,
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
P12
0
. . .
0
0
0
P23
. . .
0
...
...
...
...
...
0
0
0
. . .
Pp−1,p
Pp1
0
0
. . .
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(9.22)
in which the diagonal submatrices Pii are square and the only nonzero submatrices are
P12, P23, . . . , Pp1. This corresponds to a partitioning of the states of the system into p distinct
subsets and an ordering imposed on the subsets. These subsets are referred to as the cyclic classes
of the Markov chain. The imposed ordering is such that once the system is in a state of subset i, it
must exit this subset in the next time step and enter a state of subset (i mod p) + 1.
2 We would like to point out to our readers that the weather in Northern Ireland is not as bad as the numbers in this
example, set up for illustrative purposes only, would have us believe.

244
Discrete- and Continuous-Time Markov Chains
Example 9.23
Consider the Markov chain whose transition diagram is given in Figure 9.13, in
which the states have been ordered according to their cyclic classes.
1
2
3
6
7
8
9
10
4
5
0.3
0.4
0.3
1.0
1.0
1.0
1.0
1.0
0.3
0.7
0.2
0.8
0.2
0.8
0.5
0.5
Figure 9.13. A cyclic Markov chain.
Its transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1.0
0.3
0.3
0.4
1.0
1.0
0.3
0.7
0.2
0.8
0.8
0.2
1.0
1.0
0.5
0.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Evidently, this Markov chain has periodicity p = 4. There are four cyclic classes C1 through C4
given by
C1 = {1, 2}, C2 = {3, 4, 5}, C3 = {6, 7}, and C4 = {8, 9, 10}.
On leaving any state of class Ci, i = 1, 2, 3, 4, the Markov chain can only go to states of class
C(i mod p)+1 in the next time step and therefore it can only return to a starting state after four, or some
multiple of four, steps.
Our interest in periodic Markov chains such as this is in determining its behavior at time step
n in the limit as n →∞. Speciﬁcally, we wish to investigate the behavior of Pn as n →∞, the
existence or nonexistence of a limiting probability distribution and the existence or nonexistence of
a stationary distribution. We begin by examining the case when the Markov chain possesses four
cyclic classes, i.e., the case of Equation (9.22) with p = 4. We have
P =
⎛
⎜
⎜
⎝
0
P12
0
0
0
0
P23
0
0
0
0
P34
P41
0
0
0
⎞
⎟
⎟
⎠,
and taking successive powers, we obtain
P2 =
⎛
⎜
⎜
⎝
0
0
P12P23
0
0
0
0
P23P34
P34P41
0
0
0
P41P12
0
0
⎞
⎟
⎟
⎠,

9.8 Probability Distributions
245
P3 =
⎛
⎜
⎜
⎝
0
0
0
P12P23P34
P23P34P41
0
0
0
0
P34P41P12
0
0
0
P41P12P23
0
⎞
⎟
⎟
⎠,
P4 =
⎛
⎜
⎜
⎝
P12P23P34P41
0
0
0
0
P23P34P41P12
0
0
0
0
P34P41P12P23
0
0
0
0
P41P12P23P34
⎞
⎟
⎟
⎠.
These successive powers show that, beginning in one of the states of C1, a state of C2 may be
reached in one step, at least two steps are needed to reach a state of C3, and a minimum of three
steps to reach a state of C4. After four steps, the Markov chain is back in a state of C1. Thus, after
four steps, the transition matrix is block diagonal, and each block is a stochastic matrix. It follows
then that at any time step 4n, n = 1, 2, . . . , the transition matrix has this block structure and each
block represents the transition probability matrix of an irreducible, recurrent and aperiodic Markov
chain; aperiodic since a single-step in the new chain corresponds to four steps in the original chain.
This means that we may now apply the previously discussed theory on irreducible, recurrent and
aperiodic Markov chains to each of the blocks. In particular, each has a limit as n →∞. It follows
that limn→∞P4n exists.
Example 9.24 Consider the Markov chain of Example 9.23. We ﬁnd that P4 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.6000
0.4000
0.3100
0.6900
0.7200
0.1200
0.1600
0.3700
0.2700
0.3600
0.4750
0.2250
0.3000
0.7680
0.2320
0.4780
0.5220
0.2000
0.0000
0.8000
0.0840
0.4640
0.4520
0.1420
0.2320
0.6260
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and limn→∞P4n =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.4366
0.5634
0.4366
0.5634
0.6056
0.1690
0.2254
0.6056
0.1690
0.2254
0.6056
0.1690
0.2254
0.6732
0.3268
0.6732
0.3268
0.1347
0.2614
0.6039
0.1347
0.2614
0.6039
0.1347
0.2614
0.6039
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Each diagonal block is treated as the transition probability matrix of a ﬁnite, aperiodic, irreducible
Markov chain whose limiting distribution is then equal to its stationary distribution. This stationary
distribution may be computed for each of the four blocks separately and, in the limit as n →∞,

246
Discrete- and Continuous-Time Markov Chains
each block row must become equal to this distribution. Now observe that concatenating the four
stationary distributions together yields a vector z for which z = zP. We have
z = (0.4366, 0.5634, 0.6056, 0.1690, 0.2254, 0.6732, 0.3268, 0.1367, 0.2614, 0.6039),
which when normalized becomes the stationary distribution of the original Markov chain:
(0.1092, 0.1408, 0.1514, 0.0423, 0.0563, 0.1683, 0.0817, 0.0337, 0.0654, 0.1510).
Notice that this assumes (correctly) that the Markov chain spends equal amounts of time in all
periodic classes. Since an irreducible, positive-recurrent Markov chain has a unique and strictly
positive stationary distribution, it follows that this computed solution is the only possible solution.
Reducible Markov Chains
We now turn to Markov chains that are reducible in that they contain multiple transient and
irreducible closed classes. In this case, a Markov chain will possess multiple (rather than a unique)
stationary distributions. Also any linear combination of the different stationary distributions, once
normalized to yield a probability vector, is also a stationary distribution. In such a case, it sufﬁces
to treat each irreducible closed class separately. To illustrate this, we return to a previous example.
Example 9.25 Consider the Markov chain whose transition probability matrix is
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.2
.1
.1 .3
.1
.5
.7
.3
.5
.5
1.0
.9
.1
.1
.9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
This Markov chain has three transient states, namely, states 1, 2, and 3, and three irreducible classes,
consisting of states {4, 5}, {6}, and {7, 8}, respectively. The stationary distributions of the three
irreducible classes are unique and given by
(0.6250, 0.3750) = (0.6250, 0.3750)

0.7
0.3
0.5
0.5

,
(1.0) = (1.0)(1),
and (0.5, 0.5) = (0.5, 0.5)
0.9
0.1
0.1
0.9

.
If any of these three distributions is padded with zeros to produce a vector of length 8, then the
resulting vector is a stationary distribution of the complete Markov chain. For example,
(0, 0, 0, 0.6250, 0.3750, 0, 0, 0)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.2
.1
.1 .3
.1
.5
.7
.3
.5
.5
1.0
.9
.1
.1
.9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, 0, 0.6250, 0.3750, 0, 0, 0) ,

9.8 Probability Distributions
247
and similarly for the other two. Indeed, any linear combination of all three, once normalized, is a
stationary distribution. For example, adding all three together gives (0, 0, 0, 0.6250, 0.3750,
1, 0.5, 0.5) which when normalized is (0, 0, 0, .2083, .1250, .3333, .1667, .1667), and it is easy to
verify that
(0, 0, 0, .2083, .1250, .3333, .1667, .1667)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4
.2
.2
.2
.3
.3
.1
.2
.1
.1 .3
.1
.5
.7
.3
.5
.5
1.0
.9
.1
.1
.9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, 0, .2083, .1250, .3333, .1667, .1667).
Taking successively higher powers of the matrix P, we obtain
lim
n→∞Pn =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.0
0.0
0.0 0.2778
0.1667 0.0 0.2778
0.2778
0.0
0.0
0.0 0.2083
0.1250 0.0 0.3333
0.3333
0.0
0.0
0.0 0.2778
0.1667 0.0 0.2778
0.2778
0.6250
0.3750
0.6250
0.3750
1.0
0.5
0.5
0.5
0.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Although not all rows are equal, all rows are stationary distributions and are equal to some
linear combination of the three individual solutions. Finally, the elements of limn→∞Pn are those
predicted by Theorem 9.4.2. All transition probabilities into transient states are zero and transition
probabilities from ergodic states back to the same ergodic state are strictly positive. As for the ﬁnal
part of this theorem, namely, that the transition probability from any state i into an ergodic state j
is given by
lim
n→∞p(n)
i j = fi j lim
n→∞p(n)
j j ,
we must ﬁrst compute the elements fi j of the reachability matrix F. Fortunately, we have already
computed these in a previous example, Equation (9.17), which allows us to compute results
such as
lim
n→∞p(n)
14 = 0.4444 × 0.6250 = 0.2778,
lim
n→∞p(n)
28 = 0.6667 × 0.5000 = 0.3333,
lim
n→∞p(n)
35 = 0.4444 × 0.3750 = 0.1667,
and so on.
To conclude, the states of a Markov chain may be partitioned into disjoint subsets according to
its irreducible closed classes and transient states. The states of each irreducible closed subset are
recurrent and may be analyzed independently as separate Markov chains.

248
Discrete- and Continuous-Time Markov Chains
9.9 Reversibility
We deﬁned a discrete-time Markov chain {Xn, n = 0, 1, 2, . . .}, as a stochastic process that satisﬁes
the Markov property: for all natural numbers n and all states xn,
Prob{Xn+1 = xn+1|Xn = xn, Xn−1 = xn−1, . . . , X0 = x0} = Prob{Xn+1 = xn+1|Xn = xn}.
The state in which the system ﬁnds itself at time step n + 1 depends only on the state it occupied
at time step n and not on the states occupied at time steps prior to n. Our Markov chain evolves
as time moves forward: n = 0, 1, 2, . . . . Beginning the process at time step 0 is convenient, but
not necessary. We may deﬁne the process so that it begins at −∞and continues on to +∞. To
accommodate this possibility the previous deﬁnition should be modiﬁed to allow −∞< n < +∞
with the understanding that, for all permissible values of n, Xn is a state of the Markov chain.
Suppose now, instead of watching the process move forward in time, we watch it moving backward
in time, from the state it occupies at some time step m to the state it occupies at time step m −1 to
its state at time step m −2, and so on. In an analogy with watching a movie, the original Markov
chain {Xn, n ≥0} represents the usual showing of the movie with one frame per value of n, while
watching the process in reverse mode corresponds to showing the movie backward, frame by frame.
When time is turned backward in this fashion, the process is called the reversed process. In most
cases, the original process, which we shall now refer to as the forward process, is not the same as
the reversed process. However, when both are stochastically identical, the Markov chain is said to
be reversible. Using the accepted (and unfortunate) current terminology, all Markov chains can be
“reversed”, but not all Markov chains are “reversible.” If a movie were reversible (which is rather
difﬁcult to imagine with our understanding of the term), one would not be able to tell if the operator
were running it in the forward direction or in the backward direction. For a Markov chain to be
reversible, it must be stationary and homogeneous, hence we remove the dependence on n and write
the transition probabilities as pi j and not pi j(n).
We consider an irreducible, discrete-time Markov chain that is stationary and homogeneous with
transition probability matrix P and a unique stationary distribution π. Recall that an irreducible
Markov chain has a unique stationary distribution π with π = π P if and only if all states are
positive recurrent. The probability of moving from state i at time step n to state j at time step n −1
is obtained as
Prob{Xn−1 = j | Xn = i} = Prob{Xn−1 = j, Xn = i}
Prob{Xn = i}
= Prob{Xn = i | Xn−1 = j}Prob{Xn−1 = j}
Prob{Xn = i}
= p jiπ j
πi
.
(9.23)
Let ri j = p jiπ j/πi. Then the matrix R, whose i j element is ri j, is just the single-step transition
matrix for the reversed Markov chain. Observe that R is stochastic since ri j ≥0 for all i and j and

all j
ri j = 1
πi

all j
π j p ji = 1
πi
πi = 1 for all i.
In matrix terminology, we have
R = diag{π}−1 PT diag{π},
where diag{π} is a diagonal matrix whose ith diagonal element is πi and PT is the transpose of P.

9.9 Reversibility
249
Example 9.26 Consider a Markov chain whose transition probability matrix is
P =
⎛
⎝
.8
.15
.05
.7
.2
.1
.5
.3
.2
⎞
⎠.
Its unique stationary distribution is given by (.76250, .16875, .06875). In fact, this is the transition
matrix for the Belfast weather example. The transition probability matrix of the reversed chain is
R =
⎛
⎝
.76250
0
0
0
.16875
0
0
0
.06875
⎞
⎠
−1 ⎛
⎝
.8
.7
.5
.15
.2
.3
.05
.1
.2
⎞
⎠
⎛
⎝
.76250
0
0
0
.16875
0
0
0
.06875
⎞
⎠
=
⎛
⎝
.80000
.15492
.04508
.67778
.20000
.12222
.55455
.24545
.20000
⎞
⎠.
Observe that R is a stochastic matrix: all elements lie between zero and one and, the row sums are
all equal to 1.
A Markov chain is said to be (time) reversible if and only if ri j = pi j for all i and j; in other
words, if and only if R = P. Thus the Belfast weather example is not reversible. From Equation
(9.23), we see that this condition (R = P) is equivalent to
πi pi j = π j p ji
for all i, j.
(9.24)
Equations (9.24) are called the detailed balance equations. Detailed balance is a necessary and
sufﬁcient condition for reversibility. Importantly, detailed balance also implies global balance. By
summing Equation (9.24) over all i, we obtain

all i
πi pi j =

all i
π j p ji
for all j
and since 
all i π j p ji = π j

all i p ji = π j
π j =

all i
πi pi j
for all j,
which in matrix form gives the global balance equations
π = π P.
Global balance, on the other hand, does not imply detailed balance. The detailed balance equations
imply that π exists and is the stationary distribution vector of both the forward and backward
Markov chains.
Example 9.27 Consider a random walk on the integers 0, 1, 2, . . . with p01 = 1, pi,i+1 = p and
pi,i−1 = 1 −p = q for i > 0. The transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
· · ·
q
0
p
0
· · ·
0
q
0
p
0
· · ·
0
0
q
0
p
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
We saw previously that all the states in this Markov chain are positive recurrent if and only if p < q
and we assume that this condition holds in this example. Since the Markov chain is irreducible, it

250
Discrete- and Continuous-Time Markov Chains
now follows that it has a unique stationary probability vector π and we may write π = π P. We
wish to show that the detailed balance equations are satisﬁed, i.e., that
πi pi j = π j p ji
for all i, j.
First observe that this holds when |i −j| ≥2, since in this case pi j = p ji = 0. It also holds trivially
when i = j. To show that the detailed balanced equations hold in the remaining cases, we expand
the equations π = π P.
π0 = π1q
⇒π0 p01 = π1 p10,
π1 = π0 + qπ2
⇒π1(1 −q) = π2q
⇒π1 p12 = π2 p21,
π2 = (1 −q)π1 + qπ3
⇒π2(1 −q) = π3q
⇒π2 p23 = π3 p32,
and, as can be shown by induction,
πi = (1 −q)πi−1 + qπi+1
⇒πi(1 −q) = πi+1q ⇒πi pi,i+1 = πi+1 pi+1,i
for all remaining values of i. Hence this Markov chain is reversible.
This example can be generalized in a straightforward manner to cover the case when the transition
probability matrix has the form
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
· · ·
q1
0
p1
0
· · ·
0
q2
0
p2
0
· · ·
0
0
q3
0
p3
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
where pi + qi = 1 and where the values of pi are qi are such that a unique stationary distribution
exists. Later we shall see that all birth–death processes, an important class of queueing models that
we discuss in detail in Chapter 12, fall into the continuous-time version of this category, and so
we may conclude that all birth–death processes are reversible. More generally, a positive-recurrent,
tree-structured Markov chain is reversible. A discrete-time Markov chain is said to be tree structured
if between any two distinct states i and j there is one, and only one, path consisting of distinct states
k1, k2, . . . , km for which
pik1 pk1k2 · · · pkm−1km pkm j > 0.
Figure 9.14 illustrates a tree-structured Markov chain.
Figure 9.14. Transition diagram of a tree-structured Markov chain.

9.9 Reversibility
251
The curious reader may wonder why we have not as yet tried to pigeonhole ergodic Markov
chains into our discussion of reversibility, instead conﬁning ourselves to irreducible, positive-
recurrent Markov chains. The reason lies in the aperiodicity property of an ergodic Markov chain.
An ergodic Markov chain is both positive recurrent and aperiodic, but as the previous example
shows, a periodic Markov chain can be reversible. The Markov chain of Example 9.27 has period
2. However, a Markov chain having a period greater than 2 cannot be reversible. Consider any two
distinct states i and j for which pi j > 0 in a Markov chain with period strictly greater than 2. Then,
in a single step, this Markov chain can move from state i to state j and we have πi pi j > 0. But
there is no possibility of the chain being able to move from j to i in a single step, for otherwise
the chain could not have period greater than 2. Hence π j p ji = 0 and the detailed balance equations
are not satisﬁed: a Markov chain with period strictly greater than 2 cannot be reversible. Ergodicity
is not a necessary condition for a Markov chain to be reversible whereas irreducibility and positive
recurrency are necessary. Reversible Markov chains having period equal to 2, as illustrated by the
birth–death examples, form an important class of reversible Markov chains.
An irreducible, positive-recurrent Markov chain whose transition probability matrix is symmet-
ric, i.e., pi j = p ji for all i, j, or in matrix terms, P = PT , is reversible. Such a matrix is doubly
stochastic: both its row sums and its column sums are equal to 1. It follows that all the elements in
the stationary distribution are equal and hence the detailed balance equations are satisﬁed. However,
a matrix that is doubly stochastic is reversible only if it is symmetric. Since all the components in
the stationary distribution of a doubly stochastic matrix are equal, the two diagonal matrices in
R = diag{π}−1 PT diag{π} cancel each other and we are left with R = PT .
A matrix A is said to have a symmetric structure if for all ai j ̸= 0 we have a ji ̸= 0. A necessary
condition for an irreducible, positive-recurrent Markov chain to be reversible is that its transition
probability matrix P have a symmetric structure. It follows that if pi j > 0 and p ji = 0 the detailed
balance equations cannot be satisﬁed.
Except in some special cases such as tree-structured Markov chains, it can be sometimes difﬁcult
to determine if a Markov chain is reversible. In the absence of any knowledge that might be obtained
from the system concerning its reversibility, the usual approach is to ﬁrst compute the stationary
distribution and verify that the detailed balance equations are satisﬁed. There is fortunately one other
characteristic of reversible Markov chains that can sometimes be valuable. This is encapsulated in
Kolmogorov’s criterion.
Kolmogorov’s criterion An irreducible, positive-recurrent Markov chain is reversible if and only
if the probability of traversing any closed path (cycle) is the same in both the forward and backward
directions.
To see why this is true, consider an irreducible, positive-recurrent Markov chain with transition
probability matrix P and the closed path
pi0i1 pi1i2 pi2i3 · · · pim−1im pimi0 > 0.
If this Markov chain is reversible, and R is the transition probability matrix of the reversed chain,
then R = P and
pi0i1 pi1i2 pi2i3 · · · pim−1im pimi0 = ri0i1ri1i2ri2i3 · · ·rim−1imrimi0
=
#
pi1i0πi1/πi0
$ #
pi2i1πi2/πi1
$ #
pi3i2πi3/πi2
$
· · ·
#
pi0imπi0/πim
$
= pi1i0 pi2i1 pi3i2 · · · pi0im
= pi0im · · · pi3i2 pi2i1 pi1i0,
and the probabilities in the forward and backward directions are the same. The converse is equally
easy to show. It follows that the reversibility property of a Markov chain can be veriﬁed by checking
that the Kolmogorov criterion is satisﬁed along all closed paths.

252
Discrete- and Continuous-Time Markov Chains
An important case which gives rise to a reversible Markov chain occurs in the following context.
Consider an arbitrarily connected undirected graph with positive weights on each edge. Let the
weight on the edge connecting node i and j be wi j, for all i and j. If the nodes correspond to cities,
for example, then the weights might represent the distance in miles between cities. The weighted
adjacency matrix of such a graph is symmetric. We now transform this adjacency matrix to the
transition probability matrix of a Markov chain by setting pi j = wi j/wi with wi = 
j wi j < ∞.
The matrix P obtained in this fashion, although not necessarily symmetric, does have a symmetric
structure. If w = 
i wi < ∞, then the ith component of the stationary distribution is given by
πi =

j wi j

k

j wkj
= wi
w
for all i.
Observe that πi > 0 for all i, that

i
πi =

i
wi
w = 1,
and
πi = wi
w =

k
wik
w =

k
wki
w =

k
wk
w
wki
wk
=

k
πk pki,
i.e., that π = π P and hence π is indeed the stationary distribution. Furthermore, since
πi pi j = wi
w
wi j
wi
= wi j
w = w ji
w = w ji
w
w j
w j
= w j
w
w ji
w j
= π j p ji,
the detailed balance equations are satisﬁed and this Markov chain is reversible.
Example 9.28 Consider the undirected graph whose adjacency matrix is
⎛
⎝
0
5
4
5
0
1
4
1
0
⎞
⎠.
Converting this to the transition probability matrix of a Markov chain gives
P =
⎛
⎝
0
5/9
4/9
5/6
0
1/6
4/5
1/5
0
⎞
⎠
and
π = (9/20, 6/20, 5/20).
The transition probability matrix of the reversed chain is
R =
⎛
⎝
20/9
0
0
0
20/6
0
0
0
20/5
⎞
⎠
⎛
⎝
0
5/6
4/5
5/9
0
1/5
4/9
1/6
0
⎞
⎠
⎛
⎝
9/20
0
0
0
6/20
0
0
0
5/20
⎞
⎠
=
⎛
⎝
0
5/9
4/9
5/6
0
1/6
4/5
1/5
0
⎞
⎠= P.
To close this section on reversible Markov chains, we consider the case of truncated chains.
Let P be the transition probability matrix of a reversible Markov chain and consider what happens
when some of the states are removed leaving a truncated process. In the state transition diagram
not only are the states removed but all edges leading to, or emanating from, the eliminated states

9.10 Continuous-Time Markov Chains
253
are also removed. The states that remain must be able to communicate with each other so that the
irreducibility property is conserved. For example, if the original Markov chain is tree structured,
and hence reversible, truncated Markov chains can be formed by removing some of the branches
but not by sawing the trunk in two. Without loss of generality, we may assume that the state space is
ordered in such a way that states remaining after the truncation precede those that are removed. This
means that after the truncation, only the upper left-hand portion of P contains nonzero probabilities
and we illustrate this as follows
P =
 P11 P12
P21 P22

,
ˆP =
 P11 0
0 0

,
where ˆP represents the truncated process. Notice that ˆP, or more precisely P11, is not stochastic.
Since the original Markov chain is irreducible, P12 cannot be identically zero and therefore the sum
of the probabilities across at least one row of P11 must be strictly less than 1. However, if the rows of
P11 are normalized so that they sum to 1 (replace pi j with pi j/ 
j pi j) then P11 becomes stochastic
and, furthermore, if the Markov chain represented by P11 is irreducible, then it is also reversible.
Let π be the stationary distribution of the original Markov chain and let E be the set of states in
the truncated chain. Since πi pi j = π j p ji for all states prior to truncation, it must also hold for the
states in E after truncation. The restriction of π to the states of E is not a proper distribution since
σ = 
i∈E πi < 1. However, when divided by σ it become a proper distribution and is in fact the
stationary distribution of the truncated chain. To summarize, an irreducible, truncated Markov chain,
obtained from a reversible Markov chain with stationary probability vector π, is itself reversible and
its stationary distribution is given by π/σ. Truncated processes arise frequently in queueing models
with ﬁnite waiting rooms.
9.10 Continuous-Time Markov Chains
In a discrete-time Markov chain, we deﬁne an inﬁnite (denumerable) sequence of time steps at which
the chain may either change state or remain in its current state. The transition probability matrix in
this case can have nonzero probabilities on the diagonal, called self-loops, which allows for the
possibility of the Markov chain remaining in its current state at any time step. With a continuous-
time Markov chain, a change of state may occur at any point in time. Analogous to the deﬁnition
of a discrete-time Markov chain, we say that a stochastic process {X(t), t ≥0} is a continuous-
time Markov chain if for all integers (states) n, and for any sequence t0, t1, . . . , tn, tn+1 such that
t0 < t1 < . . . < tn < tn+1,
Prob{X(tn+1) = xn+1|X(tn) = xn, X(tn−1) = xn−1, . . . , X(t0) = x0}
= Prob{X(tn+1) = xn+1|X(tn) = xn}.
Notice from this deﬁnition that not only is the sequence of previously visited states irrevelant to the
future evolution of the chain, but so too is the amount of time already spent in the current state.
Equivalently, we say that the stochastic process {X(t), t ≥0} is a continuous-time Markov chain
if for integers (states) i, j, k and for all time instants s, t, u with t ≥0, s ≥0, and 0 ≤u ≤s, we
have
Prob{X(s + t) = k|X(s) = j, X(u) = i} = Prob{X(s + t) = k|X(s) = j}.
If a continuous-time Markov chain is nonhomogeneous, we write
pi j(s, t) = Prob{X(t) = j|X(s) = i},
where X(t) denotes the state of the Markov chain at time t ≥s. When the continuous-time Markov
chain is homogeneous, these transition probabilities depend on the difference τ = t −s rather than

254
Discrete- and Continuous-Time Markov Chains
on the actual values of s and t. In this case, we simplify the notation by writing
pi j(τ) = Prob{X(s + τ) = j|X(s) = i} for all s ≥0.
This denotes the probability of being in state j after an interval of length τ, given that the current
state is state i. It depends on the length τ but not on s, the speciﬁc moment at which this time
interval begins. It follows that

all j
pi j(τ) = 1
for all values of τ.
9.10.1 Transition Probabilities and Transition Rates
Whereas in a discrete-time Markov chain the interactions among the states are given in terms of
transition probabilities, the interactions in a continuous-time Markov chain are usually speciﬁed
in terms of the rates at which transitions occur. This simpliﬁes the notation and the analysis. A
continuous-time Markov chain in some state i at time t will move to some other state j at rate
qi j(t) per unit time. In this manner, while a discrete-time Markov chain is represented by its matrix
of transition probabilities, P(n), at time step n, a continuous-time Markov chain is represented by
its matrix of transition rates, Q(t), at time t. It therefore behooves us to examine the relationship
between these two quantities (probabilities and rates) in the context of Markov chains.
The probability that a transition occurs from a given source state depends not only on the source
state itself but also on the length of the interval of observation. In what follows, we shall consider
a period of observation τ = 
t. Let pi j(t, t + 
t) be the probability that a transition occurs from
state i to state j in the interval [t, t + 
t). As the duration of this interval becomes very small,
the probability that we observe a transition also becomes very small. In other words, as 
t →0,
pi j(t, t+
t) →0 for i ̸= j. It then follows from conservation of probability that pii(t, t+
t) →1
as 
t →0. On the other hand, as 
t becomes large, the probability that we observe a transition
increases. As 
t becomes even larger, the probability that we observe multiple transitions becomes
nonnegligible. Observation periods are chosen sufﬁciently small that the probability of observing
multiple events in any observation period is of order o(
t). Recall that o(
t), “little oh,” is a
quantity for which
lim

t→0
o(
t)

t
= 0.
It is something small compared to 
t; in particular, o(
t) tends to zero faster than 
t.
A rate of transition does not depend on the length of an observation period; it is an instanta-
neously deﬁned quantity that denotes the number of transitions that occur per unit time. Let qi j(t)
be the rate at which transitions occur from state i to state j at time t. Note that the rate of transition
in a nonhomogeneous Markov chain, like the probability of transition, may depend on the time
t. However, unlike the transition probability, it does not depend on a time interval 
t. To help
understand these concepts, the reader may wish to associate probability with distance and rate with
speed and recall the relationship between distance and speed, where speed may be thought of as the
instantaneous rate at which distance is covered. We have
qi j(t) = lim

t→0
 pi j(t, t + 
t)

t
%
for i ̸= j.
(9.25)
It then follows that
pi j(t, t + 
t) = qi j(t)
t + o(
t)
for i ̸= j,
(9.26)
which in words simply states that, correct to terms of order o(
t), the probability that a transition
occurs from state i at time t to state j in the next 
t time units is equal to the rate of transition
at time t multiplied by the length of the time period 
t—exactly as it should be. In terms of the

9.10 Continuous-Time Markov Chains
255
distance/speed analogy, this simply says that the distance covered in a certain time (= pi j(t, t +
t))
is equal to the speed (= qi j(t)) multiplied by the time interval (= 
t).
From conservation of probability and Equation (9.26), we ﬁnd
1 −pii(t, t + 
t) =

j̸=i
pi j(t, t + 
t)
(9.27)
=

j̸=i
qi j(t)
t + o(
t).
(9.28)
Dividing by 
t and taking the limit as 
t →0, we get
lim

t→0
1 −pii(t, t + 
t)

t
%
= lim

t→0

j̸=i qi j(t)
t + o(
t)

t
%
=

j̸=i
qi j(t).
In continuous-time Markov chains, it is usual to let this be equal to −qii(t), or, alternatively, the
transition rate corresponding to the system remaining in place is deﬁned by the equation
qii(t) = −

j̸=i
qi j(t).
(9.29)
When state i is an absorbing state, qii(t) = 0. The fact that qii(t) is negative should not be surprising.
This quantity denotes a transition rate and as such is deﬁned as a derivative. Given that the system
is in state i at time t, the probability that it will transfer to a different state j increases with time,
whereas the probability that it remains in state i must decrease with time. It is appropriate in the
ﬁrst case that the derivative at time t be positive, and in the second that it be negative. Substitution
of Equation (9.29) into Equation 9.28 provides the analog of (9.26). For convenience we write them
both together,
pi j(t, t + 
t) = qi j(t)
t + o(
t)
for i ̸= j,
pii(t, t + 
t) = 1 + qii(t)
t + o(
t).
When the continuous-time Markov chain is homogeneous, we have
qi j = lim

t→0
 pi j(
t)

t

, i ̸= j;
q j j = lim

t→0
 p j j(
t) −1

t

.
(9.30)
In a continuous-time Markov chain, unlike its discrete-time counterpart, we do not associate self-
loops with states—continuously leaving and simultaneously returning to a state makes little sense.
The matrix Q(t) whose ijth element is qi j(t) is called the inﬁnitesimal generator or transition-
rate matrix for the continuous-time Markov chain. In matrix form, we have
Q(t) = lim

t→0
 P(t, t + 
t) −I

t
%
,
where P(t, t + 
t) is the transition probability matrix, its ijth element is pi j(t, t + 
t), and I is the
identity matrix. It is apparent from Equation (9.29) that the sum of all elements in any row of Q(t)
must be zero. When the continuous-time Markov chain is homogeneous, the transition rates qi j are
independent of time, and the matrix of transition rates is written simply as Q.
Let {X(t), t ≥0} be a homogeneous continuous-time Markov chain and suppose that at time
t = 0 the Markov chain is in nonabsorbing state i. Let Ti be the random variable that describes the

256
Discrete- and Continuous-Time Markov Chains
time until a transition out of state i occurs. Then, with s > 0, t > 0,
Prob{Ti > s + t | X(0) = i} = Prob{Ti > s + t | X(0) = i, Ti > s} Prob{Ti > s | X(0) = i}
= Prob{Ti > s + t | X(s) = i} Prob{Ti > s | X(0) = i}
= Prob{Ti > t | X(0) = i} Prob{Ti > s | X(0) = i},
(9.31)
and hence
ˆF(s + t) = ˆF(t) ˆF(s),
where ˆF(t) = Prob{Ti > t | X(0) = i}, t > 0. This equation is satisﬁed if and only if
ˆF(t) = e−μit for some positive parameter μi > 0 and t > 0. Thus the sojourn time in state i
must be exponentially distributed. Equation (9.31) affords a different perspective on this. If it is
known that the Markov chain started at time t = 0 in state i and has not moved from state i by time
s, i.e., Prob{Ti > s | X(0) = i} = 1, then
Prob{Ti > s + t | Ti > s} = Prob{Ti > t},
and the continuous random variable Ti is memoryless. Since the only continuous distribution that has
the memoryless property—the distribution of residual time being equal to the distribution itself—is
the exponential distribution, it follows that the duration of time until a transition occurs from state i
is exponentially distributed.
Given a homogeneous continuous-time Markov chain in which state i is a nonabsorbing state,
there is one or more states j ̸= i to which a transition from state i can occur. As we have just seen,
the memoryless property of the Markov chain forces the duration of time until such a transition
occurs to be exponentially distributed. Furthermore, we have also seen that the rate of transition
from state i to state j ̸= i is qi j. Putting these together, it follows that the distribution of the time
to reach some state j ̸= i is exponentially distributed with rate qi j. Furthermore, on exiting state i,
it is possible that many different states can be reached. A race condition is thereby established and
the actual transition occurs to the state that wins, i.e., the state which minimizes the sojourn time
in state i. Since the minimum value of a number of exponentially distributed random variables
is also an exponentially distributed random variable with rate equal to the sum of the original
rates, it follows that the time spent in state i of a homogeneous, continuous-time Markov chain
is exponentially distributed and that the parameter of the exponential distribution of this sojourn
time is μi = 
j̸=i qi j. Thus the probability distribution of the sojourn time in state i is given by
Fi(x) = 1 −e−μi x,
x ≥0,
where
μi =

j̸=i
qi j = −qii.
(9.32)
The arguments outlined above show that the sojourn time in any state of a homogeneous,
continuous-time Markov chain must be exponentially distributed. This does not hold if the Markov
chain is nonhomogeneous. The sojourn time in a nonhomogeneous continuous-time Markov chain is
not exponentially distributed, just as we saw previously that the sojourn time in a nonhomogeneous
discrete-time Markov chain is not geometrically distributed.
Example 9.29 Cars arrive at a QuikLube service center at an average rate of ﬁve per hour and
it takes on average ten minutes to service each car. To represent this situation as a homogeneous
continuous-time Markov chain, we must ﬁrst specify the state space of the model. We shall assume
that our only concern is with the number of cars in the QuickLube center at any time, which means
that we can use the nonnegative integers 0, 1, 2, . . . to represent the situation in which there are
0, 1, 2, . . . cars in the center. This is our state space. Our next concern is the interaction among the

9.10 Continuous-Time Markov Chains
257
states. If we assume that no more than one car can arrive at any moment, that no more than one car
can exit from service at any moment and that cars do not arrive and depart simultaneously,3 then the
transitions among states can only be to nearest neighbors. Possible transitions among the states are
shown in Figure 9.15.
4
0
1
2
3
Figure 9.15. The only permissible transitions are between neighboring states.
Transitions from any state i to its next highest neighbor i + 1 occur as the result of an arrival and
we are told that the mean time between arrivals is 1/5 hours. To satisfy the Markov property, this
interarrival time must be exponentially distributed with mean 1/5. In other words, the rate of going
from any state i to state i + 1 is qi,i+1 = 5/hour for i ≥0. Transitions from state i + 1 to state i
occur as the result of a car departing from the service center. Again, to satisfy the Markov property,
the distribution of time spent servicing a car must be exponentially distributed, with mean service
time equal to ten minutes. Thus the rate of going from any state i + 1 to state i is qi+1,i = 6/hour
for i ≥0.
Since the only transitions possible are to nearest neighbors, the inﬁnitesimal generator (or
transition-rate matrix) Q must be tridiagonal with superdiagonal elements all equal to 5 and
subdiagonal elements all equal to 6. The diagonal elements, as speciﬁed by Equation (9.29), are
the negated sums of the off-diagonal elements. It follows then that
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−5
5
0
0
0
· · ·
6
−11
5
0
0
· · ·
0
6
−11
5
0
· · ·
0
0
6
−11
5
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
9.10.2 The Chapman-Kolmogorov Equations
The Chapman-Kolmogorov equations for a nonhomogeneous continuous-time Markov chain may
be obtained directly from the Markov property. They are speciﬁed by
pi j(s, t) =

all k
pik(s, u)pkj(u, t)
for i, j = 0, 1, . . . and s ≤u ≤t.
In passing from state i at time s to state j at time t (s < t), we must pass through some intermediate
state k at some intermediate time u. When the continuous-time Markov chain is homogeneous, the
Chapman-Kolmogorov equation may be written as
pi j(t + 
t) =

all k
pik(t)pkj(
t)
for t, 
t ≥0
=

k̸= j
pik(t)pkj(
t) + pi j(t)p j j(
t).
(9.33)
3 We shall see later in our study of queueing systems that the exponential nature of interarrival and service times implies
that the probability of multiple arrivals, multiple departures, and simultaneous arrivals and departures are all equal to o(
t)
and disappear when certain limits are taken.

258
Discrete- and Continuous-Time Markov Chains
Thus
pi j(t + 
t) −pi j(t)

t
=

k̸= j
pik(t) pkj(
t)

t
+ pi j(t) p j j(
t)

t
−pi j(t)

t
=

k̸= j
pik(t) pkj(
t)

t
+ pi j(t)
 p j j(
t) −1

t

.
Taking the limit as 
t →0 and recalling Equation (9.30),
dpi j(t)
dt
=

k̸= j
pik(t)qkj + pi j(t)q j j.
In other words,
dpi j(t)
dt
=

all k
pik(t)qkj
for i, j = 0, 1, . . . .
These are called the Kolmogorov forward equations. In matrix form they are written as
d P(t)
dt
= P(t)Q.
(9.34)
In a similar manner, by writing Equation (9.33) in the form
pi j(t + 
t) =

all k
pik(
t)pkj(t)
for t, 
t ≥0,
we may derive the Kolmogorov backward equations, which are
dpi j(t)
dt
=

all k
qik pkj(t)
for i, j = 0, 1, . . . ,
or, in matrix form,
d P(t)
dt
= QP(t).
The solution of the Kolmogorov forward equations (9.34) is given by the matrix exponential
P(t) = ceQt = eQt =

I +
∞

n=1
Qntn
n!

.
(9.35)
for some constant of integration c = P(0) = I. Unfortunately, the computation of the matrix
exponential can be rather difﬁcult and unstable to compute. We shall take up this topic again in a later
section. For a nonhomogeneous Markov chain, the forward and backward Kolmogorov equations
are given by
d P(t)
dt
= P(t)Q(t)
and
d P(t)
dt
= Q(t)P(t)
respectively. In the remainder of this chapter, unless otherwise stated, we consider only homoge-
neous continuous-time Markov chains.
Example 9.30 The Matlab function expm (Q) may be used to ﬁnd the matrix exponential of a matrix
Q. Let us use it to compute P(t) for t = 0.0001, 0.01, 1, 2, 5, 10, and 100 when the inﬁnitesimal

9.10 Continuous-Time Markov Chains
259
generator matrix is given by
Q =
⎛
⎜
⎜
⎝
−4
4
0
0
3
−6
3
0
0
2
−4
2
0
0
1
−1
⎞
⎟
⎟
⎠.
We ﬁnd
P(0.0001) =
⎛
⎜
⎜
⎝
0.9996
0.0004
0.0000
0.0000
0.0003
0.9994
0.0003
0.0000
0.0000
0.0002
0.9996
0.0002
0.0000
0.0000
0.0001
0.9999
⎞
⎟
⎟
⎠,
P(0.01) =
⎛
⎜
⎜
⎝
0.9614
0.0380
0.0006
0.0000
0.0285
0.9426
0.0286
0.0003
0.0003
0.0190
0.9612
0.0195
0.0000
0.0001
0.0098
0.9901
⎞
⎟
⎟
⎠,
P(1) =
⎛
⎜
⎜
⎝
0.2285
0.2572
0.2567
0.2576
0.1929
0.2282
0.2573
0.3216
0.1283
0.1715
0.2503
0.4499
0.0644
0.1072
0.2249
0.6035
⎞
⎟
⎟
⎠,
P(2) =
⎛
⎜
⎜
⎝
0.1514
0.1891
0.2470
0.4125
0.1418
0.1803
0.2450
0.4329
0.1235
0.1633
0.2409
0.4723
0.1031
0.1443
0.2362
0.5164
⎞
⎟
⎟
⎠,
P(5) =
⎛
⎜
⎜
⎝
0.1208
0.1608
0.2402
0.4782
0.1206
0.1605
0.2401
0.4788
0.1201
0.1601
0.2400
0.4798
0.1195
0.1596
0.2399
0.4810
⎞
⎟
⎟
⎠,
P(10) = P(100) =
⎛
⎜
⎜
⎝
0.1200
0.1600
0.2400
0.4800
0.1200
0.1600
0.2400
0.4800
0.1200
0.1600
0.2400
0.4800
0.1200
0.1600
0.2400
0.4800
⎞
⎟
⎟
⎠.
9.10.3 The Embedded Markov Chain and State Properties
If we ignore the time actually spent in any state of a continuous-time Markov chain {X(t), t ≥0}
and consider only the sequence of transitions that actually occur, we deﬁne a new, discrete-time,
Markov chain —the embedded Markov chain (EMC), also called the jump chain. Let the nth state
visited by the continuous-time Markov chain be denoted by Yn. Then {Yn, n = 0, 1, 2, . . .} is the
embedded Markov chain derived from {X(t), t ≥0}. For a continuous-time Markov chain with no
absorbing states, the one-step transition probabilities of its embedded discrete-time Markov chain,
denoted by wi j, i.e., wi j ≡Prob {Yn+1 = j|Yn = i}, are equal to zero if i = j and
wi j =
qi j

j̸=i qi j
,
j ̸= i.
Since no state is absorbing, the denominator 
j̸=i qi j cannot be zero. If we let W denote the
transition probability matrix of the EMC, then in matrix terms we have
W = I −D−1
Q Q,
where DQ = diag{Q} is a diagonal matrix whose nonzero diagonal elements are equal to the dia-
gonal elements of Q. Note that all the elements of W satisfy 0 ≤wi j ≤1 and that 
j, j̸=i wi j = 1

260
Discrete- and Continuous-Time Markov Chains
for all i. Thus W possesses the characteristics of a transition probability matrix for a discrete-time
Markov chain.
Example 9.31 Consider a continuous-time Markov with inﬁnitesimal generator matrix
Q =
⎛
⎜
⎜
⎝
−4
4
0
0
3
−6
3
0
0
2
−4
2
0
0
1
−1
⎞
⎟
⎟
⎠;
we ﬁnd its discrete-time embedded chain to be
W =
⎛
⎜
⎜
⎝
0
1
0
0
.5
0
.5
0
0
.5
0
.5
0
0
1
0
⎞
⎟
⎟
⎠.
Many of the properties of the states of a continuous-time Markov chain {X(t), t ≥0} can
be deduced from those of its corresponding embedded chain. In a continuous-time Markov chain,
{X(t), t ≥0}, we say that state i can reach state j, for any i ̸= j, if there is a t ≥0 such that
pi j(t) > 0. This allows us to deﬁne the concept of irreducibility in a continuous-time Markov chain.
A continuous-time Markov chain is irreducible if, for any two states i and j, there exists reals
t1 ≥0, t2 ≥0 such that pi j(t1) > 0 and p ji(t2) > 0. It is then easy to show that a continuous-
time Markov chain is irreducible if and only if its embedded chain is irreducible. Notice from the
relationship W = I −D−1
Q Q that, for i ̸= j, wi j = 0 if and only if qi j = 0, and thus, if Q is
irreducible, W is irreducible, and vice versa.
The concepts of communicating class, closed class, absorbing state, recurrence, transience, and
irreducibility are all inherited directly from the embedded Markov chain. For example, a state i is
transient in a continuous-time Markov chain if and only if it is transient in its embedded chain.
Also, a state j is recurrent in a continuous-time Markov chain if and only if it is recurrent in its
embedded chain. However, a certain amount of care must be exercised, for it is possible for a state
j to be positive recurrent in a continuous-time Markov chain and null recurrent in its corresponding
embedded chain, and vice versa!
The concept of periodicity has no meaning in a continuous-time Markov chain, since there is no
concept of time steps at which transitions either occur or do not occur. Thus, a state of a continuous-
time Markov chain is said to be ergodic if it is positive recurrent and the Markov chain itself is said
to be ergodic if all its states are ergodic. In particular, a ﬁnite, irreducible continuous-time Markov
chain is ergodic.
When a continuous-time Markov chain is reducible with closed communicating classes, we often
wish to ﬁnd the probabilities of absorption into closed classes as well as the mean time to absorption.
Previous results, obtained in the context of discrete-time Markov chains and now written in terms of
the elements of Q, can be applied to the embedded Markov chain to produce corresponding results
for continuous-time Markov chains. Let A be a closed subset of states of a continuous-time Markov
chain. Then h A
i , the probability of hitting a state of A from any state i, is the ith component of the
solution to the system of equations
h A
i = 1
for i ∈A,

j
qi jh A
j = 0
for i ̸∈A.

9.10 Continuous-Time Markov Chains
261
Example 9.32 Consider a reducible, continuous-time Markov chain whose inﬁnitesimal generator
is
Q =
⎛
⎜
⎜
⎝
−4
2
1
1
3
−6
2
1
0
0
0
0
0
0
0
0
⎞
⎟
⎟
⎠,
and let us ﬁnd the probability of absorption from states 1 and 2 into the closed class A = {3}. We
have
h A
3 = 1,
q11h A
1 + q12h A
2 + q13h A
3 + q14h A
4 = 0,
q21h A
1 + q22h A
2 + q23h A
3 + q24h A
4 = 0,
q41h A
1 + q42h A
2 + q43h A
3 + q44h A
4 = 0.
Since h A
3 = 1 and h A
4 = 0, we obtain
−4h A
1 + 2h A
2 + 1 = 0,
3h A
1 −6h A
2 + 2 = 0,
which yields the result h A
1 = 0.5556 and h A
2 = 0.6111.
In a similar manner, it may be shown that k A
i , the mean time to absorption into a closed subset
A from some state i in a reducible, continuous-time Markov chain, is given as the ith component of
the solution of the system of equations
k A
i = 0
for i ∈A,
−

j
qi jk A
j = 1
for i ̸∈A.
Example 9.33 Consider a reducible, continuous-time Markov chain whose inﬁnitesimal generator
is
Q =
⎛
⎜
⎜
⎝
−4
4
0
0
3
−6
2
1
0
2
−4
2
0
0
0
0
⎞
⎟
⎟
⎠,
and let us ﬁnd the mean time to absorption from state 1 into the closed class (absorbing state)
A = {4}. We have
k A
4 = 0,
−q11k A
1 −q12k A
2 −q13k A
3 −q14k A
4 = 1,
−q21k A
1 −q22k A
2 −q23k A
3 −q24k A
4 = 1,
−q31k A
1 −q32k A
2 −q33k A
3 −q34k A
4 = 1.
Since k A
4 = 0, we obtain
4k A
1 −4k A
2 = 1,
−3k A
1 + 6k A
2 −2k A
3 = 1,
−2k A
2 + 4k A
3 = 1,

262
Discrete- and Continuous-Time Markov Chains
which has the solution (1.3750, 1.1250, 0.8125). It follows that the mean time to absorption
beginning from state 1 is 1.3750.
These equations have a simple, intuitive justiﬁcation. Starting in state 1, we spend a time equal
to −1/q11 = 1/4 in that state before moving to state 2. The mean time to absorption from state 1,
which we have called k1, is therefore equal to 1/4 plus the mean time to absorption from state 2, i.e.,
k1 = 1/4 + k2 or 4k1 −4k2 = 1 which gives us the ﬁrst equation. Similarly, starting in state 2, we
spend a time equal to −1/q22 = 1/6 there before going to state 1 with probability 3/6, or to state 3
with probability 2/6 or to state 4 with probability 1/6. Therefore the mean time to absorption from
state 2 is given by k2 = 1/6 + k1/2 + k3/3, i.e., the second equation −3k1 + 6k2 −2k3 = 1, and
so on.
9.10.4 Probability Distributions
Transient Distributions
Consider a system that is modeled by a continuous-time Markov chain. Let πi(t) be the probability
that the system is in state i at time t, i.e.,
πi(t) = Prob{X(t) = i}.
The probability that the system is in state i at time t + 
t, correct to terms of order o(
t), must be
equal to the probability that it is in state i at time t and it does not change state in the period [t, 
t),
plus the probability that it is in some state k ̸= i at time t and moves to state i in the interval 
t,
i.e.,
πi(t + 
t) = πi(t)
⎛
⎝1 −

all j̸=i
qi j(t)
t
⎞
⎠+
⎛
⎝
all k̸=i
qki(t)πk(t)
⎞
⎠
t + o(
t).
Since qii(t) = −
all j̸=i qi j(t), we have
πi(t + 
t) = πi(t) +

all k
qki(t)πk(t)


t + o(
t)
and
lim

t→0
πi(t + 
t) −πi(t)

t

= lim

t→0

all k
qki(t)πk(t) + o(
t)/
t

,
i.e.,
dπi(t)
dt
=

all k
qki(t)πk(t).
In matrix notation, this gives
dπ(t)
dt
= π(t)Q(t).
When the Markov chain is homogeneous, we may drop the dependence on time and simply write
dπ(t)
dt
= π(t)Q.
It follows that the solution π(t) is given by
π(t) = π(0)eQt = π(0)

I +
∞

n=1
Qntn
n!

.
(9.36)

9.10 Continuous-Time Markov Chains
263
Observe that this could have been obtained directly from Equation (9.35) since, by deﬁnition
π(t) = π(0)P(t) = π(0)eQt.
Example 9.34 Returning to Example 9.31, and assuming that the Markov chain begins in state 1,
i.e., π(0) = (1, 0, 0, 0), we ﬁnd
π(0.0001) = (0.9996, 0.0004, 0.0000, 0.0000),
π(0.01) = (0.9614, 0.0381, 0.0006, 0.0000),
π(1) = (0.2285, 0.2572, 0.2567, 0.2576),
π(2) = (0.1514, 0.1891, 0.2470, 0.4125),
π(5) = (0.1208, 0.1608, 0.2402, 0.4782),
π(10) = π(100) = (0.1200, 0.1600, 0.2400, 0.4800).
Stationary, Limiting, and Steady-State Distributions
In a manner similar to that described for a discrete-time Markov chain, we deﬁne an invariant vector
of a homogeneous continuous-time Markov chain with generator matrix Q as any nonzero vector z
for which zQ = 0. If this system of equations has a solution z which is a probability vector (zi ≥0
for all i and ∥z∥1 = 1), then z is a stationary distribution. If replacement of one of the equations
by a normalizing equation causes the coefﬁcient matrix to become nonsingular, then the stationary
distribution is unique. This is the case when the Markov chain is irreducible and ﬁnite.
We now turn to limiting distributions. In a homogeneous, continuous-time Markov chain, the ith
element of the vector π(t) is the probability that the chain is in state i at time t and we have just
seen that these state probabilities are governed by the system of differential equations
dπ(t)
dt
= π(t)Q.
If the evolution of the Markov chain is such that there arrives a point in time at which the rate of
change of the probability distribution vector π(t) is zero, then the left-hand side of this equation,
dπ(t)/dt, is identically equal to zero. In this case, the system has reached a limiting distribution,
which is written simply as π in order to show that it no longer depends on time t.
When the limiting distribution exists, when all its components are strictly positive, and when it
is independent of the initial probability vector π(0), then it is unique and it is called the steady-state
distribution, also referred to as the equilibrium or long-run probability vector—its ith element πi is
the probability of being in state i at statistical equilibrium. For a ﬁnite, irreducible, continuous-time
Markov chain, the limiting distribution always exists and is identical to the stationary distribution of
the chain. The steady-state distribution may be obtained by solving the system of linear equations
π Q = 0,
(9.37)
subject to the condition that ∥π∥1 = 1. Just as in the discrete-time case, these equations are called
the global balance equations. From 
all i πiqi j = 0, we have

i, i̸= j
πiqi j = −π jq j j
= π j

i, i̸= j
q ji.
The left-hand side represents the total ﬂow from all states i, different from j, into state j, while
the right-hand side represents the total ﬂow out of state j into all other states i ̸= j. Thus these
equations equate the ﬂow into and out of states.

264
Discrete- and Continuous-Time Markov Chains
Example 9.35 Consider the homogeneous continuous-time Markov chain of Example 9.31:
Q =
⎛
⎜
⎜
⎝
−4
4
0
0
3
−6
3
0
0
2
−4
2
0
0
1
−1
⎞
⎟
⎟
⎠.
We derive its stationary/limiting distribution by solving the system of equations
π Q = (π1, π2, π3, π4)
⎛
⎜
⎜
⎝
−4
4
0
0
3
−6
3
0
0
2
−4
2
0
0
1
−1
⎞
⎟
⎟
⎠= (0, 0, 0, 0).
Arbitrarily setting π1 = 1, we obtain from the ﬁrst equation
−4π1 + 3π2 = 0 =⇒π2 = 4/3.
Now, using the second equation,
4π1 −6π2 + 2π3
=⇒π3 = 2,
and from the third equation,
3π2 −4π3 + π4 = 0 =⇒π4 = 4.
The vector π = (1, 4/3, 2, 4) must be normalized by dividing each component by 25/3 to obtain
the same distribution as before, namely, π = (.12, .16, .24, .48).
The stationary distribution of an irreducible, continuous-time Markov chain may be found
from the stationary distribution of its embedded chain, assuming it exists. Let φ be the stationary
distribution of the embedded chain, i.e.,
φ(I −W) = 0
and ∥φ∥1 = 1,
where
W = I −D−1
Q Q
or
D−1
Q Q = (I −W).
(9.38)
Premultiplying Equation (9.38) by φ, we see that
φD−1
Q Q = φ(I −W) = 0
and hence −φD−1
Q (whose ith component is −φi/qii) is an invariant vector for Q. This becomes
a stationary distribution of Q once we normalize it, so that the sum of its components is one.
Speciﬁcally, the stationary distribution of Q is given as
π = −φD−1
Q
∥φD−1
Q ∥1
.
Example 9.36 It is easy to check that the stationary probability vector of the embedded Markov
chain of Example 9.31 is given by
φ = (1/6, 1/3, 1/3, 1/6).
Since
DQ =
⎛
⎜
⎜
⎝
−4
0
0
0
0
−6
0
0
0
0
−4
0
0
0
0
−1
⎞
⎟
⎟
⎠,

9.11 Semi-Markov Processes
265
we have
−φD−1
Q = (1/6, 1/3, 1/3, 1/6)
⎛
⎜
⎜
⎝
1/4
0
0
0
0
1/6
0
0
0
0
1/4
0
0
0
0
1
⎞
⎟
⎟
⎠= (1/24, 1/18, 1/12, 1/6),
which when normalized yields the stationary probability vector of the continuous-time Markov
chain as
π = (.12, .16, .24, .48)
—the same result as that obtained previously.
9.10.5 Reversibility
The concept of reversibility as discussed previously in the context of discrete-time Markov chains
carries over in a straightforward manner to continuous-time Markov chains. Given a homogeneous
continuous-time Markov chain with inﬁnitesimal generator Q and stationary probability vector π,
its reversed chain can be constructed. If ˜Q denotes the inﬁnitesimal generator of the reversed chain,
then
˜qi j = q ji
π j
πi
,
and the Markov chain is reversible if and only if ˜Q = Q. The detailed balance equation are given by
πiqi j = π jq ji
for all i, j.
Kolomogorov’s criterion continues to apply: a positive-recurrent, continuous-time Markov chain is
reversible if and only if the product of the transition rates along any circuit is the same in both the
forward and backward directions. Finally, a continuous-time Markov chain is reversible if and only
if its embedded Markov chain is reversible.
9.11 Semi-Markov Processes
Semi-Markov Processes (SMPs) are generalizations of Markov chains in that the future evolution of
the process is independent of the sequence of states visited prior to the current state and independent
of the time spent in each of the previously visited states, as is the case for discrete- and continuous-
time Markov chains. Semi-Markov processes differ from Markov chains in that the probability
distribution of the remaining time in any state can depend on the length of time the process has
already spent in that state. Let {Xn, n = 0, 1, 2, . . .} be a discrete-time Markov chain where Xn
is the nth state visited by a certain stochastic process. Let τn be the time at which the nth transition
of the stochastic process occurs, with τ0 = 0. Then the (homogeneous) continuous-time process
{Z(t), t ≥0} deﬁned by
Z(t) = Xn on τn ≤t < τn+1
is a semi-Markov process and is represented by its associated semi-Markov kernel,
Ki j(t) = Prob{Xn+1 = j, τn+1 −τn ≤t | X0, X1, . . . , Xn = i; τ0, τ1, . . . τn}
= Prob{Xn+1 = j, τn+1 −τn ≤t | Xn = i},
t ≥0.
If the semi-Markov process is nonhomogeneous, its kernel is given by
Ki j(s, t) = Prob{Xn+1 = j, τn+1 ≤t | Xn = i, τn = s},
0 ≤s ≤t.

266
Discrete- and Continuous-Time Markov Chains
However, we consider only homogeneous semi-Markov processes in this text. Deﬁne
pi j = lim
t→∞Ki j(t) = Prob{ Xn+1 = j |Xn = i},
n = 0, 1, 2, . . . .
These are the elements of the transition probability matrix P of the homogeneous discrete-time
(embedded) Markov chain which determines the state to which the process will next go on leaving
state i.
Thus, a semi-Markov process consists of two components: (i) a discrete-time Markov chain
{X(n), n = 0, 1, . . .} with transition probability matrix P which describes the sequence of states
visited by the process, and (ii) Hi j(t), the conditional distribution function of a random variable Ti j
which describes the time spent in state i from the moment the process last entered that state
Hi j(t) = Prob{Ti j ≤t} = Prob{τn+1 −τn ≤t | Xn+1 = j, Xn = i},
t ≥0.
The random variable Ti j is the sojourn time in state i per visit to state i prior to jumping to state
j. Observe that Hi j(t) can depend on both the source state i and the destination state j of the
transition. The probability distributions Hi j(t) and Ki j(t) are related. Let us make the assumption
that if pi j = 0 for some i and j; then Hi j(t) = 1 and Ki j(t) = 0 for all t. Then
pi j Hi j(t) = Ki j(t),
since
Prob{τn+1 −τn ≤t | Xn+1 = j, Xn = i} Prob{ Xn+1 = j |Xn = i}
= Prob{Xn+1 = j, τn+1 −τn ≤t | Xn = i}.
The evolution of a semi-Markov process is as follows:
1. The moment the semi-Markov process enters any state i, it randomly selects the next state
to visit j according to P, its transition probability matrix.
2. If state j is selected, then the time the process remains in state i before moving to state j is
a random variable Ti j with probability distribution Hi j(t).
Thus the next state to visit is chosen ﬁrst and the time to be spent in state i chosen second, which
allows the sojourn time per visit to depend on the destination state as well as the source state. A semi-
Markov process deﬁned in reverse order would force Hi j(t) = Hi(t) for all j—the same distribution
for all destination states. A semi-Markov process in which Hi j(t) is an exponential distribution, e.g.,
Hi j(t) = 1 −e−μi jt
or equivalently Ki j(t) = pi j
#
1 −e−μi jt$
,
t ≥0,
is a (homogeneous) continuous-time Markov chain and whereas the Markov (memoryless) property
of a semi-Markov process holds only at the moments of transition t = τk, k = 0, 1, 2, . . . , it
holds at all times t when Hi j(t) has an exponential form and only when Hi j(t) has this form. Thus
although a semi-Markov process provides a great deal of latitude in the choice of sojourn times Ti j,
it does so at the cost of losing the memoryless property at all but a sequence of points in time. When
a semi-Markov process evolves over a discrete set of time points and when the time between any
two transitions is taken to be one unit, i.e., τn+1 −τn = 1 for all n, then Hi j(t) = 1 for all i, j and
the semi-Markov process is a discrete-time Markov chain.
Since the random variable Ti j denotes the conditional sojourn time in state i given that the next
state to visit is state j, the random variable Ti, deﬁned as
Ti =

j
pi jTi j,

9.12 Renewal Processes
267
is the unconditional sojourn time per visit to state i and has probability distribution function
Hi(t) = Prob{Ti ≤t} =

j
pi j Hi j(t) =

j
Ki j(t),
t ≥0.
Sample paths in a semi-Markov process are speciﬁed by providing both the sequence of states
visited and the times at which each state along the path is entered—a sequence of paired state and
entrance times, such as
(x0, τ0), (x1, τ1), (x2, τ2), . . . , (xk, τk), (xk+1, τk+1), . . . ,
signifying that the process enters state xk at instant τk and remains there during the period [τk, τk+1).
To ﬁnd the stationary distribution π of a semi-Markov process,we need to ﬁnd the stationary
distribution of its embedded Markov chain and the mean sojourn time in each state. Let φ be the
stationary distribution of the embedded Markov chain, i.e., φ = φP and ∥φ∥1 = 1, and let Ti be the
random variable that describes the per visit sojourn time in state i and Ti j the random variable that
describes the time in state i prior to a jump to state j. The mean sojourn time in state i is
E[Ti] =

j
pi j E[Ti j]
and the stationary distribution is the vector whose ith component is
πi =
φi E[Ti]

all j φ j E[Tj],
where the denominator forces the probabilities to sum to 1. In words, this says that the stationary
probability of the semi-Markov process being in any state i is just the (normalized) product of the
stationary probability that its embedded Markov chain is in state i and the average amount of time
the semi-Markov process spends in state i.
9.12 Renewal Processes
A renewal process is a special type of counting process. A counting process, {N(t), t ≥0}, is a
stochastic process which counts the number of events that occur up to (and including) time t. Thus
any counting process N(t) is integer valued with the properties that N(t) ≥0 and N(t1) ≤N(t2) if
t1 ≤t2. We shall let Xn, n ≥1, be the random variable that denotes the time which elapses between
events (n −1) and n. This allows us to deﬁne a renewal process as follows:
Deﬁnition 9.11.1 (Renewal process) A counting process {N(t), t ≥0} is a renewal process if the
sequence {X1, X2, . . .} of nonnegative random variables that represent the time between events are
independent and identically distributed.
Observe that this deﬁnition permits events to occur simultaneously (Xn = 0), but we shall restrict
ourselves to renewal processes where this cannot happen. In our deﬁnition, X1, the time until the
ﬁrst event, is independent of, and has the same distribution as, all other random variables Xi, i > 1.
This may be interpreted in terms of a zeroth event which occurs at time t = 0, giving X1 the same
interevent time meaning as the other Xi. An alternative approach is to assume that the time until the
occurrence of the ﬁrst event has a different distribution from the random variables Xi, i > 1.
The word “renewal” is appropriate since on the occurrence of each event, the process essentially
renews itself: at the exact moment of occurrence of an event, the distribution of time until the next
event is independent of everything that has happened previously. The term “recurrent process” is
sometimes used in place of “renewal process.”

268
Discrete- and Continuous-Time Markov Chains
Example 9.37
1. Suppose the interarrival times of pieces of junk mail are independent and identically
distributed. Then {N(t), t ≥0}, the number of pieces of junk mail that have arrived by
time t, is a renewal process.
2. Assume an inﬁnite supply of standard ﬂashlight batteries whose lifetimes are independent
and identically distributed. As soon as one battery dies it is replaced by another. Then
{N(t), t ≥0}, the number of batteries that have failed by time t, is a renewal process.
3. A biased coin is tossed at times t = 1, 2, . . . . The probability of a head appearing at any
time is ρ, 0 < ρ < 1. Then {N(t), t ≥0}, with N(0) = 0, the number of heads obtained
up to and including time t, is a renewal process. The time between renewals in this case all
have the same geometric probability distribution function:
Prob{Xn = i} = ρ(1 −ρ)i−1,
i ≥1.
The renewal process that results is called a binomial process.
Let {N(t), t ≥0} be a renewal process with interevent (or interrenewal) periods X1, X2, . . . and
let Sn be the time at which the nth event/renewal occurs, i.e.,
S0 = 0,
Sn = X1 + X2 + · · · + Xn, n ≥1.
In other words, the process renews itself for the nth time at time Sn. The sequence {Sn, n ≥0} is
called a renewal sequence. The period between renewals is called a cycle; a cycle is completed the
moment a renewal occurs.
Example 9.38 Let {Yn, n ≥0} be a homogeneous, discrete-time Markov chain whose state space is
the nonnegative integers and assume that at time t = 0 the chain is in state k. Let Sn denote the time
at which the nth visit to state k begins. Since {Xn = Sn −Sn−1, n ≥1} is a sequence of independent
and identically distributed random variables, it follows that {Sn, n ≥0} is a renewal sequence and
{N(t), t ≥0}, the number of visits to state k in (0, t], is a renewal process associated with state k.
The initial visit to state k (the process starting at time t = 0 in state k) must not be included in this
count. A similar statement can be made with respect to a continuous-time Markov chain.
Let the distribution function of the random variables {Xn, n ≥1} be denoted by FX(t), i.e.,
Prob{Xn ≤t} = FX(t),
and let us ﬁnd the distribution function of the renewal process {N(t), t ≥0}. First note that since
the random variables Xn are independent and identically distributed, the distribution of the renewal
sequence Sn = X1 + X2 + · · · + Xn is given as F(n)
X (t), the n-fold convolution of FX(t) with itself.4
The only way the number of renewals can exceed or equal n at time t (N(t) ≥n) is if the nth renewal
occurs no later than time t (Sn ≤t). The converse is also true: the only way that the nth renewal can
occur no later than time t is if the number of renewals prior to t is at least equal to n. This means
that N(t) ≥n if and only if Sn ≤t and we may write
Prob{N(t) ≥n} = Prob{Sn ≤t}.
(9.39)
4 Recall that if H(t) is the distribution function that results when two random variables having distribution functions F(t)
and G(t) are added, then the convolution is deﬁned as
H(t) =
 t
0
F(t −x) dG(x) =
 t
0
G(t −x) dF(x).
Less rigorously, we may write H(t) =
 t
0 F(t −x)g(x)dx =
 t
0 G(t −x) f (x)dx, where f (x) and g(x) are the corresponding
probability density functions.

9.12 Renewal Processes
269
Therefore the probability of exactly n renewals by time t is given by
Prob{N(t) = n} = Prob{N(t) ≥n} −Prob{N(t) ≥n + 1}
and using Equation (9.39) we conclude
Prob{N(t) = n} = Prob{Sn ≤t} −Prob{Sn+1 ≤t} = F(n)
X (t) −F(n+1)
X
(t).
Example 9.39 Assume that interrenewal times are exponentially distributed with parameter λ, i.e.,
FX(t) = 1 −e−λt,
t ≥0.
Then
F(n)
X (t) = 1 −
n−1

k=0
e−λt (λt)k
k! ,
t ≥0,
an Erlang distribution, and
Prob{N(t) = n} = F(n)
X (t) −F(n+1)
X
(t) = e−λt (λt)n
n!
—the renewal process {N(t), t ≥0} has a Poisson distribution and is called a Poisson process.
We now show that it is not possible for an inﬁnite number of renewals to occur in a ﬁnite
period of time. Let ω = E[Xn], n ≥1, be the mean time between successive renewals. Since
{Xn, n ≥1} are nonnegative random variables and we have chosen the simplifying assumption that
Prob{Xn = 0} = FX(0) < 1, it follows that ω must be strictly greater than zero, i.e., 0 < ω ≤∞.
In instances in which the mean interrenewal time is inﬁnite, we shall interpret 1/ω as zero. We now
show that
Prob{N(t) < ∞} = 1 for all ﬁnite t ≥0.
From the strong law of large numbers
Prob
 Sn
n →ω
%
= 1 as n →∞,
and since 0 < ω ≤∞, Sn must tend to inﬁnity as n tends to inﬁnity:
Prob{ lim
n→∞Sn = ∞} = 1.
Now using the fact that N(t) = max{n : Sn ≤t}, we ﬁnd, for ﬁnite t,
Prob{N(t) = ∞} = Prob{max{n : Sn ≤t} = ∞} = Prob{ lim
n→∞Sn ≤t < ∞}
= 1 −Prob{ lim
n→∞Sn = ∞} = 0.
This result is for ﬁnite t only and does not hold as t →∞. When t →∞, we have limt→∞N(t) = ∞.
Finding the probability distribution of an arbitrary renewal function can be difﬁcult so that
frequently only E[N(t)], the expected number of renewals by time t, is computed. The mean
E[N(t)] is called the renewal function and is denoted by M(t). We have (see also Exercise 9.12.2)
M(t) = E[N(t)] =
∞

n=1
Prob{N(t) ≥n} =
∞

n=1
Prob{Sn ≤t}
=
∞

n=1
F(n)
X (t).

270
Discrete- and Continuous-Time Markov Chains
We now show that M(t) uniquely determines the renewal process. Let
˜M(s) =
 ∞
0
e−stdM(t),
˜F X(s) =
 ∞
0
e−stdFX(t),
and
˜F(n)
X (s) =
 ∞
0
e−stdF(n)
X (t)
be the Laplace-Stiltjes transform (LST) of M(t), FX(t), and F(n)
X (t), respectively. Then
˜M(s) =
 ∞
0
e−std
 ∞

n=1
F(n)
X (t)

=
∞

n=1
 ∞
0
e−stdF(n)
X (t) =
∞

n=1
˜F(n)
X (s)
=
∞

n=1
# ˜F X(s)
$n =
˜F X(s)
1 −˜F X(s).
Thus
˜M(s) =
˜F X(s)
1 −˜F X(s)
or
˜F X(s) =
˜M(s)
1 + ˜M(s)
so that once ˜M(s) is known completely, so also is ˜F X(s), and vice versa. Since a renewal process is
completely characterized by FX(t), it follows that it is also completely characterized by the renewal
function M(t). Continuing with the renewal function, observe that
M(t) =
∞

n=1
F(n)
X (t) = F(1)
X (t) +
∞

n=1
F(n+1)
X
(t) = FX(t) +
∞

n=1
 t
0
F(n)
X (t −s)dFX(s)
= FX(t) +
 t
0
∞

n=1
F(n)
X (t −s)dFX(s) = FX(t) +
 t
0
M(t −s)dFX(s).
(9.40)
This is called the fundamental renewal equation. Taking the LST of both sides gives the previous
result, namely,
˜M(s) = ˜F X(s) + ˜M(s) ˜F X(s).
Example 9.40 Suppose that interrenewal times are exponentially distributed with parameter λ,
i.e., FX(t) = 1 −e−λt. Then
˜F X(s) =
λ
λ + s
and
˜M(s) =
λ/(λ + s)
1 −λ/(λ + s) = λ
s
which when inverted gives
M(t) = λt.
We may conclude that, when the expected number of renewals increases linearly with time, the
renewal process is a Poisson process. Furthermore, from the uniqueness property, the Poisson
process is the only renewal process with a linear mean-value (renewal) function.
We now give two important results on the limiting behavior of renewal processes. We have
previously seen that the limit as t tends to inﬁnity of N(t) is inﬁnite. Our ﬁrst result concerns
the rate at which N(t) →∞. Observe that N(t)/t is the average number of renewals per unit time.

9.12 Renewal Processes
271
We now show that, with probability 1,
N(t)
t
→1
ω
as t →∞,
and point out why 1/ω is called the rate of the renewal process. Recall that Sn is the time at which
the nth renewal occurs. Since N(t) is the number of arrivals that occur prior to or at time t, it must
follow that SN(t) is just the time at which the last renewal prior to or at time t occurred. Likewise,
SN(t)+1 is the time at which the ﬁrst renewal after time t occurs. Therefore
SN(t) ≤t < SN(t)+1
and
SN(t)
N(t) ≤
t
N(t) < SN(t)+1
N(t) .
Since SN(t)/N(t) is the average of N(t) independent and identically distributed random variables:
SN(t)
N(t) =
1
N(t)
N(t)

i=1
Xi.
Given that ω = E[Xn] is the mean time between (successive) renewals, it follows from the strong
law of large numbers that
lim
N(t)→∞
SN(t)
N(t) = ω,
which is the same as writing
lim
t→∞
SN(t)
N(t) = ω,
since N(t) →∞when t →∞. Also, by means of a similar argument,
lim
N(t)→∞
SN(t)+1
N(t)
=
lim
N(t)→∞
SN(t)+1
N(t) + 1
N(t) + 1
N(t)
=

lim
N(t)→∞
SN(t)+1
N(t) + 1
 
lim
N(t)→∞
N(t) + 1
N(t)

= ω.
Thus we see that t/N(t) is sandwiched between two numbers each of which tends to ω as t →∞.
Therefore t/N(t) must also converge to ω as t →∞. Now using the fact that 1/x is a continuous
function, we have, with probability 1,
lim
t→∞
N(t)
t
= 1
ω.
Intuitively, this result makes sense. N(t)/t is the average number of renewals per unit time over the
interval (0, t] and in the limit as t tends to inﬁnity, it is the average rate of renewals. In other words,
the expected rate of renewals for all renewal processes is 1/ω, i.e., the reciprocal of the expected
time between renewals. Given an average interrenewal time of six minutes (ω = 6) we would expect
to get renewals at the rate of ten per hour, or 1/6 per minute.
The second result, which we state without proof, is similar. It can be shown that, with
probability 1,
lim
t→∞
M(t)
t
= 1
ω.
The derivative of the renewal function M(t) = E[N(t)] is called the renewal density and is
denoted by m(t).
m(t) = dM(t)
dt
=
∞

n=1
f (n)
X (t),

272
Discrete- and Continuous-Time Markov Chains
where f (n)
X (t) is the derivative of F(n)
X (t). This leads to the renewal equation (as opposed to the
fundamental renewal equation, Equation (9.40)):
m(t) = fX(t) +
 t
0
m(t −s) fX(s)ds.
It may be shown that
lim
t→∞m(t) = 1
ω.
For small values of 
t, m(t)
t gives the probability that a renewal will occur in the interval
(t, t + 
t]. For a Poisson process, we saw that M(t) = λt. Hence m(t) = λ when a renewal
process is Poisson, as we might have expected.
Renewal Reward Processes
Consider a renewal process {N(t), t ≥0} and let Xn be the nth interrenewal time. Assume that
on the completion of a cycle, a “reward” is received or alternatively a “cost” is paid. Let Rn be
the reward (positive or negative) obtained at the nth renewal. We assume that the rewards Rn are
independent and identically distributed. This does not prevent Rn from depending on Xn, the length
of the cycle in which the reward Rn is earned. The total reward received by time t is given by
R(t) =
N(t)

n=1
Rn.
Observe that, if Rn = 1 for all n, then R(t) = N(t), the original renewal process. We now show that
lim
t→∞
R(t)
t
= E[R]
E[X],
where E[R] is the expected reward obtained in any cycle, and E[X] is the expected duration of a
cycle. We have
R(t)
t
=
N(t)
n=1 Rn
t
=
N(t)
n=1 Rn
N(t)
× N(t)
t
.
Taking the limits of both sides as t →∞, and using the fact that the strong law of large numbers
allows us to write
lim
t→∞
N(t)
n=1 Rn
N(t)
= E[R],
gives us the desired result:
lim
t→∞
R(t)
t
= E[R] × lim
t→∞
N(t)
t
= E[R]
E[X].
This tells us that the long run rate of reward is equal to the expected reward per cycle divided by the
mean cycle length. It may also be shown that
lim
t→∞
E[R(t)]
t
= E[R]
E[X],
i.e., the expected reward per unit time in the long run is also equal to the expected reward per cycle
divided by the mean cycle length.
Renewal reward models arise in the context of an ongoing process in which a product, such
as a car or piece of machinery, is used for a period of time (a cycle) and then replaced. In
order to have a renewal process, the new car/machine is assumed to have identical characteristics

9.12 Renewal Processes
273
(more precisely, an identical lifetime function) to the one that is replaced. A replacement policy
speciﬁes a recommended time T at which to purchase the new product and the cost c1 of doing so
at this time. A cost c2 over and above the replacement cost c1 must be paid if for some reason (e.g.,
the car/machine breaks down) replacement must take place prior to the recommended time T . In
some scenarios, a third factor, the resale value of the car/machine, is also included.
Let Yn, n ≥1, be the lifetime of the nth machine and assume that the Yn are independent
and identically distributed with probability distribution function F(t). If Sn is the time of the nth
replacement, then the sequence {Sn, n ≥0} is a renewal sequence. Let Xn be the time between two
replacements, i.e., Xn is the duration of the nth cycle and we have
Xn = min {Yn, T } .
In the absence of a resale value, the reward (or more properly, cost) Rn is given by
Rn =

c1,
Yn ≥T,
c1 + c2,
Yn < T,
and R(t), the total cost up to time t, is a renewal reward process:
R(t) =
N(t)

n=1
Rn,
N(t) > 0.
Using X to denote the duration of an arbitrary cycle and Y the lifetime of an arbitrary machine, the
expected cost per cycle is
E[R] = c1Prob{Y ≥T } + (c1 + c2)Prob{Y < T } = c1 + c2F(T ).
We now need to compute E[X], the expected length of a cycle. Since the length of the nth cycle is
min{Yn, T } we have
E[X] =
 T
0
x f (x)dx +
 ∞
T
T f (x)dx =
 T
0
x f (x)dx + T (1 −F(T )),
where f (t) is the lifetime density function of an arbitrary machine. Hence the long-run average
cost is
E[R]
E[X] =
c1 + c2F(T )
 T
0 x f (x)dx + T (1 −F(T ))
.
It makes sense to try to ﬁnd the optimal value of T , the value which minimizes the expected cost. If
T is chosen to be small, the number of replacements will be high but the cost due to failure will be
small. If T is chosen to be large, then the opposite occurs.
Example 9.41
A manufacturing company produces widgets from a machine called an “Inneall”
whose lifetimes Xn are independent and identically uniformly distributed between three and ﬁve
years. In the context of the renewal reward model just described, we shall let c1 = 5 and c2 = 1
(hundred thousand dollars). Let us ﬁnd the optimal replacement policy. The probability distribution
and density functions of the lifetime of the Inneall machine are
FX(x) =
⎧
⎨
⎩
0,
x < 3,
(x −3)/2,
3 ≤x ≤5,
1,
x > 5,
and
fX(x) =
⎧
⎨
⎩
0,
x < 3,
1/2,
3 ≤x ≤5,
0,
x > 5.

274
Discrete- and Continuous-Time Markov Chains
Then
E[R]
E[X] =
c1 + c2F(T )
 T
0 x fX(x)dx + T (1 −F(T ))
=
c1 + c2(T −3)/2
T 2/4 + T (1 −[(T −3)/2]) =
7/2 + T/2
5T/2 −T 2/4.
To ﬁnd the minimum, we take the derivative of this function and set the numerator to zero:
5T
2 −T 2
4
 1
2 −
T
2 + 7
2
 5
2 −T
2

= 0.
Simplifying, this reduces to
T 2 + 14T −70 = 0,
which has roots equal to −17.9087 and 3.9087, the latter of which gives the value of T that
minimizes the long-run average cost of the Inneall machine.
It is interesting to observe what happens when the lifetime of the product has an exponential
distribution with parameter μ, i.e., FY(t) = 1 −e−μt,
fY(t) = μe−μt. In this case
E[R]
E[X] =
c1 + c2
#
1 −e−μT $
 T
0 μxe−μxdx + T e−μT =
c1 + c2
#
1 −e−μT $
−xe−μx|T
0 +
 T
0 e−μxdx + T e−μT
= c1 + c2
#
1 −e−μT $
#
1 −e−μT $
/μ
=
μc1
1 −e−μT + μc2,
—a monotonically decreasing function of T ! The exponential property of the lifetime distribution
holds that at any point in time, the remaining lifetime is identical to the lifetime of a brand new
machine.
Alternating Renewal Processes
An alternating renewal process is a renewal process in which the random variables Xn, n ≥1,
representing the time between renewals, are constructed from the sum of two or more random
variables. In the simplest case there are just two random variables which we denote by Yn and
Zn. Thus Xn = Yn + Zn. If Yn and Zn are independent and identically distributed, and are
independent of each other, then the process with interrenewal periods Xn = Yn +Zn is an alternating
renewal process. The extension to more than two random variables is immediate. Our interest with
alternating renewal processes is generally in determining the proportion of time that the process
spends in one of the phases that constitute a part of a cycle. This is accomplished by associating a
reward with the duration of the selected phase.
Example 9.42 Consider a machine that works correctly for a time and then breaks down. As soon
as it breaks down a repair process is begun and the machine brought back to the functioning state.
In this case, a cycle consists of a working period followed by a repair period. If Yn is the random
variable that describes the length of the working period in the nth cycle, and Zn the corresponding
repair period and if Yn and Zn have the independence and distribution properties described above,
then the process is an alternating renewal process.
To relate this to renewal reward processes, we let the reward be the length of time that the
machine functions correctly. This allows us to compute the proportion of time that the machine is
functioning as
E[Y]
E[X] =
E[Y]
E[Y] + E[Z].
It may be shown that this is also the long-run probability that the machine is working.

9.13 Exercises
275
9.13 Exercises
Exercise 9.1.1 Give a real (or imagined) world example of
(a) a stationary, homogeneous stochastic process,
(b) a stationary, nonhomogeneous stochastic process,
(c) a nonstationary, homogeneous stochastic process, and
(d) a nonstationary, nonhomogeneous stochastic process.
In each case, state whether your process is a discrete-state or a continuous-state process and whether it is a
discrete-time or a continuous-time process.
Exercise 9.1.2 In the four scenarios of your answer to the previous question, state whether each stochastic
process is, or is not, a Markov process. Be sure to justify your answer.
Exercise 9.2.1 Brand preference model.
Suppose that there are ﬁve types of breakfast cereal, which we call A, B, C, D, and E. Customers tend to
stick to the same brand. Those who choose type A choose it the next time around with probability 0.8; those
who choose type B choose it next time with probability 0.9. The probabilities for types C, D, and E are given
by 0.7, 0.8, and 0.6, respectively. When customers do change brand, they choose one of the other four equally
probably. Explain how this may be modeled by a Markov chain and give the transition probability matrix.
Exercise 9.2.2 Inventory chain model.
Consider a store that sells television sets. If at the end of the day there is one or zero sets left, then that
evening, after the store has closed, the shopkeeper brings an enough new sets so that the number of sets in
stock for the next day is equal to ﬁve. This means that each morning, at store opening time, there are between
two and ﬁve television sets available for sale. Such a policy is said to be an (s, S) inventory control policy. Here
we have assigned the values s = 1, S = 5. The shopkeeper knows from experience that the probabilities of
selling 0 through 5 sets on any given day are 0.4, 0.3, 0.15, 0.15, 0.0, and 0.0.
Explain how this scenario may be modeled by a Markov chain {Xn, n = 1, 2, . . .}, where Xn is the random
variable that deﬁnes the number of television sets left at the end of the nth day. Write down and explain the
structure of the transition probability matrix.
Exercise 9.2.3 The transition probability matrix of a discrete-time Markov chain is given by
P =
⎛
⎜
⎜
⎜
⎜
⎝
0
0
0
0
1.0
0
0
1.0
0
0
0
0
0
1.0
0
0
0.8
0.2
0
0
0.4
0
0.6
0
0
⎞
⎟
⎟
⎟
⎟
⎠
.
Draw all sample paths of length 4 that begin in state 1. What is the probability of being in each of the states 1
through 5 after four steps beginning in state 1?
Exercise 9.2.4 Consider a discrete-time Markov chain consisting of four states a, b, c, and d and whose
transition probability matrix is given by
P =
⎛
⎜
⎜
⎝
0.0
0.0
1.0
0.0
0.0
0.4
0.6
0.0
0.8
0.0
0.2
0.0
0.2
0.3
0.0
0.5
⎞
⎟
⎟
⎠.
Compute the following probabilities:
(a) Prob{X4 = c, X3 = c, X2 = c, X1 = c | X0 = a}.
(b) Prob{X6 = d, X5 = c, X4 = b | X3 = a}.
(c) Prob{X5 = c, X6 = a, X7 = c, X8 = c | X4 = b, X3 = d}.
Exercise 9.2.5 A Markov chain with two states a and b has the following conditional probabilities: If it is in
state a at time step n, n = 0, 1, 2, . . . , then it stays in state a with probability 0.5(0.5)n. If it is in state b at

276
Discrete- and Continuous-Time Markov Chains
time step n, then it stays in state b with probability 0.75(0.25)n. If the Markov chain begins in state a at time
step n = 0, compute the probabilities of the following sample paths:
a −→b −→a −→b
and
a −→a −→b −→b.
Exercise 9.2.6 The transition probability matrix of an embedded Markov chain is
P E =
⎛
⎜
⎜
⎝
0.0
0.3
0.4
0.3
0.1
0.0
0.2
0.7
0.3
0.2
0.0
0.5
0.4
0.4
0.2
0.0
⎞
⎟
⎟
⎠.
Given that the homogeneous, discrete-time Markov chain P from which this embedded chain is extracted,
spends on average, 1 time unit in state 1, 2 time units in state 2, 4 time units in state 3, and 2.5 time units in
state 4, derive the original Markov chain, P.
Exercise 9.3.1 The following matrix is the single-step transition probability matrix of a discrete-time Markov
chain which describes the weather. State 1 represents a sunny day, state 2 a cloudy day, and state 3 a rainy day.
P =
⎛
⎝
Sunny
Cloudy
Rainy
Sunny
0.7
0.2
0.1
Cloudy
0.3
0.5
0.2
Rainy
0.2
0.6
0.2
⎞
⎠.
(a) What is the probability of a sunny day being followed by two cloudy days?
(b) Given that today is rainy, what is the probability that the sun will shine the day after tomorrow?
(c) What is the mean length of a rainy period?
Exercise 9.3.2 Consider the four-state discrete-time Markov chain whose transition probability matrix at time
step n, n = 0, 1, . . . , is
P(n) =
⎛
⎜
⎜
⎝
0
0.6
0.4
0
0.8
0
0
0.2
0
0
0.5(0.5)n
1 −0.5(0.5)n
0
0
0.8(0.8)n
1 −0.8(0.8)n
⎞
⎟
⎟
⎠.
What is the probability distribution after two steps if the Markov chain is initiated in (a) state 1; (b) state 4?
Exercise 9.3.3 William, the collector, enjoys collecting the toys in McDonald’s’ Happy Meals. And now
McDonald’s has come out with a new collection containing ﬁve toy warriors. Each Happy Meal includes
one randomly chosen warrior. Naturally William has to collect all ﬁve different types.
(a) Use a discrete-time Markov chain to represent the process that William will go through to collect all
ﬁve warriors and draw the state transition diagram.
(b) Construct the stochastic transition probability matrix for this discrete-time Markov chain and compute
the probability distribution after William has eaten three happy meals.
(c) Let T denote the total number of Happy Meals that William will eat to enable him to get all ﬁve
warriors. Compute E[T ] and Var [T ].
Exercise 9.3.4 A prisoner in a Kafkaesque prison is put in the following situation. A regular deck of 52 cards
is placed in front of him. He must choose cards one at a time to determine their color. Once chosen, the card is
replaced in the deck and the deck is shufﬂed. If the prisoner happens to select three consecutive red cards, he
is executed. If he happens to select six cards before three consecutive red cards appear he is granted freedom.
(a) Represent the prisoner’s situation with a Markov chain and draw its transition diagram.
Hint: let state i denote the number of successive red cards obtained.
(b) Construct the transition probability matrix, and the initial state probability vector.
(c) Determine the probability that the prisoner will be set free.

9.13 Exercises
277
Exercise 9.4.1 Give example state transition diagrams for the following types of Markov chain states. Justify
your answers. (a) Transient state, (b) positive-recurrent state, (c) periodic state, (d) absorbing state, (e) closed
set of states, and (f) irreducible chain.
Exercise 9.4.2 Give as precise a classiﬁcation as possible to each of the states of the discrete-time Markov
chain whose transition probability matrix is
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
0
.3
0
.7
0
0
0
0
0
0
0
.6
0
0
.4
0
0
0
0
0
0
0
.5
0
0
.5
0
0
0
0
0
0
.2
0
0
0
.8
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
.9
.1
0
0
0
0
0
0
0
.8
0
0
.2
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Exercise 9.4.3 Find the mean recurrence time of state 2 and the mean ﬁrst passage time from state 1 to state 2
in the discrete-time Markov chain whose transition probability matrix is
P =
⎛
⎝
0.1
0.3
0.6
0.2
0.5
0.3
0.4
0.4
0.2
⎞
⎠.
Hint: use Matlab.
Exercise 9.5.1 In the Markov chain whose transition probability matrix P is given below, identify examples
of the following (if they exist):
(a) a return state,
(b) a nonreturn state,
(c) an absorbing state,
(d) a closed communicating class,
(e) an open communicating class,
(f) a closed communicating class containing recurrent states,
(g) an open communicating class containing recurrent states,
(h) a closed communicating class containing transient states,
(i) an open communicating class containing transient states, and
(j) a communicating class with both transient and recurrent states.
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0.5
0
0.5
0
0
0
0
0.5
0
0.5
0
0
0
0
0
0
0
0
0.5
0
0.5
0
0
0
0
0
0
1.0
0
0
0
0
0
0
0
0
0
0
1.0
0
0
0
0
0
1.0
0
0
0
0
0
0
1.0
0
0
0
0
0
0
0
0
0
1.0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

278
Discrete- and Continuous-Time Markov Chains
Exercise 9.5.2 In the Markov chain whose transition probability matrix P is given below, what are the
communicating classes to which the following states belong: (a) state 2, (b) state 3, (c) state 4, and (d) state 5.
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.5
0.25
0.25
0
0
0
0
0.25
0.5
0
0.25
0
0
0
0
0
0.5
0.5
0
0
0
0
0
0
0
0.5
0.5
0
0
0
0
0
0.5
0.25
0.25
0
0
0
0
0.25
0.5
0.25
0
0
0
0
0.25
0.25
0.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Exercise 9.5.3 Consider a discrete-time Markov chain with transition probabilities given by
pi j = e−λ
j

n=0
 i
n

pnqi−n
λ j−n
( j −n)!,
where p + q = 1, 0 ≤p ≤1 and λ > 0.
(a) Is this chain reducible? Explain.
(b) Is this chain periodic? Explain.
Exercise 9.5.4 Prove that if one state in a communicating class C is transient, then all states in C are transient.
Exercise 9.5.5 Let C be a nonclosed communicating class. Show that no state in C can be recurrent. (This
means that every recurrent class is closed.)
Exercise 9.6.1 Compute the potential matrix of the Markov chain whose transition probability matrix is
P =
⎛
⎜
⎜
⎝
.5
.5
0
0
.5
0
.5
0
.5
0
0
.5
0
0
0
1
⎞
⎟
⎟
⎠
and from it ﬁnd (a) the expected number of visits to state 3 beginning from state 1, (b) the expected number of
visits to state 1 beginning in state 3, and (c) the expected number of visits to state 2 beginning in state 4.
Exercise 9.6.2 Consider the gambler’s ruin problem in which a gambler begins with $50, wins $10 on each
play with probability p = 0.45, or loses $10 with probability q = 0.55. The gambler will quit once he doubles
his money or has nothing left of his original $50.
(a) What is the expected number of times he has $90 before quitting?
(b) What is the expected number of times he has $50 before quitting?
(c) What is the expected number of times he has $10 before quitting?
Exercise 9.6.3 Consider a discrete-time Markov chains whose transition probability matrix is
P =
⎛
⎜
⎜
⎜
⎜
⎝
.2
.8
0
0
0
0
.4
.6
0
0
0
0
.6
.4
0
0
0
0
.8
.2
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
.
Compute the mean and variance of the time to absorption from all transient starting states.

9.13 Exercises
279
Exercise 9.6.4 The transition probability matrix of a discrete-time Markov chain is
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.6
.4
0
0
0
0
0
0
0
.2
.6
0
.2
0
0
0
.2
0
.3
0
0
0
0
.5
0
0
0
0
0
.9
0
.1
0
0
0
0
0
1.0
0
0
0
0
0
0
.5
0
.5
0
0
0
0
0
1.0
0
0
0
0
0
0
0
0
0
0
1.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Assume that the initial state is 2 with probability 0.4 and 4 with probability 0.6. Compute
(a) the mean and variance of the number of times the Markov chain visits state 1 before absorption;
(b) the mean and variance of the total number of steps prior to absorption.
Exercise 9.6.5 Consider a discrete-time Markov chain whose transition probability matrix is
P =
⎛
⎜
⎜
⎜
⎜
⎝
.2
.8
0
0
0
0
.4
.6
0
0
0
0
.6
.4
0
.2
0
0
.6
.2
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
.
(a) Compute the part of the reachability matrix that corresponds to transitions among transient states (the
matrix called H in the text).
(b) Beginning in state 1, what is the probability of going to state 4 exactly four times?
(c) Beginning in state 1, how many different states are visited, on average, prior to absorption?
Exercise 9.6.6 In the gambler’s ruin scenario of Exercise 9.6.2:
(a) What is the mean number of plays he makes (starting with $50) before stopping?
(b) How many different amounts of money does he have during the time he is playing?
Exercise 9.6.7 Compute the absorbing chain and from it, the matrix of absorbing probabilities, of the Markov
chain whose transition probability matrix is
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.6
.4
0
0
0
0
0
0
0
.2
.6
0
.2
0
0
0
.2
0
.3
0
0
0
0
.5
0
0
0
0
0
.9
0
.1
0
0
0
0
0
1.0
0
0
0
0
0
0
.5
0
.5
0
0
0
0
0
1.0
0
0
0
0
0
0
0
0
0
0
1.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
What are the probabilities of being absorbed into state 8 from states 1, 2, and 3?
Exercise 9.6.8
Consider a random walk problem on the set of integers, 0, 1, . . . , 10. Let the transition
probabilities be given by pi,i+1 = 0.45; pi+1,i = 0.55, i = 0, 1, . . . , 9, p00 = 0.55, p10,10 = 0.45. Beginning
in state 5, what is the probability of reaching state 0 before state 10?

280
Discrete- and Continuous-Time Markov Chains
Exercise 9.7.1 Random walk on the integers 0, ±1, ±2, . . ..
Consider the Markov chain whose transition probability matrix is
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
...
...
...
...
...
...
...
· · ·
· · ·
0
p
0
0
0
0
· · ·
· · ·
q
0
p
0
0
0
· · ·
· · ·
0
q
0
p
0
0
· · ·
· · ·
0
0
q
0
p
0
· · ·
· · ·
0
0
0
q
0
p
· · ·
· · ·
0
0
0
0
q
0
· · ·
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Show that
p(2n+1)
00
= 0
and
p(2n)
00
=
2n
n

pnqn, for n = 1, 2, 3, . . . .
Use Sterling’s formula, n! ≈nn+1/2e−n√
2π, to show that (2n)!/(n!n!) = 4n/√nπ and hence that
p(2n)
00
≈(4pq)n
√nπ .
Write an expression for ∞
n=1 p(n)
00 and show that this is inﬁnite if and only if p = 1/2, and hence that the
Markov chain is recurrent if and only if p = 1/2 and is transient for all other values of p.
Exercise 9.7.2 Consider a Markov chain whose transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎝
q
p
0
· · ·
q
0
p
0
· · ·
0
q
0
p
0
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎠,
where p > 0, q > 0, and p + q = 1. This is a variant of the random walk problem with Bernoulli boundary.
By considering the system of equations z = zP with ze = 1 and Theorem 9.7.1, ﬁnd conditions under which
the states of the Markov chain are ergodic and, assuming that these conditions hold, ﬁnd the values of zk for
k = 0, 1, . . . .
Exercise 9.7.3 Complete the analysis for the gambler’s ruin problem by considering the case when p = q =
1/2. In particular, show that the probability of winning N dollars when starting with i dollars is xi = i/N, for
i = 1, 2, . . . , N.
Exercise 9.7.4 One of the machines at a gambling casino pays a return that is slightly favorable to the player,
rather than to the house. On each play, the player has a 51% chance of winning. What is the probability that a
gambler with $20 doubles his money? What is the probability of doubling his money if he starts with $100?
Exercise 9.7.5 On each play in roulette, a player wins $10 with probability 18/38 and loses $10 with
probability 20/38. Kathie has $100 in her purse but really needs $250 to purchase the pair of shoes she has
just seen in the shop next to the casino. What is the probability of her converting her $100 into $250 at the
roulette table?
Exercise 9.7.6 Harold Nunn is an entrepreneur. He wishes to establish a high-stakes gambling saloon based
only on a single roulette table. Each bet costs a player $500. Harold knows that the odds of 20/38 are in his
favor. Suppose he starts with a capital of $100,000, what is the probability that Harold makes an unlimited
fortune?
Exercise 9.8.1 A square matrix is said to be stochastic if each of its elements lies in the interval [0, 1] and
the sum of the elements in each row is equal to 1. It is said to be doubly stochastic if, in addition, the sum of
elements in each column is equal to 1. Show that each element of the stationary distribution of an irreducible,
ﬁnite, K-state Markov chain whose transition probability matrix is doubly stochastic is equal to 1/K. Is this
stationary distribution unique? Does this same result hold when the Markov chain is reducible? Explain your

9.13 Exercises
281
answer. Can an irreducible Markov chain whose transition probability matrix is doubly stochastic have transient
states? Explain your answer. Does the limiting distribution necessarily exist for an irreducible Markov chain
whose transition probability matrix is doubly stochastic? Explain your answer.
Exercise 9.8.2 Let {Xn, n ≥0} be a two-state Markov chain, whose transition probability matrix is given by
P =
 1 −p
p
q
1 −q

with 0 < p, q < 1. Use the fact that Pn+1 = Pn P to show
p(n+1)
11
= (1 −p −q)p(n)
11 + q.
Now prove by induction that
p(n)
11 =
q
p + q +
p
p + q (1 −p −q)n
and hence ﬁnd the limit: limn→∞Pn.
Exercise 9.8.3 Let {Xn, n ≥0} be a two-state Markov chain, whose transition probability matrix is given by
P =
1 −p
p
q
1 −q

,
where 0 < p < 1 and 0 < q < 1. Let Prob{X0 = 0} = π0(0) be the probability that the Markov chain begins
in state 0. Prove by induction that
Prob{Xn = 0} = (1 −p −q)nπ0(0) + q
n−1

j=0
(1 −p −q) j.
Exercise 9.8.4 In the scenario of Exercise 9.3.1,
(a) What is the unconditional probability of having a sunny day?
(b) What is the mean number of rainy days in a month of 31 days?
(c) What is the mean recurrence time of sunny days?
Exercise 9.8.5 For each of the following Markov chains (A, B, C, and D), represented by transition
probability matrices given below, state whether it has (i) a stationary distribution, (ii) a limiting distribution,
(iii) a steady-state distribution. If your answer is negative, explain why? If your answer is positive, give the
distribution.
PA =
⎛
⎝
0.5
0.5
0.0
0.0
0.5
0.5
0.5
0.0
0.5
⎞
⎠,
PB =
⎛
⎝
0.0
1.0
0.0
0.0
0.0
1.0
1.0
0.0
0.0
⎞
⎠, PC =
⎛
⎜
⎜
⎝
0.5
0.5
0.0
0.0
0.5
0.5
0.0
0.0
0.0
0.0
0.5
0.5
0.0
0.0
0.5
0.5
⎞
⎟
⎟
⎠.
PD(n) =

1/n
(n −1)/n
(n −1)/n
1/n

,
n = 1, 2, . . . .
Exercise 9.8.6 Consider a Markov chain having state space S = {0, 1, 2} and transition matrix
⎛
⎝
0
1
2
0
1/3
1/3
1/3
1
1/4
1/2
1/4
2
1/6
1/3
1/2
⎞
⎠.
Show that this chain has a unique stationary distribution π and ﬁnd π.
Exercise 9.8.7 Consider a machine that at the start of any particular day is either broken down or in operating
condition. Assume that if the machine is broken down at the start of the nth day, the probability that it will be

282
Discrete- and Continuous-Time Markov Chains
successfully repaired and in operating condition at the start of the (n + 1)th day is p. Assume also that if the
machine is in operating condition at the start of the nth day, the probability that it will fail and be broken down
at the start of the (n + 1)th day is q. Let π0(0) denote the probability that the machine is broken down initially.
(a) Find the following probabilities:
Prob{ Xn+1 = 1 | Xn = 0 }, Prob{ Xn+1 = 0 | Xn = 1 }, Prob{ Xn+1 = 0 | Xn = 0 }.
Prob{ Xn+1 = 1 | Xn = 1 } and Prob{ X0 = 1 }.
(b) Compute Prob{Xn = 0} and Prob{Xn = 1} in terms of p, q, and π0(0).
(c) Find the steady-state distribution lim
n→∞Prob{Xn = 0} and lim
n→∞Prob{X n = 1}.
Exercise 9.8.8 Consider a Markov chain deﬁned on the nonnegative integers and having transition probabilities
given by
pn,n+1 = p and pn,0 = 1 −p
for all n, where 0 < p < 1.
(a) Compute the mean recurrence time of state 0.
(b) Show that the Markov chain is positive recurrent.
(c) Prove that the limiting probabilities π exist.
(d) Find π (in terms of p).
Exercise 9.8.9 Remaining lifetime.
Analyze the Markov chain whose transition probability matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
p0
p1
p2
p3
· · ·
1
0
0
0
· · ·
0
1
0
0
· · ·
0
0
1
0
· · ·
0
0
0
1
· · ·
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
This Markov chain arises in models of a system component which fails and which is instantaneously replaced
with an identical component. When this identical component also fails, it too is instantaneously replaced with
another identical component, and so on. The Markov chain, {Xn; n ≥0}, denotes the remaining lifetime of the
component in use at time step n. The probability pk denotes the probability that a newly installed component
will last for k time steps.
(a) Under what condition(s) does a stationary distribution exist for this Markov chain?
(b) Compute this distribution assuming your condition(s) hold.
Exercise 9.9.1 The transition probability matrices of a number of Markov chains are given below. For each
different P, ﬁnd the transition probability matrix of the reversed chain.
P1 =
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠,
P2 =
⎛
⎝
.4
.2
.4
.1
.3
.6
.5
.5
0
⎞
⎠,
P3 =
⎛
⎝
1/3
1/3
1/3
1/4
1/2
1/4
1/6
1/3
1/2
⎞
⎠,
P4 =
⎛
⎜
⎜
⎝
0
1
0
0
0
0
.6
.4
0
0
0
1
.6
.4
0
0
⎞
⎟
⎟
⎠.
Exercise 9.9.2 Which of the transition probability matrices given below belong to reversible Markov chains?
Give reasons for your answer.
P1 =
⎛
⎜
⎜
⎝
.25
.25
.25
.25
.25
.50
.25
0.0
.50
0.0
.25
.25
.10
.20
.30
.40
⎞
⎟
⎟
⎠,
P2 =
⎛
⎜
⎜
⎝
0
3/4
1/4
0
1/3
0
4/9
2/9
1/10
4/10
0
5/10
0
2/7
5/7
0
⎞
⎟
⎟
⎠,

9.13 Exercises
283
P3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.1
0
0
.9
0
0
0
0
0
0
1
0
0
0
0
0
1
0
.4
0
0
0
.2
.4
0
.5
.3
.2
0
0
0
0
0
1
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
P4 =
⎛
⎝
.5
.1
.4
.2
.4
.4
.3
.2
.5
⎞
⎠.
Exercise 9.10.1 A triple redundancy system consists of three computing units and a“voter.” The purpose of
the voter is to compare the results produced by the three computing units and to select the answer given by at
least two of them. The computing units as well as the voter are all susceptible to failure. The rate at which a
computing unit fails is taken to be λ failures per hour and the rate at which the voter fails is ξ failures per hour.
All items are repairable: computing units are repaired at a rate of μ per hour and the voter at a rate of ν per
hour. During the repair of the voter, each computing unit is inspected and repaired if necessary. The time for
repair of the computing units in this case is included in the 1/ν hours needed to repair the voter. This system is
to be represented as a continuous-time Markov chain.
(a) What conditions are imposed by representing this system as a continuous-time Markov chain?
(b) What is a suitable state descriptor for this Markov chain?
(c) Draw the state transition rate diagram.
(d) Write down the inﬁnitesimal generator for this Markov chain.
Exercise 9.10.2 The time between arrivals at a service center is exponentially distributed with a mean
interarrival time of ﬁfteen minutes. Each arrival brings one item to be serviced with probability p (0 < p < 1)
or two items with probability 1 −p. Items are serviced individually at a rate of ﬁve per hour. Represent this
system as a continuous-time Markov chain and give its inﬁnitesimal generator Q.
Exercise 9.10.3 Construct the embedded chain of the continuous-time Markov chains whose inﬁnitesimal
generators are given below:
Q1 =
−5
5
2
−2

,
Q2 =
⎛
⎝
−3
2
1
0
−5
5
0
2
−2
⎞
⎠.
Exercise 9.10.4 Consider a six-state (0, 1, . . . , 5) continuous-time Markov chain with a tridiagonal inﬁnitesi-
mal generator given by
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
2
−3
1
4
−6
2
6
−9
3
8
−12
4
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(a) Find the probability of being absorbed into state 0 from state 3.
(b) Find the probability of being absorbed into state 5 from state 2.
(c) Let B = {0, 5}. Find the mean time to absorption from state 1 into the closed subset B.
(d) Find the mean time to absorption from state 4 into B.
Exercise 9.10.5 Suppose a single repairman has been assigned the responsibility of maintaining three
machines. For each machine, the probability distribution of up time (machine is functioning properly) before
a breakdown is exponential with a mean of nine hours. The repair time is also exponentially distributed with
a mean of two hours. Calculate the steady-state probability distribution and the expected number of machines
that are not running.

284
Discrete- and Continuous-Time Markov Chains
Exercise 9.10.6 Consider a continuous-time Markov chain with transition rate matrix
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
2
−3
1
4
−6
2
6
−9
3
8
−12
4
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Use Matlab and its matrix exponential function to compute the probability of being in state 1, state 4, and
state 6 at times t = 0.01, 0.1, 0.5, 1.0, 2.0, and 5.0, given that the system begins in state 4.
Exercise 9.10.7 Compute the stationary probability distribution of the continuous-time Markov chain whose
inﬁnitesimal generator Q is shown below.
(a) From the stationary distribution of its embedded chain, and
(b) By solving the system of equation π Q = 0.
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−1
1
−2
2
−3
3
−4
4
−5
5
6
−6
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
Exercise 9.12.1 Suppose the interrenewal time in a renewal process has a Poisson distribution with mean λ.
Find Prob{Sn ≤t}, the probability distribution of the renewal sequence Sn, and calculate Prob{N(t) = n}.
Exercise 9.12.2 As it relates to renewal processes, use the deﬁnition
Prob{N(t) = n} = F(n)
X (t) −F(n+1)
X
(t)
to show that
E[N(t)] =
∞

n=1
F(n)
X (t).
Exercise 9.12.3 Brendan replaces his car every T years, unless it has been involved in a major accident prior to
T years, in which case he replaces it immediately. If the lifetimes of the cars he purchases are independent and
identically distributed with distribution function FX(x), show that, in the long run, the rate at which Brendan
replaces cars is given by
1
 T
0 x fX(x)dx + T (1 −FX(T )))
,
where fX(x) is the density function of his cars’ lifetimes.
Exercise 9.12.4 The builder of the Inneall machine of Example 9.41 is prepared to offer the widget
manufacturer a trade-in W(T) on the purchase of a new machine. Write an expression for the long-run average
cost given this trade-in function. Now, using the parameters of Example 9.41, ﬁnd the value of T that minimizes
the long-run average cost if the trade-in function is given by W(T ) = 3 −T/2.
Exercise 9.12.5 Consider a queueing system that consists of a single server and no waiting positions. An
arriving customer who ﬁnds the server busy is lost—i.e., departs immediately without receiving service. If the
arrival process is Poisson with rate λ and the distribution of service time has expectation 1/μ, show that the
queueing process is an alternating renewal process and ﬁnd the probability that the server is busy.

Chapter 10
Numerical Solution of Markov Chains
10.1 Introduction
We now turn our attention to the computation of stationary and transient distributions of Markov
chains. For the computation of stationary distributions, we consider ﬁrst the case when the
number of states can be large but ﬁnite. In particular, we examine direct methods based on
Gaussian elimination, point and block iterative methods such as point and block Gauss–Seidel
and decompositional methods that are especially well suited to Markov chains that are almost
reducible. Second, when the Markov chain has an inﬁnite state space and the pattern of nonzero
elements in the transition matrix is block repetitive, we develop and analyze the matrix geometric
and matrix analytic methods. As for the computation of transient distributions, we describe methods
that are appropriate for small-scale Markov chains, as well as the popular uniformization method
and methods based on differential equation solvers for large-scale chains. Throughout this chapter,
our concern is with stationary, homogeneous Markov chains only.
10.1.1 Setting the Stage
We begin by considering ﬁnite, irreducible Markov chains. Such Markov chains have a unique
stationary probability distribution π whose elements are strictly greater than zero. When the Markov
chain is also aperiodic, this unique stationary distribution is also the steady-state distribution. Let P
be the transition probability matrix of a ﬁnite, irreducible discrete-time Markov chain. Then π is the
left-hand eigenvector corresponding to the dominant (and simple) unit eigenvalue of P, normalized
so that its elements sum to 1:
π P = π
with πe = 1.
(10.1)
If the Markov chain evolves in continuous time rather than in discrete time and its inﬁnitesimal
generator is denoted by Q, then its stationary distribution may be found from the system of linear
equations
π Q = 0 with πe = 1.
(10.2)
Observe that both these equations may be put into the same form. We may write the ﬁrst,
π P = π, as π(P −I) = 0 thereby putting it into the form of Equation (10.2). Observe that
(P −I) has all the properties of an inﬁnitesimal generator, namely, all off-diagonal elements are
nonnegative; row sums are equal to zero and diagonal elements are equal to the negated sum of
off-diagonal row elements. On the other hand, we may discretize a continuous-time Markov chain.
From
π Q = 0 with πe = 1
we may write
π(Q
t + I) = π
with πe = 1,
(10.3)

286
Numerical Solution of Markov Chains
thereby posing it in the form of Equation (10.1). In the discretized Markov chain, transitions take
place at intervals 
t, 
t being chosen sufﬁciently small that the probability of two transitions taking
place in time 
t is negligible, i.e., of order o(
t). One possibility is to take

t ≤
1
maxi |qii|.
In this case, the matrix (Q
t + I) is stochastic and the stationary1 probability vector π of the
continuous-time Markov chain, obtained from π Q = 0, is identical to that of the discretized chain,
obtained from π(Q
t + I) = π. It now follows that numerical methods designed to compute
the stationary distribution of discrete-time Markov chains may be used to compute the stationary
distributions of continuous-time Markov chains, and vice versa.
Example 10.1 In the previous chapter we saw that one very simple method for computing the
stationary distribution of a discrete-time Markov chain, is to successively compute the probability
distribution at each time step until no further change in the distribution is observed. We shall now
apply this method (which we shall later refer to as the power method) to the continous-time Markov
chain whose inﬁnitesimal generator is given by
Q =
⎛
⎜
⎜
⎝
−4
4
0
0
3
−6
3
0
0
2
−4
2
0
0
1
−1
⎞
⎟
⎟
⎠.
Setting 
t = 1/6, we obtain
Q
t + I =
⎛
⎜
⎜
⎝
1 −4/6
4/6
0
0
3/6
1 −6/6
3/6
0
0
2/6
1 −4/6
2/6
0
0
1/6
1 −1/6
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
1/3
2/3
0
0
1/2
0
1/2
0
0
1/3
1/3
1/3
0
0
1/6
5/6
⎞
⎟
⎟
⎠.
Beginning with π(0) = (1, 0, 0, 0), and successively computing π(n) = π(n−1)(Q
t + I) for
n = 1, 2, . . . , we obtain
π(0) = (1.0000
0.0000
0.0000
0.0000),
π(1) = (0.3333
0.6667
0.0000
0.0000),
π(2) = (0.4444
0.2222
0.3333
0.0000),
π(3) = (0.2593
0.4074
0.2222
0.1111),
...
...
π(10) = (0.1579
0.1929
0.2493
0.3999),
...
...
π(25) = (0.1213
0.1612
0.2403
0.4773),
...
...
π(50) = (0.1200
0.1600
0.2400
0.4800),
and, correct to four decimal places, no further change is observed in computing the distribution
at higher step values. Thus we may take (0.1200, 0.1600, 0.2400, 0.4800) to be the stationary
distribution of the continuous-time Markov chain represented by the inﬁnitesimal generator Q.
Thus, depending on how we formulate the problem, we may obtain the stationary distribution
of a Markov chain (discrete or continuous time) by solving either π P = π or π Q = 0.
1 We caution the reader whose interest is in transient solutions (i.e., probability distributions at an arbitrary time t) that
those of the discretized chain, represented by the transition probability matrix Q
t + I, are not the same as those of the
continuous-time chain, represented by the inﬁnitesimal generator Q. However, both have the same stationary distribution.

10.1 Introduction
287
From the perspective of a numerical analyst, these are two different problems. The ﬁrst is
an eigenvalue/eigenvector problem in which the stationary solution vector π is the left-hand
eigenvector corresponding to a unit eigenvalue of the transition probability matrix P. The second is a
linear equation problem in which the desired vector π is obtained by solving a homogeneous (right-
hand side identically equal to zero) system of linear equations with singular coefﬁcient matrix Q. In
light of this, it behooves us to review some eigenvalue/eigenvector properties of stochastic matrices
and inﬁnitesimal generators.
10.1.2 Stochastic Matrices
A matrix P ∈ℜn×n is said to be a stochastic matrix if it satisﬁes the following three conditions:
1. pi j ≥0 for all i and j.
2. 
all j pi j = 1 for all i.
3. At least one element in each column differs from zero.
Matrices that obey condition 1 are called nonnegative matrices, and stochastic matrices form a
proper subset of them. Condition 2 implies that a transition is guaranteed to occur from state i to
at least one state in the next time period (which may be state i again). Condition 3 speciﬁes that,
since each column has at least one nonzero element, there are no ephemeral states, i.e., states that
could not possibly exist after the ﬁrst time transition. In much of the literature on stochastic matrices
this third condition is omitted (being considered trivial). In the remainder of this text we also shall
omit this condition. We shall now list some important properties concerning the eigenvalues and
eigenvectors of stochastic matrices.
Property 10.1.1 Every stochastic matrix has an eigenvalue equal to 1.
Proof: Since the sum of the elements of each row of P is 1, we must have
Pe = e,
where e = (1, 1, . . . , 1)T . It immediately follows that P has a unit eigenvalue.
Corollary 10.1.1 Every inﬁnitesimal generator Q has a zero eigenvalue.
Proof: This follows immediately from the fact that Qe = 0.
Property 10.1.2 The eigenvalues of a stochastic matrix must have modulus less than or equal to 1.
Proof: To prove this result we shall use the fact that for any matrix A,
ρ(A) ≤∥A∥∞= max
j

all k
|a jk|

,
where ρ(A) denotes the spectral radius (magnitude of the largest eigenvalue) of A. The spectrum
of A is the set of eigenvalues of A. For a stochastic matrix P, ∥P∥∞= 1, and therefore we may
conclude that
ρ(P) ≤1.
Hence, no eigenvalue of a stochastic matrix P can exceed 1 in modulus.
Notice that this property, together with Property 10.1.1, implies that the spectral radius of a
stochastic matrix is 1, i.e.,
ρ(P) = 1.
Property 10.1.3 The right-hand eigenvector corresponding to a unit eigenvalue λ1 = 1 of a
stochastic matrix P is given by e where e = (1, 1, . . .)T .

288
Numerical Solution of Markov Chains
Proof: Since the sum of each row of P is 1, we have Pe = e = λ1e.
Property 10.1.4 The vector π is a stationary probability vector of a stochastic matrix P iff it is a
left-hand eigenvector corresponding to a unit eigenvalue.
Proof: By deﬁnition, a stationary probability vector π, does not change when post-multiplied by a
stochastic transition probability matrix P, which implies that
π = π P.
Therefore π satisﬁes the eigenvalue equation
π P = λ1π
for λ1 = 1.
The converse is equally obvious.
The following additional properties apply when P is the stochastic matrix of an irreducible
Markov chain.
Property 10.1.5
The stochastic matrix of an irreducible Markov chain possesses a simple unit
eigenvalue.
The theorem of Perron and Frobenius is a powerful theorem which has applicability to irreducible
Markov chains [50]. Property 10.1.5 follows directly from the Perron–Frobenius theorem.
Property 10.1.6 For any irreducible Markov chain with stochastic transition probability matrix P,
let
P(α) = I −α(I −P),
(10.4)
where α ∈ℜ′ ≡(−∞, ∞)\{0} (i.e., the real line with zero deleted). Then 1 is a simple eigenvalue
of every P(α), and associated with this unit eigenvalue is a uniquely deﬁned positive left-hand
eigenvector of unit 1-norm, which is precisely the stationary probability vector π of P.
Proof: Notice that the spectrum of P(α) is given by λ(α) = 1 −α(1 −λ) for λ in the spectrum of
P. Furthermore, as can be veriﬁed by substituting from Equation (10.4), the left-hand eigenvectors
of P and P(α) agree in the sense that
xT P = λxT
if and only if
xT P(α) = λ(α)xT
for all α.
In particular, this means that regardless of whether or not P(α) is a stochastic matrix, λ(α) = 1 is
a simple eigenvalue of P(α) for all α, because λ = 1 is a simple eigenvalue of P. Consequently,
the entire family P(α) has a unique positive left-hand eigenvector of unit 1-norm associated with
λ(α) = 1, and this eigenvector is precisely the stationary distribution π of P.
Example 10.2 Consider the 3 × 3 stochastic matrix given by
P =
⎛
⎝
.99911
.00079
.00010
.00061
.99929
.00010
.00006
.00004
.99990
⎞
⎠.
(10.5)
Its eigenvalues are 1.0, .9998, and .9985. The maximum value of α that we can choose so that P(α)
is stochastic is α1 = 1/.00089. The resulting stochastic matrix that we obtain is given by
P(α1) =
⎛
⎝
0
.88764
.11236
.68539
.20225
.11236
.06742
.04494
.88764
⎞
⎠,

10.1 Introduction
289
and its eigenvalues are 1.0, .77528, and −.68539. Yet another choice, α2 = 10,000, yields a P(α2)
that is not stochastic. We ﬁnd
P(α2) =
⎛
⎝
−7.9
7.9
1.0
6.1
−6.1
1.0
0.6
0.4
0.0
⎞
⎠.
Its eigenvalues are 1.0, −1.0, and −14.0. The left-hand eigenvector corresponding to the unit
eigenvalue for all three matrices is
π = (.22333, .27667, .50000)
—the stationary probability vector of the original stochastic matrix.
This theorem has important consequences for the convergence of certain iterative methods used
to ﬁnd stationary distributions of discrete- and continuous-time Markov chains. As we shall see later,
the rate of convergence of these methods depends upon the separation of the largest (in modulus)
and second largest eigenvalues. This theorem may be used to induce a larger separation, as can be
seen in Example 10.5 in moving from P to P(α1), with a resulting faster convergence rate.
10.1.3 The Effect of Discretization
Let us now ﬁnd the values of 
t > 0 for which the matrix P = Q
t + I is stochastic.
Example 10.3 Consider a two-state Markov chain whose inﬁnitesimal generator is
Q =
−q1
q1
q2
−q2

,
with q1, q2 ≥0. The transition probability matrix is then
P = Q
t + I =
1 −q1
t
q1
t
q2
t
1 −q2
t

.
Obviously the row sums are equal to 1. To ensure that 0 ≤q1
t ≤1 and 0 ≤q2
t ≤1, we require
that 0 ≤
t ≤q−1
1
and 0 ≤
t ≤q−1
2 . Let us assume, without loss of generality, that q1 ≥q2. Then
0 ≤
t ≤q−1
1
satisﬁes both conditions. To ensure that 0 ≤1 −q1
t ≤1 and 0 ≤1 −q2
t ≤1
again requires that 
t ≤q−1
1 . Consequently the maximum value that we can assign to 
t subject
to the condition that P be stochastic is 
t = 1/ maxi |qi|.
Similar results hold for a general stochastic matrix P = Q
t + I to be stochastic, given that Q
is an inﬁnitesimal generator. As before, for any value of 
t, the row sums of P are unity, since by
deﬁnition the row sums of Q are zero. Therefore we must concern ourselves with the values of 
t
that guarantee the elements of P lie in the interval [0,1]. Let q be the size of the largest off-diagonal
element:
q = max
i, j i̸= j(qi j)
and
qi j ≥0
for all i, j.
Then 0 ≤pi j ≤1 holds if 0 ≤qi j
t ≤1, which is true if 
t ≤q−1. Now consider a diagonal
element pii = qii
t + 1. We have
0 ≤qii
t + 1 ≤1
or
−1 ≤qii
t ≤0.
The right-hand inequality holds for all 
t ≥0, since qii is negative. The left-hand inequality
qii
t ≥−1 is true if 
t ≤−q−1
ii , i.e., if 
t ≤|qii|−1. It follows then that, if 0 ≤
t ≤
(maxi |qii|)−1, the matrix P is stochastic. (Since the diagonal elements of Q equal the negated

290
Numerical Solution of Markov Chains
sum of the off-diagonal elements in a row, we have maxi |qii| ≥maxi̸= j(qi j).) Thus, 
t must be
less than or equal to the reciprocal of the absolute value of the largest diagonal element of Q.
The choice of a suitable value for 
t plays a crucial role in some iterative methods for
determining the stationary probability vector from Equation (10.3). As we mentioned above, the
rate of convergence is intimately related to the magnitude of the eigenvalues of P. As a general rule,
the closer the magnitudes of the subdominant eigenvalues are to 1, the slower the convergence rate.
We would therefore like to maximize the distance between the largest eigenvalue, λ1 = 1, and the
subdominant eigenvalue (the eigenvalue that in modulus is closest to 1). Notice that, as 
t →0, the
eigenvalues of P all tend to unity. This would suggest that we choose 
t to be as large as possible,
subject only to the constraint that P be a stochastic matrix. However, choosing 
t = (maxi |qii|)−1
does not necessarily guarantee the maximum separation of dominant and subdominant eigenvalues.
It is simply a good heuristic.
Example 10.4 Consider the (2 × 2) case as an example. The eigenvalues of P are the roots of the
characteristic equation |P −λI| = 0, i.e.,

1 −q1
t −λ
q1
t
q2
t
1 −q2
t −λ
 = 0.
These roots are λ1 = 1 and λ2 = 1 −
t(q1 + q2). As 
t →0, λ2 →λ1 = 1. Also notice that the
left-hand eigenvector corresponding to the unit eigenvalue λ1 is independent of the choice of 
t.
We have
(q2/(q1 + q2) q1/(q1 + q2))

1 −q1
t
q1
t
q2
t
1 −q2
t

= (q2/(q1 + q2) q1/(q1 + q2)) .
This eigenvector is, of course, the stationary probability vector of the Markov chain and as such
must be independent of 
t. The parameter 
t can only affect the speed at which matrix iterative
methods converge to this vector. As we mentioned above, it is often advantageous to choose 
t
to be as large as possible, subject only to the constraint that the matrix P be a stochastic matrix.
Intuitively, we may think that by choosing a large value of 
t we are marching more quickly toward
the stationary distribution. With small values we essentially take only small steps, and therefore it
takes longer to arrive at our destination, the stationary distribution.
10.2 Direct Methods for Stationary Distributions
10.2.1 Iterative versus Direct Solution Methods
There are two basic types of solution method in the ﬁeld of numerical analysis: solution methods
that are iterative and solution methods that are direct. Iterative methods begin with an initial
approximation to the solution vector and proceed to modify this approximation in such a way that,
at each step or iteration, it becomes closer and closer to the true solution. Eventually, it becomes
equal to the true solution. At least that is what we hope. If no initial approximation is known, then
a guess is made or an arbitrary initial vector is chosen instead. Sometimes iterative methods fail
to converge to the solution. On the other hand, a direct method attempts to go straight to the ﬁnal
solution. A certain number of well deﬁned steps must be taken, at the end of which the solution has
been computed. However, all is not that rosy, for sometimes the number of steps that must be taken
is prohibitively large and the buildup of rounding error can, in certain cases, be substantial.
Iterative methods of one type or another are by far the most commonly used methods for
obtaining the stationary probability vector from either the stochastic transition probability matrix
or from the inﬁnitesimal generator. There are several important reasons for this choice. First,
an examination of the standard iterative methods shows that the only operation in which the
matrices are involved, is their multiplication with one or more vectors—an operation which leaves

10.2 Direct Methods for Stationary Distributions
291
the transition matrices unaltered. Thus compact storage schemes, which minimize the amount of
memory required to store the matrix and which in addition are well suited to matrix multiplication,
may be conveniently implemented. Since the matrices involved are usually large and very sparse,
the savings made by such schemes can be considerable. With direct equation solving methods, the
elimination of one nonzero element of the matrix during the reduction phase often results in the
creation of several nonzero elements in positions which previously contained zero. This is called
ﬁll-in and not only does it make the organization of a compact storage scheme more difﬁcult, since
provision must be made for the deletion and the insertion of elements, but in addition, the amount of
ﬁll-in can often be so extensive that available memory can be exhausted. A successful direct method
must incorporate a means of overcoming these difﬁculties.
Iterative methods have other advantages. Use may be made of good initial approximations to
the solution vector and this is especially beneﬁcial when a series of related experiments is being
conducted. In such circumstances the parameters of one experiment often differ only slightly from
those of the previous; many will remain unchanged. Consequently, it is to be expected that the
solution to the new experiment will be close to that of the previous and it is advantageous to use the
previous result as the new initial approximation. If indeed there is little change, we should expect to
compute the new result in relatively few iterations.
Also, an iterative process may be halted once a prespeciﬁed tolerance criterion has been satisﬁed,
and this may be relatively lax. For example, it may be wasteful to compute the solution of a
mathematical model correct to full machine precision when the model itself contains errors of the
order of 5–10%. In contrast, a direct method must continue until the ﬁnal speciﬁed operation has
been carried out. And lastly, with iterative methods, the matrix is never altered and hence the buildup
of rounding error is, to all intents and purposes, nonexistent.
For these reasons, iterative methods have traditionally been preferred to direct methods. However,
iterative methods have a major disadvantage in that often they require a very long time to converge to
the desired solution. Direct methods have the advantage that an upper bound on the time required to
obtain the solution may be determined before the calculation is initiated. More important, for certain
classes of problems, direct methods can result in a much more accurate answer being obtained in
less time. Since iterative method will in general require less memory than direct methods, these
latter can only be recommended if they obtain the solution in less time. Unfortunately, it is often
difﬁcult to predict when a direct solver will be more efﬁcient than an iterative solver.
In this section we consider direct methods for computing the stationary distribution of Markov
chains while in the next section we consider basic iterative methods. Some methods, such as
preconditioned projection methods may be thought of as a combination of the direct and iterative
approaches, but their study is beyond the scope of this text. Readers wishing further information on
projection methods should consult one of the standard texts, such as [47, 50].
10.2.2 Gaussian Elimination and LU Factorizations
Direct equation solving methods for obtaining the stationary distribution of Markov chains are
applied to the system of equations
π Q = 0, π ≥0, πe = 1,
(10.6)
i.e., a homogeneous system of n linear equations in which the n unknowns πi, i = 1, 2, . . . n are
the components of the stationary distribution vector, π. The vector e is a column vector whose
elements are all equal to 1. The system of equations (10.6) has a solution other than the trivial
solution (πi = 0, for all i) if and only if the determinant of the coefﬁcient matrix is zero, i.e.,
if and only if the coefﬁcient matrix is singular. Since the determinant of a matrix is equal to

292
Numerical Solution of Markov Chains
the product of its eigenvalues and since Q possesses a zero eigenvalue, the singularity of Q and
hence the existence of a non-trivial solution, follows. The standard direct approaches for solving
systems of linear equations are based on the method of Gaussian elimination (GE) and related LU
factorizations. Gaussian elimination is composed of two phases, a reduction phase during which the
coefﬁcient matrix is brought to upper triangular form, and a backsubstitution phase which generates
the solution from the reduced coefﬁcient matrix. The ﬁrst (reduction) phase is the computationally
expensive part of the algorithm, having an O(n3) operation count when the matrix is full. The
backsubstitution phase has order O(n2). A detailed discussion of Gaussian elimination and LU
factorizations can be found in Appendix B. Readers unfamiliar with the implementation of these
methods should consult this appendix before proceeding.
Gaussian Elimination for Markov Chains
Consider the Gaussian elimination approach for computing the stationary distribution of a ﬁnite,
irreducible continuous-time Markov chain. In this case the system of equations, π Q = 0, is
homogeneous, the coefﬁcient matrix is singular and there exists a unique solution vector π. We
proceed by means of an example.
Example 10.5 Suppose we seek to solve
(π1, π2, π3, π4)
⎛
⎜
⎜
⎝
−4.0
1.0
2.0
1.0
4.0
−9.0
2.0
3.0
0.0
1.0
−3.0
2.0
0.0
0.0
5.0
−5.0
⎞
⎟
⎟
⎠= (0, 0, 0, 0).
Although not strictly necessary, we begin by transposing both sides of this equation, thereby putting
it into the standard textbook form, Ax = b, for implementing Gaussian elimination. We get
Ax =
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠= b.
We now proceed to perform the reduction phase of Gaussian elimination. The ﬁrst step is to use
the ﬁrst equation to eliminate the nonzero elements in column 1 that lie below the pivot element,
a11 = −4. This is accomplished by adding multiples of row 1 into rows 2 through n = 4. Once this
has been done, the next step is to eliminate nonzero elements in column 2 that lie below the pivot
element in row 2 (the newly modiﬁed a′
22 = −8) by adding multiples of row 2 into rows 3 through
n = 4. And so the reduction phase of Gaussian elimination continues until all nonzero elements
below the diagonal have been eliminated—at which point the reduction phase of the algorithm has
been completed. Following the notation introduced in Appendix B, we obtain the reduction phase:
Multipliers
0.25
0.50
0.25

⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠=⇒
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0 −8.0
1.0
0.0
0.0
4.0
−3.0
5.0
0.0
4.0
2.0
−5.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠,

10.2 Direct Methods for Stationary Distributions
293
Multipliers
0.50
0.50

⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0 −8.0
1.0
0.0
0.0
4.0 −3.0
5.0
0.0
4.0
2.0 −5.0
⎞
⎟
⎟
⎠=⇒
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0 −2.5
5.0
0.0
0.0
2.5
−5.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠,
Multipliers
1.0

⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0 −2.5
5.0
0.0
0.0
2.5
−5.0
⎞
⎟
⎟
⎠=⇒
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠.
At the end of these three steps, the coefﬁcient matrix has the following upper triangular form;
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠.
Observe that, since the pivotal elements are the largest in each column, the multipliers do not exceed
1. Explicit pivoting which is used in general systems of linear equation to ensure that multipliers do
not exceed 1, is generally not needed for solving Markov chain problems, since the elements along
the diagonal are the largest in each column and this property is maintained during the reduction
phase.
We make one additional remark concerning the reduction process. In some cases it can be useful
to keep the multipliers for future reference. Since elements below the diagonal are set to zero during
the reduction, these array positions provide a suitable location into which the multipliers may be
stored. In this case, the coefﬁcient matrix ends up as
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.25
−8.0
1.0
0.0
0.50
0.5
−2.5
5.0
0.25
0.5
1.0
0.0
⎞
⎟
⎟
⎠
but only the part on and above the diagonal participates in the backsubstitution phase.
We now turn to the backsubstitution phase wherein π4 is found from the last equation of the
reduced system, π3 found from the second last equation and so on until π1 is obtained from the ﬁrst
equation. However, the last row contains all zeros (we ignore all multipliers that may have been
stored during the reduction step) and does not allow us to compute the value of π4. This should not
be unexpected, because the coefﬁcient matrix is singular and we can only compute the solution to
a multiplicative constant. One possibility is to assign the value 1 to π4 and to compute the value of
the other πi in terms of this value. Doing so, we obtain
Equation 4:
π4 = 1,
Equation 3:
−2.5 π3 + 5 × 1 = 0
=⇒
π3 = 2,
Equation 2:
−8 π2 + 1 × 2 = 0
=⇒
π2 = 0.25,
Equation 1:
−4 π1 + 4 × 0.25 = 0
=⇒
π1 = 0.25.
Therefore our computed solution is (1/4, 1/4, 2, 1). However, this is not the stationary distribution
vector just yet, since the sum of its elements does not add to 1. The stationary distribution vector is
obtained after normalization, i.e., once we divide each element of this vector by the sum of all its
components. Since all the components are positive, this is the same as dividing each element by the
1-norm of the vector. In our example, we have
∥π∥1 = |π1| + |π2| + |π3| + |π4| = 3.5,

294
Numerical Solution of Markov Chains
and the stationary distribution vector, the normalized computed solution, is therefore
π = 2
7 (1/4, 1/4, 2, 1) .
Another way to look on this situation is to observe that the system of equations π Q = 0
does not tell the whole story. We also know that πe = 1. The n equations of π Q = 0 provide
only n −1 linearly independent equations, but together with πe = 1, we have a complete basis
set. For example, it is possible to replace the last equation of the original system with πe = 1
which eliminates the need for any further normalization. In this case the coefﬁcient matrix becomes
nonsingular, the right-hand side becomes nonzero and a unique solution, the stationary probability
distribution, is computed. Of course, it is not necessary to replace the last equation of the system
by this normalization equation. Indeed, any equation could be replaced. However, this is generally
undesirable, for it will entail more numerical computation. For example, if the ﬁrst equation is
replaced, the ﬁrst row of the coefﬁcient matrix will contain all ones and the right-hand side will
be e1 = (1, 0, . . . , 0)T . The ﬁrst consequence of this is that during the reduction phase, the
entire sequence of elementary row operations must be performed on the right-hand side vector,
e1, whereas if the last equation is replaced, the right-hand side is unaffected by the elementary
row operations. The second and more damaging consequence is that substantial ﬁll-in, the situation
in which elements of the coefﬁcient matrix that were previously zero become nonzero, will occur
since a multiple of the ﬁrst row, which contains all ones, will be added into higher numbered rows
and a cascading effect will undoubtedly occur in all subsequent reduction steps. An equally viable
alternative to replacing the last equation with πe = 1 is to set the last component πn to one,
perform the back substitution and then to normalize the solution thus computed, just as we did
in Example 10.5.
To summarize, Gaussian elimination for solving the balance equations arising from ﬁnite,
irreducible, continuous-time Markov chains, consists of the following three-step algorithm, where
we have taken A = QT (i.e., the element ai j in the algorithm below is the rate of transition from
state j into state i).
Algorithm 10.1: Gaussian Elimination for Continuous-Time Markov Chains
1. The reduction step:
For i = 1, 2, . . . , n −1 :
a ji = −a ji/aii
for all j > i
% multiplier for row j
a jk = a jk + a jiaik for all j, k > i
% reduce using row i
2. The backsubstitution step:
xn = 1
% set last component equal to 1
For i = n −1, n −2, . . . , 1 :
xi = −
⎡
⎣
n

j=i+1
ai jx j
⎤
⎦/aii
% backsubstitute to get xi
3. The ﬁnal normalization step:
norm = n
j=1 x j
% sum components
For i = 1, 2, . . . , n :
πi = xi/ norm
% component i of stationary probability vector

10.2 Direct Methods for Stationary Distributions
295
An alternative to the straightforward application of Gaussian elimination to Markov chains as
described above, is the scaled Gaussian elimination algorithm. In this approach, each equation of
the system QT π T = 0 is scaled so that the element on each diagonal is equal to −1. This is
accomplished by dividing each element of a row by the absolute value of its diagonal element,
which is just the same as dividing both left and right sides of each equation by the same value.
Example 10.6 Observe that the two sets of equations
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠
and
⎛
⎜
⎜
⎝
−1
1
0
0
1/9
−1
1/9
0
2/3
2/3
−1
5/3
1/5
3/5
2/5
−1
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠
are identical from an equation solving point of view.
This has the effect of simplifying the backsubstitution phase. It does not signiﬁcantly decrease the
the number of numerical operations that must be performed, but rather facilitates programming the
Gaussian elimination method for Markov chains. The previous three-step algorithm now becomes:
Algorithm 10.2: Scaled Gaussian Elimination for Continuous-Time Markov Chains
1. The reduction step:
For i = 1, 2, . . . , n −1:
aik = −aik/aii
for all k > i
% scale row i
a jk = a jk + a jiaik for all j, k > i
% reduce using row i
2. The backsubstitution step:
xn = 1
% set last component equal to1
For i = n −1, n −2, . . . , 1:
xi = n
j=i+1 ai jx j
% backsubstitute to get xi
3. The ﬁnal normalization step:
norm = n
j=1 x j
% sum components
For i = 1, 2, . . . , n :
πi = xi/ norm
% component iof stationary probability vector
Observe that during the reduction step no attempt is made actually to set the diagonal elements to
−1: it sufﬁces to realize that this must be the case. Nor is any element below the diagonal set to
zero, for exactly the same reason. If the multipliers are to be kept, then they may overwrite these

296
Numerical Solution of Markov Chains
subdiagonal positions. At the end of the reduction step, the elements strictly above the diagonal of
A contain that portion of the upper triangular matrix U needed for the backsubstitution step.
Example 10.7 Beginning with
A =
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠,
the matrices obtained for each different value of i = 1, 2, 3 during the reduction phase are
A1 =
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−8.0
1.0
0.0
2.0
4.0
−3.0
5.0
1.0
4.0
2.0
−5.0
⎞
⎟
⎟
⎠,
A2 =
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−8.0
0.125
0.0
2.0
4.0
−2.5
5.0
1.0
4.0
2.5
−5.0
⎞
⎟
⎟
⎠,
A3 =
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−8.0
0.125
0.0
2.0
4.0
−2.5
2.0
1.0
4.0
2.5
0.0
⎞
⎟
⎟
⎠.
In this last matrix, only the elements above the diagonal contribute to the backsubstitution step.
Given that the diagonal elements are taken equal to −1, this means that the appropriate upper
triangular matrix is
U =
⎛
⎜
⎜
⎝
−1.0
1.0
0.0
0.0
0.0
−1.0
0.125
0.0
0.0
0.0
−1.0
2.0
0.0
0.0
0.0
0.0
⎞
⎟
⎟
⎠.
The backsubstitution step, beginning with x4 = 1, successively gives x3 = 2 × 1 = 2, x2 =
0.125×2 = 0.25 and x1 = 1.0×0.25 = 0.25 which when normalized gives exactly the same result
as before.
LU Factorizations for Markov Chains
When the coefﬁcient matrix in a system of linear equations Ax = b can be written as the product of
a lower triangular matrix L and an upper triangular matrix U, then
Ax = LUx = b
and the solution can be found by ﬁrst solving (by forward substitution) Lz = b for an intermediate
vector z and then solving (by backward substitution) Ux = z for the solution x. The upper triangular
matrix obtained by the reduction phase of Gaussian elimination provides an upper triangular matrix
U to go along with a lower triangular matrix L whose diagonal elements are all equal to 1 and
whose subdiagonal elements are the multipliers with a minus sign in front of them. In the Markov
chain context, the system of equations is homogeneous and the coefﬁcient matrix is singular,
QT x = (LU)x = 0.
If we now set Ux = z and attempt to solve Lz = 0, we ﬁnd that, since L is nonsingular (it
is a triangular matrix whose diagonal elements are all equal to 1) we must have z = 0. This
means that we may proceed directly to the back substitution on Ux = z = 0 with unn = 0. It
is evident that we may assign any nonzero value to xn, say xn = η, and then determine, by simple
backsubstitution, the remaining elements of the vector x in terms of η. We have xi = ciη for some
constants ci, i = 1, 2, ..., n, and cn = 1. Thus the solution obtained depends on the value of η.

10.2 Direct Methods for Stationary Distributions
297
There still remains one equation that the elements of a probability vector must satisfy, namely that
the sum of the probabilities must be 1. Normalizing the solution obtained from solving Ux = 0
yields the desired unique stationary probability vector π.
Example 10.8 With the matrix QT of Example 10.5, given by
QT =
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠,
we ﬁnd the following LU decomposition:
L =
⎛
⎜
⎜
⎝
1.00
0.0
0.0
0.0
−0.25
1.0
0.0
0.0
−0.50
−0.5
1.0
0.0
−0.25
−0.5
−1.0
1.0
⎞
⎟
⎟
⎠,
U =
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.0
⎞
⎟
⎟
⎠,
whose product LU is equal to QT . The upper triangular matrix U is just the reduced matrix
given by Gaussian elimination while the lower triangular matrix L is the identity matrix with the
negated multipliers placed below the diagonal. Forward substitution on Lz = 0 successively gives
z1 = 0, z2 = 0, z3 = 0, and z4 = 0. Since we know in advance that this must be the case, there
is no real need to carry out these operations, but instead we may go straight to the backsubstitution
stage to compute x from Ux = z = 0. The sequence of operations is identical to that of the
backsubstitution phase of Example 10.5.
The above discussion concerns ﬁnite, irreducible Markov chains. If the Markov chain is
decomposable into k irreducible closed communicating classes, these k irreducible components may
be solved independently as separate irreducible Markov chains. If Gaussian elimination is applied to
the entire transition rate matrix of a Markov chain with k > 1 separable components, the reduction
phase will lead to a situation in which there are k rows whose elements are all equal to zero. This
is because such a matrix possesses k eigenvalues all equal to zero; only n −k of the equations are
linearly independent. Although our algorithms can be adjusted to take this situation into account, it
is much easier to treat each of the k components separately.
An issue of concern in the implementation of direct methods is that of the data structure used to
hold the coefﬁcient matrix. Frequently the matrices generated from Markov models are too large to
permit regular two-dimensional arrays to be used to store them in computer memory. Since these
matrices are usually very sparse, it is economical, and indeed necessary, to use some sort of packing
scheme whereby only the nonzero elements and their positions in the matrix are stored. The most
suitable candidates for solution by direct methods are Markov chains whose transition matrix is
small, of the order of a few hundred, or when it is banded, i.e., the only nonzero elements of the
coefﬁcient matrix are not too far from the diagonal. In this latter case, it means that an ordering can
be imposed on the states such that no single step transition from state i will take it to states numbered
greater than i + δ or less than i −δ. All ﬁll-in will occur within a distance δ of the diagonal, and the
amount of computation per step is proportional to δ2.
Direct methods are generally not recommended when the transition matrix is large and not
banded, due to the amount of ﬁll-in that can quickly overwhelm available storage capacity. When
the coefﬁcient matrix is generated row by row the following approach allows matrices of the order
of several thousand to be handled by a direct method. The ﬁrst row is generated and stored in a
compacted form, i.e., only the nonzero element and its position in the row is stored. Immediately
after the second row has been obtained, it is possible to eliminate the element in position (2,1) by
adding a multiple of the ﬁrst row to it. Row 2 may now be compacted and stored. This process
may be continued so that when the i-th row of the coefﬁcient matrix is generated, rows 1 through

298
Numerical Solution of Markov Chains
(i −1) have been derived, reduced to upper triangular form, compacted and stored. The ﬁrst (i −1)
rows may therefore be used to eliminate all nonzero elements in row i from column positions (i, 1)
through (i, i −1), thus putting it into the desired triangular form. Note that since this reduction is
performed on QT , it is the columns of the inﬁnitesimal generator that are required to be generated
one at a time and not its rows.
This method has a distinct advantage in that once a row has been generated in this fashion,
no more ﬁll-in will occur into this row. It is suggested that a separate storage area be reserved to
hold temporarily a single unreduced row. The reduction is performed in this storage area. Once
completed, the reduced row may be compacted into any convenient form and appended to the rows
which have already been reduced. In this way no storage space is wasted holding subdiagonal
elements which, due to elimination, have become zero, nor in reserving space for the inclusion
of additional elements. The storage scheme should be chosen bearing in mind the fact that these
rows will be used in the reduction of further rows and also later in the algorithm during the back-
substitution phase.
This approach cannot be used for solving general systems of linear equations because it inhibits a
pivoting strategy from being implemented. It is valid when solving irreducible Markov chains since
pivoting is not required in order for the reduction phase to be performed in a stable manner.
The Grassmann–Taksar–Heyman Advantage
It is appropriate at this point to mention a version of Gaussian elimination that has attributes that
appear to make it even more stable than the usual version. This procedure is commonly referred to as
the GTH (Grassmann–Taksar–Heyman) algorithm. In GTH the diagonal elements are obtained by
summing off-diagonal elements rather than performing a subtraction: it is known that subtractions
can sometimes lead to loss of signiﬁcance in numerical computations. These subtractions occur in
forming the diagonal elements during the reduction process. Happily, it turns out that at the end of
each reduction step, the unreduced portion of the matrix is the transpose of a transition rate matrix
in its own right. This has a probabilistic interpretation based on the restriction of the Markov chain
to a reduced set of states and it is in this context that the algorithm is generally developed. It also
means that the diagonal elements may be formed by adding off-diagonal elements and placing a
minus sign in front of this sum instead of performing a single subtraction. When in addition, the
concept of scaling is also introduced into the GTH algorithm the need to actually form the diagonal
elements disappears (all are taken to be equal to −1). In this case, the scale factor is obtained by
taking the reciprocal of the sum of off-diagonal elements.
Example 10.9 Let us return to Example 10.7 and the matrices obtained at each step of Gaussian
elimination with scaling.
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0 −8.0
1.0
0.0
2.0
4.0
−3.0
5.0
1.0
4.0
2.0
−5.0
⎞
⎟
⎟
⎠
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−8.0
0.125
0.0
2.0
4.0 −2.5
5.0
1.0
4.0
2.5
−5.0
⎞
⎟
⎟
⎠
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−8.0
0.125 0.0
2.0
4.0
−2.5
2.0
1.0
4.0
2.5
0.0
⎞
⎟
⎟
⎠.
Observe that the submatrix contained in each lower right-hand block is the transpose of a transition
rate matrix with one less state than its predecessor. Consider the ﬁrst reduction step, whereby
elements in positions a21, a31, and a41 are to be eliminated. Off-diagonal elements in row 1 are
scaled by dividing each by the sum a21 + a31 + a41 = 1 + 2 + 1, although for this ﬁrst row, it

10.2 Direct Methods for Stationary Distributions
299
is just as easy to use the absolute value of the diagonal element. After the ﬁrst row is scaled, it
must be added into the second row to eliminate the a21 element. While this should result in the
subtraction −9 + 1 into position a22, we choose instead to ignore this particular operation and leave
the a22 element equal to −9. Throughout the entire reduction process, the diagonal elements are
left unaltered. Once the elements a21, a31 and a41 have been eliminated, we are ready to start the
second reduction step, which begins by forming the next scale factor as a32 + a42 = 4 + 4, the sum
of elements below a22. The process continues in this fashion, ﬁrst computing the scale factor by
summing below the current diagonal element, then scaling all elements to the right of the current
diagonal element and and ﬁnally by carrying out the reduction step. These steps are shown below.
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠
scale
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
1.0
3.0
2.0
−5.0
⎞
⎟
⎟
⎠
reduce
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0 −9.0
1.0
0.0
2.0
4.0
−3.0
5.0
1.0
4.0
2.0
−5.0
⎞
⎟
⎟
⎠
scale
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0 −9.0
0.125
0.0
2.0
4.0
−3.0
5.0
1.0
4.0
2.0
−5.0
⎞
⎟
⎟
⎠
reduce
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−9.0
0.125
0.0
2.0
4.0 −3.0
5.0
1.0
4.0
2.5
−5.0
⎞
⎟
⎟
⎠
scale
−→
⎛
⎜
⎜
⎝
−4.0
1.0
0.0
0.0
1.0
−9.0
0.125
0.0
2.0
4.0 −3.0
2.0
1.0
4.0
2.5
−5.0
⎞
⎟
⎟
⎠.
Algorithm 10.3: GTH for Continuous-Time Markov Chains with A = QT
1. The reduction step:
For i = 1, 2, . . . , n −1:
aik = aik/ n
j=i+1 a ji
for all k > i
% scale row i
a jk = a jk + a jiaik
for all j, k > i, k ̸= j
% reduce using row i
2. The backsubstitution step:
xn = 1
% set last component equal to1
For i = n −1, n −2, . . . , 1 :
xi = n
j=i+1 ai jx j
% backsubstitute to get xi
3. The ﬁnal normalization step:
norm = n
j=1 x j
%sum components
For i = 1, 2, . . . , n :
πi = xi/norm
% component i of stationary probability vector
Comparing this algorithm with the scaled Gaussian elimination algorithm, we see that only the
ﬁrst step has changed, and within that step only in the computation of the scale factor. The GTH
implementation requires more numerical operations than the standard implementation but this may
be offset by a gain in precision when the matrix Q is ill conditioned. The extra additions are not
very costly when compared with the overall cost of the elimination procedure, which leads to the
conclusion that the GTH advantage should be exploited where possible in elimination procedures.

300
Numerical Solution of Markov Chains
If the transition rate matrix is stored in a two-dimensional or band storage structure, access is easily
available to both the rows and columns of Q, and there is no difﬁculty in implementing GTH.
Unfortunately, the application of GTH is not quite so simple when the coefﬁcient matrix is stored
in certain compacted representations.
Matlab code for Gaussian Elimination
function [pi] = GE(Q)
A = Q’;
n = size(A);
for i=1:n-1
for j=i+1:n
A(j,i) = -A(j,i)/A(i,i);
end
for j=i+1:n
for k=i+1:n
A(j,k) = A(j,k) + A(j,i)*A(i,k);
end
end
end
x(n) = 1;
for i=n-1:-1:1
for j=i+1:n
x(i) = x(i) + A(i,j)*x(j);
end
x(i) = -x(i)/A(i,i);
end
pi = x/norm(x,1);
Matlab code for Scaled Gaussian Elimination
function [pi] = ScaledGE(Q)
A = Q’;
n = size(A);
for i=1:n-1
for k=i+1:n
A(i,k) = -A(i,k)/A(i,i);
end
for j=i+1:n
for k=i+1:n
A(j,k) = A(j,k) + A(j,i)*A(i,k);
end
end
end
x(n) = 1;
for i=n-1:-1:1

10.3 Basic Iterative Methods for Stationary Distributions
301
for j=i+1:n
x(i) = x(i) + A(i,j)*x(j);
end
end
pi = x/norm(x,1);
Matlab code for GTH
function [pi] = GTH(Q)
A = Q’;
n = size(A);
for i=1:n-1
scale = sum(A(i+1:n,i));
for k=i+1:n
A(i,k) = A(i,k)/scale;
end
for j=i+1:n
for k=i+1:n
A(j,k) = A(j,k) + A(j,i)*A(i,k);
end
end
end
x(n) = 1;
for i=n-1:-1:1
for j=i+1:n
x(i) = x(i) + A(i,j)*x(j);
end
end
pi = x/norm(x,1);
10.3 Basic Iterative Methods for Stationary Distributions
Iterative methods for solving systems of equations begin with some approximation to, or guess at,
the solution, and successively apply numerical operations designed to make this approximation
approach the true solution. The coefﬁcient matrix is not altered during the execution of the
algorithm which makes iterative methods well suited to compacted storage schemes. An item of
constant concern with iterative methods is their rate of convergence, the speed at which the initial
approximation approaches the solution.
10.3.1 The Power Method
Perhaps the approach that ﬁrst comes to mind when we need to ﬁnd the stationary distribution of an
ﬁnite, ergodic, discrete-time Markov chain, is to let the chain evolve over time, step by step, until
it reaches its stationary distribution. Once the probability vector no longer changes as the process
evolves from some step n to step n + 1, that vector can be taken as the stationary probability vector,
since at that point we have zP = z.

302
Numerical Solution of Markov Chains
Example 10.10 Consider a discrete-time Markov chain whose matrix of transition probabilities is
P =
⎛
⎝
.0
.8
.2
.0
.1
.9
.6
.0
.4
⎞
⎠.
(10.7)
If the system starts in state 1, the initial probability vector is given by
π(0) = (1, 0, 0).
Immediately after the ﬁrst transition, the system will be either in state 2, with probability .8, or in
state 3, with probability .2. The vector π(1), which denotes the probability distribution after one
transition (or one step) is thus
π(1) = (0, .8, .2).
Notice that this result may be obtained by forming the product π(0)P.
The probability of being in state 1 after two time steps is obtained by summing (over all i) the
probability of being in state i after one step, given by π(1)
i , multiplied by the probability of making
a transition from state i to state 1. We have
3

i=1
π(1)
i
pi1 = π(1)
1
× .0 + π(1)
2
× .0 + π(1)
3
× .6 = .12.
Likewise, the system will be in state 2 after two steps with probability .08 (= .0×.8+.8×.1+.2×.0),
and in state 3 with probability .8 (= .0 × .2 + .8 × .9 + .2 × .4). Thus, given that the system begins
in state 1, we have the following probability distribution after two steps:
π(2) = (.12, .08, .8).
Notice once again that π(2) may be obtained by forming the product π(1)P:
π(2) = (.12, .08, .8) = (0.0, 0.8, 0.2)
⎛
⎝
.0
.8
.2
.0
.1
.9
.6
.0
.4
⎞
⎠= π(1)P.
We may continue in this fashion, computing the probability distribution after each transition step.
For any integer k, the state of the system after k transitions is obtained by multiplying the probability
vector obtained after (k −1) transitions by P. Thus
π(k) = π(k−1)P = π(k−2)P2 = · · · = π(0)Pk.
At step k = 25, we ﬁnd the probability distribution to be
π = (.2813, .2500, .4688),
and thereafter, correct to four decimal places,
(.2813, .2500, .4688)
⎛
⎝
.0
.8
.2
.0
.1
.9
.6
.0
.4
⎞
⎠= (.2813, .2500, .4688),
which we may now take to be the stationary distribution (correct to four decimal places).
When the Markov chain is ﬁnite, aperiodic, and irreducible (as in Example 10.10), the vectors
π(k) converge to the stationary probability vector π regardless of the choice of initial vector. We
have
lim
k→∞π(k) = π.

10.3 Basic Iterative Methods for Stationary Distributions
303
This method of determining the stationary probability vector is referred to as the power method
or power iteration. The power method is well known in the context of determining the right-hand
eigenvector corresponding to a dominant eigenvalue of a matrix, A, and it is in this context that
we shall examine its convergence properties. However, recall that the stationary distribution of a
Markov chain is obtained from the left-hand eigenvector, so that in a Markov chain context, the
matrix A must be replaced with PT , the transpose of the transition probability matrix. Let A be a
square matrix of order n. The power method is described by the iterative procedure
z(k+1) = 1
ξk
Az(k),
(10.8)
where ξk is a normalizing factor, typically ξk = ∥Az(k)∥∞, and z(0) is an arbitrary starting vector.
Although this formulation of the power method incorporates a normalization at each iteration,
whereby each element of the newly formed iterate is divided by ξk, this is not strictly necessary.
Normalization may be performed less frequently.
To examine the rate of convergence of the power method, let A have eigensolution
Axi = λixi,
i = 1, 2, . . . , n,
and suppose that
|λ1| > |λ2| ≥|λ3| ≥· · · ≥|λn|.
Let us further assume that the initial vector may be written as a linear combination of the
eigenvectors of A, i.e.,
z(0) =
n

i=1
αixi.
The rate of convergence of the power method may then be determined from the relationship
z(k) = Akz(0) = Ak
n

i=1
αixi =
n

i=1
αi Akxi =
n

i=1
αiλk
i xi = λk
1
	
α1x1 +
n

i=2
αi
 λi
λ1
k
xi

. (10.9)
It may be observed that the process converges to the dominant eigenvector x1. The rate of
convergence depends on the ratios |λi|/|λ1| for i = 2, 3, . . . , n. The smaller these ratios, the
quicker the summation on the right-hand side tends to zero. It is, in particular, the magnitude of
the subdominant eigenvalue, λ2, that determines the convergence rate. The power method will not
perform satisfactorily when |λ2| ≈|λ1|. Obviously major difﬁculties arise when |λ2| = |λ1|.
Example 10.11 Returning to the 3×3 example given by Equation (10.7), the eigenvalues of P are
λ1 = 1 and λ2,3 = −.25 ± .5979i. Thus |λ2| ≈.65. Notice that .6510 ≈.01, .6525 ≈2 × 10−5,
and .65100 ≈2 × 10−19. Table 10.1 presents the probability distribution of the states of this
example at speciﬁc steps, for each of three different starting conﬁgurations. After 25 iterations, no
further changes are observed in the ﬁrst four digits for any of the starting conﬁgurations. The table
shows that approximately two decimal places of accuracy have been obtained after 10 iterations
and four places after 25 iterations, which coincides with convergence guidelines obtained from the
subdominant eigenvalue. Furthermore, after 100 iterations it is found that the solution is accurate to
full machine precision, again as suggested by the value of .65100.
We hasten to point out that the magnitude of |λ2|k does not guarantee a certain number of decimal
places of accuracy in the solution, as might be construed from the preceding example. As a general
rule, the number of decimal places accuracy is determined from a relative error norm; a relative
error norm of 10−j yielding approximately j decimal places of accuracy. However, some ﬂexibility
in the Markov chain context may be appropriate since both matrix and vectors have unit 1-norms.

304
Numerical Solution of Markov Chains
Table 10.1. Convergence in power method.
Step
Initial state
Initial state
Initial state
1.0
.0
.0
.0
1.0
.0
.0
.0
1.0
1
.0000
.8000
.2000
.0000
.1000
.9000
.6000
.0000
.4000
2
.1200
.0800
.8000
.5400
.0100
.4500
.2400
.4800
.2800
3
.4800
.1040
.4160
.2700
.4330
.2970
.1680
.2400
.5920
4
.2496
.3944
.3560
.1782
.2593
.5626
.3552
.1584
.4864
...
...
...
...
...
...
...
...
...
...
10
.2860
.2555
.4584
.2731
.2573
.4696
.2827
.2428
.4745
...
...
...
...
...
...
...
...
...
...
25
.2813
.2500
.4688
.2813
.2500
.4688
.2813
.2500
.4688
Although omitted from Equation (10.9), in the general formulation of the power method, it is
usually necessary to normalize successive iterates, since otherwise the term λk
1 may cause successive
approximations to become too large (if λ1 > 1) or too small (if λ1 < 1) and may result in overﬂow or
underﬂow. Additionally, this normalization is required to provide a standardized vector with which
to implement convergence testing. However, in Markov chain problems, the coefﬁcient matrix has
1 as a dominant eigenvalue (λ1 = 1) and the requirement for periodic normalization of iterates in
the power method disappears. Indeed, if the initial starting approximation is a probability vector,
all successive approximations will also be probability vectors. As we noted earlier, when the power
method is applied in the Markov chain context, it is the left-hand eigenvector corresponding to a unit
eigenvalue that is required and so the matrix to which the method is applied is PT and the above
iteration, Equation (10.8), takes the form
z(k+1) = PT z(k).
(10.10)
It is known that the unit eigenvalue of a stochastic matrix is a dominant eigenvalue and that if
the matrix is irreducible, there are no other unit eigenvalues. When the matrix is periodic, however,
there exist other eigenvalues on the unit circle, which are different from 1 but whose modulus is
equal to 1. A straightforward application of the power method in this case will fail. This situation
may be circumvented by a slight modiﬁcation that leaves the unit eigenvalue and its corresponding
eigenvector unchanged. The matrix P is usually obtained from the inﬁnitesimal generator by means
of the relationship
P = (Q
t + I),
where 
t ≤1/maxi |qii|. If 
t is chosen so that 
t < 1/ maxi |qii|, the resulting stochastic matrix
has diagonal elements pii > 0 and therefore cannot be periodic. Under these conditions (irreducible
and aperiodic), the power method can be guaranteed to converge. Its rate of convergence is governed
by the ratio |λ2|/|λ1|, i.e., by |λ2|.
Unfortunately, the difference between theoretical conditions for the convergence of an iterative
method and its observed behavior in practical situations can be quite drastic. What in theory will
converge may take such a large number of iterations that for all practical purposes the method
should be considered unworkable. This occurs in the power method when the modulus of the
subdominant eigenvalue, |λ2|, is close to unity. For example, stochastic matrices that are nearly

10.3 Basic Iterative Methods for Stationary Distributions
305
completely decomposable (NCD) arise frequently in modeling physical and mathematical systems;
such matrices have subdominant eigenvalues that are necessarily close to 1. In these cases the power
method will converge extremely slowly.
Given the fact that it often takes many iterations to achieve convergence, it may be thought that
a more economical approach is to repeatedly square the matrix P. Let k = 2m for some integer m.
Using the basic iterative formula π(k) = π(k−1)P requires k iterations to obtain π(k); each iteration
includes a matrix-vector product. Repeatedly squaring the matrix requires only m matrix products
to determine P2m, from which π(k) = π(0)Pk is quickly computed. It may be further speculated
that, since a matrix-vector product requires n2 multiplications and a matrix-matrix product requires
n3, that the squaring approach is to be recommended when mn3 < 2mn2, i.e., when nm < 2m.
Unfortunately, this analysis completely omits the fact that the matrix P is usually large and sparse.
Thus, a matrix-vector product requires only nz multiplications, where nz is the number of nonzero
elements in P. The matrix-squaring operation will increase the number of nonzero elements in the
matrix (in fact, for an irreducible matrix, convergence will not be attained before all the elements
have become nonzero), thereby increasing not only the number of multiplications needed but also
the amount of memory needed. It is perhaps memory requirements more than time constraints that
limit the applicability of matrix powering.
10.3.2 The Iterative Methods of Jacobi and Gauss–Seidel
Iterative solution methods are frequently obtained from the speciﬁcation of a problem as an equation
of the form f (x) = 0. The function f (x) may be a linear function, a nonlinear function, or even
a system of linear equations, in which case f (x) = Ax −b. An iterative method is derived from
f (x) = 0 by writing it in the form x = g(x) and then constructing the iterative process
x(k+1) = g(x(k))
with some initial approximation x(0). In other words, the new iterate is obtained by inserting
the value at the previous iterate into the right-hand side. The standard and well-known iterative
methods for the solution of systems of linear equations are the methods of Jacobi, Gauss–Seidel,
and successive overrelaxation (SOR). These methods derive from a nonhomogeneous system of
linear equations
Ax = b,
or, equivalently,
Ax −b = 0,
an iterative formula of the form
x(k+1) = Hx(k) + c,
k = 0, 1, . . . .
(10.11)
This is accomplished by splitting the coefﬁcient matrix A. Given a splitting
A = M −N
with nonsingular M, we have
(M −N)x = b
or
Mx = Nx + b,
which leads to the iterative procedure
x(k+1) = M−1Nx(k) + M−1b = Hx(k) + c,
k = 0, 1, . . . .
The matrix H = M−1N is called the iteration matrix and it is the eigenvalues of this matrix that
determine the rate of convergence of the iterative method. The methods of Jacobi and Gauss–Seidel
differ in their choice M and N. We begin with the method of Jacobi.

306
Numerical Solution of Markov Chains
Consider a nonhomogeneous system of linear equations, Ax = b in which A ∈ℜ(4×4) is
nonsingular and b ̸= 0. Writing this in full, we have
a11x1 + a12x2 + a13x3 + a14x4 = b1,
a21x1 + a22x2 + a23x3 + a24x4 = b2,
a31x1 + a32x2 + a33x3 + a34x4 = b3,
a41x1 + a42x2 + a43x3 + a44x4 = b4.
Bringing all terms with off-diagonal components ai j, i ̸= j to the right-hand side, we obtain
a11x1 =
−a12x2 −a13x3 −a14x4 + b1,
a22x2 = −a21x1
−a23x3 −a24x4 + b2,
a33x3 = −a31x1 −a32x2
−a34x4 + b3,
a44x4 = −a41x1 −a42x2 −a43x3
+ b4.
We are now ready to convert this into an iterative procedure. Taking components of x on the right-
hand side to be the old values (the values computed at iteration k) allows us to assign new values as
follows:
a11x(k+1)
1
=
−a12x(k)
2
−a13x(k)
3
−a14x(k)
4
+ b1,
a22x(k+1)
2
= −a21x(k)
1
−a23x(k)
3
−a24x(k)
4
+ b2,
a33x(k+1)
3
= −a31x(k)
1
−a32x(k)
2
−a34x(k)
4
+ b3,
a44x(k+1)
4
= −a41x(k)
1
−a42x(k)
2
−a43x(k)
3
+ b4.
(10.12)
This is the Jacobi iterative method. In matrix form, A is split as A = D −L −U where2
• D is a diagonal matrix,
• L is a strictly lower triangular matrix,
• U is a strictly upper triangular matrix,
and so the method of Jacobi becomes equivalent to
Dx(k+1) = (L + U)x(k) + b
or
x(k+1) = D−1(L + U)x(k) + D−1b.
Notice that the diagonal matrix D must be nonsingular for this method to be applicable. Thus the
method of Jacobi corresponds to the splitting M = D and N = (L + U). Its iteration matrix is
given by
HJ = D−1(L + U).
In the Markov chain context, the system of equations whose solution we seek is
π Q = 0,
or, equivalently, QT π T = 0.
For notational convenience, set x = π T and let QT = D −(L + U). The matrix D is nonsingular,
since for all j, d j j ̸= 0 and so D−1 exists. Once the kth approximation x(k) has been formed, the
next approximation is obtained by solving the system of equations
Dx(k+1) = (L + U)x(k)
2 The matrices L and U should not be confused with the LU factors obtained from direct methods such as Gaussian
elimination.

10.3 Basic Iterative Methods for Stationary Distributions
307
or
x(k+1) = D−1(L + U)x(k).
In scalar form,
x(k+1)
i
= 1
dii
⎧
⎨
⎩

j̸=i
#
li j + ui j
$
x(k)
j
⎫
⎬
⎭,
i = 1, 2, . . . , n.
(10.13)
Example 10.12 Consider a four-state Markov chain with stochastic transition probability matrix
P =
⎛
⎜
⎜
⎝
.5
.5
0
0
0
.5
.5
0
0
0
.5
.5
.125
.125
.25
.5
⎞
⎟
⎟
⎠.
Since we are given P rather than Q, we need to write π P = π as π(P−I) = 0 and take Q = P−I:
Q =
⎛
⎜
⎜
⎝
−.5
.5
0
0
0
−.5
.5
0
0
0
−.5
.5
.125
.125
.25 −.5
⎞
⎟
⎟
⎠.
Transposing this, we obtain the system of equations
⎛
⎜
⎜
⎝
−.5
0
0
.125
.5
−.5
0
.125
0
.5
−.5
.250
0
0
.5
−.500
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
π1
π2
π3
π4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎠.
Writing this in full, we have
−.5π1 + 0π2 + 0π3 + .125π4 = 0,
.5π1 −.5π2 + 0π3 + .125π4 = 0,
0π1 + .5π2 −.5π3 + .25π4 = 0,
0π1 + 0π2 + .5π3 −.5π4 = 0,
or
−.5π1 =
−.125π4,
−.5π2 = −.5π1
−.125π4,
−.5π3 =
−.5π2
−.25π4,
−.5π4 =
−.5π3.
(10.14)
From this we can write the iterative version,
−.5π(k+1)
1
=
−.125π(k)
4 ,
−.5π(k+1)
2
= −.5π(k)
1
−.125π(k)
4 ,
−.5π(k+1)
3
=
−.5π(k)
2
−.25π(k)
4 ,
−.5π(k+1)
4
=
−.5π(k)
3 ,
which leads to
π(k+1)
1
= .25π(k)
4 ,
π(k+1)
2
= π(k)
1
+ .25π(k)
4 ,
π(k+1)
3
= π(k)
2
+ .5π(k)
4 ,
π(k+1)
4
= π(k)
3 .

308
Numerical Solution of Markov Chains
We may now begin the iterative process. Starting with
π(0) = (.5, .25, .125, .125),
we obtain
π(1)
1
=
.25π(0)
4
= .25 × .125 = .03125,
π(1)
2
= π(0)
1
+ .25π(0)
4
= .5 + .25 × .125 = .53125,
π(1)
3
=
π(0)
2
+ .5π(0)
4
= .25 + .5 × .125 = .31250,
π(1)
4
=
π(0)
3
= .12500.
In this particular example, the sum of the components of π(1) is equal to 1, so a further normalization
is not necessary. In fact, since at any iteration k + 1,
4

i=1
π(k+1)
i
= .25π(k)
4
+ π(k)
1
+ .25π(k)
4
+ π(k)
2
+ .50π(k)
4
+ π(k)
3
=
4

i=1
π(k)
i
= 1,
the sum of the components of all the approximations to the stationary distribution will always be
equal to 1, provided the initial approximation has components that sum to 1. Continuing with the
iterative Jacobi procedure, we obtain the following sequence of approximations:
π(0) = (.50000, .25000, .12500, .12500),
π(1) = (.03125, .53125, .31250, .12500),
π(2) = (.03125, .06250, .59375, .31250),
π(3) = (.078125, .109375, .21875, .59375).
....
Notice that substitution of QT = D −(L + U) into QT x = 0 gives (L + U)x = Dx, and since
D is nonsingular, this yields the eigenvalue equation
D−1(L + U)x = x,
(10.15)
in which x is seen to be the right-hand eigenvector corresponding to a unit eigenvalue of the matrix
D−1(L + U). This matrix will immediately be recognized as the iteration matrix for the method of
Jacobi, HJ. That HJ has a unit eigenvalue is obvious from Equation (10.15). Furthermore, from the
zero-column-sum property of QT , we have
d j j =
n

i=1, i̸= j
(li j + ui j),
j = 1, 2, . . . ,
with li j, ui j ≤0 for all i, j, i ̸= j, and it follows directly from the theorem of Gerschgorin that no
eigenvalue of HJ can have modulus greater than unity. This theorem states that the eigenvalues of
any square matrix A of order n lie in the union of the n circular disks with centers ci = aii and radii
ri = n
j=1, j̸=i |ai j|. The stationary probability vector π is therefore the eigenvector corresponding
to a dominant eigenvalue of HJ, and the method of Jacobi is identical to the power method applied
to the iteration matrix HJ.
Example 10.13 Returning to the previous example, the Jacobi iteration matrix is given by
HJ =
⎛
⎜
⎜
⎝
−.5
0
0
0
0
−.5
0
0
0
0
−.5
0
0
0
0
−.5
⎞
⎟
⎟
⎠
−1 ⎛
⎜
⎜
⎝
0
0
0
−.125
−.5
0
0
−.125
0
−.5
0
−.250
0
0
−.5
0
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
.25
1.0
0
0
.25
0
1.0
0
.50
0
0
1.0
0
⎞
⎟
⎟
⎠.

10.3 Basic Iterative Methods for Stationary Distributions
309
The four eigenvalues of this matrix are
λ1 = 1.0,
λ2 = −.7718,
λ3,4 = −0.1141 ± 0.5576i.
The modulus of λ2 is equal to 0.7718. Observe that 0.771850 = .00000237, which suggests about
ﬁve to six places of accuracy after 50 iterations.
We now move on to the method of Gauss–Seidel. Usually the computations speciﬁed by Equation
(10.13) in the method of Jacobi are carried out sequentially; the components of the vector x(k+1) are
obtained one after the other as x(k+1)
1
, x(k+1)
2
, . . . , x(k+1)
n
. When evaluating x(k+1)
i
, only components
of the previous iteration x(k) are used, even though elements from the current iteration x(k+1)
j
for
j < i are available and are (we hope) more accurate. The Gauss–Seidel method makes use of
these most recently available component approximations. This may be accomplished by simply
overwriting elements as soon as a new approximation is determined.
Referring back to Equation (10.12) and rewriting it using the most recent values, we obtain
a11x(k+1)
1
=
−a12x(k)
2
−a13x(k)
3
−a14x(k)
4
+ b1,
a22x(k+1)
2
= −a21x(k+1)
1
−a23x(k)
3
−a24x(k)
4
+ b2,
a33x(k+1)
3
= −a31x(k+1)
1
−a32x(k+1)
2
−a34x(k)
4
+ b3,
a44x(k+1)
4
= −a41x(k+1)
1
−a42x(k+1)
2
−a43x(k+1)
3
+ b4.
Observe that in the second equation, the value of the newly computed ﬁrst component, x1, is used,
i.e., we use x(k+1)
1
rather than x(k)
1 . Similarly, in the third equation we use the new values of x1 and
x2, and ﬁnally in the last equation we use the new values of all components other than the last. With
n linear equations in n unknowns, the ith equation is written as
aiix(k+1)
i
=
⎛
⎝bi −
i−1

j=1
ai jx(k+1)
j
−
n

j=i+1
ai jx(k)
j
⎞
⎠,
i = 1, 2, . . . , n.
(10.16)
Rearranging these equations so that all new values appear on the left-hand side, we ﬁnd
a11x(k+1)
1
=
−a12x(k)
2
−a13x(k)
3
−a14x(k)
4
+ b1,
a21x(k+1)
1
+ a22x(k+1)
2
=
−a23x(k)
3
−a24x(k)
4
+ b2,
a31x(k+1)
1
+ a32x(k+1)
2
+ a33x(k+1)
3
=
−a34x(k)
4
+ b3,
a41x(k+1)
1
+ a42x(k+1)
2
+ a43x(k+1)
3
+ a44x(k+1)
4
=
b4.
(10.17)
Using the same D −L −U splitting as for Jacobi, the Gauss–Seidel iterative method is equivalent
to
(D −L)x(k+1) = Ux(k) + b.
(10.18)
This is just the matrix representation of the system of equations (10.17). It may be written as
x(k+1) = D−1(Lx(k+1) + Ux(k) + b)
or
x(k+1) = (D −L)−1Ux(k) + (D −L)−1b.
(10.19)
Thus the iteration matrix for the method of Gauss–Seidel is given by
HGS = (D −L)−1U.

310
Numerical Solution of Markov Chains
This iterative method corresponds to the splitting M = (D −L) and N = U and is applicable only
when the matrix D −L is nonsingular. Gauss–Seidel usually, but not always, converges faster than
Jacobi.
For homogeneous systems of equations such as those which arise in Markov chains, the right-
hand side is zero and Equation (10.19) becomes
x(k+1) = (D −L)−1Ux(k),
i.e., x(k+1) = HGSx(k).
Furthermore, since the diagonal elements of D are all nonzero, the inverse, (D −L)−1, exists.
The stationary probability vector π = xT obviously satisﬁes HGSx = x, which shows that x is
the right-hand eigenvector corresponding to a unit eigenvalue of HGS. As a consequence of the
Stein–Rosenberg theorem ([53], p. 70) and the fact that the corresponding Jacobi iteration matrix
HJ possesses a dominant unit eigenvalue, the unit eigenvalue of the matrix HGS is a dominant
eigenvalue. The method of Gauss–Seidel is therefore identical to the power method applied to HGS.
Example 10.14
We now apply Gauss–Seidel to the problem previously solved by the method of
Jacobi:
−.5π1 =
−.125π4,
−.5π2 = −.5π1
−.125π4,
−.5π3 =
−.5π2
−.25π4,
−.5π4 =
−.5π3.
In the Gauss–Seidel iterative scheme, this becomes
−.5π(k+1)
1
=
−.125π(k)
4 ,
−.5π(k+1)
2
= −.5π(k+1)
1
−.125π(k)
4 ,
−.5π(k+1)
3
=
−.5π(k+1)
2
−.25π(k)
4 ,
−.5π(k+1)
4
=
−.5π(k+1)
3
,
π(k+1)
1
= .25π(k)
4 ,
π(k+1)
2
= π(k+1)
1
+ .25π(k)
4 ,
π(k+1)
3
= π(k+1)
2
+ .5π(k)
4 ,
π(k+1)
4
= π(k+1)
3
.
We may now begin the iterative process. Starting with
π(0) = (.5, .25, .125, .125),
we obtain
π(1)
1
=
.25π(0)
4
= .25 × .125 = .03125,
π(1)
2
= π(1)
1
+ .25π(0)
4
= .03125 + .25 × .125 = .06250,
π(1)
3
=
π(1)
2
+ .5π(0)
4
= .06250 + .5 × .125 = .12500,
π(1)
4
=
π(1)
3
= .12500.
Notice that the sum of elements in π(1) does not add up to 1, and so it becomes necessary to
normalize. We have
∥π(1)∥1 = 0.34375,
so dividing each element by 0.34375, we obtain
π(1) = (0.090909, 0.181818, 0.363636, .363636) = 1
11(1, 2, 4, 4).

10.3 Basic Iterative Methods for Stationary Distributions
311
By continuing this procedure, the following sequence of approximations is computed:
π(1) = (0.090909, 0.181818, 0.363636, .363636),
π(2) = (0.090909, 0.181818, 0.363636, .363636),
π(3) = (0.090909, 0.181818, 0.363636, .363636),
and so on. For this particular example, Gauss–Seidel converges in only one iteration! To see why
this is so, we need to examine the iteration matrix
HGS = (D −L)−1U.
(10.20)
In the example considered above, we have
D =
⎛
⎜
⎜
⎝
−.5
0
0
0
0
−.5
0
0
0
0
−.5
0
0
0
0
−.5
⎞
⎟
⎟
⎠, L =
⎛
⎜
⎜
⎝
0
0
0
0
−.5
0
0
0
0
−.5
0
0
0
0
−.5
0
⎞
⎟
⎟
⎠, U =
⎛
⎜
⎜
⎝
0
0
0
−.125
0
0
0
−.125
0
0
0
−.25
0
0
0
0
⎞
⎟
⎟
⎠,
and
HGS = (D −L)−1U =
⎛
⎜
⎜
⎝
0
0
0
.25
0
0
0
.50
0
0
0
1.0
0
0
0
1.0
⎞
⎟
⎟
⎠.
Since U has nonzeros only in the last column, it must be the case that HGS has nonzeros only in the
last column and hence the only nonzero eigenvalue of HGS must be the last diagonal element. More
generally, the eigenvalues of upper or lower triangular matrices are equal to the diagonal elements
of the matrix. Since the rate of convergence of the power method when applied to Markov chain
problems depends on the magnitude of the subdominant eigenvalue (here equal to 0), convergence
must occur immediately after the ﬁrst iteration which is indeed what we observe in this example.
As indicated in Equation (10.13), the method of Gauss–Seidel corresponds to computing the ith
component of the current approximation from i = 1 through n, i.e., from top to bottom. To denote
speciﬁcially the direction of solution, this is sometimes referred to as forward Gauss–Seidel. A
backward Gauss–Seidel iteration takes the form
(D −U)x(k+1) = Lx(k),
k = 0, 1, . . . ,
and corresponds to computing the components from bottom to top. Forward and backward iterations
in a Jacobi setting are meaningless, since in Jacobi only components of the previous iteration are
used in the updating procedure. As a general rule of thumb, a forward iterative method is usually
recommended when the preponderance of the elemental mass is to be found below the diagonal, for
in this case the iterative method essentially works with the inverse of the lower triangular portion
of the matrix, (D −L)−1, and, intuitively, the closer this is to the inverse of the entire matrix, the
faster the convergence. Ideally, in a general context, a splitting should be such that M is chosen as
close to QT as possible, subject only to the constraint that M−1 be easy to ﬁnd. On the other hand,
a backward iterative scheme works with the inverse of the upper triangular portion, (D −U)−1, and
is generally recommended when most of the nonzero mass lies above the diagonal. However, some
examples that run counter to this “intuition” are known to exist.
10.3.3 The Method of Successive Overrelaxation
In many ways, the successive overrelaxation method (SOR) resembles the Gauss–Seidel method.
When applied to Ax = b, a linear system of n equations in n unknowns, the ith component of the

312
Numerical Solution of Markov Chains
(k + 1)th iteration is obtained from
aiix(k+1)
i
= aii(1 −ω)x(k)
i
+ ω
⎛
⎝bi −
i−1

j=1
ai jx(k+1)
j
−
n

j=i+1
ai jx(k)
j
⎞
⎠,
i = 1, 2, . . . , n.
Observe that the expression within the large parentheses on the right-hand side completely
constitutes the method of Gauss–Seidel as deﬁned by Equation (10.16) and that SOR reduces to
Gauss–Seidel when ω is set equal to 1. A backward SOR relaxation may also be written. For ω > 1,
the process is said to be one of overrelaxation; for ω < 1 it is said to be underrelaxation. It may be
shown that the SOR method converges only if 0 < ω < 2. This is a necessary, but not sufﬁcient,
condition for convergence.
The choice of an optimal, or even a reasonable, value for ω has been the subject of much
study, especially for problems arising in the numerical solution of partial differential equations.
Some results have been obtained for certain classes of matrices but unfortunately, little is known at
present about the optimal choice of ω for arbitrary nonsymmetric linear systems. If a series of related
experiments is to be conducted, it may well be worthwhile to carry out some numerical experiments
to try to determine a suitable value; some sort of adaptive procedure might be incorporated into
the algorithm. For example, it is possible to begin iterating with a value of ω = 1 and, after
some iterations have been carried out, to estimate the rate of convergence from the computed
approximations. The value of ω may now be augmented to 1.1, say, and after some further iterations
a new estimate of the rate of convergence computed. If this is better than before, ω should again be
augmented, to 1.2, say, and the same procedure used. If the rate of convergence is not as good, the
value of ω should be diminished.
When applied to the homogeneous system QT x = (D−L −U)x = 0, the SOR method becomes
x(k+1)
i
= (1 −ω)x(k)
i
+ ω
⎧
⎨
⎩
1
dii
⎛
⎝
i−1

j=1
li jx(k+1)
j
+
n

j=i+1
ui jx(k)
j
⎞
⎠
⎫
⎬
⎭,
i = 1, 2, . . . , n,
or in matrix form
x(k+1) = (1 −ω)x(k) + ω
!
D−1 #
Lx(k+1) + Ux(k)$"
.
(10.21)
Rearranging, we ﬁnd
(D −ωL)x(k+1) = [(1 −ω)D + ωU]x(k)
or
x(k+1) = (D −ωL)−1[(1 −ω)D + ωU]x(k),
(10.22)
and thus the iteration matrix for the SOR method is
Hω = (D −ωL)−1[(1 −ω)D + ωU].
It corresponds to the splitting M = ω−1 [D −ωL] and N = ω−1 [(1 −ω)D + ωU].
From Equation (10.22), it is evident that the stationary probability vector is the eigenvector
corresponding to a unit eigenvalue of the SOR iteration matrix. However, with the SOR method, it
is not necessarily true that this unit eigenvalue is the dominant eigenvalue, because the eigenvalues
depend on the choice of the relaxation parameter ω. It is possible that Hω has eigenvalues which
are strictly greater than 1. When the unit eigenvalue is the dominant eigenvalue, then the SOR
method is identical to the power method applied to Hω. The value of ω that maximizes the difference
between this unit eigenvalue and the subdominant eigenvalue of Hω is the optimal choice for the
relaxation parameter, and the convergence rate achieved with this value of ω can be a considerable
improvement over that of Gauss–Seidel.

10.3 Basic Iterative Methods for Stationary Distributions
313
To summarize to this point, we have now seen that the power method may be used to obtain
π from one of four sources: PT , HJ, HGS, and Hω. The eigenvalues (with the exception of the
unit eigenvalue) will not be the same from one matrix to the next, and sometimes a considerable
difference in the number of iterations required to obtain convergence may be observed. Since the
computational effort to perform an iteration step is the same in all four cases, it is desirable to apply
the power method to the matrix that yields convergence in the smallest number of iterations, i.e., to
the matrix whose subdominant eigenvalues are, in modulus, furthest from unity.
The following Matlab code performs a ﬁxed number of iterations of the basic Jacobi, forward
Gauss–Seidel, or SOR method on the system of equation Ax = b. It accepts an input matrix A
(which may be set equal to an inﬁnitesimal generator QT ), an initial approximation x0, a right-
hand side vector b (which may be set to zero—in which case, a normalization must also be
performed), and the number of iterations to be carried out, itmax. It returns the computed solution,
soln, and a vector containing the residual computed at each iteration, resid. It is not designed to be
a “production” code, but rather simply a way to generate some results in order to get some intuition
into the performance characteristics of these methods.
Matlab code for Jacobi/Gauss–Seidel/SOR
function [soln,resid] = gs(A,x0,b,itmax)
%
Performs ‘‘itmax’’ iterations of Jacobi/Gauss-Seidel/SOR on Ax = b
[n,n] = size(A); L = zeros(n,n); U = L;
D = diag(diag(A));
for i = 1:n,
for j = 1:n,
if i<j, U(i,j) = -A(i,j); end
if i>j, L(i,j) = -A(i,j); end
end
end
M = inv(D-L);
B = M*U;
%
B is GS iteration matrix
%M = inv(D);
B = M*(L+U); % Use this for Jacobi
%w = 1.1; b = w*b; M = inv(D-w*L); B = M*((1-w)*D + w*U) % Use this for SOR
for iter = 1:itmax,
soln = B*x0+M*b;
if norm(b,2) == 0 soln = soln/norm(soln,1); end % Normalize when b=0.
resid(iter) = norm(A*soln-b,2);
x0 = soln;
end
resid = resid’;
if norm(b,2) == 0 soln = soln/norm(soln,1); end
% Normalize when b = 0.
10.3.4 Data Structures for Large Sparse Matrices
We focus next on some algorithmic details that must be taken into account when implementing
iterative methods for solving large-scale Markov chains. When the transition matrix is larger than
several hundred it becomes impractical to keep it in a two-dimensional array, the format in which we
are used to seeing matrices. In most cases the transition matrix is sparse, since each state generally
can reach only a small number of states in a single step, and hence there are only a few nonzero
elements in each row. In this section we shall consider approaches for storing this matrix efﬁciently,
by taking advantage of its sparsity. One of the major advantages that iterative methods have over
direct methods is that no modiﬁcation of the elements of the transition matrix occurs during the

314
Numerical Solution of Markov Chains
execution of the algorithm. Thus, the matrix may be stored once and for all in some convenient
compact form without the need to provide mechanisms to handle insertions (due to zero elements
becoming nonzero) and deletions (due to the elimination of nonzero elements). As a constraint,
the storage scheme used should not hinder the numerical operations that must be conducted on the
matrix. The basic numerical operation performed by the iterative methods we consider, in fact, the
only numerical operation performed on the matrix, is its pre- and postmultiplication by a vector, i.e.,
z = Ax and z = AT x.
One simple approach is to use a real (double-precision) one-dimensional array aa to store the
nonzero elements of the matrix and two integer arrays ia and ja to indicate, respectively, the row
and column positions of these elements. Thus, if the nonzero element ai j is stored in position k of
aa, i.e., aa(k) = ai j, we have ia(k) = i and ja(k) = j.
Example 10.15 The (4 × 4) matrix A given by
A =
⎛
⎜
⎜
⎝
−2.1
0.0
1.7
0.4
0.8
−0.8
0.0
0.0
0.2
1.5
−1.7
0.0
0.0
0.3
0.2
−0.5
⎞
⎟
⎟
⎠
may be stored as
aa :
−2.1
1.7
0.4
−0.8
0.8
−1.7
0.2
1.5
−0.5
0.3
0.2
ia :
1
1
1
2
2
3
3
3
4
4
4
ja :
1
3
4
2
1
3
1
2
4
2
3
or as
aa :
−2.1
0.8
0.2
−0.8
1.5
0.3
1.7
−1.7
0.2
0.4
−0.5
ia :
1
2
3
2
3
4
1
3
4
1
4
ja :
1
1
1
2
2
2
3
3
3
4
4
or yet again as
aa :
−2.1
−0.8
−1.7
−0.5
1.7
0.8
0.4
0.2
0.3
1.5
0.2
ia :
1
2
3
4
1
2
1
4
4
3
3
ja :
1
2
3
4
3
1
4
3
2
2
1
and so on. In the ﬁrst case, the matrix is stored by rows, by columns in the second case, and in a
random fashion in the third (although diagonal elements are given ﬁrst).
Irrespective of the order in which the elements of the matrix A are entered into aa, the following
algorithm, in which nz denotes the number of nonzero elements in A, computes the product z = Ax.
Algorithm 10.4: Sparse Matrix-Vector Multiplication I
1. Set z(i) = 0 for all i.
2. For next = 1 to nz do
• Set nrow = ia(next).
• Set ncol = ja(next).
• Compute z(nrow) = z(nrow) + aa(next) × x(ncol).
To perform the product z = AT x it sufﬁces simply to interchange the arrays ia and ja. This algorithm
is based on the fact that when multiplying a matrix by a vector, each element of the matrix is
used only once: element ai j is multiplied with x j and constitutes one term of the inner product
n
j=1 ai jx j = zi, the ith component of the result z. It follows that the elements in the array aa can
be treated consecutively from ﬁrst to last, at the end of which the matrix-vector product will have
been formed.

10.3 Basic Iterative Methods for Stationary Distributions
315
A more efﬁcient storage scheme can be implemented if a partial ordering is imposed on the
positions of the nonzero elements in the array aa. Consider the case when the nonzero elements of
the matrix are stored by rows; elements of row i precede those of row i + 1 but elements within a
row may or may not be in order. This is frequently the case with Markov chains, since it is usual to
generate all the states that may be reached in a single step from a given state i before generating the
states that can be reached from the next state, i +1. Hence the matrix is generated row by row. When
the nonzero elements are stored by rows in this fashion, it is possible to dispense with the integer
array ia and to replace it with a smaller array. The most commonly used compact storage scheme
uses the elements of ia as pointers into the arrays aa and ja. The kth element of ia denotes the
position in aa and ja at which the ﬁrst element of row k is stored. Thus, we always have ia(1) = 1.
Additionally, it is usual to store the ﬁrst empty position of aa and ja in position (n + 1) of ia. Most
often this means that ia(n + 1) = nz + 1. The number of nonzero elements in row i is then given by
ia(i + 1) −ia(i). This makes it easy to immediately go to any row of the matrix, even though it is
stored in a compact form—the ia(i + 1) −ia(i) nonzero elements of row i begin at aa[ia(i)]. This
row-wise packing scheme is sometimes referred to as the Harwell-Boeing format.
Example 10.16 Consider, once again, the same 4 × 4 matrix:
A =
⎛
⎜
⎜
⎝
−2.1
0.0
1.7
0.4
0.8
−0.8
0.0
0.0
0.2
1.5
−1.7
0.0
0.0
0.3
0.2
−0.5
⎞
⎟
⎟
⎠.
In this row-wise packing scheme, A may be stored as
aa :
−2.1
1.7
0.4
−0.8
0.8
−1.7
0.2
1.5
−0.5
0.3
0.2
ja :
1
3
4
2
1
3
1
2
4
2
3
ia :
1
4
6
9
12
It is not necessary for the elements in any row to be in order; it sufﬁces that all the nonzero
elements of row i come before those of row i + 1 and after those of row i −1. Using this storage
scheme, the matrix-vector product z = Ax may be computed by
Algorithm 10.5: Sparse Matrix-Vector Multiplication II
1. For i = 1 to n do
• Set sum = 0.
• Set initial = ia(i).
• Set last = ia(i + 1) −1.
• For j = initial to last do
◦Compute sum = sum + aa( j) × x( ja( j)).
• Set z(i) = sum.
In our discussion of the SOR algorithm, it may have appeared to have been numerically more
complex than the simple power method or Gauss–Seidel and that incorporation of a sparse storage
data structure would be more challenging. However, this is not the case. The SOR method requires
only a matrix-vector multiplication per iteration, the same as the power method, or for that matter
the Jacobi or Gauss–Seidel method. When programming SOR, we use the formula
x(k+1)
i
= (1 −ω)x(k)
i
+ ω
aii
⎛
⎝bi −
i−1

j=1
ai jx(k+1)
j
−
n

j=i+1
ai jx(k)
j
⎞
⎠.

316
Numerical Solution of Markov Chains
By scaling the matrix so that aii = 1 for all i and setting bi = 0, for all i, this reduces to
x(k+1)
i
= (1 −ω)x(k)
i
−ω
⎛
⎝
i−1

j=1
ai jx(k+1)
j
+
n

j=i+1
ai jx(k)
j
⎞
⎠
= x(k)
i
−ω
⎛
⎝
i−1

j=1
ai jx(k+1)
j
+ aiix(k)
i
+
n

j=i+1
ai jx(k)
j
⎞
⎠.
At iteration k, the program may be written (assuming A is stored in the row-wise compact form just
described) simply as
Algorithm 10.6: Sparse SOR
1. For i = 1 to n do
• Set sum = 0.
• Set initial = ia(i).
• Compute last = ia(i + 1) −1.
• For j = initial to last do
◦Compute sum = sum + aa( j) × x( ja( j)).
• Compute x(i) = x(i) −ω × sum.
Observe that the only difference between this and the straightforward matrix-vector multiply
algorithm, Algorithm 10.5, occurs in the very last line. In the SOR algorithm, the elements of x
are determined sequentially and hence it is possible to overwrite them with their new values as soon
as they have been computed. The computation of element x(i + 1) does not begin until the new
value of x(i) has already been computed.
When using the SOR algorithm to obtain the stationary distribution of a Markov chain, the matrix
A above must be replaced by QT , the transpose of the inﬁnitesimal generator. This may pose a
problem if Q is generated by rows, as is often the case. If it is not possible to generate Q by
columns (which means that for each state we need to ﬁnd the states that can access this state in
one step), then it becomes necessary to transpose the matrix and to do so without expanding it into
full two-dimensional format. If sufﬁcient storage is available to store the compacted matrix and
its compacted transpose, then the operation of transposition can be effected in O(nz) operations,
where nz is the number of nonzero elements stored. If space is not available for a second compacted
copy, then the transposition may be carried out in place in O(nz log nz) operations, using a standard
sorting procedure. The moral is obviously to try to store the matrix Q by columns. Unfortunately,
in many Markov chain applications, it is much more convenient to determine all destination states
that occur from a given source state (row-wise generation) than to determine all source states which
lead to a given destination state (column-wise generation).
10.3.5 Initial Approximations, Normalization, and Convergence
During the iterative process, a suitably chosen initial approximation is successively modiﬁed until
it converges to the solution. This poses three problems:
• What should we choose as the initial vector?
• What happens to this vector at each iteration?
• How do we know when convergence has occurred?
These three questions are answered in this section. When choosing an initial starting vector for an
iterative method, it is tempting to choose something simple, such as a vector whose components are

10.3 Basic Iterative Methods for Stationary Distributions
317
all zero except for one or two entries. If such a choice is made, care must be taken to ensure that
the initial vector is not deﬁcient in some component of the basis of the solution vector, otherwise a
vector containing all zeros may arise, and the process will never converge.
Example 10.17 Consider the (2 × 2) transition rate matrix
QT =
−λ
μ
λ
−μ

= (D −L −U).
Applying the Gauss–Seidel method with x(0) = (1, 0)T yields x(k) = (0, 0)T for all k ≥1, since
x(1) = (D −L)−1Ux(0) = (D −L)−1
0
−μ
0
0
 1
0

= (D −L)−1
0
0

=
0
0

,
and all successive vectors x(k), k = 1, 2, . . . , are identically equal to zero.
If some approximation to the solution is known, then it should be used. If this is not possible, the
elements of the initial iterate may be assigned random numbers uniformly distributed between 0 and
1 and then normalized to produce a probability vector. A simpler approach is to set each element
equal to 1/n, where n is the number of states.
We now turn to the second question. At each iteration the current approximation must be
multiplied by the iteration matrix. In methods other than the power method, the result of this
multiplication may not be a probability vector, i.e., the sum of the components of the computed
vector may not be one, even when the initial approximation is chosen to be a probability vector.
This vector, however, may be made into a probability vector by normalization, by dividing each
component by the sum of the components. In large-scale Markov chains, this can be a costly
operation, since the sum of all n components must be computed and then followed by n divisions—
indeed it can almost double the complexity per iteration. In most cases, it is unnecessary to
constrain successive approximations to be probability vectors. Normalization is only required prior
to conducting a test for convergence, since we need to compare standardized vectors, and to prevent
problems of overﬂow and underﬂow of the elements of the successive approximations.
Underﬂow. While this is not fatal, it often gives rise to an undesirable error message. This may
be avoided by periodically checking the magnitude of the elements and by setting those that are
less than a certain threshold (e.g., 10−25) to zero. Note that the underﬂow problem can arise even
when the approximations are normalized at each iteration, since some elements of the solution
vector, although strictly positive, may be extremely small. The concern when normalization is not
performed is that all of the elements will become smaller with each iteration until they are all set
to zero. This problem can be avoided completely by a periodic scan of the approximation to ensure
that at least one element exceeds a certain minimum threshold and to initiate a normalization when
this test fails.
Overﬂow. With a reasonable starting vector, overﬂow is unlikely to occur, since the eigenvalues
of the iteration matrices should not exceed one. All doubt can be eliminated by keeping a check on
the magnitude of the largest element and normalizing the iterate if this element exceeds a certain
maximum threshold, say 1010.
The ﬁnal question to be answered concerns knowing when convergence has occurred. This
generally comes under the heading of convergence testing and numerical accuracy. The number
of iterations k needed to satisfy a tolerance criterion ϵ may be obtained approximately from the
relationship
ρk = ϵ,
i.e., k = log ϵ
log ρ ,
where ρ is the spectral radius of the iteration matrix. In Markov chain problems, the magnitude of
the subdominant eigenvalue is used in place of ρ. Thus, when we wish to have six decimal places of

318
Numerical Solution of Markov Chains
accuracy, we set ϵ = 10−6 and ﬁnd that the number of iterations needed for different spectral radii
is as follows:
ρ
.1
.5
.6
.7
.8
.9
.95
.99
.995
.999
k
6
20
27
39
62
131
269
1, 375
2,756
13,809
Since the size of the subdominant eigenvalue is seldom known in advance, the usual method of
testing for convergence is to examine some norm of the difference of successive iterates. When this
difference becomes less than a certain prespeciﬁed tolerance, the iterative procedure is stopped. This
is a satisfactory approach when the procedure converges relatively rapidly and the magnitude of the
largest element of the vector is of order unity. However, when the procedure is converging slowly,
it is possible that the difference in successive iterates is smaller than the tolerance speciﬁed, even
though the vector may be far from the solution.
Example 10.18 Consider the inﬁnitesimal generator
Q =
⎛
⎜
⎜
⎝
−.6
0
.6
0
.0002
−.7
0
.6998
.1999
.0001
−.2
0
0
.5
0
−.5
⎞
⎟
⎟
⎠.
Using the Gauss–Seidel method with initial approximation π(0) = (.25, .25, .25, .25), we ﬁnd, after
199 and 200 iterations,
π(199) = (0.112758, 0.228774, 0.338275, 0.3201922),
π(200) = (0.112774, 0.228748, 0.338322, 0.3201560),
which appears to give about four decimal places of accuracy. If the tolerance criterion had been set
to ϵ = 0.001, the iterative procedure would have stopped prior to iteration 200. However, the true
solution is
π = (0.131589, 0.197384, 0.394768, 0.276259)
and the Gauss–Seidel method will eventually converge onto this solution. In this example, the
subdominant eigenvalue of the Gauss–Seidel iteration matrix HGS is 0.9992 and over 6,000
iterations are needed to achieve an accuracy of just ϵ = .01.
This problem may be overcome by testing, not successive iterates ∥π(k) −π(k−1)∥< ϵ, but rather
iterates spaced further apart, e.g.,
∥π(k) −π(k−m)∥< ϵ.
Ideally, m should be determined as a function of the convergence rate. A simple but less desirable
alternative is to allow m to assume different values during the iteration procedure. For example,
when the iteration number k is
k < 100
letm = 5,
100 ≤k < 500
let m = 10,
500 ≤k < 1,000
let m = 20,
k ≥1, 000
let m = 50.
A second problem arises when the approximations converge to a vector in which the elements
are all small. Suppose the problem has 100,000 states, and all are approximately equally probable.
Then each element of the solution is approximately equal to 10−5. If the tolerance criterion is
set at 10−3 and the initial vector is chosen such that π(0)
i
= 1/n for all i, then the process will
probably “converge” after one iteration! This same problem can arise in a more subtle context if

10.4 Block Iterative Methods
319
the approximations are not normalized before convergence testing and the choice of initial vector
results in the iterative method converging to a vector in which all components are small. This may
happen even though some elements of the solution may be large relative to others. For example, it
happens in the (2 × 2) Markov chain of Example 10.17 when the initial approximation is chosen as
(ξ, 10−6) for any value of ξ. (Note that the opposite effect will occur if the approximations converge
to a vector with all elements relatively large, e.g., if the initial vector is (ξ, 106) in Example 10.17.)
A solution to this latter aspect of the problem (when vectors are not normalized) is, of course, to
normalize the iterates before testing for convergence. If this normalization is such that it produces a
probability vector, the original problem (all of the components may be small) still remains. A better
choice of normalization in this instance is ∥π(k)∥∞= 1 (i.e., normalize so that the largest element
of π(k) is equal to 1—only the ﬁnal normalization needs to produce a probability vector). A better
solution, however, and the one that is recommended, is to use a relative measure, e.g.,
max
i

|π(k)
i
−π(k−m)
i
|
|π(k)
i |

< ϵ.
This effectively removes the exponent from consideration in the convergence test and hence gives a
better estimate of the precision that has been achieved.
Another criterion that has been used for convergence testing is to check the size of the residuals
(the magnitude of ∥π(k)Q∥), which should be small. Residuals will work ﬁne in many and perhaps
even most modeling problems. Unfortunately, a small residual does not always imply that the error
in the solution vector is also small. In ill-conditioned systems the residual may be very small indeed,
yet the computed solution may be hopelessly inaccurate. A small residual is a necessary condition
for the error in the solution vector to be small—but it is not a sufﬁcient condition. The most suitable
approach is to check the residual after the relative convergence test indicates that convergence has
been achieved. In fact, it is best to envisage a battery of convergence tests, all of which must be
satisﬁed before the approximation is accepted as being sufﬁciently accurate.
We now turn to the frequency with which the convergence test should be administered. Often
it is performed during each iteration. This may be wasteful, especially when the matrix is very
large and the iterative method is converging slowly. Sometimes it is possible to estimate the rate
of convergence and to determine from this rate the approximate numbers of iterations that must be
performed. It is now possible to proceed “full steam ahead” and carry out this number of iterations
without testing for convergence or normalizing. For rapidly converging problems this may result
in more iterations being performed than is strictly necessary, thereby achieving a more accurate
result than the user actually needs. When the matrix is large and an iteration costly, this may be
undesirable. One possibility is to carry out only a proportion (say 80–90%) of the estimated number
of iterations before beginning to implement the battery of convergence tests.
An alternative approach is to implement a relatively inexpensive convergence test (e.g., using the
relative difference in the ﬁrst nonzero component of successive approximations) at each iteration.
When this simple test is satisﬁed, more rigorous convergence tests may be initiated.
10.4 Block Iterative Methods
The iterative methods we have examined so far are sometimes referred to as point iterative
methods in order to distinguish them from their block counterparts. Block iterative methods are
generalizations of point iterative methods and can be particularly beneﬁcial in Markov chain
problems in which the state space can be meaningfully partitioned into subsets. In general such
block iterative methods require more computation per iteration, but this is offset by a faster rate of

320
Numerical Solution of Markov Chains
convergence. Let us partition the deﬁning homogeneous system of equations π Q = 0 as
(π1, π2, . . . , πN)
⎛
⎜
⎜
⎜
⎝
Q11
Q12
· · ·
Q1N
Q21
Q22
· · ·
Q2N
...
...
...
...
QN1
QN2
· · ·
QN N
⎞
⎟
⎟
⎟
⎠= 0.
We now introduce the block splitting
QT = DN −(L N + UN),
where DN is a block diagonal matrix and L N and UN are, respectively, strictly lower and upper
triangular block matrices. We have
DN =
⎛
⎜
⎜
⎜
⎝
D11
0
· · ·
0
0
D22
· · ·
0
...
...
...
...
0
0
· · ·
DN N
⎞
⎟
⎟
⎟
⎠,
L N =
⎛
⎜
⎜
⎜
⎝
0
0
· · ·
0
L21
0
· · ·
0
...
...
...
...
L N1
L N2
· · ·
0
⎞
⎟
⎟
⎟
⎠,
UN =
⎛
⎜
⎜
⎜
⎝
0
U12
· · ·
U1N
0
0
· · ·
U2N
...
...
...
...
0
0
· · ·
0
⎞
⎟
⎟
⎟
⎠.
In analogy with Equation (10.18), the block Gauss–Seidel method is given by
(DN −L N)x(k+1) = UN x(k)
and corresponds to the splitting M = (DN −L N); N = UN. The ith block equation is given by
Diix(k+1)
i
=
⎛
⎝
i−1

j=1
Li jx(k+1)
j
+
N

j=i+1
Ui jx(k)
j
⎞
⎠
i = 1, 2, . . . , N,
(10.23)
where the subvectors xi are partitioned conformally with Dii, i = 1, 2, . . . , N. This implies that at
each iteration we must now solve N systems of linear equations
Diix(k+1)
i
= zi,
i = 1, 2, . . . , N,
(10.24)
where
zi =
⎛
⎝
i−1

j=1
Li jx(k+1)
j
+
N

j=i+1
Ui jx(k)
j
⎞
⎠,
i = 1, 2, . . . , N.
The right-hand side zi can always be computed before the ith system has to be solved. If our
Markov chain is irreducible, the N systems of equations (10.24) are nonhomogeneous and have
nonsingular coefﬁcient matrices. We may use either direct or iterative methods to solve them.
Naturally, there is no requirement to use the same method to solve all the diagonal blocks. Instead,
it is possible to tailor methods to particular block structures. If a direct method is used, then an LU
decomposition of block Dii may be formed once and for all before beginning the iteration, so that
solving Diix(k+1)
i
= zi, i = 1, . . . , N, in each global iteration simpliﬁes to a forward and backward
substitution. The nonzero structure of the blocks may be such that this is a particularly efﬁcient
approach. For example, if the diagonal blocks are themselves diagonal matrices, or if they are upper
or lower triangular matrices or even tridiagonal matrices, then it is very easy to obtain their LU
decomposition, and a block iterative method becomes very attractive.

10.4 Block Iterative Methods
321
If the diagonal blocks are large and do not possess a suitable nonzero structure, it may be
appropriate to use matrix iterative methods (such as point Gauss–Seidel) to solve these block
equations—in which case we can have multiple inner (or local) iterative methods (one per block
thus analyzed) within an outer (or global) iteration. A number of tricks may be used to speed up
this process. First, the solution computed using any block Dii at iteration k should be used as the
initial approximation to the solution using this same block at iteration k + 1. Second, it is hardly
worthwhile computing a highly accurate solution in early (outer) iterations. We should require only
a small number of digits of accuracy until the global process begins to converge. One convenient
way to achieve this is to carry out only a ﬁxed, small number of iterations for each inner solution.
Initially, this will not give much accuracy, but when combined with the ﬁrst suggestion, the accuracy
achieved will increase from one outer iteration to the next.
Intuitively, it is expected that for a given transition rate matrix Q, the larger the block sizes
(and thus the smaller the number of blocks), the fewer the number of (outer) iterations needed for
convergence. This has been shown to be true under fairly general assumptions on the coefﬁcient
matrix for general systems of equations (see [53]). In the special case of only one block, the method
degenerates to a standard direct method and we compute the solution in a single “iteration.” The
reduction in the number of iterations that usually accompanies larger blocks is offset to a certain
degree by an increase in the number of operations that must be performed at each iteration. However,
in some important cases it may be shown that there is no increase. For example, when the matrix
is block tridiagonal (as in quasi-birth-death processes) and the diagonal blocks are also tridiagonal,
it may be shown that the computational effort per iteration is the same for both point and block
iterative methods. In this case the reduction in the number of iterations makes the block methods
very efﬁcient indeed.
In a similar vein to the block Gauss–Seidel method, we may also deﬁne a block Jacobi method
Diix(k+1)
i
=
⎛
⎝
i−1

j=1
Li jx(k)
j
+
N

j=i+1
Ui jx(k)
j
⎞
⎠,
i = 1, 2, . . . , N,
and a block SOR method
x(k+1)
i
= (1 −ω)x(k)
i
+ ω
⎧
⎨
⎩D−1
ii
⎛
⎝
i−1

j=1
Li jx(k+1)
j
+
N

j=i+1
Ui jx(k)
j
⎞
⎠
⎫
⎬
⎭,
i = 1, 2, . . . , N.
Example 10.19 We apply the block Gauss–Seidel method to ﬁnd the stationary distribution of the
continuous-time Markov chain with inﬁnitesimal generator given by
Q =
⎛
⎜
⎜
⎜
⎜
⎝
−4.0
2.0
1.0
0.5
0.5
0.0
−3.0
3.0
0.0
0.0
0.0
0.0
−1.0
0.0
1.0
1.0
0.0
0.0
−5.0
4.0
1.0
0.0
0.0
1.0
−2.0
⎞
⎟
⎟
⎟
⎟
⎠
.
We put the ﬁrst three states in the ﬁrst subset and the remaining two states into a second subsest.
Transposing Q and writing out the system of equations, we have
QT x =

D11
−U12
−L21
D22
 
x1
x2

=
⎛
⎜
⎜
⎜
⎜
⎝
−4.0
0.0
0.0
1.0
1.0
2.0
−3.0
0.0
0.0
0.0
1.0
3.0
−1.0
0.0
0.0
0.5
0.0
0.0 −5.0
1.0
0.5
0.0
1.0
4.0
−2.0
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
π1
π2
π3
π4
π5
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎝
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
.

322
Numerical Solution of Markov Chains
Equation (10.23) becomes
Diix(k+1)
i
=
⎛
⎝
i−1

j=1
Li jx(k+1)
j
+
2

j=i+1
Ui jx(k)
j
⎞
⎠,
i = 1, 2,
and leads to the two block equations
i = 1:
D11x(k+1)
1
=
⎛
⎝
0

j=1
L1 jx(k+1)
j
+
2

j=2
U1 jx(k)
j
⎞
⎠= U12x(k)
2 ,
i = 2:
D22x(k+1)
2
=
⎛
⎝
1

j=1
L2 jx(k+1)
j
+
2

j=3
U2 jx(k)
j
⎞
⎠= L21x(k+1)
1
.
Writing these block equations in full, they become
⎛
⎝
−4.0
0.0
0.0
2.0
−3.0
0.0
1.0
3.0
−1.0
⎞
⎠
⎛
⎝
π(k+1)
1
π(k+1)
2
π(k+1)
3
⎞
⎠= −
⎛
⎝
1.0
1.0
0.0
0.0
0.0
0.0
⎞
⎠

π(k)
4
π(k)
5

and
−5.0
1.0
4.0
−2.0
 π(k+1)
4
π(k+1)
5

= −
0.5
0.0
0.0
0.5
0.0
1.0
 ⎛
⎝
π(k+1)
1
π(k+1)
2
π(k+1)
3
⎞
⎠.
We shall solve these block equations using LU decompositions. Since D11 is lower triangular, the
ﬁrst subsystem may be solved by forward substitution alone:
D11 =
⎛
⎝
−4.0
0.0
0.0
2.0
−3.0
0.0
1.0
3.0
−1.0
⎞
⎠= L × I.
Forming an LU decomposition of the second subsystem, we have
D22 =

−5.0
1.0
4.0
−2.0

=

1.0
0.0
−0.8
1.0
 
−5.0
1.0
0.0
−1.2

= L × U.
Taking the initial distribution to be
π(0) = (0.2, 0.2, 0.2, 0.2, 0.2)T
and substituting the ﬁrst block equation, we ﬁnd
⎛
⎝
−4.0
0.0
0.0
2.0
−3.0
0.0
1.0
3.0
−1.0
⎞
⎠
⎛
⎝
π(1)
1
π(1)
2
π(1)
3
⎞
⎠= −
⎛
⎝
1.0
1.0
0.0
0.0
0.0
0.0
⎞
⎠
0.2
0.2

= −
⎛
⎝
0.4
0.0
0.0
⎞
⎠.
Forward substitution successively gives
π(1)
1
= 0.1000,
π(1)
2
= 0.0667,
and π(1)
3
= 0.3000.
The second block system now becomes

1.0
0.0
−0.8
1.0
 
−5.0
1.0
0.0
−1.2
 
π(1)
4
π(1)
5

= −

0.5
0.0
0.0
0.5
0.0
1.0
 ⎛
⎝
0.1000
0.0667
0.3000
⎞
⎠= −

0.0500
0.3500

.

10.4 Block Iterative Methods
323
This is in the standard form LUx = b from which the solution is computed by ﬁrst obtaining z
from Lz = b and then x from Ux = z. From

1.0
0.0
−0.8
1.0
 
z1
z2

= −

0.0500
0.3500

,
we obtain
z1 = −0.0500 and z2 = −0.3900.
Now, solving

−5.0
1.0
0.0
−1.2
 
π(1)
4
π(1)
5

=

−0.0500
−0.3900

,
we ﬁnd
π(1)
4
= 0.0750 and π(1)
5
= 0.3250.
We now have
π(1) = (0.1000, 0.0667, 0.3000, 0.0750, 0.3250)
which, when normalized so that the elements sum to 1, gives
π(1) = (0.1154, 0.0769, 0.3462, 0.0865, 0.3750).
The next iteration may now be initiated. Interested readers who perform some additional iterations
will see that, in this example, all subsequent iterations yield exactly the same result. Indeed, for
this speciﬁc example, the block Gauss–Seidel method requires only a single iteration to obtain the
solution to full machine precision. A examination of the matrix UN reveals that this matrix has only
two nonzero elements in positions (1, 4) and (1, 5) and they are both equal. It must follow that the
iteration matrix can have only two nonzero columns, the fourth and ﬁfth, and they must be identical.
It may be concluded that the iteration matrix has an eigenvalue equal to 1 and four equal to 0, which
explains the fact that convergence is achieved in a single iteration.
The following code is a Matlab implementation of the block Gauss–Seidel method. The program
accepts a stochastic matrix P; a partitioning vector ni, whose ith component stores the length of the
ith block; and two integers: itmax1, which denotes the number of outer iterations to perform, and
itmax2, which denotes the number of iterations to use to solve each of the blocks if Gauss–Seidel is
used to solve these blocks. The program returns the solution vector π and a vector of residuals. It
calls the Gauss–Seidel program given previously.
Matlab code for Block Gauss–Seidel
function [x,res] = bgs(P,ni,itmax1,itmax2)
[n,n] = size(P); [na,nb] = size(ni);
%
BLOCK Gauss-Seidel FOR P^T x = x
bl(1) = 1;
%
Get beginning and end
for k = 1:nb, bl(k+1) = bl(k)+ni(k); end
%
points of each block
x = ones(n,1)/n;
%
Initial approximation
%%%%%%%%%%%%%%%%%%%%%%%%
BEGIN OUTER LOOP
%%%%%%%%%%%%%%%%%%%%%%%%%
for iter = 1:itmax1,
for m = 1:nb,
% All diagonal blocks

324
Numerical Solution of Markov Chains
A = P(bl(m):bl(m+1)-1,bl(m):bl(m+1)-1)’;
% Get A_mm
b = -P(1:n,bl(m):bl(m+1)-1)’*x+A*x(bl(m):bl(m+1)-1);
%
RHS
z = inv(A-eye(ni(m)))*b;
%
Solve for z
%
***
To solve the blocks using Gauss--Seidel
***
%
***
instead of a direct method, substitute
***
%
***
the next two lines for the previous one.
***
%**
x0 = x(bl(m):bl(m+1)-1);
% Get starting vector
%**
[z,r] = gs(A-eye(ni(m)),x0,b,itmax2);
% Solve for z
x(bl(m):bl(m+1)-1) = z;
%
Update x
end
res(iter) = norm((P’-eye(n))*x,2);
%
Compute residual
end
x = x/norm(x,1); res = res’;
10.5 Decomposition and Aggregation Methods
A decompositional approach to solving Markov chains is intuitively very attractive since it appeals
to the principle of divide and conquer: if the model is too large or complex to analyze in toto,
it is divided into subsystems, each of which is analyzed separately, and a global solution is then
constructed from the partial solutions. Ideally the problem is broken into subproblems that can
be solved independently and the global solution is obtained by “pasting” together the subproblem
solutions. Although it is rare to ﬁnd Markov chains that can be divided into independent subchains,
it is not unusual to have Markov chains in which this condition almost holds. An important class
of problems that frequently arise in Markov modeling are those in which the state space may be
partitioned into disjoint subsets with strong interactions among the states of a subset but with weak
interactions among the subsets themselves. Such problems are sometimes referred to as nearly
completely decomposable (NCD), nearly uncoupled, or nearly separable. It is apparent that the
assumption that the subsystems are independent and can therefore be solved separately does not
hold. Consequently an error arises. This error will be small if the assumption is approximately true.
An irreducible NCD stochastic matrix P may be written as
P =
⎛
⎜
⎜
⎜
⎝
P11
P12
. . .
P1N
P21
P22
. . .
P2N
...
...
...
...
PN1
PN2
. . .
PN N
⎞
⎟
⎟
⎟
⎠,
where
||Pii|| = O(1),
i = 1, 2, . . . , N,
and
||Pi j|| = O(ϵ),
i ̸= j.
In addition to its unit eigenvalue, such a matrix possesss N −1 eigenvalues extremely close to 1.
None of the point iterative methods discussed previously is effective in handling this situation. On
the other hand, block and decompositional methods can be very effective.

10.5 Decomposition and Aggregation Methods
325
We begin by examining what happens when the off-diagonal blocks are all zero, i.e.,
(π1, π2, . . . , πN)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
P11
0
. . .
0
0
0
P22
. . .
0
0
...
...
...
...
...
0
0
. . .
PN−1N−1
0
0
0
. . .
0
PN N
⎞
⎟
⎟
⎟
⎟
⎟
⎠
= (π1, π2, . . . , πN).
In this case, each Pii is a stochastic matrix. The Markov chain is reducible into N distinct irreducible
classes and a stationary distribution can be found for each class. Each πi can be found directly from
πi Pii = πi,
i = 1, 2, . . . , N.
This allows us to envisage the following approximate solution procedure when the Markov chain is
NCD rather than completely decomposable:
(a) Solve the diagonal blocks as if they are independent.
The solution obtained for each block should provide an approximation to the probability of
being in the different states of that block, conditioned on being in that block.
(b) Estimate the probability of being in each block.
This will allow us to remove the condition in part (a).
(c) Combine (a) and (b) into a global approximate solution.
We now examine each of these three steps in more detail.
Step (a): Solve blocks as if independent
The initial step in approximating the solution of π P = π when Pi j ̸= 0 is to assume that
the system is completely decomposable and to compute the stationary probability distribution for
each component. A ﬁrst problem that arises is that the Pii are not stochastic but rather strictly
substochastic. A simple way around this problem is to simply ignore it; i.e., to work directly with
the substochastic matrices Pii themselves. In other words, we may use the normalized eigenvector
corresponding to the Perron root (the eigenvalue closest to 1) of block Pii as the probability vector
whose elements denote the probabilities of being in the different states of the block, conditioned on
being in this block.
Example 10.20
To help, we shall illustrate the procedure using the 8 × 8 Courtois matrix, an
examination of which reveals that it is nearly completely decomposable (NCD) into a block of
order 3, a second of order 2, and a third of order 3:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.85
.0
.149
.0009
.0
.00005
.0
.00005
.1
.65
.249
.0
.0009
.00005
.0
.00005
.1
.8
.0996
.0003
.0
.0
.0001
.0
.0
.0004
.0
.7
.2995
.0
.0001
.0
.0005
.0
.0004
.399
.6
.0001
.0
.0
.0
.00005
.0
.0
.00005 .6
.2499
.15
.00003
.0
.00003 .00004
.0
.1
.8
.0999
.0
.00005
.0
.0
.00005 .1999
.25
.55
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
For this matrix, we have the following blocks, Perron roots, and corresponding left-hand eigenvec-
tors:
P11 =
⎛
⎝
.85
.0
.149
.1
.65
.249
.1
.8
.0996
⎞
⎠,
λ11 = .99911,
u1 = (.40143, .41672, .18185),

326
Numerical Solution of Markov Chains
P22 =

.7
.2995
.399
.6

,
λ21 = .99929,
u2 = (.57140, .42860),
P33 =
⎛
⎝
.6
.2499
.15
.1
.8
.0999
.1999
.25
.55
⎞
⎠,
λ31 = .9999,
u3 = (.24074, .55563, .20364).
To summarize, in part (a), the left-hand eigenvector ui of length ni corresponding to the
eigenvalue closest to 1, λi1, in each block i, 1 ≤i ≤N, is computed. In other words, we solve
the N eigenvalue problems
ui Pii = λi1ui,
uie = 1,
i = 1, 2, . . . , N.
Step (b): Estimate block probabilities
The second problem is that, once we have computed the stationary probability vector for each
block, simply concatenating them together will not give a probability vector. The elements of each
subvector sum to 1. We still need to weight each subvector by the probability of being in its subblock
of states. The probability distributions computed from the Pii are conditional probabilities in the
sense that they express the probability of being in a given state of subset i, i = 1, 2, . . . , N,
conditioned on the fact that the Markov chain is in one of the states of that subset. We need to
remove that condition. To determine the probability of being in a given block of states we need to
construct a matrix whose element i j gives the probability of a transition from block i to block j.
This is an (N × N) stochastic matrix that characterizes the interactions among blocks. To construct
this matrix, which is called the coupling matrix, we need to shrink each block Pi j of P down to a
single element. The stationary distribution of the coupling matrix provides the block probabilities,
the weights needed to form the global approximation.
In terms of the running example, we need to ﬁnd weights ξ1, ξ2, and ξ3 such that
(ξ1u1, ξ2u2, ξ3u3)
is an approximate solution to π. Here ξi is the proportion of time we spend in block i. We need to
shrink our original (8×8) stochastic matrix down to a (3×3) stochastic matrix. This is accomplished
by ﬁrst replacing each row of each block by the sum of its elements. The sum of the elements of
row k of block i j gives the probability of leaving state k of block i and entering into (one of the
states of) block j. It no longer matters which particular state of block j is this destination state.
Mathematically, the operation performed for each block is Pi je.
Example 10.21 Summing across the block rows of the Courtois matrix gives
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.999
.0009
.0001
.999
.0009
.0001
.9996
.0003
.0001
.0004
.9995
.0001
.0009
.999
.0001
.00005 .00005 .9999
.00006 .00004 .9999
.00005 .00005 .9999
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
The next step is to use these results to ﬁnd the probability of leaving (any state of) block i to
enter (any state of) block j. This means that we must reduce each column subvector, Pi je, to a
scalar. As we have just noted, the kth element of Pi je is the probability of leaving state k of block
i and entering into block j. To determine the total probability of leaving (any state of) block i to
enter into (any state of) block j we need to sum the elements of this vector after each element has

10.5 Decomposition and Aggregation Methods
327
been weighed by the probability of being in that state (given that the Markov chain is in one of the
states of that block). These weighing factors may be obtained from the elements of the stationary
probability vector. They are the components of πi/||πi||1. The i jth element of the reduced (N × N)
matrix is therefore given by
(C)i j =
πi
||πi||1
Pi je = φi Pi je,
where φi = πi/||πi||1. If P is an irreducible stochastic matrix, then C also is irreducible and
stochastic. Let ξ denote its left eigenvector, i.e., ξC = ξ and ξe = 1. The ith component of ξ is the
stationary probability of being in (one of the states of) block i. It is easy to show that
ξ = (||π1||1, ||π2||1, ..., ||πN||1).
Of course, the vector π is not yet known, so that it is not possible to compute the weights ||πi||1.
However, they may be approximated by using the probability vector computed from each of the
individual Pii, i.e., by setting φi = ui. Consequently, the weights ξi can be estimated and an
approximate solution to the stationary probability vector π obtained. Any of the previously used
methods to ﬁnd stationary probability vectors may be used.
Example 10.22 Performing these operations, we obtain the following coupling matrix for the
Courtois example:
(.40143, .41672, .18185, | .57140, .42860, | .24074, .55563, .20364)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.999
.0009
.0001
.999
.0009
.0001
.9996
.0003
.0001
.0004
.9995
.0001
.0009
.999
.0001
.00005 .00005 .9999
.00006 .00004 .9999
.00005 .00005 .9999
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎝
.99911 .00079 .00010
.00061 .99929 .00010
.00006 .00004 .99990
⎞
⎠= C.
Its eigenvalues are 1.0, .9998, and .9985 and its stationary probability vector is
ξ = (.22252, .27748, .50000).
Step (c): Compute global approximation
We are now in a position to form the ﬁnal approximation to the stationary probability vector. It is
given by the approximation
π ≈(ξ1u1, ξ2u2, . . . , ξNuN),
where the ui are approximations to πi/∥πi∥1.
Example 10.23 In the running example we obtain the approximate solution
π∗= (.08932, .09273, .04046, .15855, .11893, .12037, .27781, .10182),
which may be compared to the exact solution
π = (.08928, .09276, .04049, .15853, .11894, .12039, .27780, .10182).
To summarize, an approximation to the stationary probability vector of an NCD Markov chain
may be obtained by ﬁrst solving each of the blocks separately, then forming and solving the

328
Numerical Solution of Markov Chains
coupling matrix, and ﬁnally constructing the approximate solution from these pieces. It is implicitly
understood that the states have been ordered so that the transition probability matrix has the required
NCD block structure. Algorithmically, the entire procedure may be written as
Algorithm 10.7: NCD Decomposition Approximation
1. Solve the individual blocks: ui Pii = λi1ui, uie = 1 for i = 1, 2, . . . , N.
2. (a) Form the coupling matrix: (C)i j = ui Pi je.
(b) Solve the coupling problem: ξ = ξC, ξe = 1.
3. Construct the approximate solution: π∗= (ξ1u1, ξ2u2, . . . , ξNuN).
The question now arises as to whether we can incorporate this approximation back into the
decomposition algorithm to get an even better approximation. Notice that the ui are used to form
the coupling matrix. If we replace them with the new approximations ξiui, we will obtain exactly
the same solution to the coupling matrix as before. Hence, there will be no change in the computed
approximation. However, it was found that applying a power step to the approximation before
plugging it back into the decomposition method had a very salutory effect. Later this power step
was replaced by a block Gauss–Seidel step and became known as a disaggregation step; forming
and solving the matrix C being the aggregation step. The entire procedure is referred to as iterative
aggregation/disaggregation (IAD).
The algorithm is presented below. The iteration number is indicated by a superscript in
parentheses on the appropriate variable names. The initial vector is designated as π(0). This may
be the approximation obtained from the simple decomposition approach, or it may be just a random
selection. Observe that many of the steps have corresponding steps in the decomposition algorithm.
For instance, the formation of the coupling matrix (Step 3) is identical in both. Step 4(a) has its
counterpart in the formation of the computed approximation in the decomposition algorithm, while
Step 4(b) corresponds to solving the different blocks, except that in the IAD algorithm, a block
Gauss–Seidel step is used.
Algorithm 10.8: Iterative Aggregation/Disaggregation
1. Let π(0) = (π(0)
1 , π(0)
2 , . . . , π(0)
N ) be a given initial approximation
to the solution π, and set m = 1.
2. Compute φ(m−1) = (φ(m−1)
1
, φ(m−1)
2
, . . . , φ(m−1)
N
), where
φ(m−1)
i
=
π(m−1)
i
||π(m−1)
i
||1
,
i = 1, 2, . . . , N.
3. (a) Form the coupling matrix: C(m−1)
i j
= φ(m−1)
i
Pi je for i, j = 1, 2, . . . , N.
(b) Solve the coupling problem: ξ (m−1)C(m−1) = ξ (m−1), ξ (m−1)e = 1.
4. (a) Construct the row vector
z(m) = (ξ (m−1)
1
φ(m−1)
1
, ξ (m−1)
2
φ(m−1)
2
, . . . , ξ (m−1)
N
φ(m−1)
N
).
(b) Solve the following N systems of equations to find π(m):
π(m)
k
= π(m)
k
Pkk +

j>k
z(m)
j
Pjk +

j<k
π(m)
j
Pjk, k = 1, 2, . . . , N.
(10.25)
5. Normalize and conduct a test for convergence.
If satisfactory, then stop and take
π(m) to be the required
solution vector.
Otherwise set m = m + 1 and go to step 2.

10.5 Decomposition and Aggregation Methods
329
In these methods, it is important that the matrix has the block structure needed by the algorithms
and it may be necessary to reorder the states to get this property. Only after reordering the states
can we guarantee that the resulting transition matrix will have a property that directly reﬂects the
structural characteristics of the NCD system. If the partitioning provided to the algorithm does not
match the decomposability characteristics of the matrix, the convergence behavior may be much
less satisfactory. A reordering of the states can be accomplished by treating the Markov chain as a
directed graph in which edges with small weights (probabilities) are removed. A graph algorithm
must then be used to ﬁnd the connected components of ˆP + ˆPT , where ˆP is the modiﬁed transition
probability matrix. The complexity of the algorithm is O(|V | + |E|), where |V | is the number of
vertices and |E| is the number of edges in the graph. Details are provided in [14].
Example 10.24 When applied to the Courtois matrix, we ﬁnd that both the iterative aggregation
and disaggregation (IAD) method and the block Gauss–Seidel (BGS) method are very effective.
The table below shows that convergence is achieved to full machine precision in only four iterations
with the IAD method and nine iterations with BGS. In both cases, the diagonal block equations are
solved using LU decomposition.
IAD and BGS residuals for the Courtois NCD matrix
Iteration
IAD residual
BGS residual
1.0e−05×
1.0e−05×
1
0.93581293961421
0.94805408435419
2
0.00052482104506
0.01093707688215
3
0.00000000280606
0.00046904081241
4
0.00000000000498
0.00002012500900
5
0.00000000000412
0.00000086349742
6
0.00000000000351
0.00000003705098
7
0.00000000000397
0.00000000158929
8
0.00000000000529
0.00000000006641
9
0.00000000000408
0.00000000000596
10
0.00000000000379
0.00000000000395
We now turn our attention to some implementation details. The critical points are Steps 3 and
4(b). In Step 3, it is more efﬁcient to compute Pi je only once for each block and to store it
somewhere for use in all future iterations. This is only possible if sufﬁcient memory is available;
otherwise it is necessary to compute it each time it is needed. To obtain the vector ξ in Step 3(b),
any of the methods discussed in the previous section may be used, since the vector ξ is simply the
stationary probability vector of an irreducible stochastic matrix C.
In Step 4(b), each of the N systems of equations in (10.25) can be written as Bx = r where
B = (I −Pkk)T and
r T =

j>k
z j Pjk +

j<k
π j Pjk, k = 1, 2, . . . , N.
In all cases, Pkk is a strictly substochastic matrix so that B is nonsingular. The vector r will
have small norm if the system is NCD. If a direct method is used, the LU decomposition of
(I −Pkk), k = 1, 2, . . . , N, need only be performed once, since this remains unchanged from
one iteration to the next. If an iterative method is used we have an iteration algorithm within an
iteration algorithm. In this case it is advantageous to perform only a small number of iterations,
(e.g., 8–12 of the Gauss–Seidel method) each time a solution of (I −Pkk)T x = r is needed but to
use the ﬁnal approximation at one step as the initial approximation the next time the solution of that
same subsystem is needed.

330
Numerical Solution of Markov Chains
Example 10.25 Returning again to the Courtois matrix, the table below shows the number of
iterations needed to achieve full machine precision when the diagonal block equations in the IAD
method are solved using the Gauss–Seidel iterative method. It can be seen that now an additional
iteration is needed.
IAD: Gauss–Seidel for Block solutions
Iteration
Residual: ˆπ(I −P)
1.0e−03×
1
0.14117911369086
2
0.00016634452597
3
0.00000017031189
4
0.00000000015278
5
0.00000000000014
6
0.00000000000007
7
0.00000000000006
8
0.00000000000003
9
0.00000000000003
10
0.00000000000006
Also, if we check the convergence of the inner Gauss–Seidel method for the diagonal blocks—as
shown in the table below for the ﬁrst diagonal block of size (3×3)—it can be seen that the iterations
stagnate after just a few steps. After about six iterations during each global iteration, progress slows
to a crawl. It is for this reason that only a small number of iterations should be used when iterative
methods are used to solve the (inner) block equations.
Global iteration
Inner Iteration
1
2
3
4
1
0.0131607445
0.000009106488
0.00000002197727
0.00000000002345
2
0.0032775892
0.000002280232
0.00000000554827
0.00000000000593
3
0.0008932908
0.000000605958
0.00000000142318
0.00000000000151
4
0.0002001278
0.000000136332
0.00000000034441
0.00000000000037
5
0.0001468896
0.000000077107
0.00000000010961
0.00000000000011
6
0.0001124823
0.000000051518
0.00000000003470
0.00000000000003
7
0.0001178683
0.000000055123
0.00000000003872
0.00000000000002
8
0.0001156634
0.000000053697
0.00000000003543
0.00000000000002
9
0.0001155802
0.000000053752
0.00000000003596
0.00000000000002
10
0.0001149744
0.000000053446
0.00000000003562
0.00000000000002
11
0.0001145044
0.000000053234
0.00000000003552
0.00000000000002
12
0.0001140028
0.000000052999
0.00000000003535
0.00000000000002
13
0.0001135119
0.000000052772
0.00000000003520
0.00000000000002
14
0.0001130210
0.000000052543
0.00000000003505
0.00000000000002
15
0.0001125327
0.000000052316
0.00000000003490
0.00000000000002
16
0.0001120464
0.000000052090
0.00000000003475
0.00000000000002
The following Matlab code may be used to experiment with IAD methods. It mirrors the block
Gauss–Seidel method in its parameters. It was used to produce the results provided above.

10.5 Decomposition and Aggregation Methods
331
Matlab code for Iterative Aggregation/Disaggregation
function [soln,res] = kms(P,ni,itmax1,itmax2)
[n,n] = size(P); [na,nb] = size(ni);
%%%%%%%% ITERATIVE AGGREGATION/DISAGGREGATION FOR pi*P = pi %%%%%%%%%%%%%%%%
bl(1) = 1;
%
Get beginning and end
for k = 1:nb, bl(k+1) = bl(k)+ni(k); end
%
points of each block
E = zeros(n,nb);
%
Form (n x nb) matrix E
next = 0;
%
(This is needed in forming
for i = 1:nb,
%
the coupling matrix: A )
for k = 1:ni(i); next = next+1; E(next,i) = 1; end
end
Pije = P*E;
%
Compute constant part of coupling matrix
Phi = zeros(nb,n);
%
Phi, used in forming the coupling matrix,
for m = 1:nb,
%
keeps normalized parts of approximation
for j = 1:ni(m), Phi(m,bl(m)+j-1) = 1/ni(m); end
end
A = Phi*Pije;
%
Form the coupling matrix A
AA = (A-eye(nb))’;
en = [zeros(nb-1,1);1];
xi = inv([AA(1:nb-1,1:nb);ones(1,nb)])*en;
%
Solve the coupling matrix
z = Phi’*xi;
%
Initial approximation
%%%%%%%%%%%%%%%%%%%%%%%%%
BEGIN OUTER LOOP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
for iter = 1:itmax1,
for m = 1:nb,
%
Solve all diag. blocks; Pmm y = b
Pmm = P(bl(m):bl(m+1)-1,bl(m):bl(m+1)-1);
%
Get coefficient block, Pmm
b = z(bl(m):bl(m+1)-1)’*Pmm-z’*P(1:n,bl(m):bl(m+1)-1);
%
RHS
y = inv(Pmm’-eye(ni(m)))*b’;
%
Substitute this line for the 2
% lines below it, to solve Pmm by iterative method.
%x0 = z(bl(m):bl(m+1)-1);
%
Get new starting vector
%[y,resid] = gs(Pmm’-eye(ni(m)),x0,b’,itmax2);
% y is soln for block Pmm
for j = 1:ni(m), z(bl(m)+j-1) = y(j); end %
Update solution vector
y = y/norm(y,1);
%
Normalize y
for j = 1:ni(m),
Phi(m,bl(m)+j-1) = y(j);
%
Update Phi
end
end
pi = z;

332
Numerical Solution of Markov Chains
res(iter) = norm((P’-eye(n))*pi,2);
% Compute residual
A = Phi*Pije; AA = (A-eye(nb))’;
% Form the coupling matrix A
xi = inv([AA(1:nb-1,1:nb);ones(1,nb)])*en; % Solve the coupling matrix
z
= Phi’*xi;
% Compute new approximation
end
soln = pi; res = res’;
10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
We now turn to the numerical solution of Markov chains whose transition matrices have a special
block structure—a block structure that arises frequently when modeling queueing systems—and
examine an approach pioneered by Neuts [39, 40]. In the simplest case, these matrices are inﬁnite
block tridiagonal matrices in which the three diagonal blocks repeat after some initial period. To
capture this nonzero structure, we write such a matrix as
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
0
0
0
0
· · ·
B10
A1
A2
0
0
0
· · ·
0
A0
A1
A2
0
0
· · ·
0
0
A0
A1
A2
0
· · ·
...
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(10.26)
in which submatrices A0, A1, and A2 are square and have the same dimension; the matrix B00 is
also square and need not have the same size as A1, and the dimensions of B01 and B10 are deﬁned to
be in accordance with the dimensions of B00 and A1. A transition matrix having this structure arises
when each state of the Markov chain can be represented as a pair {(η, k), η ≥0, 1 ≤k ≤K} and
the states ordered, ﬁrst according to increasing value of the parameter η and, for states with the same
η value, by increasing value of k. This has the effect of grouping the states into “levels” according to
their η value. The block tridiagonal effect is achieved when transitions are permitted only between
states of the same level (diagonal blocks), to states in the next highest level (superdiagonal blocks),
and to states in the adjacent lower level (subdiagonal blocks). The repetitive nature of the blocks
themselves arises if, after boundary conditions are taken into consideration (which gives the initial
blocks B00, B01, and B10) the transition rates/probabilities are identical from level to level. A
Markov chain whose transition matrix has this block tridiagonal structure is said to belong to the
class of quasi-birth-death (QBD) processes.
Example 10.26 Consider the Markov chain whose state transition diagram is shown in Figure 10.1.
 μ/2  
 λ  1 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 γ 
 γ 
 1 
 2 
 μ 
 μ 
 μ 
 μ 
 μ 
0,1
0,2
1,1
1,2
1,3
2,1
2,2
2,3
3,1
3,2
3,3
4,1
4,2
4,3
5,1
5,2
5,3
 1 
 1 
 1 
 μ/2  
Figure 10.1. State transition diagram for an M/M/1-type process.

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
333
Its transition rate matrix is
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∗
γ1
λ1
γ2
∗
λ2
∗
γ1
λ1
μ/2
μ/2 γ2
∗
γ1
γ2
∗
λ2
∗
γ1
λ1
μ
γ2
∗
γ1
γ2
∗
λ2
∗
γ1
λ1
μ
γ2
∗
γ1
γ2
∗
λ2
∗
γ1
λ1
μ
γ2
∗
γ1
γ2
∗
λ2
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and has the typical block tridiagonal structure which makes it an ideal candidate for solution by the
numerical techniques described in this section. Its diagonal elements, marked by asterisks, are such
that the sum across each row is zero. We have the following block matrices:
A0 =
⎛
⎝
0
0
0
0
μ
0
0
0
0
⎞
⎠, A1 =
⎛
⎝
−(γ1 + λ1)
γ1
0
γ2
−(μ + γ1 + γ2)
γ1
0
γ2
−(γ2 + λ2)
⎞
⎠, A2 =
⎛
⎝
λ1
0
0
0
0
0
0
0
λ2
⎞
⎠,
and
B00 =

−(γ1 + λ1)
γ1
γ2
−(γ2 + λ2)

,
B01 =

λ1
0
0
0
0
λ2

,
B10 =
⎛
⎝
0
0
μ/2 μ/2
0
0
⎞
⎠.
The most common extensions of this simplest case are matrices which are block upper
Hessenberg (referred to as M/G/1 type and solved using the matrix analytic approach) and those that
are block lower Hessenberg (referred to as G/M/1 type, solved using the matrix geometric approach).
Both are treated in this text. Results are also available for the case in which the parameters η and k
are both ﬁnite, which results in a ﬁnite Markov chain, as well as for the case in which transitions
may be dependent on the level η. These last extensions are beyond the scope of this text and the
interested reader is invited to consult the references.
10.6.1 The Quasi-Birth-Death Case
Quasi-birth-death processes can be conveniently and efﬁciently solved using the matrix geometric
method. We begin by considering the case when the blocks of a QBD process are reduced to single
elements. Consider, for example, the following inﬁnite inﬁnitesimal generator:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−λ
λ
μ
−(λ + μ)
λ
μ
−(λ + μ)
λ
μ
−(λ + μ)
λ
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.

334
Numerical Solution of Markov Chains
This may be associated with a random walk problem in which the probability of moving from any
state k to state k + 1 is λ/(λ + μ) and the probability of moving from state k to state k −1 is
μ/(λ + μ). In a later chapter, we shall associate this inﬁnitesimal generator with the M/M/1 queue.
From π Q = 0, we may write −λπ0 + μπ1 = 0, which gives π1 = (λ/μ)π0, and in general
λπi−1 −(λ + μ)πi + μπi+1 = 0.
We proceed by induction to show that πi+1 = (λ/μ)πi for i = 1, 2, . . .. We have already established
the basis clause π1 = (λ/μ)π0. From the inductive hypothesis, we have πi = (λ/μ)πi−1 and hence
πi+1 =
λ + μ
μ

πi −
 λ
μ

πi−1 =
 λ
μ

πi,
which is the desired result. It now follows that
πi =
 λ
μ
i
π0 = ρiπ0,
where ρ = λ/μ. Thus, once π0 is known, the remaining values, πi, i = 1, 2, . . . , may be
determined recursively. A similar result exists when Q is a QBD process: In this case, the parameter
ρ becomes a square matrix R of order K and the components πi of the stationary distribution
become subvectors of length K.
Let Q be the inﬁnitesimal generator of a QBD process. Then
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
0
0
0
0
· · ·
B10
A1
A2
0
0
0
· · ·
0
A0
A1
A2
0
0
· · ·
0
0
A0
A1
A2
0
· · ·
...
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
and the stationary distribution is obtained from π Q = 0. Let π be partitioned conformally with Q,
i.e.,
π = (π0, π1, π2, . . .),
where
πi = (π(i, 1), π(i, 2), . . . π(i, K))
for i = 0, 1, . . ., and π(i, k) is the probability of ﬁnding the system in state (i, k) at steady state.
This gives the following equations:
π0B00 + π1B10 = 0,
π0B01 + π1A1 + π2A0 = 0,
π1A2 + π2A1 + π3A0 = 0,
...
πi−1A2 + πi A1 + πi+1A0 = 0,
i = 2, 3, . . . .
In analogy with the point situation, it may be shown that there exists a constant matrix R such that
πi = πi−1R for i = 2, 3, . . . .
(10.27)
The subvectors πi are said to be geometrically related to each other since
πi = π1Ri−1
for i = 2, 3, . . . .
(10.28)

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
335
If the subvectors π0 and π1 and the rate matrix R can be found then the remaining subvectors of the
stationary distribution may be formed using Equation (10.27). Returning to
πi−1A2 + πi A1 + πi+1 A0 = 0
and substituting from Equation (10.28), we obtain, for i = 2, 3, . . . ,
π1Ri−2A2 + π1Ri−1A1 + π1Ri A0 = 0,
i.e.,
π1Ri−2 #
A2 + RA1 + R2 A0
$
= 0.
It is now apparent that R can be computed from
#
A2 + RA1 + R2 A0
$
= 0.
(10.29)
The simplest way to accomplish this is by successive substitution. From Equation (10.29) and using
the fact that A1 must be nonsingular, we have
A2A−1
1
+ R + R2A0A−1
1
= 0,
i.e.,
R = −A2A−1
1
−R2A0A−1
1
= −V −R2W,
where V = A2A−1
1
and W = A0A−1
1 . This leads to the successive substitution procedure proposed
by Neuts, namely,
R(0) = 0,
R(k+1) = −V −R2
(k)W,
k = 0, 1, 2, . . . .
(10.30)
Neuts has shown that the sequence of matrices R(k), k = 0, 1, 2, . . . , is nondecreasing and
converges to the rate matrix R. The process is halted once successive differences are less than a
speciﬁed tolerance criterion. Unfortunately, this simple approach has the disadvantage of frequently
requiring many iterations before a sufﬁciently accurate matrix R is obtained. On the other hand,
a logarithmic reduction algorithm developed by Latouche and Ramaswami [27] has extremely fast
quadratic convergence (the number of decimal places doubles at each iteration). The development of
this algorithm is beyond the scope of this text: it is presented in pseudocode and without additional
comment, at the end of this section. There are also situations in which the rate matrix R can be
computed explicitly, without the need to conduct any iterations at all.
The only remaining problem is the derivation of π0 and π1. The ﬁrst two equations of π Q = 0
are
π0B00 + π1B10 = 0,
π0B01 + π1A1 + π2A0 = 0.
Replacing π2 with π1R and writing these equations in matrix form, we obtain
(π0, π1)

B00
B01
B10
A1 + RA0

= (0, 0).
(10.31)
Given the rate matrix R and blocks B00, B01, B10, A1, and A0, this system may be solved to obtain
π0 and π1. Since this is a homogeneous system of equations, the computed solution needs to be

336
Numerical Solution of Markov Chains
normalized so that the components of π sum to 1. In other words, we insist that πe = 1. Thus
1 = πe = π0e + π1e +
∞

i=2
πie
= π0e + π1e +
∞

i=2
π1Ri−1e
= π0e +
∞

i=1
π1Ri−1e = π0e +
∞

i=0
π1Rie.
This implies the condition
π0e + π1
 ∞

i=0
Ri

e = 1.
The eigenvalues of R lie inside the unit circle, which means that (I −R) is nonsingular and hence
 ∞

i=0
Ri

= (I −R)−1.
(10.32)
This enables us to complete the normalization of the vectors π0 and π1 by computing
α = π0e + π1 (I −R)−1 e
and dividing the computed subvectors π0 and π1 by α.
In the simpler case, when B00 is the same size as the A blocks and when B01 = A2, then
πi = π0Ri for i = 1, 2, . . .. In this case the system of equations (10.31) can be replaced with
the simpler system, π0(B00 + RB10) = 0, from which π can be computed and then normalized so
that π0 (I −R)−1 e = 1.
In a previous discussion about a random walk problem, it was observed that the Markov chain
is positive recurrent only if the probability of moving to higher-numbered states is strictly less than
the probability of moving to lower-numbered states. A similar condition exists for a QBD process
to be ergodic, namely, that the drift to higher-numbered levels must be strictly less than the drift to
lower levels. Let the stationary distribution of the inﬁnitesimal generator A = A0 + A1 + A2 be
denoted by πA. For a QBD process to be ergodic, the following condition must hold:
πA A2e < πA A0e.
(10.33)
Recall that the elements of A2 move the process up to a higher-numbered level while those of A0
move it down a level. Indeed, it is from this condition that Neuts shows that the spectral radius of R
is strictly less than 1 and consequently that the matrix I −R is nonsingular.
When the Markov chain under consideration is a discrete-time Markov chain characterized
by a stochastic transition probability matrix rather than the continuous-time version discussed in
this section, then an almost identical analysis can be performed: it sufﬁces to replace −A−1
1
with
(I −A1)−1. But be careful: in the ﬁrst case, A1 is the repeating diagonal block in a transition rate
matrix while in the second it is the repeating diagonal block in a stochastic matrix. The formulation
of the equations is the same, just the values in the blocks change according to whether the global
matrix is an inﬁnitesimal generator or a stochastic matrix. Before proceeding to an example, we
summarize the steps that must be undertaken when solving a QBD process by the matrix geometric
method:
1. Ensure that the matrix has the requisite block structure.
2. Use Equation (10.33) to ensure that the Markov chain is ergodic.

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
337
3. Use Equation (10.30) to compute the matrix R.
4. Solve the system of equations (10.31) for π0 and π1.
5. Compute the normalizing constant α and normalize π0 and π1.
6. Use Equation (10.27) to compute the remaining components of the stationary distribution
vector.
Example 10.27
We shall apply the matrix geometric method to the Markov chain of Example
10.26 using the following values of the parameters:
λ1 = 1, λ2 = .5, μ = 4, γ1 = 5, γ2 = 3.
The inﬁnitesimal generator is then given by
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−6
5.0
1
3
−3.5
.5
−6
5
1
2
2.0
3
−12
5.0
3
−3.5
.5
−6
5
1
4
3
−12
5.0
3
−3.5
.5
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
1. The matrix obviously has the correct QBD structure.
2. We check that the system is stable by verifying Equation (10.33). The inﬁnitesimal generator
matrix
A = A0 + A1 + A2 =
⎛
⎝
−5
5
0
3
−8
5
0
3
−3
⎞
⎠
has stationary probability vector
πA = (.1837, .3061, .5102)
and
.4388 = πA A2e < πA A0e = 1.2245.
3. We now initiate the iterative procedure to compute the rate matrix R. The inverse of A1 is
A−1
1
=
⎛
⎝
−.2466
−.1598
−.2283
−.0959
−.1918
−.2740
−.0822
−.1644
−.5205
⎞
⎠,
which allows us to compute
V = A2A−1
1
=
⎛
⎝
−.2466
−.1598
−.2283
0
0
0
−.0411
−.0822
−.2603
⎞
⎠
and
W = A0A−1
1
=
⎛
⎝
0
0
0
−.3836
−.7671
−1.0959
0
0
0
⎞
⎠.

338
Numerical Solution of Markov Chains
Equation (10.30) becomes
R(k+1) =
⎛
⎝
.2466
.1598
.2283
0
0
0
.0411
.0822
.2603
⎞
⎠+ R2
(k)
⎛
⎝
0
0
0
.3836
.7671
1.0959
0
0
0
⎞
⎠,
and iterating successively, beginning with R(0) = 0, we ﬁnd
R(1) =
⎛
⎝
.2466
.1598
.2283
0
0
0
.0411
.0822
.2603
⎞
⎠,
R(2) =
⎛
⎝
.2689
.2044
.2921
0
0
0
.0518
.1036
.2909
⎞
⎠,
R(3) =
⎛
⎝
.2793
.2252
.3217
0
0
0
.0567
.1134
.3049
⎞
⎠, · · · .
Observe that the elements are nondecreasing, as predicted by Neuts. After 48 iterations,
successive differences are smaller than 10−12, at which point
R(48) =
⎛
⎝
.2917
.2500
.3571
0
0
0
.0625
.1250
.3214
⎞
⎠.
4. Proceeding to the boundary conditions,
(π0, π1)

B00
B01
B10
A1 + RA0

= (π0, π1)
⎛
⎜
⎜
⎜
⎜
⎝
−6
5.0
1
0
0
3
−3.5
0
0
.5
0
0
−6
6.0
0
2
2.0
3
−12.0
5.0
0
0
0
3.5
−3.5
⎞
⎟
⎟
⎟
⎟
⎠
= (0, 0).
We can solve this by replacing the last equation with the equation π01 = 1, i.e., setting the
ﬁrst component of the subvector π0 to 1. The system of equations becomes
(π0, π1)
⎛
⎜
⎜
⎜
⎜
⎝
−6
5.0
1
0
1
3
−3.5
0
0
0
0
0
−6
6.0
0
2
2.0
3
−12.0
0
0
0
0
3.5
0
⎞
⎟
⎟
⎟
⎟
⎠
= (0, 0 | 0, 0, 1)
with solution
(π0, π1) = (1.0, 1.6923, | .3974, .4615, .9011).
5. The normalization constant is
α = π0e + π1 (I −R)−1 e
= (1.0, 1.6923)e + (.3974, .4615, .9011)
⎛
⎝
1.4805
.4675
.7792
0
1
0
.1364
.2273
.15455
⎞
⎠e
= 2.6923 + 3.2657 = 5.9580,
which allows us to compute
π0/α = (.1678, .2840),
and π1/α = (.0667, .0775, .1512).

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
339
6. Successive subcomponents of the stationary distribution are now computed from πk = πk−1R.
For example,
π2 = π1R = (.0667, .0775, .1512)
⎛
⎝
.2917
.2500
.3571
0
0
0
.0625
.1250
.3214
⎞
⎠= (.0289, .0356, .0724),
π3 = π2R = (.0289, .0356, .0724)
⎛
⎝
.2917
.2500
.3571
0
0
0
.0625
.1250
.3214
⎞
⎠= (.0130, .0356, .0336),
and so on.
Steps 2 through 6 of the basic matrix geometric method for QBD processes are embedded in the
following Matlab code in which n is the order of the blocks Ai and m is the order of B00:
Matlab code for QBD processes
function[R,pi0,pi1] = mgm(n,A0,A1,A2,m,B00,B01,B10)
%
Matrix Geometric Method
%%%%%%%%%%%%%%
Form Neuts’ R matrix
%%%%%%%%%%%%%%%
V = A2 * inv(A1);
W = A0 * inv(A1);
R = zeros(n,n);
Rbis = -V - R*R * W;
iter = 1;
while (norm(R-Rbis,1) > 1.0e-10)
R = Rbis;
Rbis = -V - R*R * W;
iter = iter+1;
end
R = Rbis;
%%%%%%%%
Form and solve boundary equations
%%%%%%%%
C = [B00,B01;B10,A1+R*A0];
z = zeros(n+m,1); z(1) = 1;
Cn = [C(1:n+m,1:n+m-1),z];
rhs = zeros(1,n+m); rhs(n+m) = 1;
pi = rhs*inv(Cn);
pi0 = pi(1:m);
pi1 = pi(m+1:m+n);
%%%%%%%%%%%
Normalize computed solution
%%%%%%%%%%%
alpha = norm(pi0,1)+ norm(pi1*inv(eye(n)-R),1);
pi0 = pi0/alpha;
pi1 = pi1/alpha;
%%%%%%%%%%
Compute successive subvectors
%%%%%%%%%%
pi2 = pi1*R;
pi3 = pi2*R;
% and so on.
Algorithm 10.10: Logarithmic Reduction—Quadratic Convergence
for QBD Processes
1. Initialize: i = 0;
C0 = (I −A1)−1A0;
C2 = (I −A1)−1A2;
if stochastic, or
%%%
C0 = −A−1
1 A0;
C2 = −A−1
1 A2;
if inﬁnitesimal generator.
T = C0;
S = C2;

340
Numerical Solution of Markov Chains
2. While ∥e −Se∥∞< ϵ:
• i = i + 1;
•
A∗
1 = C0C2 + C2C0;
A∗
0 = C2
0;
A∗
2 = C2
2;
• C0 = (I −A∗
1)−1A∗
0;
C2 = (I −A∗
1)−1A∗
2;
• S = S + TC2;
T = TC0;
3. Termination
G = S;
U = A1 + A0G;
R = A0(I −U)−1.
10.6.2 Block Lower Hessenberg Markov Chains
In a lower Hessenberg matrix A, all elements ai j must be zero for values of j > i + 1. In other
words, if moving from top right to bottom left, we designate the three diagonals of a tridiagonal
matrix as the superdiagonal, the diagonal, and the subdiagonal, then a lower Hessenberg matrix can
have nonzero elements only on and below the superdiagonal. For example, the following matrix is
a 6 × 6 lower Hessenberg matrix:
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
a00
a01
0
0
0
0
a10
a11
a12
0
0
0
a20
a21
a22
a23
0
0
a30
a31
a32
a33
a34
0
a40
a41
a42
a43
a44
a45
a50
a51
a52
a53
a54
a55
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Block lower Hessenberg matrices, found in G/M/1-type stochastic processes, are just the block
counterparts of lower Hessenberg matrices. In a similar manner, we can deﬁne block upper
Hessenberg matrices, which are found in M/G/1-type stochastic processes, as block matrices whose
only nonzero blocks are on or above the diagonal blocks and along the subdiagonal block. One is
essentially the block transpose of the other. This leads us to caution the reader about the notation
used to designate the individual blocks. With block lower Hessenberg matrices, the nonzero blocks
are numbered from right to left as A0, A1, A2, . . ., while in block upper Hessenberg matrices they
are numbered in the reverse order, from left to right. As we have just seen, we choose to denote
the blocks in QBD processes (which are simultaneously both upper and lower block Hessenberg)
from left to right, i.e., using the block upper Hesssenberg notation. Our interest in this section is
with Markov chains whose inﬁnitesimal generators Q have the following repetitive block lower
Hessenberg structure:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
0
0
0
0
0
· · ·
B10
B11
A0
0
0
0
0
· · ·
B20
B21
A1
A0
0
0
0
· · ·
B30
B31
A2
A1
A0
0
0
· · ·
B40
B41
A3
A2
A1
A0
0
· · ·
...
...
...
...
...
...
· · ·
...
...
...
...
...
...
...
· · ·
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Markov chains with this structure occur when the states are grouped into levels, similar to those for
QBD processes, but now transitions are no longer conﬁned to interlevel and to adjacent neighboring
levels; transitions are also permitted from any level to any lower level. In this particular case,
we perform the analysis using two boundary columns (Bi0 and Bi1, i = 0, 1, 2, . . .). Certain
applications give rise to more than two boundary columns, which may necessitate a restructuring of
the matrix. This is considered at the end of this section.

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
341
As always, our objective is to compute the stationary probability vector π from the system
of equations π Q = 0. As for QBD processes, let π be partitioned conformally with Q, i.e.
π = (π0, π1π2, . . .) where πi = (π(i, 1)π(i, 2), . . . π(i, K)) for i = 0, 1, . . ., and π(i, k) is the
probability of ﬁnding the Markov chain in state (i, k) at statistical equilibrium. Neuts has shown
that there exists a matrix geometric solution to this problem and that it mirrors that of a QBD
process, speciﬁcally that there exists a positive matrix R such that
πi = πi−1R for i = 2, 3, . . . ,
i.e., that
πi = π1Ri−1
for i = 2, 3, . . . .
Observe that from π Q = 0 we have
∞

k=0
πk+ j Ak = 0,
j = 1, 2, . . . ,
and, in particular,
π1A0 + π2A1 +
∞

k=2
πk+1Ak = 0.
Substituting πi = π1Ri−1
π1A0 + π1RA1 +
∞

k=2
π1Rk Ak = 0
or
π1

A0 + RA1 +
∞

k=2
Rk Ak

= 0
provides the following relation from which the matrix R may be computed:
A0 + RA1 +
∞

k=2
Rk Ak = 0.
(10.34)
Notice that Equation (10.34) reduces to Equation (10.29) when Ak = 0 for k > 2. Rearranging
Equation (10.34), we ﬁnd
R = −A0 A−1
1
−
∞

k=2
Rk Ak A−1
1 ,
which leads to the iterative procedure
R(0) = 0,
R(l+1) = −A0 A−1
1
−
∞

k=2
Rk
(l)Ak A−1
1 , l = 0, 1, 2, . . . ,
which is, as Neuts has shown, nondecreasing and converges to the matrix R. In many cases, the
structure of the inﬁnitesimal generator is such that the blocks Ai are zero for relatively small
values of i, which limits the computational effort needed in each iteration. As before, the number of
iterations needed for convergence is frequently large, and now the extremely efﬁcient logarithmic
reduction algorithm is no longer applicable—it is designed for QBD processes only. However, more
efﬁcient but also more complex algorithms have been developed and may be found in the current
literature [3, 36, 43, 44].
We now turn to the derivation of the initial subvectors π0 and π1. From the ﬁrst equation of
π Q = 0, we have
∞

i=0
πi Bi0 = 0

342
Numerical Solution of Markov Chains
and we may write
π0B00 +
∞

i=1
πi Bi0 = π0B00 +
∞

i=1
π1Ri−1Bi0 = π0B00 + π1
 ∞

i=1
Ri−1Bi0

= 0,
(10.35)
while from the second equation of π Q = 0,
π0B01 +
∞

i=1
πi Bi1 = 0,
i.e., π0B01 + π1
∞

i=1
Ri−1Bi1 = 0.
(10.36)
Putting Equations (10.35) and (10.36) together in matrix form, we see that we can compute π0 and
π1 from
(π0, π1)
⎛
⎝
B00
B01
∞
i=1 Ri−1Bi0
∞
i=1 Ri−1Bi1
⎞
⎠= (0, 0).
The computed values of π0 and π1 must now be normalized by dividing them by
α = π0e + π1
 ∞

i=1
Ri−1

e = π0e + π1(I −R)−1e.
Example 10.28 We shall apply the matrix geometric method to the Markov chain of Example
10.27 now modiﬁed so that it incorporates additional transitions (ξ1 = .25 and ξ2 = .75) to lower
non-neighboring states, Figure 10.2.
 1 
 λ  1 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 1 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 γ 
 γ 
 1 
 2 
 μ 
 μ 
 μ 
 μ 
 μ 
0,1
0,2
1,1
1,2
1,3
2,1
2,2
2,3
3,1
3,2
3,3
4,1
4,2
4,3
5,1
5,2
5,3
 ξ 
 ξ 
 ξ 
 ξ 
 ξ 
 ξ 
 ξ 
 ξ 
 ξ 
 1 
 1 
 1 
 1 
 1 
 1 
 ξ 
 ξ 
 ξ 
 2 
 2 
 2 
 2 
 2 
 2 
 μ/2 
 μ/2 
 1 
Figure 10.2. State transition diagram for a G/M/1-type process.
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−6
5.0
1
3
−3.5
.5
−6
5
1
2
2
3
−12
5.0
3 −3.5
.5
.25
−6.25
5
1
4
3.00 −12
5.00
.75
3 −4.25
.5
.25
−6.25
5
1
4
3.00 −12
5.00
.75
3 −4.25
.5
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
343
The computation of the matrix R proceeds as previously: The inverse of A1 is now
A−1
1
=
⎛
⎝
−.2233
−.1318
−.1550
−.0791
−.1647
−.1938
−.0558
−.1163
−.3721
⎞
⎠,
which allows us to compute
A0A−1
1
=
⎛
⎝
−.2233
−.1318
−.1550
0
0
0
−.0279
−.0581
−.1860
⎞
⎠,
A2A−1
1
=
⎛
⎝
0
0
0
−.3163
−.6589
−.7752
0
0
0
⎞
⎠,
A3A−1
1
=
⎛
⎝
−.0558
−.0329
−.0388
0
0
0
−.0419
−.0872
−.2791
⎞
⎠.
The iterative process is
R(k+1) =
⎛
⎝
.2233
.1318
.1550
0
0
0
.0279
.0581
.1860
⎞
⎠+R2
(k)
⎛
⎝
0
0
0
.3163
.6589
.7752
0
0
0
⎞
⎠+R3
(k)
⎛
⎝
.0558
.0329
.0388
0
0
0
.0419
.0872
.2791
⎞
⎠,
and iterating successively, beginning with R(0) = 0, we ﬁnd
R(1) =
⎛
⎝
.2233
.1318
.1550
0
0
0
.0279
.0581
.1860
⎞
⎠,
R(2) =
⎛
⎝
.2370
.1593
.1910
0
0
0
.0331
.0686
.1999
⎞
⎠,
R(3) =
⎛
⎝
.2415
.1684
.2031
0
0
0
.0347
.0719
.2043
⎞
⎠, . . . .
After 27 iterations, successive differences are smaller than 10−12, at which point
R(27) =
⎛
⎝
.2440
.1734
.2100
0
0
0
.0356
.0736
.1669
⎞
⎠.
The boundary conditions are now
(π0, π1)

B00
B01
B10 + RB20
B11 + RB21 + R2B31

= (0, 0),
i.e.,
= (π0, π1)
⎛
⎜
⎜
⎜
⎜
⎝
−6.0
5.0
1
0
0
3.0
−3.5
0
0
.5
.0610
.1575 −5.9832
5.6938
.0710
2.0000
2.000
3.000
−12.0000
5.0000
.0089
.1555
.0040
3.2945
−3.4624
⎞
⎟
⎟
⎟
⎟
⎠
= (0, 0).
As before, we solve this by replacing the last equation with the equation π01 = 1. The system of
equation becomes
(π0, π1)
⎛
⎜
⎜
⎜
⎜
⎝
−6.0
5.0
1
0
1
3.0
−3.5
0
0
0
.0610
.1575 −5.9832
5.6938
0
2.0000
2.000
3.000
−12.0000
0
.0089
.1555
.0040
3.2945
0
⎞
⎟
⎟
⎟
⎟
⎠
= (0, 0 | 0, 0, 1)

344
Numerical Solution of Markov Chains
with solution
(π0, π1) = (1.0, 1.7169 | .3730, .4095, .8470).
The normalization constant is
α = π0e + π1 (I −R)−1 e
= (1.0, 1.7169)e + (.3730, .4095, .8470)
⎛
⎝
1.3395
.2584
.3546
0
1
0
.0600
.1044
1.2764
⎞
⎠e
= 2.7169 + 2.3582 = 5.0751,
which allows us to compute
π0/α = (.1970, .3383) and π1/α = (.0735, .0807, .1669).
Successive subcomponents of the stationary distribution are now computed from πk = πk−1R. For
example,
π2 = π1R = (.0735, .0807, .1669)
⎛
⎝
.2440
.1734
.2100
0
0
0
.0356
.0736
.1669
⎞
⎠= (.0239, .0250, .0499)
and
π3 = π2R = (.0239, .0250, .0499)
⎛
⎝
.2440
.1734
.2100
0
0
0
.0356
.0736
.1669
⎞
⎠= (.0076, .0078, .0135),
and so on.
Some simpliﬁcations occur when the initial B blocks have the same dimensions as the A blocks
and when the inﬁnitesimal generator can be written in the following commonly occurring form:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
A0
0
0
0
0
0
· · ·
B10
A1
A0
0
0
0
0
· · ·
B20
A2
A1
A0
0
0
0
· · ·
B30
A3
A2
A1
A0
0
0
· · ·
B40
A4
A3
A2
A1
A0
0
· · ·
...
...
...
...
...
...
· · ·
...
...
...
...
...
...
...
· · ·
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
In this case,
πi = π0Ri
for i = 1, 2, . . . .
Furthermore, ∞
i=0 Ri Bi0 is an inﬁnitesimal generator and the subvector π0 is the stationary
probability vector of ∞
i=0 Ri Bi0 normalized so that π0(I −R)−1e = 1.

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
345
In some applications, such as queueing systems with bulk arrivals, more than two boundary
columns can occur. Consider, for example the generator matrix
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
B02
A0
B10
B11
B12
A1
A0
B20
B21
B22
A2
A1
A0
B30
B31
B32
A3
A2
A1
A0
B40
B41
B42
A4
A3
A2
A1
A0
B50
B51
B52
A5
A4
A3
A2
A1
A0
B60
B61
B62
A6
A5
A4
A3
A2
A1
A0
B70
B71
B72
A7
A6
A5
A4
A3
A2
A1
A0
B80
B81
B82
A8
A7
A6
A5
A4
A3
A2
A1
A0
...
...
...
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
At present, this matrix is not block lower Hessenberg. However, if it is restructured into the form
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
A0
0
0
· · ·
B10
A1
A0
0
· · ·
B20
A2
A1
A0
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
B02 A0
B10
B11
B12 A1
A0
B20
B21
B22 A2
A1
A0
B30
B31
B32 A3
A2
A1 A0
B40
B41
B42 A4
A3
A2 A1
A0
B50
B51
B52 A5
A4
A3 A2
A1
A0
B60
B61
B62 A6
A5
A4 A3
A2
A1 A0
B70
B71
B72 A7
A6
A5 A4
A3
A2 A1
A0
B80
B81
B82 A8
A7
A6 A5
A4
A3 A2
A1
A0
...
...
...
...
...
...
...
... ...
...
... ...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
with
A0 =
⎛
⎝
A0
A1
A0
A2
A1
A0
⎞
⎠,
A1 =
⎛
⎝
A3
A2
A1
A4
A3
A2
A5
A4
A3
⎞
⎠,
B00 =
⎛
⎝
B00
B01
B02
B10
B11
B12
B20
B21
B22
⎞
⎠,
. . .
then the matrix Q does have the desired block lower Hessenberg form and hence the techniques
described in this section may be successfully applied to it.
In the case of discrete-time Markov chains, as opposed to the continuous-time case just outlined,
it sufﬁces to replace −A−1
1
with (I −A1)−1, as we described for QBD processes.
10.6.3 Block Upper Hessenberg Markov Chains
We now move to block upper Hessenberg Markov chains, also called M/G/1-type processes and
solved using the matrix analytic method. In the past two sections concerning QBD and G/M/1-type
processes, we posed the problem in terms of continuous-time Markov chains, and mentioned that
discrete-time Markov chains can be treated if the matrix inverse A−1
1
is replaced with the inverse

346
Numerical Solution of Markov Chains
(I −A1)−1 where it is understood that A1 is taken from a stochastic matrix. This time we shall
consider the discrete-time case. Speciﬁcally, we consider the case when the stochastic transition
probability matrix is irreducible and has the structure
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
B02
B03
· · ·
B0 j
· · ·
B10
A1
A2
A3
· · ·
A j
· · ·
0
A0
A1
A2
· · ·
A j−1
· · ·
0
0
A0
A1
· · ·
A j−2
· · ·
0
0
0
A0
· · ·
A j−3
· · ·
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
in which all submatrices A j, j
=
0, 1, 2, . . . are square and of order K, and B00 is square
but not necessarily of order K. Be aware that whereas previously the block A0 was the rightmost
nonzero block of our global matrix, this time it is the leftmost nonzero block. Notice that the matrix
A = ∞
i=0 Ai is a stochastic matrix. We shall further assume that A is irreducible, the commonly
observed case in practice. Instances in which A is not irreducible are treated by Neuts.
πA A = πA
and πAe = 1.
The Markov chain P is known to be positive recurrent if the following condition holds:
πA
 ∞

i=1
i Ai e

≡πA b < 1.
(10.37)
Our objective is the computation of the stationary probability vector π from the system of equations
π P = π. As before, we partition π conformally with P, i.e.,
π = (π0, π1, π2, . . .),
where
πi = (π(i, 1), π(i, 2), . . . , π(i, K))
for i = 0, 1, . . . and π(i, k) is the probability of ﬁnding the system in state (i, k) at statistical
equilibrium. The analysis of M/G/1-type processes is more complicated than that of QBD or G/M/1-
type processes because the subvectors πi no longer have a matrix geometric relationship with one
another.
The key to solving upper block Hessenberg structured Markov chains is the computation of a
certain matrix G which is stochastic if the Markov chain is recurrent, which we assume to be the
case. This is the same matrix G that appears in the logarithmic reduction algorithm and has an
important probabilistic interpretation. The element Gi j of this matrix is the conditional probability
that starting in state i of any level n ≥2, the process enters level n −1 for the ﬁrst time by arriving
at state j of that level. This matrix satisﬁes the ﬁxed point equation
G =
∞

i=0
AiGi
and is indeed the minimal non-negative solution of
X =
∞

i=0
Ai Xi.
It can be found by means of the iteration
G(0) = 0,
G(k+1) =
∞

i=0
AiGi
(k),
k = 0, 1, . . . .

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
347
Once the matrix G has been computed, then successive components of π can be obtained
from a relationship, called Ramaswami’s formula, which we now develop. We follow the algebraic
approach of Bini and Meini [3, 36], rather than the original probabilistic approach of Ramaswami.
We begin by writing the system of equations π P = π as π(I −P) = 0, i.e.,
(π0, π1, . . . , π j, . . .)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
I −B00
−B01
−B02
−B03
· · ·
−B0 j
· · ·
−B10
I −A1
−A2
−A3
· · ·
−A j
· · ·
0
−A0
I −A1
−A2
· · ·
−A j−1
· · ·
0
0
−A0
I −A1
· · ·
−A j−2
· · ·
0
0
0
−A0
· · ·
−A j−3
· · ·
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, . . . , 0, . . .).
(10.38)
The submatrix in the lower right is block Toeplitz. Bini and Meini have shown that there exists
a decomposition of this Toeplitz matrix into a block upper triangular matrix U and block lower
triangular matrix L with
U =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
A∗
1
A∗
2
A∗
3
A∗
4
· · ·
0
A∗
1
A∗
2
A∗
3
· · ·
0
0
A∗
1
A∗
2
· · ·
0
0
0
A∗
1
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
and
L =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I
0
0
0
· · ·
−G
I
0
0
· · ·
0
−G
I
0
· · ·
0
0
−G
I
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
We denote the nonzero blocks of U as A∗
i rather than Ui since we shall see later that these blocks
are formed using the Ai blocks of P. Once the matrix G has been formed then L is known. Observe
that the inverse of L can be written in terms of the powers of G. For example,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I
0
0
0
· · ·
−G
I
0
0
· · ·
0
−G
I
0
· · ·
0
0
−G
I
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I
0
0
0
· · ·
G
I
0
0
· · ·
G2
G
I
0
· · ·
G3
G2
G
I
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
0
· · ·
0
1
0
0
· · ·
0
0
1
0
· · ·
0
0
0
1
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
From Equation (10.38), we have
(π0, π1, . . . , π j, . . .)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
I −B00 −B01 −B02 −B03 · · · −B0 j · · ·
−B10
0
0
U L
0
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, . . . , 0, . . .),
which allows us to write
π0 (−B01, −B02, . . . ) + (π1, π2, . . . ) U L = 0
or
π0 (B01, B02, . . . ) L−1 = (π1, π2, . . . ) U,

348
Numerical Solution of Markov Chains
i.e.,
π0 (B01, B02, . . . )
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I
0
0
0
· · ·
G
I
0
0
· · ·
G2
G
I
0
· · ·
G3
G2
G
I
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
= (π1, π2, . . . ) U.
Forming the product of (B01, B02, . . . ) and L−1 leads to the important result
π0
#
B∗
01, B∗
02, . . .
$
= (π1, π2, . . . ) U,
(10.39)
where
B∗
01 = B01 + B02G + B03G2 + · · · =
∞

k=1
B0kGk−1,
B∗
02 = B02 + B03G + B04G2 + · · · =
∞

k=2
B0kGk−2,
...
B∗
0i = B0i + B0,i+1G + B0,i+2G2 + · · · =
∞

k=i
B0kGk−i.
The system of equations (10.39) will allow us to compute the successive components of the vector
π once the initial component π0 and the matrix U are known. To see this, write Equation (10.39) as
π0
#
B∗
01, B∗
02, · · ·
$
= (π1, π2, · · · )
⎛
⎜
⎜
⎜
⎜
⎜
⎝
A∗
1
A∗
2
A∗
3
A∗
4
· · ·
0
A∗
1
A∗
2
A∗
3
· · ·
0
0
A∗
1
A∗
2
· · ·
0
0
0
A∗
1
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
and observe that
π0B∗
01 = π1A∗
1
=⇒π1 = π0B∗
01A∗
1
−1,
π0B∗
02 = π1A∗
2 + π2A∗
1
=⇒π2 = π0B∗
02A∗
1
−1 −π1A∗
2 A∗
1
−1,
π0B∗
03 = π1A∗
3 + π2A∗
2 + π3A∗
1
=⇒π3 = π0B∗
03A∗
1
−1 −π1A∗
3 A∗
1
−1 −π2A∗
2 A∗
1
−1,
....
In general, we ﬁnd
πi =
#
π0B∗
0i −π1A∗
i −π2A∗
i−1 −· · · −πi−1 A∗
2
$
A∗
1
−1
=

π0B∗
0i −
i−1

k=1
πk A∗
i−k+1

A∗
1
−1,
i = 1, 2, . . . .
To compute the ﬁrst subvector π0 we return to
π0
#
B∗
01, B∗
02, . . .
$
= (π1, π2, . . . ) U

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
349
and write it as
(π0, π1, . . . , π j, . . .)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
I −B00 −B∗
01
−B∗
02
−B∗
03
· · ·
−B∗
0 j
· · ·
−B10
A∗
1
A∗
2
A∗
3
· · ·
A∗
j
· · ·
0
0
A∗
1
A∗
2
· · ·
A∗
j−1
· · ·
0
0
0
A∗
1
· · ·
A∗
j−2
· · ·
0
0
0
0
· · ·
...
· · ·
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, . . . , 0, . . .).
From the ﬁrst two equations, we have
π0 (I −B00) −π1B10 = 0
and
−π0B∗
01 + π1A∗
1 = 0.
This latter gives
π1 = π0B∗
01A∗
1
−1
which when substituted into the ﬁrst gives
π0 (I −B00) −π0B∗
01A∗
1
−1B10 = 0
or
π0

I −B00 −B∗
01A∗
1
−1B10

= 0,
from which we may now compute π0, but correct only to a multiplicative constant. It must be
normalized so that ∞
i=0 πi = 1 which may be accomplished by enforcing the condition
π0e + π0
 ∞

i=1
B∗
0i
  ∞

i=1
A∗
i
−1
e = 1.
(10.40)
We now turn our attention to the computation of the matrix U. Since
U L =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I −A1
−A2
−A3
· · ·
−A j
· · ·
−A0
I −A1
−A2
· · ·
−A j−1
· · ·
0
−A0
I −A1
· · ·
−A j−2
· · ·
0
0
−A0
· · ·
−A j−3
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
we have
⎛
⎜
⎜
⎜
⎜
⎜
⎝
A∗
1
A∗
2
A∗
3
A∗
4
· · ·
0
A∗
1
A∗
2
A∗
3
· · ·
0
0
A∗
1
A∗
2
· · ·
0
0
0
A∗
1
· · ·
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I −A1
−A2
−A3
· · ·
−A j
· · ·
−A0
I −A1
−A2
· · ·
−A j−1
· · ·
0
−A0
I −A1
· · ·
−A j−2
· · ·
0
0
−A0
· · ·
−A j−3
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I
0
0 0 · · ·
G
I
0 0 · · ·
G2 G
I
0 · · ·
G3 G2 G I · · ·
...
...
... ...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,

350
Numerical Solution of Markov Chains
and it is now apparent that
A∗
1 = I −A1 −A2G −A3G2 −A4G3 −· · · = I −
∞

k=1
AkGk−1,
A∗
2 = −A2 −A3G −A4G2 −A5G3 −· · · = −
∞

k=2
AkGk−2,
A∗
3 = −A3 −A4G −A5G2 −A6G3 −· · · = −
∞

k=3
AkGk−3,
...
A∗
i = −Ai −Ai+1G −Ai+2G2 −Ai+3G3 −· · · = −
∞

k=i
AkGk−i,
i ≥2.
We now have all the results we need. The basic algorithm is
• Construct the matrix G.
• Obtain π0 by solving the system of equations π0
#
I −B00 −B∗
01A∗
1
−1B10
$
= 0,
subject to the normalizing condition, Equation (10.40).
• Compute π1 from π1 = π0B∗
01A∗
1
−1.
• Find all other required πi from πi =

π0B∗
0i −i−1
k=1 πk A∗
i−k+1

A∗
1
−1,
where
B∗
0i =
∞

k=i
B0kGk−i, i ≥1;
A∗
1 = I −
∞

k=1
AkGk−1 and
A∗
i = −
∞

k=i
AkGk−i,
i ≥2.
This obviously gives rise to a number of computational questions. The ﬁrst is the actual
computation of the matrix G. We mentioned previously that it can be obtained from its deﬁning
equation by means of the iterative procedure
G(0) = 0,
G(k+1) =
∞

i=0
AiGi
(k),
k = 0, 1, . . . .
However, this is rather slow. Neuts proposed a variant that converges faster, namely,
G(0) = 0;
G(k+1) = (I −A1)−1

A0 +
∞

i=2
AiGi
(k)

,
k = 0, 1, . . .
Among ﬁxed point iterations such as these, the following suggested by Bini and Meini [3, 36], has
the fastest convergence:
G(0) = 0,
G(k+1) =

I −
∞

i=1
AiGi−1
(k)
−1
A0,
k = 0, 1, . . . .
Nevertheless, ﬁxed point iterations can be very slow in certain instances. More advanced techniques
based on cyclic reduction have been developed and converge much faster.
The second major problem is the computation of the inﬁnite summations that appear in the
formulae. Frequently the structure of the matrix is such that Ak and Bk are zero for relatively small
(single-digit integer) values of k, so that forming these summations is not too onerous. In all cases,
the fact that ∞
k=0 Ak and ∞
k=0 Bk are stochastic implies that the matrices Ak and Bk are negligibly
small for large values of k and can be set to zero once k exceeds some threshold kM. In this case, we

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
351
take kM
k=0 Ak and kM
k=0 Bk to be stochastic. More precisely, a nonnegative matrix P is said to be
numerically stochastic if Pe < μe where μ is the precision of the computer. When kM is not small,
ﬁnite summations of the type kM
k=i AkGk−i should be evaluated using Horner’s rule. For example,
if i = 1 and kM = 5:
A∗
1 =
5

k=1
AkGk−1 = A1 + A2G + A3G2 + A4G3 + A5G4
should be evaluated from the innermost parentheses outward as
A∗
1 = A1 + (A2 + (A3 + (A4 + A5G)G)G)G.
Example 10.29 We shall apply the matrix analytic method to the Markov chain of Example 10.27,
now modiﬁed so that it incorporates additional transitions (ζ1 = 1/48 and ζ2 = 1/16) to higher-
numbered non-neighboring states. The state transition diagram is shown in Figure 10.3.
 1 
 λ  1 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 λ 
 1 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 γ 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 2 
 γ 
 γ 
 1 
 2 
 μ 
 μ 
 μ 
 μ 
 μ 
0,1
0,2
1,1
1,2
1,3
2,1
2,2
2,3
3,1
3,2
3,3
4,1
4,2
4,3
5,1
5,2
5,3
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ 
 ζ  1 
 1 
 1 
 1 
 1 
 1 
 2 
 2 
 2 
 2 
 2 
 2 
 μ/2 
 μ/2 
 1 
Figure 10.3. State transition diagram for an M/G/1-type process.
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
23/48
5/12
1/12
1/48
1/4
31/48
1/24
1/16
23/48
5/12
1/12
1/48
1/3
1/3
1/4
1/12
1/4
31/48
1/24
1/16
23/48
5/12
1/12
2/3
1/4
1/12
1/4
31/48
1/24
23/48
5/12
2/3
1/4
1/12
1/4
31/48
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We have the following block matrices:
A0 =
⎛
⎝
0
0
0
0
2/3
0
0
0
0
⎞
⎠, A1 =
⎛
⎝
23/48
5/12
0
1/4
0
1/12
0
1/4
31/48
⎞
⎠, A2 =
⎛
⎝
1/12
0
0
0
0
0
0
0
1/24
⎞
⎠,
A3 =
⎛
⎝
1/48
0
0
0
0
0
0
0
1/16
⎞
⎠,
B00 =

23/48
5/12
1/4
31/48

,
B01 =

1/12
0
0
0
0
1/24

,

352
Numerical Solution of Markov Chains
B02 =
1/48
0
0
0
0
1/16

,
and
B10 =
⎛
⎝
0
0
1/3
1/3
0
0
⎞
⎠.
First, using Equation (10.37), we verify that the Markov chain with transition probability matrix
P is positive recurrent,
A = A0 + A1 + A2 + A3 =
⎛
⎝
.583333
.416667
0
.250000
.666667
.083333
0
.250000
.750000
⎞
⎠,
from which we can compute
πA = (.310345, .517241, .172414).
Also
b = (A1 + 2A2 + 3A3)e =
⎛
⎝
.708333
.416667
0
.250000
0
.083333
0
.250000
.916667
⎞
⎠
⎛
⎝
1
1
1
⎞
⎠=
⎛
⎝
1.125000
0.333333
1.166667
⎞
⎠.
Since
πA b = (.310345, .517241, .172414)
⎛
⎝
1.125000
0.333333
1.166667
⎞
⎠= .722701 < 1
the Markov chain is positive recurrent.
Let us consider the computation of the matrix G. As we noted previously, the i j element of G is
the conditional probability that starting in state i of any level n ≥2, the process enters level n −1
for the ﬁrst time by arriving at state j of that level. What this means for this particular example is
that the elements in column 2 of G must all be equal to 1 and all other elements must be zero, since
the only transitions from any level n to level n −1 are from and to the second element. Nevertheless,
it is interesting to see how each of the three different ﬁxed point formula given for computing G
actually perform on this example. The fact that we know the answer in advance will help in this
comparison. We take the initial value G(0) to be zero.
Formula 1: G(k+1) = ∞
i=0 AiGi
(k),
k = 0, 1, . . .,
G(k+1) = A0 + A1G(k) + A2G2
(k) + A3G3
(k).
After ten iterations, the computed matrix is
G(10) =
⎛
⎝
0
.867394
0
0
.937152
0
0
.766886
0
⎞
⎠.
Formula 2: G(k+1) = (I −A1)−1 #
A0 + ∞
i=2 AiGi
(k)
$
,
k = 0, 1, . . .,
G(k+1) = (I −A1)−1 #
A0 + A2G2
(k) + A3G3
(k)
$
.
In this case, after ten iterations, the computed matrix is
G(10) =
⎛
⎝
0
.999844
0
0
.999934
0
0
.999677
0
⎞
⎠.
Formula 3: G(k+1) =
#
I −∞
i=1 AiGi−1
(k)
$−1 A0,
k = 0, 1, . . .,
G(k+1) =
#
I −A1 −A2G(k) −A3G2
(k)
$−1 A0.

10.6 The Matrix Geometric/Analytic Methods for Structured Markov Chains
353
This is the fastest of the three and after ten iterations we have
G(10) =
⎛
⎝
0
.999954
0
0
.999979
0
0
.999889
0
⎞
⎠.
We now continue with this example and shall use the exact value of G in the remaining parts of
the algorithm. In preparation, we compute the following quantities, using the fact that Ak = 0 for
k > 3 and B0k = 0 for k > 2:
A∗
1 = I −
∞

k=1
AkGk−1 = I −A1 −A2G −A3G2 =
⎛
⎝
.520833
−.520833
0
−.250000
1
−.083333
0
−.354167
.354167
⎞
⎠,
A∗
2 = −
∞

k=2
AkGk−2 = −(A2 + A3G) =
⎛
⎝
−.083333
−.020833
0
0
0
0
0
−.062500
−.041667
⎞
⎠,
A∗
3 = −
∞

k=3
AkGk−3 = −A3 =
⎛
⎝
−.020833
0
0
0
0
0
0
0
−.062500
⎞
⎠,
B∗
01 =
∞

k=1
B0kGk−1 = B01 + B02G =

.083333
.020833
0
0
.062500
.041667

,
B∗
02 =
∞

k=2
B0kGk−2 = B02 =
.020833
0
0
0
0
.062500

,
A∗
1
−1 =
⎛
⎝
2.640
1.50
.352941
.720
1.50
.352941
.720
1.50
3.176470
⎞
⎠.
We may now compute the initial subvector π0 from
0 = π0

I −B00 −B∗
01A∗
1
−1B10

= π0

.468750
−.468750
−.302083
.302083

,
from which we ﬁnd
π0 = (.541701, .840571).
We now normalize this so that
π0e + π0
 ∞

i=1
B∗
0i
  ∞

i=1
A∗
i
−1
e = 1,
i.e.,
π0e + π0
#
B∗
01 + B∗
02
$ #
A∗
1 + A∗
2 + A∗
3
$−1 e = 1.

354
Numerical Solution of Markov Chains
Evaluating,
#
B∗
01 + B∗
02
$ #
A∗
1 + A∗
2 + A∗
3
$−1 =
.104167
.020833
0
0
.062500
.104167

×
⎛
⎝
.416667
−.541667
0
−.250000
1
−.083333
0
−.416667
.250000
⎞
⎠
−1
=

.424870
.291451
.097150
.264249
.440415
.563472

.
Thus
(.541701, .840571)

1
1

+ (.541701, .840571)

.424870 .291451 .097150
.264249 .440415 .563472
 ⎛
⎝
1
1
1
⎞
⎠= 2.888888,
which leads to
π0 = (.541701, .840571)/2.888888 = (.187512, .290967)
as the initial subvector.
We can now ﬁnd π1 from the relationship π1 = π0B∗
01A∗
1
−1. This gives
π1 = (.187512, .290967)
.083333
.020833
0
0
.062500
.041667
 ⎛
⎝
2.640
1.50
.352941
.720
1.50
.352941
.720
1.50
3.176470
⎞
⎠
= (.065888, .074762, .0518225).
Finally, all needed remaining subcomponents of π can be found from
πi =

π0B∗
0i −
i−1

k=1
πk A∗
i−k+1

A∗
1
−1.
For example, we have
π2 =
#
π0B∗
02 −π1A∗
2
$
A∗
1
−1
= (.042777, .051530, .069569),
π3 =
#
π0B∗
03 −π1A∗
3 −π2A∗
2
$
A∗
1
−1 =
#
−π1A∗
3 −π2A∗
2
$
A∗
1
−1
= (.0212261, .024471, .023088),
π4 =
#
π0B∗
04 −π1A∗
4 −π2A∗
3 −π3A∗
2
$
A∗
1
−1 =
#
−π2 A∗
3 −π3A∗
2
$
A∗
1
−1
= (.012203, .014783, .018471),
....
The probability that the Markov chain is in any level i is given by ∥πi∥1. The probabilities of this
Markov chain being in the ﬁrst ﬁve levels are given as
∥π0∥1 = .478479, ∥π1∥1 = .192473, ∥π2∥1 = .163876, ∥π3∥1 = .068785, ∥π4∥1 = .045457.
The sum of these ﬁve probabilities is 0.949070.
10.7 Transient Distributions
So far, our concern has been with the computation of the stationary distribution π of discrete- and
continuous-time Markov chains. We now turn our attention to the computation of state probability

10.7 Transient Distributions
355
distributions at an arbitrary point of time. In the case of a discrete-time Markov chain, this means
ﬁnding the distribution at some arbitrary time step n. This distribution is denoted π(n), a row vector
whose ith component is the probability that the Markov chain is in state i at time step n. As we saw
previously, it satisﬁes the relationship
π(n) = π(n−1)P(n −1) = π(0)P(0)P(1) · · · P(n −1),
where P(i) is the transition probability matrix at step i. For a homogeneous discrete-time Markov
chain, this reduces to
π(n) = π(n−1)P = π(0)Pn,
where now P(0) = P(1) = · · · = P. In this section, we shall consider only homogeneous Markov
chains. For a continuous-time Markov chain with inﬁnitesimal generator Q, we seek the distribution
at any time t. Such a distribution is denoted π(t), a row vector whose component πi(t) is the
probability that the Markov chain is in state i at time t. We have previously seen that this vector
satisﬁes the relationship
π(t) = π(0)eQt,
where eQt is the matrix exponential deﬁned by
eQt =
∞

k=0
(Qt)k/k!.
In both cases, what is usually required is seldom the probability distribution π(n) or π(t) itself, but
rather some linear combination of the components of these vectors, such as the probability that the
Markov chain is in a single state i, (π(n)ei or π(t) ei, where ei is a column vector whose elements are
all zero except the ith, which is equal to 1) or the probability that the Markov chain is in a subset of
states Ei
#
i∈Ei π(n)ei or 
i∈Ei π(t)ei
$
or yet again a weighted sum of the probabilities wherein
the unit component of ei is replaced by an arbitrary scaler. Also, the evolution of this statistic from
the initial time to the desired time step n or time t may be required rather than just its (single) value
at the ﬁnal time point.
The computation of transient distributions of discrete-time Markov chains rarely poses major
problems. The procedure consists of repeatedly multiplying the probability distribution vector
obtained at step k −1 with the stochastic transition probability matrix to obtain the probability
distribution at step k, for k = 1, 2, . . . , n. If n is large and the number of states in the Markov chain
is small (not exceeding several hundreds), then some savings in computation time can be obtained
by successively squaring the transition probability matrix j times, where j is the largest integer such
that 2 j ≤n. This gives the matrix P2 j which can now be multiplied by P (and powers of P) until
the value Pn is obtained. The distribution at time step n is now found from π(n) = π(0)Pn. If the
matrix P is sparse, this sparsity is lost in the computation of Pn so this approach is not appropriate
for large sparse Markov chains. Also, if a time trajectory of a statistic of the distribution is needed,
this approach may be less than satisfactory, because only distributions at computed values of P2 j
will be available. One ﬁnal point worth noting is that for large values of n, it may be beneﬁcial to
maintain a check on convergence to the stationary distribution, since this may occur, correct to some
desired computational accuracy, prior to step n. Any additional vector–matrix multiplications after
this point will not alter the distribution.

356
Numerical Solution of Markov Chains
Example 10.30 Let us consider a discrete-time Markov chain with transition probability matrix P
given by
P =
⎛
⎜
⎜
⎝
.4
0
.6
0
.0002
.3
0
.6998
.1999
.0001
.8
0
0
.5
0
.5
⎞
⎟
⎟
⎠.
Let us assume that this Markov chain starts in state 1, i.e., π(0) = (1, 0, 0, 0), and the value of being
in the different states at some time step n is given by the vector a = (0, 4, 0, 10)T . In other words,
the Markov chain in states 1 and 3 is worthless, but worth 4 (arbitrary units) in state 2 and 10 in
state 4. The value at time step n is π(n)a and we now wish to compute this statistic for various
values of n.
For small values of n = 1, 2, . . ., it sufﬁces to compute π(1) = π(0)P, π(2) = π(1)P, π(3) =
π(2)P, . . ., and we ﬁnd
π(1) = (.4, 0, .6, 0),
π(2) = (.27994, .00006, .7200, 0),
π(3) = (.255904, .00009, .743964, .000042),
π(4) = (.251080, .000122, .748714, .000084),
...,
which gives the following values of the statistic π(n)a:
0,
.00024,
.000780,
.001329, . . . .
These values can be plotted to show the evolution of the statistic over a number of time steps.
If the transient distribution is required at much greater values of n, then advantage should be
taken of the small size of the matrix to compute successive powers of the matrix. For example, if
the distribution is required at time step 1000, then P1000 can be found from powering the matrix
P. If, in addition, we are asked to plot the behavior of the statistic, until that time, we may wish to
compute
π(100) = (.248093, .003099, .744507, .004251),
π(200) = (.246263, .006151, .739062, .008524),
π(300) = (.244461, .009156, .733652, .012731),
π(400) = (.242688, .012113, .728328, .016871),
...
π(1000) = (.232618, .028906, .698094, .040382),
which gives the following values of the statistic π(n)a:
.054900,
.109846,
.163928,
.217161,
. . . ,
.519446.
Finally, we point out that the distribution at time step n = 1, 000 is a long way from the
stationary distribution given by π = (.131589, .197384, .394768, .276259), which has the statistic
πa = 3.552123.
In the remainder of this section, we devote our attention to the computation of transient distributions
of continuous-time Markov chains, i.e., the computation of π(t) from
π(t) = π(0)eQt,

10.7 Transient Distributions
357
where Q is the inﬁnitesimal generator of an irreducible continuous-time Markov chain. Depending
on the numerical approach adopted, π(t) may be computed by ﬁrst forming eQt and then
premultiplying this with the initial probability vector π(0). Such is the case with the matrix-scaling
and -powering methods described in Section 10.7.1. They are most suitable when the transition
rate matrix is small. The Matlab function expm falls into this category. In other cases, π(t) may be
computed directly without explicitly forming eQt. This is the approach taken by the uniformization
method of Section 10.7.2 and the ordinary differential equation (ODE) solvers of Section 10.7.3.
Both are suitable for large-scale Markov chains. Indeed, the uniformization method is a widely used
and often extremely efﬁcient approach that is applicable both to small transition rate matrices that
may be either dense or sparse and to large sparse transition matrices. It is less suitable for stiff
Markov chains. Methods for solving systems of ordinary differential equations have been actively
researched by the numerical analysis community and provide an alternative to the uniformization
method. In this section we suggest both single-step and multistep methods as possible ODE solution
approaches.
10.7.1 Matrix Scaling and Powering Methods for Small State Spaces
Moler and Van Loan [37] discuss nineteen dubious ways to compute the exponential of a relatively
small-order matrix. A major problem in all these methods is that the accuracy of the approximations
depends heavily on the norm of the matrix. Thus, when the norm of Q is large or t is large,
attempting to compute eQt directly is likely to yield unsatisfactory results. It becomes necessary
to divide the interval [0, t] into subintervals (called panels) [0, t0], [t0, t1], . . . , [tm−1, tm = t] and to
compute the transient solution at each time t j, j = 0, 1, . . . , m using the solution at the beginning
of the panel as the starting point. It often happens that this is exactly what is required by a user who
can, as a result, see the evolution of certain system performance measures with time.
Matrix-scaling and -powering methods arise from a property that is unique to the exponential
function, namely,
eQt =
#
eQt/2$2 .
(10.41)
The basic idea is to compute eQt0 for some small value t0 such that t = 2mt0 and subsequently to
form eQt by repeated application of the relation (10.41). It is in the computation of the initial eQt0
that methods differ, and we shall return to this point momentarily.
Let Q be the inﬁnitesimal generator of an ergodic, continuous-time Markov chain, and let π(0)
be the probability distribution at time t = 0. We seek π(t), the transient solution at time t. Let us
introduce an integer m and a time t0 ̸= 0 for which t = 2mt0. Then
π(t) = π(2mt0).
Writing t j = 2t j−1, we shall compute the matrices eQt j for j = 0, 1, . . . , m and consequently,
by multiplication with π(0), the transient solution at times t0, 2t0, 22t0, . . . , 2mt0 = t. Note that
P(t j) ≡eQt j is a stochastic matrix and that, from the Chapman–Kolmogorov equations,
P(t j) = P(t j−1)P(t j−1).
(10.42)
Thus, once P(t0) has been computed, each of the remaining P(t j) may be computed from Equation
(10.42) by squaring the previous P(t j−1). Thus matrix-powering methods, in the course of their
computation, provide the transient solution at the intermediate times t0, 2t0, 22t0, . . . , 2m−1t0.
However, a disadvantage of matrix-powering methods, besides computational costs proportional
to n3 and memory requirements of n2, is that repeated squaring may induce rounding error buildup,
particularly in instances in which m ≫1.

358
Numerical Solution of Markov Chains
Example 10.31 Suppose we need to ﬁnd the transition distribution at some time t = 10, of a
Markov chain with inﬁnitesimal generator
Q =
⎛
⎜
⎜
⎝
−.6
0
.6
0
.1
−.9
.1
.7
.4
.3
−.8
.1
0
.5
0
−.5
⎞
⎟
⎟
⎠.
To use the matrix-scaling and -powering method, we need to ﬁnd a t0 such that t0 is small and there
exists an integer m so that t = 2mt0. Setting m = 5 and solving for t0 gives t0 = 10/32 which we
take to be sufﬁciently small for the purpose of this example. The reader may now wish to verify
that
P(t0) = e10Q/32 =
⎛
⎜
⎜
⎝
.838640
.007076
.151351
.002933
.026528
.769552
.026528
.177393
.102080
.074581
.789369
.033970
.002075
.126413
.002075
.869437
⎞
⎟
⎟
⎠
and that performing the operations P(t j) = P(t j−1)P(t j−1) for j = 1, 2, . . . , 5 gives the result
P(t5) =
⎛
⎜
⎜
⎝
.175703
.265518
.175700
.383082
.136693
.289526
.136659
.437156
.161384
.274324
.161390
.402903
.129865
.293702
.129865
.446568
⎞
⎟
⎟
⎠= e10Q.
When premultiplied by an initial probability distribution, the distribution at time t = 10 can be
found.
We now turn our attention to the computation of eQt0. Since t0 is small, methods based on
approximations around zero are possible candidates. One good choice is that of rational Padé
approximations around the origin. The (p, q) Padé approximant to the matrix exponential eX is,
by deﬁnition, the unique (p, q) rational function Rpq(X),
Rpq(X) ≡Npq(X)
Dpq(X),
which matches the Taylor series expansion of eX through terms to the power p + q. Its coefﬁcients
are determined by solving the algebraic equations
∞

j=0
X j
j! −Npq(X)
Dpq(X) = O
#
X p+q+1$
,
which yields
Npq(X) =
p

j=0
(p + q −j)!p!
(p + q)! j!(p −j)! X j
and
Dpq(X) =
q

j=0
(p + q −j)!q!
(p + q)! j!(q −j)!(−X) j.
For more detailed information on Padé approximants, the interested reader should consult the text
by Baker [1]. A major disadvantage of Padé approximants is that they are accurate only near the
origin and so should not be used when ∥X∥2 is large. However, since we shall be using them in the

10.7 Transient Distributions
359
context of a matrix-scaling and -powering procedure, we may (and shall) choose t0 so that ∥Qt0∥2
is sufﬁciently small that the Padé approximant to eQt0 may be obtained with acceptable accuracy,
even for relatively low-degree approximants.
When p = q, we obtain the diagonal Padé approximants, and there are two main reasons why
this choice is to be recommended. First, they are more stable [37]. In Markov chain problems, all the
eigenvalues of X = Qt are to be found in the left half plane. In this case the computed approximants
Rpq(X) for p ̸= q have larger rounding errors, because either p > q and cancellation problems may
arise, or p < q and Dpq(X) may be badly conditioned. Second, we obtain a higher-order method
with the same amount of computation. To compute Rpq(X) with p < q requires about qn3 ﬂops
and yields an approximant that has order p + q. To compute Rqq(X) requires essentially the same
number of ﬂops but produces an approximant of order 2q > p+q. Similar statements may be made
when p > q.
For diagonal Padé approximants we ﬁnd
Rpp(X) =
Npp(X)
Npp(−X),
(10.43)
where
Npp(X) =
p

j=0
(2p −j)!p!
(2p)! j!(p −j)! X j ≡
p

j=0
c j X j.
The coefﬁcients c j can be conveniently constructed by means of the recursion
c0 = 1;
c j = c j−1
p + 1 −j
j(2p + 1 −j).
For actual implementation purposes, the following irreducible form offers considerable savings in
computation time at the expense of additional memory locations:
Rpp(X) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
I + 2
X p/2−1
k=0
c2k+1X2k
p/2
k=0 c2k X2k −X p/2−1
k=0
c2k+1X2k
if p is even,
−I −2
(p−1)/2
k=0
c2k X2k
X (p−1)/2
k=0
c2k+1X2k −(p−1)/2
k=0
c2k X2k
if p is odd.
(10.44)
Thus, for even values of p,
Rpp(X) = I + 2
Se
Te −Se
,
where
Se = c1X + c3X3 + · · · + cp−1X p−1
and
Te = c0 + c2X2 + c4X4 + · · · + cpX p,
while for odd values of p,
Rpp(X) = −

I + 2
So
To −So

,
where now
So = c0 + c2X2 + c4X4 + · · · + cp−1X p−1
and
To = c1X + c3X3 + · · · + cpX p.
These computations may be conveniently combined, and they cry out for a Horner-type evaluation
procedure. Indeed, Horner evaluations of the numerator and the denominator in Equation (10.44)
need only one-half the operations of a straightforward implementation of Equation (10.43).

360
Numerical Solution of Markov Chains
The following four steps, adapted from that presented in [42], implements a Padé variant of
the matrix-powering and -scaling approach for the computation of eX. In this implementation, the
integer m is chosen as m = ⌊log ∥X∥∞/ log 2⌋+ 1. To compute the transient solution at time t
of a Markov chain with generator Q and initial state π(t0), it sufﬁces to apply this algorithm with
X = Qt and then to form π(t0)R, where R is the approximation to eX computed by the algorithm.
1. Find appropriate scaling factor:
• Compute m = max(0, ⌊log ∥X∥∞/ log 2⌋+ 1).
2. Compute coefﬁcients and initialize:
• Set c0 = 1.
• For j = 1, 2, . . . , p do
◦Compute c j = c j−1 × (p + 1 −j)/( j(2p + 1 −j)).
• Compute X1 = 2−m X; X2 = X12; T = cpI; S = cp−1I.
3. Application of Horner scheme:
• Set odd = 1.
• For j = p −1, . . . , 2, 1 do
◦if odd = 1, then
∗Compute T = T × X2 + c j−1I;
else
∗Compute S = S × X2 + c j−1I.
◦Set odd = 1 −odd.
• If odd = 0, then
◦Compute S = S × X1; R = I + 2 × (T −S)−1 × S;
else
◦Compute T = T × X1; R = −(I + 2 × (T −S)−1 × S).
4. Raise matrix to power 2m by repeated squaring:
• For j = 1 to m do
◦Compute R = R × R.
This Padé approximation for eX requires a total of approximately (p +m + 4
3)n3 multiplications.
It may be implemented with three double-precision arrays, each of size n2, in addition to the storage
required for the matrix itself.
This leaves us with the choice of p. In the appendix of [37], a backward error analysis of the
Padé approximation is presented, in which it is shown that if ∥X∥2/2m ≤1
2, then

Rpp(2−m X)
 2m
= eX+E,
where
∥E∥2
∥X∥2
≤
1
2
2p−3
(p!)2
(2p)!(2p + 1)!
≈
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0.77 × 10−12 (p = 5),
0.34 × 10−15 (p = 6),
0.11 × 10−18 (p = 7),
0.27 × 10−22 (p = 8).
(10.45)
This suggests that relatively low-degree Padé approximants are adequate. However, the above
analysis does not take rounding error into account. This aspect has been examined by Ward [54],
who proposes certain criteria for selecting appropriate values for some computers. Additionally, a
discussion on “the degree of best rational approximation to the exponential function” is provided
by Saff [48]. Finally, numerical experiments on Markov chains by Philippe and Sidje [42] ﬁnd that
even values of p are better than odd values and that the value p = 6 is generally satisfactory.

10.7 Transient Distributions
361
10.7.2 The Uniformization Method for Large State Spaces
For large-scale Markov chains, methods currently used to obtain transient solutions are based
either on readily available differential equation solvers such as Runge-Kutta methods or the Adams
formulae and backward differentiation formulae (BDF) or on the method of uniformization (also
called Jensen’s method or the method of randomization). Most methods experience difﬁculty when
both max j |q j j| (the largest exit rate from any state) and t (the time at which the solution is
required) are large, and there appears to be little to recommend a single method for all situations.
In this section we discuss the uniformization method. This method has attracted much attention, is
extremely simple to program and often outperforms other methods, particularly when the solution
is needed at a single time point close to the origin. If the solution is required at many points, or if
plots need to be drawn to show the evolution of certain performance measures, then a method based
on one of the differential equation solvers may be preferable. An extensive discussion on all these
methods may be found in [50].
The uniformization method revolves around a discrete-time Markov chain that is embedded in the
continuous-time process. The transition probability matrix of this discrete-time chain is constructed
as
P = Q
t + I
with 
t ≤1/ maxi |qii|. In this Markov chain all state transitions occur at a uniform rate equal to
1/
t—hence the name uniformization. Letting γ = maxi |qii|, we may write
Q = γ (P −I)
and inserting this into the Kolmogorov forward differential equations we get
π(t) = π(0)eQt = π(0)eγ (P−I)t = π(0)e−γ teγ Pt.
Expanding eγ Pt in a Taylor series, we obtain
π(t) = π(0)e−γ t
∞

k=0
(γ t)k Pk
k!
,
i.e.,
π(t) =
∞

k=0
π(0)Pk (γ t)k
k! e−γ t.
(10.46)
Two observations may be made. First, the term π(0)Pk may be recognized as the vector that
provides the probability distribution after k steps of the discrete-time Markov chain whose stochastic
transition probability matrix is P and with initial distribution π(0). Second, the term e−γ t(γ t)k/k!
may be recognized from the Poisson process with rate γ as the probability of k events occurring in
[0, t). It is the probability that the discrete-time Markov chain makes k transition steps in the interval
[0, t). These probabilities may be interpreted as weights that when multiplied with the distribution
of the discrete-time Markov chain after k steps and summed over all possible number of steps, (in
effect, unconditioning the transient distribution which has been written in terms of power series of
P) yields the transient distribution π(t). The uniformization method computes the distribution π(t)
directly from Equation (10.46). Writing it in the form
π(t) = e−γ t
∞

k=0

π(0)Pk−1 (γ t)k−1
(k −1)!

P γ t
k
(10.47)

362
Numerical Solution of Markov Chains
exposes a convenient recursive formulation. Setting y = π = π(0) and iterating sufﬁciently long
with
y = y

P γ t
k

,
π = π + y,
allows the transient distribution to be computed as π(t) = e−γ t π.
The curious reader may wonder why we bother with Equation (10.46) when π(t) might be
thought to be more easily computed directly from the Chapman-Kolmogorov differential equations
by means of the formula
π(t) = π(0)
∞

k=0
(Qt)k
k!
.
However, the matrix Q contains both positive and negative elements and these may be greater than
one which leads to a less stable algorithm than one based on the matrix P which, being a stochastic
matrix, has all positive elements lying in the range [0, 1].
Among the numerical advantages of the uniformization technique is the ease with which it can be
translated into computer code and the control it gives over the truncation error. Let us ﬁrst discuss
the truncation error. In implementing the uniformization method, we need to truncate the inﬁnite
series in (10.46). Let
π∗(t) =
K

k=0
π(0)Pke−γ t(γ t)k/k!
(10.48)
and let δ(t) = π(t) −π∗(t). For any consistent vector norm, ||δ(t)|| is the truncation error. It is not
difﬁcult to numerically bound this error. If we choose K sufﬁciently large that
1 −
K

k=0
e−γ t(γ t)k/k! ≤ϵ,
or, equivalently, that
K

k=0
(γ t)k
k!
≥1 −ϵ
e−γ t = (1 −ϵ)eγ t,
(10.49)
where ϵ is some prespeciﬁed truncation criterion, then it follows that
∥π(t) −π∗(t)∥∞≤ϵ.
To see this, observe that
@@π(t) −π∗(t)
@@
∞
=
@@@@@
∞

k=0
π(0)Pke−γ t(γ t)k/k! −
K

k=0
π(0)Pke−γ t(γ t)k/k!
@@@@@
∞
=
@@@@@
∞

k=K+1
π(0)Pke−γ t(γ t)k/k!
@@@@@
∞
≤
∞

k=K+1
e−γ t(γ t)k/k!
=
∞

k=0
e−γ t(γ t)k/k! −
K

k=0
e−γ t(γ t)k/k!

10.7 Transient Distributions
363
= 1 −
K

k=0
e−γ t(γ t)k/k! ≤ϵ.
Example 10.32 Consider a continuous-time Markov chain with inﬁnitesimal generator
Q =
⎛
⎝
−5
2
3
1
−2
1
6
4
−10
⎞
⎠.
Given π(0) = (1, 0, 0) and t = 1, we wish to examine the behavior of Equation (10.46) as the
number of terms in the summation increases. The uniformization process gives γ = 10 and
P =
⎛
⎝
.5
.2
.3
.1
.8
.1
.6
.4
0
⎞
⎠.
Suppose we require an accuracy of ϵ = 10−6. To ﬁnd the value of K, the number of terms to be
included in the summation, we proceed on a step-by-step basis, incrementing k until
σK =
K

k=0
(γ t)k
k!
≥(1 −ϵ)eγ t = (1 −10−6)e10 = 22, 026.4438.
Observe that successive terms in the summation satisfy
ξK+1 = ξK
γ t
K + 1
with ξ0 = 1
and that
σK+1 = σK + ξK+1
with σ0 = 1,
and so, beginning with K = 0, and using this recursion, we successively compute
σ0 = 1, σ1 = 11, σ2 = 61, σ3 = 227.6667, σ4 = 644.33331, . . . , σ28 = 22, 026.4490.
Thus K = 28 terms are needed in the summation. To obtain our approximation to the transient
distribution at time t = 1, we need to compute
π(t) ≈
28

k=0
π(0)Pke−γ t (γ t)k
k!
(10.50)
for some initial distribution, π(0) = (1, 0, 0), say. Using the recursion relation of Equation (10.47),
we ﬁnd
k = 0 :
y = (1, 0, 0);
π = (1, 0, 0),
k = 1 :
y = (5, 2, 3);
π = (6, 2, 3),
k = 2 :
y = (22.5, 19, 8.5);
π = (28.5, 21, 11.5),
k = 3 :
y = (60.8333, 77, 28.8333);
π = (89.3333, 98, 40.3333),
...
k = 28 :
y = (.009371, .018742, .004686);
π = (6, 416.9883, 12, 424.44968, 3, 184.4922).
In this example, the elements of y increase until they reach y = (793.8925, 1, 566.1563, 395.6831)
for k = 10 and then they begin to decrease. Multiplying the ﬁnal π vector by e−10 produces the
desired transient distribution:
π(1) = e−10(6, 416.9883, 12, 424.44968, 3, 184.4922) = (.291331, .564093, .144576).

364
Numerical Solution of Markov Chains
In implementing the uniformization technique, we may code Equation (10.46) exactly as it
appears, with the understanding that π(0)Pk is computed iteratively, i.e., we do not construct
the kth power of P and premultiply it with π(0), but instead form the sequence of vectors,
ψ( j + 1) = ψ( j)P, with ψ(0) = π(0), so that π(0)Pk is given by ψ(k). Alternatively we may
partition Equation (10.46) into time steps 0 = t0, t1, t2, ..., tm = t and write code to implement
π(ti+1) =
∞

k=0
π(ti)Pke−γ (ti+1−ti)γ k(ti+1 −ti)k/k!
recursively for i = 0, 1, ..., m −1. This second approach is the obvious way to perform the
computation if the transient solution is required at various points t1, t2, ... between the initial time
t0 and the ﬁnal time t. It is computationally more expensive if the transient solution is required only
at a single terminal point. However, it may prove useful when the numerical values of γ and t are
such that the computer underﬂows when computing e−γ t. Such instances can be detected a priori
and appropriate action taken. For example, one may decide not to allow values of γ t to exceed
100. When such a situation is detected, the time t may be divided into d = 1 + ⌊γ t/100⌋equal
intervals and the transient solution computed at times t/d, 2t/d, 3t/d, . . . , t. Care must be taken
in implementing such a procedure since the error in the computation of the intermediate values π(ti)
may propagate along into later values, π(t j), j > i.
An alternative to dividing a large interval γ t into more manageable pieces may be to omit from
the summation in Equation (10.48), terms for which the value of e−γ t(γ t)k/k! is so small that it
could cause numerical difﬁculties. This may be accomplished by choosing a left truncation point, l,
as the largest value for which
l−1

k=0
e−γ t (γ t)k
k!
≤ϵl
for some lower limit ϵl. The required transient distribution vector is now computed as
π∗(t) =
K

k=l
ψ(k)e−γ t(γ t)k/k!
with ψ(k) computed recursively as before. The value of l is easily computed using the same
procedure that was used to compute the upper limit of the summation, K. The amount of work
involved in summing from k = l is basically just the same as that in summing from k = 0, since
ψ(k) from k = 0 to k = K must be computed in all cases. The only reason in summing from k = l
is that of computational stability.
One other wrinkle may be added to the uniformization method when it is used to compute
transient distributions at large values of γ t—the stationary distribution of the uniformized chain
may be reached well before the last term in the summation (10.48). If this is the case, it means that,
from the point at which steady state is reached, the values ψ(k) no longer change. It is possible to
monitor convergence of the uniformized chain and to determine the point at which it reaches steady
state. Assume this occurs when k has the value ks. The transient distribution at time t may then be
computed more efﬁciently as
π∗(t) =
ks

k=l
ψ(k)e−γ t(γ t)k/k! +

K

k=ks+1
e−γ t(γ t)k/k!

ψ(ks).
The following two step implementation computes the transient solution π(t) at time t given the
probability distribution π(0) at time t = 0; P, the stochastic transition probability matrix of the
discrete-time Markov chain; γ , the parameter of the Poisson process; and ϵ, a tolerance criterion.
It is designed for the case in which the number of states in the Markov chain is large. The only

10.7 Transient Distributions
365
operation involving the matrix P is its multiplication with a vector. This implementation does not
incorporate a lower limit nor a test for convergence to the steady-state.
1. Use Equation (10.49) to compute K, the number of terms in the summation:
• Set K = 0; ξ = 1; σ = 1; η = (1 −ϵ)/e−γ t.
• While σ < η do
◦Compute K = K + 1; ξ = ξ × (γ t)/K; σ = σ + ξ.
2. Approximate π(t) from Equation (10.48):
• Set π = π(0); y = π(0).
• For k = 1 to K do
◦Compute y = yP × (γ t)/k; π = π + y.
• Compute π(t) = e−γ tπ.
10.7.3 Ordinary Differential Equation Solvers
The transient distribution we seek to compute, π(t) = π(0)eQt, is the solution of the Chapman–
Kolmogorov differential equations
dπ(t)
dt
= π(t)Q
with initial conditions π(t = 0) = π(0). The solution of ordinary differential equations (ODEs) has
been (and continues to be) a subject of extensive research, and there are numerous possibilities
for applying ODE procedures to compute transient solutions of Markov chains. An immediate
advantage of such an approach is that, unlike uniformization and matrix scaling and powering,
numerical methods for the solution of ODEs are applicable to nonhomogeneous Markov chains,
i.e., Markov chains whose inﬁnitesimal generators are a function of time, Q(t). However, since the
subject of ODEs is so vast, and our objectives in this chapter rather modest, we shall be content
to give only the briefest of introductions to this approach. Given a ﬁrst-order differential equation
y′ = f (t, y) and an initial condition y(t0) = y0, a solution is a differentiable function y(t) such that
y(t0) = y0,
d
dt y(t) = f (t, y(t)).
(10.51)
In the context of Markov chains, the solution y(t) is the row vector π(t) and the function f (t, y(t))
is simply π(t)Q. We shall use the standard notation of Equation(10.51) for introducing general
concepts, and the Markov chain notation when the occasion so warrants, such as in examples and
Matlab code.
Numerical procedures to compute the solution of Equation (10.51) attempt to follow a unique
solution curve from its value at an initially speciﬁed point to its value at some other prescribed point
τ. This usually involves a discretization procedure on the interval [0, τ] and the computation of
approximations to the solution at the intermediate points. Given a discrete set of points (or mesh)
{0 = t0, t1, t2, . . . , tη = τ} in [0, τ], we denote the exact solution of the differential Equation
(10.51) at time ti by y(ti). The step size or panel width at step i is deﬁned as hi = ti −ti−1. A
numerical method generates a sequence {y1, y2, . . . , yη} such that yi is an approximation3 to y(ti).
In some instances the solution at time τ is all that is required, and in that case the computed solution
is taken to be yη. In other cases the value of y(t) for all t0 ≤t ≤τ is required, often in the form of
a graph, and this is obtained by ﬁtting a suitable curve to the values y0, y1, y2, . . . , yη.
3 The reader should be careful not to confuse yi with y(ti). We use yi for i ̸= 0 to denote a computed approximation to
the exact value y(ti). For i = 0, the initial condition gives y0 = y(t0).

366
Numerical Solution of Markov Chains
Single-Step Euler Method
If the solution is continuous and differentiable, then in a small neighborhood of the point (t0, y0) we
can approximate the solution curve by its tangent y′
0 at (t0, y0) and thereby move from (t0, y0) to the
next point (t1, y1). Since this means that
y′
0 = y1 −y0
t1 −t0
,
we obtain the formula
y1 = y0 + h1y′
0 = y0 + h1 f (t0, y0),
(10.52)
where h1 = t1 −t0. This process may be repeated from the computed approximation y1 to obtain
y2 = y1 + h2 f (t1, y1),
(10.53)
where h2 = t2 −t1, and so on until the ﬁnal destination is reached,
yη = yη−1 + hη f (tη−1, yη−1)
(10.54)
with hη = tη−tη−1. This method is called the forward Euler method (FEM) or explicit Euler method.
From Equations (10.52)–(10.54), the FEM method may be written as
yi+1 = yi + hi+1 f (ti, yi),
i = 0, 1, . . . , η −1,
(10.55)
where hi+1 = ti+1 −ti. In computing yi+1, a method may incorporate the values of previously
computed approximations y j for j = 0, 1, . . . , i, or even previous approximations to y(ti+1). A
method that uses only (ti, yi) to compute yi+1 is said to be an explicit single-step method. It is said
to be a multistep method if it uses approximations at several previous steps to compute its new
approximation. A method is said to be implicit if computation of yi+1 requires an approximation to
y(ti+1); otherwise, it is said to be explicit. An implicit Euler method, also called a backward Euler
method, is deﬁned by using the slope at the point (ti+1, yi+1). This gives
yi+1 = yi + hi+1 f (ti+1, yi+1).
The modiﬁed Euler method incorporates the average of the slopes at both points under the
assumption that this will provide a better average approximation of the slope over the entire panel
[ti, ti+1]. The formula is given by
yi+1 = yi + hi+1
f (ti, yi) + f (ti+1, yi+1)
2
.
(10.56)
This is also referred to as the trapezoid rule. A ﬁnal formula in this category is the implicit midpoint
rule, which involves the slope at the midpoint of the interval. The formula is given by
yi+1 = yi + hi+1 f

ti + hi
2 , yi + yi+1
2

.
These are all single-step methods, since approximations preceding (ti, yi) are not used in the
generation of the next approximation yi+1.
In Markov chain problems the explicit Euler method (Equation10.55) is given by
π(i+1) = π(i) + hi+1π(i)Q,
i.e.,
π(i+1) = π(i) (I + hi+1Q) .
(10.57)
Note that π(i) is the state vector of probabilities at time ti. We use this notation, rather than πi, so as
not to confuse the ith component of the vector with the entire vector at time ti. Thus, in the explicit

10.7 Transient Distributions
367
Euler method applied to Markov chains, moving from one time step to the next is accomplished by
a scalar-matrix product and a vector-matrix product.
Example 10.33 Consider a continuous-time Markov chain with inﬁnitesimal generator
Q =
⎛
⎝
−2
1
1
3
−8
5
1
2
−3
⎞
⎠.
Suppose we wish to compute the transient distribution at time t = 1 given that the Markov chain
begins in state 1. For illustrative purposes, we shall use a constant step size equal to 0.1 and compute
π(i), i = 1, 2, . . . , 10. From the FEM formula, we obtain
π(.1) ≈π(1) = π(0) (I + .1Q) = (1, 0, 0)
⎡
⎣
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠+ .1 ×
⎛
⎝
−2
1
1
3
−8
5
1
2
−3
⎞
⎠
⎤
⎦
= (1, 0, 0)
⎛
⎝
.8
.1
.1
.3
.2
.5
.1
.2
.7
⎞
⎠= (.8, .1, .1).
π(.2) ≈π(2) = (.8, .1, .1)
⎛
⎝
.8
.1
.1
.3
.2
.5
.1
.2
.7
⎞
⎠= (.68, .12, .20).
Continuing in this fashion we ﬁnally compute
π(1.0) ≈π(10) = (.452220, .154053, .393726)
⎛
⎝
.8
.1
.1
.3
.2
.5
.1
.2
.7
⎞
⎠= (.447365, .154778, .397857).
The correct answer, obtained by a different method, is
π(1.0) = (.457446, .153269, .389285),
and a measure of the error is given as ∥π(1.0) −π(10)∥2 = .013319.
The modiﬁed Euler or trapezoid rule, Equation (10.56), when applied to Markov chains, becomes
π(i+1) = π(i) + hi+1
2
#
π(i)Q + π(i+1)Q
$
,
i.e.,
π(i+1)

I −hi+1
2
Q

= π(i)

I + hi+1
2
Q

,
(10.58)
which requires, in addition to the operations needed by the explicit Euler method, the solution of a
system of equations at each step (plus a scalar-matrix product). These additional computations per
step are offset to a certain extent by the better accuracy achieved with the trapezoid rule. If the size
of the Markov chain is small and the step size is kept constant, the matrix

I + hi+1
2
Q
 
I −hi+1
2
Q
−1
may be computed at the outset before beginning the stepping process, so that the computation
per step required by the modiﬁed Euler method becomes identical to that of the explicit Euler.
If the Markov chain is large, then the inverse should not be formed explicitly. Instead an
LU decomposition should be computed and the inverse replaced with a backward and forward

368
Numerical Solution of Markov Chains
substitution process with U and L respectively. When the step size is not kept constant, each
different value of h used requires that a system of linear equations be solved. Depending on the
size and sparsity pattern of Q, the work required by the trapezoid rule to compute the solution to
a speciﬁed precision may or may not be less than that required by the explicit Euler. The trade-off
is between an implicit method requiring more computation per step but fewer steps and an explicit
method requiring more steps but less work per step!
Example 10.34 Let us return to Example 10.33 and this time apply the trapezoid rule. Again, we
shall use a step size of 0.1 and successively compute π(i), i = 1, 2, . . . , 10. From the Trapezoid
formula, we obtain
π(i+1) = π(i)

I + hi+1
2
Q
 
I −hi+1
2
Q
−1
= π(i)
⎛
⎝
.90
.05
.05
.15
.60
.25
.05
.10
.85
⎞
⎠
⎛
⎝
1.10
−.05
−.05
−.15
1.40
−.25
−.05
−.10
1.15
⎞
⎠
−1
,
i.e.,
π(i+1) = π(i)
⎛
⎝
.832370
.072254
.095376
.213873
.459538
.326590
.098266
.130058
.771676
⎞
⎠.
Beginning with π(0) = (1, 0, 0), this allows us to compute
π(.1) ≈π(1) = (.832370, .072254, .095376),
π(.2) ≈π(2) = (.717665, .105750, .176585),
π(.3) ≈π(3) = (.637332, .123417, .239251),
...
π(1.0) ≈π(10) = (.456848, .153361, .389791).
The correct answer is still
π(1.0) = (.457446, .153269, .389285),
and a measure of the error is given as ∥π(1.0) −π(10)∥2 = .0007889 which is considerably better
than that obtained with the FEM method.
Example 10.35 This time we examine the effect of decreasing the step size using the same
inﬁnitesimal generator, taking π(0) = (1, 0, 0) and the length of the interval of integration to be
τ = 1, as before.
Q =
⎛
⎝
−2
1
1
3
−8
5
1
2
−3
⎞
⎠.
Table 10.2 shows the results obtained when Equations (10.57) and (10.58) are implemented in
Matlab.4 The ﬁrst column gives the step size used throughout the range, the second is the 2-norm
of the absolute error using the explicit Euler method, and the third column gives the 2-norm of
the absolute error when the trapezoid method is used. Observe that the accuracy achieved with the
explicit Euler method has a direct relationship with the step size. Observe also the much more
accurate results obtained with the trapezoid method. Since equal step sizes were used in these
examples, an essentially identical amount of computation is required by both. The trapezoid rule
needs only an additional (3 × 3) matrix inversion and a matrix-matrix product.
4 A listing of all the Matlab implementations of ODE methods used to generate results in this chapter is given at the end
of this section.

10.7 Transient Distributions
369
Table 10.2. Euler and Trapezoid Method Results
Explicit Euler
Trapezoid
Step size
∥y(t1) −y1∥2
∥y(t1) −y1∥2
h = .1
0.133193e−01
0.788907e−03
h = .01
0.142577e−02
0.787774e−05
h = .001
0.143269e−03
0.787763e−07
h = .0001
0.143336e−04
0.787519e−09
h = .00001
0.143343e−05
0.719906e−11
Taylor Series Methods
We now turn our attention to Taylor series methods. The forward Euler method corresponds to
taking the ﬁrst two terms of the Taylor series expansion of y(t) around the current approximation
point. Expanding y(t) as a power series in h, we have
y(t + h) = y(t) + hy′(t) + h2
2! y′′(t) + · · · .
(10.59)
If we know y(t) exactly, we may compute as many terms in the series as we like by repeated
differentiation of y′ = f (t, y(t)), the only constraint being the differentiability of the function
f (t, y(t)). Different Taylor series algorithms are obtained by evaluating different numbers of terms
in the series (10.59). For example, using the obvious notational correspondence with Equation
(10.59), the Taylor’s algorithm of order 2 may be written
yi+1 = yi + hT2(ti, yi),
where
T2(ti, yi) = f (ti, yi) + h
2 f ′(ti, yi).
The order-1 Taylor series algorithm corresponds to the FEM method, since we have
yi+1 = yi + hT1(ti, yi) = yi + h f (ti, yi).
Taylor series methods are not widely used because of the need to construct derivatives of the function
f (t, y(t)). Their use is that they provide a mechanism to develop other methods which have an
accuracy comparable to that of Taylor series of order p but which do not require derivatives of
f (t, y(t)). This leads us introduce the concept of order of convergence.
Deﬁnition 10.7.1 (Order of convergence) A numerical method is said to be convergent to order p
if the sequence of approximations {y0, y1, y2, . . . , yη} generated by the method (with constant step
size h = τ/η) satisﬁes
∥yi −y(ti)∥= O(h p),
i = 0, 1, . . . , η.
Example 10.36 Euler’s method, described by Equation (10.55), is convergent to order 1; the error
behaves like Mh, where M is a constant that depends on the problem and h is the maximum step
size.
The order of convergence of a method concerns only the truncation error made in terminating the
inﬁnite series (10.59). In particular, it does not take into account any roundoff error that might
occur. Indeed, as the truncation error tends to zero with decreasing h, the roundoff error has the
opposite effect. The smaller the step size, the greater the number of steps that must be taken, and
hence the greater the amount of computation that must be performed. With this greater amount of

370
Numerical Solution of Markov Chains
computation comes the inevitable increase in roundoff error. As h becomes smaller, the total error
(truncation error plus rounding error) decreases, but invariably there comes a point at which it begins
to increase again. Thus, it is important that h not be chosen too small.
Runge-Kutta Methods
An extremely important and effective class of single step methods is the class of Runge–Kutta
methods. A Runge–Kutta algorithm of order p provides an accuracy comparable to a Taylor
series algorithm of order p, but without the need to determine and evaluate the derivatives
f ′, f ′′, . . . , f (p−1), requiring instead the evaluation of f (t, y) at selected points. The derivation
of an order p Runge–Kutta method is obtained from a comparison with the terms through h p in the
Taylor series method for the ﬁrst step, i.e., the computation of y1 from the initial condition (t0, y0).
This is because the analysis assumes that the value around which the expansion is performed is
known exactly. A Runge–Kutta method is said to be of order p if the Taylor series for the exact
solution y(t0 + h) and the Taylor series for the computed solution y1 coincide up to and including
the term h p.
For example, a Runge–Kutta method of order 2 (RK2) is designed to give the same order of
convergence as a Taylor series method of order 2, without the need for differentiation. It is written
as
yi+1 = yi + h(ak1 + bk2),
(10.60)
where a and b are constants,
k1 = f (ti, yi),
k2 = f (ti + αh, yi + hβk1),
and α and β are constants. The constants are chosen so that the Taylor series approach agrees with
Equation (10.60) at i = 0, through the term in h2. Finding these constants requires computing the
derivatives of Equation (10.60) at i = 0, and then comparing coefﬁcients with those of the ﬁrst and
second derivatives obtained from the exact Taylor series expansion. We leave this as an exercise.
Equating the coefﬁcients imposes the following restrictions on the constants a, b, α, β:
a + b = 1,
αb = βb = 1/2.
These are satisﬁed by the choice a = b = 1
2 and α = β = 1, so the resulting RK2 method is
yi+1 = yi + h
2 f (ti, yi) + h
2 f (ti + h, yi + h f (ti, yi)).
Other choices for the constants are also possible.
The most widely used Runge–Kutta methods are of order 4. The standard RK4 method requires
four function evaluations per step and is given by
yi+1 = yi + h
6(k1 + 2k2 + 2k3 + k4),
(10.61)
where
k1 = f (ti, yi),
k2 = f (ti + h/2, yi + hk1/2),
k3 = f (ti + h/2, yi + hk2/2),
k4 = f (ti + h, yi + hk3).

10.7 Transient Distributions
371
Notice that each ki corresponds to a point in the (t, y) plane at which the evaluation of f is required.
Each is referred to as a stage. Thus, the method described by Equation (10.61) is a fourth-order,
four-stage, explicit Runge–Kutta method.
When the standard explicit fourth-order Runge–Kutta method given by Equation (10.61) is
applied to the Chapman–Kolmogorov equations π′ = π Q, the sequence of operations to be
performed to move from π(i) to the next time step π(i+1) is as follows:
π(i+1) = π(i) + h(k1 + 2k2 + 2k3 + k4)/6,
where
k1 = π(i)Q,
k2 = (π(i) + hk1/2)Q,
k3 = (π(i) + hk2/2)Q,
k4 = (π(i) + hk3)Q.
Example 10.37 We return to the continuous-time Markov chain of the previous examples and this
time apply the RK4 method to obtain the same transient distribution. With π(0) = (1, 0, 0) and
Q =
⎛
⎝
−2
1
1
3
−8
5
1
2
−3
⎞
⎠,
we ﬁnd, taking steps of size .1,
k1 = π(0)Q = (−2, 1, 1),
k2 = (π(0) + .05k1)Q = (.9, .05, .05)Q = (−1.6, .6, 1),
k3 = (π(0) + .05k2)Q = (.92, .03, .05)Q = (−1.7, .78, .92),
k4 = (π(0) + .1k3)Q = (.830, .078, .092)Q = (−1.334, .390, .944),
which allows us to compute
π(.1) ≈π(1) = π(0) + .1(k1 + 2k2 + 2k3 + k4)/6 = (.834433, .069167, .0964).
Continuing in this fashion we ﬁnd
π(.2) ≈π(2) = (.720017, .103365, .176618),
π(.3) ≈π(3) = (.639530, .121971, .238499),
...
π(1.0) ≈π(10) = (.457455, .153267, .389278),
and the ﬁnal error is
∥π(1.0) −π(10)∥2 = .00001256.
An important characterization of initial-value ODEs has yet to be discussed—that of stiffness.
Explicit methods have enormous difﬁculty solving stiff ODEs, indeed to such an extent that one
deﬁnition of stiff equations is “problems for which explicit methods don’t work” [20]! Many factors
contribute to stiffness, including the eigenvalues of the Jacobian ∂f/∂y and the length of the interval
of integration. The problems stem from the fact that the solutions of stiff systems of differential
equations contain rapidly decaying transient terms.

372
Numerical Solution of Markov Chains
Example 10.38 As an example, consider the inﬁnitesimal generator
Q =

−1
1
100
−100

with initial probability vector π(0) = (1, 0). Q has the decomposition Q = SS−1, where
S =
1
−.01
1
1

and
 =
0
0
0
−101

.
Since π(t) = π(0)eQt = π(0)Set S−1, we ﬁnd that
π(t) =
1
1.01
#
e0 + .01e−101t, .01e0 −.01e−101t$
.
The exponents e−101t in this expression tend rapidly to zero, leaving the stationary probability
distribution. In spite of this, however, when an explicit method is used, small step sizes must be
used over the entire period of integration. It is not possible to increase the step size once the terms
in e−101t are effectively zero.
The classical deﬁnition asserts that a system of ODEs is stiff when certain eigenvalues of the
Jacobian matrix (with elements ∂fi/∂yi) have large negative real parts when compared to others.
Using this deﬁnition, Markov chain problems (where the Jacobian is given by Q, the inﬁnitesimal
generator) are stiff when maxk |Re(λk)| ≫0. The Gerschgorin disk theorem is useful in bounding
this quantity. From the special properties of inﬁnitesimal generator matrices, this theorem implies
that
max
k
|Re(λk)| ≤2 max
j
|q j j|,
i.e., twice the largest total exit rate from any one state in the Markov chain.
Multistep BDF Methods
The so-called BDF (backward differentiation formulae) methods [16] are a class of multistep
methods that are widely used for stiff systems. Only implicit versions are in current use, since
low-order explicit versions correspond to the explicit Euler method (k = 1) and the midpoint rule
(k = 2), and higher-order explicit versions (k ≥3) are not stable. Implicit versions are constructed
by generating an interpolating polynomial z(t) through the points (t j, y j) for j = i−k+1, . . . , i+1.
However, now yi+1 is determined so that z(t) satisﬁes the differential equation at ti+1, i.e., in such
a way that z′(ti+1) = f (ti+1, yi+1). It is usual to express the interpolating polynomial in terms of
backward differences
∇0 fi = fi,
∇j+1 fi = ∇j fi −∇j fi−1.
With this notation, the general formula for implicit BDF methods is
k

j=1
1
j ∇j yi+1 = h fi+1
and the ﬁrst three rules are
k = 1 :
yi+1 −yi = h fi+1
(implicit Euler),
k = 2 :
3yi+1/2 −2yi + yi−1/2 = h fi+1,
k = 3 :
11yi+1/6 −3yi + 3yi−1/2 −yi−2/3 = h fi+1.

10.7 Transient Distributions
373
The BDF formulae are known to be stable for k ≤6 and to be unstable for other values of k. When
applied to Markov chains, the implicit (k = 2) BDF formula is
3π(i+1)/2 −2π(i) + π(i−1)/2 = hπ(i+1)Q,
so that the system of equations to be solved at each step is
π(i+1) (3I/2 −hQ) = 2π(i) −π(i−1)/2.
(10.62)
The solutions at two prior points π(i−1) and π(i) are used in the computation of π(i+1). To get the
procedure started, the initial starting point π(0) is used to generate π(1) using the trapezoid rule, and
then both π(0) and π(1) are used to generate π(2), using Equation (10.62).
Example 10.39 We shall apply the implicit (k = 2) BDF formula to the same continuous-time
Markov chain that we have used in previous ODE examples. When applying the trapezoid rule in
Example 10.34, we found π(1) = (.832370, .072254, .095376). We now have the two starting
points π(0) and π(1) needed to launch the implicit BDF formula. We ﬁrst compute
S = (1.5I −.1Q)−1 =
⎛
⎝
1.7
−.1
−.1
−.3
2.3
−.5
−.1
−.2
1.8
⎞
⎠
−1
=
⎛
⎝
.595870
.029485
.041298
.087021
.449853
.129794
.042773
.051622
.572271
⎞
⎠.
Had the Markov chains been large, we would have computed an LU factorization and applied this
decomposition to solve the appropriate system of equations instead of forming the inverse. We now
proceed by computing
π(.2) ≈π(2) =
#
2π(1) −.5π(0)
$
S = (.714768, .109213, .176019),
π(.3) ≈π(3) =
#
2π(2) −.5π(1)
$
S = (.623707, .127611, .239682),
...
π(1.0) ≈π(10) =
#
2π(9) −.5π(8)
$
S = (.454655, .153688, .391657),
and the ﬁnal error is
∥π(1.0) −π(10)∥2 = .003687.
Matlab Code for ODE Methods
The following code segments may be used to experiment with ODE methods for obtaining transient
distributions of relatively small Markov chains. In each case, the computed result is checked against
the results obtained from the Matlab matrix exponential function, expm. The input to each function
is the transition rate matrix Q, the initial vector y0 = π(0), the time t at which the distribution is to
be found and the step size, h. The output is an approximation to the transient distribution at time t,
i.e., π(t), and an estimate of the error, err. Two methods are included that are not discussed in the
section itself, an implicit Runge-Kutta method and an implicit Adam’s method, both of which may
be used on stiff problems.
**********************************************************************
function [y,err] = euler(Q,y0,t,h);
%%%%%%%%%%%%%%%%
Explicit Euler
[n,n] = size(Q);
I = eye(n);
eta = t/h;
y = y0;
yex = y0*expm(t*Q);
%%%%% Solution using Matlab fcn
R = I + h*Q;

374
Numerical Solution of Markov Chains
for i=1:eta,
y = y*R;
end
err = norm(y-yex,2);
***********************************************************************
function [y,err] = ieuler(Q,y0,t,h);
%%%%%%%%%%%%%%%%%
Implicit Euler
[n,n] = size(Q);
I = eye(n);
eta = t/h;
y = y0;
yex = y0*expm(t*Q);
%%%%% Solution using Matlab fcn
R = inv(I-h*Q);
for i=1:eta,
y = y*R;
end
err = norm(y-yex,2);
***********************************************************************
function [y,err] = trap(Q,y0,t,h);
%%%%%%%
Trapezoid/Modified Euler
[n,n] = size(Q);
I = eye(n);
eta = t/h;
y = y0;
yex = y0*expm(t*Q);
%%%%% Solution using Matlab fcn
R1 = I + h*Q/2;
R2 = inv(I-h*Q/2);
R = R1*R2;
for i=1:eta,
y = y*R;
end
err = norm(y-yex,2);
***********************************************************************
function [y,err] = rk4(Q,y0,t,h);
%
Explicit Runge-Kutta --- Order 4
[n,n] = size(Q);
I = eye(n);
eta = t/h;
y = y0;
yex = y0*expm(t*Q);
% Solution using Matlab fcn
for i=1:eta,
k1 = y*Q;
k2 = (y + h*k1/2)*Q;
k3 = (y + h*k2/2)*Q;
k4 = (y + h*k3)*Q;
y = y + h*(k1 + 2*k2 + 2*k3 + k4)/6;
end
err = norm(y-yex,2);
***********************************************************************
function [y,err] = irk(Q,y0,t,h);
%%%%%%%%%%%%%
Implicit Runge-Kutta
[n,n] = size(Q);
I = eye(n);
eta = t/h;
y = y0;
yex = y0*expm(t*Q);
%%%%%%% Solution using Matlab fcn

10.8 Exercises
375
R1 = I + h*Q/3;
R2 = inv(I-2*h*Q/3+ h*h*Q*Q/6);
R = R1*R2;
for i=1:eta,
y = y*R;
end
err = norm(y-yex,2);
**********************************************************************
function [y,err] = adams(Q,y0,t,h);
%%%%%%
Adams --- Implicit: k=2
[n,n] = size(Q);
I = eye(n);
eta = t/h;
yex = y0*expm(t*Q);
%%%%% Solution using Matlab fcn
R1 = I + h*Q/2;
R2 = inv(I-h*Q/2);
y1 = y0*R1*R2;
%%%%%% Trapezoid rule for step 1
S1 = (I+2*h*Q/3);
S2 =
h*Q/12;
S3 = inv(I-5*h*Q/12);
for i=2:eta,
%%%%%%% Adams for steps 2 to end
y = (y1*S1 - y0*S2)*S3;
y0 = y1; y1 = y;
end
err = norm(y-yex,2);
**********************************************************************
function [y,err] = bdf(Q,y0,t,h);
%%%%%%%%%%%
BDF --- Implicit: k=2
[n,n] = size(Q);
I = eye(n);
eta = t/h;
yex = y0*expm(t*Q);
%%%%%% Solution using Matlab fcn
R1 = I + h*Q/2;
R2 = inv(I-h*Q/2);
y1 = y0*R1*R2;
%%%%%%%% Trapezoid rule for step 1
S1 = inv(3*I/2 - h*Q);
for i=2:eta,
%%%%%%%%%%% BDF for steps 2 to end
y = (2*y1 - y0/2)*S1;
y0 = y1; y1 = y;
end
err = norm(y-yex,2);
**********************************************************************
10.8 Exercises
Exercise 10.1.1 State Gerschgorin’s theorem and use it to prove that the eigenvalues of a stochastic matrix
cannot exceed 1 in magnitude.
Exercise 10.1.2 Show that if λ is an eigenvalue of P and x its associated left-hand eigenvector, then
1 −α(1 −λ) is an eigenvalue of P(α) = I −α(I −P) and has the same left-hand eigenvector x, where

376
Numerical Solution of Markov Chains
α ∈ℜ′ ≡(−∞, ∞)\{0} (i.e., the real line with zero deleted). Consider the matrix
P(α) =
⎛
⎝
−1.2
2.0
0.2
0.6
−0.4
0.8
0.0
2.0
−1.0
⎞
⎠
obtained from the previous procedure. What range of the parameter α results in the matrix P being stochastic?
Compute the stationary distribution of P.
Exercise 10.1.3 Look up the Perron-Frobenius theorem from one of the standard texts. Use this theorem to
show that the transition probability matrix of an irreducible Markov chain possesses a simple unit eigenvalue.
Exercise 10.1.4 Part of the Perron-Frobenius theorem provides information concerning the eigenvalues of
transition probability matrices derived from periodic Markov chains. State this part of the Perron-Frobenius
theorem and use it to compute all the eigenvalues of the following two stochastic matrices given that the ﬁrst
has an eigenvalue equal to 0 and the second has an eigenvalue equal to −0.4:
P1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
.5
0
0
0
.5
.5
0
0
0
.5
0
0
0
0
.5
0
0
0
.5
0
0
0
0
.5
0
0
0
.5
0
0
0
0
.5
0
0
0
.5
.5
0
0
0
.5
0
0
0
0
.5
0
0
0
.5
0
0
0
0
.5
0
0
0
.5
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
P2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
.8
0
0
0
.2
.7
0
0
0
.3
0
0
0
0
.6
0
0
0
.4
0
0
0
0
.5
0
0
0
.5
0
0
0
0
.4
0
0
0
.6
.7
0
0
0
.3
0
0
0
0
.8
0
0
0
.2
0
0
0
0
.9
0
0
0
.1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Exercise 10.2.1 The state transition diagram for a discrete-time Markov chain is shown in Figure 10.4. Use
Gaussian elimination to ﬁnd its stationary distribution.
4
0.5 
0.5 
0.5
0.5
0.5
0.5
0.5
0.125
0.125 0.25
2
1 
3
Figure 10.4. State transition diagram for Exercise 10.2.1.
Exercise 10.2.2
The state transition diagram for a continuous-time Markov chain is shown in Figure 10.5.
Use Gaussian elimination to ﬁnd its stationary distribution.
1
1
2
3
4
5
6
2
3
1
1
1
1
2
1
1
2
Figure 10.5. State transition diagram for Exercise 10.2.2.
Exercise 10.2.3 What is the Doolittle LU decomposition for the Markov chain of Exercise 10.2.2? Verify your
answer by forming the product of L and U to obtain the inﬁnitesimal generator matrix of this Markov chain.

10.8 Exercises
377
Exercise 10.2.4 Use Gaussian elimination to ﬁnd the stationary distribution of the continuous-time Markov
chain whose transition rate matrix is
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−1
1
−2
2
−3
3
−4
4
−5
5
6
−6
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Compare the number of multiplications/divisions needed if the singularity of the coefﬁcient matrix is handled
by
(a) replacing the ﬁrst equation by πe = 1;
(b) replacing the last equation by πe = 1;
(c) replacing the row of zeros obtained during the ﬁnal reduction step by π6 = 1 and using a ﬁnal
normalization.
Exercise 10.2.5 Use Gaussian elimination to compute the stationary distribution of the discrete-time Markov
chain whose transition probability matrix P is
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
.6
0
0
.4
0
.3
0
0
0
.7
0
0
0
.4
.6
0
0
.5
0
.5
0
0
.6
.4
0
0
0
0
0
0
0
.8
0
.2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Is this stationary distribution a limiting distribution? Explain your answer. Hint: Draw a picture.
Exercise 10.2.6 Use Gaussian elimination to compute the stationary distribution of the continuous-time
Markov chain whose inﬁnitesimal generator Q is
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−1
0
1
0
0
0
0
0
−5
0
3
0
2
0
0
0
−3
0
3
0
0
0
0
0
−4
0
4
0
0
0
0
0
−5
0
5
0
4
0
0
0
−4
0
2
0
2
0
2
0
−6
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Is the stationary distribution you compute unique? Explain your answer. Hint: Draw a picture.
Exercise 10.2.7 Use the Grassman-Taskar-Heyman (GTH) algorithm to compute the stationary distribution of
the Markov chain whose transition rate matrix is
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−5
0
2
3
0
0
0
−1
0
0
0
1
0
1
−4
0
1
2
0
0
0
−1
1
0
0
0
1
1
−3
1
2
0
0
0
0
−2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Exercise 10.3.1 The stochastic transition probability matrix of a three-state Markov chain is given by
P =
⎛
⎝
0.0
0.8
0.2
0.0
0.1
0.9
0.6
0.0
0.4
⎞
⎠.
Carry out three iterations of the power method and the method of Jacobi using (0.5, 0.25, 0.25) as an initial
approximation to the solution. Find the magnitude of the subdominant eigenvalue of the iteration matrix in both

378
Numerical Solution of Markov Chains
cases to see which will converge faster asymptotically. Approximately how many decimal places of accuracy
would the subdominant eigenvalue suggest have been obtained after 100 iterations of both methods?
Exercise 10.3.2 Compute three iterations of the Gauss–Seidel method on the three-state Markov chain of
Exercise 10.3.1 using the same initial approximation. Construct the Gauss–Seidel iteration matrix and compute
the number of iterations needed to achieve ten decimal places of accuracy.
Exercise 10.3.3
The matrix P below is the stochastic transition probability matrix of a ﬁve-state Markov
chain. Carry out three iterations of the power method, using an initial vector approximation whose components
are all equal:
P =
⎛
⎜
⎜
⎜
⎜
⎝
0.2
0.0
0.005
0.795
0.0
0.0
0.0
0.998
0.002
0.0
0.002
0.0
0.0
0.0
0.998
0.8
0.001
0.0
0.198
0.001
0.0
0.998
0.0
0.002
0.0
⎞
⎟
⎟
⎟
⎟
⎠
.
Exercise 10.3.4 Returning to the stochastic transition probability matrix of Exercise 10.3.3, and using the same
initial starting approximation x(0) in which all components are equal, apply the standard (point) Gauss–Seidel
method to obtain approximations x(1), x(2), and x(3) .
Exercise 10.3.5 Given the stochastic transition probability matrix
P =
⎛
⎜
⎜
⎜
⎜
⎝
0.2
0.0
0.005
0.795
0.0
0.0
0.0
0.998
0.002
0.0
0.002
0.0
0.0
0.0
0.998
0.8
0.001
0.0
0.198
0.001
0.0
0.998
0.0
0.002
0.0
⎞
⎟
⎟
⎟
⎟
⎠
,
provide a Harwell-Boeing compact form for the corresponding transition rate matrix QT = PT −I, as it might
be used, for example, in the method of Gauss–Seidel.
Exercise 10.3.6 Consider the following stochastic transition probability matrix, provided in the Harwell-
Boeing compact storage format.
.4
.6
.4
.4
.2
.2
.8
3
1
2
1
3
3
2
1
3
6
8
Use the Gauss–Seidel iterative method and an initial approximation x(0) = (0, 1, 0)T to compute x(1) and x(2).
Exercise 10.4.1
Consider the stochastic transition probability matrix of Exercise 10.3.3 which is in fact
nearly completely decomposable. Permute this matrix to expose its NCD block structure and then perform
two iterations of block Gauss–Seidel, beginning with an initial approximation in which all components are
equal.
Exercise 10.5.1
Apply the NCD approximation procedure of Section 10.5 to the following stochastic
transition probability matrix:
P =
⎛
⎜
⎜
⎜
⎜
⎝
0.2
0.0
0.005
0.795
0.0
0.0
0.0
0.998
0.002
0.0
0.002
0.0
0.0
0.0
0.998
0.8
0.001
0.0
0.198
0.001
0.0
0.998
0.0
0.002
0.0
⎞
⎟
⎟
⎟
⎟
⎠
.
Exercise 10.5.2 Apply one complete step of the IAD method of Section 10.5 to the transition probability
matrix of Exercise 10.5.1. Choose an equiprobable distribution as your starting approximation.

10.8 Exercises
379
Exercise 10.6.1 Prove Equation (10.32), i.e., for any matrix R with spectral radius strictly less than 1
show that
∞

i=0
Ri = (I −R)−1.
Exercise 10.6.2 Implement the logarithmic reduction algorithm outlined in the text. How many iterations are
required to obtain the rate matrix R correct to ten decimal places, for a QBD process whose block matrices
A0, A1, and A2 are given below? Approximately how many iterations are required if the successive substitution
algorithm is used?
A0 =
⎛
⎜
⎜
⎝
4
0
0
0
0
4
0
0
0
0
4
0
0
0
0
4
⎞
⎟
⎟
⎠, A1 =
⎛
⎜
⎜
⎝
−16.0
12
0
0
0
−16.0
12
0
0
0
−16.0
12
0
0
0
−16.0
⎞
⎟
⎟
⎠, A2 =
⎛
⎜
⎜
⎝
0
0
0
0
0
0
0
0
0
0
0
0
12
0
0
0
⎞
⎟
⎟
⎠.
Exercise 10.6.3 Consider a QBD process of the form
Q =
⎛
⎜
⎜
⎜
⎝
B00
A2
0
0
· · ·
A0
A1
A2
0
· · ·
0
A0
A1
A2
...
...
...
...
⎞
⎟
⎟
⎟
⎠,
where A0, A1, and A2 are the same as in Exercise 10.6.2 and
B00 =
⎛
⎜
⎜
⎝
−12
12
0
0
0
−12
12
0
0
0
−12
12
0
0
0
−12
⎞
⎟
⎟
⎠.
Find the probability that this QBD process is in level 0 or 1.
Exercise 10.6.4 Consider a discrete-time Markov chain whose stochastic matrix P has the block structure
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
B00
A0
B10
A1
A0
B20
A2
A1
A0
A3
A2
A1
A0
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
with
A0 =
 1/8
1/32
1/16
1/16

, A1 =
1/16
1/32
1/8
0

, A1 =
 0
1/4
1/4
0

, A0 =
1/4
1/4
0
1/2

B00 =
3/4
3/32
3/8
1/2

, B10 =
1/2
1/4
1/4
1/2

, B20 =
1/4
1/4
1/4
1/4

.
Beginning with R(0) = 0, compute R(1) and R(2) using the successive substitution algorithm. What is the
magnitude of the difference in successive iterations after 10 steps? Finally, compute the probability of the
Markov chain being in a state of level 1.

380
Numerical Solution of Markov Chains
Exercise 10.6.5 The transition rate matrix of a continuous-time Markov chain has the form
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
B10
B11
A0
B20
0
A1
A0
B30
0
0
A1
A0
0
B41
0
0
A1
A0
0
0
A4
0
0
A1
A0
0
0
0
A4
0
0
A1
A0
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
with
A0 =
4
0
0
4

,
A1 =
−12
0
0
−10

,
A4 =
4
4
3
3

,
B00 =
−8
6
2
−4

,
B01 =
2
0
0
2

,
B10 =
1
3
2
0

,
B11 =
−8
0
0
−6

,
B20 =
0
8
6
0

,
B30 =
4
4
1
5

,
B41 =
6
2
3
3

.
Construct Neuts’ rate matrix R and the probability distribution of the states that constitute level 2.
Exercise 10.6.6 Which of the following three sequences of A blocks are taken from a positive-recurrent
Markov chain of M/G/1 type? For those that are positive recurrent, compute the approximation to G that is
obtained after 20 iterations using one of the three successive substitution strategies. Multiply your computed
value of G on the right by the vector e and estimate the accuracy obtained.
(a)
A0 =
⎛
⎝
1/4
1/4
0
0
0
0
0
1/4
1/4
⎞
⎠, A1 =
⎛
⎝
0
1/4
0
0
0
1/2
1/4
0
0
⎞
⎠, A2 =
⎛
⎝
1/8
0
0
0
1/2
0
0
0
1/8
⎞
⎠,
A3 =
⎛
⎝
1/8
0
0
0
0
0
0
0
1/8
⎞
⎠.
(b)
A0 =
⎛
⎝
1/4
1/4
0
1/4
0
1/4
0
1/4
1/4
⎞
⎠, A1 =
⎛
⎝
0
1/4
0
0
0
1/2
1/4
0
0
⎞
⎠, A2 =
⎛
⎝
3/16
0
0
0
0
0
0
0
3/16
⎞
⎠,
A3 =
⎛
⎝
1/16
0
0
0
0
0
0
0
1/16
⎞
⎠.
(c)
A0 =
⎛
⎝
1/4
1/4
0
1/4
0
1/4
0
1/4
1/4
⎞
⎠, A1 =
⎛
⎝
0
1/4
0
0
0
1/4
1/4
0
0
⎞
⎠, A2 =
⎛
⎝
3/16
0
0
0
0
0
0
0
3/16
⎞
⎠,
A3 =
⎛
⎝
1/16
0
0
0
0
0
0
0
1/16
⎞
⎠,
A4 =
⎛
⎝
0
0
0
1/8
0
1/8
0
0
0
⎞
⎠.

10.8 Exercises
381
Exercise 10.6.7 Consider a Markov chain whose inﬁnitesimal generator is given by
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−8
4
0
3
0
0
1
0
0
0
−4
4
0
0
0
0
0
0
4
0
−8
0
0
3
0
0
1
5
5
0 −18
4
0
3
0
0
1
0
0
5
0
5
0
−14
4
0
0
0
0
0
0
0
5
5
4
0
−18
0
0
3
0
0
1
5
5
0 −18
4
0
3
0
0
1
0
0
5
0
5
0
−14
4
0
0
0
0
0
0
0
5
5
4
0
−18
0
0
3
0
0
1
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Verify that the states of this Markov chain are positive recurrent and ﬁnd the probability that this Markov chain
is in level 2.
Exercise 10.7.1 Given that it starts in state 1, ﬁnd the transient distribution at time steps n = 10, 20, and 30 of
the discrete-time Markov chain whose transition probability matrix is given by
P =
⎛
⎜
⎜
⎝
.4
0
.6
0
.1
.1
.1
.7
.4
.3
.2
.1
0
.5
0
.5
⎞
⎟
⎟
⎠.
Exercise 10.7.2 When the stationary distribution π of a discrete-time Markov chain with stochastic transition
probability matrix P is required, we can ﬁnd it from either π P = π or π Q = 0, where Q = P −I. Can
the same be said for computing transient distributions? Compare the transient distribution at time step n = 10
obtained in the previous question (using the same initial distribution π (0) = (1, 0, 0, 0)) and the distribution
obtained by forming π (0)e10Q.
Exercise 10.7.3 Write Matlab code to implement the Padé algorithm for computing eQt. Check your algorithm
by testing it on the inﬁnitesimal generator
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−.15
.0
.149
.0009
.0
.00005
.0
.00005
.1
−.35
.249
.0
.0009
.00005
.0
.00005
.1
.8
−.9004
.0003
.0
.0
.0001
.0
.0
.0004
.0
−.3
.2995
.0
.0001
.0
.0005
.0
.0004
.399
−.4
.0001
.0
.0
.0
.00005
.0
.0
.00005
−.4
.2499
.15
.00003
.0
.00003 .00004
.0
.1
−.2
.0999
.0
.00005
.0
.0
.00005
.1999
.25
−.45
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and comparing it with the answer obtained by the Matlab function expm. Choose a value of t = 10.
Exercise 10.7.4 In the uniformization method, what value should be assigned to the integer K in Equation
(10.48) so that an accuracy of 10−6 is achieved when computing transient distributions for the Markov chains
with inﬁnitesimal generators:
(a)
Q =
⎛
⎜
⎜
⎝
−4
2
1
1
1
−4
2
1
1
1
−4
2
2
1
1
−4
⎞
⎟
⎟
⎠
at time t = 3.0;

382
Numerical Solution of Markov Chains
(b)
Q =
⎛
⎜
⎜
⎝
−1
1
0
0
1
−2
1
0
.5
.5
−1.5
.5
0
0
1
−1
⎞
⎟
⎟
⎠
at time t = 6.0.
Exercise 10.7.5 Returning to the previous exercise, ﬁnd the distribution at the speciﬁed times if each Markov
chain begins in state 4.
Exercise 10.7.6 The inﬁnitesimal generator of a continuous-time Markov chain is
Q =
⎛
⎜
⎜
⎝
−1
1
0
0
1
−2
1
0
.5
.5
−1.5
.5
0
0
1
−1
⎞
⎟
⎟
⎠.
If this Markov chain begins in state 4, what is the transient distribution at time t = 6 computed by (a) the
explicit Euler method and (b) the trapezoid method? Use a step size of h = .25 in both methods.
Exercise 10.7.7 In the text, a Runge-Kutta method of order 2 was described as having the form
yn+1 = yn + ak1 + bk2
with
k1 = h f (xn, yn),
k2 = h f (xn + αh, yn + βk1).
Show that equating this with the Taylor series expansion of order 2 leads to the conditions
a + b = 1 and αb = βb = 1/2.
Exercise 10.7.8 Write Matlab code to implement the implicit (k = 3) BDF method. Compare the results
obtained with this method and the implicit (k = 2) BDF method when applied to a continuous-time Markov
chain with inﬁnitesimal generator given by
Q =
⎛
⎜
⎜
⎜
⎜
⎝
−0.800
0.0
0.005
0.795
0.0
0.0
−1.000
0.998
0.002
0.0
0.002
0.0
−1.000
0.0
0.998
0.800
0.001
0.0
−0.802
0.001
0.0
0.998
0.0
0.002
−1.000
⎞
⎟
⎟
⎟
⎟
⎠
.
Use a step size of h = .25, initial starting distribution of (1,0,0,0,0) and compute the transient distribution at
time t = 12.

Part III
QUEUEING MODELS

This page intentionally left blank 

Chapter 11
Elementary Queueing Theory
11.1 Introduction and Basic Deﬁnitions
Queues form an annoying, yet indispensible, part of our everyday lives. They occur whenever there
is competition for limited resources. For example, we line up in queues outside doctors’ ofﬁces
and supermarket checkout counters; airplanes queue along airport runways; and requests to satisfy
operating system page faults queue at a computer hard drive. The entities that queue for service
are called customers, or users, or jobs, depending on what is appropriate for the situation at hand.
The limited resources are dispensed by servers and waiting lines of requests form in front of these
servers. Customers who require service are said to “arrive” at the service facility and place service
“demands” on the resource. At a doctor’s ofﬁce, the waiting line consists of the patients awaiting
their turn to see the doctor; the doctor is the server who dispenses a limited resource, her time.
Likewise, departing airplanes may have to wait in a line to use one of a small number of runways.
The planes may be viewed as users and the runway viewed as a resource that is assigned by an air
trafﬁc controller. The resources are of ﬁnite capacity meaning that there is not an inﬁnity of them
nor can they work inﬁnitely fast. Furthermore arrivals place demands upon the resource and these
demands are unpredictable in their arrival times and unpredictable in their size. Taken together,
limited resources and unpredictable demands imply a conﬂict for the use of the resource and hence
queues of waiting customers.
Our ability to model and analyze systems of queues helps to minimize their inconveniences
and maximize the use of the limited resources. An analysis may tell us something about the
expected time that a resource will be in use, or the expected time that a customer must wait. This
information may then be used to make decisions as to when and how to upgrade the system: for
an overworked doctor to take on an associate or an airport to add a new runway, for example. We
begin our discussion and analysis by considering a simple, single-server queueing system, such as
that presented in Figure 11.1.
Queue
Service Facility
Server
Departures
Arrivals
Figure 11.1. Single-server queue.
The resource and the dispenser of the resource and lumped together and referred to collectively
as the “server.” In some circumstances this is very natural, as when the server is a doctor and the
resource that is dispensed is the doctor’s time. On the other hand, for the airport scenario, the
resource is a runway and the dispenser is an air trafﬁc controller. But even here both runway and
controller are collectively referred to as the server. The server(s) and the queue into which arriving

386
Elementary Queueing Theory
customers are directed together constitute the service facility, or queueing system, or simply system.
Also, somewhat confusingly, it frequently happens that both queue and server are collectively
referred to as a queue, such as the M/M/1 queue or the M/G/1 queue; when multiple service
facilities are combined into a network, the term node is commonly used instead of service facility.
One after the other, customers arrive to the service facility, eventually receive service and then
depart. We make the following assumptions about all the queueing systems we analyze:
• If the server is free, i.e., not already serving a customer, an arriving customer goes immediately
into service. No time at all is spent in the queue.
• If the server is not free (busy), then the customer joins a queue of waiting customers and stays
in the queue until entering service.
• When the server becomes free, a customer is chosen from the queue according to a scheduling
policy and immediately enters into service. The time between the departure of one customer
and the start of service of the next customer is zero.
• Customers remain in the facility until their service is completed and then they depart.
Customers do not become impatient and leave before they receive service.
It is evident that much more detail needs to be provided before we can hope to analyze this queueing
system. For example, we must specify the manner in which arrivals occur, how the next customer
to be served is chosen from all those waiting, and so on. We address these topics in the following
sections.
11.1.1 Arrivals and Service
Congestion (i.e., queues) in a queueing system depend on the system irregularities (statistical
ﬂuctuations) and not just on average properties. In other words, the length of a queue depends
on the complete probabilistic description of the arrival and service processes—the demands being
placed on the resource—and of the capacity of the resource to service these demands. It would seem
evident that, if the average service demands of arriving customers are greater than the system service
capacity, the system will break down since unbounded queues will form. On the other hand, if the
average arrival rate is less than the system service capacity, then, due to statistical ﬂuctuations, we
still get queues. Even when the average arrival and service rates are held constant, an increase in the
variation of arrivals or service increases the congestion. Furthermore, as the average demand tends
to the system service capacity, the effects of the ﬂuctuations are magniﬁed. These ﬂuctuations are
described in terms of probability distributions. Thus we use elementary probability theory to predict
average waiting times, average queue length, distribution of queue length, etc., on the basis of
• the arrival pattern of customers to the resource,
• the service pattern of customers, and
• the scheduling algorithm, or the manner in which the next customer to be served is chosen.
In some queueing scenarios, the space available to hold customers waiting for service may be
limited. When the queue has reached its maximum capacity, it is said to be “full”; customers who
arrive to ﬁnd the queue full are said to be “lost.”
The Arrival Process
The customer arrival process may be described in two ways:
1.
By characterizing the number of arrivals per unit time (the arrival rate);
2.
By characterizing the time between successive arrivals (the interarrival time).
We use the variable λ to denote the mean arrival rate. In this case, 1/λ denotes the mean time
between arrivals. If the arrival pattern is not deterministic, the input process is a stochastic process,

11.1 Introduction and Basic Deﬁnitions
387
which means that we need its associated probability distribution. The probability distribution of the
interarrival time of customers is denoted by A(t) where
A(t) = Prob{time between arrivals ≤t}
and
1
λ =
 ∞
0
t dA(t),
where dA(t) is the probability that the interarrival time is between t and t + dt. Here we assume
that these interarrival times are independent and identically distributed, which means that only A(t)
is of signiﬁcance. If there are different types of customers, different customer classes is the usual
connotation, then each class may have its own probability distribution function to describe its arrival
process. The manner in which the arrival pattern changes in time may be important (e.g., the number
of customers who arrive at a supermarket may be greater in late afternoon than in early morning.)
An arrival pattern that does not change with time (i.e., the form and values of the parameters of A(t)
are time independent) is said to be a homogeneous arrival process. If it is invariant to shifts in the
time origin, it is said to be a stationary arrival process.
The Service Process
Just like the arrival pattern, the service pattern may be described by a rate, the number of customers
served per unit time, or by a time, the time required to serve a customer. The parameter μ is used to
denote the mean service rate, and hence 1/μ denotes the mean service time. We shall use B(x) to
denote the probability distribution of the demand placed on the system, i.e.,
B(x) = Prob{Service time ≤x}.
Thus
1
μ =
 ∞
0
x dB(x),
where dB(x) is the probability that the service time is between x and x + dx.
Notice that the service time is equal to the length of time spent in service and does not include
the time spent waiting in the queue. Furthermore service rates are conditioned on the fact that the
system is not empty. If the system is empty, then the server must be idle. Although it is usual to
associate the service time distribution with the server, the service time is actually the time that is
requested or needed by the customer who is taken into service. Obviously, it does not make sense
for a server to arbitrarily dispense service to customers without regard for their needs. The service
may be batch or single. With batch service, several customers can be served simultaneously as is
the case, for example, of customers who wait in line for taxis or buses. Also, the service rate may
depend on the following factors.
• The number of customers present (called state-dependent or load-dependent service). For
example, a server may speed up when the queue starts to become full, or slow down as it
starts to empty.
• The time (called time-dependent or nonhomogeneous service). This is, for example, the case
of a server that starts slowly in the morning and gradually speeds up during the day.
The total number of servers available at a queueing facility is denoted by c. Two possibilities arise
when there is more than one server.
1.
Each server has its own queue. For example, each supermarket checkout lane has its own
queue. However, the effect of jockeying and lane changing may mean that a supermarket

388
Elementary Queueing Theory
checkout system may be more accurately modeled as a single queue in front of all checkout
lanes.
2.
There are fewer queues than servers. In most cases, there is a single queue for all servers.
For example, a single queue usually forms in front of multiple bank tellers.
The servers may or may not be identical, i.e., B(x) may be different for different servers. Also,
given multiple classes of customers, the same server may give different service to different classes.
The capacity (space) that a service facility has to hold waiting customers (called simply, the system
capacity) is often taken to be inﬁnite. When this is not the case, then the system is referred to as a
ﬁnite queueing system.
Poisson Arrivals and Exponential Service
In stochastic modeling, numerous random variables are frequently modeled as exponentials. These
include
• interarrival time (the time between two successive arrivals to a service center),
• service time (the time it takes for a service request to be satisﬁed),
• time to failure of a component, and
• time required to repair a component.
The assertion that the above distributions are exponential should not be taken as fact, but as
an assumption. Experimental veriﬁcation of this asumption should be sought before relying on
the results of any analyses that use them. Recall that the cumulative distribution function for an
exponential random variable, X, with parameter λ > 0, is given by
F(x) =

1 −e−λx,
x ≥0,
0
otherwise,
and its corresponding probability density function, obtained simply by taking the derivative of F(x)
with respect to x, is
f (x) =
λe−λx,
x ≥0,
0
otherwise.
Its mean, second moment, and variance are given, respectively, by
E[X] = 1
λ,
E[X2] = 2
λ2 ,
and
Var [X] = 1
λ2 .
The reader may wish to review the material on the exponential distribution that was presented earlier
in Section 7.2 of this text. Reasons for the use of the exponential distribution include
1.
its memoryless (Markov) property and resulting analytical tractability;
2.
its relationship to the Poisson process.
In Section 9.11 we introduced a counting process, {N(t), t ≥0}, as a stochastic process which
counts the number of events that occur by time t. Thus any counting process N(t) is integer
valued with the properties that N(t) ≥0 and N(t1) ≤N(t2) if t1 is smaller than t2. We deﬁned
a renewal process as a counting process for which the distributions of time between any two events
(or renewals) are independent and identically distributed and we saw that when this distribution
is exponentially distributed, the renewal process has a Poisson distribution and is in fact called a
Poisson process. A counting process is said to have independent increments if the number of events
that occur in nonoverlapping time intervals are independent and is further said to have stationary
increments if the distribution of the number of events in any time interval depends only on the length
of that interval. It turns out that the Poisson process is the only continuous renewal process that has
independent and stationary increments.

11.1 Introduction and Basic Deﬁnitions
389
Deﬁnition 11.1.1 A Poisson process {N(t), t ≥0} having rate λ ≥0 is a counting process with
independent and stationary increments, with N(0) = 0, and is such that the number of events that
occur in any time interval of length t has a Poisson distribution with mean λt. This means that
Prob{N(t + s) −N(s) = n} = e−λt (λt)n
n! ,
n = 0, 1, 2, . . . .
Thus a Poisson process is a continuous-parameter, discrete-state process. We shall use it
extensively to denote the number of arrivals to a queueing system by time t. Let λ be the average
rate at which customers arrive to a service facility, then λt customers arrive on average in an interval
of length t. There are several other ways in which a Poisson process can be deﬁned. We shall have
use for two of these, which we now present.
Deﬁnition 11.1.2 Let the number of events that occur in (0, t] be denoted by N(t). When these
events occur successively in time and are such that the times between successive events are
independent and identically exponentially distributed, then the stochastic process {N(t), t ≥0}
is a Poisson process with parameter (mean rate) λ.
Deﬁnition 11.1.3 Let N(t) be the number of events that occur in an interval (0, t]. When the
following four conditions are true, then {N(t), t ≥0} is a Poisson process:
1. N(0) = 0.
2. Events that occur in nonoverlapping time intervals are mutually independent.
3. The number of events that occur in any interval depends only on the length of the interval and
not on the past history of the system.
4. For sufﬁciently small h and some positive constant λ, we have
Prob{One event in (t, t + h]} = λh + o(h),
Prob{Zero events in (t, t + h]} = 1 −λh + o(h),
Prob{More than one event in (t, t + h]} = o(h),
where o(h)—“little oh”—is any quantity that tends to zero faster than h; i.e.,
lim
h→0
o(h)
h
= 0.
Deﬁnitions 11.1.1, 11.1.2, and 11.1.3 are equivalent deﬁnitions of the Poisson process.
Let pn(t) = Prob{N(t) = n} be the probability that n arrivals occur in the interval (0, t]. Then
p0(t) = Prob{No arrivals in (0, t]}. We shall now write an expression for p0(t + h), the probability
that no arrivals occur in the interval (0, t + h]. We have, as is illustrated graphically in Figure 11.2,
p0(t + h) = Prob{No arrivals in (0, t + h]}
= p0(t)Prob{No arrivals in (t, t + h]} = p0(t)[1 −λh + o(h)],
since the number of arrivals in (0, t] is independent of the number of arrivals in (t, t + h].
0 
t
t + h
No arrivals
No arrivals
arrivals
No
Figure 11.2. Zero arrivals.

390
Elementary Queueing Theory
From this we obtain
p0(t + h) −p0(t)
h
= −λp0(t) + o(h)
h
p0(t)
and in the limit as h →0
dp0(t)
dt
= −λp0(t),
which implies that
p0(t) = αe−λt.
The initial condition p0(0) = 1 implies that α = 1 and hence
p0(t) = e−λt,
i.e., the probability that zero arrivals occur in the interval (0, t] is equal to e−λt.
We now use a proof by induction to show that
pn(t) = e−λt (λt)n
n! ,
n = 0, 1, 2, . . . .
(11.1)
We have just shown that the base clause holds, i.e., that p0(t) = e−λt. Now consider p1(t + h), the
probability of a single arrival between time 0 and time t + h. In this case, either an arrival occurred
in the interval (0, t] and no arrivals occurred in (t, t + h] or no arrival occurred in the ﬁrst interval
(0, t] but one did in the second, (t, t + h]. We have
p1(t + h) = p1(t)[1 −λh + o(h)] + p0(t)[λh + o(h)]
= p1(t) −λhp1(t) + λhp0(t) + o(h),
and hence
p1(t + h) −p1(t)
h
= −λp1(t) + λp0(t) + o(h)
h ,
dp1(t)
dt
= λp0(t) −λp1(t),
which yields the solution p1(t) = λte−λt as can be veriﬁed by direct substitution.
We now assume that Equation (11.1) holds for all integers through n −1 and thereby prove it
must also hold for n. Using the law of total probability, we may write
pn(t + h) =
n

k=0
Prob{k arrivals in (0, t]} Prob{(n −k) arrivals in (t, t + h]} =
n

k=0
pk(t)pn−k(h).
Now applying the properties of the Poisson process given under Deﬁnition 11.1.3, we ﬁnd
pn(t + h) =
n

k=0
pk(t)pn−k(h) = pn(t)p0(h) + pn−1(t)p1(h) + o(h)
= pn(t)(1 −λh) + pn−1(t)λh + o(h).
Hence
pn(t + h) −pn(h)
h
= −λpn(t) + λpn−1(t) + o(h)
h .
Taking the limit as h tends to zero yields
dpn(t)
dt
= −λpn(t) + λpn−1(t),
n > 0,

11.1 Introduction and Basic Deﬁnitions
391
which is a set of differential-difference equations having the solution
pn(t) = e−λt (λt)n
n! ,
n = 0, 1, 2, . . . ,
which once again can be readily veriﬁed by direct substitution. Therefore, the number of arrivals,
N(t), in the interval (0, t] has a Poisson distribution with parameter λt.
To compute the mean number of arrivals in an interval of length t, we proceed as follows:
E[N(t)] =
∞

k=1
kpk(t) =
∞

k=1
k e−λt (λt)k
k!
= λt
 ∞

k=1
(λt)k−1
(k −1)!

e−λt
= λt
 ∞

k=0
(λt)k
k!

e−λt
= λt.
It is now evident why λ is referred to as the rate of the Poisson process, since the mean number of
arrivals per unit time, E[N(t)]/t, is equal to λ. It may be shown, and indeed has already been shown
in Part I of this text, that the variance of a Poisson process is equal to its expectation. Recall that in
the usual notation, found in statistics and probability texts, the Poisson distribution is written as
f (k; α) = e−α αk
k! ,
k = 0, 1, 2, . . . ,
and has mean α and variance α. In our case, we have α = λt.
Example 11.1 The arrival of jobs to a supercomputing center follows a Poisson distribution with a
mean interarrival time of 15 minutes. Thus the rate of arrivals is λ = 4 jobs per hour. We have the
following:
• Prob{Time between arrivals ≤τ hours} = 1 −e4τ.
• Prob{k arrivals in τ hours} = e−4τ(4τ)k/k!.
Poisson Processes Have Exponential Interarrival Times
We now show that if the arrival process is a Poisson process, then the associated random variable
deﬁned as the time between successive arrivals (the interarrival time) has an exponential distribution.
Example 11.2 Continuing with the previous example of arrivals to a supercomputing center at a rate
of one every 15 minutes, suppose 45 minutes have passed without an arrival. Then the expected time
until the next arrival is still just 15 minutes, a result of the memoryless property of the exponential
distribution.
Since the arrival process is Poisson, we have
pn(t) = e−λt (λt)n
n! ,
n = 0, 1, 2, . . . ,
which describes the number of arrivals that have occurred by time t. Let X be the random variable
that denotes the time between successive events (arrivals). Its probability distribution function is
given by
A(t) = Prob{X ≤t} = 1 −Prob{X > t}.
But Prob{X > t} = Prob{0 arrivals in (0, t]} = p0(t). Thus
A(t) = 1 −p0(t) = 1 −e−λt,
t ≥0,
with corresponding density function, obtained by differentiation,
a(x) = λe−λt,
t ≥0.

392
Elementary Queueing Theory
This deﬁnes the exponential distribution. Thus X is exponentially distributed with mean 1/λ and
consequently, for a Poisson arrival process, the time between arrivals is exponentially distributed.
Let X1 be the random variable that represents the time until the ﬁrst arrival. Then
Prob{X1 > t} = Prob{N(t) = 0} = e−λt.
If Xi, i > 1, is the random variable that denotes the time between arrival i −1 and arrival i, then Xi
has exactly the same distribution as X1. Suppose arrival i −1 occurs at some time s and our concern
is the time t until the next arrival. We have
Prob{Xi > t | Ai−1 = s} = Prob{0 arrivals in (s, s + t] | Ai−1 = s}
= Prob{0 arrivals in (s, s + t]}
= e−λt,
where the condition vanishes due to the independent nature of arrivals, while the number of arrivals
in any interval depends only on the length of that interval. The time until the nth arrival may now
be found as the sum Sn = n
i=1 Xi and was previously seen (Section 7.6.2) to have an Erlang-n
distribution:
FSn(t) = 1 −
n−1

i=0
e−λt (λt)i
i! ,
fSn(t) = λe−λt (λt)n−1
(n −1)!,
t ≥0.
In Chapter 7 we used the Laplace transform to justify this result. We now use a more direct approach
by arguing that arrival n will occur before time t if and only if N(t) ≥n. Thus
FSn(t) = Prob{Sn ≤t} = Prob{N(t) ≥n} =
∞

i=n
pi(t) =
∞

i=n
e−λt (λt)i
i!
= 1 −
n−1

i=0
e−λt (λt)i
i! .
The corresponding density function is obtained by differentiation. We have
fSn(t) = λe−λt
∞

i=n
(λt)i−1
(i −1)! −λe−λt
∞

i=n
(λt)i
i!
= λe−λt (λt)n−1
(n −1)!.
Let us now examine the distribution of these arrival times. Suppose we are told that exactly one
arrival occurred in the interval [0, t]. We would like to ﬁnd the distribution of the time at which it
actually arrived, i.e., we seek the distribution Prob{X1 < s | N(t) = 1} for s ≤t. We have
Prob{X1 < s | N(t) = 1} = Prob{X1 < s, N(t) = 1}
Prob{N(t) = 1}
= Prob{1 arrival in [0, s)} × Prob{0 arrivals in [s, t)}
Prob{N(t) = 1}
= λse−λse−λ(t−s)
λte−λt
= s
t .
In other words, the distribution of the time at which the arrival occurs is uniformly distributed over
the interval [0, t]. This result can be extended. If we are told that exactly two arrivals occur in [0, t],
then the two arrival instants are independently and uniformly distributed over [0, t]; with n arrivals,
all n are independently and uniformly distributed over [0, t]. It is for this reason that arrivals which
occur according to a Poisson process are sometimes referred to as random arrivals.

11.1 Introduction and Basic Deﬁnitions
393
Example 11.3 Customers arrive at a service center according to a Poisson process with rate λ.
Suppose that three customers have been observed to arrived in the ﬁrst hour. Let us ﬁnd the
probability that all three arrived in the ﬁrst 15 minutes and the probability that at least one of the
three arrived in the ﬁrst 15 minutes. The key to answering this question lies in the fact that the
arrivals are uniformly distributed over the one hour interval.
Since the arrival instants are uniformly distributed over the ﬁrst hour, the probability of a
customer arriving in the ﬁrst 15 minutes is equal to 15/60 = 1/4. Furthermore since the arrivals are
independent, the probability that all three customers arrive in the ﬁrst 15 minutes is 1/43 = 1/64.
With three customers and 1/4 hour intervals, we may visualized the situation as a unit cube
partitioned into 64 smaller cubes each having dimension 1/4 × 1/4 × 1/4. Only the small cube
at the origin corresponds to all three customers arriving in the ﬁrst 15 minutes.
To ﬁnd the probability that at least one of the three arrived in the ﬁrst 15 minutes, we simply
need to sum the number of small cubes in sides that correspond to one of the three arriving in the
ﬁrst 15 minutes and dividing this total by 64, being sure not to count the same small cube more
than once. This gives the probability to be (16 + 12 + 9)/64 = 37/64. Alternatively, the different
possibilities are
• one customer arrives in the ﬁrst ﬁfteen minutes (probability 1/4) and the other two arrive at
any time,
• one customer does not arrive in the ﬁrst 15 minutes, one arrives in the ﬁrst 15 minutes and the
third arrives at any time (probability 3/4 × 1/4),
• two customers do not arrive in the ﬁrst 15 minutes, but the third does (probability 3/4×3/4×
1/4 = 9/64).
The sum of these probabilities is equal to 37/64. Yet another possibility is to compute the probability
that none of the three arrived in the ﬁrst 15 minutes and subtract this from one. This gives
1 −
3
4
3
= 1 −27
64 = 37
64.
Notice that in this example, the actual rate of the Poisson process does not ﬁgure in the computation.
Superposition/Decomposition of Poisson Streams
When two or more independent Poisson streams merge, the resulting stream is also a Poisson
stream. It follows then that multiple arrival processes to a single service center can be merged to
constitute a single arrival process whose interarrival times are exponentially distributed, so long as
the interarrivals times of the individual arrival processes are exponentially distributed (although pos-
sibly having different parameters), as illustrated in Figure 11.3. If the ith stream, i = 1, 2, . . . , n, has
parameter λi, then the parameter for the merged stream (also called the pooled stream) is
given by
λ =
n

i=1
λi.
Pooled stream
λ
λ
λ
λ
λ 
1
2
3
n
Figure 11.3. Superposition of Poisson streams.

394
Elementary Queueing Theory
The reverse operation is also possible. A single Poisson stream may be decomposed into
multiple independent Poisson streams if the decomposition is such that customers are directed into
substreams i, i = 1, 2, . . . , n, with probability pi and n
i=1 pi = 1, as illustrated in Figure 11.4.
The proof of these results is left to the exercises, where some helpful hints are provided.
1
λ 
λ
λ
λ
p
p
p
p
λ
1
2
3
n
n    Poisson streams
pn
p2
p
Figure 11.4. Decomposition of Poisson streams.
Example 11.4 To stress the importance of the independence of the decomposed streams, consider
the following. Customers enter a shoe store according to a Poisson process with a mean of 20
per hour. Four out of every ﬁve customers who enter are female. Thus, on average 16 female and
4 male customers enter the store each hour and we can consider the original Poisson stream to be
decomposed into a Poisson stream of female customers at a rate of 16 per hour and a Poisson stream
of male customers with a rate of 4 per hour. Suppose now that we are told that 32 female customers
entered the store during an hour. The expected number of all customers who entered during that
hour is equal to 32 + 4 = 36. Because the streams are independent, the fact that considerably more
female customers entered during that hour than should be expected has no inﬂuence on the expected
number of male customers, which must remain equal to 4. The expected total number of customers
who entered is thus equal to 36.
PASTA: Poisson Arrivals See Time Averages
An important property of the Poisson arrival process is that the distribution of customers seen by an
arrival to a queueing facility is, stochastically, the same as the limiting distribution of customers at
that facility. In other words, once the queueing system has reached steady state, each arrival from a
Poisson process ﬁnds the system at equilibrium. If pn is the probability that the system contains n
customers at equilibrium and an denotes the probability that an arriving customer ﬁnds n customers
already present, then PASTA says that an = pn. This implies that the Poisson process sees the same
distribution as a random observer, i.e., at equilibrium, Poisson arrivals take a random look at the
system.
This result is a direct consequence of the memoryless property of the interarrival time distribution
of customers to a queueing system fed by a Poisson process. In particular, it does not depend on the
service time distribution. To prove the PASTA property, we proceed as follows. Let
N(t) = Actual number of customers in system at time t,
pn(t) = Prob{System is in state n at time t} = Prob{N(t) = n},
an(t) = Prob{Arrival at time t ﬁnds system in state n},
A(t, t + δt] = The event “an arrival occurs in (t, t + δt].”

11.1 Introduction and Basic Deﬁnitions
395
Then
an(t) = lim
δt→0 Prob{N(t) = n|A(t, t + δt]}
= lim
δt→0
Prob{N(t) = n and A(t, t + δt]}
Prob{A(t, t + δt]}
= lim
δt→0
Prob{A(t, t + δt]|N(t) = n}Prob{N(t) = n}
Prob{A(t, t + δt]}
= lim
δt→0
Prob{A(t, t + δt]}Prob{N(t) = n}
Prob{A(t, t + δt]}
= Prob{N(t) = n} = pn(t).
The crucial step in the above argument is
Prob{A(t, t + δt]|N(t) = n} = Prob{A(t, t + δt]}.
This results from the fact that, since interarrival times possess the memoryless property,
Prob{A(t, t + δt]} is independent of the past history of the arrival process and hence independent
of the current state of the queueing system. With the Poisson arrival process having (constant)
rate λ, the probability of having an arrival in (t, t + δt]} is equal to λδt + o(δt) which does not
depend on N(t).
The PASTA property does not hold for arrival processes other than Poisson. Consider, for
example, a queueing system in which arrivals occur at ﬁxed intervals of time, exactly one minute
apart, and which require a ﬁxed service length of 30 seconds. It follows that for exactly half the time,
there is a single customer in service; the system is empty for the other half. We have p0 = p1 = 0.5
and pn = 0 for n ≥2. However, an arriving customer always ﬁnds the system empty: the previous
customer arrived 60 seconds ago, and left after 30. In this case, a0 = 1 and an = 0 for all n ≥1.
11.1.2 Scheduling Disciplines
We now turn our attention to the manner in which customers are selected from the queue and taken
into service. This is called the scheduling policy, scheduling algorithm, or scheduling discipline.
Unless otherwise stated, it is assumed that the time it takes to select a customer and to enter that
customer into service, is zero: the two events, the departure of a customer and a waiting customer
begins service, occur instantaneously. We now distinguish between policies that can interrupt the
service of the customer in service (called preemptive policies) and those that cannot (Figure 11.5).
Preemptive policies may be used when there are different types of customers, having different
service priorities: high priority customers are served before low priority ones. When a high priority
customer arrives and ﬁnds a customer of lower priority being served, the service of the lower priority
customer is interrupted and later resumed (called preempt-resume). The preempted customer is
reinserted into the queue. This means that in a preemptive system several customers may be at
various stages of completion at the same time. When a previously preempted customer returns to
Arrivals
Queue
Departures
Preemption
Server
Scheduling
algorithm
Figure 11.5. Scheduling customers for service.

396
Elementary Queueing Theory
service it may not be possible to continue from the point at which the preemption occurred. Some
previously completed work may have been lost. Indeed in some cases it may even be necessary to
begin the service all over again (called preempt-restart).
In a nonpreemptive system customers are served one at a time to completion. The server will
commence the service of a low priority customer if no customer of higher priority is present. But,
once the server has selected a customer, it is committed to serving that customer even if customers
of higher priority arrive during its service.
Within the context of preemptive and nonpreemptive service, several scheduling disciplines exist.
We brieﬂy describe the most common.
• FCFS (ﬁrst come, ﬁrst served). Also called FIFO (ﬁrst-in, ﬁrst-out). This is a nonpreemptive
policy that selects the customer at the head of the queue, the one that has waited the longest,
as the next one to be served.
• LCFS (last come, ﬁrst served). The opposite of FCFS. The last customer to arrive is the one
next chosen to be served. While this may seem to be unfair, it is exactly the situation used
when a stack data structure is implemented in a computer program.
• LCFS-PR (last come, ﬁrst served, preempt resume). This is the situation when the preempt
resume policy is applied to the LCFS scheduling discipline.
• SIRO (service in random order). The next customer to be served is chosen at random among
all those waiting.
• RR (round robin). In this scheduling discipline, the duration of service provided each time
a customer begins service is limited to a ﬁxed amount (frequently called a time slice). If a
customer completes service at any point during the time slice, it immediately departs from the
service center and the customer at the head of the queue begins to be served. If, after the time
slice has ﬁnished, a customer has still not completed its service requirement, that customer is
reinserted back into the end of the queue to await its turn for a second, or indeed third, fourth,
or more time slice.
• PS (processor sharing). This is the limiting case of RR in which the time slice goes to zero. It
is used to approximate RR, because it is much more tractable analytically.
• IS (inﬁnite server). The server has the ability to serve all customers simultaneously. It follows
that an arriving customer goes immediately into service and there is no need for a queue.
• PRIO (priority scheduling). In this case the customer chosen from the queue is the one with
the highest priority. FCFS is used to handle the case in which multiple customers of the same
priority are waiting.
Combinations of the above scheduling disciplines are possible and some will be considered later.
They all describe the order in which customers are taken from the queue and allowed into service.
11.1.3 Kendall’s Notation
Kendall’s notation is used to characterize queueing systems. It is given by A/B/C/X/Y/Z, where
A indicates the interarrival time distribution,
B indicates the service time distribution,
C indicates the number of servers,
X indicates the system capacity,
Y indicates the size of the customer population, and
Z indicates the queue scheduling discipline.
Some possible distributions (for A and/or B) are M (for Markovian), E (for Erlang), Ck (for Coxian
of order k), G (for general), and D for deterministic (or constant). For a general arrival process,
the notation GI is sometimes used to replace G to indicate that, although the interarrival time

11.1 Introduction and Basic Deﬁnitions
397
distribution may be completely general, successive arrivals are independent of each other. The
number of servers (C) is often taken to be 1. These ﬁrst three parameters are always provided. Thus,
for example, the M/M/1 queue means that the arrival process and service process are both Markovian
(although we usually say that the arrival process is Poisson and the service time distribution is
exponential) and there is a single server. This simplest of all queues is discussed in detail later.
The letters that specify the system capacity, customer population and scheduling discipline may be
omitted with the understanding that the default values for capacity and population size is inﬁnite and
the default for the scheduling discipline is FCFS. Thus the M/M/1 queue has an unlimited amount
of space in which to hold waiting customers, an inﬁnite population from which customers are drawn
and applies a FCFS scheduling policy. We shall provide additional details on Kendall’s notation as
and when it is needed.
Example 11.5 Consider as an example the case of travelers who arrive at a train station and
purchase tickets at one of six ticket distribution machines. It is reasonable to assume that the time
taken to purchase a ticket is constant whereas it may be observed that arrivals follow a Poisson
process. Such a system could be represented as an M/D/6 queue. If the time needed to purchase a
ticket is almost, but not quite constant, perhaps an M/E5/6 queue may be more appropriate, since
an Erlang-5 queue has some limited amount of variability associated with it. If no more than 50
travelers can squeeze into the ticket purchasing ofﬁce, then an M/E5/6/50 queue should be used.
Other scenarios are readily envisaged.
11.1.4 Graphical Representations of Queues
There exist a number of ways of graphing queueing systems and these can frequently provide insight
into the behavior of the system. For a FCFS, single server queue, one possibility is shown in Figure
11.6. We let Cn denote the nth customer to enter the queueing system. If the service discipline is not
FCFS, then Cn may not be the nth customer to depart. We let τn denote the time at which Cn arrives
in the system and tn the interarrival time between the arrival of Cn−1 and Cn (i.e., tn = τn −τn−1).
We shall assume that the tn are drawn from a distribution such that Prob{tn ≤t} = A(t), i.e.,
independent of n, where A(t) is a completely arbitrary interarrival time distribution. Similarly,
we shall deﬁne xn to be the service time for Cn and Prob{xn ≤x} = B(x), where B(x) is a
completely arbitrary (but independent) service time distribution. We are now in a position to graph
N(t), the number of customers in the system at time t. In Figure 11.6, wn represents the waiting
time (in queue) for Cn, and sn represents the system time (queue plus service) for Cn. (Note that
sn = wn + xn).
τ
τ
τ τ
τ
τ
τ
1
2
3
5
6
7
4
x1
x2
x3
x4
x5
x6
t
t
t
t
t
3
6
2
5
7
t4
Number in
system
Time
0
1
2
3
4
w2
Figure 11.6. Graphical representation No. 1.

398
Elementary Queueing Theory
d
a
τ
τ
τ τ
τ
τ
τ
1
2
3
5
6
7
4
0
1
2
3
4
x1
x2
x3
x4
x5
x6
s
w
N(t)
3
3
Time
BUSY
IDLE
BUSY
Cumulative number
of arrivals
Cumulative number
of departures
(t)
Cumulative
number of
arrivals/departures
(t)
Figure 11.7. Graphical representation No. 2.
An alternative graphical representation is given in Figure 11.7. In this graph N(t) = a(t) −d(t),
where a(t) denotes the number of arrivals in [0, t) and d(t) denotes the number of departures in this
same time period.
There is also a double-time-axis graphical representation which is shown in Figure 11.8 (for the
single-server, FCFS case). This particular variant is relatively easy to generalize to many servers
and to disciplines other than FCFS.
C1
C1
C2
C2
C3
C3
C4
C4
C5
C5
C6
C6
C7
x
t2
t3
t4
t5
t6
t7
x1
2
3
4
5
6
τ
τ
τ
τ
τ
τ
τ
1
2
3
4
5
6
7
Queue
Server
Time
Time
x
x
x
x
Figure 11.8. Double-time-axis notation.
11.1.5 Performance Measures—Measures of Effectiveness
When we analyze a queueing system we do so for the purpose of obtaining the values of certain
system properties. For example, we may wish to ﬁnd
• the number of customers in the system;
• the waiting time for a customer;
• the length of a busy or idle period;
• the current work backload (in units of time).
These are called measures of effectiveness. They are all random variables and, whereas we might
wish to know their complete probabilistic descriptions (i.e., their PDFs), most times we must

11.1 Introduction and Basic Deﬁnitions
399
be content to be able to derive just their ﬁrst few moments (mean, variance, etc.). In addition,
mathematical models of queueing systems provide quantitative answers to speciﬁc questions, such
as: “By how much will the congestion be reduced if the variation of service time is reduced by 10%?
50%?” or “What happens if we replace two slow servers by a single fast server?”, and so on. Also, a
queueing analyst may wish to design an optimal system which implies the need to balance waiting
time with idle time according to some inherent cost structure; e.g., to ﬁnd the optimal number of
servers, for example. We now consider the most commonly obtained measures of effectiveness.
Number of customers
We shall let N be the random variable that describes the number of customers in the system at
steady state. The probability that at steady state the number of customers present in the system is n
is denoted by pn,
pn = Prob{N = n},
and the average number in the system at steady state is
L = E[N] =
∞

i=0
npn.
Within the queueing system, customers may be present in the queue waiting for their turn to receive
service or they may be receiving service. We shall let Nq be the random variable that describes the
number of customers waiting in the queue and we shall denote its mean by Lq = E[Nq].
System Time and Queueing Time
The time that a customer spends in the system, from the instant of its arrival to the queue to the
instant of its departure from the server, is called the response time or sojourn time. We shall denote
the random variable that describes response time by R, and its mean value by E[R].
The response time is composed of the time that the customer spends waiting in the queue, called
the waiting time, plus the time the customer spends receiving service, called the service time. We
shall let Wq be the random variable that describes the time the customer spends waiting in the queue
and its mean will be denoted by E[Wq].
System Utilization
In a queueing system with a single server (c = 1), the utilization U is deﬁned as the fraction
of time that the server is busy. If the rate at which customers arrive at, and are admitted into, a
queueing facility is λ and if μ is the rate at which these customers are served, then the utilization
is equal to λ/μ. Over a period of time T , this queueing system, in steady state, receives an average
of λT customers, which are served in an average of λT/μ seconds. In many queueing systems,
the Greek letter ρ is deﬁned as ρ = λ/μ and consequently is identiﬁed with the utilization.
However, λ is generally deﬁned as the arrival rate to the system and this may or may not be the
rate at which customers actually enter the queueing facility. Thus it is not always the case that λ/μ
correctly deﬁnes the utilization. Some customers may be refused admission (they are said to be
“lost” customers) so that the effective arrival rate into the queueing facility is less than λ and hence
the utilization is less than ρ = λ/μ. However, unless stated otherwise, we assume that all customers
who arrive at a queueing facility are admitted.
In a G/G/1 queue where p0 is the probability that the system is empty, it must follow that
U = 1 −p0. In a stable system (i.e., one in which the queue does not grow without bound), the
server cannot be busy 100% of the time. This implies that we must have λ/μ < 1 for the queueing
system to be stable. Thus, in any time interval, the average number of customers that arrive must be
strictly less than the average number of customers that the server can handle.

400
Elementary Queueing Theory
In the case of queueing systems with multiple servers (c > 1), the utilization is deﬁned as
the average fraction of servers that are active—which is just the rate at which work enters the
system divided by the maximum rate (capacity) at which the system can perform this work, i.e.,
U = λ/(cμ). In multiserver systems, it is usual to deﬁne ρ as ρ = λ/(cμ) with the same caveat as
before concerning the identiﬁcation of ρ as the utilization.
System Throughput
The throughput of a queueing system is equal to its departure rate, i.e., the average number of
customers that are processed per unit time. It is denoted by X. In a queueing system in which all
customers that arrive are eventually served and leave the system, the throughput is equal to the
arrival rate, λ. This is not the case in queueing systems with ﬁnite buffer, since arrivals may be lost
before receiving service.
Trafﬁc Intensity
We deﬁne the trafﬁc intensity as the rate at which work enters the system, so it is therefore given as
the product of the average arrival rate of customers and the mean service time, i.e., λ¯x = λ/μ, where
¯x = 1/μ and μ is the mean service rate. Notice that in single-server systems the trafﬁc intensity is
equal to the utilization. For multiple servers, the trafﬁc intensity is equal to cU.
11.1.6 Little’s Law
Little’s law is perhaps the most widely used formula in queueing theory. It is simple to state and
intuitive, widely applicable, and depends only on weak assumptions about the properties of the
queueing system. Little’s law equates the number of customers in a system to the product of the
effective arrival rate and the time spend in the system. We present only an intuitive justiﬁcation.
Consider the graph of Figure 11.7 once again, which represents the activity in a single-server,
FCFS queueing system. Recall that a(t) denotes the number of arrivals in [0, t) and d(t) denotes
the number of departures in this same period. The number of customers in the system at any time
t is given by a(t) −d(t). Let the total area between the two curves up to some point t be denoted
by g(t). This represents the total time (the total number of seconds, for example) all customers have
spent in the system during the interval [0, t).
0
1
2
3
4
Time
Cumulative
number of
arrivals/departures
N(t)
t
a (t)
d (t)
Shaded area = 
g(t)
Figure 11.9. Graph No. 2 again.
From Figure 11.9, it is apparent that
λt ≡a(t)
t

11.1 Introduction and Basic Deﬁnitions
401
is the average arrival rate in the interval (e.g., number of customers arrivals per second);
Rt ≡g(t)
a(t)
is the average system time (time spent queueing plus time spent in service) per customer up to
time t, and
Lt ≡g(t)
t
is the average number of customers in the system between 0 and t. It follows then that
Lt = g(t)
t
= g(t)
a(t) × a(t)
t
= Rtλt.
Now let t →∞and assume the following limits exist:
lim
t→∞Lt = L = E[N],
lim
t→∞λt = λ,
lim
t→∞Rt = E[R].
Then
E[N] = λE[R].
This says that the average number of customers in the system is equal to the average arrival rate of
customer to the system multiplied by the average system time per customer, and is known as Little’s
law. Historically, Little’s law has been written as
L = λW
and in this usage it must be remembered that W is deﬁned as “response time,” the time spent in the
queue and at the server, and not just simply as the time spent “waiting” to be served. As before L
refers to the mean number of customers in the system and λ the arrival rate.
Little’s law can be applied when we relate L to the number of customers waiting to receive
service and W to the time spent waiting for service. The same applies also to the servicing aspect
itself. In other words, Little’s law may be applied individually to the different parts of a queueing
facility, namely the queue and the server. Given that Lq is the average number of customers waiting
in the queue, Ls is the average number of customers receiving service, Wq the average time spent in
the queue, and ¯x the average time spent receiving service, then, from Little’s law,
Lq = λWq
and
Ls = λ¯x,
and we have
L = Lq + Ls = λWq + λ¯x = λ(Wq + ¯x) = λW.
Little’s law may be applied even more generally than we have shown here. For example, it may
be applied to separate parts of much larger queueing systems, such as subsystems in a queueing
network. In such a case, L should be deﬁned with respect to the number of customers in a subsystem
and W with respect to the total time in that subsystem. Little’s law may also refer to a speciﬁc class
of customer in a queueing system, or to subgroups of customers, and so on. Its range of applicability
is very wide indeed. Finally, we comment on the amazing fact that the proof of Little’s law turns out
to be independent of
• speciﬁc asumptions regarding the arrival distribution A(t);
• speciﬁc asumptions regarding the service time distribution B(t);
• the number of servers;
• the particular queueing discipline.

402
Elementary Queueing Theory
11.2 Birth-Death Processes: The M/M/1 Queue
Birth–death processes are continuous-time1 Markov chains with a very special structure. If the states
of the Markov chain are indexed by the integers 0, 1, 2, . . . , then transitions are permitted only from
state i > 0 to its nearest neighbors, namely, states i −1 and i + 1. As for state i = 0, on exiting
this state, the Markov chain must enter state 1. Such processes are also called skip-free processes
because to go from any state i to any other state j, each intermediate state must be visited: no state
between these two can be skipped. As we shall see in this chapter, birth-death processes arise in
a variety of simple single-server queueing systems and the special structure makes their stationary
distributions relatively easy to compute. An arrival to the queueing system results in one additional
unit in the system, and is identiﬁed with a birth, whereas a departure removes a unit from the system
as is referred to as a death. Observe that in this analogy, a birth can occur when the system is empty!
The simplest of all queueing systems is the M/M/1 queue. This is a single server queue with
FCFS scheduling discipline, an arrival process that is Poisson and service time that is exponentially
distributed. It is shown graphically in Figure 11.10 where λ is the parameter of the Poisson
arrival process and μ is the exponential service rate. The mean time between arrivals (which is
exponentially distributed) is 1/λ and the mean service time is 1/μ.
μ
λ
Figure 11.10. The M/M/1 queue.
11.2.1 Description and Steady-State Solution
At any time t, the state of an M/M/1 queue is completely characterized by specifying the number of
customers present. We shall use the integers 0, 1, 2, . . . to represent these states: n denotes the state
in which there are n customers in the system, including the one in service. We would like to be able
to compute the state probabilities, i.e., the probability that the system is in any given state n at any
time t. We write these as
pn(t) = Prob{n in system at time t}.
This is often a difﬁcult task, even for this simplest of queueing processes. Instead, at least for the
moment, we shall look for the steady-state probabilities,
pn = lim
t→∞pn(t).
If this limit exists, then the probability of ﬁnding the system in any particular state eventually
becomes independent of the starting state, so that no matter when we query the system after it
settles into steady state, the probability of ﬁnding n customers present does not change. The steady
state probabilities pn can be interpreted as the probability of ﬁnding n customers in the system at an
arbitrary point in time after the process has reached steady state. It is not true that all systems reach
steady state, i.e., for some queueing systems it is possible that limt→∞pn(t) may not yield a true
probability distribution.
We ﬁrst seek to compute the probability that the system is in state n at time t + 
t, namely
pn(t + 
t). It is evident that the system will be in the state n at time t + 
t if one of the following
1 Birth-death processes in discrete time may be developed analogously to that of the continuous-time version considered
in this chapter.

11.2 Birth-Death Processes: The M/M/1 Queue
403
(mutually exclusive and collectively exhaustive) events occur:
1. The system is in state n at time t and no change occurs in (t, t + 
t].
2. The system is in state n −1 at time t and an arrival occurs in (t, t + 
t].
3. The system is in state n + 1 at time t and a departure occurs in (t, t + 
t].
For sufﬁciently small values of 
t, we have
Prob{1 arrival in (t, t + 
t]} = λ
t + o(
t),
Prob{1 departure in (t, t + 
t]} = μ
t + o(
t),
Prob{0 arrivals in (t, t + 
t]} = 1 −λ
t + o(
t),
Prob{0 departures in (t, t + 
t]} = 1 −μ
t + o(
t).
These assumptions (which follow from the exponential nature of the arrivals and service) imply that
the probability of multiple arrivals, of multiple departures, or of any near simultaneous combinations
of births and deaths in some small interval of time 
t, is negligibly small, i.e., of order o(
t). It
follows that the state transition probability diagram is as shown in Figure 11.11.
n-1
n
n+1
1
0
λ
λ
μ
μ
μ
μ
μ
λ
λ
μ
λ
λ
Figure 11.11. State transitions in the M/M/1 queue.
For n ≥1, we may write
pn(t + 
t) = pn(t)[1 −λ
t + o(
t)][1 −μ
t + o(
t)]
+pn−1(t)[λ
t + o(
t)]
+pn+1(t)[μ
t + o(
t)].
When n = 0, we have
p0(t + 
t) = p0(t)[1 −λ
t + o(
t)]
+p1(t)[μ
t + o(
t)].
Expanding the right-hand side,
pn(t + 
t) = pn(t) −(λ + μ)
tpn(t) + λ
tpn−1(t) + μ
tpn+1(t) + o(
t),
n ≥1,
p0(t + 
t) = p0(t) −λ
tp0(t) + μ
tp1(t) + o(
t).
Subtracting pn(t) from each side and dividing by 
t,
pn(t + 
t) −pn(t)

t
= −(λ + μ)pn(t) + λpn−1(t) + μpn+1(t) + o(
t)

t ,
n ≥1,
p0(t + 
t) −p0(t)

t
= −λp0(t) + μp1(t) + o(
t)

t .
Taking the limit as 
t →0 yields
dpn(t)
dt
= −(λ + μ)pn(t) + λpn−1(t) + μpn+1(t),
n ≥1,
(11.2)
dp0(t)
dt
= −λp0(t) + μp1(t),
(11.3)

404
Elementary Queueing Theory
i.e., a set of differential difference equations which represents the dynamics of the system. This
turns out to be hard to solve for pn(t), but not for the stationary distribution. Let us assume that the
steady state exists. Then
dpn(t)
dt
= 0
and we get
0 = −(λ + μ)pn + μpn+1 + λpn−1,
n ≥1,
0 = −λp0 + μp1
=⇒
p1 = λ
μ p0,
which is a set of second-order difference equations with constant coefﬁcients. There exists a variety
of ways of solving such equations. The direct approach is left as an exercise and some hints are
provided. In a later section we shall use the generating function approach, but for the moment we
proceed by using a recursive argument. Rearranging the set of difference equations, we obtain
pn+1 = λ + μ
μ
pn −λ
μ pn−1,
n ≥1.
Substituting n = 1 yields
p2 = λ + μ
μ
 λ
μ

p0 −λ
μ p0
= λ
μ
λ + μ
μ
−1

p0
= λ
μ
 λ
μ

p0 = λ2
μ2 p0.
Similarly, we may show that
p3 =
 λ
μ
3
p0,
which leads us to conjecture that
pn =
 λ
μ
n
p0.
The proof is by induction.
The formula clearly holds for n = 1 and 2, as we have just shown.
Let us assume it to be true for n −1 and n and prove it true for n + 1. We have
pn+1 = λ + μ
μ
 λ
μ
n
p0 −λ
μ
 λ
μ
n−1
p0
=
 λ
μ
n &λ + μ
μ
−1
'
p0
=
 λ
μ
n+1
p0.
Now it only remains to determine p0. We have
1 =
∞

n=0
pn =
∞

n=0
 λ
μ
n
p0 =
∞

n=0
ρn p0

11.2 Birth-Death Processes: The M/M/1 Queue
405
and therefore
p0 =
1
∞
n=0 ρn ,
where ρ ≡λ/μ, is the utilization factor, a measure of the average use of the service facility, i.e., it
is the expected number of arrivals per mean service time.
Notice that ∞
n=0 ρn is the geometric series and converges if and only if |ρ| < 1. Therefore, the
existence of a steady state solution demands that ρ = λ/μ < 1; i.e., that λ < μ. Note that if λ > μ
the mean arrival rate is greater than the mean service rate and the server will get further and further
behind. Thus the system size will keep increasingly without limit. It is not as intuitively obvious
as to why this limitless increase occurs when λ = μ. From the mathematics of the situation, we
know that the geometric series does not converge when ρ = 1 and we have previously seen that the
corresponding random walk problem is null-recurrent when λ = μ.
For ρ < 1,
∞

n=0
ρn =
1
1 −ρ
and hence p0 = 1 −ρ, or alternatively ρ = 1 −p0. In summary, the steady-state solution for the
M/M/1 queue is given by
pn = ρn(1 −ρ) for ρ = λ/μ < 1,
which is the probability mass function of a modiﬁed geometric random variable. Observe that the
equilibrium solution of the M/M/1 queue depends on the average arrival rate λ and the average
service rate μ only through their ratio ρ.
The probability that the queue contains at least k customers has a particularly nice formula.
We have
Prob{n ≥k} =
∞

i=k
pi = (1 −ρ)
∞

i=k
ρi = (1 −ρ)
 ∞

i=0
ρi −
k−1

i=0
ρi

= (1 −ρ)

1
1 −ρ −1 −ρk
1 −ρ

= ρk.
Example 11.6
The arrival pattern of cars to the local oil change center follows a Poisson
distribution at a rate of four per hour. If the time to perform an oil change is exponentially distributed
and requires on average of 12 minutes to carry out, what is the probability of ﬁnding more than 3
cars waiting for the single available mechanic to service their car?
This question requires us to compute ∞
i=4 pi = 1 −p0 −p1 −p2 −p3 from an M/M/1 queue
with λ = 4 and 1/μ = 12/60 or μ = 5. Thus ρ = 4/5 is strictly less than 1 and the system is
stable. Also p0 = 1 −ρ = 1/5 and
p1 = ρ(1 −ρ) = 4/25, p2 = ρp1 = 16/125, p3 = ρp2 = 64/625,
which allows us to compute the answer as
1 −1
5 −4
25 −16
125 −64
625 = .4096
which is exactly equal to ρ4.

406
Elementary Queueing Theory
Matrix Formulation of the M/M/1 Queue
The M/M/1 queue with service rate μ and arrival rate λ has the following inﬁnite inﬁnitesimal
generator:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−λ
λ
μ
−(λ + μ)
λ
μ
−(λ + μ)
λ
μ
−(λ + μ)
λ
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
In the notation used for Markov chains, we have π Q = 0 (with πi = pi for all i) and it is obvious
that −λπ0 + μπ1 = 0, i.e., that π1 = (λ/μ)π0. In general, we have
λπi−1 −(λ + μ)πi + μπi+1 = 0,
from which, by induction, we may derive
πi+1 = ((λ + μ)/μ)πi −(λ/μ)πi−1 = (λ/μ)πi.
Thus, once π0 is known, the remaining values πi, i = 1, 2, . . . , may be determined recursively just
as before. For the M/M/1 queue it has already been shown that the probability that the system is
empty is given by π0 = (1 −λ/μ).
Observe that the coefﬁcient matrix is tridiagonal. and that once p0 is known, the solution is just
a forward elimination procedure. However, there is no computational advantage to be gained by
setting up and solving the matrix equation, rather than using the previously developed recursive
relations. We show this formulation at this time because it will become useful in other, more
complex, cases when the matrix is Hessenberg, rather than tridiagonal.
11.2.2 Performance Measures
We now turn our attention to computing various performance measures concerning the M/M/1
queue, such as mean number in system, mean queue length, and so on.
Mean Number in System
Let N be the random variable that describes the number of customers in the system at steady state,
and let L = E[N]. Then
L =
∞

n=0
npn =
∞

n=0
n(1 −ρ)ρn = (1 −ρ)
∞

n=0
nρn = (1 −ρ)ρ
∞

n=0
nρn−1.
(11.4)
If we assume that the system is stable, then ρ < 1 and
∞

n=0
nρn−1 = ∂
∂ρ
( ∞

n=0
ρn
)
= ∂
∂ρ
&
1
1 −ρ
'
=
1
(1 −ρ)2 .
(11.5)
It now follows from Equation (11.4) that the mean number of customers in the M/M/1 queue is given
by
L = (1 −ρ)
ρ
(1 −ρ)2 =
ρ
1 −ρ =
λ
μ −λ.
Variance of Number in System
To compute the variance of the number of customers in an M/M/1 queue, we use the formula
Var [N] = E[N 2] −E[N]2.

11.2 Birth-Death Processes: The M/M/1 Queue
407
The second moment is computed as follows:
E[N 2] =
∞

n=0
n2 pn =
∞

n=0
n2(1 −ρ)ρn = (1 −ρ)
∞

n=0
n2ρn = (1 −ρ)ρ
∞

n=0
n2ρn−1
= (1 −ρ)ρ ∂
∂ρ
( ∞

n=0
nρn
)
.
Substituting from Equation (11.5) we have
E[N 2] = (1 −ρ)ρ ∂
∂ρ
&
ρ
(1 −ρ)2
'
= (1 −ρ)ρ (1 −ρ)2 + 2(1 −ρ)ρ
(1 −ρ)4
= ρ 1 + ρ
(1 −ρ)2 .
Finally, the variance is computed as
Var [N] = ρ 1 + ρ
(1 −ρ)2 −

ρ
1 −ρ
2
=
ρ
(1 −ρ)2 .
Mean Queue Length
Let Nq be the random variable that describes the number of customers waiting in the queue at steady
state, and let Lq = E[Nq]. Then
Lq = E[Nq] = 0 × p0 +
∞

n=1
(n −1)pn,
Lq =
∞

n=1
npn −
∞

n=1
pn = L −(1 −p0) =
ρ
1 −ρ −ρ =
ρ2
1 −ρ = ρL.
Thus,
Lq = ρL = L −ρ.
Average Response Time
Let R be the random variable that describes the response time of customers in the system. Little’s
law states that the mean number of customers in a queueing system in steady state is equal to the
product of the arrival rate and the mean response time, i.e.,
E[N] = λE[R]
(L = λW).
Then
E[R] = 1
λ E[N] = 1
λ
ρ
1 −ρ = 1/μ
1 −ρ =
1
μ −λ
which is the average service time divided by the probability that the server is idle. Thus the
congestion in the system, and hence the delay, builds rapidly as the trafﬁc intensity increases, (as
ρ →1).
Notice that we have not speciﬁed any particular scheduling discipline in our computation of
the mean response time and it may be (erroneously) thought that this result applies only to FCFS
scheduling. It is manifestly true in the FCFS case, since an arriving customer must wait while those
already in the queue, including the one in service, are served. With k already present, and all k
having a mean service time of 1/μ (including the one being served), the arriving customer will
spend a total time equal to ∞
k=0 pk(k + 1)/μ where the factor (k + 1)/μ is the time needed to
complete service for the k customers already present plus the time needed to serve the entering

408
Elementary Queueing Theory
customer. We evaluate this as follows:
∞

k=0
pk(k + 1) 1
μ =
∞

k=0
ρk(1 −ρ)(k + 1) 1
μ = 1 −ρ
μ
∞

k=0
ρk(k + 1) = 1 −ρ
μ
∞

k=0
#
kρk + ρk$
= 1 −ρ
μ

ρ ∂
∂ρ
∞

k=0
ρk +
∞

k=0
ρk

= 1 −ρ
μ

ρ
(1 −ρ)2 +
1
1 −ρ

=
1
μ(1 −ρ).
However, this same result applies to all scheduling disciplines that have the property that the server
is never idle when there are customers present. This is true only for the mean value and not for
higher moments. We shall consider the distribution of response time momentarily.
Average Waiting Time Wq
We have, from Little’s law,
Lq = λWq
and so
Wq =
λ
μ(μ −λ) =
ρ
μ −λ.
Similar comments regarding the scheduling discipline, made in our discussion of the mean response
time, also apply here.
Effective Queue Size for Nonempty Queue, L′
q
In this case we ignore instants in which the queue is empty.
L′
q = E[Nq|Nq ̸= 0] =
∞

n=1
(n −1)p′
n =
∞

n=2
(n −1)p′
n
where p′
n = Prob{n in system | n ≥2 }. It therefore follows that
p′
n = Prob{n in system and n ≥2}
Prob{n ≥2}
=
pn
∞
n=2 pn
=
pn
1 −p0 −p1
=
pn
1 −(1 −ρ) −(1 −ρ)ρ = pn
ρ2 ,
n ≥2.
Notice that the probability distribution p′
n is the probability distribution pn normalized when the
cases n = 0 and n = 1 are omitted. It now follows that
L′
q =
∞

n=2
(n −1) pn
ρ2 = 1
ρ2
( ∞

n=2
npn −
∞

n=2
pn
)
= 1
ρ2

(L −p1) −(1 −p0 −p1)
 
= 1
ρ2

ρ/(1 −ρ) −(1 −ρ)ρ −1 + (1 −ρ) + (1 −ρ)ρ
 
= 1
ρ2

ρ/(1 −ρ) −ρ
 
= 1
ρ

1/(1 −ρ) −1
 
=
1
1 −ρ .
Thus
L′
q =
1
1 −ρ =
μ
μ −λ.
Collecting these results together in one convenient location, we have
L = E[N] =
ρ
1 −ρ ,
Lq = E[Nq] =
ρ2
1 −ρ ,
Var [N] =
ρ
(1 −ρ)2 ,
(11.6)

11.2 Birth-Death Processes: The M/M/1 Queue
409
W = E[R] =

1
1 −ρ

E[S],
Wq =

ρ
1 −ρ

E[S],
L′
q =
1
1 −ρ ,
(11.7)
where E[S] = 1/μ is the mean service time.
Example 11.7 Let us compute these performance measures for the oil change center of
Example 11.6. Using the parameters λ = 4, μ = 5, and ρ = 4/5, we have the following measures:
Mean number in system:
ρ/(1 −ρ) = 4 cars,
Variance of number in system:
ρ/(1 −ρ)2 = 20 cars,
Average response time:
1/(μ −λ) = 1 hour,
Average time in queue prior to service:
ρ/(μ −λ) = .8 hours (or 48 minutes),
Average size of queue when it is not empty:
μ/(μ −λ) = 5 cars.
Throughput, Utilization, and Trafﬁc Intensity
It is apparent from the deﬁnitions given in the previous section that the throughput of the M/M/1
queue is just equal to the arrival rate, i.e., X = λ, and that the utilization is equal to the trafﬁc
intensity and is given by U = λ/μ.
Example 11.8 Consider a cable modem which is used to transmit 8-bit characters and which has a
capacity of 4 megabits per second (Mbps). Thus the maximum rate is 500,000 characters per second
(cps). Given that trafﬁc arrives at a rate of 450,000 cps, let us compute some standard performance
measures, when this system is modelled as an M/M/1 queue.
In this example, we have λ = 450,000 cps and μ = 500,000 cps. Thus the utilization (and hence
the trafﬁc intensity) of the cable modem is ρ = λ/μ = 0.9. The mean number of characters in the
system L, and the mean number of characters waiting to be transmitted Lq, are
L =
ρ
1 −ρ = 0.9
0.1 = 9
and
Lq =
ρ2
1 −ρ = 0.92
0.1 = 8.1;
the average transmission time per character (the response time) is equal to the expected number in
the system divided by λ (from Little’s law) which gives
ρ
1 −ρ × 1
λ = 0.9
0.1 ×
1
450,000 = .00002 seconds.
The throughput is just equal to λ = 450,000 cps.
Distribution of Time Spent in an M/M/1 Queue (Response Time)
Most performance measures presented so far have been average values: the mean number of
customers in the system, the mean time spent waiting prior to service, and so on. We may also
wish to ﬁnd the distributions of response time and queueing time, just as in pn, n = 0, 1, 2, . . . ,
we have the distribution of customer population. Whereas queue length and pn are unaffected by
the scheduling policy employed by a queueing system, it should be apparent that the distribution of
time that a customer spends in the system is a function of this policy. Here our concern is with the
M/M/1 queue and the FCFS scheduling discipline.
We ﬁrst consider the total time (response time) a customer has to spend in the system, which
includes the time spent in the queue waiting for service plus the time spent actually receiving
service. Denote this random variable, the response time, by R, its PDF by Wr(t), its density by
wr(t), and its expected value by E[R]. We shall show that
wr(t) = (μ −λ)e−(μ−λ)t,
t > 0,

410
Elementary Queueing Theory
and
E[R] =
1
μ −λ.
We use the following two properties of an M/M/1 queue:
1. From the PASTA property of a Poisson stream, an arriving customer sees the steady-state
distribution of the number of customers in the system. This arriving customer, with probability
pn, ﬁnds n customers already waiting or in service.
2. From the memoryless property of the exponential distribution, if the new arrival ﬁnds a
customer in service, the remaining service time of that customer is distributed exponentially
with mean 1/μ, i.e., identical to the service requirements of all waiting customers.
The response time of an arriving customer who ﬁnds n customers in the system is therefore the
sum of (n + 1) exponentially distributed random variables, the n already present plus the arriving
customer itself. Such a sum has an (n + 1) stage Erlang density function. Then, if Bk is the service
time of customer k, we have
Prob{R > t} = Prob
	n+1

k=1
Bk > t

.
Now, conditioning on the number present when the arrival occurs and using the independence of
arrivals and service, we obtain
Prob{R > t} =
∞

n=0

Prob
	n+1

k=1
Bk > t


pn =
∞

n=0

e−μt
n

k=0
(μt)k
k!

(1 −ρ)ρn
=
n

k=0
∞

n=k

e−μt (μt)k
k!

(1 −ρ)ρn =
n

k=0

e−μt (μt)k
k!
 ∞

n=k
(1 −ρ)ρn
=
n

k=0

e−μt (μt)k
k!

ρk = e−μ(1−ρ)t,
t ≥0.
Hence the probability distribution function for the response time in an M/M/1 queue is
Prob{R ≤t} = Wr(t) = 1 −e−(μ−λ)t,
i.e., the exponential distribution with mean
E[R] =
1
μ −λ =
1
μ(1 −ρ).
Notice that it is not possible to write this performance measure just in terms of ρ. It depends on λ
and μ but not just through their ratio ρ. This means that it is possible to assign values to λ and μ
in such a way that the system can be almost saturated with large queues but still have a very short
expected response time.
The probability density function for the response time can be immediately found as the density
function of the exponential distribution with parameter μ(1 −ρ) or evaluated directly as
wr(t) =
∞

n=0
pngn+1(t) = (1 −ρ)μe−μt
∞

n=0
(ρμt)n
n!
= (μ −λ)e−(μ−λ)t,

11.2 Birth-Death Processes: The M/M/1 Queue
411
where
gn+1(t) = μ(μt)ne−μt
n!
is the density function for the Erlang distribution with (n + 1) phases.
Queueing Time Distribution in an M/M/1 Queue
The waiting time distribution is the distribution of time that an arriving customer must spend waiting
in the queue before entering service. It is possible that a customer does not have to wait at all: an
arriving customer can, with positive probability, ﬁnd that the system is empty and enter immediately
into service. The time spent waiting in this case is zero. In all other cases, an arrival ﬁnds the server
busy and must wait. This means that the random variable, Tq, which deﬁnes the time spent waiting
in the queue, is part discrete and part continuous. Let Wq(t) be deﬁned as the probability distribution
function of Tq. It follows from the Poisson arrival property that
Wq(0) = Prob{Tq ≤0} = Prob{Tq = 0} = Prob{System is empty upon arrival} = p0 = 1 −ρ.
We now need to ﬁnd Wq(t) when Tq > 0. If there are n customers already in the system upon the
arrival of a tagged customer at time t = 0, then in order for the tagged customer to go into service
at some future time between 0 and t, all n prior customers must have been served by time t. The
service received by each of these n customers is independent and exponentially distributed, and the
sum of n of them (the convolution of n exponential random variables) is an Erlang-n distribution.
In this case the density function, which describes the time needed for the completion of service of
these n customers is given by
gn(x) = μ(μx)n−1
(n −1)! e−μx,
x ≥0.
Since the input is Poisson, the probability that an arrival ﬁnds n in the system is identical to the
stationary distribution of system size (PASTA). Therefore
Wq(t) = Prob{Tq ≤t}
=
∞

n=1

Prob{n completions in ≤t | arrival ﬁnds n in system} × pn
 
+ Wq(0)
= (1 −ρ)
∞

n=1
ρn
 t
0
μ(μx)n−1
(n −1)! e−μxdx + (1 −ρ)
= (1 −ρ)ρ
 t
0
μe−μx
∞

n=1
(μxρ)n−1
(n −1)! dx + (1 −ρ)
= ρ(1 −ρ)
 t
0
μe−μ(1−ρ)xdx + (1 −ρ)
= 1 −ρe−μ(1−ρ)t,
t > 0.
Since this also holds when t = 0, we may write the distribution of waiting time in the queue as
Wq(t) = 1 −ρe−μ(1−ρ)t,
t ≥0.
The corresponding probability density function may be found by differentiation. We have
wq(t) = d
dt
#
1 −ρe−μ(1−ρ)t$
= ρμ(1 −ρ)e−μ(1−ρ)t.

412
Elementary Queueing Theory
Now we can get the expected waiting time:
Wq = E[Tq] =
 ∞
0
tρμ(1 −ρ)e−μ(1−ρ)tdt
=
 ∞
0
t λ
μ(μ −λ)e−(μ−λ)tdt
= λ
μ
 ∞
0
t(μ −λ)e−(μ−λ)tdt
= λ
μ
1
μ −λ,
since
 ∞
0
t(μ −λ)e−(μ−λ)tdt =
1
μ −λ
is the mean of the exponential distribution. Hence
Wq =
λ
μ(μ −λ).
11.2.3 Transient Behavior
It is possible to derive equations that characterize the transient solution of the M/M/1 queue.
However, here we shall be content to outline the steps needed to arrive at this solution, rather than
go through the tedious procedure of deriving the solution. For those who seek more details, we
recommend the analysis provided in Gross and Harris [19].
The transient solution of the M/M/1 queue is obtained by solving the set of differential-difference
equations which we derived in Section 11.2.1. These equations are
dpn(t)
dt
= −(λ + μ)pn(t) + λpn−1(t) + μpn+1(t),
n ≥1,
dp0(t)
dt
= −λp0(t) + μp1(t).
The solution is obtained by ﬁrst deﬁning the time-dependent transform
P(z, t) ≡
∞

n=0
pn(t)zn
for complex z,
multiplying the nth equation by zn, and summing over all permissible n to obtain a single differential
equation for the z-transform of pn(t). This yields a linear, ﬁrst-order partial differential equation for
P(z, t),
z ∂
∂t P(z, t) = (1 −z)[(μ −λz)P(z, t) −μp0(t)].
This equation must once again be transformed, this time using the Laplace transform for P(z, t),
given by
P∗(z, s) ≡
 ∞
0+ e−st P(z, t)dt.

11.3 General Birth-Death Processes
413
We obtain
P∗(z, s) = zP(z, 0) −μ(1 −z)P∗
0 (s)
sz −(1 −z)(μ −λz)
.
Thus we have transformed the set of differential-difference equations for pn(t) both on the discrete
variable n and on the continuous variable t. It is now necessary to turn to Rouché’s theorem to
determine the unknown function P∗
0 (s), by appealing to the analyticity of the transform. After much
work, this leads to an explicit expression for the double transform which must then be inverted on
both transform variables. The ﬁnal solution for the transient solution of the M/M/1 queue is obtained
as
pn(t) = e−(λ+μ)t
⎡
⎣ρ(n−i)/2In−i(at) + ρ(n−i−1)/2In+i+1(at) + (1 −ρ)ρn
∞

j=n+i+2
ρ−j/2I j(at)
⎤
⎦,
where i is the initial system size (i.e., pn(0) = 1 if n = i, and pn(0) = 0 if n ̸= i), ρ = λ/μ,
a = 2μρ1/2, and
In(x) ≡
∞

m=0
(x/2)n+2m
(n + m)!m!
is the modiﬁed Bessel function of the ﬁrst kind and of order n.
11.3 General Birth-Death Processes
A birth-death process may be viewed as a generalization of the M/M/1 queueing system. Perhaps
more correctly, the M/M/1 queue is a special type of birth-death process. Whereas the birth and
death rates in the M/M/1 queue (λ and μ, respectively) are the same irrespective of the number
of customers in the system, a general birth-death process allows for different rates depending on
the number of customers present. Arrivals to the system continue to be referred to as births and
departures as deaths but now we introduce a birth rate λn, which is deﬁned as the rate at which
births occur when the population is of size n, and a death rate μn, deﬁned as the rate at which deaths
occur when the population is of size n. Notice that, for all n, both λn and μn are independent of t
which means that our concern is with continuous-time homogeneous Markov chains. On the other
hand, the parameters λn and μn can, and frequently do, depend on the currently occupied state of
the system, namely, state n.
11.3.1 Derivation of the State Equations
As for the M/M/1 queue, a state of the system at any time is characterized completely by specifying
the size of the population at that time. Let pn(t) be the probability that the population is of size n at
time t. We assume that births and deaths are independent and that
Prob{One birth in (t, t + 
t] | n in population} = λn
t + o(
t),
Prob{One death in (t, t + 
t] | n in population} = μn
t + o(
t),
Prob{Zero births in (t, t + 
t] | n in population} = 1 −λn
t + o(
t),
Prob{Zero deaths in (t, t + 
t] | n in population} = 1 −μn
t + o(
t).
These assumptions mean that the probability of two or more births, of two or more deaths, or of
near simultaneous births and deaths in some small interval of time 
t is negligibly small, i.e., of
order o(
t).

414
Elementary Queueing Theory
n-1
n
n+1
1
0
λ
λ
μ
μ
μ
λ
λ
μ
λ1
1
n
n+1
n+1
n
n-1
2
0
μn-1
Figure 11.12. State transitions in a birth-death process.
The state transition diagram is shown in Figure 11.12. Notice that we make the assumption that
it is impossible to have a death when the population is of size 0 (i.e., μ0 = 0) but that one can
indeed have a birth when the population is zero (i.e., λ0 > 0). We now need to develop the set of
differential-difference equations that deﬁne the dynamics of this birth-death process, and to do so,
we could proceed as for the M/M/1 queue, by deriving a relationship for pn(t + 
t) and taking the
limit of

pn(t + 
t) −pn(t)
 
/
t as 
t tends to zero. Instead, we shall use a short cut based on
the fact that the rate at which probability “accumulates” in any state n is the difference between the
rates at which the system enters and leaves that state. The rate at which probability “ﬂows” into state
n at time t is
λn−1 pn−1(t) + μn+1 pn+1(t),
while the rate at which it “ﬂows” out is
(λn + μn)pn(t).
Subtracting these gives the effective probability ﬂow rate into n, i.e.,
dpn(t)
dt
= λn−1 pn−1(t) + μn+1 pn+1(t) −(λn + μn)pn(t),
n ≥1,
(11.8)
and
dp0(t)
dt
= μ1 p1(t) −λ0 p0(t),
which are forms of the Chapman–Kolmogorov forward equations. This shortcut is illustrated in
Figure 11.13 where we have separated out state n by surrounding it with a dashed line. The net
rate of probability ﬂow into n is found by computing the ﬂow across this boundary, using opposing
signs for entering and leaving. Notice that if λn = λ and μn = μ for all n, we get exactly the same
equation that we previously derived for the M/M/1 queue.
0
1
n-1
n
n+1
μ1
μ2
μn-1
μn+1
n+1
λ
λn
λ1
λ0
λ
μn
n-1
Figure 11.13. Transitions from/to state n.
Example 11.9 A pure birth process.
In a pure birth process, there are no deaths, only births. The rates of transition are given by
λn = λ
for all n,
μn = 0
for all n.

11.3 General Birth-Death Processes
415
The equations become
dpn(t)
dt
= −λpn(t) + λpn−1(t)
for n ≥1,
dp0(t)
dt
= −λp0(t).
If initially pn(0) = 1 if n = 0 and is equal to 0 if n ≥1, which means that the system begins empty,
then
p0(t) = e−λt,
since the solution is given by p0(t) = ce−λt where c, the constant of integration, must be equal to
one (p0(0) = 1 = ce0). Furthermore, from
dp1(t)
dt
= −λp1(t) + λe−λt
the solution is clearly
p1(t) = λte−λt
since application of the product rule (uv)′ = udv + vdu with u = λt and v = e−λt yields
λt
#
−λe−λt$
+ λe−λt = −λp1(t) + λe−λt.
Similarly, continuing by induction, we obtain
pn(t) = e−λt (λt)n
n! ,
n ≥0, t ≥0.
This is now seen as the formula for the Poisson distribution with mean λt. In other words, the
Poisson process is a pure birth process with constant birth rate λ.
11.3.2 Steady-State Solution
In the steady state, which we assume exists,
dpn(t)
dt
= 0,
and hence Equation (11.8) becomes
0 = λn−1 pn−1 + μn+1 pn+1 −(λn + μn)pn,
n ≥1,
(11.9)
0 = −λ0 p0 + μ1 p1
=⇒p1 = λ0
μ1
p0,
where pn is deﬁned as the limiting probability, pn = limt→∞pn(t), the probability that the system
contains n customers once it reaches steady state where all inﬂuence of the starting state has been
erased. These equations are called the global balance equations. They may also be written as
pn+1 = λn + μn
μn+1
pn −λn−1
μn+1
pn−1,
n ≥1,
p1 = λ0
μ1
p0.

416
Elementary Queueing Theory
We obtain a solution by iteration
p2 = λ1 + μ1
μ2
p1 −λ0
μ2
p0 = λ1 + μ1
μ2
 λ0
μ1
p0

−λ0
μ2
p0 = λ1λ0
μ2μ1
p0.
Similarly
p3 = λ2λ1λ0
μ3μ2μ1
p0.
This leads us to conjecture that
pn = λn−1λn−2 . . . λ0
μnμn−1 . . . μ1
p0 = p0
n
i=1
λi−1
μi
,
n ≥1.
We prove this by induction. We assume it to be true for all n ≤k, and prove it true for n = k + 1.
We have
pk+1 = λk + μk
μk+1
p0
k
i=1
λi−1
μi
−λk−1
μk+1
p0
k−1

i=1
λi−1
μi
= p0
λk
μk+1
k
i=1
λi−1
μi
+ p0
μk
μk+1
k
i=1
λi−1
μi
−p0
( k
i=1
λi−1
μi
)
μk
μk+1
= p0
k+1

i=1
λi−1
μi
.
We shall return to this “product-form” equation frequently in the remainder of this chapter. Indeed, it
has been referred to as the principal equation in elementary queueing theory. Notice that by returning
to Equation (11.9) and rearranging, we get
λn pn −μn+1 pn+1 = λn−1 pn−1 −μn pn
= λn−2 pn−2 −μn−1 pn−1
= · · ·
= λ0 p0 −μ1 p1 = 0.
Thus
λn pn −μn+1 pn+1 = 0
or
pn+1 =
λn
μn+1
pn,
for n ≥0
(11.10)
and
pn+1 =
λn
μn+1
λn−1
μn
pn−1,
n ≥1
which eventually yields
pn+1 =
λn
μn+1
λn−1
μn
· · · λ0
μ1
p0 = p0
n
i=0
λi
μi+1
,
n ≥0
pn = p0
n−1

i=0
λi
μi+1
= p0
n
i=1
λi−1
μi
,
n ≥1.
Recall that the set of differential-difference equations that represent the dynamics of a general
birth-death process can be obtained by examining the ﬂow into and out of each individual state.

11.3 General Birth-Death Processes
417
Consider what happens to that ﬂow once the system reaches equilibrium. Since the system is
in equilibrium, the rate into any state must be equal to the rate of ﬂow out of that state. More
generally, the rate of ﬂow into and out of any closed set of states must possess this same property
of conservation of probability ﬂow. Let us choose special sets of states that allow us to generate
Equation (11.10). Speciﬁcally, let us choose the following sequence of sets of states: the ﬁrst set
contains state 0, the second set contains states 0 and 1, the third contains state 0, 1 and 2, and so
on, as is illustrated in Figure 11.14. Observe now that equating the ﬂow of across the nth boundary
gives us what we need (Equation11.10), namely
λn−1 pn−1 = μn pn.
0
1
n-1
n
n+1
μ1
μ2
μn-1
μn
μn+1
n+1
λ
λn
λn-1
λ1
λ0
Figure 11.14. Transitions across groups of states.
This method of generating the equilibrium balance equations turns out to be useful in many different
circumstances. The reader might wonder why we cluster the states in this fashion and observe the
ﬂow into and out of each cluster, rather than simply draw a vertical line to separate neighboring
states and equate ﬂow across this line. In this particular case, there is no difference, but in more
complex situations, for example, when states are drawn as a two-dimensional grid, separating states
with a single line becomes more onerous and error-prone.
We now turn our attention to the computation of p0. We have
1 =

n≥0
pn = p0 +

n≥1
pn = p0 +

n≥1
p0
n
i=1
λi−1
μi
= p0
(
1 +

n≥1
n
i=1
λi−1
μi
)
.
Setting ρi = λi−1/μi, we obtain
p0 =
1
1 + 
n≥1
n
i=1 ρi
.
To simplify the notation somewhat, let
ξk =
1,
k = 0,
k
i=1 ρi,
k > 0.
Then
pn = p0ξn =
ξn
∞
k=0 ξk
for n = 0, 1, 2, . . . .
The limiting distribution (p0, p1, p2, . . .) is now completely determined. The limiting probabilities
are nonzero provided that p0 > 0; otherwise pi = 0 for all i = 0, 1, 2, . . . and there is no
steady-state distribution. Thus the existence of a stationary distribution depends on whether or
not there exists a ﬁnite probability that the queueing system empties itself. If it does then the

418
Elementary Queueing Theory
steady-state distribution exists, otherwise it does not. To characterize the states of the system we
need to introduce two quantities, namely,
α =
∞

k=0
ξk = 1 + λ0
μ1
+ λ0λ1
μ1μ2
+ λ0λ1λ2
μ1μ2μ3
+ · · ·
and
β =
∞

k=0
 1
λkξk

.
(11.11)
The states of a general birth-death process are
• transient
if and only if β < ∞,
• null recurrent
if and only if α = ∞and β = ∞,
• ergodic
if and only if α < ∞.
The last of these conditions is what we need for the existence of a steady-state distribution. However,
from a practical point of view, it is usually easier to simply show that p0 > 0. An alternative, and
equally facile condition for ergodicity, is to show that there exists an integer k0 and a constant A < 1
such that
λk
μk+1
≤A < 1
for all k ≥k0.
In this case, there is a point in the state space such that the rate of ﬂow of arrivals into any state at
or beyond this point is less than the rate of departures from that state. This effectively prevents the
population from growing without bound.
Example 11.10 For the M/M/1 queue, with λn = λ and μn = μ for all n and with λ/μ = ρ < 1,
we ﬁnd
α =
∞

k=0
ξk = 1 +
∞

k=1
k
i=1
λi−1
μi
= 1 +
∞

k=1
k
i=1
λ
μ = 1 +
∞

k=1
ρk =
∞

k=0
ρk =
1
1 −ρ < ∞,
which shows that the M/M/1 queue is stable when ρ < 1. Notice also that
β =
∞

k=0
 1
λkξk

= 1
λ
∞

k=0
1
ρk = ∞
since limk→∞ρk = 0.
Matrix Formulation for Birth-Death Processes
As for the M/M/1 queue, we can write the inﬁnite inﬁnitesimal generator of a birth-death process
with birth rates λk and death rates μk:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−λ0
λ0
μ1
−(λ1 + μ1)
λ1
μ2
−(λ2 + μ2)
λ2
μ3
−(λ3 + μ3)
λ3
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
From π Q = 0 , it is obvious that −λ0π0 +μ1π1 = 0, i.e., that π1 = (λ0/μ1)π0. In general, we have
λi−1πi−1 −(λi + μi)πi + μi+1πi+1 = 0,
i.e.,
πi+1 = λi + μi
μi+1
πi −λi−1
μi+1
πi−1,

11.4 Multiserver Systems
419
and from which, by induction, we may derive
πk+1 = π0
k+1

i=1
λi−1
μi
.
Thus, once again, if π0 is known, the remaining values πi, i = 1, 2, . . . , may be determined
recursively.
11.4 Multiserver Systems
11.4.1 The M/M/c Queue
Substitution of λn = λ and μn = μ for all n turns the general birth-death system into the M/M/1
queue. In the sections that follow, we will analyze a number of important queueing systems, simply
by assigning particular values to λn and μn and solving the resulting birth-death equations. We
begin with the case in which there are c identical servers, the M/M/c queue. This is sometimes also
referred to as a queue with parallel channels. Figure 11.15 illustrates this situation.
μ
μ
μ
Arrivals
λ
Departures
c   Servers
…
Figure 11.15. The M/M/c queue.
Customers arrive according to a Poisson process with rate λn = λ for all n and are served in
ﬁrst-come ﬁrst-served order by any available server. Each of the c servers provides independent and
identically distributed exponential service at rate μ. When the number of customers is greater than or
equal to c, i.e., n ≥c, then all the servers are busy and the effective service rate, also called the mean
system output rate (MSOR), is equal to cμ. If the number of customers is less than c, then only n out
of the c servers are busy and the MSOR is equal to nμ. The term load dependent service center is
used to designate a service center in which the customers departure rate is a function of the number
of customers present. With c identical exponential servers, each providing service at rate μ, and a
total of n customers present, the load dependent service rate, written μ(n), is μ(n) = min(n, c)μ.
The state transition diagram is shown in Figure 11.16.
1
0
μ
μ
λ
λ
λ
λ
λ
μ
λ
c-2
c-1
c
c+1
c μ
2
(c-1)
μ
(c-2)
c μ
cμ
Figure 11.16. State transitions in the M/M/c queue.
It follows that the appropriate birth and death rates to deﬁne the M/M/c queue are given by
λn = λ
for all n,
μn =
nμ,
1 ≤n ≤c,
cμ,
n ≥c.

420
Elementary Queueing Theory
The original birth-death system gave rise to the equations
pn = p0
n
i=1
λi−1
μi
,
which in the M/M/c queue become
pn = p0
n
i=1
λ
iμ = p0
 λ
μ
n 1
n!
if 1 ≤n ≤c,
and
pn = p0
c
i=1
λ
iμ
n
i=c+1
λ
cμ = p0
 λ
μ
n 1
c!
1
c
n−c
if n ≥c.
We deﬁne ρ = λ/(cμ), and in order for the system to be stable, we must have ρ < 1. This
implies that the mean arrival rate must be less than the mean maximum potential rate with which
customers can be served. This expression for ρ is consistent with our deﬁnition in terms of the
expected fraction of busy servers, the utilization in an M/M/c queue, since the expected number of
busy servers is equal to cρ = λ/μ.
Returning to the previous equation and substituting cρ for λ/μ, we obtain
pn = p0
(cρ)n
n!
for n ≤c,
pn = p0
(cρ)n
cn−cc! = p0
ρncc
c!
for n ≥c.
All that remains is to solve for p0:
1 =
∞

n=0
pn = p0 +
∞

n=1
pn = p0
(
1 +
c−1

n=1
(cρ)n
n!
+
∞

n=c
ρncc
c!
)
,
i.e.,
p0 =
(
1 +
c−1

n=1
(cρ)n
n!
+
∞

n=c
ρncc
c!
)−1
.
Since
∞

n=c
ρncc
c!
= 1
c!
∞

n=c
ρncc = (cρ)c
c!
∞

n=c
ρn−c = (cρ)c
c!
1
1 −ρ ,
p0 =
(
1 +
c−1

n=1
(cρ)n
n!
+ (cρ)c
c!
1
1 −ρ
)−1
.
This gives us what we need. To summarize, the steady-state distribution of customers in an M/M/c
queue is given by
pn = p0
(cρ)n
n!
for n ≤c,
pn = p0
(cρ)n
cn−cc! = p0
ρncc
c!
for n ≥c,

11.4 Multiserver Systems
421
where
p0 =
( c−1

n=0
(cρ)n
n!
+ (cρ)c
c!
1
1 −ρ
)−1
.
Performance Measures for the M/M/c Queue
It is clearly easier to begin by computing the expected queue length Lq rather than the mean number
in the system, since the only pn’s involved are those for which n ≥c. These are the only states for
which customers must wait in the queue. We have
Lq =
∞

n=c
(n −c)pn
with
pn = (ρc)n
cn−cc! p0
for n ≥c,
which implies that
Lq =
∞

n=c
n
cn−cc!(ρc)n p0 −
∞

n=c
c
cn−cc!(ρc)n p0.
Let us consider each of the terms on the right-hand side separately. The ﬁrst term is given by
p0
c!
∞

n=c
n(ρc)n
cn−c .
Notice that, since
nρncn
cn−c = nρncc = nρn−c−1ρc+1cc+1
c
= (ρc)c+1
c
nρn−c−1,
we have
p0
c!
∞

n=c
n(ρc)n
cn−c
= p0
c!
&(ρc)c+1
c
' ∞

n=c

(n −c)ρn−c−1 + cρn−c−1 
= p0
c!
&(ρc)c+1
c
' 	 ∞

n=c
(n −c)ρn−c−1 +
∞

n=c
cρn−c−1

= p0
c!
&(ρc)c+1
c
' 
1
(1 −ρ)2 + c
ρ
1
1 −ρ
%
,
using derivatives of the geometric series,
= p0
c!
&(ρc)c+1
c
' 
1
(1 −ρ)2 + c/ρ
1 −ρ
%
.
Now for the second term. We have
p0
c!
∞

n=c
c(ρc)n
cn−c
= p0
c!
∞

n=c
cρcρn−ccc = p0
c! c(ρc)c
∞

n=c
ρn−c
= p0
c!
c(ρc)c
1 −ρ = p0
c!
&(ρc)c+1
c
' c/ρ
1 −ρ .

422
Elementary Queueing Theory
Bringing these two terms together we obtain the mean number of customers waiting in the queue as
Lq = p0
c!
&(ρc)c+1
c
' 
1
(1 −ρ)2 + c/ρ
1 −ρ −c/ρ
1 −ρ
%
and thus
Lq = (ρc)c+1/c
c!(1 −ρ)2 p0
or, alternatively,
Lq =
(λ/μ)cλμ
(c −1)!(cμ −λ)2 p0.
Having computed Lq, we are now in a position to ﬁnd other performance measures, either by
means of Little’s law or from simple relationships among the measures. To ﬁnd Wq, the mean time
spent waiting prior to service; W, the mean time spent in the system, and L, the mean number of
customers in the system, we proceed as follows:
1. Use Lq = λWq to ﬁnd Wq.
2. Use W = Wq + 1/μ to ﬁnd W.
3. Use L = λW to ﬁnd L.
We obtain
Wq =
&
(λ/μ)cμ
(c −1)!(cμ −λ)2
'
p0,
W =
&
(λ/μ)cμ
(c −1)!(cμ −λ)2
'
p0 + 1
μ,
L =
&
(λ/μ)cλμ
(c −1)!(cμ −λ)2
'
p0 + λ
μ.
The probability that an arriving customer is forced to wait in the queue, which means that there
is no server available, leads to the “Erlang-C formula.” It is the probability that all servers are busy
and is given by
Prob{queueing} =
∞

n=c
pn = p0
∞

n=c
cc
c!ρn = p0
cc
c!
& ρc
1 −ρ
'
=
(cρ)c
c!(1 −ρ) p0 =
(λ/μ)cμ
(c −1)!(cμ −λ) p0.
This then is the Erlang-C formula. It is denoted by C(c, λ/μ). Observe that mean queue/system
lengths and mean waiting times can all be written in terms of this formula. We have
Lq = (ρc)c+1/c
c!(1 −ρ)2 p0 =
(ρc)c
c!(1 −ρ) p0 ×
ρ
(1 −ρ) =
ρ
(1 −ρ)C(c, λ/μ) =
λ
cμ −λC(c, λ/μ).
Wq = 1
λ
ρC(c, λ/μ)
(1 −ρ)
= C(c, λ/μ)
cμ −λ ,
W = 1
λ
ρC(c, λ/μ)
(1 −ρ)
+ 1
μ = C(c, λ/μ)
cμ −λ
+ 1
μ,
L = ρC(c, λ/μ)
(1 −ρ)
+ cρ = λC(c, λ/μ)
cμ −λ
+ λ
μ.

11.4 Multiserver Systems
423
Example 11.11 Let us compare, on the basis of average response time, the performance of two
identical servers each with its own separate queue, to the case when there is only a single queue to
hold customers for both servers. The systems to compare are illustrated in Figure 11.17. We shall
also check to see how these two possibilities compare to a single processor working twice as fast.
μ
μ
(a)  Separate queue system
μ
μ
λ
(b)   Common queue
/ 2
λ
/ 2
λ
Figure 11.17. Two conﬁgurations.
In the ﬁrst case, we have two independent M/M/1 queues, each with arrival rate λ/2 and service
rate μ. It follows that ρ = (λ/2)/μ = λ/(2μ). The mean number in each M/M/1 queue is given by
ρ/(1 −ρ) so that the mean number of customers in this ﬁrst scenario is given as
L1 = E[N1] = 2 ×
ρ
1 −ρ =
2ρ
1 −ρ .
The average response time can now be found using Little’s law. We have
E[R1] = 1
λ E[N1] = 1
λ
2ρ
(1 −ρ) =
2
2μ −λ.
Now consider the second scenario in which the system may be represented as an M/M/2 queue. To
ﬁnd E[R2], we ﬁrst must ﬁnd E[N2] (= L2). The mean number of customers in an M/M/c queue
with arrival rate λ and service rate μ per server is given by
E[N2] = λ
μ +
(λ/μ)cλμ
(c −1)!(cμ −λ)2 p0
with
λ
cμ = ρ
or
λ/μ = cρ.
With c = 2, we obtain
L2 = E[N2] = λ
μ + (λ/μ)2λμ
(2μ −λ)2 p0 = λ
μ +
(λ/μ)2(λ/μ)
(1/μ2)(2μ −λ)2 p0
= λ
μ +
(λ/μ)3
(2 −λ/μ)2 p0
= 2ρ +
(2ρ)3
(2 −2ρ)2 p0.

424
Elementary Queueing Theory
The probability of the system being empty, p0, is computed as
p0 =
(
1 +
c−1

n=1
(cρ)n
n!
+ (cρ)c
c!

1
1 −ρ
)−1
=
&
1 + 2ρ + (2ρ)2
2!
1
1 −ρ
'−1
= 1 −ρ
1 + ρ .
Thus,
L2 = 2ρ +
8ρ3(1 −ρ)
4(1 −ρ)2(1 + ρ) = 2ρ(1 −ρ)(1 + ρ) + 2ρ3
(1 −ρ)(1 + ρ)
=
2ρ
1 −ρ2 .
Now, using Little’s formula,
E[R2] = 1
λ E[N2] = 2ρ/λ
1 −ρ2 =
1/μ
1 −ρ2 =
4μ
4μ2 −λ2 .
Finally, to compare these to a single superserver working twice as fast, we use the M/M/1 queue
with arrival rate λ and service rate 2μ. We obtain
L3 = E[N3] =
ρ
1 −ρ
and
E[R3] =
1/2μ
1 −λ/2μ =
1
2μ −λ.
These results are summarized below.
E[N1] =
2ρ
1 −ρ
≥
E[N2] =
2ρ
1 −ρ ·
1
1 + ρ
≥
E[N3] =
ρ
1 −ρ ,
E[R1] =
2
2μ −λ
≥
E[R2] =
1/μ
1 −ρ2
≥
E[R3] =
1
2μ −λ,
or, setting α = 2ρ/(1 −ρ) and β = 2/(2μ −λ)—notice that α = λβ,
E[N1] = α
≥
E[N2] = α ·
1
1 + ρ
≥
E[N3] = α/2,
E[R1] = β
≥
E[R2] = β ·
1
1 + ρ
≥
E[R3] = β/2.
These results show that the ﬁrst scenario, that of two separate M/M/1 queues, is the worst of the
three scenarios and indeed is twice as bad as the single super M/M/1 queue which works twice as
fast. The second scenario, that of an M/M/2 queue, performs somewhere between the other two.
Since 0 ≤ρ ≤1, we must have 1 ≤1 + ρ ≤2, meaning that the performance of the M/M/2
queue approaches that of the pair of M/M/1 queues as the trafﬁc intensity decreases (ρ →0), and
approaches that of the super M/M/1 queue when the trafﬁc intensity increases,2 i.e., as ρ →1.
Results similar to those in this illustrative example are readily obtained when the number of
separate, independent M/M/1 queues is increased to c, the M/M/2 queue replaced by an M/M/c
queue, and the single super M/M/1 queue works at rate cμ. In particular, the response time of c
individual M/M/1 queues is exactly c times as long as that of the super M/M/1 queue. These results
are also intuitive. One should expect c independent M/M/1 queues to perform worse than a single
M/M/c queue, since, with the multiple queues, it is possible for some to be working and others to
be idle. Furthermore, situations will evolve in which there are even waiting lines in front of some
of the servers of the individual M/M/1 queues, while other servers are idle. As for the M/M/c queue
2 Be aware, when interpreting these results, that α (and hence β) also depends on ρ.

11.4 Multiserver Systems
425
and the super M/M/1 queue, both will perform identically as long as there are at least c customers
present, since in this case all c servers will be busy. The super M/M/1 queue will outperform the
M/M/c queue when the number of customers is strictly less than c, since not all of the c servers in
the M/M/c queue can be working while the super M/M/1 queue continues to operate at rate cμ. Thus
a single queue is better that a separate queue for each server. This minimizes variations in Wq and
Lq, and as a general rule, customers prefer less variation—a truism even when it means a longer
overall wait. Also, one fast server is better than two servers fed by a single queue each working half
as fast. This result, however, does not consider the question of reliability, of the possibility of some
of the servers breaking down and becoming unavailable.
11.4.2 The M/M/∞Queue
We now consider the case when c tends to inﬁnity. This gives the M/M/∞queue, also called the
inﬁnite server, responsive server or ample servers queue or queues with unlimited service. Examples
are self-service-type situations. We have
λn = λ
for all n,
μn = nμ
for all n,
which leads to
pn =
λn
nμ(n −1)μ · · · 2μ1μ p0 =
λn
n!μn p0.
Since
∞

n=0
pn = 1,
we obtain
p0 =
( ∞

n=0
λn
n!μn
)−1
= e−λ/μ,
and this is applicable for all ﬁnite λ/μ. Therefore
pn = (λ/μ)ne−λ/μ
n!
,
which is the Poisson distribution with parameter λ/μ. Notice that λ/μ is not in any way restricted for
the existence of a steady-state solution. This result also holds for the M/G/∞queue. The probability
pn depends only on the mean of the service time distribution.
Since no customer will ever wait in an M/M/∞queue, the mean time spent in the system must
be equal to the mean service time 1/μ, and so the mean number of customers present must be equal
to λ times this or L = λ/μ. This is also easy to verify from ﬁrst principles since
L =
∞

n=1
npn = e−λ/μ
∞

n=1
(λ/μ)n
(n −1)! = e−λ/μ λ
μ
∞

n=1
(λ/μ)n−1
(n −1)! = λ
μ.
Obviously Lq = 0 and Wq = 0, so that W = 1/μ. It is also obvious that the response time
distribution must be identical to the service time distribution. Since there is no waiting time before
entering service, an M/M/∞queue is occasionally referred to as a delay (sometimes pure delay)
station, the probability distribution of the delay being that of the service time.

426
Elementary Queueing Theory
11.5 Finite-Capacity Systems—The M/M/1/K Queue
We now turn our attention to the M/M/1/K queue, illustrated graphically in Figure 11.18. Customers
arrive according to a Poisson process at rate λ and receive service that is exponentially distributed
with a mean service time of 1/μ from a single server. The difference from the M/M/1 queue is
that at most K customers are allowed into the system. A customer who arrives to ﬁnd the system
full simply disappears. That customer is not permitted to enter the system. There is no halt to the
arrival process, which continues to churn out customers according to a Poisson process at rate λ, but
only those who arrive to ﬁnd strictly fewer than K present are permitted to enter. In communication
systems, rejected customers are called “lost” customers and the system is called a “loss” system. In
the particular case of K = 1, in which a customer is permitted to enter only if the server is idle, the
term “blocked calls cleared” is used. In other contexts, the M/M/1/K queue is referred to as a queue
with truncation.
μ
λ
Lost
K
Figure 11.18. The M/M/1/K queue.
To analyze the M/M/1/K queue, we use the birth-death model and choose the parameters so that
the Poisson arrivals stop as soon as K customers are present. The parameters of the exponential
service time distribution are unchanged. Thus
λn =
λ,
n < K,
0,
n ≥K,
μn = μ
for n = 1, 2, . . . , K.
1
0
μ
μ
λ
λ
μ
μ
μ
λ
λ
K-2
K
K-1
2
λ
μ
λ
Figure 11.19. State transitions in M/M/1/K queue.
The state transition diagram is shown in Figure 11.19. The principal equation for birth-death
processes is given by
pn = p0
n
i=1
λi−1
μi
,
n ≥1,
and this equation continues to hold in the case when the state space is truncated. Since this
continuous-time Markov chain is ﬁnite and irreducible for all strictly positive λ and μ, it follows

11.5 Finite-Capacity Systems—The M/M/1/K Queue
427
that all states are ergodic. We have
p1 = λ
μ p0,
pn+1 = λ + μ
μ
pn −λ
μ pn−1,
1 ≤n < K −1,
pK = λ
μ pK−1.
Thus, for all possible values of n, and setting ρ = λ/μ,
pn =
 λ
μ
n
p0 = ρn p0,
0 ≤n ≤K.
Notice, in particular, that this holds when n = K. We now need to compute p0. We use
K

n=0
pn = 1
and thus
p0 =
1
K
n=0 ρn .
The divisor is a ﬁnite geometric series and
K

n=0
ρn = 1 −ρK+1
1 −ρ
if ρ ̸= 1
and is equal to K + 1 if ρ = 1. Hence
p0 =

(1 −ρ)/(1 −ρK+1)
if ρ ̸= 1
1/(K + 1)
if ρ = 1.
Therefore, for all 0 ≤n ≤K,
pn = (1 −ρ)ρn
1 −ρK+1
if ρ ̸= 1,
and is equal to 1/(K + 1) if ρ = 1. Thus, the steady-state solution always exists, even for ρ ≥1.
The system is stable for all positive values of λ and μ. When λ > μ, the number of customers in
the system will increase, but it is bound from above by K. Also notice that ρ no longer represents
the utilization. We shall derive an expression for this in just a moment. Finally, notice what happens
as K →∞and ρ < 1. We have
lim
K→∞
1 −ρ
1 −ρK+1 ρn = (1 −ρ)ρn,
which is the result previously obtained for the M/M/1 queue.
Example 11.12 Perhaps the simplest example we can present is the M/M/1/1 queue. This gives rise
to a two-state birth–death process, as illustrated in Figure 11.20.
1
0
μ
λ
Figure 11.20. State transition diagram for the M/M/1/1 queue.

428
Elementary Queueing Theory
Given the following results for the M/M/1/K queue:
pn =
 λ
μ
n
p0
for
n ≥1
and
p0 =
( K

n=0
ρn
)−1
,
and applying them to the M/M/1/1 queue, we have
p0 =
1
1 + ρ
and
p1 =
ρ
1 + ρ .
The M/M/1/1 example has application to the so-called machine breakdown problem, wherein a
machine (or machine component) fails and is subsequently repaired. State 0 is used to represent the
machine in its working condition while state 1 represents the machine undergoing repair. To qualify
as a birth-death process, it is necessary that both failures and repairs occur according to exponential
distributions with rates λ and μ, respectively. Thus the mean time to failure (MTTF) is equal to 1/λ,
the mean time to repair (MTTR) is equal to 1/μ, and the steady-state availability of the machine is
just the steady-state probability of the system being in state 0. The availability is therefore given by
1
1 + ρ =
1
1 + λ/μ =
1/λ
1/λ + 1/μ =
MTTF
MTTF + MTTR.
Performance Measures for the M/M/1/K Queue
We consider the case when ρ ̸= 1, and let L be the expected system size. Then
L =
K

n=0
npn
and
pn = ρn p0.
We have
L = p0ρ
K

n=0
nρn−1 = p0ρ
K

n=0
d
dρ (ρn) = p0ρ d
dρ
 K

n=0
ρn

= p0ρ
 d
dρ
&1 −ρK+1
1 −ρ
'
= p0ρ
[1 −ρ][−(K + 1)ρK] + 1 −ρK+1
(1 −ρ)2

= p0ρ
−(K + 1)ρK + (K + 1)ρK+1 + 1 −ρK+1
(1 −ρ)2

= p0ρ
1 −(K + 1)ρK + KρK+1
(1 −ρ)2

.
Using
p0 =
1 −ρ
1 −ρK+1
we ﬁnd
L = ρ

1 −(K + 1)ρK + KρK+1 
(1 −ρ)2
1 −ρ
1 −ρK+1
= ρ

1 −(K + 1)ρK + KρK+1 
(1 −ρ)(1 −ρK+1)
.

11.5 Finite-Capacity Systems—The M/M/1/K Queue
429
To compute Lq, we use
Lq = L −(1 −p0) = L −

1 −
1 −ρ
1 −ρK+1

= L −1 −ρK+1 −1 + ρ
1 −ρK+1
= L −ρ(1 −ρK)
1 −ρK+1 .
To ﬁnd W, we use Little’s law, (W = L/λ). However, what we need is the effective arrival rate,
which we denote by λ′. This is the mean rate of customers actually entering the system and is
given by
λ′ = λ(1 −pK)
since customers only enter when space is available. W and Wq may now be obtained from Little’s
law,
W = 1
λ′ L
and
Wq = 1
λ′ Lq.
Alternatively, we could also use
Wq = W −1
μ.
Notice that in computing Lq we did not use the formula Lq = L −ρ as we did in the M/M/1 queue
with ρ = λ/μ, instead choosing to write this as Lq = L −(1−p0). The reason should now be clear,
as we require the effective arrival rate λ′ and not λ. Thus, we could have used Lq = L −λ′/μ.
Throughput and Utilization in the M/M/1/K Queue
In a queueing situation in which customers may be lost, the throughput cannot be deﬁned as
being equal to the customer arrival rate—not all arriving customers actually enter the queue. The
probability that an arriving customer is lost is equal to the probability that there are already K
customers in the system, i.e., pK. The probability that the queue is not full, and hence the probability
that an arriving customer is accepted into the queueing system is 1 −pK. Thus the throughput, X,
is given by
X = λ(1 −pK).
It is interesting to examine the throughput in terms of what actually leaves the M/M/1/K queue. So
long as there are customers present, the server serves these customers at rate μ. The probability that
no customers are present is p0, so the probability that the server is working is given by 1 −p0. It
follows that the throughput must be given by X = μ(1 −p0). Thus
X = λ(1 −pK) = μ(1 −p0).
As we have just seen,
p0 =
(1 −ρ)
(1 −ρK+1)
and
pn = (1 −ρ)ρn
1 −ρK+1 .
Therefore
1 −p0
1 −pK
=
1 −

(1 −ρ)/(1 −ρK+1)
 
1 −

(1 −ρ)ρK/(1 −ρK+1)
 =
(1 −ρK+1) −(1 −ρ)
(1 −ρK+1) −(1 −ρ)ρK = ρ −ρK+1
1 −ρK
= ρ
and hence
λ(1 −pK) = μ(1 −p0).
Observe that (1 −pK) is the probability that the queue is not full: it is the probability that new
customers can enter the system and λ(1−pK) is the effective arrival rate, the rate at which customers
enter the system. Similarly, (1 −p0) is the probability that the system is busy and so μ(1 −p0) is

430
Elementary Queueing Theory
the effective departure rate from the system. The equation λ(1 −pK) = μ(1 −p0) simply says that
the effective rate into and out of the system are the same at steady state.
The utilization of the M/M/1/K queue is the probability that the server is busy. We have just seen
that this is given by
U = 1 −p0 = 1
μ X = ρ(1 −pK).
Finally, observe that X = μU.
Example 11.13
In a multiprogramming system, computer processes utilize a common CPU and
an I/O device. After computing for an exponentially distributed amount of time with mean 1/μ,
a process either joins the queue at the I/O device (with probability r) or exits the system (with
probability 1 −r). At the I/O device, a process spends an exponentially distributed amount of time
with mean 1/λ and then joins the CPU queue. Successive CPU and I/O bursts are assumed to be
independent of each other. The number of processes allowed in the system at any time is ﬁxed at a
constant degree of multiprogramming equal to K—the system is set up so that as soon as a process
exits after a CPU burst, an identical process enters the CPU queue. Since the processes are assumed
to be stochastically identical, this may be represented by feeding a departing process back into the
CPU queue. This situation is shown graphically in Figure 11.21.
1−r
λ
μ
I/O
CPU
r
Figure 11.21. Model of a multiprogramming system.
Let the number of processes in the CPU denote the state of the system. It is not necessary to
simultaneously give the number of processes in the CPU queue and at the I/O device to completely
characterize a state of this system. If there are n processes at the CPU, there must be K −n at the
I/O device. Thus we use the number of processes at the CPU to represent the state of the system.
Figure 11.22 displays the state transition diagram. Notice that this is identical to the M/M/1/K queue
with μ replaced by rμ.
r
r
r
r
r
1
0
λ
2
3
λ
λ
λ
K
μ
μ
μ
μ
μ
λ
Figure 11.22. State transitions for a multiprogramming system.
Setting ρ = λ/(rμ), the steady-state probability is given by
pi = (ρ)i p0,
with
p0 =
( K

i=0
ρi
)−1
=

(1 −ρ)/(1 −ρK+1),
ρ ̸= 1,
1/(K + 1),
ρ = 1.

11.5 Finite-Capacity Systems—The M/M/1/K Queue
431
The CPU utilization, UCPU = 1 −p0, is given by
UCPU = ρ −ρK+1
1 −ρK+1
if ρ ̸= 1,
UCPU =
K
K + 1
if ρ = 1.
To calculate the throughput, we need to be careful about using the previous deﬁnition given for the
M/M/1/K queue, namely that X = μU. In the multiprogramming scenario, the throughput should
relate to the processes that actually leave the system rather than those which depart from the CPU.
Whenever the CPU is busy (given by UCPU), it pushes processes through at a rate of μ per time unit
but of these only a fraction (given by 1 −r) actually depart from the system. Therefore the average
system throughput is given by (1 −r)μ UCPU.
Transient behavior of the M/M/1/1 Queue
It is relatively easy to compute the transient behavior of the M/M/1/K queue when only one customer
is allowed into the system (K = 1). In this case the birth-death process has only two states, one in
which no one is present in the system, and the second in which the server is busy serving a customer
and no other customer is allowed to enter. The state transition diagram is shown in Figure 11.23.
1
0
μ
λ
Figure 11.23. State transitions in the M/M/1/1 queue.
We have
p1(t) + p0(t) = 1
at any time t
and
dp0(t)
dt
= −λp0(t) + μp1(t),
dp1(t)
dt
= −μp1(t) + λp0(t).
Also,
p0(t) = 1 −p1(t)
and
p1(t) = 1 −p0(t)
and so
dp1(t)
dt
= −μp1(t) + λ(1 −p1(t)),
dp1(t)
dt
+ (λ + μ)p1(t) = λ.
This is a ﬁrst-order, linear, ordinary differential equation with constant coefﬁcients. We begin by
multiplying through with e(λ+μ)t to obtain
dp1(t)
dt
e(λ+μ)t + (λ + μ)e(λ+μ)t p1(t) = λe(λ+μ)t.
Observe that the left-hand side is the derivative of the product of p1(t) and e(λ+μ)t and therefore
d
dt
#
e(λ+μ)t p1(t)
$
= λe(λ+μ)t.

432
Elementary Queueing Theory
Integrating, we obtain
e(λ+μ)t p1(t) =
λ
λ + μe(λ+μ)t + c
and so the general solution is given as
p1(t) =
λ
λ + μ + ce−(λ+μ)t.
We shall assume the initial condition, p1(t) = p1(0) at t = 0, which results in
p1(0) = c +
λ
λ + μ
and thus
c = p1(0) −
λ
λ + μ.
The complete solution is
p1(t) =
λ
λ + μ +

p1(0) −
λ
λ + μ

e−(λ+μ)t
=
λ
λ + μ
#
1 −e−(λ+μ)t$
+ p1(0)e−(λ+μ)t,
and
p0(t) = 1 −p1(t) =
μ
λ + μ

1 −e−(λ+μ)t 
+ p0(0)e−(λ+μ)t.
To obtain the steady-state solution from this, we let t →∞and obtain
p1 = lim
t→∞p1(t) =
λ
λ + μ
since
lim
t→∞e−(λ+μ)t = 0.
Also, this must mean that
p0 =
μ
λ + μ.
11.6 Multiserver, Finite-Capacity Systems—The M/M/c/K Queue
We now proceed to a combination of two of the previous queueing systems, the M/M/c queue and
the M/M/1/K queue. The resulting queue is denoted M/M/c/K, otherwise known as a queue with
parallel channels and truncation, or the “c-server loss system.” The state transition diagram for this
system is shown in Figure 11.24.
1
0
μ
μ
λ
λ
λ
λ
λ
μ
λ
c-2
c-1
c
c+1
c
2
(c-1)
μ
(c-2)
c μ
cμ
λ
c
μ
μ
K
Figure 11.24. State transitions in an M/M/c/K queue.

11.6 Multiserver, Finite-Capacity Systems—The M/M/c/K Queue
433
As we mentioned earlier, the principal equation in elementary queueing theory, the steady-state
solution of a general birth-death process, is given by
pn = p0
n
i=1
λi−1
μi
in which p0 is computed from ∞
n=0 pn = 1. We have seen that results for a variety of simple
queueing systems are obtained from this by substituting appropriate values for λn and μn. In
particular, for the M/M/c queue, we substituted as follows
λn = λ ∀n,
μn =
nμ,
1 ≤n ≤c,
cμ,
n ≥c,
while for the M/M/1/K queue we used
λk =

λ,
k < K,
0,
k ≥K,
μk = μ
for all k.
For the M/M/c/K queue, the values for the arrival and service rates are taken to be
λn =

λ,
0 ≤n < K,
0,
n ≥K,
μn =

nμ,
1 ≤n ≤c,
cμ,
c ≤n ≤K.
Returning to the fundamental equation,
pn = p0
n
i=1
λi−1
μi
,
and making the appropriate substitutions, we get
pn =
λn
nμ(n −1)μ · · · 2μ1μ p0,
0 ≤n ≤c,
pn =
λn
cμcμ · · · cμcμ(c −1)μ(c −2)μ · · · 2μ1μ p0,
c ≤n ≤K,
where the cμ in the denominator appears n −c + 1 times. Thus
pn = 1
n!
 λ
μ
n
p0,
0 ≤n ≤c,
pn =
1
cn−cc!
 λ
μ
n
p0,
c ≤n ≤K.
Using
K

i=0
pi = 1,
we ﬁnd that
p0 =
( c−1

n=0
1
n!
 λ
μ
n
+
K

n=c
1
cn−cc!
 λ
μ
n)−1
.
The interested reader may wish to check that the case c = 1 leads to the results previously obtained
for the M/M/1/K queue, and that letting K tend to inﬁnity reproduces the M/M/c results.

434
Elementary Queueing Theory
To determine performance measures, we ﬁrst compute
Lq =
K

n=c
(n −c)pn = p0
(λ/μ)c λ/cμ
c!(1 −λ/cμ)2
(
1 −
 λ
cμ
K−c+1
−

1 −λ
cμ

(K −c + 1)
 λ
cμ
K−c)
= p0
(cρ)cρ
c!(1 −ρ)2

1 −ρK−c+1 −(1 −ρ)(K −c + 1)ρK−c 
,
where we have deﬁned ρ = λ/(cμ). This allows us to obtain the average number of customers in
the system (see also Exercise 11.6.1):
L = Lq +

c −
c−1

n=0
(c −n)pn

= Lq + c −
c−1

n=0
(c −n)(λ/μ)n
n!
p0,
(11.12)
which is the number waiting in the queue plus the average number of busy servers. In the special
case of c = 1, this reduces to
L = Lq + (1 −p0) = Lq + λ′
μ ,
where λ′ = λ(1 −pK) is the effective arrival rate. The time spent in the system can now obtained
from W = L/λ′. Finally, the mean time spent in the queue waiting for service to begin is found
from Wq = W −1/μ or Wq = Lq/λ′.
Erlang’s Loss Formula
For the special case when K = c, i.e., the M/M/c/c queue, the so-called “blocked calls cleared with c
servers” system, these results lead to another well-known formula associated with the name Erlang.
The probability that there are n customers in the M/M/c/c queue is given as
pn =
(λ/μ)n/n!
c
i=0(λ/μ)i/i! = p0
(λ/μ)n
n!
.
The formula for pc is called “Erlang’s loss formula” and is the fraction of time that all c servers are
busy. It is written as B(c, λ/μ) and called “Erlang’s B formula”:
B(c, λ/μ) =
(λ/μ)c/c!
c
i=0(λ/μ)i/i! = p0
(λ/μ)c
c!
.
Notice that the probability that an arrival is lost is equal to the probability that all channels are
busy. Erlang’s loss formula is also valid for the M/G/c/c queue. In other words, the steady-state
probabilities are a function only of the mean service time, and not of the complete underlying
cumulative distribution function. An efﬁcient recursive algorithm for computing B(c, λ/μ) is
given by
B(0, λ/μ) = 1,
B(c, λ/μ) =
(λ/μ)B(c −1, λ/μ)
c + (λ/μ)B(c −1, λ/μ).
We leave its proof as an exercise. It is also possible to express Erlang’s C formula in terms of
B(c, λ/μ) and again, we leave this as an exercise.
11.7 Finite-Source Systems—The M/M/c//M Queue
The M/M/c//M queue is a c-server system with a ﬁnite customer population. No longer do we have
a Poisson input process with an inﬁnite user population. There is a total of M customers and a
customer is either in the system, or else is outside the system and is in some sense “arriving.” When

11.7 Finite-Source Systems—The M/M/c//M Queue
435
a customer is in the arriving condition, then the time it takes for that particular customer to arrive is
a random variable with exponential distribution whose mean is 1/λ. This is the mean time spent per
customer in the “arriving” condition. All customers act independently of each other so that if there
are k customers in the “arriving” state, the total average arrival rate is kλ.
Example 11.14 Consider the situation of M students in a computer lab, each student in front of a
computer terminal. All terminals are connected to a single (c = 1) central processor. Each student
thinks for a period of time that is exponentially distributed at rate λ, then enters a command and
waits for a reply: students alternate between a “thinking” state and an “idle” state. At a time when
the processor is handling k requests, there must be M−k students thinking so that the rate of requests
to the processor is equal to (M −k)λ. Observe that the arrival process itself may be modeled as an
M/M/∞queue. When a student receives a reply from the processor, that student does not need to
queue before beginning the thinking process. The response from the central processor adds one unit
(one thinking student) to the M/M/∞queue.
To summarize then, the total average arrival rate to an M/M/c//M queue in state n is λ(M −n).
Also, each of the c servers works at rate μ. It follows that the parameters to insert into the birth-death
equations are given by
λn =

λ(M −n),
0 ≤n < M,
0,
n ≥M,
μn =
nμ,
0 ≤n ≤c,
cμ,
n ≥c.
From the fundamental equation
pn = p0
n
i=1
λi−1
μi
,
we obtain
pn = M!/(M −n)!
n!
 λ
μ
n
p0 =

M
n
  λ
μ
n
p0
if
0 ≤n ≤c,
(11.13)
pn = M!/(M −n)!
cn−cc!
 λ
μ
n
p0 =
M
n

n!
cn−cc!
 λ
μ
n
p0
if
c ≤n ≤M,
(11.14)
where
M
n

=
M!
n!(M −n)!
is the binomial coefﬁcient.
As usual, p0 is found from the normalization equation and is given by
p0 =
(
1 +
c−1

n=1

M
n
  λ
μ
n
+
M

n=c

M
n

n!
cn−cc!
 λ
μ
n)−1
.
A special case occurs with c = 1, the single-server problem. In this case, the formulae are
pn = p0
 λ
μ
n
M!
(M −n)!,
0 ≤n ≤M,

436
Elementary Queueing Theory
with pn = 0 when n > M. Also, in this case (c = 1)
p0 =
( M

n=0
 λ
μ
n
M!
(M −n)!
)−1
.
In the general case, (c ≥1), the average number of customers in the system, may be computed from
L = M
n=0 npn using Equations (11.13) and (11.14). We have
L =
M

n=0
npn =
c−1

n=0
n

M
n
  λ
μ
n
p0 +
M

n=c
n

M
n

n!
cn−cc!
 λ
μ
n
p0
= p0
( c−1

n=0
n

M
n
  λ
μ
n
+ 1
c!
M

n=c
n

M
n
 n!
cn−c
 λ
μ
n)
.
There is no neater expression for L. First p0 must be calculated and then multiplied by the sum
of two series. When L has been computed, it then becomes possible to compute other quantities
such as
Lq = L −λ′
μ ,
W = L
λ′ ,
Wq = Lq
λ′ ,
where λ′ is equal to the mean arrival rate into the system. To compute this effective arrival rate,
observe that, when n customers are present, the arrival rate is λ(M −n) and thus,
λ′ =
M

n=0
λ(M −n)pn = λM
M

n=0
pn −λ
M

n=0
npn = λM −λL = λ(M −L).
Hence
Lq = L −λ(M −L)
μ
,
W =
L
λ(M −L),
Wq =
Lq
λ(M −L),
Example 11.15 The machine repairman problem.
In the classical version of this problem there are M machines that are subject to breakdown and
a single repairman to ﬁx them. Each machine operates independently of the other machines and its
time to failure is governed by an exponential distribution with parameter λ. Thus each machine fails
at rate λ per unit time. The single repair man requires 1/μ times units to repair a failed machine and
again the repair process is independent of the failure process and has an exponential distribution. If
we allow i to denote the case in which i machines have failed, then the state transition diagram is
as shown in Figure 11.25. Observe that the rate of transition from state i, in which M −i machines
are functioning, to state i + 1 is (M −i)λ. It is apparent that this system falls into the category of
the M/M/1//M queue, the ﬁnite population queueing model.
1
0
μ
μ
2
λ
μ
M
M-1
μ
M
λ
(M-1)
λ
2λ
Figure 11.25. State transitions for machine repairman problem.
If there are c repair men, rather than just one, then we obtain the M/M/c//M queue. Systems like
this are also called parallel redundant system. The system is considered to be functional if there is
at least one component working correctly. The objective is to compute the system availability, the

11.8 State-Dependent Service
437
probability that the system is available, i.e., the probability 1 −pM. For the M/M/1//M queue, the
availability is given by
A = 1 −pM = 1 −p0
 λ
μ
M
M! = 1 −
(λ/μ)M M!
M
k=0(λ/μ)k
M!
(M−k)!
.
For example, if the mean time to repair a component is one-ﬁfth the mean time to failure, then
λ/μ = .2 and we obtain the availability for a system with different numbers of components.
M
1
2
3
4
5
· · ·
10
Availability
.833333
.945946
.974576
.984704
.989061
· · ·
.993168
Obviously, the availability of the system must increase as the number of components increases. In
most practical situations the value of λ/μ is considerably smaller than 1/5 and just a small number
of components is sufﬁcient to give a system availability very close to one.
11.8 State-Dependent Service
As a ﬁnal example of the type of queueing system that lends itself to analysis through the general
birth-death equations, we consider systems in which the mean service rate depends on the state of
the system. We consider the case in which the server works at rate μ1 until there are k customers
in the system, at which point it changes to a different rate μ2. The mean service times now depend
explicitly on the system state. We have
λn = λ
for all n,
μn =

μ1,
1 ≤n < k,
μ2,
n ≥k.
The extension to three, four, or more different rates is straightforward. The balance equations
become
pn =
 λ
μ1
n
p0
if 0 ≤n < k,
pn =
λn
μk−1
1
μn−k+1
2
p0
if n ≥k,
and
p0 =
(k−1

n=0
 λ
μ1
n
+
∞

n=k
λn
μk−1
1
μn−k+1
2
)−1
=
(k−1

n=0
 λ
μ1
n
+
∞

n=k
λk−1
μk−1
1
λn−k+1
μn−k+1
2
)−1
=
(k−1

n=0
 λ
μ1
n
+ ρk−1
1
ρ2
∞

n=k
ρn−k
2
)−1
where ρ1 = λ/μ1 and ρ2 = λ/μ2 < 1. This gives
p0 =
(
1 −ρk
1
1 −ρ1
+ ρ2ρk−1
1
1 −ρ2
)−1
.
Notice that if μ1 = μ2, we get the usual M/M/1 queue.

438
Elementary Queueing Theory
To ﬁnd the expected system size, we have
L =
∞

n=0
npn = p0
(k−1

n=0
nρn
1 +
∞

n=k
nρk−1
1
ρn−k+1
2
)
= p0
(k−1

n=0
nρn
1 + ρk−1
1
ρ2
∞

n=k
nρn−k
2
)
= p0
(k−1

n=0
nρn
1 + ρk−1
1
ρ2
 ∞

n=k
(n −k)ρn−k
2
+
∞

n=k
kρn−k
2
)
= p0
(k−1

n=0
nρn
1 + ρk−1
1
ρ2
 ∞

n=0
nρn
2 + k
∞

n=0
ρn
2
)
.
The ﬁrst term within the brackets can be handled as follows:
k−1

n=0
nρn
1 = ρ1
k−1

n=0
d
dρ1
ρn
1 = ρ1
d
dρ1
1 −ρk
1
1 −ρ1

.
The remaining terms can be treated in a similar fashion and the entire equation simpliﬁes to
L = p0
(
ρ1(1 + (k −1)ρk
1 −kρk−1
1
)
(1 −ρ1)2
+ ρ2ρk−1
1
[k −(k −1)ρ2]
(1 −ρ2)2
)
.
From this we can calculate
Lq = L −(1 −p0),
W = L/λ,
and
Wq = Lq/λ.
Here we cannot use the relationship W = Wq + 1/μ, since μ is not constant.
11.9 Exercises
Exercise 11.1.1 Customers arrive at a service center according to a Poisson process with a mean interarrival
time of 15 minutes.
(a) What is the probability that no arrivals occur in the ﬁrst half hour?
(b) What is the expected time until the tenth arrival occurs?
(c) What is the probability of more than ﬁve arrivals occurring in any half-hour period?
(d) If two customers were observed to have arrived in the ﬁrst hour, what is the probability that both
arrived in the last 10 minutes of that hour?
(e) If two customers were observed to have arrived in the ﬁrst hour, what is the probability that at least
one arrived in the last 10 minutes of that hour?
Exercise 11.1.2 Superposition of Poisson streams is Poisson: let X j, j = 1, 2, . . . , n be n independent
Poisson processes with parameters λ1, λ2, . . . , λn respectively and let X = n
j=1 X j. By expanding the
z-transform of X, E[zX], show that X is a Poisson process. Recall that the z-transform of the Poisson
process X j is given as eα j (z−1), where here we take α = λ jt.
Exercise 11.1.3 Decomposition of a Poisson stream is Poisson: consider a Poisson process X with parameter
λ that is decomposed into k separate Poisson streams X j, j = 1, 2, . . . , k. The decomposition is such that
a customer in the original stream is directed into sub-stream X j with probability p j and k
j=1 p j = 1.

11.9 Exercises
439
Conditioning on n, the number of arrivals in the original stream up to some time t, show ﬁrst that
Prob{X1 = n1, X2 = n2, . . . , Xn = nk | X = n} =
n!
n1!n2! · · · nk! pn1
1 pn2
2 · · · pnk
k ,
where k
j=1 n j = n. Now remove the condition to obtain the desired result, that
Prob{X1 = n1, X2 = n2, . . . , Xn = nk} =
k
j=1
(λp jt)n j
n j!
e−λp j t,
which shows that the k decomposed streams are indeed Poisson processes with parameters λp j.
Exercise 11.1.4 Let (τi, xi) , for i = 1, 2, . . . , 12 denote the arrival time and service requirement of the ﬁrst
12 customers to arrive at a service center that contains two identical servers. Use the third graphical approach
to draw this situation given the following arrivals and services:
(1, 10); (2, 6); (4, 1); (6, 6); (7, 2); (9, 4); (10, 6); (11, 3); (15, 2); (24, 2); (25, 5); (28, 4).
Indicate on your graph, the arrival and departure instant of each customer and the busy and idle periods for
each server.
Exercise 11.1.5 The single server at a service center appears to be busy for four minutes out of every ﬁve, on
average. Also the mean service time has been observed to be equal to half a minute. If the time spent waiting
for service to begin is equal to 2.5 minutes, what is the mean number of customers in the queueing system and
the mean response time?
Exercise 11.2.1 It is known that the solution of a set of second-order difference equations with constant
coefﬁcients such as
0 = −λx0 + μx1,
0 = λxn−1 −(λ + μ)xn + μxn+1,
n ≥1,
is given by
xn = α1xn
1 + α2xn
2,
n = 0, 1, 2, . . . ,
where r1 and r2 are the roots of the quadratic polynomial equation
μr 2 −(λ + μ)r + λ = 0.
Use this information to compute the equilibrium solution of the M/M/1 queue.
Exercise 11.2.2 The transmission rate of a communication node is 480,000 bits per second. If the average size
of an arriving data packet is 120 8-bit characters and the average arrival rate is 18,000 packets per minute, ﬁnd
(a) the average number of packets in the node, (b) the average time spent in the node by a packet and (c) the
probability that the number of packets in the node exceeds 5. Assume the arrival process to be Poisson and the
service time to be exponentially distributed.
Exercise 11.2.3 Each evening Bin Peng, a graduate teaching assistant, works the takeaway counter at
Goodberry’s frozen yogurt parlor. Arrivals to the counter appear to follow a Poisson distribution with mean
of ten per hour. Each customer is served one at a time by Bin and the service time appears to follow an
exponential distribution with a mean service time of four minutes.
(a) What is the probability of having a queue?
(b) What is the average queue length?
(c) What is the average time a customer spends in the system?
(d) How fast, on average, does Bin need to serve customers in order for the average total time a customer
spends in Goodberry’s to be less than 7.5 minutes?
(e) If Bin can spend his idle time grading papers and if he can grade, on average, 24 papers an hour, how
many papers per hour he can average while working at Goodberry’s?

440
Elementary Queueing Theory
Exercise 11.2.4 Consider a barber’s shop with a single barber who takes on average 15 minutes to cut a client’s
hair. Model this situation as an M/M/1 queue and ﬁnd the largest number of incoming clients per hour that can
be handled so that the average waiting time will be less than 12 minutes.
Exercise 11.2.5 Customers arrive at Bunkey’s car wash service at a rate of one every 20 minutes and the
average time it takes for a car to proceed through their single wash station is 8 minutes. Answer the following
questions under the assumption of Poisson arrivals and exponential service.
(a) What is the probability that an arriving customer will have to wait?
(b) What is the average number of cars waiting to begin their wash?
(c) What is the probability that there are more than ﬁve cars altogether?
(d) What is the probability that a customer will spend more than 12 minutes actually waiting before her
car begins to be washed?
(e) Bunkey is looking to expand and he can justify the creation of a second wash station so that two cars
can be washed simultaneously if the average time spent waiting prior to service exceeds 8 minutes.
How much does the arrival rate have to increase in order for Bunkey to justify installing this second
wash station?
Exercise 11.2.6 Let Wr(t) = Prob{R ≤t} be the response time distribution function in an M/M/1 queue.
Derive an expression for Wr(t) in terms of the probability of an arriving customer ﬁnding n customers already
present. Now use the PASTA property to show that
Wr(t) = 1 −e−(μ−λ)t.
Exercise 11.3.1 The rate at which unemployed day workers arrive at a job employment center is a function of
the number already present: they reason that the larger the number already present, the less likely they are to
ﬁnd a job that day. Let this function be such that when n workers are already in the job queue, the arrival rate
is given by αnλ for some α < 1. The system is to be modeled as a birth-death process with constant departure
rate equal to μ.
(a) Draw the state transition rate diagram for this system.
(b) Show that this system is stable.
(c) What are the equilibrium equations?
(d) Solve the equilibrium equations to obtain the steady-state probabilities.
(e) What is the trafﬁc intensity ρ?
Exercise 11.3.2 An M/M/1 queue with discouraged arrivals has parameters
λn =
λ
n + 1
and
μn = μ
for all relevant n.
(a) Does the PASTA property apply to this queue? Justify your answer.
(b) Use Equation (11.11) to ﬁnd the conditions necessary for all states to be ergodic (and which
guarantees the stability of this queue).
(c) Compute the stationary distribution under the assumption that the conditions of part (b) apply.
(d) What is the effective arrival rate needed for the application of Little’s law?
(e) Find the mean number in the queueing system and the mean response time.
Exercise 11.3.3 Consider a birth-death process with the following birth and death rates:
λn =
λ1 + λ2,
n < K,
λ1,
n ≥K,
μn = μ
for all n.
Such a process may be used to model a communication system consisting of a single channel that handles
two different types of packets: voice packets that arrive at rate λ1 and data packets that arrive at rate λ2. Data
packets are refused once the total number of packets reaches a preset limit K. The sizes of both voice and data
packets are assumed to be exponentially distributed with mean equal to 1/μ.
Draw the state transition diagram for this birth-death process and then derive and solve the steady-state
balance equations.

11.9 Exercises
441
Exercise 11.3.4 Consider a two-server queueing system in which one of the servers is faster than the other.
Both servers provide exponential service, the ﬁrst at rate μ1 and the second at rate μ2. We asume that μ1 > μ2.
Let λ be the arrival rate. Deﬁne a state of the system to be the pair (n1, n2) where n1 ≥0 is the number of
customers waiting in the queue plus 1 if the faster server is busy, and n2 ∈{0, 1} is the number of customers
being served by the slower server. For example, the state (5, 1) means that both servers are busy and four
customers are waiting for one of the servers to become free; the state (0, 1) means that only the slow server is
busy and that server is serving the only customer in the system; (2, 0) is not a possible state. The scheduling
discipine is FCFS and an arrival that occurs when the system is empty goes to the faster server.
(a) Draw the state transition rate diagram for this system.
(b) What are the equilibrium equations?
(c) Solve the equilibrium equations to obtain the steady-state probabilities.
(d) Compute the system utilization ρ.
Hint: Observe that not all states have transitions only to linearly adjacent neighbors so that, strictly speaking,
this is not a birth-death process. It can be handled by the birth-death solution procedure by paying special
attention to the balance equations involving the initial states.
Exercise 11.3.5 For each of the following queueing systems, draw the state transition rate diagram, then write
the equilibrium ﬂow equations and compute the probablity that the system is empty.
(a) The M/Er/1/1 queue.
(b) The M/H2/1/1 queue with μ1 = μ and μ2 = 2μ.
Exercise 11.4.1 Prove that the stability condition for an M/M/c queue whose Poisson arrival process has a
mean interarrival time equal to 1/λ and whose c servers provide independent and identically exponentially
distributed service with rate μ, is that ρ = λ/cμ < 1.
Exercise 11.4.2 Consider an M/M/c queue with the following parameters: λ = 8, μ = 3, and c = 4. What is
the probability that an arriving customer ﬁnds all servers busy and is forced to wait? What is the average time
spent waiting like this? and what is the mean number of customers in the system?
Exercise 11.4.3 A crisis center is manned by a staff of trained volunteers who answer phone calls from people
in distress. Experience has taught them that as Christmas approaches they need to be able to cater to a peak
demand when phone calls arrive at a rate of six per hour. Each call requires approximately 20 minutes to calm
and advise the distressed person. At present the center plans to have ﬁve volunteers on hand to handle this
peak period. Analyze this situation as an M/M/c queue and advise the center on whether the ﬁve volunteers
are enough so that the average time a distressed person spends waiting to talk to a counselor is less than 15
seconds. What will this average time be if the center provides six volunteers?
Exercise 11.4.4 The local photographic club is putting on an exhibition in a vast amphitheatre. Visitors arrive
at a rate of two every ﬁve minutes and spend on average a total of 20 minutes looking over the photos. If
arrivals follow a Poisson process and the time spent in the exhibition is exponentially distributed, what is the
average number of visitors present at any one time? What is the probability of ﬁnding more than ten visitors in
attendance?
Exercise 11.5.1 A shoe-shine stand at the local airport has two chairs and a single attendant. A person arriving
to ﬁnd one chair free and the attendant busy with a customer seated on the other chair, waits his turn in the
available chair. Potential customers ﬁnding both chairs occupied simply leave. Under the assumption that
potential customers arrive randomly according to a Poisson process at rate ten per hour and that the time taken
to polish shoes is exponentially distributed with mean 5 minutes, answer the following questions:
(a) Write down the balance equations and ﬁnd the distributions of occupied chairs.
(b) Find the percentage time the attendant is busy and the mean number of customers served in an 8-hour
day.
(c) What percentage of potential customers succeed in getting their shoes shined?

442
Elementary Queueing Theory
Exercise 11.5.2 Suppose that in the multiprocessor system of Example 11.13, the I/O service rate λk is a
function of the number of requests, k, that it has to handle. In particular, assume that
1/λk =
τ
k + 1 + 1
α = ατ + k + 1
α(k + 1) ,
where 1/α is the mean record transmission time, and τ is the mean rotation time, of the I/O device. Derive
expressions for the CPU utilization and the average system throughput.
Exercise 11.6.1 The mean number of customers actually waiting in an M/M/c/K queue is given by Lq =
K
n=c(n −c)pn. Work to incorporate L = K
n=0 npn, the mean number of customers in an M/M/c/K queueing
center, into the right-hand side of this equation and hence deduce Equation (11.12).
Exercise 11.6.2 In the airport shoe-shine scenario of Exercise 11.5.1, assume that there are now two attendants
who work at the same rate.
(a) Write down the balance equations and ﬁnd the distributions of occupied chairs.
(b) Find the mean number of customers served in an 8-hour day.
(c) What percentage of potential customers fail to get their shoes shined?
Exercise 11.6.3 A bank manager observes that when all three of his tellers are busy and the line of customers
waiting to reach a teller reaches six, further arriving customers give this line one glance and promptly leave,
thinking of coming back some other time or even changing banks in frustration. Assuming that the arrival
process of bank customers is such that a potential customer enters the bank every 90 seconds and the time to
serve a customer averages ﬁve minutes, is the bank manager better off by hiring another teller, or by means
of incentives, persuading his three tellers to work about 20% faster so that the time to serve each customer is
reduced to four minutes? You may assume that the arrival process of customers is Poisson, that the service time
distribution is exponential, and that only one of the two options is available to the manager. The main criterion
for choosing one option over the other should be to minimize the probability of customers leaving because the
line is too long. You should also consider waiting time as a secondary criterion.
Exercise 11.6.4 Derive the following recurrence relation for Erlang’s B (loss) formula:
B(0, λ/μ) = 1,
B(c, λ/μ) =
(λ/μ)B(c −1, λ/μ)
c + (λ/μ)B(c −1, λ/μ).
Exercise 11.6.5 What is wrong with the following analysis? Given
C(c, λ/μ) =
(λ/μ)cμ
(c −1)!(cμ −λ) p0,
it follows that
C(c, λ/μ) = (λ/μ)c
(c −1)! p0
μ
cμ −λ = (λ/μ)c
c!
p0
cμ
cμ −λ = B(c, λ/μ)
cμ
cμ −λ =
B(c, λ/μ)
1 −λ/(cμ).
Show that the correct relationship between the Erlang B and C formulae is given by
C(c, λ/μ) =
B(c, λ/μ)
1 −λ/(cμ)

1 −B(c, λ/μ)
.
Exercise 11.7.1 Suppose a single repairman has been assigned the responsibility of maintaining three
machines. For each machine, the probability distribution of running time before a breakdown is exponential
with a mean of 9 hours. The repair time is also exponentially distributed with a mean of 2 hours.
(a) Calculate the steady state probability distribution and the expected number of machines that are not
running.
(b) As a crude approximation, it could be assumed that the calling population is inﬁnite so that the input
process is Poisson with a mean arrival rate of 3 every 9 hours. Compare the result of part 1 of this
question with those obtained from (i) an M/M/1 model, and (ii) an M/M/1/3 model.

11.9 Exercises
443
Exercise 11.7.2 A service station has one gasoline pump. Cars wanting gas arrive according to a Poisson
process at a mean rate of 20/hour. However, if the pump is in use, these potential customers may balk (refuse
to join the queue). To be more precise, if there are n cars already at the service station, the probability that
an arriving potential customer will balk is n/4 for n = 1, 2, 3, 4. The time required to serve a car has an
exponential distribution with a mean service time of 3 minutes.
(a) Construct the transition rate diagram for this system.
(b) For all k, ﬁnd the differential-difference equations for pk(t) = Prob{k in system at time t}.
(c) Determine the stationary probability distribution and hence ﬁnd the average number of cars at the
station.

Chapter 12
Queues with Phase-Type Laws:
Neuts’ Matrix-Geometric Method
In the single-server queues that we have considered so far, the only probability law used to model
the interarrival or service time distributions, is the exponential distribution. This gave rise to queues
that we collectively referred to as birth-death processes. In these systems, transitions from any state
are to adjacent states only and the resulting structure of the transition matrix is tridiagonal. But
sometimes the exponential distribution is simply not adequate. Phase-type distributions allow us
to consider more general situations. Such distributions were discussed previously in Chapter 7 and
were seen to include such common distributions as the Erlang and hypergeometric distributions.
In this chapter we consider how phase-type distributions may be incorporated into single-server
queues. Queueing system with phase-type arrival or service mechanisms give rise to transition
matrices that are block tridiagonal and are referred to as quasi-birth-death (QBD) processes.
Whereas a simple birth-death process gives rise to a tridiagonal matrix in which elements below
the diagonal represent service completions (departures from the system) and elements above the
diagonal describe customer arrivals, subdiagonal blocks in a QBD process represent a more complex
departure process and superdiagonal blocks represent a more complex arrival process. In the past,
the mathematical techniques used to solve such queueing systems depended upon the use of the
z-transform. Nowadays, with the advent of high-speed computers and efﬁcient algorithms, the
matrix-geometric approach introduced by Neuts is more commonly employed. We begin by
analyzing the M/Er/1 queue using Neuts’ method.
12.1 The Erlang-r Service Model—The M/Er/1 Queue
Consider a single server system for which the arrival process is Poisson with rate λ and the service
time, having an Erlang-r distribution, is represented as a sequence of r exponential services each
with rate rμ, as shown in Figure 12.1.
r μ
r
2
μ
r
1
μ
r
Server
Queue
Figure 12.1. The M/Er/1 queue.
The reader should be careful not to associate these r phases with r distinct servers. There cannot
be more than one customer receiving service at any time—the single server provides each customer
taken into service with r consecutive service phases and then ejects that customer from the system.

12.1 The Erlang-r Service Model—The M/Er/1 Queue
445
(0, 0)
(1, 2)
(1, r)
(2, r)
(3, 1)
(3, 2)
(3, 3)
(3, r)
(k, 2)
(k, r)
(2, 1)
(k, 1)
(k, 3)
(1, 1)
(2, 2)
(1, 3)
(2, 3)
λ
λ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
r
λ
λ
λ
λ
λ
r
Figure 12.2. State transition diagram for the M/Er/1 queue.
The density functions of the arrival and service processes are given by
a(t) = λe−λt,
t ≥0,
b(x) = rμ(rμx)r−1e−rμx
(r −1)!
,
x ≥0.
Our ﬁrst task is to formulate a state descriptor for this queueing system. In an M/Er/1 queue, we
must record both the number of customers present and the current phase of service of the customer
in service (if any). Because of the exponential nature of the distribution of both the times between
arrivals and the service times at each of the r phases, a knowledge of the number of customers in
the system and the current phase of service is sufﬁcient to capture all the relevant past history of this
system. It follows that a state of the system can be completely described by the pair (k, i), where
k(k ≥0) is the number of customers in the system, including the one in service, and i(1 ≤i ≤r)
denotes the current phase of service. If k = 0, then the value of i is irrelevant. If k > 0, then r −i +1
denotes the number of phases of service yet to be completed by the customer in service. The future
evolution of the system is a function only of the current state (k, i) and consequently a Markovian
analysis may be carried out.
The state transition diagram is shown in Figure 12.2 where the states are arranged into levels
according to the number of customers present. The states that have exactly k customers are said to
constitute level k. From the structure of this state transition diagram, it is apparent that the transition
rate matrix has the typical block-tridiagonal (or QBD—quasi-birth-death) form:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
0
0
0
· · ·
B10
A1
A2
0
0
· · ·
0
A0
A1
A2
0
· · ·
0
0
A0
A1
A2
· · ·
0
0
0
A0
A1
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠

446
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
in which the matrices Ai, i ≥0 are square and of order r. The matrices A0 represent service
completions at rate rμ from the last service phase at some level k > 0 to the ﬁrst service phase
at level k −1, i.e., transitions from state (k,r) to state (k −1, 1). Thus the only nonzero element in
A0 is the r1 element A0(r, 1) which has the value rμ. The matrices A2 represent arrivals at rate λ
which can occur during service at any phase i at any level k > 0, i.e., transitions from state (k, i)
to state (k + 1, i). It follows that the only nonzero elements in this matrix are the diagonal elements
which are all equal to λ. The superdiagonal elements of the matrices A1 represent phase completion
at rate rμ in service phase i < r at level k > 0, i.e., transitions from state (k, i) to state (k, i + 1).
The diagonal elements are set equal to the negated sum of the off-diagonal elements of Q. All other
elements in A1 are equal to zero. Thus,
A0 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
0
· · ·
0
0
0
0
0
· · ·
0
0
0
0
0
· · ·
0
0
0
0
0
· · ·
0
...
...
...
...
...
...
rμ
0
0
0
· · ·
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A2 = λI,
and
A1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−λ −rμ
rμ
0
0
· · ·
0
0
−λ −rμ
rμ
0
· · ·
0
0
0
−λ −rμ
rμ
· · ·
0
...
...
...
...
...
...
0
0
0
0
...
rμ
0
0
0
0
· · ·
−λ −rμ
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
The matrix B01 is a 1 × r row matrix all of whose elements are zero except for the ﬁrst which is
equal to λ. This is the rate of transition from state (0, 0) to state (1, 1) and corresponds to an arrival
to an empty system. The matrix B10 is a r × 1 column matrix all of whose elements are zero except
for the last which is equal to rμ. This is the rate of transition from state (1,r) to state (0, 0) and
corresponds to the complete service termination of the only customer in the system. The matrix B00
is a 1 × 1 matrix whose nonzero element is −λ and reﬂects the fact that the sum across the ﬁrst row
must be zero.
Example 12.1 Throughout this section we shall illustrate the matrix-geometric approach by means
of an M/Er/1 queue with parameters λ = 1, μ = 1.5, and r = 3. In this case, the inﬁnitesimal
generator Q and its submatrices are as follows:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−1
1
0
0
0
0
0
0
0
0
· · ·
0
−5.5
4.5
0
1
0
0
0
0
0
· · ·
0
0
−5.5
4.5
0
1
0
0
0
0
· · ·
4.5
0
0
−5.5
0
0
1
0
0
0
· · ·
0
0
0
0
−5.5
4.5
0
1
0
0
· · ·
0
0
0
0
0
−5.5
4.5
0
1
0
· · ·
0
4.5
0
0
0
0
−5.5
0
0
1
· · ·
0
0
0
0
0
0
0
−5.5
4.5
0
· · ·
0
0
0
0
0
0
0
0
−5.5
4.5 · · ·
0
0
0
0
4.5
0
0
0
0
−5.5 · · ·
...
...
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,

12.1 The Erlang-r Service Model—The M/Er/1 Queue
447
A0 =
⎛
⎝
0
0
0
0
0
0
4.5
0
0
⎞
⎠,
A1 =
⎛
⎝
−5.5
4.5
0
0
−5.5
4.5
0
0
−5.5
⎞
⎠,
A2 =
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠.
B00 = −1,
B01 = (1, 0, 0),
B10 =
⎛
⎝
0
0
4.5
⎞
⎠.
Our quest is to compute the stationary probability vector π from the system of homogeneous
linear equations π Q = 0 using the matrix geometric method. This method is described in
Chapter 10 in the context of solving Markov chains with structured transition rate matrices. The
M/Er/1 queue, and indeed all the queues of this chapter fall into this category. The reader may
wish to review the matrix geometric solution procedure before proceeding. We begin by writing the
stationary probability vector π as
π = (π0, π1, π2, . . . , πk, . . .),
where π0 is vector of length 1 (whose value is equal to the probability of an empty system) and
πk, k = 1, 2, . . . is a row vector of length r. Its ith component gives the probability of being in phase
i when there are k customers in the system. It is shown in Chapter 10 that successive subvectors of
π satisfy the relationship πi+1 = πi R for i = 1, 2, . . . , where R is the so-called Neuts’ rate matrix.
Thus the ﬁrst step in applying the matrix-geometric approach is the computation of this matrix R.
One possibility for a block tridiagonal matrix Q is to use the iterative scheme, called successive
substitution:
Rl+1 = −(V + R2
l W),
l = 0, 1, 2, . . . ,
where V = A2A−1
1
and W = A0A−1
1
and using R0 = 0 to initiate the procedure. As is shown by
Neuts, the sequence Rl is monotone increasing and converges to R. Due to the nonzero structure of
A1, nonzero elements along the diagonal and superdiagonal and zero elements everywhere else, its
inverse is explicitly available. The i j element of the inverse of any matrix of the form
M =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
d
a
0
0
· · ·
0
0
d
a
0
· · ·
0
0
0
d
a
· · ·
0
...
...
...
...
...
...
0
0
0
0
...
a
0
0
0
0
· · ·
d
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
is given as
#
M−1$
i j = (−1) j−i 1
d
a
d
 j−i
if i ≤j ≤r,
(12.1)
and is equal to zero otherwise. For the M/Er/1 queue, we have d = −(λ + rμ) and a = rμ. This
facilitates the computation of the matrices V and W used in the iterative scheme. Since W = A0A−1
1
and A0 has a single nonzero element equal to rμ in position (r, 1) it follows that W has only one
nonzero row, the last with elements given by rμ times the ﬁrst row of A−1
1 . Therefore
Wri = −

rμ
λ + rμ
i
for 1 ≤i ≤r
and Wki = 0 for 1 ≤k < r and 1 ≤i ≤r. Also, since A2 = λI , the computation of V = A2A−1
1
is also easy to ﬁnd. It sufﬁces to multiply each element of A−1
1
by λ. With the matrices V and W in
hand, the iterative procedure can be initiated and Neuts’ R matrix computed.

448
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
Example 12.2 Continuing our example of the M/E3/1 queue with parameters λ = 1 and μ = 1.5,
we compute A−1
1
using Equation (12.1) and obtain
A−1
1
=
⎛
⎝
−2/11
−18/121
−162/1331
0
−2/11
−18/121
0
0
−2/11
⎞
⎠
and hence
V =
⎛
⎝
−2/11
−18/121
−162/1331
0
−2/11
−18/121
0
0
−2/11
⎞
⎠
and W =
⎛
⎝
0
0
0
0
0
0
−9/11
−81/121
−729/1331
⎞
⎠.
We may now begin the iterative scheme
Rl+1 = −V −R2
l W
with R0 = 0. We obtain successively,
R1 =
⎛
⎝
2/11
18/121
162/1331
0
2/11
18/121
0
0
2/11
⎞
⎠,
R2 =
⎛
⎝
0.236136
0.193202
0.158075
0.044259
0.218030
0.178388
0.027047
0.022130
0.199924
⎞
⎠,
etc.
Continuing in this fashion, we obtain
R50 =
⎛
⎝
0.331961
0.271605
0.222222
0.109739
0.271605
0.222222
0.060357
0.049383
0.222222
⎞
⎠= R.
The second step in using the matrix-geometric approach is the computation of initial vectors so
that successive subvectors πi+1 can be computed from πi+1 = πi R. This requires us to ﬁnd π0 and
π1. From π Q = 0, i.e., from
(π0, π1, π2, . . . , πi, . . .)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
B01
0
0
0
· · ·
B10
A1
A2
0
0
· · ·
0
A0
A1
A2
0
· · ·
0
0
A0
A1
A2
· · ·
0
0
0
A0
A1
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, 0, . . . , 0, . . .),
we have
π0B00 + π1B10 = 0,
π0B01 + π1A1 + π2A0 = 0.
Writing π2 as π1R, we obtain
(π0, π1)
B00
B01
B10
A1 + RA0

= (0, 0).
(12.2)
Observe in this system of equation that π0 is a scalar quantity and π1 is a row vector of length r.
This system of equations can be solved using standard techniques from numerical linear algebra.
Since it is unlikely that r will be large, Gaussian elimination can be recommended. One ﬁnal point
must be taken into account: there is no unique solution to the system of Equation (12.2) and so the
computed π must be normalized so that the sum of its components is equal to one. Letting e denote

12.1 The Erlang-r Service Model—The M/Er/1 Queue
449
a column vector of length r whose components are all equal to 1, then
1 = π0 +
∞

k=1
πke = π0 +
∞

k=0
π1Rke = π0 + π1(I −R)−1e
provides the means by which π0 and π1 may be uniquely determined. Once π1 has been computed,
successive subvectors of the stationary probability distribution can be computed from
πk+1 = πk R.
Example 12.3 Returning to the M/E3/1 example, we now seek to compute π0 and π1. Observe that
4.5 × 0.222222 = 1 and so
RA0 =
⎛
⎝
1
0
0
1
0
0
1
0
0
⎞
⎠
and
A1 + RA0 =
⎛
⎝
−4.5
4.5
0
1
−5.5
4.5
1
0
−5.5
⎞
⎠.
This allows us to compute π0 and π1 from
(π0, π1)
⎛
⎜
⎜
⎝
−1
1
0
0
0
−4.5
4.5
0
0
1
−5.5
4.5
4.5
1
0
−5.5
⎞
⎟
⎟
⎠= (0, 0).
The coefﬁcient matrix has rank 3, so arbitrarily setting π0 = 1 we may convert this system to the
following nonhomogeneous system of equations with nonsingular coefﬁcient matrix:
(π0, π11, π12, π13)
⎛
⎜
⎜
⎝
−1
1
0
1
0
−4.5
4.5
0
0
1
−5.5
0
4.5
1
0
0
⎞
⎟
⎟
⎠= (0, 0, 0, 1).
Its solution is readily computed and we have
(π0, π11, π12, π13) = (1, 0.331962, 0.271605, 0.222222).
This solution needs to be normalized so that
π0 + π1(I −R)−1e = 1.
Substituting, we obtain
1 + (0.331962, 0.271605, 0.222222)
⎛
⎝
1.666666
0.666666
0.666666
0.296296
1.518518
0.518518
0.148148
0.148148
1.370370
⎞
⎠
⎛
⎝
1
1
1
⎞
⎠= 3.
Thus, the normalized solution is given as
(π0, π11, π12, π13) = (1/3, 0.331962/3, 0.271605/3, 0.222222/3)
= (1/3, 0.110654, 0.090535, 0.0740741).
Additional probabilities may now be computed from πk+1 = πk R. For example, we have
π2 = π1R = (0.051139, 0.058302, 0.061170),
π3 = π2R = (0.027067, 0.032745, 0.037913),
π4 = π3R = (0.014867, 0.018117, 0.021717),
π5 = π4R = (0.008234, 0.010031, 0.012156),

450
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
and so on. The probability of having 0, 1, 2, . . . customers is found by adding the components of
these subvectors. We have
p0 = 1/3, p1 = 0.275263, p2 = 0.170610, p3 = 0.097725, . . . .
We shall defer questions concerning stability and performance measures until towards the end of
this chapter, when we shall provide results that are applicable to all types of Ph/Ph/1 queues.
12.2 The Erlang-r Arrival Model—The Er/M/1 Queue
We now move on to the case of a single server queue for which the arrival process is Erlang-r and
the service time is exponential with rate μ as shown in Figure 12.3.
r
1
r
Server
2
r
r
μ
λ
λ
λ
Queue
Arrival  process
Figure 12.3. The Er/M/1 queue.
The density functions of the arrival and service processes, respectively, are given by
a(t) = rλ(rλt)r−1e−rλt
(r −1)!
,
t ≥0,
b(x) = μe−μx,
x ≥0.
Before actually appearing in the queue proper, an arriving customer must pass through r exponential
phases each with parameter rλ. When a customer completes the arrival process another immediately
begins. In other words, it is assumed that there is an inﬁnite pool of available customers waiting to
enter the arrival mechanism, and since only one can be “arriving” at any given instant of time, a new
customer cannot enter the left-most exponential phase until all arrival phases have been completed.
The instant that one customer completes the arrival process, a second customer simultaneously
begins the process.
The state descriptor for the Er/M/1 queue must specify the number of customers in the system,
k, and also, the phase in which the arriving customer is to be found, i. In this way, the state diagram
can be set up on a two-dimensional grid (k, i), arranged into levels according to the number of
customers present, in a manner similar to that undertaken for the M/Er/1 queue. The state transition
diagram for the Er/M/1 queue is shown in Figure 12.4.
The transition rate matrix has the typical block tridiagonal structure found in quasi-birth-death
processes:
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
B00
A2
0
0
0
0
· · ·
A0
A1
A2
0
0
0
· · ·
0
A0
A1
A2
0
0
· · ·
0
0
A0
A1
A2
0
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,

12.2 The Erlang-r Arrival Model—The Er/M/1 Queue
451
λ
(1, 2)
(1, r)
(2, r)
(3, 1)
(3, 2)
(3, 3)
(3, r)
(k, 2)
(k, r)
(2, 1)
(k, 1)
(k, 3)
(1, 1)
(2, 2)
(1, 3)
(2, 3)
(0, 2)
(0, r)
(0, 3)
(0, 1)
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
μ
μ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
r
r
r
r
λ
λ
rλ
rλ
rλ
λ
r
λ
r
r
Figure 12.4. State transition diagram for the Er/M/1 queue.
where all the subblocks are square and of order r. Speciﬁcially, we have
A0 = μI,
A1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−μ −rλ
rλ
0
0
· · ·
0
0
−μ −rλ
rλ
0
· · ·
0
0
0
−μ −rλ
rλ
· · ·
0
...
...
...
...
...
0
0
0
0
...
rλ
0
0
0
0
· · ·
−μ −rλ
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
and
A2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
0
· · ·
0
0
0
0
0
· · ·
0
0
0
0
0
· · ·
0
0
0
0
0
· · ·
0
...
...
...
...
...
...
rλ
0
0
0
· · ·
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
The matrices A0 represent service completions at rate μ from a state at some level k > 0 and
some number i −1 of completed arrival phases to the state with the same number of completed
arrival phases, but with one fewer customer, i.e., transitions from state (k, i) to state (k −1, i). It
follows that the only nonzero elements in this matrix are the diagonal elements which are all equal to
μ. The matrices A2 represent an actual arrival to the system. If the arrival process is in its last phase,
then at rate rλ, this ﬁnal phase terminates and the number of customers actually present increases
by 1. At this same instant, the arrival process begins again from phase 1. Thus the only nonzero
element in A2 is the r1 element A2(r, 1) which has the value rλ and represents transitions from state
(k,r) to state (k + 1, 1). The superdiagonal elements of the matrices A1 represent the completion of
one arrival phase i < r, at rate rλ, and the initiation of the next one, i +1, i.e., transitions from state
(k, i) to state (k, i + 1). The diagonal elements are set equal to the negated sum of the off-diagonal
elements of Q. All other elements in A1 are equal to zero. The matrix B00 differs from A1 only in

452
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
that its diagonal elements are all equal to −rλ, whereas those of A1 are each equal to −μ −rλ.
Like the diagonal elements of A1, the diagonal elements of B00 ensure that the sum of the elements
across each row of Q is equal to zero.
In comparing these matrices with the corresponding ones for the M/Er/1 queue, the reader will
notice that the structure of A0 in the Er/M/1 queue is identical to that of A2 in the M/Er/1 queue
and that the structure of A2 in the Er/M/1 queue is identical to that of A0 in the M/Er/1 queue.
Furthermore, as far as the actual nonzero values in these matrices are concerned, the λ’s in one are
replaced by μ’s in the other and vice versa.
Example 12.4 We consider an Er/M/1 queue with parameters λ = 1.0, μ = 1.5 and r = 3. In this
case, the inﬁnitesimal generator and its submatrices are
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−3
3
0
0
0
0
0
0
0
· · ·
0
−3
3
0
0
0
0
0
0
· · ·
0
0
−3
3
0
0
0
0
0
· · ·
1.5
0
0
−4.5
3
0
0
0
0
· · ·
0
1.5
0
0
−4.5
3
0
0
0
· · ·
0
0
1.5
0
0
−4.5
3
0
0
· · ·
0
0
0
1.5
0
0
−4.5
3
0
· · ·
0
0
0
0
1.5
0
0
−4.5
3
· · ·
0
0
0
0
0
1.5
0
0
−4.5 · · ·
...
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A0 =
⎛
⎝
1.5
0
0
0
1.5
0
0
0
1.5
⎞
⎠,
A1 =
⎛
⎝
−4.5
3
0
0
−4.5
3
0
0
−4.5
⎞
⎠,
A2 =
⎛
⎝
0
0
0
0
0
0
3
0
0
⎞
⎠,
B00 =
⎛
⎝
−3
3
0
0
−3
3
0
0
−3
⎞
⎠.
Once again, our quest is the computation of the stationary probability vector π, with π Q = 0,
by means of the matrix geometric approach. This stationary probability vector π may be written as
π = (π0, π1, π2, . . . , πk, . . .)
where each πk, k = 0, 1, 2, . . . is a row vector of length r whose ith component gives the probability
of the arrival process having completed exactly i −1 phases when there are k customers present in
the system and waiting for or receiving service. As before, the successive subvectors of π satisfy
the relationship πi+1 = πi R for i = 1, 2, . . . , where R is obtained from the iterative scheme
Rl+1 = −(V + R2
l W),
l = 0, 1, 2, . . . ,
with V = A2A−1
1
and W = A0A−1
1
and using R0 = 0 to initiate the procedure. The inverse of A1, is
an upper-trangular matrix which may be computed in exactly the same manner used in the M/Er/1
queue. The nonzero i j elements of A−1
1
for the Er/M/1 queue are given by
(A−1
1 )i j = (−1) j−i 1
d
a
d
 j−i
for i ≤j ≤r,
where d = −(μ + rλ) and a = rλ. Since V = A2A−1
1
and A2 has a single nonzero element equal
to rλ in position (r, 1) it follows that V has only one nonzero row, the last, with elements given by

12.2 The Erlang-r Arrival Model—The Er/M/1 Queue
453
rλ times the ﬁrst row of A−1
1 . Therefore
Vri = −

rλ
μ + rλ
i
for 1 ≤i ≤r
and Vki = 0 for 1 ≤k < r and 1 ≤i ≤r. Also, since A0 = μI , the computation of W = A0A−1
1
is
easy to ﬁnd. It sufﬁces to multiply each element of A−1
1
by μ. With the matrices V and W in hand,
the iterative procedure can be initiated and R computed via the iterative procedure.
Example 12.5 We shall continue with the E3/M/1 example and compute the matrix R. First we
need to compute A−1
1 . Using (12.1), we ﬁnd
A−1
1
=
⎛
⎝
−2/9
−4/27
−8/81
0
−2/9
−4/27
0
0
−2/9
⎞
⎠
and hence
W = A0A−1
1
=
⎛
⎝
−1/3
−2/9
−4/27
0
−1/3
−2/9
0
0
−1/3
⎞
⎠
and V = A2A−1
1
=
⎛
⎝
0
0
0
0
0
0
−2/3
−4/9
−8/27
⎞
⎠.
Beginning with R0 = 0, and iterating according to Rl+1 = −V −R2
l W, we ﬁnd
R1 =
⎛
⎝
0
0
0
0
0
0
2/3
4/9
8/27
⎞
⎠,
R2 =
⎛
⎝
0
0
0
0
0
0
0.732510
0.532236
0.3840878
⎞
⎠, . . . ,
which eventually converges to
R50 =
⎛
⎝
0
0
0
0
0
0
0.810536
0.656968
0.532496
⎞
⎠= R.
Observe that the matrix R has only one nonzero row. This is due to the fact that A2 has only one
nonzero row (which propagates into V ) and since R is initialized to be the zero matrix, it never has
the possibility of introducing nonzero elements into rows corresponding to zero rows of A2.
The boundary equations are different in the Er/M/1 queue from those in the M/Er/1 queue. There
is only a single B block, namely B00, whereas in the previous model there were three. Furthermore,
in the Er/M/1 queue, the B00 block has the same dimensions as all other blocks, r × r. Because of
the structure of these boundary equations, in particular the form of B00, only a single subvector π0
needs to be found before all other subvectors can be constructed. We have
πi+1 = πi R = π0Ri+1
for i = 0, 1, 2, . . .
From π Q = 0, i.e., from
(π0, π1, π2, . . . , πi, . . .)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
B00
A2
0
0
0
· · ·
A0
A1
A2
0
0
· · ·
0
A0
A1
A2
0
· · ·
0
0
A0
A1
A2
· · ·
0
0
0
A0
A1
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, 0, . . . , 0, . . .),

454
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
we have
π0B00 + π1A0 = π0B00 + π0RA0 = π0(B00 + RA0) = 0.
A solution to this system of equations may be obtained using Gaussian elimination; a unique
solution is found by enforcing the constraint that the sum of all components in π be equal to one.
This constraint implies that
1 =
∞

k=0
πke =
∞

k=0
π0Rke = π0(I −R)−1e
and provides the means by which π0 may be uniquely determined.
Example 12.6 Finishing off the example, we are now in a position to compute the initial subvector
π0. To do so, we need to solve the system of equations π0(B00 + RA0) = 0. Since this is a
homogeneous system of equations without a unique solution, we substitute the last equation with
π01 = 1 to obtain a speciﬁc solution. We have
π0(B00 + RA0) = (π01, π02, π03)
⎛
⎝
−3
3
1
0
−3
0
1.215803
0.98545
0
⎞
⎠= (0, 0, 1).
The solution is π0 = (1, 1.810536, 2.467504) which must now be normalized in such a way that
π0(I −R)−1e = 1. Since
(1, 1.810536, 2.467504)
⎛
⎝
1
0
0
0
1
0
−0.810536
−0.656968
0.467504
⎞
⎠
−1 ⎛
⎝
1
1
1
⎞
⎠= 15.834116,
we divide each component of π0 by 15.834116 to get the correctly normalized answer:
π0 = (0.063155, 0.114344, 0.155835).
The remaining subvectors of π may now be found from πk = πk−1R = π0Rk. We have
π1 = π0R = (0.126310, 0.102378, 0.082981),
π2 = π1R = (0.067259, 0.054516, 0.044187),
π3 = π2R = (0.035815, 0.029030, 0.023530),
π4 = π3R = (0.019072, 0.015458, 0.012529),
etc.
The probability of having 0, 1, 2, . . . customers is found by adding the components of these
subvectors. We have
p0 = 1/3, p1 = 0.311669, p2 = 0.165963, p3 = 0.088374, . . . .
12.3 The M/H2/1 and H2/M/1 Queues
Single-server queues with either hyperexponential service or arrival distributions can be handled
by the matrix geometric approach just as easily as was the case for Erlang distributions. In this

12.3 The M/H2/1 and H2/M/1 Queues
455
Queue
Server
μ
μ
1
2
α  
1−α  
λ
2
1
1
2
1
2
1
2
1
1
2
1
1
1
1
1
2
2
2
2
2
2
αμ
αμ
(1−α)μ
μ
αμ
(1−α)μ
(1−α)μ
(1−α)μ
(1−α)μ
αμ
λ
αμ
αμ
αμ
αμ
αμ
(1−α)μ
(1−α)μ
(1−α)μ
(1−α)μ
(1−α)μ
λ
λ
λ
λ
μ
λ
αμ
αλ
(1−α)λ
λ
λ
λ
λ
(k, 2)
(3, 2)
(2, 2)
(1, 2)
(1, 1)
(k, 1)
(2, 1)
(3, 1)
(0, 0)
Figure 12.5. The M/H2/1 queue (above) and its transition rate diagram.
section we shall consider only H2 hyperexponential distributions, but the extension to higher orders
is straightforward. Consider ﬁrst the M/H2/1 queue, as illustrated in Figure 12.5. Arrivals are
generated according to a Poisson distribution at rate λ, while the service is represented by a two-
phase hyperexponential distribution. With probability α a customer entering service receives service
at rate μ1, while with probability 1 −α this customer receives service at rate μ2. As for the M/Er/1
queue, a state of this queueing system is given by the pair (n, i) where n is the total number present
and i is the current service phase.
The transition rate diagram for the M/H2/1 queue is also shown in Figure 12.5. Increasing the
number of phases from 2 to r has the effect of incorporating additional columns into this ﬁgure.
With r possible service choices, a departure from any state (n, i), n > 1 can take the system to

456
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
any of the states (n −1, i) at rate αiμi where αi is the probability that an arriving customer
enters service phase i, for i = 1, 2, . . . ,r. The transition rate matrix for the M/H2/1 queue is
given by
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−λ
αλ
(1 −α)λ
0
0
0
0
· · ·
μ1
−(λ + μ1)
0
λ
0
0
0
· · ·
μ2
0
−(λ + μ2)
0
λ
0
0
· · ·
0
αμ1
(1 −α)μ1 −(λ + μ1)
0
λ
0
0
αμ2
(1 −α)μ2
0
−(λ + μ2)
0
λ
0
0
0
αμ1
(1 −α)μ1 −(λ + μ1)
0
...
0
0
0
αμ2
(1 −α)μ2
0
−(λ + μ2) ...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and clearly has the block tridiagonal structure from which the blocks A0, A1, A2, B00, B01, and B10
can be identiﬁed. We have
A0 =

αμ1
(1 −α)μ1
αμ2
(1 −α)μ2

, A1 =

−(λ + μ1)
0
0
−(λ + μ2)

, A2 =

λ
0
0
λ

,
B00 = (−λ), B01 = (αλ, (1 −α)λ), B10 =

μ1
μ2

.
The procedure is now identical to that carried out for the M/Er/1 queue in that Neuts’ R matrix
must be formed, the boundary equations set up and solved for π0 and π1 and from this all other
components of the stationary probability vector computed.
Rather than working our way through an example at this point, we shall move on to the H2/M/1
queue and postpone examples until after we formulate a Matlab program suitable for analyzing
general QBD queueing systems. The H2/M/1 queue is shown in Figure 12.6 and its transition rate
diagram in Figure 12.7.
2
1
α
1−α
μ
λ
λ
Service  facility
Arrival process
Figure 12.6. The H2/M/1 queue.
The instant a customer enters the queue proper, a new customer immediately initiates its arrival
process. The time between the moment at which this new customer initiates its arrival process and
the time at which it actually arrives in the queue is exponentially distributed. With probability α this
exponential distribution has rate λ1, while with probability 1 −α it has rate λ2. The transition rate

12.3 The M/H2/1 and H2/M/1 Queues
457
2
2
1
1
1
1
1
1
2
2
2
2
2
2
1
1
2
1
1
1
1
2
2
2
(1−α)λ
(1−α)λ
(1−α)λ
(1−α)λ
(1−α)λ
αλ
(1−α)λ
αλ
αλ
αλ
αλ
αλ
μ
αλ
αλ
μ
(1−α)λ
μ
(1−α)λ
μ
(1−α)λ
μ
(1−α)λ
(1−α)λ
αλ
μ
(1−α)λ
αλ
μ
αλ
αλ
μ
μ
μ
μ
μ
(0, 1)
(0, 2)
(k, 2)
(3, 2)
(2, 2)
(1, 2)
(1, 1)
(k, 1)
(2, 1)
(3, 1)
Figure 12.7. Transition diagram for the H2/M/1 queue.
matrix for the H2/M/1 queue is given by
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−λ1
0
αλ1
(1 −α)λ1
0
0
0
0
· · ·
0
−λ2
αλ2
(1 −α)λ2
0
0
0
0
· · ·
μ
0
−(λ1 + μ)
0
αλ1
(1 −α)λ1
0
0
· · ·
0
μ
0
−(λ2 + μ)
αλ2
(1 −α)λ2
0
0
· · ·
0
0
μ
0
−(λ1 + μ)
0
αλ1
(1 −α)λ1
0
0
0
μ
0
−(λ2 + μ)
αλ2
(1 −α)λ2
0
0
0
0
μ
0
−(λ1 + μ)
0
0
0
0
0
0
μ
0
−(λ2 + μ)
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and the block submatrices are
A0 =
μ
0
0
μ

, A1 =
−(λ1 + μ)
0
0
−(λ2 + μ)

, A2 =
αλ1
(1 −α)λ1
αλ2
(1 −α)λ2

,
B00 =

−λ1
0
0
−λ2

, B01 =

αλ1
(1 −α)λ1
αλ2
(1 −α)λ2

= A2, B10 =

μ
0
0
μ

= A0.
The matrix-geometric approach which may now be initiated, is identical to that of the Er/M/1
queue. Neuts’ R matrix is computed, the boundary equations solved and successive components of
π formed from the initial subvector π0. Because of the repetitive nature of this analysis, it behooves
us to write a simple Matlab program to solve systems of this type.

458
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
12.4 Automating the Analysis of Single-Server Phase-Type Queues
The procedure for solving phase-type queueing systems by means of the matrix-geometric approach
has four steps, namely,
1. Construct the block submatrices.
2. Form Neuts’ R matrix.
3. Solve the boundary equations.
4. Generate successive components of the solution.
We shall write Matlab code for each of these four steps separately; a complete Matlab program is
obtained by concatenating these four sections of code. In moving from one phase-type queueing
system to another only the ﬁrst of these sections should change. In other words we shall write the
Matlab program so that steps 2, 3, and 4 will work for all single-server phase-type queues.
To avoid any confusion with the interpretation of the code, it is important to maintain a consistent
notation throughout. Thus, in the code, we shall use the keyword lambda to represent the reciprocal
of the expectation of an exponential interarrival time distribution and the keyword mu to represent
the reciprocal of the expectation of an exponential service time distribution. When a distribution
is represented by more than one exponential phase, we shall append indices (1, 2, . . .) to these
keywords to represent the parameter of the exponential distribution at these phases.
Step 1: The Block Submatrices
In the examples treated so far, the submatrices A0, A1, and A2 have all been of size r × r, while
the size of the boundary blocks, B00, B01, and B10, were either of size r × r, for the Er/M/1 and
H2/M/1 or else of size 1 × 1, 1 × r, and r × 1 respectively for the M/Er/1 and M/H2/1 queues. We
shall introduce a parameter, l, to denote the size of the square block B00. For example, the following
Matlab codeforms the required block submatrices for the M/H2/1 queue.
%%%%%%
Construct submatrices for M/H_2/1 Queue
%%%%%
l = 1;
r = 2;
lambda = 1;
alpha = 0.25;
mu1 = 1.5;
mu2 = 2.0;
A0 = zeros(r,r);
A0(1,1) = alpha * mu1; A0(1,2) = (1-alpha) * mu1;
A0(2,1) = alpha * mu2; A0(2,2) = (1-alpha) * mu2;
A1 = zeros(r,r);
A1(1,1) = -lambda -mu1; A1(2,2) = -lambda -mu2;
A2 = lambda * eye(r);
B00 = -lambda * eye(l);
B01 = [alpha*lambda, (1-alpha)*lambda];
B10 = [mu1;mu2];
Step 2: Neuts’ R Matrix
The boundary blocks (i.e., the B blocks) do not enter into the computation of Neut’s R matrix. We
need to form V = A2A−1
1
and W = A0A−1
1
and then iterative successively with Rk+1 = −V −R2
k W
with R0 = 0 until Rk converges to the matrix R. In the Matlab code given below we iterative until
∥Rk+1 −Rk∥1 ≤10−10.

12.4 Automating the Analysis of Single-Server Phase-Type Queues
459
%%%%%%%%%%%%%%
Form Neuts’ R matrix
%%%%%%%%%%%%%%%
V = A2 * inv(A1);
W = A0 * inv(A1);
R = zeros(r,r);
Rbis = -V - R*R * W;
iter = 1;
while (norm(R-Rbis,1) > 1.0e-10)
R = Rbis;
Rbis = -V - R*R * W;
iter = iter+1;
end
R = Rbis;
Step 3: Solve the Boundary Equations
In our analysis of the Er/M/1 queue, we took the boundary equations to be
π0(B00 + RA0) = 0,
solved for π0, and then used πk = π0Rk, k = 1, 2, . . . , to get the other block components of the
stationary probability vector. For the M/Er/1 queue we used
(π0, π1)
B00
B01
B10
A1 + RA0

= 0,
from which we computed both π0 and π1. The other block components of the solution were obtained
from πk = π1Rk−1, k = 2, 3, . . . . Since this latter approach will work in all cases, this is the version
that we shall implement. To obtain a unique solution to this system of equations we replace one of
the equations by an equation in which the ﬁrst component of the subvector π0 is set equal to 1.
A ﬁnal normalization so that
1 =
∞

k=0
πke = π0e +
∞

k=1
πke = π0e +
∞

k=0
π1Rke = π0e + π1(I −R)−1e
allows us to correctly compute the subvectors π0 and π1. In the above equation, e is a column
vector of 1’s whose size is determined by its context. The following Matlab code implements these
concepts.
%%%%%%%%%%%%%
Solve boundary equations
%%%%%%%%%%%%
N = [B00,B01;B10,A1+R*A0];
% Boundary equations
N(1,r+l) = 1;
% First component equals 1
for k=2:r+l
N(k,r+l) = 0;
end
rhs = zeros(1,r+l); rhs(r+l)= 1;
soln = rhs * inv(N);
% Un-normalized pi_0 and pi_1
pi0 = zeros(1,l);
pi1 = zeros(1,r);
for k=1:l
% Extract pi_0 from soln
pi0(k) = soln(k);
end

460
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
for k=1:r
% Extract pi_1 from soln
pi1(k) = soln(k+l);
end
e = ones(r,1);
% Normalize solution
sum = norm(pi0,1) +
pi1 * inv(eye(r)-R) * e;
pi0 = pi0/sum;
pi1 = pi1/sum;
Step 4: Generate Successive Components of the Solution
The last step is to construct the different subblocks of the stationary probability vector. In addition
to this, the Matlab code below also computes and prints the probability distribution of customers in
the system (up to some given maximum number).
%%%%
Generate successive components of solution
%%%%
max = 10;
%
Maximum population requested
pop = zeros(max+1,1);
pop(1) =
norm(pi0,1);
for k=1:max
pi = pi1 * R^(k-1);
% Successive components of pi
pop(k+1) = norm(pi,1);
end
pop
%
Print population distribution
As mentioned previously, this code allows for all types of single-server phase-type queueing
systems: it sufﬁces to provide step 1 with the correct submatrices of the transition rate matrix. As a
further example, the code for the H2/M/1 queue is provided below. Other examples are provided in
the subsections that follow.
%%%%%%%%%%
Construct submatrices for H_2/M/1 queue
%%%%%%%%%
l = 2; r = 2;
lambda1 = 1;
lambda2 = 1.5;
mu = 2.0;
alpha = 0.25;
A0 = mu * eye(r);
A1 = zeros(r,r);
A1(1,1) = -lambda1 - mu; A1(2,2) = -lambda2 - mu;
A2 = [alpha*lambda1, (1-alpha)*lambda1;alpha*lambda2,
(1-alpha)*lambda2];
B00 = [-lambda1,0;0,-lambda2];
B01 = A2;
B10 = A0;
12.5 The H2/E3/1 Queue and General Ph/Ph/1 Queues
We analyze the H2/E3/1 queue in order to provide some insight into the analysis of more complex
Ph/Ph/1 queues. The arrival process of the H2/E3/1 queue is a two-phase hyperexponential
distribution while the service process is an Erlang-3 distribution both of which have been considered
in previous sections. This queueing system is shown graphically in Figure 12.8 where, for a proper

12.5 The H2/E3/1 Queue and General Ph/Ph/1 Queues
461
Erlang-3 distribution, we require μ1 = μ2 = μ3 = 3μ. When these parameters do not all have the
same value, then the distribution is more properly called a hypoexponential distribution.
3
λ
λ
2
1
Arrival process
α
μ
α
Service  facility
μ
2
μ
1
1
2
Figure 12.8. The H2/E3/1 queue.
To describe a state of this system we need three parameters, the ﬁrst, k, to denote the number of
customers actually present, the second, a, to describe the arrival phase of the “arriving” customer,
and the last, s, to indicate the current phase of service. In drawing the state transition rate diagram,
we shall present the states in rows (or levels) according to the number of customers present.
Within each level k, states are ordered ﬁrst according to the arrival phase of the hyperexponential
distribution and second according to the phase of service. In other words, the states (k, a, s) are
ordered lexicographically. Since the state transition rate diagram can become rather complicated,
we show the states and the transitions generated by the arrival process in one diagram, and in a
separate diagram, we show the states and the transitions generated by the service process. Obviously
the complete diagram is obtained by superimposing both sets of transitions onto the same state
space. The ﬁrst three levels of states and transitions generated by the service process are shown in
Figure 12.9.
Transitions at rate μ1 move the system from states (k, a, 1) to state (k, a, 2); those at rate μ2
move the system from states (k, a, 2) to state (k, a, 3) while those at at rate μ3 move the system
from states (k, a, 3) to state (k −1, a, 1), for k = 1, 2, . . . . It is only when the last service phase is
3
3
3
2
2
2
2
1
1
1
1
3
(2, 1, 2)
(2, 1, 3)
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
μ
(2, 2, 2)
(0, 2, 0)
(1, 2, 1)
(1, 2, 2)
(1, 2, 3)
(2, 2, 1)
(2, 2, 3)
(0, 1, 0)
(1, 1, 1)
(1, 1, 2)
(1, 1, 3)
(2, 1, 1)
Figure 12.9. Transition diagram for the H2/E3/1 queue, part (a).

462
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
2λ1
α 2λ 1
α 2λ1
α 2λ 2
α2
α
2
2
2
1
1
1
α 1λ 2
α1λ 2
α1λ 2
1
α
α λ
1
2
λ2
α2
α2λ 2
α1λ 2
1
α1λ 2
α 2λ 2
α 2λ 1
2
λ
1
α
1
λ
2
α
α
α
α
α
λ
λ
1
λ
λ
λ
1
1
1 1
α
λ
(1, 2, 2)
(1, 2, 1)
(0, 2, 0)
(1, 2, 3)
(2, 2, 1)
(2, 2, 2)
(2, 2, 3)
(0, 1, 0)
λ
(1, 1, 1)
(1, 1, 2)
(1, 1, 3)
(2, 1, 1)
(2, 1, 2)
(2, 1, 3)
Figure 12.10. Transition diagram for the H2/E3/1 queue, part (b).
completed that a departure actually occurs. Transitions among the same set of states generated by
the arrival process are illustrated in Figure 12.10.
From states (k, 1, s) transitions occur at rate λ1. With probability α1 the next customer to begin
its arrival process enters phase 1, so that with rate α1λ1, the system moves from state (k, 1, s) to
state (k + 1, 1, s); with probability α2 the customer beginning its arrival phase chooses phase 2 and
so at rate α2λ1 the system moves from state (k, 1, s) to state (k + 1, 2, s). A similar set of transitions
is obtained from states in which the customer arrives from phase 2. We obtain transitions at rate
α1λ2 from states (k, 2, s) to states (k + 1, 1, s) and transitions at rate α2λ2 from states (k, 2, s) to
(k + 1, 2, s). The ﬁrst few rows of the complete matrix are given by
−λ1
0
α1λ1
0
0
α2λ1
0
0
0
0
0
0
0
0
· · ·
0
−λ2 α1λ2
0
0
α2λ2
0
0
0
0
0
0
0
0
· · ·
0
0
∗
μ1
0
0
0
0
α1λ1
0
0
α2λ1
0
0
· · ·
0
0
0
∗
μ2
0
0
0
0
α1λ1
0
0
α2λ1
0
· · ·
μ3
0
0
0
∗
0
0
0
0
0
α1λ1
0
0
α2λ1 · · ·
0
0
0
0
0
∗
μ1
0
α1λ2
0
0
α2λ2
0
0
· · ·
0
0
0
0
0
0
∗
μ2
0
α1λ2
0
0
α2λ2
0
· · ·
0
μ3
0
0
0
0
0
∗
0
0
α1λ2
0
0
α2λ2 · · ·
0
0
0
0
0
0
0
0
∗
μ1
0
0
0
0
· · ·
0
0
0
0
0
0
0
0
0
∗
μ2
0
0
0
· · ·
0
0
μ3
0
0
0
0
0
0
0
∗
0
0
0
· · ·
0
0
0
0
0
0
0
0
0
0
0
∗
μ1
0
· · ·
0
0
0
0
0
0
0
0
0
0
0
0
∗
μ2
· · ·
0
0
0
0
0
μ3
0
0
0
0
0
0
0
∗
· · ·
0
0
0
0
0
0
0
0
0
0
0
0
0
0
· · ·
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...

12.5 The H2/E3/1 Queue and General Ph/Ph/1 Queues
463
where the elements marked by an asterisk in each diagonal block are given by
diag {−(λ1 + μ1), −(λ1 + μ2), −(λ1 + μ3), −(λ2 + μ1), −(λ2 + μ2), −(λ2 + μ3)}
so that the sum across each row is equal to zero.
With diagrams of all possible transitions among states, it is possible to construct the block
submatrices A0, A1, A2, B00, B01, and B10 and then apply the matrix-geometric approach. However,
it is evident that this can become quite messy so we look for a better way to form these blocks.
The phase-type distributions that we have considered so far, the Erlang, hyperexponential, and even
Coxian distributions, all have very speciﬁc structures—the phases are laid out either in series or in
parallel. However, as described in Section 7.6.6, it is possible to build much more general phase-
type distributions. An arbitrary Markov chain with a single absorbing state and an initial probability
distribution contains the essence of a phase-type distribution. The phase-type distribution is deﬁned
as the distribution of the time to absorption into the single absorbing state when the Markov chain
is started with the given initial probability distribution.
Example 12.7 For a three-stage hypoexponential distribution with parameters μ1, μ2, and μ3 this
Markov chain is completely deﬁned by the following transition rate matrix S′ (we shall use S for
service processes) and initial probability distribution σ ′:
S′ =
⎛
⎜
⎜
⎝
−μ1
μ1
0
0
0
−μ2
μ2
0
0
0
−μ3
μ3
0
0
0
0
⎞
⎟
⎟
⎠=

S
S0
0
0

,
σ ′ = (1
0
0 | 0) = (σ
0).
For a two stage hyperexponential distribution with branching probabilities α1 and α2 (= 1−α1) and
exponential phases with rates λ1 and λ2, the Markov chain is completely deﬁned by the transition
rate matrix T ′ (we use T for arrival processes) and initial probability distribution ξ ′ given as
T ′ =
⎛
⎝
−λ1
0
λ1
0
−λ2
λ2
0
0
0
⎞
⎠=
T
T 0
0
0

,
ξ ′ = (α1
α2 | 0) = (ξ
0).
We shall use the partitioned matrices, S, S0, σ and T , T 0 and ξ to construct formulae for
the speciﬁcation of the block submatrices needed in the matrix-geometric approach. Indeed, for a
Ph/Ph/1 queue with ra phases in the description of the arrival process and rs phases in the description
of the service process, the block submatrices required for the application of the matrix geometric
approach are given by
A0 = Ira ⊗(S0 · σ),
A1 = T ⊗Irs + Ira ⊗S,
and
A2 = (T 0 · ξ) ⊗Irs
B00 = T,
B01 = (T 0 · ξ) ⊗σ,
and
B10 = Ira ⊗S0
where In is the identity matrix of order n and the symbol ⊗denotes the Kronecker (or tensor)
product. The Kronecker product of an m × n matrix A with a matrix B is given by
A ⊗B =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
a11B
a12B
a13B
· · ·
a1n B
a21B
a22B
a23B
· · ·
a2n B
a31B
a32B
a33B
· · ·
a3n B
...
...
...
...
...
am1B
am2B
am3B
· · ·
amn B
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
For example, the Kronecker product of
A =
a
b
c
d
e
f

and
B =
α
β
γ
δ


464
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
is
A ⊗B =

aB
bB
cB
d B
eB
f B

=
⎛
⎜
⎜
⎝
aα
aβ bα
bβ
cα
cβ
aγ
aδ bγ
bδ
cγ
cδ
dα
dβ eα
eβ
f α
fβ
dγ
dδ eγ
eδ
f γ
f δ
⎞
⎟
⎟
⎠.
With these deﬁnitions in hand, we may compute the block submatrices for the H2/E3/1 queue (with
ra = 2 and rs = 3) as follows:
A0 = I2 ⊗(S0 · σ) = I2 ⊗
⎛
⎝
0
0
μ3
⎞
⎠(1
0
0) = I2 ⊗
⎛
⎝
0
0
0
0
0
0
μ3
0
0
⎞
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
0
0
0
0
0
0
0
0
0
μ3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
μ3
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A1 = T ⊗I3 + I2 ⊗S =

−λ1
0
0
−λ2

⊗I3 + I2 ⊗
⎛
⎝
−μ1
μ1
0
0
−μ2
μ2
0
0
−μ3
⎞
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−λ1
0
0
0
0
0
0
−λ1
0
0
0
0
0
0
−λ1
0
0
0
0
0
0
−λ2
0
0
0
0
0
0
−λ2
0
0
0
0
0
0
−λ2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
+
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−μ1
μ1
0
0
0
0
0
−μ2
μ2
0
0
0
0
0
−μ3
0
0
0
0
0
0
−μ1
μ1
0
0
0
0
0
−μ2
μ2
0
0
0
0
0
−μ3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A2 = (T 0 · ξ) ⊗I3 =

λ1
λ2
 #
α1 α2
$
⊗I3 =

α1λ1
α2λ1
α1λ2
α2λ2

⊗I3
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
α1λ1
0
0
α2λ1
0
0
0
α1λ1
0
0
α2λ1
0
0
0
α1λ1
0
0
α2λ1
α1λ2
0
0
α2λ2
0
0
0
α1λ2
0
0
α2λ2
0
0
0
α1λ2
0
0
α2λ2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
B00 =

−λ1
0
0
−λ2

,
B01 = (T 0 · ξ) ⊗σ =
 λ1
λ2

(α1
α2) ⊗(1
0
0) =
α1λ1
α2λ1
α1λ2
α2λ2

⊗(1
0
0)
=
α1λ1
0
0
α2λ1
0
0
α1λ2
0
0
α2λ2
0
0

,

12.5 The H2/E3/1 Queue and General Ph/Ph/1 Queues
465
B10 = I2 ⊗S0 = I2 ⊗
⎛
⎝
0
0
μ3
⎞
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
0
μ3
0
0
0
0
0
0
μ3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
It is now clear that these formulae give the same results as those obtained previously by constructing
the matrix directly from the transition rate diagram. This technique is easily incorporated into
Matlab code using the Matlab operator kron(A,B) to form the Kronecker product of A and B.
Furthermore, the size of the blocks may be obtained directly by means of the Matlab size operator:
size(A,2) gives the number of columns in a matrix A. As an example, the following Matlab code
constructs the block matrices for the H2/E3/1 queue. Observe that only the ﬁrst eight lines of code
(four for each of the arrival and service processes) are speciﬁc to this queue. The last eight are
appropriate for all Ph/Ph/1 queues.
%%%%%%%%%%
Construct submatrices for H_2/E_3/1 Queue
%%%%%%
%%%
Specific parameters for H_2/E_3/1 queue:
%%%
H_2 Arrival Process:
alpha1 = 0.4; alpha2 = 0.6; lambda1 = 1; lambda2 = 2;
T = [-lambda1, 0 ; 0, -lambda2];
T0 = [lambda1;lambda2];
xi = [alpha1, alpha2];
%%%
E_3 Service Process:
mu1 = 4; mu2 = 8; mu3 = 8;
S = [-mu1, mu1, 0; 0, -mu2, mu2; 0,0, -mu3];
S0 = [0;0;mu3];
sigma = [1,0,0];
%%% Block Submatrices for all types of queues:
ra = size(T,2);
rs = size(S,2);
A0 = kron(eye(ra), S0*sigma);
A1 = kron(T, eye(rs)) + kron(eye(ra), S);
A2 = kron(T0*xi, eye(rs));
B00 = T;
B01 = kron(T0*xi,sigma);
B10 = kron(eye(ra),S0);
l = size(B00,2);
r = size(A0,2);
The remaining sections of the Matlab code, those for generating Neuts’ R matrix, solving the
boundary equations and for constructing successive solution components, remain unchanged and
may be appended directly to the code just given to form a complete Matlab program that may be
used to solve any Ph/Ph/1 queue. Only one issue remains: that of determining the stability of the
queue.

466
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
12.6 Stability Results for Ph/Ph/1 Queues
The M/M/1 queue is stable—the number of customers does not increase indeﬁnitely—when the
mean arrival rate λ is strictly less than the mean service rate μ. If E[A] is the mean interarrival time
and E[S] is the mean service time, then this stability condition can be written as
1
E[A] <
1
E[S]
or
E[S] < E[A],
which means that the M/M/1 queue is stable when the average time it takes to serve a customer
is strictly less than the average time between customer arrivals. A similar condition holds for
all Ph/Ph/1 queues: the mean interarrival time must be greater than the mean service time. This
condition may be readily veriﬁed, especially when the phase-type distributions involved are standard
distributions such as the Erlang and hyperexponential distributions used in the previous examples.
Example 12.8 The expectation of a two-phase hyperexponential distribution used to model
an arrival process is given by E[A] = α1/λ1 + α2/λ2, while the expectation of a three-phase
hypoexponential distribution used to model a service process is E[S] = 1/μ1 + 1/μ2 + 1/μ3.
Using the values of previous examples, (α1 = 0.4, α2 = 0.6, λ1 = 1, λ2 = 2, μ1 = 4, μ2 = 8,
and μ3 = 8), we ﬁnd
E[S] = 1
4 + 1
8 + 1
8 = 0.5 <
0.4
1 + 0.6
2 = 0.7 = E[A].
Alternatively, if we let λ be the effective arrival rate into the H2/E3/1 and μ be the effective service
rate, then λ = 1/0.7 = 1.428571 and μ = 1/0.5 = 2 and the condition ρ = λ/μ < 1 is satisﬁed.
For a general phase-type distribution, with an inﬁnitesimal generator matrix given by
Z′ =

Z
Z0
0
0

,
and an initial probability distribution ζ, the expectation of the time to absorption (see Section 9.6.2)
is given by
E[A] = ∥−ζ Z−1∥1.
Example 12.9 Using the above formulation, the average interarrival time in the H2/E3/1 queue is
obtained as
E[A] =
@@@@@−(α1, α2)

−λ1
0
0
−λ2
−1@@@@@
1
=
@@@@@−(0.4, 0.6)

−1
0
0
−2
−1@@@@@
1
= ∥(0.4, 0.3)∥1 = 0.7.
Similarly, the average service time may also be computed.
The same stability condition may be derived from the matrices A0, A1 and A2 used in the matrix-
geometric method. The matrix A = A0 + A1 + A2 is an inﬁnitesimal generator in its own right.
In other words, its off-diagonal elements are nonnegative, its diagonal elements are nonpositive and
the sum across all rows is equal to zero. As such it has a stationary probability vector which we shall
call γ , i.e.,
γ A = γ (A0 + A1 + A2) = 0.
The nonzero elements of the block subdiagonal matrices A0 are responsible for moving the system
down a level, i.e., from some level (number of customers) l to the next lower level l −1. This relates
directly to service completions in a Ph/Ph/1 queue. On the other hand, the nonzero elements of the
superdiagonal blocks A2 move the system up from some level l to the next level l + 1: the number
of customers in the system increases by one. For stability we require the effect of the matrices A2
to be less than the effect of the matrices A0: the system must have a tendency to drift to the point

12.6 Stability Results for Ph/Ph/1 Queues
467
of zero customers rather than the opposite—similar to the condition that was found to be necessary
for the states of a random walk problem to be positive recurrent. The matrix A = A0 + A1 + A2
encapsulates these drift tendencies and the condition for stability can be written as
∥γ A2∥1 < ∥γ A0∥1.
Example 12.10 Substituting previously used values for the H2/E3/1 queue, we ﬁnd
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−4.6
4.0
0
0.6
0
0
0
−8.6
8.0
0
0.6
0
8.0
0
−8.6
0
0
0.6
0.8
0
0
−4.8
4.0
0
0
0.8
0
0
−8.8
8.0
0
0
0.8
8.0
0
−8.8
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and its stationary probability vector, obtained by solving γ A = 0 with ∥γ ∥1 = 1, is
γ = (0.285714, 0.142857, 0.142857, 0.214286, 0.107143, 0.107143).
Computing ∥γ A2∥1 and ∥γ A0∥1, we ﬁnd
λ = ∥γ A2∥1 =
@@@@@@@@@@@@
γ
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.4
0
0
0.6
0
0
0
0.4
0
0
0.6
0
0
0
0.4
0
0
0.6
0.8
0
0
1.2
0
0
0
0.8
0
0
1.2
0
0
0
0.8
0
0
1.2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
@@@@@@@@@@@@
1
=∥(0.285714, 0.142857, 0.142857, 0.428571, 0.214286, 0.214286)∥1 =1.428571
and
μ = ∥γ A0∥1 =
@@@@@@@@@@@@
γ
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
@@@@@@@@@@@@
1
= ∥(1.142857, 0, 0, 0.857143, 0, 0)∥1 = 2.0
which is the same as that obtained previously.
It is an interesting exercise to vary the parameters of the H2/E3/1 queue so that different
values are obtained for ρ and to observe the effect of this on the number of iterations needed for
convergence to Neuts’ R matrix. Some results obtained by varying only the parameter λ1, are shown
in Table 12.1.
The numbers under the column heading SS are the numbers of iterations needed to converge
when the successive substitution algorithm presented in the previous Matlab code segments is used.
This table clearly shows that the closer ρ is to 1, the slower the convergence. However, recall
from Chapter 11 that there are alternative, more sophisticated, approaches to computing Neuts’ R
matrix when the model is of the quasi-birth-death type, as in this case. In particular, the logarithmic
reduction algorithm may be used to great effect. The number of iterations needed to obtain R to the
same precision using this method is given in the table under the column heading LR. Matlab code
which implements the logarithmic-reduction algorithm is provided in the program at the end of this
section.

468
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
Table 12.1. Effect of varying λ1 on ρ and convergence to R.
λ1
ρ
SS
LR
0.1
0.1163
28
5
0.5
0.4545
50
6
1.0
0.7143
98
6
1.5
0.8824
237
8
1.6
0.9091
303
8
1.7
0.9341
412
8
1.8
0.9574
620
9
1.9
0.9794
1197
10
1.95
0.9898
2234
11
2.0
1.0
∞
∞
12.7 Performance Measures for Ph/Ph/1 Queues
So far the only performance measures we have obtained for Ph/Ph/1 queues are the stationary
probabilities of the underlying Markov chains. It is of course, possible to get much useful
information directly from these. For example, the probability that there are k customers present
in the queueing system, for k ≥1, is obtained by adding the components of the kth subvector, i.e.,
pk = ∥πk∥1 = ∥π1Rk−1∥1.
In particular, the probability that the system is empty is given by p0 = ∥π0∥1 while the probability
that the system is busy is 1 −p0. Of course, these results could also be obtained as p0 = 1 −ρ =
1 −λ/μ where λ and μ are computed from λ = 1/E[A] and μ = 1/E[S] and
E[A] = ∥−ξT −1∥1
and
E[S] = ∥−σ S−1∥1.
The probability that there are k or more customers present can also be obtained relatively easily. We
have
Prob{N ≥k} =
∞

j=k
∥π j∥1 =
@@@@@@
π1
∞

j=k
R j−1
@@@@@@
1
=
@@@@@@
π1Rk−1
∞

j=0
R j
@@@@@@
1
=
@@π1Rk−1(I −R)−1@@
1 .
As in this last case, Neuts’ R matrix can be used to compute the mean number of customers in
a Ph/Ph/1 queueing system and this, together with Little’s law and other relationships, allow us
to compute the expected number of customers waiting for service and the expected response and
waiting times. The average number of customers in a Ph/Ph/1 queueing system is obtained as
E[N] =
∞

k=1
k ∥πk∥1 =
∞

k=1
k ∥π1Rk−1∥1 =
@@@@@π1
∞

k=1
d
d R Rk
@@@@@
1
=
@@@@@π1
d
d R
 ∞

k=1
Rk
@@@@@
1
=
@@@@π1
d
d R
#
(I −R)−1 −I
$@@@@
1
=
@@π1(I −R)−2@@
1 .
The mean number of customers waiting in the queue, E[Nq]; the average response time, E[R]
and the average time spent waiting in the queue, E[Wq] may now be obtained from the standard
formulae. We have
E[Nq] = E[N] −λ/μ,
E[R] = E[N]/λ,
E[Wq] = E[Nq]/λ.

12.8 Matlab code for Ph/Ph/1 Queues
469
12.8 Matlab code for Ph/Ph/1 Queues
A combination of the codes and procedures developed in this chapter results in the following Matlab
program.
Function ph_ph_1()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
Example 1:
M/E_4/1 Queue
%%%
Exponential arrival:
%
lambda = 4;
%
T = [-lambda]; T0=[lambda]; xi = [1];
%%%
Erlang-4 Service (use mu_i = r*mu per phase)
%
mu1 = 20; mu2 = 20; mu3 = 20; mu4 = 20;
%
S = [-mu1, mu1, 0,0; 0, -mu2, mu2,0; 0,0 -mu3,mu3;0,0,0, -mu4];
%
S0 = [0;0;0;mu4];
%
sigma = [1,0,0,0];
%%%
Example 2: H_2/Ph/1 queue:
%%%
H_2 Arrival Process:
alpha1 = 0.4; alpha2 = 0.6; lambda1 = 1.9; lambda2 = 2;
T = [-lambda1, 0 ; 0, -lambda2];
T0 = [lambda1;lambda2];
xi = [alpha1, alpha2];
%%%
Hypo-exponential-3 Service Process:
mu1 = 4; mu2 = 8; mu3 = 8;
S = [-mu1, mu1, 0; 0, -mu2, mu2; 0,0, -mu3];
S0 = [0;0;mu3];
sigma = [1,0,0];
%%%%%%%%%
Block Submatrices for all types of queues:
%%%%%%
ra = size(T,2);
rs = size(S,2);
A0 = kron(eye(ra), S0*sigma);
A1 = kron(T, eye(rs)) + kron(eye(ra), S);
A2 = kron(T0*xi, eye(rs));
B00 = T;
B01 = kron(T0*xi,sigma);
B10 = kron(eye(ra),S0);
l = size(B00,2);
r = size(A0,2);
%%%%%%%%%
Check stability
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
meanLambda = 1/norm(-xi* inv(T),1);
meanMu = 1/norm(-sigma * inv(S),1);

470
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
rho = meanLambda/meanMu
%%%%%%%%%%%
Alternatively:
%%%%%%%%%
A = A0+A1+A2;
for k=1:r
A(k,r) = 1;
end
rhs = zeros(1,r); rhs(r)= 1;
ss = rhs*inv(A);
rho = norm(ss*A2,1)/norm(ss*A0,1);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
if
rho >=0.999999
error(’Unstable System’);
else
disp(’Stable system’)
end
%%%%%%%%%%
Form Neuts’ R matrix
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%
by
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%
Successive Substitution
%%%%%%%%%%%%%%%%%%%%%%%%%
V = A2 * inv(A1);
W = A0 * inv(A1);
R = -V;
Rbis = -V - R*R * W;
iter = 1;
while (norm(R-Rbis,1)> 1.0e-10 & iter<100000)
R = Rbis;
Rbis = -V - R*R * W;
iter = iter+1;
end
iter
R = Rbis;
%%%%%%%%%%%%
or
by
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
Logarithmic Reduction %%%%%%%%%%%%%%%%%%%%%%%%%
%
Bz = -inv(A1)*A2;
Bt = -inv(A1)*A0;
%
T = Bz;
S = Bt;
%
iter = 1;
%
while (norm(ones(r,1)-S*ones(r,1) ,1)> 1.0e-10 & iter<100000)
%
D = Bz*Bt + Bt*Bz;
%
Bz = inv(eye(r)-D) *Bz*Bz;
%
Bt = inv(eye(r)-D) *Bt*Bt;
%
S = S + T*Bt;
%
T = T*Bz;
%
iter = iter+1;
%
end
%
iter
%
U = A1 + A2*S;
%
R = -A2 * inv(U)

12.9 Exercises
471
%%%%%%%%%%%%%%
Solve boundary equations
%%%%%%%%%%%%%%%%%%%%
N = [B00,B01;B10,A1+R*A0];
% Set up boundary equations
N(1,r+l) = 1;
% Set first component equal to 1
for k=2:r+l
N(k,r+l) = 0;
end
rhs = zeros(1,r+l); rhs(r+l)= 1;
soln = rhs * inv(N);
% Un-normalized pi_0 and pi_1
pi0 = zeros(1,l);
pi1 = zeros(1,r);
for k=1:l
pi0(k) = soln(k);
% Extract pi_0
end
for k=1:r
pi1(k) = soln(k+l);
% Extract pi_1
end
e = ones(r,1);
sum = norm(pi0,1) +
pi1 * inv(eye(r)-R) * e;
% Normalize solution
pi0 = pi0/sum; pi1 = pi1/sum;
%%%%%%%%%%%%%%%%%
Print results %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
max = 10;
%
maximum population requested
pop = zeros(max+1,1);
pop(1) =
norm(pi0,1);
for k=1:max
pi = pi1 * R^(k-1);
% Get successive components of pi
pop(k+1) = norm(pi,1);
end
pop
%%%%%%%%%%%%%
Measures of Effectiveness
%%%%%%%%%%%%%%%%%%%%
EN = norm(pi1*inv(eye(r)-R)^2,1)
%
ENq = EN-meanLambda/meanMu
%
ER =
EN/meanLambda
%
EWq = ENq/meanLambda
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
12.9 Exercises
Exercise 12.1.1 Use Equation (12.1) to form the inverse of the following matrices:
M1 =
⎛
⎜
⎜
⎜
⎜
⎝
1
1
0
0
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
,
M2 =
⎛
⎜
⎜
⎜
⎜
⎝
−5
4
0
0
0
0
−5
4
0
0
0
0
−5
4
0
0
0
0
−5
4
0
0
0
0
−5
⎞
⎟
⎟
⎟
⎟
⎠
.

472
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
Exercise 12.1.2 Prove that the i j element of the inverse of any matrix of the form
M =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
d
a
0
0
· · ·
0
0
d
a
0
· · ·
0
0
0
d
a
· · ·
0
...
...
...
...
...
...
0
0
0
0
...
a
0
0
0
0
· · ·
d
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
is
M−1
i j = (−1) j−i 1
d
a
d
 j−i
for i ≤j ≤r,
and is equal to zero otherwise.
Exercises 12.1.3–12.3.2 should be answered from ﬁrst principles, following the manner of Sec-
tions 12.1–12.3.
Exercise 12.1.3 Draw the state transition diagram of an M/E4/1 queue with mean interarrival time of 1/4 and
mean service time of 1/5. Form the submatrices A0, A1, A2, B00, B01, and B10 that are needed in solving this
queueing system by the method of Neuts.
Exercise 12.1.4 For the following M/Er/1 queues, compute the matrices V and W to be used in the successive
substitution iterative algorithm for computing Neuts’ R matrix. Then, beginning with R0 = 0, compute R1
and R2.
(a) M/E2/1 with λ = 4.0 and, at each service phase, μi = 10.0.
(b) M/E4/1 with λ = 4.0 and, at each service phase, μi = 20.0.
Exercise 12.1.5 It is known that Neuts’ R matrix for the M/E2/1 queue with mean interarrival time E[A] =
1/4 and mean service time E[S] = 1/5 is given by
R =
0.5600
0.4000
0.1600
0.4000

,
while for the M/E4/1 queue with the same expected interarrival and service times, it is given by
R =
⎛
⎜
⎜
⎝
0.3456
0.2880
0.2400
0.2000
0.1456
0.2880
0.2400
0.2000
0.1056
0.0880
0.2400
0.2000
0.0576
0.0480
0.0400
0.2000
⎞
⎟
⎟
⎠.
For each of these queues, compute the probability that
(a) the system is empty;
(b) the system contains exactly three customers;
(c) the system contains three or more customers.
Exercise 12.2.1 Draw the state transition diagram of an E4/M/1 queue with mean interarrival time of 1/3 and
mean service time of 1/4. Form the submatrices A0, A1, A2, B00, B01, and B10 that are needed in solving this
queueing system by the method of Neuts.
Exercise 12.2.2 For the following Er/M/1 queues, compute the matrices V and W to be used in the successive
substitution iterative algorithm for computing Neuts’ R matrix. Then, beginning with R0 = 0, compute R1
and R2.
(a) E2/M/1 with mean interarrival time of 1/3 and mean service time of 1/4.
(b) E4/M/1 with mean interarrival time of 1/3 and mean service time of 1/4.

12.9 Exercises
473
Exercise 12.2.3 It is known that Neuts’ R matrix for the E2/M/1 queue with mean interarrival time of 1/3 and
mean service time of 1/4 is given by
R =

0
0
0.8229
0.6771

,
while for the E4/M/1 queue with mean interarrival time of 1/3 and mean service time of 1/4 it is given by
R =
⎛
⎜
⎜
⎝
0
0
0
0
0
0
0
0
0
0
0
0
0.8882
0.7889
0.7007
0.6223
⎞
⎟
⎟
⎠.
For each of these queues, compute the probability that
(a) The system is empty;
(b) The system contains exactly three customers;
(c) The system contains more than three customers.
Exercise 12.3.1 Draw the state transition diagram and the transition rate matrix for each of the following
queues. In both cases, generate the matrices V and W needed in the computation of Neuts’ R matrix, and carry
out two iterations of the successive substitution method (beginning with R0 = 0).
(a) The M/H3/1 queue with λ = 2, α1 = 0.5, α2 = 0.3, α3 = 0.2, μ1 = 4, μ2 = 2, and μ3 = 1.
(b) The H3/M/1 queue with α1 = 0.6, α2 = 0.3, α3 = 0.1, λ1 = 4, λ2 = 3, λ3 = 2, and μ = 4.
Exercise 12.3.2
It is known that Neuts’ R matrix for the M/H3/1 queue with λ = 2, α1 = 0.5, α2 = 0.3,
α3 = 0.2, μ1 = 4, μ2 = 2, and μ3 = 1 is given by
R =
⎛
⎝
0.4343
0.0909
0.0808
0.1515
0.6364
0.1212
0.2020
0.1818
0.8283
⎞
⎠,
while for the H3/M/1 queue with α1 = 0.6, α2 = 0.3, α3 = 0.1, λ1 = 4, λ2 = 3, λ3 = 2, and μ = 4. It is
given by
R =
⎛
⎝
0.5179 0.3302 0.1519
0.3884 0.2477 0.1139
0.2590 0.1651 0.0759
⎞
⎠.
For each of these queues, compute the probability that
(a) the system is empty;
(b) the system contains exactly three customers;
(c) the system contains more than three customers.
Exercise 12.4.1 Develop the Matlab code necessary to construct the block submatrices for an M/Er/1 queue
and show that when r is taken to be equal to 3 and this code is incorporated with the Matlab code of Section
12.4, that the resulting program produces the same results as those obtained in the text.
Exercise 12.4.2 Develop the Matlab code necessary to construct the block submatrices for an Er/M/1 queue
and show that when r is taken to be equal to 3 and this code is incorporated with the Matlab code of Section
12.4, that the resulting program produces the same results as those obtained in the text.
Exercises 12.5.1–12.5.10 may be answered using the Matlab code in Section 12.8.
Exercise 12.5.1
Consider a queueing system in which the arrival process is Poisson at rate λ = 2 and for
which the density function of the service process is given as
b(x) = 64xe−8x, x ≥0.
Compute the probability of having three or more customers in the system.

474
Queues with Phase-Type Laws: Neuts’ Matrix-Geometric Method
Exercise 12.5.2 Consider a queueing system in which the service process is exponential at rate μ = 1.2 and
for which the density function of the arrival process is given as
a(x) = 3(3x)2e−3x
2
, x ≥0.
Compute the probability of having exactly three customers in the system.
Exercise 12.5.3 Cars arriving at Bunkey’s Car Wash follow a Poisson distribution with a mean interarrival
time of 10 minutes. These cars are successively vacuumed, washed, and hand-dried, and the time to perform
each of the three tasks is exponentially distributed with a mean of 3 minutes. How long should an arriving
customer expect to wait before vacuuming begins on her car? Bunkey realizes that this is excessive and hires
additional help so that the time to vacuum and hand-dry a car is reduced to 1 minute each. By how much does
this reduce the customer’s waiting time?
Exercise 12.5.4 The distribution of the time it takes to perform an oil change at QuikLube has an expectation
of 4 minutes and a variance of 4. If car arrivals follow a Poisson process with mean interarrival time of 5
minutes, ﬁnd
(a) the probability of having more than 2 cars at QuikLube;
(b) the mean number of cars present;
(c) the mean time spent waiting prior to the commencement of service.
Exercise 12.5.5 The mean interarrival time between patients to a doctor’s ofﬁce is 16 minutes and has a
variance of 32. The time spent with the doctor is exponentially distributed with mean 12 minutes. What is
the probability of ﬁnding at least two patients sitting in the waiting room? Also ﬁnd the mean number in the
doctor’s ofﬁce and the expected time spent in the waiting room.
Exercise 12.5.6 What is the Markov chain and initial probability vector that represent a four-phase Coxian
distribution with parameters μi, αi, i = 1, 2, 3, 4.
Exercise 12.5.7 Draw the phase-type distribution that corresponds to the following Markov chain and initial
probability vector:
Z ′ =
⎛
⎜
⎜
⎜
⎜
⎝
−μ1
0
α1μ1
(1 −α1)μ1
0
μ2
−μ2
0
0
0
0
α3μ3
−μ3
(1 −α3)μ3
0
0
α4μ4
0
−μ4
(1 −α4)μ4
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
,
ζ ′ = (0.6
0.4
0
0 | 0).
Exercise 12.5.8 Consider a single-server queueing system in which the service process has a mean service time
of E[S] = 3 and standard deviation σS = 4 while the arrival process has a mean interarrival time of E[A] = 4
and variance Var[A] = 5. Model the arrival and service processes by phase-type distributions and analyze the
resulting Ph/Ph/1 queue to determine the probability of having exactly i (for i = 0, 1, 2, 3, and 4) customers
in the system.
Exercise 12.5.9 Use the Matlab code developed in the text to investigate the effect of reducing the variance
in the service time distribution of a single serve queue with a hyperexponential arrival process. In particular,
assume that the arrival process has an H2 distribution with parameters α1 = 0.4, λ1 = 1, and λ2 = 4 and that
the service process has a mean service rate equal to 2 (mean service time equal to 1/2). Examine the effect of
modeling the service process with an Er distribution for r = 1, 2, 4, and 8 and investigate how the expected
number of customers in the system, E[N], varies with r.
Exercise 12.5.10 The same as the previous question, but this time reverse the arrival and service time laws.
Assume that the service process has an H2 distribution with parameters α1 = 0.4, μ1 = 1.0, and μ2 = 8.0
and that the arrival process has a mean interarrival time equal to 1/2 (mean arrival rate equal to 2). Examine
the effect of modeling the arrival process with an Er distribution for r = 1, 2, 4, and 8 and investigate how the
expected number of customers in the system, E[N], varies with r.

Chapter 13
The z-Transform Approach to Solving
Markovian Queues
Queueing systems that can be represented by a set of states in which the sojourn time is
exponentially distributed are called Markovian queues. Examples include all of the queues discussed
so far, from the M/M/1 queue and its extensions of Chapter 11 through the Ph/Ph/1 type queues of
Chapter 12. When the states can be arranged in a linear fashion and the transitions are to nearest
neighbors only, the process is called a birth-death process; the transition rate matrix is tridiagonal
and efﬁcient solution approaches are readily available as was seen in Chapter 11. When states can
be arranged into equally sized levels and transitions are permitted within a level and to neighboring
levels only, the process is called a quasi-birth-death process; the transition rate matrix is block
diagonal and the efﬁcient matrix geometric method can be used as in Chapter 12. In both cases,
a limited number of boundary states that do not meet the transition restriction to nearest neighbor
states (or levels) can be handled. Prior to Neuts’ development of the matrix geometric approach,
the standard method for solving advanced Markovian models was the z-transform approach which
we develop in this chapter, the only other alternative being the numerical solution methods which
were discussed extensively in Part II of this text. Although the z-transform approach is sometimes
considered to be an analytic as opposed to a numerical procedure, it is not entirely analytic, because,
as we shall see in later sections, the roots of a polynomial equation frequently must be found
numerically and used in various formulae.
13.1 The z-Transform
In Chapter 5, the probability mass function of a discrete random variable is shown to be uniquely
determined by its z-transform (also called its probability generating function). Furthermore this
z-transform provides a convenient means for computing the mean and higher moments of the
distribution. Indeed, in Chapter 5 our only interest in the z-transform is in using it to compute
expectations and higher moments. The property of the z-transform that is of interest to us in this
chapter is that it provides an effective method for solving the sets of difference equations that arise
in analyzing certain queueing models. The approach is to combine the difference equations into a
single equation from which the z-transform can be extracted and moments subsequently computed.
In some cases, it is possible to invert the transform to compute the actual probability distribution of
customers in the system. We begin by recalling the deﬁnition of the z-transform.
Deﬁnition: The z-transform of a sequence pk, k = 0, 1, . . . , is deﬁned as
P(z) =
∞

k=0
pkzk,
where z is a complex variable and is such that P(z) is analytic i.e., ∞
k=0 pkzk < ∞.

476
The z-Transform Approach to Solving Markovian Queues
If the sequence pk represents the probability distribution of a discrete random variable X, then
a valid range for z includes the unit disk, |z| ≤1. Observe that the z-transform has combined the
sequence into a single number. Each term has been “tagged” by a different power of z, and this
enables the reconstruction of the sequence from the transform. In some cases, the easiest way to
invert the transform and retrieve the sequence is to expand the transform in powers of z and to
associate the coefﬁcient of zk with pk.
Example 13.1 The z-transform of a sequence is given by
P(z) =
1
1 −αz
for some constant α. Expanding this transform in terms of z, we ﬁnd
1
1 −αz =
∞

k=0
αkzk = z0 + αz1 + α2z2 + α3z3 + · · · .
Identifying the coefﬁcients of the powers of z with successive terms in the sequence shows that the
sequence is given by
1, α, α2, α3, . . . .
In this case, for the transform to be analytic, we require that |z| < 1/α. The constant α itself may
be greater than or less than 1.
In other cases, it may be more appropriate to compute individual elements of the sequence from
the formula
pk = 1
k!
dk
dzk P(z)

z=0
for k = 0, 1, . . . .
(13.1)
For example, we have
1
0! P(z)

z=0
= p0 × 1 |z=0 + p1 × z1 |z=0 + p2 × z2 |z=0 + p3 × z3 |z=0 · · · = p0,
1
1!
d
dz P(z)

z=0
= p0 × 0 |z=0 + p1 × 1 |z=0 + p2 × 2z |z=0 + p3 × 3z2 |z=0 · · · = p1,
1
2!
d2
dz2 P(z)

z=0
= 1
2 (p1 × 0 |z=0 + p2 × 2 |z=0 + p3 × 6z |z=0 · · · ) = p2,
and so on. The mean and higher moments of the probability distribution may be conveniently
computed directly from the z-transform by evaluating these same derivatives at z = 1 rather than at
z = 0. The mean is obtained as
E[X] = d
dz P(z)

z=1
.
Second and higher moments may be computed from correspondingly higher derivatives. The kth
derivative of the z-transform evaluated at z = 1 gives the kth factorial moment:
lim
z→1 P(k)(z) = E[X(X −1) . . . .(X −k + 1)].
Thus, for example,
lim
z→1 P′′(z) =
∞

k=0
k(k −1)pk = E[X(X −1)] = E[X2] −E[X]

13.1 The z-Transform
477
and the variance may now be found as
Var[X] = E[X(X −1)] + E[X] −E2[X].
Example 13.2 Consider the probability sequence p0 = 0.5, p1 = 0.25, p2 = 0.25 and pk = 0 for
all k ≥3. Its z-transform is given by P(z) = 0.5×z0+0.25×z1+0.25×z2 = 0.25z2+0.25z+0.5.
Observe that the terms of the sequence are reconstructed as
p0 = P(z)|z=0 = 0.25z2 + 0.25z + 0.5

z=0 = 0.5,
p1 = d
dz P(z)

z=0
= 2 × 0.25z + 0.25|z=0 = 0.25,
p2 = 1
2
d2
dz2 P(z)

z=0
= 1
2 × 2 × 0.25

z=0
= 0.25,
and for higher values k ≥3, the kth derivative of P(z) is zero. The mean of this distribution is
given by
E[X] = d
dz P(z) |z=1 = (2 × 0.25z + 0.25) |z=1 = 0.75,
which may be veriﬁed by direct calculation of 0 × p0 + 1 × p1 + 2 × p2.
Example 13.3 Consider now the sequence pk = 1/2k+1 for all k ≥0. This is the geometric
sequence {1/2, 1/4, 1/8, . . .} whose sum is 1. Its z-transform is given by
P(z) =
∞

k=0
pkzk =
∞

k=0
zk
2k+1 = 1
2
∞

k=0
 z
2
k
.
Since we assume |z| < 1, we may apply the well-known formula for the sum of an inﬁnite geometric
series to obtain
P(z) = 1
2

1
1 −z/2

=
1
2 −z .
Once again, we observe how taking successive derivatives with respect to z and evaluating the results
at z = 0 returns the individual terms of the sequence. We have
p0 = P(z)|z=0 =
1
2 −z

z=0
= 0.5,
p1 = d
dz P(z)

z=0
=
1
(2 −z)2

z=0
= 0.25,
p2 = 1
2
d2
dz2 P(z)

z=0
= 1
2
d
dz
1
(2 −z)2

z=0
=
1
2
 2(2 −z)
(2 −z)4

z=0
= 0.125,
etc.
The mean of this distribution can be found by evaluating
E[X] = d
dz P(z)

z=1
=
1
(2 −z)2

z=1
= 1.0.
This may be veriﬁed by computing the mean directly. We have
E[X] = 0 × (1/2) + 1 × (1/4) + 2 × (1/8) + 3 × (1/16) + · · ·
= 1
2[1 × (1/2)1 + 2 × (1/2)2 + 3 × (1/2)3 + · · · ] = 1
2
∞

i=1
i2−i = 1
2 × 2.

478
The z-Transform Approach to Solving Markovian Queues
In Example 13.1, the z-transform of the sequence αi, i = 0, 1, 2, . . . , for some constant α was
shown to be
P(z) =
∞

k=0
αkzk =
1
1 −αz
with |z| < 1/α for the transform to be analytic. The sequence and its transform are said to form a
pair and are written as
αk ⇐⇒
1
1 −αz .
A similar reasoning shows that, for constant A,
Aαk ⇐⇒
A
1 −αz ,
(13.2)
and for sequences αk and βk, k = 0, 1, 2, . . . , and constants A and B
Aαk + Bβk ⇐⇒
A
1 −αz +
B
1 −βz .
In general, for sequences pk and qk, k = 0, 1, 2 . . . , with transforms P(z) and Q(z), respectively,
and constants A and B,
Apk + Bqk ⇐⇒AP(z) + BQ(z).
The double arrow symbol (⇐⇒) is used to associate a sequence with its transform. Thus
pk ⇐⇒P(z)
indicates that the z-transform of the sequence pk, k = 0, 1, . . . , is given by P(z) and vice versa.
Also, sequences are denoted by lower-case letters and their transforms by the corresponding upper-
case letter. The value of pk is assumed to be zero for k < 0.
13.2 The Inversion Process
The derivative method described earlier for inverting z-transforms (using Equation (13.1)) is
appropriate if only the ﬁrst few terms in the sequence are needed. It is not convenient when we
require the entire sequence. Fortunately, it is frequently possible to obtain the entire sequence just
by inspection, using known relationships between the transform and its inverse such as the transform
pair of Equation (13.2):
A αk ⇐⇒
A
1 −αz .
The most common of these transform-inverse pairs are given in tabular form at the end of this
section.
We now describe the approach to use when the inverse is not known or is not directly available
from tables. The basic idea is to take the transform P(z) and to write it as a sum of much simpler
terms, each of which can be individually transformed using known transform pairs. If the transform
can be written as the quotient of two polynomials in z, then one possibility is to obtain a partial
fraction expansion of P(z). Let us write the quotient as
P(z) = N(z)/D(z),
where the denominator polynomial has constant term (coefﬁcient of z0) equal to 1. If the constant
term of the denominator polynomial is different from 1, then dividing top and bottom by the constant

13.2 The Inversion Process
479
term achieves the desired effect. Consider ﬁrst the case when the degree of the denominator D(z)
is strictly greater than the degree of the numerator, N(z). Let the degree of the polynomial D(z)
be r. Then D(z) possesses r roots and if, for the moment, we assume all r roots to be distinct, this
polynomial may be written as
D(z) = (1 −z/z1)(1 −z/z2) · · · (1 −z/zr)
where z1, z2, . . . , zr are the r roots. With such a D(z), we may obtain a partial fraction expansion
of P(z) as
P(z) = N(z)
D(z) =
N(z)
(1 −z/z1)(1 −z/z2) · · · (1 −z/zr)
=
A1
(1 −z/z1) +
A2
(1 −z/z2) + · · · +
Ar
(1 −z/zr),
(13.3)
where the constants Ai are determined from
Ai = (1 −z/zi) N(z)
D(z)

z=zi
.
Observe that each of the terms of the summation in Equation (13.3) can be inverted very simply by
substituting α = 1/zi into the transform pair:
αk ⇐⇒
1
1 −αz .
We have, for each term i in the summation,
Ai
 1
zi
k
⇐⇒
Ai
1 −z/zi
.
Now, repeatedly using the property Apk +Bqk ⇐⇒AP(z)+BQ(z), we can show that the sequence
corresponding to the transform P(z) is as follows:
P(z) =
A1
1 −z/z1
+
A2
1 −z/z2
+ · · · +
Ar
1 −z/zr
⇐⇒A1
 1
z1
k
+ A2
 1
z2
k
+ · · · + Ar
 1
zr
k
.
Example 13.4 The z-transform of a sequence is given by
P(z) =
(1 −z)
2z2 −7z + 3.
We wish to determine the sequence. We must ﬁrst conﬁrm that the degree of the numerator
polynomial (=1) is strictly less than the degree of the denominator polynomial (=2), which it is.
Our next task is to factor the denominator to obtain
2z2 −7z + 3 = (2z −1)(z −3) = 3(1 −z/z1)(1 −z/z2)
with z1 = 1/2 and z2 = 3. This gives
P(z) =
(1 −z)/3
(1 −z/z1)(1 −z/z2)
= N(z)
D(z),
which we must now write in partial fraction form. We get
P(z) =
A1
1 −z/z1
+
A2
1 −z/z2

480
The z-Transform Approach to Solving Markovian Queues
with
A1 = (1 −z/z1) N(z)
D(z)

z=z1
= (1 −z)/3
(1 −z/z2)

z=z1
= (1 −z1)/3
(1 −z1/z2) = (1 −1/2)/3
(1 −(1/2)/3) = 1/6
5/6 = 1
5
and
A2 = (1 −z/z2) N(z)
D(z)

z=z2
= (1 −z)/3
(1 −z/z1)

z=z2
= (1 −z2)/3
(1 −z2/z1) = (1 −3)/3
(1 −3 × 2) = −2/3
−5
= 2
15.
Notice that
P(z) =
A1
1 −z/z1
+
A2
1 −z/z2
=
1/5
1 −z/(1/2) +
2/15
1 −z/3 = (1/5) × (1 −z/3) + (2/15) × (1 −2z)
(1 −2z)(1 −z/3)
= (1/5) + (2/15) −(z/15) −(4z/15)
(1 −2z)(1 −z/3)
=
(1/3)(1 −z)
(1 −2z)(1 −z/3) =
(1 −z)
2z2 −7z + 3,
as indeed it should be. We now have the transform P(z) in a form that is easy to transpose. Using
the transform pair of Equation (13.2) we have
P(z) =
1/5
1 −2z +
2/15
1 −z/3 ⇐⇒1
52k + 2
15
1
3
k
.
The sequence for which we have been looking is therefore given by
pk = 1
5

2k + 2
3
1
3
k
for k = 0, 1, . . . .
We now consider the case when the degree of the numerator polynomial N(z) is greater than
or equal to the degree of the denominator polynomial D(z). To make use of the partial fraction
expansion technique that we have just discussed, it becomes necessary to rewrite the transform
P(z). Ideally, it may be possible to factor out some power of the numerator so that we can write
P(z) = zi N(z)
D(z)
with the degree of N(z) strictly less than that of D(z), and then use the transform property
pk−i ⇐⇒zi P(z)
or perhaps to factor out a term like (1 −zi) and use the transform property
pk −pk−i ⇐⇒(1 −zi)P(z).
Example 13.5 Suppose we have
P(z) =
z2(1 −z)
2z2 −7z + 3.
We can write this as
P(z) = z2
(1 −z)
2z2 −7z + 3 = z2 N(z)
D(z).
Now, using the previously computed sequence corresponding to N(z)/D(z) and applying the
appropriate transform property, we obtain
N(z)
D(z) ⇐⇒1
5

2k + 2
3
1
3
k

13.2 The Inversion Process
481
and
P(z) = z2 N(z)
D(z) ⇐⇒1
5

2k−2 + 2
3
1
3
k−2
.
If it is not possible to factor the numerator polynomial, it is generally possible to divide the
denominator into the numerator to obtain a polynomial in z plus a remainder term. If, for example
P(z) = N(z)
D(z)
with
N(z) = D(z)M(z) + R(z)
then
P(z) = M(z) + R(z)
D(z).
The individual terms in the polynomial M(z) can be inverted separately, and generally without
difﬁculty, while the degree of the numerator polynomial obtained as the remainder R(z) will be
strictly less than the degree of the denominator D(z) and may now be expanded into partial fractions.
Obtaining the partial fraction expansion of a quotient of two polynomials is more complex
when the denominator polynomial contains roots that are not simple. For example, the polynomial
z2 −2z + 1 = (z −1)2 has a root of multiplicity 2 at z = 1. We shall assume that the denominator
polynomial D(z) has j distinct roots and that the ith root has multiplicity mi. It follows that D(z) is
a polynomial of degree  j
i=1 mi. We shall assume that the degree of the numerator polynomial is
strictly less than this, for otherwise, we will need to adopt the procedures described in the paragraph
above. We can now write
D(z) =
j
i=1
(1 −z/zi)mi,
where the j distinct roots are given by z1, z2, . . . , z j. In this case, it may be shown that the partial
fraction expansion of P(z) is given by
P(z) =
A11
(1 −z/z1)m1 +
A12
(1 −z/z1)m1−1 + · · · +
A1m1
(1 −z/z1)
+
A21
(1 −z/z2)m2 +
A22
(1 −z/z2)m2−1 + · · · +
A2m2
(1 −z/z2) + · · ·
+
A j1
(1 −z/z j)m j +
A j2
(1 −z/z j)m j−1 + · · · +
A jm j
(1 −z/z j).
The constant terms are computed from
Ai j =
1
( j −1)! (−zi) j−1 d j−1
dz j−1
&
(1 −z/zi)mi N(z)
D(z)
'
z=zi
.
Example 13.6 In Example 13.4 we had
P(z) =
(1 −z)
2z2 −7z + 3 =
(1/3)(1 −z)
(1 −2z)(1 −z/3) =
(1/3)(1 −z)
(1 −z/z1)(1 −z/z2).
Let us modify this by supposing that
P(z) = N(z)
D(z) =
(1/3)(1 −z)
(1 −2z)(1 −z/3)2 =
(1/3)(1 −z)
(1 −z/z1)(1 −z/z2)2 .
In this case, the denominator is now a cubic polynomial with a simple root (m1 = 1) at z = 1/2
and a double root (m2 = 2) at z = 3. When we expand P(z) as a partial fraction we will obtain

482
The z-Transform Approach to Solving Markovian Queues
three terms
P(z) =
A11
(1 −z/z1) +
A21
(1 −z/z2)2 +
A22
(1 −z/z2)
with z1 = 1/2 and z2 = 3. The Ai j are computed from the formula as follows:
A11 = (1 −z/z1) N(z)
D(z)

z=z1
=
(1 −z)/3
(1 −z/z2)2

z=z1
= (1 −z1)/3
(1 −z1/z2)2 =
(1 −1/2)/3
(1 −(1/2)/3)2 =
1/6
25/36 = 6
25
A21 = (1 −z/z2)2 N(z)
D(z)

z=z2
= (1 −z)/3
(1 −z/z1)

z=z2
= (1 −z2)/3
(1 −z2/z1) = (1 −3)/3
(1 −3 × 2) = −2/3
−5
= 2
15
A22= 1
1!(−z2)1 d
dz
&
(1 −z/z2)2 N(z)
D(z)
'
z=z2
= −z2
d
dz
& (1 −z)/3
(1 −z/z1)
'
z=z2
= −3 d
dz
&(1 −z)/3
(1 −2z)
'
z=z2
= −d
dz
& (1 −z)
(1 −2z)
'
z=z2
= −(1 −2z)(−1) −(1 −z)(−2)
(1 −2z)2

z=z2
= −
1
(1 −2z)2

z=z2
= −1
25.
We therefore have
P(z) =
A11
(1 −z/z1) +
A21
(1 −z/z2)2 +
A22
(1 −z/z2)
=
6/25
(1 −2z) +
2/15
(1 −z/3)2 −
1/25
(1 −z/3).
(13.4)
The reader should verify this result by checking that
6/25
(1 −2z) +
2/15
(1 −z/3)2 −
1/25
(1 −z/3) =
(1/3)(1 −z)
(1 −2z)(1 −z/3)2 = P(z).
We are now in a position to invert P(z),
P(z) =
6/25
(1 −2z) +
2/15
(1 −z/3)2 −
1/25
(1 −z/3),
and determine the sequence. The ﬁrst and third terms fall into the category of transform pairs
previously discussed. For the second term, we need to use the relation
1
(1 −αz)2 ⇐⇒(k + 1)αk.
Treating each term of Equation (13.4) separately, we obtain
6/25
(1 −2z) ⇐⇒6
252k,
2/15
(1 −z/3)2 ⇐⇒2
15(k + 1) 1
3k ,
1/25
(1 −z/3) ⇐⇒1
25
1
3k ,

13.2 The Inversion Process
483
which when taken together provide the ﬁnal result
P(z) ⇐⇒6 × 2k
25
+ 2(k + 1)
15 × 3k −
1
25 × 3k = 1
75

18 × 2k + (10k + 7)3−k 
.
Table 13.1 shows some of the most commonly used transform pairs. In this table, the
symbol ⋆implies convolution and uk is the unit function which has the value 1 if k = 0 and the
value 0 otherwise.
Table 13.1. Some common transform pairs.
Sequence
Transform
Apk + Bqk
⇐⇒
AP(z) + BQ(z)
pk ⋆qk
⇐⇒
P(z)Q(z)
αk pk
⇐⇒
P(αz)
pk+1
⇐⇒
[P(z) −p0]/z
pk+i
⇐⇒
>
P(z) −i
j=1 z j−1 p j−1
?
/zi
pk−1
⇐⇒
zP(z)
pk−i
⇐⇒
zi P(z)
pk −pk−1
⇐⇒
(1 −z)P(z)
pk −pk−i
⇐⇒
(1 −zi)P(z)
Sequence
Transform
uk
⇐⇒
1
uk−i
⇐⇒
zi
{1, α, α2, . . . } = αk
⇐⇒
1/(1 −αz)
{0, α, 2α2, 3α3, . . . } = kαk
⇐⇒
αz/(1 −αz)2
{0, 1, 2, 3, . . . } = k
⇐⇒
z/(1 −z)2
{1, 2, 3, . . . } = k + 1
⇐⇒
1/(1 −z)2
1
m!(k + m)(k + m −1) . . . (k + 1)αk
⇐⇒
1/(1 −αz)m+1

484
The z-Transform Approach to Solving Markovian Queues
13.3 Solving Markovian Queues using z-Transforms
13.3.1 The z-Transform Procedure
As we mentioned previously, the z-transform provides an effective method for solving sets of
difference equations, and it is to the difference equations that arise in Markovian queueing systems
that we now apply it. The general procedure is speciﬁed in the following steps.
1. By considering transitions into and out of an arbitrary state, generate the difference equations
for the queueing system.
2. Multiply the jth equation by z j and sum over all applicable j.
3. Identify the z-transform P(z) and isolate it on the left-hand side.
4. Eliminate all unknowns from the right-hand side. Once this step has been completed, various
moments of customer occupancy may be found.
5. Invert the z-transform to obtain the stationary probability distribution of customers in the
system.
In general, it is this last step that creates the most difﬁculties.
13.3.2 The M/M/1 Queue Solved using z-Transforms
We illustrate the z-transform approach by an analysis of the M/M/1 queue.
Step 1: The following set of difference equations arises when solving for the stationary
distribution of customers in the M/M/1 queue. The probability that the queueing system contains
n + 1 customers (pn+1) is related to the probabilities that it contains n customers (pn) and n −1
customers (pn−1) through the difference equations
pn+1 = λ + μ
μ
pn −λ
μ pn−1 = (ρ + 1)pn −ρpn−1,
n ≥1,
and
p1 = λ
μ p0 = ρp0.
Step 2: To solve these equations using the z-transform, we must ﬁrst multiply the nth equation
by zn and sum over all applicable n. We obtain
pn+1zn = (ρ + 1)pnzn −ρpn−1zn,
n ≥1,
z−1 pn+1zn+1 = (ρ + 1)pnzn −ρzpn−1zn−1,
n ≥1,
and hence
z−1
∞

n=1
pn+1zn+1 = (ρ + 1)
∞

n=1
pnzn −ρz
∞

n=1
pn−1zn−1.
(13.5)
Step 3: We identify the z-transform P(z) by writing Equation (13.5) as
z−1
	 ∞

n=−1
pn+1zn+1 −p1z −p0

= (ρ + 1)
	 ∞

n=0
pnzn −p0

−ρz
∞

n=1
pn−1zn−1,
and observing that
P(z) ≡
∞

n=−1
pn+1zn+1 =
∞

n=0
pnzn =
∞

n=1
pn−1zn−1.

13.3 Solving Markovian Queues using z-Transforms
485
Therefore
z−1 {P(z) −p1z −p0} = (ρ + 1) {P(z) −p0} −ρzP(z)
or
P(z) −p1z −p0 = z(ρ + 1) {P(z) −p0} −ρz2P(z).
Bringing terms in P(z) to the left-hand side,
P(z) −z(ρ + 1)P(z) + ρz2P(z) = p1z + p0 −z(ρ + 1)p0,
we obtain
P(z) = p1z + p0 −z(ρ + 1)p0
1 −z(ρ + 1) + ρz2
.
Step 4: Two unknowns, p0 and p1, need to be removed from the right-hand side. The second of
these can be eliminated by using the fact that p1 = ρp0. This gives
P(z) = ρp0z + p0 −z(ρ + 1)p0
1 −zρ −z + ρz2
= ρz + 1 −zρ −z
1 −zρ −z + ρz2 p0 =
(1 −z)
(1 −z)(1 −zρ) p0.
We therefore conclude that
P(z) =
p0
1 −zρ .
To eliminate the remaining unknown, p0, we use the fact that the sum of the probabilities must
be equal to 1. Substituting z = 1 gives P(1) = ∞
n=0 pn = 1. Thus 1 = p0/(1 −ρ) and hence
p0 = 1 −ρ. It follows that the z-transform for the M/M/1 queue is given by
P(z) = 1 −ρ
1 −zρ .
Step 5: To invert this transform we can expand P(z) = (1 −ρ)/(1 −ρz) as a power series and
pick off the terms. Observe that 1/(1 −zρ) is equal to the sum of a geometric series:
1
1 −zρ = 1 + zρ + (zρ)2 + (zρ)3 + · · · .
Therefore
P(z) =
∞

k=0
(1 −ρ)ρkzk
and so the coefﬁcient of zk (which we previously speciﬁed to be pk) is equal to (1−ρ)ρk. This gives
the ﬁnal solution as pk = (1 −ρ)ρk which is the same as we obtained before.
Notice that we could also have inverted the transform using known transform pairs. Identifying
the right-hand side of the transform pair
Aαn ⇐⇒
A
1 −αz
with the transform
P(z) = 1 −ρ
1 −ρz ,
where A has the value A = 1 −ρ, we directly obtain the sequence
pk = (1 −ρ)ρk.

486
The z-Transform Approach to Solving Markovian Queues
13.3.3 The M/M/1 Queue with Arrivals in Pairs
0
1
2
 j-1
j
j+1
μ
μ
μ
μ
μ
μ
μ
λ
λ
λ
λ
λ
Figure 13.1. Arrivals in groups of size 2.
Consider an M/M/1 queue in which at each arrival instant exactly two customers arrive. The state
transition rate diagram for this queueing system is shown in Figure 13.1 and its transition rate matrix
is given by
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−λ
0
λ
0
μ −(λ + μ)
0
λ
0
μ
−(λ + μ)
0
λ
0
...
0
0
μ
−(λ + μ)
0
λ
...
0
μ
−(λ + μ)
0
...
0
0
μ
−(λ + μ) ...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Observe that Q has a block tridiagonal (quasi-birth-death) structure and hence the matrix-geometric
approach, discussed in the previous chapter, may be used to solve this system. Leaving this approach
as Exercise 13.3.1, let us instead proceed to move through the various steps involved in solving this
system by the method of z-transforms.
Step 1: By inspection, the balance equations are
λp0 = μp1,
j = 0,
(λ + μ)p1 = μp2,
j = 1,
(λ + μ)p j = λp j−2 + μp j+1,
j ≥2.
Step 2: Multiply the jth equation by z j and sum over all j ≥0:
λp0 = μp1,
j = 0,
(λ + μ)p1z = μp2z,
j = 1
(λ + μ)p jz j = λp j−2z j + μp j+1z j,
j ≥2.
Thus
λp0 + (λ + μ) p1z + (λ + μ)
∞

j=2
p jz j = μp1 + μp2z + λ
∞

j=2
p j−2z j + μ
∞

j=2
p j+1z j.
(13.6)
Step 3: Identify the z-transform P(z) = ∞
j=0 p jz j and bring it to the left-hand side: Simplifying
Equation (13.6) and subsequently identifying P(z), we obtain
λ
∞

j=0
p jz j + μ
∞

j=1
p jz j = λ
∞

j=2
p j−2z j + μ
∞

j=0
p j+1z j
λP(z) + μ[P(z) −p0] = λz2P(z) + μ
z [P(z) −p0]

13.3 Solving Markovian Queues using z-Transforms
487
P(z) =
μp0(1 −1/z)
λ(1 −z2) + μ(1 −1/z) =
μp0
μ −λz(z + 1).
Step 4: Eliminate the unknown p0 from right-hand side. Since the average arrival rate ¯λ is equal
to 2λ, we have ρ = ¯λ¯x = 2λ/μ. Thus
P(z) =
2p0
2 −ρz(z + 1).
Also observe that P(1) = 1 = 2p0/(2 −2ρ), i.e., p0 = 1 −ρ, and hence
P(z) =
2(1 −ρ)
2 −ρz(z + 1).
The mean number of customers in the system can be obtained at this point:
E[N] = d P(z)
dz

z=1
= 2(1 −ρ)ρ(2z + 1)
[2 −ρz(z + 1)]2

z=1
= 2(1 −ρ)ρ(3)
(2 −2ρ)2
= 3
2
ρ
1 −ρ .
Step 5: Invert the transform. We have
P(z) =
2(1 −ρ)
2 −ρz(z + 1) = 2(1 −ρ)/ρ
2/ρ −z −z2 .
Let the roots of z2 + z −2/ρ be z1 and z2, i.e.,
z2 + z −2/ρ = (z −z1)(z −z2) with z1,2 = −1
2 ±
√1 + 8/ρ
2
.
Observe that z1z2 = −2/ρ. We have
P(z) =
2(1 −ρ)/ρ
2/ρ −z −z2 = (1 −ρ)(−2/ρ)
z2 + z −2/ρ = (1 −ρ)(−2/ρ)
(z −z1)(z −z2)
=
(1 −ρ)(z1z2)
(z −z1)(z −z2) =
(1 −ρ)
(1 −z/z1)(1 −z/z2).
Deﬁning N(z) and D(z) from
P(z) =
(1 −ρ)
(1 −z/z1)(1 −z/z2) = N(z)
D(z),
we proceed to form the partial fraction expansion of P(z) as
P(z) =
A1
1 −z/z1
+
A2
1 −z/z2
with
A1 = (1 −z/z1) N(z)
D(z)

z=z1
=
(1 −ρ)
(1 −z1/z2)
and
A2 = (1 −z/z2) N(z)
D(z)

z=z2
=
(1 −ρ)
(1 −z2/z1).
This allows us to obtain the successive probabilities from the transform pair,
P(z) ⇐⇒A1
 1
z1
k
+ A2
 1
z2
k
.

488
The z-Transform Approach to Solving Markovian Queues
Suppose, for example, that λ = 4 and μ = 15. This gives ρ = 8/15 and the roots of the quadratic
polynomial are z1 = 3/2 and z2 = −5/2. The values of A1 and A2 are
A1 =
(1 −ρ)
(1 −z1/z2) = 7
24
and
A2 =
(1 −ρ)
(1 −z2/z1) = 7
40
and successive values of pk = Prob{N = k} are found as
pk = 7
24
2
3
k
+ 7
40

−2
5
k
.
Using this formula, we see, for example, that
p0 = 0.466667, p1 = 0.124444, p2 = 0.157630, p3 = 0.075220, p4 = 0.062093, etc.
Substituting k = 10 directly into the formula gives p10 = 0.005076.
13.3.4 The M/Er/1 Queue Solved using z-Transforms
It is possible to implement the z-transform approach to solve the M/Er/1 queue using the two-
dimensional state descriptor (k, i) seen in the matrix geometric method of the previous chapter. This
is the approach developed in the text by Gross and Harris [19]. It is also possible to combine the
components k and i into a single quantity, namely, the number of service phases yet to be completed
by all customers in the system and to apply the z-transform to the difference equations derived using
this compact descriptor. This is the approach developed in the text by Kleinrock [24]. We choose to
follow this second approach, since it is somewhat simpler. Indeed our application of the z-transform
to the queueing systems of the next three sections closely mirrors that of Kleinrock.
When an M/Er/1 queue contains k customers and the customer in service is in phase i, the total
number of service phases still to be completed is
j = (k −1)r + (r −i + 1) = rk −i + 1.
Let Pj denote the probability of having j phases in the system at steady state:
Pj = Prob{Number of phases in system = j}.
It follows that, if we let pk denote the equilibrium probability of having k customers in the system,
i.e.,
pk = Prob{Number of customers in system = k},
then
pk =
kr

j=(k−1)r+1
Pj,
k = 1, 2, 3, . . . ,
and therefore the distribution of customers in the system at any time may be easily computed from a
knowledge of the number of service phases remaining at that time. Using the one-dimensional state
descriptor, the state transition diagram may be drawn as shown in Figure 13.2.
0
1
2
r
r+1
j−r
j
j+1
λ
λ
λ
λ
λ
λ
λ
μ
μ
μ
μ
μ
μ
μ
r
r
r
r
r
r
r
Figure 13.2. Alternate state transition diagram for the M/Er/1 queue.

13.3 Solving Markovian Queues using z-Transforms
489
Step 1: We obtain the difference equations for this queueing system by considering transitions
into and out of an arbitrary state j ≥r. This state is entered from state j −r due to an arrival and
from state j + 1 due to a departure. State j is exited either due to an arrival, at rate λ, or due to a
service completion, at rate rμ. Special boundary conditions exist for states 0, 1, . . . ,r −1.
The difference equations are
λP0 = rμP1,
(λ + rμ)P1 = rμP2,
(λ + rμ)Pj = rμPj+1,
1 ≤j ≤r −1,
(λ + rμ)Pj = λPj−r + rμPj+1,
j ≥r.
Step 2: We solve these difference equations using z-transforms, where the z-transform of the
sequence {P0, P1, P2, . . .} is deﬁned as
P(z) ≡
∞

j=0
Pjz j.
To do so, we multiply the jth equation by z j and sum over all j ≥1. We obtain
∞

j=1
(λ + rμ)Pjz j =
∞

j=r
λPj−rz j +
∞

j=1
rμPj+1z j.
Step 3: We now arrange to identify the z-transform. We have
(λ + rμ)
⎡
⎣
∞

j=0
Pjz j −P0
⎤
⎦= λzr
∞

j=r
Pj−rz j−r + rμ
z
∞

j=1
Pj+1z j+1,
(λ + rμ) [P(z) −P0] = λzr P(z) + rμ
z [P(z) −P0 −P1z] ,
P(z) = P0

λ + rμ −(rμ)/z
 
−rμP1
λ + rμ −λzr −(rμ)/z
.
Step 4: To eliminate the unknowns P0 and P1 from the right-hand side, we ﬁrst make use of the
equation λP0 = rμP1, and obtain
P(z) =
rμP0

1 −1/z
 
λ + rμ −λzr −(rμ)/z =
rμP0 [1 −z]
rμ + λzr+1 −(λ + rμ)z .
We now use the fact that P(1) = 1 to evaluate P0. To do so, we need L’Hôpital’s rule which
allows us to ﬁnd the limit (if any) of a quotient of functions when both numerator and denominator
approach zero. If
lim
x→c f (x) = lim
x→c g(x) = 0
and if
lim
x→c
 f ′(x)
g′(x)

= L
then
lim
x→c
 f (x)
g(x)

= L.

490
The z-Transform Approach to Solving Markovian Queues
Thus, using L’Hôpital’s rule (since P(1) = 0/0),
1 = P(1) = lim
z→1 P(z) = lim
z→1
&
rμP0 [1 −z]
rμ + λzr+1 −(λ + rμ)z
'
= lim
z→1
&
−rμP0
λ(r + 1)zr −(λ + rμ)
'
=
−rμP0
λ(r + 1) −(λ + rμ) =
rμP0
rμ −λr =
P0
1 −(λr)/(rμ),
which implies that P0 = 1 −λ/μ = 1 −ρ (which might have been expected).
In this system, the arrival rate is λ and the average service time is held ﬁxed at 1/μ independent
of r. Therefore the utilization factor is given by
ρ = λ¯x = λ/μ.
It follows that
P(z) =
rμ(1 −ρ)(1 −z)
rμ + λzr+1 −(λ + rμ)z .
Notice that substituting r = 1 in the above expression yields
P(z) =
μ(1 −ρ)(1 −z)
μ + λz2 −(λ + μ)z = μ(1 −ρ)(1 −z)
(λz −μ)(z −1)
= μ(1 −ρ)
(μ −λz) = 1 −ρ
1 −ρz ,
which is the z-transform for the M/M/1 queue, namely,
P(z) = 1 −ρ
1 −ρz
⇐⇒
(1 −ρ)ρ j,
the geometric distribution as expected.
Step 5: To obtain the distribution of the number of phases in the system, we need to invert this
transform. The usual approach is to make a partial fraction expansion and to invert each term by
inspection. But, to carry out the expansion, we need to identify the r + 1 zeros of the denominator
polynomial,
λzr+1 −(λ + rμ)z + rμ = 0.
Unity must be one of the roots, since (1 −z) is a factor in the numerator. Dividing (1 −z) into D(z)
gives
rμ −λ(z + z2 + · · · + zr).
The same result may be obtained by observing that
(1 −z)

rμ −λ(z + z2 + · · · + zr)
 
= rμ −λz −λz2 −λz3 −· · · −λzr
−zrμ + λz2 + λz3 + · · · + λzr + λzr+1
= rμ −(λ + rμ)z + λzr+1.
We shall also factor out rμ so that both top and bottom of P(z) can be divided by rμ(1 −z):
P(z) =
(1 −ρ)
1 −λ(z + z2 + · · · + zr)/rμ.
(13.7)

13.3 Solving Markovian Queues using z-Transforms
491
Denote the r yet to be determined roots by z1, z2, . . . , zr. These are the roots of the denominator in
Equation (13.7). Once we ﬁnd them, we can write the denominator as
(1 −z/z1)(1 −z/z2) · · · (1 −z/zr),
and so the expression for the z-transform becomes
P(z) =
1 −ρ
(1 −z/z1)(1 −z/z2) · · · (1 −z/zr) = N(z)
D(z).
In this form, the partial fraction expansion is easy to compute. It is given by
P(z) =
r

i=1
Ai
(1 −z/zi),
where
Ai = (1 −z/zi)P(z)
z=zi .
We may now invert this transform by repeated use of the transform relationship
Ai
(1 −z/zi)
⇐⇒
Ai
 1
zi
 j
.
It then follows that Pj, the probability of j service phases, can be obtained as
P(z) =
r

i=1
Ai
(1 −z/zi)
⇐⇒
r

i=1
Ai
 1
zi
 j
= Pj.
The equilibrium distribution of customers in the M/Er/1 queue may now be computed as
pk =
kr

j=(k−1)r+1
Pj,
k = 1, 2, 3, . . . .
The expected number of phases in the system E[N] is obtained by evaluating P′(z)|z=1. We have
P′(z)|z=1 = −[rμ + λzr+1 −(λ + rμ)z] −(1 −z)[−(λ + rμ) + λ(r + 1)zr]
[rμ + λzr+1 −(λ + rμ)z]2
rμ(1 −ρ).
To evaluate at z = 1, we must use L’Hôpital’s rule (twice) to get
E[N] = P′(1) = (r + 1)ρ
2(1 −ρ),
the expected number of phases. The expected time spent in the queue waiting for service, Wq, is
given by
Wq =
#
P′(z)|z=1
$ 1
rμ = 1
rμ
(r + 1)ρ
2(1 −ρ) =
(r + 1)ρ
2rμ(1 −ρ),
since, from PASTA, an arriving customer ﬁnds E[N] service phases already present and all of these
must be completed (at a mean service time of 1/rμ each) before service can begin on the newly
arriving customer.
We may now obtain W from W = Wq + 1/μ and Lq and L from Little’s law:
Lq = λWq = (r + 1)ρ
2r(1 −ρ)
λ
μ = r + 1
2r
λ2
μ(μ −λ)
and L = λW.

492
The z-Transform Approach to Solving Markovian Queues
0
1
2
μ
μ
3
4
5
λ
λ
λ
λ
λ
λ
μ
μ
μ
μ
μ
μ
3
3
3
3
3
3
3
3
μ
3
λ
λ
j
j+1
λ
Figure 13.3. State transition diagram for the M/E3/1 queue.
Example 13.7 The Erlang-3 Service Model
Let us consider an M/Er/1 queue with parameters λ = 1.0, μ = 1.5, and r = 3. We would
like to compute the probability of having k customers in this system and to plot this information
graphically. As before, let k denote the number of customers and j denote the number of service
phases in the system. Also
pk = Prob{Number of customers in system = k}
and
Pj = Prob{Number of phases in system = j}.
It follows that
pk =
kr

j=(k−1)r+1
Pj =
3k

j=3(k−1)+1
Pj = P3k−2 + P3k−1 + P3k,
k = 1, 2, 3, . . . .
The balance equations, obtained by inspection from the state transition diagram (Figure 13.3) are
given by
λP0 = 3μP1,
(λ + 3μ)P1 = 3μP2,
(λ + 3μ)P2 = 3μP3,
(λ + 3μ)Pj = λPj−3 + 3μPj+1,
j ≥3.
To prepare to generate the z-transform, we multiply the jth equation by z j and obtain
λP0 = 3μP1,
(λ + 3μ)P1z = 3μP2z,
(λ + 3μ)P2z2 = 3μP3z2,
(λ + 3μ)Pjz j = λPj−3z j + 3μPj+1z j,
j ≥3.
Summing both sides over all permissible values of j gives
λP0 + (λ + 3μ)
∞

j=1
Pjz j = λ
∞

j=3
Pj−3z j + 3μ
∞

j=0
Pj+1z j,
(λ + 3μ)
∞

j=0
Pjz j −3μP0 = λz3
∞

j=0
Pjz j + 3μz−1
⎛
⎝
∞

j=0
Pjz j −P0
⎞
⎠,
(λ + 3μ)P(z) −3μP0 = λz3P(z) + 3μz−1(P(z) −P0),
(λ + 3μ −λz3 −3μz−1)P(z) = 3μP0 −3μz−1P0.

13.3 Solving Markovian Queues using z-Transforms
493
It follows then that
P(z) =
3μP0(1 −z−1)
λ + 3μ −λz3 −3μz−1 =
3μP0(1 −z)
λz4 −(λ + 3μ)z + 3μ =
(9/2)P0(1 −z)
z4 −(1 + 9/2)z + 9/2
=
9P0(1 −z)
2z4 −11z + 9 =
9P0(1 −z)
(z −1)(2z3 + 2z2 + 2z −9) =
9P0
9 −2z(z2 + z + 1).
From P(1) = 1, we obtain
9 −2(1 + 1 + 1) = 9P0
=⇒
P0 = 1/3.
The roots of the denominator are
z1 = 1.2169,
z2, 3 = −1.1085 ± 1.5714i.
Notice that z1z2z3 = 9/2. These roots may be determined in a number of ways. It is possible
to use pocket calculators, software such as Matlab, root ﬁnding algorithms, such as the secant
method or Newton’s method, and so on. Also, for cubics, a closed form exists. The cubic1
x3 + 3ax2 + 3bx + c = 0 has three roots given by
x1 = (s1 + s2) −a,
x2 = −
s1 + s2
2
−a

+ i
√
3
2 (s1 −s2),
x3 = −
s1 + s2
2
−a

−i
√
3
2 (s1 −s2),
where
s1 =
>
r +
.
q3 + r2
?1/3
,
s2 =
>
r −
.
q3 + r2
?1/3
,
and q = b −a2,
r = (3ab −c)/2−a3.
We may now obtain the partial fraction expansion as
P(z) =
3
9 −2z(z2 + z + 1) =
3
−2(z −z1)(z −z2)(z −z3)
=
3/(z1z2z3)
2[(1 −z/z1)(1 −z/z2)(1 −z/z3)]
=
1/3
(1 −z/z1)(1 −z/z2)(1 −z/z3) =
A1
(1 −z/z1) +
A2
(1 −z/z2) +
A3
(1 −z/z3),
where A1 = .15649, A2 = .088421−.0017774i, A3 = .088421+.0017774i. The Ai are obtained
as indicated previously. For example,
A1 = (1 −z/z1)P(z)
z=z1 =
1/3
(1 −z/z2)(1 −z/z3)
z=z1 = 0.15649.
Therefore, using
Pk = A1
 1
z1
k
+ A2
 1
z2
k
+ A3
 1
z3
k
,
1 Note that all cubic equations may be written in this form.

494
The z-Transform Approach to Solving Markovian Queues
we get
p0 = P0 = A1 + A2 + A3 = .33333,
pk = P3k−2 + P3k−1 + P3k =
A1
 1
z1
3k−2 (
1 +
 1
z1

+
 1
z1
2)
+ A2
 1
z2
3k−2 (
1 +
 1
z2

+
 1
z2
2)
+ A3
 1
z3
3k−2 (
1 +
 1
z3

+
 1
z3
2)
.
Notice that we could have obtained these solutions more simply by just inserting the appropriate
values into the general formulae that we have already developed. To follow this approach we ﬁrst
compute the roots z1, z2, . . . , zr of the denominator polynomial
rμ −λ(z + z2 + · · · + zr).
With these roots we then compute the coefﬁcients
Ai = (1 −z/zi)P(z)
z=zi .
We can now ﬁnd the probabilities for the number of service phases in the system as
Pj = (1 −ρ)
r

i=1
Ai
z j
i
,
j = 1, 2, . . . ,
and ﬁnally we can compute (and plot) the required probabilities of the number of customers in the
system from
pk =
kr

j=(k−1)r+1
Pj, k = 1, 2, . . . .
The Matlab program given below carries out these operations. Notice that two of the roots of the
denominator polynomial turn out to be a complex pair. However, the probabilities Pj are always
real, as the following analysis shows. First, both z3 and A3 are real. Let
z1,2 = a ± ib =⇒z1,2 = ξe±iθ,
where ξ =
√
a2 + b2 and θ = tan−1(b/a), and let
A1,2 = c ± id
=⇒
A1,2 = γ e±iα.
Since
A1
z j
1
+ A2
z j
2
=
γ e+iα
(ξe+iθ) j +
γ e−iα
(ξe−iθ) j = γ
ξ j
#
ei(α−θ j) + e−i(α−θ j)$
= 2γ cos(α −θ j)
ξ j
is real, it follows that
Pj = (1 −ρ)
(
A1
z j
1
+ A2
z j
2

+ A3
z j
3
)
is also real. The following Matlab code computes and plots (Figure 13.4) the distribution of
customers in the M/E3/1 queue.

13.3 Solving Markovian Queues using z-Transforms
495
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Probability Distribution for Erlang−3 Queue
Number of Customers
Probability
Figure 13.4. Probability distribution for Erlang service model as plotted by Matlab.
Function E3service()
poly=[-1 -1 -1 4.5];
z = roots(poly); rho = 1/1.5;
P0 = 1 - rho;
A1 = P0/((1-z(1)/z(2))*(1-z(1)/z(3)));
A2 = P0/((1-z(2)/z(1))*(1-z(2)/z(3)));
A3 = P0/((1-z(3)/z(1))*(1-z(3)/z(2)));
for j=1:30
P(j) = (A1*z(1)^(-j)+A2*z(2)^(-j)+A3*z(3)^(-j));
end
for k=1:10
p(k) = sum(P(3*k-2:3*k));
end
k = 0:10;
Prob=[P0 p]’
plot(k,Prob)
title(’Probability Distribution for Erlang-3 Queue’)
xlabel(’Number of Customers’)
ylabel(’Probability’)
The roots of the denominator polynomial and the coefﬁcients of the partial fraction expansion, as
computed by Matlab, are
z1 = −1.1085 + 1.5714i,
A1 = 0.0884 −0.0018i,
z2 = −1.1085 −1.5714i,
A2 = 0.0884 + 0.0018i,
z3 = 1.2169,
A3 = 0.1565.

496
The z-Transform Approach to Solving Markovian Queues
The initial entries of the computed probability distribution are
p0 = 0.33333333333333,
p1 = 0.27526291723823,
p2 = 0.17061034683687,
p3 = 0.09772495709866,
p4 = 0.05470074691842,
p5 = 0.03042090224876,
p6 = 0.01688928695838,
p7 = 0.00937279128319,
p8 = 0.00520098215039,
p9 = 0.00288598001521.
13.3.5 The Er/M/1 Queue Solved using z-Transforms
In applying the z-transform approach to the Er/M/1 queue we shall again follow Kleinrock [24]
and adopt a procedure similar to that used for the M/Er/1 queue. We begin by converting the two-
dimensional state description of the matrix-geometric approach to a one-dimensional one. In the
two-dimensional description, a state (k, i) indicates that k customers are present in the system and
the customer who is in the process of arriving is in arrival phase i. Each customer who has arrived
(but not yet departed; there are k of them) has completed all r “arrival” phases. To this we shall add
the number of arrival phases actually completed by an arriving customer, (i −1), and use the total
number of arrival phases in the system as a one-dimensional state descriptor. Thus, when there are k
customers in the system and the arriving customer is in phase i, the total number of arrival phases is
j = rk + (i −1).
This then is the state descriptor for the Er/M/1 queue. The state transition diagram is shown in
Figure 13.5 where it may be seen that a service completion results in the removal of r arrival phases.
0
1
2
r
r+1
λ
λ
λ
λ
λ
μ
μ
μ
μ
μ
r
r
r
r
r
j
j+1
λ
λ
λ
λ
r
r
r
r
μ
μ
μ
j−1
Figure 13.5. State transition diagram for the Er/M/1 queue.
Let Pj be the equilibrium probability of having j arrival phases in the system,
Pj = Prob{Number of arrival phases in system = j},
and let pk be the equilibrium probability of having k customers present,
pk = Prob{Number of customers in system = k}.
Then
p0 =
r−1

j=0
Pj,
p1 =
2r−1

j=r
Pj,
and in general,
pk =
r(k+1)−1

j=rk
Pj,
k = 0, 1, 2, . . . .

13.3 Solving Markovian Queues using z-Transforms
497
The equilibrium equations can be found by inspection and are
rλP0 = μPr,
rλPj = rλPj−1 + μPj+r,
1 ≤j ≤r −1,
(rλ + μ)Pj = rλPj−1 + μPj+r,
j ≥r.
Multiplying the jth equation by z j and adding over all equations, we obtain, on the left-hand side,
∞

j=0
rλPjz j +
∞

j=r
μPjz j = rλP(z) + μP(z) −μ
r−1

j=0
Pjz j
= (rλ + μ)P(z) −μ
r−1

j=0
Pjz j,
and for the right-hand side,
∞

j=1
rλPj−1z j +
∞

j=0
μPj+rz j = rλz
∞

j=1
Pj−1z j−1 + μ
zr
∞

j=0
Pj+rz j+r
= rλzP(z) + μ
zr
⎡
⎣
∞

j=0
Pjz j −
r−1

j=0
Pjz j
⎤
⎦= rλzP(z) + μ
zr P(z) −μ
zr
r−1

j=0
Pjz j.
Equating both sides gives
(rλ + μ)P(z) −μ
r−1

j=0
Pjz j = rλzP(z) + μ
zr P(z) −μ
zr
r−1

j=0
Pjz j.
Bringing all terms in P(z) to the right-hand side and all other terms to the left-hand side,
P(z)
&
(μ + rλ) −rλz −μ
zr
'
= μ
r−1

j=0
Pjz j −μ
zr
r−1

j=0
Pjz j,
and ﬁnally solving for P(z)
P(z) =
(μ/zr) r−1
j=0 Pjz j −μ r−1
j=0 Pjz j
rλz −(μ + rλ) + (μ/zr)
=
μ r−1
j=0 Pjz j −μzr r−1
j=0 Pjz j
rλzr+1 −(μ + rλ)zr + μ
,
i.e.,
P(z) =
(1 −zr) r−1
j=0 Pjz j
rρzr+1 −(1 + rρ)zr + 1,
(13.8)
where, as always, ρ = λ¯x = λ/μ.
The denominator polynomial, rρzr+1 −(1 +rρ)zr + 1, being of degree r + 1, has r + 1 zeros, of
which one is unity. Of the remaining r zeros, it can be shown from Rouché’s theorem that exactly
r −1 of them lie in the range |z| < 1 and the last, which we denote z0, has modulus strictly greater
than 1. We now turn our attention to the numerator (1 −zr) r−1
j=0 Pjz j. Unlike the numerator of
P(z) in the M/Er/1 queue, which contains only one unknown probability P0, the numerator for
the Er/M/1 queue contains r unknown probabilities Pj, j = 0, 1, . . . ,r −1. To overcome this
difﬁculty, we use the fact that the z-transform of a probability distribution must be analytic in the

498
The z-Transform Approach to Solving Markovian Queues
range |z| < 1, i.e.,
P(z) =
∞

j=0
Pjz j < ∞for |z| < 1.
Since we have just seen that the denominator has r −1 zeros in this range, the numerator must also
have zeros at exactly the same r −1 points, for otherwise P(z) blows up. Observe that the numerator
consists of two factors. All of the zeros of the ﬁrst factor, (1−zr), have absolute value equal to unity.
None of these can compensate for the r roots of the denominator that lie in the range |z| < 1. The
second factor, a summation, is a polynomial of degree r −1 and therefore has r −1 zeros. Therefore
the “compensating” zeros in the numerator must come from its summation. This means that once
the roots z = 1 and z = z0 are factored out of the denominator, we can set the result equal to some
constant K times the summation in the numerator. This gives
rρzr+1 −(1 + rρ)zr + 1
(1 −z)(1 −z/z0)
= K
r−1

j=0
Pjz j
and allows us to conclude that
P(z) =
(1 −zr)
K(1 −z)(1 −z/z0).
The constant K can be found by using the fact that P(1) = 1:
1 = lim
z→1 P(z) = lim
z→1
1 −zr
K(1 −z)(1 −z/z0).
To continue, we need to use L’Hôpital’s rule to get
1 = lim
z→1
−rzr−1
K[−(1 −z/z0) −(1/z0)(1 −z)] =
−r
−K[1 −1/z0]
and hence
K =
r
(1 −1/z0)
and
P(z) = (1 −zr)(1 −1/z0)/r
(1 −z)(1 −z/z0)
.
This then is the generating function for the number of phases in the Er/M/1 queue. Taking r = 1
should yield the same results as previously for the M/M/1 queue. Performing this substitution gives
P(z) = (1 −z)(1 −1/z0)
(1 −z)(1 −z/z0) = (1 −1/z0)
(1 −z/z0).
To compute z0 we need to ﬁnd the second root of the quadratic
ρz2 −(1 + ρ)z + 1 = 0.
These are z = 1 and z0 = 1/ρ and thus
P(z) = 1 −ρ
1 −ρz
as before.

13.3 Solving Markovian Queues using z-Transforms
499
To invert the transform in the case when r > 1, we must ﬁrst write it in partial fraction form.
Since the degree of the numerator is greater than the degree of the denominator, we must write the
transform as
P(z) = (1 −zr)
&
(1 −1/z0)
r(1 −z)(1 −z/z0)
'
and now compute the partial fraction expansion of the part within the brackets. This gives
P(z) = (1 −zr)
& 1/r
1 −z −1/(rz0)
1 −z/z0
'
.
The factor (1 −zr) implies that if f j is the inverse transform of the terms within the brackets, the
inverse transform of P(z) itself is given by f j −f j−r, i.e., Pj = f j −f j−r. Using the transform
relationship
Aαn
⇐⇒
A
1 −αz ,
we can invert the terms within the brackets and get
1/r
1 −z −1/(rz0)
1 −z/z0
⇐⇒
f j = 1
r 1 j −1
rz0
 1
z0
 j
.
Thus
f j = 1
r (1 −z−j−1
0
),
j ≥0,
and
f j = 0,
j < 0.
It follows then that
Pj = 1
r (1 −z−j−1
0
) −1
r (1 −z−j+r−1
0
) = 1
r −1
r z−j−1
0
−1
r + 1
r z−j+r−1
0
,
i.e.,
Pj = 1
r zr−j−1
0
(1 −z−r
0 ) for j ≥r.
Notice that, since z0 is a root of D(z) = rρzr+1 −(1 + rρ)zr + 1,
rρzr+1 −(1 + rρ)zr + 1
z=z0 = 0,
rρz0 −(1 + rρ) + z−r
0
= 0,
which yields
rρ(z0 −1) = 1 −z−r
0 ,
and therefore
Pj = 1
r zr−j−1
0
(1 −z−r
0 ) = 1
r zr−j−1
0
rρ(z0 −1) = ρ(z0 −1)zr−j−1
0
,
j ≥r.
Also, for 0 ≤j < r, we have f j−r = 0. To conclude then, we have
Pj =
⎧
⎨
⎩

1 −z−j−1
0

/r,
0 ≤j < r,
ρ(z0 −1)zr−j−1
0
,
j ≥r.

500
The z-Transform Approach to Solving Markovian Queues
To ﬁnd the distribution of the number of customers present, we use our earlier relation between
pk and Pj. We proceed ﬁrst for the case when k > 0. For k > 0,
pk =
r(k+1)−1

j=rk
Pj
=
ρ
r(k+1)−1

j=rk
(z0 −1)zr−j−1
0
= ρ(z0 −1)zr−1
0
r(k+1)−1

j=rk
 1
z0
 j
= ρ(z0 −1)zr−1
0
(
(1/z0)rk −(1/z0)r(k+1)
1 −1/z0
)
= ρ(z0 −1)zr
0
(
(1/z0)rk −(1/z0)r(k+1)
z0 −1
)
= ρzr
0

z−rk
0
(1 −z−r
0 )
 
,
which yields the result:
pk = ρ(zr
0 −1)z−rk
0
for k > 0.
For a single server, we know that p0 = 1 −ρ. We wish to verify that r−1
j=0 Pj gives this result.
p0 =
r−1

j=0
Pj =
r−1

j=0
1
r

1 −z−j−1
0

= 1 −1
r
r−1

j=0
z−j−1
0
= 1 −1
rz0
r−1

j=0
 1
z0
 j
= 1 −1
rz0
1 −(1/z0)r
1 −1/z0

,
i.e.,
p0 = 1 −1 −(1/z0)r
r(z0 −1) .
Thus our task is to show that
ρ = 1 −(1/z0)r
r(z0 −1) .
This equation may be written as
r(z0 −1)ρ = 1 −(1/z0)r,
i.e.,
rρzr+1
0
−rρzr
0 = zr
0 −1
or
rρzr+1
0
−(1 + rρ)zr
0 + 1 = 0.
This equation must be true, since z0 is a root of
rρzr+1 −(1 + rρ)zr + 1 = 0,
the denominator of P(z).

13.3 Solving Markovian Queues using z-Transforms
501
0
1
2
3
4
5
λ
λ
λ
λ
λ
λ
λ
λ
λ
3
3
3
3
3
3
μ
μ
μ
μ
μ
μ
3
3
3
μ
μ
j
j−1
Figure 13.6. State transition diagram for the E3/M/1 queue.
Example 13.8 The Erlang-3 arrival model.
Let us consider an Er/M/1 queue with parameters λ = 1.0, μ = 1.5, and r = 3 (Figure 13.6).
We would like to compute the probability of having k customers in this system and to plot this
information graphically. As before, let k denote the number of customers and j denote the number
of arrival phases in the system. Also
pk = Prob{Number of customers in system = k}
and
Pj = Prob{Number of phases in system = j}.
It follows that
pk =
r(k+1)−1

j=rk
Pj =
3(k+1)−1

j=3k
Pj = P3k + P3k+1 + P3k+2,
k = 0, 1, 2, . . . .
The balance equations, which are obtained by inspection from the state transition diagram, are
given by
3λPj = μPj+3,
j = 0,
3λPj = 3λPj−1 + μPj+3,
j = 1, 2,
(3λ + μ)Pj = 3λPj−1 + μPj+3,
j ≥3.
To prepare to generate the z-transform, we multiply the jth equation by z j and obtain
3λP0 = μP3,
3λP1z = 3λP0z + μP4z,
3λP2z2 = 3λP1z2 + μP5z2,
(3λ + μ)Pjz j = 3λPj−1z j + μPj+3z j,
j ≥3.
Summing both sides over all permissible values of j gives
3λ ∞
j=0 Pjz j + μ ∞
j=3 Pjz j = 3λ ∞
j=1 Pj−1z j + μ ∞
j=0 Pj+3z j,
i.e.,
3λP(z) + μP(z) −μ
#
P0 + P1z + P2z2$
= 3λzP(z) + μ
z3
∞

j=0
Pj+3z j+3
= 3λzP(z) + μ
z3 P(z) −μ
z3
#
P0 + P1z + P2z2$
P(z)

3λ + μ −3λz −μ
z3

= μ
#
P0 + P1z + P2z2$
−μ
z3
#
P0 + P1z + P2z2$
,

502
The z-Transform Approach to Solving Markovian Queues
and we ﬁnd
P(z) =
#
μ −μ/z3$ #
P0 + P1z + P2z2$
3λ(1 −z) + μ(1 −1/z3)
= (z3 −1)
#
P0 + P1z + P2z2$
3ρz3(1 −z) + (z3 −1)
= (1 −z3)
#
P0 + P1z + P2z2$
3ρz4 −(1 + 3ρ)z3 + 1
,
which is identical to Equation (13.8) with r = 3. Since (z −1) is a factor in both numerator and
denominator, we may divide through by (z −1) and obtain
P(z) = (1 + z + z2)(P0 + P1z + P2z2)
1 + z + z2 −3ρz3
.
From P(1) = 1, we obtain
1 + 1 + 1 −3 × 2/3 = (1 + 1 + 1)(P0 + P1 + P2) =⇒
P0 + P1 + P2 = 1/3.
Notice that
p0 = P0 + P1 + P2 = 1/3 = 1 −λ/μ = 1 −ρ.
The roots of the denominator are
z0 = 1.2338,
z1,2 = −.36688 ± .52026i,
and only the ﬁrst of these (the only root that is greater than 1) is of interest to us. We may then write
the ﬁnal result for the probability distribution of customers in the E3/M/1 queue:
p0 = 1 −ρ = 1/3,
pk = ρ(z3
0 −1)z−3k
0
=
2
3
 1.23383 −1
1.23383k

,
k = 1, 2, . . . .
The following Matlab code computes and plots (Figure 13.7) the distribution of customers in the
E3/M/1 queue.
Function E3arrival()
poly=[2 -3 0 0 1];
z = roots(poly); rho = 1/1.5;
p0 = 1 - rho;
z0 = z(1);
for k=1:10
p(k) = (z0^3-1)/(1.5*(z0^(3*k)));
end
k = 0:10;
Prob=[p0 p]’
plot(k,Prob)
title(’Probability Distribution for Erlang-3 Arrival Model’)
xlabel(’Number of Customers’)
ylabel(’Probability’)

13.3 Solving Markovian Queues using z-Transforms
503
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Probability Distribution for Erlang−3 Arrival Model
Number of Customers
Probability
Figure 13.7. Probability distribution for Erlang arrival model as plotted by Matlab.
The roots of the denominator polynomial as computed by Matlab are
z0 = 1.2338,
z1 = 1.0000,
z2 = −0.3669 + 0.5203i,
z3 = −0.3669 −0.5203i.
The initial entries of the computed probability distribution are
p0 = 0.33333333333333,
p1 = 0.31166923803768,
p2 = 0.16596266712920,
p3 = 0.08837448011891,
p4 = 0.04705906979796,
p5 = 0.02505877315792,
p6 = 0.01334370005349,
p7 = 0.00710546881108,
p8 = 0.00378363473570,
p9 = 0.00201477090307.
13.3.6 Bulk Queueing Systems
There is a connection between queueing systems in which multiple customers may arrive simulta-
neously (as a bulk) and the M/Er/1 queue. In the M/Er/1 queue each customer must pass through

504
The z-Transform Approach to Solving Markovian Queues
r phases of service to complete its total service. When we analyzed this queue, we counted the
number of service phases each customer contributed upon arrival and used the total number of
phases present to deﬁne a state of the system. Now look at this another way. Consider each customer
arrival to be in reality the arrival of r customers, each of these r customers requiring only a single
phase of service. These two points of view deﬁne identical systems: the M/Er/1 queue and the
M/M/1 queue with bulk arrivals. In this case, the bulk size is ﬁxed at r. Similarly, the Er/M/1 queue
may be viewed as a queue with bulk service! The single server takes r customers from the queue
and gives exponential service to the bulk. All r leave at the same instant. If there are fewer than
r customers, the server waits until there are r in the queue. In the Er/M/1 queue, service completion
is equivalent to losing r service phases.
Let us now consider a more general case and permit bulks of arbitrary sizes to arrive at each
(Poisson) arrival instant. We take the arrival rate of bulks to be λ and the probability mass function
of bulk size to be
gi ≡Prob{Bulk size is i}.
A state of the system is completely characterized by the number of customers present. Since we
have not imposed any limit to the size of bulks, transitions out of any state k can occur to any state
k + i, i = 1, 2, . . . , at rate λgi and so the net exit rate from state k due to arrivals is
λg1 + λg2 + · · · = λ
∞

i=1
gi = λ.
Also from any state k > 0 a transition can occur due to a service completion at rate μ and move the
system to state k −1, and so the overall departure rate from state k > 0 is λ + μ.
State k may be entered from any state k −i, i = 1, 2, . . . , k, at rate λgi, since the arrival of
a bulk of size i in state k −i will move the system to state k. A transition is also possible from
state k + 1, due to a service completion at rate μ, to state k. The equilibrium equations are therefore
given by
(λ + μ)pk = μpk+1 +
k−1

i=0
piλgk−i,
k ≥1,
with
λp0 = μp1,
and we may solve them by the use of z-transforms,
(λ + μ)
∞

k=1
pkzk = μ
z
∞

k=1
pk+1zk+1 +
∞

k=1
k−1

i=0
piλgk−izk.
(13.9)
Figure 13.8 shows that the double summation may be rewritten as
∞

k=1
k−1

i=0
=
∞

i=0
∞

k=i+1
.
All the terms to be included are at the intersection of the dashed and dotted lines in this ﬁgure, and
it is immaterial whether these are traversed vertically, following the dotted lines as in the original
order, or horizontally, following the dashed lines.

13.3 Solving Markovian Queues using z-Transforms
505
0
1
0
1
2
3
4
2
3
4
5
k
k=i+1
 i
Figure 13.8. Double summations horizontally and vertically.
This yields
∞

k=1
k−1

i=0
piλgk−izk = λ
∞

i=0
pizi
∞

k=i+1
gk−izk−i
= λ
∞

i=0
pizi
∞

j=1
g jz j,
and Equation (13.9) can be written as
(λ + μ)
∞

k=1
pkzk = μ
z
∞

k=1
pk+1zk+1 + λ
∞

i=0
pizi
∞

j=1
g jz j.
Let G(z) be the z-transform of the distribution of bulk size:
G(z) ≡
∞

k=1
gkzk.
Then
(λ + μ)[P(z) −p0] = μ
z

P(z) −p0 −p1z
 
+ λP(z)G(z).
Using λp0 = μp1 and simplifying, we obtain
P(z) =
μp0(1 −z)
μ(1 −z) −λz(1 −G(z)).
To eliminate p0, we would like to use the fact that P(1) = 1, but since unity is a root of both the
numerator and the denominator, we must have recourse to L’Hôpital’s rule:
1 = lim
z→1
−μp0
λG(z) + λzG′(z) −λ −μ
=
−μp0
λ + λG′(1) −λ −μ =
μp0
μ −λG′(1).
Therefore
p0 = μ −λG′(1)
μ
= 1 −λG′(1)
μ
.
Observe that λG′(1)/μ is just the utilization factor, ρ, for this queue, is the quotient of the average
effective arrival rate and the average service rate (G′(1) is the average bulk size). Hence p0 = 1−ρ.


13.4 Exercises
507
Exercise 13.1.3 Prove the following relations, taken from Table 13.1:
Sequence
Transform
pk ⋆qk
⇐⇒
P(z)Q(z)
pk+1
⇐⇒
[P(z) −p0]/z
pk−1
⇐⇒
zP(z)
uk−i
⇐⇒
zi
{0, α, 2α2, 3α3, . . . } = kαk
⇐⇒
αz/(1 −αz)2
{1, 2, 3, . . . } = k + 1
⇐⇒
1/(1 −z)2
Exercise 13.1.4 Show that the z-transform of the sequence pk = −2uk −2uk−1 + 3, where uk is the unit
function, is given by
P(z) = 1 + 2z2
1 −z .
Exercise 13.1.5 Consider the inﬁnite sequence whose kth term is given by pk = (1 −ρ)ρk, k = 0, 1, . . . , with
0 < ρ < 1. Find the z-transform of this sequence and show that the kth term in the sequence is recovered when
the kth derivative of the transform is formed and evaluated at z = 0.
Exercise 13.1.6 The z-transform of a probability sequence is given by
P(z) = 1 + z4
10
+ z + z2 + z3
5
.
Find the sequence
(a) by writing the transform as a power series and comparing coefﬁcients;
(b) by differentiation and evaluation of the transform.
Exercise 13.2.1 Find the sequence whose z-transform is given by
P(z) =
1 + 3z
2 −z −z2 .
Exercise 13.2.2 Find the sequence whose z-transform is given by
P(z) =
14 −27z
24 −84z + 36z2 .
Exercise 13.2.3 Find the sequence whose z-transform is given below. Observe that unity is one of the roots of
the denominator.
P(z) = 30 −57z + 9z2
40 −42z + 2z3 .
Exercise 13.2.4 Find the sequence whose z-transform is given by
P(z) =
48z −18z2
40 −10z −5z2 .

508
The z-Transform Approach to Solving Markovian Queues
Exercise 13.2.5 Find the sequence whose z-transform is given by
P(z) = 1 + z2
1 −2z .
Exercise 13.2.6 Find the sequence whose z-transform is given by
P(z) = 2z −1/2
(1 −2z)2 .
Exercise 13.2.7 Find the sequence whose z-transform is given by
P(z) =
−1 + 62z + 49z2 −60z3
4 + 24z + 4z2 −96z3 + 64z4 .
Hint: z1 = 1 is a double root of the denominator.
Exercise 13.3.1 For the queuing system with arrivals in pairs discussed in Section 13.3.3, what are the block
matrices needed to apply the matrix-geometric method? Apply the Matlab code of Chapter 12 to ﬁnd the
probabilities pi, i = 0, 1, 2, and 3.
Exercise 13.3.2
Consider a single-server queueing system with bulk arrivals and for which the probability
density of bulk sizes is
g1 = .5, g2 = .25, g3 = 0, g4 = .25, gk = 0, k > 4.
(a) Derive the z-transform P(z) of this system
• by identifying this system with an M/M/1 queue with bulk arrivals;
• by writing down the equilibrium probabilities and ﬁnding the z-transform from ﬁrst principles.
(b) Use the z-transform to compute the mean number of customers present at steady state.
(c) Using the values λ = 1 and μ = 4, invert the transform to ﬁnd the distribution of customers in this
queueing system.

Chapter 14
The M/G/1 and G/M/1 Queues
In this chapter we examine two important single-server queues, the M/G/1 queue and its dual, the
G/M/1 queue. Our concern is with the stationary behavior of these queues, and it turns out that
this can be obtained rather conveniently by constructing and then solving a Markov chain that is
embedded at certain well-speciﬁed instants of time within the stochastic processes that deﬁne the
systems. For the M/G/1 queue, we choose the instants at which customers depart and it turns out that
the solution at these instants is also the stationary solution we seek. For the G/M/1 queue, we choose
arrival instants which allows us to ﬁnd the distribution of customers at arrival instants. Although this
solution is not equal to the distribution as seen by an arbitrary observer of the system (which is what
we seek), it does open the way for the computation of the stationary distribution. Unfortunately, the
embedded Markov chain approach does not apply to the G/G/1 queue and more advanced methods,
such as the use of the Lindsley integral equation, must be used. Since this is beyond the level of
this text, we do not consider the G/G/1 queue further except to say that one possibility might be to
model the general arrival and service time distributions by phase-type distributions and to apply the
matrix geometric methods of Chapter 12.
14.1 Introduction to the M/G/1 Queue
The M/G/1 queue is a single-server queue, illustrated graphically in Figure 14.1.
μ
λ
General
service time
distribution
Poisson arrival
process
Figure 14.1. The M/G/1 queue.
The arrival process is Poisson with rate λ, i.e., its distribution function is
A(t) = 1 −e−λt,
t ≥0.
The service times of customers are independent and identically distributed and obey an unspeciﬁed
arbitrary or general distribution function. In particular, the remaining service time may no longer be
independent of the service already received. As usual, the mean service rate is denoted by μ. We
denote the service time distribution function as
B(x) = Prob{S ≤x},
where S is the random variable “service time”; its density function, denoted b(x), is given by
b(x)dx = Prob{x < S ≤x + dx}.

510
The M/G/1 and G/M/1 Queues
We shall take the scheduling discipline to be FCFS although the results derived in this chapter
frequently apply to many other nonpreemptive scheduling policies. As a special case of the M/G/1
queue, if we let B(x) be the exponential distribution with parameter μ, we obtain the M/M/1
queueing system. If the service times are assumed to be constant, then we have the M/D/1 queueing
system.
Recall that with the M/M/1 queue, all that is required to summarize its entire past history is a
speciﬁcation of the number of customers present, N(t), and, in this case, the stochastic process
{N(t), t ≥0} is a Markov process. For the M/G/1 queue, the stochastic process {N(t), t ≥0} is not
a Markov process since, when N(t) ≥1, a customer is in service and the time already spent by that
customer in service must be taken into account—a result of the fact that the service process need
not possess the memoryless property of the exponential distribution.
Let us examine this time dependence of service a little more closely. Consider the conditional
probability C(x) that the service ﬁnishes before x + dx knowing that its duration is greater than x,
C(x) = Prob{S ≤x + dx|S > x} = Prob{x < S ≤x + dx}
Prob{S > x}
=
b(x)dx
1 −B(x).
Generally C(x) depends on x. However, if B(x) is the exponential distribution, B(x) = 1 −e−μx,
b(x) = μe−μx, and
C(x) =
μe−μxdx
1 −1 + e−μx = μdx,
which is independent of x. In this particular case, if we start to observe the service in progress at
an arbitrary time x, the probability that the service completes on the interval (x, x + dx] does not
depend on x, the duration of service already received by the customer. However, for the M/G/1
queue, where C(x) generally depends on x, the process {N(t), t ≥0} is not Markovian and if we
start to observe the process at an arbitrary time x, the probability that the service completes on the
interval (x, x +dx], which implies a transition of the process N(t), is not independent of x, the time
already spent in service for the customer currently in service. In other words, the probability that a
transition will occur depends on its past history, and therefore the process is non-Markovian. What
this means is that, if at some time t we want to summarize the complete relevent past history of an
M/G/1 queue, we must specify both
1.
N(t), the number of customers present at time t, and
2.
S0(t), the service time already spent by the customer in service at time t.
Notice that, while N(t) is not Markovian, [N(t), S0(t)] is a Markov process, since it provides all the
past history necessary for describing the future evolution of an M/G/1 queue. The component S0(t) is
called a supplementary variable and the approach of using this state description to solve the M/G/1
queue is called the method of supplementary variables. We shall not dwell on this approach, which
involves working with two components, the ﬁrst discrete and the second continuous, but rather we
shall seek an alternative solution based on a single discrete component: the embedded Markov chain
approach.
14.2 Solution via an Embedded Markov Chain
As its name suggests, in the embedded Markov chain approach, we look for a Markov chain within
(i.e., at certain time instants within) the stochastic process [N(t), S0(t)] and solve for the distribution
of customers at these times. One convenient set of time instants is the set of departure instants. These
are the times at which a customer is observed to terminate service and leave the queueing system.
At precisely these instants, the customer entering service has received exactly zero seconds of
service. At these instants we know both N(t) and S0(t), the latter being equal to zero. It follows that

14.2 Solution via an Embedded Markov Chain
511
this embedded Markov chain approach allows us to replace the two-dimensional state description
[N(t), S0(t)] with a one-dimensional description Nk, where Nk denotes the number of customers
left behind by the kth departing customer. Of course, when we solve the embedded Markov chain,
we obtain the distribution of customers at departure instants. Conveniently, however, for the M/G/1
queue, it turns out that the distribution of customers at departure instants is also identical to the
distribution of customers at any time instant.
Let us now examine the distribution of time between customer departures. When a customer
departs and leaves at least one customer behind, the time until the next departure is the same as the
distribution of the service time. On the other hand, when a customer departs and leaves behind
an empty system, the next departure will not occur until after a new customer arrives and has
been served. In this case the distribution of time between departures is equal to the convolution
of the exponential interarrival time distribution and the general service time distribution. Thus the
system that we have just described is actually a semi-Markov process, since the time between state
transitions (departure instants) obeys an arbitrary probability distribution whereas in a discrete-time
Markov chain the time between state transitions must be geometrically distributed. As we shall see,
we need only be concerned with the Markov chain embedded within this semi-Markov process and
not with the semi-Markov process in its entirety.
At this point is is worthwhile digressing a little to discuss the concept of regeneration points,
or renewal instants—see also Section 9.11 and the discussion on renewal processes. As the name
suggests, a regeneration point is an instant in time at which the future behavior of a system depends
only on its state at that instant. It is completely independent of the evolutionary path that led to
its current state. The system is reborn. For example, each departure instant in an M/G/1 queue is
a regeneration point. Let {X(t), t ≥0} be an arbitrary stochastic process and let t0 be an arbitrary
instant. Generally we do not have
Prob{X(t)|X(t0)} = Prob{X(t)|X(s), s ≤t0}.
If this relation is true for all t > t0, then t0 is a regeneration point. Notice that a stochastic process
{X(t), t ≥0} is a Markov process if and only if every instant t0 is a regeneration point. Furthermore,
if there exists a sequence {tk}k=1,2,... of regeneration points, then {X(tk), k = 1, 2, . . .} ≡{Xk, k =
1, 2, . . .} is a Markov chain. It is the Markov chain embedded in the stochastic process {X(t), t ≥0},
since, from the deﬁnition of regeneration points,
Prob{Xk+l|Xk} = Prob{Xk+l|Xk, Xk−1, . . .}
for all k.
Consider now the stochastic process {N(t), t ≥0} associated with the M/G/1 queue and the
sequence {tk}k=1,2,... where tk is deﬁned as the instant of the end of service for the kth customer.
Let Nk ≡N(tk)k=1,2,... be the number of customers in the system just after the departure of the kth
customer to be served. Given Nk, the value of Nk+l depends just on the number of arrivals between
tk and tk+l, since the number of departures is exactly equal to l. However, the arrival process in
the M/G/1 queue is Poisson which means that the number of arrivals between tk and tk+l does not
depend on the past, i.e., on Nk−j, j = 1, 2, . . . . Therefore
Prob{Nk+l|Nk} = Prob{Nk+l|Nk, Nk−1, . . .}
and so {Nk, k = 1, 2, . . .} is a Markov chain. It is a Markov chain embedded within the stochastic
process {N(t), t ≥0}.
It only remains to show that the solution obtained at departure instants is also the solution at any
point of time, and we do so in two steps. First, we have previously deﬁned an to be the probability
that an arriving customer ﬁnds n customers already present. Now let dn be the probability that a
departing customer leaves n customers behind. Since the overall arrival rate is equal to λ, the rate
of transition from state n to state n + 1—a transition occasioned by an arrival ﬁnding n customers
present—is given by λ an. Furthermore, at equilibrium, the departure rate is also equal to λ and so

512
The M/G/1 and G/M/1 Queues
the rate of transition from state n + 1 to state n—a transition occasioned by a departure leaving
behind n customers—is equal to λ dn. At equilibrium, the rate of transition from state n to n + 1
is equal to the rate of transition from state n + 1 to n and thus it follows that λ an = λ dn, i.e.,
that an = dn. This result applies more generally: the limiting distribution of customers found by
a new arrival in any system that changes states by increments of plus or minus one is the same as
the limiting distribution of customers left behind by a departure, assuming that these limits actually
exist. In the M/G/1 queue this condition holds since customers arrive and depart individually and the
limits exist when the mean interarrival time is strictly greater than the mean service time (λ/μ < 1).
Therefore the solution at departure instants in an M/G/1 queue is also the solution at arrival instants:
in other words, customers arriving to the system see the same customer population distribution as
that seen by customers departing the system.
Second, due to the Poisson nature of arrivals to the M/G/1 queue, we know from the PASTA
(Poisson arrivals see time averages) property that an arriving customer sees the same distribution as
a random observer. Therefore, in the M/G/1 queue, the distribution of customers at arrival instants is
also the same as the distribution of customers at any time instant. Taking these two facts together, we
may conclude that the solution obtained at departure instants in an M/G/1 queue is also the solution
at any arbitrary instant of time.
Let Ak be the random variable that denotes the number of customers that arrive during the service
time of the kth customer. We now derive a relationship for the number of customers left behind by
the (k + 1)th customer in terms of the number left behind by its predecessor and Ak+1. First, if
Nk = i > 0, we must have
Nk+1 = Nk −1 + Ak+1,
since there are Nk present in the system when the (k + 1)th customer enters service, an additional
Ak+1 arrive while this customer is being served, and the number in the system is reduced by 1 when
this customer ﬁnally exits. Similarly, if Nk = 0 we must have
Nk+1 = Ak+1.
We may combine these into a single equation with the use of the function δ(Nk) deﬁned as
δ(Nk) =

1,
Nk > 0,
0,
Nk = 0.
Since
Nk+1 = Nk −1 + Ak+1,
Nk > 0,
Nk+1 = Ak+1,
Nk = 0,
we have
Nk+1 = Nk −δ(Nk) + A.
(14.1)
Recall that Nk is the number of customers the kth customer leaves behind, and Ak is the number
of customers who arrive during the service time of the kth customer. Since the service times are
independent and identically distributed and independent of the interarrival times, which are also
independent and identically distributed, it follows that the random variables Ak are independent and
identically distributed. Thus Ak must be independent of k, which allows us to write it simply as A
in Equation (14.1).
We shall now ﬁnd the stochastic transition probability matrix for the embedded Markov chain
{Nk, k = 1, 2, . . .}. The i jth element of this matrix is given by
fi j(k) = Prob{Nk+1 = j|Nk = i},
which in words says that the probability the (k +1)th departing customer leaves behind j customers,
given that the kth departure leaves behind i, is fi j(k). As might be expected, and as we shall see

14.2 Solution via an Embedded Markov Chain
513
momentarily, this is independent of k, and so we write the transition probability matrix simply as
F. Let p be the stationary distribution of this Markov chain:
pF = p.
The jth component of p gives the stationary probability of state j, i.e., the probability that a
departing customer leaves j customers behind. It provides us with the probability distribution of
customers at departure instants. The single-step transition probability matrix is given by
F =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
α0
α1
α2
α3
α4
· · ·
α0
α1
α2
α3
α4
· · ·
0
α0
α1
α2
α3
· · ·
0
0
α0
α1
α2
· · ·
...
...
...
...
...
...
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(14.2)
where αi is the probability that i arrivals occur during an arbitrary service. Since a departure cannot
remove more than one customer, all elements in the matrix F that lie below the subdiagonal must
be zero. These are the elements fi j(k) for which i > j + 1. Also, all elements that lie above the
diagonal are strictly positive, since any number of customers can arrive during the service of the kth
customer. Observe that it is possible to reach all states from any given state, since αi > 0 for all
i ≥0, and so the Markov chain is irreducible. If i ≤j, we can go from state i to state j in a single
step; if i > j, we need (i −j) steps to go from state i to state j. Furthermore, since the diagonal
elements are nonzero, the Markov chain is aperiodic.
Given that the random variables Ak are independent and identically distributed, and assuming
that the kth departing customer leaves at least one customer behind (Nk = i > 0), we have
Prob{Nk+1 = j|Nk = i} ≡fi j(k) = α j−i+1
for j = i −1, i, i + 1, i + 2, . . . .
In words, if there are i > 0 customers at the start and one departs, then we need j −i + 1 arrivals
to end up with a total of j, since i −1 + ( j −i + 1) = j. On the other hand, in the case of the kth
departure leaving behind an empty system (Nk = i = 0), we have
Prob{Nk+1 = j|Nk = 0} ≡f0 j(k) = α j
for j = 0, 1, 2, . . . .
Thus the transition probabilities fi j(k) do not depend on k, which means that the Markov chain
{Nk, k = 1, 2, . . .} is homogeneous.
Example 14.1 Keeping in mind the fact that the indices i and j range from an initial value of 0
and not 1, we have the following examples: The jth component of row 0 is f0 j = α j. Thus the
fourth component in row 0, corresponding to j = 3, is equal to α3 and gives the probability that the
previous customer left behind an empty system (i = 0) and that during the service of the customer
who arrived to this empty system, exactly j (i.e., 3) customers arrived.
Example 14.2 Now consider the ﬁfth component in the third row of the matrix F which is equal to
α3. This element is the probability
f24 = Prob{Nk+1 = 4|Nk = 2} = α4−2+1 = α3.
For customer k + 1 to leave behind four customers, given that the previous customer left behind
only two, three new customers must arrive during the time that customer k + 1 is being served.
We now need to calculate the probabilities αi, i = 0, 1, 2, . . . . From our deﬁnition of S as the
random variable “service time” and b(x) the probability density function of this random variable,
we have
Prob{A = i and x < S ≤x + dx} = Prob{A = i | x < S ≤x + dx}Prob{x < S ≤x + dx}.

514
The M/G/1 and G/M/1 Queues
The ﬁrst term in this product is Poisson and equal to ((λx)i/i!)e−λx; the second is equal to b(x)dx.
Therefore
αi = Prob{A = i} =
 ∞
0
(λx)i
i!
e−λxb(x)dx.
(14.3)
To accommodate all types of service time distributions (i.e., discrete, continuous, and mixed), the
Stieltjes integral form should be used and b(x)dx replaced with d B(x) in Equation (14.3). This then
completely speciﬁes the transition probability matrix F.
If the αi’s of Equation (14.3) can be conveniently computed, then the upper Hessenberg structure
of F facilitates the computation of the stationary probability vector of the M/G/1 queue, especially
since we frequently know, a priori, the probability that the system is empty, i.e., p0 = 1 −λ/μ =
1 −ρ. Successively writing out the equations, we have
α0 p0 + α0 p1 = p0,
α1 p0 + α1 p1 + α0 p2 = p1,
α2 p0 + α2 p1 + α1 p2 + α0 p3 = p2,
... .
Rearranging,
p1 = 1 −α0
α0
p0,
p2 = 1 −α1
α0
p1 −α1
α0
p0,
p3 = 1 −α1
α0
p2 −α2
α0
p1 −α2
α0
p0,
...
and, in general,
p j = 1 −α1
α0
p j−1 −α2
α0
p j−2 −· · · −α j−1
α0
p1 −α j−1
α0
p0.
To compute any element p j, we only need to know the values of previous elements pi, i < j,
and given that we know the value of p0 = 1 −ρ, a recursive procedure may be set in motion to
compute all required components of the stationary distribution vector. Furthermore, since successive
elements are monotonic and concave, we will eventually reach a point at which component values
are sufﬁciently small as to be negligible. However, from a computational standpoint, it is preferable
to stop once the sum of successive elements is sufﬁciently close to one rather than stopping once a
component is reached that is less than some speciﬁed small value.
Example 14.3
Consider an M/D/1 queue for which λ = 1/2 and μ = 1. The elements of the
transition probability matrix can be found from
αi = 0.5i
i! e−0.5.

14.3 Performance Measures for the M/G/1 Queue
515
This yields the following values:
α0 = 0.606531, α1 = 0.303265, α2 = 0.075816, α3 = 0.012636,
α4 = 0.001580, α5 = 0.000158, α6 = 0.000013, α7 = 0.000001.
Using the recursive procedure and beginning with p0 = 0.5, we obtain the following:
p1 = 0.324361:
1

i=0
pi = 0.824361,
p2 = 0.122600:
2

i=0
pi = 0.946961,
p3 = 0.037788:
3

i=0
pi = 0.984749,
p4 = 0.010909:
4

i=0
pi = 0.995658,
p5 = 0.003107:
5

i=0
pi = 0.998764,
p6 = 0.000884:
6

i=0
pi = 0.999648,
...
...
When the elements αi of the transition probability matrix F can be computed, but the initial
value to begin the recursion, p0 = 1 −ρ, is not known, it is still possible to use the same
solution procedure. It sufﬁces to assign p0 an arbitrary value, say p0 = 1, and to compute all
other components from this value. In this case the process must be continued until a pk+1 (or better
still a sum of consecutive elements i=k+l
i=k+1 pi—the larger the value of l the better) is found which
is so small that it can be taken to be zero. At this point, the computed values of p0 through pk must
be normalized by dividing each by the sum i=k
i=0 pi.
14.3 Performance Measures for the M/G/1 Queue
14.3.1 The Pollaczek-Khintchine Mean Value Formula
Let us return to Equation (14.1), i.e., Nk+1 = Nk −δ(Nk) + A, and use it to obtain the mean value
for Nk. We assume the existence of a steady state and write
lim
k→∞E[Nk+1] = lim
k→∞E[Nk] = L.
Whereas it might be more appropriate to include a superscript (D) on the quantity L so as to denote
departure instants, we now know that the solution at departure instants is also the solution at any
point in time, and so we write it simply as L. We begin by taking the expected value of Equation
(14.1) as k →∞and obtain
L = L −lim
k→∞E[δ(Nk)] + lim
k→∞E[A].

516
The M/G/1 and G/M/1 Queues
This implies that
E[A] = lim
k→∞E[δ(Nk)] = lim
k→∞
∞

i=0
δ(Nk)Prob{Nk = i}
= lim
k→∞
∞

i=1
Prob{Nk = i} = lim
k→∞Prob{Nk > 0}
= lim
k→∞Prob{server is busy}.
The ﬁnal equality holds since the M/G/1 queue has but a single server. Therefore
E[A] = lim
k→∞Prob{server is busy} = ρ,
i.e., the average number of arrivals in a service period is equal to ρ.
While this is interesting, it is not what we are looking for—it does not give us the mean number
of customers in the M/G/1 queue. To proceed further, we square both sides of Equation (14.1) and
obtain
N 2
k+1 = N 2
k + δ(Nk)2 + A2 −2Nkδ(Nk) −2δ(Nk)A + 2Nk A
= N 2
k + δ(Nk) + A2 −2Nk
−2δ(Nk)A + 2Nk A.
Now, taking the expectation of each side, we ﬁnd
E[N 2
k+1] = E[N 2
k ] + E[δ(Nk)] + E[A2] −2E[Nk] −2E[Aδ(Nk)] + 2E[ANk].
Taking the limit as k →∞and letting N = limk→∞Nk gives
0 = E[δ(N)] + E[A2] −2E[N] −2E[Aδ(N)] + 2E[AN]
= ρ + E[A2] −2E[N] −2E[A]E[δ(N)] + 2E[A]E[N]
= ρ + E[A2] −2E[N] −2ρ2 + 2ρE[N],
(14.4)
where we have made use of the relationships E[A] = E[δ(N)] = ρ and the independence of arrivals
and the number of customers at departure instants. Rearranging Equation (14.4), we obtain
E[N](2 −2ρ) = ρ + E[A2] −2ρ2
and hence
L = E[N] = ρ −2ρ2 + E[A2]
2(1 −ρ)
.
It now only remains to ﬁnd E[A2]. A number of approaches may be used to show that E[A2] =
ρ + λ2E[S2] where E[S2] = σ 2
s + E[S]2 is the second moment of the service time distribution and
σ 2
s is the variance of the service time. Exercise 14.3.1 at the end of this chapter guides the reader
through one approach. With this result we ﬁnally obtain
L = ρ −2ρ2 + ρ + λ2E[S2]
2(1 −ρ)
= 2ρ(1 −ρ) + λ2E[S2]
2(1 −ρ)
= ρ + λ2E[S2]
2(1 −ρ).
(14.5)
This result is frequently given in terms of the squared coefﬁcient of variation:
L = ρ + λ2E[S2]
2(1 −ρ) = ρ + λ2 #
σ 2
s + 1/μ2$
2(1 −ρ)
= ρ + ρ2 (C2
s + 1)
2(1 −ρ)
= ρ +
1 + C2
s
2

ρ2
1 −ρ ,

14.3 Performance Measures for the M/G/1 Queue
517
where C2
s = μ2σ 2
s is the squared coefﬁcient of variation. Equation (14.5) is called the Pollaczek-
Khintchine mean value formula: it gives the average number of customers in the M/G/1 queue.
Notice that this average depends only upon the ﬁrst two moments of the service time distribution
and that the mean number in the system increases linearly with the squared coefﬁcient of variation,
C2
s . The expected response time (total time spent in the system) may now be computed using Little’s
formula, L = λW. Thus
W = 1
μ + λE[S2]
2(1 −ρ) = 1
μ + λ[(1/μ)2 + σ 2
s ]
2(1 −λ/μ)
.
Given W and L, we may also obtain Wq and Lq from W = Wq +1/μ and Lq = L −ρ, respectively.
We have
Wq = λ[(1/μ)2 + σ 2
s ]
2(1 −λ/μ)
= λE[S2]
2(1 −ρ)
and
Lq = λ2E[S2]
2(1 −ρ).
The above equations for W, Wq, and Lq are also referred to as Pollaczek-Khintchine mean value
formulae. Comparing these results with those obtained in the M/M/1 context reveals an interesting
relationship. The Pollaczek-Khintchine formulae we have just derived may be written as
L = ρ +
1 + C2
s
2

ρ2
1 −ρ ,
Lq =
1 + C2
s
2

ρ2
1 −ρ ,
W =

1 +
1 + C2
s
2

ρ
1 −ρ

E[S],
Wq =
1 + C2
s
2

ρ
1 −ρ E[S],
where E[S] = 1/μ is the mean service time. Comparing the formulae in this form with the
corresponding M/M/1 formulae given in Equations (11.6) and (11.7), we see that the only difference
is the extra factor (1 + C2
s )/2.
We shall see one more Pollaczek-Khintchine mean value formula, one that concerns the residual
service time. However, before venturing there, we shall derive the Pollaczek-Khintchine transform
equations for the distributions of customers, response time, and queueing time in an M/G/1 queue.
Example 14.4 Consider the application of the Pollaczek-Khintchine mean value formula to the
M/M/1 and M/D/1 queues. For the M/M/1 queue, we have
L = ρ + ρ2 (1 + 1)
2(1 −ρ) =
ρ
1 −ρ ,
while for the M/D/1 queue, we have
L = ρ + ρ2
1
2(1 −ρ) =
ρ
1 −ρ −
ρ2
2(1 −ρ),
which shows that, on average, the M/D/1 queue has ρ2/2(1 −ρ) fewer customers than the M/M/1
queue. As we have already noted in the past, congestion increases with the variance of service time.
Example 14.5
Let us now consider the M/H2/1 queue of Figure 14.2. The arrival rate is λ = 2;
service at phase 1 is exponential with parameter μ1 = 2 and exponential at phase 2 with parameter
μ2 = 3. Phase 1 is chosen with probability α = 0.25, while phase 2 is chosen with probability
1 −α = 0.75.
The probability density function of the service time is given by
b(x) = 1
4(2)e−2x + 3
4(3)e−3x,
x ≥0.

518
The M/G/1 and G/M/1 Queues
2
1
1−α 
α 
μ  
μ  
Poisson arrivals
λ
Server
Figure 14.2. The M/H2/1 queue.
The mean service time ¯x = 1/μ is given by
¯x = 1/4
2
+ 3/4
3
= 3
8,
and the variance σ 2
s is equal to 29/192 since
σ 2
s = x2 −¯x2 = 2
1/4
4
+ 3/4
9

−9
64 = 29
192.
(Recall that the second moment of an r-phase hyperexponential distribution is x2 = 2 r
i=1 αi/μ2
i .)
So,
μ2σ 2
s = C2
s = 64
9 × 29
192 = 29
27.
Therefore,
L = ρ + ρ2(1 + 29/27)
2(1 −ρ)
=
ρ
1 −ρ + (2/27)ρ2
2(1 −ρ) = 3.083333.
14.3.2 The Pollaczek-Khintchine Transform Equations
Distribution of Customers in System
To derive a relationship for the distribution of the number of customers in the M/G/1 queue, we need
to return to the state equations
(p0, p1, p2, . . .) = (p0, p1, p2, . . .)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
α0
α1
α2
α3
α4
· · ·
α0
α1
α2
α3
α4
· · ·
0
α0
α1
α2
α3
· · ·
0
0
α0
α1
α2
· · ·
...
...
...
...
...
...
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where p j is the limiting probability of being in state j. The jth equation is given by
p j = p0α j +
j+1

i=1
piα j−i+1,
j = 0, 1, 2, . . . .
Multiplying each term of this equation by z j and summing over all applicable j, we ﬁnd
∞

j=0
p jz j =
∞

j=0
p0α jz j +
∞

j=0
j+1

i=1
piα j−i+1z j.
(14.6)
Figure 14.3 shows that the double summation ∞
j=0
 j+1
i=1 may be rewritten as ∞
i=1
∞
j=i−1.
All the terms to be included are at the intersection of the dashed and dotted lines in this ﬁgure, and

14.3 Performance Measures for the M/G/1 Queue
519
0
1
2
3
4
j
0
1
2
3
4
5
i = j+1        j = i-1
 i
5
Figure 14.3. Double summations horizontally and vertically.
it is immaterial whether these are traversed vertically, following the dotted lines as in the original
order, or horizontally, following the dashed lines.
Replacing the double summation ∞
j=0
 j+1
i=1 with ∞
i=1
∞
j=i−1 in Equation (14.6) and
identifying the generating function P(z) ≡∞
j=0 p jz j, we obtain
P(z) = p0
∞

j=0
α jz j +
∞

i=1
∞

j=i−1
piα j−i+1z j
= p0
∞

j=0
α jz j +
∞

i=1
∞

k=0
piαkzk+i−1
= p0
∞

j=0
α jz j + 1
z
( ∞

i=1
pizi
∞

k=0
αkzk
)
.
Let the z-transform of the number of arrivals that occur during a service period be denoted by
G A(z), i.e.,
G A(z) ≡
∞

j=0
α jz j.
Then
P(z) = p0G A(z) + 1
z [P(z) −p0]G A(z)
or
P(z) = (z −1)p0G A(z)
z −G A(z)
.
This equation contains two unknowns, p0 and G A(z). We determine p0 ﬁrst. Since
G A(1) =
∞

j=0
α jz j

z=1
=
∞

j=0
α j = 1,
we ﬁnd that limz→1 P(z) has the indeterminate form 0/0, forcing us to apply L’Hôpital’s rule. We
obtain
1 = lim
z→1 P(z) = lim
z→1
&
p0
(z −1)G′
A(z) + G A(z)
1 −G′
A(z)
'
= p0
1
1 −G′
A(1).

520
The M/G/1 and G/M/1 Queues
We now show that G′
A(1) = λ/μ. We have
d
dz G A(1) =
∞

j=0
jα jz j−1

z=1
=
∞

j=0
jα j.
Since α j is the probability of j arrivals during an arbitrary service, it follows that ∞
j=0 jα j is just
equal to the expected number of customers that arrive during a service which in turn is just equal to
λ/μ = ρ. Therefore
1 =
p0
1 −ρ ,
i.e., p0 = 1 −ρ, and hence
P(z) = (1 −ρ)(z −1)G A(z)
z −G A(z)
.
(14.7)
We now turn our attention to the second unknown, namely, G A(z). From Equation (14.3) we have
αi =
 ∞
0
(λx)i
i!
e−λxb(x)dx,
which means that
G A(z) =
∞

j=0
α jz j =
∞

j=0
 ∞
0
(λx) j
j!
z je−λxb(x)dx
=
 ∞
0
e−λx
∞

j=0
(λxz) j
j!
b(x)dx
=
 ∞
0
e−λx(1−z)b(x)dx = B∗[λ(1 −z)],
where B∗[λ(1−z)] is the Laplace transform of the service time distribution evaluated at s = λ(1−z).
(Recall that the Laplace transform of a function f (t) is given by F∗(s) =
 ∞
0−f (t)e−stdt.)
Substituting this into Equation (14.7), we get the Pollaczek-Khintchine transform equation No. 1
(for the distribution of the number of customers in the system):
P(z) = (1 −ρ)(z −1)B∗[λ(1 −z)]
z −B∗[λ(1 −z)]
.
The average number of customers in the system at steady state may be found by taking the derivative
with respect to z and then taking the limit as z →1. The reader may wish to verify that the result
derived in this manner is the same as that given by Equation (14.5).
Example 14.6 The M/M/1 queue.
We have
b(x) = μe−μx
for x > 0 and
B∗(s) =
μ
s + μ.
Therefore
B∗[λ(1 −z)] =
μ
λ(1 −z) + μ
and hence
P(z) =
μ
λ(1 −z) + μ
&
(1 −ρ)(1 −z)
μ/(λ(1 −z) + μ) −z
'
= 1 −ρ
1 −ρz .

14.3 Performance Measures for the M/G/1 Queue
521
We can now take derivatives and evaluate at z = 1. However, in this case, it is easy to invert the
transform, which yields
pk = (1 −ρ)ρk
for k = 0, 1, 2, . . . .
Example 14.7 Let us consider an M/H2/1 queue with the following parameters: λ = 1, α =
0.25, μ1 = 7/2, and μ2 = 9/2. These values, which might appear random, actually allow us to
obtain the inverse of the Pollaczek-Khintchine transform equation quite easily. We have
b(x) = 1
4(7/2)e−7x/2 + 3
4(9/2)e−9x/2,
x ≥0,
B∗(s) =
1
4

7/2
s + 7/2 +
3
4

9/2
s + 9/2 =
17s + 63
4(s + 9/2)(s + 7/2).
Since s = λ(1 −z) = 1 −z,
B∗(1 −z) =
63 + 17(1 −z)
4(11/2 −z)(9/2 −z)
and hence
P(z) =
(1 −ρ)(1 −z)[63 + 17(1 −z)]
63 + 17(1 −z) −4(11/2 −z)(9/2 −z)z
= (1 −ρ)(1 −z)[63 + 17(1 −z)]
80 −116z + 40z2 −4z3
= (1 −ρ)(1 −z)[80 −17z]
4(1 −z)(4 −z)(5 −z)
= (1 −ρ)[1 −(17/80)z)]
(1 −z/5)(1 −z/4)
.
Factorizing the cubic in the denominator on the second line is facilitated by the fact that we know
that z = 1 must be one of the roots. Expanding into partial fractions we obtain
P(z) = (1 −ρ)
&
1/4
1 −z/5 +
3/4
1 −z/4
'
,
which may be inverted by inspection to yield
pk = (1 −ρ)
(
1
4
1
5
k
+ 3
4
1
4
k)
for k = 0, 1, 2, . . . .
Response Time Distributions
Let R be the random variable that describes the response time of a customer and let w(x) be its
probability density function. Then the probability that i arrivals occur during the response time of a
customer is equal to
pi =
 ∞
0
(λx)i
i!
e−λxw(x)dx.
The derivation of this equation is identical to that used to derive Equation (14.3). Multiplying pi by
zi and summing over all i, we obtain
P(z) =
∞

i=0
pizi =
∞

i=0
 ∞
0
(λxz)i
i!
e−λxw(x)dx =
 ∞
0
e−λx(1−z)w(x)dx = W ∗[λ(1 −z)],
(14.8)
where W ∗is the Laplace transform of customer response time evaluated at s = λ(1 −z).

522
The M/G/1 and G/M/1 Queues
It is interesting to derive Equation (14.8) in an alternative fashion, by examining the number of
arrivals that occur during two different time periods, the ﬁrst period being that of an arbitrary service
time and the second that of an arbitrary response time. We have just seen that
G A(z) ≡
∞

j=0
α jz j = B∗[λ(1 −z)],
where B∗[λ(1−z)] is the Laplace transform of the service time distribution evaluated at s = λ(1−z)
and α j = Prob{A = j} is the probability of j arrivals during an arbitrary service. In other words,
G A(z) is the z-transform of the distribution of the number of customer arrivals in a particular
interval, where the arrival process is Poisson at rate λ customers per second—the particular interval
is a service interval with distribution B(x) and Laplace transform B∗(s). Observe that this is a
relationship between two random variables, the ﬁrst which counts the number of Poisson events that
occur during a certain period of time and the second which characterizes this period of time. If we
now change the distribution of the time period but leave the Poisson nature of the occurrence of the
events unchanged, the form of this relationship must remain unaltered.
With this in mind, let us change the time period to “time spent in the system,” i.e., response
time, instead of “service time.” In other words, our concern is now with the number of arrivals
during the response time of a customer instead of the number of arrivals during the service time of a
customer. This means that in the above formula, we need to replace α j (the probability of j arrivals
during an arbitrary service), with p j, (the probability of j arrivals during an arbitrary sojourn time
of a customer in the M/G/1 queue). Remember that p j has previously been deﬁned as the stationary
probability that the system contains j customers, but since this is the distribution seen by a customer
arriving to an M/G/1 queue which is turn must be equal to the number left behind by a departing
customer, it must also be the number of arrivals that occur during the time that a customer spends
in the system—assuming a FCFS discipline. Thus we may immediately write
P(z) = W ∗[λ(1 −z)],
where W ∗denotes the Laplace transform of customer response time, and hence we are back to
Equation (14.8).
Recalling our earlier expression for P(z) given by the Pollaczek-Khintchine transform equation
No. 1,
P(z) = (1 −ρ)(z −1)B∗[λ(1 −z)]
z −B∗[λ(1 −z)]
,
and applying Equation (14.8), we conclude that
W ∗[λ(1 −z)] = (1 −ρ)(z −1)B∗[λ(1 −z)]
z −B∗[λ(1 −z)]
.
Letting s = λ −λz gives z = 1 −s/λ and hence
W ∗(s) = B∗(s)
s(1 −ρ)
s −λ + λB∗(s).
This is called the Pollaczek-Khintchine transform equation number No. 2. It provides the Laplace
transform of the distribution of response time (total time spent waiting for and receiving service) in
an M/G/1 queue with service provided in ﬁrst-come ﬁrst-served order.

14.4 The M/G/1 Residual Time: Remaining Service Time
523
Example 14.8 The M/M/1 queue.
W ∗(s) = B∗(s)
s(1 −ρ)
s −λ + λB∗(s)
=
μ
s + μ
&
s(1 −ρ)
s −λ + λμ/(s + μ)
'
=
μ(1 −ρ)
s + μ(1 −ρ).
This can be inverted to obtain the probability distribution and density functions of total time spent
in the M/M/1 queue, distributions that have already been described in Section 11.2.2.
Queueing Time Distributions
The probability distribution function of the time spent waiting in the queue before beginning service
may be computed from the previous results. We have
R = Tq + S,
where R is the random variable that characterizes the total time (response time) in the system, Tq
the random variable for time spent in the queue and S the random variable that describes service
time. From the convolution property of transforms, we have
W ∗(s) = W ∗
q (s)B∗(s),
i.e.,
B∗(s)
s(1 −ρ)
s −λ + λB∗(s) = W ∗
q (s)B∗(s)
and hence
W ∗
q (s) =
s(1 −ρ)
s −λ + λB∗(s).
This is known as the Pollaczek-Khintchine transform equation No. 3: it gives the Laplace transform
of the distribution of time spent waiting in the queue.
14.4 The M/G/1 Residual Time: Remaining Service Time
When a customer arrives at an M/G/1 queue and ﬁnds at least one customer already present, at
that arrival instant a customer is in the process of being served. In this section our concern is with
the time that remains until the completion of that service, the so-called residual (service) time. In
the more general context of an arbitrary stochastic process, the terms residual lifetime and forward
recurrence time are also employed. The time that has elapsed from the moment service began until
the current time is called the backward recurrence time. Since we have previously used R to denote
the random variable “response time,” we shall let R (i.e., calligraphic R as opposed to italic R)
be the random variable that describes the residual service time and we shall denote its probability
density function by fR(x). If the system is empty, then R = 0. As always, we shall let S be the
random variable “service time” and denote its density function by b(x). In an M/M/1 queue, we
know that the remaining service time is distributed exactly like the service time itself and in this
case fR(x) = b(x) = μe−μx, x > 0.
The mean residual service time is easily found from the Pollaczek-Khintchine mean value
formulae in the context of a ﬁrst-come, ﬁrst-served scheduling policy. Given that we have an
expression for the expected time an arriving customer must wait until its service begins, Wq, and
another expression for the expected number of customers waiting in the queue, Lq, the mean residual

524
The M/G/1 and G/M/1 Queues
service time is obtained by subtracting 1/μ times the second from the ﬁrst. Since
Wq = λE[S2]
2(1 −ρ)
and
Lq = λ2E[S2]
2(1 −ρ),
it follows that
E[R] = Wq −1
μ Lq = λE[S2]
2(1 −ρ) −1
μ
λ2E[S2]
2(1 −ρ) = λE[S2]
2(1 −ρ)(1 −ρ) = λE[S2]
2
.
The expression E[R] = λE[S2]/2 is another Pollaczek-Khintchine mean value formula. It provides
the expected residual service time as seen by an arriving customer. However, since the arrival
process is Poisson, this is also the expected residual service time as seen by a random observer,
i.e., the expected residual service at any time. Although derived in the context of a ﬁrst-come ﬁrst-
served policy, it can be shown that this result is also applicable to any work conserving scheduling
algorithm, i.e., when the server continues working as long as there are customers present, and
customers wait until their service is completed. Finally, observe the interesting relationship between
E[R] and Wq, i.e., that E[R] = (1 −ρ)Wq, where (1 −ρ) is the probability that the server is idle.
It is a useful exercise to derive the result E[R] = λE[S2]/2 from ﬁrst principles. Let Si = 1/μi
be the mean service time required by customer i. Figure 14.4 displays a graph of R(t) and shows
the residual service requirement at any time t. Immediately prior to the initiation of service for a
customer, this must be zero. The moment the server begins to serve a customer, the residual service
time must be equal to the total service requirement of that customer, i.e., Si for customer i. As
time passes, the server reduces this service requirement at the rate of one unit per unit time; hence
the slope of −1 from the moment service begins until the service requirement of the customer
has been completely satisﬁed, at which time the remaining service time is equal to zero. If at this
instant another customer is waiting in the queue, the residual service time jumps an amount equal
to the service required by that customer. Otherwise, the server becomes idle and remains so until a
customer arrives to the queueing system.
i
R (t)
Idle
Idle
t
Time
Average residual
Busy
Busy
Busy
S
S
3
i
Idle
S2
S1
S1
S2
S3
S4
S5
S
Figure 14.4. R(t): Residual service time in an M/G/1 queue.
Let us assume that at time t = 0 the system is empty and let us choose a time t at which the
system is once again empty. If λ < μ we are guaranteed that such times occur inﬁnitely often. Let
M(t) be the number of customers served by time t. The average residual service time in [0, t] is the
area under the curve R(t) divided by t and since the area of each right angle triangle with base and
height equal to Si is S2
i /2, we ﬁnd
1
t
 t
0
R(τ)dτ = 1
t
M(t)

i=1
S2
i
2 = 1
2 × M(t)
t
×
M(t)
i=1 S2
i
M(t)
.

14.4 The M/G/1 Residual Time: Remaining Service Time
525
Taking the limit as t →∞gives the desired result:
E[R] = lim
t→∞
1
t
 t
0
R(τ)dτ = 1
2 lim
t→∞
M(t)
t
lim
t→∞
M(t)
i=1 S2
i
M(t)
= 1
2λE[S2],
where
λ = lim
t→∞
M(t)
t
and
E[S2] = lim
t→∞
M(t)
i=1 S2
i
M(t)
.
These limits exist since our system is ergodic. Notice that we set λ equal to the mean output rate,
which is appropriate since at equilibrium the arrival rate is equal to the departure rate. We now turn
our attention to the derivation of the probability distribution of the residual service time, and we
do so with the understanding that it is conditioned on the server being busy. We shall denote this
random variable Rb. Let X be the random variable that denotes the service time of the customer in
service when an arrival occurs and let fX(x) be its density function. Observe that X and S describe
different random variables. Since the service time is a random variable having a general distribution
function, the service received by some customers will be long while that received by other customers
will be short. Everything else being equal, it is apparent that an arrival is more likely to occur
during a large service time than in a small service interval. Therefore the probability that X is of
duration x should be proportional to the length x. The other factor that plays into this scenario is the
frequency of occurrence of service intervals having length x, namely, b(x)dx: both length of service
and frequency of occurrence must be taken into account. Thus
fX(x)dx = Prob{x ≤X ≤x + dx} = α x b(x)dx,
where the role of α is to ensure that this is a proper density function, i.e., that
 ∞
0
α x b(x)dx = 1.
Since E[S] =
 ∞
0
x b(x)dx, it follows that α = 1/E[S] and hence
fX(x) = x b(x)
E[S] .
So much for the distribution of service periods at an arrival instant. Let us now consider the
placement of an arrival within these service periods. Since arrivals are Poisson, hence random, an
arrival is uniformly distributed over the service interval (0, x). This means that the probability that
the remaining service time is less than or equal to t, 0 ≤t ≤x, given that the arrival occurs in a
service period of length x, is equal to t/x, i.e., Prob{Rb ≤t | X = x} = t/x. It now follows that
Prob{t ≤Rb ≤t + dt | X = x} = dt
x ,
t ≤x.
Removing the condition, by integrating over all possible x, allows us to obtain probability
distribution function of the residual service time conditioned on the server being busy. We have
Prob{t ≤Rb ≤t + dt} = fRb(t)dt =
 ∞
x=t
dt
x fX(x)dx =
 ∞
x=t
b(x)
E[S]dx dt = 1 −B(t)
E[S]
dt
and hence
fRb(t) = 1 −B(t)
E[S]
.
The mean residual time is found from
E[Rb] =
 ∞
0
t fRb(t)dt =
1
E[S]
 ∞
0
t (1 −B(t))dt.

526
The M/G/1 and G/M/1 Queues
Taking u = [1 −B(t)], du = −b(t) dt, dv = t dt, v = t2/2, and integrating by parts (

u dv =
uv −

v du), we ﬁnd
E[Rb] =
1
E[S]
(
[1 −B(t)]t2
2

∞
0
+
 ∞
0
t2
2 b(t)dt
)
=
1
2E[S]
 ∞
0
t2 b(t)dt = E[S2]
2E[S] = μE[S2]
2
.
Higher moments are obtained analogously. We have
E[Rk−1
b
] = μE[Sk]
k
,
k = 2, 3, . . . .
Finally, notice that
E[R] = ρE[Rb],
where ρ = 1 −p0 is the probability that the server is busy.
Exactly the same argument we have just made for the forward recurrence time applies also to the
backward recurrence time, and in particular the mean backward recurrence time must also be equal
to E[S2]/(2E[S]). Therefore the sum of the mean forward and backward recurrence times is not
equal to the mean service time! This is generally referred to as the paradox of residual life.
Example 14.9 Observe that, when the service time is exponentially distributed, the remaining
service time is the same as the service time itself. We have
fRb(t) = 1 −B(x)
E[S]
= 1 −(1 −e−μt)
1/μ
= μe−μt.
The mean forward and backward recurrence times are both equal to 1/μ and hence their sum is
equal to twice the mean service time.
Example 14.10 Given that the ﬁrst and second moments of a random variable having an Erlang-
r distribution (r exponential phases each with parameter rμ), are E[S] = 1/μ and E[S2] =
r(r + 1)/(rμ)2, respectively, the expected residual time in an M/Er/1 queue is
E[Rb] = (1 + 1/r)/μ2
2/μ
= (1 + 1/r)
2μ
.
As r →∞, E[Rb] →1/(2μ) = E[S]/2, which, as may be veriﬁed independently, is the
expected residual service time in an M/G/1 queue when the service process is deterministic. With
deterministic service times, E[S2] = E[S]2 and the sum of the mean forward and backward
recurrence times is equal to the mean service time.
14.5 The M/G/1 Busy Period
A stable queueing system always alternates between periods in which there are no customers present
(called idle periods) and periods in which customers are present and being served by the server
(called busy periods). More precisely, a busy period is the length of time between the instant a
customer arrives to an empty system (thereby terminating an idle period) and the moment at which
the system empties (thereby initiating the next idle period). This is illustrated in Figure 14.5.
In this ﬁgure the horizontal line is the time axis and, at any time t, the corresponding point
on the solid line represents the amount of work that remains for the server to accomplish before it
becomes idle. This line has a jump when a new customer arrives (represented by an upward pointing
arrow), bringing with it the amount of work that must be performed to serve this customer. This is

14.5 The M/G/1 Busy Period
527
Idle
Busy 
Busy 
Busy
Time
Idle
Figure 14.5. Busy and idle periods in an M/G/1 queue.
added to the amount of work already in the system (represented by a dotted vertical line). The
solid line decreases at a rate of one unit per unit time until it reaches zero and the server become
idle once again. The dashed line leading to a downward pointing arrow represents the continuation
and eventually the completion of the service and the subsequent departure of a customer when the
scheduling algorithm is FCFS.
An idle period begins at the end of a busy period and terminates with an arrival. Since arrivals in
an M/G/1 queue have a (memoryless) Poisson distribution, the time until the idle period terminates
must also have this same distribution. Thus the distribution of idle periods is given by 1 −e−λt.
Finding the distribution of the busy period is somewhat more difﬁcult. We begin by ﬁnding its
expectation. Let Y be the random variable that describes the length of a busy period and let
G(y) ≡Prob{Y ≤y}
be the busy period distribution. We have already seen that the probability of having zero customers
(idle period) in an M/G/1 queue is 1 −ρ. Notice also that the expected length of time between the
start of two successive busy periods is 1/λ + E[Y] and, since 1/λ is the length of the idle period,
the probability of the system being idle is the ratio of these. Combining these results we have
1 −ρ =
1/λ
1/λ + E[Y].
Rearranging, we ﬁnd
λE[Y] =
1
1 −ρ −1 =
ρ
1 −ρ
or
E[Y] = 1/μ
1 −ρ =
1
μ −λ,
and so the average length of a busy period depends only on the mean values λ and μ. Observe,
furthermore, that the average length of a busy period in an M/G/1 queue is exactly equal to the
average amount of time a customer spend in an M/M/1 queue!
In Figure 14.5, the implied scheduling of customers for service is FCFS. While a particular
scheduling algorithm can make a signiﬁcant difference to customer waiting times, it does not have
any effect on the distribution of the busy period if the scheduling discipline is a work-conserving
discipline. A work-conserving discipline is one in which the server continues to remain busy while
there are customers present and customers cannot exit the system until they have been completely
served. The calculation of the busy period distribution is made much easier by choosing a scheduling
procedure that is very different from FCFS. Let the ﬁrst customer who ends an idle period be
called c0. This customer on arrival is immediately taken into service and served until it has ﬁnished.
Assume that during the service time of this customer, n other customers arrive. Let these be called

528
The M/G/1 and G/M/1 Queues
c1, c2, . . . , cn. Once customer c0 exits the system, the service of customer c1 begins and continues
uninterrupted until it too exits the system. At this point the scheduling algorithm changes. During
the service time of c1 other customers arrive and these customers are to be served before service
begins on customer c2. Indeed, the service of customer c2 does not begin until the only remaining
customers in the system are c2, c3, . . . , cn. The process continues in this fashion: the n customers
who arrive during the service time of the ﬁrst customer are served in order, but the service of any
one of them does not begin until all later arrivals (customers who arrive after the departure of c0)
have been served. In this manner, the time to serve c1 and all the customers who arrive during
c1’s service has the same distribution as the time to serve c2 and the customers who arrive during
c2’s service, and so on. Furthermore, this common distribution for all n arrivals must be distributed
exactly the same as the busy period we seek! Essentially, the beginning of service of any of the
ﬁrst n customers, say ci, may be viewed as the beginning of an “included” or “sub-” busy period
that terminates when the last customer who arrives during the service of ci leaves the system. The
distribution of the “included” busy period begun by the initiation of service of any of the customers
ci, i = 1, 2, . . . , n, must be identical for all n of them, and must also be the same as the distribution
of the busy period initiated by the arrival of c0.
With this understanding, we may now proceed to ﬁnd the busy period distribution itself. Let
G∗(s) ≡
 ∞
0
e−sydG(y) = E[e−sY ]
be the Laplace transform for the probability density function associated with Y, the random variable
that describes the length of a busy period. We shall show that
G∗(s) = B∗[s + λ −λG∗(s)],
where B∗[s+λ−λG∗(s)] is the previously deﬁned Laplace transform of the service time distribution,
this time evaluated at the point s+λ−λG∗(s). We have just seen that the duration of the busy period
initiated by the arrival of customer c0 is the sum of the random variable that describes the service
time for c0 and the n random variables that describe the n “included” busy periods associated with
customers c1 through cn. Let Yi be the random variable which characterizes the “included” busy
period associated with ci, i = 1, 2, . . . , n. Conditioning Y on the number of arrivals, A = n, that
occur while c0 is being served and on S = x, the length of service provided to c0, we have
E[e−sY | A = n, S = x] = E[e−s(x+Y1+Y2+···+Yn)] = E[e−sxe−sY1e−sY2 · · · e−sYn]
= E[e−sx]E[e−sY1]E[e−sY2] · · · E[e−sYn]
= e−sx E[e−sY1]E[e−sY2] · · · E[e−sYn].
The last equality arises from the fact that x is a given constant, while the equality preceding the last
is a result of the independence of the “included” busy periods. Since the “included” busy periods
are identically distributed and equal to that of the overall busy period, we have
E[e−sYi ] = E[e−sY] = G∗(s)
for i = 1, 2, . . . , n,
and hence
E[e−sY | n, x] = e−sx[G∗(s)]n.
We now remove the two conditions, ﬁrst on n, the number of arrivals during the service of c0, and
then on x, the service time of c0. Since the arrival process is Poisson with rate λ, the distribution of

14.5 The M/G/1 Busy Period
529
n must be Poisson with mean value λx. We obtain
E[e−sY | x] =
∞

k=0
E[e−sY | n, x] Prob{n = k}
=
∞

k=0
e−sx[G∗(s)]ke−λx (λx)k
k!
=
∞

k=0
e−sxe−λx [λG∗(s)x]k
k!
= e−x(s+λ−λG∗(s)).
Now, removing the condition on x by integrating with respect to the service time distribution
function B(x), we ﬁnd
G∗(s) = E[e−sY ] =
 ∞
0
e−x(s+λ−λG∗(s))d B(x),
i.e.,
G∗(s) = B∗[s + λ −λG∗(s)].
(14.9)
This is a functional equation which must be solved numerically for G∗(s) at any given value of s.
For example, it may be shown that the iterative scheme
G∗
j+1(s) = B∗[s + λ −λG∗
j(s)]
converges to G∗(s) when ρ < 1 and the initial value G∗
0(s) is chosen to lie in the interval [0, 1].
We can use Equation (14.9) to calculate moments of the busy period. We already formed the
expected busy period length using a different approach and saw that
E[Y] =
1
μ −λ.
The derivation of this result by means of differentiation is left as an exercise, as is the formation of
the second moment and the variance, given respectively by
E[Y 2] =
E[S2]
(1 −ρ)3
and
Var[Y] = Var[S] + ρE[S]2
(1 −ρ)3
,
where we recall that S is the random variable “service time.”
Example 14.11 In general, it is not possible to invert Equation (14.9). One instance in which it is
possible is the M/M/1 queue. The transform for the service time distribution in the M/M/1 queue
is given by B∗(s) = μ/(s + μ). Replacing s with s + λ −λG∗(s) and identifying the result with
Equation (14.9) results in
G∗(s) =
μ
[s + λ −λG∗(s)] + μ.
This gives the quadratic equation
λ[G∗(s)]2 −(μ + λ + s)G∗(s) + μ = 0,

530
The M/G/1 and G/M/1 Queues
which may be solved for G∗(s). Making the appropriate choice between the two roots (|G∗(s)| ≤1),
the result obtained is
G∗(s) = (μ + λ + s) −
.
(μ + λ + s)2 −4λμ
2λ
.
This can be inverted and the probability density function of the busy period obtained as
g(y) =
1
y√ρ e−(λ+μ)y I1(2y
.
λμ),
where I1 is the modiﬁed Bessel function of the ﬁrst kind of order 1.
Distribution of Customers Served in a Busy Period
A similar analysis may be used to derive a functional equation for the transform of the number of
customers served during a busy period. Since arrivals are Poisson and the lengths of “included”
busy periods are independent and identically distributed and equal to the distribution of the length
of the entire busy period, the number of customers served during each ‘included” busy period is
independent and identically distributed and equal to the distribution of the number served in a
complete busy period. If Ni is the random variable that denotes the number of customers served
during the ith, i = 1, 2, . . . , n, “included” busy period, then Nb, the number served during the
entire busy period, is given by
Nb = 1 + N1 + N2 + · · · + Nn.
Let hk = Prob{Nb = k} and
H(z) = E[zNb] =
∞

k=0
hkzk.
Conditioning on the number of arrivals during the service period of customer c0, we have
E[zNb | A = n] = E[z1+N1+N2+···+Nn] = z
n
k=1
E[zNk] = zH(z)n,
so that when we now remove the condition, we obtain
H(z) = E[zNb] =
∞

n=0
E[zNb | A = n]Prob{A = n}
=
∞

n=0
zH(z)n
 ∞
0
(λx)n
n! e−λxb(x)dx
= z
 ∞
0
e−[λ−λH(z)]x b(x)dx
= zB∗[λ −λH(z)].
(14.10)

14.6 Priority Scheduling
531
The expectation, second moment, and variance of the number served in a busy period may be
computed from Equation (14.10). They are given respectively by
Mean:
1
1 −ρ ,
Second moment: 2ρ(1 −ρ) + λ2x2
(1 −ρ)3
+
1
1 −ρ ,
Variance:
ρ(1 −ρ) + λ2x2
(1 −ρ)3
.
14.6 Priority Scheduling
In a queueing system in which customers are distinguished by class, it is usual to assign priorities
according to the perceived importance of the customer. The most important class of customers is
assigned priority 1; classes of customers of lesser importance are assigned priorities 2, 3, . . . . When
the system contains customers of different classes, those with priority j are served before those
of priority j + 1, j = 1, 2, . . . . Customers within each class are served in ﬁrst-come, ﬁrst-served
order. The question remains as to how a customer in service should be treated when a higher-priority
customer arrives. This gives rise to two scheduling policies.
The ﬁrst policy is called preemptive priority and in this case, a lower-priority customer in
service is ejected from service the moment a higher-priority customer arrives. The interrupted
customer is allowed back into service once the queue contains no customer having a higher priority.
The interruption of service may mean that all of the progress made toward satisfying the ejected
customer’s service requirement has been lost, so that it becomes necessary to start this service
from the beginning once again. This is called preempt-restart. Happily, in many cases, the work
completed on the ejected customer up to the point of interruption is not lost so that when that
customer is again taken into service, the service process can continue from where it left off. This is
called preempt-resume.
The second policy is nonpreemptive. The service of a low-priority customer will begin when
there are no higher-priority customers present. Now, however, once service has been initiated on a
low-priority customer, the server is obligated to serve this customer to completion, even if one or
more higher-priority customers arrive during this service.
14.6.1 M/M/1: Priority Queue with Two Customer Classes
To ease us into the analysis of such queues, we shall start with the M/M/1 queue with just
two customer classes that operate under the preemptive priority policy. Customers of class 1
arrive according to a Poisson process with rate λ1; those of class 2 arrive according to a
second (independent) Poisson process having rate λ2. Customers of both classes receive the
same exponentially distributed service at rate μ. We shall let ρ1 = λ1/μ, ρ2 = λ2/μ, and
assume that ρ = ρ1 +ρ2 < 1. Given that the service is exponentially distributed, it matters little
whether we designate preempt-resume or preempt-restart—thanks to the memoryless property of
the exponential distribution. Furthermore, since we have conveniently chosen the service time of all
customers to be the same, the total number of customers present, N, is independent of the scheduling
policy chosen. It then follows from standard M/M/1 results that
E[N] =
ρ
1 −ρ .

532
The M/G/1 and G/M/1 Queues
From the perspective of a class 1 customer, class 2 customers do not exist, since service to
customers of class 2 is immediately interrupted upon the arrival of a class 1 customer. To a class 1
customer, the system behaves exactly like an M/M/1 queue with arrival rate λ1 and service rate μ.
The mean number of class 1 customers present and the mean response time for such a customer are
given, respectively, by
E[N1] =
ρ1
1 −ρ1
and
E[R1] =
1
μ(1 −ρ1).
We may now compute the mean number of class 2 customers present from
E[N2] = E[N] −E[N1] =
ρ
1 −ρ −
ρ1
1 −ρ1
=
ρ1 + ρ2
1 −ρ1 −ρ2
−
ρ1
1 −ρ1
=
ρ2
(1 −ρ1)(1 −ρ1 −ρ2).
The mean response time of class 2 customers follows from the application of Little’s law. We have
E[R2] =
1/μ
(1 −ρ1)(1 −ρ1 −ρ2).
Let us now move on to the nonpreemptive scheduling policy. This time an arriving class 1
customer ﬁnding a class 2 customer in service is forced to wait until that class 2 customer ﬁnishes
its service. From PASTA, we know that an arriving class 1 customer will ﬁnd, on average, E[N1]
class 1 customers already present, each of which requires 1/μ time units to complete its service.
The arriving customer also has a mean service time of 1/μ, and if the arriving customer ﬁnds a
class 2 customer in service, a further 1/μ time units must be added into the total time the arriving
class 1 customer spends in the system. The probability of an arriving customer ﬁnding a class 2
customer in service is equal to ρ2—recall that the probability the system has at least one customer
is ρ = ρ1 + ρ2. Summing these three time periods together, we compute the mean response time
for a class 1 customer as
E[R1] = E[N1]
μ
+ 1
μ + ρ2
μ .
Now, using Little’s law, we have E[N1] = λ1E[R1], which when substituted into the previous
equation gives
E[R1] = λ1E[R1]
μ
+ 1
μ + ρ2
μ .
Solving for E[R1] yields
E[R1] = (1 + ρ2)/μ
1 −ρ1
and we now ﬁnd the mean number of class 1 customers present as
E[N1] = (1 + ρ2)ρ1
1 −ρ1
.
As before, the mean number of class 2 customers can be found from
E[N2] = E[N] −E[N1] =
ρ1 + ρ2
1 −ρ1 −ρ2
−(1 + ρ2)ρ1
1 −ρ1
= ρ2 −ρ1ρ2 + ρ2
1ρ2 + ρ1ρ2
2
(1 −ρ1)(1 −ρ1 −ρ2)
= ρ2[1 −ρ1(1 −ρ1 −ρ2)]
(1 −ρ1)(1 −ρ1 −ρ2) ,

14.6 Priority Scheduling
533
and ﬁnally, from Little’s law, the mean response time for class 2 customers is
E[R2] = E[N2]
λ2
= [1 −ρ1(1 −ρ1 −ρ2)]/μ
(1 −ρ1)(1 −ρ1 −ρ2) .
Example 14.12 Consider an M/M/1 queue in which class 1 customers arrive at rate λ1 = .3 and
class 2 customers at rate λ2 = .5 per unit time. Let the mean of the exponential service time be
1 time unit, i.e., μ = 1. Treating both classes identically results in a standard M/M/1 queue with
λ = .8 and μ = 1. The mean number of customers and mean response time are given by
E[N] =
.8
1 −.8 = 4,
E[R] =
1
1 −.8 = 5.
With the priority policies we have
ρ1 = .3 and ρ2 = .5.
When the preemptive priority policy is applied, we ﬁnd
E[N1] = .3
.7 = .428571,
E[N2] =
.5
.7 × .2 = 3.571429,
E[R1] = 1
.7 = 1.428571,
E[R2] =
1
.7 × .2 = 7.142857.
When the nonpreemptive priority policy is applied, we have
E[N1] = 1.5 × .3
.7
= .642857,
E[N2] = .5(1 −.3 × .2)
.7 × .2
= 3.357143,
E[R1] = .642857
.3
= 2.142857,
E[R2] = 3.357143
.5
= 6.714286.
Thus the mean response time for class 2 customers increases from 5.0 with the FCFS policy to
6.71 with nonpreemptive priority to 7.14 with preemptive priority. Observe that the mean number
of customers present in all three cases is E[N] = E[N1] + E[N2] = 4. Furthermore, something
similar is apparent with the waiting times: the mean waiting time in the FCFS case is equal to a
weighted sum of the waiting times in the two priority cases, the weights being none other than ρ1
and ρ2. With the ﬁrst-come, ﬁrst-served policy we have
ρW q = ρ
ρ
μ −λ = .8 ×
.8
1 −.8 = 3.2,
while in both priority systems we have
ρ1W q
1 + ρ2W q
2 = .3 × (E[R1] −1) + .5 × (E[R2] −1) = 3.2.
We shall see later that there exist a conservation law that explains this phenomenon. However, in
the case of preempt-resume, it applies only when the service time is exponentially distributed, as in
the present example. It applies to all distributions when the policy is nonpreemptive. More details
are provided in a later section.
14.6.2 M/G/1: Nonpreemptive Priority Scheduling
Let us now consider the general case of J ≥2 different classes of customer in which the different
classes can have different service requirements. We assume that the arrival process of class j
customers, j = 1, 2, . . . , J, is Poisson with parameter λ j and that the service time distribution
of this class of customers is general, having a probability density function denoted by b j(x), x ≥0,
and expectation x j = 1/μ j. We shall let ρ j = λ j/μ j and assume that ρ = J
j=1 ρ j < 1. We

534
The M/G/1 and G/M/1 Queues
shall let L j and Lq
j be the mean number of class j customers in the system and waiting in the queue
respectively; we shall let E[R j] and W q
j be the mean response time and mean time spent waiting
respectively; and we shall let E[R j] be the expected residual service time of a class j customer.
We consider ﬁrst the non-preemptive priority scheduling case. Here the time an arriving class j
customer, to whom we refer to as the “tagged” customer, spends waiting in the queue is the sum of
the following three time periods.
1. The residual service time of the customer in service.
2. The sum of the service time of all customers of classes 1 through j that are already present
the moment the tagged customer arrives.
3. The sum of the service time of all higher-priority customers who arrive during the time the
tagged customer spends waiting in the queue.
The response time of the tagged customer is found by adding its service requirement to this total.
Two features allow us to treat the ﬁrst of these three periods relatively simply. First, an arriving
customer, whatever the class, must permit the customer in service to ﬁnish its service: the probability
that the customer in service is of class j is given by ρ j (observe that ρ = ρ1 + ρ2 + · · · + ρJ is the
probability that the server is busy). Second, we know that the expected remaining service time of
any customer in service as seen by an arriving customer whose arrival process is Poisson is E[R j]
if the customer in service is of class j. In Section 14.4 we saw how to compute these residuals. Thus
the expected residual service time as experienced by the tagged customer is
E[R] =
J

i=1
ρi E[Ri].
The second time period is found by using the PASTA property: the tagged customer ﬁnds the
queueing system at steady state, and hence observes the stationary distribution of all classes of
customer already present. Given that, at equilibrium, the mean number of class i customers waiting
in the queue is Lq
i , the mean duration to serve all customers of equal or higher priority found by the
arriving tagged customer is
j

i=1
Lq
i xi.
With just these two time periods, we already have sufﬁcient information to compute results
for the highest-priority customers. The mean length of time such customers spend waiting in the
queue is
W q
1 = Lq
1x1 +
J

i=1
ρi E[Ri].
Now, applying Little’s law, Lq
1 = λ1W q
1 ,
W q
1 = λ1W q
1 x1 +
J

i=1
ρi E[Ri] = ρ1W q
1 +
J

i=1
ρi E[Ri],
which leads to
W q
1 =
J
i=1 ρi E[Ri]
1 −ρ1
.
(14.11)
From this result, the response time of class 1 customers can be computed and then, using Little’s
law, the mean number in the system and the mean number waiting in the queue. Later on,

14.6 Priority Scheduling
535
Equation (14.11) will serve as the basis clause of a recurrence relation involving customers of lower
priority classes.
For customers of class 2 or greater, we need to compute the third time period, the time spent
waiting for the service completion of all higher-priority customers who arrive while the tagged
customer is waiting. Given that we have deﬁned W q
j to be the total time spent waiting by a
class j customer, the time spent serving higher-priority customers who arrive during this wait is
 j−1
i=1 λiW q
j xi = W q
j
 j−1
i=1 ρi. Thus the total time spent waiting by a class j customer is
W q
j =
J

i=1
ρi E[Ri] +
j

i=1
Lq
i xi + W q
j
j−1

i=1
ρi.
Using Little’s law to replace Lq
i with λiW q
i , we ﬁnd
W q
j

1 −
j−1

i=1
ρi

=
J

i=1
ρi E[Ri] +
j

i=1
λiW q
i xi
=
J

i=1
ρi E[Ri] +
j−1

i=1
ρiW q
i + ρ jW q
j ,
(14.12)
which leads to
W q
j

1 −
j

i=1
ρi

=
J

i=1
ρi E[Ri] +
j−1

i=1
ρiW q
i .
Comparing the right-hand side of this equation with the right-hand side of Equation (14.12), we see
that
W q
j

1 −
j

i=1
ρi

= W q
j−1

1 −
j−2

i=1
ρi

,
and multiplying both sides with (1 − j−1
i=1 ρi) yields the convenient recursive relationship
W q
j

1 −
j

i=1
ρi
 
1 −
j−1

i=1
ρi

= W q
j−1

1 −
j−1

i=1
ρi
 
1 −
j−2

i=1
ρi

.
It was with this relationship in mind that we derived Equation (14.11) for the highest-priority
customers. Repeated application of this recurrence leads to
W q
j

1 −
j

i=1
ρi
 
1 −
j−1

i=1
ρi

= W q
1 (1 −ρ1),
so that using Equation (14.11) we ﬁnally obtain
W q
j =
J
i=1 ρi E[Ri]

1 − j
i=1 ρi
 
1 − j−1
i=1 ρi
,
j = 1, 2, . . . , J.
(14.13)
The mean response time of class j customers can now be found by adding x j to this equation, and
then Little’s law can be used to determine the mean number of class k customers in the system and
waiting in the queue.

536
The M/G/1 and G/M/1 Queues
14.6.3 M/G/1: Preempt-Resume Priority Scheduling
Our ﬁnal objective in this section is to derive the corresponding results when the scheduling policy
is such that a low-priority customer in service is interrupted to allow an arriving customer of
a higher priority to begin service immediately. The interrupted customer is later scheduled to
continue its service from the point at which it was interrupted. With this policy, customers of
class j + 1, j + 2, . . . , J do not affect the progress of class j customers; customers with lower
priorities are essentially invisible to higher-priority customers. In light of this we may set λk = 0
for k = j + 1, j + 2, . . . , J when analyzing the performance of class j customers. There are two
common ways of determining W q
j , the time that a class j customer spends waiting in the queue. We
refer to them as approaches A and B, respectively. The ﬁrst is to compute the time T A
1 it takes to
serve all customers of equal or higher priority that are present at the moment the tagged customer
arrives and then to add to this T A
2 , the time that is spent serving all higher-priority customers that
arrive during the total time that the tagged customer spends in the system. The second approach is
to compute T B
1 , the time spent waiting until the tagged class j customer is taken into service for the
ﬁrst time and to add to this T B
2 , the time taken by all higher-priority customers who arrive after the
tagged customer ﬁrst enters service. We begin with approach A.
Let T A
1 be the average time it takes to serve all customers of equal or higher-priority that are
present at the moment a class j customer arrives. This time must be identical to the time that
an arriving customer must wait prior to being served in a standard (no priority) M/G/1 queue.
The amount of work present at the moment of arrival must all be ﬁnished and it matters little in
which order, i.e., which (work-preserving) scheduling policy is used. The total amount of work is
independent of the order in which customers are served. T A
1 is equal to the sum of the residual
service time of the customer in service and the time required to serve all waiting customers: i.e.,
T A
1 =
j

i=1
ρi E[Ri] +
j

i=1
xi Lq
i ,
where E[Ri] is the residual service time of a class i customer as seen by a Poisson arrival, xi is the
mean service time of a class i customer, and Lq
i is the average number of class i customers found
waiting in the queue at equilibrium. We continue the analogy with the (standard) M/G/1 queue. The
number of class i customers who arrive during a period of length T A
1 is λiT A
1 . Since the number
of customers present in the queue at a departure instant is equal to the number present at an arrival
instant, we must have Lq
i = λiT A
1 . Thus
T A
1 =
j

i=1
ρi E[Ri] +
j

i=1
ρiT A
1 ,
which leads to
T A
1 =
 j
i=1 ρi E[Ri]
1 − j
i=1 ρi
.
Now we need to ﬁnd T A
2 , the time taken by higher-priority arrivals during the time that the tagged
customer is in the system, i.e., during the mean response time of customer j, E[R j]. The number of
class i arrivals during this period is λi E[R j]. Therefore the time to serve these customer is
T A
2 =
j−1

i=1
ρi E[R j] = (W q
j + 1/μ j)
j−1

i=1
ρi.

14.6 Priority Scheduling
537
The total waiting time for a class j customer is then equal to
W q
j = T A
1 + T A
2 =
 j
i=1 ρi E[Ri]
1 − j
i=1 ρi
+ (W q
j + 1/μ j)
j−1

i=1
ρi.
Solving for W q
j gives
W q
j =
 j
i=1 ρi E[Ri]

1 − j
i=1 ρi
 
1 − j−1
i=1 ρi
 + 1/μ j
 j−1
i=1 ρi

1 − j−1
i=1 ρi
.
(14.14)
The expected total time spent in the system, the mean response or sojourn time of a class j customer,
can now be found as
E[R j] = W q
j + 1
μ j
=
 j
i=1 ρi E[Ri]

1 − j
i=1 ρi
 
1 − j−1
i=1 ρi
 +
1/μ j

1 − j−1
i=1 ρi
.
It is also possible to solve for the mean response time directly from
E[R j] = T A
1 + T A
2 + 1
μ j
=
 j
i=1 ρi E[Ri]
1 − j
i=1 ρi
+ E[R j]
j−1

i=1
ρi + 1
μ j
so that, solving for E[R j], being sure not to confuse italic R with calligraphic R, gives
E[R j] =
1
1 − j−1
i=1 ρi
 j
i=1 ρi E[Ri]
1 − j
i=1 ρi
+ 1
μ j

as before. The mean number of class j customers present in the system, L j, and the number waiting
in the queue, Lq
j, can now be obtained from Little’s law.
We now consider approach B, the second common way of analyzing the performance character-
istics of a class j customer when the scheduling policy is preempt-resume. As we indicated earlier,
this is to compute the sum of the time spent waiting until a class j customer enters service for the
ﬁrst time, T B
1 , and the remaining time, T B
2 , that is spent in service and in interrupted periods caused
by higher priority customers who arrive after the customer ﬁrst starts its service.
Under a non-preemptive policy, the ﬁrst time a customer enters service is also the only time
it does so, whereas in a preempt-resume policy, a customer may enter service, be interrupted,
enter service again, and so on. We now relate the time that a customer of class j spends waiting,
prior to entering service for the ﬁrst time, in both policies. The reader may have noticed that the
ﬁrst term on the right hand side of Equation (14.14) is almost identical to the right-hand side of
Equation (14.13), which gives the time spent prior to entering service (for the ﬁrst time) when
the priority policy is non-preemptive. The only difference is that the summation is over all classes
1 through J in the nonpreemptive case while it is just over classes 1 through j in the preempt-
resume policy. Under both policies, all equal- or higher-priority customers that are present upon
an arrival must ﬁrst be served, as must all higher priority customers who arrive while the class j
customer is waiting. In addition, in the nonpreemptive case, and only in the nonpreemptive case,
any lower-priority customer that happens to be in service when a class j customer arrives, must be
permitted to complete its service. This explains the difference in both formulae: the difference in
the upper limits of the summation arises since a class j customer under the preempt-resume policy
ignores lower-priority customers so that λi = 0 for i = j + 1, j + 2, . . . , J and hence ρi = 0 for
i = j + 1, j + 2, . . . , J and can be omitted from the summation. Thus, under a preempt-resume
scheduling discipline, the time spent waiting by a class j customer prior to entering service for the

538
The M/G/1 and G/M/1 Queues
ﬁrst time, is given by
T B
1 =
 j
i=1 ρi E[Ri]

1 − j
i=1 ρi
 
1 − j−1
i=1 ρi
.
It must then be the case that the second term of Equation (14.14) is equal to T B
2 , i.e.,
T B
2 = 1/μ j
 j−1
i=1 ρi
1 − j−1
i=1 ρi
.
This may be shown independently as follows. Observe that T B
2 + 1/μ is the time that elapses from
the moment the tagged customer enters service for the ﬁrst time until it leaves the queueing system.
During this time, λi
#
T B
2 + 1/μ
$
class i customers arrive and each has a service requirement equal
to xi. Therefore
T B
2 =
j−1

i=1
λixi
#
T B
2 + 1/μ
$
=
#
T B
2 + 1/μ
$ j−1

i=1
ρi.
Solving for T B
2 gives the desired result.
Example 14.13 Consider a queueing system which caters to three different classes of customers
whose arrival processes are all Poisson. The most important customers require x1 = 1 time unit of
service and have a mean interarrival period of 1/λ1 = 4 time units. The corresponding values for
classes 2 and 3 are x2 = 5, 1/λ2 = 20, and x3 = 20, 1/λ3 = 50, respectively. Thus ρ1 = 1/4,
ρ2 = 5/20, ρ3 = 20/50, and ρ = ρ1 + ρ2 + ρ3 = .9 < 1. To facilitate the computation of the
residual service times, we shall assume that all service time distributions are deterministic. Thus
R1 = .5, R2 = 2.5, and R3 = 10.0.
With the nonpreemptive priority policy, the times spent waiting in the queue by a customer of
each of the three classes are as follows:
W q
1 = ρ1R1 + ρ2R2 + ρ3R3
(1 −ρ1)
= 4.75
.75 = 6.3333,
W q
2 = ρ1R1 + ρ2R2 + ρ3R3
(1 −ρ1 −ρ2)(1 −ρ1) =
4.75
.50 × .75 = 12.6667,
W q
3 =
ρ1R1 + ρ2R2 + ρ3R3
(1 −ρ1 −ρ2 −ρ3)(1 −ρ1 −ρ2) =
4.75
.10 × .50 = 95.0.
With the preempt-resume policy, the corresponding waiting times are
W q
1 =
ρ1R1
(1 −ρ1) = .125
.75 = 0.16667,
W q
2 =
ρ1R1 + ρ2R2
(1 −ρ1 −ρ2)(1 −ρ1) + ρ1/μ2
1 −ρ1
=
.75
.50 × .75 + 1.25
.75 = 3.6667,
W q
3 =
ρ1R1 + ρ2R2 + ρ3R3
(1 −ρ1 −ρ2 −ρ3)(1 −ρ1 −ρ2) + (ρ1 + ρ2)/μ3
(1 −ρ1 −ρ2) =
4.75
.10 × .50 + 10
.5 = 115.0.
14.6.4 A Conservation Law and SPTF Scheduling
It is true what they say—there is no free lunch, and this applies to queueing systems as much as
to everything else. When some classes of customers are privileged and have short waiting times,

14.6 Priority Scheduling
539
it is at the expense of other customers who pay for this by having longer waiting times. Under
certain conditions, it may be shown that a weighted sum of the mean time spent waiting by all
customer classes is constant, so that it becomes possible to quantify the penalty paid by low-
priority customers. When the queueing system is work preserving (i.e., the server remains busy
as long as there are customers present and customers do not leave before having been served), when
the service times are independent of the scheduling policy, and when the only preemptive policy
permitted is preempt-resume (in which case all service time distributions are exponential with the
same parameter for all customer classes), then
J

j=1
ρ jW q
j = C,
where C is a constant. To justify this result, we proceed as follows. We consider an M/G/1 queue
with a nonpreemptive scheduling algorithm. Let U be the average amount of unﬁnished work in the
system. This is equal to the expected work yet to be performed on the customer in service plus the
sum, over all customer classes j, j = 1, 2, . . . , J, of the mean number of class j customers present
times the mean service time each class j customer requires. Thus
U =
J

j=1
ρ j E[R j] +
J

j=1
Lq
j E[Sj] = E[R] +
J

j=1
ρ jW q
j ,
(14.15)
where, as usual, E[R] is the expected remaining service time of the customer in service, Lq
j is the
mean number of class j customers waiting in the queue and W q
j is the mean time spent by a class
j customer prior to entering service. Observe that this quantity U is independent of the scheduling
algorithm and thus has the same value for all scheduling algorithms. No matter the order in which
the customers are served, the remaining work is reduced by the single server at a rate of one unit of
work per unit time. Furthermore, in a nonpreemptive system, E[R] is independent of the scheduling
algorithm—once a customer is taken into service, that customer is served to completion regardless
of the scheduling policy. Now since the right-hand side of Equation (14.15) is independent of the
scheduling policy, as is its ﬁrst term, E[R], it must be the case that J
j=1 ρ jW q
j is the same for all
nonpreemptive scheduling algorithms and hence
J

j=1
ρ jW q
j = C.
The astute reader will have noticed that it is the required independence of E[R] that limits preempt-
resume policies to those in which the service times are exponentially distributed with the same
parameter for all classes.
We now relate this to the mean waiting time in a standard M/G/1 queue. The unﬁnished work in
a FCFS M/G/1 queue is given by
UFCFS = E[R] + Lq E[S] = E[R] + ρW q.
In this case the constant C that is independent of the scheduling algorithm is easy to ﬁnd. We have
ρW q = ρ λE[S2]
2(1 −ρ) =
ρ
1 −ρ
λE[S2]
2
=
ρ
1 −ρ E[R]
and hence
J

j=1
ρ jW q
j =
ρ
1 −ρ E[R].
This is called the Kleinrock conservation law.

540
The M/G/1 and G/M/1 Queues
Example 14.14 Consider a nonpreemptive priority M/G/1 queue with three classes of customer.
The ﬁrst two moments of the highest priority customers are given by E[S1] = 5 and E[S2
1] = 28,
respectively. The corresponding values for classes 2 and 3 are E[S2] = 4; E[S2
2] = 24 and
E[S3]= 22; E[S2
3] = 1,184, respectively. Arrival rates for the three classes are such that ρ1 = 1/3,
ρ2 = 11/30, and ρ3 = 11/60, respectively, and so ρ = ρ1 + ρ2 + ρ3 = 53/60. The mean residual
service times are
E[R1] = E[S2
1]
2E[S1] = 28
10 = 2.8,
E[R2] = E[S2
2]
2E[S2] = 24
8 = 3,
E[R3] = E[S2
3]
2E[S3] = 1184
44
= 26.909091.
This allows us to compute the mean time spent waiting by customers of each class. Given that
3
j=1 ρ j E[R j] = 6.966667,
W q
1 =
3
j=1 ρ j E[R j]
1 −ρ1
= 6.966667
2/3
= 10.45,
W q
2 =
3
j=1 ρ j E[R j]
(1 −ρ1 −ρ2)(1 −ρ1) =
6.966667
3/10 × 2/3 = 34.833333,
W q
3 =
3
j=1 ρ j E[R j]
(1 −ρ1 −ρ2 −ρ3)(1 −ρ1 −ρ2) =
6.966667
7/60 × 3/10 = 199.047619,
and hence
3

j=1
ρiW q
i = 1
3 × 10.45 + 11
30 × 34.833333 + 11
60 × 199.047619 = 52.747619.
This is the constant that is independent of the scheduling policy. In the same M/G/1 queue without
priority, we ﬁnd E[R] = 6.966667 so that
ρ
1 −ρ E[R] = 53/60
7/60 × 6.966667 = 52.747619,
as expected.
We now turn to a scheduling algorithm known as the shortest processing time ﬁrst (SPTF), also
called the shortest job next (SJN), policy. As its name implies, among all waiting customers, the next
to be brought into service by an SPFT algorithm is the customer whose service requirement is least.
Once a customer is taken into service, that customer continues uninterrupted until its service has
been completed. Viewed in this light, the SPTF policy is a nonpreemptive scheduling policy in which
customers with the shortest service time get priority over those with longer service requirements.
The length of service can be given in integer units (discrete) but here we assume that any service
duration is possible (continuous). We consider an M/G/1 queue whose Poisson arrival process has
rate λ and which operates the SPTF policy. As always, the random variable “service time” is denoted
by S and its probability density function is denoted by bS(x). Thus
Prob{x ≤S ≤x + dx} = bS(x)dx.
A customer for which x ≤S ≤x + dx, where dx →0, is said to have priority x. The set of
all customers whose service time is x constitutes class x. Such customers have a ﬁxed service

14.6 Priority Scheduling
541
requirement equal to x and so their residual service time is
E[Rx] = x/2.
The arrival rate of customers of class-x, denoted λx, is given by
λx = λ Prob{x ≤S ≤x + dx} = λ bS(x)dx.
Since the service time of these customers is equal to x, the utilization of the system generated by
class-x customers is
ρx = λxx = λxbS(x)dx,
and so the total utilization is
ρ = λ
 ∞
0
xbS(x)dx = λE[S].
We now proceed by analogy with the nonpreemptive priority system having J classes of
customers as discussed in Section 14.6.2. We make use of two correspondences. First, we previously
found the mean residual time to be
E[R] =
J

j=1
ρ j E[R j].
Similarly, with the SPTF policy, using ρx = λxbS(x)dx and E[Rx] = x/2 and integrating rather
than summing, we have
 ∞
0
λxbS(x)x
2 dx = λ
2
 ∞
0
x2bS(x)dx = λE[S2]
2
= ρ E[S2]
2E[S].
Second, in Section 14.6.2, we formed the expected waiting time of a class j customer as
W q
j =
J
i=1 ρi E[Ri]

1 − j
i=1 ρi
 
1 − j−1
i=1 ρi
,
j = 1, 2, . . . , J.
In the continuous SPTF case, we very naturally replace  j
i=1 ρi with
 x
0 λtbS(t)dt. We also replace
 j−1
i=1 ρi with the exact same quantity,
 x
0 λtbS(t)dt. This is because the difference between a class-
x customer and a class-(x + δ) customer tends to zero as δ tends to 0. Thus we obtain the mean
waiting time of a class-x customer to be
W q
x =
λE[S2]/2
#
1 −λ
 x
0 tbS(t)dt
$2 .
The expected time spent waiting in the queue by an arbitrary customer is obtained as
W q =
 ∞
0
W q
x bS(x)dx =
 ∞
0
λE[S2]bS(x)dx
2
#
1 −λ
 x
0 tbS(t)dt
$2 = λE[S2]
2
 ∞
0
bS(x)dx
#
1 −λ
 x
0 tbS(t)dt
$2 .
Example 14.15 Let us show that the SPTF policy obeys the same conservation law as other
nonpreemptive policies. We have
ρx = λxbS(x)dx
and W q
x =
λE[S2]/2
#
1 −λ
 x
0 tbS(t)dt
$2

542
The M/G/1 and G/M/1 Queues
and we wish to show that
 ∞
0
ρxW q
x dx =
ρ
1 −ρ E[R].
Therefore
 ∞
0
ρxW q
x dx = λE[S2]
2
 ∞
0
λxbS(x)dx
#
1 −
 x
0 λtbS(t)dt
$2
= λE[S2]
2
1
#
1 −
 x
0 λtbS(t)dt
$

∞
0
= λE[S2]
2

1
1 −λE[S] −1

=
ρ
1 −ρ E[R],
as required.
14.7 The M/G/1/K Queue
As for the M/G/1 queue, arrivals to an M/G/1/K queue follow a Poisson process with rate λ and the
service time distribution is general, its probability density function denoted by b(x). However, this
time a maximum of K customers can be in the system at any one time: K −1 awaiting service plus
a single customer receiving service. Arrivals to an M/G/1/K queue that occur when there are already
K customers present are lost. Consider now the embedded Markov chain obtained at departure
instants in an M/G/1/K queue. This Markov chain can have only K states, since a customer leaving
an M/G/1/K queue can leave only 0, 1, 2, . . . , K −1 customers behind. In particular, a departing
customer cannot leave K customers behind since this would imply that K + 1 customers were
present prior to its departure. Furthermore, the ﬁrst K −1 columns of the transition probability
matrix F(K) of this embedded Markov chain must be identical to those of the transition probability
matrix F in an M/G/1 queue: when the M/G/1/K queue contains 0 < k < K customers, the next
departure can leave behind k −1 customers with probability α0, k with probability α1, k + 1 with
probability α2, and so on, up to the penultimate value; a departure can leave behind K −2 customers
with probability αK−1−k, where, as before, αi is the probability of exactly i arrivals into the system
during the service period of an arbitrary customer. This, we saw, is equal to
αi =
 ∞
0
(λx)i
i!
e−λxb(x)dx.
Only the elements in the last column differ. The largest number any departure can leave behind is
K −1. When the system contains k customers, the next departure can leave K −1 customers behind
only when K −k or more customers arrive during a service (all but the ﬁrst K −k of which are
denied access). This happens with probability 1 −α0 −α1 −· · · −αK−1−k. Alternatively, since the
matrix is stochastic each row sum is equal to 1, i.e., K−1
k=0 f (K)
ik
= 1 for i = 0, 1, . . . , K −1 and so
the elements in the last column are given by
f (K)
i,K−1 = 1 −
K−2

k=0
f (K)
ik ,
i = 0, 1, . . . , K −1.

14.7 The M/G/1/K Queue
543
Thus the K × K matrix of transition probabilities is
F(K) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
α0
α1
α2
· · ·
αK−2
1 −K−2
k=0 αk
α0
α1
α2
· · ·
αK−2
1 −K−2
k=0 αk
0
α0
α1
· · ·
αK−3
1 −K−3
k=0 αk
0
0
α0
· · ·
αK−4
1 −K−4
k=0 αk
...
...
...
...
...
...
0
0
0
· · ·
α0
1 −α0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Given the upper Hessenberg structure of this matrix, it is now possible to solve it and obtain
the probability distribution of customers at departure instants, using the same numerical procedure
(recursive solution) adopted for the M/G/1 queue. This time however, we do not know, a priori, as
we did for the M/G/1 queue, the ﬁrst component of the solution vector. Instead, the ﬁrst component
can be assigned an arbitrary value and a later normalization used to construct the correct probability
distribution.
Letting π(K)
i
be the probability that a departing customer in an M/G/1/K queue leaves behind i
customers, the system to be solved is
π(K) #
F(K) −I
$
= 0
where the ﬁrst K −1 equations are identical to the corresponding ﬁrst K −1 equations in an M/G/1
queue. Given this homogeneous system of equations in which the coefﬁcient matrix is singular,
we replace one equation (the Kth) by the normalization equation and the solution obtained is
proportional to a vector whose K elements are the ﬁrst K elements in the solution of the M/G/1
queue, i.e., we have
π(K)
i
=
p(∞)
i
K−1
k=0 p(∞)
k
,
0 ≤i < K,
where 1/ K−1
k=0 p(∞)
k
is the constant of proportionality and where we now explicitly write p(∞)
i
to
denote the probability of a random observer ﬁnding i customers present in an M/G/1 queue.
In this way, we can compute the distribution of customers in an M/G/1/K queue at departure
instants. However, what we most often require is the distribution of customers at any instant and
not just at departure instants. Since the PASTA property holds in an M/G/1/K queue, the distribution
of customers at arrival instants is the same as the distribution of customers as seen by a random
observer. But, unlike the M/G/1 queue, arrivals and departures in an M/G/1/K queue do not see
the same distribution. This is most clearly demonstrated by the fact that a customer arriving to an
M/G/1/K queue will, with nonzero probability, ﬁnd that the system is full (K customers present) and
is therefore lost, whereas the probability that a departure in an M/G/1/K queue will leave behind K
customers must be equal to zero. However, the departure probabilities π(K)
i
are the same as those
seen by an arriving customer given that that customer is admitted into the system.
We shall let p(K)
i
be the stationary probability of ﬁnding i customers in an M/G/1/K queue.
In particular, the probability that an M/G/1/K queue contains K customers is p(K)
K . This is the
probability that an arriving customer is refused admission. Alternatively, 1 −p(K)
K
is the probability
that an arriving customer is admitted. It follows that the probability of an arriving customer ﬁnding
i < K customers in an M/G/1/K queue is
p(K)
i
= Prob{N = i | arriving customer is admitted} = π(K)
i

1 −p(K)
K

,
0 ≤i < K,

544
The M/G/1 and G/M/1 Queues
and, in particular,
p(K)
0
= π(K)
0

1 −p(K)
K

.
This leads us to seek an expression for p(K)
K . At equilibrium, it is known that the ﬂow into and out
of the queueing system must be equal. The ﬂow into the system, the effective arrival rate, is given
by λ(1 −p(K)
K ), since arrivals occur at rate λ as long as the system is not full. On the other hand, the
ﬂow out of the system is given by μ(1 −p(K)
0
) since customers continue to be served at rate μ while
there is at least one present. This gives us our equation for p(K)
K :
λ

1 −p(K)
K

= μ

1 −p(K)
0

= μ

1 −π(K)
0
>
1 −p(K)
K
?
.
Solving for p(K)
K
we ﬁnd
p(K)
K
= ρ + π(K)
0
−1
ρ + π(K)
0
or
1 −p(K)
K
=
1
ρ + π(K)
0
,
where, as usual, ρ = λ/μ. It now follows that
p(K)
i
=
π(K)
i
ρ + π(K)
0
,
0 ≤i < K.
(14.16)
Although we know how to compute the departure probabilities π(K)
i
for i = 0, 1, . . . , K −1, it
is more usual to present the results for the distribution of customers in an M/G/1/K queue in terms
of the probability distributions in an M/G/1 queue. Returning to the equation
π(K)
i
=
p(∞)
i
K−1
k=0 p(∞)
k
,
0 ≤i < K,
and writing
σK =
∞

k=K
p(∞)
k
= 1 −
K−1

k=0
p(∞)
k
,
we have
π(K)
i
=
p(∞)
i
1 −σK
, 0 ≤i < K
with
π(K)
0
=
p(∞)
0
1 −σK
= 1 −ρ
1 −σK
,
0 ≤i < K.
Substituting into Equation (14.16), we obtain
p(K)
i
=
p(∞)
i
/(1 −σK)
ρ + (1 −ρ)/(1 −σK) =
p(∞)
i
1 −ρσK
,
0 ≤i < K.
Observe that, when σK is small, the probability distribution in an M/G/1/K queue is approximately
equal to that in the corresponding M/G/1 queue. Finally, for p(K)
K
we obtain
p(K)
K
= ρ + (1 −ρ)/(1 −σK) −1
ρ + (1 −ρ)/(1 −σK)
= (1 −ρ)σK
1 −ρσK
.

14.7 The M/G/1/K Queue
545
Example 14.16 Consider an M/D/1/4 queue for which λ = 1/2 and μ = 1. When we examined
the corresponding M/D/1 queue (Example 14.3) having these parameters, we saw that
αi = 0.5i
i! e−0.5,
which gave the following values:
α0 = 0.606531, α1 = 0.303265, α2 = 0.075816, α3 = 0.012636.
Therefore the (4 × 4) transition probability matrix for the embedded Markov chain at departure
instants is
F(4) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
α0
α1
α2
1 −2
k=0 αk
α0
α1
α2
1 −2
k=0 αk
0
α0
α1
1 −1
k=0 αk
0
0
α0
1 −α0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.606531
0.303265
0.075816
0.014388
0.606531
0.303265
0.075816
0.014388
0
0.606531
0.303265
0.090204
0
0
0.606531
0.393469
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We may now solve the system of equations, π(4)(F(4) −I) = 0 using the recursive procedure
discussed previously, beginning with π(4)
1
= 1, and then renormalizing once all four components
have been found. We shall leave this as an exercise. Alternatively, we may replace the last equation
with the normalizing equation directly and solve using Matlab. The system of equations then
becomes
π(4)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−0.393469
0.303265
0.075816
1.0
0.606531
−0.696735
0.075816
1.0
0
0.606531
−0.696735
1.0
0
0
0.606531
1.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= (0, 0, 0, 1)
and the solution obtained by Matlab is
π(4) = (0.507744, 0.329384, 0.124499, 0.038373).
This is the probability distribution as seen by departing customers. Given that ρ = λ/μ = 1/2,
we may now compute p(4)
4 from
p(4)
4
= ρ + π(4)
0
−1
ρ + π(4)
0
= 1/2 + 0.507744 −1.0
1/2 + 0.507744
= 0.007684,
and the rest of the probability distribution as seen by a random observer from
p(4)
i
=
π(4)
i
ρ + π(4)
0
=
π(4)
i
0.5 + 0.507744,
0 ≤i < 4.
This results in the following probability distribution of the number of customer in the M/D/1/4
queue:
p(4) = (0.503842, 0.326853, 0.123542, 0.038078, 0.007684).
Let us now compute these same results using the probability distribution obtained for the M/D/1
queue of Example 14.3. In this example, we found the probability distribution to be
p(∞) = (0.5, 0.324361, 0.122600, 0.037788, 0.010909, 0.003107, . . .).

546
The M/G/1 and G/M/1 Queues
Given that
σ4 = 1 −
3

k=0
p(∞)
k
= 0.015251,
and using the results
p(4)
i
=
p(∞)
i
1 −ρσ4
, i = 0, 1, 2, 3;
p(4)
4
= (1 −ρ)σ4
1 −ρσ4
,
we obtain
p(4) =

1
1 −0.015251/2[0.5, 0.324361, 0.122600, 0.037788, 0.015251/2]

,
which when evaluated gives the same result as before.
14.8 The G/M/1 Queue
Similarly to the M/G/1 queue, the G/M/1 queue is a single-server queue, but this time the
distributions of arrivals and service times are reversed: the service process has an exponential
distribution with mean service time 1/μ, i.e.,
B(x) = 1 −e−μx,
x ≥0,
while the arrival process is general with mean interarrival time equal to 1/λ. Customers arrive
individually and their interarrival times are independent and identically distributed. As a result, the
notation GI/M/1 is sometimes used to stress this independence of arrivals. We shall denote the arrival
distribution by A(t) and its probability density function by a(t).
To represent this system by a Markov chain, it is necessary to keep track of the time that
passes between arrivals, since the distribution of interarrival times does not in general possess the
memoryless property of the exponential. As was the case for the M/G/1 queue, a two-component
state descriptor may be used; the ﬁrst to indicate the number of customers present and the second to
indicate the elapsed time since the previous arrival. In this way, the G/M/1 queue can be solved using
the method of supplementary variables. It is also possible to deﬁne a Markov chain embedded within
the G/M/1 queue, and this is the approach that we shall follow here. The embedded time instants
are precisely the instants of customer arrivals, since the elapsed interarrival time at these moments
is known—it is exactly equal to zero. This allows us to form a transition probability matrix and to
compute the distribution of customers as seen by an arriving customer. Unfortunately, the PASTA
property no longer holds (we do not have Poisson arrivals) and so we cannot conclude that the
distribution as seen by an arrival is the same as that seen by a random observer: indeed they are not
the same.
We shall now construct the transition probability matrix of the Markov chain embedded at arrival
instants and write its solution. Let Mk be the number of customers present in a G/M/1 queue just
prior to the kth arrival. Let Bk+1 be the number of service completions that occur between the arrival
of the kth customer and that of the (k + 1)th customer. It follows that
Mk+1 = Mk + 1 −Bk+1.
The i j element of the transition probability matrix F, namely,
fi j = Prob{Mk+1 = j | Mk = i},
is equal to the probability that i + 1 −j customers are served during an arbitrary interarrival time.
This must be equal to zero when i < j −1 (the second of two consecutive arrivals cannot ﬁnd more

14.8 The G/M/1 Queue
547
than one additional customer to the number that the ﬁrst ﬁnds). Also, an arrival can ﬁnd any number
of customers from a minimum of zero to a maximum of one more than its predecessor ﬁnds. In
other words, the transition probability matrix has a lower Hessenberg structure.
The probability that i customers complete their service during the period between the kth and
(k + 1)th arrivals is given by
βi = Prob{Bk+1 = i} =
 ∞
0
e−μt (μt)i
i! d A(t),
and, given the independence and identical distribution of interarrival times, the transition probability
matrix is given by
F =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1 −β0
β0
0
0
· · ·
1 −1
i=0 βi
β1
β0
0
· · ·
1 −2
i=0 βi
β2
β1
β0
· · ·
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
It is apparent that this matrix is irreducible and aperiodic when β0 > 0 and β0 + β1 < 1. It turns
out that the solution to this Markov chain is geometric in form. In other words, there exists a ξ,
0 < ξ < 1, such that
πi = Cξ i,
i ≥0,
where πi is now the stationary probability of an arrival ﬁnding i customers already present. To show
that this is the case, we shall replace πi with Cξ i in the system of equations, π = π F, and ﬁnd the
restrictions that this imposes on ξ so that πi = Cξ i is indeed the solution we seek. Extracting the
kth equation from this system of equations, we ﬁnd
πk =
∞

i=0
πi fik =
∞

i=k−1
πi fik =
∞

i=k−1
πiβi+1−k
for k ≥1.
We do not need to be concerned with the case k = 0, since in considering π(F −I) = 0, the ﬁrst
equation is a linear combination of all the others. Substituting πk = Cξ k gives
Cξ k =
∞

i=k−1
Cξ iβi+1−k
for k ≥1.
Now, when we divide through by Cξ k−1, we obtain
ξ =
∞

i=k−1
ξ i−k+1βi+1−k =
∞

j=0
ξ jβ j = G B(ξ),
where G B(z) is the z-transform of the number of service completions that occur during an
interarrival period. This then is the condition that we must impose on ξ in order for the solution
to be given by πi = Cξ i: namely, that ξ be a root of the equation z = G B(z). One root of this
system is obviously ξ = 1; however, we need a root that is strictly less than 1. When the steady
state exists, the strict convexity of G B(z) implies that there is exactly one root of z = G B(z) that lies
strictly between zero and one: this root is the value ξ for which Cξ i is the probability of an arrival

548
The M/G/1 and G/M/1 Queues
ﬁnding i customers already present. Furthermore,
ξ =
∞

j=0
ξ jβ j =
∞

j=0
ξ j
 ∞
0
(μt) j
j! e−μtd A(t) =
 ∞
0
e−(μ−μξ)td A(t) = F∗
A(μ −μξ).
Observe that the right-hand side is the Laplace transform of the probability density function of
interarrival time, evaluated at the point μ −μξ. The solution ξ to this functional equation may be
obtained by successively iterating with
ξ ( j+1) = F∗
A(μ −μξ ( j))
(14.17)
and taking the initial approximation, ξ (0), to lie strictly between zero and one. As for the constant
C, it may be determined from the normalization equation. We have
1 =
∞

i=0
Cξ i = C
1
1 −ξ
i.e., C = (1 −ξ). This leads us to conclude that
πi = (1 −ξ)ξ i.
It is impossible not to recognize the similarity between this formula and the formula for the number
of customers in an M/M/1 queue, namely, pi = (1 −ρ)ρi, i ≥0, where ρ = λ/μ. It follows by
analogy with the M/M/1 queue that performance measures, such as the mean number of customers
present, can be obtained by replacing ρ with ξ in the corresponding formulae for the M/M/1 queue.
Thus, for example, whereas 1 −ρ is the probability that no customers are present in an M/M/1
queue, 1 −ξ is the probability that an arrival in a G/M/1 queue ﬁnds it empty; the mean number in
an M/M/1 queue is ρ/(1 −ρ) and the variance is ρ/(1 −ρ)2, while the mean and variance of the
number of customers seen by an arrival in a G/M/1 queue are ξ/(1−ξ) and ξ/(1−ξ)2, respectively,
and so on. It is important to remember, however, that π is the probability distribution as seen by an
arrival to a G/M/1 queue and that this in not equal to the stationary distribution of this queue. If,
indeed, the two are the same, it necessarily follows that G = M.
Finally, results for the stationary distribution of customers in a G/M/1 queue—the equilibrium
distribution as seen by a random observer, rather that that seen by an arriving customer—are readily
available from the elements of the vector π. First, although π0 = 1 −ξ is that probability that
an arrival ﬁnds the system empty, the stationary probability of the system being empty is actually
p0 = 1 −ρ. Furthermore Cohen [10] has shown that the stationary probability of a G/M/1 queue
having k > 0 customers is given by
pk = ρ(1 −ξ)ξ k−1 = ρπk−1
for k > 0.
Thus once the variable ξ has been computed from Equation (14.17), the stationary distribution is
quickly recovered.
Example 14.17 Let us show that ξ = ρ when the arrival process in a G/M/1 queue is Poisson,
i.e., when G = M. To show this, we need to solve the functional equation ξ = F∗
A(μ −μξ) when
a(t) = λe−λt. The Laplace transform is now
F∗
A(s) =
 ∞
0
e−stλe−λtdt = −
λ
s + λe−(s+λ)t

∞
0
=
λ
s + λ,
so that the functional equation becomes
ξ =
λ
μ(1 −ξ) + λ

14.8 The G/M/1 Queue
549
or
(ξ −1)(μξ −λ) = 0,
with the two solutions ξ = 1 and ξ = λ/μ = ρ. Only the latter solution satisﬁes the requirement
that 0 < ξ < 1 and this is exactly what we wanted to show.
Example 14.18 The D/M/1 Queue.
In this case, the interarrival time is constant and equal to 1/λ. The probability distribution
function has the value 0 for t < 1/λ and has the value 1 for t ≥1/λ. The density function is a
Dirac impulse at the point t = 1/λ and its Laplace transform is known to be F∗
A(s) = e−s/λ. Thus
the functional equation we need to solve for ξ is
ξ = e−(μ−μξ)/λ = e−(1−ξ)/ρ.
To proceed any further it is necessary to give a numeric value to ρ and solve using an iterative
procedure. Let us take ρ = 3/4 and begin the iterative process with ξ (0) = 0.5. Successive
iterations of
ξ ( j+1) = exp
ξ ( j) −1
0.75

give
0.5, 0.513417, 0..522685, 0.529183, 0.533788, 0.537076, . . . ,
which eventually converges to ξ = 0.545605. The mean number in this system at arrival epochs is
given by
E[NA] =
ξ
1 −ξ = 1.200729,
while the probability that an arrival to this system ﬁnds it empty is
π0 = 1 −ξ = 0.454395.
Example 14.19 Consider a G/M/1 queue in which the exponential service distribution has mean
service time equal to 1/2 and in which the arrival process has a hypoexponential distribution
function, represented as a passage through two exponential phases, the ﬁrst with parameter λ1 = 2
and the second with parameter λ2 = 4. The Laplace transform (see Section 7.6.3) of this distribution
is given by
F∗
A(s) =

λ1
s + λ1
 
λ2
s + λ2

.
Its expectation is 1/λ1 +1/λ2 = 1/2+1/4 = 3/4, which allows us to compute ρ = (4/3)/2 = 2/3.
Substituting the values for λ1 and λ2 we ﬁnd
F∗
A(s) =
8
(s + 2)(s + 4)
so that
F∗
A(μ −μξ) =
8
(2 −2ξ + 2)(2 −2ξ + 4) =
2
(ξ −2)(ξ −3),
and the ﬁxed point equation ξ = F∗
A(μ −μξ) now becomes
ξ 3 −5ξ 2 + 6ξ −2 = 0.
Since this is a cubic equation, its roots may be computed directly without having to resort to an
iterative procedure. Furthermore, since we know that ξ = 1 is a root we can reduce this cubic to a

550
The M/G/1 and G/M/1 Queues
quadratic, thereby making the computation of the remaining roots easy. Dividing the cubic equation
by ξ −1, we obtain the quadratic
ξ 2 −4ξ + 2 = 0,
which has the two roots 2 ±
√
2. We need the root that is strictly less than 1, i.e., the root
ξ = 2 −
√
2 = 0.585786. This then allows us to ﬁnd any and all performance measures for
this system. For example, the probabilities that it contains zero, one, or two customers are given,
respectively, by
p0 = 1 −ρ = 1/3, p1 = ρ(1 −ξ)ξ 0 = 0.27614, p2 = ρ(1 −ξ)ξ 1 = 0.161760;
the probability that an arrival ﬁnds the system empty is 1 −ξ = 0.414214; the mean number of
customers seen by an arrival is ξ/(1 −ξ) = 1.414214 and so on.
Waiting Time Distributions in a G/M/1 Queue
Whereas the distribution of customers at arrival epochs is generally not that which is sought (the
distribution of customers as seen by a random observer being the more usual), it is exactly that which
is needed to compute the distribution of the time spent waiting in a G/M/1 queue before beginning
service. When the scheduling policy is ﬁrst come, ﬁrst served, an arriving customer must wait until
all the customers found on arrival are served before this arriving customer can begin its service. If
there are n customers already present, then an arriving customer must wait through n services, all
independent and exponentially distributed with mean service time 1/μ. An arriving customer that
with probability πn = (1 −ξ)ξ n ﬁnds n > 0 customers already present, experiences a waiting time
that has an Erlang-n distribution. There is also a ﬁnite probability π0 = 1 −ξ that the arriving
customer does not have to wait at all. The probability distribution function of the random variable
Tq that represents the time spent waiting for service will therefore have a jump equal to 1 −ξ at the
point t = 0. For t > 0, we may write
Wq(t) = Prob{Tq ≤t} =
∞

n=1
 t
0
μ(μx)n−1
(n −1)! e−μxdx (1 −ξ)ξ n + (1 −ξ)
= ξ(1 −ξ)
 t
0
μe−μx(1−ξ)dx + (1 −ξ)
= 1 −ξe−μ(1−ξ)t,
t > 0.
This formula also gives the correct result 1 −ξ when t = 0 and so we may write
Wq(t) = 1 −ξe−μ(1−ξ)t, t ≥0,
(14.18)
and thus the time spent queueing in a G/M/1 queue is exponentially distributed with a jump of
(1 −ξ) at t = 0. Observe that we have just witnessed the (well-known) fact that a geometric sum
of exponentially distributed random variables is itself exponentially distributed. The probability
density function of queueing time may be found by differentiating Equation (14.18). We have
wq(t) = d
dt
#
1 −ξe−μ(1−ξ)t$
= ξμ(1 −ξ)e−μ(1−ξ)t.
This allows us to compute the mean queueing time as
Wq = E[Tq] =
ξ
μ(1 −ξ).
(14.19)
Of course, all these results could have been obtained directly from the corresponding results for the
M/M/1 queue by simply replacing ρ with ξ. For example, we saw that the mean waiting time in

14.9 The G/M/1/K Queue
551
the M/M/1 queue is given by
Wq =
λ
μ(μ −λ).
Since λ/μ = ρ, replacing λ with μξ gives Equation (14.19).
For the sake of completeness, we provide also the probability distribution and density functions
for the response time (total system time) in a G/M/1 queue. By analogy with the corresponding
results in an M/M/1 queue, we ﬁnd
W(t) = 1 −e−μ(1−ξ)t,
t ≥0,
and
w(t) = μ(1 −ξ)e−μ(1−ξ)t,
t ≥0.
Example 14.20 Returning to Example 14.19, we can write the distribution function of this G/M/1
queue as
Wq(t) = 1 −

2 −
√
2

e−2(1−2+
√
2)t = 1 −

2 −
√
2

e−2(
√
2−1)t,
t ≥0,
which has the value
√
2 −1 when t = 0. The mean time spent waiting in this system is
Wq =
ξ
μ(1 −ξ) =
2 −
√
2
2(
√
2 −1)
= 0.707107.
14.9 The G/M/1/K Queue
When we examined the M/G/1/K queue we saw that the matrix of transition probabilities is of size
K × K, since the number of customers that a departing customer leaves behind has to lie between
zero and K −1 inclusive. In particular, a departing customer could not leave K customers behind.
In the G/M/1/K queue, the transition probability matrix is of size (K + 1) × (K + 1) since an arrival
can ﬁnd any number of customers present between zero and K inclusive. Of course, when an arrival
ﬁnds K customers present, that customer is not admitted to the system, but is lost. It follows then
that the matrix of transition probabilities for the embedded Markov chain is given by
F(K) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1 −β0
β0
0
0
· · ·
0
1 −1
i=0 βi
β1
β0
0
· · ·
0
1 −2
i=0 βi
β2
β1
β0
· · ·
0
...
...
...
...
...
...
1 −K−1
i=0 βi
βK−1
βK−2
βK−3
· · ·
β0
1 −K−1
i=0 βi
βK−1
βK−2
βK−3
· · ·
β0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
For example, if an arrival ﬁnds two customers already present (row 3 of the matrix), the next arrival
can ﬁnd three customers (with probability β0 there were no service completions between the two
arrivals), or two customers (with probability β1 there was a single service completion between the
two arrivals), or one or no customers present. The last two rows are identical because whether an
arrival ﬁnds K −1 or K customers present, the next arrival can ﬁnd any number between K and 0
according to the probabilities β0 through 1 −K−1
i=0 βi.

552
The M/G/1 and G/M/1 Queues
The probabilities πi, i = 0, 1, . . . , K, which denote the probabilities of an arrival ﬁnding i
customers, can be obtained from the system of equations π(F(K) −I) = 0 by means of a reverse
recurrence procedure. The last equation is
πK = β0πK−1 + β0πK
which we write as
β0πK−1 + (β0 −1)πK = 0.
If we assign a value to the last component of the solution, such as πK = 1, then we can obtain
πK−1 as
πK−1 = 1 −β0
β0
.
With the value thus computed for πK−1, together with the chosen value of πK, we can now use the
second to last equation to obtain a value for πK−2. This equation, namely,
β0πK−2 + (β1 −1)πK−1 + β1πK = 0,
when solved for πK−2 gives
πK−2 = (1 −β1)πK−1
β0
−β1
β0
.
The next equation is
β0πK−3 + (β1 −1)πK−2 + β2πK−1 + β2πK = 0,
which when solved for πK−3 gives
πK−3 = (1 −β1)πK−2
β0
−β2πK−1
β0
−β2
β0
.
In this manner, a value can be computed for all πi, i = 0, 1, . . . , K. At this point a ﬁnal
normalization to force the sum of all K + 1 components to be equal to 1 will yield the correct
stationary probability distribution of customers at arrival epochs in a G/M/1/K queue.
Example 14.21 Consider a D/M/1/5 queue in which customers arrive at a rate of one per unit time
and for which the mean of the exponential service time is equal to 3/4. We ﬁrst compute the elements
of the transition probability matrix from the relation
βi = μi
i! e−μ = (4/3)i
i!
e−4/3 = (4/3)i
i!
0.263597.
This gives the following values for βi, i = 0, 1, . . . , 5:
β0 = 0.263597,
β1 = 0.351463,
β2 = 0.234309,
β3 = 0.104137,
β4 = 0.034712,
β5 = 0.009257.

14.10 Exercises
553
The transition probability matrix is therefore given by
F(K) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1 −β0
β0
0
0
0
0
1 −1
i=0 βi
β1
β0
0
0
0
1 −2
i=0 βi
β2
β1
β0
0
0
1 −3
i=0 βi
β3
β2
β1
β0
0
1 −K−1
i=0 βi
β4
β3
β2
β1
β0
1 −K−1
i=0 βi
β4
β3
β2
β1
β0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.736403
0.263597
0.0
0.0
0.0
0.0
0.384940
0.351463
0.263597
0.0
0.0
0.0
0.150631
0.234309
0.351463
0.263597
0.0
0.0
0.046494
0.104137
0.234309
0.351463
0.263597
0.0
0.011782
0.034712
0.104137
0.234309
0.351463
0.263597
0.011782
0.034712
0.104137
0.234309
0.351463
0.263597
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We now set π5 = 1.0 and apply the reverse recurrence procedure to the system of equations
π(F(K) −I) = 0. This gives
π4 = 1 −β0
β0
= 2.793668,
π3 = (1 −β1)π4
β0
−β1
β0
= 5.540024,
π2 = (1 −β1)π3
β0
−β2π4
β0
−β2
β0
= 10.258164,
and so on. Continuing the process yields
π1 = 18.815317 and ﬁnally π0 = 34.485376.
The sum of all ﬁve elements is given by
∥π∥1 = 72.892549,
so that, dividing through by this sum, we obtain the ﬁnal answer
π = (0.473099, 0.258124, 0.14073, 0.076003, 0.038326, 0.013719).
It is important to notice that the elements become excessively larger and care must be taken
to preserve numerical accuracy as the size of the matrix grows. This may be accomplished by
periodically renormalizing the partial solution obtained to that point.
14.10 Exercises
Exercise 14.2.1 Show that the following two conditions are both necessary and sufﬁcient for the transition
probability matrix of the M/G/1 queue, i.e., the matrix F of Equation (14.2), to be irreducible.
(a) α0 > 0,
(b) α0 + α1 < 1.

554
The M/G/1 and G/M/1 Queues
Exercise 14.2.2 Consider an M/D/1 queue for which λ = 0.8 and μ = 2.0. Find
(a) the probability that the system contains exactly four customers;
(b) the probability that it contains more than four customers.
Exercise 14.2.3
A company has a very large number of computers which fail at a rate of two per 8-hour
day and are sent to the company’s repair shop. It has been experimentally veriﬁed that the occurrence of these
failures satisﬁes a Poisson distribution. It has also been observed that the failures fall into three categories: the
ﬁrst which happens 60% of the time requires exactly one hour to ﬁx; the second which occurs 30% of the time
requires 3 hours to ﬁx. The third type of failure requires 5 hours to repair. Find αi, the probability of i arrivals
to the repair shop during an average repair time and the probability that the repair shop has more than three
computers waiting for or undergoing service.
Exercise 14.2.4
A university professor observes that the arrival process of students to her ofﬁce follows a
Poisson distribution. Students’ questions are answered individually in the professor’s ofﬁce. This professor has
calculated that the number of students who arrive during the time she is answering the questions of a single
student has the following distribution:
Prob{0 students arrive} = 0.45,
Prob{1 student arrives} = 0.40,
Prob{2 students arrive} = 0.10,
Prob{3 students arrive} = 0.05.
Using the fact that the trafﬁc intensity ρ is equal to the expected number of arrivals during one service, ﬁnd the
probability of having exactly three students waiting outside the professor’s ofﬁce.
Exercise 14.3.1 It was shown that the probability of k arrivals to an M/G/1 queue during the service time of
an arbitrary customer is gven by
αk = Prob{A = k} =
 ∞
0
(λx)k
k! e−λxb(x)dx.
We also know that the jth derivative of the Laplace transform of the probability density function of a random
variable evaluated at zero is equal to the jth moment of the distribution multiplied by (−1) j.
(a) Form the z-transform for the random variable A, denote it by V (z), and show that V (z) = B∗(λ−λz),
where B∗is the Laplace transform of the probability density function of service time.
(b) Compute the ﬁrst derivative of V (z) with respect to z, evaluate it at z = 1 and show that V ′(1) = ρ.
(c) Show that the second derivative of V (z) with respect to z, evaluated at z = 1, is equal to λ2x2.
(d) Finally, using the fact that successive derivatives of V (z) evaluated at z = 1 give factorial moments,
show that E[A2] = ρ + λ2E[S2] = ρ + λ2σ 2
s + ρ2.
Exercise 14.3.2 Use the Pollaczek-Khintchine mean value formula to ﬁnd the mean number of customers in
an M/Er/1 queue with arrival rate λ and mean service time equal to 1/μ.
Exercise 14.3.3 Consider an M/H2/1 queue. Customers arrive according to a Poisson distribution with
parameter λ. These customers receive service at the hyperexponential server at rate λ with probability 0.25
or at rate 2λ with probability 0.75.
(a) Write down the density function for the hyperexponential server.
(b) Compute the mean, variance, and squared coefﬁcient of variation of the service time.
(c) Use the Pollaczek-Khintchine mean value formula to compute the mean number of customers in this
system.
(d) Compare this result with the results obtained from an M/M/1 queue and an M/D/1 queue whose mean
service times are the same as that of the hyperexponential server.
Exercise 14.3.4 Six out of every ten patrons of a hairdressing salon are female and and the average time they
require to have their hair done is 50 minutes. Male customers require only 25 minutes on average. If on average
a new customer arrives every hour, what is the expected number of customers in the salon and how long does

14.10 Exercises
555
the average patron spend there? How many minutes on average does a customer wait for his/her turn with the
hairdresser? Assume that the arrival process is poisson.
Exercise 14.3.5 A manufacturing process that deposits items onto a production line sometimes fails and
produces less than perfect samples. These imperfect items are detected during their progress through the factory
and are automatically transferred to a repair station where they are repaired and returned to the production line.
It has been observed that, on average, 32 faulty items are detected and repaired each day and that the mean
repair time is 8 minutes with a variance of 25. Furthermore, it has been estimated that each hour an item spends
in the repair facility costs the company $240 (per item in the facility). Management has recently become aware
of the existence of a greatly improved repair machine, which can repair items in an average of two minutes with
variance also equal to two. However, this machine costs $400,000 to purchase and has an additional operating
cost of $150,000 per year. Should the company purchase this new machine if it wishes to recoup its cost in two
years? Assume that arrivals to the repair facility have a Poisson distribution and that the factory operates for 8
hours per day, 250 days per year.
Exercise 14.3.6 Arrivals to a service center follow a Poisson process with mean rate λ = 1/4. The probability
density function of the service mechanism is given by
b(x) = 2/x3
if x ≥1,
and is zero otherwise. Show that ρ = 1/2, which means that the queueing system is stable, but that in spite
of this, the expected number in the system is inﬁnite. (This awkward situation arises because the stability of
a queueing system is deﬁned in terms of the ﬁrst moments of the arrival and service processes, while in the
M/G/1 queue, increased variance in service time leads to increased queue lengths: the variance of the service
process in this example is inﬁnite!)
Exercise 14.3.7 Consider an M/H2/1 queue in which the arrival rate is given by λ = 1. The ﬁrst of the two
server phases is chosen with probability α = 1/4 and has a mean service time given by 1/μ1 = 1; the second
service phase is selected with probability 1 −α and the mean service time at this phase is given by 1/μ2 = .5.
Find the Pollaczek-Khintchine transform for the distribution of customers in this queue, invert this transform
and then compute the probability that the system contains at least two customers.
Exercise 14.4.1 Derive the following version of the Pollaczek-Khintchine mean value formula:
E[R] =
ρ
(1 −ρ) E[R] + E[S].
Exercise 14.4.2 Find the expected residual service time, conditioned on the server being busy, when the service
time distribution is
(a) uniformly distributed between 2 and 8 minutes;
(b) distributed according to a two-phase Erlang distribution in which the mean time spent at each phase
is 2 minutes;
(c) distributed according to a two-phase hyperexponential distribution in which the mean service time at
phase 1 is equal to 40 minutes and that of phase 2 is equal to 16 minutes. A customer entering service
chooses phase 1 with probability α = .25.
Exercise 14.4.3 Customers of three different classes arrive at an M/G/1 queue according to a Poisson process
with mean interarrival time equal to 6 minutes. On average, eight out of every twenty customers belong to class
1; eleven out of every twenty belong to class 2 and the remainder belong to class 3. The service requirements of
each class are those of the previous question—the service time distribution of customers of class 1 is uniformly
distributed on [2, 8]; those of class 2 have an Erlang-2 distribution and those of class 3 are hyperexponentially
distributed and the parameters of these latter two distributions are given in the previous question. Find the mean
residual service time.
Exercise 14.5.1 Use the transform equation
G∗(s) = B∗[s + λ −λG∗(s)]
to compute the expectation, second moment and variance of the busy period distribution in an M/G/1 queue.

556
The M/G/1 and G/M/1 Queues
Exercise 14.5.2 Consider an M/G/1 queue in which the arrival rate is given by λ = 0.75 and in which the
mean service time is equal to 1/μ = 1. Find the mean and variance of the busy period when the service time
distribution is (a) exponential, (b) Erlang-2, (c) Erlang-4, and (d) constant.
Exercise 14.5.3 In the M/G/1 queue of Exercise 14.3.4, ﬁnd
(a) the mean and variance of the busy period and
(b) the mean and variance of the number of customers served during the busy period.
Exercise 14.6.1 Customers of two different types arrive at an M/M/1 queue. Customers of class 1 arrive at a
rate of ﬁfteen per hour while those of class 2 arrive at a rate of six per hour. The mean service requirement of
all customers is 2 minutes. Find the mean response time when the customers are handled in ﬁrst-come, ﬁrst-
served order. How does this performance measure change when class 1 customers are given (a) nonpreemptive
priority and (b) preempt-resume priority, over class 2?
Exercise 14.6.2 This time the M/M/1 queue of the previous exercise is changed so that class 2 customers
require 4 minutes of service. All other parameters remain unchanged. Find both the mean response time and
the mean number of customers present under all three possible scheduling policies: FCFS, preempt-resume
priority, and nonpreemptive priority. Summarize your results in a table.
Exercise 14.6.3 Travelers arrive individually at the single check-in counter operated by a local airline
company. Coach-class travelers arrive at a rate of 15 per hour and each requires exactly 2 minutes to get
boarding tickets and luggage checked in. First-class customers arrive at a rate of four per hour and since they
need be pampered, require a ﬁxed time of 5 minutes to check-in. Lastly, at a rate of two per hour, late-arriving
customers, about to miss their ﬂights, require an exponentially distributed amount of time with a mean check-in
time of 1 minute. The arrival process for all types of customers is Poisson.
(a) The airline company, in the interest of fairness to all, at ﬁrst operated on a ﬁrst-come, ﬁrst-served
basis. Determine the mean time spent by a traveler at the check-in facility while this policy was
enforced.
(b) As the airline company began to lose money, it decided to implement a priority policy whereby ﬁrst-
class customers have nonpreemptive priority over coach travelers and late-arriving customers have
preemptive priority over both ﬁrst- and coach-class customers. Find the mean time spent at the check-
in facility by an arbitrary traveler under this new system.
Exercise 14.6.4 Computer programs (jobs) be to executed arrive at a computing unit at a rate of 24 every
hour. On average, two of these jobs are compute intensive, requiring E[S3] = 10 minutes of compute time and
having a standard deviation equal to 6.0; 12 of the jobs are regular jobs with E[S2] = 2 minutes and a standard
deviation equal to 3.0; the remaining jobs are short jobs with E[S1] = .5 and having a standard deviation equal
to 1. What is the average response time for a typical job? If short jobs are given the highest priority and long
jobs the lowest priority, ﬁnd the response time for all three job classes under both the preempt-resume policy
and the nonpreemptive policy.
Exercise 14.6.5 Printed circuit boards arrive at a component insertion facility according to a Poisson
distribution at a rate of one every 6 seconds. The number of components to be inserted onto a board has a
geometric distribution with parameter p = 0.5. The time to insert each component is a constant τ seconds.
(a) Compute the mean, second moment, and variance of the time required to insert the components onto
an arbitrary printed circuit board.
(b) Assuming that the printed circuit boards are serviced in ﬁrst-come, ﬁrst-served order, ﬁnd the mean
time spent by an arbitrary circuit board at the insertion facility.
(c) Now assume that the servicing of printed circuit boards requiring the insertion of only one component
(called type-1 circuit boards) are given nonpreemptive priority over the servicing of all other circuit
boards (called type-2 circuit boards). Compute the mean response time of type-1 and type-2 circuit
boards in this case.
(d) Given that τ = 2, ﬁnd the mean response time of an arbitrary printed circuit board under the
nonpreemptive priority scheduling of part (c) and compute the relative improvement over the FCFS
order.

14.10 Exercises
557
Exercise 14.6.6 Consider an M/G/1 queue operating under the SPTF scheduling policy. The Poisson arrival
process has parameter λ = 1/10 and the service time is uniformly distributed over [2, 8]. Write an expression
for the mean time spent waiting to enter service for a customer whose service time is τ, with 2 ≤τ ≤8.
Compute the expected waiting time for τ = 2, 4, 6, and 8. Compare this with the mean waiting time when the
FCFS scheduling is used instead of SPTF.
Exercise 14.6.7 Consider an M/M/1 queue with arrival rate λ, service rate μ and which operates under the
SPTF scheduling algorithm. Derive an expression for the mean waiting time in this queueing system.
Exercise 14.7.1 Use the recursive approach to compute the distribution of customers at departure instants in
the M/D/1/4 queue of Example 14.16.
Exercise 14.7.2 Assume that the repair shop of the company described in Exercise 14.2.3 can hold at most
four failed computers. Broken computers that arrive when the workshop is full are sent to an external facility
(which turns out to be very expensive). Find the probability that a broken computer is shipped to an external
facility.
Exercise 14.7.3 Assume in the scenario of Exercise 14.2.4 that a student who arrives and ﬁnds ﬁve students
already present becomes discouraged and leaves. Compute the probability of this happening.
Exercise 14.8.1 Consider an E2/M/1 queue in which each exponential phase of the arrival process has mean
rate λi = 2.0, i = 1, 2, and the mean service rate is μ = 3.0. Compute the probability of having at least three
customers in the system.
Exercise 14.8.2 Generalize the previous question by showing that the root ξ for an E2/M/1 queue whose
exponential arrival phases each have rate 2λ and for which the exponential service has mean service time 1/μ
is given by
ξ = 1 + 4ρ −√1 + 8ρ
2
.
Prove that this value of ξ satisﬁes 0 < ξ < 1 whenever ρ < 1.
Exercise 14.8.3 At the outbreak of a serious infectious disease in a remote part of the globe, volunteer doctors
and nurses work in pairs (doctor plus nurse) to provide some form of relief. They arrange their service, one
volunteer pair per affected village, in such a way that the population of the village ﬁrst must pass some
preliminary tests with the nurse before being seen by the doctor. These preliminary tests, of which there are
two, may be assumed to be exponentially distributed with means of 1/4 and 1/6 hours, respectively. The time
spent with the doctor may be assumed to be exponentially distributed with mean service time equal to 1/9
hours.
Explain how the system at a single village may be modeled as a G/M/1 queue and compute the mean waiting
time a villager spends between leaving the nurse and being called by the doctor.
Exercise 14.8.4 In the scenario of the previous question, suppose that the doctor can assign some of his work
to the nurse, so that the nurse must now perform three tasks (assumed exponential) with mean durations of 4,
6, and 2 minutes, and the time now needed with the doctor is reduced to 8 minutes. What is the time now spent
by patients between nurse and doctor?
Exercise 14.8.5 Arrivals to a H2/M/1 queue choose the ﬁrst exponential phase which has rate λ1 = 2 with
probability α1 = 0.75; the rate at the second exponential phase is λ2 = 1. The service time is exponentially
distributed with rate μ = 2. Compute the probability that this queueing system contains exactly three
customers, and ﬁnd the mean response time.
Exercise 14.8.6 Consider the situation in which the arrival process in a G/M/1 queue is given in terms of a
discrete probability distribution containing k points. Write down the functional equation that must be solved to
obtain the parameter ξ.

558
The M/G/1 and G/M/1 Queues
Consider now the speciﬁc case in which the distribution of the arrival process in a G/M/1 queue is given in
the following table.
ti
a(t)
A(t)
1
0.2
0.2
2
0.3
0.5
3
0.4
0.9
4
0.1
1.0
If the exponential service time distribution has a mean of 1/μ = 2.0, what is the mean number of customers in
this system as seen by an arrival, and what is the mean response time?

Chapter 15
Queueing Networks
15.1 Introduction
15.1.1 Basic Deﬁnitions
So far, we have considered queueing systems in which each customer arrives at a service center,
receives a single service operation from this center, and then departs, never to return. This is
sometimes called a “single-node” system. A “multiple-node” system is one in which a customer
requires service at more than one node. Such a system may be viewed as a network of nodes,
in which each node is a service center having storage room for queues to form and perhaps with
multiple servers to handle customer requests. Throughout this chapter we shall use the words “node”
and “service center” interchangeably. Customers enter the system by arriving at one of the service
centers, queue for and eventually receive service at this center, and upon departure either proceed
to some other service center in the network to receive additional service, or else leave the network
completely.
It is not difﬁcult to envisage examples of such networks. When patients arrive at a doctor’s ofﬁce,
they often need to ﬁll out documentation, for insurance purposes or to update their medical records,
then it’s off to the nurse’s station for various measurements like weight, blood pressure, and so on.
The next stop is generally to queue (i.e., wait patiently) for one of the doctors to arrive and begin the
consultation and examination. Perhaps it may be necessary to have some X-rays taken, an ultrasound
may be called for, and so on. After these procedures have been completed, it may be necessary to
talk with the doctor once again. The ﬁnal center through which the patient must pass is always the
billing ofﬁce. At each of these different points, the patient may have to wait while other patients are
being treated. Thus the patient arrives, receives service/treatment at a number of different points,
and ﬁnally leaves to return to work or to go home.
A number of new considerations arise when one considers networks of queues. For instance,
the topological structure of the network is important since it describes the permissible transitions
between service centers. Thus it would be inappropriate for a patient to leave the nurse’s station
and go directly for an X-ray before consulting with the doctor. Also, the path taken by individual
customers must somehow be described. Some patients may leave the nurse’s station to see Dr.
Sawbones while others may leave the same nurse’s station to see Dr. Doolittle. Some of Dr.
Sawbones’ patients may need to go to the X-ray lab, others to have an ultrasound, while others
may be sent directly to the billing ofﬁce.
In queueing networks we are faced with a situation in which customers departing from one
service center may mix with customers leaving a second service center and the combined ﬂow may
be destined to enter a third service center. Thus there is interaction among the queues in the network
and this ultimately may have a complicating effect on the arrival process at downstream service
centers. Customer arrival times can become correlated with customer service times once customers
proceed past their point of entry into the network and this correlation can make the mathematical
analysis of these queues difﬁcult.

560
Queueing Networks
Another difﬁculty is that of the actual customer service requirements. In the example of the
doctors’ ofﬁce, it is reasonable to assume that the service required at one service center is relatively
independent of the service required at other service centers. However, for other queueing models
this may not be the case. Consider a queueing network representing a data network of transmission
lines and routers. Messages pass through the network from one point to another, and may need to
transit through several routers on their journey. Messages are queued at the routers while waiting
for the transmission line to become free. If we assume that the delay across the transmission line
depends on the size of the message, and that the size of the message does not change, then the service
requirement of a given message must be the same at every transmission line in the message’s journey
across the data network.
Example 15.1 Consider the simple case of two transmission lines in tandem, both having equal
capacity (service rate). This is shown graphically in Figure 15.1.
λ
μ
μ
Figure 15.1. Representation of two transmission lines in tandem.
Assume that the arrival process of packets to the ﬁrst node (transmission point) is Poisson with
rate λ packets/second, and assume furthermore that all packets have equal length with transmission
time equal to 1/μ. This means that the ﬁrst node is essentially an M/D/1 queue and the average
packet delay may be obtained from the Pollaczek-Khintchine formula. Now consider what happens
at the second node. Here the interarrival time cannot be less than 1/μ since this is the time that it
takes a package to move through the ﬁrst node. As long as the ﬁrst node is not empty, packets will
arrive regularly every 1/μ seconds. If the ﬁrst node is empty, a packet will not arrive at the second
node until one arrives at the ﬁrst node and then spends 1/μ seconds in transmission. Thus there is a
strong correlation among interarrival times and packet lengths. But this is not all. Since the packet
transmission times are the same at both nodes, a packet that arrives at the second node will take
exactly the same amount of time that the next packet needs at the ﬁrst node so that the ﬁrst packet
will exit the second node at the same instant that the second packet leaves the ﬁrst node. Thus there
is no waiting at the second node.
The fact that all packets have equal length obviously complicated matters. Even when the packet
lengths are exponentially and independently distributed and are independent of arrival times at the
ﬁrst queue, this difﬁculty remains. It stems from the fact that the service time requirements of a
packet do not change as it moves from the ﬁrst node to the second. The ﬁrst node may be modeled as
an M/M/1, but not the second since the interarrival times at the second node are strongly correlated
with the packet lengths. The interarrival time at the second node must be greater than or equal to the
transmission time at the ﬁrst node. Furthermore, consider what happens to long packets. Typically
they will have shorter waiting times at the second node because the length of time they require to
exit the ﬁrst node gives packets at the second node more time to empty out.
Queueing networks, even the simplest, in which each customer has identical service requirements
at multiple service centers, such as the packets in the previous example, are difﬁcult to solve. We
will not consider this possibility further. Instead we shall assume that the service requirements of
customers change as they move through the network, just as was illustrated in the example of the
doctors’ ofﬁce.
15.1.2 The Departure Process—Burke’s Theorem
Our concern is with the stochastic processes that describe the ﬂow of customers through the network.
If we know the various properties of the variables that are associated with the service centers/nodes,

15.1 Introduction
561
the number of servers, their service time distribution, the scheduling discipline and so on, then the
only item missing is the stochastic process that describes the arrivals. This is linked to the departure
processes from nodes that feed the particular service center as well as the ﬂow of customers directly
from the exterior into the center. For example, in the case of a tandem queueing system where
customers departing from node i immediately enter node i + 1, we see that the interdeparture
times from the former generate the interarrival times of the latter. For this reason, we now examine
departure processes.
Consider an M/G/1 system and let A∗(s), B∗(s), and D∗(s) denote the Laplace transform of the
PDFs describing the interarrival times, the service times, and the interdeparture times, respectively.
Let us calculate D∗(s). When a customer departs from the server, either another customer is
available in the queue and ready to be taken into service immediately, or the queue is empty. In
the ﬁrst case, the time until the next customer departs from the server will be distributed exactly as
a service time and we have
D∗(s)|server nonempty = B∗(s).
On the other hand, if the ﬁrst customer leaves an empty system behind, we must wait for the sum
of two intervals: (a) the time until the next customer arrives and (b) the service time of this next
customer. Since these two intervals are independently distributed, the transform of the sum PDF is
the product of the transforms of the individual PDFs, and we have
D∗(s)|server empty = A∗(s)B∗(s) =
λ
s + λ B∗(s).
In our discussion of the M/G/1 queue, we saw that the probability of a departure leaving behind an
empty system is the same as the stationary probability of an empty system, i.e., 1−ρ. Thus we may
write the unconditional transform for the interdeparture PDF as
D∗(s) = ρB∗(s) + (1 −ρ)
λ
s + λ B∗(s).
In the particular case of exponential service times (the M/M/1 system), the Laplace transform is
given by
B∗(s) =
μ
s + μ,
where μ is the service rate. Substituting this into the previous expression for D∗(s) yields
D∗(s) = λ
μ
μ
s + μ +

1 −λ
μ

λ
s + λ
μ
s + μ
=
λ
s + μ +
λ
s + λ
μ
s + μ −λ
μ
λ
s + λ
μ
s + μ
=
λ(s + λ)
(s + μ)(s + λ) +
λμ −λ2
(s + λ)(s + μ)
=
λ(s + μ)
(s + μ)(s + λ) =
λ
s + λ,
and so the interdeparture time distribution is given by
D(t) = 1 −e−λt,
t ≥0.
Thus we must conclude that the interdeparture times are exponentially distributed with the same
parameter as the interarrival times. This result can be written more generally as Burke’s theorem [6]
which we now give without proof.

562
Queueing Networks
Theorem 15.1.1 (Burke) Consider an M/M/1, M/M/c, or M/M/∞system with arrival rate λ.
Suppose that the system is in steady state. Then the following hold true:
1. The departure process is Poisson with rate λ.
2. At each time t, the number of customers in the system is independent of the sequence of
departure times prior to t.
We shall have need of the second part of Burke’s theorem later. It says that we cannot make any
statements about the number of customers at a node after having observed a sequence of departures.
For example, a spate of closely spaced departures does not imply that the last of these leaves behind
a large number of customers, nor does the end of an observation period having few departures imply
that these departures leave behind a largely empty system. Burke’s theorem simply does not allow
us to draw these conclusions.
As a ﬁnal note, it has been shown that the above systems, (i.e., M/M/1, M/M/c, or M/M/∞) are
the only such FCFS queues with this property. In other words, if the departure process of an M/G/1
or G/M/1 system is Poisson, then G = M.
15.1.3 Two M/M/1 Queues in Tandem
Consider a queueing network with Poisson arrivals and two nodes in tandem as illustrated in
Figure 15.2. As we mentioned a moment ago, we now assume that the service times of a customer
at the ﬁrst and second nodes are mutually independent as well as independent of the arrival process.
We now show that, as a result of this assumption and Burke’s theorem, the distribution of customers
in the two nodes is the same as if they were two isolated independent M/M/1 queues.
2
1
λ
μ
μ
Figure 15.2. Two M/M/1 queues in tandem.
Let the rate of the Poisson arrival process be λ and let the mean service times at servers 1 and
2 be 1/μ1 and 1/μ2, respectively. Let ρ1 = λ/μ1 and ρ2 = λ/μ2 be the corresponding utilization
factors, and assume that ρ1 < 1 and ρ2 < 1. Notice that node 1 is an M/M/1 queue, and so by part
1 of Burke’s theorem, its departure process (and hence the arrival process to node 2) is Poisson.
By assumption, the service time at node 2 is independent of node 1. Therefore node 2 behaves like
an M/M/1 queue, and can be analyzed independently of node 1. Thus, from our earlier results on
M/M/1 queues
Prob{n at node 1} = ρn
1(1 −ρ1),
Prob{m at node 2} = ρm
2 (1 −ρ2).
From part 2 of Burke’s theorem, it follows that the number of customers present in node 1 is
independent of the sequence of earlier arrivals at node 2. Consequently, it must also be independent
of the number of customers present in node 2. This implies that
Prob{n at node 1 and m at node 2} = Prob{n at node 1} × Prob{m at node 2}
= ρn
1(1 −ρ1)ρm
2 (1 −ρ2).
Thus, under steady state conditions, the number of customers at node 1 and node 2 at any given
time are independent and the joint distribution of customers in the tandem queueing system has a
product-form solution.

15.2 Open Queueing Networks
563
15.2 Open Queueing Networks
15.2.1 Feedforward Networks
It is apparent that Burke’s theorem allows us to string together, one after the other in tandem
fashion, any number of multiple-server nodes, each server providing service that is exponentially
distributed, and the decomposition just witnessed continues to apply. Each node may be analyzed in
isolation and the joint distribution obtained as the product of these individual solutions. Furthermore,
more general arrangements of nodes are possible and this product-form solution continues to be
valid. The input to each node in the network will be Poisson if the network is a feedforward network
such as the one illustrated in Figure 15.3. Indeed, since the aggregation of mutually independent
Poisson processes is a Poisson process, the input to nodes that are fed by the combined output
of other nodes is Poisson. Likewise, since the decomposition of a Poisson process yields multiple
Poisson processes, a single node can provide Poisson input to multiple downstream nodes.
2
4
3
6
5
4
3
2
1
1
μ
μ
μ
μ
μ
μ
λ
λ
λ
λ
Figure 15.3. A feedforward queueing network.
Feedforward networks do not permit feedback paths since this may destroy the Poisson nature of
the feedback stream. To see this, consider the simple single-server queue with feedback illustrated in
Figure 15.4. If μ is much larger than λ, then, relatively speaking, customer arrivals from the exterior
occur rather infrequently. These external arrivals are served rather quickly, but then, with rather high
probability, the customer is rerouted back to the queue. The overall effect from the server’s point of
view is that a burst of arrivals is triggered each time an external customer enters the node. Such as
arrival process is clearly not Poisson.
μ
λ
μ   >>  λ
1−ε
ε
Figure 15.4. A single queue with feedback.
15.2.2 Jackson Networks
Burke’s theorem allows us to analyze any feedforward queueing network in which all the servers
are exponential. The work of Jackson [22, 23] shows that, even in the presence of feedback loops,
the individual nodes behave as if they were fed by Poisson arrivals, when in fact they are not. We
now consider Jackson networks. Consider a network consisting of M nodes. The network is open,
i.e., there is at least one node to which customers arrive from an external source and there is at least

564
Queueing Networks
one node from which customers depart from the system. The system is a Jackson network if the
following are true for i, j = 1, 2, . . . , M:
• Node i consists of an inﬁnite FCFS queue and ci exponential servers, each with parameter μi.
• External arrivals to node i are Poisson with rate γi.
• After completing service at node i, a customer will proceed to node j with probability ri j
independent of past history (notice that this permits the case where rii ≥0) or will depart
from the system, never to return again, with probability 1 −M
j=1 ri j.
The service centers in this network may be load dependent, meaning that the rate at which
customers depart from service can be a function of the number of customers present in the node. For
example, when node i contains ci identical exponential servers, each providing service at rate μi,
and there are k customers present in this node, then the load dependent service rate, written μi(k), is
given by μi(k) = min(k, ci)μi. The routing matrix R = [ri j] determines the permissible transitions
between service centers and it is from this matrix that we determine the total average arrival rate of
customers to each center. We shall denote the total average arrival rate of customers to service center
i by λi. This total rate is given by the sum of the (Poisson) arrivals from outside the system plus
arrivals (not necessarily Poisson) from all internal service centers. Thus we obtain the following set
of trafﬁc equations:
λi = γi +
M

j=1
λ jr ji,
i = 1, 2, . . . , M.
(15.1)
In steady state we also have a trafﬁc equation for the network as a whole:
M

i=1
γi = γ =
M

i=1
λi
⎛
⎝1 −
M

j=1
ri j
⎞
⎠.
This simply states that, at equilibrium, the total rate at which customers arrive into the queueing
network from the outside, γ , must be equal to the rate at which they leave the network. Equation
(15.1) constitutes a nonhomogeneous system of linear equations (nonzero right-hand side) with
non-singular coefﬁcient matrix. Its unique solution may be found using standard methods such as
Gaussian elimination and the ith component of this solution is the effective arrival rate into node i.
A Jackson network may be viewed as a continuous-time Markov chain with state descriptor
vector
k = (k1, k2, . . . , kM),
in which ki denotes the number of customers at service center i. At a given state k, the possible
successor states correspond to a single customer arrival from the exterior, a departure from the
network or the movement of a customer from one node to another. The transition from state k to
state
k(i+) = (k1, . . . , ki−1, ki + 1, ki+1, . . . , kM)
corresponds to an external arrival to node i and has transition rate γi. The transition rate from state
k to state
k(i−) = (k1, . . . , ki−1, ki −1, ki+1, . . . , kM)
corresponds to a departure from center i to the exterior and has transition rate μi(ki)(1 −M
j=1 ri j).
Finally the transition from state k to state
k(i+, j−) = (k1, . . . , ki−1, ki + 1, ki+1, . . . , k j−1, k j −1, k j+1, . . . , kM)

15.2 Open Queueing Networks
565
corresponds to a customer moving from node j to node i (possibly with i = j) and has transition
rate μ j(k j)r ji. Let
P(k) = P(k1, k2, . . . , kM)
denote the stationary distribution of the Markov chain. In other words, P(k) is the equilibrium
probability associated with state k. We denote the marginal distribution of ﬁnding ki customers in
node i by Pi(ki). Jackson’s theorem, which we now state without proof, provides the steady-state
joint distribution for the states of the network.
Theorem 15.2.1 (Jackson)
For a Jackson network with effective arrival rate λi to node i, and
assuming that λi < ciμi for all i, the following are true in steady state:
1. Node i behaves stochastically as if it were subjected to Poisson arrivals with rate λi.
2. The number of customers at any node is independent of the number of customers at every
other node.
In other words, Jackson’s theorem shows that the joint distribution for all nodes factors into the
product of the marginal distributions, that is
P(k) = P1(k1)P2(k2) · · · PM(kM),
(15.2)
where Pi(ki) is given as the solution to the classical M/M/1-type queue studied earlier. It is an
example of a product-form solution. In the special case when all the service centers contain a single
exponential server (ci = 1 for all i), the above expression becomes
P(k) =
M

i=1
(1 −ρi)ρki
i
where
ρi = λi
μi
.
The obvious stability condition for a Jackson network is that ρi = λi/(ciμi) < 1 for i =
1, 2, . . . , M. To solve a Jackson network, we therefore ﬁrst need to solve the trafﬁc equations to
obtain the effective arrival rate at each node. Each node is then solved in isolation and the global
solution formed from the product of the individual solutions.
Example 15.2 Consider the open queueing network illustrated in Figure 15.5. Node 1 consists of
two identical exponential servers that operate at rate μ1 = 20. Customers departing from this node
go next to node 2 with probability 0.4 or to node 3 with probability 0.6. Nodes 2 and 3 each have
a single exponential server with rates μ2 = 13.75 and μ3 = 34, respectively. On exiting either of
these nodes, a customer goes to node 4 with probability 1. The last node, node 4 is a pure delay
station at which customers spend an exponentially distributed amount of time with mean equal to
1/7 time units. All arrivals from the exterior are Poisson and occur at rate γ1 = 6 to node 1 and
at rates γ2 = 3 and γ3 = 5 to nodes 2 and 3 respectively. Departures to the exterior are from
node 4 only. Customers departing from this node either depart the network with probability 0.5
or go to node 1 with probability 0.5. Let us compute P(0, 0, 0, 4), the probability that there are 4
customers in the network and that all four are at node 4. Arrivals from the exterior are given by
1
1
2
4
3
1
2
3
μ
μ
μ
μ
μ
γ 
γ
γ 
0.6  
0.4  
Figure 15.5. The open queueing network of Example 15.2.

566
Queueing Networks
γ1 = 6, γ2 = 3, γ3 = 5, and γ4 = 0. The routing probability matrix is
R =
⎛
⎜
⎜
⎝
0
.4
.6
0
0
0
0
1
0
0
0
1
.5
0
0
0
⎞
⎟
⎟
⎠.
We now solve the trafﬁc equations
λi = γi +
4

j=1
λ jr ji,
i = 1, 2, 3, 4.
Writing these in full, we have
λ1 = γ1 + λ1r11 + λ2r21 + λ3r31 + λ4r41 = 6 + .5λ4,
λ2 = γ2 + λ1r12 + λ2r22 + λ3r32 + λ4r42 = 3 + .4λ1,
λ3 = γ3 + λ1r13 + λ2r23 + λ3r33 + λ4r43 = 5 + .6λ1,
λ4 = γ4 + λ1r14 + λ2r24 + λ3r34 + λ4r44 = λ2 + λ3.
Solving this linear system, we ﬁnd its unique solution to be λ1 = 20, λ2 = 11, λ3 = 17, and
λ4 = 28. We use the notation
ρ1 = λ1/(2μ1)
and
ρi = λi/μi,
i = 2, 3, 4,
which gives ρ1 = 20/(2 × 20) = .5, ρ2 = 11/13.75 = .8, ρ3 = 17/34 = .5, and ρ4 = 28/7 = 4.
We wish to ﬁnd the probabilities P(0, 0, 0, 4) = P1(0)P2(0)P3(0)P4(4) where we treat node 1 as an
M/M/2 queue, nodes 2 and 3 as M/M/1 queues and node 4 as an M/M/∞queue. The probability of
zero customers in an M/M/2 queue is obtained using the results of Chapter 12. We ﬁnd
P1(0) =
&
1 + 2ρ1 +
(2ρ1)2
2(1 −ρ1)
'−1
=
&
1 + 1 +
1
2(1 −.5)
'−1
= 1
3.
Since nodes 2 and 3 are M/M/1 queues, we have
P2(0) = 1 −ρ2 = .2
and
P3(0) = 1 −ρ3 = .5.
Finally, the probability of ﬁnding four customers in an M/M/∞queue is
P4(4) = ρn
4e−ρ4
4!
= 44e−4
24
= 10.6667 × .018312 = .1954.
Hence we may conclude that
P(0, 0, 0, 4) = P1(0)P2(0)P3(0)P4(4) = .3333 × .2 × .5 × .1954 = .006512.
Other state probabilities may be computed in a similar manner. For example, we ﬁnd
P(0, 1, 1, 1) = P1(0)P2(1)P3(1)P4(1) = .3333 × .16 × .25 × .07326 = .0009768.
Although arrivals at each service center in a Jackson network are not generally Poisson,
customers arriving at a service center “see” the same distribution of customers in the service center
as a random observer in steady state. This implies that the response time distribution at each node i
is exponential with parameter μi −λi, just like in the isolated M/M/1 system. This does not follow
from Jackson’s theorem since this theorem considers steady state in continuous time. Our concern
here is with the steady state at arrival instants. We have the following theorem.
Theorem 15.2.2 (Random observer property)
In a Jackson queueing network, let P(k) be the
stationary probability of state k. Then the probability that the network is in state k immediately prior
to an arrival to any service center is also P(k).

15.2 Open Queueing Networks
567
15.2.3 Performance Measures for Jackson Networks
In computing performance measure for Jackson networks, we shall assume that all nodes in the
network are M/M/1 queues, that the service rate at node i is ﬁxed and equal to μi, and that the
utilization factor is ρi = λi/μi < 1. The results in the more general case of multiple-server nodes
are readily obtained by extension.
Mean Number of Customers
The mean number of customers in node i, Li, is that of an isolated M/M/1 queue with arrival rate
λi. It is given by
Li =
ρi
1 −ρi
,
i = 1, 2, . . . , M.
It follows that the total mean number of customers in the network is given by
L = L1 + L2 + · · · + L M =
M

i=1
ρi
1 −ρi
.
Example 15.3 Let us return to Example 15.2 and compute Li = E[Ni], i = 1, 2, 3, 4, the mean
number of customers at each of the four service centers. The mean number of customers present at
node 1, an M/M/2 queue, is obtained from the standard formula and using the previously computed
value of P1(0) = 1/3 we ﬁnd
E[N1] = 2ρ1 +
(2ρ1)3
(2 −2ρ1)2 P1(0) = 1 + 1
1 P1(0) = 4
3.
Nodes 2 and 3 are each M/M/1 queues, so
E[N2] =
ρ2
1 −ρ2
= 11/13.75
2.75/13.75 = .8
.2 = 4,
E[N3] =
ρ3
1 −ρ3
= 17/34
17/34 = 1.
Node 4 is an M/M/∞queue which allows us to compute
E[N4] = ρ4 = 28
7 = 4.
The mean number of customers in the network is therefore given by L = E[N1]+ E[N2]+ E[N3]+
E[N4] = 10.3333.
Mean Time Spent in Network
The average waiting times are just as easy to derive. The average time that a customer spends in the
network may be found from Little’s law. It is given by
W = L
γ ,
where γ = M
i=1 γi is the total average arrival rate. The average time that a customer spends at node
i, during each visit to node i, is given by
Wi = Li
λi
=
1
μi(1 −ρi).
Notice that W ̸= W1 + W2 + · · · + WM.

568
Queueing Networks
The average time that a customer spends queueing prior to service on a visit to node i is
W Q
i
= Wi −1
μi
=
ρi
μi(1 −ρi).
Throughput
The throughput of a single node is the rate at which customers leave that node. The throughput of
node i is denoted by Xi and is given by
Xi =
∞

k=1
Pi(k)μi(k),
where μi(k) is the load-dependent service rate. At a service center in a Jackson network, and
indeed in many other queueing networks, the throughput, at equilibrium, is equal to the arrival
rate. However, if a node contains a ﬁnite buffer which causes customers to be lost if full, then not
all customers will be served, and its throughput will be less than its arrival rate.
The throughput of the entire open queueing network is the rate at which customers leave the
network. If customers are not lost to the network, nor combined nor split in any way, then at steady
state, the throughput of the network is equal to the rate at which customers enter the network.
15.3 Closed Queueing Networks
15.3.1 Deﬁnitions
When we visualize a system of queues and servers, we typically think of customers who arrive
from the exterior, spend some time moving among the various service centers and eventually depart
altogether. However, in a closed queueing network, a ﬁxed number of customers forever cycles
among the service centers, never to depart. Although this may seem rather strange, it does have
important applications. For example, it has been used in modeling a multiprogramming computer
system into which only a ﬁxed number of processes can be handled at any one time. If the processes
are identical from a stochastic point of view, then the departure of one is immediately countered
by the arrival of a new, stochastically identical, process. This new arrival is modeled by routing
the exiting process to the entry point of the new process. Similar situations in other contexts
are readily envisaged. Closed Jackson queueing networks are more frequently called Gordon and
Newell [17, 18] networks, after the names of the pioneers of closed queueing networks.
Consider a closed single-class queueing network consisting of M nodes. Let ni be the number of
customers at node i. Then the total customer population is given by
N = n1 + n2 + · · · + nM.
A state of the network is deﬁned as follows:
n = (n1, n2, . . . , nM),
ni ≥0,
M

i=1
ni = N.
The total number of states is given by the binomial coefﬁcient

N + M −1
M −1

.
This is the number of ways in which N customers can be placed among the M nodes. The queueing
discipline at each node is FCFS and the service time distributions are exponential. Let μi be the
mean service rate at node i. When the service rate is load dependent, we shall denote the service
rate by μi(k).

15.3 Closed Queueing Networks
569
Let pi j be the fraction of departures from node i that go next to node j and let λi be the overall
arrival rate to node i. Since the rate of departure is equal to the rate of arrival, when the network is
in steady state, we may write
λi =
M

j=1
λ j p ji,
i = 1, 2, . . . , M,
i.e.,
λ = λP
or
(PT −I)λT = 0.
(15.3)
The linear equations deﬁned by (15.3) are called the trafﬁc equations. Unlike Equation (15.1), which
relates to the situation found in open queueing networks, this system of equations does not have
a unique solution. Observe that P is a stochastic matrix and we ﬁnd ourselves in the situation
discussed when solving Markov chains numerically: the λi can be computed only to a multiplicative
constant.
We now introduce vi, the visit ratio at node i. This gives the mean number of visits to node i
relative to a speciﬁed node, which in our case will always be node 1. Thus v1 = 1 and vi is the mean
number of visits to node i between two consecutive visits to node 1. Notice that vi may be greater
(or smaller) than 1. If Xi(N) is the throughput of node i in a closed network with N customers, then
we must have vi = Xi(N)/X1(N). At steady state, the rate of arrival to a node is equal to the rate
of departure (read throughput) so that we may write vi = λi/λ1 which means that the vi uniquely
satisfy the system of equations
v1 = 1
and
vi =
M

j=1
v j p ji,
i = 1, 2, . . . , M.
Viewed another way, the visit ratios are just the arrival rates normalized so that λ1 = 1.
The major result for this closed queueing network is that the equilibrium distribution is given by
the product form
P(n) = P(n1, n2, . . . , nM) =
1
G(N)
M

i=1
fi(ni),
(15.4)
where
fi(ni) =
vni
i
ni
k=1 μi(k)
if node i is load dependent,
(15.5)
fi(ni) =
 vi
μi
ni
= Y ni
i
if node i is load independent.
(15.6)
The constant G(N) is called the normalization constant. It ensures that the sum of the probabilities is
equal to one. It is apparent from Equation (15.4) that once G(N) is known the stationary distribution
of any state can readily be computed using only G(N) and easily computable network parameters.
To characterize the normalization constant we proceed as follows: Let the set of all possible states
be denoted by S(N, M). Then
S(N, M) =
	
(n1, n2, . . . , nM)

M

i=1
ni = N, ni ≥0, i = 1, 2, . . . , M

and we must have
1 =

S(N,M)
P(n) =
1
G(N)

S(N,M)
M

i=1
fi(ni),

570
Queueing Networks
which implies that
G(N) =

S(N,M)
( M

i=1
fi(ni)
)
.
Notice that, although G(N) is written as a function of N only, it is also a function of M, pi j, vi,
and so on.
15.3.2 Computation of the Normalization Constant: Buzen’s Algorithm
We shall use the convolution algorithm, also referred to as Buzen’s algorithm [7], to evaluate the
normalization constant. To proceed, we deﬁne the auxiliary function
gm(n) =

S(n,m)
m

i=1
fi(ni).
Observe that G(N) = gM(N). Notice also that
gM(n) = G(n),
n = 0, 1, 2, . . . , N,
which is the normalizing constant for the network with only n customers. We now derive a recursive
procedure to compute gm(n). We have
gm(n) =

S(n,m)
m

i=1
fi(ni) =
n

k=0
⎡
⎢⎣

S(n,m)
nm=k
m

i=1
fi(ni)
⎤
⎥⎦
=
n

k=0
fm(k)
⎡
⎣

S(n−k,m−1)
m−1

i=1
fi(ni)
⎤
⎦.
This gives the recursive formula
gm(n) =
n

k=0
fm(k)gm−1(n −k).
(15.7)
Some simpliﬁcations are possible when node i is load independent. In this case, and using
fm(k) = Y k
m = Ym fm(k −1),
we ﬁnd
gm(n) =
n

k=0
Y k
mgm−1(n −k) = Y 0
mgm−1(n) +
n

k=1
Ym fm(k −1)gm−1(n −k)
= gm−1(n) + Ym
n−1

l=0
fm(l)gm−1(n −1 −l),
which gives the result
gm(n) = gm−1(n) + Ymgm(n −1).
(15.8)
Initial conditions for these recursions are obtained from
g1(n) =

S(n,1)
1

i=1
fi(ni) =

{(n)}
f1(n) = f1(n),
n = 0, 1, 2, . . . , N,
(15.9)

15.3 Closed Queueing Networks
571
and
gm(0) = 1,
m = 1, 2, . . . , M.
This latter is obtained from
gm(0) =

S(0,m)
f1(n1) f2(n2) · · · fm(nm)
which, for n = (0, 0, . . . , 0), is just
gm(0) = f1(0) f2(0) · · · fm(0).
For example, in the load-independent case, this becomes
gm(0) = Y 0
1 Y 0
2 · · · Y 0
m = 1.
Equations (15.7) and (15.8) are to be used in computing the normalization constant.
The Convolution Algorithm: Implementation Details
The only value we need from the convolution algorithm is G(N). However, if we look closely at
Equations (15.7) and (15.8), we see that we need to keep a certain number of values during the
execution of the algorithm. For convenience we reproduce these equations side by side. We have,
for the load-independent and load-dependent cases, respectively,
for m = 1, 2, . . . , M :
gm(n) = gm−1(n) + Ymgm(n −1),
n = 0, 1, 2, . . . , N,
LI case,
(15.7)
gm(n) =
n

k=0
fm(k)gm−1(n −k),
n = 0, 1, 2, . . . , N,
LD case.
(15.8)
Notice that the recursion proceeds along two directions, n and m, with n assuming values from 0
through N and m assuming value from 1 through M. The ﬁrst of these corresponds to increasing
numbers of customers in the network, and the second to a progression through the nodes of the
network. All the values of gm(n) may be visualized as the elements in a two-dimensional table
having N + 1 rows and M columns, as illustrated in Figure 15.6.
G(N−1)
1
1
1
1
1
0
1
2
n
N
1
2
m−1
m
M
g  (n)
G(N)
m
1
1
1
1
f (1) 
f (2) 
f (n)
f (N) 
m
n
g  (n−1)
m
m−1
g     (n)
Figure 15.6. Tabular layout for computation of normalization constant.

572
Queueing Networks
The elements may be ﬁlled in one after the other, column by column until the table is completed
and the required element, G M(N), is the ﬁnal value in the last column. The ﬁrst row and column are
obtained from initial conditions: the ﬁrst row contains all ones (since gm(0) = 1, m = 1, 2, . . . , M)
and the ﬁrst column is obtained as g1(n) = f1(n), n = 0, 1, . . . , N. The implementation proceeds
by considering the nodes of the network one after the other. The actual order in which they are taken
is not an immediate concern. As m takes on values from 1 through M, in other words, as we proceed
through all the nodes in the network, we use one or the other of the Equations (15.7) and (15.8),
depending on whether the current value of m corresponds to a load independent or a load dependent
node.
A small disadvantage of this visualization offered by Figure 15.6 is that it may lead one to think
that a two-dimensional array is required to implement Buzen’s algorithm. This is not the case. As we
now show, a single column, a vector of length N +1, sufﬁces. Since each of these equations involves
gm−1 as well as gm, it is apparent that at least some of the values computed for each preceding node
will be needed in the evaluation of the g values for the current node. Let us ﬁrst consider the case
when node m is a load-independent node.
We assume that we have already processed nodes 1 through m −1 and in particular, we have
all the values gm−1(n) for n = 0, 1, . . . , N and that these values are in consecutive locations in a
one-dimensional array. Equation (15.7) is the appropriate equation in the load-independent case:
gm(n) = gm−1(n) + Ymgm(n −1),
n = 0, 1, 2, . . . , N.
The ﬁrst element is the same for all values of m: from the initial conditions we have, for all m,
gm(0) = 1. As successive elements are computed, i.e., as the value of n in gm(n) increases from 0
to 1 to 2 and so on, the newly computed value of gm(n) overwrites the previous value of gm−1(n) in
the array. This is illustrated graphically in Figure 15.7. The ﬁgure shows the situation immediately
before and immediately after the element gm(n) is computed from Equation (15.7). In this ﬁgure, the
current contents of the array are contained within the bold lines. Overwritten values of gm−1(n) are
g   (n−2)
m
g        (n)
g   (n−1)
m
*Y
g        (N)
m−1
g   (1)
m
m
m
g   (2)
g        (0)
g        (1)
g        (2)
m−1
m−1
m−1
g        (n−2)
m−1
g        (n−1)
m−1
g        (n+1)
m−1
g        (n)  +
m−1
g   (n−1)
m
*Y
g   (n)  =
m
g        (0)
g        (1)
g        (2)
g        (n−2)
g        (n−1)
g        (n+1)
g        (N)
m−1
m−1
m−1
m−1
m−1
g        (n)
g   (0)
g   (0)
g   (1)
g   (2)
g   (n−2)
g   (n−1)
m
m
m
m
m
m
g   (n)
m−1
m−1
m−1
Previous values
with new values
m−1
of  g   (j)
of g        (j)
m
now overwritten
j=0, 1, ... , n−1
Next element
to be overwritten
One dimensional array prior 
to computation of g   (n)
One dimensional array after 
computation of g   (n)
m
m
+
m−1
Figure 15.7. Array status for a load-independent node.

15.3 Closed Queueing Networks
573
displayed within dashed lines on the upper left and yet to be computed values of gm(n) are displayed
within dashed lines on the lower right. It is apparent that a single one-dimensional array is all that
is needed in this case.
Let us now consider the case when node m is a load-dependent node, and as before, we assume
that all nodes prior to node m, whether load dependent or load independent, have already been
handled. This time, Equation (15.8) is the appropriate equation to use:
gm(n) =
n

k=0
fm(k)gm−1(n −k),
n = 0, 1, 2, . . . , N.
Observe that, whereas in the load-independent case, we require only a single value from the previous
node, gm−1(n), this time we need n + 1 values, namely, gm−1( j) for j = n, n −1, . . . , 0. It is
instructive to write Equation (15.8) as shown below. Notice that we begin with gm(1), since we know
that gm(0) = 1 for all m:
gm(1) =
fm(0)gm−1(1) + fm(1)gm−1(0),
gm(2) =
fm(0)gm−1(2) + fm(1)gm−1(1) + fm(2)gm−1(0),
gm(3) = fm(0)gm−1(3) + fm(1)gm−1(2) + fm(2)gm−1(1) + fm(3)gm−1(0),
...
gm(N −1) =
fm(0)gm−1(N −1) +
· · ·
+ fm(N −1)gm−1(0),
gm(N) =
fm(0)gm−1(N) + fm(1)gm−1(N −1) + · · · + fm(N)gm−1(0).
This shows that gm−1(1) is needed in computing all gm(n) for n = 1, 2, . . . , N and cannot be
overwritten until all these values have been computed. Likewise, gm−1(2) is needed in computing
all gm(n) for n = 2, 3, . . . , N and cannot be overwritten before then. On the other hand, gm−1(N)
is used only once, in computing gm(N). Once gm(N) has been computed it may be placed into the
g        (n−1)
g        (n)
m
m
m−1
One dimensional array prior 
to computation of g   (n)
One dimensional array after 
computation of g   (n)
m
m
g   (N)
g   (N−1)
g   (n+1)
g   (n)
g        (n−1)
m−1
g       (0)
g       (1)
g       (2)
m−1
m−1
m−1
g   (0)
g   (1)
g   (2)
g   (n−1)
m
m
m
m−1
g        (N)
g        (N−1)
m−1
m
m
g       (0)
g       (1)
g       (2)
m−1
m−1
m−1
g   (0)
m
g   (1)
m
g   (2)
g        (n−1)
m−1
g   (n−1)
m
g   (n+1)
m
g        (N−1)
m−1
g        (N)
m−1
g   (N−1)
m
g   (N)
m
m
m
m−1
g       (n)
m−1
g    (n)
m
g        (n+1)
m−1
Previous values
of g        (j)
m−1
now overwritten
with new values
of  g   (j)
m
j=N, N−1, ..., n+1
to be overwritten
Next element
+
+
+
+
+
* f    (0)
m
* f    (1)
m
* f    (n−2)
m
* f    (n)
m
* f    (n−1)
m
Figure 15.8. Array status for a load-dependent node.

574
Queueing Networks
array location previously occupied by gm−1(N). Also, gm−1(N −1) is used only twice, in computing
gm(N −1) and gm(N). Its position in the array may be overwritten when it is no longer needed, i.e.,
once both gm(N) and gm(N −1) have been computed. In this way, we may proceed from the last
element of the array to the ﬁrst, overwriting elements gm−1( j) with gm( j) from j = N, N−1, . . . , 1.
This is illustrated in Figure 15.8.
There is one instance in which this more complicated approach for handing load dependent
nodes can be avoided—by designating the load dependent node as node number 1. Then the column
(column number 1) is formed from the initial conditions rather than from the convolution formula.
Of course, if there is more than one load dependent node, only one of them can be treated in this
fashion.
One ﬁnal point concerning the convolution algorithm is worth noting. The coefﬁcients fm(n)
may be computed from Equations (15.5) and (15.6) and incorporated directly into the scheme just
described for updating the one-dimensional array. In particular, it is not necessary to generate the
fm(n) in advance nor to store them in a two-dimensional array. It sufﬁces that the values of vi
and μi(k) be available. The complete algorithm, written in pseudocode, is now presented. A Java
implementation is presented at the end of this chapter.
Buzen’s Algorithm
int N;
\\ Number of customers
int M;
\\ Number of nodes
boolean LI[1:M];
\\ True if node m is load independent; otherwise false
double G[0:N];
\\ Array to hold normalizing constants
G[0] = 1.0;
\\ Handle first node first
if (LI[1]) then {
\\ Node 1 is Load Independent
double Y = lambda[1]/mu[1];
for (int n=1; n<=N; n++) {
G[n] = G[n-1]*Y;
}
}
else {
\\ Node 1 is Load Dependent
double Y = lambda[1];
for (int n=1; n<=N; n++) {
G[n] = G[n-1]*Y/mu[1,n]
}
}
for (int m=2, m<=M; m++) {
\\ Handle remaining nodes
if (LI[m]) then {
\\ Node m is Load Dependent
double Y = lambda[m]/mu[m];
\\ Compute coefficient
for (int n=1; n<=N; n++) {
G[n] = G[n] + G[n-1]*Y;
\\ Compute auxiliary function
}
}
else {
\\ Node m is Load Dependent
f[0] = 1;
for (int n=1; n<=N; n++) {
f[n] = f[n-1]*lambda[m]/mu[m,n];
\\ Compute coefficient

15.3 Closed Queueing Networks
575
}
for (int n=N; n>=1; n--) {
double sum = G[n];
for (int k=1; k<=n; k++) {
sum = sum + f[k]*G[n-k];
\\ Compute auxiliary function
}
G[n] = sum;
}
}
Example 15.4
As an example we shall apply Buzen’s algorithm to compute the normalization
constant for the central server model shown in Figure 15.9. In this model, the central server (node 1
in the diagram) possesses two exponential servers each having rate μ = 1.0. The three nodes that
feed the central server (nodes 2, 3, and 4 in the diagram) each possess a single server providing
exponential service at rates 0.5, 1.0, and 2.0 respectively. On exiting the central server, customers
go to node i = 2, 3, 4 with probabilities 0.5, 0.3, and 0.2, respectively. We shall assume that N = 3
customers circulate in this network.
4
3
2
4
3
2
1
1
1
μ
μ
μ
μ
μ
0.5
0.2
0.3
μ   = 1.0
μ   = 0.5
μ   = 1.0
μ   = 2.0
Figure 15.9. Central server queueing model.
The ﬁrst step is to construct the routing matrix and then solve the trafﬁc equations to obtain the
v’s. From the diagram, we see that the routing matrix is given by
R =
⎛
⎜
⎜
⎝
0
.5
.3
.2
1
0
0
0
1
0
0
0
1
0
0
0
⎞
⎟
⎟
⎠.
The trafﬁc equations are given by
(v1, v2, v3, v4, ) = (v1, v2, v3, v4, )
⎛
⎜
⎜
⎝
0
.5
.3
.2
1
0
0
0
1
0
0
0
1
0
0
0
⎞
⎟
⎟
⎠,
which may be written more conveniently as
v1 = v2 + v3 + v4,
v2 = .5v1,
v3 = .3v1,
v4 = .2v1.

576
Queueing Networks
By setting v1 = 1, these equations yield
v1 = 1,
v2 = .5,
v3 = .3,
v4 = .2.
We saw that in the program segment given above that the values of the fm(n) may be incorporated
directly into the algorithm. However, in this example, we shall compute them explicitly in order to
show as clearly as possible, how the normalization constant is formed. Node 1 is a load-dependent
node and hence the values of fm(n) are computed from Equation (15.5). Rewriting this equation
again, we have
fi(ni) =
vni
i
ni
k=1 μi(k)
with i taking the value 1 (node 1) and n1 taking all possible values from 0 through 3 (all possible
number of customers present at node 1). This gives
f1(0) = v0
1/
0

k=1
μ1(k) = 1,
f1(1) = v1
1/μ1(1) = 1/1 = 1,
f1(2) = v2
1/[μ1(1)μ1(2)] = 1/(1 × 2) = 0.5,
f1(3) = v3
1/[μ1(1)μ1(2)μ1(3)] = 1/(1 × 2 × 2) = 0.25.
Nodes 2, 3, and 4 are all load-independent nodes and so we use Equation (15.6):
fi(ni) =
 vi
μi
ni
= (visi)ni.
We obtain
f2(0) = 1,
f3(0) = 1,
f4(0) = 1,
f2(1) = v2/μ2 = .5/.5 = 1,
f3(1) = v3/μ3 = .3/1 = .3,
f4(1) = v4/μ4 = .2/2 = .1,
f2(2) = (v2/μ2)2 = 1, f3(2) = (v3/μ3)2 = (.3)2 = .09, f4(2) = (v4/μ4)2 = (.1)2 = .01,
f2(3) = (v2/μ2)3 = 1, f3(3) = (v3/μ3)3 = (.3)3 = .027, f4(3) = (v4/μ4)3 = (.1)3 = .001.
We now move on to the auxilary functions and the computation of G(N) itself. We will compute the
elements of the array G node by node. All nodes other than the ﬁrst are load independent and hence
successive updates for m = 2, 3, and 4 are obtained from
gm(n) = gm−1(n) + Ymgm(n −1),
with
Y2 = .5/.5 = 1,
Y3 = .3/1 = .3,
Y4 = .2/2 = .1.
The values for node 1 are obtained from the initial conditions, Equation (15.9) having designated
this single load-dependent node as node number 1, precisely to avoid the use of the convolution
formula. We have
g1(n) = f1(n)
for n = 0, 1, 2, . . . , N

15.3 Closed Queueing Networks
577
and so the initial values in G are
g1[0] = 1,
g1[1] = 1,
g1[2] = 0.5,
g1[3] = 0.25.
Given these initial values, we successively obtain
g2[0] = 1,
g3[0] = 1,
g4[0] = 1,
g2[1] = 1 + 1 × 1 = 2,
g3[1] = 2 + .3 × 1 = 2.3,
g4[1] = 2.3 + .1 × 1 = 2.4,
g2[2] = .5+1×2 = 2.5,
g3[2] = 2.5+.3×2.3 = 3.19,
g4[2] = 3.19 + .1 × 2.4 = 3.43,
g2[3] = .25+1×2.5 = 2.75, g3[3] = 2.75+.3×3.19 = 3.707, g4[3] = 3.707+.1×3.43 = 4.05.
We are now in a position to compute the stationary distribution of any state of this queueing network.
For example, the probability of having all three customers at the central server is given by
P(n) = P(3, 0, 0, 0) =
1
G(N)
4

i=1
fi(ni)
=
1
4.05 f1(3) f2(0) f3(0) f4(0) = 0.25
4.05 = 0.0617.
15.3.3 Performance Measures
We now proceed to develop a number of important measures of effectiveness for this type of
queueing network. We shall compute the marginal queue length distributions, the throughput and
utilization of a node and the mean queue lengths.
Marginal Queue Length Distributions
We seek the probability of ﬁnding n customers at node i, which we denote as pi(n, N), for
n = 0, 1, 2, . . . , N. We have
pi(n, N) =

S(N,M)
ni=n
P(n1, . . . , ni−1, n, ni+1, . . . , nM)
for n = 0, 1, 2, . . . , N.
This gives
pi(n, N) =
1
G(N)

S(N,M)
ni=n
M

j=1
f j(n j) = fi(n)
G(N)

S(N,M)
ni=n
M

j=1, j̸=i
f j(n j).
To evaluate this, we use another auxilary function deﬁned as
gi
m(n) =

S(N,m)
ni=N−n
m

j=1, j̸=i
f j(n j).
This is the normalization constant of a new network with node i removed and only n customers.
Thus
pi(n, N) = fi(n)
G(N)gi
M(N −n)
for n = 0, 1, 2, . . . , N.
Notice that
gM
M(n) = gM−1(n)
for n = 0, 1, 2, . . . , N,

578
Queueing Networks
since
gM
M(n) =

S(N,M)
nM=N−n
M

j=1, j̸=M
f j(n j) =

S(n,M−1)
M−1

j=1
f j(n j) = gM−1(n).
We now need an algorithm to compute this new auxilary function. Since the sum of the marginal
probabilities must be equal to 1, we have
1 =
N

k=0
pi(k, N) =
N

k=0
fi(k)
G(N)gi
M(N −k)
and therefore
G(N) =
N

k=0
fi(k)gi
M(N −k).
We may therefore write the recursive formula
gi
M(N) = G(N) −
N

k=1
fi(k)gi
M(N −k).
(15.10)
Observe that this formula also holds when N is replaced by n for any permissible value of n. The
values of the auxilary function may be computed iteratively using the initial conditions
gi
M(0) = G(0) = 1,
fi(0) = 1,
fi(k) =
vi
μi(k) fi(k −1).
Some simpliﬁcations are possible in the load-independent case. We have
gi
M(n) = G(n) −
n

k=1
fi(k)gi
M(n −k)
= G(n) −
n−1

k=0
fi(k + 1)gi
M(n −1 −k)
= G(n) −Yi
n−1

k=0
fi(k)gi
M(n −1 −k)
= G(n) −YiG(n −1)
with G( j) = 0 when j < 0. To summarize, for a load-independent node we have
pi(n, N) =
Y n
i
G(N) [G(N −n) −YiG(N −1 −n)] ,
(15.11)
with G( j) = 0 when j < 0, while for a load-dependent node we have
pi(n, N) = fi(n)
G(N)gi
M(N −n).
(15.12)
Example 15.5 Continuation of Example 15.4. We now compute the marginals for the central server
node. To do so we ﬁrst need to compute g1
M(n) for n = 0, 1, 2, 3. From Equation (15.10), with

15.3 Closed Queueing Networks
579
M = 4, i = 1, and N replaced with n which takes values from 0 through 3, we obtain
g1
4(0) = G(0) = 1,
g1
4(1) = G(1) −f1(1)g1
4(0)
= 2.4 −1 × 1 = 1.4,
g1
4(2) = G(2) −f1(1)g1
4(1) −f1(2)g1
4(0)
= 3.43 −1 × 1.4 −.5 × 1 = 1.53,
g1
4(3) = G(3) −f1(1)g1
4(2) −f1(2)g1
4(1) −f1(3)g1
4(0)
= 4.05 −1 × 1.53 −.5 × 1.4 −.25 × 1 = 1.57.
We are now ready to compute the distribution of the number of customers at the central server.
Setting M = 4 and i = 1 in Equation (15.12) we obtain
p1(0) = f1(0)
G(3) g1
4(3) =
1
4.051.57 = .3877,
p1(1) = f1(1)
G(3) g1
4(2) =
1
4.051.53 = .3778,
p1(2) = f1(2)
G(3) g1
4(1) =
.5
4.051.4 = .1728,
p1(3) = f1(3)
G(3) g1
4(0) = .25
4.051.0 = .0617.
Throughput of a Node
The throughput of a node is deﬁned as the rate at which customers leave that node. Let Xi(N) denote
the throughput of node i in a network with N customers. Then
Xi(N) =
N

n=1
pi(n, N)μi(n) =
N

n=1
fi(n)
G(N)gi
M(N −n)μi(n)
=
N

n=1
vi
μi(n)
fi(n −1)
G(N)
gi
M(N −n)μi(n)
=
vi
G(N)
N

n=1
fi(n −1)gi
M(N −n) =
vi
G(N)
N−1

n=0
fi(n)gi
M(N −1 −n)
= vi
G(N −1)
G(N)
.
In a closed queueing network, the overall throughput of the network is deﬁned as the throughput of
the node for which the visit ratio is equal to 1. Thus
X(N) = X1(N) = Xi(N)
vi
.
It then follows that
X(N) = G(N −1)
G(N)
.
(15.13)

580
Queueing Networks
Example 15.6 Continuing with Example 15.4, let us compute the throughput of the three load-
independent nodes. We have
X2 = .53.43
4.05 = .4235,
X3 = .33.43
4.05 = .2541,
X4 = .23.43
4.05 = .1694.
The overall throughput of the network is
X = X1 = Xi
vi
= 3.43
4.05 = 0.85.
Utilization of a Node
Let Ui(N) denote the utilization of node i in a network with N customers. Then
Ui(N) =
N

n=1
pi(n, N) = 1 −pi(0, N)
= 1 −gi
M(N)
G(N) .
There is a simpler formula available for a load-independent node. Buzen has shown that in this case
Ui(N) = 1
μi
Xi(N)
which implies that, for a load-independent node,
Ui(N) = Yi
G(N −1)
G(N)
.
Example 15.7 Continuation of Example 15.4. The utilizations of the three load-independent nodes
are given by
U2 = X2
μ2
= .4235
.5
= .8470,
U3 = X3
μ3
= .2541
1
= .2541,
U4 = X4
μ4
= .1694
2
= .0847.
Mean Number of Customers at a Node
The mean number of customers at node i in a network with N customers is given by
¯ni(N) =
N

n=1
npi(n, N).

15.3 Closed Queueing Networks
581
This is most easily evaluated for the case of load-independent nodes, where it may be shown that
the mean can be computed from the simple recursive formula
¯ni(N) = Ui(N) [1 + ¯ni(N −1)]
(15.14)
with ¯ni(0) = 0. We prove this in two steps. First we show that
¯ni(N) =
N

n=1
Y n
i
G(N −n)
G(N)
(15.15)
and then, from this equation, we obtain the recursive formula, Equation (15.14). To prove
Equation (15.15) we ﬁrst deﬁne
Qi(n) =

S(N,M)
ni≥n
P(n1, n2, . . . , nM).
Observe that the quantity we require may be written in terms of Qi(n), since
¯ni(N) =
N

n=1
Qi(n).
From the deﬁnition of Qi(n) we have
Qi(n) =

S(N,M)
ni≥n
1
G(N)
M

j=1
f j(n j)
= fi(n)
G(N)

S(N−n,M)
M

j=1
f j(n j)
=
Y n
i
G(N)G(N −n),
and Equation (15.15) immediately follows. We now derive the recursive equation. We have
¯ni(N) =
N

n=1
Qi(n) =
N

n=1
Y n
i
G(N −n)
G(N)
= Yi
G(N −1)
G(N)
+
N

n=2
Y n
i
G(N −n)
G(N)
= Ui(N) +
Yi
G(N)
N−1

n=1
Y n
i G(N −1 −n)
= Ui(N) + Ui(N)
N−1

n=1
Y n
i
G(N −1 −n)
G(N −1)
= Ui(N) [1 + ¯ni(N −1)] ,
which is the result we seek.
The computation is messier for load-dependent servers. It must be evaluted from its deﬁnition.
However, it can be sped up by performing it in conjunction with the evaluation of the normalizing

582
Queueing Networks
constant, if we allow it to be the ﬁnal (i.e., Mth) node. We have
¯nM(N) =
N

n=1
npM(n, N) =
N

n=1
n fM(n)
G(N) gM
M(N −n)
=
1
G(N)
N

n=1
nfM(n)gM−1(N −n).
Notice the similarity between this summation and the last step in computing G(N). Recall that for
G(N) we have
G(N) =
N

n=0
fM(n)gM−1(N −n).
Example 15.8 Continuation of Example 15.4. We now ﬁnd the mean number of customers at the
three load independent nodes in the central server model. We shall use Equation (15.15), which we
may now write as
¯ni = Yi
G(2)
G(3) + Y 2
i
G(1)
G(3) + Y 3
i
G(0)
G(3).
This gives
¯n2 =
.5
.5

× 3.43
4.05 +
.5
.5
2
× 2.4
4.05 +
.5
.5
3
×
1
4.05 = 6.83
4.05 = 1.6864,
¯n3 =
.3
1

× 3.43
4.05 +
.3
1
2
× 2.4
4.05 +
.3
1
3
×
1
4.05 = 1.2721
4.05
= 0.3141,
¯n4 =
.2
2

× 3.43
4.05 +
.2
2
2
× 2.4
4.05 +
.2
2
3
×
1
4.05 = 0.3681
4.05
= 0.0909.
15.4 Mean Value Analysis for Closed Queueing Networks
The convolution algorithm provides a means of computing the complete set of marginal probabilities
of each node in a product-form queueing network. The disadvantage of this method is that it requires
forming the normalization constant and this can be numerically unstable, as well as time consuming.
Furthermore, in some circumstances, the complete set of marginal probabilities is not needed; all
that is required in many cases are mean values, such as the mean number of customers at each node.
The mean value analysis (MVA) approach allows us to compute mean values without having to ﬁrst
compute a normalization constant. This approach is based on the arrival theorem which was proved
independently by Lavenberg and Reiser [28] and by Sevcik and Mitrani [49]. We state this theorem
without proof.
Theorem 15.4.1 (Arrival theorem) In a closed queueing network, the stationary distribution
observed by a customer arriving at a node of the network (having left one node and about to enter
another) is equal to the stationary distribution of customers in the same network with one fewer
customer.
In other words, at the moment of arrival to a node, a customer in a closed queueing network
containing k customers sees the same stationary distribution of customers over the nodes of the

15.4 Mean Value Analysis for Closed Queueing Networks
583
network that an external observer sees when the network contains only k −1 customers. This
theorem hints at a recursion based on the number of customers in the network. Beginning with
an analysis of the network with a single customer, we shall seek to successively add customers one
at a time until we have the results for the network with the required number of customers, N. The
arrival theorem by itself is insufﬁcient to allow us to do this. We also need to use Little’s law, applied
to the overall network as well as to the individual nodes of the network. Thus, the MVA approach is
based on
1. the arrival theorem—to compute the average response time at each node;
2. Little’s law applied to the entire network—to compute the overall throughput of the network
and from it, the throughput of the individual nodes;
3. Little’s law applied to each node—to compute the mean number of customers at each node.
We begin by considering a closed queueing network in which each of the M service nodes
contains a single FCFS exponential server. The service rate at node i is given by μi. We recall
the following notation for the network at steady state and containing k customers:
• Li(k) = E[Ni(k)] is the mean number of customers at node i, for i = 1, 2, . . . , M.
•
Ri(k) is the response time at node i, for i = 1, 2, . . . , M. It is equal to the time spent queueing
plus the time being served.
• Xi(k) is the throughput at node i, for i = 1, 2, . . . , M.
• vi is the visit ratio for node i, for i = 1, 2, . . . , M. It gives the number of times node i is
visited between two successive visits to node 1.
Consider now what the arrival theorem tells us at the instant a customer, departing from some
node in the network, arrives at node i. This customer observes a certain number of other customers
already present. The arrival theorem tells us how many. The mean number already present at the
moment of arrival is equal to the mean number at this node when only k −1 customers circulate in
the network. This mean number is given by Li(k −1). Thus the arriving customer ﬁnds Li(k −1)
customers already present, all of which will be served before the arriving customer, a result of the
FCFS scheduling policy. The arriving customer will wait in the queue until all are served, (i.e.,
Li(k −1)/μi) and will then spend a time period equal to 1/μi being served itself. The average
response time (per visit to node i) for the arriving customer is equal to the sum of these, i.e.,
Ri(k) = 1
μi
(Li(k −1) + 1),
i = 1, 2, . . . , M.
(15.16)
Although it is not intuitively obvious, this result which was derived for exponential servers and
FCFS scheduling, is also applicable to PS nodes and to LCFS-PR nodes and in these cases, the
service time distributions need not be exponential. For IS nodes, no customer waits for service and
in this case the average response time is just equal to the average service time:
Ri(k) = 1
μi
,
i = 1, 2, . . . , M.
We now apply Little’s law to the entire network. The mean number of customers in the network
is simply the ﬁxed number of customers that circulate in the network, i.e., k. By Little’s law this
must be equal to the product of the arrival rate and the mean response time. Since the network is in
steady state, the arrival rate is equal to the overall throughput of the network i.e., the throughput of
node 1 which is X1(k) = X(k). The mean response time is equal to M
i=1 vi Ri(k). Expressing the
throughput in terms of the two other quantities, we have
X(k) =
k
M
i=1 vi Ri(k)
.
(15.17)

584
Queueing Networks
Observe that the throughput of the individual nodes may be computed from this overall throughput.
We have
Xi(k) = vi X(k),
i = 1, 2, . . . , M.
Finally, we need to apply Little’s law to the individual nodes of the network. Equating the arrival
rate to the throughput, Xi(k), at node i, we have
Li(k) = Xi(k)Ri(k) = X(k)vi Ri(k),
i = 1, 2, . . . , M.
(15.18)
With these new values of Li(k), we may return to equation (15.16) and begin all over
again, with k incremented by one. Thus, the MVA algorithm is obtained by recursively applying
Equations (15.16), (15.17), and (15.18) beginning with the boundary conditions
Li(0) = 0,
i = 1, 2, . . . , M.
In this manner we may compute the mean response time, the mean number of customers, as well
as the throughput and utilization at each node of the network. What we cannot compute are the
marginal distributions at each node. For this we need the normalization constant G(N). However,
using the results obtained previously during our discussion of the convolution algorithm, we saw
that the throughput could be written as
X(k) = G(k −1)
G(k)
,
i.e.,
G(k) = G(k −1)
X(k)
.
This may be incorporated into the second step of the MVA algorithm and the normalization constant
computed recursively using the initial condition G(0) = 1. Once the normalization constant has
been obtained, the marginals follow directly from Equation (15.11). We illustrate these concepts by
means of the following example.
Example 15.9 Consider the closed queueing network of the central server type illustrated in
Figure 15.10. Node 1 (the central server) is an inﬁnite server with service rate μ1 = 2.0. Customers
leaving the central server go to node 2, 3, or 4 with probability 0.2, 0.3, and 0.5, respectively. These
nodes have a single exponential server which provide service at rates μ2 = 1.0, μ3 = 2.0, and
μ4 = 4.0 respectively. Customers leaving nodes 2, 3, and 4 return to the central server.
3
2
3
4
2
4
1
μ
μ
μ
0.3 
 8 
μ  = 4.0
0.2 
0.5 
μ  = 2.0
μ  = 1.0 
μ  = 2.0
Figure 15.10. The central server model of Example 15.9.
We begin the recursive process using Equation (15.16) with k = 1 and obtain
R1(1) = μ−1
1
= 0.5,
R2(1) = μ−1
2 (L2(0) + 1) = μ−1
2 (0 + 1) = 1.0,
R3(1) = μ−1
3 (L3(0) + 1) = μ−1
3 (0 + 1) = 0.5,
R4(1) = μ−1
4 (L4(0) + 1) = μ−1
4 (0 + 1) = 0.25.

15.4 Mean Value Analysis for Closed Queueing Networks
585
We next use Equation (15.17) to compute the overall throughput. We have
X(1) =
1
4
i=1 vi Ri(1)
=
1
1.0 × 0.5 + 0.2 × 1.0 + 0.3 × 0.5 + 0.5 × 0.25 =
1
0.975 = 1.0256.
At this point we may now compute G(1). We have
G(1) = G(0)
X(1) =
1
1.0256 = 0.975.
The ﬁnal calculation for this ﬁrst round is to use Equation (15.18) to determine the mean customer
population at each node. We have
L1(1) = X(1)v1R1(1) = 1.0256 × 1.0 × 0.5 = 0.5128,
L2(1) = X(1)v2R2(1) = 1.0256 × 0.2 × 1.0 = 0.2051,
L3(1) = X(1)v3R3(1) = 1.0256 × 0.3 × 0.5 = 0.1539,
L4(1) = X(1)v4R4(1) = 1.0256 × 0.5 × 0.25 = 0.1282.
We are now ready to begin the second round. Returning to Equation (15.16), this time using k = 2,
we obtain
R1(2) = μ−1
1
= 0.5,
R2(2) = μ−1
2 (L2(1) + 1) = μ−1
2 (0.2051 + 1) = 1.2051,
R3(2) = μ−1
3 (L3(1) + 1) = μ−1
3 (0.1539 + 1) = 0.5770,
R4(2) = μ−1
4 (L4(1) + 1) = μ−1
4 (0.1282 + 1) = 0.2821.
Again we use Equation (15.17) to compute the overall throughput. We have
X(2) =
2
4
i=1 vi Ri(2)
=
2
1.0 × 0.5 + 0.2 × 1.2051 + 0.3 × 0.5770 + 0.5 × 0.2821
=
2
1.0552 = 1.8954.
At this point we may now compute G(2). We have
G(2) = G(1)
X(2) = 0.975
1.8954 = 0.5144.
The ﬁnal calculation for this second round is to use Equation (15.18) to determine the mean
population size at each node. We have
L1(2) = X(2)v1R1(2) = 1.8954 × 1.0 × 0.5000 = 0.9477,
L2(2) = X(2)v2R2(2) = 1.8954 × 0.2 × 1.2051 = 0.4568,
L3(2) = X(2)v3R3(2) = 1.8954 × 0.3 × 0.5770 = 0.3281,
L4(2) = X(2)v4R4(2) = 1.8954 × 0.5 × 0.2821 = 0.2674.
The process continues in this fashion until the network contains the desired number of customers.
Tables 15.1 and 15.2 show the results obtained after one more step, k = 3 and with k = 10.
It is interesting to observe that the size of the normalization constant becomes smaller with large
population sizes. Indeed, if we continue the experiments, we ﬁnd that G(20) = 1.3462E–12 and
G(50) = 1.4631E–33. As we mentioned previously, the computation of the normalization constant
can be computationally unstable.

586
Queueing Networks
Table 15.1. Performance measures for k = 3.
Service center
1
2
3
4
Mean number of customers Li(3)
1.3055
0.7608
0.52013
0.4136
Mean response time Ri(3)
0.5000
1.4569
0.6640
0.3168
Throughput Xi(3)
2.6110
0.5222
0.7833
1.3055
Normalization constant G(3)
0.1970052083333333
Table 15.2. Performance measures for k = 10.
Service center
1
2
3
4
Mean number of customers Li(10)
2.3347
4.4554
1.9305
1.2794
Mean response time Ri(10)
0.5000
4.7709
1.3782
0.5480
Throughput Xi(10)
4.6693
0.9339
1.4008
2.3347
Normalization constant G(10)
1.0820657639390957E-5
Finally, to compute the marginal distribution of customers at node 2, for example, we use the
formula for a load-independent node namely,
pi(n, N) =
Y n
i
G(N) [G(N −n) −YiG(N −1 −n)]
with Y2 = .2/1 = .2 and G(0) = 1, G(1) = .975, G(2) = .5144 and G(3) = .1970 to obtain
p2(0, 3) =
.20
G(3) [G(3) −.2G(2)] = .4778,
p2(1, 3) =
.21
G(3) [G(2) −.2G(1)] = .3243,
p2(2, 3) =
.22
G(3) [G(1) −.2G(0)] = .1574,
p2(3, 3) =
.23
G(3) [G(0)] = .0406.
MVA and Load-Dependent Service Centers
There remains one case that we have not yet discussed, the case of load-dependent nodes. This is
the case that arises when a node contains c > 1 identical servers, for example. In our discussions
of the convolution algorithm, we saw that the analysis was somewhat more complex in this case,
and the same is true for the MVA algorithm. The difﬁculty arises in computing the mean response
time in load-dependent nodes. We shall let μiαi(k) be the service rate when k customers are
present. A suitable choice of αi(k) allows us to handle all possible cases. For example, setting
αi(k) = min(ci, k), is what is needed for a node with ci identical servers. Setting αi(k) = βi(k)/μi
sets the service rate to βi(k).
In the load-independent case, it was sufﬁcient to know the mean number of customers present
at a node to compute the mean response time, since the server works at the same rate no matter
how many are present. In a load dependent node, this is no longer the case: the rate of service is a
function of the number of customers present. Now it is necessary to know the probability distribution
of customers. We shall let pi( j, k) denote the equilibrium probability of having j customer present

15.4 Mean Value Analysis for Closed Queueing Networks
587
at node i when a total of k customers circulates in the network. These are the marginal probabilities
of node i in a network of k customers.
Consider what happens when a customer arrives at load-dependent server i and ﬁnds j −1,
j = 1, 2, . . . , k, customers already present. From the arrival theorem, we know that the probability
of this happening is given by pi( j −1, k −1). Once the customer has arrived, the node will contain
j customers all of which must be served. With j customers, the mean service time is 1/μiαi( j). It
follows then that the response time at node i is given by
Ri(k) =
k

j=1
j
μiαi( j) pi( j −1, k −1).
This equation replaces Equation (15.16) when the node is load dependent. Of course this entails
a knowledge of the marginal distribution of customers at a load-dependent node and it is to this
that we now turn our attention. It is fortunate that these too may be computed from the marginal
distribution of customers in a network with one fewer customer. From the results obtained using the
convolution algorithm, Equation (15.12), we saw that the marginal distribution of customers at node
i is given by
pi( j, k) = fi( j)gi
M(k −j)
G(k)
.
Since
fi( j) =
vi
μiαi( j) fi( j −1)
and writing gi
M(k −j) = gi
M(k −1 −j + 1) = gi
M((k −1) −( j −1)), we have
pi( j, k) =
vi
μiαi( j) fi( j −1)gi
M((k −1) −( j −1))
G(k)
=
vi
μiαi( j)
G(k −1)
G(k)
fi( j −1)gi
M((k −1) −( j −1))
G(k −1)
=
Xi(k)
μiαi( j) pi( j −1, k −1).
(15.19)
Thus, given the throughput of a load-dependent node and its marginal probability distribution when
the network has k −1 customers, we can compute the marginals when the network has k customers.
The algorithm is bootstrapped from the initial condition pi(0, 0) = 1, which simply says that
there cannot be any customers at service center i if there are none anywhere in the network. For
any value k > 0, Equation (15.19) allows us to compute the probabilities pi( j, k) for values of
j = 1, 2, . . . , k. This still leaves the problem of computing pi(0, k). However, since the sum of the
marginal probabilities at any node must be 1, i.e., k
j=0 pi( j, k) = 1, we must have
pi(0, k) = 1 −
k

j=1
pi( j, k).
Unfortunately, this subtraction can lead to a loss of signiﬁcance and is a major difﬁculty in
implementing a robust version of the MVA algorithm. Observe that it is not necessary to use a
full two-dimensional array for the computation of these probabilities. A single vector of length
N + 1 is sufﬁcient, provided the computation is conducted from the last value to the ﬁrst. This is
illustrated below, where the double up-arrows indicate the positions into which the new probability
will be placed. Notice that if the insertions are made from right to left, i.e., pi( j, k) is computed

588
Queueing Networks
and inserted before pi( j −1, k), then the values needed in the computation of any pi( j, k) are not
overwritten until they are no longer needed.
[ p(0,0)
⇑
⇑
[ p(0,1)
p(1,1)
⇑
⇑
⇑
[ p(0,2)
p(1,2)
p(2,2)
⇑
⇑
⇑
⇑
[ p(0,3)
p(1,3)
p(2,3)
p(3,3)
We now present an example to illustrate these concepts.
Example 15.10
We shall modify the previous example to include a load dependent server. The
modiﬁed queueing network is shown in Figure 15.11.
2
3
4
1
5
5
5
4
3
2
μ  = 2.0
0.3 
 8 
0.2 
0.5 
μ  = 2.0
μ 
μ 
μ
μ
μ
μ  = 4.0
μ  = 1.0 
μ  = 2.5
Figure 15.11. The modiﬁed queueing network of Example 15.10.
In this example, rather than returning to service center 1, customers departing from service centers
2, 3, and 4 proceed on to the new node. This node, denoted service center 5, contains m5 = 2
identical exponential servers with parameter μ5 = 2.5. Thus the load dependent service rate is
given by μ( j) = μ5α5( j) where α5(1) = 1 and α5( j) = 2 for j > 1. Customers leaving this service
center return to service center 1. As previously, service center 1 is an inﬁnite server with service
rate μ1 = 2.0. Customers leaving this node go to nodes 2, 3, or 4 with probabilities 0.2, 0.3, and 0.5
respectively. These nodes have a single exponential server which provide service at rates μ2 = 1.0,
μ3 = 2.0, and μ4 = 4.0 respectively.
We begin the recursive process with k = 1. From Equation (15.16), we have
R1(1) = μ−1
1
= 0.5,
R2(1) = μ−1
2 (L2(0) + 1) = μ−1
2 (0 + 1) = 1.0,
R3(1) = μ−1
3 (L3(0) + 1) = μ−1
3 (0 + 1) = 0.5,
R4(1) = μ−1
4 (L4(0) + 1) = μ−1
4 (0 + 1) = 0.25,
R5(1) = (α5(1)μ5)−1 p5(0, 0) = μ−1
5
= 0.4,
where we have used the initial condition p5(0, 0) = 1. We next use Equation (15.17) to compute the
overall throughput. We have
X(1) =
1
5
i=1 vi Ri(1)
=
1
1.0 × 0.5 + 0.2 × 1.0 + 0.3 × 0.5 + 0.5 × 0.25 + 1.0 × 0.4 = 0.7273.

15.4 Mean Value Analysis for Closed Queueing Networks
589
At this point we may now compute G(1). We have
G(1) = G(0)
X(1) =
1
.7273 = 1.375.
The next step is to use Equation (15.18) to determine the mean number of customers at each node.
We have
L1(1) = X(1)v1R1(1) = 0.7273 × 1.0 × 0.5 = 0.3636,
L2(1) = X(1)v2R2(1) = 0.7273 × 0.2 × 1.0 = 0.1455,
L3(1) = X(1)v3R3(1) = 0.7273 × 0.3 × 0.5 = 0.1091,
L4(1) = X(1)v4R4(1) = 0.7273 × 0.5 × 0.25 = 0.0909,
L5(1) = X(1)v5R5(1) = 0.7273 × 1.0 × 0.4 = 0.2909.
Before ﬁnishing with k = 1, we must ﬁrst compute the marginals for node 5. In particular we need
p5(0, 1) and p5(1, 1). We ﬁrst compute p5(1, 1) from Equation (15.19). We have
p5(1, 1) =
X5(1)
μ5α5(1) p5(0, 0) = 0.7273
2.5
= 0.2909.
and, from p5(0, 1) = 1 −p5(1, 1), we have p5(0, 1) = 0.7091.
We are now ready to begin the second round, this time with k = 2. We ﬁrst compute the mean
response time for all ﬁve nodes:
R1(2) = μ−1
1
= 0.5,
R2(2) = μ−1
2 (L2(1) + 1) = μ−1
2 (0.1455 + 1) = 1.1455,
R3(2) = μ−1
3 (L3(1) + 1) = μ−1
3 (0.1091 + 1) = 0.5546,
R4(2) = μ−1
4 (L4(1) + 1) = μ−1
4 (0.0909 + 1) = 0.2727,
R5(2) =
1
α5(1)μ5
p5(0, 1) +
2
α5(2)μ5
p5(1, 1)
=
1
1 × 2.5 × 0.7091 +
2
2 × 2.5 × 0.2909 = 0.4.
Again we use Equation (15.17) to compute the overall throughput. We have
X(2) =
2
5
i=1 vi Ri(2)
=
2
1.0 × 0.5 + 0.2 × 1.1455 + 0.3 × 0.5546 + 0.5 × 0.2727 + 1.0 × 0.4
= 1.3968.
At this point we may now compute G(2). We have
G(2) = G(1)
X(2) = 1.375
1.3968 = 0.9844.
The ﬁnal calculation for this second round is to use Equation (15.18) to determine the mean
population sizes. We have
L1(2) = X(2)v1R1(2) = 1.3968 × 1.0 × 0.5000 = 0.6984,
L2(2) = X(2)v2R2(2) = 1.3968 × 0.2 × 1.1455 = 0.3200,
L3(2) = X(2)v3R3(2) = 1.3968 × 0.3 × 0.5546 = 0.2324,
L4(2) = X(2)v4R4(2) = 1.3968 × 0.5 × 0.2727 = 0.1905,
L5(2) = X(2)v5R5(2) = 1.3968 × 1.0 × 0.4000 = 0.5587.

590
Queueing Networks
The marginals at node 5 may now be computed. We have, from Equation (15.19),
p5(1, 2) =
X5(2)
α5(1)μ5
p5(0, 1) = 1.3968
1 × 2.5 × 0.7091 = 0.3962,
p5(2, 2) =
X5(2)
α5(2)μ5
p5(1, 1) = 1.3968
2 × 2.5 × 0.2909 = 0.0813,
and hence p5(0, 2) = 1.0 −p5(1, 2) −p5(2, 2) = 0.5225.
We shall do one more step, now augmenting k to 3. Again the ﬁrst quantities to compute, from
Equation (15.16), are the mean response times. We obtain
R1(3) = μ−1
1
= 0.5,
R2(3) = μ−1
2 (L2(2) + 1) = μ−1
2 (0.3200 + 1) = 1.3200,
R3(3) = μ−1
3 (L3(2) + 1) = μ−1
3 (0.2324 + 1) = 0.6162,
R4(3) = μ−1
4 (L4(2) + 1) = μ−1
4 (0.1905 + 1) = 0.2976,
R5(3) =
1
α5(1)μ5
p5(0, 2) +
2
α5(2)μ5
p5(1, 2) +
3
α5(3)μ5
p5(2, 2)
=
1
1 × 2.5 × 0.5225 +
2
2 × 2.5 × 0.3962 +
3
2 × 2.5 × 0.0813 = 0.4163.
Again we use Equation (15.17) to compute the overall throughput. We have
X(3) =
3
5
i=1 vi Ri(3)
=
3
1.0×0.5+0.2×1.3200+0.3×0.6162+0.5 × 0.2976 + 1.0 × 0.4163
= 1.9816.
The normalization constant is obtained as
G(3) = G(2)
X(3) = 0.9844
1.9816 = 0.4968.
The ﬁnal calculation for this third round is to use Equation (15.18) to determine the means:
L1(3) = X(3)v1R1(3) = 1.9816 × 1.0 × 0.5000 = 0.9908,
L2(3) = X(3)v2R2(3) = 1.9816 × 0.2 × 1.3200 = 0.5231,
L3(3) = X(3)v3R3(3) = 1.9816 × 0.3 × 0.6162 = 0.3663,
L4(3) = X(3)v4R4(3) = 1.9816 × 0.5 × 0.2976 = 0.2949,
L5(3) = X(3)v5R5(3) = 1.9816 × 1.0 × 0.4163 = 0.8249.
The marginals at node 5 may now be computed. We have, from Equation (15.19),
p5(1, 3) =
X5(3)
α5(1)μ5
p5(0, 2) = 1.9816
1 × 2.5 × 0.5225 = 0.4142,
p5(2, 3) =
X5(3)
α5(2)μ5
p5(1, 2) = 1.9816
2 × 2.5 × 0.3962 = 0.1570,
p5(3, 3) =
X5(3)
α5(3)μ5
p5(2, 2) = 1.9816
2 × 2.5 × 0.0813 = 0.0322,
and hence p5(0, 3) = 1.0 −p5(1, 3) −p5(2, 3) −p5(3, 3) = 0.3966.

15.5 The Flow-Equivalent Server Method
591
And so the process continues in this fashion until the network contains the desired number of
customers. Table 15.3 shows the results obtained once we arrive at k = 10. The marginal distribution
of customers at node 5, when the network contains k = 10 customers is shown in Table 15.4.
Table 15.3. Performance measures for k = 10.
Service center
1
2
3
4
5
Mean number of customers Li(10)
2.0122
2.6327
1.3338
0.9391
3.0823
Mean response time Ri(10)
0.5000
3.2710
1.1047
0.4667
0.7659
Throughput Xi(10)
4.0243
0.8049
1.2073
2.0122
4.0244
Normalization constant G(10)
1.0724123796354306E-4
Table 15.4. Marginal distribution at node 5 with k = 10.
n
0
1
2
3
4
5
6
7
8
9
10
p5(n) 0.1009 0.1885 0.1719 0.1519 0.1283 0.1018 0.0739 0.0470 0.0246 0.0093 0.0019
15.5 The Flow-Equivalent Server Method
As a ﬁnal possibility for solving closed queueing networks, we brieﬂy review the so-called “ﬂow-
equivalent server” method. This method is based on Norton’s theorem from electric circuit theory
and was shown to be applicable to queueing networks. In its simplest form, one node in the
network is selected for analysis and the remainder of the network replaced by a single server
whose ﬂow is “equivalent” to that of the part of the network it replaces. This gives rise to a
reduced system consisting of the selected node and the ﬂow-equivalent server. Norton’s theorem
may now be applied to show that the behavior of the selected node is identical in both the reduced
network and in the original network. The ﬂow-equivalent server has load-dependent service rates,
μ(k), k = 1, 2, . . . , N where N is the number of customers who circulate in the network. These
rates are determined by short-circuiting the selected node in the network (simply set its mean service
time to zero) and computing the throughput across that short-circuited path. This throughput is
computed for each possible nonzero number of customers in the network, k = 1, 2, . . . , N and
provides the values for μ(k), k = 1, 2, . . . , N. The visit ratio of both nodes in the reduced network
is taken to be equal to the visit ratio of the selected node in the original network. The normalization
constant, G(N), of the reduced network is also the normalization constant of the original network
and now Equation (15.4) can be used to ﬁnd the stationary probability of any state of the queueing
network. These concepts are best illustrated by means of an example.
1
1
2
3
4
2
3
4
1
μ
μ
μ
μ
μ
μ  = 1.0
μ  = 0.5 
μ  = 1.0 
μ  = 2.0
0.2 
0.5 
0.3 
Figure 15.12. A central server queueing model.

592
Queueing Networks
3
2
3
1
1
2
0.2 
0.5 
0.3 
ν 
ν 
ν 
ν  = 1.0 
ν  = 2.0
ν  = 0.5 
Figure 15.13. The short-circuited model.
Example 15.11 We shall use the model of Example 15.4 which is illustrated (again) in Figure 15.12.
In this model, the central server (node 1 in the diagram) possesses two exponential servers each
having rate μ = 1.0. The three nodes that feed the central server (nodes 2, 3, and 4 in the diagram)
each possess a single server providing exponential service at rates 0.5, 1.0, and 2.0 respectively.
On exiting the central server, customers go to node i = 2, 3, 4 with probabilities 0.5, 0.3, and 0.2,
respectively. We assume that N = 3 customers circulate in this network.
We choose to compute the marginal distribution at the central server using the ﬂow-equivalent
method so our ﬁrst step is to short-circuit this node and compute the throughput across its path for
k = 1, 2, 3 customers. The short-circuited network is shown in Figure 15.13.
Either the convolution algorithm or MVA approach is possible and here we go with the Buzen’s
convolution algorithm. The visit ratios are given by γ1 = .5, γ2 = .3, and γ3 = .2 and we have
ν1 = .5, ν2 = 1, and ν3 = 2. Since the nodes are all load independent, we use the formula
fi(ni) =
γi
νi
ni
,
which allows us to compute
f1(n1) = 1n1,
f2(n2) = .3n2,
f3(n3) = .1n3.
Progressing through the computation of the normalization constant by Buzen’s algorithm, we ﬁnd
the successive columns of G(n) to be
1
1
1
1
1
1.3
1.39
1.417
1
1.4
1.53
1.57
G(0) = 1,
G(1) = 1.4,
G(2) = 1.53,
G(3) = G(N) = 1.57,
so that we can compute the throughput with one, two, and three customers in the network as
X(1) = G(0)/G(1) = .7143,
X(2) = G(1)/G(2) = .9150,
X(3) = G(2)/G(3) = .9745.
These constitute the load-dependent rates at the ﬂow-equivalent server. Note carefully that had the
visit ratio to the central server in the original network been different from 1, then these rates would
have to be multiplied by that visit ratio.
We are now ready to analyze the two-node reduced model. This is shown in Figure 15.14. In this
model the visit ratios are both equal to 1 and the service rates are
μ1(1) = 1,
μ1(2) = 2,
μ1(3) = 2,
μ2(1) = .7143,
μ2(2) = .9150,
μ2(3) = .9745.

15.5 The Flow-Equivalent Server Method
593
(k)
1
1
μ
μ
μ
Figure 15.14. The reduced model.
We now form the quantities fi(ni) for i = 1, 2 and ni = 0, 1, 2, 3. Both nodes being load
dependent, we use
fi(ni) =
vni
i
ni
k=1 μi(k).
This gives
f1(0) = 1, f1(1) =
v1
μ1(1) = 1, f1(2) =
v2
1
μ1(1)μ1(2) = 1/2, f1(3) =
v3
1
μ1(1)μ1(2)μ1(3) = 1/4,
f2(0) = 1, f2(1) =
v2
μ2(1) = 1.4, f2(2) =
v2
2
μ2(1)μ2(2) = 1.53, f2(3) =
v3
2
μ2(1)μ2(2)μ2(3) = 1.57.
We now begin the computation of the normalizing constant by means of
gm(n) =
n

k=0
fm(k)gm−1(n −k),
load-dependent case,
and we obtain the following table.
m = 1
m = 2
n = 0
1
1
n = 1
f1(1) = 1
1 × 1 + 1.4 × 1 = 2.4
n = 2
f1(2) = .5
1 × .5 + 1.4 × 1 + 1.53 × 1 = 3.43
n = 3
f1(3) = .25
1 × .25 + 1.4 × .5 + 1.53 × 1 + 1.57 × 1 = 4.05
Thus we have
G(0) = 1,
G(1) = 2.4,
G(2) = 3.43,
G(N) = G(3) = 4.05.
Observe that the computed value of G(N) is the same as that obtained for the original network in
Example 15.4. Our goal is to compute the marginal distribution at the central server which means
that we must ﬁrst compute g1
M(n) for n = 0, 1, 2, 3 from
g1
M(n) = G(n) −
n

k=1
f1(k)g1
M(n −k).
This gives
g1
M(0) = G(0) = 1,
g1
M(1) = G(1) −f1(1)g1
M(0) = 2.4 −1 × 1 = 1.4,
g1
M(2) = G(2) −f1(1)g1
M(1) −f1(2)g1
M(0) = 3.43 −1 × 1.4 −.5 × 1 = 1.53,
g1
M(3) = G(3)−f1(1)g1
M(2)−f1(2)g1
M(1)−f1(3)g1
M(0) = 4.05−1×1.53−.5×1.4−.25×1 = 1.57.

594
Queueing Networks
It follows that the marginals at the central server are
pi(n, N) = fi(n)
G(N)gi
M(N −n)
for n = 0, 1, 2, 3,
which gives
p1(0, N) = f1(0)
G(N)g1
M(3) =
1
4.05 × 1.57 = .3877,
p1(1, N) = f1(1)
G(N)g1
M(2) =
1
4.05 × 1.53 = .3778,
p1(2, N) = f1(2)
G(N)g1
M(1) =
.5
4.05 × 1.4 = .1728,
p1(3, N) = f1(3)
G(N)g1
M(0) = .25
4.05 × 1 = .0617,
and is identical to the distribution obtained in Example 15.4.
In the example we just studied, a single node was selected for analysis and the rest of the
network was replaced by one ﬂow-equivalent server. This allowed the chosen node to be analyzed
using a reduced two-node tandem network. However, the ﬂow-equivalent method has much wider
applicability: it can be used when a subset of nodes is chosen for analysis. In this case all the nodes in
the selected set are short circuited and the throughput within the remainder of the network computed
for all k = 1, 2, . . . , N. These throughputs constitute the service rates for a ﬂow equivalent server
which replaces the nonselected nodes in the network. An analysis of this reduced network, which
need no longer be a tandem network, provides all required performance measures concerning the
selected nodes.
The ﬂow equivalent method does not have any advantages either in computation time or in
memory requirements over the convolution algorithm or MVA when it is used to analyze a closed
queueing network with a given ﬁxed set of parameters. The real advantages of this method become
apparent in two cases:
1. When the parameters of one node (or a selected subset of nodes) are to be varied while keeping
the parameters of the remaining nodes of the network ﬁxed. Rather than analyzing the entire
network for each different set of parameters of the chosen node(s), the original network can
be replaced with a smaller reduced network in which nodes with nonvarying parameters are
replaced by a single ﬂow equivalent server. Once the service rates of the ﬂow equivalent server
have been computed, all parameter-varying experiments can be conducted on the reduced
network thereby resulting in saving in computation time. The performance measures for the
selected node(s) are the same in the original network and in the reduced network.
2. The ﬂow-equivalent server method provides the basis for a large number of approximation
methods. One such approach is Marie’s method which, in some respects, is similar to an
iterated form of the ﬂow-equivalent server approach.
15.6 Multiclass Queueing Networks and the BCMP Theorem
The convolution and MVA algorithms discussed previously in the context of closed single-class
queueing networks form the basis of numerous approaches to solving more complex models such
as queueing networks with multiple classes of customer, load-dependent arrival rates and routing
probabilities, and, in some cases, having general service time distributions. As its name suggests,

15.6 Multiclass Queueing Networks and the BCMP Theorem
595
a multiclass queueing network allows customers to belong to different classes which may have
different routing probabilities among the nodes of the network. Furthermore, at the same service
facility, customers belonging to one class may be provided service at a different rate from customers
of a different class. When some of the classes are open, meaning that customers of that class enter the
network from the exterior and eventually depart to the exterior, and others are closed, the term mixed
queueing network is used. In some networks customers may change from one class to another as
they move from one node to another. All these possibilities come together in the theorem of Baskett,
Chandy, Muntz, and Palacios, the so-called BCMP theorem, which holds that a network with one or
more of these properties has a product form solution: the stationary distribution of any state can be
written as the product of functions fi(yi) which depend on the number of customers of each class
and on the type of service facility to which node i belongs. Such networks are often referred to
generically as “product-form networks.”
15.6.1 Product-Form Queueing Networks
In previous sections of this chapter we saw that the stationary probability of the states of single-
class open and closed networks could be written as a product of the solutions of the individual
nodes in the network. Thus it is not necessary to solve the global balance equations directly—the
computationally easier approach is to solve for the solution of each of the individual nodes and to
present the global solution as the (possibly normalized) product of the separate solutions. At that
time we did not ask ourselves what it is about these particular networks that makes this approach
work. This is a topic that we now address, because it will enable us to ascertain whether other,
more complex, queueing networks can be handled in a similar manner. Whereas it may be feasible
to generate and solve the global balance equations for relatively simple models having few nodes
and a limited number of customers in the closed case, this becomes computational impossible with
complex models, having many nodes, large customer populations belonging to different classes, and
more advanced scheduling algorithms and general service time distributions. To proceed, we must
ﬁrst introduce the concept of local balance and contrast it with global balance.
Global Balance Equations
We assume that the Markov chain that underlies the queueing network is ﬁnite and ergodic. This
Markov chain has a unique stationary distribution π whose element πi is the probability of ﬁnding
the Markov chain in state i at equilibrium. Its inﬁnitesimal generator, or transition rate matrix,
is denoted by Q. Element qi j, i ̸= j of this matrix gives the rate at which the Markov chain at
equilibrium moves from state i into state j and qii = −
j̸=i qi j. We have previously shown that
π Q = 0, or more explicitly, for the ith equation, that 
all j π jq ji = 0. Hence
−πiqii =

j̸=i
π jq ji
or
πi

j̸=i
qi j =

j̸=i
π jq ji.
Whereas the left-hand side of this equation give the ﬂow out of state i, the right-hand side is the
ﬂow into state i. Thus the ﬂow into and out of state i are balanced. Since this is true for all states i,
the set of equations represented by π Q = 0 are called the global balance equations. The business
of solving a queueing network is the business of computing the stationary probability vector π. One
possibility is to compute this vector directly by solving the global balance equations. However, in
all but the simplest of queueing networks, this is computationally too expensive. In seeking a more
efﬁcient approach, we are led to the topic of local balance.

596
Queueing Networks
Local Balance Equations
Whereas each global balance equation equates the total ﬂow into a state to the total ﬂow out of that
state, local balance equations identify different balanced subﬂows into and out of the same state.
Since they are balanced, each subﬂow into a state is equal to the corresponding subﬂow out of the
state. Furthermore, when summed, the subﬂows revert to the global balanced equation from which
they were extracted. The concept of local balance was introduced by Chandy in 1972 [8].
Deﬁnition 15.6.1 (Local balance) A node of a queueing network is said to have local balance when
the ﬂow out of a state due to the completion of service and subsequent departure of a customer from
the node is equal to the ﬂow into the state caused by the arrival of a customer to that same node.
In multiclass networks, the class of the customer must be taken into account. Thus a node is said
to have local balance when the ﬂow out of a state due to the service completion of a customer of
class r at the node is equal to the ﬂow into that state due to the arrival of a class r customer to that
node.
Example 15.12
Consider the simple central server model shown in Figure 15.15 in which
p1 + p2 = 1. Assume that only two customers circulate in this network. Then there are six states,
namely, (2, 0, 0), (1, 1, 0), (1, 0, 1), (0, 2, 0), (0, 1, 1), and (0, 0, 2), where the state (n1, n2, n3) has
n1 customers at the central server and n2 and n3 customers at nodes 2 and 3, respectively.
1
3
p
p2
2
1
μ
μ
μ
Figure 15.15. A central server queueing model.
The global balance equations are
π Q = (π1, π2, π3, π4, π5, π6)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−μ1
p1μ1
p2μ1
0
0
0
μ2
−(μ1 + μ2)
0
p1μ1
p2μ1
0
μ3
0
−(μ1 + μ3)
0
p1μ1
p2μ1
0
μ2
0
−μ2
0
0
0
μ3
μ2
0
−(μ2 + μ3)
0
0
0
μ3
0
0
−μ3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= 0.
Consider the second global balance equation:
(μ1 + μ2)π2 = p1μ1π1 + μ2π4 + μ3π5.
This separates into the following two local balance equations:
μ2π2 = p1μ1π1,
μ1π2 = μ2π4 + μ3π5.
In order to clarify particular states, we set π1 ≡π(2,0,0), π2 ≡π(1,1,0), . . . , π6 ≡π(0,0,2). The ﬁrst
local balance equation is now written as
μ2π(1,1,0) = p1μ1π(2,0,0),

15.6 Multiclass Queueing Networks and the BCMP Theorem
597
and it is apparent that this equates the ﬂow rate out of state (1, 1, 0) due to a departure (at rate
μ2) from node 2 with the ﬂow into state (1, 1, 0) due to an arrival (at rate p1μ1) into node 2 from
node 1.
The second of the two local balance equations, now written as
μ1π(1,1,0) = μ2π(0,2,0) + μ3π(0,1,1),
shows that the ﬂow out of state (1, 1, 0) due to a departure from node 1 is equal to the ﬂow into
state (1, 1, 0) due to an arrival at node 1. It may also be shown that global balance equations 3 and
5 can each be separated into two local balance equations, while the remaining three global balance
equations are themselves local balance equations. It is a useful exercise to draw the transition
rate diagram for the states of this queueing network and to identify on it, for each global balance
equation, the corresponding local balance equations.
It has been shown that the following four types of queueing node possess local balance.
Type 1: A FCFS node having c servers, each of which provides exponentially distributed service
that is the same for all customer classes. The service rate can depend on the total number
of customers present in the node.
Type 2: A node which operates in processor-sharing (PS) mode. If ni customers are present at PS
node i, all ni are served simultaneously with each receiving 1/ni of the service capacity.
Type 3: An inﬁnite-server (IS) node, in which each customer has its own dedicated server.
Type 4: A last-come, ﬁrst-served preempt-resume (LCFS-PR) node.
In all four types of node, the service rate can be a function of the total number of customers present
in the node. Also, for nodes of type 2–4, the service time distribution can be general subject only
to having a rational Laplace transform and furthermore, customers of different classes can have
different general service time distributions. The rate of service at these nodes (types 2–4) can either
be a function of the total number of customers present in the node, or alternatively the rate of
service for a given customer class can depend on the number of customers of that class present in
the node. We note in passing that the Coxian distribution has a rational Laplace transform and that
any distribution can be modeled arbitrarily closely using a Coxian distribution.
Not all queueing systems have global balance equations that can be separated into a consistent set
of local balance equations. Such is the case when a network has one or more FCFS nodes at which
customers of different classes have different service time distributions. When a solution to the set of
local balance equations can be found, the queueing system itself is said to have local balance and it
follows that the solution of the local balance equations must also be the unique solution of the global
balance equations—after all, each global balance equation is the sum of a subset of local balance
equations. Thus we may think of local balance as being a sufﬁcient, but not necessary, condition
for global balance. Furthermore, it has also been shown that when all the nodes in a network have
local balance, then the network itself has local balance and a solution to the local balance equations
exists.
Although the local balance equations are less complex than the global balance equations, there
are generally more of them and this might have been the end of the story but for a result due to Muntz
[38]. In 1973, Muntz proved that, when a network possesses local balance, the global balance
equations have a product-form solution: the stationary probability of the states of the network
can be written as the product of factors which describe the state of the different nodes. We have
already developed two product-form solutions, namely, Equation (15.2) in our analysis of open
Jackson networks in Section 15.2 and Equation (15.4) when analyzing closed Gordon and Newell
networks in Section 15.3, and seen the substantial beneﬁts that derive from product form. Hence the
importance of local balance. In a network in which all the nodes have local balance, the stationary
distribution of the network can be determined from an analysis of each of the nodes in isolation. It is

598
Queueing Networks
not necessary, and indeed computationally very inefﬁcient, if not impossible, to generate and solve
all of the global balance equations or even local balance equations in their totality.
15.6.2 The BCMP Theorem for Open, Closed, and Mixed Queueing Networks
The research of Chandy and Muntz, together with some related work of Baskett and Palacios merged
into what is now called the theorem of Baskett, Chandy, Muntz, and Palacios, the BCMP theorem
[2]. This is arguably the most widely referenced theorem in queueing networks, its only possible
competitor being Little’s theorem. A BCMP network has the following characteristics:
1. The network consists of M nodes, each node belonging to one of types 1–4, as described
previously.
2. Customers belong to one of R classes. The network is said to be open if all customer classes
are open, closed if all customer classes are closed, and mixed if both open classes and closed
classes coexist in the network. The total number of customers in the network at any time is
denoted by n, the total number at node i is ni and the total number of class r customers at
node i is nir. Thus
n =
M

i=1
ni =
M

i=1
R

r=1
nir.
3. If class switching is not permitted, the number of customers in each closed class must be
constant.
If class switching is permitted, then customer classes must be partitioned into (routing)
chains. Customers of classes r and s belong to the same chain if it is possible for a customer
of class r to become a customer of class s. In this manner the number of customers in a closed
chain is constant. A chain consists of all (node, class) pairs (i,r) and ( j, s) such that a class
r customer leaving node i can (eventually) enter node j as a customer of class s. We shall let
C be the number of chains in the network and Co the number of open chains. If Co = 0, the
network is closed; if Co = C, the network is open, and if 0 < Co < C, the network is mixed.
The total number of customers in the network that belong to chain c is denoted by nc.
4. Open class customers arrive from the exterior according to a Poisson process. The rate of
arrival can depend on the number of customers present in the network in one of two ways.
(a) There is a single Poisson arrival process whose rate λ(n) depends on the total number of
customers present in the network at the moment of arrival. An arrival goes to node i as a
class r customer with probability p0,ir, where M
i=1
R
r=1 p0,ir = 1. It is understood that
p0,ir = 0 if r is a closed class.
(b) There is a separate Poisson arrival process for each open chain. The rate of arrival λc(nc)
for open chain c depends on the number of customers of that chain currently present
in the network. An arrival goes to node i of the chain with probability p0,ir, where
M
i=1
R
r=1 p0,ir = 1 and p0,ir = 0 if r does not belong to open chain c.
A combination of (a) and (b) is not possible.
5. Each customer class has its own routing matrix, which describes how customers of that class
transition among the nodes of the network. On exiting node i, a class-r customer enters node
j as a class-s customer with probability pir, js. With probability pir,0 = 1−M
j=1
R
s=1 pir, js,
a class-r customer exits the network from node i.
The concept of class change deserves some elaboration. It is only during the transition from one
node to another that this transformation can take place and not while the customer is already present
at a node. Each customer class is assigned to one and only one chain in such a way that if the routing
probability matrix of a class-r customer permits this customer to become a class-s customer, or vice

15.6 Multiclass Queueing Networks and the BCMP Theorem
599
versa, then classes r and s are combined into the same chain. This means that a customer belonging
to a class of one chain can never become a customer of a class belonging to any other chain. Since
the number of customers in each closed chain is ﬁxed, it is apparent that a customer from an open
class cannot change to become a customer of a closed class and vice versa.
The conditions of the BCMP theorem call for the chains to be ergodic. The discrete-time Markov
chain represented by the transition probability matrix whose states are the (node, class) pairs (i,r)
with i = 0, 1, . . . , M (i = 0 represents the exterior from which customers belonging to open
classes emanate and to which they eventually proceed) and r = 1, 2, . . . , R, is decomposable into
C chains, each of which is an ergodic Markov chain in its own right. Hence, if it is possible for a
customer of closed class r to become a customer of closed class s, the reverse must also be possible.
If this were not the case, then eventually, there would remain no customers of class r in the network;
closed class-r customers would have no effect whatever on the stationary distribution of the network
and could be eliminated from the beginning. Since all open class customers eventually exit to the
exterior, the same problem does not arise, although it may be useful to distinguish a “different”
exterior for different open chains.
Corresponding to each chain Ck ∈C of the network, is a set of trafﬁc equations which must be
solved. These are given by
vir = p0,ir +

all ( j,s)∈Ck
v js p js,ir
for all (i,r) ∈Ck.
(15.20)
The quantities vir are visit ratios similar to those introduced in Section 15.3 in the context of single-
class closed queueing networks. Equation (15.20) has a unique solution if the chain Ck is open, i.e.,
if there is at least one pair (i,r) ∈Ck for which p0,ir > 0. If the chain Ck is closed, then p0,ir = 0 for
all (i,r) ∈Ck and the solution of Equation (15.20) can be found only to a multiplicative constant.
Having thus characterized a BCMP network, we are ready to deﬁne a state of its underlying
Markov chain. Since the network contains M nodes, a state S is a vector quantity whose ith
component Si is the state of node i:
S = (S1, S2, . . . , SM).
In turn, the state of each node is also a vector quantity which describes the number, class, state of
service and any other relevant information concerning the customers at the node. This information
varies depending on the type of node.
• For type-1 nodes, which have FCFS scheduling and identical exponential service time
distribution for all classes of customer, it is necessary to specify the class of the customer
at each position in the node. If node i is a type-1 node and there are ni customers present, then
Si is a vector of length ni:
Si = (ri1, ri2, . . . , rini),
where rik is the class of the customer in position k, k = 1, 2, . . . , ni. If ni > 0, the customer
at position 1 is the customer currently being served.
• For nodes of type 2 or 3, it sufﬁces to specify the number of class-r customers, r =
1, 2, . . . , R, that are in each of the possible service phases. Let uir be the number of service
phases in the Coxian representation of the service time distribution of class r customers at
node i. So, if node i is a type-2 or -3 node, then Si is a vector of length R:
Si = (si1, si2, . . . , si R),
whose rth component sir is a subvector of length equal to uir given by
sir = (σ1r, σ2r, . . . , σuirr),
and σkr is the number of class r customers in phase k.

600
Queueing Networks
It is also possible to deﬁne the state of a type-2 or -3 node in a manner similar to that used for
type-1 nodes, with the understanding that, although arranged into order of arrival, this order is
of no consequence, since all customers at type-2 and -3 nodes are served concurrently. Each
component rik of Si = (ri1, ri2, . . . , rini) now becomes the pair (rik, ηk) whose components
provide the class and phase of service of the customer in position k. The ﬁrst approach gives
much shorter node descriptors when the network has many customers and the Coxian service
time distributions have only a small number of phases.
• Nodes of type 4 have both an ordering component (LCFS-PR) and Coxian service time
distributions. The representation of the node is similar to that of type-1 nodes, with the
additional requirement that along with the class of the customer at position k in the node,
the current phase of service must also be provided. Thus, for nodes of type 4,
Si = (si1, si2, . . . , sini),
where ni is the number of customers at node i and sik = (rik, ηk) is a vector of length 2 whose
ﬁrst component rik gives the class of the customer in position i in the node and whose second
component ηk gives the current phase of service of this customer.
The global balance equations, which relate the ﬂow into and out of each state S
=
(S1, S2, . . . , SM) can now be formed and it may be shown that the stationary probability of these
states has a product form. As we have seen, these state descriptors contain much more information
than is usually required, information such as the position of each customer in FCFS and LCFS-
PR queues and the phase of service of all customers other than those in type-1 nodes. Most often, it
sufﬁces to know only the number of customers of each class at each of the nodes. The BCMP authors
provide both. The product form involving the complete state description is provided, but then it is
shown that by summing over groups of states, a more compact aggregated product form can be
obtained. An aggregated state S = (y1, y2, . . . , yM) is a vector of length M whose ith component,
yi = (ni1, ni2, . . . , ni R), is a subvector of length R. The rth component of this subvector nir is the
number of class-r customers at node i. The product form is
Prob{S = (y1, y2, . . . , yM)} = 1
G d(S)
M

i=1
fi(yi),
where G is a normalization constant which insures that the sum of the probabilities is equal to 1;
d(S) is a function of the arrival processes, and the functions fi(yi) differ according to the type of
node. The condition for stability depends only on the open chains: if the utilization at each node
generated by open class customers is less than one, then the network is stable, irrespective of the
number of closed class customers present. When the Poisson arrival processes are load independent,
the utilization of node i caused by open class customers is
ρi =
R

r=1
λr
vir
μi
(single server, FCFS node);
ρi =
R

r=1
λr
vir
μir
(PS, IS, LCFS-PR nodes).
Let us consider the terms d(S) and fi(yi) in more detail. If the network has no open chains then
d(S) = 1. Otherwise, d(S) depends on which of the two arrival processes is in force. If there is a
single Poisson arrival process whose rate depends on n, the total number of customers of all classes
present, open and closed, then
d(S) =
n−1

k=0
λ(k),

15.6 Multiclass Queueing Networks and the BCMP Theorem
601
where λ(k) is the arrival rate when the network contains k customers. If there are Co independent
Poisson streams, one for each open chain, and the arrival process is of the second kind, then
d(S) =
Co

c=1
nc−1

k=0
λc(k),
where nc is the number of customers of chain c present in the network and λc(k) is the rate of arrival
to chain c when this chain contains k customers.
The terms fi(yi) are deﬁned below for nodes of each type. In these formulae, nir is the number of
class-r customers at node i and ni = R
r=1 nir is the total number of customers at node i, 1/μir is
the mean service time of a class-r customer at node i, and vir is the visit ratio for a class-r customer
at node i, obtained by solving Equation (15.20):
For nodes of type 1:
fi(yi) = ni!
 R

r=1
1
nir!vnir
ir
  1
μi
ni
.
For nodes of types 2 and 4:
fi(yi) = ni!
R

r=1
1
nir!
 vir
μir
nir
.
For nodes of type 3:
fi(yi) =
R

r=1
1
nir!
 vir
μir
nir
.
Observe that the deﬁnition of fi(yi) presented for nodes 2 and 3 is also valid for nodes of type 1,
since in nodes of type 1, 1/μir = 1/μi for all r = 1, 2, . . . , R, and ni = R
r=1 nir. When the
service rates depend on the total number of customers at a node, the factors in the product form
become
For nodes of type 1:
fi(yi) = ni!
 R

r=1
1
nir!vnir
ir
 ⎛
⎝
ni

j=1
1
μi( j)
⎞
⎠,
For nodes of type 2 and 4:
fi(yi) = ni!
R

r=1
⎛
⎝1
nir!vnir
ir
ni

j=1
1
μir( j)
⎞
⎠,
For nodes of type 3:
fi(yi) =
R

r=1
⎛
⎝1
nir!vnir
ir
ni

j=1
1
μir( j)
⎞
⎠,
where 1/μir( j) is the mean service time of a class-r customer at node i when the customer
population at the node is j. The BCMP theorem also treats the case when the service time of a
class r customer depends on the number of customers of class-r present at node i, but we do not
consider that possibility here. As noted explicitly by the authors of the BCMP theorem, these are
surprising results. The only moment of the service time distributions to appear in these formulae is
the ﬁrst—the service time distributions for the different classes might as well have been exponential!
Notice, nevertheless, that the class of a customer still plays a role, through the different mean service
times 1/μir.
There remains one problem, the computation of G, the constant of normalization. For closed
multiclass networks, the convolution algorithm of Buzen developed in Section 15.3 can be extended
by the introduction of vector quantities to replace the scalars used in the single-class environment.
The modiﬁcation of the auxiliary functions is also straightforward. For mixed queueing networks,
it is usual to combine all open classes into a singe open chain and to compute the utilization at

602
Queueing Networks
each node due to open class customers only. The normalization constant is then computed in the
absence of open class customers. The open class utilizations can now be used to scale the factors
of the product form. The MVA algorithm can also be modiﬁed to solve open, closed, and mixed
multiclass networks. Indeed there is a large number of possible computational algorithms currently
available, each having different advantages and disadvantages. Some of these include LBANC
(local balance algorithm for normalizing constant) developed by Chandy and Sauer [9], the tree
convolution algorithm of Lam and Lien [26], and RECAL (recursion by chain) by Conway and
Georganas [11]. A later book by the last two authors [12] places many of these computational
algorithms onto a common framework.
As might be expected, the addition of different classes of customer into a network substantially
increases the computational complexity and memory requirements of both the convolution algo-
rithm and the MVA algorithm. For example, the complexity of Buzen’s algorithm when applied to
a closed queueing network with N customers and M nodes each with a single load-independent
exponential server is O(M N); when the service is load dependent, this number becomes O(M N 2);
when there are R customer classes and Nr customers of class r = 1, 2, . . . , R, with no class
switching permitted and only load-independent service, the computational complexity has order
O(MR R
r=1(Nr + 1)). The MVA algorithm has similar complexity. It will quickly be recognized
that the amount of computation needed for large networks, particularly those with many different
classes, can become huge, sometimes to such an extent that this approach must be abandoned and
others sought.
Approximate methods are used in product form networks when the computation complexity is
so large that it effectively prevents the application of either the convolution algorithm or the MVA
method. Most approximate methods in these cases are based on the MVA algorithm and the results
obtained are frequently more than adequate in terms of the accuracy achieved. Numerical problems
associated with underﬂow and overﬂow in the computation of the normalization constant can be
avoided. Approximations are also used when the queueing network does not have a product form
solution. These commonly occur in the presence of a ﬁrst-come, ﬁrst-served scheduling policy with
nonexponential service time distributions. One possibility is to replace the nonexponential servers
with exponential servers having the same mean. Although this can work reasonably well in some
cases, and in closed networks better than open networks, it is not always satisfactory. The number
and type of approximation methods currently available is extensive and warrants a text all to itself.
The book by Bolch et al. [4] comes as close as any to achieving this goal.
15.7 Java Code
The Convolution Algorithm
import java.util.*;
class
buzen
{
public static void main (String args[])
{
// Example 1 from Notes
int N = 3;
// Number of customers
int M = 4;
// Number of nodes
boolean [] LI = {false, true, true, true};
// True if node m
// is load independent; otherwise false
double [] lambda = {1.0, 0.5, 0.3, 0.2};

15.7 Java Code
603
double [][] mu = { {0.0,1.0,2.0,2.0},{0.5,0.5,0.5,0.5},{1.0,1.0,1.0,1.0},
{2.0,2.0,2.0,2.0} };
/*
// Example 2 from Notes
int N = 2;
// Number of customers
int M = 4;
// Number of nodes
boolean [] LI = {false, true, true, false};
// True if node m
// is load independent; otherwise false
double [] lambda = {1.0, 0.4, 0.6, 1.0};
double [][] mu = { {0.0,1.0,2.0}, {0.8,0.8,0.8}, {0.4,0.4,0.4},
{0.0,0.5,1.0} };
*/
double [] G = new double [N+1];
// Array to hold normalizing constants
double [] f = new double [N+1];
System.out.println("Node 0");
G[0] = 1.0;
if (LI[0]) {
double Y = lambda[0]/mu[0][0];
for (int n=1; n<=N; n++) {
G[n] = G[n-1]*Y;
}
for (int n=0; n<=N; n++) {
System.out.println("
G-LI = " + G[n]);
}
}
else {
double Y = lambda[0];
for (int n=1; n<=N; n++) {
G[n] = G[n-1]*Y/mu[0][n];
}
for (int n=0; n<=N; n++) {
System.out.println("
G-LD = " + G[n]);
}
}
for (int m=1; m<M; m++) {
System.out.println("Node "+ m);
if (LI[m]) {
double Y = lambda[m]/mu[m][m];
for (int n=1; n<=N; n++) {
G[n] = G[n] + G[n-1]*Y;
}
for (int n=0; n<=N; n++) {
System.out.println("
G-LI = " + G[n]);
}
}
else {
f[0] = 1;
for (int n=1; n<=N; n++) {

604
Queueing Networks
f[n] = f[n-1]*lambda[m]/mu[m][n];
System.out.println("f(n) = " + f[n]);
}
for (int n=N; n>0; n--) {
double sum = G[n];
for (int k=1; k<=n; k++) {
sum = sum + f[k]*G[n-k];
}
G[n] = sum;
}
G[0] = 1;
for (int n=0; n<=N; n++) {
System.out.println("
G-LD = " + G[n]);
}
}
}
}
}
The MVA Algorithm
import java.util.*;
import java.text.*;
class mva
{
public static void main (String args[]) {
/*
***** First part defines the queueing network
*****
***** This is the only part that is should be altered
*****
*/
//
Example from Notes
============>
int M = 5;
// Number of nodes
int N = 10;
// Population size
int [] qType = {0,1,2,2,2,3};
// Node types (1, 2 or 3)
int [] mnum = {0,10,1,1,1,2};
// Number of servers per node
double [] mu = {0,2,1,2,4,2.5};
// Service rates for each node
double [] V
= {0,1.0,0.2,0.3,0.5,1.0}; // Visit ratios for each node
/*
//
Example from Bolch et al.
============>
int M = 4;
int N = 3;
int [] qType = {0,3,2,2,1};
int [] mnum =
{0,2,1,1,3};
double [] mu = { 0, 2, 1.66666666666, 1.25, 1};
double [] V =
{0, 1.0, 0.5, 0.5, 1.0};
*/

15.7 Java Code
605
/*
***** The MVA algorithm
*****
***** There is no need to alter beyond this point
****
*/
double G = 1;
// Normalization constant
double [] L = new double [M+1];
// Mean number of customers
double [] R = new double [M+1];
// Mean response times
double [] X = new double [M+1];
// Throughputs
int numLD = 0;
for (int i=1; i<=M; i++) {
if (qType[i] == 3) {numLD++;}
// Determine number of LD nodes
}
double [][] pp = new double [N+1][numLD+1];
for (int m=0; m<=numLD; m++) {
pp[0][m] = 1;
// Marginals for LD nodes
}
//
***** Begin iterating over customer population
***
for (int k=1; k<=N; k++){
//
***
MVA Step 1:
Compute mean response times
***
int LDcount = 0;
// Indicator for LD nodes
for (int m=1; m <= M; m++) {
if (qType[m] == 1) { R[m] = 1/mu[m]; }
// Infinite server node
if (qType[m] == 2) { R[m] = (L[m]+1)/mu[m]; }
// FCFS exponential, etc.
if (qType[m] == 3) {
// Load dependent node
R[m] = 0;
//
... considered to be a
for (int j=1; j <=k; j++) {
//
multiserver expon. node
// In this implementation, alpha_i(j) is equal to min(m_i, j)
double alpha = j; if (mnum[m] < j){alpha = mnum[m];}
R[m] = R[m] + j/(mu[m] * alpha) * pp[j-1][LDcount];
}
LDcount++;
}
}
//
***
MVA Step 2:
Compute throughputs
double bottom = 0;
for (int m=1; m<=M; m++){
bottom = bottom + V[m]*R[m];
}
X[1] = k/bottom;
// Overall throughput
for (int m=2; m<=M; m++) {
X[m] = V[m]*X[1];
// Throughput of node m

606
Queueing Networks
}
//
Update normalization constant
G = G/X[1];
//
*** MVA Step 3:
Compute mean number of customers
***
for (int m=1; m<= M; m++) {
L[m] = X[m]*R[m];
}
//
Compute marginals for all load dependent nodes
LDcount = 0;
//
Indicator for LD nodes
for (int m=1; m<=M; m++) {
if (qType[m] == 3) {
double p0 = 0;
for (int j=k; j >=1; j--) {
// In this implementation, alpha_i(j) is equal to min(m_i, j)
double alpha = j; if (mnum[m] < j){alpha = mnum[m];}
pp[j][LDcount] = X[1]/(mu[m] * alpha) * pp[j-1][LDcount];
p0 = p0 + pp[j][LDcount];
}
pp[0][LDcount] = 1-p0;
LDcount++;
}
}
}
//
Output results
NumberFormat fmat = NumberFormat.getNumberInstance();
fmat.setMaximumFractionDigits(6);
fmat.setMinimumFractionDigits(6);
System.out.println("
" );
System.out.println("
R[m]
X[m]
L[m] " );
for (int m=1; m <=M; m++) {
System.out.println(" m = " + m + ":
" + fmat.format(R[m]) + " "
+ fmat.format(X[m]) + "
" + fmat.format(L[m]));
}
System.out.println(" " );
double sum = 0;
for (int m=1; m <=M; m++) {
sum = sum + L[m];
}
System.out.println("Sum check: Population size = "+ fmat.format(sum));
System.out.println("Normalization constant: G = " + G);
System.out.println("
" );
}
}

15.8 Exercises
607
15.8 Exercises
Exercise 15.2.1 Consider the most basic of feedback networks, a single node with feedback as shown in Figure
15.16. This could be used, for example, to model a data transmission channel with probability r that a message
is transmitted unsuccessfully and requires retransmission. Find the distribution and mean number of customers
in this network. Assume a Poisson arrival process and exponential service time distribution.
r
1−r
μ
γ
Figure 15.16. Single queue with feedback.
Exercise 15.2.2 Consider an open queueing network of the central server type. Customers arrive at the central
server according to a Poisson process with rate 0.1. The central server behaves stochastically like an M/M/2
queue in which each of the servers operates at rate 2.0. After receiving service at the central server, customers
either leave the network (with probability 0.2) or else proceed to one of three other nodes with probabilities 0.3,
0.3, and 0.2 respectively. Service at each of the three service centers is exponentially distributed at rate 0.5, 1.0,
and 2.0 respectively. After receiving service at one of these three nodes, customers return to the central server.
Compute the probability that the central server is idle and the mean number of customers in the network.
Exercise 15.2.3 Consider the queueing model shown in Figure 15.17. This may be used to model a computer
CPU connected to an I/O device as indicated on the ﬁgure. Processes enter the system according to a Poisson
process with rate γ , and use the CPU for an exponentially distributed amount of time with mean 1/μ1. Upon
exiting the CPU, with probability r, a process uses the I/O device for a time that is exponentially distributed
with mean 1/μ2, or with probability 1 −r, it exits the system. Upon exit from the I/O device, a process again
joins the CPU queue. Assume that all service times, including successive service times of the same process
at the CPU or the I/O device are independent. Find the mean number of customers at the CPU and at the I/O
device. What is the average time a process spends in the system? What is the utilization at the CPU and at the
I/O device?
I / O
CPU
r
1
2
1−r
μ
μ
γ
Figure 15.17. Queueing network representing a CPU and I/O device.
Exercise 15.2.4 This question relates the queueing network with feedback of Exercise 15.2.3 to the two node
tandem queueing network without feedback shown in Figure 15.18. In this tandem queueing network, the ﬁrst
node represents the CPU of the previous example and the second represents the I/O device. The arrival process
is Poisson with rate γ . Each arriving process requires the CPU only once, and holds it for an average of S1
seconds. On exiting the CPU, each process must go to the I/O device and spends an average of S2 seconds there
after which it departs never to return.
The following values for S1 and S2 are given:
S1 =
1
μ1(1 −r),
S2 =
r
μ2(1 −r).

608
Queueing Networks
CPU
2
I / O
1 / S
1
1 / S
γ
Figure 15.18. The related tandem queue.
(a) Show that the utilization factor in this tandem model is the same as the utilization factor in the
feedback model of Exercise 15.2.3.
(b) Show that the response time (average time a process spends in the network) is the same in both
models.
(c) Show that the probability distribution of customers is the same in both networks.
(d) Generate a counter-example to show that the waiting time distribution is not the same in both
networks. Hint: Choose μ1 ≫μ2.
(e) How should S1 and S2 be interpreted in terms of CPU and I/O requirements?
Exercise 15.3.1 Consider a closed queueing network of three nodes. Each node contains a single exponential
server and the scheduling discipline is FCFS. The routing probability matrix is
⎛
⎝
0
0.7
0.3
1
0
0
1
0
0
⎞
⎠.
The exponential service time distributions have a mean service time of 2.0 at node 1, 1.0 at node 2, and 0.5 at
node 3. The number of customers that circulate in the network is equal to 3. Use the convolution algorithm to
determine the marginal distribution of customers at node 1.
Exercise 15.3.2 This time answer the previous question using a Markovian analysis. In other words, compute
the inﬁnitesimal generator, and solve it using a numerical method. Be sure your answers to this question and
the previous are the same.
Exercise 15.3.3 Consider a closed queueing network with three nodes each containing a single exponential
server with rates μ1 = 1, μ2 = 2, and μ3 = 4 respectively. A total of three customers circulates in this network
and scheduling at all nodes is ﬁrst-come, ﬁrst-served. The routing matrix is given by
R =
⎛
⎝
.2
.3
.5
.2
.3
.5
.2
.3
.5
⎞
⎠.
Draw this queueing network and then use the convolution approach to ﬁnd the throughput of the network and
the distribution of customers at node 1.
Exercise 15.3.4 Consider a closed queueing network with two service centers in tandem between which three
customers circulate. The ﬁrst service center contains two identical exponential servers which provide service
at rate μ1 = 1 while the second service center is a pure delay node (inﬁnite server) where customers spend an
exponentially distributed amount of time with mean .5 time units. Use the convolution algorithm to compute
the margin distribution of customers at the ﬁrst service center.
Exercise 15.3.5 Consider a closed queueing network in which only two identical customers circulate. The
network is composed of four nodes; the ﬁrst node contains two exponential servers each of which provides
service at rate μ1 = 1; node 2 is an inﬁnite-server node, in which each server provides exponentially distributed
service at rate μ2 = 0.5; nodes 3 and 4 both contain a single exponential server with rates μ3 = 0.8 and
μ4 = 0.4, respectively. On exiting from node 1, all customers go to node 2; on exiting node 2 customers will
next go to node 3 with probability 0.4 or to node 4 with probability 0.6. On exiting either node 3 or 4, customers
return to node 1. Use Buzen’s algorithm to ﬁnd the marginal distribution at nodes 1 and 2 and the mean number
of customers at each of the four nodes.
Exercise 15.4.1 Three service centers operate in tandem with the third feeding back into the ﬁrst. The ﬁrst
node is a pure delay with mean equal to 1 time unit. The other two nodes are single server nodes whose service

15.8 Exercises
609
rates are given respectively by μ2 = 2 and μ3 = 4. A total of four customers circulates in this network . Use
the MVA algorithm to compute the mean number of customers at each node and the throughput of the network.
Exercise 15.4.2 Return to Exercise 15.3.3, but this time answer the question using the MVA algorithm.
Exercise 15.4.3 Return to Exercise 15.3.4, but this time answer the question using the MVA algorithm.
Exercise 15.6.1 Write down each of the six global balance equations belonging to the queueing network of
Example 15.12. Separate each into one or more local balance equations and for each explain why they are
balanced.
Exercise 15.6.2 Consider a closed multiclass central server queueing model with two classes of customer as
shown in Figure 15.19. The central server consists of a single exponential server that operates in LCFS-PR
mode. The mean service time for class i customers is 1/μi, i = 1, 2. On exiting the central server, class 1
customers go to the ﬁrst of the two auxiliary nodes while class 2 customers go to the second. On leaving the
auxiliary nodes, customers return to the central server. Both auxiliary nodes contain a single exponential server
operating in FCFS order with rates as indicated on the ﬁgure. To keep the model simple, assume that only one
customer of each class circulates in the model.
(a) Write down the global balance equations for this network.
(b) Separate the global balance equations into local balance equations and explain the rationale for each
local balance equation.
(c) Now assume that the scheduling policy at the central server is changed to FCFS, while still
maintaining different service rates for each class of customer. What are the global balance equations
in this case?
(d) Which of the local balance equations obtained in part (b) are modiﬁed by the FCFS policy and are the
new versions local balance equations? Explain why.
i
4
Class 2
Class 2
Class 1 
3
Class 1 
μ
μ
μ
Figure 15.19. A central server multiclass queueing model.

This page intentionally left blank 

Part IV
SIMULATION

This page intentionally left blank 

Chapter 16
Some Probabilistic and Deterministic
Applications of Random Numbers
In the very ﬁrst chapter of this book, we introduced a certain number of probability axioms
and used them to develop a theory by which different probabilistic scenarios could be analyzed
mathematically. Behind all of this was the implied assumption that if a probability experiment could
be conducted a very large number of times (in fact, an inﬁnite number of times) the same results
could be obtained without the theory. This we referred to as the frequency approach to probability.
A ﬁnite, but sufﬁciently large number of times would give an excellent approximation, certainly
good enough for most applications.
Unfortunately, conducting a probability experiment, even as simple as tossing a coin or choosing
a card, for a large number of times can be very time consuming. A computer, however, could do
such tasks extremely rapidly, if we could only decide what it means for a computer to toss a coin,
or choose a card! This is where the concept of random numbers becomes useful. If we can use the
computer to generate a sequence of real numbers that are randomly distributed in the interval [0, 1),
then we could multiply each number in the sequence by 2 and associate the result, if it is strictly
less than 1, with the event that the experiment gives a tail. If the result is greater than or equal to
1, associate it with the event that a head is obtained. With a fair coin, the probability of getting a
tail on a single toss, is 1/2. With a random sequence of reals in the interval [0, 2) we should also
expect to ﬁnd a number less than 1 with probability 1/2. Similarly, if our concern is with choosing
a card from a well shufﬂed regular deck of 52 cards, then a computer generated sequence of reals
in the interval [0, 1) could be multiplied by 52, and each resulting number in an interval [i, i + 1),
for i = 0, 1, . . . , 51, could be associated with the appearance of a particular card. But what exactly
does it mean to generate a random sequence of reals in any interval? We shall return to this question
later in the chapter, but for the moment, intuitively, it means that if we partition the interval into
a set of collectively exhaustive and mutually exclusive equal subintervals, the next number in the
sequence should be as likely to fall into any one of these subintervals as another.
The fact that the numbers are generated at random does not mean that they cannot be applied
to deterministic scenarios. Indeed, one of the ﬁrst uses of randomly generated numbers was the
evaluation of deﬁnite integrals. Towards the end of this chapter, we shall provide some examples of
this for illustrative purposes.
16.1 Simulating Basic Probability Scenarios
For the moment, we shall assume that we have access to a randomly generated sequence of real
numbers that lie in the interval [0, 1). From this sequence we shall generate a sequence of integers,
randomly distributed in the interval [0, T −1] by multiplying each real r by T and choosing the
nearest integer that is less than or equal to the result. In other words, we compute ⌊rT ⌋. We shall use
this sequence to answer probability questions that, in previous chapters, we solved mathematically
by means of the probability axioms.

614
Some Probabilistic and Deterministic Applications of Random Numbers
Consider the case when all the elementary events of a sample space are equiprobable. Let us
assume that the sample space contains T elementary events, ei, i = 1, 2, . . . , T . Our concern is
to determine the probability associated with a certain event E, deﬁned as a proper subset of these
elementary events. As we proceed through the simulation, we shall generate random reals in the
interval [0, 1), multiply each by T and take the ﬂoor of the result. If the product of the real number
and T lies in the interval [i −1, i), then the random integer obtained is i −1 and it is associated
with the occurrence of elementary event ei. We may now test if the elementary event ei is in the
event subset E, i.e., if the event E occurred or not. We shall keep a count of the number of trials
conducted, n, and the number of these, nE, in which the desired event E occurred. As n becomes
large, the ratio of these, i.e., the ratio nE/n, tends to the probability of the event E. The algorithm
is as follows:
Initialize event count: n_E = 0
For a total of "n" trials {
* Perform the probability experiment
* If event E occurs, increment n_E
}
Estimated probability of event E = n_E/n
The statement, "* Perform the probability experiment", means that a random integer
between 0 and T −1 is chosen to represent the outcome of the trial. The event E occurs if this
integer represents an elementary event in the subset that deﬁnes E. This is perhaps best illustrated
by a number of examples. We shall use these examples to show how a simulation run mimics a
trial of the probability experiment with particular emphasis on the number of random numbers that
each trial requires and the large number of trials that are generally needed to obtain even modest
precision.
Probability Scenario 1
Let us use simulation to determine the probability of getting a total of three spots or less when two
fair dice are thrown simultaneously. In this case, there are 36 equiprobable elementary events in the
sample space, denoted (i, j) for i, j = 1, 2, 3, 4, 5, 6. We could begin by indexing the 36 elementary
events from 0 through 35 as follows
Index
0
1
2
3
4
5
6
· · ·
33
34
35
E. Event
(1,1)
(2,1)
(1,2)
(3,1)
(2,2)
(1,3)
(4,1)
· · ·
(6,5)
(5,6)
(6,6)
The event “three spots or fewer” corresponds to the subset that contains the ﬁrst three elementary
events. We initialize the counters n and nE to zero and begin generating random numbers in the
interval [0, 1). Each random number is multiplying by 36 and the counter n is incremented. For
each result that is strictly less than 3, the counter nE is also incremented, since a result of 0, 1, or
2 corresponds to the only three elementary events whose sum is 3 or less. Table 16.1 displays the
results obtained using the Java random number generator, and appears to be converging to 0.08333,
i.e., one twelfth, the correct answer.
Table 16.1. Probability of getting three or fewer spots when throwing two dice.
n
50
100
1,000
10,000
50,000
100,000
500,000
1,000,000
nE
4
7
84
850
4,178
8,533
41,648
83,317
nE/n
0.0800
0.0700
0.0840
0.0850
0.08356
0.08533
0.083296
0.083317

16.1 Simulating Basic Probability Scenarios
615
These results were obtained using the Java code presented below. This code will be used in
the remaining examples in this chapter, but we shall only provide the portion that lies between
the *** Begin Simulation *** and *** End Simulation *** comments, since the initial
six statements and the last statement are identical in all the remaining examples. Only the part
that is unique to the simulation experiment will be given in future examples. A single run of the
program actually performs eight simulations with an increasing number of trials ranging from 50 to
1,000,000. This enables us to observe that, generally speaking, accuracy increases with increased
number of trials.
import java.util.Random;
class scenario {
public static void main (String args[]) {
int max [] = {50,100,1000,10000,50000,100000,500000,1000000};
Random generator = new Random();
for (int i=0; i<8; i++) {
// *** Begin Simulation ***************************
int n_E = 0;
// Initialize success count
for (int n=1; n<max[i]; n++) {
// Generate max[i] random integers ...
int r_e = generator.nextInt(36);
//
between 0 and 35
if (r_e < 3){n_E++;}
// Increment success count
}
double prob = (double)n_E/max[i];
// Approximate probability
// *** End Simulation *****************************
System.out.println("n_E = " + n_E + "
max = " + max[i] +
"
n_E/max[i] = " + prob);
}
}
}
In this example, we knew that there were 36 elementary events and that only three of them lead
to success, so we effectively knew the answer was 3/36 before we even started. Had we not known
the number of elementary events nor the number of them that lead to success, we would have had to
perform the simulation in a different fashion, by more closely following the way in which the trials
themselves are actually carried out. We would have had to simulate tossing one die by generating a
random integer between 1 and 6, then tossing a second die by generating a second integer between
1 and 6, and then adding the two and checking to see if the result were less than or equal to three.
The body of the simulation program would then have been as follows:
// *** Begin Simulation ***************************
int n_E = 0;
// Initialize success count
for (int n=0; n<max[i]; n++) {
// Perform max[i] trials ...
int r_e1 = 1 + generator.nextInt(6);
// Die 1 (result 1,2,...,6)
int r_e2 = 1 + generator.nextInt(6);
// Die 2 (result 1,2,...,6)
if (r_e1+re2 <= 3){n_E++;}
// Increment success count
}
double prob = (double)n_E/max[i];
// Approximate probability
// *** End Simulation *****************************

616
Some Probabilistic and Deterministic Applications of Random Numbers
This is, computationally, more expensive than the previous implementation, because each trial
requires the generation of not one, but two random numbers. However, it is not unusual to need
several random numbers in creating a trial, as we shall see in the next few examples.
Probability Scenario 2
In this example we shall use simulation to determine the probability of getting exactly three heads
in ﬁve tosses of a fair coin. Each time we generate a random number r in the interval [0, 1) we shall
multiply it by 2 and take the ﬂoor of the result, ⌊2r⌋. When this gives 0, we shall associate it with
the event that a tail occurs; when it is 1, that a head occurs. In ﬁve consecutive tosses, we need to
know when exactly 3 heads occur; in other words, when the total of ﬁve consecutive values of ⌊2r⌋
is exactly equal to 3. In this example the body of the Java code may be written as
// *** Begin Simulation ***************************
int n_E = 0;
// Initialize success count
for (int n=0; n<max[i]; n++) {
// Perform max[i] trials
int heads = 0;
for (int j=1; j<=5; j++) {
// Trial consists of 5 tosses
int r_e = generator.nextInt(2);
// 1 if heads; 0 if tails
heads = heads+r_e;
}
if (heads == 3){n_E++;}
// Count successes
}
double prob = (double)n_E/max[i];
// Approximate probability
// *** End Simulation *****************************
Notice that we may replace the integer 3 in line 9 with any integer between 0 and 5 inclusive to
determine the probability of getting that number of heads in ﬁve tosses of a coin. We may also
replace the integer 5 in line 5 to allow for more or fewer tosses. In this case, the number of heads
sought must obviously lie between 0 and the total number of tosses. One execution of this program
led to the results in Table 16.2.
Table 16.2. Probability of getting exactly three heads in ﬁve tosses.
n
50
100
1,000
10,000
50,000
100,000
500,000
1,000,000
nE
18
28
321
3,165
15,631
31,310
155,989
312,257
nE/n
0.3600
0.2800
0.3210
0.3165
0.31262
0.3131
0.311978
0.312257
It appears that the answer is converging to the correct result, 0.3125. Each trial in this simulation
requires the generation of ﬁve random numbers, one for each toss of the coin. This is the “natural”
way to simulate this problem. An alternative approach requiring less random numbers is to generate
a single random integer between 0 and 31 and use it to represent all ﬁve tosses. This random integer
may be converted to its 5-bit binary equivalent in which the number of 1’s represents the number of
heads thrown. If the number of 1’s in this representation is exactly equal to 3, then the event “throw
exactly three heads” has occurred and the success count should be incremented.
Probability Scenario 3
In this example, we wish to ﬁnd the probability that in a class of 23 students, at least two students
have the same birthday. To solve this problem using simulation, for each value of n we shall

16.1 Simulating Basic Probability Scenarios
617
randomly choose 23 numbers between 1 and 365 and increment nE only if these twenty-three
numbers contains at least one repetition.
// *** Begin Simulation ***************************
int n_E = 0;
// Initialize success count
for (int n=0; n<max[i]; n++) {
// Perform max[i] trials
int [] birthday = new int [23];
// Array to store dates
boolean no_match = true;
int j = 0;
while (no_match && j<23) {
// Generate up to 23 dates
birthday[j] = generator.nextInt(365);
// New date
for (int k=0; k<j; k++) {
// Compare with others
if (birthday[j] == birthday[k]){no_match = false;} // Match?
}
j++;
}
if (no_match == false){n_E++;}
// Increment success count
}
double prob = (double)n_E/max[i];
// Approximate probability
// *** End Simulation *****************************
One execution of this program led to the results in Table 16.3.
Table 16.3. Probability of at least two people among 23 having the same birthday.
n
50
100
1,000
10,000
50,000
100,000
500,000
1,000,000
nE
25
47
523
5,056
25,386
50,678
253,529
507,666
nE/n
0.5000
0.4700
0.5230
0.5056
0.50772
0.50678
0.507058
0.507666
Simulating a trial in this example requires the generation of up to 23 random numbers (in the case
when no matches are found) and approximately up to 232/2 comparisons (again, when no matches
are found). It is evident that simulations requiring a large number of simulated trials can become
computationally expensive.
Probability Scenario 4
The ﬁrst of two boxes contains b1 blue balls and r1 red balls; the second contains b2 blue balls and
r2 red balls. One ball is randomly chosen from the ﬁrst box and put into the second. When this has
been accomplished, a ball is chosen at random from the second box and put into the ﬁrst. A ball is
now chosen from the ﬁrst box. What is the probability that it is blue?
// *** Begin Simulation ***************************
int n_E = 0;
// Initialize success count
for (int n=0; n<max[i]; n++) {
// Perform max[i] trials
int b1 = 3; int r1 = 7; int b2 = 4; int r2 = 3;
// Select colors
int r_e = generator.nextInt(b1+r1);
if (r_e < b1){b1--; b2++;}
// Blue ball moves from box 1 to box 2, or
else {r1--; r2++;}
// Red
ball moves from box 1 to box 2

618
Some Probabilistic and Deterministic Applications of Random Numbers
r_e = generator.nextInt(b2+r2);
if (r_e < b2){b2--; b1++;}
// Blue ball moves from box 2 to box 1, or
else {r2--; r1++;}
// Red
ball moves from box 2 to box 1
r_e = generator.nextInt(b1+r1);
// Choose a ball from box 1
if (r_e < b1){n_E++;}
// Success if this ball is blue
}
double prob = (double)n_E/max[i];
// Approximate probability
// *** End Simulation *****************************
We obtain the results in Table 16.4. The exact answer from Chapter 1 is equal to 0.32375. This
simulation mirrors exactly the probability experiment of moving balls from bucket to bucket.
Table 16.4. Probability that the last ball chosen is blue.
n
50
100
1,000
10,000
50,000
100,000
500,000
1,000,000
nE
18
34
320
3,263
16,345
32,662
161,896
323,807
nE/n
0.3600
0.3400
0.3200
0.3263
0.3269
0.32662
0.323792
0.323807
16.2 Simulating Conditional Probabilities, Means, and Variances
Conditional Probabilities
Consider now the simulation of conditional probabilities. The basic algorithm needs to be modiﬁed.
We must ﬁrst test for the condition, and only when the condition is met do we test for the desired
event. The answer is given by the ratio of the number of times the desired event is met to the number
of times that the condition is satisﬁed. The basic algorithm to determine the probability of the event
A given the event B, i.e., Prob{A|B}, may be written as follows:
Initialize event counts for A and B:
n_A = n_B = 0;
For a total of "n" trials {
* Perform the probability experiment
* If event B occurs, then {
~ increment n_B;
~ if event A occurs, then { increment n_A }
}
}
probability of A|B = n_A/n_B
Probability Scenario 5
The following example involves conditional probabilities. A box contains two white balls, two
red balls, and a black ball. Two balls are chosen without replacement from the box. What is the
probability that the second ball is white given that the ﬁrst ball chosen is white. Although it is pretty
obvious that the answer is 0.25, we shall use simulation to illustrate how it is used with conditional
probabilities. Here, event B occurs when the ﬁrst ball chosen is white, and event A occurs when the
second ball chosen is white.
// *** Begin Simulation ***************************
int n_A = 0;
int n_B = 0;
// Initialize event counts
for (int n=0; n<max[i]; n++) {
// Perform max[i] trials
int w = 2; int r = 2; int b = 1;
// Select colors

16.2 Simulating Conditional Probabilities, Means, and Variances
619
int r_e = generator.nextInt(w+r+b);
// Choose a ball
if (r_e < w){
// If it is white, then
n_B++;
// Event B has occurred, so
r_e = generator.nextInt(w+r+b-1);
// Choose another ball
if (r_e < w-1){n_A++;}
// Success, if this is white
}
}
double prob = (double)n_A/n_B;
// Approximate probability
// *** End Simulation *****************************
We obtained the results in Table 16.5 on one execution of the simulation program.
Table 16.5. Probability that the second ball is white given that the ﬁrst is white.
n
50
100
1,000
10,000
50,000
100,000
500,000
1,000,000
n A
1
13
94
1,028
4,924
9,921
50,173
99,803
nB
20
37
403
3,972
19,969
39,923
199,752
399,812
n A/nB
0.0500
0.35135
0.23325
0.25881
0.24658
0.25026
0.25118
0.24962
Means and Variances by Simulation
When we use simulation to determine the probabilities of certain events, we simulate the experiment,
count the number of times that a successful outcome occurs and take the ratio of this number to the
total number of trails generated. When using simulation to compute means and higher moments of
a random variable X, we need to adopt a somewhat different strategy. In these cases, we simulate
the probability experiment a large number of times, take note of the value that the random variable
assumes at each trial and, to form the mean, take the average of these values. Higher moments
are computed in a similar fashion. This is best illustrated by means of the following example, in
which we compute the mean E[X] and the variance as Var [X] = E[X2] −(E[X])2 of the random
variable X.
Probability Scenario 6
Balls are drawn from an urn containing w white balls and b black balls until a white ball appears.
The simulation program given below computes an estimate of the mean and the variance of the
number of balls drawn, assuming that each ball is replaced after being drawn. In this program,
the mean number incorporates a count for when the white ball is chosen. As mentioned when this
example was ﬁrst discussed in Chapter 1, the mean number of balls drawn before the white ball
is drawn is one less than this number. The variance is unaffected. The following table shows the
results of a succession of runs of this simulation program, each using the same number of trials,
10,000. The number of white balls is chosen to be w = 2 and the number of black balls to be b = 8.
As expected, the results (Table 16.6) are not too far off the exact answers of 1 + b/w = 5.0 and
b(w + b)/w2 = 20.0 respectively.
Table 16.6. Mean and variance of number of balls chosen.
E[X]
5.0061
5.0065
4.9604
5.0235
4.9948
5.0081
4.9938
5.0390
Var [X]
19.8725
20.24981
19.9034
20.0370
19.3104
20.2276
20.2590
20.7259

620
Some Probabilistic and Deterministic Applications of Random Numbers
import java.util.Random;
class mean {
public static void main (String args[]) {
Random generator = new Random();
int w = 2; int b = 8;
// Select colors
int max = 10000;
double average = 0; double variance = 0;
for (int n = 1; n<=max; n++) {
// Perform "max" trails
int n_d = 0;
// Initialize drawn count
boolean not_white = true;
do {
// Do while unsuccessful
int r_e = generator.nextInt(w+b);
// Pick a ball, and ...
n_d++;
//
Increment total drawn
if (r_e < w){
// Is it white? ...
not_white = false;
//
Yes, so stop
}
}
while (not_white);
// This trial drew n_d balls
average = average + n_d;
// Building up average
variance = variance + n_d*n_d;
// Building up variance
}
average = average/max;
// Compute average
variance = variance/max - average*average;
// Compute variance
System.out.println("mean = " + average);
// Print results
System.out.println("variance= " + variance);
}
}
Given that the above six probability scenarios were all previously solved mathematically, the
reader may wonder whether simulation serves a useful purpose or not. The answer must be a
resounding yes, for there are many problems that are very hard, or even impossible, to solve
mathematically, but yet are easily amenable to the simulation approach. Consider the problem of
rolling three dice until six spots appear on all three of them. We might like to know the probability
that this will take at least 30 throws. While it is not difﬁcult to imagine how to write a simulation
to approximate this probability, solving this problem mathematically is very hard indeed. When
mathematical solutions can be found, then they should be used, for often their formulation provides
more information about the underlying probability experiment itself. Simulation simply gives us
a number. Additionally, complex simulation programs require time to program, may be hard to
debug and may need extensive computational resources. However, in the absence of mathematical
solutions, they can be invaluable.
16.3 The Computation of Deﬁnite Integrals
Random numbers are not limited to the evaluation of probability scenarios, but have a much wider
ﬁeld of application. However, since this takes us somewhat outside of intended scope of this text,
we shall be content to brieﬂy describe only one, albeit a rather important, application, namely the
computation of deﬁnite integrals. Deﬁnite integration is usually described in terms of computing the

16.3 The Computation of Deﬁnite Integrals
621
area under a curve between two limits. Let f (x) be a bounded function on [a, b], i.e., there exists
a constant c such that f (x) ≤c for all x ∈[a, b]. Then I =
 b
a f (x)dx is the area beneath the
curve y = f (x) between the limits x = a and x = b. This area is contained in a bounding rectangle
whose base has length equal to b −a and with height equal to c, as illustrated in Figure 16.1. The
area we seek to compute, namely I, the shaded portion of the ﬁgure, constitutes a certain percentage
of this rectangle. If we were to generate random points within this rectangle, then the proportion of
these points to fall into the shaded area would be the same as the percentage of the area c(b −a)
occupied by I. This gives us a means to evaluate the integral using random numbers.
y = f(x)
x
y
a
b
y = c 
Figure 16.1. Illustration of deﬁnite integration.
If r1 and r2 are uniformly distributed on [0, 1], then x1 = a+(b−a)r1 is uniformly distributed on
[a, b]; y1 = cr2 is uniformly distributed on [0, c] and the point (x1, y1) is uniformly distributed over
the rectangle with base length b −a and height c. For any point generated like this, that point lies
in the shaded area if y1 ≤f (x1). The percentage of the rectangle occupied by the shaded portion
can be approximated by generating many points (xi, yi) randomly and uniformly distributed over
the rectangle and forming the ratio of the number that satisfy the constraint yi ≤f (xi) to the total
number of points generated. The value of the integral is then obtained by multiplying this percentage
by c(b −a), the area of the rectangle. This approach is called the “accept–reject” method. A point
(xi, yi) for which yi ≤f (xi) is “accepted”—it lies in the shaded area; otherwise it is rejected. The
proportion of the rectangle occupied by the area under y = f (x) is given by the ratio of the number
of accepted points to the total number of points generated. Later we shall see that this approach can
be modiﬁed to provide a means of generating random numbers that are not uniformly distributed
but which instead satisfy some arbitrary density function.
While this method of evaluating a deﬁnite integral is simple to understand, it is not very effective,
requiring two uniformly distributed random numbers for each point in the rectangle and a very large
number of points for reasonable accuracy. What we really need is the average value of f (x) on the
interval [a, b] which could then be multiplied by b −a to provide the value of I. To proceed in this
direction, let us ﬁrst consider the case when the limits of integration are a = 0 and b = 1 and hence
I =
 1
0 f (x)dx. If r is a uniformly distributed random number in the interval [0, 1] , then f (r) is a
point at random on the curve y = f (x) within the region of interest. Generating many such points
allows us to approximate the expected value of f (x) for x ∈[0, 1]. In particular, if r is uniformly
distributed on [0, 1], then
I = E[ f (r)],
the expected value of the function on [0, 1] times the length of the interval, 1. Given a sequence
r1, r2, . . . of uniformly distributed random numbers on [0, 1], the corresponding random variables
f (r1), f (r2), . . . are independent and identically distributed with mean value equal to I, the integral

622
Some Probabilistic and Deterministic Applications of Random Numbers
we seek. From the strong law of large numbers, we have, with probability 1,
lim
n→∞
n

i=1
f (ri)
n
= E[ f (r)] = I.
This approach to evaluating deﬁnite integrals is generally termed “Monte Carlo” integration.
Example 16.1 As a simple example, let us compute I =
 1
0 exdx which we know to be equal to
e −1 = 1.718282. The following Java program follows the guidelines of the previous examples and
estimates the value of I using different numbers of uniformly distributed random numbers.
import java.util.Random;
import static java.lang.Math.*;
class integrate {
public static void main (String args[]) {
int max [] = {50,100,1000,10000,50000,100000,500000,1000000};
Random generator = new Random();
for (int i=0; i<8; i++) {
// ***
Begin Simulation ***************************
double s = 0;
for (int n=1; n<max[i]; n++) {
// Generate max[i] runs.
double r = generator.nextDouble();
s = s + exp(r);
// f(x) = exp(x)
}
double Ivalue = s/(double)max[i];
// Approximate integral
// ***
End Simulation ***************************
System.out.println("n = " + max[i] + "
Ivalue = " + Ivalue);
}
}
}
A single run of this program produced the results in Table 16.7.
Table 16.7. Monte Carlo evaluations of the deﬁnite integral
 1
0 exdx.
n
50
100
1,000
10,000
50,000
100,000
500,000
1,000,000
Ivalue
1.832951
1.704603
1.725699
1.717236
1.718372
1.718015
1.718362
1.718751
We now examine the case when the limits of integration are a and b rather than 0 and 1. Observe
that if y is uniformly distributed on [0, 1] then x = a + (b −a)y is uniformly distributed on [a, b].
This suggests a change of variable. Letting
y = x −a
b −a
gives
dy =
dx
b −a

16.4 Exercises
623
and thus
 b
a
f (x)dx =
 1
0
f (a +[b −a]y) (b−a)dy = (b−a)
 1
0
f (a + [b −a]y) dy = (b−a)
 1
0
g(y)dy,
where g(y) = f (a + [b −a]y). This means that we can compute I =
 b
a f (x)dx by generating
uniformly distributed random numbers y ∈[0, 1], estimating the average value of g(y) on [0, 1] and
multiplying the result by b −a.
Example 16.2 To use this approach to obtain approximations to
 3
1 ex dx using the previous Java
program, it sufﬁces to modify the program as follows.
// ***
Begin Simulation ***************************
double s = 0;
double a = 1;
double b = 3;
for (int n=1; n<max[i]; n++) {
// Generate max[i] runs.
double r = generator.nextDouble();
double y = a+(b-a)*r;
s = s + exp(y);
// g(y) = exp(y)
}
double Ivalue = (b-a)*s/(double)max[i];
// Approximate integral
// ***
End Simulation ***************************
At this point we shall leave our discussion of the use of random numbers to approximate deﬁnite
integrals. It is not difﬁcult to extend these simple concepts to the case when one or both limits are
inﬁnite or to the case of multivariate integration. Indeed it is in this last area that the Monte Carlo
method excels.
Our goal in this chapter has been to show some of the uses to which uniformly distributed random
numbers can be put and to provide a feeling for some of the issues involved, such as the number
of random numbers needed to mimic an event and the large number of trials usually needed to
obtain a reasonably accurate result. This has set the stage for an examination of some important
topics. In the next chapter we shall concern ourselves with mechanisms for generating sequences
of uniformly distributed random numbers and procedures for testing if the numbers we generate
are actually “random” and “uniformly distributed.” Following that, in Chapter 18, we shall consider
procedures for generating numbers that are not uniformly distributed over some interval, but rather
are distributed according to some other distribution function, such as exponential or normal. In
Chapter 19 we investigate how simulation programs should be structured and examine some of
the problems involved in developing computer simulations of complex systems. A major issue in
simulation concerns the accuracy of the results obtained by a simulation. Once we have written a
simulation program and used it to solve a physical or hypothetical problem, how can we estimate
the accuracy of the computed solution? This is the topic of the ﬁnal chapter of this book.
16.4 Exercises
Exercise 16.1.1 Write code similar to that in Section 16.1 to simulate each of the three scenarios described
below. Also, for each scenario, compute the exact result mathematically and compare your simulation answers
with the exact answer.
(a) A prisoner in a Kafkaesque prison is put in the following situation. A regular deck of 52 cards is
placed in front of him. He must choose cards one at a time to determine their color. Once chosen, the

624
Some Probabilistic and Deterministic Applications of Random Numbers
card is replaced in the deck and the deck is shufﬂed. If the prisoner happens to select three consecutive
red cards, he is executed. If he happens to select six cards before three consecutive red cards appear
he is granted freedom. What is the probability that the prisoner is executed?
(b) Three cards are placed in a box; one is white on both sides, one is black on both sides, and the third is
white on one side and black on the other. One card is chosen at random from the box and placed on a
table. The (uppermost) face that shows is white. What is the probability that the hidden face is black?
(c) A factory has three machines that manufacture widgets. The percentages of a total day’s production
manufactured by the machines are 10%, 35%, and 55% respectively. Furthermore, it is known that
5%, 3%, and 1% of the outputs of the respective three machines are defective. What is the probability
that a randomly selected widget at the end of the day’s production runs will be defective?
Exercise 16.1.2 Consider the problem of rolling three dice until six spots appear on all three of them. Simulate
this scenario to estimate the probability that this will take at least 30 throws.
Exercise 16.1.3 Consider the gambler’s ruin problem in which a gambler begins with $50, wins $10 on each
play with probability p = 0.45, or loses $10 with probability q = 0.55. The gambler will quit once he doubles
his money or has nothing left of his original $50. Write a simulation program to compute the expected number
of times the gambler has
(a) $90 before ending up broke;
(b) $90 before doubling his money;
(c) $10 before doubling his money;
(d) $10 before going broke.
Exercise 16.1.4 By enclosing a circle of unit radius and centered at the origin inside a square box whose sides
are of length 2, compute an approximation to the number π using a sequence of uniformly distributed random
numbers.
Exercise 16.1.5 Evaluate the following integrals using the Monte Carlo method.
(a)
Ia =
 2
0
x dx
√
2x2 + 1
.
(b)
Ib =
 π/2
π/6
cos2 θ sin θ dθ.
(c)
Ic =
 ln 2
0
e2xdx
4√
ex + 1.

Chapter 17
Uniformly Distributed “Random” Numbers
In the previous chapter we assumed that we had access to a sequence of random numbers in the
half-open interval [0, 1). But now the question arises: “What is a random number?” Is 101 a random
number? Is 33? We would be wrong to answer in the afﬁrmative to either of these questions. A
number by itself cannot be random. It is only when we look at a sequence of numbers that we may
begin to ask meaningful questions about randomness. It (randomness) is not something that we can
easily explain, but we believe that we know it when we see it! If we were to generate a sequence
of T random integers that lie the range [0, 9], then we would expect 0 to occur T/10 times, and
the same for the other nine integers. We would not accept a sequence of T zeros as being random
and yet the probability of getting T zeros is just the same as the probability of getting any other
speciﬁc sequence, including the one we end up with. It would appear that we wish to obtain random
sequences that are not entirely random!
There is not as yet any satisfactory deﬁnition of randomness, although a number of authors have
made valiant efforts. In particular, Knuth [25] devotes considerable attention to the topic and we
highly recommend reading his work to ﬂesh out the limited amount of space that we can devote to
it in this chapter. We shall be content with the deﬁnition that we gave earlier, that, by random, we
mean that if the interval were partitioned into equally sized subintervals, the next random number
to be generated would be equally likely to fall into any of the subintervals. Random numbers having
this property are said to be uniformly distributed random numbers.
Before the advent of computers, random numbers were generated by purely mechanical means.
Random digits produced in this fashion are often said to be truly random, to distinguish them from
computer-generated pseudorandom numbers. Urns containing numbered balls, much like those seen
today in lotteries, roulette machines, and other speciﬁcally engineered devices were used to produce
random integers which were arranged into tables and printed. Typically, books that contained tables
of logarithms and the like, also included several pages of random numbers arranged into groups
of ﬁve. A user began somewhere in the middle of the table and used the numbers from that point
onward. Beginning in 1927 and continuing through 1955, a number of different sources produced
tables containing from 1,600 through 250,000 random digits. Then, in 1955, the RAND corporation
published a book containing one million random digits along with material on the generation process
and the use of these tables. These were produced using electronically generated noise to increment
electronic counters.
Once computers became more widely used, programs were written to produce random numbers.
One of the earliest was the so-called middle square method von Neumann. This approach was very
simple. The method begins with an initial number called the seed: the next number in the sequence is
found by taking the middle part of the square of the current number. For example, an n-digit number
is squared to produce a 2n-digit number and some middle n-digits chosen as the next number in the
“random” sequence. However, numbers generated like this are most certainly not random. Indeed,
the sequence is completely deterministic. However, the idea is that, if examined in its entirety, the
sequence of numbers looks as if it has the properties that we like to see in sequences of random
numbers. Additionally, numbers generated in this fashion have the highly desirable property that
they can be reproduced, which can be extremely helpful in many circumstances. Unfortunately, and

626
Uniformly Distributed “Random” Numbers
despite its simplicity, the middle square method is not a good method, for the sequence repeats
after a relatively short cycle, or else degenerates into a string of zeros. With all computer generated
sequences of random numbers, the sequence will begin to repeat after a certain point. The length
of the sequence before this point is called its cycle length. Current computer generated methods
usually have extremely long cycles, as we shall see later. This is not a property that the middle
square method shared.
The development of efﬁcient and effective random number generators, generally abbreviated
to RNG, is an ongoing hot topic of research and has application in many important branches
of science, technology and entertainment, such as cryptology and gambling, to name just two.
Substantial progress has been made with both true RGNs (TRNGs) and pseudo RNGs (PRGNs).
The Distributed Systems Group at Trinity College, Dublin generates truly random sequences by
means of atmospheric noise and makes them available to the general public for free from their
web site www.Random.org. Since these numbers are genuinely random, they are not reproducible
so a user wishing to repeat an experiment, to locate a problem for example, must store the
random numbers as they are generated, to enable the exact same simulation experiment to be
repeated at a later time. This need to store random number is a major disadvantage of TRNGs
and explains why computer-generated sequences are often preferred. In situations when it is
unnecessary to rerun an experiment, such as in gambling, TRNGs are often preferred. Not only
are truly random numbers used, but the inability to reproduce the sequence is a deterrent to
cheating.
Our interest in this chapter lies not with true RNGs but with pseudo-RNGs. Pseudorandom
number generators are invariably provided as an integral function in programming languages.
However, a user should be careful because the PRNG used in some programming languages leaves
a lot to be desired. The paper by L’Ecuyer [30] is particularly enlightening in this regard. Before we
leave this general discussion on RNGs, we mention one interesting possibility based on successive
digits in the number π. Quartically and even quintically converging methods (in which the number
of correct digits of π is quintupled at each iteration) are now available and hundreds of billions of
digits have been produced. There does not appear to be a ﬁnite cycle length, and mathematicians
tell us that none exists. Additionally, statistical tests indicate that the successive digits in π satisfy
stringent requirements for randomness.
17.1 Linear Recurrence Methods
Our goal is to produce a sequence of random numbers in which each number is independently
drawn from the continuous uniform distribution on the interval [0, 1], i.e., the distribution with
density function
fX(x) =

1,
0 ≤x ≤1,
0
otherwise.
Thus each and every number in the sequence has mean equal to 1/2 and variance equal to 1/12.
This means that if we partition the interval [0, 1] into n panels, a sequence of length N will have an
average of N/n random numbers in each panel and the panel into which the next random number
is placed is independent of the placement of all previous random numbers. This then is the goal.
Unfortunately, computer generated sequences of random numbers, by virtue of a deterministic
algorithm which generates them, cannot truly meet this goal. Consequently, the onus is on the user
of such software products to ensure that the generated sequence is sufﬁciently close to the goal to
satisfy his or her requirements, and in particular to verify that the numbers are uniformly distributed
and that there is no apparent autocorrelation among them. The properties which all good random

17.1 Linear Recurrence Methods
627
number generators should possess include the following.
1.
To the greatest extent possible, the random numbers that they generate should be
independently and identically distributed on [0, 1].
2.
The cycle length must be long, since complex simulations require extremely large
quantities of random numbers. For this same reason, the algorithm must also be fast.
3.
The numbers should be replicable, so that identical runs of the simulation can be repeated
to detect anomalies.
4.
The software should be portable, so that the generator can be run and tested on a variety
of computer systems and computer languages.
5.
A “jump-ahead” feature, by which the random number at any distance η from a given
random number can be easily computed, is useful when the software is run in parallel.
The most commonly used methods for generating sequences of uniformly distributed random
numbers today are extensions and variants of the linear congruent method. The linear congruent
method (LCM), ﬁrst proposed by Lehmer [31], begins with an initial seed z0 and generates random
integers according to the recurrence formula
zi = (azi−1 + c) mod m.
(17.1)
The parameter a is called the multiplier, c the increment, and m the modulus. The initial value z0
is called the seed. The fact that the arithmetic is carried out modulo m means that Equation (17.1)
will produce at most m distinct integers. Also the process is guaranteed to be periodic: each time
the recurrence formula produces a number that is equal to the seed, one cycle has been completed
and the next cycle, which will be identical to the last, begins.
Example 17.1 A poor choice of the parameters a, c, z0, and m results in a very poor sequence.
For example, with a = 1, c = 2, z0 = 3, and m = 10, we obtain the sequence
5, 7, 9, 1, 3, 5, 7, 9, 1, 3, 5, 7, 9, . . . .
Not only do none of the even integers appear, but the odd integers are in increasing order.
When a cycle contains all possible integers from 0 through m −1, it is said to have a full period.
Such a cycle is a permutation of the integers 0, 1, . . . , m −1, the particular permutation generated
being a function of the multiplier a and the increment c. Reals in the interval [0, 1) are generated as
the successive numbers ui = zi/m. Observe that not all reals in the interval [0, 1) can be generated.
Only real numbers equal to i/m, i = 0, 1, . . . , m −1 can possibly appear. Furthermore, it is
blatantly apparent that these numbers are not “random” since each is determined mathematically
from the previous; yet we expect to end up with a sequence which “appears” random. To achieve
this appearance of randomness, the parameters m, c, and a should be chosen to satisfy the following
conditions:
• a ≥0, c ≥0,
• m > max(z0, a, c),
• c and m are relatively prime, i.e., they have no common divisor,
• a −1 is a multiple of p—for every prime p that divides m, and
• a −1 is a multiple of 4—if m is a multiple of 4.
These conditions alone are not sufﬁcient to produce satisfactory sequences of random numbers. The
last three are meant to guarantee that a full period is achieved and, for this property, these three
conditions are both necessary and sufﬁcient. In general, m should be chosen as large as possible,
since the period can never be longer than this. A good choice is the largest integer that can be
represented on the computer being used. The parameters a and c should be chosen to satisfy the
above conditions. An example from the literature which satisﬁes these as well as some additional

628
Uniformly Distributed “Random” Numbers
conditions and which appears to produce acceptable sequences is m = 231, a = 314159265, the ﬁrst
nine digits of π, and c = 453806245. In some references, the number a appears as a = 314159269.
When c is chosen to be zero, the algorithm reduces to
zi = azi−1 mod m
(17.2)
and is called the multiplicative congruent method (MCM). It involves fewer numeric operations and
is consequently somewhat faster than the LCM. Although it is not possible to get a full period with
this multiplicative alternative, periods of up to m−1 can be achieved. For a 32-bit machine, choosing
m as m = 231 −1 = 2147483647, which just happens to be a prime number, and a = 75 = 16807,
gives a period that is equal to m −1.
Example 17.2 Given a = 16807, m = 231 −1 = 2147483647, and initial seed z0 = 230455, we
obtain the following:
zi = (azi−1) mod m
ui = zi/231
z1 = 1725773538
u1 = z1/231 = 0.80362592730671
z2 = 1161716784
u2 = z2/231 = 0.54096653312445
z3 = 52670164
u3 = z3/231 = 0.02452645637095
z4 = 464183784
u4 = z4/231 = 0.21615241840482
z5 = 1876251784
u5 = z5/231 = 0.87369782105088.
Notice that the division is with m + 1, a power of 2, rather than with m. This makes the division
more efﬁcient and has negligible consequences on the quality of the generator.
The MCM method with these speciﬁc values of a and m was used extensively until rather
recently. It was the generator used in the widely distributed “Arena” software simulation package
and apparently proved to be satisfactory over a period of many years. However, some deﬁciencies
were found with this generator [30] and more sophisticated approaches are now used in Arena and
elsewhere.
A consensus is building that even cycles of length m = 231 are insufﬁcient for current critical
applications and this has lead to the development of methods that are collectively referred to as
multiple recursive generators (MRGs). The basic multiplicative congruent method may be extended
to incorporate not just the previous random integer into the formula, but a linear combination of
several of the preceding random integers. This approach forms the basis of the most widely used
RNGs today. For example, the sequence generated from
zi = (a1zi−1 + a2zi−2) mod m
incorporates the two previous random integers. In this case, two initial seeds z0 and z1 are required
but this can be accomplished by running the standard multiplicative method with one initial seed
to generate a second. A variant of this, called the additive congruent method, sets both coefﬁcients
equal to 1 and uses the previous random integer and one that is back a distance η. This yields
zi = (zi−1 + zi−η)
mod m,
and has the advantage that no multiplication is required. Since the performance characteristics of
this generator are less well known than that of the MCM generator, Knuth recommends a careful
validation of the sequence generated. Today it is common to use more than just the two previous
integers to produce the next in the sequence. In general, the ith value is obtained from the preceding

17.1 Linear Recurrence Methods
629
k values by means of the formula
zi =
⎛
⎝
k

j=1
a jzi−j
⎞
⎠mod m.
This is referred to as a modulus m, order k, MRG. It requires k initial seeds, z0, z1, . . . zk−1, all of
which may be found from the basic multiplicative congruential method and a single starting seed.
Uniformly distributed random numbers in the interval [0, 1) are obtained as ui = zi/m. With an
appropriate selection of the coefﬁcients ai, cycles of up to mk −1 can be obtained [25]. However
the number of different reals in the interval [0, 1) is not increased — this procedure’s chief purpose
is to increase the cycle length without sacriﬁcing either efﬁciency or effectiveness.
Example 17.3 Given initial seeds s0 = 3, s1 = 5, s2 = 7 and parameters a1 = 7, a2 = 1, a3 = 3,
and m = 11:
• zi = (a1zi−1) mod m has period equal to 10,
• zi = (a1zi−2 + a2zi−1) mod m has period equal to 60, and
• zi = (a1zi−3 + a2zi−2 + a3zi−1) mod m has period equal to 1330.
In all three cases, only ten distinct integers are produced, namely, 1–10 inclusive. The periods
however, increase from 10 to 60 to 1330; the ﬁrst and last of these are the maximum possible
with m = 11 and order k = 1 and k = 3.
The RANMAR generator at Cern falls into this category. It requires 103 initial seeds which are
usually set by a single integer. The period is approximately equal to 1043. Even larger periods can
be obtained by combining the results of multiple MRGs that execute in parallel. One example is
L’Ecuyer’s MGR32k3a generator [30] which combines two MRGs of order 3. Let these two MRGs
be as follows:
z1,i = (a1,1z1,i−1 + a1,2z1,i−2 + a1,3z1,i−3) mod m1,
z2,i = (a2,1z2,i−1 + a2,2z2,i−2 + a2,3z2,i−3) mod m2.
Then the next random integer in the sequence is chosen to be
xi = (z1,i −z2,i) mod m1.
L’Ecuyer recommends the following parameter values:
a1,1 = 0,
a1,2 = 1403580,
a1,3 = −810728,
m1 = 232 −209,
a2,1 = 527612,
a2,2 = 0,
a2,3 = −1370589,
m2 = 232 −22853,
and shows that the resulting period is close to 2191 ≈3 × 1057. But even the length this period pales
besides that of the colorfully named Mersenne twister generator of Matsumoto and Nishimura [33]
which has a period that is equal to 219937−1. Unlike the MGR32k3a generator, which uses the values
m1 = 232 −209 and m2 = 232 −22853, the Mersenne twister is one of a class of generators that are
based on recurrences modulo 2. Such generators produce uniformly distributed random numbers in
the interval [0, 1) in a bitwise fashion.
We conclude this section with the following two observations:
1. When it is possible to jump directly from a given random integer zi to zi+η, for some
arbitrary integer η, a property that well-designed generators should possess, then multiple
sequences can be obtained from the same recurrence formula. This is particularly appropriate
for implementing in parallel.

630
Uniformly Distributed “Random” Numbers
2. The generally stated objective of a computer program that produces sequences of random
numbers is to output numbers that are uniformly distributed on the interval [0, 1] and to do so
efﬁciently. However the occurrence of either of the end points 0 and 1 could lead to difﬁculties
in certain applications such as in the generation of random numbers that have a distribution
other than uniform. Furthermore, if any zi takes the value 0 in Equation (17.2), all succeeding
random integers will also be zero. Thus the output of a random number generator should
always lie in the interval (0, 1). This may be accomplished by using zi + 1 instead of zi and
dividing the result by m + 1 rather than m. Unless noted otherwise, in what follows, we shall
use the term “uniformly distributed random number” to designate a random number uniformly
distributed in (0, 1). Having said this, for the purpose of analysis, the interval is usually taken
to be [0, 1).
17.2 Validating Sequences of Random Numbers
There are two approaches to testing random number generators. The ﬁrst, the empirical approach,
involves running the generator many times and applying a series of statistical tests to evaluate the
“randomness” of the sequences generated. This is the approach that we develop in this section and
it can be applied to sequences of numbers obtained from both true and pseudorandom number
generators. The second approach is a mathematical analysis of the formulae upon which the
generator is based. This approach relies heavily on an understanding of mathematical number theory.
Obviously, such an approach can only be applied to computer generated random numbers. Readers
interested in this approach should consult Knuth [25].
Empirical approaches, of which there exists a large number, fall into one of two categories
depending on their objective. The ﬁrst category consists of tests whose purpose is to verify that
the numbers in a given sequence are uniformly distributed over the interval [0, 1). We shall consider
two tests in this category, the chi-square “goodness-of-ﬁt” test and the Kolmogorov-Smirnov test.
The null hypothesis H0 for these tests is that the sequence of random numbers is indeed uniformly
distributed over the unit interval. The reader is cautioned that a failure to reject the null hypothesis
means only that no evidence of nonuniformity was observed and not that the generator is guaranteed
to produce sequences that satisfy the uniform distribution criterion. The second category contains
tests designed to ensure that the numbers in the sequence are independent, that there is no correlation
among them. Here a large number of tests is at our disposition and we shall review a number of
different run tests, a gap test, and the poker test. The null hypothesis for these tests is that the
numbers satisfy our independence criterion, again with the same caveat as before. Each test requires
the speciﬁcation of a signiﬁcance level, which we denote by α and which is usually of the order of
0.01 or 0.05. This is the probability of rejecting the null hypothesis given that the null hypothesis is
in fact true. We have
α = Prob{H0 is rejected | H0 is true}.
Thus if we have a random number generator that is known to have good uniformity properties and
we perform a statistical test with a signiﬁcance level of α = 0.05 on 20 sequences of random
numbers output by this generator, we should expect that, purely by chance, the null hypothesis is
rejected once. When the statistical test is run on 100 generated sequences, we should expect the null
hypothesis to be rejected 100α = 5 times. The fact that the generator fails the test on average 1
out of every 20 times does not mean that it is defective and should be discarded but only that the
element of chance must be taken into account.
17.2.1 The Chi-Square “Goodness-of-Fit” Test
The chi-square goodness-of-ﬁt test allows us to compare a sample distribution against a theoretical
one. In our case, we would like to check just how uniformly a particular sequence of random

17.2 Validating Sequences of Random Numbers
631
numbers is distributed. To proceed, we partition an interval containing n pseudorandom numbers
into k equal subintervals and compare the count of random numbers in each subinterval to the
theoretical count of n/k. To get meaningful results, k should typically be larger than 10 and n
should be larger than 10 × k. The word “binning” is sometimes used to describe the placement of
observations into different categories, or bins. What we seek to do is to compare the distribution of
one (experimental) random variable, the one found by the experiment that consists of generating the
n pseudorandom numbers, with that of a second (theoretical) random variable, the one that consists
of exactly n/k numbers per subinterval. A very speciﬁc random variable has been constructed to
allow us to perform this comparison. It is called the chi-square, χ2, random variable and can be
used to measure the degree of similarity of two probability mass functions. If its value is smaller
than a critical value, we may conclude that the two random variables used to form χ2 have identical
distributions.
If ni denotes the number of random numbers found in the ith interval and ¯ni is the number we
should theoretically expect to ﬁnd, then the statistic
χ2 =
k

i=1
(ni −¯ni)2
¯ni
(17.3)
deﬁnes the random variable χ2. In our case, with a uniform distribution, ¯ni = n/k for all i. Values
of the cumulative distribution function of a chi-square random variable are usually obtained from
tables. These tables give the value of χ2 under different circumstances. Each row corresponds to a
different number of degrees of freedom. This is the number of ways in which the two distributions
may vary from each other. When associated with discrete densities, the number of degrees of
freedom is equal to the number of subintervals, k, minus the number of differences in the density
parameters. With the uniform density there can only be one, the mean value, so for the problem at
hand, Equation (17.3) has k −1 degrees of freedom and the (k −1)th row in the table of χ2 values
must be used.
Each column in the table corresponds to a different probability, α, typically .99, .95, .90, .50,
.10, .05, and .01. The hypothesis that the random numbers are uniformly distributed, i.e., the null
hypothesis, must be accepted if the computed value of χ2 is less than the value χ2
α obtained from the
table. We have Prob{χ2 ≤χ2
α} = 1 −α which simply means that in many tests, we should expect
the computed value of χ2 to exceed χ2
α about 100α percent of the time. A signiﬁcant variation from
this percentage, either above or below, should be viewed with concern.
Example 17.4 This may be confusing, but an example should help make the situation more clear.
The Java program presented below generates 1,000 random numbers in the range 0–9 and then
applies the chi-square test to the result. One particular run of this program produced the following:
Count[] = 94 105 90 90 108 98 100 111 101 103
which results in the value χ2 = 4.6. Let us now ask what this means in terms of the “randomness”
of the numbers generated. Since we choose k = 10, we need to look at the row that corresponds to
nine degrees of freedom (DF). This row is given as
DF
0.995
0.990
0.975
0.950
0.900
0.500
0.100
0.050
0.025
0.010
0.005
8
9
1.73
2.09
2.70
3.33
4.17
8.34
14.68
16.92
19.02
21.67
23.59
10

632
Uniformly Distributed “Random” Numbers
The acceptance region for the null hypothesis, with nine degrees of freedom and a signiﬁcance level
of α = 0.05, is χ2 ≤16.92 and our computed value of
χ2 =
1
100
10

i=1
(ni −100)2 = 4.6
which is clearly within this region. This means that the hypothesis that the random numbers are
uniformly distributed should not be rejected. To simplify, it means that with probability 0.95, the
numbers are uniformly distributed. As mention just above, these numbers were obtained with a
single run of the Java program. In other runs, different values of χ2 were obtained. Consecutive
values of 11.86, 2.94, 8.84, 9.44, 10.72 were all obtained prior to obtaining the above value of 4.6.
In all these cases, χ2 is less than 16.92 indicating that the random numbers are uniformly distributed.
import java.util.Random;
class chi2 {
public static void main (String args[]) {
Random generator = new Random();
int n = 1000;
int k = 10; double chiSquare = 0;
int [] count = new int[k];
for (int i=0; i<n; i++) {
// Generate and partition 1000 random numbers
int subinterval
= generator.nextInt(k);
count[subinterval]++;
}
for (int i=0; i<k; i++) {
// Form chi-square statistic
chiSquare = chiSquare + (count[i]-n/k) * (count[i]-n/k);
}
chiSquare = chiSquare/(n/k);
for (int i=0; i<k; i++) {
// Print results
System.out.println("Count[] = " + count[i]);
}
System.out.println("chi-Square = " + chiSquare);
}
}
It is easy to see how two-, three-, and higher-dimensional versions of this test may be developed.
The chi-square goodness-of-ﬁt test in more than one dimension is called a serial test. Serial tests
consider the numbers in the test, not one at a time, but rather in groups of two, three, or more.
Suppose we have generated a sequence of random integers between 0 and 9. Partitioning these
random numbers into consecutive pairs may be viewed as forming points on a two dimensional
plane bounded by the corners (0,0), (9,0), (0,9), and (9,9). If the numbers are independent and
identically, uniformly distributed then we should expect the points to be evenly distributed across
the 100 unit squares. A two-dimensional chi-square test can be used to test this hypothesis. Similarly,
if the sequence of random integers is partitioned into groups of three, each group of three may be
viewed as a point in three-dimensional space, and we would expect them to be equally distributed
among the 1,000 unit cubes. In general, forming groups of d ≥4 integers, may be viewed as
generating vectors of dimension d in a hyperspace of dimension d. A d-dimensional chi-square test

17.2 Validating Sequences of Random Numbers
633
may be used to test the hypothesis that these vectors are uniformly distributed in this hyperspace. A
disadvantage of high-dimension serial tests is that they require large numbers of random numbers,
in order that each hypercube will collect some minimum number of points.
17.2.2 The Kolmogorov-Smirnov Test
To set the stage for the Kolmogorov-Smirnov test, consider a sequence of N uniformly distributed
random numbers z1, z2, . . . , zN in the interval [0, 1). These numbers may be thought of as the
values assumed by a discrete random variable Z, and the probability that this random variable
assumes any particular number is taken to be 1/N. If these N numbers are now arranged in non-
decreasing order, the probability distribution function of the random variable will have the typical
staircase shape with steps occurring at each of the points z1, z2, . . . , zN and height equal to 1/N.
Example 17.5 Suppose N = 5 and the uniformly distributed random numbers happen to be .70,
.27, .16, .88 and .56. Arranging these into increasing order gives .16, .27, .56, .70, and .88 and the
probability mass function of our supposed random variable is
zi
.16
.27
.56
.70
.88
pZ(zi)
.2
.2
.2
.2
.2
Prob{Z ≤zi}
.2
.4
.6
.8
1.0
The probability distribution function is shown on the left in Figure 17.1. The distribution function
on the right arises when ten rather than ﬁve random numbers are selected.
We shall refer to the staircase function, built around a sequence of N uniformly distributed
random numbers z1, z2, . . . , zN as SN(x) and it is easy to see that SN(x) can be deﬁned by
SN(x) = number of z1, z2, . . . , zN that are ≤x
N
.
Figure 17.1 also shows the line y = x, and in the interval [0, 1] this represents the continuous
uniform distribution:
FX(x) = x,
0 ≤x ≤1.
1.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.88
0.16 0.27 
0.56 
0.70 
Five uniformly distributed random numbers.
Ten uniformly distributed random numbers.
0.88 0.93 
0.70
0.61
0.56
0.44
0.32
0.27
0.16
0.05
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 17.1. Illustration of the function SN(x) for two sets of random numbers.

634
Uniformly Distributed “Random” Numbers
As N becomes large, we should expect SN(x) to approach FX(x), a characteristic we see in
Figure 17.1 when N is increased from 5 to 10. This is where the Kolmogorov-Smirnov test plays a
role, for this test enables us to compare the continuous probability distribution function FX(x) to the
probability distribution function obtained from the sample of N random numbers. The Kolmogorov-
Smirnov test is based on the statistic
D = max |FX(x) −SN(x)|
over the range of x. We hasten to point out that this test is not only applicable when it is
desirable to compare experimental observations to the continuous uniform distribution. Indeed,
the Kolmogorov-Smirnov test is frequently used to compare customer arrivals with a Poisson
distribution, population samples with the normal distribution, reliability statistics with the Weibull
distribution, and so on.
As was the case for the chi-square random variable, the distribution of D is available from
statistical tables wherein the rows correspond to the value of N and the columns correspond
to typical signiﬁcance level values such as .20, .10, .05 and .01. Since the maximum value of
|FX(x) −SN(x)| must occur at the points at which SN(x) increases in value, it is not necessary
to evaluate |FX(x) −SN(x)| at all possible values of x ∈[0, 1) but only at the step points
zi, i = 1, 2, . . . , N. In summary, to apply the Kolmogorov-Smirnov test:
1. Generate a set of N random numbers z1, z2, . . . , zn in the interval [0, 1) and arrange them
into nondecreasing order.
2. Compute the quantities
D+ = max
1≤i≤N
 i
N −zi

,
D−= max
1≤i≤N

zi −i −1
N

,
D = max
#
D+, D−$
.
3. Consult tables of Kolmogorov-Smirnov critical values and choose the value Dα in row N
having the desired signiﬁcance level α.
4. If D ≤Dα accept the hypothesis that the numbers are uniformly distributed.
Although both the chi-square test and the Kolmogorov-Smirnov test may be used when the value
of N is large (N ≥50, for example), the Kolmogorov-Smirnov test is considered to be the more
reliable of the two. When N is small, only the Kolmogorov-Smirnov test should be used.
Example 17.6 Let us run the Kolmogorov-Smirnov test on the ﬁrst of the two data sets used to
generate Figure 17.1. With N = 5, we have
zi
.16
.27
.56
.70
.88
i/N
.20
.40
.60
.80
1.0
i/N −zi
.04
.13
.04
.10
.12
zi −(i −1)/N
.16
.07
.16
.10
.08
and thus D+ = .13, D−= .16 and D = .16. Table 17.1 gives the critical values needed to complete
the Kolmogorov-Smirnov test. For a 0.05 signiﬁcance level, we see that D.05 = .563 and, since this
is greater that D = .16, we accept the null hypothesis.
17.2.3 “Run” Tests
The chi-square and Kolmogorov-Smirnov tests as implemented in the previous section may be
used to test the distribution of randomly generated numbers against a given distribution and in
particular, to test whether they appear to be uniform or not. However, they do not provide us with
any information concerning the independence of the generated numbers. A number of run tests are
available that, while not proving independence, allow us to be reasonably conﬁdent that the numbers

17.2 Validating Sequences of Random Numbers
635
Table 17.1. Table of critical Kolmogorov-Smirnov values for some values of N.
N
α = .20
α = .10
α = .05
α = .01
...
5
.447
.510
.563
.669
...
10
.323
.369
.409
.490
...
12
.296
.338
.375
.449
...
20
.232
.265
.294
.352
...
N > 35
1.07/
√
N
1.22/
√
N
1.36/
√
N
1.63/
√
N
in the sequence appear to have this property. A run-up in a sequence of numbers is a subsequence
in which each successive number is greater than the previous. A run-down is a subsequence with
the opposite property. While this is easy to state, the devil is in the details. Currently, there is little
consensus as to what constitutes the length of a subsequence. For some, the number on which a
run-up ends is also the number at which a run-down must begin (for example, the number 8 in
the subsequence 2, 8, 7, 6, 9), while for others the number that follows the last in a run is always
discarded. Furthermore, for some, there must be at least two consecutive numbers to constitute a
run-up or run-down—for them, a number in isolation cannot be a run. A run-up of length 1 begins
with a number whose predecessor and successor are greater than it, and whose successor’s successor
is less than the successor, such as in the sequence 9, 2, 8, 7, where the numbers 2 and 8 constituting
a run-up of length 1. For others, a run-up of length 1 can be achieved by a single number such as the
number 7 in the subsequence 2, 8, 7, 6, 9, whereas 2, 8 and 6, 9 both constitute run-ups of length
2. The easy case is when the numbers involved are actually binary digits, which arises frequently,
for here the length of a run of 1’s is the number of ones bound before and after with a zero and the
length of a run of 0’s is the number of zeros bound between two ones: in essence, there is no run-up
or run-down but only runs. We shall adopt the convention that when the tests involve the sum total
of runs (both run-ups and run-downs), then a run must consist of at least two numbers. When only
run-ups or run-downs are being considered we shall follow Knuth [25] and deﬁne the length of a
run to be the number of numbers it contains.
Example 17.7 Consider the sequence 3, 5, 9, 2, 8, 7, 6, 9, 1, 3, 5, 2, 2, 6, 7, 1, 5. The diagram below
shows three run-ups of lengths 1 and three of length 2 (these are drawn above the numbers) and four
run-downs of length 1 and one of length 2 (drawn below the numbers). This is the scheme we shall
follow when comparing the total number of runs (run-ups plus run-downs) to the total number we
should expect to ﬁnd if the arrangement of the numbers in the sequence were in fact independent.
|-------|
|---|
|---|
|-------|
|-------|
|---|
3
5
9
2
8
7
6
9
1
3
5
2
2
6
7
1
5
|---|
|-------|
|---|
|---|
|---|
On the other hand, when we wish to compare the number of run-ups of length 1, the number of
run-ups of length 2, the number of run-ups of length 3, etc., i.e., the distribution of run-ups, to the
number of run-ups of different lengths that should probabilistically occur if the numbers are indeed

636
Uniformly Distributed “Random” Numbers
independent, we apply Knuth’s numbering scheme and for the particular sequence given above, we
ﬁnd there are two run-ups of length 1, three of length 2 and three of length 3 as illustrated below.
|-------|
|---|
|
|---|
|-------|
|
|-------|
|---|
3
5
9
2
8
7
6
9
1
3
5
2
2
6
7
1
5
With these speciﬁcations in place we are now ready to describe a number of run tests.
Run Test Number 1: The Total Number of Runs
The sequence of ten random numbers
.05, .23, .27, .31, .36, .55, .56, .67 .82, .93
has only one run, a run-up of length 9, while the sequence
.05, .27, .23, .36, .31, .56, .55, .82 .67, .93
has nine runs, ﬁve run-ups and four run-downs. These represent the minimum and maximum
number of runs possible in a sequence of ten numbers. If the random numbers in the sequence
were independent, it is unlikely that either of these two arrangements would occur; more correctly,
the probability of their occurrence would be small. With N random numbers, the minimum number
of runs must be 1 and the maximum must be N −1 and the expected number would lie somewhere
between these two. Let X be the random variable that represents the total number of runs in a
sequence of N uniformly distributed and independent numbers. The expectation and variance of X
are known to be
E[X] = 2N −1
3
and
Var[X] = 16N −29
90
.
When N is not small, in excess of 20 or 30, for example, the probability distribution of X
may be approximated by a normal distribution having this mean and variance, the distribution
N(E[X], Var[X]). Thus it becomes possible to test how well the number of runs x found in a
generated sequence of random numbers adheres to the theoretical estimate. The test statistic to
use is
z = x −E[X]
σX
= x −(2N −1)/3
√(16N −29)/90.
If we seek a signiﬁcance level of α, then the null hypothesis that the random numbers are
independent should not be rejected if −zα/2 ≤z ≤zα/2. For a typical signiﬁcance level, such
as α = 0.05 for example, we need to determine the value of zα/2 = z.025. Standard normal tables
provide the values of
	Z(z) =
1
√
2π
 z
−∞
e−t2/2dt = Prob{Z ≤z}.
Since Prob{Z > z} = 1 −Prob{Z ≤z}, we check the table to obtain the value that corresponds
to 1 −.025 = .975 and ﬁnd the critical value to be 1.96. Thus, for the 5% signiﬁcance level, the
condition for accepting the null hypothesis is
−zα/2 ≤z ≤zα/2
=⇒
−z.025 ≤z ≤z.025
=⇒
−1.96 ≤z ≤1.96.
This means that 95% of the normal curve lies between the limits −1.96 and +1.96, often called the
2σ level (convention rounding 1.96 to 2) or two standard deviations.
Example 17.8 A sequence of 50 random numbers is found to have a total of 30 runs. We wish to
determine if, with a signiﬁcance level of α = .05, the null hypothesis of independence should be

17.2 Validating Sequences of Random Numbers
637
rejected. Given that N = 50 and x = 30, we have
z = 30 −(100 −1)/3
√(800 −29)/90 =
−3
2.9269 = −1.0250.
We now need to determine the value of zα/2 = z.025. Since we have just found this to be 1.96 and
since −1.96 ≤−1.0250 ≤1.96, the null hypothesis should not be rejected.
Suppose that instead of ﬁnding 30 runs, a total of 40 runs is found in the sequence of 50 random
numbers. Should the hypothesis be rejected at the α = .10 signiﬁcance level?
This time we ﬁnd
z = 40 −(100 −1)/3
√(800 −29)/90 =
7
2.9269 = 2.3916.
From standard normal tables, we seek the value that corresponds to 1 −.05 = .95 and this time the
critical value is 1.645. Since 2.3916 > 1.645, the hypothesis should be rejected.
Run Test Number 2: Runs Above or Below the Mean
A somewhat similar test involves subsequences that occur above or below the mean. Consider the
following sequence of 20 two-digit random numbers in the interval [0, 1):
.05, .15, .23, .18, .06 .27, .31, .36, .32, .45, .55, .56, .51, .67, .62, .83, .96, .90, .70, .77.
That these numbers satisfy all the previous tests for uniformity and independence is left to the
exercises. However, a generator that produces such a sequence should not be used: The ﬁrst ten
numbers are all less than the mean while the last ten are all greater than the mean. This is not what
we should expect if the numbers are truly independent. Our second run test helps to detect such
an anomaly. This time a run is deﬁned as a subsequence of consecutive numbers that are less than,
or greater than, the mean. The total number of such subsequences is computed and compared to
a theoretical estimate. Let X be the random variable that represents the sum of the total number
of runs-below and runs-above the mean in a sequence of N uniformly distributed and independent
numbers. Also let n1 be the number of random numbers in the entire sequence that are greater than
the mean, and n2 the number less than the mean. Numbers that are equal to the mean are ignored.
In this case the expectation and variance of X are known to be
E[X] = 2n1n2
n1 + n2
+ 1
and
Var[X] = 2n1n2(2n1n2 −n1 −n2)
(n1 + n2)2(n1 + n2 −1) .
(17.4)
As long as one of n1 or n2 is sufﬁciently large (greater than 20, for example), then X is
approximately normally distributed and, as was the case for run test number 1, the test statistic
we need to use is
z = (x ± 0.5) −E[X]
√Var[X]
,
where E[X] and Var[X] are given in Equation (17.4) and the ±0.5 is a continuity correction; when
x > E[X] we use −0.5; when x < E[X] we use +0.5. The continuity correction may be omitted
when n1 and n2 are large.
Example 17.9 The following 40 numbers were obtained from a single run of a Java program using
the built-in Java random number generator:
.11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .00,
.06, .84, .94, .06, .60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96, .96, .09, .13, .36.
In this sequence 19 numbers are greater than the mean (n1 = 19) and 21 are smaller than the mean
(n2 = 21). The total number of runs is x = 23 which is illustrated below, where numbers less than

638
Uniformly Distributed “Random” Numbers
the mean are represented by a minus sign and those greater than the mean, by a plus sign.
−−+ + −+ + −+ + −+ −−−+ −−−−
−+ + −+ −+ −+ −+ + −+ + + + −−−
Let us now check to see if these numbers satisfy run test number 2. We have
E[X] = 2 × 19 × 21
40
+ 1 = 20.95,
Var[X] = 2 × 19 × 21(2 × 19 × 21 −40)
402 × 39
= 9.6937,
which allows us to compute
z = (23.0 −0.5) −20.95
√
9.6937
=
1.55
3.1135 = .4978.
Since this is smaller than 1.96 (the critical value of the normal distribution for a signiﬁcance level
of α = 0.05), the hypothesis that the random numbers are independent should not be rejected by
the outcome of this test.
Run Test Number 3: The Distribution of Run-ups (or Run-downs)
In this test, a generated sequence of random numbers is traversed once and the number of run-ups
(or run-downs) of length 1, 2, 3, 4, 5, and greater than or equal to 6, are counted. Since our concern
is with either run-ups or run-downs, we count an isolated element as a run of length 1. Thus, as we
mentioned previously, the sequence 3, 5, 9, 2, 8, 7, 6, 9, 1, 3, 5, 2, 2, 6, 7, 1, 5 has two run-ups of
length 1, three of length 2, and three of length 3, as illustrated below.
|-------|
|---|
|
|---|
|-------|
|
|-------|
|---|
3
5
9
2
8
7
6
9
1
3
5
2
2
6
7
1
5
From the counts of the number of run-ups of different lengths, a random variable is constructed
whose distribution tends to that of the χ2 distribution with six degrees of freedom. If we let ri,
i = 1, 2, . . . , 6, denote the number of run-ups of length i, then for large values of n, i.e., n ≥4, 000,
the random variable
X = 1
n
6

j=1
6

k=1
(r j −n × b j)(rk −n × bk)a jk
tends to the χ2 distribution. The elements of A and B have been computed by Knuth and are as
follows:
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
4529
9045
13568
18091
22615
27892
9045
18097
27139
36187
45234
55789
13568
27139
40721
54281
67852
83685
18091
36187
54281
72414
90470
111580
22615
45234
67852
90470
113262
139476
27892
55789
83685
111580
139476
172860
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
B =
#1/6
5/24
11/120
19/720
29/5040
1/840$
.
It is also possible to develop a similar test where the run lengths are taken to be the lengths of runs
above (or below) the mean. However, we caution the reader that this test should be used only when
the population from which the random numbers are drawn is large.
Example 17.10 For the sample 17 random number given above with run-ups of r1 = 2, r2 = 3,
r3 = 3 and r4 = r5 = r6 = 0, we obtain a value of X = 2.324. By examining the row corresponding

17.2 Validating Sequences of Random Numbers
639
to 6 degrees of freedom in the table of χ2 values, we may conclude, with conﬁdence level of 0.75,
that the sequence is independent.
The Java program below generates n = 4, 000 random integers in the interval [0, 9], counts the
number of run-ups of length 1 through 5 and greater than or equal to 6 and then computes the χ2
value. Removing the comment marks from the two statements that are commented out will run the
17 integer example described above.
import java.util.Random;
class run_up {
public static void main (String args[]) {
Random generator = new Random();
int n = 4000;
// n = 17;
int [] array = {3,5,9,2,8,7,6,9,1,3,5,2,2,6,7,1,5};
// Example
int [] r = new int [6];
int count = -1;
int last = -999;
for (int i=0; i<n; i++) {
int next = generator.nextInt(100);
// Next random number is ...
// next = array[i];
// Example
if (next > last) {count++; last = next; }
// ... larger than previous
else { if (count > 5) {count = 5;}
// ... smaller than previous
r[count]++;
count = 0; last = next;}
}
r[count]++;
// Print run-up counts:
System.out.println("r = " + r[0] + "
"
+ r[1] + "
"
+ r[2] +
"
"
+r[3] + "
"
+ r[4] + "
"
+ r[5]);
// A & B Parameter values
double [][] A = { { 4529,
9045, 13568,
18091,
22615,
27892},
{ 9045, 18097, 27139,
36187,
45234,
55789},
{13568, 27139, 40721,
54281,
67852,
83685},
{18091, 36187, 54281,
72414,
90470, 111580},
{22615, 45234, 67852,
90470, 113262, 139476},
{27892, 55789, 83685, 111580, 139476, 172860} };
double [] B = {1.0/6, 5.0/24, 11.0/120, 19.0/720, 29.0/5040, 1.0/840};
double X = 0;
// Compute chi-square
for (int j=0; j<6; j++) {
for (int k=0; k<6; k++) {
X = X + (r[j] - n*B[j]) * (r[k] - n* B[k])* A[j][k];
}
}
X = X/n;
System.out.println("X = " + X);
// Print chi-square
}
}

640
Uniformly Distributed “Random” Numbers
17.2.4 The “Gap” Test
This test calculates the distance between the repetition of numbers in a given sequence and makes
comparisons with what should be expected theoretically if the numbers are indeed independent.
The Kolmogorov-Smirnov test is used to assess the statistical signiﬁcance of the distance between
repetitions. If the numbers are independent, then the length of the gaps should be geometrically
distributed. When the total number of different numbers that can possibly occur is γ , the probability
that a gap has length k is
Prob{gap has length k} =
γ −1
γ
k  1
γ

,
since each individual number has probability 1/γ of occurring. The probability of a number
occurring, other than the number being tested, is (γ −1)/γ .
Example 17.11 In the following sequence of 35 single-digit integers, the integer 5 occurs in
positions 2, 9, 17, 28, and 33:
9
5
4
1
6
8
3
4
5
8
9
0
0
2
7
2
5
3
1
7
1
6
9
3
4
7
8
5
0
1
6
8
5
9
2
This gives four gaps, of length 6 (= 9 −2 −1), 7 (= 17 −9 −1), 10 (= 28 −17 −1), and 4
(= 33 −28 −1) where the length of a gap is the number of digits between the two repeated digits.
With γ = 10, the probabilities of gaps of these lengths are
Prob{L = 6} = 0.96 × 0.1 = .05314,
Prob{L = 7} = 0.97 × 0.1 = .04783,
Prob{L = 10} = 0.910 × 0.1 = .03487,
Prob{L = 4} = 0.94 × 0.1 = .06561.
When performing the gap test, it is not sufﬁcient to check the gaps between the occurrences of one
speciﬁc number, or even two or three. The distribution of the gaps for all possible numbers must
be found. When the sequence to which the gap test is applied contains real numbers rather than
integers, it is appropriate to establish intervals, such as [0, .1), [.1, .2), . . . , [.9, 1), and to tabulate
the gaps between a real number falling into one interval and the next real number that falls into the
same interval.
Example 17.12 Continuing with the example of the 35 integers, the following table shows, for
each integer, the length of each gap. Thus, for example, the integer 0 has two gaps, one of length
0 and the other of length 15 and, as we mentioned before, the integer 5 has 4 gaps of lengths
4, 6, 7, and 10.
Integer
0
1
2
3
4
5
6
7
8
9
Gap size
0
1
1
5
6
4
8
4
2
9
15
8
18
10
16
6
16
5
3
10
14
7
16
11
10
Observe that the total number of gaps is 35 −10 = 25. A moment’s reﬂection should convince the
reader that the number of gaps is always equal to N −γ , where N is the number of numbers in the

17.2 Validating Sequences of Random Numbers
641
sequence and γ is the number of different values possible. Using these numbers, the computed gap
length distribution and the relative frequency of occurrence, for this set of 35 integers, is
Gap length
0–1
2–3
4–5
6–7
8–9
10–11
12–13
14–15
16–17
18–19
Occurs
3
2
4
3
3
4
0
2
3
1
Relative Frequency
.12
.08
.16
.12
.12
.16
.00
.08
.12
.04
This table illustrates the fact that the gap lengths are usually combined into groups of size 2, 3, 4,
or 5.
The theoretical distribution, the distribution which arises if the numbers are truly independent
and against which the computed distribution must be compared, via the Kolmogorov-Smirnov test,
is as follows:
FX(x) = Prob{X ≤x} = 1
γ ×
x

k=0
γ −1
γ
k
= 1 −
γ −1
γ
x+1
.
Example 17.13 Continuing with the example of 35 integers, the values of x at which we evaluate
FX(x) = Prob{X ≤x} are x = 1, 3, 5, 7, 9, 11, 13, 15, 17 , 19.
Gap length
0–1
2–3
4–5
6–7
8–9
10–11
12–13
14–15
16–17
18–19
Relative
.12
.08
.16
.12
.12
.16
.00
.08
.12
.04
Frequency
SN(x)
.12
.20
.36
.48
.60
.76
.76
.84
.96
1.0
FX(x)
.1900
.3439
.4686
.5695
.6513
.7176
.7712
.8417
.8499
.8784
|FX(x) −SN(x)|
.0700
.1439
.1086
.0895
.0513
.0424
.0112
.0017
.1101
.1216
Thus maxx |FX(x) −SN(x)| = .1439. The critical value Dα to use in the Kolmogorov-Smirnov test
is found from tables as D0.05 = .409, and, since .1439 is less than this, the null hypothesis should
not be rejected on the basis of this gap test.
17.2.5 The “Poker” Test
This test identiﬁes sequences of random numbers with “hands” in a poker game. When the random
numbers belong to a certain range, such as the integers 0 through 9, or have been “binned” into
a ﬁxed number of categories, then it becomes possible to take them in groups and test for the
occurrence of different combinations. In the poker analogy, the numbers are taken in groups of ﬁve
(a hand) and checked to see if the hand contains all different numbers (or belong to ﬁve different
bins), a pair (two the same and the other three different from each other and different from the pair)
and so on. If we let A, B, C, D, and E be ﬁve different possible numbers, or the designation of
ﬁve different bins, then examples of the possible poker hands are
ABC DE
All different,
AAC DE
One pair,
AACC E
Two pair,
AAADE
Three of a kind,
AAABB
Full house,
AAAAB
Four of a kind.

642
Uniformly Distributed “Random” Numbers
If the numbers are uniformly and independently distributed, then it is possible to compute the
probability of each of these hands and with this information it becomes possible to gauge the
independence property of the random number generator. The chi-square test is used to determine
if the random sequence conforms to the expected distribution. Even though it is not possible to get
ﬁve of a kind in poker, it is possible to handle this situation (AAAAA) in the poker test. Also, the
number of cards in a hand need not be equal to ﬁve, as the next example now shows.
Example 17.14 Consider a sequence in which each element (hand) is a three-digit random number.
This allows for the possibility of only three poker hands, with theoretical probabilities given by
Prob{3 different digits} = 1 × .9 × .8 = .72,
Prob{3 of a kind} = 1 × .1 × .1 = .01,
Prob{1 pair} = 1 −.72 −.01 = .27.
Suppose a random number generator produces 1000 such numbers and it is found that 700 of then
have three digits all different, 15 of then have all three digits the same and the rest, 285, have exactly
one pair. Forming the chi-square statistic, we ﬁnd
χ2 = (720 −700)2
720
+ (10 −15)2
10
+ (270 −285)2
270
= .5556 + 2.5000 + .8333 = 3.8889.
Consulting tables of chi-square critical values, and using line 2 corresponding to two degrees of
freedom (one less than the number of intervals) we ﬁnd χ2
.05 = 5.99. Since χ2 < χ2
.05, the hypothesis
that the numbers are independent cannot be rejected.
In practice, the poker test is frequently replaced with a simpler test, but one that is easier to work
with and whose performance is almost as good as the regular poker test. In this new version we
simply count the number of different values in the hand. Thus the two categories, “two pairs” and
“three of a kind” are combined into the single category “three different values,” while both “full
house” and “four of a kind” are combined into the single category, “two different values.”
r = 5
all 5 cards are different,
r = 4
one pair,
r = 3
two pairs or three of a kind,
r = 2
full house or four of a kind,
r = 1
ﬁve of a kind.
The probabilities of these different possibilities when the digits are independent are:
Number of different values, r
5
4
3
2
1
Probability
.3024
.5040
.1800
.0135
.0001
For example, the probability of getting ﬁve different values is 1 × .9 × .8 × .7 × .6 = .3024, while
the probability of getting all ﬁve the same is 1 × .1 × .1 × .1 × .1 = .0001. It can be seen that the
probability of getting only one or two different values is quite small, .0001 and .0135, respectively.
Before applying the chi-square test it is usual to lump low-probability categories together. In fact,
this lumping of low probabilities is recommended whenever the chi-square test is used.
The formula for computing these probabilities in a more general context is given in terms of
Sterling numbers of the second kind, S(k,r), as
Prob{r different values} = d(d −1)(d −2) · · · (d −r + 1)
dk
S(k,r),

17.2 Validating Sequences of Random Numbers
643
where d is the number of different “cards” (d = 10 in our example, the digits 0, 1, . . . , 9), k is the
number of cards in a hand (k = 5 in a regular poker hand) and r is the number of different values
in a hand (r = 2 for “four of a kind” or “full house”). The Sterling numbers can be computed by
means of the recursive formula
S(k,r) = S(k −1,r −1) + r × S(k −1,r).
This leads to the following convenient “Sterling’s Triangle”:
r = 1
r = 2
r = 3
r = 4
r = 5
r = 6
k = 1
1
k = 2
1
1
k = 3
1
3
1
k = 4
1
7
6
1
k = 5
1
15
25
10
1
k = 6
1
31
90
65
15
1
Example 17.15 A sequence of 1,000 octal numbers generated at random shows the following
distribution of different values found in consecutive groups of four:
r
4
3
2
1
Number
99
120
28
3
Thus, for example, out of the 250 groups of four, exactly 99 have all different octal values. Given
that d = 8 and k = 4 , the probabilities for obtaining r = 4, 3, 2, 1 different octal values, under the
assumption that numbers in the sequence are independent of each other, are computed as
Prob{4 different values} = 8 × 7 × 6 × 5
84
× 1 = .410156,
Prob{3 different values} = 8 × 7 × 6
84
× 6 = .492188,
Prob{2 different values} = 8 × 7
84
× 7 = .095703,
Prob{1 different value} = 8
84 × 1 = .001953.
The expected frequency of occurrence is obtained by multiplying these probabilities by 250. Since
the last case in which all four octal numbers are the same (r = 1) has a very small probability, we
shall lump it in with the preceding case (r = 2) and we apply the chi-square test to the data
r
4
3
≤2
Observed
99
120
31
Expected
102.54
123.05
24.41
The appropriate chi-square statistic to use is
3.542
102.54 + 3.052
123.05 + 6.592
24.41 = 1.9769.

644
Uniformly Distributed “Random” Numbers
With two degrees of freedom and a signiﬁcance value of α = .05, the critical chi-square value is
χ2
α = 5.99, and since 1.9769 is less than this we cannot reject the null hypothesis that the 1,000
numbers are independent.
17.2.6 Statistical Test Suites
The statistical tests for uniformity and independence of the numbers produced by random
number generators presented in this chapter constitute only a small portion of the tests that are
possible and which have been incorporated into various software packages. Two test suites of
renown are the NIST (National Institute of Standards and Technology) statistical test suite, which
contains a collection of 16 tests and is available at http://csrc.nist.gov/rng/index.html
and the DIEHARD suite developed by George Marsaglia at Florida State University
(http://stat.fsu.edu/pub/diehard/ which contains 12 different tests (more if we count the
three binary rank test and the different overlapping tests separately). Many of these tests have
colorful names such as “the squeeze test,” “the birthday spacings test,” “the parking lot test,” and
so on. Descriptions of all these tests along with software implementations can be found on the
appropriate web page.
17.3 Exercises
Exercise 17.1.1 A linear congruential random number generator has a multiplier of ﬁve, an increment of 3,
and a modulus of 16.
(a) What is the period of this generator?
(b) Using z0 = 3, generate the complete cycle of random integers and their associated random numbers
in the interval [0, 1].
Exercise 17.1.2 Given that a LCM has multiplier equal to 13, increment equal to 5, and the modulus equal to
64, does this LCM satisfy the conditions for a full period? Using these parameter values, compute z453, given
that z0 = 7.
Exercise 17.1.3 Consider the following two MRGs:
z1,i = (a1,1z1,i−1 + a1,2z1,i−2 + a1,3z1,i−3) mod m1,
z2,i = (a2,1z2,i−1 + a2,2z2,i−2 + a2,3z2,i−3) mod m2,
which are to be combined to produce the next random integer in the sequence as
xi = (z1,i −z2,i) mod m1.
Given the following starting values:
z1,1 = 45, z1,2 = 100, z1,3 = 57,
z2,1 = 17, z2,2 = 1, z2,3 = 24
and parameters
a1,1 = 12,
a1,2 = 0,
a1,3 = 89,
m1 = 27 −1 = 127,
a2,1 = 0,
a2,2 = 14,
a2,3 = 28,
m2 = 25 −1 = 31,
compute the ﬁrst six random integers.
Exercise 17.1.4 The following are well-known results on modulo arithmetic: If m is a positive integer and
a1, a2, . . . , ak are integers, then
(a1 + a2 + · · · + ak)mod m = [(a1mod m) + (a2mod m) + · · · + (akmod m)] mod m,
(a1a2 . . . ak)mod m = [(a1mod m)(a2mod m) · · · (akmod m)] mod m.

17.3 Exercises
645
With these results, it is easy to see that the “jump ahead by η” computation from some random integer zi in an
MCM generator with multiplier a and modulus m can be accomplished by forming
zi+η = (aηzi)
mod m,
since
zi+1 = (azi)
mod m,
zi+2 = (azi+1)
mod m = a[(azi)
mod m]
mod m = (a2zi)
mod m,
and so on, where we have used the fact that a mod m = a, since a < m. Derive a formula for the “jump ahead
by η” procedure when the recurrence relation is given by
zi = (a1zi−1 + a2zi−2) mod m.
Hint: Think in terms of matrices. Generalize this to the case when the recurrence relation is
zi =

k

j=1
a jzi−j

mod m.
Exercise 17.2.1 Consider the following sequence of 20 two-digit random numbers in the interval [0, 1):
.05, .15, .23, .18, .06, .27, .31, .36, .32, .45, .55, .56, .51, .67, .62, .83, .96, .90, .70, .77.
Do these numbers satisfy, at the α = .05 signiﬁcance level, the chi-square goodness of ﬁt test and the
Kolmogorov-Smirnov test for uniformly distributed random numbers?
Exercise 17.2.2 A sequence of 500 random numbers in the interval [0, 1) is analyzed and partitioned into ten
equal subintervals (or bins). The following distribution is found:
Count[] = 45 54 39 6251 36 65 40 47 56
Apply the chi-square goodness-of-ﬁt test to this data and determine if the sequence from which it was drawn
can be assumed to be uniformly distributed, based on the results of this test.
Exercise 17.2.3 The following ﬁve numbers were produced by a random number generator: .11, .98, .66, .76,
.75. Use the Kolmogorov-Smirnov test to determine whether this generator is likely to be satisfactory.
Exercise 17.2.4 Dr. Harry Perros is examining data that he has just collected concerning arrivals and which
he wishes to incorporate into his queueing model. He would be very happy if the data showed that the arrival
pattern is Poisson. Here is the data he has collected on the number of customers that were observed to arrive in
a one-hour period
Number of Customers
0–1
2–3
4–5
6–7
8–9
10–11
12–13
14–15
Observed Frequency
18
24
40
68
37
16
10
8
Show that the average arrival rate is λ = 6.4005. and use the Kolmogorov-Smirnov test to see if Dr. Perros can
accept the hypothesis that his arrival process is Poisson. You may take λ = 6.4 which will enable you to use
tables of the cumulative Poisson distribution function.
Exercise 17.2.5
Apply each of the three different run tests to the following set of 60 two-digit random
numbers:
.10, .23, .69, .72, .66, .57, .01, .61, .62, .08, .17, .72, .87, .91, .12, .20, .57, .71, .04, .17,
.36, .08, .13, .34, .53, .49, .35, .99, .95, .12, .08, .29, .55, .05, .13, .22, .36, .81, .02, .21,
.70, .26, .21, .55, .44, .62, .72, .11, .69, .94, .45, .29, .14, .18, .82, .03, .26, .46, .06, .67.
Exercise 17.2.6 Let the interval [0, 1) be partitioned into ﬁve equal subintervals, [0, .2), [.2, .4), [.4, .6), [.6, .8)
and [.8, 1)] and label these intervals 0–4. Then each of the 60 random numbers in Exercise 17.2.5 falls into
exactly one of these subintervals and the sequence can now be written as 0, 1, 3, 3, 3, 2, . . .. Apply the gap
test to this sequence.

646
Uniformly Distributed “Random” Numbers
Exercise 17.2.7 Apply the modiﬁed poker test to the following sequence of 96 ﬁve-digit numbers to determine
if the digits in the sequence of 96 × 5 numbers are independent.
21888
32305
11069
96023
55298
34930
93277
84330
79985
75668
61162
68190
77828
71216
46311
55800
09750
10581
30819
83187
42839
98288
57491
09273
55645
95543
31450
56349
75385
96466
63466
09912
10236
97266
57016
16208
17728
79112
20577
10417
36081
33453
49359
99512
08295
50513
22368
10221
70262
15544
62721
16994
45291
41882
03264
60667
36964
96614
87652
49247
83383
56255
37500
69181
83673
75866
24760
86738
00851
69676
38679
62215
16544
28211
67438
39437
69344
27255
68909
92754
91468
20987
61089
57695
64151
10170
38985
20017
77179
06903
44204
60726
15736
59590
92961
07884.

Chapter 18
Nonuniformly Distributed “Random” Numbers
A fundamental concept in simulation is the notion of an event. An event may be deﬁned as any action
which causes the system undergoing study to change state, and the occurrence and effect of these
events on the system must be examined and analyzed. The distribution of the occurrence of events is
most likely not uniformly distributed so it becomes necessary to have access to sequences of random
numbers that are perhaps exponentially distributed or normally distributed, or have some other
appropriate distribution, rather than the uniform distribution. This is accomplished by generating
a sequence of uniformly distributed random numbers and then transforming this sequence to a
sequence having the required distribution. In this chapter we shall examine the principal approaches
to performing this transformation. These approaches are the inverse transformation method, a
“mimicry” approach, the accept-reject method, and compositional methods, including methods
that partition the area under the density function, such as the rectangle-wedge-tail method and
the ziggurat method. In some cases, a speciﬁc probability distribution is best suited to only one
transformation method, while other distributions may be handled equally conveniently by more than
one method. Random numbers that satisfy a particular distribution function are sometimes referred
to as random variates.
18.1 The Inverse Transformation Method
The ﬁrst method we discuss is that of inversion, also called the inverse transformation method,
which may be used only when the distribution function can be analytically inverted. Suppose we
wish to generate random numbers that are distributed according to some cumulative distribution
function F(x). The basic idea is to generate a sequence of uniformly distributed random numbers
and to derive from this sequence a different sequence of random numbers that has the distribution
F(x). Observe that F(x) is a probability, the probability that the random variable X has a value less
than or equal to x, i.e.,
F(x) ≡FX(x) = Prob{X ≤x},
and hence has a value that lies between 0 and 1, just like our uniformly distributed random numbers.
If the distribution function F(x) is a strictly increasing function, then there exists an inverse function,
denoted by F−1, that corresponds to F. In other words, if x = F−1(y), then y = F(x).
We saw in a much earlier chapter that, given a random variable X, a new random variable Y
that is some function of X may be created, such as Y = X2 + 2X, Y = eX, and so on. In the
inverse transformation method we let the deﬁning function of Y be none other than the probability
distribution function of X itself. We now show that the distribution function of Y, deﬁned in this
manner, is the uniform distribution in the interval [0, 1]. We have
FY(y) = Prob{Y ≤y} = Prob{FX(x) ≤y}.
If we assume that FX(x) can be inverted, then we have
Prob{FX(x) ≤y} = Prob{X ≤F−1
X (y)}
for
0 ≤y ≤1.

648
Nonuniformly Distributed “Random” Numbers
In other words
FY(y) = Prob{X ≤F−1
X (y)}
for
0 ≤y ≤1.
But Prob{X ≤x} = FX(x), and replacing the x with F−1
X (y), we obtain
FY(y) = FX(F−1
X (y)) = y
for
0 ≤y ≤1.
In other words, Y is uniformly distributed on the interval [0, 1].
To summarize therefore, to generate random numbers according to some distribution function F
that is invertible, it sufﬁces to generate random numbers that are uniformly distributed and to apply
the inverse function F−1 to these numbers. This is illustrated graphically in Figure 18.1.
1
F(x)
x
a
F(a)
−1
u
F    (u)
y
Figure 18.1. Illustration of an invertible function.
Given a uniformly distributed random number u, a line parallel to the x-axis is drawn until it
reaches the curve y = F(x) at which point a vertical line parallel to the y-axis is drawn. The
point at which this line intersects the x-axis, F−1(u), is taken as a random number that satisﬁes the
distribution F.
Not all distribution functions are invertible. For example, no discrete distribution function can be
inverted, since such a function is a stepwise function rather than one that is strictly increasing.
1
x
b
a
F(x)
y
Figure 18.2. Illustration of step function.
In such cases, it is still possible to use the inversion approach. As before, a uniformly distributed
random number is generated and a line parallel to the x-axis drawn through this number. This line
intersects a riser within the step function. The x-value at this riser is taken as the value of the
discrete random variable. Thus, a number that falls in the interval [F(xi−1), F(xi)) is associated
with the event X = xi. This is illustrated in Figure 18.2. Put another way, this approach partitions
the interval [0, 1] into subintervals, and the length of each subinterval corresponds to the probability
of one of the events that can occur. Since the sum of the probabilities of all the events is exactly
equal to one, each subinterval corresponds to one, and only one, event. This is exactly the procedure
we used in Chapter 16 when simulating simple probability experiments.

18.1 The Inverse Transformation Method
649
We now consider a number of probability distributions that can be handled by this method.
Since we have formulated the analysis in terms of y = F(x), we shall, in this section, refer to
the sequences of uniformly distributed random numbers as yi rather than ui.
18.1.1 The Continuous Uniform Distribution
The probability density and cumulative distribution functions for continuous random variables that
are uniformly distribution on the interval [a, b] are respectively given by
f (x) =
	
1/(b −a),
x ∈[a, b],
0
otherwise,
and
F(x) =
 x
a
1
b −a dt = x −a
b −a .
Setting y = (x −a)/(b −a), we obtain the inverse by writing x in terms of y. We have
x = a + (b −a)y.
Thus a sequence of uniformly distributed random numbers (y) in the interval [0, 1] is converted to
a sequence of random numbers (x) that is uniformly distributed in the interval [a, b].
Example 18.1 A sequence of uniformly distributed random numbers1 that begins
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .20, . . .
converts to the following sequence of random numbers uniformly distributed in the range (2, 6):
xi = 2.44, 2.60, 4.48, 5.76, 2.44, 5.68, 4.28, 2.08, 5.96, 2.80, . . . ,
where each xi is obtained by multiplying yi by 4 and adding 2.
18.1.2 “Wedge-Shaped” Density Functions
The hypotenuse of a right angle triangle of height h and width w with right angle placed at the origin
can, under certain constraints on h and w, be taken as the density function of a random variable X
deﬁned on [0, w] as shown in Figure 18.3. The area of this right-angle triangle is hw/2. For the
hypotenuse to represent a density function this area must be equal to 1 and thus we must have
hw/2 = 1 or h = 2/w. The density function is given by
fX(x) =

(1 −x/w)h,
0 ≤x ≤w,
0
otherwise.
w
h
0
x
y
Figure 18.3. A “wedge-shaped” density function on [0, w].
1 Recall that, with respect to the generation of random numbers, we use the term uniformly distributed random numbers
to mean random numbers that are uniformly distributed in (0, 1).

650
Nonuniformly Distributed “Random” Numbers
The cumulative distribution function is obtained by integrating fX(t) between 0 and x. We have,
for 0 ≤x ≤w,
 x
0
f (t)dt = h
 x
0

1 −t
w

dt = h

t −t2
2w

x
0
= hx−hx2
2w = hx

1 −x
2w

= 2x
w

1 −x
2w

.
Hence the cumulative distribution function is
FX(x) =
⎧
⎪
⎨
⎪
⎩
0,
x < 0,
2x
w

1 −x
2w

,
0 ≤x ≤w,
1,
x > w.
We now invert this function in order to be able to generate random numbers that obey a wedge
density function. Let
y = 2x
w

1 −x
2w

.
This gives rise to the quadratic equation
x2 −2wx + w2y = 0
with roots
x = 2w ±
.
4w2 −4w2y
2
= w ± w
.
1 −y = w(1 ±
.
1 −y).
We require the root x = w(1 −√1 −y), since our concern is with x ∈[0, w]. This is the inverse
transformation we need.
Example 18.2 Let us consider the speciﬁc wedge density function having h = 1. It follows that
w = 2 and, from the following sequence of uniformly distributed random numbers yi:
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01,
each xi is obtained as xi = 2(1−√1 −yi). Thus the following sequence of random numbers satisfy
the wedge density:
0.1132, 0.1561, 0.7671, 1.5101, 0.1132, 1.4343, 0.6885, 0.0201, 1.8000, 0.9046,
0.3875, 0.8511, 0.3875, 0.4000, 0.0921, 0.6000, 0.2224, 0.1238, 0.4638, 0.0100.
For example, 2(1 −√1 −.11) = .1132, 2(1 −√1 −.15) = .1561, and so on.
Naturally, it is possible to deﬁne a wedge density similar to the one given above, but having the
right angle at some distance a from the origin. It is also possible for the right angle to be on the right,
rather than on the left as was the case discussed above. These possibilities are left to the exercises.
18.1.3 “Triangular” Density Functions
Related to wedge-shaped density functions are the triangle density functions, one of which is
illustrated in Figure 18.4. Given that the base is of length (b −a), the height h must be equal to
2/(b −a) for the function to be a genuine density function. The point c, the x-value at which the
triangle peaks, is called the “mode.”

18.1 The Inverse Transformation Method
651
a
0
c
2/(b−a)
h
y
b
Figure 18.4. A “triangle” density function on [a, b].
The reader may wish to verify that the density function is given by
fX(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
2(x −a)
(b −a)(c −a),
a ≤x ≤c,
2(b −x)
(b −a)(b −c),
c ≤x ≤b,
and by integrating fX(x), the cumulative distribution function is found to be
FX(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
x ≤a,
(x −a)2
(b −a)(c −a),
a ≤x ≤c,
1 −
(b −x)2
(b −a)(b −c),
c ≤x ≤b,
1,
x ≥b.
This distribution has mean and standard deviation
E[X] = a + b + c
3
,
σ =
.
(a −b)2 + (a −c)2 + (b −c)2
6
.
To generate random numbers having this distribution, it is necessary to invert the cumulative density
function. This must be done in two parts, the ﬁrst part corresponding to the base (a, c) and the second
corresponding to the base (c, b). For a ≤x ≤c we set
y =
(x −a)2
(b −a)(c −a),
which inverts to give
x = a +
.
(b −a)(c −a)y,
0 ≤y ≤c −a
b −a .
Notice that we need to pay attention to the range of values of y. When x = a, this implies that
y = 0, while when x = c, we ﬁnd that y = (c −a)/(b −a).
In a similar manner, for c ≤x ≤b we set
y = 1 −
(b −x)2
(b −a)(b −c),
which inverts to give
x = b −
.
(b −a)(b −c)(1 −y),
c −a
b −a ≤y ≤1.

652
Nonuniformly Distributed “Random” Numbers
Example 18.3 Let us generate some random numbers according to a triangular density function
with parameters a = 2, c = 6, and b = 7 and using the sequence of uniformly distributed random
numbers
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01.
When yi ≤(c −a)/(b −a) = 4/5 we use the formula
xi = a +
.
(b −a)(c −a)y = 2 +
.
5 × 4 × yi = 2(1 +
.
5yi),
and when yi ≥4/5, we use
xi = b −
.
(b −a)(b −c)(1 −yi) = 7 −
.
5 × 1 × (1 −yi) = 7 −
.
5(1 −yi).
Observe that both give the same value when yi = 4/5, namely, xi = 6. The above sequence of
uniformly distributed random numbers gives rise to the following sequence of random numbers that
obey the designated triangular density function:
3.4832, 3.7321, 5.5214, 6.4523, 3.4832, 6.3675, 5.3764, 2.6325, 6.7764, 5.7417,
4.6458, 5.6606, 4.6458, 4.6833, 3.3416, 5.1937, 4.0494, 3.5492, 4.8636, 2.4472.
For example, x1 = 2(1 +
√
.55) = 3.4832 and x4 = 7 −√5(1 −.94) = 6.4523.
18.1.4 The Exponential Distribution
The probability density and cumulative distribution functions for an exponentially distributed
random variable are as follows:
f (x) =

λe−λx,
x ≥0; λ > 0,
0
otherwise,
and
F(x) =
 x
0
f (t)dt = 1 −e−λx.
Since F(x) = 1 −e−λx, we obtain the inverse, x, as follows:
y = 1 −e−λx,
e−λx = 1 −y,
−λx = ln(1 −y),
x = −1
λ ln(1 −y).
(18.1)
Generating random numbers that are uniformly distributed on (0, 1) and substituting them into the
right-hand side of Equation (18.1) results in a sequence of random numbers that are exponentially
distributed with parameter λ. This formula may be simpliﬁed somewhat, by observing that if y is
uniformly distributed on (0, 1), then so also is 1 −y and we may write
x = −1
λ ln(y),
(18.2)
thereby saving one arithmetic operation. However, some sources prefer to use the original Equation
(18.1), arguing that the saving is minimal and preferring, on a purely aesthetic basis, that a large
value of y give a large value of x, and vice versa. Equation (18.2) has reverse monotonicity, meaning
that large values of y give small values of x, and vice versa.
Example 18.4 A sequence of uniformly distributed random numbers in (0, 1) that begins
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .20, . . .

18.1 The Inverse Transformation Method
653
converts to the following sequence of exponentially distributed random numbers with mean value
0.25 (parameter λ = 4):
xi = .5518, .4743, .1195, .0155, .5518, .0208, .1405, .9780, .0025, .4024, . . . .
Each xi is obtained as xi = −ln(yi)/4.
A drawback with the inverse function method for generating random numbers that are exponen-
tially distributed is the computational cost associated with the logarithmic function.
18.1.5 The Bernoulli Distribution
The Bernoulli probability mass function is given by
Prob{X = 0} = q,
Prob{X = 1} = p,
where p + q = 1. Its corresponding cumulative distribution function is given as
F(x) =
⎧
⎨
⎩
0,
x < 0,
q,
0 ≤x < 1,
1,
x ≥1,
and hence has the typical step function graph. If the uniformly distributed random number that is
generated lies in the interval [0, q), the random variable is considered to have the value 0; if the
random number lies in the interval [q, 1) it is taken to have the value 1.
Example 18.5 Suppose a biased coin turns up heads eight times for every ten times tossed.
A sequence of uniformly distributed random numbers in (0, 1) that begins
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .20, . . .
produces the following sequence of tosses:
xi = H, H, H, T, H, T, H, H, T, H, . . . .
A head (H) is obtained every time yi is strictly less than 0.8; otherwise, a tail is obtained.
18.1.6 An Arbitrary Discrete Distribution
Consider a more general case of a random variable having a discrete probability mass function and
probability distribution function. Suppose, for example, these are given as
p(xi) = Prob{X = xi}
and
F(x) = Prob{X ≤x}.
We now generate uniformly distributed random numbers in the interval [0, 1). As we mentioned
before, a number that falls in the interval [F(xi−1), F(xi)) is associated with the event X = xi.
Example 18.6 Suppose we have
xi
1
2
3
4
5
6
7
8
p(xi)
0.05
0.10
0.25
0.15
0.10
0.20
0.10
0.05
and
xi
1
2
3
4
5
6
7
8
F(xi)
0.05
0.15
0.40
0.55
0.65
0.85
0.95
1.00

654
Nonuniformly Distributed “Random” Numbers
If the uniformly distributed number that we generate is given by y, then
if y < 0.05,
then X = 1,
if 0.05 ≤y < 0.15,
then X = 2,
if 0.15 ≤y < 0.40,
then X = 3,
if 0.40 ≤y < 0.55,
then X = 4,
if 0.55 ≤y < 0.65,
then X = 5,
if 0.65 ≤y < 0.85,
then X = 6,
if 0.85 ≤y < 0.95,
then X = 7,
if 0.95 ≤y < 1.00,
then X = 8.
Thus, given the following sequence of uniformly distributed random numbers in (0, 1):
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .20, . . . ,
the random variable X assumes the values
xi = 2, 3, 5, 7, 2, 7, 5, 1, 8, 3, . . . .
18.2 Discrete Random Variates by Mimicry
A number of important discrete random variables arise in modeling probabilistic scenarios and so it
becomes possible to generate random numbers having the distribution of these random variables by
mimicking (or simulating) the probability experiment. Binomial, geometric, and Poisson random
variables fall into this category and the “mimicry” approach for generating random numbers
having these distributions is very convenient. As we shall see, multiple uniformly distributed
random numbers ui ∈(0, 1) are required to produce a single random number having the sought-
for distribution. It is important to remember that each ui can be used only once. For example, if
three values are needed to produce a single random number having the required distribution, then
u1, u2 and u3 should be used to generate the ﬁrst; u4, u5 and u6 to generate the second and so on. In
particular, it is wrong to use u1, u2, and u3 for the ﬁrst and then use u2, u3, and u4 for the second.
18.2.1 The Binomial Distribution
The probability mass function of a binomial random variable X is given by
pk = Prob{X = k) = pX(k) =
⎧
⎨
⎩

n
k

pkqn−k,
0 ≤k ≤n,
0
otherwise.
The corresponding cumulative distribution function is denoted by B(t; n, p) and is given as
B(t; n, p) = FX(t) =
⌊t⌋

i=0

n
i

pi(1 −p)n−i.
We previously identiﬁed this distribution with the number of successes obtained in a sequence of
Bernoulli trials. We simply simulate this situation to generate random numbers that are distributed
according to the binomial law. We generate a sequence of n uniformly distributed random numbers
in (0, 1) and count the number of times that we obtain a number less than p.
int k = 0;
for (int i=0; i< n; i++) {
u = random();
if ( u < p ) { k++; }
return k;

18.2 Discrete Random Variates by Mimicry
655
This approach can be very costly when n is large. We shall return to this distribution in the next
section as it will provide an example for the accept-reject method of generating nonuniform random
numbers.
Example 18.7 A simulation study incorporates a binomial random variable with parameters n = 5
and p = .25. The following sequence of uniformly distributed random numbers:
ui = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01
.06, .84, .94, .06, .60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96, .96, .09, .13, .36
produces the following sequence of binomial random numbers:
k = 3, 1, 1, 3, 2, 1, 1, 2.
For example, the ﬁrst group of ﬁve numbers, u1–u5, contains three that are strictly less than 0.25
and hence k = 3.
18.2.2 The Geometric Distribution
The probability mass function of a random variable that has a geometric distribution is given by
pX(k) = qk−1 p = p(1 −p)k−1, k = 1, 2, . . . .
This corresponds to a sequence of k −1 failures (with probability q = 1 −p) followed by a single
success (with probability p, 0 < p < 1). Its image is the set of integers greater than or equal to one.
The corresponding cumulative distribution function is given as
FX(k) = Prob{X ≤k} =
k

i=1
p(1 −p)i−1 = 1 −(1 −p)k = 1 −qk
for k = 1, 2, . . . .
Thus we obtain geometrically distributed random numbers by generating uniformly distributed
random numbers between 0 and 1 and counting the number of times the random number lies in
the interval [0, q) before we obtain one that does not satisfy this property. To this total, we add 1 for
the “successful” outcome.
int k = 1;
u = random();
while ( u < q ) {
k++;
u = random();}
return k;
This algorithm is most efﬁcient if the while loop is executed only a small number of times, i.e., if the
probability of success is high (q is small). Otherwise, inverting the cumulative distribution function
may be more efﬁcient. In this case, k is obtained from
k =
Cln(y)
γ
D
+ 1,
where γ = ln q. Observe that γ need only be computed once, and not for every random number
generated. The proof of the inverse formula is left to the exercises.
Example 18.8 Let us repeat the previous example, this time using the geometric distribution with
parameter q = 1 −p = .45. The sequence of uniformly distributed random numbers, is
.11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01
.06, .84, .94, .06, .60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96, .96, .09, .13, .46.

656
Nonuniformly Distributed “Random” Numbers
The following geometric random numbers are generated:
k = 3, 1, 2, 1, 2, 1, 2, 4, 6, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 3.
For example, since u1 = .11 < .45 and u2 = .15 < .45 but u3 = .62 ≥.45, we ﬁnd k = 2 + 1 = 3
to start the sequence of geometric random numbers.
18.2.3 The Poisson Distribution
The probability mass function of a Poisson random variable N with mean value α > 0, usually
denoted as f (k, α), is given by
Prob{N = k} = f (k, α) = e−α αk
k! for k = 0, 1, 2, . . . .
The cumulative distribution function is given by
F(x) = e−α
j

k=0
αk
k!
for
j ≤x < j + 1.
We previously associated this distribution with the number of arrivals in a given period of time, or
more generally, with the number of successes during a ﬁxed time period. We also showed that, with
a Poisson distribution, the time between arrivals is exponentially distributed. It is in this sense that
we consider the mimicry approach to generating random numbers according to a Poisson process.
We interpret N as the number of arrivals from a Poisson process in one time unit, and the times
between successive arrivals, which we denote A1, A2, . . . , are exponentially distributed. Therefore
N = k
if and only if
k

i=1
Ai ≤1 <
k+1

i=1
Ai.
In words, it is possible to have N = k in one time unit only if at least k arrivals occur at or before
the end of the time unit and the next arrival, the (k + 1)th, occurs after the end of the time unit, and
vice versa. The technique is therefore to generate exponentially distributed random numbers with
parameter α, add them until they exceed 1 and then back of by one arrival. This gives
k

i=1
−1
α ln(ui) ≤1 <
k+1

i=1
−1
α ln(ui).
Applying the summation law for logarithms and multiplying through by −α we obtain
ln
 k
i=1
ui

≥−α > ln
k+1

i=1
ui

,
i.e.,
k
i=1
ui ≥e−α >
k+1

i=1
ui.
Thus the procedure reduces to generating uniformly distributed random numbers in (0, 1) and
multiplying them together until the product is strictly less than e−α. The value of k is one less
than the number of products formed. Notice that when N = k, a total of k + 1 uniformly distributed
random numbers will be needed. Given that the mean of the Poisson distribution is α, it follows that,
on average α + 1 uniformly distributed random numbers will be needed to generate one Poisson
distributed random number. This can be quite expensive when α is large.

18.3 The Accept-Reject Method
657
Example 18.9 We return to the previous example one more time, this time using the Poisson
distribution with parameter α = 2.5. The sequence of uniformly distributed random numbers, is
.11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01
.06, .84, .94, .06, .60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96, .96, .09, .13, .46.
We ﬁrst compute e−2.5 = .0821. We now begin multiplying numbers in the sequence until the result
falls below .0821 and then subtract one from the number of terms that formed the product. We ﬁnd
that the following Poisson random numbers are generated:
k = 1, 2, 2, 4, 1, 2, 1, 1, 1, 3, 4, 4, 1.
Thus, for example u1 > .0821 but u1 × u2 < .0821; u3 × u4 > .0821 but u3 × u4 × u5 < .0821,
and so on.
A useful alternative to this approach when α is large is now presented. Random numbers having
the Poisson distribution can be found with only a single uniformly distributed random number
using the inverse transformation method, but it is ﬁrst necessary to generate and store the Poisson
probabilities
p0 = Prob{N = 0} = e−α,
p1 = Prob{N = 1} = αe−α,
p2 = Prob{N = 2} = 1/2α2e−α,
... .
The interval [0, 1) must now be partitioned into consecutive subintervals of length p0, p1, . . . . The
interval into which each uniformly distributed random number falls gives the value of the random
variable. Notice that the Poisson probabilities are easily computed by means of the recursion:
p0 = e−α,
pk = αpk−1
k
for k ≥1.
Another possibility when α is large is to use the normal approximation. For large α, (N −
α)/√α is approximately standard normal distributed. This means that procedures for generating
random numbers that are normally distributed can be used to produce random numbers that are
approximately Poisson distributed. If Z is a random number having standard normal distribution,
then
N = ⌈α + √αZ −.5⌉
has a Poisson distribution. The .5 in the formula is designed to force a “round-up” operation. The
generation of random numbers that are normally distributed is considered in some detail in a later
section.
18.3 The Accept-Reject Method
It is sometimes possible to determine the probability density function f (x) for some random
variable, but computation of the cumulative distribution function F(x) (or its inverse) is difﬁcult or
impossible. An example is the normal random variable. In such instances, the accept-reject method,
or more simply, the rejection method, can be rather effective. Let us assume that the density function
is deﬁned on a ﬁnite interval, or support, [a, b] and further assume that there exists a constant
c < ∞which bounds f (x) on this interval, i.e., f (x) ≤c for all a ≤x ≤b. Usually we choose
c = max f (x) in [a, b]. We now generate two uniformly distributed random numbers, u1 and u2 and

658
Nonuniformly Distributed “Random” Numbers
set x1 = a + (b −a)u1. The number x1 is taken as a random number that obeys the density function
f (x) whenever it satisﬁes the condition
cu2 ≤f (x1).
(18.3)
If it does not satisfy this condition, it is rejected. To put it another way, we generate a random point
(u1, u2), uniformly distributed over a unit square, and convert it to a point (a + (b −a)u1, cu2)
uniformly distributed over a box of height c and base (b −a). If this point falls below the density
curve, then x1 = a +(b−a)u1 is taken as a random number that satisﬁes the density, otherwise both
u1 and u2 are rejected. The rejection method is illustrated in Figure 18.5 in which x1 is accepted and
x3 is rejected.
x
x
y
c
a
b
Accept
Reject
u 4
3
2
u
1
c
c
x
Figure 18.5. Illustration of the accept-reject method.
Observe that, unlike the method of inversion, the accept-reject approach requires the evaluation
of the density function for different values of its argument, and this may be costly. The validity
of the approach may be established by considering conditional probabilities. Let X and Y be two
random variables, the ﬁrst uniformly distributed on [a, b] and the second uniformly distributed on
[0, c]. Then
Prob{x ≤X ≤x + dx | Y ≤fX(X)} = Prob{x ≤X ≤x + dx, Y ≤fX(x)}
Prob{Y ≤fX(X)}
=
 dx
b −a
  fX(x)
c
 
1
c(b −a)
−1
= fX(x)dx.
On the second line of this equation, the ﬁrst term in parentheses arises because X is uniformly
distributed on [a, b], the second in parentheses is because Y is uniformly distributed on [0, c] and
the third because both X and Y are independent and the random pair is uniformly distributed over a
rectangle of area c(b −a). Finally, we note that the accept-reject method may be applied when the
density function has nonzero values outside the range [a, b], but in this case, only an approximation
is obtained.
Example 18.10 Consider the density function
fX(x) =

−3x2 + 4x,
0 ≤x ≤1,
0
otherwise.
The maximum value of this density function in [0, 1] occurs at x = 2/3 with maximum value equal
to 4/3. This provides us with the value of c. The accept-reject method requires us to generate a
uniformly distributed number x1 in the range [a, b], here equal to [0, 1], and a second uniformly
distributed number u2 so that cu2 lies in the range [0, c], here equal to [0, 4/3]. The number x1 is
accepted if cu2 ≤f (x1) = −3x2
1 + 4x1. Suppose the sequence of uniformly distributed random

18.3 The Accept-Reject Method
659
numbers generated is
ui =.11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01.
Then the following values of xi, cui+1, f (xi) are obtained and the decision to accept or reject x1
as having probability density f (x) subsequently taken.
ui
ui+1
xi
cui+1
f (xi)
Acc/Rej
.11
.15
.11
0.20
0.40
Acc
.62
.94
.62
1.25
1.33
Acc
.11
.92
.11
1.23
0.40
Rej
.57
.02
.57
0.03
1.31
Acc
.99
.70
.99
0.93
1.02
Acc
.35
.67
.35
0.89
1.03
Acc
.35
.36
.35
0.48
1.03
Acc
.09
.51
.09
0.68
0.34
Rej
.21
.12
.21
0.16
0.71
Acc
.41
.01
.41
0.01
1.14
Acc
The sequence of random numbers satisfying this density function is
.11, .62, .57, .99, .35, .35, .21, .41.
As well as requiring the evaluation of the function f (x), which may be expensive, the accept-
reject method requires a minimum two uniformly distributed random numbers per accepted random
number with density f (x). Each time a rejection occurs, two uniformly distributed random numbers
have been generated but no random number satisfying the required density has been obtained. It
follows then that the rejection method works best when the area under the density curve f (x)
occupies most of the area in the rectangle of length b −a and height c since in this case, an accept
becomes much more common than a reject. In light of this, it is possible to replace the rectangle
with a different object, but one which still bounds f (x) on [a, b]. Let g(x) be a probability density
function on [a, b] such that
f (x)
g(x) ≤c for all a ≤x ≤b
for some c ≥1. The constant c is usually found by ﬁnding the maximum value of f (x)/g(x) for
x ∈[a, b] and the function cg(x) is said to majorize f (x). The following algorithm allows us to
generate random numbers that satisfy the density function f (x):
• Generate a random number x1 having density function g(x).
• Generate a uniformly distributed random number u2 on (0, 1).
• If cu2 ≤f (x1)/g(x1), accept x1; otherwise reject.
This algorithm is most efﬁcient when the number of rejections is low and the probability of success
is high. Observe that success occurs when the test u2 ≤f (x1)/cg(x1) is satisﬁed: hence we accept
x1 with probability f (x1)/cg(x1). Each test has, independently, a probability of acceptance equal
to 1/c and so the number of tests (or iterations) for acceptance is geometrically distributed with
parameter 1/c and mean value c. Thus c must be greater than 1 and the closer it is to 1, the fewer
the number of rejections.
When we bound f (x) by the uniform density function in [a, b], namely, g(x) = 1/(b −a), then
the value of c is taken as the maximum value of (b −a) f (x) in the interval [a, b]. In this case, the

660
Nonuniformly Distributed “Random” Numbers
test condition is
cu2 = u2 (b −a) max
a≤x≤b f (x) ≤f (x1)/g(x1) = (b −a) f (x1)
or
u2 max
a≤x≤b f (x) ≤f (x1),
which returns us to Equation (18.3).
The advantage of choosing a majorizing function other than a bounding constant, is that we
expect it to more closely bound f (x) with a resulting reduction in the number of rejections.
However, it has the additional cost of requiring the generation of a random number that satisﬁes
the density function g(x). While this is easy when g(x) is uniform on [a, b], it is more costly
with other density functions. It is therefore evident that g(x) should be chosen keeping in
mind that random numbers having this distribution will need to be generated. Distributions for
which the inverse transformation approach leads to simple formulae are frequently used. These
include wedge-shaped density functions, the triangle density function, and the exponential density
function.
18.3.1 The Lognormal Distribution
If Y is a normally distributed random variable, then the random variable, X = eY is said to have
a lognormal distribution. Lognormal random variables are frequently used to model service times.
The probability density function of a lognormal random variable is given by
fX(x) =
1
σ x
√
2π
exp

−(ln(x) −μ)2
2σ 2

,
x > 0,
and is zero elsewhere. It should be noted that the preferred approach for generating random numbers
that have a lognormal distribution, is to generate a normally distributed random number (Y) and
to apply the deﬁning transformation X = eY. We include the lognormal distribution here just to
illustrate the application of the accept-reject method.
Example 18.11 We shall take μ = 0 and σ = 1 as parameters for the lognormal density and we
shall use the exponential function exp(−0.8x), x ≥0, as the bounding function. These are shown
in Figure 18.6. Suppose our interest lies only in the lognormal density constrained to the interval
[0, 6]. Since the area under the lognormal density between 0 and 6 is given by
1
2 + 1
2erf
ln(6)
√
2

= 0.9634,
we set f (x) = fLN/0.9634 so that f (x) is a true density function on the interval [0, 6]. We must also
be sure that our bounding function is a true density function on [0, 6]. The area under exp(−0.8x)
between 0 and 6 is equal to 1.2397 and so we choose the bounding density function to be
g(x) = e−0.8x
1.2397
for
0 ≤x ≤6,
and zero otherwise. Now that we have both f (x) and g(x), we need to ﬁnd c, the maximum value
of f (x)/g(x) on [0, 6]. This maximum occurs at x = 6 and is equal to 2.0875. Thus we shall set
c = 2.1.

18.3 The Accept-Reject Method
661
0
1
2
3
4
5
6
7
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
Lognormal function
Exponential bounding function
Figure 18.6. The lognormal density and its exponential bounding function.
With a sequence of uniformly distributed random numbers u1, u2, . . . already available, the
steps required to generate random numbers with this lognormal distribution are
1. Use ui to generate xi having density g(x) = exp(0.8x)/1.2397 on [0, 6]. This is accomplished
by using the inverse transformation for exponential densities, and gives
xi = −ln(1 −.9918ui)
0.8
.
The reader may wish to verify that a density function of the form exp(−λx)/γ , λ > 0, gives
rise to the inversion formula x = −ln(1 −λγ u)/λ.
2. Accept xi if cui+1 ≤f (xi)/g(xi).
Given the following sequence of uniformly distributed random numbers:
ui = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01,
we obtain the following results:
ui
ui+1
xi
cui+1
f (xi)/g(xi)
Acc/Rej
.11
.15
0.1444
0.315
0.6135
Acc
.62
.94
1.1929
1.974
1.1003
Rej
.11
.92
0.1444
1.932
0.6135
Rej
.57
.02
1.0414
0.042
1.1331
Acc
.99
.70
5.0136
1.470
1.5410
Acc
.35
.67
0.5330
1.407
1.2103
Rej
.35
.36
0.5330
0.756
1.2103
Acc
.09
.51
0.1169
1.071
0.4817
Rej
.21
.12
0.2919
0.252
1.0408
Acc
.41
.01
0.6524
0.021
1.2105
Acc

662
Nonuniformly Distributed “Random” Numbers
The sequence of random numbers satisfying this density function is
0.1444, 1.0414, 5.0136, 0.5330, 0.2919, 0.6524.
18.4 The Composition Method
In some instances, random variables can be written in terms of other, simpler, random variables and
this provides a mechanism to generate random numbers having complex distributions using their
more convenient underlying distributions. This is the case of the Erlang-r and hyperexponential
distributions which consist of combinations of exponential distributions and which are considered
below. An identical approach can be used to generate random numbers that satisfy any type of phase-
type distribution. Furthermore, a more generic version of the composition method can be developed
and applied to many random variables whose density functions do not decompose naturally into
any particular structure. We refer to this extension as a density partitioning approach. It includes the
rectangle-wedge-tail method and the ziggurat method.
18.4.1 The Erlang-r Distribution
The probability density and cumulative distribution functions for a random variable that is
distributed according to an Erlang-r law with mean r/ξ and variance r/ξ 2 are as follows:
b(x) = ξrxr−1e−ξx
(r −1)!
,
x ≥0,
B(x) = 1 −
r−1

k=0
(ξx)k
k! e−ξx,
x ≥0.
Since this distribution function is not readily inverted, it is more convenient to consider the Erlang-
r distribution as a sequence of exponential phases in tandem as represented in Figure 18.7 and
discussed in previous chapters.
r
1 
2 
ξ
ξ
ξ
Figure 18.7. Erlang distribution: r exponential service phases in tandem.
To generate random numbers that satisfy an Erlang-r distribution, the straightforward approach
would then be to generate r random numbers according to an exponential distribution and to add
these together. However, this would work out to be too expensive, requiring the computation of r
natural logarithms. Instead we generate r uniformly distributed random numbers, ui, i = 1, . . . ,r,
set xi =−ln(ui)/ξ and, using the summation rule for logarithms, obtain a single Erlang-r distributed
random number as
r

i=1
xi = −1
ξ
r

i=1
ln(ui) = −1
ξ ln
 r
i=1
ui

,
and a sequence zk of such numbers as
zk =
rk

i=r(k−1)+1
xi = −1
ξ ln
⎛
⎝
rk

i=r(k−1)+1
ui
⎞
⎠,
k = 1, 2, . . . .

18.4 The Composition Method
663
Given the already large computation costs usually associated with simulation, computational saving
devices such as this use of the summation rule for logarithms are extremely important.
Example 18.12 We wish to use the following sequence of 40 uniformly distributed random
numbers in the unit interval to generate a sequence of random numbers that have an Erlang-4
distribution with mean 2.0.
ui = [.11, .15, .62, .94], [.11, .92, .57, .02], [.99, .70, .35, .67], [.35, .36, .09, .51],
[.21, .12, .41, .01], [.06, .84, .94, .06], [.60, .34, .51, .16], [.81, .43, .59, .51],
[.15, .70, .77, .96], [.96, .09, .13, .36].
Given that r = 4 and r/ξ = 2, this implies that ξ = 2 and successive random numbers from
this Erlang-4 distribution are formed by multiplying four consecutive ui values together, taking the
natural logarithm of the result and then dividing by −2. This gives
2.3222, 3.3824, .9085, 2.5764, 4.5888, 2.9315, 2.0478, 1.1278, 1.2780, 2.7553, . . . ,
where
−ln(.11 × .15 × .62 × .94)
2
= 2.3222,
−ln(.11 × .92 × .57 × .02)
2
= 3.3824,
. . . .
18.4.2 The Hyperexponential Distribution
The probability density and cumulative distribution functions for the hyperexponential distribution
are given by
b(x) =
r

i=1
αiμie−μi x,
x ≥0;
B(x) =
r

i=1
αi(1 −e−μi x),
x ≥0.
The ﬁrst two moments are given, respectively, by
E[X] =
r

i=1
αi
μi
and
E[X2] = 2
r

i=1
αi
μ2
i
.
As is the case for the Erlang distribution function, a hyperexponential distribution function is not
easy to invert and we resort to our earlier construction of the hyperexponential distribution as a
number of exponential phases in parallel as illustrated in Figure 18.8. Exponential phase i, with rate
μi, is chosen with probability αi. The branching probabilities are deﬁned so that r
i=1 αr = 1.
μ
μ
2
1
μ
Server
α
α
2
r
r
α1
Figure 18.8. Hyperexponential distribution: r exponential phases in parallel.

664
Nonuniformly Distributed “Random” Numbers
A convenient way to avoid having to invert the distribution function is to ﬁrst generate a
uniformly distributed random number to select a particular exponential phase i, followed by the
generation of a random number that satisﬁes an exponential distribution with rate μi. This allows
us to generate hyperexponentially distributed random numbers rather easily.
Example 18.13 Suppose we wish to model a scenario in which 20% of customers at a service center
require 15 minutes of service while the other 80% need, on average only 10 minutes. This allows
us to set α1 = .2, α2 = .8, μ1 = 4, and μ2 = 6, where we specify mean service time (= 1/μ)
in units of hours. Let us use the following sequence of 20 uniformly distributed random numbers
in the unit interval to generate a sequence of random numbers which obey this hyperexponential
distribution:
ui =.11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01.
We take these random numbers in groups of two. If the ﬁrst number in the pair is strictly less than
0.2, then we will use the second to generate an exponentially distributed random number having
mean 1/4; if the ﬁrst number in the pair is greater than or equal to 0.2, then we shall use the
second to generate an exponentially distributed random number with mean 1/6. For example, the
ﬁrst number is .11, which means that we use the second, .15, to generate an exponentially distributed
random number with mean 1/4, (or rate λ = 4). This gives −ln(.15)/4 = .4743 hours, or 28.45
minutes. Continuing like this, we obtain the following sequence of hyperexponentially distributed
random numbers:
.4743, .0103, .0208, .6520, .0594, .0667, .1703, .1683, .3534, .7675.
18.4.3 Partitioning of the Density Function
We now turn to a more general version of the composition approach. Observe that if
f1(x), f2(x), . . . , fn(x) are n density functions and p1, p2, . . . , pn are n probabilities such that
n
i=1 pi = 1, then
f (x) = p1 f1(x) + p2 f2(x) + · · · + pn fn(x)
(18.4)
is also a probability density function. It may be thought of as composed of a linear combination of
the n densities fi(n); hence the characterization of the approach we now describe as a compositional
approach. Suppose that we require random numbers that are distributed according to some density
function f (x). Let the interval over which the density function is deﬁned be partitioned into n,
not necessarily equal, subintervals, [x0, x1], [x1, x2], . . . , [xn−1, xn]. The ith subinterval, [xi−1, xi],
i = 1, 2, . . . , n, gives rise to a panel whose base is [xi−1, xi] and whose ceiling is formed by the
density curve between f (xi−1) and f (xi). The area of each panel is necessarily positive and less
than 1 and so can be taken to be a probability. Let pi be the area of panel i. Furthermore, the sum
of these n areas must be equal to 1, since the entire area under the density curve is equal to 1.
Thus it follows that these pi, i = 1, 2, . . . , n, can be taken to constitute the n coefﬁcients of the
composition in Equation (18.4). The n density functions fi(x) are deﬁned by the original density
function f (x) restricted to the panel [xi−1, xi].
So far, so good, but as yet nothing has been gained. However, a method for generating random
numbers that approximately satisfy the distribution f (x) is to replace the ith panel with a rectangle
having the same base but with height given by hi = [ f (xi−1)+ f (xi)]/2, the midpoint of the density
function in this subinterval. This allows us to compute the area of panel i as ˆpi = (xi −xi−1)hi
and the density function fi(x) in this panel is replaced with a simpler uniform density function. Of
course, the sum n
i=1 ˆpi need no longer be equal to one, but an appropriate normalization can take
care of this. This approach is illustrated in Figure 18.9.

18.4 The Composition Method
665
10
x 
x 
x 
x 
x 
x 
x 
x 
x
x 
x 
0
1
2
3
4 
5 
6 
7 
8 
9
Figure 18.9. A density function f (x) approximated by ten uniform densities.
The procedure for producing random numbers that approximate the density f (x) is to ﬁrst choose
a particular panel according to its area. Speciﬁcally, choose panel k with probability ˆpk. Next
generate a number u that is uniformly distributed on (0, 1). Then xk−1 + u(xk −xk−1) satisﬁes,
approximately, the density f (x). What this means in terms of Equation (18.4) is that we ﬁrst choose
which density function fi(x) to use and then generate a random number which satisﬁes this density.
The accuracy of the approximation depends, of course, on the particular density function and on the
number of panels chosen, the greater the number of panels, the better the accuracy. However, with
an excessive number of panels, time is wasted in deciding which one to choose (the selection based
on ˆpk), particularly, if the selection is done in a linear fashion instead of a binary search. This has
led to the development of better approximations, which we now consider.
Let us once again consider the area under the density curve and this time divide it into rectangles
that are contained completely within the area and wedgelike pieces that sit on top of the rectangles
so that the entire area is covered by these n rectangles (areas 1 through n) and n wedgelike pieces
(areas n + 1 through 2n), as illustrated in Figure 18.10. We call the pieces that sit on top of the
rectangles “wedgelike” rather than “wedges,” reserving this latter for bounding functions in the
accept-reject method.
2
10
8
7
 6
5
2
1
11
12
13
14
15
16
17
18
19
20
3
4
9
x
x
x
x 
x 
x 
x 
x 
x 
x
x
1 
3 
4
5
6
7 
8 
9
10
0
Figure 18.10. Area under f (x) partitioned into ten rectangles and ten wedgelike pieces.
When the distribution has a tail, then this tail will constitute yet another wedgelike piece. It is
expected that most of the area will be covered by the rectangles and only a small portion covered by
the wedgelike pieces. This is important because generating random numbers that obey a rectangular

666
Nonuniformly Distributed “Random” Numbers
density is very inexpensive when compared to the effort needed to generate numbers that obey
the wedgelike densities. Each rectangle and wedgelike piece is assigned a probability equal to its
area. In an ideal situation the sum of the rectangular areas should be close to 1 and the sum of the
wedgelike areas close to zero. Thus the probability that when a component density function fi(x)
is chosen it is likely that it will be a rectangular density. In the few cases when the chosen density
is wedgelike, then a random number having this density must be generated using, for example, the
accept-reject method. The accept-reject method is to be recommended since most of the areas above
the rectangles are almost right-angle triangles themselves and can be bound by a close-matching
wedge which makes the probability of a reject very small. The algorithm is very similar to the
previous. Let the area under the density curve be partitioned into n rectangles and m wedge-like
pieces and let pi, i = 1, 2, . . . , n + m, be the area of the ith partition.
• Choose partition i, 1 ≤i ≤n + m, with probability pi.
• If i ≤n, generate u, uniformly distributed on (0, 1), and take xi−1 + u(xi −xi−1) as the next
random number that obeys f (x).
• If i > n, use the accept-reject method to generate y which obeys fi(x) on [xi−1−n, xi−n].
When a wedgelike partition is selected, care must be taken to move this area down on to the x-axis
and scale it so that it is a true density function, since, as we have seen, the two functions f (x) and
g(x) used in the accept-reject method are density functions. Of course, all of this is taken care of
before the generation of the random numbers begins. The relevant information is computed once at
initiation, stored in tables and accessed as needed during the running of the generation algorithm.
Example 18.14 Let us generate random numbers having a Cauchy distribution using this approach.
The probability density function of a random variable having this distribution is
fX(x) =
1
π(1 + x2),
−∞< x < ∞.
Since this is obviously symmetric, we shall generate random variates in the upper right quadrant
and randomly (with probability .5) assign a −or a +. The density function of |X|, shown in
Figure 18.11, is
f (x) ≡f|X|(x) =
2
π(1 + x2),
0 ≤x < ∞.
We shall partition the area under this density curve into seven pieces, three rectangles with
bases [0, 1], [1, 2] and [2, 3], three wedges that sit on top of these rectangles and a tail section,
from coordinate x = 3 onward. A more rigorous approach might involve many more than seven
partitions—here our only purpose is to present an example. The heights of the three rectangles are
2/[π(1 + k2)] for k = 1, 2, 3, the point at which their right sides intersect with the density curve.
Hence the areas (and the associated probabilities) of these three rectangles are
p1 =
2
π(1 + 1) = 1
π = .318310,
p2 =
2
π(1 + 4) = 2
5π = .127324,
and
p3 =
2
π(1 + 9) =
2
10π = .063662.
We now calculate the area of the three wedges. This is made easy since it is known that

dx
1 + x2 = arctan(x).

18.4 The Composition Method
667
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
x
f(x)
Cauchy density function
Figure 18.11. The density function of |X| where X is a Cauchy random variable.
The area under the density curve between 0 and any point x is 2 arctan(x)/π. It follows that the
areas of the three wedges are
p4 = 2
π arctan(1) −p1 = 0.5 −1
π = .181690,
p5 = 2
π [arctan(2) −arctan(1)] −p2 = 0.204828 −2
5π = .077509,
p6 = 2
π [arctan(3) −arctan(2)] −p3 = 0.090334 −1
5π = .026672.
The area (and probability) of the tail is
p7 = 1 −2
π arctan(3) = 0.204833.
The reader may wish to verify that the sum of these areas is indeed equal to 1. The cumulative
probabilities obtained from the pi, i = 1, 2, . . . , 7, are
0.318310,
0.445634,
0.509296,
0.690986,
0.768495,
0.795167,
1.0.
A uniformly distributed random number is used to select one of the seven partitions according to
the particular interval into which it falls. If the random number ui falls into partition k, k = 1, 2, 3,
we take ui + (k −1) as a Cauchy random number. If ui falls into partition 4, we could use the
accept-reject approach with a straight line through 2/π as the boundary. If it falls into partition 5
or 6, we use a wedge as the boundary. Finally, if it falls into the last partition, we use accept-reject
taking 1/x2 as the bounding density function. This density function is the reciprocal of a random
variable that is uniformly distributed on (0, 1), which simpliﬁes the process. In fact this could also
be used as the bounding function for wedgelike sections 5 and 6.
Suppose we wish to generate Cauchy random numbers from the following uniformly distributed
random numbers:
ui = .4627, .8926, .0145, .1352, .7246, .4502, .8819, .5654.

668
Nonuniformly Distributed “Random” Numbers
Since u1 ∈[.445634, .509296), the third interval, we take k = 3 and compute 2 + .4627(3 −2) =
2.4627. To this we must add a plus sign (since u2 = .8926 > .5) and take y1 = +2.4627 as the ﬁrst
Cauchy random number.
Next we have u3 = .0145 which places us in the ﬁrst rectangle, k = 1 and we compute
0 + .0145(1 −0) to which we append a minus sign (since u4 = .1352 < .5). This gives our
second Cauchy random number, y2 = −.0145.
Since u5 = .7246 falls in the interval [.690986, .768495), which gives k = 5, we now have the
opportunity to see how a wedgelike area is handled and the accept reject method used to generate
the next Cauchy random number. This wedgelike area is situated on top of the rectangle based on the
interval [1, 2]. It must be moved down to the x-axis and then scaled to give a true density function.
i.e., we need to derive the density function f5(x) from the density function f (x). Since it must be a
true density, the area under f5(x) in the interval [1, 2] must be equal to 1. Let α be the area under
f (x) in the interval [1, 2]. Then
α = 2
π (arctan(2) −arctan(1)) = .204833.
Hence the density function f5(x) is
f5(x) =
2
απ(1 + x2),
1 ≤x ≤2,
and is zero otherwise. This function is concave on [1, 2] and is conveniently bound by the straight
line that passes through the endpoints f5(1) = 1/(απ) and f5(2) = 2/(5απ). The equation of this
straight line is
y = (8 −3x)
5απ
.
Integrating this straight line between x = 1 and 2 gives
1
5απ
 2
1
(8 −3x)dx =
7
10απ ,
and hence the density function used in the acceptance test, g(x), is
g(x) = (8 −3x)
5απ
× 10απ
7
= 2
7(8 −3x),
1 ≤x ≤2,
and is zero otherwise. These functions are plotted in Figure 18.12.
Since we want c to satisfy
cg(x) = (8 −3x)
5απ
,
this means that
c2(8 −3x)
7
= (8 −3x)
5απ
=⇒c =
7
10απ = 1.0878.
Observe that c is very close to 1, which means that we should expect the number of rejections to be
relatively small. Also notice that
f5(x)
cg(x) =
10
(1 + x2)(8 −3x)
and hence
f5(1)
cg(1) = f5(2)
cg(2) = 1,

18.4 The Composition Method
669
1
1.2
1.4
1.6
1.8
2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
1
1.2
1.4
1.6
1.8
2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Density curve:  f5(x)
Density function: g(x)
Density curve: f5(x).
Bounding function: cg(x)
Figure 18.12. Left: f5(x) and straight line y = (8 −3x)/(5απ). Right: f5(x) and the wedge density
function g(x).
which should be expected since f5(x) and cg(x) are equal at the end points. Also, the maximum
difference between f5(x) and cg(x) occurs at x = 1.453617 and
f5(1.453617)
cg(1.453617) = .882715.
At this point we have gathered together all the information we need to implement the accept-reject
method, namely, the density function for the random variates f (x), the bounding function g(x), and
the constant c. Recall the three steps needed to implement the accept-reject method.
1. Use u1, uniformly distributed on (0, 1), to generate x1 having density function g(x).
2. Generate a uniformly distributed random number u2 on (0, 1).
3. If cu2 ≤f (x1)/g(x1), accept x1; otherwise reject.
1. We shall use the method of inversion to generate a random deviate having density function
g(x) = 2(8 −3x)/7. To do so, we ﬁrst form the cumulative distribution function
G(x) =
 x
1
g(t)dt = 1
7
 x
1
2(8 −3t)dt = 1
7(−3x2 + 16x −13).
Setting u = (−3x2 + 16x −13)/7 and inverting, we obtain
x = 16 ±
.
162 −12(13 + 7u)
6
= 16 ± √100 −84u
6
.
(18.5)
With our next uniformly distributed random number u6 = .4502, we ﬁnd x1 = 2.6667 ± 1.3143.
We choose the root that lies in [1, 2], namely, x1 = 2.6667−1.3143 = 1.3524. This then completes
step 1.
2. We use the next number in our sequence, u7 = .8819.
3. Finally, we conduct the test. Given that
f5(1.3524)
cg(1.3524) = .8965
and since u7 = .8819 < .8965, we accept y3 = 1.3524 as a Cauchy random number. We choose a
positive result since u8 = .5654 > .5.

670
Nonuniformly Distributed “Random” Numbers
While it might appear that the generation of a random number corresponding to a wedge-shaped
partition is rather complex, remember that most of the work will have been done well before the ﬁrst
random number is generated. For each wedge-shaped partition, the function g(x) and its inversion,
as well as the constant c, are computed in advance.
18.5 Normally Distributed Random Numbers
The density function of a normally distributed random variable with mean value μ and variance
σ 2 is
f (x) =
1
σ
√
2π
e−(x−μ)2/2σ 2
for −∞< x < ∞.
(18.6)
This normal density function has the familiar bell-shaped curve. Because of its widespread
applicability, one should not be surprised to learn that procedures for the generation of random
numbers that obey this distribution have received much study and that there exist many different
approaches that may be used. We examine four approaches in this section. However, perhaps the
best method for generating standard normal variates is the ziggurat method which is described in
Section 18.6. As a general rule, the different methods generate random numbers that are standard
normal, N(0, 1), i.e., with μ = 0 and σ = 1, and then convert these to random numbers that satisfy
N(μ, σ 2). This is easily accomplished, since if Z has a standard normal distribution N(0, 1), the
random variable μ + σ Z has the distribution N(μ, σ 2).
18.5.1 Normal Variates Via the Central Limit Theorem
The central limit theorem states that a sum of n independent and identically distributed random
variables, X1, X2, . . . , Xn with mean value μ and variance σ 2 is approximately normally distributed
with mean nμ and variance nσ 2, and the greater the value of n, the better the approximation. Thus
the sum of n uniformly distributed random numbers in (0, 1) approximates a normal distribution
with mean n/2 and variance n/12, since a uniformly distributed random number in (0, 1) has mean
value 1/2 and variance 1/12. In particular, when n = 12, the random variable X = 12
i=1 Xi −6 is
N(0, 1). The algorithm therefore is to generate sequences of tewlve uniformly distributed random
numbers in (0, 1), add them together and subtract the integer 6, to produce a single random variable
that is standard normal. This is perhaps the simplest of all methods for generating normally
distributed random numbers, but is also one of the slowest requiring the generation of twelve
uniformly distributed random numbers for each normally distributed random number.
Example 18.15 The following sequence of 36 uniformly distributed random numbers produces
three standard normal random variates, obtained by adding them in groups of twelve and
subtracting 6:
.11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67,
.35, .36, .09, .51, .21, .12, .41, .01, .06, .84, .94, .06,
.60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96.
The standard normal variates are 0.15, −2.04, and 0.53.
18.5.2 Normal Variates via Accept-Reject and Exponential Bounding Function
The accept-reject method works best when the bounding function is close to the density function
for then the probability of a reject is low. Since at least part of the standard normal density

18.5 Normally Distributed Random Numbers
671
function looks similar to an exponential curve, we shall use a (negative) exponential function as
the majorizing function. Also, since the standard normal distribution is symmetric about the origin,
we shall produce normally distributed random values in the right plane only and then randomly
(with probability 1/2) assign a + or −to this value. This means that we must be concerned with the
absolute value of Z for which the probability density function is
f (x) ≡f|Z|(x) = 2 ×
1
√
2π
e−x2/2 =
.
2/π e−x2/2,
0 < x < ∞.
The exponential density function we use is given by
g(x) = e−x,
0 < x < ∞,
and we compute c as the maximum value of f (x)/g(x) for 0 < x < ∞, i.e.,
c = max
0<x<∞f (x)/g(x) = max
0<x<∞
√2/π e−x2/2
e−x
= max
0<x<∞
.
2/π ex−x2/2.
This maximum must occur when x −x2/2 is maximized, which happens when x = 1. Thus
c =
.
2/π e1/2 =
.
2e/π = 1.3155.
This then allows us to establish the acceptance test, cu2 ≤f (x1)/g(x1), as
.
2e/π u2 ≤
.
2/π ex1−x2
1/2
or
u2 ≤ex1−x2
1/2−1/2 = e−(x1−1)2/2,
where x1 is exponentially distributed with mean value equal to 1. The complete algorithm is given
as follows:
• Generate a uniformly distributed random number, u1, in (0, 1) and set x1 = −ln u1.
• Generate a uniformly distributed number u2 on (0, 1).
• If u2 ≤e−(x1−1)2/2 then
— generate a random sign, attach it to x1 and accept x1,
— otherwise reject.
Example 18.16 Let us generate some standard normal random numbers from the following
sequence of uniformly distributed random numbers. For the sign, we choose −when 0 ≤u < .5
and + otherwise.
ui = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21,
.12, .41, .01, .55.
ui
xi = −ln ui
ui+1
exp(−(xi −1)2/2
Acc/Rej
ui+2
±
.11
2.2073
.15
.4825
Acc
.62
+
.94
0.0619
.11
.6440
Acc
.92
+
.57
0.5621
.02
.9086
Acc
.99
+
.70
0.3567
.35
.8131
Acc
.67
+
.35
1.0498
.36
.9988
Acc
.09
−
.51
0.6733
.21
.9480
Acc
.12
−
.41
0.8916
.01
.9941
Acc
.55
+
The sequence of random numbers satisfying this density function is
+2.2073, +0.0619, +0.5621, +0.3567, −1.0498, −0.6733, +0.8916.

672
Nonuniformly Distributed “Random” Numbers
18.5.3 Normal Variates via Polar Coordinates
As the previous example shows, each standard normal random number produced by the accept-
reject method requires the generation of three uniformly distributed random numbers, the generation
of an exponentially distributed random number, two function evaluations ( f (x) and g(x)) and a
test. Sometimes, no standard normal random variate is even obtained. The polar method, which we
now present, tries to avoid this cost. This method is based on the concept that a pair of uniformly
distributed random numbers can represent a point in the plane, a point that can be transformed into
polar coordinates. Box and Muller [5] show that if u1 and u2 are two uniformly distributed random
numbers, then
x1 = cos(2πu1)
.
−2 ln u2
and
x2 = sin(2πu1)
.
−2 ln u2
are standard normal, N(0, 1), random numbers. These may be transferred to the distribution
N(μ, σ 2) by setting
x1 = σ x1 + μ
and
x2 = σ x2 + μ.
Notice that this method takes two uniformly distributed random numbers and produces two
normally distributed random numbers. Unfortunately, however, it has the disadvantage of involving
square roots, natural logarithms, and sine and cosine functions, all of which are, relatively speaking,
computationally expensive to compute. This situation can be much improved by moving from these
rectangular coordinates to polar coordinates. In this case, two numbers (2u1 −1) and (2u2 −1) that
are uniformly distributed on the interval (−1, +1) are generated. If both these numbers lie within
the unit circle, i.e., if r = (2u1 −1)2 + (2u2 −1)2 < 1, then
y1 = (2u1 −1)ρ
and
y2 = (2u2 −1)ρ
both obey a standard normal distribution, where ρ = √−2 lnr/r. Since the area of the square is 4
and that of the circle is π, it should be expected that a reject will occur approximately 21% of the
time.
Example 18.17 Let us use the polar method to generate a sequence of normally distributed random
numbers from the following sequence of uniformly distributed random numbers in (−1, 1):
2ui −1 = .0597, .9119, −.7527, −.3070, .9723, .3205, −.7669, .2644, .8208,
.9690, −.1384, −.2326.
For each pair ui and ui+1, we form r = (2ui −1)2 + (2ui+1 −1)2, and if this is less than 1, we take
yi = (2ui −1)ρ and yi+1 = (2ui+1 −1)ρ to be standard normal variates, where ρ = √−2 lnr/r.
This gives the following table of results:
2ui −1
2ui+1 −1
r
ρ
yi
yi+1
.0597
.9119
0.8351
0.6569
0.0392
0.5990
−.7527
−.3070
0.6608
1.1198
−0.8429
−0.3438
.9723
.3205
1.0481
−.7669
.2644
0.6580
1.1278
−0.8649
0.2982
.8208
.9690
1.6127
−.1384
−.2326
0.0733
8.4474
−1.1691
−1.9649
The sequence of normally distributed random numbers is
.0392, .5990, −.8429, −.3438, −.8649, .2982, −1.1691, −1.9649.

18.6 The Ziggurat Method
673
18.5.4 Normal Variates via Partitioning of the Density Function
We now turn to the application of the partitioning approach to the generation of normally distributed
random numbers. The method is due to Marsaglia [34] and we follow the approach presented by
Knuth [25]. We choose n = 15 intervals of length .2 which gives rise to 15 rectangles, 15 wedgelike
pieces, and a tail as shown in Figure 18.13. Thus the name, the rectangle-wedge-tail method.
15 Rectangles
x
f(x)
0
1 
2 
3
Tail
15 Wedge−like pieces
Figure 18.13. Area under the normal curve partitioned into rectangles, wedgelike pieces, and a tail.
The base of the kth rectangle, k = 1, 2, . . . , 15, is [.2(k −1), .2k]; its height is f (.2k). Given
that f (x) = √2/πe−x2/2, this height is equal to √2/πe−k2/50. It follows that the area of the
kth rectangle is
pk = .2
.
2/πe−k2/50 =
0
2
25π e−k2/50,
k = 1, 2, . . . , 15.
Since 15
k=1 pk = .9183, the total remaining area, the sum of the areas of the remaining 16
pieces, is equal to .0817. The area of each can be approximated by replacing each with a wedge,
followed by a normalization to insure that the sum of all 16 is equal to .0817. In this way, all 31
probabilities pk can be obtained.
Furthermore, since 15
k=1 pk = .9183, random numbers that satisfy f (x) are generated from a
uniform distribution almost 92% of the time. They are obtained from the simple formula
xi = ui/5 + ξ,
where ξ is an offset which is equal to (k −1)/5 with probability pk. For the other 8% of the time
either one of the wedgelike areas or the tail area is chosen and the accept-reject approach must be
used to generate a random number having this density.
18.6 The Ziggurat Method
Marsaglia has improved upon this even further. His ziggurat method [35] results in a normally
distributed random number being generated from a uniform distribution 99% of the time: a reject
is expected only once in every 100 tries. Unlike the basic approach described above, the rectangles
in the ziggurat method are arranged horizontally rather than vertically. The lowest section consists

674
Nonuniformly Distributed “Random” Numbers
of a rectangle plus the tail of the distribution. The rectangular portion of each section that lies
within the density function has been called the core. When the number of segments is large, it is to
be expected that the core occupies almost all the rectangle which means that normally distributed
random numbers will be produced with almost every try. The deﬁning characteristic of the ziggurat
method is that all the rectangles, (and the lowest section of rectangle plus tail) all have exactly the
same area. This means that during the running of the generation procedure, the choice of a particular
rectangle is immediate. This is illustrate graphically in Figure 18.14, which is not drawn to scale.
The 99% acceptance rate is obtained with 255 rectangles plus the lowest section of rectangle and
tail.
Rectangles
x
f(x)
0 x 1
Core
Tail
x
x2
8
Figure 18.14. Ziggurat (n = 8): seven rectangles plus base section of rectangle and tail.
Marsaglia’s ziggurat method applies to density functions other than standard normal and in
particular appears to be a very effective method for generating exponentially distributed random
numbers. However, here our concern is with the generation of standard normal variates where the
density function is
f (x) =
0
1
2π e−x2/2,
−∞< x < ∞.
The term √1/2π is usually ignored since it is just a normalizing constant—the ziggurat method
works with f (x) = exp(−x2/2). Let xk, k = 2, 3, . . . , n be the coordinate of the right side of
rectangle k −1, where rectangle 1 is the topmost rectangle; x1 = 0 is the left boundary of all
rectangles. Let us postpone momentarily how these xi can be found and assume that they are already
available. Let αk = xk/xk+1, k = 1, 2, . . . , n −1 and αn = xn f (xn)/V where V is the area of the
base section (rectangle plus tail). Thus α1 = 0 and αk, k = 2, 3, . . . , n is the proportion of rectangle
k occupied by its core. The generation procedure consists of the following steps.
• Generate a uniformly distributed random number u1 and select rectangle k = ⌈nu1⌉.
• Generate a second uniformly distributed random number u2:
— if u2 ≤αk, take (2u2 −1) xk as a standard normal random number,
— otherwise apply the accept-reject method to the wedge (or tail) in rectangle k.

18.6 The Ziggurat Method
675
We now return to the technical problems of computing xi, i = 2, 3, . . . , n. If the last intercept
xn (the right coordinate of rectangle n −1 and the point at which rectangle and tail meet in the last
section) is known, then is possible to compute the common area as
V = xn f (xn) +
 ∞
xn
f (x) dx.
Furthermore, it now becomes possible to compute the right coordinate xk of each of the other
rectangles using the fact that all rectangles have exactly the same area. The area of rectangle k
(and the area of all rectangles and the area of the bottom section) is given by (base times height)
V = xk+1 ×

f (xk) −f (xk+1)
 
,
k = 1, 2, . . . , n −1,
so that we may compute xk as
xk = f −1

f (xk+1) +
V
xk+1

.
Thus once we know V and xn we can ﬁnd all of the points xi, i = n −1, . . . , 2. The problem, of
course, lies in the computation of V and xn. Consider the function
V (r) = r f (r) +
 ∞
r
f (x) dx.
This is the area of a rectangle of height f (r) and length r, plus the area of the tail of the distribution
from the point r. At the point r = xn, V (r) = V , the area we seek to determine. Let us now
introduce a second function,
z(r) = x2(r)[ f (0) −f (x2(r))] −V (r) = x2(r)[1 −f (x2(r))] −V (r),
(18.7)
using the fact that since f (x) = exp(−x2/2), f (x1) = f (0) = 1. In this equation xk(r) is the
coordinate of the right edge of rectangle k −1 when r is the point at which rectangle and tail meet
in the lowest segment. Observe that x2(r)[1 −f (x2(r))] is the area of the top rectangle and when r
is such that x2(r)[1 −f (x2(r))] = V (r) we have found the value of r we seek, the value of xn. The
problem then is to ﬁnd a root of Equation (18.7): for a given value of r, the function z(r) may be
computed as
Algorithm: Given n, f (x), and r, ﬁnd z(r):
xn = r;
V = r f (r) +
 ∞
r
f (x) dx
for k = n −1, −1, 2
xk = f −1 ( f (xk+1) + V/xk+1)
z = V −x2 (1 −f (x2))
The procedure then is to use a standard root ﬁnding procedure to solve z(r) = 0 using the
algorithm just given to evaluate z(r) as and when needed. The right coordinate of rectangle n −1,
namely, xn, is given as the solution of z(r) = 0. The area of each rectangle can now be found from
V = xn f (xn) +
 ∞
xn
f (x) dx,
and all other values of xk found from
xk = f −1

f (xk+1) +
V
xk+1

,
k = n −1, . . . , 2.

676
Nonuniformly Distributed “Random” Numbers
Naturally, these values are generated only once and stored in tables. Finding the proper root of
Equation (18.7) can sometimes be quite challenging and for this reason Marsaglia provides the
following values of xn and V :
n
xn
V
128
3.442619855899
.0099125630353
256
3.6541528853610088
.0049286732339
Software implementations of this method incorporate these values directly into the code and
generate the tables from them. This is done once only, at initialization. Since the area of each
rectangle is approximately equal to .0049287 when n = 256, an accept will occur 99.33% of the
time. See Exercise 18.6.1.
To terminate this section, let us return once more to the exponential distribution. We saw
previously that random numbers having an exponential distribution can be found quite conveniently
using the natural logarithm function, the only drawback being the expense of computing a natural
logarithm. It turns out that the ziggurat method can be conveniently applied to the exponential
distribution and with 255 rectangles and a base section of rectangle and tail, it achieves an efﬁciency
of 98.9%. Thus, approximately 99 times out of 100, an exponentially distributed random number
can be obtained as u xk where k is the selected rectangle. When f (x) = e−x, Marsaglia provides the
following values for xn and V :
n
xn
V
256
7.69711747013104972
.0039496598225815571993
18.7 Exercises
Exercise 18.1.1 The cumulative distribution function of a random variable X is given by
FX(x) =
⎧
⎨
⎩
0,
x < 0,
x3/8,
0 ≤x ≤2,
1,
x > 2.
Use the inverse transformation method to obtain random variates with this distribution from the following
sequence of uniformly distributed random numbers
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01.
Exercise 18.1.2 Consider a right-angle triangle of height h and width w with right angle placed at a distance
a + w from the origin. Under certain constraints on h and w, the hypotenuse of this triangle can be taken as a
density function of a random variable X deﬁned on [a, a + w]. Show that this density function is given by
fX(x) =
h(x −a)/w,
a ≤x ≤a + w,
0
otherwise,
and specify the conditions that must be imposed on h and w. Find the corresponding cumulative distribution
function. The inverse transformation method is to be used to generate random numbers that obey the density
function fX(x). Find the inverse function that is to be used and apply it to the following sequence of uniformly
distributed random numbers in the speciﬁc case when a = 2 and h = .5:
yi = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01.

18.7 Exercises
677
Exercise 18.1.3 Let X be a random variable with mean value E[X] = 11/3 and whose density function is
triangular on the basis (a, b) = (2, 6). From the following sequence of uniformly distributed random numbers
generate a corresponding sequence of random numbers having this triangular density:
.06, .84, .94, .06, .60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96, .96, .09, .13, .36.
Exercise 18.1.4 The three-parameter Weibull density function is given as
fX(x) = β
η
 x −ξ
η
β−1
e−[(x−ξ)/η]β
for x ≥0,
and is equal to zero otherwise. Derive the inverse function needed to generate random numbers having this
distribution in the special case when the location parameter ξ is equal to zero.
Exercise 18.1.5 Prove that the inverse function used to generate sequences of geometrically distributed random
numbers is given by
k =
 ln(y)
α
!
+ 1.
Exercise 18.2.1 Use the following sequence of uniformly distributed random numbers to obtain random
numbers that obey the probability mass function
pX(k) =
⎧
⎨
⎩
8
k

.15k(.85)8−k,
0 ≤k ≤8,
0
otherwise.
.68, .12, .09, .05, .94, .13, .25, .60, .46, .03, .82, .40, .34, .98, .81, .78,
.70, .43, .87, .04, .69, .14, .62, .57, .64, .96, .98, .87, .06, .86, .02, .94,
.64, .56, .84, .08, .68, .24, .47, .80, .94, .58, .64, .53, .17, .61, .16, .68.
Exercise 18.2.2 Consider a machine that at the start of each day is either in working condition or has broken
down. If it is working, then during the day, it will with probability p = .05 break down. Let X be the random
variable that denotes the number of consecutive days that the machine is found to be in working condition. Use
the following sequence of uniformly distributed random numbers to generate a sequence of random numbers
that simulate X.
.64, .36, .98, .87, .06, .86, .02, .94, .70, .43, .87, .04, .69, .14, .62, .57,
.94, .58, .64, .53, .17, .61, .16, .68, .64, .56, .84, .08, .68, .24, .47, .80,
.46, .03, .82, .40, .34, .98, .81, .78, .68, .12, .09, .05, .94, .13, .25, .96.
Exercise 18.2.3 A bank opens its doors at 8:00 am each day and customers begin to arrive according to
a Poisson process with parameter λ = 15 customers per hour. Given the following sequence of uniformly
distributed random numbers
.73, .36, .45, .02, .94.
Simulate the time of arrival of the ﬁrst ﬁve customers.
Exercise 18.2.4 Returning to the bank of the previous exercise, and given the following sequence of uniformly
distributed random numbers:
.73, .36, .45, .02, .94, .98, .87, .06, .60, .14, .62, .57, .43, .87, .04, .69,
.15, .16, .68, .58, .64, .53, .17, .61, .54, .24, .47, .80, .56, .84, .08, .68,
.25, .34, .98, .81, .78, .03, .82, .40, .88, .12, .25, .96, .09, .05, .94, .13,

678
Nonuniformly Distributed “Random” Numbers
generate a sequence of Poisson distributed random numbers, with parameter λ = 15, by forming products of
these uniformly distributed random numbers.
Exercise 18.2.5 The same as the last exercise, except this time use the method of inversion and the following
sequence of uniformly distributed random numbers:
.87, .06, .72, .25, .96, .09, .56, .35.
Exercise 18.3.1 The density function of a β(i, j) random variable is
fX(x) =
β(i, j)xi−1(1 −x) j−1,
0 < x < 1,
0
otherwise,
where the β function β(i, j) is deﬁned as
β(i, j) =
(i + j −1)!
(i −1)!( j −1)!.
This exercise concerns the accept-reject method for generating random numbers having a β(2, 2) distribution.
Compute the maximum value of the density function in the interval (0, 1) and use a straight line through
this point as the bounding function. Use the following sequence of uniformly distributed random numbers to
generate random numbers with a β(2, 2) distribution:
ui = .11, .15, .62, .94, .11, .92, .57, .02, .99, .70, .35, .67, .35, .36, .09, .51, .21, .12, .41, .01.
Exercise 18.3.2 The same as Exercise 18.3.1 except this time use the β(2, 4) distribution instead of the β(2, 2)
distribution.
Exercise 18.4.1 Use the following sequence of uniformly distributed random numbers:
ui = .06, .84, .94, .06, .60, .34, .51, .16, .81, .43, .59, .51, .15, .70, .77, .96, .96, .09, .13
to generate random numbers that satisfy the Coxian distribution shown in Figure 18.15. The values of the
parameters of the exponential phases are μ1 = 1/4, μ2 = 2, and μ3 = 5.
3
0.25
0.25
0.75
0.75
μ 
μ 
μ 
1
2
Figure 18.15. Coxian-3 distribution.
Exercise 18.4.2 A function g(x) = mx + d is a straight line between x = a and x = b and is zero elsewhere.
Find the values of m and d in terms of a, b, g(a) and g(b) and derive a condition for this function to be a density
function. (Such density functions are frequently used as bounding functions in the accept-reject method.)
Assuming this condition is true and g(x) is indeed a density function, ﬁnd the corresponding cumulative
distribution function G(x), in terms of m, d and a and from it, the inverse relationship which converts a random
number u which is uniformly distributed on (0, 1), to a random variate that satisﬁes the density function g(x)
on (a, b).
Exercise 18.4.3 Generate at least one Cauchy random number, using the same partitioning as in Example
18.14, from the following uniformly distributed random numbers:
ui = .7824, .8723, .6473, .4375.

18.7 Exercises
679
Exercise 18.4.4 Like the previous exercise, this one also asks you to generate at least one Cauchy random
number, using the same partitioning as in Example 18.14, from the following uniformly distributed random
numbers:
ui = .5542, .8723, .6390, .2943, .9171, .0107.
This time u1 leads to the wedgelike section on top of the ﬁrst rectangle. This area is to be bounded by a
horizontal line.
Exercise 18.4.5 One ﬁnal exercise on the generation of Cauchy random variables. Generate at least one
Cauchy random number, using the same partitioning as in Example 18.14, from the following uniformly
distributed random numbers:
ui = .8572, .9825, .3846, .4924.
This time u1 leads to the ﬁnal tail section and you should use the function 1/x2 as the bounding function.
Exercise 18.5.1 Use the polar method to generate a sequence of normally distributed random numbers from
the following sequence of uniformly distributed random numbers:
ui = .0597, .9119, .7527, .3070, .9723, .3205, .7669, .2644, .8208, .9690, .1384, .2326.
Exercise 18.6.1
Show that the efﬁciency of the ziggurat method in generating standard normal random
numbers, using n = 256 rectangles/sections, is 99.33%. Show that its efﬁciency in generating exponentially
distributed random numbers, using the same number of rectangles/section, is 98.9%.
Exercise 18.6.2 This question concerns the application of a simple n = 4 ziggurat method to the generation
of random numbers that have the exponential distribution, f (x) = e−x. Given r = xn = 2.25991014168348:
(a) Find the area of each of the three rectangles and the rectangle/tail base section.
(b) Find the coordinates xi, i = 3, 2, 1.
(c) Compute the value of z(r).
(d) Compute the area of the topmost rectangle and verify that it is the same as in part (a).
(e) Given the following sequence of uniformly distributed random numbers, use the n = 4 ziggurat
method to generate a sequence of exponentially distributed random variables:
ui = .5081, .2716, .4338, .0325, .3419, .6440, .9621, .1204, .0653, .4907.

Chapter 19
Implementing Discrete-Event Simulations
19.1 The Structure of a Simulation Model
In this chapter, our concern is the computer implementation of discrete-event simulations. Although
the system being simulated exists in continuous time, changes in the simulation model take place
only upon the occurrence of well-deﬁned events which are discrete (hence the name, discrete-event
simulation) and which take place at discrete instants of time. This is not the only type of simulation
that is possible. The simulation of continuous processes, such as weather patterns, chemical
reactions, and the like, are frequently simulated and modeled as systems of partial differential
equations. Simulations that use sequences of random numbers and which are independent of time,
such as the use of random numbers to evaluate deﬁnite integrals, are called Monte Carlo simulations.
We begin by discussing in a general context, the key features involved in the development of a
discrete event simulation model and then illustrate these features by means of a number of examples.
The State Descriptor
One of the most important aspects of simulation is that of model construction. For large complex
systems, it is seldom possible to model the system in its entirety, but rather, critical parts of the
system are analyzed independently. In some cases this is all that is required, but in others it may be
necessary to incorporate critical subsystems into a hierarchical model. The ﬁrst task of the modeler
is to determine which questions the simulation study is supposed to answer. This will help in an
appropriate choice for representing the system. The representation of the system is called the state
descriptor vector or simply, the system state. It is a function of time. In very simple models, it
may be just a single integer. For example, if a system such as an auto-repair garage has a single
repair bay, all that may be needed to represent its state is the number of cars waiting or undergoing
repair. However, most commonly, the state descriptor vector consists of a number of items called
components and each component has its own speciﬁc set of attributes. In fact, each component can
be viewed as a discrete random variable which assumes the values of its attributes.
Events and Their Management
Once all the components of the system state and their different attributes have been deﬁned, the
next task of the modeler is to deﬁne all possible events and specify their effects on the system state.
Naturally, the speciﬁcation of the state descriptor and the deﬁnition of the events will likely occur
pretty much simultaneously in the mind of the modeler. We separate them here to provide an outline
of the steps needed to build a simulation model. The occurrence (or ﬁring) of an event can cause the
components of the state descriptor to change values. A particular event may alter the value of only
one component or of many components. The occurrence of an event may also leave the system state
unchanged. Furthermore, the ﬁring of an event can, in some simulation models, alter the scheduled
ﬁring times of other events.

19.1 The Structure of a Simulation Model
681
Example 19.1 In a simple M/M/1 queue, there are only two events:
• a new customer arrives at the queue;
• a customer completes service and departs.
A single integer which speciﬁes the number of customers present sufﬁces to represent the state of
the system. An arrival increments the system state by 1; a service completion decrements it by one.
In a single-server feedback queue, in which customers are returned to the queue with probability α,
a service completion will, with probability α, leave the system state unchanged.
Events succeed each other in time and it is important in the simulation to schedule these correctly.
Thus it is usual to keep an event list which holds the time for the next occurrence of each different
possible event. Both unordered and ordered list-type data structures can be used to hold these times.
• If an unordered list data structure is used, each event is assigned its own memory location
(either a location within an array or its own data record) which contains, at a minimum, the
time of the next occurrence of that event. With an unordered data structure, it is necessary to
search through the entire list to ﬁnd the event that is next to ﬁre. Once the event occurs, the
time until the next occurrence of this same event is generated and inserted into its assigned
position in the data structure.
• If an ordered list data structure is used, the event at the head of the list is the next to ﬁre. An
event which ﬁres must be removed from the head of the list. The time of the next occurrence of
this event is then generated and inserted into the list at the appropriate position—the position
that maintains the ordered property of the list.
List data structures may be array-based or linked. Both singly linked and doubly linked variants are
possible. Array-based lists are more memory efﬁcient since they do not require pointer variables. In
a linked list with n elements, insertion and deletion are O(1) while obtaining any element i (called
a lookup) is O(n), whereas in an array-based list, insertion and deletion are O(n) and lookup is
O(1). Searching for a key value is O(n) in both cases, but in an array based list, ordered according
to key value (here the times of the next occurrence of each event), a binary search algorithm can be
used which reduces the cost to log2(n). Indeed, when the key values are approximately uniformly
distributed, a dictionary search having complexity log2 log2 n can be used. A variant of binary
search applied to a linked list is called a skip list but requires additional link pointers.
There are other data structures and algorithms available for maintaining event lists. Stacks and
queues are restricted types of lists and can be either array based or linked. They do not have all
the functionality of a general list data structure, but they can be implemented more simply which
makes them more efﬁcient. A heap data structure is essentially an array-based, complete, binary
tree whose smallest element is at the root of the tree and hence imminently accessible. Insertion into
and deletion from a heap are log2(n). However, ﬁnding a speciﬁc key value is O(n), since a heap is
not unique. We shall not pursue the issue of appropriate data structures for implementing an event
list any further. Someone wishing to program their own simulation should be sufﬁciently adept at
handling the data structures typically found in an undergraduate computer science course as to be
capable of doing this correctly. Otherwise, someone using a simulation language will have built-in
data structures already available.
Simulation Time
The only remaining question is the duration T of the simulation experiment. This is not the time
required to execute the simulation program, (sometimes called the wall-clock time) but rather the
simulated time, the time over which the model is exercised (sometimes called the internal clock
time). It can be either a ﬁxed period of time or the time until a pre-speciﬁed number of one or more
events has occurred. A variable t is used to indicate the passage of time within the simulation. Each

682
Implementing Discrete-Event Simulations
execution of a simulation from the point at which it is initiated, t = 0, until it is halted at t = T , is
called a simulation run and deﬁnes a sample path of the simulation model. The evolution over time
t ∈[0, T ] of the state descriptor vector, which we described above as a vector of random variables,
is a sample path. A number of performance statistics must to be collected during a simulation run.
These might include the number of times that a particular event occurred, the average duration
between occurrences of the same event, or indeed any other statistic that is deemed necessary. In
order to provide for this, the simulation program must contain various variables, usually counters,
which are updated when events occur. The values of these variables at the end of a simulation run
are used to evaluate performance statistics.
The internal clock can be advanced in one of two ways: either at ﬁxed increments of time,
called synchronous simulation, or upon the ﬁring of events, called asynchronous simulation, since
the system state changes only at these ﬁring instants. The ﬁrst is generally easier to program but
important decisions need to be made concerning the length of the ﬁxed time increment. If this is
chosen too long, then multiple events may occur in each interval which complicates event handling;
if it is chosen too small, then there will be many intervals in which no events at all occur and
computer time is wasted checking this situation. The usual approach to advancing the clock in
discrete event simulation is the second, even though more complicated data structures are usually
required.
Program Structure
Once a state descriptor vector and the set of events has been deﬁned, there remains the problem
of incorporating them into a computer program. The most straightforward way, and perhaps
also the most efﬁcient way, to program a discrete-event simulation is to generate a module
(subroutine/method) for each different type of event, and a main routine whose responsibility is
the sequencing of event occurrences and calling the appropriate event at the appropriate time. The
event modules should include actions that
• update the internal clock;
• change the components of the state descriptor vector in accordance with the effect of the event
on the system;
• increment a counter of the number of times this event has occurred;
• generate the time of the next occurrence of this event;
• update other variables that might be required for the computation of performance measures.
The main routine is responsible for
• initializing all variables for a new simulation run;
• selecting the next event to occur;
• passing control to the event module and waiting until control is returned;
• determining when the current simulation run ﬁnishes;
• computing performance statistics for the simulation run.
19.2 Some Common Simulation Examples
19.2.1 Simulating the M/M/ 1 Queue and Some Extensions
We now consider a number of examples in more detail, beginning with the simple M/M/1 queue.
This queue was treated analytically in Part III of this text, and although it is rather straightforward
to simulate (and indeed to treat analytically), it is adequate for illustrating all the most important
aspects of designing and implementing a simulation. To recap, the M/M/1 queue is a single server
queue with ﬁrst-come ﬁrst-served scheduling of customers. The arrival process is Poisson with

19.2 Some Common Simulation Examples
683
parameter λ, i.e., interarrival times have an exponential distribution with mean 1/λ; the service time
is also exponentially distributed and the mean service time is 1/μ.
The different variables that we shall employ are now speciﬁed:
• A single integer n is sufﬁcient to completely characterize a state of the system and thus is
taken as the state descriptor vector.
• There are only two events: the arrival of a new customer to the system and the departure of a
customer who has just completed service.
• Integer variables na and nd count the number of arrivals and departures, respectively.
• We shall run the simulation until a total of N customers have been served and then compute
the mean interarrival time, the mean service time and the mean time spent waiting in the
queue.
• The variable t will be used to denote the internal clock time. The variables ta and td denote
the scheduled times of the next arrival and the next departure.
• We use tλ and tμ to denote generated interarrival times and service times, respectively. During
the simulation run, totλ and totμ are running totals of arrival times and service times.
• Since we require the mean time spent in the system, we keep the arrival time of each customer.
Element i of array T will hold the arrival time of customer i, for i ≤N. Subtracting T [i] from
the time customer i departs gives customer i’s total system time (the variable response). The
average queueing time (variable wait) is found by subtracting the sum of all service times
from the sum of all response times and dividing by N.
• At simulation initialization, we shall set t = 0 and td = ∞where ∞is taken to be a number
so large that there is little possibility of t ever reaching this value during the simulation. The
initialization will also generate the time of the ﬁrst arrival and initialize ta to this value.
The following actions must be performed by the program modules that handle events:
1. An arrival:
• Advance the internal clock to the time of arrival.
• An additional customer has arrived so modify system state n appropriately (n = n+1).
• Increment na, the number of arrivals so far and, if na ≤N, set T [na] = t.
• Generate tλ, the time until the next arrival; compute the next arrival instant ta = t + tλ
and add tλ into totλ.
• If n = 1, signifying that the server had been idle prior to this arrival, generate tμ, the
service time for this customer, compute its departure time td = t + tμ and add tμ into
totμ.
2. A departure:
• Advance the internal clock to time of departure.
• A customer has left so modify system state n appropriately (n = n −1).
• Increment nd, the number of departures so far. Compute the time this departing
customer spent in the system and add into response.
• If n = 0, set td = ∞. Otherwise generate tμ, the service time of the next customer;
compute the next departure instant td = t + tμ and add tμ into totμ.
All that remains is to specify the main module. This module must
• Perform all initialization functions as described above.
• If the number of departures is less than N:
– initiate an arrival event if ta < td: otherwise initiate a departure event (ta ≥td).
• Generate terminal statistics and print report.
These features have been incorporated into the following Java program. Because of the simplicity
of the model, we have not deﬁned separate modules to handle arrival and departure events, but

684
Implementing Discrete-Event Simulations
simply incorporated their actions into the body of the program. The statements in the program
closely follow the description just given and eliminates the need for extensive comments. The model
parameters are taken to be N = 100, λ = 1.0 and μ = 1.25.
import java.util.Random;
import static java.lang.Math.*;
class mm1 {
public static void main (String args[]) {
//
*** Variable definitions and initializations ***
Random generator = new Random();
double u;
int N = 100;
double lambda = 1.0; double mu = 1.25;
double infty = 999*N*mu;
int n = 0;
int n_a = 0;
int n_d = 0;
double t = 0;
double t_a;
double t_d = infty;
double t_lambda = 0; double t_mu = 0; double tot_lambda = 0;
double tot_mu = 0;
double[] TA = new double [N+1]; double response = 0; double wait = 0;
u = generator.nextDouble(); t_a = -log(u)/lambda;
tot_lambda = t_a;
//
*** Begin
simulation proper ***
while (n_d < N) {
if (t_a < t_d) {
// Arrival event
t = t_a;
n++;
n_a++;
if(n_a <= N) {TA[n_a] = t; }
u = generator.nextDouble(); t_lambda = -log(u)/lambda;
t_a
= t + t_lambda;
tot_lambda += t_lambda;
if (n==1) {
// Arrival to an empty system
u = generator.nextDouble(); t_mu = -log(u)/mu;
t_d = t+t_mu;
tot_mu += t_mu;
}
}
else {
// Departure event
t = t_d;
n--;
n_d++;
response += t-TA[n_d];
if (n==0) {
t_d = infty;
}
else {
u = generator.nextDouble(); t_mu = -log(u)/mu;
t_d = t+t_mu;
tot_mu += t_mu;
}
}
}
//
*** Generate statistics and print report ***

19.2 Some Common Simulation Examples
685
wait = (response-tot_mu)/N;
System.out.println("
");
System.out.println("Mean interarrival time: " + tot_lambda/n_a);
System.out.println("Mean queueing time: " + wait);
System.out.println("Mean service time: " + tot_mu/N);
}
}
Sample runs of this program with different values of N give the following results:
N
Mean interarrival time
Mean queueing time
Mean service time
102
0.8486
3.6909
0.8665
104
0.9915
3.0980
0.7915
106
0.9995
3.2153
0.8001
Exact
1.0
3.2
0.8
Observations
1. There are many ways to write a program to simulate an M/M/1 queue and this particular
program is only one of them. This Java program is not designed to be a robust production
code, but should be used for illustration purposes only.
2. Since there are only two events, we choose not to use an event list. The next event to occur is
found using a single if statement.
3. The simulation run ends once N = 100 customers have been served and we store the arrival
times of these customers in an array T of length N + 1, since Java indexes arrays from 0.
Location zero is not used and the arrival time of customer i, i = 1, 2, . . . , 100 is stored in
location i.
4. Once N customers have arrived, we could have set ta, the time of the next arrival, to inﬁnity
and saved on some computation. In this case, care must be taken to correctly compute the
mean interarrival time. This simulation program permits arrivals to continue to occur up until
the time the N th customer departs.
5. Other implementations may seek to compute the lengths of busy and idle periods.
Some Extensions to the M/M/1 Queue
1. Non-exponential interarrival and service time distributions.
This can be handled quite easily. The previous code can be used with the exponential random
number generator replaced by a random number generator for whatever type of distribution
is required.
2. Limited waiting room (M/M/1/K).
In this case, some customers will not be able to enter the queue and a counter is needed
to keep track of the number of lost customers. Only the arrival event module needs to be
modiﬁed. After the count of lost customers is incremented and the internal clock updated,
the only remaining task is to generate the time of the next arrival, and update the sum of
interarrival times.
3. Multiple servers (M/M/c).
Both arrival and departure event modules need to be modiﬁed. If, upon an arrival, the number
in the system is less than c, that arriving customer can go immediately into service—the
condition n == 1 should be replaced with n <= c. Within the departure event module, the
mechanism for computing the queueing time must be modiﬁed, since departures need not

686
Implementing Discrete-Event Simulations
be in the same order as arrivals. One possibility is to use an additional array T S whose ith
elements keeps the time at which customer i enters service. Then TS[i] −TA[i] is the time
that customer i waits in the queue.
4. The M/M/1 queue with probability α of feedback.
Only the departure event module will need to be changed. It is necessary to generate a
uniformly distributed random number u ∈(0, 1) to determine whether a departure is fedback
or not. If u < α then the system state must be left unaltered. Everything else remains
unchanged.
5. Different customer classes with nonpreemptive scheduling.
In this case, the system state must be replaced by a vector of length K where K is the
maximum number of possible classes. At each arrival instant, the arrival module must ﬁrst
generate a uniformly distributed random number u ∈(0, 1) and use it to select the class of
the next arrival. When this has been accomplished, it must then generate the time of the next
arrival for a customer of that class and increment the system state according. If upon arrival
the system is empty, a service time speciﬁc to the class of the arriving customer is generated
and the arriving customer goes immediately into service.
At each departure instant, the departure event module must select the next customer to enter
service (highest priority ﬁrst), modify the appropriate component of the state descriptor
vector and generate the service time of the next customer of the same class, if such a customer
is present. Since departures are not necessarily in the same order as arrivals, the comments
made with respect to the M/M/c queue and the array TS also apply here.
19.2.2 Simulating Closed Networks of Queues
We consider a network consisting of M service centers, each of which contains a single queue that
feeds a single server. Service times are exponentially distributed but may be different at different
centers. A ﬁxed number of customers, N, circulates among these centers: after leaving service center
i, a customer next proceeds to service center j with probability ri j for i, j = 1, 2, . . . , M and
M
j=1 ri j = 1 for i = 1, 2, . . . , M. Customers do not arrive from the exterior, nor do they leave the
network.
The different variables we use in simulating this network are as follows:
• The variable t denotes the internal clock time.
• We shall run the simulation until the ﬁrst departure from any service center after time T . Our
objective is to ﬁnd the bottleneck service center, the service center with the most customers.
• The system state is represented by a vector of length M whose ith component holds the number
of customers at service center i. We shall use the array SS (for system state) to hold this vector.
• Only departure events need be considered, and there is one for each service center in the
network. An array EL (for event list) keeps the scheduled time of next departure from each
service center. If service center i is empty, then EL[i] = ∞.
• The service center from which a departure occurs is designated the “source” center; the service
center to which a customer arrives is designated the “destination” center.
• The parameter of the exponential service time distribution at service center i is stored in
position i of array mu.
• At initialization, we set t = 0 and partition the N customers among the M service centers. For
each i such that SS[i] > 0, we generate an exponentially distributed random number having
parameter mu [i] and store it in EL[i]. All other positions in EL[i] are set equal to inﬁnity.
The main simulation module contains all variable deﬁnitions and initializations. It is responsible
for computing ﬁnal statistics and printing the report at the end of the simulation. While the internal

19.2 Some Common Simulation Examples
687
clock time t does not exceed the simulation terminal time T , the main simulation module repeatedly
searches through the event list to ﬁnd the ﬁring time of the next event and the index, “source,” of
the service center from which it occurs. It then passes control to the event module that handles
departures from service center “source.” The following actions must be performed by each event-
handling module.
1. Advance the internal clock to the time of this event.
2. Generate a uniformly distributed random number and use it to determine the destination,
“dest,” of the transition from this, the source service center.
3. Move one customer from this service center to the destination service center.
4. Generate the time of the next transition from the source service center.
5. If the destination service center now contains only one customer, signifying that the arrival
occurred to an empty station, generate the next departure time from this “destination” service
center.
The Java program given below incorporates these features. It closely follow the description just
given so extensive comments are not included. The model parameters are taken to be M = 4,
N = 10 and mu = (3, 1, .75, 2). The simulation begins at time t = 0 and ends at time T = 100.
import java.util.Random;
import static java.lang.Math.*;
class qn {
public static void main (String args[]) {
//
*** Variable definitions and initializations ***
int M=4;
int N=10;
double t=0;
double T=100;
int [] SS = new int [M];
double [] EL = new double [M];
double [] mu = {3, 1, .75, 2};
double [][] RR = {{.2,.4,.4,1}, {.5,1,1,1}, {0,.4,.4,1}, {0,0,1,1}};
double infty = 99999999.99;
Random generator = new Random();
double u;
SS[0] = N;
for (int i=1; i<M; i++) { SS[i] = 0;}
for (int i=0; i<M; i++) {
EL[i] = infty;
if ( SS[i] > 0 ) {
u = generator.nextDouble(); EL[i] = -log(u)/mu[i];
}
}
//
*** Begin
simulation proper ***
while (t <=
T) {
int source = 0;
double small = EL[0];
// Get source
for (int i=1; i<M; i++) {
if (small > EL[i]) { small = EL[i]; source = i;}
}

688
Implementing Discrete-Event Simulations
u = generator.nextDouble();
//
Get destination
int dest = 0;
while (u > RR[source][dest]) dest++;
t = EL[source];
SS[source]--;
SS[dest]++;
// Next state
EL[source] = infty;
if (SS[source] > 0) {
u = generator.nextDouble();
EL[source] = t-log(u)/mu[source];
}
if (SS[dest] == 1) {
u = generator.nextDouble();
EL[dest] = t-log(u)/mu[dest];
}
}
//
*** Generate statistics and print report ***
System.out.println("State at time T:");
System.out.println(SS[0] + " " + SS[1] + " " + SS[2] + " " + SS[3]);
}
}
Observations
1. In this program, the M service centers are indexed from 0 through M −1.
2. To facilitate the choice of the destination service center, each row of the two-dimensional
routing matrix contains cumulative probabilities. Thus RR[i][ j] =  j
k=0 rik where rik is the
probability that on leaving service center i the destination is service center k. In this example,
the routing probability matrix R and its stored cumulative variant, RR, are
R =
⎛
⎜
⎜
⎝
.2
.2
0
.6
.5
.5
0
0
0
.4
0
.6
0
0
1
0
⎞
⎟
⎟
⎠,
RR =
⎛
⎜
⎜
⎝
.2 .4 .4 1
.5 1 1 1
0 .4 .4 1
0 0 1 1
⎞
⎟
⎟
⎠.
3. A linear search is used to ﬁnd the next event. Extensions to this basic model can result in this
list becoming much larger, in which case some other approach might need to be used.
4. Since all the service centers in this queueing network are identical, except for the rate of
service they provide, it sufﬁces to have only one event module to cater for departures from
all M service centers. The appropriate service rate can be picked up from the array mu. Also,
since this is very straightforward, it is incorporated directly into the body of the program.
5. The objective of the simulation is to ﬁnd the bottleneck service center. Several runs of
this program produced (0, 1, 9, 0), (0, 3, 7, 0), (0, 1, 8, 1) as the ﬁnal state, from which it
is apparent that, for this particular model, the third service center (with index 2) is the
bottleneck.
Some Extensions to the Basic Model
1. While some of the extensions identiﬁed previously for the M/M/1 queue can be applied
in a straightforward manner to the individual service centers of a queueing network—
exponentially distributed service times can be immediately replaced with more general
distributions—others can induce considerable complexity. When limitations are placed on

19.2 Some Common Simulation Examples
689
the number of customers a service center can hold, blocking occurs. In a closed queueing
network, customers cannot be lost, so those who are refused admission to a service center
must be accommodated elsewhere.
2. Allowing for multiple customer classes, some of which have customers who arrive from the
exterior and eventually depart from the network while others have customers who forever
cycle among the service centers of the network, adds another order of magnitude to the
complexity of the simulation. The length of the system state descriptor can become large,
as can the number of possible events, requiring data structures and algorithms that are much
more sophisticated than a linear search on an event list maintained as a short array—the
approach adopted here.
3. To compute the marginal distribution of customers at any service center it is ﬁrst necessary
to run the simulation until the effect of the initial placement of customers in the network has
faded. This can be accomplished by running the simulation until the system state appears
to ﬂuctuate around a certain set of values, instead of appearing to move in some particular
direction. The reader should try running the Java program to observe this phenomenon. Once
this point has been passed, then it is possible to begin to total the periods of time that the
speciﬁed service center has 0, 1, . . . , N customers. Dividing each by the total time minus the
initialization period gives the required marginal distributions.
19.2.3 The Machine Repairman Problem
In this scenario, a factory possesses a total of M identical machines that are subject to breakdown.
A certain minimum number of working machines N < M is required for the factory to function
correctly. When less than N of the machines are in working order, the factory is forced to stop
production. Machines in working order in excess of the N needed for the factory to keep functioning
are kept as spares. When a machine breaks down, it is immediately replaced by a spare, assuming
one is available, and the broken machine is set aside for repair. We shall assume that machines
break down independently and we shall let B(t) be the (general) probability distribution function
of the time from the moment a machine is placed in service until it breaks down. A single repair
man is available and he repairs broken machines one at a time. The distribution of repair time has
a (general) probability distribution function R(t). Once repaired, a machine immediately becomes
available as a spare. Spare machines are not used and so do not break down.
In simulating this system, we shall use the following variables:
• The total number of machines is M and a minimum of N of them are needed for the factory
to continue to function. The variable t will be used to track internal clock time.
• The number of machines that are in working order at time t is denoted by n while the number
that are broken is denoted by b. Since at any time t, n + b = M, only one if these variables is
actually needed, but we shall use both to facilitate clarity.
• Either n or b can be used as the system state descriptor.
• There are two different types of event—a machine breaks down, or a machine is repaired and
returns to the spare category.
• Since the distribution of machine breakdown times need not be exponentially distributed, it is
necessary to keep the next breakdown time for each of the N working machines.
These times are stored in positions 0 through N −1 of the event list, EL.
• The time until the next repair completion, which we take to be ∞if b = 0 (the number of
broken machines is zero) is stored in position N of the event list.
• We shall use this simulation to compute the length of time until the number of working
machines ﬁrst falls below N and the factory is forced to halt production.

690
Implementing Discrete-Event Simulations
As always, the main simulation module contains all variable deﬁnitions and initializations and is
responsible for printing the report at the end of the simulation. So long as the number of working
machines is greater than or equal to N, the main module searches the event list to determine the
time and type of the next event and passes control to the appropriate event handing module. If the
next event is a breakdown, the index of the machine that failed must also be obtained.
The following actions must be performed by the module that handles repair events.
1. Advance the internal clock to the time of this event.
2. Increment the number of working machines and decrement the number of broken machines.
3. If there is another machine to be repaired, compute the time at which the repair will be
ﬁnished, and insert this time into the event list at position N. If the repair queue is empty, the
time until the next repair completion is set to ∞.
The following actions must be performed by the module that handles breakdown events.
1. Advance the internal clock to the time of this event.
2. Decrement the number of working machines and increment the number of broken machines.
3. If there is a spare machine available (n > N), put it into service and generate a random
number having distribution B(t) with which to determine the time at which this new machine
will fail. This time should be inserted into the event list at position index.
4. If b = 1, signifying that prior to this breakdown, the repairman was idle, generate a random
number having distribution R(t) and use it to determine the time at which the repair will be
completed.
The following Java program incorporates these features. It closely follow the description just
given so extensive comments are not included. The model parameters are taken to be M = 10 and
N = 6. Both repair time and breakdown times are taken to be normally distributed: mean repair
times are 15 time units with standard deviation σ = 1 while times between breakdown have mean
600 time units and standard deviation σ = 100. As was the case for the previous examples, and for
exactly the same reason, the event modules are incorporated directly into the main program routine.
import java.util.Random;
import static java.lang.Math.*;
class mr {
public static void main (String args[]) {
//
*** Variable definitions and initializations ***
int M=10;
int N=6;
double t=0;
int n = M;
int b = 0;
double [] EL = new double [N+1];
double infty = 99999999.99;
Random generator = new Random();
double u1;
double u2;
double sigma1 = 100; double mu1 = 600;
double sigma2 = 1;
double mu2 = 15;
for (int i=0; i<N; i=i+2) {
u1 = generator.nextDouble();
u2 = generator.nextDouble();
double x1 = cos(2*PI*u1) * sqrt(-2 * log(u2));
double x2 = sin(2*PI*u1) * sqrt(-2 * log(u2));
EL[i] = sigma1*x1 + mu1; EL[i+1] = sigma1*x2 + mu1;
}
EL[N] = infty;

19.2 Some Common Simulation Examples
691
//
*** Begin
simulation proper ***
while (n >= N) {
int index= 0;
double small = EL[0];
for (int i=1; i<=N; i++) {
// Get next event
if (small > EL[i]) { small = EL[i]; index = i;}
}
t = EL[index];
if (index == N)
{
// Next event is a repair completion
n++;
b--;
EL[N] = infty;
if (b > 0) {
// Get time until next repair completion
u1 = generator.nextDouble();
u2 = generator.nextDouble();
double x1 = cos(2*PI*u1) * sqrt(-2 * log(u2));
EL[N] = t + sigma2*x1 + mu2;
}
}
else {
// Next event is a breakdown
n--;
b++;
if ( n > N ) {
// Get time until the new machine fails
u1 = generator.nextDouble();
u2 = generator.nextDouble();
double x1 = cos(2*PI*u1) * sqrt(-2 * log(u2));
EL[index] = t + sigma1*x1 + mu1;
}
if (b==1) {
// Repair queue was empty
u1 = generator.nextDouble();
u2 = generator.nextDouble();
double x1 = cos(2*PI*u1) * sqrt(-2 * log(u2));
EL[N] = t + sigma2*x1 + mu2;
}
}
}
//
*** Generate statistics and print report ***
System.out.println("System halts at time " + t);
}
}
Observations
1. The polar method is used to compute normally distributed random numbers. This method uses
two uniformly distributed random numbers u1 and u1 to generate two normally distributed
random numbers x1 and x2. Both are used in the initialization phase. However in the event
handling modules, when normally distributed random numbers are required one at a time,
only x1 is used. Some computation time can be saved by computing x2 and using it the next
time a normally distributed random number having the same distribution is needed.
2. The program does not contain a test to ensure that the generated normally distributed random
numbers are positive. The particular normal distributions used in the example make the
occurrence of negative numbers unlikely. This may not be so for other normal distributions.

692
Implementing Discrete-Event Simulations
3. The event list is not stored according to shortest ﬁring time and a linear search is used to
obtain the next event. This is appropriate when the number of machines is not large, less than
20, for example. When the number of machines is of the order of hundreds or greater, then a
different data structure may be more appropriate.
Some Extensions to the Basic Model
1. The most common extension to the basic machine repairman problem is the incorporation of
additional repairmen. In this case, the state descriptor needs to include the number of busy
(or idle) repairmen and the event list must be augmented to include a slot for each repairman
(unless repair times are exponentially distributed). If a machine breaks down while at least
one repairman is idle, repairs on the machine can begin immediately.
2. This simulation halts once the number of working machines falls below the minimum
required. In other scenarios, the percentage of unproductive time might be what is needed.
In this case, the simulation continues until some predetermined ending time T and the sum
of the lengths of time for which n < N is computed. The ratio of this to T can be used to
compute the percentage down-time during the simulation run.
3. In a factory, there are possibly different types of machine and some limited number of each
is needed in order for production to continue. Thus spares of each of the different types are
kept on standby. Furthermore, machines of different type may have different breakdown and
repair time distributions and repairmen may be specialized to handle just one or two different
types of machine. The objective of simulating such a system may be to determine the optimal
number of spares of each machine type to keep on standby given certain pricing and cost
constraints.
19.2.4 Simulating an Inventory Problem
We consider a shop which sells a certain product. Customers arrive at the shop according to a
Poisson process with parameter λ and purchase a certain quantity of the product. We assume that
the product is available in unit measurements and the number of units of the product purchased by a
customer is a discrete random variable having an arbitrary probability mass function. If the shop has
less units available than the customer wants, the customer purchases all available units but does not
seek to back-order the rest from the shop. Perhaps the customer goes elsewhere to ﬁll the remainder
of his order. In any case, from the shop’s point of view, this is lost business.
The shopkeeper purchases the product from a supplier at a cost of $c per unit and sells it at a cost
of $d per unit. The amount of the product on hand in the shop, the number of units of the product
available for sale, is called the shop inventory. There is a certain cost associated with holding this
product. For example, in a car dealership, the dealership owner may have obtained a bank loan to
cover his costs and the longer each car remains unsold in his inventory, the more interest he pays to
the bank. The cost of holding a unit of stock is taken to be a linear function of the time it is held.
When the inventory falls below a certain threshold, more is ordered from the supplier. Speciﬁcally,
when the number of units available for sale falls to s or less, a sufﬁcient quantity is ordered to bring
the amount in stock up to S. This is called an (s, S) ordering policy. The shop owner incurs a cost
in ordering additional units of the product over and above the cost of the units purchased. This is
typically a ﬁxed cost to cover transportation. Furthermore, there is usually a delay between sending
the order to the supplier and the units of the product arriving at the store. We assume this delay has
a general probability distribution given by G(t). Only one order at a time is given to the supplier
and the shopkeeper waits until that order has been ﬁlled before sending an additional order. We seek
to determine the amount of proﬁt (or loss) the shopkeeper makes during a time period equal to T ,
given that he starts with an initial S units of the product.

19.2 Some Common Simulation Examples
693
In simulating this inventory scenario, we use the following variables:
• The variable t denotes internal clock time. The simulation runs until the completion of the
ﬁrst event after time T .
• The state of the system is deﬁned by the pair (n, m), where n is the number of units of the
stock in the inventory and m is the number of units on order and awaiting delivery to the
shop.
• There are only two possible events:
1. a customer arrives and requests a certain number of units of the product;
2. the arrival of previously ordered units from the supplier.
• The ﬁrst position in the event list EL[0] stores the time of the next customer arrival;
EL[1] holds the next arrival time of new supplies to the shop.
• The shopkeeper’s cost of one unit of the product is $c, which he sells at $d.
• The variable h is the cost of holding one unit of the product for one time unit. At any time t,
the variable totH keeps the total cost of holding inventory up to time t.
• The variable totS holds the total of supply costs up to time t: totS includes the cost of the
product to the shopkeeper plus the delivery cost, $z per delivery.
• A variable called totR is used to keep the total revenue earned by the shopkeeper.
The main simulation module contains all variable deﬁnitions and initializations and is responsible
for printing the report at the end of the simulation. While t < T , it checks the event list to determine
the time of the next event and passes control to either the customer arrival module or the order
arrival module; it then waits for control to be returned from the event handling module at which
time it launches the next event.
The following actions must be performed by the module that handles customer arrival events.
1. Update total inventory holding costs: totH = totH + n (EL[0] −t) h.
2. Advance the internal clock to the time of this event: t = EL[0].
3. Generate x, the number of units of the product required by the customer, from a general
probability mass function.
4. Determine the number of units actually sold to the customer: r = min(n, x).
5. Update the inventory level: n = n −r.
6. Update shopkeeper’s revenue: totR = totR + r ∗d.
7. If n < s and m = 0, order m = S −n units of the product, generate the time at which the
order will arrive (general probability distribution G(t)) and store it in EL[1].
8. Generate time of next customer arrival (exponential interarrival times) and store it in
EL[0].
The following actions must be performed by the module that handles inventory resupply events.
1. Update total inventory holding costs: totH = totH + n (EL[1] −t) h.
2. Advance the internal clock to the time of this event: t = EL[1].
3. Update shopkeeper’s total costs: totS = totS + m × c + z.
4. Update state descriptor: n = n + m and m = 0.
5. Set EL[1] = ∞since there is no longer an outstanding order.
The following Java program incorporates these features. It closely follow the description just
given so extensive comments are not included. The model parameters are taken to be s = 4 and
S = 10. The cost and selling price of one unit of the product are given by c = 3 and d = 5
respectively. The inventory holding cost is taken to be h = .5 per unit of product per time unit and
the ﬁxed cost of transportation from the supplier to the shop is z = 4. The shopkeeper incurs an
initial cost of cS + z, the price of the initial inventory. The probability pk that an arriving customer

694
Implementing Discrete-Event Simulations
requests k units of the product is
p1 = .25,
p2 = .55,
p3 = .20,
and
pk = 0 otherwise.
We take the probability distribution of both customer arrival times and order arrival times to be
Poisson, the former with parameter λ = 2 and the latter with parameter γ = .4.
import java.util.Random;
import static java.lang.Math.*;
class inv {
public static void main (String args[]) {
//
*** Variable definitions and initializations ***
double t=0;
double T = 100;
int s = 4;
int S = 10;
int n = S;
int m = 0;
double [] EL = new double [2];
double lambda = 2;
double gamma = .4;
double infty = 99999999.99;
double c = 3;
double d = 5;
double h=.5;
double z = 4;
double totH = 0;
double totS = S*c+z;
double totR = 0;
Random generator = new Random();
double u = generator.nextDouble();
EL[0] = -log(u)/lambda;
EL[1] = infty;
//
*** Begin
simulation proper ***
while (t < T) {
if (EL[0] < EL[1]) {
// Next event is a customer arrival.
totH = totH+n*(EL[0]-t)*h;
t = EL[0];
u = generator.nextDouble();
int x = 1;
if (u >=.25 && u < .75) x = 2;
if (u >= .75) x = 3;
int r = x;
if (r > n) r = n;
n = n-r;
totR = totR + r*d;
if (n<s && m==0) {
m = S-n;
u = generator.nextDouble();
EL[1] = t-log(u)/gamma;
}
u = generator.nextDouble();
EL[0] = t-log(u)/lambda;
}
else {
// Next event is an order arrival.
totH = totH+n*(EL[1]-t)*h;
t = EL[1];
totS = totS + m*c+z;
n = n+m;
m=0;
EL[1] = infty;
}
}

19.3 Programming Projects
695
//
*** Generate statistics and print report ***
double profit = totR-totS-totH;
double meanProfit = profit/t;
System.out.println(" " );
System.out.println("System halts at time " + t);
System.out.println("Profit/Loss: " + profit);
System.out.println("Profit/Loss per unit time: " + meanProfit);
}
}
One run of this program produced the following output:
System halts at time 100.53463369921157
Profit/Loss: 183.03161503538936
Profit/Loss per unit time: 1.8205827017085434
Some Extensions to the Basic Model
1. The basic program provided above computes only the proﬁt/loss over a ﬁxed period of time.
Statements could be inserted directly into the program to compute other quantities, such as
the loss of revenue due to an insufﬁcient supply of the product.
2. Often the purpose of running simulations of inventory systems is to determine optimal
values for the parameters s and S of the ordering policy, given that the costs are all
known.
3. In some inventory models, the product is only viable for a limited period of time and must
be discarded at the expiration date. This is the case of many groceries in a grocery store, or
daily newspapers at a newsagent’s store. Sometimes the product has a residual value once
its expiration date is reached. Unsold newspapers can perhaps be sold to a paper recycling
facility for a small fraction of their face value.
19.3 Programming Projects
Exercise 19.1.1 Modify the M/M/1 Java simulation program by replacing the exponential distributions with
Erlang-4 distributions having the same means. Generate output for a total of six simulation runs. Compare your
results with the exact answer, obtained by running the code for the Ph/Ph/1 queue provided in Part III of this
text.
Exercise 19.1.2 Extend the M/M/1 Java simulation program so that it simulates
(a) an M/M/1/K queue with K = 10. Run your simulation six times and estimate the probability that an
arriving customer is lost.
(b) an M/M/c queue with c = 2. Run your simulation six times and estimate the probability that both
servers are simultaneously busy.
Exercise 19.1.3 Modify the Java simulation program qn.java so that it computes the marginal distribution at
the ﬁrst service center. Then use your program to simulate the following central server queueing network and
determine the marginal distribution of customers at the ﬁrst service center. Compare your output to the exact
results obtained using the algorithms of Chapter 16 of this text.

696
Implementing Discrete-Event Simulations
The model consists of three service centers, each of which contains a single exponential server and the
scheduling discipline is FCFS. The routing probability matrix is given as
⎛
⎝
0
0.7
0.3
1
0
0
1
0
0
⎞
⎠.
The exponential service time distributions have a mean service time of 2.0 at server 1, 1.0 at server 2, and 0.5
at server 3. The number of customers that circulate in the network is equal to 3 and all service centers have
sufﬁcient space that blocking is not a problem.
Exercise 19.1.4 Extend the Java simulation program qn.java to two classes of customer, the ﬁrst of which
arrives from, and departs to, the exterior, while the second cycles permanently among the service centers. Each
class has its own routing matrix and service rates, and customers of class 1 have nonpreemptive priority over
those of class 2.
Exercise 19.1.5 Modify the machine repairman simulation code to incorporate two repairmen. Run the same
experiment as in the text to determine by how much the additional repairman extends the time until the entire
system halts.
Exercise 19.1.6 Write a simulation program to determine the time to failure in the following multicomponent
system. There are M1 machines of the ﬁrst type and M2 of the second type. To function, the factory needs at
least N1 machines of the ﬁrst type and N2 of the second type. The probability distribution of failure times and
repair times are all normally distributed. Machines of type 1 fail according to an N(600, 104) distribution and
are repaired according to an N(15, 1) distribution. The corresponding distributions for machines of type 2 are
N(1200, 164) and N(50, 4), respectively.
Exercise 19.1.7 Modify the inventory model code so that it computes the number of units of product that the
shopkeeper missed out on selling because his inventory was empty. Run your program ten times and compute
the average of these values. Now modify the values of s and S and repeat the experiment. In your experiments,
which values of s and S perform best?

Chapter 20
Simulation Measurements and Accuracy
So far we have seen how having access to a sequence of uniformly distributed random numbers
allows us to obtain approximate answers to different probabilistic scenarios. We have investigated
how to generate such sequences of random numbers and how to convert a sequence of uniformly
distributed random numbers into distributions, both discrete and continuous, that are not uniform.
We have seen how to implement simulation experiments and studied a number of simulation
problems. In this ﬁnal chapter we ask some questions on the nature of simulation itself.
When simulating a system, whether a simple probability experiment or a complex system
simulation, each execution or run of the experiment produces a single result such as a head or a
tail, the maximum number of customers in a waiting room, the length of time that patients wait to
see a doctor, the number of messages lost in a communication system, the amount of unsold stock
in an inventory, and so on. Each result (we shall also use the word output) need not be a single real
number, but could also be the values that a set of parameters have acquired during the simulation.
Nevertheless, each execution of the simulation produces just one output, be it scalar or vector, and
the value of that result is a function of the random numbers used in the simulation. Running the
simulation again will almost certainly produce a different result. This raises a number of questions
that must be asked, including
• how many times do we perform the simulation?
• what do we choose as the ﬁnal “answer” ?
• how do we measure the accuracy of the results obtained?
Basic to these questions is the concept of sampling to which we now turn, since it is useful to think
of the output of a simulation run as a sample from the inﬁnite sample space of all possible outputs.
20.1 Sampling
To motivate our discussion on sampling, consider the following hypothetical situation concerning
a problem faced by the Cary, North Carolina, town council. The council is about to pass new laws
concerning smoking in public places, but they have decided to ﬁrst conduct a survey to determine
the number of Cary residents who smoke. Cary is a town of approximately 100,000 inhabitants.
The company charged with conducting the survey has a number of important decisions to make,
including just how many of the town’s population they need to contact and interview, and how these
particular individuals should be chosen. They also need to infer the number of Cary citizens who
smoke based on the information they collect. The set of individuals chosen to be interviewed is
called the sample set and its size is called the sample size. The 100,000 inhabitants of Cary form the
population for the sampling experiment.
Suppose the company decides that a sample size of 50 is adequate for sampling purposes. Leaving
aside for the moment, a discussion of how large the sample size should be, an important question to
pose is that of how to choose these 50 individuals. It would appear natural to choose them completely
at random, so that each citizen had an equal chance of being selected. If this is indeed the case, the

698
Simulation Measurements and Accuracy
sample is said to be unbiased. On the other hand, sampling bias arises when the sample set does not
represent the population. Suppose, for instance, that a travel agency needs information concerning
the choice of holiday destinations for the residents of Cary, and establishes a polling booth at the
local airport. The results would lead to inaccurate answers, because only airline travelers would be
interviewed and no information would be collected concerning those who take motoring vacations,
or vacations in the beautiful North Carolina mountains and beaches. Furthermore, no matter how
large the sample size, this sample bias would not be removed.
There is another kind of error that can arise in choosing a sample set. It may just happen that an
unrepresentative selection could occur by accident. For example suppose an unusually large number
of pregnant mothers were chosen and interviewed as to their smoking habits. It is possible that many
of them who previously smoked may have stopped smoking during the pregnancy and the number
of smokers might be underreported. The same thing might occur if the survey is conducted during
the ﬁrst week of the year, when many might have made a New Year’s resolution to quit. Or perhaps,
just by accident, the survey is sent only to non-smokers. This is called the problem of sampling
variability and is more likely to occur when the sample size is small. Unlike sample bias, sample
variability can be reduced by choosing a larger sample size.
To make the sampling process more concrete, let us shrink the population of Cary from 100,000
to ten and let us assume that of these ten citizens, three smoke. Thus we would like our sampling
experiment to return the value 0.3 as the probability that a given citizen is a smoker. Let us choose
a sample size of 2 and let us also assume that the sampling is unbiased. Choosing a sample of
two citizens from the ten can yield two, one, or zero smokers. Since the sample is assumed to
be unbiased, the probability that both citizens are smokers is the product 3/10 × 2/9 = 6/90,
the probability that neither are smokers is given as 7/10 × 6/9 = 42/90, whereas the probability
that only one of them smokes is given as 3/10 × 7/9 + 7/10 × 3/9 = 42/90. Observe that the
individuals chosen in the sample can be modeled as random variables. Let Xi, i = 1, 2, . . . , n,
be the random variable that describes the ith citizen where Xi = 1 if that citizen is a smoker and
Xi = 0 otherwise. Since they are random variables, X1 and X2 have distributions; indeed, they
have exactly the same distribution, which, since the sample is unbiased, is also the distribution
of the entire population, namely, X1 = X2 = 1 with probability 0.3 and X1 = X2 = 0 with
probability 0.7. They must also have the same mean value as the population mean. In other words,
if the population mean is μ (in our example, μ = 0.3), then E[X1] = E[X2] = μ. The same is true
for the variance, i.e., Var[Xi] = σ 2, where σ 2 is the variance of the entire population, and indeed it
is true for all higher moments. However, X1 and X2 are not independent. For example, if X1 = 1
then Prob{X2 = 1} = 2/9 whereas if X1 = 0, then Prob{X2 = 1} = 3/9. This is a result of the fact
that the sampling is conducted without replacement. If the sampling were done with replacement,
then X1 and X2 would be independent.
20.1.1 Point Estimators
The mean value for a sample set of size n, called the sample mean, is deﬁned as
¯X = 1
n
n

i=1
Xi.
Its expectation is exactly equal to the overall population mean, since, using the fact that E[Xi] = μ
for i = 1, 2, . . . , n, we have
E[ ¯X] = 1
n
n

i=1
E[Xi] = 1
n
n

i=1
μ = μ.

20.1 Sampling
699
In this case, the estimator is said to be unbiased. In general, an estimator of a parameter is said to
be unbiased when its expectation is equal to the parameter. It is said to be asymptotically unbiased,
a weaker property, when its expectation tends to the parameter in the limit as n goes to inﬁnity.
The sample mean is a random variable, since it is constituted from the random variables
Xi, i = 1, 2, . . . , n. It is used as an estimator of μ. Recall that we do not know the value of μ, but
yet we wish to determine just how good ¯X is as an estimator of μ. More generally, any function of
the observations Xi, i = 1, 2, . . . , n, is called a statistic and since the Xi are random variables, so
are statistics deﬁned as functions of them. The sample mean is a case in point. A point estimator
is a statistic that is used to approximate some parameter of the distribution from which the Xi are
chosen. Often the only point estimators of interest are the (sample) mean and (sample) variance but
others may be deﬁned. There is no reason why we should not develop estimators, not only for the
mean, and variance, but for any of the moments of the distribution function of the population.
We have just seen that the expectation of ¯X is equal to the population mean. We now wish to
examine the variance of this estimator. If the random variables X1, X2, . . . , Xn are independent
then, using the facts that for any constants α and β,
Var[αX + β] = α2Var[X]
and for independent random variables X1, X2, . . . , Xn,
Var[X1 + X2 + · · · + Xn] = Var[X1] + Var[X2] + · · · + Var[Xn],
we obtain
Var [ ¯X] = Var
& X1 + X2 + · · · + Xn
n
'
= 1
n2 Var[X1 + X2 + · · · + Xn]
= 1
n2 (Var[X1] + Var[X1] + · · · + Var[X1])
= 1
n2 (nσ 2) = σ 2
n .
In other words, the variance of ¯X is equal to the population variance divided by n, the sample size.
An estimator whose variance goes to zero with increased n is said to be a consistent estimator. Thus
the sample mean is an unbiased, consistent estimator. Since the variance of a random variable ¯X
having mean value μ is deﬁned as Var[ ¯X] = E[( ¯X −μ)2], we have
E[( ¯X −μ)2] = σ 2
n
or n E[( ¯X −μ)2] = σ 2,
(20.1)
which means that the larger the value of n, the closer ¯X is to μ, at least probabilistically. More
correctly, we say that the sample mean is a good estimator of μ when the standard deviation,
σ/√n, is small. We have seen, however, that the Xi will not be independent if sampling is done
without replacement. When the population size is much larger than the sample size, then there is
little difference in sampling with and without replacement. This is the case in the example of the
town of Cary. There is very little difference in sampling with or without replacement when the
sample size is 200 and the population size is 100,000. In general, to be able to use the independence
property that permits variances to be added, it is usual to assume that the sampling method used is
sampling with replacement, even though this may not be strictly true. On the other hand, the fact
that the expectation of the sample mean is equal to the population mean in unbiased sampling does
not depend on the independence property.
Returning to Equation (20.1), i.e., E[( ¯X −μ)2] = σ 2/n, it is important to note that the accuracy
of ¯X as an estimator of the population mean μ increases as the sample size grows. Furthermore, this

700
Simulation Measurements and Accuracy
accuracy does not depend on the size of the actual population. Thus, if the analysis reveals that a
sample size of 200 is sufﬁcient to accurately estimate the percentage of smokers in Cary, a town with
about 100,000 inhabitants, this same number is sufﬁcient when applied to any other large city such
as New York, London, or Paris with many millions of inhabitants. This may seem counterintuitive at
ﬁrst; it may be supposed that in some way or other, the ratio of sample size to population size should
play some role in the accuracy of the estimator. However, this is not true. As is clearly shown in the
formula, only the size of the sample space is important. Of course, this assumes that the sampling is
conducted in an unbiased fashion.
The probability that the difference | ¯X −μ| is greater than some error bound ϵ may be obtained
by using Chebychev’s inequality. This inequality is generally given in the form
Prob{|X −E[X]| ≥t} ≤σ 2
X
t2 .
For the case of the sample mean, we make the substitutions X = ¯X, E[X] = μ, σ 2
X = σ 2/n. We
also replace t with ϵ and obtain
Prob{| ¯X −μ| ≥ϵ} ≤Var[ ¯X]
ϵ2
= σ 2
nϵ2 ,
which goes to zero as n tends to inﬁnity. This formula gives us a means of asking certain questions
concerning the actual accuracy of ¯X. Unfortunately, the Chebychev inequality gives rather course
bounds: much better are bounds obtained when n is sufﬁciently large that it allows us to apply the
central limit theorem. If the population distribution is not normal, then the distribution of ¯X will
not be normal either. However, the central limit theorem allows us to state that, if the underlying
distribution has mean μ and variance σ 2 but is not necessarily normally distributed, then, as n
becomes large, ¯X approaches the normal distribution with mean μ and variance σ 2/n. Put another
way, when n is larger than 30, for example, the distribution of the random variable ( ¯X −μ)/(σ/√n)
is approximately N(0, 1) and
Prob{| ¯X −μ| ≥ϵ} ≈Prob

|Z| > ϵ√n
σ
%
= 2
&
1 −	
ϵ√n
σ
'
,
where 	(x) is the cumulative standard normal distribution function. This typically gives a much
tighter bound than the Chebychev bound. For example, with ϵ√n/σ = 1.645 we ﬁnd
Prob{| ¯X −μ| ≥ϵ} ≈Prob {|Z| > 1.645} = 2 [1 −	 (1.645)] = 0.1
since 	(1.645) = .95, whereas with the Chebychev bound we ﬁnd the much weaker result
Prob{| ¯X −μ| ≥ϵ} ≤σ 2
nϵ2 =
1
1.6452 = .3695.
Example 20.1 The duration of telephone calls taken by the Town of Cary is normally distributed
with mean μ = 180 seconds and standard deviation σ = 30 seconds. Ten calls are chosen at
random and sampled. Since the duration of a call is normally distributed, the ten random variables
X1, X2, . . . , X10 are also normally distributed, and from the linearity property of the normal
distribution,
¯X = 1
n (X1 + X2 + · · · + X10)
is normally distributed as well. Indeed, ¯X has a N(180, 90) distribution, since its variance is given
by σ 2/n. The probability that ¯X is in error by more than 15 seconds may be obtained by observing

20.1 Sampling
701
that the standard deviation of ¯X is
.
302/10 = 9.4868. Then
Prob{| ¯X −μ| > 15} = 1 −Prob{| ¯X −μ| ≤15}
= 1 −Prob{−15 ≤¯X −μ ≤15}
= 1 −Prob

−
15
9.4868 ≤
¯X −μ
σ
≤
15
9.4868
%
= 1 −Prob{−1.5811 ≤Z ≤1.5811}
= 1 −2 Prob{Z ≤1.5811}
= 1 −2 × 0.4429 = .1142,
where we have read off the value Prob{Z ≤1.5811} = 0.4429 from tables of the cumulative normal
distribution function. There is therefore an 11% chance that ¯X misses the true mean by more than 15
seconds when only 10 samples are chosen. The error rises to 29.38% that ¯X misses the true mean by
more than 10 seconds with only 10 samples. The reader may wish to verify that, when 20 samples
are chosen, we obtain
Prob{| ¯X −μ| > 15} = 0.0250
and
Prob{| ¯X −μ| > 10} = 0.1362.
The astute reader will have observed that we made the implicit assumption that we know the
variance of the distribution of the population as a whole, but not its mean. We assumed that we
knew this variance and used it to judge the accuracy of the sample mean, ¯X, as an estimator of μ,
the mean of the population distribution. This might seem strange given that our purpose is to derive
an estimator of the population mean and it might seem odd that we know its variance but not its
mean. We now take up this question concerning our knowledge (or lack thereof) of the variance of
the source population and develop a point estimator for it. One possibility is to use the function S2
deﬁned as
S2 = 1
n
n

i=1
(Xi −¯X)2.
Although this may at ﬁrst appear to be a natural way to deﬁne an estimator of the variance, it turns
out that it is a biased estimator. To see this, we use the fact that the expected value of a sum is equal
to the sum of the expected values and proceed as follows:
E[S2] = E
(
1
n
n

i=1
(Xi −¯X)2
)
= 1
n
n

i=1
E[(Xi −¯X)2] = E[(X1 −¯X)2]
(20.2)
= E[X2
1 −2X1 ¯X + ¯X2] = E[X2
1] −2E[X1 ¯X] + E[ ¯X2].
(20.3)
Equation (20.2) holds since
E[(X1 −¯X)2] = E[(X2 −¯X)2] = · · · = E[(Xn −¯X)2].

702
Simulation Measurements and Accuracy
We now consider the terms on the right-hand side of Equation (20.3) one at a time. For the ﬁrst term
we use a property of variances, namely, Var[X] = E[X2] −E[X]2, to obtain
E[X2
1] = σ 2 + μ2.
For the second term, we have
E[X1 ¯X] = E
(
X1
1
n
n

i=1
Xi
)
= 1
n E
(
X2
1 +
n

i=2
X1Xi
)
= 1
n

E[X2
1] +
n

i=2
E[X1Xi]

= (σ 2 + μ2) + (n −1)μ2
n
.
To simplify the third term, we once again apply the same property of variances used to simplify the
ﬁrst term and obtain
E[ ¯X2] = σ 2
n + μ2.
Taking these three together, the μ2’s cancel out and we are ﬁnally left with
E[S2] = σ 2 −σ 2
n = n −1
n
σ 2.
For S2 to be an unbiased estimator we need to have E[S2] = σ 2, so this particular estimator of
the variance turns out to be biased. It slightly underestimates the value of σ 2. However, a simple
adjustment is sufﬁcient to correct this bias. If we deﬁne S2 as S2 = n
i=1(Xi −¯X)2/(n −1) and
not as S2 = n
i=1(Xi −¯X)2/n, then S2 is an unbiased estimator of the variance. To show this we
proceed as follows:
E[S2] =
1
n −1 E
( n

i=1
(Xi −¯X)2
)
=
1
n −1 E
( n

i=1
X2
i −2
n

i=1
Xi ¯X +
n

i=1
¯X2
)
=
1
n −1 E
( n

i=1
X2
i −2 ¯X
n

i=1
Xi + n ¯X2
)
=
1
n −1 E
( n

i=1
X2
i −2 ¯X
#
n ¯X
$
+ n ¯X2
)
=
1
n −1 E
( n

i=1
X2
i −n ¯X2
)
=
1
n −1
n

i=1
E[X2
i ] −
n
n −1 E[ ¯X2].
Substituting E[X2
i ] = σ 2 + μ2 and E[ ¯X2] = σ 2/n + μ2, obtained previously, this becomes
E[S2] =
n
n −1
#
σ 2 + μ2$
−
n
n −1
σ 2
n + μ2

= nσ 2
n −1 −
σ 2
n −1 = σ 2.

20.1 Sampling
703
Hence this second formulation of S2 is an unbiased estimator of the variance. However, an estimator
may be biased and still serve a useful purpose, particularly if the bias is not great. This is the case of
the original estimator for S2, which is as close to being unbiased as (n −1)/n is to 1. We shall see in
the next section that this biased estimator provides a particularly simple formulation for conﬁdence
intervals when the samples Xi, i = 1, 2, . . . , n, assume only the values 0 or 1, as in the case of the
smoking survey example for the town of Cary.
Before leaving this section we brieﬂy give two recursive formulae for computing the sample
mean ¯X and the sample variance S2. For a given value of n, these quantities can be computed
directly and efﬁciently from their respective formulae,
¯X = 1
n
n

i=1
Xi
and
S2 =
1
n −1
n

i=1
(Xi −¯X)2.
However, it can happen that the ﬁrst choice of n is not sufﬁciently large, that the current set of
samples does not provide sufﬁcient accuracy and so it becomes necessary to generate enough
additional samples to meet the accuracy requirements. The following recursive approach (see Ross
[46]) is an efﬁcient way to carry this out. Let
¯Xn = 1
n
n

i=1
Xi
and
S2
n =
1
n −1
n

i=1
(Xi −¯X)2
with initial conditions ¯X0 = 0 and S2
0 = 0. Then
¯Xn+1 = ¯Xn + Xn+1 −¯Xn
n + 1
(20.4)
and
S2
n+1 = (1 −1/n)S2
n + (n + 1)
# ¯Xn+1 −¯Xn
$2 .
(20.5)
Thus it is possible to incorporate additional data points as necessary. Since we have seen that ¯X is
a good estimator of μ when the standard deviation σ/√n is small, the following approach can be
used:
• Choose a value of n (where n is at least 30) and generate n samples X1, X2, . . . , Xn.
• Compute the sample mean ¯X and the sample variance S2.
• If S/√n is sufﬁciently small, then stop.
Otherwise generate additional samples and incorporate into the recursive formulae until the
desired accuracy is attained.
For example, if S/√n, the standard deviation of our estimator ¯X, satisﬁes S/√n < δ then, with
a 95% conﬁdence level, we can assert that ¯X does not differ from μ by more than 1.96 δ. Thus it
becomes possible to obtain a rough estimate of the number of samples needed so that the standard
deviation of the estimator satisﬁes a given criterion. If it is estimated that a large number of
additional samples is needed, then it will likely be more efﬁcient to use the original formulas rather
than the recursive ones.
Example 20.2 Consider the following sample of n = 16 numbers:
11.6, 13.5, 9.1, 13.9, 12.1, 9.6, 10.7, 9.5, 12.7, 10.8, 8.1, 11.4, 12.4, 11.2, 9.8, 6.1.
The sample mean is ¯X = 10.7812 and the sample variance is S2 = 62.3244/15 = 4.1550.
The standard deviation of ¯X = S/√n = 2.0384/4 = .5096. At the 95% conﬁdence we can
can assert that the sample mean of 10.7812 does not differ from the true mean by more than
1.96 × .5096 = .9988.

704
Simulation Measurements and Accuracy
To estimate the number of samples needed for the standard deviation of the estimator to be less
than .25, we need the value of n such that S/√n < .25. Given that S = 2.0384, the value of n must
be greater than 66.4793, i.e., 67.
20.1.2 Interval Estimators/Conﬁdence Intervals
Conﬁdence intervals are the usual means by which the accuracy of computer simulations is gauged.
The estimators we have discussed so far generally go under the name of point estimators whereas
conﬁdence intervals are more general interval estimators. For example, the sample mean, ¯X, is
a point estimator that provides us with a single point, an approximation to the population mean.
Similarly, the sample variance, S2, is a point estimator of the population variance. A point estimator
simply lets us know that the estimated value of some parameter ξ is α. Speciﬁcally it does not give
us any information about how much, if any, α differs from the true value, ξ. A case in point is taking
the sample mean, α = ¯X, as an estimator of the population mean, ξ = μ. Interval estimators more
generally allow us to ﬁnd the probability, ρ, that the true value ξ lies in some interval (α−ϵ1, α+ϵ2).
We have
Prob{α −ϵ1 < ξ < α + ϵ2} = ρ.
(20.6)
We call the interval (α −ϵ1, α + ϵ2) a 100 × ρ percent conﬁdence interval for the parameter ξ, and
ρ is called the conﬁdence coefﬁcient. Equation (20.6) states that the probability that the true value
of the parameter ξ lies between α −ϵ1 and α + ϵ2 is ρ, but it does not guarantee that this interval
even contains ξ. When α is an estimator based on some sample set, then α is a random variable. It
follows that the interval (α −ϵ1, α + ϵ2) is a random interval of length ϵ1 + ϵ2. This interval may,
or may not, contain the true value ξ of the parameter. The probability ρ gives the likelihood that the
interval does in fact contain ξ. If for example ρ = 0.95, then there is a 95% chance that the interval
contains ξ. Put another way, if 100 sample sets are chosen under the same circumstances (same
sample size, same method of selection, etc.) and 100 intervals constructed, one from each of the α
values obtained from each sample set, then, of these 100 random intervals, one should expect the
true value of the parameter ξ to lie in 95 of them. One should also expect that ﬁve of the intervals
do not contain ξ.
Let us now look at how we may derive conﬁdence intervals for the sample mean. Conﬁdence
intervals for other statistics follow the same procedure. We return to Chebychev’s inequality
Prob{| ¯X −μ| ≥ϵ} ≤σ 2/n
ϵ2
and write it as
Prob{−ϵ < ¯X −μ < ϵ} = Prob{| ¯X −μ| < ϵ} ≥1 −σ 2
nϵ2 .
Observe that we may make this probability arbitrarily close to 1 by choosing a sufﬁciently large
value for the sample size n. Consider the situation in which the population from which the sample
set is drawn is normally distributed with mean μ and variance σ 2, i.e., with distribution N(μ, σ 2).
The objective is to compute conﬁdence intervals for the sample mean, under the assumption that
the true mean μ is unknown but the true variance σ 2 is known. Furthermore, it has previously been
shown that the sample mean is normally distributed, having distribution N(μ, σ 2/n), and that the
random variable Z = ( ¯X−μ)/(σ/√n) is distributed according to N(0, 1). We may now use standard
tables for the normal distribution to compute a value z > 0 such that Prob{|Z| < z} = ρ and relate
this back to ¯X. We have
Prob{|Z| < z} = Prob
| ¯X −μ|
σ/√n
< z
%
= Prob

| ¯X −μ| < zσ
√n
%
= ρ,

20.1 Sampling
705
i.e.,
Prob

μ −zσ
√n < ¯X < μ + zσ
√n
%
= ρ.
(20.7)
For convenience, let us deﬁne the complement of this probability to be ζ, i.e., ζ = 1 −ρ. Then the
probability that the true mean lies outside these bounds is ζ and if we assume that the distribution
is symmetric, as is the case of the normal distribution, then we may write
Prob {Z < −z} = Prob {Z > z} = ζ
2 .
The point z for which Prob {Z > z} = ζ/2 may now be read from readily available tables and the
conﬁdence interval constructed. This particular value of z is denoted by zζ/2 and the conﬁdence
interval is

¯X −zζ/2σ
√n ,
¯X + zζ/2σ
√n

.
(20.8)
This is said to be a 100(1 −ζ)% conﬁdence interval for the population mean μ. Some of the more
commonly used values for 1 −ζ and the corresponding values of zζ/2 are as follows.
1 −ζ
0.80
0.90
0.95
0.98
0.99
0.998
zζ/2
1.282
1.645
1.960
2.326
2.576
3.090
Example 20.3 Consider a set of six random samples chosen independently from a normal
distribution having mean μ = 5.0 and variance σ 2 = 0.3. Suppose the values of Xi, i = 1, 2, . . . , 6,
the chosen sample, are
4.789,
5.021,
4.856,
4.691,
5.255,
4.703.
The sample mean is given by
¯X = 1
6
6

i=1
Xi = 4.8858.
We now need to consult the table to estimate the value of zζ/2. If we require a conﬁdence level of
95%, then from the table we obtain zζ/2 = 1.96 and from it we determine the conﬁdence interval
to be

¯X −1.96σ
√n ,
¯X + 1.96σ
√n

=

4.8858 −1.96 ×
√
0.3
2.4495
, 4.8858 + 1.96 ×
√
0.3
2.4495

= (4.4475, 5.3241),
which contains the true mean μ = 5.0.
From Equation (20.7), it is apparent that zσ/√n, called the half-width of the conﬁdence interval,
is a measure of the accuracy of the sample mean. Given that z and σ remain (approximately)
unchanged with n, then the accuracy of the estimate is proportional to the inverse of the square
root of n—to double the accuracy requires a fourfold increase in the sample size! On the other
hand, reducing the variance σ by 50% has the same effect as quadrupling the sample size. Thus the
popularity of variance reduction techniques, considered later in this chapter.
When the variance of the population is not known, then the unbiased estimator S2 = n
i=1(Xi −
¯X)2/(n −1) may be used in place of σ 2. However, it is not a good idea to simply replace σ with
S in the above formula, derived for the normal distribution, unless the value of n is large, at least
24 or preferably 30 or higher, because it gives inaccurate results—when n is large, the effect of the

706
Simulation Measurements and Accuracy
central limit theorem comes to the rescue. For small values of n, all is not lost however, because the
random variable
T =
¯X −μ
S/√n
has a Student T-distribution with n −1 degrees of freedom and tables are available from which
to compute the requisite probabilities. This may be used for small values of n, although it is
usually recommended that for meaningful results, n should be at least 10. For large values of n,
this distribution approaches the normal distribution.
Example 20.4 Returning to the previous example, we compute the sample variance as
S2 = 1
5
6

i=1
(Xi −4.8858)2 = 0.1190.
Had we used this in place of 0.3 above, then 1.96S/√n = 0.2760, and we get the interval
(4.6098, 5.1618) which also contains the true mean μ = 5. Using the Student T distribution with
ﬁve degrees of freedom,
1 −ζ
0.80
0.90
0.95
0.98
0.99
0.998
tζ/2
1.476
2.015
2.571
3.365
4.032
5.893
we ﬁnd the value tζ/2 = 2.571 and hence from 2.571S/√n = 0.3621, we obtain the interval
(4.5237, 5.2479) which contains the true mean μ = 5.
We previously mentioned that biased estimators can sometimes be of value. This is true for the
slightly biased estimator of the sample variance when each sample Xi, i = 1, 2, . . . , n, is either 0
or 1. In this case X2
i = Xi, i = 1, 2, . . . , n. Indeed Xi raised to any power is 1 if Xi = 1 and is
equal to 0 if Xi = 0 and it follows that the biased estimator simpliﬁes to S2 = ¯X(1−¯X) = μ(1−μ).
We have
S2 = 1
n
n

i=1
(Xi −¯X)2 = 1
n
n

i=1
#
X2
i −2Xi ¯X + ¯X2$
= 1
n
 n

i=1
X2
i −2
n

i=1
Xi ¯X +
n

i=1
¯X2

= 1
n
n

i=1
X2
i −2 ¯X 1
n
n

i=1
Xi + ¯X2
= 1
n
n

i=1
Xi −2 ¯X2 + ¯X2 = ¯X −¯X2 = μ(1 −μ).
Equation (20.8) then simpliﬁes to become
⎛
⎝¯X −zζ/2
/
¯X(1 −¯X)
n
,
¯X + zζ/2
/
¯X(1 −¯X)
n
⎞
⎠.
Example 20.5 In its survey of 100 residents, the Town of Cary ﬁnds that 15 of its citizens smoke.
Let us compute a 90% and a 95% conﬁdence interval for the number of smokers in the town.
We compute ¯X = 0.15 and
. ¯X(1 −¯X)/n = 0.03571. For a 90% conﬁdence interval, we use
the value zζ/2 = 1.645 and compute the interval to be
(0.15 −0.05874, 0.15 + 0.05874) = (0.09126, 0.2087).

20.2 Simulation and the Independence Criteria
707
For a 95% conﬁdence interval, we use the value zζ/2 = 1.960 and compute the interval to be
(0.15 −0.06999, 0.15 + 0.06999) = (0.0800, 0.2200).
Since these are rather large intervals, the town decides to use a greater sample. Let us now compute
the conﬁdence intervals obtained when 500 citizens are sampled and 80 declare themselves to be
smokers.
In this case, we compute ¯X = 8/50 = 0.16 and √0.16 × 0.84/500 = 0.01952. The 90% and
95% conﬁdence intervals are given respectively by 0.16 ± 1.645 × 0.01952 and 0.16 ± 1.960 ×
0.01952, i.e.,
(0.1279, 0.1921) and (0.1217, 0.1983).
20.2 Simulation and the Independence Criteria
The relationship between sampling and simulation is immediate and evident. In sampling we easily
imagine a pollster conducting telephone interviews; each member of the population contacted
constitutes one sample. In simulation, random numbers provide the means by which samples are
generated. For example, in using simulation to estimate the probability of getting exactly three
heads in ﬁve tosses of a fair coin, each and every sample is obtained from a set of ﬁve random
boolean numbers. If the sample satisﬁes the criterion (the sum of the ﬁve booleans is 3) a counter
is incremented and the estimated probability of getting three heads in ﬁve tosses is taken to be the
ratio of the number of successes (the value of the counter) to the total number of tests conducted. In
sampling terminology, the estimated probability is the sample mean, i.e., ¯X = n
i=1 Xi/n, where
Xi is the random variable that has the value one if the sum of the ﬁve random boolean numbers is
three, and is zero otherwise, and n is the total number of sets of ﬁve random numbers generated.
This view of simulation allows us to apply the concepts of estimators and conﬁdence intervals
just discussed in the sampling context to simulation, and thereby gain some idea of the accuracy
of the results obtained from the simulation. In view of the important role that the independence
of the individual samples Xi, i = 1, 2, . . . , n, plays in determining conﬁdence intervals, it is
imperative to consider the independence of the random variables used in our simulations. For
elementary simulations such as the simple probability experiment just described, the independence
of the samples Xi derives from the fact that the random numbers generated are independent. In each
group of ﬁve random numbers, all are independent of each other and are independent of all other
random numbers generated either before or after these ﬁve.
Independence of the samples becomes more of a problem in complex simulations and it is
useful to consider this question of independence in somewhat more detail as it has a considerable
effect on determining the accuracy of simulation experiments. Consider the following example for
motivational purposes. The patients at a medical facility are becoming increasingly irritated at the
lengths of time they spend waiting to see a doctor, to the point that some are threatening to move
elsewhere. The facility manager decides to carry out some simulations to assess the situation in a
hurry: to order an actual survey would take many weeks, possibly months and the fear is that the
patients may have gone elsewhere by then. Since patients log in their time of arrival and doctors log
the time at which they see the patients, past records may be analyzed to determine the parameters
needed for the simulation. It is decided that the random variable Xi will represent the waiting time
of the ith patient to arrive in the ofﬁce. The manager realizes that her conﬁdence intervals will be
meaningless unless the Xi are independent—but alas, this is not the case! If patient i spends a long
time waiting, because an emergency caused the doctor to get behind, then it is very likely that patient
i + 1 will also have to wait a long time. The random variables Xi and Xi+1 are correlated. There is
yet another effect that must also be considered. When the medical facility ﬁrst opens in the morning,
the facility is empty and the early scheduled patients wait little, if at all. Only after about an hour

708
Simulation Measurements and Accuracy
or so does the ofﬁce settle into its regular pattern of behavior, and it is this pattern that the ofﬁce
manager wishes to simulate. If these problems can be solved then, with some certainty, the ofﬁce
manager can assert that the situation will improve—if additional doctors are incorporated into the
practice, or if fewer new patients are accepted, and so on. The system can be accurately simulated
and the ofﬁce manager can have conﬁdence in the results produced by the simulation analysis.
Intimately related to this question of independence is the duration and initial state of the
simulation experiment. Two different types of simulation must be distinguished. The ﬁrst, called
a transient or terminating simulation, is one in which the simulated period is well deﬁned and
relatively short: consequently the state in which the system is started plays an important part in its
evolution. The second, called a steady state or nonterminating simulation, is the opposite. There is
no ﬁxed beginning instant, other than the beginning must occur only after the initial conditions have
faded, nor is there an ending instant. We provide a number of examples of the two types.
Example 20.6 A bank opens its doors at 8:00 a.m. At that time, all the tellers and managers are
available and ready to assist customers. The bank closes at 5:00 p.m. and any customers present at
that time are served and depart. A simulation study might wish to ﬁnd the average time a customer
waits in a queue before reaching a teller/manager. This is an example of the ﬁrst type. There is a
well deﬁned beginning and end, and the initial conditions play a role.
Example 20.7 In modeling the survivability of an electronic unit under adverse conditions, the
various components that constitute the unit are taken to be new (initial condition) and the simulation
continues until the unit fails. Although the ending time is not known in advance, there is a well
speciﬁed terminating event, whose occurrence triggers the end of the experiment. This also is an
example of transient type simulation.
Example 20.8 Nonterminating simulations occur in the analysis of processes that are taken to
be permanent or ongoing, such as the emergency room at a hospital, the ﬂow of data over a
communication network, the operation of the computer system at a military installation, and so
on. In these instances, there is no speciﬁc moment at which a start or end can be identiﬁed. This can
pose a problem for simulations, because all simulations must begin from some initial point.
Transient simulations are fairly straightforward. Independent observations (or samples) can only
be obtained by running the simulation many times, using exactly the same initial conditions each
time and terminating at the same time instant or stopping event. This is called the method of
independent replications. Since the duration of the simulation should be relatively short, it is
possible to conduct many replications. Different suites of random numbers must be used in each
replication so care must be taken to properly seed random number generators (since running with
the same seed would just produce the same estimation). Let N be the number of simulation runs
conducted and assume that on each of these N runs, n samples are obtained. Denote the kth such
sample obtained during the ith simulation run as Xik. On simulation run i, i = 1, 2, . . . , N, a point
estimator ¯Xi may be obtained from Xik, k = 1, 2, . . . , n. This means that each point estimator
¯Xi will be formed from samples that are possibly correlated. However the point estimators ¯Xi are
independent, because the simulation runs from which they are derived are independent. Thus a ﬁnal
point estimator, formed from these as
¯¯X = 1
N
N

i=1
¯Xi
where
¯Xi = 1
n
n

k=1
Xik,
may be constructed. Furthermore, since the ¯Xi are independent, they may also be used to form the
sample variance and conﬁdence intervals as described in the previous sections.
Steady state simulations are generally more computationally intensive than transient simulations
and therein lies their problem. Simulation experiments of complex systems must be initiated with the

20.2 Simulation and the Independence Criteria
709
system in some particular state. During the course of the simulation the system changes state until it
reaches some sort of steady state or equilibrium situation and it is often the behavior of the system
at equilibrium that is of interest to the modelers. This means that the simulation results obtained
up to this equilibrium point should be discarded. Determining when this so-called transient phase
has ended is generally not easy to compute. One possibility is to conduct the simulation experiment
for such a long time that the simulated values obtained during this initial period are completely
swamped by later values. However, this can be rather expensive. A better alternative is to monitor
the values until they appear to settle into some sort of regular pattern. The values obtained up to this
point are then omitted from all future analysis. For example, if samples Xi, i = 1, 2, . . . , n, are
generated and the effects of the initial state are apparent in the ﬁrst k of them, then the sample mean
should be taken to be ¯X = n
k+1 Xi/(n−k). In all cases, if some information is available concerning
the equilibrium state of the system, that information should guide the choice of initial conditions. For
example, if a heavily loaded system is to be simulated, it makes more sense to initialize the system
with a large load, rather than a light (or empty) load since in this case, the simulation will reach its
equilibrium point more quickly—there is frequently a tendency to initialize queueing systems with
empty queues and wait for them to ﬁll up!
A second more serious problem with steady-state simulations is the (very) long time over which
they must be run. If we can assume that the determination of the end of the transient period can
be detected, then the problem of the independence of the individual Xi during the simulation of
the stationary phase arises. One possibility is to use the method of independent replication which
we saw was suitable for transient simulations. However, it has a major drawback. Each simulation
run requires the elimination of the transient portion and this can be expensive, particularly if the
transient period is long. This elimination of the transient phase must be done for each of the N
simulation runs before the N values of ¯Xi are obtained. To offset this somewhat, when a certain
ﬁxed total number of samples is to be generated, it is generally recommended to keep the number
of simulation runs (N) small, of the order of 10 or so, and the number of samples in each run (n)
large, rather than the other way around.
The second approach we consider is referred to as the method of batch means. The basic idea
is to perform just one simulation, remove the transient phase just for this one run and divide the
remainder of the simulation into a set of N partial simulations or batches. If the objective of the
simulation relates to some measure of customer statistics, where the concept of customer is taken
in the broadest possible sense, then the length of each batch should be determined by the length
of time needed until a ﬁxed number of customers, the same ﬁxed number for all batches, have
been processed. Otherwise the length (timed duration) of each batch should be exactly the same.
The logic behind the idea of batching is that although the Xi are correlated, this correlation is
most strong with values of X j for which j is close to i, and becomes weaker the further that j
moves away from i. Thus Xi−2, Xi−1, Xi+1 and Xi+2 may be strongly correlated with Xi, but the
correlation between Xi and Xi+100 is likely to be negligible. Each of these batch simulations is
analyzed as if it were a complete simulation, minus the transient period. Estimators derived from
the batches will be approximately independent, so long as the batch lengths are sufﬁciently long.
If L is the number of samples that are needed so that the correlation between Xi and Xi+L is
negligible, then it is recommended that the length of a batch should be at least 5L. In this case, one
can be relatively certain that estimators obtained from non-adjacent batches will be independent. As
for adjacent batches, only samples at the end of the ﬁrst and at the beginning of the second will be
correlated. But since the batch lengths are long, estimators constructed for the ﬁrst batch will mostly
be from samples that are uncorrelated from the majority of samples in the second so even estimators
constructed from samples of adjacent batches will be approximately independent. The procedure to
follow then resembles that developed for the method of independent replications. A sample mean
is constructed for each batch. These are taken to be independent and from them, an overall sample
mean, overall sample variance and conﬁdence intervals may be constructed. This approach has the

710
Simulation Measurements and Accuracy
obvious drawback that the partial simulations will not be independent, although they are assumed
to be so. Despite this drawback, the method of batch means is the approach usually adopted.
It is possible to monitor the degree of correlation among the Xi and to take action whenever this
is deemed to be excessive. As seen in Chapter 5, the covariance between two random variables X1
with mean μ1 and variance σ 2
1 and X2 with mean μ2 and variance σ 2
2 is deﬁned as
Cov(X1, X2) = E[(X1 −μ1)(X2 −μ2)] = E[X1X2] −μ1μ2.
Since the covariance can take any value between −∞and +∞, it is frequently replaced with the
correlation, deﬁned as
ρ = Corr(X1, X2) = Cov(X1, X2)
σ1σ2
and it follows that −1 ≤Corr(X1, X2) ≤1. A sequence of possibly dependent random variables
X1, X2, . . . all having the same distribution (mean μ and variance σ 2), which is the case of interest
to us in simulation, is referred to as a time series. In a time series, the covariance and correlation
become the autocovariance and autocorrelation. In particular, Cov(Xi, Xi+k) and Corr(Xi, Xi+k) are
called the lag-k autocovariance and lag-k autocorrelation. When these depends only on k and not on
i, the time series is said to be covariance stationary and in this case we write
γk = Cov(Xi, Xi+k),
ρk = Corr(Xi, Xi+k).
When ρk = 0, the random variables are not autocorrelated; the autocorrelation increases as ρk
moves from zero. When ρk > 0, the time series is positively autocorrelated: as a general rule, large
observations are followed by large observations and small one by small ones. When ρk < 0 the
opposite effect occurs: large observations tend to be followed by small observations and vice versa.
It is recommended that a test be performed to check the size of the lag-1 autocorrelation of batch
means. If N is the number of batches, ¯Xi the sample mean computed in batch i and ¯X the overall
sample mean, then the lag-1 autocorrelation may be estimated as
˜ρ1 =
N−1
i=1 ( ¯Xi −¯X)( ¯Xi+1 −¯X)
N
i=1( ¯Xi −¯X)2
.
If this is small, the problem of autocorrelation can be dismissed. On the other hand, in the presence
of autocorrelation, conﬁdence intervals computed from the usual sample variance will likely be in
error. However, it is possible to generate a more accurate estimate of the sample variance, and hence
a more meaningful conﬁdence interval, using the autocorrelation functions. After a fair amount of
algebra, it may be shown that the sample variance in the presence of autocorrelation is given by
Var( ¯X) = 1
n2
n

i=1
n

j=1
Cov(Xi, X j) = γ0
n + 2
n2
(n−1

k=1
(n −k)γk
)
,
(20.9)
where γ0 = σ 2. The γk can be estimated from the generated samples. When the length of the batches
is long, only a small number of γk need actually be formed. In this case, the summation in Equation
(20.9) contains only a few terms, say m, and the approximation becomes
Var( ¯X) ≈˜γ0
n + 2
n
( m

k=1
˜γk
)
where ˜γk are the approximations to γk obtained from using the samples. This variance may now be
used with the Student T -distribution to compute conﬁdence intervals.
The problem of eliminating the transient portion of a steady-state simulation run is non-existent
in the case of a regenerative process. As the name implies, a regenerative process is one in which
the system “regenerates” itself from time to time, essentially beginning again from scratch. The

20.3 Variance Reduction Methods
711
lengths of time between consecutive regeneration instants are called regeneration cycles and random
variables deﬁned in one regeneration cycle have the highly desirable property of being independent
from their counterparts in other cycles as well as being distributed identically to them. The evolution
of the system depends only on the state occupied at a regeneration instant and must be the same for
all regeneration instants.
Example 20.9 In a G/G/1 queueing system, arrival instants which initiate a busy period are
regeneration instants. These are the instants in which an arrival ﬁnds the system empty. In a G/M/1
queue, in which service times are exponentially distributed, each arrival instant is a regeneration
instant while in an M/G/1 queue, each departure instant is a regeneration instant.
In the simulation context, each regeneration cycle constitutes one batch and the method of batch
means becomes the regenerative method: successive batches coincide with consecutive regeneration
cycles and there is no need to ﬁrst remove a supposed transient period. Statistics gathered from
one batch will be independent of, and identical to, those gathered from any other batch. Thus the
regenerative method removes two problems: how to handle the transient portion of a steady-state
simulation and how to ensure the independence of samples. Unfortunately, the regenerative method
also has a problem. What might be taken as a point estimator, turns out to be biased and resulting
conﬁdence intervals might be seriously in error. If Li is the duration of the ith regeneration cycle
and Yi a statistic gathered during this cycle, then the point estimator ¯Y/ ¯L is biased. Since the mean
of a ratio is not equal to the ratio of the means
E
& ¯Y
¯L
'
̸= E[ ¯Y]
E[ ¯L] = E[Yi]
E[Li].
Some procedures have been devised to combat this problem, such as the Jackknife method [52].
However, we shall defer from exploring this further since opportunities for the application of the
regenerative approach in simulation are limited. Even systems that possess regeneration properties
frequently suffer from the drawback that the regeneration cycles, although ﬁnite, are frequently
long, too long to be considered as batches.
20.3 Variance Reduction Methods
Simulations require the generation of a great many uniformly distributed random numbers and,
from them, random numbers satisfying different probability distributions. One cost-effective way
to reduce this number is to reduce the variance of parameter estimators, for, as we have already
seen, halving the variance results in a fourfold decrease in the sample size needed to satisfy a given
conﬁdence interval. A number of possibilities arise and in this section, we consider just two of them,
namely antithetic variables and control variables. Other possibilities include stratiﬁed sampling,
importance sampling and conditioning on expectations. Readers seeking additional information
would do well to consult the text by Law [29] and/or the text by Ross [46]. The amount of extra
computation involved in implementing a variance reduction procedure is usually minimal, but the
decrease in variance can seldom be gauged in advance, unfortunately.
20.3.1 Antithetic Variables
The sample mean ¯X is computed from a set of n observations X1, X2, . . . , Xn, as
¯X = 1
n
n

i=1
Xi.

712
Simulation Measurements and Accuracy
All n random variables Xi are identically distributed with mean μ and variance σ 2. For any two X1
and X2 (we choose 1 and 2 for ease of notation)
E
& X1 + X2
2
'
= E[X1] + E[X2]
2
= E[X]
and
Var
& X1 + X2
2
'
= 1
4 [Var[X1] + Var[X2] + 2 Cov(X1, X2)]
= Var[X] + Cov(X1, X2)
2
,
since Var[X1] = Var[X2] = Var[X]. Here Cov(X1, X2) is the covariance and is equal to
E[X1X2] −μ2 when X1 and X2 have the same mean value μ. Since the covariance can be any
positive or negative real number, it would be advantageous if X1 and X2 were negatively correlated,
rather than being independent, since in this case the variance is reduced. The method of antithetic
variables is an attempt to introduce negative correlation into the samples. One simple approach is
to manipulate the sequence of uniformly distributed random numbers. Let us make the dependence
of X1 on the sequence of random numbers explicit by writing X1(u1, u2, . . . , uk) thereby indicating
that X1 is a function of k uniformly distributed random numbers u1, u2, . . . , uk. The same is true
for X2 which we now write as X2(v1, v2, . . . , vk) where v1, v2, . . . , vk is a different sequence of k
uniformly distributed random numbers. The role played by ui, for i = 1, 2, . . . , k, in generating X1
is identical to the role played by vi in generating X2. If we choose vi = 1 −ui, then since ui is a
uniformly distributed random number, so too is vi = 1 −ui but now both ui and vi are negatively
correlated. The hope, and expectation, is that X1 is now negatively correlated with X2. The beneﬁts
are twofold: a possible reduction in the variance which entails a reduction in the number of samples
needed and an additional reduction in the number of uniformly distributed random numbers needed,
since we use the sequence 1 −u1, 1 −u2, . . . , 1 −uk instead of generating a new sequence of k
random numbers.
The concept extends in a straightforward manner to the case of n observations. Let us assume
that n is even and equal to 2m and let U j
= {u j1, u j2, . . . , u jk},
j
= 1, 2, . . . , m, be m
independent sets each consisting of k uniformly distributed random numbers. Each of these m sets
is used to generate a sample: let U j be the set that generates X j, which we write as X j(U j). Let
Vj, j = 1, 2, . . . , m be another m independent sets of k uniformly distributed random numbers
but having the property that U j and Vj, j = 1, 2, . . . , m are dependent (even though the elements
of the sequence {U j, j = 1, 2, . . . , m} are independent of each other, as are the elements of the
sequence {Vj, j = 1, 2, . . . , m}). These latter m sequences are used to generate the remaining m
samples Xm+ j(Vj), for j = 1, 2, . . . , m. Let
X† = 1
m
m

j=1
X j(U j)
and
X‡ = 1
m
m

j=1
Xm+ j(Vj).
Then
¯X = X† + X‡
2
is an unbiased estimator for the sample mean and the sample variance is
Var[ ¯X] = 1
4

Var[X†] + Var[X‡] + 2 Cov(X†, X‡)
 
As before, the objective is to choose the sequences U j and Vj, j = 1, 2, . . . , m so that X† and X‡
are negatively correlated. Random variables that accomplish this are said to be antithetic variables.

20.3 Variance Reduction Methods
713
One possibility is to assign the element v ji of set Vj, the value 1 −u ji with the hope that replacing
small values in U with large values in V and vice versa leads to the desired negative correlation in
X† and X‡.
Example 20.10 Congestion in queueing systems increases as the arrival rate increases and the
service rate decreases. On the other hand, congestion decreases when the arrival rate decreases and
the service rate increases. Thus we should expect the antithetic variables approach to work well
when random numbers that give rise to samples with large congestion values are combined with
random numbers that generate samples of low congestion. When a low-valued sample is paired
with a high-valued sample, both are negatively correlated and their average will be closer to their
common mean, i.e., there will be less variance.
Example 20.11 An alternative approach to replacing a uniformly distributed random number u
used in generating one sample with 1 −u in an opposing sample that can be effective in queueing
situations is as follows. The sequence of random numbers used to generate arrival rates in one set of
samples can be used to generate service rates in an opposing set. This works in general since arrival
rates and service rates work in opposition to each other. Large arrival rates (and short service rates)
mean large numbers of customers present; large service rates (and short arrival rates) means few
customers present.
20.3.2 Control Variables
The idea of control variables is to obtain a second set of identically distributed samples, Yi, i =
1, 2, . . . , n such that Yi is correlated with Xi and for which E[Yi] = ν, is known. The correlation
may be either positive or negative and the more strongly correlated, the better. Leaving aside for
the moment the problem of how ν is known, the value of ¯Y = n
i=1 Yi/n can be used to modify
¯X depending upon whether ¯Y is greater than or less than ν. Suppose that ¯Y and ¯X are positively
correlated, so that a large value of ¯Y implies that ¯X is large and a small value of ¯Y implies a small
value of ¯X. Then, if during the course of a simulation, it is found that ¯Y > ν, which we can check
since we know ν, then we should also expect that ¯X will be greater than its mean μ, since ¯X is
positively correlated with ¯Y. In this case ¯X should be adjusted downwards. Similarly, if ¯Y is lower
than its mean, we can assume that ¯X is also less than its mean value and should be adjusted upwards.
The opposite effects take place when ¯X and ¯Y are negatively correlated. If ¯Y is greater than its mean,
we should suspect that ¯X will be less than its mean and should be adjusted upwards and vice versa.
Thus ¯Y controls the value of the ¯X and hence the name: the method of control variables.
Example 20.12 In a queueing system, we should expect that longer-than-average service times
( ¯Y > ν) lead to longer-than-average waiting times ( ¯X > μ), while shorter-than-average service
times ( ¯Y < ν) should lead to shorter-than-average waiting times ( ¯X < μ). Service times and waiting
times can be taken to be positively correlated. On the other hand, shorter-than-average interarrival
times are expected to generate longer-than-average waiting times, and vice versa, so interarrival
times and waiting times are negatively related. Control variables of this nature, chosen as differing
quantities of the system being modeled, are called concomitant variables.
We now tackle the question of the adjustment of ¯X. If ¯X is an unbiased estimator of X, then so
too is ¯Xc where, for any value of c
¯Xc = ¯X −c( ¯Y −ν).
(20.10)
¯Xc is an unbiased estimator since
E[ ¯Xc] = E[ ¯X] −c(E[ ¯Y] −E[ν]) = E[ ¯X] −c(ν −ν) = E[ ¯X] = μ.

714
Simulation Measurements and Accuracy
The variance of this quantity (see Section 5.2) is given by
Var( ¯Xc) = Var
 ¯X −c( ¯Y −ν)
 
= Var
 ¯X −c ¯Y
 
= Var
 ¯X
 
+ c2Var
 ¯Y
 
−2c Cov( ¯X, ¯Y), (20.11)
and naturally we wish to choose c to minimize this variance. Observe that the variance of ¯Xc is less
than the variance of ¯X if and only if
2c Cov( ¯X, ¯Y) > c2Var
 ¯Y
 
.
Considering the right-hand side of Equation (20.11) as a quadratic equation in c, the value of c that
minimizes the variance is given by
copt = Cov( ¯X, ¯Y)
Var
 ¯Y
 
.
Thus the minimum value of the variance of the controlled estimator is
Var
 ¯Xc
 
= Var
 ¯X
 
+

Cov( ¯X, ¯Y)
Var
 ¯Y
 
2
Var
 ¯Y
 
−2

Cov( ¯X, ¯Y)
Var
 ¯Y
 

Cov( ¯X, ¯Y)
= Var
 ¯X
 
−

Cov( ¯X, ¯Y)
 2
Var
 ¯Y
 
.
Dividing both sides by Var( ¯X) gives
Var
 ¯Xc
 
Var[ ¯X] = 1 −[Corr( ¯X, ¯Y)]2,
where Corr( ¯X, ¯Y) is the correlation between ¯X and ¯Y, which shows that if ¯X and ¯Y are at all
correlated, no matter how insigniﬁcantly, the variance of ¯Xc will be less that that of ¯X. Furthermore,
with greater correlation comes greater variance reduction. The only ﬂy in the ointment now appears:
to compute the optimum value of c we need the values of Var[ ¯Y] and Cov( ¯X, ¯Y). One possibility
is to estimate these quantities from the samples Xi and Yi, i = 1, 2, . . . , n, obtained during the
running of the simulation. Thus approximations to the covariance and variance are taken as
E
Cov( ¯X, ¯Y) =
n
i=1(Xi −¯X)(Yi −¯Y)
n −1
and
F
Var[ ¯Y] =
n
i=1(Yi −¯Y)2
n −1
,
and they allow us to compute an approximation to the optimal c:
ˆcopt =
n
i=1(Xi −¯X)(Yi −¯Y)
n
i=1(Yi −¯Y)2
.
When substituted into Equation (20.10), we should obtain a more accurate estimator, Xc. The
variance of the controlled estimator is taken as
S2
c ≡S2
¯Xc = S2
X + S2
Y −2SX,Y
n
,
where S2
X = n
i=1(Xi −¯X)2/(n −1),
S2
Y = F
Var[ ¯Y], and
SX,Y = E
Cov( ¯X, ¯Y), which allows
conﬁdence intervals to be generated.
Finally we return to the question of the expected value E[Yi] = ν which we assumed was known
right from the start. If this is not the case and the value of ν, or a close approximation to it, is not
available, then some other method must be used to ﬁnd it, perhaps even an independent simulation
study itself. When an extended sequence of complex simulations is to be performed, it may well be
appropriate to take the time and effort of running separate simulations to compute this mean value
before beginning the extended sequence of simulations proper.

20.3 Variance Reduction Methods
715
Example 20.13 Consider the simulation of an M/M/1 queue in which the mean inter-arrival time is
1/λ = 5 minutes and the mean service time is 1/μ = 4 minutes. A total of ten simulation runs are
conducted. Each begins with the system empty and continues until the instant the 200th customer
enters service. The objective is to obtain information concerning the average amount of time spent
by customers waiting in the queue (Xi). The mean service time (Yi) is also observed to act as a
control variable. The following table of results is obtained, one row for each of the ten simulation
runs.
i
Xi
Yi
1
18.79
4.15
2
17.97
3.82
3
18.54
4.09
4
9.58
3.53
5
14.51
4.01
6
11.61
3.90
7
9.87
3.78
8
16.66
3.94
9
27.09
5.24
10
22.92
4.97
We ﬁnd that ¯X = 16.7540 and ¯Y = 4.1430. We also know that E[Yi] = ν = 4 and from results
on the M/M/1 queue we have E[ ¯Xi] = (λ/μ)/(μ −λ) = 16.0. Thus both ¯X and ¯Y overestimate
their respective means which supports our contention that they are positively correlated. Let us now
compute the different variances and covariances. We have
S2
X = 1
9
10

i=1
(Xi −¯X)2 = 31.5586,
S2
Y = 1
9
10

i=1
(Yi −¯Y)2 = 0.2911,
SX,Y = 1
9
10

i=1
(Xi −¯X)(Yi −¯Y) = 2.7143.
The correlation between X and Y is found as
Corr(X, Y) = Cov(X, Y)
σXσY
=
2.7143
√
31.5586
√
.2911
= .8955.
The optimum value for c is taken to be
ˆcopt = SX,Y
S2
Y
= 9.3243,
and our ﬁnal approximation is
¯Xc = ¯X −ˆCopt( ¯Y −ν) = 16.7540 −9.3243(4.1430 −4) = 15.4207,
which is closer to the true mean, 16. Observe that the variance of the controlled estimator is given
by
S2
c = S2
X + S2
Y −2SX,Y
n
= 31.5586 + 0.2911 −2 × 2.7143
10
= 2.6421.

716
Simulation Measurements and Accuracy
20.4 Exercises
Exercise 20.1.1
Given the following 20 samples, compute the sample mean and sample variance. These
samples were obtained as exponentially distributed random numbers with mean value 1/λ = 4.0.
4.8622, 5.9821, 7.8550, 1.0610, 4.2413, 3.6297, 2.6534, 1.0022, 3.0925, 5.9598,
4.2795, 6.2796, 1.0365, 1.2278, 0.1223, 2.5259, 3.5029, 3.2223, 0.8769, 1.2340.
Exercise 20.1.2
Given the following 30 samples, compute the sample mean and sample variance. These
samples were obtained as N(15, 42) normally distributed random numbers.
17.3950,
9.3055, 15.2807, 12.7739, 13.8892, 16.6974, 14.6249, 16.8984,
7.0533, 17.8103,
18.4537, 14.8095, 11.3635, 14.2145, 19.0939, 19.8625, 19.0527, 19.7766,
8.8417, 15.0815,
16.0902, 12.0137, 13.7863, 21.4924, 17.6435, 18.8750, 14.5378, 11.5988, 11.0676, 16.9576.
Exercise 20.1.3 The management of a bank observes the duration of service provided to 20 customers. What
is the probability that the mean duration time observed by the management (the sample mean) is in error by
more than 30 seconds if the actual duration of customer service is μ = 5 minutes and the variance is σ 2 = 60
minutes? What is the probability that the sample mean is in error by more than 60 seconds? Repeat your
analysis for the case when the management observes 50 customers rather than 20.
Exercise 20.1.4 Given the set of samples in Exercise 20.1.2, estimate the number of additional samples that
are needed if we wish the standard deviation of the estimator ¯X to be (a) less than .25; (b) less than .1.
Exercise 20.1.5 Compute a 90% conﬁdence interval for the sample set of Exercise 20.1.1, ﬁrst in the case
when the true variance σ 2 = 16 is used and in the second case, when the estimated sample variance S2 is used.
Exercise 20.1.6 Compute a 90% and a 95% conﬁdence interval for the sample set of Exercise 20.1.2 using the
sample variance as an estimator for the true variance.
Exercise 20.2.1 Run the queueing network simulation, the Java program qn.java of the previous chapter and
determine approximately the value of t at which the initialization portion is reached. Does this particular
program need to be modiﬁed to remove this transient portion of a simulation run in order to correctly determine
the bottleneck station? Justify your answer.
Exercise 20.2.2 In the M/M/1 Java simulation program mm1.java, insert statements to output statistics at each
departure instant and use them to determine approximately when the transient period ends. Now modify the
program to remove this transient section from the ﬁnal results. What is the mean waiting time obtained by your
modiﬁed program when the simulation ends after N = 10,000 departures?
Exercise 20.2.3 This exercise relates to the previous one, but now you are asked to use the method of batch
means. Use six batches each containing 500 customer departures and construct the mean waiting time as the
average of the six batches. Is your answer more accurate than that of the previous question?
Exercise 20.2.4 The following sample means were obtained from the method of batch means applied to a
simulation experiment. Estimate the lag-1 autocorrelation of this data. Are the sample means correlated?
3.0602,
3.7349,
3.9410,
2.6324,
3.2106,
2.1325.
Exercise 20.3.1 A total of 10 simulation runs are conducted on a queueing model. During run i, i =
1, 2, . . . , 10, the computed mean time spend in the queue and mean service time, in seconds, are given below. A
simpliﬁed model indicates that the actual mean service time is 4 seconds. Use this information and the method
of control variables to obtain a better estimate of the mean waiting time.

20.4 Exercises
717
Number
Queue
Service
1
17.20
3.97
2
23.89
4.10
3
14.88
3.98
4
14.89
4.01
5
20.25
4.00
6
17.55
4.01
7
14.91
4.02
8
16.47
3.98
9
13.88
4.03
10
17.08
4.04
(Just for curiosity’s sake, these data were taken from a simulation of a system in which the exact queueing time
is equal to 16 seconds.)
Exercise 20.3.2 The following data concerning mean interarrival times, mean queueing times, and mean
service times have been collected from ten simulations of a queueing system. It is known that the exact arrival
rate is λ = 0.9 and the exact service rate is μ = 1.0.
Number
Arrival
Queue
Service
1
1.0895
4.5633
0.9830
2
1.0743
7.4785
1.0115
3
1.1367
10.9648
1.0231
4
1.0912
10.1538
0.9747
5
1.0804
14.7951
1.0011
6
1.0983
13.0862
0.9448
7
1.1431
5.5254
0.9382
8
1.1189
3.7098
0.9466
9
1.1699
3.3439
0.9775
10
1.1149
3.5893
0.9349
(a) Compute the sample mean and sample variance of inter-arrival times, queueing times, and service
times.
(b) Compute the covariance of arrival data and queueing data and the covariance of service data and
queueing data.
(c) For the given data, show that queueing time and service time are positively correlated and that arrival
data and queueing data are negatively correlated.
(d) Use the method of control variables to generate a better estimate of the sample mean queueing time
using the exact arrival rate.
(e) Same as part (d), but this time use the exact service rate.

This page intentionally left blank 

Appendix A: The Greek Alphabet
Alpha
A
α
Beta
B
β
Gamma

γ
Delta

δ
Epsilon
E
ϵ
Zeta
Z
ζ
Eta
H
η
Theta

θ
Iota
I
ι
Kappa
K
κ
Lambda

λ
Mu
M
μ
Nu
N
ν
Xi

ξ
Omicron
O
o
Pi
 
π
Rho
P
ρ
Sigma
!
σ
Tau
T
τ
Upsilon
ϒ
υ
Phi
	
φ
Chi
X
χ
Psi
$
ψ
Omega

ω

This page intentionally left blank 

Appendix B: Elements of Linear Algebra
In this appendix we brieﬂy present some of the fundamentals of linear algebra. Throughout the
body of this text, we freely introduced vectors and matrices when needed and implicitly assumed
familiarity with the manner in which they can be manipulated. The matrices used were all square
matrices with elements taken from the ﬁeld of reals and representing transition probabilities or
transition rates among the states of a Markov chain. The vectors were probability vectors, i.e., row
vectors whose elements lie between 0 and 1 inclusive, and whose sum is 1. In the more general
context of linear algebra, unless otherwise noted, vectors are taken to be column vectors, and
matrices need not be square nor real. We shall adopt the convention of describing probability vectors
as row vectors and all other vectors as column vectors. This will allow us to move freely within the
linear algebra world in which all vectors are column vectors and the world of applied probability in
which probability vectors are always row vectors. Vectors and matrices whose elements are from the
set of reals are said to be real vectors and matrices respectively. The set of all real matrices having
m rows and n columns is denoted by ℜm×n.
B.1 Vectors and Matrices
As a general rule, upper-case latin characters will denote matrices, and lower-case latin characters
will denote (column) vectors. We shall let A be a matrix having m rows and n columns. The element
on row i and column j of A is denoted by ai j. Similarly, let B have k rows and l columns and i j
element given by bi j. Many of the operations below are deﬁned on matrices. However, they may
also be applied to vectors, since a vector of length m is an m × 1 matrix and a row vector of length
n is a 1 × n matrix.
A square matrix A with the property that ai j = 0 for i ̸= j is called a diagonal matrix; nonzero
elements can only appear along the diagonal. If the elements of A are such that ai j = 0 for i < j,
then A is a lower triangular matrix; if ai j = 0 for i > j, then it is upper triangular. A diagonal
matrix is simultaneously upper and lower triangular. A diagonal matrix whose nonzero elements are
all equal to 1 is called the identity matrix and is denoted by I. The matrix whose elements are all
equal to zero is called the zero matrix and is denoted by 0.
The transpose of a matrix A is obtained by interchanging rows and columns. The ith row of A
becomes the ith column of AT . For example,
A =
−8.0
5.0
3.0
2.0
−4.0
2.0

,
AT =
⎛
⎝
−8.0
2.0
5.0
−4.0
3.0
2.0
⎞
⎠.
A matrix is said to be symmetric if it is equal to its transpose, i.e., if A = AT . The following matrix
is symmetric:
A =
⎛
⎝
−8.0
5.0
3.0
5.0
−9.0
4.0
3.0
4.0
−7.0
⎞
⎠.
A matrix is said to be skew-symmetric if A = −AT .
B.2 Arithmetic on Matrices
Let A ∈ℜm×n and B ∈ℜk×l be two real matrices.

722
Appendix B: Elements of Linear Algebra
Matrix Addition
Two matrices may be added only if they have the same dimensions. If m = k and n = l, then the
i jth element of the sum C = A + B is given by
ci j = ai j + bi j.
Vector Inner Product
When each element of a row vector x of length n is multiplied by the corresponding element of a
column vector y of the same length and these n products added, the result is a single number, called
the inner product of the two vectors. The product of an 1 × n matrix and a n × 1 matrix is a 1 × 1
matrix, i.e., a scalar. We have
ζ =
n

i=1
xi yi.
Vector Outer Product
When an m × 1 column vector is multiplied by a 1 × n row vector, the result is an m × n matrix,
called the outer product of a and b. Its i jth element is given by
Zi j = xi y j.
Matrix Multiplication
To form the product C = A × B, it is necessary that the number of columns of A be equal to the
number of rows of B, i.e., that n = k. In this case, the i j element of the product is
ci j =
n

ξ=1
aiξbξ j.
Notice that each element of C is formed by multiplying a row of A with a column of B, i.e., as the
inner product of a row of A with a column of B.
(Post)multiplication by a Vector x
To form the product y = Ax, the vector x must have the same number of elements as the number
of columns of A. We have
yi =
n

ξ=1
aiξ xξ.
(Pre)multiplication by a Row Vector u
To form the product v = u A, the row vector u must have the same number of elements as the
number of rows of A. The jth element of the resulting row vector is
v j =
n

ξ=1
uξaξ j.
Multiplication by a Scalar ρ
Each element of the matrix is multiplied by the scalar. The i jth element of C = ρ A is given by
ci j = ρai j.
Matrix multiplication is associative and distributive, but not commutative. For example,
(AB)C = A(BC) and
A(B + C) = AB + AC;
but, in most cases,
AB ̸= B A.

B.3 Vector and Matrix Norms
723
A notable exception to the commutative law occurs when one of the matrices is the identity matrix.
We have AI = I A = A.
Matrix Inverse
If, corresponding to a square matrix A, there exists a matrix B such that AB = I = B A, then B is
said to be the inverse of A. It is usually written as A−1, and we have
AA−1 = I = A−1A.
Notice that, from the matrix size compatibility requirements of matrix multiplication, both A and
A−1 must be square and have the same size. A square matrix that does not possess an inverse, is
said to be singular.
B.3 Vector and Matrix Norms
To characterize the magnitude of vectors and matrices we use vector and matrix norms. A norm
utilizes the magnitude of some or all of the elements of the vector or matrix to obtain a single real
number to represent the magnitude of the vector or matrix. For example, in two or three dimensional
space (vectors x in ℜ2 or ℜ3), the length of a line is computed as (x2
1 + x2
2)1/2 or (x2
1 + x2
2 + x2
3)1/2.
This is easily generalized to n dimensional vectors and deﬁnes a norm, called the Euclidean norm
or 2-norm.
This is not the only vector norm. More generally, a vector norm is any function f from ℜn into
ℜwith the following properties:
•
f (x) ≥0 for x ∈ℜn and f (x) = 0 if and only if x = 0,
•
f (x + y) ≤f (x) + f (y), x, y, ∈ℜn,
•
f (αx) = |α| f (x), α ∈ℜand x ∈ℜn.
Norms are denoted by a pair of straight lines on either side of the vector or matrix. For example, for
the Euclidean norm we have
∥x∥2 =
G
H
H
I
n

i=1
x2
i .
If the columns of a matrix A are piled on top of each other, then the matrix becomes a vector and
a matrix norm can be deﬁned accordingly. A matrix norm would therefore satisfy the same three
properties. We have
• ∥A∥≥0 for A ∈ℜm×n and ∥A∥= 0 if and only if A = 0,
• ∥A + B∥≤∥A∥+ ∥B∥, A, B ∈ℜm×n,
• ∥αA∥= |α| ∥A∥, α ∈ℜand A ∈ℜm×n.
However, this is not the usual way in which matrix norms are deﬁned. Given a vector norm ∥· ∥, a
corresponding matrix norm is deﬁned as
∥A∥= sup
!
∥Au∥: u ∈ℜn, ∥u∥= 1
"
.
Such a matrix norm is said to be subordinate to its vector norm. It is not difﬁcult to show that matrix
norms deﬁned in this fashion satisfy the three requirements listed above. In addition, subordinate
matrix norms satisfy a submultiplicative property, namely,
• ∥AB∥≤∥A∥∥B∥for A ∈ℜm×n and B ∈ℜn×l.
It is precisely because they satisfy this property that subordinate norms are usually preferred to other
matrix norms. Such norms are sometimes called natural matrix norms.

724
Appendix B: Elements of Linear Algebra
Some common vector norms (x ∈ℜn) and their corresponding subordinate matrix norms
(A ∈ℜn×n) are the following.
The 1-Norm
The vector 1-norm is simply the sum of the absolute values of the elements of the vector. The matrix
1-norm is the maximum of the sums of all the absolute values in each column of the matrix (called
the maximum column sum). We have
∥x∥1 =
n

i=1
|xi|,
∥A∥1 = max
j
 n

i=1
|ai j|

.
The ∞-Norm
The vector inﬁnity norm is the maximum of the absolute values of the elements in the vector. The
matrix inﬁnity norm, in the style given above, is the maximum row sum. We have
∥x∥∞= max
i
|xi|,
∥A∥∞= max
i
⎛
⎝
n

j=1
|ai j|
⎞
⎠.
It might have been thought that the Frobenius matrix norm, deﬁned as
∥A∥F =
G
H
H
I
n

i=1
n

j=1
|ai j|2,
would be the matrix norm subordinate to the vector 2-norm, but it is not. Indeed, the Frobenius norm
is not even a subordinate norm. Despite this, it has many useful properties, but that is outside the
scope of this text.
B.4 Vector Spaces
The set of all real vectors of length n is said to form a vector space of length n. It is denoted by ℜn.
Linear Combinations of Vectors
Let u1, u2, . . . , uk ∈ℜn be k real vectors and let α1, α2, . . . , αk be k scalars. Then
u = α1u1 + α2u2 + · · · + αkuk
is said to be a linear combination of the vectors u1, u2, . . . , uk. For example, in ℜ3, the vector v
v =
⎛
⎝
5
3
0
⎞
⎠= 2
⎛
⎝
1
0
0
⎞
⎠+ 3
⎛
⎝
1
1
0
⎞
⎠= 2u1 + 3u2
is a linear combination of the vectors u1 and u2.
Vector Subspaces
Let u1, u2, . . . , uk ∈ℜn be k real vectors. Then the set of all linear combinations of these k vectors
forms a vector subspace. Denote this subspace by U. Then U ⊂ℜn. For example, the vector

B.4 Vector Spaces
725
subspace U which consists of all linear combinations of u1 and u2, where
u1 =
⎛
⎝
1
0
0
⎞
⎠
and u2 =
⎛
⎝
1
1
0
⎞
⎠,
contains all vectors of length 3 whose third component is equal to zero.
Observe that the zero vector 0, the vector whose elements are all equal to zero, is an element of
every subspace. It is obtained when the scaler coefﬁcients of the vectors are all equal to zero. Vector
spaces and subspaces are closed under the operations of addition and scalar multiplication.
Linearly Independent Vectors
A set of vectors, u1, u2, . . . , uk is said to be linearly independent if all nontrivial linear combinations
are nonzero, i.e.,
α1u1 + α2u2 + · · · + αkuk = 0 if and only if α1 = α2 = · · · = αk = 0.
(B.1)
Otherwise the set of vectors is said to be linearly dependent. In this case, any one of the vectors
may be written as a linear combination of the others. For example, with a set of linearly dependent
vectors we may write
−α1u1 = α2u2 + · · · + αkuk
and hence
u1 = β2u2 + β3u3 + · · · + βkuk
with βi = −αi/α1 for i = 2, 3, . . . , k. For example, the vectors u1 and u2 given above are linearly
independent, but the set of three vectors
u1 =
⎛
⎝
1
0
0
⎞
⎠,
u2 =
⎛
⎝
1
1
0
⎞
⎠,
and u3 =
⎛
⎝
0
1
0
⎞
⎠
are not, since any one may be written as a linear combination of the others. We have
u1 = u2 −u3,
u2 = u1 + u3
and u3 = u2 −u1.
The Span of a Subspace
Consider any vector subspace W and a set of vectors w1, w2, . . . , wl in that subspace. If every
vector w ∈W can be written as a linear combination of this set of vectors, i.e., if there exists
coefﬁcients γ1, γ2 . . . , γl for which
w = γ1w1 + γ2w2 + · · · + γlwl,
then the set is said to span the subspace W. As an example, the three vectors
u1 =
⎛
⎝
1
0
0
⎞
⎠,
u2 =
⎛
⎝
1
1
0
⎞
⎠,
and u3 =
⎛
⎝
0
1
0
⎞
⎠
(B.2)
span the subspace U.
Basis Vectors
If a set of vectors span a vector space and are linearly independent, then this set of vectors is said to
form a basis for the space. The two properties
1.
the vectors span the vector space, and
2.
the vectors are linearly independent

726
Appendix B: Elements of Linear Algebra
are both required. Thus, the three vectors given in Equation (B.2) span U but they do not constitute
a basis for U, since they are not linearly independent. However, any two of these three are linearly
independent and do span U. Thus, u1 and u2 constitute a basis for U as do u1 and u3, and u2
and u3.
A particularly useful set of basis vectors for the space ℜn is the set of vectors e1, e2, . . . , en, all
of length n, given by
e1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
...
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, e2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
...
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, e3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
0
1
...
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
· · · ,
en =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
...
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
It is easy to see how any vector in ℜn may be written as a linear combination of this basis set.
Every vector in a vector space can be written uniquely in terms of basis vectors. In other words,
if the vectors z1, z2, . . . , zk constitute a basis for a subspace that contains a vector y, and if
y = α1z1 + α2z2 + · · · + αkzk
and
y = β1z1 + β2z2 + · · · + βkzk,
then αi = βi for i = 1, 2, . . . , k.
The number of vectors in a basis set (which must be the same for all basis) is called the dimension
of the vector space. Any set of vectors that constitutes a span for a vector space may be downsized
(by eliminating some of its members until its size is equal to the dimension of the subspace)
to yield a basis set. Similarly, any set of linearly independent vectors may be augmented to produce
a basis set.
The Rank of a Matrix
The n columns in an ℜm×n matrix A constitute a subspace of ℜm which is called the column space
of A, often referred to as the range of A. The dimension of the range of a matrix is called its rank.
Similarly, the m rows of A constitute a subspace of ℜn, called the row space of A. For each of these
two spaces we may deﬁne spans and basis sets. It turns out that the dimension of the column space
of a matrix is also equal to the dimension of its row space, from which it follows that any square
matrix with linearly independent rows must also have linearly independent columns.
B.5 Determinants
The determinant of a matrix A ∈ℜn×n is a single real number. It is deﬁned as follows. If A ∈ℜ2×2
is given by
A =
a b
c d

,
then the determinant of A, written as det(A) or |A| (we shall use both interchangeably) is the real
number
det(A) = ad −bc.
The determinant of a matrix A ∈ℜn×n that is larger than 2×2, is written in terms of the determinants
of smaller submatrices of A. The matrix obtained from A by removing its ith row and jth column
is written as Mi j and is called the ij-Minor of A. The determinant of this minor, together with a

B.5 Determinants
727
speciﬁc sign, is called a cofactor of A, and is denoted by Ai j. More speciﬁcally, the i j cofactor of
A is given by
Ai j = (−1)i+ j Mi j.
We may now deﬁne the determinant of a matrix A ∈ℜn×n. It is given by
det(A) = ai1Ai1 + ai2Ai2 + · · · + ain Ain.
(B.3)
Consider the following matrix as an example:
A =
⎛
⎝
−8.0
5.0
3.0
2.0
−4.0
2.0
1.0
6.0
−7.0
⎞
⎠.
Some of its minors are
M11 =
−4.0
2.0
6.0
−7.0

,
M22 =
−8.0
3.0
1.0
−7.0

,
and
M32 =
−8.0
3.0
2.0
2.0

.
The cofactors corresponding to these minors are
A11 = (−1)2

−4.0
2.0
6.0
−7.0
 = 28 −12 = 16,
A22 = (−1)4

−8.0
3.0
1.0
−7.0
 = 56 −3 = 53,
and
A32 = (−1)5

−8.0
3.0
2.0
2.0
 = −(16 −6) = −22.
To compute the determinant of A, we need to expand across any row (or column). Equation (B.3)
deﬁnes the expansion in terms of row i. For example, we have
det(A) = a11(−1)1+1 det(M11) + a12(−1)1+2 det(M12) + a13(−1)1+3 det(M13)
= −8

−4.0
2.0
6.0
−7.0
 −5

2.0
2.0
1.0
−7.0
 + 3

2.0
−4.0
1.0
6.0

= −8 × 16 −5 × (−16) + 3 × 16 = 0.
Thus, in this particular example, the determinant of the matrix is zero. The reader should observe
that the matrix in this example is an inﬁnitesimal generator matrix, the transition rate matrix of
a 3-state continuous-time Markov chain. It is always the case that determinants of inﬁnitesimal
generator matrices are zero. The reader may wish to verify that expanding A along any of the other
two rows, or indeed along any of the columns, produces the same result. The reader may also wish
to verify that the matrix P produced from A by discretization, i.e., by setting P = I + 0.125A, has
a determinant that is nonzero.
The determinant of a matrix has many many properties, most of which will not be needed in this
text. Indeed, the only reason that determinants are introduced at all is due to the important role they
play in deﬁning eigenvalues. This is discussed in Appendix B.7. Some properties of determinants
are given below.
1.
The determinant of a matrix changes sign when any two rows are interchanged.
2.
Adding a multiple of any row into any other row leaves the determinant unchanged.
3.
The determinant of a matrix having two identical rows is zero.
4.
The determinant of a matrix whose rows are linearly dependent is zero.

728
Appendix B: Elements of Linear Algebra
5.
The determinant of a matrix whose rows are linearly independent is nonzero.
6.
A matrix is singular if and only if its determinant is zero.
7.
The determinant of a triangular matrix is equal to the product of its diagonal elements.
8.
det(AT ) = det(A), and for two compatible matrices A and B, det(AB) = det(A) det(B).
B.6 Systems of Linear Equations
Generally our ﬁrst introduction to systems of linear equations occurs when we are given a problem
of the type,
1 apple,
1 banana and a cantaloupe cost $6.00, while
1 apple, 2 bananas and three cantaloupe cost $14.00 and
1 apple, 3 bananas and four cantaloupe cost $19.0.
How much does
a cantaloupe cost?
We quickly realize that we may manipulate these three equations with their three unknown
quantities, a, the price of an apple, b, the price of a banana, and c, the price of a cantaloupe,
a + b + c = $6.0,
a + 2b + 3c = $14.0,
a + 3b + 4c = $19.0,
to obtain a simpler system consisting of only two equations in two unknowns. Subtracting the ﬁrst
equation from the second and from the third gives the simpler system
b + 2c = 8.0,
2b + 3c = 13.0.
Now, on subtracting twice the ﬁrst of these from the second, it becomes obvious that the price of a
cantaloupe is $3. Furthermore, it is now easy to determine the values of the other unknowns. Since
b + 2c = 8.0 and c = 3, it follows that b = 2 and since a + b + c = 6, using b = 2 and c = 3 we
must have a = 1.0.
The three equations
a + b + c = 6.0,
a + 2b + 3c = 14.0,
a + 3b + 4c = 19.0
may be written in matrix form as
⎛
⎝
1
1
1
1
2
3
1
3
4
⎞
⎠
⎛
⎝
a
b
c
⎞
⎠=
⎛
⎝
6.0
14.0
19.0
⎞
⎠.
If we denote the 3 × 3 matrix by A, the vector containing the unknowns by x, and the right-hand
side vector by b, we have
Ax = b.
(B.4)
The matrix A is called the coefﬁcient matrix. The operations applied to this example reduced it to
⎛
⎝
1
1
1
0
1
2
0
0
−1
⎞
⎠
⎛
⎝
a
b
c
⎞
⎠=
⎛
⎝
6.0
8.0
−3.0
⎞
⎠.
The coefﬁcient matrix in the resulting system of equations is upper triangular and the three
unknowns are then obtained by backward substitution.

B.6 Systems of Linear Equations
729
The reduction of a coefﬁcient matrix to triangular form is achieved by means of elementary row
operations. Any two rows may be interchanged and a multiple of any row may be added into any
other row. A matrix derived from another by means of a sequence of elementary row operations is
completely equivalent to the original matrix; equivalent in the sense that none of the information
content is lost. The same elementary row operations must also be applied to the elements of the
right-hand side vector. The matrix and right-hand side vector taken together is called the augmented
system.
In our example, the matrix A has an inverse. It is given by
A−1 =
⎛
⎝
1.0
1.0
−1.0
1.0
−3.0
2.0
−1.0
2.0
−1.0
⎞
⎠.
Multiplying both sides of Equation (B.4) on the left by A−1 yields
A−1 Ax = A−1b and hence x = A−1b.
For the particular example considered above, we have
A−1b =
⎛
⎝
1.0
1.0
−1.0
1.0
−3.0
2.0
−1.0
2.0
−1.0
⎞
⎠
⎛
⎝
6.0
14.0
19.0
⎞
⎠=
⎛
⎝
1.0
2.0
3.0
⎞
⎠.
Not all systems of linear equations have a solution. If the second equation in the example is
replaced by 2a + 2b + 2c = 12, then no solution can be found. In this case the second equation is
redundant because it is just equal to twice the ﬁrst. It does not provide us with any information that
we cannot obtain from the ﬁrst equation. If we now proceed as we did before, to add a multiple of the
ﬁrst into the second with the goal of eliminating one of the unknowns, the result is to eliminate all
of the unknowns from the second equation. Each element in the second row of the matrix becomes
zero. In this case, the rows of the matrix are not linearly independent, its determinant is zero and the
matrix is singular.
It is, however, possible to derive some information from the ﬁrst and third equations, since these
two are linearly independent. Given only that

1
1
1
1
3
4
 ⎛
⎝
a
b
c
⎞
⎠=

6.0
19.0

,
we may assert that if the cost of an apple is $1.00 (essentially we are adding in a third equation,
1a = 1.00) then the cost of a banana is $2.0 and the cost of a cantaloupe is $3.0, or again, if the
cost of an apple is $5.00, then the cost of a banana is $10.0 and the cost of a cantaloupe is $15.0.
In reality, all we can do is to state the relative values of the three unknowns: a banana costs twice
as much as an apple while a cantaloupe costs three times as much. There is no unique solution to
this system of equations: any nonzero scalar multiple of a solution is also a solution. In a system
of linear equations with n unknowns and only r < n linearly independent equations, n −r of the
unknowns can be assigned random values and the remaining unknowns obtained in terms of these
assigned values. In the example described above, n −r = 1.
One other case is important for us, the case when the right-hand side vector in Equation (B.4)
is zero. It is from such a system of linear equation that we compute the stationary distribution of a
continuous-time Markov chain. There are two possibilities:
1. If the coefﬁcient matrix A is nonsingular, then it must follow that the only solution to Ax = 0
is x = 0. If A is nonsingular, its determinant is nonzero and its n columns form a set
of n linearly independent vectors. Let these columns be denoted by c1, c2, . . . , cn and let
the n unknowns of the vector x be denoted by ξ1, ξ2, . . . , ξn. Then Ax represents a linear

730
Appendix B: Elements of Linear Algebra
combination of the rows of A. We have
Ax = ξ1c1 + ξ2c2 + · · · + ξncn.
However, from Equation (B.1), we know that n vectors are linearly independent if and only
if all nontrivial linear combinations are nonzero. In other words,
Ax = ξ1c1 + ξ2c2 + · · · + ξncn = 0
if and only if ξ1 = ξ2 = · · · = ξn = 0. Thus, for nonsingular A, the only solution to Ax = 0
is x = 0.
2. On the other hand, if A is singular, then the number of linearly independent rows of A is
strictly less that n. Let the number of linearly independent rows be given by r. In this case,
random values may be assigned to n −r of the unknowns and the remaining r unknowns
computed as a function of these values. There is no unique solution.
To summarize, the system of equations Ax = 0 has a nontrivial solution if and only if A is singular.
B.6.1 Gaussian Elimination and LU Decompositions
Gaussian Elimination
Gaussian elimination (GE) is the standard algorithm for solving systems of linear equations, and
due to its importance we now proceed to a detailed description of this method applied to the system
Ax = b in which A is nonsingular and b ̸= 0. Writing this system in full, we have
a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1,
a21x1 + a22x2 + a23x3 + · · · + a2nxn = b2,
a31x1 + a32x2 + a33x3 + · · · + a3nxn = b3,
...
... ...
an1x1 + an2x2 + an3x3 + · · · + annxn = bn,
i.e., a system of n linear equations in n unknowns. The ﬁrst step in Gaussian elimination is to
use one of these equations to eliminate one of the unknowns in the other n −1 equations. This is
accomplished by adding a multiple of one row into the other rows; the particular multiple is chosen
to zero out the coefﬁcient of the unknown to be eliminated. The particular equation chosen is called
the pivotal equation and the diagonal element in this equation is called the pivot. The equations that
are modiﬁed are said to be reduced. The operations of multiplying one row by a scalar and adding
or subtracting two rows are elementary operations; they leave the system of equations invariant. For
instance, using the ﬁrst equation to eliminate x1 from each of the other equations gives the reduced
system
a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1,
0 + a′
22x2 + a′
23x3 + · · · + a′
2nxn = b′
2,
0 + a′
32x2 + a′
33x3 + · · · + a′
3nxn = b′
3,
(B.5)
...
... ...
0 + a′
n2x2 + a′
n3x3 + · · · + a′
nnxn = b′
n.
The ﬁrst coefﬁcient in the second row, a21, becomes zero when we multiply the ﬁrst row by −a21/a11
and then add the ﬁrst row into the second row. The other coefﬁcients in the second row, and the right-
hand side, are affected by this operation: a2 j becomes a′
2 j = a2 j −a1 j/a11 for j = 2, 3, . . . , n and
b2 becomes b′
2 = b2 −b1/a11. Similar effects occur in the others rows; row 1 must be multiplied
by −a31/a11 and added into row 3 to eliminate a31, and in general, to eliminate the coefﬁcient

B.6 Systems of Linear Equations
731
ai1 in column i, row 1 must be multiplied by −ai1/a11 and added into row i. When this has been
completed for all rows i = 2, 3, . . . , n, the ﬁrst step of Gaussian elimination is completed and the
system of equations is that of Equation (B.5).
When the ﬁrst step of Gaussian elimination is ﬁnished, one equation involves all n unknowns
while the other n −1 equations involve only n −1 unknowns. These n −1 equations may be treated
independently of the ﬁrst. They constitute a system of n −1 linear equations in n −1 unknowns,
which we may now solve using Gaussian elimination. We use one of them to eliminate an unknown
in the n −2 others. If we choose the ﬁrst of the n −1 equations as the next pivotal equation and use
it to eliminate x2 from the other n −2 equations, we obtain
a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1,
0 + a′
22x2 + a′
23x3 + · · · + a′
2nxn = b′
2,
0 +
0 + a′′
33x3 + · · · + a′′
3nxn = b′′
3,
(B.6)
...
...
...
0 + 0 + a′′
n3x3 + · · · + a′′
nnxn = b′′
n.
We now have one equation in n unknowns, one equation in n −1 unknowns, and n −2 equations in
n −2 unknowns. If we continue this procedure, we would hope to end up with one equation in one
unknown, two equations in two unknowns, and so on up to n equations in n unknowns:
a11x1 + a12x2 + a13x3 +
· · ·
+ a1nxn = b1,
0 + a′
22x2 + a′
23x3 +
· · ·
+ a′
2nxn = b′
2,
0 +
0 + a′′
33x3 +
· · ·
+ a′′
3nxn = b′′
3,
...
¯an−1,n−1xn−1 + ¯an−1,nxn = ¯bn−1,
¯annxn = ¯bn.
At this stage the so-called elimination phase or reduction phase of Gaussian elimination is
ﬁnished and we proceed to the backsubstitution phase during which the values of the unknowns
are computed. Given one equation in one unknown (the last equation in the reduced system), the
value of that unknown is easily computed. With ¯annxn = ¯bn, we compute xn = ¯bn/¯ann. Since we
now know xn, the value of xn−1 can be found from the penultimate equation in the reduced system:
¯an−1,n−1xn−1 + ¯an−1,nxn = ¯bn−1.
We have
xn−1 =
¯bn−1 −¯an−1,nxn
¯an−1,n−1
.
The process of backsubstitution continues in this fashion until all the unknowns have been evaluated.
The computationally intensive part of Gaussian elimination is generally the reduction phase. The
number of numerical operations for reduction is of the order n3/3, whereas the complexity of the
backsubstitution phase is only n2. At this point it is worthwhile doing an example in full.
Example B.1 Consider the following system of four linear equation in four unknowns:
−4.0x1 + 4.0x2 + 0.0x3 + 0.0x4 = 0.0,
1.0x1 −9.0x2 + 1.0x3 + 0.0x4 = 0.0,
2.0x1 + 2.0x2 −3.0x3 + 5.0x4 = 0.0,
0.0x1 + 2.0x2 + 0.0x3 + 0.0xn = 2.0.

732
Appendix B: Elements of Linear Algebra
In the more convenient matrix representation this becomes
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
0.0
2.0
0.0
0.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0.0
0.0
0.0
2.0
⎞
⎟
⎟
⎠.
We are now ready to begin the reduction phase. This will consist of three steps. In the ﬁrst we shall
eliminate the unknown x1 from rows 2, 3, and 4; in the second we shall eliminate x2 from rows 3
and 4; and in the last, we shall eliminate x3 from row 4. These are shown below at the beginning of
each step. To the left we indicate the multiplier, the value by which the pivot row must be multiplied
prior to being added into the row to be reduced. Notice that, in this example, the right-hand side does
not change. This is simply because the only nonzero element is the last one, so adding multiples of
lower-indexed elements to higher-indexed elements will not change the right-hand side. In other
cases, the right-hand side may need to be modiﬁed. In many texts, the right-hand side is appended
to the matrix A to give an n × (n + 1) array, called the augmented matrix. All the elementary
operations are carried out on the augmented matrix.
Reduction Phase
Multipliers
0.25
0.50
0.00

⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
0.0
2.0
0.0
0.0
⎞
⎟
⎟
⎠=⇒
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0 −8.0
1.0
0.0
0.0
4.0
−3.0
5.0
0.0
2.0
0.0
0.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0.0
0.0
0.0
2.0
⎞
⎟
⎟
⎠,
Multipliers
0.50
0.25

⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0 −8.0
1.0
0.0
0.0
4.0
−3.0
5.0
0.0
2.0
0.0
0.0
⎞
⎟
⎟
⎠=⇒
⎛
⎜
⎜
⎝
−4.0
4.0
0.00
0.0
0.0
−8.0
1.00
0.0
0.0
0.0 −2.50
5.0
0.0
0.0
0.25
0.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0.0
0.0
0.0
2.0
⎞
⎟
⎟
⎠,
Multipliers
0.10

⎛
⎜
⎜
⎝
−4.0
4.0
0.00
0.0
0.0
−8.0
1.00
0.0
0.0
0.0 −2.50
5.0
0.0
0.0
0.25
0.0
⎞
⎟
⎟
⎠=⇒
⎛
⎜
⎜
⎝
−4.0
4.0
0.0 0.0
0.0
−8.0
1.0 0.0
0.0
0.0
−2.5 5.0
0.0
0.0
0.0 0.5
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0.0
0.0
0.0
2.0
⎞
⎟
⎟
⎠.
At the end of these three steps, the matrix has the following upper triangular form and we are now
ready to begin the backsubstitution phase:
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.5
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0.0
0.0
0.0
2.0
⎞
⎟
⎟
⎠.
Backsubstitution Phase
In the backsubstitution phase, we compute the components of the solution in reverse order, using
the ith equation to compute the value of xi. The values of previously computed components,
x j, j = i + 1, i + 2, . . . , n, are substituted into the ith equation, so the only remaining unknown is
xi itself. We have the following:
Equation 4 :
0.5 x4 = 2
=⇒
x4 = 4,
Equation 3 :
−2.5 x3 + 5 × 4 = 0
=⇒
x3 = 8,
Equation 2 :
−8 x2 + 1 × 8 = 0
=⇒
x2 = 1,
Equation 1 :
−4 x1 + 4 × 1 = 0
=⇒
x1 = 1.
Therefore the complete solution is xT = (1, 1, 8, 4).

B.6 Systems of Linear Equations
733
Observe in this example that the multipliers do not exceed 1.0. This is a desirable property for
it lessens the likelihood of rounding error build-up. In this example the multipliers are less than 1
because at each step, the pivot element is larger than the elements to be reduced. Implementations
of Gaussian elimination usually incorporate a pivoting strategy. Most often the chosen strategy is
partial pivoting whereby at step i of the algorithm, the row with the largest element in magnitude in
the unreduced part of column i is chosen as the pivotal equation. A more costly version is complete
pivoting whereby the largest element in magnitude in the entire unreduced portion of the matrix is
chosen to be the pivot. Explicit pivoting is generally not needed for solving Markov chain problems,
since the elements along the diagonal are already the largest in magnitude in each column and this
property is maintained during the reduction phase.
LU Decompositions
There exists an interesting relationship between the multipliers and the upper triangular matrix
that results from Gaussian elimination. Let U be this upper triangular matrix and deﬁne a lower
triangular matrix L whose diagonal elements are all equal to 1 and whose subdiagonal elements are
the negated multipliers.
Example B.2 Returning to the previous example, we ﬁnd
L =
⎛
⎜
⎜
⎝
1.00
0.00
0.0
0.0
−0.25
1.00
0.0
0.0
−0.50
−0.50
1.0
0.0
0.00
−0.25
−0.1
1.0
⎞
⎟
⎟
⎠,
U =
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.5
⎞
⎟
⎟
⎠,
and the product of these two triangular matrices is the original matrix A, i.e., LU = A:
⎛
⎜
⎜
⎝
1.00
0.00
0.0
0.0
−0.25
1.00
0.0
0.0
−0.50
−0.50
1.0
0.0
0.00
−0.25
−0.1
1.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.5
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
1.0
−9.0
1.0
0.0
2.0
2.0
−3.0
5.0
0.0
2.0
0.0
0.0
⎞
⎟
⎟
⎠.
When a matrix A can be written as the product of a lower triangular matrix L and an upper
triangular matrix U, these two triangular matrices are said to constitute an LU decomposition or LU
factorization of A. If A is nonsingular, then the factors L and U must both be nonsingular, as is
the case in Example B.2. Given a system of equations Ax = b and an LU decomposition of A, the
solution x may be computed by means of a forward substitution step on L followed by a backward
substitution step on U. For example, suppose we are required to solve Ax = b with det(A) ̸= 0 and
b ̸= 0 and suppose further that the decomposition A = LU is available. We have
Ax = LUx = b.
Let z = Ux. Then the solution x is obtained in two easy steps:
Forward substitution: Solve Lz = b for z;
Backward substitution: Solve Ux = z for x.
In other words, the vector z may be obtained by forward substitution on Lz = b, since both L and
b are known quantities. Subsequently, the solution x may be obtained from Ux = z by backward
substitution since by this time both U and z are known quantities. When we implement Gaussian
elimination, the ﬁrst of these two operations is carried out automatically. The solution z that results
from the forward substitution step is just equal to the right hand side after the elementary row
operations are performed.

734
Appendix B: Elements of Linear Algebra
Example B.3 Solving Lz = b (by forward substitution) for z:
⎛
⎜
⎜
⎝
1.00
0.00
0.0
0.0
−0.25
1.00
0.0
0.0
−0.50
−0.50
1.0
0.0
0.00
−0.25
−0.1
1.0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
z1
z2
z3
z4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
2
⎞
⎟
⎟
⎠
successively gives z1 = 0, z2 = 0, z3 = 0, and z4 = 2.
Now solving Ux = z (by backward substitution) for x:
⎛
⎜
⎜
⎝
−4.0
4.0
0.0
0.0
0.0
−8.0
1.0
0.0
0.0
0.0
−2.5
5.0
0.0
0.0
0.0
0.5
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
0
0
0
2
⎞
⎟
⎟
⎠
successively gives x4 = 4, x3 = 8, x2 = 1, and x1 = 1.
An LU decomposition, when it exists, is not unique. The particular one that arises out of
Gaussian elimination, as shown above, is called the Doolittle decomposition, and is characterized by
ones along the diagonal of L. Computing an LU decomposition involves determining the n(n+1)/2
elements in the lower triangular part of L and the n(n + 1)/2 elements in the upper triangular part
of U. To obtain these n(n + 1) elements we have only a total of n2 equations; the inner product of
the ith row of L with the jth column of U must be equal to ai j. With n2 equations and n(n + 1)
unknowns, we have n more unknowns than equations which means that we may assign values
to n of the unknowns and solve the n2 equations to determine the remaining n2 unknowns. It
makes sense to do this assignment in such a way as to make the computation of the remaining
n2 unknowns as easy as possible. In the Doolitle decomposition, the diagonal elements of L are set
to 1. Another often used approach is the Crout decomposition, which assigns the value of 1 to each
of the diagonal elements of U and solves for the remaining unknowns in L and U. As we have just
seen, an LU decomposition can be constructed during the course of Gaussian elimination. However,
when pivoting is not required, an alternative approach allows the triangular factors to be formed via
inner vector products which can be accumulated in double precision and which yields a more stable
algorithm.
B.7 Eigenvalues and Eigenvectors
Again in this section our concern is with real square matrices. Let A ∈ℜn×n and 0 ̸= x ∈ℜn and
consider the result obtained when the product Ax is formed. If the result obtained is once again the
vector x, i.e., if Ax = x, then x is said to be a right-hand eigenvector of A. If instead of x, a scalar
multiple of x is obtained, i.e., if Ax = λx for some scalar λ, then x is still a right-hand eigenvector
of A. In this case λ is said to be an eigenvalue of A and x is its corresponding right-hand eigenvector.
In the case Ax = x, the vector x is an eigenvector corresponding to a unit (λ = 1) eigenvalue. The
eigenvalue/vector problem is then to ﬁnd scalars λ and vectors x ̸= 0 which satisfy the equation
Ax = λx.
(B.7)
This may be written as
(A −λI)x = 0,
from which it is clear that a nontrivial x exists if and only if the coefﬁcient matrix A−λI is singular.
In other words, the determinant, det(A), must be zero. This provides us with a means of obtaining

B.7 Eigenvalues and Eigenvectors
735
the eigenvalues. They are precisely the values for which
|A −λI| = 0.
(B.8)
Equation (B.8) is called the characteristic equation. If we write this out in more detail, we have

a11 −λ
a12
a13
· · ·
a1n
a21
a22 −λ
a23
· · ·
a2n
...
...
...
...
...
an1
an2
an3
· · ·
ann −λ

= 0,
and it becomes apparent that this represents a polynomial of degree n in λ, the roots of which are
the eigenvalues of A. Consider the 2 × 2 case. We have

a11 −λ
a12
a21
a22 −λ
 = (a11 −λ)(a22 −λ) −a12a21 = 0,
which is the quadratic polynomial
λ2 −(a11 + a22)λ + (a11a22 −a12a21) = 0.
It has two and only two roots, which means that a 2 × 2 matrix has exactly two eigenvalues. They
are denoted by λ1 and λ2. These two roots (eigenvalues) need not be distinct (for example, the
identity matrix of order 2 has two eigenvalues both equal to 1) and even though the matrix has only
real elements, the eigenvalues can be complex, forming a complex pair. The characteristic equation
of a matrix of size 3 × 3 is a cubic polynomial with exactly three roots, and thus a 3 × 3 matrix
has exactly three eigenvalues. In general, an n × n matrix has exactly n eigenvalues, denoted by
λ1, λ2, . . . , λn. Finding the eigenvalues of a matrix thus reduces to ﬁnding the roots of a polynomial
equation, although often this is much easier said than done.
Example B.4 As an example, let us ﬁnd the eigenvalues of the matrix
A =
 8
6
−3
−1

.
Expanding the characteristic equation, we ﬁnd

(8 −λ)
6
−3
(−1 −λ)
 = (8 −λ)(−1 −λ) + 18 = λ2 −7λ + 10 = (λ −5)(λ −2) = 0.
The two eigenvalues are therefore given by λ1 = 5 and λ2 = 2.
Some important results concerning eigenvalues are the following:
• The eigenvalues of A and AT are identical.
Observe that since det(A) = det(AT ), it follows that det(A −λI) = det(AT −λI T ) =
det(AT −λI) and hence the result follows.
• The sum of the diagonal elements of a matrix, A, is called its trace and is denoted by tr(A).
The sum of the eigenvalues of A is equal to the trace of A, i.e.,
n

i=1
λi = tr(A).

736
Appendix B: Elements of Linear Algebra
• The product of the eigenvalues of a matrix A is equal to the determinant of A, i.e.,
n
i=1
λi = det(A).
The spectrum of a matrix is the set of its eigenvalues and the largest of the magnitudes of the
eigenvalues is called the spectral radius of the matrix and is denoted by ρ. We have
ρ(A) = max
s
|λs(A)|,
where λs(A) denotes the sth eigenvalue of A. The spectral radius of any matrix cannot exceed the
value of any of its natural matrix norms, ∥· ∥. We have
ρ(A) ≤||A||.
We are now able to give the matrix norm that is subordinate to the vector 2-norm. It is given by
∥A∥=
.
ρ(AT A),
i.e., the square root of the spectral radius of AT A. Finally, an important theorem for the localization
of the eigenvalues of a matrix is Gerschgorin’s theorem.
Theorem B.7.1 (Gerschgorin) Every eigenvalue of A lies in at least one of the n circles with center
aii and radius ri = n
k=1,k̸=i |aik|.
Eigenvectors are associated with eigenvalues and they must have at least one nonzero element.
To ﬁnd the eigenvector corresponding to one of the eigenvalues of a matrix A, we return to the
deﬁning equation
(A −λI)x = 0,
insert the value of λ, and solve for x. Observe that this system of equations is homogeneous (its
right-hand side is zero) and its coefﬁcient matrix is singular, since the value of λ is one of the roots
of det(A −λI). Therefore a unique solution does not exist; if x is an eigenvector corresponding to
an eigenvalue λ, then αx, for α ̸= 0, is also an eigenvector corresponding to the eigenvalue λ. This
is easily seen by multiplying each side of Equation (B.7) by α. From
Ax = λx,
which shows x to be an eigenvector corresponding to eigenvalue λ, we may write
α(Ax) = α(λx) or
A(αx) = λ(αx) for α ̸= 0,
and hence
Ay = λy
where y = αx,
which shows that y too is an eigenvector corresponding to eigenvalue λ.
Using the same example as before, let us compute the eigenvector corresponding to the
eigenvalue λ1 = 5. Substituting λ = 5 into Equation (B.7), we ﬁnd
8 −5
6
−3
−1 −5
 x1
x2

=
0
0

,
i.e.,
 3
6
−3
−6
 x1
x2

=
0
0

.

B.7 Eigenvalues and Eigenvectors
737
As expected, this gives us only a single equation to work with. We have x1 +2x2 = 0, and so, taking
x1 = 2, we ﬁnd x2 = −1. We can verify that this is indeed an eigenvector by substituting it back.
We have

8
6
−3
−1
 
2
−1

= 5

2
−1

.
The reader may wish to verify that (−1, +1)T is an eigenvector corresponding to the eigenvalue
λ2 = 2. Observe also, that the two eigenvectors are linearly independent. It turns out that
eigenvectors corresponding to distinct eigenvalues must be linearly independent.
Although an n × n matrix always has n eigenvalues, it may have fewer than n eigenvectors. This
will only happen when some of the eigenvalues are identical. For example, the matrix
A =
1
0
0
1

has eigenvalues λ1 = λ2 = 1 and two distinct eigenvectors. We have

1
0
0
1
 
1
0

= 1

1
0

and

1
0
0
1
 
0
1

= 1

0
1

.
These eigenvectors are linearly independent, even though the eigenvalues are identical. On the other
hand, the matrix
A =
1
1
0
1

also has two eigenvalues, both equal to 1, but only a single eigenvector. We have

1
1
0
1
 
1
0

= 1

1
0

but

1
1
0
1
 
0
1

̸= 1

0
1

.
No vector, other than a scalar multiple of (1, 0)T , can be found to be an eigenvector of this matrix.
This matrix is said to be defective. When the eigenvalues are all distinct, there exists a complete set
of corresponding eigenvectors, and furthermore, these eigenvectors form a basis for ℜn. Even when
the eigenvalues are not distinct, there may exist n linearly independent eigenvectors. Having distinct
eigenvalues is a sufﬁcient, but not necessary condition for the existence of n linearly independent
eigenvectors.
Now consider a matrix A ∈ℜn×n whose n eigenvalues are λ1, λ2, . . . , λn and with correspond-
ing, linearly independent, eigenvectors, x1, x2, . . . , xn. Let X be the n × n matrix whose n columns
are these n eigenvectors, i.e.,
X = (x1, x2, . . . , xn).
Notice that X is not necessarily a real matrix. Some of its columns may be complex, corresponding
to complex eigenvalues of A. Since the columns of X are linearly independent, det(X) ̸= 0 and
the inverse, X−1, of X exists. Let  be the diagonal matrix whose elements are the eigenvalues,
λ1, λ2, . . . , λn of A. Then, since Axi = λixi for i = 1, 2, . . . n, we have
AX = X.
Notice that the correct effect is obtained only if  multiplies X on the right. We may rewrite this
equation as
X−1 AX = 
which shows that A may be diagonalized. When a matrix is multiplied on the left by X−1 and on
the right by X, it is said to have undergone a similarity transformation. Advanced methods for

738
Appendix B: Elements of Linear Algebra
determining the eigenvalues of a matrix use sequences of similarity transformations to force the
matrix to approach diagonal, or tridiagonal form.
The preceding discussions centered on eigenvalues and right-hand eigenvectors. The same
discussions could have taken place using left-hand eigenvectors instead. A left-hand eigenvector
corresponding to an eigenvalue λ of A is a vector y ∈ℜn such that
yT A = λyT .
It follows that left-hand eigenvectors of A are the right-hand eigenvectors of AT since
yT A = λyT
implies that
AT y = λy.
We saw previously that the eigenvalues of A and AT are identical. However, this is not true for
the eigenvectors. Left-hand and right-hand eigenvectors corresponding to the same eigenvalue are
generally different. An exception is when the matrix is symmetric.
B.8 Eigenproperties of Decomposable, Nearly Decomposable,
and Cyclic Stochastic Matrices
B.8.1 Normal Form
A nonnegative matrix is one in which the elements are all greater than or equal to zero. A square
nonnegative matrix A is said to be decomposable if it can be brought by a symmetric permutation
of its rows and columns to the form
A =

U
0
W
V

,
(B.9)
where U and V are square, nonzero matrices and W is, in general, rectangular and nonzero. We may
relate this concept of decomposability with that of reducibility in the context of Markov chains; in
fact, the two terms are often used interchangeably. If a Markov chain is reducible, then there is an
ordering of the state space such that the transition probability matrix has the form given in (B.9). If
the matrices U and V are of order n1 and n2 respectively (n1 + n2 = n, where n is the order of A),
then the states si, i = 1, 2, . . . , n of the Markov chain may be decomposed into two nonintersecting
sets
B1 = {s1, s2, . . . , sn1}
and
B2 = {sn1+1, . . . , sn}.
Let us assume that W ̸= 0. The nonzero structure of the matrix A in Equation (B.9) shows that
transitions are possible from states of B2 to B1 but not the converse, so that if the system is at any
time in one of the states s1, . . . , sn1, it will never leave this set. If the system is initially in one of
the states of B2, it is only a question of time until it eventually ﬁnishes in B1. The set B1 is called
isolated or essential, and B2 is called transient or nonessential. Since the system eventually reduces
to B1, it is called reducible and the matrix of transition probabilities is said to be decomposable [45].
If W = 0, then A is completely decomposable, and in this case both sets, B1 and B2, are essential.
It is possible for the matrix U itself to be decomposable, in which case the set of states in B1
may be reduced to a new set whose order must be less than n1. This process of decomposition may
be continued until A is reduced to the form, called the normal form of a decomposable nonnegative

B.8 Eigenproperties of Decomposable, Nearly Decomposable,
and Cyclic Stochastic Matrices
739
matrix, given by
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
A11
0
0
· · ·
0
0
· · ·
0
0
A22
0
· · ·
0
0
· · ·
0
...
...
...
...
...
...
...
...
0
0
0
· · ·
Akk
0
· · ·
0
Ak+1,1
Ak+1,2
· · ·
· · ·
Ak+1,k
Ak+1,k+1
· · ·
0
...
...
...
...
...
...
...
...
Am,1
Am,2
· · ·
· · ·
Am,k
Am,k+1
· · ·
Am,m
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(B.10)
The submatrices Aii, i = 1, . . . , m, are square, nonzero, and nondecomposable. All submatrices to
the right of the diagonal blocks are zero, as are those to the left of Aii for i = 1, 2, . . . , k. As for
the remaining blocks, for each value of i ∈[k + 1, m] there is at least one value of j ∈[1, i −1]
for which Ai j ̸= 0. The diagonal submatrices are as follows:
•
Aii, i = 1, . . . , k, isolated and nondecomposable,
•
Aii, i = k + 1, . . . , m, transient, and again, nondecomposable.
If Bi is the set of states represented by Aii, then states of Bi for i = 1, . . . , k have the property
that once the system is in any state of Bi, it cannot move out of that set. For sets Bi, i = k+1, . . . , m,
any transition out of these sets is to one possessing a lower index only, so that the system eventually
ﬁnishes in one of the isolated sets B1, . . . , Bk. If all of the off-diagonal blocks are zero, (k = m),
then A is said to be a completely decomposable nonnegative matrix.
B.8.2 Eigenvalues of Decomposable Stochastic Matrices
Let P be a decomposable stochastic matrix of order n written in normal decomposable form as
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
P11
0
0
· · ·
0
0
· · ·
0
0
P22
0
· · ·
0
0
· · ·
0
...
...
...
...
...
...
...
...
0
0
0
· · ·
Pkk
0
· · ·
0
Pk+1,1
Pk+1,2
· · ·
· · ·
Pk+1,k
Pk+1,k+1
· · ·
0
...
...
...
...
...
...
...
...
Pm,1
Pm,2
· · ·
· · ·
Pm,k
Pm,k+1
· · ·
Pm,m
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(B.11)
The submatrices Pii, i = 1, . . . , k, are all nondecomposable stochastic matrices and thus each
possesses one and only one unit eigenvalue. The matrices Pii, i = k + 1, . . . , m, are said to be
substochastic since the sum of elements on at least one row is strictly less than unity. The Perron-
Frobenius theorem allows us to state that the spectral radius of any matrix that is substochastic is
strictly less than one. Thus, the largest eigenvalue of each of the matrices Pii, i = k + 1, . . . , m,
of (B.12) is strictly less than unity in modulus.

740
Appendix B: Elements of Linear Algebra
Since the set of eigenvalues of a block (upper or lower) triangular matrix (such as P in
Equation (B.11)) is the union of the sets of eigenvalues of the individual blocks, a decomposable
stochastic matrix of the form (B.11) has exactly as many unit eigenvalues as it has isolated subsets;
i.e., k.
Example B.5 Consider the Markov chain deﬁned diagrammatically by Figure B.1.
Figure B.1. A Markov chain.
The corresponding stochastic matrix is given by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.5
0.5
0.5
0.5
0.7
0.3
0.5
0.5
0.5
0.5
0.6
0.4
0.001
0.497
0.001
0.001
0.5
0.1
0.9
0.9
0.1
0.3
0.7
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
which has no clearly deﬁned form. However, if the rows and columns are permuted according to
New state no.
1
2
3
4
5
6
7
8
9
10
Old state no.
1
2
4
5
8
9
6
3
7
10
the matrix acquires the normal decomposable form (B.11) shown below. Clearly it may be seen that
once the system is in set B1 consisting of reordered states {1, 2, 3, 4}, or in B2 with reordered states
{5, 6}, it will never leave it. If it is initially in any other set, e.g., B3 = {7} or B4 = {8, 9, 10}, it will
in the long run leave it and move into B1 or B2. The set of eigenvalues of this matrix is
{1.0, 1.0, 0.9993, −0.8000, 0.7000, −0.2993, 0.0000, 0.0000, 0.0000, 0.0},
containing two unit eigenvalues and with the other eigenvalues having modulus strictly less than
unity. It possesses one eigenvalue that is identically zero and three that are less than 10−5 but

B.8 Eigenproperties of Decomposable, Nearly Decomposable,
and Cyclic Stochastic Matrices
741
nonzero.
Ppermuted =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.5
0.5
...
...
...
0.5
0.5
...
...
...
0.5
0.5
...
...
...
0.5
0.5
...
...
...
. . .
. . .
. . .
. . .
...
. . .
. . .
...
. . .
...
. . .
. . .
. . .
...
0.1
0.9
...
...
...
0.9
0.1
...
...
. . .
. . .
. . .
. . .
...
. . .
. . .
...
. . .
...
. . .
. . .
. . .
0.6
...
0.4
...
0.0
...
. . .
. . .
. . .
. . .
...
. . .
. . .
...
. . .
...
. . .
. . .
. . .
...
...
...
0.7
0.3
0.001
...
0.001
...
0.001
...
0.497
0.5
...
...
...
0.3
0.7
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
B.8.3 Eigenvectors of Decomposable Stochastic Matrices
Let the stochastic matrix P be decomposable into the form (B.11), and let yl, l = 1, . . . , k, be a set
of right-hand eigenvectors corresponding to the k unit eigenvalues. Then by deﬁnition
Pyl = yl,
l = 1, 2, . . . , k.
Let the yl be partitioned into m disjoint parts corresponding to the m different sets of essential and
transient states, i.e.,
yl =
#
y1
l , y2
l , . . . , yi
l , . . . , ym
l
$T .
If ni is the number of states in the ith set, then yi
l consists of elements 1+i−1
j=1 n j through i
j=1 n j
of the eigenvector yl. We have
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
P11
0
0
. . .
0
0
. . .
0
0
P22
0
. . .
0
0
. . .
0
...
...
...
...
...
...
...
...
0
0
0
. . .
Pkk
0
. . .
0
Pk+1,1
Pk+1,2
0
. . .
Pk+1,k
Pk+1,k+1
. . .
0
...
...
...
...
...
...
...
...
Pm,1
Pm,2
0
. . .
Pm,k
Pm,k+1
. . .
Pm,m
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
y1
l
y2
l
...
yk
l
yk+1
l
...
ym
l
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
y1
l
y2
l
...
yk
l
yk+1
l
...
ym
l
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
and therefore Pii yi
l
= yi
l for i = 1, 2, . . . , k, which implies that yi
l is a right eigenvector
corresponding to a unit eigenvalue of Pii. But for values of i = 1, 2, . . . , k, Pii is a nondecom-
posable stochastic matrix and has a unique unit eigenvalue. The subvector yi
l is therefore the right

742
Appendix B: Elements of Linear Algebra
eigenvector corresponding to the unique unit eigenvalue of Pii, and as such all of its elements must
be equal. But this is true for all i = 1, 2, . . . , k. This allows us to state the following theorem:
Theorem B.8.1 In any right-hand eigenvector corresponding to a unit eigenvalue of the matrix P,
the elements corresponding to states of the same essential class have identical values.
The above reasoning applies only to the states belonging to essential classes. When the states
of the Markov chain have not been ordered, it might be thought that the values of components
belonging to transient states might make the analysis more difﬁcult. However, it is possible to
determine which components correspond to transient states. These are the only states that have
zero components in all of the left-hand eigenvectors corresponding to the k unit eigenvalues. These
states must have a steady-state probability of zero. Consequently these states may be detected and
eliminated from the analysis of the right-hand eigenvectors that is given above.
Notice ﬁnally that it is possible to choose linear combinations of the right-hand eigenvectors to
construct a new set of eigenvectors y′
i, i = 1, 2, . . . , k, in which the components of eigenvector
i corresponding to states of essential set i are all equal to 1, while those corresponding to states
belonging to other essential subsets are zero.
Example B.6 Referring back to the previous example, the eigenvectors corresponding to the three
dominant eigenvalues are printed below.
Eigenvalues:
λ1 = 1.0,
λ2 = 1.0,
λ3 = 0.9993.
Right-hand eigenvectors:
y1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−.4380
−.4380
−.4380
−.4380
0.0000
0.0000
−.2628
−.2336
−.2336
−.2336
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, y2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.4521
.4521
.4521
.4521
−.1156
−.1156
.2250
.1872
.1872
.1872
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, y3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.5778
0.5765
0.5778
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Left-hand eigenvectors:
x1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−0.5
−0.5
−0.5
−0.5
0.0
0.0
0.0
0.0
0.0
0.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, x2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.0
0.0
0.0
0.0
0.7071
0.7071
0.0
0.0
0.0
0.0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, x3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.1836
0.1828
0.1836
0.1833
0.3210
0.3207
−0.0003
−0.5271
−0.3174
−0.5302
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Since 0.9993 is not exactly equal to unity, we must consider only the right-hand eigenvectors y1
and y2. In y1, the elements belonging to the essential set B2 are zero. We may readily ﬁnd a linear
combination of y1 and y2 in which the elements of the essential set B1 are zero. Depending upon the
particular software used to generate the eigenvectors, different linear combinations of the vectors

B.8 Eigenproperties of Decomposable, Nearly Decomposable,
and Cyclic Stochastic Matrices
743
may be produced. However, it is obvious from the vectors above that regardless of the combination
chosen, the result will have the property that the components related to any essential set are all
identical. One particular linear combination of interest is αy1 + βy2, with
α =
(1 −
.4521
−.1156)
−.4380
and β =
1
−.1156,
which gives αy1 + βy2 = e.
Notice that in this example, the elements of the right-hand eigenvectors corresponding to the
transient class B4 are also identical. With respect to the left-hand eigenvectors, notice that regardless
of the linear combination of x1 and x2 used, the components belonging to the two transient classes
will always be zero.
B.8.4 Nearly Decomposable Stochastic Matrices
If the matrix P given by (B.11) is now altered so that the off-diagonal submatrices that are zero
become nonzero, then the matrix is no longer decomposable, and P will have only one unit
eigenvalue. However, if these off-diagonal elements are small compared to the nonzero elements of
the diagonal submatrices, then the matrix is said to be nearly decomposable in the sense that there
are only weak interactions among the diagonal blocks. In this case there are multiple eigenvalues
close to unity.
Example B.7 In the simplest of all possible cases of completely decomposable matrices, we have
P =
1.0
0.0
0.0
1.0

.
The eigenvalues are, of course, both equal to 1.0. When off-diagonal elements ϵ1 and ϵ2 are
introduced, we have
P =

1 −ϵ1
ϵ1
ϵ2
1 −ϵ2

,
and the eigenvalues are now given by λ1 = 1.0 and λ2 = 1.0 −ϵ1 −ϵ2. As ϵ1, ϵ2 →0, the system
becomes completely decomposable, and λ2 →1.0.
In a strictly nondecomposable system a subdominant eigenvalue close to 1.0 is often indicative
of a nearly decomposable matrix. When P has the form given by (B.11), the characteristic equation
|λI −P| = 0 is given by
P(λ) = P11(λ)P22(λ) · · · Pmm(λ) = 0,
(B.12)
where Pii(λ) is the characteristic equation for the block Pii. Equation (B.12) has a root λ = 1.0
of multiplicity k. Suppose a small element ϵ is subtracted from Pii, one of the ﬁrst k blocks, i.e.,
i = 1, . . . , k, and added into one of the off-diagonal blocks in such a way that the matrix remains
stochastic. The block Pii is then strictly substochastic, and hence its largest eigenvalue is strictly
less than 1. The modiﬁed matrix now has a root λ = 1.0 of multiplicity (k −1), a root λ = 1−O(ϵ),
and (m −k) other roots of modulus < 1.0; i.e., the system is decomposable into (k −1) isolated sets.
Since the eigenvalues of a matrix are continuous functions of the elements of the matrix, it follows
that as ϵ →0, the subdominant eigenvalue tends to unity and the system reverts to its original k
isolated sets. If we consider the system represented by the previous ﬁgure, it may be observed that
the transient set B4 = {8, 9, 10} is almost isolated, and as expected, the eigenvalues include one
very close to unity. However, the existence of eigenvalues close to 1.0 is not a sufﬁcient condition
for the matrix to be nearly decomposable.
Nearly completely decomposable (NCD) Markov chains arise frequently in modelling applica-
tions [13]. In these applications the state space may be partitioned into disjoint subsets with strong

744
Appendix B: Elements of Linear Algebra
interactions among the states of a subset but with weak interactions among the subsets themselves.
Efﬁcient computational procedures exist to compute the stationary distributions of these systems
and are discussed in Chapter 10 of this text.
B.8.5 Cyclic Stochastic Matrices
In an irreducible discrete-time Markov chain, when the number of single-step transitions required
on leaving any state to return to that same state (by any path) is a multiple of some integer p > 1,
the Markov chain is said to be periodic of period p, or cyclic of index p. One of the fundamental
properties of a cyclic stochastic matrix of index p is that it is possible by a permutation of its rows
and columns to transform it to the form, called the normal cyclic form
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
P12
0
. . .
0
0
0
P23
. . .
0
...
...
...
...
...
0
0
0
. . .
Pp−1,p
Pp1
0
0
. . .
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(B.13)
in which the diagonal submatrices Pii are square, zero, and of order ni and the only nonzero
submatrices are P12, P23, . . . , Pp1. This corresponds to a partitioning of the states of the system
into p distinct subsets and an ordering imposed on the subsets. The ordering is such that once the
system is in a state of subset i, it must exit this subset in the next time transition and enter a state of
subset (i mod p) + 1. The matrix P is said to be cyclic of index p, or p-cyclic. When the states of the
Markov chain are not periodic, (i.e., p = 1), then P is said to be primitive or aperiodic or acyclic.
The matrix given in equation (B.13) possesses p eigenvalues of unit modulus. Indeed, it follows
as a direct consequence of the Perron–Frobenius theorem that a cyclic stochastic matrix of index p
possesses, among its eigenvalues, all the roots of λp −1 = 0. Thus a cyclic stochastic matrix of
• index 2 possesses eigenvalues 1, −1;
• index 3 possesses eigenvalues 1, −(1 ±
√
3i)/2;
• index 4 possesses eigenvalues 1, −1, i, −i;
• etc.
Notice that a cyclic stochastic matrix of index p has p eigenvalues equally spaced around the unit
circle. Consider what happens as p becomes large. The number of eigenvalues spaced equally on
the unit circle increases. It follows that a cyclic stochastic matrix will have eigenvalues, other than
the unit eigenvalue, that will tend to unity as p →∞. Such a matrix is not decomposable or nearly
decomposable. As pointed out previously, eigenvalues close to unity do not necessarily imply that
a Markov chain is nearly decomposable. On the other hand, a nearly completely decomposable
Markov chain must have subdominant eigenvalues close to 1.

Bibliography
[1]
A. Baker. Essentials of Padé Approximants. Academic Press, New York, 1975.
[2]
F. Baskett, K. M. Chandy, R. R. Muntz, and F. G. Palacios. Open, Closed, and Mixed Networks
of Queues with Different Classes of Customers. Journal of the ACM, Vol. 22, No. 2, pp. 248–260,
1975.
[3]
D. Bini and B. Meini. On Cyclic Reduction Applied to a Class of Toeplitz Matrices Arising in
Queueing Problems. Pages 21–38 in W. J. Stewart, editor, Computations with Markov Chains.
Kluwer Academic, Boston, Mass. 1995.
[4]
G. Bolch, S. Greiner, H. de Meer, and K. S. Trivedi. Queueing Networks and Markov Chains.
Wiley Interscience, New York, 1998.
[5]
G. E. P. Box and M. F. Muller. A Note of the Generation of Random Normal Deviates. Annals of
Mathematical Statistics, Vol. 29, pp. 610–611, 1958.
[6]
P. J. Burke. The Output of a Queueing System. Operations Research, Vol. 4, No. 6, pp. 699–704,
1956.
[7]
J. P. Buzen. Computational Algorithms for Closed Queueing Networks with Exponential Servers.
Comm. of the ACM, Vol. 16, No. 9, pp. 527–531, 1973.
[8]
K. M. Chandy. The Analysis and Solution of General Queueing Networks. Pages 224–228 in Proc.
6th Annual Princeton Conf. on Information Sciences and Systems, Princeton University Press,
Princeton, N.J., 1972.
[9]
K. M. Chandy and C. H. Sauer. Computational Algorithms for Product Form Queueing Networks.
Communication of the ACM, Vol. 23, No. 10, pp. 573–583, 1980.
[10]
J. W. Cohen. The Single Server Queue. Wiley, New York, 1969.
[11]
A. E. Conway and N. D. Georganas. A New Method for Computing the Normalization Constant
for Multiple-Chains Queueing Networks. INFOR, Vol. 24, No. 3, pp. 184–196, 1986.
[12]
A. E. Conway and N. D. Georganas. Queueing Networks—Exact Computational Algorithms. MIT
Press, Cambridge, Mass., 1989.
[13]
P-J. Courtois. Decomposablity: Queueing and Computer Systems Applications. Academic Press,
New York, 1977.
[14]
T. Dayar. Permuting Markov Chains to Nearly Completely Decomposable Form. Technical
Report BU-CEIS-9808, Department of Computer Engineering and Information Science, Bilkent
University, Ankara, Turkey, August 1998.
[15]
W. Feller. An Introduction to Probability Theory and its Applications, Vol. I and II. John Wiley and
Sons, New York, 1968.
[16]
C. W. Gear. Numerical Initial Value Problems in Ordinary Differential Equations, Prentice-Hall,
Englewood Cliffs, N.J., 1971.
[17]
W. J. Gordon and G. F. Newell. Closed Queueing Networks with Exponential Servers. Operations
Research, Vol. 15, No. 2, pp. 254–265, 1967.
[18]
W. J. Gordon and G. F. Newell. Cyclic Queueing Systems with Restricted Length Queues.
Operations Research, Vol. 15, No. 2, pp. 266–277, 1967.
[19]
D. Gross and C. M. Harris. Fundamentals of Queueing Theory, John Wiley and Sons, New York,
1974.
[20]
E. Hairer and G. Wanner. Solving Ordinary Differential Equations II: Stiff and Differential-
Algebraic Problems, Series in Computational Mathematics. Springer-Verlag, Berlin, 1991.
[21]
B. R. Haverkort. Performance of Computer Communication Systems, John Wiley and Sons,
Chichester, U.K., 1998.
[22]
J. R. Jackson. Networks of Waiting Lines. Operations Research, Vol. 5, No. 4, pp. 518–521, 1957.

746
Bibliography
[23]
J. R. Jackson. Jobshop Like Queueing Systems. Management Science, Vol. 10, No. 1, pp. 131–142,
1963.
[24]
L. Kleinrock. Queueing Systems, Vol. 1: Theory. John Wiley and Sons, New York, 1975.
[25]
D. E. Knuth. The Art of Computer Programming, Vol. 2: Seminumerical Algorithms. Third edition,
Addison-Wesley, Reading, Mass., 1998.
[26]
S. S. Lam and Y. I. Lien. A Tree Convolution Algorithm for the Solution of Queueing Networks.
Comm. of the ACM, Vol. 26, No. 3, pp. 203–215, 1983.
[27]
G. Latouche and Y. Ramaswami. A Logarithmic Reduction Algorithm for Quasi Birth and Death
Processes. J. Appl. Probab., Vol. 30, pp. 650–674, 1994.
[28]
S. S. Lavenberg and M. Resier. Stationary State Probabilities at Arrival Instants for Closed
Queueing Networks with Multiple Types of Customers. Journal of Applied Probability, Vol. 17,
No. 4, pp. 1048–1061, 1980.
[29]
A. M. Law and W. D. Kelton. Simulation Modeling and Analysis. Third edition, McGraw-Hill,
New York, 2000.
[30]
P. L’Ecuyer. Software for Uniform Random Number Generation: Distinguishing the Good and the
Bad. Pages 95–105 in Proceedings of the 2001 Winter Simulation Conference, IEEE Press, 2001.
[31]
D. H. Lehmer. Mathematical Methods in Large Scale Computing Units, Pages 141–146 in
Proceedings of the Second Symposium on Large-Scale Digital Computing Machinery, Harvard
University Press, Cambridge, Mass., 1951.
[32]
R. Marie. Calculating Equilibrium Probabilities for λ(n)/Ck/1/N Queues. ACM Sigmetrics
Performance Evaluation Review, Vol. 9, No. 2, pp. 117–125, 1980.
[33]
M. Matsumoto and T. Nishimura. Mersenne Twister: A 623-Dimensionally Equidistributed
Uniform Pseudo-Random Number Generator. ACM Transactions on Modeling and Computer
Simulation, Vol. 8, No. 1, pp 3–30, 1998.
[34]
F. Marsaglia and T. A. Bray. A Convenient Method for Generating Normal Variables. SIAM Review,
Vol. 6, No. 3, pp. 260–264, 1964.
[35]
G. Marsaglia and W. W. Tsang, The Ziggurat Method for Generating Random Variables. Journal
of Statistical Software, Vol. 5, No. 8, pp. 17, 2000.
[36]
B. Meini. New Convergence Results on Functional Techniques for the Numerical Solution of
M/G/1 Type Markov Chains. Numer. Math., Vol. 78, pp. 39–58, 1997.
[37]
C. B. Moler and C. F. Van Loan. Nineteen Dubious Ways to Compute the Exponential of a Matrix.
SIAM Review, Vol. 20, No. 4, pp. 801–836, October 1978.
[38]
R. R. Muntz. Poisson Departure Process and Queueing Networks. Pages 435–440 in Proceedings
of the 7th Annual Princeton Conference on Information Sciences and Systems, Princeton University
Press, Princeton, N.J., 1973.
[39]
M. F. Neuts. Matrix Geometric Solutions in Stochastic Models—An Algorithmic Approach. Johns
Hopkins University Press, Baltimore, MD, 1981.
[40]
M. F. Neuts. Structured Stochastic Matrices of M/G/1 Type and Their Applications. Marcel Dekker,
New York, 1989.
[41]
T. Osogami and M. Harchol-Balter. Necessary and Sufﬁcient Conditions for Representing General
Distributions by Coxians. Carnegie Mellon University Technical Report, CMU-CS-02-178, 2002.
[42]
B. Philippe and B. Sidje. Transient Solution of Markov Processes by Krylov Subspaces, Technical
Report, IRISA—Campus de Beaulieu, Rennes, France, 1993.
[43]
V. Ramaswami. A Stable Recursion for the Steady State Vector in Markov chains of M/G/1 type.
Commun. Statist. Stochastic Models, Vol. 4, pp. 183–188, 1988.
[44]
V. Ramaswami and M. F. Neuts. Some explicit formulas and computational methods for inﬁnite
server queues with phase type arrivals. J. Appl. Probab., Vol. 17, pp. 498–514, 1980.
[45]
V. I. Romanovsky. Discrete Markov Chains, Wolters-Noordhoff, Groningen, The Netherlands,
1970.
[46]
S. M. Ross. Simulation. Second edition, Academic, Press, New York, 1997.
[47]
Y. Saad. Iterative Methods for Sparse Linear Systems. Second edition, SIAM Publications,
Philadelphia, 2003.

Bibliography
747
[48]
E. B. Saff. On the Degree of the Best Rational Approximation to the Exponential Function. Journal
of Approximation Theory, Vol. 9, pp. 97–101, 1973.
[49]
K. C. Sevcik and I. Mitrani. The Distribution of Queueing Network States at Input and Output
Instants. Journal of the ACM, Vol. 28, No. 2, pp. 358–371, 1981.
[50]
W. J. Stewart. Introduction to the Numerical Solution of Markov Chains. Princeton University
Press, Princeton, N.J., 1994.
[51]
R. J. Tarjan. Depth First Search and Linear Graph Algorithms. SIAM Journal of Computing, Vol. 1,
No. 2, pp. 146–160, 1972.
[52]
J. W. Tukey. Bias and Conﬁdence in not quite Large Samples. Annals of Mathematical Statistics,
Vol. 29, page 614, 1958.
[53]
R. S. Varga. Matrix Iterative Analysis. Prentice-Hall, Englewood Cliffs, N.J., 1962.
[54]
R. C. Ward. Numerical Computation of the Matrix Exponential with Accuracy Estimate, SIAM
Journal on Numerical Analysis, Vol. 14, No. 4, pp. 600–610, 1977.

This page intentionally left blank 

Index
σ-ﬁeld/algebra, 12
k-dependent process, 200
(s,S) ordering policy, 692
Absolutely convergent integral, 91
Absorbing chain, 225
Absorbing state, 170, 194
Absorption
mean time to, 219–223, 260
probability, 219, 223–228, 260
variance of time to, 222
Accept-reject method, 621, 657–660, 670, 671
Additive congruent method, 628
Aggregation methods, 324–330
Allocation problem, 25
Alternating renewal process, 274
Antithetic variables, 711, 713
Arrival
process, 45, 386, 387
rate, 386
theorem, 582
Assignment problem, 32
Axioms of probability, 9–11
Backward Euler method, 366
Backward Gauss-Seidel, 311
Balance equations
detailed, 249
global, 239, 249, 415
local, 596, 597
Banded matrix, 297
Basis vectors, 725
Batch means, method of, 709
Bayes’ rule, 20, 21, 80
BCMP theorem, 594, 595, 598
BDF: Backward differentiation formula, 372, 373
Bernoulli
distribution, 116, 117, 122
sequences, 34
trials, 33–36
Binning, 631
Binomial
coefﬁcient, 29
distribution, 113, 117–120
theorem, 34
Birth-death process, 250, 402, 413
condition for ergodicity of, 418
matrix form of, 418
state characterization, 418
state equations, 413
steady-state solution, 415
Birthday problem, 28, 616
Block
Gauss–Seidel, 320
iterative methods, 319–323
Jacobi, 321
SOR, 321
splitting, 320
Toeplitz matrix, 347
tridiagonal matrix, 332
upper Hessenberg matrix, 345–354
Bulk arrivals, 345
Bulk queueing systems, 503
Burke’s theorem, 560–562
Buzen’s algorithm, 570, 571, 602
Load-dependent node, 573
Load-independent node, 572
Cauchy
distribution, 666
random variable, 187, 666
random variates, 667
Cauchy-Schwartz inequality, 166
Central limit theorem, 145, 184–187, 700
and normal distribution, 184
and sample mean, 185
continuity correction, 186
Central server model, 575, 592
Chain matrix, see Transition probability matrix
Chain, routing, 598
Chapman–Kolmogorov equations, 202–206,
257–259
ODE solvers for, 365
Characteristic equation, 735
Characteristic function, 108
Chebychev inequality, 91, 180–182, 700
conﬁdence intervals and, 181
Chernoff bound, 180, 182
Chi-square, 146
degrees of freedom, 631
goodness-of-ﬁt test, 630–633
random variable, 631
Closed queueing network, 568
performance measures, 577–580, 582
Coefﬁcient of variation, 89
Cofactor, 727

750
Index
Coﬁdence intervals, 704
Combinations, 25
with replacement, 31, 33
without replacement, 29, 33
Compact storage, 315
Composition method, 662
Erlang-r variates, 662, 663
hyperexponential variates, 663, 664
Concomitant variables, 713
Conditional
expectation, 98–100
probability, 12–14
variance, 98–100
Conditional failure rate, see Hazard rate
Congruent method
additive, 628
linear, 627
multiplicative, 628
Conservation law, 538
Continuity correction, 145, 186
Control variables, 713, 714, 716
Convergence
in probability, 184
testing, 317–319
Convolution, 80–82
Convolution algorithm, 571
Correlation, 96, 630, 709, 710
Counting process, 267, 388
Coupling matrix, 326
Courtois matrix, 325
Covariance, 95, 710, 712
Coxian distribution, 166–168, 597
Crout decomposition, 734
Cumulative distribution function,
46–51
conditional, 60
Cumulative hazard, 154
Cycle length, 626
Cyclic reduction, 350
Cyclic stochastic matrix
eigenvalues of, 744
De Moivre–Laplace Theorem, 144
Decomposable matrix, 738
eigenvalues of, 739
eigenvectors of, 741
normal form, 738
Decomposition methods, 324–330
Deﬁnite integrals, 613
DeMorgan’s law, 7
Density function, 51–53
wedge shaped, 56, 650, 660
Departure process, 560
Detailed balance equations, 249
Determinant
deﬁnition, 726
properties, 727
DIEHARD test suite, 644
Direct methods, 290, 291
data structure, 297
packing scheme, 297
Discrete-event simulation, 680
Discretization, 135, 138, 289, 290
Distribution function, 40
cumulative, 46–51
Distribution(s)
continuous
Coxian, 166–168
Erlang-r, 146, 158–161
Erlang-2, 155–158
exponential, 136–141
extended Erlang, 167
gamma, 145–149
hyperexponential, 164–166
hypoexponential, 162, 163
normal, 141–145
phase-type, general, 168–171
phase-type, 155
standard normal, 142
uniform, 134–136
Weibull, 149–155
discrete
Bernoulli, 116, 117, 122
binomial, 117–120
geometric, 113, 120–122
hypergeometric, 127, 128
modiﬁed geometric, 122, 123
multinomial, 128, 129
negative binomial, 123, 124, 131
Poisson, 124–127
uniform, 115, 116
Doolittle decomposition, 734
Er/M/1 queue, 450–454
via z-transform, 496–500
Effectiveness, see Performance measures
Eigenvalue/eigenvector problem, 287, 734
Embedded Markov chain, 201, 202, 259–262, 264,
509–512, 542, 546, 551
Equilibrium distribution, see Steady state distribution
Erlang
arrival model, 450–454
B formula, 434
C formula, 422
distribution, 146, 155–161
relationship with Poisson, 159
extended distribution, 167
service model, 444, 446–450

Index
751
Erlang-r random variates, 662, 663
Error norm, relative, 303
Estimator
interval, 704
point, 698
unbiased, 699
Event(s), 3–8, 46
collectively exhaustive, 7
complement of, 5
difference of two, 6
elementary, 4, 40
equiprobable, 9
independent, 15–18
intersection of, 6
list, 681
mutually exclusive, 7, 15
null or impossible, 5
sequencing, 682
space, 8, 41, 42, 59
union of, 5
universal or certain, 5
Expectation, 87–92
conditional, 98–100
law of total, 98
linearity property, 95
of a sum, 95
of functions of RVs, 92–94
of joint distributions, 94–97
properties of, 100
Expected reward, 272
Explicit Euler method, see Forward Euler
method
Exponential distribution(s), 136–141
maximum of, 109
memoryless property, 136, 138
minimum of, 110
relationship with geometric, 138
relationship with Poisson, 141
smaller of two, 140
sum of, 140
Exponential random variates
via inverse transformation, 652
via ziggurat method, 676
Exponential service time, 388
Exponentially distributed random numbers, 652, 653
Extended Erlang distribution, 167
Failure rate function, see Hazard rate
FCFS scheduling, 597
Feed forward networks, 563
FEM: Forward Euler method, 366
Fill-in, 291, 294
Finite-capacity systems, 426, 432, 434
Finite-source systems, 434
Flow-equivalent server method, 591–594
Force of mortality, see Hazard rate
Forward Gauss-Seidel, 311
Forward recurrence time, see Residual service
time
Fourier transform, 108
Frobenius matrix norm, 724
Fundamental matrix, 218–223
G/M/1 queue, 509, 546–550
waiting time distribution, 550, 551
G/M/1 type process, 333, 340
G/M/1/K queue, 551, 553
Gambler’s ruin, 229, 233–235
Gamma distribution, 145–149
relationship with chi-square, 146
relationship with Erlang, 146
relationship with exponential, 146
Gamma function, 145, 149
incomplete, 147
Gap test, 640
Gauss-Seidel method, 305, 309–311
block, 320
forward/backward, 311
Gaussian distribution, see Normal distribution
Gaussian elimination, 291, 292, 294, 295, 730
scaled, 295, 296, 299
General phase-type distributions, 463
Generating function
moment, 103–108
Laplace transform, 107
probability, 100–103
Geometric distribution, 113, 120–122
Gerschgorin’s theorem, 308, 372, 736
Global balance equations, 239, 249, 263, 595
Greek alphabet, 719
GTH method, 298–300
H2/E3/1 queue, 460–464
H2/M/1 queue, 454–457
Harwell-Boeing format, 315
Hazard
cumulative, 154
function
bathtub shape, 155
rate, 153, 154
Hessenberg
lower block matrix, 340–345
upper block matrix, 345–354
Histogram correction, see Continuity correction
Homogeneous
stochastic process, 194
system of equations, 292
Hyperexponential distribution, 164–166

752
Index
Hyperexponential random variates, 663, 664
Hypoexponential distribution, 162, 163
IAD: Iterative aggregation/disaggregation, 328
Implicit Euler method, 366
Incomplete gamma function, 147
Independent events, 15–18
Independent replications, method of, 708
inﬁnite server, 597
Inﬁnitesimal generator, 170, 255, 285, 287, 406, 418,
466
Initial approximation, 291, 316, 317
Instantaneous failure rate, 153
Intensity function, see Hazard rate
Interarrival time, 141, 386
Internal clock time, 681
Interval estimators, 704
Invariant distribution, 235
Invariant measure, 235
Inventory system, 692
Inverse function, 647
Inverse transformation method, 647–649
Bernoulli variates, 653
exponential variates, 652
triangular variates, 650, 651
uniform continuous variates, 649
wedge-shape variates, 649, 650
Inversion, see Inverse transformation method
Irreducibility, 214–218, 260
Iteration matrix, 305
Iterative methods, 290, 291, 301–306, 308–313
Jackson network, 563, 564
performance measures for, 567, 568
Jackson’s theorem, 565
Jacobi iterative method, 305, 306, 308, 309
block, 321
Jensen’s method, see Uniformization method
Joint
conditional distributions, 77–80
cumulative distribution, 64–68
conditional, 80
marginal, 67
density function(s), 71–77
conditional, 79
independence, 76
marginal, 79
marginals, 76
distribution, 67, 69
probability mass function, 68–71, 78
random variables
correlation, 96
covariance, 95
difference of, 97
expectation of, 94–97
uniformly distributed, 73
Jump chain, 259
Kendall’s notation, 396
Kolmogorov backward equations, 258
Kolmogorov forward equations, 258
Kolmogorov’s criterion, 251, 265
Kolmogorov-Smirnov test, 630, 633, 634, 640
Kronecker product, 463
L’Hôpital’s rule, 489
Lag-k
autocorrelation, 710
autocovariance, 710
Laplace random variable, 114
Laplace transform, 107, 270, 520, 523, 554
Law of large numbers
strong, 184
weak, 183, 184
LCFS-PR scheduling, 597
Life data analysis, 150
Limiting probability distribution, 237, 263
Linear congruent method, 627
Linear equations, system of, 728
Linear recurrence method, 626
List data structures, 681
Little oh, 254
Little’s law, 400, 401, 583
Load dependent service center, 564
Load independent service center, 570
Local balance equations, 596, 597
Logarithmic reduction algorithm, 335, 346, 467
Long-run distribution, see Steady state distribution
LU decomposition/factorization, 296–298, 733
M/D/1 queue, 560
M/Er/1 queue, 444, 446–450
via z-transform, 488–491
M/G/1 queue, 509–514, 561
busy period, 526–529
distribution of customers served, 530, 531
customer distribution, 518–521
embedded Markov chain, 510, 512–514
nonpreemptive priority, 533
performance measures, 515, 517, 518
preempt-resume priority, 536
queueing time distribution, 523
response time distribution, 521, 522
shortest processing time ﬁrst, 540
M/G/1 type process, 333, 345
M/G/1/K queue, 542–545
M/H2/1 queue, 454–457
M/M/∞queue, 425
M/M/1 queue, 402, 682

Index
753
average queueing time, 408
average response time, 407
busy period, 529
description, 402
distribution of queueing time, 411
distribution of response time, 409
matrix form of, 406
mean number in system, 406
mean number waiting, 407
priority scheduling, 531
steady state, 402–405
throughput, 409
trafﬁc intensity, 409
transient behavior, 412
utilization, 409
variance of number in system, 406
via z-transform, 484–486
M/M/1 queues in tandem, 562
M/M/1/K queue, 426, 428
performance measures, 428, 429
M/M/c queue, 419–421
performance measures, 421–425
M/M/c//M queue, 434–437
M/M/c/K queue, 432, 434
Machine breakdown model, 428
Machine repairman problem, 436, 689
Marginal distribution, 67
Marie’s method, 594
Markov chain, 170, 193, 194
k-dependent, 200, 201
block lower Hessenberg, 340–345
continuous-time, 253–262
transient distributions of, 356
cyclic, see periodic
cyclic classes, 243
discrete-time, 195–206
transient distribution of, 355
embedded, 201, 202, 259–262, 264, 509–512, 542,
546, 551
ergodic, 212
evolution of, 196
holding time, 201
homogeneous, 196, 253
irreducible, 215
irreducible and
aperiodic, 241
ergodic, 242
null-recurrent, 240
periodic, 243
positive-recurrent, 241
transient, 240
nonhomogeneous, 196, 253, 365
periodic, 243
positive-recurrent, 229, 346
reducible, 215
reversible, 249
sojourn time, 201
tree-structured, 250
truncated, 252
Markov inequality, 180, 181
Markov matrix, 195
Markov model
Ehrnefest, 198
nonhomogeneous, 198
social mobility, 197
weather, 196
Markov process, 193, 194
Markov property, 138, 193–195
Markov, Andrei Andreevich, 193
Matlab code for
block Gauss-Seidel, 323
Gaussian elimination, 300
GTH, 301
IAD method, 331
Jacobi, Gauss-Seidel & SOR, 313
M/H2/1 queue, 458
ODE solvers, 373
Ph/Ph/1 queues, 465, 469
QBD processes, 339
Scaled Gaussian elimination, 296
Matrix
arithmetic, 721
cofactor, 727
data structures, 313–316
decomposable, 738
defective, 737
deﬁnitions, 721
exponential, 258, 355
inverse, 723
minor, 726
nonnegative, 738
rank, 726
scaling/powering, 357, 359, 360
spectrum, 736
splitting, 305
squaring/powering, 305
subordinate norm, 723
Matrix analytic method, 332, 333, 345–354
Matrix geometric method, 332–336, 338–345, 444,
447, 458
Maximum of independent RVs, 108–110
Mean
ﬁrst passage time, 211
recurrence time, 209
time to absorption, 219–223, 260
value, 87
Measure theory, 11
Median, 87

754
Index
Memoryless property, 136, 138, 194
Mersene twister, 629
MGR32k3a generator, 629
Middle square method, 625
Mimicry, 654
Minimum of independent RVs, 108–110
Minor, of a matrix, 726
Mixed queueing network, 595
Mode, 87
Modiﬁed Euler method, 367
Modiﬁed geometric distribution, 122, 123
Moment generating function, 103–108
Laplace transform, and, 107
linear translation, 107
properties of, 107
Moments, 87–92
central, 88
factorial, 102
second, 88
Monte Carlo integration, 622
Monte Carlo simulation, 680
Multiserver systems, 419–421, 432, 434
Multiclass queueing networks, 594–598
Multinomial
coefﬁcient, 31, 36
distribution, 128, 129
Multiple-node system, 559
Multiplicative congruent method, 628
Multiplicative recursive generators, 628
MVA: Mean value analysis, 582
load-dependent servers, 586–591
load-independent servers, 582–585
NCD: Nearly completely decomposable, 304, 324
Nearly decomposable matrix, 743
Nearly separate, see NCD
Nearly uncoupled, see NCD
Negative binomial distribution, 123, 124, 131
Networks of queues, see Queueing network
Neuts’
matrix geometric method, 444
rate matrix, 447, 458
NIST test suite, 644
Nonhomogeneous Markov chain, 365
Norm
1-norm, 724
inﬁnity, 724
vector & matrix, 723
Normal distribution, 141–145
and the central limit theorem, 145
approximation to binomial, 144
continuity correction, 145
generating function, 143
Normal form
of periodic Markov chain, 243
of transition probability matrix, 218
Normal random variates
via accept-reject method, 670, 671
via central limit theorem, 670
via partitioning, 673
via polar coordinates method, 672
via ziggurat method, 674–676
Normalization, 303, 304, 317, 319
Normalization constant, 569, 584
Normally distributed random numbers,
670–673
Norton’s theorem, 591
ODE solvers, 365–369
Open queueing networks, 563
Order of convergence, 369
Padé approximants, 358–360
diagonal, 359
for eX, 360
Paradox of residual life, 526
Parameter space, 194
Partitioning
for random variates, 664–666
PASTA, 394, 512
Performance measures, 398
Permutations, 25
with replacement, 26, 33
without replacement, 27, 33
Perron–Frobenius theorem, 288
Ph/Ph/1 queue, 460, 464, 465
performance measures, 468
stability conditions, 466, 467
Phase type distribution(s), 155, 444
Coxian, 166–168
Erlang-r, 158–161
Erlang-2, 155–158
extended Erlang, 167
ﬁtting means and variances to, 171–176
general, 168–171
hyperexponential, 164–166
hypoexponential, 162, 163
Pigeon hole problem, 28
Pivot, 730
Pivoting strategy, 298
Point estimators, 698
Poisson arrivals, 388, 394
Poisson distribution, 113, 124–127
as limiting case of binomial, 126
Poisson process, 388, 389, 391, 392
Poisson streams
superposition/decomposition of, 393, 394
Poker test, 641

Index
755
Polar coordinates method, 672
Pollaczek–Khintchine
mean value formula, 515, 517,
518, 523
transform equation 1, 518–521
transform equation 2, 522
transform equation 3, 523
Pollaczek-Khintchine formula, 560
Positive recurrent, 209
Positive recurrent Markov chain, 346
Potential matrix, 218–223
Power method, 301–305, 313
convergence properties, 303
rate of convergence, 303, 304
power set, 11
Priority scheduling, 531
nonpreemptive, 533
preempt-resume, 536
Probability
axiomatic approach to, 10
axioms, 9–11
conditional, 12–14
frequency approach to, 10
generating function, 100–103
law of total, 18–20
limit theorems, 180
prior, 12
space, 11
Probability bounds, 180
Chebychev inequality, 180–182
Chernoff, 180, 182
Markov inequality, 180, 181
Probability density function, 51–53
conditional, 60
Probability distribution, 235–247,
262–265
limiting, 237, 263
of reducible Markov chains, 246
stationary, 235, 263
steady-state, 238, 263
transient, 235, 262
unique stationary, 237
Probability distribution function see Cumulative
distribution function,
Probability mass function, 43–46
conditional, 58
Processor sharing scheduling, 597
Product form, 416, 595, 597
Product-form queueing networks, 595
Projection methods, 291
Pseudorandom numbers, 625
Pure birth process, 414
QBD: quasi-birth-death process, 332–336, 338–340,
444
condition for ergodicity, 336
Queueing network, 559–562, 591–594
closed, 568
open, 563
Race condition, 256
Ramaswami’s formula, 347
RAND Corporation, 625
Random arrivals, 392
Random numbers, 613
arbitrarily distributed, 653
continuous uniform, 649
exponentially distributed, 56, 652,
653
linear recurrence, 626
normally distributed, 670–673
uniform, 613–620, 622, 623, 625
uniformly distributed, 56
via mimicry, 654
Random observer, 394
Random observer property, 566
Random sample, 182
Random variable(s), 40
characteristic function, 108
coefﬁcient of variation, 89
conditioned, 58–60
continuous, 40–43
Cauchy, 187
Coxian, 166–168
Erlang, 45
Erlang-r, 146, 158–161
Erlang-2, 155–158
exponential, 45, 49, 136–141
gamma, 145–149
hyperexponential, 164–166
hypoexponential, 162, 163
Laplace, 114
normal, 141–145
normal, sum of, 144
Rayleigh, 62
standard normal, 142
uniform, 134–136, 155
Weibull, 149–155
derived, 53
discrete, 40–43, 47
Bernoulli, 45, 116, 117, 122
binomial, 45, 117–120
geometric, 86, 120–122
hypergeometric, 127, 128
modiﬁed geometric, 122, 123

756
Index
discrete (continued)
multinomial, 128, 129
negative binomial, 123, 124, 131
Poisson, 45, 81, 124–127, 132
Poisson, sum of, 81
uniform, 115, 116
expectation, 87–92
family of, 45
functions of, 53–58
expectation of, 92–94
independent, 100
maximum of, 108–110
minimum of, 108–110
sum of, 80–82, 106
mean value, 87
median, 87
mode, 87
moments, 87–92
shifted, 55
squared coefﬁcient of variation, 89
standard deviation, 89
uncorrelated, 100
uniform continuous, 49
variance, 89
Random variates
having Bernoulli distribution, 653
having binomial distribution, 654, 655
having geometric distribution, 655, 656
having lognormal distribution, 660, 662
having Poisson distribution, 656, 657
having triangular density, 650–652
having wedge-shape density, 649, 650
via composition, 662
via partitioning, 664–666
via Ziggurat method, 673–676
Random vector, 68
Random walk(s), 228–232
Randomization method, see Uniformization method
Rank of a matrix, 726
RANMAR, 629
Rate of convergence, 290
Reachability matrix, 212, 218, 223–228
Recurrent process, 267
Redundancy, 151
Regeneration points, 511
Regenerative process, 710
Region of convergence, 103
Rejection method, see Accept-reject method
Relative error norm, 303
Reliability, 150
components in parallel, 150, 151
diminishing returns, 152
components in series, 150
Reliability function, 150, 154
Reliability modeling, 35, 149–155
Remaining service time, see Residual service time
Renewal
density, 271
equation, 272
function, 269
fundamental equation, 270
process
alternating, 274
expected rate of, 271
limiting behavior, 270
rate of, 271
reward process, 272
sequence, 268
Renewal instant, 511
Renewal process, 267–274, 388
Replacement policy, 273
Residual service time, 523
Residual service time, 524–526
Residual testing, 319
Reversed process, 248
Reversibility, 248–253, 265
Reversible
Markov chain, 249
process, 248
Rounding error, 370
Routing chain, 598
Row-wise packing, 315
Run test, 634
distribution of runs, 638
number of runs, 636
runs above/below mean, 637
Run-down, 635
Run-up, 635
Runge-Kutta methods, 370, 371
Sample mean, 185, 698
Sample path, 196, 199, 203, 267
Sample set/size, 697
Sample space, 3, 42
equivalent, 5
partition, 8, 41
Sampling, 697
bias, 698
variability, 698
Scaled Gaussian elimination, 295, 296, 299
Scheduling
disciplines, 395, 396
FCFS: ﬁrst-come, ﬁrst-served, 396, 597,
599, 600
FIFO, see FCFS
IS: Inﬁnite server, 396, 597, 599, 600

Index
757
LCFS-PR: LCFS Preempt resume, 396, 597, 599,
600
LCFS: last-come ﬁrst-served, 396
policy, 386
preemptive policies, 395
PRIO: priority, 396
PS: processor sharing, 396, 597, 599, 600
RR: round robin, 396
SIRO: service in random order, 396
Selection
with replacement, 25
without replacement, 25
Selection problem, 25
Self loop, 196, 253
Semi-Markov process, 265–267
Serial test, 632
Service type, 387
Shortest job next, see Shortest processing time ﬁrst
Shortest processing time ﬁrst, 540
Sigma ﬁeld/algebra, 12
Simulation
discrete event, 680
independence criteria, 707
Monte Carlo, 680
of conditional probabilties, 618, 619
of deﬁnite integrals, 620, 622, 623
of mean and variances, 619, 620
of probability experiments, 613–617
steady state, 708
synchronous/asynchronous, 682
terminating/transient, 708
Simulation models
inventory system, 692–695
M/M/1 queue, 682–686
machine repairman problem, 689–692
network of queues, 686–689
Simulation run, 682
Simulation time, 681
Skip-free process, 402
Sojourn time, 201, 256, 266
SOR: successive overrelaxation method,
312, 313
block, 321
convergence conditions, 312
Span of a subspace, 725
Spectral radius, 287, 736
Spectrum, 287
Splitting, 305
block, 320
Squared coefﬁcient of variation, 89
Standard deviation, 89
Standard normal distribution, 142
State dependent service, 437, 438
State descriptor vector, 680
State(s)
absorbing, 207
accessible, see reachable
aperiodic, 212, 217
classiﬁcation, 206
closed subset of, 214
communicating, 216
communicating class of, 216
cyclic, see periodic
ephemeral, 207
ergodic, 212, 217
ﬁrst passage, 211
ﬁrst return to, 207
mean ﬁrst passage, 211
mean recurrence time, 209
nonreturn, 216
null-recurrent, 207, 209, 212, 217
periodic, 212
positive-recurrent, 207, 209, 212, 217
reachable, 215
recurrent, 206, 208, 260
recurrent nonnull, see positive-recurrent
transient, 206, 207, 209, 217, 260
Stationary
probability distribution, 229, 235, 263
stochastic process, 194
Statistic, 699
Statistical test suites, 644
Steady state probability distribution, 238, 263
Steady-state simulation, 708
Stein-Rosenberg theorem, 310
Sterling numbers, 642
Stochastic matrix, 195, 287–289
numerically, 351
Stochastic process, 193, 194
Strong law of large numbers, 184
Student T-distribution, 706
Substochastic, 220
Successive substitution, 335, 447, 467
superset, 11
Supplementary variables, 510
Survivor function, 150
Symmetric structure, 251
Taylor series methods, 369
Throughput, 400
Time series, 710
Tolerance criterion, 291
Topological structure, 559
Total expectation, 98
Total probability, 18–20, 79, 80
Trafﬁc equations, 564, 599
Trafﬁc intensity, 400
Transient probability distribution, 235, 262, 354–357

758
Index
Transient simulation, 708
Transition probabilities, 195, 254
single-step, 195
Transition probability matrix, 195
m-step, 204
normal form of, 218
single-step, 204
Transition rate matrix, 170, 254, 255
Transition rates, 254, 255
Trapezoid rule, 367
Tree-structured Markov chain, 250
Trial, 3
Triangle density function, 650, 660
Truncated Markov chain, 252
Truncation error, 370
Uniform distribution
continuous, 134–136
discrete, 115, 116
Uniformization method, 361, 362, 364, 365
Utilization, 399
Variance, 89
conditional, 98–100
of a sum, 95
properties of, 100
Variance reduction methods, 711
antithetic variables, 711, 713
control variables, 713, 714, 716
Vector
norm, 723
space, 724
subspace, 724
Visit ratio, 243, 569, 599
Wall clock time, 681
Weak law of large numbers, 183, 184
Wedge, 56, 74
Wedge-shaped density function, 649, 660
Weibull distribution, 149–155
z-transform, 101, 475–483
and bulk queues, 503
and difference equations, 475
and factorial moments, 476
and Markovian queues, 484
deﬁnition of, 475
Er/M/1 queue, 496–500
for Markovian queues, 475
inversion of, 478
M/Er/1 queue, 488–491
M/M/1 queue, 484–486
partial fraction expansion of, 478–483
transform pairs, 478, 483
Ziggurat method, 673–676
for exponential variates, 676
for normally distributed variates, 674–676

