Probabilistic graphical models and algorithms for genomic analysis
by
Poe Xing
B.S. (Tsinghua University) 1993
M.S. (Rutgers University) 1998
Ph.D. (Rutgers University) 1999
A dissertation submitted in partial satisfaction
of the requirements for the degree of
Doctor of Philosophy
in
Computer Science
in the
GRADUATE DIVISION
of the
UNIVERSITY OF CALIFORNIA, BERKELEY
Committee in charge:
Professor Richard Karp, co-Chair
Professor Michael Jordan, co-Chair
Professor Stuart Russell, co-Chair
Professor Gene Myers
Professor Terry Speed
Fall 2004

The dissertation of Poe Xing is approved:
co-Chair
Date
co-Chair
Date
co-Chair
Date
Date
Date
University of California, Berkeley
Fall 2004

Probabilistic graphical models and algorithms for genomic analysis
Copyright c⃝2004
by
Poe Xing

Abstract
Probabilistic graphical models and algorithms for genomic analysis
by
Poe Xing
Doctor of Philosophy in Computer Science
University of California, Berkeley
Professor Richard Karp, co-Chair
Professor Michael Jordan, co-Chair
Professor Stuart Russell, co-Chair
In this thesis, I discuss two probabilistic modeling problems arising in metazoan genomic anal-
ysis: identifying motifs and cis-regulatory modules (CRMs) from transcriptional regulatory se-
quences, and inferring haplotypes from genotypes of single nucleotide polymorphisms. Motif and
CRM identiﬁcation is important for understanding the gene regulatory network underlying meta-
zoan development and functioning. I discuss a modular Bayesian model that captures rich structural
characteristics of the transcriptional regulatory sequences and supports a variety of motif detection
tasks. Haplotype inference is essential for the understanding of genetic variation within and among
populations, with important applications to the genetic analysis of disease propensities. I discuss a
Bayesian model based on a prior distribution constructed from a Dirichlet process – a nonparamet-
ric prior which provides control over the size of the unknown pool of population haplotypes, and
on a likelihood function that allows statistical errors in the haplotype/genotype relationship. Our
models use the “probabilistic graphical model” formalism, a formalism that exploits the conjoined
capabilities of graph theory and probability theory to build complex models out of simpler pieces.
I discuss the mathematical underpinnings for the models, how they formally incorporate biolog-
ical prior knowledge about the data, and I present a generalized mean ﬁeld theory and a generic
algorithm for approximate inference on such models.
1

co-Chair
Date
co-Chair
Date
co-Chair
Date
2

Dedicate to my wife — Wei
and
to my parents
for encouraging me
to pursue my dream
and
for sharing my joy and frustration
in this endeavor
i

Acknowledgements
I wish to thank my advisers at Berkeley, Richard Karp, Michael Jordan and Stuart Russell, for
their kindness, patience, and cooperativeness in working as a “dream team” along with me to make
possible a smooth transformation for me from a novice to a professional in computer science during
the past ﬁve years, and for giving me so much freedom to discover and explore new subjects in ma-
chine learning, statistics and computational biology. I thank Richard Karp for his generous support,
invaluable trust, inspiring discussions and insightful suggestions on my research in computational
biology, and for being a great friend and a source of encouragement and understanding. I thank
Michael Jordan for his great patience and unparalleled technical guidance early in my development,
and his inspiration, enthusiasm and encouragement on my research in machine learning. I am also
greatly indebted to Stuart Russell, for sharing with me his wisdom and humor, his insightful cri-
tiques and stimulating ideas, and for his extensive technical and moral support on my research and
career development. A Ph.D. under their mentorship is the experience of a lifetime.
My other committee members have also been very supportive. Gene Myers has been a warm
supporter on my endeavor in computational biology, and a source of new problems, new ideas and
objective opinions from the non-machine-learning community. Terry Speed inspired my interest
in statistical genetics, and has also brought lots of useful outsider’s perspective to the thesis. In
particular, it was in writing a term paper for one of his classes that I worked out the ﬁrst piece of
this dissertation — the new motif detection model.
I would like to thank my many friends and colleagues at Berkeley with whom I have had
the pleasure of working over the years. These include Eyal Amir, David Blei, Nando de Freitas,
Bhaskara Marthi, Brian Milch, Erik Miller, Kevin Murphy, Andrew Ng, Xuanlong Nguyen, Mark
Paskin, Matthias Seeger, Yee-Whye Teh, Martin Wainwright, Yair Weiss, Andy Zimdars, Alice
Zheng, and all the members of the SAIL and RUGS groups. Their encouragement and friendship
and their help have brought me incredible joy during my Berkeley days. I particularly want to thank
Brian Milch for his critical reading of this thesis, which greatly improved its clarity and readability
ii

(although any remaining errors in the thesis are of course my fault).
I would also like to thank Eric Horvitz, Tanveer Syeda-Mahmood and Jeonghee Yi for hiring
me as an intern at Microsoft Research in 2001, as a consultant at IBM Research in 2001, and
as an instructor at IBM Research during 2001-2002, respectively, during which I gained valuable
experience in industrial R&D and in advanced teaching.
I want to extent my gratitude to other friends in Soda hall, on campus, and beyond for friendship
and support. Finally, I wound like to thank my wife Wei for bearing with my countless weekend
and late night stays in the ofﬁce, for listening to my wild ideas and endless details, and for her
encouragement on my work with love, patience and understanding.
iii

Contents
1
Introduction
1
1.1
Genomic Analysis and the Graphical Model Approach
. . . . . . . . . . . . . . .
2
1.1.1
The Architecture and Function of the Genome . . . . . . . . . . . . . . . .
2
1.1.2
The Populational Diversity and Evolution of the Genome . . . . . . . . . .
4
1.1.3
Probabilistic Graphical Models and Genomic Analysis . . . . . . . . . . .
7
1.2
Thesis Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.2.1
The Problem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.2.2
Contributions of This Thesis . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.2.3
Importance for Bioinformatics, Computer Science and Statistics . . . . . .
14
1.3
Technical Results of This Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.3.1
A Modular Parametric Bayesian Model for Transcriptional Regulatory Se-
quences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.3.1.1
Proﬁle Bayesian models for motif sequence pattern
. . . . . . .
18
1.3.1.2
Bayesian HMM for motif organization . . . . . . . . . . . . . .
20
1.3.1.3
The LOGOS model . . . . . . . . . . . . . . . . . . . . . . . .
21
1.3.2
A Non-Parametric Bayesian Model for Single Nucleotide Polymorphisms .
22
1.3.3
The Generalized Mean Field Algorithms for Variational Inference . . . . .
24
1.4
Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2
Modeling Transcriptional Regulatory Sequences for Motif Detection
iv

28
2.1
Biological Foundations and Motivations . . . . . . . . . . . . . . . . . . . . . . .
29
2.2
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2.1
Motif Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2.2
Computational Tasks for In Silico Motif Detection
. . . . . . . . . . . . .
35
2.2.3
General Setting and Notation . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.2.4
The LOGOS Framework: a Modular Formulation
. . . . . . . . . . . . .
38
2.3
An Overview of Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.3.1
Background Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.3.1.1
The models
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.3.1.2
The use of background models
. . . . . . . . . . . . . . . . . .
41
2.3.2
Local Models — for the Consensus and Stochasticity of Motif Sites . . . .
43
2.3.2.1
Product multinomial model . . . . . . . . . . . . . . . . . . . .
43
2.3.2.2
Constrained PM models . . . . . . . . . . . . . . . . . . . . . .
44
2.3.2.3
Motif Bayesian networks . . . . . . . . . . . . . . . . . . . . .
46
2.3.3
Global Models — for the Genomic Distributions of Motif Sites
. . . . . .
47
2.3.3.1
The oops and zoops model
. . . . . . . . . . . . . . . . . . . .
47
2.3.3.2
General uniform and independent models . . . . . . . . . . . . .
49
2.3.3.3
The dictionary model . . . . . . . . . . . . . . . . . . . . . . .
51
2.3.3.4
The sliding-window approaches . . . . . . . . . . . . . . . . . .
53
2.3.3.5
The hidden Markov model
. . . . . . . . . . . . . . . . . . . .
54
2.3.4
Other Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.3.4.1
Comparative genomic approach . . . . . . . . . . . . . . . . . .
56
2.3.4.2
Joint models for motifs and expression proﬁles . . . . . . . . . .
58
2.3.5
Summary: Understanding Motif Detection Algorithms . . . . . . . . . . .
59
2.4
MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif
Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
v

2.4.1
Categorization of Motifs Based on Biological Classiﬁcation of DNA Bind-
ing Proteins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
2.4.2
HMDM: a Bayesian Proﬁle Model for Motif Families
. . . . . . . . . . .
67
2.4.2.1
Training a MotifPrototyper . . . . . . . . . . . . . . . . . . . .
70
2.4.3
Mixture of MotifPrototypers . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.4.3.1
Classifying motifs . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.4.3.2
Bayesian estimation of PWMs
. . . . . . . . . . . . . . . . . .
72
2.4.3.3
Semi-unsupervised de novo motif detection . . . . . . . . . . . .
73
2.4.4
Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
2.4.4.1
Parameter estimation
. . . . . . . . . . . . . . . . . . . . . . .
74
2.4.4.2
Motif classiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . .
76
2.4.4.3
PWM estimation and motif scoring . . . . . . . . . . . . . . . .
77
2.4.4.4
De novo motif discovery . . . . . . . . . . . . . . . . . . . . . .
79
2.4.5
Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
2.5
CisModuler: Modeling the Syntactic Rules of Motif Organization
. . . . . . . . .
85
2.5.1
The CisModuler Hidden Markov Model . . . . . . . . . . . . . . . . . . .
86
2.5.2
Bayesian HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
2.5.3
Markov Background Models . . . . . . . . . . . . . . . . . . . . . . . . .
91
2.5.4
Posterior Decoding Algorithms for Motif Scan
. . . . . . . . . . . . . . .
91
2.5.4.1
The baseline algorithm
. . . . . . . . . . . . . . . . . . . . . .
91
2.5.4.2
Bayesian inference and learning . . . . . . . . . . . . . . . . . .
92
2.5.5
Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.5.5.1
MAP prediction of motifs/CRMs . . . . . . . . . . . . . . . . .
95
2.5.5.2
Motif/CRM prediction via thresholding posterior probability proﬁle 96
2.5.6
Summary and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
2.6
LOGOS: for Semi-unsupervised de novo Motif Detection
. . . . . . . . . . . . .
100
2.6.1
Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
vi

2.6.1.1
Performance on semi-realistic sequence data . . . . . . . . . . .
102
2.6.1.2
Motif detection in yeast promoter regions . . . . . . . . . . . . .
105
2.6.1.3
Motif detection in Drosophila regulatory DNAs
. . . . . . . . .
106
2.7
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
3
Modeling Single Nucleotide Polymorphisms for Haplotype Inference
111
3.1
Biological Foundations and Motivation . . . . . . . . . . . . . . . . . . . . . . . .
112
3.2
Problem Formulation and Overview of Related Work . . . . . . . . . . . . . . . .
114
3.2.1
Baseline Finite Mixture Model and the EM Approach . . . . . . . . . . . .
116
3.2.2
Bayesian Methods via MCMC . . . . . . . . . . . . . . . . . . . . . . . .
117
3.2.2.1
Simple Dirichlet priors
. . . . . . . . . . . . . . . . . . . . . .
117
3.2.2.2
The coalescent prior . . . . . . . . . . . . . . . . . . . . . . . .
118
3.2.3
Bayesian Network Prior
. . . . . . . . . . . . . . . . . . . . . . . . . . .
118
3.2.4
Summary and Prelude to Our Approach . . . . . . . . . . . . . . . . . . .
120
3.3
Haplotype Inference via the Dirichlet Process . . . . . . . . . . . . . . . . . . . .
121
3.3.1
Dirichlet Process Mixture
. . . . . . . . . . . . . . . . . . . . . . . . . .
122
3.3.2
DP-Haplotyper: a Dirichlet Process Mixture Model for Haplotypes
. . . .
123
3.3.3
Haplotype Modeling Given Partial Pedigree . . . . . . . . . . . . . . . . .
126
3.4
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
3.4.1
Simulated Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
3.4.2
Real Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
3.5
Conclusions and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4
Probabilistic Inference I: Deterministic Algorithms
136
4.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
4.1.1
Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
vii

4.2
Exact Inference Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
4.2.1
The Junction Tree Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
141
4.3
Approximate Inference Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .
145
4.3.1
Cluster-factorizable Potentials . . . . . . . . . . . . . . . . . . . . . . . .
145
4.3.2
Exponential Representations . . . . . . . . . . . . . . . . . . . . . . . . .
146
4.3.3
Lower Bounds of General Exponential Functions . . . . . . . . . . . . . .
147
4.3.3.1
Lower bounding probabilistic invariants
. . . . . . . . . . . . .
149
4.3.4
A General Variational Principle for Probabilistic Inference . . . . . . . . .
150
4.3.4.1
Variational representation . . . . . . . . . . . . . . . . . . . . .
151
4.3.4.2
Mean ﬁeld methods . . . . . . . . . . . . . . . . . . . . . . . .
152
4.3.4.3
Belief propagation . . . . . . . . . . . . . . . . . . . . . . . . .
154
4.4
Generalized Mean Field Inference . . . . . . . . . . . . . . . . . . . . . . . . . .
155
4.4.1
GMF Theory and Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
156
4.4.1.1
Naive mean ﬁeld approximation . . . . . . . . . . . . . . . . . .
156
4.4.1.2
Generalized mean ﬁeld theory . . . . . . . . . . . . . . . . . . .
158
4.4.2
A more general version of GMF theory . . . . . . . . . . . . . . . . . . .
163
4.4.3
A Generalized Mean Field Algorithm . . . . . . . . . . . . . . . . . . . .
164
4.4.4
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
4.5
Graph Partition Strategies for GMF Inference . . . . . . . . . . . . . . . . . . . .
169
4.5.1
Bounds on GMF Approximation . . . . . . . . . . . . . . . . . . . . . . .
171
4.5.2
Variable Clustering via Graph Partitioning . . . . . . . . . . . . . . . . . .
172
4.5.2.1
Graph partitioning . . . . . . . . . . . . . . . . . . . . . . . . .
172
4.5.2.2
Semi-deﬁnite relaxation of GP
. . . . . . . . . . . . . . . . . .
174
4.5.2.3
Finding a closest feasible solution . . . . . . . . . . . . . . . . .
176
4.5.3
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
4.5.3.1
Partitioning random graphs . . . . . . . . . . . . . . . . . . . .
177
4.5.3.2
Single-node marginals . . . . . . . . . . . . . . . . . . . . . . .
178
viii

4.5.3.3
Bounds on the log partition function
. . . . . . . . . . . . . . .
179
4.6
Extensions of GMF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
4.6.1
Higher Order Mean Field Approximation . . . . . . . . . . . . . . . . . .
181
4.6.2
Alternative Tractable Subgraphs . . . . . . . . . . . . . . . . . . . . . . .
182
4.6.3
Alternative Graph Partitioning Schemes . . . . . . . . . . . . . . . . . . .
182
4.7
Application to the LOGOS Model . . . . . . . . . . . . . . . . . . . . . . . . . .
183
4.7.1
A GMF Algorithm for Bayesian Inference in LOGOS . . . . . . . . . . .
184
4.7.2
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
4.7.2.1
Convergence behavior of GMF . . . . . . . . . . . . . . . . . .
186
4.7.2.2
A comparison of GMF and the Gibbs sampler for motif inference
187
4.8
Conclusions and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
5
Probabilistic Inference II: Monte Carlo Algorithms
191
5.1
A Brief Overview of Monte Carlo Methods
. . . . . . . . . . . . . . . . . . . . .
191
5.2
A Gibbs Sampling Algorithm for LOGOS . . . . . . . . . . . . . . . . . . . . . .
193
5.2.1
The Collapsed Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . .
193
5.2.2
Convergence Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
5.3
Markov Chain Monte Carlo for Haplotype Inference
. . . . . . . . . . . . . . . .
196
5.3.1
A Gibbs Sampling Algorithm
. . . . . . . . . . . . . . . . . . . . . . . .
197
5.3.2
A Metropolis-Hasting Sampling Algorithm . . . . . . . . . . . . . . . . .
201
5.3.3
A Sketch of MCMC Strategies for the Pedi-haplotyper model
. . . . . . .
202
5.3.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
5.4
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
6
Conclusions
207
6.1
Conclusions from This Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
6.2
Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
ix

6.2.1
Modeling Gene Regulation Networks of Higher Eukaryotes in Light of Sys-
tems Biology and Comparative Genomics . . . . . . . . . . . . . . . . . .
208
6.2.2
Genetic Inference and Application Based on Polymorphic Markers . . . . .
211
6.2.3
Automated Inference in General Graphical Models . . . . . . . . . . . . .
213
A More details on inference and learning for motif models
215
A.1
Multinomial Distributions and Dirichlet Priors . . . . . . . . . . . . . . . . . . . .
215
A.2
Estimating Hyper-Parameters in the HMDM Model . . . . . . . . . . . . . . . . .
217
A.3
Computing the Expected Sufﬁcient Statistics in the Global HMM . . . . . . . . . .
219
A.4
Bayesian Estimation of Multinomial Parameters in the HMDM Model . . . . . . .
220
B
Proofs
222
B.1
Theorem 2: GMF approximation . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
B.2
Theorem 5: GMF bound on KL divergence
. . . . . . . . . . . . . . . . . . . . .
224
x

Chapter 1
Introduction
Understanding the structure and functional organization of the genome is a fundamental problem
in biology. This thesis introduces new computational statistical approaches for analyzing two par-
ticular types of genomic data: gene regulatory sequences, and single nucleotide polymorphisms.
It presents the methodology of applying the probabilistic graphical model formalism to designing
novel parametric and non-parametric Bayesian models for genomic data, in accordance with bio-
logical prior knowledge or genetic hypotheses about the population of subjects under investigation.
In particular, it presents algorithms for the problems of motif detection and haplotype inference, and
develops the general theory and algorithms of generalized mean ﬁeld approximation for variational
inference on large-scale, hybrid, multivariate probabilistic models.
Although the major goal of this thesis is to develop probabilistic models and computational
algorithms for deciphering biological data and exploring the mechanisms and evolution of biological
systems based on mathematical principles, most of the ideas and results reported here can also
serve as building blocks of generic intelligent systems for a wide range of applications that involve
predictive understanding and reasoning under uncertainty.
1

1.1 Genomic Analysis and the Graphical Model Approach
1.1
Genomic Analysis and the Graphical Model Approach
1.1.1
The Architecture and Function of the Genome
According to the central dogma, the genetic information that determines the functional and mor-
phological properties of the cells in a living organism is encoded in the DNA genome [Crick, 1970].
Biochemically, DNAs are double-stranded macromolecules representable as a pair of long comple-
mentary sequences of characters — A, T, G and C, denoting four kinds of basic elements, known as
nucleotides, that make up the DNA molecules. Residing in (and inherited via) the DNA molecules,
are a rich set of coding sequences referred to as genes, which determine the structures and functions
of an essential set of biopolymer molecules, mostly proteins, but also including RNAs, which are
the main determinants of various cellular and physiological activities taking place in a living sys-
tem, such as biochemical catalysis, signal transduction, cellular defense, etc. [Lewin, 2003]. Also
abundant in the DNAs are a large number of so-called non-coding sequences, whose role was orig-
inally thought to be purely structural (e.g., serving as the physical scaffold of a chromosome —
a long thread of DNA tightly packaged with the aid of several auxiliary proteins), but have been
recently discovered to play essential roles in the cellular implementation of the gene regulation
network [Davidson, 2001; Alberts et al., 2002].
DNAs usually reside in the nucleus (or the nuclear region for prokaryotic organisms) of the
cell. Via a process called transcription (to be explained shortly), some genes in the DNA genome
are copied to molecules called messenger RNA (mRNA), which can travel out of the nucleus to
the protein synthesis apparatus, where proteins are assembled based on the coding information car-
ried by mRNA via a process called translation [Alberts et al., 2002]. Although different cells of
an organism have the same DNA genome, it is well known that they have different protein com-
position and perform different functions [Davidson, 2001]. For example, red blood cells are rich
in hemoglobins that can carry oxygen, whereas muscle cells contain a large number of myosins
for muscular contraction. Even the same cell may bear different protein contents at different times
during its life span. This kind of diversity is a consequence of spatially and temporally regulated
2

1.1 Genomic Analysis and the Graphical Model Approach
expression of genes. It is believed that much of the information that determines when and in what
cellular environment a gene is expressed is encoded in certain genomic sequences, which possibly
account for a major portion of the total sequence of the genome, especially in the higher eukaryotes,
such as human [Davidson, 2001; Michelson, 2002].
Figure 1.1: The transcriptional regulatory machinery ( adapted from [Wasserman and Sandelin, 2004] ). TFBS: transcrip-
tion factor binding site, CRM: cis-regulatory module; chromatin: a long, extended thread of DNA packed with histone
proteins.
The creation of diverse cell types from an invariant set of genes is governed by complex bio-
chemical processes that regulate gene activities. Transcription, the initial step of gene expression,
is central to the regulatory mechanisms. Transcription refers to the process of making a single-
stranded mRNA molecule using one of the DNA strands as template. The timing and volume of
transcription are controlled by complex transcription regulatory machinery made up of both protein
and DNA elements [Ptashne, 1988; Ptashne and Gann, 1997]. As shown in Fig. 1.1, the signals that
activate or suppress the transcription of a gene are physically mediated by different types of gene
regulatory proteins called transcription factors (TFs). To bring these signals into effect on a target
3

1.1 Genomic Analysis and the Graphical Model Approach
gene at a speciﬁc time in a speciﬁc cell, certain TFs must recognize speciﬁc binding sites in the
vicinity of the target gene, so that they can jointly interact with the basal transcription apparatus,
made up of an RNA polymerase and some general TFs, to turn on or off transcription in the right
physiological/developmental context.
DNA motifs are the protein binding sites on DNA sequences that can be recognized by speciﬁc
TFs to integrate complex gene regulatory signals (hence they are also referred to as transcription
factor binding sites, or TFBS). These sites are usually located in the vicinity of the transcription
initiation sites of the genes under their regulation — an extended sequence region generally re-
ferred to as the transcriptional regulatory region [Lewin, 2003]. Depending on which organism the
genomic sequences are from, the complexities of the transcriptional regulatory regions vary signiﬁ-
cantly. Their lengths range from a few hundred base pairs (e.g, in simple bacteria such as E. coli) to
over several hundred thousand base pairs (e.g., in more complex insects such as Drosophila); their
locations can be either immediately proximal to the transcription initiation sites, or much further
upstream or even downstream (i.e., into the intron regions of gene sequences); and their contents
range anywhere from sparse single-motif-promoters, to multiple complex cis-regulatory modules
(CRMs) each containing arrays of multiple motifs [Davidson, 2001] (Fig. 1.1). Motifs, together
with their speciﬁc pattern of deployment (e.g., ordering, contexts) in the genome, constitute the
hardwired part of the transcription regulatory machinery, which is present in every cell of an or-
ganism, although different subsets of motifs will be involved in gene regulation in different cells.
Deciphering the gene control circuitry encoded in DNA, its structure and its functional organization
is a fundamental problem in biology, and is a focus of this thesis.
1.1.2
The Populational Diversity and Evolution of the Genome
When the human genome project was launched over a decade ago, there was an interesting debate
over who should have the honor (but not without the courage of relinquishing the utmost privacy) to
have his/her genome sequenced. One rumor goes that the chief of the Celera company had taken this
privilege. This debate struck a key issue in genetics — that at the very sequence level, there exist
4

1.1 Genomic Analysis and the Graphical Model Approach
individual distinctions and even populational diversities in the DNA genome. This phenomenon is
referred to as genetic polymorphism.
A polymorphism is a neutral genetic variant that appears in at least 1% of the human population,
and does not directly elicit any substantial advantage or disadvantage for the survival of the individ-
ual bearing it [Kruglyak and Nickerson, 2001]. Polymorphisms are often regarded as ﬁngerprints
of ancestral genetic alterations left on modern genomic sequences during evolution and can serve
as genetic markers of population- or disease-related phenotypes [Clark, 2003]. Common poly-
morphisms include insertion/deletion of minisatellites, microsatellites, Alu segments, etc., which
are non-functional DNA segments of various sizes; as well as single nucleotide polymorphisms
(SNPs) [Stoneking, 2001].
Figure 1.2:
Single nucleotide polymorphisms as appeared in two chromosomes from a population (adapted
from [Chakravarti, 2001]).
SNP refers to the existence of two possible kinds of nucleotides at a single chromosomal locus
in a population; each variant is called an allele (Fig. 1.2). SNPs reﬂect past mutations that were
mostly (but not exclusively) unique events, and two individuals sharing a variant allele are thereby
marked with a common evolutionary heritage [Patil et al., 2001; Stoneking, 2001]. In other words,
5

1.1 Genomic Analysis and the Graphical Model Approach
our genes have ancestors, and analyzing shared patterns of SNP variations can identify them. The
real importance of SNPs lies in their abundance. It is estimated that there are more than 5 million
common SNPs each with frequency 10-50% in the whole human population, which translates to
about one SNP in every 600 base pairs in the human genome [Zhang et al., 2002]. These SNPs
account for more than 90% of human DNA sequence difference.
As SNPs are remnants of ancient neutral DNA alterations dated back to a time measured at
a genealogical scale, they contain more ﬁne-grained information on molecular evolution than that
revealed by orthologous genomic sequences from multiple species, whose differences are accu-
mulated over a geological period of time and are subject to selection. In general, the higher the
frequency of a SNP allele, the older the mutation that produced it, so high-frequency SNPs largely
predate human population diversiﬁcation. Therefore, population-speciﬁc alleles may bear important
information about human evolution that involves speciﬁc migrations (such as those that populated
Polynesia and the Americas) [Stoneking, 2001].
Most human variation that is inﬂuenced by genes can be related to SNPs (either as associated
markers or causative elements), especially for such medically (and commercially) important traits
as how likely one is to become afﬂicted with a particular disease, or how one might respond to
a particular pharmaceutical treatment, as discussed in [Chakravarti, 2001]. Even when a SNP is
not directly responsible, the dense distribution of SNPs in the genome suggests they can also be
used to locate genes that inﬂuence such traits based on a linkage disequilibrium test (for gametic
association between the putative causal gene(s) and SNPs in the vicinity) [Akey et al., 2001; Daly et
al., 2001; Pritchard, 2001]. For higher organisms, accurate inferences concerning population history
or association studies of disease propensities and other complex traits usually demand the analysis
of the states of sizable segments of the subject’s chromosome(s) [Kenneth and Clark, 2002]. To this
end, it is advantageous to study haplotypes, which consist of several closely spaced (hence linked)
SNPs and often prove to be more powerful discriminators of genetic variations within and among
populations, and hence serve as more informative markers for linkage analysis and evolutionary
studies.
6

1.1 Genomic Analysis and the Graphical Model Approach
1.1.3
Probabilistic Graphical Models and Genomic Analysis
Due to the stochastic nature of genomic data, and the abundance of empirical biological prior knowl-
edge about their properties, the general methodologies adopted in this thesis are built on probabilis-
tic models that accommodate uncertainty and statistical errors associated with the data, and that
incorporate certain prior information in a principled way.
The models we develop in this thesis use a formalism called probabilistic graphical mod-
els [Pearl, 1988; Cowell et al., 1999; Lauritzen and Sheehan, 2002], which refer to a family of
probability distributions deﬁned in terms of a directed or undirected graph with probabilistic se-
mantics (Fig. 1.3).
X1
X2
X3
X6
X5
X4
Figure 1.3: A directed graphical model for a joint probability distribution over {x1, x2, x3, x4, x5, x6}.
It entails
p(x1, x2, x3, x4, x5, x6) = p(x1)p(x2|x1)p(x4|x1)p(x3|x2)p(x5|x4)p(x6|x2, x5).
A graphical model has both a structural (or topological) component — encoded by a graph
G(V, E), where V is the set of nodes and E is the set of edges of the graph; and a parametric compo-
nent — encoded by numerical “potentials” {φC(xC) : C ⊂V}, a set of positive numbers associated
with the state conﬁgurations of subsets of nodes in the graph. Each node in the graph represents
a random variable Xi, which can be either observed or latent, as indicated by the shading of the
node 1; the presence of edges between nodes denotes direct dependencies between the correspond-
ing variables. Independent and identically distributed (iid) random variables can be represented by
a macro called a plate, which allows a subgraph to be replicated. For example, the assertion that
1In the sequel, we use upper-case X (resp. X) to denote a random variable (resp. variable set), and lower-case x
(resp. x) to denote a certain state (or value, conﬁguration, etc.) taken by the corresponding variable (resp. variable set).
7

1.1 Genomic Analysis and the Graphical Model Approach
variables {Xi} are conditionally iid given θ can be represented by a plate over Xi (Fig.1.4a). The
family of joint probability distributions associated with a given graph can be parameterized in terms
of a product over potential functions associated with subsets of nodes in the graph. For directed
graphical models (associated with acyclic directed graphs), which are often referred to as Bayesian
networks, each node, Xi, and its parents, Xπi, constitute the basic subset on which a potential func-
tion is deﬁned, and the potential function turns out to be the local conditional probability p(xi|xπi).
Hence, we have the following representation for the joint probability:
p(x) =
Y
i∈V
p(xi|xπi).
(1.1)
For undirected graphical models, which are often referred to as Markov random ﬁelds, the basic
subsets are cliques (completely connected subsets of nodes) of the graph, {XDα : α ∈A}, where
Dα denotes the set of node indices of clique α, and A denotes the index set of all cliques. The joint
probability in this case is:
p(x) = 1
Z
Y
α∈A
φα(xDα),
(1.2)
where Z is a normalizing constant, ensuring that
R
p(x)dx = 1 (or P
x p(x) = 1 for discrete
models).
X1
N
θ
≡
X1
X2
XN
...
(a)
θ
⇒
θ
(b)
Figure 1.4: Various graphical models. Shaded nodes denote observed variables. (a) Plate. (b) From a ﬂat parametric
model to a Bayesian model.
8

1.1 Genomic Analysis and the Graphical Model Approach
Graphical models provide a compact graph-theoretic representation of probabilistic distribu-
tions in a way that clearly exposes the structure of a complex domain. They also provide a conve-
nient vehicle to adopt the Bayesian philosophy, because hierarchical Bayesian models can be natu-
rally speciﬁed as directed graphical models. For example, putting a prior on the model parameter θ,
now treated as a random variable, is equivalent to adding a parent node that denotes the hyperparam-
eter and associating the newly introduced edge with a prior distribution (Fig. 1.4b). A distinctive
feature of the graphical model approach is its naturalness in formulating large probabilistic models
of complex phenomena, by facilitating modular combination of heterogeneous submodels, using
the property of the product rule of the joint distribution. Thus, a complex model can be assembled
in a piecewise fashion, and even solved via a divide-and-conquer approach, as will be done in this
thesis.
The ﬁeld of computational genomics is fertile ground for the application of graphical models,
and many of its complex problems can be readily handled within this formalism in a canonical and
systematic way [Lauritzen and Sheehan, 2002]. For example, in a typical statistical genetics setting,
we may want to model some complex genetic patterns with both observed and hidden variables
using a likelihood model, and we concern ourselves with a sample set of N individuals (Fig. 1.5,
bottom level). If we imagine that the genetic pattern of each individual is stochastically sampled
from K possible populational genetic patterns, or in other words, they form K clusters, then we can
make this explicit by adding the plate and nodes denoting K cluster centroids and the associated
variances (Fig. 1.5, middle level). However, usually we do not know the number of clusters and
where the centroids lie; in that case we can use a non-parametric Bayesian prior model to introduce
a distribution over the space of all possible centroid sets (Fig. 1.5, top level). By this modular
construction, we end up with a graphical model that corresponds to an inﬁnite mixture model, as
depicted in Fig 1.5. As you will see shortly, this graphical model is actually the formal foundation
of a haplotype inference model we will develop in this thesis.
In summary, the graphical model framework provides a clean mathematical formalism that has
made it possible to understand the relationships among a wide variety of network-based approaches
9

1.2 Thesis Overview
0
Γ
k
θ
A k
Hn2
Hn1
Gn
α
Γ
N
K
Figure 1.5: A graphical model representation of an inﬁnite mixture model for complex populational genetic patterns.
to statistical computation, and in particular to understand many domain-speciﬁc statistical inference
algorithms and architectures as instances of a broad probabilistic methodology. These features of
graphical models help to greatly simplify the design of complex probabilistic models needed for our
problems, and hopefully also make them easier to understand.
1.2
Thesis Overview
1.2.1
The Problem
In silico motif detection is the task of identifying potential motif patterns from DNA sequences using
a pattern recognition program. Most contemporary motif detection algorithms were originally moti-
vated by promoter analysis of yeast or bacteria genomes, which in general have a simple motif struc-
ture and organization [Bailey and Elkan, 1995a; Lawrence and Reilly, 1990; Lawrence et al., 1993;
Liu et al., 1995; Hughes et al., 2000; Liu et al., 2001]. Therefore, these algorithms usually em-
ploy a naive approach for motif modeling, which typically assumes that, locally, the probabilities
of the nucleotides at different sites within a motif are independent of each other; and globally,
instances of motifs are distributed uniformly and independently in the regulatory sequence. In
most cases, such an approach does not incorporate any prior knowledge of motif structures and
motif organizations, even though there is a wealth of valuable information regarding these prop-
erties present in the biological community.
These deﬁciencies, although well recognized very
10

1.2 Thesis Overview
early on, did not become a practical performance bottleneck (due to the small size and mod-
est complexity of the study sequences being considered) until the recent completion of several
grand sequencing projects that involve much more complex multicellular higher eukaryotes, such
as Drosophila and human [Venter et al., 2001]. With the availability of genomic sequences of
these complex organisms, contemporary research in functional genomics is moving toward under-
standing the mechanisms and coding schemes of gene regulation networks driving biological pro-
cesses unique to complex organisms, such as embryogenesis, differentiation, etc., which bear great
relevance to medical and pharmaceutical interests [Markstein et al., 2002; Berman et al., 2002;
Michelson, 2002]. A hallmark of the gene regulatory sequences of higher eukaryotes is the remark-
able sophistication of the control program they employ to direct combinatorially ﬁne-tuned gene
expression in a time- and space-speciﬁc manner [Davidson, 2001]. The presence of highly sophis-
ticated deterministic and stochastic constraints on motif deployment and the diverse categorization
of motif structures in the aforementioned control programs, and the enormous size of the regula-
tory sequences in which motifs must be found, render existing methods inadequate for uncovering
motif signals from the complex genomic background. More powerful models and computational
algorithms are needed to cope with such challenge.
For autosomal loci in the genome of diploid organisms, when only the genotypes of mul-
tiple SNPs for each individual are provided, the haplotype for those individuals with multiple
heterozygous genotypes is inherently ambiguous [Clark, 1990; Hodge et al., 1999]. The prob-
lem of inferring haplotypes from genotypes of SNPs is essential for the understanding of ge-
netic variations within and among populations, with important applications to the genetic analy-
sis of disease propensities and other complex traits [Clark, 2003]. The problem can be formu-
lated as a mixture model, where the set of mixture components corresponds to the pool of hap-
lotypes in the population [Excofﬁer and Slatkin, 1995; Niu et al., 2002; Stephens et al., 2001;
Kimmel and Shamir, 2004]. The size of this pool is unknown; indeed, knowing the size of the pool
would correspond to knowing something signiﬁcant about the genome and its history. Extant meth-
ods have largely bypassed explicitly modeling the uncertainty of this important quantity. Speaking
11

1.2 Thesis Overview
under a broader context, this problem is closely related to the perennial problem of ”how many
clusters?” in the clustering literature, and is particularly salient in large data sets where the number
of clusters needs to be relatively large and open-ended. Current approaches based on ﬁxing the
number of clusters and using the mixing proportions or an information-theoretic score to gauge the
appropriate number are clearly not adequate.
For many bioinformatics problems, including the problems we address in this thesis, proba-
bilistic models have an inherent appeal, because they provide an elegant and powerful methodology
to formulate various types of important problems such as classiﬁcation, clustering, prediction and
reasoning under uncertainly, and can systematically handle issues such as missing values, noisy
data, prior knowledge, data fusion, etc. [Lauritzen and Sheehan, 2002; Jordan, 2004]. However,
large-scale probability models, as are often needed in bioinformatics problems, have outgrown the
ability of current (and probably future) exact inference algorithms to compute posteriors and learn
parameters. This is particularly true for the models developed in this thesis, which involve high-
dimensional Bayesian missing data problems. Although Monte Carlo algorithms [Gilks et al., 1996]
enjoy asymptotic correctness, and are often easy to implement, their prohibitive computational cost
renders them practically infeasible for some of the challenging problems, as we encountered in
motif detection. Some extant deterministic approximate inference algorithms, such as loopy belief
propagation [Pearl, 1988; Murphy et al., 1999], provide an alternative solution, but their generality
and quality remain an open problem, which hinders their widespread application.
1.2.2
Contributions of This Thesis
In this thesis, we present a modularly designed hierarchical Bayesian Markovian model for motif
detection in complex genomic sequences. This model, referred to as LOGOS, captures the de-
pendency structure of regulatory elements at two levels: the conservation dependencies between
sites within motifs, and the clustering of motifs into regulatory modules. In order to uncover un-
known motifs de novo from higher eukaryotic genomes based solely on un-curated sequence data
(a realistic scenario we have to face in animal genome annotation), LOGOS employs a mixture of
12

1.2 Thesis Overview
proﬁle motif models, which can be trained on biologically identiﬁed motifs categorized according
to protein-binding mechanisms and which can serve as a structured Bayesian prior for a probabilis-
tic motif representation. Such a model biases the likelihoods of nucleotide strings toward those
corresponding to biologically meaningful motifs rather than trivial patterns recurring in the ge-
nomic sequence, but does so without a priori committing to any speciﬁc consensus sequences. To
our knowledge, this is the ﬁrst model that enables de novo motif detection to beneﬁt from prior
knowledge of biologically identiﬁed motifs, and classiﬁes motifs based on protein binding mecha-
nisms. To model the locational organization of motifs in the genome, LOGOS also uses a hidden
Markov model (HMM) to encode the syntactic rules of motif dependencies, with model parameters
smoothed under empirical Bayesian priors. Using the graphical model formalism, the aforemen-
tioned model ingredients addressing different aspects of motif properties can be integrated into a
composite joint probabilistic model. The modular architecture of LOGOS manifests a principled
framework for developing, extending and computing expressive biopolymer sequence models.
The second result is an extension of the ﬁnite mixture models to the more ﬂexible paradigm of
countably-inﬁnite mixture models. We present a nonparametric Bayesian model using the Dirichlet
process prior, in the context of SNP haplotype inference for multiple SNPs. The model, which is
referred to as DP-haplotyper, deﬁnes a prior distribution over both the centroids and the cardinality
of a mixture model, that is, the identities and the numbers of the possible haplotypes in a population
(rather than setting the number of haplotypes to an ad hoc ﬁxed constant in extant models). It also
employs a ﬂexible likelihood model for each haplotype (i.e., each mixture component) to model
the relationship between the haplotypes and the genotypes. As a result, DP-haplotyper accommo-
dates growing data collections as well as noisy and/or incomplete observations during experimental
genotyping, and imposes an implicit bias toward a small variety of haplotypes (i.e., a small number
of centroids in the mixture model terminology) which is reminiscent of parsimony methods. This
model outperforms the state-of-the-art haplotyping program, and is very promising as a building
block for expressive models necessary in more complex problems related to SNP analysis.
Finally, the thesis presents a generalized mean ﬁeld (GMF) theory for variational inference in
13

1.2 Thesis Overview
exponential family graphical models (to be deﬁned in the sequel). A GMF method uses a fam-
ily of tractable distributions deﬁned on arbitrary disjoint model decompositions to approximate an
intractable distribution, and solves the optimal approximation using a generic message passage pro-
cedure provably convergent to globally consistent ﬁxed points of marginals and leading to a lower
bound on the likelihood of observed data under the distribution. This framework generalizes several
previous studies on model-speciﬁc structured variational approximation, yet specializes a previous
study suggesting non-disjoint model decompositions, and appears to strike the right balance be-
tween quality of approximation and computational complexity. This algorithm has been used as
the main inference engine for motif detection using the LOGOS model. The thesis also shows that
the task of model decomposition, which is a prerequisite for the GMF algorithm, can be automated
and optimized using graph partitioning; it demonstrates the empirical superiority of a minimal cut
over other partition schemes, as well as giving theoretical justiﬁcations. This combination of GMF
inference with combinatorial optimization represents an initial foray into the development of a truly
turnkey algorithm for distributed approximate inference with bounded performance.
1.2.3
Importance for Bioinformatics, Computer Science and Statistics
The immediate use of these models and algorithms is in allowing us to develop software for solving
certain long-standing computational genomics problems, speciﬁcally, motif detection and haplo-
type inference, under realistic and complex biological contexts, with noisy and incomplete mea-
surements, and in light of empirical prior knowledge as well as theoretical insight from biological
literature.
Biological systems are intrinsically complex and stochastic. In recognition of this, we have
strived to develop large-scale mathematical models using principles of probability theory, graph
theory and information theory to capture and appropriately handle these issues. It is our belief
that the lack of mathematical sophistication in many extant bioinformatics models and programs is
a concession to computational complexity, rather than a reﬂection of the biological reality of the
systems or mechanisms under study. As a step toward dealing with these realities, this thesis also
14

1.2 Thesis Overview
concentrates on exploring computational techniques that can reliably and efﬁciently solve challeng-
ing large-scale probabilistic models.
Throughout the thesis, the formalism of probabilistic graphical models has been used to con-
struct problem-speciﬁc Bayesian models, and guide the implementation of computational algo-
rithms for inference and learning in solving the associated computational biology problem. The
longer term value of this thesis and the most important idea from it, we would hope, is that, in
certain problem domains, one can use probabilistic graphical models from beginning to end as a
general-purpose modeling language to systematically, modularly, and formally build large-scale
models for a complex domain in a divide-and-conquer and bottom-up fashion, avoiding being en-
tangled in the immensely complex and often messy details one has to face in these domains; and
to exploit the availability of general-purpose inference and learning algorithms for graphical mod-
els. As you proceed, the creation of the LOGOS model from the MotifPrototyper and CisModuler
models, and the elaboration of Pedi-haplotyper from the basic DP-haplotyper hopefully serve as
motivating examples.
We would particularly like to point out that, when pursuing probabilistic (in particular, Bayesian)
approaches to complicated statistical problems, such as those in the biological domain, it is helpful,
conceptually, to distinguish two separate issues [Stephens and Donnelly, 2003]:
• The model (e.g., prior distribution or likelihood function) for the quantities of interest. Exam-
ples (detailed shortly in the technical section) include, special prior models for the positional
weight matrices of motifs, or for the ancestral haplotype templates of individual haplotypes.
For a given data set, different model assumptions will in general lead to different posterior
distributions and hence to different estimates.
• The computational algorithm used. For challenging problems, including the ones addressed
in this thesis, the posterior distribution cannot be calculated exactly. Instead, computational
methods — such as a variational inference algorithm, or Monte Carlo algorithms — are used
to approximate it. Different computational tricks, or different settings of the “free knobs” in
15

1.2 Thesis Overview
the algorithms (e.g., number of iterations, convergence test, etc.), will change the quality of
the approximation to the true posterior.
Not separating these two aspects in the face of a complex problem can be counter-productive. For
example, it is not unusual to see summary sentences or listings like “we compare our algorithm
TIGER with the extant algorithms CAT, EM, the Gibbs sampler, and the hidden Markov model ...”,
which is technically confusing and misleading, and strictly speaking, formally inappropriate. It ob-
scures the technical ingredients of each algorithm, and conceals possible distinctions (or very often,
lack of technical distinctions) between different algorithms—be it a model distinction, an algorith-
mic distinction for computation, or a distinction in the implementation. For instance, algorithm
“TIGER” may also employ a Gibbs sampling algorithm for computation, and the “EM” and “Gibbs
sampler” may have adopted the same probabilistic model. This blurring can cause unnecessary
confusion when analyzing different models and possible duplication of previous work, and makes
it difﬁcult for practitioners or end-users to pick the appropriate algorithm for a certain task, and
for developers to identify technical aspects subject to improvement. In this thesis, we intentionally
make explicit these two aspects of computational probabilistic methodology in the exposition of
existing and new models and algorithms.
The main theme of this thesis is the application of statistical machine learning approaches to
computational biology. However, computational biology is not about simple matching between
textbook algorithms and biological datasets. Close interactions between well-designed biological
experiments and elegant yet realistic formulation of the mathematical models, as well as the de-
velopment of efﬁcient algorithms, are all essential to computational biology research. This thesis
attempts to reﬂect the intimate interactions between biological concepts, mathematical formalisms,
and computational algorithms, via an exposition that starts from highly problem-speciﬁc modeling
efforts, followed by generalizations and combinations thereof, and eventually motivates an attempt
to develop a generic computation technique. We believe that progress in the ﬁelds of machine learn-
ing and in biological research can be synergistic. Insights gained from theoretical and algorithmic
research in machine learning can bring a new perspective and tools for studying biological objects,
16

1.3 Technical Results of This Thesis
and can foster new applications. On the other hand, biological research, facing systems of immense
complexity and stochasticity rarely encountered elsewhere, challenges advanced mathematical and
computational techniques for analysis and interpretation, and could lead to new developments that
ﬁnd broader application in ﬁelds outside biology that involve predictive understanding, learning and
reasoning under uncertainty.
1.3
Technical Results of This Thesis
1.3.1
A Modular Parametric Bayesian Model for Transcriptional Regulatory Se-
quences
Most conventional motif models lack a clean formalism for imposing useful controls over where to
search for motifs (hence, all regions are taken as equally likely to harbor motifs) and what substring
patterns are preferred over others as candidate motifs (therefore, all recurring substring patterns are
equally likely to be accepted as functionally meaningful motifs). In Chapter 2, we propose a princi-
pled framework for introducing such controls for motif modeling. The goal is to develop a formal-
ism that is expressive (in terms of being able to capture the internal structures, organizational rules,
and other properties of motifs, and readily incorporating prior knowledge about these properties
from biological literature), yet mathematically and algorithmically transparent and well-structured,
hence simplifying model construction, computation and extension. Based on the product rule of the
joint probability in the graphical model formalism, we outline the formal architecture of a modular
motif model with the following three components: the local alignment model, which captures the
intrinsic properties within motifs, including characteristic position weight matrices (PWMs) and
site dependencies; the global distribution model, which models the frequencies of different motifs
and the dependencies between motif occurrences in a sequence; and the background model, which
deﬁnes the distribution of non-motif nucleotide sequences. The model components can be designed
separately, and then fused into a consistent, more expressive joint model.
17

1.3 Technical Results of This Thesis
1.3.1.1
Proﬁle Bayesian models for motif sequence pattern
It is well known that the DNA-binding domains of gene-regulatory proteins fall into several distinc-
tive classes, such as the zinc-ﬁnger class or the helix-turn-helix class. This classiﬁcation strongly
suggests that different motif patterns with different consensus sequences may share some local
structural regularities intrinsic to a family of different motifs corresponding to a speciﬁc class of
DNA-binding proteins.
In Section §2.4, we address the problem of modeling generic features of structurally but not
textually related DNA motifs, that is, motifs whose consensus sequences are entirely different, but
nevertheless share “meta-sequence features” reﬂecting similarities in the DNA binding domains of
their associated protein recognizers. We present MotifPrototyper, a proﬁle hidden Markov Dirichlet-
multinomial (HMDM) model that is able to capture regularities of the nucleotide-distribution pro-
totypes and the site-conservation couplings typical to a particular family of motifs that correspond
to regulatory proteins with similar types of structural signatures in their DNA binding domains.
Central to this framework is the idea of formulating a proﬁle motif model as a family-speciﬁc struc-
tured Bayesian prior model for the PWMs of motifs belonging to the family being modeled, thereby
relating these motif patterns at the meta-sequence level.
The HMDM model assumes that positional dependencies within a motif are induced at a higher
level among a ﬁnite number of informative Dirichlet priors, rather than directly between the position-
speciﬁc distributions (which are generally set to be multinomials) of the nucleotides of the sites
inside a motif. Under this framework, one can explicitly capture meta-sequence features, such
as different conservation patterns of nucleotide distribution (e.g., being homogeneous or heteroge-
neous), and the 1st-order Markov dependencies of such patterns between adjacent sites. In general,
the HMDM model can be used to formally encode prior knowledge about the intrinsic structure of a
family of different motifs sharing meta-sequence features, by learning the parameters of the model
from experimentally identiﬁed motifs of the family. This can be done by using a stochastic EM al-
gorithm to compute the empirical Bayes estimate of the parameters. The result is a family-speciﬁc
Bayesian proﬁle model that implicitly encodes meta-sequence features shared in this family.
18

1.3 Technical Results of This Thesis
We then show how the family-speciﬁc proﬁle HMDMs, or MotifPrototypers, can be used to
classify aligned multiple instances of motifs into different classes each corresponding to a certain
class of DNA-binding proteins; and most importantly, how a mixture model built on top of multiple
proﬁle models can facilitate a Bayesian estimation of the PWM of a novel motif. The Bayesian
estimation approach connects biologically identiﬁed motifs in the database to previously unknown
motifs in a statistically consistent way (which is not possible under the single-motif-based repre-
sentations described previously) and turns de novo motif detection, a task conventionally cast as an
unsupervised learning problem, into a semi-unsupervised learning problem that makes substantial
use of existing biological knowledge.
A recent paper by Barash et al. proposes several expressive Bayesian network representations
(e.g., tree network, mixture of trees, etc.) for motifs, which are also intended for modeling de-
pendencies between motif sites [Barash et al., 2003]. An important difference between these two
approaches is that, in Barash’s Bayesian network representations, the site-dependencies are modeled
directly at the level of site-speciﬁc nucleotide distributions in a “sequence-context dependent” way;
whereas in the HMDM model, the site-dependencies are modeled at the level of the prior distribu-
tions of the site-speciﬁc nucleotide-distributions in a “conservation-context dependent” way. Thus,
Barash’s motif models have one-to-one correspondence with particular motif consensus patterns,
and need to be trained on an one-model-per-motif basis. On the other hand, the HMDM model
corresponds to a generic signature structure at the meta-sequence level; it is not meant to commit
to any speciﬁc consensus motif sequence, but aims at generalizing across different motifs bearing
similar conservation structures. In terms of the resulting computational task in de novo motif de-
tection, Barash’s model needs to be estimated in an unsupervised fashion and makes no use of the
biologically identiﬁed motifs in the database, whereas the HMDM model helps to turn the model
estimation task into a semi-unsupervised learning problem that draws a connection between novel
motifs to be found and the biologically identiﬁed motifs via a shared Bayesian prior, so that the pat-
terns to be found are biased toward biologically more plausible motifs. It is interesting to note that
19

1.3 Technical Results of This Thesis
these two approaches are complementary in that Barash’s models provide a more expressive likeli-
hood model of the motif instances, and the HMDM model can be straightforwardly generalized to
deﬁne a prior distribution for these more expressive models (e.g., replacing the Markov chain for
the prototype sequence in the HMDM model with a tree model and/or introducing Dirichlet mixture
priors for the parameters of Barash’s models).
1.3.1.2
Bayesian HMM for motif organization
In complex multi-cellular organisms such as higher eukaryotes, the distribution of motif strings
in the genome often follows a general principle called modular organization. That is, the motifs
that are involved in regulating the expression of a given gene are not distributed uniformly and at
random in the regulatory region of the gene. Instead, they are organized into a series of discrete se-
quence regions called cis-regulatory modules, each of which controls a distinct aspect of the gene.
Within each module certain combinations of motifs occur with increased frequency; these motifs
are capable of integrating, amplifying, or attenuating multiple regulatory signals via combinatorial
interaction with multiple regulatory proteins. This architecture is somewhat analogous to the gram-
matical rules we use to synthesize natural language from words. A motif detection algorithm that
ignores these syntactic rules often fails to correctly score true signals in a motif-dense region but on
the other hand is sensitive to false positives in the background region.
Taking an approach that has been widely adopted in many language and sequence segmentation
problems, we assume that underlying each sequence of nucleotides is a 1st-order hidden Markov
model, whose realizable state sequences correspond to segmentations of the DNA sequence. For
states corresponding to motif sites, the PWM of the corresponding motif is used to deﬁne the emis-
sion probabilities of observed nucleotides. For a non-motif state, it is assumed that probability of
the corresponding nucleotide is kth-order Markovian. What is unique about this specialized HMM
model, which we refer to as CisModuler, is the design of the state space of the hidden variables,
which corresponds to a rich set of possible functional annotations of each position in the transcrip-
tional regulatory sequences; and the state-transition scheme, which encodes the stochastic syntactic
20

1.3 Technical Results of This Thesis
rules of the CRM organizations of motifs known from the literature. Also somewhat novel is that
this model is trained in a semi-unsupervised fashion, from unlabeled sequences under a Bayesian
prior centered around empirical guesses of state transition probabilities. Thus, soft controls over the
distances between motif instances and motif modules, and over their dependencies, can be imposed
based on empirical knowledge from some reasonable sources (e.g., domain experts, literature, etc.),
and, due to the Bayesian approach, are subject to dominance by (rather than over) the evidence
when the study data is abundant.
1.3.1.3
The LOGOS model
A combination of the MotifPrototyper and CisModuler models, using the product rule of joint prob-
ability in a graphical model, leads to a novel Bayesian model that is signiﬁcantly more expressive
than any extant motif detection model. It is referred to as LOGOS, for integrated LOcal and GlObal
motif Sequence model.2 In LOGOS, the functional annotations of a DNA sequence that determine
the motif locations and modular structures are determined by a CisModuler HMM model; but the
emission probabilities of the motif states, or the PWMs of the motifs, are assumed to be generated
from the MotifPrototyper model or a mixture of MotifPrototypers, whereby prior knowledge re-
garding both global motif organization and local motif structure is incorporated. As in other recent
motif models, the background model used by LOGOS is a local 3rd-order Markov model. Under
the trained prior models, LOGOS performs de novo motif detection in a semi-unsupervised fashion.
Note that LOGOS deﬁnes a very general framework for modeling gene regulatory sequences,
using a modular graphical model. Each module can be designed separately to model different
aspects of the motif properties and can be updated without overhauling the whole model. The
Bayesian missing data problem associated with LOGOS is a challenging computational problem
that cannot be handled by extant exact inference algorithms. Nevertheless, the modular structure
of the LOGOS model motives a divide-and-conquer approach for approximate inference using the
2Not to be confused with ‘logo,’ a graphic representation of an aligned set of biopolymer sequences ﬁrst introduced by
Tom Schneider [Schneider and Stephens, 1990] to help in visualizing the consensus and the entropy (or “information”)
patterns of monomer frequencies. A logo is not a motif ﬁnding algorithm, but is often used as a way to present motifs
visually.
21

1.3 Technical Results of This Thesis
GMF algorithm (described shortly). GMF essentially couples local exact inference computations
for each submodel of LOGOS using an iterative procedure, and leads to a variational approximation
to the Bayesian estimation. The theoretical and algorithmic issues of variational inference in general
and of GMF in particular are addressed in Chapter 4.
Thanks to the ﬂexibility of assembling a full motif model with different combinations of sub-
models under the LOGOS framework, several variants of the LOGOS model that differ in model
expressiveness (e.g., MotifPrototyper + CisModuler, PWMs + uniform global model, etc.) are con-
structed to examine the performance gain (or loss) due to different model components. There is
strong evidence that improvements introduced in this thesis on both the local aspect (i.e., Motif-
Prototyper over the independent PWMs) and the global aspect (i.e., CisModuler over the uniform
model) of the motif model improve performance. Due to the lack of a sufﬁcient number of well
annotated human regulatory sequences for model evaluation, validations are primarily conducted
on yeast and Drosophila DNA sequences. It is evident that on both the regulatory sequences of
yeast and those of Drosophila—whose sizes and complexity are comparable to that of human—the
LOGOS model outperforms the popular MEME and AlignACE algorithms.
1.3.2
A Non-Parametric Bayesian Model for Single Nucleotide Polymorphisms
The problem of inferring haplotypes from genotypes of single nucleotide polymorphisms can be
formulated as a mixture model, where the mixture components correspond to the haplotypes in
the population. The size of the pool of haplotypes is unknown, and biologically, a parsimonious
bias toward a more compact haplotype reconstruction (i.e., a pool with smaller number of distinct
population haplotypes sufﬁcient for explaining the genotypes) is desired. Thus methods for ﬁtting
the genotype mixture must crucially address the problem of estimating a mixture with an unknown
number of mixture components and the parsimony bias. Chapter 3 presents a Bayesian approach
to this problem based on a nonparametric prior known as the Dirichlet process [Ferguson, 1973],
which attempts to provide more explicit control over the number of inferred haplotypes than has
been provided by the statistical methods proposed thus far. The resulting inference algorithm has
22

1.3 Technical Results of This Thesis
commonalities with parsimony-based schemes.
In the setting of ﬁnite mixture models, the Dirichlet process—not to be confused with the
Dirichlet distribution—is able to capture uncertainty about the number of mixture components [Es-
cobar and West, 2002]. The basic setup can be explained in terms of an urn model, and a process
that proceeds through data sequentially. Consider an urn which at the outset contains a ball of a
single color. At each step we either draw a ball from the urn, and replace it with two balls of the
same color, or we are given a ball of a new color which we place in the urn, with a parameter
deﬁning the probabilities of these two possibilities. The association of data points to colors deﬁnes
a “clustering” of the data. As pointed out by Tavare and Ewens [1998], this process is not only a
mathematically convenient model to deal with uncertainty of the cardinality of a mixture model, but
it indeed corresponds to an interesting metaphor of “biological evolution without selection.”
To make the link with Bayesian mixture models, we associate with each color a draw from
the distribution deﬁning the parameters of the mixture components. This process deﬁnes a prior
distribution for a mixture model with a random number of components. Multiplying this prior by a
likelihood yields a posterior distribution. In Chapter 5, Markov chain Monte Carlo algorithms are
developed to sample from the posterior distributions associated with Dirichlet process priors.
The usefulness of this framework for the haplotype problem should be clear—using a Dirich-
let process prior we in essence maintain a pool of haplotype candidates that grows as observed
genotypes are processed. The growth is controlled via a parameter in the prior distribution that cor-
responds to the choice of a new color in the urn model, and via the likelihood, which assesses the
match of the new genotype to the available haplotypes. This latter point also manifests an advantage
of the probabilistic formalism in that it is straightforward to elaborate the observation model for the
genotypes to include the possibility of errors. Trading off these errors against the size of the pool of
haplotypes can be gauged in a natural and statistically consistent way. Overall, the Dirichlet process
mixture naturally imposes an implicit bias toward small ancestral pools during inference (reminis-
cent of parsimony methods), and does so in a well-founded statistical framework that permits errors.
We call the this non-parametric Bayesian model DP-haplotyper.
23

1.3 Technical Results of This Thesis
The state-of-the-art algorithm for haplotype inference is the algorithm known as “PHASE.”
The performance of DP-haplotyper is equivalent to PHASE on the easier phasing problems that we
study, and improves on PHASE for the hardest problem; also DP-haplotyper requires less compu-
tation time. It also provides an upgrade path to models that permit recombination and incorporate
pedigrees as we outline in section §3.3, and can potentially generalize to linkage analysis and other
population genetics problems. Thus, DP-haplotyper serves as a promising building block for more
expressive models necessary for more complex problems.
1.3.3
The Generalized Mean Field Algorithms for Variational Inference
A critical limitation of using sophisticated probabilistic models for complex problems has been
the time and space complexity of the inference and learning algorithms. For example, to predict
motif locations and estimate motif PWMs under the LOGOS model, one has to manipulate (e.g.,
marginalize) a posterior distribution over the Cartesian product of a continuous state space and a
discrete one, both of very high dimension. Such computations are prohibitively expensive for any
exact algorithms. Although applying Monte Carlo algorithms is possible, efﬁciency and perfor-
mance concerns motivated us to pursue deterministic approximation methods based on a variational
calculus technique.
In Chapter 4, we present a class of generalized mean ﬁeld algorithms for approximate inference
in exponential family graphical models. GMF is analogous to cluster variational methods such
as generalized belief propagation (GBP). While those (GBP) methods are based on overlapping
clusters of variables in the model to deﬁne local marginals to be approximated and messages to
be exchanged among local marginals, GMF is based on nonoverlapping variable clusters. Unlike
the cluster variational methods, GMF is proved to converge to a globally consistent set of cluster
marginals and a lower bound on the likelihood, while providing much of the ﬂexibility associated
with cluster variational methods.
Given an arbitrary decomposition of the original model into disjoint clusters, the GMF algo-
rithm computes the posterior marginal for each cluster given its own evidence and the expected
24

1.3 Technical Results of This Thesis
sufﬁcient statistics, obtained from its neighboring clusters, of the variables in the cluster’s Markov
blanket (to be deﬁned in the sequel) — thence referred to as the Markov blanket messages. The al-
gorithm operates in an iterative, message-passing style until a ﬁxed point is reached. We show that
under very general conditions on the nature of the inter-cluster dependencies, the cluster marginals
retain exactly the intra-cluster dependencies of the original model, which means that the inference
problem within each cluster can be solved independently of the other clusters (given the Markov
blanket messages) by any inference method.
One way to understand the algorithm is to consider a situation in which all the Markov blanket
variables of each cluster are observed. In that case, the joint posterior decomposes:
p(xC1, . . . , xCn|xE) =
Y
i
p(xCi|MB(xCi), xEi,Ci),
where MB(xCi) denotes the Markov blanket of cluster Ci, and xEi,Ci denotes the evidence node
within cluster i. GMF approximates this situation, using the expected Markov blanket (obtained
from neighboring clusters) instead of an observed Markov blanket and iterating this process to
obtain the best possible “self-consistent” approximation.
In its use of expectations in messages between clusters, GMF resembles the expectation propa-
gation (EP) algorithm [Minka, 2001], but in the basic EP algorithm the messages convey the inﬂu-
ence of only a single variable. In providing a generic variational algorithm that can be applied to a
broad range of models with convergence guarantees, GMF resembles VIBES [Bishop et al., 2003],
whose original version was based on a decomposition into individual variables, and later generalized
to allow more coarse-grained disjoint decompositions similar to what we used for GMF [Bishop and
Winn, 2003]. Thus GMF is a generic algorithm suitable for approximate inference in large, complex
probability models.
Disjoint clusters have another virtue as well, which is explored in the second half of Chapter 4
— they open the door to a role for graph partitioning algorithms in choosing clusters for inference.
We provide a preliminary formal analysis and a thoroughgoing empirical exploration on how to
choose a good partition of the graph automatically using graph partitioning algorithms, so that the
25

1.4 Thesis Organization
entire GMF inference algorithm can be implemented in a fully autonomous way, with little or no
human intervention. We present a theorem that relates the weight of the graph cut to the quality of
the bound of GMF approximation, and study random graphs and a variety of settings of parameter
values. We compare several different kinds of partitioning algorithms empirically and the results
turn out to provide rather clear support for a clustering algorithm based on minimal cut, which is
consistent with implications drawn from the formal analysis.
The combination of GMF inference with graph partitioning based on combinatorial optimiza-
tion make it possible to develop truly turnkey algorithms for distributed approximate inference with
bounded performance.
1.4
Thesis Organization
The thesis stands at the intersection of several areas, namely, computer science, statistics, molecular
biology and genetics, and draws heavily on statistical machine learning, Bayesian statistics, opti-
mization theory, graph theory, and various biology-related sub-areas. Nonetheless, the reader is not
assumed to have a thorough background in any of these areas, but a general knowledge of the basic
concepts and techniques (e.g., discrete and continuous probability, EM algorithms, etc.) and I have
made some effort to make the thesis readable to a general audience in machine learning, statistics,
and computational biology.
Chapters 2-5 present the main contributions in this thesis. Chapters 2, 3, and 4 are self-contained
and can be read separately from the rest, whereas Chapter 5 should be read in the context of Chap-
ters 2 and 3. Chapter 2 describes a modular parametric Bayesian model for motif detection in
complex genomes. Chapter 3 presents a non-parametric Bayesian model for inferring the haplo-
types of SNPs in a population. Chapter 4 presents a generalized mean ﬁeld theory and algorithm
for variational inference in exponential family graphical models (to be deﬁned in the sequel) and its
application to motif detection using the models developed in Chapter 2. Chapter 5 provides Monte
Carlo algorithms for inferring motifs and haplotypes based on models in Chapters 2 and 3.
Those readers most interested in novel motif detection techniques as well as a detailed overview
26

1.4 Thesis Organization
of extant methods are advised to read Chapter 2 ﬁrst and then chapter 4 and section 2 of Chapter
5. Those interested in new models for haplotype inference should start with Chapter 3 and continue
to sections 3-5 of Chapter 5. Those interested in approximate inference theory and algorithms are
advised to read Chapter 4, and then Chapter 2 as an instance of large-scale application.
Chapter 6 summarizes the results of this thesis, draws a few conclusions and presents a set of
open questions and directions for further investigation.
Some of the material in this thesis has appeared before in [Xing et al., 2003a; Xing et al., 2003b;
Xing et al., 2004a; Xing et al., 2004c; Xing et al., 2004b; Xing and Karp, 2004].
27

Chapter 2
Modeling Transcriptional Regulatory
Sequences for Motif Detection
— A Parametric Bayesian Approach
Motifs are short recurring string patterns scattered in biopolymer sequences such as DNA and
proteins. The characteristic sequence patterns of motifs and their locations often relate to important
biological functions, such as serving as the cis-elements for gene regulation or as the catalytic sites
for protein activity. The identiﬁcation of motif sites within biopolymer sequences is an important
task in molecular biology and is essential in advancing our knowledge about biological systems.
It is well known that only a small fraction of the genomic sequences in multi-cellular higher or-
ganisms constitute the protein coding information of the genes (e.g., only 1.5% for human genomes
[Alberts et al., 2002]), whereas the rest of the genome, besides playing purely structural roles such
as forming the centromeres and telomeres of the chromosomes, contains a large number of short
DNA motifs that make up the immensely rich codebook of the gene regulation program, known as
the cis-regulatory system [Blackwood and Kadonaga, 1998; Davidson, 2001]. It is believed that
this regulatory program determines the level, location and chronology of gene expression, which
signiﬁcantly, if not predominantly, contributes to the developmental, morphological and behavioral
diversity of complex organisms [Davidson, 2001].
For proteins, functional speciﬁcities are usually realized by the presence of sporadic, but struc-
turally pivotal and/or biochemically reactive activity sites in the amino acid sequences [Lockless
28

2.1 Biological Foundations and Motivations
and Ranganathan, 1999; Li et al., 2003]. Therefore, proteins with very different overall sequences
and structures can fall into common functional categories, such as kinase and methylase, and bear
common polypeptide motifs (which constitute the activity sites) embedded in diverse sequence and
structural environments. Polypeptide motifs are regarded as signatures of unique biophysical and
biochemical functions. Due to the functional importance of polypeptide motifs to their host pro-
teins, they usually represent regions in protein sequences that resist drift and are prone to stabilizing
selection [Page and Holmes, 1998]. Thus, protein motifs provide important clues to understanding
the function and evolution of proteins and organisms.
Motifs can be identiﬁed via “wet lab” biological experiments, such as DNAase protection assay
(for DNA motifs) [Ludwig et al., 2000] and site-speciﬁc mutagenesis (for protein motifs) [Haldimann
et al., 1996], which are often very labor-intensive and time-consuming, but arguably most reliable in
the biological sense (although in some cases the truthfulness of in vitro assays or mutational pertur-
bation results with respect to biological reality is debatable). The best collections of experimentally
identiﬁed and veriﬁed motifs can be found in the TRANSFAC and the PROSITE databases [Win-
gender et al., 2000; Sigrist et al., 2002]. But since experimental motif identiﬁcation is often very
expensive and tedious, with the rapid accumulation of genomic sequence information from more
and more species, advances in molecular biology call for the development of more cost-effective,
computation-based methods for motif detection directly from the sequence data. In this chapter, we
review previous advances and extant methods in this direction and present a new Bayesian approach
we developed. Some of the material in this chapter has appeared before in [Xing et al., 2003a;
Xing et al., 2004b; Xing and Karp, 2004]. To simplify our exposition, we use DNA motif detection
as a running example, but it should be clear that the models we present are readily applicable to
protein motifs.
2.1
Biological Foundations and Motivations
Transcription, the process of making a single-stranded RNA molecule using one of the two DNA
strands of a gene sequence as a template, is exquisitely but robustly controlled by the interactions
29

2.1 Biological Foundations and Motivations
between the transcription factors that bind the cis-regulatory elements in DNA, the basal transcrip-
tional apparatus, additional co-factors, plus the inﬂuences from the chromatin structures (Fig. 1.1).
An initial step in the analysis of the function and behavior of any gene is the identiﬁcation of ge-
nomic regions that might harbor the cis-regulatory elements, and the elucidation of the identities
and organization of these elements.
Figure 2.1: Motif recognition and transcriptional regulation.
DNA motifs can be recognized by speciﬁc regulatory proteins, which relay complex regulatory
signals to the basal transcriptional machinery made up of an RNA polymerase and general tran-
scriptional factors via physical interactions, and accordingly turn on/off or ﬁne-tune the expression
of a gene [Ptashne and Gann, 1997] (Fig. 2.1). The speciﬁc motif-protein recognition underly-
ing the physical foundation of transcription regulation suggests that there exists a unique structural
complementarity between each motif sequence and the corresponding protein recognizer [Stormo
and Fields, 1998; Stormo, 2000; Benos et al., 2002]. For a simple organism such as a bacterium,
the cis-regulatory systems usually contain a small number of motifs located closely proximal to
the transcription initiation sites of the genes [Alberts et al., 2002]. On the other hand, in complex
multi-cellular organisms such as higher eukaryotes, the distribution of motif sites in the genomic
sequences often follows a general principle called modular organization [Davidson, 2001]. The
top panel of Fig. 2.2 shows a diagram of the regulatory region of the Drosophila even-skipped
30

2.1 Biological Foundations and Motivations
(eve) gene. This gene is involved in establishing the body segmentation during Drosophila em-
bryogenesis by expressing itself in different parts of the early embryo, know as the stripes (mid-
dle panel, Fig. 2.2), at different times, to determine the developmental fate of the correspond-
ing stripes [Harding et al., 1989; Goto et al., 1989; Stanojevic et al., 1991; Small et al., 1996;
Sackerson et al., 1999; Fujioka et al., 1999]. For example, the ﬁrst two stripes shown in Fig. 2.2
will grow into the head of the animal, and the third one will become a pair of legs [Gilbert, 2003;
Alberts et al., 2002]. As shown in this diagram, the motifs that are involved in regulating the expres-
sion of this gene are not distributed uniformly and at random in the regulatory region of the gene.
Instead, they are organized into a series of discrete sequence regions called cis-regulatory mod-
ules (or CRMs), each of which controls a distinct aspect of the gene’s expression pattern, namely,
when, in which stripe, and in roughly how many copies it is to be transcribed [Davidson, 2001;
Michelson, 2002]. This general architecture applies to most transcriptional regulatory sequences in
complex organisms, but does not apply to simple uni-cellular biological systems [Davidson, 2001].
stripe 3/7
stripe 2
eve transcript
stripe 4/6
stripe 1/5
Figure 2.2: The Drosophila CRMs and their roles in early embryogenesis.
As mentioned, in silico motif detection is the task of identifying potential motif patterns from
DNA sequences using a pattern recognition program. However, unlike another pattern recognition
problem on DNA sequences — gene ﬁnding — which searches for “macroscopic” entities such
31

2.1 Biological Foundations and Motivations
as genes (or more precisely, exons of the genes) in the genome, motif ﬁnding focuses on “micro-
scopic” substring patterns embedded in a long stream of noisy background full of false positive
signals. To the best of our knowledge, most of the biologically veriﬁed motifs are very short (i.e.,
about 6 ∼30 monomers), stochastic (i.e., different instances of the same motif usually differ slightly
in sequence content, as detailed shortly in §2.2.1), and poorly structured (i.e., they contain no sub-
structure bearing universal sequence signatures such as the intron-exon junctions for genes). Thus,
a coarse-grained model, such as a generic HMM that captures a universal intron/exon boundary
signature and the overall nucleotide frequencies of coding sequences, as used in the GENESCAN
program [Burge and Karlin, 1997], is infeasible for detecting small and very diverse motif signals.
More speciﬁc models for short sequence patterns, which correspond to regulatory proteins that bear
unique functions, are necessary to represent and search for DNA motifs.
What distinguishes a motif sequence from other random patterns in the background? Besides
the fact that a motif has a recurring consensus polynucleotide pattern, numerous studies of the
biophysical mechanisms of DNA-protein binding underlying the cis-trans regulatory interactions
reveal that a typical binding protein (e.g., a transcription factor with helix-turn-helix binding motifs
or tandem zinc-ﬁngers) only interacts with a DNA motif through a few highly speciﬁc amino acid-
nucleotide interactions, but is tolerant of variations in other sites [Stryer, 1995; Eisen, 2003]. It is
also well known that for higher eukaryotic organisms, motifs usually cluster into CRMs [Davidson,
2001]. Each CRM consists of a locally enriched battery of motifs occurring in a certain combination
and ordering, capable of enhancing or integrating multiple regulatory signals via concurrent physical
interaction with multiple TFs [Berman et al., 2002]. The spatial organization of CRMs in the
regulatory regions of the genes is also essential for coordinating gene activities. These features are
not directly reﬂected in the composition or consensus of a sequence pattern, and therefore can be
referred to as meta-sequence features.
The meta-sequence features of motif structure and motif organization, which are believed to be
crucial in distinguishing biologically meaningful motifs from a random background or trivial re-
curring patterns, have raised signiﬁcant challenges to conventional motif-ﬁnding algorithms, most
32

2.2 Problem Formulation
of which rely on models that describe motifs only at their sequence level and use simplifying in-
dependence assumptions that decouple potential associations among sites within each single mo-
tif and among multiple instances of motifs [Bailey and Elkan, 1995a; Bussemaker et al., 2000;
Hughes et al., 2000; Liu et al., 2001; Gupta and Liu, 2003]. Therefore, although there is much
success for motif detection on short, well curated bacterial or yeast gene regulatory sequences using
extant methods, generalization to longer, more complex and weakly characterized input sequences
such as those from higher eukaryotic genomes seems less immediate [Papatsenko et al., 2002;
Rajewsky et al., 2002]. A recent survey by Eisen [2003] raises concerns over the inability of some
contemporary motif models to incorporate biological knowledge of global motif distribution, motif
structure and motif sequence composition.
2.2
Problem Formulation
2.2.1
Motif Representation
To formulate the motif detection problem, we begin with a brief discussion on how to represent a
motif pattern. The representations of motif patterns largely fall into two categories: deterministic
representations, and stochastic representations.
For concreteness, Fig. 2.3 shows an example of a stretch of regulatory DNA sequence that
contains instances of multiple motifs. All the sub-strings highlighted with the same color in this
example correspond to the binding sites that can be recognized by the same TF. The simplest way
to represent a motif pattern corresponding to a TF is to consider each motif as a “word” — a de-
terministic substring pattern. However, as shown in Fig. 2.3, the instances of a motif are merely
“similar,” but not identical to each other. Thus some ﬂexibility is needed to accommodate dis-
crepancies among instances of the same motif. Usually, biologists record a motif pattern using
a multiple alignment of all the instances of a motif (Fig. 2.3a). An inspection of the alignment
shown in Fig. 2.3a suggests that the word “TTTTTATG” may be a reasonable representation of this
motif because it records the most frequent nucleotide at each column of the alignment (although
nucleotides “T” and “A” draw a tie at the 6th column). A word derived from a multiple alignment
33

2.2 Problem Formulation
in this way is called a consensus of the motif [Stormo, 2000]. In using a consensus sequence to
match additional instances of the motif it represents, some deviations, such as k mismatches with
the consensus (where k is a small integer compared to the length of the consensus, say, m), are
usually allowed between the instances and the consensus. A regular expression — in the foregoing
case, “TTTTTXTG”, where “X” means “don’t care” — is another popular deterministic represen-
tation. It can be used to restrict the allowable mismatches to certain positions when matching for
motif instances [Mehldau and Myers, 1993]. There have been several approaches for motif de-
tection directly based on word enumeration [van Helden et al., 1998; Sinha and Tompa, 2000;
Bussemaker et al., 2000], some of which will be reviewed in the sequel.
(a)
(b)
Figure 2.3: A close-up of motif instances and CRMs in a Drosophila sequence.
Alternative to the deterministic representations, due to the stochastic nature of motif patterns,
it is also natural to consider motif instances as samples drawn from a stochastic representation,
which usually corresponds to a generative probabilistic model. For example, a motif can be rep-
resented by a position weight matrix [Cardon and Stormo, 1992; Hertz and Stormo, 1996], which
records the nucleotide frequencies at each column of the alignment. Pictorially, a motif pattern
can also be represented by a sequence logo [Schneider and Stephens, 1990], in which the height
34

2.2 Problem Formulation
of each column corresponds to the degree of conservativeness, measured by the entropy of the nu-
cleotide distribution at a column of the motif alignment; the height of each character in a column
relates to the relative frequency of the respective nucleotides in the corresponding position in the
motif (Fig. 2.3b). With a stochastic motif representation, one can rank the “strength” of matches
of candidate motif instances to a given motif representation with a score that bears probabilistic or
information-theoretic interpretation, such as likelihood or log odds [Stormo, 2000]. In this thesis,
we focus on probabilistic representations of motifs and explore models and algorithms for learning
and prediction under such representations.
2.2.2
Computational Tasks for In Silico Motif Detection
The term “motif detection”, or “motif ﬁnding”, has been heavily loaded in the literature, often
with ambiguous meanings in terms of the exact nature of the intended computational task. To
avoid possible confusion in the forthcoming exposition, in the following we make explicit three
distinct, but related, computational tasks underlying a typical motif detection problem, and assign a
technically unambiguous handle to each.
First, given a set of experimentally identiﬁed instances of a certain motif (i.e., all the DNA seg-
ments elucidated from a DNAase-protection assay for a speciﬁc DNA-binding regulatory protein),
we call the task of extracting a motif representation, or a motif model, from such a set, motif train-
ing. In machine learning terminology, motif training can be understood as a supervised learning
problem, and the aforementioned set of instances of the motif is called a training set. As elaborated
in the sequel, depending on the choice of motif representation, different approaches can be used for
motif training, which typically begins with a multiple alignment of all the instances in the training
set, followed by speciﬁc procedures to learn the representations, such as a regular expression, a
consensus, or a probabilistic model, from the resulting alignment. In particular, when a motif patten
is represented as a probabilistic model (e.g., the local models in the sequel), motif training boils
down to parameter estimation of the probabilistic model.
Second, given a model or a representation of a known motif, the task of searching for the
35

2.2 Problem Formulation
presence of the sites of this motif in an unannotated set of sequences via computational means is
called motif scan. Frequently, generalization to simultaneous multiple-motif scan is needed. In
many combinatorial motif detection algorithms, motif scan is typically formulated as a “k-m string
matching” problem, that is, ﬁnding all substrings of length m (i.e., m-mers, where m is the length of
the given motif pattern) that has at most k mismatches with the motif pattern. Accordingly, the motif
patterns to be scanned for are represented by their respective consensus sequences. From a machine
learning point of view, in the simplest case, motif scan can be formulated as a standard classiﬁcation
problem for all substrings in the sequences according to a deterministic or probabilistic motif model.
Under a more sophisticated formulation, in which the contextual information and the dependencies
among instances are to be considered, an explicit locational distribution model of motifs (e.g., the
global model in the sequel) can be used, and motif scan can be cast as the problem of probabilistic
inference for latent random variables in the model that indicate the locations of the motifs.
Finally, given only a set of unannotated sequences potentially containing previously unchar-
acterized motifs (i.e., motifs whose representations are not known), the task of learning the repre-
sentations of these unknown motifs and at the same time locating all the instances of these motifs
in the study sequences is referred to as de novo motif detection. Under a combinatorial setting,
which typically adopts a deterministic representation of a motif pattern, de novo motif detection
often amounts to ﬁnding all over-represented m-mers from the sequences, where m is the length of
the anticipated motifs. Some parameters are needed to qualify a match (e.g., the k in the aforemen-
tioned k-m score) and to determine how many over-represented patterns are to be accepted (e.g.,
a cutoff value for the minimal number of matches) [Papatsenko et al., 2002]. Under a probabilis-
tic framework, which will be studied in detail in this thesis, one can view de novo motif detection
as a coupled missing value inference and parameter estimation problem, often formulated as an
unsupervised learning problem [Bailey and Elkan, 1995a].
As we will elaborate shortly, besides avoiding possible ambiguities about what one means by
“motif detection”, the foregoing clariﬁcation of the computational tasks underlying the motif detec-
tion problem also bodes well for the logic of a modular formulation of the in silico motif detection
36

2.2 Problem Formulation
problem, and a divide-and-conquer strategy to solve such problems. Just as a quick overview, it is
not difﬁcult to realize that the models (and the algorithms) for motif training and motif scan, respec-
tively, can be viewed as submodels (and subroutines) of the more difﬁcult de novo motif detection
problem in that, computationally, de novo motif detection often amounts to an iterative procedure
(modulo some technical issues regarding how to jump start the iteration, whether it will ever con-
verge, etc.) that alternates between: 1) scanning for instances of a motif using a newly-trained motif
model, and, 2) training an updated motif model using the newly-scanned set of motif instances.
Hence, the full model and the algorithm for de novo motif detection is in essence a combination of
the two models underlying motif scan and motif training, respectively. This modular logic indeed
underlies the two main families of algorithms currently in use for de novo motif detection under
various model settings, namely, expectation-maximization (EM) (e.g., [Lawrence and Reilly, 1990;
Bailey and Elkan, 1995a]) and Monte Carlo (MC) (e.g. [Lawrence et al., 1993; Liu et al., 2001]) al-
gorithms. In the sequel, we will adopt this logic to analyze several extant motif models and present
new models and algorithms for motif detection.
2.2.3
General Setting and Notation
Now we introduce the necessary notation for the formal presentation. We denote a regulatory DNA
sequence by a character string y = (y1, . . . , yT ) ∈NT , where N = {A,T,C,G} denotes the set of
all possible nucleotides (nt) that make up a DNA sequence (for proteins, this set can be redeﬁned
as the set of all possible amino acids). An indicator string x signals the locations of the motif
occurrences (the range of x dependents on its speciﬁc deﬁnition and the model, see later sections
for details). Following biological convention, we denote the multi-alignment of M instances of a
motif of length L by an M × L matrix A, in which each column corresponds to a position or site
in the motif. The multi-alignment of all instances of motif k speciﬁed by the indicator string x in
sequence y is denoted by A(k)(x, y). We deﬁne a counting matrix h(A(k)) (or h(k)(x, y)) for each
motif alignment, where each column hl = [hl,1, . . . , hl,4]t is an integer vector with four elements
(the superscript t denotes vector or matrix transpose), specifying the number of occurrences of
37

2.2 Problem Formulation
each nucleotide at position l of the motif. (Similarly we deﬁne the counting vector hbg for the
background sequence y −A, where the somewhat abusive use of the minus sign means excluding
all motif sub-sequences in A from y.) We assume that the nucleotides at position l of motif k
admit a position-speciﬁc multinomial distribution (PSMD), θ
(k)
l
=[θ
(k)
l,1, . . . , θ
(k)
l,4]t. The ordered set
of position-speciﬁc multinomial parameters of all positions of motif k, θ(k) = [θ
(k)
1 , . . . , θ
(k)
L(k)], is
referred to as a position weight matrix. It is clear that the counting matrix h(k) corresponds to the
sufﬁcient statistics for estimating the PWM θ(k). Formally, the problem of motif training is that of
estimating θ(k) given the multiple alignment A(k), for each k; the problem of motif scan is that of
inferring x(n) given a sequence y(n) and θ(k), ∀n, k; and the problem of de novo motif detection is
that of inferring x={x(1), . . . , x(N)} and estimating θ={θ(1), . . . , θ(K)} simultaneously, given a set
of sequences y={y(1), . . . , y(N)}. For simplicity, we omit the superscript k (motif type index) of
the variable θ and the superscript n (sequence index) of the variables x and y wherever it is clear
from the context that we are focusing on a generic motif type or a generic sequence.
2.2.4
The LOGOS Framework: a Modular Formulation
Without loss of generality, assume that the locations of motifs in a DNA sequence, as indicated by
x, are governed by a global distribution model p(x|Θg, Mg), and for each type of motif, the nu-
cleotide sequence of all its instances (collected in an alignment matrix) jointly admits a local align-
ment model p(A(x, y)|x, Θl, Ml). Further assume that the background non-motif sequences are
modeled by a conditional model, p(y−A(y, x)|x, Θbg, Mbg), where the background nt-distribution
parameters Θbg are usually assumed to be estimated a priori from the entire sequence. The symbols
Θ[·] and M[·] stand for the parameters (e.g., the PWMs) and model classes (e.g., a product multi-
nomial model as described in the sequel) in the respective submodels. Thus, marginalizing over all
possible values of the indicator sequence x, the likelihood of a regulatory sequence y is:
p(y|Θ, M)
=
X
x
p(x|Θg, Mg)p(y|x, Θl, Ml, Θbg, Mbg)
=
X
x
p(x|Θg, Mg)p(A|x, Θl, Ml)p(y−A|x, Θbg, Mbg),
(2.1)
38

2.2 Problem Formulation
where A ≜A(x, y). Note that Θl here is not necessarily equivalent to the PWMs, θ, of the motifs,
but is a generic symbol for the parameters of a more general model of the aligned motif instances.
(E.g., in the HMDM model to be deﬁned shortly, Θl refers to the hyperparameters that describe a
distribution of PWMs.)
Equation (2.1) makes explicit the modular structure of the probabilistic framework for generic
motif models. The submodel p(x|Θg, Mg) captures properties such as the frequencies of different
motifs, the dependencies between motif occurrences, and the global organization of motif instances.
On the other hand, the submodel p(A|x, Θl, Ml) captures the intrinsic properties within motifs that
can help to improve sensitivity and speciﬁcity to genuine motif patterns. Depending on the value
of the latent indicator xt (e.g., motif or not) at each position t, yt follows different probabilistic
distributions, such as a speciﬁc nucleotide distribution of a particular position inside a motif or a
background distribution. This probabilistic architecture is named LOGOS, for integrated LOcal
and GlObal motif Sequence model.
As equation (2.1) suggests, the speciﬁc submodels in LOGOS can be designed separately, and
they are roughly aligned with our speciﬁcation of the actual computational tasks underlying the
motif detection problem. The local model alone sufﬁces to solve the motif training task, the global
model plus a given set of motif representations sufﬁces to answer the motif scan problem, and their
combination represents the de novo motif detection problem. Recall that the graphical model for-
malism facilitates a modular combination of heterogeneous submodels, using the property of the
product rule of the joint distribution. LOGOS is an instance of such a modeling strategy, and es-
sentially facilitates a bottom-up approach for solving the complex de novo motif detection problem,
by starting from relatively simpler subproblems. This strategy clearly exposes the main technical
issues involved in the motif detection problem, which helps in analyzing existing algorithms and
understanding their merits and limitations. It also enables one to design more sophisticated models
in a piecewise manner to address different aspects of the problem without being overburdened by
the complexity of the overall problem, and to envisage a straightforward path toward solving even
more complex problems, such as joint modeling of motifs and gene expression patterns, by using
39

2.3 An Overview of Related Work
existing or designing new models for each problem (now viewed as subproblems) separately, and
integrating them under the joint graphical model formalism.
2.3
An Overview of Related Work
In the following, we brieﬂy review some representative models for motif detection in the literature.
We will describe these models from the LOGOS point of view, by making explicit the background,
local and global components of the model, even though almost none of the models were originally
constructed and described in such a way, so that the pros and cons of these models can be clearly
understood and compared.
2.3.1
Background Models
2.3.1.1
The models
It is generally assumed that the sequences outside the motifs have diverged sufﬁciently to be mod-
eled as random background. Thus a simple but very popular model for all the non-motif nucleotides
in the the background sequence is an iid multinomial model:
p(y −A|x, Θbg) =
Y
t∈B
Y
i∈N

θbg
I(yt,i) =
Y
i∈N

θbg
hbg,i,
(2.2)
where B is the set of indices of the background positions, and θbg denotes the vector of multinomial
parameters of the background model, which is usually directly computed as the overall nucleotide
frequency distributions of the entire input sequence, assuming that motif instances are sparse in the
sequence and thus would not bias the estimated frequencies [Bailey and Elkan, 1995a; Hughes et
al., 2000; Liu et al., 2001]. (This assumption is somewhat unwarranted in some early literature in
which the input sequences are usually assumed to be sets of 100 ∼200-mers, each containing, say,
one motif, which suggests a quite signiﬁcant 10% motif coverage! e.g., [Cardon and Stormo, 1992;
Lawrence et al., 1993])
Several recent papers have stressed the importance of using a richer background model for the
non-motif sequences [Thijs et al., 2001; Liu et al., 2001; Huang et al., 2004]. In particular, a number
40

2.3 An Overview of Related Work
of higher-order Markov models have been explored by various authors and reportedly contribute to
notable improvements in the performance of motif scan and de novo motif detection. Under a global
kth-order Markov model for non-motif nucleotide sequences, the conditional probability of a single
nucleotide j at site t is contingent on the k preceding bases following the usual Markov dependency
deﬁnition
p(Yt = i|Xt = bg) = p(Yt = i|yt−1, . . . , yt−k) = fi(yt−1, . . . , yt−k).
Thus the probabilities of all the background can be computed by enumerating all (k + 1)-tuples
of nucleotides in the entire sequence y (note that these probabilities need to be computed only
once, and then stored for repeated references during probabilistic inference). The total time for this
operation is O(T), where T is the total length of the input sequences. One can also use a local
kth-order Markov model, in which the conditional probability of a nucleotide i at position t, ft
i (·),
is estimated from a local window centered at position t.
2.3.1.2
The use of background models
As detailed in the sequel, one family of motif scan algorithms seek to score candidate sequence
segments for their similarity to a known motif pattern. The background model plays an important
role in formulating a good scoring function. For example, the standard likelihood ratio score for
candidate segment yt,t+L−1 at positions t to t + L −1 is computed as follows
rt =
p(yt,t+L−1|Θl, Ml)
p(yt,t+L−1|Θbg, Mbg).
(2.3)
A variant of the likelihood ratio score is the log odds score
lt = log rt = log p(yt,t+L−1|Θl, Ml) −log p(yt,t+L−1|Θbg, Mbg).
(2.4)
From these two scoring schemes, it is apparent that, even though the motif model, which deﬁnes
the probability of a motif segment, is the most important component in these scoring functions, a
good background model will help to improve the contrast of motif to background, and therefore the
discriminating power of rt or lt at each position.
41

2.3 An Overview of Related Work
Note that while probabilities are used in constructing the scoring functions, the scores them-
selves (e.g., likelihood ratio, log odds) cannot be interpreted statistically. Usually, they will be
compared against an ad hoc cutoff value to generate computational motif predictions, and choosing
the score cutoff values for each motif and background model is generally difﬁcult. This may have
contributed to the large number of false positive predictions seen in practice. To assess the signiﬁ-
cance for a set of predicted motif instances, Liu et al. [1995] developed a rank test that compares the
prediction results from the study data with those from control data generated by a random shufﬂe
of the study data. They applied a Wilcoxon signed rank test to the predictions made from paired
(concatenated) study and control data, and obtain a p value of the prediction from the study data
under the rationale that, under the null hypothesis, the motifs are equally likely to be solicited from
either the study or the control sequences. Huang et al. [2004] proposed a p-value based scoring
scheme, which computes the probability that the null (i.e., background) model can achieve a stan-
dard log-likelihood score for a candidate sequence segment at least as high as that of a signal (i.e.,
motif) model deﬁned by PWMs. They developed an exact algorithm based on probability generating
functions to compute the p-value for a general kth-order Markov background model with respect
to motif models represented by PWMs. The CREME program by Sharan et al. [2003] proposed
a number of closed-form statistical scores for assessing the signiﬁcance of single motif abundance
or abundance of a motif cluster (multiple spatially-close motifs) out of a subregion of a study se-
quence over that of the background sequences. Note that “abundance” (i.e., the number of motif
matches), rather than the score of the matches, is tested for signiﬁcance in the CREME program, and
it was demonstrated to be a competent method to scan for CRMs in higher eukaryotic transcription
regulatory sequences.
Generally, depending on the choice of grammatical models for global sequence annotation, the
background model can be plugged in as a conditional model for the background state and contribute
to various scoring functions for motif detection. For example, it can be used as an emission model
under a background state in the case of a HMM global model (see §2.5 for details). Rather than
contributing an r or l score, in these cases, the background model will contribute indirectly to the
42

2.3 An Overview of Related Work
posterior probability distribution of the indicator sequence x.
2.3.2
Local Models — for the Consensus and Stochasticity of Motif Sites
A local alignment model attempts to captures the consensus and the accompanied stochasticity of
the set of binding sites (i.e., motif instances) corresponding to a certain TF.
2.3.2.1
Product multinomial model
The position weight matrix introduced in §2.2.3 is the most commonly used representation for a
motif pattern in extant motif detection algorithms [Bailey and Elkan, 1995a; Hughes et al., 2000;
Liu et al., 2001; Frith et al., 2001; Liu et al., 2002; Gupta and Liu, 2003]. Statistically, a PWM can
be used to deﬁne a product multinomial (PM) model for every observed instance of a motif [Liu
et al., 1995]. Formally, given the PWM, θ = [θ1, θ2, . . . , θL], of a motif, the probability of an
observed instance of this motif, which corresponds to a row in the motif alignment matrix A, say,
Am = [Am,1, Am,2,, . . . , Am,L], is
p(Am|Θl) =
L
Y
l=1
Y
i∈N

θl,i
I(Am,l,i).
(2.5)
For an alignment of M motif instances, A = {Am}M
m=1, the joint probability of all motif
instances in A is
p(A|Θl) =
M
Y
m=1
p(Am|Θl) =
L
Y
l=1
Y
i∈N

θl,i
hl,i.
(2.6)
Recall that h ≡{hl,i} is the nucleotide count matrix associated with alignment A, thus, hl,i =
P
m I(Am,l, i).
The PM model inherently assumes that the nt-contents of positions within the motif are inde-
pendent of each other. Thus, a PWM only models independent statistical variations with respect to
a consensus pattern of a motif, but ignores potential couplings between positions inside the motif
— a limitation that often weakens its ability to discern genuine instances of a motif from a very
complex background that may harbor random recurring patterns, due to the low signal/noise ratios
reﬂected in the likelihood-based scores computed from the PM model.
43

2.3 An Overview of Related Work
Given a set of aligned instances of a certain motif (i.e., a training alignment), under the PM
model, the PWM of this motif can be obtained via maximal likelihood estimation (MLE), which is
equivalent to computing the nt-frequency at each motif position. If the training alignment contains
only a small number of motif instances, MLE tends to lead to a non-robust model (i.e., with a
high variance associated with the estimates of the model parameters), which tends to generalize
poorly to unseen instances of the same motif. For example, if a particular nucleotide does not
appear at a certain position among all the instances in the training alignment, possibly just because
the alignment is too small to be sufﬁciently representative, then every candidate instance from a
new dataset that bears this nucleotide at this position (but is otherwise highly consistent with the
motif consensus,) will be assigned a zero probability. This artifact is called overﬁtting in statistical
learning, and should be avoided when learning from a small training dataset. In the motif modeling
literature, the most popular remedy is to add to the actual count matrix h a uniform pseudo-count
matrix (i.e., all elements of the matrix are equal) [Lawrence et al., 1993; Bailey and Elkan, 1994],
which can be regarded as the nt-count from an imaginary set of “motif instances”. The column sum
of the pseudo-count matrix, typically set to 1, can be understood as the total number of “imaginary
motif instances” from which the “count” is obtained. The larger this number is, the more difﬁcult
to override the pseudo-counts with the actual counts from the training data.
Mathematically, incorporating uniform pseudo-counts into the MLE of a PWM is equivalent to
introducing a symmetric Dirichlet prior (Appendix A.1) for the values of each column of the PWM,
and the resulting motif model is also called a product Dirichlet (PD) model [Bailey and Elkan,
1995b; Liu et al., 1995]. Note that pseudo-counts or PD models are primarily used for smoothing,
rather than for explicitly incorporating prior knowledge about motifs, and the parameters are chosen
ad hoc.
2.3.2.2
Constrained PM models
Although there are some obvious limitations of PWMs, they have proved to be reasonably effective
in describing the set of sequences bound by a given TF and have shown considerable predictive
44

2.3 An Overview of Related Work
power [Stormo, 2000]. However, in an unsupervised de novo motif ﬁnding scenario where the
PWMs have to be estimated ab initio, the estimated PWMs under the PM model, or even with
pseudo-counts or symmetric Dirichlet priors, are sensitive to noise and random or trivial recurrent
patterns (e.g., poly-N or repetitions of short k-mers such as CpG islands). Furthermore, the PM
model is unable to capture potential position dependencies inside the motifs.
Various pattern-driven approaches have been developed to handle motifs with speciﬁc motif
patterns. For example, in the early Lawrence and Reilly paper [1990], the authors introduced con-
straints on the parameters of the PWM to enforce palindromicity. Frech et al.’s method [1993]
proposed to originate the PWMs from a highly conserved consensus core and then extend the core
in one or both directions. Some of the recent methods provide ad hoc ways of allowing motifs to
have two conserved blocks separated by a few background sites, such as splitting a “two-block”
motif into two coupled sub-motifs [Liu et al., 2001; Bailey and Elkan, 1995a]. The fragmentation
model of Liu et al. [1995] allows an arbitrary L < W positions in an aligned segment of width W
to constitute the conserved motif sites.
Note that in addition to the nt-frequencies represented by the matrix elements, PWMs can also
provide the information content proﬁle [Schneider et al., 1986] of the corresponding motif. The
information content (IC) at a position l in a motif is given by
Il = log2 |N| +
X
i∈N
log2 θl,i,
(2.7)
and can be thought as a measure of how conserved position l is.
Keles et al. [2003] noted that the PWMs describing motifs with very different nt-speciﬁcities
can have similar information proﬁles, and speculated that there is a direct relationship between the
structural footprint of a TF and the information content proﬁle of the corresponding motif. They
developed a method that explicitly enforces nt-biases, e.g., high versus low information contents at
various positions, when computing the MLE of the PWM from samples. They have proposed several
canonical information content patterns, such as the one with a U-shaped contour, or a bell-shaped
contour, to be plausible constraints for de novo motif detection, and have developed a sequential
45

2.3 An Overview of Related Work
quadratic programing method to solve the constrained optimization problem. A constrained EM
algorithm was developed by Kechris et al. [2004] to incorporate similar IC constrains for estimating
motif PWMs.
The IC-constraint approach represents a signiﬁcant advance in learning local motif models be-
cause it takes into consideration the commonalities shared among motifs of different TFs (with dif-
ferent nt-speciﬁcities), and reveals something intrinsic to biologically genuine motifs. However, it
deﬁnes a hard constraint that must be respected no matter how many actual motif instances are used
to estimate the PWM, and cannot be overridden when the IC of an abundant novel motif deviates
from the predetermined constraints.
2.3.2.3
Motif Bayesian networks
A recent article by Barash et al. [2003] proposed a family of more sophisticated representations
to capture richer characteristics of motifs. These representations are based on directed probabilis-
tic graphical models, i.e., Bayesian networks. Barash et al. suggested that a mixture of product
multinomial models (MPM),
pMP M (A|Θl) =
X
j
wjpP M (A|θ(j)),
(2.8)
where wj is the weight of the jth mixture component and θ(j) is the PM parameter of the jth mixture
component, can capture potential multi-modalities of the biophysical mechanism underlying the
protein-DNA interaction between a TF and its target motif sites. Under the MPM model, a motif
is characterized by multiple PWMs, each corresponding to a component PM model. Barash et
al. further proposed a tree-based Bayesian network capable of capturing pairwise dependencies
of nucleotide contents between nonadjacent positions within the motif. A natural combination of
the above two models leads to a more expressive model, a mixture of trees, which captures more
complex dependency characteristics of motifs. In a series of experiments with simulated and real
data, Barash et al. showed that these more expressive motif models lead to better likelihood scores
for motifs, and can improve the sensitivity and speciﬁcity of motif detection in yeast regulatory
sequences under a simple scenario of motif occurrence (i.e., at most one motif per sequence).
46

2.3 An Overview of Related Work
In principle, it is possible to construct even more expressive models for motifs by systematically
exploiting the power of graphical models, although ﬁtting more complex models reliably demands
more training data. Thus, striking the right balance between expressiveness and complexity remains
an open research problem in motif modeling.
2.3.3
Global Models — for the Genomic Distributions of Motif Sites
The local model of a motif pattern only creates aligned multiple instances of a motif, but does
not complete the generation of the observed sequence set, even with the addition of the background
model. It is necessary to have a set of “rules” that deﬁne where and how instances of one or multiple
motifs are embedded in the background sequence so that they can constitute a sort of “language”
or “program” interpretable by the TFs in a liquid solution environment, and in a TF composition
and concentration sensitive manner. In the LOGOS framework, these “rules” are encoded in the
global distribution model for the indicator variable sequence x that can specify the locations and
organization of all motif instances.
2.3.3.1
The oops and zoops model
The probabilistic model for de novo motif detection developed by Lawrence and Reilly in their
seminal 1990 paper [Lawrence and Reilly, 1990] assumes that each of the N input DNA sequences
contains exactly one binding site of the same TF. This assumption is an idealization of a scenario in
which a set of “co-regulated” genes are analyzed, and the co-regulation is induced by a single TF
that can bind to a unique motif site present in the regulatory region of each of the regulated genes.
Accordingly, this model is called a “one motif per sequence” (oops) model. Although hardly a
realistic model, the oops model has historical importance (and is still in use in many contemporary
programs) in that it provides a clean abstraction that helps in understanding the motif detection
problem and points out a direction for formulating and upgrading the global model. Formally, let
y(n) = {y
(n)
t }Tn
t=1, y
(n)
t
∈N, denote the data of the nth sequence with length Tn; and let X(n) ∈
{1, . . . , Tn −L + 1} denote the latent address variable of a motif with length L in sequence n (the
address of the motif is deﬁned as the position of the most proximal nucleotide in the motif instance,
47

2.3 An Overview of Related Work
w.r.t. the end of the study sequence). The oops model assumes that the location of the (only) motif
in each sequence admits a uniform distribution over all possible positions in the sequence. That is,
p(X(n) = t) =
1
Tn −L + 1.
(2.9)
Given x(n), the oops model assumes that nucleotides at positions not corresponding to the mo-
tif, hence falling into the background, are independently and identically distributed; whereas the
nucleotides of all positions within a motif instance jointly follow a local motif model (e.g., a PM
model as in [Lawrence and Reilly, 1990]). Motif instances in different input sequences are assumed
to be independent and identically distributed. Thus, given the PWM θ of the motif and θbg for the
background, and denoting the nt-count vector of the entire sequence y(n) by h(n), the joint proba-
bility distribution of the observed sequence y(n) and the latent addresses x(n) of the motif therein
is:
p(X(n) = t, y(n)|Θ = {θ, θbg})
=
p(X(n) = t)p(y(n)|X(n) = t, Θ = {θ, θbg})
=
1
Tn −L + 1
L−1
Y
l=0
Y
j∈N

θl,j
I(y(n)
t+l,j) ·
Y
j∈N

θbg,j
h(n)
bg,j,
=
1
Tn −L + 1
L−1
Y
l=0
Y
j∈N
h θl,j
θbg,j
iI(y(n)
t+l,j)
·
Y
j∈N

θbg,j
h(n)
j ,(2.10)
which leads to the following posterior distribution of the latent variable X(n):
p(X(n) = t|y(n), Θ)
=
QL−1
l=0
Q
j∈N

θl,j/θbg,j
I(y(n)
t+l,j)
PT−L+1
t′=1
QL−1
l=0
Q
j∈N

θl,j/θbg,j
I(y(n)
t′+l,j) .
(2.11)
Thus, the probability of position t being a motif start address is proportional to the likelihood ratio
of a sub-sequence of length L started at t being a motif sequence with respect to its probability of
being a background sequence, which is exactly the likelihood ratio score we described in §2.3.1.
A simple extension of the oops model is the zoops model, for “zero or one motif per sequence”,
adopted by Bailey and Elkan [1994] in their precursor of the MEME algorithm. As the name
suggests, this model is slightly more ﬂexible than oops. For each sequence, zoops introduces an
indicator variable Zn ∈{0, 1}, which indicates the presence (Zn = 1) or absence (Zn = 0) of
48

2.3 An Overview of Related Work
a motif instance in sequence n. The prior distribution of Zn can be deﬁned as a simple Bernoulli
distribution, Zn ∼Ber(α, 1 −α), and the indicator is independent and identically distributed for
each sequence. Under this setting, the conditional likelihood of the observed sequence is
p(y(n)|X(n) = t, Zn = 1, Θ = {θ, θ0})
=
1
Tn −L + 1
L−1
Y
l=0
Y
j∈N
h θl,j
θ0,j
iI(y(n)
t+l,j)
·
Y
j∈N

θ0,j
h(n)
j ,
p(y(n)|Zn = 0, Θ = {θ, θ0})
=
Y
j∈N

θ0,j
h(n)
j .
(2.12)
Thus the probability of having a motif at position t of sequence n is regularized by the prior
probability of motif presence,
p(X(n) = t, Zn = 1|y(n), Θ)
=
p(y(n)|X(n) = t, Zn = 1, Θ)p(X(n) = t)p(Zn = 1)
p(y(n)|Θ)
=
QL−1
l=0
Q
j∈N

θl,j/θ0,j
I(y(n)
t+l,j)
PT−L+1
t′=1
QL−1
l=0
Q
j∈N

θl,j/θ0,j
I(y(n)
t′+l,j) + (1−α)(Tn−L+1)
α
.
(2.13)
2.3.3.2
General uniform and independent models
Essentially, both the oops and zoops models are uniform (over all possible positions in a study
sequence) and independent (between motif instances and between study sequences) global models,
or in short, UI models, which are intended for simple and idealized motif-bearing sequences. It
is straightforward to generalize the baseline UI model to handle slightly more complex scenarios,
such as multiple motifs per sequence. For example, rather than allowing at most one motif per
sequence, some motif detection algorithms assume that a ﬁxed number of motif instances can be
present independently with uniform probability at all possible locations in a sequence [Bailey and
Elkan, 1995a]. That is, the joint distribution of the addresses of, say, M motif instances in a study
sequence n, x(n) = {x
(n)
1 , . . . , x
(n)
M }, can be written as p(x(n)) = QM
m=1 p(x
(n)
m ), where p(x
(n)
m =t)
is the prior probability of the mth motif instance at location t in sequence n, in this case, a uniform
distribution over all valid t’s and the same for all m = 1, . . . , M. It can be shown that the marginal
posterior probability of any one of the addresses under this setting, p(x
(n)
m |y(n), Θ), is proportional
49

2.3 An Overview of Related Work
to the likelihood ratio score of each sequence position (i.e., Eq. (2.11)), and the joint posterior of
all addresses is the product of the posterior probabilities of individual addresses, p(x(n)|y(n), Θ) =
Q
m p(x
(n)
m |y(n), Θ). This model is called an M motifs per sequence model (mops), and is used in
various contemporary algorithms, such as MEME. A drawback of this approach is the requirement
of a prespeciﬁed number of motif instances. An inaccurately supplied number will lead to either
signiﬁcant false positives, or false negatives, or both. Also problematic is that the mops model
ignores possible constraints on co-occurrences of motifs.
A slightly more sophisticated model for multiple motif locations per sequence is an iid Bernoulli
indicator model [Liu et al., 1995], which assumes that each host sequence y is associated with
a binary indicator sequence, x = (x1, . . . , xT ), xt ∈{0, 1}, where 0 signals background and 1
signals a motif starting at position t; each Xt is an independent Bernoulli random variable, xt ∼
Ber(α); and the Bernoulli parameter [α, 1 −α] follows a Beta prior. This model is essentially a
clustering model for all possible L-mers of the sequences, allowing any L-mer to be either a motif
or background.
Under both the mops and Bernoulli indicator models, there is no formal model constraint to pre-
vent having overlapping motif instances. Although overlapping motifs are possible in real genomic
sequences, the possibility of overloading every sequence position with multiple motif instances is
not desirable. Therefore, in practice, both models are augmented heuristically with an artiﬁcial non-
overlapping constraint, which requires that no L-mer is allowed to harbor, say, more than l motif
start positions, where 1 < l ≤L. This constraint is enforced by either rescaling the joint posterior
p(x|y) (as in an EM-based inference strategy for MEME [Bailey and Elkan, 1995a]), or simply
throwing away any overlapping motif samples (when using a Gibbs-sampling-based inference strat-
egy [Liu et al., 1995]). Nevertheless, these heuristics may result in inconsistencies between the
computed motif distribution and the one deﬁned by the model, and incur a sizable overhead due to
wasteful computations.
Despite their simplicity and some unwarranted heuristic assumptions, UI models appear to be
competent in motif scan and de novo motif detection for bacterial or simple yeast sequence sets, in
50

2.3 An Overview of Related Work
which the input sequences are usually short and enriched (e.g., pre-screened according to mRNA
co-expression). But some recent studies including our own experiments suggest that the correctness
of motif detection based on the UI assumptions starts to break down for less well pre-screened input
sequences or for those with clustered motif occurrences, such as the Drosophila gene regulatory
sequences [Berman et al., 2002]. Higher eukaryotic genomes indeed present a challenge to the
computational identiﬁcation of motifs because of their long non-coding regions and large number
of repeat elements.
2.3.3.3
The dictionary model
Bussemaker et al. proposed a novel formulation of the motif-ﬁnding problem, which is based on
word segmentation and dictionary construction [Bussemaker et al., 2000]. In their MobyDick algo-
rithm, they view the regulatory DNA sequences as sentences written in an unknown language built
from an alphabet of 4 characters (i.e., N = {A, T, G, C} as deﬁned previously), with no separators
between words. (In fact, some major human languages, such as Chinese and Japanese, are of this
kind, although using a much larger alphabet, e.g., > 104, for Chinese.) Under this framework,
the motif-ﬁnding problem can be cast as ﬁnding over-represented words from consecutive lists of
characters, and the algorithm boils down to an iterative procedure alternating between building up a
dictionary of words and estimating the values of parameters of a language model from a given word
segmentation (e.g., word frequencies), and ﬁnding the optimum segmentation of the sequence given
the dictionary and the language model. From the LOGOS point of view, the noise-less “words”
in the motif dictionary represent a deterministic local model of the motifs. Consequently, a de-
generate motif pattern could be represented by several similar words during the construction of the
dictionary, which can be merged into a consensus afterward. The “language model” adopted by the
MobyDick algorithm consists of an array of word-usage probabilities and an assemblage scheme
(analogous to a “grammar” of word usage in natural language) of the sequences, which manifests
a novel global model for motif distribution, and largely contributes to the strength of the Moby-
Dick algorithm. This language model assumes that each word is associated with a frequency of its
51

2.3 An Overview of Related Work
usage, and a sequence is realized by a non-overlapping concatenation of words sampled according
to their frequency (which implicitly assumes that the background model corresponds to a large set
of short words with low frequencies). The conditional probability of a sequence given a possible
segmentation speciﬁed by indicator sequence x under this setting is
p(y|x) = 1
Z
Y
k
ρnk(x)
k
,
(2.14)
where ρk denotes the frequency of word k, nk(x) denotes the counts of word k in y under segmen-
tation x, and Z is a normalization constant (i.e., the partition function).
Some key advantages of this global model are its emphasis on combinatorial analysis of a large
set of potential motifs (up to all possible substring patterns of k-mers allowable by the computing
resource), and an explicit non-overlapping constraint on individual substrings induced by the word
segmentation. Since the number of segmentations of an average-sized sequence could be huge,
computing the partition function Z of Eq. (2.14) and various derivatives of Z is non-trivial, and a
dynamic programming algorithm is developed.
To account for the sequence variations of each motif pattern, in a recent paper, Gupta and
Liu [2003] extended the MobyDick model to a stochastic dictionary (SD) model by replacing the
words in the dictionary with PWMs. From a LOGOS point of view, this corresponds to upgrading
the local model of MobyDick from deterministic words to PM. Let x denote a word-segmentation
of sequence set y, H = {h(1), . . . , h(D)} denote the set of nt-count matrices for all the words (with
PWMs {θ(k)}D
k=1) due to segmentation x, and N = {n1, . . . , nD} denote the counts of word oc-
currences. The complete data likelihood given all PWMs θ and the word usage probabilities ρ
is:
p(N, H, x|θ, ρ)
∝
D
Y
k=1
ρnk
k
Lk
Y
l=1
4
Y
j=1
[θ
(k)
l,j ]h(k)
l,j .
(2.15)
Essentially, SD adopts a speciﬁc distribution model for motif instances which treats the ob-
served sequences as being generated by concatenating words independently drawn from a dictio-
nary according to a vector of word usage probabilities, while retaining the PM model for aligned
52

2.3 An Overview of Related Work
motif instances. This is equivalent to upgrading the UI model to a ﬁnite mixture model with mixture
components being all the PWMs in the dictionary weighted by the word usage probabilities. Due
to this nice connection to the conventional motif models, many of the modeling ideas originally
introduced to the conventional models were readily adaptable to the SD model, such as smoothing
the model with conjugate priors for the PWM and word usage probabilities, and an extension al-
lowing stochastic insertions and deletions in motif instances (to model gaped motifs.) Since in the
model SD, the motif indicator X is deﬁned as a segmentation variable, with a huge state space that
prohibits exact inference, inference and parameter estimation are performed using a Monte Carlo
procedure.
2.3.3.4
The sliding-window approaches
The uniform and independent models and the word-segmentation models described above treat all
regions in a sequence equally, ignoring potential coupling of multiple motif instances in any sub-
regions of the sequence. However, as discussed in the introduction, in higher eukaryotic genomes,
motifs are often organized into cis-regulatory modules, in which the motif occurrences tend to be
signiﬁcantly enriched compared to the background region, and encode some complex combinatorial
signals. This architecture implies that during motif scan, locally clustered weak motif signals may
need to be treated with higher weights because they may suggest co-occurring weak binding sites in
a CRM; whereas an occasional seemingly strong signal out of a long stretch of sequence with low
score may need to be weighted lower because it may be just a spurious signal in the background.
The sliding-window method is one of the most popular approaches for motif and CRM predic-
tion that tries to incorporate the aforementioned architectural features of motif distribution [Halfon
et al., 2002; Papatsenko et al., 2002; Rajewsky et al., 2002; Nazina and Papatsenko, 2004]. Typ-
ically, a sliding-window method counts the number of matches of some minimal strength to given
motif patterns within a certain window of DNA sequences using certain scoring functions, such as a
likelihood ratio (when the motifs are represented by PWMs [Sharan et al., 2003]) or the k-m score
53

2.3 An Overview of Related Work
(when the motifs are represented by deterministic words) [Papatsenko et al., 2002]. From a mod-
eling point of view, this family of algorithms assumes that motifs are uniformly and independently
distributed only within each window. An ad hoc window size needs to be speciﬁed and careful
statistical analysis of matching strength is required to determine a good cutoff or scoring scheme.
The CREME algorithm [Sharan et al., 2003] uses a comparative genomic approach to identify
the putative CRM regions (thus avoiding the need to specify the window size), and a number of
sophisticated scoring functions were proposed to measure the statistical signiﬁcance of local en-
richment of candidate motif matches in these regions.
Nazina and Papatsenko [2004] addressed the issue of compensating the matching scores for
co-occurring weak motif sites using an updatable “word-frequency” measure, which leads to higher
scores for motifs occurring more frequently within a window of a given size. This approach is anal-
ogous to a MobyDick model applied to each window. A sliding-window version of the stochastic
dictionary model was used by Rajewsky et al. in their Ahab/Argos program [Rajewsky et al., 2002].
2.3.3.5
The hidden Markov model
Another way to handle sequences bearing rich motif content and architecture is to explicitly model
the organizations of the motifs using a stochastic sequential model that encodes “rules” to generate
such motif organizations. For example, the program Cister [Frith et al., 2001] assumes that the
indicator sequence of a study sequence, x, admits a 1st-order Markov model,
p(x) = p(x1)
T
Y
t=2
p(xt|xt−1),
(2.16)
whose state space consists of background states and motif states. The occurrences of motifs and
CRMs are induced by an emission model, p(yt|xt), which generates state-speciﬁc nucleotide out-
puts belonging to a motif or the background. Note that the stochastic rules of the motif organization,
such as how often a CRM appears, how long a CRM tends to be, and how often motifs appear in a
CRM, are encoded in the state-transition probabilities of these indicator variables. Since the indica-
tors x are not observed, this is a classical hidden Markov model with discrete output.
54

2.3 An Overview of Related Work
An HMM for motif scan renders both the window size and the score cutoff unnecessary, and
takes into account not only the strengths of motif matches, but also the spatial distances between
matches (arguably more informative than co-occurrences within a window). The hidden Markov
model used in Cister translates to a set of soft speciﬁcations of the expected CRM length and the
inter-motif distance (i.e., in terms of geometric distributions). However, since training data for ﬁtting
the HMM parameters hardly exists, these parameters have to be determined based on empirical
guesses.
2.3.4
Other Models
The local and global models discussed in the previous sections concern pure DNA sequence data,
and implicitly assume that the sequences to be analyzed come from a single species. With the avail-
ability of near complete sequences of several complex genomes, such as human and Drosophila,
and the anticipation of sequencing more evolutionarily related species in near future, comparative
genomic analysis of sequences from multiple evolutionarily related species has become a promis-
ing direction for in silico motif detection [Pennacchio and Rubin, 2001; Rubin, 2001; Wasserman
and Sandelin, 2004]. The emergence of high-throughput gene expression or protein-binding pro-
ﬁling techniques, such as microarray analysis [Shalon et al., 1996] and ChIP-array analysis [Ren
et al., 2000], provides another source of information to decode the transcription regulatory pro-
gram. In particular, joint analysis of regulatory sequences together with the expression patterns of
the genes regulated by these sequences appears to be a practical approach for motif detection, and
is potentially more informative than methods solely based on sequence data [Segal et al., 2003b;
Wasserman and Sandelin, 2004]. A detailed discussion of motif detection models along these two
directions is beyond the scope of this thesis. In the following, we brieﬂy overview major research
along these lines, and we point out their connection to the LOGOS framework and how an integra-
tion can be pursued.
55

2.3 An Overview of Related Work
2.3.4.1
Comparative genomic approach
Under the assumption that mutations within functional regions of the genome will accumulate
more slowly than mutations in regions without sequence-speciﬁc functions, the comparison of se-
quences from orthologous genes and their associated regulatory regions can indicate segments that
might direct transcription. For the prediction of motifs and CRMs, a major family of algorithms
motivated by comparative genomic analysis is phylogenetic footprinting [Blanchette et al., 2002;
Ureta-Vidal et al., 2003]. A phylogenetic footprinting algorithm usually consists of three compo-
nents: 1) deﬁning suitable orthologous gene sequences for comparison, 2) aligning the promoter
sequences of orthologous genes, and, 3) identifying segments of signiﬁcant conservation. For each
component, there exist a wide variety of methods/programs, whose details are beyond the scope
of this thesis. To name a few, to generate a multiple alignment of regularity regions of ortholo-
gous genes, BLASTZ [Schwartz et al., 2003] and LAGAN [Brudno et al., 2003] are often used
because they tend to ﬁnd a proper balance between preserving short stretches of highly conserved
regions and ﬁnding long but marginally conserved regions; to interpret the aligned data, one can use
a VISTA [Loots et al., 2002] browser to plot the amount of nt-identity across the aligned sequences
from multiple species within a sliding window, or use a dynamic programming algorithm to ﬁnd an
optimal segmentation of homogeneous and heterogeneous regions from the alignment [Xing et al.,
2001].
Kellis et al. [2003] developed a suite of techniques that work together for whole genome mo-
tif detection on the basis of within-genome over-representativeness and cross-genome evolutionary
conservation of motif patterns. In their approach, they adopt a regular expression representation for
a motif pattern, and begin with exhaustive enumeration of all over-represented regular expressions
in all the genomes under their study to generate a long list of candidate core motifs referred to as
“partial words.” Then they iteratively prune this list based on three evolutionarily-motivated crite-
ria drawn from an empirical study of the conservation patterns of the gal4 motifs in multiple yeast
species: 1) overall genome-wide intergenic conservation, 2) preference for intergenic (i.e. between
56

2.3 An Overview of Related Work
gene sequences) conservation over genic (i.e., within a gene) conservation, 3) differential conserva-
tion in upstream-only vs. downstream-only regions. Finally they extend the qualiﬁed core regular
expressions to include neighboring positions, collapsing degenerate regular expressions based on
sequence similarity and genome-wide co-occurrence. They reported a remarkable analysis of the
Saccharomyces cerevisiae genome in light of draft sequences of three related yeast species, in which
they conﬁrmed numerous well characterized motifs and identiﬁed several previously unknown mo-
tifs. It is noteworthy that Kellis’s approach does not attempt to pursue formal modeling of the
motif properties under an evolutionary context, such as a stochastic motif model for intra-species
variation, evolutionary model for inter-species variations, global distribution model for motif or-
ganization, etc.. Their approach also heavily relies on high-quality gene ﬁnding results, and the
assumption that even for each instance of a motif, one can expect an one-to-one correspondence
of its presence across species (i.e., several particular instances of a motif, one in each species, are
evolved from the same ancestor, whereas other instances of the same motif in a species have their
own counterparts in other species), and that this correspondence can be revealed in a multiple align-
ment of the whole genomes (which implicitly assumes that, certain, but not arbitrary instances of
a motif across species are “orthologous”, and multiple “paralogous” instances of the same motif
in each particular species are order-preserving across species so that they can be aligned to their
respective orthologous counterparts in other species in a single multiple alignment). This is a rather
strong assumption, which may not hold for higher eukaryotic genomes.
In summary, extant phylogenetic footprinting and other comparative genomics approaches are
restricted to short regulatory sequences from very closely related species, or genomes of simple
organisms in which high-quality gene identiﬁcation is possible and the regulation involves simple
motif organization. For evolutionarily distant species and large complex genomes, not only are the
non-coding regions hard to identify and align, but also the assumption that the aligned non-coding
sequences are orthologous is often not substantiated for small and degenerate functional elements
such as motifs and CRMs. Formal modeling of motifs under an evolutionary context is still an
open and little addressed problem, and could lead to important methodological advances in motif
57

2.3 An Overview of Related Work
detection.
2.3.4.2
Joint models for motifs and expression proﬁles
A direct consequence of combinatorial interactions between TFs and their corresponding motifs is
the highly regulated and coordinated transcription of the genes under their control. It is reasonable
to expect that the expression proﬁle of genes in a certain genome must bear some information
useful for predicting the presence and the identity of regulatory motifs. Indeed, this idea was used
even when motif detection models and algorithms were still in their infancy, although in a rather
primitive fashion. For example, many algorithms assume that co-expression of a set of genes implies
their co-regulation, and furthermore, implies co-existence of instances of the same motif in their
respective regulatory regions [Cardon and Stormo, 1992; Helden et al., 2000]. Unfortunately, this
assumption may not hold true for genes under complex control mechanisms. Among the more
sophisticated and explicit applications of expression proﬁles, especially high throughput data from
mRNA microarrays, for inferring motif patterns are the “regression-based methods,” which try to
capture some of the interactions among motifs (e.g., their cumulative effects), and relate them to
gene expression levels via a deterministic function. For example, Bussemaker et al. [2001] used a
linear regression model to capture correlations between the abundances of regulatory elements and
gene expression. It is straightforward to generalize this method to a logistic regression model that
captures non-linear response (i.e., binary “on” and “off” response) between gene expression and
motif presence. Keles et al. [2004] proposed a more expressive logic regression function to capture
complex interactions, such as logical OR, between motifs. Segal et al. [2003a; 2003b] went beyond
merely ﬁtting deterministic mapping functions between motif spectrum and gene expression, and
proposed to jointly model probabilistic distributions of gene expressions recorded in time series
or other experimental conditions, together with the locations and stochastic variations of motifs,
using a large-scale probabilistic graphical model. Segal’s approach spearheads an emerging trend of
using the systems biology principle in computational analysis of biological data, that is, combining
correlated data from heterogeneous sources for complex prediction tasks. A recent publication went
58

2.3 An Overview of Related Work
as far as combining gene expressions, motif-bearing sequences and sequences from evolutionarily
related species under a uniﬁed computational protocol [Chiang et al., 2003]. However, although
some extant models and algorithms have reached an unprecedented level of complexity in terms
of the size and diversity of objects being modeled, the level of sophistication and the biological
foundation of the models for different aspects of the heterogeneous biological data are far from
satisfactory and lack systematic veriﬁcation. For example, in almost all cases, the simple PM model
and UI model were used to model the local and global aspects of motifs. More investigations
are needed to make full and appropriate use of the systems biology approach for in silico motif
detection.
2.3.5
Summary: Understanding Motif Detection Algorithms
The design of motif detection algorithms can be understood as a quest for realistic and well-founded
mathematical models that capture the biological nature of the structure, organization and function
of TF binding sites in the genome; for efﬁcient computational algorithms that solve such models;
and for data fusion strategies that integrate diverse sources of experimental data and produce consis-
tent and both biologically and mathematically interpretable hypotheses and predictions. Different
algorithms can differ in only one of these three aspects (e.g., using different techniques, such as EM
or Monte Carlo methods, for probabilistic inference), or more aspects. To understand the essential
differences between different motif detection algorithms, it is important to analyze these algorithms
with respect to the aforementioned three aspects and identify their merits and deﬁciencies in these
aspects, so that improvement can be made systematically and purposefully. In this section, we at-
tempted to provide an overview of a wide range of modeling strategies currently in use, which is, to
our opinion, the most important aspect that determines the capacity of a motif detection algorithm.
In Table 2.1, we brieﬂy summarize representative motif detection algorithms and/or software pack-
ages in the literature in terms of their model speciﬁcities, as well as the computational algorithm
and data fusion strategies.
As Table 2.1 makes clear, many early methods lack any mechanism for incorporating knowl-
edges about meta-sequence features of the motifs at both the local and global level. Recent studies
59

2.3 An Overview of Related Work
Table 2.1: A summary of popular motif detection software/algorithms
Software/ Al-
gorithm
local model
global model
data fusion
inference al-
gorithm
task
ref.
MEME
PM/PD
UI (mops)
-
EM
de novo
[Bailey and Elkan, 1995a]
BioProspector
PM/PD
UI (mops)
-
Gibbs
de novo
[Liu et al., 2001]
AlignACE
PM
UI (Bernouli
indicator)
-
Gibbs
de novo
[Hughes et al., 2000]
MobyDick
word
word
con-
catenation
-
DP
de novo
[Bussemaker et al., 2000].
SD
PM
word
con-
catenation
-
Gibbs
de novo
[Gupta and Liu, 2003]
Cister
PM
HMM
-
Forward-
Backward
scan
[Frith et al., 2001]
CREME
word/PWM
window
-
various tests
scan
[Sharan et al., 2003]
Ahab/Argos
word/PWM
window
-
exhaustive
search
scan/ de novo
[Rajewsky et al., 2002]
PRM
PM
UI
sequence
+
microarray
BP
de novo
[Segal et al., 2003a; Segal
et al., 2003b]
FootPrinter
word
UI
sequence
of
multiple
species
Phylogenetic
Footprinting
de novo
[Blanchette
and
Tompa,
2003; Blanchette et al.,
2002]
have tried to address these problems from several different angles. Though these attempts head in the
direction of more expressive motif models, it is not clear whether these ideas can be integrated to as-
semble a powerful yet transparent and computationally efﬁcient motif detection algorithm. There is
a trend of combining heterogeneous source of data and constructing composite models for such data
from simple building blocks. It is argued that the correlations and internal dependencies between
different sources of data could serve to validate each other, and lead to more reliable predictions.
However, it is also possible that, due to the lack of sophistication of each component submodel
for different aspects of a complex model, and the difﬁculties of performing exact inference com-
putations on such models, errors resulting from approximate inference or from deﬁciencies of each
submodel may tend to propagate rather cancel. Thus, large composite models built from a plethora
of heterogeneous components may generate highly unreliable predictions if the biological legiti-
macy, computational tractability and quality of approximate computation of the model components
are unwarranted.
In summary, numerous advances notwithstanding, successful results of in silico motif discovery
remain limited to simple bacterial and yeast sequences. Performance on sequences with complex
intra- or inter motif structures are far less robust. One of the possible causes for compromised
60

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
generalizability and scalability of many extant algorithms is believed to be their incorrect inde-
pendence assumptions about motif sites and motif occurrences, which leads to inability to cap-
ture possible intra-motif spatial dependencies corresponding to the signature physical structure for
unique recognition and stable molecular interaction, and inter-motif dependencies that elicit syn-
ergy or simply avoid overlap. As mentioned in the introduction, there have been some recent at-
tempts at addressing these issues at various levels [Hertz and Stormo, 1999; Helden et al., 2000;
Frith et al., 2001; GuhaThakurta and Stormo, 2001; Rajewsky et al., 2002; Barash et al., 2003;
Nazina and Papatsenko, 2004]. In the following, we will develop an expressive modular motif
model that builds on these previous lines of research.
2.4
MotifPrototyper: Modeling Canonical Meta-Sequence Features
Shared in a Motif Family
For the gene regulatory system to work properly, a TF must display much higher binding afﬁnities to
its own recognition sites than to non-site DNA. This correspondence suggests possible regularities
in the DNA motif structure that match the structural signatures in the DNA-binding domains of their
corresponding TFs. Can these regularities hidden in the true DNA motif patterns be exploited to
improve sensitivity and speciﬁcity during motif discovery? As Michael Eisen has pointed out (pri-
vate communications), there should be great potential for improving motif recognition by modeling
and exploiting such structural regularities.
As reviewed in the previous sections, all extant local models of DNA motifs are essentially
motif-speciﬁc and are intended to generalize only to different instances of the same motif. An im-
portant issue that remains little addressed is how to build models that can generalize over different
motifs that are somewhat related (for instance, belonging to a family of regulatory sites that are
targets of TFs bearing the same class of binding domains) even though they do not share appar-
ent commonality in consensus sequences. This issue is important in computational motif analysis
because,
• often, we want to roughly predict the biological property of an in silico identiﬁed motif pattern
61

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
(e.g., to what kind of TFs it is likely to bind) to reduce the search space of experimental
veriﬁcation;
• we may need to introduce some generic but biologically meaningful bias during de novo motif
detection so that we can distinguish a biologically plausible binding site (i.e., speciﬁcally
recognizable by some TF) from a trivial recurring pattern (e.g., micro-satellites);
• we may also want to restrict attention to a particular class of proteins in performing tasks
such as: “ﬁnd a regulatory site that potentially binds to type X TF”, or “ﬁnd co-occurring
regulatory sites that can be recognized by type X and type Y TFs, respectively.”
These tasks are important in inferring gene regulatory networks from genomic sequences, possibly
in conjunction with relevant expression information.
In this section, we address the problem of modeling generic features of structurally but not
textually related DNA motifs, that is, motifs whose consensus sequences are entirely different, but
nevertheless share “meta-sequence features” reﬂecting similarities in the DNA binding domains of
their associated protein recognizers. We present MotifPrototyper, a proﬁle hidden Markov Dirichlet-
multinomial (HMDM) model which can capture regularities of nt-distribution prototypes and site-
conservation couplings typical to each particular family of motifs that corresponds to TFs with
similar types of structural signatures in their DNA binding domains. Central to this framework is
the idea of formulating a proﬁle motif model as a family-speciﬁc structured Bayesian prior model
for the PWMs of motifs belonging to the family being modeled, thereby relating these motif patterns
at the meta-sequence level. In the following, after a brief discussion of the biological motivation
underlying our model, we will ﬁrst develop the theoretical framework of the HMDM model, and
then show how to learn family-speciﬁc proﬁle HMDMs, or MotifPrototypers, from biologically
identiﬁed motifs categorized in standard biological databases; how the model can be used as a
classiﬁer for aligned multiple instances of motifs; and most importantly, how a mixture model
built on top of multiple proﬁle models can facilitate Bayesian estimation of the PWM of a novel
motif. The Bayesian estimation approach connects biologically identiﬁed motifs in the database
62

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
to previously unknown motifs in a statistically consistent way (which is not possible under the
single-motif-based representations described previously) and turns de novo motif detection, a task
conventionally cast as an unsupervised learning problem, into a semi-unsupervised learning problem
that makes substantial use of existing biological knowledge.
2.4.1
Categorization of Motifs Based on Biological Classiﬁcation of DNA Binding
Proteins
TF categorization from TRANSFAC r6.0
Superclass
class
# of training matrices
#
of
test
alignments
1.1 : Leucine zipper factors (bZIP)
34
1.2 : Helix-loop-helix factors (bHLH)
13
1.3 : Helix-loop-helix/leucine zipper factors
22
1.4 : NF-1
6
1.5 : RF-X
1
1:
Basic domains
1.6 : bHSH
6
82
48
2.1 : Cys4 zinc ﬁnger or nuclear receptor type
14
2.2 : diverse Cys4 zinc ﬁngers
13
2.3 : Cys2His2 zinc domain
21
2.4 : Cys5 cycteine-zinc cluster
3
2:
Zinc-coordinating
DNA-binding
domains
2.5 : Zinc ﬁngers of alternating composition
1
52
36
3.1 : Homeo domain
41
3.2 : Paired box
6
3.3 : Fork head / winged helix
4
3.4 : Heat shock factors
7
3.5 : Tryptophan clusters
17
3:
Helix-turn-helix
3.6 : TEA domain
1
76
64
4.1 : RHR (Rel homology region)
15
4.2 : STAT
13
4.3 : p53
2
4.4 : MADs box
9
4.5 : β-Barrel α-helix TFs
1
4.6 : TATA-biding proteins
3
4.7 : HMG
5
4.8 : Heteromeric CCAAT factors
9
4.9 : Grainyhead
2
4.10: Cold-shock domain factors
0
4:
beta-scaffold fac-
tors
4.11: Runt
1
60
13
0: Other TFs
. . .
. . .
1
Table 2.2: The TRANSFAC categorization of transcription factors (for the training set, the counts are made on TFs that
have more than 10 biologically identiﬁed binding sites; for the test set, TFs with at least 6 sites are counted.)
Unlike proteins or genes, which usually have a one-to-one correspondence to monomer se-
quences and hence are directly comparable based on sequence similarity, a DNA motif is a collective
object referring to a set of similar short DNA substrings that can be recognized by a speciﬁc protein
transcription factor. Different motifs are characterized by differences in consensus, stochasticity
63

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
and number of occurrences. Since each motif usually corresponds to a proﬁle of gap-less, multiple-
aligned instances rather than a single sequence as for genes and proteins, comparisons based on
sequence similarity for different motif patterns are not as straightforward as for genes or proteins.
From a biological point of view, perhaps the most informative way of categorizing DNA motifs
is according to the regularities of the DNA-binding domains of their corresponding transcription
factors. Advances in structural biology have provided an extensive categorization of the biophysical
structures of DNA-binding proteins. The most recent update of the TRANSFAC database [Wingen-
der et al., 2000] lists 4219 entries, many of which are homologous proteins from different species
but are nevertheless indicative of the vast number of transcription factors now known that regulate
gene expression. Table 2.2 shows a fraction (the top two levels in the cluster hierarchy) of the
TRANSFAC categorization of TFs. This categorization provides a good indication of the types of
binding mechanisms involved in motif-TF recognition. For concreteness, the following is a brief
summary of the structural regularities of four of the major classes of DNA-binding proteins, para-
phrasing [Stryer, 1995]. Due to the correspondence between a TF and a DNA motif, the TF catego-
rization strongly suggests possible features in the structure of motif sequences that are intrinsic to a
family of motifs corresponding to a speciﬁc class of TFs.
The leucine zipper signature (Figure 2.4a) under the superclass of basic-domain is an important
feature of many eukaryotic regulatory proteins. The hallmark of leucine zipper proteins is the
presence of leucine at every 7th position in a stretch of 35 residues. This regularity suggests the
presence of a zipper-like α-helical coiled coil bringing together a pair of DNA-binding modules to
bind two adjacent DNA sequences. Leucine zippers can couple identical or nonidentical chains,
suggesting a homodimeric or heterodimeric signature in the recognition site. A variation of this
structural theme often seen in prokaryotic transcription factors is the helix-loop-helix signature. In
this case, the basic DNA-binding helices are connected into a dimer by a short loop.
The zinc ﬁnger domain (Figure 2.4b) is also common in eukaryotic TFs and regulates gene
expression by binding to extended DNA sequences. A zinc ﬁnger grips a speciﬁc region of DNA,
binds to the major groove of DNA and wraps part of the way around the double helix. Each ﬁnger
64

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
(a) leucine zipper
(b) zinc fingers
(c) helix−turn−helix
(d) beta scaffold
Figure 2.4: DNA binding domains in TFs.
makes contact with a short stretch of the DNA, and residues from the amino-terminal part of the
α-helix form hydrogen bonds with the exposed bases in the major groove. Zinc-ﬁnger DNA binding
proteins are highly versatile and can have various numbers of zinc ﬁngers in the binding domain.
Arrays of zinc ﬁngers are well suited for combinatorial recognition of DNA sequences.
The helix-turn-helix domain (Figure 2.4c) contains two α-helices separated by 34 ˚A - the pitch
of a DNA double helix. Molecular modeling studies showed that these two helices would ﬁt into
two successive major grooves. This domain, common in bacterial DNA-binding proteins, such
as the bacteriophage λ Cro protein, also occurs in the eukaryotic homeobox proteins controlling
development in insects and vertebrates.
The beta-scaffold factors (Figure 2.4d) are somewhat unusual in that they bind to the minor
groove of DNA. The binding domain is globular rather than elongated, suggesting extensive contact
between the DNA sequence and the protein binding domain.
65

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
 
0
1
2
bits
5′
1
T
A
G
2
G
3
GA
4
TG
5
C
A
T
6
T
7
8
C
G
A
9
C
10
G
C
A
T
11
12
AC
13
T
C
G
3′
 
0
1
2
bits
5′
1
T
A
C
G
2
C
3
C
4
C
G
A
5
T
C
G
6
CG
7
T
C
G
8
T
C
G
A
9
AT
10
T
G
A
C
11
G
12
G
A
C
T
13
G
14
T
A
G
C
15
GC
16
G
CT
17
T
G
C
3′
gal4−
permute
pho4−
permute
 
0
1
2
bits
5′
1
C
2
G
3
CG
4
C
G
A
5
T
A
C
G
6
T
A
G
C
7
T
C
G
A
8
T
G
C
9
AT
10
T
C
G
11
G
CT
12
T
G
A
C
13
T
C
G
14
G
A
C
T
15
GC
16
C
17
G
3′
 
0
1
2
bits
5′
1
C
A
T
2
3
G
C
A
T
4
5
C
G
A
6
AC
7
GA
8
C
9
G
10
T
11
TG
12
T
C
G
13
T
A
G
3′
gal4
pho4
Figure 2.5: Conservation-coupling of a zinc-ﬁnger motif gal4 and a helix-loop-helix motif pho4.
Since typical
conservation-couplings are often reﬂected in the “contour shape” (e.g., U- or bell-shape) of the motif logo (a graph-
ical display of the spatial pattern of information content over all sites), we can understand this property as a “shape
bias”.
These class-speciﬁc protein-binding mechanisms suggest the existence of features that are char-
acteristic of different families of DNA motifs, and shared by different motifs in the same family.
It is evident that the positions within the motifs are not necessarily uniformly conserved, nor are
the conserved positions randomly distributed. Since only a subset of the positions inside the motif
are directly involved in protein binding, the degree of conservation of positions inside the motif is
likely to be spatially dependent, and such dependencies may be typical for each motif family cor-
responding to a TF class due to structural complementarity between motifs and the corresponding
TFs. It is also possible that due to different degrees of variability-tolerance for different TF classes,
each family of motifs may require a different selection of prototypes for the distributions of possible
nucleotides at the positions within the motifs. Note that such regularities are less likely to be pre-
served in a non-functional recurring pattern, thus they also provide important clues to distinguishing
genuine from false motif patterns during de novo motif ﬁnding. Figure 2.5 provides two examples
for the so-called conservation-coupling property of the position dependencies in functional motifs.
On the left-hand side are two genuine motifs from two different families. On the right are artiﬁ-
cial patterns resulting from a column permutation of the original motifs. Although the two patterns
will receive the same likelihood score under conventional PWM representations, clearly the pat-
terns on the left are biologically more plausible because of the complementarity of their patterns of
conserved positions to the structures of their binding proteins. Again, it is important to remember
that the conservation-coupling property and nt-distribution prototypes are only associated with the
generic biophysical properties of a motif family, but not with any speciﬁc consensus sequence of a
single motif; thus, they are called meta-sequence features.
66

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
2.4.2
HMDM: a Bayesian Proﬁle Model for Motif Families
The goal is to build a statistical model to capture the generic properties of a motif family so that it
can generalize to novel motifs belonging to the same family. In the following we develop such a
model using a hierarchical Bayesian approach.
The column of nucleotides at each position in a motif can be modeled by a position speciﬁc
multinomial distribution (PSMD). A multinomial distribution over K symbols can be viewed a
point in a regular (K −1)-dimensional simplex; the probabilities of the symbols are the distances
from the point to the faces of the simplex (an example of a 2-dimensional simplex is shown in
Figure 2.6a). A Dirichlet distribution is a particular type of distribution over the simplex, hence a
distribution over the multinomial distributions. Each speciﬁc Dirichlet is characterized by a vector
of K parameters. It can impose a bias toward a particular type of PSMD in terms of how strongly
it is conserved, and to what nucleotide it is conserved. For example, in Figure 2.6a, the center of
probability mass is near the center of the simplex, meaning that the multinomial distributions that
deﬁne a near uniform probability for all possible nucleotides will have a higher prior probability.
But for a Dirichlet density whose center of mass is close to a corner associated with a particular
nucleotide, say, “A” (Figure 2.6b), the multinomial distributions with high frequencies for “A” have
high prior probabilities. Therefore, we can regard a Dirichlet distribution as a “prototype” for the
PSMDs of motifs.
(a)
(b)
Figure 2.6: Dirichlet densities over a three-nucleotide simplex.
We propose a generative model that generates a multi-alignment A containing M instances of a
motif of length L, in the following way (as illustrated in Figure 2.7). 1) Sample a sequence of states
67

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
s = (s1, . . . , sL) from a ﬁrst-order Markov chain with initial distribution υ and transition matrix
Υ. The states in this sequence can be viewed as prototype indicators for the columns (positions)
of the motif. Associated with each state, is a corresponding Dirichlet distribution speciﬁed by the
value of the state. For example, if sl = i, then column l is associated with a Dirichlet distribution
αi = [αi,1, . . . , αi,4]t. 2) For each l ∈{1, . . . , L}, sample a multinomial distribution θl according
to p(θ|αsl), the probability deﬁned by the Dirichlet component αsl. 3) Generate all the nucleotides
in column l iid according to the multinomial distribution parametrized by θl.
I
s
s
θ
θ
θ
1
2
L
1
2
L
...
...
s
Am,1
M
M
M
Am,2
Am,L
profile HMDP
α
{
}
υ,Υ
Figure 2.7: The graphical model representation of a MotifPrototyper. Empty circles represent random variables associated
with a single motif and the boxes are plates representing iid replicates (i.e., M observed instances of the motif). Black
arrows denote dependencies between the variables. Parameters of the MotifPrototyper are represented by the center-
dotted circles, and the round-cornered box over the α parameter denotes I sets of Dirichlet parameters. The round-
cornered dashed box denotes plate of parameters of a single HMDM model, and hence represent a possible mixture of
HMDMs.
Thus, the complete likelihood of a motif alignment AM×L characterized by a nucleotide-count
matrix h is:
p(A, s, θ|α, υ, Υ) = p(A|θ)p(θ|s, α)p(s|υ, Υ).
(2.17)
where (using the update properties of the Dirichlet distribution and letting si
l = 1 if sl is at state i
and 0 otherwise)
p(h|x, θ)p(θ|s, α)
=
L
Y
l=1
IY
i=1
Dir(αi + hl)si
l,
(2.18)
p(s|υ, Υ)
=
IY
i=1
[υi]si
1
L−1
Y
l=1
IY
i,j=1
[Υi,j]si
lsj
l+1.
(2.19)
68

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
Technically, such a model, which is named a MotifPrototyper, is a hidden Markov Dirichlet-
multinomial model. It deﬁnes a structured prior for the PWM of a motif.
With the availability of a categorization for motifs, each family of motifs can be associated with
a family-speciﬁc proﬁle HMDM model that imposes PSMD prototypes and positional-dependencies
unique to this family.
What do we gain from a MotifPrototyper? First, a MotifPrototyper introduces prior information
about the joint distribution of the nt-distribution in different positions of a motif of the corresponding
family, and gives high probabilities to those commonly found distributions possibly compatible
with the degree of variability-tolerance intrinsic to the class of TFs corresponding to the motif
family. Under a MotifPrototyper, a posteriori, each PSMD in a motif follows a family-speciﬁc
mixture of multiple Dirichlet distributions, which blends the different prototypes that might dictate
the nt-distribution at that position. Furthermore, a MotifPrototyper stochastically imposes family-
speciﬁc spatial dependencies for different columns within a motif. As Figure 2.7 makes clear, a
MotifPrototyper is not a simple HMM for sequence data. In an HMM model the transitions would
be between the emission models (i.e., multinomials) themselves, and the output at each step would
be a single monomer in the sequence. In MotifPrototyper, the transitions are between different prior
components for the emission models, and the direct output of this HMM is the parameter vector of
a generative model, which will be sampled multiple times at each position to generate iid instances.
This approach is especially useful when we have prior knowledge about motif properties, such as
conservation-coupling or other positional dependencies. In contract to the IC proﬁle used in the
constrained PM model, due to the stochastic nature of a probabilistic model, MotifPrototyper will
in general not be rigidly conﬁned to any particular motif shape (unless we explicitly forbid certain
transitions in the transition matrix Υ of the hidden Markov chain). These properties relieve our
motif model from the restricted, often brittle constraints needed in other models, such as exactly
what shape to look for, the widths of the conserved and unserved patches in a motif, the length of
the whole motif 1, etc., and as a result provide desirable ﬂexibility and robustness under practical
1In an HMDM model, the length of the motif pattern to be modeled does not have to be rigorously deﬁned but only
69

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
motif detection environment.
Secondly, rather than using a maximum likelihood (ML) approach to estimate the PWM, which
considers only the relative frequency of nucleotides but is indifferent to the actual number of in-
stances observed, MotifPrototyper facilitates a Bayesian estimation of the PWM under a family-
speciﬁc prior, thus taking into consideration the actual number of observations available for PWM
estimation along with the biological prior. It is possible with only a few instances to obtain a robust
estimation of the nucleotide frequency at each position of a motif.
Note that a MotifPrototyper deﬁnes a family-speciﬁc structured prior for the PWMs without
committing to any speciﬁc consensus motif sequence.
2.4.2.1
Training a MotifPrototyper
Given biologically identiﬁed instances of motifs of a particular family, we can compile a multiple-
alignment for each motif and write down the joint likelihood of the training data under a single
proﬁle model (i.e., a MotifPrototyper) by marginalizing out the PWMs (i.e., θ’s) and the hidden
Markov states (i.e., s) of each motif in Eq. (2.17). This likelihood is a function of the model
parameters. Thus we can compute the empirical Bayes estimation of the model parameters, Θl :=
{α, υ, Υ}, by maximizing the likelihood over each parameter using a EM algorithm with a quasi-
Newton procedure for the parameter update step [Sj¨olander et al., 1996] (see Appendix A.2 for
details). The result is a set of parameters intrinsic to the training data.
Note that this training process also involves a model selection issue of how many Dirichlet
components should be used. As in any statistical model, a balance must be struck between the
complexity of the model and the data available to estimate the parameters of the model. Empirically,
we found that 8 components appears to be a robust choice and also provides good interpretability.
needs a rough speciﬁcation (e.g., a length, say, L =20 bp, that is unlikely to be exceeded by most of the plausible motifs).
Because under the HMDM model, both conserved and heterogeneous sites are allowed in a candidate motif pattern; a
motif whose length is smaller than L can still be picked up by the model with a over-speciﬁed length with high probability
by allowing heterogeneous sites padded at the ends of the true motif pattern to make up the total length.
70

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
2.4.3
Mixture of MotifPrototypers
Now we have built a model that captures the meta-sequence features of structurally but not textually
related motifs. The model is a Bayesian proﬁle model that is deﬁned on each motif family rather than
each individual motif; thus we call it a MotifPrototyper. To estimate the PWM of a novel motif, since
we do not know which family-speciﬁc MotifPrototyper is corresponds best to the novel pattern, we
can assume that the motifs are generated from a weighted combination of several MotifPrototypers.
Statistically, this deﬁnes a mixture of MotifPrototypers as the prior distribution of the PWM of a
motif,
p(θ|{α, υ, Υ}k, k = 1, . . . , K) =
K
X
k=1
wkp(θ|{α, υ, Υ}k),
(2.20)
where wk is the mixing weight of each family-speciﬁc MotifPrototyper.
Under this setting, one can perform several important probabilistic computations regarding
motif detection, such as classifying motifs in terms of their preferred binding protein family by
identifying the most likely MotifPrototyper for a given motif alignment; computing the Bayesian
estimations of the motif parameters; and biasing the de novo motif detection to solutions that are
structurally more consistent with biologically genuine motifs. In other words, we effectively turn
the originally unsupervised de novo motif detection into a semi-unsupervised learning problem that
integrates the observed sequences with prior knowledge about motif structures.
2.4.3.1
Classifying motifs
Identifying that a motif belongs to a family, and relating it to other members of the family, often
allows inference about its functions. Given multiple proﬁle models each corresponding to a distinct
motif family, we can compute the conditional likelihood of a set of aligned instances of an unlabeled
motif under each proﬁle model by integrating out the hidden variables (i.e., θ and s) in each result-
ing complete likelihood function. The posterior probability of each possible assignment of class
membership to the motif under test is proportional to the magnitude of the conditional likelihood
multiplied by the prior probabilities of the respective motif families (which can be computed from
the empirical frequency of each motif family). Letting Z denote the family membership indicator,
71

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
the posterior probability of Z = k is proportional to the magnitude of the conditional likelihood
under the kth MotifPrototyper multiplied by the prior probability of Z = k:
p(Z = k|A)
∝
p(Z = k)p(A|{α, υ, Υ}k)
Thus, we can estimate the family membership by a maximum a posteriori (MAP) scheme. It is
noteworthy that, here, we are classifying a set of aligned instances of a motif as a whole, rather than
a single sequence substring as in a standard classiﬁcation task, such as predicting the function or
structure of a protein based on its amino acid sequence [Karchin et al., 2002; Moriyama and Kim,
2003].
2.4.3.2
Bayesian estimation of PWMs
Given a set of aligned instances of a motif, if we know the family membership of this motif, we can
directly compute the posterior distribution of its PWM, using the family-speciﬁc MotifPrototyper
as a prior according to Bayes rule. The Bayesian estimate of a PWM is deﬁned as the expectation
of the PWM w.r.t. this posterior.
If the family membership is not known a priori (i.e., we do not pre-specify what family of motif
to look for, but allow the motif to come from any family), then we can simply assume that the PWM
admits a mixture of proﬁle models. The posterior distribution of a PWM under a mixture prior is
only slightly more complex:
p(θ|A, {α, υ, Υ}K
k=1)
=
X
k
p(θ|A, {α, υ, Υ}k, Z = k)p(Z = k|A, {α, υ, Υ}K
k=1)
∝
X
k
p(θ|A, {α, υ, Υ}k)p(A|{α, υ, Υ}k)p(Z = k),
(2.21)
where Z denotes the family membership indicator. A useful variant of this mixture model is to
replace the mixture with the maximal-likelihood component:
p(θ|A, {α, υ, Υ}K
k=1)
≡
p(θ|A, {α, υ, Υ}k∗), where k∗= arg max
k
p(A|{α, υ, Υ}k).(2.22)
It is straightforward to generalize the current formulation of the MotifPrototyper model to
family-speciﬁc prior distributions over more sophisticated motif representations, such as trees or
72

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
mixture of trees, by slightly reparameterizing the MotifPrototyper model. The training procedure
and the usage for classiﬁcation and de novo motif detection require little modiﬁcation.
2.4.3.3
Semi-unsupervised de novo motif detection
In de novo motif detection where locations of motif instances are not known, the motif matrix A
is an unobserved random variable. One can iterate between predicting motif locations based on the
current Bayesian estimate of the motif PWM, and updating the Bayesian estimate based on newly
predicted motif instances. We will elaborate on this point in §2.5, where we describe a LOGOS
model that uses MotifPrototyper as the local model and an expressive hidden Markov model to
be developed in the next section, CisModuler, as the global model, for de novo motif detection in
higher eukaryotic genomic sequences. But as a “prove-of-concept” demonstration of the inﬂuence
of MotifPrototyper on the performance of de novo motif detection, in this section, we only use a
simple oops model as the global model. It can be proved that the iterative procedure we described
is guaranteed to converge to a locally optimal solution (cf. Chapter 4). But unlike the standard EM
algorithm for estimating a PWM, since we can compute the Bayesian estimate based on a trained
proﬁle motif prior, we essentially turn de novo motif detection from an originally unsupervised
learning problem into a semi-unsupervised learning problem that can make use of biological training
data without committing to any particular consensus motif pattern.
2.4.4
Experiments
Under the LOGOS framework, the MotifPrototyper and mixture of MotifPrototypers are both struc-
tured Bayesian upgrades of the standard PM local models for motifs. These models can be learned
from categorized training motifs to deﬁne family-speciﬁc priors for the PWMs, and when coupled
with a global model, can be used to introduce useful bias during de novo motif detection.
In this sub-section, we present results of learning MotifPrototyper models from categorized
families of motifs, and demonstrate applications of the learned MotifPrototypers with three exper-
iments, each addressing a typical issue of interest in in silico motif analysis. (1) Given instances
of a (computationally) identiﬁed motif, assign the motif to a motif family that corresponds to a
73

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
particular class of transcription factors. (2) Provide a Bayesian estimate of a PWM which may
be more informative than a maximum likelihood estimate. (3) Improve de novo motif detection by
casting the problem as a semi-supervised learning task that makes use of biological prior knowledge
incorporated in the family-speciﬁc MotifPrototypers.
2.4.4.1
Parameter estimation
The TRANSFAC database (version r6.0) contains 336 nucleotide-count matrices of aligned motif
sequences. These matrices summarize a signiﬁcant portion of the biologically identiﬁed transcrip-
tion regulatory motifs reported in the literature, and are well categorized and curated. (Although
the original aligned sequences corresponding to the count matrices are not provided.) We used 271
of the matrices as training data, each derived from at least 10 recognition sites of a TF in one of
the 4 well-represented superclasses (Table 2.2), to compute the empirical Bayes estimates of the
parameters of 4 proﬁle Bayesian models of motif families.
2
4
6
8
2
4
6
8
2
4
6
8
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
2
4
6
8
2
4
6
8
1
2
3
4
5
6
7
8
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
2
4
6
8
2
4
6
8
1
2
3
4
5
6
7
8
0
0.5
1
(color scale:
)
2
4
6
8
2
4
6
8
1
2
3
4
5
6
7
8
A
C
G
T
0
5
10
15
20
A
C
G
T
0
5
10
15
20
A
C
G
T
0
5
10
15
20
A
C
G
T
0
5
10
15
20
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
A
C
G
T
0
5
10
Basic domain
Zinc−fingers
Helix−turn−helix
Beta−scaffold
Υ
α
υ
υ
Υ
α
Figure 2.8: Parameters of 4 proﬁle models learned from training motifs. Each of the 8 panels under α represents the
4-dimensional parameter vector of a Dirichlet component (the height of the bar represents the magnitude of the corre-
sponding element in the vector); vector υ and matrix Υ are represented by color images, of which each element of υ or
Υ speciﬁes the color of a rectilinear patch in the image.
We performed 50 random restarts for the quasi-Newton algorithm for parameter estimation and
picked the solutions corresponding to the highest log likelihood achieved at convergence. Figure 2.8
illustrates the parameters of the 4 resulting proﬁle models pictorially. Here we do not intend to fully
74

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
interpret these numerical representations of each proﬁle model in terms of their biological impli-
cations. But based on a rough inspection, it is not difﬁcult to read off some interesting high-level
biological characteristics. For example, for the basic-domain proﬁle model, the transition probabil-
ities between the 4 conserved nt-distribution prototypes 2 (the ﬁrst four mixture components of the
Dirichlet mixture) appear to be rather high (evident from the bright diagonal block at the upper left
corner of the Υ matrix), as are the self-transition probabilities of all of the 4 non-conserved Dirichlet
components (evident from the bright diagonal stripe at the lower right corner of the Υ matrix). The
transition probabilities between the conserved and non-conserved Dirichlet components are rela-
tively low (dark off-diagonal areas in Υ). Furthermore, it appears that the initial probability is high
for the 6th Dirichlet component, a fairly non-conserved one. This suggests a general meta-sequence
feature, namely that motifs of the basic-domain family are likely to begin with a consecutive run of
mostly non-conserved positions, followed by a consecutive stretch of mostly conserved positions,
and possibly followed by another consecutive run of mostly non-conserved positions, reminiscent of
the bell-shaped signature in Figure 2.5. Although it is possible to ﬁnd many other similar high-level
characteristics, some of which may even reveal previously unnoticed biological features (e.g., char-
acteristic PSMD prototypes of motif families), here we refrain from such elaborations, but simply
maintain that MotifPrototyper is a formal mathematical abstraction of the meta-sequence properties
intrinsic to a motif proﬁle represented by the training examples.
To evaluate the training quality of the proﬁle models, we deﬁne the training error as the per-
centage of misclassiﬁcation of the superclass-identities of the training motif matrices using proﬁle
models learned from the full training set. As Table 2.3 shows, our training errors range from 10-
28%, with the beta-scaffold MotifPrototyper having the best ﬁt. Given that “motif family” is rather
loosely deﬁned based on TF superclasses, and that each superclass still has very diverse and ambigu-
ous internal structures, these training errors indicate that family-speciﬁc regularities can be captured
2Note that the parameter vector of a Dirichlet component can be regarded as a vector of pseudo-counts of the
nucleotides. Thus a Dirichlet parameter vector with a dominant element implies a conserved nt-distribution proto-
type, whereas a Dirichlet parameter vector without a dominant elements implies a heterogeneous, or non-conserved
nt-distribution prototype.
75

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
reasonably well by MotifPrototyper.
Table 2.3: Learning MotifPrototyper
Basic domains
Zinc-ﬁngers
Helix-turn-helix
beta-scaffold
training error
0.168
0.173
0.276
0.100
2.4.4.2
Motif classiﬁcation
To examine the generalizability of MotifPrototyper to newly encountered motif patterns, we per-
formed a 10-fold cross-validation (CV) test for motif classiﬁcation, in which the proﬁle models are
learned from 90% of the training motif matrices, and their classiﬁcation performance is evaluated
on the remaining 10% of the motif matrices. We do so 10 times so that each motif pattern corre-
sponding to a particular TF will be classiﬁed exactly once as a test case. The performances over
each family of motifs are summarized in Table 2.4. Classiﬁcation error rates for both the entire
dataset and the reduced dataset that contains only the major motif subclasses (i.e., those with at
least 10 different motifs) under each superclass are presented. Not surprisingly, performance on the
dataset with only major subclasses is signiﬁcantly better, suggesting that the minor classes in each
superclass are possibly more ambiguous and less typical with respect to the overall characteristics
of the superclass. In fact, some minor classes are unanimously assigned to a different superclass
by our classiﬁer, for example, all 6 members of class 1.6 (bHSH) and all 7 members of class 3.4
(heat shock factors) are assigned to superclass 4 (beta-scaffold), whereas all 5 members of class 4.7
(HMG) are assigned to superclass 3 (helix-turn-helix). Whether such inconsistencies reﬂect a deﬁ-
ciency of our classiﬁer or possible true biological ambiguity of these motif patterns is an interesting
problem to be investigated further.
Table 2.4: Motif classiﬁcation using MotifPrototyper
Basic domains
Zinc-ﬁngers
Helix-turn-helix
Beta-scaffold
CV error (whole set)
0.256
0.423
0.443
0.403
CV error (major classes)
0.217
0.373
0.379
0.178
To our knowledge, there has been no algorithm that classiﬁes aligned sets of motif instances as
76

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
collective objects based on meta-sequence features shared within motif families. The closest coun-
terpart in sequence analysis is the proﬁle HMM (pHMM) model for protein classiﬁcation [Krogh et
al., 1994], but pHMM is based on the assumption that proteins of the same family share sequence-
level similarities, and the objects classiﬁed are single sequences. Thus, no direct comparison can be
made between pHMM and MotifPrototyper. Nevertheless, note that although pHMM is based on
much more stringent features at the sequence level and aimed at the relatively simpler task of eval-
uating single sequences, the typical accuracy of pHMM is around 20-50% for short polypeptides
(i.e., < 100 aa) [Karchin et al., 2002; Moriyama and Kim, 2003], comparable to the performance of
motif classiﬁcation using MotifPrototyper. Thus we believe that MotifPrototyper exhibits a reason-
able performance given that the labeling of motif family membership is more ambiguous than that
of single protein sequences, the meta-sequence features we use are far less stringent than sequence
similarities, and motif patterns are much shorter than polypeptides.
2.4.4.3
PWM estimation and motif scoring
A major application of MotifPrototyper is to serve as an informative prior for Bayesian estimation
of the PWM from a set of aligned instances of a novel motif. Since in a realistic de novo motif
detection scenario, one has to evaluate many substrings corresponding to either a true motif, or ran-
dom patterns in the background, it is expected that the Bayesian estimate of a PWM resulting from
MotifPrototyper is more reliable than the maximum likelihood estimate in discriminating between
true motifs and background sequences. We demonstrate this ability by comparing the likelihood of
a true motif substring with the likelihoods of background substrings, all scored under the estimated
PWM of the motif. To get an objective evaluation for this comparison, the following experiments
were performed: 1) for a set of aligned instances of a motif, compute the Bayesian estimate of the
PWM from 66% of the instances, and then use it to score (i.e., compute the likelihood of) the re-
maining 34% of the instances in terms of their joint log likelihood; 2) use the same PWM to score
M sets of background strings, each having the same length and number of instances as the motif
instances being scored in step 1; 3) compute the mean log-likelihood-odds between the motif and
77

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
the background substrings (over M sets of randomly sampled background substrings). For each
motif, we repeat this procedure 3 times so that each motif substring will be scored exactly once.
The performance on each motif is summarized by the average log-likelihood-odds per motif in-
stance. (Larger odds means that the background substrings are less likely to be mistakenly accepted
as motif instances, and thence, the false positive rate is smaller).
Since the original aligned motif sequences corresponding to the count matrices used for Mo-
tifPrototyper training are not provided in TRANSFAC and are hard to retrieve from the original
literature, we compiled an independent collection of aligned motif instances for 161 TFs in TRANS-
FAC, each of which has at least 6 binding sites whose sequence information is available (Table 2.2).
Background substrings from a uniform and random model were simulated 3.
0
20
40
60
80
100
120
140
160
−100
0
100
200
300
400
500
600
700
800
900
log−likelihood odds / instance
motif types
0
10
20
30
40
50
60
70
80
−100
−50
0
50
100
150
200
250
300
350
400
log−likelihood odds / instance
motif types
0
10
20
30
40
50
−100
0
100
200
300
400
500
600
700
800
900
log−likelihood odds / instance
motif types
all motifs
motifs with > 10 instances
motifs with <10 instances
Figure 2.9: Evaluation of PWM estimation by 4 different schemes. Cyan:symmetric-Dirichlet smoothing; green: ML;
red: mixture of proﬁle models; black: maximal-likelihood proﬁle model out of the mixture. Motifs are listed along the
x-axis, ordered by the log-likelihood-odds of their PWM based on the “true” (according to their original family label)
proﬁle prior model.
The results of the evaluation are highlighted in Figure 2.9. We compared 4 PWM estimation
schemes: maximum likelihood estimation (i.e., plain relative frequencies); Bayesian smoothing
using a single symmetric Dirichlet prior; Bayesian estimation using a mixture of proﬁle models;
and Bayesian estimation using the maximal-likelihood proﬁle model from the mixture. Depicted
as the bars in Figure 2.9 for reference are the results for Bayesian estimation using a single proﬁle
model corresponding to the original family label of each motif, an unrealistic scenario in de novo
3This corresponds to examining the log-likelihood-odds under a motif model w.r.t. a uniform and random null hy-
pothesis. Sampling of background substrings from a genuine genomic sequence as the null hypothesis was also done
at a small scale (for some motifs) and yields largely the same results. But since the motifs we studied are from diverse
genomic sources, a comprehensive evaluation in this manner is tedious and hence was omitted.
78

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
motif detection.
As evident from Figure 2.9 and 2.10, the discriminative power of the Bayesian estimate of the
PWM, measured by the log-likelihood-odds (of motif vs. background substrings), is indeed better
than that of the maximum likelihood estimate for most of the motifs we tested. In particular, in
cases where only a small number of instances are available for estimation, the mixture of proﬁle
models still leads to a good estimate that generalizes well to new instances and results in high
log-likelihood-odds, whereas the ML estimation does not generalize as well (Fig. 2.9).
0
50
100
150
200
250
300
350
0
50
100
150
200
250
300
350
Bayesian Est. of PWM
ML Est. of PWM
Figure 2.10: A comparison of Bayesian and ML estimates of the PWM. Each point represents a motif being tested, the
x-coordinate (resp. y-coordinate) represents the log-likelihood-odds due to the ML (resp. Bayesian) estimation.
These results give strong support to the claim that in many cases, a MotifPrototyper-based
approach can signiﬁcantly improve the sensitivity and speciﬁcity for novel motifs, and provide a
robust estimation of their PWMs under few observations. These are very useful properties for de
novo motif detection in complex genomic sequences.
2.4.4.4
De novo motif discovery
Now we present a comparison of the proﬁle Bayesian motif model – MotifPrototyper – with the
conventional PM model for de novo motif detection, using semi-realistic test data for which the
ground truth (i.e., full annotation of motif types and locations) is known for evaluating the prediction
results. Note that the experiments described here are a small excerpt from a large suite of de novo
motif detection experiments under various scenarios. We will return to the bulk of these experiments
79

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
in §2.7, after the development of a more powerful global model.
We tested on 28 well-represented yeast motifs from the Promoter Database of Saccharomyces
cerevisiae (SCPD). Each motif has 5 to 32 recorded instances, all of which have been identi-
ﬁed/veriﬁed via biological experiments and hence are considered “authentic”. For each motif, a
test dataset is created by planting each of the “authentic” instances of that motif at a random posi-
tion in a 500bp simulated background sequence (i.e., one motif per sequence). To further increase
the difﬁculty of the motif detection task, a “decoy” signal, which is an artiﬁcial pattern obtained by
randomly permuting the positions in the motif, was inserted into the study sequence 4. Since each
sequence has only one true motif occurrence, prediction was made by ﬁnding the position with the
maximal log-likelihood ratio (for the substring that begins with that position) under the estimated
motif PWM (obtained at the convergence point of a procedure that iterates between computing the
posterior distribution of motif locations based on current estimate of the PWM, and computing the
Bayesian estimate of the PWM based on the current posterior distribution of motif locations), and
under the background nt-distribution (assumed to be the nt-frequencies estimated from the entire
sequence). This scenario frees us from modeling the global distribution of motif occurrences, as
needed for more complex sequences (cf. the LOGOS model), and therefore demonstrates the in-
ﬂuence of different models for motif patterns on de novo detection. We evaluate the performance
based on hit-rate, the ratio of correctly identiﬁed motif instances (within ± 3bp offset with respect
to the locations of the authentic instances) to the total number of instances to be identiﬁed. To obtain
robust estimation, for each motif 40 experiments were performed, each with a different test dataset
(i.e., with different background sequences, motif and decoy locations, and decoy patterns).
Speciﬁcity of a single MotifPrototyper.
Before presenting the full-scale test of the mixture of
MotifPrototypers trained on four categories of motifs from the TRANSFAC database on the yeast
motifs from the SCPD database, here we ﬁrst examine whether the motif properties captured in
a MotifPrototyper effectively bias the posterior prediction of motif presence toward the desired
4By permutation we mean that the same permuted order is applied to all the instances of a motif so that the multinomial
distribution of each position is not changed but their order is changed.
80

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
pattern represented in the training set. For this purpose we trained a MotifPrototyper from the 28
yeast motifs in the SCPD database described above, and examined the resulting MotifPrototyper for
its ability to detect motifs present in this training set in the presence of a “decoy”. Figure 2.11 shows
the Boxplot (which shows the median, lower quartile, upper quartile, outliers, etc.) of the hit (i.e.,
ﬁnding the genuine motif) and mishit (i.e., ﬁnding the decoy) rate of MotifPrototyper on abf1 and
gal4. Note the dramatic contrast of the speciﬁcity of the MotifPrototyper to true motifs compared
to that of the PM model.
It is noteworthy that the MotifPrototyper model actually does not contain any explicit infor-
mation about the consensus sequences of the training motifs; it merely captures the dependencies
between general heterogeneous and homogeneous motif sites whose nucleotide distributions are not
ﬁxed, but instead are drawn from speciﬁed priors over the space of nucleotide distributions. Thus,
the high speciﬁcity of MotifPrototyper to a genuine motif pattern under the interference of a false
motif pattern suggests its remarkable ability to implicitly capture sensible “motif shapes”.
1
2
0
0.2
0.4
0.6
0.8
1
gal4 (hit)
 
 
1
2
0
0.2
0.4
0.6
0.8
1
gal4 (mis−hit)
 
 
1
2
0
0.2
0.4
0.6
0.8
1
abf1 (hit)
 
 
1
2
0
0.2
0.4
0.6
0.8
1
abf1 (mis−hit)
 
 
Figure 2.11: Boxplots of hit and mishit rate of MotifPrototyper (1) and PM (2) on two motifs used during MotifPrototyper
training.
Generalizability of a single MotifPrototyper.
How well does a MotifPrototyper generalize to
motifs not present in the training set? Here we use the MotifPrototyper learned from 20 of the 28
SCPD motifs to detect motifs from an independent test set containing the rest of the 8 SCPD motifs.
In the ﬁrst motif ﬁnding task, we use synthetic sequences each having only one true motif instance
at a random position. Figure 2.12 summarizes the results over 40 experiments. As shown in the
ﬁgure, the MotifPrototyper signiﬁcantly outperforms the PM model for motifs abf1, gal4 and crp,
and achieves comparable performance for motifs gcn4 and mig1. It does poorly for motifs mat-a2
and mcb. Note that these two motifs are quite short and somewhat uniformly “conserved,” which is
81

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
in fact “atypical” in the training set. The smallish sizes of the motifs also diminish the utility of the
Markov model in MotifPrototyper.
1
2
0
0.2
0.4
0.6
0.8
abf1
 
 
1
2
0
0.2
0.4
0.6
0.8
1
gal4
 
 
1
2
0
0.2
0.4
0.6
0.8
gcn4
 
 
1
2
0
0.2
0.4
0.6
0.8
gcr1
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mat−a2
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mcb
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mig1
 
 
1
2
0
0.2
0.4
0.6
0.8
crp
 
 
Figure 2.12: Boxplots of hit rate of MotifPrototyper (1) and PM (2) on sequences each embedded with one motif instance
from the test dataset.
In the foregoing motif detection task, the PM model shows decent performance, especially for
those more-or-less uniformly conserved motifs such as gcn1, mat and mcb. But it already shows
signs of failure for motifs with more complex shapes (e.g. gal4). The second task is more challeng-
ing and biologically more realistic, where we have both the true motifs and the permuted “decoys.”
Figure 2.13 shows the boxplot of the hit-rate as well as the mishit-rate for motif detection over
40 experiments. As expected, under the interference of the decoys, the PM model apparently gets
confused and often decides to pick the permuted false motifs. Only two of the eight motifs are cor-
rectly detected by the PM model with high hit-rate. In contrast, the MotifPrototyper model exhibits
remarkable robustness under this more difﬁcult situation, and maintains a high hit-rate in six of the
eight motifs. But for two of the motifs (again, mat-a2 and mcb), MotifPrototyper biases toward the
permuted version, which suggests that indeed the original mat-a2 and mcb patterns are not captured
by MotifPrototyper, consistent with the result from the ﬁrst task.
De novo motif detection using a mixture of MotifPrototypers.
Now we conclude this section
with an evaluation of a mixture of MotifPrototypers trained from the TRANSFAC database. We
test this model on all the 28 motifs from the SCPD database. As shown in Figure 2.14, the mixture
of MotifPrototypers signiﬁcantly outperforms the PM model (i.e. with > 20% margin) on 11 of
the 28 motifs, and is comparable to the PM model (within ±10% difference) for the remaining
82

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
1
2
0
0.2
0.4
0.6
0.8
abf1
 
 
1
2
0
0.2
0.4
0.6
0.8
1
gal4
 
 
1
2
0
0.2
0.4
0.6
0.8
gcn4
 
 
1
2
0
0.2
0.4
0.6
0.8
gcr1
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mat−a2
 
 
1
2
0
0.2
0.4
0.6
0.8
mcb
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mig1
 
 
1
2
0
0.2
0.4
0.6
0.8
crp
 
 
1
2
0
0.2
0.4
0.6
0.8
abf1
 
 
1
2
0
0.2
0.4
0.6
0.8
gal4
 
 
1
2
0
0.2
0.4
0.6
gcn4
 
 
1
2
0
0.2
0.4
0.6
0.8
gcr1
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mat−a2
 
 
1
2
0
0.2
0.4
0.6
0.8
mcb
 
 
1
2
0
0.2
0.4
0.6
0.8
1
mig1
 
 
1
2
0
0.2
0.4
0.6
0.8
crp
 
 
Figure 2.13: Boxplots of hit (left panel) and mishit (right panel) rates of MotifPrototyper (1) and PM (2) on sequences
each containing one motif instance from the test dataset together with a permuted decoy.
17 motifs. Overall, the mixture of MotifPrototypers correctly identiﬁes 50% or more of the motif
instances for 16 of the 28 motifs, whereas the PM model achieves 50% hit-rate for only 8 of the
28 motifs. Note that the mixture of MotifPrototypers is fully autonomous and requires no user
speciﬁcation of which particular proﬁle motif model to use. If we are willing to introduce a manual
post-processing step, in which we use each of the 4 proﬁle motif models described before separately
for de novo motif ﬁnding, and generate 4 sets of motif predictions instead of one (as for the mixture
of MotifPrototypers) for visual inspection, it is possible to obtain even better predictions (diamond
symbols in Figure 2.14).
−0.2
0
0.2
0.4
0.6
0.8
1
median hit−rate
motif types
gcn4
gcr1
mcb
bas1−pho2
HSE,HTSF
uash
pho2
gata
hap1
uasphr
gal4
rep−car1
tbp
mig1
reb1
tata−tbp
rap1
ccbf−swi6
abf1
CSRE
mcm1
rox1
ste12
mat−a2
pho4
pdr1
scb
urs1h
Figure 2.14: Median hit-rates of de novo detection of 28 yeast motifs using MotifPrototyper (square), PM (circle), and
the best outcome out of 4 single-proﬁle-based predictions using MotifPrototyper (diamond). Motifs are listed along the
x-axis, ordered by the hit-rates of MotifPrototyper for each motif.
The ability to provide multiple candidate solutions, each corresponding to a speciﬁc TF cate-
gory, manifests a key advantage of the proﬁle motif model. It allows a user to capture different
types of prior knowledge about motif structures and bias motif prediction toward a particular meta-
sequence structure in a well-controlled way. A human observer given a visual presentation of the
most likely motifs suggested by different proﬁle motif models could easily pick out the best one
83

2.4 MotifPrototyper: Modeling Canonical Meta-Sequence Features Shared in a Motif Family
from these candidates, whereas the PM model can yield only a single most likely answer.
2.4.5
Summary and Discussion
We have presented MotifPrototyper, a novel proﬁle Bayesian motif model that captures generic
meta-sequence features shared by motifs corresponding to common transcription factor superclasses.
It is a probabilistic graphical model that captures the positional dependencies and nucleotide dis-
tribution prototypes typical to each motif family, and deﬁnes a prior distribution of the position
weight matrices of motifs for each family. We demonstrated how MotifPrototyper can be trained
from biologically identiﬁed motif examples, and its applications for motif classiﬁcation, Bayesian
estimation of PWMs, and de novo motif detection.
To the best of our knowledge, all extant motif models are intended to be motif-speciﬁc, empha-
sizing the ability to characterize sequence-level features unique to a particular motif pattern. Thus
when one deﬁnes such a model for a novel motif not biologically characterized before, one needs to
solve a completely unsupervised learning problem to identify the possible instances and ﬁt the motif
parameters simultaneously. Under this unsupervised framework, there is little explicit connection
between the novel motif to be estimated from the unannotated sequences and the rich collection
of biologically identiﬁed motifs recorded in various databases. It is reasonable to expect that the
fruitful biological investigations of gene regulatory mechanisms and the resulting large number of
known motifs could contribute more information to the unraveling of novel motifs. MotifProto-
typer represents an initial foray into the development of a new framework that turns de novo motif
detection into a semi-unsupervised learning problem. It provides more control during the search
for novel motif patterns by making use of prior knowledge implied in the known motifs, helps to
improve sensitivity to biologically plausible motifs, and potentially reduces spurious solutions often
occurred in a purely unsupervised setting.
It may be possible to build a stronger motif classiﬁer using discriminative approaches such as
neural networks or support vector machines, and we are currently pursuing this direction. But since
the goal of this chapter is not merely to build a classiﬁer, but to develop a model that can easily
84

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
be integrated into a more general architecture for de novo motif detection, a generative framework,
especially via a Bayesian prior model, provides the desired generalizability and ﬂexibility for such
tasks. As discussed in §2.2.4, a graphical model formalism of the motif detection problem allows
a modular combination of heterogeneous submodels each addressing a particular component of the
overall problem. The design of MotifPrototyper aligns with this principle, and serves as an advanced
“local” submodel under the LOGOS framework.
2.5
CisModuler: Modeling the Syntactic Rules of Motif Organization
As discussed in previous sections, the transcription regulatory sequences in higher eukaryotic genomes
often consist of multiple CRMs. Each CRM contains locally enriched occurrences of binding sites
for a certain array of regulatory proteins, capable of integrating, amplifying or attenuating multi-
ple regulatory signals via combinatorial interaction with these proteins. The architecture of CRM
organization is reminiscent of the grammatical rules underlying a natural language, and provides
the potential for implementing sophisticated regulatory circuits directing temporally/spatially co-
ordinated expression of genes during development and differentiation. It also presents a particular
challenge to computational motif and CRM identiﬁcation in higher eukaryotes. In this section, we
present CisModuler, a Bayesian hidden Markov model that attempts to capture the stochastic syn-
tactic rules of CRM organization and integrates over (and thus draws inﬂuence from) all possible
values of the Markov transition probabilities weighted by their corresponding prior probabilities
that reﬂect general knowledge of the CRM structure. Under the CisModuler model, all candidate
sites are evaluated based on a posterior probability measure that takes into consideration their sim-
ilarity to known binding sites, their contrasts against local genomic context, and their ﬁrst-order
dependencies on upstream sequence elements. We compare this approach to the standard window-
based likelihood scoring approach described previously, and demonstrate superior results on large
scale analysis of Drosophila early developmental enhancers. This model provides a useful and ar-
guably superior alternative for CRM/motif detection given motif PWMs, and can be also used as a
submodel (i.e., the global model) for de novo motif/CRM detection in higher eukaryotic genomes.
85

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
2.5.1
The CisModuler Hidden Markov Model
Hidden Markov models have been widely used in computational biology to capture simple sequence
structures (e.g., segmentations) inherent in bio-polymer sequences. Despite their limited expressive
power compared to more complex models such as stochastic context free grammars (SCFGs) [Lari
and Young, 1990] or hierarchical hidden Markov models (hHMMs) [Fine et al., 1998], they have
enjoyed remarkable success in problems such as gene-ﬁnding in DNAs [Burge and Karlin, 1997]
and domain modeling in proteins [Krogh et al., 1994], and in many cases appear to strike the right
balance between simplicity and expressiveness.
We propose to use an HMM to model the global distribution of motif instances in genomic
sequences, by encoding a set of stochastic syntactic rules presumably underlying the CRM organi-
zation and motif dependencies using a discrete ﬁrst-order Markov process. We call this specialized
HMM a CisModuler. The CisModuler HMM deﬁnes a probability distribution over possible func-
tional states of each single position in a DNA sequence. The space of allowed functional states
is constructed in a way that captures detailed architectural features of genomic sequences bearing
CRMs.
More precisely, let X = (X1, . . . , XT ) be a chain of “hidden” state variables associated with
an “observed” DNA sequence y = (y1, . . . , yT ), specifying which functional state (e.g., a back-
ground, the l-th position of motif k, etc.) is responsible for generating the observed nucleotide at
each position. By deﬁnition, xt ∈S, where the state space S includes all possible functional states
of a position in a CRM-bearing DNA sequence. Speciﬁcally, S = M ∪M′ ∪Bp ∪Bd ∪{bg, bc},
where M = {1(1) . . . L
(1)
1 , 1(2) . . . L
(2)
2 , . . . , 1(k) . . . L
(k)
k } is the set of all possible sites within a motif
on the forward DNA strand (i.e., states 1(1) to L
(1)
1
correspond to the sites in motif type 1 on the
forward strand, and so on); M′ is the set of all possible sites within a motif if it is on the reverse
complementary DNA strand; Bp = {b(1)
p , . . . , b(k)
p } denotes the set of proximal-buffer states associ-
ated with each type of motif 5; Bd = {b
(1)
d , . . . , b
(k)
d } denotes the set of distal-buffer states associated
5Here, proximal-buffer refers to the background sites immediately next to the proximal-end of the motif. For consis-
tency, orientations are deﬁned with respect to the initial position of the input sequence. That is, the 1st position of the
input sequence corresponds to the proximal end, and the last position corresponds to the distal end.
86

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
with each type of motif; bc represents the intra-cluster (local) background state; and bg represents
the inter-cluster (global) background state. Permissible transitions between these states are illus-
trated in Figure 2.15. The distribution of X follows a ﬁrst-order Markov process according to this
transition scheme, with state-transition probabilities parameterized as shown in the state-transition
diagram. Note that we have not included functional states related to gene annotation and basal pro-
moters, but such extensions are straightforward if co-identiﬁcation of CRMs and genes is desired.
(1)
(1)
(1)
...
1
2
L
(1’)
...
1
L
(1’)
2
(1’)
1
1
1
1
1
1
βk,1
β1,k
βk,k
1
1
1
1
(k)
(k)
(k)
...
1
2
L
(k’)
...
L
(k’)
2
(k’)
1
k
k
β1,1
c,c
β
g,g
β
βg,1
βg,k
β1,g
βk,g
βc,1
βc,k
βk,c
β1,c
...
α/2
α/2
α/2
α/2
1
1
1
1
global bg
cluster bg
Motif 1
Motif k
buffer
buffer
buffer
buffer
(a)
(b)
Figure 2.15: The CisModuler HMM. (a) The state-transition diagram. Labeled circular and diamond nodes represent the
functional states in DNA sequences; arrows between nodes represent permissible state-transitions; numbers and param-
eter symbols accompanied the arrows (with the parameter subscripts denoting the source and target of the transitions)
denote the corresponding transition probabilities. (b) A typical segmentation of a piece of DNA sequence induced by
CisModuler. Background sites are colored as in the state-transition diagram; the blue, magenta, yellow and red segments
represent motifs of 4 different kinds; segments with parallel stripes denote motifs in reverse-complementary orientation.
The motivation for this Markov model is that generally one expects to see occasional motif clus-
ters in a large ocean of global background sequences (represented by state bg). Each motif instance
87

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
in a cluster is like an island in a sea of intra-cluster background sequences (bc’s), with surrounding
coastal water of motif-speciﬁc buffer sequences (i.e., b(i)
p ’s and b
(i)
d ’s for motif i) (Fig. 2.15). We
refer to a motif instance together with its surrounding buffers as a motif envelope. The CisModuler
model assumes that the distance between clusters is geometrically distributed with mean 1/(1−βg,g),
and the span of the intra-cluster sea is also geometrically distributed with mean 1/(1 −βc,c). How-
ever, the distances between motifs admit a much richer distribution, because the widths of the motif
envelopes are modeled on a motif-speciﬁc basis, and the transitions between envelopes can occur ei-
ther by sailing through the intra-cluster sea or by bypassing it. These modeling choices are intended
not only to reﬂect uncertainty about the CRM structure, but also to offer substantial ﬂexibility to
accommodate potential richness of CRM structures. As shown in Figure 2.15a and 2.15b, one can
begin with a global background state, then either loop over this state, or with some probability βg,i,
move into the proximal-buffer state of a motif i; with equal probability αi,m/2, a proximal-buffer
state b(i)
p reaches the start states 1(i) (resp. L
(i′)
i ) of motif i on the forward (resp. reverse) strand,
deterministically passes through all internal sites of motif i, and transitions to the distal-buffer state
b
(i)
d , thereby stochastically generating a non-empty motif envelope 6; each b
(i)
d has some probability
βi,j/2 of transitioning to the proximal-buffer state of another motif j (or of the same motif when
j = i) to concatenate another motif envelope, or with probability βi,c to pad with some intra-cluster
background before adding more envelopes; all distal-buffer states also have probability βi,g of re-
turning to the global background state, terminating a CRM stretch. It is not difﬁcult to see that a
path in such a state space according to this HMM grammar bears a structure similar to a genomic
sequence containing motif modules (Figure 2.15b). Note that the HMM model does not impose
rigid constraints on the number of motif instances or modules; the actual number of instances is
determined by the posterior distribution of the sequence of functional states, p(x|y).
The use of an HMM to model the CRM distribution has been previously described by Frith et
al. in the Cister program [Frith et al., 2001]. But the CisModuler model we present here uses a
6Note that the distinction between the proximal and distal buffers avoids generating empty envelopes (because other-
wise, a single buffer state would not be able to remember whether a motif has been generated beyond k positions prior to
the current position under a kth order Markov model.)
88

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
much more sophisticated design of the functional state space that allows couplings between motifs
within the CRMs to be captured, and models inter-motif distances with more ﬂexible distributions
(rather than a simple geometric distribution). Furthermore, as will be detailed in the following
sections, we provide a Bayesian treatment for the state transition probabilities, which in previous
models are regarded as ﬁxed parameters and rely on empirical default values or user speciﬁcation.
We also combine the newly designed HMM with a more expressive kth-order Markov model for
the background, which turns out to contributes to signiﬁcantly improving the speciﬁcity for CRM
detection.
2.5.2
Bayesian HMM
One caveat of the standard HMM approach for CRM modeling is the difﬁculty of ﬁtting the model
parameters, such as the state-transition probabilities, due to the scarcity of fully annotated CRM-
bearing genomic sequences. In principle, one can learn the maximal likelihood estimates of the
model parameters in an unsupervised fashion, using the Baum-Welch algorithm, directly from the
unannotated sequences while analyzing them. But in practice, such a completely likelihood-driven
approach tends to result in spurious results, such as over-estimation of the motif and CRM frequen-
cies and poor stringency of the learned models of potential motif patterns. Previous methods tried to
overcome this by reducing as much as possible the number of parameters needed, and setting them
according to some best guesses of the motif/CRM frequencies or CRM sizes [Frith et al., 2001]. But
as a result, such remedies compromise the expressive power of the already simple HMM, and risk
misrepresenting the actual CRM structures. In the following, we propose a Bayesian approach that
introduces the desired “soft constraints” and smoothing effect for an HMM of rich parameteriza-
tion, using only a small number of hyper-parameters. Essentially, this approach deﬁnes a posterior
probability distribution over all possible value-assignments for the HMM parameters, given the ob-
served unannotated sequences and empirical prior distributions of the parameters that reﬂect general
knowledge of CRM structures. The resulting model allows probabilistic queries (i.e., estimating the
probability of a functional state) to be answered based on the aforementioned posterior distribution
89

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
rather than on ﬁxed given values of the HMM parameters.
We assume that the self-transition probability of the global background state βg,g, and the to-
tal probability mass of transitioning into a motif-buffer state P
k∈Bp βg,k (note that βg,g = 1 −
P
k∈Bp βg,k), admit a beta distribution, Beta(ξg,1, ξg,2), where a small value was chosen for
ξg,2
ξg,1+ξg,2 ,
corresponding to a prior expectation of a low CRM frequency. Similarly, a beta prior Beta(ξc,1, ξc,2)
is deﬁned for the self- and total motif-buffer-going transition probabilities [βc,c, P
k∈Bp βc,k] asso-
ciated with the intra-cluster background state; and another beta prior Beta(ξp,1, ξp,2) for the self-
and motif-going transition probabilities [αi,i, αi,m] associated with the proximal-buffer state of
a motif. Finally, it is assumed that, for the distal-buffer state, the self-transition probability, the
total mass of transition probabilities into a proximal-buffer state, the probability of transitioning
into the intra-cluster background, and the probability of transitioning into the global background,
[βi,i, P
k∈Bp βi,k, βi,c, βi,g], admit a 4-dimensional gamma distribution, Gamma(ξd,1, ξd,2, ξd,3, ξd,4).
Note that due to conjugacy between the prior distributions described above and the corre-
sponding transition probabilities they model, the hyper-parameters of the above prior distribu-
tions can be understood as pseudo-counts of the corresponding transitioning events, which can
be roughly speciﬁed according to empirical guesses of the motif and CRM frequencies. But un-
like the standard HMM approach, in which the transition probabilities are ﬁxed once speciﬁed,
the hyper-parameters only lead to a soft enforcement of the empirical syntactic rules of CRM
organization in terms of prior distributions, allowing controlled posterior updating of the HMM
transition probabilities during analysis of the unannotated sequences. For the CisModuler HMM,
we specify the hyperparameters (i.e., the pseudo-counts) using estimated frequencies of the cor-
responding state-transition events, multiplied by a “prior strength” N, which corresponds to an
imaginary “total number of events” from which the estimated frequencies are “derived”. That is,
for the beta priors, we let [ξ[·,1], ξ[·,2]] = [1 −ω[·], ω[·]] × N, where the “·” in the subscript denotes
either the g, c, or p state, and ω[·] is the corresponding frequency. For the gamma prior, we let
[ξd,1, ξd,2, ξd,3, ξd,4] = [ωd,1, 1 −P
j ωd,j, ωd,2, ωd,3] × N. Overall, 7 hyper-parameters need to
be speciﬁed (of course one can use different “strengths” for different prior, with a few additional
90

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
parameters), a modest increase compared to the three needed in Cister [Frith et al., 2001].
2.5.3
Markov Background Models
Several previous studies have stressed the importance of using a richer background model for the
non-motif sequences [Liu et al., 2001; Huang et al., 2004]. In accordance with these results, Cis-
Moduler uses a global kth-order Markov model for the emission probabilities of the global back-
ground state. For the emission probabilities of the intra-cluster background state and the motif-
buffer states, we used two local Markov models of order m and m′, respectively. Since the models
are deﬁned to be local, the conditional probability of a nucleotide at a position t is now estimated
from all (m + 1)- (resp. (m′ + 1)-) tuples from a window of 2d (resp. 2d′) centered at t. These
probabilities can also be computed off-line and stored for subsequent use. With a careful bookkeep-
ing scheme (i.e., using a “sliding window” to compute the local Markov model of each successive
position, each with a constant “update cost” based on the previous one, except for the initial win-
dow that needs a cost quadratic in the window size), this computation takes only O(T) time. For
the emission probabilities of the motif states, we directly use the appropriate columns of nucleotide
frequencies in the PWM of the corresponding motif.
2.5.4
Posterior Decoding Algorithms for Motif Scan
2.5.4.1
The baseline algorithm
Given the initial state distribution and transition probability matrix of the HMM, the background
probabilities of each nucleotide, and the PWMs of the motifs to be searched for, the posterior prob-
ability distribution of the functional states at each position of the sequences, p(xt|y), ∀t, can be
computed using the forward-backward algorithm. One can read off the functional annotation (or
segmentation) of the input sequences from p(xt|y) according to a maximal a posteriori (MAP)
scheme, that is, the predicted functional state of position t is:
x∗
t = arg max
s∈S p(Xt = s|y)
(2.23)
Note that by using such a posterior decoding scheme (rather than the Viterbi algorithm), one
91

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
integrates the contributions of all possible functional state paths for the input sequence (rather than
a single “most probable” path), into the posterior probability for each position. Therefore, although
the HMM architecture does not explicitly model overlapping motifs, the inference procedure does
take into account possible contributions of DNA binding sites interacting with competing TFs.
2.5.4.2
Bayesian inference and learning
Under the Bayesian framework described in § 2.5.2, the parameters in the HMM are treated as con-
tinuous random variables (collectively referred to as Ω) with a prior distribution. Now to compute
the posterior probability of functional states needed in Eq. (2.23), one needs to marginalize out
these parameter variables:
p(xt|y) =
Z
p(xt|y, Ω)p(Ω|y)dΩ
(2.24)
This computation is intractable in closed form. One approach to obtaining an approximate so-
lution is to use Markov chain Monte Carlo methods (e.g., a Gibbs sampling scheme). Here we use a
more efﬁcient, deterministic approximation scheme based on generalized mean ﬁeld (GMF) infer-
ence, also referred to as variational Bayesian learning [Ghahramani and Beal, 2001] in the special
scenario that is applicable to our problem setting. We will discuss the theoretical and algorithmic
details of GMF inference at length in Chapter 4. Operationally, a posterior decoding algorithm un-
der the Bayesian HMM setting can be understood as replacing the single-round posterior decoding
with an iterative procedure consisting of the following two steps:
• Compute the expected counts for all state-transition events (i.e., sufﬁcient statistics) using the
forward-background algorithm, using current values of the HMM parameters.
• Compute the Bayesian estimate (to be detailed shortly) of the HMM parameters based on
their prior distribution and the expected sufﬁcient statistics from the last step. Update the
HMM parameters with these estimations.
This procedure is different from the standard EM algorithm which alternates between inference
about the hidden variables (the E step) and maximal likelihood estimation of the model parameters
92

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
(the M step). In a GMF algorithm, the “M” step is a Bayesian estimation step, in which one com-
putes the posterior expectation of the HMM parameters, which will lead to an optimal lower bound
on the true likelihood of the data (which is intractable to compute exactly for a Bayesian HMM)
(see Chapter 4).
Now we outline the formulas for Bayesian estimation of the HMM parameters. Note that be-
cause the state-transition probability distributions (which are multinomial) and the prior distribu-
tions of the transition parameters (which are either beta or gamma) are conjugate-exponential [Beal
et al., 2001], we have to compute the Bayesian estimate of the logarithms of the transition parame-
ters (referred to as the natural parameters) rather than of the parameters themselves. For example,
for the state-transition parameter βg,g, we have:
E[ln(βg,g)] =
Z
βg,g
ln βg,gp(βg,g|ξg,1, ξg,2, E[ng,g])dβg,g
= Ψ(ξg,1 + E[ng,g]) −Ψ(
X
j
ξg,j +
X
k∈Bp
E[ng,k]),
(2.25)
where Ψ(x) = ∂log Γ(x)
∂x
=
˙Γ(x)
Γ(x) is the digamma function; E[·] denotes the expectation with respect
to the posterior distribution of the argument; and ng,g refers to the sufﬁcient statistic of the parameter
βg,g (i.e., the counts of the transition event g →g). The Bayesian estimate of the original parameter
is simply β∗
g,g = exp(E[ln(βg,g)]). (In fact we keep using the natural parameterization in the actual
forward-background inference algorithm to avoid numerical underﬂow caused by a long product of
probability terms.) The individual “motif-buffer-going” probability βg,i can be estimated similarly.
The initial state probability of the the CisModuler HMM is not important for CRM prediction
as it only directly determines the functional state of the ﬁrst position of the input sequences and its
inﬂuence diminishes quickly along the sequence. One can simply ﬁx the initial state to be a global
background state with probability 1.
2.5.5
Experiments
Although the literature on transcription regulation mechanisms in higher eukaryotes is very rich,
there still exist great biological ambiguities in motif and CRM annotations in many metazoan
93

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
genomes. To evaluate our model using relatively unambiguous criteria, we focus on 14 loci in the
Drosophila genome that are well known to be involved in regulating the transcription of Drosophila
early developmental genes (Table 2.5). Papatsenko et al. [2002] have carefully curated these loci
based on an extensive study of the literature. Their compilation delineates 19 best-known early
Drosophila developmental enhancers from these loci, where reside binding sites for 3 maternal tran-
scription factors: Bicoid (Bcd), Caudal (Cad), Dorsal (DI), as well as the zygotic gap gene factors
Hunchback (Hb), Kruppel (Kr), Knirps (Kni), Tailless (Tll), and Gaint (Gt). To mimic the typical
motif/CRM search scenario in metazoan genomic analysis (i.e., using a single long sequence po-
tentially containing numerous motifs rather than multiple short promoter regions from co-regulated
genes of simple organisms such as yeast), for each locus we extract a 5000 to 20000 bp long ge-
nomic region surrounding the enhancers as input data. Note that it is possible that there may exist
additional unknown motifs/CRMs in these extended regions.
Table 2.5: Developmental regulatory loci in Drosophila genome.
locus (target gene)
regulators
length
# of CRMs
Abdominal-A
Hb, Kr, Gt,
10000
1
Buttonhead
Bcd, Hb
5000
1
Engrailed
Cad, Ftz
10000
1
Even-skipped
Hb, Kni, Bcd, Kr, Gt
20000
3
Fushi-Tarazu
Ftz, Ttk, Cad
10000
2
Gooseberry
Eve, Prd (HD)
10000
1
Hairy
Kr, Hb, Kni, Cad, Gt
10000
3
Kruppel
Bcd, Hb, Gt, Kni
10000
1
Orthodentcile
Bcd
5000
1
Runt
Kr, Gt, Hb, Kni
10000
1
Spalt
Bcd, Hb, Kr, Cad
10000
1
Tailless
Bcd, Cad
8227
1
Ultrabithorax BRE
Hb, Ftz, Tll
10000
1
Ultrabithorax PBX
Hb, Ftz, Tll
10000
1
As discussed above, the hyperparameters of the CisModuler model reﬂect prior beliefs about
the architectural features of the CRM structure, such as rough spans of the inter- or intra-module
background and distances between motif instances. We specify these hyperparameters as follows:
for the global background, ωg = 0.0002; for the intra-module background, ωc = 0.01; for the
proximal motif buffer, ωp = 0.1; for the distal buffer hyperparameters, ωd,1 = 0.1, ωd,2 = 0.4,
ωd,3 = 0.4; and for the strength of the hyperparameters, N = 500. The background probability
of the nucleotide at each position was computed locally using a 3rd-order Markov model from a
94

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
sliding window of 600 bp centered at the corresponding position. Since we are scanning for known
motifs, the PWMs of the motifs to be found are taken from [Papatsenko et al., 2002] and [Berman
et al., 2002].
2.5.5.1
MAP prediction of motifs/CRMs
The locations of individual motif instances and CRMs in a DNA sequence can be determined from
its associated state sequence that corresponds to the MAP states of all the DNA sites. Figure 2.16a
shows the MAP states and the associated posterior probabilities of these states in a 5000 bp region
at the Drosophila buttonhead locus. As shown in the graphical illustration below the MAP plot, this
region contains a CRM between positions 330 and 1504, and part of the coding sequence of the
buttonhead gene. Fine-grained annotations indicate that a core subregion at the proximal end of this
CRM (positions 447-660) harbors 5 Bcd motifs. Another 4 Bcd instances are clustered at the distal
end of this CRM (positions 1150-1354). Three additional motifs (2 Bcds and 1 Hb) are scattered in
the middle of this CRM, but they appear to be weaker matches to the motif consensus compared to
the ones in the core subregions [Papatsenko et al., 2002]. Using MAP estimation under CisModuler,
7 of the 12 motifs, 3 in the proximal and 4 in distal subregions of the CRM, are identiﬁed and the
core regions of the buttonhead CRM are correctly identiﬁed. Overall, among the 335 motif instances
(of 11 different regulatory proteins) and 19 CRMs contained in the loci we analyzed, 80 motifs and
16 CRMs are correctly identiﬁed, out of a total prediction of 316 motifs and 51 CRMs.
Under the aforementioned parameterization of CisModuler, the sensitivity measure of our pre-
diction (i.e., correct predictions/total annotated motifs) is about 25%. But it is worth pointing out
that this result is obtained at a very low noise-to-signal ratio (i.e., incorrect predictions/correct pre-
dictions) of less than 3. Most extant algorithms report a list of predictions ranked by the score and
provide no quantitative measure of prediction accuracy suitable for a comparison. A few extant
algorithms reported higher sensitivity, but at an extremely high N/S ratio. For example, the log-
odds-based MATCH program [Quandt et al., 1995] achieves a ∼90% sensitivity with a N/S ratio of
95

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
1379 and 784 for Ap-1 and NEAT sites, respectively; the more sophisticated comparative-genomics-
based rVista program [Loots et al., 2002] achieves a similar sensitivity with N/S ratios of about 69
and 38, respectively. Such a high N/S ratio can make experimental veriﬁcation extremely hard or
even infeasible, signiﬁcantly compromising the value of the predictions. Also worth mentioning is
that our CRM-prediction using CisModuler is even more reliable, with an 84% sensitivity, and 2.18
N/S. Thus it is possible to ﬁrst identify the CRMs using a coarser-grained model, and then zoom in
to ﬁnd motifs within the CRMs using a ﬁner-grained model.
Note that unlike many other scoring schemes for motif/CRM detection, such as the log odds
or likelihood score regularized by word frequencies, our MAP prediction does not require a cutoff
value for the scores, nor a window to measure the local concentration of motif instances, both of
which are difﬁcult to set optimally. To show the potential advantage of the MAP approach, Fig 2.16b
shows the log odds of all sites in the buttonhead locus. Simply from inspection, it is apparent that,
even though we compute the log odds based on a more discriminating Markovian background model
together with the motif PWMs, we end up with too many positive signals (i.e., peaks with log odds
> 0). Exponentiating the log odds of all sites and thus transforming them to likelihood ratios (the
lower small panel in the graph) can signiﬁcantly improve the contrast, but compared to the MAP
plot and the sketch of the genomic structure of this region, pruning away noises via a good cutoff
value and scoring window is still a non-trivial task.
2.5.5.2
Motif/CRM prediction via thresholding posterior probability proﬁle
The MAP prediction described in the previous section considers only a single (i.e. the a posteriori
most probable) functional state for each site in a DNA sequence, and to some degree underuses
the posterior probabilities of all possible functional states for each site. An alternative approach
is to use the full posterior probability distribution at each site as a score function, and analyze the
score proﬁle of the whole sequence using strategies conventionally applied to log odds or likelihood
proﬁles, such as thresholding motif scores with a cutoff value (to qualify a motif instance) and
measuring local motif concentrations with a sliding window (to qualify a CRM).
96

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
CRM
CDS
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1000
2000
3000
4000
5000
0
1
2
3
4
5
6
7
8
1000
2000
3000
4000
5000
−30
−20
−10
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
(a)
(b)
Figure 2.16: Motif- and CRM-scan using CisModuler. (a) MAP plot of the buttonhead locus under the CisModuler model.
The y axis denotes posterior probability, and the x axis represents sites in the sequence. The black curve corresponds
to the global background state, the green curve corresponds to the intra-cluster background and buffer states, and other
color curves correspond to various motif states (red:Hb, blue:Bcd, and dotted curves correspond to the state of a reverse
oriented motif represented by the same color). For each site, only the posterior probability of the MAP state is plotted.
(b) Log odds of each site under the motif PWM versus a 3rd-order local Markov background model. Only positive scores
(i.e., higher motif prob. than background prob.) are shown in the large panel. Complete log odds proﬁles (including
the negative scores that indicate the background) are shown in the upper small panel for reference. The likelihood ratio
scores derived from the log odds are shown in the lower small panel. Between panels (a) and (b) is a graphical illustration
of the biological annotation of this region.
97

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
0
0.2
0.4
0.6
0
5
10
15
20
25
30
Sensitivity
Noise/Signal
Motif prediction (total map)
logodds
posterior
MAP
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
1
2
3
4
5
6
7
Sensitivity
Noise/Signal
CRM prediction
logodds
posterior
MAP
(a)
(b)
Figure 2.17: Validation of the posterior score under CisModuler and the likelihood ratio score: trade-off between sensi-
tivity and noise. (a) Motif detection. (b) CRM detection. The performance curves record sensitivity and N/S achieved at
a wide range of score cutoff values.
Fig. 2.17a shows the trade-off between sensitivity and noise during motif detection, in terms of
the proportion of the known binding sites detected and the amount of concomitant noise generated.
Following [Huang et al., 2004], Fig. 2.17a traces the balance of sensitivity versus N/S ratio achieved
at a wide range of score cutoffs. Two score proﬁles were analyzed, the posterior probability proﬁle
computed using the CisModuler model for the 14 Drosophila loci described before (red curve in
Fig. 2.17a), and a likelihood ratio proﬁle for the same dataset computed using motif PWMs and
a 3rd-order local Markov model (black curve in Fig. 2.17a). Overall, the CisModuler posterior
probability score outperforms the likelihood ratio score over the entire range of noise-to-signal
ratio. Although not directly comparable (since different datasets are used), the performance curve
is similar to that of [Huang et al., 2004] (or arguably better because of the longer input sequences
used and the presence of CRMs that complicate motif identiﬁcations.) It is interesting to note that
the MAP prediction seems to be trying to pick the best possible sensitivity in the low noise-to-signal
ratio region.
Fig. 2.17b shows the trade-off between sensitivity and N/S ratio for CRM (rather than motif)
detection. The following scheme were used to identify a CRM based on the score proﬁle: under a
given cutoff value of motif score, if the motif density within a sliding window of length W is at least
c, the corresponding sequence stretch is regarded as covered by a CRM. A contiguous region swept
98

2.5 CisModuler: Modeling the Syntactic Rules of Motif Organization
by a sliding window that meets this criteria is regarded as a CRM. Following typical characteristics
of CRMs reported in the literature, we set W = 500 and c = 2%. The sensitivity of CRM detection
is deﬁned to be the ratio of the number of correctly predicted CRMs to the total number of CRMs;
and the N/S ratio as the ratio of the total length of all predicted CRMs over the length of correctly
predicted CRMs. From Fig. 2.17b, it appears that CisModuler slightly outperform the the likelihood
ratio scheme in the high N/S region, and is signiﬁcantly better in the low N/S region. As mentioned
earlier, the MAP prediction ﬁnds a good trade-off between the sensitivity and N/S ratio.
From the experiments reported above, we are optimistic that CisModuler is superior in motif
and CRM detection in a complex genomic context. But to predict based on the full posterior prob-
ability proﬁle, a cutoff value is needed to qualify the possible presence of motif instances, and a
window size will be used to infer CRMs based on within-window concentration. Usually, such val-
ues have to be carefully determined from a training dataset, or via a statistical signiﬁcance criterion
as in [Huang et al., 2004]. As a reward, we can take advantage of both the motif dependencies and
syntactic architecture of motif distributions explicitly captured in the CisModuler, the ﬂexibility of
the thresholding scheme, and the nice statistical guarantee provided by a signiﬁcance test.
2.5.6
Summary and Discussion
In this section, we presented a model-based Bayesian approach for CRM and motif prediction,
which combines many of the desirable features provided by extant methods, and introduces several
important novel elements that overcome some of the shortcomings of extant methods. The exten-
sions and contributions includes: a more sophisticated HMM model that is intended to capture, to
a reasonable degree, the detailed syntactic structure of CRM and cis-regulatory regions containing
CRMs; Bayesian priors for various state-transition parameters of the HMM grammar, which in prin-
ciple alleviate user speciﬁcation of model parameters 7; and several kth-order Markov models for
various types of background sequences.
We compared our approach to the standard likelihood-ratio (or log odds) scoring approach,
7Although sophisticated users could choose to decide the “strength” of the priors, or deﬁne their own priors.
99

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
and demonstrated superior results on large scale Drosophila early developmental enhancer analysis.
CisModuler provides a useful and arguably superior alternative approach to detect CRM and motif
occurrences based on a given PWM, and can be also used as a subroutine in de novo motif/CRM
detection from higher eukaryotic genomes.
2.6
LOGOS: for Semi-unsupervised de novo Motif Detection
Recall that under the LOGOS framework, the local, global and background submodels jointly de-
ﬁne the likelihood of an observed DNA sequence that contains unspeciﬁed motifs. Each submodel
can be designed separately to address different aspects of the biological characteristics of a transcrip-
tional regulatory sequence, and combination of submodels each from a wide spectrum of possible
designs is possible. Therefore, LOGOS facilitates a ﬂexible trade-off between expressiveness and
complexity for motif modeling.
Most extant models for de novo motif detection fall into the most basic submodel combination,
namely, a PM local model plus a UI global model (denoted by LOGOSpu in the sequel). Exam-
ples of LOGOSpu include the basic models underlying the MEME [Bailey and Elkan, 1995a] and
AlignACE [Hughes et al., 2000] programs (although both programs have more sophisticated and
efﬁcient implementation, e.g., more careful initiation schemes for over-represented words, which in
practice improve their performance over a basic LOGOSpu model).
Having both the MotifPrototyper model for local motif structure and the CisModuler model for
global motif organization (which also includes the kth-order Markov model for the background),
one can envisage a novel generative model for transcriptional regulatory sequences that is signiﬁ-
cantly more expressive than any extant motif detection models. A graphical representation of such
a model, which is referred to as LOGOShh (standing for HMDM + HMM), is depicted in Fig-
ure 2.18. (For simplicity, in the sequel we abbreviate LOGOShh with the unsubscripted “LOGOS”
when no confusion arises in the context, e.g., no comparison with other variations of LOGOS is
being made.) Speciﬁcally, in such a LOGOS model, the functional annotations of a DNA sequence
that determine the motif locations and modular structures are determined by a CisModuler HMM
100

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
Figure 2.18: A modular construction of the LOGOShh model for motif-bearing sequences. For simplicity, only one
possible motif and one study sequence are included in this model.
model; but the emission probabilities of the motif states, or the PWMs of the motifs, are generated
from a MotifPrototyper model, whereby prior knowledge regarding both global motif organization
and local motif structure is incorporated for de novo motif detection, making it a semi-unsupervised
learning problem. Note that the context-sensitive dependency induced by the latent motif indicator
x couples each sequence variable with all existing motif parameters. Thus the nucleotide identity of
any position in a sequence is determined by a complex dependency structure that captures intra and
inter motif dependencies reﬂecting both intrinsic structural properties and higher-level correlations
of the motifs (detailed in §4.7).
The expressiveness of LOGOS comes at the cost of resulting in a very expensive computational
problem for probabilistic inference in this model. Speciﬁcally, the inference and learning problem
associated with LOGOS is that of computing the maximum a posteriori prediction of motif lo-
cations and the Bayesian estimate of motif PWMs. But unlike the situations we discussed when
dealing with only the MotifPrototyper or the CisModuler submodels of LOGOS, which essentially
intend to solve only one of the two aforementioned queries assuming the solution to the other is
known, now one needs to solve the two queries jointly, and this turns out to be intractable due to the
101

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
exploded state space of the joint model.
Since no off-the-shelf exact algorithm works for LOGOS, some approximation schemes have
to be used. One option is to pursue a stochastic approximation using MCMC techniques such as
Gibbs sampling. In Chapter 5, we describe a Gibbs sampler algorithm for posterior inference on
LOGOS. But as demonstrated in a preliminary experiment on modest-sized input sequences (see
§4.7.2), the Gibbs sampler converges very slowly and appears impractical for supporting a realistic
motif detection program. In Chapter 4, we develop a deterministic approximation method called
generalized mean ﬁeld inference. Essentially, a GMF algorithm alternates between solving one of
the two sub-problems mentioned before in the respective submodel of LOGOS, conditioning on
the approximate solution of the other sub-problem, and then updating the approximate solutions
using the newly obtained solutions, which yields a better approximation. It can be shown that this
algorithm is guaranteed to converge to a locally optimal solution, and deﬁnes a lower bound on the
likelihood of the study sequences. A full description of the theory and algorithm of GMF inference
in general graphical models, and speciﬁcally the ﬁxed-point equations for the LOGOS model, is
deferred to Chapter 4. In the following, we present an extensive validation of the LOGOS model
on fully annotated semi-realistic datasets and real genomic sequences from yeast, and a preliminary
test on a small set of unannotated Drosophila genomic sequences.
2.6.1
Experiments
2.6.1.1
Performance on semi-realistic sequence data
Recall that in §2.5, we validated the utility of the MotifPrototyper model for de novo motif de-
tection in conjunction with a trivial global model — oops, which assume one motif per sequence.
Now we consider a more realistic scenario, in which each study sequence contains multiple mo-
tifs. We compare three variants of the LOGOS model for this setting, ordered by decreasing model
expressiveness, HMDM+HMM (LOGOShh), PM+HMM (LOGOSph) and PM+UI (LOGOSpu).
Speciﬁcally, a slightly simpliﬁed LOGOShh is used for the task herein, where the global submodel
is a simpler HMM containing only a single global background state in addition to the motif states
102

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
(meaning that no CRM structure is modeled), rather than using the highly elaborated CisModuler
Bayesian HMM tailored for higher eukaryotic sequences.
Single motif, and multiple instances per sequence.
Under a realistic motif detection condition,
the number of motif instances is unknown. Rather than trying all possible numbers of occurrences
suggested by the user or decided by the algorithm and reporting a heuristically determined plau-
sible number, LOGOS uses the global HMM model to describe a posterior distribution for motif
instances, which depends on both the prespeciﬁed indicator state transition probabilities and the ac-
tual sequence y to be analyzed. In this experiment, the transition probabilities are empirically set at a
default value to reﬂect our rough estimates of motif frequencies (i.e., 5%). But as more training data
of annotated regulatory sequences are collected, these parameters can be ﬁt in a genome-speciﬁc
fashion.
Table 2.6: Performance of LOGOS for single motif detection, with unknown number of instances per sequence.
motif
LOGOShh
LOGOSph
LOGOSpu
name
FP
FN
FP
FN
FP
FN
abf1
0.3115
0.2116
0.6774
0.1957
0.7917
0.9123
gal4
0.1569
0.1569
0.1895
0.1534
0.2917
0.7939
gcn4
0.1820
0.2355
0.6142
0.2821
0
0.9594
gcr1
0.1962
0.2134
0.3371
0.2038
0.3333
0.9437
mat
0.0723
0.0337
0.3563
0
0.5000
0.9643
mcb
0.3734
0.0910
0.3628
0.0792
0.3333
0.9431
mig1
0.0774
0
0.0854
0
0.9764
0.1000
crp
0.3768
0.3398
0.2727
0.5294
0
0.9487
Table 2.6 summarizes the performance of three variants of LOGOS for single motif detection,
with an unknown number of instances per sequence. We present the median false positive (FP)
and false negative (FN) rates (in terms of ﬁnding each instance of the motifs within an offset of
3 bp) of motif detection experiments over 20 test datasets. Each test dataset consists of 20 se-
quences, each generated by planting (uniformly at random) 0–7 instances of a motif (real sites from
SCPD), together with its permuted “decoy,” in a 300–400 bp random background sequence. As
Table 2.6 shows, LOGOSpu yields the weakest results, losing in all 8 motif detections (in terms
of (FP+FN)/2), suggesting that the conventional PM+UI model, which is used in MEME, and with
103

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
slight variation, in AlignACE and BioProspector, is not powerful enough to handle non-trivial de-
tection tasks as posed by our test set. LOGOSph improves signiﬁcantly over LOGOSpu, even
yielding the best performance in one case (for mcb), suggesting that the HMM global model we
introduced indeed strengthens the motif detector. Finally, as hoped, LOGOShh yields the strongest
results, performing best on 7 of the 8 motifs, convincingly showing that capturing the internal struc-
tures of motifs and making use of prior knowledge from known motifs, combined with the use of
the HMM global model, can yield substantially improved performance. Our results are reasonably
robust under different choices of the global HMM parameters.
Simultaneous detection of multiple motifs.
Detecting multiple motifs simultaneously is ar-
guably a better strategy than detecting one at a time and then deleting or masking the detected
motifs, especially when motif concentrations are high, because the latter strategy mistakenly treats
the other motifs as background, causing potentially suboptimal estimation of both motif and back-
ground parameters. The global HMM model we propose readily handles simultaneous multiple
motif detection (say, ﬁnding K motifs at a time): we only need to encode all motif states into the
state space S of the motif indicator X, and perform standard HMM inference. The locations of all
motifs can be directly read off from the state conﬁguration of x. Table 2.7 summarizes the results
on 20 test sets each containing 20 sequences harboring motifs abf1, gal4 and mig1 (0–6 total in-
stances/seq). The upper panels show the predictive performance based on the optimal (in terms of
maximal log-likelihood of y from 50 independent runs of the GMF algorithm) posterior expectation
of X. Note that with a MotifPrototyper local model, LOGOShh exhibits better performance. In the
lower panels, we show the best FP-FN results in the top three predictions (i.e., top 3 PWMs for each
of the K motifs we look for) made by LOGOS (note that ‘K-at-a-time’ prediction yields a total
of 3K possibly redundant motif patterns). This is close to the stochastic dictionary scenario where
the predicted motif is to be identiﬁed from the optimal dictionary of the patterns resulting from the
motif detection program [Gupta and Liu, 2003]. It is expected that a human observer could easily
pick out the biologically more plausible motifs when given a visual presentation of the most likely
104

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
motifs suggested by a motif ﬁnder.
Table 2.7: Simultaneous multiple motif detection (median FP-FN rate over 20 test sets containing three motifs.)
LOGOShh
LOGOSph
FP
FN
FP
FN
abf1
0.3591
0.3274
0.7778
0.7434
MAP pre-
diction
gal4
0.1259
0.1714
0.3751
0.1491
mig1
0.3849
0.2243
0.3481
0
abf1
0.3841
0.2400
0.4721
0.3972
best of top 3
prediction
gal4
0.0926
0.0986
0.2609
0.1255
mig1
0.1250
0.0333
0.2318
0
Detecting motifs of uncertain lengths.
A useful property of the MotifPrototyper submodel is
that it actually does not need to know the exact lengths of the motifs to be detected, since the Mo-
tifPrototyper allows a motif to start (and end) with consecutive heterogeneous sites. Thus, a blurred
motif boundary is permissible, especially when the resulting window is large enough to cover at
least the entire length of the motif. As a result, we do not have to know the exact length of the
motif, but just need to roughly guess it conservatively, during de novo motif detection. This is
another appealing feature of LOGOS, which extends its ﬂexibility. As shown in Table 2.8, even
in simultaneous multiple motif detection, with improperly speciﬁed motif lengths, LOGOShh per-
forms nearly as well as when motif lengths are precisely speciﬁed, whereas LOGOSph is not as
good.
Table 2.8: Simultaneous detection of three motifs, with lengths improperly speciﬁed (18, 22, and 20 bp, respectively,
instead of the actual 13, 17, and 11 bp).
LOGOShh
LOGOSph
FP
FN
FP
FN
abf1
0.7295
0.6667
0.8021
0.7680
MAP pre-
diction
gal4
0.1167
0.2042
0.2357
0.1325
mig1
0.4183
0.2128
0.8150
0.8381
abf1
0.3310
0.2804
0.5742
0.4821
best of top 3
prediction
gal4
0.0955
0.1222
0.1882
0.1250
mig1
0.2124
0.1327
0.3218
0.1623
2.6.1.2
Motif detection in yeast promoter regions
In this section we report a performance comparison of LOGOS (HMM+HMDM) with two popular
motif detection programs, MEME and AlignACE, on 12 yeast genomic sequence sets gathered
105

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
from the SCPD database (the selection is based on having at least a total of 5 motif instances in
all sequences and the motif being independent of our training set). Each sequence set consists of
multiple yeast promoter regions each about 500 bp long and containing on both strands an unknown
number of occurrences of a predominant motif (but also possibly other minor motifs) as speciﬁed
by the name of the dataset (Table 2.9, where the rightmost column gives the number of sequences
in each dataset). Note that both the relatively large sizes of the input sequences and the possible
presence of motifs other than what has been annotated make the motif ﬁnding task signiﬁcantly more
difﬁcult than a semi-realistic test data or small, well curated real test data. We use the following
command to run MEME: “ meme $eﬁle -p 2 -dna -mod tcm -revcomp -nmotifs 1. ” In practice, this
means that it searches for a DNA sequence on both strands for at most one motif, which can occur
zero or more times in any given sequence. AlignACE is run with default command-line arguments
nearly identical to those for MEME, with the only difference that AlignACE can return multiple
predicted motifs (of which we select the best match from the top ﬁve MAP predictions). LOGOS
is set in the multiple-detection mode and is used to make two motif predictions simultaneously. As
shown in Table 2.9, for this non-trivial de novo motif detection task, LOGOS outperforms the other
two programs by a signiﬁcant margin.
Table 2.9: Comparison of motif detectors on yeast promoter sequences.
set
LOGOS
MEME
AlignACE
seq
name
FP
FN
FP
FN
FP
FN
no.
abf1
0.7949
0.6522
1.0000
1.0000
0.5294
0.6087
20
csre
0.4444
0.1667
0.7778
0.5000
0.8000
0.5000
4
gal4
0.1333
0.0714
0.1667
0.2857
0.3333
0.1429
6
gcn4
0.3529
0.1852
1.0000
1.0000
0.3333
0.5556
9
gcr1
0.2859
0.6154
1.0000
1.0000
0.4545
0.4615
6
hstf
0.8571
0.5556
0.6000
0.5556
0.8500
0.6667
6
mat
0.4194
0
0.3750
0.5625
0.2500
0.2500
7
mcb
0.4706
0.2500
0.2000
0.3333
0.2500
0.2500
6
mig1
0.8077
0.2857
1.0000
1.0000
0.8333
0.7857
22
pho2
0.9024
0.5000
1.0000
1.0000
1.0000
1.0000
3
swi5
0.7647
0.5000
1.0000
1.0000
0.9412
0.7500
2
uash
0.8250
0.6818
1.0000
1.0000
0.9231
0.9545
18
2.6.1.3
Motif detection in Drosophila regulatory DNAs
In this section, we report on a preliminary de novo motif discovery analysis of the regulatory re-
gions of the 9 Drosophila genes involved in body segmentation. The input data consists of 9 DNA
106

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
sequences ranging from 512 to 5218 bp, as described in [Berman et al., 2002]. Biologically iden-
tiﬁed motifs include bcd, cad, hb, kni and kr. For comparison, we provide the PWMs postulated
by Berman et al. for these ﬁve motifs, which were used in their motif scan analysis (Figure 2.19).
The sources of all PWMs are biologically identiﬁed sequence segments from the literature (which
are unaligned, ranging from 5 to 93 instances per motif, and about 20 ∼40 bases in length). The
PWMs are derived from an alignment of all these identiﬁed motif sequences.
 
0
1
2
bits
5′
1
T
G
A
2
G
3
4
G
A
T
C
5
A
6
G
C
T
7
A
8
A
9
A
10
C
T
G
A
3′
 
0
1
2
bits
5′
1
A
2
A
3
G
A
C
4
G
T
5
CA
6
G
7
G
C
A
8
C
T
G
9
C
10
A
3′
 
0
1
2
bits
5′
1
T
C
G
A
2
G
CA
3
T
CA
4
G
T
C
A
5
TG
6
AG
7
C
A
G
8
GT
9
A
C
G
T
10
C
T
G
A
3′
 
0
1
2
bits
5′
1
A
T
C
2
C
A
T
3
GA
4
A
5
G
T
6
A
TC
7
G
T
C
8
T
C
G
3′
 
0
1
2
bits
5′
1
G
C
A
T
2
T
3
G
CT
4
T
5
T
6
T
7
T
C
G
A
8
G
C
A
T
9
C
T
G
10
11
C
G
T
3′
bcd
cad
hb
kni
kr
Figure 2.19: Berman et al.’s Drosophila motif patterns derived from multi-alignments of biologically identiﬁed motif
instances.
 
0
1
2
bits
5′
1
2
A
G
T
3
C
AT
4
A
G
CT
5
A
C
GT
6
A
C
GT
7
G
C
AT
8
G
T
A
9
C
G
A
T
10
T
G
11
T
G
12
G
C
T
13
G
A
C
T
14
G
C
T
15
A
G
C
T
3′
 
0
1
2
bits
5′
1
A
G
T
2
GT
3
GC
4
GC
5
G
C
T
A
6
T
C
A
G
7
A
CG
8
C
A
T
9
10
C
G
T
A
11
12
GC
13
G
TC
14
G
A
C
15
T
G
A
16
AT
17
G
A
CT
18
C
3′
 
0
1
2
bits
5′
1
C
T
2
G
T
C
3
T
4
TC
5
AG
6
A
T
C
7
G
A
C
T
8
C
G
T
9
A
C
T
10
3′
 
0
1
2
bits
5′
1
T
G
A
2
A
C
T
3
4
G
T
AC
5
C
T
GA
6
G
T
C
A
7
G
C
AT
8
A
C
T
9
G
T
A
10
G
A
T
11
A
C
G
T
12
T
A
C
13
T
A
G
14
15
T
A
C
16
T
GA
17
AT
18
T
G
CA
19
TA
20
C
TA
3′
 
0
1
2
bits
5′
1
AT
2
C
TA
3
G
T
A
4
T
CA
5
TA
6
TA
7
AT
8
C
G
T
9
G
A
T
10
T
G
A
3′
 
0
1
2
bits
5′
1
AT
2
C
T
A
3
A
T
C
4
A
T
G
C
5
T
G
A
C
6
T
G
C
A
7
G
T
A
8
GC
9
C
10
C
11
T
G
A
12
C
T
A
13
C
AT
14
C
15
GC
3′
 
0
1
2
bits
5′
1
C
T
G
2
C
3
C
4
T
A
G
5
CG
6
T
A
G
C
7
A
T
C
G
8
G
C
A
9
A
G
T
10
C
3′
 
0
1
2
bits
5′
1
AT
2
T
A
G
C
3
4
A
GC
5
CG
6
G
7
AC
8
9
C
T
A
10
A
C
G
T
11
T
C
G
12
A
G
C
13
G
C
T
A
14
T
C
G
A
15
G
AT
3′
1
2
3
5
7
4
6
8
Figure 2.20: Motif patterns detected by LOGOS in the regulatory regions of 9 Drosophila genes.
We applied LOGOS (which is set to identify 4 motifs at a time) to the Drosophila dataset;
Figure 2.20 gives a partial list of the top-scoring motif patterns (of the top three runs out of a total
of 50 runs, evaluated by the likelihood under the LOGOS model at convergence). Note that the
logos shown here are not the conventional sequence logos based on counts of aligned nucleotides;
instead we use the logo visualization software to graphically present the Bayesian estimate of the
position-speciﬁc multinomial parameters θ of each motif, so they are not necessarily equal to the
usual nt frequencies of aligned sequences, but represent a more robust probabilistic model of the
107

2.6 LOGOS: for Semi-unsupervised de novo Motif Detection
motif sequences. A visual inspection reveals that patterns 1 and 5 correspond to the hb and cad
binding sites, respectively (as conﬁrmed by the matching locations of our results and the sequence
annotations). Part of pattern 2 agrees with the reverse complement of the kr motif (containing -
CCCxTT-), but this motif seems to be actually a “two-block” motif because the pattern we detected
under a longer estimated motif length contains an additional co-occurring conserved pattern a few
bases upstream. Part of pattern 7 is close to the bcd motif (containing -AATCC-) but also contains
additional sites (i.e., the three highly conserved C’s upstream), which turned out to result from a
number of false positive substrings picked up together with the true bcd motifs. A careful exami-
nation of pattern 6 suggests that it may be actually derived from putative motif subsequences that
correspond to the kni binding site. This is not obvious at ﬁrst because it appears quite different from
the kni logo in Figure 2.19. But after seeing an example kni site in stripe 2/7: 5’agaaaactagatca3’,
starting at position 35, we realized that this answer might be plausible. The discrepancy is likely due
to artifacts in the original generation of the alignment data supporting the kni logo: only 5 biolog-
ically identiﬁed instances were used and they are quite diverse; the resulting multiple alignment is
visually sub-optimal in that homogeneous sites are severely interspersed with heterogeneous sites.
Patterns 3, 4, and 8 are putative motifs not annotated in the input sequences. We also ran the same
dataset through MEME (also 4 patterns to be found a time) and the output is in general weaker and
harder to interpret. Figure 2.21 shows the best three patterns, from which one could recognize a hb
(pattern 1) and a cad (pattern 3). Note that the motif logos given in Figure 2.19 are based on the
nucleotide-frequency proﬁles of biologically identiﬁed instances from many sources. Thus it is not
surprising that some of the patterns we found are similar to but do not match the logos in Figure 2.19
exactly since our logos are derived from Bayesian estimates of the motif parameters and our data
source consists of a small number of regulatory regions of the Drosophila genome, which might be
smaller and less representative compared to the data source underlying Figure 2.19 (except for kni).
 
0
1
2
bits
5′
1
G
C
T
2
AT
3
A
GT
4
G
AT
5
C
G
T
6
G
T
C
7
A
C
T
G
8
A
TC
9
G
T
C
10
CG
11
C
G
A
12
T
A
C
G
13
A
T
C
14
A
C
G
T
15
G
T
C
A
3′
 
0
1
2
bits
5′
1
C
A
TG
2
T
G
C
3
T
G
AC
4
T
GA
5
G
AT
6
C
T
GA
7
G
T
CA
8
C
T
A
9
C
T
G
A
10
G
C
A
3′
 
0
1
2
bits
5′
1
C
G
A
2
T
C
A
G
3
T
GA
4
CA
5
TA
6
T
C
A
7
G
C
T
8
G
C
T
9
G
T
A
10
C
AG
11
T
C
G
A
12
A
C
T
13
T
A
G
C
14
G
T
A
15
A
T
C
G
16
A
C
T
G
17
GT
18
C
G
AT
19
A
G
T
20
G
T
C
3′
1
2
3
Figure 2.21: Motif patterns detected by MEME in the regulatory regions of the Drosophila eve-skipped gene.
108

2.7 Conclusions
2.7
Conclusions
In this chapter, we presented a modular, parametric Bayesian model, LOGOS, to capture various
aspects of the characteristics of DNA motifs in the transcriptional regulatory sequences, including
canonical structures of motif families, syntax of motif organization, and the distribution of back-
ground sequences. Using a graphical model formalism, LOGOS manifests a modular architecture
for the motif model, which consists of a local submodel for the sequence composition of motif sites,
a global submodel for the locational distribution of motif sites in the genomic sequences, and a
background submodel for non-motif sequences — addressing different aspects of motif properties
in a divide-and-conquer fashion.
We developed a MotifPrototyper model for local motif alignment, which captures site depen-
dencies inside motifs and incorporates learnable prior knowledge from known motifs for Bayesian
estimation of the PWMs of novel motifs in unseen sequences. We also developed a CisModuler
HMM model for the global motif distribution, which introduces dependencies among motif in-
stances and allows efﬁcient and consistent inference of motif locations. A deterministic algorithm
based on generalized mean ﬁeld approximation will be described in Chapter 4 to solve the complex
missing value and Bayesian inference problems associated with the LOGOS model. As will be
explained shortly, GMF allows probabilistic inference in the local alignment and the global distribu-
tion submodels to be carried out virtually separately with a proper Bayesian interface connecting the
two processes. This divide and conquer strategy aligned with the modular architecture of LOGOS
makes it much easier to develop more sophisticated models for various aspects of motif analysis
without being overburdened by the daunting complexity of the full motif problem.
Due to the functional diversity of the DNA motifs, it is expected that there could exist more
complex dependencies and regularities in the structures of motifs. Thus, further investigations into
these properties and more powerful local models for motifs are needed. Similarly, the HMM-based
global model we proposed is only a ﬁrst step beyond the conventional UI model, and is only able to
capture dependencies between motifs and motif clusters at a very limited level (e.g., it cannot model
109

2.7 Conclusions
higher-order dependencies such as hierarchical structures and long-distance inﬂuence between mo-
tifs). More expressive models are needed to achieve these goals. Nevertheless, under the LOGOS
architecture, extensions from baseline models are modular and the probabilistic computations in-
volved can also be handled in a divide-and-conquer fashion via generalized mean ﬁeld inference.
We are optimistic that LOGOS can serve as a ﬂexible framework for motif analysis in biopolymer
sequences.
110

Chapter 3
Modeling Single Nucleotide
Polymorphisms for Haplotype Inference
— A Nonparametric Bayesian Approach
In addition to unveiling the genetic code underlying the structure, localization, and regulation of
biopolymer macromolecules such as proteins and RNAs that are essential for biological activities,
and thereby facilitating mechanistic analysis of the function and evolution of various organisms, the
availability of nearly complete genome sequences for organisms such as humans also makes it possi-
ble to begin to explore individual differences between DNA sequences on a genome-wide scale, and
to search for associations of such genotypic variations with diseases and other phenotypes [Risch,
2000].
The largest class of individual differences in DNA are the single nucleotide polymorphisms, or
SNPs. Millions of SNPs have been detected thus far out of an estimated total of ten million common
SNPs [Sachidanandam et al., 2001; Venter et al., 2001]. SNPs are promising markers for population
genetic studies and for localizing genetic variations potentially responsible for complex diseases due
to their high density, low mutation rate, and amenability to automated genotyping [Patil et al., 2001].
However, each individual SNP only yields limited information regarding populational variation and
disease association [Akey et al., 2001]. It is known that studies using haplotype information of
multiple linked SNPs generally outperform those using single-marker analysis [Weiss and Clark,
2002; Clark, 2003]. Thus it is important to know the haplotype structure of the genome in the
111

3.1 Biological Foundations and Motivation
population under study. In this chapter, we present a novel nonparametric Bayesian approach for
haplotype inference from SNP genotype data. Some of the material in this thesis has appeared
before in [Xing et al., 2004c].
3.1
Biological Foundations and Motivation
Recall that a chromosome is a complete strand of DNA in the genome. For diploid organisms such
as humans, each individual has two physical copies of each chromosome in his/her somatic cells.
One copy is inherited from the mother, and the other from the father.
A SNP commonly has two variants, or alleles, at a single chromosomal locus in the popula-
tion, corresponding to two speciﬁc nucleotides chosen from {A, C, G, T}.
1 Essentially, SNPs
are genetic variations in the same chromosomal locus among different individuals in a population,
which are usually neutral nucleotide substitutions that are not necessarily functionally essential and
do not substantially affect the ﬁtness of their bearers [Kruglyak and Nickerson, 2001]. Thus, they
are believed to result from ancient neutral mutations that took place in the ancestors of the modern
population, and may carry important information about tribal or ethnic group formation, evolution
and migration [Stoneking, 2001].
Figure 3.1: SNP haplotypes and possible phenotype associations.
A haplotype is a list of alleles at consecutive sites in a local region of a single chromosome.
1An allele is a variant of a SNP, a gene, or some other entity associated with sites in DNA. In our case (SNPs), the sites
are single nucleotides, and the alleles can generally be assumed to be binary, reﬂecting the fact that lightning (mutation)
doesn’t tend to strike twice in the same place.
112

3.1 Biological Foundations and Motivation
It can be regarded as a state conﬁguration of a particular chromosome (Fig. 3.1). Although in
general the individual SNPs are themselves not related to functionality, the SNPs haplotypes may
co-occur with some disease-related phenotypes due to physical proximity of the haplotype to pos-
sible causal regions on the DNA genome, which could lead to co-inheritance [Akey et al., 2001;
Daly et al., 2001; Pritchard, 2001]. Therefore, haplotypes can be used for inferring the chromo-
somal locations of the genes underlying diseases. Assuming no recombination in a local region
containing multiple SNPs, a haplotype is inherited as a unit. Recall that for diploid organisms
(such as humans) the chromosomes come in pairs.
Thus two haplotypes go together to make
up a genotype, which is the list of unordered pairs of alleles in a region. That is, a genotype
is obtained from a pair of haplotypes by omitting the speciﬁcation of the association of each al-
lele with one of the two chromosomes—its phase. Phase information can be critical to the map-
ping of a disease gene, by allowing a more precise and robust localization of it within a target
area via a linkage analysis which assesses the level and signiﬁcance of statistical associations be-
tween disease phenotypes and genetic markers [Akey et al., 2001; Clark, 2003]. To date, hap-
lotype mapping has been successfully employed for a number of monogenic diseases, such as
cystic ﬁbrosis and Huntington’s [Lazzeroni, 2001]; and has appeared valuable in locating sus-
ceptibility genes in complex multigenic disorders [Puffenberger et al., 1994; Hugot et al., 2001;
Rioux et al., 2001]. In these cases, exploiting haplotype information can greatly reduce the num-
ber of assays necessary to genotype a subject’s genome and thus facilitate comprehensive whole-
genome association studies for mapping complex diseases.
Common biological methods for assaying genotypes typically do not provide phase information
for individuals with heterozygous genotypes at multiple autosomal loci (Fig. 3.2); phase can be
obtained at a considerably higher cost via molecular haplotyping [Patil et al., 2001]. In addition to
being costly, these methods are subject to experimental error and are low-throughput. Alternatively,
phase can also be inferred from the genotypes of a subject’s close relatives [Hodge et al., 1999].
But this approach is often hampered by the fact that typing family members increases the cost and
does not guarantee full informativeness. It is desirable to develop automatic and robust methods for
113

3.2 Problem Formulation and Overview of Related Work
Figure 3.2: Phase ambiguity. For a heterozygous individual, who has different SNP alleles on the pair of chromosomes at
multiple loci, a standard sequencing experiment only yields the joint identity of both alleles of each SNP locus (i.e., the
genotypes), whereas the exact chromosomal association of the alleles (i.e., the haplotype, or phase) of the SNP sequence
is lost. (This is because that it is technically difﬁcult to sequence the paired chromosomes in a cell separately in a
standard sequencing experiment, which simply blends all cell extracts in the same test tube.) It is often the case that for
given genotypes of multiple SNPs, there exist multiple consistent haplotype reconstructions. For example, the genotypes
shown here can be consistently explained by either one of these two possible associations of alleles to chromosomes.
inferring haplotypes from genotypes and possibly other data sources (e.g., pedigrees). As pursued in
this chapter, in silico phasing programs based on explicit statistical models are a feasible approach
to meet these goals.
3.2
Problem Formulation and Overview of Related Work
From the point of view of population genetics, the basic model underlying the haplotype inference
problem is a ﬁnite mixture model. That is, letting H denote the set of all possible haplotypes
associated with a given region (a set of cardinality 2k in the case of binary polymorphisms, where k
is the number of heterozygous SNPs), the probability of a genotype is given by:
p(g) =
X
h1,h2∈H
p(h1, h2)p(g|h1, h2),
(3.1)
where the likelihood model (i.e., 2nd term on the r.h.s.), which deﬁnes the probability of an (ob-
served) genotype pattern given a pair of (latent) haplotype patterns, is referred to as a genotype
model; the mixing proportion (i.e., 1st term on the r.h.s.), which deﬁnes the joint probability of a
114

3.2 Problem Formulation and Overview of Related Work
pair of haplotypes, is referred to as a haplotype model; and the space of all possible haplotypes in
this region in a population is called the population haplotype pool. In the standard setting (e.g., [Ex-
cofﬁer and Slatkin, 1995]), one usually assumes that:
• The genotype model is deterministic (i.e., typing is considered as noiseless),
p(h1, h2)p(g|h1, h2) = I(h1 ⊕h2 = g)
where I(h1 ⊕h2 = g) is the indicator function of the event that haplotypes h1 and h2 are
consistent with g.
• The pair of haplotypes of an individual are subject to Hardy-Weinberg equilibrium (HWE)
(i.e., the pair of haplotypes are independently inherited) [Lange, 2002], an assumption that is
standard in the literature and will also be made here,
p(h1, h2) = p(h1)p(h2)
• The size of the the population haplotype pool is set ﬁxed (to a manageable integer) to avoid
exhaustive enumeration,
H = K ≪2k
Given this basic statistical structure, the haplotype inference problem can be viewed a missing
value inference and parameter estimation problem. Numerous statistical models and statistical in-
ference approaches have been developed for this problem, which will be brieﬂy reviewed shortly.
There is also a plethora of combinatorial algorithms based on various deterministic models of hap-
lotypes. While recognizing their effectiveness in a number of occasions and important insights they
provide to the problem, we choose to forego an extensive discussion of this literature (but see [Gus-
ﬁeld, 2004] for an overview) and focus on statistical methods in this chapter. It is our view that
the statistical approaches provide more ﬂexibility in handling missing values (e.g., occasional miss-
ing genotyping outcomes), typing errors, evolution modeling and more complex scenarios on the
horizon in haplotype modeling (e.g., recombinations, gene linkage, etc.).
115

3.2 Problem Formulation and Overview of Related Work
3.2.1
Baseline Finite Mixture Model and the EM Approach
Given the statistical structure illustrated in Eq. (3.1), the simplest methodology for haplotype in-
ference is maximum likelihood via the EM algorithm, treating the haplotype identities as latent
variables and estimating the parameters p(h), usually referred to as population haplotype frequen-
cies fh, assuming that the individual haplotypes are iid following a multinomial distribution pa-
rameterized by {fh : h ∈H} [Excofﬁer and Slatkin, 1995]. This methodology has rather severe
computational requirements, in that a probability distribution must be maintained on the (large) set
of possible haplotypes, but even more fundamentally it fails to capture the notion that small sets of
haplotypes should be preferred. This notion derives from an underlying assumption that for rela-
tively short regions of the chromosome there is limited diversity due to population bottlenecks and
relatively low rates of recombination and mutation.
The key shortcoming of the aforementioned EM-based ﬁnite mixture model lies in its inability
to take into account uncertainty about the the number of haplotypes (i.e., the number of mixture
components), and to impose appropriate statistical bias. This problem is, up to a terminological
mapping, closely related to clustering problems that are commonly studied in machine learning and
data mining literature. In particular, collaborative ﬁltering involves the clustering of sets of choices
made by sets of individuals, and this clustering problem is closely related to the clustering of sets
of alleles in sets of chromosomes. In these domains, the perennial problem of ”how many clus-
ters?” is well known, and is particularly salient in large data sets where the number of clusters needs
to be relatively large and open-ended. At one time, an EM algorithm can only handle a pre-ﬁxed
integer number of mixture components (e.g., 2k or a smaller number K of possible haplotypes).
In haplotype phasing, such an approach does not return any estimate of uncertainty about the spe-
ciﬁc number of haplotypes that it ﬁnds, and heuristics such as cross-validation needs to be used to
empirically pick a favorable K.
116

3.2 Problem Formulation and Overview of Related Work
3.2.2
Bayesian Methods via MCMC
One approach to dealing with the issue of the unknown number of mixture components, and the
desirable bias for more compact phase reconstruction, is to formulate a notion of “parsimony,” and
to develop algorithms that directly attempt to maximize parsimony. Several important papers have
taken this approach [Clark, 1990; Clark et al., 1998; Gusﬁeld, 2002; Eskin et al., 2003] and have
yielded new insights and algorithms. Another approach is to elaborate the probabilistic model,
in particular by incorporating priors on the parameters. Different priors have been discussed by
different authors as outlined in the following. These models provide implicit notions of parsimony,
via the implicit “Ockham factor” of the Bayesian formalism [Bernardo and Smith, 1994].
3.2.2.1
Simple Dirichlet priors
The PL model proposed by Niu et al. [Niu et al., 2002], which was implemented in the software
HAPLOTYPER, incorporates simple Dirichlet priors to the haplotype frequencies, {fh}, to be esti-
mated (no prior for the haplotypes themselves are introduced):
p({fh}) = Γ(P
h βh)
Q
h Γ(βh)
Y
h

fh
βh−1
(3.2)
As indicated by Stephens et al. [2003], the Dirichlet priors correspond to a simple, but highly
unrealistic assumption about the genetic processes underlying the evolution of the study population
— that the genetic sequence of a mutant offspring does not depend on the progenitor sequence.
As is standard in Bayesian inference, an MCMC algorithm, speciﬁcally, a novel Gibbs sam-
pling scheme, was used to compute the Monte Carlo estimates, i.e., the haplotype frequencies and
the individual haplotypes. In particular, two computational tricks — prior annealing and partition-
ligation (from which comes the name of the model) — appeared to signiﬁcantly reduce the com-
putational effort required to obtain a good approximation to the true posterior distribution of the
aforementioned estimators of interest.
117

3.2 Problem Formulation and Overview of Related Work
3.2.2.2
The coalescent prior
The model introduced by Stephens et al. [2001] (referred to as the SSD model after its authors) is
based on a more elaborate Bayesian framework, which assumes that the unobserved haplotypes are
subject to a prior that considers how randomly sampled individuals are related genealogically via a
neutral coalescent [Stephens and Donnelly, 2000]. For computational feasibility (i.e., not having to
marginalize over a space of all valid genealogical trees), they devised a Gibbs sampler that samples
individual haplotypes from a conditional distribution that approximates the coalescent:
π(h = β|H) =
X
α∈H
∞
X
s=0
rα
r

θ
r + θ
s
θ
r + θ(T s)αβ,
(3.3)
where rα is the number of haplotypes of type α in the set H (the set of all haplotypes in the study
population excluding the next sampled haplotype, h), r is the cardinality of H, θ is a scaled mutation
rate, and T is the substitution probability matrix between all pairs of haplotypes.
The coalescent prior is arguably more realistic than the “parent-independent” mutation model
underlying the simple Dirichlet prior in the PL model, and favors mutant offspring that differ only
slightly from the progenitor sequences, hence implicitly introducing a parsimonious bias. A recent
paper by Lin et al. [2002] also described a number of modiﬁcations to the SSD model, which appears
to slightly compromise the approximation to the coalescent prior but on the other hand improves the
efﬁciency of the sampling algorithm in the original implementation of SSD. The latest version of
the software PHASE, where the coalescent prior is used, also assimilates the computational tricks
(i.e. prior annealing and ligation) contributed by Niu et al. [2002], and represents the state of the art
haplotype inference program, signiﬁcantly and constantly beating other extant methods on real and
simulated data.
3.2.3
Bayesian Network Prior
Note that the coalescent model does not readily generalize to more complex scenarios such as pos-
sible recombinations within a stretch of SNPs. A recombination could reduce the linkage disequi-
librium, or in other words, decouple the subsets of SNPs on the two sides of the recombination spot
118

3.2 Problem Formulation and Overview of Related Work
on the chromosome. Exploiting this phenomenon, or ﬁnding SNP blocks with high internal linkage
(even though they do not necessarily arise from the presence of true recombinations hotspots that
deﬁne their boundaries), could help to optimally decompose the difﬁcult problem of phasing long
stretches of SNPs into multiple subproblems of manageable sizes, i.e., phasing each block of SNPs
separately, and then trying to stitch together the sub-solutions. Greenspan et al. [2003] attempted to
model the events of recombination on a chromosome as a 1st-order hidden Markov process, deﬁning
a segmentation of the whole sequence of SNPs. Associated with each block of consecutive SNPs
resulting from the segmentation is a block-speciﬁc distribution of ancestral haplotypes. For each
chromosome, the choice of ancestral haplotype at each block is determined by the latent recombina-
tion variables associated with each block. The conﬁguration of the recombination variable at each
block is 1st-order Markovian with respect to the recombination variables of the previous blocks.
Within each block, each individual haplotype is a possibly corrupted (via mutations) version of the
ancestral haplotype under a stochastic mutation model. This model readily handles missing values
and mis-typings in SNP data acquisition, and elegantly facilitates a divide-and-conquer strategy for
large phasing problem. Note that the number of ancestral haplotypes at each block and the bound-
aries of the blocks are unknown model parameters. A minimum description length (MDL) criterion
is used for model selection in conjunction with an EM algorithm.
Identifying and interpreting haplotype blocks is a standing-along problem that has received
much attention in recent years due to its relevance to understanding the linkage disequilibrium
structures of chromosomes and the evolution history of the genome. In addition to the work of
Greenspan et al. [2003], numerous methods outside the context of haplotype phasing (i.e., focusing
on empirically phased data), such as dynamic programming [Zhang et al., 2002], HMM [Daly et
al., 2001], and MDL [Anderson and Novembre, 2003], have been reported. We view these as a
complementary issue to the problem we are interested in here, and forego an extensive review.
119

3.2 Problem Formulation and Overview of Related Work
3.2.4
Summary and Prelude to Our Approach
Extant approaches for phasing rely on the plausible assumption that, locally, haplotype data has lim-
ited diversity. This constraint is modeled in different ways by the different methods, often leading to
“guessing” in advance a parameter that represents the size of the genetic pool in the population. No
current approach suggests an explicit probabilistic model for this quantity, but rather an empirical
estimate is used. Such an approach fails to take into account uncertainty in this important quantity.
Moreover, to ensure success, this quantity has to be set large so that no or few individual haplo-
type conﬁgurations will be missed. This heuristic causes a computational burden and may bias the
algorithm toward non-parsimonious solutions with a large number of rare haplotypes.
In the following we also take a Bayesian statistical approach, but we attempt to provide more ex-
plicit control over the number of inferred haplotypes than has been provided by the statistical meth-
ods proposed thus far. The resulting inference algorithm has commonalities with the parsimony-
based schemes.
The approach to be presented is based on a nonparametric prior known as the Dirichlet pro-
cess [Ferguson, 1973]. In the setting of ﬁnite mixture models, the Dirichlet process — not to be
confused with the Dirichlet distribution — is able to capture uncertainty about the number of mix-
ture components [Escobar and West, 2002]. The basic setup can be explained in terms of an urn
model, and a process that proceeds through data sequentially. Consider an urn which at the out-
set contains a ball of a single color. At each step (i.e., for each data point) we either draw a ball
from the urn and replace it with two balls of the same color, or we are given a ball of a new color
which we place in the urn, with a parameter deﬁning the probabilities of these two possibilities. The
association of data points to colors deﬁnes a “clustering” of the data.
To make the link with Bayesian mixture models, we associate with each color a draw from
the distribution deﬁning the parameters of the mixture components. This process deﬁnes a prior
distribution for a mixture model with a random number of components. Multiplying this prior by
a likelihood yields a posterior distribution. Markov chain Monte Carlo algorithms have been de-
veloped to sample from the posterior distributions associated with Dirichlet process priors [Escobar
120

3.3 Haplotype Inference via the Dirichlet Process
and West, 2002; Neal, 2000].
The usefulness of this framework for the haplotype problem should be clear—using a Dirich-
let process prior we in essence maintain a pool of haplotype candidates that grows as observed
genotypes are processed. The growth is controlled via a parameter in the prior distribution that cor-
responds to the choice of a new color in the urn model, and via the likelihood, which assesses the
match of the new genotype to the available haplotypes.
To expand on this latter point, an advantage of this probabilistic formalism is its ability to
elaborate the observation model for the genotypes to include the possibility of errors. In particular,
the indicator function I(h1 ⊕h2 = g) in Eq. (3.1) is suspect—there are many reasons why an
individual genotype may not match with a current pool of haplotypes, such as the possibility of
mutation or recombination in the meiosis for that individual, and/or errors in the genotyping or data
recording process. Such sources of small differences should not lead to the inference procedure
spawning new haplotypes.
In the following we present a statistical model for haplotype inference based on a Dirichlet
process prior and a likelihood that includes error models for genotypes. A Markov chain Monte
Carlo procedure, in particular a procedure that makes use of both Gibbs and Metropolis-Hasting
updates, for posterior inference, will be described in Chapter 5.
3.3
Haplotype Inference via the Dirichlet Process
The input to a phasing algorithm can be represented as a genotype matrix G with columns cor-
responding to SNPs in their order along the chromosome and rows corresponding to genotyped
individuals. Gi,j represents the information on the two alleles of the i-th individual for SNP j. we
denote the two alleles of a SNP by 0 and 1, and Gi,j can take on one of four values: 0 or 1, indicating
a homozygous site; 2, indicating a heterozygous site; and ’?’, indicating missing data.2
We will describe the model in terms of a pool of ancestral haplotypes, or templates, from which
2Although we focus on binary data here, it is worth noting that our methods generalize immediately to non-binary
data.
121

3.3 Haplotype Inference via the Dirichlet Process
each individual haplotype originates [Greenspan and Geiger, 2003]. The haplotype itself may un-
dergo point mutation with respect to its template. The size of the pool and its composition are both
unknown, and are treated as random variables under a Dirichlet process prior. We begin by pro-
viding a brief description of the Dirichlet process and subsequently show how this process can be
incorporated into a model for haplotype inference.
3.3.1
Dirichlet Process Mixture
Rather than present the Dirichlet process in full generality, we focus on the speciﬁc setting of mix-
ture models, and make use of an urn model to present the essential features of the process. For a
fuller presentation, see, e.g., Ishwaran and James [2001]. Assume that data x arise from a mixture
distribution with mixture components p(x|φ). Also assume the existence of a base measure G(φ),
which is one of the two parameters of the Dirichlet process. (The other is the parameter τ, which
we present below). The parameter G(φ) is not the prior for φ, but is used to generate a prior for φ,
in the manner that we now discuss.
Consider the following process for generating samples {x1, x2, . . . , xn} from a mixture model
consisting of an unspeciﬁed number of mixture components, or equivalence classes:
• The ﬁrst sample x1 is sampled from a distribution p(x|φ1), where the parameter φ1 is sampled
from the base measure G(φ).
• The ith sample, xi, is sampled from the distribution p(x|φci), where:
– The equivalence class of sample i, ci, is drawn from the following distribution:
p(ci = cj for some j < i|c1, . . . , ci−1) =
ncj
i −1 + τ
(3.4)
p(ci ̸= cj for all j < i|c1, . . . , ci−1) =
τ
i −1 + τ ,
(3.5)
where nci is the occupancy number of class ci—the number of previous samples be-
longing to class ci.
122

3.3 Haplotype Inference via the Dirichlet Process
– The parameter φci associated with the mixture component ci is obtained as follows:
φci
=
φcj
if ci = cj for some j < i (i.e., ci is a populated equivalence class)
φci
∼
G(φ)
if ci ̸= cj for all j < i (i.e., ci is a new equivalence class)
Eqs. (3.4) and (3.5) deﬁne a conditional prior for the equivalence class indicator ci of each sam-
ple during a sequential sampling process. They imply a self-reinforcing property for the choice of
equivalence class for each new sample—previously populated classes are more likely to be chosen.
It is important to emphasize that the process that we have discussed will be used as a prior
distribution. We now embed this prior in a full model that includes a likelihood for the observed
data. In Section 5.3 we develop Markov chain Monte Carlo inference procedures for this model.
3.3.2
DP-Haplotyper: a Dirichlet Process Mixture Model for Haplotypes
Hi1
Gi
Hi0
Ci1
Ci0
γ
τ
A k,2
A k,J
A k,1
...
Hi  ,2
Hi  ,J
0
0
Hi  ,1
0
...
Hi  ,2
Hi  ,J
1
1
Hi  ,1
1
Gi,2
Gi,J
I
...
...
Gi,1
θ
k
Ak
G
0
K
Figure 3.3: The graphical model representation of the haplotype model with a Dirichlet process prior. Circles represent
the state variables, ovals represent the parameter variables, and diamonds represent ﬁxed parameters. The dashed boxes
denote sets of variables corresponding to the same ancestral template, haplotype, and genotype, respectively. The solid
boxes correspond to i.i.d. replicates of sets of variables, each associated with a particular individual, or ancestral template,
respectively.
Now we present a probabilistic model, DP-Haplotyper, for the generation of haplotypes in
a population and for the generation of genotypes from these haplotypes. We assume that each
123

3.3 Haplotype Inference via the Dirichlet Process
individual’s genotype is formed by drawing two random templates from an ancestral pool, and
that these templates are subject to random perturbation. To model such perturbations we assume
that each locus is mutated independently from its ancestral state with the same error rate. Finally,
assume that we are given noisy observations of the resulting genotypes. The model is displayed as
a graphical model in Figure 3.3.
Let J be an ordered list of loci of interest. For each individual i, denote his/her paternal haplo-
type by Hi0 := [Hi0,1, . . . , Hi0,J] and maternal haplotype by Hi1 := [Hi1,1, . . . , Hi1,J]. We denote
a set of ancestral templates by A = {A1, A2, . . .}, where Ak := [Ak,1, . . . , Ak,J] is a particular
member of this set. The set A is a random variable whose cardinality and composition are not ﬁxed,
but rather vary with realizations of the Dirichlet process and vary with the observed data.
In our framework, the probability distribution of the haplotype variable Hit, where the sub-
subscript t ∈{0, 1} indexes paternal or maternal origin, is modeled by a mixture model with an
unspeciﬁed number of mixture components, each corresponding to an equivalence class associated
with a particular ancestor. For each individual i, we deﬁne the equivalence class variables Ci0
and Ci1 for the paternal and maternal haplotypes, respectively, to specify the ancestral origin of
the corresponding haplotype. The Cit are the random variables corresponding to the equivalence
classes of the Dirichlet process. The base measure G of the Dirichlet process is a joint measure on
ancestral haplotypes A and mutation parameters θ, where the latter captures the probability that an
allele at a locus is identical to the ancestor at this locus. We let G(A, θ) = p(A)p(θ), and we assume
that p(A) is a uniform distribution over all possible haplotypes. We let p(θ) be a beta distribution,
Beta(αh, βh), and we choose a small value for βh/(αh + βh), corresponding to a prior expectation
of a low mutation rate.
Given Cit and a set of ancestral templates, we deﬁne the conditional probability of the corre-
sponding haplotype instance h := [h1, . . . , hJ] to be:
p(Hit = h|Cit = k, A = a, θ)
=
p(Hit = h|Ak = a, θk = θ)
=
Y
j
p(hj|aj, θ),
(3.6)
124

3.3 Haplotype Inference via the Dirichlet Process
where p(hj|aj, θ) is the probability of having allele hj at locus j given its ancestor. Eq. (3.6)
assumes that each locus is mutated independently with the same error rate. For haplotypes, Hit,j
takes values from a set B of alleles. We use the following single-locus mutation model:
p(hj|aj, θ) = θI(hj=aj) 1 −θ
|B| −1
I(hj̸=aj)
(3.7)
where I(·) is the indicator function.
The joint conditional distribution of haplotype instances h = {hit : t ∈{0, 1}, i ∈{1, 2, . . . , I}}
and parameter instances θ = {θ1, . . . , θK}, given the ancestor indicator c of haplotype instances
and the set of ancestors a = {a1, . . . , aK}, can be written explicitly as:
p(h, θ|c, a) ∝
Y
k
θmk+αh−1
k
 1 −θk
|B| −1
m′
k
1 −θk
βh−1
(3.8)
where mk = P
j
P
i
P
t I(hit,j = ak,j)I(cit = k) is the number of alleles that were not mutated
with respect to the ancestral allele, and m′
k = P
j
P
i
P
t I(hit,j ̸= ak,j)I(cit = k) is the number
of mutated alleles. The count mk = {mk, m′
k} is a sufﬁcient statistic for the parameter θk and
the count m = {mk, m′
k} is a sufﬁcient statistic for the parameter θ. The marginal conditional
distribution of haplotype instances can be obtained by integrating out θ in Eq. (3.8):
p(h|c, a) =
Y
k
R(αh, βh)Γ(αh + mk)Γ(βh + m′
k)
Γ(αh + βh + mk + m′
k)

1
|B| −1
m′
k
(3.9)
where Γ(·) is the gamma function, and R(αh, βh) =
Γ(αh+βh)
Γ(αh)Γ(βh) is the normalization constant asso-
ciated with Beta(αh, βh). (For simplicity, we use the abbreviation Rh for R(αh, βh) in the sequel).
We now introduce a noisy observation model for the genotypes. We let Gi = [Gi,1, . . . , Gi,J]
denote the joint genotype of individual i at loci [1, . . . , J], where each Gi,j denotes the genotype at
locus j. We assume that the observed genotype at a locus is determined by the paternal and maternal
alleles of this locus as follows:
p(gi,j|hi0,j, hi1,j, γ)
=
γI(hi,j=gi,j)[µ1(1 −γ)]I(hi,j
1
̸=gi,j)[µ2(1 −γ)]I(hi,j
2
̸=gi,j)
where hi,j ≜hi0,j ⊕hi1,j denotes the unordered pair of two actual SNP allele instances at locus
j; “
1
̸=” denotes set difference by exactly one element (i.e., the observed genotype is heterozygous,
125

3.3 Haplotype Inference via the Dirichlet Process
while the true one is homozygous); “
2
̸=” denotes set difference of both elements (i.e., the observed
and true genotypes are different and both are homozygous); and µ1 and µ2 are appropriately de-
ﬁned normalizing constants
3. We place a beta prior Beta(αg, βg) on γ. Assuming independent
and identical error models for each locus, the joint conditional probability of the entire genotype
observation g = {gi : i ∈{1, 2, . . . , I}} and parameter γ, given all haplotype instances is:
p(g, γ|h)
=
Y
i
p(gi, γ|hi0, hi1)
=

γ
u+αg−1
µ1(1 −γ)
u′
µ2(1 −γ)
u′′
1 −γ
βg−1
=
γαg+u−1
1 −γ
βg+u′+u′′−1µu′
1 µu′′
2 ,
(3.10)
where the sufﬁcient statistics u = {u, u′, u′′} are computed as u = P
i,j I(hi,j = gi,j), u′ =
P
i,j I(hi,j
1
̸= gi,j), and u′′ = P
i,j I(hi,j
2
̸= gi,j), respectively. Note that u + u′ + u′′ = IJ. To
reﬂect an assumption that the observation error rate is low we set βg/(αg + βg) to a small constant
(0.001). Again, the marginal conditional distribution of g is computed by integrating out γ.
Having described the Bayesian haplotype model, the problem of phasing individual haplotypes
and estimating the size and conﬁguration of the latent ancestral pool can be solved via posterior
inference given the genotype data. In Chapter 5, we describe Markov chain Monte Carlo (MCMC)
algorithms for this purpose.
3.3.3
Haplotype Modeling Given Partial Pedigree
For diploid organisms such as humans, a subject has two physical copies of each chromosome in
his/her somatic cells, which carry the two haplotypes of the SNP sequence in a speciﬁc region.
When an offspring is to be produced, each of the parents donates a haploid gamete (i.e., a sperm for
the male and an egg for the female), which carries only one of the two copies of every chromosome
3 For simplicity, we may let µ1 = µ2 = 1/V , where V is the total number of ways a single SNP haplotype hi,j and
a single SNP genotype gi,j can differ (i.e., 2 for binary SNPs). When different µ1 and µ2 are desired to penalize single-
and double-disagreement differently, one must be careful to treat the case of homozygous hi,j and heterozygous hi,j
differently, because they are related to noisy genotype observations in different manners. For example, a heterozygous
hi,j (e.g., 01) cannot be related to any genotype with a double disagreement, whereas a homozygous hi,j (e.g., 00) can
(e.g., w.r.t. gi,j = 11).
126

3.3 Haplotype Inference via the Dirichlet Process
of a parent (i.e., one of the two haplotypes). The two gametes of opposite sex then fuse (after mating)
to produce a diploid fertilized egg and re-pair the paternal and maternal copies of the chromosome
(and therefore, their respective associated haplotypes). The fertilized egg eventually grow into an
adult offspring which can be typed.
When the parent-offspring triplet (or even other close biological relatives) are (geno)typed, the
ambiguity of haplotypes of an individual can sometimes be resolved by exploiting the dependen-
cies among the haplotypes of family members induced by genetic inheritance and segregation just
described. For example, if both parents are homozygous, i.e., g1 = a ⊕a, g0 = b ⊕b, and the
offspring are heterogeneous, i.e., gλ10 = a ⊕b, where λ10 denotes the offspring of subjects “1” and
“0”, then we can infer that the haplotypes of the offspring are hλ10 = (a, b). This special case of
triplet genotypes is regarded as fully informative. Clearly, not all genotypes are fully informative,
and inheritance of haplotypes may be more than mere faithful copying. In particular, chromosomal
inheritance could be accompanied by single-generation mutations, which alter single or multiple
SNPs on the chromosomes; and recombinations, which disrupt and recombine some chromosome
pairs in gamete donors to generate novel (i.e., mosaic) haplotypes. Although genotypes of this na-
ture do not directly lead to full resolution of each individual’s haplotypes, undoubtedly the strong
dependencies that exist among the genotype data (in contrast to the iid genotypes we studied in the
last section) could be exploited to reduce the ambiguity of the phasing.
Given the genotypes from a population and partial pedigrees that relate members of various
subsets of a population, in order to apply the pedigree constraints in haplotype inference, we need
to introduce a few new ingredients into the basic DP-haplotyper model described in the last section
to model the distribution of individual haplotypes in a population consisting of now partially coupled
(rather than conditionally independent) individuals (Fig. 3.4). We refer to this expanded model as
the Pedi-haplotyper model.
Formally, we introduce a segregation random variable, Sit,j, for each one of the two SNP alleles
of each locus of an individual, to indicate its meiotic origin (i.e., from which one of the two SNP
alleles of a parent it is inherited). For example, Sit,j = 1 indicates that allele Hit,j is inherited from
127

3.3 Haplotype Inference via the Dirichlet Process
H1,i1
H2,i1
HJ,i1
G1,i
G2,i
GJ,i
Hi1
Gi
Hi0
H1,i0
H2,i0
HJ,i0
...
S 1,i0
S 2,i0
S J,i0
S 1,i0
S 2,i0
S J,i0
...
H1,i’1
H2,i’1
HJ,i’1
G1,i’
G2,i’
GJ,i’
Hi’1
Gi’
Hi’0
Ci’1
H1,i’0
H2,i’0
HJ,i’0
H1,i"
1
H2,i"
1
HJ,i"1
G1,i"
G2,i"
GJ,i"
Hi"
1
Gi"
Hi"
0
Ci"
1
Ci"
0
H1,i"0
H2,i"0
HJ,i"0
A 2,i
(k)
A J,i
(k)
A i
(k)
A 1,i
(k)
∞
...
θ
(k)
γ
τ
Ci’0
ξ
...
...
...
...
...
...
...
...
...
Figure 3.4: The graphical model representation of the Pedi-haplotyper model.
128

3.3 Haplotype Inference via the Dirichlet Process
the maternal allele of individual i’s t-parent (where t = 0 means father and t = 1 means mother).
We denote the t-parent of individual i by π(it), and his/her paternal (resp. maternal) allele by π0(it)
(resp. π1(it)). We use the following conditional distribution to model possible mutation during
single generation inheritance.
p(hit,j|sit,j = r, hπ0(it),j, hπ1(it),j, ϵt)
=

ϵt
I(hit,j=hπr(it),j) 1 −ϵt
|B| −1
I(hit,j̸=hπr(it),j),(3.11)
where 1 −ϵt is the mutation rate during inheritance, and r ∈{0, 1} represents the choice of the
paternal or maternal alleles of a parent subject by an offspring. Note that this single generation
inheritance model allows different mutational rates for the parental and maternal alleles if desired
(e.g., to reﬂect the difference in gamete environment in a male or a female body), by letting ϵ0 and
ϵ1 take different values, or giving them different beta prior distributions in case we want to model
uncertainty of ϵt in a Bayesian framework.
To model possible recombination events during single generation inheritance, we assume that
the list of segregation random variables, [Sit,1, . . . , Sit,J], associated with individual haplotype Hit,
forms a 1st-order Markov chain, with transition matrix ξ:
p(Sit,j+1 = r′|Sit,j = r)
=
ξrr′
=

ξ
I(r=r′)
1 −ξ
I(r̸=r′),
(3.12)
where 1−ξ is the probability of a recombination event (i.e., a swap of parental origin) at position j.
This model is equivalent to assuming that the recombination events follow a Poisson point process
of rate ξ along the chromosome. If desired, a beta prior Beta(αs, βs) can be introduced for ξ.
Again, the recombination rates in males and females can be different if desired.
Looking back to the overall graphical topology of the Pedi-haplotyper model, as illustrated
in Figure 3.4, for founding members in the pedigree (i.e., those without parental information), or
half founding members (i.e., those with information from only one of the two parents), we assume
that their un-progenitored haplotype(s) are inherited from some ancestors, thus following the basic
haplotype model described in §3.3. For the haplotypes of the offspring in the pedigree, we couple
129

3.4 Experimental Results
them to their parents using the single generation mutation and recombination model described in the
previous paragraphs. Thus, the Pedi-haplotyper model proposed in this section is fully generalizable
to any pedigree structure.
Solving the Pedi-haplotyper model is slightly more difﬁcult than for the basic Dirichlet process
mixture model, DP-haplotyper, for iid populations. But as we show in Chapter 5, most of the
methods we developed for the DP-haplotyper can be directly used in this more elaborate framework,
with the addition of a few new sampling steps for the newly introduced random variables.
3.4
Experimental Results
We validated our algorithm by applying it to simulated and real data and compared its performance
to that of the state-of-the-art PHASE algorithm [Stephens et al., 2001] and other current algorithms.
We report on the results of both variants of our algorithm: the Gibbs sampler, denoted DP(Gibbs),
and the Metropolis-Hasting sampler, denoted DP(MH). Throughout the experiments, we set the
hyperparameter τ in the Dirichlet process to be roughly 1% of the population size, i.e., for a data
set of 100 individuals, τ = 1. We used a burn-in of 2000 iterations (or 4000 for datasets with more
than 50 individuals), and used the next 6000 iterations for estimation.
3.4.1
Simulated Data
In our ﬁrst set of experiments we applied our method to simulated data (“short sequence data”)
from Stephens et al. [2001]. This data contains sets of 2n haplotypes, randomly paired to form
n genotypes, under an inﬁnite-sites model with parameters η = 4 and R = 4 determining the
mutation and recombination rates, respectively. We used the ﬁrst 40 datasets for each combination
of individuals and sites, where the number of individuals ranged between 10 and 50, and the number
of sites ranged between 5 and 30.
To evaluate the performance of the algorithms we used the following error measures: errs, the
ratio of incorrectly phased SNP sites over all non-trivial heterozygous SNPs (excluding individu-
als with a single heterozygous SNP); erri, the the ratio of incorrectly phased individuals over all
130

3.4 Experimental Results
DP(MH)
PHASE
EM
#individuals
errs
erri
ds
errs
erri
ds
erri
10
0.060
0.216
0.051
0.046
0.182
0.054
0.424
20
0.039
0.152
0.039
0.029
0.136
0.046
0.296
30
0.036
0.121
0.038
0.024
0.101
0.027
0.231
40
0.030
0.094
0.029
0.019
0.071
0.026
0.195
50
0.028
0.082
0.024
0.019
0.072
0.025
0.167
Table 3.1: Performance on data from Stephens et al. [2001]. The results for the EM algorithm are adapted from Stephens
et al. [2001].
non-trivial heterogeneous individuals; and ds, the switch distance, which is the number of phase
ﬂips required to correct the predicted haplotypes over the total number of non-trivial heterogeneous
SNPs. The results are summarized in Table 3.1. Overall, we perform slightly worse than PHASE
on the ﬁrst two measures, and slightly better on the switch distance measure (which uses 100,000
sampling steps). Both algorithms provide a substantial improvement over EM.
DP(Gibbs)
DP(MH)
PHASE
HAP
HAPLOTYPER
block
id.
length
errs
erri
ds
errs
erri
ds
errs
erri
ds
errs
errs
1
14
0.223
0.485
0.229
0
0
0
0.003
0.030
0.003
0.007
0.039
2
5
0
0
0
0.007
0.026
0.007
0.007
0.026
0.007
0.036
0.065
3
5
0
0
0
0
0
0
0
0
0
0
0.008
4
11
0.143
0.262
0.128
0
0
0
0
0
0
0.015
-
5
9
0.020
0.066
0.020
0.011
0.033
0.011
0.011
0.033
0.011
0.027
0.151
6
27
0.071
0.191
0.074
0.005
0.043
0.005
0
0
0
0.018
0.041
7
7
0.005
0.018
0.005
0.005
0.018
0.005
0.005
0.018
0.005
0.068
0.214
8
4
0
0
0
0
0
0
0
0
0
0
0.252
9
5
0.029
0.097
0.029
0.012
0.032
0.012
0.012
0.032
0.012
0.057
0.152
10
4
0.007
0.025
0.007
0.007
0.025
0.007
0.008
0.025
0.008
0.042
0.056
11
7
0.010
0.034
0.005
0.005
0.017
0.005
0.011
0.034
0.011
0.033
0.093
12
5
0.010
0.037
0.020
0
0
0
0
0
0
0
0.077
Table 3.2: Performance on the data of Daly et al. [2001], using the block structure provided by Halperin and Eskin [2002].
The results of HAP and HAPLOTYPER are adapted from Halperin and Eskin [2002]. Since the error rate in Halperin
and Eskin [2002] uses the number of both heterozygous and missing sites as the denominator, whereas we used only the
non-trivial heterozygous ones, we rescaled the error rates of the two latter methods to be comparable to ours.
3.4.2
Real Data
We applied our algorithm to two real datasets and compared its performance to that of PHASE
[Stephens et al., 2001] and other algorithms.
The ﬁrst dataset contains the genotypes of 129 individuals over 103 polymorphic sites [Daly et
al., 2001]. In addition it contains the genotypes of the parents of each individual, which allows the
131

3.4 Experimental Results
1
2
3
4
5
6
7
8
9
10
0
50
100
150
   112.3680
   42.8975
   24.0215
   15.8180
   11.4060
    8.5905
    6.6175
    5.2755
    4.2945
    3.5750
1
2
3
4
5
6
7
8
9
10
0
50
100
150
  155.9960
   61.1640
   33.7340
    4.9460
    2.0220
    0.1100
    0.0220
    0.0060
         0
           0
1
2
3
4
5
6
7
8
9
10
0
50
100
150
   154.8800
   62.8620
   33.2480
    4.9700
    2.0000
    0.0400
         0
         0
         0
         0
Figure 3.5: The top ten ancestral templates during Metropolis-Hasting sampling for block 1 of the data of [Daly et al.,
2001]. (The numbers in the panels are the posterior means of the frequencies of each template). (a) Immediately after
burn-in (ﬁrst 2000 samples). (b) 3000 samples after burn-in. (c) 6000 samples after burn-in.
inference of a large portion of the haplotypes as in Eskin et al. [2003]. The results are summarized in
Table 3.2. It is apparent that the Metropolis-Hasting sampling algorithm signiﬁcantly outperforms
the Gibbs sampler, and is to be preferred given the relatively limited number of sampling steps (∼
6000). The overall performance is comparable to that of PHASE and better than both HAP [Halperin
and Eskin, 2002; Eskin et al., 2003] and HAPLOTYPER [Niu et al., 2002].
It is important to emphasize that our methods also provide a posteriori estimates of the ancestral
pool of haplotype templates and their frequencies. We omit a listing of these haplotypes, but provide
an illustrative summary of the evolution of these estimates during sampling (Figure 3.5).
The second dataset contains genotype data from four populations, 90 individuals each, across
several genomic regions [Gabriel et al., 2002]. We focused on the Yoruban population (D), which
contains 30 trios of genotypes (allowing us to infer most of the true haplotypes) and analyzed the
genotypes of 28 individuals over four medium-sized regions (see below). The results are summa-
rized in Table 3.3. All methods yield higher error rates on these data, compared to the analysis of
the data of Daly et al. [2001], presumably due to the low sample size. In this setting, over all but
one of the four regions, our algorithm outperformed PHASE for all three types of error measures. A
preliminary analysis suggests that our performance gain may be due to the bias toward parsimony
induced by the Dirichlet process prior. We found that the number of template haplotypes inferred in
our algorithm is typically small, whereas in PHASE, the hypothesized haplotype pool can be very
large (i.e., region 7b has 83 haplotypes, compared to 10 templates in our case and 28 individuals
overall).
132

3.5 Conclusions and Discussions
DP(MH)
PHASE
region
length
errs
erri
ds
errs
erri
ds
16a
13
0.185
0.480
0.141
0.174
0.440
0.130
1b
16
0.100
0.250
0.160
0.200
0.450
0.180
25a
14
0.135
0.353
0.115
0.212
0.588
0.212
7b
13
0.105
0.278
0.066
0.145
0.444
0.092
Table 3.3: Performance on the data of Gabriel et al. [2002].
0
1000
2000
3000
4000
5000
6000
7000
8000
0
10
20
30
40
50
60
# of samples
# of ancestral templates
Figure 3.6: Sampling trace of the number of population haplotypes derived from the genotypes. As can be seen, the
Markov chain starts from a rather non-parsimonious estimation, and converges to a parsimonious solution after about two
thousand samples.
In terms of computational efﬁciency, we noticed that PHASE typically required 20,000 to
100,000 steps until convergence, while our DP-based method required around 2,000∼6,000 steps
to convergence (Fig. 3.6).
3.5
Conclusions and Discussions
In this chapter, we have proposed a Bayesian approach to the modeling of genotypes based on a
Dirichlet process prior. We have shown that the Dirichlet process provides a natural representation
of uncertainty regarding the size and composition of the pool of haplotypes underlying a population.
We will present in Chapter 5 several Markov chain Monte Carlo algorithms for haplotype inference
under either a basic DP mixture haplotype model intended for an iid population, or, an extended
graphical DP mixture model — Pedi-haplotyper model — for a population containing both iid sub-
jects and subjects coupled by partial pedigrees. The experiments on the basic DP mixture haplotype
model show that this model leads to effective inference procedures for inferring the ancestral pool
133

3.5 Conclusions and Discussions
and for haplotype phasing based on a set of genotypes. The model accommodates growing data col-
lections and noisy and/or incomplete observations. The approach also naturally imposes an implicit
bias toward small ancestral pools during inference, reminiscent of parsimony methods, doing so in
a well-founded statistical framework that permits errors.
Our focus here has been on adapting the technology of the Dirichlet process to the setting of
the standard haplotype phasing problem. But an important underlying motivation for our work, and
a general motivation for pursuing probabilistic approaches to genomic inference problems, is the
potential value of our model as a building block for more expressive models. In particular, as in
Greenspan and Geiger [2003] and Lauritzen and Sheehan [2002], the graphical model formalism
naturally accommodates various extensions, such as segmentation of chromosomes into haplotype
blocks and the inclusion of pedigree relationships. In section §3.4, we have outlined a preliminary
extension of the basic Dirichlet process mixture model that incorporates pedigree relationships and
brieﬂy discussed how to model realistic biological processes that might inﬂuence haplotype forma-
tion and diversiﬁcation, such as recombination and mutation during single generation inheritance.
We recognize that many other important issues also deserve careful attention, for example, haplo-
type recombinations among the ancestral haplotype pools (so far, we assume that these ancestral
haplotypes relate to modern individual haplotypes only via mutations), aspects of evolutionary dy-
namics (e.g., coalescence, selection, etc.), and linkage analysis under joint modeling of complex
traits and haplotypes. We believe that the graphical model formalism we proposed can readily
accommodate such extensions. In particular, it appears reasonable to employ an ancestral recombi-
nation hypothesis (rather than single generation recombination) to account for common individual
haplotypes that are distant from any single ancestral haplotype template, but can be matched piece-
wise to multiple ancestral haplotypes. This may be an important aspect of chromosomal evolution
and can provide valuable insight into the dynamics of populational genetics in addition to point-
mutation-based coalescence theory, and can potentially improve efﬁciency and quality of haplotype
inference.
134

3.5 Conclusions and Discussions
The Dirichlet process parameterization also provides a natural upgrade path for the consider-
ation of richer models; in particular, it is possible to incorporate more elaborate base measures G
into the Dirichlet process framework—the coalescence-based distribution of Stephens et al. [2001]
would be an interesting choice. In Chapter 5, while developing MCMC algorithms for haplotype in-
ference, we will also brieﬂy discuss a heuristic for constructing an informative base measure for the
DP using low-quality but inexpensive haplotype information (e.g., that obtained from a conventional
EM algorithm). Note that the partition structure of the Dirichlet process is equivalent to that induced
by the Ewens sampling formula (ESF) [Tavare and Ewens, 1998] known to the population genetics
community. The ESF represents a non-Darwinian theory of evolution which claims that “the exten-
sive genetic variation observed in natural populations is, on the whole, not due to natural selection,
but arises rather as a result of purely stochastic changes in gene (allele) frequencies in a ﬁnite pop-
ulation” [Tavare and Ewens, 1998]. The fact that our DP mixture model performed adequately in
a number of problems suggests that such non-Darwinian evolution may apply to SNP distribution,
which is interesting, yet would appear paradoxical, if we proceed to use haplotypes to map clearly
non-neutral genes (say, those that relate to biological disorders) via linkage disequilibrium.
135

Chapter 4
Probabilistic Inference I: Deterministic
Algorithms
The Bayesian graphical models presented in the last two chapters both deﬁne high-dimensional,
hybrid probability distributions for which important statistical queries may be difﬁcult to compute.
For example, in the LOGOS model, the sequence variable Yt at site t of a study sequence depends on
all the motif parameters {θ
(k)
l
|∀l, k}, each of which in turn depends on one of the PSMD prototype
(i.e., Dirichlet component) indicators {S
(k)
l
|∀l, k} coupled by a ﬁrst-order Markov chain. Thus, to
compute the posterior probability distributions p(xt|y) and p(θ
(k)
l |y) for MAP prediction of motif
locations and Bayesian estimation of motif PWMs, one has to integrate over the Cartesian product
of a continuous state space for the PWMs and the discrete spaces for the PSMD prototype (denoted
as D) and for the sequence annotation indicators (denoted as S). The complexity of such a state
space is on the order of
R4×P
k Lk × |D|
P
k Lk × |S|T ,
which translates to O(R120 × 101000) for a 1000 bp sequence harboring only two possible motif
patterns each of length 15 bp. Clearly, this computation is in general intractable with any off-the-
shelf exact algorithm and some approximation scheme is necessary. In this chapter, we present a
general variational approach for computing deterministic approximations to such intractable dis-
tributions.
In the next chapter, we brieﬂy discuss stochastic approximation methods based on
sampling. Some of the materials covered in this chapter have appeared in [Xing et al., 2003b;
136

4.1 Background
Xing et al., 2004a].
4.1
Background
For a multivariate probability distribution p(xH, xE), where XH and XE denote the sets of all un-
observed (i.e., hidden) and observed (i.e., evidence) variables, respectively (and, following conven-
tion, their lower case counterparts denote states or values of the corresponding variables), the gen-
eral problem of probabilistic inference is that of computing the conditional probabilities p(xF|xE),
where F ⊆H is the index set of an arbitrary subset of hidden variables.
Probabilistic inference techniques play an important role in any probabilistic methodology for
prediction and learning. For example, probabilistic prediction of unobserved events or patterns
in real world tasks such as weather forecasting, text segmentation and tagging, robot localization,
image analysis, ﬁltering and smoothing of sequential data streams, and various computational biol-
ogy problems such as motif, haplotype and pedigree inference considered in this thesis, all involve
performing probabilistic inference on a domain-speciﬁc, high-dimensional, and often hybrid (i.e.,
comprising both discrete and continuous variables) probability model. Probabilistic inference is
also indispensable for the acquisition of probability models from incomplete or partially observed
data using statistical learning methods, because many of these methods amount to parameter estima-
tion based on a maximum likelihood or an empirical Bayes principle [Efron, 1996], which employs
an inference subroutine to impute the unobserved variable(s) for computing the necessary sufﬁcient
statistics.
Solving an inference query can be understood as a marginalization computation. To see this,
observe that the conditional probability p(xF|xE) is equal to:
p(xF|xE) = p(xF, xE)
p(xE)
=
P
xH\F p(xH\F, xF, xE)
P
xF p(xF, xE)
,
(4.1)
where the summation (or integration in case of continuous variables) over all possible values of
some (or all) hidden variables in the model is called marginalization. Typically, an inference query
involves computing the conditional probabilities for only small subsets of variables (e.g., that of
137

4.1 Background
singleton hidden variables such as xt in the LOGOS model), and sometimes a large number of
such queries need to be processed (e.g., all xt’s for motif detection under LOGOS). This is often
a computationally expensive operation, as the state space to be swept during marginalization grows
exponentially with the number of variables being marginalized. The graphical model formalism
provides a systematic and efﬁcient approach to such computation. General exact inference algo-
rithms have been developed, which take advantage of the conditional independencies present in the
joint distribution p(xH, xE), which can be inferred from the pattern of missing edges in the graph,
to distribute the high-dimensional combinatorial summation over all hidden variables in a standard
marginalization operation into a sequence of low-dimensional local summations each over a (small)
subset of hidden variables (Fig. 4.1). We will brieﬂy describe a representative of these algorithms,
the junction tree algorithm, in the next section.
X1
X2
X3
X6
X5
X4
Figure 4.1: Inference on a graphical model. The dark shading indicates the node on which we condition, the unshaded
node is the one for which we wish to compute the conditional probability distribution, and the lightly shaded nodes are
those that need to be marginalized out in computing the posterior probability p(x1|x6). For this graphical model, the sum-
mations for computing the joint marginal can be distributed to subsets of variables in the following way (formally known
as an elimination algorithm): p(x1, x6) = P
x2,x3,x4,x5 p(x1)p(x2|x1)p(x4|x1)p(x3|x2)p(x5|x4)p(x6|x2, x5) =
p(x1) P
x2 p(x2|x1) P
x3 p(x3|x2) P
x4 p(x4|x1) P
x5 p(x5|x4)p(x6|x2, x5)
.
Although there are many cases in which the exact algorithms provide a satisfactory solution
to the inference and learning problems, large-scale probability models arising from complex real
world domains have outgrown the ability of current (and probably future) exact inference algorithms
to compute marginals and learn parameters. This is particularly true for models we developed
in this dissertation, which concern complex gene regulation elements and genetic polymorphism
patterns in the genomic sequences. As illustrated at the beginning of this chapter, the time and
138

4.1 Background
space complexity of the exact algorithms is unacceptable and it is necessary to have recourse to
approximation procedures.
For this reason, the development of efﬁcient and broadly applicable approximation algorithms
for probabilistic inference is critical to further progress. Two commonly used approximation tech-
niques are Monte Carlo methods (such as Markov chain Monte Carlo, or MCMC) and variational
methods. MCMC techniques are asymptotically exact and easy to apply. The BUGS system uses
MCMC within a general-purpose statistical modeling language (see [Gilks et al., 1996]), and the
inference process can be set up automatically for a variety of models. Unfortunately, MCMC often
converges very slowly. Variational methods, on the other hand, are claimed to exhibit fast conver-
gence and (in some cases) give a deterministic lower bound on the true likelihood. The original
belief propagation (BP) method [Pearl, 1988] is now understood as a variational algorithm [Yedidia
et al., 2001b] that (if it converges) calculates an optimal approximation to the true posterior dis-
tribution among those approximate distributions that include only pairwise dependencies among
variables. BP can be applied straightforwardly to a wide range of probability models and it has been
used for biological classiﬁcation/clustering problems expressed as complex graphical models [Se-
gal et al., 2001]. A generalized BP (GBP) algorithm can be derived that operates with dependency
structures on larger clusters of variables and often gives more accurate results [Yedidia et al., 2001a]
1. Like BP, GBP sometimes fails to converge. It may also fail to give a lower bound on the true
likelihood due to the use of an ad hoc approximation to the intractable entropy term in the objective
functional it optimizes (to be detailed in §4.3.4.3). Other variational approximation methods based
on structured mean ﬁeld approximation have been developed that are guaranteed to converge to
lower bounds on the true likelihood (see, e.g., [Jordan et al., 1999]), but these methods often require
model-speciﬁc derivation of iteration equations.
In this chapter, we develop a generalized mean ﬁeld (GMF) theory which leads to a generic
variational inference algorithm that is straightforwardly applicable to a wide range of models and is
guaranteed to converge to a lower bound on the true likelihood. Given an arbitrary decomposition of
1Similar techniques called cluster variational methods (CVMs) have also been developed in the statistical physics
community [Kappen and Wiegerinck, 2002].
139

4.1 Background
the original model into disjoint clusters of variables, the algorithm computes the posterior marginal
for each cluster given its own evidence and the expected sufﬁcient statistics, obtained from its neigh-
boring clusters, of the variables in the cluster’s Markov blanket. Optimal clustering of the variables
can be obtained in a principled fashion via a graph partition algorithm. The algorithm operates in
an iterative, message-passing style until a ﬁxed point is reached. We show that the cluster marginals
retain exactly the intra-cluster dependencies of the original model, which means that the inference
problem within each cluster can be solved independently of the other clusters (given the Markov
blanket messages) by any inference method. This GMF algorithm is applied to the Bayesian motif
prediction and learning problem under the LOGOS model and shows signiﬁcant improvement over
a sampling-based approach (discussed in the next chapter).
4.1.1
Notation
Before starting the technical sections, here is a summary of some necessary notations and deﬁnitions
needed in our exposition.
We consider a graph (directed or undirected) G(V, E), where V denotes the set of nodes (ver-
tices) and E the set of edges (links) of the graph. Let Xn denote the random variable associated
with node n, for n ∈V; let XC denote the subset of variables associated with a subset of nodes C,
for C ⊆V, and let X = XV denote the collection of all variables associated with the nodes of the
graph. We use upper-case X (resp. X) to denote a random variable (resp. variable set), and lower-
case x (resp. x) to denote a certain state (or value, conﬁguration, etc.) taken by the corresponding
variable (resp. variable set). We refer to a graph H = (V, E′), where E′ ⊆E, as a subgraph of G.
We use C = {C1, C2, . . . , CI} to denote a disjoint partition (or, a clustering) of all nodes in graph
G, where Ci refers to the set of indices of nodes in cluster i; likewise, D = {D1, D2, . . . , DK}
denotes a set of cliques (i.e., completely connected subsets of nodes) of G. For a given clustering,
we deﬁne the border clique set Bi as the set of cliques that intersect with but are not contained in
cluster i; and the neighbor cluster set Ni as the set of clusters that contain nodes connected to nodes
in cluster i. For undirected graphs, the Markov blanket of a cluster i (MBi) is the set of all nodes
140

4.2 Exact Inference Algorithms
outside Ci that connect to some node in Ci, and, for directed graphs, the Markov blanket is the set
of all nodes outside Ci that are parents, children, or co-parents (other than those already in Ci) 2 of
some node in Ci (Fig. 4.2). Clusters that intersect with MBi are called the Markov blanket clusters
(MBCi) of Ci.
...
y11
y12
y13
y14
y15
y16
...
y71
y72
y73
y74
y75
y76
...
y21
y22
y23
y24
y25
y26
...
y61
y62
y63
y64
y65
y66
...
y51
y52
y53
y54
y55
y56
...
y
y42
y43
y44
y45
y46
41
...
y31
y32
y33
y34
y35
y36
C1
Figure 4.2: The Markov blanket MB1 (blue-shaded nodes) of cluster 1 in a directed graph. Shaded blobs constitute
MBC1.
4.2
Exact Inference Algorithms
In this section, we give a brief overview of the junction tree algorithm [S. Lauritzen, 1988]. It is a
general purpose algorithm which subsumes many other exact inference inference algorithms (e.g.,
belief propagation for tree models [Pearl, 1988], the forward-backward algorithms for HMMs [Ra-
biner and Juang, 1986], the peeling algorithm for pedigree models [Thompson, 1981], etc.) as
special cases.
4.2.1
The Junction Tree Algorithm
As described in Chapter 1, for a directed graphical model G(V, E), the joint probability distribution
for all the |V| nodes in the graph can be written as the product of all local conditional distributions
deﬁned on each node and its parent(s):
p(x) =
|V|
Y
i=1
p(xi|xπi).
(4.2)
2A co-parent of a node, say, Xv, is deﬁned as the parent (other than Xv) of a child node Xu of Xv.
141

4.2 Exact Inference Algorithms
For an undirected graphical model, the joint probability distribution is equal to the product of the
potential functions associated with each clique of the graph, up to a normalization constant:
p(x) =
Q|D|
α=1 φα(xDα)
Z
,
(4.3)
where Z = P
x
Q|D|
α=1 φα(xDα) is referred to as a “partition function”.
X1
X2
X3
X6
X5
X4
⇒
X1
X2
X3
X6
X5
X4
Figure 4.3: Moralization of a directed graph.
A directed graphical model can be converted into an equivalent undirected graphical model via
an operation called “moralization,” which connects all parents of a common child node pairwise
with undirected edges, and then drops the directionality of all other edges in the graph (Fig. 4.3).
The resulting graph is called a “moral graph,” in which all the nodes originally involved in a local
conditional distribution in the directed graph now appear together in a common clique. Thus, local
conditional distributions in a directed graph can be thought of as normalized potential functions
in the corresponding moral graph, and the product rule (i.e., Eq. (4.2) and Eq. (4.3)) of the joint
distribution gives the same outcome for the directed model and its undirected counterpart. Due to
the equivalence of the undirected moral graph to the original directed graph in representing a joint
probability distribution, the junction tree algorithm concerns only undirected graphs.
The junction tree algorithm starts with the moralized graph. It ﬁrst chooses an elimination order
for all nodes in the graph, and applies an operation called triangulation to this order as follows: 1)
choose the next node in the elimination order, 2) add edges to link all remaining pairs of nodes
that are neighbors of this node and, 3) remove the node (and all its incident edges) from the graph.
Taking the new edges added in this process and adding them to the original moralized graph yield a
triangulated graph (Fig. 4.4a).
142

4.2 Exact Inference Algorithms
A triangulated graph allows the creation of a data structure known as a junction tree (Fig. 4.4b),
on which a generalized message-passing algorithm can be deﬁned. A full discussion of the con-
struction of a junction tree is beyond the scope of this thesis; in short, it is a maximal spanning
tree of cliques in the triangulated graph, with weights deﬁned by the cardinality of the intersections
between cliques. A key property of the junction tree is the so called running intersection property,
which says that if a node appears in any two cliques in the tree, it appears in all cliques that lie on
the path between the two cliques. As a consequence of this property, in a junction tree, local con-
sistency (i.e., potentials of adjacent cliques in the tree agree on marginals of any shared variables)
implies global consistency (i.e., potentials of all cliques in the tree agree on marginals of common
variables).
X1
X2
X6
X5
X4
X3
X1 X2 X4
,
,
X2 X4 X5
,
,
X2 X5 X6
,
,
X2 X5
,
X2 X3
,
X2 X4
,
X2
(a)
(b)
Figure 4.4: Construction of the junction tree. (a) The triangulated graph of the graphical model in Fig. 4.3. (b) The
junction tree. Squares represent original cliques in the triangulated graph, ellipsoids represent separators of adjacent
cliques.
With the junction tree, the joint probability distribution can now be expressed in the following
factored form:
p(x)
=
Q
Ci∈CT ψi(xCi)
Q
Sj∈ST φj(xSj),
(4.4)
where CT is the set of all cliques in the triangulated graph and ST is the set of separators (i.e., clique
intersections) spanned by the junction tree.
143

4.2 Exact Inference Algorithms
The clique potentials ψ(·) and separator potentials φ(·) can be updated by running a message-
passing protocol on the junction tree, with the following update rule:
φ∗
j(xSj) =
X
xCi\Sj
ψi(xCi),
ψ∗
k(xCk) =
φ∗
j(xSj)
φj(xSj)ψk(xCk),
where XSj denotes the set of variables that separates cliques XCi and XCj, and the “message”
is now passed from clique i to clique k via separator j (Fig. 4.5). The protocol typically starts
by picking a root of the tree, and then ﬁrst passing messages from root to all leaves along tree
branches, and then collecting messages from all leaves to the root, which leads to ψi = p(xCi) and
φj = p(xSj) for all i, j, when the message passing terminates. Note that a single run of the junction
tree algorithm yields all clique marginals, not merely that corresponding to a single clique.
X1 X2 X4
,
,
X2 X4 X5
,
,
X2 X5 X6
,
,
X2 X5
,
X2 X3
,
X2 X4
,
X2
i
X
k
X
j
X
Figure 4.5: Message passing in a junction tree.
It is easy to see that the computational bottleneck of the junction tree algorithm is determined
by the size of the maximal clique in the triangulated graph, which is affected by the choice of the
elimination order that induces the triangulated graph. The minimum of the maximal clique size
among all possible triangulations is know as the tree width of the graph. Choosing an elimination
order that minimizes the maximal clique size is non-trivial (indeed, it is an NP-hard problem for
arbitrary graphs, but can often be effectively approached on special graphs). There are many special-
purpose exact inference algorithms for speciﬁc families of graphical models (e.g., the forward-
backward algorithm for HMMs, Pearl’s belief propagation algorithm for trees, etc.), almost all of
which are essentially special cases of the the junction tree algorithm applied to special graphs, using
a special and often optimal choice of the elimination order for triangulation.
144

4.3 Approximate Inference Algorithms
4.3
Approximate Inference Algorithms
As mentioned, for a complex distribution, computing the marginal (or conditional) distributions,
as well as the maximum a posteriori conﬁgurations, of an arbitrary subset of the random variables
is intractable. The variational approach to these inference problems involves converting them into
an optimization problem, then approximating the feasible set of the solution or the function to
be optimized (or both), and solving the relaxed optimization problem. Thus, given a probability
distribution p(x|θ) that factors according to a graph, the variational methods yield approximations
to marginal probabilities via the solution to an optimization problem that generally exploits some
of the graphical structure. In the sequel, we describe a general variational principle for inference
in probabilistic graphical models, on which a variety of extant deterministic approximate inference
techniques are based, and from which we draw the mathematical foundations for the subsequent
development of a more general approach for approximate inference called generalized mean ﬁeld
(GMF) inference. We begin with some necessary deﬁnitions and algebraic preliminaries.
4.3.1
Cluster-factorizable Potentials
Given a clustering C of all nodes in G(V, E), some cliques in D may intersect with multiple
clusters (Fig. 4.6).
Cluster-factorizable potentials are potential functions which take the form
φβ(xDβ) = Fβ(φβi(xDβ∩Ci), . . . , φβj(xDβ∩Cj)), where F(·) is a (multiplicatively, or additively)
factorizable function over its arguments; i.e., in the case of two clusters, F(a, b) = a × b or a + b.
Factorizable potentials are common in many model classes. For example, the classical Ising model
is based on singleton and pairwise potentials of the following factorizable form (under the expo-
nential representation, as described shortly): φ(xi) = θixi, φ(xi, xj) = θijxixj; higher-order Ising
models and many more general discrete models also admit factorizable potentials; conjugate expo-
nential pairs, such as the Dirichlet-multinomial, linear-Gaussian, etc., are also factorizable; ﬁnally,
for logistic functions and other generalized linear models (GLIMs) that are not directly factorizable,
it is often possible to obtain a factorizable variational transformation in the exponential family that
145

4.3 Approximate Inference Algorithms
lower bounds the original function [Jaakkola and Jordan, 2000]. In other cases (e.g., tabular poten-
tials over a clustering of variables), a more general treatment based on peripheral marginal potentials
can be used (see §4.4.3). We will see that cluster-factorizable potentials allow the decoupling of the
computation of expected potentials.
Ci
Ck
Cl
D
β
Figure 4.6: A clique Dβ intersecting with three clusters {Ci, Cj, Ck} in an undirected graph.
4.3.2
Exponential Representations
In order to formulate variational inference as a generic optimization problem, it is convenient to use
the following exponential representation for a graphical model.
Similar to the general parameterization of graphical models introduced in Chapter 1, under
exponential representations, for undirected graphical models, the family of joint probability distri-
butions associated with a given graph can be parameterized in terms of a set of potential functions
associated with a set of cliques in the graphs 3. For a set of cliques D = {Dα|α ∈A} associated
with an undirected graph, indexed by a set A, let φ = {φα|α ∈A} denote the set of potential
functions deﬁned on the cliques, and θ = {θα|α ∈A} the set of parameters associated with these
potential functions (for simplicity, we label φ and θ with the corresponding clique index, e.g., α,
rather than with the clique Dα itself). The family of joint distributions determined by φ can be
3More precisely, these potential functions are now exponential potential functions that are semantically different from
what we meant by “potential functions” in our early exposition of graphical models. Technically, however, little difference
exists in their deﬁnitions, except that the range of the exponential potential functions is all real numbers whereas the
original potential functions have positive values. For ﬁxed potential weights, there exists a one-to-one correspondence
between the two types of potential functions. For simplicity, in the sequel we still use the term “potential functions” in
our exposition under the exponential representations.
146

4.3 Approximate Inference Algorithms
expressed as follows:
p(x|θ) = exp{
X
α∈A
θαφα(xDα) −A(θ)}
(4.5)
where A(θ) is the log partition function. We also deﬁne the energy, E(x) = −P
α θαφα(xDα), for
state x.
For directed graphical models, in which the joint probability is deﬁned as p(x) = Q
i p(xi|xπi),
we transform the underlying directed graph into a moral graph, and set the potential functions
φi(xi, xπi) equal to the logarithms of the local conditional probabilities p(xi|xπi). In the sequel, we
will focus on models based on conditional exponential families. That is, the conditional distributions
p(xi|xπi) can be expressed as:
p(xi|xπi) = u(xi) exp{θT
i φi(xi, xπi) −A(θi, xπi)},
(4.6)
where φi(xi, xπi) is a vector of potentials associated with the variable set {xi, xπi}.
The exponential representation applies to a wide range of models of practical interest, including
discrete models, Gaussian, Poisson, exponential, and many others.
4.3.3
Lower Bounds of General Exponential Functions
Now we review some basic results from standard calculus that provide a principled way of con-
structing higher-order bounds for regular functions. Start from a simple bound for a function f0(x):
f0(x) ≥b0(x), ∀x ∈X.
Lemma 1 For anti-derivatives f1(x) of f0 and b1(x) of b0 such that f1(a) = b1(a) for some a ∈X:
f1(x) ≤b1(x)
for x ≤a
f1(x) ≥b1(x)
for x ≥a
Proof. Due to the simple bound assumption, for x ≥a:
Z x
a
dzf0(z)
≥
Z x
a
dzb0(z)
⇒
f1(x) −f1(a)
≥
b1(x) −b1(a)
⇒
f1(x) −b1(x)
≥
f1(a) −b1(a) = 0.
147

4.3 Approximate Inference Algorithms
The other direction (i.e., when x ≤a) follows similarly.
Lemma 2 For anti-derivatives f2(x) of f1 and b2(x) of b1 such that f2(a) = b2(a), f1(a) = b1(a):
f2(x) ≥b2(x)
for x ∈X
Proof. Due to Lemma 1, for x ≤a:
Z a
x
dzf1(z)
≤
Z a
x
dzb1(z)
⇒
f2(x) −b2(x)
≥
f2(a) −b2(a) = 0
For x ≥a, the same inequality follows similarly.
Thus we have the following theorem:
Theorem 1 Let fk(x) denote the kth-order anti-derivative of the function f(x). Given a lower
bound b(x) of the function f(x), the 2nd-order anti-derivative b2(x) of the original bound, parame-
terized by a variational parameter µ such that b1(µ) = f1(µ) and b2(µ) = f2(µ), is a lower bound
of f2(x). Likewise (by induction), bounds for higher-order anti-derivatives of f can be successively
constructed.
Since the anti-derivative of the exponential function is just itself, we can easily use Theorem 1
to obtain linear and higher-degree polynomial bounds from bounds of lower order. For example,
the well known linear bound of the exponential function, its tangent at x = µ (see Fig 4.7), can be
readily derived from the trivial bound exp(x) > 0 using Theorem 1:
f(x) = exp(x) ≥exp(µ)(1 + x −µ) = b2(x), ∀x, µ
(4.7)
Integrating over both sides twice, and denoting the variational parameters in the new bound
as ν (which means that new bound “touches” the original function at ν), we have the following
third-order bound:
f(x)
=
exp(x)
≥
exp(ν)

1 + x −ν + exp(ξ)
 1 −ξ
2
(x −ν)2 + 1
6(x −ν)3	
,
=
b4(x).
(4.8)
148

4.3 Approximate Inference Algorithms
where ξ = µ −ν. When ξ = 0, that is, restricting the higher order bound to “touch” the original
function at the same point as the lower order bound, we have b4(x) = 1
6 exp(µ)((x−µ)3 +3(x−µ)2 +
6(x −µ + 1)). From Figure 4.7, we can see that this bound is much tighter than the linear bound.
Figure 4.7: The tangent (blue curve) and polynomial (red curve) bounds for an exponential function (black curve).
4.3.3.1
Lower bounding probabilistic invariants
The tangent and polynomial bounds of exponential functions can be used to deﬁne objective func-
tionals underlying the variational principle for probabilistic inference by introducing bounds for the
probabilistic invariants associated with a distribution and/or data, such as the likelihood and the par-
tition function. Let q(xH) = exp{−E′(xH)} represent an arbitrary probability distribution (written
in an exponential representation) over the hidden variables of a model to be approximated. A bound
for the likelihood can be characterized by the following lemma.
Lemma 3 Every marginal distribution q(xH) = exp{−E′(xH)} deﬁnes a lower bound of likeli-
hood:
p(xE)
≥
Z
dxH exp
n
−E′(xH)
o
1 −A(xE) −
 E(xH, xE) −E′(xH)

,
(4.9)
where xE denotes observed variables (evidence).
149

4.3 Approximate Inference Algorithms
Proof. Using the tangent bound of the exponential function (Eq. 4.7), for a joint distribution
p(xH, xE) = exp{−E(xH, xE) −A(xE)} (where A(xE) is the original log-partition function plus
the constant evidence potentials), we replace x in Eq. (4.7) with −(E(xH, xE) + A(xE)) and lower
bound the joint distribution p(xH, xE) as follows:
p(xH, xE) ≥q(x)(1 −A(xE) −(E(xH, xE) −E′(xH))),
(4.10)
where E′(xH) deﬁnes a variational marginal distribution. Integrating over xH on both sides, we
obtain the ﬁrst-order lower bound in Eq. (4.9).
This bound is similar to the well-known Jensen bound on the log-likelihood: log p(xE) ≥
R
dxHq(xH) log
q(xH)
p(xH,xE), and has the same maximizer, but it is more general in that it can be
further upgraded to higher order bounds for tighter approximation using Eq. (4.8).
Rearranging terms on the right hand side of inequality (4.9), we have the following compact
form of the lower bound on the likelihood:
p(xE)
≥
C −

E(xH, xE)

q(xH) +

log q(xH)

q(xH)
=
C −

E

q −Hq,
(4.11)
where the ﬁrst term C is a constant related to the log-partition function of the original distribution,
the second term

E

q is the expected energy under distribution q, and the third term Hq is the en-
tropy of distribution q. Note that when no variable in a model is observed, the foregoing exposition
can lead to a lower bound on the log-partition function:
A
≥
1 −

E

q −Hq.
(4.12)
For simplicity, we focus on the likelihood in the sequel, but the exposition applies readily to the
bound on the log-partition function.
4.3.4
A General Variational Principle for Probabilistic Inference
The likelihood bound derived in the previous section plays a pivotal role in formulating a proba-
bilistic inference problem variationally, because it makes explicit an objective functional that can be
150

4.3 Approximate Inference Algorithms
optimized over the space of all distributions, and leads to a variational representation of a probability
distribution.
4.3.4.1
Variational representation
Let Q denote the set of all distributions on X n. Given any distribution p represented in the form
(4.5), from Eq. (4.9), it is apparent from our discussion so far that the associated likelihood function
p(xE) can be recovered as a solution of the following optimization problem:
p(xE)
=
max
q∈Q
n
−

E

q −Hq
o
=
min
q∈Q
n
E

q + Hq
o
.
(4.13)
Moreover, the optimum is uniquely attained when q = p. Note that here the optimization prob-
lem is deﬁned on a ﬁrst-order lower bound of the likelihood, and an equivalent result can also be
obtained from the well-known minimal KL problem: minq∈Q KL(q∥p) = 0, attained at q = p,
where KL(q∥p) ≡
R
x log q(x) log q(x)
p(x) is the Kullback-Leibler divergence from q to p. But for
higher-order bounds of p(xE), although the solution (i.e., the optimizer) remains the same, a differ-
ent optimization problem needs to be solved, whose relaxation may lead to better approximation.
Consider exponential family graphical models. In this case, the optimization problem described
above takes place over a space that includes all choices of potential functions φ and all valid weight
parameters θ associated with these potential functions. It should be clear that depending on the
choice of canonical parameterization for the density functions q(·), the formal deﬁnition of the opti-
mization space varies signiﬁcantly. For example, under the exponential parameterization as we used
here for exponential families, θ belongs to the set Θ ≡{θ ∈R|D| |A(θ) < inf}; under the mean
parameterization for discrete distributions, one needs to optimize over a marginal polytope [Wain-
wright and Jordan, 2003], M ≡{µ ∈R|D| | ∃p(·) s.t.
R
φ(x)p(x)dx = µ}, where φ(x) denotes
the vector of all potential functions associated with the graphical model. Wainwright et al. [2003]
pointed out that if and only if the exponential representation is minimal (i.e., no afﬁne combination
of φ(x) is equal to a constant), there is a one-to-one mapping from Θ to M.
151

4.3 Approximate Inference Algorithms
In general, computing the entropy for an arbitrary distribution q, and hence the objective func-
tion in Eq. (4.13) is intractable. Furthermore, in many cases of interest, characterizing the op-
timization space (e.g., the marginal polytope) is not possible. Thus usually one cannot solve the
variational representation deﬁned by Eq. (4.13) analytically. Variational inference amounts to seek-
ing an optimal q∗under a relaxed variational representation, which is entailed by approximating the
entropy Hq; or redeﬁning (e.g., relaxing or tightening) the optimization space Q, so that within the
redeﬁned space, referred to as a feasible space, the entropy of q is tractable; or doing both. We refer
to the resulting q∗as a variational approximation to the true distribution p:
Deﬁnition 1 Variational approximation
(VP)
q
=
arg max
q∈Qv
n
−

E

q −Fv(q)
o
(4.14)
where Qv is the feasible space of realizable distributions, and Fv(q) is an approximate entropy term
deﬁned on q.
4.3.4.2
Mean ﬁeld methods
One class of variational inference methods attempts to approximate a distribution p using a family
of tractable distributions, q(x|γ), which are deﬁned on subgraphs of the original graph G(p), for
which exact computation of the entropy Hq is feasible. The γ are a set of free “variational pa-
rameters.” This class of methods is referred to as “mean ﬁeld methods” [Jordan et al., 1999], a
terminology that reﬂects the classical setting in which q(x|γ) is taken to be a completely factorized
distribution. From an optimization theoretic point of view, a mean ﬁeld method solves a reduced
version of problem (4.14), in which Qv = T , where T denotes the space of all distributions that
factor according to tractable subgraphs of G(p). This is an inner approximation of the space of all
possible distributions (i.e., T ⊂Q) [Wainwright and Jordan, 2003]. In these methods, Fv(q) = Hq,
is the exact entropy for q. It is easy to see that such a reduction deﬁnes a lower bound on the
likelihood p(xE) (because we are optimizing over a subspace of the original optimization space),
and hence mean ﬁeld methods are essentially maximizing a lower bound of the true likelihood, a
152

4.3 Approximate Inference Algorithms
nice property useful in justifying their application, especially in likelihood-based model learning
(i.e., parameter estimation), although in practice the tightness of the bound heavily depends on the
choice of feasible space.
Recall that Q consists of two components: the space of potential functions φ and the space
of parameters θ. For a general multivariate probability distribution, the potential space spans the
choices of both the coupling topology (i.e., which subsets of variables xD come under a single poten-
tial) and the coupling kernel (i.e., the functional form of φ(·)). The coupling topology is encoded in
the graphical representation of a multivariate distribution, and the coupling kernels reﬂect choices
of mappings from the joint state conﬁgurations of variable subsets to values related to their joint
probabilities. In principle, optimization could take place in the space of, 1) all tractable subgraphs,
2) all valid potential functions (kernels) on such subgraphs, and 3) all valid parameters associated
with the given set of potentials. In practice, nearly all extant mean ﬁeld algorithms focus on param-
eter optimization (i.e., the 3rd aspect) but rarely explore the other two aspects, or only do so in an
ad hoc way. For example, the classical mean ﬁeld method makes use of the simplest subgraph of
G(p)—the fully disjoint graph (i.e., with all edges removed), and chooses the potential function of
each singleton to be the variable itself (i.e., φ(x) = x). More recent structured variational inference
methods [Jordan et al., 1999] use more complex subgraphs of G(p), in particular, some speciﬁc dis-
joint partitions of G(p) motivated by both domain knowledge and computational tractability, and
a set of model-speciﬁc choices of potential functions associated with the subgraph. To explore the
third aspect of the optimization space, these methods seek an optimal value of the variational pa-
rameters via an iterative procedure using ﬁxed-point equations derived in a problem-speciﬁc manner
(e.g., depending on the choice of the coupling topology and the potential functions for the approx-
imate distribution). Since substantial mathematical skills are usually involved, sophisticated mean
ﬁeld methods have not gained much popularity among practitioners of approximate inference.
153

4.3 Approximate Inference Algorithms
4.3.4.3
Belief propagation
Recently, Yedidia et al. [2001b] realized that Pearl’s belief propagation (BP) algorithm—when ap-
plied to general loopy graphs—is also a variational algorithm. The inference problem is trans-
formed to an optimization functional—the “Bethe free energy”—that imposes local consistency
on the approximate marginals. Speciﬁcally, BP, and related algorithms (e.g., GBP, CVM, etc.),
seek to directly estimate a set of marginals of interest associated with the study distribution p,
for example, all marginals of variable pairs that are adjacent in the graph G(V, E), i.e., {µij ≡

XiXj

p |∀i, j, s.t., (ij) ∈E} , and all the singleton marginals, i.e., {µi ≡

Xi

p |∀i, s.t., i ∈V},
by optimizing a so-called Bethe free energy. As pointed out by Wainwright and Jordan [2003], this
problem can be understood as seeking a particular mean parameterization for an approximate dis-
tribution.
Under the general framework of variational approximation described by Eq. (4.14), the Bethe
free energy is equal to the sum of the expected energy

E

q as in Eq. (4.14), and another term
called the Bethe entropy, HBethe, which is an approximation to the true entropy Hq. Recall that Hq
is intractable for general distributions; HBethe makes use of all single node entropies Hi(µi) and
edgewise mutual information terms Iij(µij) to form an approximation to Hq:
HBethe(µ)
≜
−
X
i∈V
Hi(µi) +
X
(i,j)∈E
Iij(µij).
(4.15)
An exact characterization of the marginal polytope given all the potential functions of the distribu-
tion p is intractable. To overcome this, BP optimizes over the space of locally consistent pairwise
marginals (i.e., tree-consistent marginals): MB ≡{τ ≥0| P
xi τi(xi) = 1, P
xi τij(xi, xj) =
τj(xj)}, which is an outer approximation to the original marginal polytope. The recently devel-
oped GBP algorithm optimizes over marginals of larger clusters of nodes to capture more complex
couplings (than the pairwise couplings in baseline loopy BP) in the distribution p, which leads to a
more complex optimization problem over the space of locally-consistent cluster marginals (a tighter
outer approximation of the marginal polytope of p than that from the pairwise marginals), and on
an objective function known as the Kikuchi free energy [Kikuchi, 1951] (a better approximation to
154

4.4 Generalized Mean Field Inference
the true free energy than the Bethe free energy). Similar to the mean ﬁeld methods, essentially BP
algorithms also begin with an ad hoc choice of coupling topology (that determines variables to be
included in cluster marginals), followed by an iterative procedure to search for ﬁxed-points in the
relaxed feasible space of marginals associated with each cluster.
An advantage of the Bethe (or Kikuchi) variational approach is the simplicity of the BP algo-
rithms. Generic ﬁxed-point equations can be derived based on the variational principle [Yedidia et
al., 2001b], which alleviates the need for model speciﬁc derivations in applications to a variety of
speciﬁc problems. The ﬂexibility provided by the ability to choose clusters of varying sizes in the
GBP and CVM algorithms is a signiﬁcant important step forward. However, due to the ad hoc re-
laxation of the original optimization functional and the feasible space for tractability, the marginals
resulting from GBP are not necessarily globally consistent (i.e., not necessarily in the marginal
polytope), so the inequality in Eq. (4.11) may no longer apply. Thus, the GBP approximation does
not necessarily yield a lower bound on the likelihood and a GBP algorithm may not converge. Also
note that since, in general, ﬁnding the mapping function from mean parameterization to the usual
exponential parameterization is as difﬁcult as performing inference, obtaining an explicit form of
the approximate distribution via BP is non-trivial, which makes certain probabilistic queries, e.g.,
arbitrary marginals of p, difﬁcult to handle. By contrast, in the mean ﬁeld method, the solution is
an explicit approximate distribution in exponential parameterization, on which general inference is
tractable.
4.4
Generalized Mean Field Inference
Mean ﬁeld methods can provide ﬂexibility similar to that by the GBP methods via the choice of
approximating distribution q(x|γ), and so-called “structured mean ﬁeld methods” have been based
on choosing q(x|γ) to be a tree or some other sparse subgraph of the original graph to which an exact
inference algorithm such as the junction tree algorithm can be feasibly applied [Saul and Jordan,
1996]. Recently, Wiegerinck presented a general framework for structured mean ﬁeld methods
involving arbitrary clusterings [Wiegerinck, 2000]. In particular, his approach allows the use of
155

4.4 Generalized Mean Field Inference
overlapping clusters, which leads to a set of mean ﬁeld equations reminiscent of a junction tree
algorithm. Although there continue to be developments in this area (e.g., [El-Hay and Friedman,
2001; Bishop et al., 2003; Bishop and Winn, 2003]), it is fair to say that in practice the use of
mean-ﬁeld-based variational methods requires substantial mathematical skill and that a systematic
approach with the generality, ﬂexibility and ease of implementation of GBP has yet to emerge. In
this section we describe a generalized mean ﬁeld method that aims to ﬁll this gap. The approach
yields a simple general methodology that applies to a wide range of models. To obtain the desired
simplicity our approach makes use of nonoverlapping clusters, specializing Wiegerinck’s general
approach, and yielding a method that is somewhat reminiscent of block methods in MCMC such as
Swendsen-Wang [Swendsen and Wang, 1987].
Note that the choice of clusters is generally done manually both within the GBP tradition and
the mean ﬁeld tradition. Another reason for our interest in nonoverlapping clusters is that it suggests
algorithms for automatically choosing clusters based on graph partitioning ideas. We will discuss a
preliminary exploration of these ideas in § 4.5.
4.4.1
GMF Theory and Algorithm
As stated, the mean ﬁeld approximation refers to a class of variational approximation methods
that approximate the true distribution p(x|θ) on a graph G with a simpler distribution, q(x|γ), for
which it is feasible to do exact inference. Such distributions are referred to as tractable families. A
tractable family usually corresponds to a subgraph of a graphical model.
4.4.1.1
Naive mean ﬁeld approximation
The naive mean ﬁeld approximation makes use of a subgraph that is completely disconnected. Thus,
the approximating distribution is fully factorized:
q(x) =
Y
i∈V
qi(xi).
(4.16)
For example, to use this family of distributions to approximate the joint probability of the Boltzmann
machine: p(x) =
1
Z exp{P
i<j θijxixj + P
i θi0xi} where xi ∈{0, 1}, one deﬁnes qi(xi) =
156

4.4 Generalized Mean Field Inference
µxi
i (1 −µi)1−xi, where the µi are the variational parameters. Minimizing the Kullback-Leibler
(KL) divergence between q and p, which is equivalent to solving Eq. (4.14) over the space of µi,
one obtains the classical “mean ﬁeld equations”:
µi = σ
  X
j∈Ni
θijµj + θi0

,
(4.17)
where σ(z) = 1/(1 + e−z) is the logistic function, and Ni is the set of nodes neighboring i. A little
algebra shows that indeed each singleton marginal can be expressed as a conditional distribution of
the relevant node given the expectation of all its neighbors, and this distribution reuses the set of
coupling weights of the original distribution p:
qi(xi)
=
exp
n
θi0xi +
X
j∈Ni
θijxi

Xj

qj + Ai
o
=
p(xi|{

Xj

qj | j ∈Ni}).
(4.18)
As the second line of Eq. (4.18) suggests, the mean ﬁeld approximation to the singleton marginal is
isomorphic to the corresponding singleton conditional under the original distribution p, with all the
neighboring nodes of the singleton being conditioned on replaced by their expectations under their
own singleton marginals. Conceptually,

Xj

qj resembles a ’message” sent from node j to i, and
{

Xj

qj | j ∈Ni} forms a “mean ﬁeld” applied to Xi from its neighborhood (Fig. 4.8).
i
X
Figure 4.8: Mean ﬁeld messages. The red node (Xi) denotes the variable whose marginal is being approximated; the
blue nodes are neighbors that send the messages (assuming that these are the nodes whose couplings to node i, i.e., θij,
are non-zero).
157

4.4 Generalized Mean Field Inference
Naive mean ﬁeld approximation can be efﬁciently solved by ﬁxed-point iteration. Procedu-
rally, this is similar to a Gibbs sampling scheme (see Chapter 5) in which one iteratively samples
each variable using a predictive distribution that conditions on the previously sampled values of
the neighboring variables. However, due to the deterministic replacement of the true value with an
expectation taken under an approximate marginal, the quality of the naive mean ﬁeld approximation
for arbitrary graphical models could break down in cases where the original graphs are sparse (so
that the distribution of inﬂuences from the neighborhood may not be highly concentrated over an
expectation) and the pairwise couplings are not uniform over all edges (i.e., the magnitudes of θi,j
vary signiﬁcantly over different node pairs, so that presence of strongly coupled pairs can bias the
approximation).
4.4.1.2
Generalized mean ﬁeld theory
The completely disconnected subgraph underlying the naive mean ﬁeld approximation differs sig-
niﬁcantly from the original graph, implying that many of the dependencies present in the original
model are left uncaptured. Intuitively, a subgraph with fewer edges removed would capture more
such dependencies and would deﬁne a family of distributions better at approximating the origi-
nal distribution. The basic idea of generalized mean ﬁeld approximation is to employ a richer set of
tractable approximate distributions which correspond to a subgraphs made up of tractable connected
components of clusters of nodes.
Given a (disjoint) variable clustering C, we deﬁne a cluster-factorized distribution as a distri-
bution of the form q(x) = Q
Ci∈C qi(xCi), where qi(xCi) = exp{−E′
i(xCi)}, ∀Ci ∈C, are free
distributions to be optimized. As discussed in §4.3.4, this optimization problem can be cast as that
of maximizing a lower bound on the likelihood over the space of all valid cluster marginals respect-
ing a given clustering C. The solution to this problem leads a generalized mean ﬁeld approximation
to the original distribution p(x). In the following, we present the generalized mean ﬁeld theorem
that states this result.
To make the exposition of the theorem and the resulting algorithm simple, we introduce some
158

4.4 Generalized Mean Field Inference
deﬁnitions.
Deﬁnition 2 (Mean ﬁeld factor): For a factorizable potential φβ(xDβ), let Iβ denote the set of
indices of those clusters that have nonempty intersection with Dβ. Thus, φβ(xDβ) has as factors the
potentials φβi(xCi∩Dβ), ∀i ∈Iβ. Then, the mean ﬁeld factor fiβ is deﬁned as:
fiβ ≜⟨φβi(XCi∩Dβ)⟩qi,
for i ∈Iβ
(4.19)
where ⟨·⟩qi denotes the expectation with respect to qi.
Deﬁnition 3 (Generalized mean ﬁelds): For any cluster Cj in a given variable partition, the set of
mean ﬁeld factors associated with the nodes in its Markov blanket is referred as the set of generalized
mean ﬁelds of cluster Cj:
Fj ≜{fiβ : Dβ ∈Bj, i ∈Iβ, i ̸= j}.
(4.20)
From Eq. (4.9), replacing E′(xH) with P
Ci∈C E′
i(xCi) and omitting A(xE) (which is a constant
determined by p, the distribution to be approximated) the optimal generalized mean ﬁeld approxi-
mation to p is speciﬁed as the solution of the following constrained optimization problem:
{E′GMF
i
(xCi)}Ci∈C
=
arg
max
E′
i∈E(xCi )
Z
dx exp

−
X
Ci∈C
E′
i(xCi)
	
1 −
 E(x)−
X
Ci∈C
E′
i(xCi)

,
(4.21)
where E(xCi) denotes the set of all valid energy functions of variable set xCi. (Because evidence
variables are ﬁxed constants in inference, for simplicity, we omit explicit mention of the evidence
xE, and the subscript H in the energy term E(·) above and in other relevant terms in the following
derivation. In should be clear that, in situations where such subscripts are omitted, x and related
symbols denote only the hidden variables.) The solution to this problem leads to the follow Gener-
alized Mean Field Theorem (the proof is provided in Appendix B.1),
159

4.4 Generalized Mean Field Inference
Theorem 2 (GMF approximation): For a general undirected probability model p(xH, xE) where
xH denotes hidden nodes and xE denotes evidence nodes, and for a clustering C : {xH,Ci, xE,Ci}I
i=1
of both hidden and evidence nodes, if all the potential functions that cross cluster borders are
cluster-factorizable, then the generalized mean ﬁeld approximation to the joint posterior p(xH|xE)
with respect to clustering C is a product of cluster marginals qGMF(xH) = Q
Ci∈C qGMF
i
(xH,Ci)
satisfying the following generalized mean ﬁeld equations:
qGMF
i
(xH,Ci)
=
p(xH,Ci|xE,Ci, Fi),
∀i.
(4.22)
Remark 1 Note that each variational cluster marginal is isomorphic to the isolated model fragment
corresponding to original cluster posterior given the intra-cluster evidence and the generalized mean
ﬁelds from outside the cluster. Thus, each variational cluster marginal inherits all local dependency
structures inside the cluster from the original model.
The mean ﬁeld equations in Theorem 2 are analogous to naive mean ﬁeld approximation by
Eq. (4.18). The generalized mean ﬁelds appearing in Eq. (4.22) play a role that is similar to the
conventional mean ﬁeld, now applying to the entire cluster rather than a single node, and conducting
probabilistic inﬂuence from the remaining part of the model to the cluster. It is easy to verify that
when the clusters reduce to singletons, Eq. (4.22) is equivalent to the classical mean ﬁeld equation
Eq. (4.17) (Fig. 4.9). From a conditional independence point of view, the generalized mean ﬁelds
can be also understood as an expected Markov blanket of the corresponding cluster, rendering its
interior nodes conditionally independent of the remainder of the model and hence localizing the
inference within each cluster given its generalized mean ﬁelds.
Mean ﬁeld approximation for directed models is also covered by Theorem 2. This is true be-
cause any directed network can be converted into an undirected network via moralization, and des-
ignation of the potentials as local conditional probabilities. The following corollary makes this
generalization explicit:
160

4.4 Generalized Mean Field Inference
X i
...
...
...
...
...
...
...
...
y11
y12
y13
y14
y15
y16
22
y23
...
y71
y72
y
y74
y75
y76
...
y61
y62
y63
y64
y65
y66
...
y51
y52
y53
y54
y55
y56
...
y
y42
y43
y44
y45
y46
41
...
y32
y33
y34
y35
C1
y
21y
y24
y25
y26
73
y31
y36
(a)
(b)
Figure 4.9: The generalized mean ﬁelds in: (a) a naive mean ﬁeld approximation and, (b) a GMF approximation. Red ar-
rows denote GMFs received by the center cluster (or node) from its neighborhood, green arrows denote GMFs contributed
by the center cluster (or node) to its neighborhood.
Corollary 3 For a directed probability model p(xH, xE) = Q
i p(xi|xπi) and a given disjoint vari-
able partition, if all the local conditional models p(xi|xπi) across the cluster borders admit cluster-
factorizable potentials, then the generalized mean ﬁeld approximation to the original distribution
has the following form: qGMF(xH) = Q
Ci∈C qGMF
i
(xH,Ci), and
qGMF
i
(xH,Ci)
=
p(xH,Ci|xE,Ci, Fi),
∀i,
(4.23)
where Fi refers to the generalized mean ﬁelds of the exterior parents, children and co-parents of the
variables in cluster i.
These theorems make it straightforward to obtain generalized mean ﬁeld equations. All that
is needed is to decide on a subgraph and a variable clustering, to identify the Markov blanket of
each cluster, and to plug in the mean ﬁelds of the Markov blanket variables according to Eqs. (4.22)
or (4.23). We illustrate the application of the generalized mean ﬁeld theorem to several typical
cases—undirected models, directed models, and models that combine continuous and discrete ran-
dom variables.
Example 1 (2-d nearest-neighbor Ising model): For a 2-d nearest neighbor Ising model, we can
pick a subgraph whose connected components are square blocks of nodes in the original graph
161

4.4 Generalized Mean Field Inference
(Fig. 4.10). The cluster marginal of a square block Gk is simply q(xGk) = exp{P
(ij)∈E(Gk) θijxixj +
P
i∈V(Gk) θi0xi + P
(ij)∈E(G),j∈MB(Gk) θij⟨xj⟩xi}, an Ising model of smaller size, with singleton poten-
tials for the peripheral nodes adjusted by the mean ﬁelds of the adjacent nodes outside the block
(which are the MB of xGk).
⋄
Example 2 (factorial hidden Markov models):
For the fHMM, whose underlying graph consists
of multiple chains of discrete hidden Markov variables coupled by a sequence of output nodes,
taken to be linear-Gaussian for concreteness, a possible subgraph that deﬁnes a tractable family is
shown in Figure 4.12, in which we retain only the edges within each chain of the original graph.
Given a clustering C, in which each cluster k contains a subset of HMM chains ck (the dashed
boxes in Fig. 4.12), the MB of each cluster consists of all nodes outside the cluster. Hence the
cluster marginal of ck is: q({x(mi)}i∈ck) ∝Q
i∈ck p(x(mi))p(y|{x(mi)}i∈ck, {f(x(mj))}j∈cl,l̸=k),
where x(mi) denotes variables of chain mi, p(x(mi)) is the usual HMM of a single chain, and p(y|·)
is linear-Gaussian. When each ck contains only a single chain, we recover the structured variational
inference equations in Ghahramani and Jordan [1997].
⋄
Example 3 (Variational Bayesian learning): Following the standard setup in Ghahramani and
Beal [2001], we have a complete data likelihood P(x, y|θ), where x is hidden, and a prior p(θ|η, ν),
where η, ν are hyperparameters. Partitioning all domain variables into two clusters, {x, y} and {θ},
if the potential function at the cluster border, φ(x, θ), is factorizable (which is equivalent to the con-
dition of conjugate exponentiality in Ghahramani and Beal [2001]), we obtain the following cluster
marginals using Corollary 3:
q(θ)
=
p(θ|η, ν, f(x), y) ∝p(f(x), y|θ)p(θ|η, ν)
q(x)
=
p(x|y, f(θ)).
These coupled updates are identical to the variational Bayesian learning updates of Ghahramani and
Beal [2001] and Attias [2000].
⋄
162

4.4 Generalized Mean Field Inference
4.4.2
A more general version of GMF theory
Recall that the GMF theory developed in the last section assumes the potential functions of the
cliques in the graphical models are cluster-factorizable, which is not always true for general distri-
butions, for example, in case of a distribution deﬁned by tabular potential functions. Now we brieﬂy
sketch a more general version the GMF theory, which subsumes the previous version.
Given a disjoint variable partitioning, C, the true cluster conditional of each variable cluster Ci
given its Markov blanket MBi is:
p(xCi|XMBi = xMBi)
∝
exp
n X
Dα⊆Ci
θαφα(xDα) +
X
Dβ⊆Bi
θβφβ(xDβ∩Ci, xDβ∩MBi)
o
,
(4.24)
where Bi denotes the set of cliques that intersect with but are not contained in cluster Ci. Note that
in Eq. (4.24), we distinguish two types of variables in each clique: xDβ∩Ci represents the variables
in the intersection of clique Dβ and cluster Ci, and xDβ∩MBi represents the variables in clique Dβ
but outside cluster Ci. Without loss of generality, we assume that all the potentials are positively
weighted (i.e., θ > 0) and the signs are subsumed in the potential functions.
Given a clique Dβ, recall that we use Iβ to denote the set of indices of clusters that have non-
empty intersection with Dβ. Let Iβi denotes Iβ \ i, which indexes the set of clusters other than Ci
that intersect with clique Dβ; let qIβi(·) = Q
j∈Iβi qj(xCj) denote the marginal distribution (deﬁned
by a product of mean ﬁeld cluster marginals) over the variables in these clusters (note that xDβ∩MBi
is a subset of the set of all variables in these clusters: {xCj |j ∈Iβi}). Finally, let us refer to the
(marginal) expectation of the potential φβ(XDβ) under the mean ﬁeld cluster marginals indexed by
Iβi as a peripheral marginal potential of cluster Ci:
φ′
β(xDβ∩Ci, qIβi)
≜

φβ(xDβ)

qIβi
=
Z
φβ(xDβ∩Ci, xDβ∩MBi)qIβi(xDβ∩MBi)dxDβ∩MBi,
(4.25)
which is only a functions of the variables in the intersection of clique Dβ cluster Ci, and qIβi(·).
163

4.4 Generalized Mean Field Inference
Given the peripheral marginal potentials of all the cliques intersecting with cluster Ci, we can
easily show (similar to the proof of Theorem 2 in Appendix B.1, and hence omitted) that the GMF
approximation to the cluster marginal of this cluster is:
qi(xCi)
∝
exp
n X
Dα⊆Ci
θαφα(xDα) +
X
Dβ⊆Bi
θβφ′
β(xDβ∩Ci, qIβi)
o
,
(4.26)
from which the isomorphism of the GMF approximation of the cluster marginal to the true cluster
conditional (i.e., Eq. (4.24)) is apparent.
The deﬁnition of peripheral marginal potentials is more general than the mean ﬁeld messages
deﬁned in the last section, which can be viewed as a special case that applies to cluster-factorizable
potentials. For other non-factorizable potentials, such as tabular potentials, peripheral marginal
potentials are still well deﬁned.
4.4.3
A Generalized Mean Field Algorithm
Eqs. (4.22) and (4.23) are a coupled set of nonlinear equations, which are solved numerically via
asynchronous iteration until a ﬁxed point is reached. This iteration constitutes a simple, message-
passing style, generalized mean ﬁeld algorithm.
GMF ( model: p(xH, xE), partition: {xH,Ci, xE,Ci}I
i=1 )
Initialization
– Randomly initialize the hidden nodes at the border of cluster i, ∀i.
– Initialize f 0
iβ by evaluating the potentials using the current values of the associ-
ated nodes.
– Initialize F 0
i with the current f 0
iβ.
While not converged
For i = 1 : I
– Update qt+1
i
(xH,Ci) = p(xH,Ci|xE,Ci, F t
i ).
– Compute the mean ﬁeld factors f t+1
iβ
of all potential factors at the border of Ci
via local inference using qt+1
i
as in Eq. (4.19).
– Send the f t+1
iβ
messages to all Markov blanket clusters of i by updating the
appropriate elements in their GMFs: Ft
j →Ft+1
j
, ∀j ∈MBCi.
End
Return q(xH) = Q
i qi(xH,Ci), the GMF approximation
164

4.4 Generalized Mean Field Inference
Remark 2 Note that the right-hand side of the mean ﬁeld equation of cluster marginal qi (Eqs. (4.22)
and (4.23)) depends only on a set of cluster marginals that are functions on the Markov blanket vari-
ables of cluster Ci; this set of marginals does not include qi. Thus, the iterative update is a form of
coordinate ascent in the factored model space (i.e., we ﬁx all qj(xH,Cj), j ̸= i and maximize with
respect to qi(xH,Ci) at each step), which will lead to a ﬁxed point. Therefore we have the following
convergence theorem.
Theorem 4 The GMF algorithm is guaranteed to converge to a local optimum, which is a lower
bound for the likelihood of the model (see Remark 2 for a proof sketch).
Theorem 4 is an important consequence of the use of a disjoint variable partition underlying the
variational approximate distribution. It distinguishes GMF from other variational methods such as
GBP [Yedidia et al., 2001b], or the general case in Wiegerinck’s framework [Wiegerinck, 2000],
in which overlapping variable partitions are used, and which optimize an approximate free energy
function with respect to marginals which must satisfy local constraints.
The complexity of each iteration of GMF is exponential in the tree width of the local networks of
each cluster of variables, since inference is reduced to local operations within each cluster. However,
this also means that a computational advantage can only be obtained if the maximum clique size
of qi is much smaller than that of p, suggesting that an appropriate variable partition which breaks
large cliques is important for the success of GMF, an issue we explore in the next section.
Since GMF is guaranteed to converge to a local optimum, in practice it can be performed in a
stochastic multiple-initialization setting similar to the usual practice in EM, to increase the chance
of ﬁnding a better local optimum.
4.4.4
Experimental Results
Although GMF supports several types of applications, such as ﬁnding bounds on the likelihood or
log-partition function, computation of approximate marginal probabilities, and parameter estima-
tion, in this section we focus solely on the quality of approximate marginals. We have performed
experiments on three canonical models: a nearest neighbor Ising model (IM), a sigmoid network
165

4.4 Generalized Mean Field Inference
(SN), and a factorial HMM (fHMM); and we have compared the performance of GMF using differ-
ent tractable families (speciﬁcally, using variable clusterings of different granularity) with regard to
the accuracy on single-node marginals. To assess the error, we use an L1-based measure
1
PN
i=1 Mi
N
X
i=1
Mi
X
k=1
|p(Xi = k) −q(Xi = k)|,
where N is the total number of variables, and Mi is the number of (discrete) states of the variable
xi. The exact marginals were obtained via the junction tree algorithm. We also compared the
performance with that of the belief propagation (BP) algorithm, especially in cases where BP is
expensive, and examined whether GMF provides a reasonably efﬁcient alternative.
We used randomly generated problems for the IM and SN and real data for the fHMM. For the
ﬁrst two cases, in any given trial we speciﬁed the distribution p(x|θ) by a random choice of the
model parameter θ from a uniform distribution. For models with observable output (i.e., evidence),
observations were sampled from the random model. Details of the sampling are speciﬁed in the
tables presenting the results. For each problem, 50 trials were performed. The fHMM experiment
was performed on models learned from a training data set.
Figure 4.10: Ising model and GMF approximations.
Ising models: We used an 8 × 8 grid with binary nodes. Two different tractable models were
used for the GMF approximation, one based on a clustering of 2 × 2 blocks, the other on 4 × 4
blocks (Fig. 4.10). Results on strongly attractive and repulsive Ising models (which are known to be
difﬁcult for naive MF) are reported in Table 4.1. The rightmost column also shows the mean CPU
time (in seconds).
166

4.4 Generalized Mean Field Inference
Table 4.1: L1 errors on nearest neighbor Ising models. Upper panel: attractive IM (θi0 ∈(−0.25, 0.25), θij ∈(0, 2)); Lower
panel: repulsive IM (θi0 ∈(−0.25, 0.25), θij ∈(−2, 0)).
Algorithm
Mean ± std
Median
Range
time
2 × 2 GMF
0.366±0.054
0.382
[0.276,0.463]
2.0
4 × 4 GMF
0.193±0.103
0.226
[0.004,0.400]
29.4
BP
0.618±0.304
0.663
[0.054,0.995]
17.9
GBP
0.003±0.002
0.002
[0.000,0.005]
166.3
2 × 2 GMF
0.367±0.052
0.383
[0.279,0.449]
1.2
4 × 4 GMF
0.185±0.102
0.161
[0.009,0.418]
22.1
BP
0.351±0.286
0.258
[0.009,0.954]
14.3
GBP
0.003±0.003
0.003
[0.000,0.014]
117.5
As expected, GMF using a clustering with fewer nodes decoupled yields more accurate esti-
mates than a clustering in which more nodes are decoupled, albeit with increased computational
complexity. Overall, the performance of GMF is better than that of BP, especially for the attractive
Ising model. For this particular problem, we also compared to the GBP algorithm, which also de-
ﬁnes beliefs on larger subsets of nodes, with a more elaborate message-passing scheme. We found
that for Ising models, GBP performs signiﬁcantly better than the other methods, but at a cost of
signiﬁcantly longer time to convergence.
Figure 4.11: Sigmoid network and GMF approximations.
Sigmoid belief networks: The two sigmoid networks we studied are composed of three hidden
layers (18 nodes), with and without a fourth observed layer (10 nodes), respectively. We used a row
clustering and a block clustering of nodes as depicted in Figure 4.11 for GMF. Table 4.2 summarizes
the results.
Table 4.2: L1 errors on sigmoid networks (θij ∈(0, 1)). Upper: hidden layers only; Lower: with observation layer.
Algorithm
Mean ± std
Median
Range
time
block GMF
0.013±0.004
0.013
[0.006,0.032]
6.8
row GMF
0.172±0.036
0.175
[0.100,0.244]
0.5
BP
0.273±0.025
0.271
[0.227,0.346]
9.2
block GMF
0.018±0.009
0.014
[0.009,0.038]
8.4
row GMF
0.061±0.021
0.059
[0.023,0.145]
0.7
BP
0.187±0.044
0.189
[0.096,0.312]
139.2
For the network without observations, the block GMF, which retains a signiﬁcant number of
167

4.4 Generalized Mean Field Inference
edges from the original graph, is more accurate by an order of magnitude than the row GMF, which
decouples the original network completely. Interestingly, when a bottom layer of observed nodes is
included in the network, a signiﬁcant improvement in approximation accuracy is seen for the row
GMF, but it still does not surpass the block GMF. The performance of BP is poor on both problems,
and the time complexity scales up signiﬁcantly for the network with the observation layer, because
of the large fan-in associated with the nodes in the bottom layer.
...
...
Figure 4.12: An fHMM and a GMF approximation (illustrative graph; the actual model contains 6 chains and 40 steps).
Factorial HMM: We studied a 6-chain fHMM, with (6-dimensional) linear-Gaussian emis-
sions, ternary hidden state and 40 time steps. The model was trained using the EM algorithm (with
exact inference) on 40 Bach chorales from the UCI Repository [Blake and Merz, 1998]. Inference
was performed with the trained model on another 18 test chorales. GMF approximations were based
on clusterings in which each cluster contains either singletons (i.e., naive mean ﬁeld), one hidden
Markov chain, two chains, or three chains, respectively. The statistics of the L1 errors are presented
in Table 4.3.
Table 4.3: L1 errors on factorial HMM
Algorithm
Mean ± std
Median
Range
time
naive MF
0.254±0.095
0.269
[0.083,0.397]
9.8
1-chain GMF
0.237±0.107
0.233
[0.029,0.392]
14.3
2-chain GMF
0.092±0.081
0.064
[0.019,0.314]
5.6
3-chain GMF
0.118±0.092
0.089
[0.035,0.357]
15.6
BP
0
0
-
106.2
Since the moral graph of an fHMM is a clique tree, BP is exact in this case, but the compu-
tational complexity grows exponentially with the number of chains and the cardinality of the vari-
ables; hence BP cannot scale to large models. Using GMF, we obtain reasonable accuracy, which
in general increases with the granularity of the variable clustering. The 2-chain GMF appears to be
168

4.5 Graph Partition Strategies for GMF Inference
a particularly good granularity of clustering in this case, leading to both better estimation and faster
convergence.
In summary, GMF shows reasonable performance in all three of the canonical models we tested,
and provides a ﬂexible way to trade off accuracy for computation time. It is guaranteed to converge,
and the computational complexity is determined by the treewidth of the subgraph. BP, on the other
hand, may fail to converge. Furthermore, the complexity of computing BP messages is exponential
in the size of the maximal clique in the moralized graph, which makes it very expensive in directed
models with dense local dependencies. However, note that there are multiple ways of decomposing a
graphical model (Fig. 4.13); in all three examples just studied, the clusterings of variables are chosen
manually by examining the graph topology and studying the model semantics, and the choice affects
the approximation quality signiﬁcantly. Can we do this in a more principled way, especially for less
structured graphs? In the following section, we address this problem.
22
y23
y71
y72
y
y74
y75
y76
y61
y62
63
y64
y65
y66
y51
y52
y53
y54
y55
y56
y
y42
y43
y44
y45
y46
41
y32
y33
y34
y35
y
21y
y24
y25
y26
73
y31
y36
y
y11
y12
y13
y14
y15
y16
22
y23
y71
y72
y
y74
y75
y76
y61
y62
y63
y64
y65
y66
y51
y52
y53
y54
y55
y56
y
y42
y43
y44
y45
y46
41
32
y33
y34
y35
C1
y
21
y
y24
y25
y26
73
y31
y36
y11
y12
y13
y14
y15
y16
y
22
y23
y71
y72
y
y74
y75
y76
y61
y62
63
y64
y65
y66
y51
y52
y53
y54
y55
y56
y
y42
y43
y44
y45
y46
41
y32
y33
y34
y35
y
21y
y24
y25
y26
73
y31
y36
y
y11
y12
y13
y14
y15
y16
Figure 4.13: Two possible schemes for partitioning a graph to construct the GMF approximation. Which one is better?
4.5
Graph Partition Strategies for GMF Inference
What are the prospects for fully autonomous algorithms for variational inference in graphical mod-
els? Recent years have seen an increasingly systematic treatment of an increasingly ﬂexible range
169

4.5 Graph Partition Strategies for GMF Inference
of algorithms for variational inference. In particular, the cluster variational framework has provided
a range of algorithms that extend the basic “belief propagation” framework [Yedidia et al., 2001a].
Similarly, general clusters of variables are also allowed in recent treatments of structured mean ﬁeld
algorithms [Wiegerinck, 2000]. Empirical results have shown that both kinds of generalization can
yield more effective algorithms.
While these developments provide much-needed ﬂexibility for the design of effective algo-
rithms, they also raise a new question—how are the clusters to be chosen? Until now, this issue has
generally been left in the hands of the algorithm designer; moreover, the designer has been provided
with little beyond intuition for making these choices. For some graphical model architectures, there
are only a few natural choices, and these can be explored manually. In general, however, we wish
to envisage a general piece of software for variational inference which can be asked to perform in-
ference for an arbitrary graph. In this setting, it is essential to begin to explore automatic methods
for choosing clusters.
In the previous section, we presented a generalized mean ﬁeld algorithm for inference based on
a disjoint clustering of random variables in a graphical model, noting that the assumption of disjoint
clusters leads to a simple and generic set of inference equations that can be easily implemented.
Disjoint clusters have another virtue as well, which is the subject of this section—they open the
door to a role for graph partitioning algorithms in choosing clusters for inference.
There are several intuitions that support a possible role for graph partitioning algorithms in the
autonomous choice of clusters for graphical model inference. The ﬁrst is that minimum cuts are to
be preferred, so that as much as possible of the probabilistic dependence is captured within clus-
ters. It also seems likely that the values of parameters should matter because they often reﬂect the
“coupling strength” of the probabilistic dependences among random variables. Another intuition is
that maximum cuts should be preferred, because in this case the mean ﬁeld acting across a large cut
may have an expectation that is highly concentrated, a situation which corresponds to the basic as-
sumption underlying mean ﬁeld methods [Jordan et al., 1999]. Again, speciﬁc values of parameters
should matter.
170

4.5 Graph Partition Strategies for GMF Inference
In this section we provide a preliminary formal analysis and a thoroughgoing empirical explo-
ration of these issues. We present a theorem that relates the weight of the graph cut to the quality of
the bound of GMF approximation, and study random graphs and a variety of settings of parameter
values. We compare several different kinds of partitioning algorithms empirically. As we will show,
our results turn out to provide rather clear support for a clustering algorithm based on minimal cut,
which is consistent with implications drawn from the formal analysis. These promising results open
up the possibility for a fully autonomous variational inference algorithm for complex models based
on automatic node partitioning of a graphical model and GMF ﬁxed point iterations as illustrated in
the following ﬂowchart in Figure 4.14.
GP
GMF
q(x  )
posterior:
approximate joint
graphical model:
node clustering
p(x  , x  )
H
p(x  , x  )
H
E
E
H
Figure 4.14: Flowchart of a autonomous variational inference algorithm.
4.5.1
Bounds on GMF Approximation
The quality of the GMF approximation depends critically on the choice of variable clustering of the
graphical model. The following is a theorem that formally characterizes this relationship.
Theorem 5 (GMF bound on KL divergence): The Kullback-Leibler divergence from the GMF
approximate joint posterior to the true joint posterior is bounded by the sum of the weights of
potential functions associated with the cross-border cliques, up to some constants intrinsic to the
graphical model:
aW ≤KL(q∥p) ≤bW,
(4.27)
where W = P
Dβ⊆∪Bi θβ and, a and b are constants determined by the potential functions of the
cross-border cliques (but independent of the potentials internal to the clusters.)
A proof of this theorem is provided in Appendix B.2. Theorem 5 provides a clear guideline
for choosing a desirable partitioning of a general graphical model: heuristically, it is desirable
171

4.5 Graph Partition Strategies for GMF Inference
to break cliques associated with small weights while clustering the variables in the graph; more
systematically, we can use a graph partitioning algorithm to seek an optimal decomposition of the
graph underlying the model. In the following, we explore several graph partitioning strategies on
random graphs with pairwise potentials (each clique contains only two variables) to conﬁrm and
exploit Theorem 5 experimentally.
4.5.2
Variable Clustering via Graph Partitioning
A wide variety of graph partitioning algorithms have been explored in recent years in a number of
ﬁelds (e.g., [Goemans and Williamson, 1995; Rendl and Wolkowicz, 1995]). Given our focus on
disjoint clusters in the GMF approach, these algorithms have immediate relevance to the problem
of choosing clusters for inference. In this section, we describe the methods that we have explored.
4.5.2.1
Graph partitioning
Let G(V, E, A) be a weighted undirected graph with node set V = {1, . . . , n}, edge set E and
nonnegative weights aij, for (i, j) ∈E (aij = 0 if there is no edge between node i and j; also aii =
0, ∀i). We refer to the symmetric matrix A = {aij} as the afﬁnity matrix. We equip the space of n×
n matrices with the trace inner product A • B = tr AB; let A ⪰0 denote positive semideﬁniteness
(A ⪰B denotes A −B ⪰0); and let A ≥0 denote elementwise non-negativity of A. The linear
operator Diag(a) forms a diagonal matrix from the vector a, and its adjoint operator diag(A) yields
a vector containing the diagonal elements of A. We denote by ek the vector containing k ones.
Equi-MinCut.
We ﬁrst consider graph partitioning (GP) problems based on minimum cuts. Given
a graph G(V, E, A) as described above, a classical formulation [Rendl and Wolkowicz, 1995] asks
to partition the node set into k disjoint subsets, (C1, . . . , Ck), of speciﬁed sizes m1 ≥m2 ≥. . . ≥
mk, Pk
j=1 mj = n, so as to minimize the total weight of the edges connecting nodes in distinct
subsets of the partition. This is known as the minimum k-cut of G. In this section, we concern
ourselves with the special case of this problem in which all subsets have equal cardinality m, a
172

4.5 Graph Partition Strategies for GMF Inference
problem that we refer to as k equi-MinCut (k-MinC). 4 Equi-MinCut avoids potentially skewed
cuts on highly imbalanced graphs, and leads to a balanced distribution of computational complexity
among clusters.
A k-way node partition can be represented by an indicator matrix X ∈Rn×k with the j-th
column, xj = (x1j x2j . . . xnj)t, being the indicator vector for the set Cj, ∀j:
xij =
 1
:
if i ∈Cj
0
:
if i /∈Cj .
Thus, k-equipartitions of a graph are in one-to-one correspondence with the set
Fk = {X : Xek = en, Xten = mek, xij ∈{0, 1}}.
For each partition X, the total weight of the edges connecting nodes within cluster Ci to nodes in
its complement ¯Ci is equal to 1
2xt
i(D −A)xi, where D = Diag(Aen). As a result, the total weight
of the k-cut is
Ck =
X
i
1
2xt
i(D −A)xi = 1
2trXtLX,
(4.28)
where L ≜D −A is the Laplacian matrix associated with G.
Thus, k equi-MinCut can be modeled as the following integer programming problem
(k-MinC)
MinC∗
k := min{tr XtLX : X ∈Fk}.
Equi-MaxCut.
We may also wish to ﬁnd a k-partition that maximizes the total weight of the cut.
This problem is known as the Max k-Cut in combinatorial optimization. Even without any size
constraint this problem is NP-hard. In this paper, we again concern ourselves with a constrained
version of the problem, in which all subsets have equal cardinality m. Thus we have the following
k equi-MaxCut (k-MaxC) problem
(k-MaxC)
MaxC∗
k
:=
max{tr XtLX : X ∈Fk}
=
X
i
dii −min{tr XtAX : X ∈Fk}.
4In combinatorial optimization, this problem is traditionally referred to as the k-partition problem. It is NP-hard, and
to be distinguished from unconstrained minimum cut, which is not NP-hard.
173

4.5 Graph Partition Strategies for GMF Inference
We see that both k-MaxC and k-MinC are quadratic programs, and the relaxations that we con-
sider will treat them identically. Note that due to the equality in the second line of the above equa-
tion, k-MaxC can be solved in a similar manner to k-MinC, which amounts to using a different
“cost” matrix in the objection function.
Weight matrices.
The design of the afﬁnity matrix has a fundamental impact on the results that
are produced by graph partition algorithms. The naive choice in our case is to simply let aij = 1
when node i and j are connected in a graphical model, and let aij = 0 otherwise. Intuitively, an
equi-MinCut using such an afﬁnity matrix will capture more of the local dependency structure in the
model, while an equi-MaxCut will lead to lower computational cost for inference in each cluster.
One can also partition the graphical model based on coupling strength, i.e., letting aij = θij,
the weight of the pairwise potential, so that an equi-MinCut results in clusters with strong intra-
cluster coupling, whereas an equi-MaxCut produces a clustering with only weak couplings left in
each cluster.
It also seems sensible to consider weighting schemes that favor large cuts with small coupling
strength, or small cuts and large coupling strengths. We explore such a scheme by choosing weights
that are inversely related to coupling strength.
The following table summarizes the various partition strategies explored in this paper, and the
corresponding design of the afﬁnity matrix.
Table 4.4: Graph partition schemes
GP
scheme
k-
MinCa
k-
MinCb
k-
MinCc
k-
MaxCa
k-
MaxCb
k-
MaxCc
aij
value
{1, 0}
{θij, 0}
{ 1
θij , 0}
{1, 0}
{θij, 0}
{ 1
θij , 0}
4.5.2.2
Semi-deﬁnite relaxation of GP
Both k equi-MinCut and k equi-MaxCut are NP-hard. But there exist a variety of heuristics for
ﬁnding approximate solutions to these problems [Frieze and Jerrum, 1995; Karisch and Rendl,
1998]. some applicable to quite large graphs [Falkner et al., 1994]. In the sequel, we describe
174

4.5 Graph Partition Strategies for GMF Inference
an algorithm that ﬁnds an approximate solution to k-MinC and k-MaxC using a semideﬁnite
programming (SDP) relaxation [Karisch and Rendl, 1998].
Semideﬁnite programming.
Semideﬁnite programming (SDP) refers to the problem of optimiz-
ing a convex function over the convex cone of symmetric and positive semideﬁnite matrices, subject
to linear equality constraints [Vandenberghe and Boyd, 1996]. A canonical (primal) SDP takes the
form:
(SDP)





min
C • X
s.t.
Ai • X = bi
for i = 1, . . . , m
X ⪰0
Because of the convexity of the objective function and the feasible space, every SDP problem has
a single global optimum. With the development of efﬁcient, general-purpose SDP solvers based
on interior-point methods (e.g., SeDuMi [Sturm, 1999]), SDP has become a powerful tool in solv-
ing difﬁcult combinatorial optimization problems. Here, we describe a simple SDP relaxation for
solving graph partitioning problems.
SDP relaxation of GP.
We now derive a semideﬁnite relaxation for GP. For simplicity, we illus-
trate it only for k-MinC; k-MaxC follows similarly with the appropriate change to the objective.
The ﬁrst step in SDP relaxation involves replacing XtLX with tr LY , where Y is equal to
XXt; this linearizes the objective. Let us deﬁne the set Tk:
Tk := {Y : ∃X ∈Fk such that Y = XXt}.
Thus k-MinC reads:
MinC∗
k := min{tr LY : Y ∈conv(Tk)}.
Note that due to linearization of the objective, our feasible set can be rewritten as the convex hull
of the original set Tk. The next step is to approximate the convex hull of Tk by outer approximations
that can be handled efﬁciently. Karisch and Rendl [1998] describe a nested sequence of outer
approximations for GP that leads from the well-known eigenvalue bound of Donath and Hoffman
to increasingly accurate bounds. Omitting details, one of their relaxation schemes results in the
following SDP relaxation for k-MinC:
175

4.5 Graph Partition Strategies for GMF Inference
(P)













max
1
2tr LY
s.t.
diag(Y ) = en
Y en = men
Y ≥0
elementwise
Y ⪰0, Y = Y t
(P) is an SDP and can be solved by an interior-point solver such as SeDuMi.
4.5.2.3
Finding a closest feasible solution
While in some cases a bound is the major goal of a relaxation, in our case we require that the
relaxation give us a (feasible) solution. In particular, the optimal solution of problem (P) is in
general not feasible for the original GP problem, and we need to recover from the approximate
solution a closest feasible solution, X, to the original GP problem. We use the following scheme in
this section.
• From the relaxed solution Y , ﬁnd a decomposition Y = X′X′t via SVD (note that X′ is usually full
rank rather than of rank k as in the feasible case).
• Treat each row in X′ as a point in Rn; cluster these points using a variant of the standard K-means
algorithm that ﬁnds equi-size clusters. (We use multiple restarts and pick the result with the best cut
value).
• Complete the feasible index matrix X: xij = 1 iff row i of X′ gets assigned to cluster j.
This rounding scheme is related to the randomized projection heuristic studied by Goemans and
Williamson [1995] in their work on Max-Cut. In this approach, the label (-1 or +1) of each vector
is chosen according to whether the vector is above or below a randomly chosen hyperplane passing
through the origin. Frieze and Jerrum [1995] generalized this scheme to max k-cut. Rendl and
Wolkowicz [1995] proposed another alternative involving a ﬁrst-order Taylor expansion of the cost
function around the relaxed X′. However, these schemes make it difﬁcult to enforce size constraints
on the clusters, and occasionally produce artifacts such as having an empty cluster. Empirically, we
have found that a K-means heuristic usually leads to superior and often near-optimal results.
176

4.5 Graph Partition Strategies for GMF Inference
4.5.3
Experimental Results
In this section, we combine graph partitioning with the GMF algorithm to perform inference on
randomly generated undirected graphical models with singleton and pairwise potentials. We an-
alyze three aspects of the overall procedure—the quality of the graph partition, the accuracy of
the approximate marginal probabilities, and the tightness of the lower bounds on the log partition
function.
For each trial, we use a random graph of 24 nodes 5 and specify the distribution p(x|θ) by
making a random choice of the model parameter θ from a uniform distribution U(a, b). For single
node weights θi, we set a = −wobs and b = −wobs. For pairwise weights θij, we set a = −wcoup,
b = 0 for repulsive coupling; a = −wcoup, b = wcoup for mixed coupling; and a = 0, b = wcoup for
attractive coupling, respectively.
Table 4.5: GP performance. (default: K-means rounding; rp: random projection rounding)
Equi-MinCut
Equi-MaxCut
k
lower-b
feas. X
f/b
upper-b
feas. X
f/b
feas. X (rp)
f/b (rp)
3
34
38
1.10±0.03
78
75
0.96±0.02
71
0.91±0.04
4
41
45
1.09±0.02
82
80
0.97±0.02
74
0.90±0.04
6
52
55
1.06±0.03
83
81
0.97±0.01
77
0.93±0.02
8
59
61
1.03±0.02
83
82
0.99±0.01
79
0.95±0.02
3
73
77
1.05±0.02
122
119
0.97±0.01
113
0.92±0.03
4
86
90
1.05±0.02
135
130
0.97±0.01
122
0.91±0.03
6
104
207
1.03±0.01
140
137
0.98±0.01
128
0.91±0.02
8
116
118
1.02±0.01
140
138
0.99±0.01
131
0.93±0.01
4.5.3.1
Partitioning random graphs
Our graphs are generated by sampling an edge with probability p for each pair of nodes. Table 4.5
summarizes the performance (over 100 trials) of various graph partition schemes on random graphs.
To assess performance, we compute the ratio f/b between the feasible cut that was found and the
bound on the optimal cut provided by the SDP relaxation (the optimal solution must fall between
f and b). In the top panel, we show results for partitioning unweighted graphs with p = 0.3 into
5In fact, a standard SDP solver can readily handle larger graphs (e.g., with more than 100 nodes). But the exact
solutions of the singleton marginals for larger graphs are very expensive to compute, which makes it difﬁcult to obtain
good estimates of the inference error.
177

4.5 Graph Partition Strategies for GMF Inference
k = 3, 4, 6, and 8 clusters. The bottom panel shows results for partitioning denser unweighted
graphs with p = 0.5. Partitioning on weighted graphs show similar performance.
3
4
6
8
0.8
1
1.2
1.4
1.6
x 10
−3Repulsive coupling (0.25,0.12)
3
4
6
8
0.08
0.09
0.1
0.11
0.12
Attractive coupling (0.25,0.25)
3
4
6
8
0.23
0.24
0.25
Attractive coupling (0.25,0.5)
3
4
6
8
0.23
0.24
Attractive coupling (0.25,1)
3
4
6
8
1
1.5
2
2.5
x 10
−3 Attractive coupling (0.25,0.12)
3
4
6
8
0.24
0.25
0.26
0.27
0.28
0.29
Repulsive coupling (0.25,1)
3
4
6
8
4
6
8
10
12
14
x 10
−4
Mixed coupling (0.25,0.12)
3
4
6
8
1.5
2
2.5
3
3.5
4
4.5
5
x 10
−3
Mixed coupling (0.25,0.25)
3
4
6
8
0.05
0.1
0.15
Repulsive coupling (0.25,0.5)
3
4
6
8
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Mixed coupling (0.25,0.5)
3
4
6
8
0.18
0.2
0.22
0.24
0.26
0.28
0.3
Mixed coupling (0.25,1)
3
4
6
8
4
5
6
7
8
9
10
x 10
−3 Repulsive coupling (0.25,0.25)
Figure 4.15: L1 errors of singleton marginals on random graphs, with different coupling types and strengths. Each
experiment is based on 20 trials. The sampling ranges of the model parameters for each set of trials are speciﬁed on
top of each graph as (wobs, wcoup). (x-axis: the number of clusters; y-axis: the ℓ1 error; solid lines: cut based on
θij-weighted A; dashed lines: cut based on unweighted A; dashed-dot lines: cut based on 1/θij-weighted A; lines with
diamond symbols: equi-MaxCut (black); lines with round-dot symbols: equi-MinCut (blue); dotted line with square
symbols: random cut (red). For reference, the dotted (red) line with no symbol marks the baseline error of naive mean
ﬁeld.)
We see that the SDP-based GP provides very good and stable partitioning results, usually no
worse than 10% off the optimal cut values, and often within 5%. Note also that the K-means
rounding scheme outperforms the random projection rounding (rp).
4.5.3.2
Single-node marginals
We compared the performance of GMF using different graph partition schemes with regard to the
accuracy on single-node marginals. We used all six GP strategies summarized in Table 4.4, as well
as a random clustering scheme. To assess the error, we use an L1-based measure as described in the
178

4.5 Graph Partition Strategies for GMF Inference
last section. The exact marginals are obtained via exhaustive enumeration. We used graphs of two
different densities in our experiments: moderately connected graphs, with treewidth 12 ± 1, more
than an order of magnitude greater than the largest cluster to be formed; and densely connected
graphs, with treewidth 16 ± 1. For simplicity, we show only results for the moderately connected
graphs.
Figure 4.15 shows that for all variable clusterings, GMF almost always improves over the naive
mean ﬁeld. As expected, equi-MinCut always provides better results than other partition strate-
gies. In particular, equi-MinCut based on coupling strength yields the best results (consistent with
Theorem 5), followed by equi-MinCut based on node degree, then equi-MinCut that cuts the least
number of heavy edges. This suggests that, to better approximate the true marginals, it is important
to capture strong couplings within clusters. Equi-MaxCut fares less well; indeed, it is worse than
a random cut in most cases. It is worth noting, however, that cutting lightweight edges (i.e., max-
imizing the sum of
1
θij across clusters) leads to better performance than degree- or coupling-based
cuts.
Not surprisingly, the performance of GMF improves as the size of the clusters increase, which
allows more dependencies to be captured within each cluster.
For denser graphs (results not shown), the performance gap between different clustering schemes
becomes smaller, but the trend and the relative order remain the same.
4.5.3.3
Bounds on the log partition function
Figure 4.16 shows the lower bounds on the log partition functions given by the GMF approxima-
tions. Comparing to Figure 4.15, we see that there is a good correspondence between the perfor-
mance on approximating marginals and the tightness of the lower bound, a reassuring result in the
context of mean-ﬁeld algorithms.
In summary, our empirical results provide rather clear support for a weighted version of MinCut
as a useful clustering algorithm for GMF inference, which is consistent with the implications from
179

4.5 Graph Partition Strategies for GMF Inference
3
4
6
8
0.988
0.99
0.992
0.994
0.996
0.998
Attractive coupling (0.25,0.12)
3
4
6
8
0.955
0.96
0.965
0.97
0.975
0.98
0.985
Attractive coupling (0.25,0.25)
3
4
6
8
0.974
0.976
0.978
0.98
0.982
Attractive coupling (0.25,0.5)
3
4
6
8
0.98
0.982
0.984
0.986
0.988
0.99
Attractive coupling (0.25,1)
3
4
6
8
0.99
0.992
0.994
0.996
0.998
Repulsive coupling (0.25,0.12)
3
4
6
8
0.96
0.965
0.97
0.975
0.98
0.985
0.99
Repulsive coupling (0.25,0.25)
3
4
6
8
0.94
0.945
0.95
0.955
0.96
Repulsive coupling (0.25,1)
3
4
6
8
0.992
0.994
0.996
0.998
1
Mixed coupling (0.25,0.12)
3
4
6
8
0.975
0.98
0.985
0.99
0.995
Mixed coupling (0.25,0.25)
3
4
6
8
0.9
0.93
0.96
Mixed coupling (0.25,1)
3
4
6
8
0.88
0.9
0.92
0.94
0.96
Repulsive coupling (0.25,0.5)
3
4
6
8
0.92
0.94
0.96
0.98
Mixed coupling (0.25,0.5)
Figure 4.16: Accuracy of the lower bound on the log partition function. The ordering of the panels and the legends are the
same as in Fig 4.15, except that the y-axis now corresponds to the ratio of the lower bound of the log partition function
due to GMF versus the true log partition function.
180

4.6 Extensions of GMF
the formal analysis. This combination of graph partitioning algorithms with the generalized mean
ﬁeld inference algorithm manifests a promising prototype for an autonomous variational inference
algorithm for arbitrary graphical models, optimizing variational approximations over the space of
model parameters as well as over the choice of tractable families used for the variational approxi-
mation, and making it possible to perform distributed approximate inference on large-scale network
models arising from challenging problems in ﬁelds such as systems biology and sensor networks.
4.6
Extensions of GMF
In light of the foregoing exposition, there are a number of extensions of the research reported here
that potentially lead to further improved GMF approximation.
4.6.1
Higher Order Mean Field Approximation
One possible extension involves the use of higher-order expansions in the basic variational bounds.
Leisink and Kappen [2001] have shown how to upgrade ﬁrst-order variational bounds such as that
shown in Eq. (4.7) to yield higher-order bounds. In particular, the following third-order lower bound
can be obtained for the likelihood:
p(xE) ≥
Z
dx exp

−E′(xH)
	h
1 −∆+ 1
2 exp(ξ)∆2i
,
where ξ = 1
3⟨∆3⟩/⟨∆2⟩, ∆= E(xH, xE) −E′(xH), and ⟨·⟩denotes expectation over the approx-
imate distribution q(xH) = exp{−E′(xH)}. The optimizer of this lower bound cannot be found
analytically. However, we can compute the gradient of the lower bound with respect to E′
i (assuming
a cluster-factorized approximate distribution), which requires computation of up to third-order cu-
mulants of the nodes in the bordering cliques in the subgraph. Leisink and Kappen [2001] reported
an application of such a strategy to the 2-D lattice model and sigmoid belief network, approximated
by a completely disconnected subgraph, and reported signiﬁcantly improved bounds. In the GMF
setting, which uses an approximating subgraph with more structure, the computation of the gradient
is even simpler because fewer nodes are involved in the cumulant calculation.
181

4.6 Extensions of GMF
4.6.2
Alternative Tractable Subgraphs
Another possible extension is to replace the disjoint clustering with a tree-connected clustering.
The term W in the GMF bound can be also viewed as the total weight of the disrupted cliques (with
respect to the original graph) in the subgraph underlying a GMF approximation. Thus, we may
further reduce W by departing from the completely disjoint clustering to tree-connected clusters,
in which we connect all the disjoint clusters resulting from a graph partition using a tree whose
nodes are clusters. The link between every pair of connected clusters is chosen to be the maxi-
mally weighted clique shared by the clusters. Such a tree can be easily obtained by constructing a
maximal spanning tree of variable clusters. The motivation of using tree-connected clusters rather
than arbitrary subgraphs to approximate the true joint distribution is that under such a subgraph, the
message-passing-based GMF algorithm described earlier is still guaranteed to converge and yield a
set of globally consistent approximate cluster marginals.
4.6.3
Alternative Graph Partitioning Schemes
Eq. (B.5) in the appendix suggests that it may be advantageous to use other weighting schemes,
such as the entropy-like clique weights (expected potentials) ⟨φβ⟩q, and seek a partition that min-
imizes the sum of expected cross-border potentials. Obviously, exact computation of the entropy-
like weights requires the true joint distribution, and is thus infeasible. We may approximate the
expected potential of each clique by replacing the true marginal distribution of the variables in
the corresponding clique with a naive mean-ﬁeld-like approximation to the marginal: q(xDβ) ∝
exp{θβφβ(xDβ)|Fβ} where F denotes mean ﬁeld messages from neighboring cliques; this turns
the computation of the expectation into a local computation. It is possible to use an algorithm that
iterates between GMF (to update the marginal q(·)) and GP (to update the partition). It would be
also interesting to look at unequal partitions in MinCut, which allows modularities of the graph
structure to be explored in a more ﬂexible way (e.g., as we do in the following for the LOGOS
model).
182

4.7 Application to the LOGOS Model
4.7
Application to the LOGOS Model
The generalized mean ﬁeld theorem makes it straightforward to obtain the ﬁxed-point equations
of the variational approximation to a variety of probability distributions of practical interest. All
that is needed is to decide on a subgraph and a variable clustering, to identify the Markov blanket
of each cluster, and to plug in the mean ﬁelds of the Markov blanket variables according to Eqs.
(4.23) (or more generally, the marginal potentials of the peripheral cliques of each cluster). As
pointed out in Remark 1, since all the original intra-cluster dependencies are preserved in the mean
ﬁeld cluster marginal, probabilistic inference in the GMF approximate distribution is reduced to
local and modular operations within each cluster. Hence, the overall inference problem is fully
decomposed based on the variable clustering.
Figure 4.17: The modular structure of the LOGOS motif model.
For the LOGOS model developed in Chapter 2 for N sequences containing K types of mo-
tifs, the modularity of the model structure naturally suggests a bipartite variable clustering: a mo-
tif cluster {S
(k)
l , θ
(k)
l
| k = 1, . . . , K, l = 1, . . . , Lk}, and a sequence cluster {Y
(n)
t
, X
(n)
t
| n =
1, . . . , N, t = 1, . . . , Tn} (Fig. 4.17). Applying Corollary 3, we obtain the following GMF cluster
marginals:
qs(x) ∝
N
Y
n=1
p(x(n)|υ, Υ)p(y(n)|x(n), {⟨ln θ(k)⟩qm}K
k=1, θbg),
(4.29)
qm(θ, s) =
K
Y
k=1
p(s(k)|ν, Ω)p(θ(k)|s(k), α, {⟨h(k)(x, y)⟩qs}K
k=1)
(4.30)
183

4.7 Application to the LOGOS Model
where ⟨h(k)(x, y)⟩qs is the expectation of the sufﬁcient statistics for motif k determined from DNA
sequence set y by state sequences x (i.e., the count matrix of nucleotides of all sequence sites that are
of motif k as speciﬁed by x); and ⟨ln θ(k)⟩qm is the posterior means of the logarithms of the position-
speciﬁc multinomial parameters of the motif k (often referred to as the natural parameters of the
multinomial distribution). Note that qs(x) is now just a re-parameterized HMM, and qm(θ, s) is a
re-parameterized HMDM model. Inference in both submodels is straightforward and inexpensive.
For simplicity, again we omit the super(sub)scripts k and n in the following expositions, and give
equations for a generic motif type or a generic sequence.
4.7.1
A GMF Algorithm for Bayesian Inference in LOGOS
Due to the isomorphism of GMF approximations of the cluster marginals to the original local and
global submodels of LOGOS (Eqs. (4.29∼4.30)), variational Bayesian inference on LOGOS can
be “divided and conquered” into coupled local inferences on: 1) the isolated local alignment model,
i.e., an HMDM, as if we had “observations”, ¯h = ⟨h(x, y)⟩qs, to obtain the posterior distribu-
tion of the PWM of each motif; and 2) the isolated global distribution model, i.e., an HMM,
as if the position-speciﬁc multinomial parameters of the motifs, in the natural parameter form
¯φ(θ) = ⟨ln θ⟩qm were given, to compute the posterior probabilities of motif locations. This gives
rise to the following EM-like ﬁxed-point iteration procedure (referred to as a “variational EM” al-
gorithm in [Ghahramani and Beal, 2001], although strictly speaking the analogy is only procedural
but not mathematical, because GMF is not doing maximal likelihood parameter estimation as in an
EM algorithm but Bayesian estimation.), which is a special case of the GMF algorithm in §4.4.3:
Variational “E” step: Compute the expected sufﬁcient statistics, the count matrix ¯h, via inference
in the global motif distribution model given ¯φ(θ) and sequence y:
¯h =
T−L+1
X
t=1
h(yt:t+L−1)p(Xt = 1|y, ¯φ),
(4.31)
where p(Xt = 1|y) is the posterior probability of the indicator at position t being the motif-
start state, which can be computed using the forward-backward algorithm. (See Appendix A.3
184

4.7 Application to the LOGOS Model
for details.)
Variational “M” step: Compute the posterior mean of the natural parameter, ¯φ(θ), via inference
in the local motif alignment model given ¯h:
¯φ(θl,j) =
Z
θ
X
sl
ln θl,jp(θl|sl, α, ¯h)p(sl|¯h)dθl
=
I
X
i=1
p(Sl = i|¯h)
 Ψ(αi,j + ¯hl,j) −Ψ(|αi| + |¯hl|)

,
(4.32)
where Ψ(x) is the digamma function, and p(Sl = i|¯h) is the posterior probability of hidden
state q given ’observation’ ¯h, which can be computed using the standard forward-backward
algorithm of HMM. (See Appendix A.4 for details.)
According to Theorem 4, this message-passing procedure will converge. Once it converges, we
can compute the MAP estimate of motif locations in the global HMM submodel and the Bayesian
estimate of the motif PWMs from the local HMDM submodels.
The generalized mean ﬁeld theory provides a divide-and-conquer computational tool to work
with complex models, especially for those coming from a modular design using the graphical model
formalism. It provides computational support for an upgrade path toward more sophisticated mod-
els, which may be needed for improving motif detection. For example, the global distribution model
is completely open to user design and can be made highly sophisticated to model complex properties
of multiple motifs without complicating the inference in the local alignment model. Similarly, the
local motif alignment model can also be more expressive without interfering with the motif distri-
bution model. In the literature, Bayesian inference in large scale models are usually approached via
a Monte Carlo sampling algorithm. In Chapter 5 we describe a collapsed Gibbs sampling procedure
for Bayesian inference on LOGOS. Following is an illustration of the convergence behavior of the
GMF algorithm on LOGOS and an empirical comparison of the GMF algorithm and the Gibbs
sampling algorithm on LOGOS for motif detection tasks of modest difﬁculty.
185

4.7 Application to the LOGOS Model
4.7.2
Experimental Results
We use semi-realistic test datasets described before, each containing 20 artiﬁcially generated DNA
sequences (500-600 bp long) harboring one real motif or three different real motifs (of length 18,
22, and 26 bases, respectively). The performance of inference is evaluated based on the error rate
((false positive + false negative)/2) of predicted motif occurrences.
4.7.2.1
Convergence behavior of GMF
Since the GMF algorithm is only guaranteed to convergence to a local minimum, we run GMF
with 50 random restarts, each followed by ﬁxed-point (FP) iterations until convergence. To obtain
a “convergence curve” of a full run of multiply restarted GMF, we sequentialize the output of all
rounds of FP iterations. After each single cross-update step in each single round of FP-iteration, we
record the lowest value of the free energy, ⟨E⟩q + Hq, achieved so far (since the ﬁrst round of FP
iteration), and compute the empirical error rates of motif prediction made from the GMF posterior
q corresponding to the current lowest free energy, which gives a performance trace.
0
2
4
6
x 10
4
−1600
−1400
−1200
−1000
−800
CPU time
− (free energy)
Convergence of GVI
GMF
0
20
40
60
−2000
−1750
−1500
−1250
−1000
−750
FP−iteration step
− (free energy)
Convergence of FP−iteration
(a)
(b)
Figure 4.18: (a) Convergence of a single round of FP-iteration of GMF (Each point represents one step of iteration.) (b)
The “convergence curve” of GMF with 50 random re-starts. (The solid line is the mean value over 10 independent runs,
and the dashed line represents the std.)
Figure 4.18 illustrates the convergence behavior of GMF on a motif detection task. Typically,
a single round of GMF takes about 30 ∼60 iterations to converge (Fig 4.18a). GMF with multiple
random restarts in general plateaus within less then 50 restarts (Fig 4.18b), suggesting a possibility
186

4.7 Application to the LOGOS Model
of reaching global (or near global) optimality. Thus, in the following experiments, we perform GMF
with 50 random restarts and pick the one resulting in the lowest free energy for the given sequences
as the ﬁnal result.
4.7.2.2
A comparison of GMF and the Gibbs sampler for motif inference
We compared the performance of motif inference on the LOGOS model using GMF and a Gibbs
sampler (see §5.2). Convergence of the Gibbs sampler is diagnosed based on the standard Gelman-
Rubin (GR) statistics [Gelman, 1998]. We infer motif locations using the sample means of X during
Gibbs sampling, which yield an on-line measure of the performance.
Table 4.6: Median hit-rate of motif detection in test set containing one genuine and one decoy motif.
abf1
gal4
gcn4
gcr1
mat
mcb
mig1
crp
GMF
0.81
0.82
0.71
0.65
0
0.
0.73
0.58
Gibbs
0.71
0.79
0.65
0.53
0.75
0
0.91
0.63
Table 4.6 summarizes the results obtained via GMF and Gibbs sampling for motif detection in
a simple one-per-sequence setting using HMDM as the local model (see §2.4.4). The performances
of the two algorithms are largely comparable, with the Gibbs sampler slightly better. However, the
convergence time for the Gibbs sampler is signiﬁcantly longer, typically 5 to 10 times that of GMF.
0
2
4
6
x 10
4
0
0.2
0.4
0.6
0.8
1
CPU time
error rate
Performance of GSVI
GMF
0
1
2
3
4
x 10
5
0
0.2
0.4
0.6
0.8
1
CPU time
error rate
Performance of Gibbs sampler
(a)
(b)
Figure 4.19: Comparison of GMF and Gibbs sampler on performance (in terms of predictive error rate vs. time). Note
the difference in the scale of the x-axis in the two plots.
For more difﬁcult problems, such as simultaneous detection of multiple motifs in a large dataset,
the mixing time of the Gibbs sampler becomes prohibitively long, and the results obtained within
187

4.8 Conclusions and Discussions
a tolerable time span from a Gibbs sampler are not comparable to those of the GMF, which uses
far less time. Figure 4.19 illustrates the convergence curve, in terms of predictive error rate versus
time, for GMF and the Gibbs sampler (obtained from the same experiment from which we plotted
the convergence curve in Fig. 4.18). As evident in Fig 4.19a, the error rate of motif detection using
GMF generally follows an improving trend consistent with that of the free energy in Fig. 4.18b,
although not exactly monotonically decreasing, which is not surprising since the generative model
described by LOGOS does not necessarily model the motif sequences exactly (thus some local
optima may yield slightly better predictions than others). The error curve of the Gibbs sampler,
on the other hand, is less stable (Fig 4.19b), showing that the sampling process explores the state
space in a non-deterministic fashion, therefore providing less reliable performance in ﬁnite time.
The choice of random seeds seems to affect convergence quality for both GMF and Gibbs.
Table 4.7: Performance (mean error rate) of GMF and Gibbs over 5 test datasets.
dataset
1
2
3
4
5
GMF
0.27±0.17
0.26±0.13
0.38±0.18
0.35±0.18
0.39±0.17
Gibbs
0.49±0.19
0.41±0.23
0.56±0.20
0.41±0.23
0.49±0.21
Table 4.7 summarizes the the performances of GMF and the Gibbs sampler over 5 different test
datasets for simultaneous detection of three motifs (as described in §2.6.1). GMF outperforms the
Gibbs sampler (run with ﬁnite allowable time, i.e. 10× the time for GMF) in all cases. We reason
that for a complex motif model such as LOGOS, the state space is likely to be highly multi-modal
and poorly connected, and thus tends to trap the Gibbs sampler at sub-optimality; whereas GMF can
explore such a space much more efﬁciently by doing more random restarts than a Gibbs sampler
can afford, and is guaranteed to reach a local minimum from each restart.
4.8
Conclusions and Discussions
We have presented a generalized mean ﬁeld approach to probabilistic inference in graphical models,
in which a complex probability distribution is approximated via a distribution that factorizes over
a disjoint partition of the graph. Locally optimal variational approximations are obtained via an
algorithm that performs coordinate ascent on a lower bound of the log-likelihood, with guaranteed
188

4.8 Conclusions and Discussions
convergence. For a broad family of models in practical use, we showed that the GMF approxima-
tions of the cluster marginals are isomorphic to the original model in the sense that they inherit all
of its intra-cluster independence structure. Moreover, these marginals are independent of the rest of
the model given the expected potential factors (mean ﬁelds) of the Markov blanket of the cluster.
The explicit and generic formulation of the “mean ﬁelds” in terms of the Markov blanket of vari-
able clusters also leads to a simple, generic message-passing algorithm for complex models. This
result is somewhat surprising as it shows that we can do approximate inference for arbitrary subsets
of hidden variables locally and tractably by capturing all the dependences external to the variable
subset with an expected Markov blanket, and applying existing inference algorithms locally (i.e., on
the cluster marginal) as a subroutine.
Disjoint clusterings have also been used in sampling algorithms to improve mixing rates for
large problems. For example, the Swendsen-Wang algorithm [1987] samples the Ising (or Potts)
model at critical temperatures by grouping neighboring nodes with the same spin value, thereby
forming random clusters (of coupled spins) that are effectively independent of each other, allowing
an MCMC process to collectively sample the spin of each cluster independently and at random. This
method often dramatically speeds up the mixing of the MCMC chain. [Gilks et al., 1996] also noted
that when variables are highly correlated in the stationary distribution, blocking highly correlated
components into higher-dimensional components may improve mixing. However, in the sampling
framework, clusterings are usually obtained dynamically, based on the coupling strength rather than
the topology of the network.
We have also investigated combinations of graph partitioning algorithms with the generalized
mean ﬁeld algorithm, which allows mean ﬁeld approximations to be optimized over both parameter
space and variable partition space in an autonomous fashion. We proved that the quality of the GMF
approximation is bounded by the total absolute weight of the potentials of the disrupted cliques due
to the disjoint variable clustering. Empirically, we conﬁrmed that although all graphs partitions
lead to improvement over a naive mean ﬁeld approximation, a minimal cut equipartition clearly
yields the best GMF approximation, measured both by singleton marginals and lower bounds of
189

4.8 Conclusions and Discussions
the true log partition function. Moreover, there is a good association between the qualities of the
approximate marginals and lower bounds.
Our work represents an initial foray into the problem of choosing clusters for cluster-based
variational methods. There is clearly much more to explore. First, we should note that we are
far from the ideal approach, where we base the clustering criterion on the ultimate goal—that of
obtaining accurate estimates of marginal probabilities. This is of course an ambitious goal to aim for,
and in the near term it seems advisable to attempt to ﬁnd effective surrogates. In particular, we do not
want the problem of choosing clusters to be as computationally complex as the inference problem
that we wish to solve! (Fortunately, many efﬁcient solvers are available to solve the GP problem
nearly optimally via SDP or spectral relaxation.) We should consider surrogates that involve more
general combinations of parameter values along cuts. In particular, we found little support for the
use of maximum cuts in our experiments, but perhaps if we search for large cuts along which the
parameter values are uniformly small we will have more success in this regard. In general, we might
ask for a surrogate that aims to capture both the setting under which mean ﬁeld approximations are
effective, and the setting under which important local dependencies can be treated tractably.
Note also that we have focused on partitioning methods that decompose a large graphical model
into clusters of equal size. With no prior knowledge of the local connectivity within the clusters,
this equal-size heuristic seems reasonable; we wish to distribute resources roughly equally to each
cluster (e.g., to balance the load in a parallel computing setting). Again, however, it would be useful
to explore surrogates that attempt to capture local connectivity in the clustering criterion.
In an exemplary biological motif detection problem involving Bayesian inference in a hybrid,
large-scale graphical model, GMF outperforms conventional Gibbs sampling methods in both con-
vergence speed and error rate. We believe that due to its ﬂexibility and efﬁciency, GMF simpliﬁes
the application of variational methods to general probabilistic inference, and can signiﬁcantly in-
crease the expressive power of languages that can be considered “practical” for knowledge repre-
sentation and reasoning under uncertainty.
190

Chapter 5
Probabilistic Inference II: Monte Carlo
Algorithms
Monte Carlo algorithms are based on the fact that while it may not be feasible to perform statis-
tical computations on a complex joint or posterior distribution, say, p(x), it may be possible to
obtain samples from p(x), or from a closely related distribution, such that marginals and other
expectations can be approximated using sample-based averages. In contrast to the variational in-
ference approaches discussion in the previous chapter, which seek deterministic approximations to
p(x), Monte Carlo algorithms yield a stochastic representation of p(x) that is asymptotically ex-
act and easy to apply. General-purpose Monte Carlo inference software such as the BUGS system
has been developed for use with a general-purpose statistical modeling language (see [Gilks et al.,
1996]). For some statistical models, such as the Dirichlet process mixture model for haplotypes and
non-parametric Bayesian models in general, although the variational approach has been vigorously
pursued [Blei and Jordan, 2004], so far Monte Carlo algorithms are still the only practical approach
to yield reliable performance.
5.1
A Brief Overview of Monte Carlo Methods
We discuss two examples of Monte Carlo algorithms—Gibbs sampling and the Metropolis-Hastings
algorithm—that are commonly used in the graphical model setting and in particular within the
Bayesian paradigm.
191

5.1 A Brief Overview of Monte Carlo Methods
Gibbs sampling is an example of a Markov chain Monte Carlo (MCMC) algorithm. In an
MCMC algorithm, samples are obtained via a Markov chain whose stationary distribution is the
desired p(x) (typically a complex multivariate distribution). The state of the Markov chain is an
assignment of a value to each of the variables. After a suitable “burn-in” period so that the chain
approaches its stationary distribution, these states are used as samples.
The Markov chain for the Gibbs sampler is constructed in the following way: 1) at each step
one of the variables Xi is selected (at random or according to some ﬁxed sequence); 2) the condi-
tional distribution p(xi|xV\i) is computed (recall that V is the set of indices of all the variables in
a graphical model G(V, E)); 3), a value xi is sampled from this distribution; and 4) the sampled xi
replaces the previous value of the ith variable.
The Markov properties of graphical models are particularly useful for a Gibbs sampler: condi-
tioning on the so-called Markov blanket of a given node renders the node independent of all other
variables. Therefore, p(xi|xV\i) = p(xi|xMBi). Thus, the implementation of Gibbs sampling re-
duces to the computation of the conditional distributions of individual variables given their Markov
blankets. For graphical models, these conditionals take the following form:
p(xi|xV\i)
=
p(xi|xMBi)
=
Q
α∈MBKi φα(xDα)
P
xi
Q
α∈MBKi φα(xDα)
(5.1)
where MBKi denotes the set of cliques containing Xi and its Markov blanket nodes (note the
difference of MBKi and MBCi deﬁned in Chapter 4). The set MBKi is often much smaller than
the set D of all cliques of G, and in such cases each step of the Gibbs sampler can be implemented
efﬁciently. Indeed the computation of the conditionals often takes the form of a simple message-
passing algorithm that is reminiscent of the junction tree algorithm or the GMF algorithm.
When the computation in Eq. (5.1) is overly complex, the Metropolis-Hastings algorithm can
provide an effective alternative. Metropolis-Hastings is an MCMC algorithm that is not based on
conditional probabilities, and thus does not require normalization as in Eq. (5.1). Given the current
state x of the algorithm, Metropolis-Hastings chooses a new state x∗from a “proposal distribution”
192

5.2 A Gibbs Sampling Algorithm for LOGOS
q(x∗|x), which often simply involves picking a variable Xi at random and choosing a new value for
that variable, again at random. The algorithm then computes the “acceptance probability”:
ξ = min

1, q(x|x∗)
q(x∗|x)
Q
α∈A φα(x∗
Dα)
Q
α∈A φα(xDα)

.
(5.2)
With probability ξ the algorithm accepts the proposal and moves to x∗, and with probability 1 −ξ
the algorithm remains in state x. For graphical models, if only one of the variables (say Xi) is
resampled, this computation also turns out to often take the form of a simple message-passing
algorithm, of which samples of the Markov blanket of Xi can be regarded as the message.
The principal advantages of Monte Carlo algorithms are their simplicity of implementation and
their generality. Under weak conditions, the algorithms are guaranteed to converge. A problem with
the Monte Carlo approach, however, is that convergence times can be long (e.g., see §4.7.2), and it
can be difﬁcult to diagnose convergence.
5.2
A Gibbs Sampling Algorithm for LOGOS
In the last chapter, we described a GMF algorithm for variational Bayesian inference for de novo
motif detection under the LOGOS model, which deterministically approximates the posterior distri-
bution of motif locations and the Bayesian estimates (resulted from an integration over the posterior
distribution) of PWMs. Here we present a Gibbs sampling algorithm for the same tasks. A compar-
ison of its performance to that of the GMF algorithm was given in §4.7.2.
5.2.1
The Collapsed Gibbs Sampler
Given a set of DNA sequences denoted by y = {y(n)}N
n=1, where y(n) = (y
(n)
1 , . . . , y
(n)
Tn ), a
Gibbs sampler periodically samples the state conﬁgurations of latent variables from variable sets
X = {X(n)}N
n=1, Θ = {θ(k)}K
k=1 and S = {S(k)}K
k=1, one at a time, conditioning on the state
conﬁgurations of the rest of the variables sampled during the previous iterations. Again, for sim-
plicity we drop in the sequel the superscript n associated with variables X, Y , and the superscript
k associated with variables θ, S. The predictive distributions to be derived for sampling apply to
every sequence and motif.
193

5.2 A Gibbs Sampling Algorithm for LOGOS
In principal, a standard data augmentation (DA) [Tanner and Wong, 1987] approach can be
used to solve the Bayesian missing data problem for motif detection under LOGOS. But the fact
that θ is a high-dimensional continuous variable implies that it is very expensive to approximate its
posterior distribution by samples and also, the Markov chain that generates these samples can mix
very slowly. As pointed out in Liu [1994], the conjugacy between p(θ, s) and p(x, y|θ) suggests
that we can integrate out θ and derive a collapsed Gibbs sampling scheme. Essentially, we sample
iteratively from only two sets of discrete variables in the Markov chain: the Dirichlet component
indicator sequence S = (S1, . . . , SL) in the local HMDM model, and the motif location indicator
sequence X = (X1, . . . , XT) in the global HMM model.
Let l denote an arbitrary state taken by Xt (i.e, column l of a motif whose index is omitted)
and, and h denote the sufﬁcient statistics of the PWM θ (i.e., the nucleotide count matrix of the
aligned instances of each motif). For convenience, we use the subscript [−t] to denote an index set
excluding the tth element for variables, or an indication of the source (i.e., all but the tth element)
from which a sufﬁcient statistic is collected. To keep the exposition simple, in the sequel we focus
on a Bayesian treatment of the PWMs only, and let the transition probability matrices {Ωi,j} and
{Υi,j} in the global and local model, respectively, be constant. A Bayesian treatment of these
parameters (e.g., Ω) was discussed in §3.2, and can be similarly implemented in the collapsed Gibbs
sampler. Given the current states of all (discrete) variables in the LOGOS model except Xt, the
Bayesian conditional predictive distribution for Xt is:
p(Xt = l|x[−t], s, y)
=
p(Xt = l|xt−1, xt+1, yt, h[−t], s)
=
1
Z p(Xt = l|xt−1, xt+1)p(yt|Xt = l, h[−t], sl)
=
1
Z Υxt−1→lΥl→xt+1
Γ(|αsl|)
Q
j∈N Γ(αsl,j)
Z Y
j∈N
θ
αsl,j+h[−t],l,j+δ(yt,j)−1
j
dθj
=
1
Z Υxt−1→lΥl→xt+1
Γ(|αsl|)
Q
j∈N Γ(αsl,j) ·
Q
j∈N Γ(αsl,j + h[−t],l,j + δ(yt, j))
Γ(|αsl + h[−t],l + 1|)
=
1
Z Υxt−1→lΥl→xt+1
Γ(|αsl|)
Q
j∈N Γ(αsl,j) ·
Q
j∈N Γ(αsl,j + hl,j)
Γ(|αsl + hl|)
,
(5.3)
194

5.2 A Gibbs Sampling Algorithm for LOGOS
where hl represents the count vector of column l of a motif (whose index is omitted) resulting
from the current assignment of x[−t] plus the count induced by xt, and Υα→β denotes the transition
probability from state α to β.
Given the current states of all variables except Sl, the Bayesian conditional predictive distribu-
tion of variable Sl is:
p(Sl = i|s[−l], x, y)
=
p(Sl = i|sl−1, sl+1, hl)
=
1
Z p(Sl = i|sl−1, sl+1)p(hl|sl = i)
=
1
Z Ωsl−1→iΩi→sl+1
Γ(|αi|)
Q
j∈N Γ(αi,j)
Z Y
j∈N
θαi,j+hl,j−1
j
dθj
=
1
Z Ωsl−1→iΩi→sl+1
Γ(|αi|)
Q
j∈N Γ(αi,j) ·
Q
j∈N Γ(αi,j + hl,j)
Γ(|αi + hl|)
(5.4)
A full sweep of variables Xn results in a new set of labellings of motif/background in a DNA se-
quence and requires O(TKL) operations. The maximal a posteriori estimates of the motif locations
are obtained by summarizing the empirical sample statistics.
5.2.2
Convergence Diagnosis
Since motifs are short stochastic substring patterns in a large “sea” of background sequences, the
posterior distribution deﬁned by LOGOS is not only very high-dimensional, but also likely to be
multi-modal due to the possible presence of many genuine or pseudo motif patterns in the sequences.
Such a distribution can cause very slow mixing for the Markov chain, as well difﬁculties in detecting
when the stationary distribution is reached.
0
1000
2000
1
1.5
2
chain 1
0
1000
2000
1
1.5
2
chain 2
0
1000
2000
1
1.5
2
chain 3
0
1000
2000
1
1.5
2
chain 4
0
1000
2000
1
1.5
2
chain 5
Figure 5.1: Multiple runs of Gibbs sampling, as traced by the column-average entropy of ¯ℏ.
195

5.3 Markov Chain Monte Carlo for Haplotype Inference
To increase the chance of proper mixing, M independent runs of sampling, with different
random seeds, are simultaneously performed (Fig. 5.1). Convergence can be monitored at run-
time using an on-line minimal pairwise Gelman-Rubin (GR) statistics [Gelman, 1998] of some
scalar summaries of the model parameters obtained in each Markov chain. For LOGOS, two
scalar summaries of the model parameters are used: 1) the posterior means of all the count ma-
trices ¯h 1; 2) the column-average entropy of the column-normalized h (denoted by ¯ℏ): Ent(¯ℏ) =
(PK
k=1
PLk
l=1 H(¯ℏ
(k)
l ))/ PK
k=1 Lk, as suggested in [Lawrence et al., 1993]. In the ﬁrst case, 4 ×
PK
k=1 Lk values (i.e., elements of ¯h) need to be monitored, and the minimal GR statistic (which
is a matrix {GR(¯hij)}, containing the GR statistics of all the elements of ¯ℏ’s of a pair of chains)
is computed as the GR statistic that has the minimal Frobenius norm (among all pairs of MCMC
chains). To diagnosis convergence, we act conservatively by monitoring the maximum element in
this minimal GR statistic matrix. For the second strategy, we just compute the GR statistics of the
scalar summary Ent(¯ℏ) for all possible pairs of MCMC chains. For both cases, we stop when the
minimum among all pairwise GR statistics reaches 1 + ϵ, where ϵ is set to be a small scaler (e.g.
0.05). The rationale underlying this approach is that it is unlikely for identical suboptimal conver-
gence to be reached by several independent MCMC chains before the optimum solution is found
once.
A comparison of this Gibbs sampler with the GMF algorithm on motif detection was presented
in §4.7.
5.3
Markov Chain Monte Carlo for Haplotype Inference
In this section, we describe a Gibbs sampling algorithm for exploring the posterior distribution
under the Dirichlet process mixture model for haplotypes, including the latent ancestral pool. We
also present a Metropolis-Hastings variant of this algorithm that appears to mix better in practice.
We follow the notations used in Chapter 3 and hereby omit an reiteration of notational details.
1A simple matching heuristic is used to match the count matrices from different chains when different chains number
the motifs differently (e.g., the same set of motifs (1, 2, 3) found in chain 1 may be numbered (3, 1, 2) in another chain).
We use minimum discrepancy amount all permutations to ﬁnd the best matching.
196

5.3 Markov Chain Monte Carlo for Haplotype Inference
5.3.1
A Gibbs Sampling Algorithm
Recall that the Gibbs sampler draws samples of each random variable from a conditional distribution
of that variable given (previously sampled) values of all the remaining variables. The variables
needed in our algorithm are: Cit, the index of the ancestral template of a haplotype instance t of
individual i; A
(k)
j , the allele pattern at the jth locus of the kth ancestral template; Hit,j, the t-th allele
of the SNP at the jth locus of individual i; and Gi,j, the genotype at locus j of individual i (the only
observed variables in the model). All other variables in the model—θ and γ—are integrated out.
The Gibbs sampler thus samples the values of Cit, A
(k)
j
and Hit,j.
Conceptually, the Gibbs sampler alternates between two coupled stages. First, given the current
values of the hidden haplotypes, it samples the cit and subsequently a
(k)
j , which are associated with
the Dirichlet process prior. Second, given the current state of the ancestral pool and the ancestral
template assignment for each individual, it samples the hit,j variables in the basic haplotype model.
In the ﬁrst stage, the conditional distribution of cit is:
p(cit = k |c[−it], h, a)
∝
p(cit = k |c[−it])
Z
p(hit |cit = k, θk, a(k))p(θ(k)|{hi′
t′ : i′
t′ ̸= it, ci′
t′ = k}, a(k))dθ(k)
=
p(cit = k |c[−it])p(hit |a(k), c, h[−it])
=
( n[−it],k
n−1+τ p(hit|a(k), m[−it],k)
if k = ci′
t′ for some i′
t′ ̸= it
τ
n−1+τ
P
a′ p(hit|a′)p(a′)
if k ̸= ci′
t′ for all i′
t′ ̸= it
(5.5)
where [−it] denotes the set of indices excluding it; n[−it],k represents the number of ci′
t′ for i′
t′ ̸= it
that are equal to k; n represents the total number of instances sampled so far; and m[−it],k denotes
the sufﬁcient statistics m associated with all haplotype instances originating from ancestor k, except
hit. This expression is simply Bayes theorem with p(hit|a(k), c, h[−it],) playing the role of the
likelihood and p(cit = k |c[−it]) playing the role of the prior.
The likelihood p(hit|a(k), m[−it],k) is obtained by integrating over the parameter θ(k), as in
Eq. (3.9), up to a normalization constant:
p(hit|a(k), m[−it],k)
∝
R(αh, βh)
Γ(αh + mit,k)Γ(βh + m′
it,k)
Γ(αh + βh + mit,k + m′
it,k)

1
|B| −1
m′
it,k,
(5.6)
197

5.3 Markov Chain Monte Carlo for Haplotype Inference
where mit,k = m[−it],k + P
j I(hit,j = a
(k)
j ) and m′
it,k = m′
[−it],k + P
j I(hit,j ̸= a
(k)
j ), both
functions of hit (note that mit,k + m′
it,k = nJ) 2. It is easy to see that the normalization constant
is the marginal likelihood p(m[−it],k | a(k)), which leads to:
p(hit|a(k), m[−it],k) =
Γ(αh + mit,k)Γ(βh + m′
it,k)
Γ(αh + m[−it],k)Γ(βh + m′
[−it],k) · Γ(αh + βh + (nk −1)J)
Γ(αh + βh + nkJ)

1
|B| −1
J
.
(5.7)
For p(hit|a), the computation is similar, except that the sufﬁcient statistics m[−it],k are now null
(i.e., no previous matches with a newly instantiated ancestor):
p(hit|a)
=
R(αh, βh)Γ(αh + mit)Γ(βh + m′
it)
Γ(αh + βh + J)

1
|B| −1
m′
it,
(5.8)
where mit = P
j I(hj,it = aj) and m′
it = J −mit,k are the relevant sufﬁcient statistics associated
only with haplotype instance hit.
The conditional probability for a newly proposed equivalence class k that is not populated by
any previous samples requires a summation over all possible ancestors: p(hit) = P
a′ p(hit|a′)p(a′).
Since the gamma function does not factorize over loci, computing this summation takes time that is
exponential in the number of loci. To skirt this problem we endow each locus with its own mutation
parameter θ
(k)
j , with all parameters admitting the same prior Beta(αh, βh) 3. This gives rise to a
closed-form formula for the summation and also for the normalization constant in Eq. (5.5). It is
also, arguably, a more accurate reﬂection of reality. Speciﬁcally,
p(hit|a)
=
Y
j
R(αh, βh)
Γ(αh + mit,j)Γ(βh + m′
it,j)
Γ(αh + βh + 1)

1
|B| −1
m′
it,j
=
Y
j

αh
αh + βh
I(hit,j=aj)
βh
(|B| −1)(αh + βh)
I(hit,j̸=aj)
.
(5.9)
2Recall that in §3.3.2 we use the symbol mk to denote the count of matching SNP alleles in those individual haplotypes
associated with ancestor a(k) (and m′
k for those inconsistent with the ancestor a(k)). Here, we use a variant of these
symbols to denote the pair of random counts (as indicated by the additional subscript it) resulting from the original mk
(or m′
k) for individual haplotypes known to associate with a(k) plus a randomly assigned haplotype hit (whose actual
associated ancestor is unknown).
3Note that now we also need to split counts m[−it],k, mit,k and mit into site-speciﬁc counts, m[−it],k,j, mit,k,j and
mit,j, respectively, where j denotes a single SNPs site.
198

5.3 Markov Chain Monte Carlo for Haplotype Inference
Assuming that loci are also independent in the base measure p(a) of the ancestors and that the base
measure is uniform, we have:
X
a
p(hit|a)p(a)
=
Y
j
 X
l∈B
p(aj = l)p(hit,j|aj = l)

=
Y
j
 X
l∈B
1
|B|

αh
αh + βh
I(hit,j=l)
βh
(|B| −1)(αh + βh)
I(hit,j̸=l)!
=
 1
|B|
J
(5.10)
In this case (that each locus has its own mutation parameter), the conditional likelihood com-
puted in Eq. (5.7) is:
p(hit,j|a
(k)
j , m[−it],k,j)
=
Y
j
 αh + m[−it],k,j
αh + βh + nk −1
I(hit,j=a(k)
j
)
βh + m′
[−it],k,j
(|B| −1)(αh + βh + nk −1)
I(hit,j̸=a(k)
j
)
(5.11)
Note that during the sampling of cit, the numerical values of cit are arbitrary, as long as they
index distinct equivalence classes.
Now we need to sample the ancestor template a(k), where k is the newly sampled ancestor index
for cit. When k is not equal to any other existing index ci′
t′, a value for ak needs to be chosen from
p(a|hit), the posterior distribution of A based on the prior p(a) and the single dependent haplotype
hit. On the other hand, if k is an equivalence class populated by previous samples of ci′
t′, we draw
a new value of a(k) from p(a|{hit, : cit = k}). If after a new sample of cit, a template is no longer
associated with any haplotype instance, we remove this template from the pool. The conditional
distribution for this Gibbs step is therefore:
p(a(k)|a(−k), h, c)
=
p(a(k)|{hit, : cit = k})
=
p({hit, : cit = k}|a(k))
P
a p({hit, : cit = k}|a(k) = a)
=
Y
j
p(mk,j|a
(k)
j )
P
l∈B p(mk,j|a
(k)
j
= l).
(5.12)
199

5.3 Markov Chain Monte Carlo for Haplotype Inference
We can sample a
(k)
1 , a
(k)
2 , . . . , sequentially:
p(a
(k)
j
|{hit,j : cit = k}) =



























1
Z p(hit,j|a
(k)
j )
=
 αh
αh+βh
I(hit,j=a(k)
j
) βh
(|B|−1)(αh+βh)
I(hit,j̸=a(k)
j
)
if k is not previously instantiated
1
Z p({hit,j : cit = k}|a
(k)
j )
= 1
Z
Γ(αh+mk,j)Γ(βh+m′
k,j)
Γ(αh+βh+nk)·(|B|−1)
m′
k,j
=
Γ(αh+mk,j)Γ(βh+m′
k,j)/(|B|−1)
m′
k,j
P
l∈B Γ(αh+mk,j(l))Γ(βh+m′
k,j(l))/(|B|−1)
m′
k,j(l)
if k is previously instantiated,
(5.13)
where mk,j (respectively, m′
k,j) is the number of allelic instances originating from ancestor k at
locus j that are identical to (respectively, different from) the ancestor, when the ancestor has the
pattern a
(k)
j ; and mk,j(l) (respectively, m′
k,j(l)) is the value of mk,j (respectively, m′
k,j) when
a
(k)
j
= l. 4
We now proceed to the second sampling stage, in which we sample the haplotypes hit. We
sample each hit,j, for all j, i, t, sequentially according to the following conditional distribution:
p(hit,j|h[−(i,j)], hi¯t,j, c, a, g)
∝
p(gi|hit,j, hi¯t,j, u[−(i,j)])p(hit,j|a
(k)
j , m[−(it,j)],k)
=
Rg
Γ(αg + u)Γ(βg + (u′ + u′′))
Γ(αg + βg + IJ)
[µ1]u′[µ2]u′′ × Rh
Γ(αh + mit,k,j)Γ(βh + m′
it,k,j)
Γ(αh + βh + nk) · (|B| −1)m′
it,k,j ,
(5.14)
where [−(it, j)] denotes the set of indices excluding (it, j) and mit,k,j = m[−(it,j)],k,j + I(hit,j =
a
(k)
j ) (and similarly for the other sufﬁcient statistics). Note that during each sampling step, we do
not have to recompute the Γ(·), because the sufﬁcient statistics are either not going to change (e.g.,
4Note that here the counts mk (and m′
k) vary with different possible conﬁgurations of the ancestor a(k) under given
h, unlike previously in Eqs. (5.6)-(5.11), in which they vary with different possible conﬁgurations of hit under given
a(k).
200

5.3 Markov Chain Monte Carlo for Haplotype Inference
when the newly sampled hit,j is the same as the old sample), or only going to change by one (e.g.,
when the newly sampled hit,j results in a change of the allele). In such cases the new gamma
function can be easily updated from the old one.
5.3.2
A Metropolis-Hasting Sampling Algorithm
Note that for a long list of loci, a p(a) that is uniform over all possible ancestral template patterns
will render the probability of sampling a new ancestor inﬁnitesimal, due to the small value of the
smoothed marginal likelihood of any haplotype pattern hit, as computed from Eq. (5.5). This could
result in slow mixing.
An alternative sampling strategy is to use a partial Gibbs sampling strategy with the following
Metropolis-Hasting updates, which could allow more complex p(a) (e.g., non-factorizable and non-
uniform) to be readily handled. To sample the equivalence class of hit from the target distribution
π(cit) = p(cit|c[−it], h, a) described in Eq. 5.5, consider the following proposal distribution:
q(c∗
it = k|c[−it]) =
(n[−it],k
n−1+τ : if k = ci′
t′ for some i′
t′ ̸= it
τ
n−1+τ : if k ̸= ci′
t′ for all i′
t′ ̸= it
(5.15)
Then we sample a
(c∗
it ) from the prior p(a). For the target distribution p(cit = k|c[−it], h, a), the
proposal factor cancels when computing the acceptance probability ξ 5 , leaving:
ξ(c∗
it, cit) = min
h
1, p(hit|a
c∗
it, c, h[−it])
p(hit|acit, c, h[−it])
i
.
(5.17)
The choice of a more informative p(a) is an open issue. Besides using a uniform prior, one can,
for example, begin with a (small and hence inexpensive) ﬁnite mixture model using EM to roughly
5 The cancellation of the proposal in ξ can be seen from the following steps:
q(cit|c[−it])
q(c∗
it|c[−it])
π(c∗
it)
π(cit)
=
q(cit|c[−it])
q(c∗
it|c[−it])
p(c∗
it|c[−it], h, a)
p(cit|c[−it], h, a)
=
q(cit|c[−it])
q(c∗
it|c[−it])
p(c∗
it|c[−it])p(hit|a
(c∗
it ), c, h[−it])
p(cit|c[−it])p(hit|a(cit ), c, h[−it])
=
p(hit|a
(c∗
it ), c, h[−it])
p(hit|a(cit ), c, h[−it]),
(5.16)
201

5.3 Markov Chain Monte Carlo for Haplotype Inference
ascertain major population haplotypes, and then construct a p(a) by letting the EM-derived popu-
lational haplotypes take a large portion of the probability mass, and leaving some mass uniformly
to all other possible ancestors. Using a non-rigorous heuristic, we sample according to Eq. (5.13).
It can be shown that with this proposal, the acceptance rate deﬁned by Eq. 5 still roughly holds 6.
In practice, we found that the above modiﬁcation to the Gibbs sampling algorithm leads to substan-
tial improvement in efﬁciency for long haplotype lists (even with a uniform base measure for A),
whereas for short lists, the Gibbs sampler remains better due to the high (100%) acceptance rate.
5.3.3
A Sketch of MCMC Strategies for the Pedi-haplotyper model
Recall that the Pedi-haplotyper model is an extension of the basic Dirichlet process mixture hap-
lotype model (i.e., the DP haplotyper model) that incorporates pedigree information for some in-
dividuals in a study population. The MCMC sampling strategy for the Pedi-haplotype model is
similar to the one for the basic DP-haplotyper described above, except that we need to sample a few
more variables newly introduced on top of the DP-haplotyper model, which requires collecting a
few more sufﬁcient statistics for updating the predictive distributions of these variables.
In addition to the sufﬁcient statistics m (for the consistency between the ancestral and individual
haplotypes (i.e., the number of cases of which the ancestral and individual haplotypes agree in a
single sweep during sampling), and u (for the consistency between the individual haplotypes and
genotype (i.e., the number of cases of which the genotype and its corresponding haplotype pair
agree in a single sweep during sampling), needed in the DP-haplotyper model, we need to update
the following sufﬁcient statistics during each sampling step that sweeps all the random variables:
6 To see this, note that now the proposal distribution is: q(cit|c[−it])p(a(cit )|a[−cit ], h, c), and the desired equilibrium
distribution is π(cit, a(cit )) = p(cit, a(cit ) | c[−it], h, a[−cit ]). The Markov transition probability is therefore:
q(cit|c[−it])
q(c∗
it|c[−it])
p(acit |a[−cit ], h, c)
p(ac∗
it |a
[−c∗
it ], h, c)
π(c∗
it, a
[c∗
it ])
π(cit, a[cit ])
=
q(cit|c[−it])
q(c∗
it|c[−it])
p(acit |a[−cit ], h, c)
p(a
c∗
it |a
[−c∗
it ], h, c)
p(c∗
it|c[−it])p(hit|a
[c∗
it ], c, h[−it])
p(cit|c[−it])p(hit|a[cit ], c, h[−it])
p(ac∗
it |a[−c∗
it ], h, c[−it])
p(acit |a[−cit ], h, c[−it])
≈
p(hit|a
c∗
it , c, h[−it])
p(hit|acit , c, h[−it]).
(5.18)
202

5.3 Markov Chain Monte Carlo for Haplotype Inference
• w: the sufﬁcient statistics of the transition probability ζ,
wrr′ =
X
t
X
i
X
j
1(sit,j = r)1(sit,j+1 = r′).
If we prefer to model the recombination rates in males and females differently, then we com-
pute wt separately for t = 0 and t = 1.
• v: the sufﬁcient statistics of the single generation inheritance (i.e., non-mutation) rate ϵ,
v =
X
t
X
r
X
i
X
j
1(hit,j = hπr(it),j)1(sit,j = r).
The ancestral template indicators associated with the founding subjects and the ancestor pool
can be sampled as usual using the Gibbs and/or MH procedures used for the basic Dirichlet process
mixture model for haplotypes. Now we derive the additional predictive distributions needed for
collapsed Gibbs sampling for the Pedi-haplotyper model. For each predictive distribution of the
hidden variables, we integrate out the model parameters given their (conjugate) priors (see §3.3 and
§5.3.1 for deﬁnitions of most of the notations used here).
• To sample a founding haplotype:
p(hit,j|h[−(i,j)], hi¯t,j, s, c, a, g)
=
p(hit,j|hi¯t,j, hλ(i),j, sλ(i),j, acit,j, gi, v[−(i,j)], u[−(i,j)], m[−(i,j)])
∝
p(hit,j, hλ(i),j, gi|hi¯t,j, sλ(i),j, acit,j, v[−(i,j)], u[−(i,j)], m[−(i,j)])
=
p(hλ(i),j|hit,j, hi¯t,j, sλ(i),j, v[−(i,j)])p(gi|hit,j, hi¯t,j, u[−(i,j)])p(hit,j|acit,j, m[−(i,j)])
=
Rm
Γ(αm + v(hit,j))Γ(βm + v′(hit,j))
Γ(αm + βm + v(hit,j) + v′(hit,j)) ×
Rg
Γ(αg + u(hit,j))Γ(βg + u′(hit,j) + u′′(hit,j))
Γ(αm + βm + IJ)
µu′
1 µu′′
2 ×
Rh
Γ(αh + m(hit,j))Γ(βh + m′(hit,j))
Γ(αh + βh + m(hit,j) + m′(hit,j)) · (|B| −1)m′(hit,j) ,
(5.19)
where hλ(i),j refers to the allele in the child of i that is inherited from i. For simplicity, we
suppose only one child. For the case of multiple children, the ﬁrst term of Eq. (5.19) becomes
a product of such terms each corresponding to one child.
203

5.3 Markov Chain Monte Carlo for Haplotype Inference
• To sample a non-founding haplotype:
p(hit,j|h[−(i,j)], hi¯t,j, s, c, a, g)
=
p(hit,j|h[−(i,j)], hi¯t,j, hλ(i),j, hπ(it)0,j, hπt(it),j, sit,j, sλ(i),j, gi, v[−(i,j)], u[−(i,j)])
∝
p(hit,j, hλ(i),j, gi|h[−(i,j)], hi¯t,j, hπ(it)0,j, hπt(it),j, sit,j, sλ(i),j, v[−(i,j)], u[−(i,j)])
=
p(hit,j|hπ(it)0,j, hπ(it)1,j, sit,j, v[−(i,j)])p(hλ(i),j|hit,j, hi¯t,j, sλ(i),j, v[−(i,j)])
p(gi|hit,j, hi¯t,j, u[−(i,j)])
=
Rm
Γ(αm + v(hit,j))Γ(βm + v′(hit,j))
Γ(αm + βm + v(hit,j) + v′(hit,j)) ×
Rg
Γ(αg + u(hit,j))Γ(βg + u′(hit,j) + u′′(hit,j))
Γ(αm + βm + IJ)
µu′
1 µu′′
2 .
(5.20)
• To sample the segregation variable:
p(sit,j|h, s[−(i,j)], si¯t,j, c, a, g)
=
p(sit,j|hit,j, hπ0(it),j, hπ1(it),j, sit,j−1, sit,j+1, v[−(i,j)], w[−(it,j)])
∝
p(hit,j|hπ0(it),j, hπ1(it),j, sit,j, v[−(i,j)])p(sit,j−1|sit,j, w[−(it,j)])
=
p(sit,j|sit,j+1, w[−(it,j)])
=
Rm
Γ(αm + v(sit,j))Γ(βm + v′(sit,j))
Γ(αm + βm + v(sit,j) + v′(hit,j)) ×
Rs
Γ(αs + w00(sit,j) + w11(sit,j))Γ(βs + +w01(sit,j) + w10(sit,j))
Γ(αs + βs + |w|)
,
(5.21)
where |w| = P
r,r′ wr,r′.
5.3.4
Summary
In this section we presented stochastic inference algorithms based on a pure Gibbs sampling scheme
and a variant based on a Metropolis Hasting scheme for haplotype inference under a Dirichlet pro-
cess mixture model—DP haplotyper. We also sketched Pedi-haplotyper, a Gibbs sampler for hap-
lotype inference with pedigree information. We implemented the DP-haplotyper and validated it on
204

5.4 Conclusion
both simulated and real genotype data (see §3.4), and demonstrated superior performance compared
to the state-of-the-art algorithm for haplotype inference. An implementation of the Pedi-haplotyper
Gibbs sampler is deferred to future work.
If desired, we can also use these algorithms as subroutines to compute Bayesian estimates of
model parameters of interest, such as the recombination rate ζ under the Pedi-haplotyper model.
This can be done via a Monte Carlo EM algorithm, where in the E step we sample the hidden
variables using the algorithms just presented, and in the M step we use sufﬁcient statistics from the
samples to estimate the parameter based on sample average. We omit further discussion on this
subject.
5.4
Conclusion
In comparison to the GMF algorithms presented in the previous chapter, modulo time complexity
(for reaching equilibrium) and space complexity (for storing the samples), Monte Carlo methods
are arguably more general and easier to apply for statistical computations (especially Bayesian in-
ference) in a wide range of probabilistic models. Under the graphical model formalism where
the conditional independencies among variables are made explicit, implementing a Markov chain
Monte Carlo algorithm such as a Gibbs sampler is particularly straightforward—the proposal dis-
tribution of each variable reduces to a conditional distribution under the Markov blanket of the
variable, which is easily identiﬁable from the graph topology and can be automated. In this chapter,
we presented MCMC algorithms for the large-scale Bayesian models we developed for motif de-
tection and haplotype inference, taking advantage of the simplicity offered by our graphical model
formalism.
In particular, for certain graphical models, such as the non-parametric Bayesian models deﬁned
via a Dirichlet process prior (as for haplotype inference), MCMC algorithms appear to be the only
practical methodology for probabilistic inference. They naturally handle the issue of representing
the densities of a potentially inﬁnite dimensional mixture model via sequentially generated samples
from such a distribution, whereas it seems that a variational approximation (still being developed
205

5.4 Conclusion
by several authors, including the author of this thesis) has to apply an ad hoc predetermined trunca-
tion scheme to represent the approximate density. This reduces the original distribution to a ﬁnite
mixture model [Blei and Jordan, 2004], greatly diminishing the ﬂexibility offered by the original
non-parametric model. Our Gibbs sampling algorithm for the Dirichlet process mixture model for
haplotypes is quite competent in performance, although a comparison to a variational inference
algorithm under the same model would be interesting to reveal any performance/cost trade-off.
Such a comparison was done for the LOGOS model, which belongs to the family of parametric
Bayesian models (and hence is of ﬁxed dimensionality). Despite the simplicity of implementing
the Gibbs sampler for LOGOS, and acceptable performance in small-scale test problems, we found
that the GMF algorithm signiﬁcantly outperforms the Gibbs sampler in more challenging large-scale
problems given ﬁnite time (see §4.7). This suggests that GMF is a competent and efﬁcient alter-
native to Monte Carlo methods for what we believe to be a wide range of large ﬁxed-dimensional
parametric models, especially when the performance/cost trade-off needs to be tilted toward lower-
ing the computational cost without sacriﬁcing signiﬁcantly in performance.
206

Chapter 6
Conclusions
6.1
Conclusions from This Work
In this work, we focused on probabilistic graphical models and algorithms for analyzing two par-
ticular types of genomic data known as gene regulatory sequences and single nucleotide polymor-
phisms. We presented new algorithms to solve the related computational biology tasks of motif
detection and haplotype inference.
In Chapter 2, we re-formulated the conventional unsupervised de novo motif detection problem
in genomic analysis as a semi-unsupervised learning problem, and developed a modular Bayesian
Markovian model called LOGOS, which can be trained on biologically identiﬁed motifs and gener-
alized to novel motif patterns. This model captures various properties of motifs, including canonical
structures of motif families, syntax of motif occurrences, and the distribution of nucleotides in back-
ground sequences. The graphical model formalism enables us to model these aspects with individual
submodels in a divide-and-conquer fashion, and results in a joint model that can be efﬁciently solved
using an approximate inference algorithm based on generalized mean ﬁeld approximation.
Chapter 3 introduces a novel application of the non-parametric Bayesian approach to the hap-
lotype inference problem. Our model extends a conventional ﬁnite mixture model to a potentially
inﬁnite mixture model via a Dirichlet process that induces a prior distribution over the centroids
(i.e., the identities of populational ancestral haplotypes) and the cardinality (i.e., the number of dis-
tinct ancestral haplotypes) of the mixture model. Such an extension is particularly suitable for data
207

6.2 Future Work
with a complex unknown distribution. It provides an alternative approach to the conventional model
selection methods based on a ﬁnite model space, imposes an implicit parsimonious bias on the de-
gree of diversity of haplotypes and allows a model to expand in a statistically consistent fashion
to accommodate increasing data that may have new patterns. Our model also incorporates a like-
lihood factor that naturally handles missing values and statistical errors in the haplotype/genotype
relationship.
Another major technical focus of this thesis is the development of efﬁcient approximate algo-
rithms for probabilistic inference in complex models that are intractable for exact algorithms. In
Chapter 4, we developed a generalized mean ﬁeld theory for approximate probabilistic inference
in complex graphical models using a generic optimization procedure based on graph partitioning
and message passing that provably converges to globally consistent marginals and a lower bound on
the likelihood. This framework generalizes previous works on model-speciﬁc structured variational
approximation yet specializes a previous study suggesting non-disjoint model decompositions, and
appears to strike the right balance between approximation quality and complexity. This work aims to
develop a turnkey algorithm for distributed approximate inference with bounded performance. The
GMF algorithm has been successfully used as the main algorithm for inference and learning in the
LOGOS model and exhibits superior performance compared to its MCMC counterpart. However,
under a non-parametric Bayesian setting, as used for haplotyping, MCMC algorithms developed in
Chapter 5 still appear to be the only practical approach.
6.2
Future Work
6.2.1
Modeling Gene Regulation Networks of Higher Eukaryotes in Light of Systems
Biology and Comparative Genomics
It is widely believed that using diverse sources of related data and modeling them jointly is essential
to gain deep insight into complex biology phenomena. As discussed brieﬂy in §2.8, joint models
comprising aspects of regulatory sequences, gene expression (e.g., microarray data), protein bind-
ing (CHiP data), and phylogenetic information, have begun to emerge and have shown promising
208

6.2 Future Work
potential. We intend to explore extensions of our motif models along these directions under the
LOGOS framework.
In particular, we are interested in studying the gene regulatory networks of higher eukaryotic or-
ganisms under a developmental context that involves temporal-spatial regulation of gene activities.
Note that during the formation of a multicellular system such as an early embryo from a single cell
such as a fertilized egg, each cell in the embryo has the same DNA content, but almost every single
cell has a different function. This is somewhat analogous to a massive heterogeneous parallel sys-
tem bootstrapped from the same program and subsequently differentiated by executing (temporally
and spatially) context-speciﬁc subroutines of the common program. Deciphering the control mech-
anisms underlying such a system is crucial for understanding many biological processes typical of
higher eukaryotes but nonexistent in bacteria or yeast, such as embryogenesis and differentiation,
which are closely related to important biomedical problems such as birth defects and cancer devel-
opment. Due to the high complexity of higher eukaryotic genomes and the technical difﬁculties of
directly proﬁling gene expression patterns in such species, (e.g., conventional approaches such as
cDNA microarrays used in uni-cellular organisms, which reﬂect the average effect of a homoge-
neous cell population, are not sufﬁciently informative), a mere extrapolation of extant techniques
developed on the bacteria/yeast platform is not sufﬁcient. Departing from the LOGOS model, we
plan to develop more accurate and expressive statistical models that facilitate investigations of the
gene regulation networks of higher eukaryotes in light of richer information from systems biology
and comparative genomics. Speciﬁcally, the following extensions are of particular interest:
Richer motif models.
To improve the sensitivity and speciﬁcity of motif prediction in higher eu-
karyotic genomic sequences, it is necessary to upgrade both the local submodel and global submodel
of the current LOGOS model to encode richer regulatory grammar and capture higher-order depen-
dencies among and within the regulatory signals. An immediate extension of the models presented
in this thesis is to replace the 1st-order Markov models over sequence positions with more elab-
orated Bayesian networks to model richer dependencies. Another promising future direction is to
209

6.2 Future Work
combine the generative framework we adopted in this thesis with discriminative models such as con-
ditional random ﬁelds [Lafferty et al., 2001], so that long range interactions of sequence elements
and the inﬂuence of neighborhood statistical properties on motif locations can be comprehensively
integrated in a semi-supervised fashion.
Joint models for temporal-spatial proﬁles of gene expression.
Image proﬁles of in situ hy-
bridization (e.g., see Fig. 2.2) and immuno-staining are standard tools for cell and developmental bi-
ologists to study the whole-body temporal-spatial patterns of gene expression in higher eukaryotes,
and prove to be much more informative than microarray proﬁles of homogenized tissue samples.
Correlating this representation of gene expression with cis-regulatory sequences is an intriguing
open problem, which demands much effort in both computational image analysis and the devel-
opment of appropriate probabilistic models that can interface the image models and the sequence
models.
Joint models for comparative genomics.
It can be highly informative to investigate an organ-
ism in the light of its evolutionary relationship to other organisms. Therefore, comparative studies
of non-protein-coding genomic sequences in several related species can potentially help to improve
motif detection. Along this direction, plausible evolutionary models of motif sequences, and general
methods to address the problem of low-quality alignment of regulatory sequences (compared to that
of gene sequences) during comparative genomic analysis (which critically depends on alignment
quality) are still to come. We intend to develop a joint model that correctly models within-species
and cross-species variations of motif sequences resulting from genomic stochasticity and from spe-
ciation, respectively, in order to infer the compositions and locations of these recurring elements
from either aligned or unaligned genomic sequences of multiple species.
In summary, an integration of heterogeneous biological data via uniﬁed and consistent joint
mathematical models is essential for analyzing biological systems at a much more comprehensive
scale, and will greatly help the pursuit of a predictive understanding of how developmental gene
210

6.2 Future Work
regulatory networks are encoded and evolved.
6.2.2
Genetic Inference and Application Based on Polymorphic Markers
SNPs comprise the largest class of individual differences in genomes, and have become a focus
of research interest because of their value for investigating the genetic and evolutionary basis of
multi-factorial diseases and complex traits. Such investigations require an integration of polymor-
phic molecular markers, such as the SNP markers we studied, with genetic linkage maps, complex
phenotypic traits, pedigrees, etc., under a uniﬁed model. Continuing on our current work on phasing
SNP haplotypes of an iid population, future directions include both theoretical explorations of the
evolutionary mechanisms and dynamics of the populational diversity reﬂected in the haplotypes and
their implications for trait diversiﬁcation and inheritance; and practical upgrades of our current mod-
els into ones that can be used to infer SNP blocks concurrently with phase resolution, to infer haplo-
types under the constraints of partial pedigrees (brieﬂy sketched in §3.6), to infer map-locations of
genetic traits associated with phenotypic patterns, etc. The graphical model framework used in this
thesis makes it straightforward to pursue these future directions by constructing advanced models
using the Dirichlet process mixture model developed in this thesis as a basic building block. For
example, the following extensions are immediately on the horizon:
Bayesian treatment of the scaling parameter in DP.
The scaling parameter τ in the Dirichlet
process controls the prior tendency to instantiate new ancestral haplotypes in a population. Since DP
can be described by a metaphor of non-Darwinian evolution process, τ may indeed reveal certain
aspects of the dynamics of genetic drift and ﬁxation during evolution and hence plays an interesting
role in modeling populational diversity. In Bayesian non-parametrics, it is standard to introduce
an easy-to-handle prior for τ [West et al., 1994; Rasmussen, 2000], which makes it adaptable to
populational diversity, and allows it to be estimated a posteriori.
Hierarchical DP for ethnic-group-speciﬁc populational diversity.
The early split of an ances-
tral population following a populational bottle-neck (e.g., due to sudden migration or environmental
211

6.2 Future Work
changes) may lead to ethnic-group-speciﬁc populational diversity, which features both ancient hap-
lotypes (that have high variability) shared among different ethnic groups, and modern haplotypes
(that are more strictly conserved) uniquely present in different ethnic groups. This structure is anal-
ogous to a hierarchical clustering setting in which different groups comprising multiple clusters
may share clusters with common centroids (e.g., different new topics may share some common key
words). The hierarchical Dirichlet process mixture model developed by Teh et al. [2004] provides
a promising Bayesian approach to model such structure. We are pursuing an extension of our (ﬂat)
DP haplotyper model using this approach.
Linkage analysis.
The degree of correlation between haplotypes of genetic markers (SNPs) and
phenotypic traits (e.g., disease susceptibility, drug response, body features, etc.), formally known
as linkage disequilibrium, reﬂects the frequency of genetic recombinations (hence the physical dis-
tance) between the marker(s) and the potential causal gene(s) of the phenotypic traits on the chro-
mosome, a measure of great medical and clinical value. In principle, a joint model for linkage
analysis and haplotype inference can be obtained by replacing (or extending) the simple genotype
model discussed in this thesis with a more sophisticated phenotype model that comprises 1) a re-
combination submodel, describing the dependencies between the marker and the target gene, e.g.,
via a stochastic process capturing distance-dependent decay of the recombination rate, 2) a pene-
tration submodel, describing the correspondence between the target gene and the phenotypic traits,
and 3) a likelihood submodel, capturing the stochasticity in phenotypic measurements. In practice,
for multi-factorial traits, the problem is complicated by the necessity of modeling complex depen-
dencies between multiple causal genes and their net effects at the phenotype level, which is still an
open-ended problem that calls for advances in modeling and probabilistic inference methodology.
In summary, a long-term goal we intend to pursue along this direction is to build clinical-
grade phasing and mapping software that performs routine genetic diagnosis based on individual or
familial SNP records. Generalizing SNPs to general markers, the model to be developed can also
212

6.2 Future Work
be extended to general pedigree inference, which is applicable to forensic analysis based on genetic
material, a problem also of great interest and practical value. We believe that with the models and
inference algorithms developed in this thesis, technical foundations are in place for developing a
full-scale joint model for statistical genetic inference.
6.2.3
Automated Inference in General Graphical Models
Large-scale probability models, such as the ones we developed in this thesis, have outgrown the
ability of current (and probably future) exact inference algorithms to compute posteriors and learn
parameters. For this reason, development of efﬁcient and broadly applicable approximation algo-
rithms is critical to further progress. The generalized mean ﬁeld theory we developed potentially
opens paths to the implementation of efﬁcient and general-purpose variational inference engines
that are easily scalable and adaptable to a wide range of complex probabilistic models using canoni-
cal computational procedures, which should require little or no work on model-speciﬁc derivations,
and should be capable of answering arbitrary probabilistic queries. To further improve the approxi-
mation quality, we also expect that better tractable families associated with higher-order approxima-
tions or novel model decomposition schemes will need to be explored. Analysis of the relationships
between the structure of the optimization space and the quality of the resulting bounds on approxi-
mation error also deserves further investigation.
To conclude, in order to pursue a predictive understanding of how developmental gene regula-
tory networks are encoded and evolved, and the genetic basis of multi-factorial diseases and complex
traits, thorough understanding of the biological entities under investigation and high-throughput
generation of experimental data must join forces with rigorous quantitative models based on solid
mathematical foundations and algorithms for efﬁcient computation. In particular, we expect that the
exploration of formalisms for data fusion and for modularizing large-scale probabilistic models, and
the development of more powerful inference and learning algorithms scalable to complex models,
213

6.2 Future Work
will be essential to keep up with the rapid pace of biological research, and furthermore will con-
tribute to applications in other science and engineering domains involving predictive understanding
and reasoning under uncertainty.
214

Appendix A
More details on inference and learning
for motif models
A.1
Multinomial Distributions and Dirichlet Priors
To model a categorical random variable Z, which can take J possible discrete values (e.g., all 4
possible nucleotides, A, C, G and T, in a DNA sequence), a standard distribution is the multinomial
distribution: p(Z = j|θ) = θj, |θ| = PJ
j=1 θj = 1, θj > 0, ∀j, where j represents one of the J
possible values. The (column) vector θ = [θ1, . . . , θJ]t is called the multinomial parameter vector 1.
For a set of M i.i.d. samples of Z, z = (z1, . . . , zM) (e.g. a whole column of nucleotides in a multi-
alignment A), the sufﬁcient statistics are the counts of each possible value: hj = PM
m=1 δ(zm, j),
where δ(a, b) = 1 if a = b and 0 otherwise. Under a multinomial distribution, the likelihood of a
single sample zm is:
p(zm|θ) =
J
Y
j=1

θj
δ(zm,j),
(A.1)
and the joint likelihood of the i.i.d. sample set z is:
p(z|θ) =
M
Y
m=1
J
Y
j=1

θj
δ(zm,j) =
J
Y
j=1

θj
hj.
(A.2)
1Note that for simplicity, in this thesis we reuse the symbol θ (and also h and α in the sequel) to denote a single column
vector, whose elements are singly subscripted (e.g. θj); whereas in the main text and the next section, these symbols each
denote a two-dimensional array consisting of a sequence of column vectors, whose elements are consequently doubly
subscripted (e.g., θl,j).
215

A.1 Multinomial Distributions and Dirichlet Priors
To model uncertainty about the multinomial parameters, we can treat θ as a multivariate con-
tinuous random variable, and use a Dirichlet density to deﬁne a prior distribution Dir(α) for θ:
p(θ|α) = C(α)
J
Y
j=1

θj
αj−1,
(A.3)
where the hyperparameters α = [α1, . . . , αJ]t, αj > 0, ∀j are called the Dirichlet parameters, and
C(α) is the normalizing constant which can be computed analytically:
C(α) =
Γ(|α|)
QJ
j=1 Γ(αj)
,
(A.4)
where Γ(·) is the gamma function.
Now we can calculate the joint probability p(θ, z|α):
p(θ, z|α)
=
p(z|θ)p(θ|α)
=
C(α)
J
Y
j=1

θj
αj+hj−1
=
C(α)
C(α + h)Dir(α + h).
(A.5)
Integrating Eq. (A.5) over θ, we obtain the marginal likelihood:
p(z|α)
=
Z
p(θ, z|α)dθ
=
Γ(|α|)
Γ(|α| + |h|)
J
Y
j=1
Γ(αj + hj)
Γ(αj)
=
C(α)
C(α + h).
(A.6)
From Eq. (A.6) we can see that the quantity αj −1 can be thought of as an imaginary count of
the number of times that event (Z = j) has already occurred. Furthermore, we have the posterior
distribution p(θ|z, α) = p(θ, z|α)/p(z|α) = Dir(α + h), which is isomorphic to the prior distribu-
tion, and thus analytically integrable. This isomorphism between the prior and posterior is called
conjugacy and priors of such nature are called conjugate priors.
216

A.2 Estimating Hyper-Parameters in the HMDM Model
A.2
Estimating Hyper-Parameters in the HMDM Model
We can compute the maximum likelihood estimate of the hyper-parameters Θ = {α, υ, Υ} of the
HMDM model from a training dataset of known motifs using an EM algorithm. This approach is
often referred to as empirical Bayes parameter estimation.
Following Sj¨olander et al. [1996], for a given set of multi-alignment matrices {A(1), . . . , A(K)},
where each A(k) represents a multiple alignment of Mk biologically identiﬁed instances of motif k
of length Lk, the likelihood of the count vector h
(k)
l
summarizing the column of aligned nucleotides
at site l of motif k, under the Dirichlet prior αi, is
p(h
(k)
l |αi)
=
Γ(|h
(k)
l | + 1)Γ(|αi|)
Γ(|h
(k)
l | + |αi|)
4
Y
j=1
Γ(h
(k)
l,j + αi,j)
Γ(h
(k)
l,j + 1)Γ(αi,j).
(A.7)
Note that this formula is slightly different from Eq. (A.6) because h
(k)
l
can result from
Γ(|h(k)
l
|+1)
Q4
j=1 Γ(h(k)
l,j +1)
distinct permutations of the Mk nucleotides. Since no particular ordering of the motif instances
in multi-alignment matrices is assumed for the training data, it is more appropriate to model the
probability of the count matrices h resulting from A than that of A itself [Sj¨olander et al., 1996].
Thus, the complete log likelihood of the count matrices h(k) = {h
(k)
1 , . . . , h
(k)
Lk}, ∀k, and the
latent HMDM state sequences s(k) = {s
(k)
1 , . . . , s
(k)
Lk}, ∀k, can be obtained by replacing the A(k)’s
in Eq. (2.17) with h(k)’s, integrating over each θ(k) (which results in a term like Eq. (A.7) for each
count vector), and taking the logarithm of the resulting marginal:
lc({α, υ, Υ})
=
log p(h(1), . . . , h(K), s(k), . . . , s(K)|{α, υ, Υ})
=
log
( K
Y
k=1

p(s
(1)
1 |υ) ·
h Lk−1
Y
l=1
p(s
(k)
l+1|s
(k)
l , Υ)
i
·
h Lk
Y
l=1
p(h
(k)
l |s
(k)
l , α)
i)
=
K
X
k=1
I
X
i=1
δ(s
(k)
1 , i) log υi +
K
X
k=1
Lk−1
X
l=1
I
X
i,i′=1
δ(s
(k)
l , i)δ(s
(k)
l+1, i′) log Υi,i′
+
K
X
k=1
Lk
X
l=1
I
X
i=1
δ(s
(k)
l , i)

log Γ(|h
(k)
l | + 1)Γ(|αi|)
Γ(|h
(k)
l | + |αi|)
+
4
X
j=1
log
Γ(h
(k)
l,j + αi,j)
Γ(h
(k)
l,j + 1)Γ(αi,j)

.
(A.8)
217

A.2 Estimating Hyper-Parameters in the HMDM Model
The EM algorithm is essentially a coordinate ascent procedure that maximizes the expected
complete log likelihood EQ(s)[lc({α, υ, Υ})] (also written as ⟨lc(Θ)⟩Q for simplicity) over the dis-
tribution Q(s) and the parameters Θ = {α, υ, Υ} [Neal and Hinton, 1998]. In the E step, we
seek Q(s) = arg maxQ ⟨lc(Θ)⟩Q, which turns out to be Q(s) = p(s|h, Θ) = Q
k p(s(k)|h(k), Θ).
Thus the E step is equivalent to computing ⟨lc(Θ)⟩p(s|h,Θ), which reduces to replacing the sufﬁcient
statistics dependent on s(k) in Eq. (A.8) with their expectations with respect to p(s(k)|h(k), Θ). In
the M step, we compute Θ = arg maxΘ ⟨lc(Θ)⟩Q. Speciﬁcally, we iterate between the following
two steps until convergence:
E step:
• Compute the posterior probabilities p(s
(k)
l |h(k)) of the hidden states, and the matrix of co-
occurrence probabilities p(s
(k)
l , s
(k)
l+1|h(k)) for each motif k, using the forward-backward al-
gorithm in a hidden Markov model with initial and transition probabilities deﬁned by {υ, Υ}
and emission probabilities deﬁned by p(h
(k)
l |S
(k)
l
= i) = p(h
(k)
l |αi) (i.e., Eq. (A.7)).
M step:
• Baum-Welch update for the HMM parameters {υ, Υ} based on expected sufﬁcient statistics
computed from all the p(s
(k)
l |h(k)) and p(s
(k)
l , s
(k)
l+1|h(k)):
υi
=
P
k,l p(S
(k)
l
= i|h(k))
P
k Lk
(A.9)
Υi,j
=
P
k,l p(S
(k)
l
= i, S
(k)
l+1 = j|h(k))
P
k,l
P
j p(S
(k)
l
= i, S
(k)
l+1 = j|h(k))
(A.10)
• Gradient ascent (one step per M-step) for the Dirichlet parameters: (To force the Dirichlet
parameters to be positive, we reparameterize the Dirichlet parameters as αi,j = ewi,j, ∀i, j,
as described by Sj¨olander et al. [1996].)
wi,j = wi,j + η∂⟨lc(Θ)⟩
∂wi,j
(A.11)
218

A.3 Computing the Expected Sufﬁcient Statistics in the Global HMM
where
∂⟨lc(Θ)⟩
∂wi,j
= ∂⟨lc(Θ)⟩
∂αi,j
∂αi,j
∂wi,j
=
K
X
k=1
Lk
X
l=1
αi,jp(S
(k)
l
= i|h(k))

Ψ(|αi|) −Ψ(|h
(k)
l | + |αi|) + Ψ(h
(k)
l,j + αi,j) −Ψ(αi,j)

,
(recall that Ψ(x) = ∂log Γ(x)
∂x
=
˙Γ(x)
Γ(x) is the digamma function) and η is the learning rate,
usually set to be a small constant.
A.3
Computing the Expected Sufﬁcient Statistics in the Global HMM
We show how to compute the expected sufﬁcient statistics ¯h in a global HMM, in which the emission
parameters are deﬁned by the background distribution and the motif multinomial parameters (or
their estimates).
Note that the overall counting matrix equals the summation of the counting matrices of all
identiﬁed motif instances (each single instance forms a matrix with four rows, one per nucleotide;
each column of such a matrix has only one nonzero element, whose row index corresponds to the
observed nucleotide at the position of the column and the value of this element is equal to 1):
h =
X
t
h(yt:t+L−1)I(Xt:t+L−1 = (1, . . . , L)),
where I(·) is an indicator function matching a sequence of states to a given motif state sequence.
Taking the expectation on both sides with respect to the joint distribution qs(x), we have:
¯h
=
Eqs(x)[h]
=
X
x
qs(x)
T−L+1
X
t=1
h(yt:t+L−1)I(xt:t+L−1 = (1, . . . , L)).
We have to sum over all possible conﬁgurations of X. Under the GMF approximation, qs(x) is
a reparameterized HMM p(x|y, ¯φ(θ), θbg) (Eq. 4.29), which leads to the following simpliﬁcation:
¯h =
T−L+1
X
t=1
h(yt:t+L−1)p(Xt:t+L−1 = (1, . . . , L)|y)
219

A.4 Bayesian Estimation of Multinomial Parameters in the HMDM Model
where
p(Xt:t+L−1 = (1, . . . , L)|y) =
QL−1
l=1 p(Xt+l = l + 1|Xt+l−1 = l)α(Xt = 1)β(Xt+L−1 = L) QL−1
l=1 p(yt+l|Xt+l = l + 1)
p(y)
,
where α(xt) ≜p(y1, . . . , yt, xt) and β(xt) ≜p(yt+1, . . . , yT |xt) are the two standard intermediate
probabilistic terms computed in the forward-backward algorithm for HMMs. With a little algebra
and using the assumption that for the global HMM state transitions within a motif are deterministic,
it is easy to show that
p(Xt:t+L−1 = (1, . . . , L)|y) = α(Xt = 1)β(Xt = 1)
p(y)
= p(Xt = 1|y),
which means that the posterior probability of a subsequence of states being a motif state sequence is
just the posterior probability of the ﬁrst indicator in the sequence being the motif-start state, which
is surprisingly simple. Now,
¯h =
T−L+1
X
t=1
h(yt:t+L−1)p(Xt = 1|y),
(A.12)
where p(Xt = 1|y) can be computed using the forward-backward algorithm. The time complexity
of this inference is linear in the length of the sequence, and quadratic in the number of motif states.
Since all within-motif state transitions are deterministic, careful bookkeeping during implementa-
tion can reduce the complexity to quadratic in the number of motif types, that is, O(K2T). For
multiple input sequences, the overall expected counting matrix ¯h is just the sum of the expected
counting matrices computed from each sequence using Eq. (A.12).
A.4
Bayesian Estimation of Multinomial Parameters in the HMDM
Model
We now show how to compute the Bayesian estimate of φ(θ), the natural parameter of the multino-
mial distribution, in an HMDM model given the expected sufﬁcient statistics ¯h.
First, we compute the posterior probability of a hidden state sequence s given ¯h. Plugging ¯h
220

A.4 Bayesian Estimation of Multinomial Parameters in the HMDM Model
into Eq. (2.17) and integrating over θ, we have the marginal probability:
p(¯h, s|α, υ, Υ)
=
p(s1)
L−1
Y
l=1
p(sl+1|sl)
L
Y
l=1
p(¯hl|sl),
(A.13)
which is a standard (local) HMM with emission probability:
p(¯hl|Sl = i)
=
Γ(|αi|)
Γ(|¯hl| + |αi|)
4
Y
j=1
Γ(¯hl,j + αi,j)
Γ(αi,j)
.
(A.14)
With this fully speciﬁed HMM, we can compute the posterior probabilities of the hidden states
p(sl|¯h) and the matrix of co-occurrence probabilities p(sl, sl+1|¯h) using the standard forward-
backward algorithm for HMMs.
Then, the Bayesian estimate of φ(θ) = ln(θ) (in which ln(·) is a componentwise operation) is
computed as follows:
¯φl,j
=
Z
θl
X
sl
ln θl,jp(θl|sl, α, ¯h)p(sl|α, ¯h)dθl
=
X
sl
p(sl|¯h)
Z
θl
ln θl,jp(θl|αl, ¯hl)dθl
=
I
X
i=1
p(Sl = i|¯h)
 Ψ(αi,j + ¯hl,j) −Ψ(|αi| + |¯hl|)

.
(A.15)
221

Appendix B
Proofs
B.1
Theorem 2: GMF approximation
For clarity, we restate the GMF theorem here, with the evidence symbol and hidden variable sub-
scripts omitted. Our subsequent proof starts from this simpliﬁed statement.
Theorem (GMF): For a general undirected probability model p(x) and a clustering C : {XCi}I
i=1,
if all the potential functions that cross cluster borders are cluster-factorizable, then the generalized
mean ﬁeld approximation to p(x) with respect to clustering C is a product of cluster marginals
qGMF(x) = Q
Ci∈C qGMF
i
(xCi) satisfying the following generalized mean ﬁeld equations:
qGMF
i
(xCi)
=
p(xCi|Fi),
∀i.
(B.1)
To prove the GMF theorem we need to use the calculus of variations [Sagan, 1992] to solve
the optimization deﬁned by Eq. (4.21). For convenience, we distinguish two subsets of nodes in a
cluster i, the interior nodes and the border nodes, i.e., letting XCi denote the nodes in cluster Ci, we
have XCi = {YCi, ZCi} where YCi ∩XBi = ∅(i.e., the interior nodes) and ZCi ⊂XBi (i.e., the
border nodes).
Proof. From Eq. (4.21), to ﬁnd the optimizer of:
Z
dyCidzCi exp

−
X
Ci∈C
E′
i(yCi, zCi)
	 1 −∆

,
222

B.1 Theorem 2: GMF approximation
where ∆≡E −P
Ci∈C E′
i + A(θ), subject to the constraints that each E′
i deﬁnes a valid marginal
distribution qi(yCi, zCi) over all hidden variables in cluster i, we solve the Euler equations for a
variational extremum, deﬁned over Lagrangians f(E′
i, xCi) =
R
dx[·\i]

exp{−P
j E′
j}(1 −∆) −
P
j λj exp{−E′
j}

(where x[·\i] refers to all hidden variables excluding those from cluster i):
∂f
∂E′
i
−
d
dxCi
  ∂f
∂˙E′i

= 0
∀i.
(B.2)
Since f does not depend on ˙E′i (=
dE′
i
dxCi ), we have:
Z
dx[·\i]
Y
j̸=i
exp{−E′
j}(E −
X
j
E′
j) −λi = 0
⇒
E′
i
=
Z
dx[·\i]
Y
j̸=i
exp{−E′
j}(E −
X
j̸=i
E′
j) −λi
=
C −
X
Dα⊆Ci
θαφα(yDα) −
X
Dβ∈Bi
θβ

φβ(zCi∩Dβ, {zCj∩Dβ : j ∈Iβi})

qIβi
,
where qj = exp{−E′
j(yCj, zCj)} is the local marginal of cluster j; Iβi denotes index set of the set
of clusters other than Ci that intersect with clique Dβ (i.e., all the clusters neighboring cluster i that
intersect with clique β); and qIβi = Q
j∈Iβi qj is the marginal over cluster set Iβi.
When the potential functions at the cluster boundaries factorize (say, multiplicatively) with
respect to the clustering, we have:
E′
i
=
C −
X
Dα⊆Ci
θαφα(yDα) −
X
Dβ∈Bi
θβFβ(φβi(zCi∩Dβ), {⟨φβj(ZCj∩Dβ)⟩qj : j ∈Iβi}).
=
C −
X
Dα⊆Ci
θαφα(yDα) −
X
Dβ∈Bi
θβφβi(zCi∩Dβ)
Y
j∈Iβi
⟨φβj(ZCj∩Dβ)⟩qj.
So,
qi(yCi, zCi)
=
exp{−E′
i}
=
p(yCi, zCi|{⟨φβj(ZCj∩Dβ)⟩qj}j∈Iβi,Dβ∈Bi)
=
p(xCi|Fi),
∀i.
(B.3)
The presence of evidence xCi,E merely changes Eq. (B.3) to q(xCi) ∝p(xCi, xCi,E|Fi). After
normalization, this leads to q(xCi) = p(xCi|xCi,E, Fi).
223

B.2 Theorem 5: GMF bound on KL divergence
B.2
Theorem 5: GMF bound on KL divergence
Proof.
According to the GMF theorem, the GMF approximation to p(x) is
q(x)
=
Y
i
q(xCi)
=
1
Zq
exp
n X
i
X
Dα⊆Ci
θαφα(xDα) +
X
i
X
Dβ⊆Bi
θβφ′
β(xDβ∩Ci)
o
=
1
Zq
exp
n X
α
θαφα(xDα) −
X
Dβ⊆∪Bi
θβφβ(xDβ) +
X
Dβ⊆∪Bi
kβθβφ′
β(xDβ∩Ci)
o
=
1
Zq
exp
nX
α
θαφα(xDα) +
X
Dβ⊆∪Bi
θβ
 kβφ′
β(xDβ∩Ci)−φβ(xDβ)
o
,
(B.4)
where kβ = |Iβ| is the number of clusters intersecting with clique β (note that for simplicity, we
omit the argument qIβi in the peripheral marginal potentials). Thus, the KL divergence from q to p
is:
KL(q∥p)
=
Z
x
q(x) log q(x)
p(x)dx
=
X
Dβ⊆∪Bi
θβ

kβ

φ′
β(XDβ∩Ci)

q −

φβ(XDβ)

q

−log Zq
Zp
=
X
Dβ⊆∪Bi
θβ(kβ −1)

φβ(XDβ)

q −log Zq + log Zp.
(B.5)
Now, letting φβ,max = maxx φβ(xDβ), and φβ,min = minx φβ(xDβ), we have φβ,min ≤⟨φβ(XDβ)⟩q ≤
φβ,max. Deﬁne aφ = minDβ⊆∪Bi(kβ −1)φβ,min, and bφ = maxDβ⊆∪Bi(kβ −1)φβ,max. Then (since
all the θs are positive by deﬁnition),
aφ
X
Dβ⊆∪Bi
θβ ≤
X
Dβ⊆∪Bi
θβ(kβ −1)

φβ(XDβ)

q ≤bφ
X
Dβ⊆∪Bi
θβ.
(B.6)
224

B.2 Theorem 5: GMF bound on KL divergence
To bound the log partition function, we ﬁnd that
Zq
=
X
x
exp
n X
α
θαφα(xDα)
o
× exp
n
X
Dβ⊆∪Bi
θβ
 kβφ′
β(xDβ∩Ci) −φβ(xDβ)
o
≤
X
x
exp
n X
α
θαφα(xDα)
o
× exp
n
bZ
X
Dβ⊆∪Bi
θβ
o
=
Zp exp
n
bZ
X
Dβ⊆∪Bi
θβ
o
,
(B.7)
where
bZ
=
max
β (kβφβ,max −φβ,min)
=
max
β
 (kβ −1)φβ,max + (φβ,max −φβ,min)

.
(B.8)
Similarly,
Zq
≥
Zp exp
n
aZ
X
Dβ⊆∪Bi
θβ
o
,
(B.9)
where
aZ
=
min
β
 (kβ −1)φβ,min + (φβ,min −φβ,max)

.
(B.10)
Thus,
log Zp + aZ
X
Dβ⊆∪Bi
θβ ≤log Zq ≤log Zp + bZ
X
Dβ⊆∪Bi
θβ.
(B.11)
Putting these together, we have
aW ≤KL(q∥p) ≤bW,
(B.12)
where a = max(0, aφ −bZ) and b = bφ −aZ.
In the special case where kβ = k, for all β (e.g., all potentials are pairwise), aφ −bZ =
(k −1) minβ,β′(φβ,min −φβ′,max) + minβ(φβ,min −φβ,max) ≥k minβ,β′(φβ,min −φβ′,max), and b =
bφ −aZ ≤k maxβ,β′(φβ,max −φβ′,min) ≡k∆φ. Since KL divergence is always nonnegative, we
have
0 ≤KL(q∥p) ≤k∆φW.
(B.13)
225

Bibliography
[Akey et al., 2001] J. Akey, L. Jin, and M. Xiong. Haplotypes vs single marker linkage disequilib-
rium tests: what do we gain? Eur J Hum Genet, 9:291–300, 2001.
[Alberts et al., 2002] B. Alberts, A. Johnson, J. Lewis, M. Raff, K. Roberts, and P. Walter. Molec-
ular Biology of the Cell, 4th Edition. Taylor and Francis, 2002.
[Anderson and Novembre, 2003] E. C. Anderson and J. Novembre.
Finding haplotype block
boundaries by using the minimum-description-length principle. Am J Hum Genet, 73:336–354,
2003.
[Attias, 2000] H. Attias. A variational Bayesian framework for graphical models. In Advances in
Neural Information Processing Systems 12, 2000.
[Bailey and Elkan, 1994] T. L. Bailey and C. Elkan. Fitting a mixture model by expectation maxi-
mization to discover motifs in biopolymers. In Proc. of the 2nd International Conf. on Intelligent
Systems for Molecular Biology, 1994.
[Bailey and Elkan, 1995a] T. L. Bailey and C. Elkan. Unsupervised learning of multiple motifs in
biopolymers using EM. Machine Learning, 21:51–80, 1995.
[Bailey and Elkan, 1995b] T. L. Bailey and C. Elkan. The value of prior knowledge in discovering
motifs with MEME. In Proc. of the 3rd International Conf. on Intelligent Systems for Molecular
Biology, 1995.
226

BIBLIOGRAPHY
BIBLIOGRAPHY
[Barash et al., 2003] Y. Barash, G. Elidan, N. Friedman, and T. Kaplan. Modeling dependencies in
protein-DNA binding sites. In Proc. of the 7th International Conf. on Research in Computational
Molecular Biology, 2003.
[Beal et al., 2001] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The inﬁnite hidden Markov
model. In Advances in Neural Information Processing Systems 13, 2001.
[Benos et al., 2002] P. V. Benos, A. S. Lapedes, and G. D. Stormo. Is there a code for protein-DNA
recognition? Probab(ilistical)ly? Bioassays, 24(5):466–475, 2002.
[Berman et al., 2002] B. P. Berman, Y. Nibu, B. D. Pfeiffer, P. Tomancak, S. E. Celniker, M. Levine,
G. M. Rubin, and M. B. Eisen. Exploiting transcription factor binding site clustering to identify
cis-regulatory modules involved in pattern formation in the Drosophila genome. Proc. Natl.
Acad. Sci. USA, 99(2):757–762, 2002.
[Bernardo and Smith, 1994] J. M. Bernardo and A. F. M. Smith. Bayesian Theory. John Wiley,
New York, 1994.
[Bishop and Winn, 2003] C. M. Bishop and J. Winn. Structured variational distributions in VIBES.
In Proceedings Artiﬁcial Intelligence and Statistics, 2003.
[Bishop et al., 2003] C. M. Bishop, D. Spiegelhalter, and J. Winn. VIBES: A variational inference
engine for Bayesian networks. In Advances in Neural Information Processing Systems 15, 2003.
[Blackwood and Kadonaga, 1998] E. M. Blackwood and J. T. Kadonaga. Going the distance: A
current view of enhancer action. Science, 281(5373):60–63, 1998.
[Blake and Merz, 1998] C.L. Blake and C.J. Merz. UCI repository of machine learning databases,
1998.
[Blanchette and Tompa, 2003] M. Blanchette and M. Tompa. FootPrinter: A program designed for
phylogenetic footprinting. Nucleic Acids Research, 31 (13):3840–3842, 2003.
227

BIBLIOGRAPHY
BIBLIOGRAPHY
[Blanchette et al., 2002] M. Blanchette, B. Schwikowski, and M. Tompa. Algorithms for phyloge-
netic footprinting. J Comput Biol, 9 (2):211–223, 2002.
[Blei and Jordan, 2004] D. Blei and M. I Jordan. Variational methods for the Dirichlet process. In
Proceedings of the 21st International Conference on Machine Learning, 2004.
[Brudno et al., 2003] M. Brudno, G. B. Do, G. M. Cooper, M. F. Kim, E. Davydov, E. D. Green,
A. Sidow, and S. Batzoglou. LAGAN and multi-LAGAN: Efﬁcient tools for large-scale multiple
alignment of genomic DNA. Genome Res, 13 (4):721–731, 2003.
[Burge and Karlin, 1997] C. Burge and S. Karlin. Prediction of complete gene structure in human
genomic DNA. J. Mol. Biol, 268:78–94, 1997.
[Bussemaker et al., 2000] H. Bussemaker, H. Li, and E. Siggia. Building a dictionary for genomes:
Identiﬁcation of presumptive regulatory sites by statistical analysis. Proc. Natl. Acad. Sci. USA,
97, 2000.
[Bussemaker et al., 2001] H. J. Bussemaker, H. Li, and E. D. Siggia. Regulatory element detection
using correlation with expression. Nat Genet., 27(2):167–171, 2001.
[Cardon and Stormo, 1992] L. R. Cardon and G. Stormo. Expectation maximization algorithm for
identifying protein-binding sites with variable lengths from unaligned DNA fragments. J Mol
Biol., 223 (1):159–70, 1992.
[Chakravarti, 2001] A. Chakravarti. Single nucleotide polymorphisms: . . .to a future of genetic
medicine. Nature, 409:822–823, 2001.
[Chiang et al., 2003] D. Y. Chiang, A. M. Moses, M. Kellis, E. S. Lander, and M. B. Eisen. Phylo-
genetically and spatially conserved word pairs associated with gene-expression changes in yeasts.
Genome Res, 4 (7):R43, 2003.
[Clark et al., 1998] A. Clark, K. M. Weiss, D. A. Nickerson, S. L. Taylor, A. Buchanan, J. Stengard,
V. Salomaa, E. Vartiainen, M. Perola, E. Boerwinkle, and C. F. Sing. Haplotype structure and
228

BIBLIOGRAPHY
BIBLIOGRAPHY
population genetic inferences from nucleotide-sequence variation in human lipoprotein lipase.
American Journal of Human Genetics, 63:595–612, 1998.
[Clark, 1990] A. Clark. Inferences of haplotypes from PCR-ampliﬁed samples of diploid popula-
tions. Mol. Biol. Evol, 7:111–122, 1990.
[Clark, 2003] A. Clark. Finding genes underlying risk of complex disease by linkage disequilib-
rium mapping. Curr Opin Genet Dev, 13(3):296–302, 2003.
[Cowell et al., 1999] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. Proba-
bilistic Networks and Expert Systems. Springer, 1999.
[Crick, 1970] F. Crick. Central dogma of molecular biology. Nature, 227(258):561–3, 1970.
[Daly et al., 2001] M. J. Daly, J. D. Rioux, S. F. Schaffner, T. J. Hudson, and E. S. Lander. High-
resolution haplotype structure in the human genome. Nature Genetics, 29(2):229–232, 2001.
[Davidson, 2001] E. H. Davidson. Genomic Regulatory Systems. Academic Press, 2001.
[Efron, 1996] B. Efron. Empirical Bayes methods for combining likelihoods (with discussion). J.
Amer. Statist. Assoc., 91:538–565, 1996.
[Eisen, 2003] M. Eisen. Structural properties of transcription factor-DNA interactions and the in-
ference of sequence speciﬁcity. submitted, 2003.
[El-Hay and Friedman, 2001] T. El-Hay and N. Friedman. Incorporating expressive graphical mod-
els in variational approximations: Chain-graphs and hidden variables. In Proceedings of the 17th
Annual Conference on Uncertainty in AI, 2001.
[Escobar and West, 2002] M. D. Escobar and M. West. Bayesian density estimation and inference
using mixtures. Journal of the American Statistical Association, 90:577–588, 2002.
229

BIBLIOGRAPHY
BIBLIOGRAPHY
[Eskin et al., 2003] E. Eskin, E. Halperin, and R.M. Karp. Efﬁcient reconstruction of haplotype
structure via perfect phylogeny. Journal of Bioinformatics and Computational Biology, 1:1–20,
2003.
[Excofﬁer and Slatkin, 1995] L. Excofﬁer and M. Slatkin.
Maximum-likelihood estimation of
molecular haplotype frequencies in a diploid population.
Molecular Biology and Evolution,
12(5):921–7, 1995.
[Falkner et al., 1994] J. Falkner, F. Rendl, and H. Wolkowitz. A computational study of graph
partitioning. Mathematical Programming, 66(2):211–239, 1994.
[Ferguson, 1973] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of
Statistics, 1:209–230, 1973.
[Fine et al., 1998] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden Markov model: Anal-
ysis and applications. Machine Learning, 32:41–62, 1998.
[Frech et al., 1993] K. Frech, G. Herrmann, and T. Werner. Computer-assisted prediction, clas-
siﬁcation, and delimitation of protein binding sites in nucleic acids.
Nucleic Acids Res., 21
(7):1655–1664, 1993.
[Frieze and Jerrum, 1995] A. Frieze and M. Jerrum. Improved approximation algorithms for MAX
k-CUT and MAX BISECTION. In Egon Balas and Jens Clausen, editors, Integer Programming
and Combinatorial Optimization, volume 920, pages 1–13. Springer, 1995.
[Frith et al., 2001] M. C. Frith, U. Hansen, and Z. Weng. Detection of cis-element clusters in higher
eukaryotic DNA. Bioinformatics, 17:878–889, 2001.
[Fujioka et al., 1999] M. Fujioka, Y. Emi-Sarker, G. L. Yusibova, T. Goto, and J. B. Jaynes. Anal-
ysis of an even-skipped rescue transgene reveals both composite and discrete neuronal and early
blastoderm enhancers, and multi-stripe positioning by gap gene repressor gradients. Develop-
ment, 126(11):2527–38, 1999.
230

BIBLIOGRAPHY
BIBLIOGRAPHY
[Gabriel et al., 2002] S. B. Gabriel, S. F. Schaffner, H. Nguyen, et al. The structure of haplotype
blocks in the human genome. Science, 296:2225–2229, 2002.
[Gelman, 1998] A. Gelman. Inference and monitoring convergence. In W. E. Gilks, S. Richardson,
and D. J. Spiegelhalter, editors, Markov Chain Monte Carlo in Practice. Chapman & Hall/CRC,
Boca Raton, Florida, 1998.
[Ghahramani and Beal, 2001] Z. Ghahramani and M.J. Beal. Propagation algorithms for variational
Bayesian learning. In Advances in Neural Information Processing Systems 13, 2001.
[Ghahramani and Jordan, 1997] Z. Ghahramani and M. I. Jordan. Factorial hidden Markov models.
Machine Learning, 29:245–273, 1997.
[Gilbert, 2003] S. F. Gilbert. Developmental Biology, Seventh Edition. Sinauer Associates, 2003.
[Gilks et al., 1996] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain
Monte Carlo in Practice. Chapman and Hall, 1996.
[Goemans and Williamson, 1995] M. X. Goemans and D. P. Williamson. Improved approximation
algorithms for maximum cut and satisﬁability problems using semideﬁnite programming. J.
Assoc. Comput. Mach., 42:1115–1145, 1995.
[Goto et al., 1989] T. Goto, P. Macdonald, and T. Maniatis. Early and late periodic patterns of even
skipped expression are controlled by distinct regulatory elements that respond to different spatial
cues. Cell, 57(3):413–22, 1989.
[Greenspan and Geiger, 2003] D. Greenspan and D. Geiger. Model-based inference of haplotype
block variation. In Proceedings of RECOMB 2003, 2003.
[GuhaThakurta and Stormo, 2001] D. GuhaThakurta and G. D. Stormo. Identifying target sites for
cooperatively binding factors. Bioinform., 17:608–621, 2001.
[Gupta and Liu, 2003] M. Gupta and J. S. Liu. Discovery of conserved sequence patterns using a
stochastic dictionary model. J. Amer. Statist. Assoc., 98, 2003.
231

BIBLIOGRAPHY
BIBLIOGRAPHY
[Gusﬁeld, 2002] D. Gusﬁeld. Haplotyping as perfect phylogeny: Conceptual framework and efﬁ-
cient solutions. In Proceedings of RECOMB 2002, pages 166–175, 2002.
[Gusﬁeld, 2004] D. Gusﬁeld.
An overview of combinatorial methods for haplotype inference.
Technical Report, UC Davis, 2004.
[Haldimann et al., 1996] A. Haldimann, M. K. Prahalad, S. L. Fisher, S. Kim, C. T. Walsh, and
B. L. Wanner. Altered recognition mutants of the response regulator PhoB: A new genetic strat-
egy for studying protein-protein interactions. Proc. Natl. Acad. Sci. USA, 93:14361–14366, 1996.
[Halfon et al., 2002] M. S. Halfon, Y. Grad, G. M. Church, and A. M. Michelson. Computation-
based discovery of related transcriptional regulatory modules and motifs using an experimentally
validated combinatorial model. Genome Research, 12:1019–1028, 2002.
[Halperin and Eskin, 2002] E. Halperin and E. Eskin. Haplotype reconstruction from genotype data
using imperfect phylogeny. Technical Report, Columbia University, 2002.
[Harding et al., 1989] K. Harding, T. Hoey, R. Warrior, and M. Levine. Autoregulatory and gap
gene response elements of the even-skipped promoter of Drosophila. EMBO J., 8(4):1205–12,
1989.
[Helden et al., 2000] J. Van Helden, A. Rios, and J. Collado-Vides. Discovering regulatory ele-
ments in non-coding sequences by analysis of spaced dyads. Nucleic Acids Res., 28:1808–1818,
2000.
[Hertz and Stormo, 1996] G. Z. Hertz and G. D. Stormo. Escherichia coli promoter sequences:
Analysis and prediction. Meth. Enzymol., 273:30–42, 1996.
[Hertz and Stormo, 1999] G. Z. Hertz and G. D. Stormo. Identifying DNA and protein patterns
with statistically signiﬁcant alignments of multiple sequences. Bioinform., 15:563–577, 1999.
[Hodge et al., 1999] S. E. Hodge, M. Boehnke, and M. A. Spence. Loss of information due to
ambiguous haplotying of SNPs. Nat Genet, 21:360–361, 1999.
232

BIBLIOGRAPHY
BIBLIOGRAPHY
[Huang et al., 2004] H. Huang, M. Kao, X. Zhou, J. S. Liu, and W. H. Wong.
Determination
of local statistical signiﬁcance of patterns in Markov sequences with application to promoter
element identiﬁcation. Journal of Computational Biology, 11 (1), 2004.
[Hughes et al., 2000] J. D. Hughes, P. W. Estep, S. Tavazoie, and G. M. Church. Computational
identiﬁcation of cis-regulatory elements associated with groups of functionally related genes in
Saccharomyces cerevisiae. J. Mol. Biol, 296(5):1205–14, 2000.
[Hugot et al., 2001] J. P. Hugot, M. Chamaillard, H. Zouali, S. Lesage, J. P. Cezard, J. Belaiche,
S. Almer, C. Tysk, G. A. O’Morain, M. Gassull, V. Binder, Y. Finkel, A. Cortot, R. Modigliani,
P. Laurent-Puig, C. Gower-Rousseau, J. Macry, J. F. Colombel, M. Sahbatou, and G. Thomas.
Association of NOD2 leucine-rich repeat variants with susceptibility to Crohn’s disease. Nature,
411 (6837):599–603, 2001.
[Ishwaran and James, 2001] H. Ishwaran and L. F. James.
Gibbs sampling methods for stick-
breaking priors. Journal of the American Statistical Association, 90:161–173, 2001.
[Jaakkola and Jordan, 2000] T. S. Jaakkola and M. I. Jordan. Bayesian logistic regression: A vari-
ational approach. Statistics and Computing, 10:25–37, 2000.
[Jordan et al., 1999] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction
to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical
Models, pages 105–161. Kluwer Academic Publishers, 1999.
[Jordan, 2004] M. I. Jordan. Graphical models. Bayesian Statistics, special issue:in press, 2004.
[Kappen and Wiegerinck, 2002] H. J. Kappen and J. Wiegerinck. A novel iteration scheme for the
cluster variation method. In Advances in Neural Information Processing Systems 14, 2002.
[Karchin et al., 2002] R. Karchin, K. Karplus, and D. Haussler.
Classifying G-protein coupled
receptors with support vector machines. Bioinformatics, 18(1):147–159, 2002.
233

BIBLIOGRAPHY
BIBLIOGRAPHY
[Karisch and Rendl, 1998] S. E. Karisch and F. Rendl.
Semideﬁnite programming and graph
equipartition. In P. M. Pardalos and H. Wolkowicz, editors, Topics in Semideﬁnite and Interior-
Point Methods, volume 18, pages 77–95. AMS, 1998.
[Kechris et al., 2004] K. J. Kechris, E. van Zwet, P. J. Bickel, and M. B. Eisen. Detecting dna
regulatory motifs by incorperating positional trends in information content. Genome Biology,
5:R50, 2004.
[Keles et al., 2003] S. Keles, M. J. van der Laan, S. Dudoit, B. Xing, and M. B. Eisen. Super-
vised detection of regulatory motifs in DNA sequences. Statistical Applications in Genetics and
Molecular Biology, 2 (1), 2003.
[Keles et al., 2004] S. Keles, M. J. van der Laan, and C. Vulpe. Regulatory motif ﬁnding by logic
regression. Bioinformatics, page in press, 2004.
[Kellis et al., 2003] M. Kellis, N. Paterson, M. Endrizzi, B. Birren, and E. S. Lander. Sequencing
and comparison of yeast species to identify genes and regulatory elements. Nature, 423:241–254,
2003.
[Kenneth and Clark, 2002] M. W. Kenneth and A. G. Clark. Linkage disequilibrium and the map-
ping of complex human traits. TRENDS in Genetics, 18(1):19–24, 2002.
[Kikuchi, 1951] R. Kikuchi. Theory of cooperative phenomena. Phys. Rev., 81:988, 1951.
[Kimmel and Shamir, 2004] G. Kimmel and R. Shamir. Maximum likelihood resolution of multi-
block genotypes. In Proceedings of RECOMB 2004, pages 847–56, 2004.
[Krogh et al., 1994] A. Krogh, M. Brown, I.S. Mian, K. Sjolander, and D. Haussler.
Hidden
Markov models in computational biology: Applications to protein modeling.
J Mol Biol.,
235:1501–1531, 1994.
[Kruglyak and Nickerson, 2001] L. Kruglyak and D. A. Nickerson. Variation is the spice of life.
Nature Genetics, 27:234–236, 2001.
234

BIBLIOGRAPHY
BIBLIOGRAPHY
[Lafferty et al., 2001] J. Lafferty, F. Pereira, and A. McCallum. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling sequence data. In Proceedings of the International
Conference on Machine Learning, 2001.
[Lange, 2002] K. Lange. Mathematical and Statistical Methods for Genetic Analysis. Springer,
2002.
[Lari and Young, 1990] K. Lari and S. J. Young. The estimation of stochastic context free grammars
using the inside-outside algorithm. Computer Speech and Language, 4, 1990.
[Lauritzen and Sheehan, 2002] S. L. Lauritzen and N. A. Sheehan. Graphical models for genetic
analysis. TR R-02-2020, Aalborg University, 2002.
[Lawrence and Reilly, 1990] C. Lawrence and A. Reilly. An expectation maximization (EM) al-
gorithm for the identiﬁcation and characterization of common sites in unaligned biopolymer
sequences. Proteins, 7:41–51, 1990.
[Lawrence et al., 1993] C. E. Lawrence, S. F. Altschul, M. S. Boguski, J. S. Liu, A. F. Neuwald,
and J. C. Wootton. Detecting subtle sequence signals: A Gibbs sampling strategy for multiple
alignment. Science, 262:208–214, 1993.
[Lazzeroni, 2001] L. C. Lazzeroni. A chronology of ﬁne-scale gene mapping by linkage disequi-
librium. Stat Methods Med Res, 10:57–76, 2001.
[Leisink and Kappen, 2001] M. A. R. Leisink and H. J. Kappen. A tighter bound for graphical
models. In Advances in Neural Information Processing Systems 13, 2001.
[Lewin, 2003] B. Lewin. Genes VIII. Prentice Hall, 2003.
[Li et al., 2003] L. Li, E. I. Shakhnovich, and L. A. Mirny. Amino acids determining enzyme-
substrate speciﬁcity in prokaryotic and eukaryotic protein kinases. Proc. Natl. Acad. Sci. USA,
100(8):4463–4468, 2003.
235

BIBLIOGRAPHY
BIBLIOGRAPHY
[Lin et al., 2002] S. Lin, C. J. Cutler, M. E. Zwick, and A. Chakravarti. Haplotype inference in
random population samples. Am J Hum Genet, 71:1129–1137, 2002.
[Liu et al., 1995] J. S. Liu, A. F. Neuwald, and C. E. Lawrence. Bayesian models for multiple local
sequence alignment and Gibbs sampling strategies. J. Amer. Statist. Assoc, 90:1156–1169, 1995.
[Liu et al., 2001] X. Liu, D. L. Brutlag, and J. Liu. Bioprospector: Discovering conserved DNA
motifs in upstream regulatory regions of co-expressed genes. In Proc. of Pac Symp Biocomput,
pages 127–138, 2001.
[Liu et al., 2002] X. S. Liu, D. L. Brutlag, and J. S. Liu. An algorithm for ﬁnding protein-DNA
binding sites with applications to chromatin immunoprecipitation microarray experiments. Nat
Biotechnol, 20(8):835–9, 2002.
[Liu, 1994] J. S. Liu. The collapsed Gibbs sampler with applications to a gene regulation problem.
J. Amer. Statist. Assoc, 89:958–966, 1994.
[Lockless and Ranganathan, 1999] S W. Lockless and R. Ranganathan. Evolutionarily conserved
pathways of energetic connectivity in protein families. Science, 286(5438):295–299, 1999.
[Loots et al., 2002] G. G. Loots, I. Ovcharenko, L. Pachter, I. Dubchak, and E. M. Rubin. rVista for
comparative sequence-based discovery of functional transcription factor binding sites. Genome
Res, 12 (5):832–839, 2002.
[Ludwig et al., 2000] M. Z. Ludwig, C. Bergman, N. H. Patel, and M. E. Kreitman. Evidence for
stabilizing selection in a eukaryotic enhancer element. Nature, 403(6769):564–7, 2000.
[Markstein et al., 2002] M. Markstein, P. Markstein, V. Markstein, and M. S. Levine. Genome-
wide analysis of clustered Dorsal binding sites identiﬁes putative target genes in the Drosophila
embryo. Proc. Natl. Acad. Sci. USA, 99(2):763–768, 2002.
[Mehldau and Myers, 1993] G. Mehldau and E. W. Myers. A system for pattern matching applica-
tions on biosequences. Computer Applications in the BioSciences, 9(3):299–314, 1993.
236

BIBLIOGRAPHY
BIBLIOGRAPHY
[Michelson, 2002] A. M. Michelson. Deciphering genetic regulatory codes: A challenge for func-
tional genomics. Proc. Natl. Acad. Sci. USA, 99:546–548, 2002.
[Minka, 2001] T. Minka. Expectation propagation for approximate Bayesian inference. In Pro-
ceedings of the 15th Annual Conference on Uncertainty in AI, 2001.
[Moriyama and Kim, 2003] E. N. Moriyama and J. Kim. Protein family classiﬁcation with dis-
criminant function analysis. In Proceedings of Stadler Genetics Symposium, 2003.
[Murphy et al., 1999] K. Murphy, Y. Weiss, and M. Jordan. Loopy belief propagation for approx-
imate inference: An empirical study. In Proceedings of the 15th Annual Conference on Uncer-
tainty in AI, 1999.
[Nazina and Papatsenko, 2004] A. G. Nazina and D. A Papatsenko.
Statistical extraction of
eukaryotic cis-regulatory modules using exhaustive assessment of local word frequency.
(http://homepages.nyu.edu/ dap5/CV/word frequency.pdf), 2004.
[Neal and Hinton, 1998] R. M. Neal and G. E. Hinton. A view of the em algorithm that justiﬁes
incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models,
pages 355–368. Kluwer Academic Publishers, 1998.
[Neal, 2000] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models.
J. Computational and Graphical Statistics, 9(2):249–256, 2000.
[Niu et al., 2002] T. Niu, S. Qin, X. Xu, and J. Liu. Bayesian haplotype inference for multiple
linked single nucleotide polymorphisms. American Journal of Human Genetics, 70:157–169,
2002.
[Page and Holmes, 1998] R. D. M. Page and E. C. Holmes. Molecular Evolution: A Phylogenetic
Approach. Blackwell, Oxford, 1998.
237

BIBLIOGRAPHY
BIBLIOGRAPHY
[Papatsenko et al., 2002] D. A. Papatsenko, V. J. Makeev, A. P. Lifanov, M. Regnier, A. G. Nazina,
and C. Desplan. Extraction of functional binding sites from unique regulatory regions: The
Drosophila early developmental enhancers. Genome Research, 12:470–481, 2002.
[Patil et al., 2001] N. Patil, A. J. Berno, D. A. Hinds, et al. Blocks of limited haplotype diversity
revealed by high-resolution scanning of human chromosome 21. Science, 294:1719–1723, 2001.
[Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent System: Networks of Plausible Infer-
ence. Morgan Kaufmann, 1988.
[Pennacchio and Rubin, 2001] L. A. Pennacchio and E. M. Rubin. Genomic strategies to identify
mammalian regulatory sequences. Nature Reviews Genetics, 2(2):100–109, 2001.
[Pritchard, 2001] J. K. Pritchard. Are rare variants responsible for susceptibility to complex dis-
ease? Am J Hum Genet, 69:124–137, 2001.
[Ptashne and Gann, 1997] M. Ptashne and A. Gann. Transcriptional activation by recruitment. Na-
ture, 386:569–577, 1997.
[Ptashne, 1988] M. Ptashne. How eukaryotic transcriptional activators work. Nature, 335:683–689,
1988.
[Puffenberger et al., 1994] E. G. Puffenberger, E. R. Kauffman, S. Bolk, T. C. Matise, S. S. Wash-
ington, M. Angrist, J. Weissenbach, K. L. Garver, M. Mascari, and R. Ladda et al. Identity-
by-descent and association mapping of a recessive gene for Hirschsprung disease on human
chromosome 13q22. Hum Mol Genet, 3 (8):1217–25, 1994.
[Quandt et al., 1995] K. Quandt, K. Frech, H. Karas, E. Wingender, and T. Werner. MatInd and
MatInspector: New fast and versatile tools for detection of consensus matches in nucleotide
sequence data. Nucleic Acids Res, 23 (23):4878–84, 1995.
[Rabiner and Juang, 1986] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov mod-
els. IEEE ASSP Magazine, pages 4–15, January 1986.
238

BIBLIOGRAPHY
BIBLIOGRAPHY
[Rajewsky et al., 2002] N. Rajewsky, M. Vergassola, U. Gaul, and E. D. Siggia. Computational
detection of genomic cis-regulatory modules, applied to body patterning in the early Drosophila
embryo. BMC Bioinformatics, 3:30:1–13, 2002.
[Rasmussen, 2000] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural
Information Processing Systems 12, 2000.
[Ren et al., 2000] B. Ren, F. Robert, J. Wyrick, O. Aparicio, E. Jennings, I. Simon, J. Zeitlinger,
J. Schreiber, N. Hannett, E. Kanin, T. Volkert, C. Wilson, S. Bell, and R. Young. Genome-wide
location and function of DNA binding proteins. Science, 290 (5500):2306–2309, 2000.
[Rendl and Wolkowicz, 1995] F. Rendl and H. Wolkowicz. A projection technique for partitioning
the nodes of a graph. Annals of Operations Research, 58:155–180, 1995.
[Rioux et al., 2001] J. D. Rioux, M. J. Daly, M. S. Silverberg, K. Lindblad, H. Steinhart, Z. Cohen,
T. Delmonte, K. Kocher, and K. Miller et al. Genetic variation in the 5q31 cytokine gene cluster
confers susceptibility to Crohn disease. Nat Genet, 29 (2):223–8, 2001.
[Risch, 2000] N. J. Risch. Searching for genetic determinants in the new millennium. Nature,
405(6788):847–56, 2000.
[Rubin, 2001] G. M. Rubin. The draft sequences: Comparing species. Nature, 409:820–821, 2001.
[S. Lauritzen, 1988] D. Spiegelhalter S. Lauritzen. Local computations with probabilities on graph-
ical structures and their applications to expert systems (with discussion). Journal of the Royal
Statistical Society, Series B, 50:157–224, 1988.
[Sachidanandam et al., 2001] R. Sachidanandam, D. Weissman, S. C. Schmidt, et al. A map of
human genome sequence variation containing 1.42 million single nucleotide polymorphisms.
Nature, 291:1298–2302, 2001.
[Sackerson et al., 1999] C. Sackerson, M. Fujioka, and T. Goto. The even-skipped locus is con-
tained in a 16-kb chromatin domain. Dev Biol., 211(1):39–52, 1999.
239

BIBLIOGRAPHY
BIBLIOGRAPHY
[Sagan, 1992] H. Sagan. Introduction to the Calculus of Variations. Dover Publications, 1992.
[Saul and Jordan, 1996] L. K. Saul and M. I. Jordan.
Exploiting tractable substructures in in-
tractable networks. In Advances in Neural Information Processing Systems 8, 1996.
[Schneider and Stephens, 1990] T. D. Schneider and R. M. Stephens. Sequence logos: A new way
to display consensus sequences. Nucl. Acids Res., 18:6097–6100, 1990.
[Schneider et al., 1986] T. D. Schneider, G. D. Stormo, L. Gold, and A. Ehrenfeucht. Information
content of binding sites on nucleotide sequences. J Mol Biol., 188(3):415–31, 1986.
[Schwartz et al., 2003] S. Schwartz, W. J. Kent, A. Smit, Z. Zhang, R. Baertsch, R. C. Hardison,
D. Haussler, and W. Miller. Human-mouse alignments with BLASTZ. Genome Res, 13 (1):103–
107, 2003.
[Segal et al., 2001] E. Segal, B. Taskar, A. Gasch, N. Friedman, and D. Koller. Rich probabilis-
tic models for gene expression. In Ninth International Conference on Intelligent Systems for
Molecular Biology, 2001.
[Segal et al., 2003a] E. Segal, Y. Barash, I. Simon, N. Friedman, and D. Koller. From promoter
sequence to expression: a probabilistic framework. In Proceedings of RECOMB 2002, pages
263–272, 2003.
[Segal et al., 2003b] E. Segal, M. Shapira, A. Regev, D. Pe’er, D. Botstein, D. Koller, and N. Fried-
man. Module networks: identifying regulatory modules and their condition-speciﬁc regulators
from gene expression data. Nature Genetics, 34(2):166–76, 2003.
[Shalon et al., 1996] D. Shalon, S. J. Smith, and P. O. Brown.
A DNA microarray system for
analyzing complex DNA samples using two-color ﬂuorescent probe hybridization. Genome Re-
search, 6(7):639–45, 1996.
240

BIBLIOGRAPHY
BIBLIOGRAPHY
[Sharan et al., 2003] R. Sharan, I. Ovcharenko, A. Ben-Hur, and R. Karp. Creme: A framework for
identifying cis-regulatory modules in human-mouse conserved segments. Bioinformatics, Suppl.
1:i283–i291, 2003.
[Sigrist et al., 2002] C. Sigrist, L. Cerutti, N. Hulo, A. Gattiker, L. Falquet L, M. Pagni, A. Bairoch,
and P. Bucher. PROSITE: A documented database using patterns and proﬁles as motif descrip-
tors. Brief Bioinform, 3:265–274, 2002.
[Sinha and Tompa, 2000] S. Sinha and M. Tompa. A statistical method for ﬁnding transcription
factor binding sites. In Proceedings of the Eighth International Conference on Intelligent Systems
for Molecular Biology, pages 344–354, 2000.
[Sj¨olander et al., 1996] K. Sj¨olander, K. Karplus, M. Brown, R. Hughey, A. Krogh, I.S. Mian, and
D. Haussler.
Dirichlet mixtures: A method for improving detection of weak but signiﬁcant
protein sequence homology. Computer Applications in the Biosciences, 12, 1996.
[Small et al., 1996] S. Small, A. Blair, and M. Levine. Regulation of two pair-rule stripes by a
single enhancer in the Drosophila embryo. Dev Biol., 175(2):314–24, 1996.
[Stanojevic et al., 1991] D. Stanojevic, S. Small, and M. Levine.
Regulation of a segmen-
tation stripe by overlapping activators and repressors in the Drosophila embryo.
Science,
254(5036):1385–7, 1991.
[Stephens and Donnelly, 2000] M. Stephens and P. Donnelly. Inference in molecular population
genetics. Journal of the Royal Statistical Society, Series B, 62:605–655, 2000.
[Stephens and Donnelly, 2003] M. Stephens and P. Donnelly. A comparison of Bayesian meth-
ods for haplotype reconstruction from population genotype data. American Journal of Human
Genetics, 73:1162–1169, 2003.
241

BIBLIOGRAPHY
BIBLIOGRAPHY
[Stephens et al., 2001] M. Stephens, N. Smith, and P. Donnelly. A new statistical method for hap-
lotype reconstruction from population data. American Journal of Human Genetics, 68:978–989,
2001.
[Stoneking, 2001] M. Stoneking. Single nucleotide polymorphisms: From the evolutionary past. .
. Nature, 409:821–822, 2001.
[Stormo and Fields, 1998] G. D. Stormo and D. S. Fields. Speciﬁcity, free energy and information
content in protein-DNA interactions. Trends in Biochemical Sciences, 23:109–113, 1998.
[Stormo, 2000] G. D. Stormo. DNA binding sites: representation and discovery. Bioinformatics,
16 (1):16–23, 2000.
[Stryer, 1995] L. Stryer. Biochemistry (4th. edition). W. H. Freeman and Company, 1995.
[Sturm, 1999] J.F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmet-
ric cones. Optimization Methods and Software, 11–12:625–653, 1999. Special issue on Interior
Point Methods (CD supplement with software).
[Swendsen and Wang, 1987] R. Swendsen and J-S Wang.
Non-universal critical dynamics in
Monte Carlo simulation. Physical Review Letters, 58:86–88, 1987.
[Tanner and Wong, 1987] M. A. Tanner and W. H. Wong. The calculation of posterior distributions
by data augmentation. Journal of the American Statistical Association, 82:528–540, 1987.
[Tavare and Ewens, 1998] S. Tavare and W.J. Ewens. The Ewens sampling formula. Encyclopedia
of Statistical Sciences, Update Volume 2.:230–234, 1998.
[Teh et al., 2004] Y. Teh, M. I. Jordan, M. Beal, and D. Blei. Hierarchical Dirichlet processes.
Technical Report 653, Department of Statistics, University of California, Berkeley, 2004.
[Thijs et al., 2001] G. Thijs, M. Lescot, K. Marchal, S. Rombauts, B. De Moor, P. Rouz, and
Y. Moreau. A higher-order background model improves the detection of promoter regulatory
elements by Gibbs sampling. Bioinformatics, 17 (12):1113–112, 2001.
242

BIBLIOGRAPHY
BIBLIOGRAPHY
[Thompson, 1981] E. A. Thompson. Pedigree analysis of Hodgkin’s disease in a Newfoundland
genealogy. Annals of Human Genetics, 45:279–292, 1981.
[Ureta-Vidal et al., 2003] A. Ureta-Vidal, L. Ettwiller, and E. Birney.
Comparative genomics:
genome-wide analysis in metazoan eukaryotes. Nature Reviews Genetics, 4:251–262, 2003.
[van Helden et al., 1998] J. van Helden, B. Andre, and J. Collado-Vides. Extracting regulatory sites
from the upstream region of yeast genes by computational analysis of oligonucleotide frequen-
cies. J. Mol. Biol., 281:827–42, 1998.
[Vandenberghe and Boyd, 1996] L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM
Review, 38(1):49–95, 1996.
[Venter et al., 2001] C. Venter, M. D. Adams, E. W. Myers, et al. The sequence of the human
genome. Science, 291:1304–51, 2001.
[Wainwright and Jordan, 2003] M. J. Wainwright and M. I. Jordan. Variational inference in graph-
ical models: The view from the marginal polytope. Invited paper; Allerton Conference on Com-
munication, Control, and Computing, Oct. 2003.
[Wasserman and Sandelin, 2004] W. W. Wasserman and A. Sandelin. Applied bioinformatics for
the identiﬁcation of regulatory elements. Nature Reviews Genetics, 5:276–287, 2004.
[Weiss and Clark, 2002] K. Weiss and A. Clark. Linkage disequilibrium and the mapping of com-
plex trains. Trends. in Genetics, 18(1):19–24, 2002.
[West et al., 1994] M. West, P. Muller, and M. D. Escobar. Hierarchical priors and mixture models,
with application in regression and density estimation. Aspects of Uncertainty: A Tribute to D V
Lindley, 1994.
[Wiegerinck, 2000] W. Wiegerinck. Variational approximations between mean ﬁeld theory and the
junction tree algorithm. In Proceedings of the 16th Annual Conference on Uncertainty in AI,
2000.
243

BIBLIOGRAPHY
BIBLIOGRAPHY
[Wingender et al., 2000] E. Wingender, X. Chen, R. Hehl, H. Karas, I. Liebich, V. Matys, T. Mein-
hardt, M. Pruss, I. Reuter, and F. Schacherer. TRANSFAC: An integrated system for gene ex-
pression regulation. Nucleic Acids Res., 28:316–319, 2000.
[Xing and Karp, 2004] E. P. Xing and R. M. Karp. MotifPrototyper: a proﬁle Bayesian model for
motif family. Proc. Natl. Acad. Sci. USA, 101(29):10523–28, 2004.
[Xing et al., 2001] E. P. Xing, D. Wolf, I. Dubchak, S. Spengler, M. Zorn, C. Kulikowski, and
I. Muchnik.
Automatic discovery of sub-molecular sequence domains in multi-aligned se-
quences: A dynamic programming algorithm for multiple alignment segmentation. Journal of
Theoretical Biology, 212(2):129–139, 2001.
[Xing et al., 2003a] E. P. Xing, M. I. Jordan, R. M. Karp, and S. Russell. A hierarchical Bayesian
Markovian model for motifs in biopolymer sequences. In Advances in Neural Information Pro-
cessing Systems 15, 2003.
[Xing et al., 2003b] E. P. Xing, M. I. Jordan, and S. Russell. A generalized mean ﬁeld algorithm
for variational inference in exponential families. In Proceedings of the 19th Annual Conference
on Uncertainty in AI, 2003.
[Xing et al., 2004a] E. P. Xing, M. I. Jordan, and S. Russell. Graph partition strategies for general-
ized mean ﬁeld inference. In Proceedings of the 20th Annual Conference on Uncertainty in AI,
2004.
[Xing et al., 2004b] E. P. Xing, W. Wu, M. I. Jordan, and R. M. Karp. Logos: A modular Bayesian
model for de novo motif detection.
Journal of Bioinformatics and Computational Biology,
2(1):127–154, 2004.
[Xing et al., 2004c] E.P. Xing, R. Sharan, and M.I Jordan. Bayesian haplotype inference via the
Dirichlet process. In Proceedings of the 21st International Conference on Machine Learning,
2004.
244

BIBLIOGRAPHY
BIBLIOGRAPHY
[Yedidia et al., 2001a] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Generalized belief propagation.
In Advances in Neural Information Processing Systems 13, 2001.
[Yedidia et al., 2001b] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief prop-
agation and its generalizations.
In Distinguished Lecture track, the 17th International Joint
Conference on AI, 2001.
[Zhang et al., 2002] K. Zhang, M. Deng, T. Chen, M. Waterman, and F. Sun. A dynamic program-
ming algorithm for haplotype block partitioning. Proc. Natl. Acad. Sci. USA, 99(11):7335–39,
2002.
245

