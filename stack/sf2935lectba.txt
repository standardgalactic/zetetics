sf2935 Modern Methods of Statistical Learning:
Bayesian Learning, Bayesian Reasoning (&
Bayesian Perceptrons)
Timo Koski

1
Introduction
1.1
Learning a Distribution
In these lecture notes we treat parametric models and model families for
I.I.D. data and the estimation of them. We present the learning method of
posterior probabilities and Bayes factors.
1.2
General Summary
Bayes’ theorem or Bayes’ rule for events, as known from ﬁrst courses in prob-
ability and statistics, can be stated mathematically as the following equation
P(B|A) = P(B) P(A|B)
P(A)
,
(1.1)
where A and B are events, P(A) > 0. For a very long time, in the era until
WWII, this was known as ’inverse probability’, as the roles of A and B are
inverted in (1.1).
By learning from data one often means the process of inferring a general
law or principle from the observations of particular instances. The general
law is a piece of knowledge about the mechanism of nature that generates
the data. The learning can be done by use of ’MODELS’, which serve as the
language in which the constraints predicated on the data can be described.
We shall in these lectures ﬁrst talk about parametric statistical models,
these are in a sense only vehicles of learning, not an end in themselves.
A formal Bayesian modelling articulates the information in a (training)
data set with evidence other than that of the (training) set. It is thought
that there is always such evidence or that there is no such thing as the ’right
analysis’ if there is none.
The evidence is assessed by judgement and is
expressed in probability theory terms:
(1) a probability distribution speciﬁes the probability of any sequence of data
conditional on certain parameters;
(2) a prior expresses uncertainty about the parameters.
When (1) is combined with the training sequence we get the likelihood func-
tion of the sequence. The likelihood function is combined with (2) via Bayes’
1

rule to produce a posterior distribution for the parameters of the model and
this is the output of the formal Bayesian analysis.
A probabilist/statistician has reminisced :
Perhaps I should start from my undergraduate studies in math-
ematics at the University of Rome ’La Sapienza’, where Bruno
De Finetti, the father of neo-Bayesianism, had been Professor in
Probability for 30 years. Although I never met him, his legacy
was very much alive in the probability classes, where we were
taught not only about theorems, but also about the philosophy
and the interpretation of probability. Once you accept the subjec-
tive (Bayesian) interpretation of probability, you see that statisti-
cal inference is all about probabilistic modeling and computation
of conditional probabilities, without forgetting about model com-
parison and testing the goodness-of-ﬁt of the models.
One key word in the preceding is model comparison or model choice. It
will be seen frequently in the sequel that Bayesian statistical methods will
(at least asymptotically) coincide with the frequentist statistical procedures.
However, this is not the case for model choice, for which there are few stan-
dard procedures outside Bayesian learning.
1.3
Bayesian Reasoning
Suppose
P(B|A) > P(B).
In words, if A is true, B becomes more likely. Now, using Bayes’ rule we can
invert the roles of A and B. It follows clearly that
P(A) P(B|A)
P(B)
> P(A)
and Bayes’ rule (1.1) gives
P(A | B) > P(A).
In words: if B is true, A becomes more likely. For example, if it is raining,
the sidewalk is likely to be wet. Now, the sidewalk is wet, it is likely to be
raining. In fact, that if P(B|A) > P(B), then we have the statements
2

1. If A is true, then B becomes more likely.
2. If not B is true, then A becomes less likely.
3. If B is true, A becomes more likely.
4. If not A is true, then B becomes less likely.
These look like soft versions of Boolean logic.
2
Bayes’ Theorem
2.1
Bayes’ rule for Random Variables: Formal Expres-
sions
The following form of Bayes’ rule is contains more of structure than (2.2). If
Hi, i = 1, . . . , n are an exhaustive partition of the outcome space, one has
P(Hi | A) =
P(Hi)P(A | Hi)
Pn
j=1 P(Hj)P(A | Hj) .
(2.2)
Here we talk about the posterior probability of the hypothesis Hi given the
evidence A. P(A|Hi) is the likelihood of Hi given evidence, and P(Hi) is
the prior probability of Hi. The fact that P(A) = Pn
j=1 P(Hj)P(A | Hj) is
known as the law of total probability.
Consider two random variables X and Y . In principle, Bayes’ rule applies
to the events A = {X = x} and B = {Y = y}. However, to remain useful,
Bayes’ rule is to be formulated in terms of the pertinent probability densities
or probability mass functions.
Let us recall the joint (simultaneous) probability density of (X, Y ), a
continuous two dimensional R.V.
fX,Y (x, y)
s.t.fX,Y (x, y) ≥0,
Z +∞
−∞
Z +∞
−∞
fX,Y (x, y)dxdy = 1.
Then we deﬁne the conditional probability density of Y given X = x
as
fY |X(y|x)
def
= fX,Y (x, y)
fX(x)
,
fX(x) > 0
(2.3)
3

where
fX(x) =
Z +∞
−∞
fX,Y (x, y)dy.
(2.4)
The conditional probability density of X given Y = y, fY |X(y|x), is
deﬁned analogously.
By applying the deﬁnition (2.3) a few times we obtain (2.7). The formulas
in (2.5) - (2.6) are obtained in similar ways.
• For X continuous and Y discrete,
fX|Y (x|y) = P(Y = y | X = x) fX(x)
P(Y = y)
.
(2.5)
• For X is discrete and Y continuous,
PX|Y (X = x | y) = fY |X(y | x) P(X = x)
fY (y)
.
(2.6)
• If both X and Y are continuous,
fX|Y (x|y) = fY |X(y|x) fX(x)
fY (y)
.
(2.7)
It is useful to express the denominator using the law of total probability.
For fY (y), we compute the integral
fY (y) =
Z ∞
−∞
fY |X(y | x) fX(x) dx.
(2.8)
The equations (2.5) - (2.8) are the formal tools of the Bayesian statistical
inference and learning. Already this brief glance should hint that the essential
computational issue here is the mixing integral (2.8).
2.2
Parametric Statistical Models
One type of learning that we will be concerned with, is inferring, or analysing
data with a model family indexed by parameters. What does this mean?
Suppose that f (x|θ) is a probability density on Rn. f (x|θ) is a known
mathematical expression in x and θ.
θ is an unknown parameter of the
density.
4

The textbooks applying no Bayesian methosd write f (x|θ), as
f (x; θ), where θ is treated as a mathematical parameter in an
expression for a function.
Let us regard x is an observation of a random variable X, x can be of the
form x = (x1, . . . , xn), or x can be continuous or discrete variate or a mixed
case.
We assume
X ∼f (x | θ)
Suppose that f (x|θ) is a probability density on Rn.
f (x|θ) is a known
mathematical expression in x ∈Rn and θ, which is a parameter that lies in
some known set of parameters Θ.
[Parametric statistical model] f (x|θ) is a probabilistic mech-
anism of generating data, characterizes the behaviour of future
observations conditional on θ. The parameter θ is unknown, but
is assumed to lie in some known parameter space Θ. The expres-
sion P(x | θ) is for a discrete x.
[Statistical model family ]
M = {f(x|θ), θ ∈Θ}
or
M = {P(X = x|θ), θ ∈Θ}.
Some examples illustrate these concepts.
Example 2.1 θ = (µ, σ) ∈Θ = R × (0, ∞) .
f (x|θ) =
1
σ
√
2πe−1
σ2 (x−µ)2, −∞< x < ∞.
We say that x is an observation from the normal distribution N (µ, σ2).
5

Example 2.2 Consider the r.v. X which is binary, its set of values is coded
as {0, 1}, Θ = {θ|0 < θ < 1} and
P(X = x | θ) = θx · (1 −θ)1−x.
(2.9)
Then we say X is distributed according to the Bernoulli distribution with
the parameter θ.
X | Θ = θ ∼Be(θ).
Example 2.3 Consider the r.v. X with values x in {0, 1, 2, . . . , n}, Θ =
{θ|0 < θ < 1}.
P (x|θ) = P (X = x|θ) =
 n
k

θk(1 −θ)n−k.
(2.10)
Then we say X is distributed according to the Binomial distribution with
the parameters n and θ.
X | Θ = θ ∼Bin(n, θ).
Example 2.4 The r.v.
X has the non-negative integers as values x ∈
{0, 1, . . .}, and Θ = {θ | θ > 0} and
P (x|θ) = P(X = x | θ) = e−θ θx
x!
(2.11)
Then we say that X is distributed according to the Poisson distribution with
the parameter θ.
X | Θ = θ ∼Po(θ),
2.3
Parametric Statistical Models and Priors
Bayesian standpoint regards an unknown parameter as a
random variable. Uncertainty about the unknown θ is modelled
by a probability distribution π (θ), and then π (θ | x) expresses
the uncertainty about the unknown θ after the observation of x.
6

Formally, θ is an outcome of a r.v. Θ, and lies in a vector space of ﬁnite
dimension Θ. For simplicity of writing Θ is assumed to be non denumerable.
We take
X | Θ = θ ∼f (x|θ) ,
where f (x|θ) is a probability density conditionally on Θ = θ. The main goal
is to ﬁnd π (θ | x), this is where Bayes’ formula comes in.
[Bayesian statistical model family ]
M = {f(x|θ), π(θ), θ ∈Θ}.
Suppose now that X is continuous. From (2.7) we get the posterior
probability density of Θ given X = x as
π(θ|x) = f(x|θ) π(θ)
f(x)
.
(2.12)
Here π(θ) is the prior density of Θ. In (2.12)
fX(x) =
Z ∞
−∞
f(x | θ) π(θ) dθ.
(2.13)
In view of (2.6) we have for discrete X we have the posterior probability
density
π(θ|x) = P(X = x|θ) π(θ)
f(x)
,
(2.14)
where
fX(x) =
Z ∞
−∞
P(X = x|θ) π(θ) dθ.
(2.15)
Example 2.5 From Example 2.4
X | Θ = θ ∼Po(θ).
Let the parameter Θ be exponentially distributed
π(θ) =
 λe−λθ,
θ > 0
0,
θ < 0.
7

In this situation λ is called a hyperparameter. Then we get in (2.5)
f(x | θ) =
(
e−θ θx
x! λe−λθ
P(X=x)
,
θ > 0
0,
θ < 0,
(2.16)
where
P(X = x) =
Z +∞
0
e−θ θx
x!λe−λθdθ
= λ
x!
Z +∞
0
θxe−θ(1+λ)dθ.
Here
Z +∞
0
θxe−θ(1+λ)dθ = x!
1
(1 + λ)x+1.
When we insert in (2.16) we get
π(θ | x) =

(1 + λ)x+1e−θ(1+λ) θx
x!
θ > 0
0
θ < 0.
(2.17)
This is the probability density function of Γ(x + 1, 1/(1 + λ)), a Gamma
distribution with parameters x + 1 and 1/(1 + λ). Hence
Θ | X = x ∼Γ(x + 1, 1/(1 + λ))
Let us note that the prior distribution here, the exponential distribution, can
be seen as Γ(1, 1/λ). We have here an example of a conjugate prior or a prior
closed under sampling for f(x | θ): the posterior and prior are members of
the same family of distributions.
Example 2.6 Xi | Θ = θ ∼N (θ, σ2
0), Θ ∼N (µ, s2). x(n) = (x1, . . . , xn) an
I.I.D. sample of Xi, respectively, x = 1
n
Pn
i=1 xi.
Θ | (X1, . . . , Xn) ∼N
nx/σ2
0 + µ/s2
n/σ2
0 + 1/s2 ,
1
n/σ2
0 + 1/s2

i.e., π
 θ|x(n)
is the density of this normal distribution. Here µ and s2 are
called hyperparameters.
8

3
Bayesian Learning/Inference of Parameters
3.1
Likelihood, Conjugate Family of Priors, MLE,
MAP
The expression f (x | θ) regarded as a function of θ is known as the likelihood
function
Lx (θ) = f (x | θ) .
The likelihood function Lx (θ) thus compares the plausibilities of diﬀerent
parameter values for given x.
lx (θ) = −log Lx (θ)
is called (−1×) the log likelihood function. By Bayes formula we get the
schematic formulation
posterior ∝likelihood × prior
Any function π(θ) such that
π (θ) ≥0,
θ ∈Θ
and
Z
Θ
π (θ) dθ = 1,
might serve as a prior probability density function. But even functions with
the properties π (θ) ≥0, and
Z
Θ
π (θ) dθ = ∞,
are also invoked as priors, and are called improper priors.
The choice of π(θ) should, however, reﬂect judgements and understand-
ing about the domain to be studied, prior to seeing data, reﬂected in the
additional mathematical properties of the function π(θ). Later in section 8
there will be more about the selection of prior, but, for ease of presentation,
we give one rule quantifying prior information right now.
9

Deﬁnition 3.1 Conjugate Family of Priors
A family F of probability distributions on Θ is said to be conjugate or
closed under sampling for a likelihood function
Lx (θ) = f (x | θ) ,
if for every π(θ) ∈F, the posterior distribution π (θ|x) also belongs to F.
We have seen examples of conjugate priors in Examples 2.5 and 2.6.
A
more complete treatment is found in the sf2935 lecture/slides on
exponential families and exponential deep learning. An intuitive way
of understanding conjugate priors is that with conjugate priors the prior
knowledge can be translated into equivalent sample information. E.g., in
Example 2.6,
N
nx/σ2
0 + µ/s2
n/σ2
0 + 1/s2 ,
1
n/σ2
0 + 1/s2

.
Deﬁnition 3.2 The maximum likelihood estimate/estimator The max-
imum likelihood estimate, MLE, bθML of θ, is deﬁned by
bθML = argmaxθ∈Θf (x | θ)
= argminθ∈Θlx (θ) .
When we regard MLE as a function of X, i.e., bθML(X), then we talk about
a Maximum Likelihood Estimator.
bθML is a parameter value that gives the observed x the highest possible prob-
ability.
Deﬁnition 3.3 The maximum a posterior estimate/estimator The
maximum a posterior estimate, MAP, bθMAP of θ is deﬁned by
bθMAP = argmaxθ∈Θπ (θ | x)
When we regard MAP as a function of X, i.e., bθMAP(X), then we talk about
a Maximum A Posterior Estimator.
10

We have in fact
bθMAP = argmaxθ∈Θf (x | θ) π (θ) .
Example 3.1 x1 . . . xn are independent samples of N(µ, σ2), θ = (µ, σ) ∈
Θ = {µ ∈R, σ > 0}. Then the likelihood function is obtained by multipli-
cation
f(x1 | θ) · · · f(xn | θ) =
1
σn(
√
2π)ne−
1
2σ2
Pn
i=1(xi−µ))2
If we set the partial derivatives of lx (µ, σ2) w.r.t. µ and σ2 to zero and solve
the system of equations we get
¯x = 1
n
n
X
j=1
xj, s2 = 1
n
n
X
j=1
(xj −¯x)2
as the maximum likelihood estimates of µ and σ2, respectively.
3.2
MAP for multivariate regression & MAP for logis-
tic regression
3.2.1
MAP for multivariate regression
Y = βTX + ϵ
Y ∼Nn
 βTX, σ2I

f (y | β) =
1
σn(2π)n/2e−1
2 (y−βT X)T (y−βT X)
and maximum likelihood estimate
bβ = (XTX)−1XTy
is nothing but the least squares estimate.
Take β ∼Np+1(µ, ψ2I). Then it follows that
bβMAP = (XTX + σ2
ψ2I)−1XTY
For ψ →∞, the MAP -estimate becomes the ML-estimate.
11

3.2.2
MAP for logistic regression
Logistic regression is a model for a binary r.v. Y with values y in {−1, +1}.
It says that
P(Y = y | X1 . . . , Xl) = σ
 yβTX

,
where σ(t) = 1/(1 + e−t) and X is a vector. We have a training set
S = {(X1, y1) , . . . , (Xl, yl)} ,
where yi = ±1. We choose a prior distribution β ∼N (0, Σ)
The logarithm of the posterior probability of the parameters β is
ln π (β | X) =
l
X
i=1
ln P (yi | Xi) −1
2βTΣ−1β + c
Thereby we need only to deal with (
F (β) =
l
X
i=1
log σ
 yiβTXi

−1
2βTΣ−1β
to maximize the posterior density, i.e., to ﬁnd the MAP estimate of β
∂
∂βF (β) =
l
X
i=1
yiXiσ(−yiβTXi) −Σ−1β
∂
∂βF (β) =
l
X
i=1
yiXiσ(−yiβTXi) −Σ−1β
This follows by
d
dt log σ(t) =
e−t
1 + e−t =
1
1 + et = σ(−t)
and by the derivative of a quadratic form (diﬀerential calculus for matrices)
∂
∂β
1
2βTΣβ = Σβ
and by
∂
∂ββTX = ∂
∂βXTβ = X.
12

Thus if we set the gradient
∂
∂βF (β) = 0 (= a column vector of zeros) we get
l
X
i=1
yiXiσ(−yiβTXi) = Σ−1β
i.e. the MAP bβ will satisfy
bβ =
l
X
i=1
yiσ(−yibβ
TXi)ΣXi
This system of equations is solved by numerical algorithms. When we insert
bβ back to P (y | X) (see Lecture 5 on Logistic Regression) we have
bP (y | X) = σ

ybβ
TX

= σ
 
y
l
X
i=1
yiσ(−yibβ
TXi)XT
i ΣX
!
.
Here we recognize what is in machine learning and statistics called a ’valid
kernel’
K (Xi, X) := XT
i ΣX.
4
Issues of Bayesian Learning (1)
4.1
General Aspects of Bayesian Inference, McMC,
ABC
Bayesian learning uses probability as tool for all parts of data analysis. This
is part of what is being called coherence.
• Inference is based on the observed x, not on an unobserved sample
space, c.f. unbiased and eﬀective estimators.
• π (θ|x) is the only quantity evaluated for inference about θ.
However, evaluation of π (θ|x) is not in general possible by explicit means
of integral calculus, due to the diﬃculties of computing the denominator.
13

This is where Markov chain Monte Carlo (McMC) has contributed to the
widespread popularity of Bayesian learning methods. BUGS, useful software
for McMC and practical Bayesian analysis is described in Lunn et.al. (2013).
We are in these lectures dealing with models, where an analytical formula
for the likelihood function can typically be derived. For complex models,
an analytical formula might be elusive or the likelihood function might be
computationally very costly to evaluate.
Approximate Bayesian computation (ABC) methods bypass by computa-
tional means the evaluation of the likelihood function. Therefore, the ABC
methods widen the realm of models for which statistical learning can be
considered, c.f. Sunn˚aker e.t.al. (2013).
4.2
Estimators
This example is due to Dennis Lindley.
x = (x1, . . . , xn) , θ = (θ1, . . . , θn)
Xi|Θi = θi ∼N (θi, 1) ,
I.I.D.
Θi ∼N (0, 1) ,
I.I.D.
One can check that fX (Xi) is N(0, 2), and Θi|Xi = xi ∼N
  xi
2 , 1
2

. We use
bθi = xi
2 as a point estimator of θi. Then
1
n
n
X
i=1
bθi
2 = 1
n
n
X
i=1
x2
i
4 →1
2
by the law of large numbers as n →∞, but
1
n
n
X
i=1
θ2
i →1
by the law of large numbers as n →∞.
Bayesian analysis works with the distribution of 1
n
Pn
i=1 θ2
i given x. The
expectation w.r.t. this distribution is
E
"
1
n
n
X
i=1
θ2
i | x
#
= 1
n
n
X
i=1
x2
i
4 + 1
2
14

and
1
n
n
X
i=1
x2
i
4 + 1
2 →1.
by law of large numbers as n →∞. Hence the posterior distribution has the
right properties, not the estimator.
4.3
Conﬁdence intervals
Xi | Θ = θ ∼N (θ, σ2
0), Θ ∼N (µ, s2). x(n) = (x1, . . . , xn) an I.I.D. sample
of Xi (conditionally on Θ = θ, as explained later), respectively, and x =
1
n
Pn
i=1 xi.
Θ | x(n) ∼N
nx/σ2
0 + µ/s2
n/σ2
0 + 1/s2 ,
1
n/σ2
0 + 1/s2

i.e., π
 θ|x(n)
is the density of this normal distribution. Here µ and s2 are
again hyperparameters.
• P (a(x) ≤Θ ≤b(x)) =
R b(x)
a(x) f (θ|x) dθ
•
P (a(x) ≤Θ ≤b(x))
|
{z
}
This is a probability, not a degree of conﬁdence
Let s →∞(the prior becomes improper). Then
N
nx/σ2
0 + µ/s2
n/σ2
0 + 1/s2 ,
1
n/σ2
0 + 1/s2

→N

x, σ2
0
n

.
But in this limit the Bayesian conﬁdence interval, e.g., P (a(x) ≤Θ ≤b(x)) =
0.95 becomes the familiar conﬁdence interval for the mean of a normal dis-
tribution with known variance, or x ± λ0.025
σ0
√n.
This conﬁdence interval
is taught as having a conﬁdence level equalling the the probability of the
interval [x −λ0.025
σ0
√n, x + λ0.025
σ0
√n)] covering the unknown θ.
But from the point of the limit above the common erroneous but natural
interpretation of
P

x −λ0.025
σ0
√n ≤Θ ≤x + λ0.025
σ0
√n

as the probability of Θ lying in [a(x), b(x)], looks like being a correct one !
15

My friend and colleague Gunnar Englund, retired senior lecturer of math-
ematical statistics at KTH, has recently sent this to Editors of Scientiﬁc
American:
As a statistician I regrettably noted that again (for approximately
the umpteenth time) an otherwise excellent article is stained by
an incorrect interpretation of a so-called hypothesis testing. In
”Neutrinos at the ends of the earth” Francis Halzen states the
probability is greater than 99.9999 percent that these events tru-
ely come from deep space. This is again a case of fallacy of the
transposed conditional, i.e. the claim by Halzen is that the prob-
ability of the null hypothesis (the events do not come from deep
space) given the data is 0.0001 percent, while the correct inter-
pretation is that this is the probability of the data given the null
hypothesis. In fact, using a Bayesian approach a statement like
Halzen’s could be made, but that would presuppose an (arbitrary)
a priori probability that the theory is correct.
5
Probabilistic Models with Conditional In-
dependence
Next we study a family of models that explicitly considers a sequence of data.
Here we introduce the conditional independence as a part of the model. We
give a somewhat generic deﬁnition: X and Y are conditionally independent
given Z if and only if
P((X, Y ) | Z) = P(X | Z)P(Y | Z).
It follows from this deﬁnition that if X and Y are conditionally independent
given Z, then
P(Y | X, Z) = P(Y | Z).
We shall consider two examples of how and why Bayesian thinking uses this.
5.1
Modelling and Learning by Tosses of a Thumbtack
5.1.1
The Model Family
The mathematics involved here is found in greater detail in (v. Mises and
Geiringer 1964) or in (Heckerman 2008).
16

We consider a sequence of ﬂips of a thumbtack. If we throw a thumbtack
in the air, it will come to rest either on its point (0) or on its head (1).
Suppose we ﬂip the thumbtack n times (ﬁxing n in advance), making sure
that that the physical properties of the thumbtack and the conditions under
which it is ﬂipped remain stable over time. We let x denote the sequence of
outcomes of the ﬂips, x = xi1xi2 . . . xin, xil ∈{0, 1}. Let now Θ be a random
variable (quantity), whose values are numbers, denoted by θ, between zero
and one, 0 ≤θ ≤1. These values θ correspond to the possible values of the
chance of obtaining heads in tossing thumbtack .
MODEL FAMILY:
CONDITIONED ON Θ = θ, THE SYMBOLS IN x ARE INDEPENDENT.
Or more completely:
M = the symbols in x are outcomes of conditionally independent
Bernoulli random variables with the parameter Θ = θ, Θ = {0 ≤
θ ≤1}.
Note that this is diﬀerent from what you may have gathered from earlier
teaching, where θ is ﬁxed but unknown, has no probability distribution and
X1 . . . Xn are independent random variables.
Hence, in view of (2.9), for any a model in the family the observed x is
given the probability assignment
P (x | Θ = θ) =
n
Y
l=1
θxil · (1 −θ)1−xil =
θ
Pn
l=1 xil · (1 −θ)n−Pn
l=1 xil = θk · (1 −θ)n−k ,
(5.1)
if Pn
l=1 xil = k. In the thumbtack example we can understand learning as
follows. We have observed n outcomes of ﬂips of a thumbtack x and wish to
determine which of the models in the family that best describes this set of
ﬂips.
5.1.2
The Posterior Density
To progress with this we express our uncertainty about θ ∈Θ using a prior
π (θ), By (2.14) we get the posterior
π (θ | x) =
P (x | Θ = θ) · π (θ)
R 1
0 P (x | Θ = θ) · π (θ) dθ
, 0 ≤θ ≤1
(5.2)
17

and zero elsewhere.
The posterior π (θ | x) expresses our updated belief in the statement that
a certain θ is the true chance of obtaining heads given that we have observed
x.
One way to get further from here is to use a conjugate prior π (θ). This
is not the only useful choice, but at least analytically quite advantageous.
Let us consider the uniform prior (which is a Beta probability density, see
appendix 11.3) given by
π (θ) =
 1
0 ≤θ ≤1
0
elsewhere.
The uniform prior is often interpreted as a representation of complete igno-
rance about θ.
By an insertion we can calculate
Z 1
0
P (x | Θ = θ) · π (θ) dθ =
Z 1
0
θk · (1 −θ)n−k dθ = k!(n −k)!
(n + 1)!
by the Beta integral, see (A.10). Then we have
π (θ | x) =
(
(n+1)!
k!(n−k)! · θk (1 −θ)n−k
0 ≤θ ≤1
0
elsewhere.
(5.3)
This is a Beta density, see appendix 11.3. Hence the family of Beta densities
is conjugate priors to the likelihood in (5.1).
5.1.3
π (θ | x) evolves for x = 1101101110
Suppose we see a success (1 ↔S). What is posteriorin π(θ|S)? We write
x
F
S
pX(x|θ)
1 −θ
θ
so that likelihood × prior becomes
pX(S | θ)π(θ) = θ · 1.
Then the marginal data likelihood is
m(S) =
Z 1
0
pX(S | θ)π(θ)dθ =
Z 1
0
θdθ =
θ2
2
1
0
= 1
2.
18

Hence Bayes’ theorem gives
π(θ | S) =
 2θ
0 ≤θ ≤1
0
elsewhere.
The Figure (from Berry, Donald A: Bayesian clinical trials, Nature reviews
Drug discovery,5, pp. 27-36, 2006) is in twelve ﬁelds. In the topmost left
ﬁeld one plots the prior π(θ) and in the following (After S) to the right
π(θ | S) = 2θ is plotted.
Suppose we get again (S). What will be π(θ|SS)? We assume conditional
independence. Then we get
pX(SS | θ) = pX(S | θ) · pX(S | θ) = θ2
and
m(SS) =
Z 1
0
pX(SS | θ)π(θ)dθ =
Z 1
0
θ2dθ =
θ3
3
1
0
= 1
3
And Bayes’ theorem gives
π(θ | SS) =
 3θ2
0 ≤θ ≤1
0
elsewhere.
This is the curve in the second ﬁeld (Another S). We have a posterior skewed
to the right. Next outcome is F.
pX(SSF | θ) = pX(S | θ) · pX(S | θ) · pX(F | θ)
|
{z
}
=(1−θ)
= θ2(1 −θ).
Then
m(SSF) =
Z 1
0
pX(SSF | θ)π(θ)dθ =
Z 1
0
θ2(1 −θ)dθ =
=
Z 1
0
θ2dθ −
Z 1
0
θ3dθ = 1
3 −1
4 = 1
12
and by Bayes’ theorem we ﬁnd
π(θ | SSF) =
 12θ2(1 −θ)
0 ≤θ ≤1
0
elsewhere.
We can clearly continue like this to ﬁnd π (θ | x) for x = 1101101110 (coded
as SSFSSFSSSF).
19

The ﬁgure shows how π (θ | x) evolves for read from left to right and
starting from the ﬂat prior.
5.1.4
The Maximum Likelihood Estimate
To understand better the alluded properties of fΘ|x (θ | x) we consider the
maximum likelihood estimate bθML of θ from
bθML = argmax0≤θ≤1P (x | Θ = θ) = argmax0≤θ≤1θk · (1 −θ)n−k .
The rationale for this is that we try to ﬁnd the model within the family
that gives the (training) sequence x the highest possible probability. The
likelihood function is
L (θ) = P (x | Θ = θ) .
The likelihood function L (θ), as said above, compares the plausibilities of
diﬀerent models for given x. A maximization of the likelihood function gives
bθML = k
n.
(5.4)
20

5.2
The Bernstein −von Mises theorem
According to the Bernstein - von Mises theorem, the asymptotic distribution
of the MAP estimator depends for large data sets on the Fisher information
and not on the prior.
We need the deﬁnition of the Fisher information,
denoted by I (θ). We assume Θ is one dimensional. The integral
I (θ)
def
= −
Z
Rn
d2
dθ2 log f (x|θ) f (x | θ) dx
(5.5)
is called the Fisher information. We can establish for the thumbtack toss
model family in this section that
I (θ) =
1
θ · (1 −θ).
By a Taylor expansion of log P (x | Θ = θ), see section 14 for further heuristic
details around bθML we obtain something like the statement of the Bernstein
−von Mises theorem, or
π (θ | x) ≈e−1
2 nI(bθML)·(θ−bθML)
2
= e
−1
2
n
bθML·(1−bθML)·(θ−bθML)
2
.
(5.6)
We can empirically, say for x drawn from a pseudo random number generator,
plot the posterior density (5.3) as a function of θ and observe the property
(5.6), when the length of a string x increases. In fact this holds independently
of the prior density. This behaviour is clearly present in a typical simulation,
the posterior densities in the Figure are not properly standardized.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Posterior for 
    
θ
k=18, n= 27
k=5, n=7
21

5.3
Three MAPs for Bin(n, θ)
In all cases in this subsection X ∼Bin(n, θ) and thus from Example 2.3 for
k = {0, 1, . . . , n},
P (X = k|θ) =
 n
k

θk(1 −θ)n−k.
• Jeﬀreys’ Prior (c.f. Example 8.1 below) on Θ is
π (θ) = θ−1/2 (1 −θ)−1/2
B (1/2, 1/2)
.
Then
bθMAP = arg max
θ∈[0,1]
 θk−1/2(1 −θ)n−k−1/2
⇔
bθMAP = arg max
θ∈[0,1] [(k −1/2) log θ + (n −k −1/2) log(1 −θ)]
We set
φ (θ) = (k −1/2) log θ + (n −k −1/2) log(1 −θ).
Then by some simple calculus we see that
d
dθφ (θ) = 0 ⇔θ = k −1/2
n −1 ,
which clearly requires n ≥2 and k > 0. If n ≥2 and k = 0, the
solution is negative, and cannot be used. φ (θ) decreases to −∞as θ
approaches 0, and φ (θ) decreases to −∞as θ approaches 1 . Hence
bθ(1)
MAP = max
k −1/2
n −1 , 0

The special cases n = 2 and n = 1 can be treated by study of
φ (θ) = (k −1/2) log θ + (n −k −1/2) log(1 −θ)
22

• Haldane’s prior on Θ is
π (θ) = θ−1 (1 −θ)−1
φ (θ) = (k −1) log θ + (n −k −1) log(1 −θ)
Same kind of calculations as in the ﬁrst case yield
d
dθφ (θ) = 0 ⇔
θ = k −1
n −2,
which clearly requires n ≥3 and k > 0. By reasoning similar to the
one used above we get for n ≥3 and k ≥0
bθ(2)
MAP = max
k −1
n −2, 0

• Laplace’s prior is the uniform density on [0, 1], which we already have
studied.
φ (θ) = k log θ + (n −k) log(1 −θ)
d
dθφ (θ) = 0 ⇔
θ = k
n
which, as it should, equals also the maximum likelihood estimate, al-
ready stated above.
bθ(3)
MAP = k
n.
We summarize: If n > 2
bθ(1)
MAP = max
k −1/2
n −1 , 0

,
bθ(2)
MAP = max
k −1
n −2, 0

,
bθ(3)
MAP = k
n.
When n is large, then all three estimates are equivalent, as is expected by
the Bernstein -von Mises Theorem.
23

6
More on Modelling and Learning: Dirich-
let Priors and Posteriors
6.1
The Model Family
Let X1, X2, . . . , Xn be independent random variables assuming values in
X = {x1, · · · , xL}
with the common distribution
θl = P (Xi = xl) , l = 1, 2, . . . , L.
Hence θ1 + θ2 + . . . + θL = 1. Let x = xi1xi2 . . . xin be a string of symbols
from X and let for l = 1, 2, . . . , L
nl = the number of times the symbol xl is found in xi1xi2 . . . xin.
We set
θ = (θ1, θ2, . . . , θL)
and consider Θ as a multivariate random variable that assumes values in the
simplex
Θ = {θ | θ1 + θ2 + . . . + θL = 1, θl ≥0, l = 1, . . . , L}.
THE MODEL FAMILY M:
CONDITIONED ON Θ = θ ∈Θ, THE SYMBOLS IN x ARE INDEPENDENT.
Thus
P (x | θ) = θi1 · θi2 · · · θin = θn1
1 · θn2
2 · · · θnL
L .
Again we ﬁnd a prior π (θ) for Θ. Let us consider the Dirichlet prior
given by
π (θ) =
(
Γ(α)
QL
j=1 Γ(αqj)
QL
j=1 θ
αqj−1
j
θ ∈Θ
0
elsewhere,
where the hyperparameters are α > 0, qj ≥0, PL
j=1 qj = 1, Γ (z) is the Euler
gamma function as given in the appendix. The prior π(θ) is in (A.7) in the
appendix given the symbol
Dir (αq1, . . . , αqL) .
24

By extension of Bayes’ rule we get the posterior
π (θ|x; α) =
P (x | Θ = θ) · φ (θ)
R
Θ P (x | Θ = θ) · π (θ) dθ, θ ∈Θ
(6.7)
and zero elsewhere. Using the Dirichlet integral expounded in the appendix
we get
Proposition 6.1 The posterior density φΘ|x (θ|x; α) is a Dirichlet density
Dir (n1 + αq1, . . . , nL + αqL)
or
π (θ|x; α) =
Γ (n + α)
QL
i=1 Γ (αqi + ni)
L
Y
i=1
θni+αqi−1
i
.
(6.8)
The posterior density is in the same family of densities as the prior. Hence
the prior is closed under sampling or a conjugate prior.
6.2
Mean Posterior Estimate and a Rule of Succession
One useful property of the Dirichlet density is that we can compute explicitly
the expectation of any θi with respect to the posterior density. In fact this
expectation is by (A.9) and (6.8)
bθi =
Z
Θ
θiφ (θ1, . . . , θL|x; α) dθ1 . . . dθL = ni + αqi
n + α .
(6.9)
This result can be seen as a regularization adding pseudocounts αqi to the
vector of observed counts n and then normalising so that PL
i=1 bθi = 1. If we
have n = 0, the estimate is simply qi.
Here we may note that a way of referring to α in (6.9) is to talk about
the ﬂattening constant (Bender 1996, pp. 554 - 555). The ﬂattening constant
determines a linear interpolation between the maximum likelihood estimate
(see proposition 12.1 in section 12) ni
n of θi and the prior estimate qi. Hence α
has the interpretation as the degree of conﬁdence we distribute between the
data and the prior. The probability in (6.9) is known as a rule of succession.
25

7
Issues of Bayesian Learning (2): Bayes Fac-
tor & Bayesian Model Comparison/Selection
& Occam’s Razor
7.1
Bayes Factor
The Bayes factor compares posterior odds ratios to prior odds ratios of
model families without supposing that any model is true or false. This deals
with questions of the kind treated in hypothesis testing, but in particular
with model comparison.
Bayes factor was developed in statistics for the purpose of evaluating the
evidence in favor of a scientiﬁc theory i.e. hypothesis testing. Bayes factors
oﬀer a way of evaluating evidence in favour of a null hypothesis.
An example of model selection is determining the optimum level of com-
plexity (= number of hidden layers and the width of the layers) required to
develop an artiﬁcial neural network (ANN) for a given problem. This is a
diﬃcult task, and there is perhaps no generally accepted formal systematic
model selection method for ANNs, but Bayes factors or the evidence deﬁned
below have been tested, when augmented by additional checks of complexity.
Bayesian model comparison is a method of model selection based on Bayes
factors. Given that we are asked to choose between two parametric model
families M1 and M0 on the basis of observed data x, the plausibility of the
two diﬀerent model families, parametrised by model parameter vectors θ1
and θ0 is assessed by the Bayes factor Bπ
01.
We shall compare models based on the marginal likelihood a.k.a. (weight
of) the evidence for each model family, which is the probability the model
family assigns to the observed data.
We might choose the model that gives higher probability to the data, or
average predictions from both models with weights based on their marginal
likelihood. In the sequel x represents an observed data set x = {x1, . . . , xn}.
If the likelihood corresponding to the maximum likelihood estimate of the
parameter for each model is used, then we get a likelihood-ratio test.
The Bayes factor is deﬁned as the ratio of the posterior probabilities of
two model families M1 and M0 over the ratio of the prior probabilities of
26

the model families, i.e.,
Bπ
01 :=
Π(M0|x)
Π(M1|x)
Π(M0)
Π(M1)
= posterior odds ratio
prior odds ratio
We compute
Π (Mi|x) = p (x|Mi) · Π (Mi)
p (x)
,
where
p (x) =
1
X
i=0
p (x|Mi) · Π (Mi) .
The Bayes’ factor becomes
Bπ
01 := p (x|M0)
p (x|M1),
where the marginal likelihood or evidence is
p (x | Mi) =
Z
Θi
fi (x|θi) πi (θ) dθ.
In section 13.4 it is shown that the marginal likelihood has a minimizing
property of Kullback distances averaged over the prior. The following qual-
itative interpretation of a Bayes Factor Bπ
01 is known. For the unit dHart,
see section 13.1. dHart is a change in a weight of evidence about as ﬁne as
humans can reasonably be expected to quantify their degree of belief in a
model.
10 · log10 Bπ
01 = Bayes factor in dHart.
• < 0 [dHart]: negative
• 0 −5 [dHart]: is barely worth a mention
• 5 −10 [dHart]: is substantial
• 10 −15 [dHart]: is strong
• 15 −20 [dHart]: is very strong
• over a 20 [dHart]: is decisive evidence in favour of model family M0.
27

Values in dHart below 0 take the inverted interpretation in favour of
model family M1.
Example 7.1 We compute the Bayes factor (Bernardo and Smith (1994,
pp. 392−393) under two parametric model families
M0 :
Poisson distribution with Θ0 ∼Γ (α0, β0)
M1 :
Geometric Distribution with Θ1 ∼Be (α1, β1)
Geometric Distribution is the parametric model
f1 (x | θ1) = θ1 · (1 −θ1)x , x = 0, 1, 2, . . . ,
Theta1 = {θ | 0 ≤θ ≤1}.
xi|Θ1 = θ1 ∼f (x|θ1) , I.I.D. ,
x(n) = (x1, x2, . . . , xn)
f1
 x(n) | θ1

= θn
1 · (1 −θ1)
Pn
i=1 xi
Poisson distribution is the parametric model
f0 (x | θ0) = e−θ0 θx
0
x!, x = 0, 1, 2, . . . ,
Theta0 = {θ | 0 < θ}.
xi|Θ0 = θ0 ∼f0 (x|θ0) , I.I.D. ,
x(n) = (x1, x2, . . . , xn)
f0
 x(n) | θ0

= e−nθ0 θ
Pn
i=1 xi
0
Qn
i=1 xi!
If we consider single parameter values the Bayes ratio becomes a likelihood
ratio:
log f0
 x(n)|θ0

f1 (x(n)|θ1) =
n
X
i=1
xi log

θ0
1 −θ1

−n log θ1 −nθ0 −
n
X
i=1
log xi
Then the marginal likelihood of x(n) under M0 is
Z
Θ0
f0
 x(n) | θ0

π0 (θ0) dθ0 =
28

=
Z ∞
0
e−nθ0 θ
Pn
i=1 xi
0
Qn
i=1 xi!
βα0
0
Γ(α0)θα0−1
0
e−β0θ0dθ0
=
βα0
0
Γ(α0) Qn
i=1 xi!
Z ∞
0
e−(β0+n)θ0θ
Pn
i=1 xi+α0−1
0
dθ0
|
{z
}
=
Γ(α0+Pn
i=1 xi)
(n+β0)α0+Pn
i=1 xi
=
βα0
0
Γ(α0) Qn
i=1 xi!
Γ(α0 + Pn
i=1 xi)
(n + β0)α0+Pn
i=1 xi .
The marginal likelihood of x(n) under M1 is
Z
Θ1
f1
 x(n) | θ1

π1 (θ1) dθ1 =
=
Z 1
0
θn
1 · (1 −θ1)
Pn
i=1 xi Γ(α1 + β1)
Γ(α1)Γ(β1)θα1−1
1
(1 −θ1)β1−1dθ1
= Γ(α1 + β1)
Γ(α1)Γ(β1)
Z 1
0
θn+α1−1
1
(1 −θ1)
Pn
i=1 xi+β1−1dθ1
|
{z
}
=
Γ(n+α1)Γ(Pn
i=1 xi+β1)
Γ(n+Pn
i=1 xi+α1+β1)
= Γ(α1 + β1)
Γ(α1)Γ(β1)
Γ(n + α1)Γ(Pn
i=1 xi + β1)
Γ(n + Pn
i=1 xi + α1 + β1) .
Then Bayes’ factor becomes
Bπ
01 =
R
Θ0 f (x|θ0) π0 (θ0) dθ0
R
Θ1 f (x|θ1) π1 (θ1) dθ1
=
βα0
0
Γ(α0) Qn
i=1 xi!
Γ(α0+Pn
i=1 xi)
(n+β0)α0+Pn
i=1 xi
Γ(α1+β1)
Γ(α1)Γ(β1)
Γ(n+α1)Γ(Pn
i=1 xi+β1)
Γ(n+Pn
i=1 xi+α1+β1)
.
The Bayes factor in favour of Poisson distribution relative to Geometric dis-
tribution established above depends only on the data and the hyperparam-
eters (contrary to the ratio of likelihoods).
This is because all unknown
parameters are integrated out. All constants of integration are to be kept.
In likelihood ratio tests only those parts of the densities that depend on the
parameters are kept.
29

7.2
Bayesian Model Comparison & Occam’s Razor
In the fourteenth century, William of Ockham proposed:
Pluralitas non est ponenda sine neccesitate
which is by Latinists translated as entities should not be multiplied unneces-
sarily. Its original historic context was theological, but the (heuristic) princi-
ple remains relevant for machine learning today, where it is called Occam’s
Razor, and it might be rephrased as models should be no more complex than
is suﬃcient to explain the data, Tipping (2004) or Given multiple hypotheses
that are consistent with the data, the simplest should be preferred.
We quote Legg, Shane and Hutter, Marcus: Universal intelligence: A
deﬁnition of machine intelligence, Minds and Machines, 17, 391−444, 2007:
Consider the following type of question which commonly appears
in intelligence tests. There is a sequence such as 2, 4, 6, 8, and
the test subject needs to predict the next number.
Of course
the pattern is immediately clear: the numbers are increasing by
2 each time, or more mathematically, the kth item is given by
2k. An intelligent person would easily identify this pattern and
predict the next digit to be 10. However, the polynomial 2k4 −
20k3 + 70k2 −98k + 48 is also consistent with the data, in which
case the next number in the sequence would be 58. Why then,
even if we are aware of the larger polynomial, do we consider the
ﬁrst answer to be the most likely one? It is because we apply,
perhaps unconsciously, the principle of Occam,s razor. The fact
that intelligence tests deﬁne this as the ”correct” answer, shows
us that using Occam,s razor is considered the intelligent thing
to do. Thus, although we do not usually mention Occam,s razor
when deﬁning intelligence, the ability to eﬀectively use it is an
important facet of intelligent behaviour.
A more generally operational form of the razor is next derived from
Bayesian model comparison.
This is based on Bayes factors and can be
used to compare models that do not ﬁt the observations equally well. These
methods can sometimes optimally balance the complexity and power of a
model. The exact Occam factor is analytically intractable, but approxima-
tions such as BIC= Bayesian information criterion, and others, can be used
without diﬃculty.
30

7.3
Bayes factor has a built-in Occam’s Razor
7.3.1
A Picture
Suppose M0 is a more complex model than M1, in the sense that M0 has
more parameters. Then M0 will model the data more closely. The Bayes
factor is capable of making a trade-oﬀbetween good ﬁt and model complexity
(elaborated in, e.g., MacKay (1992)).
Bayes ratio measures, intuitively speaking, the proportion of how much
the model family predicted the data that occurred. A simple model family
makes a limited range of predictions, a more complex family M0 will be
able to over-ﬁt a small number of cases to a huge number of models, or ﬁt
a greater variety of data sets but with low probability p (x|M0), hence the
less complex model family will make p (x|M1) high in a small region of data
and is there the more probable model.
 
C  
P(x|     ) 
x  
P(x|    ) 
    
  
M
0 
M 1 
Evidence 
The ﬁgure, from MacKay (1992), another is found in Tipping (2004),
gives an intuition of how complex models are penalized. The horizontal axis
represents the space of all possible data sets x. Bayes factor rewards models
31

in proportion to how much they predicted the data that occurred, which is
quantized by the evidence p (x|M0). A simple model family M1 makes only
a limited range of predictions. A more powerful model family M0 has more
free parameters than M1 and is able to predict a greater variety of data sets.
This means, however, that M0 does not predict the data sets in region C
with as strong evidence as M1. In the region C a complex model family M0
with many parameters, each of which is free to vary over a large range △θ0
0,
will be penalized with a small Bayes factor w.r.t. a simpler model family.
7.3.2
Mathematical Approximation, BIC
Some piece of mathematical approximation can be added. The integration
method of Laplace is a technique used to approximate integrals of the form
Z b
a
eMf(x) dx,
when we assume that the function f(x) has a unique global maximum at x0
and is twice diﬀerentiable. Main contributions to the integral of f(x) will
come only from points x in a neighbourhood of x0, the size of which can be
estimated. The formula obtained is
Z b
a
eMf(x) dx ≈
s
2π
M|f ′′(x0)|eMf(x0) as M →∞.
To apply this idea we write
Z
Θi
f (x|θi) πi (θ) dθ =
Z
Θi
eln f(x|θi)πi(θ)dθ
and make series expansions of ln f (x|θi) πi (θ), c.f. Appendix 14.
Thus, the integrand in the evidence can be approximated by the height
of the peak of the integral at the most probable parameters, bθMAP, times its
width, △θi.
Z
Θi
f (x|θi) πi (θ) dθ ≈fi

x|bθi,MAP

πi

bθi,MAP

△θi.
Or
p (x|Mi)
|
{z
}
Evidence
≈
fi

x|bθi,MAP

|
{z
}
Best ﬁt likelihood
πi

bθi,MAP

△θi
|
{z
}
Occam factor
.
32

The quantity △θi is the posterior uncertainty in θ. Suppose that πi

bθi,MAP

=
1
△θ0
i . Here △θ0
i represents the range of values of θ that Mi thought possible
before data arrived. Then
Occam factor = △θi
△θ0
i
.
This is seen as the ratio of the posterior accessible volume of Θi to the
prior accessible volume or as the factor by which Θ collapses when the data
arrive. The model family Mi is composed of a certain number of equivalent
submodels, of which only one survives after the data arrive. The Occam
factor is the inverse of that number. The log of the Occam factor can be
interpreted as the amount of information we gain about the model family
when the data arrive.
The Bayes’ factor is now written as
Bπ
01 = p (x|M0)
p (x|M1) ≈
f

x|bθ0,MAP

f

x|bθ1,MAP
 · △θ0
△θ0
0
△θ0
1
△θ1
.
BIC=Bayesian Information Criterion
By some tedious multivariable calculus it can be demonstrated that the
logarithm of evidence can be well approximated by
ln p (x | Mi) = ln f

x|bθi

−|θi|
2 ln n,
(7.1)
where bθi is the maximum likelihoood estimate of θi and |θi| is the dimen-
sion of θi, i.e. the number of parameters in the vector θi and n is got
from x = {x1, . . . , xn}.
The expression in the right hand side of (7.2) is the BIC=Bayesian Infor-
mation Criterion,
BICi
def
= ln f

x|bθi

−|θi|
2 ln n.
(7.2)
The criterion BICi is maximized to ﬁnd the best model family amongst a
ﬁnite number of model families Mi.
Here we see that a complex model
33

giving good ﬁt to data will yield a high value of ln f

x|bθi

, but this is
punished/counterbalanced by the term |θi|
2 ln n.
8
Issues of Bayesian Learning (3):Quantiﬁca-
tion of the Prior
The angry controversies around the Bayesian procedures concern amongst
other things the nature of the prior probability density π(θ). The diﬃculty
is a subjective judgement being merged into data analysis that is expected to
be an objective science without arbitrary elements. Recall Gunnar Englund’s
words above
. . . In fact, using a Bayesian approach a statement like Halzen’s
could be made, but that would presuppose an (arbitrary) a priori
probability that the theory is correct.
8.1
How do we choose π (θ) ?
• Conjugate prior
• Non-informative or reference prior:
Reference priors produce objective Bayesian inference, in the sense that
inferential statements depend only on the assumed model and the avail-
able data, and the prior distribution used to make an inference is least
informative (in a certain information-theoretic sense). Reference pri-
ors have been rigorously deﬁned in speciﬁc contexts and heuristically
deﬁned in general.
– Laplace’s prior
– Jeﬀreys, prior: Let I (θ) be the Fisher information (5.5) of a para-
metric model. Take the prior density as
π(θ)
def
=
p
I (θ)
R p
I (θ)dθ
,
assuming the integral exists. This prior is invariant to monotonous
transformations of θ.
34

Example 8.1 It turns out that Jeﬀreys, prior for θ in binomial
likelihood is obtained by taking α = 1/2 and β = 1/2 in
π(p) =
(
Γ(α+β)
Γ(α)Γ(β)pα−1(1 −p)β−1
0 < p < 1
0
elsewhere.
and with Γ(1/2 + 1/2) = 1, Γ(1/2) = √π,
πJeffreys(p) =

1
πp−1/2(1 −p)−1/2
0 < p < 1
0
elsewhere.
• Maximum entropy prior (Jaynes 2003). E.T. Jaynes (2003) regards the
Bayesian reasoning as the very logic of science.
One form of Bayesian learning relies upon a personalistic theory of prob-
ability for quantiﬁcation of prior knowledge. In such a theory
• probability measures the conﬁdence that a particular individual (asses-
sor) has in the truth of a particular proposition
• no attempt is made to specify which assessments are correct
• personal probabilities should satisfy certain postulates of coherence.
Personal probabilities do follow the ordinary rules for probability cal-
culus. The validity of these rules expresses to the self-consistency in
terms of responses to various betting propositions. Consistent decisions
can be obtained from these probabilities.
There has been a development of diverse methods of assessing personalistic
probability:
1. Choice of prior distributions: questionnaires R.L.Winkler (work pub-
lished in
• Robert L. Winkler:
The Assessment of Prior Distributions in
Bayesian Analysis
Journal of the American Statistical Association, Vol. 62, No. 319.
(Sep., 1967), pp. 776-800.)
35

devises questionnaires (or interviews) to elicit information to write
down a prior distribution.
Students of University of Chicago were
asked to, e.g., assess the uncertainty about the probability of a ran-
domly chosen student of University of Chicago being Roman Catholic
using a probability distribution. The assessment was done by four dif-
ferent methods, like giving fractiles, making bets, assessing impact of
additional data, drawing graphs. One interesting ﬁnding is that the
assessments by the same person using diﬀerent methods may be con-
ﬂicting.
2. Assessing Priors: Conjugate Prior The interviews by Winkler were
mathematically speaking all concerned with assessing the prior of θ
in a Bernoulli Be (θ) −I.I.D. process.
Winkler claims a sensitivity
analysis (loc.cit p. 791) showing that the prior distributions assessed
by the interviews yielded posterior distributions that were ‘only little’
diﬀerent (by a test of goodness-of-ﬁt) from those obtained from Beta
densities on θ.
3. Choice of prior distributions by elicitation
• A. O
′Hagan: Eliciting Expert Beliefs in Substantial Practical Ap-
plications. The Statistician , 47, pp. 21−35, 1998.
Not only priors are elicited in
• L. Gingnell, U. Franke, R. Lagerstr¨om, E. Ericsson, J. Lilliesk¨old:
Quantifying Success Factors for IT Projects - An Expert-Based
Bayesian Model. Information Systems Management, pp. 21−−36,
2014
• R.L. Keeney & D. von Winterfeldt:
Eliciting Probabilities in
Complex Technical Problems. IEEE Transactions on Engineer-
ing Management, 38, pp.191−201, 1991.
4. Choice of prior distributions by assessment can evolve rapidly to a topic
of research in cognitive science and/or economic behaviour, c.f.,
• C-A. S. Stael von Holstein: Assessment and Evaluation of Sub-
jective Probability Distributions. 1970, Stockholm School of Eco-
nomics.
36

• A. G. Wilson: Cognitive factors aﬀecting subjective probability as-
sessments.
ISDS Discussion Paper # 94-02,
http://www.isds.duke.edu/
9
On the fundamental task of statistical learn-
ing
9.1
General Outline
It has been said that the fundamental task of statistical learning is that of
passing from one set of data or observations S = x to express an opinion
about another, as yet unobserved set T = y. We shall handle this topic
in terms of the predictive probability density or predictive probability mass
function
g (y|x) =
Z
Θ
f (y|θ) π (θ | x) dθ.
(9.1)
This section will give an extensive derivation and argument to justify g (y|x)
as the expression we desire for. We illustrate by an example the meaning of
the issue to be treated.
Example 9.1 (The Probability for the Outcome of the Next Toss)
In the thumbtack model we may be concerned with
P (Xn+1 = head|x) ,
if Xn+1 is a random variable modelling the next toss T , given n ﬂips of the
thumbtack as recorded in S = x. In other words, P (Xn+1 = head|x) can be
found by an integral of the form (9.1). For this we shall rerun some of the
machinery in the thumbtack model family in a context given by Th. Bayes
himself.
The book Bishop (2006) explains how (9.1) is used in training neural net-
works.
37

9.2
Predictive Distribution
9.2.1
Formal manipulations
We assume a parametric model with continuous variables and joint distribu-
tion (x, y, θ) ∼φ (x, y, θ) so that:
φ (x, y) =
Z
Θ
φ (x, y, θ) dθ.
The aim is to derive (9.1) from this. We set
m(x) =
Z Z
Θ
φ (x, y, θ) dθdy.
The conditional distribution g(y|x) is then
g (y|x) = φ(x, y)
m(x) =
R
Θ φ (x, y, θ) dθ
m(x)
.
By successive applications of the deﬁnition of conditional density
φ(x, y)
m(x) =
R
Θ φ (x, y, θ) dθ
m(x)
=
R
Θ g (y|x, θ) ψ (x, θ) dθ
m(x)
=
R
Θ g (y|x, θ) f (x|θ) π (θ) dθ
m(x)
.
Here
m(x) =
Z Z
Θ
φ (x, y, θ) dθdy =
=
Z
Θ
Z
g (y|x, θ) dyf (x|θ) π (θ) dθ =
Z
Θ
f (x|θ) π (θ) dθ,
since g (y|x, θ) is a probability density. Thus
g (y|x) =
R
Θ g (y|x, θ) f (x|θ) π (θ) dθ
m(x)
=
R
Θ g (y|x, θ) f (x|θ) π (θ) dθ
R
Θ f (x | θ) · π (θ) dθ
=
Z
Θ
g (y|x, θ)
f (x|θ) π (θ)
R
Θ f (x | θ) · π (θ) dθdθ.
Here we recognize Bayes, rule. Thus
g (y|x) =
Z
Θ
g (y|x, θ) π (θ | x) dθ.
38

If y and x are conditionally independent given Θ = θ, then
g (y|x, θ) = g (y|θ)
and
g (y|x) =
Z
Θ
g (y|θ) π (θ | x) dθ.
If we assume that g (y|θ) = f (y|θ), then we get the ﬁnal formula as expected
in (9.1). We give a name to what we have dealt with.
[(Posterior) Predictive Distribution]
g (y|x) =
Z
Θ
f (y|θ) π (θ | x) dθ.
(9.2)
For some of us, this is one of the best illustrations of the importance of Bayes,
rule. To claim this in a more acute manner, we consider updates.
9.2.2
The probability of the next data point
We introduce an ordering of data, like, e.g., in a sequence.
x = x(n) =
(x1, . . . , xn) and y = xn+1.
g
 xn+1|x(n)
=
Z
g
 xn+1|x(n), θ

π
 θ | x(n)
dθ.
We assume conditional independence w.r.t. Θ = θ
φ (x, y|θ) = φ
 x(n), xn+1|θ

= (f (x1|θ) · · · f (xn|θ)) · f (xn+1|θ)
= φ
 x(n)|θ

· f (xn+1|θ)
φ
 x(n), xn+1 | θ

= φ
 x(n)|θ

· f (xn+1|θ) .
Then
g
 xn+1|x(n), θ

= φ
 x(n), xn+1 | θ

φ (x(n)|θ)
= f (xn+1|θ)
and
g
 xn+1|x(n)
=
Z
Θ
f (xn+1|θ) π
 θ | x(n)
dθ.
Note that there was conditional independence w.r.t. Θ = θ, but xn+1 and
x(n) are no longer independent in g
 xn+1|x(n)
.
We can learn something
about xn+1 from x(n).
39

9.2.3
Updates of Posterior
In order to update g
 xn+1|x(n)
for the next data point, i.e. to get g
 xn+2|x(n+1)
,
it suﬃces to update π
 θ | x(n)
. The ﬁrst expression for up-date of posterior
distribution is
π
 θ | x(n+1)
=
f
 x(n+1)|θ

π (θ)
R
Θ f (x(n+1) | θ) · π (θ) dθ
=
f (xn+1|θ) f (x1|θ) · · · f (xn|θ) π (θ)
R
f (xn+1|θ) f (x1|θ) · · · f (xn|θ) · π (θ) dθ
=
f (xn+1|θ) f(x1|θ)···f(xn|θ)π(θ)
m(x(n))
R
Θ f (xn+1|θ) f(x1|θ)···f(xn|θ)π(θ)
m(x(n))
dθ
=
f (xn+1|θ) π
 θ|x(n)
R
Θ f (xn+1|θ) π (θ|x(n)) dθ.
The ﬁnal up-date of posterior distribution is thus
π
 θ | x(n+1)
=
f (xn+1|θ) π
 θ|x(n)
R
Θ f (xn+1|θ) π (θ|x(n)) dθ
Hence, under the assumptions made, we can update posterior distribution
in a sequential manner when new data points accrue. Or, we can use the
posterior of π
 θ|x(n)
as a new prior for computing π
 θ | x(n+1)
.
Example 9.2 f(x | θ) is the density of N (µ, σ2), the mean µ and variance
σ2 are unknown, i.e., θ = (µ, σ2). The (improper) prior density is taken as
π (θ) ∝dµ 1
σdσ.
Let x = xn =
 x(1), . . . , x(n)
, x(i) ∼N (µ, σ2). We have the estimates bµ
= 1
n
Pn
i=1 x(i), and s2 =
1
n−1
Pn
i=1
 x(i) −bµ
2. The predictive density is
g
 x(n+1)|xn

=
r
n
(n2 −1)π ·
Γ
  n
2

Γ
  1
2(n −1)s
 ·
 
1 + n
 x(n+1) −bµ
2
(n2 −1)s2
!−n/2
is known in Bayesian statistics as ’t-like distribution’1 .
1J.M. Dickey (1968): Three Multidimensional-Integral Identities with Bayesian Appli-
cations. The Annals of Mathematical Statistics, 39, pp. 1615−1628.
40

9.3
Bayes’ Billiard Ball
9.3.1
White Ball and Orange Balls
A billiard ball W is rolled on a line of length one, with a uniform probability
of stopping anywhere. It stops at θ, not disclosed to us. A second ball O
is rolled n times under the same assumptions and X denotes the number of
times O stops to the left of W. Given X = x, what can we learn about θ?
(In the ﬁgure x ↔θ.) It is, however, not perfectly clear what questions the
good reverent was set to study by this arrangement.
We let Θ be a random variable, with values in 0 ≤θ ≤1. Conditionally
on Θ = θ, the rolls are taken as outcomes of I.I.D Be(θ) R.V’s. Hence for
x = 0, 1, 2, . . . , n,
f(x|θ) = P (X = x | Θ = θ)
=
 n
x

θx · (1 −θ)n−x ,
41

which is the Binomial distribution. We introduce the Beta prior Beta(α, β)
π(θ) =
(
Γ(α+β)
Γ(α)Γ(β)θα−1(1 −θ)β−1
0 < θ < 1
0
elsewhere,
and
B (α, β) := Γ(α)Γ(β)
Γ(α + β) .
α > 0 and β > 0 are the hyperparameters. α = β = 1 yields the uniform
distribution on (0, 1). Then it follows that
π (θ | x) =
(
1
B(x+α,n−x+β) · θx+α−1 (1 −θ)β+n−x−1
0 ≤θ ≤1
0
elsewhere.
This is the Beta density Beta(α+x, β+n−x). Note that we get the posterior
in (5.2) when α = β = 1.
9.3.2
What did Bayes want to ﬁnd out?
There is an interpretation of Bayes, work claiming that the problem really
attacked and solved by Bayes was: What should π(θ) be so that
m(x) =
Z 1
0
f (x | θ) · π(θ)dθ =
1
(n + 1)
(9.3)
holds for the billiard balls. One can check that if the prior is Beta(1, 1), the
uniform distribution on [0, 1], then
m(x) =
Z 1
0
f (x | θ) · dθ =
 n
x
 x!(n −x)!
(n + 1)!
=
n!
x!(n −x)!
x!(n −x)!
(n + 1)!
=
1
(n + 1).
The more diﬃcult mathematical question is the show that if (9.3) is true,
then the prior must be Beta(1, 1). If this was actually what Th. Bayes proved,
then his approach to ﬁnding priors was quite diﬀerent from the approaches
of his followers mentioned in section 8.1 above.
42

9.4
Next rolls of the Bayes Orange Ball
The predictive distribution of y positions of O left of W in r additional rolls
is with uniform prior density π(θ) given by
g (y|x) =
Z 1
0
f (y|θ) π (θ | x) dp
=
 r
y
 Z 1
0
θy · (1 −θ)r−y π (θ | x) dθ
y = 0, 1, . . . , r
Z 1
0
θy · (1 −θ)r−y π (θ | x) dθ =
Z 1
0
θy · (1 −p)r−y
1
B(x + 1, n −x + 1) · px (1 −θ)n−x dp =
=
1
B(x + 1, n −x + 1)
Z 1
0
θy+x · (1 −θ)r+n−y−x dθ =
= B(y + x + 1, r + n −x −y + 1)
B(x + 1, n −x + 1)
.
by the Beta integral. In other words
g(y|x; r) =
 r
y
 Z 1
0
θy · (1 −θ)r−y π (θ | x) dθ
=
 r
y
 B(y + x + 1, r + n −x −y + 1)
B(x + 1, n −x + 1)
.
=
r!
(r −y)!y!
Γ(n + 2)Γ(y + x + 1)Γ(r + n −x −y + 1)
Γ(x + 1)Γ(n −x + 1)Γ(r + n + 2)
.
Hence the predictive distribution for y = 1 in the next (r = 1) roll of
Bayes’ Ball is nothing but
g(1|x; 1) = Γ(x + 2)Γ(n −x + 1)Γ(n + 2)
Γ(x + 1)Γ(n −x + 1)Γ(n + 3)
= (x + 1)!(n −x)!(n + 1)!
x!(n −x)!(n + 2)!
= x + 1
n + 2
43

This is (notoriously) famous predictive probability, known as Laplace
′s rule
of succession,
x + 1
n + 2 =
x + 1
(n + 1) + 1
The prior knowledge can be translated into equivalent sample information.
We know that
bθML = x
n
If you observed x = 0, would you believe in the estimate bθ = 0 for all future
purposes ? The predictive probability found above
x + 1
n + 2 =
x + 1
(n + 1) + 1
is a maximum likelihood estimate of θ when n + 1 rolls of the ball O and the
ﬁrst roll of the ball W are included in the data.
Wilson (1927) suggests for the thumbtack model a diﬀerent rule of suc-
cession
bθW = k + α2/2
n + α2 .
(9.4)
Wilson says that the value of α depends on ‘our readiness to to gamble on
the typicalness of our experience’.
10
Priors on Perceptrons
10.1
Bayesian Version of the Computational Learning
Theory (Buntine 1992)
We are given a training set S = {zi = (x(i), y(i)), i = 1, ..., n}, where x ∈X is
an input and y ∈Y. This is drawn from an unknown probability distribution
PZ = PX,Y .
We are given a ﬁxed set H of functions h, X
h7→Y. H is called the hypoth-
esis space. The task of learning is to ﬁnd the function h∗, that performs best
on new, yet unseen patterns z = (x, y) drawn from PZ = PX,Y in the sense
of predicting y from x. The loss function penalizing the prediction error is
l(h(x), y). (Here l does not necessarily refer to loglikelihood). The
44

• The training error of h is
Remp(h)
def
= 1
m
n
X
i=1
l(h
 x(i)
, y(i)).
• The generalization error of h is
R(h)
def
= E [l(h(X), Y )] .
where the expectation is w.r.t. PZ = PX,Y .
Remp(h) provides an estimate of R(h). The posterior probability of the hy-
pothesis h given the training set is
P(H = h | S) = P(Y (n) = y(n) | X(n) = x(n), H = h)P(X(n) = x(n))P(H = h)
C(S)
.
The Bayes classiﬁcation strategy is to minimize the expected loss computed
according to P(H = h | zn). Or,
BayesS(x) = argminy∈YEH|S [l(H(x), y)]
Note that this strategy does not correspond to any ﬁxed h, and is therefore
computationally demanding. Let A denote a generic algorithm for learning
h. It can be proven, Herbrich et.al. (2001), that the Bayes classiﬁcation
strategy minimizes the average generalization error
Rm(A)
def
= EH

EZm|H=h

EX

EY |X=x,H=h [l (A(S)(x), Y )]

.
under some not so hard conditions. Bayes point machine minimizes
ABp(S)(z) = argminh∈HEX

EH|S [l(h(X), H(X))]

and is, under some conditions, a good approximation of the Bayes classiﬁca-
tion strategy.
10.2
Bayesian Learning of Perceptrons
We recall from Lecture 1.
f (x) =≪w, x ≫+b.
45

The function f determines a half plane in the Euclidean space Rp
D = {x ∈Rp | f (x) = 0}
which divides the space into two half spaces, so that R1 and R2, as
R+1 = {x ∈Rp | f (x) > 0}
R−1 = {x ∈Rp | f (x) < 0}
The perceptron rule of classiﬁcation is to assign y = +1 to x ∈R1 and
y = −1 to x ∈R−1.
Next we follow an idea due to Radford M. Neal. It is known that the
vector c in the hyperlane D that is closest to the origin, 0, is given by
c = (−b)
∥w∥2w.
The vector c determines the orientation of the hyperplane uniquely. Thus
we can write
f (x) =≪(−b)
∥w∥2w, x ≫+b.
But then the separating hyperplane is
D = {x ∈X |≪w, x ≫= 1}
We can additionally take ∥w∥= 1 by the scaling freedom (Lecture 1). Here
H = {x 7→sign(≪w, x ≫−1) |≪w, x ≫= 1}.
We believe that there exists a a hyperplane so that S is linearly separable
(Lecture 1). The Bayesian standpoint asks for a prior probability distribution
on H. By the nature of the situation it suﬃces to put a prior on vectors w
such that ≪w, w ≫= 1, or, equivalently a probability distribution on the
vector c closest to origin. We can take a random vector u in the unit circle,
and r uniformly distributed in [0, a], or c = ru. Thus our prior expresses the
judgement that the separating hyperplane is no further than the distance a
from the origin.
The parametric model family is given by
P(y(i) = y|x(i), u, w) =
 1
if yu(≪w, x(i) ≫−1) > 0
0
if yu(≪w, x(i) ≫−1) < 0
46

where u ∈{−1, +1} and w are unknown parameters of the model.
The
likelihood function is
n
Y
i=1
P(y(i) = y|x(i), u, w)
=
 1
if yu(≪w, x(i) ≫−1) > 0 for i = 1, . . . , n
0
if yu(≪w, x(i) ≫−1) < 0 otherwise.
The posteriori for u ∈{−1, +1} and w is the same as their prior except
that the parameter values incompatible with the data are eliminated. After
normalizing the posterior probabilities, the parameter values compatible with
the data will have higher probability than they did in the prior.
The predictive probability of y∗with x∗is
Pxast(y∗= +1 | S) =
Z
X
u=±1
Pxast(y∗= +1 | u, w)P(u, w | S)dw.
Using a sample of m values from the posterior (w(i), u(i)), i = 1, ..., m} we
can approximate this as
Pxast(y∗= +1 | S) ≈1
m
m
X
j=1
Pxast(y∗= +1 | u(i), w(i)).
The average is just the fraction of lines drawn from the posterior that would
put (xast, y∗) in the class +1.
The Bayes point machine wBp would seem to correspond the vector max-
imizing the posterior on w.
Some simulations....(in progress).
47

Appendices for Technical Details
11
About Dirichlet Densities
11.1
Euler’s gamma function
The gamma function Γ(z) is deﬁned for complex numbers z, whose real part
is positive, by the deﬁnite integral
Γ(z) =
Z ∞
0
xz−1e−xdx.
(A.1)
A special case, obtained by the substitution x = u2/2 is
Γ
1
2

= √π.
The recursion formula is
Γ(z) = (z −1)Γ(z −1).
(A.2)
Hence, if z = n, where n is a positive integer, we have the factorial
Γ(n) = (n −1)!.
(A.3)
11.2
The Dirichlet density
Let Θ ⊂RL be the simplex
Θ =
(
(θ1, . . . , θL) |θi ≥0, i = 1, . . . , L,
L
X
i=1
θi = 1
)
.
(A.4)
Let for αi > 0
φ (θ1, . . . , θL) =
( QL
i=1 θαi−1
i
Z
,
if θ1, . . . , θL ∈Θ
0
otherwise.
(A.5)
Here
1
Z =
Γ
PL
i=1 αi

QL
i=1 Γ (αi)
.
(A.6)
48

The density φ (θ1, . . . , θL) is called a Dirichlet density. We designate it sym-
bolically by
Dir (α1, . . . , αL) .
(A.7)
If α1 = α2 = . . . = αL = κ, then we talk about a symmetric Dirichlet density.
For the proof of the fact that
Z
Θ
φ (θ1, . . . , θL) dθ1 . . . dθL = 1.
(A.8)
This means also that
Z
Θ
L
Y
i=1
θαi−1
i
dθ1 . . . dθL =
QL
i=1 Γ (αi)
Γ
PL
i=1 αi
.
(A.9)
(Gupta and Richards 1987) is a concise compendium of knowledge about the
Dirichlet distributions.
11.3
Beta density
As a special case for L = 2 we obtain in (A.9) the Beta integral
Z 1
0
θα1−1(1 −θ)α2−1dθ = Γ (α1) · Γ (α2)
Γ (α1 + α2) .
(A.10)
Thus
f (θ) =
(
Γ(α1+α2)
Γ(α1)·Γ(α2)θα1−1 (1 −θ)α2−1
0 ≤θ ≤1
0
elsewhere.
(A.11)
is a probability density called the Beta density and denoted by
Be (θ; α1, α2) .
Note the diﬀerence in the heuristic notation between Beta and Bernoulli
Be(p). If θ = (θ1, . . . , θL) is a random variable that assumes values in Θ in
(A.4) and has the symmetric Dir (α, . . . , α) distribution, then the marginal
density of any θi is given by
θi ∈Be (θ; α, (L −1)α) .
(A.12)
49

12
Maximum Likelihood for a Finite Table
of Probabilities
The maximum likelihood estimate of θ (a ﬁnite table of probabilities) is by
a familiar principle given by
bθML = argmaxθ∈ΘP (x | θ) = argmaxθ∈Θθn1
1 · θn2
2 · · · θnL
L .
The presence of Θ imposes a constrained problem of maximization. We take
the natural logarithm of P (x | θ), which gives us the loglikelihood function
l (θ1, θ2, . . . , θL) = log P (x | θ)
We may equivalently seek the maximum of l (θ1, θ2, . . . , θL). Since the con-
straint θ1 + θ2 + . . . + θL = 1 must be met, we consider the new auxiliary
function in L −1 free variables
el (θ1, θ2, . . . , θL−1) = l (θ1, θ2, . . . , 1 −(θ1 + θ2 + . . . + θL−1)) .
This gives
el (θ1, θ2, . . . , θL−1) = n1·log θ1+n2·log θ2+. . .+nL·log (1 −(θ1 + θ2 + . . . + θL−1)) .
Vi diﬀerentiate partially el (θ1, θ2, . . . , θL−1) with respect to θ1, θ2, . . . , θL−1
and set the partial derivatives equal to zero. This gives us the system of
equations
∂
∂θ1
el (θ1, θ2, . . . , θL−1) = n1
θ1
−
nL
1 −(θ1 + θ2 + . . . + θL−1) = 0,
...
∂
∂θL−1
el (θ1, θ2, . . . , θL−1) = nL−1
θL−1
−
nL
1 −(θ1 + θ2 + . . . + θL−1) = 0.
This leads to the equalities
n1
θ1
= n2
θ2
= . . . =
nL
1 −(θ1 + θ2 + . . . + θL−1).
Let us denote the common value of these ratios as λ so that
θ1 = n1
λ , θ2 = n2
λ , . . . , θL = nL
λ .
50

We determine λ from the constraint θ1 + θ2 + . . . + θL = 1, which gives
1 = θ1 + θ2 + . . . + θL = n1
λ + n2
λ + . . . + nL
λ
or
λ = n1 + n2 + . . . + nL = n.
Hence we have obtained the solution to ∇el (θ1, θ2, . . . , θL−1) = 0 written in a
component wise form as
bθi = ni
n , i = 1, . . . , L.
Strictly taken we have yet to prove that this yields a maximum. For this we
could check the matrix of second order partial derivatives of el, (Khuri 1993
p. 283). There is a more instructive way to prove that the estimate found
above actually gives the maximum. In fact the proof of the next proposition
shows that it is not even necessary to diﬀerentiate to prove that we have
found the maximum likelihood estimate.
Proposition 12.1 The maximum likelihood estimate bθML of θ is
bθML =
n1
n , n2
n , . . . , nL
n

.
Proof: Clearly the candidate solution bθML belongs to Θ and is thus admis-
sible. Since P (x | θ) = QL
i=1 θni
i , the following identity is evident
H

bθML

= −1
n log P

x | bθML

,
(B.1)
where
H

bθML

= −
L
X
i=1
bθi log bθi
(B.2)
is the (empirical) Shannon’s entropy in nats.
Next we take an arbitrary θ in Θ. Then we have in view of (B.2) for an
arbitrary θ in Θ another evident identity
P (x | θ) =
L
Y
i=1
θni
i = e−n(D(bθML|θ)+H(bθML)).
(B.3)
51

Here we have used the Kullback distance between two discrete probability
distributions deﬁned as
D (f|g) =
X
x∈X
f(x) log f(x)
g(x).
Thus from (B.1) and (B.3)
P

x | bθML

P (x | θ)
= e−nH(bθML) · en(D(bθML|θ)+nH(bθML)) =
= enD(bθML|θ) ≥1,
where the last inequality follows due to the fact that D

bθML|θ

is the Kull-
back distance, which is known to be nonnegative. Equality holds if and only
if bθML = θ. Thus
P

x | bθML

≥P (x | θ)
for every θ in Θ and the assertion is proved.
13
Kullback Distance
13.1
Entropy, shannon and dHart
We deﬁne the entropy of a discrete r.v. X as
H(X) := −
L
X
i=1
fX(xi) log(fX(xi))
(C.1)
This has the dimension [bits/symbol] if logarithms to the base 2 are used.
Example 13.1 ( Binary entropy function) For the special case X = {x1, x2},
with p := fX(x1),
h(p) := −p log2(p) −(1 −p) log2(1 −p)
(C.2)
is the (binary) entropy function.
This is also the entropy of a Bernoulli
random variable X ∈Be(p) with X = {0, 1}).
The unit of information
52

is shannon and corresponds to the binary entropy of X ∈Be(1/2). The
shannon (symbol Sh) is a unit deﬁned by IEC 80000-132.
Example 13.2 ( Uniform Distribution on the Ten Digits ) Let X as-
sume the ten digits as values with equal probabilities. Then
H(X) = −10
 1
10 log2
1
10

= log2 10 = 3.32[Sh]
(C.3)
dHart is a measure information or entropy, using logarithms on the base 10.
Hence, when X assumes the ten digits as values with equal probabilities, i.e.
fX(xi) =
1
10 for i = 1, . . . , 10, the dHart is
dHart(X) = −
L
X
i=1
fX(xi) log10(fX(xi)) = log10 10 = 1.
Thus one dHart equals 3.32 [Sh].
13.2
Deﬁnition and Examples
The entropy H(X) is a measure of the uncertainty in bits (=binary infor-
mation units) of the random variable X. It is also a lower bound for the
number of bits (binary digits) needed on the average to describe the random
variable.
Now we introduce the relative entropy
based on the distribution g(x)
when the ”true” distribution is f(x). The relative entropy is then deﬁned as
follows.
Deﬁnition 13.1 Let X = {x1, · · · , xL} be an alphabet and let
f := (f(x1), · · · , f(xL))
and
g := (g(x1), · · · , g(xL))
2IEC 80000-13 is an international standard that deﬁnes quantities and units used in
information science, and speciﬁes names and symbols for these quantities and units.
53

be two probability distributions deﬁned on X. Then their relative entropy or
the Kullback distance between f and g is deﬁned by
D (f | g) =
L
X
i=1
f(xi) log f(xi)
g(xi).
(C.4)
Here we use the conventions 0 · log
0
g(xi) = 0 and f(xi) log f(xi)
0
= ∞. The
logarithm is the natural logarithm unless otherwise stated. Note that this
is not a distance in the sense of being a metric, since e.g. the symmetry
property of a metric obviously need not hold.
Example 13.3 ( Information Content) If X is a random variable with
the distribution f = (f(x1), · · · , f(xL)), any probability distribution on an
alphabet of L symbols, and if g = (1/L, · · · , 1/L) is the uniform distribution,
then
D (f | g) = log L −H(X).
(C.5)
This quantity is sometimes known as the information content (of f).
Example 13.4 (Two Bernoulli distributions) Let X = {0, 1} and 0 ≤
p ≤1 and 0 ≤g ≤1. Let f = (1 −p, p) and g = (1 −g, g) be the two
Bernoulli distributions Be(p) and Be(g), respectively. Then
D (f | g) = (1 −p) · log 1 −p
1 −g + p · log p
g.
(C.6)
We can also rewrite this as
D (f | g) = −(1 −p) · log(1 −g) −p · log g −h(p),
where h(p) is the entropy function (C.2) in natural logarithm.
54

13.3
Properties
Next we give a general proof of the important fact that the relative entropy
is nonnegative.
Proposition 13.5 For any probability distributions f and g on the same
alphabet
D (f | g) ≥0.
(C.7)
Proof: Let X be a random variable that has the distribution f. We write
p(x) and g(x) for the generic values of the probability of x ∈X in the two
distributions. Then we have
D (f | g) = E

log p (X)
g (X)

and this equals
D (f | g) = −E

log g (X)
p (X)

.
Since φ(x) = −log x is a convex function we have that
−E

log g (X)
p (X)

≥−log E
g (X)
p (X)

,
where we have used Jensen’s inequality. But
E
g (X)
p (X)

=
L
X
i=1
f(xi)g(xi)
f(xi) = 1
and since log 1 = 0, we have proved our assertion.
Since D (f | g) ≥0 we get by (C.5) that
H(X) ≤log L.
13.4
The marginal data likelihood and the Kullback
Distance, Aitchison (1975)
Let
Rπ (q) =
Z
Θ
D
 N
Y
i=1
p
 x(i) | θ

| q
!
dπ(θ)
55

and deﬁne
m∗= arg min
q
Rw (q) .
(C.8)
Then the marginal data likelihood is the minimizer, or,
m∗ x(1), . . . , x(N)
=
Z
Θ
N
Y
i=1
p
 x(i) | θ

dπ(θ).
(C.9)
To prove this we write
Rπ (q) =
Z
Θ
D
 N
Y
i=1
p
 x(i) | θ

| m∗
!
dπ(θ)
+
Z
Θ
Z
x∈X N
N
Y
i=1
p
 x(i) | θ

ln
m∗(x)
q(x)

dxdπ(θ),
=
Z
Θ
D
 N
Y
i=1
p
 x(i) | θ

| m∗
!
dπ(θ)+
Z
x∈X N
"Z
Θ
N
Y
i=1
p
 x(i) | θ

dπ(θ)
#
ln
m∗(x)
q(x)

dx
=
Z
Θ
D
 N
Y
i=1
p
 x(i) | θ

| m∗
!
dπ(θ) +
Z
x∈X N m∗(x) ln
m∗(x)
q(x)

dx.
Thus the conclusion follows.
14
Asymptotic Shape of the Likelihood
Parametric Statistical Model, n I.I.D. |θ rv’s
xi|θ ∈f (x|θ) , I.I.D. ,
or independent, identically, distributed conditional on θ
x(n) = (x1, x2, . . . , xn) ∈X n
f (x|θ) is a probability density on Rp. f (x|θ) is a known function of x and θ.
Let us assume that f(x|θ) is a density with a scalar parameter (for simplicity
56

of notation), and that f(x|θ) is some k ≥2 times diﬀerentiable in θ. We let
bθML be the maximum likelihood estimate of θ. We expand the log likelihood
function around bθML
log f
 x(n)|θ

=
log f

x(n)|bθML

+

θ −bθML
 d
dθ log f

x(n)|bθML

+1
2

θ −bθML
2 d2
dθ2 log f

x(n)|bθML

+ Rn (θ)
But here bθML is a solution of the equation
d
dθ log f

x(n)|bθML

= 0
Hence
log f
 x(n)|θ

=
1
2

θ −bθML
2 d2
dθ2 log f

x(n)|bθML

+ Rn (θ) .
We have by assumption of conditionally I.I.D. data
d2
dθ2 log f
 x(n)|θ

=
n
X
l=1
d2
dθ2 log f (xl|θ) .
We set Yl =
d2
dθ2 log f (xl|θ). Then the Law of Large Numbers says that
1
n
n
X
l=1
Yl →E [Y ] ,
as n →∞,
where
E [Y ] =
Z
X
d2
dθ2 log f (x|θ) f (x | θ) dx.
The integral
I (θ) = −
Z
X
d2
dθ2 log f (x|θ) f (x | θ) dx
is called the Fisher information. Then we may feel inclined to take that
d2
dθ2 log f

x(n)|bθML

=
n
X
l=1
d2
dθ2 log f

xl|bθML

≈−n · I

bθML

.
57

Note that even bθML depends on n. This gives
log f
 x(n)|θ

≈log f

x(n)|bθML

−1
2

θ −bθML
2
n · I

bθML

.
The ﬁrst term does not involve θ. Then
f
 x(n)|θ

≈e−n
2(θ−bθML)
2·I(bθML).
The interpretation of the relation is that the likelihood function can be for
large n be approximated by a normal density for which the mean is bθML and
the variance is
1
nI(bθML).
15
References and further reading:
1 Journal articles and technical reports on Bayesian learning/machine learning and Dirich-
let distribution:
• J. Aitchison (1975): Goodness of prediction ﬁt. Biometrika,62, pp. 547−181.
• W.L. Buntine (1992) A theory of learning classiﬁcation rules, PhD Thesis.
School of Computing Science, Sydney University of Technology, Sydney.
• P. Cheeseman (1988): An Inquiry into Computer Understanding. Computa-
tional Intelligence, 4, 58−66.
• J.M. Dickey (1983): Multiple Hypergeometric Functions: Probabilistic Inter-
pretation and Statistical Uses. Journal of the American Statistical Associa-
tion, 78, pp. 628−637.
• R.D.
Gupta and D.St.P. Richards (1987): Multivariate Liouville Distribu-
tions. Journal of Multivariate Analysis, 23, pp. 232−256.
• D. Heckerman (2008): A tutorial on learning with Bayesian networks, in
Innovations in Bayesian Networks, pp. 33−82, Springer, Berlin.
• R. Herbrich, T. Graepel and C. Campbell (2001):Bayes point machines. The
Journal of Machine Learning Research, 1, 245−279,
• D.J.C. MacKay (1992): Bayesian methods for adaptive models, PhD Thesis,
California Institute of Technology.
• D.V.
Lindley (1970): A non-frequentist view of probability and statistics.
The Teaching of Probability & Statistics, L. R˚ade editor, Almqvist & Wicksell,
Uppsala, pp. 209−222.
• J. Rissanen (1997): Stochastic complexity and learning. Journal of Computer
and System Sciences, 55, pp. 89−95.
58

• H.V. Roberts (1965): Probabilistic Prediction. Journal of the American Sta-
tistical Association, 60, pp. 50−62.
• M. Sunn˚aker, A. Busetto, G. Alberto, E. Numminen, J. Corander, M. Foll,
and C. Dessimoz, (2013): Approximate bayesian computation. PLoS Com-
putational Biology, 9, pp. e1002803
• M. E. Tipping: Bayesian Inference: An Introduction to Principles and Prac-
tice in Machine Learning.
pp. 49−70, in O. Bousquet, U. von Luxburg,
G. R¨atsch: (ed.s) Advanced Lectures on Machine Learning, Lecture Notes
in Artiﬁcial Intelligence Vol. 3176, 2004.
• E.B. Wilson (1927): Probable inference, the law of succession, and statistical
inference. Journal of the American Statistical Association, 22, pp. 209−212.
2 Books:
• D. Barber (2012): Bayesian Reasoning and Machine Learning. Cambridge
University Press, Cambridge, U.K.
• J.M. Bernardo and A.F.M. Smith (1994): Bayesian Theory. John Wiley and
Sons, Chichester, New York, Brisbane, Toronto and Singapore.
• C.M. Bishop (2006): Pattern recognition and machine learning, 2006, Springer
Berlin.
• P. Gregory (2005): Bayesian Logical Data Analysis for the Physical Sciences.
Cambridge University Press, Cambridge, U.K..
• E.T. Jaynes (2003): Probability theory: the logic of science, Cambridge Uni-
versity Press, Cambridge, U.K.
• D. Lunn, C. Jackson, N. Best, A. Thomas and D. Spiegelhalter (2013): The
BUGS book: A practical introduction to Bayesian analysis, CRC Press, Boca
Raton, London, New York.
• D.J. MacKay (2003):
Information Theory, Inference and Learning Algo-
rithms, Cambridge University Press, Cambridge, U.K..
• R. v. Mises with H. Geiringer (1964): Mathematical Theory of Probability and
Statistics. Academic Press, New York and London.
3 Software:
• BUGS (Bayesian inference Using Gibbs Sampling), software for the Bayesian
analysis of complex statistical models using Markov chain Monte Carlo (McMC)
methods. http://www.mrc-bsu.cam.ac.uk/software/bugs/
59

