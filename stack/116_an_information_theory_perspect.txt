An Information-Theoretic Perspective on
Variance-Invariance-Covariance Regularization
Ravid Shwartz-Ziv∗
New York University
Randall Balestriero
Meta AI, FAIR
Kenji Kawaguchi
National University of Singapore
Tim G. J. Rudner
New York University
Yann LeCun
New York University & Meta AI, FAIR
Abstract
Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised learn-
ing (SSL) method that has shown promising results on a variety of tasks. However,
the fundamental mechanisms underlying VICReg remain unexplored. In this paper,
we present an information-theoretic perspective on the VICReg objective. We
begin by deriving information-theoretic quantities for deterministic networks as an
alternative to unrealistic stochastic network assumptions. We then relate the opti-
mization of the VICReg objective to mutual information optimization, highlighting
underlying assumptions and facilitating a constructive comparison with other SSL
algorithms and derive a generalization bound for VICReg, revealing its inherent
advantages for downstream tasks. Building on these results, we introduce a family
of SSL methods derived from information-theoretic principles that outperform
existing SSL techniques.
1
Introduction
Self-supervised learning (SSL) is a promising approach to extracting meaningful representations by
optimizing a surrogate objective between inputs and self-generated signals. For example, Variance-
Invariance-Covariance Regularization (VICReg) [7], a widely-used SSL algorithm employing a
de-correlation mechanism, circumvents learning trivial solutions by applying variance and covariance
regularization.
Once the surrogate objective is optimized, the pre-trained model can be used as a feature extractor
for a variety of downstream supervised tasks such as image classification, object detection, instance
segmentation, or pose estimation [15, 16, 48, 67]. Despite the promising results demonstrated by
SSL methods, the theoretical underpinnings explaining their efficacy continue to be the subject of
investigation [5, 42].
Information theory has proved a useful tool for improving our understanding of deep neural networks
(DNNs), having a significant impact on both applications in representation learning [3] and theoretical
explorations [60, 72]. However, applications of information-theoretic principles to SSL have made
unrealistic assumptions, making many existing information-theoretic approaches to SSL of limited
use. One such assumption is to assume that the DNN to be optimized is stochastic—an assumption
that is violated for the vast majority DNNs used in practice. For a comprehensive review on this topic,
refer to the work by Shwartz-Ziv and LeCun [62].
∗Correspondence to: ravid.shwartz.ziv@nyu.edu.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

In this paper, we examine Variance-Invariance-Covariance Regularization (VICReg), an SSL method
developed for deterministic DNNs, from an information-theoretic perspective. We propose an
approach that addresses the challenge of mutual information estimation in deterministic networks by
transitioning the randomness from the networks to the input data—a more plausible assumption. This
shift allows us to apply an information-theoretic analysis to deterministic networks. To establish a
connection between the VICReg objective and information maximization, we identify and empirically
validate the necessary assumptions. Building on this analysis, we describe differences between
different SSL algorithms from an information-theoretic perspective and propose a new family of
plug-in methods for SSL. This new family of methods leverages existing information estimators
and achieves state-of-the-art predictive performance across several benchmarking tasks. Finally,
we derive a generalization bound that links information optimization and the VICReg objective to
downstream task performance, underscoring the advantages of VICReg.
Our key contributions are summarized as follows:
1. We introduce a novel approach for studying deterministic deep neural networks from an
information-theoretic perspective by shifting the stochasticity from the networks to the in-
puts using the Data Distribution Hypothesis (Section 3)
2. We establish a connection between the VICReg objective and information-theoretic optimization,
using this relationship to elucidate the underlying assumptions of the objective and compare it
to other SSL methods (Section 4).
3. We propose a family of information-theoretic SSL methods, grounded in our analysis, that
achieve state-of-the-art performance (Section 5).
4. We derive a generalization bound that directly links VICReg to downstream task generalization,
further emphasizing its practical advantages over other SSL methods (Section 6).
2
Background & Preliminaries
We first introduce the necessary technical background for our analysis.
2.1
Continuous Piecewise Affine (CPA) Mappings.
A rich class of functions emerges from piecewise polynomials: spline operators. In short, given a
partition Ωof a domain RD, a spline of order k is a mapping defined by a polynomial of order k
on each region ω ∈Ωwith continuity constraints on the entire domain for the derivatives of order
0,...,k −1. As we will focus on affine splines (k = 1), we only define this case for clarity. A
K-dimensional affine spline f produces its output via f(z) = P
ω∈Ω(Aωz + bω)1{z∈ω}, with input
z ∈RD and Aω ∈RK×D, bω ∈RK, ∀ω ∈Ωthe per-region slope and offset parameters respectively,
with the key constraint that the entire mapping is continuous over the domain f ∈C0(RD).
Deep Neural Networks as CPA Mappings.
A deep neural network (DNN) is a (non-linear)
operator fΘ with parameters Θ that map a input x ∈RD to a prediction y ∈RK. The precise
definitions of DNN operators can be found in Goodfellow et al. [27]. To avoid cluttering notation, we
will omit Θ unless needed for clarity. For our analysis, we only assume that the non-linearities in
the DNN are CPA mappings—as is the case with (leaky-) ReLU, absolute value, and max-pooling
operators. The entire input-output mapping then becomes a CPA spline with an implicit partition Ω,
the function of the weights and architecture of the network [51, 6]. For smooth nonlinearities, our
results hold using a first-order Taylor approximation argument.
2.2
Self-Supervised Learning.
SSL is a set of techniques that learn representation functions from unlabeled data, which can then
be adapted to various downstream tasks. While supervised learning relies on labeled data, SSL
formulates a proxy objective using self-generated signals. The challenge in SSL is to learn useful
representations without labels. It aims to avoid trivial solutions where the model maps all inputs
to a constant output [11, 36]. To address this, SSL utilizes several strategies. Contrastive methods
like SimCLR and its InfoNCE criterion learn representations by distinguishing positive and negative
examples [16, 54]. In contrast, non-contrastive methods apply regularization techniques to prevent
the collapse [14, 17, 28].
2

Variance-Invariance-Covariance Regularization (VICReg).
A widely used SSL method for
training joint embedding architectures [7]. Its loss objective is composed of three terms: the invariance
loss, the variance loss, and the covariance loss:
• Invariance loss: The invariance loss is given by the mean-squared Euclidean distance between
pairs of embedding and ensures consistency between the representation of the original and
augmented inputs.
• Regularization: The regularization term consists of two loss terms: the variance loss—a hinge
loss to maintain the standard deviation (over a batch) of each variable of the embedding—and
the covariance loss, which penalizes off-diagonal coefficients of the covariance matrix of the
embeddings to foster decorrelation among features.
VICReg generates two batches of embeddings,
Z
=
[f(x1), . . . , f(xB)] and Z′
=
[f(x′
1), . . . , f(x′
B)], each of size (B × K). Here, xi and x′
i are two distinct random augmen-
tations of a sample Ii. The covariance matrix C ∈RK×K is obtained from [Z, Z′]. The VICReg
loss can thus be expressed as follows:
L = 1
K
K
X
k=1

α max

0, γ −
p
Ck,k + ϵ

+β
X
k′̸=k
(Ck,k′)2


|
{z
}
Regularization
+ η∥Z −Z′∥2
F /N
|
{z
}
Invariance
.
(1)
2.3
Deep Neural Networks and Information Theory
Recently, information-theoretic methods have played an essential role in advancing deep learning by
developing and applying information-theoretic estimators and learning principles to DNN training [3,
9, 34, 56, 64, 65, 69, 72]. However, information-theoretic objectives for deterministic DNNs often
face a common issue: the mutual information between the input and the DNN representation is
infinite. This leads to ill-posed optimization problems.
Several strategies have been suggested to address this challenge. One involves using stochastic DNNs
with variational bounds, where the output of the deterministic network is used as the parameters of
the conditional distribution [43, 61]. Another approach, as suggested by Dubois et al. [22], assumes
that the randomness of data augmentation among the two views is the primary source of stochasticity
in the network. However, these methods assume that randomness comes from the DNN, contrary to
common practice. Other research has presumed a random input but has made no assumptions about
the network’s representation distribution properties. Instead, it relies on general lower bounds to
analyze the objective [71, 75].
3
Self-Supervised Learning in DNNs: An Information-Theoretic Perspective
To analyze information within deterministic networks, we first need to establish an information-
theoretic perspective on SSL (Section 3.1). Subsequently, we utilize the Data Distribution Hypothesis
(Section 3.2) to demonstrate its applicability to deterministic SSL networks.
3.1
Self-Supervised Learning from an Information-Theoretic Viewpoint
Our discussion begins with the MultiView InfoMax principle, which aims to maximize the mutual
information I(Z; X′) between a view X′ and the second representation Z. As demonstrated in
Federici et al. [24], we can optimize this information by employing the following lower bound:
I(Z, X′) = H(Z) −H(Z|X′) ≥H(Z) + Ex′[log q(z|x′)].
(2)
Here, H(Z) represents the entropy of Z. In supervised learning, the labels Y remain fixed, making
the entropy term H(Y ) a constant. Consequently, the optimization is solely focused on the log-loss,
Ex′[log q(z|x′)], which could be either cross-entropy or square loss.
However, for joint embedding networks, a degenerate solution can emerge, where all outputs “collapse”
into an undesired value [16]. Upon examining Equation (2), we observe that the entropies are not
fixed and can be optimized. As a result, minimizing the log loss alone can lead the representations to
collapse into a trivial solution and must be regularized.
3

3.2
Understanding the Data Distribution Hypothesis
Previously, we mentioned that a naive analysis might suggest that the information in deterministic
DNNs is infinite. To address this point, we investigate whether assuming a dataset is a mixture
of Gaussians with non-overlapping support can provide a manageable distribution over the neural
network outputs. This assumption is less restrictive compared to assuming that the neural network
itself is stochastic, as it concerns the generative process of the data, not the model and training process.
For a detailed discussion about the limitations of assuming stochastic networks and a comparison
between stochastic networks vs stochastic input, see Appendix N. In Section 4.2, we verify that this
assumption holds for real-world datasets.
The so-called manifold hypothesis allows us to treat any point as a Gaussian random variable with a
low-rank covariance matrix aligned with the data’s manifold tangent space [25], which enables us
to examine the conditioning of a latent representation with respect to the mean of the observation,
i.e., X|x∗∼N(x; x∗, Σx∗). Here, the eigenvectors of Σx∗align with the tangent space of the data
manifold at x∗, which varies with the position of x∗in space. In this setting, a dataset is considered a
collection of distinct points x∗
n, n = 1, ..., N, and the full data distribution is expressed as a sum of
Gaussian densities with low-rank covariance, defined as:
X ∼
N
X
n=1
N(x∗
n, Σx∗n)I{T =n}
with
T ∼Cat(N).
(3)
Here, T is a uniform Categorical random variable. For simplicity, we assume that the effective
support of N(x∗
i , Σx∗
i ) do not overlap (for empirical validation of this assumption see Section 4.2).
The effective support is defined as {x ∈RD : p(x) > ϵ}. We can then approximate the density
function as follows:
p(x) ≈N

x; x∗
n(x), Σx∗
n(x)

/N,
(4)
where N (x; ., .) is the Gaussian density at x and n(x) = arg minn(x −x∗
n)T Σx∗n(x −x∗
n).
3.3
Data Distribution Under the Deep Neural Network Transformation
Let us consider an affine spline operator f, as illustrated in Section 2.1, which maps a space of
dimension D to a space of dimension K, where K ≥D. The image or the span of this mapping is
expressed as follows:
Im(f) ≜{f(x) : x ∈RD} =
[
ω∈ΩAff(ω; Aω, bω)
(5)
In this equation, Aff(ω; Aω, bω) = {Aωx + bω : x ∈ω} denotes the affine transformation of
region ω by the per-region parameters Aω, bω. Ωdenotes the partition of the input space where x
resides. To practically compute the per-region affine mapping, we set Aω to the Jacobian matrix of
the network at the corresponding input x, and b to be defined as f(x) −Aωx. Therefore, the DNN
mapping composed of affine transformations on each input space partition region ω ∈Ωbased on the
coordinate change induced by Aω and the shift induced by bω.
When the input space is associated with a density distribution, this density is transformed by the
mapping f. Calculating the density of f(X) is generally intractable. However, under the disjoint
support assumption from Section 3.2, we can arbitrarily increase the density’s representation power
by raising the number of prototypes N. As a result, each Gaussian’s support is contained within the
region ω where its means lie, leading to the following theorem:
Theorem 1. Given the setting of Equation (4), the unconditional DNN output density, Z, can be
approximated as a mixture of the affinely transformed distributions x|x∗
n(x):
Z ∼
N
X
n=1
N

Aω(x∗n)x∗
n + bω(x∗n), AT
ω(x∗
n)Σx∗nAω(x∗n)
1{T =n},
where ω(x∗
n) = ω ∈Ω⇐⇒x∗
n ∈ω is the partition region in which the prototype x∗
n lives in.
Proof. See Appendix B.
In other words, Theorem 1 implies that when the input noise is small, we can simplify the
conditional output density to a single Gaussian: (Z′|X′ = xn) ∼N (µ(xn), Σ(xn)) , where
µ(xn) = Aω(xn)xn + bω(xn) and Σ(xn) = AT
ω(xn)ΣxnAω(xn).
4

0.0
0.2
0.4
0.6
0.8
Normalized Input Noise (σ)
0.01
0.10
0.20
0.30
p-value
→ More Gaussian
SimCLR
VICReg
SwAV
MNIST
CIFAR10
CIFAR100
Flowers102
Food101
FGVCAircraft
0
25
50
75
100
125
150
175
l2 Distances
Figure 1: Left: The network output for SSL training is more Gaussian for small input noise. The
p-value of the normality test for different SSL models trained on ImageNet for different input noise
levels. The dashed line represents the point at which the null hypothesis (Gaussian distribution) can
be rejected with 99% confidence. Right: The Gaussians around each point are not overlapping.
The plots show the l2 distances between raw images for different datasets. As can be seen, the
distances are largest for more complex real-world datasets.
4
Information Optimization and the VICReg Optimization Objective
Building on our earlier discussion, we used the Data Distribution Hypothesis to model the conditional
output in deterministic networks as a Gaussian mixture. This allowed us to frame the SSL training
objective as maximizing the mutual information, I(Z; X′) and I(Z′; X).
However, in general, this mutual information is intractable. Therefore, we will use our derivation for
the network’s representation to obtain a tractable variational approximation using the expected loss,
which we can optimize.
The computation of expected loss requires us to marginalize the stochasticity in the output. We can
employ maximum likelihood estimation with a Gaussian observation model. For computing the
expected loss over x′ samples, we must marginalize the stochasticity in Z′. This procedure implies that
the conditional decoder adheres to a Gaussian distribution: (Z|X′ = xn) ∼N(µ(xn), I + Σ(xn)).
However, calculating the expected log loss over samples of Z is challenging. We thus focus on a
lower bound – the expected log loss over Z′ samples. Utilizing Jensen’s inequality, we derive the
following lower bound:
Ex′ [log q(z|x′)] ≥Ez′|x′ [log q(z|z′)] = 1
2(d log 2π −(z −µ(x′))2 −Tr log Σ(x′)).
(6)
Taking the expectation over Z, we get
Ez|x

Ez′|x′ [log q(z|z′)]

= 1
2(d log 2π −(µ(x) −µ(x′))2 −log (|Σ(x)| · |Σ(x′)|)).
(7)
Combining all of the above, we obtain
I(Z; X′) ≥H(Z) + d
2 log 2π −1
2Ex,x′[(µ(x) −µ(x′))2 + log(|Σ(x)| · |Σ(x′)|)].
(8)
The full derivations are presented in Appendix A. To optimize this objective in practice, we approxi-
mate p(x, x′) using the empirical data distribution
L(x1 . . . xN, x′
1 . . . x′
N) ≈1
N
N
X
i=1
H(Z) −log (|Σ(xi)| · |Σ(x′
i)|)
|
{z
}
Regularizer
−1
2 (µ(xi) −µ(x′
i))2
|
{z
}
Invariance
.
(9)
5

4.1
Variance-Invariance-Covariance Regularization: An Information-Theoretic Perspective
Next, we connect the VICReg to our information-theoretic-based objective. The “invariance term”
in Equation (9), which pushes augmentations from the same image closer together, is the same term
used in the VICReg objective. However, the computation of the regularization term poses a significant
challenge. Entropy estimation is a well-established problem within information theory, with Gaussian
mixture densities often used for representation. Yet, the differential entropy of Gaussian mixtures
lacks a closed-form solution [55].
A straightforward method for approximating entropy involves capturing the distribution’s first two
moments, which provides an upper bound on the entropy. However, minimizing an upper bound
doesn’t necessarily optimize the original objective. Despite reported success from minimizing an
upper bound [46, 53], this approach may induce instability during the training process.
Let ΣZ denote the covariance matrix of Z. We utilize the first two moments to approximate the
entropy we aim to maximize. Because the invariance term appears in the same form as the original
VICReg objective, we will look only at the regularizer. Consequently, we get the approximation
¯L(x1 . . . xN, x′
1 . . . x′
N) ≈
N
X
i=1
log |ΣZ(x1 . . . xN)|
|Σ(xi)| · |Σ(x′
i)|.
(10)
Theorem 2. Assuming that the eigenvalues of Σ(xi) and Σ(x′
i), along with the differences between
the Gaussian means µ(xi) and µ(x′
i), are bounded, the solution to the maximization problem
max
ΣZ
( N
X
i=1
log |ΣZ(x1 . . . xN)|
|Σ(xi)| · |Σ(x′
i)|
)
(11)
involves setting ΣZ to a diagonal matrix.
Proof. See Appendix J.
According to Theorem 2, we can maximize Equation (10) by diagonalizing the covariance matrix and
increasing its diagonal elements. This goal can be achieved by minimizing the off-diagonal elements
of ΣZ–the covariance criterion of VICReg–and by maximizing the sum of the log of its diagonal
elements. While this approach is straightforward and efficient, it does have a drawback: the diagonal
values could tend towards zero, potentially causing instability during logarithm computations. A
solution to this issue is to use an upper bound and directly compute the sum of the diagonal elements,
resulting in the variance term of VICReg. This establishes the link between our information-theoretic
objective and the three key components of VICReg.
4.2
Empirical Validation of Assumptions About Data Distributions
Validating our theory, we tested if the conditional output density P(Z|X) becomes a Gaussian as
input noise lessens. We used a ResNet-50 model trained with SimCLR or VICReg objectives on
CIFAR-10, CIFAR-100, and ImageNet datasets. We sampled 512 Gaussian samples for each image
from the test dataset, examining whether each sample remained Gaussian in the DNN’s penultimate
layer. We applied the D’Agostino and Pearson’s test to ascertain the validity of this assumption [19].
Figure 1 (left) displays the p-value as a function of the normalized std. For low noise levels, we reject
that the network’s conditional output density is non-Gaussian with an 85% probability when using
VICReg. However, the network output deviates from Gaussian as the input noise increases.
Next, we verified our assumption of non-overlapping effective support in the model’s data distribution.
We calculate the distribution of pairwise l2 distances between images across seven datasets: MNIST
[41], CIFAR10, CIFAR100 [40], Flowers102 [52], Food101 [12], and FGVAircaft [45]. Figure 1
(right) reveals that the pairwise distances are far from zero, even for raw pixels. This implies that we
can use a small Gaussian around each point without overlap, validating our assumption as realistic.
6

5
Self-Supervised Learning Models through Information Maximization
The practical application of Equation (8) involves several key “design choices”. We begin by
comparing how existing SSL models have implemented it, investigating the estimators used, and
discussing the implications of their assumptions. Subsequently, we introduce new methods for SSL
that incorporate sophisticated estimators from the field of information theory, which outperform
current approaches.
5.1
VICReg vs. SimCLR
In order to evaluate their underlying assumptions and strategies for information maximization, we
compare VICReg to contrastive SSL methods such as SimCLR along with non-contrastive methods
like BYOL and SimSiam.
Contrastive Learning with SimCLR.
In their work, Lee et al. [43] drew a connection between the
SimCLR objective and the variational bound on information regarding representations by employing
the von Mises-Fisher distribution. By applying our analysis for information in deterministic networks
with their work, we compare the main differences between SimCLR and VICReg:
1e0
1e1
1e2
1e3
Steps
3
6
9
H(Z)
Sim CLR
VICReg
BYOL
VICReg+ LogDet
VICReg+ PairDist
Figure 2: VICReg has higher Entropy during
training. The entropy along the training for dif-
ferent SSL methods. Experiments were conducted
with ResNet-18 on CIFAR-10. Error bars represent
one standard error over 5 trials.
(i) Conditional distribution: SimCLR assumes
a von Mises-Fisher distribution for the encoder,
while VICReg assumes a Gaussian distribu-
tion. (ii) Entropy estimation: SimCLR approx-
imated it based on the finite sum of the input
samples. In contrast, VICReg estimates the en-
tropy of Z solely based on the second moment.
Developing SSL methods that integrate these
two distinctions form an intriguing direction for
future research.
Empirical comparison.
We trained ResNet-
18 on CIFAR-10 for VICReg, SimCLR, and
BYOL and compared their entropies directly
using the pairwise distances entropy estimator.
(For more details, see Appendix K.) This esti-
mator was not directly optimized by any method
and was an independent validation. The results
(Figure 2), showed that entropy increased for all
methods during training, with SimCLR having
the lowest and VICReg the highest entropy.
5.2
Family of alternative Entropy Estimators
Next, we suggest integrating the invariance term of current SSL methods with plug-in methods that
optimize entropy.
Entropy estimators.
The VICReg objective seeks to approximate the log determinant of the
empirical covariance matrix through its diagonal terms. As discussed in Section 4.1, this approach
has its drawbacks. An alternative is to employ different entropy estimators. The LogDet Entropy
Estimator [74] is one such option, offering a tighter upper bound. This estimator employs the
differential entropy of α order with scaled noise and has been previously shown to be a tight estimator
for high-dimensional features, proving robust to random noise. However, since this estimator
provides an upper bound on entropy, maximizing this bound doesn’t guarantee optimization of
the original objective. To counteract this, we also introduce a lower bound estimator based on the
pairwise distances of individual mixture components [38]. In this family, a function determining
pairwise distances between component densities is designated for each member. These estimators are
computationally efficient and typically straightforward to optimize. For additional entropy estimators,
see Appendix F. Beyond VICReg, these methods can serve as plug-in estimators for numerous SSL
algorithms. Apart from VICReg, we also conducted experiments integrating these estimators with
the BYOL algorithm.
7

Table 1: The proposed entropy estimators outperform previous methods. CIFAR-10, CIFAR-100,
and Tiny-ImageNet top-1 accuracy under linear evaluation using ResNet-18, ConvNetX and VIT as
backbones. Error bars correspond to one standard error over three trials.
Method
CIFAR-10
Tiny-ImageNet
CIFAR-100
ResNet-18
ConvNetX
VIT
ConvNetX
VIT
SimCLR
89.72 ± 0.05
50.86 ± 0.13
51.16 ± 0.13
67.21 ± 0.24
67.31 ± 0.18
Barlow Twins
88.81 ± 0.10
51.34 ± 0.10
51.40 ± 0.16
68.54 ± 0.15
68.02 ± 0.12
SwAV
89.12 ± 0.13
50.76 ± 0.14
51.54 ± 0.20
68.93 ± 0.14
67.89 ± 0.21
MoCo
89.46 ± 0.08
52.36 ± 0.21
53.06 ± 0.21
70.32 ± 0.15
69.89 ± 0.14
VICReg
89.32 ± 0.09
51.02 ± 0.26
52.12 ± 0.25
70.09 ± 0.20
70.12 ± 0.17
BYOL
89.21 ± 0.11
52.24 ± 0.17
53.44 ± 0.20
70.01 ± 0.27
69.59 ± 0.22
VICReg + PairDist (ours)
90.37 ± 0.09
52.61 ± 0.15
53.70 ± 0.13
71.10 ± 0.16
70.50 ± 0.19
VICReg + LogDet (ours)
90.27 ± 0.08
52.91 ± 0.17
54.89 ± 0.20
71.23 ± 0.18
70.61 ± 0.17
BYOL + PairDist (ours)
90.19 ± 0.14
53.47 ± 0.22
54.33 ± 0.21
71.39 ± 0.25
71.09 ± 0.24
BYOL + LogDet (ours)
90.11 ± 0.16
53.19 ± 0.25
54.67 ± 0.27
71.20 ± 0.21
70.79 ± 0.26
Setup.
Experiments were conducted on three image datasets: CIFAR-10, CIFAR-100 [39], and
Tiny-ImageNet [20]. For CIFAR-10, ResNet-18 [31] was used. In contrast, both ConvNeXt [44]
and Vision Transformer [21] were used for CIFAR-100 and Tiny-ImageNet. For comparison, we
examined the following SSL methods: VICReg, SimCLR, BYOL, SwAV [14], Barlow Twins [73],
and MoCo [33]. The quality of representation was assessed through linear evaluation. A detailed
description of different methods can be found in Appendix H.
Results.
As evidenced by Table 1, the proposed entropy estimators surpass the original SSL
methods. Using a more precise entropy estimator enhances the performance of both VICReg and
BYOL, compared to their initial implementations. Notably, the pairwise distance estimator, being a
lower bound, achieves superior results, resonating with the theoretical preference for maximizing a
true entropy’s lower bound. Our findings suggest that the astute choice of entropy estimators, guided
by our framework, paves the way for enhanced performance.
6
A Generalization Bound for Downstream Tasks
In earlier sections, we linked information theory principles with the VICReg objective. Now, we aim
to extend this link to downstream generalization via a generalization bound. This connection further
aligns VICReg’s generalization with information maximization and implicit regularization.
Notation.
Consider input points x, outputs y ∈Rr, labeled training data S = ((xi, yi))n
i=1 of size
n and unlabeled training data ¯S = ((x+
i , x++
i
))m
i=1 of size m, where x+
i and x++
i
share the same
(unknown) label. With the unlabeled training data, we define the invariance loss
I ¯S(fθ) = 1
m
m
X
i=1
∥fθ(x+
i ) −fθ(x++
i
)∥,
where fθ is the trained representation on the unlabeled data ¯S. We define a labeled loss ℓx,y(w) =
∥Wfθ(x) −y∥where w = vec[W] ∈Rdr is the vectorization of the matrix W ∈Rr×d. Let
wS = vec[WS] be the minimum norm solution as WS = minimizeW ′ ∥W ′∥F such that
W ′ ∈arg min
W
1
n
n
X
i=1
∥Wfθ(xi) −yi∥2.
We also define the representation matrices
ZS = [f(x1), . . . , f(xn)] ∈Rd×n
and
Z ¯S = [f(x+
1 ), . . . , f(x+
m)] ∈Rd×m,
8

and the projection matrices
PZS = I −ZS
⊤(ZSZS
⊤)†ZS
and
PZ ¯
S = I −Z ¯S
⊤(Z ¯SZ ¯S
⊤)†Z ¯S.
We define the label matrix YS = [y1, . . . , yn]⊤∈Rn×r and the unknown label matrix Y ¯S =
[y+
1 , . . . , y+
m]⊤∈Rm×r, where y+
i is the unknown label of x+
i . Let F be a hypothesis space of fθ.
For a given hypothesis space F, we define the normalized Rademacher complexity
˜Rm(F) =
1
√mE ¯S,ξ
"
sup
f∈F
m
X
i=1
ξi∥f(x+
i ) −f(x++
i
)∥
#
,
where ξ1, . . . , ξm are independent uniform random variables in {−1, 1}. It is normalized such that
˜Rm(F) = O(1) as m →∞.
6.1
AGeneralizationBoundforVariance-Invariance-CovarianceRegularization
Now we will show that the VICReg objective improves generalization on supervised downstream
tasks. More specifically, minimizing the unlabeled invariance loss while controlling the covariance
Z ¯SZ ¯S
⊤and the complexity of representations ˜Rm(F) minimizes the expected labeled loss:
Theorem 3. (Informal version). For any δ > 0, with probability at least 1 −δ,
Ex,y[ℓx,y(wS)] ≤I ¯S(fθ) +
2
√m∥PZ ¯
SY ¯S∥F +
1
√n∥PZSYS∥F + 2 ˜Rm(F)
√m
+ Qm,n,
(12)
where Qm,n = O(G
p
ln(1/δ)/m +
p
ln(1/δ)/n) →0 as m, n →∞. In Qm,n, the value of G for
the term decaying at the rate 1/√m depends on the hypothesis space of fθ and w whereas the term
decaying at the rate 1/√n is independent of any hypothesis space.
Proof. The complete version of Theorem 3 and its proof are presented in Appendix I.
The term ∥PZ ¯
SY ¯S∥F in Theorem 3 contains the unobservable label matrix Y ¯S. However, we can
minimize this term by using ∥PZ ¯
SY ¯S∥F ≤∥PZ ¯
S∥F ∥Y ¯S∥F and by minimizing ∥PZ ¯
S∥F . The factor
∥PZ ¯
S∥F is minimized when the rank of the covariance Z ¯SZ ¯S
⊤is maximized. This can be enforced
by maximizing the diagonal entries while minimizing the off-diagonal entries, as is done in VICReg.
The term ∥PZSYS∥F contains only observable variables, and we can directly measure the value
of this term using training data. In addition, the term ∥PZSYS∥F is also minimized when the
rank of the covariance ZSZS
⊤is maximized. Since the covariances ZSZS
⊤and Z ¯SZ ¯S
⊤concen-
trate to each other via concentration inequalities with the error in the order of O(
p
(ln(1/δ))/n +
˜Rm(F)
p
(ln(1/δ))/m), we can also minimize the upper bound on ∥PZSYS∥F by maximizing the
diagonal entries of Z ¯SZ ¯S
⊤while minimizing its off-diagonal entries, as is done in VICReg.
Thus, VICReg can be understood as a method to minimize the generalization bound in Theorem 3 by
minimizing the invariance loss while controlling the covariance to minimize the label-agnostic upper
bounds on ∥PZ ¯
SY ¯S∥F and ∥PZSYS∥F . If we know partial information about the label Y ¯S of the
unlabeled data, we can use it to minimize ∥PZ ¯
SY ¯S∥F and ∥PZSYS∥F directly.
6.2
Comparison of Generalization Bounds
The SimCLR generalization bound [58] requires the number of labeled classes to go infinity to close
the generalization gap, whereas the VICReg bound in Theorem 3 does not require the number of
label classes to approach infinity for the generalization gap to go to zero. This reflects that, unlike
SimCLR, VICReg does not use negative pairs and thus does not use a loss function based on the
implicit expectation that the labels of a negative pair are different. Another difference is that our
VICReg bound improves as n increases, while the previous bound of SimCLR [58] does not depend
on n. This is because Saunshi et al. [58] assumes partial access to the true distribution per class for
setting, which removes the importance of labeled data size n and is not assumed in our study.
9

Consequently, the generalization bound in Theorem 3 provides a new insight for VICReg regarding
the ratio of the effects of m v.s. n through G
p
ln(1/δ)/m +
p
ln(1/δ)/n. Finally, Theorem 3 also
illuminates the advantages of VICReg over standard supervised training. That is, with standard
training, the generalization bound via the Rademacher complexity requires the complexities of
hypothesis spaces, ˜Rn(W)/√n and ˜Rn(F)/√n, with respect to the size of labeled data n, instead
of the size of unlabeled data m. Thus, Theorem 3 shows that using SSL, we can replace the
complexities of hypothesis spaces in terms of n with those in terms of m. Since the number of
unlabeled data points is typically much larger than the number of labeled data points, this illuminates
the benefit of SSL.
6.3
Understanding Theorem 2 via Mutual Information Maximization
Theorem 3, together with the result of the previous section, shows that, for generalization in the
downstream task, it is helpful to maximize the mutual information I(Z; X′) in SSL via minimizing the
invariance loss I ¯S(fθ) while controlling the covariance Z ¯SZ ¯S
⊤. The term 2 ˜Rm(F)/√m captures
the importance of controlling the complexity of the representations fθ. To understand this term
further, let us consider a discretization of the parameter space of F to have finite |F| < ∞. Then,
by Massart’s Finite Class Lemma, we have that ˜Rm(F) ≤C
p
ln |F| for some constant C > 0.
Moreover, Shwartz-Ziv [60] shows that we can approximate ln |F| by 2I(Z;X). Thus, in Theorem
3, the term I ¯S(fθ) +
2
√m∥PZ ¯
SY ¯S∥F +
1
√n∥PZSYS∥F corresponds to I(Z; X′) which we want to
maximize while compressing the term of 2 ˜Rm(F)/√m which corresponds to I(Z; X) [23, 63, 66].
Although we can explicitly add regularization on the information to control 2 ˜Rm(F)/√m, it is
possible that I(Z; X|X′) and 2 ˜Rm(F)/√m are implicitly regularized via implicit bias through
design choises [29, 68, 30]. Thus, Theorem 3 connects the information-theoretic understanding of
VICReg with the probabilistic guarantee on downstream generalization.
7
Limitations
In our paper, we proposed novel methods for SSL premised on information maximization. Although
our methods demonstrated superior performance on some datasets, computational constraints pre-
cluded us from testing them on larger datasets. Furthermore, our study hinges on certain assumptions
that, despite rigorous validation efforts, may not hold universally. While we strive for meticulous
testing and validation, it’s crucial to note that some assumptions might not be applicable in all
scenarios or conditions. These limitations should be taken into account when interpreting our study’s
results.
8
Conclusions
We analyzed the Variance-Invariance-Covariance Regularization for self-supervised learning through
an information-theoretic lens. By transferring the stochasticity required for an information-theoretic
analysis to the input distribution, we showed how the VICReg objective can be derived from
information-theoretic principles, used this perspective to highlight assumptions implicit in the VI-
CReg objective, derived a VICReg generalization bound for downstream tasks, and related it to
information maximization.
Building on these findings, we introduced a new VICReg-inspired SSL objective. Our probabilistic
guarantee suggests that VICReg can be further improved for the settings of partial label information
by aligning the covariance matrix with the partially observable label matrix, which opens up several
avenues for future work, including the design of improved estimators for information-theoretic quan-
tities and investigations into the suitability of different SSL methods for specific data characteristics.
10

References
[1] Alessandro Achille, Giovanni Paolini, and Stefano Soatto. Where is the information in a deep
neural network? arXiv preprint arXiv:1905.12213, 2019.
[2] N.A. Ahmed and D.V. Gokhale. Entropy expressions and their estimators for multivariate
distributions. IEEE Transactions on Information Theory, 35(3):688–692, 1989. doi: 10.1109/
18.30996.
[3] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational
information bottleneck. arXiv preprint arXiv:1612.00410, 2016.
[4] Rana Ali Amjad and Bernhard Claus Geiger. Learning representations for neural network-based
classification using the information bottleneck principle. IEEE transactions on pattern analysis
and machine intelligence, 2019.
[5] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj
Saunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv
preprint arXiv:1902.09229, 2019.
[6] Randall Balestriero and Richard Baraniuk. A spline theory of deep networks. In Proc. ICML,
volume 80, pages 374–383, Jul. 2018.
[7] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regular-
ization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.
[8] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
[9] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio,
Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv
preprint arXiv:1801.04062, 2018.
[10] Itamar Ben-Ari and Ravid Shwartz-Ziv. Attentioned convolutional lstm inpaintingnetwork for
anomaly detection in videos. arXiv preprint arXiv:1811.10228, 2018.
[11] Ido Ben-Shaul, Ravid Shwartz-Ziv, Tomer Galanti, Shai Dekel, and Yann LeCun. Reverse
engineering self-supervised learning. arXiv preprint arXiv:2305.15614, 2023.
[12] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative
components with random forests. In Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446–461. Springer,
2014.
[13] Brendon J Brewer. Computing entropies with nested sampling. Entropy, 19(8):422, 2017.
[14] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural
Information Processing Systems, 33:9912–9924, 2020.
[15] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 9650–9660, 2021.
[16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning, pages 1597–1607. PMLR, 2020.
[17] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750–15758,
2021.
[18] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels
based on centered alignment. The Journal of Machine Learning Research, 13(1):795–828, 2012.
11

[19] Ralph B. D’Agostino.
An omnibus test of normality for moderate and large
size
samples.
Biometrika,
58(2):341–348,
1971.
ISSN
00063444.
URL
http://www.jstor.org/stable/2334522.
[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009.
[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
[22] Yann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, and Chris J Maddison. Lossy compression
for lossless prediction. Advances in Neural Information Processing Systems, 34, 2021.
[23] Marco Federici, Anjan Dutta, Patrick Forr´e, Nate Kushman, and Zeynep Akata. Learning robust
representations via multi-view information bottleneck. In International Conference on Learning
Representations, 2019.
[24] Marco Federici, Anjan Dutta, Patrick Forr´e, Nate Kushman, and Zeynep Akata. Learning robust
representations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017, 2020.
[25] Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29(4):983–1049, 2016.
[26] Ziv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian
Kingsbury, and Yury Polyanskiy. Estimating information flow in deep neural networks. arXiv
preprint arXiv:1810.05728, 2018.
[27] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning, volume 1. MIT Press, 2016.
[28] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural
information processing systems, 33:21271–21284, 2020.
[29] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati
Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information
Processing Systems, pages 6151–6159, 2017.
[30] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient
descent on linear convolutional networks. In Advances in Neural Information Processing
Systems, pages 9461–9471, 2018.
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015.
[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016.
[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 9729–9738, 2020.
[34] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,
Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information
estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
[35] Marco Huber, Tim Bailey, Hugh Durrant-Whyte, and Uwe Hanebeck. On entropy approximation
for gaussian mixture random vectors. In IEEE International Conference on Multisensor Fusion
and Integration for Intelligent Systems, pages 181 – 188, 09 2008. doi: 10.1109/MFI.2008.
4648062.
12

[36] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse
in contrastive self-supervised learning. In International Conference on Learning Representa-
tions, 2022. URL https://openreview.net/forum?id=YevsQ05DEN7.
[37] Kenji Kawaguchi, Zhun Deng, Kyle Luh, and Jiaoyang Huang. Robustness Implies General-
ization via Data-Dependent Generalization Bounds. In International Conference on Machine
Learning (ICML), 2022.
[38] Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances.
Entropy, 19(7):361, 2017.
[39] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
[40] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical Report 0, University of Toronto, Toronto, Ontario, 2009.
[41] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[42] Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know
helps: Provable self-supervised learning. Advances in Neural Information Processing Systems,
34, 2021.
[43] Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive
visual representations. Advances in Neural Information Processing Systems, 34, 2021.
[44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 11976–11986, 2022.
[45] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-
grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.
[46] Julieta Martinez, Jashan Shewakramani, Ting Wei Liu, Ioan Andrei Bˆarsan, Wenyuan Zeng, and
Raquel Urtasun. Permute, quantize, and fine-tune: Efficient compression of neural networks. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
15699–15708, 2021.
[47] David McAllester and Karl Stratos. Formal limitations on the measurement of mutual infor-
mation. In International Conference on Artificial Intelligence and Statistics, pages 875–884.
PMLR, 2020.
[48] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant rep-
resentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6707–6717, 2020.
[49] Neeraj Misra, Harshinder Singh, and Eugene Demchuk. Estimation of the entropy of a mul-
tivariate normal distribution. Journal of Multivariate Analysis, 92(2):324–342, 2005. ISSN
0047-259X. doi: https://doi.org/10.1016/j.jmva.2003.10.003.
[50] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2012.
[51] Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of
linear regions of deep neural networks. In Proc. NeurIPS, pages 2924–2932, 2014.
[52] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large
number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image
Processing, pages 722–729. IEEE, 2008.
[53] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
systems, pages 271–279, 2016.
13

[54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[55] Liam Paninski. Estimation of entropy and mutual information. Neural computation, 15(6):
1191–1253, 2003.
[56] Zoe Piran, Ravid Shwartz-Ziv, and Naftali Tishby. The dual information bottleneck. arXiv
preprint arXiv:2006.04641, 2020.
[57] Ben Poole, Sherjil Ozair, A¨aron van den Oord, Alexander A. Alemi, and George Tucker.
On variational bounds of mutual information.
CoRR, abs/1905.06922, 2019.
URL
http://arxiv.org/abs/1905.06922.
[58] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khande-
parkar. A theoretical analysis of contrastive unsupervised representation learning. In Interna-
tional Conference on Machine Learning, pages 5628–5637. PMLR, 2019.
[59] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
[60] Ravid Shwartz-Ziv. Information flow in deep neural networks. arXiv preprint arXiv:2202.06749,
2022.
[61] Ravid Shwartz-Ziv and Alexander A Alemi. Information in infinite ensembles of infinitely-wide
neural networks. In Symposium on Advances in Approximate Bayesian Inference, pages 1–17.
PMLR, 2020.
[62] Ravid Shwartz-Ziv and Yann LeCun. To compress or not to compress–self-supervised learning
and information theory: A review. arXiv preprint arXiv:2304.09355, 2023.
[63] Ravid Shwartz-Ziv and Naftali Tishby. Compression of deep neural networks via information,
(2017). arXiv preprint arXiv:1703.00810, 2017.
[64] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
information. arXiv preprint arXiv:1703.00810, 2017.
[65] Ravid Shwartz-Ziv, Amichai Painsky, and Naftali Tishby. Representation compression and
generalization in deep neural networks, 2018.
[66] Ravid Shwartz-Ziv, Randall Balestriero, and Yann LeCun. What do we maximize in self-
supervised learning? arXiv preprint arXiv:2207.10081, 2022.
[67] Ravid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann Le-
Cun, and Andrew Gordon Wilson. Pre-train your loss: Easy bayesian transfer learning with
informative priors. arXiv preprint arXiv:2205.10279, 2022.
[68] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,
19(1):2822–2878, 2018.
[69] Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual
information. In Conference on Learning Theory, pages 3437–3452. PMLR, 2020.
[70] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625,
2019.
[71] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through
alignment and uniformity on the hypersphere. In International Conference on Machine Learning,
pages 9929–9939. PMLR, 2020.
[72] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of
learning algorithms. Advances in Neural Information Processing Systems, 30, 2017.
14

[73] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-
supervised learning via redundancy reduction. In International Conference on Machine Learn-
ing, pages 12310–12320. PMLR, 2021.
[74] Zhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant
entropy estimator. arXiv preprint arXiv:2105.03705, 2021.
[75] Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland
Brendel. Contrastive learning inverts the data generating process. In International Conference
on Machine Learning, pages 12979–12990. PMLR, 2021.
15

Appendix
Table of Contents
This appendix is organized as follows:
• In Appendix A, we provide a detailed derivation of the lower bound of Equation (8) .
• In Appendix B, we provide full proof of our Theorem 1 on the network’s representation distribu-
tion.
• In Appendix C, we provide additional empirical validations. Specifically, we empirically check
if the optimal solution to the information maximization problem in Section 4.1 is the diagonal
matrix.
• In Appendix D, we show the collapse phenomenon under Gaussian Mixture Model (GMM) using
Expectation Maximization (EM) and demonstrate how it is related to SSL and how we can prevent
it.
• Appendix E provides additional details on the SimCLR method.
• Appendix F provides a detailed review of entropy estimators, their implications, assumptions,
and limitations.
• In Appendix G, we provide proofs for known lemmas that we are using throughout our paper.
• In Appendix H, we provide detailed information on the hyperparameters, datasets, and architec-
tures used in our experiments in Section 5.2.
• In Appendix I, we provide full proof of our generalization bound for downstream tasks from
Section 6.
• In Appendix J, we provide full proof of the theorems for Section 4.1 on the connection between
information optimization and the VICReg objective.
• Appendix K provides experimental details on experiments conducted in Section 5.1 for entropy
comparison between different SSL methods.
• In Appendix L, we provide detailed information on the reproducibility of our study.
• In Appendix O, we discuss the broader impact of our work. This section explores the implications,
significance, and potential applications of our findings beyond the scope of the immediate study.
16

Appendix A
Lower bounds on Ex′ [log q(z|x′)]
In this section of the supplementary material, we present the full derivation of the lower bound
on Ex′ [log q(z|x′)]. Because Z′|X′ is a Gaussian, we can write it as Z′ = µ(x′) + L(x′)ϵ where
ϵ ∼N(0, 1) and L(x′)T L(x′) = Σ(x′). Now, setting Σr = I, will give us:
Ex′ [log q(z|x′)]
≥Ez′|x′ [log q(z|z′)]
(13)
=Ez′|x′
d
2 log 2π −1
2 (z −z′)T (I))−1 (z −z′)

(14)
=d
2 log 2π −1
2Ez′|x′,
h
(z −z′)2i
(15)
=d
2 log 2π −1
2Eϵ
h
(z −µ(x′) −L(x′)ϵ)2i
(16)
=d
2 log 2π −1
2Eϵ
h
(z −µ(x′))2 −2 (z −µ(x′) ∗L(x′)ϵ) +

(L(x′)ϵ)T (L(x′)ϵ)
i
(17)
=d
2 log 2π −1
2Eϵ
h
(z −µ(x′))2i
+ (z −µ(x′)L(x′)) Eϵ [ϵ] −1
2Eϵ

ϵT L(x′)T L(x′)ϵ

(18)
=d
2 log 2π −1
2 (z −µ(x′))2 −1
2Tr log Σ(x′)
(19)
where Ex′ [log q(z|x′)] = Ex′ 
log Ez′|x′ [q(z|z′)]

≥Ez′ [log q(z|z′)] by Jensen’s inequality,
Eϵ[ϵ] = 0 and Eϵ

ϵ
 L(x′)T L(x′
ϵ

= Tr log Σ(x′) by the Hutchinson’s estimator.
Ez|x

Ez′|x′ [log q(z|z′)]

=Ez|x
d
2 log 2π −1
2 (z −µ(x′))2 −1
2Tr log Σ(x′)

(20)
=d
2 log 2π −1
2Ez|x
h
(z −µ(x′))2i
−1
2Tr log Σ(x′)
(21)
=d
2 log 2π −1
2Eϵ
h
(µ(x) + L(x)ϵ −µ(x′))2i
−1
2Tr log Σ(x′)
(22)
=d
2 log 2π −1
2Eϵ
h
(µ(x) −µ(x′))2i
+ Eϵ [(µ(x) −µ(x′)) L(x)ϵ]
−1
2Eϵ

ϵT L(x)T L(x)ϵ

−1
2Tr log Σ(x′)
(23)
=d
2 log 2π −1
2 (µ(x) −µ(x′))2 −1
2Tr log Σ(x) −1
2Tr log Σ(x′)
(24)
=d
2 log 2π −1
2 (µ(x) −µ(x′))2 −1
2 log (|Σ(x)| · |Σ(x′)|)
(25)
17

Appendix B
Data Distribution after Deep Network Transformation
Theorem 4. Given the setting of Equation (4), the unconditional DNN output density denoted as Z
approximates (given the truncation of the Gaussian on its effective support that is included within a
single region ω of the DN’s input space partition) a mixture of the affinely transformed distributions
x|x∗
n(x) e.g. for the Gaussian case
Z∼
N
X
n=1
N

Aω(x∗n)x∗
n + bω(x∗n), AT
ω(x∗n)Σx∗nAω(x∗n)
T =n
,
where ω(x∗
n) = ω ∈Ω⇐⇒x∗
n ∈ω is the partition region in which the prototype x∗
n lives in.
Proof. We know that If
R
ω p(x|x∗
n(x))dx ≈1 then f is linear within the effective support of p.
Therefore, any sample from p will almost surely lie within a single region ω ∈Ω, and therefore the
entire mapping can be considered linear with respect to p. Thus, the output distribution is a linear
transformation of the input distribution based on the per-region affine mapping.
Appendix C
Additional Empirical Validation
To validate empirically Theorem 2, we checke empirically if the optimal solution for
X
i
log
|ΣZ|
|ΣZ|Xi||ΣZ′|X′
i|
is a diagonal matrix. We trained VICReg on ResNet18 on CIFAR-10 and did random perturbations
(with different scales) for ΣZ. Then, for each perturbation, we calculated the average distance of this
perturbed matrix from a diagonal matrix and the actual value of the term
X
i
log |ΣZ||ΣZ′|X′
i|
|ΣZ|Xi|
. In Figure 3, we plot the difference from the optimal value of this term as a function of the distance
from the diagonal matrix. As we can see, we get an optimal solution where we are close to the
diagonal matrix. This observation gives us an empirical validation of Theorem 2.
Figure 3: The optimal solution for the optimization problem is a diagonal matrix. The average
distance from a diagonal matrix for different perturbation scales. Experiments were conducted on
CIFAR-10 with the ResNet-18 network.
18

H(Z)=45
data
centroids
H(Z)=40
data
centroids
H(Z)=34
data
centroids
Figure 4: Evolution of GMM training when enforcing a one-to-one mapping between the data
and centroids akin to K-means i.e. using a small and fixed covariance matrix. We see that
collapse does not occur. Left - In the presence of fixed input samples, we observe that there is
no collapsing and that the entropy of the centers is high. Right - when we make the input samples
trainable and optimize their location, all the points collapse into a single point, resulting in a sharp
decrease in entropy.
Appendix D
EM and GMM
Let us examine a toy dataset on the pattern of two intertwining moons to illustrate the collapse
phenomenon under GMM (Figure 1, right). We begin by training a classical GMM with maximum
likelihood, where the means are initialized based on random samples, and the covariance is used as the
identity matrix. A red dot represents the Gaussian’s mean after training, while a blue dot represents
the data points. In the presence of fixed input samples, we observe that there is no collapsing and
that the entropy of the centers is high (Figure 4, left). However, when we make the input samples
trainable and optimize their location, all the points collapse into a single point, resulting in a sharp
decrease in entropy (Figure 4, right).
To prevent collapse, we follow the K-means algorithm in enforcing sparse posteriors, i.e. using small
initial standard deviations and learning only the mean. This forces a one-to-one mapping which
leads all points to be closest to the mean without collapsing, resulting in high entropy (Figure 4 -
middle, in the Appendix). Another option to prevent collapse is to use different learning rates for
input and parameters. Using this setting, the collapsing of the parameters does not maximize the
likelihood. Figure 1 (right) shows the results of GMM with different learning rates for learned inputs
and parameters. When the parameter learning rate is sufficiently high in comparison to the input
learning rate, the entropy decreases much more slowly and no collapse occurs.
Appendix E
SimCLR
In contrastive learning, different augmented views of the same image are attracted (positive pairs),
while different augmented views are repelled (negative pairs). MoCo [33] and SimCLR [16] are
recent examples of self-supervised visual representation learning that reduce the gap between self-
supervised and fully-supervised learning. SimCLR applies randomized augmentations to an image
to create two different views, x and y, and encodes both of them with a shared encoder, producing
representations rx and ry. Both rx and ry are l2-normalized. The SimCLR version of the InfoNCE
objective is:
Ex,y
"
−log
 
e
1
η rT
y rx
PK
k=1 e
1
η rT
yk rx
!#
,
where η is a temperature term and K is the number of views in a minibatch.
19

Appendix F
Entropy Estimators
Entropy estimation is one of the classical problems in information theory, where Gaussian mixture
density is one of the most popular representations. With a sufficient number of components, they can
approximate any smooth function with arbitrary accuracy. For Gaussian mixtures, there is, however,
no closed-form solution to differential entropy. There exist several approximations in the literature,
including loose upper and lower bounds [35]. Monte Carlo (MC) sampling is one way to approximate
Gaussian mixture entropy. With sufficient MC samples, an unbiased estimate of entropy with an
arbitrarily accurate can be obtained. Unfortunately, MC sampling is very computationally expensive
and typically requires a large number of samples, especially in high dimensions [13]. Using the first
two moments of the empirical distribution, VIGCreg used one of the most straightforward approaches
for approximating the entropy. Despite this, previous studies have found that this method is a poor
approximation of the entropy in many cases [35]. Another option is to use the LogDet function.
Several estimators have been proposed to implement it, including uniformly minimum variance
unbiased (UMVU) [2], and Bayesian methods [49]. These methods, however, often require complex
optimizations. The LogDet estimator presented in [74] used the differential entropy α order entropy
using scaled noise. They demonstrated that it can be applied to high-dimensional features and is
robust to random noise. Based on Taylor-series expansions, [35] presented a lower bound for the
entropy of Gaussian mixture random vectors. They use Taylor-series expansions of the logarithm of
each Gaussian mixture component to get an analytical evaluation of the entropy measure. In addition,
they present a technique for splitting Gaussian densities to avoid components with high variance,
which would require computationally expensive calculations. Kolchinsky and Tracey [38] introduce
a novel family of estimators for the mixture entropy. For this family, a pairwise-distance function
between component densities is defined for each member. These estimators are computationally
efficient as long as the pairwise-distance function and the entropy of each component distribution
are easy to compute. Moreover, the estimator is continuous and smooth and is therefore useful for
optimization problems. In addition, they presented both a lower bound (using Chernoff distance)
and an upper bound (using the KL divergence) on the entropy, which are exact when the component
distributions are grouped into well-separated clusters,
Appendix G
Known Lemmas
We use the following well-known theorems as lemmas in our proofs. We put these below for
completeness. These are classical results and not our results.
Lemma G.1. (Hoeffding’s inequality) Let X1, ..., Xn be independent random variables
such that a ≤Xi ≤b almost surely.
Consider the average of these random variables,
Sn = 1
n(X1 + · · · + Xn). Then, for all t > 0,
PS
 
E [Sn] −Sn ≥(b −a)
r
ln(1/δ)
2n
!
≤δ,
and
PS
 
Sn −E [Sn] ≥(b −a)
r
ln(1/δ)
2n
!
≤δ.
Proof. By using Hoeffding’s inequality, we have that for all t > 0,
PS (E [Sn] −Sn ≥t) ≤exp

−
2nt2
(b −a)2

,
and
PS (Sn −E [Sn] ≥t) ≤exp

−
2nt2
(b −a)2

,
20

Setting δ = exp

−2nt2
(b−a)2

and solving for t > 0,
1/δ = exp

2nt2
(b −a)2

=⇒ln(1/δ) =
2nt2
(b −a)2
=⇒(b −a)2 ln(1/δ)
2n
= t2
=⇒t = (b −a)
r
ln(1/δ)
2n
It has been shown that generalization bounds can be obtained via Rademacher complexity [8, 50, 59].
The following is a trivial modification of [50, Theorem 3.1] for a one-sided bound on the nonnegative
general loss functions:
Lemma G.2. Let G be a set of functions with the codomain [0, M]. Then, for any δ > 0, with
probability at least 1 −δ over an i.i.d. draw of m samples S = (qi)m
i=1, the following holds for all
ψ ∈G:
Eq[ψ(q)] ≤1
m
m
X
i=1
ψ(qi) + 2Rm(G) + M
r
ln(1/δ)
2m
,
(26)
where Rm(G) := ES,ξ[supψ∈G
1
m
Pm
i=1 ξiψ(qi)] and ξ1, . . . , ξm are independent uniform random
variables taking values in {−1, 1}.
Proof. Let S = (qi)m
i=1 and S′ = (q′
i)m
i=1. Define
φ(S) = sup
ψ∈G
Ex,y[ψ(q)] −1
m
m
X
i=1
ψ(qi).
(27)
To apply McDiarmid’s inequality to φ(S), we compute an upper bound on |φ(S) −φ(S′)| where S
and S′ be two test datasets differing by exactly one point of an arbitrary index i0; i.e., Si = S′
i for all
i ̸= i0 and Si0 ̸= S′
i0. Then,
φ(S′) −φ(S) ≤sup
ψ∈G
ψ(qi0) −ψ(q′
i0)
m
≤M
m .
(28)
Similarly, φ(S) −φ(S′) ≤M
m . Thus, by McDiarmid’s inequality, for any δ > 0, with probability at
least 1 −δ,
φ(S) ≤ES[φ(S)] + M
r
ln(1/δ)
2m
.
(29)
Moreover,
ES[φ(S)] = ES
"
sup
ψ∈G
ES′
"
1
m
m
X
i=1
ψ(q′
i)
#
−1
m
m
X
i=1
ψ(qi)
#
(30)
≤ES,S′
"
sup
ψ∈G
1
m
m
X
i=1
(ψ(q′
i) −ψ(qi))
#
(31)
≤Eξ,S,S′
"
sup
ψ∈G
1
m
m
X
i=1
ξi(ψ(q′
i) −ψ(qi))
#
(32)
≤2Eξ,S
"
sup
ψ∈G
1
m
m
X
i=1
ξiψ(qi)
#
= 2Rm(G),
(33)
where the first line follows the definitions of each term, the second line uses Jensen’s inequality and the
convexity of the supremum, and the third line follows that for each ξi ∈{−1, +1}, the distribution of
each term ξi(ℓ(f(x′
i), y′
i)−ℓ(f(xi), yi)) is the distribution of (ℓ(f(x′
i), y′
i)−ℓ(f(xi), yi)) since S and
S′ are drawn iid with the same distribution. The fourth line uses the subadditivity of supremum.
21

Appendix H
Implentation Details for Maximizing Entropy Estimators
In this section, we will provide more details on the implantation of the experiments conducted in
Section 5.2.
Setup Our experiments are conducted on CIFAR-10 [40]. We use ResNet-18 [32] as our backbone.
Training Procedure: The experimental process is organized into two sequential stages: unsupervised
pretraining followed by linear evaluation. Initially, the unsupervised pretraining phase is executed,
during which the encoder network is trained. Upon its completion, we transition to the linear
evaluation phase, which serves as an assessment tool for the quality of the representation produced
by the pretrained encoder.
Once the pretraining phase is concluded, we adhere to the fine-tuning procedures used in established
baseline methods, as described by [14].
During the linear evaluation stage, we start by performing supervised training of the linear classifier.
This is achieved by using the representations derived from the encoder network while keeping the
network’s coefficients frozen, and applying the same training dataset. Subsequently, we measure the
test accuracy of the trained linear classifier using a separate validation dataset. This approach allows
us to evaluate the performance of our model in a robust and systematic manner.
The training process for each model unfolds over 800 epochs, employing a batch size of 512. We
utilize the Stochastic Gradient Descent (SGD) optimizer, characterized by a momentum of 0.9 and a
weight decay of 1e−4. The learning rate is initiated at 0.5 and is adjusted according to a cosine decay
schedule complemented by a linear warmup phase.
During the data augmentation process, two enhanced versions of every input image are generated.
This involves cropping each image randomly and resizing it back to the original resolution. The
images are then subject to random horizontal flipping, color jittering, grayscale conversion, Gaussian
blurring, and polarization for further augmentation.
For the linear evaluation phase, the linear classifier is trained for 100 epochs with a batch size of 256.
The SGD optimizer is again employed, this time with a momentum of 0.9 and no weight decay. The
learning rate is managed using a cosine decay schedule, starting at 0.2 and reaching a minimum of
2e −4.
Appendix I
A Generalization Bound for Downstream Tasks
In this Appendix, we present the complete version of Theorem 3 along with its proof and additional
discussions.
I.1
Additional Notation and details
We start to introduce additional notation and details. We use the notation of x ∈X for an input and
y ∈Y ⊆Rr for an output. Define p(y) = P(Y = y) to be the probability of getting label y and
ˆp(y) = 1
n
Pn
i=1 1{yi = y} to be the empirical estimate of p(y). Let ζ be an upper bound on the
norm of the label as ∥y∥2 ≤ζ for all y ∈Y. Define the minimum norm solution W ¯S of the unlabeled
data as W ¯S = minimizeW ′ ∥W ′∥F s.t. W ′ ∈arg minW
1
m
Pm
i=1 ∥Wfθ(x+
i ) −g∗(x+
i )∥2. Let κS
be a data-dependent upper bound on the per-sample Euclidian norm loss with the trained model as
∥WSfθ(x) −y∥≤κS for all (x, y) ∈X × Y. Similarly, let κ ¯S be a data-dependent upper bound
on the per-sample Euclidian norm loss as ∥W ¯Sfθ(x) −y∥≤κ ¯S for all (x, y) ∈X × Y. Define the
difference between WS and W ¯S by c = ∥WS −W ¯S∥2. Let W be a hypothesis space of W such that
W ¯S ∈W. We denote by ˜Rm(W ◦F) =
1
√mE ¯S,ξ[supW ∈W,f∈F
Pm
i=1 ξi∥g∗(x+
i ) −Wf(x+
i )∥] the
normalized Rademacher complexity of the set {x+ 7→∥g∗(x+) −Wf(x+)∥: W ∈W, f ∈F}.
we denote by κ a upper bound on the per-sample Euclidian norm loss as ∥Wf(x) −y∥≤κ for all
(x, y, W, f) ∈X × Y × W × F.
We adopt the following data-generating process model that was used in a previous paper on analyzing
contrastive learning [58, 10]. For the labeled data, first, y is drawn from the distribution ρ on Y,
and then x is drawn from the conditional distribution Dy conditioned on the label y. That is, we
have the join distribution D(x, y) = Dy(x)ρ(y) with ((xi, yi))n
i=1 ∼Dn. For the unlabeled data,
22

first, each of the unknown labels y+ and y−is drawn from the distritbuion ρ, and then each of the
positive examples x+ and x++ is drawn from the conditional distribution Dy+ while the negative
example x−is drawn from the Dy−. Unlike the analysis of contrastive learning, we do not require
negative samples. Let τ ¯S be a data-dependent upper bound on the invariance loss with the trained
representation as ∥fθ(¯x) −fθ(x)∥≤τ ¯S for all (¯x, x) ∼D2
y and y ∈Y. Let τ be a data-independent
upper bound on the invariance loss with the trained representation as∥f(¯x) −f(x)∥≤τ for all
(¯x, x) ∼D2
y, y ∈Y, and f ∈F. For simplicity, we assume that there exists a function g∗such that
y = g∗(x) ∈Rr for all (x, y) ∈X × Y. Discarding this assumption adds the average of label noises
to the final result, which goes to zero as the sample sizes n and m increase, assuming that the mean
of the label noise is zero.
0.2
Proof of Theorem 3
Proof of Theorem 3. Let W = WS where WS is the the minimum norm solution as WS =
minimizeW ′ ∥W ′∥F s.t. W ′ ∈arg minW
1
n
Pn
i=1 ∥Wfθ(xi) −yi∥2. Let W ∗= W ¯S where
W ¯S is the minimum norm solution as W ∗
=
W ¯S
=
minimizeW ′ ∥W ′∥F s.t.
W ′
∈
arg minW
1
m
Pm
i=1 ∥Wfθ(x+
i ) −g∗(x+
i )∥2. Since y = g∗(x),
y = g∗(x) ± W ∗fθ(x) = W ∗fθ(x) + (g∗(x) −W ∗fθ(x)) = W ∗fθ(x) + φ(x)
where φ(x) = g∗(x) −W ∗fθ(x). Define LS(w) = 1
n
Pn
i=1 ∥Wfθ(xi) −yi∥. Using these,
LS(w) = 1
n
n
X
i=1
∥Wfθ(xi) −yi∥
= 1
n
n
X
i=1
∥Wfθ(xi) −W ∗fθ(xi) −φ(xi)∥
≥1
n
n
X
i=1
∥Wfθ(xi) −W ∗fθ(xi)∥−1
n
n
X
i=1
∥φ(xi)∥
= 1
n
n
X
i=1
∥˜Wfθ(xi)∥−1
n
n
X
i=1
∥φ(xi)∥
where ˜W = W −W ∗. We now consider new fresh samples ¯xi ∼Dyi for i = 1, . . . , n to rewrite the
above further as:
LS(w) ≥1
n
n
X
i=1
∥˜Wfθ(xi) ± ˜Wfθ(¯xi)∥−1
n
n
X
i=1
∥φ(xi)∥
= 1
n
n
X
i=1
∥˜Wfθ(¯xi) −( ˜Wfθ(¯xi) −˜Wfθ(xi))∥−1
n
n
X
i=1
∥φ(xi)∥
≥1
n
n
X
i=1
∥˜Wfθ(¯xi)∥−1
n
n
X
i=1
∥˜Wfθ(¯xi) −˜Wfθ(xi)∥−1
n
n
X
i=1
∥φ(xi)∥
= 1
n
n
X
i=1
∥˜Wfθ(¯xi)∥−1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥−1
n
n
X
i=1
∥φ(xi)∥
This implies that
1
n
n
X
i=1
∥˜Wfθ(¯xi)∥≤LS(w) + 1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥+ 1
n
n
X
i=1
∥φ(xi)∥.
23

Furthermore, since y = W ∗fθ(x) + φ(x), by writing ¯yi = W ∗fθ(¯xi) + φ(¯xi) (where ¯yi = yi since
¯xi ∼Dyi for i = 1, . . . , n),
1
n
n
X
i=1
∥˜Wfθ(¯xi)∥= 1
n
n
X
i=1
∥Wfθ(¯xi) −W ∗fθ(¯xi)∥
= 1
n
n
X
i=1
∥Wfθ(¯xi) −¯yi + φ(¯xi)∥
≥1
n
n
X
i=1
∥Wfθ(¯xi) −¯yi∥−1
n
n
X
i=1
∥φ(¯xi)∥
Combining these, we have that
1
n
n
X
i=1
∥Wfθ(¯xi) −¯yi∥≤LS(w) + 1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥
(34)
+ 1
n
n
X
i=1
∥φ(xi)∥+ 1
n
n
X
i=1
∥φ(¯xi)∥.
To bound the left-hand side of equation 34, we now analyze the following random variable:
EX,Y [∥WSfθ(X) −Y ∥] −1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥,
(35)
where ¯yi = yi since ¯xi ∼Dyi for i = 1, . . . , n. Importantly, this means that as WS depends on yi,
WS depends on ¯yi. Thus, the collection of random variables ∥WSfθ(¯x1) −¯y1∥, . . . , ∥WSfθ(nn) −
¯yn∥is not independent. Accordingly, we cannot apply standard concentration inequality to bound
equation 35. A standard approach in learning theory is to first bound equation 35 by Ex,y∥WSfθ(x)−
y∥−1
n
Pn
i=1 ∥WSfθ(¯xi) −¯yi∥≤supW ∈W Ex,y∥Wfθ(x) −y∥−1
n
Pn
i=1 ∥Wfθ(¯xi) −¯yi∥for
some hypothesis space W (that is independent of S) and realize that the right-hand side now contains
the collection of independent random variables ∥Wfθ(¯x1) −¯y1∥, . . . , ∥Wfθ(nn) −¯yn∥, for which
we can utilize standard concentration inequalities. This reasoning leads to the Rademacher complexity
of the hypothesis space W. However, the complexity of the hypothesis space W can be very large,
resulting in a loose bound. In this proof, we show that we can avoid the dependency on hypothesis
space W by using a very different approach with conditional expectations to take care the dependent
random variables ∥WSfθ(¯x1) −¯y1∥, . . . , ∥WSfθ(nn) −¯yn∥. Intuitively, we utilize the fact that for
these dependent random variables, there is a structure of conditional independence, conditioned on
each y ∈Y.
We first write the expected loss as the sum of the conditional expected loss:
EX,Y [∥WSfθ(X) −Y ∥] =
X
y∈Y
EX,Y [∥WSfθ(X) −Y ∥| Y = y]P(Y = y)
=
X
y∈Y
EXy[∥WSfθ(Xy) −y∥]P(Y = y),
where Xy is the random variable for the conditional with Y = y. Using this, we decompose
equation 35 into two terms:
EX,Y [∥WSfθ(X) −Y ∥] −1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥
(36)
=

X
y∈Y
EXy[∥WSfθ(Xy) −y∥]|Iy|
n
−1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥


+
X
y∈Y
EXy[∥WSfθ(Xy) −y∥]

P(Y = y) −|Iy|
n

,
24

where
Iy = {i ∈[n] : yi = y}.
The first term in the right-hand side of equation 36 is further simplified by using
1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥= 1
n
X
y∈Y
X
i∈Iy
∥WSfθ(¯xi) −y∥,
as
X
y∈Y
EXy[∥WSfθ(Xy) −y∥]|Iy|
n
−1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥
= 1
n
X
y∈˜
Y
|Iy|

EXy[∥WSfθ(Xy) −y∥] −
1
|Iy|
X
i∈Iy
∥WSfθ(¯xi) −y∥

,
where ˜Y = {y ∈Y : |Iy| ̸= 0}. Substituting these into equation equation 36 yields
EX,Y [∥WSfθ(X) −Y ∥] −1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥
(37)
= 1
n
X
y∈˜
Y
|Iy|

EXy[∥WSfθ(Xy) −y∥] −
1
|Iy|
X
i∈Iy
∥WSfθ(¯xi) −y∥


+
X
y∈Y
EXy[∥WSfθ(Xy) −y∥]

P(Y = y) −|Iy|
n

Importantly, while ∥WSfθ(¯x1) −¯y1∥, . . . , ∥WSfθ(¯xn) −¯yn∥on the right-hand side of equation 37
are dependent random variables, ∥WSfθ(¯x1) −y∥, . . . , ∥WSfθ(¯xn) −y∥are independent random
variables since WS and ¯xi are independent and y is fixed here. Thus, by using Hoeffding’s inequality
(Lemma G.1), and taking union bounds over y ∈˜Y, we have that with probability at least 1 −δ, the
following holds for all y ∈˜Y:
EXy[∥WSfθ(Xy) −y∥] −
1
|Iy|
X
i∈Iy
∥WSfθ(¯xi) −y∥≤κS
s
ln(| ˜Y|/δ)
2|Iy|
.
This implies that with probability at least 1 −δ,
1
n
X
y∈˜
Y
|Iy|

EXy[∥WSfθ(Xy) −y∥] −
1
|Iy|
X
i∈Iy
∥WSfθ(¯xi) −y∥


≤κS
n
X
y∈˜
Y
|Iy|
s
ln(| ˜Y|/δ)
2|Iy|
= κS

X
y∈˜
Y
r
|Iy|
n


s
ln(| ˜Y|/δ)
2n
.
Substituting this bound into equation 37, we have that with probability at least 1 −δ,
EX,Y [∥WSfθ(X) −Y ∥] −1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥
(38)
≤κS

X
y∈˜
Y
p
ˆp(y)


s
ln(| ˜Y|/δ)
2n
+
X
y∈Y
EXy[∥WSfθ(Xy) −y∥]

P(Y = y) −|Iy|
n

25

where
ˆp(y) = |Iy|
n .
Moreover, for the second term on the right-hand side of equation 38, by using Lemma 1 of [37], we
have that with probability at least 1 −δ,
X
y∈Y
EXy[∥WSfθ(Xy) −y∥]

P(Y = y) −|Iy|
n

≤

X
y∈Y
p
p(y)EXy[∥WSfθ(Xy) −y∥


r
2 ln(|Y|/δ)
2n
≤κS

X
y∈Y
p
p(y)


r
2 ln(|Y|/δ)
2n
where p(y) = P(Y = y). Substituting this bound into equation 38 with the union bound, we have
that with probability at least 1 −δ,
EX,Y [∥WSfθ(X) −Y ∥] −1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥
(39)
≤κS

X
y∈˜
Y
p
ˆp(y)


s
ln(2| ˜Y|/δ)
2n
+ κS

X
y∈Y
p
p(y)


r
2 ln(2|Y|/δ)
2n
≤

X
y∈Y
p
ˆp(y)

κS
r
2 ln(2|Y|/δ)
2n
+

X
y∈Y
p
p(y)

κS
r
2 ln(2|Y|/δ)
2n
≤κS
r
2 ln(2|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

Combining equation 34 and equation 39 implies that with probability at least 1 −δ,
EX,Y [∥WSfθ(X) −Y ∥]
(40)
≤1
n
n
X
i=1
∥WSfθ(¯xi) −¯yi∥+ κS
r
2 ln(2|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

≤LS(wS) + 1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥
+ 1
n
n
X
i=1
∥φ(xi)∥+ 1
n
n
X
i=1
∥φ(¯xi)∥+ κS
r
2 ln(2|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

.
We will now analyze the term 1
n
Pn
i=1 ∥φ(xi)∥+ 1
n
Pn
i=1 ∥φ(¯xi)∥on the right-hand side of equa-
tion 40. Since W ∗= W ¯S,
1
n
n
X
i=1
∥φ(xi)∥= 1
n
n
X
i=1
∥g∗(xi) −W ¯Sfθ(xi)∥.
By using Hoeffding’s inequality (Lemma G.1), we have that for any δ > 0, with probability at least
1 −δ,
1
n
n
X
i=1
∥φ(xi)∥≤1
n
n
X
i=1
∥g∗(xi) −W ¯Sfθ(xi)∥≤Ex+[∥g∗(x+) −W ¯Sfθ(x+)∥] + κ ¯S
r
ln(1/δ)
2n
.
26

Moreover, by using [50, Theorem 3.1] with the loss function x+ 7→∥g∗(x+) −Wf(x+)∥(i.e.,
Lemma G.2), we have that for any δ > 0, with probability at least 1 −δ,
Ex+[∥g∗(x+) −W ¯Sfθ(x+)∥] ≤1
m
m
X
i=1
∥g∗(x+
i ) −W ¯Sfθ(x+
i )∥+ 2 ˜Rm(W ◦F)
√m
+ κ
r
ln(1/δ)
2m
(41)
where ˜Rm(W ◦F) =
1
√mE ¯S,ξ[supW ∈W,f∈F
Pm
i=1 ξi∥g∗(x+
i ) −Wf(x+
i )∥] is the normalized
Rademacher complexity of the set {x+ 7→∥g∗(x+)−Wf(x+)∥: W ∈W, f ∈F} (it is normalized
such that ˜Rm(F) = O(1) as m →∞for typical choices of F), and ξ1, . . . , ξm are independent
uniform random variables taking values in {−1, 1}. Takinng union bounds, we have that for any
δ > 0, with probability at least 1 −δ,
1
n
n
X
i=1
∥φ(xi)∥≤1
m
m
X
i=1
∥g∗(x+
i ) −W ¯Sfθ(x+
i )∥+ 2 ˜Rm(W ◦F)
√m
+ κ
r
ln(2/δ)
2m
+ κ ¯S
r
ln(2/δ)
2n
Similarly, for any δ > 0, with probability at least 1 −δ,
1
n
n
X
i=1
∥φ(¯xi)∥≤1
m
m
X
i=1
∥g∗(x+
i ) −W ¯Sfθ(x+
i )∥+ 2 ˜Rm(W ◦F)
√m
+ κ
r
ln(2/δ)
2m
+ κ ¯S
r
ln(2/δ)
2n
.
Thus, by taking union bounds, we have that for any δ > 0, with probability at least 1 −δ,
1
n
n
X
i=1
∥φ(xi)∥+ 1
n
n
X
i=1
∥φ(¯xi)∥
(42)
≤2
m
m
X
i=1
∥g∗(x+
i ) −W ¯Sfθ(x+
i )∥+ 4Rm(W ◦F)
√m
+ 2κ
r
ln(4/δ)
2m
+ 2κ ¯S
r
ln(4/δ)
2n
To analyze the first term on the right-hand side of equation 42, recall that
W ¯S = minimize
W ′
∥W ′∥F s.t. W ′ ∈arg min
W
1
m
m
X
i=1
∥Wfθ(x+
i ) −g∗(x+
i )∥2.
(43)
Here, since Wfθ(x+
i ) ∈Rr, we have that
Wfθ(x+
i ) = vec[Wfθ(x+
i )] = [fθ(x+
i )⊤⊗Ir] vec[W] ∈Rr,
where Ir ∈Rr×r is the identity matrix, and [fθ(x+
i )⊤⊗Ir] ∈Rr×dr is the Kronecker product
of the two matrices, and vec[W] ∈Rdr is the vectorization of the matrix W ∈Rr×d. Thus, by
defining Ai = [fθ(x+
i )⊤⊗Ir] ∈Rr×dr and using the notation of w = vec[W] and its inverse
W = vec−1[w] (i.e., the inverse of the vectorization from Rr×d to Rdr with a fixed ordering), we
can rewrite equation 43 by
W ¯S = vec−1[w ¯S]
where
w ¯S = minimize
w′
∥w′∥F s.t. w′ ∈arg min
w
m
X
i=1
∥gi −Aiw∥2,
with gi = g∗(x+
i ) ∈Rr. Since the function w 7→Pm
i=1 ∥gi −Aiw∥2 is convex, a necessary and
sufficient condition of the minimizer of this function is obtained by
0 = ∇w
m
X
i=1
∥gi −Aiw∥2 = 2
m
X
i=1
A⊤
i (gi −Aiw) ∈Rdr
This implies that
m
X
i=1
A⊤
i Aiw =
m
X
i=1
A⊤
i gi.
27

In other words,
A⊤Aw = A⊤g
where A =


A1
A2
...
Am

∈Rmr×dr and g =


g1
g2
...
gm

∈Rmr
Thus,
w′ ∈arg min
w
m
X
i=1
∥gi −Aiw∥2 = {(A⊤A)†A⊤g + v : v ∈Null(A)}
where (A⊤A)† is the Moore–Penrose inverse of the matrix A⊤A and Null(A) is the null space of
the matrix A. Thus, the minimum norm solution is obtained by
vec[W ¯S] = w ¯S = (A⊤A)†A⊤g.
Thus, by using this W ¯S, we have that
1
m
m
X
i=1
∥g∗(x+
i ) −W ¯Sfθ(x+
i )∥= 1
m
m
X
i=1
v
u
u
t
r
X
k=1
((gi −Aiw ¯S)k)2
≤
v
u
u
t 1
m
m
X
i=1
r
X
k=1
((gi −Aiw ¯S)k)2
=
1
√m
v
u
u
t
m
X
i=1
r
X
k=1
((gi −Aiw ¯S)k)2
=
1
√m∥g −Aw ¯S∥2
=
1
√m∥g −A(A⊤A)†A⊤g∥2 =
1
√m∥(I −A(A⊤A)†A⊤)g∥2
where the inequality follows from the Jensen’s inequality and the concavity of the square root function.
Thus, we have that
1
n
n
X
i=1
∥φ(xi)∥+ 1
n
n
X
i=1
∥φ(¯xi)∥
(44)
≤
2
√m∥(I −A(A⊤A)†A⊤)g∥2 + 4Rm(W ◦F)
√m
+ 2κ
r
ln(4/δ)
2m
+ 2κ ¯S
r
ln(4/δ)
2n
By combining equation 40 and equation 44 with union bound, we have that
EX,Y [∥WSfθ(X) −Y ∥]
(45)
≤LS(wS) + 1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥+
2
√m∥PAg∥2
+ 4Rm(W ◦F)
√m
+ 2κ
r
ln(8/δ)
2m
+ 2κ ¯S
r
ln(8/δ)
2n
+ κS
r
2 ln(4|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

.
where ˜W = WS −W ∗and PA = I −A(A⊤A)†A⊤.
We will now analyze the second term on the right-hand side of equation 45:
1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥≤∥˜W∥2
 
1
n
n
X
i=1
∥fθ(¯xi) −fθ(xi)∥
!
,
(46)
28

where ∥˜W∥2 is the spectral norm of ˜W. Since ¯xi shares the same label with xi as ¯xi ∼Dyi (and
xi ∼Dyi), and because fθ is trained with the unlabeled data ¯S, using Hoeffding’s inequality (Lemma
G.1) implies that with probability at least 1 −δ,
1
n
n
X
i=1
∥fθ(¯xi) −fθ(xi)∥≤Ey∼ρE¯x,x∼D2y[∥fθ(¯x) −fθ(x)∥] + τ ¯S
r
ln(1/δ)
2n
.
(47)
Moreover, by using [50, Theorem 3.1] with the loss function (x, ¯x) 7→∥fθ(¯x) −fθ(x)∥(i.e., Lemma
G.2), we have that with probability at least 1 −δ,
Ey∼ρE¯x,x∼D2y[∥fθ(¯x) −fθ(x)∥] ≤1
m
m
X
i=1
∥fθ(x+
i ) −fθ(x++
i
)∥+ 2 ˜Rm(F)
√m
+ τ
r
ln(1/δ)
2m
(48)
where ˜Rm(F) =
1
√mE ¯S,ξ[supf∈F
Pm
i=1 ξi∥f(x+
i ) −f(x++
i
)∥] is the normalized Rademacher
complexity of the set {(x+, x++) 7→∥f(x+) −f(x++)∥: f ∈F} (it is normalized such that
˜Rm(F) = O(1) as m →∞for typical choices of F), and ξ1, . . . , ξm are independent uniform
random variables taking values in {−1, 1}. Thus, taking union bound, we have that for any δ > 0,
with probability at least 1 −δ,
1
n
n
X
i=1
∥˜W(fθ(¯xi) −fθ(xi))∥
(49)
≤∥˜W∥2
 
1
m
m
X
i=1
∥fθ(x+
i ) −fθ(x++
i
)∥+ 2 ˜Rm(F)
√m
+ τ
r
ln(2/δ)
2m
+ +τ ¯S
r
ln(2/δ)
2n
!
.
By combining equation 45 and equation 49 using the union bound, we have that with probability at
least 1 −δ,
EX,Y [∥WSfθ(X) −Y ∥]
(50)
≤LS(wS) + ∥˜W∥2
 
1
m
m
X
i=1
∥fθ(x+
i ) −fθ(x++
i
)∥+ 2 ˜Rm(F)
√m
+ τ
r
ln(4/δ)
2m
+ τ ¯S
r
ln(4/δ)
2n
!
+
2
√m∥PAg∥2 + 4Rm(W ◦F)
√m
+ 2κ
r
ln(16/δ)
2m
+ 2κ ¯S
r
ln(16/δ)
2n
+ κS
r
2 ln(8|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

= LS(wS) + ∥˜W∥2
 
1
m
m
X
i=1
∥fθ(x+
i ) −fθ(x++
i
)∥
!
+
2
√m∥PAg∥2 + Qm,n
where
Qm,n = ∥˜W∥2
 
2 ˜Rm(F)
√m
+ τ
r
ln(3/δ)
2m
+ τ ¯S
r
ln(3/δ)
2n
!
+ κS
r
2 ln(6|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

+ 4Rm(W ◦F)
√m
+ 2κ
r
ln(4/δ)
2m
+ 2κ ¯S
r
ln(4/δ)
2n
.
Define Z ¯S = [f(x+
1 ), . . . , f(x+
m)] ∈Rd×m. Then, we have A = [Z ¯S
⊤⊗Ir]. Thus,
PA = I −[Z ¯S
⊤⊗Ir][Z ¯SZ ¯S
⊤⊗Ir]†[Z ¯S ⊗Ir] = I −[Z ¯S
⊤(Z ¯SZ ¯S
⊤)†Z ¯S ⊗Ir] = [PZ ¯
S ⊗Ir]
where PZ ¯
S = Im −Z ¯S
⊤(Z ¯SZ ¯S
⊤)†Z ¯S ∈Rm×m. By defining Y ¯S = [g∗(x+
1 ), . . . , g∗(x+
m)]⊤∈
Rm×r, since g = vec[Y ⊤
¯S ],
∥PAg∥2 = ∥[PZ ¯
S ⊗Ir] vec[Y ⊤
¯S ]∥2 = ∥vec[Y ⊤
¯S PZ ¯
S]∥2 = ∥PZ ¯
SY ¯S∥F
(51)
29

On the other hand, recall that WS is the minimum norm solution as
WS = minimize
W ′
∥W ′∥F s.t. W ′ ∈arg min
W
1
n
n
X
i=1
∥Wfθ(xi) −yi∥2.
By solving this, we have
WS = Y ⊤ZS
⊤(ZSZS
⊤)†,
where ZS = [f(x1), . . . , f(xn)] ∈Rd×n and YS = [y1, . . . , yn]⊤∈Rn×r. Then,
LS(wS) = 1
n
n
X
i=1
∥WSfθ(xi) −yi∥= 1
n
n
X
i=1
v
u
u
t
r
X
k=1
((WSfθ(xi) −yi)k)2
≤
v
u
u
t 1
n
n
X
i=1
r
X
k=1
((WSfθ(xi) −yi)k)2
=
1
√n∥WSZS −Y ⊤∥F
=
1
√n∥Y ⊤(ZS
⊤(ZSZS
⊤)†ZS −I)∥F
=
1
√n∥(I −ZS
⊤(ZSZS
⊤)†ZS)Y ∥F
Thus,
LS(wS) =
1
√n∥PZSY ∥F
(52)
where PZS = I −ZS
⊤(ZSZS
⊤)†ZS.
By combining equation 50–equation 52 and using 1 ≤
√
2, we have that with probability at least
1 −δ,
EX,Y [∥WSfθ(X) −Y ∥] ≤cI ¯S(fθ) +
2
√m∥PZ ¯
SY ¯S∥F +
1
√n∥PZSYS∥F + Qm,n,
(53)
where
Qm,n = c
 
2 ˜Rm(F)
√m
+ τ
r
ln(3/δ)
2m
+ τ ¯S
r
ln(3/δ)
2n
!
+ κS
r
2 ln(6|Y|/δ)
2n
X
y∈Y
p
ˆp(y) +
p
p(y)

+ 4Rm(W ◦F)
√m
+ 2κ
r
ln(4/δ)
2m
+ 2κ ¯S
r
ln(4/δ)
2n
.
Now,
Appendix J
Information Optimization and the VICReg Objective
Assumption 1. The eigenvalues of Σ(xj) are in some range a ≤λ(Σ(xj)) ≤b.
Assumption 2. The differences between the means of the Gaussians are bounded
M = max
i,j ∥µ(Xi) −µ(Xj)∥2
Lemma J.1. The maximum eigenvalue of each µ(Xj)µ(Xj)T is at most M.
30

Proof. The term µ(Xj)µ(Xj)T is an outer product of the mean vector µ(Xj), which is a symmetric
matrix. The eigenvalues of a symmetric matrix are equal to the squares of the singular values of the
original matrix. Since the singular values of a vector are equal to its absolute values, the maximum
eigenvalue of µ(Xj)µ(Xj)T is equal to the square of the maximum absolute value of µ(Xj). By the
second assumption, this is at most M.
Lemma J.2. The maximum eigenvalue of −µZµT
Z is non-positive and its absolute value is at most
M.
Proof. The term −µZµT
Z is a negative outer product of the overall mean vector µZ, which is a
symmetric matrix. Its eigenvalues are non-positive and equal to the negative squares of the singular
values of µZ. Since the singular values of a vector are equal to its absolute values, the absolute value
of the maximum eigenvalue of −µZµT
Z is equal to the square of the maximum absolute value of µZ,
which is also bounded by M by the second assumption.
Lemma J.3. The sum of the eigenvalues of ΣZ is bounded
X
i
λi(Z) ≤(b + M) × K
Proof. Given a Gaussian mixture model where each component Z|xj has mean µ(Xj) and covariance
matrix Σ(xj), the mixture can be written as:
Z =
X
j
pjZ|xj
where pj are the mixing coefficients. The covariance matrix of the mixture, ΣZ, is then given by:
ΣZ =
X
j
pj
 Σ(xj) + µ(Xj)µ(Xj)T 
−µZµT
Z
where µZ is the mean of the mixture distribution.
By Lemmas 1.1, 1.2, and assumptions 1 and 2, the maximum eigenvalues of (Σ(xj), µ(Xj)µ(Xj)T
and µZµT
Z. are at most b, M, and M, respectively. Therefore, by Weyl’s inequality for the sum of
two symmetric matrices, the maximum eigenvalue of ΣZ is at most b + M.
λmax(ΣZ) ≤1
K
K
X
i=1
(max(λ(Σ(Xi))) + M) ≤b + M
It means that we can bound the sum of the eigenvalues of ΣZ with
X
i
λi(ΣZ) ≤(b + M) × K
Lemma J.4. Let ΣZ be a positive semidefinite matrix of size N × N. Consider the optimization
problem given by:
maximize log det(ΣZ)
such that:
N
X
i=1
λi(ΣZ) ≤c
ΣZ ⪰0
where λi(ΣZ) denotes the i-th eigenvalue of ΣZ and c is a constant. The solution to this problem is
a diagonal matrix with equal diagonal elements.
31

Proof. The determinant of a matrix is the product of its eigenvalues, so the objective function
log det(ΣZ) can be rewritten as PN
i=1 log(λi(ΣZ)). Our problem is then to maximize this sum under
the constraints that the sum of the eigenvalues does not exceed c and that ΣZ is positive semi-definite.
Applying Jensen’s inequality to the concave function log(x) with weights 1/N, we find that
1
N
PN
i=1 log(λi(ΣZ)) ≤log( 1
N
PN
i=1 λi(ΣZ)). Equality holds if and only if all λi(ΣZ) are equal.
Setting λi(ΣZ) = x for all i, we see that the constraint PN
i=1 λi(ΣZ) ≤c becomes Nx ≤c, leading
to the optimal eigenvalue x = c/N under the constraint.
Since ΣZ is positive semi-definite, it can be diagonalized via an orthogonal transformation without
changing the sum of its eigenvalues or its determinant. Therefore, the solution to the problem is a
diagonal matrix with all diagonal entries equal to c/N.
This completes the proof.
Theorem J.5. Given a Gaussian mixture model where each component Z|Xi has covariance matrix
Σ(Xi), under the assumptions above, the solution to the optimization problem
maximize
X
i
log
|ΣZ|
|Σ(Xi)|
is a diagonal matrix ΣZ with equal diagonal elements.
Proof. The objective function can be decomposed as follows:
X
i
log
|ΣZ|
|Σ(Xi)| =
X
i
(log |ΣZ| −log |Σ(Xi)|)
= K log |ΣZ| −
X
i
log
Σ(Xi)
 ,
where K is the number of components in the Gaussian mixture model.
In this optimization problem, we are optimizing over ΣZ. The term P
i log |Σ(Xi)| is constant with
respect to ΣZ, therefore we can focus on maximizing K log |ΣZ|.
As the determinant of a matrix is the product of its eigenvalues, log |ΣZ| is the sum of the logs of the
eigenvalues of ΣZ. Thus, maximizing log |ΣZ| corresponds to maximizing the sum of the logarithms
of the eigenvalues of ΣZ.
According to Lemma 1.4, when we have a constraint on the sum of the eigenvalues, the solution to
the problem of maximizing the sum of the logarithms of the eigenvalues of a positive semidefinite
matrix ΣZ is a diagonal matrix with equal diagonal elements.
From Lemma 1.3, we know that the sum of the eigenvalues of ΣZ is bounded by (b + M) × K.
Therefore, when we maximize K log |ΣZ| under these constraints, the solution will be a diagonal
matrix with equal diagonal elements. This completes the proof of the theorem.
Appendix K
Entropy Comparison - Experimental Details
We use ResNet-18 [32] as our backbone. Each model is trained with 512 batch size for 800 epochs.
We use the SGD optimizer with a momentum of 0.9 and a weight decay of 1e−4. The initial
learning rate is 0.5. This learning rate follows the cosine decay with a linear warmup schedule.
For augmentation, two augmented versions of each input image are generated. During this process,
each image is cropped with random size, and resized to the original resolution, followed by random
applications of horizontal mirroring, color jittering, grayscale conversion, Gaussian blurring and
solarization. For the entropy estimation, we use the same method as in [38], which uses a lower
bound of the entropy using the distances of the representations under the assumption of a mixture of
the Gaussians around the representations with constant variance.
H(Z) := H(Z|C) −
X
i
ci ln
X
j
cje−D(pi||pj).
(54)
32

101
102
103
104
Number of SSL Samples (m)
1.00
1.1
1.2
1.3
1.4
1.5
Generalization Bound
SSL Bound
Supervised Bound
0.2
0.4
0.6
0.8
1.0
1.2
Generalization Gap in Loss
0.0
0.2
0.4
0.6
0.8
1.0
VICReg Bound
ResNet18
ResNet30
ResNet56
ResNet110
Polynomial Fit
Figure 5: Our generalization bound predicts more accurately the generalization gap in the loss.
(left) Our SSL VICReg generalization bound outperforms state-of-the-art supervised generalization
bounds. (right) Strong correlation between the generalization gap and our generalization bound for
VICReg. Pearson correlation - 0.9633. Conducted on CIFAR-10.
where pi and pj are the distributions of the representation of the i-th and j-th examples, and D(pi||pj)
represents the divergence between these distributions. Also, ci indicates the weight of component i
(ci ≥0, P
i ci = 1), and C is a discrete random variable where P(C = i) = ci .
Appendix L
Reproducibility Statement
All of the methods in our study are based on existing methods and their open-source implementations.
We provide a detailed implementation setup for both the pre-training and downstream experiments.
Additionally, we are committed to ensuring reproducibility and open science. Therefore, after
publication, we will provide pretrained checkpoints and make the code openly available on a public
repository. This will enable other researchers to reproduce our results and build upon our work.
Appendix M
Expriemtns on the generalization bound
In this section we have more empirical results on the connection between our generalization to the
generalization gap.
Appendix N
Limitations of assuming inherent network randomness
First, we provide a brief overview of the challenges associated with estimating mutual information
(MI) in DNNs. This context will set the stage for a detailed discussion on the advantages of our
method
Estimating MI in DNNs poses various practical challenges. For example, deterministic DNNs with
strict monotonic nonlinearities yield infinite or constant MI, complicating optimization [4] or making
optimization ill-posed. One solution is to assume an unknown distribution over the representation and
estimating MI directly [9, 57]. However, this is challenging theoretically and requires large sample
sizes [55, 47] and is prone to significant estimation errors [70].
To address these issues, prior works have proposed various assumptions about randomness in neural
networks. For example, [1] posits noise in DNNs’ parameters, while [1] assumes noise in the
representations. These assumptions are valid if they outperform deterministic DNNs or yield similar
representations to those trained in practice.
Our empirical evidence suggests that noise in the input has much better performance and represen-
tations similar to the deterministic network compared to noise in the network itself. We compared
our model, which introduces noise at the input level, with models that add noise to network repre-
sentations [26], using CIFAR-100 and Tiny-ImageNet and VICReg with ConvNeXt. The results,
33

tabulated below, indicate that the noise injected into the representation has much worse performance
degradation compared to our input noise model.
Table 2: Comparison of Noisy Network and Noisy Input across different datasets for different noise
levels
β (Noise Level)
Tiny-ImageNet
CIFAR100
Noisy Network
Noisy Input (ours)
Noisy Network
Noisy Input (ours)
β = 0 (no noise)
53.1
53.1
70.1
70.1
β = 0.05
51.7
53.0
69.7
70.0
β = 0.1
50.2
52.8
68.8
69.6
β = 0.2
48.1
52.3
67.1
68.9
Our approach, focusing on input noise, does not necessitate explicit noise injection during training—
unlike methods assuming inherent network noise.
Lastly, we validate that the assumption of input noise leads to better alignment with deterministic
networks. Following [14], we used Centered Kernel Alignment (CKA) [18] to compare deterministic
DNNs with our noise model (input noise) to DNNs with noise in the representations [26]. The results,
also tabulated below, confirm that assuming input noise aligns more closely with deterministic DNN
representations than assuming noise in the DNN parameters.
Table 3: Performance comparison of different methods at varying noise levels
Noise Level/Method
Deterministic Network
Noisy Network
Noisy Input (our method)
β = 0.05
0.97
0.82
0.93
β = 0.1
0.97
0.69
0.85
β = 0.2
0.97
0.54
0.77
β = 0.3
0.97
0.32
0.69
In summary, our input noise model more effectively captures deterministic DNN behavior and is
more robust than assuming inherent network randomness.
Appendix O
Boader Impact
In the landscape of machine learning, SSL algorithms have found a broad array of practical applica-
tions, ranging from image recognition to natural language processing. With the proliferation of data
in recent years, their utility has grown exponentially, often serving as key components in numerous
real-world use cases. This paper introduces a new family of SSL algorithms that have demonstrated
superior performance. Given the widespread use of SSL algorithms, the advancements proposed in
this paper have the potential to considerably enhance the effectiveness of systems that rely on these
techniques. As a result, the impact of these improved algorithms could be far-reaching, potentially
influencing a multitude of applications and sectors where machine learning is currently employed.
With this potential for value also comes the potential that proposed methods make false promises
that will not benefit real-world practitioners and may, in fact, cause harm when deployed in sensitive
applications. For this reason, we release our numerical results and implementation details for the
sake of transparency and reproducibility. As with all new state-of-the-art methods, our improvements
may also improve models used for malicious intentions.
34

