Mathematical 
Statistics for 
Economics and 
Business
Ron C. Mittelhammer
Second Edition

Mathematical Statistics
for Economics and Business
Second Edition


Ron C. Mittelhammer
Mathematical
Statistics
for Economics
and Business
Second Edition
With 93 Illustrations

Ron C. Mittelhammer
School of Economic Sciences
Washington State University
Pullman, Washington
USA
ISBN 978-1-4614-5021-4
ISBN 978-1-4614-5022-1 (eBook)
DOI 10.1007/978-1-4614-5022-1
Springer New York Heidelberg Dordrecht London
Library of Congress Control Number: 2012950028
# Springer Science+Business Media New York 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or
dissimilar methodology now known or hereafter developed. Exempted from this legal reservation
are brief excerpts in connection with reviews or scholarly analysis or material supplied speciﬁcally
for the purpose of being entered and executed on a computer system, for exclusive use by the
purchaser of the work. Duplication of this publication or parts thereof is permitted only under the
provisions of the Copyright Law of the Publisher’s location, in its current version, and permission for
use must always be obtained from Springer. Permissions for use may be obtained through RightsLink
at the Copyright Clearance Center. Violations are liable to prosecution under the respective
Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are
exempt from the relevant protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of
publication, neither the authors nor the editors nor the publisher can accept any legal responsibility
for any errors or omissions that may be made. The publisher makes no warranty, express or implied,
with respect to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

To my wife Linda,
and to the memory of Carl, Edith, Dolly,
and Ralph.


n
Preface to the Second Edition
of Mathematical Statistics for
Economics and Business
n
n
The general objectives of the second edition of Mathemat-
ical Statistics for Economics and Business remain the same as the ﬁrst, namely,
to provide a rigorous and accessible foundation in the principles of probability
and in statistical estimation and inference concepts for beginning graduate
students and advanced undergraduate students studying primarily in the ﬁelds
of economics and business. Since its publication, the ﬁrst edition of the book has
found use by those from other disciplines as well, including the social sciences
(e.g., psychology and sociology), applied mathematics, and statistics, even
though many of the applied examples in later chapters have a decidedly “eco-
nomics and business” feel (although the examples are chosen in such a way that
they are fairly well “self-contained” and understandable for those who have not
studied either discipline in substantial detail).
The general philosophy regarding how and why the book was originally
written was presented in the preface to the ﬁrst edition and in large measure
could be inserted at this point for motivating the fundamental rationale for the
second edition. This philosophy includes the necessity of having a conceptual
base of probability and statistical theory to be able to fully understand the
application and interpretation of applied econometric and business statistics
methods, coupled with the need to have a treatment of the subject that, while
rigorous, also assumes an accessible level of prerequisites that can be expected
to have been met by a large majority of graduate students entering the ﬁelds.
The choice of topic coverage is also deliberate and decidedly chosen to form
the fundamental foundation on which econometric and business statistics
methodology is built. With the ongoing expansion, in both scope and depth,

of econometric and statistical methodology for quantitative analyses in both
economics and business, it has never been more important, and many are now
thinking absolutely essential, that a base of formal probability and statistics
understanding become part of student training to enable effective reading of the
literature and success in the ﬁelds.
Regarding the nature of the updates and revisions that have been made in
producing the second edition, many of the basic probability and statistical
concepts remain in common with the ﬁrst edition. The fundamental base of
probability and statistics principles needed for later study of econometrics,
business statistics, and a myriad of stochastic applications of economic and
business theory largely intersects the topics covered in the ﬁrst edition. While
a few topics were deleted in the second edition as being less central to that
foundation, many more have been added. These include the following: greater
detail on the issue of parametric, semiparametric, and nonparametric models; an
introduction to nonlinear least squares methods; Stieltjes integration has been
added strategically in some contexts where continuous and discrete random
variable properties could be clearly and efﬁciently motivated in parallel; addi-
tional testing methodology for the ubiquitous normality assumption; clearer
differentiation of parametric and semiparametric testing of hypotheses; as well
as many other reﬁnements in topic coverage appropriate for applications in
economics and business.
Perhaps the most important revision of the text has been in terms of the
organization, exposition, and overall usability of the material. Reacting to the
feedback of a host of professors, instructors, and individual readers of the ﬁrst
edition, the presentation of both the previous and new material has been notably
reorganized and rewritten to make the text easier to study and teach from. At the
highest level, the compartmentalization of topics is now better and easier to
navigate through. All theorems and examples are now titled to provide a better
foreshadowing of the content of the results and/or the nature of what is being
illustrated. Some topics have been reordered to improve the ﬂow of reading and
understanding (i.e., the relatively more esoteric concept of events that cannot be
assigned probability consistently has been moved to the end of a chapter and the
review of elements of real analysis has been moved from the beginning of the
asymptotic theory chapter to the appendix of the book), and in some cases,
selected proofs of theorems that were essentially pure mathematics and that
did little to bolster the understanding of statistical concepts were moved to
chapter appendices to improve readability of the chapter text. A large number
of new and expanded exercises/problems have been added to the chapters.
While a number of texts focused on statistical foundations of estimation and
inference are available, Mathematical Statistics for Economics and Business is a
text whose level of presentation, assumed prerequisites, examples and problems,
and topic coverage will continue to provide a solid foundation for future study of
econometrics, business statistics, and general stochastic economic and business
theory and application. With its redesigned topic organization, additional topic
coverage, revision of exposition, expanded set of problems, and continued focus
on accessibility and motivation, the book will provide a conceptual foundation
viii
Preface to the Second Edition of Mathematical Statistics for Economics and Business

on which students can base their future study and understanding of rigorous
econometric and statistical applications, and it can also serve as an accessible
refresher for practicing professionals who wish to reestablish their understand-
ing of the foundations on which all of econometrics, business statistics, and
stochastic economic and business theory are based.
Acknowledgments
In addition to all of the acknowledgments presented in the
ﬁrst edition, which certainly remain deserving of inclusion here, I would like to
thank Ms. Danielle Engelhardt, whose enormous skills in typing, formatting,
and proof-checking of the text material and whose always cheerful and positive
“can-do” personality made the revision experience a much more enjoyable and
efﬁcient process. I am also indebted to Dr. Miguel Henry-Osorio for proofreading
every character of every page of material and pointing out corrections, in addi-
tion to making some expositional suggestions that were very helpful to the
revision process. Mr. Sherzod Akhundjanov also provided expert proof-checking,
for which I am very grateful. I also thank Haylee and Hanna Gecas for their
constant monitoring of my progress on the book revision and for making
sure that I did not stray too far from the targeted timeline for the effort. I also
wish to thank my colleague Dr. Tom Marsh, who utilized the ﬁrst edition of this
book for many years in the teaching of his econometrics classes and who
provided me with helpful feedback on student learning from and topic coverage
in the book. Finally, a deep thank you for the many comments and helpful
suggestions I continued to receive over the years from my many doctoral
students, the students who attended my statistics and econometrics classes
here at the university; the many additional questions and comments I received
from students elsewhere; and the input received from a host of individuals all
over the world – the revision of the book has beneﬁtted substantially from your
input. Thank you all.
Preface to the Second Edition of Mathematical Statistics for Economics and Business
ix


n
Preface (First Edition)
n
n
This book is designed to provide beginning graduate
students and advanced undergraduates with a rigorous and accessible foundation
in the principles of probability and mathematical statistics underlying statisti-
cal inference in the ﬁelds of business and economics. The book assumes no prior
knowledge of probability or statistics and effectively builds the subject “from
the ground up.” Students who complete their studies of the topics in this text
will have acquired the necessary background to achieve a mature and enduring
understanding of statistical and econometric methods of inference and will be
well equipped to read and comprehend graduate-level econometrics texts. Addi-
tionally, this text serves as an effective bridge to a more advanced study of both
mathematical statistics and econometric theory and methods. The book will
also be of interest to researchers who desire a decidedly business and economic-
based treatment of the subject in terms of its topics, depth, breadth, examples,
and problems.
Without the unifying foundations that come with training in probability and
mathematical statistics, students in statistics and econometrics classes too
often perceive the subject matter as a potpourri of formulae and techniques
applied to a collection of special cases. The details of the cases and their
solutions quickly fade for those who do not understand the reasons for using
the procedures they attempt to apply. Many institutions now recognize the need
for a more rigorous study of probability and mathematical statistics principles in
order to prepare students for a higher-level, longer-lasting understanding of the
statistical techniques employed in the ﬁelds of business and economics. Fur-
thermore, quantitative analysis in these ﬁelds has progressed to the point where

a deeper understanding of the principles of probability and statistics is now
virtually necessary for one to read and contribute successfully to quantitative
research in economics and business. Contemporary students themselves know
this and need little convincing from advisors that substantial statistical training
must be acquired in order to compete successfully with their peers and to
become effective researchers. Despite these observations, there are very few
rigorous books on probability and mathematical statistics foundations that are
also written with the needs of business and economics students in mind.
This book is the culmination of 15 years of teaching graduate level statistics
and econometrics classes for students who are beginning graduate programs in
business (primarily ﬁnance, marketing, accounting, and decision sciences), eco-
nomics, and agricultural economics. When I originally took on the teaching
assignment in this area, I cycled through a number of very good texts in mathe-
matical statistics searching for an appropriate exposition for beginning graduate
students. With the help of my students, I ultimately realized that the available
textbook presentations were optimizing the wrong objective functions for our
purposes! Some books were too elementary; other presentations did not cover
multivariate topics in sufﬁcient detail, and proofs of important results were
omitted occasionally because they were “obvious” or “clear” or “beyond the
scope of the text.” In most cases, they were neither obvious nor clear to students,
and in many cases, useful and accessible proofs of the most important results
can and should be provided at this level of instruction. Sufﬁcient asymptotic
theory was often lacking and/or tersely developed. At the extreme, material was
presented in a sterile mathematical context at a level that was inaccessible to
most beginning graduate students while nonetheless leaving notable gaps in
topic coverage of particular interest to business and economics students. Noting
these problems, gaps, and excesses, I began to teach the course from lecture
notes that I had created and iteratively reﬁned them as I interacted with scores of
students who provided me with feedback regarding what was working—and
what wasn’t—with regard to topics, proofs, problems, and exposition. I am
deeply indebted to the hundreds of students who persevered through, and
contributed to, the many revisions and continual sophistication of my notes.
Their inﬂuence has had a substantial impact on the text: It is a time-tested and
class-tested product. Other students at a similar stage of development should
ﬁnd it honest, accessible, and informative.
Instructors attempting to teach a rigorous course in mathematical statistics
soon learn that the typical new graduate student in economics and business is
thoroughly intelligent, but often lacks the sophisticated mathematical training
that facilitates understanding and assimilation of the mathematical concepts
involved in mathematical statistics. My experience has been that these students
can understand and become functional with sophisticated concepts in mathe-
matical statistics if their backgrounds are respected and the material is
presented carefully and thoroughly, using a realistic level of mathematics. Fur-
thermore, it has been my experience that most students are actually eager to see
proofs of propositions, as opposed to merely accepting statements on faith, so
long as the proofs do not insult the integrity of the nonmathematician.
xii
Preface

Additionally, students almost always remark that the understanding and the
long-term memory of a stated result are enhanced by ﬁrst having worked
through a formal proof of a proposition and then working through examples
and problems that require the result to be applied.
With the preceding observations in mind, the prerequisites for the book
include only the usual introductory college-level courses in basic calculus
(including univariate integration and differentiation, partial differentiation,
and multivariate integration of the iterated integral type) and basic matrix
algebra. The text is largely self-contained for students with this preparation.
A signiﬁcant effort has been made to present proofs in ways that are accessible.
Care has been taken to choose methods and types of proofs that exercise and
extend the learning process regarding statistical results and concepts learned
prior to the introduction of the proof. A generous number of examples are
presented with a substantial amount of detail to illustrate the application of
major theories, concepts, and methods. The problems at the end of the chapters
are chosen to provide an additional perspective to the learning process. The
majority of the problems are word problems designed to challenge the reader
to become adept at what is generally the most difﬁcult hurdle—translating
descriptions of statistical problems arising in business and economic settings
into a form that lends itself to solutions based on mathematical statistics
principles. I have also warned students through the use of asterisks (*) when a
proof, concept, example, or problem may be stretching the bounds of the
prerequisites so as not to frustrate the otherwise diligent reader, and to indicate
when the help of an instructor or additional readings may be useful.
The book is designed to be versatile. The course that inspired this book is a
semester-long four-credit intensive mathematical statistics foundation course.
I do not lecture on all of the topics contained in the book in the 50 contact hours
available in the semester. The topics that I do not cover are taught in the ﬁrst
half of a subsequent semester-long three-credit course in statistics and econo-
metric methods. I have tended to treat Chapters 1–4 in detail, and I recommend
that this material be thoroughly understood before venturing into the statistical
inference portion of the book. Thereafter, the choice of topics is ﬂexible. For
example, the instructor can control the depth at which asymptotic theory is
taught by her choice of whether the starred topics in Chapter 5 are discussed.
While random sampling, empirical distribution functions, and sample moments
should be covered in Chapter 6, the instructor has leeway in the degree of
emphasis that she places on other topics in the chapter. Point estimation and
hypothesis testing topics can then be mixed and matched with a minimal
amount of back-referencing between the respective chapters.
Distinguishing features of this book include the care with which topics are
introduced, motivated, and built upon one another; use of the appropriate level
of mathematics; the generous level of detail provided in the proofs; and a familiar
business and economics context for examples and problems. This text is bit
longer than some of the others in the ﬁeld. The additional length comes from
additional explanation, and detail in examples, problems, and proofs, and not
from a proliferation of topics which are merely surveyed rather than fully
Preface
xiii

developed. As I see it, a survey of statistical techniques is useful only after one
has the fundamental statistical background to appreciate what is being sur-
veyed. And this book provides the necessary background.
Acknowledgments
I am indebted to a large number of people for their encouragement and
comments. Millard Hastay, now retired from the Washington State University
economics faculty, is largely responsible for my unwavering curiosity and
enthusiasm for the ﬁeld of theoretical and applied statistics and econometrics.
George Judge has been a constant source of encouragement for the book project
and over the years has provided me with very valuable and selﬂess advice and
support in all endeavors in which our paths have crossed. I thank Jim Chalfant
for giving earlier drafts of chapters a trial run at Berkeley, and for providing me
with valuable student and instructor feedback. Thomas Severini at Northwest-
ern provided important and helpful critiques of content and exposition. Martin
Gilchrist at Springer-Verlag provided productive and pleasurable guidance to the
writing and revision of the text. I also acknowledge the steadfast support of
Washington State University in the pursuit of the writing of this book. Of the
multitude of past students who contributed so much to the ﬁnal product and
that are too numerous to name explicitly, I owe a special measure of thanks to
Don Blayney, now of the Economic Research Service, and Brett Crow, currently
a promising Ph.D. candidate in economics at WSU, for reviewing drafts of the
text literally character by character and demanding clariﬁcation in a number of
proofs and examples. I also wish to thank many past secretaries who toiled
faithfully on the book project. In particular, I wish to thank Brenda Campbell,
who at times literally typed morning, noon, and night to bring the manuscript to
completion, without whom completing the project would have been inﬁnitely
more difﬁcult. Finally, I thank my wife Linda, who proofread many parts of the
text, provided unwavering support, sustenance, and encouragement to me
throughout the project, and despite all of the trials and tribulations, remains
my best friend.
xiv
Preface

n
Contents
n
n
1. Elements of Probability Theory
1
1.1. Introduction
1
1.2. Experiment, Sample Space, Outcome and Event
2
1.3. Nonaxiomatic Probability Deﬁnitions
6
1.4. Axiomatic Deﬁnition of Probability
9
1.5. Some Probability Theorems
16
1.6. Conditional Probability
19
1.7. Independence
25
1.8. Bayes’ Rule
30
1.9. Appendix: Digression on Events that Cannot Be Assigned
Probability
33
Keywords, Phrases, and Symbols
35
Problems
35
2. Random Variables, Densities, and Cumulative
Distribution Functions
45
2.1. Introduction
45
2.2. Univariate Random Variables and Density Functions
46
2.2.1
Probability Space Induced by a Random Variable
47
2.2.2
Discrete Random Variables and Probability Density
Functions
50

2.2.3
Continuous Random Variables and Probability Density
Functions
53
2.2.4
Classes of Discrete and Continuous PDFs
57
2.2.5
Mixed Discrete-Continuous Random Variables
60
2.3. Univariate Cumulative Distribution Functions
62
2.3.1
CDF Properties
64
2.3.2
Duality Between CDFs and PDFs
66
2.4. Multivariate Random Variables, PDFs, and CDFs
68
2.4.1
Multivariate Random Variable Properties
and Classes of PDFs
70
2.4.2
Multivariate CDFs and Duality with PDFs
74
2.4.3
Multivariate Mixed Discrete-Continuous
and Composite Random Variables
78
2.5. Marginal Probability Density Functions and CDFs
78
2.5.1
Bivariate Case
79
2.5.2
n-Variate Case
82
2.5.3
Marginal Cumulative Distribution Functions (MCDFs)
84
2.6. Conditional Density Functions
84
2.6.1
Bivariate Case
85
2.6.2
Conditioning on Elementary Events in Continuous
Cases-Bivariate
87
2.6.3
Conditioning on Elementary Events in Continuous
Cases: n-Variate Case
89
2.6.4
Conditional CDFs
90
2.7. Independence of Random Variables
90
2.7.1
Bivariate Case
91
2.7.2
n-Variate
94
2.7.3
Marginal Densities Do Not Determine an n-Variate
Density Without Independence
95
2.7.4
Independence Between Random Vectors and Between
Functions of Random Vectors
96
2.8. Extended Example of Multivariate Concepts
in the Continuous Case
98
Keywords, Phrases, and Symbols
100
Problems
101
3. Expectations and Moments of Random Variables
111
3.1. Expectation of a Random Variable
111
3.2. Stieltjes Integration and Implicit Support Convention
117
3.3. Expectation of a Function of Random Variables
118
3.4. Expectation Properties
122
3.4.1
Scalar Random Variables
122
3.4.2
Multivariate Random Variables
123
xvi
Contents

3.5. Conditional Expectation
125
3.5.1
Regression Function in the Bivariate Case
129
3.5.2
Conditional Expectation and Regression in the
Multivariate Case
131
3.6. Moments of a Random Variable
132
3.6.1
Relationship Between Moments About the Origin
and Mean
138
3.6.2
Existence of Moments
139
3.6.3
Nonmoment Measures of Probability Density
Characteristics
141
3.7. Moment Generating Functions
142
3.7.1
Uniqueness of MGFs
144
3.7.2
Inversion of MGFs
146
3.8. Cumulant Generating Function
147
3.9. Multivariate MGFs and Cumulant Generating Functions
148
3.10. Joint Moments, Covariance, and Correlation
149
3.10.1
Covariance and Correlation
150
3.10.2
Correlation, Linear Association and Degeneracy
153
3.11. Means and Variances of Linear Combinations
of Random Variables
157
3.12. Appendix: Proofs and Conditions for Positive
Semideﬁniteness
162
3.12.1
Proof of Theorem 3.2
162
3.12.2
Proof of Theorem 3.4 (Jensen’s Inequalities)
164
3.12.3
Necessary and Sufﬁcient Conditions for Positive
Semideﬁniteness
164
Keywords, Phrases, and Symbols
165
Problems
166
4. Parametric Families of Density Functions
175
4.1. Parametric Families of Discrete Density Functions
176
4.1.1
Family Name: Uniform
176
4.1.2
Family Name: Bernoulli
177
4.1.3
Family Name: Binomial
178
4.1.4
Family Name: Multinomial
180
4.1.5
Family Name: Negative Binomial, and Geometric
Subfamily
181
4.1.6
Family Name: Poisson
183
4.1.7
Poisson Process and Poisson PDF
185
4.1.8
Family Name: Hypergeometric
188
4.1.9
Family Name: Multivariate Hypergeometric
190
4.2. Parametric Families of Continuous Density Functions
192
4.2.1
Family Name: Uniform
192
4.2.2
Family Name: Gamma, and Exponential
and Chi-Square Subfamilies
193
Contents
xvii

4.2.3
Gamma Subfamily Name: Exponential
195
4.2.4
Gamma Subfamily Name: Chi-Square
197
4.2.5
Family Name: Beta
200
4.2.6
Family Name: Logistic
202
4.3. The Normal Family of Densities
203
4.3.1
Family Name: Univariate Normal
203
4.3.2
Family Name: Multivariate Normal Density
208
4.3.3
Marginal Normal Densities
212
4.3.4
Conditional Normal Densities
215
4.4. The Exponential Class of Densities
220
Keywords, Phrases, and Symbols
222
Problems
222
5. Basic Asymptotics
231
5.1. Introduction
231
5.2. Convergence in Distribution
233
5.2.1
Asymptotic Distributions
237
5.2.2
Convergence in Distribution for Continuous Functions
238
5.3. Convergence in Probability
239
5.3.1
Convergence in Probability Versus Convergence
in Distribution
241
5.3.2
Properties of the Plim Operator
242
5.3.3
Relationships Involving Both Convergence
in Probability and in Distribution
243
5.3.4
Order of Magnitude in Probability
246
5.4. Convergence in Mean Square (or Convergence
in Quadratic Mean)
247
5.4.1
Properties of the Mean Square Convergence Operator
248
5.4.2
Relationships Between Convergence in Mean Square,
Probability, and Distribution
249
5.5. Almost-Sure Convergence (or Convergence
with Probability 1)
250
5.5.1
Relationships Between Almost-Sure Convergence
and Other Convergence Modes
251
5.5.2
Additional Properties of Almost-Sure Convergence
252
5.6. Summary of General Relationships Between
Convergence Modes
254
5.7. Laws of Large Numbers
254
5.7.1
Weak Laws of Large Numbers (WLLNs)
255
5.7.2
Strong Laws of Large Numbers (SLLNs)
257
5.8. Central Limit Theorems
261
5.8.1
Independent Scalar Random Variables
262
5.8.2
Triangular Arrays
268
5.8.3
Dependent Scalar Random Variables
270
5.8.4
Multivariate Central Limit Theorems
272
xviii
Contents

5.9. Asymptotic Distributions of Differentiable Functions
of Asymptotically Normally Distributed Random Variables:
The Delta Method
273
5.10. Appendix: Proofs and Proof References for Theorems
276
Keywords, Phrases, and Symbols
287
Problems
288
6. Sampling, Sample Moments and Sampling Distributions
295
6.1. Introduction
295
6.2. Sampling
297
6.2.1
Simple Random Sampling
299
6.2.2
Random Sampling Without Replacement
302
6.2.3
General Random Sampling
303
6.2.4
Commonalities in Probabilistic Structure
of Probability Samples
304
6.2.5
Statistics
306
6.3. Empirical Distribution Function
306
6.3.1
EDF: Scalar Case
307
6.3.2
EDF: Multivariate Case
311
6.4. Sample Moments and Sample Correlation
312
6.4.1
Scalar Case
312
6.4.2
Multivariate Case
318
6.5. Properties of Xn and S2
n when Random Sampling
is from a Normal Distribution
326
6.6. Sampling Distributions of Functions of Random Variables
330
6.6.1
MGF Approach
330
6.6.2
CDF Approach
331
6.6.3
Event Equivalence Approach (for Discrete
Random Variables)
332
6.6.4
Change of Variables Approach (for Continuous
Random Variables)
333
6.7. t-and F-Densities
339
6.7.1
t-Density
339
6.7.2
F-Density
343
6.8. Random Sample Simulation
346
6.9. Order Statistics
349
Keywords, Phrases, and Symbols
353
Problems
354
7. Point Estimation Theory
363
7.1. Parametric, Semiparametric, and Nonparametric
Estimation Problems
363
7.1.1
Parametric Models
365
7.1.2
Semiparametric Models
365
Contents
xix

7.1.3
Nonparametric Models
366
7.1.4
Scope of Parameter Estimation Problems
367
7.2. Additional Considerations for the Speciﬁcation
and Estimation of Probability Models
368
7.2.1
Specifying a Parametric Functional Form
for the Sampling Distribution
369
7.2.2
The Parameter Space for the Probability Model
371
7.2.3
A Word on Estimation Phraseology
373
7.3. Estimators and Estimator Properties
373
7.3.1
Evaluating Performance of Estimators
374
7.3.2
Finite Sample Properties
375
7.3.3
Asymptotic Properties
385
7.4. Sufﬁcient Statistics
391
7.4.1
Minimal Sufﬁcient Statistics
395
7.4.2
Sufﬁcient Statistics in the Exponential Class
400
7.4.3
Relationship Between Sufﬁciency
and MSE: Rao-Blackwell
401
7.4.4
Complete Sufﬁcient Statistics, Minimality,
and Uniqueness of Unbiased Estimation
403
7.4.5
Completeness in the Exponential Class
404
7.4.6
Completeness, Minimality, and Sufﬁciency
of Functions of Sufﬁcient Statistics
407
7.5. Minimum Variance Unbiased Estimation
408
7.5.1
Cramer-Rao Lower Bound on the Covariance Matrix
of Unbiased Estimators
411
7.5.2
CRLB Attainment
420
7.5.3
Asymptotic Efﬁciency and the CRLB
422
7.5.4
Complete Sufﬁcient Statistics and MVUEs
423
Keywords, Phrases, and Symbols
424
Problems
425
8. Point Estimation Methods
433
8.1. Introduction
433
8.2. The Least Squares Estimator
434
8.2.1
Linear and Nonlinear Regression Models
434
8.2.2
Least Squares Estimator Under the Classical General
Linear Model Assumptions
436
8.2.3
Violations of Classic GLM Assumptions
455
8.2.4
GLM Assumption Violations: Property Summary
and Epilogue
463
8.2.5
Least Squares Under Normality
464
8.2.6
Overview of Nonlinear Least Squares Estimation
467
8.3. The Method of Maximum Likelihood
472
8.3.1
MLE Implementation
472
8.3.2
MLE Properties: Finite Sample
477
xx
Contents

8.3.3
MLE Properties: Large Sample
479
8.3.4
MLE Invariance Principle and Density
Reparameterization
491
8.3.5
MLE Property Summary
495
8.4. Method of Moments and Generalized Method of Moments
Estimation
496
8.4.1
Method of Moments
497
8.4.2
Generalized Method of Moments (GMM)
502
8.5. Appendix: Proofs and Proof References for Theorems
509
Keywords, Phrases, and Symbols
514
Problems
514
9. Hypothesis Testing Theory
523
9.1. Introduction
523
9.2. Statistical Hypotheses
524
9.3. Basic Hypothesis Testing Concepts
529
9.3.1
Statistical Hypothesis Tests
529
9.3.2
Type I Error, Type II Error, and General Non-Existence
of Ideal Statistical Tests
531
9.3.3
Controlling Type I and II Errors
533
9.3.4
Type I Versus Type II Error Tradeoff and Protection
Against Errors
537
9.3.5
Test Statistics
540
9.3.6
Null and Alternative Hypotheses
541
9.4. Parametric Hypothesis Tests and Test Properties
542
9.4.1
Maintained Hypothesis
543
9.4.2
Power Function
544
9.4.3
Properties of Statistical Tests
545
9.5. Classical Hypothesis Testing Theory: UMP
and UMPU Tests
553
9.5.1
Neyman-Pearson Approach
553
9.5.2
Monotone Likelihood Ratio Approach
563
9.5.3
UMP and UMPU Tests in the Exponential Class
of Densities
569
9.5.4
Conditioning in the Multiple Parameter Case
582
9.5.5
Examples of UMPU Tests in the Multiparameter
Exponential Class
586
9.5.6
Concluding Remarks
596
9.6. Noncentral t-Distribution
597
9.7. Appendix: Proofs and Proof References for Theorems
598
Keywords, Phrases, and Symbols
602
Problems
603
Contents
xxi

10. Hypothesis Testing Methods and Conﬁdence Regions
609
10.1. Introduction
609
10.2. Heuristic Approach to Statistical Test Construction
610
10.3. Generalized Likelihood Ratio Tests
615
10.3.1
GLR Test Properties: Finite Sample
616
10.3.2
GLR Test Properties: Asymptotics
622
10.4. Lagrangian Multiplier Tests
627
10.4.1
LM Tests in the Context of Maximum Likelihood
Estimation
628
10.4.2
LM Tests in Other Estimation Contexts
631
10.5. Wald Tests
633
10.6. Tests in the Context of the Classical GLM
637
10.6.1
Tests When e Is Multivariate Normal
637
10.6.2
Tests in Semiparametric Models
648
10.7. Conﬁdence Intervals and Regions
654
10.7.1
Deﬁning Conﬁdence Intervals and Regions
via Duality with Rejection Regions
656
10.7.2
Properties of Conﬁdence Regions
661
10.7.3
Conﬁdence Regions from Pivotal Quantities
663
10.7.4
Using Conﬁdence Regions to Conduct
Statistical Tests
667
10.8. Nonparametric Tests of Distributional Assumptions
667
10.8.1
Nonparametric Tests of Functional Forms
of Distributions
668
10.8.2
Testing the iid Assumption
677
10.9. Noncentral w2- and F-Distributions
680
10.9.1
Family Name: Noncentral w2-Distribution
680
10.9.2
Family Name: Noncentral F-Distribution
682
10.10. Appendix: Proofs and Proof References for Theorems
683
Keywords, Phrases, and Symbols
689
Problems
689
Math Review Appendix: Sets, Functions,
Permutations, Combinations, Notation, and Real Analysis
697
A1.
Introduction
697
A2.
Deﬁnitions, Axioms, Theorems, Corollaries,
and Lemmas
697
A3.
Elements of Set Theory
698
Set Deﬁning Methods
699
Set Classiﬁcations
701
Special Sets, Set Operations, and Set Relationships
701
Rules Governing Set Operations
703
Some Useful Set Notation
705
A4.
Relations, Point Functions, Set Functions
707
Cartesian Product
707
xxii
Contents

Relation (Binary)
708
Function
709
Real-Valued Point Versus Set Functions
712
A5.
Combinations and Permutations
715
A6.
Summation and Integration Notation
718
A7.
Elements of Real Analysis
720
Sequences of Numbers and Random Variables
720
Limit of a Real Number Sequence
722
Continuous Functions
726
Convergence of a Function Sequence
728
Order of Magnitude of a Sequence
729
Keywords, Phrases, and Symbols
731
Problems
731
Useful Tables
735
B1.
Cumulative Normal Distribution FðxÞ ¼ R x
1
1ﬃﬃﬃﬃ
2p
p
et2=2dt
735
B2.
Student’s t Distribution. The ﬁrst column lists
the number of degrees of freedom (v). The headings
of the other columns give probabilities (P) for t
to exceed the entry value. Use symmetry
for negative t values
736
B3.
Chi-square Distribution. The ﬁrst column lists
the number of degrees of freedom (v). The headings
of the other columns give probabilities (P) for the w2
v
random variable to exceed the entry value
737
B4.
F-Distribution: 5% Points. The ﬁrst column lists
the number of denominator degrees of freedom (v2).
The headings of the other columns list the numerator
degrees of freedom (v1). The table entry is the value
of c for which PðFv1;v2  cÞ ¼ :05
739
B5.
F-Distribution: 1% Points. The ﬁrst column lists
the number of denominator degrees of freedom (v2).
The headings of the other columns list the numerator
degrees of freedom (v1). The table entry is the value
of c for which PðFv1;v2  cÞ ¼ :01
741
Contents
xxiii


n
List of Figures
n
n
Figure 1.1
Conditional sample space B.
20
Figure 1.2
Probability assignments in S.
30
Figure 2.1
Random variable X.
46
Figure 2.2
Event equivalence: event A and associated inverse
image, B, for X.
48
Figure 2.3
Probability represented as area.
55
Figure 2.4
A CDF for a continuous X.
63
Figure 2.5
A CDF for a discrete X.
64
Figure 2.6
A CDF for a mixed discrete-continuous X.
65
Figure 2.7
Discontinuity in a CDF.
66
Figure 2.8
Television screen.
73
Figure 3.1
Weights on a weightless rod.
112
Figure 3.2
Density function “weights” on a weightless rod.
112
Figure 3.3
Continuous mass on a weightless rod.
114
Figure 3.4
Approximation of area under f(x).
115
Figure 3.5
f(x) ¼ 3x2I[0,1](x)
116
Figure 3.6
Density functions and variances.
135
Figure 3.7
Density symmetric about m.
136
Figure 3.8
Nonsymmetric density function of Example 3.15.
137
Figure 3.9
Skewed density functions.
138
Figure 3.10
rXY ¼ +1, Discrete case.
155
Figure 3.11
rXY ¼ +1, Continuous case.
156
Figure 3.12
Convex function g.
164
Figure 4.1
Uniform density, N ¼ 6.
177
Figure 4.2
Bernoulli density, p ¼ .3.
178
Figure 4.3
Binomial density, n ¼ 5, p ¼ 0.3.
179
Figure 4.4
Partial poisson density, l ¼ 4.5.
185

Figure 4.5
Gamma densities, case I, b ¼ 1.
193
Figure 4.6
Gamma densities, case II, a ¼ 3.
194
Figure 4.7
Exponential densities.
197
Figure 4.8
w2 densities.
198
Figure 4.9
Upper and lower a-level tails of a w2 density
199
Figure 4.10
Beta densities.
201
Figure 4.11
Behavior of normal densities for ﬁxed s, m3 > m2 > m1.
204
Figure 4.12
Behavior of normal densities for ﬁxed m, s2 > s1.
204
Figure 4.13
Upper and lower a-level tails of N(m,s2).
207
Figure 4.14
Bivariate normal density; (a) view of normal density
in three-dimensions, and (b) an iso-density ellipse.
210
Figure 4.15
Iso-density ellipses of bivariate normal density
(all origins at (E(X1),E(X2))).
211
Figure 4.16
Regression line of X1 on X2.
218
Figure 5.1
Convergence in distribution (CDF).
234
Figure 5.2
Convergence in distribution (PDFs).
235
Figure 5.3
Graph of limn!1FnðyÞ.
235
Figure 5.4
General convergence mode relationships.
254
Figure 6.1
Overview of the statistical inference process.
296
Figure 6.2
EDF for wheat yields.
308
Figure 6.3
Monotonically increasing function.
334
Figure 6.4
Monotonically decreasing function.
335
Figure 6.5
Function with no inverse.
338
Figure 6.6
Standard normal and t-density.
342
Figure 6.7
t-density symmetry.
343
Figure 6.8
F-density for v1 > 2.
344
Figure 6.9
F-density for v1 ¼ 2.
345
Figure 6.10
F-density for v1 ¼ 1.
345
Figure 7.1
General point estimation procedure.
374
Figure 7.2
Scalar estimator PDFs for various values of .
375
Figure 7.3
The densities of two unbiased estimators of q(Y).
382
Figure 8.1
GLM representation of E(Yi) in bivariate.
437
Figure 9.1
Test of a statistical hypothesis.
530
Figure 9.2
General hypothesis testing procedure.
531
Figure 9.3
Potential outcomes of statistical test relative to true
probability distribution.
531
Figure 9.4
Probabilities of type II error for sample sizes: n ¼ 100
and n ¼ 200.
534
Figure 9.5
Probabilities of type II error for tests of H: p  .02.
539
Figure 9.6
Power function of test for H: p  .2 in Example 9.1.
545
Figure 9.7
Two power functions and the ideal power function for testing
H: Y  c.
546
Figure 9.8
Power function for testing H0: m  15 versus Ha: m > 15,
n ¼ 100.
551
xxvi
List of Figures

Figure 9.9
Power function for the UMPU level .057 test
of H0: p ¼ .5 versus Ha: p 6¼ .5.
573
Figure 9.10
Power function of the UMPU level .10 Test of H0:
m ¼ 2 versus Ha: m 6¼ 2.
575
Figure 9.11
Power function of UMPU level .05 test of H0: y ¼ 5
versus Ha: y 6¼ 5.
578
Figure 9.12
Power function of UMP unbiased level .05 test of H0:
1.75  y  2.25 versus Ha: y < 1.75 or y > 2.25.
580
Figure 9.13
Power function of UMPU level .05 test
of H0: 4.9  y  5.1 Versus Ha: y < 4.9 or y > 5.1.
581
Figure 9.14
Power function of UMPU level .05 unbiased test of H0: s2  4
versus Ha: s2 > 4.
588
Figure 9.15
Graph of RðXjrÞ ¼
x : P2
i¼1 xi  m0
ð
Þ2  r2
n
o
:
592
Figure 9.16
Power function of UMPU level .05 test of H0: m  40
versus Ha: m > 40, where l ¼ ðm40Þ=ðs=
ﬃﬃﬃﬃﬃﬃ
28
p
Þ:
594
Figure 9.17
Power function of UMPU level .05 test of H0:m ¼ 40
versus Ha: m 6¼ 40, where l ¼ (m  40)/(s/
ﬃﬃﬃﬃﬃﬃ
28
p
).
596
Figure 10.1
Power function of UMPU size .05 test of H0: b2 ¼ 0
versus Ha: b2 6¼ 0.
612
Figure 10.2
Asymptotic power of GLR test for H0: Y ¼ Y0
versus Ha: Y > Y0, local alternatives Yn ¼ Y0 + n1/2 d,
l ¼ d2/2.
627
Figure A.1
Venn diagrams illustrating set relationships.
704
Figure A.2
A  B ¼ shaded area.
708
Figure A.3
xSy in shaded area.
710
Figure A.4
Function with domain D(f) and range R(f).
711
Figure A.5
Point versus set function.
713
Figure A.6
Illustration of the sequence {yn} for
which limn!1 yn ¼ y.
723
List of Figures
xxvii


n
List of Tables
n
n
Table 1.1
Disjointness Versus Independence
28
Table 2.1
Relationship Between Original and X-Induced
Probability Spaces
49
Table 4.1
Binomial PDF for n ¼ 5, p ¼ .3
180
Table 4.2
Poisson Density for l ¼ 4.5
185
Table 4.3
Summary of beta density shapes
200
Table 6.1
EDF of wheat yields.
307
Table 8.1
Assumptions and properties of ^b and ^S
2 in the
GLM point estimation problem
453
Table 8.2
General Least Squares Estimator Properties
Under GLM Assumption Violations
463
Table 10.1
Statistical tests of the value of Rb relative to r
643
Table 10.2
Asymptotic level a tests on R b
ð Þ using a Z-statistic
649
Table 10.3
Asymptotic level a tests on s2 using a Z-statistic
653
Table 10.4
Relationships between hypothesis tests
and conﬁdence regions
661
Table 10.5
Pivotal quantities and some associated
conﬁdence regions
664
Table 10.6
Relationships between events for pivotal quantity
and conﬁdence intervals
666
Table A.1
Summation and integration notation
718

1
n
Elements of Probability
Theory
n
n
n
1.1
Introduction
1.2
Experiment, Sample Space, Outcome and Event
1.3
Nonaxiomatic Probability Deﬁnitions
1.4
Axiomatic Deﬁnition of Probability
1.5
Some Probability Theorems
1.6
Conditional Probability
1.7
Independence
1.8
Bayes’ Rule
1.9
Appendix: Digression on Events that Cannot Be
Assigned Probability
1.1
Introduction
The objective of this chapter is to deﬁne a quantitative
measure of the propensity for an uncertain outcome to occur, or the degree of
belief that a proposition or conjecture is true. This quantitative measure will be
called probability and is relevant for quantifying such things as how likely it is
that a shipment of smart phones contains less than 5 percent defectives, that a
gambler will win a crap game, that next year’s corn yields will exceed 80 bushels
per acre, or that electricity demand in Los Angeles will exceed generating
capacity on a given day. This probability concept will also be relevant for
quantifying the degree of belief in such propositions as it will rain in Seattle
tomorrow, Congress will raise taxes next year, and the United States will suffer
another recession in the coming year.
The value of such a measure of outcome propensity or degree of belief can be
substantial in the context of decision making in business, economics, govern-
ment, and everyday life. In the absence of such a measure, all one can effectively
say when faced with an uncertain situation is “I don’t know what will happen”
or “I don’t know whether the proposition is true or false.” A rational decision-
maker will prefer to have as much information as possible about the ﬁnal
outcome or state of affairs associated with an uncertain situation in order to
more fully consider its impacts on proﬁts, utility, welfare, or other measures of
well-being. Indeed, the problem of increasing proﬁt, utility, or welfare through

appropriate choices of production and inventory levels and scheduling, product
pricing, advertising effort, trade policy, tax strategy, input or commodity
purchases, technology adoption, and/or capital investment is substantially
more difﬁcult when the results of one’s choice are affected by factors that are
simply unknown, as opposed to occurring with varying degrees of likelihood.
Probability is a tool for distinguishing likely from unlikely outcomes or
states of affairs and provides business managers, economists, legislators,
consumers, and individuals with information that can be used to rank the
potential results of their decisions in terms of propensity to occur or degree of
validity. It then may be possible to make choices that maximize the likelihood of
a desired outcome, provide a high likelihood of avoiding disastrous outcomes, or
achieve a desirable expected result (where “expected” will be rigorously deﬁned
in the next chapter).
There have been many ways proposed for deﬁning the type of quantitative
measure described above, and that are useful in many respects, but they have not
gained widespread acceptance and/or use. These include the concept of belief
functions (Schaefer), structural probability (Fraser), and ﬁducial probability
(Fisher). Four principal deﬁnitions that have found substantial degrees of accep-
tance and use, and that have been involved in the modern development of
probability theory, include classical probability, relative frequency probability,
subjective probability, and the axiomatic approach. We will brieﬂy discuss the
ﬁrst three deﬁnitions, and then concentrate on the modern axiomatic approach,
which will be seen to subsume the other three approaches as special cases.
Prior to our excursion into the realm of probability theory, it is helpful to
examine how the terms “experiment,” “sample space,” “outcome,” and “event”
will be used in our discussion. The next section provides the necessary
information.
1.2
Experiment, Sample Space, Outcome and Event
The term experiment is used very generally in the ﬁeld of probability and
statistics, and is not at all limited to the colloquial interpretation of the term
as referring to activities that scientists perform in laboratories.
Deﬁnition 1.1
Experiment
Any activity for which the outcome or ﬁnal state of affairs cannot be speciﬁed
in advance, but for which a set containing all potential outcomes or ﬁnal
states of affairs can be identiﬁed.
Thus, determining the yield per acre of a new type of wheat, observing the
quantity of a commodity sold during a promotional campaign, identifying the fat
percentage of a hundredweight of raw farm milk, observing tomorrow’s closing
Dow Jones Industrial Average on the NY Stock Exchange, or analyzing the
underlying income elasticity affecting the demand for gasoline are all examples
of experiments according to this deﬁnition of the term.
2
Chapter 1
Elements of Probability Theory

The ﬁnal state of affairs resulting from an experiment is referred to as an
outcome.
Deﬁnition 1.2
Outcome of an
Experiment
A ﬁnal result, observation, or measurement occurring from an experiment.
Thus, referring to the preceding examples of experiments, 80 bushels per
acre, 2,500 units sold during a week of promotions, 3.7 percent fat per hundred-
weight, a DJIA of 13,500, and an income elasticity of .75 are, respectively,
possible outcomes.
Prior to analyzing probabilities of outcomes of an experiment, it is necessary
to identify what outcomes are possible. This leads to the deﬁnition of the sample
space of an experiment.
Deﬁnition 1.3
Sample Space, S
A set that contains all possible outcomes of a given experiment.
Note that our deﬁnition of sample space, which we will henceforth denote
by S, does not necessarily identify a unique set since we require only that the
sample space contains all possible outcomes of an experiment. In many cases,
the set of all possible outcomes will be readily identiﬁable and not subject to
controversy, and in these cases it will be natural to refer to this set as the sample
space. For example, the experiment of rolling a die and observing the number of
dots facing up has a sample space that can be rather uncontroversially speciﬁed
as {1, 2, 3, 4, 5, 6} (as long as one is ruling out that the die will not land on an
edge!). However, deﬁning the collection of possible outcomes of an experiment
may also require some careful deliberation. For instance, in our example of
measuring the fat percentage of a given hundredweight of raw farm milk, it is
clear that the outcomes must reside in the set A ¼ x : 0  x  100
f
g. However,
the accuracy of our measuring device might only allow us to observe differences
in fat percentages up to hundredths of a percent, and thus a smaller set
containing all possible measurable fat percentages might be speciﬁed as B ¼
0; :01; :02; :::; 100
f
g where B  A. It might be argued further that fat percentages
of greater than 20 percent and less than 1 percent will simply not occur in raw
farm milk, and thus the smaller set C ¼ 1; 1:01; 1:02; :::; 20
f
gwhere C  B  A
could represent the sample space of the fat-measuring experiment. Fortunately,
as the reader will come to recognize, the principal concern of practical impor-
tance is that the sample space be speciﬁed large enough to contain the set of all
possible outcomes of the experiment as a subset. The sample space need not be
identically equal to the set of all possible outcomes. The reader may wish to
suggest appropriate sample spaces for the remaining four example experiments
described above.
Consistent with set theory terminology and the fact that the sample space is
indeed a set, each outcome in a sample space can also be referred to as an
element or member of the sample space. In addition, the outcomes in a sample
1.2
Experiment, Sample Space, Outcome and Event
3

space are also sometimes referred to as sample points. The reader should be
aware of these multiple names for the same concept, and there will be other
instances ahead where concepts are referred to by multiple different names.
The sample space, as all sets, can be classiﬁed according to whether the
number of elements in the set is ﬁnite, countably inﬁnite, or uncountably
inﬁnite.1 Two particular types of sample spaces will ﬁgure prominently in our
study due to the fact that probabilities will ultimately be assigned using either
ﬁnite mathematics, or via calculus, respectively.
Deﬁnition 1.4
Discrete Sample Space
A sample space that is ﬁnite or countably inﬁnite.
Deﬁnition 1.5
Continuous Sample
Space
An uncountably inﬁnite sample space that consists of a continuum of points.
The fundamental entities to which probabilities will be assigned are events,
which are equivalent to subsets in set theoretic terminology.
Deﬁnition 1.6
Event
A subset of the sample space.
Thus, events are simply collections of outcomes of an experiment. Note that
a technical issue in measure theory can arise when we are dealing with
uncountably inﬁnite sample spaces, such that certain complicated subsets can-
not be assigned probability in a consistent manner. For this reason, in more
technical treatments of probability theory, one would deﬁne the term event to
refer to measureable subsets of the sample space. We provide some background
relating to this theoretical problem in the Appendix of this chapter. As a practi-
cal matter, all of the subsets to which an empirical analyst would be interested
in assigning probability will be measureable, and we refrain from explicitly
using this qualiﬁcation henceforth.
In the special case where the event consists of a single element or outcome,
we will use the special term elementary event to refer to the event.
Deﬁnition 1.7
Elementary Event
An event that is a singleton set, consisting of one element of the sample
space.
1A countably inﬁnite set is one that has an inﬁnite number of elements that can be “counted” in the sense of being able to place the
elements in a one-to-one correspondence with the positive integers. An uncountable inﬁnite set has an inﬁnite number of elements
that cannot be counted, i.e., the elements of the set cannot be placed in a one-to-one correspondence with the positive integers.
4
Chapter 1
Elements of Probability Theory

One says that an event A has occurred if the experiment results in an
outcome that is a member or element of the event or subset A.
Deﬁnition 1.8
Occurrence
of an Event
An event is said to have occurred if the outcome of the experiment is an
element of the event.
The real-world meaning of the statement “the event A has occurred” will be
provided by the real-world deﬁnition of the set A. That is, verbal or mathemati-
cal statements that are utilized in a verbal or mathematical deﬁnition of set A, or
the collection of elements or description of elements placed in brackets in an
exhaustive listing of set A, provide the meaning of “the event A has occurred.”
The following examples illustrate the meaning of both event and the occurrence
of an event.
Example 1.1
Occurrence of
Dice Events
An experiment consists of rolling a die and observing the number of dots facing
up. The sample space is deﬁned to be S ¼ {1, 2, 3, 4, 5, 6}. Examine two events in
S: A1 ¼ {1, 2, 3}and A2 ¼ {2, 4, 6}. Event A1 has occurred if the outcome, x, of the
experiment (the number of dots facing up) is such that x∈A1. Then A1 is an event
whose occurrence means that after a roll, the number of dots facing up on the die
is three or less. Event A2 has occurred if the outcome, x, is such that x∈A2. Then
A2 is an event whose occurrence means that the number of dots facing up on the
die is an even number.
□
Example 1.2
Occurrence of
Survey Events
An experiment consists of observing the percentage of a large group of
consumers, representing a consumer taste panel, who prefer Schpitz beer to its
closest competitor, Nickelob beer. The sample space for the experiment is
speciﬁed as S ¼
x : 0  x  100
f
g . Examine two events in S:A1 ¼ {x:x < 50},
and A2 ¼ {x:x > 75}. Event A1 has occurred if the outcome, x, of the experiment
(the actual percentage of the consumer panel preferring Schpitz beer) is such that
x∈A1. Then A1 is an event whose occurrence means that less than 50 percent of
the consumers preferred Schpitz to Nickelob or, in other words, the group of
consumers preferring Schpitz were in the minority. Event A2 has occurred if the
outcome x∈A2. Then A2 is an event whose occurrence means that greater than
75 percent of the consumers preferred Schpitz to Nickelob.
□
When two events have no outcomes in common, they are referred to as
disjoint events.
Deﬁnition 1.9
Disjoint Events
Events that are mutually exclusive, having no outcomes in common.
The concept is identical to the concept of mutually exclusive or disjoint sets,
where A1 and A2are disjoint events iff A1 \ A2 ¼ ;. Examples 1.1 and 1.2 can be
used to illustrate the concept of disjoint events. In Example 1.1, it is recognized
that events A1 and A2 are not mutually exclusive events, sinceA1 \ A2 ¼ 2
f g 6¼ ;.
1.2
Experiment, Sample Space, Outcome and Event
5

Events that are not mutually exclusive can occur simultaneously. Events A1 and
A2 will occur simultaneously (which cannot be the case for mutually exclusive
events)iff x 2 A1 \ A2 ¼ 2
f g. In Example 1.2, events A1 and A2 are disjoint events
since A1 \ A2 ¼ ;. Events A1 and A2 cannot occur simultaneously since if the
outcome is such that x∈A1, then it follows that x=2A2, or if x∈A2, then it follows
that x=2A1.
We should emphasize that in applications it is the researcher who speciﬁes
the events in the sample space whose occurrence or lack thereof provides useful
information from the researcher’s viewpoint. Thus, referring to Example 1.2,
if the researcher were employed by Schpitz Brewery, the identiﬁcation of which
beer was preferred by a majority of the participants in a taste comparison would
appear to be of signiﬁcant interest to the management of the brewery, and thus
event A1 would be of great importance. Event A2 in that example might be
considered important if the advertising department of Schpitz Brewery wished
to utilize an advertising slogan such as “Schpitz beer is preferred to Nickelob by
more than 3 to 1.”
1.3
Nonaxiomatic Probability Deﬁnitions
There are three prominent nonaxiomatic deﬁnitions of probability that have
been suggested in the course of the development of probability theory. We brieﬂy
discuss each of these alternative probability deﬁnitions. In the deﬁnition below,
N(A) is the size-of-set function whose value equals the number of elements that
are contained in the set A (see Deﬁnition A.21).
Deﬁnition 1.10
Classical Probability
Let S be the sample space for an experiment having a ﬁnite number N(S) of
equally likely outcomes, and let A  S be an event containing N(A) elements.
Then the probability of the event A, denoted by P(A), is given by P(A) ¼
N(A)/N(S).
In the classical deﬁnition, probabilities are images of sets generated by a set
function P ð Þ. The domain of P ð Þ is DðPÞ ¼ A : A  S
f
g, i.e., the collection of all
subsets of a ﬁnite (and thus discrete) sample space, while the range is contained
in the unit interval, i.e., RðPÞ  0; 1
½
, since NðAÞ  NðSÞ. The following example
illustrates the application of the classical probability concept.
Example 1.3
Fair Dice and
Classical Probability
Reexamine the die-rolling experiment of Example 1.1, and now assume that the
die is fair so that the outcomes in the sample space S ¼ {1, 2, 3, 4, 5, 6} are equally
likely. The number of elements in the sample space is given by N(S) ¼ 6. Let Ei,
i ¼ 1,. . .,6, represent the elementary events in the set S. Then according to the
classical probability deﬁnition, P(Ei) ¼ N(Ei)/N(S) ¼ 1/6 for all i ¼ 1,. . .,6, so
that the probability of each elementary event is 1/6. Referring to the events A1
and A2 of Example 1.1, note that
6
Chapter 1
Elements of Probability Theory

PðA1Þ ¼ NðA1Þ
NðSÞ ¼ 3
6 ¼ 1
2 and PðA2Þ ¼ NðA2Þ
NðSÞ ¼ 3
6 ¼ 1
2 :
Therefore, the probability of rolling a three or less and the probability of
rolling an even number are both 1/2. Note ﬁnally that P(S) ¼ N(S)/N(S) ¼
6/6 ¼ 1, which states that the probability of the event that the outcome of the
experiment is an element of the sample space is 1, as it intuitively should be if
the number 1 is to be associated with an event that will occur with certainty.□
The classical deﬁnition has two major limitations that preclude its use as
the foundation on which to build a general theory of probability. First, the
sample space must be ﬁnite or else N(S) ¼ 1 and, depending on the event,
possibly N(A) ¼ 1 as well. Thus, probability in the classical sense is not useful
for deﬁning the probabilities of events contained in a countably inﬁnite or
uncountably inﬁnite sample space. Another limitation of the classical deﬁnition
is that outcomes of an experiment must be equally likely. Thus, for example,
if in a coin-tossing experiment it cannot be assumed that the coin is fair, the
classical probability deﬁnition provides no information about how probabilities
should be deﬁned. In order to relax these restrictions, we examine the relative
frequency approach.
Deﬁnition 1.11
Relative Frequency
Probability
Let n be the number of times that an experiment is repeated under identical
conditions. Let A be an event in the sample space S, and deﬁne nA to be the
number of times in n repetitions of the experiment that the event A occurs.
Then the probability of event A is equal to PðAÞ ¼ limn!1nA n
= .
It is recognized that in the relative frequency deﬁnition, the probability of an
event A is the image of A generated by a set function P(), where the image is
deﬁned as the limiting fraction of the total number of outcomes of the n
experiments that are observed to be members of the set A. As in the classical
deﬁnition of probability, the domain of P ð Þ is DðPÞ ¼ A : A  S
f
g , i.e., the
collection of all subsets of the sample space, while the range is contained in
the unit interval, i.e., RðPÞ  0; 1
½
, since 0  nA  n. The following example
illustrates the application of the relative frequency concept of probability.
Example 1.4
Coin Tossing and
Relative Frequency
Probability
Consider the following collection of coin-tossing experiments, where a coin was
tossed various numbers of times and, based on the relative frequency deﬁnition
of probability, the fraction of the tosses resulting in heads was recorded for each
collection of experiments.2
2These experiments were actually performed by the author, except the author did not actually physically ﬂip the coins to obtain the
results listed here. Rather, the coin ﬂips were simulated by the computer. In the coming chapters the reader will come to understand
exactly how the computer might be used to simulate the coin-ﬂipping experiment, and how to simulate other experiments as well.
1.3
Nonaxiomatic Probability Deﬁnitions
7

No. of tosses
No. of heads
Relative frequency
100
48
.4800
500
259
.5180
1,000
489
.4890
5,000
2,509
.5018
75,000
37,447
.4993
It would appear that as n ! 1, the observed relative frequency of heads is
approaching .5.
□
The relative frequency deﬁnition enjoys some advantages over the classical
deﬁnition. For one, the sample space can be an inﬁnite set, since the ability to form
the relative frequency nA/n does not depend on the underlying sample space being
ﬁnite. Also, there is no need to assume that outcomes are equally likely, since the
concept of limn!1nA n
=
does not depend on the outcomes being equally likely.
Unfortunately, there are problems with the relative frequency deﬁnition
that reduce its appeal as a foundation for the development of a general theory
of probability. First of all, while it is an empirical fact in many types of
experiments, such as the coin-tossing experiment in Example 1.4, that the
relative frequencies tend to stabilize as n increases, how do we know that nA/n
will actually converge to a limit in all cases? Indeed, how could we ever observe
the limiting value if an inﬁnite number of repetitions of the experiment
are required? Furthermore, even if there is convergence to a limiting value in
one sequence of experiments, how do we know that convergence to the same
value will occur in another sequence of the experiments? Lacking a deﬁnitive
answer to these conceptual queries, we refrain from using the relative frequency
deﬁnition as the foundation for the probability concept.
A third approach to deﬁning probability involves personal opinion,
judgments, or educated guesses, and is called subjective probability.
Deﬁnition 1.12
Subjective Probability
A real number, P(A), contained in [0,1] and chosen to express the degree of
personal belief in the likelihood of occurrence or validity of event A,
the number 1 being associated with certainty.
Like the preceding deﬁnitions of probability, subjective probabilities can be
viewed as images of set functions P ð Þ having domain DðPÞ ¼ A : A  S
f
g and
range RðPÞ  0; 1
½
. Note that the subjective probability assigned to an event can
obviously vary depending on who is assigning the probabilities and the personal
beliefs of the individual assigning the probabilities. Even supposing that two
individuals possess exactly the same information regarding the characteristics of
an experiment, the way in which each individual interprets the information may
result in differing probability assignments to an event A.
8
Chapter 1
Elements of Probability Theory

Unlike the relative frequency approach, subjective probabilities can
be deﬁned for experiments that cannot be repeated. For example, one might be
assigning probability to the proposition that a recession will occur in the coming
year. Deﬁning the probability of the event “recession next year” does not
conveniently ﬁt into the relative frequency deﬁnition of probability, since one
can only run the experiment of observing whether a recession occurs next year
once. In addition, the classical deﬁnition would not apply unless a recession, or
not, were equally likely, a priori. Similarly, assigning probability to the event
that one or the other team will win in a Superbowl game is commonly done in
various ways by many individuals, and a considerable amount of betting is based
on those probability assignments. However, the particular Superbowl “experi-
ment” cannot be repeated, nor is there usually any a priori reason to suspect that
the outcomes are equally likely so that neither relative frequency nor classical
probability deﬁnitions apply.
In certain problem contexts the assignment of probabilities solely on the
basis of personal beliefs may be undesirable. For example, if an individual is
betting on some game of chance, that individual would prefer to know the
“true” likelihood of the game’s various outcomes and not rely merely on his or
her personal perceptions. For example, after inspecting a penny, suppose you
consider the coin to be fair and (subjectively) assign a probability of ½ to each of
the outcomes “heads” and “tails.” However, if the penny was supplied by a
ruthless gambler who altered the penny in such a way that an outcome of heads
is twice as likely to occur as tails, the gambler could induce you to bet in such a
way that you would lose money in the long run if you adhered to your initial
subjective probability assignments and bet as if both outcomes were equally
likely – the game would not be “fair.”
There is another issue relating to the concept of subjective probability that
needs to be considered in assessing its applicability. Why should one assume
that the numbers assigned by any given individual behave in a manner that
makes sense as a measure of the propensity for an uncertain outcome to occur,
or the degree of belief that a proposition or conjecture is true? Indeed, they might
not, and we seek criteria that individuals must follow so that the numbers they
assign do make sense as probabilities.
Given that objective (classical and relative frequency approaches) and sub-
jective probability concepts might both be useful, depending on the problem
situation, we seek a probability theory that is general enough to accommodate
all of the concepts of probability discussed heretofore. Such an accommodation
can be achieved by deﬁning probability in axiomatic terms.
1.4
Axiomatic Deﬁnition of Probability
Our objective is to devise a quantitative measure of the propensity of events to
occur, or the degree of belief in various events contained in a sample space. How
should one go about deﬁning such a measure? A useful approach is to deﬁne the
measure in terms of properties that are believed to be generally appropriate and/
1.4
Axiomatic Deﬁnition of Probability
9

or necessary for the measure to make sense for its intended purpose. So long as
the properties are not contradictory, the properties can then be viewed collec-
tively as a set of axioms on which to build the concept of probability.
Note, as an aside, that the approach of using a set of axioms as the founda-
tion for a body of theory should be particularly familiar to students of business
and economics. For example, the neoclassical theory of the consumer is founded
on a set of behavioral assumptions, i.e., a set of axioms. The reader might recall
that the axioms of comparability, transitivity, and continuity of preferences are
sufﬁcient for the existence of a utility function, the maximization of which,
subject to an income constraint, depicts consumption behavior in the neoclassi-
cal theory.3 Many other disciplines have axiomatic bases for bodies of theory.
What mathematical properties should a measure of probability possess? First
of all, it seems useful for the measure to be in the form of a real-valued set
function, since this would allow probabilities of events to be stated in terms of
real numbers, and moreover, it is consistent with all of the prior deﬁnitions of
probability reviewed in the previous section. Thus, we begin with a set function,
say P, which has as its domain all of the events in a sample space, S, and has as its
range a set of real numbers, i.e., we have P : U ! R, where U is the set of all events
in S and R denotes the real line 1; 1
ð
Þ. The set U is called the event space.
Deﬁnition 1.13
Event space
The set of all events in the sample space S.
We have in mind that the images of events under P will be probabilities of
the events, i.e., P(A) will be the probability of the eventA 2 U. Now, what type of
properties seem appropriate to impose on the real-valued set function P?
Reviewing the three deﬁnitions of probability presented in Section 1.3, it is
recognized that in each case, probability was deﬁned to be a nonnegative num-
ber. Since each of the previous nonaxiomatic deﬁnitions of probability possess
some intuitive appeal as measures of the propensity of an event to occur or the
degree of belief in an event (despite our recognition of some conceptual
difﬁculties), let us agree that the measure should be nonnegative valued. By
doing so, we will have deﬁned the ﬁrst axiom to which the measure must adhere
while remaining consistent with all of our previous probability deﬁnitions.
Since we decided that our measure would be generated by a set function, P, our
assumption requires that the set function be such that the image of any event A,
P(A), be a nonnegative number. Our ﬁrst axiom is thus
3See G. Debreu (1959), Theory of Value: An Axiomatic Analysis of Economic Equilibrium. Cowles Monograph 17. New York: John
Wiley, pp. 60–63. Note that additional axioms are generally included that are not needed for the existence of a utility function per se,
but that lead to a simpliﬁcation of the consumer maximization problem. See L. Phlips (1983), Applied Consumption Analysis. New
York: North Holland, pp. 8–11.
10
Chapter 1
Elements of Probability Theory

Axiom 1.1
Nonnegativity
For any event A  S; PðAÞ  0:
Now that we have committed to a measure that is nonnegative, what
nonnegative number should the measure associate with the certain event, S?4
There are some advantages to choosing the number 1 to denote the propensity of
occurrence of, or the degree of belief in, the certain event. First, it is consistent
with all of our nonaxiomatic deﬁnitions of probability discussed earlier. Second,
it allows the probability of any event A  S to be directly interpreted as a
proportion of certainty. That is, if we assume that our set function is such that
P(S) ¼ 1, and if P(A) ¼ k, say, then the measure of event A relative to the
measure of the certain event S is P(A)/P(S) ¼ k/1 ¼ k, so that P(A) ¼ k P(S),
and thus the event A is assigned a proportion, k, of certainty. Our second
axiom is then
Axiom 1.2
Probability of the
Certain Event
P(S) ¼ 1.
Regarding the value of k above, it is clear that what we intuitively had in
mind was a number k∈[0,1]. Our intuitive reasoning would be that if S is the
certain event, then the occurrence ofA  Ssurely cannot be “more than certain.”
That is, unless A ¼ S, there are outcomes in S that are not in A (i.e., S  A 6¼ ;).
Thus, while the event S will always occur, the event A may or may not, and
surely A is no more certain to occur than S. This suggests that our measure must
be such that P(A)  1. However, we can proceed further and extend this argu-
ment. If A and B are any two events such thatA  B, then following the same
logic, we would require that P(A)  P(B), since every element of A is also in B, but
B may contain outcomes that are not in A, and thus A can surely be no more
likely to occur than B.
Investigating this line of reasoning still further, if A  B, and thus P(B) 
P(A), to what should we ascribe the remaining portion, P(B)  P(A), of the
probability assigned to event B? An intuitively obvious answer comes to mind.
The set BA represents the additional outcomes remaining in B after we remove
the outcomes it has in common with A. Since A and BA are disjoint and B ¼ A
[ B  A
ð
Þ, the event B can be partitioned5 into two disjoint events. Represented
this way, B can occur iff either A or (B  A) occur. If P(B) > P(A), the added
probability, P(B)  P(A), of event B occurring compared to event A must be due
to the probability of the occurrence of the event B  A. Thus, we must attribute
any remaining probability measure, P(B)  P(A), to the event B  A.
4By deﬁnition, since S contains all possible outcomes of the experiment, the event S is then certain to occur.
5A partition of a set B is a collection of disjoint subsets of B, say Bi; i 2 I
f
g such that B ¼ [i2IBi.
1.4
Axiomatic Deﬁnition of Probability
11

Then our measure should have the property that for events A and B for
which A  B, P(B) ¼ P(A) + P(B  A). Note that since P(B  A)  0 by Axiom
1.1, this implies our previous requirement that if A  B, P(B)  P(A). However,
we have discovered much more than just another way of stating a potential third
axiom. We have actually found that for any two disjoint events A1 and A2, our
measure should have the property that P A1 [ A2
ð
Þ ¼ P A1
ð
Þ þ P A2
ð
Þ. To see this,
deﬁne the set B ¼ A1 [ A2, where A1 \ A2¼;. Then B  A1 ¼ A2 because A1 and
A2 are disjoint, and substituting A2 for (B  A1) in P(B) ¼ P(A1) + P(B  A1),
yields P(A1 [ A2) ¼ P(A1) + P(A2). Thus, we have demonstrated that probability
should be additive across any two disjoint events.
The preceding additivity argument can be extended to three or more disjoint
events. To motivate the extension, ﬁrst examine the case of three disjoint
events, A1, A2, and A3. Note that the two events A1 [ A2 and A3 are a pair of
disjoint events, since
ðA1 [ A2Þ \ A3 ¼ ðA1 \ A3Þ [ ðA2 \ A3Þ ¼ ; [ ; ¼ ;
where Ai\Aj ¼ ;fori 6¼ jby the disjointness of A1, A2, and A3. Then applying our
probability additivity result for the case of two disjoint events results in P(A1 [ A2
[ A3) ¼ P(A1 [ A2) + P(A3). But since A1 and A2 are disjoint, P(A1 [ A2) ¼
P(A1) + P(A2), so that by substitution for P(A1 [ A2), we obtain P(A1 [ A2
[ A3) ¼ P(A1) + P(A2) + P(A3), which implies that probability is additive
across any three disjoint events. Recognizing the sequential logic of the
extension of probability additivity from two to three disjoint events, the
reader can no doubt visualize the repetition of the argument ad inﬁnitum
to establish that probability should be additive across an arbitrary number
of disjoint events. A concise, formal way of establishing the extension is
through the use of mathematical induction.
Lemma 1.1
Mathematical
Induction Principle
Let P1, P2, P3,. . . be a sequence of propositions. Each of the propositions in the
sequence is true provided
(a) P1 is true; and
(b) For an arbitrary positive integer k, if Pk were true, it would necessarily
follow that Pk+1 is true.
Returning to our probability additivity argument, the ﬁrst proposition in the
sequence of propositions we are interested in is “P(A1 [ A2) ¼ P2
i¼1 P Ai
ð
Þ for
disjoint events Ai,i ¼ 1,2.” We have already defended the validity of this propo-
sition. Now consider the proposition that for some k, “P [k
i¼1Ai


¼ Pk
i¼l P Ai
ð
Þfor
disjoint events Ai,i ¼ 1, 2,. . .,k.” Using the method of mathematical induction,
we tentatively act as if this proposition were true, and we attempt to demon-
strate that the truth of the next proposition in the sequence follows from the
truth of the previous proposition, i.e., is “P [kþ1
i¼1 Ai


¼ Pkþ1
i¼l P Ai
ð
Þfor disjoint
12
Chapter 1
Elements of Probability Theory

events Ai,i ¼ 1, 2,. . .,k + 1” then true? Note that the two events [k
i¼1Ai and Ak+1
are disjoint, since
[
k
i¼1
Ai
 
!
\ Akþ1 ¼
[
k
i¼1
Ai \ Akþ1
ð
Þ ¼
[
k
i¼1
; ¼ ;
where Ai \ Akþ1 ¼ ;8i 6¼ k þ 1by the disjointness of the k + 1 events A1,. . .,Ak+1.
But then by additivity for the two-event case,
P
[
k
i¼1
Ai
 
! [
Akþ1
 
!
¼ P
[
k
i¼1
Ai
 
!
þ P Akþ1
ð
Þ ¼
X
kþ1
i¼1
P Ai
ð
Þ;
where the last equality follows from the assumed validity of probability additiv-
ity in the k-disjoint event case. Then by mathematical induction, we have
demonstrated that P [m
i¼1Ai


¼ Pm
i¼1 P Ai
ð
Þ for disjoint events A1; . . . ; Am; 8 posi-
tive integer m, i.e., probability is additive across any number of disjoint events.
We ﬁnally state our probability additivity requirement as a third probabil-
ity axiom, where we generalize the representation of the collection of disjoint
events by utilizing an index set of subscripts rather than unnecessarily
restricting ourselves to an increasing ordered integer sequence given by 1, 2,
3, . . .,m.
Axiom 1.3
Countable Additivity
Let I be a ﬁnite or countably inﬁnite index set of positive integers, and let
{Ai: i∈I}
be
a
collection
of
disjoint
events
contained
in
S.
Then,
P [i2IAi
ð
Þ ¼ P
i2I P Ai
ð
Þ:
The Russian mathematician A.N. Kolmogorov suggested that Axioms
1.1–1.3 provide an axiomatic foundation for probability theory.6 As it turns
out, sufﬁcient information concerning the behavior of probability is contained
in the three axioms to be able to derive from them the modern theory of
probability. We begin deriving some important probability results in the next
section.
In summary, we have deﬁned the concept of probability by deﬁning a num-
ber of properties that probabilities should possess. Speciﬁcally, probabilities will
be generated by a set function that has the collection of events of a sample space,
i.e., the event space, as its domain; its range will be contained in the interval
[0,1]; the image of the certain event S will be 1; and the probability of a countable
union of disjoint events of S will be equal to the sum of the probabilities of the
individual events comprising the union. Any set function, P(), that satisﬁes the
6See A.N. Kolmogorov (1956) Foundations of the Theory of Probability, 2nd ed. New York: Chelsea.
1.4
Axiomatic Deﬁnition of Probability
13

three Axioms 1.1, 1.2, and 1.3 will be called a probability measure or probability
set function. The image of an event A generated by a probability set function P is
called the probability of event A.
Deﬁnition 1.14
Probability Set Function
(or Probability
Measure)
A set function that adheres to the three axioms of probability.
Deﬁnition 1.15
Probability
An image of an event generated by a probability set function.
The following examples provide illustrations of probability set functions for
ﬁnite, countably inﬁnite, and uncountably inﬁnite sample spaces.
Example 1.5
Probability Set Function
Veriﬁcation: Discrete S
Let S ¼ {1, 2, 3, 4, 5, 6} be the sample space for rolling a fair die and observing the
number of dots facing up. Then P(A) ¼ N(A)/6, for A  S, deﬁnes a probability set
function on the events in S. We can verify that P() is a probability measure
by noting that P(A)  0 for all A  S, P(S) ¼ N(S)/6 ¼ 6/6 ¼ 1 and P [i2IAi
ð
Þ ¼
P
i2I P Ai
ð
Þfor any collection
Ai; i 2 I
f
gof disjoint subsets of S. For example, if
A1 ¼ {1,2} and A2 ¼ {4, 5, 6}, then
P A1
ð
Þ ¼ N A1
ð
Þ
6
¼ 2=6;
P A2
ð
Þ ¼ N A2
ð
Þ
6
¼ 3=6;
P A1 [ A2
ð
Þ ¼ N A1 [ A2
ð
Þ
6
¼ 5=6;
and thus P(A1 [ A2) ¼ P(A1) + P(A2). More generally, if
Ai; i 2 I
f
gare disjoint
events, then
N
[
i2I
Ai
 
!
¼
X
i2I
N Ai
ð
Þ;
and thus
P
[
i2I
Ai
 
!
¼
X
i2I
N Ai
ð
Þ=6 ¼
X
i2I
P Ai
ð
Þ:
□
Example 1.6
Probability Set Function
Veriﬁcation:
Countable S
Let S ¼ {x: x is a positive integer}, and examine the set function deﬁned by
P(A) ¼ P
x2A (1/2)x for A  S. The set function, so deﬁned, is a probability set
function since, ﬁrst of all, P(A)  0 because P(A) is deﬁned as the sum of a
collection of nonnegative numbers. To verify that P(S) ¼ 1, recall the following
results from real analysis:
14
Chapter 1
Elements of Probability Theory

Lemma 1.1
S
n
j¼1 arj ¼
a rrnþ1
ð
Þ
1r
and for |r| < 1, S
1
j¼1 arj ¼ lim
n!1
aðrrnþ1Þ
1r
¼ ar
1r :
In
the
case
at
hand,
a ¼ 1
and
r ¼ 1/2,
so
that
PðSÞ ¼ P1
x¼1 1=2
ð
Þx ¼ 1=2
ð
Þ
1  1=2
ð
Þ
ð
Þ
=
¼ 1. Finally, by deﬁnition of the summa-
tion operation, if Ai, i∈I, are disjoint subsets of S, then
P
[
i2I
Ai
 
!
¼
X
x2 [i2IAi
1=2
ð
Þx ¼
X
i2I
X
x2Ai
1=2
ð
Þx ¼
X
i2I
P Ai
ð
Þ:
□
Example 1.7
Probability Set Function
Veriﬁcation:
Uncountable S
Let S ¼ x : 0  x<1
f
g be the sample space corresponding to the experiment of
observing the operating life, in hours, of computer memory chips produced by a
chip
manufacturer.
Let
the
probability
set
function
be
given
by
PðAÞ ¼
R
x2A
1
2 ex=2 dx , with the event space, U, (the domain of P) being the
collection of all interval subsets of S together with any sets that can be formed
by a countable number of union, intersection, and/or complement operations
applied to the interval subsets.7
We can verify that PðAÞ  08A 2 U, since PðAÞ ¼
R
x2A
1
2 ex=2 dx has a non-
negative integrand and the integral of a nonnegative integrand is nonnegative
valued.8 It is also true that P(S) ¼ 1, since PðSÞ ¼
R 1
0
1
2 ex=2 dx ¼  ex=2 j1
0 ¼ 1.
Finally, if A ¼ [n
i¼1 Ai, with the sets A1, A2,. . .,An being disjoint, it follows from
the additivity property of the Riemann integral that
P
[
n
i¼1
Ai
 
!
¼
Z
x2[n
i¼1Ai
1
2 ex=2 dx ¼ S
n
i¼1
Z
x2Ai
1
2 ex=2 dx ¼ S
n
i¼1 PðAiÞ;
and so countable additivity holds.
□
All problems involving the assignment of probabilities to the various events
in a sample space will formally share a common mathematical structure given
by a three-tuple of objects, collectively referred to as the probability space of an
experiment.
7Unlike the previous example which used a countable sample space, when the sample space is uncountable, not all subsets of S can
technically be considered events, i.e., there may be subsets of S to which probability cannot be assigned. The collection of subsets
deﬁned here are the Borel sets contained in S, all of which can be considered events in S. We discuss this technical question further in
the Appendix to the Chapter.
8We will tacitly assume, unless explicitly stated otherwise, that the orientation of integral ranges is from lowest to highest values in
deﬁning the integral over any set A.
1.4
Axiomatic Deﬁnition of Probability
15

Deﬁnition 1.16
Probability Space
A probability space is the three-tuple {S,U,P}, where S is the sample space of
an experiment, U is the event space, and P is a probability set function having
domain U.
In any probabilistic analysis of an experiment, we will seek to establish
1. A universal set, S, that contains all of the potential outcomes or elementary
events of an experiment;
2. A set of sets, U, representing the collection of events or subsets of S on which
probability will be deﬁned; and
3. A probability set function, P, that can be used to assign the appropriate
probabilities to the events in S.
Once the probability space is deﬁned, all of the information is available that
is needed to assign probabilities to the various events of interest related to an
experiment is available. As one might suspect, it is the discovery of the appro-
priate probability set function that represents a major challenge in the applica-
tion of probability and statistics, and we examine the discovery problem in the
latter half of the text when we discuss topics in inferential statistics. Our
immediate goal in the remaining sections of this chapter is to establish a number
of useful results in probability theory that are implied by the probability axioms.
1.5
Some Probability Theorems
The three axioms governing the behavior of probability set functions, together
with results from set theory, can be used to prove probability theorems that
contribute to the development of probability theory. In stating and proving such
theorems, additional insights and truths are established about how probability
assignments must behave.
Theorem 1.1
P(A) ¼ 1  P(A).
Proof
By the deﬁnition of the complement of A, A [ A ¼ S. Thus, by substitution, and
by Axiom 1.2, P(S) ¼ 1 ¼ P(A [ A). However, since A \ A ¼ ;, Axiom 1.3 allows
us to state that 1 ¼ P(A) + P( A). Subtracting P( A) from both sides obtains the
result.
n
Theorem 1.2
P(;) ¼ 0.
Proof
Let A ¼ ; in Theorem 1.1. Then since
A ¼ S, it follows that P( ; ) ¼ 1 
P(S) ¼ 1  1 ¼ 0 since P(S) ¼ 1 by Axiom 1.2.
n
Theorem 1.3
If A  B, then P(A)  P(B) and P(B  A) ¼ P(B)  P(A).
16
Chapter 1
Elements of Probability Theory

Proof
Since A  B, B ¼ A [ (B  A). The sets A and B  A are disjoint and thus by
Axiom 1.3, P(B) ¼ P(A) + P(B  A). Since P(B  A)  0 by Axiom 1.1, dropping
P(B  A) from the probability equality implies P(B)  P(A). Subtracting P(A)
from both sides of the probability equality yields the second result of the
theorem.
n
Theorem 1.4
P(A) ¼ P(A \ B) + P(A \ B).
Proof
A ¼ A \ S ¼ A \ (B [ B) ¼ (A \ B) [ (A \ B) since the intersection operation is
distributive and S ¼ B [ B. Then because the events A \ B and A \ B are disjoint,
it follows by Axiom 1.3 that P(A) ¼ P(A \ B) + P(A \ B).
n
Theorem 1.5
P(A [ B) ¼ P(A) + P(B)  P(A \ B).
Proof
A [ B ¼ (A [ B) \ S ¼ (A [ B) \ (B [ B) ¼ B [ (A \ B) since the union operation is
distributive and S ¼ B [ B. Because events B and (A \ B) are disjoint, it follows by
Axiom 1.3 that P(A [ B) ¼ P(B) + P(A \ B). However, Theorem 1.4 implies that
P(A \ B) ¼ P(A)  P(A \ B), and thus by substitution, P(A [ B) ¼ P(A) + P(B) 
P(A \ B).
n
Corrolary 1.1
P(A [ B)  P(A) + P(B). (Boole’s Inequality)9
Proof
Note that the corollary follows directly from Theorem 1.5 since P(A \ B)  0.
Theorem 1.6
P(A) ∈[0, 1].
Proof
;  A implies P(;)  P(A) and A  S implies P(A)  P(S), by Theorem 1.3. Since
P(S) ¼ 1 by Axiom 1.2 and P(;) ¼ 0 by Theorem 1.2, we have 0  P(A)  1.
n
Theorem 1.7
P(A \ B)  1  P(A)  P(B). (Bonferroni’s Inequality-2 event case)10
Proof
By Theorem 1.1, P(A \ B) ¼ 1  PðA \ BÞ. DeMorgan’s law indicates that A \ B
¼ A [ B , and thus
P A \ B
ð
Þ ¼ 1  P A [ B


by substitution. Theorem 1.5
indicates that P( A [ B) ¼ P( A) þ P(B)  P( A \ B), and thus P(A \ B) ¼ 1  [P( A) þ
P(B)  P( A \ B)] ¼ 1  P( A)  P(B) þ P( A \ B), again by substitution. Finally,
since P( A \ B)  0 by Axiom 1.1, we have that P(A \ B)  1  P( A)  P(B).
n
Theorem 1.8
P \k
i¼1 Ai


 1  Pk
i¼1 P Ai


. (Bonferroni’s Inequality – General).
9Named after the English mathematician and logician George Boole.
10Named for the Italian mathematician, C.E. Bonferroni.
1.5
Some Probability Theorems
17

Proof
We have already proven the validity of the proposition when n ¼ 2 by Theorem
1.7. Suppose for purposes of invoking the induction principle (recall Lemma 1.1)
that we assume
P \k
i¼1Ai


 1 
Xk
i¼1 P Ai


is true. Using Theorem 1.7, we know that
P \kþ1
i¼1 Ai


¼P
\k
i¼1 Ai


\ Akþ1


 1  P \k
i¼1 Ai


 P Akþ1


:
Theorem 1.1 allows us to rewrite the inequality as
P \kþ1
i¼1 Ai


 P \k
i¼1 Ai


 P Akþ1


Then by the assumption that the Bonferroni inequality is valid for k events, we
may write
P \kþ1
i¼1 Ai


 1 
Xk
i¼1 P Ai


 PðAkþ1Þ
which implies that Bonferroni’s inequality is valid for k + 1 events.
n
Theorem 1.9
Classical Probability
Let S be the ﬁnite sample space for an experiment having N(S) equally likely
outcomes, and let A  S be an event containing N(A) elements. Then PðAÞ ¼
N(A)/N(S).
Proof
Let E1,. . .,En represent the n ¼ N(S) outcomes (or elementary events) in the
sample space S. Since all outcomes are equally likely, P Ei
ð
Þ ¼ k; 8i, and since
the outcomes are disjoint and S ¼ [n
i¼1 Ei


, we have
PðSÞ ¼
XNðSÞ
i¼1 PðEiÞ ¼
XNðSÞ
i¼1 k ¼ NðSÞk ¼ 1
by Axioms 1.2 and 1.3. It follows that P(Ei) ¼ k ¼ 1=NðSÞ for i ¼ 1,. . .,NðSÞ. Let
I  {1, 2,. . .,NðSÞ} be the index set identifying the N(A) number of outcomes (or
elementary events) that deﬁne the event A, i.e., A ¼ [i2I Ei. Then by Axiom 1.3,
PðAÞ ¼
X
i2I P Ei
ð
Þ ¼
X
i2I 1=NðSÞ ¼ NðAÞ
NðSÞ:
n
By proving Theorem 1.9, we have shown that the classical probability
deﬁnition is implied by the axiomatic deﬁnition of probability. Thus, whenever
the conditions of the classical probability deﬁnition apply, we are free to follow
the classical prescription for assigning probabilities to events. It can also be
shown that the relative frequency deﬁnition of probability is implied by the
axiomatic deﬁnition. Among other things, this implies that the axiomatic foun-
dation for probability theory provides the rationale for the existence of the limit
of relative frequencies referred to in the relative frequency deﬁnition of proba-
bility. We will need to develop results relating to asymptotic theory (Chapter 5)
before a proof of this proposition can be provided.
18
Chapter 1
Elements of Probability Theory

Finally, the subjective probability deﬁnition is implied by the axiomatic
deﬁnition in the sense that an individual assigning subjective probabilities to
events will be required to adhere to the axioms in making those assignments.
The requirement is interpreted by subjective probabilists as a consistency con-
dition for subjective probability assignments.
The example below illustrates the use of the preceding probability theorems
(for an example of Theorem 1.9, recall Example 1.3).
Example 1.8
Applying Probability
Theorems
Let S ¼ 2; 3; :::; 12
f
g be the sample space corresponding to the experiment of
rolling a pair of fair dice and observing the total number of dots facing up. We
will see in Chapter 2 that the probability set function appropriate for assigning
probabilities to events A in S is given by PðAÞ ¼ P
x2A
6 x7
j
j
36
.
Theorem 1.1 The event of winning a game of “craps” in Las Vegas on the ﬁrst roll
of the dice is the event A ¼ 7; 11
f
g. Using the probability set function deﬁned
above, we have that PðAÞ ¼ 8=36. It follows immediately from the theorem that
P A
 
¼ 1  PðAÞ ¼ 28=36.
Theorem
1.2
Deﬁne
the
event
of
rolling
an
even
number
of
dots,
B ¼ 2; 4; 6; 8; 10; 12
f
g. Examine the event of rolling an even number of dots and
winning the game of craps on the ﬁrst roll of the dice, A \ B ¼ ;. From the
theorem, P A \ B
ð
Þ ¼ P ;
ð Þ ¼ 0.
Theorem 1.3 Deﬁne the elementary event C ¼ 7
f g. Noting that C  A, we know
from the theorem that PðCÞ  PðAÞ, and in fact, PðCÞ ¼ 6=36<8=36 ¼ PðAÞ.
Theorem 1.4 Deﬁne the event of rolling a six or less, D ¼ 2; 3; 4; 5; 6
f
g. From the
theorem, we have that PðBÞ ¼ P B \ D
ð
Þ þ P B \ D


¼ 9/36 þ 9/36 ¼ 18/36.
Theorem 1.5 and Corollary 1.1 Note that A [ D ¼ 2; 3; 4; 5; 6; 7; 11
f
g , and a
calculation using the probability set function results in P A [ D
ð
Þ ¼ 23=36. The
theorem indicates that the probability can be represented as P(A [ D) ¼ P(A) þ
P(D)  P(A \ D) ¼ 8/36 þ 15/36  0 ¼ 23/36. The corollary indicates that the
probability adheres to P(A [ D)  P(A) þ P(D), which it clearly does as a weak
inequality.
Theorem 1.6 The theorem indicates that P(A)∈[0,1] for any event contained in S,
which is how probabilities assigned via the probability set function above will
behave. Note that all of the probability assignments above adhere to this
restriction.
Theorem 1.7 The theorem implies that P(B \ D)  1 – P(B)  P(D), which is true,
since 9/36  1  18/36  21/36.
Theorem 1.8 The theorem implies that P A \ B \ C


 1  PðAÞ  PðBÞ  PðCÞ,
which is true since 10=36  1  8=36  18=36  6=36.
□
1.6
Conditional Probability
When an experiment is conducted, the one event that is certain to occur is the
event S since the outcome of an experiment must be an element of the sample
space. We now study the effect that additional information concerning the
1.6
Conditional Probability
19

outcome of an experiment has on the probability of events. In particular, if it is
known that the outcome of the experiment is an element of some subset, B, of
the sample space, what is the effect of this additional information on the
probabilities of events in S? As an example of such a situation, it appears
intuitively plausible that the probability of a company earning $10 million in
annual proﬁts would be higher if the company were randomly chosen from the
list of Fortune 500 companies than if the company were chosen from among all
companies in the United States. For another example, examine the experiment
of tossing two fair coins in succession, and let the sample space for the experi-
ment be deﬁned by S ¼ {(H,H), (H,T), (T,H), (T,T)}, where H ¼ heads and T ¼
tails. The probability (unconditional) of observing two tails is 1/4 since, by the
Classical Probability theorem, P((T,T)) ¼ N(A)/N(S) ¼ 1/4, where A ¼ {(T,T)}.
However, the probability of observing two tails must be zero given that the
outcome of the ﬁrst coin toss was heads. We develop the notion of conditional
probability ahead.
Suppose that we are analyzing an experiment with an associated probability
space {S,U,P} and it is given that the outcome of the experiment is some element
of a subset, B, of the sample space. How should the probability of an event, A, be
deﬁned given the additional information that event B has occurred? By making a
number of observations concerning properties that conditional probabilities
should possess, we will be led to a deﬁnition of the conditional probability of
event A given event B.
First of all, since it is given that B occurs, it is certain that B will not occur. In
effect, the sample space has been reduced to the subset B, i.e., the outcomes in S-
B are no longer relevant and B can be interpreted as a new conditional sample
space. Letting the symbol P(A|B) represent the conditional probability of event
A, given event B, it follows that since the new sample space B is an event that is
now certain to occur, the conditional probability assigned to event B should be
1, so that P(B|B) ¼ 1. Note further that since B will occur, it is clear that an event
A can also occur iff A occurs concurrently with B, that is, iff A\B occurs (see
Figure 1.1). This suggests that conditional probability should be deﬁned so that
P(A|B) ¼ P(A \ B|B) for any event A.
B
S
A
A ∩ B
A ∩ B
Figure 1.1
Conditional sample
space B.
20
Chapter 1
Elements of Probability Theory

Now note that Theorem 1.4 implies the probability equation P(B) ¼ P(A \
B) + P(A\ B) since B can be partitioned into the two disjoint subsets, as B ¼ (A \
B) [ ( A \ B). Dividing both sides of the probability equality by P(B) (assuming
P(B) 6¼ 0) obtains a proportional decomposition of the probability of event B as
1 ¼ PðA\BÞ
PðBÞ þ
P A\B
ð
Þ
PðBÞ , where the proportion k ¼ P(A \ B)/P(B)∈[0,1] of event B’s
probability is attributable to event A \ B with the remaining proportion, (1  k),
attributable to event A \ B. Then since event A \ B accounts for a proportion, k,
of the probability that event B will occur, and since B is now certain to occur and
so is assigned (conditional) probability P(B|B) ¼ 1, then the proportion, k, of this
unit probability should be attributable to the event A \ B. We thus assign P(A \
B|B) ¼ k ¼ P(A \ B)/P(B), and since we also require that P(A|B) ¼ P(A \ B|B), we
are led to the following deﬁnition of conditional probability.
Deﬁnition 1.17
Conditional Probability
Let A and B be any two events in a sample space S. If P(B) 6¼ 0, then the condi-
tional probability of event A, given event B, is given by P(A|B) ¼ P(A \ B)/P(B).
Our intuition concerning the meaning of conditional probability can be
enhanced by examining the deﬁnition in light of the classical and relative
frequency deﬁnitions of probability. In an experiment for which classical proba-
bility is applicable, the probability space consists of a ﬁnite sample space, an
event space that consists of all subsets of the sample space, and a probability set
function that assigns probability to an event A  S as P(A) ¼ N(A)/N(S).
Conditioning on an event B  S, the probability of an event A, by the
deﬁnition of conditional probability, is given by
PðAjBÞ ¼ PðA \ BÞ
PðBÞ
¼ NðA \ BÞ=NðSÞ
NðBÞ=NðSÞ
¼ NðA \ BÞ
NðBÞ
:
Since all outcomes are equally likely in this case, and since we are effectively
restricting the sample space to the set B by conditioning on event B, it stands to
reason that the probability of observing A is given by the number of outcomes in
B that result in A’s occurring, i.e., NðA \ BÞ, relative to the total number of
outcomes in B. This is, of course, consistent with the classical probability
deﬁnition applied to the event A in the context of the new sample space, B.
Regarding the relative frequency deﬁnition and conditional probability,
recall that the probability set function assigns probabilities to events via PðAÞ
¼ limn!1 nA=n
ð
Þ. Conditioning on event B  S, the probability of event A, by the
deﬁnition of conditional probability, is given by
11See R.C. Buck (1978) Advanced Calculus, 3rd edition, McGraw-Hill, p. 44. We will discuss the concept of limits in more detail in
Chapter 5. For now, a more intuitive understanding of limits is sufﬁcient.
1.6
Conditional Probability
21

PðAjBÞ ¼ PðA \ BÞ
PðBÞ
¼
lim
n!1 ðnA\B =nÞ
lim
n!1 ðnB =nÞ ¼ lim
n!1 ðnA\B = nBÞ
Note the last equality follows from the fact that the limit of a ratio equals the
ratio of the limits if all limits exist, and if the limit in the denominator of the
ratio of limits is not zero.11 Restricting the sample space to the set B by condi-
tioning on event B, P(A|B) is seen to equal the limiting fraction of the number of
occurrences of event B that also result in the occurrence of A. Consistent with
the logic of the relative frequency deﬁnition of probability, P(A|B) could then be
interpreted as the limit of the frequency of observing event A relative to the total
number of outcomes generated from the conditional sample space B.
The conditional probability set function P(|B) can be used to deﬁne two new
probability spaces {S, U, P(|B)} and {B,UB, P(|B)}, where UB is the collection of all
events contained in the set B. It follows that conditional probabilities can be
legitimately assigned by P(|B) to all of the events in the original sample space S
as well as to all of the events in the conditional sample space B. One can
formally demonstrate that P(|B) adheres to the three probability axioms and
that conditional probabilities are being generated from a legitimate probability
set function. It is suggested that the reader pursue such a demonstration.
Since it can be shown that P(A|B) adheres to the probability axioms, all of the
theorems that were proved for unconditional probabilities apply equally well to
conditional probabilities.12 This follows because the validity of the theorems is
derived from the probability axioms regardless of whether the set function is
representing unconditional or conditional probabilities. Put another way, the
proofs of all of the theorems would apply analogously to conditional
probabilities by simply changing P() to P(|D), say, in the proofs and recognizing
that the probability axioms apply to the set function P(|D). Note that we use the
letter D here to allow for the possibility that the event being conditioned on is
different than the events, A and B, that are referred to in some of the previous
probability theorems. For convenience, we list the probability theorems below
as they apply to conditional probabilities. It is assumed that the conditional
probability is deﬁned, i.e., P(D) 6¼ 0, and that A and/or B are events in either the
conditional sample space D or the original sample space S.
Theorem 1.1c P(A|D) ¼ 1  P(A|D).
Theorem 1.2c P(;|D) ¼ 0.
Theorem 1.3c If A  B, then P(A|D)  P(B|D) and P(B  A|D) ¼ P(B|D)  P(A|D).
Theorem 1.4c P(A|D) ¼ P(A \ B|D) + P(A \ B|D).
Theorem 1.5c P(A [ B|D) ¼ P(A|D) + P(B|D)  P(A \ B|D).
Corollary 1.1c P(A [ B|D)  P(A|D) + P(B|D).
Theorem 1.6c P(A|D)∈[0,1].
Theorem 1.7c P(A \ B|D)  1  P(A|D)  P(B|D).
12Note that, in a sense, all probabilities could be viewed as conditional, where P(A|S) could be used to denote probabilities in previous
sections. We will continue to use “unconditional” to refer to the case where the original sample space, S, has been left
“unconditioned.”
22
Chapter 1
Elements of Probability Theory

Theorem 1.8c P \k
i¼1 Ai jD


 1  Pk
i¼1 P AijD


:
The following examples illustrate the application of conditional probability.
Example 1.9
Conditional
Probabilities in Coin
Tossing
Consider the experiments of tossing two coins in succession, and let S ¼ {(H,H),
(H,T), (T,H), (T,T)}, where H ¼ head, T ¼ tail. Assume all outcomes are equally
likely.
(a) What is the probability of obtaining two heads, given the ﬁrst coin toss was
heads?
Answer: B ¼ {(H,H), (H,T)} is the event that the ﬁrst coin toss results in
heads, which is our conditional sample space. A ¼ {(H,H)} is the event of
obtaining two heads. Then P(A|B) ¼ P(A \ B)/P(B) ¼ (1/4)/(1/2) ¼ 1/2.
(b) What is the probability of obtaining two heads, given that at least one of the
coins comes up heads?
Answer: C ¼ {(H,H), (H,T), (T,H)} is the event that at least one of the
coins came up heads, which is our conditional sample space. Then
P(A|C) ¼ P(A \ C)/P(C) ¼ (1/4)/(3/4) ¼ 1/3.
(c) What is the probability of obtaining one head and one tail given that the ﬁrst
coin is a tail?
Answer: D ¼ {(T,H), (T,T)} is the event that the ﬁrst coin was a tail. E ¼
{(T,H), (H,T)} is the event of obtaining one head and one tail. Then
P(E|D) ¼ P(E \ D)/P(D) ¼ (1/4)/(1/2) ¼ 1/2.
□
Example 1.10
Conditional
Probabilities in Sample
Selection
A perplexed investor must choose an investment instrument from among 15
different stocks, 10 different bonds, and 5 different mutual funds. Allowing each
instrument an equal probability of being chosen, the investor randomly chooses
an instrument. Given that the chosen instrument was not a bond, what is the
probability that a stock was chosen?
Answer: We are conditioning on the event, B, that the instrument is either a
stock or a mutual fund (i.e., not a bond), and P(B) ¼ 2/3. Let A represent the
event that the outcome is a stock, so that P(A \ B) ¼ 1/2. Then
P AjB
ð
Þ ¼ PðA \ BÞ=PðBÞ ¼ ð1=2Þ=ð2=3Þ ¼ 3=4
is the probability we seek. Note this makes sense from the standpoint that the
conditional sample space is 20 investment instruments, of which 15 are stocks
and 5 are mutual funds. The classical probability deﬁnition suggests that the
probability of observing a stock in this sample space of 20 instruments is
15/20 ¼ 3/4.
□
The deﬁnition of conditional probability can be transformed to obtain a
result known as the multiplication rule. The multiplication rule allows one to
calculate the probability of the event A \ B from knowledge of the conditional
probability of event A, given event B, and the unconditional probability of B.
1.6
Conditional Probability
23

Theorem 1.10
Multiplication Rule:
Two Events
Let A and B be any two events in the sample space for which P(B) 6¼ 0. Then
P(A \ B) ¼ P(A|B) P(B).
Proof
Multiply both sides of P(A|B) ¼ P(A \ B)/P(B) in Deﬁnition 1.7 by P(B).
n
The multiplication rule is especially useful in cases where an experiment
can be viewed as being conducted in two stages.
Example 1.11
Multiplication Rule for
Two Card Draw
What is the probability of drawing two aces in succession from a well-shufﬂed
deck of poker cards? Assume cards drawn are not replaced in the deck.
Answer: Let B be the event that the ﬁrst card drawn is an ace. Since there are four
aces in a poker deck, with a total of 52 cards in the deck, P(B) ¼ 4/52 ¼ 1/13.
Now let A be the event that the second card drawn is an ace. Given that the ﬁrst
card drawn is an ace (i.e., given the event B), there are three aces remaining to be
chosen from the remaining 51 cards, and thus the probability that the second
draw is an ace, given that the ﬁrst card drawn is an ace, i.e., P(A|B), equals
3/51 ¼ 1/17. Then by the multiplication rule, the probability that both draws
result in aces is given by
PðA \ BÞ ¼ PðAjBÞPðBÞ ¼ ð1=17Þð1=13Þ ¼ 1=221:
□
Example 1.12
Multiplication Rule in
One-Stage Inspections
As part of its quality control program, an apparel manufacturer has inspectors
examine every garment the company produces. A garment is shipped to a retail
outlet only if it passes inspection. The probability that a garment is defective
is .02. The probability that an inspector assigns a “pass” to a defective garment is
.05. What is the probability that a garment is defective and shipped to a retail
outlet?
Answer: Let D be the event that a garment is defective. Let B be the event that
the inspector assigns a “pass” to a garment. We know that P(D) ¼ .02 and P(Bj
D) ¼ .05. B \ D is the event that a garment is defective and is passed by the
inspector. Then P(B \ D) ¼ P(BjD)P(D) ¼ (.05)(.02) ¼ .001.
□
The multiplication rule can be extended to three or more events, in which
case the probability of the intersection of all of the events can be represented as
follows.
Theorem 1.11
Multiplication Rule:
General
Let A1, A2,. . .,An, n  2 be events in the sample space. Then if all of the
conditional probabilities exist, P \n
i¼1 Ai


¼ PðA1Þ Qn
i¼2 P Ai j \i1
j¼1 Aj


.
Proof
We know from Theorem 1.10 that the result holds for n ¼ 2 (note \1
j¼1 Aj ¼ A1
by deﬁnition). In an attempt to invoke the mathematical induction principle
assume that the result is true for n ¼ k, where k is some arbitrary positive
integer 3. Let B ¼ \k
i¼1Ai. Then
24
Chapter 1
Elements of Probability Theory

P \kþ1
i¼1 Ai


¼ PðAkþ1 \BÞ ¼ PðAkþ1 jBÞPðBÞðTheorem 1:11Þ
¼ PðA1Þ P
k
i¼2 P Ai j \i1
j¼1 Aj


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
PðBÞ
PðAkþ1 jBÞ assuming result holds for n ¼ k
ð
Þ
¼ PðA1Þ P
kþ1
i¼2 P Ai j \i1
j¼1 Aj


substitution for B
ð
Þ
Thus, by mathematical induction, the theorem holds. (See Deﬁnition 1.12)
n
Similar to the case of the multiplication rule for two events, the extended
multiplication rule is especially useful in cases where an experiment can be
viewed as being conducted in n stages.
Example 1.13
Multiplication Rule for
Three Card Draw
What is the probability of drawing four aces in succession from a well-shufﬂed
deck of poker cards? Assume cards drawn are not replaced in the deck.
Answer: Let Ai be the event that the ith card drawn is an ace, i ¼ 1, 2, 3, 4. Then
using Theorem 1.12,
P \4
i¼1 Ai


¼ PðA1ÞPðA2 j A1ÞPðA3 j A1 \ A2ÞPðA4 j A1 \ A2 \ A3Þ
¼
4
52


3
51


2
50


1
49


¼ :3693  105:
□
Example 1.14
Multiplication Rule in
Two-Stage Inspections
Recall the garment inspection problem of Example 1.12. Suppose the retailers
who market the garments of the apparel manufacturer also inspect each garment
they purchase and place on sale only those for which they perceive no defects.
The probability that a retailer places a defective garment on sale is .10. What is
the probability that a garment is defective, shipped to the retail outlet, and
placed on sale by retailers?
Answer: Let A be the event that the retailer places a garment on sale. A \ B \ D is
the event of interest, and by Theorem 1.11,
PðA \ B \ DÞ ¼ PðDÞPðBjDÞPðAjB \ DÞ ¼ ð:02Þð:05Þð:10Þ ¼ :0001
□
1.7
Independence
In everyday language, if one were to say that two events are independent, it is
generally meant that the occurrence of one event does not affect the likelihood
of an occurrence of the other, and vice-versa. This meaning of independence can
1.7
Independence
25

be formalized within the theory of probability. We begin with the technical
deﬁnition of independent events.
Deﬁnition 1.18
Independence of Events:
Two Event Case
Let A and B be two events in a sample space S. Then A and B are independent
events iff P(A \ B) ¼ P(A)P(B). If A and B are not independent, A and B are said
to be dependent events.
An interpretation of the independence condition in Deﬁnition 1.18 that is
closely aligned with our layman’s interpretation of the word independence is
available when P(A) > 0 and P(B) > 0. In this case, P(A \ B) ¼ P(A)P(B) implies
PðAjBÞ ¼PðA \ BÞ=PðBÞ ¼PðAÞPðBÞ=PðBÞ ¼PðAÞ;
and
PðBjAÞ ¼PðB \ AÞ=PðAÞ ¼PðBÞPðAÞ=PðAÞ ¼PðBÞ:
Thus the probability of event A occurring is unaffected by the occurrence of
event B, and the probability of event B occurring is unaffected by the occurrence
of event A.
If event A and/or event B has probability zero, then by deﬁnition, events A
and B are independent. This follows immediately from the fact that if either
P(A) ¼ 0 or P(B) ¼ 0, then P(A \ B) ¼ 0 ¼ P(A)P(B) (since (A \ B)  A and (A \ B)
 B imply both P(A \ B)  P(A) and P(A \ B)  P(B) by Theorem 1.3, and P(A \
B)  0 by Axiom 1.1, so that together the inequalities imply P(A \ B) ¼ 0), and
thus the independence condition is fulﬁlled. However, in this case one or both
conditional probabilities P(A|B) and P(B|A) are undeﬁned, and thus the basis no
longer exists for stating that “the independence of events A and B implies the
probability of either event is unaffected by the occurrence of the other.”
If A and B are independent, then it follows that A and B, A and B, and A and B
are also independent. This result can be demonstrated by showing that the
independence deﬁnition is satisﬁed for each of the preceding pairs of events if
independence is satisﬁed for A and B. We state the result as a theorem.
Theorem 1.12
Independence of
Complement
Event Pairs
If events A and B are independent, then eventsA and B, A and B, and Aand Bare
also independent.
PðA \ BÞ ¼ PðAÞ  PðA \ BÞðTheorem 1:4Þ
¼ PðAÞ  PðAÞPðBÞðIndependence of A and BÞ
¼ PðAÞ½1  PðBÞðAlgebraÞ
¼ PðAÞPðBÞðTheorem 1:1Þ
26
Chapter 1
Elements of Probability Theory

PðA \ BÞ ¼ PðBÞ  PðA \ BÞðTheorem 1:4Þ
¼ PðBÞ  PðAÞPðBÞðIndependence of A and BÞ
¼ PðBÞ½1  PðAÞðAlgebraÞ
¼ PðAÞPðBÞðTheorem 1:1Þ
PðA \ BÞ ¼ PðA [ BÞ ðDeMorgan’s lawsÞ
¼ 1  PðA [ BÞ ðTheorem 1:1Þ
¼ 1  ðPðAÞ þ PðBÞ  PðA \ BÞÞðTheorem 1:5Þ
¼ 1  PðAÞ  PðBÞ þ PðAÞPðBÞðIndependence of A and BÞ
¼ PðAÞ  PðBÞ½1  PðAÞðTheorem 1:1 and AlgebraÞ
¼ PðAÞ½1  PðBÞðAlgebra and Theorem 1.1Þ
¼ PðAÞPðBÞðTheorem 1:1Þ
n
The following example illustrates the concept of independence of events.
Example 1.15
Independence of
Gender and Occupation
The work force of the Excelsior Corporation has the following distribution
among type and gender of workers:
Type of worker
Sex
Sales
Clerical
Production
Total
Male
825
675
750
2,250
Female
1,675
825
250
2,750
Total
2,500
1,500
1,000
5,000
In order to promote loyalty to the company, the company randomly chooses
a worker to receive an all-expenses paid vacation each month. Is the event of
choosing a female independent of the event of choosing a clerical worker?
Answer: Let F ¼ event of choosing a female and C ¼ event of choosing a clerical
worker. From the data in the table, we know that P(F) ¼ .55 and P(C) ¼ .30. Also,
P(F \ C) ¼ .165. Then P(F \ C) ¼ P(F)P(C), and the events are independent.
Is the event of choosing a female independent of the event of choosing a produc-
tion worker?
Answer: Let A ¼ event of choosing a production worker. Then P(A) ¼ .20 and
P(A \ F) ¼ .05, and thus P(A \ F) ¼ .05
6¼ .11 ¼ P(A)P(F), so the events are
dependent.
□
The property that A and B are independent events is sometimes confused
with the property that sets A and B are disjoint. It should be noted that the two
properties are distinct, but related concepts. The disjointness property is a
property of events, while the independence property is a property of the
1.7
Independence
27

probability set function deﬁned on the events. Table 1.1 presents the relation-
ship between the two properties.
We now verify the three cases in which an immediate conclusion can be
reached regarding the independence of events A and B.
Theorem 1.13
Independence and
Disjointness
1. P(A) > 0, P(B) > 0, A \ B ¼ ; ¼) A and B are dependent.
2. P(A) and/or P(B) ¼ 0, A \ B ¼ ; ¼) A and B are independent.
3. P(A) and/or P(B) ¼ 0, A \ B 6¼ ; ¼) A and B are independent.
Proof
1. P(A \ B) ¼ P(;) ¼ 0 by Theorem 1.2. Since P(A \ B) ¼ 0 < P(A)P(B) because
P(A) and P(B) are both positive, A and B cannot be independent events, and so
they are dependent.
2. P(A \ B) ¼ P(;) ¼ 0 by Theorem 1.2. Since P(A) and/or P(B) ¼ 0, then P(A)
P(B) ¼ 0, and thus P(A \ B) ¼ 0 ¼ P(A)P(B). Therefore, A and B are
independent.
3. P(A \ B)  P(A) and P(A \ B)  P(B) by Theorem 1.3, since (A \ B)  A and
(A \ B)  B. If P(A) and/or P(B) ¼ 0, then P(A \ B)  0. P(A \ B)  0 by Axiom
1.1. Then P(A \ B) ¼ 0 ¼ P(A)P(B), and A and B are independent events.
n
The concept of independent events can be generalized to more than two
events.
Deﬁnition 1.19
Independence of Events:
n Event Case
Let A1, A2,. . .,An be events in the sample space S. The events A1, A2,. . .,An are
independent iff P \j2J Aj


¼ Q
j2J PðAjÞ for all subsets J  {1, 2,. . .,n} for which
N(J)  2. If the events A1, A2,. . .,An are not independent, they are said to be
dependent events.
Note that the independence concept deﬁned in Deﬁnition 1.19 is sometimes
referred to as the joint, mutual, or complete independence of the events A1,
A2,. . .,An when n  3 to emphasize that additional conditions are required
beyond the condition given in Deﬁnition 1.18 applicable to pairs of events. We
will refrain from using these additional adjectives and simply refer to the inde-
pendence of events, regardless of n. Furthermore, note that if the condition P(Ai
\Aj) ¼ P(Ai)P(Aj) of Deﬁnition 1.18 applies to all pairs of events in A1, A2,. . .,An,
the events are referred to as being pairwise independent whether or not they are
independent in the sense of Deﬁnition 1.19.
Table 1.1
Disjointness Versus Independence
A \ B ¼;
A \ B 6¼;
P(A) > 0 and P(B) > 0
Dependent
Independent iff P(A \ B) ¼ P(A)P(B)
P(A) ¼ 0 and/or P(B) ¼ 0
Independent
Independent
28
Chapter 1
Elements of Probability Theory

In the case of three events, independence requires that P(A1 \A2) ¼ P(A1)
P(A2), P(A1 \ A3) ¼ P(A1)P(A3),P(A2 \ A3) ¼ P(A2)P(A3), and P(A1 \ A2 \ A3) ¼
P(A1)P(A2)P(A3). Note that if all three events have nonzero probability, then the
reader can straightforwardly verify from the deﬁnition of conditional probability
that independence implies P(Ai|Aj) ¼ P(Ai) 8i 6¼ j, P(Ai|Aj \ Ak) ¼ P(Ai)8i 6¼ j 6¼ k,
and P(Ai \ Aj |Ak) ¼ P(Ai \ Aj) 8 i 6¼ j 6¼ k. It is not as straightforward to
demonstrate that independence implies P(Ai|Aj [ Ak) ¼ P(Ai) for i 6¼ j 6¼ k, and
so we demonstrate the result below.
PðAi j Aj [ AkÞ ¼ P Ai \ðAj [ AkÞ


PðAj [ AkÞ
ðby definitionÞ
¼ P ðAi \ AjÞ [ ðAi \ AkÞ


PðAj [ AkÞ
ðdistributive lawÞ
¼ PðAi \ AjÞ þ PðAi \ AkÞ  PðAi \ Aj \ AkÞ
PðAj [ AkÞ
ðTheorem 1:5Þ
¼ PðAiÞPðAjÞ þ PðAiÞPðAkÞ  PðAiÞPðAjÞPðAkÞ
PðAj [ AkÞ
ðindependence)
¼ PðAiÞ PðAjÞ þ PðAkÞ  PðAjÞPðAkÞ
	

PðAj [ AkÞ
ðalgebraÞ
¼ PðAiÞPðAj [ AkÞ
PðAj [ AkÞ
ðTheorem 1.5 and independenceÞ
¼ PðAiÞ
It is thus recognized that if events A1, A2, and A3 are independent in the sense of
Deﬁnition 1.19, and if each of the events occurs with nonzero probability, then
the probability of any one of the events is unaffected by the occurrence of any
of the remaining events and is also unaffected by the occurrence of the union or
intersection of the remaining events. This interpretation extends in a straight-
forward way to cases involving four or more independent events, and the reader
should attempt some of the possible extensions.
The reader may wonder whether the numerous conditions (for n events, the
number of conditions will be 2n  n  1) cited in Deﬁnition 1.19 for indepen-
dence
of
events
are
all
necessary,
i.e.,
wouldn’t
the
one
condition
P \n
j¼1Aj


¼ Qn
j¼1 P Aj


sufﬁce and imply all of the others? Unfortunately, the
answer is no – all of the conditions are required. The following example
illustrates the point.
Example 1.16
Need for Pairwise
Independence
Conditions
The Venn diagram in Figure 1.2 summarizes the probabilities assigned to some
of the events in the sample space S. Note that P(A \ B \ C) ¼ .15 ¼ (.5)(.6)(.5) ¼
P(A)P(B)P(C). However, P(A \ B) ¼ .25 6¼ .3 ¼ P(A)P(B), P(A \ C) ¼ .20 6¼ .25 ¼
P(A)P(C), P(B \ C) ¼ .40 6¼ .3 ¼ P(B)P(C), and thus P(A \ B \ C) ¼ P(A)P(B)P(C)
does not imply the pairwise independence conditions.
□
1.7
Independence
29

The reader should construct an example illustrating that pairwise indepen-
dence among A, B, and C does not imply P(A \ B \ C) ¼ P(A)P(B)P(C).
1.8
Bayes’ Rule
Bayes’ rule, discovered by the 17th century English clergyman and mathemati-
cian, Thomas Bayes, provides an alternative representation of conditional
probabilities. At ﬁrst glance, the representation may appear somewhat convo-
luted in comparison to the representation of conditional probability given in
Deﬁnition 1.17. However, the rule is well suited for providing conditional
probabilities in certain experimental situations, which we will identify follow-
ing the formal derivation of the rule.
Bayes’ rule is actually a simple consequence of a result known as the
Theorem of Total Probability, which we state and prove next.
Theorem 1.14
Theorem of Total
Probability
Let the events Bi, i∈I, be a ﬁnite or countably inﬁnite partition of the sample
space, S, so that Bj \ Bk ¼ ; for j 6¼ k, and [i2I Bi ¼ S. Let P(Bi) > 08i. Then
P(A) ¼ Si2IPðA \ BiÞ ¼ Si2IP(A|Bi)P(Bi).
Proof
Since [i2IBi ¼ S, it follows that A ¼ A \ ([i2IBi) ¼ [i2I(A \ Bi), where we have
used the fact that the intersection operation is distributive over the union
operation. Now note that (A \ Bj) \ (A \ Bk) ¼ ; for j 6¼ k since the Bi’s are
disjoint. But then by Axiom (1.3), PðAÞ ¼ P [i2I A \ Bi
ð
Þ
ð
Þ ¼ Si2IPðA \ BiÞ: The
result of the theorem follows from applying the multiplication rule to each term,
P(A \ Bi), in the sum.
n
Regarding the title of Theorem 1.14, note that the “total probability” of
event A is represented as the sum of the portions of A’s probability distributed
over the events in the partition of A represented by the events A \ Bi, i∈I. We
now state Bayes’ rule.
A
B
S
C
.20
.10
.10
.15
.05
.25
.05
.10
Figure 1.2
Probability assignments in
S.
30
Chapter 1
Elements of Probability Theory

Theorem 1.15
Bayes’ Rule
Let events Bi, i∈I, be a ﬁnite or countably inﬁnite partition of the sample space,
S, so that Bj \ Bk ¼ ; for j 6¼ k and [i2I Bi ¼ S. Let P(Bi) > 0 8i∈I. Then, provided
P(A) 6¼ 0,
PðBjjAÞ ¼
PðAjBjÞPðBjÞ
P
i2I PðAjBiÞPðBiÞ ; 8j 2 I:
Proof
From the multiplication rule, P A \ Bj


¼ PðAjBjÞPðBjÞ, and from the Theorem
of Total Probability, PðAÞ ¼ P
i2I PðAjBiÞPðBiÞ . Substituting respectively into
the numerator and denominator of the representation of PðBjjAÞabove yields
P A \ Bj


=PðAÞ, the conditional probability of Bj given A.
n
Corollory 1.2 Bayes’
Rule: Two Event Case
PðBjAÞ ¼
PðAjBÞPðBÞ
PðAjBÞPðBÞ þ PðAjBÞPðBÞ
Proof
This is a direct consequence of Theorem 1.15 when I ¼ {1, 2}.
n
In the next two examples we provide illustrations of the types of experimen-
tal situations for which Bayes’ rule have particularly useful application.
Example 1.17
Bayes Rule in Oil
Discovery
Explorations, Inc. is in the oil well-drilling business. Let B be the event that a
well being drilled will produce oil and let A be an event representing geological
well-site characteristics that are conducive to discovering oil at a site. Suppose
further that, from past experience, it is known that the unconditional probabil-
ity of a successful strike when drilling for oil is .06. Also suppose it is known
that when oil is discovered, the probability is .85 that the geological
characteristics are given by event A, whereas the probability is only .4 that
geological characteristics represented by A are present when no oil is discovered.
If event A occurs at a site, what is the probability of discovering oil at the site,
i.e., what is P(B|A)?
Answer: It is known that P(B) ¼ .06, P(B) ¼ .94, P(A|B) ¼ .85, and P(A|B) ¼ .40.
Bayes’s rule applies here, so that
PðBjAÞ ¼
PðAjBÞPðBÞ
PðAjBÞPðBÞ þ PðAjBÞPðBÞ ¼
ð:85Þð:06Þ
ð:85Þð:06Þ þ ð:40Þð:94Þ ¼ :12
Note that the occurrence of event A increases considerably the probability of
discovering oil at a particular site.
□
Example 1.18
Bayes Rule in Assessing
Blood Test Accuracy
A blood test developed by a pharmaceutical company for detecting a certain
disease is 98 percent effective in detecting the disease given that the disease is, in
fact, present in the individual being tested. The test yields a “false positive”
result (meaning a person without the disease is incorrectly indicated as having
the disease) for only 1 percent of the disease-free persons tested.
1.8
Bayes’ Rule
31

If an individual is randomly chosen from the population and tested for the
disease, and given that .1 percent of the population actually has the disease,
what is the probability that the person tested actually has the disease if the test
result is positive (i.e., the disease is indicated as being present by the test)?
Answer: In this case, let A be the event that the test result is positive, and let B
be the event that the individual actually has the disease. Then, from the preced-
ing discussion concerning the characteristics of the test, it is known that
P(A|B) ¼ .98, P(B) ¼ .001, and P(A|B) ¼ .01. Then, an application of Bayes’ rule
yields
PðBjAÞ ¼
PðAjBÞPðBÞ
PðAjBÞPðBÞ þ PðAjBÞPðBÞ ¼
ð:98Þð:001Þ
ð:98Þð:001Þ þ ð:01Þð:999Þ ¼ :089
Thus, one has very little conﬁdence that a positive test result implies that the
disease is present.
□
A common thread in the two examples, consistent with the statement of
Bayes’s rule itself, is that the sample space is partitioned into a collection of
disjoint events (Bi,i∈I), that are of interest and whose nonzero probabilities are
known. Furthermore, an event occurs whose various conditional probabilities
formed by conditioning on each of the events in the partition are known. Given
this background information, Bayes’ rule provides the means for reevaluating
the probabilities of the various events in the partition of S, given the information
that event A occurs. The probabilities of the events in the partition are, in effect,
“updated” in light of the new information provided by the occurrence of A. This
interpretation of Bayes’s rule has led to the use of the terms prior probabilities
and posterior probabilities being used to refer to the P(Bi)’s and P(Bi|A)’s, respec-
tively. That is, P(Bi) is the probability of event Bi in the partition of S prior to the
occurrence of event A, whereas P(Bi|A) is the probability of Bi posterior to, or
after, event A occurs.
Another prominent interpretation of Bayes’ rule is that of being a tool for
inverting conditional probabilities. In particular, note that Theorem 1.15 begins
with information relating to the conditional probabilities PðAjBjÞ 8jand then
inverts them into the conditional probabilities PðBjjAÞ 8j. Given this interpreta-
tion, some have referred to Bayes Rule alternatively as The Law of Inverse
Probabilities.
Returning to the oil well-drilling example, note that each elementary event
in the (implied) sample space is given by a pair of observations, one being
whether or not the geological characteristics of the well-site favor the discovery
of oil, and the other being whether or not oil is actually discovered at the well
site. The partition of the sample space that is of interest to the oil well-drilling
company is the event “oil is discovered” versus the complementary event that
“oil is not discovered” at the well site. The additional information used to
update the prior probabilities concerning oil discovery is whether the geological
characteristics of the site favor the discovery of oil. Bayes’ rule can be applied to
generate posterior probabilities of oil discovery because the conditional
probabilities of favorable well-site characteristics being observed, with and
32
Chapter 1
Elements of Probability Theory

without the condition of oil being discovered, are known. Also, Bayes’ rule is
inverting the conditional probabilities that geological characteristics conducive
to oil production are present when oil is discovered and when it is not, to
conditional probabilities that oil is discovered or not, given the presence of
geological characteristics conducive to oil discovery.
The reader can provide a characterization of the sample space, partition of
interest, and additional information used to update and invert probabilities in
the case of the drug test example.
1.9
Appendix: Digression on Events that Cannot Be Assigned Probability
When we ﬁrst began our discussion of the axiomatic approach to the deﬁnition
of probability, we stated that probability is generated by a set function whose
domain consisted of all of the events in a sample space. This collection of “all of
the events in a sample space” was termed the event space. However, in uncount-
able sample spaces it can be the case that there are some very complicated events
(subsets) of the sample space to which probability cannot be assigned. The issue
here is whether a set function can have a domain that literally consists of all of
the subsets of a sample space and still adhere to the three axioms of probability.
In the case of a countable sample space, the domain can consist of all of the
subsets of the sample space and still have the set function exhibiting the
properties required by the probability axioms. Henceforth, whenever we are
dealing with a countable sample space, the domain will always be deﬁned as
the collection of all of the subsets of the sample space, unless explicitly stated
otherwise.
The situation is more complicated in the case of an uncountably inﬁnite
sample space. In this case, the collection of all subsets of S is, in a sense, so large
that a set function cannot have this collection of sets for its domain and still
have the probability axioms hold true for all possible applications. The problem
is addressed in a ﬁeld of mathematics called measure theory and is beyond the
scope of our study. As a practical matter, essentially any subset of S that will be
of practical interest as an event in real-world applications will be in the domain
of the probability set function. Put another way, the subsets of S that are not in
the domain are by deﬁnition so complicated that they will not be of interest as
events in any real-world application.
While it takes a great deal of ingenuity to deﬁne a subset of an uncountably
inﬁnite sample space that is not an event, the reader may still desire a more
precise and technically correct deﬁnition of the domain of the probability set
function in the case of an uncountably inﬁnite sample space so that one is
certain to be referring to a collection of subsets of S for which each subset can
be assigned a probability. This can be done relatively straightforwardly so long as
we restrict our attention to real-valued sample spaces (i.e., sample spaces whose
sample points are all real numbers). Since we can always “code” the elements of
a sample space with real numbers (this relates to the notion of random variables,
which we address in Chapter 2), restricting attention to real-valued sample
1.9
Appendix: Digression on Events that Cannot Be Assigned Probability
33

spaces does not involve any loss of generality, and so we proceed on the assump-
tion that S  ℝn. Our characterization depends on the notion of Borel sets,
named after the French mathematician Emile Borel, which we deﬁne next.
The deﬁnition uses the concept of rectangles in ℝn, which are generalizations
of intervals.
Deﬁnition A.1
Rectangles in ℝn
a. Closed rectangle: {(x1,. . .,xn): ai  xi  bi, i ¼ 1,. . .,n}
b. Open rectangle: {(x1,. . .,xn): ai < xi < bi, i ¼ 1,. . .,n}
c. Half-open/Half-closed rectangle:
x1; :::; xn
ð
Þ : ai<xi  bi; i ¼ 1; :::; n
f
g
x1; :::; xn
ð
Þ : ai  xi<bi; i ¼ 1; :::; n
f
g
where the ai’s and bi’s are real numbers, with 1 or 1 being admissible for
strong inequalities. Clearly, rectangles are intervals when n ¼ 1.
The collection of Borel sets contained in a sample space S will include all of
the rectangle subsets of S as well as an inﬁnite number of other sets that can be
formed from them via set operations as deﬁned below.
Deﬁnition A.2
Borel Sets in S
Let S  ℝn. The collection of Borel sets in S consists of all closed, open, and
half-open/half-closed rectangles contained in S, as well as any other set that
can be deﬁned by applying a countable number of union, intersection, and/or
complement operations to these rectangles.
The collection of Borel sets in S is an example of what is known as a sigma-
ﬁeld (s-ﬁeld), or a sigma-algebra (s-algebra). A s-ﬁeld is a nonempty set of sets
that is closed under countable union, intersection, and complement operations.
The use of the word “closed” here means that if Ai, i∈I, all belong to the s-ﬁeld,
any set formed by applying a countable number of unions, intersections, and/or
complement operations to the Ai’s is also a set that belongs to the s-ﬁeld, where
I is any countable index set.
The collection of Borel sets is extremely large and will contain any subset of
the real-valued sample space that will be of practical interest as events in real-
world applications. In particular, all open and all closed (rectangular or
nonrectangular) sets are contained in the collection of Borel sets. Most impor-
tantly, probabilities can always be assigned to Borel sets. Consequently, we will
tacitly assume that the collection of Borel sets is our domain for probability set
functions associated with real-valued sample spaces. However, we will continue
to refer to the event space as the domain of a probability set function, and act as if
the domain consisted of all subsets of the sample space, with little worry that we
will ever encounter a subset of S in practice that cannot be assigned probability.
34
Chapter 1
Elements of Probability Theory

Keywords, Phrases, and Symbols
Axiomatic approach
Bayes’ Rule
Bonferroni’s inequality
Borel sets
, contained in
Certain event, S
Classical probability
Conditional probability of event A,
given event B
Conditional sample space
Continuous sample space
Discrete sample space
Element (or member)
Elementary event
∈, element of
Event
Event space, U
Experiment
s-ﬁeld
iff, if and only if
), implies that
Independence of events
8, for all
\, intersection
Inverting conditional probabilities
Joint, mutual, or complete
independence of events
Mathematical induction
P
n
i¼1 or P
j2j, multiplication
Multiplication rule
Mutually exclusive (or disjoint)
events
=2, not an element of
;, null set
Outcome
P(A|B)
Pairwise independence
Partition of a set
Posterior probability
Prior probability
Probability of event A
Probability set function (or
probability measure), P
Probability space
Probability theorems
Product notation
Relative frequency probability
Sample point
Sample space
Subjective probability
The Law of Inverse Probabilities
The occurrence of event A
[, union
Problems
1. Deﬁne an appropriate sample space for each of the
experiments described below:
a. At the close of business each day, the Acme Depart-
ment Store’s accountant counts the number of cus-
tomer transactions that were made in cash. On a
particular day, there were 100 customer transactions
at the department store. The outcome of interest is
the number of cash transactions made.
b. An Italian restaurant in the city of Spokane runs an
ad in the city newspaper, The Spokesman Review,
that contains a coupon that allows a customer to
purchase two meals for the price of one for each
newspaper coupon the customer has. The coupon is
valid for 30 days after the ad is run. The outcome of
interest is how many free meals the restaurant serves
at the end of the 30-day period.
c. On a local 11 o’clock news broadcast for the town of
College Station, the weather report includes the high
and low temperatures, in Fahrenheit, for the preced-
ing 24 hours. The outcome of interest is the pair of
high and low temperatures on any given day.
d. A local gasoline jobber supplies a number of the area’s
independent gas stations with unleaded gasoline. The
outcome of interest is the quantity of gasoline
demanded from the jobber in any given week.
e. The mutual funds management company of Dewey,
Cheatum, and Howe posts the daily closing net asset
value of shares in its mutual fund on a readerboard
outside of its headquarters. The outcome of interest
is the posted net asset value of the shares at the end of
a given day.
f. The ofﬁce manager of a business specializing in
copying services is counting the number of copies
that
a
given
copying
machine
produces
before
suffering a paper jam. The outcome of interest is the
number of copies made before the machine suffers a
paper jam.
2. For each of the sample spaces you have deﬁned above,
indicate whether the sample space is ﬁnite, countably
inﬁnite, or uncountably inﬁnite. Justify your answers.
3. The sales team of a large car dealership in Seattle
consists of the following individuals:
Name
Sales experience
Age
Education
Married
Tom
4 years
34
High school
Yes
Karen
12 years
31
< High school
No
Frank
21 years
56
College grad
Yes
Problems
35

Eric
9 years
42
High school
Yes
Wendy
3 years
24
College grad
No
Brenda
7 years
29
High school
No
Scott
15 years
44
College grad
Yes
Richard
2 years
25
< High school
No
A customer visiting the dealership randomly chooses one
of the salespersons to discuss the purchase of a new vehi-
cle. Deﬁne the set and assign the probability associated
with each of the following events:
a. A woman is chosen.
b. A man less than 40 years of age is chosen.
c. An individual with at least 10 years of sales experi-
ence is chosen.
d. A married College graduate is chosen.
e. A married female with a high school education and at
least 5 years of sales experience is chosen.
f. An individual with at least 2 years’ experience and at
least 21 years of age is chosen.
4. Assign probabilities to the events a to f in the preced-
ing question, but include the condition “given that the
individual chosen is  30 years old.”
5. The manager of the cost accounting department of a
large computer manufacturing ﬁrm always tells three
jokes during her monthly report to the board of directors
in an attempt to inject a bit of levity into an otherwise
sobering presentation. She has an inventory of a dozen
different jokes from which she chooses three to present
for any given monthly report.
a. If she chooses the three jokes randomly from the inven-
tory of 12 each month, what is the probability that, in
any given month, at least one of the three jokes will be
different from the jokes she told the month before?
b. If she chooses the three jokes randomly from the
inventory of 12 each month, what is the probability
that, in any given month, all three jokes will be dif-
ferent from the three she told the month before?
6. Schneider’s Plumbing and Heating, located in Fargo,
North Dakota, has 300 accounts receivable distributed as
follows:
Current
1–30 days
past due
31–60 days
past due
61–90 days
past due
Sent for
collection
140
80
40
25
15
An auditor is coming to inspect Schneider’s ﬁnancial
records. Included in the auditor’s analysis is a randomly
chosen sample of four accounts from the company’s col-
lection of accounts receivable.
a. What is the probability that all of the accounts cho-
sen by the auditor will be current accounts?
b. What is the probability that all of the accounts cho-
sen by the auditor will be less than or equal to 60 days
past due?
c. What is the probability that all of the accounts cho-
sen by the auditor will be more than 60 days past due?
d. What is the probability that two of the accounts will
be current, and two will be 1–30 days past due?
7. A computer manufacturing ﬁrm produces three prod-
uct lines: (1) desktop computer systems, (2) notebook
computers, and (3) subnotebook computers. The sales
department has convened its monthly meeting in which
the four staff members of the department provide the
department manager with their indications of whether
sales will increase for each of the product lines in the
coming month. Let Ai represent the event that sales for
product line i (¼1, 2, or 3) will increase in the coming
month. The manager will consider the information of a
given staff member to be usable if that information is
internally consistent, where internally consistent in this
context means consistent with the axioms and theorems
of probability. Which of the staff members have provided
the manager with usable information? Be sure to provide a
convincing reason if you decide that a staff member’s
information needs to be discarded.
Staff member
Tom
Dick
Harry
Sally
P(A1)
.5
.3
.3
.2
P(A2)
.3
.2
.6
.3
P(A3)
.7
.8
.4
.5
P(A1\A2)
.9
.4
.4
.2
P(A1\A3)
.6
.15
.2
.3
P(A2\A3)
.15
.1
.1
.4
P(A1\A2\A3)
.1
1.5
.05
.1
8. A large electronics ﬁrm is attempting to hire six new
electrical engineers. It has been the ﬁrm’s experience that
35 percent of the college graduates who are offered
positions with the ﬁrm have turned down the offer of
employment.
After
interviewing
candidates
for
the
positions, the ﬁrm offers employment contracts to seven
college graduates. What is the probability that the ﬁrm
36
Chapter 1
Elements of Probability Theory

will receive acceptances of employment from one too
many engineers? You may assume that the decisions of
the college graduates are independent.
9. A computer manufacturing ﬁrm accepts a shipment
of CPU chips from its suppliers only if an inspection of 5
percent of the chips, randomly chosen from the shipment,
does not contain any defective chips. If a shipment
contains ﬁve defective chips and there are 1,000 chips in
the shipment, what is the probability that the shipment
will be accepted?
10. The probability that a stereo shop sells at least one
ampliﬁer on a given day is .75; the probability of selling at
least one CD player is .6; and the probability of selling at
least one ampliﬁer and at least one CD player is .5.
a. What is the probability that the stereo shop will sell
at least one of the two products on a given day?
b. What is the probability that the stereo shop will sell
at least one CD player, given that the shop sells at
least one ampliﬁer?
c. What is the probability that the stereo shop will sell
at least one ampliﬁer, given that the shop sells at
least one CD player?
d. What is the probability that the shop sells neither of
the products on a given day?
11. Prove that the set function deﬁned by
PðAjBÞ ¼ PðA \ BÞ
PðBÞ
for PðBÞ 6¼ 0
is a valid probability set function in the probability space
{B,UB,P(|B)}, where UB is the event space for the sample
space B.
12. A large midwestern bank has devised a math aptitude
test that it claims provides valuable input into the hiring
decision for bank tellers. The bank’s research indicates
that 60 percent of all tellers hired by midwestern banks
are classiﬁed as performing satisfactorily in the position
at their initial 6-month performance review, while the
rest are rated as unsatisfactory. Of the tellers whose per-
formance is rated as satisfactory, 90 percent had passed
the math aptitude test. Of the tellers who were rated
unsatisfactory, only 20 percent had passed the math apti-
tude test.
a. What is the probability that a teller would be rated as
satisfactory at her 6-month performance review,
given that she passed the math aptitude test?
b. What is the probability that a teller would be rated as
satisfactory at her 6-month performance review,
given that she did not pass the math aptitude test?
c. Does the test seem to be an effective screening device
to use in hiring tellers for the bank? Why or why not?
13. A large-scale ﬁrm specializing in providing tempo-
rary
secretarial
services
to
corporate
clients
has
completed a study of the main reason why secretaries
become dissatisﬁed with their work assignments, and
how likely it is that a dissatisﬁed secretary will quit her
job. It was found that 20 percent of all secretaries were
dissatisﬁed with some aspect of their job assignment. Of
all dissatisﬁed secretaries, it was found that 55 percent
were dissatisﬁed mainly because they disliked their
supervisor; 30 percent were dissatisﬁed mainly because
they felt they were not paid enough; 10 percent were
dissatisﬁed mainly because they disliked the type of
work; and 5 percent were dissatisﬁed mainly because
they
had
conﬂicts
with
other
employees.
The
probabilities that the dissatisﬁed secretaries would quit
their jobs were respectively .20, .30, .90, and .05.
a. Given that a dissatisﬁed secretary quits her job, what
is the most probable main reason why she was dissat-
isﬁed with her job assignment?
b. If a secretary were chosen at random, what is the
probability that she would be dissatisﬁed, mainly
because of her pay?
c. Given that a secretary is dissatisﬁed with her job
assignment, what is the probability that she will
quit?
14. A
clerk
is
maintaining
three
different
ﬁles
containing job applications submitted for three different
positions currently open in the ﬁrm at which the clerk is
employed. One ﬁle contains two completed applications,
one ﬁle contains one complete and one incomplete appli-
cation, and the third ﬁle contains two incomplete
applications. The clerk wishes to examine the ﬁles and
chooses one of the ﬁles at random. She then chooses at
random one of the applications contained in the chosen
ﬁle. If the application chosen is complete, what is the
probability that the remaining application in the ﬁle is
also complete?
15. A company manages three different mutual funds.
Let Ai be the event that the ith mutual fund increases in
value on a given day. Probabilities of various events relat-
ing to the mutual funds are given as follows:
Problems
37

PðA1Þ¼:55; PðA2Þ¼:60; PðA3Þ¼:45; PðA1 [ A2Þ¼:82;
PðA1 [ A3Þ ¼ :7525; PðA2 [ A3Þ ¼ :78; PðA2 \ A3 j A1Þ ¼ :20:
a. Are events A1, A2, and A3 pairwise independent?
b. Are events A1, A2, and A3 independent?
c. What is the probability that funds 1 and 2 both
increase in value, given that fund 3 increases in
value? Is this different from the unconditional proba-
bility that funds 1 and 2 both increase in value?
d. What is the probability that at least one mutual fund
will increase in value on a given day?
16. Answer the following questions regarding the valid-
ity of probability assignments. If you answer false, explain
why the statement is false.
a. If P(A) ¼ .2, P(B) ¼ .3, and A\B ¼ ; , then P(A[B)
¼ .06. True or False?
b. If A\B ¼ ; and P(B) ¼ .2, then P(A|B) ¼ 0. True or
False?
c. If P(B) ¼ .05, P(A|B) ¼ .80, and P(A| B ) ¼ .5, then
P(B|A) ¼ .0777 (to four digits of accuracy). True or
False?
d. If P(A) ¼ .8 and P(B) ¼ .7, then P(A\B)  .5. True or
False?
e. It is possible that P(A) ¼ .7, P(B) ¼ .4, and A\B ¼ ;.
True or False?
17. The ZAP Electric Co. manufactures electric circuit
breakers. The circuit breakers are produced on two differ-
ent assembly lines in the company’s Spokane plant.
Assembly line I is highly automated and produces 85 per-
cent of the plant’s output. Assembly line II uses older
technology that is more labor intensive, producing 15 per-
cent of the plant’s output. The probability that a circuit
breaker manufactured on assembly line I is defective is
.04, while the corresponding probability for assembly line
II is .01.
As part of its quality control program, ZAP uses a testing
device for determining whether a circuit breaker is faulty.
Some important characteristics of the testing device are
as follows:
PðAjBÞ ¼ PðAjBÞ ¼ :985;
where A is the event that the testing device indicates that
a circuit breaker is faulty and B is the event that the
circuit breaker really is faulty.
a. If a circuit breaker is randomly chosen from a bin
containing a day’s production and the circuit breaker
is actually defective, what is the probability that it
was produced on assembly line II?
b. What is the probability that the testing device
indicates that a circuit breaker is not faulty, given
that the circuit breaker really is faulty?
c. If the testing device is applied to circuit breakers
produced on assembly line I, what is the probability
that a circuit breaker really is faulty, given that the
testing device indicates that the circuit breaker is
faulty? Would you say that this is a good testing
device?
18. The ACME Computer Co. operates three plants that
manufacture notebook computers. The plants are located
in Seattle, Singapore, and New York. The plants produce
20, 30, and 50 percent of the company’s output, respec-
tively. ACME attaches the labels “Seattle,” “SING,” or
“NY” to the underside of the computer in order to iden-
tify the plant in which a notebook computer was
manufactured. The computers carry a 2-year warranty,
and if a customer requires repairs during the warranty
period, he or she must send the computer back to the
plant in which the computer was manufactured. There
is also a stamp on the motherboard inside the computer
which technicians at a plant can use as an additional way
of identifying which plant manufactured the computer.
The consumer is unable to examine this inside stamp,
because if the consumer opens up the computer housing
to look inside, a seal is broken which voids the warranty.
Regarding quality control at the plants, the warranty-
period failure rates of computers manufactured in the
three plants are known to be .01, .05, and .02 for the
Seattle, Singapore, and New York plants, respectively.
You have bought an ACME computer, and it has failed
during the warranty period. You need to send the com-
puter back to the plant for repairs, but the label on the
underside of the computer has been lost and so you don’t
know which plant manufactured your computer.
a. Which plant is the most probable plant to have
manufactured your computer?
38
Chapter 1
Elements of Probability Theory

b. Which plant is the least probable plant to have
manufactured your computer?
c. What is the probability that an ACME notebook com-
puter will fail during the warranty period?
d. Given that an ACME computer does not fail during
the warranty period, what is the probability that the
computer was manufactured in New York?
19. The diagram below indicates how probabilities have
been assigned to various subsets of the sample space S:
a. Are the three events A, B, and C pairwise indepen-
dent events?
b. Are the three events A, B, and C independent events?
c. What is the value of P(A\B)? What is the value of
P(A\B | C)?
d. Suppose
event
D
is
such
that
P(D) ¼ .05
and
D\(A[B[C) ¼ ;. Are events D and A independent?
Are events D and (A[B[C) independent?
e. What is the probability of event C, given (A\B)?
20. A large sack contains 1,000 ﬂower seeds consisting of
300 carnations and 700 impatiens. Of the 300 carnation
seeds, 200 will produce red ﬂowers and 100 will produce
white ﬂowers. Of the 700 impatiens seeds, 400 will pro-
duce red ﬂowers and 300 will produce white ﬂowers.
a. If you randomly choose ﬁve seeds in succession
(without replacing any seeds that have been chosen),
what is the probability that these seeds will produce
two impatiens with red ﬂowers, two carnations with
red ﬂowers, and one carnation with white ﬂowers?
b. If you randomly choose four seeds in succession
(without replacing any seeds that have been chosen),
what is the probability that these seeds will produce
red- and white-ﬂowered impatiens and red- and
white-ﬂowered carnations?
c. Given that four randomly chosen seeds all produce
carnations, what is the probability that three are red
ﬂowered and one is white ﬂowered?
d. Is the event of randomly choosing a carnation seed on
the ﬁrst draw independent of choosing an impatiens
seed on the second draw?
21. For each case below, determine whether or not the
real-valued set function P(A) is in fact a probability set
function:
a. Sample space S ¼ {1, 2, 3, 4, 5, 6, 7, 8}, Event space
U ¼ fA : A  Sg
,
Set
Function
P(A) ¼
Sx2Aðx=36Þ for A 2 U.
b. Sample space S ¼ [0, 4), Event Space U ¼ fA : A is an
interval subset of S, or any set formed by unions,
intersections, or complements of these interval
subsets}, Set Function P(A) ¼
R
x2A exdx for A 2U.
c. Sample Space S ¼ {x : x is a positive integer},
Event Space U ¼ fA : A  Sg; Set Function P(A) ¼
Sx2Aðx2 = 105Þ for A 2 U
d. Sample Space S ¼ (0, 1), Event Space U ¼ fA : A is an
interval subset of S, or any set formed by unions,
intersections, or complements of these interval
subsets}, Set Function P(A) ¼ R
x2A 12xð1  x Þ2dx
for A 2 U
22. The Smith Floor Wax Company manufactures and
sells industrial-strength ﬂoor wax in the wholesale market
for home care products. The factory produces 10,000 gal of
ﬂoor wax daily and currently has an inventory of 5,000 gal
of ﬂoor wax in its warehouse. If sales of ﬂoor wax exceed
production, the company meets the excess demand by
using inventory; while if sales are less than production,
the company adds this excess production to inventory.
The company economist provides you with the following
information concerning probabilities of daily sales events,
where events are measured in gallons of wax sold.
A ¼ [0, 5,000], P(A) ¼ .25
B ¼ (5,000, 10,000], P(B) ¼ .65
C ¼ [2,500, 7,500], P(C) ¼ .35
Problems
39

D ¼ (5,000, 7,500], P(D) ¼ .20
a. What is the probability that inventory will have to be
used to satisfy sales on a given day?
b. What is the probability that fewer than 2,500 gal of
wax will be sold on a given day?
c. What is the probability of the event E ¼ [0, 2,500)[
(7,500,10,000]?
23. A box contains four different computer disks, labeled
1, 2, 3, and 4. Two disks are selected at random from the
box “with replacement,” meaning that after the ﬁrst
selection is made, the selected disk is returned to the
box before the second selection is made. “At random”
means that all disks in the box have an equal chance of
being selected.
a. Deﬁne the sample space for this experiment.
b. Is the event of choosing disk 1 or 3 on the ﬁrst selec-
tion independent of choosing disk 1 or 2 on the sec-
ond selection? Why or why not?
c. Is the event of choosing disk 1 on the ﬁrst selection
independent of choosing disk 1 on the second selec-
tion? Why or why not?
d. Is the event of choosing disk 1 on both the ﬁrst and
second selections independent of the event that nei-
ther disk 3 nor 4 is chosen in the selection process?
24. The AJAX Microchip Company produces memory
chips for personal computers. The company’s entire pro-
duction is generated from two assembly lines, labeled I
and II. Assembly line I uses more rapid assembly
techniques and produces 80 percent of the company’s
output, while assembly line II produces 20 percent of the
output. The probability that a memory chip produced on
assembly line I is defective is .05, while the corresponding
probability for assembly line II is .01.
A memory chip is chosen at random from a bin containing
a day’s production. Given that the chip is found to be
defective, what is the probability that the chip was made
on assembly line II? (Hint: Can you put this problem in a
form for which Bayes rule would be applicable?)
25. The management of the AJAX Microchip Company
(mentioned in Problem 24) is interested in increasing
quality control at the plant, and is considering the pur-
chase of a testing device that can determine when a mem-
ory chip is faulty. In particular, the speciﬁcations on the
device are as follows:
PðAjBÞ ¼PðAjBÞ¼:98;
where A is the event that the testing device indicates that
a memory chip is faulty, and B is the event that the
memory chip really is faulty.
a. What is the probability that the testing device
indicates that a memory chip is not faulty, given
that the memory chip really is faulty?
b. If the testing device is applied to the memory chips
produced by AJAX’s assembly line I, what is P(B|A),
i.e., the probability that a chip really is faulty, given
that the testing device indicates the chip is faulty.
c. Suppose AJAX management wants P(B|A) to be .95.
What is the value of r ¼ PðAjBÞ ¼ PðAjBÞ that will
ensure this testing accuracy if the test is applied to
the chips produced on assembly line I.
26. Let S ¼ [0, 5] be a sample space containing all possi-
ble values of the daily quantity demanded of electric
power for a large midwestern city in the summer months.
The units of measurement are millions of megawatts, and
the capacity of the power grid is ﬁve million megawatts.
Answer the following questions concerning probability
assignments to events in the sample space, S, relating to
the daily demand for electric power. Treat the informa-
tion provided in the questions as cumulative. Justify your
answers.
a. Given that A ¼ [0, 4], B ¼ [3, 5], P(A) ¼ .512 and
P(B) ¼ .784, what is the probability that the power
demand will be no greater than 4 million megawatts
and no less than three million megawatts, i.e., what
is the probability of A\B?
b. What is the probability of the event C ¼ [0, 3)?
c. Can P(D) ¼ .6 given that D ¼ [0, 2.5]?
d. Given that P([0, 2]) ¼ .064, what is the probability of
the event E ¼ (2,4]?
27. SUPERCOMP, a retail computer store, sells personal
computers and printers. The number of computers and
printers
sold
on
any
given
day
varies,
with
the
probabilities of the various possible sales outcomes
being given by the following table:
40
Chapter 1
Elements of Probability Theory

Number of computers sold
0
1
2
3
4
Number
of
printers
sold
0
.03
.03
.02
.02
.01
Probabilities of
elementary
events
1
.02
.05
.06
.02
.01
2
.01
.02
.10
.05
.05
3
.01
.01
.05
.10
.10
4
.01
.01
.01
.05
.15
a. Deﬁne an appropriate sample space for the experi-
ment of observing how many computers and printers
are sold on any given day.
b. Can the information provided in the table be used to
deﬁne
a
probability
set
function
for
assigning
probabilities to all events in the sample space?
Explain (brieﬂy, but clearly).
c. What
is
the
probability
that
more
than
two
computers will be sold on any given day? What is
the probability that more than two printers will be
sold on any given day? In each case, deﬁne the set of
outcomes in S that corresponds to the stated events.
d. What is the probability of selling more than two
printers, given that more than two computers are
sold? Show your calculation.
e. What is the probability of selling more than two
printers and more than two computers? Show your
calculation.
f. What is the probability that SUPERCOMP has no
sales on a given day? Given that SUPERCOMP sells
no computers, what is the probability that it sells no
printers on a given day?
28. Consider the experiment of tossing a fair coin (mean-
ing heads and tails are equally likely on each toss) three
times and observing the sequence of heads and tails that
results. Let H denote heads and T denote tails.
a. Deﬁne the sample space, S, for this experiment.
b. Let A be the event that at least one of the tosses
results in heads. Deﬁne the appropriate subset of S
that deﬁnes A. Find PðAÞ.
c. Let B be the event that at least two of the tosses
results in tails. Deﬁne the appropriate subset of S
that deﬁnes B. Find PðBÞ.
d. Deﬁne the probability that at least one of the tosses
results in heads and at least two of the tosses results
in tails, P A \ B
ð
Þ.
e. Let C be the event that all three tosses result in tails.
Are A and C disjoint events? Are B and C disjoint
events?
f. What is the probability of A or C occurring? What is
the probability of B or C occurring? What is the prob-
ability of A or B occurring?
29. BuyOnLine is a large internet-based online retailer
that
maintains
four
different
teams
of
sales
representatives.
The
ages
of
unpaid
invoices
from
each of the four sales teams is summarized in the table
below.
a. If an invoice is selected randomly from the pooled set
of invoices, what is the probability that it is from
sales team C?
b. What is the probability that a randomly selected
invoice from the pooled set of invoices is over
180 days old?
c. If an invoice is selected randomly from the pooled set
of invoices, what is the probability that is over
180 days old and from sales team C?
d. If an invoice is selected randomly from the pooled set
of invoices, what sales team has the lowest probabil-
ity of being associated with the invoice?
Sales teams
Age of invoice
A
B
C
D
Under 120 days
34
103
45
97
120–180 days
27
39
65
47
Over 180 days
18
25
19
10
30. The Port Authority of a large East Coast City is
investigating the trafﬁc ﬂow in and out of a large train
station in the middle of the city. There are six entry gates
and six exit gates that travelers can use to enter or leave
the train station. An experiment is to be conducted to
observe the number of gates open (i.e., a person is
traveling through it) at a given point in time. You can
assume that each point in the sample space is equally
likely to occur, where an outcome is characterized by
the two-tuple x; y
ð
Þdenoting that x entry gates are open
and yexit gates are open.
a. Deﬁne an appropriate sample space,
S , for this
experiment.
b. Deﬁne the appropriate probability set function for
assigning probability to any event A  S.
Problems
41

c. What is the probability that one entry and one exit
gate will be open?
d. What is the probability that at least half the gates will
be open in each direction?
e. What is the probability that the same number of gates
will be open in each direction?
f. What is the probability that the total number of gates
open will be less than four?
31. Let Ai; i ¼ 1; :::; 4, represent four events in a sample
space, S . For each of the situations below, determine
which assignment of probabilities are actually possible
(i.e., do not contradict Kolmogorov’s axioms), and which
are not. Justify your answers.
a. P A1
ð
Þ ¼ :3; P A2
ð
Þ ¼ :3; P A3
ð
Þ ¼ :2; P A4
ð
Þ ¼ :2
b. P A1
ð
Þ ¼ :3; P Aj


 P A1
ð
Þ for j ¼ 2; 3; 4
c. P A1
ð
Þ ¼ :7; P A2
ð
Þ ¼ :6; P A1 \ A2
ð
Þ ¼ :1
d. P A1
ð
Þ ¼ :7; P A2
ð
Þ ¼ :6; P A1 [ A2
ð
Þ ¼ :1
e. P A1
ð
Þ ¼ :3; P A2
ð
Þ ¼ :4; P A3
ð
Þ ¼ :1; P A4
ð
Þ ¼ :2
where Ai  Aj; i<j
f. P A1
ð
Þ ¼ :4; P A2
ð
Þ ¼ :3; P A1 [ A2
ð
Þ ¼ :5
where A1 \ A2 ¼ ;
32. In each case below, determine whether the set func-
tion, P, is a probability set function.
a. PðAÞ ¼ 1
91
X
x2A
x2 for A  S; S ¼ 1; 2; 3; 4; 5; 6
f
g
b. PðAÞ¼
Z
x2A
:25e:25xdx; where A is any Borel subset of S
¼ 0; 1
½
Þ
c. PðAÞ ¼
X
x2A
:3x:71x for A  S; S ¼ 0; 1
f
g
d. PðAÞ ¼
Z
x2A
4x3dx where A is any Borel subset of S
¼ 0; 1
½

33. BuyOnLine is a large internet-based online retailer
that
maintains
four
different
teams
of
sales
representatives.
The
ages
of
unpaid
invoices
from
each of the four sales teams is summarized in the table
below.
a. If an invoice is selected randomly from the pooled set
of invoices, what is the probability that it is from
sales team C, given that it is from either team C or D?
b. What is the probability that two randomly selected
invoices from the pooled set of invoices, selected
sequentially without replacement, are both over
180 days old?
c. If an invoice is selected randomly from the pooled set
of invoices, what is the probability that it is from
sales team C, given that is it under 120 days old?
d. If an invoice is selected randomly from the pooled set
of invoices, what is the probability that it is from
sales team A or B, given that it is less than or equal
to 180 days old?
Sales teams
Age of invoice
A
B
C
D
Under 120 days
34
103
45
97
120–180 days
27
39
65
47
Over 180 days
18
25
19
10
34. If P (A) ¼ .3, P (B) ¼ .4, P (A|B) ¼ .3, what is the value
of
a. P(A\B)
b. P (A[B)
c. P(A|B)
d. P AjB


e. P AjB


f. P(B|A)
g. P A \ B


h. P A [ B


35. A regional airline implements a standard sales prac-
tice of “overbooking” their ﬂights, whereby they sell
more tickets for a ﬂight then there are seats available for
passengers. Their rationale for this practice is that they
want to ﬁll all of the seats on their planes for maximum
proﬁtability, and there is a positive probability that a
42
Chapter 1
Elements of Probability Theory

customer who has been sold a ticket will not use their
ticket on the day of the ﬂight, so that even if there are
more tickets sold than seats available, there may be sufﬁ-
cient seats available to accommodate the customers who
actually use their tickets and take a ﬂight on any given
day. Assuming that the event that a customer actually
uses their ticket is .995, the airline’s planes have 100
seats, and the events that customers use their tickets on
the day of the ﬂight are jointly independent, answer the
following
questions
relating
to
their
overbooking
practice.
a. If the airline does not overbook, and only sells 100
tickets for each of their ﬂights, what is the probability
that a given ﬂight will ﬂy full ?
b. Using the sales strategy in (a), what is the probability
that one or more seats for a given ﬂight will be
empty?
c. For the sales strategy in (a), if the airline has 10 ﬂights
per day from the Seattle-Tacoma airport, what is the
probability that all of the ﬂights will ﬂy full?
d. For the sales strategy in (a), what is the probability
that there will be one or more empty seats among the
1,000 seats available on the airline’s 10 ﬂights from
the Seattle-Tacoma airport on a given day?
36. An automobile manufacturer will accept a shipment
of tires only if an inspection of 5 percent of the tires,
randomly chosen from the shipment, does not contain
any defective tires. The manufacturer receives a shipment
of 500 tires, and unknown to the manufacturer, ﬁve of the
tires are defective.
a. What is the probability that the shipment will be
accepted?
b. What percent of the tires would need to be randomly
chosen and inspected if the manufacturer wanted to
reject such a shipment described above, with .90
probability?
37. The table below indicates the probabilities of various
outcomes with regard to the size of purchases and method
of payment for customers that enter to a large New York
electronics store:
a. Is the event of a customer paying cash independent of
the event that the customer spends < $100?
b. Given that the customer pays cash, what is the prob-
ability that the customer spends  $500?
c. Given that the customer pays by credit card, what is
the probability that the customer spends  $500?
d. What is the probability that the customer pays by
credit card, given that the purchase is  $500?
e. Given that the customer spends $100 or more, what
is the probability that the customer will not pay by
cash?
Method of payment
Size of purchase
Cash
Credit card
Layaway plan
< $100
.20
.10
.01
$100–500
.15
.15
.05
> $500
.05
.20
.09
38. A new medical test has been developed by a major
pharmaceutical manufacturer for detecting the incidence
of a bacterial infection. Of the people who actually have the
disease, the test will correctly indicate that the disease is
present 95 percent of the time. Among people who do not
have the disease, the test incorrectly indicates the disease
is present 5 percent of the time. The health department of a
major east coast city is contemplating making the test
available to a population in which .2 percent of the
individuals in the population actually have the disease.
a. For a randomly selected individual from the popula-
tion, if the test indicates that the person has the
disease, what is the probability that the person actu-
ally does have the disease?
b. For a randomly selected individual from the popula-
tion, if the test indicates that the person does not
have the disease, what is the probability that the
person actually does not have the disease?
39. A large food processor operates three processing
plants on the west coast. The plants, labeled 1, 2, and 3,
differ in size, and produce 20, 35, and 45 percent of the
food processor’s total output of spinach, respectively.
Given past history of USDA inspections for sanitation,
the
probability
of
a
contaminated
box
of
spinach
emanating from each of the three plants can be assumed
to be .0001, .0002, and .0005, respectively.
Contamination of the food processor’s spinach product
with Ecoli was identiﬁed in a box of spinach shipped to
an east coast grocery store, but the bill of lading has been
misplaced on the shipment so that it is not known from
which plant the shipment originated from. Let Ai,i ¼ 1, 2,
3, denote the events that a box of spinach came from
plants 1, 2, and 3, respectively. Let the event C denote
Problems
43

that a shipment is contaminated, and thus C denotes that
a shipment is contamination free.
a. Which plant is the most probable to have produced
this contaminated spinach?
b. Which plant is the least probable to have produced
this contaminated spinach?
c. What is the probability that the contaminated spin-
ach was produced at one of the two smaller plants?
40. This problem is the famous “Birthday Problem” in
the statistics literature. The problem is the following: In a
room of n people, what is the probability that at least two
people share the same birthday? You can ignore leap years,
so assume there are 365 different birthday possibilities,
and you can also assume that a person being born on any
of the 365 days is equally likely.
a. If there are 23 people in the room, what is the probabil-
ity that at least two people share the same birthday?
b. How many people need to be in the room for there to
be a .99 probability that at least two people share the
same birthday?
41. The Baseball World Series in the U.S. consists of
seven games, and the ﬁrst team to win four games is the
winner of the series. Assume that the teams are evenly
matched.
a. What is the probability that the team that wins the
ﬁrst game of the series will go on to win the World
Series?
b. What is the probability that a team that has lost the ﬁrst
three games of the series will win the World Series?
42. The BigVision Electronic Store sells a large 73 inch
diagonal big screen TV. The TV comes with a standard
1 year warranty on parts and labor so that if anything
malfunctions on the TV in the ﬁrst year of ownership, the
company repairs or replaces the TV for free. The store also
sells an “extended warranty,” which a customer can pur-
chase that extends warranty coverage on the TV for
another 2 years, for a total of three years of coverage. The
daily numbers of TVs and extended warranties sold, and
their probabilities, are represented by the following proba-
bility space
S; U; P
f
g, where x denotes the number of TVs
sold and y denotes the number of extended Uwarranties
sold:
S ¼
x; y
ð
Þ : x and y 2 0; 1; 2; 3; 4
f
g; x  y
f
g;ϒ¼ A : A  S
f
g;
and
PðAÞ¼
X
x;y
ð
Þ2A
f x; y
ð
Þ
where the nonzero values of f x; y
ð
Þ are deﬁned in the
following table:
Number of extended warranties
0
1
2
3
4
0
0
0
0
0
0
1
.02
.03
0
0
0
Number of TVs
2
.04
.05
.06
0
0
3
.06
.07
.08
.09
0
4
.08
.09
.10
.11
.12
a. What is the probability that all of the TVs sold on a
given day will be sold with extended warranties?
b. Given that  2 TVs are sold, what is the probability
that all of the TVs will be sold with extended
warranties?
c. Let A be the event that no TVs are sold, and B be the
event that no extended warranties are sold. Are A and
B independent events?
d. What is the probability that  3 extended warranties
are sold on a given day?
e. What
is
the
probability
that
more
extended
warranties are sold than TVs?
f. Find an algebraic representation of the function f(x,y)
that can be used to replace the table of values above.
44
Chapter 1
Elements of Probability Theory

2
n
Random Variables, Densities,
and Cumulative Distribution
Functions
n
n
n
2.1
Introduction
2.2
Univariate Random Variables and Density Functions
2.3
Univariate Cumulative Distribution Functions
2.4
Multivariate Random Variables, PDFs, and CDFs
2.5
Marginal Probability Density Functions and CDFs
2.6
Conditional Density Functions
2.7
Independence of Random Variables
2.8
Extended Example of Multivariate Concepts in the
Continuous Case
2.1
Introduction
It is natural for the outcomes of many experiments in the
real world to be measured in terms of real numbers. For example, measuring the
height and weight of individuals, observing the market price and quantity
demanded of a commodity, measuring the yield of a new variety of wheat, or
measuring the miles per gallon achievable by a new hybrid automobile all result
in real-valued outcomes. The sample spaces associated with these types of
experiments are subsets of the real line or, if multiple values are needed to
characterize the outcome of the experiment, subsets of n-dimensional real
space, Rn.
There are also experiments whose outcomes are not inherently numbers and
whose sample space is not inherently a subset of a real space. For example,
observing whether a tossed coin results in heads or tails, observing whether an
item selected from an assembly line is defective or nondefective, observing the
type of weeds growing in a garden, and observing which engine components
caused an engine failure in an automobile are not experiments characterized
inherently by real-valued outcomes. It will prove to be both convenient and
useful to convert such sample spaces into real-valued sample spaces by
associating a real number to each outcome in the original sample space. This
process can be viewed as coding the outcomes of an experiment with real
numbers.

Furthermore, the outcomes of an experiment may not be of direct interest in
a given problem setting; instead, real-valued functions of the outcomes may be
of prime importance. For example, in a game of craps, it is not the outcome of
each die that is of primary importance, but rather the sum of the dots facing up
determines whether a player has won or lost. As another example, if a ﬁrm is
interested in calculating the proﬁt associated with a given operation, it is the
price of the product multiplied by the quantity sold, deﬁning revenue, that will
be of primary importance in the proﬁt calculation, and not price and quantity,
per se.
All of the previous situations involve the concept of a random variable,
which can be used to characterize the ultimate experimental outcomes of inter-
est as real numbers. We now develop the concept of a random variable.
2.2
Univariate Random Variables and Density Functions
We begin with the deﬁnition of the term random variable appropriate for the
univariate, or one-variable, case.
Deﬁnition 2.1
Univariate
Random Variable
Let {S, ϒ, P} be a probability space. IfX : S ! Ris a real-valued function having
as its domain the elements of S, then X is a random variable.
A pictorial illustration of the random variable concept is given in Figure 2.1.
The reader might ﬁnd it curious, and perhaps even consider it a misnomer,
for the term “random variable” to be used as a label for the concept just given.
The expression random-valued function would seem more appropriate since it
is, after all, a real-valued function that is at the heart of the concept presented in
the deﬁnition. Nonetheless, usage of “random variable” has become standard
terminology, and we will use it also.
The phrase outcome of the random variable refers to the particular image
element in the range of the random variable, R(X), that occurs as a result of
w ∈ S
X: S →
x = X (w)∈
Figure 2.1
Random variable X.
46
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

observing the outcome of a given experiment, i.e., if the outcome of an experi-
ment is w ∈S, then the outcome of the random variable is x ¼ X(w).
Deﬁnition 2.2
Random Variable
Outcome
The image x ¼ X(w) of an outcome w 2 S generated by a random variable X.
Henceforth, we will use upper case letters, such as X, to denote random
variables and their lower case counterparts to denote an image value of the
random variable, as x ¼ X(w) for w ∈S. The letter X that we use here is arbitrary,
and any other symbol could be used to denote a random variable. For the most
part, we will use letters in the latter part of the alphabet for representing random
variables. Letters at the beginning of the alphabet will be used to denote
constants, and so the expression x ¼ a will mean that the value, x, of the random
variable, X, equals the constant a. Similarly, x ∈A will mean that the value of X
is an element of the set A.
If the outcomes of an experiment are real numbers to begin with, they are
directly interpretable as outcomes of a random variable, since we can always
represent the real-valued outcomes w ∈S as images of an identity function, e.g.,
X(w) ¼ w. If the outcomes of an experiment are not initially in the form of real
numbers, a random variable can always be deﬁned that associates a real number
with each outcome w ∈S, as X(w) ¼ x, and thus as we noted above, a random
variable effectively codes the outcomes of a sample space with real numbers.
Through the use of the random variable concept, all experiments with univariate
outcomes can be ultimately interpreted as having sample spaces consisting of
real-valued elements. In particular, the range of the random variable, RðXÞ ¼
x : x ¼ XðwÞ; w 2 S
f
g, represents a real-valued sample space for the experiment.
2.2.1
Probability Space Induced by a Random Variable
Given a real-valued sample space that has been deﬁned for a given experiment
via a random variable, we seek a probability space that can be used for assigning
probabilities to events involving random variable outcomes. This requires that
we establish how probabilities are to be assigned to subsets of the real-valued
sample space R(X). In so doing, we must deﬁne an appropriate probability set
function for assigning probabilities to subsets of R(X) and identify the event
space or domain of the probability set function.
Given the probability space, {S, ϒ, P}, we are initially equipped to assign
probabilities to events in S. What is the probability that an outcome of X resides
in the set A  R(X)? Suppose an event in S can be deﬁned, say B, that occurs iff
the event A in R(X) occurs. Then, since the two events occur only simulta-
neously, they must have the same probability of occurring and we can state that
PX(A)  P(B) when A , B, where PX ð Þ is used to denote the probability set
function for assigning probability to events for outcomes of X. Two events that
occur only simultaneously are called equivalent events, where the fundamental
implication of the term is that the probabilities of the events are equivalent or
2.2
Univariate Random Variables and Density Functions
47

the same. The event B in S that is equivalent to event A in RðXÞcan be deﬁned as
B ¼ {w:X(w) ∈A, w ∈S}, which is the set of inverse images of the elements of A
deﬁned by the function X. By deﬁnition, w ∈B , x ∈A, and thus A and B are
equivalent events (see Figure 2.2). It is clear that in order for two equivalent
events to represent different sets of outcomes, they must reside in different
probability spaces – if they resided in the same probability space, they could
occur only simultaneously iff they were the same event.
Deﬁnition 2.3
Equivalent Events
Let S1 and S2 be different sample spaces. If A  S1 occurs iff B  S2 occurs,
then A and B are said to be equivalent events.
Based on the preceding discussion, we have the following representation of
probability assignments to events involving random variable outcomes:
PXðAÞ  PðBÞ for B ¼ w : XðwÞ 2 A; w 2 S
f
g:
Thus, probabilities assigned to events in S are transferred to events in R(X)
through the functional relationship x ¼ X(w), which relates outcomes w in S to
outcomes x in R(X). Note, this underscores a fundamental difference between
ordinary real-valued functions and random variables, which are also real-
valued functions. In particular, random variables are deﬁned on a domain,
S, that belongs to a probability space, {S, ϒ, P}, and thus the random variable
function not only maps domain elements into image elements x 2 RðXÞ ,
but it also maps probabilities of events in S to events in RðXÞ. An ordinary
real-valued function does only the former mapping, and its domain does not
reside in a probability space, and thus there is no simultaneous probability
mapping.
What is the domain of PXðÞ, i.e., what is the event space for X? It is clear
from the foregoing discussion that to be able to assign probabilities to a set
A  R(X) it must be the case that its associated inverse image in S, B ¼
{w : X(w) ∈A, w ∈S}, can be assigned probability based on the known probabil-
ity space {S, ϒ, P}. If not, there is no basis for assigning probability to either sets
B or A from knowledge of the probability space {S, ϒ, P}. No difﬁculty will arise
if S is a ﬁnite or countably inﬁnite sample space, since then the event space ϒ
equals the collection of all subsets of S, and whatever subset B  S is
associated with the subset A  R(X), B can be assigned probability. Thus, any
real-valued function deﬁned on a discrete sample space will generate a
real-valued sample space for which all subsets can be assigned probability.
A⊂R(X )⊂
B={w:X(w)∈A,w∈S}
Sample Space
S
B
[
]
A
Figure 2.2
Event equivalence: event A
and associated inverse
image, B, for X.
48
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Henceforth, the event space, ϒX, for outcomes of random variables deﬁned on
ﬁnite or countably inﬁnite sample spaces is deﬁned to be the set of all subsets
of R(X).
In order to avoid problems that might occur when S is uncountably inﬁnite,
one can simply restrict the types of real-valued functions that are used to deﬁne
random variables to those for which the problem will not occur. To this effect, a
proviso is generally added, either explicitly or implicitly, to the deﬁnition of a
random variable X requiring the real-valued function deﬁned on S to be such that
for every Borel set, A, contained in R(X), the set B ¼ {w: X(w) ∈A, w ∈S} is an
event in S that can be assigned probability (which is to say, it is measureable in
terms of probability). Then, since every Borel set A  R(X) would be associated
with an event B  S, every Borel set could be assigned a probability as PX(A) ¼ P(B).
Since the collection of Borel sets includes all intervals in R(X) (and thus all
points in R(X)), as well as all other sets that can be formed from the intervals
by a countable number of union, intersection, and/or complement operations,
the collection of Borel sets deﬁnes an event space sufﬁciently large for all real
world applications.
In practice, it requires a great deal of ingenuity to deﬁne a random variable
for which probability cannot be associated with each of the Borel sets in R(X),
and the types of functions that naturally arise when deﬁning random variables in
actual applications will generally satisfy the aforementioned proviso. Hence-
forth, we will assume that the event space, ϒX, for random variable outcomes
consists of all Borel sets in R(X) if R(X) is uncountable. We add that for all
practical purposes, the reader need not even unduly worry about the latter
restriction to Borel sets, since any subset of an uncountable R(X) that is of
practical interest will be a Borel set.
In summary, a random variable induces an alternative probability space for
the experiment. The induced probability space takes the form {R(X), ϒX, PX}
where the range of the random variable R(X) is the real-valued sample space, ϒX
is the event space for random variable outcomes, and PX is a probability set
function deﬁned on the events in ϒX. The relationship between the original and
induced probability spaces associated with a random variable is summarized in
Table 2.1.
Table 2.1
Relationship Between Original and X-Induced Probability Spaces
Probability
space
Random variable
X: S!R
Induced probability space
{S, ϒ, P(∙)}
x ¼ X(w)
RðXÞ ¼ x : x ¼ XðwÞ; w 2 S
f
g
ϒX ¼ A : A is an event in RðXÞ
f
g
PXðAÞ ¼ PðBÞ; B ¼ w : XðwÞ 2 A; w 2 S
f
g; 8A 2 ϒX
8
>
<
>
:
9
>
=
>
;
2.2
Univariate Random Variables and Density Functions
49

Example 2.1
An Induced Probability
Space
Let S ¼ {1, 2, 3,. . .,10} represent the potential number of cars that a car salesper-
son sells in a given week, let the event space ϒ be the set of all subsets of S, and
let the probability set function be deﬁned as P(B) ¼
1=55
ð
ÞP
w2B w for B ∈ϒ.
Suppose the salesperson’s weekly pay consists of a base salary of $100/week plus
a $100 commission for each car sold. The salesperson’s weekly pay can be
represented by the random variable X(w) ¼ 100 þ 100w, for w ∈S. The induced
probability space {R(X),ϒX, PX} is then characterized by R(X) ¼ {200, 300,
400,. . .,1100}, ϒX ¼ {A: A  R(X)}, and
PX ðAÞ ¼ 1=55
ð
Þ P
w2B w
for B ¼ {w:
(100 þ 100w) ∈A, w ∈S} and A ∈ϒX. Then, for example, the event that the
salesperson makes
 $300/week, A ¼ {200, 300}, has probability PX ðAÞ ¼
1=55
ð
Þ P
w2f1;2g w ¼ 3=55
ð
Þ.
□
A major advantage in dealing with only real-valued sample spaces is that all
of the mathematical tools developed for the real number system are available
when analyzing the sample spaces. In practice, once the induced probability
space has been identiﬁed, the underlying probability space {S, ϒ, P} is generally
ignored for purposes of deﬁning random variable events and their probabilities.
In fact, we will most often choose to deal with the induced probability space
{R(X),ϒX,PX} directly at the outset of an experiment, paying little attention to the
underlying deﬁnition of the function having the range R(X) or to the original
probability space {S, ϒ, P}. However, we will sometimes need to return to the
formal relationship between {S, ϒ, P} and {R(X),ϒX,PX} to facilitate the proofs of
certain propositions relating to random variable properties.
Note for future reference that a real-valued function of a random variable is,
itself, a random variable. This follows by deﬁnition, since a real-valued function
of a random variable, say Y deﬁned by y ¼ Y(X(w)) for w ∈S, is a function of a
function (i.e., a composition of functions) of the elements in a sample space S,
which is then indirectly also a real-valued function of the elements in the
sample space S. One might refer to such a random variable as a composite
random variable.
2.2.2
Discrete Random Variables and Probability Density Functions
In practice, it is useful to have a representation of the probability set function, PX
that is in the form of a well-deﬁned algebraic formula and that does not require
constant reference either to events in S or to the probability set function deﬁned
on the events in S. A conceptually straightforward way of representing PX is
available when the real-valued sample space R(X) contains, at most, a countable
number of elements. In this case, any subset of R(X) can be represented as the
union of the speciﬁc elements comprising the subset, i.e., if A  R(X) then A ¼
[x2A{x}. Since the elementary events in A are clearly disjoint, we know from
Axiom 1.3 that PX(A) ¼ P
x2A PX fxg
ð
Þ: It follows that once we know the proba-
bility of every elementary event in R(X), we can assign probability to any other
event in R(X) by summing the probabilities of the elementary events contained
in the event. This suggests that we deﬁne a point function f:R(X)!R as f(x) 
probability of x ¼ PX fxg
ð
Þ8x ∈R(X). Once f is deﬁned, then PX can be deﬁned for
50
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

all events as PX(A) ¼ P
x2A fðxÞ. Furthermore, knowledge of fðxÞeliminates the
need for any further reference to the probability space {S, ϒ, P} for assigning
probabilities to events in R(X).
In the following example we illustrate the speciﬁcation of the point function, f.
Example 2.2
Assigning Probabilities
with a Point Function
Examine the experiment of rolling a pair of dice and observing the number of
dots facing up on each die. Assume the dice are fair. Letting i and j represent the
number of dots facing up on each die, respectively, the sample space for the
experiment is S ¼
i; j
ð
Þ : i and j 2 1; 2; 3; 4; 5; 6
f
g
f
g. Now deﬁne the random vari-
able x ¼ X
i; j
ð
Þ
ð
Þ ¼ i þ j for i; j
ð
Þ ∈S. Then the following correspondence can be
set up between outcomes of X, events in S, and the probability of outcomes of X
and events in S, where w ¼
i; j
ð
Þ:
X(w) ¼ x
Bx ¼ {w : X(w) ¼ x,w ∈S}
f(x) ¼ P(Bx)
R(X)
2
{(1,1)}
1/36
3
{(1,2), (2,1)}
2/36
4
{(1,3), (2,2), (3,1)}
3/36
5
{(1,4), (2,3), (3,2), (4,1)}
4/36
6
{(1,5), (2,4), (3,3), (4,2), (5,1)}
5/36
7
{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)}
6/36
8
{(2,6), (3,5), (4,4), (5,3), (6,2)}
5/36
9
{(3,6), (4,5), (5,4), (6,3)}
4/36
10
{(4,6), (5,5), (6,4)}
3/36
11
{(5,6), (6,5)}
2/36
12
{(6,6)}
1/36
The range of the random variable is R(X) ¼ {2, 3,. . .,12}, which represents the
collection of images of the points i; j
ð
Þ 2 S generated by the function x ¼ X
i; j
ð
Þ
ð
Þ
¼ i þ j. Probabilities of the various outcomes of X are given by f(x) ¼ P(Bx), where
Bx is the collection of inverse images of x.
If we desired the probability of the event that x ∈A ¼ {7, 11}, then PX(A) ¼
P
x2A fðxÞ ¼ f 7
ð Þ þ f 11
ð
Þ ¼ 8=36 (which, incidentally, is the probability of win-
ning a game of craps on the ﬁrst roll of the dice). If A ¼ {2}, the singleton set
representing “snake eyes,” we ﬁnd that PX(A) ¼ P
x2A fðxÞ ¼ f 2
ð Þ ¼ 1=36.
□
In examining the outcomes of X and their respective probabilities in Exam-
ple 2.2, it is recognized that a compact algebraic speciﬁcation can be suggested
for f(x), namely1 fðxÞ ¼ 6  jx  7j
ð
Þ=36 If2;3;:::;12g ðxÞ. It is generally desirable to
express the relationship between the domain and image elements of a function
1Notice that the algebraic speciﬁcation faithfully represents the positive values of f(x) in the preceding table of values, and deﬁnes f(x)
to equal 0 8 x =2{2, 3,. . .,12}. Thus, the domain of f is the entire real line. The reason for extending the domain of f from R(X) to R will be
discussed shortly. Note that assignments of probabilities to events as PXðAÞ ¼ P
x2A fðxÞ are unaffected by this domain extension.
2.2
Univariate Random Variables and Density Functions
51

in a compact algebraic formula whenever possible, as opposed to expressing the
relationship in tabular form as in Example 2.2. This is especially true if the
number of elements in R(X) is large. Of course, if the number of elements in the
domain is inﬁnite, the relationship cannot be represented in tabular form and
must be expressed algebraically. The reader is asked to deﬁne an appropriate
point function f for representing probabilities of the elementary events in the
sample space R(X) of Example 2.1.
We emphasize that if the outcomes of the random variable X are the
outcomes of fundamental interest in a given experimental situation, then
given that a probability set function, PX(A) ¼ P
x2A fðxÞ, has been deﬁned on
the events in R(X), the original probability space {S, ϒ, P} is no longer needed for
deﬁning probabilities of events in R(X). Note that in Example 2.2, given f(x), the
probability set function PX(A) ¼ P
x2A fðxÞ can be used to deﬁne probabilities for
all events A  R(X) without reference to {S, ϒ, P}.
The next example illustrates a case where an experiment is analyzed exclu-
sively in terms of the probability space relating to random variable outcomes.
Example 2.3
Probability Set
Function Deﬁnition
via Point Function
The Bippo Lighter Co. manufactures a Piezo gas BBQ grill lighter that has a .90
probability of lighting the grill on any given attempt to use the lighter. The
probability that it lights on a given trial is independent of what occurs on any
other trial. Deﬁne the probability space for the experiment of observing the
number of ignition trials required to obtain the ﬁrst light. What is the probability
that the lighter lights the grill in three or fewer trials?
Answer: The range of the random variable, or equivalently the real-valued
sample space, can be speciﬁed as R(X) ¼ {1, 2, 3,. . .}. Since R(X) is countable,
the event space ϒX will be deﬁned as the set of all subsets of R(X). The probabil-
ity that the lighter lights the grill on the ﬁrst attempt is clearly .90, and so
f(1) ¼ .90. Using independence of events, the probability it lights for the ﬁrst
time on the second trial is (.10) (.90) ¼ .09, on the third trial is (.10)2 (.90) ¼ .009,
on the fourth trial is (.10)3 (.90) ¼ .0009, and so on. In general, the probability
that it takes x trials to obtain the ﬁrst light is f(x) ¼ (.10)x1 .90 I{1,2,3. . .}(x). Then
the probability set function is given by PX(A) ¼ P
x2A :10
ð
Þx1:90If1;2;3;:::gðxÞ: The
event that the lighter lights the grill in three trials or less is represented by
A ¼ {1, 2, 3}. Then PX(A) ¼ P3
x¼1 :10
ð
Þx1:90 ¼ :999.
□
The preceding examples illustrate the concept of a discrete random variable
and a discrete probability density function, which we formalize in the following
deﬁnitions.
Deﬁnition 2.4
Discrete Random
Variable
A random variable is called discrete if its range consists of a countable
number of elements.
Deﬁnition 2.5
Discrete Probability
Density Function
The discrete probability density function, f, is deﬁned as f(x)  probability
of x, 8x ∈R(X), and f(x) ¼ 0, 8x =2 R(X).
52
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Note that in the case of discrete random variables, some authors refer to fðxÞ
as a probability mass function as opposed to a discrete probability density
function. We will continue to use the latter terminology.
It should be noted that even though there is only a countable number of
elements in the range of the discrete random variable, X, the probability density
function (PDF) deﬁned here has the entire (uncountable) real line for its domain.
The value of f at a point x in the range of the random variable is the probability of
x, while the value of f is zero at all other points on the real line. This deﬁnition is
adopted for the sake of mathematical convenience – it standardizes the domain
of all discrete density functions to be the real line while having no effect on the
assignment of event probabilities made via the set function PXðAÞ ¼ P
x2A fðxÞ.
This convention will provides a considerable simpliﬁcation in the deﬁnition of
marginal and conditional density functions which we will examine ahead.
In our previous examples, the probability space for the experiment was a
priori deducible under the stated assumptions of the problems. It is most often
the case in practice that the probability space is not a priori deducible, and an
important problem in statistical inference is the identiﬁcation of the appropriate
density function, f(x), to use in deﬁning the probability set function component
of the probability space.
2.2.3
Continuous Random Variables and Probability Density Functions
So far, our discussion concerning the representation of PX in terms of the point
function, f(x), is applicable only to those random variables that have a countable
number of possible outcomes. Can PX be similarly represented when the range of X
is uncountably inﬁnite? Given that we can have an event A deﬁned as an uncount-
able subset of R(X), it is clear that the summation operation over the elements of
the set, (i.e., P
x2A ) is not generally deﬁned. Thus, deﬁning a probability set
function on the events in R(X) as P(A) ¼ P
x2A fðxÞ will not be possible. However,
integration over uncountable sets is possible, suggesting that the probability set
function might be deﬁned as P(A) ¼
Ð
x2A fðxÞdxwhen R(X) is uncountably inﬁnite.
In this case the point function f(x) would be deﬁned so that Ð
x2A fðxÞ dx deﬁnes the
probability of the event A. The following example illustrates the speciﬁcation of
such a point function f(x) when R(X) is uncountably inﬁnite.
Example 2.4
Probabilities by
Integrating a Point
Function
Suppose a trucking company has observed that accidents are equally likely to
occur on a certain 10-mile stretch of highway, beginning at point 0 and ending at
point 10. Let R(X) ¼ [0, 10] deﬁne the real-valued sample space of potential
accident points.
A
a
b
10
0
2.2
Univariate Random Variables and Density Functions
53

It is clear that given all points are equally likely, the probability set function
should assign probabilities to intervals of highway, say A, in such a way that the
probability of an accident is equal to the proportion of the total highway length
represented by the stretch of highway, A, as
PXðAÞ ¼ length of A
10
¼ b  a
10
;
for A ¼ a; b
½
:
If we wish to assign these probabilities using PX(A) ¼
Ð
x2A fðxÞdx, we require
that
Ð b
a fðxÞdx  ba
10 for all 0  a  b  10:The following lemma will be useful in
deriving the explicit functional form of f(x):
Lemma 2.1
Fundamental Theorem
of Calculus
Let
f(x)
be
a
continuous
function
at
b
and
a,
respectively.2
Then
@ Ð b
a fðxÞdx
@b
¼ fðbÞ and
@ Ð b
a fðxÞdx
@a
¼ fðaÞ.
Applying the lemma to the preceding integral identity yields
@
Ð b
a fðxÞdx
@b
¼ fðbÞ  @ ba
10


@b
¼ 1
10
8 b 2 ½0; 10;
which implies that the function deﬁned by f(x) ¼ .1 I[0,10](x) can be used to deﬁne
the probability set function PX(A) ¼
Ð
x2A .1 dx, for A ∈ϒX. For an example of the
use of this representation, the probability that an accident occurs in the ﬁrst half
of the stretch of highway, i.e., the probability of the event A ¼ [0, 5], is given by
PXðAÞ ¼
Ð 5
0 :1dx ¼ :5.
□
The preceding example illustrates the concept of a continuous random
variable and a continuous probability density function, which we formalize in
the next deﬁnition.
Deﬁnition 2.6
Continuous Random
Variables and
Continuous Probability
Density Functions
A random variable is called continuous if (1) its range is uncountably inﬁnite,
and (2) there exists a nonnegative-valued function f(x), deﬁned for all
x ∈(1, 1), such that for any event A  R(X), PX(A) ¼
Ð
x2A fðxÞdx, and
f(x) ¼ 0 8 x =2 R(X). The function f(x) is called a continuous probability density
function.
Clariﬁcation of a number of important characteristics of continuous random
variables is warranted. First of all, note that probability in the case of a
2See F.S. Woods (1954) Advanced Calculus, Boston: Ginn and Co., p. 141. Regarding continuity of f(x), note that f(x) is continuous at a
point d ∈D(f) if, 8e > 0, ∃a number d(e) > 0 such that if |x  d| < d(e), then f(x)  f(d) < e. The function f is continuous if it is
continuous at every point in its domain. Heuristically, a function will be continuous if there are no breaks in the graph of y ¼ f(x). Put
another way, if the graph of y ¼ f(x) can be completely drawn without ever lifting a pencil from the graph paper, then f is a continuous
function.
54
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

continuous random variable is represented by the area under the graph of the
density function f and above the points in the set A, as illustrated in Figure 2.3.
Of course, the event in question need not be an interval, but given our
convention regarding the event space ϒX, the event will be a Borel set for
which an integral can be deﬁned. A justiﬁcation for the existence of the integral
for Borel sets is beyond the scope of this text, but implementation of
the integration process in these cases is both natural and straightforward.3
The next example illustrates the procedure of determining probabilities for
events more complicated than a single interval.
Example 2.5
Probabilities
for Non-Continuous
Events
Reexamine the highway accident example (Example 2.4) where R(X) ¼ [0,10]
and f(x) ¼ .1 I[0,10](x).
a. What is the probability of A ¼ [1,2] [ [7, 9]? The probability of A is given by
the area above the points in A and below the graph of f, i.e.,
PX ðAÞ ¼
ð
x2A
fðxÞdx ¼
ð2
1
1
10


dx þ
ð9
7
1
10


dx ¼ :1 þ :2 ¼ :3
b. Given A deﬁned above, what is the probability of A ¼ [0, 1) [ (2, 7) [ (9, 10]?
The area representing the probability in question is calculated as
PX ðAÞ ¼
ð
x2A
fðxÞdx ¼
ð1
0
1
10


dx þ
ð7
2
1
10


dx þ
ð10
9
1
10


dx
¼ :1 þ :5 þ :1 ¼ :7:
□
A consequence of the deﬁnition of the probability set function PX in Deﬁnition
2.6 is that, for a continuous random variable, the probability of any elementary
event is zero, i.e., if A ¼ {a}, then PXðAÞ ¼
Ð a
a fðxÞdx ¼ 0. Note this certainly does
y
y=f(x)
PX (A) = ∫a f (x) dx
x
A={x : a ≤ x ≤ b}
b
a
b
Figure 2.3
Probability represented
as area.
3It can be shown that Borel sets are representable as the union of a collection of disjoint intervals, some of which may be single points.
The collective area in question can then be deﬁned as the sum of the areas lying above the various intervals and below the graph of f.
2.2
Univariate Random Variables and Density Functions
55

not imply that every outcome of X in R(X) is impossible, since some elementary
event in R(X) will occur as a result of a given experiment. Instead, PX({x}) ¼ 0 8x
∈R(X) suggests that zero probability is not synonymous with impossibility. In
cases where an event, say A, can occur, but the probability set function assigns the
event the value zero, we say event A occurs with probability zero. The reader might
intuitively interpret this to mean that event A is relatively impossible, i.e., relative
to the other outcomes that can occur (R(X)  A), the likelihood that Awould occur
is essentially nil. Note that the above argument together with Theorem 1.1 then
suggest that if PX(A) ¼ 1 for a continuous random variable, it does not follow that
event A is certain to occur. In the spirit of our preceding discussion, if event A is
assigned a probability of 1, we say event A occurs with probability 1, and if in
addition A 6¼ R(X), we might interpret thisto meanthat event A is relatively certain.
Note that an important implication of the preceding property for continuous
random variables, which has already been utilized in Example 2.5b, is
that the sets [a,b], (a,b], [a,b), and (a,b) are all assigned the same probability
value
Ð b
a fðxÞdx since adding or removing a ﬁnite number of elementary events to
another event will be adding or removing a collection of outcomes that occur
with probability zero. That is, since [a,b] ¼ (a,b] [ {a} ¼ [a,b) [ {b} ¼ (a,b) [ {a} [
{b}, and since PX({a}) ¼ PX({b}) ¼ 0, Axiom 1.3 implies that PX([a,b]) ¼ PX((a,b])
¼ PX([a,b)) ¼ PX((a,b)), so that the integral
Ð b
a fðxÞdx
sufﬁces to assign the
appropriate probability to all four interval events.
There is a fundamental difference in the interpretation of the image value f(x)
depending on whether f is a discrete or continuous PDF. In particular, while f(x) is
the probability of the outcome x in the discrete case, f(x) is not the probability of x
in the continuous case. To motivate this latter point, recognize that if f(x) were
the probability of outcome x in the continuous case, then by our argument above,
f(x) ¼ 0 8 x ∈R(X) since the probability of elementary events are zero. But
this would imply that for every event A, including the certain event R(X),
PX(A) ¼
Ð
x2A fðxÞdx ¼
Ð
x2A 0dx ¼ 0, since having an integrand of 0 ensures that
the integral has a zero value. The preceding property would contradict the
interpretation of PX as a probability set function, and so f(x) is clearly not inter-
pretable as a probability. It is interpretable as a density function value, but
nothing more – the continuous PDF must be integrated to deﬁne probabilities.
As in the discrete case, a continuous PDF has the entire real line for its
domain. Again, this convention is adopted for the sake of mathematical conve-
nience, as it standardizes the domain of all continuous density functions while
leaving probabilities of events unaffected. It also simpliﬁes the deﬁnition of
marginal and conditional probability density functions, which we will examine
ahead. We now provide another example of a continuous random variable
together with its density function, where the latter, we will assume, has been
discovered by your personnel department.
Example 2.6
Probabilities of Lower
and Upper Bounded
Events
Examine the experiment of observing the amount of time that passes between
employee work-related injuries at a metal fabricating plant. Let R(X) ¼ {x : x  0}
represent the potential outcomes of the experiment measured in hours, and let
the density of the continuous random variable be given by
56
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

fðxÞ ¼
1
100 ex=100 Ið0;1Þ ðxÞ:
(a) What is the probability of the event that 100 or more hours pass between
work related injuries? Letting A ¼ {x:x100} represent the event in question,
PðAÞ ¼
ð1
100
1
100 ex=100 dx ¼  ex=100 j1
100 ¼  e1=100 þ e1 ¼ e1 ¼ :37:
(b) What is the probability that an injury occurs within 50 hours of the previous
injury? Letting B ¼ {x : 0  x  50} represent the event in question,
PðBÞ ¼
ð50
0
1
100 ex=100 dx ¼  ex=100 j50
0 ¼  e50=100 þ e0 ¼ 1  :61 ¼ :39:
□
2.2.4
Classes of Discrete and Continuous PDFs
In our later study of statistical inference, we will generally identify an appropriate
range for a random variable based on the characteristics of a particular experi-
ment being analyzed and have as an objective the identiﬁcation of an appropriate
f(x) with which to complete the speciﬁcation of the probability space. The fact
that for all events A  R(X) the values generated by P
x2A fðxÞor
Ð
x2A fðxÞdx must
adhere to the probability axioms places some general restrictions on the types of
functions that can be used as density functions, regardless of the speciﬁc
characteristics of a given experiment. These general restrictions on the admissi-
ble choices of f(x) are identiﬁed in the following deﬁnition.
Deﬁnition 2.7
The Classes of Discrete
and Continuous
Probability Density
Functions: Univariate
Case
a. Class of Discrete Density Functions. The function f : R ! R is a member
of the class of discrete density functions iff (1) the set C ¼ {x:f(x) > 0,
x ∈R} (i.e., the subset of points in R having a positive image under f) is
countable, (2) f(x) ¼ 0 for x ∈C, and (3) P
x2C fðxÞ ¼ 1.
b. Class of Continuous Density Functions. The function f : R ! R is a
member of the class of continuous density functions iff (1) f(x)  0 for
x ∈(  1,1), and (2)
Ð 1
1 fðxÞdx ¼ 1.
Note for future reference that the set of outcomes for which the PDF of a
random variable assigns positive density weightings, i.e.,
x : fðxÞ>0; x 2 R
f
g, is
called the support of the random variable.
Deﬁnition 2.8
Support of a Random
Variable
The set x : fðxÞ>0; x 2 R
f
g is called the support of the random variable.
Thus, in Deﬁnition 2.7, the set C is the support of the discrete random
variable X when fðxÞ is the PDF of X. We will henceforth adopt the convention
that the range of a random variable is synonymous with its support. This
2.2
Univariate Random Variables and Density Functions
57

simply implies that any value of X for which fðxÞ ¼ 0(probability zero in the
discrete case, and probability density equal to zero in the continuous case) is
not part of the range RðXÞ, and is thus not considered a relevant outcome of the
random variable.4 We formalize this equivalence in the following deﬁnition.
Deﬁnition 2.9
Support and Range
Equivalence
RðXÞ  x : fðxÞ>0 for x 2 R
f
g
Some clarifying remarks concerning Deﬁnition 2.7 are warranted. First, it
should be noted that the deﬁnition simply identiﬁes the respective classes of
function speciﬁcations that are candidates for use as PDFs. The speciﬁc func-
tional form of the density function appropriate for a real-world experimental
situation depends on the particular characteristics of the process generating the
outcomes of the experiment.
A second observation concerns the fact that the deﬁnitions focus exclu-
sively on real-valued functions having the entire real line for their domains.
As we discussed earlier, this is a convention adopted as a matter of mathematical
convenience. To ensure that subsets of points outside of the range of X are
properly assigned zero probability, all one needs to do is to extend the domain
of f to the remaining points R  R(X) on the real line by assigning to each point a
zero density weighting, i.e., f(x) ¼ 0 if x ∈RðXÞ.
A ﬁnal remark concerns the rationale in support of the properties that
are required for a function f to be considered a PDF. The properties are imposed
on f: R ! R to ensure that the set functions constructed from f, i.e., PX(A) ¼
P
x2A f(x) or PX(A) ¼
Ð
x2A f(x)dx, are in fact probability set functions, which of
course requires that the probability assignments adhere to the axioms of prob-
ability. To motivate the sufﬁciency of these conditions, ﬁrst examine the
discrete case. Since f(x)  0 8x, PX(A) ¼ P
x2A fðxÞ  0 for any event, A, and
Axiom 1.1 is satisﬁed. Letting R(X) equal the set C deﬁned in Deﬁnition 2.7.a,
it follows that PX(R(X)) ¼ P
x2RðXÞ fðxÞ ¼ 1, satisfying Axiom 1.2. Finally, if
[i2IAi is the union of a collection of disjoint events indexed by the index set I,
then summing over all of the elementary events in A ¼ [i2I Ai obtains
PX [i2IAi
ð
Þ ¼ P
x2A fðxÞ ¼ P
i2I
P
x2Ai fðxÞ


¼ P
i2I PX Ai
ð
Þ . Satisfying Axiom
1.3. Thus, the three probability axioms are satisﬁed, and PX is a probability
set function.
To motivate sufﬁciency in the continuous case, ﬁrst note that Axiom 1.1 is
satisﬁed since if f(x)  0 8x, then PX(A) ¼
Ð
x2A f(x)dx  0 because integrating a
4Note that in the discrete case, it is conceptually possible to deﬁne a random variable that has an outcome that occurs with zero
probability. For example, if fðyÞ ¼ I 0;1
½
ðyÞis the density function of the continuous random variable Y, then X ¼ I 0;1
½
ÞðYÞis a discrete
random variable that takes the value 1 with probability 1 and the value 0 with probability zero. Such random variables have little use
in applications, and for simplicity, we suppress this possibility in making the range of the random variable synonymous with its
support.
58
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

nonnegative integrand over any interval (or Borel) set, A, results in a nonnegative
number. Furthermore, since
Ð 1
1 fðxÞdx ¼ 1, there exists at least one event A 
(1,1) such that
Ð
x2A fðxÞdx ¼ 1(the event can be (1,1) itself, or else there may
be some other partition of (1,1) into A [ B such that
Ð
x2A fðxÞdx ¼ 1 and
Ð
x2B fðxÞdx ¼ 0). Letting R(X) ¼ A, we have that PX(R(X)) ¼
Ð
x2RðXÞ fðxÞdx ¼ 1 and
Axiom 1.2 is satisﬁed. Finally, if D ¼ [i2IAi is the union of a collection of disjoint
events indexed by the index set I, then by the additive property of integrals, PXðDÞ
¼
Ð
x2D fðxÞdx ¼ P
i2I
Ð
x2Ai fðxÞdx


¼ P
i2I PX Ai
ð
Þ satisfying Axiom 1.3. Thus,
the three probability axioms are satisﬁed, and PX is a probability set function.
It can also be shown that the function properties presented in Deﬁnition 2.7
are actually necessary for the discrete case and practically necessary in the
continuous case. For the discrete case, ﬁrst recall that f(x) is directly interpret-
able as the probability of the outcome x, and this requires that f(x) 0 8x ∈ℝ(or
else we would be assigning negative probabilities to some x’s). Second, the
number of outcomes that can receive positive probability must be countable
in the discrete case since R(X) is countable, leading to the requirement that
C ¼ {x : f(x) > 0, x ∈R} is countable. Finally, P
x2C fðxÞ ¼ 1 is required if the
probability assigned to the certain event is to equal one.
In the continuous case, it is necessary that
Ð 1
1 fðxÞdx ¼ 1. To see this, ﬁrst
note that R(X) is the certain event, implying P(R(X)) ¼ 1. Now since R(X) and
RðXÞ are disjoint, we have that PX(R(X) [ RðXÞ) ¼ PX(R(X)) þ PX(RðXÞ) ¼ 1 þ
PXðRðXÞ), which implies PX(RðXÞ) ¼ 0 since probabilities cannot exceed 1. But,
since R(X) [ RðXÞ ¼ R by deﬁnition, then P(R(X)) [ (RðXÞ) ¼
Ð 1
1 fðxÞdx ¼ 1.
Regarding the requirement that f(x)  0 for x ∈(1,1), note that the condition
is technically not necessary. It is known from the properties of integrals that the
value of Ð b
a fðxÞdx is invariant to changes in the value of f(x) at a ﬁnite number of
isolated points, and thus f(x) could technically be negative for such a ﬁnite
number of x values without affecting the values of the probability set function.
As others do, we will ignore this technical anomaly since its practical signiﬁ-
cance in deﬁning PDFs is nil. We thus insist, as a practical matter, on the
nonnegativity of f(x).
Example 2.7
Verifying Probability
Density Functions
In each case below, determine whether the stated function can serve as a PDF:
a. fðxÞ ¼ 1=2I½0;2ðxÞ:
Answer:
The function can serve as a continuous probability density func-
tion since f(x)  0 8x ∈(1,1) (note f(x) ¼ 1/2 > 0 8x ∈[0, 2] and f(x) ¼ 0 for
x=2[0, 2]), and
Ð 1
1 fðxÞdx ¼
Ð 1
1 1=2
ð
ÞI½0;2ðxÞdx ¼
Ð 2
0 1=2dx ¼ x=2j2
0 ¼ 1.
b. f(x) ¼ (.3)x(.7)1x I{0,1}(x).
Answer:
The function can serve as a discrete probability density function,
since f(x) > 0 on the countable set {0, 1}, P1
x¼0 fðxÞ ¼ 1, and f(x) ¼ 08x=2{0, 1}.
c. fðxÞ ¼ ðx2 þ 1ÞI½1;1ðxÞ:
Answer:
The function cannot serve as a PDF. While f(x)  0 8x ∈(1,1),
the function does not integrate to 1:
2.2
Univariate Random Variables and Density Functions
59

ð1
1
fðxÞdx ¼
ð1
1
ðx2 þ1Þ I½1;1 ðxÞdx
¼
ð1
1
ðx2 þ1Þdx
¼ x3
3 þ x j1
1 ¼ 8
3 6¼ 1
d. fðxÞ ¼ 3=8
ð
Þðx2 þ 1ÞI½1;1ðxÞ:
Answer:
The reader should demonstrate that this function can serve as a
continuous probability density function. Note its relationship to the func-
tion in part c.
□
2.2.5
Mixed Discrete-Continuous Random Variables
The categories of discrete and continuous random variables do not exhaust the
possible types of random variables. There is a category of random variable called
mixed discrete-continuous which exhibits the characteristics of a discrete
random variable for some events and the characteristics of a continuous random
variable for other events. In particular, a mixed discrete-continuous random vari-
able is such that a countable subset of the elementary events are assigned positive
probabilities, as in the case of a discrete random variable, except the sum of the
probabilities over the countable set does not equal 1. The remaining probability is
attributable to an uncountable collection of elementary events, each elementary
event being assigned zero probability, as in the case of a continuous random
variable. The following example illustrates the concept of a mixed discrete-
continuous random variable.
Example 2.8
Operating Life as a
Mixed Discrete-
Continuous RV
Let X be a random variable representing the length of time, measured in units of
one hundred thousand hours, that a LCD color screen for a laptop computer
operates properly until failure. Assume the probability set function associated
with the random variable is PX(A) ¼ .25 IA(0) þ .75
Ð
x2A exIð0;1ÞðxÞdx for every
event A (i.e., Borel set) contained in R(X) ¼ [0,1).
a. What is the probability that the color screen is defective, i.e., it does not
function properly at the outset?
Answer:
The event in question is A ¼ {0}. Using PX, we calculate the
probability to be PX({0}) ¼ .25 I{0}(0) þ .75
Ð
x2f exdx ¼ .25 . (Note: By deﬁni-
tion,
Ð
x2f fðxÞdx ¼ 0).
b. What is the probability that the color screen operates satisfactorily for less
than 100,000 hours?
Answer:
Here, A ¼ [0,1). Using PX, we calculate PX([0,1)) ¼ .25 I[0,1) (0) þ
.75
Ð 1
0ex dx ¼ .25 þ .474 ¼ .724.
c. What is the probability that the color screen operates satisfactorily for at
least 50,000 hours?
60
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Answer:
The event in question is A ¼ [.5,1). The probability assigned to
this
event
is
given
by
PX ([.5,1)) ¼ .25
I[.5,1)
(0) þ .75
Ð 1
:5 exdx
¼ 0 þ .4549 ¼ .4549.
□
We formalize the concept of a mixed discrete-continuous random variable in
the following deﬁnition.
Deﬁnition 2.10
Mixed Discrete-
Continuous Random
Variables
A random variable is called mixed discrete-continuous iff
a. Its range is uncountably inﬁnite;
b. There exists a countable set, C, of outcomes of X such that PX({x}) ¼
fd (x) > 0 8x ∈C, fd(x) ¼ 0 8x =2 C, and P
x2C fdðxÞ< 1, where the function
fd is referred to as the discrete density function component of the proba-
bility set function of X;
c. There exists a nonnegative-valued function, fc, deﬁned for all x ∈(1,1)
such that for every event B  R(X)  C, PX(B) ¼
Ð
x2B fcðxÞdx, f(x) ¼ 08x ∈
R  R(X), and
Ð 1
1 fcðxÞdx ¼ 1  P
x2C fdðxÞ , where the function fc is
referred to as the continuous density function component of the probabil-
ity set function of X; and
d. The probability set function for X is given by combining or mixing the
discrete and continuous density function components in (b) and (c) above,
as PX(A) ¼ P
x2A\C fdðxÞ+
Ð
x2A fcðxÞdx for every event A.
To see how the deﬁnition applies to a speciﬁc experimental situation, recall
Example 2.8. If we substitute fd(x) ¼ .25 I{0}(x), C ¼ {0}, and fc(x) ¼ .75ex I(0,1)(x)
into the deﬁnition of PX given in Deﬁnition 2.10.d, we obtain
PXðAÞ ¼
X
x2A\f0g
:25I 0
f gðxÞ


þ :75
ð
x2A
ex Ið0;1Þ ðxÞdx ¼ :25IAð0Þ þ :75
ð
x2A
exI 0;1
ð
ÞðxÞdx;
which is identical to the probability set function deﬁned in Example 2.8.
As the reader may have concluded from examining the deﬁnition, the con-
cept of a mixed discrete-continuous random variable is more complicated than
either the discrete or continuous random variable case, since there is no single
PDF that can either be summed or integrated to deﬁne probabilities of events.5
On the other hand, once the discrete and continuous random variable concepts
are understood, the notion of a mixed discrete-continuous random variable is a
rather straightforward conceptual extension. Note that the deﬁnition of the
probability set function in Deﬁnition 2.10.d essentially implies that the proba-
bility of an event A is equivalent to adding together the probabilities of the
5In a more advanced treatment of the subject, we could resort to more general integration methods, in which case a single integral
could once again be used to deﬁne PX. On Stieltjes integration, see R.G. Bartle (1976) The Elements of Real Analysis, 2nd ed.,
New York: John Wiley, and Section 3.2 of the next chapter.
2.2
Univariate Random Variables and Density Functions
61

discrete event A \ C and the continuous event A. Assigning probability to the
event A \ C is done in a way that emulates the discrete random variable case – a
real-valued function (the discrete density component) is summed over the points
in the event A \ C. The probability of the event A is calculated in a way that
emulates the continuous random variable case – a real-valued function (the
continuous density component) is integrated over the points in the event A.
Adding together the results obtained for the discrete event A \ C and
the continuous event A deﬁnes the probability of the “mixed” event A.
Note that the overlap of discrete points A \ C in the event A is immaterial
when the probability of A is assigned via the continuous PDF component since
Ð
x2A A\C
ð
Þ fcðxÞdx ¼
Ð
x2A fcðxÞdx, i.e., the integral over the countable points in
A \ C will be zero.6
2.3
Univariate Cumulative Distribution Functions
Situations arise in practice that require ﬁnding the probability that the outcome
of a random variable is less than or equal to some real number, i.e., the event in
question is {x: x  b, x ∈R(X)} for some real number b. These types of
probabilities are provided by the cumulative distribution function (CDF),
which we introduce in this section.
Henceforth, we will eliminate the random variable subscript used heretofore
in our probability set function notation; we will now write P(A) rather than
PX(A) whenever the context makes it clear to which probability space the event
A refers. Thus, the notation P(A) will be used to represent the probability of
either an event A  S or an event A  R(X). To economize on notation further, we
introduce an abbreviated set deﬁnition for representing events.
Deﬁnition 2.11
Abbreviated Set
Deﬁnition for Events
For an event {x: set deﬁning conditions, x ∈R(X)} and associated probability
represented by P({x: set deﬁning conditions, x ∈R(X)}), the abbreviated set
deﬁnition for the event and associated probability are respectively {set-
deﬁning conditions} and P(set-deﬁning conditions), the condition x ∈R(X)
always being tacitly assumed. Alternatively, S may appear in place of R(X).
For an example of an abbreviated set deﬁnition that is particularly relevant
to our current discussion of CDFs, note that {x  b} will be used to represent
{x: x  b, x ∈R(X)}, and P(x  b) will be used to represent P({x: x  b,
x ∈R(X)}).7
6There are still other types of random variables besides those we have examined, but they are rarely utilized in applied work. See T.S.
Chow and H. Teicher (1978) Probability Theory, New York: Springer-Verlag, pp. 247–248.
7Alternative shorthand notation that is often used in the literature is respectively {X  b} and P(X  b). Our notation establishes a
distinction between the function X and a value of the function x.
62
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

The formal deﬁnition of the cumulative distribution function, and its
particular algebraic representations in the discrete, continuous, and mixed
discrete-continuous cases, are given next.
Deﬁnition 2.12
Univariate Cumulative
Distribution Function
The cumulative distribution function of a random variable X is deﬁned by
F(b)  P(x  b) 8 b ∈(1,1). The functional representation of F(b) in
particular cases is as follows:
a. Discrete: F(b) ¼
P
x  b; fðxÞ>0
fðxÞ, b ∈(1,1)
b. Continuous: F(b) ¼
Ð b
1 fðxÞdx, b ∈(1,1)
c. Mixed
discrete-continuous:
FðbÞ ¼
P
x  b; fdðxÞ>0
fdðxÞ +
Ð b
1 fcðxÞdx;
b ∈(1,1).
Example 2.9
CDF for Continuous RV
Reexamine Example 2.6, where the amount of time that passes between work-
related injuries is observed. We can deﬁne the cumulative distribution function
for X as
FðbÞ ¼
ðb
1
1
100 ex=100 Ið0;1Þ ðxÞdx ¼ 1  eb=100
h
i
Ið0;1Þ ðbÞ:
If one were interested in the event that an injury occurs within 50 hours of the
previous injury, the probability would be given by
Fð50Þ ¼ ½1  e50=100Ið0;1Þð50Þ ¼ 1  :61 ¼:39:
A graph of the cumulative distribution function is given in Figure 2.4.
□
50
F(x)
0.2
0.4
0.6
0.8
1
0
100
200
300
x
Figure 2.4
A CDF for a continuous X.
2.3
Univariate Cumulative Distribution Functions
63

Example 2.10
CDF for Discrete RV
Examine the experiment of rolling a fair die and observing the number of dots
facing up. Let the random variable X represent the possible outcomes of the
experiment, so that R(X) ¼ {1, 2, 3, 4, 5, 6} and f(x) ¼ 1/6 I{1,2,3,4,5,6}(x). The
cumulative distribution function for X can be deﬁned as
FðbÞ ¼
X
x  b; fðxÞ>0
1
6 If1;2;3;4;5;6g ðxÞ ¼ 1
6 truncðbÞI 0;6
½
ðbÞ þ I 6;1
ð
ÞðbÞ;
where trunc(b) is the truncation function deﬁned by assigning to any domain
element b the number that results after truncating the decimal part of b. For
example, trunc(5.97) ¼ 5, or trunc(2.12) ¼ 2. If we were interested in the
probability of tossing a 3 or less, the probability would be given by
Fð3Þ ¼ 1
6 truncð3ÞI 0;6
½
ð3Þ þ I 6;1
ð
Þð3Þ ¼ 1
2 þ 0 ¼ 1
2 :
A graph of the cumulative distribution function is given in Figure 2.5.
□
Example 2.11
CDF for a Mixed
Discrete Continuous RV
Recall Example 2.8, where color screen lifetimes were represented by a mixed
discrete-continuous random variable. Thecumulative distributionfor X isgiven by
FðbÞ ¼ :25I½0;1ÞðbÞ þ :75
ðb
1
exI 0;1
ð
ÞðxÞdx
¼ :25I½0;1ÞðbÞ þ :75 1  eb
h
i
I 0;1
ð
ÞðbÞ:
If one were interested in the probability that the color screen functioned for
100,000 hours or less, the probability would be given by
Fð1Þ ¼ :25I½0;1Þð1Þ þ :75 1  e1

	
I 0;1
ð
Þð1Þ
¼ :25 þ :474 ¼ :724:
□
A graph of the cumulative distribution function is given in Figure 2.6.
2.3.1
CDF Properties
The graphs in the preceding examples illustrate some general properties of
CDFs. First, CDFs have the entire real line for their domain, while their range
is contained in the interval [0, 1]. Secondly, the CDF exhibits limits as
1
4/6
2/6
1
2
3
5
4
6
x
F(x)
Figure 2.5
A CDF for a discrete X.
64
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

lim
b!1 FðbÞ¼ lim
b!1 P x  b
ð
Þ ¼ P ;
ð Þ ¼ 0
and
lim
b!1 FðbÞ ¼ lim
b!1 Pðx  bÞ ¼PðRðXÞÞ ¼ 1:
It is also true that if a < b, then necessarily F(a) ¼ P(x  a)  P(x  b) ¼ F(b),
which is the deﬁning property for F to be an increasing function, i.e., if 8xi and xj
for which xi < xj, F(xi)  F(xj), F is an increasing function.8
The CDFs of discrete, continuous, and mixed discrete-continuous random
variables can be distinguished by their continuity properties and by the behavior
of F(b) on sets of domain elements for which F is continuous. The CDF of a
continuous random variable must be a continuous function on the entire real
line, as illustrated in Figure 2.4, for suppose the contrary that there existed a
discontinuous “jumping up” point at a point d. Then Pðx ¼ dÞ ¼ limb!dPðb<x
 dÞ ¼ FðdÞ  limb!dFðbÞ>0
because of the discontinuity (see Figure 2.7),
contradicting that P(x ¼ d) ¼ 0 8d if X is continuous.9
0
1
2
3
0.2
0.4
0.6
0.8
1
F(x)
x
•
Figure 2.6
A CDF for a mixed discrete-
continuous X.
8For those readers whose recollection of the limit concept from calculus courses is not clear, it sufﬁces here to appeal to intuition and
interpret the limit of F(b) as “the real number to which F(b) becomes and remains inﬁnitesimally close to as b increases without bound
(or as b decreases without bound).” We will examine the limit concept in more detail in Chapter 5.
9limb!d indicates that we are examining the limit as b approaches d from below (also called a left-hand limit). limb!dþ would indicate
the limit as b approached d from above (also called a right-hand limit). For now, it will sufﬁce for the reader to appeal to intuition and
interpret limb!d F(b) as “the real number to which F(b) becomes and remains inﬁnitesimally close to as b increases and becomes
inﬁnitesimally close to d.”
2.3
Univariate Cumulative Distribution Functions
65

The CDFs for both discrete and mixed discrete-continuous random variables
exhibit a countable number of discontinuities at “jumping up” points,
representing the assignments of positive probabilities to a countable number
of elementary events (recall Figures 2.5 and 2.6). The discrete case is distin-
guished from the mixed case by the property that the CDF in the former case is a
constant function on all intervals for which F is continuous. The mixed case will
have a CDF that is an increasing function of x on one or more interval subsets of
the real line.10
2.3.2
Duality Between CDFs and PDFs
A CDF can be used to derive a PDF as well as discrete and continuous density
components in the mixed discrete-continuous random variable case.
Theorem 2.1
Discrete PDFs from
CDFs
Let x1 < x2 < x3 < . . ., be the countable collection of outcomes in the range of
the discrete random variable X. Then the discrete PDF for X can be deﬁned as
fðx1Þ¼ Fðx1Þ;
fðxiÞ ¼ FðxiÞ  Fðxi1Þ;
i¼ 2; 3;:::;
fðxÞ ¼ 0 for x=2RðXÞ:
Proof
The proof follows directly from the deﬁnition of the CDF, and is left to the reader.n
Note, in a large number of empirical applications of discrete random
variables, the range of the random variable exhibits an identiﬁable smallest
value, x1, as in Theorem 2.1. In cases where the range of the random variable
does not have a ﬁnite smallest value, the Theorem can be restated simply as
f(xi) ¼ F(xi)  F(xi1), for xi>xi1and f(x) ¼ 0 for x =2 R(X).
d
F(x)
F(d)
P(x=d)
x
lim F (b)
bÆd -
Figure 2.7
Discontinuity in a CDF.
10A strictly increasing function has F(xi) < F(xj) when Xi < Xj.
66
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Theorem 2.2
Continuous PDFs from
CDFs
Let f(x) and F(x) represent the PDF and CDF for the continuous random variable
X. The density function for X can be deﬁned as f(x) ¼ dFðxÞ=dx wherever f(x) is
continuous, and f(x) ¼ 0 (or any nonnegative number) elsewhere.
Proof
By the fundamental theorem of the calculus (recall Lemma 2.1), it follows that
dFðxÞ
dx
¼ d
Ð x
1 fðtÞdt
dx
¼ fðxÞ
wherever f(x) is continuous, so the ﬁrst part of the theorem is demonstrated.
Now, since X is a continuous random variable, then P(x  b) ¼ F(b)¼
Ð b
1 fðxÞdx
exists 8b by deﬁnition. Changing the value of the nonnegative integrand at
points of discontinuity will have no effect on the value of F(b) ¼
Ð b
1 fðxÞdx,11 so
that f(x) can be deﬁned arbitrarily at the points of discontinuity.
n
Theorem 2.3
Density Components of
a Mixed Discrete-
Continuous Random
Variable from CDFs
Let X be a mixed discrete-continuous random variable with a CDF, F. Let
x1 < x2 < x3 < . . . be the countable collection of outcomes of X for which F(x)
is discontinuous. Then the discrete density component of X can be deﬁned as
fd(xi) ¼ F(xi)  limb!x
i F(b) for i ¼ 1, 2, 3, . . .; and fd(x) ¼ 0 (for any nonnegative
numbers) elsewhere.
The continuous density component of X can be deﬁned as fc(x) ¼ dFðxÞ=dx
wherever f(x) is continuous, and f(x) ¼ 0 (or any nonnegative number) elsewhere.
Proof
The proof is a combination of the arguments used in the proofs of the preceding
two theorems and is left to the reader.
n
Given Theorems 2.1–2.3, it follows that there is a complete duality between
CDFs and PDFs whereby either function can be derived from the other.
We illustrate Theorems 2.1–2.3 in the following examples.
Example 2.12
Deriving Discrete PDF
via Duality
Recall Example 2.10, where the outcome of rolling a fair die is observed. We can
deﬁne the discrete density function for X using the CDF for X as follows:
fð1Þ ¼ Fð1Þ ¼ 1
6
fðxÞ ¼
FðxÞ  Fðx  1Þ ¼ x
6  x  1
6
¼ 1=6 for x ¼ 2; 3; 4; 5; 6;
0 elsewhere
8
>
<
>
:
:
11This can be rigorously justiﬁed by the fact that under the conditions stated: (1) the (improper) Riemann integral is equivalent to a
Lebesque integral; (2) the largest set of points for which f(x) can be discontinuous and still have the integral Ð b
1 fðxÞdx deﬁned 8b has
“measure zero;” and (3) the values of the integrals are unaffected by changing the values of the integrand on a set of points having
“measure zero.” This result applies to multivariate integrals as well. See C.W. Burill, 1972, Measure, Integration, and Probability,
New York: McGraw-Hill, pp. 106–109, for further details.
2.3
Univariate Cumulative Distribution Functions
67

A more compact representation of f(x) can be given as f(x) ¼ 1=6 I{1,2,3,4,5,6}(x), which
we know to be the appropriate discrete density function for the case at hand.
□
Example 2.13
Deriving Continuous
PDF via Duality
Recall Example 2.9, where the time that passes between work-related injuries is
observed. We can deﬁne the continuous density function for X using the stated
CDF for X as follows:
fðxÞ ¼
dFðxÞ
dx
¼ d 1  ex=100


Ið0;1Þ ðxÞ
dx
¼
1
100 ex=100
for x 2 ð0; 1Þ
0
for x 2 ð1; 0Þ
8
<
:
The derivative of F(x) does not exist at the point x ¼ 0 (recall Figure 2.4), which is
a reﬂection of the fact that f(x) is discontinuous at x ¼ 0. We arbitrarily assign
f(x) ¼ 0 when x ¼ 0 so that the density function of x is ultimately deﬁned by
f(x) ¼ 1=100 ex/100 I(0,1)(x), which we know to be an appropriate continuous
density function for the case at hand.
□
Example 2.14
Deriving Mixed
Discrete-Continuous
PDF via Duality
Recall Example 2.11, where the operating lives of notebook color screens are
observed. The CDF of the mixed discrete-continuous random variable X is
discontinuous only at the point x ¼ 0 (recall Figure 2.6). Then the discrete
density component of X is given by
fd(0) ¼ F(0)  lim
b!0 F(b) ¼ .25  0 ¼ .25 and fd(x) ¼ 0, x 6¼ 0, or alternatively,
fdðxÞ ¼:25If0gðxÞ;
which we know to be the appropriate discrete density function component in
this case.
The continuous density function component can be deﬁned as
fc ¼
dFðxÞ
dx
¼ :75ex
for x 2 0; 1
ð
Þ;
0
for x 2 1; 0
ð
Þ;
8
<
:
but the derivative of F(x) does not exist at the point x ¼ 0 (recall Figure 2.6). We
arbitrarily assign fc(x) ¼ 0 when x ¼ 0, so that the continuous density function
component of X is ﬁnally representable as fc(x) ¼ .75 ex I(0,1)(x), which we know
to be an appropriate continuous density function component in this case.
□
2.4
Multivariate Random Variables, PDFs, and CDFs
In the preceding sections of this chapter, we have examined the concept of a
univariate random variable, where only one real-valued function was deﬁned on
the elements of a sample space. The concept of a multivariate random variable is
an extension of the univariate case, where two or more real-valued functions are
concurrently deﬁned on the elements of a given sample space. Underlying the
concept of a multivariate random variable is the notion of a real-valued vector
function, which we deﬁne now.
68
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Deﬁnition 2.13
Real-Valued Vector
Function
Let gi :A ! R, i ¼ 1,. . .,n, be a collection of n real-valued functions, where
each function is deﬁned on the domain A. Then the function g : A ! Rn
deﬁned by
y ¼
y1
:
:
:
yn
2
66664
3
77775
¼
g1 ðwÞ
:
:
:
gn ðwÞ
2
66664
3
77775
¼ gðwÞ; for w 2 A;
is called an (n-dimensional) real-valued vector function. The real-valued
functions g1,. . .,gn are called coordinate functions of the vector function g.
Note that the real-valued vector function g : A ! Rn is distinguished from the
scalar function g : A ! R by the fact that its range elements are n-dimensional
vectors of real numbers as opposed to scalar real numbers. The range of the real-
valued vector function is given by R(g) ¼ {(y1, . . .,yn): yi ¼ gi(w), i ¼ 1,. . .,n; w ∈A}.
We nowprovideaformal deﬁnition of the notion ofa multivariaterandom variable.
Deﬁnition 2.14
Multivariate (n-variate)
Random Variable
Let {S, ϒ, P} be a probability space. If X: S ! Rn is a real-valued vector function
having as its domain the elements of S, then X is called a multivariate
(n-variate) random variable.
Since the multivariate random variable is deﬁned by
x
ðn	1Þ ¼
x1
x2
:
:
:
xn
2
6666664
3
7777775
¼
X1ðwÞ
X2ðwÞ
:
:
:
XnðwÞ
2
6666664
3
7777775
¼ XðwÞ for w 2 S;
ðn	1Þ
it is admissible to interpret X as a collection of n univariate random variables,
each deﬁned on the same probability space {S, ϒ, P}. The range of the n-variate
random variable is given by R(X) ¼ {(x1,. . .,xn) : xi ¼ Xi(w), i ¼ 1,. . .,n; w ∈S}.
The multivariate random variable concept applies to any real world experi-
ment in which more than one characteristic is observed for each outcome of the
experiment. For example, upon making an observation concerning a futures
trade on the Chicago Mercantile Exchange, one could record the price, quantity,
delivery date, and commodity grade associated with the trade. Upon conducting
a poll of registered voters, one could record various political preferences and a
myriad of sociodemographic data associated with each randomly chosen inter-
viewee. Upon making a sale, a car dealership will record the price, model, year,
color, and the selections from the options list that were made by the buyer.
Deﬁnitions for the concept of discrete and continuous multivariate random
variables and their associated density functions are as follows:
2.4
Multivariate Random Variables, PDFs, and CDFs
69

Deﬁnition 2.15
Discrete Multivariate
Random Variables and
Probability Density
Functions
A multivariate random variable is called discrete if its range consists of a
countable number of elements. The discrete joint PDF, f, for a discrete multi-
variate random variable X ¼ (X1,. . .,Xn) is deﬁned as f(x1,. . .,xn)  {probability
of (x1,. . .,xn)} if (x1,. . .,xn) ∈R(X), f(x1,. . .,xn) ¼ 0 otherwise.
Deﬁnition 2.16
Continuous
Multivariate Random
Variables and
Probability Density
Functions
A multivariate random variable is called continuous if its range is
uncountably inﬁnite and there exists a nonnegative-valued function f(x1,. . .,
xn), deﬁned for all (x1,. . .,xn) ∈Rn, such that P(A) ¼
Ð
x1;...;xn
ð
Þ2A fðx1; :::; xnÞ dx1
:::dxn
for any event A  R(X), and f(x1,. . .,xn) ¼ 0
8 (x1,. . .,xn)=2R(X).
The function f(x1,. . .,xn) is called a continuous joint PDF.
2.4.1
Multivariate Random Variable Properties and Classes of PDFs
A number of properties of discrete and continuous multivariate random
variables, and their joint probability densities, can be identiﬁed through analogy
with the univariate case. In particular, the multivariate random variable induces
a new probability space, {R(X),ϒX, PX}, for the experiment. The rationale under-
lying the transition from the probability space {S, ϒ, P} to the induced probability
space {R(X), ϒX, PX} is precisely the same as in the univariate case, except for the
increased dimensionality of the elements in R(X) in the multivariate case. The
probability set function deﬁned on the events in the event space is represented in
terms of multiple summation of a PDF in the discrete case, and multiple
integration of a PDF in the continuous case. In the discrete case, f(x1,. . .,xn) is
directly interpretable as the probability of the outcome (x1,. . .,xn); in the contin-
uous case the probability of each elementary event is zero and f(x1,. . .,xn) is not
interpretable as a probability. As a matter of mathematical convenience, both
density functions are deﬁned to have the entire n-dimensional real space for
their domains, so that f(x1,. . .,xn) ¼ 0 8 x =2R(X).
Regarding the classes of functions that can be used as discrete or continuous
joint density functions, we provide the following generalization of Deﬁnition 2.5:
Deﬁnition 2.17
The Classes of Discrete
and Continuous Joint
Probability Density
Functions
a. Class of discrete joint density functions. A function f : Rn ! R is a
member of the class of discrete joint density functions iff:
1. the set C ¼ {(x1,. . .,xn):f(x1,. . .,xn) > 0, (x1,. . .,xn) ∈Rn} is countable;
2. f(x1,. . .,xn) ¼ 0 for x ∈C; and
3. P
x1;:::;xn
ð
Þ2C f x1; :::; xn
ð
Þ ¼ 1.
b. Class of continuous joint density functions. A function f : Rn ! R is a
member of the class of continuous joint density functions iff:
1. f(x1,. . .,xn)  0 8(x1,. . .,xn) ∈Rn; and
2.
Ð
x2Rn f x1; :::; xn
ð
Þdx1:::dxn ¼ 1.
70
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

The reader can generalize the arguments used in the univariate case to
demonstrate that the properties stated in Deﬁnition 2.17 are sufﬁcient, as well
as necessary in the discrete case and “almost necessary” in the continuous case,
for set functions deﬁned as
PðAÞ ¼
P
x1;...;xn
ð
Þ2A
f x1; . . . ; xn
ð
Þ
(discrete case),
Ð
x1;...;xn
ð
Þ2A
f x1; . . . ; xn
ð
Þdx1 . . . dxn
(continuous case)
8
>
<
>
:
to satisfy the probability axioms 8A ∈ϒX.
Similar to the univariate case, we deﬁne the support of a multivariate
random variable, and the equivalence of the range and support as follows.
Deﬁnition 2.18
Support of a
Multivariate Random
Variable
The set x : f x
ð Þ>0; x 2 Rn
f
gis called the support of then 	 1random variable X.
Deﬁnition 2.19
Support and Range
Equivalence of
Multivariate Random
Variables
R X
ð Þ  x : f x
ð Þ>0 for x 2 Rn
f
g
The following is an example of the speciﬁcation of the probability space for a
bivariate discrete random variable.
Example 2.15
Probability Space for a
Bivariate Discrete RV
For the experiment of rolling a pair of dice in Example 2.2, distinguish the two
die by letting the ﬁrst die be “red” and the second “green.” Thus an outcome (i,j)
refers to i dots on the red die and j dots on the green die. Deﬁne the following two
random variables: x1 ¼ X1(w) ¼ i, and x2 ¼ X2(w) ¼ i þ j
The range of the bivariate random variable (X1, X2) is given by R(X) ¼ {(x1,
x2): x1 ¼ i, x2 ¼ i þ j, i and j ∈{1,. . .,6}}. The event space is ϒX ¼ {A: A  R(X)}.
The correspondence between elementary events in R(X) and elementary events
in S is displayed as follows:
x1
1
2
3
4
5
6
2
(1,1)
3
(1,2)
(2,1)
4
(1,3)
(2,2)
(3,1)
5
(1,4)
(2,3)
(3,2)
(4,1)
6
(1,5)
(2,4)
(3,3)
(4,2)
(5,1)
x2
7
(1,6)
(2,5)
(3,4)
(4,3)
(5,2)
(6,1)
Elementary events in S
8
(2,6)
(3,5)
(4,4)
(5,3)
(6,2)
9
(3,6)
(4,5)
(5,4)
(6,3)
10
(4,6)
(5,5)
(6,4)
11
(5,6)
(6,5)
12
(6,6)
2.4
Multivariate Random Variables, PDFs, and CDFs
71

It follows immediately from the correspondence with the probability space
{S, ϒ, P} that the discrete density function for the bivariate random variable (X1,
X2) can be represented as
fðx1; x2Þ ¼ 1
36 If1;:::;6g ðx1Þ If1;:::;6g ðx2  x1Þ;
and the probability set function deﬁned on the events in R(X) is then
PðAÞ ¼
X
x1;x2
ð
Þ2A
fðx1; x2Þ for A 2 ϒX:
Let A ¼ {(x1,x2): 1  x1  2, 2  x2  5, (x1, x2) ∈R(X)}, which is the event of
rolling 2 or less on the red die and a total of 5 or less on the pair of dice. Then the
probability of this event is given by
PðAÞ ¼
X
x1;x2
ð
Þ2A
fðx1; x2Þ ¼ S
2
x1¼1
S
5
x2¼x1 þ1 fðx1; x2Þ ¼ 7
36:
□
The preceding example illustrates two general characteristics of the multivar-
iate random variable concept that should be noted. First of all, even though a
multivariate random variable can be viewed as a collection of univariate random
variables, it is not necessarily the case that the range of the multivariate
random variable X equals the Cartesian product of the ranges of the univariate
random variable deﬁning X. Depending on the deﬁnition of the Xi’s, either R X
ð Þ
6¼ 	n
i¼1R Xi
ð
Þ and R X
ð Þ  	n
i¼1R Xi
ð
Þ, or R X
ð Þ ¼ 	n
i¼1R Xi
ð
Þ is possible. Example 2.15
is an example of the former case, where a number of scalar outcomes that are
individually possible for the univariate random variables X1 and X2 are not simul-
taneously possible as outcomes for the bivariate random variable (X1, X2). Sec-
ondly, note that our convention of deﬁning f(x1, x2) ¼ 0 8 (x1, x2) =2 R(X) allows an
alternative summation expression for deﬁning the probability of event A in Exam-
ple 2.15:
PðAÞ ¼ S
2
x1¼1
S
5
x2¼2 fðx1; x2Þ ¼ 7
36:
We have included the point (2,2) in the summation above, which is an impossi-
ble event – we cannot roll a 2 on the red die and a total of 2 on the pair of dice, so
that (2,2) =2 R(X). Nonetheless, the probability assigned to A is correct since
f(2,2) ¼ 0 by deﬁnition. In general, when deﬁning the probability of an event A
for an n-dimensional discrete random variable X, f(x1,. . .,xn) can be summed over
the points identiﬁed in the set-deﬁning conditions for A without regard for the
condition that x ∈R(X), since any x =2R(X) will be such that f(x1,. . .,xn) ¼ 0, and
the value of the summation will be left unaltered. This approach can be espe-
cially convenient if set A is deﬁned by individual, independent set-deﬁning
conditions applied to each Xi in an n-dimensional random variable (X1,. . .,Xn),
as in the preceding example. An analogous argument applies to the continuous
case, with integration replacing summation.
We now present an example of the speciﬁcation of the probability space for a
bivariate continuous random variable.
72
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Example 2.16
Probability Space for a
Bivariate
Continuous RV
Your company manufactures big-screen television sets. The screens are 3 ft high
by 4 ft wide rectangles that must be coated with a metallic reﬂective coating (see
Figure 2.8). The machine that is coating the screens begins to randomly produce
a coating ﬂaw at a point on the screen surface, where all points on the screen are
equally likely to be the point of the ﬂaw. Letting (0,0) be the center of the
screen, we represent the collection of potential ﬂaw points as R(X) ¼ {(x1,x2):
x1 ∈[2,2], x2 ∈[1.5, 1.5]}
□.
Clearly, the total area of the screen is 3 · 4 ¼ 12 ft2, and any closed rectangle on
the screen having width W and height H contains the proportion WH/12, of the
total area of the screen. Since all of the points are equally likely, the probability set
function deﬁned on the events in R(X) should assign to each closed rectangle of
points a probability equal to WH/12 where Wand H are, respectively, the width and
height of the rectangular event. We thus seek a function f(x1, x2) such that
ðd
c
ðb
a
fðx1; x2Þ dx1 dx2  ðb  aÞðd  cÞ
12
8 a, b, c, and d such that  2  a  b  2 and 1.5 c d 1.5. Differentiating
the iterated integral above, ﬁrst with respect to d and then with respect to
b, yields f(b,d) ¼ 1/12 8 b ∈[2,2] and 8 d ∈[1.5, 1.5].12 The form of the
continuous PDF is then deﬁned by the following:
fðx1; x2Þ ¼ 1=12 I½2;2 x1
ð
ÞI½1:5;1:5 x2
ð
Þ:
4 feet
x1
x2
3 feet
Figure 2.8
Television screen.
12The differentiation is accomplished by applying Lemma 2.1 twice: once to the integral Ð d
c
Ð b
a fðx1; x2Þ dx1
h
i
dx2, differentiating with
respect to d to yield
Ð b
a f(x1, d) dx1, and then differentiating the latter integral with respect to b to obtain f(b,d). In summary,
@2=@b@d

 Ð d
c
Ð b
a f(x1,x2) dx1dx2 ¼ f(b,d).
2.4
Multivariate Random Variables, PDFs, and CDFs
73

The probability set function is thus deﬁned as PðAÞ ¼
Ð
x1;x22A
ð
Þ ð1=12Þdx1dx2 .
Then, for example, the probability that the ﬂaw occurs in the upper left quarter
of the screen is given by
Pð2  x1  0; 0  x2  1:5Þ ¼
ð1:5
0
ð0
2
1
12 dx1 dx2 ¼
ð1:5
0
1=6 dx2 ¼ :25:
2.4.2
Multivariate CDFs and Duality with PDFs
The CDF concept can be generalized to the multivariate case as follows:
Deﬁnition 2.20
Multivariate
Cumulative Distribution
Function
The cumulative distribution function of an n-dimensional random variable X
is deﬁned by
F b1; :::; bn
ð
Þ ¼ Pðxi  bi; i ¼ 1; :::; nÞ 8ðb1; :::; bnÞ 2 Rn:
The algebraic representation of F(b1,. . .,bn) in the discrete and continuous
cases can be given as follows:
a. Discrete X: F b1; . . . ; bn
ð
Þ ¼
P
x1  b1
   P
xn  bn
f x1;...;xn
ð
Þ>0
f x1; . . . ; xn
ð
Þfor b1; . . . ; bn
ð
Þ 2 Rn.
b. Continuous
X:
Fðb1; :::; bnÞ ¼
Ð bn
1 :::
Ð b1
1 f x1; . . . ; xn
ð
Þdx1; . . . ; dxn
for
(b1,. . .,bn) ∈Rn.
Some general properties of the joint cumulative distribution function
include:
1. limbi!1F b1; . . . ; bn
ð
Þ ¼ P ;
ð Þ ¼ 0; i ¼ 1; . . . ; n;
2. limbi!18iF b1; . . . ; bn
ð
Þ ¼ P RðXÞ
ð
Þ ¼ 1;
3. F(a)  F(b) for a < b, where
a ¼
a1
..
.
an
2
64
3
75<
b1
..
.
bn
2
64
3
75 ¼ b
The vector inequality above is taken in the usual sense to mean ai  bi 8i, and
ai < bi for at least one i. The reader should convince herself that these properties
follow directly from the deﬁnition of the multivariate cumulative distribution
function and the probabilities of the events identiﬁed by the appropriate event-
deﬁning conditions.
Similar to the univariate case, the joint CDF can be used to derive joint
discrete and continuous probability densities. For the discrete case, we discuss
the result for bivariate random variables only. For multivariate random variables
of three dimensions or higher, the large number of terms required in the
74
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

density-deﬁning procedure makes its use somewhat cumbersome. We state the
generalization in a footnote.13
Theorem 2.4
Discrete Bivariate PDFs
from Bivariate CDFs
Let (X, Y) be a discrete bivariate random variable with joint cumulative distri-
bution function F(x,y), and let x1 < x2 < x3 . . ., and y1 < y2 < y3 < . . ., repre-
sent the possible outcomes of X and Y. Then
a. f(x1, y1) ¼ F(x1, y1);
b. f(x1, yj) ¼ F(x1, yj)  F(x1, yj1), j  2;
c. f(xi, y1) ¼ F(xi, y1)  F(xi1, y1), i  2; and
d. f(xi, yj) ¼ F(xi, yj)  F(xi, yj1)  F(xi1, yj) + F(xi1, yj1), i and j  2.
Proof
The proof is left to the reader.
n
As we remarked in the univariate case, if the range of the random variable is
such that a lowest ordered outcome does not exist, then the deﬁnition simpliﬁes
to f(xi, yj) ¼ F(xi, yj)  F(xi, yj1)  F(xi1, yj) + F(xi1, yj1), 8i and j.
Theorem 2.5
Continuous
Multivariate PDFs
from CDFs
Let F(x1,. . .,xn) and f(x1,. . .,xn) represent the CDF and PDF for the continuous
multivariate random variable X ¼ (X1,. . .,Xn). The PDF of X can be deﬁned as
fðx1; :::; xnÞ ¼
@n Fðx1; :::; xnÞ
@ x1 :::@ xn
where fðÞis continuous
0 ðor any nonnegative numberÞ elsewhere:
8
<
:
Proof
The ﬁrst part of the deﬁnition follows directly from an n-fold application of
Lemma 2.1 for differentiating the iterated integral deﬁning the joint CDF. In
particular,
@n Fðx1; :::; xnÞ
@ x1 :::@ xn
¼ @n Ð xn
1 :::
Ð x1
1 fðt1; :::; tnÞ dt1 ::: dtn
@ x1 :::@ xn
¼ fðx1; :::; xnÞ
wherever f(·) is continuous.
Regarding the second part of the deﬁnition, as long as the integral
exists, arbitrarily changing the values of the nonnegative integrand at the points
of discontinuity will not affect the value of Fðb1; :::; bnÞ ¼
Ð bn
1 :::
Ð b1
1 f x1; . . . ; xn
ð
Þ
dx1; . . . ; dxn (recall footnote 11).
n
Example 2.17
Piecewise Deﬁnition of
Discrete Bivariate CDF
Examine the experiment of tossing two fair coins independently and observing
whether heads (H) or tails (T) occurs on each toss, so that S ¼ {(H,H), (H,T), (T,H),
(T,T)} with all elementary events in S being equally likely. Deﬁne a bivariate
13In the discrete m-dimensional case, the PDF can be deﬁned as f x
ð Þ ¼ F x
ð Þ þ lim
d!0þ
P
m
i¼1
1
ð
Þi P
v2Si
F x  dv
ð
Þ
 
!
where Si is the set of all of
the different (m 	 1) vectors that can be constructed using i 1’s and m-i 0’s.
2.4
Multivariate Random Variables, PDFs, and CDFs
75

random variable on the elements of S by letting x represent the total number of
heads and y represent the total number of tails resulting from the two tosses.
The joint density function for the bivariate random variable (X,Y) is then deﬁned
by f(x,y) ¼ 1/4 I{(0,2), (2,0)}(x,y) þ 1/2 I{(1,1)}(x,y).
It follows from Deﬁnition 2.14 that the joint CDF for (X,Y) can be
represented as
Fðb1; b2Þ ¼ 1
4 I½2;1Þ ðb1Þ Ið1;1Þ ðb2Þ þ 1
4 Ið1;1Þ ðb1Þ I½2;1Þ ðb2Þ
þ 1
2 I½1;2Þ ðb1Þ I½1;2Þ ðb2Þ þ 3
4 I½2;1Þ ðb1Þ I½1;2Þ ðb2Þ
þ 3
4 I½1;2Þ ðb1Þ I½2;1Þ ðb2Þ þ I½2;1Þ ðb1Þ I½2;1Þ ðb2Þ:
The CDF no doubt appears to be somewhat “pieced together”, making the
deﬁnition of F a rather complicated expression. Unfortunately, such piecewise
functional deﬁnitions often arise when specifying joint CDFs in the discrete
case, even for seemingly simple experiments such as the one at hand. To under-
stand more clearly the underlying rationale for the preceding deﬁnition of F, it is
useful to partition R2 into subsets that correspond to the events in S. In particu-
lar, we are interested in deﬁning the collection of elements w ∈S for which
X(w)  b1 and Y(w)  b2 is true for the various values of (b1,b2) ∈R2. Examine
the following table:
b1
b2
A ¼ {w: X(w)  b1, Y(w)  b2, w ∈S}
P(A)
b1 < 1
b2 < 1
;
0
1  b1 < 2
b2 < 1
;
0
b1 < 1
1  b2 < 2
;
0
b1  2
b2 < 1
{(H,H)}
1/4
b1 < 1
b2  2
{(T,T)}
1/4
1  b1 < 2
1  b2 < 2
{(H,T), (T,H)}
1/2
b1  2
1  b2 < 2
{(H,T), (T,H), (H,H)}
3/4
1  b1 < 2
b2  2
{(H,T), (T,H), (T,T)}
3/4
b1  2
b2  2
S
1
The reader should convince herself using a graphical representation of R2 that
the conditions deﬁned on (b1, b2) can be used to deﬁne nine disjoint subsets of R2
that exhaustively partitionR2 (i.e., the union of the disjoint sets ¼R2). The reader
will notice that the indicator functions used in the deﬁnition of F were based on
the latter six sets of conditions on (b1, b2) exhibited in the preceding table. If one
were interested in the probability P(x  1, y  1) ¼ F(1,1), for example, the joint
CDF indicates that 1/2 is the number we seek.
□
Example 2.18
Piecewise Deﬁnition
of Continuous
Bivariate CDF
Reexamine the projection television screen example, Example 2.16. The joint
CDF for the bivariate random variable (X1, X2), whose outcome represents the
location of the ﬂaw point, is given by
76
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Fðb1; b2Þ ¼
ðb2
1
ðb1
1
1
12 I½2;2 ðx1Þ I½1:5;1:5 ðx2Þ dx1 dx2
¼ ðb1 þ2Þðb2 þ1:5Þ
12
I½2;2 ðb1Þ I½1:5;1:5 ðb2Þ
þ 4ðb2 þ1:5Þ
12
Ið2;1 ðb1Þ I½1:5;1:5 ðb2Þ
þ 3ðb1 þ2Þ
12
I½2;2 ðb1Þ Ið1:5;1Þ ðb2Þ
þ Ið2;1Þ ðb1Þ Ið1:5;1Þ ðb2Þ:
It is seen that piecewise functional deﬁnitions of joint CDFs occur in the
continuous case as well. To understand the rationale for the piecewise deﬁni-
tion, ﬁrst note that if b1 < 2 and/or b2 < 1.5, then we are integrating over a
set of (x1, x2) points {(x1, x2): x1 < b1, x2 < b2} for which the integrand has a zero
value, resulting in a zero value for the deﬁnite integral. Thus, F(b1,b2) ¼ 0 if
b1 < 2 and/or b2 < 1.5. If b1 ∈[2,2] and b2 ∈[1.5, 1.5], then taking the
effect of the indicator functions into account, the integral deﬁning F can be
represented as
Fðb1; b2Þ ¼
ðb2
1:5
ðb1
2
1
12 dx1 dx2 ¼ ðb1 þ2Þðb2 þ1:5Þ
12
which is represented by the ﬁrst term in the preceding deﬁnition of F. If b1 > 2,
but b2 ∈[1.5, 1.5], then since the integrand is zero for all values of x1 > 2, we
can represent the integral deﬁning F as
Fðb1; b2Þ ¼
ðb2
1:5
ð2
2
1
12 dx1 dx2 ¼ 4ðb2 þ1:5Þ
12
which is represented by the second term in our deﬁnition of F. If b2 > 1.5, but
b1 ∈[2,2], then since the integrand is zero for all values of x2 > 1.5, we have that
Fðb1; b2Þ ¼
ð1:5
1:5
ðb1
2
1
12 dx1 dx2 ¼ 3ðb1 þ2Þ
12
which is represented by the third term in our deﬁnition of F. Finally, if both
b1 > 2 and b2 > 1.5, then since the integrand is zero for all values of x1 > 2
and/or x2 > 1.5, the integral deﬁning F can be written as
Fðb1; b2Þ ¼
ð1:5
1:5
ð2
2
1
12 dx1 dx2 ¼ 1
which justiﬁes the ﬁnal term in our deﬁnition of F. The reader should convince
herself that the preceding conditions on (b1, b2) collectively exhaust the possible
values of (b1, b2) ∈R2.
If one were interested in the probability P(x1  1, x2  1), the “relevant piece”
in the deﬁnition of F would be the ﬁrst term, and thus Fð1; 1Þ ¼ ð3Þð2:5Þ
12
¼ :625.
Alternatively, the probability P(x1  1, x2  10) would be assigned using the third
term in the deﬁnition of F, yielding F(1,10) ¼ .75.
□
2.4
Multivariate Random Variables, PDFs, and CDFs
77

2.4.3
Multivariate Mixed Discrete-Continuous and Composite Random Variables
A discussion of multivariate random variables in the mixed discrete-continuous
case could be presented here. However, we choose not to do so. In fact, we will
not examine the mixed case any further in this text. We are content with having
introduced the mixed case in the univariate context. The problem is that in the
multivariate case, representations of the relevant probability set functions –
especially when dealing with the concepts of marginal and conditional densities
which will be discussed subsequently – become extremely tedious and cumber-
some unless one allows a more general notion of integration than that of
Riemann, which would then require us to venture beyond the intended scope
of this text. We thus leave further study of mixed discrete-continuous random
variables to a more advanced course. Note, however, that since elements of both
the discrete and continuous random variable concepts are involved in the mixed
case, our continued study of the discrete and continuous cases will provide the
necessary foundation on which to base further study of the mixed case.
As a ﬁnal remark concerning our general discussion of multivariate random
variables, note that a function (or vector function) of a multivariate random
variable is also a random variable (or multivariate random variable). This follows
from the same composition of functions argument that was noted in the univar-
iate case. That is, y ¼ Y XðwÞ
ð
Þ, or y ¼ Y X1ðwÞ; . . . ; XnðwÞ
ð
Þ, or
y
m	1
¼
y1
..
.
ym
2
664
3
775 ¼
Y1 X1 ðwÞ; . . . ; Xn ðwÞ
ð
Þ
...
Ym X1 ðwÞ; . . . ; Xn ðwÞ
ð
Þ
2
664
3
775 ¼ Y XðwÞ
ð
Þ
m	1
are all in the context of “functions of functions,” so that ultimately Y is a
function of the elements w ∈S, and is therefore a random variable.14 One
might refer to these as composite random variables.
2.5
Marginal Probability Density Functions and CDFs
Suppose that we have knowledge of the probability space corresponding to an
experiment involving outcomes of the n-dimensional random variable X(n) ¼
(X1,. . .,Xm, Xm+1,. . .,Xn), but our real interest lies in assigning probabilities to
events involving only the m-dimensional random variable X(m) ¼ (X1,. . .,Xm),
m < n. In practical terms, this relates to an experiment in which n different
14The reader is reminded that we are suppressing the technical requirement that for every Borel set of y values, the associated
collection of w values in S must constitute an event in S for the function Y to be called a random variable. As we have remarked
previously, this technical difﬁculty does not cause a problem in applied work.
78
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

characteristics were recorded for each outcome but we are speciﬁcally interested
in analyzing only a subset of the characteristics. We will now examine the
concept of a marginal probability density function (MPDF) for X(m), which will
be derived from knowledge of the joint density function for X(n). Once deﬁned,
the MPDF can be used to identify the appropriate probability space only for the
portion of the experiment characterized by the outcomes of (X1,. . .,Xm), and we
will be able to use the MPDF in the usual way (summation in the discrete case,
integration in the continuous case) to assign probabilities to events concerning
(X1,. . .,Xm).
The key to understanding the deﬁnition of a marginal probability density is
to establish the equivalence between events of the form (x1,. . .,xm) ∈B in the
probability space for (X1,. . .,Xm) and events of the form (x1,. . .,xn) ∈A in the
probability space for (X1,. . .,Xn) since it is the latter events to which we can
assign probabilities knowing f(x1,. . .,xn).
2.5.1
Bivariate Case
Let f(x1, x2) be the joint density function and R(X) be the range of the bivariate
random variable (X1, X2). Suppose we want to assign probability to the event
x1 ∈B. Which event for the bivariate random variable is equivalent to event B
occurring for the univariate random variable X1? By deﬁnition, this event is
given by A ¼ {(x1, x2): x1 ∈B, (x1, x2) ∈R(X)}, i.e., the event B occurs for x1 iff
the outcome of (X1, X2) is in A so that x1 ∈B. Then since B and A are equivalent
events, the probability that we will observe x1 ∈B is identically equal to the
probability that we will observe (x1, x2) ∈A (recall the discussion of equivalent
events in Section 2.2).
For the discrete case, the foregoing probability correspondence implies that
PX1ðBÞ ¼ P x1 2 B
ð
Þ ¼ PðAÞ ¼
X
x1;x2
ð
Þ2A
f x1; x2
ð
Þ:
Our convention of deﬁning f(x1, x2) ¼ 0 8(x1, x2) =2R(X) allows the following
alternative representation of PX1(B):
PX1ðBÞ ¼
X
x12B
X
x22R X2
ð
Þ
f x1; x2
ð
Þ
The equivalence of the two representations ofPX1(B) follows from the fact that the
set of elementary events being summed over the latter case, C ¼ {(x1,x2): x1 ∈B,
x2 ∈R(X2)} is such that A  C, and f(x1,x2) ¼ 0 8(x1,x2) ∈CA. The latter repre-
sentation of PX1(B) leads to the following deﬁnition of the marginal probability
density of X1:
f1 x1
ð
Þ ¼
X
x22R X2
ð
Þ
f x1; x2
ð
Þ:
This function, when summed over the points comprising the event x1 ∈B,
yields the probability that x1 ∈B, i.e.,
2.5
Marginal Probability Density Functions and CDFs
79

PX1 ðBÞ ¼
X
x12B
f1 ðx1Þ ¼
X
x12B
X
x22RðX2Þ
fðx1; x2Þ:
Heuristically, one can think of the marginal density of X1 as having been deﬁned
by “summing out” the values of x2 in the bivariate PDF for (X1, X2). Having
deﬁned f1(x1), the probability space for the portion of the experiment involving
only X1, can then be deﬁned as R X1
ð
Þ; ϒX1; PX1
f
g where PX1ðBÞ ¼ P
X12B f1ðx1Þ for
B 2 ϒX1. Note that the order in which the random variables are originally listed
is immaterial to the approach taken above, and the marginal density function
and probability space for X2 could be deﬁned in an analogous manner by simply
reversing the roles of X1 and X2 in the preceding arguments. The MPDF for X2
would be deﬁned as
f2 ðx2Þ ¼
X
x12RðX1Þ
fðx1; x2Þ;
with the probability space for X2 deﬁned accordingly.
Example 2.19
Marginal PDFs in a
Discrete Bivariate Case
Reexamine Example 1.16, in which an individual was to be drawn randomly
from the work force of the Excelsior Corporation to receive a monthly “loyalty
award.” Deﬁne the bivariate random variable (X1,X2) as
x1 ¼
0
1
(
)
if
male
female
(
)
is drawn,
x2 ¼
0
1
2
8
>
>
<
>
>
:
9
>
>
=
>
>
;
if
sales
clerical
production
8
>
>
<
>
>
:
9
>
>
=
>
>
;
worker is drawn;
so that the bivariate random variable is measuring two characteristics of the out-
come of the experiment: gender and type of worker. The joint density of the bivariate
random variable is represented in tabular form below, where the nonzero values of f
(x1,x2) are given in the cells formed by intersecting an x1-row with a x2-column.
R(X2)
0
1
2
f1(x1)
0
RðX1Þ
1
.165
.135
.150
.450
.335
.165
.050
.550
f2(x2)
.500
.300
.200
The nonzero values of the marginal density of X2 are given in the bottom
margin of the table, being the deﬁnition of the marginal density
f2ðx2Þ ¼
X
x12RðX1Þ
fðx1; x2Þ ¼
X
1
x1¼0
fðx1; x2Þ ¼ :5I 0
f gðx2Þ þ :3I 1
f gðx2Þ þ :2I 2
f gðx2Þ:
The probability space for X2 is thus {R(X2),ϒX2,PX2} with ϒX2 ¼ {A: A  R(X2)} and
PX2(A) ¼ P
x22A f2 x2
ð
Þ. If one were interested in the probability that the individual
80
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

chosen was a sales or clerical worker, i.e., the event A ¼ {0,1}, then PX2 (A) ¼
P1
x2¼0 fx2(x2) ¼ .5 þ .3 ¼ .8.
The nonzero values of the marginal density for X1 are given in the right-hand
margin of the table, the deﬁnition of the density being
f1 x1
ð
Þ ¼
X
x22R X2
ð
Þ
f x1; x2
ð
Þ ¼
X
2
x2¼0
f x1; x2
ð
Þ ¼ :45Ið0Þ x1
ð
Þ þ :55Ið1Þ x1
ð
Þ:
The probability space for X1 is thus {R(X1),ϒX1,PX1} withϒX1 ¼ {A: A  R(X1)} and
PX1(A) ¼ P
x12A f1 x1
ð
Þ. If one were interested in the probability that the individ-
ual chosen was male, i.e., the event A ¼ {0}, then PX1(A) ¼ P0
x1¼0 fx1 x1
ð
Þ ¼ .45.□
The preceding example provides a heuristic justiﬁcation for the term mar-
ginal in the bivariate case and reﬂects the historical basis for the name marginal
density function. In particular, by summing across the rows or columns of a
tabular representation of the joint PDF f(x1,x2) one can calculate the marginal
densities of X1 and X2 in the margins of the table.
We now examine the marginal density function concept for continuous
random variables. Recall that the probability of event B occurring for the
univariate random variable X1 is identical to the probability that the event
A ¼ {(x1,x2):x1 ∈B, (x1,x2) ∈R(X)} occurs for the bivariate random variable
X ¼ (X1,X2). Then
PX1ðBÞ ¼ P x1 2 B
ð
Þ ¼ PðAÞ ¼
ð
x1;x2
ð
Þ2A
f x1; x2
ð
Þdx1dx2:
Our convention of deﬁning f(x1, x2) ¼ 0 8(x1, x2) =2 R(X) allows an alternative
representation of PX1ðBÞ to be given by
PX1ðBÞ ¼
ð
x12B
ð1
1
f x1; x2
ð
Þdx2dx1:
The equivalence of the two representations follows from the fact that the set of
elementary events being integrated over in the latter case, C ¼ {(x1, x2): x1 ∈B,
x2 ∈(1,1)}, is such that A  C, and f(x1, x2) ¼ 0 8 (x1, x2) ∈CA. The latter
representation of PX1(B) leads to the deﬁnition of the marginal density of X1 as
f1ðx1Þ ¼
ð1
1
fðx1;x2Þdx2:
This function, when integrated over the elementary events comprising the event
x1 ∈B, yields the probability that x1 ∈B, i.e.,
PX1ðBÞ ¼
ð
x12B
f1 x1
ð
Þ dx1 ¼
ð
x12B
ð1
1
f x1; x2
ð
Þdx2dx1:
Heuristically, one might think of the marginal density of X1 as having been deﬁned
by “integrating out” the values of X2 in the bivariate density function for (X1, X2).
2.5
Marginal Probability Density Functions and CDFs
81

Having deﬁned f1(x1), the probability space for the portion of the experiment
involving only X1 can then be deﬁned as {R(X1), ϒX1,PX1} where PX1ðAÞ ¼
Ð
x12A
f1ðx1Þdx1 for A ∈ϒX1 . Since the order in which the random variables were
originally listed is immaterial, the marginal density function and probability
space for X2 can be deﬁned in an analogous manner by simply reversing the roles
of X1 and X2 in the preceding arguments. The MPDF for X2 would be deﬁned as
f2 ðx2Þ ¼
ð1
1
fðx1; x2Þ dx1
with the probability space for X2 deﬁned accordingly.
Example 2.20
Marginal PDFs in a
Continuous Bivariate
Case
The Seafresh Fish Processing Company operates two ﬁsh processing plants. The
proportion of processing capacity at which each of the plants operates on any
given day is the outcome of a bivariate random variable having joint density
function f(x1, x2) ¼ (x1 þ x2) I[0,1](x1) I[0,1](x2). The marginal density function for
the proportion of processing capacity at which plant 1 operates can be deﬁned by
integrating out x2 from f(x1, x2) as
f1 ðx1Þ ¼
ð1
1
fðx1; x2Þ dx2 ¼
ð1
1
ðx1 þ x2Þ I½0;1 ðx1Þ I½0;1 ðx2Þ dx2
¼
ð1
0
x1 þ x2
ð
Þ I½0;1 ðx1Þ dx2 ¼
x1 x2 þ x2
2
2


I½0;1 ðx1Þ





1
0
¼ ðx1 þ1=2Þ I½0;1 ðx1Þ:
The probability space for plant 1 outcomes is given by
R X1
ð
Þ; ϒX1; PX1
f
g, where
R(X1) ¼ [0,1], ϒX1 ¼ {A: A is a Borel set  R(X1)}, and PX1ðAÞ ¼ Ð
x12A f1ðx1Þdx1,
8A ∈ϒX1. If one were interested in the probability that plant 1 will operate at less
than half of capacity on a given day, i.e., the event A ¼ [0, .5), then
PX1 x1  :5
ð
Þ ¼
ð:5
0
x1 þ 1
2


I 0;1
½
 x1
ð
Þdx1 ¼ x2
1
2 þ x1
2
:5
0







¼ :375:
□
Regarding other properties of marginal density functions, note that the
signiﬁcance of the term marginal is only to indicate the context in which the
density was derived, i.e., the marginal density of X1 is deduced from the joint
density for (X1, X2). Otherwise, the MPDF has no special properties that differ
from the basic properties of any other PDF.
2.5.2
n-Variate Case
The concept of a discrete MPDF can be straightforwardly generalized to the
n-variate case, in which case the marginal densities may themselves be joint
density functions. For example, if we have the density function f(x1, x2, x3) for
the trivariate random variable (X1,X2,X3), then we may conceive of six marginal
82
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

density functions:
f1 ðx1Þ; f2 ðx2Þ; f3 ðx3Þ;
f12(x1, x2), f13(x1, x3),
f23 ðx2; x3Þ .
In general, for an n-variate random variable, there are (2n  2) possible MPDFs
that can be deﬁned from knowledge of f(x1,. . .,xn). We present the n-variate
generalization in the following deﬁnition. We use the notation fj1 ::: jm ðxj1; :::; xjmÞ
to represent the MPDF of the m-variate random variableðXj1; :::; XjmÞwith the ji’s
being the indices that identify the particular random vector of interest. The
motivation for the deﬁnition is analogous to the argument in the bivariate
case
upon
identifying
the
equivalent
events
ðxj1; :::; xjmÞ 2 B
and
A ¼ fx : ðxj1; :::; xjmÞ 2 B; x 2 R X
ð Þg is left to the reader as an exercise.
Deﬁnition 2.21
Discrete Marginal
Probability Density
Functions
Let f(x1,. . .,xn) be the joint discrete PDF for the n-dimensional random vari-
able (X1,. . .,Xn). Let J ¼ {j1,j2,. . .,jm}, 1  m < n, be a set of indices selected
from the index set I ¼ {1, 2,. . .,n}. Then the marginal density function for the
m-dimensional discrete random variable (Xj1,. . .,Xjm) is given by
fj1...jm xj1; . . . ; xjm


¼
X
  
X
xi2R Xi
ð
Þ; i2IJ
ð
Þ
fðx1; :::; xnÞ:
In other words, to deﬁne a MPDF in the general discrete case, we simply
“sum out” the variables that are not of interest in the joint density function. We
are left with the marginal density function for the random variable in which we
are interested. For example, if n ¼ 3, so that I ¼ {1,2,3}, and if J ¼ {j1,j2} ¼ {1,3} so
that I-J ¼ {2}, then Deﬁnition 2.21 indicates that the MPDF of the random
variable (X1, X3) is given by
f13 ðx1; x3Þ ¼
X
x22RðX2Þ
fðx1; x2; x3Þ:
Similarly, the marginal density for x1 would be deﬁned by
f1 x1
ð
Þ ¼
X
x22RðX2Þ
X
x32RðX3Þ
fðx1; x2; x3Þ:
The concept of a continuous MPDF can be generalized to the n-variate case as
follows:
Deﬁnition 2.22
Continuous Marginal
Probability Density
Functions
Let f(x1,. . .,xn) be the joint continuous PDF for the n-variate random variable
(X1,. . .,Xn). Let J ¼ {j1, j2,. . .,jm}, 1  m < n, be a set of indices selected from
the index set I ¼ {1, 2,. . .,n}. Then the marginal density function for the
m-variate continuous random variable (Xj1,. . ., Xjm) is given by
fj1...jm xj1; . . . ; xjm


¼
ð1
1
  
ð1
1
f x1; . . . ; xn
ð
Þ
Y
i2IJ
dxi:
In other words, to deﬁne a MPDF function in the general continuous case,
we simply “integrate out” the variables in the joint density function that are not
2.5
Marginal Probability Density Functions and CDFs
83

of interest. We are left with the marginal density function for the random
variables in which we are interested. An example of marginal densities in the
context of a trivariate random variable will be presented in Section 2.8.
2.5.3
Marginal Cumulative Distribution Functions (MCDFs)
Marginal CDFs are simply CDFs that have been derived for a subset of the
random variables in X ¼ (X1,. . .,Xn) from initial knowledge of the joint PDF or
joint CDF of X. For example, ordering the elements of a continuous random
variable (X1,. . .,Xn) so that the ﬁrst m < n random variables are of primary
interest, the MCDF of (X1,. . .,Xm) can be deﬁned as
F1:::m ðb1; :::; bmÞ ¼ PX1 ::: Xm ðxi  bi; i ¼ 1; :::; mÞ Def:of CDF
ð
Þ
¼ Pðxi  bi; i ¼ 1; :::; m; xi <1; i ¼ m þ 1; :::; nÞ equivalent events
ð
Þ
¼ Fðb1; :::; bm; 1; :::; 1Þ Def: in terms of joint CDF
ð
Þ
¼
ðb1
1
:::
ðbm
1
ð1
1
:::
ð1
1
fðx1; :::; xnÞ dxn ::: dx1 Def: in terms of joint PDF
ð
Þ
¼
ðb1
1
:::
ðbm
1
f1:::m ðx1; :::; xmÞ dxm ::: dx1 Def: in terms of marginal PDF
ð
Þ:
In the case of an arbitrary subset ðXj1; :::; Xjm Þ, m < n of the random variables
(X1,. . .,Xn), the MCDF in terms of the joint CDF or marginal PDF can be
represented as
Fj1 ::: jm ðbj1; :::; bjmÞ ¼ FðbÞ ¼
ðbj1
1
:::
ðbjm
1
fj1 ::: jm ðxj1; :::; xjmÞ dxjm ::: dxj1
where bji is the jith entry in b and bi ¼ 1 if i =2{j1,. . .,jm}.
Examples of marginal CDFs in the trivariate case are presented in Section 2.8.
The discrete case is analogous, with summation replacing integration.
2.6
Conditional Density Functions
Suppose that we have knowledge of the probability space corresponding to
an experiment involving outcomes of the n-dimensional random variable
X(n) ¼ (X1,. . .,Xm, Xm+1,. . .,Xn), and we are interested in assigning probabilities
to the event (x1,. . .,xm) ∈C given that (xm+1,. . .,xn) ∈D. In practical terms,
this relates to an experiment in which n different characteristics were recorded
for each outcome and we are speciﬁcally interested in analyzing a subset of
these characteristics given that a ﬁxed set of possibilities will occur with
certainty for the remaining characteristics. Note that this is different from
asking for the probability of observing the event (x1,. . .,xm) ∈C and (xm+1,. . .,
xn) ∈D, for we are saying that (xm+1,. . .,xn) ∈D will happen with certainty.
How do we assign the appropriate probability in this case? Questions of this
type can be addressed through the use of conditional PDFs, which can be
derived from knowledge of the joint density function f(x1,. . .,xn).
The key to the deﬁnition of a conditional PDF is to establish the equivalence
between events for the m-dimensional random variable (X1,. . .,Xm) and
84
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

(nm)-dimensional random variable (Xm+1,. . .,Xn) with events for the n-dimen-
sional random variable (X1,. . .,Xn). Then conditional probabilities in the probability
space for (X1,. . .,Xn) can be used to deﬁne a conditional PDF.
2.6.1
Bivariate Case
Let f(x1, x2) be the joint density function and R(X) be the range of the bivariate
random variable (X1, X2). The event for the bivariate random variable that is
equivalent to the event x1 ∈C occurring for the scalar random variable X1 is
given by A ¼ {(x1, x2): x1 ∈C, (x1, x2) ∈R(X)}. That is, A is the set of all
possible outcomes for the two-tuple (x1, x2) that result in the ﬁrst coordinate
x1 residing in C. Similarly, the event for the bivariate random variable that is
equivalent to the event D occurring for the random variable X2 is given by
B ¼ {(x1, x2): x2 ∈D, (x1, x2) ∈R(X)}. Then the probability that x1 ∈C given
that x2 ∈D can be deﬁned by the conditional probability
PX1 j X2 ðCjDÞ ¼ Pðx1 2 Cjx2 2 DÞ ¼ PðAjBÞ ¼ PðA \ BÞ
PðBÞ
for PðBÞ 6¼ 0;
where A \ B ¼ {(x1, x2): x1 ∈C, x2 ∈D, (x1, x2) ∈R(X)}.
In the case of a discrete random variable, the foregoing conditional probabil-
ity is represented by
PX1 j X2 ðCjDÞ ¼ PðAjBÞ ¼
P
x1;x2
ð
Þ2A\B
fðx1; x2Þ
P
x1;x2
ð
Þ2B
fðx1; x2Þ
Given our convention that f(x1, x2) ¼ 0 whenever (x1, x2) =2 R(X), we can ignore
the set-deﬁning condition (x1, x2) ∈R(X) in both the sets A \ B and B, and
represent the conditional probability as
PX1 j X2 ðCjDÞ ¼
P
x12C
P
x22D fðx1; x2Þ
P
x12RðX1Þ
P
x22D fðx1; x2Þ ¼
X
x12C
P
x22D fðx1; x2Þ
P
x22D f2 ðx2Þ
"
#
where we have used the fact that f2(x2) ¼ P
x12RðX1Þ f x1; x2
ð
Þ. The expression in
brackets is the conditional density function we seek, since it is the function that
would be summed over the elements in C to assign probability to the event
x1 ∈C, given x2 ∈D, for any event C. We will denote the conditional density
of X1, given x2 ∈D, by the notation f(x1|x2 ∈D). If D is a singleton set d
f g, we
will also represent the conditional density function as f(x1|x2 ¼ d).
In the case of a continuous bivariate random variable, the probability that
x1 ∈C given that x2 ∈D would be given by (assuming PX2 ðDÞ ¼ P(B) 6¼ 0)
PX1 j X2 ðCjDÞ ¼ Pðx1 2 Cj x2 2 DÞ ¼ PðAjBÞ ¼
Ð
x1;x2
ð
Þ2A\B fðx1; x2Þ dx1 dx2
Ð
x1;x2
ð
Þ2B fðx1; x2Þ dx1 dx2
Using our convention that f(x1,x2) ¼ 0 8 (x1, x2) =2 R(X), we can also represent the
conditional probability as
2.6
Conditional Density Functions
85

PX1 j X2 ðCjDÞ ¼
Ð
x12C
Ð
x22D fðx1; x2Þ dx2 dx1
Ð 1
1
Ð
x22D fðx1; x2Þ dx2 dx1
¼
ð
x12C
Ð
x22D fðx1; x2Þ dx2
Ð
x22D f2 ðx2Þ dx2
"
#
dx1;
where we have used the fact that f2 x2
ð
Þ ¼
Ð 1
1 f x1; x2
ð
Þdx1 . The expression in
brackets is the conditional density function we seek, since it is the function that
would be integrated over the elements in C to assign probability to the event
x1 ∈C, given x2 ∈D, for any event C. As in the discrete case, we will use the
notation f(x1|x2 ∈D) or f(x1|x2 ¼ d) to represent the conditional density func-
tion. In both the discrete and continuous cases, we will eliminate the random
variable subscripts on PX1 j X2 ð Þwhen the random variable context of the proba-
bility set function is clear.
Once derived, a conditional PDF exhibits all of the standard properties of a
PDF. The signiﬁcance of the term conditional PDF is to indicate that the density
of X1 was derived from the joint density for (X1, X2) conditional on a speciﬁc
event for X2. Otherwise, there are no special general properties of a conditional
PDF that distinguishes it from any other PDF.
We provide examples of the derivation and use of discrete and continuous
conditional PDF’s in the following examples.
Example 2.21
Conditional PDF in a
Bivariate Discrete Case
Recall the dice example, Example 2.15, where f(x1,x2) ¼ (1/36) I{1,. . .,6} (x1)
I{1,. . .,6} (x2  x1). The conditional density function for X1, given that x2 ¼ 5,
is given by
fðx1 j x2 ¼ 5Þ ¼ fðx1; 5Þ
f2 ð5Þ ¼
1
36 If1;:::;6g ðx1Þ If1;:::;6g ð5  x1Þ
6j57j
36
If2;:::;12g ð5Þ
¼ 1
4 If1;:::;4g ðx1Þ:
The probability of rolling a 3 or less on the red die, given that the total of the two
dice will be 5, is then
P x1  3jx2 ¼ 5
ð
Þ ¼
X
3
x1¼1
f x1jx2 ¼ 5
ð
Þ ¼ 3
4 :
Note that the unconditional probability that x1  3 is equal to 1/2.
The conditional density function for X1, given that x2 ∈D ¼ {7, 11}, is given by
fðx1 j x2 2 DÞ ¼
P
x22D fðx1; x2Þ
P
x22D f2ðx2Þ
¼
1
36 If1;:::;6g ðx1Þ If1;:::;6g ð7  x1Þ þ If1;:::;6g ð11  x1Þ

	
8
36
¼ 1
8 If1;:::;4g ðx1Þ þ 1
4 If5;6g ðx1Þ
The probability of rolling a 3 or less on the red die, given that the total of the two
dice will be either a 7 or 11, is then
Pðx1  3jx2 2 DÞ ¼
X
3
x1¼1
f x1jx2 2 D
ð
Þ¼ 3
8 :
□
86
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Example 2.22
Conditional PDF in a
Bivariate Continuous
Case
Recall Example 2.20 regarding the proportion of daily capacity at which two
processing plants operate. The conditional density function of plant 1’s capacity,
given that plant 2 operates at less than half of capacity, is given by
fðx1 j x2  :5Þ ¼
Ð :5
1 fðx1; x2Þ dx2
Ð :5
1 f2 ðx2Þ dx2
¼
Ð :5
0 ðx1 þ x2Þ I½0;1 ðx1Þ dx2
Ð :5
0 ðx2 þ1=2Þ dx2
¼ :5 x1 þ:125
:375
I½0;1 ðx1Þ ¼
4
3 x1 þ 1
3


I½0;1 ðx1Þ
The probability that x1  .5, given that x2  .5, is given by
Pðx1  :5j x2  :5Þ ¼
ð:5
0
4
3 x1 þ 1
3


dx1 ¼ 1
3:
Recall that the unconditional probability that x1  .5 was .375.
□
2.6.2
Conditioning on Elementary Events in Continuous Cases-Bivariate
A problem arises in the continuous case when deﬁning a conditional PDF for X1,
conditional on an elementary event occurring for X2. Namely, because all ele-
mentary events are assigned probability zero in the continuous case, with the
integral over a singleton set being zero, our deﬁnition of the conditional density,
as presented earlier, yields
fðx1 j x2 ¼ bÞ ¼
Ð b
b fðx1; x2Þ dx2
Ð b
b f2 ðx2Þ dx2
¼ 0
0;
which is an indeterminate form. Thus f(x1|x2 ¼ b) is undeﬁned so that
P(x1 ∈A | x2 ¼ b) is undeﬁned as well. This is different than the discrete
case, where
fðx1 j x2 ¼ bÞ ¼ fðx1; bÞ
f2 ðbÞ
is well-deﬁned, provided f2(b) 6¼ 0.
The problem is circumvented by redeﬁning the conditional probability,
P(x1 2 A|x2 ¼ b), in the continuous case in terms of a limit as
Pðx1 2 Aj x2 ¼ bÞ ¼ lim
e!0þ Pðx1 2 Aj x2 2 ½b  e; b þ eÞ
¼ lim
e!0þ
Ð
x12A
Ð bþe
be f x1; x2
ð
Þ dx2 dx1
Ð bþe
be f2 x2
ð
Þdx2
"
#
where lime!0þ means we are examining a limit for a sequence of e-values that
approach zero from positive values (i.e., e > 0). The idea is to examine the
limiting value of a sequence of probabilities that are conditioned on a
corresponding sequence of events, [b  e, b þ e] for e!0+, that converge to the
2.6
Conditional Density Functions
87

elementary event {b}. The following lemma will facilitate the identiﬁcation of
the limit.
Lemma 2.2
Mean Value Theorem
for Integrals
If g(x) is continuous 8x ∈[c1,c2], then ∃x0 ∈[c1,c2] such that
Ð c2
c1 gðxÞdx ¼
gðx0Þðc2  c1Þ.15
To use the mean value theorem, and to ensure that the limit of the condi-
tional probabilities exists, we assume that there exists a choice ofe > 0 such that
f2(x2) and f(x1, x2) are continuous in x2, 8x2 ∈[b  e, b þ e], and that f2(b) > 0.
Then, by the mean value theorem,
Pðx1 2 Aj x2 ¼ bÞ ¼ lim
e!0þ
Ð
x12A
Ð bþe
be fðx1; x2Þ dx2dx1
Ð bþe
be f2 x2
ð
Þdx2
"
#
¼ lim
e!0þ
2e
Ð
x12A fðx1; x0
2Þ dx1
2e f2 ðx
2Þ
"
#
where both x0
2 and x
2 ∈[b  e, b þ e], and x0
2 will generally depend on the value of
x1.16 The 2 e’s in the numerator and denominator cancel each other, and as e!0+,
the interval [b  e , b þ e ] reduces to [b,b] ¼ b, so that in the limit, both
x0
2 and x
2 ¼ b. The limiting value of the conditional probability is then
Pðx1 2 Aj x2 ¼ bÞ ¼
ð
x12A
f x1; b
ð
Þ
f2ðbÞ dx:
Since the choice of event A is arbitrary, it follows that the appropriate condi-
tional probability density in this case is
fðx1 j x2 ¼ bÞ ¼ fðx1; bÞ
f2 ðbÞ ;
which is precisely of the same form as the discrete case. Thus, the deﬁnition of
conditional density functions, when conditioning on elementary events, will be
identical for continuous and discrete random variables, provided f2(b) 6¼ 0.
Example 2.23
PDF Conditioned on an
Elementary Event
Recall Example 2.22. The conditional PDF for plant 1’s proportion of capacity X1,
given that plant 2’s capacity proportion is x2 ¼ .75, can be deﬁned as
fðx1 j x2 ¼ :75Þ ¼ fðx1; :75Þ
f2 ð:75Þ ¼ ðx1 þ :75ÞI 0;1
½
 x1
ð
Þ
1:25
¼
4
5 x1 þ 3
5


I 0;1
½
 x1
ð
Þ
The probability that x1  .5, given that x2 ¼ .75, is then given by P(x1  .5 |
x2 ¼ .75) ¼
Ð 5
0
4
5 x1 þ 3
5


dx1 ¼ :4
□
15R. Courant and F. John, Introduction to Calculus and Analysis, New York, John Wiley-Interscience, 1965, p. 143.
16In applying the mean value theorem to the numerator, we treat f(x1, x2) as a function of the single variable x2, ﬁxing the value of x1 for
each application.
88
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

2.6.3
Conditioning on Elementary Events in Continuous Cases: n-Variate Case
The preceding concepts of discrete and continuous conditional PDFs in the
bivariate case can be generalized to the n-variate case, as indicated in the
following deﬁnition, which subsumes n ¼ 2 as a special case:
Deﬁnition 2.23
Conditional Probability
Density Functions
Let f(x1,. . .,xn) be the joint density function for the n-dimensional random
variable (X1,. . .,Xn) . The conditional density function for the m-dimensional
random variable X1; :::; Xm
ð
Þ, given that Xmþ1; :::; Xn
ð
Þ 2 D and PXmþ1;:::;XnðDÞ>0,
is as follows:
Discrete Case:
fðx1; :::; xmjxmþ1; :::; xnÞ 2 DÞ ¼
P
ðxmþ1;:::;xnÞ2D fðx1; :::; xnÞ
P
ðxmþ1;:::;xnÞ2D fmþ1;:::;nðxmþ1; :::; xnÞ
Continuous Case:
fðx1; :::; xmjxmþ1; :::; xnÞ 2 DÞ
¼
Ð
xmþ1;:::;xn
ð
Þ2D f x1; . . . ; xn
ð
Þdxmþ1 . . . dxn
Ð
xmþ1;:::;xn
ð
Þ2D fmþ1;:::;n xmþ1; :::; xn
ð
Þdxmþ1 . . . dxn
If D is equal to the elementary event ðdmþ1; :::; dnÞ then the deﬁnition of the
conditional density in both the discrete and continuous cases can be
represented as
f x1; :::; xmjxi ¼ di; i ¼ m þ 1; :::; n
ð
Þ ¼ fðx1; :::; xm; dmþ1; :::; dnÞ
fmþ1;:::;n dmþ1; :::; dn
ð
Þ
when the marginal density in the denominator is positive valued.17
For example, if n ¼ 3, then the conditional density function of (X1, X2), given
that x3 ∈D, would be deﬁned as
fðx1; x2jx3 2 DÞ ¼
P
x32D fðx1; x2; x3Þ
P
x32D f3 x3
ð
Þ
in the discrete case, with integration replacing summation in the continuous
case. If D ¼ d3, then for both the discrete and continuous cases,
f x1; x2jx3 ¼ d3
ð
Þ ¼ f x1; x2; d3
ð
Þ
f3 d3
ð
Þ
:
17In the continuous case, it is also presumed that f and fmþ1;:::;n are continuous in xmþ1; :::; xn
ð
Þ within some neighborhood of points
around the point where the conditional density is evaluated in order to justify the conditional density deﬁnition via a limiting
argument analogous to the bivariate case. Motivation for the conditional density expression when conditioning on an elementary
event in the continuous case can then be provided by extending the mean-value theorem argument used in the bivariate case. See R.G.
Bartle, Real Analysis, p. 429 for a statement of the general mean value theorem for integrals.
2.6
Conditional Density Functions
89

An example of conditional PDFs in the trivariate case will be presented in
Section 2.8.
In summary, if we begin with the joint density function appropriate for
assigning probabilities to events involving the n-dimensional random variable
(X1,. . .,Xn), we can derive a conditional probability density function that is the
PDF appropriate for assigning probabilities to events for an m-dimensional
subset of the random variables in (X1,. . .,Xn), given (or conditional) on an event
for the remaining nm random variables. The construction of the conditional
density involves both the joint density of (X1,. . ., Xn) and the marginal density of
the (nm) dimensional random variable on which we are conditioning. In the
special case where we are conditioning on an elementary event, the conditional
density function simply becomes the ratio of the joint density function divided
by the marginal density function, replacing the arguments of these functions
with their conditioned values for those arguments corresponding to random
variables on which we are conditioning (which represents all of the arguments
of the marginal density, and a subset of the arguments of the joint density).
2.6.4
Conditional CDFs
We can deﬁne the concept of a conditional CDF by simply using a conditional
density function in the deﬁnition of the CDF. For example, for the bivariate
random variable (X1, X2), we can deﬁne
Fðb1 j x2 2 DÞ ¼ P x1  b1 j x2 2 D
ð
Þ ¼
ðb1
1
fðx1 j x2 2 DÞ dx1
as one such conditional CDF, representing the CDF of X1, conditional on
x2 ∈D. Once deﬁned, the conditional CDF possesses no special properties
that distinguish it in concept from any other CDF. The reader is asked to
contemplate the various conditional CDFs that can be deﬁned for the n-dimen-
sional random variable (X1,. . .,Xn).
2.7
Independence of Random Variables
From our previous discussion of independence of events, we know that A and B
are independent iff P(A \ B) ¼ P(A)P(B). This concept can be applied directly to
determine whether two particular events for the n-dimensional random variable
(X1,. . .,Xn) are independent. The general deﬁnition of independence of events
(Deﬁnition 1.19) can also be applied to examine the independence of k speciﬁc
events for the random variable (X1,. . .,Xn).
The concept of independence of events will now be extended further to the idea
of independence of random variables, which is related to the question of whether
the n events (recall the abbreviated set deﬁnition notation of Deﬁnition 2.7)
{xi ∈Ai}  {(x1,. . .,xn):xi ∈Ai,(x1,. . .,xn) ∈R(X)}, i ¼ 1,. . .,n, are independent
for all possible choices of the events A1,. . .,An. If so, the n random variables are
said to be independent. In effect, the concept is one of global independence of
90
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

events for random variables – we deﬁne an event Ai for each of the n random
variables in (X1,. . .,Xn) and, no matter how we deﬁne the events (which is the
meaning of the term “global” here), the events {xi ∈Ai}, i ¼ 1,. . .,n, are indepen-
dent. Among other things, we will see that this implies that the probability
assigned to any event Ai for any random variable Xi in (X1,. . .,Xn) is unaffected
by conditioning on any event B for the remaining random variables (assuming
P(B) > 0 for the existence of the conditional probability).
2.7.1
Bivariate Case
We seek to establish a condition that will ensure that the events {x1 ∈A1} and
{x2 ∈A2} are independent for all possible choices of the events A1 and A2. This
can be accomplished by applying independence conditions to events in the
probability space, {R(X),ϒ,P} for the bivariate random variable X ¼ (X1, X2).
The events x1 ∈A1 and x2 ∈A2 are equivalent, respectively, to the following
events for the bivariate random variable:
B1 ¼
x1; x2
ð
Þ : x1 2 A1; x1; x2
ð
Þ 2 RðXÞ
f
gand B2 ¼
x1; x2
ð
Þ : x2 2 A2; x1; x2
ð
Þ 2 RðXÞ
f
g:
The two events B1 and B2 are independent iff P(B1\B2) ¼ P(B1)P(B2), which can
also be represented using our abbreviated notation as P(x1 ∈A1, x2 ∈A2) ¼
P(x1 ∈A1)P(x2 ∈A2). Requiring the independence condition to hold for all
choices of the events A1 and A2 leads to the deﬁnition of the independence
condition for random variables.
Deﬁnition 2.24
Independence of
Random Variables:
Bivariate
The random variables X1 and X2 are said to be independent iff P(x1 ∈A1,
x2 ∈A2) ¼ P(x1 ∈A1) P(x2 ∈A2) for all events A1, A2.
There is an equivalent characterization of independence of random variables
in terms of PDFs that can be useful in practice and that also further facilitates
the investigation of the implications of random variable independence.
Theorem 2.6
Bivariate Density
Factorization for
Independence of
Random Variables
The random variables X1 and X2 with joint PDF f(x1, x2) and marginal PDFs
fi(xi), i ¼ 1, 2, are independent iff the joint density factors into the product of the
marginal densities as f(x1, x2) ¼ f1(x1) f2(x2) 8 (x1, x2).18
18Technically, the factorization need not hold at points of discontinuity for the joint density function of a continuous random variable.
However, if the random variables are independent, there will always exist a density function for which the factorization can be formed.
This has to do with the fundamental non-uniqueness of PDFs in the continuous case, which can be redeﬁned arbitrarily at a countable
number of isolated points without affecting the assignment of any probabilities of events through integration. There are few practical
beneﬁts of this non-uniqueness, and we suppress this technical anomaly here.
2.7
Independence of Random Variables
91

Proof
Discrete Case Let A1 and A2 be any two events for X1 and X2, respectively. Then
if the joint density function f(x1, x2) factors,
P x1 2 A1; x2 2 A2
ð
Þ ¼
X
x12A1
X
x22A2
f x1; x2
ð
Þ ¼
X
x12A1
f1 x1
ð
Þ
X
x22A2
f2 x2
ð
Þ ¼ P x1 2 A1
ð
ÞP x2 2 A2
ð
Þ
so that X1 and X2 are independent. Thus, factorization is sufﬁcient for
independence. Now assume (X1,X2) are independent random variables. Let
A1 ¼ {a1} and A2 ¼ {a2} for any choice of elementary events, ai ∈R(Xi),
corresponding to the random variable Xi,i ¼ 1,2, respectively. Then, by
independence,
Pðx1 ¼ a1; x2 ¼ a2Þ ¼ fða1; a2Þ ¼ Pðx1 ¼ a1ÞPðx2 ¼ a2Þ ¼ f1ða1Þf2ða2Þ
If ai =2 R(Xi), then fi(ai) ¼ 0 and f(a1,a2) ¼ 0 for i ¼ 1,2, and thus factorization will
automatically hold. Thus, factorization is necessary for independence.
Continuous case Let A1 and A2 be any two events for X1 and X2, respectively.
Then if the joint density function f(x1, x2) factors,19
P x1 2 A1; x2 2 A2
ð
Þ ¼
ð
x22A2
ð
x12A1
f x1; x2
ð
Þdx1dx2
¼
ð
x12A1
f1 x1
ð
Þdx1
ð
x22A2
f2 x2
ð
Þdx2
¼ P x1 2 A1
ð
ÞP x2 2 A2
ð
Þ;
so that X1 and X2 are independent. Thus, factorization is sufﬁcient for indepen-
dence. Now assume (X1,X2) are independent random variables. Let Ai ¼ {xi:xi 
ai} for arbitrary choice of ai,i ¼ 1,2. Then by independence,
Pðx1  a1; x2  a2Þ ¼
ða2
1
ða1
1
f x1; x2
ð
Þ dx1 dx2
¼ Pðx1  a1ÞPðx2  a2Þ ¼
ða1
1
f1ðx1Þ dx1
ða2
1
f2ðx2Þ dx2 :
Differentiating the integrals with respect to a1 and a2 yields f(a1, a2) ¼ f1(a1) f2(a2)
wherever the joint density function is continuous. Thus, the factorization condition
stated in the theorem is necessary for independence.
n
In other words, two random variables are independent iff their joint PDF can be
expressed equivalently as the product of their respective marginal PDFs (the
condition not being required to hold at points of discontinuity in the continuous
case). An important implication of independence of X1 and X2 is that the
19Any points of discontinuity can be ignored in the deﬁnitions of the probability integrals without affecting the probability
assignments.
92
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

conditional and marginal PDFs of the respective random variables are identical.20
For example, assuming independence,
fðx1 j x2 2 BÞ ¼
Ð
x22B f x1; x2
ð
Þ dx2
Ð
x22B f2 x2
ð
Þ dx2
¼
f1 x1
ð
Þ
Ð
x22B f2ðx2Þ dx2
Ð
x22B f2ðx2Þ dx2
¼ f1ðx1Þ
(in the discrete case, replace integration by summation). The fact that condi-
tional and marginal PDFs are identical implies that the probability of x1 ∈A,
for any event A, is unaffected by the occurrence or nonoccurrence of event B for
X2. For example, in the continuous case,
P x1 2 Ajx2 2 B
ð
Þ ¼
ð
x12A
f x1jx2 2 B
ð
Þdx1 ¼
ð
x12A
f1 x1
ð
Þdx1 ¼ P x1 2 A
ð
Þ
(replace integration by summation in the discrete case). The result holds for any
events involving X2 for which the conditional density function is deﬁned. The
roles of X1 and X2 can be reversed in the preceding discussion.
Example 2.24
Independence
of Bivariate
Continuous RVs
Recall Example 2.16 concerning coating ﬂaws in the manufacture of television
screens. The horizontal and vertical coordinates of the coating ﬂaw was the
outcome of a bivariate random variable with joint density function
fðx1;x2Þ ¼ 1
12I½2;2ðx1ÞI½1:5;1:5ðx2Þ:
Are the random variables independent?
Answer: The marginal densities of X1 and X2 are given by
f1ðx1Þ ¼ I 2;2
½
 x1
ð
Þ
ð1:5
1:5
1
12 dx2 ¼ :25I 2;2
½
 x1
ð
Þ
f2 x2
ð
Þ ¼ I 1:5;1:5
½
 x2
ð
Þ
ð2
2
1
12 dx1 ¼ 1
3 I 1:5;1:5
½
 x2
ð
Þ :
It follows that f(x1, x2) ¼ f1(x1)f2(x2) 8(x1, x2), and the random variables are
independent. Therefore, knowledge that an event for X2 has occurred has no
effect on the probability assigned to events for X1, and vice versa.
□
Example 2.25
Independence
of Bivariate
Discrete RVs
Recall the dice example, Example 2.15. Are X1 and X2 independent random
variables?
Answer: Examine the validity of the independence condition:
f x1; x2
ð
Þ ¼
? f1 x1
ð
Þf2 x2
ð
Þ 8 x1; x2
ð
Þ;
20We will henceforth suppress constant reference to the fact that factorization might not hold for some points of discontinuity in the
continuous case – it will be tacitly understood that results we derive based on the factorization of f(x1,x2) may be violated at some
isolated points. For example, for the case at hand, marginal and conditional densities may not be equal at some isolated points.
Assignments of probability will be unaffected by this technical anomaly.
2.7
Independence of Random Variables
93

or, speciﬁcally,
1
36 I 1;2;...;6
f
g x1
ð
ÞI 1;2;...;6
f
g x2  x1
ð
Þ ¼
? 1
6 I 1;2;...;6
f
g x1
ð
Þ 6  jx2  7j
36


I 2;...;12
f
g x2
ð
Þ
8 x1; x2
ð
Þ
The random variables X1 and X2 are not independent, since, for example, letting
x1 ¼ 2 and x2 ¼ 4 results in 1/36 6¼ 1/72. Therefore, knowledge that an event for
X2 has occurred can affect the probability assigned to events for X1, and vice
versa.
□
2.7.2
n-Variate
The independence concept can be extended beyond the bivariate case to the case
of independence of random variables X1,. . .,Xn. The formal deﬁnition of inde-
pendence in the n-variate case is as follows:
Deﬁnition 2.25
Independence of
Random Variables
(n-Variate)
The random variables X1, X2,. . .,Xn are said to be independent iff P(xi ∈Ai,
i ¼ 1,. . .,n) ¼ Qn
i¼1 Pðxi 2 AiÞ for all choices of the events A1,. . .,An.
The motivation for the deﬁnition is similar to the argument used in the
bivariate case. For Bi ¼ ðx1; :::; xnÞ : xi 2 Ai; ðx1; :::; xnÞ 2 RðXÞ
f
g; i ¼ 1; :::; n to be
independent events, we require (recall Deﬁnition 1.19)
Pð \
j2J BjÞ ¼ P
j2J PðBjÞ 8J  f1; 2; :::; ng with NðJÞ  2:
If we require this condition to hold for all possible choices of the events (B1,. . .,Bn),
then the totality of the conditions can be represented as
Pðxi 2 Ai; i ¼ 1; :::; nÞ ¼ P
\
n
i¼1 Bi


¼ P
n
i¼1 PðBiÞ ¼ P
n
i¼1 Pðxi 2 AiÞ
for all choices of the events A1,. . .,An (or, equivalently, for corresponding choices
of B1,. . .,Bn). Any of the other conditions required for independence of events,
i.e.,
P
\
j2J Bj


¼ P
j2J PðBjÞ with J  f1; 2; :::; ng and NðJÞ<n;
are implied by the preceding condition upon letting Aj ¼ R(Xj) (or equivalently,
Bj ¼ R(X)) for j2 J.
The generalization of the joint density factorization theorem is given as
Theorem 2.7. The proof is a direct extension of the arguments used in proving
Theorem 2.6, and is left to the reader.
94
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Theorem 2.7
Density Factorization
for Independence of
Random Variables
(n-Variate Case)
The random variables X1, X2,. . .,Xn with joint PDF f(x1,. . .,xn) and marginal PDFs
fi(xi), i ¼ 1,. . .,n, are independent iff the joint density can be factored into the
product of the marginal densities as fðx1; :::; xnÞ ¼ Qn
i¼1 fi xi
ð
Þ 8 x1; . . . ; xn
ð
Þ.21
An example of the application of Theorem 2.7 is given in Section 2.8.
2.7.3
Marginal Densities Do Not Determine an n-Variate Density Without
Independence
If (X1,. . .,Xn) are independent random variables, then knowing the marginal
densities fi(xi), i ¼ 1,. . .,n is equivalent to knowing the joint density function
for (X1,. . .,Xn), since then f(x1,. . .,xn) ¼ Qn
i¼1 fi xi
ð
Þ . However, if the random
variables in the collection (X1,. . .,Xn) are not independent, then knowing each
of the marginal densities of the Xi’s is generally not sufﬁcient to determine the
joint density function for (X1,. . .,Xn). In fact, it can be shown that an uncountably
inﬁnite family of different joint density functions can give rise to the same
collection of marginal density functions.22 We provide the following counter
example in the bivariate case to the proposition that knowledge of the marginal
PDFs is sufﬁcient for determining the n-variate PDF.
Example 2.26
Marginal Densities Do
Not Imply n-Variate
Densities
Examine the function
faðx1;x2Þ ¼ ½1það2x1  1Þð2x2  1ÞI½0;1ðx1ÞI½0;1ðx2Þ:
The reader should verify that fa(x1, x2) is a PDF 8 a ∈[1,1]. For any choice of
a ∈[1,1], the marginal density function for X1 is given by f1ðx1Þ ¼
Ð 1
1 fa x1; x2
ð
Þ
dx2 ¼ I 0;1
½
 x1
ð
Þ. Similarly, the marginal density of X2, for any choice of a ∈[1,1],
is given by f2ðx2Þ ¼
Ð 1
1 fa x1; x2
ð
Þdx1 ¼ I 0;1
½
 x2
ð
Þ.
Since the same marginal density functions are associated with each of an
uncountably inﬁnite collection of bivariate density functions, it is clear that
knowledge of f1(x1) and f2(x2) is insufﬁcient to determine which is the appropri-
ate joint density function for (X1, X2). If we knew the marginal densities of X1 and
X2, as stated, and if X1 and X2 are independent random variables, then we would
know that f(x1, x2) ¼ I[0,1](x1) I[0,1](x2).
□
21The same technical proviso regarding points of discontinuity in the case of continuous random variables hold as in the bivariate case.
See Footnote 18.
22E.J. Gumbel (1958) Distributions a’ plusieurs variables dont les marges sont donne´es, C.R. Acad. Sci., Paris, 246, pp. 2717–2720.
2.7
Independence of Random Variables
95

2.7.4
Independence Between Random Vectors and Between Functions
of Random Vectors
The independence concepts can be extended so that they apply to independence
among two or more random vectors. Essentially, all that is required is to interpret the
Xi’s as multivariate random variables in the appropriate deﬁnitions and theorems
presented heretofore, and the statements are valid. Motivation for the validity of the
extensions can be provided using arguments that are analogous to those used previ-
ously. For example, to extend the previous bivariate result to two random vectors, let
X1 ¼ (X11,...,X1m) be an m-dimensional random variable and X2 ¼ (X21,...,X2n) be
an n-dimensional random variable. Then X1 and X2 are independent iff
Pðx1 2 A1; x2 2 A2Þ ¼ Pððx11;:::;x1mÞ 2 A1;ðx21;:::;x2nÞ 2 A2Þ
¼ Pððx11;:::;x1mÞ2 A1ÞPððx21;:::;x2nÞ 2 A2Þ¼ Pðx1 2 A1ÞPðx2 2 A2Þ
for all event pairs A1, A2. Furthermore, in terms of joint density factorization, X1
and X2 are independent iff
fðx1; x2Þ ¼ fðx11; :::; x1m; x21; :::; x2nÞ
¼ f1 ðx11; :::; x1mÞ f2 ðx21; :::; x2nÞ
¼ f1 ðx1Þ f2 ðx2Þ 8ðx1; x2Þ
The reader can contemplate the myriad of other independence conditions that
can be constructed for discrete and continuous random vectors.
Implications of the extended independence deﬁnitions and theorems are
qualitatively similar to the implications identiﬁed previously for the case
where the Xi’s were interpreted as scalars. For example, if X1 ¼ (X11,. . .,X1m)
and X2 ¼ (X21,. . .,X2n) are independent random variables, then
Pððx11;:::;x1mÞ 2 A1jðx21;:::;x2nÞ 2 A2ÞÞ ¼Pððx11;:::;x1mÞ 2 A1Þ;
i.e., conditional and unconditional probability of events for the random variable
X1 are identical (and similarly for X2) for all choices of A1 and A2 for which the
conditional probability is deﬁned.
It is also useful to note some results concerning the independence of random
variables that are deﬁned as functions of other independent random variables.
We begin with the simplest case of two independent random variables X1 and X2.
Theorem 2.8
If X1 and X2 are independent random variables, and if the random variables Y1
and Y2 are deﬁned by y1 ¼ Y1(x1) and y2 ¼ Y2(x2), then Y1 and Y2 are indepen-
dent random variables.
Proof
The event involving outcomes of Xi that is equivalent to the event yi ∈Ai is
given by Bi ¼ {xi: Yi(xi) ∈Ai, xi ∈R(Xi)} for i ¼ 1, 2. Then
Pðy1 2 A1;y2 2 A2Þ ¼ Pðx1 2 B1;x2 2 B2Þ
¼ Pðx1 2 B1ÞPðx2 2 B2Þ
ðby independence of x1; x2Þ
¼ Pðy1 2 A1ÞPðy2 2 A2Þ;
and since this holds for every event pair A1, A2, the random variables Y1 and Y2
are independent.
n
96
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Example 2.27
Independence of
Functions of
Continuous RVs
A large service station sells unleaded and premium-grade gasoline. The
quantities sold of each type of fuel on a given day is the outcome of a bivariate
random variable with density function23
fðx1;x2Þ ¼ 1
20eð:1 x1 þ:5 x2ÞIð0;1Þðx1ÞIð0;1Þðx2Þ;
where the xi’s are measured in thousands of gallons. The marginal densities are
given by (reader, please verify)
f1 ðx1Þ ¼ 1
10 e:1x1 Ið0;1Þ ðx1Þand f2 ðx2Þ ¼ 1
2 e:5x2 Ið0;1Þ ðx2Þ
and so the random variables are independent. The prices of unleaded and pre-
mium gasoline are $3.25 and $3.60 per gallon, respectively. The wholesale cost
of gasoline plus federal state and local taxes amounts to $2.80 and $3.00 per
gallon, respectively. Other daily variable costs in selling the two products
amount to Ci(xi) ¼ 20 x2
i , i ¼ 1, 2. Are daily proﬁts above variable costs for the
two products independent random variables?
Answer: Yes. Note that the proﬁt levels in the two cases are P1 ¼ 450x1  20x2
1
and P2 ¼ 600x2  20x2
2, respectively. Since P1 is only a function of x1, P2 is only
a function of x2, and X1 and X2 are independent, then P1 and P2 are independent
by Theorem 2.8.
□
A more general theorem explicitly involving random vectors is stated as
follows:
Theorem 2.9
Let X1,. . .,Xn be a collection of n independent random vectors, and let the
random vectors Y1,. . .,Yn be deﬁned by yi ¼ Yi(xi), i ¼ 1,. . .,n. Then the random
vectors Y1,. . .,Yn are independent.
Proof
The event involving outcomes of the random vector Xi that is equivalent to the
event Ai for the random vector Yi is given by Bi ¼ {xi: Yi(xi) ∈Ai, xi ∈R(Xi)},
i ¼ 1,. . .,n. Then
Pðyi 2 Ai;i¼1;:::;nÞ ¼ Pðxi 2 Bi;i¼1;:::;nÞ
¼ P
n
i¼1Pðxi 2 BiÞ (by independence of random vectors)
¼ P
n
i¼1 Pðyi 2 AiÞ
and since this holds for every collection of events A1,. . .,An, the random vectors
Y1,. . .,Yn are independent by a vector interpretation of the random variables in
Deﬁnition 2.19.
n
23This must be an approximation – why?
2.7
Independence of Random Variables
97

Example 2.28
Independence of
Functions of Discrete
RVs
Examine the experiment of independently tossing two fair coins and rolling
three fair dice. Let X1 and X2 represent whether heads (xi ¼ 1) or tails (xi ¼ 0)
appears on the ﬁrst and second coins, respectively, and let X3, X4, and X5
represent the number of dots facing up on each of the three dice, respectively.
Since the random variables are independent, the probability density of X1,. . .,X5
can be written as
fðx1;:::;x5Þ ¼P
2
i¼1
1
2 If0;1gðxiÞ P
5
i¼3
1
6 If1;:::;6gðxiÞ
Deﬁne two new random vectors Y1 and Y2 using the vector functions
y1 ¼
y11
y12
"
#
¼
x1 þ x2
x1 x2
"
#
¼ Y1ðx1; x2Þ;
y2 ¼
y21
y22
"
#
¼
x3 þ x4 þ x5
x3 x4 = x5
"
#
¼ Y2ðx3; x4; x5Þ
Then since the vector y1 is a function of (x1, x2), y2 is a function of (x3, x4, x5), and
since the random vectors (X1, X2) and (X3,X4,X5) are independent (why?), Theo-
rem 2.9 indicates that the random vectors Y1 and Y2 are independent. This is
clearly consistent with intuition, since outcomes of the vector Y1 obviously
have nothing to do with outcomes of the vector Y2. The reader should note that
within vectors, the random variables are not independent, i.e., Y11 and Y12 are
not independent, and neither are Y21 and Y22.
□
2.8
Extended Example of Multivariate Concepts in the Continuous Case
We now further illustrate some of the concepts of this chapter with an example
involving a trivariate continuous random variable. Let (X1, X2, X3) have the PDF
f(x1, x2, x3) ¼ (3/16) x1 x2
2 ex3I[0,2](x1) I[0,2] (x2) I[0,1) (x3).
a. What is the marginal density of X1? of X2? of X3?
Answer:
f1 x1
ð
Þ ¼
ð1
1
ð1
1
f x1; x2; x3
ð
Þdx2dx3
¼ 3
16x1I 0;2
½
 x1
ð
Þ
ð1
1
x2
2I 0;2
½
 x2
ð
Þdx2
ð1
1
ex3I 0;1
½
 x3
ð
Þdx3
¼ 3
16x1I 0;2
½
 x1
ð
Þ 8
3
 
ð1Þ ¼ 1
2x1I 0;2
½
 x1
ð
Þ:
Similarly,
f2 x2
ð
Þ ¼
ð1
1
ð1
1
f x1; x2; x3
ð
Þdx1dx3 ¼ 3
8 x2
2I 0;2
½
 x2
ð
Þ
f3 x3
ð
Þ ¼
ð1
1
ð1
1
f x1; x2; x3
ð
Þdx1dx2 ¼ ex3I 0;1
½
 x3
ð
Þ:
98
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

b. What is the probability that x1  1?
Answer: P x1  1
ð
Þ ¼
Ð 1
1 f1 x1
ð
Þdx1 ¼
Ð 2
1
1
2x1dx1 ¼
x2
1
4





2
1
¼ :75:
c. Are the three random variables independent?
Answer: Yes. It is clear that f(x1, x2, x3) ¼ f1(x1) f2(x2) f3(x3) 8 (x1,x2,x3).
d. What is the marginal cumulative distribution function for X1? for X3?
Answer: By deﬁnition,
F1 ðbÞ ¼
ðb
1
f1ðx1Þ dx1 ¼
ðb
1
1
2x1I 0;2
½
ðx1Þ dx1
¼ 1
2
x2
1
2





b
0
I½0;2 ðbÞ þ Ið2;1Þ ðbÞ ¼ b2
4 I½0;2 ðbÞ þ Ið2;1Þ ðbÞ;
F3 ðbÞ ¼
ðb
1
f3ðx3Þ dx3 ¼
ðb
1
ex3 I½0;1Þ ðx3Þ dx3
¼ ex3





b
0
I½0;1Þ ðbÞ ¼ ð1  ebÞ I½0;1Þ ðbÞ:
e. What is the probability that x1  1? that x3 > 1?
Answer: P(x1  1) ¼ F1(1) ¼ .25. P(x3 > 1) ¼ 1  F3(1) ¼ e1 ¼ .3679.
f. What is the joint cumulative distribution function for X1, X2, X3?
Answer: By deﬁnition:
Fðb1; b2; b3Þ ¼
ðb1
1
ðb2
1
ðb3
1
f x1; x2; x3
ð
Þ dx3 dx2 dx1
¼
ðb1
1
1
2 x1 I½0;2 ðx1Þ dx1
ðb2
1
3
8
ðb2
1
x2
2 I½0;2 ðx2Þ dx2
ðb3
1
ex3 I½0;1Þ ðx3Þ dx3
¼
b2
1
4 I½0;2 ðb1Þ þ Ið2;1Þ ðb1Þ
"
#
3b3
2
24 I½0;2 ðb2Þ þ Ið2;1Þ ðb2Þ


1  eb3


I½0;1Þ ðb3Þ
h
i
g. What is the probability that x1  1, x2  1, x3  10?
Answer: F(1,1,10) ¼ (1/4)(3/24)(1  e10) ¼ .031.
h. What is the conditional PDF of X1, given that x2 ¼ 1 and x3 ¼ 0?
Answer: By deﬁnition, f(x1|x2 ¼ 1,x3 ¼ 0) ¼ fðx1;1;0Þ
f23 ð1;0Þ. Also,
f23ðx2;x3Þ ¼
ð1
1
fðx1;x2;x3Þdx1¼ 3
8 x2
2I½0;2ðx2Þex3I½0;1Þðx3Þ. Thus,
fðx1jx2¼ 1;x3¼ 0Þ ¼ð 3
16Þ x1 I½0;2 ðx1Þ
3
8
¼ 1
2 x1I½0;2ðx1Þ
i. What is the probability that x1 ∈[0, 1/2], given that x2 ¼ 1 and x3 ¼ 0?
2.8
Extended Example of Multivariate Concepts in the Continuous Case
99

Answer:
Pðx1 2 ½0; 1
2j x2 ¼ 1; x3 ¼ 0Þ ¼
ð1=2
0
fðx1 j x2 ¼ 1; x3 ¼ 0Þ dx1
¼
ð1=2
0
1
2 x1 I½0;2 ðx1Þ dx1 ¼ x2
1
4





l=2
0
¼ 1
16
j. Let the two random variables Y1 and Y2 be deﬁned by y1 ¼ Y1(x1, x2) ¼ x2
1x2
and y2 ¼ Y2(x3) ¼ x3/2. Are the random variables Y1 and Y2 independent?
Answer: Yes, they are independent. The bivariate random variable (X1, X2) is
independent of the random variable X3 since f(x1, x2, x3) ¼ f12(x1,x2) f3(x3),
i.e., the joint density function factors into the product of the marginal
density of (X1, X2) and the marginal density of X3. Then, since y1 is a function
of only (x1, x2) and y2 is a function of only x3, Y1 and Y2 are independent
random variables, by Theorem 2.9.
Keywords, Phrases, and Symbols
[), interval, closed lower bound and
open upper bound
(], interval, open lower bound and
closed upper bound
[], interval, closed bounds
(), interval, open bounds
Abbreviated set notation
CDF
Classes of discrete and continuous
density functions
Composite random variable
Conditional cumulative distribution
function
Conditional density function
Continuous density component
Continuous joint PDF
Continuous PDF
Continuous random variable
Cumulative distribution function
Density factorization for
independence
Discrete density component
Discrete joint PDF
Discrete PDF
Discrete random variable
Duality between CDFs and PDFs
∃, there exists
Equivalent events
Event A is relatively certain
Event A is relatively impossible
Event A occurs with probability one
Event A occurs with probability zero
F(b)
f(x1,. . .,xm|(xm+1,. . .,xn) ∈B)
f(x1,. . .,xn)
f1. . .m(x1,. . .,xm)
Increasing function
Independence of random variables
Induced probability space, {R(X), ϒX,
PX}
J, complement of J
Marginal cumulative distribution
function
Marginal PDF
MCDF
Mixed discrete-continuous random
variables
MPDF
Multivariate cumulative distribution
function
Multivariate random variable
,, mutual implication or iff
Outcome of the random variable, x
P(x  b)
PDF
R(X)
Random variable, X
Real-valued vector function
Truncation function
X(w)
X: S ! R
100
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

Problems
1. Which of the following are valid PDFs? Justify your
answer.
a. f(x) ¼ (.2)x (.6)1x I{0,1} (x)
b. f(x) ¼ (.3) (.7)x I{0,1,2,. . .} {x}
c. f(x) ¼ .6 ex/4 I(0,1) (x)
d. f(x) ¼ x1 I[1,e] (x)
2. Graph each of the probability density functions in
Problem 1.
3. Sparkle Cola, Inc., manufactures a cola drink. The
cola is sold in 12 oz. bottles. The probability distribution
associated with the random variable whose outcome
represents the actual quantity of soda place in a bottle of
Sparkle Cola by the soda bottling line is speciﬁed to be
fðxÞ ¼ 50

e100ð12xÞIð1;12ðxÞ þ e100ðx12ÞIð12;1ÞðxÞ
	
:
In order to be considered full, a bottle must contain
within .25 oz. of 12 oz. of soda.
a. Deﬁne a random variable whose outcome indicates
whether or not a bottle is considered full.
b. What is the range of this random variable?
c. Deﬁne a PDF for the random variable. Use it to assign
probability to the event that a bottle is “considered
full.”
d. The PDF f(x) is only an approximation. Why?
4. A health maintenance organization (HMO) is cur-
rently treating 10 patients with a deadly bacterial infection.
The best-known antibiotic treatment is being used in these
cases, and this treatment is effective 95 percent of the time.
If the treatment is not effective, the patient expires.
a. Deﬁne a random variable whose outcome represents
the number of patients being treated by the HMO
that survive the deadly bacterial infection. What is
the range of this random variable? What is the event
space for outcome of this random variable?
b. Deﬁne the appropriate PDF for the random variable
you deﬁned in (a). Deﬁne the probability set function
appropriate for assigning probabilities to events
regarding the outcome of the random variable.
c. Using the probability space you deﬁned in (a) and (b),
what is the probability that all 10 of the patients
survive the infection?
d. What is the probability that no more than two
patients expire?
e. If 50 percent of the patients were to expire, the govern-
ment would require that the HMO suspend operations,
and an investigation into the medical practices of the
HMO would be conducted. Provide an argument in
defense of the government’s actions in this case.
5. Star Enterprises is a small ﬁrm that produces a prod-
uct that is simple to manufacture, involving only one
variable input. The relationship between input and out-
put levels is given by q ¼ x5, where q is the quantity of
product produced and x is the quantity of variable input
used. For any given output and input prices, Star
Enterprises
operates
at
a
level
of
production
that
maximizes its proﬁt over variable cost. The possible
prices in dollars facing the ﬁrm on a given day is
represented
by
a
random
variable
V
with
R
(V) ¼ {10,20,30} and PDF
fðvÞ ¼ :2If10gðvÞþ:5If20gðvÞþ:3If30gðvÞ:
Input prices vary independently of output prices, and
input price on a given day is the outcome of W with R
(W) ¼ {1,2,3} and PDF
gðwÞ ¼ :4If1gðwÞþ:3If2gðwÞþ:3If3gðwÞ:
a. Deﬁne a random variable whose outcome represents
Star’s proﬁt above variable cost on a given day. What
is the range of the random variable? What is the event
space?
b. Deﬁne the appropriate PDF for proﬁt over variable
cost. Deﬁne a probability set function appropriate
for assigning probability to events relating to proﬁt
above variable cost.
c. What is the probability that the ﬁrm makes at least
$100 proﬁt above variable cost?
d. What is the probability that the ﬁrm makes a positive
proﬁt on a given day? Is making a positive proﬁt a
certain event? Why or why not?
e. Given that the ﬁrm makes at least $100 proﬁt above
variable cost, what is the probability that it makes at
least $200 proﬁt above variable cost?
6. The ACME Freight Co. has containerized a large
quantity of 4-gigabyte memory chips that are to be
Problems
101

shipped
to
a
personal
computer
manufacturer
in
California. The shipment contains 1,000 boxes of mem-
ory chips, with each box containing a dozen chips. The
chip manufacturer calls and says that due to an error in
manufacturing, each box contains exactly one defective
chip. The defect can be detected through an easily
administered nondestructive continuity test using an
ohmmeter. The chip maker requests that ACME break
open the container, ﬁnd the defective chip in each box,
discard them, and then reassemble the container for ship-
ment. The testing of each chip requires 1 min to
accomplish.
a. Deﬁne a random variable representing the amount of
testing time required to ﬁnd the defective chip in a
box of chips. What is the range of the random vari-
able? What is the event space?
b. Deﬁne a PDF for the random variable you have
deﬁned in (a). Deﬁne a probability set function appro-
priate for assigning probabilities to events relating to
testing time required to ﬁnd the defective chip in a
box of chips.
c. What is the probability that it will take longer than
5 min to ﬁnd the defective chip in a box of chips?
d. If ACME uses 28-hour-shift workers for one shift
each to perform the testing, what is the probability
that testing of all of the boxes in the container will be
completed?
7. Intelligent Electronics, Inc., manufactures mono-
chrome liquid crystal display (LCD) notebook computer
screens. The number of hours an LCD screen functions
until failure is represented by the outcome of a random
variable X having range R(X) ¼ [0,1) and PDF
fðxÞ ¼ :01 exp  x
100


I½0;1ÞðxÞ:
The value of x is measured in thousands of hours. The
company has a 1-year warranty on its LCD screen, during
which time the LCD screen will be replaced free of charge
if it fails to function.
a. Assuming that the LCD screen is used for 8,760 hours
per year, what is the probability that the ﬁrm will
have to perform warranty service on an LCD screen?
b. What is the probability that the screen functions for
at least 50,000 hours? Given that the screen has
already functioned for 50,000 hours, what is the
probability that it will function for at least another
50,000 hours?
8. People Power, Inc., is a ﬁrm that specializes in
providing temporary help to various businesses. Job
applicants
are
administered
an
aptitude
test
that
evaluates mathematics, writing, and manual dexterity
skills. After the ﬁrm analyzed thousands of job applicants
who took the test, it was found that the scores on the
three tests could be viewed as outcomes of random
variables with the following joint density function (the
tests are graded on a 0–1 scale, with 0 the lowest score and
1 the highest):
fðx1; x2; x3Þ ¼ :80ð2 x1 þ3 x2Þ x3
Y
3
i¼1
I½0;1 ðxiÞ:
a. A job opening has occurred for an ofﬁce manager.
People Power, Inc., requires scores of > .75 on both
the mathematics and writing tests for a job applicant
to be offered the position. Deﬁne the marginal den-
sity function for the mathematics and writing scores.
Use it to deﬁne a probability space in which probabil-
ity questions concerning events for the mathematics
and writing scores can be answered. What is the
probability that a job applicant who has just entered
the ofﬁce to take the test will qualify for the ofﬁce
manager position?
b. A job opening has occurred for a warehouse worker.
People Power, Inc., requires a score of > .80 on the
manual dexterity test for a job applicant to be offered
the position. Deﬁne the marginal density function for
the dexterity score. Use it to deﬁne a probability
space in which probability questions concerning
events for the dexterity score can be answered. What
is the probability that a job applicant who has just
entered the ofﬁce to take the test will qualify for the
warehouse worker position?
c. Find the conditional density of the writing test score,
given that the job applicant achieves a score of > .75
on the mathematics test. Given that the job applicant
scores > .75 on the mathematics test, what is the
probability that she scores > .75 on the writing test?
Are the two test scores independent random variables?
d. Is the manual dexterity score independent of the
writing and mathematics scores? Why or why not?
102
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

9. The weekly average price (in dollars/foot) and total
quantity sold (measured in thousands of feet) of copper
wire manufactured by the Colton Cable Co. can be
viewed as the outcome of the bivariate random variable
(P,Q) having the joint density function:
fðp; qÞ ¼ 5 pepq I½:1;:3 ðpÞ Ið0;1Þ ðqÞ:
a. What is the probability that total dollar sales in a
week will be less than $2,000?
b. Find the marginal density of price. What is the proba-
bility that price will exceed $.25/ft?
c. Find the conditional density of quantity, given price
¼ .20. What is the probability that > 5,000 ft of cable
will be sold in a given week?
d. Find the conditional density of quantity, given price
¼ .10. What is the probability that > 5,000 ft of cable
will be sold in a given week? Compare this result to
your answer in (c). Does this make economic sense?
Explain.
10. A personal computer manufacturer produces both
desktop
computers
and
notebook
computers.
The
monthly proportions of customer orders received for desk-
top and notebook computers that are shipped within
1 week’s time can be viewed as the outcome of a bivariate
random variable (X,Y) with joint probability density
fðx; yÞ ¼ ð2  x  yÞ I½0;1 ðxÞ I½0;1 ðyÞ:
a. In a given month, what is the probability that more
than 75 percent of notebook computers and 75 per-
cent of desktop computers are shipped within 1 week
of ordering?
b. Assuming that an equal number of desktop and note-
book computers are ordered in a given month, what is
the probability that more than 75 percent of all orders
received will be shipped within 1 week?
c. Are the random variables independent?
d. Deﬁne the conditional probability that less than 50
percent of the notebook orders are shipped within
1 week, given that x proportion of the desktop orders
are shipped within 1 week (the probability will be a
function of the proportion x). How does this probabil-
ity change as x increases?
11. A small nursery has seven employees, three of whom
are salespersons, and four of whom are gardeners who
tend to the growing and caring of the nursery stock.
With such a small staff, employee absenteeism can be
critical. The number of salespersons and gardeners absent
on any given day is the outcome of a bivariate random
variable (X,Y). The nonzero values of the joint density
function are given in tabular form as:
Y
0
1
2
3
4
0
.75
.025
.01
.01
.03
X
1
.06
.03
.01
.01
.003
2
.025
.01
.005
.005
.002
3
.005
.004
.003
.002
.001
a. What
is
the
probability
that
more
than
two
employees will be absent on any given day?
b. Find the marginal density function of the number of
gardeners that are absent. What is the probability that
more than two gardeners will be absent on any given
day?
c. Are the number of gardener absences and the number
of
salesperson
absences
independent
random
variables?
d. Find the conditional density function for the number
of salespersons that are absent, given that there are no
gardeners absent. What is the probability that there
are no salespersons absent, given that there are no
gardeners absent? Is the conditional probability
higher or lower given that there is at least one gar-
dener absent?
12. The joint density of the bivariate random variable
(X,Y) is given by
fðx; yÞ ¼ xy I½0;1 ðxÞ I½0;2 ðyÞ:
a. Find the joint cumulative distribution function of (X,Y).
Use it to ﬁnd the probability that x  .5 and y  1.
b. Find the marginal cumulative distribution function
of X. What is the probability that x  .5?
c. Find the marginal density of X from the marginal
cumulative distribution of X.
13. The joint cumulative distribution function for (X,Y)
is given by
Fðx; yÞ ¼
1  ex=10  ey=2 þ eðxþ5yÞ=10


Ið0;1Þ ðxÞ Ið0;1Þ ðyÞ:
Problems
103

a. Find the joint density function of (X,Y).
b. Find the marginal density function of X.
c. Find the marginal cumulative distribution function
of X.
14. The cumulative distribution of the random variable
X is given by
FðxÞ ¼ ð1  pxþ1Þ If0;1;2;:::g ðxÞ; for some choice of p ∈(0,1).
a. Find the density function of the random variable X.
b. What is the probability that x  8 if p ¼ .75?
c. What is the probability that x  1 given that x  8?
15. The federal mint uses a stamping machine to make
coins. Each stamping produces 10 coins. The number of
the stamping at which the machine breaks down and
begins to produce defective coins can be viewed as the
outcome of a random variable, X, having a PDF with
general functional form f(x) ¼ a (1  b)x1 I{1, 2, 3, . . .} (x),
where b ∈(0,1).
a. Are there any constraints on the choice of a for f(x) to
be a PDF? If so, precisely what are they?
b. Is the random variable X a discrete or a continuous
random variable? Why?
c. It is known that the probability the machine will
break down on the ﬁrst stamping is equal to .05.
What is the speciﬁc functional form of the PDF f(x)?
What is the probability that the machine will break
down on the tenth stamping?
d. Continue to assume the results in (a–c). Derive a
functional representation for the cumulative distri-
bution function corresponding to the random vari-
able X. Use it to assign the appropriate probability
to the event that the machine does not break down
for at least 10 stampings.
e. What is the probability that the machine does not
break down for at least 20 stampings, given that the
machine does not break down for at least 10
stampings?
16. The daily quantity demanded of unleaded gasoline in
a regional market can be represented as Q ¼ 100  10p þ E,
where p ∈[0,8], and E is a random variable having a
probability density given by fðeÞ ¼ 0:025I 20;20
½
ðeÞ:
Quantity demanded, Q, is measured in thousands of
gallons, and price, p, is measured in dollars.
a. What is the probability of the quantity demanded
being greater than 70,000 gal if price is equal to $4?
if price is equal to $3?
b. If the average variable cost of supplying Q amount of
unleaded gasoline is given by C(Q) ¼ Q5/2, deﬁne a
random variable that can be used to represent the
daily proﬁt above variable cost from the sale of
unleaded gasoline.
c. If price is set equal to $4, what is the probability that
there will be a positive proﬁt above variable cost on a
given day? What if price is set to $3? to $5?
d. If price is set to $6, what is the probability that quan-
tity demanded will equal 40,000 gal?
17. For each of the cumulative distribution functions
listed below, ﬁnd the associated PDFs. For each CDF,
calculate P(x  6).
a. F(b) ¼ (1  eb/6) I (0,1) (b)
b. F(b) ¼ (5/3) (.6  .6trunc(b)+1)I{0,1} (b)
18. An economics class has a total of 20 students with
the following age distribution:
# of students
age
10
19
4
20
4
21
1
24
1
29
Two students are to be selected randomly, without
replacement, from the class to give a team report on the
state of the economy. Deﬁne a random variable whose
outcome represents the average age of the two students
selected. Also, deﬁne a discrete PDF for the random vari-
able. Finally, what is the probability space for this
experiment?
19. Let X be a random variable representing the mini-
mum of the two numbers of dots that are facing up after
a pair of fair dice is rolled. Deﬁne the appropriate proba-
bility density for X. What is the probability space for the
experiment of rolling the fair dice and observing the min-
imum of the two numbers of dots?
20. A package of a half-dozen light bulbs contains two
defective bulbs. Two bulbs are randomly selected from the
package and are to be used in the same light ﬁxture. Let
the random variable X represent the number of light bulbs
104
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

selected that function properly (i.e., that are not defec-
tive). Deﬁne the appropriate PDF for X. What is the prob-
ability space for the experiment?
21. A committee of three students will be randomly
selected from a senior-level political science class to pres-
ent an assessment of the impacts of an antitax initiative
to some visiting state legislators. The class consists of ﬁve
economists, eight political science majors, four business
majors, and three art majors. Referring to the experiment
of drawing three students randomly from the class, let the
bivariate random variable (X,Y) be deﬁned by x ¼ number
of economists on the committee, and y ¼ number of busi-
ness majors on the committee.
a. What is the range of the bivariate random variable
(X,Y)? What is the PDF, f(x,y), for this bivariate
random variable? What is the probability space?
b. What is the probability that the committee will con-
tain at least one economist and at least one business
major?
c. What is the probability that the committee will con-
sist of only political science and art majors?
d. On the basis of the probability space you deﬁned in
(a) above, is it possible for you to assign probability to
the event that the committee will consists entirely of
art majors? Why or why not? If you answer yes, cal-
culate this probability using f(x,y) from (a).
e. Calculate the marginal density function for the ran-
dom variable X. What is the probability that the com-
mittee contains three economists?
f. Deﬁne the conditional density function for the num-
ber of business majors on the committee, given that
the committee contains two economists. What is the
probability that the committee contains less than one
business major, given that the committee contains
two economists?
g. Deﬁne the conditional density function for the num-
ber of business majors on the committee, given that
the committee contains at least two economists.
What is the probability that the committee contains
less than one business major, given that the commit-
tee contains at least two economists?
h. Are the random variables X and Y independent? Jus-
tify your answer.
22. The Imperial Electric Co. makes high-quality porta-
ble compact disc players for sale in international and
domestic markets. The company operates two plants in
the United States, where one plant is located in the Paciﬁc
Northwest and one is located in the South. At either
plant, once a disc player is assembled, it is subjected to a
stringent quality-control inspection, at which time the
disc player is either approved for shipment or else sent
back for adjustment before it is shipped. On any given day,
the proportion of the units produced at each plant that
require adjustment before shipping, and the total produc-
tion of disc players at the company’s two plants, are
outcomes of a trivariate random variable, with the follow-
ing joint PDF:
fðx; y; zÞ ¼ 2
3ðx þ yÞ ex Ið0;1Þ ðxÞ Ið0;1Þ ðyÞ Ið0;1Þ ðzÞ;
where
x ¼ total production of disc players at the two plants,
measured in thousands of units,
y ¼ proportion of the units produced at the Paciﬁc North-
west plant that are shipped without adjustment, and
z ¼ proportion of the units produced in the southern
plant that are shipped without adjustment.
a. In this application, the use of a continuous trivariate
random variable to represent proportions and total
production values must be viewed as only an approx-
imation to the underlying real-world situation. Why?
In the remaining parts, assume the approximation is
acceptably accurate, and use the approximation to
answer questions where appropriate.
b. What is the probability that less than 50 percent of
the disc players produced in each plant will be
shipped without adjustment and that production
will be less than 1,000 units on a given day?
c. Derive the marginal PDF for the total production of
disc players at the two plants. What is the probability
that less than 1,000 units will be produced on a given
day?
d. Derive the marginal PDF for the bivariate random
variable (Y,Z). What is the probability that more
than 75 percent of the disc players will be shipped
without adjustment from each plant?
e. Derive the conditional density function for X, given
that 50 percent of the disc players are shipped from
the Paciﬁc Northwest plant without adjustment.
What is the probability that 1,500 disc players will
be produced by the Imperial Electric Co. on a day for
which 50 percent of the disc players are shipped from
the Paciﬁc Northwest plant without adjustment?
Problems
105

f. Answer (e) for the case where 90 percent of the disc
players are shipped from the Paciﬁc Northwest plant
without adjustment.
g. Are the random variables (X, Y, Z) independent ran-
dom variables?
h. Are the random variables (Y, Z) independent random
variables?
23. ACE Rentals, a car-rental company, rents three types
of cars: compacts, mid-size sedans, and large luxury cars.
Let (x1, x2, x3) represent the number of compacts, mid-size
sedans, and luxury cars, respectively, that ACE rents per
day. Let the sample space for the possible outcomes of (X1,
X2, X3) be given by
S = f (x1;x2;x3Þ :x1;x2, and x3 2 (0,1,2,3)g
(ACE has an inventory of nine cars, evenly distributed
among the three types of cars).
The discrete PDF associated with (X1, X2, X3) is given by
fðx1; x2; x3Þ ¼ :004 3 þ 2x1 þ x2
ð
Þ
1 þ x3
ð
Þ


P
3
i¼1 If0;1;2;3g ðxiÞ:
The compact car rents for $20/day, the mid-size sedan
rents for $30/day, and the luxury car rents for $60/day.
a. Derive the marginal density function for X3. What is
the probability that all three luxury cars are rented on
a given day?
b. Derive the marginal density function for (X1, X2).
What is the probability of more than one compact
and more than one mid-size sedan being rented on a
given day?
c. Derive the conditional density function for X1, given
x2  2. What is the probability of renting no more
than one compact care, given that two or more mid-
size sedans are rented?
d. Are X1, X2, and X3 jointly independent random
variables? Why or why not? Is (X1, X2) independent
of X3?
e. Derive the conditional density function for (X1, X2),
given that x3 ¼ 0. What is the probability of renting
more than one compact and more than one mid-size
sedan given that no luxury cars are rented?
f. If it costs $150/day to operate ACE Rentals, deﬁne a
random variable that represents the daily proﬁt made
by the company. Deﬁne an appropriate density func-
tion for this random variable. What is the probability
that ACE Rentals makes a positive daily proﬁt on a
given day?
24. If (X1, X2) and (X3, X4) are independent bivariate ran-
dom variables, are X2 and X3 independent random
variables? Why or why not?
25. The joint density function of the discrete trivariate
random variable (X1, X2, X3) is given by
fðx1; x2; x3Þ ¼ :20 If0;1g ðx1Þ If0;1g ðx2Þ Ifj x1  x2 jg ðx3Þ
þ :05 If0;1g ðx1Þ If0;1g ðx2Þ If1j x1  x2 jg ðx3Þ:
a. Are (X1, X2), (X1, X3), and (X2, X3) each pairwise inde-
pendent random variables?
b. Are X1, X2, X3 jointly independent random variables?
26. SUPERCOMP, a retail computer store, sells personal
computers and printers. The number of computers and
printers
sold
on
any
given
day
varies,
with
the
probabilities of the various possible sales outcomes
being given by the following table:
Number of computers sold
0
1
2
3
4
Probabilities
of
elementary
events
Number
of
printers
0
.03
.03
.02
.02
.01
1
.02
.05
.06
.02
.01
2
.01
.02
.10
.05
.05
3
.01
.01
.05
.10
.10
4
.01
.01
.01
.05
.15
a. If SUPERCOMP has a proﬁt margin (product sales
price – product unit cost) of $100 per computer sold
and $50 per printer sold, deﬁne a random variable
representing aggregate proﬁt margin from the sale of
computers and printers on a given day. What is the
range of this random variable?
b. Deﬁne a discrete density function appropriate for use
in calculating probabilities of all events concerning
aggregate proﬁt margin outcomes on a given day.
c. What is the probability that the aggregate proﬁt mar-
gin is  $300 on a given day?
d. The daily variable cost of running the store is $200/day.
What is the probability that SUPERCOMP’s aggregate
proﬁt margin on computer and printer sales will equal
or exceed variable costs on a given day?
106
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

e. Assuming that events involving the number of
computers and printers sold are independent from
day to day, what is the probability that for any given
6-day business week, aggregate proﬁt margins equal
or exceed variable cost all 6 days?
27. Given the function deﬁnitions below, determine
which can be used as PDFs (PDFs) and which cannot.
Justify your answers.
a. fðxÞ ¼
1
4
 x for
x ¼ 0; 1; 2; :::
0
otherwise

b. fðxÞ ¼
1
4
 x
I 0;1
ð
ÞðxÞ
c. f x; y
ð
Þ ¼
2x þ y
ð
Þ=100; for
x and y ¼ 0; 1; 2; 3; 4; and y  x
0
otherwise

d. f x; y
ð
Þ ¼ 6xy2I 0;1
½
ðxÞI 0;1
½
ðyÞ
28. Given the function deﬁnitions below, determine
which can be used as cumulative distribution functions
(CDFs) and which cannot. Justify your answers.
a. FðcÞ ¼
ec
1 þ ec for c 2 1; 1
ð
Þ
b. FðcÞ ¼
1  x2; for
c 2 1; 1
ð
Þ
0
otherwise

c. FðcÞ ¼
1  :5
ð
ÞfloorðcÞ for
c  1
0
otherwise:

where ﬂoor (c)  round down the value c.
d.
F c1; c2
ð
Þ ¼
1 if c1 and c2 2 1; 1
ð
Þ
c3
1I 0;1
½
 c1
ð
Þif c2 2 1; 1
ð
Þ
c2
2I 0;1
½
 c2
ð
Þif c1 2 1; 1
ð
Þ
c3
1c2
2I 0;1
½
 c1
ð
ÞI 0;1
½
 c2
ð
Þ for c1 and c2 2 1; 1
ð
Þ
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
e. F c1; c2
ð
Þ ¼ 1  ec1
ð
Þ 1  ec2
ð
ÞI 0;1
½
Þ c1
ð
ÞI 0;1
½
Þ c2
ð
Þ
29. For those functions in (28) that are actually cumulative
distribution functions (CDFs), use the duality principle to
derive the PDFs (PDFs) that are associated with the CDFs.
30. The daily quantity demanded of milk in a regional
market, measured in 1,000’s of gallons, can be represented
during the summer months as the outcome of the follow-
ing random variable:
Q¼ 200  50pþV;
where V is a random variable having a probability density
deﬁned by
fðvÞ ¼ 0:02I 25;25
½
ðvÞ and p is the price of milk, in dollars
per gallon.
a. What is the probability that the quantity demanded
will be greater than 100,000 gal if price is equal to $2?
if price is equal to $2.25?
b. If the variable cost of supplying Q amount of milk is
given by the cost function CðQÞ ¼ 20 Q5; deﬁne a
random variable that represents the daily proﬁt
above variable cost from the sale of milk.
c. If price is equal to $2, what is the probability that
there will be a positive proﬁt above variable cost on a
given day? What if price is set to $2.25?
d. Is there any conceptual problem with using the
demand function listed above to model quantity
demanded if p ¼ 4? If so, what is it?
31. A small locally-owned hardware store in a western
college town accepts both cash and checks for purchasing
merchandise from the store. From experience, the store
accountant has determined that 2 percent of the checks
that are written for payment are “bad” (i.e., they are
refused by the bank) and cannot be cashed. The accoun-
tant deﬁnes the following probability model
RðXÞ; fðxÞ
ð
Þ
for the outcome of a random variable X denoting the
number of bad checks that occur in n checks received on
a given day at the store:
fðxÞ ¼
n!
nx
ð
Þ!x! :02
ð
Þx :98
ð
Þnx for
x 2 RðXÞ ¼ 0; 1; 2; :::; n
f
g
0
elsewhere
8
<
:
If the store receives 10 checks for payment on a given day,
what is the probability that:
a. Half are bad?
b. No more than half are bad?
c. None are bad?
d. None are bad, given that no more than half are bad?
32. Let an outcome of the random variable T represent
the time, in minutes, that elapses between when an order
is placed at a ticket counter by a customer and when the
ticket purchase is completed. The following probability
model
RðTÞ; fðtÞ
ð
Þ governs the behavior of the random
variable T:
Problems
107

fðtÞ ¼
3e3t for t
2 R T
f g ¼ 0; 1
½
Þ
0
elsewhere

a. What is the probability that the customer waits less
than 3 min to have her ticket order completed?
b. Derive the cumulative distribution function for T.
Use it to deﬁne the probability that it takes longer
than 10 min to have the ticket order completed.
c. Given that the customer’s wait will be less than
3 min, what is the probability that it will be less
than 1 min?
d. Given that the customer has already waited more
than 3 min, what is the probability that the customer
will wait at least another 3 min to have the ticket
order completed?
33. Outcomes of the random variable Z represent the
number of customers that are waiting in a queue to be
serviced at Fast Lube, a quick stop automobile lubrication
business, when the business opens at 9 A.M. on any given
Saturday. The probability model
RðZÞ; fðzÞ
ð
Þfor the ran-
dom variable Z is given by:
fðzÞ ¼
:5zþ1 for z
2 RðZÞ ¼ 0; 1; 2; 3; :::
f
g
0
elsewhere

a. Derive the cumulative distribution function for Z.
b. What is the probability that there will be less than 10
people waiting?
c. What is the probability that there will be more than 3
people waiting?
d. Given that no more than two people will be waiting,
what is
the probability that there will be no
customers when business opens at 9 A.M.?
34. The daily wholesale price and quantity sold of etha-
nol in a Midwestern regional market during the summer
months is represented by the outcome of a bivariate ran-
dom variable (P,Q) having the following probability model
(R(P,Q),f (p,q)):
f p; q
ð
Þ
:5pepq for
p; q
ð
Þ 2 R P; Q
ð
Þ ¼ 2; 4
½
 	 0; 1
½
Þ
0
elsewhere

where price is measured in dollars and quantity is
measured
in
100,000
gal
units
(e.g.,
q ¼ 2
means
200,000 gal were sold).
a. Derive the marginal probability density of price. Use it
to determine the probability that price will exceed $3.
b. Derive the marginal cumulative distribution func-
tion for price. Use it to verify your answer to part (a)
above.
c. Derive the marginal probability density of quantity.
Use it to determine the probability that quantity sold
will be less than $500,000 gal.
d. Let the random variable D ¼ PQ denote the daily
total dollar sales of ethanol during the summer
months. What is the probability that daily total dollar
sales will exceed $300,000?
e. Are P and Q independent random variables?
35. The BigVision Electronic Store sells a large 73 inch
diagonal big screen TV. The TV comes with a standard
1 year warranty on parts and labor so that if anything
malfunctions on the TV in the ﬁrst year of ownership,
the company repairs or replaces the TV for free. The store
also sells an “extended warranty” which a customer can
purchase that extends warranty coverage on the TV for
another 2 years, for a total of 3 years of coverage. The daily
numbers of TVs and extended warranties sold can be
viewed as the outcome of a bivariate random variable (T,
W) with probability model (R(T,W), f (t,w)) given by
f t; w
ð
Þ
2t þ w
ð
Þ=100; for
t and w ¼ 0; 1; 2; 3; 4; and w  t
0
otherwise
(
a. What is the probability that all of the TVs sold on a
given day will be sold with extended warranties?
b. Derive the marginal density function for the number
of TVs sold. Use it to deﬁne the probability that
2 TVs are sold on a given day?
c. Derive the marginal density function for the number
of warranties sold. What is the probability that 3
warranties are sold on a given day?
d. Are T and W independent random variables?
36. The following function is proposed as a cumulative
distribution function for the bivariaterandom variable (X,Y):
F x; y
ð
Þ ¼
1 þ e x=10þy=20
ð
Þ  ex=10  ey=20


I 0;1
ð
ÞðxÞI 0;1
ð
ÞðyÞ
a. Verify
that
the
function
has
the
appropriate
properties to serve as a cumulative distribution
function.
b. Derive the marginal cumulative distribution func-
tion of Y.
108
Chapter 2
Random Variables, Densities, and Cumulative Distribution Functions

c. Derive the marginal PDF of Y.
d. Derive the joint PDF of (X,Y).
e. What is the probability that X  10 and Y  20?
f. Are X and Y independent random variables?
37. For each of the joint PDFs listed below, determined
which random variables are independent and which are
not.
a. f x; y
ð
Þ ¼ e xþy
ð
ÞI 0;1
½
ÞðxÞI 0;1
½
ÞðyÞ
b. f x; y
ð
Þ ¼ x 1 þ y
ð
Þ
300
I 1;2;3;4;5
f
gðxÞI 1;2;3;4;5
f
gðyÞ
c. f x; y; z
ð
Þ ¼ 8xyzI 0;1
½
ðxÞI 0;1
½
ðyÞI 0;1
½
ðzÞ
d. f x1; x2; x3
ð
Þ ¼ :5x1:2x2:75x3
10
Y
3
i¼1
I 0;1;2;:::
f
g xi
ð
Þ
38. The daily wholesale price and quantity sold of etha-
nol in a Midwestern regional market during the summer
months is represented by the outcome of a bivariate ran-
dom variable (P,Q) having the following probability model
{R(P,Q),f (p,q)}:
f p; q
ð
Þ ¼
:5pepq for
p; q
ð
Þ 2 R P; Q
ð
Þ ¼ 2; 4
½
 	 0; 1
½
Þ
0
elsewhere

where price is measured in dollars and quantity is
measured
in
100,000
gal
units
(e.g.,
q ¼ 2
means
200,000 gal were sold).
a. Derive the conditional-on-p PDF for quantity sold.
b. What is the probability that quantity sold exceeds
50,000 gal if price ¼ $2. What is the probability that
quantity sold exceeds 50,000 gal if price ¼ $4. Does
this make economic sense?
c. What is the probability that quantity sold exceeds
50,000 gal if price is greater than or equal to $3.00?
39. Let the random variable X represent the product of
the number of dots facing up on each die after a pair of fair
dice is rolled. Let Y represent the sum of the number of
dots facing up on the pair of dice.
a. Deﬁne a probability model (R(X),f (x)) for the random
variable X.
b. What is the probability that X  16?
c. Deﬁne a probability model (R(X,Y),f(x,y)) for the ran-
dom vector (X,Y).
d. What is the probability that X  16 and Y  8?
e. Are X and Y independent random variables?
f. Deﬁne the conditional PDF of X given that Y ¼ 7.
g. What is the probability that X  10 given that Y ¼ 7?
40. The production of a certain volatile commodity is the
outcome of a stochastic production function given by Y
¼ L:5K:25ev , where v is a random variable having the
cumulative distribution function
FðvÞ ¼
1
1þe2 v1
ð
Þ , L
denotes units of labor and K denotes units of capital.
a. If labor is applied at 9 units and capital is applied at
16 units, what is the probability that output will
exceed 12 units?
b. Given the input levels applied in (a), what is the
probability that output will be between 12 and
16 units?
c. What level of capital and labor should be applied so
that the probability of producing a positive proﬁt is
maximized when output price is $10, labor price is
$5, and capital price is $10?
d. What is the value of the maximum probability of
obtaining positive proﬁt?
Problems
109

3
n
Expectations and Moments
of Random Variables
n
n
n
3.1
Expectation of a Random Variable
3.2
Stieltjes Integration and Implicit Support Convention
3.3
Expectation of a Function of Random Variables
3.4
Expectation Properties
3.5
Conditional Expectation
3.6
Moments of a Random Variable
3.7
Moment Generating Functions
3.8
Cumulant Generating Function
3.9
Multivariate MGFs and Cumulant Generating
Functions
3.10
Joint Moments, Covariance, and Correlation
3.11
Means and Variances of Linear Combinations
of Random Variables
3.12
Appendix: Proofs and Conditions for Positive
Semideﬁniteness
3.1
Expectation of a Random Variable
The deﬁnition of the expectation of a random variable can
be motivated both by the concept of a weighted average and through the use of
the physics concept of center of gravity, or the balancing point of a distribution
of weights. We ﬁrst examine the case of a discrete random variable and look at a
problem involving the balancing-point concept.1
Example 3.1
Balancing Weights
Suppose that a weightless rod is placed on a fulcrum, a weight of 10 lb is placed
on the rod exactly 4 ft to the right of the fulcrum, and a weight of 5 lb is placed on
the rod exactly 8 ft to the left of the fulcrum, as shown in Figure 3.1.
1Readers who recollect earlier days spent on a seesaw should possess ample intuition regarding the placement of weights appropriate
distances from a fulcrum so as to achieve a “balanced seesaw.”

Assume that d ¼ 0 is the point at which the fulcrum is placed, so that the
10 lb weight is at the point x1 ¼ 4, and the 5 lb weight is at the point x2 ¼ 8. Let
mass(x) denote the mass placed at point x. The moment of any mass placed at a
point x is deﬁned to be the product of the mass times its signed distance from the
fulcrum, [mass(x)] (x  d), where d is the point at which the fulcrum is placed.
Thus, the moment of the 10 lb weight is 10(4  0) ¼ 40, while the moment of
the 5 lb pound weight is 5(8  0) ¼ 40. A system of weights with fulcrum
placed at d will balance if the sum of the moments Pn
i¼1 [mass(xi)](xi  d), called
the total moment of the system, is equal to zero. Our system balances, since
40 + (40) ¼ 0.
The moments concept illustrated in Example 3.1 can be used to identify the
point at which a probability density “balances.” Recall the dice example of
Example 2.2. We place the probability “weights” on a weightless rod at the
points corresponding to the outcomes with which the probabilities are
associated, as shown in Figure 3.2.
□
At what point should the fulcrum be placed so that the distribution of
weights balances? We require that the total moment of the system be zero.
Thus, we require that
5
10
x1
x2
8 ft.
4 ft.
δ
Figure 3.1
Weights on a
weightless rod.
2
3
4
5
6
7
8
9
10
11
12
x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
x11
1/36
2/36
1/36
2/36
3/36
4/36
3/36
5/36
4/36
5/36
6/36
Figure 3.2
Density function “weights”
on a weightless rod.
112
Chapter 3
Expectations and Moments of Random Variables

X
11
i¼1
mass xi
ð
Þ
½
 xi  d
ð
Þ ¼
X
11
i¼1
f xi
ð
Þ xi  d
ð
Þ ¼ 0;
which implies
X
11
i¼1
f xi
ð
Þxi ¼ d
X
11
i¼1
f xi
ð
Þ
"
#
¼ d;
where the sum in brackets equals one because the density function f(x) is being
summed over the entire range of the random variable X. Substituting the appro-
priate values of xi and f(xi) in the expression obtains the result d ¼ 7. Thus, if
the fulcrum were placed at the point 7, the system of weights would balance.
The quantity d is precisely what is meant by the expected value of the discrete
random variable X with density function f(x). Thus, the expected value of a
discrete random variable is a measure of the center of gravity of its density
function.
Deﬁnition 3.1
Expectation of a
Random Variable:
Discrete Case
The expected value of a discrete random variable is deﬁned by
EðXÞ ¼
P
x2RðXÞ xfðxÞ; provided the sum exists.2
Since fðxÞ  0 8x 2 RðXÞ and P
x2RðXÞ fðxÞ ¼ 1 , the expected value of a
discrete random variable can also be straightforwardly interpreted as a weighted
average of the possible outcomes (or range elements) of the random variable. In
this context the weight assigned to a particular outcome of the random variable
is equal to the probability that the outcome occurs (as given by the value of f(x)).
Example 3.2
Expected Gain from
Insurance
A life insurance company offers a 50-year old male a $1,000 face value, 1-year
term life insurance policy for a premium of $14. Standard mortality tables
indicate that the probability a male in this age category will die within the
year is .006. What is the insurance company’s expected gain from issuing this
policy?
Answer: Deﬁne a random variable X having range R(X) ¼ {14, 986}, the
outcomes corresponding, respectively, to the premium of $14 collected
2 It is an unfortunate fact of inﬁnite sums involving both positive and negative terms that, unless the sum is absolutely convergent, an
appropriate reordering of terms will result in the inﬁnite sum converging to other real numbers (Bartle, Real Analysis, p. 292). This is
hardly consistent with the notion of a balancing point of the density f(x). Moreover, the nonuniqueness of values to which the inﬁnite
sum can converge makes any particular convergence point arbitrary and meaningless as the expectation of X. Thus, to ensure the
ﬁniteness and uniqueness of the converged value in the countably inﬁnite case, a technical condition can be added whereby E(X) is
said to exist iff P
x2RðXÞ x
j jfðxÞ<1 which is to say, iff P
x2RðXÞ xfðxÞ is absolutely convergent. For virtually any problem of practical
interest, if the sum used in the deﬁnition of the expectation is ﬁnite, the expectation can be said to exist. It should also be noted that in
many applications, random variables are nonnegative valued, in which case if the sum is convergent, it is necessarily absolutely
convergent.
3.1
Expectation of a Random Variable
113

by the company if the person lives, or the net payment of $986 ($1,000 minus
the premium collected) to the person’s estate if he dies. The probabilities of
the two elementary events are .994 and .006, respectively. Then, E(X) ¼
(14) (.994)  (986) (.006) ¼ 8.
□
Note that the expected value of X need not be a value in the range of X as the
previous and following examples illustrate.
Example 3.3
Expected Value not
Necessarily in R(X)
Examine the experiment of rolling a die, and recall that the density function
associated with the dots facing up on the die is f(x) ¼ (1/6) I{1,. . ., 6}(x). In this case
R(X) ¼ {1, 2, 3, 4, 5, 6}. The expected value of X equals E(X) ¼ P
x2RðXÞ xfðxÞ ¼
P6
x¼1 x=6
ð
ÞI 1;...;6
f
gðxÞ ¼ 3:5, and thus E(X) =2R(X).
□
In the continuous case, the physics problem of balancing mass on a weight-
less rod can no longer be conceptualized as having weights applied to speciﬁc
points on the rod. Instead, the mass is interpreted as being continuously spread
out along the rod, exerting downward force along a continuum of points
on the rod. The mass function, mass(x), is now a density of the mass at point
x,
R b
a massðxÞdx equals the total mass placed on the rod, and
R b
a massðxÞdx equals
the mass lying between the points a and b (see Figure 3.3)3. The mass is balanced
on the rod when the fulcrum is placed at the point d such that the total moment
of the mass,
R 1
1 massðxÞðx  dÞdx, equals zero.
Viewing our density function as a probability mass, the continuous density
“balances” with a fulcrum placed at the point d if R 1
1 fðxÞ x  d
ð
Þdx ¼ 0, which
implies that
Z 1
1
xfðxÞdx ¼ d
Z 1
1
fðxÞdx
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
1
¼ d:
Again, it is this balancing point or center of gravity, d, of the density that represents
the expectation of the continuous random variable X having density f(x).
mass (x) dx
b
a∫
a
b
x
Figure 3.3
Continuous mass on a
weightless rod.
3 The reader might notice that the mass function would exhibit properties similar to a probability density function, except the integral
over the real line would not necessarily ¼ 1, but rather equals the number reﬂecting the total mass placed on the rod.
114
Chapter 3
Expectations and Moments of Random Variables

Deﬁnition 3.2
Expected Value of a
Random Variable:
Continuous Case
The expected value of the continuous random variable X is deﬁned by
EðXÞ ¼
R 1
1 xfðxÞdx, provided the integral exists.
The expected value of X in the continuous case can also be viewed, in a limit
sense, as a weighted average of the possible outcomes (or range elements) of
the random variable. This interpretation follows fundamentally from the deﬁni-
tion of the deﬁnite integral as the limit of a Riemann sum.4 For the sake of
exposition, we assume that the positive values of f(x) all occur for x within the
interval x∈[a,b] for ﬁnite a and b, although a similar argument holds when a
and/or b are inﬁnite.
Let xo ¼ a, xn ¼ b, xo < x1 < x2 < . . . < xn, Dxi ¼ xi  xi  1, and examine
the Riemann sum Pn
i¼1 x0
i f(x0
i )Dxi, where x0
i is a value chosen such that x0
i
∈[xi  1, xi]. This situation can be represented by the diagram in Figure 3.4. Thus,
each x0
i is weighted by the value f x0
i


Dxi, an area indicated in the diagram by a
shaded rectangle, and a summation is taken over all the chosen values of the x0
i .
We have effectively divided the interval [a,b] into a collection of subintervals of
various widths Dxi, i ¼ 1,. . .,n. The subinterval of maximum width is referred to
as the mesh (or sometimes, the norm) of the collection of subintervals, i.e.,
mesh ¼ max (Dx1,. . .,Dxn). If f(x) is continuous,5 then as we increase without
bound the number of subintervals, and in so doing decrease the mesh to zero, we
have
f(x)
b
x4
0
x3
0
x2
0
x1
0
a
Δx1
Δx2
Δx3
Δx4
x4
x3
x2
x1
x0
Figure 3.4
Approximation of area
under f(x).
4 Bartle, Real Analysis, pp. 213–214.
5The argument can still be applied to cases where there are a ﬁnite number of discontinuities.
3.1
Expectation of a Random Variable
115

lim
n!1; mesh!0
X
n
i¼1
x0
i f x0
i


Dxi ¼
Z b
a
xfðxÞdx ¼
Z 1
1
xfðxÞdx
and
lim
n!1; mesh!0
X
n
i¼1
f x0
i


Dxi ¼
Z b
a
fðxÞdx ¼
Z 1
1
fðxÞdx ¼ 1
(presuming the ﬁrst limit, and hence ﬁrst integral, exists—the second limit, and
integral, necessarily exists since f(x) is a density function). Therefore, under the
assumptions of our argument, the expected value of X can be viewed, in a
limiting sense, as a weighted average of an inﬁnite number of possible outcomes
of the random variable.
Example 3.4
Expectation of a
Continuous RV
A large domestic automobile manufacturer mails out quarterly customer satis-
faction surveys to owners who have purchased new automobiles within the last
3 years. The proportion of surveys returned in any given quarter is the outcome
of a random variable X having density function f(x) ¼ 3x2I[0,1](x). What is the
expected proportion of surveys returned in any given quarter?
Answer: By deﬁnition,
EðXÞ ¼
Z 1
1
xfðxÞdx ¼
Z 1
1
x 3 x2 I½0;1 ðxÞ


dx ¼
Z 1
0
3 x3 dx ¼ 3 x4
4
1
0
 ¼ :75
This is represented diagrammatically in Figure 3.5.
□
In applications, the following result can often be a useful sufﬁcient condi-
tion for the existence of the expected value in either the discrete or continuous
case.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7E(x) 0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
Δ
Figure 3.5
f(x) ¼ 3x2I[0,1](x)
116
Chapter 3
Expectations and Moments of Random Variables

Theorem 3.1
Existence of E(X) for
Bounded R(X)
If x
j j< c 8x∈R(X) for some choice of c∈(0,1), then E(X) exists.
Proof
By the assumption of the theorem, f(x) ¼ 0 8x such that x
j j  c. Then
Discrete:
S
x2RðXÞ jxjfðxÞ<
S
x2RðXÞ cfðxÞ ¼ c
S
x2RðXÞ fðxÞ ¼ c<1; so that E(X) exists.
Continuous:
R 1
1 x
j jfðxÞdx<
R 1
1 cfðxÞdx ¼ c
R 1
1 fðxÞdx ¼ c<1, so that E(X) exists.
n
The theorem indicates that expectations exist for any random variable
whose outcomes are bounded in absolute value because the respective sum or
integral is absolutely convergent.
3.2
Stieltjes Integration and Implicit Support Convention
We will henceforth introduce a convention that serves to economize on notation
whereby we tacitly assume that whenever a discrete density function is involved
in a summation expression, summation is understood to occur only over the
countable number of points for which f(x) > 0, i.e., only over points in the
range or the support of the random variable (recall Deﬁnitions 2.8 and 2.9).
Thus, we will generally abbreviate expressions such as
P
x2A; fðxÞ>0 fðxÞ or
P
x2A\RðXÞ fðxÞ by P
x2A fðxÞ, the condition f(x) > 0 or x 2 RðXÞalways being
implicitly understood to apply.
We will also make occasional use of the Stieltjes integration operation
(pronounced “Steel-yah”) when it is useful in deﬁning or proving concepts for
both the discrete and continuous random variables simultaneously. This type of
integration encompasses summation as a special case, when based on the differ-
ential of the cumulative distribution function of a discrete random variable. The
Stieltjes integration concept actually generalizes the operation of Riemann
integration in a number of important ways, but for our purposes, it sufﬁces to
think of the operation as simply deﬁning a notation for representing either
summation or ordinary Riemann integration when we have a discrete or contin-
uous random variable case, respectively. We deﬁne the operator applied in our
context below.
Deﬁnition 3.3
Stieltjes Integration
Notation
Let FðxÞbe the cumulative distribution function associated with the random
variable X. Then the Stieltjes integral of the function gðxÞ with respect to F is
deﬁned as
Z
x2A
gðxÞ dFðxÞ 
P
x2A
gðxÞfðxÞ
R
x2A
gðxÞ fðxÞdx
8
<
:
9
=
;when X is
discrete
continuous


:
3.2
Stieltjes Integration and Implicit Support Convention
117

Note we have already employed our implicit support convention in the
discrete case. The deﬁnition can be applied to both scalar and multivariate
cases, where one simply interprets the x argument in the preceding deﬁnition
as a vector to deﬁne the notation for the multivariate application. Using Stieltjes
integration notation, one can express the expectation of a random variable X, for
both the discrete and continuous cases, as follows.
Deﬁnition 3.4
Expected Value of a
Random Variable (via
Stieltjes Integration)
The expected value of a random variable X is deﬁned by EðXÞ ¼ R 1
1 x dFðxÞ,
provided the integral exists.
Based on the above discussion of the Stieltjes integral notation, if X is
discrete,
EðXÞ ¼
Z 1
1
x dFðxÞ ¼
X
x2RðXÞ
x fðxÞ;
and if X is continuous,
EðXÞ ¼
Z 1
1
x dFðxÞ ¼
Z 1
1
x fðxÞdx:
3.3
Expectation of a Function of Random Variables
Many cases arise in practice where one is interested in the expectation of a
function of a random variable rather than the expectation of a random variable
itself. For example, the proﬁt on a stock investment will be a function of the
difference between the per share buying and selling prices of the stock, and the
net return on an advertising campaign is a function of consumer buying response
to the campaign—both the stock selling price and consumer buying response
might be viewed as random variables. How should E(Y) be determined when
y ¼ g(x), x∈R(X), and X has density function f(x)? By deﬁnition, if we know the
density of Y, h(y), then
EðYÞ ¼
Z 1
1
y dHðyÞ ¼
P
y2RðYÞ
yhðyÞ ðdiscreteÞ
R 1
1 yhðyÞdy
continuous
ð
Þ
8
<
:
where HðyÞ is the cumulative distribution function. To use this expectation
deﬁnition directly, one would need to establish the density of Y. This
can be done in principle by exploiting the functional relationship between
y and x, given knowledge of the density f(x), but ﬁnding h(y) can sometimes
be challenging (we will examine methods for deriving such densities in
Chapter 6). Fortunately, one does not need to derive the density function of y
to obtain E(Y).
118
Chapter 3
Expectations and Moments of Random Variables

Since Y is deﬁned via a composition of the functions g and X, and since the
domain of X is conceptually the sample space S, then Y is deﬁned on the
elements of S via the composition, i.e., an outcome of y can be viewed as being
given by y ¼ g(X(w)) for w∈S, so that y: S ! R. This implies that the range of Y
and probabilities of events for Y can be represented alternatively as
RðY) = f y:y¼gðxÞ;x 2 RðX)g = f y:y¼gðXðwÞÞ;w 2 Sg
and
PYðAÞ ¼PX(f x:gðxÞ 2 A;x 2 RðX)g ) = P(f w:gðXðwÞÞ 2 A;w 2 Sg ):
Therefore, we can concentrate our attention on the g function component of the
composition, which has a real-valued domain R(X), and conceptualize the
outcomes of Y as being generated by y ¼ g(x) for x∈R(X), where y: R(X) ! R.
In so doing, we lose no information concerning the possible outcomes of Yor the
probabilities of events for Y, and we gain the convenience of being able to ignore
the original probability space {S,ϒ,P} and deal exclusively with a real-valued
domain for the function Y. We will generally focus on this latter interpretation
of the function Y in our subsequent study, unless we make explicit reference to
the domain of Y as being S.
We now present a theorem identifying a straightforward approach for
obtaining the expectation of Y ¼ g(X) by using density weightings applied to
the outcomes of X.
Theorem 3.2
Expectations of
Functions of Random
Variables
Let X be a random variable having density function f(x). Then the expectation of
Y ¼ g(x) is given by6
EðgðXÞÞ ¼
Z 1
1
gðxÞdFðxÞ ¼
P
x2RðXÞ
gðxÞfðxÞ ðdiscreteÞ
R 1
1 gðxÞfðxÞdx ðcontinuousÞ
8
<
:
Proof
See Appendix
n
Example 3.5
Expectation of Proﬁt
Function
Let the daily proﬁt function of a ﬁrm be given by P(X) ¼ pq(X)  rX, where X is a
random variable whose outcome represents the daily quantity of a highly per-
ishable agricultural commodity delivered to the ﬁrm for processing, measured in
hundredweights (100 lb units), p ¼ 5 is the price of the processed product
per pound, r ¼ 2 is the cost of the raw agricultural commodity per pound, and
qðxÞ ¼ x:9 is the production function indicating the relationship between raw
6It is tacitly assumed that the sum and integral are absolutely convergent for the expectation to exist.
3.3
Expectation of a Function of Random Variables
119

and ﬁnished product measured in hundredweights. Let the density function of X
be f(x) ¼ 1þ2x
110 I[0,10](x). What is the expected value of daily proﬁt?
Answer: A direct application of Theorem 3.2 yields
E PðXÞ
ð
Þ ¼
Z 1
1
PðxÞfðxÞdx ¼
Z 10
0
5x:9  2x

 1 þ 2x
110

	
dx ¼ 13:77:
Since quantities are measured in hundredweights, this means that the expected
proﬁt is $1,377 per day.
□
Example 3.6
Expectation of Per Unit
Proﬁt Function
Your company manufactures a special 1/4-inch hexagonal bolt for the Defense
Department. For the bolt to be useable in its intended application, the bolt must
be manufactured within a 1 percent tolerance of the 1/4-inch speciﬁcation. As
part of your quality assurance program, each bolt is inspected by a laser measur-
ing device that is 100 percent effective in detecting bolts that are not within the
1 percent tolerance. Bolts not meeting the tolerance are discarded. The actual
size of a bolt manufactured on your assembly line is represented by a random
variable, X, having a probability density f(x) ¼ (.006)1 I[.247, .253](x), where x is
measured in inches. If your proﬁt per bolt sold is $.01, and if a discarded bolt
costs your company $.03, what is your expected proﬁt per bolt manufactured?
Answer: We deﬁne a discrete random variable whose outcome represents
whether a bolt provides the company with a $.01 proﬁt or a $.03 loss. Speciﬁ-
cally, Y ¼ g(X) ¼ .01(I[.2475, .2525] (X)).03 (1I[.2475, .2525] (X)) is the function of X
that we seek, where y ¼ .01 if x ∈[.2475, .2525] (i.e., the bolt is within tolerance)
and y ¼  .03 otherwise. Then
EðYÞ ¼ E gðXÞ
ð
Þ ¼
Z :253
:247
gðxÞfðxÞdx
¼ :01Pð:2475  x  :2525Þ  :03½1  Pð:2475  x  :2525Þ
¼ :01ð:833Þ  :03ð:166Þ ¼ :0033:
□
The reader should note that in the preceding example, while X was a contin-
uous random variable, Y ¼ g(X) is a discrete random variable. Whether Y ¼ g(X)
is discrete or continuous depends on the nature of the function g and whether X
is discrete or continuous. The reader should convince herself that if X is discrete,
then Y must be discrete, but if X is continuous, then Y can be continuous or
discrete (or mixed discrete-continuous).
Upon close examination of Example 3.6, the reader may have noticed that
the expectation of an indicator function equals the probability of the set being
indicated. In fact, any probability can be represented as an expectation of an
appropriately deﬁned indicator function.
Theorem 3.3
Probabilities Expressed
as Expectations
Let X be a random variable with density function f(x), and suppose A is an event
for X. Then E IAðXÞ
ð
Þ ¼ PðAÞ.
120
Chapter 3
Expectations and Moments of Random Variables

Proof
By deﬁnition,
E IAðXÞ
ð
Þ ¼
P
x2RðXÞ
IAðxÞfðxÞ ¼ P
x2A
fðxÞ ðdiscreteÞ
R1
1
IAðxÞfðxÞdx ¼
R
x2A
fðxÞ dx ðcontinuousÞ
8
>
>
<
>
>
:
9
>
>
=
>
>
;
¼ PðAÞ
n
It should be noted that the existence of E(X) does not imply that E(g(X))
exists, as the following example illustrates.
Example 3.7
Existence of EðXÞ6 )
Existence of E gðXÞ
ð
Þ
Let
X
be
a
random
variable
with
density
function f(x) ¼ (1/2)I{0,1}(x).
Then
EðXÞ ¼ 0  1=2 þ 1  1=2 ¼ 1=2. Deﬁne a new random variable Y ¼ g(X) ¼ X1. Since
1=0
j
j 1=2
ð
Þ þ 1=1
j
j 1=2
ð
Þ≮1, E(g(X)) does not exist.
□
The preceding example also illustrates that, in general, E(g(X)) 6¼ g(E(X)),
because E(X) ¼ 1/2, so that g(E(X)) ¼ (E(X))1 ¼ 2, which does not equal E(g(X))
since E(g(X)) does not exist. In the special case where the function g is either
concave or convex,7 there is a deﬁnite relationship between E(g(X)) and g(E(X)), as
indicated by the following theorem.
Theorem 3.4
Jensen’s Inequalities
Let X be a random variable with expectation E(X), and let g be a continuous
function on an open interval I containing R(X). Then
(a) E(g(X))  g(E(X)) if g is convex on I, and E(g(X)) > g(E(X)) if g is strictly
convex on I and X is not degenerate8;
(b) E(g(X))
 g(E(X)) if g is concave on I, and E(g(X)) < g(E(X)) if g is strictly
concave on I and X is not degenerate.
Proof
See Appendix.
n
Example 3.8
Expectation of Concave
Function
Suppose that the yield per acre of a given agricultural crop under standard
cultivation practices is represented by Y ¼ 5X  .1X2, where outcomes of Y are
measured in bushels, and X represents total rainfall during the growing season,
measured in inches. If E(X) ¼ 20, can you place an upper bound on the expected
yield per acre for this crop?
Answer: Yes. Note that Y ¼ 5X  .1X2 is a concave function, so that Jensen’s
inequality applies. Then E(Y) ¼ E(g(X))  g(E(X)) ¼ 5E(X)  .1(E(X))2 ¼ 60 is an
upper bound to the expected yield. In fact, the function is strictly concave, and so
the inequality can be made strict (it is reasonable to assume that rainfall is not a
degenerate random variable).
7A continuous function, g, deﬁned on a set D is called concave if 8x∈D, 9 a line going through the point (x,g(x)) that lies on or above the
graph of g. The function is convex if 8x∈D, 9 a line going through the point (x,g(x)) that lies on or below the graph of g. The function is
strictly convex or concave if the aforementioned line has only the point (x,g(x)) in common with the graph of g.
8A degenerate random variable is a random variable that has one outcome that is assigned a probability of 1. More will be said about
degenerate random variables in Section 3.6.
3.3
Expectation of a Function of Random Variables
121

3.4
Expectation Properties
There are a number of properties of the expectation operation that follow
directly from its deﬁnition. We ﬁrst present results that involve scalar random
variables. We then introduce results involving multivariate random variables.
3.4.1
Scalar Random Variables
Theorem 3.5
If c is a constant, then E(c) ¼ c.
Proof
Let g(X) ¼ c. Then, by Theorem 3.2,
EðcÞ ¼
Z 1
1
c dFðxÞ ¼ c
Z 1
1
dFðxÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
1
¼ c:
n
In words, “the expected value of a constant is the constant itself.”
Theorem 3.6
If c is a constant, then E(cX) ¼ cE(X).
Proof
Let g(X) ¼ cX. Then, by Theorem 3.2,
EðcXÞ ¼
Z 1
1
cxdFðxÞ ¼ c
Z 1
1
x dFðxÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
EðXÞ
¼ cEðXÞ
n
In words, “the expected value of a constant times a random variable is the
constant times the expected value of the random variable.”
Theorem 3.7
E Pk
i¼1 gi ðXÞ


¼ Pk
i¼1 E gi ðXÞ
ð
Þ:
Proof
Let g(x) ¼ Pk
i¼1 gi(X). Then, by Theorem 3.2,
E
Xk
i¼1 gi ðXÞ


¼
Z 1
1
X
k
i¼1
giðxÞ
"
#
dFðxÞ ¼
X
k
i¼1
Z 1
1
giðxÞ dFðxÞ ¼
X
k
i¼1
E giðXÞ
ð
Þ
In words, “the expectation of a sum is the sum of the expectations” regard-
ing k functions of the random variable X.
A useful Corollary to Theorem 3.7 concerns the expectation of a linear
function of X:
n
Corollary 3.1
Let Y ¼ a þ bX for real constants a and b, and let E(X) exist. Then E(Y) ¼ a þ
bE(X).
Proof
This follows directly from Theorem 3.7 by deﬁning g1(X) ¼ a, g2(X) ¼ bX, and
then applying Theorems 3.5 and 3.6.
n
122
Chapter 3
Expectations and Moments of Random Variables

3.4.2
Multivariate Random Variables
The concept of an expectation of a function of a random variable is generalizable
to a function of a multivariate random variable as indicated in the following
Theorem. The proof is based on an extension of the proof of Theorem 3.2 and is
omitted.9
Theorem 3.8
Expectation of a
Function of a
Multivariate Random
Variable
Let (X1,. . .,Xn) be a multivariate random variable with joint density function
f(x1,. . .,xn). Then the expectation of Y ¼ g(X1,. . .,Xn) is given by10
EðYÞ ¼
X
x1;...;xn
ð
Þ2RðXÞ
g x1; . . . ; xn
ð
Þf x1; . . . ; xn
ð
Þ
(discrete)
EðYÞ ¼
Z 1
1
  
Z 1
1
g x1; . . . ; xn
ð
Þf x1; . . . ; xn
ð
Þdx1 . . . dxn (continuous):
We remind the reader that since f x1; . . . ; xn
ð
Þ ¼ 0 8 x1; . . . ; xn
ð
Þ=2RðXÞ, one could
also sum over the points
x1; . . . ; xn
ð
Þ 2 n
i¼1R Xi
ð
Þ to deﬁne E(Y) in the discrete
case.
Example 3.9
Expectation of a
Function of a
Bivariate RV
Let the bivariate random variable (X1, X2) represent the proportions of operating
capacity at which two electricity generation plants operate on a given spring day
in an east coast power grid. Assume the joint density of (X1, X2) is given by
f x1; x2
ð
Þ ¼ 6x1x2
2I 0;1
½
 x1
ð
ÞI 0;1
½
 x2
ð
Þ:
What is the expected average proportion of operating capacity at which the two
plants operate?
Answer: Deﬁne the average proportion of operating capacity via the function
g(X1, X2) ¼ .5(X1 + X2). By Theorem 3.8,
E gðX1; X2Þ
ð
Þ ¼
Z 1
0
Z 1
0
3x2
1x2
2dx1dx2 þ
Z 1
0
Z 1
0
3x1x3
2dx1dx2
¼ 1
3 þ 3
8 ¼ 17
24 ¼ :7083:
□
The expectation property in Theorem 3.7 concerning the sum of functions of
a random variable X can also be extended to the sum of functions of a multivari-
ate random variable, as the following theorem indicates.
9See Steven F. Arnold, (1990), Mathematical Statistics, Englewood Cliffs, NJ: Prentice Hall, pp. 92, 98.
10It is tacitly assumed that the sum and integral are absolutely convergent for the expectation to exist.
3.4
Expectation Properties
123

Theorem 3.9
E Pk
i¼1 gi X1; :::; Xn
ð
Þ


¼ Pk
i¼1 E gi X1; :::; Xn
ð
Þ
ð
Þ:
Proof
Let g X1; . . . ; Xn
ð
Þ ¼ Pk
i¼1 gi X1; . . . ; Xn
ð
Þ: Then, by Theorem 3.8, E(g(X1,. . .,Xn)) is
given by
E
X
k
i¼1
gi X1; :::; Xn
ð
Þ
 
!
¼
Z 1
1
  
Z 1
1
X
k
i¼1
gi x1; . . . ; xn
ð
Þ
 
!
dFðx1; :::; xnÞ
¼
X
k
i¼1
Z 1
1
  
Z 1
1
gi x1; :::; xn
ð
Þ dF x1; :::; xn
ð
Þ
¼
X
k
i¼1
E gi X1; :::; Xn
ð
Þ
ð
Þ
n
A useful corollary to Theorem 3.9 involving the sum of random variables
themselves is given as follows:
Corollary 3.2
E Pn
i¼1 Xi


¼ Pn
i¼1 E Xi
ð
Þ:
Proof
This is an application of Theorem 3.9 with gi(X1,. . .,Xn) ¼ Xi, i ¼ 1,. . .,n, and
k¼n
n
In words, “the expectation of a sum is equal to the sum of the expectations”
regarding the n random variables X1,. . .,Xn.
If the random variables (X1,. . .,Xn) are independent, we can prove that “the
expectation of a product is the product of the expectations.”
Theorem 3.10
Let (X1,. . .,Xn) be independent random variables. Then
E
Y
n
i¼1
Xi
 
!
¼
Y
n
i¼1
E Xi
ð
Þ
Proof
Letting g(X1,. . .,Xn) ¼ Qn
i¼1 Xi in Theorem 3.8, we have
E
Y
n
i¼1
Xi
 
!
¼
Z 1
1
  
Z 1
1
Y
n
i¼1
xi dF x1; :::; xn
ð
Þ
¼
Z 1
1
  
Z 1
1
Y
n
i¼1
xi
Y
n
j¼1
dFj xj


(by independence)
¼
Y
n
i¼1
Z 1
1
xi dFi xi
ð
Þ dxi ¼
Y
n
i¼1
E Xi
ð
Þ:
n
124
Chapter 3
Expectations and Moments of Random Variables

Later in our study we will ﬁnd it necessary to take expectations of a vector or
matrix of random variables. The following deﬁnition describes what is involved
in such an operation.
Deﬁnition 3.5
Expectation of a Matrix
of Random Variables
Let W be an n  k matrix of random variables whose (i,j)th element is Wij.
Then E(W), the expectation of the matrix W, is the matrix of expectations of
the elements of W, where the (i,j)th element of E(W)is equal to E(Wij).
If we let k ¼ 1 in the above deﬁnition, we have that the expectation of a
vector is the vector of expectations, i.e.,
EðWÞ ¼ E
W1
W2
...
Wn
2
6664
3
7775 ¼
E W1
ð
Þ
E W2
ð
Þ
..
.
E Wn
ð
Þ
2
6664
3
7775
In general,
E W
ð
Þ
nk
ð
Þ
¼
E W11
ð
Þ
  
E W1k
ð
Þ
..
.
..
.
..
.
E Wn1
ð
Þ
  
E Wnk
ð
Þ
2
64
3
75;
i.e., “the expectation of a matrix is the matrix of expectations.”
Having introduced the concept of the expectation of a vector, we note that a
multivariate Jensen’s inequality (Theorem 3.4) holds true for multivariate ran-
dom variables. In fact, the appropriate extension is made by letting X denote an
n  1 random vector and I represent an open rectangle in the statement of
Theorem 3.4. The reader is asked to prove the multivariate version of Theorem
3.4 (replace the line‘(x) ¼ a + bx with the hyperplane ‘ðxÞ ¼ a þ Pn
i¼1 bixi in the
Appendix proof).
3.5
Conditional Expectation
Up to this point, expectations of random variables and functions of random
variables have been taken unconditionally, assuming that no additional infor-
mation was available relating to the occurrence of an event for a subset of the
random variables (X1,. . .,Xn). When information is given concerning the occur-
rence of events for a subset of the random variables (X1,. . .,Xn), the concept of
conditional expectation of a random variable becomes relevant.
There is a myriad of situations that arise in which the concept of conditional
expectation is relevant. For example, the expected number of housing starts
calculated for planning purposes by building supply manufacturers would
depend on the given level of mortgage interest assumed, or the expected sales
tax revenue accruing to state government would be a function of whatever
3.5
Conditional Expectation
125

reduced level of employment was assumed due to the downsizing of a major
industry in the state. More generally, we will see that conditional expectation is
at the heart of regression analysis, whereby one attempts to explain the expected
value of one random variable as a function of the values of other related random
variables, e.g., the expected yield/acre of an agricultural crop is conditional on
the level of rainfall, temperature, sunshine, and the degree of weed and pest
infestation.
The difference between unconditional and conditional expectation is that
the unconditional density function is used to weight outcomes in the former
case, while a conditional density function supplies the weights in the latter case.
The conditional expectation of a function of a random variable is deﬁned in the
bivariate case as follows.
Deﬁnition 3.6
Conditional
Expectation-Bivariate
Let X and Y be random variables with joint density function f(x,y). Let the
conditional density of Y, given x∈B, be f(y|x∈B). Let g(Y) be a real-valued
function of Y. Then the conditional expectation of g(Y), given x∈B, is
deﬁned as
E gðYÞjx 2 B
ð
Þ ¼
X
y2RðYÞ
gðyÞf yjx 2 B
ð
Þ;
(discrete)
E gðYÞjx 2 B
ð
Þ ¼
Z 1
1
gðyÞf yjx 2 B
ð
Þdy:
(continuous)
Note in the special case where g(Y) ¼ Y, we have by Deﬁnition 3.6 that
EðYjx 2 BÞ ¼
R 1
1 yfðyjx 2 BÞdy in the continuous case, and
EðYjx 2 BÞ ¼
P
y2RðYÞ yfðyjx 2 BÞin the discrete case.
Example 3.10
Conditional Expectation
in Bivariate Case
Let the bivariate random variable (X, Y) represent the per dollar return on two
investment projects. Let the joint density of (X, Y) be
fðx;yÞ ¼ 1
96ðx2þ2xyþ2y2ÞI½0;4ðxÞI½0;2ðyÞ:
What is the conditional expectation of the per dollar return on the second
project, given that the per dollar return on the ﬁrst project is x ¼ 1?
Answer: To answer the question, we ﬁrst need to establish the conditional
density f(y|x ¼ 1). This in turn requires knowledge of the marginal density of
X, which we ﬁnd as
fX ðxÞ ¼
Z 1
1
f x; y
ð
Þdy ¼ 1
96
Z 2
0
ðx2 þ2xy þ 2 y2ÞI 0;4
½
ðxÞdy
¼
1
48x2 þ 1
24x þ 1
18


I 0;4
½
ðxÞ:
126
Chapter 3
Expectations and Moments of Random Variables

Then
fðyjx ¼ 1Þ ¼ fð1; yÞ
fXð1Þ ¼
1
96ð1 þ 2y þ 2 y2Þ I½0;2 ðyÞ
17
144
¼ :088235 þ :176471ðy þ y2Þ


I½0;2 ðyÞ:
Finally, by Deﬁnition 3.6,
EðYjx ¼ 1Þ ¼
Z 1
1
yfðyjx ¼ 1Þdy ¼
Z 2
0
:088235y þ :176471 y2 þ y3




dy
¼ :088235y2
2
þ :176471 y3
3 þ y4
4


2
0
¼ 1:3529:
□
It is important to note that all of the properties of expectations derived
previously apply equally well to conditional expectations. This follows from
the fact that the operations of taking an unconditional or a conditional expecta-
tion are precisely the same once a PDF has been derived, and the genesis of the
PDF is irrelevant to the expectation properties derived heretofore (note that the
origin of a PDF in previous sections was never an issue).
Rather than specifying a particular elementary event for the outcome of
X when deﬁning a conditional expectation of g(Y), we might conceptualize
leaving the elementary event for X unspeciﬁed, and express the conditional
expectation of g(Y) as a function of x. Let (x) ¼ E(g(Y)|x) denote the function of
x whose value when x ¼ b is E(g(Y)|x ¼ b). Then, by deﬁnition, we can interpret
(X) ¼ E(g(Y)|X) as a random variable. If we take E((X)) ¼ E(E(g(Y)|X)) we obtain
the unconditional expectation of g(Y).
Theorem 3.11
Iterated Expectation
Theorem
E E gðYÞ
ð
Þ X
j
ð
Þ ¼ E gðYÞ
ð
Þ
Proof
(continuous) Let fX(x) be the marginal density of X and f(x,y) be the joint density
of X and Y. Then
ðxÞ ¼ EðgðYÞjxÞ ¼
Z 1
1
gðyÞ fðx; yÞ
fX ðxÞ dy
and
EððXÞÞ ¼ EðEðgðYÞjXÞÞ ¼
Z 1
1
Z 1
1
gðyÞ fðx; yÞ
fX ðxÞdy


fX ðxÞdx
¼
Z 1
1
Z 1
1
gðyÞfðx; yÞdydx ¼ E gðYÞ
ð
Þ:
The proof in the discrete case is left to the reader.
n
3.5
Conditional Expectation
127

Example 3.11
Unconditional
Expectation via
Iteration
Suppose that the expectation of market supply for some commodity, given price
p, is represented by E(Q|p) ¼ 3p + 7 and E(P) ¼ 2. Then by the iterated expecta-
tion theorem, the unconditional expectation of market supply is given by
E E QjP
ð
Þ
ð
Þ ¼ Eð3P þ 7Þ ¼ 3EðPÞ þ Eð7Þ ¼ 13:
□
In cases where one is conditioning on an elementary event x ¼ b, there are
useful generalizations of Deﬁnition 3.6 and Theorem 3.11, which are referred to
as the substitution theorem and the generalized iterated expectation theorem,
respectively.
Theorem 3.12
Substitution Theorem
E(gðX;YÞjx¼b) = E(gðb;YÞjx¼bÞ:
Proof
(Discrete Case) Let z ¼ g(x,y), and note that the PDF of Z, conditional on x ¼ b,
can be deﬁned as
hðzjx ¼ bÞ ¼ Pðgðx; yÞ ¼ zjx ¼ bÞ ¼ Pðgðx; yÞ ¼ z; x ¼ bÞ=Pðx ¼ bÞ
¼
X
fy:gðb;yÞ¼zg
fðb; yÞ=fXðbÞ ¼
X
fy:gðb;yÞ¼zg
fðyjx ¼ bÞ:
It is evident that the set of z values for which h(z|x ¼ b) > 0 is given by X ¼ {z:
z ¼ g(b,y), y is such that f(y|x ¼ b) > 0}. Then
EðgðX; YÞjx ¼ bÞ ¼ Eðzjx ¼ bÞ ¼
X
z2X
zhðzjx ¼ bÞ
¼
X
z2X
z
X
fy:gðb;yÞ¼zg
fðyjx ¼ bÞ
¼
X
y2RðYÞ
gðb; yÞfðyjx ¼ bÞ ¼ Eðgðb; YÞjx ¼ bÞ:
(Continuous Case) See A.F. Karr (1993) Probability, New York: Springer-Verlag,
p. 230.
n
The substitution theorem indicates that when taking the expectation of g(X,
Y) conditional on x ¼ b, one can substitute the constant b for X as g(b,Y)
and then take the conditional expectation with respect to the random variable
Y. The random variable X essentially acts as a constant in g(X,Y) under the
condition x ¼ b.
Theorem 3.13
Generalized Iterated
Expectation Theorem
E(E(gðX;YÞjX)) = E(gðX;YÞÞ
Proof
Using the substitution theorem but leaving the elementary event for X unspeci-
ﬁed in order to express the conditional expectation as a function of the elemen-
tary event x obtains (continuous case—discrete case is analogous)
128
Chapter 3
Expectations and Moments of Random Variables

ðxÞ ¼ EðgðX; YÞjxÞ ¼ Eðgðx; YÞjxÞ
¼
Z 1
1
g x; y
ð
ÞfðyjxÞdy:
Then
EððXÞÞ ¼ E EðgðX; YÞjXÞ
ð
Þ
¼
Z 1
1
Z 1
1
g x; y
ð
Þf yjx
ð
Þdy


fXðxÞdx
¼
Z 1
1
Z 1
1
g x; y
ð
Þfðx; yÞdydx ¼ E gðX; YÞ
ð
Þ:
Thus, taking the expectation of the conditional expectation of g(X,Y) given x
yields the unconditional expectation of g(X,Y).
n
Example 3.12
Unconditional
Expectation of Linear
Function of Bivariate RV
via Iteration
Let (X,Y) represent the per dollar return on the two investment projects of
Example 3.10. Assume $1000 is invested in each project. What is the expected
return on the portfolio, given that the per dollar return on the ﬁrst project is
x ¼ 1?
Answer: The return on the portfolio can be represented as z ¼ g(x,y) ¼ 1000x
þ 1000y. The substitution theorem allows the conditional expectation to be
deﬁned as
EðZjx ¼ 1Þ ¼ Eð1000X þ 1000Yjx ¼ 1Þ ¼ Eð1000 þ 1000Yjx ¼ 1Þ
¼
Z 2
0
1000 þ 1000y
ð
Þfðyjx ¼ 1Þdy
¼ 1000 þ 1000
Z 2
0
yfðyjx ¼ 1Þdy ¼ 2352:9:
□
Example 3.13
Unconditional
Expectation of
Nonlinear Function via
Iteration
Given the representation of expected market supply in Example 3.11, we know
by the substitution theorem that the expected dollar sales of the commodity,
expressed as a function of price, is E(pQ|p) ¼ 3p2 + 7p. Suppose E(P2) ¼ 8. Then,
using Theorem 3.13, the (unconditional) expectation of dollar sales is given by
E(E(PQ|P)) ¼ E(3P2 + 7P) ¼ 3E(P2) + 7E(P) ¼ 38.
□
3.5.1
Regression Function in the Bivariate Case
In the special case where g(Y) ¼ Y, the conditional expectation of Y expressed as
a function of x, i.e., E(Y|x), is called the regression function of Y on X. The
regression function depicts the functional relationship between the conditional
expectation of Yand the potential values of X on which the expectation might be
conditioned. In the continuous case, the graph of the function is generally a
curve in the plane, in which case E(Y|x) is often referred to as the regression
curve of Y on X.
3.5
Conditional Expectation
129

Deﬁnition 3.7
Regression Function of
Y on X
The conditional expectation of Y expressed as a function of x, as E(Y|x).
Example 3.14
Regression Function of
Investment Returns
Refer to the investment return example, Example 3.10, and rather than calculate
E(Y|x ¼ 1), we calculate E(Y|x), the regression function of Y on X. Letting f(y|x)
denote the conditional density function of y expressed as a function of x, we have
f(y|x) ¼ f(x,y)/fX(x). Then
EðYjxÞ ¼
Z 1
1
y fðx; yÞ
fX ðxÞ dy ¼
Z 2
0
yðx2 þ2xy þ 2 y2Þ I½0;4 ðxÞ
2 x2 þ4x þ 16
3


I½0;4 ðxÞ dy
¼
2 x2 þ16
3 x þ 8
2 x2 þ4x þ 16
3
"
#
for x 2 ½0; 4
and the regression function is undeﬁned for x=2 0; 4
½
. The regression function
represents the expected per dollar return on project 2 as a function of the various
potential conditioning values for the per dollar return on project 1. Note that the
regression function is a nonlinear function of x. The reader can verify that at the
point x ¼ 1, E(Y|x) ¼ 1.3529 is the value of the regression function, as it should
be given the answer to Example 3.10. The reader is encouraged to sketch the
graph of the regression function over its domain of deﬁnition.
□
The regression function has an important interpretation in terms of
approximating one random variable by a function of other random variables.
Examine the problem of choosing a function of X, say h(X), whose outcomes are
the minimum expected squared distance11 from the outcome of Y. Assuming
that (X,Y) is a continuous bivariate random variable (the discrete case is analo-
gous), we thus seek an h(X) that minimizes
EðY  hðXÞ Þ2 ¼
Z 1
1
Z 1
1
y  hðxÞ
ð
Þ2fðx; yÞdxdy
¼
Z 1
1
Z 1
1
y  hðxÞ
ð
Þ2f yjx
ð
Þdy


fXðxÞdx:
If h(x) could be chosen so as to minimize the bracketed integral for each possible
x, then it would follow that the double integral, and thus the expected squared
distance between outcomes of Y and h(X), would be minimized.
The optimal choice of approximating function is given by h(x) ¼ E(Y|x). To
see why, note that the substitution theorem allows the preceding bracketed
expression to be written as
11Recall that the distance between the points a and b is deﬁned by d(a,b) ¼ |ba| and thus squared distance would be given by d2(a,
b) ¼ (ba)2.
130
Chapter 3
Expectations and Moments of Random Variables

E
Y  hðxÞ
½
2 jx


¼ E
Y  EðYjxÞ þ EðYjxÞ  hðxÞ
½
2 jx


¼ E
Y  EðYjxÞ
½
2 jx


þ EðYjxÞ  hðxÞ
½
2
where the cross product term is zero and has been eliminated because
E Y  E Yjx
ð
Þ
½
 E Yjx
ð
Þ  hðxÞ
½
jx
ð
Þ
¼ E Yjx
ð
Þ  hðxÞ
½
E Y  E Yjx
ð
Þjx
ð
Þ by the substitution theorem
ð
Þ
¼ E Yjx
ð
Þ  hðxÞ
½
 E Yjx
ð
Þ  E Yjx
ð
Þ
½
 ¼ 0:
It follows that the choice of h(x) that minimizes E([Yh(x)]2|x) is given by
h(x) ¼ E(Y|x), since any other choice results in [E(Y|x)h(x)]2 > 0.
The preceding result suggests that if one is attempting to explain or predict
the outcome of one random variable from knowledge of the outcome of another
random variable, and if expected squared distance (also called mean square
error—to be discussed in Chapter 7) is used as the measure of closeness between
actual and predicted outcomes, then the best (closest) prediction is given by
values of the regression function, or equivalently by the conditional expectation
of the random variable of interest. For example, in Example 3.14, if one were
attempting to predict the expected dollar return on project 2 in terms of the
dollar return on project 1, the regression function E(Y|x) presented in the exam-
ple provides the predictions that minimize expected squared distance between
outcomes of Y and outcomes of h(X). If x ¼ 1, then the best prediction of Y’s
outcome would be 1.3529.
3.5.2
Conditional Expectation and Regression in the Multivariate Case
The deﬁnition of conditional expectation (Deﬁnition 3.4) and the theorems
involving conditional expectation extend to the case where Y and/or X is multi-
variate, in which case the reader can interpret Y and X as referring to random
vectors and introduce multiple summation or integration notation appropriately
when reading the deﬁnition and the theorems. Also, the notion of the regression
function of Yon X extends straightforwardly to the case where X is multivariate,
in which case E(Y|x) is interpreted as a function of the vector x and would be
deﬁned by E Yjx
ð
Þ ¼
R 1
1 yf yjx1; . . . ; xn
ð
Þdy or P
y2RðYÞ yf yjx1; :::; xn
ð
Þ in the con-
tinuous or discrete case, respectively. An argument analogous to the bivariate
case can be used to prove that h(X) ¼ E(Y|X) is the function of the multivariate X
that is the minimum expected squared distance from Y. Thus, the best approxi-
mation or prediction of Y outcomes via a function of the outcome of the multi-
variate X is provided by values of the regression function.
For convenience, we list below a number of general expectation results
applied speciﬁcally to conditional expectations involving multivariate random
variables.
3.5
Conditional Expectation
131

Deﬁnition 3.8
Conditional Expectation
(General)
Let (X1,. . .,Xn) and (Y1,. . .,Ym) be random vectors having a joint density func-
tion f(x1,. . .,xn, y1,. . .,ym). Let g(Y1,. . .,Ym) be a real-valued function of (Y1,. . .,
Ym). Then the conditional expectation of g(Y1,. . .,Ym), given (x1,. . .,xn)∈B, is
deﬁned as12:
Eðg Y1; . . . ; Ym
ð
Þj x1; . . . ; xn
ð
Þ 2 BÞ
discrete
ð
Þ
¼
X
y1;...;ym
ð
Þ2RðYÞ
g y1; . . . ; ym
ð
Þf y1; . . . ; ymj x1; . . . ; xn
ð
Þ 2 B
ð
Þ
Eðg Y1; . . . ; Ym
ð
Þj x1; . . . ; xn
ð
Þ 2 BÞ
continuous
ð
Þ
¼
Z 1
1
  
Z 1
1
g y1; . . . ; ym
ð
Þf y1; . . . ; ymj x1; . . . ; xn
ð
Þ 2 B
ð
Þdy1    dym:
Theorem 3.14
Substitution Theorem:
Multivariate
E(gðX1;:::;Xn;Y1;:::;YmÞjx¼b) = E(gðb1;:::;bn;Y1;:::; YmÞjx¼bÞ:
Theorem 3.15
Iterated Expectation
Theorems: Multivariate
E(E(gðY1;:::;YmÞjX1;:::;Xn)) = E(gðY1;:::;YmÞÞ
E(E(gðX1;:::;Xn;Y1;:::;YmÞjX1;:::;Xn)) = E(gðX1;:::;Xn;Y1;:::;YmÞÞ:
Theorem 3.16
E(cjðx1;:::;xnÞ 2 BÞ ¼c:
Theorem 3.17
E(cYjðx1;:::xnÞ 2 BÞ ¼c E(Yjðx1;:::;xnÞ 2 BÞ:
Theorem 3.18
E
S
k
i¼1 gi ðY1; :::; YmÞjðx1; :::; xnÞ 2 B

	
¼ P
k
i¼1
E gi Y1; :::; Ym
ð
Þj x1; :::; xn
ð
Þ 2 B
ð
Þ:
3.6
Moments of a Random Variable
The expectations of certain power functions of a random variable have uses as
measures of central tendency, spread or dispersion, and skewness of the density
function of the random variable, and also are important components of statisti-
cal inference procedures that we will study in later chapters. These special
expectations are called moments of the random variable (or of the density
12One can equivalently sum over the points (y1,. . .,ym) ∈m
i¼1 R(Yi) in deﬁning the expectation in the discrete case.
132
Chapter 3
Expectations and Moments of Random Variables

function). There are two types of moments that we will be concerned with—
moments about the origin and moments about the mean.
Deﬁnition 3.9
rth Moment About the
Origin
Let X be a random variable with density function f(x). Then the rth moment
of X about the origin, denoted by m0
r, is deﬁned for integers r  0 as
m0
r ¼ E Xr
ð
Þ ¼
X
x2RðXÞ
xrfðxÞ
discrete
ð
Þ
m0
r ¼ E Xr
ð
Þ ¼
Z 1
1
xrfðxÞdx:
continuous
ð
Þ
The value of r in the deﬁnition of moments is referred to as the order of the
moment, so that one would refer to E Xr
ð
Þ as the moment of order r. Note that
m0
0 ¼ 1
for
any
discrete
or
continuous
random
variable,
since
m0
0 ¼ E X0


¼ E 1
ð Þ ¼ 1.
The ﬁrst moment about the origin is simply the expectation of the random
variable X, i.e., m0
1 ¼ E X1


¼ EðXÞ, a quantity that we have examined at the
beginning of our discussion of mathematical expectation. This balancing point
of a density function, or the weighted average of the elements in the range of the
random variable, will be given a special name and symbol.
Deﬁnition 3.10
Mean of a Random
Variable (or Mean of a
Density Function)
The ﬁrst moment about the origin of a random variable, X, is called the mean
of the random variable X (or mean of the density function of X), and will be
denoted by the symbol m.
Thus, the ﬁrst moment about the origin characterizes the central tendency
of a density function. Measures of spread and skewness of a density function are
given by certain moments about the mean.
Deﬁnition 3.11
rth Central Moment
(or rth Moment About
the Mean)
Let X be a random variable with density function f(x). Then the rth central
moment of X (or the rth moment of X about the mean), denoted by mr, is
deﬁned as
mr ¼ E X  m
ð
Þr
ð
Þ ¼
X
x2RðXÞ
x  m
ð
ÞrfðxÞ;
discrete
ð
Þ
mr ¼ E X  m
ð
Þr
ð
Þ ¼
Z 1
1
x  m
ð
ÞrfðxÞdx:
continuous
ð
Þ
Note that m0¼ 1 for any discrete or continuous random variable, since m0
¼ EðX  mÞ0 ¼ E 1
ð Þ ¼ 1 . Furthermore, m1 ¼ 0 for any discrete or continuous
random variable for which E(X) exists, since m1 ¼ EðX  mÞ1 ¼ EðXÞ  EðmÞ ¼ m
m ¼ 0. The second central moment is given a special name and symbol.
3.6
Moments of a Random Variable
133

Deﬁnition 3.12
Variance of a Random
Variable (or Variance of
a Density Function)
The second central moment, E((Xm)2), of a random variable, X, is called the
variance of the random variable X (or the variance of the density function of
X), and will be denoted by the symbol s2, or by var(X).
We will also have use for the following function of the variance of a random
variable.
Deﬁnition 3.13
Standard Deviation of a
Random Variable (or
Standard Deviation of a
Density Function)
The nonnegative square root of the variance of a random variable, X, (i.e.,
þ
ﬃﬃﬃﬃﬃ
s2
p
) is called the standard deviation of the random variable X (or standard
deviation of the density function of X) and will be denoted by the symbol s, or
by std(X).
The variance (and thus also the standard deviation) of X is a measure of
dispersion or spread of the density function f(x) around its balancing point (the
mean of X). The larger the variance, the greater the spread or dispersion of the
density about its mean. In the extreme case where the entire density is
concentrated at the mean of X and thus has no spread or dispersion, i.e., f(x) ¼
I m
f gðxÞ, then E((X  m)2) ¼ 0 and the variance (and standard deviation) is zero.
In order to examine the relationship between the spread of a density and the
magnitude of the variance in more detail, we ﬁrst present Markov’s inequality
(named after the Russian mathematician A. Markov), and we will then intro-
duce Chebyshev’s inequality (named after the Russian mathematician P.L.
Chebyshev) as a corollary.
Theorem 3.19
Markov’s Inequality
Let X be a random variable with density function f(x), and let g be
a nonnegative-valued function of X. Then P(g(x)  a )  E(g(X))/ a for any
value a > 0.
Proof
E gðXÞ
ð
Þ ¼
Z 1
1
gðxÞdFðxÞ
¼
Z
x:gðxÞa
f
g
gðxÞ dFðxÞ þ
Z
x:gðxÞ<a
f
g
gðxÞ dFðxÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
0

Z
x:gðxÞa
f
g
gðxÞ dFðxÞ

Z
x:gðxÞ  a
f
g
a dFðxÞdx ðsince gðxÞ  a for all x 2 fx : gðxÞ  ag
 a
Z
x:gðxÞ  a
f
g
dFðxÞdx ¼ aP gðxÞ  a
ð
Þ;
and thus, E gðXÞ
ð
Þ
a
 P gðxÞ  a
ð
Þ:
n
134
Chapter 3
Expectations and Moments of Random Variables

Corollary 3.3
Chebyshev’s Inequality
P x  m
j
j  ks
ð
Þ  1=k2 for k>0
Proof
This follows by letting g(X) ¼ (X  m)2 and a ¼ k2s2 in Markov’s inequality and
realizing that (x  m)2k2s2 is equivalent to (x  m) ks or (x  m)ks, which
is in turn equivalent to |x  m|  ks.
n
In words, Markov’s inequality states that we can always place an upper
bound on the probability that g(x)  a so long as g(x) is nonnegative valued and
E(g(X)) exists. Chebyshev’s inequality implies that ifmand s are, respectively, the
mean and standard deviation of the density function of X, then for any positive
constant k, the probability that X will have an outcome that is k or more
standard deviations from its mean, i.e., outside the interval (m  ks, m + ks), is
less than or equal to 1/k2. Note that we are able to make these probability
statements without knowledge of the algebraic form of the density function.
Chebyshev’s inequality is sometimes stated in terms of an event that is the
complement of the event in Corollary 3.3.
Corollary 3.4
Chebyshev’s Inequality
P x  m
j
j<ks
ð
Þ  1  1=k2 for k>0
Proof
Follows directly from Corollary 3.3 noting that P(|x – m|ks) ¼ 1  P(|x–m|< ks)
 1/k2.
n
Markov’s inequality and Chebyshev’s inequalities are interesting in their
own right, but at this point we will use the concepts only to further clarify our
interpretation of the variance as a measure of the spread or dispersion of a
density function. In Corollary 3.4, let ks ¼ c, where c is any arbitrarily small
positive number. Then P(|x  m| < c) 1  s2/c2 where we have substituted for
k the value c/s. Then note that as s2 ! 0, the probability inequality approaches
P( m  c < x < m + c)
! 1, which implies that as s2
!
0, the density
concentrates in the interval (m  c, m + c) for any arbitrarily small positive c.
Diagrammatically, this can be illustrated as in Figure 3.6.
s1
2
s1
2< s2
2< s3
2
m + c
m − c
s2
2
s3
2
Figure 3.6
Density functions and
variances.
3.6
Moments of a Random Variable
135

As a concrete example, let m ¼ 0, and examine the event B ¼ {x:|x| < 10},
where we are letting c ¼ 10 in the preceding argument. Then using P(|x  mj
< ks)  1  1/k2 with ks ¼ c ¼ 10, we have:
if
s ¼ 5
s ¼ 2
s ¼ 1
2
4
3
5; then
PðBÞ 
1  1=22 ¼ :75
1  1=52 ¼ :96
1  1=102 ¼ :99
2
4
3
5;
and thus the smaller is s (and thus the smaller the variance), the larger is the
lower bound on the probability that the outcome of X occurs in the interval
(10, 10).
For an alternative argument in support of interpreting the variance as a
measure of the spread of a density function, note that the variance of X can be
directly interpreted as the expected squared distance of the random variable X
from its mean. To see this, ﬁrst recall that the distance between two points, x
and y, on the real line is deﬁned as d(x,y) ¼ |x  y|. Then d2(x,y) ¼ (x  y)2, and
letting y ¼ m, we have E(d2(X,m)) ¼ E((X  m)2) ¼ s2. Therefore, the smaller is s2,
the smaller is the expected squared distance of X from its mean.
The third central moment is used as a measure of whether the density of X is
skewed.
Deﬁnition 3.14
Symmetric and Skewed
Densities
A density is said to be symmetric about m when fðm þ dÞ ¼ fðm  dÞ8d > 0: If the
density is not symmetric, it is said to be a skewed density.
Therefore, a density is said to be symmetric aboutmif the density is such that
the graph to the right of the mean is the mirror image of the graph to the left of
the mean (see Figure 3.7). If such is not the case, the density is said to be a
skewed density. A necessary (but not sufﬁcient) condition for a density to be
symmetric about m is that m3 = E(X  mÞ3¼ 0:
x
μ − δ
μ + δ
μ
Figure 3.7
Density symmetric about m.
136
Chapter 3
Expectations and Moments of Random Variables

Theorem 3.20
Relationship Between
m3, Symmetry,
and Skewness
If m3 exists, then fðxÞis symmetric about the mean only if m3 ¼ 0. If m3 6¼ 0, then
fðxÞ is skewed.
Proof
Note that
m3 ¼
Z 1
1
x  m
ð
Þ3dFðxÞdx ¼
Z 1
m
x  m
ð
Þ3dFðxÞ þ
Z m
1
x  m
ð
Þ3dFðxÞ:
By making the substitution z ¼ x  m in the ﬁrst integral and z ¼ x + m in the
second integral, if the density is symmetric, then
m3 ¼
Z 1
0
z3dF m þ z
ð
Þ 
Z 0
1
z
ð
Þ3dF m  z
ð
Þ
¼
Z 1
0
z3dF m þ z
ð
Þ 
Z 1
0
z3dF m  z
ð
Þ ¼ 0
since f(m + z) ¼ f(m  z) 8z by the symmetry of f about m. It follows that if m3 6¼ 0,
then the density function is necessarily skewed.
n
We underscore that m3 ¼ 0 is not sufﬁcient for symmetry, i.e., a density can
be skewed, and still have m3 ¼ 0, as the following example illustrates.
Example 3.15
A Skewed Density
with m3 ¼ 0
Let the random variable X have the density function f(x) ¼ .22 I{1}(x) + .77
I{2}(x) + .01 I{4}(x) (see Figure 3.8). Note that m ¼ 1.8, and it is clear that f(x) is
not symmetric about m. Nonetheless, m3 ¼ E
X  m
ð
Þ3


¼ 0:
□
The sign of m3 is sometimes interpreted as indicating the direction of
the skew in the density function. In particular, density functions having long
“tails” to the right are called skewed to the right, and these densities tend to
have m3 > 0, whereas density functions with long left-hand tails are called
1.0
1
x
4
.5
2
Δm
f(x)
Figure 3.8
Nonsymmetric density
function of Example 3.15.
3.6
Moments of a Random Variable
137

skewed to the left, and these densities tend to have m3 < 0 (see Figure 3.9).
Unfortunately, there are exceptions to these “tendencies,” and the nature of
the skewness is best determined by examining the graph of the density itself if
the functional form of the density is known.
The fourth moment about the mean is often used to form a measure of how
“peaked” a probability density function is. Measures most often used for this
purpose include a scaled version of the fourth moment, called kurtosis, as well
as a scaled and centered (relative to a standard normal distribution, discussed in
Chapter 4) version called excess kurtosis.
Deﬁnition 3.15
Kurtosis and Excess
Kurtosis
The kurtosis of a probability density function fðxÞ is given by m4=s4. The
excess kurtosis of fðxÞ is given by m4=s4  3.
The larger (smaller) the value of the kurtosis, the more peaked (ﬂatter) is the
density function. Excess kurtosis measures the degree to which a density func-
tion is more or less (the excess or lack thereof) peaked relative to a standard
normal distribution, which has kurtosis equal to 3. The excess kurtosis measure
is centered such that it equals 0 for the standard normal distribution (Chapter 4).
A probability density that has positive excess kurtosis is referred to as being
leptokurtic. If excess kurtosis is negative the probability density is referred to as
being platykurtic. If excess kurtosis is zero, so that the probability density has
the same excess kurtosis measure as a standard normal distribution, it is referred
to as being mesokurtic.
3.6.1
Relationship Between Moments About the Origin and Mean
Integer ordered moments about the origin and about the mean are functionally
related. Central moments can be expressed solely in terms of moments about
the origin, while moments about the origin can be expressed in terms of the
mean and moments about the mean. The functional relationship is the direct
result of the binomial theorem, which we review for the reader in the following
lemma.
Lemma 3.1
Binomial Theorem.
Let a and b be real numbers. Then a þ b
ð
Þn ¼ Pn
j¼0
n
j

 
ajbnj.
m3 < 0, skewed to the left
m3 > 0, skewed to the right
m
m
Figure 3.9
Skewed density functions.
138
Chapter 3
Expectations and Moments of Random Variables

Theorem 3.21
Central Moments as
Functions of Moments
About the Origin
If mr exists and r is a positive integer, then mr ¼ Pr
j¼0 1
ð
Þj
r
j

 
m0
rjmj.
Proof
By deﬁnition, mr ¼ E X  m
ð
Þr
ð
Þ . Substituting
m
ð
Þ for a, X for b, and r for n
in the binomial theorem (Lemma 3.1) and taking an expectation yields mr ¼ E
Pr
j¼0
r
j

 
m
ð
ÞjXrj


. An application of Theorems 3.6 and 3.7 results in
mr ¼ Pr
j¼0 1
ð
Þj
r
j

 
mjE Xrj


.
n
Theorem 3.22
Moments About the
Origin as Functions of
Central Moments
If m0
r exists and r is a positive integer, then m0
r ¼ Pr
j¼0
r
j

 
mrjmj.
Proof
By deﬁnition, m0
r ¼ E Xr
ð
Þ ¼ E X  m þ m
ð
Þr. Substituting m for a, (X  m) for b, and
r for n in the binomial theorem (Lemma 3.1) and taking an expectation yield
m0
r ¼ E Pr
j¼0
r
j

 
mj X  m
ð
Þrj


. An application of Theorems 3.6 and 3.7 results in
m0
r ¼ Pr
j¼0
r
j

 
mjE
X  m
ð
Þrj


.
n
A special case of Theorem 3.21, which we will use repeatedly in later
chapters, is the case where r ¼ 2, which provides a representation of the variance
of a random variable in terms of moments about the origin. In particular, from
Theorem 3.21, var (X) ¼ m2 ¼ m0
2  m2 ¼ E X2


 EðXÞ
ð
Þ2.
Example 3.16
Variance from Moments
About the Origin
Let the random variable X have density function f(x) ¼ I[0,1](x). Since E(X) ¼ 1/2,
and since E(X2) ¼ R 1
1 x2 I[0,1](x) dx ¼
x3=3


j1
0 ¼ 1=3 , then var(X) ¼ E(X2) 
(E(X))2 ¼ 1/3  (1/2)2 ¼ 1/12.
□
3.6.2
Existence of Moments
Regarding the existence of moments, the following theorem can be useful for
determining whether a series of moments of progressively higher order up to r
exist.
Theorem 3.23
Existence of Lower
Order Moments
If E(Xr) exists for a given integer r > 0, then E(Xs) exists for all integers s∈[0,r].
Proof
Deﬁne A<1 ¼ {x:|x|s < 1} and A  1 ¼ {x:|x|s

1}. Note that
R 1
1 x
j jsdFðxÞ¼
R
x2A<1 x
j jsdFðxÞ þ
R
x2A  1 x
j jsdFðxÞ:
Since f(x)  |x|s f(x) 8x∈A<1,
3.6
Moments of a Random Variable
139

P x
j js<1
ð
Þ ¼
Z
x2A< 1
dFðxÞ 
Z
x2A< 1
x
j jsdFðxÞ:
Now let r > s, and note that |x|s  |x|r 8x∈A  1. Hence it follows that
Z
x2A  1
x
j jrdFðxÞ 
Z
x2A  1
x
j jsdFðxÞ:
Finally, since
R
x2A<1 x
j jrdFðxÞ  0,
Z 1
1
x
j jsdFðxÞ  P x
j js<1
ð
Þ þ
Z 1
1
x
j jrdFðxÞ<1;
where the right-most inequality is due to the fact that P(|x|s < 1) ∈[0,1] and E(Xr)
exists, implying the absolute convergence of the improper integral deﬁning the
expectation. It follows from
R 1
1 x
j jsdFðxÞ<1 that E(Xs) exists.
n
The theorem implies that if the existence of the rth-order moment about the
origin can be demonstrated, then lower-order moments about the origin are
known to exist. Theorem 3.23 can also be used to demonstrate the nonexistence
of moments, since if E(Xr) does not exist, then necessarily E(Xs) cannot exist for
s > r or else Theorem 3.23 would be contradicted.
Example 3.17
Nonexistence of
Moments Order 2 or
Higher
Let the random variable X have the density function f(x) ¼ 2(x + 1)3I(0,1)(x).
Examine
E(Xa)¼ R 1
0 xa 2(x + 1)3dx. To simplify the integral, make the
substitution y ¼ x + 1, so that x ¼ y  1 and dy ¼ dx, to yield E(Xa) ¼
2
R 1
1
y  1
ð
Þay3 dy. Note that if a ¼ 2, then E(X2) ¼ 2
R 1
1
y1  2y2 þ y3


dy ¼
limy!1 2(ln(y) + 2y1  (1/2)y2), and since the limit diverges, E(X2) does not
exist (i.e., note that ln(y) ! 1 as y ! 1). This implies by Theorem 3.23 that
moments of order 2 or greater do not exist for X. The reader can verify that E(X)
exists and is equal to 1.
□
Existence results analogous to Theorem 3.23 can be stated for moments
about the mean.
Theorem 3.24
Existence of Lower
Order Central Moments
If E Y  m
ð
Þr
ð
Þ exists for a given integer r > 0, then E Y  m
ð
Þs
ð
Þ exists for all
integers s∈[0,r].
Proof
This follows directly from Theorem 3.22 upon deﬁning X ¼ Y  m.
n
One can also infer the existence of moments about the mean from moments
about the origin, and vice versa.
Theorem 3.25
Existence Relationships
Between Moment Types
If E(Xr) (or E((X  m)r)) exists for a given integer r > 0, then E X  m
ð
Þs
ð
Þ (or E(Xs))
exists for all integers s∈[0,r].
Proof
Follows directly from Theorems 3.21–3.24. Details are left to the reader.
n
140
Chapter 3
Expectations and Moments of Random Variables

3.6.3
Nonmoment Measures of Probability Density Characteristics
Note that whether or not moments exist for X, there are other measures of
probability distribution characteristics that are also of interest in applications.
An alternative measure of the central tendency of a density is the median,
deﬁned as follows.
Deﬁnition 3.16
Median of X
Any number, b, satisfying P(xb)1/2 and P(xb)1/2 is called a median of
X, and is denoted by med(X).
The median is a measure of central tendency in the sense that  1/2 of the
probability mass of a density is both to the right and to the left of the median. In
the continuous case, the probability inequalities in Deﬁnition 3.16 can be met
with strict equalities, so that the median is a point at which exactly 1/2 the
probability mass is to the left, and 1/2 to the right, as,
R medðXÞ
1
fðxÞdx ¼
R 1
medðXÞ
fðxÞdx ¼ :5 . The inequalities are necessary for the concept to be generally
applicable
in
the
discrete
case
because
the
discreteness
of
probability
assignments may prevent the conditions from being met as equalities.
Depending on how the density is deﬁned, the median may not be unique
even in the continuous case. However, if b ¼ med(X), and if in the neighborhood
of the point b the CDF of X is continuous and strictly increasing, then med(X) is
unique (why?).
Example 3.18
Median not Unique
Let the random variable X have density function f(x) ¼ (1/6) I{1,2,. . .,6}(x). Then
the median of X is not unique, and can be any number in the interval [3, 4], since
P(x  b)  1/2 and P(x  b)  1/2 8b∈[3, 4].
□
In practice, when there is a continuum of possible medians, as in Example
3.18, it is often the midpoint of the possible medians that is reported as “the”
median. In Example 3.18 above, the value 3.5 would be reported.
Example 3.19
Median of Continuous
RV
The central processing unit (CPU) used by a company that manufactures per-
sonal computers has an operating life until failure that is given by the outcome
of a random variable X having density function f(x) ¼ 1=50
ð
Þex=50I 0;1
ð
ÞðxÞ, where
x is measured in thousands of hours. What is the median operating life of the
CPU?
Answer: We must solve the following equation for med(X):
Z medðXÞ
1
1
50ex=50I 0;1
ð
ÞðxÞdx ¼ :5 or 1  emedðXÞ=50 ¼ :5;
so that med(X) ¼ 34.657. It is thus equally probable that the CPU will operate
more or less than 34,657 hours until failure.
□
3.6
Moments of a Random Variable
141

Another probability density characteristic, which subsumes the median as a
special case, is called a quantile.
Deﬁnition 3.17
Quantile of X
Any number, b, satisfying P(x  b)  p and P(x  b) 1p for p∈(0, 1) is called
a quantile of X of order p (or the (100p)th percentile of the distribution of X).
Note the median is then simply the quantile of X of order .5, or the 50th
percentile of the distribution of X. As in the case of the median, the quantile
of order p may not be unique for a given random variable X. In Example 3.18, any
b ∈[4, 5] would be a quantile of X of order 2/3, while in Example 3.19, the
quantile of order 2/3 would be b ¼ 54.931.
One additional characteristic of a probability distribution that will be espe-
cially useful when we study the maximum likelihood procedure of statistical
inference is a mode of the distribution of X.
Deﬁnition 3.18
Mode of f(x)
Any point, b, at which f(x) exhibits a maximum is called a mode of X, or a
mode of the distribution of X, and is denoted by mode (X).
Some density functions may not have a unique mode. Those that do are
referred to as being unimodal. Note that the density function in Example 3.18
exhibits six modes corresponding to the points x ¼ 1, 2, 3, 4, 5, 6. The density
function in Example 3.17 has one mode at the point x ¼ 0. The density in
Example 3.19 has no mode (Why not? How might the problem be altered so
that mode (X) exists?).
3.7
Moment Generating Functions
The expectation of etX results in a function of t that, when differentiated
with respect to the argument t and then evaluated at t ¼ 0, generates moments
of X about the origin. The function is aptly called the moment-generating
function of X.
Deﬁnition 3.19
Moment Generating
Function (MGF)
The expected value of etX is deﬁned to be the moment-generating function of
X if the expected value exists for every value of t in some open interval
containing 0, i.e., 8t∈(h, h), h > 0. The moment generating function of X
will be denoted by MX(t), and is represented by
MXðtÞ ¼ E etX


¼
X
x2RðxÞ
etxfðxÞ;
discrete
ð
Þ
MXðtÞ ¼ E etX


¼
Z 1
1
etxfðxÞdx;
continuous
ð
Þ
142
Chapter 3
Expectations and Moments of Random Variables

Note that MX(0) ¼ E(e0) ¼ E(1) ¼ 1 is always deﬁned, and from this property
it is clear that a function of t cannot be a MGF unless the value of the function at
t ¼ 0 is 1. The condition that MX(t) must be deﬁned 8t∈(h,h) is a technical
condition that ensures MX(t) is differentiable at the point zero, a property whose
importance will become evident shortly.
We now indicate how the MGF can be used to generate moments about the
origin. In the following theorem, we use the notation drgðaÞ=dxr to indicate
the rth derivative of g(x) with respect to x evaluated at x ¼ a.
Theorem 3.26
Moments from MGF
Let X be a Random Variable for which the MGF, MX(t), exists. Then
m0
r ¼ E Xr
ð
Þ ¼ dr MX ð0Þ
dtr
:
Proof
The proof is facilitated by the following lemma from advanced calculus.
Lemma 3.2
If the function g(t) deﬁned by g(t) ¼ P
x2RðXÞ etxf(x) or
R 1
1 etxf(x)dx converges
for t∈(h, h), h > 0, then drgðtÞ=dtr exists 8t∈(h, h) and for all positive
integers r, and the derivative can be found by differentiating under the
summation sign or differentiating under the integral sign, respectively, as
dr gðtÞ
dtr
¼
X
x2RðxÞ
dretx
dtr fðxÞ or
Z 1
1
dretx
dtr fðxÞdx:
(see D.V. Widder 1961, Advanced Calculus, 2nd Ed., Englewood Cliffs, NJ:
Prentice Hall, pp. 442–447).
If
the
moment
generating
function
MXðtÞ ¼ E etx
ð
Þ ¼
R 1
1 etxdFðxÞ exists
(converges) for t∈(h, h), h > 0, then from Lemma 3.2,
dr MX ðtÞ
dtr
¼
Z 1
1
dr etx
dtr
dFðxÞ ¼
Z 1
1
xretxdFðxÞ:
Evaluating the rth derivative at t ¼ 0 yields
dr MX ð0Þ
dtr
¼
Z 1
1
xrdFðxÞdx ¼ E Xr
ð
Þ:
n
Example 3.20
MGF for Continuous RV
The random variable X has the density function f(x) ¼ ex I(0,1)(x). Find the
MGF, and use it to deﬁne the mean and variance of X.
Answer:
MXðtÞ ¼
Z 1
1
etxexI 0;1
ð
ÞðxÞdx ¼
Z 1
1
ex t1
ð
Þdx ¼ ex t1
ð
Þ
t  1

1
0
¼ 0 
1
t  1 ¼ 1  t
ð
Þ1 provided t<1
ð
Þ
3.7
Moment Generating Functions
143

The mean is deﬁned as m ¼ dMXð0Þ=dt ¼ 1  0
ð
Þ2 ¼ 1: For the variance, recall
from Theorem 3.21 that s2 ¼ m0
2  m2: Then, m0
2 ¼ d2MXð0Þ=dt2 ¼ 2 1  0
ð
Þ3 ¼ 2;
and thus, s2 ¼ 1.
□
There are a number of elementary results relating to moment-generating
functions that can be quite useful in applications. We present these results in
the next theorem.
Theorem 3.27
Properties of MGFs
Let (X1,. . .,Xn) be independent random variable having respective MGFs MXiðtÞ,
for i ¼ 1,. . .,n.
a. If Yi ¼ a Xi + b, then MYiðtÞ¼ ebt MXi at
ð
Þ
b. If Y ¼ Pn
i¼1 Xi; then MY(t) ¼ Qn
i¼1 MXiðtÞ
ð
Þ:
c. If Y ¼ Pn
i¼1 aiXi þ b; then MY(t) ¼ ebt Qn
i¼1 MXi ait
ð
Þ
ð
Þ:
Proof
Left to the Reader.
It is useful to note that since X could be deﬁned as a random variable
that is itself a function of other random variables, a more general conceptualiza-
tion of the MGF is Mg(X)(t) ¼ E (exp (g(X)t)) where g is a function of X.13 The MGF
Mg(X)(t) could then be used to deﬁne moments about the origin for the random
variable deﬁned by g(X). Note that because moments about the origin for g(X) ¼
X  m coincide with moments about the mean for X, the generalized MGF can be
deﬁned appropriately to generate moments about the mean directly.
Example 3.21
MGF for Generating
Central Moments
Let f(x) ¼ exI(0,1)(x), as in Example 3.20. Recall that m ¼ 1 in this case. We ﬁnd
the moment-generating function of the random variable Y ¼ g(X) ¼ X  1, and
use it to deﬁne the variance of X. First of all, note that Theorem 3.27 (a) is
applicable with a ¼ 1 and b ¼ 1. Since MX(t) ¼ (1t)1 for t < 1 from Example
3.20, it follows that M(X  1)(t) ¼ et (1  t)1 for t < 1.
To ﬁnd var (X) ¼ d2M X1
ð
Þð0Þ


=dt2 ¼ E X  1
ð
Þ2:
dM X1
ð
ÞðtÞ
dt
¼ et 1  t
ð
Þ2  1  t
ð
Þ1et
and
d2M X1
ð
ÞðtÞ
dt2
¼ 2et 1  t
ð
Þ3  et 1  t
ð
Þ2 þ 1  t
ð
Þ1et  et 1  t
ð
Þ2;
so that
d2M X1
ð
Þð0Þ
dt2
¼ 1 ¼ varðXÞ:
□
3.7.1
Uniqueness of MGFs
Apart from generating moments, the MGF can be useful for identifying the
density function of a given random variable. This is due to a uniqueness property
13Recall that exp(a)  ea, where a is a real number.
144
Chapter 3
Expectations and Moments of Random Variables

possessed by MGFs that essentially establishes a one-to-one correspondence
between density functions and MGFs. A formal statement of the uniqueness
property is given in the following theorem. The proof of the theorem relies on
the fact that the MGF is a bilateral Laplace transform of the function f(x), and
there is a unique association between a Laplace transform and the function
being transformed. These concepts are beyond the scope of our study, and the
proof will be omitted. The interested reader can refer to Widder (1989) Advanced
Calculus, pp. 459–460 and D.A.S. Fraser (1976) Probability and Statistics. North
Scituate, Duxbury Press, pp. 544–546 for details.
Theorem 3.28
MGF Uniqueness
If a moment-generating function exists for a random variable X having density
function f(x), then the moment generating function is unique. Conversely, the
moment generating function determines the density function of X uniquely, at
least up to a set of points having probability zero.
In other words, a density function has one and only one MGF associated
with it, if a MGF exists at all. Furthermore, if more than one density function is
associated with a given MGF, the densities differ only on a set of points that are
irrelevant for the purposes of assigning probabilities to events for X, i.e., they
differ on a set of points having probability zero. Thus, if one knows the MGF for a
given random variable X, and if one also knows of any density function that
produces this MGF, then that density function sufﬁces as the density function of
the random variable X for purposes of any probability assignments, or for deﬁn-
ing expectations of any functions of X. The following example illustrates the
logic followed in applying the uniqueness theorem.
Example 3.22
Identifying a PDF by its
MGF
Examine the density function f(x) ¼ (b  a)1 I[a,b](x) for a < b. The MGF
associated with this density can be identiﬁed as follows:
MX ðtÞ ¼ E eXt


¼
Z 1
1
ext b  a
ð
Þ1 I½a;b ðxÞdx ¼ b  a
ð
Þ1
Z b
a
extdx
¼ b  a
ð
Þ1 ext
t

b
a
¼
ebteat
t ba
ð
Þ for t 6¼ 0;
1 for t ¼ 0:
(
Now
suppose
a
random
variable
Z
has
a
MGF
deﬁned
by
MZ(t) ¼
(ebt  eat)/(t(b  a)) for t 6¼ 0. Then by the uniqueness theorem, since f(x) above
is associated with this same MGF, the density function of Z can be speciﬁed as
f(z) ¼ (b  a)1 I[a,b](z).
□
When it exists, an MGF can be thought of as a “ﬁngerprint” of a given
density function. In Chapter 4, we will examine a collection of density functions
that have been found to be useful in applications, and we will provide a list of
their MGFs. Later on we will examine a number of important functions
of random variables that will be used for statistical inference purposes, and in
3.7
Moment Generating Functions
145

a notable number of cases, we will be able to identify the probability densities of
these functions by matching their MGFs to the appropriate MGFs in the list we
will have assembled.
3.7.2
Inversion of MGFs
The “recognition” of an MGF as a known ﬁngerprint of some probability density
function is not the only way an MGF can be used to identify the probability
distribution of a random variable. There is an inversion relationship that allows
one to integrate a function involving the MGF to identify the CDF of a random
variable, from which the density function can be deduced. Unfortunately, the
technique involves transform theory and generally complicated integration of
expressions involving complex numbers and is beyond the scope of our study.
Nonetheless, without providing the formal details, we will provide the reader
with the general idea of what is involved.
If we replace the argument t in an MGF by (it), i being the imaginary number
i ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
, then differences in the CDF values are associated with the MGF as
FðbÞ  FðaÞ ¼ 1
2p
Z 1
1
eita  eitb
it
MX ðitÞdt;
for a < b.14 Then the density function can be determined from the CDF using
either the differencing or derivative methods described in Chapter 2 for discrete
and continuous random variables, respectively. For example, regarding the con-
tinuous case, differentiating with respect to b results in the probability density
function fðxÞ ¼ R 1
1 2p
ð
Þ1eitxMX it
ð Þdt: For further information concerning this
inversion property, the reader can consult M. Kendall and A. Stuart (1977), The
Advanced Theory of Statistics, Vol. 1. New York: Macmillan, Chapter 4.
In cases where the MGF does not exist, there is an alternative function that
always exists, called the characteristic function, which serves the same purpose
as the MGF. In particular, there is a unique relationship between characteristic
functions and density functions, analogous to the result stated in Theorem 3.28.
The characteristic function can be inverted to obtain the density function, and
the characteristic function can be used to generate any moments that exist for a
random variable by differentiating the characteristic function an appropriate
number of times, evaluating the derivative at the point zero, and then dividing
the result by (i)k, k being the order of the moment sought (equivalently, k is the
order of the derivative). The characteristic function is deﬁned as fX(t) ¼ E(eitX),
and so complex numbers are involved in the deﬁnition of the characteristic
function. When the MGF exists, the characteristic function is fX(t)  MX(it),
i.e., the characteristic function is identically the MGF evaluated at (it) rather
than t. For example, in Example 3.22, the characteristic function of X would be
ðebit  eaitÞ=ðitðb  aÞÞ for t 6¼ 0. Despite the advantage that fX(t) always exists,
we will not pursue the study of characteristic functions any further, in order to
14Phoebus Dhrymes (1989) Topics in Advanced Econometrics, Springer-Verlag, p. 254.
146
Chapter 3
Expectations and Moments of Random Variables

avoid the use of complex numbers. Interested readers can examine Kendall and
Stuart, Advanced Statistics, Chapter 4, for further details.
3.8
Cumulant Generating Function
The natural logarithm of the moment-generating function deﬁnes a function
called the cumulant-generating function which, when differentiated r times
with respect to t and then evaluated at t ¼ 0, deﬁnes the rth cumulant of a
random variable.
Deﬁnition 3.20
Cumulant-Generating
Function and
Cumulants
The cumulant-generating function of X is deﬁned as c(t) ¼ ln(MX(t)). The rth
cumulant of X is given by kr ¼
dr cð0Þ


= dtr : The ﬁrst four cumulants are
related to moments as follows: k1 ¼ m0
1; k2 ¼ s2; k3 ¼ m3; and k4 ¼ m4  3s4:
The cumulant-generating function can be used directly to generate the
mean, variance, and third moment about the mean via differentiation of
the function to the ﬁrst, second, or third order respectively. The fourth deriva-
tive produces the fourth order cumulant that, when divided by s4, generates
the excess kurtosis measure presented in the previous section. Thus, all of the
moment measures introduced previously to measure central tendency, spread,
skewness, and peakedness of a probability density function are available
through use of the ﬁrst four cumulants generated by the cumulant generating
function.
If X1,. . .,Xn are independent random variables, it follows from Deﬁnition
3.20 and Theorem 3.27 that the cumulant-generating function of Y ¼ Pn
i¼1 Xi
equals the sum of the cumulant-generating functions of the Xi’s. It then also
follows that the cumulant of the sum is the sum of the cumulants, which is the
genesis of the name “cumulant.” Often, the derivatives of the cumulant-
generating function are easier to calculate than the derivatives of the MGF.
Example 3.23
Deﬁning Moments via
the Cumulant
Generating Function
Recall Examples 3.20 and 3.21 where MX(t) ¼ (1  t)1 for t < 1. The cumulant-
generating function of X is given by cX(t) ¼ ln (MX(t)) ¼ ln(1  t) for t < 1.
Then
m ¼ d cX ð0Þ=dt ¼ 1  t
ð
Þ1 jt¼0 ¼ 1
s2 ¼ d2 cX ð0Þ= dt2 ¼ 1  t
ð
Þ2 jt¼0 ¼ 1
m3 ¼ d3 cX ð0Þ= dt3 ¼ 2 1  t
ð
Þ3 jt¼0 ¼ 2
k4 ¼ m4  3s4 ¼ d4 cX ð0Þ= dt4 ¼ 6 1  t
ð
Þ4 jt¼0 ¼ 6
□
3.8
Cumulant Generating Function
147

3.9
Multivariate MGFs and Cumulant Generating Functions
The MGF and cumulant-generating function can be extended to the case of a
multivariate random variable X ¼ (X1,. . .,Xn), as follows:
Deﬁnition 3.21
MGF and Cumulant
Generating Function-
Multivariate Case
The expected value of exp
Pn
j¼1 tjXj


is deﬁned to be the MGF of the
n-variate random variable X ¼ (X1,. . .,Xn) if the expected value exists for all
ti ∈(h,h), for some h > 0, i ¼ 1,. . .,n. The MGF will be denoted by MX(t),
where t ¼ (t1,. . .,tn). Thus,
MXðtÞ ¼
X
x1;...;xn
ð
Þ2RðXÞ
exp
X
n
j¼1
tjxj
 
!
f x1; . . . ; xn
ð
Þ
discrete
ð
Þ
MXðtÞ ¼
Z 1
1
  
Z 1
1
exp
X
n
j¼1
tjxj
 
!
f x1; . . . ; xn
ð
Þdx1 . . . dxn
continuous
ð
Þ:
The cumulant generating function of X is deﬁned as cX(t) ¼ ln (MX(t)).
Letting m0
r Xi
ð
Þ denote the rth moment of Xi about the origin, it can be shown
that
m0
r Xi
ð
Þ ¼ E Xr
i


¼ @r MXð0Þ
@ tr
i
:
Thus, the rth order partial derivative of MX(t) with respect to ti, evaluated at
t ¼ 0 (i.e., the vector t equal to the zero vector), equals the rth order moment
about zero for Xi. Similarly, the rth partial derivative of cX(t) with respect to ti,
evaluated at t ¼ 0, equals the rth cumulant of the random variable Xi, which
then allows means, variances, skewness and kurtosis measures to be calculated
directly for each of the Xi’s.
An analog of the MFG uniqueness theorem applies to the multivariate MGF.
In fact, interpreting X as a vector in the statement of Theorem 3.28 produces the
appropriate multivariate MGF uniqueness theorem.
If the MGF for an n-variate random variable (X1,. . .,Xn) is known, the mar-
ginal MGF for a subset of m < n of the random variables is easily found by
setting the ti’s associated with the remaining nm random variables to zero, as
presented in the next theorem.
Theorem 3.29
Marginal MGFs from
Multivariate MGFs
Let X ¼ (X1,. . .,Xn) have MGF MX(t), and let X(m) ¼ (Xj, j∈J) be any m-element
subset of the random variables in X, where J 	 {1, 2,. . .,n} and N(J) ¼ m < n.
Deﬁne t(m) ¼ (tj, j∈J). Then the MGF of X(m), referred to as the marginal MGF
of X(m), can be represented as MXðmÞ tðmÞ


¼ MX(t*), where the elements in t*are
deﬁned by tj IJ(j).
148
Chapter 3
Expectations and Moments of Random Variables

Proof
MX t
ð Þ ¼ E exp
X
n
j¼1
t
j Xj
 
!
 
!
¼ E exp
X
j2J
tjXj
 
!
 
!
since t
j ¼ 0 if j=2J


¼ MXðmÞ tðmÞ


by definition
ð
Þ:
n
A marginal cumulant-generating function can be deﬁned as the natural
logarithm of a marginal MGF.
If X has MGF MX(t), it can be shown that X1,. . .,Xn are independent iff
MXðtÞ ¼ Qn
i¼1 MXiðtiÞ , or equivalently, iff cX ðtÞ ¼ Pn
i¼1 cXiðtÞ (see S.F. Arnold
(1990) Mathematical Statistics, Englewood Cliffs, NJ: Prentice-Hall, pp. 118–119).
Example 3.24
Marginal MGF and
Cumulant Generating
Functions
Suppose the joint MGF of the bivariate random variable (X1, X2) is given by
MX(t) ¼ exp P2
i¼1 miti þ ð1=2Þ P2
i¼1
P2
j¼1 sijtitj


(we will see in Chapter 4 that
this is the MGF associated with a bivariate “normal” density function). Then
the marginal MGF of X1 can be deﬁned by setting t2 ¼ 0 in MX(t) to obtain
MX1 t1
ð
Þ ¼ exp(m1t1 + ð1= 2Þ s11 t2
1) (which is the MGF associated with a univariate
“normal” density function). The marginal cumulant-generating function is
given by cX1 ðtÞ ¼ ln MX1ðtÞ
ð
Þ ¼ m1t1 þ ð1=2Þs11t2
1.
□
If we take cross partial derivatives of the MGF of X, and evaluate the deriva-
tive at t ¼ 0, we obtain
@rþs MX ð0Þ
@ tr
i @ ts
j
¼ E Xr
iXs
j


:
This expectation is an example of a joint moment. The cross partial
derivative of the cumulant generating function given by
@2cX 0
ð Þ


= @ti@tj


¼
E((Xi  E(Xi))(Xj  E(Xj))) is the covariance between Xi and Xj. These concepts
are discussed further in the next section.
3.10
Joint Moments, Covariance, and Correlation
In the case of multivariate random variables, the concept of joint moments
becomes relevant. The formal deﬁnitions of joint moments about the origin
and about the mean are as follows:
Deﬁnition 3.22
Joint Moment About the
Origin
Let X and Y be two random variables having joint density function f(x,y).
Then the (r,s)th joint moment of (X,Y) (or of f(x,y)) about the origin is deﬁned
by
m0
r;s ¼
X
X
x2RðXÞy2RðYÞ
xrysf x; y
ð
Þ
discrete
ð
Þ
m0
r;s ¼
Z 1
1
Z 1
1
xrysf x; y
ð
Þdxdy
continuous
ð
Þ
3.10
Joint Moments, Covariance, and Correlation
149

Deﬁnition 3.23
Joint Moments About
the Mean (or Central
Joint Moment)
Let X and Y be two random variables having joint density function f(x,y).
Then the (r,s)th joint moment of (X,Y) (or of f(x,y)) about the mean is deﬁned
by
mr;s ¼
X
X
x2RðXÞy2RðYÞ
x  EðXÞ
ð
Þr y  EðYÞ
ð
Þsf x; y
ð
Þ
discrete
ð
Þ
mr;s ¼
Z 1
1
Z 1
1
x  EðXÞ
ð
Þr y  EðYÞ
ð
Þsf x; y
ð
Þdxdy
continuous
ð
Þ
3.10.1
Covariance and Correlation
Regarding joint moments, our immediate interest is on a particular joint
moment about the mean, m1,1, and the relationship between this moment and
moments about the origin. The central moment m1,1 is given a special name and
symbol, and we will see that m1,1 is useful as a measure of “linear association”
between X and Y.
Deﬁnition 3.24
Covariance
The central joint moment m1,1 ¼ E(X  E(X))(Y  E(Y)) is called the covari-
ance between X and Y, and is denoted by the symbol sXY, or by cov(X,Y).
Note that there is a simple relationship between sXYand moments about the
origin that can be used for the calculation of the covariance.
Theorem 3.30
Covariance in Terms of
Moments About the
Origin
sXY ¼ E XY
ð
Þ  EðXÞEðYÞ:
Proof
This result follows directly from the properties of the expectation operation. In
particular, by deﬁnition
sXY ¼ E X  EðXÞ
ð
Þ Y  EðYÞ
ð
Þ
ð
Þ ¼ EðXY  EðXÞ
ð
ÞY  XEðYÞ þ EðXÞEðYÞÞ
¼ E XY
ð
Þ  EðXÞEðYÞ
n
Example 3.25
Covariance Calculation
Let the bivariate random variable (X,Y) have a joint density function f(x,y)
¼ (x þ y) I[0,1](x)I[0,1](y). Find cov(X,Y).
Answer: Note that
E XY
ð
Þ ¼
Z 1
0
Z 1
0
xyðx þ yÞdxdy ¼
Z 1
0
Z 1
0
x2y þ xy2


dxdy ¼ 1
3
EðXÞ ¼
Z 1
0
Z 1
0
x x þ y
ð
Þdxdy ¼
Z 1
0
Z 1
0
x2 þ xy


dxdy ¼ 7
12
EðYÞ ¼
Z 1
0
Z 1
0
y x þ y
ð
Þdxdy ¼
Z 1
0
Z 1
0
yx þ y2


dxdy ¼ 7
12 :
Then, by Theorem 3.30, cov(X,Y) ¼ 1/3  (7/12)(7/12) ¼ (1/144).
□
150
Chapter 3
Expectations and Moments of Random Variables

A useful corollary to Theorem 3.30 is that the expectation of a product of
two random variables is the product of the expectations iff sXY ¼ 0, formally
stated as follows.
Corollary 3.5
Expectation of Product
Equals Product of
Expectations
E XY
ð
Þ ¼ EðXÞEðYÞ iff sXY ¼ 0:
Proof
This follows directly from Theorem 3.30 upon setting sXY to zero (sufﬁciency) or
setting E(XY) equal to E(X) E(Y) (necessity).
n
What does sXY measure? The covariance is a measure of the linear associa-
tion between two random variables, where the precise meaning of linear associ-
ation will be made clear shortly. Our discussion will be facilitated by observing
that the value of sXY exhibits a deﬁnite upper bound in absolute value which is
expressible as a function of the variances of the two random variables involved.
The bound on sXY follows from the following inequality.
Theorem 3.31
Cauchy-Schwarz
Inequality
ðEðWZÞÞ2  EðW2ÞEðZ2Þ
Proof
The quantity E((l1W + l2Z)2) must be greater than or equal to 0 8(l1, l2) since
(l1W + l2Z)2 is a random variable having only non-negative outcomes. Thus
l2
1 E(W2) + l2
2 E(Z2) + 2 l1 l2 E(WZ)  0 8(l1, l2), which in matrix terms can be
represented as
l1
l2
½
 E W2


E WZ
ð
Þ
E WZ
ð
Þ
E Z2



 l1
l2
"
#
 0 8 l1; l2
ð
Þ:
The last inequality is precisely the deﬁning property of positive semideﬁ-
niteness for the (2  2) matrix in brackets,15 and the matrix in brackets will be
positive semideﬁnite iff E(W2)E(Z2)  (E(WZ))2
 0 (see the Appendix
Section 3.12).
n
The covariance bound we seek is stated in the following theorem.
Theorem 3.32
Covariance Bound
sXY
j
j  sX sY:
Proof
Let W ¼ (X  E(X)) and Z ¼ (Y  E(Y)) in the Cauchy-Schwarz inequality. Then

EððX  EðXÞÞðY  EðYÞÞÞ
2
 EððX  EðXÞÞ2ÞEð Y  EðYÞ
ð
Þ2Þ, or equivalently, s2
XY
 s2
X s2
Y which holds iff | sXY |  sX sY.
n
15Recall that a matrix A is positive semideﬁnite iff t0At  0 8 t, and A is positive deﬁnite iff t0At > 0 8 t 6¼ 0.
3.10
Joint Moments, Covariance, and Correlation
151

Thus, the covariance between X and Y is upper-bounded in absolute value by
the product of the standard deviations of X and Y. Using this bound, we can
deﬁne a useful scaled version of the covariance, called the correlation between X
and Y, as follows.
Deﬁnition 3.25
Correlation
The correlation between two random variables X and Y is deﬁned by
corrðX; YÞ ¼ rXY ¼
sXY
sX sY.
Example 3.26
Correlation Calculation
Refer to Example 3.25. Note that
E X2


¼
Z 1
0
Z 1
0
x2 x þ y
ð
Þdxdy ¼ 5
12
E Y2


¼
Z 1
0
Z 1
0
y2 x þ y
ð
Þdxdy ¼ 5
12 ;
so that
s2
X ¼ E X2


 EðXÞ
ð
Þ2 ¼ 5=12  ð7=12Þ2 ¼ 11=144;
and
s2
Y ¼ E Y2


 EðYÞ
ð
Þ2 ¼ 5=12  7=12
ð
Þ2 ¼ 11=144:
Then the correlation between X and Y is given by
rXY ¼ sXY
sX sY
¼
1=144
ð11=144 Þ1=2 ð11=144 Þ1=2 ¼ 1
11 :
□
Bounds on the correlation between X and Y follow directly from the bounds
on the covariance between X and Y.
Theorem 3.33
Correlation Bound
 1  rXY  1:
Proof
This follows directly from Theorem 3.32 via division by sxsy.
n
The covariance equals its upper bound value of sX sY iff the correlation
equals its upper bound value of 1, and the covariance equals its lower bound
value of sX sY iff the correlation equals its lower bound value of 1.
Assuming that the covariance exists, a necessary condition for the indepen-
dence of X and Y is that sXY ¼ 0 (or equivalently, that rXY ¼ 0 if sX sY 6¼ 0).
Theorem 3.34
Relationship Between
Independence and
Covariance
If X and Y are independent, then sXY ¼ 0 (assuming the covariance exists).
152
Chapter 3
Expectations and Moments of Random Variables

Proof
If X and Y are independent, then f(x,y) ¼ fX(x) fY(y). It follows that
sXY ¼
Z 1
1
Z 1
1
x  EðXÞ
ð
Þ y  EðYÞ
ð
ÞdF x; y
ð
Þ
¼
Z 1
1
x  EðXÞ
ð
ÞdFXðxÞ
Z 1
1
y  EðYÞ
ð
ÞdFYðyÞ
¼ EðXÞ  EðXÞ
ð
Þ EðYÞ  EðYÞ
ð
Þ ¼ 0  0 ¼ 0:
n
The converse of Theorem 3.34 is not true—there can be dependence between
X and Y, even functional dependence, and the covariance between X and Y could
nonetheless be zero, as the following example illustrates.
Example 3.27
Bivariate Function
Dependence with
sXY ¼ 0
Let X and Y be two random variables having a joint density function given by
f(x,y) ¼ 1.5 I[1,1](x) I ½0;x2 (y). Note this density implies that (x,y) points are
equally likely to occur on and below the parabola represented by the graph of
y ¼ x2. There is a direct functional dependence between X and the range of Y,
so that f(y | x) will change as x changes and thus X and Y must be dependent
random variables. Nonetheless, sXY ¼ 0. To see this, note that
E XY
ð
Þ ¼ 1:5
Z 1
1
Z x2
0
xydydx ¼ 1:5
Z 1
1
1=2
ð
Þx5dx ¼ :75 x6
6

1
1
¼ 0;
EðXÞ ¼ 1:5
Z 1
1
Z x2
0
xdydx ¼ 1:5
Z 1
1
x3dx ¼ 1:5 x4
4

1
1
¼ 0;
EðYÞ ¼ 1:5
Z 1
1
Z x2
0
ydydx ¼ 1:5
Z 1
1
1=2
ð
Þx4dx ¼ :75 x5
5

1
1
¼ :3:
Therefore, sXY ¼ E(XY)  E(X)E(Y) ¼ 0  0(.3) ¼ 0.
□
3.10.2
Correlation, Linear Association and Degeneracy
We now demonstrate that when the covariance takes its maximum absolute
value, and thus rXY ¼ +1 or 1, there is a perfect positive (rXY ¼ +1) or negative
(rXY ¼ 1) linear relationship between X and Y that holds with probability one
(i.e., P(y ¼ a + bx) ¼ 1 or P(y ¼ a  bx) ¼ 1). The demonstration is facilitated
by the following useful result.
Theorem 3.35
Degeneracy when s2= 0
Let Z be a Random Variable for which s2
Z ¼ 0. Then P(z ¼ E(Z)) ¼ 1
Proof
Let g(Z) ¼ (Z  E(Z))2. Then
3.10
Joint Moments, Covariance, and Correlation
153

P EðZÞ  a<z<EðZÞ þ a
ð
Þ ¼ P ðz  EðZÞ Þ2 < a2


¼ 1  P ðz  EðZÞ Þ2  a2


 1  s2
Z = a2;
where the inequality is established using Markov’s inequality. If s2
Z ¼ 0, then
P(E(Z)  a < z < E(Z) + a) ¼ 1 8 a > 0, and since only z ¼ E(Z) satisﬁes the
inequality 8a > 0, P(z ¼ E(Z)) ¼ 1 when s2
Z¼0.
n
The result on the linear relationship between X and Y when rXY ¼ +1 or 1,
or equivalently, when sXY achieves its upper and lower bound, is as follows.
Theorem 3.36
Correlation Bounds and
Linearity
If rXY ¼ +1 or 1, then P(y ¼ a1 + bx) ¼ 1 or P(y ¼ a2bx) ¼ 1, respectively,
where a1 ¼ E(Y)  (sY/sX)E(X), a2 ¼ E(Y) + (sY/sX)E(X), and b ¼ (sY/sX).
Proof
Deﬁne Z ¼ l1 (X  E(X)) + l2 (Y  E(Y)), and note that E(Z) ¼ 0. It follows
immediately that s2
Z ¼ E(Z2) ¼ E((l1(X  E(X)) + l2(Y  E(Y)))2)
¼ l2
1 E((X 
E(X))2) + l2
2 E((Y  E(Y))2) + 2 l1 l2 sXY  0 8 l1, l2, which can be represented in
matrix terms as
s2
Z ¼ ½ l1
l2 
s2
X
sXY
sXY
s2
Y

 l1
l2
"
#
 0 8ðl1; l2Þ:
If rXY ¼ +1 or 1, then sXY achieves either its (nominal) upper or lower bound,
respectively, or equivalently, s2
XY ¼ s2
X s2
Y. It follows that the above 2  2 matrix
is singular, since its determinant would be zero. Then the columns of the matrix
are linearly dependent, so that there exist nonzero values of l1 and l2 such that
s2
X
sXY
sXY
s2
Y

 l1
l2
"
#
¼
0
0
"
#
and for these l-values, the quadratic form above, and thuss2
Z, achieves the value 0.
A solution for l1 and l2 is given by l1 ¼ sXY/s2
X and l2 ¼ 1 which can be validated
by substituting these values for l1 and l2 in the linear equations, and noting that
s2
Y ¼ s2
XY = s2
X under the prevailing assumptions. Since s2
Z ¼ 0 at these values of l1
and l2, it follows from Theorem 3.35 that P(z ¼ 0) ¼ 1 (recall E(Z) ¼ 0).
Given the deﬁnition of Z, substituting the above solution values for
l1 and l2 obtains an equivalent probability statement P(y ¼ (E(Y)  (sXY/s2
X )
E(X)) + (sXY/s2
X) x) ¼ 1. If rXY ¼ +1, then sXY ¼ sXsY, yielding P(y ¼ a1 + bx) ¼ 1
in the statement of the theorem, while if rXY ¼ 1, then sXY ¼ sXsY, yielding
P(y ¼ a2  bx) ¼ 1 in the statement of the theorem.
n
The theorem implies that when rXY ¼ +1 (or 1), the event that the out-
come of (X,Y) is on a straight line with positive (or negative) slope occurs with
probability 1. As a diagrammatic illustration, if (X,Y) is a discrete bivariate
random variable, then the situation where rXY ¼ +1 would be exempliﬁed
154
Chapter 3
Expectations and Moments of Random Variables

by Figure 3.10. Note in Figure 3.10 that f(x,y) assumes positive values only for
points along the line y ¼ a + bx, reﬂecting the fact that P(y ¼ a + bx) ¼ 1. This
situation illustrates what is known as a degenerate random variable and a
degenerate density function. The deﬁning characteristic of a degenerate random
variable is that it is an n-variate random variable (X1, . . ., Xn) whose components
satisfy one or more linear functional relationships with probability one, i.e., if P
(ai + Pn
j¼1 bijxj ¼ 0) ¼1 for i ¼ 1,. . .,m, then (X1,. . .,Xn) is a degenerate random
variable.16 A characteristic of the accompanying degenerate density function for
(X1,. . .,Xn) is that the entire mass of probability (a mass of 1) is concentrated on a
collection of points that lie on a hyperplane of dimension less than n, the
hyperplane being deﬁned by the collection of linear functional relationships.
Degeneracy causes no particular difﬁculty in the discrete case—probabilities
of events for the degenerate random variable (X1,. . .,Xn) can be calculated in the
usual way by summing the degenerate density function over the outcomes in the
event of interest. However, degeneracy in the continuous case results in f(x1,. . .,
xn) not being a density function according to our original deﬁnition of
the concept. For a heuristic description of the problem, examine the diagram-
matic illustration in Figure 3.11 for a degenerate bivariate random variable in
the continuous case. Intuitively, because there is no volume under the graph of
f(x,y),
Ðx2
x1
Ðy2
y1
f(x,y) dy dx ¼ 0 8 x1  x2 and 8 y1  y2, and f(x,y) cannot be integrated
in the usual way to assign probabilities to events for (X, Y). However, there is
area below the graph of f(x,y) and above the line y ¼ a + bx representing the
probability mass of 1 distributed over a segment (or perhaps, all) of this line.
Since only subsets of the set {(x,y):y ¼ a + bx, x∈R(X)}17 are assigned nonzero
f(x,y)
x
y = a + bx
y
Figure 3.10
rXY ¼ +1, Discrete case.
16The concept of degeneracy can be extended by calling (X1,. . .,Xn) degenerate if the components satisfy one or more functional
relationships (not necessarily linear) with probability 1. We will not examine this generalization here.
17Equivalently, {(x,y): x ¼ b–1(ya), y∈R(Y)}.
3.10
Joint Moments, Covariance, and Correlation
155

probability, the degenerate density function can be used to assign probabilities to
events by use of line integrals,18 which essentially integrate f(x,y) over subsets of
points along the line y ¼ a + bx. The general concept of line integrals is beyond
the scope of our study, but in essence, the relevant integral in the current context
is of the form
R
x2A f(x,a + bx) dx. Note the linear relationship linking y and x is
explicitly accounted for by substituting a + bx for y in f(x,y), which converts f(x,
y) into a function of the single variable x. Then the function of x is integrated
over the points in the event A for x, which determines the probability of the
event B ¼ {(x,y):y ¼ a + bx, x∈A} for the bivariate random variable (X,Y).
Having introduced the concept of degeneracy, we can alternatively charac-
terize rXY ¼ +1 or 1 as a case where the bivariate random variable (X,Y), and its
accompanying joint density function, are degenerate, with X and Y satisfying,
respectively, a positively or negatively sloped linear functional relationship,
with probability one. What can be said about the relationship between X and Y
when |rxy| < 1? The closer |rXY| is to one, the closer the relationship between X
and Y is to being linear, where “closeness” can be interpreted as follows. Deﬁne
the random variable ^Y ¼ a + bX to represent predictions of Youtcomes based on a
linear function of X. We will choose the coefﬁcients a and b so that ^Y is the best
linear prediction of Y, where best is taken to mean “minimum expected squared
distance between outcomes of Y and outcomes of ^Y.”
Theorem 3.37
Best Linear Prediction
of Y Outcomes
Let (X, Y) have moments of at least the second order, and let ^Y¼a + bX. Then the
choices of a and b that minimize E(d2(Y, ^Y)) ¼ E((Y  (a + b(X)))2) are given by
a ¼ E(Y)  (sXY/s2
X)E(X) and b ¼ (sXY/s2
X).
f(x,y)
x
y = a + bx
y
Figure 3.11
rXY ¼ +1, Continuous
case.
18For an introduction to the concept of line integrals, see E. Kreyzig (1979) Advanced Engineering Mathematics, 4th ed. New York:
Wiley, Chapter 9.
156
Chapter 3
Expectations and Moments of Random Variables

Proof
Left to the reader.
n
Now deﬁne V ¼ Y  ^Y to represent the deviations between outcomes of Y
and outcomes of the best linear prediction of Y outcomes as deﬁned in Theorem
3.37. Because E(Y) ¼ E(^Y), E(V) ¼ 0. It follows that
s2
Y ¼ E
Y  EðYÞ
ð
Þ2


¼ E
^Y  E ^Y

 
þ V

2

	
¼ s2
^Y þ s2
V þ s^YV;
where
s2
V ¼ E V2


¼ E d2 Y; ^Y




¼ E d2 Y; a þ bX
ð
Þ


¼ s2
Y  s2
XY=s2
X ¼ s2
Y 1  r2
XY


;
s2
^Y ¼ E ^Y  E ^Y

 

2
¼ s2
Yr2
XY;
s^YV ¼ E
^Y  E ^Y

 


V


¼ sXY=s2
X


E X  EðXÞ
ð
ÞV
ð
Þ ¼ 0:
Thus, the variance of Y is decomposed into a proportion r2
XY due to ^Y and a
proportion (1  r2
XY) due to V, i.e., s2
Y ¼ s2
^Y þ s2
V ¼ s2
Y r2
XY þ s2
Y 1  r2
XY


:
We can now interpret values of rXY ∈(1, 1). Speciﬁcally, r2
XY is the propor-
tion of the variance in Y that is explained by the best linear prediction of the
form ^Y ¼ a + bX, and the proportion of the variance unexplained is (1  r2
XY ).
Relatedly, s2
Y 1  r2
XY


is precisely the expected squared distance between
outcomes of Y and outcomes of the best linear prediction ^Y¼a + bX. Thus, the
closer | rXY | is to 1, the more the variance in Y is explained by the linear function
a + bX, and the smaller is the expected squared distance between Y and ^Y¼a +
bX. It is in this sense that the higher the value of | rXY |, the closer is the linear
association between Y and X.
If rXY ¼ 0, the random variables are said to be uncorrelated. In this case,
Theorem 3.37 indicates that the best linear predictor is E(Y)—there is effectively
no linear association with X whatsoever. The reader should note that Yand X can
be interchanged in the preceding argument, leading to an analogous interpreta-
tion of the degree of linear association between X and ^X¼a + bY (for appropriate
changes in the deﬁnitions of a and b).
3.11
Means and Variances of Linear Combinations of Random Variables
Determining the mean and variance of random variables that are deﬁned as
linear combinations of other random variables is a problem that often arises in
practice. While this determination can be accomplished from ﬁrst principles by
applying the basic deﬁnitions of mean and variance to the linear function of
random variables, there are certain general results that facilitate and expedite
the process. In particular, we will see that the mean and variance of a linear
combination of random variables can be expressed as simple functions of the
means, variances, and covariances of the random variables involved in the linear
combination. Our ﬁrst result concerns the determination of the mean.
3.11
Means and Variances of Linear Combinations of Random Variables
157

Theorem 3.38
Mean of a Linear
Combination
Let Y ¼Pn
i¼1 aiXi ¼ a0Xwhere theai0sare real constants. Then E(Y)¼ P
n
i¼1
aiE Xi
ð
Þ¼
a0EðXÞ, where
a ¼
a1
...
an
2
64
3
75 and X ¼
X1
...
Xn
2
64
3
75:
Proof
EðYÞ ¼ E
X
n
i¼1
aiXi
 
!
¼
X
n
i¼1
E aiXi
ð
Þ ¼
X
n
i¼1
aiE Xi
ð
Þ (Theorem 3.9 and 3:6Þ
n
Regarding the variance of the linear combination of random variables, we
have the following result.
Theorem 3.39
Variance of a Linear
Combination
Let Y ¼ Pn
i¼1 aiXi where the ai0s are real constants. Then s2
Y ¼ P
n
i¼1
a2
i s2
Xiþ
2 P P
i<j
aiajsXiXj ¼ a0CovðXÞa , where
CovðXÞ ¼ E
X  EðXÞ
ð
Þ X  EðXÞ
ð
Þ0


is
the covariance matrix of X.
Proof
s2
Y ¼ E
Y  EðYÞ
ð
Þ2


¼ E
X
n
i¼1
ai Xi  E Xi
ð
Þ
ð
Þ
 
!2
¼ E
X
n
i¼1
a2
i Xi  E Xi
ð
Þ
ð
Þ2 þ 2
X X
i<j
aiai Xi  E Xi
ð
Þ
ð
Þ Xj  E Xj




"
#
¼
X
n
i¼1
a2
i s2
Xi þ 2
X X
i<j
aiaisXiXj ¼ a0E
XEðXÞ
ð
Þ XEðXÞ
ð
Þ0


a
where the penultimate equality follows from Theorems 3.9 and 3.6.
n
We formally deﬁne the notion of covariance matrix below and further moti-
vate its content and meaning.
Deﬁnition 3.26
Covariance Matrix
The covariance matrix of an n-variate random variable X is the (n  n)
symmetric matrix whose i; j
ð
Þthelement is the covariance between Xi and Xj,
deﬁned as CovðXÞ ¼ E
XEðXÞ
ð
Þ XEðXÞ
ð
Þ0


.
Note that because the covariance betweenXi and Xi is, by deﬁnition, the
variance of Xi , the covariance matrix has the variances of the Xi
0s along its
diagonal. In order to appreciate the full informational content of the covariance
matrix, note that, by deﬁnition,
158
Chapter 3
Expectations and Moments of Random Variables

Cov
nn
ð
Þ ðXÞ ¼ E
X1  E X1
ð
Þ
...
Xn  E Xn
ð
Þ
2
66664
3
77775
X1  E X1
ð
Þ
ð
Þ    Xn  E Xn
ð
Þ
ð
Þ
½

¼ E
X1  E X1
ð
Þ
ð
Þ2
X1  E X1
ð
Þ
ð
Þ X2  E X2
ð
Þ
ð
Þ
  
X1  E X1
ð
Þ
ð
Þ Xn  E Xn
ð
Þ
ð
Þ
X2  E X2
ð
Þ
ð
Þ X1  E X1
ð
Þ
ð
Þ
X2  E X2
ð
Þ
ð
Þ2
  
..
.
...
  
..
.
...
Xn  E Xn
ð
Þ
ð
Þ X1  E X1
ð
Þ
ð
Þ
  
Xn  E Xn
ð
Þ
ð
Þ2
2
6666666664
3
7777777775
¼
s2
X1
sX1X2
  
sX1Xn
sX2X1
s2
X2
  
...
...
  
..
.
...
sXnX1
  
  
s2
Xn
2
6666666664
3
7777777775
:
Thus, the covariance matrix has the variance of the ith random variable
displayed in the (i,i)th (diagonal entry) position in the matrix, and the covariance
between the ith and jth random variables displayed in the (i,j)th position (off-
diagonal entry) in the matrix. Since sXiXj ¼ sXjXi the covariance matrix is sym-
metric, i.e., the (i,j)th entry is exactly equal to the (j,i)th entry 8i 6¼ j.
Note that it is necessarily the case that the covariance matrix is a positive
semideﬁnite matrix because s2 ¼ a0CovðXÞa  0 ; 8a, which necessarily follows
from the fact that variances cannot be negative. (Recall that a matrix Z is
positive semideﬁnite iff a0Za  0; 8a).
The preceding results can be extended to the case where Y is a vector deﬁned
by linear combinations of the n-variate random variable X. We ﬁrst extend the
results corresponding to the mean of Y.
Theorem 3.40
Mean of a Vector of
Linear Combinations
(Pre Multiplication)
Let Y ¼ AX where A is a k  n constants, and X is an n  1 vector of random
variables. Then EðYÞ ¼ E AX
ð
Þ ¼ AEðXÞ.
Proof
This follows straightforwardly from Theorem 3.38 and the fact that an expecta-
tion of a vector is the vector of expectations.
n
A useful corollary to Theorem 3.40 concerns the generalization where X is a
n  p matrix of random variables.
Corollary 3.6 Mean of a
Matrix of Linear
Combinations
(Pre Multiplication)
LetY ¼ AX whereAis a k  n matrix of real constants, andX is an n  p matrix
of random variables. Then EðYÞ ¼ E AX
ð
Þ ¼ AEðXÞ.
3.11
Means and Variances of Linear Combinations of Random Variables
159

Proof
This follows directly from Theorem 3.40 applied columnwise to the matrixAX.n
If we postmultiply rather than premultiply a random matrix X by a conform-
able matrix of constants, we obtain a result on expectation qualitatively similar
to the preceding result.
Corollary 3.7 Mean of a
Matrix of Linear
Combinations
(Post Multiplication)
Let Y ¼ XB, where X is a n  p matrix of random variables and B is a p  m
matrix of real constants. Then EðYÞ ¼ E XB
ð
Þ ¼ EðXÞB.
Proof
E XB
ð
Þ ¼ E B0X0
ð
Þ0 ðproperty of matrix transposeÞ
¼ E B0X0
ð
Þ
ð
Þ0 ðexpectation is an elementwise operatorÞ
¼ B0E X0
ð
Þ
ð
Þ0 ðCorollary 3:6Þ
¼ EðXÞB ðproperty of matrix transposeÞ
n
If a random matrix X is both premultiplied and postmultiplied by conform-
able matrices of real constants, then the previous two corollaries can be com-
bined into the following result:
Corollary 3.8 Mean of a
Matrix of Linear
Combinations (Pre and
Post Multiplication)
Let A be a k  n matrix of real constants, let X be a n  p matrix of random
variables, and letBbe a p  m matrix of real constants. ThenE AXB
ð
Þ ¼ AEðXÞB.
Proof
Let Z ¼ XB. Then by Corollary 3.6, E AXB
ð
Þ ¼ E AZ
ð
Þ ¼ AEðZÞ ¼ AE XB
ð
Þ, which
equals AEðXÞB by Corollary 3.7.
n
When Y ¼ AX is a vector of two or more random variables, we can deﬁne a
variance for eachYi, as well as a covariance for each pair Yi; Yj


. We are led to a
generalization of Theorem 3.39 that involves the deﬁnition of the covariance
matrix of the (k  1) random vector Y ¼ AX.
Theorem 3.41
Covariance Matrix of
Linear Combination
Let Y ¼ AX where A is a k  n matrix of real constants and X is a n  1 vector
of random variables. Then CovðYÞ ¼ Cov AX
ð
Þ ¼ ACovðXÞA0.
Proof
By deﬁnition,
CovðYÞ ¼ E
Y  EðYÞ
ð
Þ Y  EðYÞ
ð
Þ0


¼ E A X  EðXÞ
ð
Þ X  EðXÞ
ð
Þ0A0


ðsubstitution and Theorem 3:40Þ
¼ AE
X  EðXÞ
ð
Þ X  EðXÞ
ð
Þ0


A0 ðCorollary 3:8Þ
¼ ACovðXÞ A0
ðby definitionÞ
n
160
Chapter 3
Expectations and Moments of Random Variables

We illustrate the use of some of the above theorems in the following
example, where we also introduce the notion of a correlation matrix (see Exam-
ple 3.28 part (g)).
Example 3.28
Calculating Means,
Covariances, and
Correlations of Linear
Combinations
Your company sells two brands of blank recordable DVDs: Blueray (BR) and
standard (S). The price of a package of BR disks is $4 while the standard disks
sell for $3 a package. The quantities of the disk packages sold on any given day
are represented by the bivariate random variable Q ¼ (QBR, QS), where
EðQÞ ¼
10
30


and CovðQÞ ¼
2
3
3
5


:
a. What is the expected value of the revenue obtained from the sale of DVDs on
any given day?
Answer: Revenue (in dollars) is deﬁned as
R ¼ 4 QBR þ3 QS ¼ ½ 4
3  QBR
QS


;
and Theorem 3.39 applies. Therefore,
EðRÞ ¼ ½ 4
3  10
30
"
#
¼ 130:
b. What is the variance associated with daily revenue?
Answer: Theorem 3.39 applies here. We have that
s2
R ¼ ½ 4
3 
2
3
3
5


4
3


¼ 5:
c. Production costs per disk package are $2.50 and $2 for the Blueray
and standard DVDs, respectively. Deﬁne the expected value of the vector
R
C


, where C ¼ 2.50 QBR + 2 QS represents total cost of DVDs sold on any
given day.
Answer: Theorem 3.40 can be used here (we could also apply Theorem 3.38
to obtain E(C), since we already know E(R)from above).
E
R
C
"
#
¼
4
3
2:5
2

 10
30
"
#
¼
130
85
"
#
d. What is the covariance matrix of
R
C


?
Answer: Using Theorem 3.41,
Cov
R
C



	
¼
4
3
2:5
2


2
3
3
5


4
2:5
3
2


¼
5
3:5
3:5
2:5


:
e. What is the expected level of proﬁt on any given day?
3.11
Means and Variances of Linear Combinations of Random Variables
161

Answer: Proﬁt is deﬁned as P ¼ R  C, and Theorem 3.38 implies that
E P
ð
Þ ¼ 1 1
½

130
85


¼ 45:
f. What is the variance of daily proﬁt?
Answer: Applying Theorem 3.39 results in
s2
P ¼ ½1 1
5
3:5
3:5
2:5


1
1


¼ :5
g. A matrix of correlations (or, correlation matrix) for X ¼ X1; :::; Xn
ð
Þcan be
deﬁned by pre- and post-multiplication of the covariance matrix by the
inverse of the diagonal matrix of standard deviations, i.e., (reader please
verify):
CovðXÞ ¼
sX1
..
.
sXn
2
64
3
75
1
CovðXÞ
sX1
..
.
sXn
2
64
3
75
1
The (i,j)th entry of the correlation matrix is the correlation between Xi and
Xj. Deﬁne the correlation matrix for Q.
Answer:
CorrðQÞ ¼
ﬃﬃﬃ
2
p
0
0
ﬃﬃﬃ
5
p
"
#1
2
3
3
5
"
#
ﬃﬃﬃ
2
p
0
0
ﬃﬃﬃ
5
p
"
#1
¼
1
:949
:949
1


Note rQBR;QS ¼ .949, which is given by the off-diagonal elements in this
(2  2) case, while the diagonal elements are ones because these values
represent the correlation of a random variable with itself.
□
3.12
Appendix: Proofs and Conditions for Positive Semideﬁniteness
3.12.1
Proof of Theorem 3.2
Discrete Case
Let Y ¼ g(X). The density function for the random variable Y can be represented
by:
hðyÞ ¼ PYðyÞ ¼ PXðfx : gðxÞ ¼ y; x 2 RðXÞgÞ ¼
X
x:gðxÞ¼y
f
g
fðxÞ:
That is, the probability of the outcome y is equal to the probability of the
equivalent event {x : g(x) ¼ y}, which is the inverse image of y. Then
E gðXÞ
ð
Þ ¼ EðYÞ ¼
X
y2RðYÞ
yhðyÞ ¼
X
y2RðYÞ
y
X
x:gðxÞ¼y
f
g
fðxÞ
¼
X
y2RðYÞ
X
x:gðxÞ¼y
f
g
gðxÞfðxÞ ¼
X
x2RðXÞ
gðxÞfðxÞ;
where the next to last expression is true, since g(x) ¼ y for all x∈{x : g(x) ¼ y},
and the last expression is true since P
y2RðYÞ
P
x:gðxÞ¼y
f
g gðxÞfðxÞ is equivalent to
162
Chapter 3
Expectations and Moments of Random Variables

summing over all x∈R(X) because the collection of all y∈R(Y) (the outer sum) is
the set R(Y) ¼ {y : y ¼ g(x), x∈R(X)}.
n
Continuous Case
To prove the theorem for the continuous case, we ﬁrst need to establish the
following lemma.
Lemma 3.1
For any continuous random variable Y, the expectation of Y, if it exists, can be
written as
EðYÞ ¼
Z 1
0
Pðy>zÞdz 
Z 1
0
Pðy   zÞdz:
Proof of Lemma
Let h(y) be the density function of Y. Then P(y > z) ¼
R 1
z hðyÞdy, so that
Z 1
0
Pðy>zÞdz ¼
Z 1
0
Z 1
z
hðyÞdydz ¼
Z 1
0
Z y
0
dz


hðyÞdy ¼
Z 1
0
yhðyÞdy;
where the second equality was simply the result of changing the order of
integration (note that the inner range of integration is a function of the outer
range of integration, and the same set of (y,z) points are being integrated over).
Similarly, P(y  z) ¼
R z
1 h(y)dy, so that
Z 1
0
Pðy   zÞdz ¼
Z 1
0
Z z
1
hðyÞdydz ¼
Z 0
1
Z y
0
dz


hðyÞdy ¼ 
Z 0
1
yhðyÞdy:
Therefore,
Z 1
0
Pðy>zÞdz 
Z 1
0
Pðy   zÞdz ¼
Z 1
0
yhðyÞdy þ
Z 0
1
yhðyÞdy ¼ EðYÞ:
n
Note that the lemma (integrals and all) also applies to discrete random
variables.19
Using the lemma, we have
EðgðXÞÞ ¼
Z 1
0
P gðxÞ>z
ð
Þdz 
Z 1
0
P gðxÞ   z
ð
Þdz
¼
Z 1
0
Z
fx:gðxÞ>zg
fðxÞdxdz 
Z 1
0
Z
fx:gðxÞ  zg
fðxÞdxdz
¼
Z
fx:gðxÞ>0g
Z gðxÞ
0
dz
"
#
fðxÞdx 
Z
fx:gðxÞ  0g
Z gðxÞ
0
dz
"
#
fðxÞdx
¼
Z
fx:gðxÞ>0g
gðxÞfðxÞdx þ
Z
fx:gðxÞ  0g
gðxÞfðxÞdx:
¼
Z 1
1
gðxÞfðxÞdx:
n
19See P. Billingsley (1986) Probability and Measure, 2nd ed. New York: John Wiley, pp. 73–74 for the method of proof in the discrete
case.
3.12
Appendix: Proofs and Conditions for Positive Semideﬁniteness
163

3.12.2
Proof of Theorem 3.4 (Jensen’s Inequalities)
We prove the result for the convex case. The proof of the concave case is
analogous, with inequalities reversed.
If g is a convex function for x∈I, then there exists a line going through the
point E(X), say ‘ðxÞ ¼ a þ bx, such that gðxÞ  ‘ðxÞ ¼ a þ bx 8x 2 I and g(E(X)) ¼
a + bE(X) (see Figure 3.6). Now note that
E gðXÞ
ð
Þ ¼
X
x2RðXÞ
gðxÞfðxÞ 
X
x2RðXÞ
a þ bx
ð
ÞfðxÞ
¼ a þ bEðXÞ ¼ g EðXÞ
ð
Þ
(discrete)
E gðXÞ
ð
Þ ¼
Z 1
1
gðxÞfðxÞdx 
Z 1
1
a þ bx
ð
ÞfðxÞdx
¼ a þ bEðXÞ ¼ g EðXÞ
ð
Þ
(continuous)
since g(x)  a + bx 8x∈I,20 so that E(g(X))  g(E(X)).
If g is strictly convex, then there exists a line going through the point E(X),
say ‘ðxÞ ¼ a þ bx, such that g(x) > ‘ðxÞ ¼ a þ bx 8x∈I for which x 6¼ E(X), and
g(E(X)) ¼ a + bE(X). Then, assuming that no element in R(X) is assigned proba-
bility one, (i.e., X is not degenerate), the previous inequality results become
strict, implying E(g(X)) > g(E(X)) in either the discrete or continuous cases.
n
3.12.3
Necessary and Sufﬁcient Conditions for Positive Semideﬁniteness
To Prove that the symmetric matrix A ¼
a11
a12
a21
a22


is positive semideﬁnite
iff a11  0, a22  0, and a11 a22  a12 a21  0, note that the matrix A will be
positive semideﬁnite iff the characteristic roots of A are nonnegative (e.g., F.A.
y
x
y = g(x)
y = a+bx
E(X)
Figure 3.12
Convex function g.
20Recall the integral inequality that if h(x)  t(x) 8x∈(a,b), then
R b
a hðxÞdx 
R b
a tðxÞdx. Strict inequality holds if h(x) > t(x) 8x∈(a,b).
The result holds for a ¼ 1 and/or b ¼ 1.
164
Chapter 3
Expectations and Moments of Random Variables

Graybill (1983) Matrices with Applications in Statistics, Belmont, CA:
Wadsworth, p. 397). The characteristic roots of A are found by solving the
determinantal equation
a11  l
a12
a21
a22  l

 ¼ 0 for l, which can be represented
as (a11  l)(a22  l)  a12 a21 ¼ 0 or l2  (a11 + a22)l þ (a11 a22  a12 a21) ¼ 0.
Solutions to this equation can be found by employing the quadratic formula21
to obtain
l ¼
ða11 þ a22Þ þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ða11 þ a22 Þ2 4ða11 a22  a12 a21Þ
q
2
For l to be  0, it must be the case that the numerator term is  0. Note the
term under the square root sign must be nonnegative, since it can be rewritten as
(a11  a22)2 + 4 a12 a21, and because a12 ¼ a21 (by symmetry of A), two nonnega-
tive numbers are being added together. If (a11 + a22) > 0, then l  0 only if a11
a22  a12 a21  0, since otherwise the square root term would be larger than
(a11 + a22), and when subtracted from (a11 + a22), would result in a negative l.
Also, the term (a11 + a22) cannot be negative or else at least one of the solutions
for l would necessarily be negative. Furthermore, both a11 and a22 must be
nonnegative, for if one were negative, then there is no value for the other that
would result in both solutions for l being positive. Thus, necessity is proved.
Sufﬁciency follows immediately, since a11  0, a22  0, and a11 a22–a12 a21 
0 imply both solutions of l are  0.
n
Keywords, Phrases, and Symbols
n
i¼1 , Cartesian Product
Characteristic function
Chebyshev’s inequality
Conditional expectation E(Y|x∈B),
E(g(Y)|x∈B), E(Y|x ¼ b),
E(g(Y)|x ¼ b)
Correlation between two random
variables rXY
Correlation bound
Correlation matrix, Corr(X)
Covariance between two random
variables sXY or Cov(X,Y)
Covariance bound
Covariance matrix, Cov(X)
Cumulant generating function cX(t)
Cumulants, kr
Degenerate density function
Degenerate random variable
Expectation of a function of a
multivariate random variable,
E(g(X1,. . .,Xn))
Expectation of a function of a random
variable, E (g(X))
Expectation of a matrix of random
variables
Expectation of a random
variable, E(X)
Iterated expectation theorem
Jensen’s Inequality
Kurtosis, Excess Kurtosis
Leptokurtic
Marginal MGF, marginal cumulant
generating function
Markov’s inequality
Means and variances of linear
combinations of random
variables
Median, med(X)
Mesokurtic
MGF Uniqueness theorem
Mode, mode(X)
Moment generating function, MGF
MX(t)
Moments of a random variable
m, the mean of a random variable
mr, the rth moment about the mean or
rth central moment
mr,s, the (r,s)th joint moment about
the mean
m0
r;s, the (r,s)th joint moment about
the origin
21Recall that the solutions to the quadratic equation ax2 + bx + c ¼ 0 are given by x ¼ b 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
b2  4ac
p
2a
.
3.12
Appendix: Proofs and Conditions for Positive Semideﬁniteness
165

m0
r, the rth moment about the origin
Quantile of X
Platykurtic
Regression curve of Y on X
Regression function of Y on X
Skewed density function
Skewed to the left
Skewed to the right
Standard deviation of a random
variable s, or std(X)
Symmetric density function
Uncorrelated
Unimodal
Variance of a random variable s2, or
var (X)
Problems
1. A small domestic manufacturer of television sets
places a three-year warranty on its picture tubes. During
the warranty period, the manufacturer will replace the
television set with a new one if the picture tube fails.
The time in years until picture tube failure can be
represented as the outcome of a random variable X with
probability density function
fðxÞ ¼ :005e:005xI 0;1
ð
ÞðxÞ:
The times that picture tubes operate until failure can be
viewed as independent random variables. The company
sells 100 television sets in a given period.
(a) What is the expected number of television sets that
will be replaced due to picture tube failure?
(b) What is the expected operating life of a picture tube?
2. A small rural bank has two branches located in
neighboring
towns
in
eastern
Washington.
The
numbers of certiﬁcates of deposit that are sold at the
branch in Tekoa and the branch in Oakesdale in any
given week can be viewed as the outcome of the bivariate
random variable (X,Y) having joint probability density
function
f x; y
ð
Þ ¼
x3y3
3; 025
ð
Þ2
"
#
I 0;1;2;...;10
ð
ÞðxÞI 0;1;2;...;10
ð
ÞðyÞ:
(a) Are the random variables independent?
(b) What is the expected number of certiﬁcate sales by
the Oakesdale Branch?
(c) What is the expected number of combined certiﬁcate
sales for both branches?
(d) What is the answer to b) given that Tekoa branch sells
four certiﬁcates?
Potentially helpful result:
X
n
x¼1
x3 ¼ n2 n þ 1
ð
Þ2
4
:
3. The weekly number of luxury and compact cars sold
by “Honest” Abe Smith at the Auto Mart, a local car
dealership, can be represented as the outcome of a bivari-
ate random variable (X,Y) with the nonzero values of its
joint probability density function given by
Y
0
1
2
3
4
X
0
.20
.15
.075
.05
.03
1
.10
.075
.04
.03
.02
2
.05
.03
.02
.01
.01
3
.04
.03
.02
.01
.01
Al receives a base salary of $100/week from the dealer-
ship, and also receives a commission of $100 for every
compact car sold and $200 for every luxury car sold.
(a) What is the expected value of the weekly commission
that Al obtains from selling cars? What is the
expected value of his total pay received for selling
cars?
(b) What is the expected value of his commission from
selling compact cars? What is the expected value of
his commission from selling luxury cars?
(c) Given that Al sells four compact cars, what is the
expected value of his commission from selling luxury
cars?
(d) If 38 percent of Al’s total pay goes to federal and state
taxes, what is the expected value of his pay after
taxes?
(4) The yield, in bushels per acre, of a certain type of feed
grain in the midwest can be represented as the outcome of
the random variable Y deﬁned by
Y ¼ 3x:30
l
x:45
k eU
where xl and xk are the per acre units of labor and capital
utilized in production, and U is a random variable with
probability density function given by
fðuÞ ¼ 2e2uI 0;1
ð
ÞðuÞ:
166
Chapter 3
Expectations and Moments of Random Variables

The price received for the feed grain is $4/bushel, labor
price per unit is $10, and capital price per unit is $15.
(a) What is the expected yield per acre?
(b) What is the expected level of proﬁt per acre if labor
and capital are each applied at the rate of 10 units per
acre?
(c) Deﬁne the levels of input usage that maximize
expected proﬁt. What is the expected maximum
level of proﬁt?
(d) The acreage can be irrigated at a cost of $125 per acre,
in which case the yield per acre is deﬁned by
Y ¼ 5x:30
l
x:45
k eU:
If the producer wishes to maximize expected proﬁt,
should she irrigate?
5. The daily price/gallon and quantity sold (measured in
millions of gallons) of a lubricant sold on the wholesale
spot market of a major commodity exchange is the out-
come of a bivariate random variable (P,Q) having the joint
probability density function
fðp; qÞ ¼ 2 pepq I½:5;1 ðpÞ Ið0;1Þ ðqÞ:
(a) Deﬁne the regression curve of q on p.
(b) Graph the regression curve that you have deﬁned
in (a).
(c) What is the expected value of the quantity of lubri-
cant sold, given that price is equal to $.75 per gallon?
(d) What is the expected value of total dollar sales of
lubricant on a given day?
6. The short-run production function for a particular
agricultural crop is critically dependent on the level of
rainfall during the growing season, the relationship being
Y ¼ 30 þ 3X  .075X2, where y is yield per acre in bushels,
and x is inches of rainfall during the growing season.
(a) If the expected value of rainfall is 20 inches, can the
expected value of yield per acre be as high as 70
bushels per acre? Why or why not?
(b) Suppose the variance of rainfall is 40 square inches.
What is the expected value of yield per acre? How
does this compare to the bound placed on E(Y) by
Jensen’s inequality?
7. For each of the densities below, indicate whether the
mean and variance of the associated random variable
exist. In addition, ﬁnd the median and mode, and indicate
whether or not each density is symmetric.
(a) f(x) ¼ 3x2 I[0, 1](x)
(b) f(x) ¼ 2x3 I[1, 1](x)
(c) f(x) ¼ [p(1 + x2)]1 I(1, 1)(x)
(d) fðxÞ ¼
4
x

	
:2
ð
Þx :8
ð
Þ4xI 0;1;2;3;4
ð
ÞðxÞ
8. The daily price of a certain penny stock is a random
variable with an expected value of $2. Then the probabil-
ity is  .20 that the stock price will be greater than or
equation to $10. True or false?
9. The miles per gallon attained by purchasers of a line
of pickup trucks manufactured in Detroit are outcomes of
a random variable with a mean of 17 miles per gallon and a
standard deviation of .25 miles per gallon. How probable
is the event that a purchaser attains between 16 and 18
miles per gallon with this line of truck?
10. The daily quantity of water demanded by the
population of a large northeastern city in the summer
months
is
the
outcome
of
a
random
variable,
X,
measured in millions of gallons and having a MGF of
Mx(t) ¼ (1  .5 t)10 for t < 2.
(a) Find the mean and variance of the daily quantity of
water demanded.
(b) Is the density function of water quantity demanded
symmetric?
11. The annual return per dollar for two different invest-
ment instruments is the outcome of a bivariate random
variable (X1, X2) with joint moment-generating function
Mx(t) ¼ exp(u0t þ :5t0St), where
t ¼
t1
t2


; u ¼
:07
:11


and S ¼
:225  103
:3  103
:3  103
:625  103


:
(a) Find the mean annual return per dollar for each of the
projects.
(b) Find the covariance matrix of (X1, X2).
(c) Find the correlation matrix of (X1, X2). Do the
outcomes of X1 and X2 satisfy a linear relationship
x1 ¼ a1 + a2 x2?
(d) If an investor wishes to invest $1,000 in a way that
maximizes her expected dollar return on the invest-
ment, how should she distribute her investment
Problems
167

dollars between the two projects? What is the vari-
ance of dollar return on this investment portfolio?
(e) Suppose the investor wants to minimize the variance
of her dollar return. How should she distribute the
$1,000? What is the expected dollar return on this
investment portfolio?
(f) Suppose the investor’s utility function with respect
to her investment portfolio is U(M) ¼ 5Mb, where M
is the dollar return on her investment of $1,000. The
investor’s objective is to maximize the expected
value of her utility. If b ¼ 1, deﬁne the optimal
investment portfolio.
(g) Repeat (f), but let b ¼ 2.
(h) Interpret the investment behavior differences in (f)
and (g) in terms of investor attitude toward risk.
12. Stanley Statistics, an infamous statistician, wants
you to enter a friendly wager with him. For $1,000, he
will let you play the following game. He will continue to
toss a fair coin until the ﬁrst head appears. Letting x
represent the number of times the coin was tossed to get
the ﬁrst heads, Stanley will then pay you $2x.
(a) Deﬁne a probability space for the experiment of
observing how many times a coin must be tossed in
order to observe the ﬁrst heads.
(b) What is the expected payment that you will receive if
you play the game?
(c) Do you want to play the game? Why or why not?
13. The city of Megalopolis operates three sewage treat-
ment plants in three different locations throughout the
city. The daily proportion of operating capacity exhibited
by the three plants can be represented as the outcome of a
trivariate random variable with the following probability
density function:
fðx1; x2; x3Þ ¼ 1
3 ðx1 þ2 x2 þ3 x3Þ P
3
i¼1 Ið0;1Þ ðxiÞ;
where xi is the proportion of operating capacity exhibited
by plant i, i ¼ 1, 2, 3.
(a) What
are
the
expected
values
of
the
capacity
proportions for the three plants, i.e., what is E
X1
X2
X3
2
4
3
5?
(b) What is the expected value of the average proportion
of operating capacity across all three plants, i.e., what
is E
1
3
P3
i¼1 Xi


?
(c) Given that plant 3 operates at 90 percent of capacity,
what are the expected values of the proportions of
capacity for plants 1 and 2?
(d) If the daily capacities of plants 1 and 2 are 100,000 gal
of sewage each, and if the capacity of plant three is
250,000 gal, then what is the expected daily number
of
gallons
of
sewage
treated
by
the
city
of
Megalopolis?
14. The average price and total quantity sold of an econ-
omy brand of ballpoint pen in a large western retail mar-
ket during a given sales period is represented by the
outcome of a bivariate random variable having a probabil-
ity density function
fðp; sÞ ¼ 10 peps I½:10; :20 ðpÞ Ið0;1Þ ðsÞ
where p is the average price, in dollars, of a single pen and
s is total quantity sold, measured in 10,000-pen units.
(a) Deﬁne the regression curve of S on P.
(b) What is the expected quantity of pens sold, given that
price is equal to $0.12? (You may use the regression
curve if you wish.)
(c) What is the expected value of total revenue from the
sale of ball point pens during the given sales period,
i.e., what is E(PS)?
15. A game of chance is considered to be “equitable” or
“fair” if a player’s expected payoff is equal to zero. Exam-
ine the following games:
(a) The player rolls a pair of fair dice. Let Z represent the
amount of money that the player lets on the game
outcome. If the player rolls a 7 or 11, the player payoff
is 2Z (i.e., he gets to keep his bet of $Z, plus he
receives an additional $2Z). If the player does not
roll a 7 or 11, he loses the $Z that he bet on the
game. Is the game fair?
(b) The player spins a spinner contained within a disk,
, that is segmented into ﬁve pieces as
A
B
C
D
E
168
Chapter 3
Expectations and Moments of Random Variables

where P(A) ¼ 1/3, P(B) ¼ 1/6, P(C) ¼ 1/6, P(D) ¼ 1/12,
P(E) ¼ 1/4.
Each spin costs $1. The payoffs corresponding to
when the spinner lands in one of the ﬁve
segments are given by:
Segment
Payoff
A
$.60
B
$1.20
C
$1.20
D
$2.40
E
$.80
Is the game fair?
(c) A fair coin will be tossed repeatedly until heads
occurs. If the heads occurs on the jth toss of the
coin, the player will receive $2j. How much should
the player be charged to play the game if the game is
to be fair? (Note: This is a trick question and
represents the famous “St. Petersburg paradox” in
the statistical literature.)
16. The manager of a bakery is considering how many
chocolate cakes to bake on any given day. The manager
knows that the number of chocolate cakes that will be
demanded by customers on any given day is a random
variable whose probability density is given by
fðxÞ ¼ x þ 1
15
If0;1;2;3g ðxÞ þ 7  x
15
If4;5g ðxÞ:
The bakery makes a proﬁt of $1.50 on each cake that is
sold. If a cake is not sold on a given day, the cake is thrown
away (because of lack of freshness), and the bakery loses
$1. If the manager wants to maximize expected daily
proﬁt from the sale of chocolate cakes, how many cakes
should be baked?
17. The daily price and quantity sold of wheat in a North-
western market during the ﬁrst month of the marketing
year is the outcome of a bivariate random variable (P,Q)
having the probability density function
f p; q
ð
Þ ¼ :5pepqI½3;5ðpÞIð0;1ÞðqÞ
where p is measured in $/bushel, and q is measured in
units of 100,000 bushels.
(a) Deﬁne the conditional expectation of quantity sold as
a function of price, i.e., deﬁne E(Q | p) (the regression
curve of Q on P).
(b) Graph the regression curve you derived in (a). Calcu-
late the values of E(Q | p ¼ 3.50) and E(Q | p ¼ 4.50).
18. In each case below, calculate the expected value of
the random variable Y:
(a) E Yjx
ð
Þ ¼ 2x2 þ 3; fXðxÞ ¼ exI 0;1
ð
ÞðxÞ:
(b) E Yjx
ð
Þ ¼ 3x1x2; E X1
ð
Þ ¼ 5; E X2
ð
Þ ¼ 7; X1 and
X2 are
independent.
19. The total daily dollar sales in the ACME supermarket
is represented by the outcome of the random variable S
having a mean of 20, where s is measured in thousands of
dollars.
(a) The store manager tells you the probability that sales
will exceed $30,000 on any given day is .75. Do you
believe her?
(b) You are now given the information that the variance
of the random variable S is equal to 1.96. How proba-
ble is the sales event that s ∈(10,30)?
20. The ﬁrst three moments about the origin for the
random variable Y are given as follows: m0
1 ¼ :5; m0
2 ¼ :5; m0
3
¼ :75.
(a) Deﬁne the ﬁrst three moments about the mean for Y.
(b) Is the density of Y skewed? Why or why not?
21. The random variable Y has the PDF f(y) ¼ y2 I[1, 1) (y).
(a) Find the mean of Y.
(b) Can you ﬁnd the ﬁrst 100 moments about the origin
(i.e., m0
1; m0
2; . . . ; m0
100) for the random variable Y, why or
why not?
22. The moment-generating function of the random var-
iable Y is given by MYðtÞ ¼ 1  :25t
ð
Þ3 for t < 4.
(a) Find the mean and variance of the random variable Y.
(b) Is the PDF of Y skewed? Why or why not?
(c) It is known that the moment generating function
of the PDF fðxÞ ¼
1
baG a
ð Þ xa1ex=bI 0;1
ð
ÞðxÞ is given by
MxðtÞ ¼ 1  bt
ð
Þa for t < b1. The G(a) in the preced-
ing expression for the pdf is known as the gamma
function, which for integer values of a is such that
G(a) ¼ (a  1)!. Deﬁne the exact functional form of
the probability density function for Y, if you can.
Problems
169

23. A gas station sells regular and premium fuel. The two
storage tanks holding the two types of gasoline are reﬁlled
every week. The proportions of the available supplies of
regular and premium gasoline that are sold during a given
week in the summer is an outcome of a bivariate random
variable having the joint density function
f x; y
ð
Þ ¼ 2
5 3x þ 2y
ð
ÞI 0;1
½
ðxÞI 0;1
½
ðyÞ; where x ¼ proportion
of regular fuel sold and y ¼ proportion of premium fuel
sold.
(a) Find the marginal density function of X. What is the
probability that greater than 75 percent of the avail-
able supply of regular fuel is sold in a given week?
(b) Deﬁne the regression curve of Y on X, i.e., deﬁne E(Y |
x). What is the expected value of Y, given that
x ¼ .75? Are Y and X independent random variables?
(c) Regular gasoline sells for $1.25/gal and premium gas-
oline sells for $1.40/gal. Each storage tank holds
1,000 gal of gasoline. What is the expected revenue
generated by the sale of gasoline during a week in the
summer, given that x ¼ .75?
24. Scott Willard, a famous weatherman on national TV,
states that the temperature on a typical late fall day in the
upper midwest, measured in terms of both the Celsius
and Fahrenheit scales, can be represented as the outcome
of the bivariate random variable (C,F) such that
E C
F
"
#
¼
5
41


and CovðC; FÞ ¼
25
45
45
81


:
(a) What is the correlation between C and F?
(b) To what extent is there a linear relationship between
C and F? Deﬁne the appropriate linear relationship if
it exists.
(c) Is (C, F) a degenerate bivariate random variable? Is
this a realistic result? Why or why not?
25. A fruit processing ﬁrm is introducing a new fruit
drink, “Peach Passion,” into the domestic market. The
ﬁrm faces uncertain output prices in the initial marketing
period and intends to make a short-run decision by
choosing the level of production that maximize the
expected value of utility:
EðUðpÞÞ ¼ E p
ð Þ  avarðpÞ:
Proﬁt is deﬁned by p ¼ Pq  CðqÞ; p is the price received
for a unit of Peach Passion, U is utility, the cost function
is deﬁned by c(q) ¼ .5q2, a  0 is a risk aversion parame-
ter, and the probability density function of the uncertain
output price is given by f(p) ¼ .048(5p  p2) I[0,5] (p).
(a) If the ﬁrm were risk neutral, i.e., a ¼ 0, ﬁnd the level
of production that maximizes expected utility.
(b) Now consider the case where the ﬁrm is risk averse,
i.e., a > 0. Graph the relationship between the opti-
mal level of output and the level of risk aversion (i.e.,
the level of a). How large does a have to be for optimal
q ¼ 1?
(c) Assume that a ¼ 1. Suppose that the Dept. of Agri-
culture were to guarantee a price to the ﬁrm. What
guaranteed price would induce the ﬁrm to produce
the same level of output as in the case where price
was uncertain?
26. A Seattle newspaper intends to administer two
different
surveys
relating
to
two
different
anti-tax
initiatives on the ballot in November. The proportion of
surveys mailed that will actually be completed and
returned to the newspaper can be represented as the out-
come of a bivariate random variable (X,Y) having the
density function
f x; y
ð
Þ ¼ 2
3 x þ 2y
ð
ÞI 0;1
½
ðxÞI 0;1
½
ðyÞ;
where x is the proportion of surveys relating to initiative I
that are returned, and y refers to the proportion of surveys
relating to initiative II that are returned.
(a) Are X and Y independent random variables?
(b) What is the conditional distribution of x, given
y ¼ .50? What is the probability that less than 50
percent of the initiative I surveys are returned, given
that 50 percent of the initiative II surveys are
returned?
(c) Deﬁne the regression curve of X on Y. Graph the
regression curve. What is the expected proportion of
initiative I surveys returned, given that 50 percent of
the initiative II surveys are returned?
27. An automobile dealership sells two types of four-door
sedans, the “Land Yacht” and the “Mini-Rover.” The
number of Land Yachts and Mini-Rovers sold on any
given day varies, with the probabilities of the various
possible sales outcomes given by the following table:
Number of Mini-Rovers sold
Number of Land Yachts sold
0
1
2
3
0
.05
.05
.02
.02
1
.03
.10
.08
.03
2
.02
.15
.15
.04
3
.01
.10
.10
.05
170
Chapter 3
Expectations and Moments of Random Variables

Land Yachts sell for $22,000 each, and Mini-Rovers for
$7,500 each. These cars cost the dealership $20,000 and
$6,500, respectively, which must be paid to the car
manufacturer.
(a) Deﬁne a random variable that represents daily proﬁt
above dealer car cost, i.e., total dollar sales—total car
cost. (Let x ¼ number of Land Yachts sold and y ¼
number of Mini-Rovers sold). What is the expected
value of daily proﬁt above dealer car cost?
(b) The daily cost (other than the cost of cars) of running
the dealership is equal to $4,000. What is the proba-
bility that total proﬁt on a given day will be positive?
(c) What is the expected number of Mini-Rovers sold on
a day when no Land Yachts will be sold? What is this
expected number on a day when two Land Yachts are
sold? Are X and Y independent random variables?
Why or why not?
28. The season average price per pound, p, and total sea-
son quantity sold, q, of sweet cherries in a regional mar-
ket can be represented as the outcome of a bivariate
random variable (P,Q) with the joint probability density
function f(p,q) ¼ .5qeq(.5+p) I(0,1)(q) I(0,1) (p)
where p is measured in dollars, and q is measured in
millions of pounds.
(a) Find the marginal density of Q. What is the expected
value of quantity sold?
(b) Deﬁne the regression curve of P on Q. What is the
expected value of P, given that q ¼ ½?
(c) If the government receives 10 percent of the gross
sales of sweet cherries every season, what is the
expected value of the revenue collected by the gov-
ernment from the sale of sweet cherries given that
q ¼ 1/2?
Hint: R xeaxdx ¼ eax=a2


ax  1
ð
Þ:
29. The yield/acre of wheat on a given parcel of land can
be represented as the outcome of a random variable Y
deﬁned by Y ¼ 10x1/3ee for x∈[8,100], where
Y ¼ wheat output in bushes/acre
x ¼ pounds/acre of fertilizer applied
e ¼ is a random variable having the probability density
function f eð Þ ¼ 3e3eI 0;1
ð
Þ eð Þ.
(a) If fertilizer is applied at the rate of 27 lb/acre, what is
the probability that greater than 50 bushels/acre of
wheat will be produced?
(b) You sign a forward contract to sell your wheat for
$3.00/bushel at harvest time. Fertilizer costs $0.20/
lb. If you apply fertilizer at a rate of 27 lb/acre, what is
your expected return above fertilizer cost, per acre?
(c) What is the variance of Y if fertilizer is applied at the
rate of 27 lb/acre? Does the variance of Y change if a
different level of fertilizer is applied? Why or why not?
30. Let X have the moment generating function MX(t).
Show that
(a) M(X+a) (t) ¼ eat MX(t).
(b) MbX(t) ¼ MX(bt).
(c) M(X+a)/b(t) ¼ e(a/b)t MX(t/b).
31. The AJAX Disk Co. manufactures compact disks
(CDs) for the music industry. As part of its quality-control
program, the diameter of each disk is measured using an
electronic measuring device. Letting X1 represent the
actual diameter of the disk and X2 represent the measured
diameter of the disk,
E X1
X2


¼
4:6775
4:6775


and CovðXÞ ¼ :00011 :00010
:00010 :00010


;
where x1 and x2 are measured in inches.
(a) What is the correlation between the actual diameter
and the measured diameter of the CDs?
(b) Assume that P(x1∈[4.655, 4.700]) ¼ 1. Use a graph to
elucidate the degree of linear association between X1
and X2 that is represented by the correlation value
you calculated in (a).
(c) Given the characteristics of (X1, X2) indicated above,
the manager of the quality control-department states
that the difference between measured and actual disk
diameters is no more than .01 inches with probability
 .90. Do you agree? Why or why not?
32. An investor wishes to invest $1,000 and is examining
two investment prospects. The net dollar return per
dollar invested in the two projects can be represented
as the outcome of a bivariate random variable (X1, X2)
where
E X1
X2


¼
:15
:07


and CovðXÞ ¼
:04
:001
:001
:0001


:
Problems
171

(a) If the investor invests $500 in each project, what is
his/her expected net dollar return? What is the vari-
ance associated with the net dollar return?
(b) Suppose the investor wishes to invest the $1,000
so
that
his/her
expected
utility
is
maximized,
where E(U(R)) ¼ E(R)  .01var(R), R ¼ a1X1 + a2X2
represents
the
total
return
on
the
investment,
a1 + a2 ¼ 1,000, and ai  0 for i ¼ 1, 2. How much
money should he/she invest in each of the projects?
33. The length of time in minutes for an individual to be
served at a local restaurant is the outcome of a random
variable, T, having a mean of 6 and a variance of 1.5. How
probable is the event that an individual will be served
within 3 to 9 min?
34.
(a) Find the moment-generating function of a random
variable X having the density function
fðxÞ ¼ 1
8
3
x

	
I 0;1;2;3
ð
ÞðxÞ:
(Hint: Use of the binomial theorem may be helpful in
ﬁnding a compact representation of this function.)
Use the MGF to calculate the ﬁrst two moments
of X about the origin. Calculate the variance of X.
(b) Repeat (a) using the density function
fðxÞ ¼ 1
10 ex=10I 0;1
ð
ÞðxÞ:
35. The Rockbed Insurance Company sells 1-year term
life insurance policies for $10,000 of annual coverage,
where a 1-year premium is charged to put the policy in
force, and then if the insured person does not live through
the year, his or her estate is paid $10,000. The mortality
tables in the large market in which the company sells
insurance indicates that a 25 year old person will have a
.998 probability of living another year, and a 35 year old
person has a .995 probability of living another year. Sell-
ing and administrative costs are $25 per policy.
(a) What is the expected proﬁt on a $10,000 life insur-
ance policy sold to a randomly chosen 25 year-old for
a premium of $50?
(b) What is the premium that would need to be set on the
policy in (a) so that the policy is expected to break
even (i.e., return zero proﬁt)?
(c) What is the increment in premiums needed, above the
premium charged a 25 year old, to insure a randomly
chosen 35 year old with a $10,000 life insurance policy
such that the policy is expected to break even?
(d) If Rockbed had $1,000,000 of insurance coverage to
sell and was contemplating distributing it between
only the 25 and the 35 year old populations of poten-
tial customers at a ﬁxed premium of $60 per $10,000
policy, how should it attempt to distribute its sales so
as to maximize expected proﬁt?
36. The daily wholesale price and quantity sold of etha-
nol in a Midwestern regional market during the summer
months is represented by the outcome of a bivariate ran-
dom variable
P; Q
ð
Þ having the following probability
model R P; Q
ð
Þ; f p; q
ð
Þ
f
g:
f p; q
ð
Þ ¼
:5pepq for
p; q
ð
Þ 2 R P; Q
f
g ¼ 2; 4
½
  0; 1
½
Þ
0
elsewhere

where price is measured in dollars and quantity is
measured
in
100,000
gal
units
(e.g.,
q ¼ 2 means
200,000 gal were sold).
(a) Derive the conditional expectation of quantity sold,
as a function of the price of ethanol (i.e., derive the
regression curve of Q on P).
(b) Graph the expected conditional quantity sold as a
function of price. Does the relationship make eco-
nomic sense?
(c) What is the expected quantity sold if price is $2? If
price is $4?
(d) Derive the expected value of total dollar sales, PQ, of
ethanol.
37. Deﬁne the mean, median, mode, and .10 and .90
quantiles of the random variable X deﬁned in the proba-
bility models RðXÞ; fðxÞ
f
g below:
(a) fðxÞ ¼ x2 þ 4
50
I RðXÞ
f
gðxÞ; RðXÞ ¼ 0; 1; 2; 3; 4
f
g
(b) fðxÞ ¼ :5ex=2I RðXÞ
f
gðxÞ; RðXÞ ¼ 0; 1
½
Þ
(c) fðxÞ ¼ 3x2I RðXÞ
f
gðxÞ; RðXÞ ¼ 0; 1
½

(d) fðxÞ ¼ :05 :95
ð
Þx1I RðXÞ
f
gðxÞ; RðXÞ ¼ 1; 2; 3; :::
f
g
38. The regression curve of daily quantity demanded of
tablet computers in a Midwestern market, measured in
172
Chapter 3
Expectations and Moments of Random Variables

thousands of units and expressed as a function of price, all
else held equal, is given by
E Qjp
ð
Þ ¼ 10p:5
The marginal probability density function of price is
given by
fðpÞ ¼ 3
8 p2I 0;2
½
ðpÞ
(a) Derive the value of quantity demanded, EðQÞ.
(b) Derive the expected value of dollar sales, E PQ
ð
Þ.
39. The yield per acre of a certain dwarf watermelon is
highly dependent on the amount of rainfall that occurs
during the growing season. Following standard cultiva-
tion practices, the relationship between tons per acre, Y,
and inches of rainfall, R, is given by
Y ¼ 25 þ 2R  :05R2 for R 2 0; 40
½

(a) If expected rainfall is 10, can expected yield be equal
to 60?
(b) If expected rainfall is 15, deﬁne an upper bound for
the value of expected yield.
(c) If expected rainfall is 15, and the variance of rainfall is
5, what is the expected value of yield?
40. For each probability density function below, deter-
mine the mean and variance, if they exist, and deﬁne the
median and mode.
(a) fðxÞ ¼ p 1 þ x2



1I 1;1
ð
ÞðxÞ
(b) fðxÞ ¼ 4x3I 0;1
½
ðxÞ
(c) fðxÞ ¼ :3 :7
ð
Þx1I 1;2;3;:::
f
gðxÞ
(d) fðxÞ ¼ x
55 I 1;2;:::;10
f
gðxÞ
41. The daily dollar sales of a large retail “Big Box” store,
measured in 1,000 dollar units, is a random variable, D,
that has an expectation of 20.
(a) Provide an upper bound to the probability that dollar
sales exceed 40,000 dollars on a given day.
(b) If the variance of D is 4, deﬁne an interval in which
dollar sales will occur on any given day with
probability  .95.
42. Given the following three moments about the origin,
derive the ﬁrst three moments about the mean, and
determine whether the random variable has a probability
density function that is skewed, if you can.
(a) EðXÞ ¼ :2; E X2


¼ :2; E X3


¼ :2
(b) EðXÞ ¼ 1; E X2


¼ 2; E X3


¼ 5
(c) EðXÞ ¼ 2; E X2


¼ 8; E X3


¼ 48
43. The bivariate random variable (P,Q) represents the
weekly price, in dollars, and the quantity, in number of
kegs, of an India Pale Ale beer, HopMeister, sold in the
Paciﬁc Northwest market. The moment generating func-
tion associated with this bivariate random variable is
given by M P;Q
ð
ÞðtÞ ¼ exp m0t þ :5t0t
ð
Þ, where
m ¼
125
500


and  ¼
100
100
100
400


:
(a) What is the expected weekly price of HopMeister?
(b) What is the expected weekly quantity sold of
HopMeister?
(c) What
is
the
expected
weekly
dollar
sales
of
HopMeister?
44. Derive the moment generating function of each of
the random variables below, and use it to deﬁne the
mean and variance of the random variable.
(a) fðxÞ ¼ :2e:2xI 0;1
ð
ÞðxÞ
(b) fðxÞ ¼ 2xI 0;1
ð
ÞðxÞ
(c) fðxÞ ¼ :3x:71xI 0;1
f
gðxÞ
45. A small manufacturing ﬁrm produces and sells a
product in a market where output prices are uncertain.
The owner of the ﬁrm wishes to make a short run produc-
tion decision that will maximize her expected utility,
deﬁned by
E U p
ð Þ
ð
Þ ¼ E p
ð Þ  a var p
ð Þ
½

where U is utility, p ¼ Pq  cðqÞ is proﬁt, q is measured in
1,000’s of units, Pis the uncertain price received for a unit
of the product, the cost function is deﬁned by cðqÞ ¼ :5q
þ:1q2; a  0, is a “risk aversion” parameter, and the prob-
ability density function of the uncertain output price is
given by
fðpÞ ¼ :5e:5pI 0;1
½
ÞðpÞ
(a) If the owner were risk neutral, i.e., a ¼ 0, ﬁnd the
level of production that maximizes expected utility.
Problems
173

(b) Now consider the case where the owner is risk
averse, i.e., a > 0. Graph the relationship between
the optimal level of output and the level of risk aver-
sion (i.e., the level of a). How large does a have to be
for optimal q ¼ 5?
(c) Assume that a ¼ :1 . Suppose that the government
were to guarantee a price to the owner. What
guaranteed level of price would induce the owner to
produce the same level of output as in the case where
price was uncertain?
46. The weekly number of MACs and PCs sold by a
salesperson at the local computer store can be represented
as the outcome of a bivariate random variable (X,Y) with
the nonzero values of its joint probability density func-
tion given by the following table of probabilities:
Y
0
1
2
3
4
X
0
.15
.12
.075
.05
.03
1
.12
.075
.05
.05
.02
2
.05
.04
.03
.03
.02
3
.01
.03
.02
.02
.01
(X is the number of MACs and Y is the number of PCs).
The salesperson receives a salary of $200/week, and also
receives a commission of $50 for every MAC sold and
$100 for every PC sold.
(a) What is the expected value of the weekly commission
that the salesperson obtains from selling computers?
What is the expected value of her total pay received
from selling computers?
(b) What is the expected value of her commission from
selling MACs? What is the expected value of her
commission from selling PCs?
(c) Given that the salesperson sells three PCs, what is the
expected value of her commission from selling MACs?
(d) If 40 percent of the salesperson’s total pay is deducted
for federal and state taxes, what is the expected value
of her pay after taxes?
47. An investor has $10,000 to invest between two
investment projects. The rate of return per dollar invested
in the two projects can be represented as the outcome of a
bivariate random variable X1; X2
ð
Þwhere
E X1
X2


¼
:20
:05


and CovðXÞ ¼
:04
:002
:002
:0001


(a) If the investor invests $5,000 in each project, what is
the expected dollar return? What is the variance
associated with the dollar return?
(b) If the investor wishes to maximize expected dollar
return, how should the money be invested?
(c) If the invest wishes to minimize variance of dollar
returns, how should the money be invested?
(d) Suppose the investor wishes to invest the $10,000
so that his/her expected utility is maximized, where
E UðRÞ
ð
Þ ¼ EðRÞ  :01varðRÞ, where R ¼ a1X1 þ a2X2
represents the total return on the investment, a1 þ a2
¼ 10,000, and ai  0 for i ¼ 1, 2. How much money
should he/she invest in each of the two projects?
48. The mean vector and covariance matrix of the
trivariate random variable X is given by
EðXÞ ¼
2
4
2
2
4
3
5 and CovðXÞ ¼
10
2
1
2
5
0
1
0
1
2
4
3
5:
The random variable Y is deﬁned by Y ¼ c0X, where
c0 ¼ [5 1 3], and the bivariate random vector Z is deﬁned
by Z ¼ AX, where A ¼
1
1
1
2
3
4


.
(a) Deﬁne as many of the values E XiXj


; for i and j 2
1; 2; 3
f
g, as you can.
(b) Deﬁne the correlation matrix for X.
(c) Deﬁne the mean and variance of Y.
(d) Deﬁne the mean vector, covariance matrix, and cor-
relation matrix of Z.
49. The bivariate random variable (Y,X) has the following
mean vector and covariance matrix:
E X
Y


¼
10
5


and Cov X; Y
ð
Þ ¼
5
2
2
2


(a) Derive the values of a and b in
^Y ¼ a þ bX that
minimize the expected squared distance between Y
and ^Y, i.e., that produce the best linear predictor of Y
outcomes in terms of X outcomes.
(b) What proportion of the variance in Y is explained by
the best linear predictor that you derived above?
174
Chapter 3
Expectations and Moments of Random Variables

4
n
Parametric Families
of Density Functions
n
n
n
4.1
Parametric Families of Discrete Density Functions
4.2
Parametric Families of Continuous Density Functions
4.3
The Normal Family of Densities
4.4
The Exponential Class of Densities
A collection of speciﬁc probability density functional
forms that have found substantial use in statistical applications are examined
in this chapter. The selection includes a number of the more commonly used
densities, but the collection is by no means an exhaustive account of the vast
array of probability densities that are available and that have been applied in the
literature.1
Our density function deﬁnitions will actually identify parametric families of
density functions. That is, the algebraic expressions deﬁning the density
functions will contain one or more unknowns, called parameters, which can
be assigned values chosen from a set of admissible values called the parameter
space. A speciﬁc member of a family of probability densities will be identiﬁed by
choosing a speciﬁc value of the parameters contained in the parameter space.
The general notation f(x;y) will be used to distinguish elements, x, in the domain
1Johnson, Kotz, Balakrishnan and Kemp provide a set of volumes that provide an extensive survey of a large array of density functions
that have been used in statistical applications. These include:
Continuous Multivariate Distributions, Volume 1, Models and Applications, 2nd Edition, by Samuel Kotz, N. Balakrishnan and
Normal L. Johnson, 2000;
Continuous Univariate Distributions, Volume 1, 2nd Edition by Samuel Kotz, N. Balakrishnan and Normal L. Johnson, 1994;
Continuous Univariate Distributions, Volume 2, 2nd Edition by Samuel Kotz, N. Balakrishnan and Normal L. Johnson, 1995;
Discrete Multivariate Distributions by Samuel Kotz, N. Balakrishnan and Normal L. Johnson, 1997;
Univariate Discrete Distributions, 3rd Edition by Normal L. Johnson, Adrienne Kemp, and Samuel Kotz, 2008; all published by
John Wiley and Sons, New York.

of the density function from elements,y, in the parameter space of the parametric
family of functions.
For each parametric family of densities examined, we will present a particu-
lar parameterization of the family that identiﬁes the parameters used in the
algebraic representation of the density functions as well as the collection of
admissible values for the parameters (the latter collection being the aforemen-
tioned parameter space). We will use various English and Greek letters to
represent parameters. The parameter space will be generically represented by
the Greek letter capital omega, O. Be aware that, in general, parameterizations
are not unique. Generally, the collection of densities in a parametric family can
be reparameterized, meaning that an alternative set of parameters and an
associated parameter space can be deﬁned that equivalently identiﬁes each and
every density in a family of density functions. Possibilities for reparameter-
izations will not concern us currently, although we will revisit this issue later
when we examine methods of statistical inference. And of course, the particular
English and Greek letters we use to identify domain elements and elements of
the parameter space are arbitrary, and can be changed.
Each parametric family has its own distinguishing characteristics that make
the PDFs appropriate candidates for specifying the probability space of some
experiments and inappropriate for others. Some of the main characteristics
include whether the PDFs are discrete or continuous, whether the use of the
PDFs are restricted to nonnegative-valued and/or integer-valued random
variables, whether the densities in the family are symmetric or skewed, and
the degree of ﬂexibility with which the density can distribute probability over
events in the range of a random variable. Furthermore, the functional forms of
some parametric families of densities follow deductively from the charact-
eristics of certain types of experiments. We will point out major characteristics
and application contexts for each of the parametric families presented. We will
also introduce procedures for assessing the adequacy of the choice of a particular
family for a given application later in Chapter 10.
4.1
Parametric Families of Discrete Density Functions
4.1.1
Family Name: Uniform
Parameterization: N∈O ¼ {N: N is a positive integer}
Density Deﬁnition: f x; N
ð
Þ ¼ 1
NI 1;2;...;N
f
gðxÞ
Moments: m ¼ N þ 1
ð
Þ=2; s2 ¼ N2  1


=12; m3 ¼ 0,
m4
s4  3


¼ 
6 n2þ1
ð
Þ
5 n21
ð
Þ


MGF: MXðtÞ ¼ PN
j¼1 ejt=N
Background and Application: The discrete uniform density function assigns
equal probability to each of N possible outcomes of an experiment. The density
is used to construct a probability space for any experiment having N possible
outcomes that are all equally likely. The outcomes are coded 1, 2,. . ., N, and each
outcome is assigned probability f(x;N) ¼ 1/N.
176
Chapter 4
Parametric Families of Density Functions

Example 4.1
Probabilities of
Choosing Correctly
at Random
The ofﬁce manager has a box of six printer cables with different pin
conﬁgurations to accommodate computer hook-up of the various types of
printers used by the company. A new employee needs a printer cable to hook
up her printer. She randomly chooses printer cables from the box, one at a time
without replacement, in an attempt to hook up her printer. Letting the outcome
of X denote the number of cables tried before the correct one is found, what is
the probability density of X?
Answer: Let Ai be the event that the correct cable is chosen on the ith try. Then
fð1Þ ¼ PðA1Þ ¼ 1
6 ðclassical probabilityÞ
fð2Þ ¼ P A2jA1


P A1


¼
1
5
  5
6
 
¼ 1
6 multiplication rule
ð
Þ
fð3Þ ¼ P A3jA1 \ A2


P A2jA1


P A1


extended multiplication rule
ð
Þ
¼
1
4
  4
5
  5
6
 
¼ 1
6 ;
..
.
fð6Þ ¼ ð1Þ 1
2
  2
3
  3
4
  4
5
  5
6
 
¼ 1
6 :
Thus, X has the uniform distribution with N ¼ 6 (see Figure 4.1).
□
4.1.2
Family Name: Bernoulli
Parameterization: p∈O ¼ {p: 0  p  1}
Density Deﬁnition: f(x; p) ¼ px (1  p)1  x I{0,1} (x)
Moments: m ¼ p, s2 ¼ p(1  p), m3 ¼ 2p3  3p2 + p,
m4
s4  3


¼
16p 1p
ð
Þ
p 1p
ð
Þ


MGF: MX(t) ¼ pet + (1  p)
Background and Application: The Bernoulli density, named after Swiss mathe-
matician Jacques Bernoulli (1654–1705), can be used to construct a probability
space for an experiment that has two possible outcomes (e.g., cure versus no
cure, defective versus nondefective, success versus failure) that may or may not
1
2
3
4
5
6
x
1/6
f(x)
Figure 4.1
Uniform density,
N ¼ 6.
4.1
Parametric Families of Discrete Density Functions
177

be equally likely to occur. The two outcomes of the experiment are coded 0 and
1, where the event x ¼ 0 is assigned probability f(0;p) ¼ 1  p and the event
x ¼ 1 is assigned probability f(1; p) ¼ p.
Example 4.2
Probability of a
Dichotomous Outcome
A shipment of DVD players to a local electronics retailer contains three defec-
tive and seven nondefective players. The players are placed on the store’s
shelves, and a customer randomly chooses a player to purchase. Coding the
choice of a defective player as x ¼ 1 and the choice of a nondefective player as
x ¼ 0, the Bernoulli density can be used to construct the probability space of the
experiment by letting p ¼ .3. (see Figure 4.2)
□
4.1.3
Family Name: Binomial
Parameterization: (n,p) ∈O ¼ {(n,p): n is a positive integer, 0  p  1}
Density Deﬁnition:
fðx; n; pÞ ¼
n!
x!ðnxÞ!
px 1  p
ð
Þnxfor x ¼ 0; 1; 2; . . . ; n
0
otherwise

Moments: m ¼ np, s2 ¼ np(1  p), m3 ¼ np(1  p)(1  2p),
m4
s4  3


¼
16p 1p
ð
Þ
np 1p
ð
Þ


MGF: MX(t) ¼ (1  p + pet)n
Background and Application: The binomial density function is used to construct
a probability space for an experiment that consists of n independent repetitions
(also called Bernoulli trials) of a given experiment of the Bernoulli type (i.e., the
experiment has two possible outcomes), with the observation of interest being
how many of the n Bernoulli trials result in one of the two types of outcomes, say
type A (e.g., how many successes, defectives, or cures occur in n repetitions of the
Bernoulli-type experiment?). The value of x represents the total number of
outcomes of type A that occur in the n Bernoulli trials. The parameters n and p
refer respectively to the number of trials and the probability of observing the type
A outcome in the underlying Bernoulli-type experiment. It is assumed that the
repetitions are executed in such a way that the outcome observed on
any trial does not affect the probability of occurrence of outcomes on any other
0
1
x
.3
f(x)
.7
Figure 4.2
Bernoulli density, p ¼ .3.
178
Chapter 4
Parametric Families of Density Functions

trial (which
is
the
meaning
of the
phrase
independent
repetitions
or
independent trials).
The functional form of the density can be deduced directly from the
characteristics of the experiment described above. Let (X1,. . .,Xn) be a collection
of n independent random variables, where each Xi has a Bernoulli density with
the same value of p, as fðxi; pÞ ¼ pxi ð1  p Þ1xi If0;1g ðxiÞ. Let xi ¼ 1 indicate that
outcome A occurs on the ith Bernoulli trial. Then the random variable
X ¼ Pn
i¼1 Xi represents the number of Bernoulli trials that result in outcome
A. Since the Bernoulli random variables are independent, the probability of
obtaining a particular sequence of x outcomes of type A in a sequence of n trials
is px(1  p)n  x. The number of different sequences of n trials that result in x
outcomes of type A is given by
n
x


(which is the number of different ways of
placing x outcomes of type A into the n positions of the sequence). Since the
different sequences are mutually exclusive, it follows that the probability of
observing x outcomes of type A is given by the sum of the probabilities of the
n
x


different sequences that result in the outcome x, which is represented by
the binomial density function deﬁned above. The binomial density assigns the
probability f(x;n,p) to the outcome x ∈R(X) ¼ {0,1,2,. . .,n}.
Example 4.3
Probability of Number
of Credit Card
Transactions
Upon analyzing the cash register receipts of a large department store over an
extended period of time, it is found that 30 percent of the customers pay for their
purchases by credit card, 50 percent pay by cash, and 20 percent pay by check. Of
the next ﬁve customers that make purchases at the store, what is the probability
that three of them will pay by credit card?
Answer: Assume that how a customer pays for her purchases is the outcome of a
Bernoulli trial with xi ¼ 1 ) credit card, and xi ¼ 0 ) cash or check. Given
that the respective probabilities of these outcome are .30 and .70, and assuming
that customers’ payment methods are independent of one another, it follows
that X ¼ P5
i¼1 Xi represents the number of customers that pay by credit card, and
X has a binomial density function with n ¼ 5 and p ¼ .3. The graph of the
density is given in Figure 4.3, and the density values are displayed in Table 4.1:
Thus, P(x ¼ 3) ¼ .1323.
□
0
1
2
3
4
x
.1
f(x)
.3
.2
5
Figure 4.3
Binomial density, n ¼ 5,
p ¼ 0.3.
4.1
Parametric Families of Discrete Density Functions
179

4.1.4
Family Name: Multinomial
Parameterization: (n,p1, . . .,pm) ∈O ¼ {(n,p1,. . .,pm): n is a positive integer, 0 
pi  1,8i, Pm
i¼1 pi ¼ 1}
Density Deﬁnition:
fðx1; . . . ; xn; n; p1; . . . ; pmÞ ¼
n!
Qm
i¼1 xi!
P
m
i¼1 pxi
i for xi ¼ 1; . . . ; n; P
m
i¼1
xi ¼ n;
0
otherwise
8
<
:
Moments: For each random variable Xi
and random variable pair (Xi,Xj)
mðXiÞ ¼ npi; s2 Xi
ð
Þ¼npi 1  pi
ð
Þ; Cov Xi; Xj


¼npipj; m3 Xi
ð
Þ¼npi 1  pi
ð
Þ 1  2pi
ð
Þ;
m4 Xi
ð
Þ
s4 Xi
ð
Þ  3


¼
16pi 1pi
ð
Þ
npi 1pi
ð
Þ


MGF: MX(t) ¼ Pm
i¼1 pieti

n
Background and Application: The multinomial density function is an extension
of the binomial density function to the case where there is interest in more than
two different types of outcomes for each trial of the underlying repeated experi-
ment. In particular, the multinomial density function is used to construct a
probability space for an experiment that consists of n independent repetitions of
a given experiment characterized by m > 2 different types of outcomes. The
observation of interest is how many of each of the m different types of outcomes
of the experiment occur in n repetitions of the experiment. The value of xi
represents the total number of outcomes of type Ai that occur in the n
repetitions. The parameters n and pi refer, respectively, to the number of multi-
nomial trials conducted and the probability of observing the type Ai outcome in
one trial. It is assumed that the repetitions are conducted in such a way that the
outcome observed on any trial does not affect the probability of occurrence of
outcomes on any other trial. The motivation for the density function deﬁnition
is a direct extension of the arguments used in the binomial case upon
recognizing that the number of different sequences of n repetitions of the
experiment that result in xi-number of type Ai outcomes, i ¼ 1,. . .,m, equals
n!
x1!...xm!


. The details are left to the reader. The multinomial density assigns the
probability f(x1,. . .,xm;n,p1,. . .,pm) to the outcome (x1,. . .,xm) ∈R(X) ¼ {(x1,. . .,
xm): xi ∈(0,1,. . .,n) 8i, Pm
i¼1 xi ¼ n}.
Table 4.1
Binomial PDF for n ¼ 5, p ¼ .3
x
f(x)
0
.1681
1
.3602
2
.3087
3
.1323
4
.0284
5
.0024
180
Chapter 4
Parametric Families of Density Functions

It is useful to note that the marginal density of any of the Xi variables is
binomial with parameters n and pi . Furthermore, any subset of the random
variables (X1,. . .,Xm) has a marginal multinomial density.
Example 4.4
Probability of Customer
Payment Modes
Recall Example 4.3. Of the next ﬁve customers entering the store, what is the
probability that two will pay by credit card, two will pay by cash, and one will
pay by check?
Answer: For each of the ﬁve experiments of observing customers’ payment
methods, we have three different types of outcomes that are of interest. In the
speciﬁcation of the multinomial density, we let x1, x2, x3 refer, respectively, to
the number of payments made by credit card, cash, and check. The probabilities
of observing a payment by credit card, cash, or check in any given trial is p1 ¼ .3,
p2 ¼ .5, and p3 ¼ .2, respectively. Then the probability we seek is given by
fð2; 2; 1; :3; :5; :2Þ ¼
5!
2!2!1!
	

ð:3 Þ2 ð:5 Þ2 ð:2 Þ1 ¼:135:
□
4.1.5
Family Name: Negative Binomial, and Geometric Subfamily
Parameterization: (r,p)∈O ¼ {(r,p): r is a positive integer, 0 < p < 1} (the geomet-
ric density family is deﬁned by setting r ¼ 1)
Density Deﬁnition:
fðx; r; pÞ ¼
ðx  1Þ!
ðr  1Þ!ðx  rÞ!
pr 1  p
ð
Þxr
for
x ¼ r; r þ 1; r þ 2; . . . ;
0
otherwise
8
<
:
Moments:
m ¼ r/p,s2 ¼ r(1  p)/p2,m3 ¼ r((1  p) + (1  p)2)/p3,
m4
s4  3


¼
r 1p
ð
Þ p26pþ6
ð
Þ
p4


MGF: MX(t) ¼ ertpr 1  1  p
ð
Þet
ð
Þr for t<  ln 1  p
ð
Þ
Background and Application: The negative binomial density function (also
sometimes referred to as the Pascal distribution) can be used to construct a
probability space for an experiment that consists of independent repetitions of
a given experiment of the Bernoulli type, just like the case of the binomial
density, except that the observation of interest is now how many Bernoulli trials
are necessary to obtain r outcomes of a particular type, say type A (e.g., how
many Bernoulli trials are necessary to obtain r successes, defectives, or tails?). In
comparing the binomial and negative binomial densities, notice that the roles of
the number of Bernoulli trials and the number of successes are reversed with
respect to what is the random variable and what is the parameter. For the
negative binomial density, the value of x represents the number of Bernoulli
trials necessary to obtain r outcomes of type A.
In order to motivate the density function deﬁnition, let (X1,. . .,Xn) be a
collection of n independent random variables, where each Xi has a Bernoulli
4.1
Parametric Families of Discrete Density Functions
181

density, precisely the same as in our discussion of the binomial density. Let the
probability of obtaining an outcome of type A be p for each trial. Since the
Bernoulli random variables are independent, the probability of obtaining a
sequence of x trials that result in r outcomes of type A, with the last trial
being the rth such outcome, is pr(1  p)x  r. The number of different sequences
of x trials that result in r outcomes of type A, with the rth outcome being of type
A, is given by x  1
ð
Þ!= r  1
ð
Þ! x  r
ð
Þ!
ð
Þ (which is the number of different ways of
placing r  1 outcomes of type A in the ﬁrst x  1 positions of the sequence).
Since the different sequences are mutually exclusive, it follows that the proba-
bility of needing x Bernoulli trials to obtain r outcome of type A is given by the
sum of the probabilities of the
x1
ð
Þ!
r1
ð
Þ! xr
ð
Þ! different sequences that result in the
outcome x, this sum being represented by the negative binomial density func-
tion deﬁned above. The negative binomial density assigns the probability f(x;r,p)
to the outcome x∈R(X) ¼ {r,r + 1,r + 2,. . .}.
The geometric family of densities is a subset of the family of negative
binomial densities deﬁned by setting r ¼ 1. Thus, the geometric density is
appropriate for assigning probability to events relating to how many Bernoulli
trials are necessary to get the ﬁrst outcome of type A. The geometric density
function has a unique property in that it is the only discrete density for a
nonnegative integer-valued random variable for which P x>i þ jjx>i
½
 ¼ P x>j
½
8
i and j∈{0,1,2,. . .}. This conditional probability property is referred to as the
memoryless property, meaning that in any experiment characterized by the
geometric density, if the experiment has already resulted in i trials without a
type A outcome, the experiment has “no memory” of this fact, since the proba-
bility that more than j trials will be needed to obtain the ﬁrst type A outcome is
precisely the same as if the ﬁrst i trials had never occurred. The proof that the
geometric density has this property is left to the reader. The reader may wish to
consult V.K. Rohatgi, (1976), An Introduction to Probability Theory and Mathe-
matical Statistics, New York: John Wiley, p. 191, for a proof that the geometric
density is the only density for nonnegative integer-valued random variables that
has this property.
Example 4.5
Probability of Meeting
Quota in x Trials
A salesperson has a quota of 10 sales per day that she is expected to meet for her
performance to be considered satisfactory. If the probability is .25 that any given
customer she contacts will make a purchase, and if purchase decisions are
independent across consumers, what is the probability that the salesperson
will meet her quota with no more than 30 customer contacts?
Answer: The negative binomial density function can be applied with p ¼ .25 and
r ¼ 10. The event of interest is {x  30}, which has the probability
Pðx  30Þ ¼
S
30
x¼10
ðx  1Þ!
9!ðx  10Þ!


ð:25 Þ10 ð:75 Þx10 ¼ :1966
□
Example 4.6
Probability of Defect
after x Trials
A machine produces envelopes, and the probability that any given envelope will
be defective is p ¼ .001. The production of envelopes from the machine can be
viewed as a collection of independent Bernoulli trials (i.e., each envelope is
182
Chapter 4
Parametric Families of Density Functions

either defective or nondefective, the probability of a defective is .001, and the
occurrence of a defective does not affect the probability that other envelopes will
be defective or nondefective). What is the probability that the ﬁrst defective
envelope will occur after 500 envelopes have been produced? Given that the
machine has already produced 500 envelopes without a defective, what is the
probability that the ﬁrst defective envelope will occur after another 500
envelopes have been produced?
Answer: The geometric density can be applied with p ¼ .001. The ﬁrst event of
interest is {x > 500}. The probability of the event can be calculated by ﬁrst noting
that the cumulative distribution function for the geometric density is given by
FðbÞ ¼ ½1  ð1  p ÞtruncðbÞ I½1;1Þ ðbÞ:
Then
P x> 500
ð
Þ ¼ 1  F 500
ð
Þ ¼ 1  1  :999
ð
Þ500
h
i
¼ :6064: The graph of
f(x) in this case has a set of spikes at x ¼ 1,2,3,. . ., that decline very slowly,
beginning with f(1) ¼ .001, and with the image of x ¼ 250 still being equal to
f(250) ¼ .00078. As x ! 1; fðxÞ ! 0.
The second probability we seek is of the form P x>1,000jx>500
ð
Þ. By the
memoryless property of the geometric density, we know that this probability is
equal to P x>500
ð
Þ ¼ :6064, the same as above.
□
4.1.6
Family Name: Poisson
Parameterization: l∈O ¼ {l: l > 0}
Density Deﬁnition:
fðx; lÞ ¼
el lx
x!
for x ¼ 0; 1; 2; . . . ;
0
otherwise
8
<
:
Moments: m ¼ l, s2 ¼ l, m3 ¼ l,
m4
s4  3


¼
1
l
 
MGF: MX(t) ¼ exp l et  1
ð
Þ
ð
Þ
Background and Application: When the number of independent and identical
Bernoulli experiments is very large and p is small, the Poisson density, named
after French mathematician Simeon Poisson (1781–1840), provides an approxi-
mation to the probability that x ¼ Pn
i¼1 xi ¼ c and thus provides an approxima-
tion to probabilities generated by the binomial density. In fact, the limit
of the binomial density as n ! 1 and l ¼ np, with l set to a ﬁxed constant, is
the Poisson density. We examine this situation in more detail to provide an
example of how limiting densities arise.2 In the discussion, we will have need
for the following result.
Lemma 4.1
ev ¼ limn!1 1 þ v=n
ð
Þ
ð
Þn:
2Limiting densities will be discussed further in Chapter 5.
4.1
Parametric Families of Discrete Density Functions
183

The binomial density can be expressed alternatively as
fðx; n; pÞ ¼
Qx
i¼1 n  i þ 1
ð
Þ
x!
px ð1  p Þnx:
where we have suppressed the indicator function by assuming that x is a non-
negative integer
 n. Let np ¼ l for some l > 0, so that p ¼ l/n can be
substituted in the binomial density expression to yield
Qx
i¼1 n  i þ 1
ð
Þ
x!
l
n
 x
1  l
n

nx
:
Algebraically rearranging the above expression, and letting n ! 1 yields
lim
n!1
Qx
i¼1 n  i þ 1
ð
Þ
nx
lx
x!
1  l
n

n
1  l
n

x
¼ lim
n!1
n
n
  n  1
n


  
n  x þ 1
n

 lx
x!
1  l
n

n
1  l
n

x


¼ el lx
x!
since “the limit of a product equals the product of the limits” when all of the
limits exist, and since lim
n!1
ni
n


¼ 1
8i;
lim
n!1 1  l
n

x ¼ 1;
lim
n!1 1  l
n

n ¼ el
by Lemma 4.1, and lim
n!1
lx
x!
 
¼ lx
x! :
Therefore, the binomial density converges to the Poisson density for np ¼ l,
l>0, and n ! 1. The usefulness of this result is that for large n, and thus for
small p ¼ l/n, one can adopt the approximation
n
x


px ð1  p Þnx 
enp np
ð
Þx
x!


;
that is, one can replace the parameter l in the Poisson density by the product of
the binomial density parameters n and p. The approximation can be quite useful
since the Poisson density is relatively easy to evaluate, whereas for large n,
dealing with the factorial expressions in the binomial density can be cumber-
some. Based on two “rules of thumb”, the approximation is considered
reasonably good if n  20 and p  .05, or if n  100 and l ¼ np  10.
Example 4.7
Approximate
Probability of x Defects
in n Trials
A publishing company is typesetting a novel that is 300 pages long and averages
1,500 typed letters per page. If typing errors are as likely to occur for one letter as
another, if a typing error occurring in one place does not affect the probability of
a typing error occurring in any other place, and if the probability of mistyping a
letter is small, then the total number of typing errors in the book can be viewed
as the outcome of a random variable having, approximately, a Poisson density.
For example, if the probability of a typing error for any given letter is 105, then
l ¼ np ¼ 4.5 in the Poisson density. The probability of observing 10 or fewer
errors in the book would be approximated as
184
Chapter 4
Parametric Families of Density Functions

Pðx  10Þ 
X
10
x¼0
e4:5ð4:5 Þx =x! ¼ :9933:
A partial graph (truncated after x ¼ 10) of the Poisson density in this case is
given in Figure 4.4, and density values are exhibited in Table 4.2.
□
4.1.7
Poisson Process and Poisson PDF
Besides serving as an approximation to the binomial density for large n and small
p, the Poisson density is important in its own right for constructing probability
spaces for experiments whose outcomes are governed by the so-called Poisson
process. The Poisson process refers to a particular type of experimental situation
in which the number of occurrences of some speciﬁc type, say type A (e.g., a
trafﬁc accident, a telephone call, the arrival of a customer at a checkout stand, a
ﬂaw in a length of wire) in a time, space, volume or length dimension, possesses
f(x)
x
2
0
.2
.15
.1
3
4
6
1
7
8
9
10
5
.05
•
•
•
•
•
•
•
•
•
•
•
Figure 4.4
Partial Poisson density,
l ¼ 4.5.
Table 4.2
Poisson Density for l ¼ 4.5
x
f(x)
0
.0111
1
.0500
2
.1125
3
.1687
4
.1898
5
.1708
6
.1281
7
.0824
8
.0463
9
.0232
10
.0104
4.1
Parametric Families of Discrete Density Functions
185

the following general probabilistic characteristics (we state the conditions for a
time dimension; the conditions can interpreted within the other dimensions of
measurement as well):
Deﬁnition 4.1
Poisson Process
Let an experiment consist of observing the number of type A outcomes that
occur over a ﬁxed interval of time, say [0,t]. The experiment is said to follow
the Poisson process if:
1. The probability that precisely one type A outcome will occur in a small
time interval of length, Dt, is approximately proportional to the length of
the interval, as g[Dt] + o(Dt), where g > 0 is the proportionality factor;3
2. The probability of two or more type A outcomes occurring in a small time
interval of length Dt is negligible relative to the probability that one type
A outcome occurs, this negligible probability being of order of magnitude
o(Dt); and
3. The numbers of type A outcomes that occur in nonoverlapping time
intervals are independent events.
To appreciate what is meant by the probability of two or more type A
outcomes occurring in a small interval of length Dt being negligible relative to
the probability that just one type A outcome occurs, note that
lim
Dt!0
Pð  2 typeAÞ
Pð1 typeAÞ


¼ lim
Dt!0
oðDtÞ
g½Dt þ oðDtÞ


¼ lim
Dt!o
oðDtÞ
Dt
g½Dt
Dt þ o Dt
ð
Þ
Dt
 
!
¼ 0:
Thus, for small enough intervals Dt, P(  2 type A) is negligible relative to
P(1 type A).
We now indicate why the Poisson process leads to the Poisson density.
Theorem 4.1
Poisson Process
) Poisson Density
Let X represent the number of times event A occurs in an interval of time [0,t].
If the experiment underlying X follows the Poisson process, then the density of
X is the Poisson density.
Proof
Partition the interval [0,t] into n successive disjoint subintervals, each of
length Dt ¼ t/n, and denote these intervals by Ij, j ¼ 1,. . .,n. Let the random
variable X(Ij) denote the number of outcomes of type A that occur within
subinterval Ij, so that
X ¼
X
n
j¼1
XðIjÞ:
3o(Dt) is a generic notation applied to any function of Dt, whose values approach zero at a rate faster than Dt, so that limDt!0 o Dt
ð
Þ
ð
Þ=
Dt
ð
Þ ¼ 0. The “o(Dt)” stands for “of smaller order of magnitude than Dt.” For example, h(Dt) ¼ (Dt)2 is a function to which we could
afﬁx the label o(Dt), while h(Dt) ¼ (Dt)1/2 is not. More will be said about orders of magnitude in Chapter 5.
186
Chapter 4
Parametric Families of Density Functions

Examine the event x ¼ k, and note that P(x ¼ k) ¼ P(An) + P(Bn), where An
and Bn are the disjoint sets
An ¼
X
n
j¼1
xðIjÞ ¼ k; xðIjÞ ¼ 0 or 1; 8j
(
)
Bn ¼
X
n
j¼1
xðIjÞ ¼ k; xðIjÞ  2; for 1 or more j0s
(
)
:
Since Bn 
x Ij
 
 2 for 1 or more j’s


 [n
j¼1 x Ij
 
 2


, Boole’s inequality
implies
PðBnÞ  P
n
j¼1
PfxðIjÞ  2g ¼ P
n
j¼1
o t
n
 
¼ t
o t
n
 
t
n
	

, so that limn!1P Bn
ð
Þ ¼ 0.
Now examine P(An). For each subinterval, deﬁne a “success” as observing
exactly one type A outcome, and a “failure” otherwise. Then by property (1) of the
Poisson process, P(success) ¼ g[t/n] + o(t/n) and P(failure) ¼ 1  g[t/n]  o(t/n).
Since events in one subinterval are independent of events in other subintervals
by property (3) of the Poisson process, we can view the observations on the n
subintervals as a collection of independent Bernoulli trials, each trial yielding a
success or a failure. It follows that probability can be assigned to event An by the
binomial density as
PðAnÞ ¼
n
k


g t
n þ o t
n
 
h
ik
1  g t
n  o t
n
 
h
ink
:
We need the following extension of Lemma 4.1:
Lemma 4.2
ev ¼ limn!1 1 þ v=n
ð
Þ 	 o v=n
ð
Þ
ð
Þn:
Then, following a similar approach to the one used to demonstrate the conver-
gence of the binomial to the Poisson density, it can be shown using Lemma 4.2
that limn!1P An
ð
Þ ¼
egt gt
ð
Þk


=k!, which is the value of the Poisson density,
where l ¼ gt.
Finally, since P(x ¼ k) ¼ P(An) + P(Bn) 8n, we have Pðx ¼ kÞ ¼ lim
n!1 ½PðAnÞ þ
PðBnÞ ¼ egt ðgt Þk
k!
and so the conditions of the Poisson process lead to assignments
of probability via the Poisson density function.
n
Note that when use of the Poisson density is motivated from within the
context of the Poisson process, g can be interpreted as the mean rate of occurrence
of the type A outcome per unit of time. This follows from the fact that E(X) ¼ l
¼ gt if X has a Poisson density, and then g ¼ E(X)/t. In applications, the Poisson
density would be chosen for constructing the probability space of an experiment
when either the conditions of the Poisson process hold, or when conditions exist
for the Poisson density to be a reasonable approximation to the binomial density.
The Poisson PDF has been used to represent the probability of such random
variables as the number of machine breakdowns in a work shift, the number of
4.1
Parametric Families of Discrete Density Functions
187

customer arrivals at a checkout stand during a period of time, the number of
telephone calls arriving at a switchboard during a time interval, and the number
of ﬂaws in panes of glass.
Example 4.8
Probability of Number
of Machine Breakdowns
The milk bottling machine in a milk processing plant has a history of breaking
down, on average, once every 2 weeks. The chief of the repair and maintenance
crew is scheduling vacations for the summer months and wants to know what
the probability is that the bottling machine will break down more than three
times during the next 4 weeks. What is the probability?
Answer: When viewed in the context of ever shorter time intervals (hours,
minutes, seconds), breakdowns appear to be increasingly less likely, and it
seems reasonable to assume that the probability of two breakdowns within a
short-enough time interval would be negligible. Assuming the repair crew
returns the machine to full operating performance after each breakdown, it is
reasonable to assume that the event of a breakdown in any short interval of time
is independent of a breakdown occurring in other intervals of time.4 All told, it
would appear that the conditions of the Poisson process are a reasonable approx-
imation to this situation, and we endeavor to assign probability to the event x>3
using the Poisson density. Since the average number of breakdowns is 1 every
2 weeks and since the chief is interested in a 4-week time interval, the Poisson
density of relevance here is e22X


=x!If0;1;2;...gðxÞ, where l ¼ gt ¼ 2 was chosen
to represent a rate of 2 breakdowns every 4 weeks (g ¼ .5 breakdowns per week
times t ¼ 4 weeks). The probability of more than 3 breakdowns in the 4-week
period is then given by
P x>3
ð
Þ ¼ 1 
X
3
i¼0
Pðx ¼ iÞ ¼ 1 
X
3
i¼0
e2 2i
i! ¼ :143:
□
4.1.8
Family Name: Hypergeometric
Parameterization:
(M,K,n) ∈O ¼ {(M,K,n):
M ¼ 1,2,3,. . .;
K ¼ 0,1,. . .,M;
n ¼ 1,2,. . .,M}
Density Deﬁnition:
fðx;M;K;nÞ ¼
K
x


M K
nx


M
n


for integer values maxf0;nðM KÞgx minfn;kg
0
otherwise
8
>
>
>
>
<
>
>
>
>
:
4Ultimately, this assumption could be tested using a nonparametric test of hypothesis. We will examine tests of independence in our
discussion of hypothesis testing procedures in Chapter 10.
188
Chapter 4
Parametric Families of Density Functions

Moments:
m ¼ nK
M ;
s2 ¼ n
K
M

 M  K
M

 M  n
M  1


;
m3 ¼ n
K
M

 MK
M

 M  2K
M

 M  n
M  1

 M  2n
M  2


;
m4
s4  3


¼
M2 M  1
ð
Þ
n M  2
ð
Þ M  3
ð
Þ M  n
ð
Þ

M M þ 1
ð
Þ  6M M  n
ð
Þ
K M  K
ð
Þ


þ 3n M  n
ð
Þ M þ 6
ð
Þ
M2
 6
	

MGF: MXðtÞ ¼ [((M  n)!(M  K)!)/M!] H(n, K, M  K  n + 1, et) where H ðÞ
is the hypergeometric function Hða;b;r;ZÞ ¼ 1þab
r
Z
1!þabðaþ1Þðbþ1Þ
rðrþ1Þ
Z2
2! þ:::
(Note: this MGF is not too useful in practice since moments are deﬁned in
terms of an inﬁnite sum; to illustrate this fact, the reader should attempt to
deﬁne m by differentiating MXðtÞ once with respect to t and evaluating the
derivative at zero.)
Background and Application: The hypergeometric density is used to construct a
probability space for an experiment in which there are:
1. M objects, of which K of them are of one type, say type A;
2. The remaining M–K objects are of a different type, say B; and
3. n objects are randomly drawn without replacement from the original collec-
tion of M objects and the number, x, of type A outcomes in the collection of n
objects drawn is observed.
By drawing randomly without replacement, we mean that, at each draw, all
of the objects that remain in the collection have an equal probability of being
chosen. The hypergeometric density assigns probability to the number of objects
drawn that are of type A out of a total sample of n objects.
To motivate the density function deﬁnition, note that the number of differ-
ent ways of choosing the sample of n objects is given by
M
n


, the number of
different ways of choosing x type A items is
K
x


, and the number of different
ways to choose (n  x) type B items is
M  K
n  x


. Then, since all possible sample
outcomes having x type A and n  x type B outcomes are equally likely,
the classical probability deﬁnition states that the probability of obtaining x
outcomes of type A from a random sample (without replacement) of n objects
from the aforementioned collection is
K
x


M  K
n  x


M
n


.
4.1
Parametric Families of Discrete Density Functions
189

Note
that
the
binomial
and
hypergeometric
densities
both
assign
probabilities to the event “observe x type A outcomes in a sample of n
observations”. The important difference between experiments for which the
binomial density or hypergeometric density applies is that in the former case,
the n trials are independent and identical and would correspond to randomly
drawing objects with replacement (meaning once an object is drawn, and the
observation made, the object is placed back in the total collection of objects so
that, at each draw, all of the original objects in the collection are equally
probable to be chosen), whereas randomly drawing objects without replacement
characterizes the experiment to which the hypergeometric density is applied.
Example 4.9
Probability of No
Defectives in Random
Inspection of x items
without Replacement
Suppose a shipment of 1,000 computer memory modules contains 50 defectives.
What is the probability of obtaining no defective modules in a random drawing,
without replacement, of ﬁve modules from the shipment for inspection?
Answer: The appropriate hypergeometric density for this case has M ¼ 1,000,
K ¼ 50, and n ¼ 5. The probability assigned to the (elementary) event x ¼ 0 is
fð0Þ ¼
50!
0!50!


950!
5!945!


1000!
5!995!


¼ :7734:
□
Regarding the graph of the hypergeometric density, f(x; M,K,n) increases as x
increases until a maximum value is reached, which occurs at the largest integer
value of x satisfying x 
n þ 1
ð
Þ K þ 1
ð
Þ
ð
Þ= M þ 2
ð
Þ (this can be shown by exam-
ining the values of x for which the density is increasing, i.e., f x; M; K; n
ð
Þ=
f x  1; M; K; n
ð
Þ  1). The value of the PDF declines thereafter. Some random
variables to which the hypergeometric density has been applied include
observations on the number of responses of a certain type in the context of
auditing, quality control, and consumer or employee attitude surveys.
4.1.9
Family Name: Multivariate Hypergeometric
Parameterization:
M; K1; . . . ; KM; n
ð
Þ 2 O ¼

ðM; K1; . . . ; KM; nÞ : M ¼ 1; 2; . . . ; Ki ¼ 0; 1; . . . ; M for i ¼ 1; . . . ; m;
X
m
i¼1
Ki ¼ M; n ¼ 1; 2; . . . ; M

Density Deﬁnition:
f x1; . . . ; xm; M; n; K1; . . . ; Km
ð
Þ ¼
Qm
j¼1
Kj
xj


M
n


for xi 2 f0; 1; 2; . . . ; ng 8 i; P
n
i¼1
xi ¼ n;
0
otherwise
8
>
>
>
>
<
>
>
>
>
:
190
Chapter 4
Parametric Families of Density Functions

Moments: (expressed for each Xi)
m Xi
ð
Þ ¼ nKi
M ; s2 Xi
ð
Þ ¼ n Ki
M

 M  Ki
M

 M  n
M  1


;
m3 Xi
ð
Þ ¼ n Ki
M

 MKi
M

 M  2Ki
M

 M  n
M  1

 M  2n
M  2


m4 Xi
ð
Þ
s4 Xi
ð
Þ  3


¼
M2 M  1
ð
Þ
n M  2
ð
Þ M  3
ð
Þ M  n
ð
Þ

M M þ 1
ð
Þ  6M M  n
ð
Þ
Ki M  Ki
ð
Þ


þ 3n M  n
ð
Þ M þ 6
ð
Þ
M2
 6
	

MGF: not useful
Background and Application: The multivariate hypergeometric density is a
generalization of the hypergeometric density in the same sense as the multino-
mial density is a generalization of the binomial density. In particular, we are
considering a case where we are interested in more than two different types of
outcomes for each object chosen from the original collection of objects. Letting
Ki, i ¼ 1,. . .,m, refer to the number of objects of type i that are in the collection,
M ¼ Pm
i¼1 Ki represent the total number of objects in the collection, n represent
the number of objects randomly drawn without replacement from the collec-
tion, and xi be the number of outcome of type i, i ¼ 1,. . .,m, an extension of the
argument used to motivate the density deﬁnition in the hypergeometric case
leads to the deﬁnition of the multivariate hypergeometric density function
presented above.
Note that the marginal density of each Xi is hypergeometric with parameters
M; Ki; n
ð
Þ. Furthermore, any subset of the random variables X1; . . . ; Xm
ð
Þ also has
a multivariate hypergeometric density, as the reader can verify.
Example 4.10
Probability of Quality
Assurance Survey
Outcome
As part of their quality assurance program, a large northwestern bank regularly
interviews a randomly selected subset of the customers who transact business at
one of its branches each week. Among other questions, the customers are asked
to rank the overall service they received as being “excellent,” “good,” “average,”
“below average,” or “poor.” In one of the smaller rural branches, there were 100
customers who entered the branch during a particular week. If the bank ran-
domly chooses ﬁve of the 100 customers to interview, and if the 100 customers
were distributed across the rating categories as 50, 30, 10, 7, 3, respectively, what
is the probability that the interviews will result in 2 “excellent,” 2 “good,” and 1
“average rating?”
Answer: Use the multivariate hypergeometric density with M ¼ 100; K1 ¼ 50,
K2 ¼ 30; K3 ¼ 10; K4 ¼ 7; K5 ¼ 3; and n ¼ 5. Then
P x1 ¼ 2;x2 ¼ 2;x3 ¼ 1;x4 ¼ x5 ¼ 0
ð
Þ ¼
50
2
 
!
30
2
 
!
10
1
 
!
7
0
 
!
3
0
 
!
100
5
 
!
¼ :0708:
□
4.1
Parametric Families of Discrete Density Functions
191

4.2
Parametric Families of Continuous Density Functions
4.2.1
Family Name: Uniform
Parameterization: (a,b) ∈O ¼ {(a,b): 1 < a < b < 1}
Density Deﬁnition: f x; a; b
ð
Þ ¼ 1= b  a
ð
Þ
ð
ÞI a;b
½
ðxÞ
Moments: m ¼ a þ b
ð
Þ=2; s2 ¼ b  a
ð
Þ2=12; m3 ¼ 0,
m4
s4  3


¼  6
5
MGF:
MXðtÞ ¼
ebt  eat
b  a
ð
Þt for t 6¼ 0;
1 for t ¼ 0
8
>
<
>
:
Background and Application: The continuous uniform density is used to con-
struct probability spaces for experiments having an uncountably inﬁnite num-
ber of possible outcomes that are all equally likely in the interval [a,b], for ﬁnite
b  a. All interval subsets of [a,b] of length k are assigned equal probability, k/
(b  a). The continuous uniform density has important applications in the
computer generation of random variable outcomes for a wide array of probability
distributions. We will examine these types of applications in Chapter 6,
a preview of which is provided in the next example.
Example 4.11
Simulating the Demand
for Teller Services
An efﬁciency analyst wishes to simulate the daily demand for teller services in a
moderately sized branch of a regional bank. The probability that a customer will
require the services of a teller is known to be .30, whereas the customer will
utilize the services of a cash machine, loan ofﬁcer, or investment banker with
probability .70. The type of service demanded is independent across customers.
The efﬁciency analyst concludes that of n customers entering the branch on a
given day, the number that utilize a teller’s services is the outcome of a random
variable X having the binomial density f(x) ¼
n
x
 
!
(.30)x(.70)n  x I{0,1,. . .,n}(x). The
analyst has a computer random-number generator that produces outcomes of a
random variable Y having the uniform density h(y) ¼ I(0,1)(y). How can the
analyst simulate daily outcomes of X?
Answer: Let FðbÞ ¼ P
x  b fðxÞ be the CDF of X, and deﬁne n þ 1 intervals as I0
¼ 0; Fð0Þ
½
Þ and Ij ¼ F j  1
ð
Þ; FðjÞ
½
Þ, for j ¼ 1,. . .,n. Note that the lengths of the
intervals correspond to the respective probabilities assigned to the outcomes
x ∈{0,1,. . .,n} by the aforementioned binomial density. In particular, using
the relationship between CDFs and PDFs, P x ¼ 0
ð
Þ ¼ Fð0Þ ¼ fð0Þ and P x ¼ j
ð
Þ
¼ FðjÞ  F j  1
ð
Þ ¼ fðjÞ for j ∈{1,2,. . .,n}. Then an outcome of X can be simulated
by ﬁrst generating an outcome of Y and then calculating x ¼ Pn
j¼0 jIIjðyÞ. That
these outcomes follow the appropriate binomial density can be motivated by the
fact that x ¼ j iff y 2 Ij and PXðjÞ ¼ PY Ij
 
¼ R
y2Ij I 0;1
ð
ÞðyÞdy ¼ fðjÞ.
For a speciﬁc illustration, let n ¼ 5. Then I0 ¼ [0, .1681), I1 ¼ [.1681, .5283),
I2 ¼ [.5283, .8370), I3 ¼ [.8370, .9693), I4 ¼ [.9693, .9977), and I5 ¼ [.9977, 1).
192
Chapter 4
Parametric Families of Density Functions

If the computer random-number generator were to generate an outcome
y ¼ .6311, then the analyst would simulate that two customers required teller’s
services on a day when ﬁve customers entered the branch (since .6311 ∈I2). □
4.2.2
Family Name: Gamma, and Exponential and Chi-Square Subfamilies
Parameterization: (a, b)∈O ¼ {(a, b): a > 0, b > 0}
Density Deﬁnition: fðx; a; bÞ ¼ 1= baG a
ð Þ
ð
Þ
ð
Þ xa1 ex=b Ið0;1Þ ðxÞ , where G(a) ¼
R 1
0 ya1eydy is the gamma function, having the property that if a is a positive
integer, G(a) ¼ (a  1)!, and if a ¼ 1/2, then G(1/2) ¼ p1/2. Also, for any real
a > 0, G(a + 1) ¼ aG(a).
Moments: m ¼ ab; s2 ¼ a b2; m3 ¼ 2a b3, m4
s4  3


¼ 6
a
MGF: MX(t) ¼ (1  bt)a for t < b1
Background and Applications: The gamma family of density functions is a
versatile collection of density functions that can be used to model a wide
range of experiments whose outcomes are coded as nonnegative real numbers
and whose respective probabilities are to be assigned via a density function that
is skewed to the right. An extensive variety of density shapes are possible by
altering the parameters a and b, a few of which are illustrated in Figures 4.5 and
4.6. It can be shown that the gamma density is strictly decreasing when a  1.
The density increases to a maximum of ((a  1) e1)a1/(b G(a)), at x ¼ (a  1)b,
for a > 1.
While its wide variety of shapes makes the gamma family a candidate for
constructing the probability space of many experiments with nonnegative
outcomes, the gamma family has speciﬁc uses with regard to waiting times
between occurrences of events based on the Poisson process. In particular, let
Y have a Poisson density with parameter l, and let y refer generically to the
number of successes that occur in a period of time t, so that g ¼ l/t is the rate of
success of the Poisson process. If X measures the time that passes until the
Poisson process produces the ﬁrst success, then X has a gamma density with
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
1.2
a = 1
a = 2
a = 4
x
f(x)
Figure 4.5
Gamma densities, case I,
b ¼ 1.
4.2
Parametric Families of Continuous Density Functions
193

a ¼ 1 and b ¼ g1. If instead X measures the time that passes until the Poisson
process produces the rth success, then X has a gamma density with a ¼ r and
b ¼ g1. Proofs of these propositions can be found in S.F. Arnold (1990), Mathe-
matical Statistics, p. 166.
Example 4.12
Probability of Time
Between Machine
Breakdowns
Recall Example 4.8 regarding breakdowns of a milk bottling machine, which was
assumed to be a Poisson process with rate g ¼ l/t ¼ 2/4 ¼ .5. What is the
probability that the machine will not breakdown in four weeks of use?
Answer: The event of interest is {x > 4}, that is, the ﬁrst breakdown occurs after
4 weeks of operation. From the preceding discussion, we know that X has a
gamma density with a ¼ 1 and b ¼ g1 ¼ 2. Therefore, P(x > 4) ¼
R 1
4
1=2
ð
Þex/2
dx ¼ ex/2
1
4

¼ .1353. Note this is precisely the same probability that the
outcome 0 receives in the Poisson density of Example 4.8, which is as it should
be, given that no breakdowns occurring in 4 weeks (0 outcome for the Poisson
random variable) coincides with the ﬁrst breakdown occurring after 4 weeks (an
outcome greater than four for the gamma random variable).
□
The gamma family of densities has an important additivity property, which
we state in the following theorem.
Theorem 4.2
Gamma Additivity
Let X1,. . .,Xn be independent random variables with respective gamma
densities Gamma(ai, b), i ¼ 1,. . .,n. Then Y ¼
Pn
i¼1 Xi
has the density
Gamma Pn
i¼1 ai; b


.
Proof
Since MXiðtÞ ¼ 1  bt
ð
Þai for t<b1; i ¼ 1; . . . ; n, and since the Xi’s are indepen-
dent, Theorem 3.27 implies
MY ðtÞ ¼
Y
n
i¼1
MXi ðtÞ ¼
Y
n
i¼1
ð1  bt Þ ai ¼ 1  bt
ð
ÞPn
i¼1 ai for t<b1:
0
1
2
3
4
5
6
7
8
0
0.1
0.2
0.3
0.4
0.5
b = 2
b = 3
b = 1
f(x)
x
Figure 4.6
Gamma densities, case II,
a ¼ 3.
194
Chapter 4
Parametric Families of Density Functions

Thus, by the MGF uniqueness theorem, Y has the density Gamma Pn
i¼1 ai; b


.n
Therefore, the sum of independent gamma random variables has a gamma
distribution as long as the underlying gamma densities share the same b param-
eter value.
Scaling a gamma random variable by a positive constant results in a random
variable that also has a gamma distribution, as demonstrated in the next
theorem.
Theorem 4.3
Scaling of Gamma
Random Variables
Let X have a gamma density Gamma(a,b), and let c > 0. Then Y ¼ c X has a
gamma density Gamma(a,bc).
Proof
Since MX(t) ¼ (1  bt)a for t < b1, Theorem 3.27 implies that
MYðtÞ ¼ McXðtÞ ¼ MX ct
ð
Þ ¼ 1  bct
ð
Þa for t< bc
ð
Þ1, which by the MGF unique-
ness theorem indicates that Y has the gamma density Gamma(a,bc).
n
We will also have use for the following property of gamma PDFs.
Theorem 4.4
Gamma Inverse
Additivity
Let Y ¼ X1 + X2, where Y has density Gamma(a, b), X1 has density Gamma(a1, b),
a > a1, and X1 and X2 are independent. Then X2 has density Gamma(a  a1, b).
Proof
Since MYðtÞ ¼ 1  bt
ð
Þa and MX1ðtÞ ¼ ð1  bt Þa1 for t < b1, Theorem 3.27
implies that
MYðtÞ ¼ 1  bt
ð
Þa ¼ 1  bt
ð
Þa1MX2ðtÞ ¼ MX1ðtÞ MX2ðtÞ;
which in turn implies
MX2ðtÞ ¼ 1  bt
ð
Þa= 1  bt
ð
Þa1 ¼ 1  bt
ð
Þ aa1
ð
Þfor t< b1 :
It follows from the MGF uniqueness theorem that X2 has the gamma density
Gamma(a  a1, b).
n
Its wide variety of density shapes has resulted in the gamma family’s being
applied to a myriad of nonnegative-valued random variables suspected of having
a right-skewed PDF. Some speciﬁc applications include the waiting times
between customer arrivals or machine breakdowns or telephone calls, the
breaking strength of manufactured construction materials, the operating lives
of electronic equipment and other objects, and the length of time required to
service a customer at a store. We now examine two important subfamilies of the
gamma family of densities that are deﬁned by special choices of the parameters
a and b.
4.2.3
Gamma Subfamily Name: Exponential
Parameterization: y ∈O ¼ {y: y > 0}
4.2
Parametric Families of Continuous Density Functions
195

Density Deﬁnition: The gamma density, with a ¼ 1, and b ¼ y.
fðx; yÞ ¼ 1
y ex=y Ið0;1Þ ðxÞ
Moments: m ¼ y; s2 ¼ y2; m3 ¼ 2 y3,
m4
s4  3


¼ 6
MGF: MX(t) ¼ (1  yt)1 for t < y1
Background and Application: The exponential density is used to construct a
probability space for experiments that have a real-valued sample space given by
the nonnegative subset of the real line, [0,1), and in which interval events of
ﬁxed length d > 0 of the form [t, t + d] are to be assigned probability that
monotonically decreases as t increases. A speciﬁc application concerns the
experiment of observing the time that passes until a Poisson process with rate
g ¼ l/t produces the ﬁrst success, in which case the exponential density with
y ¼ g1 is appropriate (recall our previous discussion regarding the relationship
between the gamma density and the Poisson process).
A prominent application of the exponential density is in representing the
operating lives until failure of various objects. In this regard, the following
property of the exponential density is of notable importance.
Theorem 4.5
Memoryless Property
of Exponential Density
If X has an exponential density, then P(x > s + t | x > s) ¼ P(x > t) 8 t and
s > 0.
Proof
Pðx>s þ tjx>sÞ ¼ Pðx>s þ tÞ
Pðx>sÞ
¼
Ð 1
sþt
1
y ex=y dx
Ð 1
s
1
y ex=y dx
¼ eðsþtÞ=y
es=y
¼ et=y ¼ Pðx>tÞ
n
Interpreted in the context of operating life, the memoryless property implies
that given the object has already functioned for s units of time without failing
(which is the meaning of the conditioning event x > s), the probability that it
will function for at least an additional t units of time (which is the meaning of
the event x > s + t) is the same as the unconditional probability that it would
function for at least t units of time (the meaning of the event x > t). In effect, the
object is “as good as new” after functioning s units of time, since the probability
of functioning for at least another t units of time is the same as if it had not
previously functioned at all (i.e., as if it were “new”).5
While the memoryless property certainly is not applicable to the lifetimes of
all objects, the assumption is appropriate in the modeling of the lifetimes of
certain electronic components, fuses, jeweled watch bearings, and other objects
5The exponential density is the only density for continuous nonnegative-valued random variables that has the memoryless property.
See V.K. Rohatgi (1976) An Introduction to Probability Theory and Mathematical Statistics. New York: John Wiley, p. 209.
196
Chapter 4
Parametric Families of Density Functions

that are not subject to signiﬁcant wear and are essentially “as good as new”
if they are still functioning. Furthermore, the assumption is appropriate for
objects, such as machinery, that receive periodic maintenance that reconditions
the object to essentially new status.
If the memoryless assumption is not appropriate, the more versatile gamma
family of densities can be considered for constructing the probability space.
It can be shown that members of the gamma family, other than the exponential
family, exhibit “wear-out” effects in the sense that P(x > s + t|x > s) declines as
s increases for t > 0 and a > 1, that is, the probability that the object functions
for at least t units of time beyond the s units of time for which it has already
functioned declines as the value of s increases. For a < 1, the conditional proba-
bility actually increases as s increases – this is referred to in the literature as the
“work-hardening” effect. The graphs of some exponential densities are displayed
in Figure 4.7.
Example 4.13
Probability of Operating
Lives for “Good as
New” Electronics
The lifetime of a fuse your company manufactures is the outcome of a random
variable with mean ¼ 500 hours. The fuse is “as good as new” while function-
ing. What is the probability that the fuse functions for at least 1,000 hours?
Answer: Because of the “good as new” property, the exponential density is
appropriate with y ¼ 500. Then,
Pðx  1000Þ ¼
Z 1
1000
1
500 ex=500 Ið0;1Þ ðxÞdx ¼  ex=500 j1
1000 ¼ :1353:
□
4.2.4
Gamma Subfamily Name: Chi-Square
Parameterization: v ∈O ¼ {v: v is a positive integer}
Density Deﬁnition: The gamma density, with a ¼ v/2 and b ¼ 2.
fðx; vÞ ¼
1
2v=2 Gðv=2Þ
xðv=2Þ1 ex=2 Ið0;1Þ ðxÞ
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
1.2
f(x)
x
q = 1
q = 2
q = 4
Figure 4.7
Exponential densities.
4.2
Parametric Families of Continuous Density Functions
197

Moments: m ¼ v; s2 ¼ 2v; m3 ¼ 8v, m4
s4  3


¼ 12
v
MGF: MX(t) ¼ (1  2 t)v/2 for t <
1
2
Background and Application: The parameter v of the chi-square density is
called the degrees of freedom. The reason for this term will be clariﬁed later in
the chapter where we will show that the sum of the squares of v independent
random variables, each having a density called the standard normal (to be
discussed in Section 4.3), will have a chi-square density with v degrees of
freedom. The chi-square density with v degrees of freedom is often indicated
by the notation w2
v or w2(v). We will utilize the former. The relationship between
the chi-square and normal density makes the chi-square density especially
important in applications concerning hypothesis testing and conﬁdence interval
estimation, which is its primary application context, as will be seen in later
chapters. Note for v ¼ 2, the w2 density is equivalent to the exponential density
with y ¼ 2. Also, w2
v is a valid PDF even for noninteger values of v > 0, in which
case the PDF w2
v is referred to as the nonintegral chi-square density. Our use of w2
v
will be restricted to integer-valued v. Some chi-square densities are graphed in
Figure 4.8.
There are two important properties of the w2 density relating to sums of
random variables that we note in the following two corollaries.
Corollary 4.1
Chi-Square Additivity
Let X1,. . .,Xk be independent random variables having chi-square densities
with v1,. . .,vk degrees of freedom, respectively. Then Y ¼ Pk
i¼1 Xi has a chi-
square density with degrees of freedom v ¼ Pk
i¼1 vi.
Proof
This follows directly from Theorem 4.2 with b ¼ 2.
n
Thus the sum of independent chi-square random variables also has a chi-
square density.
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
1.2
v = 1
v = 2
v = 4
v = 5
f(x)
x
Figure 4.8
w2 densities.
198
Chapter 4
Parametric Families of Density Functions

Corollary 4.2
Chi-Square
Inverse Additivity
Let X1 and X2 be independent random variables, where X1 has a chi-square
density with v1 degrees of freedom, and Y ¼ X1 + X2 has a chi-square density
with v > v1 degrees of freedom. Then X2 has a chi-square density with vv1
degrees of freedom.
Proof
This follows directly from Theorem 4.4 with b ¼ 2, a ¼ v/2, and a1 ¼ v1/2.
n
Later in our study of hypothesis testing it will be useful to know the values
of upper- and lower-bounds h and ‘ for which P(x  h) ¼ a and P(x  ‘) ¼ a are
true, where X has a w2 density with v degrees of freedom and a ∈(0,1). These are,
of course, quantiles of the chi-square distribution and are illustrated, diagram-
matically, by the upper and lower tails of thew2
v density in Figure 4.9. Such events
and their probabilities are identiﬁed in tables of the w2 density available from
many published sources, including the Appendix of this text. Typically,
the events identiﬁed relate to quantiles p ¼ .01, .05, and .10 in the lower tail
and p ¼ .90, .95, and .99 in the upper tail. Of course, the appropriate values of
h and ‘ can be found by solving the following equations given any a‘ and ah
levels of interest ∈(0,1):
Z 1
h
1
2v=2G v
2
  xðv=2Þ1 ex=2 dx ¼ ah
Z ‘
0
1
2v=2G v
2
  xðv=2Þ1 ex=2 dx ¼ a‘
Most modern statistical computer software packages can be used to identify the
appropriate h or ‘, such as STATA, SAS, MATLAB, or the GAUSS programming
languages.
Example 4.14
Using Chi-Square Table
Let X have a w2 density with 10 degrees of freedom. Find the values of h and ‘ for
which P(x  h) ¼ P(x  ‘) ¼ .05.
Answer: In the chi-square table for the row corresponding to 10 degrees of
freedom, the values of h and ‘ associated with the upper and lower .05 tails of
the w2 density are, respectively, 18.307 and 3.940.
□
2
a = P(x ≤  )
a = P(x ≥ h)
x
h
χv
Figure 4.9
Upper and lower a-level
tails of a w2 density
4.2
Parametric Families of Continuous Density Functions
199

4.2.5
Family Name: Beta
Parameterization: (a, b)∈O ¼ {(a,b): a > 0, b > 0}
Density Deﬁnition:
fðx; a; bÞ ¼
1
Bða; bÞ xa1 ð1  x Þb1 Ið0;1Þ ðxÞ
where Bða; bÞ ¼
Ð 1
0 xa1 ð1  x Þb1 dx is called the beta function. Some useful
properties of the beta function include the fact that B(a,b) ¼ B(b,a) and Bða; bÞ
¼ GðaÞGðbÞ
GðaþbÞ
so that the beta function can be evaluated in terms of the gamma
function.
Moments:
m ¼ a=ða þ bÞ;
s2 ¼ ab=½ða þ b þ 1Þða þ b Þ2;
m3 ¼ 2ðb  aÞðabÞ=½ða þ b þ 2Þða þ b þ 1Þða þ b Þ3;
m4
s4  3


¼
6
a þ b
ð
Þ2 a þ b þ 1
ð
Þ  ab a þ b þ 2
ð
Þ
h
i
ab a þ b þ 2
ð
Þ a þ b þ 3
ð
Þ
MGF: MXðtÞ ¼ P1
r¼0 B r þ a; b
ð
Þ=B a; b
ð
Þ
ð
Þ tr=r!
ð
Þ
Background and Application: The beta density is a very versatile density (i.e.,
it can assume a large variety of shapes) for constructing probability spaces for
experiments having a continuous real-valued sample space given by the interval
[0,1]. Table 4.3 provides a quick reference regarding the numerous shape
characteristics of the beta density. The versatility of this density family makes
it useful for representing PDFs for random variables associated with virtually
any experiment whose outcomes constitute a continuum between 0 and 1. The
density has obvious applications in modeling experiments whose outcomes are
in the form of proportions, such as the proportion of time a certain machine is in
a state of being repaired, the proportion of chemical impurities in a liquid
Table 4.3
Summary of beta density shapes
Conditions on a
and b
Behavior of f(x)
a < b
Skewed to the right, m3 > 0
a > b
Skewed to the left, m3 < 0
a ¼ b
Symmetric about m ¼ 1/2
a > 1 and b > 1
Maximum value when x ¼ (a1)/(a + b2) and f(x) ! 0 if x ! 1 or x ! 0
a < 1
f(x) ! 1 as x ! 0
b < 1
f(x) ! 1 as x ! 1
a < 1 and b < 1
f(x) is U–shaped, having minimum value when x ¼ (a1)/(a + b2), and
f(x) ! 1 when x ! 1 or 1
(a1)(b1) < 0
J–shaped
a ¼ b ¼ 1
Uniform on (0,1)
200
Chapter 4
Parametric Families of Density Functions

product, or the proportion of respondents to a survey (a continuous approxima-
tion in the latter case). In Chapter 6 we will also see that the beta density has an
important application in assigning probabilities to events involving so-called
order statistics. Figure 4.10 illustrates some of the beta density shapes that are
possible.
Integration of the density cannot be accomplished in closed form for non-
integer values of the parameters a and b. Procedures are available in most
modern statistical software packages for integrating the beta density. Integrals
of the beta density have also been extensively tabled by Pearson.6 When a and b
are integers, there is a relationship between integrals of the beta density and the
binomial density that can be useful:
FðcÞ ¼
Z c
0
1
B a; b
ð
Þ xa1 ð1  x Þb1 dx ¼
X
n
i¼a
n
i
 
!
ci ð1  c Þni
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
x
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
f(x)
f(x)
f(x)
f(x)
x
a=1/2, b=2
a=3, b=3
a=2, b=3
a=1/2, b=1/2
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
x
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
x
Figure 4.10
Beta densities.
6K. Pearson, (1956), Tables of the Incomplete Beta Function, New York: Cambridge Univ. Press.
4.2
Parametric Families of Continuous Density Functions
201

where n ¼ a + b  1 and c ∈[0,1]. In any case, when a and b are integers,
integration of the beta density can be accomplished in closed form. The follow-
ing example illustrates the point.
Example 4.15
Probabilities of
Proportion Outcomes
A wholesale distributor of heating oil has a storage tank that holds the
distributor’s inventory of heating oil. The tank is ﬁlled every Monday morning.
The wholesaler is interested in the proportion of the tank’s capacity that remains
in inventory after the weekly sales of heating oil. The remaining proportion can
be viewed as the outcome of a random variable having the beta density with a ¼ 4
and b ¼ 3. What is the probability that less than 20 percent of the storage capacity
of the tank remains in inventory at the end of any work week?
Answer:
Pðx<:20Þ ¼
Z :2
0
1
B 4; 3
ð
Þ x3 ð1  x Þ2 dx ¼
Gð7Þ
Gð4ÞGð3Þ
Z :2
0
x3  2x4 þ x5


dx
¼ 6!
3!2!
x4
4  2 x5
5
þ x6
6
	

 
:2
0
¼ :01696:
□
4.2.6
Family Name: Logistic
Parameterization: (u,s) ∈O ¼ {(u,s): 1 < u < 1, s > 0}
Density Deﬁnition: f x; u; s
ð
Þ ¼
e xu
ð
Þ=s
s 1þe xu
ð
Þ=s
ð
Þ2
Moments: m ¼ u; s2 ¼ p2s2
3 ; m3 ¼ 0 ,
m4
s4  3


¼ 6
5
MGF: MXðtÞ ¼ eutB 1  st; 1 þ st
ð
Þ for st
j
j<1 where B a; b
ð
Þ ¼ GðaÞGðbÞ
G aþb
ð
Þ
is the beta
function expressed in terms of the gamma function G ð Þ.
Background and Application: The logistic distribution has been applied in a
wide range of ﬁelds to deﬁne probably models for growth and diffusion processes,
as well as to model decision making processes where decision makers are
faced with discrete alternative choices exhibiting differing characteristics.
Some speciﬁc applications include how various populations of species grow in
competition with each other, for characterizing the spread of epidemics, describing
how learning evolves, modeling how technologies diffuse and are substituted for
one another, the diffusion process relating to new product sales, and diffusion and
substitution between various energy sources.
In economic applications in particular, the distribution has appeared promi-
nently in describing the probabilities associated with dichotomous or binary
choices and has been widely used in logistic regression models of the choices of
decision makers in a wide variety of problems involving consumers’, producers’,
and policy makers’ decisions. Especially for this purpose, the logistic distribu-
tion has often been chosen in place of the normal distribution for deﬁning
probability models of discrete choices processes because of its tractability
(e.g., closed form integrable), but also because of its similarity (symmetric,
bell-shaped) with the normal distribution. In fact, the two distributions are
very similar in shape, with the logistic distribution exhibiting slightly fatter
tails than does the normal distribution.
202
Chapter 4
Parametric Families of Density Functions

Regarding the graphs of the logistic distribution, the reader can refer to the
normal distribution that will be discussed in the section ahead for similar
behavior in terms of shapes, and the changes that occur as the mean and scale
parameters are altered. The following example provides a simple illustration of
how the logistic distribution arises within a simple binary choice decision
context.
Example 4.16
Binary Choice and the
Logistic Distribution
Let Ui ¼ x0ib þ ei represent the utility (or net beneﬁt) that person i, with a vector
of personal characteristics xi, obtains from taking an action (as opposed to not
taking the action). The person takes the action if Ui>0. The unobserved residual
term,
ei ,
is
assumed
to
have
a
standard
logistic
distribution7
with
s ¼ 1 and u ¼ 0:8
Then the decision process is represented as follows, where yi ¼ 1 denotes
that the individual takes the action, while yi ¼ 0 indicates that the individual
will not take the action:
yi ¼
1
0


if Ui
>



0
where P yi ¼ 1
ð
Þ ¼ P ei>  x0ibÞ
ð
¼ 1Fei x0ib
ð
Þ ¼
1
1þexp x0ib
ð
Þ .
□
4.3
The Normal Family of Densities
The normal family of densities is the most extensively used continuous density
in applied statistics and it is for that reason that we devote an entire section to it.
We will begin our discussion of the Normal family by examining the univariate
case, and then we will proceed to the multivariate normal density function.
4.3.1
Family Name: Univariate Normal
Parameterization: (a, b) ∈O ¼ {(a, b): a∈(1,1), b > 0}
Density Deﬁnition: fðx; a; bÞ ¼
1ﬃﬃﬃﬃ
2p
p
b exp  1
2
xa
b

2
h
i
:
Moments: m ¼ a; s2 ¼ b2; m3 ¼ 0,
m4
s4  3


¼ 0
MGF: MX(t) ¼ exp[at + (1/2)b2t2]
7There are substantive conceptual considerations underlying binary choice situations that naturally lead to the use of the logistic
distribution in this context. See Train, K. (1986). Qualitative Choice Analysis: Theory, Econometrics, and an Application to
Automobile Demand, MIT Press, Chapter 2.
8Note that this “normalization” of the logistic distribution in the current model context can be done without loss of generality since
the probabilities of the decision events are unaffected thereby. This has to do with the concept of “parameter identiﬁcation” in
probability models, which we will discuss in later chapters when we examine statistical inference issues.
4.3
The Normal Family of Densities
203

Background and Applications: The univariate normal family of densities is
indexed by two parameters a and b. Given the special relationship between
these parameters and the mean and variance of the density, the normal density
function is usually represented alternatively as
fðx; m; sÞ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s
exp  1
2
x  m
s

2
	

;
and the moment-generating function of the normal density is given by
MX ðtÞ ¼ exp mt þ 1
2 s2 t2
	

The abbreviation N(z; m, s2) is often used to signify that the random variable Z
has a normal distribution with mean m and variance s2. When the random
variable being referred to is not ambiguous, the abbreviation is often shortened
to N(m, s2). Once the mean and the variance of a normal distribution are numeri-
cally speciﬁed, then a unique member of the family of density functions is
identiﬁed. The speciﬁc member of the family for which m ¼ 0 and s2 ¼ 1 is
very important in applied statistics and is given the special name of the standard
normal density or the standard normal distribution.
The normal density is symmetric about its mean, m, has points of inﬂection
at m  s and m + s, and has a characteristic bell shape. The bell becomes more
spread out as the variance increases. We illustrate the general characteristics of a
normal density in Figures 4.11 and 4.12.
m2
m1
m3
x
Figure 4.11
Behavior of normal
densities for ﬁxed s,
m3 > m2 > m1.
x
s1
s2
m
Figure 4.12
Behavior of normal
densities for ﬁxed m,
s2 > s1.
204
Chapter 4
Parametric Families of Density Functions

A very useful property of any normally distributed random variable is that it
can be easily transformed into a random variable having the standard normal
density.
Theorem 4.6
Let X have the density N(x; m, s2). Then Z ¼ (X  m)/s has the standard normal
density N(z; 0,1).
Proof
The MGF of Z is deﬁned by
MZ ðtÞ ¼ E etZ


¼ E exp t X  m
ð
Þ
s




¼ etm=s MX
t
s
 
ðTheorem 3:27Þ
¼ etm MXðtÞ
substitute t ¼ t
s


and since MX(t*) ¼ exp(mt* + (1/2) s2 t2
), it follows that
MZ ðtÞ ¼ exp tm
ð
Þ exp mt þ 1=2
ð
Þ s2 t2



¼ exp t2=2


:
By the MGF uniqueness theorem, since the MGF of Z is that of a standard normal
density, Z has a N(z; 0,1) density. (Note that exp(t2/2) ¼ exp(mt + (1/2) s2t2)
with m ¼ 0 and s2 ¼ 1).
n
In applications, Theorem 4.6 implies that the probability of an event A,
PX(A), for a random variable X having a normal density N(x; m,s) is equal to the
probability PZ(B) of the equivalent event B ¼ {z: z ¼ (x  m)/s, x ∈A}, for a
standard normal random variable Z. This accounts for the prevalence of
published tables of the standard normal CDF since, in principle, the standard
normal distribution is sufﬁcient to assign probabilities to all events involving
normally distributed random variables.
The operation of subtracting the mean from a random variable, and then
dividing by its standard deviation, is referred to as standardizing a random
variable.
Deﬁnition 4.2
Standardizing a
Random Variable
A random variable is standardized by subtracting its mean and then dividing
the result by its standard deviation, as Z ¼ XuX
sX .
The outcome value of a standardized random variable can be interpreted as a
measure of the distance of the outcome from its mean measured in standard
deviation units, (e.g., z ¼ 3 would mean the outcome of Z was three standard
deviations from its mean). Thus, if a random variable having a normal density is
standardized, the standardized random variable has a standard normal density.
The random variable having the density N(0,1) is often referred to as a standard
normal random variable.
Example 4.17
Using Standard Normal
Distribution to Assign
Probabilities
The miles per gallon (mpg) achieved by a new pickup truck produced by a Detroit
manufacturer can be viewed as a random variable having a normal density with a
mean of 17 mpg and a standard deviation of .5 mpg. What is the probability that a
new pickup will achieve between 16 and 18 mpg?
4.3
The Normal Family of Densities
205

Answer: Let X have the density N(x; 17, .25). Then,
Pð16  x  18Þ ¼ P 16  17
:5
 x  17
:5
 18  17
:5


¼ Pð2  z  2Þ
¼ Fð2Þ  Fð2Þ ¼ :9772  :0228 ¼ :9544;
where F() is the CDF of the standard normal random variable.
□
An important observation should be made concerning the application of the
normal density function in Example 4.17. Note that miles per gallon cannot be
negative, and yet the normal density assigns nonzero probability to the event
that x < 0, that is, P(x < 0) ¼
Ð 0
1 N(x; m, s2) dx > 0 8 m and s2 > 0. This is, in
fact, illustrative of a situation that arises frequently in practice where the
normal distribution is used in the construction of a probability space for an
experiment whose outcomes assume only nonnegative values. The empirical
justiﬁcation for this apparent misuse of the normal density is that P(x < 0)
should be negligible in these cases, given the relevant values of m and s2, in
which case the anomaly of P(x < 0) > 0 can be ignored for all practical purposes.
In Example 4.17, P(x < 0) < 1 
 1010.
There is a relationship between standard normal random variables and the
w2 density that is very important for developing hypothesis-testing procedures,
discussed in later chapters. We develop this relationship in the following two
theorems.
Theorem 4.7
Relationship Between
Standard Normal
and Chi-Square
If X has the density N(0,1), then Y ¼ X2 has a w2 density with 1 degree of
freedom.
Proof
The MGF of Y is deﬁned as
MY ðtÞ ¼ E eYt


¼ E expðX2tÞ


¼
Z 1
1
expðx2tÞ
1ﬃﬃﬃﬃﬃﬃ
2p
p
exp  1
2 x2


dx
¼
Z 1
1
1ﬃﬃﬃﬃﬃﬃ
2p
p
exp  1
2 x2 ð1  2tÞ


dx
¼ ð1  2t Þ1=2
Z 1
1
1
ﬃﬃﬃﬃﬃﬃ
2p
p
ð1  2t Þ1=2 exp
 1
2
x
ð1  2t Þ1=2
 
!2
0
@
1
Adx
¼ ð1  2t Þ1=2
Z 1
1
N x; 0; 1  2t
ð
Þ1


dx
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
1
¼ 1  2t
ð
Þ1=2for t< 1
2 :
Therefore, by the MGF uniqueness theorem, Y ¼ X2 has a w2 density with
1 degree of freedom.
n
206
Chapter 4
Parametric Families of Density Functions

Theorem 4.8
Sums of Squares of
Independent Standard
Normal Random
Variables is Chi-Square
Let (X1,. . .,Xn) be independent random variables, each having the density
N (0,1). Then Y ¼ Pn
i¼1 X2
i has a w2 density with n degrees of freedom.
Proof
The random variables X2
i , i ¼ 1,. . .,n, are independent by Theorem 2.9. Then,
from Theorems 3.27 and 4.7,
MY ðtÞ ¼
Y
n
i¼1
MX2
i ðtÞ ¼
Y
n
i¼1
ð1  2t Þ1=2 ¼ ð1  2t Þn=2 for t< 1
2;
which by the MGF uniqueness theorem implies that Y has a w2 density with n
degrees of freedom.
n
In words, Theorem 4.8 is often stated as the sum of squares of n independent
standard normal random variables has a x2 density with n degrees of freedom.
We can now motivate why the parameter v in a w2 density is labeled degrees
of freedom. In particular, v represents the number, or degree, of freely varying
(i.e., independent) standard normal random variables whose sum of squares
represents a w2
v-distributed random variable.
It will be useful in our later study of hypothesis testing to know the values of
h and ‘ (i.e., know the appropriate quantiles) for which the following statements
are true:
Pðx  hÞ ¼ a ¼ 1  FðhÞ and Pðx  ‘Þ ¼ a ¼ Fð‘Þ
where F() refers to the CDF of a random variable having a N(m,s2) density and
a ∈(0,1). We diagrammatically examine the upper and lower tails of the normal
family of densities in Figure 4.13. The probabilities of such events are extensively
tabled for the case of the standard normal density, where typically the values of the
cumulative standard normal density, F(c), are given for numerous choices of c (see
Appendix). The probabilities of these events for arbitrary normal distributions can
be
obtained
from
knowledge
of
the
standard
normal
distribution
via
standardization. For the event x  ‘, Pðx  ‘Þ ¼ P xm
s
 ‘m
s


¼ Pðz  ‘Þ, where
‘* ¼ ‘  m
ð
Þ=s, and z ¼ x  m
ð
Þ=s can be interpreted as the outcome of a standard
normal random variable using Theorem 4.6. Thus, the value of P(x  ‘) can be
h
α=P(x≤  )
α=P(x≥h)
Figure 4.13
Upper and lower
a-level tails of N(m,s2).
4.3
The Normal Family of Densities
207

obtained from the standard normal table as equal to the probability that z  ‘* ¼
‘  m
ð
Þ=s. Similarly, Pðx  hÞ ¼ P
xm
s
 hm
s


¼ Pðz  hÞ where h ¼ h  m
s
:
Example 4.18
Using Standard
Normal Table
Find the probability that x  5.29 where X is a random variable having a normal
distribution with mean 2 and variance 4.
Answer: Note that
Pðx  5:29Þ ¼ P x  2
2
 5:29  2
2


¼ Pðz  1:645Þ ¼ :05
which was found from the table of the standard normal CDF.
□
Modern statistical software packages contain procedures that willnumerically
integrate the standard normal density over any interval (1, c] chosen by the user.
The normal density is often used in modeling experiments for which a
symmetric, bell-shaped probability density is suspected. The normal density
has been found to be a useful representation of event probabilities for literally
thousands of real-world experiments, which is attributable in large part to the
fact that, under general conditions, certain useful functions of a collection of
independent random variables, such as sums and averages, have approximately
normal densities when the collection is large enough, even if the original ran-
dom variables in the collection do not have normal densities. In fact, the normal
density was originally discovered by A. de Moivre (1667–1745) as an approxima-
tion to the binomial density (recall that the binomial density applies to the sum
of independent Bernoulli random variables). These results are based on central
limit theorems which will be examined in Chapter 5.
Examples of applications include ﬁll weights of food and beverage
containers, employee aptitude test scores, labor hours required to construct
prefabricated homes, numbers of insurance claims ﬁled during a time period,
and weight gains of meat animals in a feedlot. In Chapter 10 we will examine
statistical tests that can be used to assess the validity of the normal PDF for
characterizing event probabilities in a given real-world experiment.
4.3.2
Family Name: Multivariate Normal Density
Parameterization:
a ¼ a1; . . . ; an
ð
Þ0 and B ¼
b11
. . .
b1n
..
.
..
.
..
.
bn1
. . .
bnn
2
64
3
75
(a,B) ∈O ¼ {(a,B):a ∈ℝn, B is a symmetric (n 
 n) positive deﬁnite matrix}.
Density Deﬁnition:
f x; a; B
ð
Þ ¼
1
ð2p Þn=2 jB j1=2 exp
 1
2 ðx  aÞ0 B1 ðx  aÞ
	

208
Chapter 4
Parametric Families of Density Functions

Moments:
m
ðn
1Þ
¼ a;
CovðXÞ ¼ B
ðn
nÞ
;
m3
ðn
1Þ
¼ 0;
m4 Xi
ð
Þ
s4 Xi
ð
Þ  3


¼ 0 8i
MGF: MX(t) ¼ exp (a0t + (1/2) t0Bt), where t ¼ (t1,. . .,tn)0.
Background and Application: The n-variate normal family of densities is
indexed by n + n(n + 1)/2 parameters consisting of n elements in the (n 
 1)
vector a and n(n + 1)/2 elements representing the distinct elements in the
(n 
 n) symmetric matrix B (while there are n2 number of elements in the
matrix B, only n(n + 1)/2 of these elements are distinct (or different) given the
symmetry of B, i.e., bij ¼ bji 8i and j). Given the special relationship between the
mean vector, the covariance matrix, and the parameters a and B, the n-variate
normal density is most often represented as
N x; m; S
ð
Þ ¼
1
ð2p Þn=2 jS j1=2 exp  1
2 ðx  mÞ0 S1 ðx  mÞ
	

where S is a popular notation for the covariance matrix of X (The problem
context will have to be relied upon to distinguish between when S designates
a covariance matrix, and when S signiﬁes summation). When it is clear which
random vector is being referred to, the notation N(x;m,S) is often shortened to
N(m,S). The MGF of X is represented by Mx(t) ¼ exp(m0t + .5 t0S t).
In order to illustrate graphically some of the characteristics of the multivar-
iate normal density, we temporarily concentrate on the bivariate case. Thus, in
the above formula, n ¼ 2, m is a (2 
 1) column vector, and S is a (2 
 2) positive
deﬁnite covariance matrix. The graph of the bivariate normal density is a three-
dimensional bell of sorts, such as the illustration in Figure 4.14a. The mode
of the normal density occurs at x ¼ m, which in the bivariate case occurs at
x1 ¼ E(X1) and x2 ¼ E(X2). Iso-density contours (i.e., the collection of (x1, x2)
points resulting in a ﬁxed value of the joint density function, as f(x;m,S) ¼ c, are
in the form of ellipses (ellipsoids in higher dimensions, e.g., a “football” in three
dimensions) with center at m, so that in the bivariate case the center of the
ellipse is at (E(X1), E(X2)). One can think of these ellipses as being formed by
“slicing” through the density at a certain height, removing the top portion of the
density, and then projecting the exposed elliptical top onto the (x1,x2)-plane
(see Figure 4.14b).
The shape and orientation of the ellipse (or ellipsoid) are determined by the
elements of the covariance matrix, S. The major (larger) axis of the ellipse as
measured from the origin, m, is in the direction of the characteristic vector of S1
associated with the smallest characteristic root of S1.9 The length of an axis is
9Recall that the characteristic roots and vectors of a square matrix A are the scalars, l, and associated vectors, p, that satisfy the
equation [A  lI] p ¼ 0. There will be as many roots and associated vectors as there are rows (or columns) in the square matrix A.
4.3
The Normal Family of Densities
209

given by 2k=
ﬃﬃﬃﬃli
p , where k ¼
ﬃﬃﬃﬃﬃﬃ
2p
p
jSj:5 c, c is the chosen value of the density, and li
is either the smallest or largest characteristic root of S1.10
x2
x1
(E(X1),E(X2))
a
b
Figure 4.14
Bivariate normal density;
(a) view of normal density
in three-dimensions, and
(b) an iso-density ellipse.
10These arguments extend in a natural way to higher dimensions, in which case we are examining n axes of the ellipsoid. See B. Bolch,
and C. Huang (1974), Multivariate Statistical Methods for Business and Economics, Englewood Cliffs, NJ: Prentice Hall, p. 19–23, for
the matrix theory underlying the derivation of the results on axis length and orientation discussed here.
210
Chapter 4
Parametric Families of Density Functions

In the bivariate case, the slope, dx1/dx2, of the major axis of the ellipse is
positive if s12 > 0 and negative if s12 < 0. The slope increases in absolute value
as the ratio of the standard deviation s1/s2 increases, holding r (the correlation
coefﬁcient) constant. As |s12| ! s1s2, so that r ! 	1, the length of the minor
axis of the ellipse ! 0, and the ellipse concentrates on the major axis, which
approaches the line x1 ¼ E X1
ð
Þ 	 s1=s2
ð
Þ x2  E X2
ð
Þ
ð
Þ (compare to Theorem 3.35
– the orientation of the slope of the line matches the sign of the correlation).
If s1 ¼ s2, the principal axis is given by the line x1 ¼ E(X1) 	 (x2  E(X2)), for any
magnitude of the correlation.
An illustration of some of the myriad of possibilities for the graphs of the
iso-density contours is given in Figure 4.15.
The multivariate normal density is often used in modeling experiments
characterized by a PDF that is symmetric about its mean vector, m, is bell-shaped
45°
s1>s2, r>0
s1<s2, r>0
45°
s1=s2, r>0
s1=s2, r=0
s1=s2, r<0
s1>s2, r=0
45°
45°
x1
x2
x2
x2
x2
x2
x2
x1
x1
x1
x1
x1
Figure 4.15
Iso-density ellipses of
bivariate normal density
(all origins at (E(X1),E(X2))).
4.3
The Normal Family of Densities
211

(when viewed in three or fewer dimensions), and is such that the highest density
weighting (mode) occurs at the mean vector with density values declining as
x becomes more distant from E(X) in any direction. Also, under rather general
conditions, certain important vector functions of a collection of independent
multivariate random variables, such as sums and averages, have approximately
a normal distribution when the collection is large enough, even if the original
multivariate random variables in the collection do not have normal probability
distributions. These results are known as multivariate central limit theorems,
which we will study in Chapter 5 and which accounts for much of the motivation
for the assumption of multivariate normality in empirical work.
4.3.3
Marginal Normal Densities
A useful property of the multivariate normal family is that the marginal density
of any subset of the random variables (X1,. . .,Xn), and the conditional density of
any subset of the random variables (X1,. . .,Xn) given an elementary event for the
remaining random variables, are in the normal family and are easy to identify.
We ﬁrst present an important theorem concerning the PDF of a linear combina-
tion of normally distributed variables. The theorem will greatly facilitate our
examination of marginal and conditional densities, but it is also useful more
generally.
Theorem 4.9
PDF of Linear
Combinations of
Normal Random
Variables
Let X be an n-variate random variable having the density function N(x;m,S).
Let A be any (k 
 n) matrix of real constants with rank k  n, and let b be any
(k 
 1) vector of real constants. Then the (k 
 1) random vector Y ¼ AX + b has
the density Nðy; Am þ b; ASA0Þ:
Proof
The MGF of Y is deﬁned as
MYðtÞ ¼ Eðexpðt0YÞÞ ¼ E ðexpðt0ðAX þ bÞÞÞ
¼ expðt0bÞ E ðexpðt0AXÞÞ ¼ expðt0bÞ expðt0Am þ 1=2
ð
Þt0ASA0tÞ
¼ expðt0ðAm þ bÞ þ 1=2
ð
Þt0ASA0tÞ
where the next-to-last equality follows from the fact that X is normally
distributed, and E(exp(t0AX)) ¼ E(exp(t*0X)) ¼ MX(t*) ¼ exp[t*0m + (1/2)t*0St*],
with t*0 ¼ t0A. Thus, MY(t) identiﬁes the multivariate normal density with
mean Am + b and covariance matrix ASA0 as the density of Y ¼ AX + b.
n
Example 4.19
Probability Density of
Proﬁt and Variable Cost
A ﬁrm uses two variable inputs in the manufacture of an output and also uses
“just-in-time” production methods so that inputs arrive precisely when they are
needed to produce the output. The weekly average input and selling prices, r and
p, during the spring quarter can be viewed as the outcomes of a trivariate normal
density function f(r1,r2,p;m,S) ¼ N(m,S) with
212
Chapter 4
Parametric Families of Density Functions

m ¼
:50
1:25
5
2
4
3
5; and S ¼
:05
:02
:01
:02
:10
:01
:01
:01
:40
2
4
3
5:
What is the density function of the bivariate random variable (P,C), where P
represents proﬁt above variable cost and C represents variable cost, in a week
where 100 units of input 1 and 150 units of input 2 are utilized, and 100 units of
output are produced and sold?
Answer: The random variables P and C can be deﬁned as
Y ¼
P
C
	

¼
100
150
100
100
150
0
	

R1
R2
P
2
4
3
5 ¼ AX þ b
where b ¼ 0 and A is the bracketed (2 
 3) matrix following the second equality
sign. Then, Theorem 4.9 implies that [P, C]0 is bivariate normally distributed as
N(m*,S*), with m* ¼ Am ¼ [262.5
237.5]0 and S ¼ ASA0 ¼
6850 3100
3100
3350
	

: □
A useful implication of Theorem 4.9 for generating or simulating outcomes
of multivariate normally distributed random variables on the computer is that
any such random variable can be represented in terms of linear combinations of
independent random variables having the standard normal density. Speciﬁcally,
if the n 
 1 random variable Z has the PDF N(0,I) so that Z1,. . .,Zn are indepen-
dent N(0,1) random variables, then the n 
 1 random variable Y with PDF
N(m,S) can be represented in terms of Z as Y ¼ m + AZ, where A is chosen so
that A0A ¼ S.11 The utility of this representation stems from the fact that many
modern statistical software packages are capable of generating independent
outcomes of a random variable having the N(0,1) density; such programs are
referred to as “standard normal random-number generators.” We will pursue the
notion of simulating random variable outcomes further in Chapter 6.
We now state an important result concerning the marginal densities of
subsets of the random variables (X1,. . .,Xn) when the n-variate random variable
has an n-variate normal density.
Theorem 4.10
Marginal Densities
for N(m,S)
Let Z have the density N(z; m, S), where
Z ¼
Zð1Þ
ðm
1Þ
Zð2Þ
ðnmÞ
1
2
64
3
75; m ¼
mð1Þ
ðm
1Þ
mð2Þ
ðnmÞ
1
2
664
3
775; and S ¼
S11
ðm
mÞ
S12
ðm
ðnmÞÞ
S21
ððnmÞ
mÞ
S22
ððnmÞ
ðnmÞÞ
2
64
3
75
11The symmetric matrix square root S1/2 of S could be chosen for A. Alternatively, there exists a lower triangular matrix, called the
Cholesky decomposition of S, which satisﬁes A0A ¼ S. Either choice of A can be calculated straightforwardly on the computer, for
example, the chol(.) command in GAUSS.
4.3
The Normal Family of Densities
213

Then the marginal PDF of Z(1) is N(m1, S11) and the marginal PDF of Z(2) is
N(m2, S22).
Proof
Let A ¼
Im
m
m
ð
Þ
j
0
m
 nm
ð
Þ
"
#
and b ¼ 0 in Theorem 4.9, where Im is the (m 
 m)
identity matrix. It follows that Z(1) ¼ AZ has the normal density N(m(1), S11).
The result for Z(2) is proven similarly be letting A ¼
0
nm
ð
Þ
m j
Inm
nm
ð
Þ
 nm
ð
Þ
"
#
.
n
Example 4.20
Marginal Densities for
a Trivariate Normal
Referring to Example 4.19, partition X, m and S as
X ¼
Xð1Þ
Xð2Þ
	

¼
X1
X2
X3
2
4
3
5; m ¼
mð1Þ
mð2Þ
"
#
¼
m1
m2
m3
2
4
3
5 ¼
:50
1:25
5
2
4
3
5
and
S ¼
S11
S12
S21
S22
	

¼
:05
:02
:01
:02
:10
:01
:01
:01
:40
2
4
3
5:
It follows from Theorem 4.10 that the marginal densities of (X1,X2) and X3 are
given by
fX1X2ðx1; x2Þ ¼ N
:50
1:25
	

;
:05
:02
:02
:10
	



; and fX3ðx3Þ ¼ Nð5; :4Þ:
□
The reader should note that the order of the random variables in the X vector
is arbitrary. For example, we might have X ¼ (X1, X2, X3, X4)0 or alternatively, the
same random variables might be listed as X ¼ (X3, X1, X4, X2)0. The point is that
Theorem 4.10 can be applied to obtain the marginal density function of any
subset of the random variable (X1,. . .,Xn) by simply ordering them appropriately
in the deﬁnition of Z in the theorem. Of course, the entries in m and S must be
correspondingly ordered so that random variables are associated with their
appropriate means, variances, and covariances.
Example 4.21
Marginal Normal
Density via Reordering
The annual percentage return on three investment instruments is the outcome
of a trivariate random variable (X1, X2, X3)0 having the density N(m, S), where
m ¼
2
7
1
2
64
3
75and S ¼
4
1
0
1
1
1
0
1
3
2
4
3
5:
214
Chapter 4
Parametric Families of Density Functions

To identify the marginal density of the returns on investments 1 and 3 using
Theorem 4.10, ﬁrst reorder the random variables as X ¼ (X1, X3, X2)0 so that
the corresponding mean vector and covariance matrix of the trivariate normal
density of X are now
m ¼
2
1
7
2
4
3
5; and S ¼
4
0
1
0
3
1
1
1
1
2
4
3
5:
Then a straightforward application of Theorem 4.10 (with the appropriate inter-
pretation of the symbols Z(1) and Z(2) used in that theorem) implies that (X1, X3)0
has the density N
2
1
	

;
4
0
0
3
	



:
□
4.3.4
Conditional Normal Densities
Deﬁning conditional densities for n-variate normally distributed random
variables is somewhat more involved than deﬁning marginal densities. We
present the case of conditioning on an elementary event for a subset of the
random variables.
Theorem 4.11
Conditional Densities
for N(m,S)
Let Z be deﬁned as in Theorem 4.10, and let
z0
n
1
ð
Þ ¼
z0
ð1Þ
m
1
ð
Þ
z0
ð2Þ
nm
ð
Þ
1
2
664
3
775
be a vector of constants. Then
fðzð1Þ jzð2Þ ¼ z0
ð2ÞÞ ¼ Nðmð1Þ þ S12 S1
22 ðz0
ð2Þ  mð2ÞÞ; S11  S12 S1
22 S21Þ
fðzð2Þ j zð1Þ ¼ z0
ð1ÞÞ ¼ Nðmð2Þ þ S21 S1
11 ðz0
ð1Þ  mð1ÞÞ; S22  S21 S1
11 S12Þ
Proof
We prove the result for the case of Z(1); the case for Z(2) can be proved analo-
gously. By deﬁnition,
f zð1Þjzð2Þ ¼ z0
ð2Þ


¼
f zð1Þ;z0
ð2Þ


fzð2Þ z0
ð2Þ


¼
1
2p
ð
Þn=2 S
j j1=2 exp
 1
2
zð1Þ mð1Þ
z0
ð2Þ mð2Þ
2
4
3
5
0
S1
zð1Þ mð1Þ
z0
ð2Þ mð2Þ
2
6664
3
7775
0
B
B
B
@
1
C
C
C
A
1
2p
ð
Þ nm
ð
Þ=2 S22
j
j1=2 exp  1
2 z0
ð2Þ mð2Þ

0
S1
22
ð
Þ z0
ð2Þ mð2Þ




4.3
The Normal Family of Densities
215

The following lemma on partitioned determinants and partitioned inversion
will be useful here.
Lemma 4.3
Partitioned Inversion
and Partitioned
Determinants
Partition the (n 
 n) matrix S as
S ¼
S11
ðm
mÞ
S12
m
ðnmÞ
S21
ðnmÞ
m
S22
ðnmÞ
ðnmÞ
2
4
3
5
a. If S11 is nonsingular, then S
j j ¼ S11
j
j  S22  S21S1
11 S12

.
b. If S22 is nonsingular, then S
j j ¼ S22
j
j  S11  S12S1
22 S21

.
c. If |S| 6¼ 0,|S11| 6¼ 0, and |S22| 6¼ 0, then
S1 ¼
ðS11 S12 S1
22 S21Þ1
ðS11 S12 S1
22 S21 Þ1 S12 S1
22
S1
22 S21 ðS11 S12 S1
22 S21Þ1
ðS22 S21 S1
11 S12 Þ1
"
#
d. The diagonal blocks in the partitioned matrix of part (c) can also be
expressed as
S11  S12 S1
22 S21

1
¼ S1
11 þ S1
11 S12
S1
22  S21 S1
11 S12

1
S21 S1
11
and
S22  S21 S1
11 S12

1
¼ S1
22 þ S1
22 S21
S1
11  S12 S1
22 S21

1
S12 S1
22 :
(see F.A. Graybill (1983), Matrices with Applications in Statistics, 2nd
Ed., Belmont, CA: Wadsworth, pp. 183–186, and H. Theil, Principles of
Econometrics, John Wiley and Sons, New York, pp. 16–19 for further
discussion and proofs).
Utilizing Lemma 4.3, note that
S22
j
j1=2
S
j j1=2 ¼ S11  S12S1
22 S21


1=2
and
exp  1
2
zð1Þ  mð1Þ
z0
ð2Þ  mð2Þ
2
4
3
5
0
S1
zð1Þ  mð1Þ
z0
ð2Þ  mð2Þ
2
4
3
5 þ 1
2 z0
ð2Þ  mð2Þ

0
S1
22 ðz0
ð2Þ  mð2ÞÞ
2
4
3
5
¼ exp
"
 1
2 zð1Þ 
mð1Þ þ S12 S1
22
z0
ð2Þ  mð2Þ





0
F1

zð1Þ 
mð1Þ þ S12 S1
22
z0
ð2Þ  mð2Þ





#
216
Chapter 4
Parametric Families of Density Functions

where F ¼ S11  S12S1
22 S21
h
i
. Then the expression for the conditional density
function reduces to
f zð1Þ j zð2Þ ¼ z0
ð2Þ


¼
1
ð2p Þm=2 jF j1=2 exp  1
2 ðzð1Þ gÞ0 F1 ðzð1Þ gÞ
	

;
where g ¼ m(1) + S12S1
22 (z(2)
0m2), which is the normal density N(g,F).
n
Example 4.22
Conditional Densities
for a Trivariate Normal
Let X ¼ (X1, X2, X3) be the trivariate normal random variable representing the
percentage returns on investments in Example 4.21. Suppose we wanted to
deﬁne the conditional density of returns on investment instrument 1, given
x2 ¼ 1 and x3 ¼ 2. By letting z(1) ¼ x1 and z(2) ¼ (x2, x3)0 in Theorem 4.11,
we know that X1 will have a normal density with mean
Eðx1 j x2 ¼ 1; x3 ¼ 2Þ ¼ 2 þ 1
0
½

1
1
1
3
	

1
6
1
	

¼ 7:5
and variance
s2
ðX1 j x2¼1;x3¼2Þ ¼ 4  1
0
½

1
1
1
3
	

1
1
0
	

¼ 2:5;
so that the conditional density of X1 is f(x1|x2 ¼ 1, x3 ¼ 2) ¼ N(7.5, 2.5).
To ﬁnd the conditional density of returns on investments 1 and 2 given
x3 ¼ 0, Theorem 4.11, with z(1) ¼ (x1, x2)0 and z(2) ¼ x3, implies that the mean
vector is equal to
E
X1
X2
"
#
j x3 ¼ 0
 
!
¼
2
7
"
#
þ
0
1
"
#
½3 1 ½1 ¼
2
20=3
"
#
;
and the covariance matrix is
CovððX1; X2Þj x3 ¼ 0Þ ¼
4 1
1 1
"
#

0
1
"
#
½3 1 ½0 1 ¼
4 1
1 2=3
"
#
:
□
In the special case where X1 is a scalar and X2 is a (k 
 1) vector, the
conditional expectation of X1 expressed as a function of x2 deﬁnes the regression
function of X1 on X2, which in this case, given the linearity of the relationship,
can also be referred to as the regression hyperplane of X1 on X2. If X2 happens to
be a scalar, the regression hyperplane is a regression line. The regression hyper-
plane, in the multivariate normal case, is thus given by
E X1j x2
k
1
ð
Þ
 
!
¼ mð1Þ þ S12 S1
22 ðxð2Þ  mð2ÞÞ¼ a þ b0xð2Þ
where b0¼ S12 S1
22 and a ¼ m(1)  b0m(2). The special case of the regression line
can be written as
E X1jx2
ð
Þ ¼ m1 þ r s1
s2
ðx2  m2Þ¼ a þ bx2;
4.3
The Normal Family of Densities
217

where r is the correlation coefﬁcient, b ¼ rs1/s2, and a ¼ m1  bm2. Figure 4.16
illustrates the regression line of X1 on X2.
The particular functional form of the normal density is the reason why the
regression function becomes a regression hyperplane, and the regression curve
becomes a regression line. In fact, the normal density belongs to a collection of
families of distributions referred to as elliptically contoured distributions, all of
which are associated with linear regression functions (S. Cambanis, et al., (1981),
On the Theory of Elliptically Contoured Distributions, Journal of Multivariate
Analysis, 11, p. 368). No other class of distributions exhibits linear regression
functions as a general property.
While not true in general, in the case of a normal distribution, zero covari-
ance implies independence of random variables, as stated in the following
theorem.
Theorem 4.12
Diagonal Cov(X) ⟹
Independence
when X has PDF N(m,S)
Let X ¼ (X1,. . .,Xn) have the density N(m,S). Then (X1,. . .,Xn) are independent iff
S is a diagonal matrix.
Proof
The only if (necessary) part of the theorem follows immediately from the fact
that independence of random variables implies zero covariance. To see the if
(sufﬁcient) part, suppose S is a diagonal matrix. Then the density of X can be
written as
Nðx; m; SÞ ¼
1
ð2p Þn=2 jS j1=2 exp  1
2 ðx  mÞ0 S1 ðx  mÞ
	

¼
Y
n
i¼1
1
ð2p Þ1=2 si
exp  1
2
ðxi  mi Þ2
s2
i
"
#
¼
Y
n
i¼1
Nðxi; mi; s2
i Þ;
x2=b
x1
x2
(x2 −E(X2))
E(X1|x2)=E(X1)+ ρσ
σ2
1
(E(X1), E(X2))
f(x1|x2=b)
f(x1|x2)
Figure 4.16
Regression line of X1 on X2.
218
Chapter 4
Parametric Families of Density Functions

since
S
j j ¼
Y
n
i¼1
s2
i ; and S1 ¼
s2
1
..
.
s2
n
2
64
3
75:
Being that the joint density factors into the product of the n marginal densities,
(X1,. . .,Xn) are jointly independent.
n
A variation on the preceding theme is the case where two vectors X(1) and
X(2) are independent by virtue of the covariances between the elements in X(1)
and the elements in X(2) all being zero.
Theorem 4.13
Block Diagonal
Cov (X) , Independent
Subvectors when X
has PDF N(m,S)
Let
Z
ðn
1Þ ¼
Zð1Þ
ðm
1Þ
Zð2Þ
ðnmÞ
1
"
#
have the multivariate normal density identiﬁed in Theo-
rem 4.10. Then the vectors Z(1) and Z(2) are independent iff S12 ¼ S0
21 ¼ 0.
Proof
The only if part of the theorem follows immediately from the fact that the
independence of random variables implies zero covariance. To see the if part,
suppose S12 ¼ S0
21 ¼ 0. Then the PDF of Z can be written as
Nðz; m; SÞ ¼
1
ð2p Þn=2 jS j1=2 exp  1
2 ðz  mÞ0 S1 ðz  mÞ
	

¼
1
ð2p Þm=2 j S11 j1=2 exp  1
2 ðzð1Þ  mð1ÞÞ0 S1
11 ðzð1Þ  mð1ÞÞ
	


1
ð2p ÞðnmÞ=2 j S22 j1=2 exp  1
2 ðzð2Þ  mð2ÞÞ0 S1
22 ðzð2Þ  mð2ÞÞ
	

¼
Y
2
i¼1
N zðiÞ; mðiÞ; Sii


because
S
j j ¼
S11
0
0
S22

 ¼ S11
j
j  S22
j
j and S1 ¼
S1
11
0
0
S1
22
"
#
Given that the joint density of Z factors into the product of the marginal
densities, Z(1) and Z(2) are independent random vectors.
n
4.3
The Normal Family of Densities
219

4.4
The Exponential Class of Densities
The majority of the discrete and continuous density function families that we
have examined in this chapter are special cases of the exponential class of
density functions.12 We will see in later chapters that problems of statistical
inference involving experiments having probability spaces that involve density
families from the exponential class are often easier to analyze regarding the
design of statistical estimation and inference procedures.
Deﬁnition 4.3
Exponential Class
of Densities
The density function f(x;Q) is a member of the exponential class of density
functions iff
fðx;QÞ ¼
exp Pk
i¼1 ci Q
ð
ÞgiðxÞ þ d Q
ð
Þ þ zðxÞ


for x 2 A;
0
otherwise,
(
where x ¼ (x1,. . .,xn)0, Q ¼ (Y1,. . ., Yk)0; ci(Q), i ¼ 1,. . .,k, and d(Y) are real-
valued functions of Q that do not depend on x; gi(x), i ¼ 1,. . .,k, and z(x) are
real-valued functions of x that do not depend on Q; and A  ℝn is a set of n-
tuples contained in n-dimensional real space whose deﬁnition does not
depend on the parameter vector Y.
In order to check whether a given density family belongs to the exponential
class, one must determine whether there exists deﬁnitions of ci(Q), d(Q), gi(x),
z(x) and A such that the density can be equivalently represented in the exponen-
tial form presented in the Deﬁnition 4.2. Of the densities we have studied in this
chapter, the Bernoulli, binomial and multinomial (for known values of n),
negative binomial (for known values of r), Poisson, geometric, gamma (including
exponential and chi-square), beta, univariate normal, and multivariate normal
density families all belong to the exponential class of densities. The discrete and
continuous uniform, the hypergeometric density, and the logistic families are
not members of the exponential class.
We now present a number of examples that illustrate how membership in
the exponential class of densities can be veriﬁed. In general, there is no standard
method of verifying whether a density family belongs to the exponential class,
and so the veriﬁcation process must rely on the ingenuity of the analyst.
12We warn the reader that some authors refer to this collection of densities as the exponential family rather than the exponential class.
The collection of densities referred to in this section is a broader concept than that of a parametric family, and nests a large number of
probability density families within it, as noted in the text. We (and others) use the term class to distinguish this broader density
collection from that of a parametric family, and to also avoid confusion with the exponential density family discussed in Section 4.2.
220
Chapter 4
Parametric Families of Density Functions

Example 4.23
Normal Family 
Exponential Class
Univariate Case. Let k ¼ 2, n ¼ 1, and deﬁne
c1 ðQÞ ¼ m
s2 ;
c2 ðQÞ ¼  1
2 s2 ;
g1 ðxÞ ¼ x;
g2 ðxÞ ¼ x2;
dðQÞ ¼  1
2
m2
s2 þ lnð2p s2Þ


;
zðxÞ ¼ 0;
A ¼ R
Substitution into Deﬁnition 4.2 yields
fðx; QÞ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s
exp  1
2
x  m
s

2
	

;
which is the univariate normal density.
Multivariate Case. Given an n-variate case, let k ¼ n + (n(n + 1)/2), and note
that the multivariate normal density can be written as
Nðx; m; SÞ ¼
1
ð2p Þn=2 jS j1=2 exp 1
2ðx  mÞ0 S1 ðx  mÞ
	

¼
1
ð2p Þn=2 jS j1=2 exp 1
2 Sn
i¼1 x2
i S1
ii
ð Þ þ 2 SS
i<j xi xj S1
ij
ð Þ  2m0S1x þ m0S1m


	

 ;
where S1
ðijÞ refers to the (i,j)th entry in the S1 matrix. Deﬁne
c Q
ð
Þ0
1
k
ð
Þ
¼  m0S1j1
2S1
11
ð
Þ; S1
12
ð
Þ; . . . ; S1
1n
ð
Þ; 1
2S1
22
ð
Þ; S1
23
ð
Þ; . . . ; S1
2n
ð
Þ; . . . ; 1
2S1
nn
ð
Þ


;
gðxÞ0
1
k
ð
Þ
¼ x0jx2
1; x1x2; . . . ; x1xn; x2
2; x2x3; . . . ; x2xn; . . . ; x2
n


;
d Q
ð
Þ ¼ 1
2 m0S1m þ ln 2p
ð
ÞnjSj


;
zðxÞ ¼ 0;
A ¼ Rn
Then it follows that exp[c(Q)0g(x) + d(Q) + z(x)] ¼ N(x; m, S).
□
Example 4.24
Bernoulli Family  Exponential Class
Let k ¼ 1, n ¼ 1, and deﬁne
c1 Y
ð
Þ ¼ ln p= 1  p
ð
Þ
ð
Þ; g1ðxÞ ¼ x; d Y
ð
Þ ¼ lnð1  pÞ; zðxÞ ¼ 0; A ¼ 0; 1
f
g:
Substitution into Deﬁnition 4.2 yields f(x;p) ¼ px(1  p)1x I{0,1}(x).
□
Example 4.25
Gamma Family  Exponential Class
Let k ¼ 2, n ¼ 1, and deﬁne
c1 Q
ð
Þ ¼ a  1; c2 Q
ð
Þ ¼ 1 b
=
ð
Þ; g1ðxÞ ¼ lnðxÞ; g2ðxÞ ¼ x;
4.4
The Exponential Class of Densities
221

d Q
ð
Þ ¼  ln baG a
ð Þ
ð
Þ; zðxÞ ¼ 0; A ¼ ð0; 1Þ:
Substitution into Deﬁnition 4.2 yields
f x; a; b
ð
Þ ¼
1
baG a
ð Þ xa1ex=bIð0;1ÞðxÞ:
□
For now, we simply acknowledge the existence of the exponential class of
densities, with the recognition that it encompasses many families of densities
that are commonly used in applications. Later we will examine certain general
properties of the class of densities that facilitate the construction and evaluation
of parameter estimation procedures and statistical hypothesis tests.
Keywords, Phrases, and Symbols
Additivity property
Bernoulli family
Beta family
Beta function, B(a,b)
Binomial family
Chi-square additivity
Chi-square sub-family, w2
v
Continuous uniform family
Degrees of freedom
Discrete uniform family
Drawing randomly with replacement
Drawing randomly without
replacement
Exponential class of densities
Exponential sub-family
Gamma family
Gamma function, G(a)
Geometric Family
Hypergeometric family
Iso-density contours
Logistic distribution
Mean rate of occurrence
Memoryless property (or “good as
new”)
Multinomial family
Multivariate normal family
Negative binomial family
Parameter space
Parameters
Parametric families of densities
Pascal distribution
Poisson density approximation to
binomial
Poisson family
Poisson process
Regression function (hyperplane) or
curve (line) of X1 on X2
Reparameterized
Standard normal density
Standard normal random variable
Standardizing a random variable
Univariate normal family
Wear-out effect
Work-hardening effect
Problems
1. A shipment of 100 DVDs contains k defective disks.
You randomly sample 20 DVDs, without replacement,
from the shipment of 100 DVDs. Letting p ¼ k/100, the
probability that you will obtain less than three defective
disks in your sample of 20 disks is then given by
Pðx  3Þ ¼ S
3
x¼0
20
x
 
!
px ð1  p Þ20x If0;1;2;...;20g ðxÞ:
True or False?
2. The daily price, p, and quantity, demanded, q, of
gasoline on a European Wholesale spot market can be
viewed (approximately) as the outcome of a bivariate
normal random variable, where the bivariate normal den-
sity has mean vector and covariance matrix as follows:
m ¼
2:50
100
"
#
; S ¼
:09
 1
1
100
"
#
The price is measured in U.S. dollars per gallon of gasoline,
and the quantity demanded is measured in thousands of
gallons.
(a) What is the probability that greater than 110,000
gallons of gasoline will be demanded on any given day?
222
Chapter 4
Parametric Families of Density Functions

(b) What is the probability that the price of gasoline will
be between $2.00 and $3.00 on any given day?
(c) Deﬁne the regression function of Q on p. Graph the
regression function. What is the expected daily quan-
tity of gasoline demanded given that price is equal to
$3.00?
(d) Given that the price equals $3.00, what is the proba-
bility that quantity demanded will exceed 110,000
gallons? What is this probability on a day when
price equals $2.00?
3. Show that the following probability density functions
are members of the exponential class of densities:
(a) Binomial family, for a ﬁxed value of n.
(b) Poisson family.
(c) Negative binomial family for a ﬁxed value of r.
(d) Multinomial family, for a ﬁxed value of n.
(e) Beta family.
4. For each PDF family below, show whether or not the
family belongs to the exponential class of densities.
(a) fðx; bÞ ¼ b xðbþ1Þ Ið1;1Þ ðxÞ; b 2 O ¼ ð0; 1Þ: (This is a
subfamily of the Pareto family of PDFs.)
(b) fðx; YÞ ¼ 1=ð2YÞ
ð
Þ expðjxj=YÞ; Y 2 O ¼ ð0; 1Þ. (This
is a subfamily of the double exponential family of
PDFs.)
(c) fðx; m; sÞ ¼
1= x
ﬃﬃﬃﬃﬃﬃ
2p
p
s




exp ðlnðxÞ  m Þ2 =ð2 s2Þ


Ið0;1Þ ðxÞ;ðm;sÞ2 O ¼ ðm;sÞ : m 2 ð1;1Þ;s>0
f
g (This is
the log-normal family of PDFs.) Hint: Expanding the
square in the exponent of e may be helpful as an
alternative representation of the exponent.
(d) fðx; rÞ ¼
1  r
ð
Þ=r
ð
Þ rx If1;2;3;...g ðxÞ; r 2 O ¼ ð0; 1Þ:
5. Prove that if X has the geometric density, then the
“memoryless
property”
P(x > s + t
|x > s) ¼ P(x > t)
holds for every choice of positive integers s and t.
6. Prove that the CDF of the geometric family of
densities can be deﬁned by F(b) ¼ [1  (1  p)trunc(b)]
I[1,1) (b).
7. The quantity of wheat demanded, per day, in a
midwestern market during a certain marketing period is
represented by
Q ¼ 100; 000  12; 500 P þ V for p 2 2; 6
½
;
where
Q is quantity demanded in bushels;
p is price/bushel; and
V is approximately normally distributed.
You know that the expected quantity demanded is given
by
EðQÞ ¼ 100; 000  12; 500 p for p 2 2; 6
½
;
and thus is a function of p, and the variance of quantity
demanded is var(Q) ¼ 16 
 106.
(a) What is the mean and variance of V?
(b) If p ¼ 4, what is the probability that more than
50,000 bushels of wheat will be demanded?
(c) If p ¼ 4.50, what is the probability that more than
50,000 bushels of wheat will be demanded?
(d) For quantity demanded to be greater than 50,000
bushels with probability .95, what does p have to be?
(e) Is it possible that V could actually be normally
distributed instead of only approximately normally
distributed? Explain.
8. An investor has $10,000 which she intends to invest
in a portfolio of three stocks that she feels are good invest-
ment prospects. During the investor’s planning horizon,
the weekly closing prices of the stocks can be viewed
as the outcome of a trivariate normal random variable
with
EðXÞ ¼
27
10
18
2
4
3
5and CovðXÞ ¼
9
2
1
2
1
1
1
1
4
2
4
3
5:
The current price of the stocks are $23, $11, and $19,
respectively.
(a) If she invests her $10,000 equally among the three
stocks, what is the expected value of her portfolio?
What is the variance of the portfolio value?
(b) What is the probability density function of the port-
folio value? What is the probability that the closing
value of her portfolio for a given week will exceed
$11,000?
(c) What is the probability density function of the value
of stock 1? If she invests the $10,000 entirely in stock
1, what is the probability that the closing value of her
portfolio will exceed $11,000?
(d) What is the conditional density function of the value
of stock 1 given that stock 3 has a value of $17? If she
invests the $10,000 entirely in stock 1, what is the
conditional probability that the closing value of her
Problems
223

portfolio will exceed $11,000, given that stock 3 has a
value of $17?
(e) If she divides her $10,000 equally among stocks 1 and
2, what is the conditional probability that this portfo-
lio will have a closing value exceeding $11,000 given
that stock 3 has a value of $17?
9. Let Y have a chi-square distribution with 15 degrees
of freedom, let X have a chi-square distribution with 5
degrees of freedom, and let Y ¼ X + Z, where X and Z are
independent random variables.
(a) Calculate P(y > 27.488).
(b) Calculate P(6.262 < y < 27.488).
(c) Find c such that P(y > c) ¼ .05.
(d) Find c such that P(z > c) ¼ .05.
10. Let Y have the density N(5, 36), X have the density
N(4, 25), let Yand X be independent random variables, and
deﬁne W ¼ X  Y.
(a) Calculate P(y > 10).
(b) Calculate P(10 < y < 10).
(c) Calculate P(w > 0).
(d) Find c such that P(w > c) ¼ .95.
11. Let X be a bivariate random variable having the
probability density N(m, S), with
m ¼
5
8
	

and S ¼
2
1
1
3
	

:
(a) Deﬁne the regression curve of X1 on X2 . What is
E(X1 | x2 ¼ 9)?
(b) What is the conditional variance of X1 given that
x2 ¼ 9?
(c) What is the probability that x1 > 5? What is the prob-
ability that x1 > 5, given that x2 ¼ 9?
In problems 12–20 below, identify the most appropriate
parametric family of density functions from those
presented in this chapter on which to base the probabil-
ity space for the experiment described, and answer the
questions using the probability space you deﬁne:
12. WAYSAFE, a large retail supermarket, has a standard
inspection policy that determines whether a shipment of
produce will be accepted or rejected. Speciﬁcally, they
examine 5 percent of the objects in any shipment
received, and, if no defective produce is found in any of
the items examined, the shipment is accepted. Other-
wise, it is rejected. The items chosen for inspection are
drawn randomly, one at a time, without replacement,
from the objects in the shipment.
A shipment of 1,000 5-lb. bags of potatoes are received at
the loading dock. Suppose that in reality, 2 percent of the
5-lb. bags have defective potatoes in them. The “objects”
that are being inspected are bags of potatoes, with a bag of
potatoes being defective if any of the potatoes in the bag
are defective.
(a) Deﬁne an appropriate probability space for the
inspection experiment.
(b) What is the probability that WAYSAFE will accept
the shipment of potatoes?
(c) What is the expected number of defective bags of
potatoes when choosing 5 percent of the bags for
inspection in the manner described above?
(d) If the inspection policy is changed so that 10 percent
of the objects will be inspected, and the shipment
will be accepted only if no defectives are found,
what is the probability that the shipment will be
accepted?
13. The FLAMES-ARE-US Co. manufactures butane cig-
arette lighters. Your top-of-the-line lighter, which has the
brand name “SURE-FLAME,” costs $29.95. As a promo-
tional strategy, the SURE-FLAME lighter carries a guaran-
tee that if it takes more than ﬁve attempts before the
lighter actually lights, then the customer will be given
$1,000,000. The terms of the guarantee require that the
demonstration of failure of a SURE-FLAME lighter to
light within ﬁve attempts must be witnessed by an ofﬁ-
cial of the company, and each original buyer of a new
SURE-FLAME lighter is allowed only one attempt at
being awarded the $1,000,000. The lighter is such that
each attempt at lighting the lighter has a probability of
success (it lights) equal to .95, and the outcomes of
attempts to light the lighter are independent of one
another.
(a) Deﬁne the appropriate probability space for the
experiment of observing the number of attempts nec-
essary to obtain the ﬁrst light with the SURE-FLAME
lighter.
(b) What is the probability that a buyer who attempts to
demonstrate the failure of the SURE-FLAME to light
in ﬁve attempts will actually be awarded $1,000,000?
224
Chapter 4
Parametric Families of Density Functions

(c) What is the expected number of attempts required to
obtain a light with the SURE-FLAME lighter?
(d) What is the expected value of the award paid to any
consumer who attempts to claim the $1,000,000?
14. An instructor in an introductory economics class has
constructed a multiple-choice test for the mid-term
examination. The test consists of 20 questions worth 5
points each. For each question, the instructor lists four
possible answers, of which only one is correct. John
Partytime, a student in the class, has not attended class
regularly, and admits (to himself) that he is really not
prepared to take the exam. Nonetheless, he has decided
to take the exam, and his strategy is to randomly choose
one of the four answers for each question. He feels very
conﬁdent that this course of action will result in an exam
score considerably more than zero.
(a) Deﬁne a probability space that can be used to assign
probability
to
events
involving
the
number
of
questions that John answers correctly.
(b) What is the probability that John receives a zero on
the exam?
(c) What is the probability that John receives at least
25 points on the exam?
(d) What is John’s expected score on the exam?
15. The liquid crystal display in the new Extime brand of
digital watches is such that the probability it continues to
function for at least x hours before failure is constant (for
any given choice of the number x), regardless of how long
the display has already been functioning. The expected
value of the number of hours the display functions before
failure is known to be 30,000.
(a) Deﬁne the appropriate probability space for the
experiment of observing the number of hours a dis-
play of the type described above functions before
failure.
(b) What is the probability that the display functions for
at least 20,000 hours?
(c) If the display has already functioned for 10,000 hours,
what is the probability that it will continue to func-
tion for at least another 20,000 hours?
(d) The display has a rather unique guarantee in the
sense that any purchaser of a Extime watch, whether
the watch is new or used, has a warranty on the
display of 2 years from the date of purchase, during
which time if the display fails, it will be replaced free
of charge. Assuming that the number of hours the
watch operates in a given period of time is essentially
the same for all buyers of the Extime watch, is it more
likely that a buyer of a used watch will be obtaining a
free display replacement than a buyer of a new watch,
given an equal period of watch ownership? Explain.
16. The Department of Transportation in a foreign county
establishes gas-mileage standards that automobiles sold in
must meet or else a “gas guzzler” tax is imposed on the
sale of the offending types of automobile. For the “com-
pact, four-door” class of automobiles, the target average
gas mileage is 25 miles per gallon.
Achievement of the standard is tested by randomly
choosing 20 cars from a manufacturer’s assembly line,
and then examining the distance between the vector of
20 observed measurements of gas mileage/gallon and a
(20 
 1) vector of targeted gas mileages for these cars.
Letting X represent the 20 
 1 vector of observed gas
mileages, and letting t represent the 20 
 1 vector of
targeted gas mileages (i.e., t ¼ (25, 25, . . ., 25)0), the dis-
tance measure is D(x,t) ¼ [(x  t)0 (x  t)]1/2.
If D(x,t)  6, then the type of automobile being tested is
judged to be consistent with the standard; otherwise, the
type of automobile will be taxed.
Speciﬁc Motors Company is introducing a new four-door
compact into the market, and has requested that this type
of automobile be tested for adherence to the gas-mileage
standard. The engineers at Speciﬁc Motors know that the
miles per gallon achieved by their compact four-door
automobile can be represented by a normal distribution
with mean 25 and variance 1.267, so that the target gas
mileage is achieved on average.
(a) What is the probability that a car randomly chosen
from Speciﬁc Motor’s assembly line will be within
1 mile per gallon of the gas-mileage standard?
(b) What is the probability that Speciﬁc Motor’s compact
four-door will be judged as being consistent with the
gas-mileage standard?
(c) A neighboring country uses a simpler test for deter-
mining whether the gas-mileage standard is met. It
also has a target of 25 miles per gallon, but its test
involves forming the simple average of the 20 ran-
domly observed miles per gallon, and then simply
testing whether the calculated average is within one
mile per gallon of 25 miles per gallon. That is, the gas-
mileage standard will be judged to have been met if
1
20 S
20
i¼1 xi 2 ½24; 26:
Problems
225

What is the probability that Speciﬁc Motors will pass
this alternative test?
17. An appliance manufacturer is conducting a survey of
consumer satisfaction with appliance purchases. All
customers that have purchased one of the company’s
appliances within the last year will be mailed a customer
satisfaction survey. The company is contemplating the
proportion of surveys that customers will actually return.
It is known from considerable past experience with these
types of surveys that the expected proportion of returned
surveys is equal to .40, with a variance of .04.
(a) What is the probability that there will be more than
50 percent of the surveys returned?
(b) What is the probability that less than 25 percent of
the surveys will be returned?
(c) What is the median level of response to this type of
survey? What is the mode?
18. Customers arrive at the rate of four per minute at a
large bank branch in downtown Seattle. In its advertising,
the bank stresses that customers will receive service
promptly with little or no waiting.
(a) What is the probability that there will be more than
25 customers entering the bank in a 5-minute period?
(b) What is the expected number of customers that will
enter the bank during a 5-minute period?
(c) If the bank staff can service 20 customer in a 5-
minute interval, what is the probability that the cus-
tomer load will exceed capacity in a 5-minute inter-
val, so that some customers will experience delays in
obtaining service?
19. The accounts of the Excelsior company are being
audited by an independent accounting ﬁrm. The company
has 200 active accounts, of which 140 are current
accounts, 45 are past due 60 or more days, and 15 accounts
are delinquent. The accounting ﬁrm will randomly
choose ﬁve different accounts in their auditing procedure.
(a) What is the probability that none of the accounts
chosen will be delinquent accounts?
(b) What is the probability that at most one of the
accounts chosen will be delinquent?
(c) What is the probability that there will be three cur-
rent, one past due, and one delinquent accounts
chosen?
(d) What are the expected numbers of the various types
of accounts that will be chosen?
20. The Stonebridge Tire Co. manufactures passenger car
tires. The manufacturing process results in tires that are
either ﬁrst-quality tires, blemished tires, or defective
tires. The proportions of the tires manufactured that fall
in the three categories are .88, .09, and .03, respectively.
The manufacturing process is such that the classiﬁcation
of a given tire is unaffected by the classiﬁcations of any
other tires produced.
(a) A lot of 12 tires are taken from the assembly line and
inspected for shipment to a tire retailer who sells
only ﬁrst-quality tires. What is the probability that
all of the tires will be ﬁrst-quality tires?
(b) For the lot of tires in part (a), what is the probability
that there will be no defective tires among the 12
tires?
(c) What are the expected number of ﬁrst-quality tires,
blemished tires, and defective tires in the lot of 12
tires from part (a)?
(d) What is the probability that the 12 tires will contain
8 ﬁrst-quality tires, 3 blemished tires, and 1 defective
tire?
21. KoShop, a large retail department store, has a stan-
dard inspection policy that determines whether a ship-
ment of products will be accepted or rejected. Speciﬁcally,
they examine a randomly chosen sample of 10 percent of
the objects in any shipment received, and, if no defectives
are found in any of the items examined, the shipment is
accepted. Otherwise, it is rejected. The items chosen for
inspection are drawn randomly, one at a time, without
replacement, from the objects in the shipment.
A shipment of 100 Blu-ray disk players is received at the
loading dock. Suppose that in reality, unknown to the
store, two of the disk players are actually defective.
(a) Deﬁne an appropriate probability model for the
inspection experiment.
(b) What is the probability that KoShop will accept the
shipment of disk players?
(c) What is the expected number of defective players
when choosing 10 percent of the players for inspec-
tion in the manner described above?
(d) If the inspection policy is changed so that 20 percent
of the objects will be inspected, and the shipment
will be accepted only if no defectives are found,
226
Chapter 4
Parametric Families of Density Functions

what is the probability that the shipment will be
accepted?
22. Customers arrive, on average, at the rate of two per
minute at a bank in downtown Portland.
(a) Deﬁne an appropriate probability model for the
experiment of observing the number of customers
entering the bank in a 5-minute period.
(b) What is the probability that there will be more than
20 customers entering the bank in a 5-minute period?
(c) What is the expected number of customers that will
enter the bank during a 5-minute period?
(d) If the bank staff can service a maximum of 20
customers in a 5-minute interval, what is the proba-
bility that the customer load will exceed capacity in a
5-minute interval, so that some customers will expe-
rience delays in obtaining service?
23. The accounts of Pullman Plumbers, Inc. are being
audited by an independent accounting ﬁrm. The company
currently has 100 active accounts, of which 50 are current
accounts, 35 are past due 60 or more days, and 15 accounts
are delinquent. The accounting ﬁrm will randomly
choose ﬁve different accounts in their auditing procedure.
(a) What is the probability that none of the accounts
chosen will be delinquent accounts?
(b) What is the probability that at most one of the
accounts chosen will be delinquent?
(c) What is the probability that there will be three cur-
rent, one past due, and one delinquent accounts
chosen?
(d) What are the expected numbers of the various types
of accounts that will be chosen?
24. An instructor in an introductory economics class has
a true-false section on the mid-term examination that
consists of 10 questions worth 3 points each. Jeff Nostudy,
a student in the class, has not attended class regularly, and
knows that he is really not prepared to take the exam.
Nonetheless, he decides to take the exam, and his strategy
is to treat true and false as equally likely for each ques-
tion, and randomly choose one as his answer to each
question. He feels very conﬁdent that this course of
action will result in a score for the true-false section
that will be considerably more than zero.
(a) Deﬁne a probability model that can be used to assign
probability
to
events
involving
the
number
of
questions that Jeff answers correctly.
(b) What is the probability that Jeff receives zero points
on the true-false section of the exam?
(c) What is the probability that Jeff receives at least 15 of
the 30 points that are possible on the true-false sec-
tion of the exam?
(d) What is Jeff’s expected score on the true-false section
of the exam?
25. In the board game of “Aggravation”, the game begins
with each player having their four game pieces in a “base”
holding area and a player is unable to place one of her
game pieces on the board for play until they role either a
one or a six with a fair die.
(a) Deﬁne a probability model that can be used to assign
probability to events involving the number of times
that a player must roll the fair die to obtain a one or a
six.
(b) What is the probability that it will take three or more
rolls of the die for a player to get their ﬁrst game piece
on the board?
(c) What is the expected number of rolls of the die for a
player to get their ﬁrst game piece on the board?
(d) Deﬁne a new probability model, if you need to, and
derive the expected number of rolls of the die for a
player to get all four of their initial game pieces on the
board.
26. The Central Processing Unit (CPU) in a laptop com-
puter that your company manufactures is known to have
the following service life characteristics:
P x>s þ tjx>s
ð
Þ ¼ P x>t
ð
Þ 8s and t >0
where outcomes, x, of the random variable X measure the
operating life of the CPU, measured in 100,000 hour
increments, until failure of the CPU. The engineers in
your company tell you that EðXÞ ¼ 3.
(a) Deﬁne an appropriate probability model for the
experiment of observing the operating life of the
CPU until the point of failure.
(b) What is the probability that the CPU fails in the ﬁrst
100,000 hours of use?
Problems
227

(c) Your company provides a warranty on the CPU stat-
ing that you will replace the CPU in any laptop in
which the CPU fails within the ﬁrst 5 years of use.
What is the probability that you will be ﬁxing one of
the computers that has your CPU in it if the laptop
were to be operating 24 hours a day?
(d) The CPU is sold to computer manufacturers at a
wholesale price of $100 per CPU. The cost to repair
a laptop that has a failed CPU is $200. What is your
expected net revenue from the sale of one of these
CPUs, taking into account the cost of warranty
repair, and assuming as in (c) that the laptop were
operating 24 hours a day?
27. The LCD screen on a popular smartphone has the
following operating life characteristic
P x>s þ tjx>s
ð
Þ < P x>t
ð
Þ 8s and t >0
where outcomes, x, of the random variable X measure the
operating life of the screen, measured in 100,000 hour
increments, until failure of the screen. The engineers in
your company tell you that EðXÞ ¼ 1 and s2
X ¼ :5.
(a) Deﬁne an appropriate probability model for the
experiment of observing the operating life of the
screen until the point of failure.
(b) Does the probability model that you deﬁned in
(a) adhere to the operating life characteristic that
was indicated above? Why or why not?
(c) What is the probability that the screen will fail in
1 year if the screen were operating 24 hours/day?
(d) The anticipated useful life of the smartphone itself is
ﬁve calendar years. If the screen is used for 4 hours
per day, what is the probability that the screen will
not fail during the useful life of the smartphone?
28. Let
X1; . . . ; X20
ð
Þbe 20 iid N 2; 4
ð
Þrandom variables.
Also, deﬁne the random variables Zi ¼ Xi2
2
for i ¼ 1; . . . ;
20. Answer the following questions relating to these ran-
dom variables.
(a) What are the values of E Z2
i


and var Z2
i


?
(b) What are the values of E P
20
i¼1
Z2
i


and var P
20
i¼1
Z2
i


?
(c) Calculate the value of P P
20
i¼1
z2
i  31:4104


.
(d) Calculate the value of
P P
10
i¼1
z2
i  12:5489 and

P
20
i¼11
z2
i  12:5489 Þ:
29. The grades assigned to students taking a midterm in a
large principles of economics class is assumed to be nor-
mally distributed with a mean of 75 and a standard devia-
tion of 7. The Professor teaching the class, known to be a
“stringent grader”, has indicated that based on this distri-
bution of grades, she intends to “grade on the curve”,
whereby the top 15 percent of the students will receive
A’s, the next 20 percent will receive B’s, the “middle”
30 percent will receive C’s, the next 20 percent will receive
D’s, and ﬁnally, the lowest 15 percent will receive F’s.
(a) Derive the grade ranges associated with each of the
letter grades that will be assigned in the class.
(b) Can the grade distribution actually be normally
distributed in this case, or must the normal distribu-
tion be only an approximation? Explain.
30. The production function for the number of toy robots
that your company manufactures in a week, expressed in
100’s of robots, is represented as follows:
Q ¼ a0x  x0Bx þ e;
where x ¼
l
k
	

is a 2 
 1vector of labor,l, and capital, k,
applied, e  N 0; 25
ð
Þ, with
a ¼
10
5
	

and B ¼
2
1
1
1
	

:
(a) Deﬁne the expected quantity produced as a function
of labor and capital.
(b) If labor and capital are each applied at the level of 10
units each, what is the probability that the quantity
produced will exceed 25,000 toy robots?
(c) Deﬁne the levels of labor and capital that will maxi-
mize the expected quantity produced. What is the
maximum expected quantity produced?
(d) If labor and capital are each applied at the level that
maximizes expected quantity produced, what is the
probability that the quantity produced will exceed
25,000 toy robots?
(e) What is the probability distribution of Q? What is the
probability distribution of Q if the expected quantity
produced is maximized?
(f) If all of the robots produced are sold, and they sell for
$10 each, and if the prices of labor and capital per unit
are $15 and $10 respectively, derive the expected
228
Chapter 4
Parametric Families of Density Functions

value, variance, and probability distribution of the
maximum weekly proﬁt that can be made from the
production and sale of toy robots.
(g) There are at least two reasons why the normal
distribution can only be viewed as an approximation
to the probability distribution of e. What are they?
Explain.
31. The weekly average price, in dollars per gallon, and
quantity sold of organic milk, measured in thousands of
gallons, in a large west coast market in the fall is
represented by the outcomes of a bivariate random
variable P; Q
ð
Þ having a multivariate normal distribution,
N m, S
ð
Þ with the following mean vector and covariance
matrix:
m ¼
3:50
100
	

and S ¼
:01
:7
:7
100
	

(a) Deﬁne an interval event, centered at the mean, that
will contain the outcomes of the price of organic milk
with .95 probability.
(b) What is the probability that quantity sold will exceed
110,000 gallons?
(c) Deﬁne the moment generating function for
P; Q
ð
Þ.
Use it to deﬁne the expected value of weekly total
sales of organic milk, in dollars.
(d) Deﬁne the regression function of quantity sold as a
function of the price of organic milk. Does the regres-
sion function make economic sense? Use the regres-
sion function to ﬁnd the expected quantity sold given
a price of $3.40.
(e) Can the bivariate distribution of price and quantity
actually be multivariate normally distributed in this
case, or must the normal distribution be only an
approximation? Explain.
Problems
229

5
n
Basic Asymptotics
n
n
n
5.1
Introduction
5.2
Convergence in Distribution
5.3
Convergence in Probability
5.4
Convergence in Mean Square (or Convergence in
Quadratic Mean)
5.5
Almost-Sure Convergence (or Convergence with
Probability 1)
5.6
Summary of General Relationships Between
Convergence Modes
5.7
Laws of Large Numbers
5.8
Central Limit Theorems
5.9
Asymptotic Distributions of Differentiable Functions
of Asymptotically Normally Distributed Random
Variables: The Delta Method
5.10
Appendix: Proofs and Proof References for
Theorems
5.1
Introduction
In this chapter we establish some results relating to
the probability characteristics of functions of n-variate random variables X(n) ¼
(X1,. . .,Xn) when n is large. In particular, certain types of functions Yn ¼ g(X1,. . .,Xn)
of an n-variate random variable may converge in various ways to a constant, its
probability distribution may be well-approximated by a so-called asymptotic
distribution as n increases, or the probability distribution of g(X(n)) may converge
to a limiting distribution as n ! 1.
There are important reasons why the study of the asymptotic (or large sample)
behavior of g(X(n)) is an important endeavor. In practice, point estimation, hypoth-
esis testing, and conﬁdence-set estimation procedures (Chapters 7–10) will all be
represented by functions of random variables, such as g(X(n)),where n will refer to
the number of sample data observations relating to the experiment being
analyzed. In order to be able to evaluate, interpret, and/or compare the merits of
these statistical procedures, and indeed to be able to deﬁne the latter two types of

procedures at all, it is necessary to establish the probability characteristics of
g(X(n)). Unfortunately, it is often the case in statistical and econometric practice
that the actual probability density or distribution of g(X(n)) is difﬁcult to derive, or
even intractable to work with analytically, when n is ﬁnite. Asymptotic theory
often identiﬁes methods that provide tractable approximations to the probability
distribution of g(X(n)) when n is sufﬁciently large, and thereby provides a means of
evaluating, comparing, and/or deﬁning various statistical inference procedures.
Asymptotic theory also provides the principal rationale for the prevalent use of
the normal probability distribution in statistical analyses.
All of the asymptotic results that will be discussed in this chapter rely on the
concept of sequences of random variables. A fundamental difference between a
non-random sequence {xn} and a random sequence {Xn} is that the elements of a
random sequence are random variables, as opposed to ﬁxed numbers in the non-
random case. The random variables in the random sequence are of course
capable of assuming any real numbers within their respective ranges. Thus,
while {xn} refers to only one sequence of real numbers, {Xn} refers to a collection
of possible real number sequences deﬁned by the various possible outcomes of
the random variables X1, X2, X3,. . . in the sequence. Since any particular real
number sequence associated with {Xn} can be thought of as an outcome of the
random variables involved in the sequence, it is then meaningful to deﬁne
probabilities of various types of events involving outcomes of the random
sequence. For example, one might be interested in the probability that the
outcomes of a sequence of scalar random variables {Xn} is bounded by a particular
value m > 0, i.e., P(|xn|  m, 8 n ∈N), or in the probability that the outcomes of
the sequence converges to a limit c, i.e., P limn!1 xn ¼ c
ð
Þ. Since the sequence is
random, convergence and boundedness questions cannot be veriﬁed as being
unequivocally true or false on the basis of a given sequence of real numbers, but
they can be assigned a probability of occurrence in the context of the probability
space for the outcomes of the random variables involved.
We will examine four basic types of random variable convergence in this
chapter:
1. Convergence in distribution;
2. Convergence in probability;
3. Convergence in mean square (or convergence in quadratic mean); and
4. Convergence almost surely (convergence with probability 1).
The material in this chapter is of a more advanced and technical nature, and
in order to facilitate readability, the proofs of theorems have been moved to a
chapter appendix, and the proofs of some of the theorems are not presented and
will be deferred to a more advanced course of study. Some readers, upon ﬁrst
reading, may wish to skim this chapter for main results, while others may wish
to move on to Chapter 6 and refer back to the results in this chapter as needed.
For those readers whose understanding of sequences, limits, continuity of
functions, and orders of magnitudes of sequences is in need of refreshing,
Appendix A.7 provides a review of some elements of real analysis that are
particularly relevant to the concepts discussed in this chapter.
232
Chapter 5
Basic Asymptotics

5.2
Convergence in Distribution
The concept of convergence in distribution involves the question of whether the
sequence of random variables {Yn} is such that lim
n!1 P yn 2 A
ð
Þ ¼ P y 2 A
ð
Þfor some
“limiting” random variable Y  FðyÞ. This is related to whether the sequence
of cumulative distribution functions associated with the random variables in
the sequence {Yn} converges to a limiting cumulative distribution function (see
Deﬁnition A.30 regarding the concept of convergence of real-valued functions).
The usefulness of the concept lies in establishing an approximation to the
true CDF (i.e., using the “limiting” CDF FðyÞ, and/or its associated probability
density functionfðyÞ) for Yn when n is large enough, where “large enough” means
that the CDF of Yn is close to its limiting CDF. Such approximating CDFs and
their associated PDFs can be extremely useful when the true CDF or PDF for Yn is
very difﬁcult (or impossible) to deﬁne or is intractable to work with, but the
limiting CDF or PDF is easier to deﬁne and analyze.
We ﬁrst characterize convergence in distribution in terms of a sequence of
CDFs. We alert the reader to the fact that all results presented henceforth can be
interpreted in terms of multivariate random variables, unless the context is
explicitly deﬁned in terms of scalar random variables.
Deﬁnition 5.1
Convergence in
Distribution (CDFs)
Let {Yn} be a sequence of random variables, and let {Fn} be the associated
sequence of cumulative distribution functions corresponding to the random
variables. If there exists a cumulative distribution function, F, such that
Fn(y) ! F(y) 8 y at which F is continuous, then F is called the limiting CDF
of {Yn}. Letting Y have the distribution F, i.e., Y ~ F, we then say that Yn
converges in distribution (or converges in law) to the random variable Y, and
we denote this convergence by Yn !
d Y or Yn !
L Y


. We also write Yn !
d F as a
short-hand notation for Yn !
d Y  F, which is read “Yn converges in distribu-
tion to F.”
If Yn !
d Y, then as n becomes large, the CDF of Yn is approximated ever
more closely by its limiting CDF, the CDF of Y (see Figure 5.1). Note that it is
admissible in the deﬁnition that Y ¼ c, i.e., the random variable can be degener-
ate so that P(y ¼ c) ¼ 1. When the limiting CDF is associated with a degenerate
random variable, we say that the sequence of random variables converges in
distribution to a constant, and we denote this by Yn !
d c.
It is not generally true that convergence in distribution will imply that the
PDFs of the random variables in a sequence will also converge correspondingly.1
However, in the cases of nonnegative integer-valued discrete random variables
1As an example of this situation, let Xn have the discrete uniform distribution on the range
1
n ; 2
n ; . . . ; n1
n ; 1


; for n ¼ 1; 2; 3; . . ., and let
X have the continuous uniform distribution on 0; 1
½
. The probability distribution of Xn converges to the distribution of X as n ! 1.
However, the sequence of PDFs are such that fnðxÞ ! 0 as n ! 1 8x 2 0; 1
½
.
5.2
Convergence in Distribution
233

and continuous random variables, convergence of a sequence of PDFs is sufﬁ-
cient for establishing convergence in distribution.
Theorem 5.1
Convergence in
Distribution (PDFs)
Let {Yn} be a sequence of either continuous or nonnegative integer-valued
discrete random variables, and let {fn} be the associated sequence of probability
density functions corresponding to the random variables. Let there exist a
density function, f, such that fn(y) ! f(y) 8 y, except perhaps on a set of points,
A, such thatPY ðAÞ ¼ 0in the continuous case, where Y ~ f. It follows that Yn!
d Y
(or Yn !
L Y).
If the PDFs of the elements of {Yn} converge to the PDF f as indicated in
Theorem 5.1, then f is referred to as the limiting density of {Yn}. The notation
Yn !
d f is sometimes used as a short hand for Yn ! Y  f, where it is understood
that the limiting CDF of {Yn} in this case coincides with the CDF F associated
with the PDF f. The term limiting distribution is often used generically to refer
to either the limiting CDF or limiting density. If Yn !
d Y and if the PDFs {fn}
converge to a limiting density, then as n becomes large, the PDF of Yn is
approximated ever more closely by its limiting density, the density of Y
(for example, see Figure 5.2).
The following example illustrates the concept of convergence in distribution
characterized through convergence of CDFs. It also provides some insight into
the qualiﬁer “such that F is continuous at y” used in Deﬁnition 5.1.
Example 5.1
Convergence of CDFs
Let {Yn} be such that the cumulative distribution function for Yn is deﬁned by
Fn ðyÞ ¼ n
2ðy  t þ n1Þ I½tn1;tþn1 ðyÞ þ Iðtþn1;1Þ ðyÞ, and let Y have the cumula-
tive distribution function FðyÞ ¼ I½t;1Þ ðyÞ. Then Yn !
d Y. To see this, note that
(see Figure 5.3) lim
n!1 Fn ðyÞ ¼ 1
2 Iftg ðyÞ þ Iðt;1Þ ðyÞ which agrees with F(y) for all
points at which F is continuous, i.e., all points except t. Thus, by deﬁnition, the
limiting distribution of Yn is F(y).
□
y
F
Fn
Fn-1
Fn-2
1
0
Figure 5.1
Convergence in
distribution (CDF).
234
Chapter 5
Basic Asymptotics

Note that F(y) in Example 5.1 makes sense as a limiting distribution for Yn
when interpreted in the context of its original purpose – that of providing an
approximation to Fn(y) for large n. Viewed in this way, the fact that limn!1FnðyÞ
does not agree with F(y) at the point t is immaterial, since it is not the limit
function per se, limn!1FnðyÞ , that we wish to approximate in any case.2
The reader will note that as n increases, Fn(y) is ever more closely approximated
by F(y). In particular, from the deﬁnition of Fn(y), it follows that P(yn > t þ n1)
¼ P(yn < t  n1) ¼ 0 8 n, implying P(t  n1  yn  t þ n1) ¼ 1 8 n, and
when n ! 1, P(yn ¼ t) ! 1, i.e., the sequence of random variables with distri-
bution functions {Fn} converges in distribution to a degenerate random variable
at t. Note that F(y) implies this degeneracy.
In the next example, we illustrate the concept of convergence in distribution
characterized through convergence of density functions.
y
f
fn-1
fn
0
Figure 5.2
Convergence in
distribution (PDFs).
1
.5
y
t
Figure 5.3
Graph of limn!1FnðyÞ.
2Note that limn!1 Fn(y) is not even a cumulative distribution function for y 2 R.
5.2
Convergence in Distribution
235

Example 5.2
Convergence of
Densities
Let X ~ N(0,1) and {Zn} be a sequence deﬁned by Zn ¼ (3 + n1)X + (2n/(n1)),
which has an associated sequence of density functions {fn} deﬁned by fn ¼
N(2n/(n1), (3 + n1)2). Since fn ! N(2,9) ¼ f (note that fn(z) ! f(z) 8 z ∈ℝ),
Zn !
d Z ~ N(2,9).
□
The uniqueness theorem of moment-generating functions (recall Theorem
3.27) can be extended to allow one to identify limiting distributions of sequences
of random variables through an examination of limiting moment-generating
functions. This result can be useful when the limit of a sequence of moment-
generating functions is more easily deﬁned than the limit of a sequence of CDFs
or density functions.
Theorem 5.2
Convergence in
Distribution (MGFs)
Let {Yn} be a sequence of random variables having an associated sequence of
moment generating functions { MYn ðtÞ }. Let Y have the moment generating
function MY (t). Then Yn !
d Y iff MYn ðtÞ ! MY ðtÞ 8t 2 ðh; hÞ; for some h > 0.
The theorem implies that if we can discover that limn!1MYnðtÞ exists and is
equal to the moment generating function MY(t) 8 t in an open interval containing
zero, then the distribution associated with MYðtÞ is the limiting distribution of
the sequence {Yn}. We illustrate the use of Theorem 5.2 in the following
example.
Example 5.3
Demonstration that
Zn ¼ Xn  n
ð
Þ=
ﬃﬃﬃﬃﬃﬃ
2n
p
!
d
Z  Nð0; 1Þ if
Xn  x2
n
Let {Xn} be a sequence of random variables, where Xn ~ w2
n 8n . The sequence {Xn}
then has an associated sequence of MGFs given by {MXnðtÞ}, where MXnðtÞ ¼
1  2t
ð
Þn=28n. Let the random sequence {Zn} be deﬁned by Zn ¼ Xn  n
ð
Þ=
ﬃﬃﬃﬃﬃﬃ
2n
p
,
which has an associated sequence of MFGs given by {MZnðtÞ}, where MZnðtÞ
¼ (1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2=n
p
t)n/2 exp(
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n=2
p
t). Note that the elements in the sequence {Zn}
are standardized w2 random variables (recall E(Xn) ¼ n and var (Xn) ¼ 2n).
We now show that ln MZnðtÞ
ð
Þ! t2/2. To see this, ﬁrst note that ln MZnðtÞ
ð
Þ ¼
 n
2 ln 1 
ﬃﬃﬃ
2
n
q
t



ﬃﬃﬃn
2
p t. Expressing the ﬁrst term on the right-hand side of the
equality in terms of a Taylor series expansion around t ¼ 0 obtains ln MZnðtÞ
ð
Þ ¼
ﬃﬃﬃn
2
p t þ t2
2 þ oð1Þ
h
i

ﬃﬃﬃn
2
p t ¼ t2
2 þ oð1Þ ! t2
2: It follows that
lim
n!1 MZnðtÞ
ð
Þ ¼ lim
n!1 exp ln MZnðtÞ
ð
Þ
ð
Þ
ð
Þ ¼ exp lim
n!1 ln MZnðtÞ
ð
Þ
ð
Þ


¼ exp t2
2


since exp(·) is a continuous function (recall Deﬁnition A.29, especially part (b)).
Since exp(t2/2) is the MGF of a standard normal distribution, we know by
Theorem 5.2 that the sequence {Zn} of standardized w2
n random variables has a
N(0,1) limiting distribution, i.e., Zn !
d Z ~ N(0,1).
□
236
Chapter 5
Basic Asymptotics

5.2.1
Asymptotic Distributions
We now introduce the concept of an asymptotic distribution for the elements in
a random sequence. In the most general sense of the term, an asymptotic
distribution for Zn in the sequence {Zn} is a distribution that provides an approx-
imation to the true distribution of Zn for large n. In this general sense, if {Zn} has
a limiting distribution, then since the limiting distribution can be interpreted as
an approximation to the distribution of Zn for large n, the limiting distribution
might be considered an asymptotic distribution for Zn. The purpose of
introducing the additional concept of asymptotic distributions is to generalize
the concept of approximating distributions for large n to include cases where the
sequence {Zn} either has no limiting distribution, or the limiting distribution is
degenerate (and hence not particularly useful as an approximation to the distri-
bution of a nondegenerate Zn).
We will focus on asymptotic distributions that apply to random variables
deﬁned by g(Xn, Yn), where Xn !
d X for nondegenerate X. This context covers
most cases of practical interest.
Deﬁnition 5.2
Asymptotic
Distribution for
g(Xn, Qn) When Xn !
d X
Let the sequence of random variables {Zn} be deﬁned by Zn ¼ g(Xn,Yn), where
Xn!
d X for nondegenerate X, and {Yn} is a sequence of real numbers, matrices,
and/or parameters. Then an asymptotic distribution for Zn is given by the
distribution of g(X, Yn), denoted by Zn

a
g(X,Yn) and meaning “Zn is
asymptotically distributed as g(X,Yn).”
Before proceeding to examples, we provide some general motivation for
Deﬁnition 5.2. Operationally, Deﬁnition 5.2 implies that if Zn can be deﬁned
as a function, g(Xn,Yn), of some random variable Xn for which Xn !
d X and X is
nondegenerate, then an asymptotic distribution for Zn is given by the probability
distribution associated with the same function applied to X, g(X,Yn). The problem
of identifying asymptotic distributions is then equivalent to the problem
of identifying distributions of functions of X since Zn ¼ g(Xn,Yn) and Xn !
d X )
Zn 
a g(X,Yn).
In order to motivate why this approach to approximating the distribution of
Zn for large n makes sense, ﬁrst note that a correspondence between events for
Zn and events for Xn can be established as zn ∈A , xn ∈Bn ¼ {xn: zn ¼ g(xn,Yn),
zn ∈A}. Note further that since Xn !
d X, we can approximate the probability
of the event Bn for Xn using the nondegenerate limiting distribution of {Xn}, as
PXn xn 2 Bn
ð
Þ PX(x ∈Bn) for large n. It follows from the equivalence of the events
A and Bn that, for large n,
PZn zn 2 A
ð
Þ ¼ PXn xn 2 Bn
ð
Þ  PXðx 2 BnÞ ¼ PXðgðx; YnÞ 2 AÞ:
Thus, an approximation to PZn zn 2 A
ð
Þ can be calculated using the probability
distribution of g(X,Yn) in the same sense, and with the same degree of accuracy,
that PXn xn 2 Bn
ð
Þ can be approximated using the probability distribution of X.
5.2
Convergence in Distribution
237

We emphasize that a limiting distribution for {Zn} ¼ {g(Xn,Yn)} need not exist for
the preceding approximation arguments to hold.
In the next example we illustrate a situation where the limiting distribution
is degenerate, but a nondegenerate asymptotic distribution can be established.
Example 5.4
Asymptotic Versus
Limiting Distribution
for n1Xn, Where Xn ~
Binomial(n, l/n)
Let {Xn} be such that Xn has a binomial distribution with parameters n
and p ¼ l/n for some ﬁxed value of l > 0, i.e., Xn ~ Binomial(n, l/n).
Note that
MXnðtÞ ¼ (1 + p(et1))n ¼ (1 + (l(et1)/n))n!
elðet 1Þ
(recall that
limn!1 1þ
ð
a=n
ð
ÞÞn ¼ ea ) which by Theorem 5.2, implies that Xn !
d
X ~
enp np
ð
Þx=x!
ð
ÞI 0;1;2;...
f
gðxÞ (Poisson distribution with parameter l ¼ np). Note
this result is in agreement with the relationship established between the
Binomial and Poisson distribution in Chapter 4.
Deﬁne the sequence {Zn} by Zn ¼ g(Xn, n) ¼ n1Xn, and note that since
MZnðtÞ ¼ (1 + (l/n)(et/n  1))n ! 1, the limiting distribution of {Zn} exists and
is degenerate at 0 by Theorem 5.2 (i.e., an MGF equal to the constant function
1 is associated with the density for which P(z ¼ 0) ¼ 1). On the other
hand the asymptotic distribution of Zn as deﬁned in Deﬁnition 5.2 is given
by the distribution of Z ¼ g(X,n) ¼ n1X, so that Zn 
a enp np
ð
Þnz= nz
ð
Þ!
ð
Þ
I{0,1/n,2/n,. . .}(z).
□
The following example illustrates a case where a limiting distribution does
not exist, but a useful asymptotic distribution can nonetheless be deﬁned.
Example 5.5
Asymptotic Versus No
Limiting Distribution
Xn 
a N(n,2n) if Xn ~ w2
n
In Example 5.3, it was demonstrated that Zn!
d Z ~ N(0,1), where Zn ¼ (Xn  n)/
ﬃﬃﬃﬃﬃﬃ
2n
p
and Xn ~ w2
n . It follows from Deﬁnition 5.2, using results on linear
combinations of normally distributed random variables, that Xn ¼ g(Wn,n) ¼
ﬃﬃﬃﬃﬃﬃ
2n
p
Zn + n 
a
ﬃﬃﬃﬃﬃﬃ
2n
p
Z þ n ~ N(n,2n). Thus, for large n, a w2-distributed random
variable is approximately normally distributed with mean n and variance 2n.
Note that {Xn} does not have a limiting distribution. As an illustration of the use
of the approximation, note that if n ¼ 100, then referring to the w2-distribution
with 100 degrees of freedom, P(x100  124.34) ¼ .95. Using the asymptotic
distribution N(100,200), the approximation to this probability is given by
^P(x100  124.34)  .9573.
□
5.2.2
Convergence in Distribution for Continuous Functions
The ﬁnal result involving the concept of convergence in distribution that we
will examine in this section facilitates identiﬁcation of the limiting distribution
of a continuous function, g(Xn), when {Xn} is such that Xn !
d X.
Theorem 5.3
Limiting Distributions
of Continuous
Functions Xn
f
g
Let Xn !
d X, and let the random variable g(X) be deﬁned by a function g(x) that
is continuous with probability 1 with respect to the probability distribution of
X. Then g(Xn) !
d g(X).
238
Chapter 5
Basic Asymptotics

Thus, if Xn !
d X, the limiting distribution of g(Xn) is given by the distribu-
tion of g(X) if g is continuous with probability 1 (with respect to the probability
distribution of X). Note that this result applies to a function of Xn that does not
depend on any arguments other than Xn itself. In particular, the function g
cannot depend on n or any other sequence of real numbers or matrices whose
elements change with n, so, for example, g xn
ð
Þ ¼ 3 þ xn would be admissible,
while g(xn,n) ¼ n1=2xn þ 2n would not.
Example 5.6
Limiting Distribution of
a Continuous Scalar
Function
Let {Zn} be such that Zn !
d ZN(0,1). It follows immediately from Theorem 5.3
and results on linear combinations of normally distributed random variables
that g(Zn) ¼ 2Zn þ 5 !
d 2Z þ 5 ~ N(5,4). One could also demonstrate via Theo-
rem 5.3 that g(Zn} ¼ Z2
n!
d Z2 ~w2
1 (since the square of a standard normal random is
a w2
1 random variable).
□
Example 5.7
Limiting Distribution
of a Continuous
Vector Function
Let {Zn} be a sequence of bivariate random variables such that Zn !
d Z ~ N(m,S),
where m ¼ [2
3]0 and S ¼
4
0
0
9
	

. Then it follows from Theorem 5.3 and
results on linear combinations of normally distributed random variables that,
for a matrix A having full row rank, g(Zn) ¼ AZn !
d AZ ~ N(Am,ASA0). For
example, if
A ¼
1
1
1
1
	

; then AZn !
d
5
1
	

;
13
5
5
13
	



:
One could also demonstrate via Theorem 5.3 that g(Zn) ¼ (Znm)0 S1(Znm) !
d
(Zm)0 S1(Zm) ~ w2
2 , based on the fact that the two independent standard
normal random variables, represented by the (2  1) vector S½(Zm), are
squared and summed.
□
5.3
Convergence in Probability
Referring to a random sequence of scalars, {Yn}, the concept of convergence in
probability involves the question of whether outcomes of the random variable
Yn are close to the outcomes of some random variable Y with high probability
when n is large enough. If so, the outcomes of Y can serve as an approximation to
the outcomes of Yn for large enough n. Stated more rigorously, the issue is
whether the sequence of probabilities associated with the sequence of events
yn; y
ð
Þ:jyn  yj<e
f
gconverges to 1 for every choice of e > 0, no matter how small.
In the case where Yn and Y are (m  k) matrices, convergence in probability of
Yn to Y requires that each element of Yn converge in probability to the
corresponding element of Y. The vector case is subsumed by letting k ¼ 1.
5.3
Convergence in Probability
239

Deﬁnition 5.3
Convergence in
Probability
The sequence of random variables, {Yn} converges in probability to the ran-
dom variable, Y, iff
a. Scalar Case: limn!1Pðj yn yj<eÞ ¼ 1; 8e>0
b. Matrix Case:
limn!1P j yn ½i; j  y½i; jj<e
ð
Þ ¼ 1; 8e>0; 8i and j:
Convergence in probability will be denoted by Yn !
p Y, or plim(Yn) ¼ Y, the
latter notation meaning the probability limit of Yn is Y.
Convergence in probability implies that as n ! 1, the joint distribution
of (Yn,Y) approaches a degenerate distribution deﬁned by linear restrictions, as
P(yn ¼ y) ¼ 1. To motivate this interpretation, note that Deﬁnition 5.3 implies
that for any arbitrarily small e > 0,P yn ½i; j 2 y½i; j  e; y½i; j þ e
ð
Þ
ð
Þ ! 1; 8i and j:
It
then
follows
from
Bonferroni’s
probability
inequality
that
P yn ½i; j 2 y½i; j  e; y½i; j þ e
ð
Þ; 8i and j
ð
Þ ! 1 . Thus, outcomes of Yn are arbi-
trarily close to outcomes of Y with probability approaching 1 as n ! 1. There-
fore, for large enough n, observing outcomes of Y is essentially equivalent to
observing outcomes of Yn,3 which motivates the idea that the random variable Y
can serve as an approximation to the random variable Yn.
Note that the random variable Y in Deﬁnition 5.3 could be degenerate, i.e., Y
could be a number or matrix of numbers. Using the notation introduced in
Deﬁnition 5.3, we denote this situation by Yn !
p c, where c is a scalar or matrix
of constants. This situation is referred to as “Yn converges in probability to a
constant.”
Example 5.8
Convergence in
Probability to a
Constant
Let {Yn} have an associated sequence of density functions, {fn}, deﬁned by fn(y)
¼ n1I{0}(y) þ (1  n1)I{1}(y). Note that lim
n!1 Pðj yn 1j ¼ 0Þ ¼ lim
n!1 ð1  n1Þ ¼ 1,
so that limn!1 Pðj yn 1j<eÞ ¼ 1 8 e>0, and plim(Yn) ¼ 1. Thus, Yn converges in
probability to the value 1.
□
Example 5.9
Convergence
in Probability to a
Random Variable
Let Y ~ N(0,1), and let {Zn} have an associated sequence of density functions, {fn},
with E(Zn) ¼ 0 and var(Zn) ¼ n1. Deﬁne the random sequence {Yn} by Yn ¼
Y þ Zn, and assume Y and Zn are independent, so that E(Yn) ¼ 0 and var(Yn) ¼
1 þ n1. Then Yn !
p Y (or plim(Yn) ¼ Y) since 8 e > 0, lim
n!1 Pðj yn yj<eÞ ¼
lim
n!1 Pðj zn j<eÞ ¼ 1, which follows by an application of Markov’s inequality.
□
3The reader may be wondering why we do not simply replace the uncountably inﬁnite collection of probability statements
Pðj yn yj<eÞ; 8e>0, in the deﬁnition of convergence in probability with the deﬁnition lim
n!1 Pðyn ¼ yÞ ¼ 1. The problem is that such
a convergence deﬁnition would not be useful in the examination of any sequence of nondegenerate continuous random variables,
since P(yn ¼ y) ¼ 0 8 n ) limn!1Pðyn ¼ yÞ ¼ 0, no matter how close the outcomes of Yn become to outcomes of Y as n ! 1.
240
Chapter 5
Basic Asymptotics

Example 5.10
Convergence in
Probability to a
Constant Vector
Let {Yn} be such that E(Yn) ¼
2
3
	

and Cov (Yn) ¼ n1
2
1
1
1
	

. Note by Markov’s
inequality that, 8 e > 0, lim
n!1 P j yn ½1  2j<e
ð
Þ  lim
n!1 1  2=n
e2


¼ 1;
and
lim
n!1 P jyn 2
½   3j<e
ð
Þ  lim
n!1
1  n1
e2


¼ 1
so that Yn[1] !
p 2 and Yn[2] !
p 3. From Deﬁnition 5.3, it follows that Yn !
p
c
¼
2
3
	

□
Since Deﬁnition 5.3 states thatplim
Yn
ðmkÞ
 
!
¼
Y
ðmkÞiff plim(Yn[i,j]) ¼ Y[i,j] 8 i,j,
it follows that “the probability limit of a matrix is the matrix of probability limits.”
Thus, just as for the expectation operator, the plim is an element-wise operator.
Deﬁnition 5.4
Probability Limits of
Matrices (and Vectors
for k ¼ 1)
Let {Yn} be a sequence of (m  k) random matrices. Then
p lim
Yn½1; 1
. . .
Yn½1; k
...
..
.
...
Yn½m; 1
. . .
Yn½m; k
0
B
@
1
C
A ¼
plim Yn½1; 1
ð
Þ
. . .
plim Yn½1; k
ð
Þ
...
..
.
...
plim Yn½m; 1
ð
Þ
. . .
plim Yn½m; k
ð
Þ
2
64
3
75
5.3.1
Convergence in Probability Versus Convergence in Distribution
At this point we emphasize a fundamental difference in concept between con-
vergence in distribution and convergence in probability (which will also apply
when comparing convergence in distribution to other types of convergence we
will study). For the convergence in distribution concept Yn!
d Y, it is immaterial
whether outcomes of the random variables in the sequence {Yn} are related in any
way to outcomes of Y. In particular, it makes no difference whether Yn and Y are
even referring to the same type of experiment. The convergence Yn !
d Y states
only that the random variables in the sequence {Yn} have a limiting distribution
that is equal to the probability distribution of Y. There is no implication that the
outcomes of Yn necessarily emulate the outcomes of Y in any way as n ! 1
(although they might). This is simply a result of the fact that random variables
with the same probability distributions are not necessarily the same random
variable. For example, X ~ f(x) ¼ ½ I{0,1}(x) could refer to tossing a fair coin where
x ¼ 1 is a head and x ¼ 0 is a tail, while Y ~ h(y) ¼ ½ I{0,1}(y) could be referring to
the rolling of a fair die, where y ¼ 0 stands for a roll of 3 or less, while y ¼ 1
stands for a roll of 4 or more. Clearly, the PDFs of X and Yare equal, i.e., f(t) ¼ h(t)
8 t. Just as clearly, outcomes of the random variables X and Y are not related in
any sense, since whether x ¼ 1 (or 0) has no bearing on whether y ¼ 1 (or 0).
Indeed, X and Y refer to two separate experiments.
5.3
Convergence in Probability
241

The situation is quite different for convergence in probability, where the
convergence being referred to involves the outcomes of the respective random
variables Yn and Y themselves, and not merely convergence of their probability
distributions. That is, convergence in probability implies that, for large n,
outcomes of the random variable Yn in the sequence {Yn} are close to the
outcomes of Y with high probability, and for large enough n, observing
an outcome of Y is a very good approximation to observing the corresponding
outcome of Yn. Thus, for large enough n, outcomes of the experiment
represented by Yn can be treated essentially as outcomes of the experiment
represented by Y.
5.3.2
Properties of the Plim Operator
It is useful to note that if a real-valued sequence of numbers or matrices {an}
converges (in the sense of real analysis) to a given number, or matrix, i.e., an ! a,
then it can also be stated that plim(an) ¼ a, since {an} can be interpreted as a
sequence of degenerate random variables.
Theorem 5.4
Lim ) Plim
Let {an} be such that
an
mk
ð
Þ
!
a
mk
ð
Þ . Then plim(an) ¼ a.
When a sequence of random variables is deﬁned via a continuous function,
g(Xn), of the random variables in another sequence {Xn}, the plim operator acts
analogously to the lim operator of real analysis (also note the analogy to
Theorem 5.3).
Theorem 5.5
Plims of Continuous
Functions
Let Xn !
p X, and let the random variable g(X) be deﬁned by a function g(x) that
is continuous with probability 1 with respect to the probability distribution of
X. Then g(Xn) !
p g(X), or equivalently, plim(g(Xn)) ¼ g(plim (Xn)).
Theorem 5.5 says that for functions that are continuous with probability 1
(with respect to the probability distribution of X), the probability limit of the
function is the function of the probability limit. The theorem can greatly
simplify ﬁnding the probability limits of complicated functions of Xn, especially
when convergence is to a constant.
Example 5.11
Plim of a Continuous
Function
a. Let {Xn} be a positive-valued random variable such that Xn !
p 3. Then Yn ¼
g(Xn) ¼ ln(Xn) + (Xn)1/2 is such that plim(Yn) ¼ plim(g(Xn)) ¼ g(plim(Xn)) ¼
ln(plim(Xn)) + (plim(Xn))1/2 ¼ ln(3) + 31/2 ¼ 2.8307.
b. Let {Xn} be such that Xn
ðk1Þ
!
p
X
ðk1Þ N (0,I), and let Yn ¼ g(Xn) ¼ Xn0Xn. Then
Yn !
p g(X) ¼ X0X ~ w2
k.
□
Through examining a number of special cases of Theorem 5.5, we can
establish additional useful properties of the plim operator.
242
Chapter 5
Basic Asymptotics

Theorem 5.6
Plim Properties
for Addition,
Multiplication,
and Division
For conformable Xn, Yn, and constant matrix A.
a. plim(AXn) ¼ A(plim (Xn));
b. plim Pm
i¼1 Xn i½ 
ð
Þ


¼Pm
i¼1 p lim Xn i½ 
ð
Þ (the plim of a sum ¼ the sum of the
plims);
c. plim(Qm
i¼1 Xn½i)¼Qm
i¼1 p lim Xn½i
ð
Þ(the plim of a product ¼ the product of the
plims);
d. plim(Xn Yn)¼ plim(Xn)plim(Yn);
e. plim(X1
n
Yn) ¼ (plim(Xn))1 plim(Yn) (assuming plim(Xn) is nonsingular).
Example 5.12
Plims of Scalar Additive
and Multiplicative
Functions
Let A ¼
2
1
1
1
	

, and let {Xn} be such that plim(Xn) ¼
2
5
	

. Then, plim
(AXn) ¼ A plim(Xn) ¼
9
7
	

, plim (Xn [1] + Xn[2]) ¼ plim(Xn [1]) + plim(Xn[2]) ¼
2 + 5 ¼ 7, and plim (Xn[1] Xn[2]) ¼ plim(Xn [1]) plim(Xn [2]) ¼ 2 	 5 ¼ 10.
□
Example 5.13
Plims of Matrix
Functions to Constant
Matrices
Let {Yn} be such that plim(Yn) ¼
1
2
2
1
	

and {Xn} be such that plim(Xn) ¼
3
1
2
1
	

. Then plim(Xn Yn) ¼ plim(Xn) plim(Yn) ¼
3
1
2
1
	

1
2
2
1
	

¼
5
7
4
5
	

,
and plim(Xn
1 Yn) ¼ (plim(Xn))1 plim(Yn) ¼
1
1
2
3
	

1
2
2
1
	

¼
1
1
4
1
	

.□
Example 5.14
Plims of Matrix
Functions to Vector
Random Variables
Let {Xn} and {Yn} be such that plim(Xn) ¼
3
2
2
4
	

and
Yn
ð21Þ
!
p
Y
ð21Þ  N 0; I
ð
Þ:
Then XnYn !
p ðplimðXnÞÞY  N 0;
13
14
14
20
	



;
and
X1
n Yn !
p ðplim ðXnÞÞ1
Y  N 0;
:3125
:2188
:2188
:2031
	



:
□
5.3.3
Relationships Involving Both Convergence in Probability and in Distribution
There are a number of useful results on convergence that involve both conver-
gence in distribution and convergence in probability. One such result has use in
situations where it is relatively easy to show (or it is known) that Xn !
d X and
(Xn  Yn) !
p 0 but more difﬁcult to demonstrate directly that Yn !
d X.
Theorem 5.7
Plim of Difference
and Convergence
in Distribution
Let {(Xn, Yn)} be a sequence of pairs of (m  k) random matrices for which Xn !
d X
and (Xn  Yn) !
p 0. Then Yn !
d X.
5.3
Convergence in Probability
243

Example 5.15
Deﬁning Limiting
Distribution Through
Plim of Differences
Let {Xn} be such that Xn ~ N(0, (n1)/n), so that Xn !
d X ~ N(0,1). Let {Zn} be such
that Zn ~ w2
n with Xn and Zn independent. Deﬁne {Yn} by Yn ¼ (1 + n1) Xn þ
n1Zn  1. Then by Theorem 5.6, Yn !
d X ~ N(0,1). To see this, ﬁrst note
that Xn  Yn ¼ 1  n1 (Xn + Zn), so that plim (Xn  Yn) ¼ 1  plim(Xn/n) 
plim(Zn/n). Now, note that E(Xn/n) ¼ 0 8 n and var (Xn/n) ¼ n3(n  1)
! 0 implies plim(Xn/n) ¼ 0 by Chebyshev’s inequality. Similarly, because
E(Zn/n) ¼ 1 8n and var(Zn/n) ¼ (2/n) ! 0, plim(Zn/n) ¼ 1 by Chebyshev’s
inequality. Then plim (Xn  Yn) ¼ 0, and since Xn !
d X ~ N(0,1), Yn !
d X ~
N(0,1) by Theorem 5.7 with m ¼ k ¼ 1.
□
The following corollary to Theorem 5.7 indicates that convergence in prob-
ability of a sequence of random variables implies convergence in distribution of
the sequence.
Corollary 5.1
Plim ) Convergence
in Distribution
Yn !
p Y ) Yn !
d Y.
Therefore, one way of discovering the limiting distribution of a sequence of
scalar or multivariate random variables is to discover the probability limit of the
sequence, in which case the limiting distribution is identical to the distribution
of the random variable representing the probability limit.
Example 5.16
Deﬁning Limiting
Distribution Through
Convergence in
Probability
Let {Yn} be deﬁned by Yn ¼ (2 þ n1) X þ 3, where X ~ N(1,2). Using properties of
the plim operator, it follows that plim(Yn)¼ plim((2 þ n1)X)þ plim (3) ¼ 2X
þ 3 ~ N(5,8). Then, Corollary 5.1 implies that Yn !
d N(5,8).
□
The converse of Corollary 5.1 is not generally true, as motivated by the discus-
sion in Section 5.3.2, i.e., convergence in distribution of two random variables does
not necessarily imply that the outcomes of the two random variables are related in
any way. However, in the special case where a sequence of random variables
converges in distribution to a constant, the converse of Corollary 5.1 does hold,
since then the random variable is degenerate in the limit, taking a constant value
with probability 1.
Theorem 5.8
Convergence in
Distribution ) Plim for
Constants
Yn !
d c ) Yn !
p c
Thus, in the case of convergence to a constant, the notions of convergence in
probability and convergence in distribution are equivalent. Otherwise, conver-
gence in probability is the more stringent type of convergence.
244
Chapter 5
Basic Asymptotics

The concepts of convergence in distribution and convergence in probability
can be combined to produce a versatile extension of Theorem 5.3 that is useful
for deriving the limiting distribution of a much wider variety of functions of Xn.
In particular, with reference to a random sequence {Xn} for which Xn !
d X, the
extension facilitates the discovery of the limiting distribution for a function of
Xn that is continuous with probability 1 (with respect to the distribution of X)
when the function also depends on a convergent sequence of numbers or matri-
ces {an}, and/or on a sequence of other random variables {Yn} that converges in
probability to a constant matrix y.
Theorem 5.9
Limiting Distribution of
Continuous Functions of
Xn; Yn; an
f
g
Let {Xn}, {Yn}, and {an} be such that Xn
ðkmÞ
!
d
X
ðkmÞ, Yn
‘q
ð
Þ
!
p
y
‘q
ð
Þ
where y is a matrix
of constants, and an
jp
ð
Þ
!
a
ðjpÞ. Let the set B be such that the probability distri-
bution of X assigns P(x∈B) ¼ 1, and let the random variable g(Xn, Yn, an) be
deﬁned by a (possibly vector) function g that is continuous at every point in the
set B  y  a. Then g(Xn, Yn, an) !
d g(X, y, a).
Theorem 5.9 reduces the problem of identifying the limiting distribution of
g(Xn, Yn, an) to the problem of identifying the distribution of g(X, y, a). Note that
either of the arguments Yn or an could be a ghost in the function g(Xn, Yn, an),
meaning that the value of the function g is unaffected by Yn or an (in which case
Yn or an could be completely ignored in Theorem 5.9).
Example 5.17
Convergence in
Distribution of
Quadratic Form in RVs
and Parameters
Let {Zn} be a sequence of bivariate random variables such that Zn !
d Z ~ N(m,S)
where m ¼ [2 3]0 and S ¼
4
0
0
9
	

. Let {mn} be any sequence of (2  1) vectors
such that mn ! m (e.g., mn ¼ (2 + n1|3/(1+exp (n)))0 and let Sn be any sequence
of nonsingular (2  2) matrices such that Sn
!
p
S, for example, Sn ¼
4 þ n1
n1
n1
9 þ n1
	

. Then it follows from Theorem 5.9 that g(Zn, Sn, mn) ¼
(Znmn)0 S1
n
(Zn  mn) !
d (Z  m)0 S1 (Z  m) ¼ g(Z, S, m) ~ w2
2 (based on the
fact that two independent standard normal random variables represented by the
(2  1) vector S1/2 (Z  m), are squared and summed.) Compare the generality
of this result to the result found in Example 5.7.
□
Example 5.18
Convergence in
Distribution of
Continuous Function of
RVs and Parameters
Let Xn !
d X ~ y1 exp (x/y) I(0,1)(x) (i.e., exponential with parameter y), Yn !
p 4,
and an ! 2. Then Zn ¼ g(Xn, Yn, an) ¼ (Yn)2 Xn/an !
d 42X/2 ¼ 8X ¼ g(X, 4, 2) ~
Gamma (1,8y).
□
A number of special cases of Theorem 5.9 provide a number of useful results
that are collectively referred to as Slutsky’s theorems. The results refer to
convergence in distribution of sums and products of random matrices.
5.3
Convergence in Probability
245

Theorem 5.10
Slutsky’s Theorems
Let Xn !
d X and Yn !
p c. Then, for conformable Xn and Yn,
a. Xn þ Yn !
d X þ c;
b. YnXn !
d cX;
c. Yn
1Xn !
d c1 X (if c1 exists).
5.3.4
Order of Magnitude in Probability
It is sometimes useful to be able to characterize or compare random sequences
and/or terms in a random sequence relative to their order of magnitude in
addition to, or in lieu of, any examination of convergence in probability of the
sequences involved. The order-of-magnitude concept is especially useful in
sorting out which random terms in the deﬁnition of a random sequence make
dominant contributions to the magnitude of the sequence outcome and which
terms are irrelevant as n increases. The order of magnitude of a random
sequence, in terms of probability, is described in the following deﬁnition. The
concept can be thought of as a random or probabilistic counterpart to the order-
of-magnitude concept in real analysis (see the Appendix, Section A.7.5).
Deﬁnition 5.5
Order of Magnitude
in Probability
Let {Xn} be a sequence of random scalars, and let {Wn} be a real-valued random
matrix sequence.
a. Op(nk): The sequence {Xn} is said to be at most of order nk in probability,
denoted by Op(nk), iff for every e > 0 there exists a corresponding positive
constant c(e) < 1 such that P nkjXnj  c eð Þ


 1  e; 8n.
b. op(nk): The sequence {Xn} is said to be of order smaller than nk in proba-
bility, denoted by op(nk), iff nkXn !
p 0.
c. If {Wn[i,j]} is Op(nk) (or op(nk)) 8 i and j, then the random matrix sequence
{Wn} is said to be Op(nk) (or op(nk)).
Given Deﬁnition 5.5, a random sequence of scalars, {Xn}, is Op(nk) iff one can
always ﬁnd a ﬁnite interval within which the outcomes of nkXn will occur with
probability arbitrarily close to (but not necessarily equal to) 1 for each term in
the sequence. The random sequence {Xn} is op(nk) iff nkXn converges in proba-
bility to zero. If a random sequence is Op(1) 
 Op(no), the random sequence is
said to be bounded in probability. Note that the ranges of the random variables in
a sequence need not be ﬁnite for the sequence to be Op(nk), or even Op(1), as the
next example illustrates.
Example 5.19
Random Sequence
of Various Orders
Let {Xn} be such that Xi ~ N(0,1), 8 i, with all terms in the sequence being
independent random variables. Deﬁne {Zn} as Zn ¼ Pn
i¼1 Xi. Then {Xn} itself is
Op(1) and {Zn} is Op(n1/2). To see this, ﬁrst note that since Xi ~ N(0,1), there
always exists a constant c(e) > 0 large enough such
R cðeÞ
cðeÞ N(x;0,1) dx  1  e for
any choice of e > 0, so that {Xn} is Op(1) (i.e., {Xn} is bounded in probability). Now,
246
Chapter 5
Basic Asymptotics

note that n1/2 Zn ¼ n1/2 Pn
i¼1 Xi ~ N(0,1), whence it follows from the preceding
argument that {Zn} is Op(n1/2). It also follows that {Xn} is op(nd) and {Zn} is
op(n1/2 þ d) 8 d > 0. Finally, in the sequence deﬁned by Yn ¼ n1/2(Xn þ Zn),
note that n1/2 Xn is op(1), while n1/2 Zn is Op(1), implying that as n ! 1,
n1/2 Zn is the dominant random term in the deﬁnition of Yn while n1/2 Xn is
stochastically irrelevant as n ! 1.
□
Rules for determining the order of magnitude of the sum or product of
sequences of random variables are analogous to the rules introduced for the
case of sequences of real numbers or matrices and will not be repeated here. In
particular, Lemmas A.2 and A.3 in Appendix Section A.7.5 apply with O and o
changed to Op and op, respectively. The reader should also note the following
relationship between O (or o) and Op (or op) for sequences of real numbers or
matrices.
Theorem 5.11
Relationship Between
O and Op
Let {an} be a sequence of real numbers or matrices. If {an} is O(nk) (or o(nk)), then
{an} is Op(nk) (or op(nk)).
Example 5.20
Orders of Function
Sequences
Let {xn} be O(n1), {Yn} be Op(n2) and {Zn} be Op(n1). Then {xnYn} is Op(n3), {xn þ
Yn þ Zn} is Op(n2), {n1xn} is Op(1), {n2xn (Yn þ Zn)} is Op(n1), {xnYnZn} is Op(n4),
and {n5xnYnZn} is op(1).
□
5.4
Convergence in Mean Square (or Convergence in Quadratic Mean)
Referring to a random sequence of scalars, {Yn}, the concept of convergence in
mean square involves the question of whether, for some random variable Y,
the sequence of expectations {E((YnY)2)} converges to zero as n ! 1. Since
E((YnY)2)can be interpreted as the expected squared distance between
outcomes of Yn and outcomes of Y, convergence in mean square implies that
outcomes of Yn and Y are “close” to one another for large enough n, and
arbitrarily close when n ! 1. When Yn and Y are m  k matrices, the question
of convergence in mean square concerns whether the preceding convergence
of expectations to zero occurs elementwise between corresponding entries in Yn
and Y.
Deﬁnition 5.6
Convergence in Mean
Square (or Convergence
in Quadratic Mean)
The sequence of random variables, {Yn}, converges in mean square to the
random variable, Y, iff
a. Scalar Case: limn!1 (E((Yn  Y)2)) ¼ 0
b. Matrix Case: limn!1 (E((Yn[i,j]  Y[i,j])2)) ¼ 0, 8 i and j.
Convergence in mean square will be denoted by Yn !
m Y.
5.4
Convergence in Mean Square (or Convergence in Quadratic Mean)
247

5.4.1
Properties of the Mean Square Convergence Operator
As in the case of convergence in probability, convergence in mean square
imposes restrictions on the characteristics of the joint probability distribution
of (Yn, Y) as n ! 1. In particular, ﬁrst- and second-order moments of
corresponding entries in Yn and Y converge to one another as indicated in the
necessary and sufﬁcient conditions for mean square convergence presented in
the following theorem:
Theorem 5.12
Necessary and
Sufﬁcient Conditions
for Mean Square
Convergence
Yn!
m Y iff 8i and j:
a. E(Yn[i,j]) ! E(Y[i,j]),
b. var(Yn[i,j]) ! var(Y[i,j]),
c. cov(Yn[i,j],Y[i,j]) ! var(Y[i,j]).
The conditions in Theorem 5.12 simplify in the case where Y is a degenerate
random variable equal to the constant matrix, c.
Corollary 5.2 Necessary
and Sufﬁcient
Conditions for
Mean Square
Convergence to c
Yn !
m c iff E(Yn[i,j]) ! c[i,j] and var(Yn[i,j]) ! 0 8i and j.
Example 5.21
Convergence in Mean
Square to a Constant
Vector
Let E(Yn) ¼
2 þ 3n1
1 þ n1
	

and Cov (Yn) ¼ n2 2
1
1
1
	

. Since E(Yn) !
2
1
	

and
DIAG(Cov (Yn)) ¼
2=n2
1=n2
	

!
0
0
	

, it follows by Corollary 5.2 that Yn !
m
2
1
	

.□
In addition to the convergence of second-order moments, convergence in
mean square implies that the correlation between corresponding entries in Yn
and Y converge to 1 if the entries in Y have nonzero variances.
Corollary 5.3
Convergence in Mean
Square ) Correlation
Convergence
Yn!
m Y ) corr(Yn[i,j], Y[i,j]) ! 1 when var(Y[i,j]) > 0, 8i, j.
The corollary indicates that if Yn !
m Y, and if none of the entries in Y are
degenerate, then corresponding entries in Yn and Y tend to be perfectly
positively correlated as n ! 1. It follows that as n ! 1, the outcomes of
Yn[i,j] and Y[i,j] tend to exhibit properties that characterize a situation in
which two random variables have perfect positive correlation and equal
variances, including the fact that P jyn i; j
½
  y i; j
½
j<e
ð
Þ ! 1 8e>0 (recall the
discussion of correlation in Chapter 3). If Y[i,j] were degenerate, then although
the correlation is undeﬁned, the preceding probability convergence result still
holds since, in this case, Yn[i,j] tends toward a degenerate random variable
(var(Yn[i,j]) ! 0) having an outcome equal to the appropriate scalar value of Y
[i,j]. Thus, if Yn !
m Y, outcomes of Yn emulate outcomes of Y ever more closely
248
Chapter 5
Basic Asymptotics

as n ! 1, closeness being measured in terms of probability as well as
expected squared distance.
5.4.2
Relationships Between Convergence in Mean Square, Probability,
and Distribution
From the preceding discussion, the fact that mean square convergence is a
sufﬁcient condition for both convergence in probability and convergence in
distribution could have been anticipated.
Theorem 5.13
Relationship Between
Convergence in Mean
Square, Probability,
and Distribution
Yn!
m Y ) Yn !
p Y ) Yn !
d Y.
The result of Theorem 5.13 can be quite useful as a tool for establishing
convergence in probability and in distribution in cases where convergence in
mean square is relatively easy to demonstrate. Furthermore, the theorem allows
convergence in probability or in distribution to be demonstrated, even in cases
where the distributions of Yn and/or Y are not fully known, so long as the
appropriate convergence properties of the relevant sequences of expectations
can be established.
Example 5.22
Using Mean Square
Convergence
to Establish
Convergence in
Probability
and Distribution
Let Y ~ N(0,1), E(Yn) ¼ 0 8 n, var (Yn) ! 1 and cov (Yn, Y) ! 1. Then, since (recall
the proof of Theorem 5.12. c) E((Yn  Y)2) ¼ var (Yn) þ var (Y)  2 cov (Yn, Y) þ
(E(Yn)  E(Y))2 ! 0, it follows that Yn!
m Y, which implies that Yn !
p Y, and
Yn !
d N(0,1). Note that while we did not know the forms of the probability
distributions associated with the random variables in the sequence {Yn}, we
can nonetheless establish convergence in probability and in distribution
via convergence in mean square.
□
The following example demonstrates that convergence in mean square is
not a necessary condition for either convergence in probability or in distribution,
so that convergence in probability or in distribution does not imply convergence
in mean square.
Example 5.23
Illustration that Yn !
p Y
and/or Yn !
d Y 6) Yn !
m Y
Let {Yn} be such that P(yn ¼ 0) ¼ 1  n2 and P(yn ¼ n) ¼ n2. Then lim
n!1 P(yn ¼ 0)
¼ 1, so that plim(Yn) ¼ 0 and Yn!
d 0. However, E((Yn  0)2) ¼ 0 (1  n2) þ n2(n2)
¼ 1 8 n, so that Yn 6!
m
0.
□
As a further illustration of differences between convergence in mean square
and convergence in probability or in distribution, we now provide an example
showing that convergence in probability and/or convergence in distribution do not
necessarily imply convergence of the ﬁrst- and second-order moments of Yn and Y.
5.4
Convergence in Mean Square (or Convergence in Quadratic Mean)
249

Example 5.24
Illustration that Yn !
p Y
and/or Yn !
d Y 6)
E(Yr
n) ! E(Yr)
Let {Yn} be such that Yn ~ fn(y) ¼ (1  n1)I{0}(y) + n1I{n}(y), and note
that fn(y) !
f(y) ¼ I{0}(y) 8 y, and thus Yn !
d
Y ~ f(y). Furthermore, since
limn!1 P(|yn  0| < e) ¼ 1 8 e > 0, then Yn!
p 0. Now, note that E(Yn) ¼ 1 8n and
E(Y2
n) ¼ n 8 n, but E(Y) ¼ 0 and E(Y2) ¼ 0 when Y ~ f(y). Thus, neither Yn !
p Y nor
Yn !
d Y implies that E(Yn) ! E(Y) or E(Y2
n) ! E(Y2).
□
In summary, convergence in mean square is a more stringent type of conver-
gence than either convergence in probability or convergence in distribution. In
addition to implying the latter two types of convergence, convergence in mean
square also implies convergence of ﬁrst- and second-order moments about the
origin and mean as well as convergence to 1 of the correlation between
corresponding entries in Yn and Y (when var(Y) > 0).
5.5
Almost-Sure Convergence (or Convergence with Probability 1)
Referring to a sequence of random variables {Yn}, which could be scalars, or
matrices, the concept of almost-sure convergence involves the question of
whether, for some random variable Y, the limit of the outcomes of the random
variable sequence converges to the outcome of Y with probability 1. That is,
does P(yn ! y) ¼ P(limn!1yn ¼ y) ¼ 1?
Deﬁnition 5.7
Almost-Sure
Convergence
(or Convergence with
Probability 1)
The sequence of random variables, {Yn}, converges almost surely to the ran-
dom variable Y iff
a. Scalar case: P(yn ! y) ¼ P(limn!1yn ¼ y) ¼ 1,
b. Matrix case: P(yn[i,j]!y[i,j])¼P(limn!1yn[i,j]¼y[i,j])¼1, 8i and j.
Almost-sure convergence will be denoted by Yn !
as Y, or by aslim(Yn)¼ Y,
the latter notation meaning the almost-sure limit of Yn is Y.
Almost-sure convergence is the random counterpart to the non-random real-
analysis concept of the limit of a sequence. Almost-sure convergence
accommodates the fact that when dealing with sequences of nondegenerate
random variables, more than one sequence outcome (perhaps an inﬁnite num-
ber) is under consideration for convergence. If Yn !
as Y, then a real-analysis-type
limit is (essentially) certain to be achieved by outcomes of the sequence {Yn}, the
limit being represented by the outcome of Y. If Y is degenerate and equal to the
constant c, then Yn !
as c implies that the outcomes of {Yn} are (essentially) certain
to converge to the value c.
Note that almost-sure convergence is deﬁned in terms of an event involving
an inﬁnite collection of random variables contained in the sequence {Yn} and in
Y, the event being that the sequence of outcomes y1, y2, y3,. . . has a limit that
250
Chapter 5
Basic Asymptotics

equals the outcome y. This is notably different than in either the case of
convergence in probability or convergence in mean square, which both relate
to sequences of marginal probability distributions and outcomes of bivariate
random variables (Yn,Y) for n ¼ 1,2,3,. . .. Through counterexamples, we will see
that neither convergence in probability nor mean square necessarily implies that
limn!1 yn ¼ y occurs with probability 1.
In order to clarify the additional restrictions imposed on the sequence {Yn} by
almost-sure convergence relative to convergence in probability, we provide an
intuitive description of why Yn !
p c does not imply that limn!1 Yn exists with
probability 1. Note that for a sequence of outcomes of {Yn} to have a limit, c,
it must be the case that 8 e > 0, there exists an integer N(e) such that for 8n 
N eð Þ; yn  c
j
j<e (recall Deﬁnition A.25). The deﬁnition of almost-sure conver-
gence ensures that the outcomes {yn} are generated in such a way that the limit is
achieved with probability 1, and so the preceding restriction on the out-
comes of {Yn} will be met with probability 1 if Yn !
as c. However, if Yn !
p c, so
that P yn  c
j
j<e
ð
Þ ! 1; 8e>0, then all that can be said is that, for any ﬁxed large
value of n, the probability that a particular yn is close to c is high, but not
necessarily equal to 1. Since Yn !
p c does not even imply that P yn  c
j
j<e
ð
Þ ¼ 1
for any ﬁxed value of n, it certainly does not imply that there exists an N(e)
for which P yn  c
j
j<e; 8n  N eð Þ
ð
Þ ¼ 1. Thus, the existence of a limit for {yn}
with probability 1 is not implied by Yn !
p c.
An alternative and equivalent characterization of almost-sure convergence
that follows directly from the fundamental deﬁnition of a limit (Deﬁnition A.25)
is presented in the next theorem. The alternative characterization facilitates
both comparisons with convergence in probability and proofs of other results
involving the concept of almost-sure convergence.
Theorem 5.14
Alternative
Characterization
of Almost-Sure
Convergence
(Scalar Case)
P(limn!1yn ¼ y) ¼ 1 , limn!1Pðj yi yj<e; i  nÞ ¼ 1; 8e>0
The alternative characterization can be extended to cases where Yn and Yare
matrices, by simply applying the characterization elementwise to the respective
entries in Yn and Y.
5.5.1
Relationships Between Almost-Sure Convergence
and Other Convergence Modes
Theorem 5.14 leads to a relatively straightforward demonstration that almost-
sure convergence implies convergence in probability.
Theorem 5.15
Relationship Between
Almost-Sure and Plim
Yn !
as Y ) Yn !
p Y
5.5
Almost-Sure Convergence (or Convergence with Probability 1)
251

The converse of Theorem 5.15 is not true, that is, convergence in probability
does not imply almost-sure convergence. Furthermore, convergence in mean
square and/or convergence in distribution also do not imply almost-sure conver-
gence. These facts are demonstrated by the following counterexample.
Example 5.25
Almost-Sure Not
Implied by Other
Convergence Modes
Yn !
m Y and/or Yn !
p
Y and/or Yn !
d
Y 6) Yn !
as Y. Let {Yn} be a sequence of
independent random variables such that Yn ~ fn(y) ¼ (1  n1) I{0}(y) + n1I{1}(y).
Since fn(y) ! f(y) ¼ I{0}(y), {Yn} converges in distribution to the constant 0, and
thus it is also true that Yn !
p
0. Furthermore, since E(Yn) ¼ n1
!
0 and
var (Yn) ¼ n1  n2 ! 0, it follows that Yn !
m 0. However, it does not follow
that Yn !
as 0. To see this, note that 8 e ∈(0,1) and 8 integer s > n,
P j yi j<e; n  i  s
ð
Þ ¼ P
s
i¼n ð1  i1Þ ¼ P
s
i¼n
i  1
i
¼ n  1
n
	
n
n þ 1 	 n þ 1
n þ 2 	 	 	 s  1
s
¼ n  1
s
! 0 as s ! 1:
Therefore, lim
n!1P yi
j
j<e; i  n
ð
Þ ¼ 0 because the probability value equals 0 8n  1,
and so the limit of the sequence of probability values equals 0 and not 1. Thus,
Yn 6!
as
0.
□
It is also true that almost-sure convergence does not imply convergence in
mean square, as the following counterexample demonstrates:
Example 5.26
Counter example
Showing Yn !
as Y 6)
Yn !
m Y
Let {Yn} be a sequence of independent random variables such that Yn ~ fn(y) ¼
(1  n2) I{0}(y) + n2 I{n}(y). Note that 8 e ∈(0,n), and 8 integer s > n,
P j yi j<e; n  i  s
ð
Þ ¼ P
s
i¼n 1  i2


¼ P
s
i¼n
i2  1
i2
¼ n  1
ð
Þ s þ 1
ð
Þ
ns
;
so that P yi
j
j<e; i  n
ð
Þ ¼ n  1=n
ð
Þ . Then 8 e > 0,
lim
n!1P yi
j
j<e; i  n
ð
Þ ¼ 1 so
that Yn !
as
0. Now, note that E(Yn)¼ n1
!
0 but var(Yn) ¼ 1  n2 6! 0.
Therefore, Yn 6!
m
0 by Corollary 5.2.
□
5.5.2
Additional Properties of Almost-Sure Convergence
Similar to the case of convergence in probability (Theorem 5.4), it is useful to
note that if a real-valued sequence of numbers, or matrices, converges (in the
sense of real analysis) to a given number, or matrix, i.e., an ! a, then it can also
be stated that an !
as a.
252
Chapter 5
Basic Asymptotics

Theorem 5.16
Lim ) Almost-Sure
Let {an} be such that an ! a. Then an !
as a.
Similar to the cases of convergence in probability and convergence in distri-
bution, a useful result for establishing almost-sure convergence of continuous
functions of Xn, when Xn !
as X, can be stated as follows.
Theorem 5.17
Almost-Sure
Convergence
of Continuous Functions
Let Xn !
as X, and let the random variable g(X) be deﬁned by a function g(x) that is
continuous with probability 1 with respect to the probability distribution of X.
Then g(Xn) !
as g(X) or, equivalently, aslim g(Xn) ¼ g(aslim (Xn)).
Note that all of the properties of the plim operator listed in Theorem 5.6
apply equally well to the aslim operator since they can all be justiﬁed as special
cases of Theorem 5.17.
Example 5.27
Almost-Sure
Convergence of Sums
and Products
Let Xn !
as
2
1
	

. Then g1(Xn) ¼ Xn[2]/Xn[1] !
as 1/2, g2(Xn) ¼ Xn[2]  Xn[1] !
as 1, and
g3(Xn) ¼ g2(Xn) g1(Xn) !
as 1/2.
□
Example 5.28
Almost-Sure
Convergence to RV
Let Xn
ð21Þ
!
as
X
ð21Þ, where X[1] ¼ 3 and X[2] ~ N(1,2). Then g(Xn) ¼ Xn[1] (1 + Xn[2])
!
as 3(1 + X[2]) ~ N(6,18).
□
The ﬁnal result we will present in this subsection provides a necessary and
sufﬁcient condition for almost-sure convergence to occur. The criterion will be
useful in proving strong laws of large numbers, which we will examine shortly.
Theorem 5.18
Cauchy’s Necessary
and Sufﬁcient Condition
for Almost-Sure
Convergence
A sequence of random variables {Yn} converges almost surely to some (possibly
degenerate) random variable iff lim
n!1 P max
m>n ym  yn
j
j<e


¼ 1; 8e>0.4
The Cauchy criterion states that for almost-sure convergence to occur, it is
necessary and sufﬁcient that the distance between the outcomes of the nth term
in the random sequence and all subsequent terms beyond the nth be arbitrarily
small with probability approaching 1 as n ! 1. This makes intuitive sense,
since for {yn} to converge to some value, eventually (i.e., for all values of n large
enough) all the values in the sequence must be arbitrarily close to the limit value
and, thus, arbitrarily close to each other.
4In the event that max does not exist, max is replaced by sup (supremum, i.e., the smallest upper bound) in the statement of the
theorem.
5.5
Almost-Sure Convergence (or Convergence with Probability 1)
253

In summary, almost-sure convergence is a more stringent type of conver-
gence than either convergence in probability or convergence in distribution. In
addition to implying the latter two types of convergence, almost-sure conver-
gence also implies that the sequence of outcomes of {Yn} converges to a limit
represented by an outcome of Y with probability 1.
5.6
Summary of General Relationships Between Convergence Modes
Collecting together the results presented for the four types of random variable
convergence discussed in the previous sections, we can summarize the
relationships between the various types of convergences in Figure 5.4.
Note that these exhaust the convergence mode relationships that can be
established in terms of general implications between them. However, there are
additional special cases that deﬁne additional less general interrelationships
between the modes of convergence. For example, if a random sequence
convergences in probability, it follows that there exists a sub-sequence that
converges almost surely.5 We leave the study of additional special cases to a
more advanced course in probability theory.
5.7
Laws of Large Numbers
In this section we examine results concerning the convergence behavior of a
speciﬁc sequence of scalar random variables deﬁned by { Xn} whose nth term is
given by Xn ¼ n1 P
n
i¼1
Xi, and where Xi is the ith element of another sequence of
random variables {Xn}. Thus, {Xn} is a sequence whose nth term is given by the
simple average of the ﬁrst n terms in the random sequence {Xn}. In the context of
m
Yn
(Y = c)
Y
⇒
⇐
d
Yn
Y
p
Yn
Y
as
Yn
Y
⇑
⇓
Figure 5.4
General convergence
mode relationships.
5Gut, Allan (2005). Probability: A Graduate Course. Springer-Verlag, New York, Theorem 3.4.
254
Chapter 5
Basic Asymptotics

a sample of observations from an experiment,Xn will be referred to as the sample
mean, and a detailed examination of its properties will be presented in
Chapter 6. A convergence result for {Xn} that uses the concept of convergence
in probability is referred to as a weak law of large numbers (WLLNs), whereas a
convergence result using almost-sure convergence is referred to as a strong law
of large numbers (SLLNs).
The types of convergence we will be examining can take either of two forms.
When all of the means of the random variables in {Xn} are equal to the same
number, m, we examine conditions for which Xn!
as m (a SLLN) or Xn!
p m (a WLLN).
When the means of the random variables in {Xn} are not necessarily equal, we
examine conditions for which Xn – mn !
as 0 (a SLLN) or Xn  mn !
p 0 (a WLLN),
where mn ¼ n1 Pn
i¼1 mi and mi ¼ E(Xi) is the ith term in the real number sequence
{mn}. Thus, in the case of equal means, we are examining convergence of {Xn} to
the common (constant) mean of the random variables in {Xn}, whereas in the case
of unequal means, we are examining whether the difference between the
outcomes of Xn and the average mean, mn , of the random variables in {Xn}
converges to zero in probability or almost surely as n ! 1.6
The reader may wonder why the convergence behavior of such a speciﬁc
sequence of random variables as {Xn} deserves explicit attention. The answer lies
in the fact that a large number of important parameter-estimation and
hypothesis-testing procedures in econometrics and statistics can be deﬁned in
terms of averages of random variables. The laws of large numbers are then useful
for analyzing the asymptotic behavior of these procedures when the samples of
data being analyzed are relatively large.
5.7.1
Weak Laws of Large Numbers (WLLNs)
There is a variety of conditions that can be placed on the random variables in the
sequence {Xn} that ensure either Xn!
p m or Xn  mn!
p 0. These conditions relate in
various ways to the independence, homogeneity of distribution, and/or
variances and covariances of the random variables in the sequence {Xn}.
The basic idea underlying weak laws of large numbers is to have the
distribution of the random variable
Yn ¼ n1 Pn
i¼1 Xi  mi
ð
Þ
collapse and
become degenerate on zero as n ! 1. For this to happen, the random variable
Pn
i¼1 Xi  mi
ð
Þ must be of smaller order of magnitude (in probability) than n.
The intuition underlying the degeneracy of Yn in the limit is perhaps clearest in
the case where the Xi’s are iid and var(Xi) ¼ s2 exists. Then the distribution of
Pn
i¼1 Xi  mi
ð
Þ has a variance, or spread, given by ns,
2 which is expanding by a
factor of n. The expanding spread of the distribution is counteracted via scaling
the random variable by the factor n1 (actually, any nd with d>1/2 will do),
leading to a distribution
Yn
having variance s2/n ! 0 and implying the
6The concepts of SLLNs and WLLNs can be generalized to the case where {mn} is a sequence of constants that are not necessarily the
means of the Xi’s. See Y.S. Chow and H. Teicher (1978), Probability Theory, p. 121.
5.7
Laws of Large Numbers
255

degeneracy of Yn at zero as n ! 1. As will be seen ahead, neither the existence of
s2 nor the iid condition are necessary for Yn to be degenerate at zero in the limit.
5.7.1.1
IID Case
TheonlyWLLNwewillexaminethatdoesnotrequiretheexistenceofthevariancesof
the random variables in the sequence {Xn} is Khinchin’s WLLN, presented as follows.
Theorem 5.19
Khinchin’s WLLN
Let {Xn} be a sequence of iid random variables, and suppose E(Xi) ¼ m < 1, 8 i.
Then Xn!
p m.
Example 5.29
Plim Xn
ð
Þ for iid
Gamma RVs
Let {Xn} be a sequence of iid random variables, with Xi ~Gamma (a,b). It follows
from Khinchin’s WLLN that Xn!
p ab. If a ¼ 2 and b ¼ 4, it follows that Xn!
p 8.□
Example 5.30
Plim Xn
ð
Þ for iid
Bernoulli RVs
Let {Xn} be a sequence of iid random variables, with Xi ~ px (1p)1x I{0,1} (x). It
follows from Khinchin’s WLLN that Xn!
p p. If p ¼ .6, it follows that Xn!
p .6.
□
Example 5.31
Illustration of WLLN
When Variance Does
Not Exist
Let
f(x) ¼ 2x3I[1,1)(x),
and
suppose
that
the
random
variables
in
the
sequence {Xn} are iid, each with density function f(x). Note that if X ~ f(x), then
E(X)¼
Ð 1
1 x(2x3)dx ¼ 2, but E(X2)¼
Ð 1
1 x2(2x3)dx ¼
Ð 1
1 2x1dx ¼ 2 ln(x)j1
1 ! 1, so
that the var(X) ¼ E(X2)  (E(X))2 does not exist. Nonetheless, by Khinchin’s
theorem we know that Xn!
p 2.
□
Khinchin’s WLLN can be used to provide support for the relative-frequency
deﬁnition of probability, as follows:
Theorem 5.20
Convergence
in Probability of
Relative Frequency via
WLLN
Let {S,ϒ,P} be the probability space of an experiment, and let A be any event
contained in S. Let an outcome of NA be the number of times that event A
occurs in n independent and identical repetitions of the experiment. Then the
relative frequency of event A occurring is such that (NA/n) !
p P(A).
Theorem 5.20 implies that as the number of independent identical repetitions
of an experiment ! 1, the probability that the relative frequency of event A is
arbitrarily close to the true probability of event A approaches 1. Thus, the WLLN
provides support for the notion that the relative frequency of the occurrence of an
event can be used as the measure of the probability of the event as n ! 1. Note,
however, that we did not yet conclude that P(limn!1(nA/n) ¼ P(A)) ¼ 1, that is, we
cannot conclude that the limit of the relative frequency exists and is equal to P(A)
with probability 1. The latter result involves the notion of almost-sure conver-
gence and will be dealt with in our subsequent discussion of SLLNs.
5.7.1.2
Non-IID Case
WLLNsthatrelaxtheiidassumptionofKhinchin’sWLLNcanbedeﬁnedbyimposing
various other conditions on the variances and covariances of the random variables in
the sequence {Xn}. The WLLN that we will present follows from the necessary and
sufﬁcient conditions for the existence of a WLLN, stated in the next theorem.
256
Chapter 5
Basic Asymptotics

Theorem 5.21
Necessary and
Sufﬁcient Conditions
for WLLN
Let {Xn} be a sequence of random variables with ﬁnite variances (not necessarily
independent), and let {mn} be the corresponding sequence of their expectations.
Then
limn!1 P jxn  mnj<e
ð
Þ¼ 1, 8 e > 0 iff E
Xnmn
ð
Þ
2
1þ Xnmn
ð
Þ
2


! 0:
Any condition placed on the random variables in the sequence {Xn} that results
in the convergence of E
Xnmn
ð
Þ
2
1þ Xnmn
ð
Þ
2


to zero results in (Xnmn) !
p 0 (or Xn!
p m in the
equal-means case) by Theorem 5.21. We present one such condition now.
Theorem 5.22
WLLN for Non-IID Case
Let {Xn} be a sequence of random variables with respective means given by {mn}.
If var Xn


! 0; then (Xnmn) !
p 0.
Example 5.32
Plim Xn
ð
Þ for Non-iid
Gamma RVs
Let {Xn} be a sequence of Gamma-distributed random variables for which E(Xi)
¼ 2i, var(Xi) ¼ 4, and sij ¼ 0, 8 i 6¼ j. Then since var Xn


¼ 4=n ! 0, it follows
by Theorem 5.22 that Xn(1.5n)/n !
p 0, where mn ¼ (1.5n)/n.
□
Example 5.33
Plim Xn
ð
Þ for Non-iid
Beta RVs
Let {Xn} be a sequence of independent, Beta-distributed, random variables for
which E(Xi) ¼ .4 8 i. Note that the variance of a Beta distribution exhibits a ﬁnite
upper bound, say s2
i  t, since P(x∈(0,1)) ¼ 1. Then, for any variances of the
random variables, var Xn


¼ n2 Pn
i¼1 s2
i  n1t ! 0. By Theorem 5.22, Xn!
p .4.
(Note: Khinchin’s theorem cannot be used here since it is not known whether
the Xi’s have identical distributions.)
□
Example 5.34
Plim Xn
ð
Þ for Non-iid
Normal RVs
Let the sequence of random variables {Xn} be such that Xi ~ N(1,1 + i1) with
sij ¼ r|ij|, r∈(0,1) and i 6¼ j. Since s2
i  2 8i, Pn
i¼1 s2
i  2n, and thus Pn
i¼1 s2
i is
o(n2). Also, given i, Pn
j>i sij ¼ Pn
j>i rjijj ! r=ð1  rÞ; so that Pn
j>i sij is o(n1).
Then, sincemn¼ 1 8n, andvar Xn


¼ n2 Pn
i¼1 s2
i þ 2 Pn
i¼1
Pn
j>i sij
h
i
¼ oð1Þimply-
ing that var XnÞ ! 0

, it follows from Theorem 5.22 that Xn!
p 1.
□
With reference to Example 5.33, it would seem reasonable to characterize
the convergence in probability by stating that Xn!
p 0, because mn¼ (1  .5n)/n
! 0. More generally, if Xn  mn!
p 0 and mn! c, we can alternatively state that
Xn!
p c as indicated below.
Theorem 5.23
WLLN via Convergence
of Difference
Xn  mn!
p 0 and mn! c ) Xn!
p c.
5.7.2
Strong Laws of Large Numbers (SLLNs)
As in the case of the WLLN, there is a variety of conditions that can be placed on
the random variables in the sequence {Xn} to ensure either
Xn !
as m (when
5.7
Laws of Large Numbers
257

E(Xi) ¼ m, 8 i) or Xn  mn !
as 0 (when E(Xi) ¼ mi, 8 i). The conditions relate to the
independence, homogeneity of distribution, and/or variances and covariances of
the random variables in the sequence. We note to the reader that the results are
somewhat more difﬁcult to establish than in the case of WLLNs.
The basic idea underlying strong laws of large numbers is to have the joint
distribution of Yn ¼ n1 Pn
i¼1 Xi  mi
ð
Þ, n ¼ 1,2,. . ., be such that the convergence
event yn ! 0 is assigned probability 1. For this to happen, it is known from
Theorem 5.14, as well as from the basic concept of a limit itself, that the event
{|yi| < e, 8i  n} must have a probability approaching 1 as n ! 1 8 e > 0. Thus the
marginal distribution of the (inﬁnite) set of random variables {Yn, Yn+1, Yn+2,. . .}
must be approaching degeneracy on a zero vector as n ! 1. Through various
constraints on the spread (variance) and/or degree of dependence of the underly-
ing random variables in the sequence {Xi}, this degenerate behavior can be
attained.
5.7.2.1
IID Case
We begin examining SLLNs by focusing on the iid case, and establishing a result
known as Kolmogorov’s inequality, which can be interpreted as a generalization
of Markov’s inequality.
Theorem 5.24
Kolmogorov’s
Inequality
Let X1,. . .,Xn be independent random variables for which E(Xi) ¼ 0 and s2
i < 1
8i. Then 8 e > 0, P
max
1  m  n j P
m
i¼1
xi j  e


 P
n
i¼1
s2
i
e2.
Note that with n ¼ 1 and X1 deﬁned to be a nonnegative-valued random
variable with zero mean, Theorem 5.24 is a statement of Markov’s inequality.
Kolmogorov’s inequality leads to Kolmogorov’s SLLN for iid random variables.
Theorem 5.25
Kolmogorov’s SLLN
Let {Xn} be a sequence of iid random variables such that E(Xi) ¼ m and var(Xi) ¼
s2 < 1, 8 i. Then Xn!
as m.
Example 5.35
Almost-Sure
Convergence of Xn
from iid Experimental
RVs
Let {Xn} be a sequence of iid exponentially distributed random variables, Xi ~
y1 exp(xi/y) I(0,1)(xi) 8i, and assume y c < 1. Note that E(Xi) ¼ m ¼ y
and var(Xi) ¼ s2 ¼ y2  c2 < 1, 8i. Theorem 5.25 applies, so that Xn !
as y.
□
The existence of variances is not necessary for a SLLN to hold in the iid case.
The following theorem provides necessary and sufﬁcient conditions for a SLLN
to hold.
Theorem 5.26
Kolmogorov’s SLLN (No
Variance Existing)
Let {Xn} be a sequence of iid random variables. Then the condition E(Xi) ¼ m <
1 is necessary and sufﬁcient for Xn !
as m.
Example 5.36
Illustration of SLLN
When Variance Does
Not Exist
Recall Example 5.31, where {Xn} was a sequence of iid random variables for
which E(Xi) ¼ 2 and for which the variance of the Xi’s did not exist. Nonetheless,
by Kolmogorov’s SLLN, we know that Xn !
as 2.
□
258
Chapter 5
Basic Asymptotics

Theorem 5.26 provides stronger support for the relative-frequency deﬁnition
of probability than do the WLLNs (recall Theorem 5.20), as the following theo-
rem indicates.
Theorem 5.27
Almost-Sure
Convergence of
Relative Frequency
Let {S,ϒ,P} be the probability space of an experiment, and let A be any event
contained in S. Let an outcome of NA be the number of times that event
A occurs in n independent and identical repetitions of the experiment. Then
the relative frequency of event A occurring is such that (NA/n) !
as P(A).
Theorem 5.27 implies that the relative frequency of the occurrence of event A
achieves a limit with probability 1 as n ! 1. Furthermore, the value of this limit
equalstheprobabilityoftheeventA.Thus,theSLLNprovidesstrongsupportfor the
notion that the relative frequency of the occurrence of an event can be used as the
measure of the probability of the event as n ! 1, since we are essentially certain
that the relative frequency of an event will converge to the probability of the event.
5.7.2.2
Non-IID Case
There are many other ways that restrictions can be placed on {Xn} so that Xn  mn
!
as 0 or Xn!
as m. We will present a SLLN for the non-identically distributed case
which can be applied whether or not the random variables in {Xn} are indepen-
dent. The theorem utilizes the concept of an asymptotic nonpositively
correlated sequence, deﬁned below.
Deﬁnition 5.8
Asymptotic
Nonpositively
Correlated Sequence
The sequence of random scalars {Xn}, where var(Xi) ¼ s2
i < 1 8i, is said to be
asymptotic nonpositively correlated if there exists a sequence of constants
{an} such that ai ∈[0,1] 8 i, P1
i¼0 ai < 1, and cov(Xi, Xi+t)  atsisi+t 8 t>0.7
Note that for P1
i¼0 ai to be ﬁnite when ai ∈[0,1] 8i, it must be the case that
an ! 0 as n ! 1. Since the at’s represent upper bounds to the correlations
between Xi and Xi+t, the deﬁnition implies that Xi and Xi+t cannot be positively
correlated when t ! 1.
Example 5.37
An Asymptotic
Nonpositively
Correlated Sequence
Let the sequence of random variables {Xn} adhere to the (ﬁrst-order) autocorrela-
tion process8:
ðIÞ Xi ¼ r Xi1 þ ei;
7Some authors use the terminology “asymptotically uncorrelated” for this concept (e.g., H. White, Asymptotic Theory, pp. 49).
However, the concept does not rule out negative correlation.
8This is an example of a stochastic process that we will revisit in our discussion of the general linear model in Chapter 8. “Stochastic
process” means any collection of random variables {Xt; t∈T}, where T is some index set that serves to order the random variables in the
collection. Special cases of stochastic processes include a scalar random variable when T ¼ {1}, an n-variate random vector when
T ¼ {1,2,. . .,n}, and a random sequence when T ¼ {1,2,3,. . .}.
5.7
Laws of Large Numbers
259

where the ei’s are iid with E(ei) ¼ 0 and var(ei) ¼ s2 2 (0,1), cov (Xi1, ei) ¼ 0 8 i,
X0 ¼ 0, and r
j j< 1. It follows that E(Xi) ¼ 0 8 i, and var(Xi) ¼ s2 Pi
j¼1 r2 j1
ð
Þ for
i  1. Note further that (I) implies Xiþt ¼ rt Xi þ S
t1
j¼0 rj etþij for t  1. To deﬁne
the value of corr(Xi+t,Xi), let s2
i ¼ var(Xi), and note that
corr Xiþt; Xi
ð
Þ ¼ E Xiþt E Xiþt
ð
Þ
ð
Þ Xi  E Xi
ð
Þ
ð
Þ
ð
Þ= siþt si
ð
Þ
¼ E
rt Xi  E Xi
ð
Þ
ð
Þ
ð
Þ
X
t1
j¼0
ri etþij
 
!
Xi  E Xi
ð
Þ
ð
Þ= siþt si
ð
Þ
¼ rt si = siþt 
0
if rt  0
rt
if rt >0

where the last inequality follows from the fact that
s2
i =s2
iþt ¼
X
i
j¼1
r2ðj1Þ =
X
iþt
j¼1
r2ðj1Þ  1 8 t  1:
To demonstrate that {Xn} is an asymptotic nonpositively correlated sequence,
deﬁne at ¼ rt if rt > 0 and at ¼ 0 if rt  0, so that corr(Xi+t, Xi)  at 8 t, and at
∈[0,1], 8 t. Then, since P
1
t¼1
at  P
1
t¼1
jr jt ¼ jrj=ð1  jrjÞ<1, the sequence {Xn} is
asymptotic nonpositively correlated.
□
Theorem 5.28
SLLN: Non IID Case
Let {Xn} be a sequence of random variables such that E(Xi) ¼ mi, var(Xi)  b < 1
8i, and {Xn} is asymptotic nonpositively correlated. Then Xn  mn!
as 0.
The theorem indicates that a sequence of random variables will adhere to a
SLLN if the variances of the random variables are bounded, and if any positive
correlation between random variables in the sequence eventually dissipates
when the random variables are far enough apart in the sequence.
Example 5.38
Plim Xn
ð
Þ for
Asymptotically
Nonpositively
Correlated Sequence
Recall Example 5.37, where it is known that {Xn} is asymptotically nonpositively
correlated. Note further that since r∈[0,1), s2
i ¼ s2 Pi
j¼1 r2 j1
ð
Þ< 1 8 i, where in
fact P1
j¼1 r2 j1
ð
Þ ¼ 1 + r2 + r4 + . . . ¼ 1/(1  r2), so that s2
i  1/(1  r2) 8 i. Since
the s2
i ’s are upper-bounded, the conditions of Theorem 5.28 are met, and it
follows that Xn !
as 0, since E(Xi) ¼ m ¼ 0 8 i.
□
Our ﬁnal result on SLLNs concerns whether Xn  mn!
as 0 and mn ! c together
imply that Xn !
as c. The answer is yes, as stated in the next theorem.
Theorem 5.29
SLLN via Convergence
of Difference
Xn  mn !
as 0 and mn ! c ) Xn !
as c
260
Chapter 5
Basic Asymptotics

5.8
Central Limit Theorems
Central limit theorems (CLTs) are concerned with the conditions under which
sequences of random variables converge in distribution to known families of
distributions. We will focus primarily on results concerning convergence in
distribution of sequences of random variables {Yn} of the following form9:
Yn ¼ b1
n
Sn  an
ð
Þ!
d N 0; S
ð
Þ;
where {Sn} is a sequence of scalar or vector random variables whose nth term is
deﬁned by Sn ¼ Pn
i¼1 Xi, {Xn} is a sequence of scalar or vector random variables,
and {an} and {bn} are suitably chosen sequences of real numbers, vectors, or
matrices. A statement of conditions on {Xn}, {an}, and {bn} for which the conver-
gence in distribution result holds true constitutes a central limit theorem.
As we remarked in the introduction to the preceding section dealing with
laws of large numbers, the reader may wonder why the particular problem
concerning convergence in distribution deﬁned above deserves such explicit
attention. The answer lies in the fact that a large number of important parameter
estimation and hypothesis-testing procedures in econometrics and statistics are
deﬁned as functions of sums of random variables (note the Sn term in the
convergence problem above). Central limit theorems are then often useful for
establishing asymptotic distributions for these procedures, as will be seen in
speciﬁc examples in subsequent chapters.
In order to illustrate the general way in which the use of a CLT might
arise in practice, suppose a CLT is applicable to a scalar random variable, say, as Yn
¼ b1
n
Sn  an
ð
Þ!
d Y  N 0; 1
ð
Þ. Then, since Sn ¼ g Yn; an; bn
ð
Þ ¼ bnYn þ an, we can
deﬁne an asymptotic distribution for Sn using Deﬁnition 5.2 as Sn 
a bnY þ an, or
Sn 
a N an; b2
n


. Thus, for large n; Sn ¼ Pn
i¼1 Xi would have an asymptotic distri-
bution that is normal, with mean an and variance b2
n. Now suppose a particular
statistical procedure is based on the random variable Wn ¼ h(Sn;cn) ¼ h(g(Yn;an,
bn);cn). Then Wn 
a h g Y; an; bn
ð
Þ; cn
ð
Þ, so that for large n, an asymptotic distribu-
tion for Wn is given by the distribution associated with the composite function
h  g of the standard normal random variable, Y, or equivalently by the distri-
bution associated with the function h of the random variable Sn under the
assumption that Sn ~ N(an, b2
n). For example, if Wn 
a h g Y; an; bn
ð
Þ; cn
ð
Þ ¼ cnY2,
then Wn 
a Gamma 1=2; 2cn
ð
Þ (since Y2  w2
1 , and then cnY2 has a gamma
distribution with a ¼ 1/2 and b ¼ 2cn).
Deﬁning asymptotic distributions for random variables of interest is most
useful, and sometimes indispensable, when the exact distributions of the ran-
dom variables are very difﬁcult or impossible to derive. Furthermore, even
if the exact distributions of random variables of interest can be deﬁned, they
9The reader who wishes to read about central limit theory in its most general form can examine Chapter 5 of R.G. Laha, and V.K.
Rohatgi (1979), Probability Theory, New York: John Wiley.
5.8
Central Limit Theorems
261

may be very difﬁcult to work with, whereas the asymptotic distribution may be
relatively easy to analyze. For both of the aforementioned reasons, central limit
theorems ﬁgure prominently in the development of econometric and statistical
theory and application.
We divide our presentation of CLTs into three subsections. The ﬁrst subsec-
tion deals with the case of independent scalar random variables. The second
subsection provides an introduction to the case where the scalar random
variables in {Xn} are not independent. In the ﬁnal subsection, we present some
CLT results relating to multivariate random variables.
5.8.1
Independent Scalar Random Variables
We examine three CLTs for independent random variables beginning with the
simplest but least general iid case. We end with a CLT that presents necessary
and sufﬁcient conditions forb1
n (Sn  an)!
d N(0,1) when the Xi’s are independent
but not necessarily identically distributed.
5.8.1.1
IID Case
We begin with the simplest of all CLTs, the Lindberg-Levy CLT.
Theorem 5.30
Lindberg-Levy CLT
Let {Xn} be a sequence of iid random variables with E(Xi) ¼ m and var(Xi) ¼ s2
∈(0,1) 8 i. Then,
n1=2s

1 Xn
i¼1 Xi  nm


¼ n1=2 Xn  m


s
!
d N 0; 1
ð
Þ:
In order to enhance one’s intuitive understanding of why the Lindberg-Levy CLT
(LLCLT) holds, we provide some additional rationale for the result, albeit at the
expense of some degree of imprecision in the mathematical details. First note
that under the conditions of the LLCLT, Pn
i¼1 Xi is a random variable that has
a mean of nm and a variance of ns2. Since both |nm| and ns2 diverge to 1 (assuming
m 6¼ 0) as n increases, it is clear that some form of centering and scaling of Pn
i¼1 Xi
will be necessary for there to be any hope of convergence to some limiting distri-
bution. By subtracting nm and then dividing by n1/2s, one deﬁnes random variables
Yn ¼ n1=2s

1 Pn
i¼1 Xi  nm


¼ n1=2s

1 Pn
i¼1 Zi which have a mean zero and
variance of 1 regardless of n. The random variables Zi ¼ Xi  m, i ¼ 1,. . .,n, are iid
random variables with zero means and variances all equal to s2.
Now a key observation concerning an additional effect of the aforemen-
tioned centering and scaling: when n ! 1, any effect of third- and higher-
order moments of Zi on the moments of Yn are “centered and scaled away” so
that all probability distributions for Zi that have the same mean and variance
will lead to precisely the same moments for Yn as n ! 1 (we assume that all
moments of the Zi’s exist). To see this, ﬁrst consider the third moment of Yn:
E Y3
n


¼
n1=2 s

3 X
n
i¼1
X
n
j¼1
X
n
k¼1
EðZi Zj ZkÞ:
Since the Zi’s are independent with zero means, it is only the case where i ¼ j
¼ k that the expectation term is nonzero and equal to m0
3, the third moment of
the distribution of the Zi’s. But there are only n of these terms, and thus
262
Chapter 5
Basic Asymptotics

E Y3
n


¼ s3 n3=2 ðnm0
3Þ ! 0 as n ! 1, and the third moment of Yn converges to
zero regardless of m0
3 . Following analogous logic applied to E Y4
n


, which is
deﬁned in terms of a quadruple sum of (Zi Zj Zk Z‘) terms premultiplied by
(n1/2s)4, it can be shown that E Y4
n


¼ s4 n2 nm0
4 þ 3nðn  1Þ s4


! 3 regard-
less of m0
4, the fourth moment of Zi. This type of argument can be continued ad
inﬁnitum to show that all higher order moments of Yn converge to known
constants and that the values of higher order moments of the Zi’s play no role
in determining any of the moments of Yn when n ! 1.
Now observe that the ﬁrst four moments of Yn, as deﬁned in the previous
paragraph, converge to the ﬁrst four moments of the standard normal distribu-
tion, 0, 1, 0, and 3, respectively. Furthermore, all higher-order moments of Yn
also converge to those of the standard normal distribution, which can in princi-
ple be veriﬁed one by one following the approach deﬁned above, and in any case
is implied by the proof of Theorem 5.30. While not true for all densities,
members of the normal family of PDFs are uniquely identiﬁed by their moment
sequences,10 so that the moments of Yn are uniquely consistent with that of a
standard normal density as n ! 1 for all underlying probability distribution of
the Xi’s having mean m and variance s2. Thus, the centering and scaling of the
Xi’s inherent in the deﬁnition of Yn remove any tendencies for higher-order
moments of the Xi’s to cause the moments of Yn to deviate from those of a
standard normal distribution as n ! 1, leading to convergence of Yn to the
N(0,1) limiting distribution.
The establishment of the LLCLT provides an opportunity to revisit the
relationship between limiting distributions and asymptotic distributions.
Under the conditions of the LLCLT, Yn ¼ (n1/2 s)1 (Pn
i¼1 Xi  nm) !
d
N(0,1).
Then, following Deﬁnition 5.2, an asymptotic distribution for Sn ¼ Pn
i¼1 Xi can
be deﬁned by noting thatSn ¼ n1=2s


Yn þ nm 
a
n1=2s


Y þ nm whereY  N 0; 1
ð
Þ,
so that Sn 
a N nm; ns2


. Thus, the normal distribution with mean nm and
variance ns2 provides an approximation to the distribution of Sn when n is
large. Regarding Xn , note that the conditions underlying the Lindberg-Levy
CLT imply Xn !
p m by Khinchins’ WLLN, so that Xn !
d m. Because the limiting
distribution of
Xn
is degenerate, it is clear that the distribution provides
no information about the variability of Xn for ﬁnite n. The asymptotic distribu-
tion for Xn is more useful in this regard. Noting that Xn ¼ g(Yn,n) ¼ (s/n1/2)
Yn þ m, it follows from Deﬁnition 5.2 that Xn 
a g Y; n
ð
Þ ¼ s=n1=2


Y þ m for Y ~
N(0,1), or Xn 
a N m; s2=n


. Thus, for large n, Xn is approximately normally
distributed with mean m and variance s2/n. The following examples illustrate
the application of the Lindberg-Levy CLT.
10M. Kendall and A. Stuart (1977), The Advanced Theory of Statistics, Volume I, New York: Macmillan, pp. 115. Note that any random
variable for which an MGF exists is such that its moment sequence uniquely identiﬁes its probability distribution.
5.8
Central Limit Theorems
263

Example 5.39
Approximating
Binomial Probabilities
via the Normal
Distribution
Let {Xn} be a sequence of iid Bernoulli-type random variables, i.e., Xi~
pxi ð1  p Þ1xi I{0,1}(xi) 8 i with p 6¼ 0 or 1. Then, by the Lindberg-Levy CLT,
Pn
i¼1 Xi  np
n1=2 ½pð1  pÞ 1=2 !
d Nð0; 1Þ; and
Xn
i¼1 Xi 
a Nðnp; npð1  pÞÞ
Since Pn
i¼1 Xi has a binomial distribution under the stated conditions, we
have discovered an alternative to the Poisson density for approximating the
binomial distribution for large n. Note that we are approximating the discrete
Binomial density with the aforementioned continuous Normal density. It has
been found in practice that such approximations are improved, especially when
n is not very large, by making a continuity correction, whereby each outcome, x,
in the range of the discrete random variable is associated with the interval event
(x  (1/2), xþ(1/2)] for the purpose of assigning probability via the asymptotic
normal density. For example, if n ¼ 40, p ¼ 1/2, and x ¼ 20, then since m ¼ np
¼ 20 and s2 ¼ np(1p) ¼ 10, we have, using the normal asymptotic density
N(20,10),
Pðx ¼ 20Þ 
R 20:5
19:5 Nðz; 20; 10Þdz ¼
R :5= ﬃﬃﬃﬃ
10
p
:5= ﬃﬃﬃﬃ
10
p
Nðz; 0; 1Þdz ¼ :1272 ðfrom
standard normal tableÞ. The actual probability assigned to the event P(x ¼ 20)
by the binomial density is P(x ¼ 20) ¼
40
20
 
!
1
2
 40 ¼ .1254.
□
Example 5.40
Approximating
x2 Probabilities via the
Normal Distribution
Let {Xn} be a sequence of iid chi-square random variables with 1 degree of
freedom, i.e., Xi ~ w2
1 8 i. By the additivity property of chi-square random
variables, P
n
i¼1
Xi  w2
n . Also E(Xi) ¼ 1 and var(Xi) ¼ 2 8 i under the prevailing
assumptions. Then, by the Lindberg-Levy CLT,
Yn ¼
P
n
i¼1
Xi  n
ð2nÞ1=2
!
d Nð0; 1Þ and
X
n
i¼1
Xi 
a N n; 2n
ð
Þ:
We have thus discovered an approximation to the w2 density function for large
degrees of freedom. As an example of its use, note that (from tables of the
w2 distribution) P ðw2
30  43.8) ¼ .95. We obtained our approximation of
this probability by utilizing
Z30 
a N 30; 60
ð
Þ , and thus
Pðz30  43:8Þ ¼
P
z30 30
ð60 Þ1=2  43:830
ð60 Þ1=2


¼ Pðz  1:783Þ ¼ :9627 ðwhere Z  Nð0; 1ÞÞ
□
The Lindberg-Levy CLT implies that any real-world experiment whose ﬁnal
outcome can be conceptualized as the result of a summation or average of the
outcomes of a large number of iid random variables having a ﬁnite mean and
variance can be treated as having approximately a normal distribution. Thus, for
example, the total number of defective objects produced on an assembly line or
the average miles per gallon achieved by a sample of Ford pickup trucks might be
considered as approximately normally distributed to the extent that the inde-
pendence and identical distribution assumptions hold true.
264
Chapter 5
Basic Asymptotics

A natural question to ask in using asymptotic distributions is how large does n
have to be for the approximation to be a good one? Unfortunately, the answer
depends on the characteristics of the true distribution underlying the sequence of
random variables, and no general answer can be given. However, a number
of inequalities have been developed that can be useful in answering the question if
something is known about the moments of the underlying densities. Speciﬁcally,
under the conditions of the Lindberg-Levy CLT, Van Beeck11 has shown that
the maximum absolute difference between P(yn  c) for the actual density of Yn ¼
n1/2(Xn  m)/s andfor thatof thestandardnormaldensityis .7975(z3/s3)n1/2where
z3 ¼ E
X  EðXÞ
j
j3


and s refer to the standard deviation of the common density of
theXi’s.Asexamplesofthebound,iftheXi’sareiidBernoulliorexponentialrandom
variables, then the bounds are respectively .7975 n1/2[12p(1p)][p(1p)]1/2 and
.1653n1/2. Then, for example, if p ¼ .5 and n ¼ 1,000, the upper bounds on the
errors when approximating P(yn  c) via the standard normal limiting distribution
are .025 and .005, respectively. It should be emphasized that VanBeeck’s result
provides omnibus bounds that apply to all random variables having the prescribed
moments, and as such the bound tends to be quite conservative, that is, the actual
approximation errors are generally much smaller than the bound.
5.8.1.2
Non-IID Case
While the Lindberg-Levy CLT is applicable in many experimental situations, it
has the disadvantage of requiring that all of the random variables have the same
mean, the same variance, and moreover, the same probability distribution.
Various other central limit theorems can be constructed that utilize alternative
conditions on the distributions of the random variables in the sequence {Xn}.12
The most general CLT for the case of independent random variables, which
subsumes the LLCLT as a special case, is the Lindberg CLT.
Theorem 5.31
Lindberg’s CLT
Let {Xn} be a sequence of independent random variables with E(Xi) ¼ mi and
var(Xi) ¼ s2
i < 1 8 i. Deﬁne b2
n ¼ Pn
i¼1 s2
i ; s2
n ¼ n1 Pn
i¼1 s2
i ; mn ¼ n1 Pn
i¼1 mi, and
let fi be the PDF of Xi. If 8 e > 0,
lim
n!1
1
b2
n
X
n
i¼1
ð
ximi
ð
Þ2  eb2
n
xi  mi
ð
Þ2fi xi
ð
Þdxi ¼ 0 ðcontinuous caseÞ
lim
n!1
1
b2
n
X
n
i¼1
X
ximi
ð
Þ2  eb2
n; fi xi
ð
Þ>0
xi  mi
ð
Þ2fi xi
ð
Þ ¼ 0; ðdiscrete caseÞ
then
Pn
i¼1 Xi  Pn
i¼1 mi
Pn
i¼1 s2
i

1=2
¼ n1=2 Xn  mn


sn
!
d N 0; 1
ð
Þ:
11P. Van Beeck (1972), An application of Fourier methods to the problem of sharpening the Berry-Esseen Inequality, Z. Wahrschein-
lichkeits Theorie und Verw. Gebiete 23, pp. 187–196.
12For example, see Y.S. Chow and H. Teicher, Probability Theory, (1978) Chapter 9, and R.G. Laha and V.K. Rohatgi, (1976) Probability
Theory, Chapter 5).
5.8
Central Limit Theorems
265

It can be shown that the limit conditions in the Lindberg CLT, known as the
Lindberg conditions, imply that lim
n!1 s2
j = Pn
i¼1 s2
i


¼ 0 8j. That is, the contri-
bution that each Xj makes to the variance of Pn
i¼1 Xi is negligible as n ! 1. The
Lindberg conditions can be difﬁcult to verify in practice, and so we will present
two useful special cases of the Lindberg CLT that rely on more easily veriﬁable
conditions. The ﬁrst special case essentially implies that if the random variables
in the sequence {Xn} are independent and bounded with probability 1, then
Yn ¼ n1/2 ðXn  mnÞ=sn !
d N(0,1). It will be seen that the boundedness condition
and Pn
i¼1 s2
i ! 1 imply the Lindberg condition.
Theorem 5.32
CLT for Bounded
Random Variables
Let {Xn} be a sequence of independent random variables such thatP xi
j
j  m
ð
Þ ¼ 1
8 i for some m ∈(0,1), with E(Xi) ¼ mi and var(Xi) ¼ s2
i < 1 8i. If Pn
i¼1 var(Xi) ¼
Pn
i¼1 s2
i ! 1 as n ! 1, then n1/2 ðXn  mnÞ=sn !
d N(0,1).
The following example illustrates the discovery of an asymptotic distribu-
tion for a simple form of the least-squares estimator, which we will examine in
more detail in Chapter 8.
Example 5.41
Asymptotic Distribution
for a Least Squares
Estimator
Let the sequence {Yn} be deﬁned by Yi ¼ zi b + ei, where:
a. b is a real number,
b. zi is the ith element in the sequence of real numbers {zn} for which n1 Pn
i¼1 z2
i
>a>0 8n and zi
j j< d < 1 8 i, and
c. ei is the ith element in a sequence of iid random variables, {en}, for which
E(ei) ¼ 0, var(ei) ¼ s2 ∈(0,1), and P( ei
j j m) ¼ 1 8 i, where m ∈(0,1).
Given the preceding assumptions, an asymptotic distribution for the least-
squares estimator of b deﬁned by ^bn ¼ Pn
i¼1 ziYi= Pn
i¼1 z2
i
can be deﬁned as
follows:
We transform the problem into a form that allows both an application of a
CLT and an application of Deﬁnition 5.2 to deﬁne an asymptotic distribution for
^bn. Note that
ð^bn  bÞ
X
n
i¼1
ziðYi  zibÞ=
X
n
i¼1
z2
i ¼
X
n
i¼1
zi ei=
X
n
i¼1
z2
i ;
so that
X
n
i¼1
z2
i
 
!1=2
^bn b


s
¼
X
n
i¼1
ziei
s2 Pn
i¼1 z2
i

1=2 ¼
X
n
i¼1
Wi
Pn
i¼1 var Wi
ð
Þ

1=2
where Wi ¼ ziei. The CLT of Theorem 5.32 is applicable to this function of
^bn : To see this, observe that E(Wi) ¼ 0 and var(Wi) ¼s2z2
i  s2d2 < 1 8 i.
Also, P( ziei
j
j  dm) ¼ P( wi
j
j dm) ¼ 1 8 i, and Pn
i¼1 var Wi
ð
Þ ¼ s2 Pn
i¼1 z2
i ! 1
266
Chapter 5
Basic Asymptotics

since n-1 Pn
i¼1 z2
i > a > 0 8 n. Then, by Theorem 5.32,
P
n
i¼1
z2
i

1=2
^bn  b


s
!
d
N(0,1), and by Deﬁnition 5.2, ^bn
a N
b; s2
P
n
i¼1
z2
i

1
 
!
.
□
The Liapounov CLT relaxes the boundedness assumption of the previous
CLT. In this case, a condition on the moments of the random variables in the
sequence {Xn} is used to imply the Lindberg conditions instead of boundedness of
the random variables.
Theorem 5.33
Liapounov CLT
Let {Xn} be a sequence of independent random variables such that E(Xi) ¼ mi
and var(Xi) ¼ s2
i < 1 8 i. If, for some d > 0,
lim
n!1
Pn
i¼1 E
Xi  mi
j
j2þd


Pn
i¼1 s2
i

1þd=2
¼ 0;
then
Pn
i¼1 Xi  Pn
i¼1 mi
Pn
i¼1 s2
i

1=2
¼ n1=2 Xn  mn


sn
!
d N 0; 1
ð
Þ:
An important implication of Liapounov’s CLT is that Pn
i¼1 Xi need not be a sum
of identically distributed nor bounded random variables to have an asymptotic
normal density. Under the conditions of the theorem, it follows that
Sn ¼ P
n
i¼1
Xi 
a N
S
n
i¼1 mi; S
n
i¼1 s2
i


and Xn 
a N mn; n1s2
n


.
Example 5.42
Limiting Distribution
for Function of Non-IID
Uniform RVs
Let {Xn} be a sequence of independent uniformly distributed random variables
such that Xi ~
1
2ci I½ ci;ci (xi) 8 i, where ci ∈[t,m], and 0 < t < m < 1. Then for
d > 0, E(|Ximi|2+d)  m2+d 8i, and var(Xi) ¼ c2
i =3  t2=3>0 8 i. Letting d ¼ 1 in
Liaponov’s CLT,
lim
n!1
Pn
i¼1 E Xi  mi
j
j3
Pn
i¼1 s2
i

3=2
 lim
n!1
ﬃﬃﬃ
3
p
m
t
 
!3
n1=2 ¼ 0;
so that (note mi ¼ 0 8i)
Yn ¼
Pn
i¼1 Xi
Pn
i¼1 c2
i =3



1=2 !
d Nð0; 1Þand Xn 
a N
0; n2 X
n
i¼1
c2
i
3


 
!
:
□
As an illustration of how either the CLT for bounded random variables or the
Liapounov CLT might be applied in practice, consider a short-run production
process of a ﬁrm and focus on the effect of labor input on production. There may
exist a systematic engineering relationship between the quantity of labor applied
5.8
Central Limit Theorems
267

to the complement of plant equipment and the expected output of the plant, say
as y ¼ f(L), for any given time period of plant operation. However, it would
undoubtedly be rare that the exact quantity of production expected in an engi-
neering sense will actually be realized. Variations from the expected quantity
could occur due to variations in the health, alertness, and general performance
level of each of the various employees, the extent of machine failures in any
given time period and their general performance level, the varying ability of
management to schedule production efﬁciently on any given day, weather
conditions if the production process is affected thereby, and so on. Viewing the
overall deviation of total production from the engineering relationship as caused
by the summation of a large number of random deviations with ﬁnite absolute
upper bounds caused by various uncontrolled factors results in a production
relationship of the form Y ¼ f(L) + e where e, and thus Y, has an asymptotic
normal density (assuming the summation of the random deviations can be
viewed
as
a
sum
of independent,
although
not
necessarily
identically
distributed, random variables).
As in the case of the Lindberg-Levy CLT, bounds have been established on
the maximum absolute deviation between the actual value of P(yn  c) for
Yn ¼ n1/2(Xnmn)/sn, and the approximated value based on the standard normal
distribution. Speciﬁcally, Zolotarev13 has shown such an upper bound to be
.9051
P
n
j¼1
x3j
 
!
P
n
j¼1
s2
xj
 
!3=2
, where x3j ¼E
Xj  E Xj



3


and s2
xj refers to the
variance of the jth random variable Xj. Thus, if one knew the appropriate
moments of the random variables X1,. . .,Xn, a bound on the approximation
error could be ascertained. If such information were not available, the researcher
knows only that as n increases, the accuracy of the approximation improves. As
we had remarked in our discussion of similar bounds for the LLCLT, these
bounds tend to be conservative and the actual approximation errors tend to be
signiﬁcantly less than the value of the bound.
5.8.2
Triangular Arrays
In analyzing the asymptotic properties of some types of econometric or statisti-
cal procedures, it is useful to be able to apply central limit theory to what is
known as a double array of random variables. For our purposes, it will be
sufﬁcient to examine a special case of such a double array, called a triangular
array of random variables.14
13M. Zolotarev (1967), A Sharpening of the Inequality of Berry-Esseen. Z. Wahrscheinlichkeits Theorie und Verw. Gebiete, 8 pp.
332–342.
14A double array is one where the second subscript of the random variables in the ith row of the array ends with the value ki, rather
than with the value i as in the case of the triangular array, and kn ! 1 as n ! 1. See Serﬂing, Approximation Theorems, pp. 31.
268
Chapter 5
Basic Asymptotics

Deﬁnition 5.9
Triangular Array of
Random Variables
The ordered collection of random variables {X11,X21,X22,X31,X32,X33,. . .,
Xnn,. . .}, or
X11;
X21
X22;
X31
X32
X33;
...
...
...
..
.
Xn1
Xn2
Xn3
Xn4
. . .
Xnn;
..
.
..
.
..
.
..
.
..
.
..
.
..
.
is called a triangular array of random variables, and will be denoted by {Xnn}.
Central limit theorems that are applied to triangular arrays of random
variables are concerned with the limiting distributions of appropriately deﬁned
functions of the row averages
XðnÞ ¼n1 Pn
i¼1 Xni . Note that all of the
CLTs examined so far have dealt with functions of averages of the type Xn ¼
n1 Pn
i¼1 Xi , the Xi’s being elements of the sequence {Xn}. It is possible that
XðnÞ ¼ Xn 8 n, which would occur if xij ¼ xj 8 i,j, that is, all of the elements in
any given column of the triangular array are identical. Thus, the CLT results
obtained heretofore apply to this special case of a triangular array. However, the
triangular array {Xnn} is more general than a sequence {Xn} in the sense that the
random variables in a row of the array need not be the same as random variables
in other rows. Furthermore, previously Xn always involved all of the random
variables in the sequence {Xn} up to the nth element, whereas in the triangular
array, XðnÞ involves only the random variables residing in the nth row of the
triangular array. The importance of this ﬂexibility will become apparent when
we analyze the asymptotic behavior of certain statistical procedures for which
central limit theory can only be effectively applied in the context of triangular
arrays of random variables.
All of the CLTs presented heretofore can be extended to the case of triangular
arrays. We present here the extension of the Liapounov CLT. For additional
details on such extensions, the reader can refer to the book by K.L. Chung
cited in the proof of the theorem. Henceforth, we will let mðnÞ ¼ n1 Pn
i¼1 mni
and s2 (n) ¼ n1 Pn
i¼1 s2
ni.
Theorem 5.34
Liapounov CLT for
Triangular Arrays
Let {Xnn} be a triangular array of random variables with independent random
variables within rows. Let E(Xij) ¼ mij and var(Xij) ¼ s2
ij < 1 8 i,j. If for some
d>0; lim
n!1
Pn
i¼1 EjXni  mni j2þd
Pn
i¼1 s2
ni

1þd=2
¼ 0;
then
Pn
i¼1 Xni  Pn
i¼1 mni
Pn
i¼1 s2
ni

1=2
¼ n1=2 XðnÞ  mðnÞ


=sðnÞ!
d Nð0; 1Þ:
5.8
Central Limit Theorems
269

Note that the random variables within a row of the triangular array are
assumed to be independent in the Liapounov CLT, but no such assumption is
required for random variables in different rows. In fact, the random variables
within a given row can be arbitrarily dependent on random variables in other
rows.
The following example applies the Liapounov CLT for triangular arrays to
establish the asymptotic normality of the least-squares estimator under more
general conditions than those utilized in Example 5.41.
Example 5.43
Asymptotic Distribution
for a Least Squares
Estimator with
Unbounded Residuals
Let the sequence {Yn} be deﬁned by yi ¼ zi b + ei, and assume the conditions of
Example 5.41, except replace the boundedness assumption P(|ei|  m) ¼ 1 8i
with the moment assumption that E|ei|2 + d  m < 1 8i for some d > 0. Then
the least-squares estimator ^bn ¼ Pn
i¼1 ziYi= Pn
i¼1 z2
i remains asymptotically nor-
mally distributed. To see this, note that
X
n
i¼1
z2
i
 
!1=2
^bn  b


s
¼
X
n
i¼1
ziei
 
!
= s2 X
n
i¼1
z2
i
 
!1=2
¼
X
n
i¼1
Wni;
where the random variables Wni are elements of a triangular array15 for which
E(Wni) ¼ 0 and s2
ni ¼ varðWniÞ ¼ z2
i = Pn
i¼1 z2
i . Because Pn
i¼1 s2
ni ¼ 1 , the limit
condition of the Liapounov CLT for triangular arrays is met since, for d > 0
lim
n!1
X
n
i¼1
E
Wni
j
j2þd


¼ lim
n!1
X
n
i¼1
zi
j j2þdE
ei
j j2þd


= s2 X
n
i¼1
z2
i
 
!1þd=2
0
@
1
A
 lim
n!1 md2þd= s2a

1þd=2


nd=2 ¼ 0:
Therefore, Pn
i¼1 Wni !
d N(0,1), which then implies that^b 
a N
b; s2
P
n
i¼1
z2
i

1
 
!
:□
5.8.3
Dependent Scalar Random Variables
In this subsection we provide an introduction to the notion of deﬁning CLTs
when the random variables in the sequence {Xn} exhibit some degree of depen-
dence. Many different CLTs can be deﬁned by allowing different types of
dependencies among the random variables in {Xn}. CLTs for dependent random
variables are generally much more complicated to state and prove than CLTs for
independent random variables, and the level of mathematics involved is beyond
our scope of study. We will explicitly examine one useful CLT for a particular
type of dependence called m-dependence. The reader can ﬁnd additional results
on CLTs for the dependent random variable case in H. White, (1980) Asymptotic
Theory, Chapters 3 and 5 and in R.J. Serﬂing (1968), Contributions to central
limit theory for dependent variables, Ann. of Math. Stat. 39 pp. 1158–1175.
15Note the Wni, i ¼ 1,. . .,n, are independent because the ei’s are independent.
270
Chapter 5
Basic Asymptotics

Deﬁnition 5.10
m-Dependence
The sequence {Xn} is said to exhibit m-dependence (or is said to be m-depen-
dent) if, for a1<a2 < . . . < ak < b1 < b2 < . . . < br, the random variables ðXa1;
Xa2 . . . ; XakÞ are independent of ðXb1; Xb2 . . . ; XbrÞ whenever b1  ak > m.
The deﬁnition states that {Xn} is m-dependent if any two groups of random
variables separated by more than m positions in the sequence are independent of
one another. Combining m-dependence with boundedness of the random
variables in the sequence {Xn} leads to the following CLT.
Theorem 5.35
CLT for Bounded m-
Dependent Sequences
Let {Xn} be an m-dependent sequence of random scalars for which E(Xi) ¼ mi and
P(|xi|  c) ¼ 1 for some c < 1 8i. Let s2
n ¼ var Pn
i¼1 Xi


. If n2/3 s2
n ! 1, then
s2
n

1=2
X
n
i¼1
Xi 
X
n
i¼1
mi
 
!
!
d Nð0; 1Þ:
Regarding the variance condition stated in the theorem, note that if the Xi’s were
independent and s2
i  b > 0 8i, then s2
n  bn ! 1 at a rate of n, so that n2/3 s2
n
 bn1/3
!
1 at a rate n1/3. If the Xi’s are dependent, then as long as the
covariance terms in the determination of var Pn
i¼1 Xi


do not collectively
decrease the rate at which s2
n increases by a factor of n1/3 or more, the variance
condition of the CLT will hold. Thus, through restricting covariance terms, the
variance condition of Theorem 5.35 places restrictions on the extent of the
dependence that can exist between the Xi’s. Note the restriction is effectively
on negative covariances, since positive covariances actually increase the vari-
ance of Pn
i¼1 Xi.
Example 5.44
Asymptotic Distribution
for a Least Squares
Estimator with
Dependent
Observations
Let the sequence {Yn} be deﬁned by Yi ¼ zi b + ei, and assume the conditions of
Example 5.41, except replace the assumption var(ei) ¼ s2 ∈(0,1) with var(ei) ¼
s2
i  t > 0 8i, replace the assumption that the ei’s are iid with the assumption of
m-dependence, and in addition assume that zi > 0 8i and cov(ei, ej)  0 8i 6¼ j.
Applying Theorem 5.35, ^b 
a N
b; s2
n
P
n
i¼1
z2
i

2
 
!
:
To see this, note that
P
n
i¼1
z2
i


^bn b


¼ P
n
i¼1
ziei ¼ P
n
i¼1
Wi; where E(Wi) ¼ 0
and P(|ziei|  db) ¼ P(|wi|  c) ¼ 1 for c ¼ db < 1 8i. Also,
s2
n ¼ var
X
n
i¼1
Wi
 
!
¼
X
n
i¼1
s2
i z2
i þ
X X
ij
j
j  m; i6¼j
zizjcov ei; ej


since by m-dependence cov(ei, ej) ¼ 0 when |ij| > m. Because, zizj cov(ei, ej)
 0 8i and j,
s2
n  Pn
i¼1 s2
i z2
i  nat , so that n2/3
s2
n
 n1/3 at ! 1.
Thus, by Theorem 5.35,
P
n
i¼1
z2
i

 ^bn b


s2
n

1=2 ¼
X
n
i¼1
Wi
s2
n

1=2 !
d Nð0; 1Þ;
and
^bn 
a
N

b; s2
n
Pn
i¼1 z2
i

2
:
□
5.8
Central Limit Theorems
271

5.8.4
Multivariate Central Limit Theorems
The central limit theorems presented so far are applicable to sequences of
random scalars. Central limit theorems can be deﬁned for sequences of random
vectors, in which case conditions are established that ensure that an appropriate
(vector) function of the random sequence converges in distribution to a multi-
variate normal distribution. Due to a result discovered by H. Cramer and H.
Wold,16 termed the Cramer-Wold device, questions of convergence in distribu-
tion for a multivariate random sequence can all be reduced to the question of
convergence in distribution of sequences of random scalars, at least in principle.
Thus, all of the central limit theorems discussed to this point remain highly
relevant to the multivariate case.
Theorem 5.36
Cramer-Wold Device
The sequence of (k  1) random vectors {Xn} converges in distribution to the
random (k1) vector X iff ℓ0Xn!
d ℓ0X 8ℓ∈ℝk.
Note that in applying Theorem 5.36, ℓ0Xn!
d ℓ0X is always trivially true when
ℓ¼ 0, and so the condition ℓ0Xn!
d ℓ0X need only be checked for ℓ6¼ 0. We will be
most concerned with convergence in distribution to members of the normal
family of distributions. In this context, Theorem 5.36 implies that to establish
convergence in distribution of the sequence of random (k  1) vectors {Xn} to the
random (k  1) vector X ~ N(m,S), it sufﬁces to demonstrate that ℓ0Xn !
d N(ℓ0m,
ℓ0Sℓ) 8ℓ∈ℝk. We formalize this observation as a corollary to Theorem 5.36.
Corollary 5.4 Cramer-
Wold Device for
Normal Limiting
Distributions
Xn !
d N(m, S) iff ℓ0Xn !
d N(ℓ0m, ℓ0Sℓ) 8ℓ∈ℝk.
The Cramer-Wold device can be used to deﬁne multivariate central limit
theorems. The following is a multivariate extension of the Lindberg-Levy CLT.
Theorem 5.37
Multivariate Lindberg-
Levy CLT
Let {Xn} be a sequence of iid (k  1) random vectors with E(Xi) ¼ m and Cov(Xi) ¼
S 8 i, whereS isa(k  k)positivedeﬁnitematrix.Thenn1=2
n1 P
n
i¼1
Xi  m


!
d N(0,S).
It follows from the multivariate Lindberg-Levy CLT that Xn 
a N m; n1S


.
Example 5.45
CLT Applied to sum of
Bernoulli vectors
Shipments of CPU chips from two different suppliers are to be inspected before
being accepted. Chips are randomly drawn, with replacement, from each shipment
and are nondestructively tested in pairs, with (X1i, X2i) representing the outcome of
the tests for pair i. An outcome of x‘i ¼ 1 indicates a faulty chip, x‘i ¼ 0 indicates a
nondefective chip, and the joint density of (X1i, X2i) is given by
16H. Cramer and H. Wold, (1936) Some Theorems on Distribution Functions, J. London Math. Soc., 11(1936), pp. 290–295.
272
Chapter 5
Basic Asymptotics

Xi ¼
X1i
X2i
	

 px1i
1 ð1  p1 Þ1x1i If0;1g ðx1iÞ px2i
2 ð1  p2 Þ1x2i If0;1g ðx2iÞ;
where pi ∈(0,1) for i ¼ 1, 2. Note that
E Xi
ð
Þ ¼
p1
p2
	

¼ p, and
CovðXiÞ ¼ S ¼
p1 ð1  p1Þ
0
0
p2 ð1  p2Þ
	

:
Letting Xn
21
ð
Þ
¼ n1 P
n
i¼1
X1i
X2i
	

; it follows from the multivariate Lindberg-Levy
CLT that Zn ¼ n1=2 Xn  p


!
d Z  Nð0; SÞ and also that Xn 
a N p; n1 S


.
If one were interested in establishing an asymptotic distribution for
the difference in the number of defectives observed in n random pairs of
CPUs from shipments 1 and 2, the random variable of interest would be c0ðnXnÞ
¼ Pn
i¼1 X1i¼1  Pn
i¼1 X2i;, where c0 ¼ [1 1]. Then c0ðnXnÞ ¼ gðZn; nÞ ¼ c0½n1=2 Zn
þnp, so that by Deﬁnition 5.2, c0 nXn



a c0 n1=2 Z þ np


 Nðnc0p; nc0ScÞ. Thus,
the asymptotic distribution for the difference in the number of defectives is given
by c0ðnXnÞ 
a N nðp1  p2Þ; n½p1 ð1  p1Þ þ p2 ð1  p2Þ
ð
Þ.
□
Another useful multivariate CLT concerns the case where the elements in
the sequence {Xn} are independent but not necessarily identically distributed
(k  1) random vectors that exhibit uniform (i.e., across all n) absolute upper
bounds with probability 1.
Theorem 5.38
Multivariate CLT for
Independent Bounded
Random Vectors
Let {Xn} be a sequence of independent (k1) random vectors such that p( x1i
j
j
m, x2i
j
jm,. . ., xki
j
j m) ¼ 1 8 i, where m ∈(0,1). Let E(Xi) ¼ mi, Cov(Xi) ¼ Ci,
and suppose that limn!1n1 Pn
i¼1 Ci ¼ C , a ﬁnite positive deﬁnite (k  k)
matrix. Then n1=2 P
n
i¼1
Xi  mi
ð
Þ!
d Nð0; CÞ:
Various other multivariate CLTs can be constructed using the Cramer-Wold
device and CLTs for random scalars. In practice, one often relies on the Cramer-
Wold device directly for establishing limiting distributions relating to statistical
procedures of interest, and so we will not attempt to compile a list of additional
multivariate CLTs here.
5.9
Asymptotic Distributions of Differentiable Functions of Asymptotically Normally
Distributed Random Variables: The Delta Method
In this section we examine results concerning the asymptotic distributions of
differentiable
functions
of
asymptotically
normally
distributed
random
variables. General conditions will be identiﬁed for which differentiable
functions
of
asymptotically
normally
distributed
random
variables
are
5.9
Asymptotic Distributions of Differentiable Functions of Asymptotically. . .
273

themselves asymptotically normally distributed. The utility of these results in
practice is that once the asymptotic distribution of Xn is known, the asymptotic
distributions of interesting functions of Xn need not be derived anew. Instead,
these asymptotic distributions can generally be deﬁned by specifying the mean
and covariance matrix of a normal distribution according to well-deﬁned and
straightforwardly implemented formulas.
All of the results that we will examine in this section are based on ﬁrst-order
Taylor series expansions of the function g(x) around a point m. Being that the
methods are based on derivatives, the methodology has come to be known as the
delta method for deriving asymptotic distributions and associated asymptotic
covariance matrices of functions of random variables. We review the Taylor
series expansion concept here, paying particular attention to the nature of the
remainder term. Recall that d(x,m) ¼ [(xm)0(xm)]1/2 represents the distance
between the vectors x and m.
Deﬁnition 5.11
First-Order Taylor
Series Expansion
and Remainder
(Young’s Form)
Let g: D ! ℝbe a function having partial derivatives in a neighborhood of the
point m∈D that are continuous at m. Let G ¼ @g m
ð Þ=@x1; . . . ; @g m
ð Þ=@xk
½

represent the 1  k gradient vector of g(x) evaluated at the point x ¼ m.
Then for x∈D, g(x) ¼ g(m) + G(xm) + d(x, m)R(x). The remainder term R(x)
is continuous at x ¼ m, and limx!m R(x) ¼ R(m) ¼ 0.
Young’s form of Taylor’s theorem is not prevalent in calculus texts. The reader
can ﬁnd more details regarding this type of expansion in G.H. Hardy (1952), A
Course of Pure Mathematics, 10th ed., Cambridge, New York, The University
Press, p. 278, for the scalar case, and T.M. Apostol (1957), Mathematical Analysis.
Cambridge, MA: Addison-Wesley, pp. 110 and 118 for the multivariate case.
Our ﬁrst result on asymptotic distributions of g(x) concerns the case where
g(x) is a scalar-valued function. As will be common to all of the results we will
examine, the principal requirement on the function g(x) is that partial
derivatives exist in a neighborhood of the point m and that they are continuous
at m so that Lemma 5.6 can be utilized. In addition, we will also make
assumptions relating to the nature of the asymptotic distribution of X.
Theorem 5.39
Asymptotic Distribution
of g(Xn) (Scalar Function
Case)
Let {Xn} be a sequence of k  1 random vectors such that n1/2(Xn  m) !
d Z ~
N(0,S). Let g(x) have ﬁrst-order partial derivatives in a neighborhood of the
point x ¼ m that are continuous at m, and suppose the gradient vector of g(x)
evaluated at x ¼ m, G 1k
ð
Þ ¼ @g m
ð Þ=@x1 . . . @g m
ð Þ=@xk
½
, is not the zero vector.
Then n1/2(g(Xn)  g(m)) !
d N(0,GSG0) and g(Xn) 
a N(g(m), n1GSG0).
Example 5.46
Asymptotic Distribution
of Xn 1  Xn
ð
Þ for IID
Bernoulli RVs
Note from Example 5.39 that if {Xn} is a sequence of iid Bernoulli-type random
variables, then for p 6¼ 0 or 1, n1/2 ( Xn p) !
d N(0, p(1p)). Consider using an
outcome of g(X) ¼ X (1  X) as an estimate of the variance p(1p) of the Bernoulli
PDF, and consider deﬁning an asymptotic distribution for g(Xn).
274
Chapter 5
Basic Asymptotics

Theorem 5.39 applies with m ¼ p and S ¼ s2 ¼ p(1  p). Note that dg(p)/dX
¼ 12p, which is continuous in p and is nonzero so long as p 6¼ .5. Also, s2 6¼ 0 if
p 6¼ 0 or 1. Then for p 6¼ 0, .5, or 1, Theorem 5.39 implies that
Xn 1  Xn



a N p 1  p
ð
Þ; n1 1  2p
ð
Þ2p 1  p
ð
Þ


:
An asymptotic density for Xn (1  Xn ) under the assumption p ¼ 1/2 can be
established using other methods (see Bickel and Doksum (1977), Mathematical
Statistics, San Francisco: Holden-Day, p. 53); however, convergence is not to a
normal distribution. Speciﬁcally, it can be shown that n Xn 1  Xn




1=4
ð
Þ !
d Z,
where Z has the density of a w2
1 random variable that has been multiplied by
(1/4). If p ¼ 0 or p ¼ 1, the Xi’s are all degenerate random variables equal to
0 or 1, respectively, and the limiting density of Xn is then degenerate at 0 or 1
as well.
□
By reinterpreting g(x) as a vector function and G as a Jacobian matrix, the
conclusion of Theorem 5.39 regarding the asymptotic distribution of the vector
function remains valid. The extension allows one to deﬁne the joint asymptotic
distribution of the random vector g(Xn).
Theorem 5.40
Asymptotic Distribution
of g(Xn) (Vector
Function Case)
Let {Xn} be a sequence of k1 random vectors such that n1/2(Xn  m) !
d Z ~
N(0, S). Let g(x) ¼ (g1(x),. . .,gm(x))0 be an (m1) vector function (m  k) having
ﬁrst order partial derivatives in a neighborhood of the point x ¼ m that are
continuous at m. Let the Jacobian matrix of g(x) evaluated at x ¼ m,
G
mk ¼
@g1 m
ð Þ=@x0
..
.
@gm m
ð Þ=@x0
2
664
3
775 ¼
@g1 m
ð Þ
@x1
. . .
@g1 m
ð Þ
@xk
..
.
..
.
..
.
@gm m
ð Þ
@x1
. . .
@gm m
ð Þ
@xk
2
6664
3
7775;
have full row rank. Then
n1/2(g(Xn)  g(m)) !
d N(0,GSG0) and g(Xn) 
a N(g(m), n1 GSG0).
Example 5.47
Asymptotic Distribution
of Products and Ratios
Let {^bn} be a sequence of (21) random vectors such that n1=2
^bn  b


!
d N(0,S),
where b ¼ [2
1]0 and S ¼
2
1
1
1
	

. We seek an asymptotic distribution for the
vector function g ^b
 
¼
3^b 1
½ ^b 2
½ 
^b 2
½ =^b 1
½ 

0
: All of the conditions of Theorem
5.40 are met, including the fact that
G
22 ¼
dg1 ðbÞ=db0
dg2 ðbÞ=db0
	

¼
3 b2
3 b1
 b2 = b2
1
1= b1
	

¼
3
6
1=4
1=2
	

has full row rank (note that the partial derivatives exist in an open rectangle
containing b, and they are continuous at the point b). Then, since g(b) ¼ [6
½]0
and GSG0 ¼
90
1:5
1:5
:125
	

, it follows from Theorem 5.40 that
5.9
Asymptotic Distributions of Differentiable Functions of Asymptotically. . .
275

gð^bnÞ ¼
3 ^bn ½1 ^bn ½2
^bn ½2= ^bn ½1
"
#

a N
6
1=2
	

; n1
90
1:5
1:5
:125
	



:
A speciﬁc distribution is obtained once n is speciﬁed. For example, if n ¼ 20,
then g ^b20



a N
6
1=2
	

;
4:5
:075
:075
:00625
	



:
□
The
ﬁnal
result
that
we
will
examine
concerning
the
asymptotic
distribution of g(Xn) generalizes the previous two theorems to cases for which
V1=2
n
(Xnm) !
d N(0,I), where {Vn} is a sequence of (mm) positive deﬁnite
matrices of real numbers such that Vn ! 0.17 Note this case subsumes the
previous cases upon deﬁning Vn ¼ n1 S, in which case
V1=2
n
Xn  m
ð
Þ
¼ S1=2 n1=2 ðXn  mÞ !
d N(0,I) by Slutsky’s theorem. The generalization allows
additional ﬂexibility in how the asymptotic distribution of Xn is initially
established and is especially useful in the context of the least squares estimator
to be discussed in Chapter 8.
Theorem 5.41
Asymptotic Distribution
of g(Xn) (Generalized)
Let {Xn} be a sequence of (k1) random vectors such that V1=2
n
ðXn  mÞ!
d N(0,I),
where {Vn} is a sequence of (mm) positive deﬁnite matrices for which Vn ! 0. Let
g(x) be a (m1) vector function satisfying the conditions of Theorem 5.40. If there
exists a sequence of positive real numbers {an} such that {[anGVnG0]1/2} is O(1) and
a1=2
n
(Xn  m) is Op(1), then (GVnG)1/2 [g(Xn)  g(m)] !
d N(0,I) and g(Xn) 
a N(g(m),
GVnG0).
5.10
Appendix: Proofs and Proof References for Theorems
Theorem 5.1
The discrete case is left to the reader. For the continuous case, see H. Scheffe´,
(1947), “A useful convergence theorem for probability distributions,” Ann.
Math. Stat., 18, pp. 434–438.
Theorem 5.2
See E. Lukacs (1970), Characteristic Functions, London: Grifﬁn, pp. 49–50, for a
proof of this theorem for the more general case characterized by convergence of
characteristic functions (which subsumes Theorem 5.2 as a special case). In the
multivariate case, t will be a vector, and convergence of the MFG must hold
8ti∈(h,h), and 8i.
17Recall that V1=2
n
is the symmetric square root matrix of Vn, and V1=2
n
is the inverse of V1=2
n . The deﬁning property of V1=2
n
is that
V1=2
n V1=2
n
¼ Vn, while V1=2
n
V1=2
n
¼ V1
n .
276
Chapter 5
Basic Asymptotics

Theorem 5.3
See the proof of Theorem 5.17 and R. Serﬂing (1980), Approximation Theorems
of Mathematical Statistics, New York: Wiley, pp. 24–25.
Theorem 5.4
See the proof of Theorem 5.16.
Theorem 5.5
See the proof of Theorem 5.17 and R. Serﬂing, op. cit., pp. 24–25.
Theorem 5.6
All of the results follow from Theorem 5.5, and the fact that the functions being
analyzed are continuous functions. Note in particular that the matrix inverse
function is continuous at all points for which the matrix is nonsingular.
Theorem 5.7
Y.S. Chow and H. Teicher (1978), Probability Theory, New York: Springer-
Verlag, New York, p. 249.
Corollary 5.1
This follows immediately from Theorem 5.7 upon deﬁning Xn¼X¼Y 8n.
Theorem 5.8: Proof
Let {Yn} be a sequence of scalar random variables and suppose Yn!
d c, so that Fn(y)
! F(y) ¼ IA(y), where A ¼ {y: y  c}. Then as n ! 1, P(|yn  c| < e)  Fn(c + t) 
Fn(c  t) ! 1, for t 2 (0,e) and 8e > 0, which implies that Yn!
p c. The multivariate
case can be proven similarly using marginal CDFs and the elementwise nature of
the plim operator.
n
Theorem 5.9
This follows from the proof in V. Fabian and J. Hannon (1985), Introduction to
Probability and Mathematical Statistics, New York: John Wiley, p. 159, and
from Theorem 5.4.
Theorem 5.10: Proof
Each function on the left-hand side of (a), (b), and (c) is of the form g(Xn, Yn, an)
and satisﬁes the conditions of Theorem 5.9, with an being a ghost in the deﬁni-
tion of the function g.
n
Theorem 5.11: Proof
This follows directly from deﬁnitions of orders of magnitude in probability upon
interpreting the sequence of real numbers or matrices as a sequence of degener-
ate random variables or random matrices.
n
Theorem 5.12: Proof
We provide a proof for the scalar case, which sufﬁces to prove the matrix case
given the elementwise deﬁnition of mean-square convergence (Deﬁnition 5.6.b).
Necessity
a. E(Yn) ! E(Y) follows from the fact that
jE Yn
ð
Þ  EðYÞj ¼ jEðYn  YÞj  EðjYn  YjÞ  E jYn  Y j2



1=2
! 0:
To see this, note that the ﬁrst inequality follows because ðyn yÞ  j yn yj.
Regarding the second inequality, note that g(z) ¼ z2 is a convex function on ℝ,
and letting Z ¼ |Yn  Y|, Jensen’s inequality implies (E(|YnY|))2  E(|Yn  Y|2)
(recall g(E(Z))  E(g(Z)). Convergence to zero occurs because E(|YnY|2) ¼
E((YnY)2) ! 0 by convergence in mean square.
5.10
Appendix: Proofs and Proof References for Theorems
277

b. E( Y2
n ) ¼ E((Yn  Y)2) þ E(Y2) þ 2E(Y(Yn  Y)), and since by the Cauchy-
Schwartz inequality,
jE Y Yn  Y
ð
Þ
ð
Þj 
E Y2


E ðYn  Y Þ2


h
i1=2
it follows
that
E ðYn  Y Þ2


þ E Y2


 2 E Y2


EððYn  Y Þ2Þ
h
i1=2
 E Y2
n


 E ðYn  Y Þ2


þ E Y2


þ 2 E Y2


E ðYn  Y Þ2


h
i1=2
Then since E ðYn  Y Þ2


! 0 by mean-square convergence, E(Yn
2)! E(Y2).
It follows that var(Yn) ! var(Y) since var(Yn) ¼ E(Yn
2)(E(Yn))2 ! E(Y2)
(E(Y))2 ¼ var(Y).
c. First note that
E ðYn  Y Þ2


¼ E Y2
n


 2E YnY
ð
Þ þ E Y2


¼ varðYnÞ þ E Yn
ð
Þ
ð
Þ2  2 covðYn; YÞ þ E Yn
ð
ÞEðYÞ
½
 þ varðYÞ
þ EðYÞ
ð
Þ2
If E ðYn  Y Þ2


! 0, then by (a) and (b), the preceding equality implies
cov(Yn, Y) ! var(Y).
Sufﬁciency
From the expression deﬁning E ðYn  Y Þ2


in the preceding proof of the neces-
sity of (c), it follows directly that (a), (b), and (c) ) E((Yn  Y)2) ! 0, which
implies Yn !
m Y.
n
Corollary 5.2: Proof
This follows directly from Theorem 5.12 upon letting Y ¼ c, and noting that
var(c[i,j]) ¼ 0 and cov(Yn[i,j],c[i,j]) ¼ 0.
n
Corollary 5.3: Proof
This follows directly from Theorem 5.12 since in the scalar case, corrðYn; YÞ
¼
covðYn; YÞ
½varðYnÞvarðYÞ 1=2 ! varðYÞ
varðYÞ ¼ 1: The matrix case follows by applying the
preceding result elementwise.
n
Theorem 5.13: Proof
(scalar case—matrix case proved by applying the argument elementwise). Note
that (Yn  Y)2 is a nonnegative-valued random variable, and letting a ¼ e2 > 0,
we have by Markov’s inequality that P((yn  y)2  e2)  E((YnY)2)/e2. Thus,
P yn  y
j
j  e
ð
Þ  E
Yn  Y
ð
Þ2


=e2 or P yn  y
j
j<e
ð
Þ  1  E
Yn  Y
ð
Þ2


=e2
By mean square convergence, E((Yn  Y)2) ! 0, so that lim
n!1 P(|yn  y| < e) ¼ 1
8
e > 0.
Thus,
plim(Yn) ¼ Y.
Convergence
in
distribution
follows
from
Corollary 5.1.
n
278
Chapter 5
Basic Asymptotics

Theorem 5.14: Proof
Necessity
limn!1yn ¼ y implies that for every e > 0, there exists an integer N(e) such that
j yi yj<e 8i  N(e) (recall Deﬁnition A.25). It follows that Pðlimn!1 yn ¼ yÞ 
Pðj yi yj<e; i  NðeÞÞ 8e > 0. If Yn !
as Y, then the left-hand side of the preceding
inequality is 1, which implies that the right-hand side is also 1. It follows that,
for 8e > 0, limn!1Pðj yi yj<e; i  nÞ ¼ 1since the values of Pðj yi yÞj<e; i  nÞ
must all be ones for n large enough if limn!1 yn ¼ y.
Sufﬁciency
R. Serﬂing, Approximation Theorems, pp. 6–7. n
Theorem 5.15: Proof
Suppose Yn !
asY. Then 8 e > 0, limn!1Pðj yi yj<e; i  nÞ ¼ 1 which follows from
Theorem
5.14.
Since
j yi yj<e; i  n ) j yn yj<e;
it
follows
that
P yi  y
j
j<e; i  n
ð
Þ  P yn  y
j
j<e
ð
Þ. Then since the left-hand side has a limiting
value of 1 8e > 0, by almost-sure convergence, it follows that the right-hand side
has a limiting value of 1, 8e > 0, implying convergence in probability.
n
Theorem 5.16: Proof
The proof is immediate from the deﬁnition of almost-sure convergence, since if
an[i,j]! a[i,j] 8 i and j, then P(limn!1an ¼ a) ¼ 1, which implies that an !
as a. n
Theorem 5.17: Proof
Assume that Xn !
as X . For outcomes of {Xn} for which xn ! x and g is continuous
at x, it must be the case that g(xn) ! g(x) (recall Deﬁnition A.29). Then deﬁning
the sets A ¼ {({xn},x): xn ! x} and B ¼ {({xn},x): g is continuous at x}, it follows that
P(g(xn) ! g(x))  P(A \ B)  1  P A
 
 P B
 
¼ 1 since P A
 
¼ 0 by almost-sure
convergence of {Xn} to X, and P B
 
¼ 0 since g is continuous with probability 1.
Thus g(Xn) !
as g(X).
n
Theorem 5.18
Y.S. Chow and H. Teicher (1978), Probability Theory, New York: Springer-
Verlag, p. 68.
Theorem 5.19: Proof
A general proof involving characteristic functions can be found in D.S.G. Pol-
lock (1979), The Algebra of Econometrics, New York: John Wiley, p. 332. An
alternative proof based entirely on probability inequalities is given by C.R. Rao,
(1965) Statistical Inference, pp. 112–113. Our proof requires the additional
assumption that MxiðtÞ exists, but the general proof is analogous with the
characteristic function replacing the MGF. The moment-generating function
of Xn is given by
M XnðtÞ ¼ E exp
t
n
X
n
i¼1
X1
 
!
 
!
¼
Y
n
i¼1
E exp X1
t
n




¼
Y
n
i¼1
MX1
t
n
 
¼ MX1
t
n
 
h
in
because the Xi’s are independent and identically distributed. It follows that
lim
n!1 M XnðtÞ ¼ lim
n!1 1 þ nMX1 t=n
ð
Þ  n
n
	

n
¼ exp lim
n!1 n MX1
t
n
 
 1
h
i




by Lemma 5.1, stated below.
5.10
Appendix: Proofs and Proof References for Theorems
279

Lemma 5.1
limn!1 1 þ an=n
½
n ¼ exp limn!1an
ð
Þ
Then applying L’Hospital’s rule,
lim
n!1
MX1 t=n
ð
Þ  1
n1


¼ lim
n!1
n2

 dMX1 t=n
ð
Þ
d t=n
ð
Þ
t
n2




¼ tm;
since the ﬁrst derivative ofMX1 t
ð Þ ! mas t* ¼ t/n ! 0. Thenlimn!1M X1ðtÞ ¼ etm,
which is the MGF of a random variable that is degenerate at m. Therefore, by
Theorem 5.2 and Theorem 5.8 Xn!
p m.
n
Theorem 5.20: Proof
Without loss of generality, we assume that S is a real-valued sample space. Let the n
iid random variables Z1,. . .,Zn represent the n independent and identical
repetitions of the experiment, and deﬁne Xi ¼ IA(Zi), for i ¼ 1,. . .,n, so that X1,. . .,
Xn are n iid (Bernoulli) random variables for which xi ¼ 1 indicates the occurrence
and xi ¼ 0 indicates the nonoccurrence of event A on the ith repetition. Since
E(Xi) ¼ P(A), 8 i, it follows from Khinchin’s WLLN that Xn!
p PðAÞ. Then, since NA

 Pn
i¼1 Xi and (NA/n) 
 Xn, we can also conclude that (NA/n) !
p P(A).
n
Theorem 5.21: Proof
Sufﬁciency
For any choice of b > 0 and a  b, a/(a + 1)  b/(b + 1). It follows that 8e > 0,
P
xn  mn
ð
Þ2  e2


 P
xn  mn
ð
Þ2
1 þ xn  mn
ð
Þ2 
e2
1 þ e2
 
!
 E
Xn  mn

2
1 þ
Xn  mn

2
"
#
=
e2
1 þ e2
	

where the ﬁrst inequality follows because the event on the left-hand side implies
the event on the right, and the second inequality is an application of Markov’s
inequality. If the expectation of the bracketed term !0 as n!1, then 8e>0
P
xn  mn
ð
Þ2<e2


¼ P xn  mn
j
j<e
ð
Þ ! 1 so that
Xn  mn


!
p 0.
Necessity
See B.V. Gnedenko (1968), The Theory of Probability, New York: Chelsea Pub-
lishing Col, pp. 246–248.
n
Theorem 5.22: Proof
The
result follows
directly from
Theorem
5.21
upon
recognizing
that
0  E
Xnmn
ð
Þ
2
1þ Xnmn
ð
Þ
2
	

 E
Xn  mn

2
h
i
¼ var Xn


:
n
Theorem 5.23: Proof
With reference to Theorems 5.4 and 5.5, we know that because mn!
p c , or
equivalently,
mn  c
ð
Þ!
p 0, and also because g Xn  mn; mn  c


¼
Xn  mn


þ
mn  c
ð
Þ ¼ Xn  c is a continuous function of
Xn  mn


and mn  c
ð
Þ, then plim
Xn  c


¼ plim Xn  mn


þ plim mn  c
ð
Þ ¼ 0, so that by Deﬁnition 5.3 Xn!
p c. n
280
Chapter 5
Basic Asymptotics

Theorem 5.24: Proof
Let Wj ¼ Pj
i¼1 Xi, and deﬁne the events
Aj ¼ fðx1; . . . ; xnÞ: wj

  e wi
j
j<e for i < jg; j ¼ 1; . . . ; n;
and A ¼
ðx1; . . . ; xnÞ:
max
1  m  n
X
m
i¼1
xi

  e
(
)
:
The events A1,. . .,An are disjoint, and A ¼ [n
i¼1 Ai, i.e., the Ai’s are a partition of A.
Furthermore,
ðIÞ E W2
n


¼ E
X
n
i¼1
X2
i þ
X X
i6¼j
XiXj
"
#
¼
X
n
i¼1
s2
i
 E W2
nIAðXÞ


¼
X
n
i¼1
E W2
nIAiðXÞ


where x ¼ (x1,. . .,xn), and the last equality holds because IAðxÞ ¼ Pn
i¼1 IAiðxÞ.
Also note that
8j<n; Wn  Wj ¼ Pn
i¼jþ1 Xi
¼ f(Xj+1,. . .,Xn) is independent
of
Wj ¼ g(X1,. . .,Xj)
and
independent
of
Wj IAj
(X)¼h(X1,. . .,Xj)
by
the
independence of (X1,. . .,Xj) and (Xj+1,. . .,Xn) (note that Xj+1,. . .,Xn are ghosts in
the function IAjðXÞ given the deﬁnition of Aj). Therefore E((WnWj)WjIAj (X))¼
[E(WnWj)][E(WjIAj(X))]¼0 since E(Wi)¼ 0, 8 i. It follows that
E W2
nIAjðXÞ


¼ E
W2
n  2 Wn  Wj


Wj


IAjðXÞ


¼ E
W2
j þ Wn  Wj

2
h
i
IAjðXÞ


 E W2
j IAjðXÞ


 e2P Aj


where the last inequality follows from the fact that w2
j  e2 when x ∈Aj. Using
this
result
in
(I)
and
recalling
that
the
Ai’s
are
a
partition
of
A,
P
n
i¼1
s2
i  e2 P
n
i¼1
P Ai
ð
Þ ¼ e2PðAÞ; 8e>0:
n
Theorem 5.25: Proof
Let Yi ¼ (Xi  m)/i and note that E(Yi) ¼ 0 and var(Yi) ¼ s2/i2. DeﬁneWk ¼ Pk
i¼1 Yi,
and examine the sums Wn+m  Wn ¼ Pnþm
i¼nþ1 Yi ¼ Pm
i¼1 Ynþi, for m ¼ 1,2,. . .,k.
By Kolmogorov’s inequality (replace xi by yn+i in the statement of the inequality), it
follows that 8e>0,
P
max
1  m  k wnþm  wn
j
j  e


¼ P
max
nþ1  m  nþk wm  wn
j
j  e



X
nþk
i¼nþ1
s2
i2e2


"
#
:
Letting k ! 1, we have that18
P max
m>n wm  wn
j
j  e



X
1
i¼nþ1
s2
i2e2


"
#
¼
s2
e2

 X
1
i¼nþ1
i2;
18If max does not exist, max is replaced by sup (the supremum).
5.10
Appendix: Proofs and Proof References for Theorems
281

and since limn!1
P1
i¼nþ1 i2 ¼ 0 ,19 it follows from the Cauchy criterion for
almost-sure convergence (Theorem 5.18) that {Wn} converges almost surely to
some random variable, say W. Now note the Kronecker lemma from real analysis.
Lemma 5.2
Kronecker’s Lemma
Let
{an}
be
a sequence
of nondecreasing
positive
numbers, and
let
{zn} be a sequence of real numbers for which P1
i¼1 zi=ai converges. Then
limn!1a1
n
Pn
i¼1 zi ¼ 0 . (See E. Lukacs (1968), Stochastic Convergence,
Andover, MA: D.C. Heath and Co., p. 96).
Let zi ¼ xi  m and ai ¼ i, so that yi ¼ zi/ai and wn ¼ Pn
i¼1 zi=ai. Since wn!w
with probability 1, and since by Kronecker’s lemma wn ! w ) n1 Pn
i¼1 zi ! 0;
then n1 Pn
i¼1 zi ¼ xn  m ! 0 with probability 1.
n
Theorem 5.26
The proof of the theorem is somewhat difﬁcult, and can be found in C. Rao,
Statistical Inference, pp. 115–116.
n
Theorem 5.27: Proof
Without loss of generality, we assume that S is a real-valued sample space. Let
the n iid random variables Z1,. . .,Zn represent the n independent and identical
repetitions of the experiment, and deﬁne Xi ¼ IA(Zi), for i ¼ 1,. . .,n, so that
X1,. . .,Xn are n iid (Bernoulli) random variables for which xi ¼ 1 indicates the
occurrence and xi ¼ 0 indicates the nonoccurrence of event A on the ith repeti-
tion of the experiment. Note that E(Xi)¼ P(A) < 1 8 i, so that Theorem 5.26 is
applicable. Then Xn!
as P(A), and since NA 
 Pn
i¼1 Xi and NA/n ¼ Xn, we can also
conclude that (NA/n) !
as P(A).
n
Theorem 5.28
This follows directly from Theorem 3.7.2 in W.F. Stout (1974), Almost-sure
Convergence. New York: Academic Press, p. 202.
Theorem 5.29: Proof
The proof is based on Theorems 5.16 and 5.17 and follows the approach of
Theorem 5.23. Details are left to the reader.
n
Theorem 5.30: Proof
We prove the theorem for the case where the moment-generating function of Xi
exists. The proof can be made general by substituting characteristic functions in
place of MGFs, and such a general proof can be found in C. Rao, Statistical
Inference, p. 127.
Let Zi ¼ (Xi  m)/s, so that E(Zi) ¼ 0 and var (Zi) ¼ 1, and deﬁne
Yn ¼ n1=2s

1 Pn
i¼1 Xi  nm


¼ n1=2 Pn
i¼1 Zi. Then
MYnðtÞ ¼
Y
n
i¼1
MZi t=n1=2


¼ MZ1 t=n1=2


h
in
;
19 Pn
i¼1 ip is the so-called p series that converges for p>1 and diverges for p∈(0,1]. Since the series converges for p ¼ 2, it must be the
case that P1
i¼nþ1 i2 ! 0 as n ! 1. See Bartle, The Elements of Real Analysis, 2nd Ed. pp. 290–291.
282
Chapter 5
Basic Asymptotics

where the last equality follows because the Zi’s are independent and identically
distributed. Taking logarithms,
lim
n!1 ln MYnðtÞ
ð
Þ ¼ lim
n!1 ln MZ1 t=n1=2


=n1


h
i
;
which has the indeterminate form 0/0, so we can apply L’Hospital’s rule. Letting
t* ¼ t/n1/2 and  t
ð Þ ¼ ln MZ1 t
ð Þ
ð
Þ,
lim
n!1 ln MYnðtÞ ¼ lim
n!1
d t
ð Þ
dt



t
2n3=2


= n2


	

¼ lim
n!1
1
2 t d t
ð Þ
dt
=n1=2
	

	

;
which remains an indeterminate form since d t
ð Þ=dt ! dð0Þ=dt ¼ E Z1
ð
Þ ¼ 0
when n ! 1. A second application of L’Hospital’s rule yields
lim
n!1 ln MYnðtÞ
ð
Þ ¼ 1=2
ð
Þ lim
n!1 t2 d2 t
ð Þ
dt2

"
#
¼ t2=2;
since
lim
n!1
d2 t
ð Þ
dt2

¼ d2ð0Þ
dt2

¼ d2MZ1ð0Þ
dt2

 dMZ1ð0Þ
dt
	

2
¼ E Z2
1


 E Z1
ð
Þ
ð
Þ2 ¼ var Z1
ð
Þ ¼ 1
Thus,
lim
n!1 MYnðtÞ
ð
Þ ¼ exp lim
n!1 ln MYnðtÞ
ð
Þ


¼ et2=2; which implies that Yn !
d
N(0,1) by Theorem 5.2.
n
Theorem 5.31
See Rohatgi, Mathematical Statistics, pp. 282–288 or Chow and Teicher, Proba-
bility Theory, pp. 291–293.
Theorem 5.32: Proof
This follows from the Lindberg CLT by ﬁrst noting that since P xi  mi
j
j  2m
ð
Þ
¼ 1 by the boundedness assumption, then 8e > 0 (for the continuous case—the
discrete case is similar),
ð
ximi
ð
Þ2  eb2
n
xi  mi
ð
Þ2fi xi
ð
Þdxi  4m2
ð
ximi
ð
Þ2  eb2
n
fi xi
ð
Þdxi
 4m2P
xi  mi
ð
Þ2  eb2
n


 4m2s2
i
eb2
n
5.10
Appendix: Proofs and Proof References for Theorems
283

where the last inequality results from Markov’s inequality. Then
1
b2
n
X
n
i¼1
ð
ximi
ð
Þ2  eb2
n
xi  mi
ð
Þ2fi xi
ð
Þdxi  1
b2
n
X
n
i¼1
4m2s2
i
eb2
n
 4m2
eb2
n


since b2
n ¼ Pn
i¼1 s2
i , so that if b2
n ! 1 when n ! 1, the right-hand side above has
a zero limit 8e>0, and the Lindberg condition is satisﬁed.
n
Theorem 5.33: Proof
This follows from the Lindberg CLT by ﬁrst noting that, for d > 0 (for the
continuous case—the discrete case is similar),
ð
ximi
ð
Þ2  eb2
n
xi  mi
ð
Þ2fi xi
ð
Þdxi ¼
ð
ximi
ð
Þ2  eb2
n
xi  mi
j
jd xi  mi
j
jd xi  mi
ð
Þ2fi xi
ð
Þdxi
 eb2
n

d=2
ð
ximi
ð
Þ2  eb2
n
xi  mi
j
j2þdfi xi
ð
Þdxi
 eb2
n

d=2E
Xi  mi
j
j2þd


;
where the ﬁrst inequality follows from the fact that over the range of integration,
(ximi)2 eb2
n implies xi  mi
j
jd eb2
n

d=2, and the second inequality results from
adding the nonnegative term eb2
n

d=2 R
ximi
ð
Þ2<eb2
n xi  mi
j
j2þdfi xi
ð
Þdxi to the right-
hand side of the ﬁrst inequality. Then
lim
n!1
1
b2
n
X
n
i¼1
ð
ximi
ð
Þ2  eb2
n
xi  mi
ð
Þ2fi xi
ð
Þdxi  lim
n!1
1
b2
n
X
n
i¼1
eb2
n

d=2E jXi  mij2þd


 ed=2 lim
n!1
X
n
i¼1
E jXi  mij2þd


b2
n

1þd=2
2
4
3
5
The assumptions of the Liapounov CLT state that, for some d > 0, P
n
i¼1
E
Ximi
j
j2þd
ð
Þ
b2
n
ð Þ
1þd=2
! 0 as n ! 1, so that, 8 e > 0 the Lindberg condition is met and the Liapounov
CLT holds.
n
Theorem 5.34
See K.L. Chung (1974), A Course in Probability Theory, 2nd Ed., New York:
Academic Press, Section 7.2.
Theorem 5.35
See
R.G.
Laha
and
V.K.
Rohatgi,
(1979)
Probability
Theory,
p.
355,
Section 5.5.31.
284
Chapter 5
Basic Asymptotics

Theorem 5.36: Proof
Sufﬁciency will be motivated assuming the existence of MGFs. The general
proof replaces MGFs with characteristic functions. From Theorem 5.2, ℓ0Xn!
d
ℓ0X ) Mℓ0XnðtÞ ! Mℓ0XðtÞ for t∈(h,h), h > 0, and 8ℓ∈ℝk. This implies E etℓ0Xn


¼ E et0Xn


! E et0X


¼ E etℓ0X


8t ¼ tℓsince ℓcan be chosen arbitrarily. But this
is equivalent to MXn t
ð Þ ! MX t
ð Þ, 8t*, which implies Xn!
d X by the multivariate
interpretation of Theorem 5.2. Necessity follows from Theorem 5.3 since ℓ0x is a
continuous function of x. The general proof based on characteristic functions
can be found in V. Fabian and J. Hannan (1985), Introduction to Probability and
Mathematical Statistics. New York: John Wiley, p. 144.
n
Theorem 5.37: Proof
Examine Zi ¼ ℓ0Xi, where ℓ6¼0. Note that mz ¼ E(Zi)¼ E(ℓ0 Xi) ¼ ℓ0m and s2
z ¼ var(Zi)
¼ var(ℓ0Xi) ¼ℓ0S ℓ8i. Now since {Xn} is a sequence of iid random vectors, then {Zn}
¼ {ℓ0Xn} is a sequence of iid random scalars, and applying the Lindberg-Levy CLT
for random scalars to the iid sequence {Zn} results in
Pn
i¼1 Zi  nmz
n1=2sz
¼
Pn
i¼1 ℓ0Xi  nℓ0m
n1=2 ℓ0Sℓ
ð
Þ1=2
¼ ℓ0 Pn
i¼1 Xi  nm


n1=2 ℓ0Sℓ
ð
Þ1=2
¼ ℓ0n1=2 n1 Pn
i¼1 Xi  m


ℓ0Sℓ
ð
Þ1=2
!
d N 0; 1
ð
Þ
Then by Slutsky’s theorem
ℓ0n1=2 n1 X
n
i¼1
Xi  m
"
#
!
d N 0; ℓ0Sℓ
ð
Þ
which holds for any choice of the vector ℓ6¼ 0. Then by the Cramer-Wold device,
we can conclude that n1=2 n1 P
n
i¼1
Xi  m
	

!
d N 0; S
ð
Þ
n
Theorem 5.38: Proof
Examine Zi ¼ ℓ0Xi, where ℓ6¼ 0. Note that E(Zi)¼ E ℓ0Xi
ð
Þ ¼ ℓ0mi and var(Zi) ¼
var ℓ0Xn
ð
Þ ¼ ℓ0ciℓ. Since {Xn} is a sequence of independent random vectors, {Zn} ¼
ℓ0Xn
f
g
is a sequence of independent random scalars. Furthermore, since
outcomes of the vector Xn are contained within the closed and bounded rectan-
gle k
i¼1 m; m
½
 in ℝk with probability 1, then for any given nonzero vector of
real numbers ℓ, outcomes of Zn ¼ ℓ0Xn exhibit an upper bound in absolute value
with probability 1 uniformly 8 n. Thus, there exists a ﬁnite real number d > 0
such that P( zi
j jd) ¼ 1 8 i. In addition, since n1 Pn
i¼1 var (Zi)¼n1 Pn
i¼1 ℓ0ciℓ!
ℓ0cℓ> 0, Pn
i¼1 var(Zi)!1. It follows from the CLT of Theorem 5.32 that
Pn
i¼1 Zi  Pn
i¼1 ℓ0mi
Pn
i¼1 var Zi
ð
Þ

1=2
¼ ℓ0 Pn
i¼1 Xi  Pn
i1 mi


Pn
i¼1 ℓ0ciℓ

1=2
¼ ℓ0n1=2 Pn
i¼1 Xi  Pn
i¼1 mi


Pn
i¼1 ℓ0 n1ci
ð
Þℓ

1=2
!
d N 0; 1
ð
Þ:
5.10
Appendix: Proofs and Proof References for Theorems
285

Premultiplying by the denominator term, and noting that
lim
n!1
X
n
i¼1
ℓ0 n1ci


ℓ
 
!1=2
¼ ℓ0cℓ
ð
Þ1=2;
Slutsky’s theorem (Theorem 5.10) then results in
ℓ0n1=2
X
n
i¼1
Xi 
X
n
i¼1
mi
 
!
!
d N 0; ℓ0cℓ
ð
Þ
which holds for any choice of the real vector ℓ6¼ 0. It follows by the Cramer-Wold
device that n1=2
P
n
i¼1
Xi  P
n
i¼1
miÞ !
d N 0; c
ð
Þ

.
n
Theorem 5.39: Proof
Representingg(Xn)intermsofaﬁrst-orderTaylorseriesexpansionaroundthepoint
m and the remainder term, as in Lemma 5.6, yields g(Xn) ¼ g(m) + G(Xn  m) +
d(Xn,
m)
R(Xn).
Multiplying
by
n1/2
and
rearranging
terms
obtains
n1=2 g Xn
ð
Þ  g m
ð Þ
ð
Þ ¼ G n1=2 Xn  m
ð
Þ


þ n1=2d Xn; m
ð
ÞR Xn
ð
Þ.
The last term converges to 0 in probability. To see this, ﬁrst note by Slutsky’s
theorem that Xn  m!
d plim(n1/2)Z ¼ 0·Z ¼ 0, so that plim(Xn)¼ m and then
plim(R(Xn))¼ R(m) ¼ 0 by the continuity of R(x) at the point x ¼ m. Also, by
Theorem 5.3,
n1=2d Xn; m
ð
Þ ¼
n1=2 Xn  m
ð
Þ

0 n1=2 Xn  m
ð
Þ



1=2
!
d Z0Z
ð
Þ1=2
so
that by Slutsky’s theorem n1/2d(Xn,m)R(Xn)!
d
Z0Z
ð
Þ1=2plim(R(Xn))¼ Z0Z
ð
Þ1=20¼0
and thus n1/2d(Xn,m) R(Xn)!
p 0.
Given the previous result, it follows by another application of Slutsky’s
theorem, that the limiting distribution of n1/2(g(Xn)  g(m)) is the same as that
of G[n1/2(Xnm)]!
d GZ~N(0,GSG0). The asymptotic distribution of g(Xn) is then
as stated in the theorem.
n
Theorem 5.40: Proof
Following the proof of Theorem 5.39, a ﬁrst-order Taylor series expansion applied
to each coordinate function in g(Xn) results in n1/2(g(Xn)  g(m)) ¼ G[n1/2(Xn  m)]
þ n1/2 d(Xn, m) R(Xn), where now R(Xn) ¼ (R1(Xn),. . .,Rm(Xn))0 is an m1 vector of
remainder terms. The approach of the proof of Theorem 5.39 can be applied
elementwise to conclude that plim[n1/2d(Xn, m) R(Xn)] ¼ 0. Then, by Slutsky’s
theorem, the limiting distribution of n1/2(g(Xn)  g(m)) is the same as the limiting
distribution of G[n1/2(Xnm)], and G[n1/2(Xnm)] !
d GZ ~ N(0, GSG0). The asymp-
totic distribution of g(Xn) follows directly.
n
Theorem 5.41: Proof
Sketch
Represent
g(Xn)
in
terms
of
a
ﬁrst-order
Taylor
series
expansion
plus
remainder, as in the proof of Theorem 5.40, to obtain [GVnG0]1/2 [g(Xn)  g(m)] ¼
[GVnG0]1/2 [G(Xn  m) + d(Xn,m) R(Xn)].
The ﬁrst term to the right of the equality is such that (GVnG0)1/2 G(Xnm)
¼(GVnG0)1/2G
V1=2
n V1=2
n
(Xnm)
!
d
N(0,I) which follows from H. White,
286
Chapter 5
Basic Asymptotics

Asymptotic
Theory,
Lemma
4.23,
p.
66,
upon
recognizing
that
{An}¼
{(GVnG0)1/2GV1=2
n
} is a O(1) sequence of matrices (note that AnAn0¼I 8n) and
V1=2
n
(Xnm)!
d N(0,I).
The second term converges in probability to the zero matrix since
GVnG0
½
1=2d Xn; m
ð
Þ ¼ anGVnG0
½
1=2 a1=2
n
Xn  m
ð
Þ0 Xn  m
ð
Þa1=2
n
h
i1=2
is Op(1) and R(Xn) !
p
0. Then the convergence in distribution result of the
theorem follows from Slutsky’s theorem, and the asymptotic distribution of
g(Xn) follows subsequently.
n
Keywords, Phrases, and Symbols
{yn} is bounded, unbounded
Almost sure convergence, Yn !
as Y
Asymptotic distribution of functions
of asymptotically normal
random variables
Asymptotic distribution, Zn 
a
g(X, Yn)
Asymptotic nonpositively correlated
Bounded in probability
Central limit theorems, CLTs
Continuity correction
Converge to y
Convergence in distribution, Yn !
d Y
or Yn !
d F
Convergence in mean square, Yn !
m Y
Convergence in probability, Yn !
p Y
Convergence in quadratic mean
Convergence of a sequence yn ! y,
limn1 yn ¼ y
Converges in distribution to a
constant, Yn !
d c
Converges in probability to a
constant
Cramer-Wold device
Delta method
iid, independent and identically
distributed
Khinchin’s WLLN
Kolmogorov’s inequality
Kolmogorov’s SLLN
Laws of large numbers
Liapounov CLT
Limit of a real number sequence
Limit of the sequence {yn}
Limiting density of {Yn}, limiting
CDF of {Yn}
Limiting distribution
Limiting function of {fn}
Lindberg CLT
Lindberg-Levy CLT
m – dependent sequence
Multivariate Lindberg-Levy CLT
Natural numbers, N
O(nk), at most of order nk
o(nk), of order smaller than nk
Op(nk), at most of order nk in
probability
op(nk), of order smaller than nk in
probability
Order of magnitude in probability
Order of magnitude of a random
sequence
Order of magnitude of a sequence
Probability limit, plim
Sequence
Sequence of probability density
functions
Sequence of random variables
Slutsky’s theorems
Strong laws of large numbers, SLLN
Symmetric matrix square root
Triangular array of random variables
Weak laws of large numbers, WLLN
Y ~ f(y), Y has probability density f(y),
or Y is distributed as f(y)
Problems
1. Given current technology, the production of active
matrix color screens for notebook computers is a difﬁcult
process that results in a signiﬁcant proportion of defective
screens being produced. At one company the daily propor-
tion of defective 9.500 and 10.400 screens is the outcome of a
bivariate random variable, X, with joint density function
f(x1,x2;a) ¼ (ax1 + (2a)x2) I[0,1](x1) I[0,1](x2), where a ∈(0,2).
The daily proportions of defectives are independent from
day to day. A collection of n iid outcomes of X will be used
to generate an estimate of the (21) vector of mean daily
proportions of defectives, m, for the two types of screens
being produced, as Xn
21
ð
Þ
¼ Pn
i¼1 XðiÞ=n, where XðiÞ ¼
X1i
X2i
	

:
(a) Does Xn !
as m? Does Xn !
p m? Does Xn !
d m?
Problems
287

(b) Deﬁne an asymptotic distribution for the bivariate
random variable Xn . If a ¼ 1 and n ¼ 200, what is
the approximate probability that Xn½1 > .70, given
that Xn½2 ¼ .60?
(c) Consider
using
an
outcome
of
the
function
gðXnÞ ¼ Xn½1=Xn½2 to generate an estimate of the
relative expected proportions of defective 9.500 and
10.400 screens, m1 / m2. Does gðXnÞ !
as m1 = m2 ? Does
gðXnÞ !
p m1 = m2? Does gðXnÞ !
d m1 = m2?
(d) Deﬁne an asymptotic distribution for gðXnÞ. If a ¼ 1
and n ¼ 200, what is the approximate probability
that the outcome of gðXnÞ will exceed 1?
2. Central limit theorems have important applications
in the area of quality control. One such application
concerns so-called control charts, and in particular, X
charts, which are used to monitor whether the variation
in the calculated mean levels of some characteristics of a
production process are within acceptable limits. The
actual chart consists of plotting calculated mean levels
(vertical axis) over time (horizontal axis) on a graph that
includes horizontal lines for the actual mean characteris-
tic level of the process, m, and for upper and lower control
limits
that
are
usually
determined
by
adding
and
subtracting two or more standard deviations, sX , to the
actual mean level. If, at a certain time period, the outcome
of the calculated mean lies outside the control limits, the
production process is considered to be no longer behaving
properly, and the process is stopped for appropriate
adjustments. For example, if a production process is
designed to ﬁll cans of soda pop to a mean level of 12
oz., if the standard deviation of the ﬁll levels is .1, and if
100 cans of soda are randomly drawn from the packaging
line to record ﬁll levels and calculate a mean ﬁll level x,
then the control limits on the daily calculated means
of the ﬁlling process might be given by 12 ∓3 std(x) ¼
12 ∓.03.
(a) Provide a justiﬁcation for the
X
chart procedure
described above based on asymptotic theory. Be sure
to clearly deﬁne the conditions under which your
justiﬁcation applies.
(b) Suppose that control limits are deﬁned by adding and
subtracting three standard deviations of
X to the
mean level m. In light of your justiﬁcation of the
control chart procedure in (a), what is the probability
that the production process will be inadvertently
stopped at a given time period, even though the
mean of the process remains equal to m?
(c) In the soda can-ﬁlling example described above, if the
process were to change in a given period so that the
mean ﬁll level of soda cans became 12.05 oz. what is
the probability that the control chart procedure
would signal a shutdown in the production process
in that period?
3. The lifetime of a certain computer chip that your
company manufactures is characterized by the population
distribution
fðz; yÞ ¼ 1
y ez=y Ið0;1Þ ðzÞ;
where z is measured in thousands of hours. Let (X1, . . ., Xn)
represent iid random variables with the density f(z;y) . An
outcome of the random variableYn ¼ 1 þ Pn
i¼1 Xi


=nwill
to be used to provide an estimate of y.
(a) Is it true that Yn!
m y? Is it true that Yn!
p y?
(b) Deﬁne an asymptotic distribution for Yn.
(c) Suppose n ¼ 100 and y ¼ 10. Use the asymptotic dis-
tribution you deﬁned in (b) to approximate the proba-
bility that Yn  15.
4. In each case below, the outcome of some function,
T(X(n)), of n iid random variables X(n)¼(X1,. . ., Xn) is being
considered for providing an estimate of some function of
parameters, q(y). Determine whether E(T(X(n))) ¼ q(y),
limn!1E T XðnÞ




¼ q y
ð Þ, and p lim TðXðnÞÞ


¼ qðyÞ.
(a) Xi’s ~ iid Gamma(a,b) and T(X(n)) ¼ Xn is being used to
estimate q(a,b)¼ab.
(b) Xi’s iid Gamma(a,b) and TðXðnÞÞ ¼ Pn
i¼1 Xi  Xn

2 /
(n1) is being used to estimate q(a,b)¼ab2
(c) Xi’s ~ iid Bernoulli (p) and TðXðnÞÞ ¼ Xnð1  XnÞ is
being used to estimate q(p) ¼ p(1p).
(d) Xi’s iid N(m,s2) and TðXðnÞÞ¼ðPn
i¼1 Xi  n1=2Þ=ðn þ 1Þ
is used to estimate m.
5. The daily number of customers entering a large gro-
cery store who purchase one or more dairy products is
given by the outcome of a binomial random variable Xt
with parameters p and nt for day t. The number of
customers who enter the grocery store on any given day,
nt, is itself an outcome of a random variable Nt that has
a
discrete
uniform
distribution
on
the
range
{200,201,. . .,300}. The Xt’s and the Nt’s are jointly inde-
pendent. The local dairy products commission wants an
estimate of the daily proportion of customers entering the
store who purchase dairy products and wants you to use
288
Chapter 5
Basic Asymptotics

an outcome of Xd ¼ 1=d
ð
Þ Pd
t¼1 Xt=Nt
ð
Þ, where d is num-
ber of days, as an estimate.
(a) Does Xd!
as p? Does Xd!
p p? Does Xd!
d p?
(b) Deﬁne an asymptotic distribution for Xd . If p ¼ .8,
d ¼ 300, and P300
i¼1 nt ¼ 75; 000, what is the approxi-
mate probability that Xd 2 ð:78; :82Þ?
6. Let (X1,. . .,Xn) be iid random variables with s2 < 1.
We know from Khinchin’s WLLN that
X ¼ n1 X
n
i¼1
Xi!
p u:
(a) Find a functional relationship between n, s, and e
such that
Pðx 2 ðm  e; m þ eÞÞ  :99:
(b) For what values of n and s will an outcome of X be
within ∓.1 of m with probability .99? Graph this
relationship between the values of n and s.
(c) If s ¼ 1, what is the value of n that will ensure that
the outcome of X will be within ∓.1 of m with proba-
bility .99?
(d) If s ¼ 1, and the Xi’s are normally distributed, what is
the value of n that will ensure that the outcome of X
will be within ∓.1 of m with probability ¼.99?
7. Let the random variables in the sequence {Xn} be iid
with a gamma density having parameters a ¼ 2 and b ¼ 3.
(a) What is the probability density for Xn?
(b) What is the asymptotic probability density for Xn?
(c) Plot the actual versus asymptotic probability density
for Xn when n ¼ 10.
(d) Repeat (c) for n ¼ 40. Interpret the graphs in (c) and (d)
in terms of asymptotic theory.
8. A pharmaceutical company claims that it has a drug
that is 75 percent effective in generating hair growth on
the scalps of balding men. In order to generate evidence
regarding the claim, a consumer research agency conducts
an experiment whereby a total of 1,000 men are randomly
chosen and treated with the drug. Of the men treated with
the drug, 621 experienced hair growth. Do the results
support or contradict the company’s claim?
9. A political candidate has hired a polling ﬁrm to assess
her chances of winning a senatorial election in a large
eastern state. She wants an estimate of the proportion of
registered voters that would vote for her “if the election
were held today.” Registered voters are to be randomly
chosen, interviewed, and their preferences recorded. The
polling ﬁrm will use the outcome of X as an estimate of
the proportion of voters in favor of the candidate. It is
known that currently between 40 percent and 60 percent
of the registered voters favor her in the election. She
wants to have an estimate that is within 2 percentage
points of the true proportion with probability ¼ .99.
How many registered voters must be interviewed, based
on the asymptotic distribution of X?
10. Let observations on the quantity supplied of a certain
commodity be generated by Yi ¼ xi b + Vi, where |xi| ∈[a,
b] 8i are scalar observations on ﬁxed prices, b is an
unknown slope coefﬁcient, and the Vi’s are iid random
variables having a mean of zero, a variance of s2 ∈(0, 1),
and P(|Vi|  m) ¼ 1 8i (a, b, and m are ﬁnite positive
constants). Two functions of the xi’s and Yi’s are being
considered for generating an estimate of the unknown
value of b:
^b¼ ðx0x Þ1 x0Y and ^br ¼ x0x þ k
ð
Þ1x0Y;
where k > 0, x is an (n1) vector of observations on the
xi’s and Y is an (n1) vector of the corresponding Yi’s.
(a) Deﬁne
the
means
and
variances
of
the
two
estimators of b.
(b) Is
it
true
that
limn!1E ^b
 
¼ b and=or limn!1
E ^br


¼ b?
(c) Deﬁne the expected squared distances of the two
estimators from b.
(d) Which, if either, of the estimators converges in mean
square to b?
(e) Which, if either, of the estimators converges in prob-
ability to b?
(f) Deﬁne asymptotic distributions for each of the
estimators.
(g) Under what circumstances would you prefer one esti-
mator to the other for generating an estimate of the
unknown value of b?
11. Let X1, . . .,Xn be iid random variables having contin-
uous uniform distributions of the form f(z) ¼ I(0,1) (z).
(a) Deﬁne
an
asymptotic
distribution
for
Xn ¼ n1 Pn
i¼1 Xi.
Problems
289

(b) Using
your
result
from
(a),
argue
that
P12
i¼1 Xi


 6  Z  N 0; 1
ð
Þ.
(This approximation is very accurate, and is sometimes
used for simulating N(0,1) outcomes using a uniform
random number generator.)
12. A company produces a popular beverage product that
is distributed nationwide. The aggregate demand for the
product during a given time period can be represented by
Q ¼
X
n
i¼1
Qi ¼
X
n
i¼1
ðai bi p þ ViÞ
where Qi is quantity purchased by the ith consumer, ai
>0, bi >0, E(Vi) ¼0, var(Vi)  c > 0, P(|vi|  m) ¼ 1 8i, and c
and m are positive constants. It can be assumed that the
quantities purchased by the various consumers are jointly
independent.
(a) Deﬁne an asymptotic distribution for the aggregate
quantity demanded, Q.
(b) If p ¼ 2, E(Q) ¼ 80, and if p¼5, E(Q) ¼ 50. If it costs
the company $2/unit to manufacture and distribute
the product, and if p¼$2.50, what is the asymptotic
distribution of aggregate company proﬁt during the
time period?
(c) Deﬁne an interval around the mean of aggregate com-
pany proﬁt that will contain the actual outcome of
aggregate proﬁt with (approximate) probability .95
when p¼$2.50.
13. The daily tonnage of garbage handled by the Enviro-
Safe Landﬁll Co. is represented as the outcome of a ran-
dom variable having some triangular distribution, as
X  f x; a
ð
Þ ¼ ½ :5  :25a
ð
Þ þ :25xI½a2;aðxÞ
þ ½ð:5 þ :25aÞ  :25xIða;aþ2ðxÞ
This distribution is represented graphically as follows:
f(x;a)
X
a−2
a
a+2
Enviro-Safe is in the process of analyzing whether or not
they need to expand their facilities. It wants an estimate
of the expected daily tonnage of garbage that it handles. It
has collected 4 years of daily observations on tonnage
handled (n ¼ 1,460 observations) and provides you with
the following summary statistic:
X
n
i¼1
xi ¼ 29; 200:
You may treat the observations as outcomes of iid random
variables.
(a) Use x ¼ n1 Pn
i¼1 xi to provide an estimate of a, the
expected tonnage of garbage handled.
(b) Based on the LLCLT, deﬁne an asymptotic distribu-
tion for x. You should be able to identify a numerical
value for the variance of the asymptotic distribution.
(c) Using the asymptotic distribution, how probable is it
that the outcome of x will be within .05 tons of the
actual expected value of daily garbage tonnage handled?
(d) Use Van Beeck’s inequality to provide an upper bound
to the approximation error in the probability value
that you assigned in part (c). Are you reasonably con-
ﬁdent that you provided Enviro-Safe with an accurate
“guess” of the expected daily tonnage? (Enviro-Safe
management said that they would be satisﬁed if they
could get an estimate that was “within 8 1 tons of
the actual expected daily tonnage.”) Explain.
(e) Using your estimate ^a ¼ x to estimate the density
function, as f(x;^a), what is your estimate of the proba-
bility that tonnage handled by Enviro-Safe will
exceed 21 tons on any given day?
14. A statistician wants to use iid outcomes from some
exponential distribution fðx; yÞ ¼ 1=y
ð
Þ ex=y Ið0;1Þ ðxÞ to
generate an estimate of the variance of the exponential
density, y 2. She wants to use the outcome of X2
n where
Xn ¼ n1 Pn
i¼1 Xi to generate an estimate of y2.
(a) Does E X2
n


¼ y2? Does limn!1E X2
n


¼ y2?
(b) Does plim
X2
n


¼y2?
(c) Deﬁne an asymptotic distribution for X2
n.
15. We have shown that if {Yn} is a sequence of w2 random
variables, where Yn  w2
n , then
Yn  n
ð
Þ=
ﬃﬃﬃﬃﬃﬃ
2n
p
!
d N 0; 1
ð
Þ.
Since
Yn~ w2
n ,
we
know
that
P(y2534.3816)
¼
P(y5063.1671) ¼ P(y100118.498) ¼ .90. Assign (approx-
imate) probabilities to the three events using asymptotic
distributions. How good are the approximations? Does
plim(Yn/n) ¼ 1? Why or why not?
290
Chapter 5
Basic Asymptotics

16. Let {Xn} be a sequence of random variables having
binomial densities, where Xn has a binomial density
with parameters n and p, i.e.,
Xn 
n
x
 
!
px ð1  p Þnx If0;1;2;...ng ðxÞ:
(a) Show that Xn  np
ð
Þ=
ﬃﬃﬃn
p
p 1  p
ð
Þ
ð
Þ1=2


!
d N 0; 1
ð
Þ.
(b) Deﬁne an asymptotic distribution for Xn. Use Van
Beeck’s inequality to provide a bound on the error in
approximating the probability P(xn  c) using the
asymptotic distribution for Xn. Calculate the numer-
ical value of this bound when p ¼ .3.
(c) Using the binomial density for Xn, and letting p ¼ .3,
it follows that
n
k
P(xn  k)
15
6
.8689
25
9
.8106
100
34
.8371
Assign (approximate) probabilities to the three
events using asymptotic distributions based on
your
answer
for
(b).
How
good
are
the
approximations?
(d) In using the CLT to approximate probabilities of
events for discrete random variables whose range
consists of equally spaced points, it has been found
that a continuity correction improves the accuracy of
the approximation. In particular, letting
RðXÞ ¼ x1; x2; x3; . . .
f
g where xiþ1  xi ¼ 2h>0 8i;
the continuity correction involves treating each elementary
event xj for the discrete random variable X as the interval
event (xj  h, xj + h] for the normal asymptotic distribution
of the random variable X. For example, if R(X) ¼ {0,1,2,3,. . .}
then
P x 2 ½1; 2
ð
Þ ¼ P2
x¼1 fðxÞ  ^P x 2 ð:5; 2:5
ð
Þ;
where
the latter (approximate) probability is assigned using the
appropriate asymptotic normal distribution for X. Use
the continuity correction to approximate the probabilities
of the three events in (c).
17. The Nevada Gaming Commission has been directed
to check the fairness of a roulette wheel used by the
WINBIG Casino. In particular, a complaint was lodged
stating that a “red” slot occurs more frequently than a
“black” slot for the roulette wheel used by WINBIG,
whereas red and black should occur with probability .5 if
the wheel is fair. The wheel is spun 100,000 times, and
the number of red and black outcomes were recorded. The
outcomes can be viewed as iid from some Bernoulli popu-
lation distribution: X  px ð1  p Þ1x If0;1g ðxÞ for p∈(0,1).
It was found that Pn
i¼1 xi ¼ 49,873, where xi ¼ 1 indicates
that the ith spin resulted in a red outcome.
(a) Use Xn to provide an estimate of p, the probability of
observing a red outcome.
(b) Deﬁne an asymptotic distribution for Xn.
(c) Using the outcome of Xn mean as an estimate of p in
the asymptotic distribution for Xn, how probable is it
that an outcome of Xn is within .005 of the true
probability? Use Chebyshev’s inequality to argue that
the estimate Xn for p should be very accurate in the
sense that outcomes of Xn are very close to p with
high probability.
(d) Use the Van Beeck’s inequality to provide an upper
bound to the approximation error that can occur in
assigning probability to events like P(Xn c) using the
asymptotic distribution for
Xn . Your bound will
unfortunately depend on the unknown value of p.
Estimate a value for the bound using your outcome
of Xn as an estimate of p.
(e) Deﬁne an asymptotic distribution for g(Xn) ¼ Xn (1 
Xn).
(f) Compare the asymptotic distribution of the estima-
tor in part (e) to the asymptotic distribution of the
estimator S2 ¼ Pn
i1 Xi  Xn

2/(n  1). If the sample
size
were
large,
would
you
prefer
one
of
the
estimators of p(1  p) over the other? Explain?
18. The Elephant Memory Chip Co. (EMC for short)
instructs its resident statistician to investigate the
operating-life characteristics of their new 4 gigabyte
memory chip in order to provide product information to
potential buyers. The population distribution of operating
lives can be speciﬁed as some exponential family distri-
bution. The statistician intends to draw a random sample
of 10,000 chips from EMC’s production and apply a non-
destructive test that will determine each chip’s operating
life. He then intends to use the outcome of the random
sample to provide estimates of both the mean and
Problems
291

variance of the chip’s operating life. He needs your help in
answering a few statistical questions.
(a) Letting y represent the unknown parameter in the
exponential population distribution, what is the dis-
tribution of the sample mean, Xn? What is the mean
and variance of this distribution? Does plim Xn


¼ y?
(b) The outcome of the random sample resulted in the
following two outcomes:
Xn ¼ 10:03702;
n1 X
n
i¼1
X2
i ¼ 199:09634:
Operating life is measured in 1,000 h units.
(Side Note: These are actual outcomes based on a
simulation of the random sample outcome using a spe-
ciﬁc value of y.)
The statistician uses Xn to estimate the mean life, y, of the
chips.
He
is
considering
using
either
X2
n
or
S2
n ¼ Pn
i¼1 Xi  Xn

2=ðn  1Þ
to estimate the variance,
y2, of operating lives. She asks the following questions
regarding the characteristics of X2
n and S2
n : (show your
work)
(a) Does E X2
n


¼ y2? Does E S2
n


¼ y2?
(b) Does limn!1E X2
n


¼ y? Does limn!1E S2
n


¼ y2?
(c) Does plim
X2
n


¼ y2? Does plim S2
n


¼ y2?
(d) Deﬁne asymptotic distributions for
X2
n


and
S2
n


.
Based on their asymptotic distributions, would you
recommend the use of one random variable over the
other for generating an estimate of y2? Why or why
not?
(e) Calculate the outcomes of both
X2
n


and S2
n


.
(Note: The actual value of y ¼ 10, and thus the
actual value of y2 ¼ 100.)
(f) The statistician has an idea he wants you to react to.
He doesn’t like the fact that EðX2
nÞ 6¼ y (that is what
you found–isn’t it?). He wants to deﬁne a new random
variable, Yn ¼ an X2
n


, for an appropriate sequence of
numbers {an}, so that E Yn
ð
Þ ¼ y2; 8n. Can he do it?
How? If he (and you) can, then use the appropriate
outcome of Yn to provide another estimate of y 2? Is it
true that plim (Yn) ¼ y 2?
19. Let (X1,. . .,Xn) be a random sample from a Poisson
population distribution. Derive the limiting distribution
of
T ¼ ðXn mÞ
ðS2
n =n Þ1=2
where S2
n ¼ n1 Pn
i¼1 Xi  Xn

2.
20. Liquid crystal displays (LCDs) that your wholesaling
company is marketing for a large Japanese electronics
ﬁrm are known to have a distribution of lifetimes of the
following Gamma-distribution form:
fðz; aÞ ¼
1
2a GðaÞ za1 ez=2 Ið0;1Þ ðzÞ;
where z is measured in 1,000’s of hours.
A set of n iid outcomes of Z will be used in an attempt to
obtain information about the expected value of the life-
time of the LCD’s.
(a) Deﬁne the functional form of the joint density of the
iid random variables say (X1,. . .,Xn), of LCD lifetimes.
(b) What is the density function of the random variable
Yn ¼ Pn
i¼1 Xi.
(c) Supposing that n were large, identify an asymptotic
distribution for the random variable Yn. (Note: since
you don’t know a at this point, your asymptotic dis-
tribution will depend on the unknown value of a.)
(d) If a were equal to 1/2, and the sample size was n ¼ 20,
what is the probability that yn  31.4104? Compare
your answer to the approximate probability obtained
using the asymptotic distribution you deﬁned in (c).
(e) If a were equal to 1/2, and the sample size was n ¼ 50,
what
is
the
probability
that
yn

67.5048?
Compare your answer to the approximate probability
obtained using the asymptotic distribution you
deﬁned in (c).
21. In each case below, determine whether the random
variable sequence Yn
f
g converges in probability and/or in
mean square, and if so, deﬁne what is being converged to.
(a)
Yj ¼ j þ 5
ð
Þ1 Pj
i¼1 Xi
for
j ¼ 1; 2; 3; . . .
;
Xi
0s  iid BernoulliðpÞ
(b)
Yj ¼ j1 P
j
i¼1
Xi  l
ð
Þ2
for
j ¼ 1; 2; 3; . . .
;
Xi
0s  iid PoissonðlÞ
292
Chapter 5
Basic Asymptotics

(c)
Yj ¼ j1 P
j
i¼1
Xi þ Zi
ð
Þ
for
j ¼ 1; 2; 3; . . .
;
Xi; Zi
ð
Þ0s  iid Normal
0
0
	

;
1
0
0
1
	



(d)
Yj ¼ j1 P
j
i¼1
XiZi
for
j ¼ 1; 2; 3; . . .
;
Xi; Zi
ð
Þ0s  iid Normal
0
0
	

;
1
0
0
1
	



22. In each case below, derive the probability limit and an
asymptotic distribution for n1 Pn
i¼1 Xi and a limiting
distribution for the random variable Yn; if they can be
deﬁned.
(a) Xi
0s iid BernoulliðpÞ; Yn ¼
n1 P
n
i¼1
Xi  p
n1=2 p 1  p
ð
Þ
ð
Þ1=2
(b) Xi
0s iid Gamma a; b
ð
Þ; Yn ¼
n1 P
n
i¼1
Xi  ab
n1=2a1=2b
(c) Xi
0s iid Uniform a; b
ð
Þ; Yn
n1 P
n
i¼1
Xi  :5 a þ b
ð
Þ
12n
ð
Þ1=2 b  a
ð
Þ
(d) Xi
0s iid GeometricðpÞ; Yn ¼
n1 P
n
i¼1
Xi  p1
np2
ð
Þ1=2 1  p
ð
Þ1=2
Problems
293

6
n
Sampling, Sample Moments
and Sampling Distributions
n
n
n
6.1
Introduction
6.2
Sampling
6.3
Empirical Distribution Function
6.4
Sample Moments and Sample Correlation
6.5
Properties of Xn and S2
n when Random Sampling is
from a Normal Distribution
6.6
Sampling Distributions of Functions of Random
Variables
6.7
t-and F-Densities
6.8
Random Sample Simulation
6.9
Order Statistics
6.1
Introduction
Prior to this point, our study of probability theory and its
implications has essentially addressed questions of deduction, being of the type:
“Given a probability space, what can we deduce about the characteristics of
outcomes of an experiment?” Beginning with this chapter, we turn this question
around, and focus our attention on statistical inference and questions of the
form: “Given characteristics associated with the outcomes of an experiment,
what can we infer about the probability space?”
The term statistical inference refers to the inductive process of generating
information about characteristics of a real-world population or process by
analyzing a sample of objects or outcomes from the population or process. For
example, a marketer may be interested in determining whether consumers with
a certain sociodemographic proﬁle (the population) would purchase a new prod-
uct (the characteristic); an auditor would be interested in assessing the accuracy
(the characteristic); of a ﬁrm’s accounts (the population); and a quality control
engineer would have interest in determining whether commodities are being
manufactured (the process) to within factory speciﬁcations (the characteristic).

The statistical inference problems would involve analyses of samples of
observations from the real-world population or process to generate inferences
regarding the characteristics of interest. Figure 6.1 provides a schematic over-
view of the process of statistical inference.
For an analysis of sample outcomes to lead to meaningful inferences about
characteristics
of
a
real-world
population
or
process,
the
sample
of
characteristics must be in some sense representative of the incidence of
characteristics in the population or process. The linkage between characteristics
in the sample and in the real-world population or process is established by
analyzing probability samples from the population or process. In particular,
the objects or outcomes in a probability sample are obtained using selection
procedures that establish connections between the incidence of characteristics
in the population or process and the probability of observing the characteristics’
outcomes in the sample. Then, based on probability samples and the
implications of probability theory, methods of statistical inference can be
devised that generate inferences about the characteristics of a population or
process with a degree of accuracy or representativeness that can be measured
probabilistically.
In this chapter we examine the concept of probability sampling and begin
identifying methods of generating information on population or process
characteristics from samples of observations. We focus primarily on sample
moments and their probability distributions, and what can be learned from
sample moment outcomes relative to the probability distribution associated
with the population or process from which the sample was obtained. This allows
Population
or
Process
Sample of
Objects or
Outcomes
Estimates and/or
Hypothesis Test
Outcomes
Sample of
Characteristic
Outcomes
Inferences about
Population or Process
Characteristics
Measure
Characteristics
SAMPLING
Estimator and/or
Test Statistics
Figure 6.1
Overview of the statistical
inference process.
296
Chapter 6
Sampling, Sample Moments and Sampling Distributions

us to establish foundational statistical concepts that illustrate how linkages are
formed between sample and population or process characteristics, and demon-
strate how knowledge about population or process characteristics can be
generated from sample outcomes. A detailed and formal analysis of estimation
and hypothesis-testing methodology will commence with Chapter 7 and
beyond.
6.2
Sampling
The objective of statistical inference is to generate information about relevant
characteristics of either the objects in some set of objects or the outcomes of
some stochastic processes. The term population will henceforth refer to any set
of objects with characteristics that a researcher wishes to identify, enumerate, or
generally obtain information about. The term stochastic process will refer to any
collection of experiments or measurement activities whose outcome can be
interpreted as the outcome of a collection of random variables1 and whose
characteristics are of interest to the analyst. Henceforth, we will shorten sto-
chastic process to simply process whenever the context is clear. The purpose of
sampling is to obtain information about the characteristics of a population or
process without having to examine each and every object or outcome relating to
a population or process.
If a population has a ﬁnite number of objects, then an examination of each
element of the population is conceivable, but there are reasons why the
researcher might not wish to do so. An obvious reason would be if the measure-
ment process were destructive, in which case measuring the characteristics of
all of the population elements would lead to the population’s destruction. This
would be clearly undesirable if, for example, the objects in a population were
manufactured by a ﬁrm that intended to offer them for sale (the tensile strength
of steel beams is an example of a destructive measurement process, in which the
beam is stressed to the breaking point). Another reason would be that the cost
involved in evaluating each and every member of a population may be prohibi-
tive. A related reason could be that the time available in which to perform the
analysis is insufﬁcient for a complete enumeration of the population. Finally,
the researcher may have no choice but to analyze a sample, instead of measuring
all conceivable outcomes of the characteristics of interest, as would be the case
for an ongoing manufacturing (or other) process having no stipulated or identiﬁ-
able end.
If we attempt to obtain information about relevant characteristics of a
population or process from characteristics observed in a sample from that popu-
lation or process, an obvious difﬁculty comes to mind. Namely, depending on
which sample is chosen, the incidence of sample characteristics can differ from
1More formally, the term stochastic process refers to any collection of random variables indexed by some index set T, i.e., {Xt, t ∈T} is
a stochastic process.
6.2
Sampling
297

sample to sample, even though there exists only one ﬁxed distribution of the
characteristics in the population or process of interest. It follows that the exis-
tence or level of a characteristic in a given sample will not necessarily coincide
with the existence or level of the characteristic in the population or process. The
discrepancy between sample information on characteristics, and the actual state
of affairs regarding the characteristics in the population or process is generically
referred to as sampling error.
As an explicit example of sampling error, suppose a shipment contained 100
objects (the population) of which 30 were defective (the characteristic of inter-
est). Not knowing the number of defectives in the shipment, suppose an indi-
vidual attempted to infer the proportion of the population of objects that were
defective by observing the proportion of defectives in a sample of 10 objects from
the shipment. It is unfortunately possible, depending on the particular set of
10 objects chosen, for the observed proportion of defectives in the sample to
range between 0 and 1. Thus, the actual sampling error in inferring the propor-
tion of defectives in the population could range between a low of 0 (if there
happened to be three defectives in the sample) and a high of .7 (if there happened
to be 10 defectives in the sample) in absolute value, although the researcher
would clearly be unaware of this range, since she is unaware that the actual
proportion of defectives is .3. Without any further information about how the
sample of observations from the population was obtained and/or how the sample
was representative of the population, nothing can be said about the reliability of
the inference. Note that sample information in this context is essentially use-
less – the observed proportion of defectives in the sample will lie between 0 and
1, and without any other information regarding the reliability of the inference,
we know nothing more about the population than when we started the analysis.
In order for an analyst to be able to assess the reliability with which popula-
tion or process characteristics are represented by sample information, it is
necessary for the sample observations to have been generated in such a way
that the researcher is able to establish some quantitative measures of conﬁdence
that the characteristics identiﬁed or enumerated in the sample accurately por-
tray their incidence in the population or process. In particular, a probability
sample will be useful for such inference.
Deﬁnition 6.1
Probability Sample
A sample obtained by a sampling mechanism by which (1) all members of a
population, or all potential types of process outcomes, have some possibility
of being observed in the sample, and (2) the speciﬁc sample of observations
can be viewed as the outcome of some random variable X.
In essence, a probability sample will refer to a sample whose values can be
viewed as being governed by a probabilistic relationship between the distribu-
tion of characteristics inherent in the population or process and the speciﬁc
method used to generate the sample observations. The linkage between the
distribution of characteristics in the population or process and the nature of
298
Chapter 6
Sampling, Sample Moments and Sampling Distributions

the joint density function of sample observations is exploited to design inference
reliability measures. The terminology random sample and outcome of the ran-
dom sample are used to refer to the random variable X and its outcome x in the
deﬁnition of probability sample above.
Deﬁnition 6.2
Random Sample
The random variable X whose outcome x represents a probability sample is
called a random sample, and its outcome is called the outcome of the random
sample.
One can think of a random sample X as representing all of the potential
outcomes of a probability sample, and it is a random sample because the sample
outcome cannot be anticipated with certainty. The outcome of the random
sample is then a speciﬁc set of n observations that represents a probability
sample. We will represent the n observations comprising a sample outcome as
the n rows of X, with the number of columns of X corresponding to the
dimensionality of the characteristics being measured. The terminology size of
the random sample will refer to the row dimension of X, effectively representing
the number of observations in the sample on the characteristics of interest.
Thus, the size of the random sample X ¼ Xi; i ¼ 1; . . . ; n
½
0 is n.
6.2.1
Simple Random Sampling
In this sampling design, sample observations can be conceptualized as the
outcome of a collection of iid random variables, as in the case of observing
the outcomes of a collection of independent and identical experiments. There-
fore the random sample X ¼ Xi; i ¼ 1; . . . ; n
½
0is such that Xi
0s  iid mðxÞ, and the
common probability distribution shared by the members of the random sample
is sometimes referred to as the population distribution. The terminology sam-
pling from the population distribution is also sometimes used to refer to this
type of sampling.
There are basically two generic types of simple random sampling. One type
involves sampling from an existent ﬁnite collection of objects, such as a ship-
ment of personal computers or the citizens of a particular region of the county.
The other type involves sampling from some ongoing stochastic process, such as
a manufacturing process (e.g., measuring the net weight of potato chips in a bag
of potato chips) or a market process (e.g., measuring stock market prices over
time). In the former type of sampling, the ultimate objective is to obtain infor-
mation about characteristics of the ﬁnite collection of objects under study, while
in the latter type the objective is to obtain information about the characteristics
of the stochastic process under study (e.g., the expected weight of potato chips in
a bag; the probability of producing a bag containing less than the advertised
quantity of chips; the variance of stock prices).
When an existent ﬁnite population is being sampled, simple random sam-
pling from a population distribution is alternatively referred to as random sam-
pling with replacement. In this case, there exists a ﬁnite set of N objects having
6.2
Sampling
299

certain characteristics of interest to the analyst. The formal description of the
steps involved in the sampling method is as follows.
Deﬁnition 6.3
Random Sampling with
Replacement
1. An object is selected from a population in a way that gives all objects in
the population an equal chance of being selected.
2. The characteristics of the object selected is observed, and the object is
returned to the population prior to any subsequent selection.
3. For a sample of size n, (1) and (2) are performed n times.
Since all members of the population are equally likely to occur at each
selection, the classical probability deﬁnition is appropriate for determining the
probability of observing any level of the characteristics on a given selection, as
mðzÞ ¼
number of population objects
that have characteristics level z
"
#
N
;
where z can henceforth be replaced by a vector f anywhere in the discussion
ahead depending on the number of characteristics being measured for each
selection, and N represents the number of objects in the population. The density
function m(z) is the population distribution of the characteristics.
Since all of the outcomes are independent, the probability of observing a
sample of size n that exhibits the collection of characteristic levels (x1,x2,. . .,xn)
is given by
f x1; x2; . . . ; xn
ð
Þ ¼
Y
n
i¼1
mðxiÞ:
The probability density function f(x1,. . .,xn) is referred to as the joint density of
the random sample.
Example 6.1
Simple Random
Sampling with
Replacement for
Defectives
Let a shipment of N keyboards contain J defective keyboards. (Note that in
practice, N will generally be known, while J will be unknown, and information
concerning J would be the objective of statistical inference.) Let z ¼ 1 denote a
defective keyboard and z ¼ 0 denote a nondefective keyboard. Then, if n objects
are sampled with replacement, and letting p ¼ J/N, the population distribution
of defective/nondefective keyboards is m(z;p) ¼ pz(1  p)1-z I{0,1}(z), where
p represents the proportion of defective keyboards in the population. The joint
density governing the probabilities of observing outcome (x1,. . .,xn) in the sam-
ple is given by
fðx1; . . . ; xn; pÞ ¼ P
n
i¼1 pxi ð1  p Þ1xiIf0;1g ðxiÞ ¼ p
Pn
i¼1 xið1  p ÞnPn
i¼1 xi P
n
i¼1If0;1g ðxiÞ:
Note that the incidence of nondefectives in the population (the value of p) has a
direct inﬂuence on the probabilities of events for the random sample outcomes.
300
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Thus,
a
probabilistic
linkage
is
established
between
the
incidence
of
characteristics in the population and in the sample.
□
When the population from which we are sampling is not ﬁnite, simple
random sampling will refer to an ongoing stochastic process in which all of the
random variables whose outcomes are being measured are independent and
identically distributed. In practice, this means that whatever is the underlying
random mechanism that determines the sample observations, it is unchanging
from observation to observation, and observing a particular outcome for a given
sample observation has no effect on the probability of outcomes of any other
sample observations. Then each sample observation is the outcome of a random
variable Z ~ m(z), where m(z) is the common PDF of characteristics outcomes
(also called the population distribution) associated with the stochastic process.
Since all of the observation experiments are identical and independent, the PDF
for the n random variables (X1,. . .,Xn) characterizing the outcomes of the n
observation experiments is given by f(x1,. . .,xn) ¼ Qn
i¼1 m(xi). As before, (X1,. . .,
Xn) is called the random sample, (x1,. . .,xn) is the outcome of the random
sample, and f(x1,. . .,xn) is the joint density function of the random sample.
Example 6.2
Simple Random
Sampling the Reliability
of a Manufactured
Product
The distribution of the operating lives until failure of halogen lamps produced by
a domestic manufacturer, i.e., the population distribution, is a given by a mem-
ber of the gamma family of densities, as
mðz; a; bÞ ¼
1
ba GðaÞ za1 ez=b Ið0;1Þ ðzÞ:
The lamps are all produced using an identical manufacturing process, and n
lamps are arbitrarily selected from the production line for reliability testing. The
n measurements on operating life are interpreted as the outcome of a random
sample with joint density function
fðx1; . . . ; xn; a; bÞ ¼
Y
n
i¼1
mðxi; a; bÞ
¼
1
bna ðGðaÞ Þn
Y
n
i¼1
xa1
i
ePn
i¼1 xi=b Y
n
i¼1
Ið0;1Þ ðxiÞ:
Note that the functional form of the population distribution, and in particular
the actual values of the parameters a and b, will have a direct inﬂuence on the
probabilities of events for random sample outcomes. Thus, a probabilistic link-
age is established between the incidence of characteristics in the process and in
the sample.
□
6.2
Sampling
301

6.2.2
Random Sampling Without Replacement
Random sampling without replacement is relevant for a ﬁnite existent popula-
tion of objects, but differs from random sampling with replacement in that once
the characteristics of an object are observed, the object is removed from the
population before another object is selected for observation. The sampling pro-
cedure is described as follows:
Deﬁnition 6.4
Random sampling
without replacement
1. The ﬁrst object is selected from the population in a way that allows all
objects in the population an equal chance of being selected.
2. The characteristics of the object are observed, but the object is not
returned to the population.
3. An object is selected from the remaining objects in the population in a
way that gives all remaining objects an equal chance of being selected,
and step (2) is repeated. For a sample of size n, step (3) is performed (n  1)
times.
In this case, the sampling process can be characterized as a collection of n
experiments that are neither identical nor independent. In particular, the proba-
bility of observing characteristic level xi on the ith selection depends on what
objects were observed and removed from the population in the preceding (i  1)
selections. For the ﬁrst selection, the probability of observing characteristics
level x1 is given by
mðx1Þ ¼
number of population objects
that have characteristics level x1
"
#
N
since all objects are equally likely to be selected. The density m(x1) can be
thought of as the initial population distribution. On the ith selection, i > 1,
the probability of observing characteristic level xi is conditioned on what was
observed and removed from the population previously, so that
fðxijx1; . . . ; xi1Þ ¼
number of objects remaining in the
population that have characteristics level
xi after the i  1 selections x1; . . . ; xi1
2
64
3
75
ðN  i þ 1Þ
for N  i + 1 > 0. The joint density deﬁning the probability of observing a
sample of size n that has the collection of characteristic levels (x1,. . .,xn) can
then be deﬁned by2
2This follows straightforwardly from the deﬁnitions of marginal and conditional density functions, as the reader should verify.
302
Chapter 6
Sampling, Sample Moments and Sampling Distributions

fðx1; . . . ; xnÞ ¼ mðx1Þfðx2 j x1Þfðx3 j x1; x2Þ . . . fðxn j x1; x2; . . . ; xn1Þ
¼ mðx1Þ
Y
n
i¼2
fðxi j x1; . . . ; xi1Þ:
Example 6.3
Random Sampling
Without Replacement
for Defectives
A shipment of N LCD screens contains J defectives. Suppose a random sample
without replacement of size n is drawn from the shipment. Let x ¼ 1 denote that
a screen is defective and x ¼ 0 denote a nondefective. Then, assuming N  n and
½ðn  1Þ  ðN  JÞ  Pn1
i¼1 xi  J;3
mðx1Þ ¼
J
N

x1
N  J
ð
Þ
N

1x1
If0;1g ðx1Þ;
fðx2 j x1Þ ¼
J  x1
N  1

x2
N  J  ð1  x1Þ
N  1

1x2 Y
2
i¼1
If0;1g ðxiÞ;
and in general, for the nth selection
f xnjx1; . . . ; xn1
ð
Þ ¼
J  Pn1
i¼1 xi
N  n  1
ð
Þ
"
#xn N  J  n  1  Pn1
i¼1 xi


N  n  1
ð
Þ
2
4
3
5
1xn
Y
n
i¼1
I 0;1
f
g xi
ð
Þ:
The joint density of the random sample (X1,. . .,Xn) is then given by
f x1; . . . ; xn
ð
Þ ¼ m x1
ð
Þf x2jx1
ð
Þ
Y
n
i¼3
f xijx1; . . . ; xi1
ð
Þ:
Note that the incidence of nondefectives in the population (the value of J) has a
direct inﬂuence on the probabilities of events for the random sample outcomes.
Thus,
a
probabilistic
linkage
is
established
between
the
incidence
of
characteristics in the population and in the sample. Compare this result to
Example 6.1.
□
As before, the n-variate random variable X ¼ (X1,. . ., Xn) is the random
sample, x ¼ (x1,. . ., xn) is the outcome of the random sample, and the joint
PDF f(x1,. . .,xn) is the joint density of the random sample.
6.2.3
General Random Sampling
We use the term general random sampling to refer to any other type of sampling
other than simple random sampling or sampling without replacement. In gen-
eral random sampling, observations on population or stochastic process
characteristics are generated by the outcomes of random variables that are not
independent and/or are associated with experiments that are not performed
3These conditions, coupled with the condition that x ∈{0,1}, ensure that the denominators in the density expressions are positive and
that the numerators are nonnegative.
6.2
Sampling
303

under identical conditions. Note that while we chose to list random sampling
without replacement separately in Section 6.2.2 because of its prevalence in
applications, it exhibits the features (i.e., non-independence and non-identical
experiments) that qualify it to be included in the general random sampling
category.
The joint density for the random sample in this case is some density f(x1,. . .,
xn) that is inherent to the collection of random variables and/or non-identical
experiments and their interrelationships in the sampling design. For example, if
the random variables happen to be independent, but are not identically
distributed, then the joint density of the random sample is given, in general,
by f(x1,. . .,xn) ¼ Qn
i¼1 fiðxiÞ where fi(xi) is the probability density associated with
the outcomes of the ith random variable or experiment.
Example 6.4
General Random
Sampling of Quantities
Demanded Across
Consumers
Let the quantity demanded of a commodity by consumer i in a given market be
represented by
Qi ¼ gðpi; yi; ziÞ þ Vi;
where Qi is quantity demanded, pi is the price of the commodity, yi is disposable
income,
and
zi
is
a
vector
of
substitute/complement
prices
and
sociodemographic characteristics for consumer i. The Vi0s are independent but
not necessarily identically distributed random variables whose outcomes repre-
sent deviations of Qi from g(pi, yi, zi) caused by errors in utility optimization,
lack of information and/or inherent random human behavior. For the sake of
exposition, assume that Vi ~ N(0,s2
i ). Then (Q1,. . .,Qn) is a random sample from
an experiment relating to n observations on a demand process. The ith experi-
ment consists of observing the quantity demanded by consumer i, for which
Qi ~ N(qi; g(pi, yi, zi),s2
i), and the joint density of the random sample is given by a
product of non-identical marginal densities as
fðq1; q2; . . . ; qn; ðpi; yi; zi; s2
i Þ; i ¼ 1; . . . ; nÞ ¼
Y
n
i¼1
N qi; g pi; yi; zi
ð
Þ; s2
i

	
:
Note that the characteristics of the demand process directly inﬂuence the
probabilities of events for random sample outcomes by their inﬂuence on the
functional form of the joint density of the random sample.
□
Again as before, the n-variate random variable X ¼ (X1,. . ., Xn) is the random
sample, x ¼ (x1,. . ., xn) is the outcome of the random sample, and the joint PDF
f(x1,. . .,xn) is the joint density of the random sample.
6.2.4
Commonalities in Probabilistic Structure of Probability Samples
The underlying rationale leading to the joint density function associated with a
simple random sample is essentially identical whether random sampling is from
an existent ﬁnite population of objects or from an ongoing stochastic process. In
304
Chapter 6
Sampling, Sample Moments and Sampling Distributions

both cases, an experiment with population distribution, m(z), is independently
repeated n times to obtain a random sample outcome, where the joint density of
the random sample is deﬁned by f(x1,. . .,xn) ¼ Qn
i¼1 m xi
ð
Þ. In other words, in
either case, the random sample can be thought of as a collection of iid random
variables each having the PDF m(z). In subsequent sections of this chapter we
will examine certain functions of random samples that have a number of
properties that will be useful in statistical inference applications and that are
derived from the fact that X1,. . .,Xn are iid random variables. Since the iid
property is shared by either type of random sample from a population distribu-
tion, any property of the function g(X1,. . .,Xn) deduced from the fact that X1,. . .,
Xn are iid will apply regardless of whether random sampling is from a population
that exists and is ﬁnite or is from an ongoing stochastic process.
There is also a commonality between how a probability sample is generated
via general random sampling and how a probability sample is generated by
random sampling without replacement. In the latter case, observations are
generated by a sequence of experiments that are neither independent nor
performed
under
identical
conditions,
and
this
characterization
of
the
experiments is subsumed under the general description of how general random
sampling occurs. Note, however, that general random sampling case is the
broader concept, encompassing literally a myriad of different ways a series of
experiments can be interrelated, leading to a myriad of deﬁnitions for the joint
density of the random sample. On the other hand, in random sampling without
replacement, there is an explicit structure to the deﬁnition of the joint density of
the random sample, where the sampling procedure leads to a joint density
deﬁnition based on the product of a collection of well-deﬁned conditional den-
sity functions. In either case, it becomes somewhat difﬁcult to establish general
properties of functions of random samples, g(X1,. . .,Xn), that are useful for
purposes of statistical inference. In the case of random sampling without
replacement, the mathematics involved in deriving properties of g(X1,. . .,Xn) is
generally more complex than in the case of simple random sampling – the iid
assumption involved in the latter case introduces considerable simpliﬁcations.
The sheer breadth of variations in the general random sampling case virtually
relegates analyses of the properties of g(X1,. . .,Xn) to analyses of special cases, and
few generalizations to the entire class of general random samples can be made.
We will focus primarily on random sampling from a population distribution
in the remainder of this chapter. However, Section 6.6 will present results for
deriving sampling distribution for function of random sample that applies to
general random sampling context. In later chapters we will examine statistical
inference methods that can be applied well beyond simple random sampling
contexts. We will also examine some problems of interest involving random
sampling with replacement from a ﬁnite population, but our analyses of these
problems will be limited. To begin further reading on sampling without replace-
ment that parallels some of the topics discussed in the remainder of this chapter,
and for a discussion of additional reﬁnements to random sampling techniques,
see M. Kendall, A. Stuart, and J. Keith Ord (1977), The Advanced Theory of
Statistics, Vol. 1, 4th ed., New York: MacMillan, pp. 319–324, and W.G.
Cochrane (1977), Sampling Techniques, 3rd ed., New York: Wiley.
6.2
Sampling
305

6.2.5
Statistics
In statistical inference, functions of random samples will be used to map
sample information into inferences regarding the relevant characteristics of a
population or process. These functions, such as T ¼ t(X1,. . .,Xn), will be ran-
dom variables whose probability densities depend on the joint density of the
random sample on which they are deﬁned. More speciﬁcally, inferential
procedures will involve special functions known as statistics, deﬁned as
follows:
Deﬁnition 6.5
Statistic
A real-valued function of observable random variables that is itself an observ-
able random variable, and not dependent on any unknown parameters.
By observable random variable, we simply mean a random variable whose
numerical outcomes can actually be observed in the real world. Note the fol-
lowing example:
Example 6.5
Statistics Versus
Nonobservables
Let the outcome of a Beta-distributed random variable X represent the propor-
tion of a given day’s telephone orders, received by the catalogue department of a
large retail store, that are shipped the same day the order is received. Deﬁne the
two random variables Y ¼ 100(X  .5) and W ¼ a(X  b). The random variable
Y is a statistic representing the number of percentage points above 50 percent
that are shipped the same day. The random variable W is not a statistic. It
depends on the unknown values of a and b, and until these values are speciﬁed,
the random variable is unobservable.
□
The reason for restricting our attention to statistics when attempting statis-
tical inference is obvious. We cannot utilize a function of a random sample
outcome whose range elements are unobservable to make inferences about
characteristics of the population or process from which we have sampled. In
subsequent sections, we will examine a number of statistics that will be useful
for statistical inference.
6.3
Empirical Distribution Function
There is a simple function of a random sample from a population distribution
that can be used to provide an empirical characterization of the underlying
population distribution from which a random sample is drawn. The function is
called the empirical distribution function (EDF – sometimes also referred to as
the sample distribution function), and we examine its deﬁnition and some of its
properties in this section. After we have discussed common measures used to
judge goodness of estimators in Chapter 7, we will see that the EDF represents a
useful estimator of the underlying population’s cumulative distribution func-
tion. Furthermore, the EDF can be used to test hypotheses about the appropriate
parametric family of distributions to which the population distribution belongs,
as we will examine in our discussion of hypothesis testing.
306
Chapter 6
Sampling, Sample Moments and Sampling Distributions

6.3.1
EDF: Scalar Case
The EDF in the scalar case is deﬁned as follows:
Deﬁnition 6.6
Empirical Distribution
Function: Scalar Case
Let the scalar random variables X1,. . .,Xn denote a random sample from some
population distribution. Then the empirical distribution function is deﬁned,
for t ∈(1,1), by FnðtÞ ¼ n1 P
n
i¼1
I 1;t
ð
 Xi
ð
Þ; an outcome of which is deﬁned
by ^FnðtÞ ¼ n1 P
n
i¼1
I 1;t
ð
 xi
ð
Þ:
An outcome of the EDF can be deﬁned alternatively using the size-of-set
function N() as
^FnðtÞ ¼ N fx : x  t; x 2 fx1; x2; . . . ; xngg
ð
Þ
n
;
that is, ^FnðtÞ equals the number of xi’s in the random sample outcome that have
values  t, divided by the sample size.
Example 6.6
EDF of Wheat Yields
A random sample of size 10 from the population distribution of the yield per acre
of a new wheat variety that a seed company has developed produced the follow-
ing 10 outcomes of wheat yield, in bushels/acre: {60, 71, 55, 50, 75, 78, 81, 78,
67, 90}. Then the EDF is deﬁned in Table 6.1, and graphed in Figure 6.2.
□
Given Deﬁnition 6.6, it is apparent that the EDF deﬁnes a random variable
for each value of t∈ℝ. In order to be able to assess the usefulness of the EDF in
representing characteristics of the underlying population distribution, it will be
informative to examine a number of important properties of the random variable
Fn(t). We begin by noting an important relationship between the binomial PDF
and the PDF of Fn(t).
Table 6.1
EDF of wheat yields.
t
^FnðtÞ
(1,50)
0
[50,55)
.1
[55,60)
.2
[60,67)
.3
[67,71)
.4
[71,75)
.5
[75,78)
.6
[78,81)
.8
[81,90)
.9
[90,1)
1.0
6.3
Empirical Distribution Function
307

Theorem 6.1
Relationship Between
EDF and Binomial
Distribution
Let Fn(t) be the EDF corresponding to a random sample of size n from a popula-
tion distribution characterized by F(t). Then the PDF of Fn(t) is deﬁned by
P ^FnðtÞ ¼ j
n


¼
n
j
0
@
1
A
½FðtÞ j ½1  FðtÞ nj for j 2 f0; 1; 2; . . . ; ng;
0
otherwise
8
>
>
>
<
>
>
>
:
Proof
From the deﬁnition of Fn(t), it follows that
P ^FnðtÞ ¼ j
n


¼ P
S
n
i¼1 Ið1;t ðxiÞ ¼ j


:
Note that Yi ¼ I(1,t](Xi) is a Bernoulli random variable with P(yi ¼ 1) ¼ P(xi  t)
¼ F(t) ¼ p, and P(yi ¼ 0) ¼ P(xi > t) ¼ 1  F(t) ¼ 1  p 8i ¼ 1,. . .,n, and since
(X1,. . .,Xn) is a random sample from a population distribution, Y1,. . .,Yn are iid
Bernoulli random variables. Then
Pn
i¼1 Yi
has a binomial density with
parameters n and p ¼ F(t) as,
P
S
n
i¼1 Ið1;t ðxiÞ ¼ j


¼ P
X
n
i¼1
yi ¼ j
 
!
¼
n
j
 
!
½FðtÞ j ½1  FðtÞ nj
for j ¼ 0,1,2,. . .,n, with all other values of j assigned probability zero.
n
The implication of the preceding theorem is that for a given choice of
t ∈(1,1), Fn(t) has the same probability distribution as the random variable
n1Zn where Zn has a binomial distribution with parameters n and p ¼ F(t). Now
that we have discovered this probability distribution for Fn(t), it is rather
.1
.5
1.0
50
60
70
80
90
100
t
0
ˆFn t
Figure 6.2
EDF for wheat yields.
308
Chapter 6
Sampling, Sample Moments and Sampling Distributions

straightforward to derive the mean, variance, probability limit and asymptotic
distribution of Fn(t):
Theorem 6.2
EDF Properties
Let Fn(t) be the EDF deﬁned in Theorem 6.1. Then, 8t∈(1,1),
a. E(Fn(t)) ¼ F(t),
b. var(Fn(t)) ¼ n1[F(t)(1  F(t))],
c. plim(Fn(t)) ¼ F(t),
d. Fn(t) 
a N(F(t), n1[F(t)(1  F(t))]).
Proof
Since Fn(t) can be represented as n1Zn, where Zn has a binomial density with
E(Zn) ¼ nF(t) and var(Zn) ¼ nF(t)(1  F(t)), properties (a) and (b) follow immedi-
ately. Property (c) follows from the fact that Fn(t) !
m F(t), since E(Fn(t)) ¼ F(t) 8n
and var(Fn(t)) ! 0 as n ! 1, which in turn implies Fn(t) !
p F(t) by mean square
convergence. Property (d) follows from the fact that Fn(t) ¼ n1Zn is the average
of n iid Bernoulli random variables, and it is known from the Lindberg-Levy CLT
that this average has an asymptotic normal distribution with mean F(t) and
variance n1[F(t)(1  F(t))].
n
It will be seen from our discussion of estimator properties in Chapter 7 that
the properties possessed by Fn(t) will make it a good statistic to use in providing
information about F(t). In particular, the distribution of Fn(t) is centered on F(t);
the variance, and thus “spread” of the distribution of Fn(t) decreases as the
sample size increases; and the probability that outcomes of Fn(t) agree with F(t)
within any arbitrarily small nonzero tolerance converges to one as the sample
size increases without bound.
The EDF can be used to generate estimates of probabilities in a way that is
analogous to the way a CDF generates information on the probability that
x ∈(a,b], namely, ^Pnðx 2 ða; bÞ ¼ ^FnðbÞ  ^FnðaÞ provides an empirical estimate
of the appropriate probability. Properties of the random variable Pn(x ∈(a,b]) ¼
Fn(b)  Fn(a) generating these empirical estimates are established in the follow-
ing theorem.
Theorem 6.3
Properties of Estimated
Probabilities from EDF
Let Fn(t) be the EDF deﬁned in Theorem 6.1. Then 8 t ∈(1,1), and for a < b,
a. E(Fn(b)  Fn(a)) ¼ F(b)  F(a),
b. var(Fn(b)  Fn(a)) ¼ n1 [F(b)  F(a)] [1  F(b) + F(a)],
c. plim(Fn(b)  Fn(a)) ¼ F(b)  F(a),
d. Fn(b)  Fn(a) 
a N(F(b)  F(a), n1[F(b)  F(a)] (1  F(b) + F(a))).
Proof
Property (a) follows directly from applying the expectation operation to the
linear combination of the two random variables and using Theorem 6.2, part
(a). Property (c) follows from Theorem 6.2, part (c), and the fact that the proba-
bility limit of a sum is the sum of the probability limits. To prove part (b), ﬁrst
6.3
Empirical Distribution Function
309

note that our previous results concerning variances of linear combinations of
random variables (Section 3.11) implies that
varðFn ðbÞ  Fn ðaÞÞ ¼ varðFn ðbÞÞ þ varðFn ðaÞÞ  2covðFn ðbÞ; Fn ðaÞÞ:
Values of the variance terms are known from Theorem 6.2, part (b). To ﬁnd the
value of the covariance term, ﬁrst note by deﬁnition,
covðFn ðbÞ; Fn ðaÞÞ ¼ cov n1 X
n
i¼1
Ið1;b ðXiÞ; n1 X
n
j¼1
Ið1;a ðXjÞ
 
!
¼ n2 X
n
i¼1
X
n
j¼1
covðIð1;b ðXiÞ; Ið1;a ðXjÞÞ
¼ n1 covðIð1;b ðX1Þ Ið1;a ðX1ÞÞ:
The last equality follows from the fact that, for i 6¼ j, I(1,b](Xi) and I(1,a](Xj)
are independent random variables since Xi and Xj are independent, which
implies the associated covariance terms are zero, and the n remaining covari-
ance terms are all identical and represented by the covariance term in the last
equality involving the functions of X1 (since X1,. . .,Xn are iid). Then since
cov(Y,Z) ¼ E(YZ)  E(Y)E(Z), by letting Y ¼ I(1,b](X1) and Z ¼ I(1,a](X1), we
have that
covðFn ðbÞ; Fn ðaÞÞ ¼ n1 ½E Ið1;b ðx1Þ Ið1;a ðx1Þ

	
 FðbÞFðaÞ
¼ n1 ½FðaÞ  FðbÞFðaÞ
¼ n1 FðaÞ½1  FðbÞ;
where the next-to-last equality follows from the fact that I(1,b](x1) I(1,a](x1) ¼
I(1,a](x1) 8 x1, since a < b.
Having found the value of the covariance term, we ﬁnally have that
varðFn ðbÞFn ðaÞÞ ¼ n1 FðbÞ½1  FðbÞ þ n1 FðaÞ½1  FðaÞ  2 n1 FðaÞ½1  FðbÞ
¼ n1 ½FðbÞ  FðaÞ½1  FðbÞ þ FðaÞ:
To prove part (d), ﬁrst note that Fn(b) and Fn(a) have an asymptotic bivariate
normal distribution with mean vector [F(b) F(a)]0 and covariance matrix
n1S ¼ n1 FðbÞ 1  FðbÞ
½

FðaÞ 1  FðbÞ
½

FðaÞ 1  FðbÞ
½

FðaÞ 1  FðaÞ
½



;
which follows from applying the multivariate Lindberg-Levy CLT to the sample
means of the iid Bernoulli random variables that deﬁne the random variables
Fn(b) and Fn(a). Now deﬁne the function g(Fn(b),Fn(a)) ¼ Fn(b)  Fn(a), g being a
function of the two asymptotically normal random variables, and note that
Theorem 5.40 is applicable. In particular, @g=@FnðbÞ
ð
Þ ¼ 1 and @g=@FnðaÞ
ð
Þ ¼ 1
so that G ¼ [1 1], and thus n1 G S G0 ¼ n1 [F(b)  F(a)][1  F(b) + F(a)].
Then given g(F(b), F(a)) ¼ F(b)  F(a), Theorem 5.39 implies part (d) of the
theorem.
n
The following example illustrates the use of the EDF to provide empirical
estimates of probabilities.
310
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Example 6.7
Calculating Empirical
Probabilities from EDF
Referring to the EDF in the wheat yield example (Example 6.6), an empirical
calculation of the probability that wheat yield is  76 bushels results in ^Pðx  76Þ
¼ ^Fnð76Þ ¼ :6: The probability that wheat yield is > 62 bushels and  80 bushels is
given by ^Pð62<x  80Þ ¼ ^Fnð80Þ  ^Fnð62Þ ¼ :8  :3 ¼ :5.
□
It can be shown that the convergence of Fn(t) to F(t) in probability as n ! 1
can be strengthened to almost-sure convergence, i.e., limn!1 (Fn(t)) ¼ F(t)
occurs with probability 1 for any t. Furthermore, it can be shown that limn!1
(Fn(t)) ¼ F(t) occurs simultaneously 8t ∈(1,1) with probability 1. These
results are given by the celebrated Glivenko-Cantelli theorem, which involves
convergence to zero of the supremum (or maximum if it exists) over all
t ∈(1,1) of the absolute difference between Fn(t) and F(t).
Theorem 6.4
Glivenko-Cantelli’s
Theorem for EDF
Convergence
Let Dn ¼ supt {|Fn(t)  F(t)|}. Then P(limn!1 Dn ¼ 0) ¼ 1.
Proof
See V. Fabian and J. Hannan (1985), Introduction to Probability and Mathemat-
ical Statistics, New York: Wiley, pp. 80–82.
Note that the Glivenko-Cantelli Theorem provides important additional
information on the use of outcomes of Fn(t) as a means of providing empirical
representations of F(t). In particular, the theorem implies that the sequence of
functions {Fn} converges as n ! 1 to the function F with probability 1; this
interpretation being supported by the fact that Fn(t) ! F(t) 8t with probability 1.
Thus, for large enough n, Fn represents a useful approximation to the function F
over its entire domain, and it is the Glivenko-Cantelli theorem that provides a
rigorous justiﬁcation for inferring the shape and functional form of F from the
shape and functional form of Fn.
6.3.2
EDF: Multivariate Case
The empirical distribution function can be extended to the case where the
random sample from the population distribution consists of a collection of
multivariate random variables, as follows.
Deﬁnition 6.7
Empirical Distribution
Function - Multivariate
Case
Let the (k  1) random vectors X1,. . .,Xn denote a random sample from some
population distribution. Then the empirical distribution function is deﬁned
for t ¼ ½t1; . . . ; tk02ℝk and A(t) ¼ k
i¼1 (1, ti] as Fn ðtÞ ¼ n1 Pn
i¼1 IAðtÞðXiÞ,
with an outcome deﬁned by ^FnðtÞ ¼ n1 Pn
i¼1 IAðtÞ xi
ð
Þ:
6.3
Empirical Distribution Function
311

An outcome of the EDF can be deﬁned alternatively using the size-of-set
function N() as
^FnðtÞ ¼ Nðfx:x  t; x 2 fx1, . . . ,xnggÞ
n
;
i.e., ^FnðtÞ equals the number of xi -vectors in the outcome of the random sample
that have values  the vector t, divided by the sample size.
The properties of the EDF in the multivariate case parallel those of the EDF
in the scalar case. In particular, all of the previous theorems apply analogously to
the multivariate case by simply reinterpreting t, a, and b as (k  1) vectors
instead of scalars in the statement of the theorems, and changing the condition
8t ∈(1, 1) to 8t ∈ℝk. This follows because precisely the same arguments
based on the relationship between the EDF and the binomial distribution apply
full well to the multivariate case. The proofs of Theorem 6.1, 6.2, and 6.3 in the
multivariate case are in fact analogous to the scalar case and are left to the
reader.
6.4
Sample Moments and Sample Correlation
Using a random sample from a population distribution (i.e., simple random
sampling), which we assume is the case throughout this section, statistics called
sample moments can be deﬁned that represent sample counterparts to the
moments
of
the
population
distribution
(henceforth
called
population
moments). The sample moments have properties that make them useful for
estimating the values of corresponding population moments. Sample moments
also form the basis for the method-of-moments estimation procedure which can
be used to provide information on other characteristics of the population distri-
bution besides moments. The method-of-moments procedure will be examined
in Chapter 8.
The deﬁnitions of the various sample moments can all be uniﬁed through
the use of the empirical distribution function concept. Speciﬁcally, applications
of the empirical substitution principle lead to the appropriate sample moment
deﬁnitions.
Deﬁnition 6.8
Empirical Substitution
Principle for EDFs
Let X ¼ X1; . . . ; Xn
½
0 be a random sample from a population distribution
having CDF F. Let q ¼ q(F) be any function of F. Then the empirical substitu-
tion principle representation of q ¼ q(F) is given by ^q ¼ q(^Fn), where ^Fn is the
EDF outcome based on X and used to estimate F.
6.4.1
Scalar Case
In order to use the empirical substitution principle to deﬁne sample moments
when (X1,. . .,Xn) is a collection of scalar random variables, ﬁrst note that
moments of the population distribution about the origin or mean can be
expressed as functions of the CDF F. Speciﬁcally, letting EF denote an expecta-
tion taken with respect to the probability distribution implied by F, we have
312
Chapter 6
Sampling, Sample Moments and Sampling Distributions

m0
r ¼ EF ðXrÞ and mr ¼ EF ((X  EF(X))r), which are functions of F. Substituting the
EDF outcome, ^Fn, for F when taking the expectations leads to the deﬁnition of
sample moments about the origin and mean via the empirical substitution
principle.
Deﬁnition 6.9
Sample Moments About
the Origin and Mean
Based on the EDF
Let the scalar random variables X1,. . .,Xn be a random sample with EDF
outcome ^Fn. Then outcomes of the rth order sample moments about the origin
and mean are deﬁned as:
Sample moments about the origin: m0r ¼ E^FnðXrÞ
Sample moments about the mean: mr ¼ E^Fn ðX  E^FnðXÞ Þr


In effect, when deﬁning sample moments, one proceeds as if X had the CDF
^Fn, and calculates moments associated with the probability distribution deﬁned
by ^Fn in precisely the same way as presented in Chapter 3. The computational
difference between the moments deﬁned in Section 3.6 and the sample moments
deﬁned here relates simply to which probability distribution is used in taking
the expectations – the one implied by F or by ^Fn.
Expectations taken with respect to the probability distribution represented
by the EDF outcome ^Fn have a common mathematical deﬁnition, regardless of
the form of the underlying CDF, F. To see this, ﬁrst recall that in either the
discrete or continuous case ^Fn is a step function whose incremental value at each
step equals the observed sample relative frequency of the random variable
outcome corresponding to the step (recall Figure 6.2). Since by its deﬁnition ^Fn
can always be interpreted as a CDF for some discrete random variable (i.e., the
EDF satisﬁes all the properties necessary for it to behave as a genuine CDF),
the value of an incremental step can be interpreted as the probability assigned to
the corresponding random variable outcome by ^Fn. In the case where an outcome
value, say x, is observed only once in the outcome of a random sample of size n,
it follows that the probability is assigned as ^pðxÞ ¼ 1=n. In general, the probabil-
ity assigned by ^Fn to an outcome x is given by ^pðxÞ ¼ n1 Pn
i¼1 I x
f gðxiÞ; which is
the relative frequency of the occurrence of outcome x in the random sample
outcome (x1,. . .,xn).
Now let
^RðXÞ ¼ fx : ^pðxÞ>0g be the set of x-values assigned a positive
density weighting by ^pðxÞ, i.e., ^RðxÞ is the collection of unique values in the
sample outcome (x1,. . .,xn). It follows that the expectation of g(X) with respect to
^pðxÞ, or equivalently with respect to the probability distribution implied by ^Fn, is
E^Fn gðXÞ
ð
Þ ¼
X
x2^RðXÞ gðxÞ^pðxÞ ¼
X
x2^RðXÞ
gðxÞ n1 X
n
i¼1
I x
f g xi
ð
Þ
¼ n1
X
x2^RðXÞ
X
n
i¼1
g xi
ð
Þ Ifxg ðxiÞ ¼ n1 X
n
i¼1
g xi
ð
Þ
6.4
Sample Moments and Sample Correlation
313

wherethethirdequalityfollowsbecauseonlyterms for whichxi ¼ xaffectthevalue
of the inner summation term. Then, deﬁning g(X) ¼ Xr or g(X) ¼
X  E^FNðXÞ

r
and taking expectations with respect to ^Fn, we obtain the following alternative
deﬁnition of sample moment outcomes.
Deﬁnition 6.10
Sample Moments About
the Origin and Mean
Derived from the EDF
Assume the conditions of Deﬁnition 6.7. Then sample moment outcomes
can be deﬁned as
Sample Moments about the Origin: m0r ¼ n1 Pn
i¼1 xr
i
Sample Moments About the Mean: mr ¼ n1 Pn
i¼1 xi  xn
ð
Þr
where xn ¼ m01 ¼ n1 Pn
i¼1 xi:
We emphasize that regardless of which representation, Deﬁnition 6.9 or
Deﬁnition 6.10, is used in deﬁning sample moment outcomes, all of the previ-
ous properties of moments m0
r and mr presented in Chapter 3 apply equally as well
to the outcomes of sample moments m0r and mr, with Fn taking the place of F.
This is so because sample moment outcomes can be interpreted as the appropri-
ate moments of a distribution deﬁned by the discrete CDF ^Fn.
We now present a number of important properties of sample moments about
the origin that are suggestive of the usefulness of sample moment outcomes for
estimating the values of corresponding population moments.
Theorem 6.5
Properties of M0r
Let M0r ¼ n1 Pn
i¼1 Xr
i be the rth sample moment about the origin for a random
sample (X1,. . .,Xn) from a population distribution. Then, assuming the appropri-
ate population moments exist,
a. E M0r
ð
Þ¼ m0
r,
b. Var M0r
ð
Þ ¼ n1 m0
2r  m0
r
ð
Þ2


,
c. plim M0r
ð
Þ ¼ m0
r,
d.
M0r  m0
r
ð
Þ= var M0r
ð
Þ
½
1=2 !
d N 0; 1
ð
Þ,
e. M0r 
a N m0
r; var M0r
ð
Þ
ð
Þ.
Proof
(a) E M0r
ð
Þ ¼ E n1 Pn
i¼1 Xr
i

	
¼ n1 Pn
i¼1 E Xr
i

	
¼ n1 Pn
i¼1 m0
r ¼ m0
r , since E Xr
i

	
¼ m0
r 8i because (X1,. . .,Xn) is a random sample with iid elements.
(b) var M0r
ð
Þ ¼ var (n1 Pn
i¼1 Xr
i ) ¼ n2 var( Pn
i¼1 Xr
i ), and note that the random
variables Xr
1; . . . ; Xr
n in the sum are iid since Xr
i is the same real-valued
function of Xi 8i, the Xi0s are identically distributed, and Theorem 2.9 applies.
It follows from independence and the results on variances of linear
combinations of random variables that
var M0
r
ð
Þ ¼ n2 X
n
i¼1
var Xr
i

	
¼ n2 X
n
i¼1
m0
2r  m0
r
ð
Þ2
h
i
¼ n1 m0
2r  m0
r
ð
Þ2
h
i
:
314
Chapter 6
Sampling, Sample Moments and Sampling Distributions

(c) Since E(M0r) ¼ m0
r 8n, and since var M0r
ð
Þ ! 0 as n ! 1, then Corollary 5.2
with k ¼ 1 implies that M0r !
m m0
r so that plim(M0r) ¼ m0
r.4
(d) and (e) Since (Xr
1; . . . ; Xr
n) are iid random variables with E(Xr
i) ¼ m0
r and var Xr
i

	
¼ m0
2r  (m0
r)2 8i, it follows upon substitution into the Lindberg-Levy central
limit theorem that
Zn ¼
Pn
i¼1 Xi  nm0
r
n1=2
m02r  m0r
ð
Þ2
h
i1=2 !
d Nð0; 1Þ:
Multiplying by 1 in the special form n1/n1 yields
Zn ¼
M0r  m0
r
n1=2
m02r  m0r
ð
Þ2
h
i1=2 !
d Nð0; 1Þ;
which in turn implies by Deﬁnition 5.10 that
M0r 
a N m0
r; n1 m0
2r  m0
r
ð
Þ2
h
i


.
n
In summary, the properties of M0r presented in Theorem 6.5 indicate that the
expected value of a sample moment is equal to the value of the corresponding
population moment, the variance of the sample moment monotonically
decreases and converges to zero as n ! 1, the sample moment converges in
probability to the value of the corresponding population moment, and the
sample moment is approximately normally distributed for large n. In the context
of utilizing outcomes of M0r as estimates of m0
r, the properties indicate that the
outcomes correctly estimatem0
r on average, the spread of the estimates decreases
and the outcomes of M0r
become arbitrarily close to m0
r
with probability
approaching one as the sample size increases, and the outcomes of M0r are
approximately normally distributed around m0
r for large enough sample sizes.
The fact that outcomes of M0r are correct on average and become highly accurate
individually with high probability as n increase contribute to M0rs being a useful
estimator of m0
r, as will be discussed further in Chapter 7. The fact that M0r is
approximately normally distributed for large enough n will facilitate testing
hypotheses about the value of m0
r, to be discussed in Chapters 9 and 10.
The ﬁrst-order sample moment about the origin is of particular importance
in a number of point estimation and hypothesis-testing situations, and it is given
a special name and symbol.
Deﬁnition 6.11
Sample Mean
Let (X1,. . .,Xn) be a random sample. The sample mean is deﬁned by
Xn ¼ n1 P
n
i¼1
Xi ¼ M01.
4An alternative proof, requiring only that moments up to the rth-order exist, can be based on Khinchine’s WLLN. Although we will not
use the property later, the reader can utilize Kolmogorov’s SLLN to also demonstrate that M0r !
as m0
r (see Chapter 5).
6.4
Sample Moments and Sample Correlation
315

Based on sample moment properties, we know that E Xn

	
¼ m; var Xn

	
¼
n1 m0
2  m2

	
¼ s2=n; plim Xn

	
¼ m; and Xn 
a N m; s2=n

	
. As a preview to a
particular problem of statistical inference, note that Xn has properties that
might be considered useful for estimating the population mean, m. In particular,
the PDF of Xn is centered on m, and as the size of the random sample increases,
the density ofXn concentrates within a small neighborhood of points around m so
that it becomes ever more improbable that an outcome of Xn would occur far
from m. Thus, outcomes of Xn can be useful as estimates of the unknown value
of a population mean.
The result on asymptotic normality extends to vectors of sample moments
about the origin, in which case a multivariate asymptotic normal density is
appropriate.
Theorem 6.6
Multivariate Asymptotic
Normality of Sample
Moments About the
Origin
n1=2
M01  m0
1
...
M0r  m0
r
2
64
3
75 !
d N
0;
r1
S
rr


and
M01
...
M0r
2
64
3
75 
a N
m0
1; . . . ; m0
r
ð
Þ0; n1S

	
;
where the nonsingular covariance matrix S has typical (j,k) entry equal to
sjk ¼ m0
jþk  m0
jm0
k:
Proof
The proof relies on the multivariate version of the Lindberg-Levy central limit
theorem. Let Yi ¼ ( X1
i ; . . . ; Xr
i )0. Since X1,. . .,Xn is a random sample with iid
elements, it follows from Theorem 2.9 that (Y1,. . .,Yn) are independent (r  1)
random vectors with E(Yi) ¼ m and Cov(Yi) ¼ S, 8 i, where m ¼ ( m0
1; . . . m0
r )0.
Then, given S is nonsingular, the multivariate Lindberg-Levy CLT applies,
establishing the convergence in distribution result, which in turn implies the
asymptotic density result.
The typical entry in S is given by
sjk ¼ cov Xj; Xk


¼ E Xj m0
j

	
Xk m0
k


¼ E Xjþk m0
j Xk m0
k Xj þm0
jm0
k


¼ m0
jþk  m0
jm0
k:
n
We will not study properties of sample moments about the mean in detail.
Unlike the case of sample moments about the origin, the properties of higher-
order sample moments about the mean become progressively more difﬁcult to
analyze. We will concentrate on properties of the second-order sample moment
about mean, called the sample variance. The reader interested in the general
properties of sample moments about the mean can refer to R. Serﬂing (1980),
Approximation Theorems of Mathematical Statistics, New York: John Wiley,
pp. 69–74.
316
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Deﬁnition 6.12
Sample Variance
Let X1,. . .,Xn be a random sample of size n. The sample variance is deﬁned as5
S2
n ¼ n1 P
n
i¼1
Xi  Xn

	2 ¼ M2.
Some important properties of the sample variance are presented in the
following theorem.
Theorem 6.7
Properties of S2
n
Let S2
n be the sample variance for a random sample (X1,. . .,Xn) from a population
distribution. Then, assuming the appropriate population moments exist,
a. E S2
n

	
¼
n1
n

	
s2
b. var S2
n

	
¼ n1
n  1
ð
Þ=n
ð
Þ2m4 
n  1
ð
Þ n  3
ð
Þ=n2

	
s4
h
i
c. plim(S2
n) ¼ s2
d. n1=2 ðS2
n  s2Þ !
d Nð0; m4  s4Þ
e. S2
n 
a N s2; n1 ðm4  s4Þ

	
.
Proof
(a) E S2
n

	
¼ E
Pn
i¼1 XiXn
ð
Þ
2
n


¼ E
Pn
i¼1 XimþmXn
ð
Þ
2
n


¼ E
Pn
i¼1 Xim
ð
Þ2
½

n

n mXn
ð
Þ
2


n


¼ s2  s2
n ¼
n1
n

	
s2
(b) The proof follows from expressing var(S2
n) ¼ E(S2
n  s2)2 in terms of Xi0s and
taking expectations. The proof is straightforward conceptually, but quite
tedious algebraically. The details are left to the reader, or see R.G. Krutchkoff
(1970), Probability and Statistical Inference, New York: Gordon and Breach,
pp. 154–157.
(c) Since E(S2
n) ¼ ((n  1)/n) s2 ! s2 as n ! 1, and since limn!1 var (S2
n) ¼ 0,
then Corollary 5.2 implies that S2
n !
m s2, so that plim(S2
n) ¼ s2.
(d) and (e) First note that
nS2
n ¼
X
n
i¼1
Xi  Xn

	2 ¼
X
n
i¼1
Xi  m þ m  Xn

	2
¼
X
n
i¼1
Xi  m
ð
Þ2 þ 2 m  Xn

	 X
n
i¼1
Xi  m
ð
Þ þ n m  Xn

	2;
so that
n1=2 S2
n  s2

	
¼
Pn
i¼1 Xi  m
ð
Þ2  ns2
n1=2
þ 2 mXn

	
n1=2 X
n
i¼1
Xi  m
ð
Þ þ n1=2 m  Xn

	2
5Some authors deﬁne the sample variance as S2
n ¼ n= n  1
ð
Þ
ð
ÞM2; so that E S2
n

	
¼ s2, which identiﬁes S2
n as an unbiased estimator of
s2 (see Section 7.2). However, this deﬁnition would be inconsistent with the aforementioned fact that M2, and not n= n  1
ð
Þ
ð
ÞM2, is the
second moment about the mean, and thus the variance, of the sample or empirical distribution function, ^Fn.
6.4
Sample Moments and Sample Correlation
317

Of the three terms added together in the previous expression, all but the ﬁrst
converge in probability to zero. To see this, note for the third term, p lim n1=2

	
m  Xn

	2 ¼ p lim n1=2op n1=2

	

	
¼ 0. For the second term, n1=2 Pn
i¼1 Xi  m
ð
Þ
!
d N 0; s2

	
by the LLCLT, and plim m  Xn

	
¼ plim(m)  plim(Xn ) ¼ 0, so by
Slutsky’s theorem, the second term converges in distribution, and thus in
probability, to the constant 0.
Regarding the ﬁrst term, let Yi ¼ (Xi  m)2, and note that E(Yi) ¼ s2 and
var(Yi) ¼ m4  s4. Then since the Yi’s are iid, the ﬁrst term converges in distri-
bution to N 0; m4  s4

	
by the LLCLT and Slutsky’s theorem. Thus, by Slutsky’s
theorem,
n1=2 S2
n  s2

	
!
d N 0; m4  s4

	
; so that S2
n 
a N s2; n1 m4  s4

	

	
:
n
As another preview to a particular problem of statistical inference, note that
S2
n has characteristics that can be useful for estimating the population variance, s2.
Speciﬁcally, the distribution of S2
n becomes centered on s2 as n ! 1, and as the
size, n, of the random sample increases, the density of S2
n concentrates within a
small neighborhood of points around s2 so that it becomes highly probable that
an outcome of S2
n will occur close to s2.
Example 6.8
Sample Measure
Variance Calculation
Calculating outcomes of the sample mean and sample variance for the wheat
yield data presented in Example 6.6 yields, respectively, x ¼ P10
i¼1
xi
10 ¼ 70:5 and
s2 ¼ P10
i¼1
xix
ð
Þ2
10
¼ 140:65.
□
6.4.2
Multivariate Case
When the random sample consists of a collection of k-variate random vectors
X1,. . .,Xn (i.e., where k characteristics are observed for each of the n sample
observations), one can deﬁne sample means, sample variances, or any rth –
order sample moment for each of the k entries in the Xi – vectors. Furthermore,
the concept of joint sample moments between pairs of entries in the Xi vectors
becomes relevant in the multivariate case, and in particular one can deﬁne the
notion of sample covariance and sample correlation.
The method of deﬁning sample moments in the multivariate case is analo-
gous to the approach in the scalar case – use the empirical substitution principle.
All of the aforementioned moments can be deﬁned as expectations taken with
respect to the probability distribution deﬁned by the (joint) EDF outcome, ^Fn,
just as population moments can be deﬁned using the population distribution
represented by the (joint) CDF, F. Using analogous reasoning to the scalar case,
the PDF implied by ^Fn is given by ^pðxÞ ¼ n1 Pn
i¼1 I x
f g xi
ð
Þ, i.e., the probability
assigned to the vector outcome x is the relative frequency of the occurrence of x
in the random sample of vector outcomes (x1,. . .,xn). Then, following an
318
Chapter 6
Sampling, Sample Moments and Sampling Distributions

argument analogous to the scalar case, expectations of g(X) with respect to the
discrete CDF ^Fn can be deﬁned as
E^Fn gðXÞ
ð
Þ ¼
X
x2^RðXÞ
gðxÞ^pðxÞ ¼ n1 X
n
i¼1
gðxiÞ:
By appropriate deﬁnitions of g(X), one can deﬁne rth sample moments about the
origin and mean for each entry in the vector X, as well as covariances between
entries in the vector X. In particular, these function deﬁnitions would be
g(X) ¼ X[j]r, g(X) ¼ (X[j]  x[j])r, and g(X) ¼ (X[i]  x[i]) (X[j]  x[j]), where x[‘]
¼ E^Fn X½‘
ð
Þ , leading to the following deﬁnition of sample moments in the
multivariate case.
Deﬁnition 6.13
Sample Moments,
Multivariate Case
Let the (k  1) vector random variables X1,. . .,Xn be a random sample from a
population distribution. Then the following outcomes of sample moments
can be deﬁned for j and ‘ ∈{1,2,. . .,k}:
Sample moments about the origin: m0r j½  ¼ n1 S
n
i¼1 xi j½ r
Sample means: x½j ¼ m01½j ¼ n1 S
n
i¼1 xi j½ 
Sample moments about the mean: mr ½j ¼ n1 S
n
i¼1 ðxi j½   x½j Þr
Sample variances: s2 ½j ¼ m2 ½j ¼ n1 S
n
i¼1 ðxi j½   x½j Þ2
Sample covariance: sj‘ ¼ n1 P
n
i¼1
xi j½   x j½ 
ð
Þ xi ‘½   x ‘½ 
ð
Þ
The properties of each of the sample moments about the origin and each of
the sample variances are precisely the same as in the scalar case. The proofs are
the same as in the scalar case upon utilizing marginal population distributions
for each X[j], j ¼ 1,. . .,k. The sample covariance has no counterpart in the scalar
case, and we examine its properties below.
Sample Covariance
For clarity of exposition, we will examine the case where
k ¼ 2, and distinguish the two random variables involved by X and Y. A random
sample of size n will then be given by the n two-tuples ((X1, Y1),. . .,(Xn, Yn)),
which can be interpreted as representing observations on two characteristics, xi
and yi, for each of n outcomes of an experiment. Applications to cases where
k > 2 are accomplished by applying the subsequent results to any pair of random
variables in the (k  1) vector X.
Theorem 6.8
Properties of Sample
Covariance
Let ((X1,Y1),. . .,(Xn,Yn)) be a random sample from a population distribution and
let SXY be the sample covariance between X and Y. Then, assuming the appro-
priate population moments exist,
a. E SXY
ð
Þ ¼
n1
n

	
sXY
b. var SXY
ð
Þ ¼ n1 m2;2  m1;1

	2


þ o n1

	
6.4
Sample Moments and Sample Correlation
319

c. plim(SXY) ¼ sxy
d. Sxy 
a N(sXY, n1(m2,2  (m1,1)2))
Proof
(a) Examine the ith term in the sum, and note that
E
Xi  Xn

	
Yi  Yn

	

	
¼ E XiYi  1
nXi
X
n
j¼1
Yj  1
nYi
X
n
j¼1
Xj þ 1
n2
X
n
j¼1
Xj
X
n
j¼1
Yj
 
!
:
Since (X1,Y1),. . .,(Xn,Yn) are iid, Xi is independent of Xj and Yj, for i 6¼ j, and Yi
is independent of Xj and Yj for i 6¼ j. Furthermore, since the (Xi,Yi)0s have the
same joint density function, and thus the same population moments 8i, it
follows that
E
Xi  Xn

	
Yi  Yn

	

	
¼ m0
1;1  2
n m0
1;1 þ n  1
ð
ÞmXmY


þ 1
n2 nm0
1;1 þ n n  1
ð
ÞmXmY


¼
n  1
n


m0
1;1  mXmY

	
¼
n  1
n


sXY:
Using this result, it follows that
E SXY
ð
Þ ¼ 1
n
X
n
i¼1
n  1
n


sXY ¼
n  1
n


sXY :
(b) The procedure used to derive the approximation is based on Taylor series
expansions, and can be found in M. Kendall and A. Stuart, The Advanced
Theory of Statistics, Vol. 1, 4th ed., pp. 246–250. An alternative motivation
for the approximation is given in the proof of part (d).
(c) Since E(SXY) ¼ ((n  1)/n) sXY ! sXY and var(SXY) ! 0 as n ! 1, it follows
by Corollary 5.2 that SXY!
m sXY, which in turn implies that plim (SXY) ¼ sXY.
(d) First note that the sample covariance can alternatively be written as
SXY ¼
P
n
i¼1
XiYi
n  XnYn


¼ M01;1  XnYn

	
; where M01;1 ¼ 1=n
ð
Þ Pn
i¼1 XiYi.6
Now examine the iid random vectors
Xi Yi
Xi
Yi
2
664
3
775; i ¼ 1; . . . ; n;
and note that 8i
6This is an example of a joint sample moment about the origin, the general deﬁnition being given by M0r;s ¼ 1=n
ð
Þ Pn
i¼1 Xr
iYs
i : The
deﬁnition for the case of joint sample moment about the mean replaces Xi with Xi  Xi, Yi with Yi  Yi, and M0rs with Mr;s.
320
Chapter 6
Sampling, Sample Moments and Sampling Distributions

E
Xi Yi
Xi
Yi
2
6664
3
7775 ¼
m0
1;1
mX
mY
2
6664
3
7775;
S ¼ Cov
Xi Yi
Xi
Yi
2
6664
3
7775 ¼ E
Xi Yi m0
1;1
Xi mX
Yi mY
2
6664
3
7775
Xi Yi m0
1;1
Xi mX
Yi mY
2
6664
3
7775
0
0
B
B
B
@
1
C
C
C
A
¼
m0
2;2  m0
1;1

	2
m0
2;1  mXm0
1;1
m0
1;2  mYm0
1;1
s2
X
sXY
ðSymmetricÞ
s2
Y
2
66664
3
77775
Then, by the multivariate Lindberg-Levy central limit theorem
n1=2
M01;1  m0
1;1
Xn  mX
Yn  mY
2
664
3
775 !
d Nð0; SÞ:
Now let g M01;1; Xn; Yn

	
¼ M01;1  XnYn, and note by Theorem 5.39 that
g() has an asymptotic normal distribution given by g ð Þ 
a N m0
1;1  mXmY;

n1GSG0Þ ¼ N (sXY, n1 GSG0), where G is the row vector of derivatives
of g with respect to its three arguments, evaluated at the expected values
of M01;1; Xn; Yn; i.e., G13 ¼ 1 mY mX
½
. The variance of the asymptotic
distribution of g() is then represented (after some algebraic simpliﬁcation) as
n1GSG0 ¼ n1 m0
2;2  ðm0
1;1 Þ2 þ6mXmYm0
1;1  4 m2
X m2
Y
h
2 m0
Y m0
2;1  2 mX m0
1;2 þ m2
Y m0
2;0 þ m2
X m0
0;2

and it can be shown by deﬁning m2,2 and m1,1 in terms of moments about
the origin that the preceding bracketed expression is identically equal to
m2,2  (m1,1)2. Thus g() 
a N(sXY, n1 (m2,2  (m1,1)2)).
n
Similar to the case of the sample mean and sample variance, the sample
covariance has properties that can be useful for estimating its population coun-
terpart, the covariance sXY. In particular, the distribution of SXY becomes cen-
tered on sXY as n ! 1 and as the sample size n increases, the density of SXY
concentrates within a small neighborhood of sXY so that it becomes highly
probable that an outcome of SXY will occur close to sXY.
Sample Correlation
Having deﬁned the concepts of sample variance and sample
covariance, a rather natural deﬁnition of the sample correlation between ran-
dom variables X and Y can be made as follows.
6.4
Sample Moments and Sample Correlation
321

Deﬁnition 6.14
Sample Correlation
Let ((X1,Y1),. . .,(Xn,Yn)) be a random sample from a population distribution.
Then the sample correlation between X and Y is given by
RXY ¼ SXY
SX SY
;
where SX ¼ S2
X

	1=2 and SY ¼ S2
Y

	1=2 are the sample standard deviations of X
and Y, respectively.
Regarding properties of RXY, outcomes of the sample correlation are lower
bounded by 1 and upper bounded by þ1, analogous to their population
counterparts.
Theorem 6.9
Sample Correlation
Bounds
The outcomes of RXY are such that rXY ∈[1,1].
Proof
This follows directly from Theorem 3.32 and 3.33 upon recognizing that sXY, s2
X
and s2
X
can be interpreted as a covariance and variances associated with a
probability distribution deﬁned by ^Fn.
n
In an analogy to the population correlation, the numerical value of the
sample correlation, rXY, can be interpreted as a measure of the degree of linear
association between given sample outcomes (x1,. . .,xn) and (y1,. . .,yn). The moti-
vation for this interpretation follows directly from Theorems 3.36 and 3.37 upon
recognizing that ^Fn can be thought of as deﬁning a probability distribution for X
and Y to which the theorems can be subsequently applied. Expectations in the
statements of the theorems are then interpreted as expectations based on ^Fn, and
all references to moments are then interpreted in the context of sample
moments.
With the preceding interpretation of Theorem 3.36 in mind, and recalling
Figure 3.10, rXY ¼ 1 implies that yi ¼ a1 þ bxi, for i ¼ 1,. . .,n, where a1 ¼ y  bx
and b ¼ (sY/sX). Thus, the sample outcomes y1,. . .,yn and x1,. . .,xn have a perfect
positive linear relationship. If rXY ¼ 1, then yi ¼ a2  bxi, for i ¼ 1,. . .,n,
where a2 ¼ y þ bx, so the yi0s and xi0s have a perfect negative linear relationship.
To interpret the meaning of rXY ∈(1,1), Theorem 3.37 and its subsequent
discussion can be applied to the discrete CDF ^Fn and the associated random
sample outcome (x1,y1),. . .,(xn,yn). The best predictor of the yi0s in terms of
a linear function of the associated xi0s is thus given by ^yi ¼ a þ bxi, where
a ¼ y  bx and b ¼ sXY/s2
X. In the current context, best means the choice of a
and b that minimizes
E^Fn d2 Y; ^Y




¼ n1 Xn
i¼1 yi  a þ bxi
ð
Þ
ð
Þ2;
322
Chapter 6
Sampling, Sample Moments and Sampling Distributions

which is a strictly monotonically increasing function of the distance d y; ^y
ð
Þ
¼ Pn
i¼1 yi  ^yi
ð
Þ2
h
i1=2
between the vector (y1,. . .,yn) and the vector ð^y1; . . . ; ^ynÞ.
(This is analogous to the least-squares criterion that will be discussed in
Section 8.2.) Then, since E^Fn d2 Y; ^Y




¼ s2
Y 1  r2
XY


(recall the discussion
following Theorem 3.37), it follows that the closer rXY is to either 1 or 1, the
smaller is the distance between the sample outcomes y1,. . .,yn and the best
linear prediction of these outcomes based on the sample outcomes x1,. . .,xn.
Therefore, rXY is a measure of the degree of linear association between sample
outcomes y1,. . .,yn and x1,. . .,xn in the sense that the larger is jrXYj, the smaller
the distance between (y1,. . .,yn) and ð^y1; . . . ; ^ynÞ, where ^yi ¼ a + bxi.
Also, recall from the discussion of Theorem 3.37 that r2
XY has an interpreta-
tion as the proportion of the (sample) variance in the yi0s that is explained by the
^yi
0s. Thus, the closer j rXY j is to 1, the more of the sample variance in the yi0s is
explained by a + bxi, i ¼ 1,. . .,n. The arguments are completely symmetric in
the yi0s and xi0s and a reversal of their roles leads to an interpretation of rXY
(or rYX) as a measure of the distance between (x1,. . .,xn) and ð^x1; . . . ; ^xnÞ with
^xi ¼ a þ byi (a and b suitably redeﬁned). Also, r2
XY (or r2
XY) is the proportion of the
sample variance in the xi0s explained by a + byi, i ¼ 1,. . .,n.
Besides its use as a measure of linear association between the sample
outcomes, one might also inquire as to the relationship between RXY and its
population counterpart, rXY. Like the case of the sample variance and the sample
covariance, it is not true that the expectation of the sample correlation equals its
population counterpart, i.e., in general, E(RXY) 6¼ rXY. Furthermore, because it is
deﬁned in terms of a nonlinear function of random variables, general expressions
for the mean and variance of RXY, as well as other ﬁnite sample properties, are
quite complicated to state and derive, and generally depend on the particular
form of the population distribution on which the random sample is based.7 We
will concentrate here on establishing the probability limit and asymptotic dis-
tribution of RXY, for which general results can be stated.
Theorem 6.10
Properties of Sample
Correlation
Let (Xi,Yi), i ¼ 1,. . .,n, be a random sample from a population distribution and
let RXY be the sample correlation between X and Y. Then
a. plim(RXY) ¼ rXY,
b. RXY 
a N(rXY, n1 t0St), with t and S deﬁned in the theorem proof.
7See Kendall and Stuart (1977), Advanced Theory, Vol. 1, pp. 246-251, for an approach based on Taylor series expansions that can be
used to approximate moments of the sample correlation.
6.4
Sample Moments and Sample Correlation
323

Proof
(a) Note that RXY ¼ SXY/(SX SY) is a continuous function of SXY, SX, and SY for
all SX > 0 and SY > 0 and, in particular, is continuous at the values sXY ¼
plim(SXY), sX ¼ plim(SX), and sY ¼ plim(SY). It follows from Slutsky’s
theorem that
plim RXY
ð
Þ ¼ plim
SXY
SXSY


¼
plim SXY
ð
Þ
plim SX
ð
Þ  plim SY
ð
Þ
ð
Þ ¼ sXY
sXsY
¼ rXY:
(b) The proof follows the approach used by Serﬂing, Approximation Theorems,
pp. 125–126. Deﬁne the (5  l) vector W ¼ X; Y; n1 Pn
i¼1 X2
i ;

n1 Pn
i¼1 Y2
i ;
n1 Pn
i¼1 XiYiÞ0; so that the sample correlation can be expressed as
RXY ¼ gðWÞ ¼
W5  W1 W2
W3  W2
1

	1=2
W4  W2
2

	1=2 :
Note that Zi ¼ (Xi,Yi, X2
i , Y2
i , XiYi)0, i ¼ 1,. . .,n are iid random vectors, so that
an application of the multivariate Lindberg-Levy CLT to the Zi0s implies that
n1=2 W  EW
ð
Þ!
d Nð0; SÞ;
because n1 Pn
i¼1 Zi ¼ Z ¼ W, where S is the covariance matrix of any Zi.
A direct application of Theorem 5.39 to RXY ¼ g(W) yields the statement in
the theorem, where
t1 ¼ @g EðWÞ
ð
Þ
@ w1
¼
rXYm0
1;0
s2
X



m0
0;1
sXsY


t2 ¼ @g EðWÞ
ð
Þ
@ w2
¼
rXYm0
0;1
s2
Y



m0
1;0
sXsY


t3 ¼ @g EðWÞ
ð
Þ
@ w3
¼ rXY
2s2
X

	
t4 ¼ @g EðWÞ
ð
Þ
@ w4
¼ rXY
2s2
Y

	
t5 ¼ @g EðWÞ
ð
Þ
@ w5
¼ sXsY
ð
Þ1:
n
As we have noted for the other statistics we have examined in this section,
the sample correlation can be useful for estimating its population counterpart,
rXY, at least in large samples. In particular, as n ! 1, the density of RXY
concentrates within a small neighborhood of rXY so that as n increases, it
becomes highly probable that an outcome of RXY will occur close to rXY.
In Example 6.9 below we introduce the concepts of sample covariance
matrices and sample correlation matrices.
324
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Example 6.9
Sample Means,
Variances, Covariance
and Correlation
Matrices
A stock analyst has 15 observations on daily average prices of three common
stocks:
Stock 1
Stock 2
Stock 3
1.38
1.66
4.85
3.45
5.95
2.26
4.80
3.02
4.41
4.68
7.08
4.61
9.91
7.55
9.62
6.01
9.49
9.34
8.13
9.43
8.35
8.64
11.96
10.02
12.54
11.32
13.97
11.20
12.09
10.37
15.20
11.85
13.75
13.52
16.98
14.59
15.77
16.81
16.80
16.26
18.03
18.64
18.21
17.07
16.95
She wants to calculate summary statistics of the data consisting of sample
means, sample variances, sample covariances, and sample correlations. She also
wants to assess the extent to which there are linear relationships between pairs
of stock prices.
Letting x represent the (15  3) matrix of stock prices. The three sample
means are given by
x
31
ð
Þ ¼ 1
15 x i; 
½
0 ¼
9:98
10:69
10:57
2
4
3
5:
The sample covariance matrix, containing the respective sample variances on
the diagonal and the sample covariances between stock i and stock j prices in the
(i,j)th entry, i 6¼ j, is
d
Cov
33
ð
Þ ðXÞ ¼ 1
15
X
15
i¼1
x i; 
½
0  x

	
x i; 
½
0  x

	0
¼
25:402
22:584
23:686
24:378
22:351
symmetric
ð
Þ
24:360
2
64
3
75:
The sample correlation matrix, containing the sample correlations between
stock i and stock j prices in the (i,j)th entry, i 6¼ j, is
6.4
Sample Moments and Sample Correlation
325

d
CorrðXÞ ¼
sx1
sx2
sx3
2
64
3
75
1
d
CovðXÞ
sx1
sx2
sx3
2
64
3
75
1
¼
1
:908
:952
1
:917
ðsymmetricÞ
1
2
64
3
75
The sample correlations indicate that sample observations have a pronounced
tendency to follow a positively sloped linear relationships between pairs of stock
prices. The linear relationship between stock 1 and stock 3 prices is the most
pronounced with (.952)2  100 ¼ 90.631 percent of the variance in either of the
stock prices explained by a linear function of the other stock price.
□
Note in the example above that we have used a matrix relationship between
sample correlation matrices and sample covariance matrices. Speciﬁcally, let-
ting SX denote the diagonal matrix of sample standard deviations of the random
variables in X, we have that d
CorrðXÞ ¼ S1
X d
CovðXÞS1
X , and it also holds that d
CovðXÞ
¼ SX d
CorrðXÞSX.
6.5
Properties of Xn and S2
n when Random Sampling is from a Normal Distribution
Additional sampling properties of Xn and S2
n are available beyond the generally
applicable results presented in earlier sections when random sampling is from a
normal population distribution. In particular, Xn and S2
n are then independent
random variables, Xn is normally distributed in ﬁnite samples, and nS2
n/s2 has a
w2 distribution with (n  1) degrees of freedom with S2
n itself being gamma-
distributed in ﬁnite samples. We establish these properties in subsequent
theorems. It can also be shown that n1/2(Xn  m)/^sn, where ^s2
n ¼ n= n  1
ð
Þ
ð
Þ S2
n,
has a so-called t-distribution, but we defer our examination of this property until
Section 6.7, where we will examine the t-distribution in detail.
We begin by stating a theorem on the independence of linear and quadratic
forms in normally distributed random variables that has applicability in a
variety of situations.
Theorem 6.11
Independence of Linear
and Quadratic Forms
Let B be a (q  n) matrix of real numbers, A be an (n  n) symmetric matrix of
real numbers having rank p, and let X be an (n  l) random vector such that
X ~ N(mX,s2I). Then BX and X’AX are independent if BA ¼ 0.8
8The theorem can be extended to the case where X ~ N (mx,S), in which case the condition for independence is that BSA ¼ 0.
326
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Proof
Since A is symmetric, it can be diagonalized by pre- and postmultiplication
using the matrix of characteristic vectors of A as
P0AP ¼ L ¼
l1
..
.
0
lp
0
0
2
6664
3
7775;
where P is the (n  n) matrix of characteristic vectors stored columnwise, and
li0s are the nonzero characteristic roots of A. It is assumed without loss of
generality that the columns of P have been ordered so that the ﬁrst p columns
correspond to characteristic vectors associated with the p nonzero characteristic
roots. Let BA ¼ 0, so that BPP0AP ¼ 0, since P is orthogonal (i.e., PP0 ¼ P0P ¼ I).
Let C ¼ BP, so that CL ¼ BPP0AP ¼ 0. Partitioning C and L appropriately, we
have that
CL ¼
C1
qp
ð
Þ
C2
q np
ð
Þ

"
#
D
pp
0
p np
ð
Þ
0
np
ð
Þp
0
np
ð
Þ np
ð
Þ
2
4
3
5 ¼
0
qn
ð
Þ;
where
D ¼
l1
..
.
lp
2
64
3
75
is the diagonal matrix of nonzero characteristic roots of A.
The above matrix equation implies that C1D ¼ 0, and since D is invertible,
we have that C1 ¼ 0. Thus
C ¼
0
qp
ð
Þ
C2
q np
ð
Þ
h
i
:
Now deﬁne Z(n1) ¼ P0X ~ N(P0mX,s2I) (recall that P0P ¼ I), and note that (Z1,. . .,
Zn) are independent random variables. Since X0AX ¼ Z0P0APZ ¼ Z0LZ ¼
Pp
i¼1 liZ2
i ¼ g1(Z1,. . .,Zp) and BX ¼ BPZ ¼ CZ ¼ C2
Zpþ1
..
.
Zn
2
64
3
75 ¼ g2(Zp+1,. . .,Zn),
and because (Z1,. . .,Zp) and (Zp+1,. . .,Zn) are independent, then by Theorem 2.9,
X0AX and BX are independent.
n
We use the preceding theorem to prove a theorem that establishes the
distribution of the random variable nS2
n /s2, which we will later ﬁnd to have
important applications in testing hypotheses about the value of s2.
6.5
Properties of Xn and S2
n when Random Sampling is from a Normal Distribution
327

Theorem 6.12
Independence of
Xn and S2
n and
nS2
n =s2  x2
n1
Under Normality
If
Xn and S2
n are the sample mean and sample variance, respectively, of a
random sample of size n from a normal population distribution with mean m
and variance s2, then
a. Xn and S2
n are independent;
b.
nS2
n=s2

	
 w2
n1.
Proof
(a) In the context of Theorem 6.11, let B(1n) ¼ [n1. . . n1], so that BX ¼ Xn.
Also, let
H
nn
ð
Þ ¼
B
B
...
B
2
664
3
775 ¼
n1
. . .
n1
...
..
.
...
n1
. . .
n1
2
64
3
75so that I  H
ð
ÞX ¼
X1  Xn
...
Xn  Xn
2
64
3
75
implying nS2
n ¼ X0ðI  HÞ0ðI  HÞX ¼ X0ðI  HÞX since I  H is symmetric
and (I  H)(I  H) ¼ I  H, i.e., I  H is idempotent. Then letting A ¼
n1(I  H), we have that X0AX ¼ S2
n.
It follows from Theorem 6.11 that Xn ¼ BX and S2
n ¼ X0AX are indepen-
dent since BA ¼ n1B(I  H) ¼ n1(B  B) ¼ 0.
(b) From the proof of part (a), it follows that
nS2
n
s2 ¼ 1
s2 ðX0ðI  HÞ0ðI  HÞXÞ:
Let mX ¼ (m m . . . m)0, and note that (I  H) mX ¼ mX  mX ¼ 0. Therefore,
n S2
n
s2 ¼ 1
s2 ðX  mXÞ0ðI  HÞ0ðI  HÞðX  mXÞ ¼ 1
s2 ðX  mXÞ0ðI  HÞðX  mXÞ;
because I  H is symmetric and idempotent. Note that tr[I  H] ¼ n  1,
which implies that (n  1) characteristic roots have value 1 because a symmetric
idempotent matrix has rank equal to its trace, and its characteristic roots are a
collection of 10s and 00s with the number of 10s equal to the rank of the matrix.
Diagonalizing (I  H) by its orthogonal characteristic vector matrix P then
yields
P0 I  H
ð
ÞP ¼
I
0
0
0


¼ L
where I is an (n  1) dimensional identity matrix. Therefore, I  H ¼ PLP0, and
then nS2
n=s2

	
¼ 1=s2

	
XmX
ð
Þ0PLP0 XmX
ð
Þ ¼ Z0LZ where Z¼ 1=s
ð
ÞP0 XmX
ð
Þ
 N 0;I
ð
Þsince P0P ¼ I. Finally nS2
n=s2

	
¼ Pn1
i¼1 Z2
i  w2
n1, giventhe deﬁnition of L,
i.e., we have the sum of squares of (n  1) iid standard normal random variables.n
It follows from part (b) of Theorem 6.12 that S2
n has a Gamma density, as
stated and proved in the following theorem.
328
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Theorem 6.13
Distribution of S2
n
Under Normality
Under the Assumptions of Theorem 6.12, S2
n ~ Gamma a; b
ð
Þ with a ¼ n  1
ð
Þ=2
and b ¼ 2s2=n

	
.
Proof
LetY ¼ nS2
n/s2.ThenitisknownfromTheorem6.12(b)thatMY(t) ¼ (1  2 t)(n1)/2.
Note that S2
n ¼
s2=n

	
Y. Then MS2
nðtÞ ¼ E exp S2
nt

	

	
¼ E exp
s2=n

	
Yt

	

	
¼
E exp Yt	
ð
Þ
ð
Þwheret	 ¼ s2t=n

	
. But sinceMY t	
ð Þ ¼ E exp Yt	
ð
Þ
ð
Þ ¼ 1  2t	
ð
Þ n1
ð
Þ=2,
it follows that MS2
nðtÞ ¼ 1  2 s2t=n

	

	 n1
ð
Þ=2 , which is associated with the
Gamma density having a ¼ (n  1)/2 and b ¼ (2s2/n).
n
Since Xn ¼ n1
Pn
i¼1 Xi is a linear combination of iid N(m,s2) random
variables, Xn is also normally distributed, as indicated in the following theorem.
Theorem 6.14
Distribution of X n
Under Normality
Under the Assumptions of Theorem 6.12, Xn ~ N m; s2=n

	
.
Proof
Let A(1n) ¼ (n1. . . n1) and b ¼ 0 in Theorem 4.9. Then since X(n1)~ N(mX,s2I),
where X ¼ (X1,. . .,Xn)0 and mX ¼ (m,. . .,m)0, then X ¼ AX ~ N(AmX, As2IA0) ¼
N m; s2=n

	
.
n
Example 6.10
Inspection of Fill
Volumes
An inspector from the state’s Department of Weights and Measures is
investigating a claim that an oil company is underﬁlling quarts of motor oil
that are sold at retail. The ﬁlling process is such that the actual volumes of oil
placed in containers can be interpreted as being normally distributed. According
to state speciﬁcations, a container is considered legally full if it contains  31.75
ounces of oil. The company claims that their ﬁlling process has a mean of 32.25
and a standard deviation of .125, and that therefore the probability that a con-
tainer is legally full exceeds .9999.
The inspector randomly samples 200 containers of oil produced by the oil
company, measures their content in ounces, and ﬁnds that P200
i¼1 xi ¼ 6; 462 and
P200
i¼1 x2
i ¼ 208; 791, so that the sample mean and sample variance are given by
x ¼ P200
i¼1 xi=200 ¼ 32:31 and s2 ¼ P200
i¼1 xi  x
ð
Þ2=200 ¼
P200
i¼1 x2
i  200x2


=200
¼ :0189 . Given the company’s claim,
Xn
~ N (32.25, .78125  104) and
(200) S2/(.015625) ~ w2
199 . If the company’s claim were true, then the event
x 2 m  4sX; m þ 4sX


¼ [32.2146,
32.2854] would
occur
with probability
> .9999. The particular outcome of X that was actually observed appears to be
unusually high compared to the claimed mean, suggesting that m might actually
be higher than the company claims it to be (if the claimed standard deviation is
correct). Regarding the sample variance, and given the company’s claim,
E(S2) ¼ (199/200) s2 ¼ .015547 and the event s2 ∈[.01265, .01875] would occur
6.5
Properties of Xn and S2
n when Random Sampling is from a Normal Distribution
329

with probability9 equal to .95. Then s2 ¼ .0189 appears to be somewhat high
compared to the claimed variance.
Overall, there is strong evidence to suggest that the ﬁlling process does not
have the characteristics claimed by the company, but the evidence given above
does not support the claim that the cans are being underﬁlled. We will examine
this issue further after we derive the t-distribution in the next Section 6.7.
□
6.6
Sampling Distributions of Functions of Random Variables
The statistics that we have examined in the preceding sections are useful in a
number of statistical inference problems, but one needs to be concerned with a
much larger variety of functions of random samples to adequately deal with the
variety of inference problems that arise in practice. Furthermore, in order to
assess the adequacy of statistical procedures, it will be necessary to identify
probability spaces for functions of random samples that are proposed as estima-
tor, hypothesis-testing, or conﬁdence interval statistics in order to evaluate their
performance characteristics probabilistically. In particular, we will have need for
deriving the functional forms of the PDFs associated with functions of random
samples, where such PDFs will be referred to as sampling densities or sampling
distributions.
6.6.1
MGF Approach
If Y ¼ g(X1,. . .,Xn) is a function of interest, then one can attempt to derive the
moment generating function of Y ¼ g(X1,. . .,Xn), i.e., Mg()(t) ¼ E(exp(g(X1,. . .,Xn)t)),
and identify the density function characterized by the moment generating
function. When X1,. . .,Xn is a random sample, one refers to the density of Y as
the sampling density or sampling distribution of Y. Of course, g can be a
vector function, in which case we would employ the multivariate moment
generating function concept in the hope of identifying the joint density
function of the multivariate random variable Y.
Example 6.11
Sampling Distribution
of Xn when Sampling
from Exponential
Family
Consider the sampling distribution of the sample mean Xn ¼ n1 Pn
i¼1 Xi, when
the random sample (X1,. . .,Xn) is drawn from a population distribution
represented by the exponential family of densities. Using the MGF approach,
we attempt to ﬁnd E(exp( Xn t)) ¼ E exp
Pn
i¼1 Xi

	
t=n

	

	
. We know that the
population distribution is given by m(z) ¼ y1 ez/y I(0,1)(z), so that the joint
density of the random sample is given by
9This interval and associated probability was obtained by noting that if Y ~ w2
199 , then P(161.83  y  239.96) ¼ .95, which was
obtained via numerical integration of the w2
199 density, leaving .025 probability in both the right and left tails of the density. Using the
relationship S2 ~ (.015625/200)Y then leads to the stated interval.
330
Chapter 6
Sampling, Sample Moments and Sampling Distributions

f x1; . . . ; xn
ð
Þ ¼ 1
yn exp  Pn
i¼1 xi
y

 Yn
i¼1 I 0;1
ð
Þ xi
ð
Þ:
Then
E exp
X
n
i¼1
Xi
 
!
t=n
 
!
 
!
¼
Y
n
i¼1
E exp Xit
n




¼
Y
n
i¼1
1  y t
n

1
¼
1  y t
n

n
for t < n/y, which is recognized as the MGF of a Gamma a; b
ð
Þ density with a ¼ n
and b ¼ y/n. Thus, the sampling distribution of X is gamma with the aforemen-
tioned parameters.
□
6.6.2
CDF Approach
One might also attempt to derive the sampling distribution of Y ¼ g(X1,. . .,Xn)
by identifying the cumulative distribution function of Y and then use the corre-
spondence between cumulative distribution functions and probability density
functions to derive the latter. The usefulness of this approach depends on how
difﬁcult it is to derive the cumulative distribution function for a particular
problem. The advantages of this approach over the MGF approach are that it is
applicable even if the MGF does not exist, and it circumvents the problem of the
researcher’s not knowing which density function corresponds to a particular
MGF when the MGF does exist.10
Example 6.12
Sampling Distribution
of X
2
n for Random
Sample from
Exponential
Refer to Example 6.11, and suppose we want the sampling distribution of Y ¼
g(X1,. . .,Xn) ¼ X
2
n. Note that
FðcÞ ¼
Pðy  cÞ
¼ P x2
n  c

	
¼ P xn 2 c1=2; c1=2



	
¼
R c1=2
0
1
baG a
ð Þ xa1ex=bdx for c>0;
0
otherwise:
8
>
<
>
:
since Xn is Gamma distributed. The following lemma regarding differentiating
an integral with respect to its bounds will be useful here and in other
applications of the CDF approach for deﬁning the PDF of g(X).
10Although, as we have mentioned previously, the density function can always be identiﬁed in principle by an integration problem
involving the MGF in the integrand.
6.6
Sampling Distributions of Functions of Random Variables
331

Lemma 6.1
Leibnitz’s Rules
Let w1(c) and w2(c) be functions which are differentiable at c, and let h(x) be
continuous at x ¼ w1(c) and x ¼ w2(c) and be integrable. Then
d
R w2ðcÞ
w1ðcÞ hðxÞdx
dc
¼ hðw2ðcÞÞ dw2ðcÞ
dc
 hðw1ðcÞÞ dw1ðcÞ
dc
:
Special Cases:
a. Let w1(c) ¼ k 8 c, where k is a constant. Then
d
R w2ðcÞ
k
hðxÞdx
dc
¼ hðw2ðcÞÞ dw2ðcÞ
dc
;
and the function h(x) need not be continuous at k. Note the result is still valid
if k is replaced by 1.
b. Let w2(c) ¼ k 8 c, where k is a constant. Then
d
R k
w1ðcÞ hðxÞdx
dc
¼ hðw1ðcÞÞ dw1ðcÞ
dc
;
and the function need not be continuous at k. Note the result is still valid if k
is replaced by 1.
(Adapted
from
D.V.
Widder
(1961)
Advanced
Calculus,
2nd
ed.,
Englewood Cliffs, NJ: Prentice-Hall, pp. 350–353 and R.G. Bartle (1976), The
Elements of Real Analysis, 2nd ed., John Wiley, pp. 245–246.)
In the case at hand, w2(c) ¼ c1/2, and w1(c) ¼ 0, so that Lemma 6.1 implies
that the sampling distribution of g(X1,. . .,Xn) ¼ X
2
n is given by
dFðcÞ
dc
¼ fðcÞ ¼
1
2 ba GðaÞ c a=2
ð
Þ1 exp c1=2
b


for c > 0
0
otherwise
8
<
:
:
6.6.3
Event Equivalence Approach (for Discrete Random Variables)
In the case of discrete random variables, a third conceptually straightforward
approach for deriving the density function of functions of random variables is
available. In fact, without calling attention to it, we have already used the
procedure, called the equivalent-events approach, in proving a theorem
concerning the expectation of functions of random variables. Speciﬁcally, if
y ¼ g(x) is a real-valued function of x, then PY(y) ¼ PX(Ay), Ay ¼ {x: y ¼ g(x),
x ∈R(X)}, because the elementary event y for Y is equivalent to the event Ay
for X in the sense that y occurs iff Ay occurs. It follows that the density function
of Y can be obtained from the density function of X for scalar X and Y as
332
Chapter 6
Sampling, Sample Moments and Sampling Distributions

hðyÞ ¼
P
x:gðxÞ¼y;x2RðXÞ
ð
Þ
fðxÞ: The extension to the case of multivariate random
variables and vector functions is straightforward. In particular, interpreting
y and/or x as vectors, we have that hðyÞ ¼
P
x:gðxÞ¼y;x2RðXÞ
f
g
fðxÞ.
In either the scalar or multivariate case, if y ¼ g(x) is invertible, so
that x ¼ g1(y) 8x ∈R(X), then the discrete density can be deﬁnes simply as
h(y) ¼ f(g1(y)).
Example 6.13
Deﬁning Sampling
Distribution via Event
Equivalence
Your company ships BluRay disks in lots of 100. As part of your quality-control
program, you assure potential buyers that no more than one disk will be defec-
tive in each lot of 100. In reality, the probability that a disk manufactured by
your company is defective is .001, and whether a given disk is defective is
independent of whether or not any other disk is defective. Then the PDF for
the number of defectives in a lot of your disks is given by the binomial density
function
fðxÞ ¼
100
x
 
!
:001
ð
Þx :999
ð
Þ100x for x 2 0; 1; . . . ; 100
f
g;
0
otherwise:
8
>
>
<
>
>
:
Deﬁne a new binary random variable, Z, such that z ¼ 1 represents the event
that your quality-control claim is valid (i.e., x ¼ 0 or 1) while z ¼ 0 represents
the event that your claim is invalid (i.e., x ¼ 2 or more) on a lot of your disks.
That is,
z ¼ gðxÞ ¼ I 0;1
f
gðxÞ. Consider deﬁning the density function of Z.
The density is given by hðzÞ ¼ P
x:I 0;1
f
gðxÞ¼z;x2RðXÞ
f
g fðxÞ for z 2 RðZÞ ¼ 0; 1
f
g ,
and hðzÞ ¼ 0 elsewhere. Then, referring to the binomial density function deﬁned
above,
hð1Þ ¼
X
1
x¼0
fðxÞ ¼ :995 and hð0Þ ¼
X
100
x¼2
fðxÞ ¼ :005; so that
h(z) ¼ .995 I{1}(z) þ .005 I{0}(z).
□
6.6.4
Change of Variables Approach (for Continuous Random Variables)
Another very useful procedure for deriving the PDF of functions of continuous
random variables is available if the functions involved are continuously differ-
entiable and if Y ¼ g(X) admits an inverse function X ¼ g1(Y). We ﬁrst examine
the case where both Y and X are scalar random variables. For the purpose of
proper interpretation of the theorem below, recall our convention established in
Deﬁnition 2.13, whereby there is equivalence between the support of the ran-
dom variable x : fðxÞ>0 for x 2 Rg
f
, and the range of the random variable RðXÞ,
i.e., RðXÞ 
 x : fðxÞ>0 for x 2 Rg
f
.
6.6
Sampling Distributions of Functions of Random Variables
333

Theorem 6.15
Change of Variables
Technique: Univariate
and Invertible
Suppose the continuous random variable X has PDF f(x). Let g(x) be continu-
ously differentiable with dg=dx 6¼ 0 8x in some open interval, D, containing
R(X). Also, let the inverse function x ¼ g1(y) be deﬁned 8 y ∈R(Y). Then the
PDF of Y ¼ g(X) is given by
hðyÞ ¼ fðg1ðyÞÞ dg1 ðyÞ
dy

for y 2 RðYÞ; with hðyÞ ¼ 0 elsewhere:
Proof
If dg=dx 6¼ 0 is a continuous function, then g is either a monotonically increasing
or monotonically decreasing function, i.e., either dg=dx>0 or dg=dx<0, respec-
tively, 8 x ∈D. (Note that dg/dx cannot > 0 for some x and < 0 for other x0s since
continuity of dg/dx would then necessarily require that dg/dx ¼ 0 at some point.)
(a) Case where dg/dx > 0: In this case, P(y  b) ¼ P(g(x)  b) ¼ P(x  g1(b)), 8
b ∈R(Y), since x ¼ g1(y) is monotonically increasing for y ∈R(Y) (see
Figure 6.3). Thus the cumulative distribution function for Y can be deﬁned
for all b ∈R(Y) by H(b) ¼ P(y  b) ¼ P(x  g1(b)) ¼ R g1ðbÞ
1
fðxÞdx:
Then using Lemma 6.1, we can derive the density function for Y by differen-
tiation as11
hðbÞ ¼ dHðbÞ
db
¼
d
R g1 ðbÞ
1
fðxÞdx
db
¼ fðg1 ðbÞÞ dg1ðbÞ
db
for b 2 RðYÞ
0
otherwise
8
>
<
>
:
y=g(x)
x
g-1(b)
b
y
Figure 6.3
Monotonically increasing
function.
11Here, and elsewhere, we are suppressing a technical requirement that f be continuous at the point g1(b), so that we can invoke
Lemma 6.1 for differentiation of the cumulative distribution function. Even if f is discontinuous at g1(b), we can nonetheless deﬁne
h(b) as indicated above, since a density function can be redeﬁned arbitrarily at a ﬁnite number of points of discontinuity without
affecting the assignment of probabilities to any events.
334
Chapter 6
Sampling, Sample Moments and Sampling Distributions

(b) Case where dg/dx < 0: In this case, P(y  b) ¼ P(g(x)  b) ¼ P(x  g1(b)),
8b ∈R(Y), since x ¼ g1(y) is monotonically decreasing for y ∈R(Y) (see
Figure 6.4). Thus the cumulative distribution function for Y can be deﬁned
for all b ∈R(Y) by HðbÞ ¼ Pðy  bÞ ¼ Pðx  g1 ðbÞÞ ¼
R 1
g1ðbÞ fðxÞdx:
Then, using Lemma 6.1, we can derive the density function for Y by differ-
entiation as
hðbÞ ¼ dHðbÞ
db
¼
d
R 1
g1 ðbÞ fðxÞdx
db
¼ fðg1 ðbÞÞ dg1 ðbÞ
db
for b 2 RðYÞ
0
otherwise
8
>
<
>
:
Note in this latter case, since
dg1ðbÞ=db

	
<0, we can write h(b) alterna-
tively as
hðbÞ ¼
fðg1 ðbÞÞ dg1 ðbÞ
db

for b 2 RðYÞ
¼ 0 otherwise
8
>
>
<
>
>
:
which is also an alternative representation of the previous case where
dg/dx > 0.
n
Example 6.14
Derivation of the Log-
Normal Distribution
A stochastic form of the Cobb-Douglas production function is given by
Q ¼ bo
P
k
i¼1 xbi
i


eW ¼ bo
P
k
i¼1 xbi
i


V;
where W ~ N(0, s2) and V ¼ eW. Consider deriving the PDF of V.
y=g(x)
x
g-1(b)
b
y
Figure 6.4
Monotonically decreasing
function.
6.6
Sampling Distributions of Functions of Random Variables
335

Note that v ¼ ew is a monotonically increasing function of w for which
dv/dw ¼ ew > 0 8w. The inverse function is given by w ¼ ln(v) 8v > 0, and
dw/dv ¼ v1. Then Theorem 6.15 applies, and the distribution of V can be
deﬁned as
hðvÞ ¼
1
ﬃﬃﬃﬃﬃﬃ
2p
p
sv
exp  ½lnðvÞ 2
2 s2
 
!
for v>0
0
elsewhere
8
>
>
<
>
>
:
The density function is called a log-normal distribution.
□
The change-of-variables approach can be generalized to certain cases where
the function deﬁned by y ¼ g(x) does not admit an inverse function. The proce-
dure essentially applies to functions that are piecewise invertible, as deﬁned
below.
Theorem 6.16
Change-of-Variables
Technique: Univariate
and Piecewise
Invertible
Suppose the continuous random variable X has PDF f(x). Let g(x) be continuously
differentiable with dgðxÞ=dx 6¼ 0 for all but perhaps a ﬁnite number of x’s in an
open interval D containing the range of X. Let RðXÞ be partitioned into a
collection of disjoint intervals D1,. . .,Dn for which g: Di ! Ri has an inverse
function g1
i : Ri ! Di 8i.12 Then the probability density of Y ¼ g(X) is given by
hðyÞ ¼
S
i2IðyÞ f g1
i
ðyÞ

	 dg1
i
ðyÞ
dy

for y 2 RðYÞ
0
elsewhere,
8
>
>
<
>
>
:
where I(y) ¼ {i: ∃x ∈Di such that y ¼ g(x), i ¼ 1,. . .,n}, and
dg1
i ðyÞ=dy

	

 0
whenever it would otherwise be undeﬁned.13
Proof
The CDF of Y ¼ g(X) can be represented as Pðy  bÞ ¼ S
n
i¼1 Pðx:gðxÞ  b; x 2 DiÞ
Note the following possibilities for P(x: g(x)  b, x ∈Di):
P(x : g(x)  b, x ∈Di)
Case
g monotonically
increasing on Di
g monotonically
decreasing on Di
1.
b < minx2Di g(x)
0
0
2.
minx2Dig(x)  b  maxx2Di g(x)
R g1
i
ðbÞ
min Di
ð
Þ fðxÞdx
R maxðDiÞ
g1
i
ðbÞ
fðxÞdx
3.
b > maxx2Di g(x)
P(xi ∈Di)
P(xi ∈Di)
12These properties deﬁne a function that is piecewise invertible on the domain [n
i¼1 Di.
13Note that I(y) is an index set containing the indices of all of the Di sets that have an element whose image under the function g is the
value y.
336
Chapter 6
Sampling, Sample Moments and Sampling Distributions

where min(Di) and max(Di) refer to the minimum and maximum values in Di.14
In either case 1 or 3, dP(x:g(x)  b, x ∈Di)/db ¼ 0, which corresponds to the
case where 6 9x such that g(x) ¼ b. In case 2, Leibnitz’s rule for differentiating the
integrals yields f(g1(b)) [dg1(b)/db] and f(g1(b))[dg1(b)/db] in the monotoni-
cally increasing and decreasing cases, respectively, which can both be
represented by f(g1(b))|dg1(b)/db|. Then the density of Y can be deﬁned as
h(b) ¼ dP(y  b)/db for b ∈R(Y), with
dg1ðyÞ=dy

	

 arbitrarily set to zero in
the ﬁnite number of instances where dgðbÞ=dx ¼ 0 for b ∈Di.
n
The following example illustrates the use of this more general change-of-
variables technique.
Example 6.15
Change of Variables
in a Noninvertible
Function Case
The manufacturing process for a certain product is sensitive to deviations from
an optimal ambient temperature of 72 F. In a poorly air-conditioned plant
owned by the Excelsior Corporation, the average daily temperature is uniformly
distributed in the range [70,74], and the deviations from 72 F are represented by
the outcome of X ~ f(x) ¼ .25 I[2,2](x). The percentage of production lost on any
given day due to temperature deviations can be represented by the outcome of
Y ¼ g(X) ¼ X2. Consider deriving the probability density for the percentage of
production lost on any given day.
First note that Theorem 6.15 does not apply since y ¼ x2 is such that
dy/dx ¼ 0 at x ¼ 0, which is in the range of X, and y ¼ x2 does not admit an
inverse function 8y ∈R(Y) ¼ [0,4] (see Figure 6.5).
However, we can utilize Theorem 6.16 to derive the density function of Y.
The function g(x) ¼ x2 is continuously differentiable 8x, and dg(x)/dx ¼ 2x 6¼ 0
8x except x ¼ 0. The sets D1 ¼ [2,0) and D2 ¼ [0,2] are two disjoint intervals
whose union equals the range of X. Also, g: D1 ! (0,4] has inverse function g1
1 :
(0,4] ! D1 deﬁned byg1
1 (y) ¼ y1/2 for y ∈(0,4], while g: D2 ! [0,4] has inverse
function g1
2 :[0,4] ! D2 deﬁned by g1
2 (y) ¼ y1/2 for y ∈[0,4].15 Finally, note that
dg1
1 ðyÞ=dy ¼  1=2
ð
Þy1=2 and dg1
2 ðyÞ=dy ¼ 1=2
ð
Þy1=2 are deﬁned 8y ∈(0,4].
Then from Theorem 6.16, it follows that
hðyÞ ¼
1
4 I½2;2 ð y1=2Þj  :5 y:5 j þ 1
4 I½2;2 ðy1=2Þj:5 y:5 j
¼ 1
8 I½0;4 ðyÞ y:5 þ 1
8 I½0;4 ðyÞ y:5 ¼ 1
4 y:5 I½0;4 ðyÞ for y 2 ð0; 4
0 elsewhere:
8
>
>
>
>
>
<
>
>
>
>
>
:
The change of variable technique can be extended to the multivariate case.
We examine the case where the inverse function exists.
14If max or min do not exist, they are replaced with sup and inf.
15We are continuing to use the convention that y1/2 refers to the positive square root of y, so that y1/2 refers to the negative square
root.
6.6
Sampling Distributions of Functions of Random Variables
337

Theorem 6.17
Change of Variables
Technique: Multivariate
and Invertible
Suppose the continuous (n  1) random vector X has joint PDF f(x). Let g(x) be a
(n  l) real-valued vector function that is continuously differentiable 8 x vector
in some open rectangle of points, D, containing the range of X. Assume the
inverse vector function x ¼ g1(y) exists, 8 y ∈RðYÞ . Furthermore, let the
Jacobian matrix of the inverse function transformation,
J ¼
@g1
1 ðyÞ
@y1
  
@g1
1 ðyÞ
@yn
..
.
..
.
..
.
@g1
n ðyÞ
@y1
  
@g1
n ðyÞ
@yn
2
66666664
3
77777775
;
be such that det(J) 6¼ 0 with all partial derivatives in J being continuous
8 y ∈R(Y). Then the joint density of Y ¼ g(X) is given by
hðyÞ ¼
f g1
1 ðyÞ; . . . ; g1
n ðyÞ

	
j detðJÞj for y 2 RðYÞ;
¼ 0 otherwise,
(
where |det(J)| denotes the absolute value of the determinant of the Jacobian.
Proof
The proof is quite complex, and is left to a more advanced course of study. The
proof is based on the change-of-variables approach in multivariate integration
problems. See T. Apostol (1974), Mathematical Analysis, 2nd ed., Reading, MA:
Addison-Wesley, pp. 421.
n
We defer an illustration of the use of this theorem until the next section,
where we will use the procedure to derive both the t-density and F-density. The
reader should note that in Theorem 6.17, there are as many coordinate functions
in the vector function g as there are elements in the random variable vector X.
In practice, if the researcher is interested in establishing the density of fewer
y=x2
y
-2
2
x
4
2
0
Figure 6.5
Function with no inverse.
338
Chapter 6
Sampling, Sample Moments and Sampling Distributions

than n random variables deﬁned as real-valued functions of the (n  1) vector X,
she is obliged to deﬁne “auxiliary” random variables to obtain an invertible
vector function having n coordinate functions, and then later integrate out
the auxiliary random variables from the n-dimensional joint density to arrive
at the marginal density of interest. This approach will be illustrated in the
next section.
A generalization of Theorem 6.17 to noninvertible functions along the lines
of Theorem 6.16 is possible. See Mood, Graybill, and Boes (1974), Introduction
to the Theory of Statistics. New York: McGraw-Hill, p. 209 for one such
generalization.
6.7
t-and F-Densities
In this section we consider two important statistics that are deﬁned in terms of
ratios of random variables related to the normal distribution. These statistics,
and their probability distributions, ﬁgure prominently in the construction of
hypothesis-testing and conﬁdence interval procedures when random sampling is
from a normal probability distribution.
6.7.1
t-Density
The t-statistic is deﬁned as the ratio of a standard normal random variable
divided by the square root of a chisquare random variable that has been divided
by its degrees of freedom. If the two random variables in the ratio are indepen-
dent, then the t-statistic has a t-distribution, with a single degrees of freedom
parameter that refers to the denominator chisquare random variable, as deﬁned
in the theorem below.
Theorem 6.18
Deﬁnition and
Derivation of the
t-Density
Let Z ~ N(0,1), Y ~ w2
v, and let Z and Y be independent random variables. Then
T ¼ Z= Y=v
ð
Þ1=2 has the t-density with v degrees of freedom, where
fðt; vÞ ¼
G vþ1
2

	
Gðv=2Þ
ﬃﬃﬃﬃﬃﬃ
pv
p
1 þ t2
v

 vþ1
2

	
:
Proof
Deﬁne the (2  1) vector function g as
t
w
"
#
¼
g1 ðz; yÞ
g2 ðz; yÞ
"
#
¼
z
ðy=v Þ1=2
y
2
64
3
75;
where g2(z,y) is an auxiliary function of (z,y) deﬁned to allow the use of Theorem
6.17. Note g is continuously differentiable 8 z and 8 y > 0, which represents an
open rectangle of (z,y) points containing the range of (Z,Y) (note the joint density
6.7
t-and F-Densities
339

of (Z,Y) in this case is the product of a standard normal density and a w2 density,
which has support (1,1)  (0,1)). The inverse vector function g1 is deﬁned
by
z
y
"
#
¼
g1
1 ðt; wÞ
g1
2 ðt; wÞ
"
#
¼
t
w
v
 	1=2
w
"
#
8 t; w
ð
Þ 2 RðT; WÞ:
The Jacobian of the inverse function is
J ¼
w
v

1=2
t
2
w
v

1=2 1
v
 
0
1
2
64
3
75;
where the elements of the Jacobian are continuous functions of (t,w) and
j detðJÞj ¼ j w=v
ð
Þ1=2j ¼ w=v
ð
Þ1=2 6¼ 0 8ðt; wÞ 2 RðT; WÞ: Given the density assump-
tions concerning Z and Y,
f z; y
ð
Þ ¼ mZðzÞmYðyÞ
¼
1
2p
ð
Þ1=2 exp z2=2

	
1
2v=2G v=2
ð
Þ
y v=2
ð
Þ1 exp y=2
ð
ÞI 0;1
ð
ÞðyÞ:
Then, by Theorem 6.17, the joint density of (T,W) is given by
hðt; wÞ ¼
1
ð2p Þ1=2 exp ð1=2Þ t2
w
v




1
2v=2 Gðv=2Þ
wðv=2Þ1
 exp  w
2


Ið0;1Þ ðwÞ w
v

1=2
¼
1
Gðv=2Þðpv Þ1=2 2ðvþ1Þ=2 wðv1Þ=2 exp  w
2
1 þ t2
v




Ið0;1Þ ðwÞ:
Since our interest centers on the density of T, we require the marginal density of
T, fT(t;v) ¼
R 1
0
h(t,w)dw. Making the substitution p ¼ (w/2)(1 þ (t2/v)) in the
integral, so that w ¼ 2p= 1 þ t2=v

	

	
and dw ¼ 2= 1 þ t2=v

	

	
dp, yields
fT ðt; vÞ ¼
Z 1
0
1
G v=2
ð
Þ pv
ð
Þ1=22 vþ1
ð
Þ=2
2p
1 þ t2
v
"
# v1
ð
Þ=2
ep
2
1 þ t2
v
"
#
dp
¼
1
Gðv=2Þ pv
ð
Þ1=2
1 þ t2
v

ðvþ1Þ=2 Z 1
0
p v1
ð
Þ=2epdp
¼
G vþ1
2

	
Gðv=2Þ pv
ð
Þ1=2
1 þ t2
v

ðvþ1Þ=2
;
where we have used the fact that the integral in the next-to-last expression is the
deﬁnition of the Gamma function G v þ 1
ð
Þ=2
ð
Þ: The density is known as
340
Chapter 6
Sampling, Sample Moments and Sampling Distributions

the t-density with v degrees of freedom, the degrees of freedom referring to the
denominator w2
v random variable in the t-ratio.
n
The preceding theorem facilitates the derivation of the probability distribu-
tion of T ¼ n1/2(Xn  m)/^sn when random sampling is from a population distri-
bution, where ^sn ¼ n= n  1
ð
Þ
ð
Þ1=2Sn. We will later ﬁnd that the random variable
T has important applications in testing hypotheses about the value of the
population mean, m.
Theorem 6.19
A T-Statistic for
Known m
Under the assumptions of Theorem 6.12 and deﬁning ^sn 
 n= n  1
ð
Þ
ð
Þ1=2Sn;
T ¼
n1=2 Xn  m

	

	
=^sn

	
has the t-density with n  1 degrees of freedom.
Proof
The proof is based on the fact that T is deﬁned as a standard normal random
variable divided by the square root of a w2
v random variable that has been divided
by its degrees of freedom v ¼ n  1, i.e.,
T ¼
Xn  m

	
s=
ﬃﬃﬃn
p
nS2
s2 n  1
ð
Þ
 
!1=2
and the two random variables are independent, so Theorem 6.18 applies. Final
details are left to the reader.
n
Example 6.16
Assessing a Hypothesis
Based on a T-Statistic
Recall Example 6.10, regarding the question of underﬁlling of containers. By
Theorem 6.19, we know that T ¼ n1/2 (Xn  m)/ ^sn has a t-distribution with
199 degrees of freedom when n ¼ 200. Assuming the company’s claim of
m ¼ 32.25 to be true, the outcome of T in this case is 6.1559. The probability of
obtaining an outcome  6.1559 is P(t  6.1559) ¼ 2.022  109 (obtained by
integrating the aforementioned t-distribution on a computer). The evidence
suggests that the company’s claim is suspect – its claimed value of m may in
fact be too low, and there is no support for the claim that the company is
underﬁlling their containers.
□
We present some properties of the t-distribution below.
6.7.1.1
Family Name: t-Family
Parameterization: v ∈O ¼ {v: v is a positive integer}
Density Deﬁnition: See Theorem 6.18.
Moments:
m ¼ 0
for
v > 1,
s2 ¼ v/(v  2)
for
v > 2,
m3 ¼ 0
for
v > 3,
m4
s4  3

	
¼
6
v4 for v>4
MFG: Does not exist
As Figure 6.6 shows, the graph of the t-density is symmetric about zero, and
when compared to the standard normal density, the t-density tends to have a
smaller mode at zero and has fatter tails. However, as v ! 1, Tv !
d N(0,1).
6.7
t-and F-Densities
341

Theorem 6.20
Convergence of t-
Distribution to N 0; 1
ð
Þ
Let Tv ¼ Z= Yv=v
ð
Þ1=2, Z  N 0; 1
ð
Þ; Yv  w2
v , and let Z and Yv be independent,
so that Tv has the t-density with v degrees of freedom. Then as v ! 1, Tv !
d
N(0,1).
Proof
Since Yv ~ w2
v , then E(Yv) ¼ v and var(Yv) ¼ 2v, and thus E(Yv/v) ¼ 1, and
var (Yv/v) ¼ 2v1. It follows that (Yv/v) !
m 1, so that plim (Yv/v) ¼ 1. Also note
that since Z ~ N(0,1) 8v, it follows trivially that Z !
d N(0,1). Then, by Slutsky’s
theorems, Tv ¼ (Yv/v)1/2 Z !
d 1Z ~ N(0,1).
n
The convergence of the t-density to the standard normal density is rapid, and
for v > 30, the standard normal density provides an excellent approximation to
the t-density. Tables of integrals of the t-density are widely available, generally
giving the value of c for which
P½tv  c ¼
ð1
c
fTðt; vÞdt ¼ a
for speciﬁc choices of a such as .01, .025 and .05, and for selected choices of v, i.e.,
the complement of the CDF is tabled. The symmetry of the t-density implies
that if c is such that P[tv  c] ¼ a, then c is such that P[tv  c] ¼ a
(see Figure 6.7). If more detail is required than what is available in the table of
the
t-density,
computer
programs
are
readily
available
for
numerically
integrating the t-density (e.g. GAUSS, Matlab, and SAS).
-4
-3.7
-3.4
-3.1
-2.8
-2.5
-2.2
-1.9
-1.6
-1.3
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
1.1
1.4
1.7
2
2.3
2.6
2.9
3.2
3.5
3.8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
f(x)
t-density, 5 df
normal
x
Figure 6.6
Standard normal and
t-density.
342
Chapter 6
Sampling, Sample Moments and Sampling Distributions

6.7.2
F-Density
The F-statistic is deﬁned as the ratio of two chisquare random variables that
have each been divided by their degrees of freedom, and if the random variables
are independent, the statistic has the F-distribution, having two parameters
equal to the degrees of freedom of the chisquare random variables, as deﬁned
in the theorem below.
Theorem 6.21
Deﬁnition and
Derivation of the
F-Density
LetY1  w2
v1; Y2  w2
v2;andletY1andY2beindependent.ThenF ¼ Y1=v1
ð
Þ= Y2=v2
ð
Þ
has the F-density with v1 numerator and v2 denominator degrees of freedom,
deﬁned as mðf; v1; v2Þ ¼
G
v1þv2
1
ð
Þ
G
v1
2
ð ÞG
v2
2
ð Þ
v1
v2
 v1=2
fðv1=2Þ1 1 þ v1
v2 f

ð1=2Þðv1þv2Þ
Ið0;1ÞðfÞ:
Proof
Deﬁne the (2  1) vector function g as
f
w
"
#
¼
g1 ðy1; y2Þ
g2 ðy1; y2Þ
"
#
¼
y1 = v1
y2 = v2
y2
2
4
3
5;
where g2(y1,y2) is an auxiliary function deﬁned to allow the use of Theorem 6.17.
Note g is continuously differentiable 8 y1 > 0 and 8 y2 > 0, which represents an
open rectangle of (y1, y2) points containing the range of (Y1, Y2) (their support
being X2
i¼1ð0; 1Þ. The inverse function g1 is deﬁned by
y1
y2
"
#
¼
g1
1 ðf; wÞ
g1
2 ðf; wÞ
"
#
¼
v1fw
v2
w
2
4
3
58 f; w
ð
Þ 2 RðF; WÞ:
The elements of the Jacobian matrix of the inverse function are continuous
functions of (f,w), and the absolute value of the determinant of the Jacobian is
-4
-3.7
-3.4
-3.1
-2.8
-2.5
-2.2
-1.9
-1.6
-1.3
-1
-0.7
-0.4
-0.1
0.2
0.5
0.8
1.1
1.4
1.7
2
2.3
2.6
2.9
3.2
3.5
3.8
f(t)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
t - density, 5 df
P(t5 ≥ 2.571)= .025
P(t5 ≤ -2.571) = .025
t
Figure 6.7
t-density symmetry.
6.7
t-and F-Densities
343

such that j detðJÞj ¼ v1w=v2 6¼ 0 8ðf; wÞ 2 RðF; WÞ: Then since Y1 and Y2 have
independent w2 densities with v1 and v2 degrees of freedom, respectively, the
joint density of (F,W) is given by Theorem 6.17 as
hðf; wÞ ¼
v1
v2
 v1 =2
f v1=2
ð
Þ1 w
v1þv2
ð
Þ=2
ð
Þ1
2ðv1 þ v2Þ=2 Gðv1=2ÞGðn2=2Þ
exp  w
2
1 þ v1 f
v2




Ið0;1Þ ðfÞ Ið0;1Þ ðwÞ:
Substituting for w using p ¼ w=2
ð
Þ 1 þ v1f=v2
ð
Þ
ð
Þ , and then integrating out
p ﬁnally yields the marginal density of F, as stated in the theorem.
n
We present some useful properties of the F-distribution below.
6.7.2.1
Family Name: F-Family
Parameterization: (v1, v2) ∈O ¼ {(v1, v2): v1 and v2 are positive integers}
Density Deﬁnition: See Theorem 6.21.
Moments:
m ¼
v2
v2 2 for v2 >2;
s2 ¼
2 v2
2 ðv1 þ v2 2Þ
v1 ðv2 2 Þ2 ðv2 4Þ
for v2 >4;
m3 ¼
v2
v1

3 8 v1 ðv1 þ v2 2Þð2 v1 þ v2 2Þ
ðv2 2 Þ3 ðv2 4Þðv2 6Þ
>0 for v2 >6;
m4
s4  3


¼ 12v1 5v2  22
ð
Þ v1 þ v2  2
ð
Þ þ v2  4
ð
Þ v2  2
ð
Þ2
v1 v2  6
ð
Þ v2  8
ð
Þ v1 þ v2  2
ð
Þ
for v2>8
MGF: Does not exist.
The graph of the F-density is skewed to the right, and for v1 > 2 has the typical
shape shown in Figure 6.8. The mode of the density occurs at F	 ¼ v2 v1  2
ð
Þ=
ðv1 v2 þ 2Þ
ð
Þ: If v1 ¼ 2, the density is monotonically decreasing, and approaches
an intercept on the vertical axis equal to t ¼ 2 v1
2 G(1 + v2/2)/G(v2/2) as F ! 0
m(f;v1 ,v2)
F* =
f
v2 (v1 _ 2)
v1 (v2 + 2)
Figure 6.8
F-density for v1 > 2.
344
Chapter 6
Sampling, Sample Moments and Sampling Distributions

(see Figure 6.9). If v1 ¼ 1, the density is monotonically decreasing, and approaches
1 as F ! 0 (see Figure 6.10).
Values of c for which P[F  c] ¼ a for selected values of v1, v2, and a are
available in tables of the F-distribution, i.e., the complement of the CDF is
tabled. However, choices of a are very limited in these tables, generally being
.05 and .01. Computer programs are available for integrating the F-density more
generally (e.g., GAUSS, Matlab, and SAS).
It is interesting to note that if T has a t-density with v degrees of freedom,
then T2 has an F-density with 1 and v degrees of freedom. The reader should
verify this result, which follows directly from deﬁnitions. The reader can also
verify that by letting v2 ! 1 while holding v1 constant, v1F !
d w2
v1 (a proof based
on Slutsky’s theorems, similar to the approach used in Theorem 6.20 can be
constructed).
Finally, note that if Fv1;v2 denotes a random variable that has the F-distribu-
tion with v1 (numerator) and v2 (denominator) degrees of freedom, then Fv1;v2

	1
(i.e., the reciprocal of Fv1;v2 ) has the F-distribution with v2 numerator and v1
f
m( f;2,v2 )
τ
Figure 6.9
F-density for v1 ¼ 2.
f
m( f;1,v2)
Figure 6.10
F-density for v1 ¼ 1.
6.7
t-and F-Densities
345

denominator degrees of freedom (note the reversal of degrees of freedom), which
follows immediately by deﬁnition. Therefore, if the value c is such that P(Fv1;v2
 c) ¼ a, then P( Fv1;v2

	1  c1) ¼ P( Fv2;v1  c1) ¼ a, which allows one to
construct lower-tail events having probability a from upper-tail events having
probability a.
Example 6.17
Using F-Tables
Suppose it is desired to ﬁnd the value of b for which P(F2,4  b) ¼ .05. From the
tables of the F-density, one can obtain the result that P(F4,2  19.25) ¼ .05
(notice the reversed order of the numerator and denominator degrees of
freedom). It follows that P(F2,4  .0519) ¼ .05, where b ¼ (19.25)1 ¼ .0519.
□
6.8
Random Sample Simulation
There is a special real-valued function of a continuous random variable X, called
the probability integral transformation, that is useful for simulating the
outcomes of a random sample of size n from a (continuous) probability distribu-
tion, f(x). In addition, the function is useful in a certain goodness-of-ﬁt test that
we will examine later. The function of interest is deﬁned as follows.
Deﬁnition 6.15
Probability Integral
Transformation (PIT)
Let X be a continuous random variable having a probability distribution
represented by the cumulative distribution function F. Then Y ¼ F(X) is
called the probability integral transformation of X.
Example 6.18
PIT for Exponential
Distribution
Let X ~ (1/y)ex/y I(0,1)(x), so that the CDF of X is given by F(x) ¼ (1  ex/y)
I(0,1)(x). Then the probability integral transformation of the random variable X is
deﬁned by Y ¼ (1  eX/y) I(0,1)(X).
□
There is an important relationship between the probability integral trans-
formation and the continuous uniform PDF which is identiﬁed in the following
theorem.
Theorem 6.22
Uniform Distribution
of the PIT
Let Y ¼ F(X) be the probability integral transformation of the continuous ran-
dom variable X ~ f(x). Then Y is uniformly distributed on the interval (0,1), as
Y ~ I(0,1)(y).
Proof
(a) Case where F is strictly increasing and continuously differentiable
on an open interval containing the range of X
Since F is strictly increasing, dF/dx ¼ f(x) > 0 8 x in the range of X, and
x ¼ F1(y) deﬁnes the inverse function 8y ∈(0,1). Then, by the change-of-
variables technique (Theorem 6.15), the density function for Y is given by
hðyÞ ¼
fðF1 ðyÞÞ dF1 ðyÞ
dy

 ¼ fðxÞ=fðxÞ ¼ 1 8y 2 ð0; 1Þ
¼ 0 elsewhere
8
>
>
<
>
>
:
346
Chapter 6
Sampling, Sample Moments and Sampling Distributions

because dF1ðyÞ=dy ¼ dFðxÞ=dx
½
1 ¼ ðfðxÞ Þ1 for y ¼ FðxÞ; so that Y is uni-
formly distributed on the interval (0,1).
(b) Case where X has MGF MX(t)
The moment generating function of the random variable deﬁned by the
probability integral transform is
MY ðtÞ ¼ E eYt

	
¼ E eFðXÞt


¼
Z 1
1
eFðxÞt fðxÞdx:
Make a change of variable in the integral using z ¼ tF(x), so that dz/dx ¼ tf(x)
and dx/dz ¼ [tf(x)]1, yielding
MY ðtÞ ¼
R t
o ðez=tÞdy ¼ t1ðet 1Þ for t 6¼ 0;
which by the MGF uniqueness theorem identiﬁes a uniform distribution
on (0,1) (recall the MGF of the uniform density presented in Section 4.2).
A general proof of the theorem along these lines can be constructed by using
the characteristic function in place of the MGF above.
n
Example 6.19
PIT Applied to
Exponential RV
Referring to Example 6.18, the density of Y ¼ F(X) ¼ g(X) can be found using the
change-of-variables approach. In particular, dg/dx ¼ (1/y) ex/y > 0 8x ∈(0,1),
the inverse function x ¼ y ln (1  y) is deﬁned and continuously differentiable
8y ∈(0,1), and {dg1(y)/dy| ¼ y/(1  y). Then,
hðyÞ ¼
1
y exp y lnð1  yÞ
y


y
1  y ¼ 1 for y 2 ð0; 1Þ
0 elsewhere
8
>
<
>
:
so that Y ~ I(0,1)(y).
□
The preceding theorem indicates how a random variable having any contin-
uous CDF, F, can be transformed into a random variable having a uniform
probability distribution on the interval (0,1). We now examine the converse to
the result of Theorem 6.22 – a uniformly distributed random variable on the
interval (0,1) can be transformed into a random variable having any continuous
CDF, F. Speciﬁcally, if the continuous CDF F admits an inverse function, then it
can be shown that X ¼ F1(Y) has the CDF F if Y ~ I(0,1)(y). Even if F1 does not
exist, one can deﬁne a function of Y that involves F and that deﬁnes a random
variable that has the CDF F. The details are provided in the next theorem.
Theorem 6.23
Generating Outcomes
of X  F using Inverse
PIT and Uniform
Distribution
Let Y ~ I(0,1)(y) and let F be a continuous CDF. If F has an inverse on the
interval (0,1), then the inverse PIT, X* ¼ F1(Y), has the CDF F. In general,
X* ¼ minx {x: F(x)  Y} has the CDF F.
Proof
(a) Case where F has an inverse on (0,1) Because F1 exists, and since F is
continuous and strictly increasing, then for a given value of y, {x:F(x)  y}
¼ {x: x  F1(y)}. The CDF of X* ¼ F1(Y) can be represented as G(c) ¼
P(x*  c) ¼ P(F1(y)  c) ¼ P(y  F(c)) ¼ F(c),
because
Y
is
uniformly
distributed on (0,1). Therefore, X* has the CDF F.
6.8
Random Sample Simulation
347

(b) General Let x* ¼ minx {x: F(x)  y}. Then P(x*  c) ¼ P(y  F(c)) ¼ F(c)
because y ~ I(0,1) (y), and thus X* has the CDF F.
n
Both of the preceding theorems have important applications. As a preview to
how Theorem 6.22 is used in practice, suppose we are dealing with a random
sample (X1,. . .,Xn) and we hypothesize that the population distribution from
which the sample was drawn is given by the CDF F. If our hypothesized distri-
bution were the correct one, it necessarily follows from Theorem 6.22 that
Yi ¼ F(Xi), i ¼ 1,. . .,n, would constitute a random sample from a uniform popu-
lation distribution on the interval (0,1). Then if the outcomes of the random
sample, i.e., yi ¼ F(xi), i ¼ 1,. . .,n, did not exhibit behavior that would be “appro-
priate” for a random sample from the uniform probability distribution, our
hypothesized CDF, F, would be suspect. We will examine how “appropriate”
behavior is evaluated in this case when we examine the question of hypothesis
testing.
Theorem 6.23 suggests a procedure that can be used for simulating outcomes
of random samples of size n from any continuous probability distribution func-
tion, F. Speciﬁcally, one begins with outcomes of independent uniformly
distributed random variables on the interval (0,1). Computer programs are read-
ily available that generate independent outcomes of Y ~ I(0,1)(y) using numerical
techniques, and are commonly referred to as “uniform random number
generators”. Theorem 6.23 indicates that the outcome of a random sample of
size n from the desired population CDF F can then be obtained by calculating
x*i ¼ minx{x:F(x)  yi},
i ¼ 1,. . .,n
where
(y1,. . .,yn)
are
the
independent
outcomes of Y ~ I(0,1)(y) generated via the computer. It is frequently that case
that the CDF F has an inverse function F1 existing, so that the random sample
can be represented more simply as x*i ¼ F1(yi), i ¼ 1,. . .,n.
Example 6.20
Simulating a Random
Sample from an
Exponential
Distribution
Suppose we wish to simulate a random sample of size ﬁve from an exponential
population distribution of computer memory chip lifetimes, where y ¼ 100, so
that X ~ (1/100) ex/100 I(0,1)(x) and F(x) ¼ (1  ex/100)I(0,1)(x), with x measured
in 1,000’s of hours. We use the result in Theorem 6.23, and note that for
y ∈(0,1), F has an inverse function deﬁned by x ¼ 100 ln(1  y). Then, using
ﬁve iid outcomes of Y ~ I(0,1)(y) generated by computer, as {.127, .871, .464, .922,
.761}, we simulate a random sample outcome of ﬁve chip lifetimes as {13.582,
204.794, 62.362, 255.105, 143.129}.
□
The preceding approach has wide applicability in simulating random
samples from continuous probability distributions, but sometimes the proce-
dure can be numerically complex if the inverse function is impossible to solve
for explicitly, or if the solution to minx{x: F(x)  y} is difﬁcult to obtain.
Other approaches exist for simulating outcomes of random samples, and
others are being developed in the literature. For an introduction to some
alternative random variable generation concepts, the interested reader can
consult M. Johnson (1987), Multivariate Statistical Simulation. New York:
John Wiley.
348
Chapter 6
Sampling, Sample Moments and Sampling Distributions

To this point we have not discussed a method for generating random samples
from a population distribution when the distribution is discrete. In principle, the
procedure in this case is relatively straightforward and, as in the continuous
case, utilizes iid outcomes of a random variable that is uniformly distributed on
(0,1). Speciﬁcally, let the range of the discrete random variable X be given by
R(X) ¼ {x1, x2, x3, . . .}, where x1 < x2 < x3 < . . ., and let the density function of X
be f(x). Deﬁne the function g(y) as
gðyÞ ¼
x1
x2
x3
...
2
6666664
3
7777775
if
0<y  Fðx1Þ
Fðx1Þ<y  Fðx2Þ
Fðx2Þ<y  Fðx3Þ
...
2
6666664
3
7777775
:
By the duality between CDFs and PDFs, this deﬁnition of g(y) implies that
P(g(y) ¼ xi) ¼ f(xi) if Y ~ I(0,1)(y) since P(y ∈(F(xi1), F(xi)]) ¼ F(xi)  F(xi1) ¼
f(xi). Thus g(y), for Y ~ I(0,1)(y), is a discrete random variable with PDF f. If the
range of X does not exhibit a ﬁnite lower bound then all outcomes would be
calculated as gðyÞ ¼ yi if F xi1
ð
Þ<y  F Xi
ð
Þ:
Example 6.21
Simulating Random
Samples from Bernoulli
Based on Uniform
In order to simulate a random sample of consumer buying behavior regarding a
given product, assume that the appropriate probability distribution from which
random sampling would occur in this case is given by the Bernoulli distribution
with p ¼ .5, so that the consumer is as likely to buy as not. Then deﬁne
gðyÞ ¼
0
if 0<y  :5;
1
if :5<y  1,
(
where 1 denotes a purchase, and 0 denotes no purchase. Utilizing the numbers
{.217, .766, .822, .402, .674} generated via a computer-based uniform random-
number generator for iid outcomes of Y  I 0;1
ð
ÞðyÞ, we calculate a simulated
random sample of consumer purchasing decisions as {0,1,1,0,1}.
□
6.9
Order Statistics
Situations arise in practice where the relative magnitudes of observations are of
primary interest. For example, the largest value in a sample of observations may
be of particular interest, such as if a business is deciding on the appropriate level
of capacity in order to successfully service the demands of customers over a
number of operating periods. The smallest observation may also be of interest,
such as in the manufacture of a consumer electronics product that will properly
function only so long as the most short-lived critical electronics component in
its circuitry. One might also be interested in the median value of sample
observations as a measure of central tendency of the observations, or the
6.9
Order Statistics
349

sample range of observations as a measure of spread. All of these statistics are
examples of order statistics or functions of order statistics, which also play an
important role in nonparametric hypothesis testing, which will be introduced in
Chapter 10.
The order statistics corresponding to a random sample (X1,. . .,Xn) are simply
the Xi0s arranged in order of increasing magnitude. The formal deﬁnition is as
follows.
Deﬁnition 6.16
Order Statistics
Let sort(x) be the n  1 vector function whose value is deﬁned by sorting the
elements of the n  1 vector x ¼ [x1,. . .,xn]0 from the lowest to the highest
values. The n  1 vector of order statistics Xo ¼ X½1; . . . ; X½n

0 corresponding
to the random sample X ¼ [X1,. . .,Xn]0 is deﬁned by Xo ¼ sort(X), and has an
outcome xo ¼ sort(x). The random variableX k
½  is called the kth order statistic.
Note that the order statistics are indeed statistics, since they are deﬁned as
(vector) functions of the random sample. It is apparent from the deﬁnition that
outcomes of order statistics satisfy the inequalities x[1]  x[2]  . . .  x[n].
We will examine the sampling distribution of order statistics under the
assumption that we are random sampling from a population distribution, i.e.,
the Xi0s are iid. In this case, the following result is available concerning the
sampling distribution of the kth order statistic.
Theorem 6.24
Sampling Distribution
of the kth Order
Statistic X[k]
Let (X1,. . .,Xn) be a random sample from a population distribution with CDF F,
and let X k
½  be the kth order statistic corresponding to the random sample. Then
the CDF of X k
½  is given by
FX½k ðbÞ ¼
X
n
j¼k
n
j
 
!
Fðb Þj ½1  FðbÞ nj:
Proof
For a given value of b, deﬁne the random variable Yi ¼ I(1,b](Xi). Note that
Yi has a Bernoulli distribution with p ¼ P(yi ¼ 1) ¼ P(xi  b) ¼ F(b). Since the
Yi0s are iid, it follows that Pn
i¼1 Yi has the binomial distribution with parameters
n and p.
Now note the equivalence of the following events:
fðx1; . . . ; xnÞ : x½k  bg ¼
ðx1; . . . ; xnÞ :
X
n
i¼1
I 1;b
ð
 xi
ð
Þ  k
(
)
:
The event to the left of the equality corresponds to the situation where the kth
largest outcome in (x1,. . .,xn) is less than or equal to b. This event can happen iff
at least k outcomes in (x1,. . .,xn) are less than or equal to b, which is the event to
the right of the equality. Hence, the events are equivalent. It then follows from
the binomial distribution of Pn
i¼1 Yi that
350
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Pðx½k  bÞ ¼ P
X
n
i¼1
yi  k
 
!
¼
X
n
j¼k
n
j
 
!
Fðb Þj ½1  FðbÞ nj :
n
The CDF identiﬁed in Theorem 6.24 simpliﬁes considerably in two cases of
particular relevance in applications – the smallest and the largest order statistic.
Corollary 6.1 Sampling
Distributions of
X[1] and X[n]
Assume the conditions of Theorem 6.24. Then FX½1 ðbÞ ¼ 1  ½1  FðbÞ n and
FX½nðbÞ ¼ Fðb Þn.
Proof
It follows from Theorem 6.24 that FX½1ðbÞ ¼ Pn
j¼1
n
j


Fðb Þj ½1  FðbÞ nj ¼
1  ½1  FðbÞ n , where the second equality follows from the fact that
Pn
j¼0
n
j


FðbÞj 1  FðbÞ
½
nj ¼ 1 because we are summing a binomial density over
all the values in the support of the random variable. Also, by direct evaluation of
the CDF, FX½nðbÞ ¼ Pn
j¼n
n
j


Fðb Þj ð1  FðbÞ Þnj ¼ ½FðbÞ n :
n
Example 6.22
Probability of Largest
Waiting Time
The waiting time between customer arrivals at the pharmacy department in a
variety store is given by the exponential PDF f(x) ¼ .2e.2x I(0,1)(x), where x is
measured in minutes. In a random sample of 10 customers, what is the proba-
bility that the smallest waiting time will be greater than 2 minutes? What is the
probability that the largest waiting time will be no greater than 5 minutes?
The CDF of the exponential population distribution is given by F(b) ¼
1  e.2b. Then
P x 1
½ >2

	
¼ 1  P x 1
½   2

	
¼ 1  FX 1
½ ð2Þ
¼ 1 
1  e:2ð2Þ


h
i10
¼ :0183;
P x 10
½
  5

	
¼ FX 10
½
ð5Þ ¼ 1  e:2ð5Þ
h
i10
¼ :0102:
□
The PDFs of the order statistics can be found using the duality between
CDFs and PDFs. If the population distribution, fðxÞ, is discrete with support
x1 < x2 < . . . < xn, then the PDF of the kth order statistic can be deﬁned in the
usual way as fX½k ðxiÞ ¼ FX½k ðxiÞ  FX½k ðxi1Þ for i  2 with fX½k ðx1Þ ¼ FX½k ðx1Þ. In
the continuous case, the PDF can be found after differentiation of the CDF, and
some algebraic manipulation, to be
fX½k ðxÞ ¼ dFXk ðxÞ
dx
¼
n!
ðk  1Þ!ðn  kÞ! fðxÞFðx Þk1 ½1  FðxÞ nk :
In order to assign probabilities to events involving outcomes of the sample
range, i.e., X[n]  X[1], or to make a joint probability statement concerning the
outcomes of the largest and smallest observations in a random sample, the joint
6.9
Order Statistics
351

sampling distribution of (X[1], X[n]) is needed. Furthermore, if the sample size n is
an even number, and one wishes to assign probabilities to events involving the
sample median deﬁned as
d
medðXÞ ¼ X½k þ X½kþ1

	
=2 for k ¼ n/2, the joint
sampling distribution of (X[k], X[k+1]) is needed. (When n is odd, the sample
median is deﬁned as d
medðXÞ ¼ X½k where k ¼ (n + 1)/2, and so Theorem 6.24
covers this case.) The sampling distribution for any pair of order statistics is
given as follows.
Theorem 6.25
Sampling Distribution
of Pairs of Order
Statistics (X½k; X½‘)
Let X[k] and X½‘, k < ‘, be the kth and ‘th order statistics corresponding to the
random sample X ¼ X1; . . . ; Xn
½
0 from a population distribution with CDF F and
PDF f. Then the joint CDF of (X[k], X½‘) is given by
FX½k;X½‘ ðbk; b‘Þ
¼
FX½‘ ðb‘Þ for bk  b‘;
P
n
i¼k
P
ni
j¼maxf0;‘ig
n!
i!j!ðn  i  jÞ! Fðbk Þi ½Fðb‘Þ  FðbkÞ j ½1  Fðb‘Þ nij for bk < b‘
8
>
>
<
>
>
:
Proof
Given k < ‘, it follows by deﬁnition of the order statistics that bk  b‘ implies
{x: x ‘½   b‘}  {x: x[k]  bk}, so that
FX½k;X½‘ ðbk; b‘Þ ¼ Pðx½k  bk; x½‘  b‘Þ ¼ Pðx½‘  b‘Þ ¼ Fx½‘ ðb‘Þ
proving the ﬁrst part of the deﬁnition of the CDF.
When bk < b‘, note that the event {x[k]  bk, x‘  b‘} corresponds to the event
that at least k of the random sample outcomes x1,. . .,xn are less than or equal to
bk and at least‘are  b‘. Deﬁning the index set I ¼ {(i,j): max{0,‘  i}  j  n  i;
k  i  n; i and j are integers}, we can then represent the event as
x k
½   bk; x ‘½   b‘


¼
[
i;j
ð
Þ2I
exactly i x0
is  bk; exactly j x0
is such that bk<xi  b‘
f
g
Now note that each of the disjoint events involved in the union operation
can be assigned probability via the multinomial distribution, where the outcome
of each Xi in the random sample is categorized into one of three types xi  bk,
xi ∈(bk, b‘], and xi > b‘ which occur with probabilities F(bk), Fðb‘Þ  F(bk), and
1  F(b‘), respectively. Then applying the multinomial distribution (Section 4.1)
and summing the probabilities of all of the disjoint events in the union operation
yield the second part of the deﬁnition of the CDF.
n
As indicated following Theorem 6.24, the PDF of (X[k], X½‘) can be obtained
through the duality between CDFs and PDFs. In the discrete case, the PDF
would be obtained by appropriate differencing of the CDF, while in the continu-
ous case, the CDF would be differentiated (recall Theorems 2.4 and 2.5). Details
are left to the reader.
The CDF simpliﬁes considerably for the case of the joint distribution of the
extreme values (X[1], X[n]).
352
Chapter 6
Sampling, Sample Moments and Sampling Distributions

Corollary 6.2 Sampling
Distribution of Extreme
Values (X[1], X[n])
Let k ¼ 1 and ‘ ¼ n in Theorem 6.25. Then by the binomial theorem,
FX½1;X½n ðb1; bnÞ ¼
Fðbn Þn for b1  bn
Fðbn Þn ½FðbnÞ  Fðb1Þ n for b1 < bn :
(
□
Example 6.23
Probability of Minimum
and Maximum Waiting
Times
Consider Example 6.22 regarding waiting time between customer arrivals. In a
sample of 10 customer arrivals, what is the probability that the minimum
waiting time will be  4 minutes and the maximum waiting time will be
 8 minutes?
The probability can be calculated by evaluating the joint CDF of the extreme
values identiﬁed in Corollary 6.2, yielding
FX½1;X½10 ð4; 8Þ ¼ Fð8 Þ10 ½Fð8Þ  Fð4Þ 10
¼ 1  e:2ð8Þ
h
i10
 e:2ð4Þ  e:2ð8Þ
h
i10
¼ :1049
□
Sampling distributions for functions of order statistics, such as the sample
range and the sample median when n is even, can be pursued using the change-
of-variable approach presented in Section 6.6. The multinomial logic of the proof
of Theorem 6.25 can be extended to derive joint densities of three or more order
statistics. For additional details on properties and functions of order statistics,
see M. Kendall and A. Stuart, (1977) Advanced Theory, Vol. 1, Chapter 14, and
the references therein.
Keywords, Phrases, and Symbols
CDF approach
Change of variables technique
Empirical distribution function
Empirical substitution principle
Equivalent events approach
F-density
General random sampling
Glivenko-Cantelli theorem
Jacobian matrix
Joint density of the random sample
Joint sample moment about
the mean
Joint sample moment about
the origin
Log-normal distribution
MGF approach
Order statistics
Outcome of the random sample
Population
Population distribution
Population moment
Probability integral transformation
Random sample
Random sampling from a population
distribution
Random sampling with replacement
Random sampling without
replacement
rth sample moment about the
mean, Mr
rth sample moment about the
origin, M0r
Sample correlation matrix
Sample correlation, RXY
Sample covariance matrix
Sample covariance, SXY
Sample mean, Xn
Sample standard deviation
Sample variance, S2
n
Sampling density or sampling
distribution of a function
of a random sample
Sampling error
Simple random sampling
Simulation
Statistic
Stochastic process
Support of f(x)
Support of the density
t-density
6.9
Order Statistics
353

Problems
1. Let X be a random sample of size n from a N(m,s2)
population distribution representing the weights, in
ounces, of cereal placed in cereal boxes for a certain brand
and type of breakfast cereal. Deﬁne ^s as in Theorem 6.19.
(a) Show that the random variable T ¼ n1=2 X  m

	
=^s has
the t-distribution with n-1 degrees of freedom.
(b) Let n ¼ 25. What is the probability that the random
interval
X  2:06^s=n1=2; X þ 2:06^s=n1=2

	
will have
an outcome that contains the value of m? (This ran-
dom interval is an example of a conﬁdence interval –
in
this
case
for
the
population
mean
m.
See
Section 10.6.)
(c) Suppose that x ¼ 16.3 and s2 ¼ .01. Deﬁne a conﬁ-
dence interval that is designed to have a .90 probabil-
ity of generating an outcome that contains the value
of the population mean weight of cereal placed in the
cereal boxes. Generate a conﬁdence interval outcome
for the mean weight.
2. Let X and Y be two independent random samples of
sizes nx and ny, respectively, from two normal population
distributions that do not necessarily have the same means
or variances. The two distributions refer to the miles per
gallon achieved by two 1/2-ton pickup trucks produced by
two rival Detroit manufacturers. Deﬁne
^s
as in
Theorem 6.19.
(a) Show that the random variable F ¼ ð^s2
X = s2
XÞ=ð^s2
Y = s2
YÞ
has the F-distribution with (nx  1) numerator and
(ny  1) denominator degrees of freedom.
(b) Let nx ¼ 21 and ny ¼ 31. What is the probability that
the random interval
:49 ^s2
Y=^s2
X

	
; 1:93 ^s2
Y=^s2
X

	

	
will
have an outcome that will contain the value of the
ratio of the variances sY
2/sX
2? (This random interval
is another example of a conﬁdence interval – in this
case for the ratio of the population variances sY
2/sX
2.)
(c) Suppose that sx
2 ¼ .25 and sy
2 ¼ .04. Deﬁne a conﬁ-
dence interval that is designed to have a .98 probabil-
ity of generating an outcome that contains the value
of the ratio of population variances associated with
the miles per gallon achieved by the two pickup
trucks. Generate a conﬁdence interval outcome for
the ratio of variances.
3. Let X be a random sample of size n from a N(m,s2)
population distribution representing the yield per acre, in
pounds, of a new strain of hops used in the production of
premium beer.
(a) Justify that the random interval (nS2/w2
a, nS2/w2
1a) will
have an outcome that contains the value of the popu-
lation variance s2 with probability a, where w2
d is a
number for which P(x > w2
d) ¼ d when X has a w2 dis-
tribution with n  1 degrees of freedom.
(b) Suppose that s2 ¼ 9 and n ¼ 20. Deﬁne a conﬁdence
interval that is designed to have a .95 probability of
generating an outcome that contains the value of the
population variance of hop yields, s2. Generate a con-
ﬁdence interval outcome for the variance.
4. The shipping and receiving department of a large toy
manufacturer is contemplating two strategies for sam-
pling incoming parts deliveries and estimating the propor-
tion, p, of defective parts in a shipment. The two
strategies are differentiated on the basis of whether ran-
dom sampling will be with or without replacement. In
each case, a sample mean will be calculated and used as an
estimate of the proportion of defective parts in the ship-
ment. The department wants to use the strategy that will
generate estimates that are smallest in expected squared
distance from p.
(a) Compare the means and variances of the sample
mean under both sampling strategies. Which strategy
should be used?
(b) Describe conditions under which there will be little
difference between the two methods in terms of
expected squared distance from the true proportion
of defectives.
(c) Do the sample means converge in probability to p in
each case? Do they converge in mean square?
Explain.
(d) If a shipment contains 250 parts of which 10 percent
are defective, and if a random sample of 50 will be
taken from the shipment, calculate the percentage
reduction in expected squared distance that can be
obtained by using the better strategy.
5. GenAG,
Inc.,
a
genetics
engineering
laboratory
specializing in the production of better seed varieties for
commercial agriculture, is analyzing the yield response to
fertilizer application for a new variety of overlineley that
it has developed. GenAg has planted 40 acres of the new
overlineley variety and has applied a different ﬁxed level
of fertilizer to each one-acre plot. In all other respects the
cultivation
of
the
crop
was
identical.
The
GenAg
scientists
maintain
that
the
relationship
between
354
Chapter 6
Sampling, Sample Moments and Sampling Distributions

observed levels of yield, in bushels per acre, and the level
of fertilizer applied, in pounds per acre, will be a quadratic
relationship as Yj ¼ b0 þ b1 fj þ b2 fj
2 þ Vj, where fj is
the level of fertilizer applied to the jth one-acre plot, the
b0s are ﬁxed parameters, and the Vj0s are iid random
variables with some continuous probability density func-
tion for which EVj ¼ 0 and var(Vj) ¼ s2.
(a) Given GenAg’s assumptions, is (Y1,. . .,Y40) a random
sample from a population distribution or more
general random sampling? Explain.
(b) Express the mean and variance of the sample mean
Y40 as a function of the parameters and fj variables. If
the sample size could be increased without bound,
would Yn converge in probability to some constant?
Explain.
(c) Is it true that Yn  b0 þb1n1 Pn
i¼1 fj þb2n1 Pn
i¼1 f2
j




!
p 0 ? If so, interpret the meaning of this result.
Based on your analysis to this point, does it appear
that an outcome of Y40 will produce a meaningful
estimate of any characteristic of the yield process?
(d) Suppose that the 40 one-acre plots were all contigu-
ous on a given 40 acre plot of land. Might there be
reasons for questioning the assumption that the Vj’s
are iid? What would the outcome of Vj represent in
GenAg’s
representation
of
the
yield
process?
Presuming that the Vj0s were not iid, would this
change your answer to (a)?
6. The Always Ready Battery Co. has developed a new
“Failsafe” battery which incorporates a small secondary
battery that becomes immediately functional upon fail-
ure of the main battery. The operating life of the main
battery is a gamma distributed random variable as X1 ~
Gamma(3,1) where X1 is measured in years. The operating
life of the secondary battery is also gamma distributed as
X2 ~ Gamma(2,1). The operating lives of the main and
secondary batteries are independent.
(a) Let Y1 ¼ X1 þ X2 represent the total operating life of
the Failsafe battery, and Y2 ¼ X1/(X1 þ X2) represent
the
proportion
of
total
operating
life
that
is
contributed by the main battery. Derive the joint
probability distribution of (Y1, Y2).
(b) Are Y1 and Y2 independent random variables?
(c) Deﬁne the marginal densities of Y1 and Y2 . What
speciﬁc families of densities do these marginal
densities belong to?
(d) What is the expected proportion of total operating life
contributed by the main battery? What is the proba-
bility that the secondary battery contributes more
than 50 percent of the total operating life of the
Failsafe battery?
(e) What is the expected total operating life of the
Failsafe battery?
7. The seasonal catch of a commercial ﬁshing vessel in a
certain ﬁshery in the southern hemisphere can be
represented by Q ¼ c(z) V, where z is a vector of
characteristics of the vessel relating to tonnage, length,
number of crew members, holding tank size, etc., c(z)
represents maximum ﬁshing capacity of the boat, Q
represents the tons of ﬁsh caught, and V ~ y vy1I(0,1)(v)
represents the proportion of ﬁshing capacity realized.
(a) Derive the density function of seasonal catch.
(b) If y ¼ 10 and c(z) ¼ 5,000, what is the expected value
of seasonal catch?
8. A company markets its line of products directly to
consumers through telephone solicitation. Salespersons
are given a base pay that depends on the number of
documented phone calls made plus incentive pay for
each phone call that results in a sale. It can be assumed
that the number of phone calls that result in sale is a
binomial random variable with parameters p (probability
of sale) and n (number of phone calls). The base pay is $.50
per call and the incentive pay is $5.00 per sale.
(a) Derive the probability distribution of pay received by
a salesperson making n calls in a day.
(b) Given that 100 calls are made in a day and p ¼ .05,
what is the expected pay of the salesperson? What is
the probability that pay will be $50 or less?
9. The daily quantity of a commodity that can be pro-
duced using a certain type of production technology is
given by the outcome of the random variable Q, deﬁned
as Q ¼ 10 x:35
1
x:5
2 V, where Q is measured in tons/day, x1
represents units of labor per day, x2 represents units of
capital per day, and v ¼ ee, with e ~ N(0, s2).
(a) Derive the probability density function of the random
variable V. (What you will have derived is a PDF that
is a member of the “lognormal” family of densities.
In general, if X ~ N(m, s2), Y ¼ eX ~ lognormal with
mean ¼ exp(m + s2/2),
variance ¼ exp(2 m + 2s2) 
exp(2 m + s2), and m0
r ¼ exp[rm + 1/2 r2s2]).
(b) Derive the density of Q if x1 ¼ x2 ¼ 2.
Problems
355

(c) Deﬁne the expected value of Q in terms of the levels
of x1, x2 and s2. What is the expected value of Q if
x1 ¼ x2 ¼ 2 and s2 ¼ 1?
(d) The above production technology is used in 1,600
plants in a country in Eastern Europe. The economy
is centrally planned in that country, and all of the
plants are required to use labor and capital at the
levels x1 ¼ 7.24579 and x2 ¼ 4. Assume that var(e)
¼ .25. An economist says that the aggregate daily
production function
Q	 ¼ S
1600
i¼1 Qi ¼ 10 x:35
1
x:5
2
S
1600
i¼1 Vi
is such that aggregate daily production, Q*, can be consid-
ered to be approximately normally distributed. Do you
agree? Justify or refute the economist’s proposition. You
may assume that Vi0s, and hence the Qi0s, are jointly
independent r.v.s.
(e) Deﬁne the appropriate Berry-Esseen inequality bound
on the approximation error corresponding to a CLT
applicable to part (d).
10. The daily price, p, and quantity sold, q, of ground beef
produced by the Red Meat Co. can be represented by
outcomes of the bivariate random variable (P,Q) having
bivariate density function
f p; q
ð
Þ ¼ 2pepqI½:5;1ðpÞIð0;1ÞðqÞ
where p is measured in dollars and q is measured in
1,000’s of pounds.
(a) Derive the probability density function for R ¼ PQ,
where outcomes of R represent daily revenue from
ground beef sales. (Hint: deﬁne W ¼ P as an “auxil-
iary” random variable and use the change of variable
approach).
(b) What is the expected value of daily revenue? What is
the probability that daily revenue exceeds $1,000?
11. Let X ¼ (X1,. . .,X26) and Y ¼ (Y1,. . .,Y31) represent two
independent random samples from two normal popula-
tion distributions. Let S2
X and S2
Y represent the sample
variances associated with the two random samples and
let X and Y represent the respective sample means. Deﬁne
^s0s as in Theorem 6.19.
(a) What is the value of P
jx  EXj
^s2
X=26

	1=2  1:316
 
!
?
(b) What is the value of P 26 s2
X
s2
X
>37:652


?
(c) What is the value of Pðs2
X >6:02432Þ assuming s2
X ¼ 4?
(d) What is the value of Pðs2
Y >1:92 s2
XÞ, assuming that
s2
X ¼ s2
Y?
(e) Find the value of c for which the following probability
statement is true:
P s2
Y ^s2
X
s2
X ^s2
Y
 c
 
!
¼ :05:
12. The daily price, p, and daily quantity sold, q, of pink
salmon produced by the AJAX Fish Packing Co. can be
represented by outcomes of the bivariate random variable
(P,Q) with density function
f p; q
ð
Þ ¼ 5pepqI½:2;:4ðpÞIð0;1ÞðqÞ
where p is measured in dollars, and q is measured in
1,000’s of pounds.
(a) Derive the density function for R ¼ PQ, where
outcomes of R represent daily revenue from ﬁsh
sales. (Hint: Deﬁne W ¼ P as an auxiliary random
variable, and use the change of variables approach).
(b) What is the expected value of daily revenue? What is
the probability that daily revenue exceeds $300?
13. The probability that a customer entering an electron-
ics store will make a purchase is equal to p ¼ .15, and
customers’ decisions whether to purchase electronics
equipment are jointly independent random variables.
(a) Simulate the buying behavior of 10 customers enter-
ing the store using the following 10 outcomes from a
Uniform(0,1) computer random number generator:
(.4194,.3454,.8133,.1770,.5761,.6869,.5394,.
5098,.4966,.5264).
(b) Calculate the sample mean and sample variance, and
compare them to the appropriate population mean
and variance.
14. Under the conditions of the previous problem:
(a) Of the ﬁrst 10 customers that enter the store on 10
consecutive days, simulate the daily number of
customers that make a purchase. Use the following
10 outcomes from a Uniform(0,1) computer random
number generator:
(.0288,.7936,.8807,.4055,.6605,.3188,.6717,
.2329,.1896,.8719).
356
Chapter 6
Sampling, Sample Moments and Sampling Distributions

(b) Calculate the sample mean and sample variance, and
compare them to the appropriate population mean
and variance.
15. The
number
of
times
that
a
copy
machine
malfunctions in a day is the outcome of a Poisson process
with l ¼ .1.
(a) Simulate the operating behavior of the copy machine
regarding the daily number of malfunctions over a
10 day period using the following 10 outcomes from
a Uniform(0,1) computer random number generator:
(.5263,.8270,.8509,.1044,.6216,.9214,.1665,
.5079,.1715,.1726)
(b) Calculate the sample mean and sample variance, and
compare them to the appropriate population mean
and variance.
16. The length of time that a 4 gigabyte PC computer
memory module operates until failure is the outcome of
an exponential random variable with mean EX ¼ 3.25,
where x is measured in 100,000 hour units.
(a) Simulate the operating lives of 10 memory modules
using the following 10 outcomes from a Uniform(0,1)
computer random number generator:
(.2558,.5938,.1424,.9476,.5748,.8641,.0968,
.5839,.3201,.1577).
(b) Calculate the sample mean and sample variance, and
compare them to the appropriate population mean
and variance.
17. The monthly proportion of purchases paid by check
to a large grocery store that are returned because of insuf-
ﬁcient funds can be viewed as the outcome of a Beta(1, 20)
distribution.
(a) Simulate 12 monthly proportions of returned checks
using the following 24 outcomes from a Uniform(0,1)
computer random number generator:
(.6829,.4283,.0505,.7314,.8538,.6762,.6895,.9955,.2201,
.9144,.3982,.9574,
.0801,.6117,.3706,.2936,.2799,.3900,.7533,.0113,.5659,
.9063,.5029,.6385)
(Hint: In a previous problem you have proven
that Y ¼ X1/(X1 + X2) has a beta distribution
with parameters (a,b) if (X1, X2) are independent
gamma-distributed
random
variables
with
parameters (a, b) and (b, b), respectively.)
(b) Calculate the sample mean and sample variance, and
compare them to the appropriate population mean
and variance.
18. The daily closing price for a certain stock issue on the
NYSE can be represented as the outcome of Yt ¼ Yt1 +Vt,
where yt is the value of the stock price on day t, and
Vt ~ N(0,4) (This is an example of a stochastic process
known as a random walk.)
(a) Use the change of variable approach to verify that
if
(U1,
U2)
are
independent
and
identically
distributed Uniform(0,1) random variables, then
V1 ¼ 2 ln U1
ð
Þ
½
:5 cos 2pU2
ð
Þ and
V2 ¼ 2 ln U1
ð
Þ
½
:5 sin 2pU2
ð
Þ
are independent and identically distributed N(0,1) random
variables.
(b) Simulate 10 days worth of stock prices (y1,. . .,y10)
using y0 ¼ 50, the result in (a), and the following
10 outcomes from a Uniform(0,1) computer ran-
dom number generator:
(.9913,.4661,.1018,.0988,.4081,.3422,.1585,.6351,.0634,
.4931).
(c) Are you simulating a random sample from a
population distribution or is this more general
random sampling? Calculate the sample mean
and sample variance and compare them to what-
ever characteristics of the random walk process
that you feel is appropriate.
19. A random sample of the gas mileage achieved by 20
domestic compact automobiles resulted in the following
outcome:
(25.52,24.90,22.24,22.36,26.62,23.46,25.46,24.98,25.82,
26.10,21.59,22.89,27.82,22.40,23.98,27.77,23.29,24.57,
23.97,24.70).
(a) Deﬁne and graph the empirical distribution function.
(b) What is the estimated probability that gas mileage
will exceed 26 miles per gallon?
(c) What is the estimated probability that gas mileage
will be between 24 and 26 miles per gallon?
(d) Acting as if the EDF is the true CDF of gas mileages,
calculate the expected value of gas mileage. Is the
value you calculated equal to the sample mean?
Why or why not?
20. The time between work-related injuries at the Impe-
rial Tool and Die Co. during a given span of time resulted
in the following 20 observations, where time was
measured in weeks:
(9.68,6.97,7.08,.50,6.71,1.13,2.20,9.98,4.63,7.59,3.99,3.26,
.92,3.07,17.96,4.69,1.80,8.73,18.13,4.02).
Problems
357

(a) Deﬁne and graph the empirical distribution function.
(b) What is the estimated probability that there will be at
least 8 weeks between work-related injuries?
(c) What is the probability that there will be between
4 and 8 weeks between work-related injuries?
(d) Acting as if the EDF is the true CDF of time between
work-related injuries, calculate the expected value of
time between injuries. Is the value you calculated
equal to the sample mean? Why or why not?
21. A realtor randomly samples homeowners who have
purchased homes in the last 2 years and records their
income, y, and home purchase price, p (the population is
large enough that one can consider this a random sample
with replacement):
Income
Price
Income
Price
21,256
49,412
37,589
74,574
97,530
170,249
137,557
232,097
24,759
56,856
67,598
124,309
18,369
45,828
83,198
144,103
35,890
73,703
46,873
92,600
38,749
80,050
24,897
61,763
57,893
11,0658
36,954
77,971
(a) Calculate the sample covariance between income
and home price.
(b) Calculate the sample correlation between income
and home price.
(c) Calculate the linear function of income of the form
^p ¼ a þ by that is minimum distance from the home
price observations.
(d) Discuss the extent to which there is a linear relation-
ship between income and home price.
22. The proportion of the work force of a large Detroit
manufacturing ﬁrm that takes at least 1 day’s sick leave in
a given work week is assumed to be the outcome of a
random variable whose PDF is well-represented by a
uniform distribution on the interval [0, .10].
(a) In a random sample of eight work weeks, what is the
probability that the maximum proportion of workers
who take sick leave is  .05?
(b) What is the probability that the sum of all eight
sample observations will be between .25 and .75?
23. A large aircraft manufacturer produces a passenger jet
having a navigation component consisting of three
sequentially functioning redundant navigation systems
that will allow the jet to be properly controlled so long
as at least one of the systems remain operational. The
operating life of each of the systems is the outcome of a
random variable having the exponential PDF f(x) ¼
.1 e.1x I(0,1) (x) where x is measured in 1,000’s of hours
and the operating lives are independent of one another.
(a) What is the probability that the navigation compo-
nent
will
continue
to
function
for
at
least
20,000 hours?
(b) In an economizing mode, a redesign of the jet is being
considered that will reduce the navigation compo-
nent from three redundant systems to two. How
does this affect the probability of the event in (a)?
24. A news agency wants to poll the population of
registered voters in the United States (over 200,000,000)
to ﬁnd out how many would vote for the Republican
candidate, Henry Washington, if the election were held
today. They intend to take a random sample, with replace-
ment,
of 1,000 registered
U.S. voters, record
their
preferences for yi ¼ 1
ð
Þor against yi ¼ 0
ð
ÞMr. Washington,
and then use the 1,000 sampled outcomes to estimate the
proportion of registered voters in favor of the candidate.
They conduct the random sampling, and observe that
Pn
i¼1 yi ¼ 593:
Given their random sampling design,
they are assuming that
yi
0s  iid BernoulliðpÞ , where
p is the proportion of registered voters in favor of
Mr. Washington. They intend to use the sample mean, X,
as an estimator for p.
(a) What is the expected value of the estimator?
(b) What is the standard deviation of the estimator?
(c) Provide a lower bound to the probability that the
estimate is within  .03 of the true proportion of
voters in favor of the candidate.
(d) What size of random sample would the agency need
to use in order to generate an estimate, based on the
sample mean, that would be within  .01 of the true
proportion of voters?
(e) What is the estimate of the proportion of voters in
favor of the candidate?
(f) Would there be much gain, in the way of lower
variance of the estimator, if the agency would have
sampled
without
replacement
instead
of
with
replacement? Explain.
358
Chapter 6
Sampling, Sample Moments and Sampling Distributions

25. Your company manufactures LCD screens that are
used in the production of a popular smartphone sold by a
major wireless cell phone service provider. The engineer-
ing department suggests that the operating life of the
screen “likely” exhibits the memoryless property
P x>s þ tjx>s
ð
Þ ¼ P x>t
ð
Þ8s and t>0
where outcomes, x, of the random variable X measure the
operating life of the screen, measured in 100,000 hour
increments, until failure of the screen. But they are not
entirely sure of that as yet. A random sample of 50 of the
LCD screens has been subjected to a test that accurately
assesses their operating lives, and the following outcomes
occurred:
0.841
0.478
2.631
0.126
2.953
0.476
0.744
0.753
3.344
0.141
0.145
2.031
0.694
0.654
0.402
0.893
3.675
2.068
0.366
3.145
0.064
0.740
0.522
0.146
1.641
0.506
0.790
3.096
1.381
2.249
2.057
1.045
0.783
0.368
0.121
5.788
1.862
2.165
1.156
0.200
2.415
1.077
5.258
0.326
3.317
2.105
0.361
7.611
2.334
0.808
(a) Deﬁne the EDF estimate of the underlying CDF for
the screen lifetimes.
(b) Graph the EDF that you estimated in (a).
(c) What is the mean of screen lifetimes implied by the
EDF?
(d) Treating the value you obtained in (c) as if it were the
true mean, plot the CDF implied by the memoryless
property on top of the EDF plot in (b). Do they appear
to be similar or not?
(e) What is the third moment about the mean implied by
the EDF? Based on this value, would you conclude
the PDF of screen lifetimes is symmetric probability
distribution?
(f) Based on the EDF, estimate the probability that the
screen will fail in 1 year if the screen were operating
24 hours/day.
(g) The anticipated useful life of the smartphone itself is
ﬁve calendar years. If the screen is used for 4 hours
per day, use the EDF to estimate the probability that
the screen will not fail during the useful life of the
smartphone.
26. The following outcomes were from a random sample
of size 25 from the joint distribution of (Y,X), where Y
denotes yield, in bushels per acre, of a new variety of
overlineley and X denotes average inches of rainfall dur-
ing the growing season:
Y
X
Y
X
81.828
26.195
77.903
17.091
71.305
17.102
79.645
17.867
75.232
21.629
72.966
19.094
75.936
17.553
74.051
21.304
74.377
24.760
78.050
24.847
77.149
22.788
74.878
23.053
81.959
25.566
68.752
13.607
75.094
19.819
71.925
21.738
81.166
21.407
76.299
21.829
77.723
19.190
83.792
35.889
72.750
18.410
73.022
23.079
75.413
26.478
78.167
21.155
73.079
19.744
(a) Deﬁne the joint EDF for the underlying joint CDF of
yield and rainfall.
(b) Deﬁne the marginal EDF for the underlying CDF of
yield.
(c) Deﬁne the sample covariance and sample correlation
between yield and rainfall based on the EDF you
deﬁned in (a).
(d) Treating the EDF as the true CDF of (Y,X), estimate
the probability that yield will be greater than 75
bushels per acres.
(e) Use the EDF to estimate the probability that yield
will be greater than 75 bushels per acre, given that
rainfall will be less than 20 inches.
(f) Use the EDF to estimate the expected yield.
(g) Use the EDF to estimate the expected yield, given
that rainfall will be less than 20 inches.
27. The following 20 iid random variable outcomes arose
from some unknown cumulative distribution FðtÞ:
2
1
3
2
1
2
1
3
4
1
2
4
2
2
1
1
3
2
2
1
(a) Deﬁne the EDF for the random variable.
Problems
359

(b) Deﬁne the range of the random variable together with
the probability density function that is implied by the
EDF.
(c) Use the probability density function you derived in
(b) to deﬁne the mean and the variance of the random
variable.
(d) Now ignore that there are “repeated observations” for
some of the numbers in the table above and assign an
equal weight of 1/20 to each of the 20 numbers.
Repeat the calculation of the mean and the variance
of the random variable acting as if each of the 20 num-
bers above are assigned 1/20 probability. Are the cal-
culated mean and variance the same values? Should
they be? Explain.
In the next FOUR questions below, use the following iid
random variable outcomes from a Uniform(0,1) probabil-
ity distribution:
0.2957
0.3566
0.8495
0.5281
0.0914
0.5980
0.4194
0.9722
0.7313
0.1020
0.5151
0.6369
0.7888
0.9893
0.1252
0.6362
0.1392
0.1510
0.4202
0.2946
0.1493
0.0565
0.4959
0.8899
0.6343
28. The number of customers entering the lobby of a
bank in any 5 minute interval during the lunch hour
from noon until 1 p.m. follows a Poisson process,
Poisson(10).
(a) Using the uniform random outcomes above row-
wise, simulate the outcomes of the number of cus-
tomer arrivals for the 12 5-minute intervals between
noon and 1 p.m.
(b) Calculate the sample mean and variance, and com-
pare them to the true mean and variance.
29. The operating time until failure, in 100,000 hour
units, of a hard disk that your company manufactures
follows an exponential distribution, Exponential(2.5).
(a) Simulate time-until-failure outcomes of 25 of your
hard disks.
(b) Calculate the sample mean and variance, and com-
pare them to the true mean and variance.
30. A ground-fault-protection circuit breaker is designed
to “trip” and interrupt power whenever a ground fault is
detected in a power line to which it attached. After it
trips, it designed to be able to “reset”, and restore power
when a reset button is pressed. For a certain brand of
circuit breaker, the number of resets that occur to get a
reset failure, at which the breaker is no longer functional,
can be viewed as the outcome of a geometric probability
distribution, Geometric(.1).
(a) Simulate 25 observations on the number of times this
brand of circuit breaker is reset,ending in a reset failure.
(b) Calculate the sample mean and variance, and com-
pare them to the true mean and variance.
31. The probability density function relating to the num-
ber of seconds that individuals remain on a website until
they leave is given by the Pareto distribution,
fðxÞ ¼
aba
xaþ1
0
 
!
for x
>



b; a ¼ 2 and b ¼ 5:
(a) Simulate observations on the number of seconds that
each of 25 visitors spends visiting the website.
(b) Calculate the sample mean and variance. Compare
them to the true mean and variance, if they exist.
32. The
monthly
production
of
beer
in
a
large
Midwestern brewery can be viewed as the outcome of
the following production process:
Q ¼ 100l:25k:5eV and V  N 0; :04
ð
Þ;
where Q is measured in 1,000’s of gallons, l is units of
labor and k represents units of capital applied in
production.
(a) Derive the probability density function of the random
variable W ¼ eV (see the note below regarding this
distribution).
(b) Derive the probability density function of quantity
produced, Q.
(c) Derive the expected quantity of beer produced as a
function of the levels of labor and capital applied.
(d) What is the median level of production, as a function
of labor and capital?
(e) If 16 units of labor and four units of capital are
applied, what are the mean and median levels of
production?
(f) At the levels of labor and capital deﬁned in part (e),
what is the standard deviation of production?
(g) At the levels of labor and capital deﬁned in part (e),
what
is
the
probability
that
greater
than
425,000 gallons of beer will be produced?
360
Chapter 6
Sampling, Sample Moments and Sampling Distributions

NOTE: What you will have derived in part (a) is a member
of the log-normal probability distribution family. (See
some general properties listed in problem 9(a)).
33. The monthly wholesale price and quantity sold of
fresh squeezed orange juice during the winter months by
a small Florida growers’ cooperative is represented by the
following bivariate probability density function:
F p; q
ð
Þ ¼ :01pe:01pqI 1:50;2:50
½
ðpÞI 0;1
½
ÞðqÞ
where p is measured in dollars per gallon and
q
is
measured in 1,000’s of gallons.
(a) Derive the probability density function of total reve-
nue, R ¼ PQ.
(b) What is the expected value of total monthly revenue?
(c) What is the probability that total monthly revenue
exceeds $100,000?
Problems
361

7
n
Point Estimation Theory
n
n
n
7.1
Parametric, Semiparametric, and Nonparametric
Estimation Problems
7.2
Additional Considerations for the Speciﬁcation
and Estimation of Probability Models
7.3
Estimators and Estimator Properties
7.4
Sufﬁcient Statistics
7.5
Minimum Variance Unbiased Estimation
The problem of point estimation examined in this chapter
is concerned with the estimation of the values of unknown parameters, or
functions of parameters, that represent characteristics of interest relating to a
probability model of some collection of economic, sociological, biological, or
physical experiments. The outcomes generated by the collection of experiments
are assumed to be outcomes of a random sample with some joint probability
density function fðx1; . . . ; xn; QÞ. The random sample need not be from a popula-
tion distribution, so that it is not necessary that X1,. . .,Xn be iid. The estimation
concepts we will examine in this chapter can be applied to the case of general
random sampling, as well as simple random sampling and random sampling
with replacement, i.e., all of the random sampling types discussed in Chapter 6.
The objective of point estimation will be to utilize functions of the random
sample outcome to generate good (in some sense) estimates of the unknown
characteristics of interest.
7.1
Parametric, Semiparametric, and Nonparametric Estimation Problems
The types of estimation problems that will be examined in this (and the next)
chapter are problems of parametric estimation and semiparametric estimation,
as opposed to nonparametric estimation problems. Both parametric and
semiparametric estimation problems are concerned with the estimates of the
values of unknown parameters that characterize parametric probability models
or semiparametric probability models of the population, process, or general

experiments under study. Both of these models have speciﬁc parametric func-
tional structure to them that becomes ﬁxed and known once values of
parameters are numerically speciﬁed. The difference between the two models
lies
in
whether
a
particular
parametric
family
or
class
of
probability
distributions underlies the probability model and is fully determined by setting
the values of parameters (the parametric model) or not (the semiparametric
model). A nonparametric probability model is a model that is devoid of any
speciﬁc parametric functional structure that becomes ﬁxed when parameter
values are speciﬁed. We discuss these models in more detail below.
Given the prominence of parameters in the estimation problems we will be
examining, and the need to distinguish their appearance and effect in
specifying parametric, semiparametric, and nonparametric probability models,
we extend the scope of the term probability model to explicitly encompass the
deﬁnition of parameters and their admissible values. Note, because it is possi-
ble that the range of the random variable can change with changing values of
the parameter vector for certain speciﬁcation of the joint probability density
function of a random variable (e.g, a uniform distribution), we emphasize this
in the deﬁnition below by including the parameter vector in the deﬁnition of
the range of X.
Deﬁnition 7.1
Probability Model
A probability model for the random variable X is deﬁned by the set
R X; Q
ð
Þ; f x; Q
ð
Þ; Q 2 O
f
g , where O deﬁnes the admissible values of the
parameter vector Q.
In the context of point estimation problems, and later hypothesis testing and
conﬁdence interval estimation problems, X will refer to a random sample relating
to some population, process, or general set of experiments having characteristics
that are the interest of estimation, and f x; Q
ð
Þ will be the joint probability density
function of the random sample. In our study, the parameter space O will generally
represent all of the values of Q for which f x; Q
ð
Þ is a legitimate PDF, and thus
represents all of the possible values for the unknowns one may be interested in
estimating. The objective of point estimation is to increase knowledge ofQbeyond
simply knowing all of its admissible values. It can be the case that prior knowledge
exists regarding the valuesQcan assume in a given empirical application, in which
case O can be speciﬁed to incorporate that knowledge.
We note, given our convention that the range and the support of the random
variable X are equivalent (recall Deﬁnition 2.13), that explicitly listing the range
of the random variable as part of the speciﬁcation of the probability model does
not provide new information, per se. That is, knowing the density function
and its admissible parameter values implies the range of the random variable
asR X; Q
ð
Þ  x : f x; Q
ð
Þ>0
f
gforQ 2 O. We will see ahead that in point estimation
problems an explicit speciﬁcation of the range of a random sample X is impor-
tant for a number of reasons, including determining the types of estimation
procedures that can be used in a given estimation problem, and for deﬁning
364
Chapter 7
Point Estimation Theory

the range of estimates that are possible to generate from a particular point
estimator speciﬁcation. We will therefore continue to explicitly include the
range of X in our speciﬁcation of a probability model, but we will reserve the
option to specify the probability model in the abbreviated form f x; Q
ð
Þ; Q 2 O
f
g
when emphasizing the range of the random variable is not germane to the
discussion.
Deﬁnition 7.2
Probability Model:
Abbreviated Notation
An abbreviated notation for the probability model of random variable X is
f x; Q
ð
Þ; Q 2 O
f
g, where R X; Q
ð
Þ  x : f x; Q
ð
Þ>0
f
g is taken as implicit in the
deﬁnition of the model.
7.1.1
Parametric Models
A parametric model is one in which the functional form of the joint probability
density function, fðx1; . . . ; xn; QÞ , contained in the probability model for the
observed sample data, x, is fully speciﬁed and known once the value of the
parameter vector, Q, is given a speciﬁc numerical value. In specifying such a
model, the analyst deﬁnes a collection of explicit parametric functional forms for
the joint density of the random sample X, as fðx1; . . . ; xn; QÞ; for Q 2 O, with the
implication that if the appropriate value of the parameter vector, say Q0, were
known, then fðx1; . . . ; xn; Q0Þ would represent true probability density function
underlying the observed outcome x of the random sample. We note that in
applications the analyst may not feel fully conﬁdent in the speciﬁcation of the
probability model
fðx; QÞ; Q 2 O
f
g, and view it as a tentative working model,
in which case the adequacy of the model may itself be an issue in need of further
statistical analysis and testing. However, use of parametric estimation method-
ology begins with, and indeed requires such a full speciﬁcation of a parametric
model for X.
7.1.2
Semiparametric Models
A semiparametric model is one in which the functional form of the joint
probability density function component of the probability model for the
observed sample data, x, is not fully speciﬁed and is not known when the
value of the parameter vector of the model, Q, is given a speciﬁc numerical
value. Instead of deﬁning a collection of explicit parametric functional forms for
the joint density of the random sample X, when deﬁning the model, as in the
parametric case, the analyst deﬁnes a number of properties that the underlying
true sampling density fðx1; . . . ; xn; Q0Þ is thought to possess. Such information
could include parametric speciﬁcations for some of the moments that the ran-
dom variables are thought to adhere to, or whether the random variables
contained in the random sample exhibit independence or not. Given a numerical
value for the parameter vector Q, any parametric structural components of the
model are given an explicit fully speciﬁed functional form, but other
components of the model, most notably the underlying joint density function
7.1
Parametric, Semiparametric, and Nonparametric Estimation Problems
365

for the random sample,
fðx1; . . . ; xn; QÞ , remains unknown and not fully
speciﬁed.
7.1.3
Nonparametric Models
A nonparametric model is one in which neither the functional form of the joint
probability density function component of the probability model for the
observed sample data, x, nor any other parametric functional component of the
probability model is deﬁned and known given numerical values of parameters Q.
These models proceed with minimal assumptions on the structure of the proba-
bility model, with the analyst simply acknowledging the existence of some
general characteristics and relationships relating to the random variables in
the random sample, such as the existence of a general regression relationship,
or the existence of a population probability distribution if the sample were
generated through simple random sampling.
For example, the analyst may wish to estimate the CDF F(z), where (X1,. . .,
Xn) is an iid random sample from the population distribution F(z), and no
mention is made, nor required, regarding parameters of the CDF. We have
already examined a method for estimating the CDF in the case where the
random sample is from a population distribution, namely, the empirical distri-
bution function, Fn, provides an estimate of F. We will leave the general study of
nonparametric estimation to a more advanced course of study; interested readers
can refer to M. Puri and P. Sen (1985) Nonparametric Methods in General Linear
Models. New York: John Wiley, F. Hampel, E. Ronchetti, P. Rousseeuw, and
W. Stahel (1986), Robust Statistics. New York: John Wiley; and J. Pratt and
J. Gibbons (1981), Concepts of Nonparametric Theory. New York: Springer-
Verlag and A. Pagan and A. Ullah, (1999), Nonparametric Econometrics,
Cambridge: Cambridge University Press.1
We illustrate the deﬁnition of the above three types of models in the follow-
ing example.
Example 7.1
Parametric,
Semiparametric,
and Nonparametric
Models of Regression
Consider the speciﬁcation of a probability model underlying a relationship
between a given n  1 vector of values z, and corresponding outcomes on the
n  1 random vector X, where the n elements in X are assumed to be indepen-
dent random variables.
For a parametric model speciﬁcation of the relationship, let the probability
model R X; Q
ð
Þ; f x; Q
ð
Þ; Q 2 O
f
g be deﬁned by X ¼ b1 þ zb2 þ «
and
1There is not universal agreement on the meaning of the terms parametric, nonparametric, and distribution-free. Sometimes
nonparametric and distribution-free are used synonymously, although the case of distribution-free parametric estimation is pervasive
in econometric work. See J.D. Gibbons, (1982), Encyclopedia of Statistical Sciences, Vol. 4. New York: Wiley, pp. 400–401.
366
Chapter 7
Point Estimation Theory

« 
Y
n
i¼1
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ps2
p
exp  1
2
e2
i
s2


for b ∈ℝ2 and s2 > 0, where then R X; b; s2


¼ R X
ð Þ ¼ ℝn for all admissible
parameter values. In this case, if the parameters b and s2 are given speciﬁc
numerical values, the joint density of the random sample is fully deﬁned and
known. Speciﬁcally, Xi  N b10 þ zib20; s2
0


; i ¼ 1; . . . ; n for given values of b0
and s2
0 . Moreover, the parametric structure for the mean of X is then fully
speciﬁed and known, asE Xi
ð
Þ ¼ b10 þ zib20; i ¼ 1; . . . ; n. Given the fully speciﬁed
probability model, the analyst would be able to fully and accurately emulate the
random sampling of X implied by the above fully speciﬁed probability model.
For a semiparametric model speciﬁcation of the relationship, the probability
model R X; Q
ð
Þ; f x; Q
ð
Þ; Q 2 O
f
g will not be fully functionally speciﬁed. For this
type of model, the analyst might specify that X ¼ b1 þ zb2 þ « with E «
ð Þ ¼ 0 and
Cov «
ð Þ ¼ s2I, so that the ﬁrst and second moments of the relationship have been
deﬁned as E X
ð Þ ¼ b11n þ zb2; i ¼ 1; . . . ; n and Cov X
ð Þ ¼ s2I. In this case, know-
ing the numerical values of b and s2 will fully identify the means of the random
variables as well as the variances, but the joint density of the random sample
will remain unknown and not fully speciﬁed. It would not be possible for the
analyst to simulate random sampling of X given this incomplete speciﬁcation of
the probability model.
Finally, consider a nonparametric model of the relationship. In this case, the
analyst might specify that X ¼ g z
ð Þ þ « with E X
ð Þ ¼ g z
ð Þ, and perhaps that X is a
collection of independent random variables, but nothing more. Thus, the mean
function, as well as all other aspects of the relationship between X and z, are left
completely general, and nothing is explicitly determined given numerical values
of parameters. There is clearly insufﬁcient information for the analyst to simu-
late random sample outcomes from the probability model, not knowing the joint
density of the random sample or even any moment aspects of the model, given
values of parameters.
□
7.1.4
Scope of Parameter Estimation Problems
The objective in problems of parameter estimation is to utilize a sample
outcome x1; . . . ; xn
½
0 of X ¼ X1; . . . ; Xn
½
0 to estimate the unknown value Q0 or
q Q0
ð
Þ, where Q0 denotes the value of the parameter vector associated with the
joint PDF that actually determines the probabilities of events for the random
sample outcome. That is, Q0 is the value of Q such that X ~ f(x;Q0) is a true
statement, and for this reason Q0 is oftentimes referred to as the true value of Q,
and we can then also speak of q Q0
ð
Þ as being the true value of q Q
ð
Þ and f(x; Q0)
as being the true PDF of X. Some examples of the many functions of Q0
that might be of interest when sampling from a distribution f(z;Q0) include
1. q1(Q0) ¼ E(Z) ¼
R 1
1 z f(z; Q0)dz (mean),
2. q2(Q0) ¼ E(Z – E(Z))2 ¼
R 1
1 (z – E(Z))2 f(z; Q0)dz (variance),
7.1
Parametric, Semiparametric, and Nonparametric Estimation Problems
367

3. q3(Q0) deﬁned implicitly by
R q3 ðy0Þ
1
f(z; Q0)dz ¼ .5 (median),
4. q4(Q0) ¼
R b
af(z; Q0)dz ¼ P(z∈[a,b]) (probabilities)
5. q5(Q0) ¼
R 1
1z f(z |x; Q0)dz ¼ (regression function of z on x)
The method used to solve a parametric estimation problem will generally
depend on the degree of speciﬁcity with which one can deﬁne the family of
candidates for the true PDF of the random sample, X. The situation for which
the most statistical theory has been developed, both in terms of the actual
procedures used to generate point estimates and in terms of the evaluation of
the properties of the procedures, is the parametric model case. In this case, the
density function candidates, f(x1,. . .,xn; Q), are assumed at the outset to belong to
speciﬁc parametric families of PDFs (e.g., normal, Gamma, binomial), and
application of the celebrated maximum likelihood estimation procedure
(presented in Chapter 8) relies on the candidates for the distribution of X being
members of a speciﬁc collection of density functions that are indexed, and fully
algebraically speciﬁed, by the values of Q.
In the semiparametric model and nonparametric model cases, a speciﬁc
functional deﬁnition of the potential PDFs for X is not assumed, although
some assumptions about the lower-order moments of f(x;Q) are often made. In
any case, it is often still possible to generate useful point estimates of various
characteristics of the probability model of X that are conceptually functions of
parameters, such as moments, quantiles, and probabilities, even if the speciﬁc
parametric family of PDFs for X is not speciﬁed. For example, useful point
estimates (in a number of respects) of the parameters in the so-called general
linear model representation of a random sample based on general random sam-
pling designs can be made, and with only a few general assumptions regarding
the lower-order moments of f(x1,. . .,xn;Q), and without any assumptions that the
density is of a speciﬁc parametric form (see Section 8.2).
Semiparametric and nonparametric methods of estimation have an advan-
tage of being applicable to a wide range of sampling distributions since they are
deﬁned in a distribution-nonspeciﬁc context that inherently subsumes many
different functional forms for f(x;Q). However, it is usually the case that superior
methods of estimating Q or q(Q) exist if a parametric family of PDFs for X can be
speciﬁed, and if the actual sampling distribution of the random sample is
subsumed by the probability model. Put another way, the more (correct) infor-
mation one has about the form of f(x;Q) at the outset, the more precisely one can
estimate Q0 or q(Q0).
7.2
Additional Considerations for the Speciﬁcation and Estimation of Probability Models
A problem of point estimation begins with either a fully or partially speciﬁed
probability model for the random sample X ¼ (X1,. . .,Xn)0 whose outcome x ¼
[x1,. . .,xn]0 constitutes the observed data being analyzed in a real-world problem
368
Chapter 7
Point Estimation Theory

of point estimation, or statistical inference. The probability model deﬁnes the
probabilistic and parametric context in which point estimation proceeds. Once
the probability model has been speciﬁed, interest centers on estimating the true
values of some (or all) of the parameters, or on estimating the true values of
some functions of the parameters of the problem. The speciﬁc objectives of any
point estimation problem depend on the needs of the researcher, who will
identify which quantities are to be estimated.
The case of parametric model estimation of Q or q(Q) is associated with a
fully speciﬁed probability model in which a speciﬁc parametric family of PDFs is
represented by {f(x;Q), Q∈O}. For example, a fully speciﬁed probability model for
a random sample of miles per gallon achieved by 25 randomly chosen trucks
from the assembly line of a Detroit manufacturer might be deﬁned as
Q25
i¼1 Nðxi; m; s2Þ; ðm; s2Þ 2 O
n
o
, where O ¼ (0,1)(0,1).
In the semiparametric model case, a speciﬁc functional form for f(x; Q) is not
deﬁned and O may or may not be fully speciﬁed. For example, in the preceding
truck mileage example, a partially speciﬁed statistical model would be {f(x;m,s2),
(m,s2)∈O}, where O ¼ (0,1)(0,1) and f(x;m,s2) is some continuous PDF. In this
latter case, the statistical model allows for the possibility that f(x;m,s2) is any
continuous PDF having a mean of m and variance of s2, with both m and s2 posi-
tive, e.g., normal, Gamma, or uniform PDFs would be potential candidates.
7.2.1
Specifying a Parametric Functional Form for the Sampling Distribution
In specifying a probability model, the researcher presumably attempts to iden-
tify an appropriate parametric family based on a combination of experience,
consideration of the real-world characteristics of the experiments involved,
theoretical considerations, past analyses of similar problems, an attempt at a
reasonably robust approximation to the probability distribution, and/or pragma-
tism. The degree of detail with which the parametric family of densities is
speciﬁed can vary from problem to problem.
In some situations there will be great conﬁdence in a detailed choice of
parametric family. For example, suppose we are interested in estimating the
proportion, p, of defective manufactured items in a shipment of N items. If a
random sample with replacement of size n is taken from the shipment (popula-
tion) of manufactured items, then
ðX1; . . . ; XnÞ  fðx1; . . . ; xn; pÞ ¼ p
Pn
i¼1 xið1  pÞnPn
i¼1 xi Y
n
i¼1
I 0;1
f
gðxiÞ
represents the parametric family of densities characterizing the joint density of
the random sample, and interest centers on estimating the unknown value of the
parameter p.
On the other hand, there will be situations in which the speciﬁcation of the
parametric family is quite tentative. For example, suppose one were interested
in estimating the average operating life of a certain brand of hard-disk based on
outcomes of a random sample of hard-disk lifetimes. In order to add some
7.2
Additional Considerations for the Speciﬁcation and Estimation of. . .
369

mathematical structure to the estimation problem, one might represent the ith
random variable in the random sample of lifetimes (X1,. . .,Xn) as Xi ¼ m + Vi,
where m represents the unknown mean of the population distribution of
lifetimes, an outcome of Xi represents the actual lifetime observed for the ith
hard disk sampled, and the corresponding outcome of Vi represents the deviation
of Xi from m. Since (X1,. . .,Xn) is a random sample from the population distribu-
tion, it follows that E(Xi)¼ m and var(Xi) ¼ s2 8i can be assumed, so that E(Vi) ¼ 0
and var(Vi) ¼ s2 8i can also be assumed. Moreover, it is then legitimate to
assume that (X1,. . .,Xn) and (V1,. . .,Vn) are each a collection of iid random
variables. Then to this point, we have already speciﬁed that the parametric
family of distributions associated with X is of the form Qn
i¼1 m xi; Q
ð
Þ, where
the density m(z; Q ) has mean m and variance s2 (what is the corresponding
speciﬁcation for V?).
Now, what parametric functional speciﬁcation ofm xi; Q
ð
Þcan be assumed to
contain the speciﬁc density that represents the actual probability distribution of
Xi or Vi? (Note, of course, that specifying a parametric family for Vi would imply
a corresponding parametric family for Xi and vice versa). One general speciﬁca-
tion would be the collection of all continuous joint density functions f(x1,. . .,xn;
Q) for which f(x1,. . .,xn;Q) ¼ Qn
i¼1 m xi; Q
ð
Þ with E(Xi)¼ m and var(Xi) ¼ s2, 8i.
The advantage of such a general speciﬁcation of density family is that we have
great conﬁdence that the actual density function of X is contained within the
implied set of potential PDFs, which we will come to see as an important
component of the speciﬁcation of any point estimation problem. In this particu-
lar case, the general speciﬁcation of the statistical model actually provides
sufﬁcient structure to the point estimation problem for a useful estimate of
mean lifetime to be generated (for example, the least squares estimator can be
used to estimate m – see Chapter 8). We will see that one disadvantage of very
general speciﬁcations of the probability model is that the interpretation of the
properties of point estimates generated in such a general context is also usually
not as speciﬁc or detailed as when the density family can be deﬁned with greater
speciﬁcity.
Consider a more detailed speciﬁcation of the probability model of hard-disk
operating lives. If we feel that lifetimes are symmetrically distributed around
some point, m, with the likelihoods of lifetimes declining the more distant the
measurement is from m, we might consider the normal parametric family for the
distribution of Vi. It would, of course, follow that Xi is then also normally
distributed, and thus the normal distribution could serve only as an approxima-
tion since negative lifetimes are impossible. Alternatively, if we felt that the
distribution of lifetimes was skewed to the right, the gamma parametric family
provides a rich source of density shapes, and we might specify that the Xi’s have
some Gamma density, and thus the Vi’s would have the density of a
Gamma–type random variable that has been shifted to the left by m units.
Hopefully, the engineering staff could provide some guidance regarding the
most defensible parametric family speciﬁcation to adopt. In cases where there
is considerable doubt concerning the appropriate parametric family of densities,
370
Chapter 7
Point Estimation Theory

tests of hypotheses concerning the adequacy of a given parametric family
speciﬁcation can be performed. Some such tests will be discussed in
Chapter 10. In some problem situations, it may not be possible to provide any
more than a general speciﬁcation of the density family, in which case the use of
semiparametric methods of parameter estimation will be necessary.
7.2.2
The Parameter Space for the Probability Model
Given that a parametric functional form is speciﬁed to characterize the joint
density of the random sample, a parameter space, O, must also be identiﬁed to
complete the probability model. There are often natural choices for the para-
meter space. For example, if the Bernoulli family were speciﬁed, then O ¼
{p: p ∈[0,1]}, or if the normal family were speciﬁed, then O ¼ {(m,s): m∈(1,1),
s > 0}. However, if only a general deﬁnition of the parametric family of densities
is speciﬁed at the outset of the point estimation problem, the speciﬁcation of the
parameter space for the parametric family will then also be general and often
incomplete. For example, a parameter space speciﬁcation for the aforemen-
tioned point estimation problem involving hard-disk lifetimes could be
O ¼ {Yo: m  0, s2  0}. In this case, since the speciﬁc algebraic form of f(x1,. . .,
xn; Q) is also not speciﬁed, we can only state that the mean and variance of hard-
disk lifetimes are nonnegative, possibly leaving other unknown parameters in
Yo unrestricted depending on the relationship of the mean and variance to the
parameters of the distribution, and in any case not fully specifying the func-
tional form of the density function. Regardless of the level of detail with which
O is speciﬁed, there are two important assumptions, presented ahead, regarding
the speciﬁcation of O that are made in the context of a point estimation problem.
The Issue of Truth in the Parameter Space
First, it is assumed that O contains the
true value of Q0, so that the probability model given by {f(x; Q), Q∈O} can be
assumed to contain the true sampling distribution for the random sample under
study. Put another way, in the context of a point estimation problem, the set O is
assumed to represent the entire collection of possible values for Q0. The rele-
vance of this assumption in the context of point estimation is perhaps obvious –
if the objective in point estimation is to estimate the value of Q0 or q(Q0) we do
not want to precludeQ0 or q(Q0) from the set of potential estimates. Note that, in
practice, this may be a tentative assumption that is subjected to statistical test
for veriﬁcation or refutation (see Chapter 10).
Identiﬁability of Parameters
The second assumption on O concerns the concept
of the identiﬁability of the parameter vector Q . As we alluded to in our
discussion of parametric families of densities in Chapter 4, parameterization of
density families is not unique. Any invertible transformation of Q, say l ¼ h(Q),
deﬁnes an alternative parameter space L ¼ {l: l ¼ h( Q ), Q ∈O} that can be
used to specify an alternative probability model for X that contains the same
PDF candidates as the statistical model based on O, i.e.,ffðx; h1ðlÞÞ; l 2 Lg ¼
ffðx; QÞ; Q 2 Og. Deﬁning m(x;l)  f(x;h1(l)), the alternative probability model
7.2
Additional Considerations for the Speciﬁcation and Estimation of. . .
371

could be written as {m(x;l), l∈L}. The analyst is free to choose whatever
parameterization appears to be most natural or useful in the speciﬁcation of a
probability model, so long as the parameters in the chosen parameterization are
identiﬁed. In stating the deﬁnition of parameter identiﬁability we use the
terminology distinct PDFs to refer to PDFs that assign different probabilities
to at least one event for X.
Deﬁnition 7.3
Parameter
Identiﬁability
Let {f(x; Q), Q∈O} be a probability model for the random sample X. The
parameter vector Q is said to be identiﬁed or identiﬁable iff 8 Q1 and Q2 ∈O,
f(x; Q1) and f(x;Q2) are distinct if Q1 6¼ Q2.
The importance of parameter identiﬁability is related to the ability of ran-
dom sample outcomes to provide discriminatory information regarding the
choice of Q∈O to be used in estimating Q0. If the parameter vector in a statistical
model is not identiﬁed, then two or more different values of the parameter
vector Q, say Q 1 and Q 2, are associated with precisely the same sampling
distribution X. In this event, random sample outcomes cannot possibly be
used to discriminate between the values of Q1 and Q2 since the probabilistic
behavior of X under either possibility is indistinguishable. We thus insist on
parameter identiﬁability in a point estimation problem so that different values
of Q are associated with different probabilistic behavior of the outcomes of the
random sample.
Example 7.2
Parameter
Identiﬁability
The yield per acre of tomatoes on 20 geographically dispersed parcels of irrigated
land is thought to be representable as the outcomes of Yi ¼ b0 + b1Ti + Vi, i ¼
1,. . .,20, where b0 and b1 are > 0, and (Ti, Vi), i ¼ 1,. . .,20, are iid outcomes of a
bivariate normal population distribution with mean vector [mT, 0] for mT > 0, and
diagonal covariance matrix with diagonal entries s2
T and s2
V . The outcome
yi represents bushels/acre on parcel i, and ti is season average temperature
measured at the growing site. If the probability model for the random sample
Y ¼ [Y1,. . .,Y20]0 is speciﬁed as
Y
20
i¼1
N yi; b0 þ b1mT; b2
1 s2
T þ s2
V


; for b0; b1; mT; s2
T; s2
v


2 O
(
)
where O ¼ 5
i¼1 (0, 1), is the parameter vector [b0, b1, mT, s2
T, s2
V] identiﬁed?
Answer: Deﬁne m ¼ b0 + b1mT and s2 ¼ b2
1 s2
T þ s2
V , and examine the probabil-
ity model for Y given by
Y
20
i¼1
Nðyi; m; s2Þ; ðm; s2Þ 2 L
(
)
;
where L ¼ 2
i¼1 0; 1
ð
Þ. Note that any choice of positive values for b0, b1, mT, s2
T,
ands2
V that result in the same given positive values for m and s2 result in precisely
the same sampling distribution for Y (there are an inﬁnite set of such choices for
each value of the vector [m,s2]0). Thus the original parameter vector is not
identiﬁed. Note that the parameter vector [m,s2]0 in the latter statistical model
for Y is identiﬁed since the sampling distributions associated with two different
positive values of the vector [m,s2]0 are distinct.
□
372
Chapter 7
Point Estimation Theory

7.2.3
A Word on Estimation Phraseology
We pause here to introduce a convention regarding the interpretation of phrases
such as estimating Q or estimating q(Q), or an estimate of Q (or of q(Q)). Since Q is
simply a parameter vector that indexes a family of density functions and that
can assume a range of alternative values (those speciﬁed in O), the reader might
wonder what such phrases could possibly mean? That is, what are we estimating
if we are estimating, say, Q ? The phrases are used as a shorthand or an
abbreviated way of stating that one is estimating the true value of Q , or
estimating the true value of q(Y), or that one has an estimate of the true value
of Q (or of q(Q)). There is widespread use of such phrases in the statistics and
econometrics literature, and we will make frequent use of such phrases in this
book as well. In general, one must rely on the context of the discussion to be sure
whether Q or q( Q ) refers to the quantity being estimated or merely to the
indexing parameter of a family of joint density functions.
7.3
Estimators and Estimator Properties
Point estimation is concerned with estimating Q or q(Q) from knowledge of the
outcome x ¼ x1; . . . ; xn
½
0 of a random sample X. It follows from this basic
description of point estimation that functions are critical to the estimation
problem, where inputs or domain elements are sample outcomes, x, and outputs
or range elements are estimates of Q or q(Q). More formally, estimates will be
generated via some function of the form t: R(X) ! R(t), where R(t) is the range of
t deﬁned as R(t) ¼ {t: t ¼ t(x), x∈R(X)}. Note that R(t) represents the set of all
possible estimates of Q or q(Q) that can be generated as outcomes of t(X). We will
always tacitly assume that t(X) is an observable random variable, and hence a
statistic, so that estimates are observable and empirically informative.
Henceforth, when the function t: R(X) ! R(t) represented by t ¼ t(x) is being
utilized to generate estimates of q(Q), we will refer to the random variable T ¼
t(X) as an estimator for q( Q ), and q( Q ) will be referred to as the estimand.
An outcome, t ¼ t(x), of the estimator will be referred to as an estimate of q(Q).
We formalize these three terms in the following deﬁnition:
Deﬁnition 7.4
Point Estimator,
Estimate, and Estimand
A statistic or vector of statistics, T ¼ t(X), whose outcomes are used to
estimate the value of a scalar or vector function, q(Q), of the parameter vector,
Q , is called a point estimator, with q( Q ) being called the estimand.2
An observed outcome of an estimator is called a point estimate.
2Note, as always, that the function q can be the identity function q(Q) Q, in which case we could be referring to estimating the vector
Q itself. Henceforth, it will be understood that since q(Q)  Q is a possible choice of q(Q), all discussion of estimating q(Q) could be
referring to estimating the vector Q itself.
7.3
Estimators and Estimator Properties
373

Figure 7.1 contains a schematic overview of the general context of the point
estimation problem to this point.
7.3.1
Evaluating Performance of Estimators
Since there is literally an uncountably inﬁnite set of possible functions of X that
are potential estimators of q(Q), a fundamental problem in point estimation is
the choice of a “good” estimator. In order to rank the efﬁcacy of estimators and/
or to choose the optimal estimator of q(Q), an objective function that establishes
an appropriate measure of “goodness” must be deﬁned.
A natural measure to use in ranking estimators would seem to be the
distance between outcomes of t(X) and q(Q), which is a direct measure of
how close estimates are to what is being estimated. In the current context,
this distance measure is d(t(x), q(Q)) ¼ ([t(x)  q(Q)]0 [t(x)  qðQ)])1/2, which
specializes to |t(x) - q(Q)| when k ¼ 1. However, this closeness measure has an
obvious practical ﬂaw for comparing alternative functions for estimating q(Q) –
the estimate that would be preferred depends on the true value of q(Q), which is
unknown (or else there would be no point estimation problem in the ﬁrst
place). This problem is clearly not the fault of the particular closeness measure
chosen since any reasonable measure of closeness between the two values t(x)
and q(Q) would depend on where q(Q) actually is in ℝk vis-a-vis where t(x) is
located. Thus, comparing alternative functions for estimating q(Q) on the basis
of the closeness to q(Q) to an actual estimate t(x) is not tractable — we clearly
need additional criteria with which to judge whether t(X) generates “good”
estimates of q(Q).
Specify Probability Model: { f (x1,..., xn; Q),Q ∈Ω}
Observe Outcome of Random Sample:
( X1,...,Xn)~f (x1,..., xn ; Q) for some Q ∈Ω
Specify Characteristics of interest: q(Q)
Define Estimator: T=t(X) of q(Q)
Use t = t(x) to estimate q(Q)
Figure 7.1
General point estimation
procedure.
374
Chapter 7
Point Estimation Theory

Various criteria for judging the usefulness of a given estimator t(X) for
estimating q(Q) have been presented in the literature.3 The measures evaluate
and rank estimators in terms of closeness of estimates to q(Q) in an expected or
probabilistic sense. Note that since t(X) is a function of X, and thus a random
variable, a sampling distribution (i.e., the probability distribution of t(X)) exists
on R(T) that is induced by the probability distribution of the random sample,
X ¼ (X1,. . .,Xn). Roughly speaking, the fact that the distribution of X depends on
Q will generally result in the sampling distribution of t(X) depending on Q as
well, and this latter dependence can lead to changes in location, spread, and/or
shape of the distribution of t(X) as Q changes. If the sampling distribution of t(X)
changes with Q in a way that keeps the spread of potential estimates generated
by t(X) narrowly focused on q(Q) so that outcomes of t(X) occur near q(Q) with
high probability under all contingencies for Q ∈O, (see Figure 7.2), then the
function T would be useful for generating estimates of q(Q).
We now turn our attention to speciﬁc estimator properties that have been
used in practice to measure whether these objectives have been achieved. In
discussing estimator properties, we will sometimes utilize a Q-subscript such as
EQ ð Þ, PQ ð Þ, or varQ ð Þ to emphasize that expectations or probabilities are being
calculated using a particular value of Q for the parameter vector of the underly-
ing
probability
distribution.
In
cases
where
the
parametric
context
of
expectations and probabilities are clear or does not need to be distinguished,
the subscript Q will not be explicitly displayed.
7.3.2
Finite Sample Properties
The properties examined in this section evaluate the performance of estimators
when the random sample is of ﬁxed size, and they are therefore referred to as
ﬁnite sample properties. This is as opposed to asymptotic properties that we will
examine later in this section, which relate to limiting results that are
established as the random sample size increases without bound (increases to
inﬁnity). All of the ﬁnite sample properties examined here are based on the ﬁrst
two moments of estimators, and thus relate to the central tendency of estimates
as well as the spread of estimates around their central tendency. Of course, if
these moments do not exist for a given estimator, then these ﬁnite sample
properties cannot be used to evaluate the performance of the estimator.
q(Q1)
f(t;Q1)
f(t;Q2)
f(t;Q3)
q(Q2)
q(Q3)
q(q)
Figure 7.2
Scalar estimator PDFs for
various values of Q.
3A concise review and comparison of a number of alternative criteria is given by T. Amemiya (1994), Introduction to Statistics and
Econometrics, Cambridge, MA, Harvard University Press, pp. 118–121.
7.3
Estimators and Estimator Properties
375

Mean Square Error and Relative Efﬁciency
The term mean square error (MSE) is an
alternative term for the expected squared distance between outcomes of an
estimator T ¼ t(X), and what it is estimating, the estimand q(Q). When T and
q(Q) are scalars, the following deﬁnition applies.
Deﬁnition 7.5
Mean Square Error:
Scalar Case
The mean square error of an estimator T of q(Q) is deﬁned as MSEQðTÞ ¼
EQ d2ðT; qðQÞÞ


¼ EQ ðT  qðQÞÞ2


8Q ∈O.
The MSE criterion accounts for both the degree of spread in the sampling
distribution of T as well as the degree to which the central tendency of T ’s
distribution deviates from q(Q). We will make this notion precise upon deﬁning
the concept of bias, as follows.
Deﬁnition 7.6
Estimator Bias
The bias of a scalar estimator T of q(Q) is deﬁned as biasQðTÞ ¼ EQðT  qðQÞÞ;
8Q 2 O: The bias vector of a vector estimator T of q(Q) is deﬁned as BiasQ T
ð Þ
¼ EQðT  qðQÞÞ; 8Q 2 O.
Thus the bias of an estimator is the expected difference between the
outcomes of the estimator and what the estimator is estimating.
The MSE of a scalar estimator T can be decomposed into the sum of the
variance of T and the squared bias of T, as
MSEQðTÞ ¼ EQ
T  EQðTÞ þ EQðTÞ  qðQÞ
ð
Þ2


¼ EQ T  EQðTÞ
ð
Þ2 þ EQðTÞ  qðQÞ
ð
Þ2
¼ varQ ðTÞ þ biasQðTÞ
ð
Þ2:
The MSE criterion thus penalizes an estimator for having a high variance, a high
bias, or both. It also follows that the MSE criterion allows a tradeoff between
variance and bias in the ranking of estimators. In the ﬁnal analysis, it is the
expected squared distance between T and q(Q) implied by varQ(T) and biasQ(T),
and not the variance and bias per se, that determines an estimator’s relative
ranking via the MSE criterion.
In the multivariate case, the MSE criterion is generalized through the use of
the mean square error matrix.
Deﬁnition 7.7
Mean Square Error
Matrix
The mean square error matrix of the estimator T of the (k  1) vector q(Q) is
deﬁned by MSEQ(T)¼EQ ðT  qðQÞÞðT  qðQÞÞ0


8Q 2 O.
To appreciate the information content of MSEQ(T), ﬁrst note that the diagonal
of the MSE matrix contains the MSE of the estimator Ti for qi(Q), i ¼ 1,. . .,k since
the ith diagonal entry in MSEQ(T) is EQ(Tiqi(Q))2. More generally, let c be any
(k  1) vector of constants, and examine the MSE of the linear combination
c0T ¼ Pk
i¼1 ciTi as an estimator of c0qðQÞ ¼ Pk
i¼1 ciqi Q
ð
Þ, deﬁned by
376
Chapter 7
Point Estimation Theory

MSEQ c0T
ð
Þ ¼ EQ
c0T  c0q Q
ð
Þ
ð
Þ2


¼ EQ c0½T  qðQÞ½T  qðQÞ0c


¼ c0MSEQ T
ð Þc:
Thus, the MSEs of every possible linear combination of the Ti’s, used as
estimators of the corresponding linear combination of the qi(Q)’s, can be
obtained from the MSE matrix. Note further that the trace of the MSE matrix
deﬁnes the expected squared distance of the vector estimator T from the vector
estimand q(Q), as
tr MSEY T
ð Þ
ð
Þ ¼ tr EY ½T  qðYÞ½T  qðYÞ0




¼ EY ½T  qðYÞ0½T  qðYÞ


¼ EY d2 ðT; qðYÞÞ


:
This is the direct vector analogue to the measure of closeness of T to q(Q) that is
provided by the MSE criterion in the scalar case.
The MSE matrix can be decomposed into variance and bias components,
analogous to the scalar case. Speciﬁcally, MSE(T) is equal to the sum of the
covariance matrix of T and the outer product of the bias vector of T, as
MSEQ T
ð Þ ¼ EQ T  EQ T
ð Þ þ EQ T
ð Þ  q Q
ð
Þ
½
 T  EQ T
ð Þ þ EQ T
ð Þ  q Q
ð
Þ
½
0


¼ CovQ T
ð Þ þ BiasQ T
ð ÞBiasQ T
ð Þ0:
The outer product of the bias vector forms a (k  k) matrix that is called the bias
matrix.
In the case of a scalar q(Q), estimators with smaller MSEs are preferred. Note,
however, that since the true Q is unknown (or else there is no point estimation
problem to begin with), one must consider the performance of an estimator for all
possible contingencies for the true value of Q, which is to say, for all Q∈O. It is
quite possible, and often the case, that an estimator will have lower MSEs than
another estimator for some values of Q∈O but not for others. These considerations
lead to the concepts of relative efﬁciency and relatively more efﬁcient.
Deﬁnition 7.8
Relative Efﬁciency:
Scalar Case
Let T and T* be two estimators of a scalar q(Q). The relative efﬁciency of T
with respect to T* is given by
REQ ðT; T	Þ ¼ MSEQ ðT	Þ
MSEQ ðTÞ ¼ EQ T	 qðQÞ
ð
Þ2
EQ T  qðQÞ
ð
Þ2 ; 8Q 2 O:
T is relatively more efﬁcient than T* if REQ(T,T*)  1 8Q∈O and > 1 for
some Q∈O.
In comparing two estimators of q(Q), if T is relatively more efﬁcient than T*,
then there is no value of Q for which T* is preferred to Ton the basis of MSE, and
for one or more values of Q, T is preferred to T*. In this case, it is evident that T*
can be discarded as an estimator of q(Q), and in this case, T* is said to be an
inadmissible estimator of q(Q), as deﬁned below.
7.3
Estimators and Estimator Properties
377

Deﬁnition 7.9
Estimator Admissibility
Let T be an estimator of q(Q). If there exists another estimator of q(Q) that is
relatively more efﬁcient than T, then T is called inadmissible for estimating
q(Q). Otherwise, T is called admissible.
It is evident that if one is judging the performance of estimators on the basis
of MSE, the analyst need not consider any estimators that are inadmissible.
Example 7.3
Two Admissible
Estimators for the
Mean of a Bernoulli
Distribution
Suppose (X1,. . .,Xn) is a random sample from a Bernoulli population distribution,
where Xi represents whether (Xi ¼ 1) or not (Xi ¼ 0) the ith customer contacted
by telephone solicitation purchases a product. Consider two estimators for the
unknown proportion, p, of the consumer population who will purchase the
product:
T ¼ X ¼ n1 X
n
i¼1
Xi and
T	 ¼ n þ 1
ð
Þ1 X
n
i¼1
Xi ¼
n
n þ 1


X:
Which estimator, if either, is the preferred estimator of p on the basis of MSE
given that n ¼ 25? Does either estimator render the other inadmissible?
Answer: Note that bias(T) ¼ E(X )  p ¼ 0, bias(T*) ¼ E
n= n þ 1
ð
Þ
ð
ÞX


 p ¼
p/(n þ 1) ¼ p/26,
var(T) ¼ p(1  p)/n ¼ p(1  p)/25,
and
var(T*) ¼
np(1  p)/(n þ 1)2 ¼ p(1  p)/27.04. Then the MSEs of the two estimators are
given by
MSEðTÞ ¼ p 1  p
ð
Þ
25
and
MSE T	
ð
Þ ¼ p 1  p
ð
Þ
27:04
þ p2
676 :
Examine the MSE of T* relative to the MSE of T, as
REp T; T	
ð
Þ ¼ MSE T	
ð
Þ
MSEðTÞ ¼ :9246 þ :0370p=ð1  pÞ:
Since the ratio depends on the value of p, which is unknown, we must consider
all of the possible contingencies for p∈[0,1]. Note that the ratio is monotonically
increasing in p, taking its smallest value of .9246 when p ¼ 0, and diverging to
inﬁnity as p ! 1. The ratio of MSEs equals 1 when p ¼ .6708. Thus, without
constraints on the potential values of p, neither estimator is preferred to the
other on the basis of MSE, and thus neither estimator is rendered inadmissible
by the other.
□
In contrast to the scalar case, a myriad of different MSE comparisons are
possible when q(Q) is a (k  1) vector. First of all, there are k individual MSE
378
Chapter 7
Point Estimation Theory

comparisons that can be made between corresponding entries in the two
estimators T* and T. One could also compare the expected squared distances of
T* and T from q(Q), which is equivalent to comparing the sums of the mean
square errors of the entries in T* and T. Furthermore, one could contemplate
estimating linear combinations of the entries in q(Q) via corresponding linear
combinations of the entries in T* and T, so that MSE comparisons between the
estimators ℓ0T* and ℓ0T for ℓ0q(Q) are then of interest. All of the preceding MSE
comparisons are accounted for simultaneously in the following strong mean
square error (SMSE) criterion.
Deﬁnition 7.10
Strong Mean Square
Error Superiority
Let T* and T be two estimators of the (k  1) vector q(Q). T* is strong mean
square error superior to T iff MSEQ(T*)MSEQ(T) is negative semideﬁnite
8Q∈O and unequal to the zero matrix for some Q∈O.
If T* is SMSE superior to T, it follows directly from Deﬁnition 7.10 that
MSEQ ðT	
i Þ 
 MSEQ(Ti) 8i and 8Q∈O because if MSEQ(T*)MSEQ(T) is negative
semideﬁnite, the matrix difference necessarily has nonpositive diagonal
entries.4 It follows that
EQ d2 ðT	; qðQÞÞ


¼
X
k
i¼1
MSEQ ðT	
i Þ
X
k
i¼1
MSEQ ðTiÞ ¼ EQ d2ðT; qðQÞÞ


8Q 2 O
Furthermore, in terms of estimating ℓ0q(Q),
MSEQ ðℓ0T	Þ ¼ ℓ0MSEQðT	Þℓ
 ℓ0MSEQðTÞℓ¼ MSEQ ðℓ0TÞ 8Q 2 O and 8ℓ:
Thus in the sense of all of the MSE comparisons deﬁned previously, T* is at least
as good as T.
The fact that MSEQ(T*)MSEQ(T) is negative semideﬁnite and unequal to
the zero matrix for some Q∈O implies that some of the weak inequalities (
) in
the aforementioned MSE comparisons become strong inequalities (<) for some Q.
To see this, note that a nonzero negative semideﬁnite symmetric matrix neces-
sarily has one or more negative diagonal entries.5 Therefore, MSEQ T	
i


<
MSEQ(Ti) for some Q and i, so that EQ (d2(T*, q(Q)) < EQ(d2(T, q(Q)) for some Q
and MSEQ(ℓ0T*) < MSEQ(ℓ0T) for some Q andℓ. Thus, T* is superior to T for at least
some MSE comparisons in addition to being no worse for any of the MSE
comparisons. We can now deﬁne multivariate analogues to the notions of
relative efﬁciency and admissibility.
4By deﬁnition, A is negative semideﬁnite iff ℓ0Aℓ
 0 8ℓ. Then the ith diagonal entry of A must be 
 0 since this entry can be deﬁned
by ℓ0Aℓwith ℓbeing a zero vector except for a 1 in the ith position.
5A nonzero matrix has at least unit rank. The rank of a negative semideﬁnite symmetric matrix is equal to the number of negatively
valued eigenvalues, and all eigenvalues of a negative semideﬁnite matrix are 
 0. The trace of a negative semideﬁnite symmetric
matrix is equal to the sum of its eigenvalues. Since all diagonal entries in a negative semideﬁnite matrix must be 
 0, it follows that a
nonzero negative semideﬁnite symmetric matrix must have one or more negative diagonal entries.
7.3
Estimators and Estimator Properties
379

Deﬁnition 7.11
Relative Efﬁciency
and Admissibility with
Respect to SMSE
Let T* and T be estimators of the (k  1) vector q(Q). If T* is SMSE superior to
T, then T* is said to be relatively more efﬁcient than T. If there exists an
estimator that is relatively more efﬁcient than T, then T is said to be inad-
missible. Otherwise, T is said to be admissible.
As in the scalar case, if MSE is being used to measure estimator perfor-
mance, the analyst need not consider any estimators of q(Q) that are inadmissi-
ble when searching for good estimators of q(Q).6
In either the scalar or multivariate case, a natural question to ask is whether
an optimal estimator exists that has the smallest MSE or MSE matrix among all
estimators of q(Q). We might call such an estimator most efﬁcient, or simply
efﬁcient. Unfortunately, no such estimator exists in general. To clarify the
issues involved, consider the scalar case and note that the degenerate estimator
T* ¼ t*(X) ¼ Yo
would
certainly
have
minimum
mean-square
error
for
estimating Y if mean-square error were evaluated at the point Y ¼ Yo, i.e.,
MSEY(T*) ¼ 0 for Y ¼ Yo. Since a similar degenerate estimator could be deﬁned
for each Y∈O, then for a given estimator to have minimum mean-square error
for every potential value of Y, (i.e., uniformly in Y) it would be necessary that
MSEY(T)¼0 8Y∈O, which would imply that varQ(T)¼0 8Y∈O, and thus, that
PY(t(x) ¼ Y) ¼ 1 8Y∈O. In order to construct an estimator T that satisﬁes the
condition P(t(x)¼Y) ¼ 1 8Y∈O, it would be necessary to be able to identify the
true value of Y directly upon observing the sample outcome, x. This essentially
requires that the range of the random sample be dependent on the value of Y,
denoted as RY(X), in such a way that the sets RY(X), Y∈O, are all mutually
exclusive, i.e., RY0ðXÞ \ RY00ðXÞ ¼ ; for Y0 6¼ Y00. Then, upon observing x, one
would only need to identify the set RY(X) to which x belonged, and Y would be
immediately known. This is rarely, if ever, possible in practice, and so adopting a
minimum mean-square error criterion for choosing an estimator of q(Q) is not
feasible. A similar argument leads to the conclusion that there is in general no
estimator of a (k  1) vector q(Q) whose MSE matrix is smallest among the MSE
matrices of all estimators of q(Q).7
While there generally does not exist an estimator that has a uniformly (i.e.,
for all Q∈O) minimum MSE or MSE matrix relative to all other estimators of
q(Q), it is often possible to ﬁnd an optimal estimator if one restricts the type of
estimators under consideration. Two such restrictions that have been widely
used in practice are unbiasedness and linearity, which we will examine in the
next two subsections ahead.
6Some analysts use a weak mean square error (WMSE) criterion that relates to only expected squared distance considerations. T* is
WMSE superior to T iff EQ(d2(T*,q(Q)) 
 EY(d2(T,q(Q)) 8Q∈O, and < for some Q∈O. Relative efﬁciency and admissibility can be
deﬁned in the context of WMSE superiority and are left to the reader.
7By “smallest MSE matrix,” we mean that MSEQ(T*)-MSEQ(T) is a negative semideﬁnite matrix for all estimators T of q(Q) and
for all Q.
380
Chapter 7
Point Estimation Theory

Unbiasedness
The property of unbiasedness refers to the balancing point or
expectation of an estimator’s probability distribution being equal to what is
being estimated.
Deﬁnition 7.12
Unbiased Estimator
An
estimator
T
is
said
to
be
an
unbiased
estimator
of
q(Q)
iff
EQðTÞ ¼ qðQÞ; 8Q 2 V. Otherwise, the estimator is said to be biased.
As in the case of the MSE criteria, it is important to appreciate the signiﬁ-
cance of the condition 8Q∈O in the above deﬁnition. In the context of the point
estimation problem, we have assumed that the true value of Q, say Q*, is some
element of the speciﬁed parameter space, O, but we do not know which one.
Thus, the property of unbiasedness is stated for all possible contingencies
regarding the potential values for the true value of Q. Due to the condition
8Q∈O, the requirement for unbiasedness essentially means that EQ T
ð Þ¼ q(Q)
regardless of which value of Q∈O is the true value. Thus, for T to be unbiased,
its density function must be balanced on the point q(Q), whatever the true value
of Q. Whether or not T has the unbiasedness property depends on the functional
deﬁnition of T, and in particular, on how the function translates the density
function of X ~ f(x1,. . .,xn;Q) into the density function of T ~ f(t;Q).
An unbiased estimator has the intuitively appealing property of being equal
to q(Q) on average, the phrase having two useful interpretations. First, since the
expectation operation is inherently a weighted average of the outcomes of T,
then the outcomes of T have a weighted average equal to q(Q). Alternatively, if
one were to repeatedly and independently observe outcomes of the random
sample X, and thus repeatedly generate estimates of q(Q) using corresponding
outcomes of the vector T, then the simple average of all of the observed
estimates would converge in probability (and, in fact, converge almost surely)
elementwise to q(Q) by Khinchin’s WLLN (or by Kolmogorov’s SLLN in the case
of almost-sure convergence), provided only that q(Q) is ﬁnite.
We provide the following example of an unbiased estimator of a parameter.
Example 7.4
Unbiased Estimator of
the Mean of an
Exponential
Distribution
Let the population distribution of hard disk operating lives for a certain brand of
hard disk be a member of the exponential family of densities f(z; y) ¼ (1/y) e1=Y
I(0,1)(z), y ∈O ¼ (0,1). Let T ¼ n1 Pn
i¼1 Xi be an estimator of the expected
operating life of the hard disk, y, where (X1,. . .,Xn) is a random sample from the
exponential population distribution. Then T is an unbiased estimator of y, since
E(Xi) ¼ y 8i, which implies that E(T) ¼ E (n1 Pn
i¼1 Xi )¼ n1 Pn
i¼1 E Xi
ð
Þ ¼ y
regardless of the value of y > 0. Thus, for example, if the true value of y were
2, then E(T) ¼ 2, or if the true value of y were100, then E(T) ¼ 100.
□
MVUE, MVLUE or BLUE, and Efﬁciency
The unbiasedness criterion ensures only
that an estimator will have a density that has a central tendency or balancing
point of q(Q). However, it is clear that we would also desire that the density not
be too spread out around this balancing point for fear that an estimate could be
7.3
Estimators and Estimator Properties
381

generated that was a signiﬁcant distance from q(Q) with high probability.
Graphically, we would prefer the estimator T to the estimator T* in Figure 7.3,
where both of these estimators are unbiased estimators of q(Q).
The foregoing considerations motivate that, if one wishes to use an unbiased
estimator of q(Q), one should use the unbiased estimator that also has minimum
variance, or minimum covariance matrix if T is a vector, among all unbiased
estimators of q(Q). Since BiasQ(T) ¼ 0 for all estimators in the unbiased class of
estimators, MSEQ(T) ¼ varQ(T) or MSEQ(T) ¼ CovQ(T), and we can thus view
the objective of minimizing var(T) or Cov(T) equivalently as searching for the
estimator with the smallest MSE or smallest MSE matrix within the class of
unbiased estimators. In the deﬁnition below, we introduce the notation AﬃB to
indicate that matrix A is smaller than matrix B by a negative semideﬁnite
matrix, i.e., A  B ¼ C is a negative semideﬁnite matrix.
Deﬁnition 7.13
Minimum Variance
Unbiased Estimator
(MVUE)
An estimator T is said to be a minimum variance unbiased estimator of q(Q)
iff T is an unbiased estimator of q(Q), and
(a) (scalar case) varQ(T) 
 varQ(T*) 8 Q ∈O and 8 T* ∈Uq(Q);
(b) (vector case) CovQ(T) ﬃCovQ(T*) 8 Q ∈O and 8 T* ∈Uq(Q);
where Uq(Q) is the set of all unbiased estimators of q(Q).8
f (t ; Q)
m (t*; Q)
E(T)=q(Q)=E(T *)
Figure 7.3
The densities
of two unbiased
estimators of q(Q).
8This is alternatively referred to in the literature by the term uniformly minimum variance unbiased estimator (UMVUE), where the
adverb “uniformly” is used to emphasize the condition “8Q∈O.” In our usage of the terms, MVUE and UMVUE will be
interchangeable.
382
Chapter 7
Point Estimation Theory

Deﬁnition7.13impliesthatanestimatoris aMVUEiftheestimatoris unbiased
and if there is no other unbiased estimator that has a smaller variance or covariance
matrix for any Q∈O. Drawing direct analogies to the discussion of the MSE criteria,
a MVUE, T, is such that MSEQ(Ti) ¼ varQ(Ti) 
 varQ( T	
i ) ¼ MSEQ( T	
i ) 8Q∈O
and 8i, where T* is any estimator in the unbiased class of estimators. Further-
more, EQ (d2(T,q(Q)) 
 EQ (d2(T*, q(Q)) 8Q∈O and MSEQ(ℓ0T) ¼ varQ(ℓ0T) 
varQ(ℓ0T*) ¼ MSEQ(ℓ0T*) 8Q∈O and 8ℓ. Thus, within the class of unbiased
estimators, a MVUE of q(Q) is at least as good as any other estimator of q(Q)
in terms of all of the types of MSE comparisons that we have discussed
previously. If T is a MVUE for q(Q), then T is said to be efﬁcient within the
class of unbiased estimators.
Unfortunately, without the aid of theorems that facilitate the discovery of
MVUES, ﬁnding a MVUE of q(Q) can be quite challenging even when the point
estimation problem appears to be quite simple. The following example
illustrates the general issues involved.
Example 7.5
MVUE of the Mean of a
Bernoulli Distribution
Consider deﬁning an MVUE for the parameter p using a random sample of
size 2 from the Bernoulli population distribution f(z;p) ¼ pz(1p)1-zI{0,1}(z).
First of all, the range of the random sample is {(0,0), (0,1), (1,0), (1,1)}, which
represents the domain of the estimator function T ¼ t(X). For t(X) to be in the
unbiased class, the following general condition must be met:
E t X
ð Þ
ð
Þ ¼ t 0; 0
ð
Þ 1  p
ð
Þ2 þ t 0; 1
ð
Þ 1  p
ð
Þp þ t 1; 0
ð
Þp 1  p
ð
Þ þ t 1; 1
ð
Þp2 ¼ p 8p 2 ½0; 1
This unbiasedness conditions implies the following set of restrictions on the
deﬁnition of t X
ð Þ:
If p 2
1
f g
0
f g
0; 1
ð
Þ
8
>
>
>
<
>
>
>
:
9
>
>
>
=
>
>
>
;
then
E t X
ð Þ
ð
Þ ¼
t 1; 1
ð
Þp2
t 0; 0
ð
Þ 1  p
ð
Þ2
t 0; 1
ð
Þ þ t 1; 0
ð
Þ
ð
Þ 1  p
ð
Þp þ p2
8
>
>
>
<
>
>
>
:
9
>
>
>
=
>
>
>
;
¼ p iff
t 1; 1
ð
Þ ¼ 1
t 0; 0
ð
Þ ¼ 0
t 0; 1
ð
Þ þ t 1; 0
ð
Þ ¼ 1
8
>
>
>
<
>
>
>
:
9
>
>
>
=
>
>
>
;
Now consider the variance of t(X), which by deﬁnition can be written as
varðtðXÞÞ ¼ t 0; 0
ð
Þ  p
ð
Þ2 1  p
ð
Þ2 þ t 0; 1
ð
Þ  p
ð
Þ2 1  p
ð
Þp
þ t 1; 0
ð
Þ  p
ð
Þ2p 1  p
ð
Þ þ t 1; 1
ð
Þ  p
ð
Þ2p2
¼ 2p2 1  p
ð
Þ2 þ t 0; 1
ð
Þ  p
ð
Þ2 1  p
ð
Þp þ t 1; 0
ð
Þ  p
ð
Þ2p 1  p
ð
Þ
7.3
Estimators and Estimator Properties
383

where we have used the facts that E(t(X))¼ p, t(0,0) ¼ 0, and t(1,1) ¼ 1 since t(X)
must be unbiased. Also because of the unbiasedness condition, we can substi-
tute t(0,1) ¼ 1t(1,0) into the variance expression to obtain
varðtðXÞ ¼ 2p2ð1  pÞ2 þ ð1  p  tð1; 0Þ Þ2 ð1  pÞp þ ðtð1; 0Þ  p Þ2 pð1  pÞ:
The ﬁrst-order condition for a minimum of the variance is given by
d var tðXÞ
dtð1; 0Þ
¼ 2ð1  p Þ2 p þ 2tð1; 0Þpð1  pÞ þ 2tð1; 0Þpð1  pÞ  2 p2 ð1  pÞ ¼ 0;
which implies that 4p(1p)t(1,0) ¼ 2(1p)2p + 2p2(1p), so that t(1,0) ¼ (1/2),
which then implies t(0,1) ¼ (1/2).
We have thus deﬁned the function T ¼ t(X) that represents a MVUE of p by
associating an appropriate outcome of T with each random sample outcome.
The preceding results can be represented collectively as t(x1, x2) ¼ (1/2) (x1 + x2)
¼ x, so that the MVUE is X.
□
A number of general theorems that can often be used to simplify the search
for a MVUE will be presented in Section 7.5.
For purposes of simplicity and tractability, as well as for cases where little
can be assumed about the probability model other than conditions on low-order
moments, attention is sometimes restricted to estimators that are unbiased and
that have minimum variance or covariance matrix among all unbiased
estimators that are linear functions of the sample outcome. Such an estimator
is called a BLUE or MVLUE, as indicated in the following deﬁnition.
Deﬁnition 7.14
Best Linear Unbiased
Estimator (BLUE) or
Minimum Variance
Linear Unbiased
Estimator (MVLUE)
An estimator T is said to be a BLUE or MVLUE of q(Q) iff
1. T is a linear function, T ¼ t(X) ¼ AX + b, of the random sample X,
2. EQ (T)¼ q(Q) 8 Q ∈O (T is unbiased),
3. T has minimum variance or covariance matrix among all unbiased
estimators that are also linear functions of the random sample X 8 Q ∈O.
A BLUE estimator of q(Q) is also referred to as an efﬁcient estimator within
the class of linear unbiased estimators. The following is an example identifying
the BLUE or MVLUE of the mean of any population distribution with a ﬁnite
mean and variance.
Example 7.6
BLUE of Population
Mean for Any
Population Distribution
Let (X1,. . .,Xn) be a random sample from some population distribution f(z;Y)
having a ﬁnite mean m ¼ q1(Y) and variance s2 ¼ q2(Y). What is the BLUE of
the mean of the population distribution?
Answer: We are examining linear estimators, and thus t(X) ¼ Pn
i¼1 aiXi þ b. For T
to be unbiased, we require that Pn
i¼1 ai ¼ 1 and b ¼ 0, since E(T)¼E Pn
i¼1 aiXi þ b
	

¼ Pn
i¼1 aiE Xi
ð
Þ
	

þ b ¼ m Pn
i¼1 ai
	

þ b ¼ m holds for all potential m iff Pn
i¼1 ai ¼ 1
384
Chapter 7
Point Estimation Theory

and b ¼ 0. The variance of T is simply s2 Pn
i¼1 a2
i because (X1,. . .,Xn) is a random
sample from f(z;Y). Thus, to ﬁnd the BLUE, we must solve the following
minimization problem:
min
a1;...;an s2 Xn
i¼1 a2
i subject to
Xn
i¼1 ai ¼ 1:
The Lagrangian form of this minimization problem is given by
L ¼ s2 S
n
i¼1 a2
i l S
n
i¼1 ai 1


;
and the ﬁrst-order conditions are
@L
@ ai
¼ 2 s2 ai l ¼ 0;
i ¼ 1; . . . n
and
@L
@l ¼ 1  S
n
i¼1 ai ¼ 0:
The ﬁrst n conditions imply a1 ¼ a2 ¼ . . . ¼ an, since ai ¼ l/2s2 8i, and
then Pn
i¼1 ai ¼ 1 requires that ai ¼ 1/n, i ¼ 1,. . .,n. Thus, t(X) ¼ Pn
i¼1 aiXi ¼
n1Pn
i¼1 Xi ¼ X, so that the sample mean is the BLUE (or MVLUE) of the mean
of any population distribution having a ﬁnite mean and variance. The reader
should check that the second-order conditions for a minimum are in fact met.□
In addition to estimating the means of population distributions, a prominent
BLUE arises in the context of least-squares estimation of the parameters of a
general linear model, which we will examine in Chapter 8.
7.3.3
Asymptotic Properties
When ﬁnite sample properties are intractable or else inapplicable due to the
nonexistence of the appropriate expectations that deﬁne means and variances,
one generally resorts to asymptotic properties to rank the efﬁciency of
estimators. In addition, asymptotic properties are of fundamental interest if
the analyst is interested in assessing the effects on estimator properties of an
ever-increasing number of sample observations.
Asymptotic properties of estimators are essentially equivalent in concept to
the ﬁnite sample properties presented heretofore, except that asymptotic
properties are based on the asymptotic distributions of estimators rather than
estimators’ exact ﬁnite sampling distributions. In particular, asymptotic
analogues to MSE, relative efﬁciency, unbiasedness, and minimum-variance
unbiasedness can be deﬁned with reference to asymptotic distributions of
estimators. However, a problem of nonuniqueness of asymptotic properties
arises because of the inherent nonuniqueness of asymptotic distributions.
To
clarify
the
difﬁculties
that
can
arise
when
using
asymptotic
distributions as a basis for deﬁning estimator properties, let Tn denote an
estimator of the scalar q(Q) based on n sample observations, and suppose
7.3
Estimators and Estimator Properties
385

b1
n ðTn qðQÞÞ !
d Nð0; 1Þ . Then one might consider deﬁning asymptotic
properties of Tn in terms of the asymptotic distribution N(q(Q),b2
n). However,
by Slutsky’s theorem it follows that n= n  k
ð
Þ
ð
Þ1=2b1
n
Tn  q Q
ð
Þ
ð
Þ !
d N 0; 1
ð
Þfor a
ﬁxed value of k since
n= n  k
ð
Þ
ð
Þ1=2 ! 1 , so that an alternative asymptotic
distribution could be Tn

a
N q Q
ð
Þ;
n  k
ð
Þ=n
ð
Þb2
n


;
producing a different
asymptotic variance with implications for estimator performance measures
that are functions of the variance of estimators. The difﬁculty is that the
centering and scaling required to achieve a limiting distribution is not unique,
leading to both nonunique asymptotic distributions and nonunique asymptotic
properties derived from them.
There are two basic ways of addressing the aforementioned nonuniqueness
problem when dealing with asymptotic properties. One approach, which we will
mention only brieﬂy, is to rank estimators only on the basis of limits of asymp-
totic property comparisons so as to remove the effects of any arbitrary scaling or
centering from the comparison. For example, referring to the previous illustra-
tion of nonuniqueness, let the asymptotic distribution of Tn be N(q(Q),b2
n) and let
T	
n have the asymptotic distribution N q Q
ð
Þ;
n  k
ð
Þ=n
ð
Þb2
n


. Asymptotic MSEs
(AMSEs) based on these asymptotic distributions would be calculated as
AMSEQ(Tn) ¼ EQ
Tn  q Q
ð
Þ
ð
Þ2


¼ b2
n and AMSEQ(T	
n) ¼ EQ
T	
n  q Q
ð
Þ

2


¼
n  k
ð
Þ=n
ð
Þb2
n. Then the asymptotic relative efﬁciency (ARE) of Tn with respect
to T	
n would be represented as
AREQ Tn; T	
n


¼ AMSEQ ðT	
nÞ
AMSEQ ðTnÞ ¼ n  k
n
:
Using the ARE in this form, one would be led to the conclusion that T	
n is
asymptotically relatively more efﬁcient than Tn, which in the context of the
previous illustration of nonuniqueness would be absurd since Tn and T	
n are
the same estimator. However, limn!1 AREQ Tn; T	
n




¼ 1, which leads to the
conclusion that Tn and T	
n are equally preferable on the basis of asymptotic MSE
considerations, which of course is the correct conclusion in the current context.
The limit operation removes the arbitrary scaling of the asymptotic variance. To
operationalize this approach for general applications requires extensions of limit
notions (limit superiors, or lim sups) which we will leave for future study (see L.
Schmetterer (1974), Introduction to Mathematical Statistics. New York:
Springer-Verlag, pp. 335–342).
An
alternative
approach
for
avoiding
nonuniqueness
of
asymptotic
properties is to restrict the use of asymptotic properties to classes of estimators
for which the problem will not occur. For our purposes, it will sufﬁce to examine
the consistent asymptotically normal (CAN) class of estimators (for other
possibilities, see E. Lehmann, Point Estimation, pp. 347–348).
Prior to identifying the CAN class of estimators, we examine the property of
consistency.
Consistency
A consistent estimator is an estimator that converges in probabil-
ity (element-wise if Tn is a vector) to what is being estimated.
386
Chapter 7
Point Estimation Theory

Deﬁnition 7.15
Consistent Estimator
Tn is said to be a consistent estimator of q(Q) iff plimQ(Tn)¼ q(Q) 8 Q ∈O.
Thus, for large enough n (i.e., for large enough sample size), there is a high
probability that the outcome of a scalar estimator Tn will be in the interval
(q(Q)e, q(Q)+e) for arbitrarily small e > 0 regardless of the value of Q. Relatedly,
the sampling density of Tn concentrates on the true value of q(Q) as the sample
size
!
1 if Tn is a consistent estimator of q(Q). Consistency is clearly a
desirable property of an estimator, since it ensures that increasing sample infor-
mation will ultimately lead to an estimate that is essentially certain to be
arbitrarily close to what is being estimated, q(Q).
Since Tn !
m q(Q) implies Tn !
p q(Q), we can state sufﬁcient conditions for
consistency of Tn in terms of unbiasedness and in terms of variance convergence
to zero. Speciﬁcally, if Tn is unbiased, or if the bias vector converges to zero as n
! 1, and if var(Tn) ! 0 as n ! 1, or Cov(Tn) ! 0 as n ! 1 if Tn is a vector,
then Tn is a consistent estimator of q(Q) by mean-square convergence.
Example 7.7
Sample Mean as a
Consistent Estimator of
a Population Mean
Let (X1,. . .,Xn) be a random sample from a population distribution, f(z;Q), of the
incomes of buyers of new Cadillacs, where E(Z) ¼ q1(Q) ¼ m and var(Z) ¼ q2(Q)
¼ s2 < 1. The sample mean tn(X) ¼ Xn is a consistent estimator of the mean
income, m, of Cadillac buyers since Xn is unbiased, and its variance is such that
(s2/n) ! 0 as n ! 1.
□
We note for future reference that a sequence of estimators can be consistent
for q(Q) even without E(Tn) ! q(Q) even if E(Tn) exists 8n. This at ﬁrst
seems counter intuitive, since if Tn is consistent, its density collapses on q(Q)
as n ! 1. Note the following counterexample.
Example 7.8
Consistency without
E(Tn)! q(Y)
Let the sampling density of Tn be deﬁned as f(tn;Y) ¼ (1n1/2)I{Q}(tn) þ
n1/2I{n}(tn). Note that as n
!
1, limn!1 P[|tnY|< e]¼1, for any e > 0,
and Tn is consistent for Y. However, since E(Tn) ¼ Y(1n1/2) + n(n1/2) ¼
Y (1n1/2) + n1/2, then as n ! 1, E(Tn) ! 1.
□
The divergence of the expectation in Example 7.8 is due to the fact that the
density function of Tn, although collapsing to the point Y as n!1, was not
collapsing at a fast enough rate for the expectation to converge to Y. In particu-
lar, the density weighting assigned to the outcome n in deﬁning the expectation
went to zero at a rate slower than n went to inﬁnity as n
! 1, causing
the divergence. A sufﬁcient condition for Tn !
p qðQÞ ) limn!1E Tn
ð
Þ ¼ qðQÞ is
provided in the following theorem:
Theorem 7.1
Sufﬁcient Condition
for Tn !
p qðYÞ )
lim n!1E Tn
ð
Þ ¼ qðQÞ
If E T2
n


exists and is bounded 8n, so that E T2
n



 m<1 8n, then convergence in
probability implies convergence in mean.
7.3
Estimators and Estimator Properties
387

Proof
Rao, Statistical Inference, pp. 121.
n
Note the sufﬁcient condition given in Theorem 7.1 does not hold in
Example 7.7.
Consistent Asymptotically Normal (CAN) Estimators
The class of consistent
asymptotically normal (CAN) estimators of q(Q) is deﬁned in the statistical
literature to be the collection of all estimators of q(Q) for which n1/2 (Tnq(Q))
!
d N([0], ST), where ST is a positive deﬁnite covariance matrix that may depend
on the value of Q. We will allow this dependence to be implicit rather than
utilize notation such a ST(Q). Note the consistency of Tn follows immediately,
since by Slutsky’s theorem n1/2[n1/2(Tnq(Q))] ¼ Tnq(Q) !
d 0 · Z ¼ 0, where
Z ~ N(0, ST), which implies Tnq(Q) !
d 0 or equivalently Tn !
d q(Q). The CAN
class contains a large number of the estimators used in empirical work.
Because all of the estimators in the CAN class utilize precisely the same
sequence of centering (i.e., q(Q) is subtracted from Tn) and scaling (i.e., Tnq(Q)
is multiplied by n1/2), the problem of nonuniqueness of asymptotic distributions
and properties does not arise. Asymptotic versions of MSEs, MSE matrices, bias
vectors, variances, and covariance matrices can be deﬁned via expectations
taken with respect to the unique asymptotic distribution of estimators, where
Tn 
a N(q(Q), n1ST). In particular, letting the preﬁx A denote an asymptotic
property, and letting EA denote an expectation taken with respect to an asymp-
totic distribution, we have within the CAN class 8Q∈O,
AMSE Tn
ð
Þ ¼ ACov Tn
ð
Þ ¼ EAð Tn  qðQÞÞðTn  qðQÞÞ0


multivariate
ð
Þ
¼ Avar Tn
ð
Þ ¼ EA ðTn  qðQÞÞ2


scalar
ð
Þ
and
ABIAS Tn
ð
Þ ¼ EAðTn  qðQÞÞ ¼ 0:
The zero value of the asymptotic bias indicates that a CAN estimator of q(Q)
is necessarily asymptotically unbiased. We pause to note that there is a lack
of consensus in the literature regarding the deﬁnition of asymptotic unbiased-
ness, and Example 7.8 is useful for illustrating the issues involved. Some
statisticians deﬁne asymptotic unbiasedness of an estimator sequence in
terms of the limit of the expected values of the estimators in the sequence,
where limn!1 E(Tn) ¼ q(Q) 8Q∈O characterizes an asymptotically unbiased
estimator. Under this deﬁnition, the estimator in Example 7.8 would not be
asymptotically unbiased, but rather would be asymptotically biased. It is clear
that this deﬁnition of asymptotic unbiasedness requires that the expectations
in the sequence exist, as they do in Example 7.8. Within the CAN class, the two
deﬁnitions of asymptotic unbiasedness will coincide if the second order
moments of the estimators in the sequence {Tn} are bounded (recall Theorem
7.1), since then limn!1 E(Tn) ¼ q(Q) ¼ EA(Tn). Otherwise, the deﬁnitions may
refer to different concepts of unbiasedness, as Example 7.8 demonstrates.
388
Chapter 7
Point Estimation Theory

Thus, one must discern the deﬁnition of asymptotic unbiasedness being used by
any analyst by the context of the discussion.
Given the preceding deﬁnition of asymptotic properties, we can now deﬁne
the meaning of asymptotic relative efﬁciency and asymptotic admissibility
uniquely for CAN estimators.
Deﬁnition 7.16
Asymptotic Relative
Efﬁciency and
Asymptotic
Admissibility
Let Tn and T	
n be CAN estimators of q(Q) such that n1/2(Tnq(Q)) !
d N(0, ST)
and n1/2(T	
nq(Q)) !
d N(0, ST	).
a. If Tn and T	
n are scalars, then the asymptotic relative efﬁciency of Tn with
respect to T	
n is given by
AREQðTn; T	
nÞ ¼ AMSEQðT	
nÞ
AMSEQðTnÞ ¼ ST	
ST
8Y 2 O:
Tn is asymptotically relatively more efﬁcient than T	
n if AREQ(Tn, T	
n)  1
8Q 2 O and > 1 for some Q 2 O.
b. Tn is asymptotically relatively more efﬁcient than T	
n iff ST  ST	 is
negative semideﬁnite 8Y 2 O and ST  ST	6¼ 0 for some Q 2 O.
c. If there exists an estimator that is asymptotically relatively more efﬁcient
than Tn, then Tn is asymptotically inadmissible. Otherwise Tn is asymp-
totically admissible.
A discussion of the meaning of ARE and asymptotic admissibility, as well as
all of the other asymptotic properties presented to this point, would be
completely analogous to the discussion presented in the ﬁnite sample case,
except now all interpretations would be couched in terms of approximations
based on asymptotic distributions. We leave it to the reader to draw the analogies.
Example 7.9
Relative Asymptotic
Efﬁciency of Two
Estimators of
Exponential Mean
Recall Example 7.4 regarding the estimation of the expected operating lives of
hard disks, y, using a random sample from an exponential population distribu-
tion. As an alternative estimator of y, consider the following:
T	
n ¼ t	
n ðXÞ ¼
1
2
n1 X
n
i¼1
X2
i
 
!
"
#1=2
¼ M0
2=2
ð
Þ1=2:
Recall that in the case of the exponential probability distribution, m0
2 ¼
E X2
i


¼ 2 y2. Since M02 is the second order sample moment based on a random
sample from a probability distribution, we know that M02 !
p 2y2and that
n1=2 M02  m0
2
ð
Þ
m04  m02
ð
Þ2
h
i1=2 ¼ n1=2 M02  2y2


20y4
	

1=2
!
d Nð0; 1Þ
7.3
Estimators and Estimator Properties
389

where m0
4 ¼
d4 1  yt
ð
Þ1=dt4


h
i
t¼0 ¼ 24y4.
Now note that T	
n is a continuous function of M02 so that plim ðT	
nÞ ¼
plimðM02=2 )1/2 ¼ (plim( M02 )/2)1/2 ¼
y by Theorem 5.5. Therefore, T	
n is a
consistent estimator of y. Furthermore, n1/2 (M02  y) has a normal limiting
distribution and is thus a CAN estimator. To see this, recall Theorem 5.39 on
the asymptotic distribution of functions of asymptotically normal random
variables, where in this application, T	
n is a function of the asymptotically
normal random variable M02 
a N(2y2, 20y4/n). Since
G ¼
d T	
n
dM02


M02¼2y2 ¼
1
4 M0
2=2
ð
Þ1=2


M02¼2y2 ¼ 4y
ð
Þ1;
which is 6¼ 0 8 y > 0 and thus of “full row rank,” it follows that n1/2 (T	
n  y) !
d
N(0,G[20y4/n]G0) ¼ N(0,1.25 y2).
In comparing T	
n with Xn as estimators of y, it is now clear that although both
are consistent and asymptotically normal estimators of y, Xn is asymptotically
more efﬁcient than
T	
n , since in comparing the asymptotic variances of
the limiting distributions of n1/2( Xn  y ) and n1/2( T	
n  y ), we have that
y2 < 1.25 y2.
□
Asymptotic Efﬁciency
At this point it would seem logical to proceed to a deﬁni-
tion of asymptotic efﬁciency in terms of a choice of estimator in the CAN class
that has the smallest asymptotic variance or covariance matrix 8Q 2 O (compare
to Deﬁnition 7.13). Unfortunately, LeCam (1953)9 has shown that such an
estimator does not exist without further restrictions on the class of estimators.
In particular, LeCam (1953) effectively showed that for any CAN estimator one
can always deﬁne an alternative estimator that has a smaller variance or covari-
ance matrix for at least one Q 2 O. The implication of this result is that one
cannot deﬁne an achievable lower bound to the asymptotic variances or covari-
ance matrices of CAN estimators, so that no asymptotically optimal estimator
exists.
On the other hand, LeCam (1953) also showed that under mild regularity
conditions, there does exist a lower bound to the asymptotic variance or covari-
ance matrix of a CAN estimator that holds for all Q 2 O except on a set of
Q-values having Lebesque measure zero, which is the Cramer-Rao Lower Bound
that will be discussed in Section 7.5. Note that the Lebesque measure of a set of
Q-values can be thought of as the volume of the set within the k-dimensional
parameter space. A set having Lebesque measure zero is a set with zero volume
in k-space, e.g., a collection of isolated points, or a set of points having dimen-
sion less than k (such as a square and its interior in a three-dimensional space, or
a line in two-dimensional space). A set of Lebesque measure zero is a
nonstochastic analogue to a set having probability zero, and such a set is thus
practically irrelevant relative to its complement. It is thus meaningful to speak
9LeCam, L., (1953) “On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayes Estimates”, University of
California Publications in Statistics, 1:277–330, 1953.
390
Chapter 7
Point Estimation Theory

of a lower bound on the asymptotic variance or covariance matrix of a
CAN estimator of q(Q) that holds almost everywhere in the parameter space
(i.e., except for a set of Lebesque measure zero), and then a search for an estima-
tor that achieves this bound becomes meaningful as well.
At this point we will state a general deﬁnition of asymptotic efﬁciency
for CAN estimators. In Section 7.5, we will be much more precise about
the functional form of the asymptotic covariance matrix of an asymptotically
efﬁcient estimator.
Deﬁnition 7.17
Asymptotic Efﬁciency
If Tn is a CAN estimator of q(Q) having the smallest asymptotic covariance
matrix among all CAN estimators 8Q 2 O, except on a set of Lebesque
measure zero, Tn is said to be asymptotically efﬁcient.
As a ﬁnal remark, it is possible to remove the qualiﬁer “except on a set of
Lebesque measure zero” if the CAN class of estimators is further restricted so
that only estimators that converge uniformly to the normal distribution are
considered. Roughly speaking, uniform convergence of a function sequence
Fn(x) to F(x) requires that the rate at which convergence occurs is uniform across
all x is the domain of F(x), unlike ordinary convergence (recall Deﬁnition 5.7)
which allows for the possibility that the rate is different for each x. The
restricted class of estimators is called the Consistent Uniformly Asymptotically
Normal (CUAN) class, and within the CUAN class it is meaningful to speak of
an estimator that literally has the smallest asymptotic covariance matrix. The
interested reader can consult C.R. Rao, (1963) “Criteria of Estimation in Large
Samples,” Sankhya, Series A, pp. 189–206 for further details.
7.4
Sufﬁcient Statistics
Sufﬁcient statistics for a given estimation problem are a collection of statistics
or, equivalently, a collection of functions of the random sample, that summarize
or represent all of the information in a random sample that is useful for
estimating any q(Q). Thus, in place of the original random sample outcome, it
is sufﬁcient to have observations on the sufﬁcient statistics to estimate any q(Q).
Of course, the random sample itself is a collection of n sufﬁcient statistics, but
an objective in deﬁning sufﬁcient statistics is to reduce the number of functions
of the random sample needed to represent all of the sample information relevant
for estimating q(Q). If a small collection of sufﬁcient statistics can be found for a
given statistical model then for deﬁning estimators of q(Q) it is sufﬁcient to
consider only functions of the smaller set of sufﬁcient statistic outcomes as
opposed to functions of all n outcomes contained in the original random sample.
In this way the sufﬁcient statistics allow a data reduction step to occur in a point
estimation problem. Relatedly, it will be shown that the search for estimators of
q(Q) having the MVUE property or small MSEs can always be restricted to
functions of the smallest collection of sufﬁcient statistics. Finally, if the
7.4
Sufﬁcient Statistics
391

sufﬁcient statistics have a special property, referred to as completeness, then an
explicit procedure utilizing the complete sufﬁcient statistics is available that is
often useful in deﬁning MVUEs. We begin by presenting a more rigorous deﬁni-
tion of sufﬁcient statistics.
Deﬁnition 7.18
Sufﬁcient Statistics
Let X ¼ X1; . . . ; Xn
½
0~ f(x;Q) be a random sample, and let s ¼ s1ðXÞ; . . . ; srðXÞ
½
0
be r statistics. The r statistics are said to be sufﬁcient statistics for f(x;Q) iff
f(x; Qjs) ¼ h(x), i.e., the conditional density of X, given s, does not depend on
the parameter vector Q.10
An intuitive interpretation of Deﬁnition 7.18 is that once the outcomes of
the r sufﬁcient statistics are observed, there is no additional information on Q in
the sample outcome. The deﬁnition also implies that given the function values
s(x) ¼ s, no other function of X provides any additional information about Q
than that obtained from the outcomes s. To motivate these interpretations, ﬁrst
note that the conditional density function f(x;Q|s) can be viewed as representing
the probability distribution of all of the various ways in which random sample
outcomes, x, occur so as to generate the conditional value of s. This is because
the event being conditioned on requires that x satisfy, s(x) ¼ s. Deﬁnition 7.18
states that if S is a vector of sufﬁcient statistics, then Q is a ghost in f(x;Q|s) i.e.,
the conditional density function really does not depend on the value of Q since
f(x;Q|s) ¼ h(x). It follows that the probabilistic behavior of the various ways in
which x results in s(x) ¼ s has nothing to do with Q, i.e., it is independent of Q.
Thus, analyzing the various ways in which a given value of s can occur, or
examining additional functions of X, cannot possibly provide any additional
information about Q since the behavior of the outcomes of X, conditioned on
the fact that s(x) ¼ s, is totally unrelated to Q.
Example 7.10
Sufﬁcient Statistic for
Bernoulli Population
Distribution
Let (X1,. . .,Xn) be a random sample from a Bernoulli population distribution
representing whether phone call solicitations to potential customers results in a
sale,sothatfðx; pÞ ¼ p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 xi P
n
i¼1 If0;1g ðxiÞwherep 2 O ¼ (0,1),xi ¼ 1
denotes a sale, and xi ¼ 0 denotes no sale on the ith call. In this case, Pn
i¼1 Xi ,
representing the total number of sales in the sample, is a sufﬁcient statistic for f(x;p).
To see that this is true, ﬁrst note that the appropriate conditioning event in the
context of Deﬁnition 7.18, would be s(x) ¼ Pn
i¼1 xi ¼ s, i.e., the total number of
10Note that the conditional density function referred to in this deﬁnition is degenerate in the general sense alluded to in Section 3.10,
footnote 20. That is since (x1,. . .,xn) satisﬁes the r restrictions si(x1,. . .,xn) ¼ si, for i ¼ 1,. . .,r by virtue of the event being conditioned
upon, the arguments x1,. . .,xn of the conditional density are not all free to vary but rather are functionally related. If one wanted to
utilize the conditional density for actually calculating conditional probabilities of events for (X1,. . .,Xn), and if the random variables
were continuous, then line integrals would be required as discussed previously in Chapter 3 concerning the use of degenerate
densities. This technical problem is of no concern in our current discussion of sufﬁcient statistics since we will have no need to
actually calculate conditional probabilities from the conditional density.
392
Chapter 7
Point Estimation Theory

sales equals the value s. It follows from the deﬁnition of conditional probability that
the conditional density function can be deﬁned as11
fðx; pjsÞ ¼ Pðx1; . . . ; xn; sðxÞ ¼ sÞ
PðsðxÞ ¼ sÞ
:
The denominator probability is given directly by
PðsðxÞ ¼ sÞ ¼
n
s
 
!
ps 1  p
ð
Þns If0;1;...;ng ðsÞ
because s(X) ¼ Pn
i¼1 Xi is the sum of iid Bernoulli random variables, which we
know to have a binomial distribution. The numerator probability is deﬁned by
an appropriate evaluation of the joint density of the random sample, as
Pðx1; . . . ; xn; sðxÞ ¼ sÞ ¼ fðx; pÞI Pn
i¼1 xi¼s
f
gðxÞ
¼ p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 xi
Y
n
i¼1
If0;1g ðxiÞ
 
!
I Pn
i¼1 xi¼s
f
gðxÞ
which is the probability of x1,. . .,xn and s(x) ¼ Pn
i¼1 xi ¼ s. Using the preceding
functional representations of the numerator and denominator probabilities in
the ratio deﬁning the conditional density function, and using the fact that
s ¼ Pn
i¼1 xi, we obtain after appropriate algebraic cancellations that
fðx; pjsÞ ¼
n
s
 
!1 Y
n
i¼1
If0;1g ðxiÞ
"
#
I Pn
i¼1 xi¼s
f
gðxÞ
for any choice of s 2 {0,1,. . .,n}, which does not depend on the parameter p. Thus,
S ¼ Pn
i¼1 Xi is a sufﬁcient statistic for p.
Note the conditional density states that, given Pn
i¼1 xi ¼ s, all outcomes of
(x1,. . .,xn) are equally likely with probability
n
s

1
, and thus the probability of a
particular pattern of sales and no sales occurring for (x1,. . .,xn), given that
Pn
i¼1 xi ¼ s, has nothing to do with the value of p. It follows that only the fact
that Pn
i¼1 xi ¼ s provides any information about p – the particular pattern of 0’s
and 1’s in (X1,. . .,Xn) is irrelevant. This is consistent with intuition in that it is
the total number of sales in n phone calls and not the particular pattern of sales
that provides information in a relative frequency sense about the probability, p,
of obtaining a sale on a phone call solicitation. Furthermore, if Y ¼ g(X) is any
other function of the random sample, then it can provide no additional informa-
tion about p other than that already provided by s(X). This follows from the fact
11The reader may wonder why we deﬁne the conditional density “from the deﬁnition of conditional probability,” instead of using the
rather straightforward methods for deﬁning conditional densities presented in Chapter 2, Section 2.6. The problem is that here we are
conditioning on an event that involves all of the random variables X1,. . .,Xn, whereas in Chapter 2 we were dealing with the usual case
where the event being conditioned upon involves only a subset of the random variable X1,. . .,Xn having fewer than n elements.
7.4
Sufﬁcient Statistics
393

that h(y|s(x) ¼ s) will not depend on p because the conditional density of Y will
have been derived from a conditional density of X that is independent of p, i.e.,
hðyjsðxÞ ¼ sÞ ¼ PðyjsðxÞ ¼ sÞ ¼
X
fx:gðxÞ¼yg
fðx; pjsÞ ¼
X
fx:gðxÞ¼yg
hðxÞ
since f(x;p|s) ¼ h(x) if s is a sufﬁcient statistic.
□
In any problem of estimating q(Q), once the outcome of a set of sufﬁcient
statistics is observed, the random sample outcome (x,. . .,xn) can effectively be
ignored for the remainder of the point estimation problem since s(x) captures all
of the relevant information that the sample has to offer regarding q(Q). Essen-
tially, it is sufﬁcient that the outcome of s be observed. For example, with
reference to Example 7.10, if a colleague were to provide the information that
123 sales were observed in a total of 250 phone calls, i.e., P250
i¼1 xi ¼ 123, we
would have no need to examine any other characteristic of the random sample
outcome (x1,. . .,x250) when estimating p, or q(p).
A signiﬁcant practical problem in the use of sufﬁcient statistics is knowing
how to identify them. A criterion for identifying sufﬁcient statistics which is
sometimes useful is given by the Neyman Factorization Theorem:
Theorem 7.2
Neyman Factorization
Theorem
Let f(x;Q) be the density function of the random sample (X1,. . .,Xn). The statis-
tics S1,. . .,Sr are sufﬁcient statistics for f(x;Q) iff f(x;Q) can be factored as f(x;Q) ¼
g(s1(x),. . .,sr(x);Q) h(x), where g is a function of only s1(x),. . .,sr(x) and of Q, and
h(x) does not depend on Q.
Proof
The proof of the theorem in the continuous case is quite difﬁcult, and we leave it
to a more advanced course of study (see Lehmann, (1986) Testing Statistical
Hypotheses, John Wiley, 1986, pp. 54–55). We provide a proof for the discrete case.
Sufﬁciency
Suppose the factorization criterion is met. Let B(a) ¼ {(X1,. . .,Xn):si(x) ¼ ai,
i ¼ 1,. . .,r; x 2 R(X)} be such that P(B(a)) > 0, and note that
PðBðaÞÞ ¼
X
ðx1;...;xnÞ2BðaÞ
fðx; YÞ ¼ gða1; . . . ; ar; QÞ
X
ðx1;...;xnÞ2BðaÞ
hðx1; . . . ; xnÞ
Therefore,
f x; Qjs x
ð Þ ¼ a
ð
Þ ¼
g a; Q
ð
Þh x
ð Þ
g a; Q
ð
Þ P
x2B a
ð Þ h x
ð Þ ¼ h	 x
ð Þ
when x satifies s x
ð Þ ¼ a
0
otherwise
8
<
:
9
=
;
which does not depend on Q and hence s1, . . ., sr are sufﬁcient statistics.
Necessity
Suppose s1, . . ., sr are sufﬁcient statistics, and note by the deﬁnition of the
conditional density that f(x;Q) ¼ f(x|si(x) ¼ ai, i ¼ 1, . . ., r) P(si(x) ¼ ai, i ¼ 1,
. . ., r) where the conditional density function does not depend on Q by the
sufﬁciency of s. Then we have factored f(x;Q) into the product of a function of
394
Chapter 7
Point Estimation Theory

s1(x),. . .,sr(x) and Q (i.e., P(si(x) ¼ ai, i ¼ 1,. . .,r) will depend on Q), and a function
that does not depend on Q.
n
As we have alluded to previously, a practical advantage of sufﬁcient statis-
tics is that they can often greatly reduce the number of random variables
required to represent the sample information relevant for estimating q(Q), as
seen in Example 7.8 and in the following example of the use of the Neyman
Factorization Theorem.
Example 7.11
Sufﬁcient Statistics via
Neyman Factorization
for Exponential
Let X ¼ (X1,. . .,Xn) be a random sample from the exponential population distri-
bution Y1ez/Y I(0,1)(z) representing waiting times between customer arrivals
at a retail store. Note that the joint density of the random sample is given
fðx1; . . . ; xn; YÞ ¼ Yn exp  Pn
i¼1 xi=Y

 Q
n
i¼1
Ið0;1Þ ðxiÞ:
The joint density can be factored into the form required by the Neyman
Factorization theorem by deﬁning g( Pn
i¼1 xi ;Y) ¼ Yn exp  Pn
i¼1 xi=Y


and
h(x) ¼ Qn
i¼1 I 0;1
ð
Þ (xi). Then from the theorem, we can conclude that S ¼ Pn
i¼1 Xi
is a sufﬁcient statistic for f(x;Y). It follows that the value of the sum of the
random sample outcomes contains all of the information in the sample
outcomes relevant for estimating q(Y).
□
Successful use of the Neyman Factorization Theorem for identifying sufﬁ-
cient statistics requires that one be ingenious enough to deﬁne the appropriate
g(s(x);Q) and h(x) functions that achieve the required joint probability density
factorization. Since the appropriate function deﬁnitions will not always be
readily apparent, an approach introduced by Lehmann–Scheffe´12 can sometimes
be quite useful for providing direction to the search for sufﬁcient statistics. We
will discuss this useful result in the context of minimal sufﬁcient statistics.
7.4.1
Minimal Sufﬁcient Statistics
At the beginning of our discussion of sufﬁcient statistics we remarked that an
objective of using sufﬁcient statistics is to reduce the number of functions of the
random sample required to represent all of the information in the random
sample relevant for estimating q(Q). A natural question to consider is what is
the smallest number of functions of the random sample that can represent all of
the relevant sample information in a given point estimation problem? This
relates to the concept of a minimal sufﬁcient statistic, which is essentially the
sufﬁcient statistic for a given f(x;Q) that is deﬁned using the fewest number of
(functionally independent) coordinate functions of the random sample.
The statement of subsequent deﬁnitions and theorems will be facilitated
by the concept of the range of X over the parameter space V, deﬁned as
12Lehmann, E.L. and H. Scheffe’ (1950). Completeness, Similar Regions, and Unbiased Estimation, Sankhya¯, 10, pp. 305.
7.4
Sufﬁcient Statistics
395

RO X
ð Þ ¼ fx : f x; Q
ð
Þ>0 for some Q 2 Og The set RO(X) represents all of the
values of x that are assigned a nonzero density weighting by f(x;Q) for at least
one Q 2 O. In other words, RO(X) is the union of the supports of the densities
f(x;Q) for Q 2 O and thus corresponds to the set of relevant x-outcomes for
the statistical model {f(x;Q), Q 2 O}. If the support of the density f(x;Q) does
not change with Q (e.g., normal, Gamma, binomial) then RO(X) ¼ R(X) ¼ {x:
f(x;Q) > 0}, where Q 2 O can be chosen arbitrarily and we henceforth treat
the range of X as being synonymous with the support of its density.
Deﬁnition 7.19
Minimal Sufﬁcient
Statistics
A sufﬁcient statistic S ¼ s(X) for f(x;Q) is said to be a minimal sufﬁcient
statistic if for every other sufﬁcient statistic T ¼ t(X) ∃a function hT(·) such
that s(x) ¼ hT(t(x)) 8x 2 RO(X).
In order to motivate what is “minimal” about the sufﬁcient statistic S in
Deﬁnition 7.19, ﬁrst note that S will have the fewest elements in its range
compared to all sufﬁcient statistics for f(x;Q). This follows from the fact that a
function can never have more elements in its range than in its domain (recall the
deﬁnition of a function, which requires that there is only one range point
associated with each domain element, although there can be many domain
elements associated with each range element), and thus if S ¼ hT(T) for any
other sufﬁcient statistic T, then the number of elements in R(S) must be no more
than the number of elements in R(T), for any sufﬁcient statistic T. So, in this
sense, S utilizes the minimal set of points for representing the sample informa-
tion relevant for estimating q(Q).
It can also be shown that a minimal sufﬁcient statistic can be chosen to have
the fewest number of coordinate functions relative to any other sufﬁcient statis-
tic, i.e., the number of coordinate functions deﬁning the minimal sufﬁcient
statistic is minimal. A rigorous proof of this fact is quite difﬁcult and is deferred
to a more advanced cause of study.13 In order to at least motivate the plausibility
of this fact, ﬁrst note that since a minimal sufﬁcient statistic, say S, is a function
of all other sufﬁcient statistics, then if T is any other sufﬁcient statistic, t(x) ¼
t(y) ) s(x) ¼ hT(t(x)) ¼ hT(t(y)) ¼ s(y). It follows that
AT ¼
x; y
ð
Þ : t x
ð Þ ¼ t y
ð Þ
f
g 
x; y
ð
Þ : s x
ð Þ ¼ s y
ð Þ
f
g ¼ B
no matter which sufﬁcient statistic, T, is being referred to. If B is to contain the
set AT, then the constraints on (x,y) representing the set-deﬁning conditions of B
cannot be more constraining than the constraints deﬁning AT, and in particular
the number of nonredundant constraints14 deﬁning B cannot be more than the
13See E.W. Barankin and M. Katz, (1959) Sufﬁcient Statistics of Minimal Dimension, Sankhya, 21:217–246; R. Shimizu, (1966)
Remarks on Sufﬁcient Statistics, Ann. Inst. Statist. Math., 18:49–66; D.A.S. Fraser, (1963) On Sufﬁciency and the Exponential Family,
Jour. Roy. Statist. Soc., Series B, 25:115–123.
396
Chapter 7
Point Estimation Theory

number deﬁning AT. Thus the number of nonredundant coordinate functions
deﬁning S must be no larger than the number of nonredundant coordinate
functions deﬁning any other sufﬁcient statistic, so that the number of coordi-
nate functions deﬁning S is minimal. Identiﬁcation of minimal sufﬁcient statis-
tics can often be facilitated by the following approach suggested by Lehmann
and Scheffe´.
Theorem 7.3
Lehmann-Scheffe´
Minimal Sufﬁciency
Theorem
Let X ~ f(x;Q). If the statistic S ¼ s(X) is such that 8x and y 2 RO(X), f(x;Q) ¼
t(x,y) f(y;Y) iff (x,y) satisﬁes s(x) ¼ s(y), then S ¼ s(X) is a minimal sufﬁcient
statistic for f(x;Q).
Proof
Deﬁne A(s) ¼ {x: s(x) ¼ s} and let xs 2 A(s)\RO(X), be chosen as a representative
element of A(s), 8s 2 R(S). Deﬁne Z(x) ¼ xs 8x 2 A(s) and 8s 2 R(S). Thus A(s) is
the set of x-outcomes whose image s(x) is s, and Z(x) is the representative
element of the set A(s) to which x belongs.
Assume that (x,y) 2 {(x,y): s(x) ¼ s(y)} ) f(x;Q) ¼ t(x,y) f(x;Q) 8 x and y 2
RO(X). Then for x 2 A(s) \ RO(X), sðxÞ ¼ sðxsÞ implies
fðx; YÞ ¼ tðx; xsÞfðxs; YÞ
¼ tðx; hðxÞÞfðhðxÞ; YÞ
substitute
hðxÞ ¼ xs
 
!
¼ hðxÞgðsðxÞ; YÞ
where h(x)  t(x,h(x)), g(s(x);Y)  f(h(x);Y), and the g-function in the latter identity
can be deﬁned from the fact that h(x) ¼ xs iff s(x) ¼ s, so that h(x)
, s(x).
If x 2 RO(X), then f(x;Y) ¼ h(x) g(s(x);Y) by deﬁning h(x) ¼ 0. Since Neyman
factorization holds, s(X) is a sufﬁcient statistic.
Now assume f(x;Y) ¼ t(x,y) f(x;Y) ) (x,y) 2 {(x,y): s(x) ¼ s(y)} 8x and y 2
RO(X). Let s*(x) be any other sufﬁcient statistic for f(x;Y). Then by Neyman
factorization, for some g*(·) and h*(·) functions, f(x;Y) ¼ g*(s*(x);Y) h*(x). If s*(x)
¼ s*(y),
then
since
g*(s*(x);Y) ¼ g*(s*(y);Y),
and
it
follows
that
f(x;Y) ¼
[h*(x)/h*(y)] f(x;Y) ¼ t(x,y) f(x;Y) whenever h*(y) 6¼ 0, so that s*(x) ¼ s*(y) ) s(x) ¼
s(y). Values of y for which h*(y) ¼ 0 are such that f(x;Y) ¼ 0 8Y 2 O by
Neyman Factorization, and are thus irrelevant to the minimal sufﬁciency of S
(recall Deﬁnition 7.19). Then s is a function of s*, as s(x) ¼ g(s*(x)), 8x 2 RO(X),
because for a representative ys*
2 {x: s*(x) ¼ s*}, s*(x) ¼ s*(ys*) ¼ s* )
s(x) ¼
s(ys*) ¼ s, and thus s(x) ¼ s ¼ g(s*) ¼ g(s*(x)). Therefore s(X) is a minimal sufﬁcient
statistic by Deﬁnition 7.19.
n
14By nonredundant, we mean that none of the constraints are implied by the others. Redundant constraints are constraints that are
ineffective or unnecessary in deﬁning sets.
7.4
Sufﬁcient Statistics
397

Before proceeding to applications of the theorem, we present two corollaries
that are informative and useful in practice.
Corollary 7.1 Lehmann-
Scheffe´ Sufﬁciency
Let X ~ f(x;Y). If the statistic S ¼ s(X) is such that 8 x and y 2 RO(X), (x,y) 2 {(x,y):
s(x) ¼ s(y)}
) f(x;Y) ¼ t(x,y) f(y;Y), then S ¼ s(X) is a sufﬁcient statistic for
f(x;Y).
Proof
The validity of this corollary is implied by the ﬁrst part of the proof of
Theorem 7.3.
n
The corollary indicates that the “only if” part of the condition in Theorem
7.3 is not required for the sufﬁciency of s(X) but it is the addition of the “only if”
part that results in minimality of s(X).
Corollary 7.2 Minimal
Sufﬁciency when R(X) is
Independent of Q
Let X ~ f(x;Y) and suppose R(X) does not depend on Y. If the statistic S ¼ s(X) is
such that fðx; YÞ=fðy; YÞ does not depend on Y iff (x,y) satisﬁes s(x) ¼ s(y) then
S ¼ s(X) is a minimal sufﬁcient statistic.
Proof
This follows from Theorem 7.3 by dividing through by f(y;Y) on the left-hand
side of the iff condition, which is admissible for all x and y in R(X) ¼
{x: f(x;Y) > 0}. Values of x and y =2 R(X) are irrelevant to sufﬁciency (recall
Deﬁnition 7.19).
n
Using the preceding results for deﬁning a minimal sufﬁcient statistic of
course still requires that one is observant enough to recognize an appropriate
(vector) function S. However, in many cases the Lehmann-Scheffe´ approach
transforms the problem into one where a choice of S is readily apparent. The
following examples illustrate the use of the procedure for discovering minimal
sufﬁcient statistics.
Example 7.12
Lehman-Scheffe´
Minimal Sufﬁciency
Approach for Bernoulli
Let X ¼ (X1,. . .,Xn) be a random sample from a nondegenerate Bernoulli popula-
tion distribution representing whether or not a customer contact results in a
sale, so that
fðx; pÞ ¼ p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 XiY
n
i¼1
If0;1g ðxiÞ for p 2 ð0; 1Þ:
In
an
attempt
to
deﬁne
a
sufﬁcient
statistic
for
f(x;p),
follow
the
Lehmann–Scheffe´ procedure by examining
fðx; pÞ
fðy; pÞ ¼
p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 xi Q
n
i¼1
If0;1g ðxiÞ
p
Pn
i¼1 yi 1  p
ð
ÞnPn
i¼1 yi Q
n
i¼1
If0;1g ðyiÞ
398
Chapter 7
Point Estimation Theory

for all values of x and y 2 R(X) ¼ n
i¼1 0; 1
f
g. The ratio will be independent of p,
iff the constraint Pn
i¼1 xi ¼ Pn
i¼1 yi is imposed. A minimal sufﬁcient statistic for
f(x;p) is thus s(X) ¼ Pn
i¼1 Xi by Corollary 7.2.
□
Example 7.13
Lehman-Scheffe´
Minimal Sufﬁciency
Approach for Gamma
Let X ¼ (X1,. . .,Xn) be a random sample from a gamma population distribution
representing the operating life until failure of a certain brand and type of per-
sonal computer, so that
fðx; a; bÞ ¼
1
bna Gn ðaÞ
Y
n
i¼1
xi
 
!a1
exp

X
n
i¼1
xi=b
 
!Y
n
i¼1
Ið0;1Þ ðxiÞ:
Using the Lehmann–Scheffe´ procedure for deﬁning a sufﬁcient statistic for
f(x;a,b), examine
fðx; a; bÞ
fðy; a; bÞ ¼
Q
n
i¼1
xi

a1
exp
 P
n
i¼1
xi=b

Q
n
i¼1
Ið0;1Þ ðxiÞ
Q
n
i¼1
yi

a1
exp
 P
n
i¼1
yi=b

Q
n
i¼1
Ið0;1Þ ðyiÞ
for all values of x and y 2 R(X) ¼ n
i¼1 (0,1). (Note the term (bnaGn(a)) has been
algebraically canceled in the density ratio). The ratio will be independent of both
a and b iff the constraints Qn
i¼1 xi ¼ Qn
i¼1 yi and Pn
i¼1 xi ¼ Pn
i¼1 yi, are imposed.
A minimal sufﬁcient statistic for f(x;a,b) is then bivariate and given by s1(X) ¼
Qn
i¼1 Xi and s2(X) ¼ Pn
i¼1 Xi, by Corollary 7.2.
□
Example 7.14
Lehman-Scheffe´
Minimal Sufﬁciency
Approach for Uniform
Let X ¼ (X1,. . .,Xn) be a random sample from a uniform population distribution
representing the number of minutes that a shipment is delivered before (x < 0)
or after (x > 0) its scheduled arrival time, so that
f x; a; b
ð
Þ ¼ b  a
ð
Þn Yn
i¼1 I a;b
½
 xi
ð
Þ:
Unlike the previous examples, here the range of X depends on the parameters a
and b. Referring to the Lehmann-Scheffe´ procedure for deﬁning a sufﬁcient
statistic for f(x;a,b) as given by Theorem 7.3, examine
t x; y; a; b
ð
Þ ¼ f x; a; b
ð
Þ=f y; a; b
ð
Þ ¼
Y
n
i¼1
I a;b
½
 xi
ð
Þ=
Y
n
i¼1
I a;b
½
 yi
ð
Þ
for all values of x 2 RO(X) ¼ {x: f(x;a,b) > 0 for some (a,b) satisfying 1 < a < b <
1},15 and for values of y for which the denominator is > 0. (Note we have algebrai-
cally canceled the (ba)n term which appears in both the numerator and the
denominator of the ratio.) The x and y vectors under consideration will be n-
element vectors, with x being any point in ℝn and y being any point in n
i¼1 [a,b].
15It may be more appropriate to assume ﬁnite lower and upper bounds for a and b, respectively. Doing so will not change the ﬁnal
result of the example.
7.4
Sufﬁcient Statistics
399

The ratio will be independent of a and b iff min (x1,. . .,xn) ¼ min (y1,. . .,yn) and
max(x1,. . .,xn) ¼ max(y1,. . .,yn), in which case the ratio will be equal to 1.
The preceding conditions also ensure that f(x;a,b) ¼ 0 when f(y;a, b) ¼ 0, so that
f(x;a,b) ¼ t(x,y)f(y;a,b) holds 8x and y 2 RO(X). A minimal sufﬁcient statistic for
f(x;a,b) is then bivariate and given by the order statistics s1(X) ¼ min(X1,. . .,Xn)
and s2(X) ¼ max(X1,. . .,Xn) by Theorem 7.3.
□
7.4.2
Sufﬁcient Statistics in the Exponential Class
The exponential class of densities represent a collection of parametric families
of density functions for which sufﬁcient statistics are straightforwardly deﬁned.
Furthermore, the sufﬁcient statistics are generally minimal sufﬁcient statistics.
Theorem 7.4
Exponential Class and
Sufﬁcient Statistics
Let f(x;Q) be a member of the exponential class of density functions
f x; Q
ð
Þ ¼ exp
X
k
i¼1
ci Q
ð
Þgi x
ð Þ þ d Q
ð
Þ þ z x
ð Þ
"
#
IA x
ð Þ:
Then s(X) ¼ (g1(X),. . .,gk(X)) is a k-variate sufﬁcient statistic, and if ci(Q),
i ¼ 1,. . .,k are linearly independent, the sufﬁcient statistic is a minimal sufﬁ-
cient statistic.
Proof
That s(X) is a sufﬁcient statistic follows immediately from the Neyman Factori-
zation theorem by deﬁning g g1ðxÞ; . . . ; gkðxÞ; Q
ð
Þ ¼ exp Pk
i¼1 ciðQÞgiðxÞþ
h
dðQÞ
i
and hðxÞ ¼ exp z x
ð Þ
ð
ÞIA x
ð Þ in the theorem.
That s(X) is a minimal sufﬁcient statistic follows from the fact that s(X) can
be derived using the Lehmann–Scheffe´ approach of Corollary 7.2. To see this,
note that
fðx; QÞ=fðy; QÞ ¼ exp S
k
i¼1 ci ðQÞ½giðxÞ  giðyÞ þ zðxÞ  zðyÞ

 IA ðxÞ
IA ðyÞ
will be independent of Q iff x and y satisfy gi(x) ¼ gi(y) for i ¼ 1,. . .,k assuming
ci(Q),i ¼ 1,. . .,k are linearly independent.16
n
Note that Theorem 7.4 could be used as an alternative approach for discov-
ering minimal sufﬁcient statistics in the problems of random sampling
16If one (or more) ci(Y) were linearly dependent on the other cj(Q)’s, then “only if” would not apply. To see this, suppose ck(Y) ¼
Pk1
i¼1 aici Q
ð
Þ. Then the exp term could be rewritten asexp
S
k1
i¼1 ci ðQÞ gi ðxÞ  gi ðyÞ þ ai ½gk ðxÞ  gk ðyÞ
½



and so gi(x) ¼ gi(y), i ¼ 1,. . .,k,
is sufﬁcient but not necessary for the term to be independent of Q, and thus s(X) would not be minimal.
400
Chapter 7
Point Estimation Theory

examined in Examples 7.11 and 7.12. It could not be used in Example 7.13 since
the uniform distribution is not in the exponential class.
7.4.3
Relationship Between Sufﬁciency and MSE: Rao-Blackwell
In addition to generating a condensed representation of the information in a
sample relevant for estimating q(Q), sufﬁcient statistics can also facilitate the
discovery of estimators of q(Q) that are relatively efﬁcient in terms of MSE. In
particular, in the pursuit of estimators with low MSE, only functions of
sufﬁcient statistics need to be examined, which is the implication of the
Rao-Blackwell theorem.
Theorem 7.5
Rao-Blackwell
Theorem - Scalar Case
Let S ¼ (S1,. . .,Sr) be an r-variate sufﬁcient statistic for f(x;Q), and let t*(X)
be any estimator of the scalar q(Q) having ﬁnite variance. Deﬁne t(X) ¼
E(t*(X)|S1,. . .,Sr) ¼ x(S1,. . .,Sr). Then t(X) is an estimator of q(Q) for which
MSEY(t(X)) 
 MSEY (t*(X)) 8 Q 2 O, with the equality being attained only
if PQ(t(x) ¼ t*(x)) ¼ 1.
Proof
First note that since S ¼ (S1,. . .,Sr) is an r-variate sufﬁcient statistic, f xjs
ð
Þ does
not depend on Q, and thus neither does the function t(X) (since it is deﬁned
as a conditional expectation using f xjs
ð
Þ), so t(X) is a statistic that can be used
as
an
estimator
of
q(Q).
Now
by
the
iterated
expectation
theorem,
Eðt XÞ
ð
Þ ¼ EEðt	 X
ð ÞjS1; . . . . . . ; SrÞ ¼ E(t*(X)), so that t(X) and t*(X) have precisely
the same expectation. Next examine
MSE t	 X
ð Þ
ð
Þ ¼ E t	 X
ð Þ  q Q
ð
Þ
ð
Þ2 ¼ E t	 X
ð Þ  t X
ð Þ þ t X
ð Þ  q Q
ð
Þ
ð
Þ2
¼ E t	 X
ð Þ  t X
ð Þ
ð
Þ2 þ 2E t	 X
ð Þ  t X
ð Þ
ð
Þ t X
ð Þ  q Q
ð
Þ
ð
Þ
þ E t X
ð Þ  q Q
ð
Þ
ð
Þ2:
The cross–product term is zero. To see this, ﬁrst note that E[(t*(X)  t(X)) (t(X) 
q(Q))] ¼ E[t(X) (t*(X)  t(X))] since E(t*(X)  t(X)) q(Q) ¼ 0 because E(t*(X)) ¼
E(t(X)). Now note that by deﬁnition t(X) is a function of only sufﬁcient statistics,
so that t(X) is a constant given s1,. . .,sr. Therefore,
E t t	 X
ð Þ  t
ð
Þjs1; . . . ; sr
½
 ¼ t E t	 X
ð Þ  t
ð
Þjs1; . . . ; sr
½
Þ ¼ 0
since Eðt	 X
ð Þjs1; . . . . . . ; srÞ ¼ t by deﬁnition, so that E[t(X)(t*(X)  t(X))] ¼ 0 by
the iterated expectation theorem.
Then dropping the nonnegative term E(t*(X)  t(X))2 on the right-hand side
of the expression deﬁning MSE(t*(X)) above yields MSEQ(t*(X))  EQ(t(X) 
q(Q))2 ¼ MSEQ(t(X)) 8 Q 2 O. The equality is attained iff EQ(t*(X)  t(X))2 ¼ 0,
which requires that PQ[t*(x) ¼ t(x)] ¼ 1.
n
The point of the theorem is that for any estimator t*(X) of q(Q) there always
exists an alternative estimator that is at least as good as t*(X) in terms of MSE
and that is a function of any set of sufﬁcient statistics. Thus, the Rao-Blackwell
theorem suggests that the search for estimators of q(Q) with low MSEs can
7.4
Sufﬁcient Statistics
401

always be restricted to an examination of functions of sufﬁcient statistics, where
hopefully the number of sufﬁcient statistics required to fully represent the
information about q(Q) is substantially less than the size of the random sample
itself.17 Note that if attention is restricted to the unbiased class of estimators, so
that t*(X) is an unbiased estimator in the statement of the theorem, then the Rao
Blackwell theorem implies that the search for a minimum variance estimator
within the class of unbiased estimators can also be restricted to functions of
sufﬁcient statistics. As an illustration, in Example 7.11, we know that Pn
i¼1 Xi is
a sufﬁcient statistic for f(x;Y). Thus, our search for estimators of Y with low
MSE can be conﬁned to functions of the sufﬁcient statistic, i.e. t(X) ¼ x Pn
i¼1 Xi


.
We note for future reference that x Pn
i¼1 Xi


¼ n1 Pn
i¼1 Xi


is the MVUE of y.
The Rao-Blackwell theorem can be extended to the vector case as follows:
Theorem 7.6
Rao-Blackwell
Theorem-Vector Case
Let S ¼ (S1,. . .,Sr) be an r-variate sufﬁcient statistic for f(x;Q), and let t*(X) be an
estimator of the (k  1) vector function q(Q) having a ﬁnite covariance matrix.
Deﬁne t(X) ¼ E(t*(X)|S1,. . .,Sr) ¼ h(S1,. . .,Sr). Then t(X) is an estimator of q(Q)
for which MSEQ(t(X))ﬃMSEQ(t*(X)) 8Q 2 O, the equality being attained only if
PQ(t(x) ¼ t*(x)) ¼ 1.
Proof
The proof is analogous to the proof in the scalar case, except that MSE matrices
are used in place of scalar MSEs in establishing that MSE(t(X)) is smaller than
MSE(t*(X)). The details are left to the reader.
n
The implications of Theorem 7.6 are analogous to those for the scalar case.
Namely, one need only examine vector functions of sufﬁcient statistics for
estimating the vector q(Q) if the objective is to obtain an estimator with a
small MSE matrix. Furthermore, the search for an MVUE of q(Q) can also be
restricted to functions of sufﬁcient statistics. As stated previously, this can
decrease substantially the dimensionality of the data used in a point estimation
problem if the minimal sufﬁcient statistics for the problem are few in number.
Revisiting Example 7.12 we note for future reference that
T ¼ x
X
n
i¼1
Xi
 
!
¼
Pn
i¼1 Xi


=n
n Pn
i¼1 Xi


 Pn
i¼1 Xi

2


=ðnðn  1ÞÞ
2
4
3
5
is the MVUE for (p, p(1p), the mean and variance of the Bernoulli population
distribution in the example.
17The reader will recall that the random sample, (X1,. . .,Xn), is by deﬁnition a set of sufﬁcient statistics for f(x;Y). However, it is clear
that no improvement (decrease) in the MSE of an unbiased estimator will be achieved by conditioning on (X1,. . .,Xn), i.e., the reader
should verify that this is a case where E(t*(X)  t(X))2 ¼ 0 and MSE equality is achieved in the Rao–Blackwell theorem.
402
Chapter 7
Point Estimation Theory

7.4.4
Complete Sufﬁcient Statistics, Minimality, and Uniqueness of Unbiased
Estimation
If a sufﬁcient statistic, S, has the property of being complete, then it is also a
minimal sufﬁcient statistic. Moreover, any unbiased estimator of q(Y) that is
deﬁned as a function of S is unique. We state the formal deﬁnition of complete-
ness, and then provide results on minimality and motivate the uniqueness of
unbiased estimators based on complete sufﬁcient statistics.
Deﬁnition 7.20
Complete Sufﬁcient
Statistics
Let S ¼ (S1,. . .,Sr) be a sufﬁcient statistic for f(x;Y). The sufﬁcient statistic S is
said to be complete iff the only real valued function t deﬁned on the range of S
that satisﬁes EY (t(S)) ¼ 0 8 Y 2 O is the function deﬁned as t(S) ¼ 0 with
probability 1 8 Y 2 O.
A complete sufﬁcient statistic condenses the random sample information as
much as possible in the sense of also being a minimal sufﬁcient statistic, as
formalized below.
Theorem 7.7
Completeness )
Minimal
s(X) is complete ) s(X) is minimal.
Proof
E. L. Lehmann and H. Scheffe’, (1950), op. cit.
n
Thus, if one ﬁnds a set of complete sufﬁcient statistics, one also knows there
is no smaller set of sufﬁcient statistics available for the statistical model being
analyzed. Furthermore, one knows that the search for complete sufﬁcient sta-
tistics can be limited to an examination of minimal sufﬁcient statistics.
If a sufﬁcient statistic, S, is complete, it follows that two different functions
of Scannothavethesameexpected value.Toseethis, supposethatE(t(S)) ¼ E(t*(S))
¼ q(Q),anddeﬁnet(S) ¼ t(S)  t*(S).ThenE(t(S)) ¼ 0,andsincet(S)isafunctionof
the complete sufﬁcient statistic S, it must be the case that t(s) ¼ t(s)  t*(s) ¼ 0
occurs with probability 1 for all Q. Thus, t(s) and t*(s) are the same function with
probability 1. An important implication of this resultis that any unbiasedestimator
of q(Q) that is a function of the complete sufﬁcient statistic is unique – there cannot
be more than one unbiased estimator of q(Q) deﬁned in terms of complete sufﬁcient
statistics. This uniqueness property leads to an important procedure for deﬁning
MVUEs that will be discussed in Section 7.5.
The following example illustrates the process of verifying the completeness
property of a sufﬁcient statistic. Veriﬁcation of the completeness property often-
times requires considerable ingenuity.
Example 7.14
Complete Sufﬁcient
Statistics for the
Binomial
Let (X1,. . .,Xn) be a random sample from the Bernoulli population distribution
pz(1p)1zI{0,1}(z) representing whether or not the administration of a particular
drug cures the disease of a patient, and suppose we wish to estimate q(p) ¼
p(1p) ¼ s2. Note that the joint density of the random sample in this case is
given by
7.4
Sufﬁcient Statistics
403

f x
ð Þ ¼ p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 xi Y
n
i¼1
If0;1g xi
ð
Þ:
Using the Neyman factorization criterion, we know that S ¼ Pn
i¼1 Xi is a
sufﬁcient statistic for f(x1,. . .,xn;p), since the joint density can be factored into
the product of
g s; p
ð
Þ ¼ ps 1  p
ð
Þns and h x1; . . . ; xn
ð
Þ ¼
Y
n
i¼1
I 0;1
f
g xi
ð
Þ:
To determine whether S ¼ Pn
i¼1 Xi is a complete sufﬁcient statistic we need to
determine whether the only real–valued function h deﬁned on the range of S that
satisﬁes E (t (S)) ¼ 0 8 p 2 [0,1] is the function deﬁned as t(S) ¼ 0 with probability
1 8 p 2 [0,1]. Note that since S has a binomial density we know that for a sample
size of n, E(t (S)) ¼ 0 8 p 2 [0,1] implies
EðtðSÞÞ ¼
Xn
i¼0 tðiÞ
n
i


pi 1  p
ð
Þni ¼ 0 8p 2 ½0; 1:
Let p 2 (0,1) and note, by dividing through by (1p)n, that the preceding summa-
tion condition can be rewritten as
xðzÞ ¼
Xn
i¼0 tðiÞ
n
i


zi ¼ 0;
which is a polynomial in z ¼ p/(1p). Differentiating n times with respect to z
yields
dn xðzÞ
dZn
¼ n!tðnÞ ¼ 0;
which implies that t(n) ¼ 0. Differentiating t(z) only (n1) times with respect to
z, and using the fact that t(n) ¼ 0 yields
dn1 xðzÞ
dzn1
¼ n!tðn  1Þ ¼ 0:
The process can be continued to ultimately lead to the conclusion thatt(i) ¼ 0 for
i ¼ 0,. . .,n is required. Thus, necessarily t(S) ¼ 0 with probability l if E(t(S)) ¼ 0,
and thus S ¼ Pn
i¼1 Xi is a complete sufﬁcient statistic for f(x1,. . .,xn;p).
□
7.4.5
Completeness in the Exponential Class
In general, the method used to verify completeness must be devised on a case-by-
case basis. However, the following theorem identiﬁes a large collection of
parametric families for which complete sufﬁcient statistics are relatively
straightforward to identify.
404
Chapter 7
Point Estimation Theory

Theorem 7.8
Completeness in the
Exponential Class
Let the joint density, f(x;Q), of the random sample X ¼ (X1,. . .,Xn)0 be a member
of a parametric family of densities belonging to the exponential class of
densities. If the range of c(Q) ¼ (c1(Q),. . .,ck(Q))0, Q
2
O, contains an open
k-dimensional rectangle, then g(X) ¼ (g1(X),. . .,gk(X))0 is a set of complete sufﬁ-
cient statistics for f(x;Q), Q 2 O.
Proof
See Lehmann, (1986) Testing Statistical Hypotheses, John Wiley, 1986, pp.
142–143 and Bickel and Doksum, (1977) Mathematical Statistics, Holden Day,
1977, pp. 123.
n
Theorem 7.8 implies that if we are dealing with a parametric family of
densities from the exponential class, once we verify that the range of c(Q)
contains an
open
k-dimensional rectangle,
we will have immediately
identiﬁed a set of complete sufﬁcient statistics, which are given by g(X) in
the exponential class representation. Regarding the open rectangle condition,
it will often be readily apparent from the deﬁnition of the range of c(Q)
whether there exists an open k-dimensional rectangle contained in R(c). Alter-
natively, it can be shown that R(c) will contain such an open rectangle if c(Q)
is continuously differentiable 8 Q in some open k-dimensional rectangle, G,
contained in the parameter space O and @c=@Y has full rank for at least one Q
2 G (Bartle, Real Analysis, pp. 381).
Example 7.16
Complete Sufﬁcient
Statistics for the Normal
Distribution
Let (X1,. . .,Xn) be a random sample from a normal population distribution with
mean m and variance s2 representing the package weights of a certain type of
cereal produced by General Mills. The joint density function for the random
sample is then given by
fðx1; . . . ; xn; m; sÞ ¼
1
2p
ð
Þn=2sn exp  1=2
ð
Þ
X
n
i¼1
xi m
s

2
 
!
:
The density is a member of the exponential class of density functions
exp
X
2
i¼1
ci ðm; sÞ gi ðxÞ þ dðm; sÞ þ zðxÞ
 
!
IA ðx1; . . . ; xnÞ
where c1 m; s
ð
Þ ¼ m=s2;
g1 x
ð Þ ¼ Pn
i¼1 xi; c2ðm; sÞ ¼ 1= 2s2


;
g2ðxÞ ¼ Pn
i¼1 x2
i ;
dðm; sÞ ¼ n=2
ð
Þ
m2=s2


þ ln 2ps2




; zðxÞ ¼ 0; and A ¼ n
i¼1 1; 1
ð
Þ.
Now note that c(m,s) ¼ (c1(m,s), c2(m,s)) has the set (1,1)  (1,0) for its
range since the range of c(m,s) in this case is, by deﬁnition,
RðcÞ ¼ fðc1; c2Þ : c1 ¼ m=s2; c2 ¼ 1= 2s2


; m 2 ð1; 1Þ; s 2 ð0; 1Þg:
The range of c(m,s) contains an open two-dimensional rectangle, i.e., there exists
a set of points {(x1, x2): ai<xi<bi, i ¼ 1,2}  R(c), and so we know from Theorem
7.8 that
Pn
i¼1 Xi; Pn
i¼1 X2
i


is a set of complete sufﬁcient statistics for the
multivariate normal family of densities in this example.
7.4
Sufﬁcient Statistics
405

As an alternative veriﬁcation of the open rectangle condition, note that c(m,s) is
continuously differentiable for all m 2 (1,1) and s > 0. Furthermore, letting
Q ¼ (m, s)0,
@c
@Q0 ¼
@c1
@m
@c2
@m
@c1
@s
@c2
@s
2
664
3
775 ¼
1
s2
0
 2m
s3
1
s3

3
75;
2
64
so that det @c=@Q0


¼ 1=s5>0; 8s>0: Thus @c=@Q has full rank 8m 2 (1,1) and
8s 2 (0,1) and the open rectangle condition is veriﬁed.
□
The reader might have noticed that in all of the preceding examples in which
sampling was from a population distribution belonging to the exponential class,
the joint density function for the random sample was also a member of the
exponential class of densities. This was not just a coincidence, as the following
theorem makes clear.
Theorem 7.9
Exponential Class
Population Implies
Exponential Class
Random Sample
Distribution
Let (X1,. . .,Xn) be a random sample from a population distribution that belongs
to the exponential class of density functions. Then the joint density function of
the random sample also belongs to the exponential class of density functions.
Proof
Suppose
Xj  f xj; Q


¼ exp
X
k
i¼1
ciðQÞgi xj


þ dðQÞ þ z xj


"
#
IA xj


8j:
Then
X1; . . . ; Xn
ð
Þ 
Y
n
j¼1
f xj; Q


¼ exp
X
k
i¼1
ciðQÞ
X
n
j¼1
gi xj


þ ndðQÞ þ
X
n
j¼1
z xj


"
#
P
n
j¼1 IA xj


¼ exp
X
k
i¼1
ciðQÞg	
i x1; . . . ; xn
ð
Þ þ d	ðQÞ þ z	 x1; . . . ; xn
ð
Þ
"
#
 IA	 x1; . . . ; xn
ð
Þ
where
g	
i x1; . . . ; xn
ð
Þ ¼
Xn
j¼1 gi xj


; d	ðQÞ ¼ ndðQÞ; z	 x1; . . . ; xn
ð
Þ ¼
Xn
j¼1 z xj


;
and A	 ¼ n
i¼1A:
Thus, the joint density function of (X1,. . .,Xn) belongs to the exponential class.n
406
Chapter 7
Point Estimation Theory

The importance of Theorem 7.9 is that if random sampling is from an
exponential class population distribution, then Theorem 7.8 might be poten-
tially useful for ﬁnding complete sufﬁcient statistics, since we know that the
joint density of the random sample is in fact in the exponential class of proba-
bility distributions.
7.4.6
Completeness, Minimality, and Sufﬁciency of Functions
of Sufﬁcient Statistics
Sufﬁcient statistics are not unique. In fact any one to one (i.e., invertible)
function of a sufﬁcient statistic S, say t(S), is also a sufﬁcient statistic, and if S
is complete or minimal, then it is also true that t(S) is complete or minimal,
respectively. This result is perhaps not surprising given that one can invert back
and forth between the two sets of statistics, using them interchangeably to
represent the same sample information. We formalize this observation in the
following theorem.
Theorem 7.10
Sufﬁciency of Invertible
Functions of Sufﬁcient
Statistics
Let S ¼ s(X) be a (r  1) sufﬁcient statistic for f(x;Y). If t(s(X)) is an (r  1)
invertible function of s(X),
a. t(s(X)) is a (r  1) sufﬁcient statistic for f(x;Y);
b. If s(X) is a minimal sufﬁcient statistic, then t(s(X)) is a minimal sufﬁcient
statistic;
c. If s(X) is a complete sufﬁcient statistic, then t(s(X)) is a complete sufﬁcient
statistic.
Proof
(a) If s(X) is sufﬁcient, then by Neyman factorization f(x;Y) ¼ g(s(x);Y) h(x).
Since t(s(X)) is invertible, it follows that s(x) ¼ t1(t(s(x))), so that
g(s(x);Y) ¼ g(t1(t(s(x));Y) ¼ g*(t(s(x));Y). Then by Neyman factorization,
since f(x;Y) ¼ g*(t(s(x));Y) h(x), t(s(X)) is a sufﬁcient statistic for f(x;Y).
(b) If
it
can
be
shown
that
t(s(X))
satisﬁes
the
conditions
of
the
Lehmann–Scheffe´ minimal sufﬁciency theorem, then we know that t is a
minimal sufﬁcient statistic. Note that t(s(x)) ¼ t(s(y)) , s(x) ¼ s(y) by the
invertibility of t. It follows that {(x,y): t(s(x)) ¼ t(s(y))} ¼ {(x,y): s(x) ¼ s(y)} so
that by Theorem 7.2 if S is a minimal sufﬁcient statistic, then so is t(S).
(c) Suppose E(h*(t(S))) ¼ 0 8Y 2 O. Since t(S) is invertible, it follows that
E(h○t1(t(S))) ¼ 0 8Y 2 O where h is such that h* ¼ h○t1. But since S is
complete, P(h(s) ¼ 0) ¼ P(h*(t(s)) ¼ 0) ¼ 1 8Y 2 O, so that t(S) is complete
by Deﬁnition 7.20.
n
The implication of Theorem 7.10 is that we can transform a set of sufﬁcient
statistics, via invertible functions, in any way that is useful or convenient. For
example, the minimal sufﬁcient statistic Pn
i¼1 Xi in Example 7.12 could be
alternatively deﬁned as Xn, the minimal sufﬁcient statistic
Pn
i¼1 Xi; Qn
i¼1 Xi


in Example 7.13 could be alternatively deﬁned as
Xn; Qn
i¼1 Xi

1=n


, and the
7.4
Sufﬁcient Statistics
407

complete sufﬁcient statistic
Pn
i¼1 Xi; Pn
i¼1 X2
i


in Example 7.16 could alterna-
tively be represented as
Xn; S2
n


. All of the alternative representations are
justiﬁed by Theorem 7.10, because each of the alternative representations can
be deﬁned via invertible functions of the original sufﬁcient statistics.
7.5
Minimum Variance Unbiased Estimation
Results that can be helpful in the search for a minimum variance unbiased
estimator (MVUE) of q(Q) are presented in this section. However, it should be
noted that MVUEs do not always exist, and when they do, MVUEs may be difﬁcult
to determine even with the aid of the theorems presented in this section. If the
results in this section are inapplicable or cannot be successfully applied in a given
estimation setting, there are numerous alternative approaches for deﬁning
estimators of q(Q) in a wide range of problems of practical interest. In Chapter 8
we will present a number of tractable procedures for deriving estimators of q(Q)
that often lead to estimators with good properties, and may also lead to MVUEs.
We begin with a theorem that identiﬁes necessary and sufﬁcient conditions
for an unbiased estimator of a scalar q(Q) to be a MVUE. Henceforth, we tacitly
restrict the class of estimators under consideration to those with ﬁnite variances
or ﬁnite covariance matrices since estimators with inﬁnite variances will
clearly not minimize variance, and in fact cannot even be compared on the
basis of variance.
Theorem 7.11
Scalar MVUEs -
Necessary and
Sufﬁcient Conditions
A necessary and sufﬁcient condition for an estimator T ¼ t(X) of the scalar q(Y)
to be a MVUE is that cov(T,W) ¼ 0, 8 Y 2 O and 8 W ¼ w(X) 2 uo, where uo is
the set of all functions of X that have expectations equal to 0.
Proof
Necessity
If T is unbiased for q(Q), and E(W) ¼ 0, then T + lW is unbiased for q(Q). Given Q
and l, the variance of T + lW equals var(T + WS) ¼ var(T) + [2l cov(T,W) + l2
var(W)]. Suppose cov(T,W) < 0. Then the bracketed term can be made negative,
and thus var(T + lW) < var(T), for l 2 0; 2cov T; W
ð
Þ=varðWÞ
ð
Þ. Alternatively,
suppose cov (T,W)>0. Then the bracketed term can be made negative, and thus
var(T + lW) < var(T), for l 2
2covðT; WÞ=varðWÞ; 0
ð
Þ. Thus, t(X) cannot be a
MVUE if cov(T,W) 6¼ 0.
Sufﬁciency
Let T* be any other unbiased estimator of q(Q). Then (T  T*) 2 uo, i.e., (T  T*)
has an expectation equal to zero. Suppose that 8Q 2 O and T* 2 uq(Q), where uq(Q)
is the class of unbiased estimators of q(Q),
cov T; T  T	
ð
Þ
ð
Þ ¼ E T  q Q
ð
Þ
ð
Þ T  T	
ð
Þ
ð
Þ ¼ E T T  T	
ð
Þ
ð
Þ ¼ E T2


 E TT	
ð
Þ
¼ 0;
408
Chapter 7
Point Estimation Theory

where we have invoked the condition in the theorem upon deﬁning w(X) ¼
t(X)t*(X). It follows that E(T2) ¼ E(TT*), and thus
cov T; T	
ð
Þ ¼ E TT	
ð
Þ  EðTÞE T	
ð
Þ ¼ E T2


 q Q
ð
Þ
ð
Þ2 ¼ varðTÞ
since both T and T* are unbiased for q(Q). The preceding result implies
varðTÞ
½
1=2 ¼
cov T; T	
ð
Þ
varðTÞ
ð
Þ1=2 var T	
ð
Þ
ð
Þ1=2
"
#
var T	
ð
Þ
ð
Þ1=2 ¼ r var T	
ð
Þ
ð
Þ1=2 
 var T	
ð
Þ
ð
Þ1=2
since the correlation coefﬁcient r 2 0; 1
½
 in this case. Thus T has minimum
variance.
n
The result in Theorem 7.11 facilitates the proof of the following proposition
that establishes important relationships between scalar and vector MVUEs.
Theorem 7.12
Relationships Between
Scalar and Vector
MVUEs
Let T ¼ t(X) be a (k  1) vector estimator of the (k  1) vector function q(Q).
Then any of the following statements implies the others:
1. T is a MVUE for q(Q).
2. Ti is a MVUE for qi(Q), i ¼ 1,. . .,k.
3. ℓ0T is a MVUE for ℓ0q(Q), 8ℓ6¼ 0.
Proof
We prove the theorem by demonstrating that (1)
,
(2) and (2)
,
(3).
Proof that (2) , (3):
Assume (2). By Theorem 7.11, for Z ¼ Pk
i¼1 ℓiTi to be a MVUE of Pk
i¼1 ℓi
qi(Q), it is necessary and sufﬁcient that cov(F,W) ¼ 0 8 W 2 uo and 8 Q 2 O. Note
that 8 W 2 uo,
covðZ; WÞ ¼ E
X
k
i¼1
ℓiTi 
X
k
i¼1
ℓiqiðQÞ
"
#
W
 
!
¼
X
k
i¼1
E ℓi Ti  qiðQÞ
½
W


¼
X
k
i¼1
ℓicov Ti; W
ð
Þ ¼ 0
where the last equality follows from the fact that Ti is a MVUE for qi(Q), and
thus by Theorem 7.11 cov(Ti,W) ¼ 0. It follows from Theorem 7.11 that Pk
i¼1 ℓi
Ti is a MVUE of Pk
i¼1 ℓi qi(Q), so that (2) ) (3).
Assume (3). Deﬁning ℓso that ℓ0T ¼ Ti (let ℓbe a vector of zeros except for a 1
in the ith position), Ti is a MVUE for qi(Q), i ¼ 1,. . .k. Thus, (3) ) (2).
Proof that (1) , (2):
Assume (1). Note that Ti ¼ ℓ0Tand var(Ti) ¼ ℓ0Cov(T)ℓ, whereℓis a zero vector
except for a 1 in the ith position. Clearly Ti is unbiased for qi(Q). Suppose
var(Ti) ¼ ℓ0Cov(T) ℓ> var (T	
i ) for some other unbiased estimator, T	
i , of qi(Q).
But then T could not be a MVUE of q(Q), since deﬁning T* equal to T, except for
T	
i replacing Ti, would imply ℓ0Cov(T)ℓ> ℓ0 Cov(T*)ℓ, contradicting the fact that
7.5
Minimum Variance Unbiased Estimation
409

Cov(T)  Cov(T*) would be negative semideﬁnite for all other unbiased
estimators T* if T were a MVUE for q(Q). Thus (1) ) (2).
Assume (2). Suppose Ti is a MVUE for qi(Q), i ¼ 1,. . .,k. Then clearly E(T) ¼
q(Q), so that the vector T is an unbiased estimator of q(Q). Now suppose there
existed another unbiased estimator, T*, of q(Q) such that ℓ0Cov(T*)ℓ< ℓ0Cov(T)ℓ
for some real vector ℓand for some Q∈O. Then ℓ0T* would be an unbiased
estimator of ℓ0q(Q) having a smaller variance than the unbiased estimator ℓ0T.
However, the latter estimator is a MVUE of ℓ0q(Q) by the proof of (2) , (3) above.
This is a contradiction, so that there is no Q for which ∃ℓsuch that ℓ0Cov(T*)ℓ<
ℓ0Cov(T)ℓ, i.e. ℓ0Cov(T)ℓ
 ℓ0Cov(T*)ℓ8ℓand 8 Q 2 O. But this implies that Cov(T)

 Cov(T*) 8 Q 2 O and for any unbiased estimator T*, so that T is the MVUE of
q(Q). Thus (2) ) (1).
n
An important implication of Theorem 7.12 is that it allows a vector MVUE
of q(Q) to be deﬁned elementwise with the knowledge that once each MVUE, Ti,
for qi(Q) has been individually deﬁned for i ¼ 1,. . .,k, then T ¼ (T1,. . .,Tk) is a
MVUE of q(Q) in the vector sense. In addition, the theorem indicates that
a MVUE for any linear combination of the entries in q(Q) is immediately
known to be ℓ0T.
Heretofore we have been referring to “a” MVUE of q(Q), rather than “the”
MVUE of q(Q). The following theorem implies that if we have discovered
a MVUE of q(Q), it is unique with probability one. We present the result for
the scalar case, and then use it to provide the extension to vector situations in
the next two theorems.
Theorem 7.13
Uniqueness of Scalar
MVUEs
If T and T* are both scalar MVUEs for q(Q), then P[t ¼ t*] ¼ 1.
Proof
If T and T* are both MVUE’s for q(Q), then necessarily var(T) ¼ var(T*) 8 Q 2 O.
From the sufﬁciency proof of Theorem 7.11, we know that (var (T))1/2 ¼
r(var (T*))1/2 where r is the correlation between T and T*. It also follows from
the
sufﬁciency
proof
of
Theorem
7.11
that
because
var(T) ¼ var(T*),
r ¼ varðTÞ=var T	
ð
Þ ¼ 1: Then from Theorem 3.36, r ¼ 1 implies that P[t ¼ a1 þ
bt*] ¼ 1 with a1 ¼ (mTsT*  mT*sT)/sT* ¼ 0 and b ¼ sT=sT	 ¼ 1; since mT ¼ mT*
¼ q(Q) and sT* ¼ sT, so that P(t ¼ t*) ¼ 1.
n
Theorem 7.14
Uniqueness of Vector
MVUEs
If the vector estimators T and T* are both MVUEs for the (k  l) vector q(Q),
then P[t ¼ t*] ¼ 1.
Proof
Let
Ai
represent
the
event
that
ti ¼
t	
i
for
i ¼ 1,
. . .,k.
Then
since
P \k
i¼1Ai


 1  Pk
i¼1 P Ai


by Bonferroni’s inequality,
P ti ¼ t	
i ; i ¼ 1; . . . ; k
	

¼ P t ¼ t	
½
  1 
X
k
i¼1
P ti 6¼ t	
i
	

¼ 1 
X
k
i¼1
0 ¼ 1
410
Chapter 7
Point Estimation Theory

where the next to last inequality follows from Theorem 7.13 and the fact that
both ti and t	
i are MVUEs for qi(Q).
n
From this point forward we will refer to the MVUE of q(Q) rather than a
MVUE since a MVUE of q(Q) is unique with probability 1.
7.5.1
Cramer-Rao Lower Bound on the Covariance Matrix of Unbiased Estimators
We now examine a lower bound for the covariance matrix of an unbiased estima-
tor of q(Q). If an unbiased estimator of q(Q) can be found whose covariance matrix
actually attains this lower bound, then the estimator is the MVUE of q(Q). The
bound is called the Cramer-Rao Lower Bound (CRLB), and its applicability relies
on a number of so-called regularity conditions (equivalently, assumptions) on the
underlying joint density function, f(x;Q), of the random sample under investiga-
tion. We state the regularity conditions below. The interpretation and application
of the regularity conditions can be quite challenging and the reader may wish to
skim the discussion of these conditions on ﬁrst reading.
Deﬁnition 7.21 CRLB
Regularity Conditions
1. The parameter space, O, of the family of densities to which f(x;Q) belongs
is an open rectangle.18
2. The support of f(x;Q), A ¼ {x: f(x;Q) > 0}, is the same 8Q 2 O.
3. @ ln fðx; QÞ
ð
Þ=@Qi exists and is ﬁnite 8i, 8 x 2 A and 8 Q 2 O.
4. 8i and j and 8 unbiased estimator t(X) of q(Q) having a ﬁnite covariance
matrix, one can differentiate under the integral or summation sign as
indicated below:
Continuous Case
a.
@
@Yj
Z 1
1
  
Z 1
1
fðx; QÞdx1 . . . dxn ¼
Z 1
1
  
Z 1
1
@fðx; QÞ
@Yj
dx1 . . . dxn
b.
@
@Yj
Z 1
1
  
Z 1
1
tiðxÞfðx; QÞdx1 . . . dxn
¼
Z 1
1
  
Z 1
1
tiðxÞ @fðx; QÞ
@Yj
dx1 . . . dxn
Discrete Case
a.
@
@Yj
X
x2A
f(x;Q) ¼
X
x2A
@fðx; QÞ
@Yj
b.
@
@Yj
X
x2A
ti(x) f(x;Q) ¼
X
x2A
ti(x) @fðx; QÞ
@Yj
18Recall that by open rectangle, we mean that the parameter space can be represented as O ¼ {(Q1,. . ., Qk): ai < Qi < bi, i ¼ 1,. . ., k},
where any of the ai’s could be 1 and any of the bi’s could be 1. This condition can actually be weakened to requiring only that the
parameter space be an open subset of ℝk, and not necessarily an open rectangle, and the CRLB would still apply.
7.5
Minimum Variance Unbiased Estimation
411

In applications, conditions 1–3 are generally not difﬁcult to verify, but
condition 4 can be problematic. Regarding regularity condition (4a) for the
continuous case, note that since
R 1
1 . . .
R 1
1 f(x;Q) dx ¼ 1 because f(x;Q) is a
density function, then the left–hand side of the equality in 4a is equal to zero.
Then a regularity condition that is equivalent to (4a) is
ð1
1
  
ð1
1
@fðx; QÞ
@Qj
dx ¼ 0; 8j:
Because @ ln fðx; QÞ=@Yj ¼ [f(x;Q)]1 @fðx; QÞ=@Yj 8 x 2 A, another regularity
condition equivalent to (4a) is given by
ð
x2A
@ ln fðx; QÞ
@Yj
fðx; QÞdx ¼ E @ ln fðx; QÞ
@Yj


¼ 0; 8j:
While sometimes mathematically challenging, condition (4a) can be veriﬁed
directly by demonstrating that one of the above functions of x equal zero,
based on the functional form of the joint density function for the random
sample. By replacing integration with summation, the preceding two equivalent
conditions sufﬁce to verify condition (4a) in the discrete as well.
Veriﬁcation of regularity condition (4b) is complicated by the fact that it
must hold true for all unbiased estimators of q(Q) with ﬁnite covariance matri-
ces, and can be a challenging exercise in real analysis. We present a sufﬁcient
condition for the validity of (4b) that is based on results in advanced calculus
relating to the validity of differentiating under the integral sign.
Theorem 7.15
Sufﬁcient Condition for
CRLB Regularity
Condition 4b
Let CRLB regularity conditions (1)–(4a) hold. Then regularity condition (4b)
holds if
E
@ ln fðX; jÞ
ð
Þ
@xj

2
"
#
<tðQÞ<1
8j; 8j 2 GðQÞ and 8Q 2 O, where G(Q) is an open rectangle containing Q.
Proof
Apostol, T.M., (1974) Mathematical Analysis, 2nd Ed., Addison-Wesley,
Reading, MA, pp. 167.
n
Note that since E @ ln fðX; jÞ
ð
Þ=@ xj


¼ 0 by condition (4a), Theorem 7.15 can
be interpreted as a boundedness condition on the variance of the random vari-
able @ ln fðX; jÞ
ð
Þ=@ xj 8j and 8Q 2 O. The following example illustrates the CRLB
regularity veriﬁcation process.
Example 7.17
Verifying CRLB
Regularity in an
Exponential Population
Let X ¼ (X1,. . .,Xn)0 be a random sample from an exponential population distri-
bution representing the waiting time between customer arrivals at a retail store,
so that
412
Chapter 7
Point Estimation Theory

fðx; yÞ ¼ yn exp 
X
n
i¼1
xi=y
 
!
P
n
i¼1 Ið0;1Þ ðxiÞ:
The parameter space, O, is an open interval since y > 0 for the exponential
density, verifying 1. The set, A, of x–values that satisfy f(x;y) > 0 is the same for
all y 2 O, i.e., A ¼ n
i¼1 (0,1), regardless of the value of Y > 0, verifying 2. Also,
@ ln fðx; yÞ
ð
Þ=@y ¼ n=y þ Pn
i¼1 xi=y2 exists and is ﬁnite 8 y
2 O and x
2
A,
verifying 3.
Regarding 4, ﬁrst note that
E @ ln fðX; yÞ
ð
Þ
@y


¼ E  n
y þ
Pn
i¼1 Xi
y2


¼ n
y þ n
y ¼ 0
since E Pn
i¼1 Xi


¼ n y, and thus 4a is met. Regarding 4b, note that
E
@ ln fðX; yÞ
ð
Þ
@y

2
 
!
¼ E
 n
y þ
Pn
i¼1 Xi
y2

2
 
!
¼ E n2
y2  2n Pn
i¼1 Xi
y3
þ
Pn
i¼1 Xi

2
y4
 
!
¼ n2
y2  2n2y
y3
þ 2ny2 þ n n  1
ð
Þy2
y4
¼ n
y2 ;
where the next to last equality follows from the fact that E(Xi
2)¼ 2 y 2 8i, and the
fact that the Xi’s are iid. It follows that whatever the value of y 2 O, there exists
an open interval G(y) ¼ (y  e, y + e) for some e > 0, and a positive number
t(y) ¼ n= y  e
ð
Þ2 such that
E
@ ln fðX; xÞ
ð
Þ
@y

2
"
#
¼ n
x2 <tðyÞ<1
8x 2 GðyÞ;
so that CRLB condition 4b is met. Thus, the CRLB regularity conditions are met
for this problem.
□
In practice, there are cases where it can be quite difﬁcult to verify all of the
CRLB regularity conditions. It is useful to note that for the large collection of
probability distributions in the exponential class discussed in Chapter 4, the
regularity conditions hold quite generally, as formalized below.
Theorem 7.16
CRLB Regularity in the
Exponential Class of
Distributions
Let f(x;Q) be a member of the exponential class of densities, as deﬁned in
Deﬁnition 4.3 and let c(Q) and d(Q) be continuously differentiable with,
@cðQÞ=@Q having full rank, for Q 2 O. If O is an open rectangle, then f(x;Q)
satisﬁes the CRLB regularity conditions.
Proof
A proof of the theorem can be constructed using the result in Lehmann (1986),
Testing Statistical Hypotheses, John Wiley, 1986, pp. 59-60.
n
Example 7.18
CRLB Regularity in
Exponential Class:
Geometric Distribution
Let (X1,. . .,Xn) be a random sample from a geometric population distribution
representing the number of customer contacts required for a salesperson to make
her ﬁrst sale, so that
7.5
Minimum Variance Unbiased Estimation
413

fðx; pÞ ¼ pn 1  p
ð
Þ
Pn
i¼1 xinY
n
i¼1
If1;2;3;...g ðxiÞ:
The PDF of the random sample is a member of the exponential class of
densities with c(p) ¼ ln(1p), d(p) ¼ ln[p/(1p)]n, g(x) ¼ Pn
i¼1 xi, z(x) ¼ 0, and
A ¼ n
i¼1 {1,2,3,. . .}. Note that @cðpÞ=@p ¼ 1= 1  p
ð
Þ 6¼ 0 and is continuous for
all p 2 (0,1), and @dðpÞ=@p ¼ n= pð1  pÞ
ð
Þ is continuous for all p 2 (0,1). Then
since O ¼ (0,1) is an open interval, f(x;p) satisﬁes the CRLB regularity conditions
by Theorem 7.16.
□
Based on the CRLB regularity conditions, we now present the CRLB. We
remind the reader that the notation AB indicates matrix A is greater than the
matrix B by a positive semideﬁnite (psd) matrix, i.e., A  B ¼ C , where the
matrix difference C is psd.
Theorem 7.17
Cramer-Rao Lower
Bound
Let t(X) be an unbiased estimator of the (k  1) vector function q(Q) and let t(X)
have a ﬁnite covariance matrix. Let the (k  m) Jacobian matrix @qðQÞ=@Q exist
8Q m1
ð
Þ 2 O. Assume the CRLB regularity conditions of Deﬁnition 7.21 hold,
where f(x;Q) is the joint density function of the random sample X. Then
CovQðtðXÞÞ @qðQÞ
@Q

0
EQ
@ ln fðX; QÞ
ð
Þ
@Q
@ ln fðX; QÞ
ð
Þ
@Q0



1 @qðQÞ
@Q


8Q 2 O;
provided the inverse matrix exists.
Proof
We prove the result for the continuous case. The discrete case is analogous, with
integrals replaced by summation, and is left to the reader.
First note that CRLB regularity condition 4a implies that (the Q-subscripts
on expectations are suppressed)
E @ ln fðX; QÞ
ð
Þ=@Yi
½
 ¼ 0; i ¼ 1; . . . ; m:
Then
deﬁning the (m  l) random vector
sðXÞ ¼ @ ln fðX; QÞ
ð
Þ=@Q1; . . . ; @ ln fðX; QÞ
ð
Þ=@Qm
½
0;
it follows that E(s(X)) ¼ 0 and
CovðsðXÞÞ ¼ EðsðXÞsðXÞ0Þ ¼ E @ ln fðX; QÞ
ð
Þ
@Q
@ ln fðX; QÞ
ð
Þ
@Q0


:
Now,
since
t(X)
is
an
unbiased
estimator
of
q(Q),
qi ðQÞ ¼
R 1
1 . . .
R 1
1 ti ðxÞfðx; QÞdx1 . . . dxn.
Then by CRLB regularity condition 4b,
@ qi ðQÞ
@ Yj
¼
ð1
1
. . .
ð1
1
ti ðxÞ @fðx; QÞ
@ Yj
dx1 . . . dxn;
¼
ð1
1
. . .
ð1
1
ti ðxÞ @ ln fðx; QÞ
ð
Þ
@ Yj
fðx; QÞdx1 . . . dxn
¼ E ti ðXÞ @ ln fðX; QÞ
ð
Þ
@ Yj


¼ E ti ðXÞ sj ðXÞ


for i; j ¼ 1; . . . ; m:
414
Chapter 7
Point Estimation Theory

Examine the covariance matrix of the (k + m)  1 random vector Z ¼
tðXÞ
sðXÞ


.
Since E(Z) ¼
qðQÞ
0


, it follows that
Cov Z
ð Þ ¼
CovðtðXÞÞ
E tðXÞsðXÞ0


E sðXÞtðXÞ0


CovðsðXÞÞ


¼
CovðtðXÞÞ
@qðQÞ0
@Q
@qðQÞ
@Q
CovðsðXÞÞ
2
64
3
75
Premultiply Cov(Z) by
D ¼
I
kk
 @qðQÞ0
@Q
½CovðsðXÞÞ 1
km
"
#
and
then
postmultiply
by
D0
to
obtain
D
Cov(Z)
D0 ¼ Cov(t(X)) 
@qðQÞ=@Q
½
0½CovðsðXÞÞ 1 @qðQÞ=@Q
½
  0 because Cov(Z) is necessarily psd, so
that D Cov(Z) D0 is positive semideﬁnite for any conformable matrix D.19 Then
given the deﬁnition of Cov(s(X)) above, the result of the theorem follows.
n
An alternative form of the CRLB that utilizes second order partial
derivatives of f(x;Q) with respect to Q is sometimes easier to use in practice.
Its applicability relies on an additional regularity condition beyond the
conditions assumed for the existence of the CRLB, and is presented in the next
theorem.
Theorem 7.18
Cramer–Rao Lower
Bound: Hessian Form
Assume the conditions of Theorem 7.17 hold, so that the Cramer-Rao Lower
Bound applies. Then if
ð1
1
. . .
ð1
1
@2 fðx; QÞ
@Yi@Yj
dx1 . . . dxn ¼ 0; 8i and j;
the CRLB can be represented alternatively as
CovQðtðXÞÞ 
@qðQÞ
@Q

0
EQ
@ ln fðX; QÞ
ð
Þ
@Q@Q0



1 @qðQÞ
@Q


8Q 2 O:
Proof
Comparing the two versions of the CRLB, it is clear that for the alternative
version to apply, we must have
E @ ln fðX; QÞ
@Q
@ ln fðX; QÞ
@Q0


¼ E @2 ln fðX; QÞ
@Q@Q0
 
!
:
19This can be easily seen, since ℓ0D Cov(Z) D’ℓ¼ ℓ0* Cov(Z) ℓ*  0, where ℓ	 ¼ D0ℓ; 8 ℓ:
7.5
Minimum Variance Unbiased Estimation
415

Note that
E @2 ln fðX; QÞ
@ Yi @ Yj
 
!
¼ E
@
@ Yj
1
fðX; QÞ
@fðX; YÞ
@ Yi




¼ E
1
fðX; QÞ
@2 fðX; QÞ
@ Yi @ Yj

1
ðfðX; QÞ Þ2
@fðX; QÞ
@ Yi
@fðX; QÞ
@ Yj
 
!
¼ E
1
fðX; QÞ
@2 fðX; QÞ
@ Yi @ Yj
 @ ln fðX; QÞ
@ Yi
@ ln fðX; QÞ
@ Yj
 
!
; 8i and j:
Proceeding in the continuous case, if
E
1
fðX; QÞ
@2 fðX; QÞ
@ Yi @ Yj
"
#
¼
ð1
1
. . .
ð1
1
@2 fðx; QÞ
@ Yi @ Yj
dx1 . . . dxn ¼ 0
8i; j;
then we can replace the matrix being inverted in Theorem 7.17 by the expecta-
tion of the negative of the second order partial derivative matrix of ln f(x;Q) with
respect to Q, justifying the alternative form of the CRLB. The discrete case
follows by replacing integrals with summation.
n
Regarding the additional regularity condition stipulated in Theorem 7.18,
note that if
@
@Yj
ð1
1
. . .
ð1
1
@fðx; QÞ
@Yi
dx1 . . . dxn ¼
ð1
1
. . .
ð1
1
@2 fðx; QÞ
@Yi@Yj
dx1 . . . dxn
8i and j,
then the integral conditions required in the theorem are met. This follows
because the CRLB regularity conditions imply that the left hand side of the
equality must be zero, because
ð1
1
. . .
ð1
1
@fðx; QÞ
@Yi
dx1 . . . dxn ¼
@
@Yi
ð1
1
. . .
ð1
1
fðx; QÞdx1 . . . dxn
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
1
¼
@
@Yi
ð1Þ ¼ 0 :
Thus,
the
alternative
CRLB
applies
if
we
can
differentiate
Ð 1
1 . . .
Ð 1
1 fðx; QÞ dx1 . . . dxn under the integral sign twice.
We note that verifying the applicability of the alternative form of the CRLB
is generally easier if the joint density of the random sample is a member of the
exponential class of distributions, because the functional form of the density
ensures the additional regularity condition of Theorem 7.16 if some straightfor-
ward differentiability properties are met. The following theorem formalizes the
result.
Theorem 7.19
CRLB Alternative
Hessian Form in the
Exponential Class
If the joint probability density of the random sample, f(x;Q), is a member of the
exponential class of densities satisfying Theorem 7.16, and if d(Q) and c(Q) are
twice continuously differentiable with respect to Q 2 O, then the alternative
Hessian form of the CRLB presented in Theorem 7.18 is valid.
416
Chapter 7
Point Estimation Theory

Proof
A proof of the theorem can be constructed using the result in Lehmann, (1986),
Testing Statistical Hypotheses, John Wiley, pp. 59–60.
n
If f(x;Q) is not in the exponential class, the admissibility of differentiating
under the integral sign a second time would need to be established, or else it
would need to be veriﬁed by direct evaluation that the integrals of the second
order derivatives of f(x;Q) presented in Theorem 7.18 are indeed zero. For the
sake of illustration, we provide an example of how the direct evaluation
process would proceed using a normal distribution, with s2 ¼ 1, as the popula-
tion distribution. The reader will note that since the normal distribution is a
member of the exponential class satisfying Theorem 7.16, and since @2cðYÞ=@Y2
and @2d Y
ð
Þ=@Y2 exist and are continuous, we already know that the outcome of
our evaluation process will result in the alternative form of the CRLB being
applicable.
Example 7.19
Direct Evaluation
of Applicability of
Hessian Form of CRLB:
Normal Case
Let X ¼ (X1,. . .,Xn) be a random sample from a normal population distribution
with s2 ¼ 1, so that
fðx; mÞ ¼
1
2p
ð
Þn=2 e 1=2
ð
ÞPn
i¼1 xim
ð
Þ2:
Note that
@fðx; mÞ
@m
¼ fðx; mÞ
X
n
i¼1
xi  m
ð
Þ;
and
@2 fðx; mÞ
@ m2
¼ nfðx; mÞ þ
X
n
i¼1
xi  m
ð
Þ
"
#2
fðx; mÞ:
It follows that
ð1
1
. . .
ð1
1
@2fðx; mÞ
@ m2
dx1 . . . dxn ¼ n
ð1
1
. . .
ð1
1
fðx; mÞ dx1 . . . dxn
þ
ð1
1
. . .
ð1
1
S
n
i¼1ðxi m Þ2 fðx; mÞ dx1 . . . dxn
¼ n þ n ¼ 0
since s2
i ¼ 1 8i, and so the alternative form of the CRLB is applicable. Thus, in
this case
E
@ ln fðX; mÞ
@m

2
 
!
¼ E @2 ln fðX; mÞ
@m2
 
!
¼ EðnÞ ¼ n:
7.5
Minimum Variance Unbiased Estimation
417

If interest centers on estimating m (as opposed to estimating some function of m),
then q(m) ¼ m in the statement of the CRLB in Theorem 7.17, so that
∂q(m)/∂m ¼ 1, and the CRLB for the variance of any unbiased estimator of m is
equal to n1.
□
Note that when the equality
E
@ ln fðX; QÞ
@ Yj

2
 
!
¼ E @2 ln fðX; QÞ
@ Y2
j
 
!
is true, the latter expectation expression can be substituted into the previously
mentioned sufﬁcient conditions for checking the validity of CRLB regularity
condition 4b (Theorem 7.15). In particular, the substitution would imply that
E @2 ln f x; j
ð
Þ=@Y2
j
h
i
<tðQÞ<1 8j
in an open rectangle G(Q) containing Q,
8Q2O, is the condition that would need to be veriﬁed.
Two special cases of the CRLB often arise in practice. The ﬁrst concerns the
case where q(Y) is a scalar function of a scalar parameter Y.
Corollary 7.3 CRLB for
Scalar Function Case
Let k ¼ m ¼ 1 in Theorem 7.17. Then the unbiased estimator T ¼ t(X) of the
scalar function q(Y) of the scalar parameter Y is such that
varY ðtðXÞÞ 
dqðYÞ=dY
ð
Þ2
EY
d fðX; YÞ
ð
Þ=dY
ð
Þ2

 8Y 2 O:
The second special case is when q(Q) ¼ Q. Since in this case, @qðQÞ=@Q ¼ I, we
have the following result:
Corollary 7.4 CRLB for
Estimating the Full
Parameter Vector
Let q(Q) ¼ Q in Theorem 7.17. Then the unbiased estimator T ¼ t(X) of the
vector Q is such that
CovQðtðXÞÞ EY
@ ln fðX; QÞ
ð
Þ
@Q
@ ln fðX; QÞ
ð
Þ
@Q0



1
; 8Q 2 O:
We now turn to examples of the use of the CRLB to discover MVUEs.
Example 7.19
Using CRLB to Deﬁne
MVUES: Bernoulli
Population
Let X ¼ (X1,. . .,Xn) be a random sample from a Bernoulli population distribution
representing whether or not a vaccine developed by a pharmaceutical company
prevents the ﬂu, so that
fðx; pÞ ¼ p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 xi Y
n
i¼1
I 0;1
f
g xi
ð
Þ:
In order to ﬁnd the MVUE of the probability that the vaccine prevents the ﬂu,
ﬁrst note that the support of f(x;p) does not depend on p 2 (0,1), and we
418
Chapter 7
Point Estimation Theory

concentrate attention on p 2 (0,1) to achieve the open rectangle regularity
condition on the parameter space20. Also, f(x;p) is continuous in p, and
ln fðx; pÞ ¼
X
n
i¼1
xi lnðpÞ þ
n 
X
n
i¼1
xi
 
!
ln 1  p
ð
Þ þ ln
Y
n
i¼1
I 0;1
f
g xi
ð
Þ
"
#
so that
d ln fðx; pÞ
dp
¼
Pn
i¼1 xi
p

n  PN
i¼1 xi


ð1  pÞ
exists and is ﬁnite 8 p 2 (0,1). Regarding CRLB regularity condition (4) note that
since df x; p
ð
Þ=dp ¼ f x; p
ð
Þ d ln f x; p
ð
Þ
ð
Þ=dp
ð
Þ exists 8 p 2 (0,1), and since we are
dealing with ﬁnite sums when deﬁning expectations, condition 4 is met.
Note further that
d2 ln fðx; pÞ
ð
Þ
dp2
¼  Pn
i¼1 xi
p2
 n  Pn
i¼1 xi


1  p
ð
Þ2
also exists, and since
P1
x1¼0    P1
xn¼0 df x; p
ð
Þ=dp
is a sum involving a
ﬁnite
number
of
terms,
differentiation
under
the
summation
signs
a
second time is permissible, so that substitution of the alternative form
E d2 ln f X; Y
ð
Þ
ð
Þ=dY2


in the CRLB, as discussed above, is allowed. Note
E d2 ln fðX; pÞ
ð
Þ
dp2
 
!
¼ np
p2  ðn  npÞ
1  p
ð
Þ2 ¼ n
p 
n
1  p ¼ nð1  pÞ  np
pð1  pÞ
¼
n
pð1  pÞ
and thus the CRLB for the variance of an unbiased estimator of q(p) ¼ p is given
by p(1p)/n. Becausevar Xn


¼ p 1  p
ð
Þ=n, andE Xn


¼ p; Xn is the MVUE of the
probability of ﬂu prevention, p.
□
Example 7.21
Using CRLB to Deﬁne
MVUES: Exponential
Population
Let X ¼ (X1,. . .,Xn) be a random sample from an exponential population distri-
bution representing the waiting time between customer arrivals at a retail store,
so that
f x; y
ð
Þ ¼ ynePn
i¼1 xi=y Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ:
To ﬁnd the MVUE of the mean waiting time between customers, y, ﬁrst note
that f(x; y) is a member of the exponential class, i.e.,
f x; y
ð
Þ ¼ ec y
ð Þg x
ð Þþd y
ð Þþz x
ð ÞIA x
ð Þ;
20Note, this rules out the degenerate cases p ¼ 1 or p ¼ 0, in which all sample observations would then be 1s and 0s, respectively.
If such were the case for the outcome of any given random sample from the Bernoulli distribution, the best, and in fact only reasonable
estimates of the parameter p would be 1 and 0, respectively, since there would be no sample variability with which to conclude
anything different.
7.5
Minimum Variance Unbiased Estimation
419

with c y
ð Þ ¼ y1; g x
ð Þ ¼ Pn
i¼1 xi; d y
ð Þ
¼ ln yn
ð
Þ; zðxÞ
¼ 0; A ¼ n
i¼1 0; 1
ð
Þ and
since d(y) and c(y) are twice continuously differentiable and dc y
ð Þ=dy 6¼ 0 for
y > 0, we know by Theorem 7.16 that the CRLB regularity conditions are met,
and also that the alternative form of the CRLB applies. Note
ln f x; y
ð
Þ
ð
Þ ¼ n ln y
ð Þ 
X
n
i¼1
xi=y þ
X
n
i¼1
ln I 0;1
ð
Þ xi
ð
Þ


;
so that
d ln f x; y
ð
Þ
ð
Þ
dy
¼ n
y þ
Pn
i¼1 xi
y2
and d2 ln f x; y
ð
Þ
ð
Þ
dy2
¼ n
y2  2 Pn
i¼1 xi
y3
;
and thus
E d2 ln fðx; yÞ
ð
Þ
dy2
 
!
¼ n
y2  2ny
y3 ¼ n
y2 :
The CRLB for the variance of an unbiased estimator of y is then y2=n. Since
E Xn


¼ y and var(Xn) ¼ y2=n, it follows thatXn is the MVUE of the mean waiting
time, y.
□
7.5.2
CRLB Attainment
The use of the CRLB for identifying the MVUE of q(Q) will only be useful if there
actually exists an unbiased estimator whose covariance matrix equals the CRLB.
A natural question to ask is under what conditions an unbiased estimator, T, of
q(Q) will have a covariance matrix that actually achieves the CRLB? It turns out
that the CRLB is achieved only when the deﬁnition of the estimator T has the
special form given in the following theorem.
Theorem 7.20
Attainment of the CRLB
Assume the CRLB regularity conditions hold, and let T ¼ t(X) be an unbiased
estimator of q(Q). Then Cov(t(X)) equals the CRLB iff
tðXÞ ¼ qðQÞ þ @qðQÞ0
@Q
E @ ln f X; Q
ð
Þ
ð
Þ
@Q
@ ln fðX; QÞ0


@Q



1 @ ln fðX; QÞ
ð
Þ
@Q
with probability 1.
Proof
Referring to the proof of the CRLB, the covariance matrix of t(X) will equal the
CRLB iff Cov(Y) ¼ 0, where the random variable Y is deﬁned as
Y ¼
I
@qðQÞ
@Q
Cov s X
ð Þ
ð
Þ
½
1


tðXÞ
sðXÞ


and sðXÞ ¼ @ ln fðX; QÞ
ð
Þ
@Q
and is the random variable that has the covariance matrix D Cov(Z) D0 in the
proof of Theorem 7.17.
420
Chapter 7
Point Estimation Theory

Since E(Y) ¼ E(t(X)) ¼ q(Q) because E(s(X)) ¼ 0, then setting Cov(Y) ¼ 0
for the attainment of the CRLB, implies that P(y ¼ q(Q)) ¼ 1, which in turn
implies P tðxÞ  @q Q
ð
Þ=@Q
ð
Þ0 Cov sðXÞ
ð
Þ
½
1sðXÞ ¼ q Q
ð
Þ


¼ 1. Upon substitution
for [Cov(s(X))]1 and s(X), the result is proved.
n
The CRLB attainment theorem leads to an explicit constructive procedure
for deriving the MVUE of q(Q) when it exists and when the CRLB exists.
Namely, given a probability model {f(x;Q), Q 2 O}, deﬁne
tðXÞ ¼ q Q
ð
Þ þ @q Q
ð
Þ0
@Q
E @ ln f X; Q
ð
Þ
ð
Þ
@Q
@ ln f X; Q
ð
Þ0


@Q



1 @ ln f X; Q
ð
Þ
ð
Þ
@Q
as a tentative estimator for q(Q), and if the expression on the right hand side of
the equality does not depend on Q, i.e. a statistic is deﬁned, then t(X) is the
MVUE for q(Q) since then t(X) is an estimator that achieves the CRLB.
Note,
unbiasedness
of
t(X)
follows
immediately
from
the
fact
that
E @ ln f X; Q
ð
Þ
ð
Þ=@Q
½
 ¼ 0 by CRLB regularity condition 4a.
Example 7.22
Using CRLB Attainment
to Deﬁne MVUEs:
Exponential Population
Reexamine Example 7.21. Because we are estimating q(y), we know that
dq y
ð Þ=dy ¼ 1: We also know from the example that d ln f X; y
ð
Þ
ð
Þ=@y ¼ n=y
ð
Þ
þ Pn
i¼1 Xi=y2


, and that
E d ln fðX; yÞ
ð
Þ
dy
d ln fðX; yÞ0


dy


¼ E d2 ln fðX; yÞ
ð
Þ
dy2
 
!
¼ n
y2:
Then
using
Theorem
7.20,
we
deﬁne
t(X)
as
tðXÞ ¼ y þ y2
n
n
y þ
Pn
i¼1 Xi
y2


¼
Pn
i¼1 Xi
n
¼ X,
and since t(X) is a statistic, t(X) is the MVUE of y.
□
It turns out that we can be speciﬁc about the class of parametric families of
density functions that will both satisfy the CRLB regularity conditions, so that
the CRLB applies as a lower covariance bound to unbiased estimators, and also
allow an unbiased estimator of q(Q) to achieve the CRLB. Speciﬁcally, the
exponential class of densities contains the parametric families of densities for
which the CRLB is attained for an unbiased estimator of some q(Q).
Theorem 7.21
Exponential Class and
CRLB Attainment
Let f(x;Q) satisfy the CRLB regularity conditions, and suppose there exists an
unbiased estimator of q(Q) that achieves the CRLB. Then f(x;Q) belongs to the
exponential class of density functions.
Proof
The proof for the scalar parameter case can be found in Bickel and Doksum,
(1977) Mathematical Statistics, Holden–Day, pp. 130–131. A discussion of the
multivariate case can be found in Zacks, (1971) The Theory of Statistical
7.5
Minimum Variance Unbiased Estimation
421

Inference, John Wiley, pp. 194–201, and Cencov, C.C., (1982) Statistical Deci-
sion Rules and Optimal Inference, American Mathematical Society, pp.
219–225.
n
Theorem 7.21 suggests that the procedure for deﬁning the MVUE for q(Q)
implied by Theorem 7.20 has potential for success only for probability models
where the joint density of the random sample belongs to the exponential class.
7.5.3
Asymptotic Efﬁciency and the CRLB
To this point, we have been using the CRLB concept in the context of examining
ﬁnite sample properties of estimators. The CRLB also plays a prominent role in
deﬁning the concept of an asymptotically efﬁcient estimator of q(Q), which
relies on the properties of consistency and asymptotic normality as the follow-
ing deﬁnition indicates.
Deﬁnition 7.22
Asymptotic Efﬁciency
or Best Asymptotically
Normal (BAN)
Tn ¼ tn(X) is an asymptotically efﬁcient estimator of q(Q) iff tn(X) is a consis-
tent estimator of q(Q) and
@q Q
ð
Þ
@Q

0
E @In f X; Q
ð
Þ
@Q
@In f X; Q
ð
Þ
@Q0



1 @q Q
ð
Þ
@Q


 
!1=2
Tn  q Q
ð
Þ
ð
Þ !
d N 0; I
ð
Þ;
so that
Tn 
a N
q Q
ð
Þ; @q Q
ð
Þ
@Q

0
E @In f X; Q
ð
Þ
@Q
@In f X; Q
ð
Þ0
@Q



1 @q Q
ð
Þ
@Q


 
!
:
Thus, if Tn is asymptotically efﬁcient, it has an asymptotic normal distribu-
tion with mean q(Q) and covariance matrix equal to the CRLB, and Tn is then
approximately MVUE for q(Q), based on the characteristics of its asymptotic
(or approximate) normal probability distribution. Also note the result by
L. LeCam21 which states that an asymptotically efﬁcient estimator in the
CAN class has an asymptotic covariance matrix which is smaller than
the asymptotic covariance matrix of any other estimator of q(Q) in the CAN
class, except, perhaps, on a set of Q values having Lebesque measure zero
In order to illustrate the asymptotic efﬁciency concept, we demonstrate that
in the case of sampling from an exponential density function (recall Example
7.21)Xn satisﬁes the conditions of Deﬁnition 7.22, and is thus an asymptotically
efﬁcient estimator of Q.
21LeCam, L., (1953) On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayesx Estimates, University of
California Publications in Statistics, pp. 1:277–330.
422
Chapter 7
Point Estimation Theory

Example 7.23
Asymptotically Efﬁcient
Estimator of Exponential
Mean
Recall Example 7.21. Since E(Xn) ¼ y, so that Xn is unbiased, and since var(Xn) ¼
y2=n ! 0 as n ! 1, then Xn!
m y, which is sufﬁcient for plim(Xn) ¼ y and so Xn is
a consistent estimator of y. The CRLB for unbiasedly estimating Y in this
case is given by y2=n (again, recall Example 7.21). By the Lindberg–Levy CLT,
Zn ¼ (Xn y)/(n1/2 y) !
d N(0,1) and thus Xn is in the CAN class of estimators.
Then by Deﬁnition 7.22, Xn is an asymptotically efﬁcient estimator of y.
The asymptotic distribution of Xn can be written as Xn 
a N(y, y 2/n).
□
There is a generic way of deﬁning asymptotic probability distributions for
asymptotically efﬁcient estimators which is inherent to the deﬁnition of asymp-
totic efﬁciency. In particular, the asymptotic distribution of an asymptotically
efﬁcient estimator can always be speciﬁed as normal, with a mean equal to
whatever is being estimated, and covariance matrix equal to the CRLB.
7.5.4
Complete Sufﬁcient Statistics and MVUEs
If complete sufﬁcient statistics exist for the statistical model {f(x;Q), Q 2 O},
then an alternative to the CRLB approach is available to aid in the search for the
MVUE of q(Q). The approach is based on the Lehmann-Scheffe´ completeness
theorem.
Theorem 7.22
Lehmann–Scheffe´
Completeness Theorem
for MVUEs
Let S1,. . .,Sr be a set of complete sufﬁcient statistics for f(x;Q). Let t(S1,. . .,Sr) be
an unbiased estimator for the (k  1) vector function q(Q). Then T ¼ t(S1,. . .,Sr)
is the MVUE of q(Q).
Proof
Let S be an r–variate complete sufﬁcient statistic and let t1(S) ¼ E(t1*(X)|S1,. . .,Sr)
and t2(S) ¼ E(t2*(X)|S1,. . .,Sr) be two unbiased estimators of q(Q) that have been
deﬁned by application of the Rao-Blackwell procedure to any two unbiased
estimators of q(Q) given by t1*(X) and t2*(X). Then letting t(S) ¼ ℓ0(t1(S)  t2(S))
for any conformable ℓ-vector, it follows from unbiasedness that
Eðt S
ð ÞÞ ¼ ℓ0E t1 S
ð Þ  t2 S
ð Þ
ð
Þ ¼ ℓ0 E t1 S
ð Þ
ð
Þ  Eðt2 S
ð Þ
ð
ÞÞ ¼ 0 8 Q 2 O:
But since S is a set of complete sufﬁcient statistics, necessarily t(S) ¼ ℓ0[t1(S) 
t2(S)] ¼ 0 with probability one because ℓ0[t1(S)  t2(S)] is a function of the set of
complete sufﬁcient statistics that has an expectation of zero 8Q 2 O. Since ℓcan
be chosen to have all zero entries except for a 1 in any position, it follows that the
(k  1) vectors t1(S) and t2(S) have the same k elements, and thus are the same
unbiased estimator, say t(S), of q(Q) with probability one, regardless of the choice
of unbiased estimators t1*(X) and t2*(X) used in the deﬁnitions of t1(S) and t2(S),
respectively. Then t(S) ¼ E(t*(X)|S1,. . .,Sr) must be the MVUE of q(Q), regardless
of the choice of unbiased estimator t*(X), because Cov(t(S))  Cov(t*(X)) for any
choice of unbiased estimator t*(X) by the Rao–Blackwell theorem.
Wenowshowthatregardlessoftheprocedureusedtodeﬁnet(S),ifE(t(S)) ¼ q(Q),
then t(S) is the MVUE of q(Q). Let t(S) be an unbiased estimator for q(Q). Note that
7.5
Minimum Variance Unbiased Estimation
423

by deﬁnition, E(t(S)|S) ¼ t(S). Then by the argument presented above, t(S) is the
MVUE of q(Q) (regardless of how we were led to the deﬁnition of t(S)).
n
The point of the Lehmann–Scheffe´ completeness theorem is that if a set of
complete sufﬁcient statistics exist for f(x;Q), then our search for the MVUE of
q(Q) is complete when we have found a function of the set of complete sufﬁcient
statistics whose expectation is q(Q). Viewed another way, Theorem 7.22 implies
that if S is complete, then for any function of S, say t(S), we have that t(S) is the
MVUE for its own expectation, i.e., t(S) is the MVUE for E(t(S)).
The Lehmann–Scheffe´ completeness theorem then suggests a least two
procedures for deﬁning the MVUE of q(Q) if a set of complete sufﬁcient
statistics, S, exists, which we delineate in the following deﬁnition of the
Lehman-Scheffe´ Completeness Approach for ﬁnding MVUEs.
Deﬁnition 7.23
Lehman-Scheffe´
Completeness
Approach for Finding
MVUEs
The following methods based on complete sufﬁcient statistics S lead to
MVUEs for a function, q(Q), of a parameter vector Q.
Method 1. Find a statistic of the form t(S) for which E(t(S)) ¼ q(Q). Then
t(S) is necessarily the MVUE of q(Q).
Method 2. Find any unbiased estimator of q(Q), say t*(X). Then t(S) ¼
E(t*(X)|S) is the MVUE of q(Q).
We emphasize that in either method in the preceding deﬁnition, t*(X) and
q(Q) can be conformable vectors.
Example 7.24
Finding MVUEs Using
Complete Sufﬁcient
Statistics
Recall Examples 7.15 and 7.16. Since Pn
i¼1 Xi is a complete sufﬁcient statistic for
the probability model of Example 7.15, and since T ¼ t Pn
i¼1 Xi


¼ n1 Pn
i¼1 Xi


¼ x is such that E(T) ¼ p, we know by the Lehmann-Scheffe´ completeness
theorem that X is the MVUE for p. In Example 7.16, Pn
i¼1 Xi and Pn
i¼1 X2
i , or
alternatively by Theorem 7.10,X and nS2/(n1) are complete sufﬁcient statistics
for the probability model. Since E X
 
¼ m and E(nS2(n1)) ¼ s2, it follows by
the Lehmann-Scheffe´ Completeness Theorem that (X, nS2/(n1)) is the MVUE
for (m, s2).
□
Keywords, Phrases, and Symbols
A  B, Matrix A is no larger than
matrix B
Alternative form of the CRLB
an estimate of Q or q(Y)
Asymptotic efﬁciency
Asymptotic MSE
Asymptotic relative efﬁciency
Asymptotically relatively more
efﬁcient
Asymptotically unbiased
Best Linear Unbiased Estimator,
BLUE
Bias
Bias matrix
Bias vector
Biased estimator
Complete sufﬁcient statistics
Completeness in the exponential
class
Consistency
Consistent estimator
Cramer-Rao Lower bound, CRLB
CRLB regularity conditions
Distinct PDFs
Distribution-free case
Distribution-speciﬁc case
424
Chapter 7
Point Estimation Theory

Efﬁcient
Estimand
Estimating Q or q(Q)
Estimator admissibility
Estimator, MVLUE
Estimator, MVUE
Exponential class and sufﬁcient
statistics
h○t (composite function)
Lehmann-Scheffe´ completeness
theorem
Lehmann-Scheffe´ minimal
sufﬁciency
Lehmann-Scheffe MVUE approach
Mean square error matrix
Mean square error, MSE
Minimal sufﬁcient statistics
Minimum variance linear unbiased
Minimum variance unbiased
Neyman factorization theorem
Nonparametric estimation
Parameter identiﬁability
Parametric estimation
Point estimate
point estimator
Range of X over the parameter space
O, RO(X)
Rao-Blackwell theorem
Relative efﬁciency, relatively more
efﬁcient
Statistical model
Strong means square error (SMSE)
criterion
Sufﬁcient statistics
Ti, T(n)
True PDF of X
True value of q(Q)
True value of Y
Unbiased estimator
Problems
1. The operating life of a certain type of math coproces-
sor installed in a personal computer can be represented as
the outcome of a random variable having an exponential
density function, as
Z  fðz; yÞ ¼ 1
y ez=y Ið0;1Þ ðzÞ;
where z ¼ the number of hours the math coprocessor
functions until failure, measured in thousands of hours.
A random sample, X ¼ (X1,. . .,Xn), of the operating
lives of 200 coprocessors is taken, where the objective
being is to estimate a number of characteristics of the
operating life distribution of the coprocessors. The out-
come of the sample mean was x ¼ 28:7.
a. Deﬁne a minimal sufﬁcient statistic for f(x;y), the
joint density of the random sample.
b. Deﬁne a complete sufﬁcient statistic for f(x; y).
c. Deﬁne the MVUE for E(Z) ¼ y if it exists. Estimate y.
d. Deﬁne
the
MVUE
for
var(Z) ¼ y2
if
it
exists.
Estimate y2.
e. Deﬁne
the
MVUE
for
E(Z2) ¼ 2y2
if
it
exists.
Estimate 2y2.
f. Deﬁne the MVUE for q y
ð Þ 31
ð
Þ ¼
y
y2
2y2
2
4
3
5 if it exists.
Estimate q(y).
g. Is the second sample moment about the origin, i.e.,
M02 ¼ Pn
i¼1 X2
i =n, the MVUE for E(Z2)?
h. Is the sample variance, S2, the MVUE for var(Z)?
i. Suppose we want the MVUE for F(b) ¼ P(z 
 b) ¼
1eb/y, where F(b) is the probability that the copro-
cessor fails before 1,000 b hours of use. It can be
shown that
tðXÞ ¼ 1 
1 
b
Pn
i¼1 Xi


 
!n1
I½b;1Þ
S
n
i¼1 Xi


is such that E(t(X)) ¼ 1eb/y. Is t(X) the MVUE for P(z 
 b)?
Why or why not? Estimate P(z 
 20).
(j) Is t	ðXÞ ¼ 1  eb=X a MVUE for F(b)? Is t*(X) a consis-
tent estimator of F(b)?
2. Use the Lehman-Scheffe´ minimal sufﬁciency theo-
rem, or some other argument, to ﬁnd a set of minimal
sufﬁcient statistics for each case below.
a. You are random sampling from a log-normal popula-
tion distribution given by
fðz; m; s2Þ ¼
1
2p
ð
Þ1=2sz
exp  1
2s2 lnðzÞ  m
ð
Þ2


Ið0;1Þ ðzÞ;
where m∈(1,1) and s2 > 0.
b. You are random sampling from a “power function”
population distribution given by
f z; l
ð
Þ ¼ lzl1Ið0;1ÞðzÞ; where l>0:
Problems
425

c. You are random sampling from a Poisson population
distribution
f x; l
ð
Þ ¼ ellx
x!
I 0;1;2;...;
f
gðxÞ:
d. You are random sampling from a negative binomial
density
fðx; r0; pÞ ¼
ðx  1Þ!
ðr0  1Þ!ðx  r0Þ! pr0 ð1  p Þxr0 Ifr0;r0 þ1;...g ðxÞ
where r0 is a known positive integer.
e. You are random sampling from a N(m,s2) population
distribution.
f. You are random sampling from a continuous uniform
density
fðx; YÞ ¼ 1
Y Ið0;YÞ ðxÞ
g. You are sampling from a Beta distribution
fðx; a; bÞ ¼
1
Bða; bÞ xa1 ð1  x Þb1 Ið0;1Þ ðxÞ:
3. Identify which of the minimal sufﬁcient statistics in
(2) are complete sufﬁcient statistics.
4. The
operating
life
of
a
small
electric
motor
manufactured
by
the
AJAX
Electric
Co.
can
be
represented as a random variable having a probability
density given as
Z  fðz; YÞ ¼
1
6Y4 z3 ez=Y Ið0;1Þ ðzÞ
where Y∈O ¼ (0,1), E(Z) ¼ 4Y, var(Z) ¼ 4Y2, and z is
measured in thousand of hours. A random sample
(X1,. . .,X100) of the operating lives of 100 electric motors
has an outcome that is summarized as x ¼ 7:65 and
s2 ¼ Pn
i¼1 xi  x
ð
Þ2=100 ¼ 1:73:
a. Deﬁne a minimal, complete sufﬁcient statistic for
estimating the expected operating life of the electric
motors produced by the AJAX Co.
b. Deﬁne the MVUE for estimating E(Z) ¼ 4Y. Justify
the MVUE property of your estimator. Generate an
estimate of E(Z) using the MVUE.
c. Is the MVUE estimator a consistent estimator of
E(Z) ¼ 4Y? Why or why not?
d. Does the variance of the estimator you deﬁned in (b)
attain the Cramer-Rao Lower Bound? (The CRLB
regularity conditions hold for the joint density of
the random sample. Furthermore, the alternative
form of the CRLB, expressed in terms of second-
order derivatives, applies in this case if you want to
use it).
5. The number of customers that enter the corner gro-
cery store during the noon hour has a Poisson distribu-
tion, i.e.,
fðz; lÞ ¼ el lz
z!
If0;1;2;3;...g ðzÞ:
Assume that
X1; X2; . . . ; Xn
ð
Þ0 is a random sample from
this Poisson population distribution.
a. Show that the Cramer-Rao lower bound regularity
conditions hold for the joint density of the random
sample.
b. Derive the CRLB for unbiased estimation of the
parameter l. Is X the MVUE for estimating l? Why
or why not?
c. Use the CRLB attainment theorem to derive the
MVUE
for
estimating
l.
Suppose
n ¼ 100
and
P100
i¼1 xi ¼ 283. Estimate l using the MVUE.
d. Is X a member of the CAN class of estimators? Is X
asymptotically efﬁcient?
e. Deﬁne the CRLB for estimating P(z ¼ 0) ¼ el. Does
there exist an unbiased estimator of el that achieves
the CRLB? Why or why not?
6. Polly Pollster wants to estimate the proportion of
voters in Washington State that are in favor of an anti-
tax initiative. She will be using a random sample (with
replacement) of 1,000 voters, and she will record their
preference regarding the anti-tax initiative. She needs
some statistical advice from you.
a. Deﬁne
a
statistical
model
for
the
problem
of
estimating the proportion of voters in favor of the
initiative.
b. Deﬁne the MVLUE for the proportion of voters in
favor of the initiative. Justify that your estimator
really is a MVLUE.
c. Is the estimator that you deﬁned in (b) a consistent
estimator of the proportion of voters in favor of the
initiative? Is it a CAN estimator? Is it asymptotically
efﬁcient?
d. Assume there are two million voters in the state.
What is the probability that the estimator you
deﬁned in (b) generates an estimate that is within
426
Chapter 7
Point Estimation Theory

.03 of the true proportion of voters favoring the
initiative? (Note: This may be a function of unknown
parameters!)
e. Polly summarized the outcome of the random sample
as P1000
i¼1 xi ¼ 670, where xi ¼ 1 indicates that the ith
sample voter was in favor of the initiative, and xi ¼ 0
otherwise. Estimate the proportion of voters in favor
of the initiative. Given your result in (d), if the elec-
tion were held today, would you predict that the
initiative would pass? Why or why not?
7. Two economics professors are arguing about the
appropriate estimator to use in estimating the mean of
the population distribution of incoming freshmen’s I.Q.’s.
The estimators will be based on a random sample from
the population distribution. One professor suggests that
they simply calculate the sample mean I.Q., xn, and use it
as an estimate of the mean of the population distribution.
The other prefers to use an estimator of the form
tðXÞ ¼ Pn
i¼1 Xi=ðn þ kÞ , where n is the random sample
size and k is some positive integer, and she argues that
her estimator has less variance than the sample mean and
that for an appropriate choice of k, her estimator would be
superior on the basis of MSE.
a. We know that X is unbiased, asymptotically unbi-
ased, BLUE, and consistent for estimating the mean
of the
population distribution.
Which
of these
properties apply to the alternative estimator?
b. Deﬁne asymptotic distributions for both estimators.
On the basis of their asymptotic distributions, do you
favor one estimator over the other?
c. Deﬁne the MSEs of the estimators. Is there any valid-
ity to the statement that “for an appropriate choice of
k” t(X) will be superior toX in terms of MSE? Explain.
d. Can you foresee any practical problems in using t(X)
to generate estimates of the population mean?
8. The diameters of blank compact disks manufactured
by the Dandy Disk Co. can be represented as outcomes of
a random variable
Z  fðz; YÞ ¼ 1
Y Ið4;4þYÞ ðzÞ; for some Y>0;
where z is measured in inches. You will be using a random
sample
X1; X2; . . . Xn
ð
Þ from the population distribution
f(z;Y) to answer the questions below.
a. Based on the random sample, deﬁne an unbiased esti-
mator of the parameter Y.
b. Is the estimator you deﬁned in (a) a BLUE for Y? If
not, ﬁnd a BLUE for Y, if it exists.
c. Is your estimator a consistent estimator for Y? Why
or why not?
d. Deﬁne an asymptotic distribution for your estimator.
e. A random sample of size n ¼ 1,000 from f(z;Y) results
in P1000
i¼1 xi ¼ 4; 100. Use your estimator to estimate
the value of Y. Using your estimate of Y, what is the
estimated probability that z∈(4.05, 4.15)?
9. Your company sells trigger mechanisms for air bags
that are used in many modern domestic and foreign-built
passenger cars. The reliability of such trigger mechanisms
is obviously critical in the event that an air bag-equipped
vehicle is involved in an accident. One large Detroit auto-
mobile manufacturer said that they would be willing to
purchase trigger mechanisms from your company if you
could provide convincing support for the statement that,
in repeated simulations of an impact of 15 mph, the
expected number of impacts needed to obtain the ﬁrst
failure of your trigger (i.e., the trigger does not signal an
air bag deployment) was greater than or equal to 1,000.
Management has randomly chosen 10,000 of the
trigger mechanisms for a testing program in which the
number of simulated impacts needed to obtain the ﬁrst
failure will be observed for each mechanism. You need
to estimate the expected number of impacts needed to
obtain the ﬁrst failure of the trigger mechanisms you
manufacture. You intend to use the outcome of the sam-
ple mean as your estimate of this expected number of
impacts.
a. Deﬁne an appropriate statistical model for the sam-
pling experiment, and justify your choice.
b. Is the sample mean an unbiased estimator in this
case? Why?
c. Is the sample mean an asymptotically unbiased esti-
mator? Why?
d. Is the sample mean a consistent estimator? Why?
e. Is the sample mean a BLUE (or equivalently, a
MVLUE)? Why?
f. Derive the Cramer-Rao Lower Bound for the variance
of unbiased estimators of the expected number of
impacts to obtain the ﬁrst failure. (Hint: You may use
the alternative (second-derivative) form of the bound –
it might be a little easier to work with in this case.)
g. Is the sample mean a MVUE? Why?
Problems
427

h. Use Theorem 7.17 on the attainment of the Cramer-
Rao lower bound to derive the MVUE of the expected
number of impacts needed to obtain the ﬁrst failure.
i. Deﬁne an appropriate asymptotic distribution for the
sample mean in this case. Is the sample mean asymp-
totically efﬁcient?
j. The 10,000 observations resulted in
P10;000
i¼1
xi ¼
1.5  107. What is your estimate of the expected
number of impacts needed to obtain the ﬁrst failure?
k. Is (Xn)1 a consistent estimator of p, the probability
that a trigger successfully signals deployment of the
air bag on any given trial?
l. Deﬁne an asymptotic distribution for the estimator
(Xn)-1 of p. Is the estimator asymptotically efﬁcient?
m. Use the estimator (Xn)1 to estimate p, and use this
estimated value in the asymptotic distribution you
deﬁned for Xn to estimate the probability that an
estimate generated by Xn
would be within  50
units of the population mean.
n. What, if anything, can you say to the Detroit manu-
facturer to convince him/her to buy your trigger
mechanisms?
10. Suppose random sampling is from an exponential
population distribution representing the waiting time
between customer arrivals at a bank, so that the statisti-
cal model is given by
fðx; yÞ ¼ ynePn
i¼1 xi=y Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ;
for y>0:
Your objective is to use an outcome of a random sample of
size n (n ﬁxed and known) to estimate q(y) ¼ y2, the vari-
ance of waiting times.
a. Deﬁne a complete sufﬁcient statistic for f(x; y).
b. Deﬁne the CRLB for unbiasedly estimating y2.
c. Does there exist an unbiased estimation of y2 whose
variance is equal to the CRLB?
d. Deﬁne the MVUE of y2 by ﬁnding an appropriate
function of the complete sufﬁcient statistic—if you
can.
e. Is the sample variance, S2
n, the MVUE for the popula-
tion variance y2?
f. If P100
i¼1 xi ¼ 257, generate a MVUE estimate of the
variance of waiting times.
11. One hundred one-acre test plots are being used to
assess the yield potential of a new variety of wheat genet-
ically engineered by Washington State University. The
actual yield-per-acre observations on the test plots can
be viewed as iid observations from some log-normal pop-
ulation, so that the statistical model for the experiment is
given by
fðy; m; s2Þ ¼
1
2p
ð
Þn=2sn Qn
i¼1 yi
 exp  1
2s2 S
n
i¼1ðlnðyiÞ  m Þ2


P
n
i¼1 Ið0;1Þ ðyiÞ
for m 2 ð1; 1Þ and s>0:
a. Deﬁne minimal sufﬁcient statistics for f(y;m,s2).
b. Are the sufﬁcient statistics you deﬁned in (a) com-
plete sufﬁcient statistics?
c. Is t1(Y) ¼ n1 Pn
i¼1 ln Yi
ð
Þ the MVUE of the parameter
m? Is it consistent? Why?
d. Is t2(Y) ¼ (n1)1Pn
i¼1 ln Yi
ð
Þn1 Pn
i¼1 ln Yi
ð
Þ

2 the
MVUE of the parameter s2? Is it consistent?
e. Deﬁne a consistent estimator of q m; s
ð
Þ ¼ emþs2=2 ,
which is the mean of the log-normal population. Jus-
tify your answer. (The MVUE of the mean exists, but
it is quite complicated to deﬁne and calculate. See D.
J. Finney (1941), “On the distribution of a variate
whose logarithm is normally distributed.” Roy. Sta-
tistical Society, Series B, 7, pp. 155–161.)
f. An overworked, underpaid, gaunt-looking research
assistant hands you an envelope that contains only
summary information on the results of the experi-
ment. In particular the information is that
X
100
i¼1
ln Yi
ð
Þ ¼ 375:00;
X
100
i¼1
ðln ðYiÞÞ2 ¼ 1455:75:
Generate an estimate of q m; s2


21
ð
Þ ¼
m
s2


using the
MVUE of q(m,s2). Generate an estimate of the mean of
the lognormal distribution using a consistent estimator.
12. In each case below, determine whether the estimator
under consideration is unbiased, asymptotically unbi-
ased, and/or consistent.
a. The random sample (X1,. . .Xn) is generated from a
Gamma
population
distribution.
The
estimator
tðXÞ ¼ Pn
i¼1 Xi=n will be used to estimate E(Xi) ¼ ab.
428
Chapter 7
Point Estimation Theory

b. The random sample (X1,. . .,Xn) is generated from an
exponential population distribution. The estimator
tðXÞ ¼ 1=2
ð
Þ Pn
i¼1 X2
i =n
will be used to estimate
var(Xi) ¼ y2.
c. The random sample (X1,. . .,Xn) is generated from a
geometric population distribution. The estimator
t(X) ¼ S2  X


will be used to estimate E X2
i


¼ p2.
d. The random sample (X1,. . .,Xn) is generated from a
Bernoulli population distribution. The estimator
t(X) ¼ Xð1  XÞ will be used to estimate var(Xi) ¼
p(1p). (Hint: 2 Pn
i¼1
Pn
j>i a ¼ n n  1
ð
Þa).
13. In Problem 12(b), above, consider the alternative
estimator t*(X) ¼ S2 for estimating y2. In a mean square
error sense, which estimator would you prefer for
estimating y2, t(X) or t*(X)? Note m0
r ¼ r!yr for an exponen-
tial PDF.
14. An incoming shipment of 1,000 toys from a toy
manufacturer is received by a large department store for a
pre-Christmas sale. The store randomly samples 50 toys
from the shipment, without replacement, and records
whether or not the sample item is defective. The store
wants to generate a MVUE estimate of the proportion of
defectives
in
the
shipment
of
toys.
The
statistical
model it uses for the sampling experiment is given by the
hypergeometricdensitywiththe following parameterization:
f x; Y
ð
Þ ¼
1000Y
x


1000 1  Y
ð
Þ
50  x


1000
50


I 0;1;2;...;50
f
gðxÞ;
where Q ∈O ¼ {0,.001,.002,. . .,1} represents the propor-
tion of defectives in the shipment.
a. Show that the X ~ f(x;Y) is a minimal, complete
sufﬁcient statistic for f(x;Y).
b. Deﬁne the MVUE for the proportion of defectives in
the shipment.
c. Suppose that the outcome of X was 3. Deﬁne a MVUE
estimate of the proportion of defectives in the
shipment.
d. Deﬁne an MVUE for the number of defective toys in
the shipment, and provide an MVUE estimate of the
this number.
15. The rates of return per dollar invested in two com-
mon stocks over a given investment period can be viewed
as the outcome of a bivariate normal distribution N(m,S).
The rates are independent between investment periods.
An investment ﬁrm intends to use a random sample from
the N(m,S) population distribution of rates of return to
generate estimates of the expected rates of return, m, as
well as the variances in the rates of return, given by the
diagonal of S.
a. Find a minimal, complete (vector) sufﬁcient statistic
for N(m,S).
b. Deﬁne the MVUE for m.
c. Deﬁne the MVUE for S and for diag(S).
d. Deﬁne the MVUE for the vector (m1, m2, S1
2, S2
2).
e. A random sample of size 50 has an outcome that is
summarized by: x ¼ [.048 .077]’, s2
1 ¼ :5  103; s2
2 ¼
:3  104; and s12 ¼ .2104. Calculate the MVUE
outcome for [m1, m2, S1
2, S2
2]’.
f. Is the MVUE of [m1, m2, S1
2, S2
2]’ consistent?
g. If an investor invests $500 in each of the two
investments, what is the MVUE of her expected dol-
lar return on the investment during the investment
period under consideration?
16. For estimating the population mean,m, based on a size
n iid random sample, X1; . . . ; Xn
ð
Þ, from some population
distribution, the following three estimators are being
considered
t1ðXÞ ¼ n1 X
n
i¼1
xi; t2ðXÞ ¼ n  1
ð
Þ
X
n1
i¼1
xi; and
t3ðXÞ ¼ n1
:5
X
n=2
i¼1
xi þ 1:5
X
n
i¼ n=2
ð
Þþ1
xi
0
@
1
A
where n is an even number.
a. Identify
which
of
the
estimators
are
unbiased
estimators of m.
b. Identify which of the estimators are asymptotically
unbiased estimators of m.
c. Deﬁne the variances of each of the estimators.
Identify which estimator has the smallest variance.
In
comparing
the
variances
of
these
particular
three estimators, would you have expected one
of these estimators to have the smallest variance, a
priori?
Problems
429

d. Identify which of the estimators are consistent esti-
mators of the population mean, justifying your
answers.
17. Consider two estimators for the probability, p, that a
tossed coin will land on “heads”, based on a iid random
sample of size n from whatever Bernoulli distribution
governs the probability of observing “heads” and “tails”.
One estimator is the sample mean xn, and the other is the
estimator deﬁned as tðXÞ ¼ Pn
i¼1 Xi= n þ k
ð
Þ, where k is
some positive integer.
a. For each of the estimators, determine which of the
following properties apply: unbiased, asymptotically
unbiased, BLUE, and/or consistent?
b. Deﬁne asymptotic distributions for both estimators.
On the basis of their asymptotic distributions, do you
favor one estimator over the other?
c. Deﬁne
the
expected
squared
distances
of
the
estimators from the unknown value of p. Is there
any validity to the statement that “for an appropriate
choice of k,” t(X) will be superior to X in terms of
expected squared distance from p? Explain.
d. Can you foresee any practical problems in using t(X)
to generate estimates of the population mean?
e. Suggest a way of estimating when it might make
sense to use t(X) in place of the sample mean for
estimating p.
18. In each case below, determine whether the estimator
under consideration is unbiased, asymptotically unbi-
ased, and/or consistent.
a. The iid random sample X1; . . . ; Xn
ð
Þis generated from
a Gamma population distribution. The estimatort X
ð Þ
¼ X will be used to estimate the value of ab.
b. The iid random sample X1; . . . ; Xn
ð
Þ is generated from
an exponential population distribution. The estima-
tor t X
ð Þ ¼ n  1
ð
Þ1 P
n
i¼1
Xi  X

2 will be used to esti-
mate the value of y2.
c. The iid random sample X1; . . . ; Xn
ð
Þ is generated from
a Poisson population distribution. The estimator t X
ð Þ
¼ Xn will be used to estimate the value of l.
d. The iid random sample
X1; . . . ; Xn
ð
Þ
is generated
from a Poisson population distribution. The estimator
t X
ð Þ ¼ n  1
ð
Þ1 P
n
i¼1
Xi  X

2 will be used to estimate
the value of l.
e. The iid random sample X1; . . . ; Xn
ð
Þ is generated from
a geometric population distribution. The estimator
t X
ð Þ ¼ S2
n will be used to estimate 1  p
ð
Þp2.
f. The iid random sample
X1; . . . ; Xn
ð
Þ
is generated
from a Bernoulli population distribution. The estima-
tor tðXÞ ¼ X 1  X


will be used to estimate p 1  p
ð
Þ.
g. The random sample of size 1, X, is generated from a
Binomial population distribution for which the
value of the parameter n is known. The estimator
t(X) ¼ X/n will be used to estimate p.
19. If you wanted to choose the estimator with the smaller
expected squared distance from the value of l, which of the
estimators in 18.c and 18.d above would you choose?
20. Let the size n random sample Y ¼ Y1; Y2; . . . Yn
½
0 be
such that Y ¼ xb þ «, where x is a n  1 non-zero vector of
“explanatory variable values”, b is an unknown parameter
value, and « is an n  1 random vector for which E «
ð Þ ¼ 0
and cov «
ð Þ ¼ s2I.
a. Is t Y
ð Þ ¼ Pn
i¼1 xiYi= Pn
i¼1 x2
i the BLUE of b? Explain.
b. Deﬁne the mean and the variance of the estimator in a).
c. Under what conditions will the estimator in a) be a
consistent estimator of b?
d. Consideranalternativeestimatort	 Y
ð Þ ¼ x0x þ k
ð
Þ1x0Y
for
b . (this is an example of a so-called “ridge
regression” estimator). Deﬁne the mean and vari-
ance of this estimator. Is this a linear estimator? Is
it unbiased? Is it asymptotically unbiased?
e. Under what conditions will the estimator in d) be a
consistent estimator of b?
f. If you wanted to use the estimator that has the
smaller expected squared distance from b , would
you prefer one estimator over the other? Explain.
21. An estimate is needed of the expected number of resets
it takes to get a certain brand of 15 amp ground fault
protecting circuit breaker to fail to reset. A random sample
of n circuit breakers are each tripped and reset as many
times as needed to produce a fail. The recorded numbers
of resets associated with the n breakers are viewed as the
outcome of iid random variables X1; . . . ; Xn
ð
Þ.
430
Chapter 7
Point Estimation Theory

a. Deﬁne the joint probability density of the random
sample of the n reset outcomes.
b. Deﬁne the MVLUE for the expected number of resets.
Justify that your estimator really is a MVLUE of this
proportion.
c. Is the estimator that you deﬁned above a consistent
estimator of the expected number of resets? Is it
asymptotically normally distributed?
d. Use the CRLB attainment theorem to ﬁnd a MVUE
of the expected number of resets, if you can. If
the MVUE exists, how does it differ from the linear
estimator you deﬁned in b)?
e. Does an unbiased estimator of the variance of reset
outcomes exist that achieves the CRLB, and is
thereby MVUE?
Problems
431

8
n
Point Estimation Methods
n
n
n
8.1
Introduction
8.2
The Least Squares Estimator
8.3
The Method of Maximum Likelihood
8.4
Method of Moments and Generalized Method of
Moments Estimation
8.5
Appendix: Proofs and Proof References for Theorems
8.1
Introduction
In this chapter we examine point estimation methods that
lead to speciﬁc functional forms for estimators of q(Q) that can be relied upon to
deﬁne estimators that often have good estimator properties. Thus far, the only
result in Chapter 7 that could be used directly to deﬁne the functional form of an
estimator is the theorem on the attainment of the CRLB, which is useful only if
the probability model {f(x;Q), Q∈O} and the estimand q(Q) are such that the CRLB
is actually attainable. We did examine a number of important results that could
be used to narrow the search for a good estimator of q(Q), to potentially improve
upon an unbiased estimator that was already available, or that could verify when
an unbiased estimator was actually the best in the sense of minimizing variance
or in having the smallest covariance matrix. However, since the functional form
of an estimator of q(Q) having good estimator properties is often not apparent
even with the aid of the results assembled in Chapter 7, we now examine
procedures that suggest functional forms of estimators.
Currently, there is no single procedure for generating estimators of q(Q) that
will always lead to the best estimator or even to an estimator that always has
“good” properties. However, the three general estimation methods that we will
examine lead to good estimators of q(Q) for a wide range of probability models
and associated random samples of data. These methods include the least
squares, maximum likelihood, and the generalized method of moments
estimators. Together, these estimators encompass a large number of the estima-
tion procedures that are currently used in empirical practice.
In this and the remaining two chapters, we will place some proofs of
theorems in the appendix to improve readability relative to the main results

that need to be conveyed in estimation and inference methodology. In particular,
proofs that tend to be both mathematically complex and relatively less informa-
tive, relative to enhancing statistical understanding, will be found in the
appendices to chapters.
8.2
The Least Squares Estimator
In this section we examine the least squares (LS) estimator for deﬁning the mean
of a random variable Y conditional on the values of other random variables X on
which the mean of Y potentially depends. We will investigate both the linear LS
and nonlinear LS estimators, providing substantial detail on the former and an
introduction to the latter. Together, these two LS methods account for a great
deal of the empirical literature in statistics and econometrics devoted to “regres-
sion analysis”.
8.2.1
Linear and Nonlinear Regression Models
In the empirical statistics and econometrics literature, a regression model is used
to characterize how the outcomes of a random sample come about, whereby the
ith random variable, Yi, in the random sample (Y1, . . ., Yn) is decomposed into the
sum of its expectation and the deviation from its expectation,1 as
Yi ¼ mi þ ei; i ¼ 1; . . . ; n:
In this conceptualization, E(Yi) ¼ mi and E(ei) ¼ 0. At this level of generality, the
decomposition is always valid so long as E(Yi) exists.
The linear regression model specializes this representation by further
assuming that mi is deﬁned via a function of m explanatory variables contained
in the (m  1) vector zi and that this function is known except for k unknown
parameters that enter the function linearly, leading to the representation
mi ¼ Pk
j¼1 hjðziÞ bj; i ¼ 1; . . . ; n, and Yi ¼ Pk
j¼1 hj zi
ð Þbj + ei, i ¼ 1,. . .,n .
We emphasize that the adjective linear in linear model refers to the assump-
tion that the function of the explanatory variables deﬁning mi is linear in
parameters. The representation need not be linear in the explanatory variables,
as is evident in the speciﬁcation above. However, it is customary to deﬁne the
variables xij  hj(zi) for i ¼ 1,. . .,n and j ¼ 1,. . .,k, so that the representation of
the mean mi is then also linear in the xij0s, as mi ¼ Pk
j¼1 xij bj; i ¼ 1; . . . ; n, and thus
Yi ¼ S
k
j¼1 xij bj þei; i ¼ 1; . . . ; n or Y ¼ xb þ «;
1We will concentrate on models for which the mean of the random variable exists. It is possible to use this characterization in other
ways when it does not, for example, using the median as the measure of the central tendency of the outcomes, such as Yi ¼ i þ eii,
where i ¼ median Yi
ð
Þ and median ei
ð Þ ¼ 0.
434
Chapter 8
Point Estimation Methods

with the latter being a compact matrix representation of the linear relationship
between the random sample Y and the variables explaining the mean (i.e., the
explanatory variables), x.
The linear regression model can be used to represent a random sample Y
whether the random sample is from a population distribution or from a more
general experiment. If the elements of Y are iid random variables from a popula-
tion distribution, then x is simply a column vector of 1’s, b is a scalar
representing the common mean of the Yi0s, and the ei0s are iid with mean zero.
More generally, if the elements of Y are generated by a more general sampling
scheme, so that the Yi0s need not be identically distributed nor independent,
then linear combinations of the elements in the rows of x are used to represent
the various mean values of the yi0s, and the ei0s have a joint probability distribu-
tion with a zero mean vector and appropriate variances and covariances.
Although it is always linear in the parameters, a linear model can be more
general than it might, at ﬁrst, appear in terms of the types of functional
relationships between Y and x that it can portray. In order to illustrate the
generality of the linear model, note that
Yi ¼ b1 þb2z2
i2 þ b3 sinðzi3Þ þ b4
zi4
zi5


þ ei; i ¼ 1; . . . ; n
is consistent with the linear model characterization, and a representation that is
linear in explanatory variables as well can be obtained upon deﬁning xi1 ¼ 1,
xi2 ¼ z2
i2, xi3 ¼ sin(zi3), and xi4 ¼ (zi4/zi5), so that Yi ¼ P4
j¼1 xijbj + ei, i ¼ 1,. . .,n.
More generally, a linear model can serve as an approximation in situations
where E(Yi) ¼ m(zi1,. . .,zim) is any continuous function of (zil,. . .,zim), at least in
principle, since by Weierstrass’s approximation theorem any continuous func-
tion can be approximated arbitrarily closely by a polynomial of sufﬁciently high
degree (see Bartle, op. cit., pp. 185–186). That is, if m(zil,. . .,zim) represents a
continuous function, then
mðzi1; . . . ; zimÞ 
X
j1;...;jm
ð
Þ2A
cj1;...;jm zj1
i1 zj2
i2 . . . zjm
im ;
where A ¼ {(j1,. . .,jm): 0  j1 + . . . + jm  d, ji0s are nonnegative integers}, and d
represents the degree of the polynomial. Letting the cj1;...;jmvalues be the entries
in the b vector, and the product terms ð zj1
i1 zj2
i2 . . . zjm
im Þ be the entries in the x
matrix, it is clear that the polynomials in the Weierstrass approximation theo-
rem can be incorporated into the linear model framework.2
Finally, a relationship between Y, x, and « that is initially nonlinear in the
parameters might be transformable into the linear model form. For example,
suppose Yi ¼ b1kb2
i ℓ
b3
i eeiwhich, for example, could represent a Cobb-Douglas pro-
duction function with yi being output, and ki and ℓi being capital and labor inputs.
2We note, however, that for highly nonlinear functions, the degree of polynomial required to provide an adequate approximation to
m(zi1,. . .,zim) may be so high that there will not be enough sample observations to estimate the unknown bj0s adequately, or at all. This
is related to a requirement that x have full column rank, which will be discussed shortly.
8.2
The Least Squares Estimator
435

Applying a logarithmic transformation results in ln(Yi) ¼ ln(b1) + b2 ln(ki) +
b3 ln(ℓi) + ln(ei), or y
i ¼ b
1 þ b2 k
i þ b3 ℓ

i þ e
i ; which is in the linear model
form for obvious deﬁnitions of the starred variables and parameters.
A nonlinear regression model is one in which the mean value function is a
nonlinear function of the parameters, as Yi ¼ g xi:; b
ð
Þ þ ei, and it is not trans-
formable into linear model form, so that the model must be estimated in the
nonlinear form. The computational aspects of estimating nonlinear models via a
least squares objective can be challenging at times, but there are substantial
similarities in the interpretation of the asymptotic behavior of both types of
estimators. There are fewer similarities in the ﬁnite sample interpretation of
estimator behavior. While we will leave a detailed investigation of the estima-
tion of nonlinear regression models to a more advanced course of study, we will
provide a general overview of the method of nonlinear least squares, and indicate
analogies to the linear least squares estimator in Section 8.2.6 ahead.
8.2.2
Least Squares Estimator Under the Classical General Linear Model
Assumptions
The Classical General Linear Model (GLM) refers to a linear model with a
speciﬁc set of additional assumptions regarding the genesis of the observed
random sample of data Y. The random variable Y is referred to as the dependent
variable, x is called the matrix of independent or explanatory variables, and « is
called the disturbance, error, or residual vector. The main objective in analyzing
the GLM will be to estimate (and later, test hypotheses about) the entries, or
functions of the entries, in the parameter vector b. In particular, we will utilize
an outcome, (y1,. . .,yn), of the random sample, (Y1,. . .,Yn), together with knowl-
edge of the values of the explanatory variables, x, to estimate the unknown
entries in b. Note the entries in « are unobservable random variables, since
they represent deviations of (y1,. . .,yn) from the unknown mean vector xb, and
thus outcomes of « will not be useful in estimating b, per se.
The following deﬁnition delineates the set of assumptions that determine
the statistical characteristics of the Classical General Linear Model.
Deﬁnition 8.1
Classical General Linear
Model (GLM)
Assumptions
GLM 1. E(Y) ¼ xb and E(«) ¼ 0
GLM 2. Cov(Y) ¼ s2I ¼ Cov(«) ¼ E(««0)
GLM 3. x is a ﬁxed matrix of values with rank(x) ¼ k
The ﬁrst assumption simply restates what we have already asserted
concerning the mean of Y in the linear model, namely, xb is representing or
explaining E(Y), which then necessarily implies that E(«) ¼ 0. In applications,
E(Yi) ¼ xi.b (recall that xi. refers to the ith row of the x matrix) means that
whatever the experiment under investigation, the expected value of the distri-
bution associated with outcomes of Yi is a linear function of the values of the
436
Chapter 8
Point Estimation Methods

explanatory
variables
in
xi..
A
graphical
illustration,
for
k ¼ 2
with
x.1 ¼ (1 1. . .1)0, is provided in Figure 8.1.
Note that under the Classical GLM assumptions, x is a ﬁxed matrix of
numbers. This implies that a sample observation (yi, xil,. . .,xik) can be interpreted
as having been generated from an outcome of the vector (Yi, xil,. . .,xik),
where this latter vector can be considered a random vector, pairing the (generally)
nondegenerate random variable Yi with the degenerate random vector (xil,. . .,xik).
When observing repeated outcomes of (Yi, xil,. . .,xik), it is implied that (xil,. . .,xik)
remains constant, while the outcome yi that is paired with the vector (xil,. . .,xik)
generally varies. The distribution of yi outcomes for given (xil,. . .,xik) values,
when k ¼ 2, is depicted in Figure 8.1 by a distribution centered at E(Yi).
It is useful to distinguish two contexts in which the x matrix can be consid-
ered ﬁxed. One context is the situation where elements of the x-matrix are
controlled or ﬁxed by the individual performing the experiment. In this case,
the x-matrix is sometimes referred to as the design matrix, meaning that the
researcher can essentially design the vectors (xil,. . .,xik), i ¼ 1,. . .,n, in a way that
is of particular interest to the investigation at hand, and then she can observe the
yi values associated with the chosen design matrix. As an example of this type of
interpretation for x, suppose various levels of inputs to a production process
were chosen, and an observation was made on the output corresponding to each
ﬁxed level of the inputs. We might expect that, “on average,” a certain level of
output, say E(Yi), would be produced given the levels of inputs speciﬁed by xi..
However, for any given observation with input level xi., deviations from the
average level of output might occur due to a myriad of non-input type factors
that are not controlled in the production process (e.g., machine function varia-
tion, labor efﬁciency, weather, temperature, and the like). Then observations on
output levels, given the chosen (or designed) levels of inputs represented by the
*
xi2
yi
xi2
xi2
E (Yi) = b1 + b2 xi2
o
b1
Figure 8.1
GLM representation of E(Yi)
in bivariate case
8.2
The Least Squares Estimator
437

elements in x, can be conceptualized as observations on the vector (Yi, xil,. . .,xik),
as the GLM classical assumptions imply. In general, the underlying rationale for
interpreting the expectation of the random variable Yi as a function of the
explanatory variables is based on the existence of some underlying systematic
economic, sociological, biological, or physical linear relationship relating the
mean of Yi to the value of xi. As we have alluded to previously, sometimes a
relationship that is a polynomial function of explanation variables, and thus a
linear function of parameters, is assumed as an approximation.
Many experimental situations do not allow the x matrix to be controlled or
designed. In economics, business, or the social sciences in general, the
researcher is often a passive observer of values of (yi, xil,. . .,xik) that have been
generated by consumers, entrepreneurs, the economy, by markets, or by the
actions of society. In such cases, it is more natural to consider some or all of
the elements in the x matrix as having been generated by an outcome of a
random matrix X, while the vector y continues to be interpreted as the outcome
of the random vector Y. Then the assumption E(Y) ¼ xb of the GLM is
interpreted in a conditional sense, i.e., the expected value of Y is conditional
on the outcome x for X, and a more revealing notation for this interpretation
would be given by EðY x
j Þ ¼ xb. The reader will recall from Chapter 3 that this
conditional expectation is literally the regression function of Y on x, and thus
the GLM maintains that the regression function is actually a regression
hyperplane.3
Note that since Y ¼ xb + « is assumed by the GLM, it follows that in the
context of operating conditionally on an outcome of X, we must then be refer-
ring to a conditional distribution of the vector Y when we say that outcomes of
Y can be decomposed into the mean vector, xb, and deviations, «, from the mean
vector. We might consider using the notation Y x ¼ xb þ «
j
jx to emphasize this
fact, implying that Eð« x
j Þ ¼ 0. In this situation, the entire analysis is cast in the
framework of being conditional on the x matrix observed, and once this inter-
pretation is afﬁxed to the GLM together with the implicit assumption that all
expectations are conditional on x, the situation (and method of analysis) will be
analogous to the previous case where the x matrix is ﬁxed. We will generally
suppress the conditional-on-x notation, leaving it to the reader to provide the
proper context for interpreting the GLM in a given problem situation.
The assumption Cov(Y) ¼ s2I ¼ Cov(«) ¼ E(««0) implies that the covariance
matrix of Y, and of «, is a diagonal matrix with s20s along the diagonal and zeroes
on the off-diagonal. The fact that all of the variances of the elements in Y (or «)
have the same value, s2, is referred to as the property of homoskedasticity
(from homoskedastic, meaning “same spread”). The off-diagonal entries being
zero imply that the covariance, or equivalently, the correlation between any two
elements of Y (or «) is zero, which is called the property of zero autocovariance or
zero autocorrelation. This assumption on the covariance matrix of Y (or «) then
3Note that by tradition, the previous case where x is designed is also referred to as a regression of Y on x, where one can think of the
conditional expectation in the degenerate sense, i.e., where x takes its observed value with probability one.
438
Chapter 8
Point Estimation Methods

implies that the random sample (Y1,. . .,Yn) is a collection of uncorrelated random
variables all having the same variance or measure of “spread.” The reader is
reminded that this assumption may be referring to the conditional distribution
of Y (and «), if we are conditioning on an outcome of the X matrix.
The assumption that rank x ¼ k, i.e. the x matrix has full column rank,
simply implies that there are no linear dependencies among the columns of the x
matrix. That is, no column of the x matrix is representable as some linear
combination of the remaining columns in the x matrix. This necessarily implies
that n  k, i.e., we must have at least as many sample observations as there are
unknown parameters in the b-vector and columns in the x-matrix.
To this point we still have not fully speciﬁed the probability model since
we have not yet speciﬁed a parametric family of densities for the random vector Y
(or «). In fact, all we have assumed, so far, is that (Y1,. . .,Yn) is a random sample
from some experiment such that the joint density of the random sample has a
mean vector xb, a covariance matrix s2I, with x having full column rank. Never-
theless, it is possible to suggest an estimator for the parameter vector b that has a
number of useful properties. We will proceed to deﬁne such an estimator of b in
the absence of a speciﬁc parametric family speciﬁcation, reserving such a speci-
ﬁcation until we have exhausted the progress that can be made in its absence. We
will also deﬁne a useful estimator of the variance parameter s2.
Estimator for b Under Classical GLM Assumptions
How should the estimator of b
be deﬁned? We examine two approaches to the problem that lead to the same
estimator for b but that offer different insights into the genesis of the estimator of b.
One approach, called the method of least squares, which is the principal
focus of this section, deﬁnes the estimator of b by associating with each obser-
vation on (y,x) the (k  l) vector b that solves the minimization problem4
b ¼ arg minh ðy  xhÞ0ðy  xhÞ


:
Note that this is equivalent to ﬁnding the vector ^y ¼ xb that is the minimum
distance from y, since the minimum of the distance d(y,^y) ¼ [(y  ^y)0(y  ^y)]1/2
and the minimum of (y  ^y)0(y  ^y) occur at precisely the same value of b.5 From
the point of view of attempting to explain y as best one can in terms of a linear
function of the explanatory variable values, x, the least squares approach
has intuitive appeal. To solve the minimization problem, note that (y  xh)0
(y  xh) ¼ y0y  2h0x0y þ h0x0xh, and the k ﬁrst order conditions for the mini-
mum can be represented in matrix form as  2x0y þ 2x0xb ¼ 0 where we have
used a result contained in the following lemma concerning matrix calculus:
4Recall that arg minw fðwÞ
f
g denotes the argument value w that minimizes f(w).
5This follows because z1/2 is a monotonic transformation of z for z  0, so that the minimum (and maximum) of z1/2 and z occur at the
same values of z, z ∈D, D being some set of nonnegative numbers.
8.2
The Least Squares Estimator
439

Lemma 8.1
Let z be a (k  1) vector, A be a (k  j) matrix, and w be a (j  l) vector. Then
a: @z0Aw
@z
¼ Aw
b: @z0Aw
@w
¼ A0z
c: @z0Az
@z
¼ 2Azðk ¼ j and A is symmetricÞ
d: @z0w
@z
¼ wðk ¼ jÞ
e: @z0w
@w ¼ zðk ¼ jÞ
Since x has full column rank, the (k  k) matrix (x0x) necessarily has full rank
and is, thus, invertible.6 The ﬁrst order conditions can then be solved for the
sum of squares minimizer b, as
b ¼ x0x
ð
Þ1x0y
which deﬁnes the estimate for b implied by the least squares method. The
estimator of b is then deﬁned by the (k  1) random vector
^b ¼ x0x
ð
Þ1x0Y:
where for the remainder of this section, we will use ^bto denote an estimator, and
b will denote the associated estimate of b. Note that the second order conditions
for the minimization problem are 2x0x, which is positive deﬁnite, so that the
second order conditions are satisﬁed for a global minimum and thus h ¼ b does
indeed minimize ðy  xhÞ0ðy  xhÞ.
Coefﬁcient of Determination, R2
A function of the minimized value of the dis-
tance between y and xb is often used as a measure of how well the y outcome has
been “explained” by the xb outcome. In particular, letting 1n be a (n  1) vector
of 1’s and ^y ¼ xb, the measure is given by
R2 ¼ 1 
d2ðy,^yÞ
d2ðy; 1nyÞ
¼ 1 
ðy  ^yÞ0ðy  ^yÞ
ðy  1nyÞ0ðy  1nyÞ ¼ 1 
S
n
i¼1 ðyi  ^yi Þ2
S
n
i¼1 ðyi y Þ2
and is called the coefﬁcient of determination or “R-squared.” It is clear that the
closer ^y is to y in terms of distance, the higher is R2, and its maximum value of 1
is achieved iff y ¼ ^y.
If x contains a column of 10s, so that Y ¼ xb + « contains an intercept term,
then R2 is lower bounded by zero and R2 is identically the square of the sample
6A matrix x has full column rank iff x0x has full rank. See Rao, C., op. cit., p. 30.
440
Chapter 8
Point Estimation Methods

correlation between y and ^y. To see this, ﬁrst deﬁne ^e ¼ y  ^y ¼ y  xb and note
that
y0y ¼ xb þ ^e
ð
Þ0 xb þ ^e
ð
Þ can be decomposed into two components as
y0y ¼ b0x0xb þ ^e0^e
since
^y0 ^e ¼ b0x0^e ¼ b0ðx0 y  xb
ð
ÞÞ ¼ 0
by the ﬁrst order
conditions of the least squares minimization problem. Also note that
10
n^e ¼ 10
ny  10
n^y ¼ 0 because x0 y  xb
ð
Þ ¼ x0^e ¼ 0 and x0 contains a row of 10s.
It follows that y ¼ ^y. Subtracting ny2 from both sides of the preceding decompo-
sition of y0y, and then dividing by y0y  ny2, implies
R2 ¼ 1 
^e0^e
ðy  1nyÞ0ðy  1nyÞ ¼
b0x0xb  ny2
ðy  1nyÞ0ðy  1nyÞ ¼
P
n
i¼1
^yi  ^y

2
P
n
i¼1
yi  y
ð
Þ2
:
Finally, since Pn
i¼1 ^yi  ^y

2 ¼ Pn
i¼1 yi  y
ð
Þ ^yi  ^y


;
R2 ¼
Pn
i¼1 yi  y
ð
Þ ^yi  ^y



	2
Pn
i¼1 yi  y
ð
Þ2Pn
i¼1 ^yi  ^y

2 ¼ r2
Y ^Y;
i.e., R2 is the square of the sample correlation between Y and ^Y.
It follows from our discussion of the sample correlation in Chapter 6 that R2
can be interpreted as the proportion of the sample variance in the yi0s that is
explained by the corresponding values of the ^yi0s. This follows from the fact that
the vector of values a + b^yi, i ¼ 1,. . .,n has the smallest expected squared distance
from y1,. . .,yn when b ¼ sY ^Y = s2
^Y ¼ s2
^Y = s2
^Y ¼ 1 and a ¼ y  b^y ¼ y  y ¼ 0.
Best Linear Unbiased Estimator (BLUE) of b
A second approach for deﬁning
an estimator of b in the Classical GLM begins with the objective of deﬁning
the BLUE of b. In order to be in the linear class, the estimator must be of the form
^b ¼ AY + d for some nonrandom k  n
ð
Þ matrix A and some nonrandom k  1
ð
Þ
vector d. If the estimator is to be in the unbiased class, then E(^b) ¼ Axb + d ¼ b,
8b,which requires that Ax ¼ I and d ¼ 0. For the estimator to be the best in the
class of linear unbiased estimators, its covariance matrix must be as small or
smaller than the covariance matrix of any other estimator in the linear unbiased
class. Since ^b ¼ AY and Cov(Y) ¼ s2I, it follows that Cov( ^b ) ¼ s2AA0 is the
covariance matrix that must be minimized through choice of the matrix A subject
to the unbiasedness constraint that Ax ¼ I. The next theorem solves the problem.
Theorem 8.1
BLUE of b in the GLM:
Gauss-Markov Theorem
Under the assumptions of the Classical GLM, ^b ¼ x0x
ð
Þ1x0Y is the best linear
unbiased estimator of b.
Proof
The linear estimator ^b ¼ AY + d is unbiased under the assumptions of the Classi-
cal GLM iff Ax ¼ I and d ¼ 0. The feasible choices for A for deﬁning a linear
unbiased estimator can be represented as A ¼ (x0x)1 x0 + D, where D is any matrix
such that Dx ¼ 0. To see this, ﬁrst note that any matrix A can be equivalently
represented as (x0x)1 x0 + D by simply choosing D ¼ A(x0x)1 x0. Now for A
to satisfy Ax ¼ I, it must be the case that Ax ¼ [(x0x)1 x0 + D]x ¼ I + Dx ¼ I,
8.2
The Least Squares Estimator
441

so that Dx ¼ 0 is implied. Substituting (x0x)1 x0 + D for A represents the
linear unbiased estimator as ^b ¼ ((x0x)1 x0 + D)Y, which has covariance matrix
Cov ^b

 
¼ s2
x0x
ð
Þ1 þ DD0
h
i
because Dx ¼ 0. Being that DD0 is necessarily
positive semideﬁnite, it follows that the covariance matrix is minimized when D
¼ 0, which implies that the BLUE estimator is ^b ¼ x0x
ð
Þ1x0Y.
n
It is also true that j0^b ¼ j0ðx0xÞ1x0Y is the BLUE of j0^b for any choice of j 6¼ 0.
Thus, once the BLUE ^bis known, one can calculate BLUE estimates of any linear
combination of the entries in b by simply calculating the corresponding linear
combination of the entries in ^b . We formalize this result in the following
theorem.
Theorem 8.2
BLUE for j0^b
Under the assumptions of the Classical GLM, j0^b ¼ j0ðx0xÞ1x0Y is the best
linear unbiased estimator of j0b.
Proof
Let T ¼ t(Y) ¼ c0Y + d be any linear estimator of the scalar j0b, where c is an
(n  1) vector and d is a scalar. For T to be unbiased, it is required that E(T) ¼
E(c0Y + d) ¼ c0xb + d ¼ j0b8b, which in turn requires that x0c ¼ j and d ¼ 0.
Note the following lemma:
Lemma 8.2
Let x be a matrix of rank k, and let be any (k  1) vector. Then x0c ¼j iff
c ¼ xðx0xÞ1j þ ½I  xðx0xÞ1x0	h;
where the (n  1) vector h can be chosen arbitrarily.
F.A. Graybill, (1983) Matrices With Applications in Statistics, 2nd Ed.,
Wadsworth, p. 153.
Using the solution for c given by the lemma, note that var(c0Y) ¼ s2[j0(x0x)1j +
h0[I  x(x0x)1x0]h]. The variance is minimized by setting h ¼ 0, and thus T ¼
c0Y ¼ j0 (x0x)1x0Y ¼ j0^b is the BLUE for j0b.
n
The estimator ^b ¼ (x0x)1x0Y is referred to as the least squares estimator of b
due to its deﬁnition via the least squares approach described previously. Without
additional assumptions on the statistical model, the BLUE property exhausts the
properties that we can attribute to ^b . In particular, we cannot demonstrate
consistency of ^b, nor can we make progress towards deﬁning a MVUE of b.
Estimator for s2 and Cov ^b

 
Under Classical GLM Assumptions
The classical
assumptions are sufﬁcient to allow the deﬁnition of an unbiased estimator of
the common variance, s2, of the Yi0s (or equivalently, the common variance of
the ei0s), and from it, an unbiased estimator of the covariance matrix of the least
squares estimator. We formalize these results in the next Theorem. Note that
we use the hat notation, ^, to distinguish this unbiased estimator from the
sample variance, S2, introduced previously in Chapter 6.
442
Chapter 8
Point Estimation Methods

Theorem 8.3
Unbiased Estimators of
s2and Cov ^b

 
in the
Classical GLM
The estimators ^S
2 ¼ (Y  x ^b )0(Y  x ^b )/(n  k) and ^S
2 x0x
ð
Þ1 are unbiased
estimators of s2 and Cov ^b

 
, respectively.
Proof
Note that substitution of the linear model representation ofY ¼ xb þ «for Y, and
the deﬁnition of the least squares estimator ^b ¼ x0x
ð
Þ1x0Y into the deﬁnition of
^S
2 results in «0 I  x x0x
ð
Þx0
ð
Þ«= n  k
ð
Þ. Then
E ^S
2


¼ E «0 I  x x0x
ð
Þx0
ð
Þ«= n  k
ð
Þ
ð
Þ
¼ E «0«= n  k
ð
Þ
ð
Þ  E tr x x0x
ð
Þx0««0
ð
Þ= n  k
ð
Þ
ð
Þ
¼ n= n  k
ð
Þ
ð
Þs2  tr x x0x
ð
Þx0E ««0
ð
Þ
ð
Þ= n  k
ð
Þ
ð
Þ
¼ n= n  k
ð
Þ
ð
Þs2  k= n  k
ð
Þ
ð
Þs2 ¼ s2:
It follows that E ^S
2 x0x
ð
Þ1


¼ E ^S
2


x0x
ð
Þ1 ¼ s2 x0x
ð
Þ1.
n
The following is an example of the use of the estimators ^b and ^S
2.
Example 8.1
Least Squares
Estimation of
Production Function
Let the production of a commodity in a given period of time be represented by
Yt ¼ b1 xb2
1t xb3
2t eet where yt ¼ thousands of units produced in time period t,
(x1t,x2t) ¼ units of labor and capital applied to the production process in year t,
and et ¼ disturbance term value in year t. Assume that E(et) ¼ 0 and var(et) ¼ s2,
8t, and let ei and ej be uncorrelated 8i and j.
Ten time periods’ worth of observations on yt, x1t, and x2t were obtained,
yielding
y ¼
47:183
53:005
43:996
38:462
54:035
59:132
79:763
67:252
55:267
38:972
2
666666666666664
3
777777777777775
;
x1 ¼
7
8
6
4
5
9
11
9
8
4
2
666666666666664
3
777777777777775
;
x2 ¼
2
3
2
5
6
4
8
5
5
4
2
666666666666664
3
777777777777775
We seek a BLUE estimate of ln (b1), b2, b3, and of b2 + b3, the latter representing
the degree of homogeneity of the function, as well as an unbiased estimate of s2.
Before we can apply the least squares estimator we must transform the
model to be in GLM form. By taking the natural logarithm of both sides of the
production function relationship, we obtain
ln Yt
ð
Þ ¼ ln b1
ð
Þ þ b2 ln x1t
ð
Þ þ b3 ln x2t
ð
Þ þ et
or
Y
t ¼ b
1 þ b2 x
1t þ b3 x
2t þet:
8.2
The Least Squares Estimator
443

The transformed model satisﬁes the classical assumptions of the GLM. Deﬁning
y
ð101Þ
¼ lnðyÞ and
x
ð103Þ
¼ ½110
lnðx1Þ
lnðx2Þ	;where110 is a (10  1) vector with
each element equal to 1, we can represent the observations on the transformed
relationship as outcomes of Y* ¼ x* b* + «, where b0
 ¼ (ln(b1) b2 b3). The least
squares (BLUE) estimate of b* is then b ¼ ðx0xÞ1x0y ¼
2:638
:542
:206
2
4
3
5:
The BLUE estimate of b2 + b3, is given by b2 + b3 ¼ .748 by Theorem 8.2
with ℓ0 ¼ [0 1 1]. An unbiased estimate of s2 is given by^s2 ¼ y  xb
ð
Þ0 y  xb
ð
Þ=7
¼ .0043.
We emphasize that in the process of transforming the production relation-
ship to GLM form, we also transformed the parameter b1 to ln(b1), which
then results in a situation where the least squares estimator is estimating
not b1, but ln(b1). The phenomenon of transforming parameters often occurs
when the overall relationship must be transformed in order to convert it into
GLM form.
□
Consistency of
^b
The following theorem demonstrates that additional
conditions are needed to ensure that the least squares estimator is a consistent
estimator of b.
Theorem 8.4
Consistency of
^b ¼ (x0x)1x0Y
If in addition to the assumptions of the Classical GLM, (x0x)1 ! 0 as n ! 1,
then ^b ¼ (x0x)1x0Y !
p b, 8b, so that ^b is a consistent estimator of b.
Proof
Given the assumptions of the Classical GLM, we know that E(^b) ¼ b, and the
covariance matrix of ^b is given by Cov(^b) ¼ s2(x0x)1. It follows from Corollary
5.2 that if Cov( ^b) ¼ s2(x0x)1 ! 0 then ^b converges in mean square to b. But
since convergence in mean square implies convergence in probability (Theorem
5.13), it follows that plim(^b) ¼ b, so that ^b is a consistent estimator of b.
n
As a practical matter, what is required of the x matrix for limn!1(x0x)1 ¼ 0?
The convergence to the zero matrix will occur iff each diagonal entry of (x0x)1
converges to zero, the necessity of this condition being obvious. The sufﬁciency
follows from the fact that (x0x)1 is positive deﬁnite since x0x is positive deﬁ-
nite,7 and then the (i,j)th entry of (x0x)1 is upper-bounded in absolute value by
the product of the square roots of the (i,i)th and (j,j)th (diagonal) entries of (x0x)1.
To see the boundedness of the (i,j)th entry of (x0x)1, note that for any symmetric
positive deﬁnite matrix, all principal submatrices formed by deleting one or
more diagonal entries together with the rows and columns in which they appear
7The inverse of a symmetric positive deﬁnite matrix is necessarily a symmetric positive deﬁnite matrix. This can be shown by noting
that a symmetric matrix is positive deﬁnite iff all of its characteristic roots are positive, and the characteristic roots of A1 are the
reciprocals of the characteristic roots of A.
444
Chapter 8
Point Estimation Methods

must also be positive deﬁnite.8 In particular, retaining only the ith and jth
diagonal entries results in a (2  2) principal submatrix of the form
aii
aij
aji
ajj


where aij is used to denote the (i,j)th entry in (x0x)1. The conditions for the
submatrix to be positive deﬁnite are the standard ones, i.e., aii > 0 and aiiajj >
aijaji, the latter condition implying aiiajj > (aij)2 by the symmetry of (x0x)1, and
thus
aij

 < (aii)1/2(ajj)1/2 which is the boundedness result mentioned above.
Therefore, if aii ! 0 8i, then aij ! 0 8i and j:
Regarding the convergence of the diagonal entries of (x0x)1 to zero, examine
the (1,1) entry of (x0x)1. We will again have use for the concept of partitioned
inversion (see Lemma 4.3), where we partition the x matrix as x ¼ [x.1 x*], where
x.1 denotes the ﬁrst column of x and x* denotes the remaining (k  1) columns of
the x matrix. Then
x0x ¼
x0
1x:1
x0
1x
x0x
1
x0x


and using partitioned inversion, the (1,1) entry in (x0x)1 is represented by
ðx0x Þ1
1;1 ¼ ðx
1
0x
1  x
1
0x x0x
ð
Þ1x0x
1Þ
1:
Thus, for the (1,1) entry in (x0x)1 to have a limit of zero as n ! 1, it is clear that
the expression in parentheses on the right-hand side of the preceding equality
must ! 1 as n ! 1. Because (x*0x*) is positive deﬁnite (recall footnote 18), this
necessarily requires that x∙10x∙1 ! 1 as n ! 1since a nonnegative quantity,
x∙10x*(x*0x*)1x*0x∙1, is being subtracted from x∙10x∙1.
Now note further that (x∙10x∙1x∙10x*(x*0x*)1x*0x.1) is the sum of the squared
deviations of x∙1 from a vector of predictions of x∙1 generated by a least squares-
estimated linear explanation of x∙1 (the dependent variable x∙1 is being predicted
by a linear function of x*, which are the explanatory variables in this context).
To see this, note that the vector x*^bis the least distance from x∙1 when we choose
^b ¼ (x*0x*)1x*0x∙1, as we have argued previously with regard to the least squares
procedure. Then the deviations between entries in x∙1 and x*^b are represented by
the vector e1 ¼ x∙1  x*(x*0x*)1x*0x∙1, and straightforward matrix multiplication
then demonstrates that e10e1 ¼ x∙10x∙1  x∙10x*(x*0x*)1x*0x∙1 which is the recipro-
cal of the (1,1) entry of (x0x)1. Thus, in order for the (1,1) entry of (x0x)1 to
converge to zero, we require that e10e1 ! 1 as n ! 1. We thus speciﬁcally rule
out the possibility that e10e1 < d < 1 i.e., that the sum of squared deviations is
bounded as n ! 1. As a practical matter, it is sufﬁcient that the average squared
error, n1e10e1, in
predicting entries in x.1 via linear combinations
of
corresponding entries in the remaining columns of the x matrix exhibits some
8Suppose B is a k  k
ð
Þsymmetric positive deﬁnite matrix. Let C be a r  k
ð
Þmatrix such that each row consists of all zeros except for a
1 in one position, and let the r-rows of C be linearly independent. Then CBC0 deﬁnes an r  r
ð
Þ principal submatrix of B. Now note that
ℓ0 CBC0ℓ¼ ℓ*
0Bℓ* > 0 8 ℓ6¼ 0 since ℓ* ¼ C0ℓ6¼ 0 8 ℓ6¼ 0 by the deﬁnition of C, and B is positive deﬁnite. Therefore, the principal
submatrix CBC0 is positive deﬁnite.
8.2
The Least Squares Estimator
445

positive lower bound and thus does not converge to zero as n ! 1. Roughly
speaking, it is sufﬁcient to assume that a linear dependence between x∙1 and the
column vectors in x* never develops regardless of sample size.
Note that the column of x that we utilize in applying the partitioned
inversion result is arbitrary—we can always rearrange the columns of the x
matrix to place whichever explanatory variable we choose in the “ﬁrst posi-
tion.” Thus, our discussion above applies to each diagonal entry in the (x0x)1
matrix. Then in summary, our sufﬁcient condition for the consistency of ^b is
that limn!1((x0x)1) ¼ 0,9 which holds if (x∙i0x∙i) ! 18i, and if 9 some positive
number d > 0 such that no column of x can be represented via a linear combina-
tion of the remaining columns of x with an average squared prediction error less
than d as n ! 1.
Consistency of ^S
2
Additional conditions beyond those underlying the Classical
GLM are necessary if ^S
2 is to be a consistent estimator s2, as the next two
theorems indicate.
Theorem 8.5
Consistency of ^S
2-iid
Residuals
If in addition to the assumptions of the Classical GLM, the elements of the
residual vector, e, are iid, then ^S
2 !
p s2, so that ^S
2 is a consistent estimator of s2.
Proof
Recall from our discussion of the unbiasedness of ^S
2 that ^S
2 ¼
ðYx^bÞ0ðYx^bÞ


= nk
ð
Þ¼ «0ðIxðx0xÞ1x0Þ«


= nk
ð
Þ: Examine
plim ^S
2


¼ plim
«0«
n  k


 plim «0xðx0x Þ1 x0«
n  k
 
!
and focus on the last term ﬁrst. Note that
«0x x0x
ð
Þ1x0«


=ðn  kÞ
is a
nonnegative-valued random variable since (x0x)1 is positive deﬁnite. Also note
that under the Classical GLM assumptions E(««0) ¼ s2I, so that
E ðn  k Þ1 «0xðx0x Þ1 x0«


¼ ðn  k Þ1 tr xðx0x Þ1 x0E ««0
ð
Þ


¼ s2 n  k
ð
Þ1tr
x0x
ð
Þ1x0x


¼ s2 ðk=ðn  kÞÞ
Then by Markov’s inequality, 8c > 0,
P ðn  k Þ1 «0xðx0x Þ1 x0«  c


 s2 ðk=ðn  kÞÞ
c
and since limn!1s2 k= n  k
ð
Þ
ð
Þ ¼ 0, it follows that plim ðn  k Þ1 «0xðx0x Þ1

x0«

¼ 0.
9Consistency of ^b can be proven under alternative conditions on x. Judge, et al., (1982) Introduction to the Theory and Practice of
Econometrics, John Wiley, pp. 269–269, prove the result using the stronger condition that limn!1 n1 x0x ¼ Q, where Q is a ﬁnite,
positive deﬁnitive matrix. Halbert White, (1984) Asymptotic Theory for Econometricians, Academic Press, p. 20, assumes that n1 x0x
is bounded and uniformly positive deﬁnite, which is also a stronger condition than the one we use here.
446
Chapter 8
Point Estimation Methods

Given the preceding result, it is clear that for plim(^S
2) ¼ s2, it is necessary
and sufﬁcient that plim «0«= n  k
ð
Þ
ð
Þ ¼ s2. Recalling that plim(WZ) ¼ plim(W)
plim(Z)) by Theorem 5.6 and since
n  k
ð
Þ=n
ð
Þ ! 1 as n ! 1, an equivalent
necessary and sufﬁcient condition is thatplim «0«=n
ð
Þ ¼ s2. If the ei0s are not only
uncorrelated, but also iid, then Khinchin’s WLLN (Theorem 5.19) allows us to
conclude the preceding plim result, because E e2
i


¼ s2; 8i the e2
i ’s are iid, and
n1«0« ¼ n1 Pn
i¼1 e2
i , which is precisely in the form of Khinchin’s theorem
applied to the sequence of iid random variables {e2
1; e2
2; e2
3,. . .}. Thus, plim «0«=n
ð
Þ
¼ s2, and ^S
2 is a consistent estimator of s2.
n
Consistency of ^S
2 for estimating s2 can be demonstrated without requiring
that the ei0s are iid.
Theorem 8.6
Consistency of ^S
2:
Non-iid Residuals
Assume the Classical Assumptions of the GLM. Then ^S
2 is a consistent estima-
tor of s2 if E e4
i


 t<1; 8i and either of the following conditions hold:
a. Pn
j¼1;j6¼i Cov e2
i ; e2
j


¼ o n1


; 8i.
b. {en} is an m-dependent sequence.
Proof
See Appendix.
n
Regarding the conditions in Theorem 8.6, ﬁrst note that the boundedness of
the moments of the residuals, i.e., E e4
i


 t<1; 8i for some t > 0, is a
relatively weak assumption. Note, for example, that these moment bounded-
ness assumptions are satisﬁed for every parametric family of density functions
that we examined in Chapter 4. The remaining conditions on the residuals relate
to the degree of association between outcomes of ei and ej as i and j become
further and further apart. Essentially, (a) implies that the average covariance
between e2
i and all of the remaining squared residuals, e2
j for j > i, converges to
zero 8i; and (b) states that ei and ej become independent of one another when
i  j
j
j exceeds a ﬁxed ﬁnite value. If the random sample refers to a time series of
observations, then i  j
j
j refers to separation of ei and ej in the time dimension. If
the random sample refers to a cross-section of observations, then one must
search for an ordering of the elements in the random sample to which the
conditions of Theorem 8.6 can be applied.
Asymptotic Normality of ^b
We establish the asymptotic normality of ^b by
initially stipulating that the ei0s are iid, that xij

 < x < 1 8i, j, i.e., the explana-
tory variable values are bounded in absolute value, and that P( ei
j j < m) ¼ 1 8i, for
m < 1, i.e., the disturbance terms are bounded in absolute value with probabil-
ity 1. As a practical matter, it is often reasonable to assume that real-world
explanatory variables in a linear model do not have values that increase without
bound; e.g., all published data on the economy, census data, samples of socio-
demographic information and the like contain ﬁnite numbers, no matter how
8.2
The Least Squares Estimator
447

vast the collection. Indeed it is safe to say that all of the numerical data ever
measured and recorded can be bounded in absolute value by some large enough
ﬁnite number, x. The boundedness assumption on the explanatory variables
thus has wide applicability.10 Similar reasoning suggests that, in practice,
boundedness of the disturbance terms in a linear model is often a reasonable
assumption. That is, if yi is an observation on real-world economic or social data
and thus bounded in magnitude, it follows that ei ¼ yi  E(Yi) would also be
bounded, so that P( ei
j j < m) ¼ 1.
We ﬁnally assume that limn!1 n1x0x


¼ Q, where Q is a ﬁnite, positive
deﬁnite matrix. This of course implies that n1x0x has full rank, not just for a
ﬁnite number of observations which would be ensured by rank(x) ¼ k, but also
in the limit as n ! 1. It also requires that the entries in n1x0x be bounded (or
else the limit would not exist). Note that this assumption poses no conceptual
difﬁculty if x is ﬁxed and designed by the researcher (or someone else), since the
x matrix can then always be designed to behave in the required manner. If x is
not designed, it must be assumed that the xij0s are generated in a way that allows
the limit of n1x0x to be ﬁnite and positive deﬁnite.
Theorem 8.7
Asymptotic Normality
of ^b–iid Residuals
Assume the Classical Assumptions of the GLM. In addition, assume that {en} is
a collection of iid random variables and that P( ei
j j < m) ¼ 1 for m < 1 and 8i.
Finally, assume that the explanatory variables are such that xij

 < x < 1 8i
and j, and that limn!1 n1x0x


¼ Q, where Q is a ﬁnite positive deﬁnite matrix.
Then
n1=2ð^b  bÞ !
d Nð0; s2Q1Þ and ^b 
a Nðb; n1s2Q1Þ:
Proof
Recall that the least squares estimator can be expressed as ^b ¼ b + (x0x)1 x0 «.
Then n1/2(^b  b) ¼ n1/2 (x0x)1 x0 « ¼ (n1 x0x)1 n1/2x0«. Note that n1/2x0« can
be written as the sum of n k-vectors as n1=2x0« ¼ n1=2 Pn
i¼1 x0i:ei where xi: is the
(l  k) vector representing the ith row of x. The expectation of the (k  l)
random vector x0i:ei is 0 since E(ei) ¼ 0. The covariance matrix of the random
vector is given by Cov x0i:ei
ð
Þ ¼ E ðx0i:eiÞðeixi:Þ
ð
Þ ¼ E e2
i


x0i:xi: ¼ s2x0i:xi:.
The random vectors x0i:ei , i ¼ 1,. . .,n, are independent because the ei0s are
independent, and the entries in the random vector x0i:ei are bounded in absolute
value with probability 1, 8i, since P( ei
j j < m) ¼ 1 and xij

 < x, so that P( xijei


< mx) ¼ 1 8i,j. Given that
10We should note, however, that in the speciﬁcation of some linear models, certain proxy variables might be used to explain y that
literally violate the boundedness assumption. For example, a linear “time trend” t ¼ (1,2,3,4,. . .) is sometimes used to explain an
apparent upward or downward trend in E(Yt)and the trend clearly violates the boundedness constraint. In such cases, one may wonder
whether it is really to be believed that t ! 1 is relevant in explaining y, or whether the time trend is just an artiﬁce relevant for a
certain range of observations, but for which extrapolation ad inﬁnitum is not appropriate.
448
Chapter 8
Point Estimation Methods

limn!1 n1 Xn
i¼1 Covðx0
i:eiÞ


¼ limn!1 n1 Xn
i¼1 s2 x0
i:xi:
ð
Þ


¼ limn!1 n1 Xn
i¼1 s2 x0x
ð
Þ


¼ s2 Q;
and since Q is a ﬁnite positive deﬁnite matrix, it follows directly from Theorem
5.38 concerning a multivariate CLT for independent bounded random vectors
that n1=2 Pn
i¼1 x0i:ei ¼ n1=2x0« !
d N 0; s2Q


.
Then by Slutsky’s theorem, since limn!1 n1x0x


¼ plim n1x0x


¼ Q, and
thus
limn!1
n1x0x

1


¼ plim
n1x0x

1


¼ plim n1x0x



1 ¼ Q1;
we have that
n1 2
= ð^b  bÞ ¼ n1 2
= ðx0xÞ1x0« ¼ ðn1x0xÞ
1n1 2
= x0« !
d Nð0; s2Q1Þ
which proves the limiting distribution part of the theorem. The asymptotic
distribution then follows directly from Deﬁnition 5.2.
□
While the previous theorem establishes the asymptotic normality of the
least squares or BLUE estimator ^b, the asymptotic distribution is expressed in
terms of the generally unobservable limit matrix Q. We now present a corollary
to Theorem 8.6 that replaces n1Q1 with the observable matrix (x0x)1, leading
to a covariance matrix for the asymptotic distribution that is identical to the
ﬁnite sample covariance matrix of ^b.
Corollary 8.1
Alternative Normal
Asymptotic Distribution
Representation for ^b–iid
Residuals
Under the conditions of Theorem 8.6, ðx0x Þ1=2 ð^b  bÞ !
d Nð0; s2IÞ and ^b 
a
Nðb; s2 ðx0x Þ1Þ.
Proof
By Theorem 8.7 and Slutsky’s theorem, it follows that Q1=2 n1=2 ð^b  bÞ !
d
Nð0; s2 IÞ . Since (n1x0x)1/2Q1/2 ! Q1/2 Q1/2 ¼ I because n1 x0x ! Q,11
Slutsky’s theorem also implies that
ðn1 x0x Þ1=2 Q1=2 Q1=2 n1=2 ð^b  bÞ ¼ ðx0x Þ1=2 ð^b  bÞ !
d Nð0; s2 IÞ;
so that ^b 
a Nðb; s2 ðx0x Þ1Þ.
n
11Regarding the symmetric square root matrix A1/2, note that P0AP ¼ L, where P is the orthogonal matrix of characteristic vectors of the
symmetric positive semideﬁnite matrix A, and L is the diagonal matrix of characteristic roots. Then A1/2 ¼ PL1/2P0, where L1/2
is the diagonal matrix formed from L by taking the square root of each diagonal element. Note that since P0P ¼ I, A1/2A1/2 ¼ PL1/2
P0PL1/2P0 ¼ PL1/2L1/2P0 ¼ PLP0 ¼ A. The matrix square root is a continuous function of the elements of A. Therefore, limn!1(An)1/2 ¼
limn!1An
ð
Þ1=2.
8.2
The Least Squares Estimator
449

The
asymptotic
normality
of
the
least
squares
estimator
can
be
demonstrated more generally without reliance on the boundedness of the dis-
turbance terms, without the iid assumption on the error terms and without
assuming the existence of a ﬁnite positive deﬁnite limit of n1 x0x as n ! 1.
The demonstration relies on somewhat more complicated central limit
arguments. We state one such result below.
Theorem 8.8
Asymptotic Normality
of ^b–non-iid Residuals
Assume the Classical Assumptions of the GLM. Then (x0x)1/2(^bb) !
d
N(0, s2I)
and ^b
a N(b,s2(x0x)1) if the elements of the sequence {en} are independent random
variables such that E(e4
i ) < t < 1 8i, and the explanatory variables are such that
xij

 < x < 1 8i and j, with det(n1 x0x) > Z > 0 and {n1 x0x} being O(1).
Proof
See Appendix.
n
The difference in assumptions between our previous result on the asymp-
totic normality of ^b and the result in Theorem 8.8 is that the existence of
moments of the ei0s of order four replaces the assumption that the errors are
bounded with probability 1, the ei0s are no longer required to be identically
distributed (although ﬁrst and second order moments are assumed to be the
same by the classical GLM assumptions), and bounds on the determinant and
elements of n1 x0x replace the assumption that limn!1(n1 x0x) exists and is
positive deﬁnite. The boundedness assumptions are less restrictive than the
assumption that limn!1 n1 x0x exists and is a nonsingular positive deﬁnite
matrix, especially in cases where there is no control over how the x matrix was
generated and no knowledge that the process generating the x matrix would
inherently lead to the existence of a limit for n1 x0x.
Regarding the moment condition on the ei0s, note that our previous assump-
tion P( ei
j j < m) ¼ 1 8i implies boundedness of the moments and, thus, implies
the moment condition in Theorem 8.7. However, the moment conditions in
Theorem 8.7 also allow situations in which there is no absolute upper bound to
the value of the error term that holds with probability one, and thus, for exam-
ple, allows the ei0s to have a normal distribution or a mean-shifted (to zero)
gamma distribution whereas the former assumption would not.
In summary, the least squares estimator ^b can be approximately normally
distributed, even if « is not multivariate normally distributed, given certain
conditions on the values of the explanatory variables and certain assumptions
regarding distributional characteristics of the disturbancevector. Notethatasymp-
totic normality of ^b can be demonstrated using still weaker assumptions—the
interested reader is directed to Chapter 5 of the book by White, H., Asymptotic
Theory for Econometricians, Academic Press, 1984, for further details.
Asymptotic Normality of ^S
2
If in addition to the classical GLM assumption it is
assumed that the ei0s are iid and have bounded fourth-order moments about the
origin, then ^S
2 will be asymptotically normally distributed.
Theorem 8.9
Asymptotic Normality
of ^S
2-iid Residuals
Under the Classical Assumptions of the GLM, if the elements of the residual
vector, «, are iid, and if E e4
i


 t<1, then
450
Chapter 8
Point Estimation Methods

n1=2 ^S
2  s2


!
d N 0; m0
4  s4


and ^S
2 
a N s2; n1 m0
4  s4




:
Proof
Note that
n1=2 ^S
2  s2


¼
n1=2
n  k
ð
Þ «0 I  x x0x
ð
Þ1x0


«  n1=2s2
¼
n1=2«0«
n  k
ð
Þ  n1=2s2


 n1=2
n  k «0x x0x
ð
Þ1x0«:
Based on Markov’s inequality,
P
n1=2
n  k


«0x x0x
ð
Þ1x0«  c


 s2 n1=2 k
cðn  kÞ
(recall the proof of Theorem 8.4), and since s2 n1/2 k/(c(n  k)) ! 0 as n ! 1,
plim
n1=2= n  k
ð
Þ


«0x x0x
ð
Þ1x0« ¼ 0: Then it follows from Slutsky’s theorem
that the limiting density of n1/2 (^S
2  s2) will depend only on the bracketed term
in its deﬁnition above. Also by Slutsky’s theorem, the limiting density is unaf-
fected if we multiply the bracketed term by n  k
ð
Þ=n, since n  k
ð
Þ=n ! 1 as
n ! 1, or if we then also add the term ks2/n1/2 since ks2/n1/2 ! 0 as n ! 1.
Thus to establish the limiting density of n1/2 (^S
2  s2), it sufﬁces to examine the
limiting density of
«0«
n1=2  ðn  kÞs2
n1=2
 ks2
n1=2 ¼ «0«  ns2
n1=2
:
Since E e4
i


 t < 1 , var e2
i


 t  s4 < 1, a direct application of the
Lindberg-Levy CLT to the sequence of iid random variables
e2
1; e2
2; e2
3; . . .


yields
«0«  ns2
n1=2 m04  s4
ð
Þ1=2 !
d Nð0; 1Þ
and thus by Slutsky’s theorem
«0«  ns2
n1=2
!
d N 0; m0
4  s4


:
Therefore,
n1=2 ^S
2  s2


!
d N 0; m0
4  s4


and ^S
2 
a N s2; m0
4s4
n


:
n
The asymptotic normality of ^S
2 can be established without assuming that
the disturbance terms are iid. We present one such result in the following
theorem.
Theorem 8.10
Asymptotic Normality
of ^S
2–Non-iid Residuals
Assume the Classical Assumptions of the GLM. Also assume that the ei0s are
independent and E je2
i  s2 j2þd


 t<1 8i for some d > 0. Then letting xn ¼
n1 Pn
i¼1 var e2
i


 >0 8n;, n1=2 ^S
2  s2


=x1=2
n
!
d N 0; 1
ð
Þ and ^S
2 
a N s2; n1xn


.
8.2
The Least Squares Estimator
451

Proof
See Appendix.
□
Summary of Least Squares Estimator Properties for the Classical General Linear
Model
Table 8.1 summarizes assumptions and the resultant properties of ^b and ^S
2 as
estimators of b and s2 in the GLM context.
Example 8.2
Least Squares Estimator
When Random
Sampling from a
Population Distribution
Let (Y1,. . .,Yn) be a random sample from a population distribution having ﬁnite
mean b and ﬁnite variance s2 (both scalars). The GLM representation of the
vector Y is given by
Y ¼
Y1
Y2
..
.
Yn
2
6664
3
7775 ¼ xb þ « ¼
1
1
..
.
1
2
664
3
775b þ
e1
e2
..
.
en
2
6664
3
7775;
where E(«) ¼ 0, and since the ei0s are iid,E ««0
ð
Þ ¼ s2I. Then note that^b ¼ (x0x)1x0Y
¼ n1 Pn
i¼1 Yi ¼ Y, so that the least squares estimator of b is the sample mean.
Furthermore, ^S
2 ¼ (Y  x^b)0(Y  x^b)/(n  k) ¼ Pn
i¼1 Yi Y

2=(n  1) is used as an
estimator of s2. The reader can verify that cases 2 and 3 in Table 8.1 are met, and
thus ^b is BLUE and consistent for b, while ^S
2 is an unbiased and consistent
estimator for s2. Under additional assumptions such as cases 5–8 in Table 8.1, or
recall those discussed in Chapter 6 concerning the asymptotic normality of the
sample mean and sample variance, ^b and ^S
2 are both asymptotically normally
distributed.
□
Example 8.3
Empirical Application of
Least Squares to a GLM
Speciﬁcation
A realtor is analyzing the relationship between the number of new one-family
houses sold in a given year in the United States and various explanatory
variables that she feels had important impacts on housing sales during the
decade of the 1980s. She has collected the following information:
# New
homes sold
(1,000s)
Conventional
mortgage
interest rate
Medium
family
income
New home
purchase price
(1,000’s)
CPI
(1982–1984 ¼ 100)
1980
545
13.95
21,023
83.2
82.4
1981
436
16.52
22,388
90.3
90.9
1982
412
15.79
23,433
94.1
96.5
1983
623
13.43
24,580
93.9
99.6
1984
639
13.80
26,433
96.8
103.9
1985
688
12.28
27,735
105.0
107.6
1986
750
10.07
29,458
119.8
109.6
1987
671
10.17
30,970
137.2
113.6
1988
676
10.30
32,191
150.5
118.3
1989
650
10.21
34,213
160.1
124.0
z
609
12.65
27,242
113.1
104.6
Source: Statistical Abstracts of the United States, (1991), U.S. Dept. of Commerce, Washington
D.C., Tables 1272, 837, 730, 825, and 771.
452
Chapter 8
Point Estimation Methods

Table 8.1
Assumptions and properties of ^b and ^S
2 in the GLM point estimation problem
Properties
Case
Classical GLM
assumptions
Additional
x-assumptions
Additional «-assumptions
^b
S2
1.
Hold
–
–
BLUE
Unbiased
2.
Hold
(x0x)1 ! 0
–
Consistent
–
3.
Hold
–
ei0s are iid
–
Consistent
4.
Hold
–
E e4
i
 
 t<1
–
Consistent
en
f
g is m  dependent or
cov e2
i ; e2
j


! 0; ji  jj ! 1
(
)
–
5.
Hold
xij

 < x < 1
n1 x0x ! Q positive
deﬁnite
ei0s are iid
P( ei
j j < m) ¼ 1

a N b; s2 x0x
ð
Þ1


–
6.
Hold
xij

 < x < 1
det(n1 x0x) > Z > 0
{n1x0x} is 0(1)
ei0s are independent
E e4
i
 
 t < 1
7.
Hold
–
ei0s are iid, E e4
i
 
 t < 1
–

a N s2; n1 m0
4  s4
ð
Þ
ð
Þ
8.
Hold
–
ei0s are independent
Eðje4
i  s2 j2þdÞ  t < 1; d > 0
xn ¼ n1 Pn
i¼1 var(e4
i )  Z > 0
–

a N(s2, n1 xn)
8.2
The Least Squares Estimator
453

She speciﬁes the dependent variable, Y, as the number of new homes sold,
and the explanatory variable matrix, x, contains a column of 1’s (for the inter-
cept), followed by the remaining four variables in the table above. That is,
x[.,1] ¼ 1, x[.,2] ¼ Conventional Mortgage Interest Rates, x[.,3] ¼ Median Fam-
ily Incomes, x[.,4] ¼ New Home Purchase Prices, x[.,5] ¼ CPIs, and the GLM is
given by Y ¼ P5
i¼1 bix½:; i	 þ «. She calculates the following values on her per-
sonal computer:
b ¼ ðx0xÞ1x0y ¼
1074:82
46:01
:0376
6:16
2:03
2
66664
3
77775
; ^s2 ¼ ðy  xbÞ0ðy  xbÞ=5 ¼ 905:32; R2 ¼ :96:
The linear model thus explains 96 percent of the observed sample variation in
new homes sold in terms of the values of mortgage interest rates, median family
incomes, home prices, and the general cost of living. An estimate of the covari-
ance matrix of the entries in ^b is calculated as
^s2ðx0xÞ1 ¼
79586:35
3389:91
5:9323
273:0789
896:8833
191:4445
:4094
14:5792
81:6034
:0013
:0538
:2665
ðsymmetricÞ
3:3401
9:5606
60:3537
2
66664
3
77775
:
The entries in b divided by their respective estimated standard deviations are
given as (3.80, 3.33, 1.05, 3.37, .26). Thus, the parameters associated with
the intercept, interest rates, and new home prices are each more than three
standard deviations away from zero (further insight into the signiﬁcance of
this observation will be provided in Chapter 10).
The realtor calculates estimates of elasticities of home sales with respect to
each of the explanatory variables, other than the intercept, evaluated at the
means of the observations, as
esales;rate ¼ b2
x2
y ¼ 46:01 12:65
609


¼ :96
esales;income ¼ b3
x3
y ¼ :0376 27; 242
609


¼ 1:68;
esales;price ¼ b4
x4
y ¼ 6:16 113:1
609


¼ 1:14;
esales;CPI ¼ b5
x5
y ¼ 2:03 104:6
609


¼ :35:
Based on the estimated elasticities, home sales appear to be quite responsive to
changes in interest rates, income levels, and home prices. Response to changes
in the general cost of living is notably inelastic.
□
454
Chapter 8
Point Estimation Methods

8.2.3
Violations of Classic GLM Assumptions
The Classical assumptions of the GLM given in Deﬁnition 8.1 form essentially
the base-level set of assumptions on which useful properties of the least squares
estimator depend. It is instructive to examine the effect that a violation in each
of the Classical assumptions has on the basic estimator properties of unbiased-
ness, BLUE, and consistency.
Assumption Violation: E(«) 6¼ 0
In this section we address implications of Eð«Þ
6¼ 0, but we retain the other classical GLM assumptions. Note the covariance
assumption must then be restated to accommodate the nonzero mean of «, as
Cov(Y) ¼ s2I ¼ Cov(«) ¼ E(«  E(«))(«  E(«))0.
^b Properties
If E(«) ¼ f 6¼ 0, then since ^b ¼ b + (x0x)1x0«, E(^b) ¼ b + (x0x)1x0f. Except for
the special case where x0f ¼ 0, the estimator ^b is a biased estimator of b since
(x0x)1x0f ¼ 0 iff x0f ¼ (x0x) ∙0 ¼ 0. If x0f 6¼ 0, then ^b is clearly not BLUE since
^b is not even unbiased. In the special case where x0f ¼ 0, ^b retains the BLUE
property, as can be veriﬁed by repeating the previous derivation of the BLUE for b
using the conditions E(«) ¼ f and x0f ¼ 0.
Regarding consistency, it follows from ^b ¼ b + (x0x)1x0«, that plim ^b ¼ b iff
plim((x0x)1x0 «Þ ¼ 0. If we assume as before that (x0x)1 ! 0 when n ! 1
(Table
8.1,
Condition
2),
then
since
E((x0x)1x0«
 (x0x)1x0f) ¼ 0
and
Cov((x0x)1x0«) ¼ s2(x0x)1 ! 0 as n ! 1 it follows from mean square conver-
gence that plim ((x0x)1x0 «  (x0x)1x0f) ¼ 0. Thus, if limn!1((x0x)1x0f) ¼
j 6¼ 0, or if (x0x)1x0f does not converge to a limit, then ^bwould not be consistent
for b since plim((x0x)1x0«) 6¼ 0 and plim (^b) 6¼ b.
^S
2 Properties
Since ^S
2 ¼ (n  k)1 tr((I  x(x0x)1x0)««0), Cov(«) ¼ s2I, and E(««0) ¼ s2I + ff0,
the expectation of ^S
2is given by
E ^S
2


¼ s2 þ
f0 I  x x0x
ð
Þ1x0


f
n  k
:
Unless f0(I  x(x0x)1x0) f ¼ 0, the estimator
^S
2
is a biased estimator of
s2 and the bias is nonnegative, since the matrix I  x(x0x)1x0 is symmetric
and idempotent, and hence positive semideﬁnite.12 Note that if E(«) ¼ f 6¼ 0,
it is impossible for both ^b and ^S
2 to remain unbiased. To see this, ﬁrst note that if
x0f ¼ 0, so that ^b is unbiased, then the bias in ^S
2 is (E(^S
2)  s2) ¼ f0f= n  k
ð
Þ > 0.
Alternatively, if ^S
2 is unbiased, then it must be the case that f0f ¼ f0x(x0x)1x0f
12Positive semideﬁniteness can be deduced from the fact that the characteristic roots of a symmetric idempotent matrix are all
nonnegative, being a collection of 0’s and 1’s.
8.2
The Least Squares Estimator
455

> 0, and since f0x(x0x)1x0f is a positive deﬁnite quadratic form in the vector x0f
(i.e., (x0x)1 is positive deﬁnite), it necessarily follows that x0f 6¼ 0 if f0x(x0x)1x0f
is positive, and thus ^b is biased. Thus at least one, and generally both of the
estimators ^b and ^S
2 will be biased if E(«) 6¼ 0.
Regarding consistency, deﬁne V ¼ « d, and note that ^S
2 ¼ (n  k)1 (f + V)0
(I  x(x0x)1x0) (f + V) where E(V) ¼ 0 and Cov(V) ¼ s2I. Letting M ¼ I  x
(x0x)1x0, it follows that ^S
2 ¼ (n  k)1[f 0M f2 f 0MV + V0MV], so that ^S
2 is
consistent iff the expression on the right hand side of the equality has a proba-
bility limit of s2. To illustrate that this is generally not true, assume that the Vi0s
are iid, which is tantamount to assuming that the ei0s are independent and
identically distributed once they are transformed to have mean zero, as ei  fi.
Then plim((n  k)1 V 0MV) ¼ s2, which follows from an argument identical to
that used in the proof of Theorem 8.5. It follows that ^S
2 will be consistent for
s2 iff (n  k)1 [f 0M f2 f 0MV] !
p
0.
Now assume |fi|  c < 1, so that the means of the residuals are ﬁnite and
f 0f ¼ O(n1), or at least assume that f 0f is o(n2), implying that the sum of the
squared means approaches inﬁnity at a rate less than n2. It follows that f 0M f is
o(n2), since 0  f 0M f ¼ f 0ff 0x(x0x)1 x0 f  f 0f. Then since E(2(n  k)1
f 0MV) ¼ 0 and var (2(n – k)–1 f 0MV) ¼ 4s2 f 0M f/(n – k)2  4s2o(n2)/(n – k)2
! 0 as n ! 1, plim (2(n – k)1d0MV) ¼ 0 follows from mean square conver-
gence. Thus, except for the special case limn!1 (n  k)1d0M f ¼ 0, ^S
2 will not
be consistent for s2.
Assumption Violation: E ««0
ð
Þ 6¼ s2I
In this section we examine the impacts of
having E ««0
ð
Þ ¼ F 6¼ s2I, so that the error terms are heteroskedastic (unequal
variances) and/or autocorrelated (nonzero correlations between some ei and ej0s,
i 6¼ j)
^b Properties
The estimator ^b remains unbiased for b regardless of heteroskedasticity or
autocorrelation so long as the remaining Classical assumptions of the GLM
remain true. This follows because the representation ^b ¼ b + (x0x)1x0 « is
unaffected, and taking expectations still yields E(^b) ¼ b + (x0x)1x0E(«) ¼ b.13
To examine consistency, note that now Cov( ^b ) ¼ (x0x)1x0fx(x0x)1. If it
were true that var(^bi) ! 0 as n ! 1 8i, then since E(^b) ¼ b, we could conclude
by convergence in mean square that plim(^b) ¼ b. Under mild conditions on the
f matrix, and assuming as before that (x0x)1 ! 0, convergence of var(^bi) to zero
8i will be achieved. The argument is facilitated by the following matrix theory
lemma:
13We are making the tacit assumption that x contains no lagged values of the dependent variable, which is as it must be if it is
presumed that x can be held ﬁxed. In the event that x contains lagged values of the dependent variable and the error terms are
autocorrelated, then in general E((X0X)1X0«) 6¼ 0, and ^b is biased. Issues related to this case are discussed in subsection 8.2.3.
456
Chapter 8
Point Estimation Methods

Lemma 8.3
Let A and B be symmetric positive semi-deﬁnite matrices of order (n  n).
Then tr(AB)  lL(A) tr(B), where lL(A) represents the value of the largest
characteristic root of the matrix A.
Proof: Let P be the (n  n) characteristic vector matrix of the symmetric
matrix A, so that P0AP ¼ L, where L is the diagonal matrix of characteristic
roots of A. Since PP0 ¼ I by the orthogonality of P, it follows by a property of
the
trace
operator
(tr(DFG) ¼ tr(GDF))
that
tr(AB) ¼ tr(APP0BPP0) ¼ tr
(P0APP0BP) ¼ tr (LC), where C ¼ P0BP. But then tr (LC) ¼ Pn
i¼1 liCii  lL(A)
Pn
i¼1 Cii where li is the ith diagonal entry in the diagonal matrix of character-
istic roots L, Cii represents the (i,i)th entry in C and li  0 and Cii  0 8i by
the positive semideﬁniteness of A, B, and P0BP. But since Pn
i¼1 Cii ¼ tr(C) ¼
tr(P0BP) ¼ tr(BPP0) ¼ tr(B), we have that tr(AB)  lL(A) tr(B).
n
Using the lemma, and properties of the trace operator, note that
tr x0x
ð
Þ1x0 F x x0x
ð
Þ1 ¼ tr F x x0x
ð
Þ2x0  lLðFÞ tr x x0x
ð
Þ2x0 ¼ lLðFÞ tr x0x
ð
Þ1:
Thus, if lL(F) < t < 1, i.e. if the largest characteristic root of F is bounded, then
since tr(x0x)1 ! 0 as n ! 1, tr(x0x)1 x0 F x(x0x)1 ! 0 as n ! 1. It follows
that all of the diagonal entries in (x0x)1 x0 F x(x0x)1 must converge to zero, since
the (k  k) matrix is (at least) positive semideﬁnite and thus has nonnegative
diagonal entries, and the sum of these k nonnegative numbers converging to zero
requires that each diagonal entry converge to zero. Thus, by convergence in
mean square, plim (^b) ¼ b even if E ««0
ð
Þ ¼ F 6¼ s2I.
As a practical matter, the assumption that lL(F) is bounded is not very
restrictive. The following matrix theory lemma will be useful for establishing
sufﬁcient conditions for the boundedness of lL(F) and the consistency of ^b.
Lemma 8.4
The absolute value of any characteristic root of a matrix, A, is less than or
equal to the sum of the absolute values of the elements in the row of A for
which the sum is largest.
See Hammarling, S.J. (1970), Latent Roots and Latent Vectors, Univ. of
Toronto Press, p. 9.
Using Lemma 8.4, we present three different sufﬁcient conditions on the
covariance matrix F that ensure the boundedness of lL(F) and thus the consis-
tency of ^b. Assume that var(ei) ¼ sii < x < 1 8i, so that the variances of the
disturbance terms exhibit some (perhaps very large) upper bound.
Consistency Sufﬁcient Condition 1: As a ﬁrst sufﬁcient condition, if the
disturbance vector is heteroskedastic, but there is zero autocorrelation, then it
is immediate that lL(F) < x, since F is a diagonal matrix of variances, and the
characteristic roots of a diagonal matrix are directly the diagonal elements.
Thus, lL(F) is bounded and plim ð^bÞ ¼ b.
8.2
The Least Squares Estimator
457

Consistency Sufﬁcient Condition 2: If the disturbance vector is either
heteroskedastic or homoskedastic and exhibits nonzero autocorrelation, then
lL(F) will be bounded if the covariance terms in F decline sufﬁciently fast as
i  j
j
j increases. The most straightforward sufﬁcient condition to examine in
this case is sij ¼ 0 for i  j
j
j > m, where m is some (perhaps very large) positive
integer m, which is true if the ei0s are m-dependent (recall Deﬁnition 5.10). Then
let row i of F be the row for which the sum of the absolute values of the elements
in the row is largest in comparison with the other rows of F. Because sij ¼ 0
when i  j
j
j > m, the maximum number of nonzero entries in row i is 2 m + 1.
Also by the bound on covariances, sij

  sii
j
j1/2 sjj

1/2 8i,j, it follows that the sum
of the absolute values of the entries in the ith row of the covariance matrix f is
upper-bounded by (2m + 1) x, and thus by Lemma 8.4, lL(F) < (2 m + 1)x. Then
since lL(F) is bounded, plim(^b) ¼ b.
Consistency Sufﬁcient Condition 3: Suppose that sij 6¼ 0 and there is no
value of i  j
j
j beyond which the covariance is assumed to be zero. Then again
using Lemma 8.4, a third sufﬁcient condition for lL(F) < t < 1 is that
Pn
j¼1 jsijj < t < 1 8i and 8n. Since we are assuming sii < x 8i, the sufﬁcient
condition can alternatively be stated in terms of the boundedness of P
j6¼i jsijj 8i.
Thus, if covariances decline sufﬁciently fast so that P
j6¼i jsijj < Z < 1 8i, then
plim (^b) ¼ b.
We should note that the consistency of ^b does not require that lL(F) be
bounded. It is sufﬁcient that lL(F) increase at a rate slower than the rate at which
tr(x0x)1 ! 0, for then it would still be true that lL(F) tr(x0x)1 ! 0. For addi-
tional results on the consistency of ^b the reader is referred to the book by H.
White, Asymptotic Theory.
Having seen that ^b is unbiased and consistent for b under general conditions
on E ««0
ð
Þ ¼ F, one may wonder whether ^b retains the BLUE property. In the
general (and usual) case where F is unknown, no linear estimator exists that has
the BLUE property. To see this, note that if E ««0
ð
Þ ¼ F, then
F1 2
= Y ¼ F1 2
= x0b þ F1 2
= « or Y ¼ x
0b þ «
satisﬁes the Assumptions of the Classical GLM (assuming all of the classical
assumptions except Cov(Y) ¼ Cov(«) ¼ s2I apply to Y ¼ xb + «). In particular,
note that E(«*«*0) ¼ F1/2 E(««0)F1/2 ¼ F1/2 F F1/2 ¼ I and E(«*) ¼ E(F1/2 «Þ
¼ F1/2 E(«) ¼ 0. Then the Gauss-Markov theorem applied to the transformed
linear model implies that
^b ¼ ðx
0xÞ1x
0Y ¼ ðx0 F1xÞ
1x0 F1Y
would be the BLUE for b. However, the deﬁnition of the BLUE estimator
depends on the unknown value of F, and because there does not then exist a
ﬁxed choice of A and b such that AY + d is BLUE for all potential values of F, no
BLUE estimator of b exists when F is unknown.
In the very special case where F is known up to a scalar multiple, i.e.,
F ¼ s2 V with V known, then the linear estimator ^b* ¼ (x0 F1x)1x0 F1Y ¼
(x0V1x)1 x0V1Y is the BLUE estimator. This special estimator is referred to in
the literature as the generalized least squares estimator of b.
458
Chapter 8
Point Estimation Methods

^S
2 Properties
The estimator ^S
2 under the condition E ««0
ð
Þ ¼ F 6¼ s2I is only useful if there
exists a counterpart to the parameter s2 which can be estimated. Two such
situations arise when F ¼ s2 V, and either V is a known positive deﬁnite
symmetric matrix, or else « is homoskedastic with common variance s2. In
either case,
E ^S
2


¼ n  k
ð
Þ1E «0 I  x x0x
ð
Þ1x0


«


¼ n  k
ð
Þ1tr
I  x x0x
ð
Þ1x0


E ««0
ð
Þ


¼ s2tr
O I  x x0x
ð
Þ1x0


n  k
ð
Þ
0
@
1
A
and thus ^S
2 is generally biased as an estimator of s2, since the trace of the matrix
in the preceding expression will generally not be equal to 1. Of course, in the
very special where V were known, ^S
2 could be scaled by the known value of the
trace to deﬁne an unbiased estimator of s2.
In certain cases, the bias in ^S
2 converges to 0 as n ! 1. Note that Lemma 8.3
implies that
s2tr
Vx x0x
ð
Þ1x0


n  k
ð
Þ
0
@
1
A  s2lL V
ð
Þ
k
n  k
ð
Þ


;
and if lL(F) is bounded or at least o(n1) as we have argued above, then s2lL(V) ¼
lL(F)14 is bounded and
s2 lL ðVÞ k= n  k
ð
Þ
ð
Þ ! 0 as n ! 1. Then since
tr(Vx(x0x)1x0)  0,15 it must be the case that s2 tr(Vx(x0x)1x0)/(n  k) ! 0 as
n ! 1. It follows that E( ^S
2 ) ! s2 as n ! 1 iff tr(V/(n  k)) ! 1 as n ! 1,
which does occur if « is homoskedastic (« may still exhibit nonzero autocorrela-
tion) with s2 representing the common variance of the ei0s, for then tr(V) ¼ n,
since the diagonal entries of V would all be equal to the number 1, and then
n/(n  k) ! 1 as n ! 1. In the heteroskedastic case, there is no reason to expect
that the preceding condition will hold.
Regarding consistency of ^S
2 for s2, Markov’s inequality can be used as in the
proof of Theorem 8.5 to show that plim((n  k)1 «0x(x0x)1x0 «) ¼ 0, assuming
lL(F) is bounded. If « is homoskedastic with variance s2, and if the conditions
of Theorem 8.6 other than E ««0
ð
Þ ¼ s2I can be assumed to hold, then the weak
law of large numbers given by Theorem 5.22 implies that plim((n  k)1 «0«) ¼
plim (n1«0«) ¼ s2, and thus ^S
2 would be consistent for s2 (consistency under
alternative conditions on « can be demonstrated as well—recall Table 8.1). If « is
14The characteristic roots of tA are equal to the characteristic roots of A times the scalar t.
15Note tr(Vx(x0x)1x0) ¼ tr(V1/2x(x0x)1x0V1/2), and since (x0x)1 is positive deﬁnite, V1/2 x(x0x)1x0 V1/2 is at least positive semideﬁnite
and its trace must be nonnegative.
8.2
The Least Squares Estimator
459

heteroskedastic with Cov(«) ¼ s2V, while maintaining the remaining preceding
assumptions, then by Theorem 5.22
plim ðn  kÞ1 «0« 
Xn
i¼1 s2oii
h
i


¼ plim n1 «0«  s2 Xn
i¼1 oii
h
i


¼ 0
where oij denotes the elements in V, and except for the very special case
limn!1 n1 Pn
i¼1 oii ¼ 1, ^S
2 will not be consistent for s2.
Assumption Violation: Rank (x) < k, or x0x
j
j  0
In this subsection we assume that
either x is less than full column rank so that rank x < k, or else there is a “near”
linear dependency among the columns of x, so that x0x
j
j  0. Note that this latter
assumption is not a violation of the assumptions of the Classical GLM per se, but
rather “nearly” a violation. We retain the other assumptions of the classical GLM.
^b Properties
If the classical assumption concerning rank(x) ¼ k is violated, then the least
squares estimator ^b ¼ (x0x)1x0Y does not exist, since (x0x)1 does not exist. In
this case, there are an inﬁnite number of solutions to the problem of minimizing
(y  xb)0(y  xb) through choice of the vector b. To see this, recall that the ﬁrst
order conditions for the minimization problem are given by (x0x) b ¼ x0y, which is
a system of k equations in the k unknowns represented by the vector b. If x is less
than full column rank, so that (x0x) is less than full rank, then this system of linear
equations effectively contains one or more redundant equations, so that there are
essentially more unknowns than equations. Then there are an inﬁnite number of
solution values for b. In this case, the parameter vector b cannot be estimated
uniquely. This problem is referred to in the literature as perfect multicollinearity.
More frequent in applications is the case where rank(x) ¼ k, but x0x is nearly
singular, i.e., its determinant is near zero. In such cases, (x0x)1 tends to have
large diagonal entries, implying that the variances of elements of ^b, given by the
diagonal elements of s2(x0x)1, are very large. This follows from our previous
application of Lemma 8.2 on partitioned inversion to the matrix (x0x)1.
In particular, the lemma implies that the variance of ^bi(i.e., the (i,i)th entry in
s2(x0x)1) is given by s2 (x•i0x•ix•i0 x*(x*0x*)1 x*0 x•i)1 ¼ s2(ei0ei)1, where ei
represents a vector of deviations of the ith column of x, represented here by x•i,
from a least squares prediction of x•i based on a linear combination of
the columns of x other than x•i (these remaining (k  1) columns being
represented by x*). The more closely x•i can be approximated by x*^b (where
^b ¼ (x*0x*)1 x*0x•i), and thus the more closely that x•i is linearly related to the
remaining column vectors in the x matrix, the smaller is ei and ei0ei, and thus
the larger is var( ^bi ) ¼ s2(ei0ei)1. Note that so long as rank(x) ¼ k, and the
appropriate other assumptions hold, ^b remains BLUE and consistent as an
estimator of b. However, the large variances associated with the ^bi ’s imply
that in small samples, outcomes of ^b can be quite distant from b with high
probability.
460
Chapter 8
Point Estimation Methods

^S
2Properties
In the case of perfect multicollinearity, ^S
2 can nonetheless be used to generate
estimates of s2, and with minor modiﬁcations to the derivations used previously
to establish properties of ^S
2, it can be shown that S2 is unbiased and consistent
for s2. Of course, the deﬁnition of ^S
2 cannot be expressed in terms of (Y  x^b)0
(Y  x ^b)/(n  k) since ^b ¼ (x0x)1x0Y does not exist in this case. However, ^S
2
¼ (e0e)/(n  k) can nonetheless be calculated, where e0e is the minimum sum of
squared errors calculated from any choice of ^b * which solves the ﬁrst order
conditions (x0x) ^b* ¼ x0y. A rigorous demonstration of the properties of ^S
2 under
perfect multicollinearity relies on the notion of generalized inverses of matrices
(e.g., see Graybill, F., (1976), Theory and Application of the Linear Model,
Duxbury Press, pp. 23–39), and given that perfect multicollinearity is by far
the exception rather than the rule, we will not pursue the details here.
In the case where rank(x) ¼ k, but (x0x) is nearly singular, the proofs of
unbiasedness and consistency of ^S
2 nonetheless apply exactly as stated previ-
ously, and thus these properties are attained by ^S
2 under the assumptions
introduced heretofore. It is also useful to note that unlike the variances of the
^bi ’s, which can increase without bound as the multicollinearity becomes
increasingly severe, the variance of ^S
2 exhibits a ﬁnite bound, and this bound
is unaffected by the degree of multicollinearity. To derive such a bound, assume
the Classical GLM Assumptions hold, assume the ei0s are iid, and let E(e4
i ) ¼ m0
4
< 1 exist. Recall that var(^S
2) ¼ E(^S
4)  (E(^S
2))2 ¼ E(^S
4)  s4 where we have used
the fact that E(^S
2) ¼ s2. Now note that
^S
2 ¼ ðn  kÞ1ð«0«  «0xðx0xÞ1x0 «Þ  ðn  kÞ1«0«;
since x(x0x)1x0 is positive semideﬁnite, which implies
E ^S
4



n  k
ð
Þ2E «0«
ð
Þ2 
n  k
ð
Þ2E
X
n
i¼1
e4
i þ 2 SS
i<j e2
i e2
j
"
#

n  k
ð
Þ2 nm0
4 þ n n  1
ð
Þs4

	
¼ tðnÞ
where
t(n)  s4,
t(n)
is
monotonically
decreasing
in
n
8n > k,
and
limn!1ðtðnÞÞ ¼ s4:
Then the variance of ^S
2 is bounded as var(^S
2)  t(n)  s4 regardless of the
severity of the multicollinearity, so long as rank(x) ¼ k. Other bounds can be
derived in cases where the ei0s are not iid.
Assumption Violation: Stochastic Xwith EððX0XÞ1X0«Þ 6¼ 0 and plim ðX0XÞ1 X0«
6¼ 0
In this section we focus explicitly on the case where the explanatory
variable matrix X is a nondegenerate random matrix. Note the condition
8.2
The Least Squares Estimator
461

E((X0X)1X0«) 6¼ 0 necessarily implies (by the iterated expectation theorem) that
E(«|x) 6¼ 0 with positive probability, and so the conditional mean of « is not
independent of X. This in turn implies that E(Y|x) 6¼ xb with positive probabil-
ity, and thus assumption 1 of the Assumptions of the Classical GLM, applied
explicitly to the stochastic X case, is violated. Furthermore, the dependence
between X and « persists in the limit, as indicated by plim((x0x)1X0«) 6¼ 0. The
interdependence of X and « is typically caused by either measurement error in
the x outcomes or by simultaneous determination of Y and X outcomes, and is
discussed in the econometric literature under the categories of errors in
variables and simultaneous equations, respectively.
^b Properties
Since E(^b) ¼ b + E((X0X)1X0«) 6¼ b because E((X0X)1X0«) 6¼ 0, it follows that the
estimator is a biased estimator of b, so that ^b is also not BLUE. Regarding
consistency, plim(^b) ¼ b + plim((X0X)1X0«) 6¼ b because plim((X0X)1X0«) 6¼ 0
and so ^b is also not consistent.
^S
2 Properties
Letting d(x) ¼ E(«|x), the expectation of (n  k) ^S
2 can be written via the iterated
expectation theorem as
E n  k
ð
Þ^S
2 ¼ E «0«
ð
Þ  E E «0X X0X
ð
Þ1X0«jX




¼ ns2  E tr X X0X
ð
Þ1X0E ««0jX
ð
Þ


h
i
¼ ns2  E tr X X0X
ð
Þ1X0 Cov «jX
ð
Þ þ dðXÞdðXÞ0






¼ n  k
ð
Þs2  E dðXÞ0X X0X
ð
Þ1X0dðXÞ


assuming Cov(«|x) ¼ s2I, 8x.16 Thus E (^S
2) < s2 in general, so that ^S
2 is biased.
Regarding the consistency of^S
2, ﬁrst note that^S
2 ¼ (n  k)1«0M « ¼ n1« 0M «
+ op(1) ¼ n1 «0«n1 «0X(X0X)1X0 « + op(1), and since (n1 «0«  s2) !
p 0,
^S
2  s2


¼ «0X X0X
ð
Þ1 n1X0X


X0X
ð
Þ1X0« þ opð1Þ:
If n1 X0X !
p Q, a positive deﬁnite symmetric matrix, and if plim((X0X)1X0«) ¼
j 6¼ 0, then (^S
2  s2) !
p  j0Qj < 0, and ^S
2 is not consistent. Even if neither
n1X0X nor (X0X)1X0« converge at all, so long as det(n1x0x) > Z > 0 8n, as in
Table 8.1, and (X0X)1X0«
p 0, it follows that ( ^S
2  s2)
p 0 and ^S
2 is not
consistent.
16Recall that E ««0
ð
Þ ¼ Cov «
ð Þ þ E «
ð ÞE «0
ð
Þ.
462
Chapter 8
Point Estimation Methods

8.2.4
GLM Assumption Violations: Property Summary and Epilogue
The basic properties of the OLS-based estimators of b and s2 under the various
violations of the Classical GLM Assumptions are summarized in Table 8.2. The
row of the table identiﬁed as “terminology” indicates the label under which the
problem is generally discussed in the econometric literature. When reading
Table 8.2, it is assumed that the conditions in Table 8.1 hold, except those that
are noted.
A major focus of research based on the GLM concerns the detection and
remedies for violations of the assumptions needed to achieve the desirable
properties of the estimators of b and s2. Indeed, ﬁelds such as Econometrics,
Psychometrics, and Sociometrics were necessitated by the fact that, unlike
disciplines in which the experiments under investigation can generally be
designed and controlled to achieve the conditions needed for the least squares
estimator to have optimal properties, many experiments in economics, busi-
ness, psychology or sociology, and other social sciences, are oftentimes not
under the control of the researcher. Variations on the least squares estimator,
such as restricted least squares, the feasible generalized least squares estima-
tor, instrumental variables estimators, two and three stage least squares
estimators, generalized method of moments, and limited and full-information
maximum likelihood estimators represent procedures for remedying violations
Table 8.2
General Least Squares Estimator Properties Under GLM Assumption Violations
Violation
E(«) ¼ f 6¼ 0
E(««0) ¼ F 6¼ s2I
det(x0x)  0
Random X, E((X0X)1X0«) 6¼ 0,
Plim((X0X1)X0«) 6¼ 0
Terminology
Speciﬁcation error
Heteroskedasticity
and/or autocorrelation
Multicollinearity
Errors in variables, simultaneity
^b Properties
Unbiased
Generally no
Yes
Yes
No
yes if x0f ¼ 0
BLUE
Generally no
No
Yes
No
yes if x0f ¼ 0
Consistent
Generally no
Generally yes
Yes
No
yes if (x0x)1x0f ! 0
if lL(F) tr(x0x)1! 0
S2 Properties
Unbiased
Generally no
Generally no
Yes
No
yes if
f0(I  x(x0x)1x0)f ! 0
Consistent
Generally no
Generally no
Yes
No
yes if
f0(I  x(x0x)1x0)f ! 0
Yes if lL(F)is o(n1)
and ei0s are homoskedastic
8.2
The Least Squares Estimator
463

of the GLM classical assumptions that the reader will encounter in her
subsequent studies. We will examine some hypothesis tests for detecting GLM
assumption violations in Chapter 10.
8.2.5
Least Squares Under Normality
Before examining the implications of making speciﬁc parametric family
assumptions for the probability model underlying sample data (in the next
section), we emphasize that the type of random sample envisioned for (Y1,. . .,
Yn) in the GLM context is generally of the general experimental-type, i.e. note
that E(Yi) does not necessarily equal E(Yj) for i 6¼ j, nor is it necessarily assumed
that (Y1,. . .,Yn), or (e1,. . .,en), are iid. We now examine the assumption of normal-
ity for (Y1,. . .,Yn) and, hence, for (e1,. . .,en).
Finite Sample Distributions of ^b and ^S
2
In the GLM, under the classical GLM
assumptions, let Y ~ N(xb, s2I). Under the normality assumption, the classical
GLM assumptions necessarily imply the ei0s are iid normal, with mean zero and
variance s2. Furthermore, all of the assumptions in Table 8.1 corresponding to
the disturbance vector hold, except for P( ei
j j < m) ¼ 1 8i, which does not hold. Of
course, our entire preceding discussion concerning the properties of ^band ^S
2 then
applies equally well to the case where Y is multivariate normally distributed.
The question addressed in this section is “what additional properties can be
attributed to the estimators when Y is multivariate normally distributed?”
One immediate property is that ^b is multivariate normally distributed for
every n  k, and not just asymptotically normally distributed. This follows
straightforwardly from the fact that ^b is a linear function of the entries in Y,
so that (by Theorem 4.9)
^b
is normally distributed with mean E( ^b ) ¼
(x0x)1x0(xb) ¼ b,
and
covariance
matrix
Cov( ^b ) ¼ (x0x)1x0[s2I]x(x0x)1 ¼
s2(x0x)1, i.e., ^b ~ N(b, s2(x0x)1).
Another property is that (n  k) ^S
2/s2 ~ w2
nk. To see this, ﬁrst note that
ðn  kÞ^S
2
s2
¼ «0½I  x x0x
ð
Þ1x0	«
s2
¼ s1 «0PLP0« s1;
where I  x(x0x)1x0 ¼ PLP0, and L and P are, respectively, the diagonal matrix of
characteristic roots and the matrix of characteristic vectors (stored columnwise)
associated with the symmetric and idempotent matrix (I  x(x0x)1x0) (recall the
proof of Theorem 6.12b). Examine the probability distribution of Z ¼ P0« s1.
Since « ~ N(0, s2I), then Z ~ N(s1 P00, s1 P0s2IPs1) ¼ N(0, I), because P0P ¼ I
by the orthogonality of P. Then
ðn  kÞ^S
2
s2
¼ Z0LZ ¼
X
nk
i¼1
Z2
i  w2
nk;
i.e., we have the sum of the squares of (n  k) iid standard normal random
variables which has a Chisquare distribution with n  k degrees of freedom.
Since by the preceding result ^S
2 can be characterized as a w2 random variable
that has been multiplied by the constant s2/(n  k), it follows that
^S
2 ~
464
Chapter 8
Point Estimation Methods

Gamma((n  k)/2, 2s2/(n  k)), as can be shown by deriving the MGF of ^S
2 .
Using properties of the gamma density, this in turn implies that
E ^S
2


¼
n  k
ð
Þ=2
ð
Þ 2s2= n  k
ð
Þ


¼ s2 and
var ^S
2


¼
n  k
ð
Þ=2
ð
Þ 2s2= n  k
ð
Þ

2 ¼ 2s4= n  k
ð
Þ


:
Still another property is that ^b and ^S
2 are independent random variables. This
follows from an application of Theorem 6.11 along the lines of the proof of
Theorem 6.12a, and is left as an exercise for the reader.
MVUE Property of ^b and S2
Perhaps the most important additional property that
results when Y ~ N(xb, s2I) is that
^b
^S
2
"
#
is then the MVUE for
b
s2


.
Theorem 8.11
MVUE Property of (^b,^S
2)
Under Normality
Assume the Assumptions of the Classical GLM, and assume that Y ~ N(xb,
s2I). Then (^b,^S
2) is the MVUE for (b,s2).
Proof
Note that the multivariate normal density belongs to the exponential class of
densities
exp
S
kþ1
i¼1 ci ðb; s2Þ gi ðyÞ þ dðb; s2Þ þ zðyÞ
 
!
IA ðyÞ ¼ expðcðb; s2Þ0gðyÞ þ dðb; s2Þ þ zðyÞÞIAðyÞ
where
cðb; s2Þ ¼
c1 ðb; s2Þ
...
ck ðb; s2Þ
ckþ1 ðb; s2Þ
2
6664
3
7775 ¼
b1 = s2
..
.
bk = s2
1
2 s2
2
666664
3
777775
¼
1
s2 b
1
2 s2
2
64
3
75; gðyÞ ¼
g1 ðyÞ
...
gkþ1 ðyÞ
2
64
3
75 ¼
x0y
y0y


d(b,s2) ¼
b0x0xb
ð
Þ=2s2


 ln((2ps2)n/2), z(y) ¼ 0, and A ¼ n
i¼1 (1,1) ¼ ℝn.
Then by Theorem 7.4,
x0Y
Y0Y


is a vector of minimal sufﬁcient statistics for
estimating b and s2. These sufﬁcient statistics are also complete sufﬁcient
statistics, since the range of the vector function represented by c(b,s2) contains
an open (k + 1) dimensional rectangle (recall Theorem 7.8). In particular, note
that the range is given by R(c) ¼ {(c1,. . .,ck+1):1 < ci < 1, i ¼ 1,. . .,k, ck+1 < 0}
since bi∈(1,1) for i ¼ 1,. . .,k, and s2 > 0, so that any open rectangle of the
form A ¼ {(c1,. . .,ck+1): ai < ci < bi, i ¼ 1,. . .,(k + 1)}, for ai < bi, and b(k+1) < 0
17Note that bi 2 (1,1), 8i, and s2 > 0 are the admissible parameter values for Y ~ N(xb, s2I). It may be the case that only a subset of
these values for the parameters are deemed to be relevant in a given estimation problem (e.g., a price effect may be restricted to be of
one sign, or the realistic magnitude of the effect of an explanatory variable may be bounded). Restricting the parameter space comes
under the realm of prior information models, which we do not pursue here. So long as the admissible values of b and s2 form an open
rectangle themselves, the range of c(b, s2) will contain an open rectangle.
8.2
The Least Squares Estimator
465

is a (k + 1) dimensional open rectangle subset of R(c).17 Then because
^b
¼ (x0x)1x0Y and
^S
2 ¼ (n  k)1 (Y  x ^b )0(Y  x ^b ) ¼ (Y0Y  Y0x(x0x)1x0Y)/
(n  k) are functions of the complete sufﬁcient statistics., and since we have
shown previously that ^b and ^S
2 are unbiased for b and s2 under the assumptions
of the Classical GLM, it follows from the Lehmann-Scheffe´ completeness theo-
rem (Theorem 7.22) that
^b
^S
2
"
#
is the MVUE for
b
s2


:
n
Note that the CRLB is
E @ ln fðY; QÞ
@Q
@ ln fðY; QÞ
@Q0

1
¼
s2 x0x
ð
Þ1
0
0
2s4
n
"
#
where Q ¼
b
s2


. Then because Cov( ^b) ¼ s2(x0x)1, ^b achieves the CRLB for
unbiased estimators of b providing an alternative demonstration that ^b is the
MVUE for b. However, since var(^S
2) ¼ 2s4/(n  k) > 2s4/n, ^S
2 does not achieve
the CRLB for unbiased estimators of s2, and the CRLB approach would have left
the question unanswered regarding whether
^S
2
was the MVUE for s2.
The approach using complete sufﬁcient statistics used above demonstrates
that no unbiased estimator of s2 can achieve the CRLB in this case, for indeed
^S
2 is the MVUE.
On the Assumption of Normality
The reader might wonder what considerations
would lead to a speciﬁcation of the normal family of densities for the probability
distribution of Y. Of course, if there is an underlying theoretical or physical
rationale for why Y is multivariate normally distributed, the assumption is
obviously supported. However, the underlying rationale for normality is often
not clear. In these cases, normality is sometimes rationalized by an appeal to a
central limit theorem. In particular, it is often argued that the elements in the
disturbance vector are themselves deﬁned as the summation of a large number
of random variables, i.e., ei ¼ Pm
j¼1 Vij for large m. The Vij0s may represent a
myriad of neglected explanatory factors which affect E(Yi), i ¼ 1,. . .,n, which,
because of the need for tractability, parsimony, or because of a lack of data, are
not explicitly represented in the speciﬁcation of E(Yi) ¼ xi.b. Alternatively, the
Vij0s may be intrinsic to the error term speciﬁcation if the GLM is being used to
represent a summation of individual micro relations. For example, an aggregate
short-run supply function in an aggregate economic analysis might be
represented as
X
m
j¼1
Qij ¼
X
m
j¼1
b1j þ b2jpi þ Vij


) Q
i ¼ b
1 þ b
2pi þ ei;
466
Chapter 8
Point Estimation Methods

where Q
i ¼ Pm
j¼1 Qij is the ith observation on aggregate supply, b
1 ¼ Pm
j¼1 b1j is
the intercept term of the aggregate supply relationship, b
2 ¼ Pm
j¼1 b2j is the
aggregate price effect, pi is the ith observation on supply price, and ei ¼ Pm
j¼1 Vij
is the disturbance term for the ith observation, deﬁned as the sum of the
individual disturbance terms of the m micro supply functions. Our investigation
of CLT’s in Chapter 5 have suggested that under a variety of conditions, sums of
large numbers of random variables are asymptotically normally distributed, i.e.,
« ¼ Pm
j¼1 V:j 
a N(m,S).
A prudent approach to the assumption of normality is to view arguments
such as those presented above as suggestive of a normal approximation to the
true distribution of «, but the assumption should be tested for acceptability
whenever possible. We will investigate testing the hypothesis of normality
(and indeed, hypotheses of other parametric families as well) in Chapter 10.
8.2.6
Overview of Nonlinear Least Squares Estimation
We provide an introduction to the nonlinear least squares (NLS) estimator in this
subsection. The nonlinear least squares estimator generalizes the ordinary least
squares estimator by allowing for more functional ﬂexibility in the way explan-
atory variables, x, and parameters, b, can affect values of E Yjx
½
	. In particular, the
relationship between Y and x is speciﬁed as Y ¼ g x; b
ð
Þ þ « where Y ¼ Y1; Y2;
ð
. . . ; YnÞ 0 is a n  1
ð
Þ vector of observable random variables, x is a n  k
ð
Þ matrix
representing n ﬁxed values of k explanatory variables, b is a k-dimensional ﬁxed
vector of unknown parameters, « is a
n  1
ð
Þ vector of unobservable random
residuals, and g is a
n  1
ð
Þ nonlinear vector-valued function of both x and b
representing the systematic component of the model.
The Nonlinear Least Squares Estimator
The NLS estimator minimizes the sums of
squared residuals function, or squared distance between Y and g(x,b),
d2 Y; g x; b
ð
Þ
ð
Þ ¼ Y  g x; b
ð
Þ
ð
Þ0 Y  g x; b
ð
Þ
ð
Þ
and the NLS estimator is deﬁned by
^b ¼ arg minb d2 Y; g x; b
ð
Þ
ð
Þ

	
¼ arg minbf Y  g x; b
ð
Þ
ð
Þ0 Y  g x; b
ð
Þ
ð
Þg :
Regarding motivation for the squared error metric used in deﬁning the estima-
tor, note that
E n1d2 Y; g x; b
ð
Þ
ð
Þ


¼ E n1 Y  g x; b
ð
Þ
ð
Þ0 Y  g x; b
ð
Þ
ð
Þ


¼ E n1 Y  g x; b0
ð
Þ þ g x; b0
ð
Þ  g x; b
ð
Þ
ð
Þ0 Y  g x; b0
ð
Þ
ð

þ g x; b0
ð
Þ  g x; bÞÞ
ð
Þ
¼ s2 þ 1
n g x; b0
ð
Þ  g x; b
ð
Þ
½
	0 g x; b0
ð
Þ  g x; b
ð
Þ
½
	
where b0 denotes the true value of the parameter vector. If g x; b0
ð
Þ 6¼ g x; b
ð
Þ 8b
6¼ b0, then it is clear that the minimum occurs uniquely at b ¼ b0. Thus, if the
8.2
The Least Squares Estimator
467

preceding expectation could actually be calculated, minimizing it would recover
b0 exactly. In practice the expectation is unknown and so the scaled sample
analog given by d2 y; g x; b
ð
Þ
ð
Þ is used instead. Under an appropriate law of large
numbers, n1d2 Y; g x; b
ð
Þ
ð
Þ will converge to the preceding expectation and thus
one would expect that ^b as deﬁned above should result in an estimate close to b0
for large enough n, justifying the choice of the squared error metric.
An alternative rationale based on a prediction criterion is analogous to the
rationale given in the linear model case. One need only replace ^Y ¼ x^b with ^Y
¼ g x; ^b


, and the arguments are identical.
Unlike the LS estimator in the general linear model, there is generally no
analytical or closed form solution for the NLS estimator. The solution for the
minimum sum of squares problem is most often found by using a computer and
software designed to ﬁnd minimums of nonlinear objective functions. A popular
minimization algorithm used for solving NLS problems is the so-called Gauss-
Newton method. We do not pursue computational methodology here, and the
reader is directed to the nonlinear regression and econometrics literature for
discussion of computational approaches (e.g., Mittelhammer, Judge, and Miller,
(2000) Econometric Foundations, Cambridge University Press).
Sampling Properties of the NLS Estimator
An overview of sampling properties of
the NLS estimator is provided in this subsection. The discussion begins imme-
diately with asymptotic properties simply because there is little that can be said
about the NLS estimator regarding generally applicable ﬁnite sample properties.
As we noted at the outset of Section 8.2, in the process of motivating
asymptotic properties, striking analogies to the linear model case will be
observed. These are based on ﬁrst order approximations to the conditional
expectation function of the model.
We can expand g(x, b) in a ﬁrst order Taylor series around b0 as
g x; b
ð
Þ  g x; b0
ð
Þ þ @g x; b0
ð
Þ
@b0
b  b0
ð
Þ ¼ g x; b0
ð
Þ þ x b0
ð
Þ b  b0
ð
Þ:
where
x b0
ð
Þ  @g x; b0
ð
Þ
@b0
 @g x; b
ð
Þ
@b0

b0
:
Substituting into the NLS objective function results in an approximate sum of
squares function deﬁned by
d2 y; g x; b
ð
Þ
ð
Þ  Z  x b0
ð
Þu
½
	0 Z  x b0
ð
Þu
½
	
where Z  Y  g(x, b0) ¼ « and u  b  b0.
Minimizing the ﬁrst order approximation is analogous to ﬁnding the least
squares estimator of u in the linear model z ¼ x(b0)u + v, and results in
^u ¼ x b0
ð
Þ0xðb0Þ

	1x b0
ð
Þ0z:
468
Chapter 8
Point Estimation Methods

It follows that the NLS estimator is characterized by the approximation
^b  b0 þ x b0
ð
Þ0x b0
ð
Þ

	1x b0
ð
Þ0«:
Therefore, to the ﬁrst order of approximation, the NLS estimator is unbiased and
its covariance matrix is s2 x b0
ð
Þ0x b0
ð
Þ

	1 , which follows from results on the
means and covariance matrices of linear combinations of random variables.
Assuming regularity conditions on x(b0) and « analogous to those that were
applied to x and « in the linear model case to establish asymptotic properties of
the LS estimator, it can be shown that
n1x b0
ð
Þ0x b0
ð
Þ ! J; n1x b0
ð
Þ0«!
p 0; and n1=2x b0
ð
Þ0« !
d N 0; s2J


;
where X is a ﬁnite, symmetric, positive deﬁnite matrix. It then follows to
the ﬁrst order of approximation, and analogous to the linear LS estimator
application, that
n1=2 ^b  b0


!
d N 0; s2J1


and ^b 
a N b0; s2 x b0
ð
Þ0x b0
ð
Þ

	1


:
Moreover, it follows from the limiting distribution result above that n1=2 ^b  b0


is Opð1Þ, so that
^b  b0


is opð1Þ, ^b  b0 !
p 0, and thus ^b is a consistent estimator
of b0.
The preceding approximations are accurate within small neighborhoods of
the true parameter vector b0, derived from the accuracy of the ﬁrst order Taylor
series in such neighborhoods. It can be shown that ^b is almost certain to be
in a neighborhood of b0 as n ! 1 under general regularity conditions. In
applications, the asymptotic normality and asymptotic covariance results pro-
vide the basis for hypothesis testing and conﬁdence region estimation, as we will
see in later chapters. Operational versions of the covariance matrix are obtained
by replacing b0 and s2 by consistent NLS estimator outcomes, as will be
discussed further ahead.
Estimating s2 and Cov ^b

 
Estimates of random residual vector outcomes are
provided by the estimator
^« ¼ Y  gðx; ^bÞ ¼ « þ g x; b0
ð
Þ  g x; ^b


h
i
where b0 is again being used to denote the true value of the parameter vector.
Given the consistency of the NLS estimator, ^b !
p b0, it follows under mild regu-
larity conditions (continuity of the g function) that ^«  « ¼ g x; b0
ð
Þ  g x; ^b


h
i
!
p
0 element-wise. Therefore, as n increases, the outcomes of ^ei will eventually
become indistinguishable from the outcomes ofei with probability converging to
1, and thus the distributions of ^ei and ei coincide as well (recall that convergence
in probability implies convergence in distribution).
We now show that ^« is asymptotically linear in «, which emulates the
relationship between the estimated and actual residuals in the linear model
8.2
The Least Squares Estimator
469

case. Expand^ei in a Taylor series around the point b0 based on the deﬁnition for^ei
above, to obtain
^ei ¼ yi  g xi
; b0
ð
Þ  @g xi
; b
ð
Þ
@b
b

^b  b0


where b ¼ l^b þ 1  l
ð
Þb0 for some l 2 [0,1] by the Mean Value Theorem. Using
the relationship ^b ¼ b0 þ x b0
ð
Þ0x b0
ð
Þ

	1x b0
ð
Þ0« þ op n1=2


from before, and
substituting ^ei for yi  g xi
; b0
ð
Þ obtains
^ei ¼ ei  xi b
ð
Þ x b0
ð
Þ0x b0
ð
Þ

	1x b0
ð
Þ0« þ op n1=2


:
Given that g xi
; b
ð
Þ is continuously differentiable, so that xi b
ð Þ is a continuous
function of b, and given that ^b !
p b0 so that necessarily b !
p b0, it follows that
xi b
ð
Þ !
p xi b0
ð
Þ. Substituting xi b0
ð
Þ for xi b
ð
Þ, and recognizing that an analogous
argument holds 8i, ﬁnally obtains
^« ¼ In  x b0
ð
Þ x b0
ð
Þ0x b0
ð
Þ

	1x b0
ð
Þ0
h
i
« þ op n1=2


¼ m b0
ð
Þ« þ op n1=2


where now op(n1/2) denotes an (n  1) vector whose elements are each op(n1/2)
and m b0
ð
Þ denotes the outer-bracketed matrix in the expression for ^« above.
Note that m b0
ð
Þ is an (n  n) idempotent matrix of rank (n  k). Comparing
this result to the associated estimator for residuals in the linear model, ^« ¼
In  x x0x
ð
Þ1x0
h
i
« , establishes the asymptotically valid analogy between the
linear and nonlinear cases. It can be shown that ^« has the smallest limiting
covariance matrix of all estimators that are asymptotically linear and consistent
for «.
Turning our attention to the issue of estimating the value of s2, examine the
inner product of the estimator ^« and use the preceding representation of it to
conclude that ^«0^« ¼ «0m b0
ð
Þ« þ 2«0m b0
ð
Þop n1=2


þ opð1Þ. Regarding the order
of magnitude of the trailing term above, note that op n1=2

0op n1=2


is the sum
of n terms of order op(n1), which then is of order of magnitude op(1) ¼ n op(n1).
Then deﬁne the estimator for s2 by
^S
2 ¼
Y  g x; ^b



0
Y  g x; ^b




n  k
:
The estimator is a consistent estimator for s2, the proof of which follows closely
the proof of the consistency of ^S
2in the linear model case (Theorems 8.5 and 8.6)
upon noting that
^S
2 ¼ «0m b0
ð
Þ«
n  k
þ op n1=2


470
Chapter 8
Point Estimation Methods

and ignoring the asymptotically irrelevant op n1=2


terms. Note that dividing
^«0^« by (n  k) in the deﬁnition of the estimator produces an estimator of s2that
has the advantage of being unbiased up to terms of order op(n1/2), in another
analogy to the linear model case (see Theorem 8.3). Note that had we divided ^«0^«
by n instead of (n  k), we would still have arrived at a consistent estimator ofs2.
It can also be shown that S2 is asymptotically normally distributed under
additional noise component moment and convergence assumptions.
Having deﬁned an estimator for information on the value of s2, we can now
address the issue of generating an operational version of the asymptotic covari-
ance matrix of the NLS estimator,
cov ^b

 
¼ s2 @g x; b0
ð
Þ
@b
@g x; b0
ð
Þ
@b0

1
¼ s2 x b0
ð
Þ0x b0
ð
Þ

1
deﬁned above. In particular, one replaces s2and b0 with consistent estimators,
yielding
c^ov ^b

 
¼ ^S
2
@g x; ^b


@b
@g x; ^b


@b0
0
@
1
A
1
¼ ^S
2 x ^b

 0
x ^b

 

1
Under the assumptions leading to the consistency, asymptotic normality, and
asymptotic linearity of the NLS estimator, it can be shown that n c^ov ^b

 


consistently estimates the covariance matrix of the limiting distribution of
n1=2 ^b  b0


.
Summary Remarks on NLS Estimation
The preceding overview of the NLS esti-
mator provides a basic introduction to nonlinear estimation of unknown
parameters in nonlinear regression models. It was shown that asymptotically,
at least to the ﬁrst order of approximation, there are striking analogies to the
least squares estimator applied in the linear model context in so far as statistical
properties are concerned. Indeed, most of the derivations of properties of the
NLS estimator can follow by analogy to the linear model derivations upon
replacing x with x b0
ð
Þ. Other results that can be demonstrated this way include
asymptotic efﬁciency of the NLS estimator and an associated asymptotically-
valid MVUE property.
We will see ahead that the analogy can be continued regarding asymptotically
valid hypotheses testing and conﬁdence interval or region generation. Of course,
as we noted at the outset, calculation of the actual NLS estimates is another
matter, and must often be done numerically on a computer. For additional
reading on both the theory and application of nonlinear estimation methodology,
the reader can begin by consulting A. R. Gallant’s Nonlinear Statistical Models,
John wiley and Sons, and R. Mittelhammer, G. Judge, and D. Miller’s Economet-
ric Foundations, Cambridge University Press.
8.2
The Least Squares Estimator
471

8.3
The Method of Maximum Likelihood
The method of Maximum Likelihood (ML) can be used to estimate the unknown
parameters, or functions of unknown parameters, corresponding to the joint
density function of a random sample. The procedure leads to an estimate of Q
or q(Q) by maximizing the so-called likelihood function of the parameters, given
the observed outcome of the random sample. We will focus initially on the
problem of estimating Q itself. It will be seen later that maximum likelihood
estimation of q(Q) is easily implemented through the so-called invariance prin-
ciple of ML once the problem of estimating Q has been solved.
The likelihood function is identical in functional form to the joint density
function of the random sample. However, there is an important interpretational
difference between the two functions. The joint density function is interpreted
as a function of the values of the random variable outcomes (x1,. . .,xn), given
values of the parameters (Y1,. . .,Yk). The interpretation of the likelihood func-
tion is just the reverse—it is a function of the values of the parameters
(Y1,. . .,Yk), given values of the random variable outcomes (x1,. . .,xn). Thus,
L(Q;x)  f(x;Q) deﬁnes the functional form of the likelihood function L(Q;x),
but now Q precedes the semicolon and x follows the semicolon to denote that
the likelihood function is a function of Q, for given values of x.
The maximum likelihood estimate of Q is the solution of the maxi-
mization problem maxQ2O LðQ; xÞ
f
g where O is the appropriate parameter
space. The maximum likelihood estimate is thus deﬁned as ^u ¼ ^QðxÞ ¼
arg maxQ2O L Q; x
ð
Þ
f
g .18 The maximum likelihood estimator (MLE) is the
random variable or vector that generates the estimates above, and is deﬁned
as ^Q ¼ ^QðXÞ ¼ arg maxQ2O L Q; X
ð
Þ
f
g. The maximum likelihood procedure can
be interpreted as choosing, from among all feasible candidates, the value of
the parameter vector Q that identiﬁes the joint density function f(x;Q)
assigning the highest probability (discrete case) or highest density weighting
(continuous case) to the random sample outcome, x, actually observed. Put
another way, the maximum likelihood procedure chooses a parameter vector
value so as to identify a particular member of a parametric family of
densities, L(Q;x)  f(x;Q), Q∈O, that assigns the highest “likelihood” to
generating the random sample outcome x actually observed. Of course,
whether the estimator of Q implied by this procedure is a “good one” depends
on the statistical properties of the maximum likelihood estimator ^Q.
8.3.1
MLE Implementation
In order to implement the ML procedure, the functional form of f(x;Q) and hence
L(Q;x) must be speciﬁed along with the feasible choices of Q, represented by the
parameter space, O. This is generally accomplished in practice by specifying a
18Recall that arg maxw{f(w)}denotes the argument value of f(w) that maximizes f(w), where argument value means the value of w. Also,
arg maxw2O{f(w)} denotes the value of w 2 O that maximizes f(w).
472
Chapter 8
Point Estimation Methods

parametric family of density functions to provide a representation of the proba-
bility model for the random sample under investigation. Once the parametric
family is identiﬁed, the estimation problem focuses on the maximization of the
likelihood function. In many cases, the likelihood function will be differentiable
with respect to the parameter vector and will possess a maximum which will be
interior to the parameter. In these cases, the classical calculus-based approach to
maximization, in which ﬁrst order conditions are used to solve for the
likelihood-maximizing value of the Q vector, can be used to solve the MLE
problem. That is, the ML estimate, ^u, can be found as the solution to the vector
equation
@LðQ; xÞ
@Q
k1
ð
Þ
¼
@LðQ; xÞ
@ Q1
...
0@LðQ; xÞ
@ Qk
2
66664
3
77775
¼
0
k  1
ð
Þ
:
The solution will be a function of the random sample outcome x, as ^u ¼ ^Q x
ð Þ
¼ argQ2O @L Q; x
ð
Þ=@Q ¼ 0
f
g.19
Note that it may not be possible to explicitly solve the ﬁrst order conditions
for ^u in terms of a function of x. If not, numerical methods are used to ﬁnd the
value of ^u that satisﬁed the ﬁrst order conditions. More generally, even if the
classical maximization approach is not appropriate (e.g., there is no interior
maximum, or L(Q;x) is not differentiable with respect to Q), a value of ^u that
solves maxQ2O L Q; x
ð
Þ
f
g is a ML estimate of Q, no matter how it is derived.
The estimator function ^u ¼ ^Y(x) will either be explicitly derivable, or else it
will be an implicit function implied by the functional dependence of ^u on x, and
in either case ^Y ¼ ^Y(X) is referred to as a maximum likelihood estimator of Q.20
The following examples illustrate the derivation of MLEs. Note that in some
problem situations the calculations are considerably simpliﬁed by maximizing
ln(L(Q;x)) as opposed to L(Q;x). Since the logarithmic transformation is strictly
monotonically increasing, ^u maximizes L(Q;x) iff it also maximizes ln(L(Q;x)), i.e.,
L ^u; x


¼ maxQ2O L Q; x
ð
Þ
f
g , ln L ^u; x




¼ maxQ2O ln L Q; x
ð
Þ
ð
Þ
f
g
and
^u ¼ arg maxQ2O L Q; x
ð
Þ
f
g ¼ arg maxQ2O ln L Q; x
ð
Þ
ð
Þ
f
g:
19Recall that argQ2O g Q
ð
Þ ¼ c
f
g represents the value of QϵO that satisﬁes or solves g(Q) ¼ c.
20We are suppressing the fact that a maximum of L(Q;x) may not be attainable. For example, if the parameter space is an open interval
and if the likelihood function is strictly monotonically increasing, then no maximum can be stated. If a maximum of L(Q;x) for Q ϵ O
does not exist, then the MLE of Q does not exist.
8.3
The Method of Maximum Likelihood
473

Thus the objective function of the ML estimation problem can be chosen to be
L(Q;x) or ln(L(Q;x)), whichever is more convenient.21 In applying the logarithmic
transformation, we deﬁne ln(0) 1 to accommodate points where the likeli-
hood function is zero-valued.
Example 8.4
MLE for Exponential
Distribution
Let X ¼ (X1,. . .,Xn) be a random sample from an exponential population distri-
bution representing the operating time until a work stoppage occurs on an
assembly line, so that Xi ~ y1 exp (xi/y)I(0,1)(xi) 8i. We seek the MLE for y,
the mean operating time until a work stoppage.
The functional form for the joint density function of the random sample, and
hence the functional form of the likelihood function, is given by
L y; x1; . . . ; xn
ð
Þ  f x1; . . . ; xn; y
ð
Þ ¼ yn exp 
X
n
i¼1
xi=y
 
! Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ;
where y ∈O ¼ (0,1). Then,
ln L y; x1; . . . ; xn
ð
Þ
ð
Þ ¼ n ln y
ð Þ 
Pn
i¼1 xi
y
þ ln
Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ
 
!
:
The ﬁrst order condition for maximizing ln(L(y;x)) with respect to y is given by
d ln L
dy
¼  n
y þ
Pn
i¼1 xi
y2
¼ 0:
Thus the solution ^y ¼ ^YðxÞ ¼ Pn
i¼1 xi=n ¼ x is the ML estimate of y, and ^Y ¼
^Y X
ð Þ ¼ Pn
i¼1 Xi=n ¼ Xn is the MLE of Y. (The second order conditions for a
maximum are met.)
□
Example 8.5
MLE for Normal
Distribution
Let X ¼ (X1,. . .,Xn) be a random sample from a normal population distribution
representing the actual ﬁll volumes of 1 liter bottles of liquid laundry detergent
so that
Xi 
1
2p
ð
Þ1=2s
exp  1
2s2 xi  m
ð
Þ2


; 8i:
21In the case where the classical ﬁrst order conditions are applicable, note that if L(Q,x) > 0 (which will necessarily be true at the
maximum value), then
@ ln LðQ; xÞ
ð
Þ
@Q
¼
1
LðQ; xÞ
@LðQ; xÞ
@Q
;
and thus any Q for which ∂L(Q;x)/∂Q ¼ 0 also satisﬁes ∂ln(L(Q;x))/∂Q ¼ 0. Regarding second order conditions, note that if Q satisﬁes
the ﬁrst order conditions, then
@2 ln LðQ; xÞ
ð
Þ
@Q@Q0
¼
1
LðQ; xÞ
@2 LðQ; xÞ
@Q@Q0
 @LðQ; xÞ
@Q
@LðQ; xÞ
@Q0
¼
1
LðQ; xÞ
@2 LðQ; xÞ
@Q@Q0
since ∂L(Q;x)/∂Q ¼ 0. Then since L(Q;x) > 0 at the maximum, @2 ln L Q; x
ð
Þ
ð
Þ=@Q@Q0 is negative deﬁnite iff @2 L Q; x
ð
Þ
ð
Þ=@Q@Q0 is
negative deﬁnite.
474
Chapter 8
Point Estimation Methods

We seek the MLE for the mean and variance of ﬁll volumes.
The functional form for the joint density of the random sample, and hence the
functional form of the likelihood function, is given by
L m; s2; x


 f x; m; s2


¼
1
2ps2
ð
Þn=2 exp  1
2s2
X
n
i¼1
xi  m
ð
Þ2
"
#
;
where m  0 and s2 > 0. Then
ln Lðm; s2; xÞ ¼  n
2 ln 2p
ð
Þ  n
2 ln s2


 1
2s2
X
n
i¼1
xi  m
ð
Þ2:
The ﬁrst order conditions for the maximum of ln(L(m,s2;x))are
@ lnðLÞ
@m
¼ s2 X
n
i¼1
xi  m
ð
Þ ¼ 0
@ lnðLÞ
@s2
¼ n
2s2 þ 1
2s4
X
n
i¼1
xi  m
ð
Þ2 ¼ 0:
The solution ^mðxÞ ¼ Pn
i¼1 xi=n and ^s2 x
ð Þ ¼ Pn
i¼1 xi  x
ð
Þ2=n deﬁnes the ML esti-
mate of m and s2, and the MLE is given by ^m X
ð Þ and ^s2 X
ð Þ. (The second order
conditions for a maximum are met.)
□
Example 8.6
Maximum Likelihood
Estimation in the GLM
Let Y ¼ xb + « with Y ~ N(xb,s2I) represent the relationship between a random
sample of family expenditures on consumer durables, Y, and the respective
levels of disposable income and other sociodemographic factors for the families,
x. We seek the MLE for the marginal effects of sociodemographic variables on
consumption, i.e., for @E Yi
ð
Þ=@x0i: ¼ b, and for the variance of consumption, s2.
The likelihood function in this case is
L b; s2y


¼ N xb; s2I


¼
1
2ps2
ð
Þn=2 exp  1
2s2 y  xb
ð
Þ0 y  xb
ð
Þ


;
where b 2 ℝk and s2 > 0.22 Then
ln Lðb; s2; yÞ


¼  n=2
ð
Þ ln 2p
ð
Þ  n=2
ð
Þ ln s2


 1=2s2


y  xb
ð
Þ0 y  xb
ð
Þ:
The ﬁrst order conditions for the maximum of ln(L(b,s2;y)) are given by (recall
Lemma 8.1)
@ ln Lðb; s2; yÞ


@b
¼ @  1
2s2 y0y  2b0x0y þ b0x0xb
ð
Þ

	
@b
¼  1
2s2 2x0y þ 2x0xb
ð
Þ ¼ 0;
@ ln Lðb; s2; yÞ


@s2
¼  n
2s2 þ 1
2s4 y  xb
ð
Þ0 y  xb
ð
Þ ¼ 0
22Economic theory may suggest constraints on the signs of some of the entries in b (e.g., the effect of income on durables consumption
will be positive), in which case b ∈Ob  ℝk may be more appropriate.
8.3
The Method of Maximum Likelihood
475

The solution to the ﬁrst order conditions implies that the ML estimate is
^b y
ð Þ ¼ x0x
ð
Þ1x0y and ^s2 y
ð Þ ¼
y  x^b y
ð Þ

0
y  x^b y
ð Þ


n
;
with the MLE given by ^b (Y) and ^s 2(Y). (The second order conditions for a
maximum are met.)
□
The following examples illustrate cases where the standard calculus
approach cannot be utilized.
Example 8.7
MLE for Uniform
Distribution
Let
X ¼ (X1,. . .,Xn)
be
a
random
sample
from
a
uniform
population
distribution representing measurements of the hardness of steel, based on the
Rockwell scale, produced by a certain foreign manufacturer, so that
Xi 
1= b  a
ð
Þ
ð
ÞI a;b
½
	 xi
ð
Þ8i. We seek an MLE for the lower and upper bounds to the
hardness measurements, a and b, respectively.
The likelihood function is given by
L a; b; x
ð
Þ  f x; a; b
ð
Þ ¼ b  a
ð
Þn Y
n
i¼1
I a;b
½
	 xi
ð
Þ:
where a < b. It is clear that for L(a,b;x) to be maximized, a and b must be chosen so
as to make (b  a) as small as possible while still maintaining Qn
i¼1 I a;b
½
	 xi
ð
Þ ¼ 1.
The smallest choice for b is given by max(x1,. . .,xn), while the largest choice for a is
given
by
min(x1,. . .,xn),
yielding
the
smallest
(b  a) ¼ max(x1,. . .,xn) 
min(x1,. . .,xn). Thus, the MLE estimates are given by outcomes of the smallest
and largest order statistics, as ^a x
ð Þ ¼ min x1; . . . ; xn
ð
Þ and ^b x
ð Þ ¼ max x1; . . . ; xn
ð
Þ;
and the MLEs are then ^a X
ð Þ and ^b X
ð Þ.
□
Example 8.8
MLE for
Hypergeometric
Distribution
Let X ~ Hypergeometric (x;M,n,k) represent the number of defective parts found
in a random sample without replacement of n parts taken from a shipment of M
parts, where M and n are known. We seek an MLE of the number of defective
parts, k, in the shipment of M parts.
The likelihood function is given by
Lðk; xÞ  fðx; kÞ ¼
k
x


M  k
n  x


M
n


for x ¼ 0; 1; . . . ; k;
where k ∈{0, 1, 2,. . .,M}. Finding the solution for k that maximizes L(k;x) is
essentially an integer programming problem. Speciﬁcally, note that
L k; x
ð
Þ
L k  1; x
ð
Þ ¼
k!
x! k  x
ð
Þ!
k  1
ð
Þ!
x! k  x  1
ð
Þ!
M  k
ð
Þ!
n  x
ð
Þ! M  k  n  x
ð
Þ
ð
Þ!
M  k þ 1
ð
Þ!
n  x
ð
Þ! M  k  n  x
ð
Þ þ 1
ð
Þ!
¼ k M  k  n  x
ð
Þ þ 1
ð
Þ
k  x
ð
Þ M  k þ 1
ð
Þ
:
476
Chapter 8
Point Estimation Methods

Then
L(k;x)/L(k  1;x)  1
iff
k(M  k  (n  x) + 1)  (k  x)(M  k + 1),
which after algebraic simpliﬁcation is equivalent to k  n1x(M + 1). The impli-
cation of the preceding result is that the likelihood function increases as the
integer value of k increases so long as k  n1x(M + 1). Therefore, the ML
estimate of k equals the largest integer not exceeding n1x(M + 1), which we
can represent as ^kðxÞ ¼ trunc n1x M þ 1
ð
Þ


, where recall that trunc(w) is the
truncation function that truncates the decimal part of w, e.g., trunc(2.76) ¼ 2.
The MLE of k would then be ^kðXÞ ¼ trunc n1X M þ 1
ð
Þ


.
□
8.3.2
MLE Properties: Finite Sample
As we stated at the beginning of this section, whether estimates produced by the
ML procedure are “good ones” depends on the statistical properties that an MLE
possesses. It turns out that there are a number of reasons why we might expect
that the ML procedure would lead to good estimates of Q. First of all, if an
unbiased estimator of Q exists that achieves the CRLB, the MLE will be this
estimator if the MLE is deﬁned by solving ﬁrst order conditions for maximizing
the likelihood function.
Theorem 8.12
MLE Attainment of the
CRLB
If an unbiased estimator, T ¼ t(X), of Q exists whose covariance matrix equals
the CRLB, and if the MLE is deﬁned by solving ﬁrst order conditions for
maximizing the likelihood function, then the MLE is equal to T ¼ t(X) with
probability 1.
Proof
If there exists an unbiased estimator, t(X), whose covariance matrix equals the
CRLB, then regardless of the value of Q ∈O, the CRLB attainment theorem
(Theorem 7.20) implies that the estimator has outcomes deﬁned by
tðxÞ ¼ Q þ E @ ln LðQ; XÞ
ð
Þ
@Q
@ ln LðQ; XÞ
ð
Þ
@Q0



1 @ ln LðQ; xÞ
ð
Þ
@Q
with probability 1, where we have expressed the result of the CRLB attainment
theorem using likelihood function notation. Substituting the ML estimate, ^u
∈O, for Q in the preceding equality implies that t(x) ¼ ^u, since ^u would satisfy
the ﬁrst order conditions ∂ln(L(^u;x)/∂Q) ¼ 0. Thus outcomes of the MLE and t(X)
coincide with probability 1.
n
Therefore, under the conditions of Theorem 8.12, the MLE will also be the
MVUE for Q.
We can also show that if the MLE is uniquely deﬁned, then the MLE can be
equivalently represented as a function of any sufﬁcient statistics for f(x;Q), and
in particular, a function of complete sufﬁcient statistics when complete sufﬁ-
cient statistics exist.
8.3
The Method of Maximum Likelihood
477

Theorem 8.13
Unique MLEs
Are Functions
of Any Sufﬁcient
Statistics for f(x;Q)
Assume that the MLE ^Q of Q is uniquely deﬁned in terms of X. If S ¼ (S1,. . .,Sr)0
is any vector of sufﬁcient statistics for f(x;Q)  L(Q;x), then there exists a
function of S, say t(S), such that ^u ¼ t(s).
Proof
The Neyman Factorization theorem states that L(Q;x)  f(x;Q) ¼ g(s1,. . .,sr; Q) h(x)
where (s1,. . .,sr) are sufﬁcient statistics, which can be chosen as complete sufﬁ-
cient statistics if they exist. Now because L(Q;x)  0, g and h can always be
deﬁned as nonnegative-valued functions, in which case for a given value of x,
L(Q;x)
/
g(s1,. . .,sr;Q),
where
“/”
means
“proportional
to”
and
the
proportionality constant is h(x). It follows that for a given value of x, if the
MLE is unique, then
^u ¼ arg max L Q; x
ð
Þ
f
g
Q2O
¼ arg max
Q2O
g s1; . . . ; sr; Q
ð
Þ
f
g:
Thus ^Q maximizes L(Q;x) iff ^u maximizes g(s1,. . .,sr;Q). But the latter maximiza-
tion problem implies that the unique maximizing choice of Q is then a function
of the values (s1,. . .,sr), so that ^Q ¼ t(s1,. . .,sr).23
n
If the sufﬁcient statistics (S1,. . .,Sr) used in the Neyman Factorization theo-
rem are complete, then the unique MLE ^Q ¼ t(S1,. . .,Sr) is a function of the
complete sufﬁcient statistics by Theorem 8.13. It follows from the Lehmann-
Scheffe’ completeness theorem that if
^Q is also unbiased (or if
^Q can be
transformed so as to be unbiased) then the MLE (or the bias-adjusted MLE) is
the MVUE for Q. We formalize this observation in the following theorem:
Theorem 8.14
MVUE Property of
Unique Unbiased or
Bias-Adjusted MLEs
Assume that the MLE ^Qof Q is uniquely deﬁned in terms of X, and that a vector
of complete sufﬁcient statistics, S, exists for f(x;Q)  L(Q;x). If ^Q or h( ^Q) is an
unbiased estimator of Q, then ^Q or h( ^Q) is the MVUE of Q.
Proof
From Theorem 8.13 it follows that the MLE is a function of the complete
sufﬁcient statistics as ^Q ¼ t(S). It follows from the Lehmann-Scheffe’ Complete-
ness Theorem (Theorem 7.22) that if ^Q is unbiased, then ^Q is the MVUE of Q.
Alternatively, if ^Q is biased, but the function h( ^Q) of the MLE is an unbiased
estimator of Q, then because h( ^Q ) ¼ h(t(S)) is a (composite) function of the
complete sufﬁcient statistics, h( ^Q) is the MVUE of Q by the Lehmann-Scheffe’
Completeness Theorem.
n
23In the event that a MLE is not unique, then the set of MLE’s is a function of any set of sufﬁcient statistics. However, a particular MLE
within the set of MLE’s need not necessarily be a function of (s1,. . .,sr), although it is always possible to choose an MLE that is a
function of (s1,. . .,sr). See Moore, D.S., (1971), “Maximum Likelihood and Sufﬁcient Statistics”, American Mathematical Monthly,
January, pp. 50–52.
478
Chapter 8
Point Estimation Methods

In Example 8.4, Pn
i¼1 Xi is a complete sufﬁcient statistic for f(x;Q)  L(Q;x),
and the MLE is unique and unbiased, and thus the MLE is MVUE. In Example
8.5, Pn
i¼1 X2
i and Pn
i¼1 Xi are complete sufﬁcient statistics, and the MLEs ^mand ^s2
are unique, so that they are functions of the complete sufﬁcient statistics.
However, ^s2 is not unbiased. The bias can be removed by multiplying ^s2 by
n/(n  1). Then the bias-adjusted MLE of m and s2, namely (^m, n^s2/(n  1)), is the
MVUE by the Lehmann-Scheffe’ completeness theorem. The case of Example
8.6 was discussed in the previous section on the least squares estimator, where
ð^b; n^s2/(n  k)) is also the bias-adjusted MLE and was found to be the MVUE of
(b,s2). In Example 8.7, it can be shown that the MLE ^b(X) ¼ max(X1,. . .,Xn) and
^a X
ð Þ ¼ min X1; . . . ; Xn
ð
Þ is unique and is itself a complete sufﬁcient statistic and
that ^a X
ð Þ ¼
n þ 1
ð
Þ=n
ð
Þ^a X
ð Þ; and ^b X
ð Þ ¼
n þ 1
ð
Þ=n
ð
Þ^b X
ð Þare unbiased estimators
of a and b, respectively. Thus, the bias-adjusted MLE
^a X
ð Þ; ^b X
ð Þ


is the MVUE
for (a,b). (See Bickel and Doksum, (1969) op. cit., pp. 125–126, and Johnson and
Kotz, Discrete Distributions, John Wiley, pp. 146–148). Finally, in Example 8.8,
it can be shown that X is a complete sufﬁcient statistic, and the unique MLE can
be transformed as t(X) ¼ (M/n)X to deﬁne an unbiased estimator of k. Then the
bias-adjusted MLE, t(X) ¼ (M/n)X, is the MVUE for k (see Bickel and Doksum,
(1969) op. cit., pp. 122–123).
As we have seen above, the maximum likelihood procedure is a rather
straight-forward approach to deﬁning estimators of unknown parameters, and
in many cases the estimator, or a simple transformation of it, will be unbiased
and the MVUE. And if not an MVUE itself, a unique MLE will always be a
function of complete sufﬁcient statistics whenever the latter exist, so that the
unique MLE is a reasonable starting point in the search for the MVUE. On the
other hand, the MLE need not be unbiased nor be the MVUE, and there may be
no apparent transformation of the MLE that achieves the MVUE property. It is
useful to examine asymptotic properties of MLE’s since, even if an MLE does not
possess the ﬁnite sample properties that one might desire, MLE’s possess desir-
able large sample properties under general conditions, and these latter properties
can still serve to rationalize the use of an MLE for estimating the parameters of a
particular statistical model.
8.3.3
MLE Properties: Large Sample
There are two basic approaches that one can follow in establishing asymptotic
properties of an MLE. First of all, if a MLE can be explicitly solved for, so that one
can analyze an explicit real-valued function, ^Q ¼ ^Q(X), of the random sample X,
then it might be possible to apply laws of large numbers and/or central limit
theorems directly to ^Q(X) to investigate the asymptotic properties of the MLE.
Note the following example:
8.3
The Method of Maximum Likelihood
479

Example 8.9
Asymptotics of
Exponential MLE via
Direct Evaluation
Recall Example 8.4, where it was found that the MLE of
y , when
random sampling from an exponential population distribution, is given by ^Y ¼
n1 Pn
i¼1 Xi ¼ Xn. Through direct evaluation of the MLE function deﬁnition, we
can establish that ^Y is a consistent, asymptotically normal, and asymptotically
efﬁcient estimator of y. In fact, the procedure for establishing these asymptotic
properties has already been carried out in Example 7.23, where it was
demonstrated that ^Y ¼ Xn was a consistent and asymptotically efﬁcient estima-
tor of y having an asymptotically normal distribution given by ^Y 
a N y; y2=n


.□
At times, the function deﬁning a MLE cannot be deﬁned explicitly, even
though the MLE estimates can be calculated, or else the explicit deﬁnition of an
MLE may be so complicated as to make it unclear how laws of large numbers or
central limit theorems could be applied. For these cases, regularity conditions on
the likelihood functions have been presented in the literature that ensure the
MLE is consistent, asymptotically normal, and asymptotically efﬁcient. As an
illustration of a situation in which direct evaluation of the asymptotic properties
(and ﬁnite sample properties) of an MLE is not possible, consider the following
example in which maximum likelihood estimation of the parameters of a
gamma density is being pursued.
Example 8.10
MLE for Parameters of a
Gamma Distribution
Let (X1,. . .,Xn) be a random sample from a gamma population distribution
representing the time between breakdowns of a certain type of refrigeration
equipment used in the frozen foods section of a major supermarket chain, so that
Xi 
1
baGðaÞ xa1
i
exi=bI 0;1
ð
Þ xi
ð
Þ 8i:
The likelihood function is given by
Lða; b; xÞ ¼
1
bna GðaÞ
½
	n
Y
n
i¼1
xa1
i
exp 
X
n
i¼1
xi=b
 
! Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ
where a > 0 and b > 0. The log-likelihood is given by
ln L a; b; x
ð
Þ
ð
Þ ¼ na ln b
ð Þ
ð
Þ  n ln G a
ð Þ
ð
Þ
ð
Þ
þ a  1
ð
Þ
X
n
i¼1
ln xi
ð
Þ 
X
n
i¼1
xi=b þ ln
Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ
 
!
:
The ﬁrst-order conditions characterizing the maximum of the log-likelihood
function are given by
@ lnðLÞ
@a
¼ n ln b
ð Þ
ð
Þ 
n
GðaÞ
dGðaÞ
da
þ
X
n
i¼1
ln xi
ð
Þ ¼ 0
@ lnðLÞ
@b
¼ n a=b
ð
Þ þ
X
n
i¼1
xi
 
!
=b2 ¼ 0:
480
Chapter 8
Point Estimation Methods

Note that the second condition implies that ab ¼ Xn; or b ¼ Xn=a. Substituting
this result for b in the ﬁrst condition implies
ln a
ð Þ  dGðaÞ
da
=GðaÞ ¼ ln xn
ð
Þ 
X
n
i¼1
ln xi
ð
Þ
 
!.
n;
and there is no explicit solution for a in terms of (x1,. . .,xn), although a is an
implicit function of (x1,. . .,xn). A unique value of a satisfying the above equality
can be solved for numerically on a computer, which can then be used to solve
for b using the equation b ¼ Xn=a.24 Thus, the ML estimates for (a,b) can be
calculated. However, since an explicit functional form for the MLE is not
identiﬁable, an analysis of the estimator’s ﬁnite sample and asymptotic
properties is quite difﬁcult.
Regarding ﬁnite sample properties of the MLE, the reader can verify by an
appeal to Theorem 7.6 that Pn
i¼1 Xi; Pn
i¼1 ln Xi
ð
Þ


is a set of complete sufﬁcient
statistics for this problem, so that the MLE
^a X
ð Þ; ^b X
ð Þ


is a (implicit) function
of complete sufﬁcient statistics. However, the MLE is biased, and no MVUE
estimator for (a,b) has been presented in the literature. Bowman and Shenton25
have obtained expressions for low order moments of
^a X
ð Þ; ^b X
ð Þ


that are
accurate to terms having order of magnitude n6. For example, when a  1 and
n  4, they found that
E ^aðXÞ
ð
Þ  a þ 3a  2
3 þ 1
9 a1 þ 13
405 a2


=ðn  3Þ;
and they suggest, as an approximately unbiased estimator of a, the following
function of the MLE for a:
^aðXÞ ¼
n  3
ð
Þ^a X
ð Þ þ 2
3


n
Since ^a X
ð Þ is a function of complete sufﬁcient statistics (because the MLE ^a X
ð Þ
is), and since ^a X
ð Þ is approximately unbiased, ^a X
ð Þ can be interpreted as being
approximately MVUE for a. For further details on ﬁnite sample properties of the
MLE
^a X
ð Þ; ^b X
ð Þ


, see Bowman and Shenton. An analysis of the asymptotic
properties of
^a X
ð Þ; ^b X
ð Þ


will be developed in a subsequent example.
□
24Alternatively, the solution for a can be determined by consulting tables generated by Chapman which were constructed speciﬁcally
for this purpose (Chapman, D.G. “Estimating Parameters of a Truncated Gamma Distribution,” Ann. Math. Stat., 27, 1956,
pp. 498–506).
25Bowman, K.O. and L.R. Shenton. Properties of Estimators for the Gamma Distribution, Report CTC–1, Union Carbide Corp., Oak
Ridge, Tennessee.
8.3
The Method of Maximum Likelihood
481

A varied collection of regularity conditions on likelihood functions have
been presented in the literature that represent sufﬁcient conditions for MLE’s
to possess desirable asymptotic properties. Most of these conditions apply spe-
ciﬁcally to the case of random sampling from a population distribution, so that
the random sample (X1,. . .,Xn) must be a collection of iid random variables. For a
survey of alternative types of regularity conditions, the reader can refer to the
article by Norden.26 We concentrate on regularity conditions that do not cover
all cases, but that are relatively simple to comprehend and apply and that focus
attention on key assumptions that lead to good asymptotic properties of MLEs.
The conditions we present do not require that the random sample be a collection
of iid random variables, and so the conditions can also be applied to cases other
than random sampling from a population distribution.
Henceforth we focus on the case where an MLE is the unique global maxi-
mizer of the likelihood function, which covers the majority of applications. An
examination of the multiple local optima case is more complicated and is best
left to a more advanced course of study.27
Consistency
We ﬁrst examine conditions that ensure the consistency of the
MLE of Y in the scalar case.
Theorem 8.15
MLE Consistency-
Sufﬁcient Conditions
for Scalar Y
Let {f(x;Y), Y∈O} be the statistical model for the random sample X, where Y is a
scalar.28 Assume the following regularity conditions:
1. The PDFs f(x;Y), Y ∈O, have common support, X
2. The parameter space, O is an open interval;
3. ln(L(Y;x)) is continuously differentiable with respect to Y ∈O, 8 x ∈X;
4. @ln LðY; xÞ
ð
Þ=@Y ¼ 0 has a unique solution for Y ∈O, and the solution
deﬁnes the unique maximum likelihood estimate, ^Y(x), 8 x ∈X;
5. limn!1P ln L Yo; x
ð
Þ
ð
Þ> ln L Y; x
ð
Þ
ð
Þ
ð
Þ ¼ 1 for Y 6¼ Yo, where Yo is the true
value of Y ∈O.29 Then ^Y !
p
Yo, and thus the MLE is consistent for Y.
Proof
Let h > 0 be such that Yoh ∈O and Yo + h ∈O, where such an h exists by
condition (2), and deﬁne the events
26Norden, R.H. (1972; 1973) “A Survey of Maximum Likelihood Estimation,” International Statistical Revue, (40): 329–354 and (41):
39–58.
27See Lehmann, E. (1983) Theory of Point Estimation, John Wiley and Sons, pp. 420–427.
28We remind the reader of our tacit assumption that Y is identiﬁed (Deﬁnition 7.2).
29By true value of Y, we again mean that Yo is the value of Y ∈O for which f(x;Yo)  L(Yo;x) is the actual joint density function of the
random sample X. The value of Yo is generally unknown, and in the current context is the objective of point estimation.
482
Chapter 8
Point Estimation Methods

An ¼ fx : ln L Yo; x
ð
Þ> ln LðYo  h; xÞg;
Bn ¼ x : ln L Yo; x
ð
Þ> ln L Yo þ h; x
ð
Þ
f
g;
and Hn ¼ An \ Bn. As n ! 1, P(Hn) ! 1, since P(Hn) ¼ P(An) + P(Bn)  P(An
[ Bn), P(An) and P(Bn) both converge to 1 as n ! 1 by Assumption (5), and
P(An [ Bn) converges to 1 since P(An [ Bn)  P(An) and P(An [ Bn)  P(Bn).
Now note that x ∈Hn ) L(Y;x) exhibits its unique maximum for some value
Y* such that Yo h < Y* < Yo + h because by the differentiability of L(Y;x), Y*
solves @ ln LðY; xÞ
ð
Þ=@Y ¼ 0 and is thus the ML estimate, ^Y(x), of Y given x. Note
further that Hn  {x: Yo  h < ^Y (x) < Yo + h} because x ∈Hn implies Yo  h <
^Y (x) < Yo + h. Then P(Hn) ! 1 as n ! 1 implies P(Hn)  P(Yo  h < ^Y(x) <
Yo + h) ! 1 as n ! 1. Since the foregoing result remains true if we decrease the
values of h to be arbitrarily close to zero (but still positive-valued), ^Y !
p Yo.
n
In some cases the veriﬁcation of condition 5 in Theorem 8.15 can be chal-
lenging. If random sampling is such that the random sample X1,. . .,Xn is a
collection of iid random variables, then condition 5 is not needed.
Theorem 8.16
MLE Consistency: iid
Random Sampling and
Scalar Y
Assume conditions 1–4 of Theorem 8.15, and assume further that the random
sample X1,. . .,Xn is a collection of iid random variables. Then ^Y !
p Yo.
Proof
Let Yℓ¼ Yo  e and Yh ¼ Yo + e for any e > 0 s.t. Yℓand Yh ∈O (such e0s
exists
by
assumption
2).
Deﬁne
H(e) ¼ {x:
ln(L(Yo;x)) > ln(L(Yℓ;x))
and
ln(L(Yo;x)) > ln(L(Yh;x))} and note that x ∈H(e) ) ^Y ∈(Yo  e, Yo + e) because
the MLE is unique and is deﬁned via @ ln LðY; xÞ
ð
Þ=@Y ¼ 0.
Now deﬁne A(Y) ¼ {x: ln(L(Yo;x)) > ln(L(Y;x))} for Y 6¼ Yo, and note that the
event A(Y) can be equivalently represented as
tn x
ð Þ ¼ n1 X
n
i¼1
ln f xi; Y
ð
Þ=f xi; Yo
ð
Þ
ð
Þ<0:
Because tn(x) can be interpreted as the sample mean of n iid random variables
of the form ln(f(Xi;Y)/f(Xi;Yo)), Khinchin’s WLLN implies that
tn X
ð Þ !
p
E ln f Xi; Y
ð
Þ=f Xi; Yo
ð
Þ
½
	
ð
Þ. Also, ln(z) is strictly concave over its domain so that,
Jensen’s inequality implies E(ln(f(Xi;Y)/f(Xi;Yo))) < ln(E(f(Xi;Y)/f(Xi;Yo))). Then
since E(f(Xi;Y)/f(Xi;Yo)) ¼ 1,30 the right-hand side of the preceding inequality is
zero. Thus, tn(X) converges in probability to a negative number, which implies
that limn!1P A Y
ð
Þ
ð
Þ ¼ 1 when Y 6¼ Yo. This in turn implies that limn!1P H eð Þ
ð
Þ
30If Xi is a continuous random variable, then since Yo is the true value of Y,
E f Xi; Y
ð
Þ=f Xi; Y0
ð
Þ
½
	 ¼
Z 1
1
f xi; Y
ð
Þ
f xi; Yo
ð
Þf xi; Yo
ð
Þdxi ¼
Z 1
1
f xi; Y
ð
Þdxi ¼ 1
because f(xi;Y) is a probability density function. The discrete case is analogous.
8.3
The Method of Maximum Likelihood
483

¼ 1 8e>0 since H(e) ¼ A(Yℓ) \ A(Yh), and thus limn!1P ^Y 2 Yo  e; Yo þ e
ð
Þ


¼ 1 8e>0 and ^Y !
p
Yo.
n
Example 8.11
Consistency of MLE for
Exponential
Distribution
Reexamine the case of estimating the value of y using the MLE when random
sampling is from an exponential density function (recall Examples 8.4, and 8.9).
In this case the joint density function of the random sample is given by
f x; y
ð
Þ ¼ ynePn
i¼1 xi=y Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ:
The parameter space is an open interval since O ¼ (0, 1). The log of the likeli-
hood function is continuously differentiable for y ∈O and @ ln L y; x
ð
Þ
ð
Þ=@y ¼
n=y
ð
Þ þ Pn
i¼1 xi


=y2 ¼ 0 has the unique solution ^y ¼ Pn
i¼1 xi=n, which is the
unique maximum likelihood estimate 8x ∈ℝn
þ .31 Therefore, it follows from
Theorem 8.16 that ^Y !
p Yo, so that the MLE is a consistent estimator.
□
We now examine sufﬁcient conditions for MLE consistency when Q is k-
dimensional. We present two sets of sufﬁcient conditions. One set allows
unbounded parameter spaces but is generally more difﬁcult to apply (1–4a).
The other set(1–3, 4b) is generally more tractable but requires the parameter
space to be a bounded and closed rectangle (or more generally, a closed and
bounded set). As a practical matter, one can often state (perhaps very large)
absolute bounds for the parameters of a statistical model based on real-world
or theoretical considerations relating to an experiment so that the boundedness
of the parameter space may not represent a serious restriction in practice.
Theorem 8.17
MLE Consistency-
Sufﬁcient Conditions
for Parameter Vectors
Let {f(x;Q), Q ∈O} be the probability model for the random sample X. Let
N(e) ¼ {Q: d(Q,Qo) < e} be an open e-neighborhood of Qo, where Qo is the true
value of Q.32 Assume the following regularity conditions:
1. The PDFs f(x;Q), Q∈O, have common support, X;
2. ln(L(Q;x)) has continuous ﬁrst-order partial derivatives with respect to
Q∈O, 8x∈X;33
3. @ ln LðQ; xÞ
ð
Þ=@Q ¼ 0 has a unique solution that deﬁnes the unique maxi-
mum likelihood estimate ^QðxÞ ¼ arg maxY2O{L(Q;x)} 8x∈X;
31That this unique solution is a maximum can be demonstrated by noting that@2 ln LðY; xÞ
ð
Þ=@Y2 ¼ n=Y2  2 Pn
i¼1 xi=Y3, which when
evaluated at the maximum likelihood estimate ^Y ¼ Pn
i¼1 xi=n, yields
@2 ln L ^y; x






= @Y2




¼ n3= Pn
i¼1 xi

2<0.
32N(e) is an open interval, the interior of a circle, the interior of a sphere, and the interior of a hypersphere in 1,2,3, and  4 dimension,
respectively.
33It is allowable that conditions (2) and (3) be violated on a set of x-values having probability zero.
484
Chapter 8
Point Estimation Methods

4a. limn!1Pðln LðQo; xÞ
ð
Þ>maxY2N eð Þ ln LðQ; xÞÞ
ð
Þ ¼ 1 8 e > 0 with O being an
open rectangle containing Qo;34
4b. limn!1P maxQ2O jn1 ln L Qo; x
ð
Þ
ð
Þ  G Q
ð
Þj


<e


¼ 1 8e>0 with G(Q) being
a continuous function that is uniquely globally maximized at Q ¼ Qo and
O is a bounded closed rectangle containing Qo.
Then ^Q !
p Qo.
Proof
See Appendix.
n
We now return to the case of random sampling from the gamma density
where there is no closed form solution for the MLE, although ^Q is implicitly
deﬁned by ﬁrst order conditions.
Example 8.12
Consistency of MLE for
Gamma Distribution
Reexamine the case of estimating the value of a and b using the MLE when
random sampling is from a gamma population distribution (recall Example 8.10).
In an attempt to utilize Theorem 8.16 for demonstrating the consistency of the
MLE, ﬁrst note that f(x;Q) > 0 for x 2 ℝn
þ , and so condition (1) is satisﬁed.
Recalling Example 8.10, it is evident that ln(L(Q;x)) is continuously differentia-
ble with respect to a and b 8x 2 ℝn
þ and for all a > 0 and b > 0, validating
condition (2). Also, the ﬁrst order conditions have a unique solution for
a; b
ð
Þ
8x 2 ℝn
þ, which deﬁnes the unique maximum likelihood estimate ^a; ^b


(again
recall Example 8.10) satisfying condition (3).
Assume that bounds can be placed on the parameter values of a and b, and
examine the validity of condition (4b) of Theorem 8.17. Note that (suppressing
the indicator function)
n1 ln L a; b; x
ð
Þ
ð
Þ ¼ a ln b
ð Þ  ln G a
ð Þ
ð
Þ þ a  1
ð
Þn1 Xn
i¼1 ln xi
ð
Þ  x=b:
Then because35
p lim n1 Xn
i¼1 ln Xi
ð
Þ


¼
dG ao
ð
Þ=da
ð
Þ= ao
ð
Þ
ð
Þ þ ln bo
ð
Þ;
it follows that
plim n1 ln L a; b; x
ð
Þ
ð
Þ


¼ G a; b
ð
Þ ¼ a ln bo
b


þ a  1
ð
Þ dG ao
ð
Þ=da
G ao
ð
Þ


 ln GðaÞ
ð
Þ  ao bo
b
 ln bo
ð
Þ
uniformly in (a,b). Furthermore, the maximum of G(a,b) occurs at a ¼ ao and
b ¼ bo. To see this, note that ao and bo solve the ﬁrst order conditions for the
maximization problem given by
34Change max to sup if max does not exist.
35This follows from Khinchin’s WLLN upon recognizing that the right hand side expression represents E(ln(xi)).
8.3
The Method of Maximum Likelihood
485

@Gða; bÞ
@a
¼ ln bo
b


þ dGðaÞ=da
GðaoÞ
 dGðaÞ=da
GðaÞ
¼ 0
and
@Gða; bÞ
@b
¼ a=b þ aobo=b2 ¼ 0:
The hessian matrix is negative deﬁnite, so that the second order conditions for a
maximum are met (see Example 8.15 for the explicit representation of the
hessian matrix), and thus condition (4b) holds. Therefore, the estimator
^Q ,
deﬁned implicitly by the ﬁrst order conditions for maximizing the likelihood
function, is consistent, i.e., ^Q !
p Qo, so that both ^a !
p ao and ^b !
p bo.
□
In the next example, we revisit Example 8.6, in which the MLE of (b,s2) for
the GLM was deﬁned for the case where the random sample from a general
sampling experiment had a joint density of the multivariate normal form, N(xb,
s2I). In this case, the MLE can be explicitly solved for, and its consistency
property evaluated directly, as we have done in Section 8.2, but for the sake of
illustration we reexamine consistency using Theorem 8.17.
Example 8.13
Consistency of MLE in
GLM with Y N(xb, s2I)
Reexamine Example 8.6. To demonstrate consistency of the MLE using Theo-
rem 8.17, ﬁrst note f(y;b,s2) > 0 for y ∈ℝn, and so condition (1) is satisﬁed.
It is evident from Example 8.6 that L(b,s2;y) is continuously differentiable
with respect to Q ∈O for all y ∈ℝn, so that condition (2) is satisﬁed. Also,
@ ln L b; s2; y




=@Q ¼ 0 has a unique solution (assuming x0x is of full rank), and
this solution deﬁnes the unique MLE of (b,s2). We assume that s2 2 s2
ℓ; s2
h
h
i
and
b 2 k
i¼1 biℓ; bih
½
	 and verify condition (4b). Note that
Zn ¼ n1 ln L b; s2; y




¼  1=2
ð
Þ ln 2ps2


 1=2
ð
Þn1
y  xb
ð
Þ0 y  xb
ð
Þ=s2

	
:
Because Y ¼ xbo + «, Zn can be expressed equivalently as
Zn ¼  1=2
ð
Þ ln 2ps2


þ bo b
ð
Þ0 n1x0x


bo b
ð
Þ=s2 þ n1«0«=s2 þ 2 bo b
ð
Þ0 n1x0«


=s2

	
:
Since the ei0s are iid because Y ~ N(xb0,s2I), it follows from Khinchin’s WLLN
that plim ðn1«0«Þ ¼ s2
0. Also note that E(n1x0«) ¼ 0 and Cov(n1x0«) ¼ s2n2x0x,
so that assuming x0x is o(n2), so that n2x0x ! 0 as n ! 1, then n1x0« !
p
0,
and thus n1x0 « !
p
0 and
bo  b
ð
Þ0 (n1x0 «)/s2
!
p
0. Assuming further that
n1x0x ! Q, a positive deﬁnite symmetric matrix, then
Zn !
p G b; s2


¼  1
2 ln 2ps2


þ bo b
ð
Þ0Q bo b
ð
Þ=s2 þ s2
o=s2

	
uniformly in b and s2.
486
Chapter 8
Point Estimation Methods

The function G(b,s2) is maximized when b ¼ bo
and s2 ¼ s2
o . To see
this, note that bo and s2
o satisfy the ﬁrst order conditions for maximizing
G(b,s2) given by
@G b; s2


@b
¼ Q bo  b
ð
Þ ¼ 0
@G b; s2


@s2
¼  1
2s2 þ bo  b
ð
Þ0Q bo  b
ð
Þ
2s2
þ s2
o
2s2 ¼ 0
The hessian matrix for checking second order conditions, evaluated at b ¼ bo
and s2 ¼ s2
o, is given by
@2 Gðbo; s2
oÞ
@2 Gðbo; s2
oÞ
@b@b0
@b@ s2
@2 Gðbo; s2
oÞ
@2 Gðbo; s2
oÞ
@ s2 @b0
@ðs2Þ2
2
666664
3
777775
¼
Q
0
0
 1
2 s4
o
2
4
3
5;
which is negative deﬁnite, and thus G(b,s2) is indeed maximized at bo, s2
o .
Therefore, by Theorem 8.17, and under the preceding assumptions on x, ^Q !
p Qo
so that plim ^b

 
¼ bo and plim ^s2


¼ s2
o.
□
Asymptotic Normality and Asymptotic Efﬁciency
In order that the MLE be asymp-
totically normally distributed, additional regularity conditions on the maximum
likelihood estimation problem are needed. We present a collection of sufﬁcient
conditions below, and note that there exist a variety of alternative sufﬁcient
conditions in the literature (see Norden, op. cit.; Amemiya, T., Advanced Econo-
metrics, pp. 111–112 for conditions related to those presented here).
Theorem 8.18
MLE Asymptotic
Normality-Sufﬁcient
Conditions
In addition to conditions (1–4) of Theorem 8.17, assume that:
1. @2 ln L Q; x
ð
Þ
ð
Þ=@Q@Q0 exists and is continuous in Q 8Q ∈O and 8x ∈J,
2. plim n1 @2 ln L Q; X
ð
Þ
ð
Þ=@Q@Q0




¼ H Qo
ð
Þ is a nonsingular matrix for any
sequence of random variables
Q
n


such that plim Q
n


¼ Qo,
3. n1=2 @ ln L Qo; X
ð
Þ
ð
Þ
ð
Þ=@Q !
d N 0; M Qo
ð
Þ
ð
Þ where M(Qo) is a positive deﬁnite
symmetric matrix.
Then the MLE, ^Q, is such that
n1=2
^Q  Qo


!
d N 0; H Qo
ð
Þ1M Qo
ð
ÞH Qo
ð
Þ1


and
^Q 
a N Qo; n1H Qo
ð
Þ1M Qo
ð
ÞH Qo
ð
Þ1


:
Proof
See Appendix.
n
8.3
The Method of Maximum Likelihood
487

We present one further condition that leads to the asymptotic efﬁciency of
the MLE.
Theorem 8.19
MLE Asymptotic
Efﬁciency: Sufﬁcient
Conditions
In addition to the assumptions in Theorem 8.17 and Theorem 8.18, assume that
M Qo
ð
Þ ¼ lim
n!1
n1E @ ln L Qo; X
ð
Þ
ð
Þ
@Q
@ ln L Qo; X
ð
Þ
ð
Þ
@Q0




¼ H Qo
ð
Þ
Then ^Q is asymptotically efﬁcient.
Proof
From Theorem 8.18, if M(Qo) and H(Qo) are as deﬁned above, then n1=2
^Q  Qo


!
d Z  N 0; M Qo
ð
Þ1


. Referring to the deﬁnition of asymptotic efﬁciency
(Deﬁnition 7.22), it follows that36
n1=2 E @ ln L Qo; X
ð
Þ
@Q
@ ln L Qo; X
ð
Þ
@Q0



1=2
n1=2
^Q  Q


!
d M Qo
ð
Þ1=2Z  N 0; I
ð
Þ
so that ^Q 
a N Qo; M Qo
ð
Þ1


is asymptotically efﬁcient.
n
In applications, since the value of Qo is unknown, the asymptotic covariance
of ^Q is also unknown. However, under the assumptions of Theorems 8.18 and
8.19, a consistent estimate of M(Qo)1 is obtained by using an outcome of
 n1 @2 ln L ^Q; X




=@Q@Q0


h
i1
(recall assumption (2)) of Theorem 8.18 and
the statement of Theorem 8.19).
We now revisit Examples 8.11 and 8.12 and illustrate the use of Theorems
8.18 and 8.19 for establishing the asymptotic normality and asymptotic efﬁ-
ciency of the MLE.
Example 8.14
Asymptotic Normality
and Efﬁciency of MLE -
Exponential Distribution
Reexamine Example 8.11. Note that @2 ln L y; x
ð
Þ
ð
Þ=@y2 ¼ n=y2


 2 Pn
i¼1 xi=y3


exists and is continuous for 8y > 0 and 8x ∈X, so assumption (1) of Theorem
8.18 is satisﬁed. Furthermore, assuming plim(Y*) ¼ yo, it follows by Slutsky’s
theorem that
plim n1 @2 ln L Y; X
ð
Þ
ð
Þ
@y2
 
!
¼ plim Y2

 2Xn=Y3



¼ y2
o
 2yo=y3
o


¼ y2
o
¼ H yo
ð
Þ
so that assumption (2) of Theorem 8.18 applies. Also note that
36Recall that the matrix square root is a continuous function of its arguments, so that plim
A1=2
n


¼ p lim An
ð
Þ1=2 . Letting
An ¼ n1E @ ln L Qo; X
ð
Þ
@Q
@ ln L Qo; X
ð
Þ
@Q0


leads to plim A1=2
n


¼ M Qo
ð
Þ1=2.
488
Chapter 8
Point Estimation Methods

n1=2 @ ln L yo; X
ð
Þ
ð
Þ
@y
¼ n1=2y1
o
þ n1=2
X
n
i¼1
Xi
y2
o
 
!
¼ 1
yo
n1=2 Xn  yo


yo
"
#
!
d N 0; y2
o


¼ N 0; M yo
ð
Þ
ð
Þ
where M(yo) ¼ y2
o , which follows by a direct application of the Lindberg-Levy
CLT to the bracketed expression and by Slutsky’s theorem, since E(Xi) ¼ yo and
var(Xi) ¼ y2
o8i. Then, from Theorem 8.18, it follows that
n1=2
^Y  yo


!
d N 0; y2
o


; and ^Y 
a N yo; n1y2
o


:
Regarding asymptotic efﬁciency, recall from Example 7.21 that
E @ ln L yo; X
ð
Þ
@y
@ ln L yo; X
ð
Þ
@y0


¼ n=y2
o
Asymptotic efﬁciency of ^Y follows immediately from Theorem 8.19 since the
results above demonstrate the equality
M yo
ð
Þ ¼ H yo
ð
Þ ¼ lim
n!1
E n1 @ ln L yo; X
ð
Þ
@y
@ ln L yo; X
ð
Þ
@y0






¼ y2
o :
□
Example 8.15
Asymptotic Normality
and Efﬁciency of MLE:
Gamma Distribution
Reexamine Example 8.12. The second order derivatives of ln(L(a,b;x)), divided by
n, are given by
1
n
@2 ln L a; b; x
ð
Þ
ð
Þ
@a2
¼  GðaÞ d2GðaÞ
da2

dGðaÞ
da

2
"
#
=G a
ð Þ2;
1
n
@2 ln L a; b; x
ð
Þ
ð
Þ
@a@b
¼ 1
n
@2 ln L a; b; x
ð
Þ
ð
Þ
@b@a
¼ b1;
1
n
@2 ln L a; b; x
ð
Þ
ð
Þ
@b2
¼ a
b2  2 x
b3 ;
and the derivatives themselves are continuous functions of (a,b) for a > 0 and
b > 0 and 8x ∈X, so that assumption (1) of Theorem 8.18 is satisﬁed.37 Letting
Q ¼
a
b


; Qo ¼
ao
bo


; , and Q* be such that Q* !
p
Qo, it follows from conti-
nuity that
37The gamma function, G(a), is continuous in a, and its ﬁrst two derivatives are continuous in a, for a > 0. G(a) is in fact strictly convex,
with its second order derivative strictly positive for a > 0. See Bartle, op. cit., p. 282.
8.3
The Method of Maximum Likelihood
489

plim n1 @2 ln L Q; XÞ
ð
Þ
ð
Þ
@Q@Q0
 
!
¼ 
G ao
ð
Þ d2G ao
ð
Þ
da2

dGðaoÞ
da

2
"
#.
G ao
ð
Þ2
b1
o
b1
o
ao
b2
o
2
6664
3
7775 ¼ H Qo
ð
Þ
(recall that plim X
 
¼ ab in this case), so that the convergence condition in (2) of
Theorem 8.18 applies.
Regarding condition (3), note that
n1=2 @ ln L Qo; XÞ
ð
Þ
ð
Þ
@Q
¼
n1=2 ln bo
ð
Þ þ
d2G ao
ð
Þ
da2
h
i
=G ao
ð
Þ2
h
i
þ n1=2 P
n
i¼1
ln Xi
ð
Þ
n1=2 ao
bo
h i
þ n1=2
Xn
b2
o
h
i
2
64
3
75
To establish the bivariate normal limiting density of this (2  1) random vector,
ﬁrst note that (Z1,. . .,Zn), where Zi ¼
ln Xi
ð
Þ
Xi=b2
o


, is a collection of iid random
variables with38
E Zi
ð
Þ ¼ m ¼
dG ao
ð
Þ
da
=G ao
ð
Þ þ ln bo
ð
Þ
ao=bo
"
#
;
Cov Zi
ð
Þ ¼ F ¼
G ao
ð
Þd2G ao
ð
Þ
da2

dGðaoÞ
da

2


=G ao
ð
Þ2
bo
1
b1
o
ao=b2
o
2
4
3
5:
The Multivariate Lindberg-Levy CLT then implies that
n1=2 Z  m


!
d N 0; F
ð
Þ; where Z ¼
n1 P
n
i¼1
ln Xi
ð
Þ
X=b2
o
2
4
3
5:
Noting that
n1=2 @ ln L Qo; X
ð
Þ
ð
Þ
ð
Þ ¼ n1=2 Z  m


!
d N 0; M Qo
ð
Þ
ð
Þ;
38A way of deriving the expectations involving ln(Xi) that is conceptually straightforward, albeit somewhat tedious algebraically, is
ﬁrst to derive the MGF of ln(Xi), which is given by btG(a + t)/G(a). Then using the MGF in the usual way establishes the mean and
variance of ln(Xi). The covariance between Xi=b2
o


and ln(Xi) can be established by noting that
b2
o EðXi lnðXiÞÞ ¼ b2
o
1
bao
o G ao
ð
Þ
h
i Z 1
o
lnðxiÞ
ð
Þxi
aoexi=bodxi
¼ aob1
o EðlnðXiÞÞ
:
where E* denotes an expectation of ln(Xi) using a gamma density having parameter values ao + 1 and bo. Then cov
Xi=b2
o


; lnðXiÞ


is
equal to
aob1
o EðlnðXiÞÞ  EðlnðXiÞÞE Xi=b2
o




¼ aob1
o
EðlnðXiÞÞ  EðlnðXiÞÞ
ð
Þ ¼ b1
o :
490
Chapter 8
Point Estimation Methods

where M(Qo) ¼ F, it follows that convergence to a bivariate normal density is
established, and thus condition (3) of Theorem 8.18 is met. Finally, note that
M Qo
ð
Þ ¼ lim
n!1
n1E @ ln L Qo; X
ð
Þ
@Q
@ ln L Qo; X
ð
Þ
@Q0




¼ H Qo
ð
Þ
which follows immediately upon taking the expectation and recalling that E X
 
¼ ab. Thus, the condition of Theorem 8.19 is met, and we can conclude that
n1=2
^Q  Qo


!
d N 0; M Qo
ð
Þ1


;
^Q 
a N Qo; n1M Qo
ð
Þ1


; and ^Q is asymptoti-
cally efﬁcient.
□
We emphasize that although we were unable to explicitly solve for the
function deﬁning the MLE in the case of random sampling from a general
gamma density, our theorems on the asymptotic properties of the MLE still
allowed us to establish the asymptotic properties of the MLE which were
implicitly deﬁned by ﬁrst order conditions. In particular, we now know that ^Q
is consistent, asymptotically normal, and asymptotically efﬁcient as an estima-
tor of (a,b).
The veriﬁcation of the MLE’s asymptotic properties can be somewhat com-
plicated at times. Regarding the density functions commonly used in practice, it
can be said that the MLE is quite generally consistent, asymptotically normal,
and asymptotically efﬁcient when the random sample is a collection of iid
random vectors. In particular, the MLE is consistent, asymptotically normal,
and asymptotically efﬁcient when random sampling from any of the exponential
class densities we have presented in Chapter 4, and these asymptotic properties
hold quite generally for exponential class densities (see Lehmann, op. cit., pp.
417–418 and pp. 438–439). In more general situations involving general
experiments, one must typically verify the asymptotic properties of the MLE
on a case-by-case basis.
8.3.4
MLE Invariance Principle and Density Reparameterization
Our discussion of MLEs to this point has concentrated on the estimation of the
parameter vector Q itself. Fortunately, the maximum likelihood procedure has
an invariance property that renders our preceding discussion entirely and
directly relevant for estimating functions of the parameter vector. Before stating
the property formally, we provide some additional background discussion
concerning the parameterization of density functions.
The reader will recall that the members of parametric families of densities
are indexed, or identiﬁed, by values of a parameter vector whose admissible
values are identiﬁed by a set of values represented by the parameter space, O.
Upon a moment’s reﬂection, it is evident that there is nothing sacrosanct about
a particular indexing of the members of a parametric family of densities. That is,
a collection of density functions, f(x;Q) for Q ∈O, could be equally well
represented by f(x;q1(j)), where j ¼ q(Q) is any invertible function of Q ∈O.
8.3
The Method of Maximum Likelihood
491

Such a parameter transformation is referred to as a reparameterization of the
density family. As a speciﬁc example, consider the exponential family f x; y
ð
Þ
¼ 1=y
ð
Þex=yI 0;1
ð
Þ x
ð Þ for y 2 O, where O ¼ (0,1). This same family of densities
could be represented equally well by h x; x
ð
Þ ¼ f x; q1
1
x
ð Þ


¼ xe  xxI 0;1
ð
Þ x
ð Þ for
x 2 Ox , where x ¼ q1(y) ¼ y1 for y ∈O, so that Ox ¼ (0,1). Another para-
meterization of this same family of densities is given by m x; t
ð
Þ ¼ f x; q1
2
t
ð Þ


¼
t1=2 exp xt12


I 0;1
ð
Þ x
ð Þ for t 2 Ot; where now t ¼ q2(y) ¼ y2 for y ∈O, so that
Ot ¼ (0,1). In all cases the exact same collection of densities is identiﬁed.
Of course, the interpretation of the parameter differs in each case, where y is the
mean of X, x is the reciprocal of the mean of X, and t is the variance of X (recall
s2 ¼ y2 in the exponential family). The fact that a density family can be
reparameterized provides ﬂexibility for redeﬁning the maximum likelihood prob-
lem in terms of a parameter or parameter vector having an interpretation that may
be of more fundamental interest to research objectives. The fact that the actual
reparameterization process is unnecessary for obtaining the maximum likelihood
estimate of q(Q) is presented in the following theorem.
Theorem 8.20
MLE Invariance
Principle: Invertible
Case
Let ^Qbe a MLE of Q, and suppose that the function q(Q), for Q ∈O, is invertible.
Then q( ^Q) is a MLE of q(Q).
Proof
The maximum likelihood estimate of Q is given by ^u ¼ arg maxY2O L Q; x
ð
Þ
f
g.
Suppose j ¼ q(Q) is invertible, so the likelihood function can be reexpressed
as a function of j by substituting Q ¼ q1(j), j ∈Ox ¼ {j : j ¼ q(Q), Q ∈O},
in L(Q;x). Then the maximum likelihood estimate for j would be
^j ¼
arg maxj2Ox L q1 j
ð Þ; x




. But since ^u maximizes L(Q;x), the value of ^j that
solves the latter likelihood maximization problem satisﬁes q1 ^j

 
¼ ^u, which
implies by the invertibility of q that ^j ¼ q ^u

 
.
n
The invariance principle in Theorem 8.20 implies that once a MLE, ^Q, of Q
has been found, one can immediately deﬁne the MLE of any invertible function
q(Q) of Q as q(^Q). The invariance property is obviously a very convenient feature
of the maximum likelihood estimation approach. In fact, the invariance princi-
ple of maximum likelihood estimation extends to more general functions of Q,
invertible or not, although the intuitive interpretation of invariance in terms of
reparameterizing a family of density functions no longer follows. The more
general invariance principle utilizes the notion of maximizing an induced like-
lihood function, and the concept of the parameter space induced by q(Q), as
deﬁned in the proof of the following generalization of Theorem 8.20.
Theorem 8.21
MLE Invariance
Principle: General Case
Let ^Q be a MLE of the (k  l) parameter vector Q, and let q(Q) be a (r  1) real-
valued vector function of Q ∈O, where r  k. Then q( ^Q) is a MLE of q(Q).
492
Chapter 8
Point Estimation Methods

Proof
See Appendix.
n
Note from the proof of Theorem 8.21 that a maximizing value of t then
maintains the intuitively appealing property of associating with the sample
outcome, x, the highest probability (discrete case) or highest density weighting
(continuous case) that is possible within the parametric family f(x;Q), Q ∈O.
The following examples illustrate both the versatility and the simplicity of
Theorems 8.20 and 8.21.
Example 8.16
MLE of Bernoulli
Variance via the
Invariance Principle
Let X ¼ (X1,. . .,Xn)0 be a random sample from the Bernoulli population distribu-
tion f(z;p) ¼ pz(1  p)1zI{0,1}(z) representing whether or not a customer contact
results in a sale. The MLE of p is given by ^p ¼ X . Then by the invariance
principle, the MLE of the variance of the Bernoulli population distribution,
s2 ¼ q(p) ¼ p(1  p), is given by ^s2 ¼ X 1  X


.
□
Example 8.17
MLEs of Parameter
Sums and Ratios, and
Residual Standard
Deviation in the GLM
via Invariance Principle
Reexamine the case of maximum likelihood estimation in the GLM (Example
8.6), where the MLE of b and s2 is given by ^b ¼ (x0x)1x0Y and ^s2 ¼ (Y  x^b)0
(Y  x ^b)/n. Then by the invariance principle, the MLE of P
i2I bi is given by
P
i2I ^bi, the MLE of bi/bjis given by ^bi/^bj, and the MLE of s is given by ^s2

1=2.□
While the invariance principle allows convenient and straightforward
deﬁnitions of MLEs for functions of Q, the invariance principle does not imply
that the estimator properties attributed to ^Q transfer to the estimator q( ^QÞ of
q(Q). In particular, the fact that ^Q may be unbiased, MVUE, consistent, asymp-
totically normal, and/or asymptotically efﬁcient does not necessarily imply that
the MLE q(^Q) possesses any of the same properties. The small sample properties
of q(^Q) as an estimator of q(Q) need to be checked on a case-by-case basis. Useful
generally applicable results are not available that delineate problem conditions
under which small sample properties of ^Q are transferred to a general function of
^Q, q( ^Y), used to estimate q(Q). However, asymptotic properties of ^Q transfer to
qð ^QÞ under fairly general conditions, as the following two theorems indicate:
Theorem 8.22
Consistency of MLE
Deﬁned via the
Invariance Principle
Let ^Q be a consistent MLE of Q, and let q(Q) be a continuous function of Q ∈O.
Then the MLE q( ^Q) is a consistent estimator of q(Q).
Proof
This follows directly from Theorem 5.5, since if q(Q) is a continuous function of
Q, and if plim ^Q


¼ Q, then plim(q( ^Qn)) ¼ q(plim( ^Qn)) ¼ q(Q).
n
8.3
The Method of Maximum Likelihood
493

Theorem 8.23
Asymptotic Normality
and Efﬁciency of MLE
Deﬁned via the
Invariance Principle
Let ^Q be a consistent, asymptotically normal, and asymptotically efﬁcient MLE
of Q, satisfying the conditions of Theorems 8.18 and 8.19 so that n1=2
^Q  Qn


!
d N 0; M Qo
ð
Þ1


. Let q(Q) be continuously differentiable with respect to Q and
let ∂q(Qo)/∂Q have full row rank. Then q( ^Q) is a consistent, asymptotically
normal and asymptotically efﬁcient MLE of q(Q).
Proof
It follows directly from Theorem 5.40 and Slutsky’s theorem that
@q Qo
ð
Þ0
@Q
M Qo
ð
Þ1 @q Qo
ð
Þ
@Q

1=2
n1=2 q ^Q


 q Qo
ð
Þ


!
d N 0; I
ð
Þ:
Using Theorems 8.18 and 8.19 and Slutsky’s theorems, it then also follows that
@q Qo
ð
Þ
@Q
0
E @ ln L Qo; X
ð
Þ
ð
Þ
@Q
@ ln L Qo; X
ð
Þ
ð
Þ
@Q0



1 @q Qo
ð
Þ
@Q
"
#1=2
q ^Q


 q Qo
ð
Þ


!
d N 0; I
ð
Þ;
which by Deﬁnition 7.22 justiﬁes the conclusion of the theorem.
n
Example 8.18
MLE Consistency,
Asymptotic Normality,
and Asymptotic
Efﬁciency for Bernoulli
Variance
Recall Example 8.16. It is clear from results on sample moment properties that
^P ¼ X is consistent for p; n1=2 ^P  p


!
d N 0; p 1  p
ð
Þ
ð
Þ, and ^P is asymptotically
efﬁcient as an estimator of p. Examine the MLE of s 2 ¼ p(1  p), which by the
invariance principle is given by ^s2 ¼ X 1  X


.
Because p(1  p) is continuous for p ∈[0,1], ^s2 is consistent for p(1  p) by
Theorem 8.20. Now note that dq(p)/dp ¼ 1  2p is continuous and is 6¼ 0 (and thus
full rank) provided p 6¼ .5. Then X 1  X


is asymptotically normal and asymptoti-
cally efﬁcient by Theorem 8.23, where ^s2 
a N s2; 1  2p
ð
Þ2p

1  p
ð
Þ=nÞ. In the case
where p ¼ .5, the asymptotic distribution of ^s2 can be shown to be equal to the
distribution of the random variable .25 (1  Y/n), where Y ~ w2
1 . See Bickel and
Doksum, op. cit., p. 53.
□
Example 8.19
MLE Consistency,
Asymptotic Normality,
and Asymptotic
Efﬁciency for Parameter
Sums and Standard
Deviation in the GLM
Recall Example 8.17. Assuming n1 x0x ! Q, a symmetric positive deﬁnite
matrix, it can be shown that the MLE
^b
^s2


is consistent, asymptotically
normal, and asymptotically efﬁcient as an estimator of
b
s2


where
n1=2
^b
^s2



bo
s2
o




!
d N 0;
s2Q1
0
0
2s4




:
Examine the MLE q1 ^b; ^s2


¼ Pk
i¼1 ^bi of q1 b; s2


¼ Pk
i¼1 bi. Since q1(b,s2) is
continuous in b and s2; q1 ^b; ^s2


is a consistent estimator of q1(b,s2) by
Theorem 8.22. Also, since ∂q1/∂Q ¼ (1 1 . . . 1 0)0 6¼ 0
where Q ¼
b
s2




,
494
Chapter 8
Point Estimation Methods

q1 ^b; ^s2


is asymptotically normal and asymptotically efﬁcient by Theorem
8.22, where Pk
i¼1 bi 
a N i0bo; i0s2 x0x
ð
Þ1i


with i a (k  1) vector of ones.
Now examine the MLE q2 ^b; ^s2


¼ ^s2

1=2 ¼ ^s of q2(b,s2) ¼ (s2)1/2 ¼ s. Since
q2(b,s2) is continuous in b and s2 > 0, ^s is a consistent estimator of s by
Theorem 8.22. Also, since ∂q2/∂Q ¼ [0 . . . 0 ((1/2)(s2)1/2)]0 6¼ 0 and the deriva-
tive is continuous for s2 > 0, ^s is asymptotically normal and asymptotically
efﬁcient by Theorem 8.23, where ^s 
a N s; s2=2n


.
□
Example 8.20
MLE Consistency,
Asymptotic Normality,
and Asymptotic
Efﬁciency for
Exponential Variance
Recall Example 8.4. We know that the MLE of y in the exponential population
distribution is given by
^Y ¼ X , and the MLE is consistent, asymptotically
normal, and asymptotically efﬁcient, where n1=2
^Y  y


!
d N 0; y2


. Examine
the MLE of the variance of the exponential population distribution, q(y) ¼ y2. By
the invariance principle, the MLE is given by q(^Y) ¼ X
2 . Since q(y) is continuous
in y, X
2 is consistent for y2 by Theorem 8.22. Since ∂q(y)/∂y ¼ 2y 6¼ 0 and the
derivative is continuous 8 y > 0, q( ^Y) is asymptotically normal and asymptoti-
cally efﬁcient by Theorem 8.23, where X
2 
a N y2; 4y4=n


.
□
8.3.5
MLE Property Summary
Overall, the maximum likelihood procedure is a relatively straightforward
approach for deﬁning point estimates of Q or a myriad of functions of Q. The
procedure can sometimes lead to a MVUE for Q or q(Q), and very often in
practice, the MLE will possess good asymptotic properties. Speciﬁcally,
properties of a MLE that we have examined in this section include:
1. A MLE is not necessarily unbiased.
2. If an unbiased estimator of Q exists that achieves the CRLB, and if the MLE
is deﬁned by solving ﬁrst order conditions for ^Q, then the MLE will be
unique, unbiased, and achieve the CRLB.
3. If a MLE is unique, then the MLE is a function of any set of sufﬁcient
statistics, including complete sufﬁcient statistics if they exist.
4. If a MLE of Q is unique, and complete sufﬁcient statistics exist, then
(a) If the MLE is unbiased for Q, it is MVUE for Q;
(b) If a function of the MLE is unbiased for Q, then this function is the
MVUE for Q.
5. Under general regularity conditions on the estimation problem, the MLE is
consistent, asymptotically normal, and asymptotically efﬁcient.
6. If ^Q is the MLE of the (k  l) parameter vector Q, then q(^Q) is the MLE of the
(r  l) vector q(Q), r  k (MLE invariance principle).
8.3
The Method of Maximum Likelihood
495

7. Under general regularity conditions, if the MLE ^Q is consistent, asymptoti-
cally normal, and asymptotically efﬁcient for estimating Q, then the MLE
qð ^QÞ is also consistent, asymptotically normal, and asymptotically efﬁcient
for estimating q(Q).
8.4
Method of Moments and Generalized Method of Moments Estimation
Both the method of moments (MOM) and the Generalized Method of Moments
(GMM) approaches to point estimation begin with the speciﬁcation of a vector of
moment conditions that involve the random variables in a random sample, y,
and the value of the parameter vector in the probability model for the random
sample, {f(y;Q), Q ∈O}. The moment conditions take the general form
E g Y; Q
ð
Þ
ð
Þ ¼ 0, where it is understood that the expectation is taken with respect
to f(y;Q), i.e.,
R
y2Rn g y; Q
ð
Þ dF y; Q
ð
Þ ¼ 0. Both the MOM and GMM approaches to
estimation are based on sample-based estimates of the moment equations,
^E g Y; Q
ð
Þ
ð
Þ ¼ 0, in order to generate estimates of the parameters.
The traditional MOM approach, originally suggested long ago by Karl
Pearson,39 is based on iid sampling and moment conditions that are equal in
number to the number of unknown parameters. The moment conditions
are represented empirically via sample moments, and then the system of
estimated sample moments is solved for the parameters to produce a MOM
estimate. A general representation of the MOM estimate can be speciﬁed as
^Q ¼ argQ ^E g Y; Q
ð
Þ
ð
Þ ¼ 0
n
o
.
The GMM approach applies in more general settings where either sampling
is not necessarily iid, or the number of sample moment conditions is larger than
the number of unknown parameters, or both. If the number of moment
equations equals the number of unknown parameters, a corresponding system
of sample-based estimates of the moments are solved for the parameter values,
as in the MOM approach, producing a GMM estimate. If the number of moment
equations exceeds the number of unknown parameters, then the GMM estimate
is the value of the parameter vector that solves the sample moments as closely as
possible in terms of weighted squared Euclidean distance. A general representa-
tion of the GMM estimate can be speciﬁed as
^Q ¼ argmin
Q
^E g Y; Q
ð
Þ
ð
Þ
h
i0
W ^E g Y; Q
ð
Þ
ð
Þ
h
i
n
o
;
where W is some positive deﬁnite weighting matrix, which may also depend on
the sample size.
Details regarding implementation and estimator properties of the MOM and
GMM approaches are discussed ahead.
39Pearson, K.P. (1894), Contributions to the Mathematical Theory of Evolution, Phil. Trans. R. Soc. London A, 185: pp. 71–110.
496
Chapter 8
Point Estimation Methods

8.4.1
Method of Moments
The basic rationale underlying MOM estimation is rooted in the laws of large
numbers. If E(g(Yt,Q)) ¼ 0 deﬁnes a vector of k moment conditions involving the
(k  1) vector Q that hold 8t when Yt  iid f Y; Q
ð
Þ, and if a law of large numbers
applies to the random variables g(Yt,Q), t ¼ 1,. . .,n, then for a size n random
sample of data from f Y; Q
ð
Þ, n1 Pn
t¼1 g Yt; Q
ð
Þ !
p 0 (weak law) or n1 Pn
t¼1 g Yt; Q
ð
Þ
!
as 0 (strong law). Moreover, if in a given experiment, the population distribution
(unknown to the analyst) is actually given by f Y; Qo
ð
Þand E(g(Yt,Q)) ¼ j(Q) 6¼ 0
when Q 6¼ Qo so that n1 Pn
t¼1 g Yt; Q
ð
Þ ! jðQÞ in probability or almost surely,
then intuitively we would expect that solving n1 Pn
t¼1 g yt; Q
ð
Þ ¼ 0 for Q should
produce an estimate value, ^u, that becomes very close to Qo as n increases. If not,
the preceding convergence results would be ultimately contradicted. In fact
under appropriate regularity conditions, this is precisely true and the MOM
estimator ^Q converges in probability, and also almost surely, to Qo. It is also
possible that ^Q is asymptotically normal and efﬁcient based on an application of
central limit theory. Related arguments establishing asymptotic properties of
GMM estimators under iid sampling can be applied in the more general context
where the dimension of g exceeds Q, in which case some weighted squared
distance between n1 Pn
t¼1 g yt; Q
ð
Þ and 0 is minimized, as
^u ¼ argmin
Q
n1 X
n
t¼1
g yt; Q
ð
Þ
"
#0
W n1 X
n
t¼1
g yt; Q
ð
Þ
"
#
(
)
with W being positive deﬁnite. We examine details of the GMM case in
Section 8.4.2.
As an explicit illustration of the MOM approach, consider a statistical
model
for
a
random
sample
of
size
n
from
a
Bernoulli
population
distribution, so that the probability model is {f(y;p), p ∈O} where f y; p
ð
Þ ¼
p
Pn
t¼1 yi 1  p
ð
ÞnPn
t¼1 yt Qn
t¼1 I 0;1
f
g yt
ð
Þ and O ¼ [0,1]. A moment condition that
must hold for each yi when p ¼ po in this probability model is given by
E(g(Yt,po)) ¼ E(Yt  po) ¼ 0, t ¼ 1,. . .,n. The sample moment counterpart to
this moment condition is n1 Pn
t¼1 g yt; p
ð
Þ ¼ n1 Pn
t¼1 yt  p
ð
Þ ¼ 0, which upon
solving for p yields the MOM estimate ^p ¼ y . In this situation, the MOM
estimator is unbiased, BLUE, MVUE, consistent, asymptotically normal, and
asymptotically efﬁcient for estimating p.
MOM Implementation
In applying the MOM procedure, it is assumed that the
random sample Y1,. . .,Yn is a collection of iid random variables. We further
assume that the Yt0s are scalar random variables, although this is not necessary
to apply the general methodology. Given a probability model for the random
sample, {f(y;Q), Q ∈O}, our initial interest will focus on estimating Q. We have
seen in our study of parametric families of densities in Chapter 4 that the
moments of density functions are functions of the parameters that characterize
the parametric family. For example, in the exponential density case, E Yt
ð
Þ ¼ y
8.4
Method of Moments and Generalized Method of Moments Estimation
497

and E Y2
t


¼ 2y2, or in the normal density case, E Yt
ð
Þ ¼ m and E Y2
t


¼ s2 þ m2. In
general,
m0
r ¼ E Yr
t


¼ hr Q
ð
Þ , i.e., the rth moment about the origin for a
parametric family of densities f(y;Q), Q ∈O, is a function of the parameter
vector Q.
A typical MOM implementation for estimating the (k  1) vector Q is to
ﬁrst deﬁne a (k  1) vector of invertible moment conditions of the form
E g Yt; Q
ð
Þ
ð
Þ ¼ E
Yt  h1 Q
ð
Þ
Y2
t  h2 Q
ð
Þ
..
.
Yk
t  hk Q
ð
Þ
2
6664
3
7775
0
B
B
B
@
1
C
C
C
A ¼ 0; t ¼ 1; . . . ; n:
The sample moment counterparts to the moment conditions are then
speciﬁed as
n1 X
n
t¼1
g yt; Q
ð
Þ ¼
m01  h1 Q
ð
Þ
m02  h2 Q
ð
Þ
..
.
m0k  hk Q
ð
Þ
2
6664
3
7775 ¼ 0
and the solution for Q deﬁnes the MOM estimate via the inverse function h1 as
^yj ¼ h1
j
m01; m02; . . . ; m0k
ð
Þ; j ¼ 1; . . . ; k.
We emphasize that while it is typical in practice to use the ﬁrst k population
moments when deﬁning the needed moment conditions, other moments about
the origin, or even moments about the mean, can be used in deﬁning the
moment conditions. The key requirement is that the moment conditions be
invertible so that Q can be solved in terms of whatever sample moments are
utilized.
Note that an alternative motivation for the MOM estimate is to deﬁne the
sample moment conditions in terms of expectations taken with respect to the
empirical
distribution
function
of
the
yi0s,
and
then
solve
for
Q
as
^u ¼ argQ E^F g Y; Q
ð
Þ
ð
Þ ¼ 0

	
. Some examples of the procedure will illustrate its
relative simplicity.
Example 8.21
MOM Estimates of
Gamma Distribution
Parameters
Let (Y1,. . .,Yn) be a random sample from a gamma population distribution of
waiting times between customer arrivals in minutes, and suppose it is desired to
estimate the parameter vector (a,b). In this case the moment conditions can be
speciﬁed as,
E g Yt; a; b
ð
Þ
ð
Þ ¼ E
Yt  ab
Y2
t  ab2 1 þ a
ð
Þ




¼ 0;
t ¼ 1; . . . ; n:
498
Chapter 8
Point Estimation Methods

The sample moment counterpart to the moment conditions is given by
n1 X
n
t¼1
g yt; a; b
ð
Þ ¼
m01  ab
m02  ab2 1 þ a
ð
Þ


¼ 0
so that the MOM estimate is deﬁned by
^a
^b


¼
m01
ð
Þ2= m02  m01
ð
Þ2
h
i
m02  m01
ð
Þ2
h
i
=m01
2
4
3
5:
For example, if m01 ¼ 1:35 and m02 ¼ 2:7, then the MOM estimate is given by
^a ¼ 2:0769 and ^b ¼ :65.
□
Example 8.22
MOM Estimates of
Normal Distribution
Parameters
Let (Y1,....,Yn) be a random sample from a normal population distribution
representing the weights, in hundredweights, of steers fed a certain ration of
feed for 6 months, and suppose it is desired to estimate the parameter vector
(m,s2). The moment conditions in this case are
E g Yt; m; s2




¼ E
Yt  m
Y2
t  s2 þ m2






¼ 0; t ¼ 1; . . . ; n:
The sample moment counterpart to the moment conditions is given by
n1 X
n
t¼1
g yt; m; s2


¼
m01  m
m02  s2 þ m2




¼ 0;
so that the MOM estimate is deﬁned by
^m
^s2


¼
m01
m02  m01
ð
Þ2


:
For example, if m01 ¼ 12:3 and m02 ¼ 155:2, then the MOM estimate is given by ^m
¼ 12.3 and ^s2 ¼ 3:91.
□
MOM Estimator Properties
It is very often the case that the inverse function ^Q
¼ h1(M01,. . .,M0k ) is continuous, in which case the MOM estimator inherits
consistency from the consistency of the sample moments M0r for m0
r, 8r.
Theorem 8.24
Consistency of MOM
Estimator of Q
Let the MOM estimator ^Q k1
ð
Þ ¼ h1 M01; . . . ; M0k
ð
Þ be such that the inverse
function h1 m0
1; . . . ; m0
k
ð
Þ is continuous 8 m0
1; . . . ; m0
k
ð
Þ 2 G ¼
m0
1; . . . ; m0
k
ð
Þ : m0
i ¼
f
hi Q
ð
Þ; i ¼ 1; . . . ; k; Q 2 Og. Then ^Q !
p Q.
Proof
This is a direct consequence of Theorem 5.5 and the probability limit properties
of sample moments about the origin. In particular, given the assumed continuity
property of h1, plim ^Q


¼ h1 plim M01
ð
Þ; . . . ; plim M0k
ð
Þ
ð
Þ ¼ h1 m0
1; . . . ; m0
k
ð
Þ ¼
Q by the invertibility of h.
n
8.4
Method of Moments and Generalized Method of Moments Estimation
499

Example 8.23
MOM Consistency for
Gamma and Normal
Distribution Parameters
In Example 8.21, (a,b) is a continuous function of m0
1 and m0
2 for all m0
1; m0
2
ð
Þ 2 G ¼
m0
1; m0
2
ð
Þ : m0
1>0; m0
2>0; m0
2> m0
1
ð
Þ2
n
o
, which constitutes all relevant values of
m0
1; m0
2
ð
Þ . (Recall s2 ¼ m0
2  m0
1
ð
Þ2 ¼ ab2>0 in the Gamma family.) Thus by
Theorem 8.23, the MOM estimator of (a,b) is consistent. In Example 8.22,
(m,s2) is a continuous function of m0
1; m0
2
ð
Þ so that the MOM estimator of (m,s2)
is consistent.
□
In addition to consistency, the MOM estimator will have an asymptotic
normal density if the inverse function Q ¼ h1 m0
1; . . . ; m0
k
ð
Þ is continuously
differentiable and its Jacobian matrix has full rank.
Theorem 8.25
Asymptotic Normality
of MOM Estimator of Q
Let the MOM estimator ^Q ¼ h1 M01; . . . ; M0k
ð
Þ be such that h1 m0
1; . . . ; m0
k
ð
Þ is
differentiable 8 m0
1; . . . ; m0
k
ð
Þ ∈G ¼ { m0
1; . . . ; m0
k
ð
Þ:m0
i ¼ hi(Q), i ¼ 1,. . .,k, Q ∈O},
and let the elements of
A m0
1; . . . ; m0
k
ð
Þ ¼
@h1
1
m0
1; . . . ; m0
k
ð
Þ
@m01
. . .
@h1
1
m0
1; . . . ; m0
k
ð
Þ
@m0k
..
.
..
.
..
.
@h1
k
m0
1; . . . ; m0
k
ð
Þ
@m01

 
 
@h1
k
m0
1; . . . ; m0
k
ð
Þ
@m0k
2
666664
3
777775
be continuous functions with A m0
1; . . . ; m0
k
ð
Þ having full rank 8 m0
1; . . . ; m0
k
ð
Þ ∈G.
Then
n1=2
^Q  Q


!
d N 0; ASA0
ð
Þ; and ^Q 
a N Q; n1ASA0


; where S ¼ Cov M0
1; . . . ;
ð
M0kÞ:
Proof
Recall that the sample moments converge to a normal limiting distribution as
n1=2
M01
..
.
M0k
2
64
3
75 
m0
1
..
.
m0
k
2
64
3
75
2
64
3
75!
d N 0; S
ð
Þ:
Then since the partial derivatives contained in A are continuous and since A has
full rank, the result follows directly from Theorem 5.40.
n
Example 8.24
MOM Asymptotic
Normality for Gamma
and Normal
Distribution Parameters
Reexamine Examples 8.21 and 8.22. In Example 8.21,
A m0
1; m0
2
ð
Þ ¼
2m0
2m0
1
m02  m01
ð
Þ2
h
i2
 m0
1
ð
Þ2
m02  m01
ð
Þ2
h
i2
 m0
2 þ m0
1
ð
Þ2
h
i
m01
ð
Þ2
1
m01
ð
Þ
2
666664
3
777775
;
500
Chapter 8
Point Estimation Methods

which exists with continuous elements 8 m0
1; m0
2
ð
Þ 2 G . Furthermore, A has
full rank, because det(A) ¼
m0
2  m0
1
ð
Þ2

1
>0 8 m0
1; m0
2
ð
Þ 2 G . Therefore, by
Theorem 8.25, the MOM estimator of (a,b) is asymptotically normally distributed.
In Example 8.22,
A m0
1; m0
2
ð
Þ ¼
1
0
2m0
1
1


;
which exists with continuous elements and has full rank 8 m0
1; m0
2
ð
Þ so that the
MOM estimator is asymptotically normally distributed by Theorem 8.25.
□
The reader should be warned that although the MOM estimator of Q is
conceptually straightforward to deﬁne, and is often quite straightforward to
compute, in some cases the inverse function may be difﬁcult to deﬁne explicitly,
if an explicit representation exists at all. A computer algorithm for solving
systems of nonlinear equations might be needed to compute the inverse func-
tion values and thereby calculate outcomes of the MOM estimator. As an
example of more complicated cases, the reader should consider MOM estima-
tion of the parameter vector Q when sampling from a Beta family of densities.
MOM estimators for a function of Q, say q(Q), can be deﬁned by the
corresponding function of the MOM estimator of Q, as q ^Q


¼ q h1 M01;
ð

. . . ;
M0kÞÞ. Under appropriate conditions on the function q, and if the MOM estima-
tor of Q satisﬁes the conditions of Theorems 8.24 and 8.25, then q ð ^QÞ is
consistent and asymptotically normal as an estimator of q(Q).
Theorem 8.26
Consistency and
Asymptotic Normality
of MOM Estimator
of q(Q)
Let the MOM estimator ^Q ¼ h1 M01; . . . ; M0k
ð
Þ of Q satisfy the conditions of
Theorems 8.24 and 8.25. If the function q(Q) is continuous for Q ∈V, then the
MOM estimator q( ^Q) is consistent for q(Q). If @q Q
ð
Þ=@Q0 exists, has full row
rank, and its elements are continuous functions of Q for Q ∈O,
n1=2 q ^Q


 q Q
ð
Þ


!
d N 0; @q Q
ð
Þ0
@Q
F @q Q
ð
Þ
@Q


and
q ^Q



a N q Q
ð
Þ; n1 @q Q
ð
Þ0
@Q
F @q Q
ð
Þ
@Q


;
where F is the covariance matrix of the limiting distribution of n1/2( ^Q  Q) in
Theorem 8.25.
Proof
The proof is analogous to the proofs of Theorem’s 8.24 and 8.25 and is left to the
reader.
n
8.4
Method of Moments and Generalized Method of Moments Estimation
501

Overall, the MOM approach to point estimation of Q or q(Q) is conceptually
simple, computation of estimates is often straightforward, and the MOM esti-
mator is consistent and asymptotically normal under fairly general conditions.
However, the MOM estimator is not necessarily unbiased, BLUE, MVUE, or
asymptotically efﬁcient. It has been most often applied in cases where the
deﬁnition or computation of other estimators, such as the MLE, are extremely
complex, or when the sample size is quite large so that the consistency of the
estimator can be relied upon to assure a reasonably accurate estimate. Further
discussion of asymptotic efﬁciency of MOM estimators will be undertaken in
the next subsection in the context of GMM estimators.
8.4.2
Generalized Method of Moments (GMM)
In the case of the GMM, the random sample is not restricted to iid random
variables, and the moment conditions can be greater in number than the number
of parameters being estimated. However, the general estimation principles
remain the same, namely, moment conditions E g Y; Q
ð
Þ
ð
Þ ¼ 0 are speciﬁed
pertaining to the probability model {f(y;Q), Q ∈O}, sample moment counterparts
are speciﬁed as ^E g Y; Q
ð
Þ
ð
Þ ¼ 0, and then the GMM estimate is deﬁned as the value
of Q that satisﬁes the sample moment conditions as closely as possible—exactly
if g is the same dimension as Q.
The following two examples demonstrate how the GMM estimator can be
formulated to subsume the least squares estimator and the MLE as special cases.
Example 8.25
Least Squares Estimator
as a GMM Estimator
Let the probability model of a random sample of the yields/acre of a particular
type of corn be (incompletely) speciﬁed by the linear model Y ¼ xb + «, where
the classical GLM assumptions are assumed to apply and b is a (k  1) vector of
parameters indicating the responsiveness of yield to various inputs. Moment
conditions for the statistical model can be speciﬁed by the (k  1) vector
function
E g Yt; b
ð
Þ
ð
Þ ¼ E x0
t: Yt  xt:b
ð
Þ
ð
Þ ¼ E x0
t:et
ð
Þ ¼ 0; t ¼ 1; . . . ; n;
which reﬂect the orthogonality of the residuals from the explanatory variables.
Corresponding sample moment conditions can be deﬁned as
n1 X
n
t¼1
g yt; b
ð
Þ ¼ n1 X
n
t¼1
x0
t: yt  xt:b
ð
Þ ¼ n1 x0y  x0xb
½
	 ¼ 0;
which when solved for b deﬁnes the GMM estimate as b ¼ x0x
ð
Þ1x0y. Thus, the
least squares estimator is a GMM estimator.
□
Example 8.26
MLE as a GMM
Estimator
Let {f(y;Q),Q ∈V} be the probability model for a random sample for which the
Yi0s are not necessarily iid, and represent the joint density of the random sample
as f(y;Q) ¼ m(y1;Q) Qn
t¼2 f ytjyt1; . . . ; y1; Q
ð
Þ. Assume that differentiation under
502
Chapter 8
Point Estimation Methods

the integral or summation sign is possible (recall CRLB regularity condition (4)
of Deﬁnition 7.21) so that moment conditions can be deﬁned as40
E g Y1; Q
ð
Þ
ð
Þ ¼ E @ ln m Y1; Q
ð
Þ
ð
Þ
@Q


¼ 0;
E g Yt; Q
ð
Þ
ð
Þ ¼ E @ ln f Ytjyt1; . . . ; y1; Q
ð
Þ
ð
Þ
@Q


¼ 0 for t ¼ 2; . . . ; n
Then specify the sample moment conditions as
n1 X
n
t¼1
g yt; Qo
ð
Þ ¼ n1 @ ln m y1; Q
ð
Þ
ð
Þ
@Q
þ
Xn
t¼2
@ ln f ytjyt1; . . . ; y1; Q
ð
Þ
ð
Þ
@Q


¼ n1 @ ln L Q; y
ð
Þ
ð
Þ
@Q
¼ 0;
which when solved for Q deﬁnes the GMM estimate ^u ¼ argQ @ ln L Q; y
ð
Þ
ð
Þ=
f
@Q ¼ 0g. Thus assuming the MLE of Q is deﬁned as the solution to ﬁrst order
conditions, the MLE is a GMM estimator.
□
The reader will come to ﬁnd in her later studies of econometrics or statistics
that the GMM subsumes a large number of the estimation procedures used in
practice. For example, instrumental variable techniques, two and three-stage
least
squares,
and
quasi-
or
pseudo-maximum
likelihood
estimation
techniques can all be interpreted as GMM procedures. The interested reader is
referred to A.R. Gallant, (1987), Nonlinear Statistical Models, NY, John Wiley,
Chapter 4, and to R. Mittelhammer, G. Judge, and D. Miller, (2000), Econometric
Foundations, Cambridge University Press, Cambridge, Chapters 16 and 17, for
additional details on applying the GMM approach to more general statistical
models.
It is possible to apply the GMM procedure in cases where the vector function
deﬁning the moment conditions is of larger dimension than the dimension of the
vector Q. In these cases the GMM estimator is deﬁned as the value of Q that
minimizes a weighted measure of the squared distance between the sample
moments and the zero vector, where the weights can depend on the sample size, as
^Q ¼ arg minQ2V
^E g Y; Q
ð
Þ
ð
Þ
h
i0
Wn y
ð Þ ^E g Y; Q
ð
Þ
ð
Þ
h
i
n
o
:
The matrix Wn(y) will be a symmetric, positive deﬁnite conformable weighting
matrix which may or may not depend on y, and which is such that Wn !
p
W
with
W
being
a
nonrandom,
symmetric,
and
positive
deﬁnite
matrix.
40Under the assumed conditions, for each density (conditional or not),
EQo
@ ln f Z; Qo
ð
Þ
@Q


¼
Z 1
1
@f z; Qo
ð
Þ
@Q
dz ¼ @
R 1
1 f z; Qo
ð
Þdz
@Q
¼ 0
in the continuous case, and likewise in the discrete case.
8.4
Method of Moments and Generalized Method of Moments Estimation
503

The previous case where g and Q are of equal dimension is subsumed within the
current context because in this case the minimum of the distance measure
occurs at the value of Q for which ^E g Y; Q
ð
Þ
ð
Þ ¼ 0 . The GMM estimator is
consistent and asymptotically normal quite generally with respect to choices
of the weighting matrix, although asymptotic efﬁciency considerations require a
speciﬁc choice of Wn. These issues will be considered in the next section.
General GMM Estimator Properties
As was the case for the MOM procedure,
general statements regarding the properties of GMM estimators are relegated
to the asymptotic variety. We will examine a set of sufﬁcient conditions for the
consistency and asymptotic normality of the GMM estimator, and a sufﬁcient
condition for asymptotic efﬁciency within the class of estimators based on a
particular set of moment conditions. The reader can refer to Gallant, op. cit., and
L.P. Hansen, (1982), “Large Sample Properties of Generalized Method of
Moments Estimators,” Econometrica, pp. 1029–1054 for alternative conditions
that lead to asymptotic properties of the GMM estimator. Throughout the
remainder of our discussion we will be referring to the GMM estimator deﬁned
in terms of the minimization of the weighted squared distance measure
presented at the end of the previous section, which subsumes the case where
the dimension of g and Q are equal.
GMM Consistency
Sufﬁcient conditions under which the GMM estimator is
consistent are given below.
Theorem 8.27
Consistency of GMM
Estimator
Let {f(y;Q), Q ∈O} be the probability model for the random sample Y, where Q
is (k  1). Let E(gt(Yt,Q)) ¼ 0 be an (m  1) moment condition for t ¼ 1,. . .,n
with m  k. Deﬁne the GMM estimator
^Q ¼ arg minQ2V Qn Y; Q
ð
Þ
f
g ¼ arg minQ2V
n1 X
n
t¼1
gt Yt; Q
ð
Þ
"
#0
Wn Y
ð Þ n1 X
n
t¼1
gt Yt; Q
ð
Þ
"
#
(
)
where plim Wn Y
ð Þ
ð
Þ ¼ W , a nonrandom positive deﬁnite symmetric matrix.
Suppose that Qo is the true parameter vector such that Y ~ f(y; Qo) and assume
that
a. V is a closed and bounded rectangle,
b. ^Q is unique,
c. plim n1 P
n
t¼1
gt Yt; Q
ð
Þ


¼ G Q
ð
Þ; a continuous nonstochastic (m  1) vector
function of Q for which G(Q) ¼ 0 iff Q ¼ Qo,
d. limn!1 P maxY2OjQn y; Q
ð
Þh Q
ð
Þj<e
ð
Þ
ð
Þ¼1; 8e>0;where h(Q) ¼ G(Q)0W G(Q).
Then ^Q !
p Qo.
Proof
See Appendix.
n
In practice, when the GMM estimator is represented by an explicit function
of the random sample, it is often more straightforward to verify consistency by
504
Chapter 8
Point Estimation Methods

taking probability limits directly. Sufﬁcient conditions, such as those in Theo-
rem 8.26, are generally needed when the GMM estimator is only an implicit
function of the random sample. We illustrate both approaches to verifying
consistency in the following example.
Example 8.27
Consistency of GMM
Instrumental Variables
Estimator
Let the aggregate demand for a given commodity be approximated by some linear
function Y ¼ Xb + V for some appropriate value of b, where the Vt0s are iid with
E(Vt) ¼ 0 and var(Vt) ¼ s2, 8t. Suppose that the (n  k) matrix X contains factors
affecting demand, including some commodity prices that are simultaneously deter-
mined with quantity demanded, Y, so that E((X0X)1 X0V) 6¼ 0 (recall Table 8.2).
Thus, the OLS estimator of b, ^b ¼ X0X
ð
Þ1X0Y, will be biased and inconsistent.
Suppose there existed a conformable (n  k) matrix of variables, Z, (called
instrumental variables in the literature) such that
E gt Yt; b
ð
Þ
ð
Þ ¼ E Z0
t: Yt  Xt:b
ð
Þ
ð
Þ ¼ E Z0
t:Vt
ð
Þ ¼ 0 for t ¼ 1; . . . ; n;
where also n1 Z0V !
p 0 , n1 Z0Z !
p Azz, a ﬁnite positive deﬁnite symmetric
matrix, and n1 Z0X !
p Azx, a ﬁnite nonsingular square matrix. The sample
moment conditions are speciﬁed by
n1 X
n
t¼1
gt yt; b
ð
Þ ¼ n1 X
n
t¼1
zt: yt  xt:b
ð
Þ ¼ n1½z0y  z0xb	 ¼ 0:
Assuming that z0x is nonsingular, the sample moment conditions have a unique
solution deﬁning the GMM estimate as ^b ¼ z0x
ð
Þ1z0y, which is also referred to
in the literature as the instrumental variables estimator. Thus condition (b) of
Theorem 8.27 is satisﬁed.
Letting bo denote the true (and unknown) value of the parameter vector, so
that Y ¼ Xbo þ V is the correct linear model, note that
plim n1 X
n
t¼1
gt Yt; b
ð
Þ
"
#
¼ Azx bo  b
½
	 ¼ GðbÞ;
which equals 0 iff b ¼ bo . Thus condition (c) of Theorem 8.27 is satisﬁed.
Now assume that O is a closed and bounded rectangle, so that the GMM
estimate can be represented equivalently as
^b ¼ arg minb Qn y; b
ð
Þ
f
g ¼ arg minb2O n1 z0y  z0xb
½
	0I z0y  z0xb
½
	n1


:
Note that maxb2OjQn y; b
ð
Þ  h b
ð Þj will exist by Weierstrass’s theorem, where
h b
ð Þ ¼ bo  b
ð
Þ0A0
zxAzx bo  b
ð
Þ, and the maximum is a continuous function of
the elements in n1z0y and n1z0x by the theorem of the maximum. It
follows that
plim max
b2O
jQn Y; b
ð
Þ  h b
ð Þj
f
g


¼ max
b2O
jp lim Qn Y; b
ð
Þ
ð
Þ  h b
ð Þj
f
g ¼ 0;
so that condition (d) of Theorem 8.27 is satisﬁed. Therefore, ^b !
p
bo.
8.4
Method of Moments and Generalized Method of Moments Estimation
505

Given the explicit functional deﬁnition of ^b, the consistency of the GMM
estimator can be proven more directly by noting that
plim ^b

 
¼ plim n1Z0X

1 n1Z0Y


;
¼ plim n1Z0X

1plim n1 Z0Xbo þ Z0V
ð
Þ


¼ A1
zx Azxbo
ð
Þ ¼ bo
where n1Z0V !
p
0.
□
Asymptotic Normality and Efﬁciency in the GMM Class
Under additional
assumptions relating to the moment conditions, such as those in the theorem
below, the GMM estimators are asymptotically normally distributed.
Theorem 8.28
Asymptotic Normality
of GMM Estimator
Assume41 the conditions of Theorem 8.27. In addition, assume that
a. @2Qn y; Q
ð
Þ=@Q@Q0 exists and is continuous 8Q ∈V;
b. For any sequence fQ
ng for which
plim Q
n


¼ Qo; @2Qn Y; Q
n


=@Q@Q0 !
p D Qo
ð
Þ;
a ﬁnite symmetric nonsingular matrix; and
c. n1=2 @Qn Y; Qo
ð
Þ=@Q
ð
Þ!
d Z  N 0; C Qo
ð
Þ
ð
Þ.
Then n1=2
^Q  Qo


!
d N 0; D Qo
ð
Þ1C Qo
ð
ÞD Qo
ð
Þ1


, and
^Q 
a N Qo; n1D Qo
ð
Þ1C Qo
ð
ÞD Qo
ð
Þ1


:
Proof
By a ﬁrst-order Taylor series representation,
@Qn Y; ^Q


@Q
¼ @Qn Y; Qo
ð
Þ
@Q
þ @2Qn Y; Q
ð
Þ
@Q@Q0
^QQo


where Q* ¼ l ^Q + (1  l)Qo and l ∈[0,1].42 By the ﬁrst order conditions to the
minimization problem deﬁning the GMM estimator, @Qn y; ^Q


=@Q ¼ 0, so that
by Slutsky’s theorems and the consistency of ^Q
n1=2
^Q  Qo


¼  @2Qn Y; Q
ð
Þ
@Q@Q0
"
#1
n1=2 @Qn Y; Qo
ð
Þ
@Q
!
d  D Qo
ð
Þ1Z  N 0; D Qo
ð
Þ1C Qo
ð
ÞD Qo
ð
Þ1


:
and the asymptotic distribution result follows directly.
n
41See T. Amemiya, op. cit., pp. 111–112 for a closely related theorem relating to extremum estimators. The assumptions of Theorem
8.26 can be weakened to assuming uniqueness and consistency of ^Q for use in this theorem.
42We are suppressing the fact that each row of the matrix @2QnðY; QÞ=@Q@Q0 will generally require a different value of Q* deﬁned by a
different value of l for the representation to hold. The conclusions of the argument will remain the same.
506
Chapter 8
Point Estimation Methods

Example 8.28
Asymptotic Normality
of GMM Instrumental
Variables Estimator
Revisit the instrumental variable estimator of Example 8.27. Note that
@2Qn y; b
ð
Þ=@b@b0 ¼ 2n2x0zz0x; which exists and is (trivially) continuous for b
2 O. Also, @2Qn Y; b
ð
Þ=@b@b0 ¼ 2 n1X0Z


Z0Xn1


!
p D bo
ð
Þ ¼ 2A0
zxAzx; which
is a ﬁnite nonsingular matrix regardless of the sequence {b} for which b !
p bo.
Finally, note that
n1=2@Qn Y; bo
ð
Þ=@b ¼ 2 n1X0Z


n1=2Z0V


!
d
 2A0
zxT  N 0; 4s2A0
zxAzzAzx


where
n1=2Z0V!
d T  N 0; s2Azz


by an application of the multivariate
Lindberg-Levy CLT, assuming that Xi:; Zi:; Vi
½
	 are iid (or else, a different central
limit theorem would be used). Then from Theorem 8.28,
n1=2 ^b  bo


!
d N 0; s2 Azx
ð
Þ1Azz A0
zx
ð
Þ1


; and ^b 
a N bo; s2=n


Azx
ð
Þ1Azz Azx
ð
Þ1


:
An estimate of the asymptotic covariance matrix can be deﬁned as
^s2 z0x
ð
Þ1z0z x0z
ð
Þ1; where ^s2 ¼ n1 y  x^b

0
y  x^b


:
Regarding the efﬁciency of the GMM estimator, L.P. Hansen, Moments
Estimators, has provided conditions that characterize the optimal choice of
Wn(y) in the deﬁnition of the GMM estimator
^bWn ¼ arg minQ
n1 X
n
t¼1
gt Yt; Q
ð
Þ
"
#0
Wn Y
ð Þ n1 X
n
t¼1
gt Yt; Q
ð
Þ
"
#
(
)
:
In particular, setting Wn(Y) equal to the inverse of the asymptotic covariance
matrix of n1=2 Pn
i¼1 gt Yt; Q
ð
Þ is the choice that deﬁnes the GMM estimator that
is asymptotically most efﬁcient relative to all choices of the weighting matrix.
Note that this result establishes asymptotic efﬁciency within the class of GMM
estimators based on a particular set of moment conditions. Results relating to the
optimal choice of moment conditions with which to deﬁne the GMM estimator
in the general case are not available, although a number of special cases have been
investigated. The interested reader can consult MacKinnon and Davidson, Esti-
mation and Inference, Chapter 17 for additional reading and references.
Example 8.29
Asymptotic Efﬁciency of
GMM Instrumental
Variables Estimator
Revisit Example 8.28 concerning the instrumental variable estimator. Note that
n1=2 X
n
t¼1
gt Yt; bo
ð
Þ ¼ n1=2 Z0Y  Z0Xbo
½
	 ¼ n1=2 Z0V
½
	!
d N 0; s2Azz


;
as presented in Example 8.28. Thus, the optimal GMM estimator, which is based
on Wn(Y) ¼ (s2Azz)1, is deﬁned by
^b ¼ arg minb n2 Z0Y  Z0Xb
ð
Þ0 s2Azz

1 Z0Y  Z0Xb
ð
Þ
n
o
¼ Z0X
ð
Þ1Z0Y:
□
8.4
Method of Moments and Generalized Method of Moments Estimation
507

In practice, if the solution for ^Q depends on a weighting matrix that contains
unknown elements (in Example 8.29, the fact that Azz was unknown is irrele-
vant—only its positive deﬁniteness mattered), an estimator that converges in
probability to the asymptotically efﬁcient GMM estimator can be constructed
using any weighting matrix that converges in probability to the appropriate
asymptotic covariance matrix of n1=2 Pn
t¼1 gt Yt; Q
ð
Þ.
Also, unlike Example 8.29, if the number of instrumental variables, and thus
equivalently the number of moment conditions, exceeds the number of
parameters, the weight matrix will appear in the solution for the GMM estima-
tor, as the following example illustrates.
Example 8.30
Asymptotic Efﬁciency of
GMM Two-Stage Least
Squares Estimator
Revisit Examples 8.27–8.29, except assume that the instrumental variable
matrix Z is m  k, with m > k. Following arguments analogous to those used
in the preceding examples, the moment conditions would be represented by
n1 X
n
t¼1
gt yt; b
ð
Þ ¼ n1 X
n
t¼1
zt: yt  xt:b
ð
Þ ¼ n1½z0y  z0xb	 ¼ 0:
To pursue an optimal GMM estimation approach in this case, recall that
n1=2 X
n
t¼1
gt Yt; bo
ð
Þ ¼ n1=2 Z0Y  Z0Xbo
½
	 ¼ n1=2 Z0V
½
	!
d N 0; s2Azz


where nowAzz ¼ plim n1Z0Z


is
m  m
ð
Þ, and thus the optimal weighting
matrix in the GMM procedure is given by W ¼ s2A1
zz . Note that the scalar s2
can be ignored since it will have no effect on the choice of b that would
minimize the weighted squared distance measure that deﬁnes the GMM. The
remaining matrix component of the optimal weight matrix is not observable,
but a consistent estimator of it is given by ^A1
zz ¼ n1Z0Z

	1.
Suppressing the n value in the estimated optimal weighting matrix (which is
irrelevant to the minimization problem), the estimated optimal GMM estimator
is ﬁnally represented by
^bGMM ¼ arg minb
n1Z0 Y  Xb
ð
Þ

	0 Z0Z
½
	1 n1Z0 Y  Xb
ð
Þ

	
n
o
:
Differentiating the objective function with respect to b and then solving
the resultant ﬁrst order conditions, it is found that the optimal GMM estimator
in this case of more instrumental variables, and thus more moment conditions,
than unknown parameters is equivalent to the so-called Two Stage Least Squares
Estimator found in the econometrics literature. The solution is given by
^bGMM ¼ ^b2SLS ¼ X0Z Z0Z
½
	1Z0X
h
i1
X0Z Z0Z
½
	1Z0Y.
□
508
Chapter 8
Point Estimation Methods

8.5
Appendix: Proofs and Proof References for Theorems
Theorem 8.6
Proof
a. Under the stated conditions, the sequence of squared residuals {e2
n} ¼ {e2
1,e2
2,. . .}
satisﬁes the assumptions of the WLLN stated in Theorem 5.22. To see this,
note that
varðe2
i Þ ¼ E e4
i


 E e2
i



2 ¼ E e4
i


 s4  t<1 8i;
so that
varðn1«0«Þ ¼ n2
S
n
i¼1 varðe2
i Þ þ S
i6¼j Covðe2
i ; e2
j Þ



t þ oðn1Þ

	
=n ! 0
as n ! 1. It follows immediately that plim ðn1«0«Þ ¼ s2. Also, given the
Assumptions
of
the
Classical
GLM,
it
can
be
shown
that
plim
ðn  k Þ1 «0xðx0x Þ1 x0«


¼ 0 using Markov’s inequality in exactly the
same way as in the proof of Theorem 8.5. It follows that
plim ^S
2


¼ plim n1«0«


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
s2
 plim
n  k
ð
Þ1«0x x0x
ð
Þ1«
h
i
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
0
¼ s2:
b. The conditions in part (b) imply the conditions in part (a), since cov(e2
i,e2
j) ¼ 0
when i  j
j
j>m by m-dependence. Thus, ^S
2 !
p s2.
n
Theorem 8.8
Proof
The proof is based on the Liapounov CLT for triangular arrays (Theorem 5.34)
and the Cramer-Wold device (Corollary 5.4). Deﬁne Vn ¼ n1 x0x and note that
x0x
ð
Þ1=2 ^b  b


¼ n1=2V1=2
n
x0« ¼ n1=2 X
n
t¼1
V1=2
n
x0
t:et:
Examine
ℓ0 x0x
ð
Þ1=2 ^b  b


¼ n1=2 X
n
t¼1
ℓ0V1=2
n
x0
t:et ¼ n1=2 X
n
t¼1
Wnt
where ℓis any conformable vector such that ℓ0ℓ¼ 1. Note that E(Wnt) ¼ 0 and
var(Wnt) ¼ s2 ℓ0 V1=2
n
x0t: xt:V1=2
n
ℓ<1 8 n and t, where the ﬁniteness of var(Wnt)
follows from the boundedness of E( W4
nt ), which is shown below. Because
E(Wnt) ¼ 0, it follows that E( Wnt  EðWntÞ
j
j4) ¼ E(W4
nt), and then
EðW4
ntÞ ¼ E e4
i


ℓ0V1=2
n
x0
t: xt:V1=2
n
ℓ
h
i2
<t ℓ0V1=2
n
x0
t: xt:V1=2
n
ℓ
h
i2
:
Now note that ℓ0V1=2
n
x0t: xt:V1=2
n
ℓ< c < 1 8n and t. This follows from the fact
that Vn
j
j > Z > 0 8n with {Vn} being 0(1) ) {V1=2
n
} is O(1) (H. White, (1982),
8.5
Appendix: Proofs and Proof References for Theorems
509

“Instrumental
Variables
Regression
with
Independent
Observations,”
Econometrica, pp. 484–485), and since x0t:xt is a positive semideﬁnite matrix
all of whose elements are bounded in absolute value by x2 , it follows that
V1=2
n
x0t:xt:V1=2
n
is a O(1) positive semideﬁnite matrix, so thatℓ0V1=2
n
x0t:xt:V1=2
n
ℓ
is bounded as claimed. Then E W4
nt


<tc2 ¼ g<1 8n and t.
Given the preceding results, Theorem 5.34 is applicable. Note that the Wnt0s
can be represented in the form of a triangular array (Deﬁnition 5.9) with typical
row (Wn1,. . .,Wnn), 8n, and the Wnt0s are independent within rows since the et0s
are independent. The limit condition of Theorem 5.34 is met, since
0  lim
n!1
P
n
t¼1
E W4
nt


P
n
t¼1
Var Wnt
ð
Þ

2
2
6664
3
7775  g lim
n!1
n
n2s4
h
i
¼ 0:
Then because n1 Pn
t¼1 Var(Wnt) ¼ s2, it follows from Theorem 5.34 that
ℓ0 x0x
ð
Þ1=2 ^b  b


¼ n1=2 n1 X
n
t¼1
Wnt
"
#
!
d N 0; s2


;
and
by
the
Cramer-Wold
device
x0x
ð
Þ1=2 ^b  b


!
d Nð0; s2IÞ ,
so
that
^b 
a N b; s2 x0x
ð
Þ1


.
n
Theorem 8.10
Proof
Similar to the proof of Theorem 8.9, we can show that the limiting distributions
of n1=2 ^S
2  s2


=x1=2
n
and of Pn
i¼1 e2
i  ns2


= n1=2x1=2
n


coincide. To see this, note
by Markov’s inequality that
P
n1=2
n  k
ð
Þx1=2
n
«0x x0x
ð
Þ1x«  c
 
!

s2n1=2k
cðn  kÞx1=2
n
;
and if xn   > 0, 8n, then the right hand side (RHS) of the inequality ! 0 as
n ! 1, so that plim n1=2=
n  k
ð
Þx1=2
n




«0x x0x
ð
Þ1x0« ¼ 0. The remaining steps
for showing the equivalence of the limiting distributions are analogous to those
in the proof of Theorem 8.9.
Liapounov’s CLT (Theorem 5.33) can now be applied to
X
n
i¼1
e2
i  ns2
 
!
= n1=2x1=2
n


:
Note that the e2
i
0s are independent, with E e2
i


¼ s2 and var e2
i


¼ E
e2
i  s2

2


<1 8i (the existence of var e2
i


follows from the existence of E je2
i  s2 j2þd


for
d > 0; recall Theorem 3.23). Furthermore, the limit condition of the Liapounov
CLT is met, because
510
Chapter 8
Point Estimation Methods

lim
n!1
Pn
i¼1 E je2
i  s2 j2þd


Pn
i¼1 var e2
i



	1þd=2
 lim
n!1
nt
n
ð
Þ1þd=2
 
!
¼ lim
n!1 t= 1þd=2


nd=2 ¼ 0
Therefore, by the Liapounov CLT,
X
n
i¼1
e2
i  ns2
 
!
= n xn
ð
Þ1=2!
d Nð0; 1Þ;
which in turn implies that
n1=2 ^S
2  s2


x1=2
n
!
d N 0; 1
ð
Þ and ^S
2 
a N s2; n1xn


:
n
Theorem 8.17
Proof
Sufﬁciency of 1–4a: Deﬁne H(«) ¼ {x: ln(L(Qo;x)) > maxQ2N eð Þ (L(Q;x))}, and note
that x ∈H(e) ) ^Q ¼ arg maxQ2O (L(Q;x)) ∈N(e), where
^Q is unique by 3.
Assumption (4a) implies that P(x ∈H(e)) ⟶1 as n ⟶1 8e > 0, which in
turn implies that P( ^Q ∈N(e)) ⟶1 as n ⟶1 8e > 0. It follows from the
deﬁnition of N(e) that ^Q!
p Qo.
Sufﬁciency of 1–3, 4b:43 Deﬁne x(e) ¼ G Qo)  maxQ2O\N eð Þ(G(Q)) (because O is
closed and bounded, the feasible space to the maximization problem is closed
and bounded by the deﬁnition of N(e), and thus the maximum exists by
Weierstrass’s theorem). Let the event An(e) be deﬁned as
An eð Þ ¼
x : maxQ2O jn1 ln LðQ; xÞ
ð
Þ  GðQÞj


<xðeÞ=2


:
Letting ^Q represent the unique MLE, it follows from x ∈An(e) that
1. G( ^Q) > n1 ln(L( ^Q;x))  x(e)/2
2. n1 ln(L(Qo;x)) > G(Qo)  x(e)/2.
By deﬁnition of the MLE, L( ^Q;x)  L(Qo;x), so it follows from (1) that
3. G( ^Q) > n1 ln(L(Qo;x))  x(e)/2.
Then substituting the inequality (2) into the right side of the inequality in (3)
yields
4. G( ^Q) > G(Qo)  x(e),
which upon substituting the deﬁnition of j(«) yields
5. G( ^Q) > maxQ2O\N eð Þ(G(Q)) ) ^Q ∈N(«).
Then since
limn!1P An eð Þ
ð
Þ ¼ 1 8e>0; limn!1P ^Q 2 N eð Þ


¼ 1 8e>0; so that ^Q !
p Qo.n
43This proof is related to a proof by T. Amemiya,(1985), Advanced Econometrics, Harvard University Press, p. 107, dealing with the
consistency of extremum estimators.
8.5
Appendix: Proofs and Proof References for Theorems
511

Theorem 8.18
Proof
The ﬁrst order derivative function of ln(L( ^Q;x)) can be represented by a Taylor
series expansion around Qo as44
@ ln L ^Q; X




@Q
¼ @ ln L Qo; X
ð
Þ
ð
Þ
@Q
þ @2 ln L Q; X
ð
Þ
ð
Þ
@Q@Q0
^Q  Q


;
where Q ¼ l X
ð Þ ^Q þ 1  l X
ð Þ
ð
ÞQo for l(X) ∈[0,1]. Since ^Qn is the ML estimator,
the value of the ﬁrst derivative vector, @ ln L ^Q; X




=@Q, is 0 by the ﬁrst order
conditions for the maximum likelihood problem. Then premultiplying the
Taylor series expansion by n1=2 obtains
n1@2 ln L ^Q
; X




@Q@Q0
n1=2
^Q  Qo


¼ n1=2 @ ln L Qo; X
ð
Þ
ð
Þ
@Q
¼ Z!
d N 0; M Qo
ð
Þ
ð
Þ
where convergence to theN 0; M Qo
ð
Þ
ð
Þlimiting density follows from assumption
(3) of the theorem.
Now note that plim Q
ð
Þ ¼ Qo since 8e > 0,
lim
n!1 P
Q  Qo
k
k<e
ð
Þ ¼ lim
n!1 P
l x
ð Þ ^Q x
ð Þ þ 1  l x
ð Þ
ð
ÞQo  Qo

<e


¼ lim
n!1 P
l x
ð Þ ^Q x
ð Þ  Qo



<e


¼ lim
n!1 P l x
ð Þ
^Q x
ð Þ  Qo



<e


where the last equality follows from the fact that l(x) ∈[0,1] and therefore l(x) is
nonnegative.
Noting
that
x :
^Q x
ð Þ  Qo

<e
n
o

x : l x
ð Þ ^Q x
ð Þ  Qo

<e
n
o
(again since l(x) ∈[0,1]), it follows that P
^Q  Qo

<e


 P l x
ð Þ ^QQo

<e


,
and since ^Q !
p Qo, the left hand side of the above inequality converges to 1 as
n ! 1, implying that the right hand side converges to 1 as n ! 1, 8e > 0.
Therefore, limn!1P
Q  Qo
k
k<e
ð
Þ ¼ 1, or plim(Q*) ¼ Qo.
By assumption (2) of the theorem, and by Slutsky’s theorem, it follows from
the preceding results that
H Qo
ð
Þ1n1 @2 ln L Q; X
ð
Þ
@Q@Q0
"
#
"
#
n1=2
^Q  Qo


!
d n1=2
^Q  Qo


!
d  H Qo
ð
Þ1Z  N 0; H Qo
ð
Þ1M Qo
ð
ÞH Qo
ð
Þ1


since the probability limit of the bracketed expression is equal to the identity
matrix. The statement in the theorem concerning the asymptotic distribution of
^Q then follows immediately from Deﬁnition 5.2.
n
44See Bartle, R.G.,(1976) The Elements of Real Analysis, 2nd Edition, John Wiley, p. 371.
512
Chapter 8
Point Estimation Methods

Theorem 8.21
Proof
LetΥ t
ð Þ ¼ Q : q Q
ð
Þ ¼ t; Q 2 O
f
g 8t 2 R q
ð Þi.e.,Υ t
ð Þis the set of Q-values having
the image value t based on the function q. The collection of t-values represented
by the range of the function, R(q), is be called the parameter space induced by
q(Q), and note that because there can be more than one Q ∈O that satisfy
q(Q) ¼ t, there can be more than one density and likelihood function,
f(x;Q)  L(Q;x), associated with t, which are identiﬁed by the Q-values that
satisfy q(Q) ¼ t. Deﬁne the likelihood function induced by q(Q) as
L t; x
ð
Þ ¼ max
Q2Υ t
ð Þ L Q; x
ð
Þ
f
g ¼ max
Q2Υ t
ð Þ f x; Q
ð
Þ
f
g
i.e., L*(t; x) is the largest likelihood consistent with q(Q) ¼ t, or equivalently,
L*(t; x) is the highest density weighting assigned to the random sample out-
come, x, by the family of densities f(x;Q), Q ∈O, given that q(Q) ¼ t. A
maximum likelihood estimate of t ¼ q(Q) is then given by a value of t ∈R(q)
that maximizes the induced likelihood function, as
^t ¼ arg max
t2RðqÞ
Lðt; xÞ
f
g ¼ arg max
t2RðqÞ
max
Q2ΥðtÞ L Q; x
ð
Þ
f
g


¼ arg max
t2RðqÞ
max
Q2ΥðtÞ f x; Q
ð
Þ
f
g


Since ^umaximizes L(Q;x)  f(x;Q), it follows that^t ¼ q(^u) maximizes the induced
likelihood function L*(t;x), so that q( ^Q) is the MLE of q(Q). (See Zehna, P.W.,
(1966) “Invariance of Maximum Likelihood Estimation,” Ann. Math. Stat., 37,
p. 755, for further discussion of the induced likelihood function concept).
n
Theorem 8.27
Proof
The function h(Q) ¼ G(Q)0WG(Q) is a continuous function of Q that is uniquely
minimized at Q ¼ Qo since W is positive deﬁnite and G(Q) ¼ 0 iff Q ¼ Qo.
Deﬁne N(e) ¼ {Q:d(Q,Qo) < e}, which is an open e-neighborhood of the true
parameter value Qo, and let x(e) ¼ minQ2O\N eð Þ h Q
ð
Þ  h Qo
ð
Þ
f
g (where the mini-
mum exists by Weierstrass’s theorem because h(Q) is continuous and O \ Nð«Þ is
closed and bounded). Let the event An(e) be deﬁned as
AnðeÞ ¼ fy : max
Q2O j Qn y; Q
ð
Þ  h Q
ð
Þj<xðeÞ=2gfor e>0:
Letting ^Q represent the unique GMM estimate, it follows that for y ∈An(e):
1. h( ^Q) < Qn(y, ^Q) + x(e)/2,
2. Qn(y,Qo) < h(Qo) + x(e)/2.
By the deﬁnition of the GMM, Qn(y, ^QÞ  Qn(y,Qo), so it follows from (1) that
3. h( ^Q) < Qn(y,Qo) + x(e)/2.
Substituting (2) into the right side of the inequality in (3) yields
4. h( ^Q) < h(Qo) + x(e).
Then substituting the deﬁnition of x(e) into (4) yields
5. h( ^Q) < minQ2O\N eð Þ h Q
ð
Þ
f
g ) ^Q ∈N(e).
8.5
Appendix: Proofs and Proof References for Theorems
513

Since by condition (d) of the Theorem 8.28 limn!1 P An eð Þ
ð
Þ
ð
Þ ¼ 1 8e>0, then
limn!1 P ^Q 2 N eð Þ




¼ 1 8e>0, and thus ^Q!
p Qo.
n
Keywords, Phrases, and Symbols
/(proportional to)
Autocorrelated
Bias-adjusted MLE
Classical Assumptions of the GLM
Coefﬁcient of determination, R2
Dependent variable
Design matrix
Disturbance, error, or residual vector
Econometrics, sociometrics,
psychometrics
Explanatory (or independent)
variables
Gauss-Markov theorem
General linear model (GLM)
Generalized least-squares estimator
Generalized method of moments
(GMM)
Heteroskedastic
Homoskedasticity
Instrumental variable estimation
Invariance principle
Least squares under normality
Least-squares estimator
Likelihood function induced by q(Q)
Likelihood function, L(Q;x)
Maximum likelihood (ML) estimate
Maximum likelihood estimator
(MLE)
Method of least squares
Method of moments (MOM)
Nonlinear least squares
Parameter space induced by q(Q)
Perfect multicollinearity
Reparameterization of the density
family
Speciﬁcation error
Zero autocovariance or zero
autocorrelation
Problems
1. The daily production of electricity generated by a
coal-ﬁred power plant operating in the midwest can be
represented as
Yi ¼ b1 lb2
i
mb3
i eei
where
Yi ¼ quantity of electricity produced on day i, measured
in megawatts;
li ¼ quantity of labor input used on day i, measured in
100’s of hours;
mi ¼ units of fossil fuel input on day i; and
ei ~ N(0,s2), the ei0s being iid.
Onehundreddaysofobservationsonthelevelsofinputs
usedandthequantityofelectricitygeneratedwerecollected.
The following information is provided to you by the utility
company, where y* ¼ ln(y), and x* is a matrix consisting of a
vector of 1’s followed by column vectors corresponding to
the natural logarithms of labor and fuel levels.
y0
y ¼ 288:93382; x0
y ¼
165:47200
180:32067
122:90436
2
64
3
75;
x0
x
ð
Þ1 ¼
:06156
:05020
:00177
:09709
:07284
symmetric
ð
Þ
:11537
2
64
3
75
(a) Transform the production function into a form in
which parameters can be estimated using the least
squares estimator. Estimate the parameters of the
transformed model.
(b) Is the estimator you used in part (a) to estimate q(b)
(1) unbiased, (2) asymptotically unbiased, (3) BLUE,
(4)
MVUE,
(5)
consistent,
and/or
(6)
normally
distributed?
(c) Is the estimator you used to estimate s2 (1) unbiased,
(2) asymptotically unbiased (3) BLUE, (4) MVUE,
(5) consistent, and/or (6) gamma-distributed?
(d) Deﬁne the MVUE for the degree of homogeneity of
the production function (i.e., deﬁne the MVUE for
q∗(b) ¼ b2 + b3. Estimate the degree of homogeneity
of the production function using the MVUE. Is the
MVUE a consistent estimator of the degree of homo-
geneity? Is the MVUE normally distributed?
(e) Deﬁne an MVUE estimator for the covariance matrix
of the least-squares estimator. Estimate the covari-
ance matrix of the least-squares estimator.
2. Smith’s Dairy is contemplating the proﬁtability of
utilizing a new bovine growth hormone for increasing
the milk production of its cows. Smith’s randomly selects
514
Chapter 8
Point Estimation Methods

cows and administers given dosages of growth hormone
at regular intervals. All animals are cared for identically
except for the levels of hormone administered. A random
sample of size n from the composite experiment measur-
ing the total milk production over the lactation of each
animal is represented by
Yt ¼ b1 þ b2xt þ et; t ¼ 1; . . . ; n;
where the et0s are presumed to be iid with a common
marginal density function
fðz; a; bÞ ¼
1
b  a Iða;bÞ ðzÞ
with a ¼ b, and b is some positive number.
The xt values are the dosages of the growth hormone,
measured in cc units, at levels deﬁned by
xt ¼ t  10 trunc t  1
10


; t ¼ 1; 2; 3; . . .
and yt is the milk production of the tth dairy cow,
measured in hundredweights.
(a) Is the least-squares estimator of b (1) unbiased, (2)
BLUE, (3) consistent, and/or (4) asymptotically nor-
mal? Justify your answer.
(b) Is the estimator S2 of s2 (1) unbiased, (2) consistent,
and/or (3) asymptotically normal? Justify your answer.
(Hint: The xt0s occur in a repeating sequence of
the
numbers
1,2,3,. . .,10.
It
follows
that
Pn
t¼1 xt=n ! 5:5 and Pn
t¼1 x2
t =n ! 38:5.)
(c) In the outcome of a random sample of size 100, Smith’s
found that y0y ¼ 4,454,656.3 and x0y ¼
21097:673
117570:78


.
Estimate the parameters of the linear model using
^b and S2.
(d) Given that raw milk sells for $10 per hundredweight,
deﬁne a BLUE for the expected marginal revenue per
cow obtained from administering the growth hor-
mone. If the total cost of the hormone treatment per
cow over the entire lactation is $12 per cc, and if the
maximum allowable dose is 10 cc, is it proﬁtable for
Smith’s to utilize the hormone, based on the BLUE
estimate of marginal revenue calculated above? Why
or why not? If so, determine what level should be
administered, and provide a BLUE estimate of the
gain in proﬁtability.
(e) Regarding Smith’s linear model assumptions, are
there other distributional assumptions that you
would suggest for consideration besides the uniform
distribution? Would this change any of your ans-
wers above? Would you suggest examining alterna-
tive functional forms for the relationship between
milk production and hormone treatment? If so,
would this affect your answers to the questions
above?
3. Suppose
Y n1
ð
Þ ¼ x nk
ð
Þb k1
ð
Þ þ « n1
ð
Þ; where
e ~
N(0,s2I) and x has full column rank.
(a) Show that
T ¼
ℓ0 ^b  b


S2 ℓ0 x0x
ð
Þ1ℓ

1=2 ;
for any conformable
ℓ6¼ 0 , has a t-distribution with
(n  k) degrees of freedom. (Hint: Transform T so that it
is expressed as a ratio of two independent random
variables, with an N(0,1) random variable in the numera-
tor, and the square root of a w2 random variable divided by
its degrees of freedom in the denominator.)
(b) Using the fact that T has a t-distribution with (n  k)
degrees of freedom, deﬁne a random interval (Z1, Z2)
which satisﬁes
Pðℓ0b 2 ðz1; z2ÞÞ ¼ :95
when n  k
¼ 25. (Hint: Deﬁne your z1 and z2 variables as appro-
priate functions of
^b; S2


, which will be suggested by
a transformation of P(t ∈(tℓ, th)) ¼ .95), for outcomes
t of T.
4. An economist is analyzing the relationship between
disposable income and food expenditure for the citizens of
a developing country. A survey of the citizens has been
taken, and data have been collected on income and food
expenditures.
The
following
statistical
model
is
postulated for the data:
Yi ¼ b1 zb2
i eei;
where
yi ¼ expenditure on food for the ith household;
Zi ¼ disposable income for the ith household; and
ei0s ~ iid N(0,s2)
(a) Transform the model into the GLM form. What
parameters or functions of parameters are being
estimated by the least-squares estimator applied to
the transformed model?
(b) Is the least-squares estimator unbiased, the BLUE,
and/or the MVUE for the parameters or functions of
parameters being estimated?
Problems
515

(c) The actual survey consisted of 5,000 observations,
and the following summary of the data is available:
x0x
ð
Þ1 ¼
:17577542
:019177442
:019177442
:0020946798


;
x0y ¼
10579:646
98598:324


;
y0y ¼ 14965:67;
where the x-matrix is a (5,000  2) matrix consisting of a
column of 1’s and a column representing the natural
logarithms of the observations on income, and the y-vector
refers to the corresponding natural logarithms of food
expenditures. Calculate the least-squares estimate of the
parameters of the transformed model. What is your
estimate of the elasticity of food expenditure with respect
to income?
(d) What is the probability distribution of ^b2, the least-
squares
estimator
of
b2?
Generate
the
MVUE
estimates of the mean and variance of this distribu-
tion. (You might ﬁnd it useful to know that (y0y  y0x
(x0x)1x0y)/(4,998) ¼ 25.369055).
Given
the
assumptions of the model, and using the MVUE
estimates
of
mean
and
variance,
what
is
the
(estimated) probability that the income elasticity
estimated by the least-squares approach will be
within ∓.2 of the true income elasticity?
(e) Discuss any alterations to the speciﬁcation of the
relationship between food expenditure and dispos-
able income that you feel is appropriate for this
problem.
5. The research department of the Personal Computer
Monthly magazine is analyzing the operating life of com-
puter chips that are produced by a major manufacturer
located in the Silicon Valley. The research staff postulates
that a random sample of lifetimes being analyzed adheres
to the statistical model
X  ynePn
i¼1 xi=y Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ;
where y>0:
The magazine publishes an index deﬁned by b ¼ 1/y to
measure the quality of computer chips, where the closer b
is to zero, the better the computer chip. The joint density
of X is reparameterized so that the density function is
parameterized by b, as
X  bnebPn
i¼1 xi Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ;
where b>0:
(a) Does the reparameterized family of density functions
belong to the exponential class of density functions?
(b) Deﬁne a set of minimal sufﬁcient statistics for the
reparameterized density function. Are the minimal
sufﬁcient statistics complete sufﬁcient statistics?
(c) Does there exist an unbiased estimator of b whose
variance achieves the Cramer-Rao lower bound?
(d) Deﬁne the maximum likelihood estimator for the
parameter b. Is the MLE a function of the complete
sufﬁcient statistic? Is the MLE a consistent estimator
of b?
(e) Is the MLE the MVUE of b? Is the MLE asymptoti-
cally normally distributed? Asymptotically efﬁcient?
6. A large commercial bank intends to analyze the accu-
racy
with
which
their
bank
tellers
process
cash
transactions. In particular, it desires an estimate of the
expected proportion of daily cash transactions that the
bank tellers process correctly. It plans to analyze 200
past observations on the daily proportion of correct cash
transactions by the tellers, and specify the statistical
model underlying the daily observations as:
fðz; aÞ ¼ aza1I 0;1
ð
ÞðzÞ; a 2 ð0; 1Þ
(i.e., f(z;a) is a beta density function with b ¼ 1).
(a) Deﬁne the maximum likelihood estimator of a.
(b) Show that the MLE is a function of the complete
sufﬁcient statistic for this problem.
(c) Is the MLE of a a consistent estimator?
(d) It can be shown (you don’t have to) that
E
Xn
i¼1 ln Xi
ð
Þ

1
¼ a= n  1
ð
Þ:
(See W.C. Guenther (1967), “A best statistic with vari-
ance Not Equal to the Cramer-Rao lower bound,” Amer-
ican Mathematical Monthly, 74, pp. 993–994, or else you
can derive the density of
Pn
i¼1 lnðXiÞ

1 and ﬁnd its
expectations—the density of Pn
i¼1 lnðXiÞ is the mirror
image (around the vertical axis at zero) of a Gamma
density). Is the MLE the MVUE for a? If not, is there a
function of the MLE that is MVUE for a?
(e) Show that the MLE is asymptotically normal and
asymptotically efﬁcient, where n1=2 ð^a  aÞ!
d Nð0; a2Þ.
(f) Deﬁne the MLE of q(a) ¼ a/(a + 1), which is the
expected proportion of correct cash transactions.
516
Chapter 8
Point Estimation Methods

(g) Is the MLE of q(a) a consistent estimator?
(h) Is the MLE of q(a) asymptotically normal and asymp-
totically efﬁcient? If so, deﬁne the asymptotic distri-
bution of the MLE estimator.
(i) The
outcome
of
the
sufﬁcient
statistic
was
Pn
i¼1 ln xi
ð
Þ ¼ 9:725.
Calculate the maximum likelihood estimate a.
Calculate the estimate of a using the MVUE of a.
(j) Calculate the maximum likelihood estimate of
q(a) ¼ a/(a + 1), the expected proportion of correct
transactions.
(k) Calculate the maximum likelihood estimate of the
probability that there will be greater than or equal to
97 percent correct cash transactions on a given day.
(Hint: Determine the appropriate function of a in
this case, and use the invariance principle.)
7. The Personnel Department of the ACME Textile Co.
administers an aptitude test to all prospective assembly
line employees. The average number of garments per hour
that an employee can produce is approximately propor-
tional to the score received on the aptitude test. In partic-
ular, the relationship is represented by
Yi ¼ xib þ ei
where
yi ¼ average number
of
garments/hour
produced
by
employee i,
xi ¼ score of employee i on aptitude test,
b ¼ proportionality factor,
ei ¼ error term, representing the deviation between actual
average number of garments per hour produced by
employee i and the production level implied by xib.
You may assume that the ei0s are independent, and
you may also assume that E(ei) ¼ 0 and E(ei
2) ¼ s2, 8i.
Suppose you had the outcome, {y1, y2,. . .,yn}, of a random
sample of average production rates for n employees,
together with their associated scores, {x1, x2,. . .,xn}, on
the aptitude test.
(a) Should the random sample {Y1,. . .,Yn} be interpreted as
a random sample from somepopulation distribution, or
should it be interpreted as a random sample generated
from a general experiment? (Note: It cannot be
expected that the aptitude scores will all be the same.)
(b) Derive the functional form of the least-squares esti-
mator of the proportionality factor, b. Is the least
squares estimator BLUE in this case? Is it the
MVUE of b?
(c) Presuming that you could increase the sample size
without bound, is the estimator you derived in (b) a
consistent estimator of b? Is it asymptotically nor-
mally distributed? Justify your answer, being explicit
about any assumptions you have made about the
behavior of the xi values and/or the ei values.
(d) From a sample of size n ¼ 100, the sample outcome
resulted in
X
100
i¼1
xiyi ¼ 92; 017 and
X
n
i¼1
x2
i ¼ 897; 235
Use the estimator you derived in (b) to generate an esti-
mate of b.
8. A business consultant to the ACME Textile Co.
suggests that the estimator
^b
 ¼ x0x þ k
ð
Þ1x0Y
might be useful to consider as an alternative to the
least-squares estimator of b in the preceding problem (the
estimator ^b
 is a special case of the so-called “ridge regres-
sion” estimator in the statistics literature). In this case,
Y ¼
Y1
Y2
...
Yn
2
6664
3
7775 and x ¼
x1
x2
...
xn
2
6664
3
7775;
and k is some positive constant.
(a) Is the estimator unbiased? If the estimator is not
unbiased, derive an expression for the bias.
(b) Derive
an
expression
for
the
variance
of
this
estimator.
(c) Is the estimator a consistent estimator of b? Justify
your answer, being explicit about any assumptions
you have made about the behavior of the xi values.
(d) Compare the mean square errors of the least-squares
estimator and the estimator
^b
 . Is one estimator
superior in MSE to the other 8b and s2? If not, can
you characterize the problem conditions under which
each estimator would be superior in terms of MSE?
9. Henri Theil, a famous economist/econometrician,
analyzed the demand for textiles in the Netherlands dur-
ing the period 1923–1939, using a general linear model
framework. In particular, he speciﬁed the relationship
between per-capita textile consumption, real price of
textiles, and per capita real income as
Problems
517

Yt ¼ b1 pb2
t
ib3
t eet; t ¼ 1923; . . . ; 1939
where
Yt ¼ per capita textile consumption in year t, represented
as an index with base year 1925;
pt ¼ retail price index of clothing divided by a general
cost-of-living index in year t, represented as an index
with base year 1925,
it ¼ real income per capita in year t, deﬁned as the total
money income of private consumers divided by the popu-
lation size and a general cost of living index, represented
as an index with base year 1925,
et ~ iid N(0,s2), t ¼ 1923,. . .,1939.
The data used by Theil in estimating the demand
relationship is presented in the following table.
YEAR
Y
i
p
1923
99.2
96.7
101.0
1924
99.0
98.1
100.1
1925
100.0
100.0
100.0
1926
111.6
104.9
90.6
1927
122.2
104.9
86.5
1928
117.6
109.5
89.7
1929
121.1
110.8
90.6
1930
136.0
112.3
82.8
1931
154.2
109.3
70.1
1932
153.6
105.3
65.4
1933
158.5
101.7
61.3
1934
140.6
95.4
62.5
1935
136.2
96.4
63.6
1936
168.0
97.6
52.6
1937
154.3
102.4
59.7
1938
149.0
101.6
59.5
1939
165.5
103.8
61.3
(a) Present the statistical model in a form that is consis-
tent
with
the
general
linear
model
framework
(variables should be measured in a way that coincides
with the way they are used in the GLM speciﬁcation).
Can the random sample ðY1; . . . ; YnÞ be interpreted as
a random sample from a population distribution?
Why or why not?
(b) Deﬁne complete (and minimal) sufﬁcient statistics
for the parameters (b,s).
(c) Deﬁne the BLUE estimator of (ln (b1), b2, b3))? Gener-
ate a BLUE estimate for this vector.
(d) Deﬁne the MVUE for the vector (ln (b1), b2, b3)).
Justify that your estimator is in fact the MVUE. Gen-
erate a MVUE estimate for the vector.
(e) What is the MVUE for (b2 + b3), i.e., what is the
MVUE for the degree of homogeneity of the demand
function in terms of relative prices and real income?
Justify that your estimator is the MVUE for (b2 + b3).
Generate an MVUE estimate of (b2 + b3).
(f) Deﬁne the probability distribution of the MVUE for
(b2 + b3). What is the probability distribution of the
MVUE for (ln (b1), b2, b3))?
(g) Present conditions under which the MVUE of (ln ((b1),
b2, b3)) would be: (1) a consistent estimator, and (2) an
asymptotically normally distributed estimator.
(h) Deﬁne the MVUE for s2. Justify that the estimator is,
in fact, the MVUE. Generate an MVUE estimate of s2.
(i) Present conditions under which the MVUE for
s2 would be a consistent estimator.
(j) Estimate the probability that your MVUE estimator
of the price elasticity of demand will generate an
estimate that is within .15 of the true price elastic-
ity. You may use estimates of unknown parameters in
generating this probability estimate.
(k) Is ^E Yt
ð
Þ ¼ e^b
1p
^b2
t i
^b3
t , where
^b
1; ^b2; ^b3


is the BLUE
estimator of (ln (b1), b2, b3)), a consistent estimator of
E(Yt) for given values of i and it?
(l) Is the estimator in (k) BLUE? MVUE?
10. In each case below, determine whether, and under
what assumptions, the stated relationship between Y
and x can be represented in general linear model form
such that the least squares estimator will provide a
BLUE estimator of the b parameters.
(a) Yi ¼ exp Pn
j¼1 xijbj þ ei


(b) Yi ¼ b0 þ b1 xi þ b2 x2
i þ ei
(c) Yi ¼ b0
Qk
j¼1 xbj
ij þ ei
(d) Yi ¼
x2i
1 þ ex1i bþei
11. The following statistical model is postulated for
representing the relationship between real aggregate dis-
posable income and real aggregate expenditure on nondu-
rable goods:
Yi ¼ exp b1 þ b2xi þ ei
ð
Þ where
yi ¼ real aggregate expenditure on nondurables in period i
measured in billions of dollar;
xi ¼ real
aggregate
disposable
income
in
period
i,
measured in billions of dollars; and ei0s are iid, with
ei þ ab 
1
ba GðaÞ ea1
i
eei=b Ið0;1Þ ðeiÞ.
518
Chapter 8
Point Estimation Methods

There are 100 observations (y1,x1),. . .,(y100,x100) avail-
able to estimate the values of b1, b2, and s2 ¼ var(ei).
Assuming the model speciﬁcation is correct, answer the
following questions:
(a) Transform
the
model
into
GLM
form.
What
parameters or functions of parameters are being
estimated by the least-squares estimator applied to
the transformed model?
(b) Is the least-squares estimator
unbiased? BLUE?
Asymptotically unbiased?
(c) Letting
x ¼
1
x1
1
x2
..
.
..
.
1
xn
2
6664
3
7775; if x0x
ð
Þ1 ! 0 and n1x0x ! Q
(a symmetric, positive deﬁnite matrix) as n ! 1, would
it follow that the least-squares estimator is consistent and
asymptotically normally distributed? Why or why not?
(d) Letting y ¼
ln yi
ð
Þ
..
.
ln yn
ð
Þ
2
64
3
75 and ^b be the BLUE of b, is it
true that
S2 ¼
Y  x^b

0
Y  x^b


= n  2
ð
Þ
is an unbiased and consistent estimator of ab2?
12. In each case, indicate whether the statement regard-
ing the relationship Y ¼ xb + « is true or false, and justify
your answer.
(a) Let the random (n  1) vector Y represent a random
sample from some composite experiment, where
E(«) ¼ 0, and E(««0) ¼ s2I. Suppose the x-matrix has
full column rank, but that the ﬁrst and second
columns of x are nearly linearly dependent and, as a
result, the determinant of x0x is near zero, equaling
.273  107. In this case, although ^b ¼ x0x
ð
Þ1x0Y is
still an unbiased estimator, it is no longer BLUE (i.e., it
loses its “best” property of having the smallest covari-
ance matrix in the linear unbiased class of estimators).
(b) The ei0s are homoskedastic and jointly independent
with E(ei) ¼ d 6¼ 0 8i. Also, xb ¼ b1i + b2Z where i is
an (n  1) column vector of 1’s, and Z is a (n  1)
column
vector
of
explanatory
variable
values.
Then if ^b is the least-squares estimator of b, ^b2 is
the BLUE of b2.
(c) The disturbance terms are related as et ¼ ret1 + Vt,
where the Vt0s are iid with E(Vt) ¼ 0 and var(Vt) ¼ s2
8t, and |r| < 1. The least squares estimator is both
BLUE and consistent.
13. Your company markets a disposable butane lighter
called “surelight.” In your product advertising, you use
the slogan “lights on the ﬁrst try-everytime!” As a quality
check, you intend to examine a random sample of 10,000
lighters from the assembly line and observe for each
lighter the number of trials required for the lighter to
light. Your assistant obtains the random sample outcome
and reports to you that a total of 10,118 trials were
required to get all of the lighters to light. She did not
record the 10,000 individual outcomes of how many trials
were required for each lighter to light. You are interested
in estimating both the expected number of trials needed
for a lighter to light and the probability, p, that the lighter
lights on any given trial.
(a) Deﬁne an appropriate statistical model for the 10,000
outcomes of how many trials were required for each
lighter to light.
(b) Deﬁne the MLE for the expected number of trials
needed for a lighter to light. Is the estimator the
MVUE? Is it consistent? Is it asymptotically normal?
Is asymptotically efﬁcient?
(c) Deﬁne the MLE for the probability that the lighter
lights on any given trial. Is the estimator the MVUE?
Is it consistent? Is it asymptotically normal? Is it
asymptotically efﬁcient? (Hint: Can you show that
t X
ð Þ ¼ ðn  1Þ=
Pn
i¼1 Xi


 1


has an expectation
equal to p?)
(d) Provide MLE estimates and MVUE estimates of both
the expected number of trials needed for the ﬁrst light
and the probability that the lighter lights on any
given trial.
14. A regional telephone company is analyzing the
number of telephone calls that are connected to wrong
numbers at its telephone exchange. It collects the number
of wrong telephone connections on each of 200 days,
and
treats
the
observations
as
the
outcome
of
a
random sample of size 200 from a Poisson population
distribution:
fðz; lÞ ¼ el lz
z!
If0;1;2;:::g ðzÞ
(a) Deﬁne the MLE of l, the expected number of wrong
connections per day.
(b) Is the MLE the MVUE for l? Is it consistent? Asymp-
totically normal? Asymptotically efﬁcient?
Problems
519

(c) If P200
i¼1 xi ¼ 4; 973, what is the ML estimate of the
expected number of wrong connections? If each
wrong connection costs the company $070, deﬁne a
MLE
for
the
expected
daily
cost
of
wrong
connections, and generate a ML estimate of this cost.
(d) Deﬁne a MLE for the standard deviation of the daily
number of wrong connections. Is the MLE consis-
tent? Is it asymptotically normal? Generate a ML
estimate of the standard deviation.
15. The number of minutes past the scheduled depar-
ture time that jets with no mechanical problems leave
the terminal in an overcrowded airport in the northeast
are iid outcomes from a uniform population distribution
of the form f(z;Y) ¼ Y1I(0,Y)(z). A random sample of
1,000 departures is to be used to estimate the parameter
Y
and
the
expected
number
of
minutes
past
the
scheduled departure time that a jet will leave the termi-
nal. Summary statistics from the outcome of the random
sample include min(x) ¼ .1, max(x) ¼ 13.8,
x ¼ 6:8 ,
s2 ¼ 15.9.
(a) Deﬁne a MLE for Y and for expected number of
minutes past the scheduled departure time that a jet
will leave the terminal. Are these MLEs functions of
minimal sufﬁcient statistics?
(b) Use the MLEs you deﬁned above to generate ML
estimates of the respective quantities of interest.
(c) Are the estimators in (a) unbiased? consistent?
(Hint:
E(max(X)) ¼ Y[n/(n + 1)]
and
E((max(X))2) ¼
Y2[n/(n + 2)])
(d) Are the estimators in (a) MVUES?
16. Deﬁne MLEs for the following problems:
(a) Estimating the pi0s based on a random sample from a
multinomial population distribution.
(b) Estimating the unknown parameters in the mean
vector m and covariance matrix S based on a random
sample from a bivariate normal distribution.
(c) Estimating b in the population distribution
f x; b
ð
Þ ¼ 2x
b exp  x2
b


Ið0;1Þ ðxÞ
(d) Estimating the parameter p in a negative binomial
population distribution where r is known.
17. Deﬁne MOM estimators in each of the following
cases. Are estimators consistent? Are they asymptotically
normal?
(a) Estimating m and s2 based on a random sample from a
normal population distribution.
(b) Estimating a and b based on a random sample from a
Beta population distribution.
(c) Estimating p based on a random sample from a geo-
metric population distribution.
(d) Estimating a and b based on a random sample from a
continuous uniform population distribution.
18. (Generalized Least Squares Estimator) Consider the
linear model Y ¼ xb + « in which all of the classical
assumptions apply, except that E(««0) ¼ F 6¼ s2I. Con-
sider a GMM estimator of the parameter vector b based
on the moment conditions E(gt(Yt,b)) ¼ E(zt∙0(Yt  xt∙b))
¼ 0 where z ¼ F1x.
(a) Assuming temporarily that F were known, identify
the sample moment conditions that would be used to
deﬁne the GMM estimator. Solve the moment
conditions to provide an explicit functional represen-
tation of the estimator (called the generalized least
squares estimator in the literature).
(b) Discuss conditions under which the estimator you
have deﬁned in (a) is consistent, asymptotically nor-
mal, and asymptotically efﬁcient.
(c) For the general case where F is unknown, discuss
how you would deﬁne an operational version of
the GMM estimator, and discuss its relationship
with the estimator in (a).
19.
(Linear and Nonlinear Models)
Determine which
of the following models can be transformed into linear
models and which cannot, and identify the transforma-
tion in the cases where it is possible.
(a) Yt ¼
Q
k
j¼1
X
bj
jt
 
!
Vt; where Vt  Gamma a; b
ð
Þ
(b)
Yl
t 1
l
¼ b1 þ b2
Xd
t 1
d


þ et; where et  N 0; s2


(c) Yt ¼ b0 þ b1Xb2
t þ et where et  N 0; s2


(d)
Yt ¼ b1 b2Lb3
t
þ 1  b2
ð
ÞKb3
t

b4 b1
=
exp et
ð Þ;
where et  fðeÞ
(e) Yt ¼ 1þexp X
0
tbþet



1
; where et  Logistic 0;s
ð
Þ
20.
(Nonlinear Least Squares Estimator)
Consider the
nonlinear regression model
Y ¼ g x; b
ð
Þ þ « ¼ b11n þ b2x þ b3xb4 þ «; for b 2 O
520
Chapter 8
Point Estimation Methods

where Y is n  1, 1n is an n  1 vector of 1’s, the n  1
vector x is not a vector of identical constants, E «jx
ð
Þ ¼ 0
and Cov «jx
ð
Þ ¼ s2I.
(a) Is a parameter vector with b4 = 0 identiﬁable in this
model?
(b) Is a parameter vector with b4 = 1 identiﬁable in this
model?
(c) Deﬁne an admissible parameter space, V, for this
model
(d) Given a random sample of data (Y,x), describe how
you would go about estimating this model based on
the least squares principle.
(e)
Identify an asymptotic distribution for the least
squares estimator of b.
(f) Suppose that b4 was “in a close neighborhood”, but
not exactly equal to one of the values 0 or 1. Could
there be any issues relating to “multicollinearity”
affecting the least squares estimator of the parameter
vector? Explain.
Problems
521

9
n
Hypothesis Testing Theory
n
n
n
9.1
Introduction
9.2
Statistical Hypotheses
9.3
Basic Hypothesis Testing Concepts
9.4
Parametric Hypothesis Tests and Test Properties
9.5
Classical Hypothesis Testing Theory: UMP and
UMPU Tests
9.6
Noncentral t-Distribution
9.7
Appendix: Proofs and Proof References for Theorems
9.1
Introduction
A primary goal of scientiﬁc research often concerns the
veriﬁcation or refutation of assertions, conjectures, currently accepted laws, or
descriptions relating to a given economic, sociological, psychological, physical,
or biological process or population. Statistical hypothesis testing concerns the
use of probability samples of observations from processes or populations of
interest, together with probability and mathematical statistics principles, to
judge the validity of stated assertions, conjectures, laws, or descriptions in
such a way that the probability of falsely rejecting a correct hypothesis can be
controlled, while the probability of rejecting false hypotheses is made as large as
possible. The precise nature of the types of errors that can be made, how the
probabilities of such errors can be controlled, and how one designs a test so that
the probability of rejecting false hypotheses is as large as possible is the subject
of this chapter.
We will henceforth use the word hypothesis generically to refer to any
statement relating to a process or population under study for which an analyst
wishes veriﬁcation or refutation. Examples of the myriad of hypotheses that
might be tested include statements such as:
1. The demand for tablet PCs in the United States is price inelastic,
2. A new medication is more effective than the currently accepted treatment
for treating high blood pressure,

3. A new LED light bulb has an expected usable life span of over 30,000 hours,
or
4. Less than 1 in 10,000 college-educated individuals engage in criminal
activity in a given year in North America.
9.2
Statistical Hypotheses
In order for probability and mathematical statistics principles to be applied to a
problem of hypothesis testing, hypotheses must be translated into statements
about characteristics of the probability space associated with a probability sam-
ple from the process or population being analyzed. The point of doing so is to
translate a hypothesis into implications for expected behavior of sample
outcomes when the hypothesis is true versus when it is false. If sample
outcomes behave differently under a true and false hypothesis, the differences
in behavior can be used to help identify when observed sampling behavior is
refuting a stated hypothesis. Relatedly, if sample outcome behavior is no differ-
ent whether a given hypothesis is true or false, then it is clear that sample data
cannot possibly be used to identify when a hypothesis is false.
In the most general meaning of the concept, a statistical hypothesis is a set
of potential probability distributions for a sample X associated with some pro-
cess or population. The statistical hypothesis is deﬁned by statements
identifying the characteristics asserted to be true about the probability space
associated with X. In parametric and semiparametric models, these statements
are often presented in the form of assertions about the values of parameters or
functions of parameters relating to the underlying probability distribution of X.
For nonparametric models, and for the components of semiparametric models
that do not involve parameters, hypothesized characteristics of the probability
space for X are stated in more general terms. Our primary focus in this chapter
will be on tests of parameters of functions of parameters, but the general
concepts of statistical hypothesis testing presented ahead extend to nonpara-
metric contexts as well.
A hypothesis is statistically testable only if it can be represented in terms of
a statistical hypothesis, as deﬁned below.
Deﬁnition 9.1
Statistical Hypothesis
A set of potential probability distributions for a probability sample from a
process or population.
If a statistical hypothesis consists of a single element that deﬁnes a speciﬁc
probability distribution for X that is completely and uniquely identiﬁed, the
statistical hypothesis is called a simple hypothesis. If the statistical hypothesis
is not simple, then it is called a composite hypothesis. A composite hypothesis
is a statistical hypothesis containing two or more (possibly inﬁnite) potential
probability distributions for the outcomes of the random sample.
524
Chapter 9
Hypothesis Testing Theory

It is customary to represent the set of probability distributions that deﬁnes a
statistical hypothesis by the capital letter H, and to use subscripts, when needed,
to distinguish between various statistical hypotheses under investigation, such
as H0, Ha, or Hi. Regarding the set deﬁning conditions that are used to represent a
statistical hypothesis, the analyst chooses the characteristics of the probability
space of X that are of interest in any given problem setting. The following
examples illustrate the representation of statistical hypotheses.
Example 9.1
Composite Statistical
Hypothesis for
Bernoulli Process
It is asserted by a manufacturer of metal hardware that the percentage of defec-
tive bolts in a shipment of 1,000 bolts is no more than 2 percent. The receiver of
the shipment intends to use a random sample, with replacement, of size n from
the bolt shipment to assess the validity of the manufacturer’s assertion.
The hypothesis of the manufacturer, translated into a statistical hypothesis
stated in terms of a random sample of size n from the bolt population, is then
(letting xi ¼ 1 indicate a defective bolt, and xi ¼ 0 represent a nondefective bolt)
H ¼
fðx; pÞ ¼
Y
n
i¼1
pxi 1  p
ð
Þ1xiI 0;1
f
g xi
ð
Þ; p 2 ½0; :02
(
)
:
This statistical hypothesis is a composite hypothesis and speciﬁes that the
probability distribution of the random sample of size n from the bolt population
is the product of n Bernoulli distributions, with the probability of observing a
defective bolt in any Bernoulli trial being p  .02. The statistical hypothesis
implies an inﬁnite set of potential probability distributions for defective bolts.□
Example 9.2
Composite Statistical
Hypothesis for
Exponential Population
It is hypothesized that the expected life of a computer chip manufactured by a
certain manufacturing process exceeds the industry average of 20,000 hours, and
it is further hypothesized that the lifetimes of computer chips manufactured by
this process are distributed according to the exponential family of distributions.
The statistical hypothesis, stated in terms of an iid random sample of size n from
the population distribution of computer chip lifetimes, is then (assuming
elements of X are measured in 1,000’s of hours):
H ¼
fðx; yÞ ¼ yn exp 
X
n
i¼1
xi=y
 
!
P
n
i¼1I 0;1
ð
Þ xi
ð
Þ; y 2 20; 1
ð
Þ
(
)
:
This statistical hypothesis is composite since a single probability distribution is
not completely and uniquely deﬁned by H—the mean life measured in 1,000’s of
hours, y, is any number larger than 20, according to H. Thus, the statistical
hypothesis implies an inﬁnite set of potential probability distributions for the
random sample of computer chip lifetimes.
□
Example 9.3
Simple Statistical
Hypothesis for
Bernoulli Process
Suppose in Example 9.1 that the hypothesis was that there are no defective bolts
in the 1,000 bolt shipment. Then the statistical hypothesis could be speciﬁed in
terms of a random sample of size n from the bolt population as
9.2
Statistical Hypotheses
525

H ¼
fðxÞ ¼
Y
n
i¼1
I 0
f g xi
ð
Þ
(
)
:
This is a simple statistical hypothesis indicating that the probability distribu-
tion is degenerate with the outcome x ¼ 0 (all xi0s are nondefective) having
probability
1.
The
statistical
hypothesis
could
have
been
equivalently
represented as in Example 9.1, but with p ¼ 0 instead of p ∈[0, .02].
□
Example 9.4
Composite Statistical
Hypothesis for non-iid
Normally-Distributed
Sample
The average yield per acre for a particular variety of wheat grown in the Paciﬁc
Northwest is represented by
Yi ¼ b0 þ b1fi þ b2ri þ ei; ei  N 0; s2


;
for appropriate ranges of fi and ri, where yi is average yield per acre measured in
bushels in year i, fi is average pounds of fertilizer applied per acre, and ri is
average rainfall per acre measured in inches. Independent observations on
30 years worth of values for fi and ri and corresponding outcomes of Yi are
available. An analyst wishes to assess the hypothesis that a 1 pound per acre
increase in fertilizer applied to the crop generates an expected wheat yield
increase of .25 bushels per acre. The observations on wheat yields can be
conceptualized as the vector outcome of a random sample generated via a
general experiment with the joint probability distribution of Y ¼ (Y1,. . .,Y30)0
being
Y 
Y
30
i¼1
N yi; b0 þ b1fi þ b2ri; s2


:
Translating the hypothesis of the researcher into a statistical hypothesis yields
H ¼ ff y; b; s2


¼ P
30
i¼1N yi; b0 þ b1fi þ b2ri; s2


; b1 ¼ :25g:
The statistical hypothesis is composite, since a unique probability distribution
for Y has not been completely identiﬁed because b0, b2, and s2 were left unspeci-
ﬁed and thus can take any values that result in a legitimate normal density
function. (Normal distributions are clearly only approximations in this case
since yields cannot be negative; it is assumed that P(yi < 0) is negligible for
the relevant fi and ri values being analyzed.)
□
Example 9.5
Composite Statistical
Hypothesis for a
Semiparametric Model
with iid Random
Sampling
The average salary of accountants in a certain region of the county is $48,500. An
allegation is made that male accountants in this region have lower than average
salaries, and an analyst wishes to assess the allegation. It is not known which
parametric family of PDFs encompasses the distribution of male accountants’
salary in the region. Translating the allegation into a statistical hypothesis
526
Chapter 9
Hypothesis Testing Theory

relating to a random sample of size n from the population distribution of male
accountants in the region yields
H ¼
f x; Q
ð
Þ ¼
Y
n
i¼1
m xi; Q
ð
Þ; m ¼ E Xi
ð
Þ ¼ h Q
ð
Þ<48; 500 8i
(
)
:
The statistical hypothesis is composite, since H contains all PDFs for X that
have E(Xi) < 48,500 8i.
□
When deﬁning H, choices of functional forms for f(x) are sometimes
suggested by the nature of the random process or population being analyzed.
For example, in Example 9.1, the Bernoulli family of density functions was
indicated by the fact that the random sample was deﬁned via random sampling,
with replacement, from a ﬁnite population of defective/nondefective bolts. In
Example 9.2, the exponential family of distributions might be motivated by
certain physical features of the computer chips under study (nonnegativity of
lifetimes is obvious; the density might be suggested by a “good as new while
functioning” characteristic of the chip—recall the “memoryless property” of the
exponential family of distributions). In Example 9.3, the stated hypothesis
implied that only one outcome of X was possible, so that the probability distri-
bution of X must be degenerate. In Example 9.4, the normal family of
distributions might be motivated via an appeal to a central limit theorem,
where the error term, ei, would be assumed to represent the additive effect of a
large number of random inﬂuences not speciﬁcally accounted for in the simple
linear relationship speciﬁed to represent E(Yi). In Example 9.5, there was no
indication of the functional form of the probability distribution except for the
range of possible values for its mean.
In representing statistical hypotheses, a simpliﬁcation is often used when
the functional form of the joint density of X is taken to be known except for the
value of some parameter or parameter vector Q, as in Example 9.1–9.4. In this
case, H is often represented as simply a set of parameter values, the idea being
that if f(x;Q) is known except for the value of Q, then Q ∈H will characterize a
set of potential probability distributions for X perfectly well and provide an
equivalent representation of a statistical hypothesis.
Example 9.6
Statistical Hypotheses
in Examples 9.1–9.4
Stated in Abbreviated
Form
Assuming the functional form for the joint density of X to be given as stated in
each example, the statistical hypotheses can be represented in abbreviated form
as H ¼ {p: 0  p  .02}, H ¼ {y: y > 20}, H ¼ {p: p ¼ 0}, and H ¼ {(b,s2):
b1 ¼ .25}, respectively.
□
The representation of statistical hypotheses is often abbreviated still further,
using general notation of the form H: set deﬁning conditions, meaning that H is
a set of parameter values deﬁned by whatever set deﬁning conditions are stated
following the colon. This further abbreviation of the representation of H is
illustrated below for the case of Examples 9.1–9.4.
9.2
Statistical Hypotheses
527

Example 9.7
Statistical Hypotheses
in Terms of Set-Deﬁning
Conditions for Examples
9.1–9.4
Alternative
abbreviated
representations
for
the
statistical
hypotheses
of Examples 9.1–9.4 are given by H: 0  p  .02, H:y > 20, H: p ¼ 0, and
H: b1 ¼ .25, respectively.
□
In general, the use of abbreviated representations of H is acceptable so long
as the context of the problem being analyzed results in no ambiguity regarding
the deﬁnition of the set of potential probability distributions for X implied by
the statistical hypothesis. The notation is prevalent in cases of parametric
hypothesis testing, where it is assumed at the outset that the probability distri-
bution of X is a member of a given parametric family of PDFs.
For later reference, note that the speciﬁcation of a statistical hypothesis, H,
speciﬁes a set of potential outcomes for the random sample. That is, each PDF in
H will imply a particular range of possible outcomes for the random sample X,
and the union of all of these respective ranges of X is logically the full set of
possible random sample outcomes implied by H. The idea is that since any of the
PDFs in H is asserted to be a candidate for the true PDF of X, then any of the
associated ranges of X are also candidates for R(X). Recalling that the range of X
is synonymous with the support of X’s PDF, we can deﬁne the set of potential
random sample outcomes implied by H as
RðXjHÞ ¼ fx : f x; Q
ð
Þ>0 and f x; Q
ð
Þ 2 Hg:
We will refer to RðXjHÞ as the range of X over H.1 Analogous deﬁnitions for the
range of X over H and over H [ H can be given as
RðXj HÞ ¼ fx : f x; Q
ð
Þ>0 and f x; Q
ð
Þ 2 Hg and
RðXjH [ HÞ ¼ fx : f x; Q
ð
Þ>0 and f x; Q
ð
Þ 2 H [ Hg:
For now, it will sufﬁce to interpret the complement of H, H, as the collection of
all probability distributions not speciﬁed in H that are potential candidates for
the true PDF of the probability sample under study. Thus, the complement
operation is being applied in the context of a universal set of all potential
probability distributions relevant for the probability sample being examined.
Then RðXjH [ HÞ can be interpreted as the set of all potential outcomes of the
random sample.
It can be the case that RðXjHÞ ¼ RðXj HÞ ¼ RðXjH [ HÞ, which occurs when
all of the supports of the PDFs in H and H are the same. For example, such is the
case in Example 9.1 where all three sets equal n
i¼1 0; 1
f
g. Alternatively, the sets
1Compare this set to the range of X over O introduced in our discussion of minimal sufﬁcient statistics, Section 7.4.
528
Chapter 9
Hypothesis Testing Theory

can differ, as in Example 9.3 where these sets are respectively 0, n
i¼1 0; 1
f
g, and
n
i¼1 0; 1
f
g . The nature of the ranges of X over H, H , and H [ H will play
important roles in determining whether an ideal statistical test of H exists
(i.e., a test which makes no errors in deciding the validity of H) and are also
useful for motivating why one cannot generally expect statistical tests of H to be
error-free. We examine these ideas in the next section.
9.3
Basic Hypothesis Testing Concepts
In this section we deﬁne the concept of a statistical hypothesis test, we identify
two types of errors that such a test can make, and we indicate in what sense such
errors can be controlled. We also point out the general fact that the incidence of
one type of test error can be lessened only at the expense of increasing the
incidence of the other if the size of the probability sample on which the test is
based is held constant.
9.3.1
Statistical Hypothesis Tests
The objective of a test of a statistical hypothesis is to assess the validity of the
statistical hypothesis. In practice, a statistical hypothesis is tested by ﬁrst
obtaining the outcome of a probability sample from the process or population
being analyzed and then using the observed outcome of the sample to assess
whether it is reasonable to conclude that one of the probability distributions for
the probability sample implied by the statistical hypothesis could have governed
the outcome behavior of the sample. Deciding when an outcome of a sample will
cause rejection of H and when it will not is accomplished by a rule or procedure
that partitions the range of potential sample outcomes into two subsets. The
hypothesis is then rejected or not depending on which subset the outcome of the
sample belongs to. In its most general sense, a test of a statistical hypothesis can
be deﬁned as follows.
Deﬁnition 9.2
Test of a Statistical
Hypothesis
A rule or procedure, based on the outcome of a probability sample from the
process or population under study, used to decide whether to reject a statisti-
cal hypothesis.
A test of a statistical hypothesis is often referred to as simply a statistical
test. It follows from Deﬁnition 9.2 that a valid functional representation of a
statistical test would be in the form of an indicator function whose domain
consists of the potential outcomes of the sample, and whose range indicates
whether or not the statistical hypothesis is rejected. A statistical test is fully
deﬁned when the set of potential sample outcomes is partitioned into two
disjoint subsets, one called the rejection region or (critical region) and the
9.3
Basic Hypothesis Testing Concepts
529

other called the acceptable region.2 Letting the set of potential sample outcomes
be represented by Cr [ Ca, where Cr is the rejection region, Ca is the acceptable
region, and Cr \ Ca ¼ ;, then
ICrðxÞ ¼
1
0
"
#
)
reject H
do not reject H


is a fully deﬁned rule for deciding whether to reject the statistical hypothesis
(see Figure 9.1).
In the remainder of the discussion of hypothesis testing concepts and
methods, it will always be tacitly assumed that Ca ¼ Cr, if an explicit deﬁnition
of the acceptable region, Ca, is not given in any hypothesis testing situation. The
complement operation will be interpreted to occur within a universal set deﬁned
by RðXjH [ HÞ, which represents all possible outcomes of the random sample
under PDFs belonging to either H or
H . With this convention in mind, a
statistical test can be deﬁned completely in terms of a rejection region, Cr, as
was the case in our indicator function representation above. Furthermore,
we will sometimes use the phrase the statistical test deﬁned by Cr to
mean a statistical test deﬁned as x ∈Cr ) reject H, x =2 Cr (or equivalently
x ∈Cr ¼ Ca) ) do not reject H.
A schematic overview of the general context of a hypothesis testing problem
is given in Figure 9.2.
The design or choice of a statistical test is seen to be equivalent to the design
or choice of the partition of the set of potential probability sample outcomes into
rejection and acceptable regions. This begs the question of how the partition
should be chosen so as to design a test that is “good” in some appropriate sense.
Operationally, this will amount to designing tests that control the probability of
incorrectly rejecting correct hypotheses while making the rejection of a false
hypothesis as large as possible. The types of incorrect decisions that can be made
using statistical tests are examined next.
Rule: ICr(x)
Cr
Ca
Potential Probability
Sample Outcomes
Decision
1
0
Reject H
Do not reject H
Figure 9.1
Test of a statistical
hypothesis.
2The terminology “acceptable region” is sometimes replaced by “acceptance region” in the literature. We will see later that while the
behavior of sample outcomes may be “acceptable” to H given the characteristics of the probability space implied by H, there are
statistical reasons why one might not want to literally conclude acceptance of H on the basis of this “acceptable” behavior. We will
clarify this subtle but important distinction with additional rigorous rationale later, which will motivate further why we choose to use
the terminology “acceptable”.
530
Chapter 9
Hypothesis Testing Theory

9.3.2
Type I Error, Type II Error, and General Non-Existence
of Ideal Statistical Tests
Given a situation where the analyst must decide whether to reject a statistical
hypothesis, H, and given that in reality the statistical hypothesis is either true or
false, there are four possible states of affairs with respect to test decisions and
their validity, as summarized in Figure 9.3.
The two potential error situations that can occur when deciding to reject H
are given distinct, albeit somewhat uninspired, names so that they can be clearly
distinguished.
Deﬁnition 9.3
Type I and
Type II Error
Let H be a statistical hypothesis being tested for rejection. Then the two types
of errors that can be made by the statistical test are:
1. Type I Error: rejecting H when H is true.
2. Type II Error: not rejecting H when H is false.
Specify a Statistical Hypothesis: X ~ f (x;Q)∈H
Define a Statistical Test of H:
Choose Cr ⊂ R(X | H ∪ H )
Observe Outcome of Probability Sample
( X1,..., Xn) ~ f (x;Q)∈H ∪ H
Test Statistical Hypothesis H
x ∈Cr
x ∉Cr
reject H
do not reject H
Figure 9.2
General hypothesis testing
procedure.
Correct
Type II Error
Type I Error
Correct
H
H
H
H
True Probability Distribution
Test Decision
Figure 9.3
Potential outcomes of
statistical test relative to
true probability
distribution.
9.3
Basic Hypothesis Testing Concepts
531

In other words, a Type I Error is committed when the statistical test mistak-
enly indicates that H should be rejected, i.e., the true probability distribution is
consistent with H, but the test outcome indicates that the true probability
distribution is not in H. A Type II Error is committed when the statistical test
mistakenly indicates that H should not be rejected, i.e., the true probability
distribution of X is not consistent with H, but the test outcome nonetheless
indicates that the true probability distribution is consistent with H.
Clearly, the ideal statistical test would be such that once an outcome of the
random sample were observed, the hypothesis would always be correctly
identiﬁed as being false or not, and thus no errors would be made. For such an
ideal statistical test to exist, it must be possible to partition the range of poten-
tial random sample outcomes a priori in such a way that outcomes in
the acceptable region, Ca, would occur iff H were true and outcomes in the
rejection region, Cr, would occur iff H were false. To deﬁne such partition, it
is clear that RðXjHÞ and RðXj HÞ need to be disjoint so that, with certainty,
x ∈Ca ¼ RðXjHÞ ) H is not rejected, and X ∈Cr ¼ RðXj HÞ ) H is false. The
following example illustrates a case where an ideal statistical test can be
deﬁned.
Example 9.8
An Ideal Statistical Test
An envelope manufacturing machine is such that when all parts are functioning
properly, the machine produces boxes containing 1,000 envelopes that never
contain more than two defective envelopes, with the probability distribution
of X, the number of defectives per box, being of the binomial form f(x;p) ¼
(2!/[x!(2  x)!])px(1  p)2x I{0,1,2} (x), for p ∈(0,1). The machine will continue
to operate if one of the teeth on the main drive gear breaks, but then the
distribution of defectives per box of 1,000 envelopes changes such that the box
will always contain between 6 and 8 defective envelopes, with the probability
distribution of the number of defectives per box changing to the distribution of
Y ¼ X + 6, i.e.,
g y; p
ð
Þ ¼ 2!= y  6
ð
Þ! 8  y
ð
Þ!
½

ð
Þpy6 1  p
ð
Þ8yI 6;7;8
f
gðyÞ
for p ∈(0,1). Any other machine problem will cause the machine to shut down,
so that no envelopes will be produced.
A quality control engineer wants to test the hypothesis that all parts of the
envelope machine are working properly. Her statistical hypothesis, in terms of a
random sample of 1 box of envelopes, is H ¼ {f(x;p), p ∈(0,1)}. The engineer
designs the statistical test as follows
x 2 Ca ¼ 0; 1; 2
f
g ) do not reject H and x 2 Cr ¼ 6; 7; 8
f
g ) reject H:
Note that the test is an ideal statistical test, which produces no decision error of
either type. For example, if the engineer were to observe that x ¼ 1 in a box of
envelopes she randomly chose to examine, she would conclude that H is true; all
parts of the machine are working properly. She is certain the assessment is
correct, since the statistical test used in making the decision is an ideal statis-
tical test.
□
532
Chapter 9
Hypothesis Testing Theory

More generally, if RðXjHÞ \ RðXj HÞ 6¼ ;, which is virtually always the case
in practice, then there are potential outcomes of the sample that reside simulta-
neously in the supports of some PDFs in H and some PDFs in H. In this case, a
sample outcome equal to one of the points in RðXjHÞ \ RðXj HÞcannot be used to
resolve with certainty whether or not H is true since such points are not
uniquely associated with either H or H. It follows that there would not exist a
dichotomous partition of RðXjH [ HÞ that would deﬁne a statistical test which,
with a priori certainty, could always correctly decide the truth of H, since any
dichotomous partition would include some points that were in both RðXjHÞ and
RðXj HÞ.
If it were true that RðXjHÞ 6¼ RðXj HÞ, then a trichotomous partition of
RðXjH [ HÞ can be devised that leads to a decision rule that makes no errors
in deciding the truth of H, but the rule will also result in situations in which
some sample outcomes lead to no decision whatsoever. The rule could be
based on the sets Cn ¼ RðXjHÞ \ RðXj HÞ , Ca ¼ RðXjHÞ  Cn, and Cr ¼
RðXj HÞ  Cn, as
x 2
Ca
Cr
Cn
8
<
:
9
=
; )
do not reject H
reject H
no decision
8
<
:
9
=
;:
While making no errors, such a rule can be wasteful of sample information,
especially when outcomes in the set Cn have a high probability of occurring.
Furthermore in the majority of hypothesis testing applications RðXjHÞ ¼ RðXj HÞ.
So not only is an ideal statistical test not available, the preceding trichotomous
partition of RðXjH [ HÞ is not available either. In practice, it is therefore gener-
ally the case that error-free statistical tests cannot be deﬁned. Instead, one must
seek to deﬁne statistical tests that control the incidence of errors. We examine
this idea next.
9.3.3
Controlling Type I and II Errors
In applications, a statistical test can often be designed to control the propensity
for Type I and Type II errors to occur. By “controlling” the errors, we mean that
Ca and Cr are chosen in a way such that the error probabilities are known, and
thus controlled, in advance of performing the test and are at levels that are, in
some sense, acceptable to the researcher.
In order to illustrate how a statistical test can be designed to “control” errors
recall Example 9.3. In this case, RðXjHÞ ¼ 0, RðXj HÞ ¼ n
i¼1 {0,1}, and thus
RðXjH [ HÞ ¼ n
i¼1 {0,1}, where the deﬁnition of RðXj HÞ follows from the fact
that if H is true, then the population distribution for the problem is a Bernoulli
density with p > 0 and so both 0 and 1 are potential outcomes for each Xi. Also
note thatRðXjHÞ \ RðXj HÞ ¼ 0 6¼ ;, so that no ideal statistical test exists for this
problem.
Suppose a statistical test of H is deﬁned by the following rejection region for
a random sample of size 100: Cr ¼ {x: P100
i¼1 xi > 0}. In other words, if a random
9.3
Basic Hypothesis Testing Concepts
533

sample of size 100 results in no defectives, we will decide H is not rejected, while
if any defectives are observed in the outcome of the random sample, H will be
declared false. Using this test, how probable are Type I and Type II errors?
Determining the probability of Type I Error is straightforward in this case,
since if H is true, then P(x =2 Cr) ¼ P(x ¼ 0) ¼ 1, which follows from the fact
that P(xi ¼ 0) ¼ 1 8i, so that P(x ¼ 0) ¼ P(xi ¼ 0, 8i) ¼ Q100
i¼1 P(xi ¼ 0) ¼ 1 (recall
that the Xi0s are independent random variables). It follows that if H is true, the
probability of rejecting H is P(Type I Error) ¼ P(x =2 Ca) ¼ P(x ∈Cr) ¼ 0. Note
that having “controlled” the Type I Error to have probability zero, we have
effectively complete conﬁdence that H is false if the test rejects H, since the
probability is 0 that the test would mistakenly reject H.
Now examine the probability of Type II Error. Assuming H: p ¼ 0 is false,
then Xi has a Bernoulli distribution with probability, H : p ∈{.001,.002,. . .,1}.
Thus, the probability of Type II Error depends on which probability distribution
in H is the correct one. Since we do not know which is correct (or else we would
have no need to test a statistical hypothesis in the ﬁrst place), we examine the
range of possibilities for Type II Error as a function of p. In this case the parame-
ter p serves as an index for the 1,000 possible alternative probability
distributions for X that are contained in H. Figure 9.4 provides a partial graph
of the functional relationship between p and the probability of Type II Error (see
graph labeled n ¼ 100).3
Note that in this case that probabilities of Type II Error are equal to P(x =2 Cr) ¼
P( P100
i¼1 xi ¼ 0) calculated on the basis of a binomial distribution for P100
i¼1 Xi
for the various values of p ∈H, with n ¼ 100. This deﬁnition of Type II Error
probabilities is motivated from the fact that the event Pn
i¼1 xi ¼ 0 results in
0.001
0.006
0.011
0.016
0.021
0.026
0.031
0.036
0
0.3
0.6
0.9
P (Type II Error)
p
n =200
n =100
Figure 9.4
Probabilities of type II error
for sample sizes: n ¼ 100
and n ¼ 200.
3For convenience, we have chosen to “connect the dots” and display the graph as a continuous curve. We will continue with this
practice wherever it is convenient and useful.
534
Chapter 9
Hypothesis Testing Theory

non-rejection of H, and p ∈H indicates that H is false, so that together these
two conditions deﬁne a situation where a false H is not rejected, i.e., a Type
II Error is committed. The explicit functional relationship between p and
the level of Type II Error in this example, which indicates the probability
levels at which Type II Error has been controlled for the various values of
p ∈H, is given by
P Type II Error
ð
Þ ¼ hðpÞ ¼ 1  p
ð
Þ100for p 2 :001; :002; . . . ; 1
f
g:
Figure 9.4 indicates that, in this problem, the probability of committing a Type II
Error declines rapidly as p increases. When p is only .03, the probability of Type II
Error is less than .05. Note, however, that when p is very close to its
hypothesized value in H, i.e., p ¼ 0, the probability of Type II Error can be
quite high. For example, if p ¼ .001, then P(Type II Error) ¼ .905, or if
p ¼ .002, then P(Type II Error) ¼ .819. Thus, in practice, if the shipment has
very few defectives (implying a very small p associated with the iid Bernoulli
random sample from the shipment), then the proposed statistical test will not be
very effective in rejecting the assertion that the shipment has no defectives, i.e.,
the probability of rejection will not be high.
In general, the probabilities of Type I Error are equal to the values of
P(x ∈Cr) for each of the PDFs contained in H. The probabilities of Type II
Error equal the values of P(x =2 Cr) for each of the PDFs contained in H. If the
error probability characteristics of a given statistical test are unacceptable,
the researcher generally has two options, which can also be pursued simulta-
neously—she can choose a different rejection region, Cr, and thus deﬁne a
different statistical test, or she can alter the size of the probability sample on
which to base the test. We continue examining the hypothesis in Example 9.3 to
illustrate these options. First suppose we increase the sample size to 200 but
otherwise use a test rule analogous to the preceding rule to test the hypothesis
that there are no defectives in the 1,000 bolt shipment. Stated in terms of the
parameter p, the statistical hypothesis, H, remains precisely as before, H: p ¼ 0.
The probability of Type I Error remains equal to zero, since if H were true,
then P(Type I Error) ¼ 1  P(x ¼ 0) ¼ 1  P( P200
i¼1 xi ¼ 0) ¼ 0, which can be
demonstrated following the same reasoning as before except that now we are
dealing with n ¼ 200 independent Bernoulli random variables as opposed to
n ¼ 100.
The probabilities of Type II Error as a function of p are now given by the
value of P(P200
i¼1 xi ¼ 0) calculated using the binomial distribution of P200
i¼1 Xi for
the various values of p > 0, except now n ¼ 200. The explicit functional rela-
tionship between p and the level of Type II Error is now represented by
P Type II Error
ð
Þ ¼ hðpÞ ¼ 1  p
ð
Þ200for p 2 :001; :002; . . . ; 1
f
g
and a partial graph of P(Type II Error) as a function of p is given in Figure 9.4 (see
the graph labeled n ¼ 200). It is seen that the probability of Type II Error has been
uniformly lowered for all p > 0. For example, the probability of Type II Error is
less than .05 when p is only .015. The probability of Type II Error is appreciably
9.3
Basic Hypothesis Testing Concepts
535

reduced even for very small values of p, although the probability remains high
for values of p very close to the hypothesized value of 0(when p ¼ .001 or .002,
the probability is still .819 and .670, respectively).
If the sample size were increased further, the probabilities of Type II Error
would continue to decline uniformly as a function of p. This becomes clear upon
examining the functional relationship between p and the probability of Type II
Error for an unspeciﬁed sample size of n, as
P Type II Error
ð
Þ ¼ h p; n
ð
Þ ¼ 1  p
ð
Þnfor p 2 :001; :002; . . . ; 1:000
f
g:
Note that dh(p,n)/dn ¼ (1  p)n ln(1  p) < 0 8 p ∈(0,1) since ln(1  p) < 0,
and so the probability of Type II Error is a decreasing function of n 8 p ∈(0,1)
(when p ¼ 1, the probability of Type II Error is 0 8n). Note also that P(Type II
Error) ¼ h(p,n) ! 0 8 p ∈(0,1) as n ! 1. These results illustrate a generally
valid principle for “good” statistical tests that the more sample information one
has, the more accurate a statistical test will be.
Suppose it were too costly, or impossible, to increase the sample size beyond
n ¼ 100. Consider an alternative rejection region in order to alter the error
probabilities of the associated statistical test. In particular, suppose we deﬁne
Cr ¼ {x: P100
i¼1 xi  2} and make test decisions accordingly. Then the probability
of Type I Error is still zero, since if H were true, then P( P100
i¼1 xi ¼ 0) ¼ 1, and so
long as the point x ¼ 0 =2 Cr, P(x =2 Cr) ¼ 1 when H is true, and so the probability
of Type I Error remains zero. The probability of Type II Error as a function of p,
based on the binomial distribution of Pn
i¼1 Xi, is now given by
P(Type II Error) ¼ P
X
100
i¼1
xi ¼ 0 or 1
 
!
¼ 1  p
ð
Þ100 þ 100p 1  p
ð
Þ99for p 2 f:001; :002; . . . ; 1g
:
It is evident that the probabilities of Type II Error are uniformly higher for this
statistical test as compared to the previous test (both tests based on n ¼ 100).
In fact the original statistical test that we examined is, for any given sample
size, the best one can do with respect to minimizing the probability of Type II
Error while maintaining the probability of Type I Error at its optimal value of
zero. This follows from the fact that the Type I Error probability will be zero iff
Cr does not contain the element x ¼ 0, and any element removed from Cr, while
not affecting the probability of Type I Error, will increase the probability of Type
II Error 8 p ∈(0,1). Note further that the only other choice of Type I Error
probability in this statistical hypothesis testing situation is the value 1, which
would be the case for any rejection region (i.e., for any statistical test) for which
0 ∈Cr. A statistical test having P(Type I Error) ¼ 1 would clearly be absurd since
if H were true, one would be essentially certain to reject H! Thus, for the case in
Example 9.3 the statistical test based on Cr ¼ {x: Pn
i¼1 xi > 0} is the best one can
do with respect to controlling Type I and Type II Error probabilities for any
sample size.
536
Chapter 9
Hypothesis Testing Theory

In general hypothesis testing applications, a primary concern is the choice of
a statistical test that provides acceptable levels of control on both Type I and
Type II errors. Unfortunately, these are most often conﬂicting objectives, as we
discuss next.
9.3.4
Type I Versus Type II Error Tradeoff and Protection Against Errors
The choice of values at which Type I Error probability can be controlled is
generally not so limited as in the example discussed in the previous section. It
is more typical for there to be a large number, or even a continuum, of possible
choices for the Type I Error probability of a statistical test of a simple hypothesis.
Furthermore, there is also then typically a trade-off between the choice of Type I
and Type II Error probabilities such that the probability of one type of error can
be decreased (increased) only at the expense of increasing (decreasing) the other.
When H is composite, there is also a range of Type I Error probabilities to
consider which results from the fact that H then contains a range of potential
probability distributions for X. We will encounter a large number of examples of
these more typical situations in the remainder of this chapter, and in Chapter 10.
In order to illustrate some of the more general characteristics of statistical
hypothesis tests, reexamine the hypothesis described in Example 9.1. In this
case
R XjH [ H


¼ R XjH
ð
Þ ¼ R Xj H


¼ n
i¼1f0; 1g;
and thus no ideal statistical test exists for the statistical hypothesis that the
probability distribution of the random sample is deﬁned via a product of identi-
cal Bernoulli densities with p  .02. How then should we choose a statistical
test of H? Suppose we want the probability of Type I Error to equal 0. In the
current testing situation, this would mean that if the true PDF for the random
sample were a product of identical Bernoulli densities having any value of
p ∈{0,.001,.002,. . ., .020}, we want our test to H with probability 1. Unfortu-
nately, the only choice for Cr  RðXjH [ HÞ that will ensure that P(x =2 Cr) ¼ 1
when H is true is Cr ¼ ;. This follows because 8 p ∈{.001,.002,. . .,.020}, each
and every point in n
i¼1{0,1} is assigned a positive probability value. Then the
statistical test would have the characteristic that P(x ∈Cr) ¼ 0, whether or not
H is true. The test would always accept H, no matter what the outcome of the
random sample, and thus to obtain a probability of Type I Error equal to 0, we
have to accept a probability of Type II Error equal to 1. This is clearly unaccept-
able—we would never reject H, even if it were profoundly false (even if all of the
bolts in the shipment were defective).
Given the preceding results, it is clear that to choose a useful statistical test
in the current situation requires that one be willing to accept some positive level
of Type I Error probability. Suppose the maximum probability of Type I Error that
one is willing to accept is .05, i.e., we are implicitly stating that P(Type I Error)
 .05 provides sufﬁcient protection against Type I Error. The interpretation of
“protection” is that if P(Type I Error)  .05, then we know that the test will
mistakenly reject a true H no more than 1 time in 20, on average, in a repeated
9.3
Basic Hypothesis Testing Concepts
537

sampling context. Then if the test actually rejects H for a given random sample
outcome, it is much more likely that the test is rejecting H because H is false than
because the test has mistakenly rejected a true H. The level of conﬁdence we
have that H is false when H is rejected by a statistical test is thus derived from the
level of protection against Type I Error that the test provides.
How should Cr be chosen in order to deﬁne a test with the desired control of
Type I Error? We need to deﬁne a rejection region Cr  RðXjH [ HÞ such that P
(x ∈Cr)  .05 no matter which probability distribution in H were true, i.e., no
matter which p ∈{0,.001,. . .,.020} were true. Subject to the previous inequality
constraint on the probability of Type I Error, we want Cr to be such that P(Type II
Error) is as small as possible no matter which probability distribution in H were
true, i.e., no matter which p > .02 were true. Intuitively, it would make sense
that if P(Type II Error) is to be minimized for a given level of control for Type I
Error, we should attempt to choose points for the rejection region, Cr, that would
have their highest probability of occurrence if H were true. In the example at
hand, this corresponds to choosing sample outcomes for inclusion in Cr that
represent a higher number of defectives than lower, since H corresponds to bolt
populations with more defectives than is the case for H.
Suppose that a random sample of size n ¼ 200 will be used. Consider the
rejection region Cr ¼ {x: P200
i¼1 xi > 7} contained in RðXjH [ HÞ ¼ 200
i¼1{0,1}. In
order to verify that the statistical test implied by Cr has a probability of Type I
Error that does not exceed .05, one must verify that P(x ∈Cr)  .05 no matter
which probability model in H were true. In other words, we must verify that the
outcome of the binomial random variable P200
i¼1 Xi is such that
P
X
200
i¼1
xi>7
 
!
¼
X
200
j¼8
200!
j! 200  j
ð
Þ!pj 1  p
ð
Þ200j  :05 8 p 2 0; :001; . . . ; :020
f
g:
One can show that P( P200
i¼1 xi > 7) achieves a maximum of .049 when p ¼ .02,
and so the inequality is met for all choices of p implied by H. Thus, the
probability of Type I Error is upper-bounded by .05. We emphasize that in this
case, since H was composite, the probability of Type I Error is a function of p,
with the range of probabilities upper-bounded by .05. The largest value of Type I
Error probability over all PDFs in H is generally referred to as the size of the
statistical test, and is a measure of the minimum degree of protection against
Type I Error provided by the test. We will examine this concept in more detail in
Section 9.4.
Now examine the probability of Type II Error as a function of p. The various
probabilities of Type II Error that are possible are given by P(x =2 Cr) ¼ P(P200
i¼1 xi
 7) evaluated with respect to binomial distributions for P200
i¼1 Xi that have
p > .02 (which are the values of p implied by H). Figure 9.5 presents a partial
graph of the probability of Type II Error as a function of p for this statistical test.
It is seen from the graph that the probability of Type II Error decreases rapidly
as p increases, with P(Type II Error)  .05 when p  .065. However, as we have
seen previously, P(Type II Error) is quite high for values of p close to those
538
Chapter 9
Hypothesis Testing Theory

implied by H, e.g., if p ¼ .025 or .03, then P(Type II Error) ¼ .87 or .75,
respectively. Thus, if the test does not reject H, the behavior of P(Type II Error)
as a function of p suggests that we would have substantial conﬁdence that
p < .065, say, since otherwise it would have been highly probable that the test
would have rejected H (with a probability of 1  P(Type II Error) ¼ .95 or
greater). However, we do not have a great deal of conﬁdence that H is literally
true, since the test does not have a high probability of rejecting H if p > .02 but
p is near .02. In fact, we will see that this situation is typical of statistical tests in
practice so that it is advisable to conclude that non-rejection of H does not
necessarily mean that H is accepted, but rather that there is insufﬁcient statis-
tical evidence to reject it.
If the relationship depicted in Figure 9.5 for the case where P(Type I Error)
 .05 were deemed unacceptable, the researcher has two basic options, as we
discussed previously. One option would be to increase the sample size to some
value larger than 200 and apply an analogous test rule to the larger random
sample. This would decrease P(Type II Error) uniformly for all p > .02 assuming
Cr were chosen to maintain P(Type I Error)  .05. The other option would be to
accept a larger probability bound on Type I Error (i.e., deﬁne a different test based
on the same sample size), which would also result in a uniform reduction in
P(Type II Error). To illustrate the tradeoff between the two types of errors that
occurs when pursuing the latter option, suppose we redesign the statistical test
using the following rejection region: Cr ¼ {x: P200
i¼1 xi > 5}. Using the binomial
distribution for
P200
i¼1 Xi , it can be shown that P( P200
i¼1 xi
> 5)  .215
8 p ∈{0,.001,. . .,.02}, and so P(Type I Error) is now upper-bounded by .215.
Thus, if H were true, then at most 21.5 percent of the time, on average in a
repeated sampling context, the test would nonetheless reject H. Note, using the
redesigned test, there is notably less conﬁdence that H is actually false when the
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0
0.2
0.4
0.6
0.8
1
P (Type II Error)
Test for P (Type I Error) ≤ .05
Test for
P(Type I
Error) ≤ .215
p
Figure 9.5
Probabilities of type II error
for tests of H: p  .02.
9.3
Basic Hypothesis Testing Concepts
539

test rejects H than was the case with the previous test. However, P(Type II Error)
is also notably decreased using the redesigned test, as illustrated in Figure 9.5.
Now if p ¼ .025 or .03, then P(Type II Error) is reduced to .62 and .44, respec-
tively, and P(Type II Error)  .062 8 p > .05.
In any application, the researcher must decide the appropriate degree of
tradeoff between the Type I and Type II Error probabilities of the test, as well
as the appropriate sample size to use in deﬁning the test. An acceptable choice of
the level of protection against test decision errors will generally depend on the
nature of the consequences of making a decision error, and on the cost and/or
feasibility of changing the sample size. For example, if the hypothesis was that
the level of residual pesticide that appears in fruit sold in supermarkets exceeds
levels that are safe for human consumption, it would seem that Type I Error
would be extremely serious—we want a great deal of protection against making
an incorrect decision that the level of residual pesticide is at a safe level. Thus,
we would need to design a test with extremely low probability of Type I Error.
Type II Error in this case would be a situation where a safe level of pesticide was
declared unsafe, potentially resulting in increased processing and growing costs,
loss of sales by the pesticide manufacturer, and the loss of an input to the
agricultural sector that could potentially lower the cost of food production.
Thus, one would also need to strive for a low P(Type II Error) in fairness to the
pesticide manufacturer, farmers, and consumers. The precise choice of the Type
I/Type II Error tradeoff must be addressed on a case-by-case basis.4
9.3.5
Test Statistics
It should be noted that the rejection regions of the statistical tests deﬁned in the
previous two subsections could all be deﬁned in terms of outcomes of a scalar
statistic. In particular, the statistic used was of the form T ¼ tðXÞ ¼ Pn
i¼1 Xi, and
the set deﬁning conditions for the rejection regions were of the general form
t(x) > c. A scalar statistic whose outcomes are partitioned into rejection and
acceptable regions to deﬁne statistical tests is called a test statistic.
Deﬁnition 9.4
Test Statistic and
Rejection Region
Let Cr deﬁne the rejection region associated with a statistical test of
the hypothesis H versus
H. If T ¼ t(X) is a scalar statistic such that
Cr ¼
x : tðxÞ 2 CT
r


, so that the rejection region can be deﬁned in terms of
outcomes, CT
r , of the statistic T, then T is referred to as a test statistic for the
hypothesis H versus H. The set CT
r will be referred to as the rejection (or
critical) region of the test statistic, T.
4If the magnitudes of the costs or losses incurred when errors are omitted can be expressed in terms of a loss function, then a formal
analysis of expected losses can lead to a choice of type I and type II error probabilities. For an introduction to the ideas involved, see
Mood, A., F. Graybill, and D. Boes, (1974). Introduction to the Theory of Statistics, 3rd Ed., New York: McGraw-Hill, pp. 414–418.
540
Chapter 9
Hypothesis Testing Theory

The use of test statistics can simplify the problem of testing statistical
hypotheses in at least two signiﬁcant ways. First of all, it allows one to check
whether a sample outcome is an element of the rejection region of a statistical
test by examining whether a scalar outcome of the test statistic resides in a set of
scalars (the set CT
r in Deﬁnition 9.4). This eliminates the need for dealing with
n-dimensional outcomes and n-dimensional rejection regions. In effect, the
rejection region of the statistical test is alternatively represented as a unidimen-
sional set of real numbers. Secondly, test statistics can facilitate the evaluation
of Type I and Type II Error probabilities of statistical tests if the PDF of the test
statistic can be identiﬁed and if the PDF can be tractably analyzed. Relatedly, if
an asymptotic distribution for a test statistic can be identiﬁed, then it may be
possible to approximate Type I and Type II Error probabilities based on this
asymptotic distribution even when exact error probabilities for the statistical
test cannot be readily established. We will examine the notion of asymptotic
tests in Chapter 10.
In practice, the large majority of statistical hypothesis testing is conducted
using test statistics, and the statistical test itself is generally deﬁned in terms of
the rejection region of the test statistic. That is, a statistical test of H versus H
will generally be deﬁned as
t x
ð Þ 2
CT
r
CT
a
(
)
)
reject
do not reject
(
)
H
We will henceforth endeavor to express statistical tests in terms of test statistics
whenever it is useful to do so.
It should be noted that test statistics are not unique. For example, the
rejection regions of the statistical tests in the preceding two subsections
can be represented in terms of the test statistics Pn
i¼1 Xi; X; or lnðXÞ (among
a myriad of other possibilities), with corresponding rejection regions for the
test statistics given by (c,n], (c/n,1], and (ln(c/n),0], respectively. The choice of
Pn
i¼1 Xi was particularly attractive in the preceding application because its PDF
(binomial) was easily identiﬁed and tractable to work with. In general, the
choice of functional form for a test statistic is motivated by the ease with
which the outcomes of the statistic can be obtained and by the tractability of
the test statistic’s PDF.
9.3.6
Null and Alternative Hypotheses
The terminology null and alternative hypotheses is often used in the context of
testing statistical hypotheses. Historically, a null hypothesis was a hypothesis
interpreted as characterizing no change, no difference, or no effect. For example,
if the average life of a “typical” 25-watt compact ﬂuorescent light bulb sold in
the U.S. is 10,000 hours, then a test of the hypothesis that a bulb manufactured
by General Electric Co. has a life equal to the average life, i.e., H: m ¼ 10,000,
could be labeled a null hypothesis, and interpreted as representing a situation
where there is “no difference” between General Electric’s bulb and the average
9.3
Basic Hypothesis Testing Concepts
541

life of a typical bulb. The hypothesis that is accepted if the null hypothesis is
rejected is referred to as the alternative hypothesis. In current usage, the term
null hypothesis is used more generally to refer to any hypothesis that, if rejected
by mistake, characterizes a Type I Error situation. Thus, Type I Error refers to the
mistaken rejection of a null hypothesis, while a Type II Error refers to a mistaken
non-rejection of the null hypothesis.
Having examined a number of basic conceptual issues related to statistical
hypothesis testing, we henceforth focus our attention on two speciﬁc classes of
hypothesis testing situations: (1) parametric hypothesis testing, and (2) testing
distributional assumptions. The next two sections examine parametric hypoth-
esis testing in some detail. We will examine testing of distributional
assumptions in Chapter 10.
9.4
Parametric Hypothesis Tests and Test Properties
As the phrase inherently implies, parametric hypothesis testing concerns the
testing of statistical hypotheses relating to the values of parameters or functions
of parameters contained in a probability model for a sample of data. In a
parametric model, it is assumed at the outset that the probability distributions
in both H and H are characterized by members of a known or given parametric
family or class of distributions that are indexed by the parameters in some way.
In a semiparametric model, the parameters refer to the parametric component of
the model, and H and H deﬁne separate and distinct sets of probability models
distinguished by the speciﬁc parametric components deﬁned by H and
H:
Examples 9.1 and 9.3 are illustrations of parametric hypothesis tests in the
context of parametric models, where the known
parametric family of
distributions underlying the deﬁnitions of the statistical hypotheses and their
complements is the Bernoulli distribution. Once it is understood that the prob-
ability models are deﬁned in terms of a Bernoulli distribution with parameter p,
then
the
statistical
hypotheses
under
consideration
can
be
compactly
represented in terms of statements about the value of p or about the value of a
function of p. Example 9.5 is an illustration of parametric hypothesis tests in the
context of a semiparametric model, where no explicit family of parametric
distributions is identiﬁed by the hypotheses, but a parametric component of the
model is identiﬁed. In particular, H relates to all distributions with m < 48; 500,
whereas H relates to distributions for which m  48; 500.
In the parametric hypothesis testing case, we will most often utilize the
abbreviated notation H: Set Deﬁning Conditions introduced in Section 9.2 to
denote the statistical hypothesis. In this case, the set deﬁning conditions will
denote that a parameter vector, or a function of the parameter vector, is
contained in a certain set of values (recall Example 9.7). We will adopt a further
simpliﬁcation that is made possible by the parametric context and interpret H
itself as a set of parameter values, and thus the notation Y ∈H will mean
that the parameter (vector) is an element of the set of hypothesized values.
We emphasize that in the parametric hypothesis testing context, H still
542
Chapter 9
Hypothesis Testing Theory

concurrently identiﬁes, at least to some degree of speciﬁcity,5 a set of probability
distributions for X.
The real-world meaning of a statistical hypothesis concerning the values of
parameters or functions of parameters depends on the characteristics of the real-
world population or process being analyzed, and on the interpretation of what
the parameters represent or measure in the context of the probability model
being analyzed. For example, the exponential family of densities used in Exam-
ple 9.2 leads to the interpretation of H: Y > 20 as being an assertion about the
mean of the distribution of computer lifetimes. The Bernoulli family of
densities used in Example 9.1 leads to the interpretation of H: p  .02 as an
assertion about the proportion of defectives in a shipment of bolts.
9.4.1
Maintained Hypothesis
A parametric hypothesis testing situation will be characterized by two major
components. One component is the statement of the statistical hypothesis, as
described and exempliﬁed earlier. The other is the maintained hypothesis, as
deﬁned below.
Deﬁnition 9.5
Maintained Hypothesis
All facts, assertions, or assumptions about the probability model of a proba-
bility sample that are in common to H and H, and that are maintained to be
true regardless of the outcome of a statistical test of H versus H.
For example, in a parametric hypothesis testing situation within the context of
a parametric model, whatever parametric family of distributions is assumed to
characterize the probability distributions in H and H is part of the maintained
hypothesis. In Example 9.1, the Bernoulli family of distributions is part of the
maintained hypothesis, while in Example 9.2, the exponential family of distri-
butions could be part of the maintained hypothesis if the distributions in H were
also assumed to be in the exponential family. Besides an assertion about the
parametric family of distributions if the model is parametric, a number of other
“facts” about the probability space might be contained in the maintained hypothe-
sis. For example, with reference to Example 9.4, in examining the hypothesis
H: b1 ¼ .25 versus H : b1 6¼ .25, the maintained hypothesis includes assertions
(explicitly or implicitly) that the normal family of distributions characterize the
probability distributions for the ei0s, the ei0s are independent random variables with
E(ei) ¼ 0 and var(ei) ¼ s2 8 i, and the expected value of wheat yield in any given time
period is a linear function of both the level of fertilizer applied and the level of
rainfall.
The facts contained in the maintained hypothesis can be thought of as
contributing to the deﬁnition of the speciﬁc context in which a statistical test
5In parametric models, sets of fully speciﬁed probability distributions will be identiﬁed, whereas in semiparametric models, only a
subset of moments or other characteristics of the underlying probability distributions are generally identiﬁed by the hypotheses.
9.4
Parametric Hypothesis Tests and Test Properties
543

of any hypothesis is to be applied and interpreted. The probabilities of Type I and
Type II Error for a given statistical test of an assertion like H: Y > 20, and in fact
the fundamental meaning of a statistical hypothesis itself, will generally depend
on the context of the parametric hypothesis testing problem deﬁned by the
maintained hypothesis.
Regarding the genesis of the maintained hypothesis, the facts that are
included in the maintained hypothesis might have been supported by the results
of previous statistical analyses. Alternatively, the facts may be derived from
laws or accepted theories about the process or population being studied. Finally,
the facts may be largely unsubstantiated conjectures about the population or
process under study that are tentatively accepted as truths for the purpose of
conducting a parametric hypothesis test. In any case, the interpretation of a
statistical test can depend critically on facts stated in the maintained hypothe-
sis, and the result of a statistical test must be interpreted as being conditional on
these facts. If one or more components of the maintained hypothesis are actually
false, the interpretation of the Type I and Type II Error probabilities of a given
statistical test, and the interpretation of a statistical hypothesis itself, can be
completely changed and/or disrupted. Thus, in a parametric hypothesis testing
situation, one must be careful that the facts stated in the maintained hypothesis
are defensible. Facts that are “tentative” will generally require statistical testing
themselves when convincing supporting evidence is lacking. At the least, the
analyst is obliged to point out any tentative assumptions contained in the
maintained hypothesis when reporting the results of a statistical test.
9.4.2
Power Function
The power function of a statistical test provides a complete summary of all of
the operating characteristics of a statistical test with respect to probabilities of
making correct/incorrect decisions about H. The power function is particularly
useful in comparing alternative statistical tests of a particular parametric
hypothesis. The power function is deﬁned as follows.
Deﬁnition 9.6
Power Function of a
Statistical Test
Let a parametric statistical hypothesis be deﬁned by H: Q ∈VH
and
let its complement be deﬁned by
H : Q ∈V H . Let the rejection region
Cr deﬁne a statistical test of H. Finally, let the CDF F(x;Q), Q ∈V ¼ H [ H;
represent the parametric family of probability distributions encompassed
by H and
H. Then the power function of the statistical test is deﬁned by
pðQÞ ¼ Pðx 2 Cr; QÞ 	
Ð
x2Cr
dFðx; QÞ for Q 2 H [ H:
In words, the power function indicates the probability of rejecting H for every
possible value of Q. The value of the function p at a particular value of the
parameter vector, Q, is called the power of the test at Q, which is the probability
of rejecting H if Q were the true value of the parameter vector. From the deﬁnition
of the power function, it follows that p(Q) is identically a probability of Type I
Error if Q ∈H, or else a probability of not making a Type II Error if Q ∈H .
544
Chapter 9
Hypothesis Testing Theory

Then [1  p(Q)] is identically the probability of Type II Error if Q ∈H and the
probability of not making a Type I Error if Q ∈H. The graph in Figure 9.6
represents a partial graph of the power function for the test of the hypothesis H:
p  .02 from Example 9.1 using the statistical test based on Cr ¼ {x: P200
i¼1 xi > 5}.
We know from before that P(Type I Error)  .215 for this test. The lines A, B, C,
and D in the ﬁgure illustrate the probabilities of not making a Type I error, making
a Type I error, making a Type II error, and not making a Type II error respectively.
The closer the power function of a test is to the theoretical ideal power
function for the hypothesis testing situation, the better the test. The ideal power
function can be compactly deﬁned as p*(Q) ¼ I H(Q) and would correspond to the
ideal statistical test for a given H, were such a test to exist. When comparing two
statistical tests of a given H, a test is better if it has higher power for Q ∈H and
lower power for Q ∈H, which implies that the better test will have lower
probabilities of both Type I and Type II Error. Figure 9.7 provides graphs of hypo-
thetical power functions for two tests of the hypothesis that H: Y  c, and a graph
of the ideal power function for the test situation, where O ¼ H [ H¼ [0,k].
From Figure 9.7, it is apparent that the statistical test associated with power
function p2(Q) is the better test, having probabilities of Type I and Type II Error
everywhere less than or equal to the corresponding probabilities associated with
the alternative test. The power function p2(Q) is also seen to be closer to the
ideal power function, p*(Q).
9.4.3
Properties of Statistical Tests
There are a number of test properties that can be examined to assess whether a
given statistical test is appropriate in a given problem context, and/or to com-
pare a test rule to competing test rules.6 The power function of a statistical test
p
1.0
.8
.6
.4
.2
.03
.01
.04
.02
.05
0
p(p)
p1
p2
[
](
H
A
B
C
D
H
•
•
Figure 9.6
Power function of test for
H: p  .02 in Example 9.1.
6The properties we will examine do not exhaust the possibilities. See E. Lehmann, (1986), Testing Statistical Hypotheses, John Wiley,
NY.
9.4
Parametric Hypothesis Tests and Test Properties
545

deﬁned in the previous section, can be used to deﬁne properties of statistical
tests, including the size of the test and the signiﬁcance level of the test, and
whether a test is unbiased, consistent, and/or uniformly most powerful. We
examine each of these test properties below.
Test Size and Level
One such property, called the size of the test, is a measure of
the minimum level of protection against Type I Error that a given statistical test
provides.
Deﬁnition 9.7
Size and Level of a
Statistical Test
Let p(Q) be the power function for a statistical test of H. Then a ¼ supQ2H
fp Q
ð
Þg is called the size of the statistical test.7 The test is a level a test if
p Q
ð
Þ  a 8Q 2 H.
The size of the statistical test is essentially the maximum probability of Type I
Error associated with a given statistical test. The lower the size of the test, the
lower the maximum probability of mistakenly rejecting H. Tests of H having
level a are tests for which PQ(Type I Error)  a 8Q ∈H. Thus, the level is an
upper bound to the Type I Error probability of statistical tests. The key difference
between the concepts of size and level of statistical tests is that size represents the
maximum (or at least the supremum) value of PQ(Type I Error), Q ∈H, for a
given test, while level is only some stated bound on size that might not equal
PQ(Type I Error) for any Q ∈H or the supremum of PQ(Type I Error) for Q ∈H for
a particular test. A statistical test of H that has size g is an a-level test for any a  g.
1.0
.8
.6
.4
.2
c
k
0
p(Θ)
p2(Θ)
p1(Θ)
π*(Θ)
[
]
H
(
]
H
Θ
•
•
•
Figure 9.7
Two power functions and
the ideal power function
for testing H: Y  c.
7Recall that supQ2Hfp Q
ð
Þg denotes the smallest upper bound to the values of p(Q) for Q ∈H (i.e., the supremum). If the maximum of
p(Q) for Q ∈H exists, then sup is the same as max.
546
Chapter 9
Hypothesis Testing Theory

In applications when a hypothesis H is (not) rejected, terminology that is
often used to indicate the protection against Type I Error used in the test is “H is
(not) rejected at the a – level.” Moreover, level is often taken to be synonymous
in meaning with size, although in statistical concept this need not be the case.
Care should be taken to eliminate ambiguity regarding whether the stated level
is equal to the size of a statistical test used. A phrase that eliminates the potential
ambiguity is “H is (not) rejected using a size-a test.” However, the use of the
term level is ubiquitous in the statistical testing applications literature, and we
simply caution the reader to be careful to understand how the term level is used
in any given application, i.e., synonymous with size of the test or not.
As an illustration of terminology, the power function in Figure 9.6 refers to a
test that has a size of .215, and a level synonymous with size is also .215,
whereas any number greater than .215 is also a legitimate upper bound to the
size of the test, and is thus a level. If the hypothesis H: p  .02 is (not) rejected,
then one could state that H: p  .02 is (not) rejected at the .215 level, or that H:
p  .02 is (not) rejected using a size .215 test.
Unbiasedness
The concept of unbiasedness within the context of hypothesis
testing refers to a statistical test that has a smaller probability of rejecting the
null hypothesis when it is true compared to when it is false.
Deﬁnition 9.8
Unbiasedness of a
Statistical Test
Let p(Q) be the power function of a statistical test of H. The statistical test is
called unbiased iff supQ2H fp Q
ð
Þg  infQ2 H {p(Q)}.8
The property of unbiasedness makes intuitive sense as a desirable property
of a test rule since we would generally prefer to have a higher probability of
rejecting H when H is false than when H is true. As indicated in Deﬁnition 9.8,
whether a test rule is unbiased can be determined from the behavior of its power
function. In particular, if the height of the power function graph is everywhere
lower for Q ∈H than for Q ∈H , the statistical test is unbiased. The tests
associated with the power functions in Figures 9.6 and 9.7 are unbiased tests.
More Powerful, Uniformly Most Powerful, Admissibility
Another desirable prop-
erty of a statistical test is that, for a given level, a, the test exhibits the highest
probability of rejecting H when H is false compared to all other competing tests
of H having level a. Such a test is called uniformly most powerful of level a,
where the term uniformly refers to the test being the most powerful (i.e., having
highest power) for each and every Q in H (i.e., uniformly in H). In the formal
deﬁnition of the concept presented below, we introduce the notation pCr (Q) to
indicate the power function of a test of H when the test is deﬁned by the
rejection region Cr.
8Recall the previous footnote, and the fact that infY2 H {p(Q)} denotes the largest lower bound to the values of p(Q) for Q∈H (i.e., the
inﬁmum). The sup and inf of p(Q) are equivalent to max and min, respectively, when the maximum and/or minimum exists.
9.4
Parametric Hypothesis Tests and Test Properties
547

Deﬁnition 9.9
Uniformly Most
Powerful (UMP)
Level a Test
Let X ¼ {Cr: supQ2H fp Q
ð
Þg  a} be the set of all rejection regions of level a for
testing the hypothesis H based on a sample X~ f(x;Q) of size n. The rejection
region C
r 2 X , and the associated statistical test of H that C
r
deﬁnes,
are uniformly most powerful of level a iff C
r has level a and pC
r Q
ð
Þ  pCr Q
ð
Þ
8Q 2 H and 8Cr 2 X .
In the case where H is a simple hypothesis, the rejection region, C
r , and the
associated test are also referred to as being most powerful (the adverb “uni-
formly” being effectively redundant in this case). The UMP test of H is thus
the test of H, among all possible level a tests, that has the highest probability of
rejecting H when H is false (equivalently, it is the level a test having the most
power uniformly in Q for Q ∈H). Such a test of H is effectively the “best one can
do” with respect to minimizing the probability of Type II Error, given that
protection against Type I Error is at level a. As implied by Deﬁnition 9.9, if one
could plot the power function graphs of all tests of level a, the power function of
the UMP level a test of H would lie on or above the power functions of all other
level a tests of H 8Q ∈H. We will examine general methods for ﬁnding UMP
tests of H (when such tests exist) in Section 9.5. Once we have introduced the
Neyman-Pearson Lemma in Section 9.5, we will also show that a UMP level a
test is also generally an unbiased test.
Unfortunately, in a signiﬁcant number of cases of practical interest, UMP
tests do not exist. We will see that it is sometimes possible to restrict attention
to the unbiased class of tests and deﬁne a UMP test within this class, but even
this approach sometimes fails. In practice, one then often resorts to the use of
statistical tests that have at least acceptable (to the analyst and to those she
wishes to convince regarding the validity of her hypotheses) power function
characteristics. Assuming that a test of level a is deemed to provide sufﬁcient
protection against Type I Error, a test that is more powerful is preferred, as
deﬁned below.
Deﬁnition 9.10
More Powerful
Level a Test
Let Cr andC
r represent two level a statistical tests of H. The test based onC
r is
said to be more powerful than the test based on Cr if pC
r Q
ð
Þ  pCr Q
ð
Þ 8Q ∈H,
with strict inequality for at least one Q ∈H.
The reason for preferring a more powerful test is clear—if both tests provide
the same desired level of protection against Type I error, then the one that
provides more protection against Type II error is better.
While in comparing level a tests the more powerful test is preferred, the less
powerful test may have some redeeming qualities. In particular, the latter test
may provide better protection against Type I Error for some or all Q ∈H, and the
test may then be appropriate in cases where a smaller test level is desired. Tests
with no such possible redeeming qualities are inadmissible tests, formally
deﬁned as follows.
548
Chapter 9
Hypothesis Testing Theory

Deﬁnition 9.11
Admissibility of a
Statistical Test
Let Cr represent a statistical test of H. If there exists a rejection region C
r such
that
pC
r Q
ð
Þ



	
pCr Q
ð
Þ 8Q 2
H
H

	
;
with strict inequality holding for some Q ∈H [ H, then the test based on Cr
is inadmissible. Otherwise, the test is admissible.
From the deﬁnition, it follows that an inadmissible test is one that is
dominated by another test in terms of protection against both types of test
errors. Inadmissible tests can be eliminated from consideration in any hypothe-
sis testing application.
Consistency
The limiting behavior of the power functions of a sequence of level
a tests as the size of the probability sample on which the tests are based
increases without bound relates to the property of consistency. In particular, a
sequence of level a tests of H for which the probability of Type II Error ! 0 as
n ! 1, will be called a consistent sequence of level a tests. Since the deﬁnition
of the rejection region of a test will generally change as the sample size, n,
changes, we will introduce the notation Crn to indicate the dependence of the
rejection region on sample size.
Deﬁnition 9.12
Consistent Sequence of
Level a Tests
Let {Crn} represent a sequence of tests of H based on a random sample (X1,. . .,
Xn) ~ f(x1,. . .,xn;Q) of increasing size, n, and let the level of the test deﬁned by
Crn be a 8 n. Then the sequence of tests is consistent iff lim
n!1 pCrm Q
ð
Þ
ð
Þ ¼ 1
8Q 2 H.
Thus, a consistent sequence of level a tests is such that, in the limit, the
probability is one that H will be rejected whenever H is false. Then, for a large
enough sample size n, the nth test in a consistent sequence of tests provides a
level of protection against Type I Error given by a probability  a and is essen-
tially certain to reject H if H is false.
P-Values
Different analysts and/or reviewers of statistical analyses do not
always agree on what the appropriate level of statistical tests should be when
testing statistical hypotheses. In order to accommodate such differences of
opinion, it is frequent in applications to report P-values (an abbreviation for
“probability values”) for each statistical test conducted. A P-value is effectively
the minimum size test at which a given hypothesis would still be rejected based
on the observed outcome of the sample.
In order for the idea of a minimum size to make sense, there must exist a
prespeciﬁed family of nested rejection regions, and thus an associated family of
statistical tests, that corresponds to increasing Type I Error probability values, a,
for testing the null hypothesis under consideration. By nested rejection regions
9.4
Parametric Hypothesis Tests and Test Properties
549

corresponding to increasing values of a, we mean that Cr(a1)  Cr(a2) for a1 < a2,
where Cr(a) denotes the rejection region corresponding to P(Type I Error) ¼ a.
Then upon observing x, one calculates the P-value as equal to the smallest a value
that identiﬁes the smallest rejection region that contains x, i.e., P-value ¼ arg
mina x 2 Cr a
ð Þ
f
g ¼ mina supQ2HfP x 2 Cr a
ð Þ; Q
ð
Þ
ð
Þgg such that x 2 Cr a
ð Þ
f
g. The
P-value can thus be interpreted as the smallest size at which the null hypothesis
would be rejected based on the nested set of rejection regions or statistical tests.
Assuming the nested set of rejection regions and associated statistical tests
to be used for testing a particular H are well-deﬁned, then once the analyst
reports the P-value, a reviewer of the statistical analysis will know whether
she would reject H based on her own choice of test size within the set of possible
size values.9 In particular, if the reviewer’s choice of size were less than (greater
than) the reported P-value, then she would not (would) reject H since her
rejection region would be smaller than (larger than) the one associated with
the reported P-value and would not (would) contain x.
The P-value is also often interpreted as indicating the strength of evidence
against the null hypothesis, where the smaller the P-value, the greater the
evidence against H. The idea is that if the P-value is small, then the protection
against Type I Error can be set high and H would still be rejected by the test.
A well-deﬁned and accepted set of nested rejection regions and statistical tests
necessary for calculating P-values generally exist in practice, especially when the
rejection regions can be deﬁned in terms of a test statistic. We illustrate both
the use of P-values and other test properties in the following example.
Example 9.9
Illustration of Power
Function, Size,
Unbiasedness,
Consistency, and
P-Values
A random sample of n ¼ 100 observations on the miles per gallon achieved by a
new model pickup truck manufactured by a major Detroit manufacturer is going
to be used to test the null hypothesis that the expected miles per gallon achieved
by the truck model in city driving is less than or equal to 15, the alternative
hypothesis being that expected miles per gallon > 15. It can be assumed that
miles per gallon measurements are independent, and that the population is
(approximately) normally distributed with a standard deviation equal to .1.
The null and alternative hypotheses in this parametric hypothesis testing
problem can be represented by H0: m  15 and Ha: m > 15, respectively. Suppose
we desire to assess the validity of the null hypothesis using a statistical test of
size
.05.
Examine
a
test
based
on
the
following
rejection
region:
Crn ¼ fx : 10n1=2 xn  15
ð
Þ  1:645g: To verify that the rejection region implies
a test of size .05, ﬁrst note that
Xn  15
s=
ﬃﬃﬃn
p
¼
Xn  15


ð:1=
ﬃﬃﬃn
p Þ ¼ 10n1=2 Xn  15


¼ 10n1=2 Xn  m


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
N 0;1
ð
Þ
þ10n1=2ðm  15Þ  N 10n1=2 m  15
ð
Þ; 1


:
9In the case of continuous X, the choice of size is generally a continuous interval contained in [0,1]. If X is discrete, the set of choices
for size is generally ﬁnite, as previous examples have illustrated.
550
Chapter 9
Hypothesis Testing Theory

It follows that the power function of the test can be deﬁned as
pnðmÞ ¼
ð1
1:645
N z; 10n1=2ðm  15Þ; 1


dz ¼
ð1
1:64510n1=2 m15
ð
Þ
Nðz; 0; 1Þdz;
so that supm15pn m
ð Þ ¼ :05 and the test is of size .05.10 The test is also unbiased,
since it is evident that supm15fpnðmÞg  infm>15fpnðmÞg ¼ :05: 11 The consistency
of the test can be demonstrated by noting that 8 m ∈Ha,
limn!1fpnðmÞg ¼ limn!1
ð1
1:64510n1=2 m15
ð
Þ
Nðz; 0; 1Þdz ¼
ð1
1
Nðz; 0; 1Þdz ¼ 1;
and so the test is a consistent test.
The test is also a uniformly most powerful test for testing the null hypothe-
sis H0: m  15 versus Ha: m > 15, i.e., there is no other test rule with p(m)  .05
8 m ∈H0 that has higher power for any m ∈Ha. The rationale in support of the
UMP property will be provided in Section 9.5.
A graph of the power function for this test is given in Figure 9.8.
Upon examination of the power function of the test, it is clear that protec-
tion against Type II Error increases rapidly for values of m > 15. In particular, it
14.97
14.99
15.01
15.03
15.05
0
.2
.4
.6
.8
1
H0
]
Ha
(
p (m)
15
m
Figure 9.8
Power function for testing
H0: m  15 versus
Ha: m > 15, n ¼ 100.
10The maximum is achievable in this case, and equals .05 when m ¼ 15.
11Note that min
m>15 pnðmÞ
f
gdoes not exist in this case. The largest possible lower bound (i.e., the inﬁmum) is .05, which is < pn(m),
8 m > 15.
9.4
Parametric Hypothesis Tests and Test Properties
551

would seem reasonable to conclude that if H0 were not rejected, then the true
value of m  15.03, say, since the probability of not rejecting H0 when m > 15.03
is < .088.
Suppose that the outcome of the random sample of 100 miles per gallon
measurements yielded x ¼ 15.02. It follows that x ∈Crn since 10n1/2(15.02  15)
¼ 2  1.645 when n ¼ 100, and thus H: m  15 is rejected using a size .05 test.
Regardingthe P-value for this test, ﬁrst note thatCrn(a) ¼ {x: 100(x  15)  k(a)}
forms a nested set of rejection regions such that for each value of size a ∈(0,1),
there exist a corresponding constant k(a) such that maxm15fPðx 2 CrnðaÞ; mÞg ¼
Pðx 2 CrnðaÞ; m ¼ 15Þ ¼ a. Moreover, given that x ¼ 15.02, the largest value of
k(a) that deﬁnes a rejection region still resulting in H being rejected is k(a) ¼ 2.
Then the smallest size test that still results in rejecting the null hypothesis
can be calculated as
P-value ¼ min
a
maxm15 Pðx 2 CrnðaÞ; mÞ such that kðaÞ  2
f
g


¼ min
a
Pðx 2 CrnðaÞ; m ¼ 15Þ such that kðaÞ  2
f
g
¼ min
a
ð1
k a
ð Þ
Nðz; 0; 1Þdz such that kðaÞ  2
(
)
¼
ð1
2
Nðz; 0; 1Þdz ¼ :023:
It follows that H0 would not be rejected at a ¼ .01 and would be rejected at
a ¼ .05.
□
Asymptotic Tests
It is sometimes difﬁcult, intractable or impossible (e.g., in
semiparametric models) to determine the exact probabilities of Type I and
Type II errors for a given statistical test. Thus the determination of the exact
power function of a test may also be difﬁcult, intractable, or impossible. Similar
to the case of point estimation, one might then rely on asymptotic properties to
assess the efﬁcacy of a given test rule.
In the majority of hypothesis testing applications, the rejection regions of
statistical tests are deﬁned in terms of test statistics. It is often the case that laws
of large numbers and central limit theory can be utilized to establish various
convergence properties of a test statistic. In particular, asymptotic distributions
of test statistics can be used to approximate power functions of tests, in which
case the efﬁcacy of a given test procedure can be assessed based on the power
function approximation. In practice, the choice of a particular statistical test for
a given hypothesis is often motivated by its adequacy in terms of Type I and Type
II Error protection that are assessed through an examination of its approximate
rather than its exact power function characteristics. We will examine the
asymptotics of some important general classes of statistical tests, including
generalized likelihood ratio, Lagrange multiplier, and Wald tests, in Chapter 10.
552
Chapter 9
Hypothesis Testing Theory

9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
The focus of this section is on the discovery of uniformly most powerful (UMP)
and uniformly most powerful unbiased (UMPU) tests of statistical hypotheses.
The theory in this area is classical in nature requiring a variety of regularity
conditions to be applicable, it is not widely applicable in multiparameter
settings, and it applies primarily to only parametric models directly, although
in semiparametric models it can be applied in an asymptotically-valid approxi-
mative sense. Moreover, it should be emphasized at the outset that UMP tests of
hypotheses do not always exist. Whether a UMP test exists depends on the type
of hypothesis being tested and on the characteristics of the joint density of the
probability sample under study. In a case where the classical results of this
section cannot be applied, or in cases where UMP tests simply do not exist, it
is generally still possible to deﬁne statistical tests with good exact or asymptotic
power function characteristics, and we will examine procedures for deﬁning
such tests in Chapter 10. Readers interested in more generally applicable, albeit
not necessarily optimal, methods and who on ﬁrst reading prefer to avoid a
number of rather complex and involved proofs may prefer to skim this section
at ﬁrst, or else move directly on to Chapter 10.
We examine four basic approaches for ﬁnding UMP tests: Neyman-Pearson,
monotone likelihood, exponential class, and conditioning approaches. While
these approaches have distinct labels, they are all interrelated to some degree.
Much of our discussion will focus on scalar parameter cases, although some
important multiparameter results will also be examined. This scalar orientation
is a practical one—UMP tests simply do not exist for most multiparameter
hypothesis testing contexts. In cases where a UMP test does not exist, it is
sometimes still possible to deﬁne a UMP test within the class of unbiased
tests, as will be seen ahead.
As we noted at the beginning of Chapter 8, and for the reasons stated there,
we will place some of the Proofs of Theorems in the Appendix to this chapter to
facilitate readability of this section.
9.5.1
Neyman-Pearson Approach
While we will examine cases involving composite alternative hypotheses, we
begin with the case where both the null and alternative hypotheses are simple.
This situation is not representative of the typical hypothesis testing problem
where at least one of the hypotheses is composite (often, the alternative hypoth-
esis), but it provides a simpliﬁed context for motivating some of the basic
principles underlying the deﬁnition of a UMP test. We note that because the
alternative hypothesis consists of a single point in this case, we could drop the
adverb “uniformly,” and characterize our objective as ﬁnding the most powerful
test of the null hypothesis versus the alternative hypothesis.
Simple Hypotheses
The two simple hypotheses can be represented as H0: Q ¼
Q0 and Ha: Q ¼ Qa, where Q0 and Qa are parameter vectors (or scalars). A most
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
553

powerful test of H0 can be motivated by the following theorem, which is referred
to as the Neyman-Pearson Lemma. 12 Recall from Def. 9.6 that the notation
P(x ∈A;Q) represents the probability that x ∈A when Q is the value of the
parameter vector.
Theorem 9.1
Neyman-Pearson
Lemma
A rejection region of the form CrðkÞ ¼ fx : fðx; Q0Þ  kfðx; QaÞg; for k>0, is a
most powerful rejection region of level a ¼ P(x ∈Cr(k);Q0) ∈(0, 1) for testing
the hypothesis H0: Q ¼ Q0 versus Ha: Q ¼ Qa. Furthermore, Cr(k) is the unique
(with probability 1) most powerful rejection region of size a.
Proof
See Appendix.
n
Application of the Neyman-Pearson approach for constructing most power-
ful tests of H: Q ¼ Q0 versus Ha: Q ¼ Qa is relatively straightforward, at least in
principle. In particular, one chooses the value of k that deﬁnes the rejection
region in Theorem 9.1 having the desired size a ¼ P(x ∈Cr(k);Q0), and then the
most powerful statistical test of level a is deﬁned by
x 2
CrðkÞ
CrðkÞ
(
)
)
reject
do not reject
(
)
H0:
Note that since the probability calculation for determining the size of the
rejection region Cr(k) is based on the density f(x;Qo), values of x for which
f(x;Qo) ¼ 0 are irrelevant and can be ignored. Then an equivalent method
of ﬁnding an a-size most powerful test is to ﬁnd the value of k for which
P(f(x;Qa)/f(x;Qo)  k1) ¼ a, and deﬁne Cr(k) accordingly. Likewise, if the
supports13 of the densities f(x;Qo) and f(x;Qa) are the same, then another equiva-
lent method is to ﬁnd the value of k for which P(f(x;Qo)/f(x;Qa)  k) ¼ a, and use
it in the deﬁnition of Cr(k). We illustrate the procedure in the following
examples. Note that in the case where X is discrete, the feasible choices of a
can be somewhat limited, as will also be illustrated below.14
Example 9.10
UMP Test of Bernoulli
Means Using Neyman-
Pearson
Your company manufactures a hair-restoring treatment for middle aged balding
men and claims that the treatment will stimulate hair growth in 80 percent of
the men who use it. A competitor of the company claims that your scientists
must be referring to the complementary event by mistake—the counter claim is
12Neyman, J. and E.S. Pearson, “On the Problem of the Most Efﬁcient Tests of Statistical Hypotheses,” Phil. Trans., A, vol. 231, 1933,
p. 289.
13Recall that the support of a density function is the set of x-values for which f(x;Q0) > 0, i.e., {x:f(x;Q0) > 0} is the support of the
density f(x;Q0).
14This limitation can be overcome, in principle, by utilizing what are known as randomized tests. Essentially, the test rule is made to
depend not only on the outcomes of X but also on auxiliary random variables that are independent of X, so as to allow any level of test
size to be achieved. However, the fact that the test outcome can depend on random variables that are independent of the experiment
under investigation has discouraged its use in practice. For an introduction to the ideas involved, see Kendall, M. and A. Stuart, (1979)
The Advanced Theory of Statistics, Vol. 2, 4th Edition, New York: MacMillan, 1979, p. 180–181. Also, see problem 9.8.
554
Chapter 9
Hypothesis Testing Theory

that your treatment stimulates hair growth in only 20 percent of the men
who use it.
A government regulatory agency steps in to settle the dispute. They intend
to test the hypothesis that the treatment is 20 percent effective versus the
alternative hypothesis that it is 80 percent effective. A random sample of 20
middle aged balding men received your company’s hair-restoring treatment. The
success (xi ¼ 1) or failure (xi ¼ 0) of the chemical in stimulating hair growth for
any given individual is viewed as the outcome of a Bernoulli trial, where
Xi  pxi 1  p
ð
Þ1xiI 0;1
f
g xi
ð
Þ for p 2 O ¼ :2; :8
f
g:
The collection of 20 trials is viewed as a random sample from a Bernoulli
population distribution, and letting X ¼ (X1,. . .,X20) represent the random sam-
ple of size 20 from a Bernoulli distribution, we then have that
fðx; pÞ ¼ p
P20
i¼1 xi 1  p
ð
Þ20P20
i¼1 xi Y
20
i¼1
I 0;1
f
gðxiÞ;
p 2 O ¼ f:2; :8g:
In order to deﬁne the most powerful level a test of the hypothesis H0: p ¼ .2
versus the hypothesis Ha: p ¼ .8, use the Neyman-Pearson Lemma and examine
CrðkÞ ¼ x : fðx; :20Þ=fðx; :80Þ  k
f
g ¼
x : :25
ð
Þ
P20
i¼1 xið4Þ20P20
i¼1 xi  k

	
¼
x :
X
20
i¼1
xi
 
!
lnð:25Þ þ
20 
X
20
i¼1
xi
 
!
ln ð4Þ  lnðkÞ
(
)
¼
x :
X
20
i¼1
xi  10  :36067 ln ðkÞ
(
)
ðto five decimal placesÞ:
Since Z ¼ P20
i¼1 xi has a binomial distribution with parameters p and n ¼ 20, the
relationship between the choice of k and the size of the test implied by Cr(k) is
given by
aðkÞ ¼ P x 2 CrðkÞ; p ¼ :20
ð
Þ ¼
X
z10:36067 lnðkÞ
20
z


:20
ð
Þz :80
ð
Þ20zIf0;1;...;20gðzÞ:
Some possible choices of test size are given as follows:
ln(k)
10.36067 ln(k)
a(k)
24.95356
1
.9885
22.18094
2
.9308
19.40832
3
.7939
16.63571
4
.5886
13.86309
5
.3704
11.09047
6
.1958
8.31785
7
.0867
5.54524
8
.0321
2.77261
9
.0100
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
555

Note because of the discrete nature of the random variables involved, there are
no other choices of a within the range [.01, .9885] other than the ones displayed
above. The most powerful statistical test of H0: p ¼ .20 versus Ha: p ¼ .80
having level .01, say, is thus given by
x 2 Cr ) reject H0 : p ¼ :20 and x=2Cr ) do not reject H0 : p ¼ :20
where we use k ¼ exp(2.77261) ¼ 16 to deﬁne
Cr ¼ x : f x; :20
ð
Þ=f x; :80
ð
Þ  16
f
g:
The rejection region can be represented more simply using the test statistic t X
ð Þ
¼ Pn
i¼1 Xi as
Cr ¼
x :
X
20
i¼1
xi  9
(
)
:
Suppose upon observing the successes and failures in the 20 treatments, the
value of the test statistic was calculated to be P20
i¼1 xi ¼ 5. It follows that x =2 Cr,
and the hypothesis H0: p ¼ .2 is not rejected. The government concludes that
your competitor’s claim has merit, and instructs your company to either provide
further proof that their claim is correct, or else refrain from claiming that your
hair-restoring treatment is 80 percent effective. (Issue to consider: Would it be
better if the null hypothesis were H0: p ¼ .8 and the alternative hypothesis were
Ha: p ¼ .2 in performing the statistical test? Why or why not?).
□
The reader may have noticed that in Example 9.10, the choice of k used in
the Neyman-Pearson Lemma to deﬁne a UMP level a test of H0: Y ¼ Y0 versus
Ha: Y ¼ Ya was not unique. For example, any choice of k such that 2.77261 
ln(k) < 5.54524, would correspond to a rejection region C
r ¼
x : P20
i¼1 xi  k
n
o
for k* ∈(8,9] that would have deﬁned a UMP rejection region of level .01. This is
due to the fact that none of the values of P20
i¼1 xi 2 8; 9
ð
Þ, or equivalently, none
of the points in {x: 2.77261 < 1n(f(x;Y0)/f(x;Ya)) < 5.54524} are in the support of
f(x;Y0), i.e., the event P20
i¼1 xi 2 8; 9
ð
Þ has probability zero under f(x;Y0). Hence-
forth, we will always assume that when applying the Neyman-Pearson Lemma,
the value of k is chosen so that x-values in the event {x: f(x;Y0) ¼ kf(x;Ya)} are in
the support of f(x;Y0). Nothing of any practical consequence is lost by the
assumption, and it allows one to avoid an irrelevant ambiguity in the deﬁnition
of UMP tests.
Example 9.11
UMP Test of Two
Exponential Means
Using Neyman-Pearson
Your company is about to ship ten LCD screens to a notebook computer
manufacturing ﬁrm. The manufacturer requested that the screens have an
expected operating life of 10,000 hours. Your company manufactures two types
of screens—one having a 10,000-hour mean life, and one having a 50,000-hour
mean life. The label on the batch of ten screens you are about to send to the
manufacturer is missing, and it is not known which type of screen you are about
to send (although they were all taken from one of the screen production lines,
556
Chapter 9
Hypothesis Testing Theory

so it is known they are all of the same type). The screens with a mean life of
50,000 hours cost considerably more to produce, and so you would rather not
send the batch of ten screens if they were the more expensive variety. The
lifetimes of the screens can be nondestructively and inexpensively determined
by a test which you perform. Having observed the lifetime of the screens, you
wish to test the hypothesis that the mean lifetime of the screens is 10,000 hours
versus the hypothesis that their mean lifetime is 50,000 hours. The lifetime
distribution of each type of screen belongs to the exponential family of
distributions as
Xi  y1exi=yI 0;1
ð
Þ xi
ð
Þ; y 2 O ¼ 1; 5
f
g
where xi is measured in 10,000 hour units.
Let X ¼ (X1,. . .,X10) be the random sample of size 10 from the appropriate
(unknown) exponential population distribution, so that
fðx; yÞ ¼ y10 exp 
X
10
i¼1
xi=y
 
!
P
10
i¼1 Ið0;1Þ xi
ð
Þ; y 2 O ¼ f1; 5g:
In order to deﬁne the most powerful level a test of the hypothesis H0: y ¼ 1
versus the hypothesis Ha: y ¼ 5, use the Neyman-Pearson Lemma and examine
CrðkÞ ¼ x : fðx; 1Þ=fðx; 5Þ  k
f
g
¼
x : :2
ð
Þ10 exp :8
X
10
i¼1
xi
 
!
 k
(
)
¼
x : 10 lnð:2Þ  :8
X
10
i¼1
xi  lnðkÞ
(
)
¼
x :
X
10
i¼1
xi  20:11797  1:25 lnðkÞ
(
)
ðto five decimal placesÞ:
Since Z ¼ P10
i¼1 Xi has a gamma distribution with parameters a ¼ 10 and b ¼ y,
the relationship between the choice of k and the size of the test implied by Cr(k)
is given by
aðkÞ ¼ Pðx 2 CrðkÞ; y ¼ 1Þ ¼
ð1
20:117971:25 lnðkÞ
1
G 10
ð
Þz9 expðzÞdz;
where we are integrating the gamma distribution with a ¼ 10 and b ¼ 1. Given
the continuity of the random variables involved, any choice for the size of the
test, a, within the range (0,1) is possible. For example, to deﬁne the most
powerful statistical test of H0: y ¼ 1 versus Ha: y ¼ 5 having level .05, the
value of k would be found by solving the integral equation a(k) ¼ .05 for k (recall
the deﬁnition of a(k) above). Through the use of numerical integration on a
computer, it can be shown that
ð1
15:70522
1
G 10
ð
Þz9expðzÞdz ¼ :05 to five decimal places).
ð
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
557

Therefore, k ¼ 34.13079 (to ﬁve decimal places), and the most powerful
statistical test of level .05 is given by
x 2 Cr ) reject H0 : y ¼ 1 and x =2 Cr ) do not reject H0 : y ¼ 1
where
Cr ¼ x : f x; 1
ð
Þ f x; 5
ð
Þ
=
 34:13079
f
g:
The rejection region can be represented more simply using the test statistic t X
ð Þ
¼ Pn
i¼1 Xi as Cr ¼
x : P
10
i¼1
xi  15:70522

	
:
Suppose the sum of the lifetimes of the ten screens ultimately had an outcome
equal to P10
i¼1 xi ¼ 54:4. It follows that x ∈Cr, and the hypothesis H0: y ¼ 1 is
rejected at the .05 level. You conclude that the batch of ten screens are the type
having a mean life of 50,000 hours. (Issue to consider: Would it be better if the
null hypothesis were H0: y ¼ 5 and the alternative hypothesis were Ha: y ¼ 1 in
performing the statistical test? Why or why not?)
□
The density ratio implied in the statement of the Neyman-Pearson Lemma
can itself be viewed as a test statistic, i.e. t(X) ¼ f(X;Qa)/f(X;Q0) is a test statistic
for given (hypothesized) values of Q0 and Qa. The test implied by the lemma can
then be conducted by determining whether t(x)  c ¼ k1. We also note that the
Neyman-Pearson Lemma can be (and often is) stated in terms of likelihood
functions instead of probability density functions. The alternative statement
of the lemma then utilizes
CrðkÞ ¼ fx : LðQ0; xÞ  kLðQa; xÞg; for k>0
in the statement of Theorem 9.1. Of course, the proof of the restated lemma
would be identical with that of Theorem 9.1 with f(x;Q0) and f(x;Qa) replaced by
L(Q0;x) and L(Qa;x). When the rejection region is expressed in terms of likeli-
hood ratio values as Cr ¼ {x: L(Qa;x)/L(Q0;x)  k1}, the test implied by the
lemma is then referred to as a likelihood ratio test.
A most powerful test of H0: Q ¼ Q0 versus Ha: Q ¼ Qa deﬁned via the
Neyman-Pearson Lemma is also an unbiased test, as we state formally below.
Theorem 9.2
Unbiasedness of
Neyman-Pearson
Most Powerful Test of
H0 : Q ¼ Q0 versus
Ha : Q ¼ Qa
Let Cr represent a most powerful level a rejection region deﬁned by the
Neyman-Pearson Lemma for testing H0: Q ¼ Q0 versus Ha: Q ¼ Qa. Then the
test implied by Cr is unbiased.
Proof
Let Z represent a Bernoulli random variable that is independent of the random
sample X and for which p(z ¼ 1) ¼ a and p(z ¼ 0) ¼ 1  a. Suppose that regard-
less (i.e., independent) of the outcome of X, the hypothesis H0 is subsequently
rejected or accepted based on whether the outcome of Z is in the rejection region
C
r ¼ {1}. Let EX and EZ represent expectations taken with respect to the
558
Chapter 9
Hypothesis Testing Theory

probability distributions of X and Z, respectively. It follows by the independence
of X and Z that EZ ðI 1
f gðZÞjxÞ ¼ a is the probability of rejecting H0 conditional on
any given outcome of X, regardless of the value of Q ∈{Q0, Qa}. Then the
(unconditional) probability of rejecting H0 is given by an application of the
iterated expectation theorem as
pC
r Q
ð
Þ ¼ P
 reject H0; Q
ð
Þ ¼ EXEZ Ið1ÞðZÞjX


¼ a;
Q 2 Q0; Qa
f
g:
Now let Cr represent a most powerful level a test of H0: Q ¼ Q0 versus Ha:
Q ¼ Qa deﬁned via the Neyman-Pearson Lemma. Using an argument analogous
to that used in the sufﬁciency proof of the lemma (replacing IC
r (x) with I{1}(z)),
ICrðxÞ  If1g ðzÞ


f x; Qa
ð
ÞhðzÞ  k1 ICrðxÞ  I½1gðzÞ


f x; Q0
ð
ÞhðzÞ;
8x; z;
where h(z) represents the Bernoulli density function for Z. Integrating both sides
of the inequality over x 2 Rn if X is continuous or else summing over all x-values
for which f(x;Q0) > 0 or f(x;Qa) > 0 if X is discrete, and then summing over the
range of Z obtains
P x 2 Cr; Qa
ð
Þ  a  k1 P x 2 Cr; Q0
ð
Þ  a
½
 ¼ 0
where the right hand side of the inequality equals zero because Cr is an a-size
rejection region. Then P(x ∈Cr;Qa)  a, so that the test implied by Cr is
unbiased.
n
The signiﬁcance of Theorem 9.2 for applications is that once a most power-
ful level a test of H0: Q ¼ Q0 versus Ha: Q ¼ Qa has been derived via the
Neyman-Pearson Lemma, there is no need to check whether the test is unbi-
ased, since such tests are always unbiased. As an illustration, the tests in
Examples 9.10–9.11 are unbiased because they were most powerful Neyman-
Pearson tests. We note, however, that unbiased tests are not necessarily most
powerful.
Composite Hypotheses
In some cases, the Neyman-Pearson Lemma can also be
used to identify UMP tests when the alternative hypotheses is composite. The
basic idea is to show via the Neyman-Pearson Lemma that the rejection region,
Cr, of the most powerful test of H0: Q ¼ Q0 versus Ha: Q ¼ Qa is the same
8 Qa ∈Oa, where Oa is the set of alternative hypothesis values. It would then
follow that the rejection region deﬁnes a uniformly most powerful test of
H0: Q ¼ Q0 versus Ha: Q ∈Oa, since the same rejection region would be most
powerful for each and every Q ∈Oa, i.e., uniformly in Q ∈Oa. We formalize the
approach in the following theorem.
Theorem 9.3
UMP Test of
H0 : Q ¼ Q0
versus Ha :Q 2 Oa
Using Neyman-Pearson
Lemma
The given rejection region, Cr, of a statistical test of H0: Q ¼ Q0 versus
Ha: Q ∈Oa deﬁnes a UMP level a test if P(x ∈Cr;Q0) ¼ a and ∃k(Qa)  0 such
that the given rejection region can be deﬁned equivalently by Cr ¼ {x: f(x;Q0)/
f(x;Qa)  k(Qa)}, 8 Qa ∈Oa, . Furthermore, Cr is then the unique (with probabil-
ity 1) UMP rejection region of size a.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
559

Proof
Because Cr is deﬁned via the Neyman-Pearson Lemma, it represents the most
powerful level a test of H0: Q ¼ Q0 versus Ha: Q ¼ Qa. Since the same and
unchanged rejection region, Cr, applies for any Qa ∈Oa, the test deﬁned by Cr
is also most powerful for any choice of Qa ∈Oa, and is thus a UMP level a test of
H0: Q ¼ Q0 versus Ha: Q ∈Oa. Finally, Cr is the unique size a rejection region
(with probability 1) since the most powerful rejection region of size a is unique
(with probability 1) by the Neyman-Pearson Lemma 8Qa.
n
Example 9.12
UMP Test of
Bernoulli Mean
Against Composite
Alternative
Recall Example 9.10 in which hypotheses regarding the effectiveness of a hair
restoring treatment were being analyzed. Examine the problem of testing the
simple hypothesis H0: p ¼ .2 versus the composite alternative hypothesis
Ha: p > .2 using a UMP test having level .01. Let pa represent any choice of
p ∈(.2,1], and deﬁne the rejection region of the test of H0: p ¼ .2 versus
Ha: p ¼ pa using the Neyman-Pearson Lemma:
Cr ¼ fx : fðx; :2Þ=fðx; paÞ  kðpaÞg
¼
x : :2=pa
ð
Þ
P20
i¼1 xi :8= 1  pa
ð
Þ
ð
Þ20P20
i¼1 xi  k pa
ð
Þ

	
¼
x :
X
20
i¼1
xi  lnðkðpaÞÞ  20 lnð:8=ð1  paÞÞ
lnð:25ð1  paÞ= paÞ
(
)
:
Using reasoning identical to that used in Example 9.10, Cr will deﬁne a size .01
test iff
Cr ¼
x :
X
20
i¼1
xi  9
(
)
regardless of the value of pa, since under H0 the statistic P20
i¼1 Xi has a binomial
distribution with n ¼ 20 and p ¼ .2. The nonnegative values of k(pa) 8pa ∈(.2,1]
can be solved for accordingly, and thus by Theorem 9.3 Cr ¼
x : P20
i¼1 xi  9
n
o
deﬁnes a UMP level .01 test of H0: p ¼ .2 versus Ha: p > .2.
□
Example 9.13
UMP Test of
Exponential Mean
Against Composite
Alternative
Recall Example 9.11 in which hypotheses regarding the operating lives of
screens for notebook computers were being analyzed. Examine the problem of
testing the simple hypothesis H0: y ¼ 1 versus the composite hypothesis
Ha: y > 1 using a UMP test having level .05. Let y a represent any choice of
y > 1, and deﬁne the rejection region of the test of H0: y ¼1 versus Ha: y ¼ y a
using the Neyman-Pearson Lemma:
Cr ¼ x : fðx; 1Þ=fðx; yaÞ  kðyaÞ
f
g
¼
x : y10
a exp 
X
10
i¼1
xi 1  y1
a


 
!
 kðyaÞ
(
)
¼
x :
X
10
i¼1
xi  10 lnðyaÞ  lnðkðyaÞÞ
ð1  y1
a Þ
(
)
560
Chapter 9
Hypothesis Testing Theory

Using reasoning identical to that used in Example 9.11, Cr will deﬁne a size .05
test iff
Cr ¼
x :
X
10
i¼1
xi  15:70522
(
)
regardless of the value of y a, because under H0 the statistic P10
i¼1 Xi has a gamma
distribution with parameters a ¼ 10 and b ¼ 1. The nonnegative values of k(y a)
8ya ∈(1,1)
can
be
solved
for
accordingly,
and
thus
by
Theorem
9.3,
Cr ¼
x : P
10
i¼1
xi  15:70522

	
deﬁnes a UMP level .05 test of H0: y ¼1 versus
Ha: y > 1.
□
The uniqueness result of Theorem 9.3 can also be used to demonstrate that a
UMP test of H0: Q ¼ Q0 versus Ha: Q ∈Oa having size a does not exist, as in the
following example.
Example 9.14
Demonstrating
Nonexistence
of a UMP Test
An untrusting gambler wishes to use a size .10 test that is also a UMP level .10
test of the hypothesis that a roulette wheel being used for betting purposes in an
Atlantic City Casino is fair. The gambler suggests that the wheel be spun 100
times, and a random sample of red and black outcomes be used to test the
“fairness” hypothesis. The joint density of the random sample in this case is
given by
f x; p
ð
Þ ¼ p
P100
i¼1 xi 1  p
ð
Þ100P100
i¼1 xi Y
100
i¼1
I 0;1
f
g xi
ð
Þ
assuming the red/black outcomes are iid Bernoulli trials, where xi ¼ 1 denotes
red and xi ¼ 0 denotes black. The null hypothesis to be tested is H0: p ¼ .5
versus the alternative hypothesis that Ha: p 6¼ .5. In the notation of Theorem
9.43, Oa ¼ [0,1]  {.5}. Following Theorem 9.3, deﬁne a rejection region as
Cr ¼
x : fðx; :5Þ
f x; pa
ð
Þ  k pa
ð
Þ

	
¼
x : :5
ð
Þ100= p
P100
i¼1 xi
a
1  pa
ð
Þ100P100
i¼1 xi


 k pa
ð
Þ

	
¼
x : ln pa= 1  pa
ð
Þ
ð
Þ S
100
i¼1xi  g pa
ð
Þ

	
;
where g(pa) ¼ 100 ln(.5/(1  pa))  ln(k(pa)). The rejection region depends on pa
(i.e., Cr is not the same 8 pa ∈Oa), and thus there is no UMP level .10 statistical
test of the “fairness” hypothesis that has size .10.
To show how Cr depends on pa, note that for pa > .5, ln(pa/(1  pa)) > 0,
while for pa < .5, ln(pa/(1  pa)) < 0. It follows that
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
561

Cr ¼
x :
X
100
i¼1
xi   pa
ð
Þ
(
)
if pa>:5
or
Cr ¼
x :
X
100
i¼1
xi   pa
ð
Þ
(
)
if pa<:5;
where (pa) ¼ g(pa)/ln(pa/(1  pa)). Given the test size of .10, the reader can
show by using the binomial distribution of Pn
i¼1 Xi with parameters n ¼ 100
and p ¼ .5 that P P100
i¼1 xi  57; p ¼ :5


¼ P P100
i¼1 xi  43; p ¼ :5


¼ :10 (to two
decimal places). Thus, for P(x ∈Cr;5) ¼ .10, two different rejection regions are
deﬁned as
Cr ¼
x :
X
100
i¼1
xi  57
(
)
for pa>:5
or
Cr ¼
x :
X
100
i¼1
xi  43
(
)
for pa<:5:
For values of pa > .5, the ﬁrst rejection region is UMP of level .10 by Theorem
9.3, while for values of pa < .5 the second rejection region is UMP of level .10 by
Theorem 9.3. Since the UMP level .10 rejection regions for pa > .5 and pa < .5
are not the same, no UMP level .10 rejection region exists for testing the fairness
hypothesis H0: p ¼ .5 versus Ha: p 6¼ .5.
□
A UMP level a test of H0: Q ¼ Q0 versus Ha: Q ∈Oa deﬁned using Theorem
9.3 is also an unbiased test, as indicated in the following extension of Theorem
9.2.
Theorem 9.4
Unbiasedness of
Uniformly Most
Powerful Test of
H0 : Q ¼ Q0 versus
Ha : Q 2 Oa
Let Cr represent a UMP level a rejection region for testing H0: Q ¼ Q0 versus Ha:
Q ∈Oa deﬁned using the Neyman-Pearson Lemma as indicated in Theorem
9.3. Then the test implied by Cr is unbiased.
Proof
Let Qa be any choice of Q ∈Oa. Using an argument analogous to the proof of
Theorem 9.2, it can be shown that P(x ∈Cr; Qa)  a  k(Qa)1 [P(x ∈Cr;Q0) 
a] ¼ 0 where, again, the right hand side of the inequality is zero because Cr is a
size a rejection region. Since this holds 8 Qa ∈Oa, it follows that P(x ∈Cr;Qa)
 a 8 Qa ∈Oa, so that the test implied by Cr is unbiased.
n
562
Chapter 9
Hypothesis Testing Theory

The theorem implies that once a UMP level a test of H0: Q ¼ Q0 versus
Ha: Q ∈Oa has been found via the Neyman-Pearson approach, one need not
check to see whether the test is unbiased, since such tests are always unbiased.
Examples 9.12 and 9.13 illustrate this fact, where both UMP level a tests are also
unbiased tests. However, we underscore that an unbiased test is not necessarily
a UMP test.
One-Sided and Two-Sided Alternative Hypotheses
The previous three examples
illustrate the concept of one-sided and two-sided alternative hypotheses. In the
current context, a one-sided alternative hypothesis is such that either Qa > Q0
8 Qa ∈Oa, or Qa < Q0 8 Qa ∈Oa, i.e., all values of Qa ∈Oa are larger than Q0, or
else they are all smaller than Q0, so that the values of Qa are all on “one-side” of
Q0. A two-sided alternative hypothesis is such that ∃Qa ∈Oa for which Qa >
Q0 and ∃Qa ∈Oa for which Qa < Q0, i.e., some values of Qa ∈Oa are larger
than Q0 and some values of Qa ∈Oa are smaller than Q0, so there are values of
Qa on “both-sides” of Q0. In practice, UMP level a tests of simple hypotheses
versus one-sided alternative hypotheses often exist when Q is a scalar, but
such is not the case when the alternative hypothesis is two-sided. In the latter
case, one must generally resort to seeking a UMP test within a smaller class of
tests, such as the class of unbiased tests. We will examine this case in more
detail later.
9.5.2
Monotone Likelihood Ratio Approach
Results in this section will be useful for constructing UMP level a tests of the
composite null hypothesis H0: Q  Q0 (or H0: Q  Q0) versus the composite one-
sided alternative hypothesis Ha: Q > Q0 (or Ha: Q < Q0). The results in this
section apply equally well to the case where H0: Q ¼ Q0, and thus, this section
can be interpreted as also providing additional results for testing H0: Q ¼
Q0 versus the composite one-sided alternative Ha: Q > Q0 (or Ha: Q < Q0).
The procedure for deﬁning UMP level a tests that we will present relies on
the concept of a monotone likelihood ratio in the statistic T ¼ t(X).
Deﬁnition 9.13
Monotone Likelihood
Ratio in the Statistic
T ¼ t(X)
Let f(x;Y), Y ∈O, be a family of probability density functions indexed by a
scalar parameter Y. The family of density functions is said to have a mono-
tone likelihood ratio in the statistic T ¼ t(X) iff 8 Y1,Y2 ∈O for which
Y1 > Y2, the likelihood ratio L(Y1;x)/L(Y2;x) is a nondecreasing function of
t(x) 8 x ∈{x: f(x;Y1) > 0 and/or f(x;Y2) > 0}.
Veriﬁcation of whether a family of density functions has a monotone likeli-
hood ratio in some statistic t(X) generally requires some ingenuity. However,
if the family of density functions belongs to the exponential class of densities
the veriﬁcation process is often simpliﬁed by the following result.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
563

Theorem 9.5
Monotone Likelihood
Ratio and the
Exponential Class of
Densities
Let f(x;Y), Y ∈O, be a one-parameter exponential class family of densities
fðx; YÞ ¼ expðcðYÞg x
ð Þ þ dðYÞ þ z x
ð ÞÞIA x
ð Þ; Y 2 O:
If c(Y) is a nondecreasing function of Y, then f(x;Y), Q ∈O has a monotone
likelihood ratio in the statistic g(X).
Proof
Let Y1 > Y2, and 8x ∈A examine the likelihood ratio
LðQ1; xÞ=LðQ2; xÞ ¼ exp cðQ1Þ  cðQ2Þ
½
gðxÞ þ dðQ1Þ  dðQ2Þ
ð
Þ ¼ ðgðxÞÞ
¼ exp c
g x
ð Þ þ d
ð
Þ
where c* ¼ c(Q1)  c(Q2) and d* ¼ d(Q1)  d(Q2). Since c(Q) is a nondecreasing
function of Q, c*  0 so that the likelihood ratio can be expressed as a nonde-
creasing function of g(x), 8 x ∈A.
n
The following examples illustrate the use of Theorem 9.5 for verifying the
monotone likelihood ratio property.
Example 9.15
Monotone Likelihood
Ratio for Bernoulli
Distribution
Let (X1,. . .,Xn) be a random sample from a Bernoulli distribution representing the
population of television viewers in a certain region who can (xi ¼ 1) or cannot
(xi ¼ 0) recall seeing a certain television commercial. Then
fðx; pÞ ¼ p
Pn
i¼1 xi 1  p
ð
ÞnPn
i¼1 xi Y
n
i¼1
I 0;1
f
g xi
ð
Þ
¼ expðcðpÞgðxÞ þ dðpÞ þ zðxÞÞIAðxÞ; p 2 ð0; 1Þ;
where c(p) ¼ ln(p/(1  p)), g(x) ¼ Pn
i¼1 xi, d(p) ¼ n ln(1  p), z(x) ¼ 0, and A ¼
n
i¼1 {0,1}. Then because dc(p)/dp ¼ [p(1  p)]1 > 0, c(p) is strictly increasing in
p, and f(x;p), p ∈(0,1) has a monotone likelihood ratio in the statistic g(X) ¼
Pn
i¼1 Xi.
□
Example 9.16
Monotone Likelihood
Ratio for the Gamma
Distribution
Let (X1,. . .,Xn) be a random sample from a gamma population distribution with
b ¼ 2 representing the survival time of cancer patients treated with a new form
of chemotherapy. Then
fðx; aÞ ¼
1
2na GðaÞ
½
n
Y
n
i¼1
xa1
i
 
!
ePn
i¼1 xi=2 Y
n
i¼1
I 0;1
ð
Þ xi
ð
Þ
¼ expðcðaÞgðxÞ þ dðaÞ þ zðxÞÞIAðxÞ; a>0;
564
Chapter 9
Hypothesis Testing Theory

wherec(a) ¼ a  1,g(x) ¼ Pn
i¼1 ln xi
ð
Þ,d(a) ¼ nln(2aG(a)),z(x) ¼  1=2
ð
Þ Pn
i¼1 xi;
and A ¼ n
i¼1 0; 1
ð
Þ . Then because dc(a)/da ¼ 1 > 0, c(a) is strictly increasing
in a and f(x;a), a > 0, has a monotone likelihood ratio in the statistic g(X) ¼
Pn
i¼1 ln Xi
ð
Þ.
□
In the next example, we illustrate veriﬁcation of the monotone likelihood
ratio property for a family of density functions that does not belong to the
exponential class of densities.
Example 9.17
Monotone Likelihood
Ratio for the
Hypergeometric
Distribution
Let X have a hypergeometric probability density function in which the
parameters n and M have known values n0 and M0, respectively, with X
representing the number of defectives found in a random sample, without
replacement, of size n0 from a shipment of M0 DVDs. The density function for
X can then be represented as
fðx; KÞ ¼
K
x


M0  K
n0  x


M0
n0


If0;1;2;...;Kg ðxÞ for K 2 0; 1; 2; . . . ; M0
f
g:
Examine the likelihood ratio
LðK; xÞ
LðK  1; xÞ ¼
K
ðK  xÞ

 ðM0 K  n0 þ1 þ xÞ
ðM0 K þ 1Þ


If0;1;...;Kg ðxÞ
If0;1;...;K1g ðxÞ


(recall Example 8.8), and note that the ratio is a nondecreasing function of
x ∈{0,1,. . .,K} ¼ {x: f(x;K) > 0 and/or f(x; K  1) > 0}, because the ratio is a
product of three nondecreasing functions (in brackets) of x. Now let K1 > K2,
and note that the likelihood ratio
L K1;x
ð
Þ
L K2;x
ð
Þ ¼
Y
K1K21
i¼0
L K1  i;x
ð
Þ
L K1  i þ 1
ð
Þ;x
ð
Þ


¼
Y
K1K21
i¼0
K1  i
K1  i  x


M0  K1  i
ð
Þ  n0 þ 1 þ x
ð
Þ
M0  K1  i
ð
Þ þ 1
ð
Þ




for x 2 f0;1;...;K2g
¼ 1 for x 2 K2 þ 1;...;K1
f
g
is a nondecreasing function of x ∈{0,1,. . .,K1} ¼ {x: f(x;K1) > 0 and/or f(x;
K2) > 0}, because it is strictly increasing for x ∈{0,1,. . .,K2}, and equals 1 (and
hence nondecreasing) for x ∈{K2 + 1,. . .,K1}. Then the hypergeometric family of
densities f(x;K), K ∈{0,1,. . .,M0}, has a monotone likelihood ratio in the statistic
t(X) ¼ X.
□
If the joint density function of the random sample can be shown to have a
monotone likelihood ratio in some statistic, then UMP level a tests will exist for
H0: Y  Y0 versus the one-sided alternative Ha: Y > Y0, or for H0: Y 
Y0 versus the one-sided alternative Ha: Y < Y0, or for H0: Y ¼ Y0 versus the
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
565

two-sided alternative of either Ha: Y > Y0 or Ha: Y < Y0. In fact, it will be seen
below that a monotone likelihood ratio is effectively a sufﬁcient condition for
the Neyman-Pearson approach of Theorem 9.3 to be applicable to a testing
problem.
Theorem 9.6
Monotone Likelihood
Ratios and
UMP Level a Tests
Let f(x;Y), Y ∈O, be a family of density functions having a monotone likeli-
hood ratio in the statistic t(X). Let Cr ¼ {x: f(x;Y0)  k f(x;Ya)} deﬁne a size
a rejection region for testing H0: Y ¼ Y0 versus Ha: Y ¼ Ya.
1. If Y0 < Ya, Cr is UMP level a for testing either
a. H0: Y ¼ Y0 versus Ha: Y > Y0, or
b. H0: Y  Y0 versus Ha: Y > Y0;
2. If Y0 > Ya, then Cr is UMP level a for testing either
a. H0: Y ¼ Y0 versus Ha: Y < Y0, or
b. Ha: Y  Y0 versus Ha: Y < Y0.
Proof
See Appendix.
n
The UMP level a rejection region deﬁned by Theorem 9.6 can be deﬁned
alternatively in terms of the statistic t(X) for which the likelihood ratio is
monotone.
Corollary 9.1 UMP
Level a Tests in Terms of
the t(X) of a Monotone
Likelihood Ratio
Let f(x;Y),Y ∈O, be a family of density functions having a monotone likelihood
ratio in t(X). Then
1. Cr ¼ {x: t(x)  c}, for choice of c such that P(t(x)  c;Y0) ¼ a, is a UMP level
a rejection region for testing H0: Y ¼ Y0 versus Ha: Y > Y0, or H0: Y 
Y0 versus Ha: Y > Y0.
2. Cr ¼ {x: t(x)  c}, for choice of c such that P(t(x)  c;Y0) ¼ a, is a UMP level
a rejection region for testing H0: Y ¼ Y0 versus Ha: Y < Y0, or H0: Y 
Y0 versus Ha: Y < Y0.
Proof
The Cr0s are equivalent representations of the Neyman-Pearson rejection regions
referred to in Theorem 9.6.
n
It is also true that the UMP level a tests based on the monotone likelihood
ratio procedure are unbiased tests. We state this fact as a second corollary to that
theorem.
566
Chapter 9
Hypothesis Testing Theory

Corollary 9.2
Unbiasedness of UMP
Tests Based on
Monotone Likelihood
Ratios
The rejection region, Cr, deﬁned in Theorem 9.6 and Corollary 9.1 deﬁnes an
unbiased size a test of the respective hypotheses stated in the theorem.
Proof
This follows immediately from the characteristics of the power function of the
test established in the proof of Theorem 9.6.
n
Example 9.18
UMP Test in
Exponential
Distribution Based on
Monotone Likelihood
Ratio
Your company manufactures personal computers and is in the process of
evaluating the purchase of hard disk controllers from various input suppliers.
Among other considerations, a necessary condition for a disk controller to be
used in the manufacture of your PCs is that the controller have a minimum
expected life of more than 50,000 operating hours. The lifetimes of all of the
various brands of disk controllers are characterized by exponential densities, as
Xi  f xi; y
ð
Þ ¼ y1exp xi=y
½
I 0;1
ð
ÞðxÞ; y>0;
where xi is measured in 10,000’s of operating hours. Consider deﬁning a UMP
level .01 test of H0: y  5 versus Ha: y > 5 based on a random sample of 100
lifetimes of a given brand of disk controller.
The
joint
density
of
the
random
sample
is
given
by
f(x;y) ¼
y100exp  P100
i¼1 xi=y
h
i Q100
i¼1 I 0;1
ð
Þ xi
ð
Þ;
y>0: The density is in the exponential
class with cðyÞ ¼ y1 , which is a nondecreasing function of y, so that f(x;y)
has a monotone likelihood ratio by Theorem 9.6. In particular, the likelihood
ratio
L(y1;x)/L(y2;x) ¼
exp  P100
i¼1 xi y1
1
 y1
2


h
i
is
a
strictly
increasing
function of t(x) ¼ P100
i¼1 xi for y1 > y2 8x ∈100
i¼1 (0,1). Then Theorem 9.6 and
its corollaries are applicable so that the UMP rejection region of level .01 is given
by Cr ¼ {x: P100
i¼1 xi  c} for c chosen so that P(x ∈Cr; y ¼ 5) ¼ .01.
To calculate the appropriate value of c, ﬁrst note that Z ¼ P100
i¼1 Xi has a
gamma distribution, with parameters a ¼ 100 and b ¼ y.15 Then c is the solu-
tion to the integral equation
ð1
c
1
y100G 100
ð
Þ
z99expðz=yÞdz ¼ :01
with y ¼ 5, which can be found with the aid of a computer to be c ¼ 623.61. The
UMP level .01 and unbiased test of H0: y  5 versus Ha: y > 5 is then deﬁned by
the rejection region for the test statistic T ¼ t(X) ¼ Pn
i¼1 Xi given by CT
r
¼ [623.61,1). An alternative test statistic for performing this UMP level .01
15This can be shown via the MGF approach, since the MGF of P100
i¼1 Xi ¼ Q100
i¼1 MXiðtÞ ¼ Q100
i¼1 1  yt
ð
Þ1 ¼ 1  yt
ð
Þ100 for t<y1, which
is of the gamma form with b ¼ y, a ¼ 100.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
567

and unbiased test is the sample mean so that the test can also be conducted using
the rejection region for the test statistic X given by Cx
r ¼ [6.2361,1).
□
Example 9.19
UMP Test in
Hypergeometric
Distribution Based on
Monotone Likelihood
Ratio
A shipment of 300 blank recordable DVDs arrives at your video store. You intend
to randomly sample, without replacement, 25 DVDs and observe whether or not
they are defective. On the basis of the sample outcome, you wish to test the
hypothesis that the shipment of DVDs contains no more than 5 percent
defectives. We seek a UMP and unbiased test of the hypothesis having level
.05 and a size as close to .05 as possible.16
The density function associated with the number of defectives observed in a
random sample, without replacement, of size 25 from the population of 300
DVDs can be deﬁned as
f x; K
ð
Þ ¼
K
x


300  K
25  x


300
25


I 0;1;2;...;25
f
gðxÞ;
i.e., hypergeometric with M ¼ 300, n ¼ 25, and K ∈{0,1,. . .,300}. As we have
shown in Example 9.17, this hypergeometric density has a monotone likelihood
ratio in the statistic t(X) ¼ X. Note that the likelihood ratio L(K1;x)/L(K2;x),
K1 > K2, is in fact strictly increasing in t(x) ¼ x for all values of x in the support
of f(x;K2), as was shown in Example 9.17. Theorem 9.6 and its corollaries are
applicable so that the UMP rejection region of size a for testing H0: K  15
versus Ha: K > 15 is represented by Cr ¼ {x: x  c} for c chosen so that
P(x ∈Cr; K ¼ 15) ¼ a. Computing the values of the hypergeometric CDF on
the computer reveals that a ¼ .027, when c ¼ 4. This is the choice of c that
generates a value of a closest to .05 without exceeding it. Thus, a UMP level .05
and unbiased size .027 test of H0: K  15 versus Ha: K > 15 in a random sample,
without replacement, of size 25 from the population of 300 DVDs is deﬁned by
the rejection region Cr ¼ [4, 25] for x.
□
There does not generally exist a UMP level a test of the hypothesis H0:
Y ¼ Y0 versus the two-sided alternative Ha: Y 6¼ Y0 in the case of monotone
likelihood ratios. This follows from the fact that the UMP level a rejection
region of the test of such a hypothesis is different, depending on “which side”
of Y0 the alternative value Y is on. We demonstrate this phenomenon for the
case where the monotone likelihood ratio statistic is a continuous random
variable.
16As we noted previously, it is possible to use a randomized test to achieve a size of .05 exactly, but the test can depend on the outcome
of a random variable that has nothing to do with the experiment being analyzed. See problem 9.8 for an example of this approach.
Randomized tests are not often used in practice.
568
Chapter 9
Hypothesis Testing Theory

Theorem 9.7
Nonexistence of UMP
Level a Test of
H0 : Y ¼ Q0 versus
Ha : Y 6¼ Q0 for
Monotone Likelihood
Ratios
Let f(x;Y), Y ∈O, be a family of PDFs having a monotone likelihood ratio in t(x)
where t(X) is a continuous random variable. Then there does not exist a UMP
level a ∈(0, 1) rejection region for testing H0: Y ¼ Y0 versus Ha: Y 6¼ Y0.
Proof
From Theorem 9.6 and Corollary 9.1, the UMP size a rejection region
for testing H0: Y ¼ Y0 versus Ha: Y > Y0 is of the form Cr ¼ {x: t(x)  c}
with P(t(x)  c) ¼ a while the UMP size a rejection region for testing
H0: Y ¼ Y0 versus Ha: Y < Y0 is of the form Cr ¼ {x: t(x)  c} with P(t(x)  c)
¼ a. Appropriate choices of c for either set of hypotheses exist 8a ∈(0,1) since
t(X) is a continuous random variable. It follows that there is no rejection
region, C
r, that is UMP level a for both Y < Y0 and Y > Y0.
n
An implication of Theorem 9.7 is that we must consider alternative criteria
than the UMP property when deﬁning statistical tests of H0: Y ¼ Y0 versus H0:
Y 6¼ Y0 and the sampling density has a monotone likelihood ratio. A similar,
albeit somewhat more complicated, argument can also be made when t(X) is a
discrete random variable. In the next subsection we will examine situations in
which UMP level a tests of H0: Y ¼ Y0 versus Ha: Y 6¼ Y0 exist within the
unbiased class of tests.17 A UMP test within the class of unbiased tests will be
referred to as a Uniformly Most Powerful Unbiased (UMPU) test.
9.5.3
UMP and UMPU Tests in the Exponential Class of Densities
For cases where the joint density of the probability sample belongs to the
exponential class of densities with a scalar parameter Y, our preceding discus-
sion of monotone likelihood ratios can be used to justify the general existence of
UMP level a tests for the case of the one-sided alternatives
H0 : Y ¼ Y0 versus Ha : Y>Y0;
H0 : Y ¼ Y0 versus Ha : Y<Y0;
H0 : Y  Y0 versus Ha : Y>Y0;
H0 : Y  Y0 versus Ha : Y<Y0;
and the general nonexistence of a UMP level a test for the case of the two-sided
alternative
H0 : Y ¼ Y0 versus Ha : Y 6¼ Y0:
17To this point, we have established UMP tests in the class of all tests of a certain level a. The reader should note that in all cases
examined heretofore, we have shown that UMP level a tests were also unbiased. This is clearly different than examining only
unbiased tests of a certain level a, and within this restricted set of tests, attempting to ﬁnd one that is UMP.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
569

We now reconsider the problem of deﬁning a test of the latter hypothesis.
We will ﬁnd that if the joint density of the probability sample belongs to the
exponential class with a scalar parameter Y, then it will generally be possible to
ﬁnd a rejection region that is UMP level a within the class of unbiased rejection
regions for testing H0: Y ¼ Y0 versus Ha: Y 6¼ Y0. We will also establish UMPU
rejection regions for testing H0: Y1  Y  Y2 versus Ha: Y =2 [Y1,Y2].18 Our
discussion will be facilitated by the following result concerning the differentia-
bility of the expectation of a function taken with respect to an exponential class
density.
Lemma 9.1
Differentiability of
g(Y) ¼ EY(f(X)) in the
Case of an Exponential
Class Density
Let f(x;Y) ¼ exp(c(Y)g(x) + d(Y) + z(x))IA(x), Y ∈O, be an exponential class
density with O deﬁned as an open interval contained in R, and with c(Y) and
d(Y) being differentiable functions of Y ∈O. Let EY denote an expectation
taken with respect to f(x;Y).
If the function f(x) is such that g(Y) ¼ EY(f(X)) exists 8 Y ∈O, then
g(Y) ¼ EY(f(X)) is a differentiable function of Y ∈O. Furthermore,
continuous
ð
Þ @r
@Yr
ð1
1
  
ð1
1
fðxÞfðx; YÞdx ¼
ð1
1
  
ð1
1
fðxÞ @rfðx; YÞ
@Yr
dx
discrete
ð
Þ @r
@Yr
S
x2R x
ð Þ fðxÞfðx; YÞ ¼
S
x2R x
ð Þ fðxÞ @rfðx; YÞ
@Yr
;
i.e., differentiation can occur under the integral or summation sign.
See E.L. Lehmann, (1986), Testing Statistical Hypotheses, 2nd Ed., John
Wiley, New York, pp. 59–60, or D.V. Widder, (1946), The LaPlace Transform,
Princeton University Press, Princeton, N.J., pp. 240–241.
The lemma allows one to establish the differentiability (and thus also conti-
nuity) of the power function associated with the rejection region of a hypothesis
test when the joint density of the probability sample is an exponential class
distribution. In particular, by deﬁning f(x) ¼ ICr x
ð Þ, Lemma 9.1 becomes a result
concerning the differentiability of power functions.
Theorem 9.8
Differentiability and
Continuity of Power
Functions for
Exponential Class
Densities
Let the joint density function of a probability sample be a member of the
exponential class of densities f(x;Y) ¼ exp(c(Y)g(x) + d(Y) + z(x))IA(x), Y ∈O,
with O being an open interval contained in R, and with c(Y) and d(Y) being
differentiable functions of Y ∈O. Let Cr be any rejection region for testing
some hypothesis H0: Y ∈O0 versus Ha: Y ∈Oa.19 Then the power function
pCr (Y) ¼ P(x ∈Cr;Y) is differentiable, and hence continuous, with respect to
Y ∈O.
18Results are available for a more general class of densities referred to as Polya distributions, which subsumes the exponential class
densities as a special case. However, the mathematics involved in analyzing the more general distributions is beyond the scope of our
study. Interested readers can consult the work of S. Karlin, (1957), “Polya Type Distributions II,” Ann. Math. Stat., 28, pp. 281–308.
19We are assuming that Cr is such that a power function is deﬁned, i.e., Cr can be assigned probability by f(x;Y), Y∈O.
570
Chapter 9
Hypothesis Testing Theory

Proof
Let f(x) 	 ICr (x) in Lemma 9.1. Then since EY(f(X)) ¼ EY(ICr (X)) ¼ pCr(Y) ¼
P(x ∈Cr;Y) 8 Y ∈O, the power function pCr(Y) is differentiable 8 Y ∈O. Conti-
nuity of pCr (Y) follows from differentiability of pCr (Y).
n
We now turn to the main result concerning the deﬁnition of UMPU level a
two-sided tests when the random sample has an exponential class density.
Theorem 9.9
UMPU Level a Two-
Sided Tests for
Exponential Class
Densities
Let the joint density of the random sample be given by f(x;Y) ¼ exp(c(Y)g(x) +
d(Y) + z(x))IA(x), Y ∈O, with O being an open interval contained in R, c(Y) and
d(Y) being differentiable functions of Y ∈O and c(Y) being strictly monotonic
(either
increasing
or
decreasing)
in
Y.
Deﬁne
a
rejection
region
as
Cr ¼ x : g x
ð Þ  c1 or g x
ð Þ  c2
f
g where c1 < c2.
1. A size a Cr deﬁnes a UMPU level a test of H0: Y ¼ Y0 versus Ha: Y 6¼ Y0 iff
Y0 ¼ arg minY2H0[Ha pCr Y
ð
Þ
f
g ¼ arg dpCR Y
ð
Þ=dY ¼ 0
½
, i.e., the power of the
test is minimized at Y0.
2. A size a Cr deﬁnes a UMPU level a test of H0: Y1  Y  Y2 versus
Ha: Y =2 [Y1, Y2], Y1 < Y2, iff pCrðY1Þ ¼ pCRðY2Þ ¼ a.
Proof
Necessity: For result (1), if the power function is not minimized at Y0, then
Cr is not an a-size unbiased test because then ∃Y ∈Ha such that pCr (Y) <
pCr(Y0) ¼ a. The power function is differentiable by Theorem 9.8, and can be
shown (see reference below) to be strictly convex, so that the minimum occurs
at Y0 iff dpCr(Y0)/dY ¼ 0.
For result (2), since the power function is continuous by Theorem 9.8, it is
necessary that pCr(Y1) ¼ pCr(Y2) ¼ a, for consider the contrary that pCr(Y1) < a
and/or pCr(Y2) < a (note that pCr(Yi) > a for i ¼ 1,2 is ruled out since the test has
size a). Then there exists a value of Y ∈Ha close to Y1 or Y2 such that pCr(Y) < a
by the continuity of pCr(Y), contradicting unbiasedness.
Sufﬁciency: E. Lehmann, Testing Statistical Hypotheses – 2nd Edition, pp.
135–137.
n
The application of Theorem 9.9 to ﬁnd UMPU level a tests of H0: Y ¼
Y0 versus Ha: Y 6¼ Ya or of H0: Y1  Y  Y2 versus Ha: Y ∈[Y1,Y2] is
conceptually straightforward although a computer will often be needed as an
aid in making the necessary calculations. Essentially, one searches for the
appropriate c1 and c2 values in Cr ¼ {x: g(x)  c1 or g(x)  c2} that produce an a-
size rejection region and that also satisfy either dpCr (Y0)/dY ¼ 0 or pCr (Y1) ¼
pCr(Y2), for the respective pairs of hypotheses. The following examples illustrate
the process.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
571

Example 9.20
UMPU Test of
H0: p ¼ p0 versus
Ha: p 6¼ p0 for Bernoulli
Population Distribution
Recall Example 9.14 where an untrusting gambler desired to test whether a
roulette wheel used by an Atlantic City Casino was fair. A random sample of
100 spins of the wheel was to be used for the test. The joint density of the
random sample was in the form of a product of Bernoulli densities, which can
be represented as a member of the exponential class f(x;p) ¼ exp[c(p) g(x)
+
d(p) + z(x)]
IA
(x)
where
c(p) ¼ ln(p/(1  p)),
g(x) ¼
P100
i¼1 xi ,
d(p) ¼
100 ln(1  p), z(x) ¼ 0, and A ¼ 100
i¼1{0,1}, with p ∈O ¼ (0,1). We demonstrated
in Example 9.14 that there does not exist a UMP level .10 test of the “fairness”
hypothesis H0: p ¼ .5 versus Ha: p 6¼ .5 having size .10. We now use Theorem 9.9
to show that a UMPU level and size a test of the fairness hypothesis does exist.
The rejection region we seek has the form
Cr ¼
x :
X
100
i¼1
xi  c1 or
X
100
i¼1
xi  c2
(
)
for choices of c1 and c2 that produce an a-size rejection region satisfying dpCr(.5)/
dp ¼ 0. Since Z ¼ P100
i¼1 Xi has a binomial distribution, Cr will have size a when
1  pCrð:5Þ ¼ P x 2 Cr; p ¼ :5


¼
X
c21
z¼c1þ1
100
z


:5
ð
Þz :5
ð
Þ100zI 0;1;...;100
f
gðzÞ ¼ 1  a
The derivative of the power function can be represented as
dpCrðpÞ
dp
¼ 
X
c21
z¼c1þ1
100
z


pz1 1  p
ð
Þ99z z  100p
½
I 0;1;...;100
f
gðzÞ;
which when evaluated at p ¼ .5 and then set equal to 0 results in the appropriate
condition on the choices of c1 and c2, as
dpCrð:5Þ=dp ¼ 
X
c21
z¼c1þ1
100
z


:5
ð
Þ98 z  50
½
 ¼ 0:
Thus c1 and c2 must be chosen so that c1 < z < c2 is a symmetric interval
around the value 50, which is a direct consequence of the binomial
density being a symmetric density around the point z ¼ 50 when p ¼ .5.
The possible choices of c1 and c2 are given by the ﬁnite set of two-tuples
A ¼
c1; c2
ð
Þ : c1 ¼ 50  i; c2 ¼ 50 þ i; i ¼ 1; . . . ; 50
f
g; which restricts the admissi-
ble choices of test sizes to beB ¼ a : a ¼ Pðz  c1 or z  c2; p ¼ :5Þ; ðc1; c2Þ 2 A
f
g.
Some speciﬁc possibilities for the size of the test are given as follows (calculated
using a computer):
c1,c2
a ¼ P(x ∈Cr; p ¼ .5)
37,63
.012
38,62
.021
39,61
.035
40,60
.057
41,59
.089
42,58
.133
572
Chapter 9
Hypothesis Testing Theory

Suppose a test size of a ¼ .057 is acceptable to the concerned gambler. Then the
rejection region of the UMPU level .057 test of the fairness hypothesis is given
by
Cr ¼
x :
X
100
i¼1
xi  40 or
X
100
i¼1
xi  60
(
)
and the hypothesis that the roulette wheel was fair would be rejected if red
(xi ¼ 1) occurred 40 or less times, or 60 or more times. Note that the values of
the power function (see Figure 9.9) for p ∈(0,1) are given by
pCrðpÞ ¼ 1 
X
59
z¼41
100
z


pz 1  p
ð
Þ100zI 0;1;...;100
f
gðzÞ:
Some selected values of the power function are given below.
p
pCrðpÞ
.5
.057
.45,.55
.185
.4, .6
.543
.3, .7
.988
.2, .8
1
Issue to Consider: Do you think the power function given in Figure 9.9 would be
acceptable to the gambler? Why or why not? If not, what could you do to alter the
power function to make it more acceptable to the gambler?)
□
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
0.2
0.4
0.6
0.8
1
p(p)
p
Figure 9.9
Power function for the
UMPU level .057 test of
H0: p ¼ .5 versus
Ha: p 6¼ .5.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
573

Example 9.21
UMPU Test of
H0: m ¼ m0 versus
Ha: m 6¼ m0 for Normal
Population Distribution
The manufacturer of a certain inexpensive battery-powered tablet computer
claims that the average operating time of the computer between full battery
charges is 2 hours. It can be assumed that the operating time, in hours, obtained
from a full battery charge is a random variable having a normal distribution with
standard deviation equal to .2, i.e., Xi ~ N(m,.04). A random sample of the
operating times of 200 of the tablet computers is to be used to test the hypothesis
that the average operating time is indeed 2 hours, i.e., a test of H0: m ¼ 2 versus
Ha: m 6¼ 2 is to be conducted. We seek a test that is UMPU level a for testing H0
versus Ha.20
The joint density of the random sample can be represented in the form
fðx; mÞ ¼
1
ð2p Þ100 ð:2 Þ200 exp 12:5 S
200
i¼1 ðxi  m Þ2


:
The density can be written in the exponential class form
fðx; mÞ ¼ exp½cðmÞg x
ð Þ þ dðmÞ þ z x
ð ÞIA x
ð Þ
with
cðmÞ ¼ 25m; gðxÞ ¼
X
200
i¼1
xi; dðmÞ ¼ 2500m2;
zðxÞ ¼  ln
2p
ð
Þ100 :2
ð
Þ200


 12:5
X
200
i¼1
x2
i ; and A ¼ Rn:
From Theorem 9.9, we seek a rejection region of the form
Cr ¼
x :
X
200
i¼1
xi  c1 or
X
200
i¼1
xi  c2
(
)
for choices of c1 and c2 that produce an a-size rejection region satisfying dpCr(2)/
dm ¼ 0. Using results for linear combinations of normally distributed random
variables we know that P200
i¼1 Xi  N 200m; 8
ð
Þ; and thus Cr will have size a when
1  pCrð2Þ ¼ Pðx 2 Cr; m ¼ 2Þ ¼
ðc2
c1
1
ﬃﬃﬃﬃﬃﬃ
2p
p
ﬃﬃﬃ
8
p exp  1
16 z  400
ð
Þ2


dz ¼ 1  a:
The derivative of the power function can be represented as
dpCr m
ð Þ
dm
¼ 
ðc2
c1
25 z  200m
½

ﬃﬃﬃﬃﬃﬃ
2p
p
ﬃﬃﬃ
8
p
exp 1
16 z  200m
ð
Þ2


dz;
which when evaluated at m ¼ 2 and then set equal to 0 results in the condition
dpCrð2Þ
dm
¼ 25
ðc2
c1
z  400
ð
ÞN z; 400; 8
ð
Þdz ¼ 0:
20One can show using the monotone likelihood ratio approach that a UMP level a test of H0 versus Ha does not exist. Recall Theorem
9.9.
574
Chapter 9
Hypothesis Testing Theory

Thus c1 and c2 must be chosen so that c1 < z < c2 is a symmetric interval
around the value 400, which is a direct consequence of the symmetry of the
normal distribution around the point z ¼ 400. The possible choices of c1 and c2
are
given
by
the
uncountably
inﬁnite
set
of
two-tuples
A ¼
c1; c2
ð
Þ : c1 ¼ 400  c; c2 ¼ 400 þ c; c 2 0; 1
ð
Þ
f
g , which implies that the
choice of test size can be any value ∈(0,1), since
B ¼ fa : a ¼ Pðz  c1 or z  c2; m ¼ 2Þ; ðc1; c2Þ 2 Ag ¼ ð0; 1Þ:
Suppose a ¼ .10 is chosen to be the size of the test. The appropriate values of c1
and c2 can be found by solving for c in the integral equation
Ð 400þc
400c N z; 400; 8
ð
Þdz
¼
Ð c= ﬃﬃ
8
p
c= ﬃﬃ
8
p N z; 0; 1
ð
Þdz ¼ :90 and then setting c1 ¼ 400  c and c2 ¼ 400 + c. Using
the computer, or the table of the standard normal distribution, we ﬁnd that
c=
ﬃﬃﬃ
8
p
¼ 1:645 , or c ¼ 4.653. Then c1 ¼ 395.347 and c2 ¼ 404.653, and the
rejection region of the UMPU size .10 test is given by
Cr ¼
x :
X
200
i¼1
xi  395:347 or
X
200
i¼1
xi  404:653
(
)
:
The rejection region can be equivalently expressed using the sample mean as a
test statistic, in which case Cr ¼ fx : x  1:977 or x  2:023g: Thus, the hypoth-
esis is rejected if the average operating time of the 200 sampled notebook
computers is  1.977 hours or  2.023 hours.
Note that the values of the power function (see Figure 9.10) for m ∈(1,1)
are given by
pCr ðmÞ ¼ 1 
ð 404:653
395:347 Nðz; 200m; 8Þdz ¼ 1 
ð ð404:653  200mÞ=
ﬃﬃﬃ
8
p
ð395:347  200mÞ=
ﬃﬃﬃ
8
p Nðz; 0; 1Þdz:
Some selected values of the power function are given below.
1.94 1.95 1.96 1.97 1.98 1.99
2
2.01 2.02 2.03 2.04 2.05 2.06
0
0.2
0.4
0.6
0.8
1
p(m)
m
Figure 9.10
Power function of the
UMPU level .10 Test of
H0: m ¼ 2 versus
Ha: m 6¼ 2.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
575

m
pCr (m)
2
.100
1.99,2.01
.183
1.98,2.02
.410
1.97,2.03
.683
1.96,2.04
.882
1.95,2.05
.971
1.94,2.06
.995
The reader may have noticed that in the preceding two examples, the
characterization of admissible values for c1 and c2 was simpliﬁed by the fact
that g(X), which was a test statistic in both examples, had a distribution that was
symmetric about its mean under the assumption H0 is true. In cases where this
symmetry in the distribution of g(X) does not occur, the characterization of
admissible (c1, c2) values is somewhat more involved.
Example 9.22
UMPU Test for
H0: Y ¼ Y0 versus
Ha: Y 6¼ Y0 in the
Exponential Population
Distribution
The operating life of a 8 gigabyte memory chip manufactured by the Elephant
Computer Chip Co. can be viewed as a random variable with a density function
belonging to the exponential family of densities. A random sample of the
lifetimes of 500 memory chips is to be used to test the hypothesis that the
mean life of the chip is 50,000 hours. With the Xi0s measured in 10,000’s of
hours, the joint density of the random sample is given by
fðx; yÞ ¼ y500 exp 
X
500
i¼1
xi=y
 
!
P
500
i¼1 Ið0;1Þ ðxiÞ for y 2 O ¼ ð0; 1Þ:
We seek a UMPU level .05 test of H0: y ¼ 5 versus Ha: y 6¼ 5.
The joint density can be written in the exponential class form
fðx; yÞ ¼ exp½cðyÞg x
ð Þ þ dðyÞ þ z x
ð ÞIA x
ð Þ
with
cðyÞ ¼ y1; gðxÞ ¼
X
500
i¼1
xi;
dðyÞ ¼ lnðy500Þ;
zðxÞ ¼ 0; and A ¼ 500
i¼1 0; 1
ð
Þ:
From Theorem 9.9, we seek a rejection region of the form
Cr ¼
x :
X
500
i¼1
xi  c1 or
X
500
i¼1
xi  c2
(
)
for choices of c1 and c2 that produce an a-size rejection region satisfying dpCr (5)/
dy ¼ 0. Using the MGF approach, it can be established that Z ¼ P500
i¼1 Xi has a
Gamma(z; 500,y) distribution i.e., Z  1= y500Gð500ÞÞ


z499 expðz=yÞI 0;1
ð
ÞðzÞ

(recall Example 9.18).
Then Cr will have size a when
576
Chapter 9
Hypothesis Testing Theory

1  pCrð5Þ ¼
ðc2
c1
1
5500G 500
ð
Þ
z499 expðz=5ÞI 0;1
ð
ÞðzÞ ¼ 1  a:
ð1Þ
The derivative of the power function can be represented as
dpCrðyÞ=dy ¼ 
ðc2
c1
z=y2


 500y1


Gammaðz; 500; yÞdz;
which when evaluated as y ¼ 5 and then set equal to 0 results in the condition
dpCrð5Þ=dy ¼ 
ðc2
c1
z=25
ð
Þ  100
½
 Gamma z; 500; 5
ð
Þdz ¼ 0:
ð2Þ
Unlike the previous two examples, the distribution of Z is not symmetric about
its mean, and the preceding condition does not imply that (c1,c2) is a symmetric
interval around E(Z) ¼ 2,500. Nonetheless, (2) deﬁnes an implicit functional
relationship between c2 and c1, say c2 ¼ g(c1), that determines admissible values
of c1 and c2. Using the computer, the simultaneous equations (1) and (2) can be
numerically solved for c1 and c2, given a choice of the test size a ∈(0,1). The
table below provides c1 and c2 values for selected test sizes:
a
c1
c2
.01
2222.920
2799.201
.05
2287.190
2725.618
.10
2320.552
2688.469
.20
2359.419
2646.057
To deﬁne a UMPU level .05 test of H0: y ¼ 5 versus Ha: y 6¼ 5, the rejection
region would be deﬁned as
Cr ¼
x :
X
500
i¼1
xi  2287:190 or
X
500
i¼1
xi  2725:618
(
)
:
One could also use the sample mean as a test statistic, in which case the rejection
region could be deﬁned alternatively as Cr ¼ fx : x  4:574 or x  5:451g:
Thus, the hypothesis H0: y ¼ 5 will be rejected if the average life of the 500
sampled chips is  4574 hours or  5451 hours.
Power function values for y > 0 are given by (see Figure 9.11)
pCrðyÞ ¼ 1 
ð 2725:618
2287:190
1
y500Gð500Þ
z499 expðz=yÞdz:
Some selected values of the power function are given below.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
577

u
pCr (u)
4
.999
4.25
.954
4.50
.649
4.75
.206
4.90
.073
5.00
.050
5.10
.073
5.25
.196
5.50
.573
5.75
.878
6.00
.981
6.25
.999
Unlike the previous two examples, the power function is not symmetric about
the point y0 ¼ 5. However, it is “nearly” symmetric becauseZn ¼ Pn
i¼1 Xi has an
asymptotic normal distribution, and a sample size of n ¼ 500 is sufﬁcient for the
asymptotic distribution to provide a good approximation to Zn0s gamma
distribution.
□
Example 9.23
UMPU Test of
H0: m ∈[m1,m2] versus
Ha: m =2 [m1, m2] for
Normal Population
Distribution
Recall Example 9.21 regarding the operating times of tablet computers between
full battery charges. Suppose instead of testing the hypothesis H0: m ¼ 2, a range
of values for the mean operating time of the computer is to be tested. In
particular, suppose we wish to test the hypothesis H0: 1.75  m  2.25 versus
Ha: m < 1.75 or m > 2.25. We know from Example 9.21 that f(x;m) belongs to the
exponential class of densities and satisﬁes the other conditions of Theorem 9.9.
Using Theorem 9.9, we seek an a-size rejection region deﬁned by
4
4.25
4.5
4.75
5
5.25
5.5
5.75
6
6.25
0
0.2
0.4
0.6
0.8
1
π(Θ)
Θ
Figure 9.11
Power function of UMPU
level .05 test of H0: y ¼ 5
versus Ha: y 6¼ 5.
578
Chapter 9
Hypothesis Testing Theory

Cr ¼
x :
X
200
i¼1
xi  c1 or
X
200
i¼1
xi  c2
(
)
for choices of c1 and c2 that satisfy
1  pCrð350Þ ¼
ðc2
c1
Nðz; 350; 8Þdz ¼
ðc2
c1
Nðz; 450; 8Þdz ¼ 1  pCrð450Þ ¼ 1  a:
Choosing a speciﬁc value of a, the preceding condition becomes a system of two
integral equations which can be solved simultaneously (using the computer) for
the two unknowns c1 and c2.
Suppose the size of the test is chosen to be a ¼ .05. Using a nonlinear
simultaneous equations solver,21 the solution to the preceding two-equation
system was found to be c1 ¼ 345.348 and c2 ¼ 454.652. Thus, the rejection
region of the UMPU level-.05 test deﬁned by Theorem 9.9 is
Cr ¼
x :
X
200
i¼1
xi  345:348 or
X
200
i¼1
xi  454:652
(
)
:
One could also use the sample mean for the test statistic, in which case
Cr ¼ x : x  1:727 or x  2:273
f
g:
Power function values are given by
pCr ðmÞ ¼ 1 
ð 454:652
345:348 Nðz; 200m; 8Þdz
(see Figure 9.12).
Some selected values of the power function are given below.
m
pCr (m)
2
0
1.76,2.24
.01
1.75,2.25
.05
1.74,2.26
.17
1.73,2.27
.41
1.72,2.28
.68
1.71,2.29
.88
1.70,2.30
.97
1.69,2.31
1
The power function veriﬁes that the test deﬁned by Cr is unbiased and has
size .05. The hypothesis H0: 1.75  m  2.25 is then rejected by the UMPU level
.05 test if the average operating time of the 200 sampled tablet computers is
 1.727 hours or  2.273 hours.
□
21The algorithm actually used was the NLSYS procedure in the GAUSS Matrix language.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
579

Example 9.24
UMPU Test of
H0: y ∈[y1, y2] versus
Ha: y =2 [y1, y2] in
Exponential Population
Distribution
Recall Example 9.22 regarding the operating life of memory chips. Suppose
instead of testing the hypothesis H0: y ¼ 5, a range of values for the mean life
of the memory chips is to be tested. In particular, suppose we wish to test the
hypothesis H0: 4.9  y  5.1 versus Ha: y < 4.9 or y > 5.1. We know from
Example 9.22 that f(x; y) belongs to the exponential class of densities and adheres
to the other conditions of Theorem 9.9. Using Theorem 9.9, we seek a size a
rejection region of the form
Cr ¼
x :
X
500
i¼1
xi  c1 or
X
500
i¼1
xi  c2
(
)
for choices of c1 and c2 that satisfy
1pCrð4:9Þ ¼
ðc2
c1
Gammaðz;500;4:9Þdz ¼
ðc2
c1
Gammaðz;500;5:1Þdz ¼ 1pCrð5:1Þ ¼ 1a:
Choosing a speciﬁc value of test size a, the preceding condition becomes a
system of two integral equations which can be solved simultaneously (using
the computer) for the two unknowns c1 and c2.
Suppose the size of the test is chosen to be a ¼ .05. Using a nonlinear
equation solver22 the solution to the two-equation system was found to be
c1 ¼ 2268.031 and c2 ¼ 2746.875. Thus, the rejection region of the UMPU
level .05 test deﬁned by Theorem 9.9 is given by
Cr ¼
x :
X
500
i¼1
xi  2268:031 or
X
500
i¼1
xi  2746:875
(
)
:
1.69
1.71
1.73
1.75
2.242 2.262 2.282 2.302
0
0.2
0.4
0.6
0.8
1
p(m)
m
Figure 9.12
Power function of UMPU
level .05 test of
H0: 1.75  y  2.25
versus Ha: y < 1.75 or
y > 2.25.
22The algorithm actually used was the NLSYS procedure in the GAUSS matrix language.
580
Chapter 9
Hypothesis Testing Theory

One could also use the sample mean for the test statistic, in which case
Cr ¼ fx : x  4:536 or x  5:494g:
Power function values are given
pCrðyÞ ¼ 1 
ð 2746:875
2268:031 Gammaðz; 500; yÞdz
(see Figure 9.13).
Some selected values of the power function are given below
u
pCr (u)
4.00
.998
4.25
.932
4.50
.576
4.75
.157
4.90
.050
5.00
.032
6.00
.973
5.75
.841
5.50
.504
5.25
.151
5.10
.050
The power function veriﬁes that the test deﬁned by Cr is a size .05 unbiased test.
The hypothesis H0: 4.9  y  5.1 is then rejected by the UMPU level .05 test if
the average life of the 500 sampled memory chips is  4,536 hours or
 5,494 hours.
□
4
4.2
4.4
4.6
4.8
5
5.2
5.4
5.6
5.8
6
6.2
0
0.2
0.4
0.6
0.8
1
p(q)
q
Figure 9.13
Power function of UMPU
level .05 test of
H0: 4.9  y  5.1 versus
Ha: y < 4.9 or y > 5.1.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
581

We observed in previous examples that the deﬁnition of the UMPU level a
rejection region for testing H0: Y ¼ Y0 versus Ha: Y 6¼ Y0 was substantially
simpliﬁed if the test statistic had a density that was symmetric about its
mean. This is a general property of the construction of UMPU tests based on
Theorem 9.9 that we now formalize.
Theorem 9.10
Two Sided UMPU Tests
of H0 : Y ¼ Y0 versus
Ha : Y 6¼ Y0 for
Symmetric PDFs
Assume the conditions of Theorem 9.9 and let the density of the test statistic
g(X) be symmetric about its mean g ¼ E(g(X)) when Y ¼ Y0. If c1 and c2 are such
that P(g(x)  c2;Y0) ¼ a/2 and c1 ¼ 2g  c2, then Cr ¼ {x: g(x)  c1 or g(x)  c2}
deﬁnes a UMPU level a test of H0: Y ¼ Y0 versus Ha: Y 6¼ Y0.
Proof
See Appendix.
n
Example 9.25
UMPU Two-Sided Tests
Using Symmetry of
Binomial and Normal
Distributions
Revisit Examples 9.20 and 9.21. In the ﬁrst example, since the binomial density
of Z ¼ P100
i¼1 Xi is symmetric about E(Z) ¼ 50, we know from Theorem 9.10 that
choosing c1 and c2 such that P(z  c2; .5) ¼ a/2 and c1 ¼ 2(50)  c2 will deﬁne a
UMPU level a rejection region Cr ¼ {x:P100
i¼1 xi  c1 or P100
i¼1 xi  c2}. In particu-
lar, P(z  60; .5) ¼ .0285, c1 ¼ 100  60 ¼ 40, and then Cr ¼ {x:P100
i¼1 xi  40 or
P100
i¼1 xi  60} deﬁnes the appropriate size .057 rejection region.
In the second example, since the normal distribution of Z ¼ P100
i¼1 Xi is
symmetric about E(Z) ¼ 400, we know from Theorem 9.10 that choosing c1
and c2 such that P(z  c2;400) ¼ a/2 and c1 ¼ 2(400)  c2 will deﬁne a UMPU
level a rejection region Cr ¼ {x: P200
i¼1 xi  c1 or
P200
i¼1 xi  c2}. In particular,
P(z  404.653) ¼ .05, c1 ¼ 800  404.653 ¼ 395.347, and then Cr ¼ {x: P200
i¼1 xi
 395.347 or
P200
i¼1 xi  404.653} deﬁnes the appropriate size .10 rejection
region.
□
It should also be noted that in the discrete case, it may not be possible to
deﬁne an unbiased test, let alone a UMPU test, without resorting to so-called
randomized tests (see Example 9.8). The reader should revisit Example 9.20 and
examine alternative choices of p to explore the difﬁculty. The problem does not
arise in the continuous case.
9.5.4
Conditioning in the Multiple Parameter Case
In the multiple parameter case, the probability sample, X, has a joint distribu-
tion, f(x;Q), that belongs to a family of distributions indexed by a (k  1) param-
eter vector Q ∈O, where k > 1. We will examine hypotheses of the form H0:
Yi ∈Oi
0 versus Ha: Yi ∈Oi
a, where Oi ¼ Oi
0 [ Oi
a represents the admissible values
of the ith parameter Yi. Note that for both H0 and Ha, the values of Yj, j 6¼ i, are
left unspeciﬁed, and so it is tacitly understood that Y1, Y2,. . .,Yi1, Yi+1,. . .,Yk
can assume any admissible values for which Y ∈O. In this hypothesis testing
context the parameters Yj, j 6¼ i, are often referred to as nuisance parameters,
since we are not interested in them from the standpoint of our stated
582
Chapter 9
Hypothesis Testing Theory

hypotheses, but they must nonetheless be dealt with in deﬁning a statistical
test of H0 versus Ha. We will also examine tests of the more general hypotheses
H0:gðQÞ 2 Og
0 versus Ha: g(Q) ∈Og
a, where g(∙) is a scalar function of the parameter
vector Q.
The approach we will use for deﬁning tests in the multiparameter case will
allow us to use the results we have previously established concerning tests for
the single parameter case. In particular, we will seek to transform the problem
into a single parameter situation by conditioning on sufﬁcient statistics, and
then apply previous results for the single parameter case to the transformed
problem. In many cases of practical interest, the approach can be used to deﬁne
UMP level a tests, or at least UMP level a tests within the unbiased class of tests.
We will examine a number of other useful and more versatile test procedures for
the multiparameter case in Chapter 10, although the procedures will not neces-
sarily be UMP or UMPU tests.
There is a substantial literature on UMP and UMPU tests of statistical
hypotheses in multiparameter situations that is more advanced than what we
present here and that also applies to a wider array of problems. For additional
reading, and a multitude of references, see E. Lehmann, (1986), Testing Statisti-
cal Hypotheses, 2nd ed., pp. 134–281.
UMPU Level a Tests via Conditioning
The basic idea of conditioning in order to
transform a multiparameter problem into a problem involving only a single
parameter, or scalar function of parameters, is as follows. Let S1,. . .,Sr be a set
of sufﬁcient statistics for f(x;Q). Suppose that the elements in the parameter
vector Q have been ordered so that Y1 is the parameter of interest in the
hypothesis test, and Y2,. . .,Yk are nuisance parameters. Suppose further that,
for each ﬁxed value of Y0
1 2 O1 , Sj,. . .,Sr are sufﬁcient statistics for the k  1
parameter density f(x;Y0
1 , Y2,. . .,Yk), (Y0
1 , Y2,. . .,Yk) ∈O. It follows from the
deﬁnition of sufﬁciency (recall Deﬁnition 7.18) that
f
ðx; Y0
1Þ ¼ fðx; Y0
1; Y2; . . . ; Ykjsj; . . . ; srÞ; Y0
1 2 O1;
i.e., the conditional distribution of X, given sj,. . .,sr, does not depend on the
nuisance parameters Y2,. . .,Yk. In effect, by conditioning on sj,. . .,sr, the problem
will have been converted into one involving only the parameter Y1. So long as
we are concerned only with results obtained in the context of the conditional
problem, single parameter UMP or UMPU hypothesis testing results would
apply.
In order to extend the previous argument to functions of parameters, con-
sider an analogous argument in terms of a reparameterized version of f(x;Q).
Suppose the joint density function of X was reexpressed in terms of the
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
583

parameter vector c, where ci ¼ ci(Q) for i ¼ 1,. . .,k, so that f
 x; c
ð
Þ, for c ∈Oc ¼
{c: ci ¼ ci(Q), i ¼ 1,. . .,k, Q ∈O}, is an alternative representation of the
parametric family of densities f(x;Q), Q ∈O. Then if, for each ﬁxed c0
1 2 O1
c, S
j ;
. . . ; S
r is a set of sufﬁcient statistics for f
(x;c0
1, c2,. . .,ck), (c0
1, c2,. . .,ck) ∈Oc, it
would follow that
f
ðx; c0
1Þ ¼ fðx; c0
1; c2; . . . ; ckjs
j ; . . . ; s
rÞ; c0
1 2 O;
i.e., the conditional distribution of X, given s
j ; . . . ; s
r, would not depend on c2,. . .,
ck. Then single parameter UMP and UMPU results could be applied to testing
hypotheses about c1(Q), conditional on s
j ; . . . ; s
r.
For the conditioning procedure to have practical value, the optimal
properties possessed by the rejection region, Cr, in the conditional problem
need to carry over to the context of the original unconditional problem. In
particular, we will focus on determining when a Cr that is UMPU level a for
the conditional problem also possesses this property in the unconditional prob-
lem. The exponential class of densities represents an important set of cases in
which the conditioning procedure works well. We will have use for the follow-
ing generalizations of Lemma 9.1 and Theorem 9.8.
Lemma 9.2
Differentiability of
g(Q) ¼ EQ(f(X)) in the
Case of a
Multiparameter
Exponential Class
Density
Let fðx; QÞ ¼ exp Pk
i¼1 ciðQÞgiðxÞ þ dðQÞ þ zðxÞ


IAðxÞ; Q 2 O; be an exponen-
tial class density with O deﬁned as an open rectangle contained inRk and with
c1(Q),. . .,ck(Q) and d(Q) being differentiable functions of Q ∈O. If the func-
tion f(x) is such that g(Q) ¼ EQ(f(X)) exists 8 Q ∈O, then g(Q) ¼ EQ(f(X)) is a
differentiable function of Q ∈O. Furthermore,
ðcontinuousÞ
@rþs
@Yr
i@Ys
j
ð1
1

ð1
1
fðxÞfðx;QÞdx ¼
ð1
1

ð1
1
fðxÞ@rþsfðx;QÞ
@Yr
i@Ys
j
dx
ðdiscreteÞ
@rþs
@Yr
i@Ys
j
X
x2R X
ð Þ
fðxÞfðx;QÞ ¼
X
x2R X
ð Þ
fðxÞ@rþsfðx;QÞ
@Yr
i@Ys
j
for i and j ¼ 1,. . .,k, i.e., differentiation can occur under the integral or sum-
mation sign.
Proof
See references listed for Lemma 9.1.
n
Theorem 9.11
Differentiability and
Continuity of Power
Functions for
Multiparameter
Exponential Class
Densities
Let the joint density of a random sample be a member of the exponential class
of densities
fðx; QÞ ¼ exp
X
k
i¼1
ciðQÞgiðxÞ þ dðQÞ þ zðxÞ
 
!
IAðxÞ;
584
Chapter 9
Hypothesis Testing Theory

Q ∈O, with O being an open rectangle contained in Rk and with c1(Q),. . .,ck(Q),
d(Q) being differentiable functions of Q ∈O. Let Cr be any rejection region
for testing the hypothesis H0: Q ∈O0 versus Ha: Q ∈Oa.23 Then the power
function pCr(Q) ¼ P(x ∈Cr;Q) is differentiable, and hence continuous, with
respect to Q ∈O.
Proof
Follows from Lemma 9.2 with a proof analogous to the proof of Theorem 9.8. n
We will state our main results more generally in terms of hypotheses
concerning the value of the c1(Q) function in the deﬁnition of the exponential
class density.24 There is no loss of generality in stating our results this way. First
note that the ordering of the functions c1(Q),. . .,ck(Q) in the exponential class
deﬁnition is arbitrary, and so the results we present can be interpreted as
referring to any of the originally-speciﬁed ci(Q) functions, given an appropriate
reordering and relabeling of the functions c1(Q),. . .,ck(Q). Furthermore, since the
parameterization of a density function is not unique, the c1(Q) function can be
deﬁned to represent various functions of Q that might be of interest to the
analyst. In particular, a parameterization of the density of X that results in
c1(Q) ¼ Yi places hypotheses about Yi under consideration. We will also be
able to extend our results to the case of testing hypotheses concerning linear
combinations of the functions c1(Q),. . .,ck(Q). Thus, types of hypotheses that can
be tested using the following theorem are much more general than they might at
ﬁrst appear.
Theorem 9.12
Hypothesis Testing in
the Multiparameter
Exponential Class
Let the density of the random sample X be given by
fðx; YÞ ¼ exp
Xk
i¼1 ciðQÞgiðxÞ þ dðQÞ þ zðxÞ


IAðxÞ;
Q ∈O  Rk, where O is an open rectangle, si(X) ¼ gi(X), i ¼ 1,. . .,k, represents
a set of complete sufﬁcient statistics for f(x;Q), and c(Q) ¼ (c1(Q),. . .,ck(Q)) and
23We are assuming that Cr is such that a power function is deﬁned, i.e., Cr can be assigned probability by f(x;Q), Q ∈O.
24Note that c ¼ (c1,. . .,ck) could be viewed as an alternative parameterization of the exponential class of densities, where
f
 x; c
ð
Þ ¼ exp
X
k
i¼1
cigi x
ð Þ þ d
 c
ð Þ þ z x
ð Þ
 
!
IA x
ð Þ; c 2 Oc;
with d
 c
ð Þ ¼ ln Ð 1
1    Ð 1
1 exp Pk
i¼1 c:g: x
ð Þ þ z x
ð Þ


IA x
ð Þdx

1
(use summation in the discrete case). This parameterization is
referred to as the natural parameterization of the exponential class of densities. Note that the deﬁnition of d*(c) is a direct result of
the fact that the density must integrate (or sum) to 1.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
585

d(Q) are differentiable functions of Q ∈O. If the size a rejection region Cr is
unbiased, then the following relationships between H0, Ha and Cr hold:
Case
H0
Ha
Cr for UMPU Level a Test
1.
c1 (Q) ¼ c0
1
c1 (Q) > c0
1
Cr ¼ {x: s1(x)  h(s2,. . .,sk)} such that
P(x ∈Cr; c0
1
s2,. . .,sk) ¼ a 8(s2,. . .,sk)25
c1 (Q)  c0
1
c1 (Q) > c0
1
2.
c1 (Q) ¼ c0
1
c1 (Q) < c0
1
Cr ¼ {x: s1(x)  h(s2,. . .,sk)} such that
P(x ∈Cr;c0
1
s2,. . .,sk) ¼ a, 8(s2,. . .,sk)25
c1 (Q)  c0
1
c1 (Q) < c0
1
3.
c1 (Q) ¼ c0
1
c1 (Q) 6¼ c0
1
Cr ¼ {x: s1(x)  h1(s2,. . .,sk), or
s1(x)  h2(s2,. . .,sk)} such that
P(x ∈Cr;c0
1
s2,. . .,sk) ¼ a, 8(s2,. . .,sk)25
4.
c1(Q) ∈[c‘
1; ch
1]
c1(Q) =2 [c‘
1; ch
1]
Cr ¼ {x: s1(x)  h1(s2,. . .,sk) or
s1(x)  h2(s2,. . .,sk)} such that
P(x ∈Cr;c‘
1
s2,. . .,sk) ¼ a,
P(x ∈Cr;ch
2
s2,. . .,sk) ¼ a, 8(s2,. . .,sk)25
Proof
See Appendix.
n
9.5.5
Examples of UMPU Tests in the Multiparameter Exponential Class
We now examine examples illustrating how Theorem 9.12 can be used in
practice to deﬁne UMPU level a tests of statistical hypotheses. Some of the
examples will illustrate the use of reparameterization for transforming a prob-
lem into a form that allows Theorem 9.12 to be used for deﬁning a statistical
test. We also identify general hypothesis testing contexts in which the results of
the examples can be applied.
Example 9.26
Testing Inequality
Hypotheses About the
Variance of a Normal
Population Distribution
The numberof miles thata certain brand ofautomobiletirecanbedrivenbeforethe
tread-wear indicators are visible, and the tires need to be replaced, is a random
variable having a normal distribution with unknown mean and variance, i.e.,
Xi ~ N(m, s2). A random sample of 28 tires is used to obtain information about the
mean and variance of the population distribution of mileage obtainable from this
brand of tire. We wish to deﬁne a level .05 UMPU test of the hypotheses H0: s2  4
versus Ha: s2 > 4, where the mileage measurement, xi, is measured in 1,000’s of
miles, and given that x ¼ 45:175 and s2 ¼ 2:652; we wish to test the hypothesis.
First
note
that
the
joint
density
of
the
random
sample,
f x; m; s2


¼
2p
ð
Þn=2sn

1
expð 1=2
ð
Þ Pn
i¼1 xi  m
ð
Þ2=s2Þ; for n ¼ 28, can be writ-
ten in exponential class form as
f x; m; s2


¼ exp
X
2
i¼1
ci m; s2


giðxÞ þ d m; s2


þ zðxÞ
"
#
25Except, perhaps, on a set having probability zero.
586
Chapter 9
Hypothesis Testing Theory

where
c1 m; s2


¼  2s2

1;
g1ðxÞ ¼
X
n
i¼1
x2
i ;
c2 m; s2


¼ m=s2


;
g2ðxÞ ¼
X
n
i¼1
xi;
d m; s2


¼  n
2 m2=s2


 ln
2p
ð
Þn=2sn


; and zðxÞ ¼ 0:
Letting si(x) ¼ gi(x) for i ¼ 1,2, s1(X) and s2(X) are complete sufﬁcient statistics
for f(x;m,s2). Examining the parameterization of the exponential class density, it
is evident that hypotheses concerning s2 can be framed in terms of hypotheses
about c1. In particular, H0: s2  4 versus Ha: s2 > 4 can be alternatively
expressed as H0: c1  .125 versus Ha: c1 > .125.
Given result (1) of Theorem 9.12, we seek h(s2) such that for c0
1 ¼  .125,
pCr c0
1


¼ P s1 x
ð Þ  h s2
ð
Þ; c0
1js2


¼ P
X
n
i¼1
x2
i  h
X
n
i¼1
xi
 
!
; c0
1j
X
n
i¼1
xi
 
!
¼ a ¼ :05
8 s2 ¼ Pn
i¼1 xi . We can simplify this problem substantially by ﬁrst recalling
(Theorem 6.12) that the two random variables
Pn
i¼1 Xi  X
ð
Þ2 ¼ Pn
i¼1 X2
i 
n1 Pn
i¼1 Xi

2 and Pn
i¼1 Xi are independent random variables, given that the
Xi0s are iid N(m,s2). Then the preceding probability equality can be written
alternatively as
P
X
n
i¼1
xi  x
ð
Þ2  h
X
n
i¼1
xi
 
!
; c0
1
 
!
¼ a ¼ :05;where h
 ¼ h  n1
Xn
i¼1 xi

2
:
Note that we have eliminated the conditional probability notation since the
probability distribution of the random variable Pn
i¼1 Xi  X

2 is unaffected by
the value of Pn
i¼1 xi given the aforementioned independence property. The value
of
Pn
i¼1 xi only serves to determine the lower bound value h
P
n
i¼1
xi


on
outcomes of the random variable Pn
i¼1 Xi  X

2 but has no effect on the random
variable’s distribution.
We
can
simplify
the
problem
still
further
by
ﬁrst
recalling
that
Pn
i¼1 Xi  X

2=s2  w2
n1 when the Xi0s are iid N(m,s2) (recall Theorem 6.12).
Now rewrite the preceding probability equality as
P
X
n
i¼1
xi  x
ð
Þ2
s2
0
 h
X
n
i¼1
xi
 
!
=s2
0; c0
1
 
!
¼ a ¼ :05; where s2
0 ¼ 4:
Note the probability is being calculated using the parameter value c0
1 ¼ :125,
which coincides with s2
0 ¼ 4. In other words, the probability is being calculated
assuming that Xi ~ N(m,4) 8i, so that Pn
i¼1 Xi  X
ð
Þ2=s2
0 ¼ Pn
i¼1 Xi  X
ð
Þ2=4 
w2
n1 . It follows that the value of h
Pn
i¼1 xi


=s2
0 that solves the probability
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
587

equality can be obtained from the table of the w2 distribution, with n  1 ¼ 27
degrees of freedom in this case. The tabled value of 40.1 corresponding to a ¼ .05
implies that
h
X
n
i¼1
xi
 
!
=s2
0 ¼
h
X
n
i¼1
xi
 
!
 n1
X
n
i¼1
xi
 
!2
2
4
3
5=s2
0
¼
h
X
28
i¼1
xi
 
!
 1
28
X
28
i¼1
xi
 
!2
2
4
3
5=4 ¼ 40:1
so that h P
28
i¼1
xi


¼ 160:4 þ
P
28
i¼1
xi

2
=28:
Given that P28
i¼1 xi ¼ 28 45:175
ð
Þ ¼ 1264:9, then h(1264.9) ¼ 57,302.258. By
Theorem 9.12 the rejection region becomes Cr ¼
x : P
28
i¼1
x2
i  57; 302:258

	
: The
rejection region can also be expressed in terms of the sample variance as
Cr ¼
x : 28s2=4  40:1


.Then since 28s2/4 ¼ 28(2.652)/4 ¼ 18.564 6 40.1, the
null hypothesis s2  4 is not rejected.
The power function of the test can be deﬁned in terms of the parameter s2 as:
pCr s2


¼ P 28s2
s2
s2
4


 40:1; s2


¼ P 28s2
s2
 160:4
s2
; s2


where again, assuming the value of s2 is true, 28S2
s2  w2
27.
The power function could also have been expressed as a function of c1, but
power expressed in terms of s2 is more natural given the original statement of
the hypothesis. We can use the CDF of the w2
27 distribution to plot a graph of the
power function for various potential values of s2: (see Figure 9.14)
3
4
5
6
7
8
9
10
11
12
13
0
0.2
0.4
0.6
0.8
1
p(s2)
s2
Figure 9.14
Power function of UMPU
level .05 unbiased test of
H0: s2  4 versus
Ha: s2 > 4.
588
Chapter 9
Hypothesis Testing Theory

s2
P 28s2=s2  160:4=s2; s2
ð
Þ ¼ P w2
27  160:4=s2; s2


4
.050
3.9, 4.1
.040, .062
3.75, 4.25
.028, .082
3.50, 4.50
.013, .123
3.00, 5.00
.002, .229
6.00
.478
7.00
.700
8.00
.829
10.00
.952
13.00
.993
Note that even though Cr deﬁnes a UMPU level .05 test (by Theorem 9.12),
the test is not very powerful for values of s2 ∈(4,7), say. To increase the power of
the test in this range, one would either need to increase the size of the random
sample on which the test is based, or else use a test size larger than .05, although
the latter approach would, of course, increase the Type I Error probability
associated with the test.
□
We can generalize the results of the previous example as follows:
Deﬁnition 9.14
UMPU Level a
Statistical Test of
H0: s2  s2
0 versus
Ha: s2 > s2
0 when
Sampling from a
Normal Population
Distribution
Let X1,. . .,Xn be a random sample from N(m, s2). The rejection region denoted by
Cr ¼
x : ns2
s2
0
 w2
n1;a
(
)
;
where w2
n1;a solves
Ð 1
w2
n1;a Chisquareðz; n  1Þdz ¼ a; deﬁnes a UMPU level a
statistical test of H0: s2  s2
0 versus H0: s2 > s2
0.
The next example illustrates that a great deal of ingenuity may be required,
concomitant with a rather high degree of mathematical sophistication, in order
to implement the results of Theorem 9.12.
Example 9.27
Testing One and Two-
Sided Hypotheses about
the Mean of a Normal
Population Distribution
Having Unknown
Variance
Recall Example 9.26 regarding mileage obtained from automobile tires. Deﬁne
UMPU level .05 tests of H0: m ¼ 40 versus Ha: m 6¼ 40, and of H0: m  40 versus
Ha: m > 40.
The parameterization used in Example 9.26 is not directly useful for
implementing the results of Theorem 9.12 to deﬁne statistical tests of the
hypotheses under consideration. In considering alternative parameterizations,
the following lemma is useful.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
589

Lemma 9.3
Reparameterizing
Exponential Class
Densities via Linear
Transformations
Let an exponential class density be given by
fðx; cÞ ¼ exp
X
k
i¼1
cigiðxÞ þ dðcÞ þ zðxÞ
"
#
IAðxÞ:
Let c
1 ¼ Pk
i¼1 aici; with a1 6¼ 0, and let c* ¼ (c
1, c2,. . .,ck)0. Then f(x;c) can be
alternatively represented as
f x; c
ð
Þ ¼ exp c
1g
1 x
ð Þ þ
X
k
i¼2
cig
i x
ð Þ þ d
 c
ð
Þ þ z x
ð Þ
"
#
IA x
ð Þ;
where
g
1ðxÞ ¼ g1ðxÞ
a1
;
g
i ðxÞ ¼ giðxÞ 
ai
a1


g1ðxÞ;
i ¼ 2; . . . ; k;
and
d
 c
ð
Þ ¼ ln
ð1
1
  
ð1
1
exp c
1g
1 x
ð Þ þ
X
k
i¼2
cig
i x
ð Þ þ z x
ð Þ
"
#
IA x
ð Þdx
"
#1
0
@
1
A:
The proof is immediate upon substituting the deﬁnitions of c
1 and g
i x
ð Þ;
i ¼ 1; . . . k; into the expression for f(x;c*), and deﬁning d(c*) so that the density
integrates to 1. Replace integration with summation in the discrete case.
n
The point of the lemma is that any linear combination of the ci(∙) functions in
the deﬁnition of an exponential class density can be used as a parameter in a
reparameterization of the density. Note that the ci(∙) functions can always be
reordered and relabeled so that the condition a1 6¼ 0 is not a restriction in
practice. Then Theorem 9.12 can be used to test hypotheses about c
1 , or
equivalently, about Pk
i¼1 aici.
In the case at hand, reorder the ci(∙) functions and gi(x) functions so that
c
1 ¼
X
2
i¼1
aici m; s2


¼ a1
m
s2


þ a2  1
2s2


:
Without knowledge of the value of s2, it is clear that forc
1 ¼ c0
1 to imply a unique
value of m, the condition c0
1 ¼ 0 is required, in which case m ¼ a2/(2a1). Then
letting c0
1 ¼ 0 and a2/(2a1) ¼ m0 ¼ 40, it follows that c
1 ¼ 0 iff m ¼ 40, and c
1
 0 iff m0  40, assuming a1 > 0. This establishes an equivalence between the
hypotheses concerning m and hypotheses concerning c
1 as follows:
H0 : m ¼ 40
H0 : c
1 ¼ 0
versus
,
versus
Ha : m 6¼ 40
Ha : c
1 6¼ 0
8
<
:
9
=
; and
H0 : m  40
H0 : c
1  0
versus
,
versus
Ha : m>40
Ha : c
1>0
8
<
:
9
=
;
590
Chapter 9
Hypothesis Testing Theory

Test of H0:m  40 versus Ha:m > 40: The reparameterized exponential class
density can be written as
f x; c
ð
Þ ¼ exp c
1
X
n
i¼1
xi=a1
 
!
þ c2
X
n
i¼1
x2
i  a2
a1
X
n
i¼1
xi
 
!
þ d
 c
ð
Þ
 
!
IA x
ð Þ
(recall Lemma 9.3, and the fact that z(x) ¼ 0 in this case). Our approach will be
simpliﬁed somewhat if we rewrite f(x;c*) as
f x; c
ð
Þ ¼ exp c
1
X
n
i¼1
xi  m0
ð
Þ
a1
þ c2
X
n
i¼1
x1  m0
ð
Þ2 þ d0 c
ð
Þ
 
!
IA x
ð Þ
¼ exp c
1s
1 þ c2s
2 þ d0 c
ð
Þ


IA x
ð Þ
where d0ðc
Þ ¼ d
ðc
Þ  nm2
0c2 þ nm0c
1=a1;
m0 ¼ 40; and we have used the fact
that a2/a1 ¼ 2 m0.
Result (1) of Theorem 9.19 indicates that a UMPU level a test of H0: c
1
 0 versus Ha: c
1 > 0 can be deﬁned by ﬁrst ﬁnding a function h(s
2) such that
P s
1 x
ð Þ  h s
2


; c0
1js
2


¼ P
X
n
i¼1
xi  m0
ð
Þ
a1
 h
X
n
i¼1
xi  m0
ð
Þ2
 
!
; c0
1js
2
 
!
¼ a
¼ :05

ð Þ
8s
2 ¼ Pn
i¼1 xi  m0
ð
Þ2, with c0
1 ¼ 0. Letting c0 ¼ c0
1; c2

0 ¼ 0; c2
½
0, it is useful to
recognize that f x; c0


¼ exp c2s
2ðxÞ þ d0 c0


h
i
IAðxÞ; which indicates that the
value of f(x;c0) changes only when the value of s
2(x) changes. This observation
then clariﬁes the nature of the conditional distribution of x given s
2, namely, the
conditional distribution of X is a uniform distribution on the range RðXjs
2Þ ¼
x : Pn
i¼1 xi  m0
ð
Þ2 ¼ s
2
n
o
, where RðXjs
2Þ deﬁnes the boundary of an n-dimen-
sional hypersphere having radius (s
2)1/2. Speciﬁcally,26
f x; c0js
2


¼
n s
2

 n1
ð
Þ=2pn=2
G n
2 þ 1


"
#1
IR Xjs
2
ð
ÞðxÞ:
It follows from the deﬁnition of the support of the density of X that fðx; c0s
2Þ is a
degenerate density function, since all of the probability mass for the (n  1)
vector X is concentrated on the surface RðXjs
2Þ which has dimension n1.
26The surface area of an n-dimensional hypersphere is given by A ¼ nrn1pn=2


=G n=2
ð
Þ þ 1
ð
Þ; where r is the radius of the hypersphere.
See R.G. Bartle, (1976), The Elements of Real Analysis, 2nd Ed., John Wiley, pp. 454–455, and note that the surface area can be deﬁned
by differentiating the volume of the hypersphere with respect to r. For n ¼ 2, the bracketed expression becomes simply 2p(s*2)1/2, which
is the familiar 2pr.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
591

We can now deﬁne the function h(s
2) that satisﬁes (*). Let h(s
2) ¼ ga(s
2)1/2,
where ga is a positive constant. Multiplying both sides of the inequality in (*) by
a1 deﬁnes
P
X
n
i¼1
xi  m0
ð
Þ  g
a s
2

1=2; c0
1js
2
 
!
¼ a ¼ :05


where g
a ¼ a1ga is chosen so that (**) is true. We motivate the choice of g
a
by examining the case where n ¼ 2, although an analogous argument applies
to arbitrary ﬁnite n. Examine a circle with center (m0,m0) and radius r ¼
s
2

1=2,
as in Figure 9.15. Recall from basic trigonometry that with (m0, m0) as the origin,
the point
x0
1  m0; x0
2  m0


can be represented in terms of polar coordinates as
(r cos(Y), r sin(Y)). The function of Y deﬁned by P2
i¼1 xi  m0
ð
Þ ¼ r(cos Y + sin Y)
is strictly concave for Y ∈(135, 225), attains a maximum at Y ¼ 45 (the
maximized value being 1.4142r to 4 decimals), and is symmetric around
Y ¼ 45. It follows that an a-proportion of the points (x1,x2) on the circumference
of the circle can be deﬁned by appropriately choosing g
a < 1.4142 as
Da ¼
ðx1; x2Þ : ðx1 mlÞ ¼ r cos Y; ðx2 m2Þ ¼ r sin Y
ð
Þ; rðcos Y
ð
Þ þ sin Y
ð
ÞÞ  g
a r


¼
ðx1; x2Þ :
X
2
i¼1
xi  m0
ð
Þ  g
a s
2

1=2
(
)
;
and since (x1, x2) is distributed uniformly on the circumference, it follows that
P Dajs
2


¼ a:
0
x1
x1
1
x2
1
x2
0
A
B
(m0 ,m0)
r
Θ
α
Figure 9.15
Graph of RðXjrÞ ¼
x : P2
i¼1 xi  m0
ð
Þ2  r2
n
o
:
592
Chapter 9
Hypothesis Testing Theory

Now note that the preceding argument applies for arbitrary r ¼ s
2

1=2 > 0,
so that g
a is the same value 8r > 0. An analogous argument leading to the
constancy of g
a can be applied to the case of an n-dimensional hypersphere.
Thus the rejection region for the test will have the form
Cr ¼
x :
X
n
i¼1
xi  m0
ð
Þ  g
a s
2

1=2
(
)
;
regardless of the value of s
2. The remaining task is then to ﬁnd the appropriate
value of g
a that deﬁnes a test of size a.
It turns out that there is an alternative representation of the rejection region
in terms of the so-called t-statistic, which is the form in which the test is usually
deﬁned in practice. To derive this alternative representation, ﬁrst note that27
s
2 ¼
X
n
i¼1
xi  m0
ð
Þ2 ¼
X
n
i¼1
xi  x
ð
Þ2 1 þ
n x  m0
ð
Þ2
Pn
i¼1 xi  x
ð
Þ2
"
#
:
Then the rejection region can be represented as
Cr ¼
x :
n x  m0
ð
Þ
Pn
i¼1 xi  x2



1=2
 g
a
1 þ
n x  m0
ð
Þ2
Pn
i¼1 xi  x
ð
Þ2
h
i
2
4
3
5
1=2
8
>
<
>
:
9
>
=
>
;
:
Multiplying both sides of the inequality by [(n  1)/n]1/2 and deﬁning
t ¼
n1=2 x  m0
ð
Þ= Pn
i¼1 xi  x
ð
Þ2= n  1
ð
Þ
h
i1=2
; the representation of Cr becomes
Cr ¼
x :
t
t2 þ n  1
ð
Þ1=2  g0
a
(
)
where
g0
a ¼ g
a=n1=2 . Since t/(t2 + n  1)1/2 ¼
1 þ n  1
ð
Þ=t2

1=2
is strictly
monotonically increasing in t, there exists a value tn1;a such that
t
t2 þ n  1
ð
Þ1=2  g0
a , t  tn1;a;
so that Cr can be substantially simpliﬁed to
Cr ¼ fx : t  tn1;ag:
Assuming m ¼ m0 to be true, T has the student t-distribution with n  1
degrees of freedom so that the value of tn1;a can be found as the value that makes
P(t  tn1;a; m0) ¼ a ¼ .05 true. From the table of the student t-distribution, with
degrees of freedom equal to n  1 ¼ 27, it is found that t27;a ¼ 1.703. Given that x
¼ 45.175 and s2 ¼ 2.652, it follows that for m0 ¼ 40, t ¼ (28)1/2 (45.175  40)/
(2.75)1/2 ¼ 16.513. Because t > 1.703 we reject H0: m  40 in favor of Ha: m > 40.
27Expanding Pn
i¼1
xi  x
ð
Þ þ x  m0
ð
Þ
½
2 leads to the result.
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
593

In order to graph the power function of the test (See Figure 9.16), we must
calculate values of p(m) ¼ P(t  1.703; m), which requires that we identify the
distribution of T when m 6¼ m0. To see what is involved, note that we can repre-
sent T as
T ¼
X  m


þ m  m0
ð
Þ
s=
ﬃﬃﬃn
p
= nS2
s2 = n  1
ð
Þ
"
#1=2
;
so that if m 6¼ m0 we have
T ¼
Z þ m  m0
s=
ﬃﬃﬃn
p


=
Y
n  1
ð
Þ

1=2
;
where
Z ~ N(0,1),
Y
~ w2
n1;
and
Z
and
Y are
independent.
Letting
l ¼ ðm  m0Þ=ðs=
ﬃﬃﬃn
p Þ; the random variable T has a noncentral t-distribution,
with noncentrality parameter l, and degrees of freedom n  1, which we denote
by Tn1(l). When l ¼ 0, the ordinary central student t-distribution is deﬁned. We
discuss properties of the noncentral t-distribution in Section 9.6 of this chapter.
For now, note that l depends not only on m, but also on s, which effectively
implies that we must either plot the power of the test in two dimensions (in
terms of values of m and s), or else in a single dimension as a function of the
noncentrality parameter. The latter is the approach most often followed in
practice, and we follow it here. Integrals of the noncentral t-distribution can be
evaluated by many modern statistical software packages, such as SAS, GAUSS,
and MATLAB.
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
p(l)
l
Figure 9.16
Power function of UMPU
level .05 test of H0: m  40
versus Ha: m > 40, where
l ¼ ðm40Þ=ðs=
ﬃﬃﬃﬃﬃ
28
p
Þ:
594
Chapter 9
Hypothesis Testing Theory

l
p(l) ¼ P(t  1.703;l) where T  T27ðlÞ
0
.050
.1,.1
.041,.061
.5,.5
.016,.124
1,1
.004,.251
1.5
.428
2
.620
3
.900
4
.988
5
.999
Test of H0: m ¼ 40 versus Ha: m 6¼ 40: In order to deﬁne a test of H0: m ¼ 40
versus Ha: m 6¼ 40, result 3 of Theorem 9.12 suggests that the rejection region can
be deﬁned as
Cr ¼
x : s
1ðxÞ  h1 s
2


or s
1ðxÞ  h2 s
2




for h1 and h2 chosen such that P(x 2 Cr; c0
1js
2) ¼ a ¼ .05 8 s
2. Using an argument
along the lines of the previous discussion (see T. Ferguson, op. cit., p. 232) it can
be shown that the rejection region can be speciﬁed as
Cr ¼ x : t  t1 or t  t2
f
g
where t is deﬁned as before, and t1 and t2 are constants chosen so that when
m ¼ m0 ¼ 40, P(x 2 Cr; m0) ¼ .05. The additional condition in result 3 of Theorem
9.12 is met by choosing t1 ¼ t2 since T has the symmetric student
t-distribution when m ¼ m0 and Theorem 9.10 applies. In particular, Cr is deﬁned
by
Cr ¼ fx : t  tn1;a=2 or t  tn1;a=2g;
where tn1;a/2 solves
Ð 1
ta=2 Tdistðz; n  1Þdz ¼ a=2 , where Tdist (Z,v) is the t-
distribution with v degrees of freedom. For a t-distribution with 27 degrees of
freedom, t.025 ¼ 2.052. Since t ¼ 16.513 > 2.052, H0: m ¼ 40 is rejected in favor
of Ha: m 6¼ 40.
The power function of the test is deﬁned by values of P(x ∈Cr;m). As before,
T has a noncentral t-distribution (see Section 9.6) with noncentrality parameter
l ¼ m  40
ð
Þ= s=
ﬃﬃﬃﬃﬃﬃ
28
p


(see Figure 9.17). Deﬁning the power function in terms of l
yields
l
p(l) ¼ P(t  2.052 or t  2.052;l) where T  T27ðlÞ
0
.050
.1,.1
.051
.5,.5
.077
1,1
.161
2,2
.488
3,3
.824
4,4
.971
5,5
.998
9.5
Classical Hypothesis Testing Theory: UMP and UMPU Tests
595

We can generalize the results of this example as follows.
Deﬁnition 9.15
UMPU Level a
Statistical Test of
H0: m  m0 Versus
Ha: m > m0, and of
H0: m ¼ m0 Versus
Ha: m 6¼ m0 When
Sampling From a
Normal Population
Distribution
LetX1,...,XnbearandomsamplefromN(m,s2),anddeﬁneT ¼ (n  1)1/2(x  m0)/S.
Then,
1. Cr ¼ {x:t  tn1,a}deﬁnesaUMPUlevelastatisticaltestofH0:m  m0versus
Ha: m > m0, where
Ð 1
tn1;a Tdist t; v
ð
Þdt ¼ a:
2. Cr ¼ fx : t  tn1;a=2 or t  tn1;a=2g deﬁnes a UMPU level a statistical
test of H0: m ¼ m0 versus Ha: m 6¼ m0, where
Ð 1
tn1;a=2 Tdist t; v
ð
Þdt ¼ a=2:
9.5.6
Concluding Remarks
In this section we examined some classical statistical theory and procedures for
deﬁning either UMP or UMPU level a tests for various types of hypotheses. As is
evident from the proofs of the theorems and the illustrative examples,
attempting to deﬁne UMP or UMPU level a tests can sometimes be a substantial
challenge. While the results provided in this section provide direction to the
search for a statistical test, application of the results still requires considerable
ingenuity, and the procedures are not always tractable or even applicable.
In Chapter 10 we will examine well-deﬁned procedures that will lead to
speciﬁc statistical tests requiring relatively less mathematical effort than the
procedures introduced so far. While leading to the deﬁnition of statistical tests
that generally work well in practice, the procedures do not explicitly incorporate
the objective of ﬁnding a UMP or UMPU level a test. As a result, they cannot
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
p(l)
l
Figure 9.17
Power function of UMPU
level .05 test of H0:m ¼ 40
versus Ha: m 6¼ 40, where
l ¼ (m  40)/(s/
ﬃﬃﬃﬃﬃ
28
p
).
596
Chapter 9
Hypothesis Testing Theory

generally be relied upon to produce tests with these properties. However, the
power functions of such tests are nonetheless often quite adequate for hypothe-
sis testing applications, and the tests frequently do possess optimal properties.
9.6
Noncentral t-Distribution
Family Name: Noncentral t-Distribution
Parameterization:
v (degrees of freedom) and l (noncentrality parameter).
(v,l) ∈O ¼ {(v,l): v is a positive integer, l ∈(1,1)}
Density Deﬁnition:
f t; v; l
ð
Þ ¼
vv=2
p1=2Gðv=2Þ2 v1
ð
Þ=2 t2 þ v

 vþ1
ð
Þ=2 exp 
l2v
2 t2 þ v
ð
Þ
"
#

ð1
0
exp  1
2
x 
lt
t2 þ v
ð
Þ1=2
 
!2
2
4
3
5xvdx
Moments:
m ¼ a1l for v > 1; s2 ¼ a2l2 þ a3 for v > 2; m3 ¼ a4l3 þ a5l for v > 3;
where:
a1 ¼ v=2
ð
Þ1=2G v  1
2


=G v
2
 
; a2 ¼
v
v  2  a2
1; a3 ¼
v
v  2 ;
a4 ¼ a1
vð7  2vÞ
ðv  2Þðv  3Þ þ 2a2
1


; and a5 ¼
3v
v  2
ð
Þ v  3
ð
Þ
ð
Þ


a1:
Note that the expression for
m4=s4  3


is quite complicated, but it can be
determined using the general result on higher order moments presented below.
MGF: Does not exist. However, higher order moments about the origin
(from which moments about the mean can also be derived) can be deﬁned
directly as follows:
m0
k ¼ E Tk


¼
n
2
 k
2 G nk
2


G n
2
  exp  l2
2
 
!
dk
dlk
exp l2
2
 
!
 
!
if n > k;
Does not exist
if n  k:
8
>
<
>
:
Background and Application: The noncentral t-distribution is the distribution
of the random variable
T ¼
Zﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Y=v
p
;
where Z  N l; 1
ð
Þ , Y  w2
v, and Z and Y are independent. Note that when l ¼ 0,
T will have the ordinary (central) student t-distribution derived in Section 6.7.
The density deﬁnition can be found by the change of variables approach, similar
to the previous derivation of the student (central) t-density.
9.6
Noncentral t-Distribution
597

When l ¼ 0, the t-distribution is symmetric around zero. When l > 0 (l < 0)
the mean of the distribution moves to the right (left) of zero, and the distribution
becomes skewed to the right (left).
Our principal use of the noncentral t-distribution is in analyzing the power
function of statistical tests based on the t-statistic. We have already encountered
a case where the statistic T ¼ n  1
ð
Þ1=2 X  m0


=S was used to test hypotheses
about the mean of a normal distribution, such as H0: m ¼ m0 versus Ha: m 6¼ m0, or
H0: m  m0 versus Ha: m > m0 (recall Example 9.27 and Deﬁnition 9.17). In par-
ticular, letting
T ¼
X  m0
s=
ﬃﬃﬃn
p
= nS2
s2 = n  1
ð
Þ
 
!1=2
¼
Z
Y= n  1
ð
Þ
ð
Þ1=2 ;
where Z  N
m  m0
ð
Þ= s=
ﬃﬃﬃn
p
ð
Þ; 1
ð
Þ; Y  w2
n1; and Z and Y are independent (as in
the context of Example 9.27), it follows from the deﬁnition of the noncentral t-
density that T ~ f(t;n  1,l), with l ¼ ðm  m0Þ=ðs=
ﬃﬃﬃn
p Þ: Then the power function
of any test deﬁned by Cr ¼
x : t 2 Ct
r


can be expressed as a function of l as
pðlÞ ¼
ð
t2Ct
r
fðt; n  1; lÞdt:
The noncentral t-distribution can be integrated numerically, or the integral can
be approximated via series expansions. Integration of the noncentral t-distribu-
tion adequate for performing power calculations is readily available in software
packages such as GAUSS, SAS, and MATLAB. Regarding power function
calculations, it is useful to note that
ð1
c
fðt; v; lÞdt increases as l increases;
ðc
1
fðt; v; lÞdt increases as l decreases;
and
ðc
c
fðt; v; lÞdt decreases as lj j increases:
For further information, see the excellent survey article by D.B. Owen, (1968),
“A Survey of Properties and Applications of the Noncentral t-Distribution,”
Technometrics, pp.445–478.
9.7
Appendix: Proofs and Proof References for Theorems
Theorem 9.1
Proof
(Most Powerful Level a) Let C
r refer to any other rejection region for which
P(x ∈C
r;Q0)  P(x ∈Cr(k);Q0) ¼ a, so that the test of H0 versus Ha based on C
r
598
Chapter 9
Hypothesis Testing Theory

has a size no larger than the size of the statistical test based on Cr. Note by
deﬁnition that
ICrðkÞðxÞ  IC
r ðxÞ ¼
1  IC
r ðxÞ  0; 8x 2 CrðkÞ
0  IC
r ðxÞ  0; 8x =2 CrðkÞ


:
Also note that
f x; Qa
ð
Þ

<

	
k1f x; Q0
ð
Þif
x 2 CrðkÞ
x =2 CrðkÞ
"
#
;
which follows directly from the deﬁnition of Cr(k). The preceding results
together imply that
ICrðkÞðxÞ  IC
r ðxÞ


f x; Qa
ð
Þ  k1 ICrðkÞðxÞ  IC
r ðxÞ


f x; Q0
ð
Þ;
8x:
Integrating both sides of the inequality over x ∈Rn if X is continuous, or
summing over all x-values for which f(x;Q0) > 0 or f(x;Qa) > 0 if X is discrete,
obtains
P x 2 CrðkÞ; Qa
ð
Þ  P x 2 C
r; Qa


 k1 P x 2 CrðkÞ; Q0
ð
Þ  P x 2 C
r; Q0




 0:
The right hand side of the ﬁrst inequality is nonnegative because k1 > 0 and
because the bracketed probability difference multiplying k1 is nonnegative
(recall that the size of the test based on C
r is no larger than that of the test
based on Cr(k)). Since the probabilities on the left hand side of the ﬁrst inequality
represent the powers of the respective tests when Q ¼ Qa, the test based on Cr(k)
is the most powerful test.
(Unique Most Powerful Size a): We discuss the proof for the discrete case.
The continuous case is proven similarly by replacing summation with integra-
tion. Let the rejection region C deﬁne any other most powerful size a test of
H0: Q ¼ Q0 versus Ha: Q ¼ Qa. Letting EQ0 and EQa denote expectations taken
using the densities f(x;Q0) and f(x;Qa), respectively, note that a ¼ EQ0 ICrðkÞ X
ð Þ


¼ EQ0 IC
r X
ð Þ


, and EQa ICrðkÞ X
ð Þ


¼ EQa IC
r X
ð Þ


, since by assumption both tests
have size a, and both tests are most powerful tests. It follows that
S
x2RðxÞ ICrðkÞðxÞ  IC
r ðxÞ


f x; Q0
ð
Þ  kf x; Qa
ð
Þ
½

¼ EQ0 ICrðkÞðXÞ  IC
r ðXÞ


 kEQa ICrðkÞXÞ  IC
r ðXÞ


¼ 0:
Note further that the summand in the preceding summation is nonpositive
valued 8x, as was shown in the proof of sufﬁciency above. Then the sum itself
can be zero-valued only if the summands equals zero 8x. This in turn implies
that ICrðkÞðxÞ ¼ IC
r ðxÞ 8x 2 G ¼ x : f x; Q0
ð
Þ 6¼ kf x; Qa
ð
Þ
f
g. Thus both Cr(k) and C
r
contain the set of x’s, G, for which f(x;Qo) 6¼ kf(x;Qa).
Regarding when f(x;Qo) ¼ kf(x;Qa) note that x ∈Cr(k) 8 x ∈K ¼ {x: f(x;Q0) ¼
kf(x;Qa)}. Let A ¼ K  C
r be the set of x-values satisfying f(x;Q0) ¼ kf(x;Qa) that
are not in C
r . Then P(x ∈A;Q0) ¼ P(x ∈A;Qa) ¼ 0, for consider the contrary.
If P(x ∈A;Q0) > 0, then since Cr(k) is an a level test and C
r ¼ Cr(k)  A,
it follows that P(x ∈C
r ;Q0) < a ¼ P(x ∈Cr(k);Q0), contradicting that C
r is an
9.7
Appendix: Proofs and Proof References for Theorems
599

a-size rejection region. If P(x ∈A;Qa) > 0, then since C
r does not contain A,
P(x ∈C
r ; Qa) < P(x ∈Cr(k);Qa), contradicting that C
r is a most powerful rejec-
tion region. Thus, the most powerful rejection region for testing H0: Q ¼
Q0 versus Ha: Q ¼ Qa has the form given in the statement of the theorem with
probability 1.
n
Theorem 9.6
Proof
We prove part 1. The proof of part 2 is analogous with appropriate inequality
reversals.
Since f(x;Ya)/f(x;Y0) has a monotone likelihood ratio in t(x), f(x;Ya)/f(x;Y0) 
k1 iff g(t(x))  k1 for some nondecreasing function g(∙). Moreover, g(t(x))  k1
iff t(x)  c where c is chosen to satisfy28 minc gðcÞ
f
g  k1. A Neyman-Pearson
most powerful level a test of H0: Y ¼ Y0 versus Ha: Y ¼ Ya, for Ya > Y0, is
deﬁned by choosing c so that P(t(x)  c;Y0) ¼ a. Note that this probability
depends only on Y0, so that the choice of c does not depend on Ya and thus the
same rejection region Cr ¼ {x: t(x)  c} deﬁnes a Neyman-Pearson level a test of
H0: Y ¼ Y0 versus Ha: Y ¼ Ya, regardless of the value of Ya > Y0. Thus, by
Theorem 9.3, Cr is UMP level a for testing H0: Y ¼ Y0 versus Ha: Y > Y0.
Now examine Y* < Y0 and let a* ¼ P(x ∈Cr;Y*). From the preceding argu-
ment, it is known that Cr represents a UMP level a* test of H0: Y ¼ Y* versus
Ha: Y > Y*, and because the test is then unbiased (Theorem 9.4), we know that
P(x ∈Cr;Y*)  P(x ∈Cr; Y0) ¼ a, which holds 8Y* < Y0. Thus, Cr represents a
level a test of H0: Y  Y0 versus Ha: Y > Y0.
Finally, let C
r represent any other level a test for testing H0: Y  Y0 versus
Ha: Y > Y0. Then by deﬁnition ao ¼ P(x ∈C
r;Yo)  supYQ0 Pðx 2 C
r; YÞ


 a,
so that C
r has size ao  a for testing H: Y ¼ Y0 versus Ha: Y > Y0. Because Cr is
UMP level a for testing this hypothesis, P(x ∈Cr;Y)  P(x ∈C
r ;Y) 8 Y > Y0,
and thus Cr is also a UMP level a test of H0: Y  Y0 versus Ha: Y > Y0.
n
Theorem 9.10
Proof
The random variable z ¼ g(X)  g is symmetric about zero, and thus
P gðxÞ  c2; Y0
ð
Þ ¼ P z  c2  g; Y0
ð
Þ ¼ P z  g  c2; Y0
ð
Þ
¼ P gðxÞ  2g  c2; Y0
ð
Þ
¼ P gðxÞ  c1; Y0
ð
Þ ¼ a=2:
It follows that Cr deﬁnes an a-size test.
To see that d pCr (Y0)/dY ¼ 0 is satisﬁed, ﬁrst note that since pCr (Y) ¼
EY(ICr (X)), it follows from the exponential class representation of the density
of X and Lemma 9.1 that (continuous case-discrete case is analogous)
28If the likelihood ratio is strictly increasing in t(x), then c can be chosen to satisfy g(c) ¼ k1.
600
Chapter 9
Hypothesis Testing Theory

dpCrðYÞ=dY ¼ d
Ð 1
1   
Ð 1
1 ICrðxÞ exp cðYÞgðxÞ þ dðYÞ þ zðxÞ
½
IAðxÞ
dY
¼ EY ICrðXÞgðXÞ
ð
Þ dcðYÞ
dY
þ d dðYÞ
ð
Þ
dY
P x 2 Cr; Y
ð
Þ:
Therefore,
dpCr Y0
ð
Þ=dY ¼ 0 iff EQ0 ICrðXÞgðXÞ
ð
Þ dc Y0
ð
Þ
dY
¼ a d d Y0
ð
Þ
ð
Þ
dY
:
The right-hand side of the iff statement holds under the current assumptions,
since
EQ0 ICrðXÞgðXÞ
ð
Þ dc Y0
ð
Þ
dY
¼ EQ0
gðXÞ  g
ð
ÞICrðxÞ dc Y0
ð
Þ
dY


þ E ICrðXÞ
ð
Þg dc Y0
ð
Þ
dY
¼ ag dc Y0
ð
Þ
dY
¼ a d d Y0
ð
Þ
ð
Þ
dY
where the last equality follows from the fact that EQ0 gðXÞ
ð
Þ dc Y0
ð
Þ=dY
ð
Þ ¼
 d d Y0
ð
Þ
ð
Þ=dY
ð
Þ; because
0 ¼ d
Ð 1
1   
Ð 1
1 expðcðYÞgðxÞ þ dðYÞ þ ZðxÞÞIAðxÞdx
dY
¼ E gðXÞ
ð
Þ dcðYÞ
dY
þ d dðYÞ
ð
Þ
dY
:
n
Theorem 9.12
Proof
We focus on the case where the density function of the random sample is
discrete. The proofs in the continuous case can be constructed using a similar
approach, except the conditional densities involved would be degenerate, so that
line integrals would be needed to assign conditional probabilities to events
(recall the footnote to our deﬁnition of sufﬁcient statistics (Deﬁnition 7.18)
regarding degenerate conditional distribution fðxjsÞ.
Given the deﬁnition of f(x;Q), examine the distribution of X, conditional on
s2,. . .,sk:
f x;Qjs2;...;sk
ð
Þ¼P fxg\fx:siðxÞ¼si;i¼2;...;kg
ð
Þ
Pðs2;...;skÞ
¼
exp c1ðQÞs1ðxÞþPk
i¼2ciðQÞsiþdðQÞþzðxÞ


IA
ðxÞ
exp
Xk
i¼2ciðQÞsi þdðQÞ


S x:si x
ð Þ¼si;i¼2;...;k
ð
Þexp c1 Q
ð
Þs1 x
ð Þþz x
ð Þ
ð
ÞIA x
ð Þ
¼expðc1ðQÞs1ðxÞþd
 c1
ð
ÞþzðxÞÞIA
ðxÞ
where A* ¼ A \ {x: si(x) ¼ si, i ¼ 2,. . .,k}, and
d
 c1
ð
Þ ¼ ln
X
x2A
 exp c1 Q
ð
Þs1 x
ð Þ þ z x
ð Þ
ð
Þ

1
:
Thus, f
ðx; c1ðYÞjs2; . . . ; skÞ is a one-parameter exponential class density with
parameter c1(Q) and parameter space Oc1 ¼ fc1 : c1 ¼ c1ðQÞ; Q 2 Og. Note that c1
9.7
Appendix: Proofs and Proof References for Theorems
601

is differentiable with respect to c1, and it is also true that d*(c1) is differentiable.
The latter result follows from the chain rule of differentiation, since the natural
logarithmic function is differentiable for all positive values of its argument, and
the positive-valued bracketed term in the deﬁnition of d*(c1) is differentiable by
Lemma 9.1 with f(x) ¼ 1.
Since the assumptions of Theorem 9.11 are met under the stated conditions,
all power functions for testing H0 versus Ha are continuous and differentiable.
It follows that, conditional, on (s2,. . .,sk), all of the previous results regarding
UMP and UMPU tests for the case of a single parameter exponential class
density apply. In particular, the deﬁnitions of the Cr0s in cases (1–4) are all
conditionally UMPU level a based on Theorem’s 9.5–9.6 and corollaries
9.1–9.2 for results (1) and (2), and Theorem 9.9 for results (3) and (4).
Examine result (1). Let C
r be any other unbiased level a test, so that pC
r c0
1


¼ a*  a. Then it must be the case that pC
r c0
1jS2; . . . ; Sk


 a with probability 1.
To see this, note that by the iterated expectation theorem
pC
r c0
1


 a
 ¼ Ec0
1 IC
r ðXÞ


 a
 ¼ E Ec0
1 IC
r ðXÞjS2; . . . ; Sk


 a
h
i
¼ 0;
and since the bracketed expression is a function of the complete sufﬁcient statis-
tics (S2,. . .,Sk) it follows by deﬁnition that the bracketed expression must equal
0 with probability 1, and thus Ec0
1 IC
r ðXÞjS2; . . . ; Sk


¼ a
  a with probability 1
(recall Deﬁnition 7.20). Thus, C
r must be conditionally unbiased of level a.
Now since Cr has maximum power for c1 > c0
1 among all unbiased level a
tests conditional on (s2,. . .,sk), it follows from the iterated expectation theorem
that for c1 > c0
1,
pCr c1
ð
Þ ¼ Ec Ec1 ICr X
ð ÞjS2; . . . ; Sk
ð
Þ
ð
Þ  Ec Ec1 IC
r X
ð ÞjS2; . . . ; Sk




¼ pC
r c1
ð
Þ
so that Cr deﬁnes a UMPU level a test of the hypotheses in case (1).
The proof of case (2) follows directly from the proof of case (1) via appropriate
inequality reversals. Moderate extensions of the preceding argument can be used
to prove results (3) and (4) (see T. Ferguson (1967), Mathematical Statistics,
Academic Press, New York, pp. 230–232).
n
Keywords, Phrases and Symbols
pCr ðYÞ power function
Acceptable region
Alternative hypothesis
Composite hypothesis
Conditioning in Multiparameter case
Consistent test sequence of level a
Controlling Type I and
Type II errors
Critical (or rejection) region
H is (not) rejected at the a-level of
signiﬁcance
H is (not) rejected using a size-a test
H: set deﬁning conditions
H0, Ha
Ideal power function
Ideal statistical test
Likelihood ratio test
Maintained hypothesis
Monotone likelihood ratio and the
exponential class of densities
Monotone likelihood ratio in the
statistic t(X)
Monotone likelihood ratios and UMP
level a tests
More powerful level a test
Most Powerful level a test
Multiparameter exponential class
and hypothesis testing
602
Chapter 9
Hypothesis Testing Theory

Neyman-Pearson lemma
Noncentral t-distribution
Noncentrality parameter
Nuisance parameters
Null hypothesis
One-sided and two-sided alternative
hypotheses
Operating characteristic function of a
statistical test
Parametric hypothesis testing
Power function of a statistical test
Power of the test at Q
Protection against Type I error
P-value
Range of X over H, R(X|H)
Rejection (or critical) region
Reparameterizing exponential class
densities via linear transformation
Signiﬁcance level of the test
Simple hypothesis
Size of test
Statistical hypothesis
Statistical test deﬁned by Cr
Sufﬁcient statistic representation of
likelihood ratio test
Sufﬁcient statistic representation of
test of H0: Q ¼ Qo versus
Ha: Q∈Oa
Test of a statistical hypothesis
Test statistic
Type I and Type II errors
Type I/Type II error tradeoff
UMP level a test of H0: Q ¼ Qo
versus Ha: Q 6¼ Qo,
nonexistence in case of MLR
UMP test of H0: Q ¼ Qo versus
Ha: Q∈Oa, Neyman-Pearson
approach
UMPU level a test of H0: Q ¼ Qo
versus Ha: Q 6¼ Qo in case of
exponential class density
UMPU level a test of
H0: Q1  Q  Q2 versus
Ha: Q < Q1 or Q > Q2 in case
of exponential class density
UMPU level a test of H0: m  m0
versus Ha: m > m0, and of
H0: m ¼ m0 versus Ha: m 6¼m0,
normal population
UMPU level a test of H0: s2  s2
0
versus Ha: s2 > s2
o when
sampling from normal
distribution
Unbiasedness of a test rule
Unbiasedness of most powerful test
of H0: Q ¼ Qo versus
Ha: Q ¼ Qa
Unbiasedness of UMP level a tests
using monotone likelihood
ratios
Uniformly most powerful level a test
Problems
1. A shipment of 20 projection screen television sets is
at the receiving dock of a large department store. The
department store has a policy of not accepting shipments
that contain more than 10 percent defective merchan-
dise. The receiving clerk is instructed to have a quality
inspection done on two sets that are randomly drawn,
without replacement, from the shipment. Letting k rep-
resent the unknown number of defective sets in the ship-
ment of 20 sets, the null hypothesis H0:k  2 will be
rejected iff both sets that are inspected are found to be
defective.
(a) Calculate the probabilities of committing type I
errors when k ¼ 0, 1, or 2.
(b) Calculate the probabilities of committing type II
errors when k  3.
(c) Plot the power function of this testing procedure.
Interpret the implications of the power function
from the standpoint of both the department store
and the television manufacturer.
2. A pharmaceutical company is analyzing the effec-
tiveness of a new drug that it claims can stimulate hair
growth in balding men. For the purposes of an adver-
tising campaign, the marketing department would like
to be able to claim that the drug will be effective for at
least 50 percent of the balding men who use it. To test
the claim, a random sample of 25 balding men are
given the drug treatment, and it is found that 10
applications were effective in stimulating hair growth.
The population of balding men is sufﬁciently large that
you may treat this as a problem of random sampling
with replacement.
(a) Test the null hypothesis H0: p  .50 using as close to
a .10-size test as you can.
(b) Plot the power function for this test. Interpret the
power function from the standpoint of both the phar-
maceutical company and the consuming public.
(c) Is the test you used in (a) a UMP test? Is it a UMPU
test? Is it an unbiased test? Is it a consistent test?
(d) Calculate and interpret the P-value for the test.
3. In each case below, identify whether the null and the
alternative
hypotheses
are
simple
or
composite
hypotheses.
(a) You are random sampling from a gamma population
distribution and you are testing H0: a  2 versus Ha:
a > 2.
Problems
603

(b) You are random sampling from a geometric popula-
tion distribution and you are testing H0: p ¼ .01 ver-
sus Ha: p > .01.
(c) The joint density of the random sample Y ¼ xb + « is
N(xb,s2I) and you are testing whether b ¼ 0.
(d) You are random sampling from a poisson population
distribution and you are testing H0: l ¼ 2 versus Ha:
l ¼ 3.
4. A large metropolitan branch of a savings and loan is
examining stafﬁng issues and wants to test the hypothe-
sis that the expected number of customers requiring the
services of bank personnel during the midweek (Tuesday-
Thursday) noon hour is  50. The bank has obtained the
outcome
of
a
random
sample
consisting
of
100
observations on the number of noon hour customers
requiring service from bank personnel. It was observed
that x ¼ 54. You may assume that the population distri-
bution is Poisson in this case.
(a) Design a UMP level .05 test of the null hypothesis
having size as close to .05 as possible. Test the
hypothesis.
(b) Plot the power function for the test. Interpret the
power
function
both
from
the
standpoint
of
management’s desire for staff reductions and the need
to provide quality customer service to bank customers.
5. The annual proportion of new restaurants that sur-
vive in business for at least 1 year in a U.S. city with
population  500,000 people is assumed to be the out-
come of some Beta population distribution. Part of the
maintained hypothesis is that b ¼ 1 in the Beta distribu-
tion, so that the population distribution is assumed to be
Beta(a,1). A random sample of size 50 from the beta popu-
lation distribution results in the geometric mean xg ¼ .84.
(a) Deﬁne a UMP level .05 test of the hypothesis that
less than three-quarters of new restaurants are
expected to survive at least 1 year in business in
U.S. cities of size  500,000. Test the hypothesis.
(b) Plot the power function for the test. Interpret the
power function both from the standpoint of a poten-
tial investor in a restaurant and from the perspective
of the managing director of a chamber of commerce.
(c) Calculate and interpret the p-value for the test.
6. A complaint has been lodged against a major domes-
tic manufacturer of potato chips stating that their 16 oz
bags of chips are being underﬁlled. The manufacturer
claims that their ﬁlling process produces ﬁll weights
that are normally distributed with a mean of 16.1 oz and
a standard deviation of .05 so that over 97 percent of
their product has a weight of  16 oz. They suggest that
their product be randomly sampled and their claims be
tested for accuracy. Two independent random samples of
observations on ﬁll weights, each of size 250, resulted in
the following summary statistics x1 ¼ 16:05; x2 ¼ 16:11;
s2
1 ¼ .0016, and s2
2 ¼ .0036.
(a) Deﬁne a UMPU level .05 test of H0: m ¼ 16.1 versus
Ha: m 6¼ 16.1 based on a random sample of size 250.
Test the hypothesis using the statistics associated
with the ﬁrst random sample outcome. Plot and
interpret the power function of this test.
(b) Deﬁne a UMPU level .05 test of H0: s  .05 versus
Ha: s > .05 based on a random sample of size 250.
Test the hypothesis using the statistics associated
with the second random sample outcome. Plot and
interpret the power function of this test.
(c) Calculate and interpret the p-values of the tests in (a)
and (b). (Hint: It might be useful to consider
Bonferroni’s inequality for placing an upper bound
on the probability of Type I Error.)
(d) Treating the hypotheses in (a) and (b) as a joint
hypothesis on the parameter vector of the normal
population distribution, what is the probability of
Type I Error for the joint hypothesis H0: m ¼ 16.1
and s  .05 when using the outcome of the two test
statistics above to determine acceptance or rejection
of the joint null hypothesis? Does the complaint
against the company appear to be valid?
(e) Repeat
(a–c)
using
a
pooled
sample
of
500
observations.
7. In a random sample of size 10 from a Bernoulli popu-
lation distribution, how many (nonrandomized) critical
regions can you deﬁne that have size  .10 and that are
also unbiased for testing the null hypothesis H0: p ¼ .4
versus Ha: p 6¼ .4 ?
8. Randomized Test It was demonstrated in Example
9.10 that the choices of size for most powerful tests of
the hypothesis H0: p ¼ .2 versus Ha: p ¼ .8 was quite
limited. Suppose that a .05 level test of the null hypothe-
sis was desired and that you were willing to utilize a
randomized test. In particular, examine the following
randomized test rule:
x  8 ) reject H0
604
Chapter 9
Hypothesis Testing Theory

x ¼ 7 ) reject H0 with probability t, do not reject H0 with
probability (1  t)
x  6 ) do not reject H0
To implement the rule when x ¼ 7 occurs, a uniform
random number z with range (0,1) could be drawn, and if
z  t, H0 would be rejected, and if z > t, H0 would not be
rejected.
(a) Find a value of t that deﬁnes a .05 test of the null
hypothesis.
(b) It is possible that two analysts, using exactly the
same random sample outcome and using exactly the
same test rule could come to different conclusions
regarding the validity of the null hypothesis. Explain.
(This feature of randomized tests has discouraged
their use.)
(c) Is the test you deﬁned in (a) an unbiased size .05 test
of the null hypothesis?
(d) Is the test you deﬁned in (a) a most powerful size .05
test of the null hypothesis?
9. The number of work-related injuries per week that
occur at the manufacturing plant of the Excelsior Corpo-
ration is a Poisson-distributed random variable with mean
l  3, according to company analysts. In an attempt to
lower insurance costs, the Corporation institutes a pro-
gram of intensive safety instruction for all employees.
Upon completion of the program, a 12-week period pro-
duced an average of two accidents per week.
(a) Design a uniformly most powerful level .10 test of
the null hypothesis H0: l  3 versus the alternative
hypothesis Ha: l < 3 having size as close to .10 as
possible without exceeding .10.
(b) Test the null hypothesis using the test you deﬁned in
part (a). What can you say about the effectiveness of
the safety program?
10. Being both quality and cost conscious, a major for-
eign
manufacturer
of
compact
disk
players
is
contemplating their warranty policy. The standard war-
ranty
for
compact disk players
sold by competing
producers is 1 year. The manufacturer is considering a
2 year warranty. The operating life until failure of your
compact disk player has the density
fðx; bÞ ¼ ðx= b2Þ expðx=bÞ Ið0;1Þ ðxÞ
for some value of b > 0, where x is measured in years.
(a) The manufacturer wants their exposure to warranty
claims to be, on average, no more than 5 percent of
the units sold. Find the values of b for which
Pðx  2; bÞ¼ Ð 2
0 fðx;bÞdx  :05:
(b) Based on a random sample of size 50, design a uni-
formly most powerful level .10 test of the null
hypothesis that b will be in the set of values you
identiﬁed in (a).
(c) A nondestructive test of the disk players that
determines their operating life until failure is applied
to 50 players that are randomly chosen from the
assembly
line.
The
measurements
resulted
in
x ¼ 4:27: Test the null hypothesis in (b).
(d) Plot the power curve of this test, and interpret its
meaning to the management of the manufacturing
ﬁrm.
11. Referring to Deﬁnition 9.14, state the form of the
UMPU level a test of the null hypothesis H0: s2  s2
0
versus Ha: s2 < s2
0. Justify your answer.
12. Referring to Deﬁnition 9.15, state the form of the
UMPU level a test of the null hypothesis H0: m  m0 versus
H0: m < m0. Justify your answer.
13. Your company supplies an electronic component that
is critical to the navigational systems of large jet aircraft.
The operating life of the component has an exponential
distribution with some mean value v, where operating life
is measured in 100,000 hour units. You are seeking a
contract to supply these components to a major aircraft
manufacturer on the West Coast. The contract calls for a
minimum mean operating life of 750,000 hours for the
component, and you must provide evidence on the reli-
ability of your product. You have a random sample of
observations on tests of 300 of your components that pro-
vide measurements on the components’ operating lives.
According to the tests performed, the mean operating life
of the components was 783,824 hours.
(a) In designing a UMP level a test in this situation,
should the null hypothesis be deﬁned as y  7.5 or
y  7.5? Base your discussion on the characteristics
of the power function of each of the tests.
(b) Design a UMP size a test of whichever null hypothe-
sis you feel is appropriate based on you discussion in
(a). Choose whatever size test you feel is appropriate,
and discuss your choice of size.
(c) Conduct the hypothesis test. Should your company
get the contract? Why or why not?
14. The Pareto distribution
Problems
605

fðx; y; cÞ ¼ cy y xð1þyÞ Iðc;1Þ ðxÞ
for y > 1 and c > 0 has been used to model the distribu-
tion of incomes in a given population of individuals,
where c represents the minimum level of income in the
population. In a certain large state on the east coast the
ofﬁce of ﬁscal management is investigating a claim that
professors at teaching colleges have average annual
salaries that exceed the state average annual salary for
middle management white collar workers, which is
known to be $62,471. A random sample of 250 professors’
salaries were obtained, and the geometric mean of the
observations was found to be
Qn
i¼1 xi

1=n ¼ 61:147
where the xi0s are measured in 1,000’s of dollars. As
part of the maintained hypothesis, the value of c is
taken to be 30.
(a) Express the mean level of income as a function of the
parameter y.
(b) Deﬁne a test statistic on which you can base a UMP
level a test of the null hypothesis H0: m  62.471
versus Ha: m > 62.471.
(c) Deﬁne a UMP size .05 test of the null hypothesis H0:
m  62.471 versus Ha: m > 62.471. (Hint: Use a test
statistic for which you can apply an asymptotic nor-
mal distribution and use the normal approximation).
(d) Test the hypothesis. Are university professors paid
more than white collar middle management workers
in this state?
15. Suppose that a random sample of size n is drawn from
a normal population distribution for which s2 is assumed
to be known and equal to the given value s2

 . Deﬁne
UMPU level a tests of the following null hypotheses:
(a) H0: m  m0 versus Ha: m > m0
(b) H0: m  m0 versus Ha: m < m0
(c) H0: m ¼ m0 versus Ha: m 6¼ m0
16. Suppose that a random sample of size n is drawn from
a normal population distribution for which m is assumed
to be known and equal to the given value m*. Deﬁne
UMPU level a tests of the following null hypotheses:
(a) H0: s2  s0
2 versus Ha: s2 > s0
2
(b) H0: s2  s0
2 versus Ha: s2 < s0
2
(c) H0: s2 ¼ s0
2 versus Ha: s2 6¼ s0
2
17. Control Charting: A large mail-order house has
initiated a quality control program. They randomly sam-
ple 100 of each day’s orders and monitor whether or not
the mail order-taking process is “under control” in the
sense that errors in order-taking are at minimum levels.
The number of orders per day is sufﬁciently large that one
can assume that the sampling is done with replacement.
The daily error proportion prior to the initiation of this
program has been 3.2 percent.
(a) Deﬁne a UMPU level a test of the hypothesis H0:
p ¼ .032 versus Ha: p 6¼ .032. You may use the
asymptotic normal distribution of the test statistic
in deﬁning the critical region.
(b) Conduct a size .05 UMPU test of the null hypothesis on
a day where x ¼ 3:4. Is the order process under control?
(c) After quality control training and closer monitoring
of clerical workers, a daily random sample resulted in
x ¼ 2:6. Is there evidence that quality has increased
over what it has been in the past? Why or why not?
(The mail order company of Alden’s Inc. was one of the
earliest companies to use control charting techniques for
monitoring clerical work. See Neter, John, (1952), “Some
Applications of Statistics for Auditing”, Journal of the
American Statistical Association, March, pp. 6–24.
18. Revisit Example 9.27 and the power function graph in
Figure 9.15 and consider the implications of the power
function graph in the two dimensional parameter space
(m,s).
(a) Plot the power surface in three dimensions, the axes
referring to power, the value of m, and the value of s.
(This is probably best done with the aid of a
computer).
(b) Plot the isopower contour in the (m,s)-plane for a
power level of .90. (An isopower contour is the set of
(m,s) points that result in the same level of power,
which in the case at hand is equivalent to the set of
(m,s) points that result in the same value of the
noncentrality parameter l). Interpret the isopower
contour with respect to the ability of the test to
detect deviations from the null hypothesis.
19. The Gibralter Insurance Co. is reevaluating the
premiums it charges on car insurance and is analyzing
classiﬁcations of cars into high risk, average risk, and
low risk on the basis of frequency of accidents. They are
606
Chapter 9
Hypothesis Testing Theory

currently examining an imported mid-size four-door
sedan and wish to examine whether its frequency of
claims history is signiﬁcantly different than a range of
values considered to be consistent with the expected fre-
quency of claims of the average risk class, the range being
between 2 percent and 4 percent of the vehicles insured. A
random sample with replacement of 400 insured vehicles
of the type in question resulted in a claims percentage of
7 percent.
(a) Design a UMPU level a test of the null hypothesis H0:
m ∈[2,4] versus Ha: m =2 [2,4]. You can base your test
on the asymptotic normal distribution of the test
statistic.
(b) Test the null hypothesis using a size .05 UMPU test.
Does the outcome contradict classifying the vehicle
in the average risk class? Why or why not?
(c) Supposing you rejected the hypothesis, what would
be your conclusion? Is further statistical analysis
warranted?
20. A certain business uses national telephone solicita-
tion to sell its product. Its sales staff have individual
weekly sales quotas of 10 sales that they must meet or
else their job performance is considered to be unsatisfac-
tory and they receive only base pay and no sales commis-
sion. In hiring sales staff, the company has claimed that
the proportion of customers solicited that will ultimately
buy the company’s product is .05, so that on average, 200
phone calls per week should produce the required 10
sales. The company requires that a salesperson keep a
record of how many phone solicitations were made, and
when the 10th sale is made, the salesperson must indicate
the number of phone calls that were made to obtain the 10
sales. The data on the last 200 weekly quotas that were
met by various salespersons indicated that 289 phone
calls were needed, on average, to meet the quota. A dis-
gruntled
employee
claims
that
the
company
has
overstated the market for the product, and wants the
quota lowered.
(a) Deﬁne a UMP level a test of the hypothesis H0:
p ¼ .05 versus Ha: p < .05. You may use an asymp-
totic normal distribution for the test statistic, if it has
one.
(b) Test the hypothesis with a UMP size .10 test. Does
the disgruntled employee have a legitimate concern?
(c) Examine the asymptotic power function of the test
(i.e., construct a power function based on the asymp-
totic normal distribution of the test statistic). Inter-
pret the implications of the power function for the
test you performed, both from the perspective of the
company and from the perspective of the employee. If
you were primarily interested in worker’s rights,
might you design the test differently and/or would
you consider testing a different null hypothesis?
Explain.
(d) Suppose there was a substantial difference in the
abilities of salespersons to persuade consumers to
purchase the company’s product. Would this have
an impact on your statistical analysis above? Explain.
Problems
607

10
n
Hypothesis Testing Methods
and Conﬁdence Regions
n
n
n
10.1
Introduction
10.2
Heuristic Approach to Statistical Test Construction
10.3
Generalized Likelihood Ratio Tests
10.4
Lagrangian Multiplier Tests
10.5
Wald Tests
10.6
Tests in the Context of the Classical GLM
10.7
Conﬁdence Intervals and Regions
10.8
Nonparametric Tests of Distributional Assumptions
10.9
Noncentral w2- and F-Distributions
10.10
Appendix: Proofs and Proof References for
Theorems
10.1
Introduction
In this chapter we examine general methods for deﬁning
tests of statistical hypotheses and associated conﬁdence intervals and regions for
parameters or functions of parameters. In particular, the likelihood ratio, Wald,
and Lagrange multiplier methods for constructing statistical tests are widely
used in empirical work and provide well-deﬁned procedures for deﬁning test
statistics, as well as for generating rejection regions via a duality principle that
we will examine. In addition, it is possible to deﬁne useful test statistics and
conﬁdence regions based entirely on heuristic principles of test construction.
None of these four methods is guaranteed to produce a statistical test with
optimal properties in all cases. In fact, no method of deﬁning statistical tests
can provide such a guarantee. The virtues of these methods are that they are
relatively straightforward to apply (in comparison to direct implementation of
many of the theorems in Section 9.5), they are applicable to a wide class of
problems that are relevant in applications, they generally have excellent asymp-
totic properties, they often have good power in ﬁnite samples, they are some-
times unbiased and/or UMP, and they have intuitive appeal.
The four methods of deﬁning test rules presented in this chapter, together
with the results on UMP and UMPU testing in Section 9.5, by no means exhaust

the ways in which statistical tests can be deﬁned. (See E. Lehmann, Testing
Statistical Hypotheses, for further reading). However, the methods we present
cover the majority of approaches to deﬁning test rules used in practice. Ulti-
mately, regardless of the genesis of a statistical test, whether the test is useful in
an application will depend on its properties, as discussed in Chapter 9.
We will also introduce in this chapter a number of nonparametric tests that
have been designed to assess assumptions relating to the functional form of
the joint density of a probability sample, as well to assess the ubiquitous iid
assumption underlying simple random samples.
As we noted at the beginning of the previous two chapters, we will choose to
move the proofs of some theorems to the Appendix of this chapter to enhance
readability.
10.2
Heuristic Approach to Statistical Test Construction
In the heuristic approach to deﬁning statistical tests, one attempts to implement
the following general heuristic principle of test construction: “Discover a test
statistic whose probabilistic behavior is different under H0 and Ha, and exploit
the difference in deﬁning a rejection region for a statistical test.” For the choice
of an appropriate test statistic T¼t(X), one might examine a good estimator of
Q or q(Q), such as a maximum likelihood, least-squares, or generalized method
of moments estimator. Alternatively, a (minimal and/or complete) sufﬁcient
statistic for Q or q(Q) might be useful for deﬁning a test statistic.
If the range of a statistic T over H0[Ha can be partitioned as CT
r [ CT
a so that
P(t∈CT
r ;Q)  a 8Q∈H0, a level a test of H0 will have been deﬁned. If it is also true
that P(t∈CT
r ;Q)  a 8 Q∈Ha, the test would be unbiased. If the power function
pT
Cr Q
ð
Þ ¼ P t 2 CT
r ; Q


is acceptable to the analyst, the test represents at least a
useful, if not optimal, statistical test of H0 versus Ha. The heuristic approach can
be a substantially less complicated method of deﬁning a test statistic than were
the classical UMP and UMPU approaches, which were discussed in Section 9.5.
The following examples illustrate the heuristic method:
Example 10.1
Heuristic Test of
H0: l  l0 Versus
Ha: l > l0 in Poisson
Distribution
At the Union Bank a debate is raging over the expected rate at which customers
arrive at the teller windows. If the expected rate is  2 per minute, management
feels that stafﬁng can be reduced. A random sample of 100 observations from the
(assumed) Poisson(l) population of customer arrivals/minute was obtained,
and the outcome of the complete sufﬁcient statistic, and MLE estimator of l, X;
was 2.48.
In order to test H0: l  2 versus Ha: l > 2, the joint density of the random
sample is given by f x; l
ð
Þ ¼
enll
P
n
i¼1
xi
Q
n
i¼1
xi!

2
4
3
5 Q
n
i¼1
I 0;1;2;...
f
g xi
ð
Þ; with n ¼ 100.
Note that the statistic nX ¼ Pn
i¼1 Xi  Poisson(nl) (straightforwardly shown via
the MGF approach), and thus the event 100x  c will be more probable for higher
values of l than for lower values. Thus heuristically, it seems reasonable to
610
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

reject H0: l  2 for large values of 100 x and not reject for small values.
Furthermore, the probability of rejection, P(100 x  c; 100l), increases as l
increases, and so to identify a value of c that deﬁnes a level a test of H0 it sufﬁces
to choose c so that P(100x  c; 200) ¼ a, since then P(100x  c; 100l) < a 8 l < 2.
The preceding observation also implies that the rejection region deﬁned by
100 x  c deﬁnes an unbiased a level test.
With the aid of the computer, we identify the following potential test sizes:
c
P(100x  c; 200) ¼ 1  Pc1
j¼0 e200200 j=j!
219
.097
220
.086
221
.075
222
.066
223
.058
224
.050
225
.044
Assuming the labor union and bank management agree that a level .05 test
is acceptable the rejection region expressed in terms of the test statistic X is
CX
r ¼
x : x  2:24
f
g. Because the outcome x ¼ 2.48 ∈CX
r , H0 is rejected at the
.05 level. Using the results of Section 9.5, it can be further shown that CX
r
deﬁnes the UMP level .05 test of H0: l  2 versus Ha: l > 2. Note that the
p-value for this test is P(x  2.48; 200) ¼ P(nx  248; 200) < .001, which indicates
there is strong evidence against H0.
□
The next example deals with a complicated case involving nuisance
parameters, and yet the heuristic principle of test construction leads rather
straightforwardly to a test with acceptable power characteristics. In fact, the
test is a UMPU level a test.
Example 10.2
Heuristic Test for
Signiﬁcance of a
Regression Parameter
Revisit Example 8.3 relating to the estimation of a linear relationship
between the number of new homes sold and determinants of home sales.
Assume that the disturbance term of the linear model is approximately normally
distributed (why must this be only an approximation?), so that (approximately)
^b ¼ x0x
ð
Þ1x0Y  N b; s2 x0x
ð
Þ1


. The realtor wishes to test whether mortgage
interest rates actually impact home sales, and so she speciﬁes the hypothesis
H0: b2 ¼ 0 versus Ha: b2 6¼ 0, where b2 is the parameter associated with the
mortgage interest rate variable.
Testing the hypothesis at the .05 level under the prevailing assumptions,
note that ^b2  N b2; var ^b2




, where var(^b 2) is the (2,2) entry in the
2  2
ð
Þ
covariance matrix s2(x0x)1. Consider the statistic T ¼ ^b2=d
std ^b2


for deﬁning a
statistical test of the hypothesis, where d
std ^b2


is the square root of the (2,2)
variance entry d
var ^b2


in the covariance matrix estimator ^S
2(x0x)1. We know
10.2
Heuristic Approach to Statistical Test Construction
611

that ^b2 and the estimator of s2, ^S
2, are independent random variables under the
normality assumption, and in the case at hand
^b2  b2


= var ^b2


h
i1=2
 N 0; 1
ð
Þ
and n  k
ð
Þ^S
2=s2  w2
nk. It follows that
^b2  b2


= var ^b2


h
i1=2
^S
2=s2
h
i1=2
¼
^b2  b2
varð^b2Þ
h
i1=2
has a (central) t-distribution with n-k degrees of freedom. Then under H0: b2 ¼ 0,
T ¼ ^b2=d
std ^b2


has a central t-distribution with n-k degrees of freedom, whereas
under Ha: b2 6¼ 0, T has a noncentral t-distribution with n-k degrees of freedom
and noncentrality parameter l ¼ b2/[var( ^b2 )]1/2 (recall Section 9.6). We then
know that P(|t|  c;l) increases monotonically as |l| ! 1, so that the event
|t|  c becomes ever more probable as |b2| ! 1. Thus heuristically, it
seems reasonable to reject H0: b2 ¼ 0 for large values of |t| and not reject
for small values. Therefore, the rejection region for the test statistic T will be
CT
r ¼(1,c][[c,1). Given the aforementioned behavior of P(|t|c;l) as a
function of l, it follows that CT
r will deﬁne an unbiased a level test of H0.
Referring to the table of the (central) t-distribution, we ﬁnd that P(t c) ¼
.025 when c ¼ 2.571 and degrees of freedom equal nk ¼ 10–5 ¼ 5. Then, by the
symmetry of the (central) t-distribution, it follows that P(t∈CT
r ) ¼ .05 whereCT
r ¼
(1,2.571][[2.571,1). Since the outcome of T is t ¼ 3.33∈CT
r , we reject
H0 and conclude that mortgage interest rates do impact home sales. The p value
of the test is given by P(|t|  3.33) ¼ .021, which suggests that the evidence
against H0 is substantial.
-6.5 -5.5 -4.5 -3.5 -2.5 -1.5 -0.5 0.5
1.5
2.5
3.5
4.5
5.5
6.5
0
0.2
0.4
0.6
0.8
1
p(l)
l
Figure 10.1
Power function of
UMPU size .05
test of H0: b2 ¼ 0
versus Ha: b2 6¼ 0.
612
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

The power function of the test can be calculated with the aid of a computer
(we use the GAUSS procedure CDFTNC here). Selected values of the power
function are displayed in the following table, and a graph of the power function
as a function of l ¼ b2/[var(^b2)]1/2 is given in Figure 10.1.
l ¼ b2/[var(^b2)]1/2
p(l) ¼ P(t∈CT
r ;l)
0
.05
.5, .5
.07
1, 1
.13
1.5, 1.5
.23
2, 2
.37
3, 3
.67
4, 4
.89
5, 5
.98
Note that l effectively measures deviations of b2 from 0 in terms of standard
deviation units (i.e., b2 is expressed relative to the standard deviation [var(^b2)]1/2).
The power function suggests that b2 must be a number of standard deviations
away from zero before the test has appreciable power in rejecting H0. This is
typical for power functions associated with tests of the signiﬁcance of
parameters estimated by least squares (or many other procedures for that
matter), and suggests that to be able to detect small departures of b2 from zero,
b2 must be estimated quite accurately (i.e., var(^b2) must be small).
In this application, it might be argued, based on considerations of economic
theory, that b2 must be nonpositively valued. If this view is adopted as part of the
maintained hypothesis, then a one-sided alternative hypothesis can be consid-
ered as Ha: b2 < 0. The reader is invited to reexamine the problem of testing H0:
b2 ¼ 0 using the one-sided alternative hypothesis and a .05 level test. In such a
case, the power of the test for b2∈Ha is increased.
□
In the next example we revisit Example 10.1 and illustrate how the asymp-
totic distribution of a test statistic can be used to deﬁne an asymptotic test of H0.
In the case at hand, this will allow the test to be conducted using a standard
normal distribution. It also allows one to circumvent the limited choice of test
sizes in this discrete case, albeit in an approximate sense.
Example 10.3
Heuristic Test of
H0: l  l0 Versus
Ha: l > l0 for Poisson
Based on Asymptotic
Normality
Revisit Example 10.1 and consider using the asymptotic normal distribution of
the test statistic X to conduct the test of H0: l  2 versus Ha: l > 2. In this case,
where we are random sampling from the Poisson population distribution,
we know that X 
a N l; n1l


, and thus Z ¼ n1=2 X  l


=l1=2 
a N 0; 1
ð
Þ. Using
heuristic reasoning analogous to that used in Example 10.1, we should reject
H0 for large values of x, and thus for large values of z. This suggests a rejection
region of the form CZ
r ¼ [c,1) for outcomes of Z. Referring to the standard
normal table assuming a .05 level test is desired, we choose c ¼ 1.645, so that
P(z  1.645) ¼ .05 (as an asymptotic approximation).
The reader may have suspected a serious practical ﬂaw in the use of Z as
a test statistic—namely, l is unknown so that Z is not a statistic. However,
10.2
Heuristic Approach to Statistical Test Construction
613

this problem is overcome by assigning l a numerical value. Setting l ¼ 2 in
the deﬁnition of Z is the logical choice under the current circumstances. This
follows because if X ~ N(l, n1 l), thenZ ¼ n1=2 X  2


=21=2  N
n=2
ð
Þ1=2 l  2
ð
Þ;

l=2

; and thus (asymptotically)
pCrðlÞ ¼ Pðz  1:645Þ

>


:05;
8l

>


2;
so that CZ
r then deﬁnes a .05 level unbiased test of H0: l  2 versus Ha: l > 2.
Restating the test in terms of a rejection region for the outcome of X itself,
and recalling that n ¼ 100 in this case, we have that z  1.645 iff x  2.23, so
that CX
r ¼ [2.23,1), which is virtually the same rejection region as in Example
10.1 based on the actual Poisson population distribution. The near equivalence
of the two rejection regions is due to the accuracy of the asymptotic normal
distribution of X. Since x ¼ 2.48 ∈CX
r , we reject H0, as before. An asymptotic p-
value for the test can be calculated based on the observed value of z ¼ 10(2.48 
2)/21/2 ¼ 3.394, so that p value ¼
Ð 1
3:394N(z; 0,1) dz < .001 provides strong evidence
for the rejection of H0. Also, under the assumption of normality for X, it could be
argued, using the results of Section 9.5, that the preceding test is a UMP level .05
test. In this case, we then state that the test is asymptotically UMP level .05.
The reader might consider plotting the power function of the test.
□
In each of the preceding examples, a speciﬁc random sample size was
involved. One might also consider whether sequences of level .05 unbiased
tests deﬁned analogously to the preceding tests have the property of consistency
as sample sizes increase without bound. Using asymptotic theory applied to
both the respective test statistics and the deﬁnitions of rejection regions, one
can argue that the respective test sequences are indeed consistent.
Example 10.4
Consistency of Some
Heuristic Tests
Revisit Examples 10.1–10.3, and consider whether the respective sequences of
.05-level unbiased tests are consistent. Regarding Example 10.1, the nth element
in the appropriate sequence of rejection regions will be of the form CX
rn ¼ [cn,1)
such that P(xn  cn)  .05 8n, where l ¼ 2 is used when deﬁning the probability
of the rejection region for Xn and cn is chosen as small as possible in order to
maximize power for l∈Ha. The preceding sequence of probability inequalities
can be written alternatively as
P n1=2 xn  l
ð
Þ=l1=2  n1=2 cn  l
ð
Þ=l1=2


 :05; 8n;
and assuming l were the true Poisson mean, Zn ¼ n1=2 Xn  l


=l1=2 !
d N 0; 1
ð
Þ by
the LLCLT. Therefore, n1/2(cn 
l)/l1/2 ! 1.645 when deﬁning the rejection
region, which implies cn ! l, and given that l ¼ 2 was used in deﬁning the
rejection region, then cn ! 2. By the consistency of Xn for l, we also know that
Xn !
p l, and if the true Poisson mean is l > 2, so that Ha is true, then Xn  cn !
p
l  2>0 so that H0 is rejected with probability converging to 1 when Ha is true.
Thus the sequence of tests is consistent, and a similar argument can be applied to
demonstrate consistency in the case of Example 10.3.
614
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Regarding Example 10.2, note that the rejection region for the T-statistic will
ultimately converge to CT
rn ! CT
r ¼ 1; 1:96
ð
 [ 1:96; 1
½
Þ since the (central)
Tn random variable is such that Tn !
d Z  N 0; 1
ð
Þ and P(z∈CT
r ) ¼ .05. Assuming
s2(x0x)1 ! 0 so that var(^b2 ) ! 0 and ^b !
p b, and assuming that ^S
2 !
p s2 (e.g.,
assume that the error terms in the linear model are iid), it follows that the
noncentrality parameter of the t-distribution associated with Tn ¼ ^b2=d
std ^b2


,
which is given by l ¼ b2/[var(^b2)]1/2, diverges to inﬁnity as n!1 whenever b2 6¼ 0.
Then P(tn∈CT
rn; l) ! P(tn∈CT
r ;l) ! 1 as n!1 when Ha is true, and thus the test is
consistent.
□
10.3
Generalized Likelihood Ratio Tests
As its name implies, a generalized likelihood ratio (GLR) test of a statistical
hypothesis is based on a test rule that is deﬁned in terms of a ratio of likelihood
function values. The adjective generalized is used here to distinguish the GLR
test from the simple ratio of two likelihood function values presented in our
discussion of the Neymann-Pearson Lemma (Theorem 9.1). In the latter context,
the likelihood function was evaluated at two distinct values of the parameter
vector, Q. In the current context, the likelihood ratio is “generalized” by
forming the ratio of likelihood function suprema1 (or maxima if maximums
exist), where the two suprema are taken with respect to two different feasible
sets–the set of null hypothesis values for Q, and the set of values of Q
represented by the union of the null and alternative hypotheses. The GLR test
is a natural procedure to use for testing hypotheses about Q or functions of Q
when maximum likelihood estimation is being used in a statistical analysis.
Deﬁnition 10.1
Generalized Likelihood
Ratio (GLR) Test
of Size a
Let the probability sample (X1,. . .,Xn) have the joint probability density
function f(x1,. . .,xn;Q) and associated likelihood function L(Q;x1,. . .,xn).
The generalized likelihood ratio (GLR) is deﬁned as
l x
ð Þ ¼
supQ2H0 L Q; x1; . . . xn
ð
Þ
f
g
supQ2H0[Ha L Q; x1; . . . ; xn
ð
Þ
f
g;
and a generalized likelihood ratio test for testing H0 versus Ha is given by
the following test rule: reject H0 iff l x
ð Þ  c, or equivalently, reject H0 iff
I½0;c l x
ð Þ
ð
) ¼ 1.
For a size a test, the constant c is chosen to satisfy
sup
Q2H0
p Y
ð
Þ
f
g ¼ sup
Q2H0
P l x
ð Þ  c; Q
ð
Þ
f
g ¼ a:
1Recall that the supremum of g(w) for w 2 A is the smallest upper bound for the value of g(w) when w 2 A. If the supremum is
attainable for some value w 2 A, the supremum is the maximum.
10.3
Generalized Likelihood Ratio Tests
615

In order to provide some intuitive rationale for the GLR test, ﬁrst note that
the numerator of the GLR is essentially the largest likelihood value that can be
associated with the sample outcome (x1,. . .,xn) when we are able to choose only
among probability distributions that are contained in the null hypothesis, H0.
The numerator can then be interpreted as the likelihood function evaluated at
the constrained maximum likelihood estimate of Q, the constraint being Q∈H0.
The denominator of the GLR is the largest likelihood value that can be associated
with (x1,. . .,xn) when we can choose any probability distribution contained in H0
and/or Ha. In most applications H0 [ Ha will be the entire parameter space, O, and
the denominator is the likelihood function evaluated at the maximum likelihood
estimate of Q. Since likelihood functions are nonnegative valued, and since the
feasible space for the numerator supremum problem is contained in the feasible
space for the denominator supremum problem, we can infer that l∈[0,1].
To see why the critical region of the test statistic l x
ð Þ is deﬁned in terms of
the lower tail, [0,c], of the range of l, note that the smaller the value of l, the
larger are the maximum likelihood for values of Q∈H0[Ha relative to the
maximum likelihood for Q∈H0. Intuitively, this means that when l is small,
there is a value of Q∈Ha that is notably “more likely” to have characterized the
true density f(x;Q) associated with the sample outcome x than any other value of
the parameter Q∈H0. When f(x;Q) is a discrete density function, we could also
infer that there is a value of Q∈Ha that implies a notably higher probability of
observing x than does any value of Q∈H0. Thus, for small values of l, say c or
less, it appears reasonable to reject H0 as containing the true probability distri-
bution of x, and to conclude instead that a better representation of the true
probability distribution of x resides in the set Ha.
10.3.1
GLR Test Properties: Finite Sample
Intuition aside, whether the GLR test represents a good statistical test for a given
statistical hypothesis ultimately depends on the statistical properties of the test.
There is no guarantee that a GLR test is UMP, or even unbiased, in ﬁnite
samples. Finite sample properties of the GLR test must be established on a
case-by-case basis and depend on both the characteristics of f(x;Q) and the
deﬁnition of the sets underlying H0 and Ha. Nonetheless, before we proceed to
asymptotic properties for which general results do exist, we point out some
parallels with the results presented in Chapter 9.
10.3.1.1
Simple Hypotheses
In the case where both H0 and Ha are simple hypotheses, the size a GLR test
and the most powerful level a test based on the Neyman-Pearson lemma
(Theorem 9.1) will be equivalent.
616
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Theorem 10.1
Equivalence of GLR Test
and Neyman-Pearson
Most Powerful Test
When H0 and Ha are
Simple Hypotheses
Suppose a size a GLR test of H0: Q ¼ Q0 versus Ha: Q ¼ Qa exists with critical
region
CGLR
r
¼ x : l x
ð Þ  c
f
g; where P x 2 CGLR
r
; Q0


¼ a 2 0; 1
ð
Þ:
Furthermore, suppose a Neyman-Pearson most powerful level a test also exists
with critical region
Cr ¼ x : L Q0; x
ð
Þ  kL Qa; x
ð
Þ
f
g; where P x 2 Cr; Q0
ð
Þ ¼ a:
Then the GLR test and the Neymann-Pearson most powerful test are equivalent.
Proof
Since l(x)∈[0,1], and given that a∈(0,1), it follows that P(l(x)  c;Q0) ¼ a only if
c < 1. Now let ^u ¼ ^Q x
ð Þ ¼ arg maxQ2 Q0;Qa
f
g L Q; x
ð
Þ
f
g, so that the GLR can be
represented as l(x) ¼ L(Q0;x)/L( ^u;x). Note the following relationship implied
by the maximization process, and leading to a dichotomous partition of the
range of X:
x 2 A ¼
x: ^Q x
ð Þ ¼ Qa
n
o
) L Q0; x
ð
Þ
L Qa; x
ð
Þ ¼ L Q0; x
ð
Þ
L ^u; x

  1;
x 2 B ¼
x: ^Q x
ð Þ ¼ Q0
n
o
) L Q0; x
ð
Þ
L Qa; x
ð
Þ  L Q0; x
ð
Þ
L ^u; x

 ¼ 1:
It follows that L Q0; x
ð
Þ=L Qa; x
ð
Þ  c < 1 only if l x
ð Þ ¼ L Q0; x
ð
Þ=L ^u; x


 c < 1.
When both preceding inequalities hold, ^u ¼ Qa and L Q0; x
ð
Þ=L Qa; x
ð
Þ ¼ L Q0; x
ð
Þ=
L ^u; x


. Thus, for c < 1 and a∈(0,1) if P(L(Q0;x)/L(^u;x)  c; Q0) ¼ a, and P(L(Q0;x)/
L(Qa;x)  c; Q0) ¼ a, then Cr ¼ CGLR
r
and the GLR test is equivalent to the
Neyman-Pearson most powerful test of size a.
n
Since the GLR test is equivalent to the Neymann-Pearson most powerful test
when H0 and Ha are simple, we also know by Theorem 9.2 that the GLR test is
unbiased. We revisit a previous example, in which a most powerful test was found
via the Neyman-Pearson Lemma to illustrate the GLR approach to the problem.
Example 10.5
GLR Test of Simple
Hypotheses for an
Exponential
Distribution
Recall Example 9.11, in which a decision is to be made regarding the population
mean life of a type of computer screen. The likelihood function for the parameter
y (the mean life of the screens) given the 10 observations on screen lifetimes
represented by (x1,. . .,x10) is given by (suppressing the indicator functions)
L y; x
ð
Þ ¼ y10 exp 
X
10
i¼1
xi=y
 
!
:
10.3
Generalized Likelihood Ratio Tests
617

The null and alternative hypotheses under consideration are H0: y ¼ 1 and Ha:
y ¼ 5, respectively.
The GLR for this problem is given by
lðxÞ ¼
Lð1; xÞ
max
y2 1;5
f
g L Y; x
ð
Þ
f
g ¼ min
1;
exp  P10
i¼1 xi


ð5Þ10 exp  P10
i¼1 xi=5


8
<
:
9
=
;
¼ min
1; :2
ð
Þ10 exp :8
X
10
i¼1
xi
 
!
(
)
:
It follows that, for c < 1, the probability that l(x)  c is given by
PðlðxÞ  cÞ ¼ P
:2
ð
Þ10exp :8
X
10
i¼1
xi
 
!
 c
 
!
¼ P
X
10
i¼1
xi  20:11797  1:25lnðcÞ
 
!
ðto five decimal placesÞ:
A size a GLR test with critical region [0,c] is deﬁned by choosing c so that
P(l(x)  c) ¼ a, where the probability value can be determined by utilizing the
fact that P10
i¼1 Xi  Gamma 10; y
ð
Þ and y ¼ 1 under H0. Comparing this result to
the deﬁnition of the most powerful rejection region given in Example 9.11, it is
evident that the two rejection regions are identical, so that the GLR test is both
unbiased and the most powerful test of H0: y ¼ 1 versus Ha: y ¼ 5. For a ¼ .05,
the critical region of the GLR test would be given by [0, 34.13079], which can be
transformed into a rejection region stated in terms of the test statistic P10
i¼1 Xi as
[15.70522, 1).
n
10.3.1.2
Composite Hypotheses
Similar to the extension of the Neyman-Pearson Lemma to the case of deﬁning
UMP tests for testing simple null versus composite alternative hypotheses (Theo-
rem 9.3), the result of Theorem 10.1 can be extended to the case of testing simple
null versus composite alternative hypotheses as follows:
Theorem 10.2
UMP Level a GLR
Test of H0: Q ¼ Q0
versus Ha: Q 2 Oa
When Cr Is Invariant
Suppose the given rejection region, CGLR
r
¼ fx : lðxÞ  cg, of the GLR test of
H0: Q ¼ Q0 versus Ha: Q∈Oa deﬁnes a size a test and 8Qa∈Ha ∃cQa 0 such
that CGLR
r
¼ x : lYaðxÞ  cQa
f
g where
lQaðxÞ ¼
L Q0; x
ð
Þ
maxQ2 Q0;Qa
f
g L Q; x
ð
Þ
f
g and P lQa x
ð Þ  cQa; Q0
ð
Þ ¼ a:
Furthermore, suppose a Neyman-Pearson UMP test of H0 versus Ha having size
a exists. Then CGLR
r
deﬁnes a UMP level a test of H0 versus Ha.
Proof
Given that a Neyman-Pearson UMP test having size a exists, it is the
Neyman-Pearson most powerful size a test for every pair (Q0, Qa), by Theorem 9.3,
618
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

where Cr is invariant to the choice of Qa ∈Ha. Because 8(Q0,Qa) pair, both the
Neyman-Pearson and GLR size a tests exist, the tests are equivalent 8(Q0, Qa)
by Theorem 10.1. Then CGLR
r
deﬁnes a UMP level a test of H0 versus Ha. n
The theorem implies that if the rejection region of a size a GLR test of H0: Q
¼ Q0 versus Ha: Q ¼ Qa is the same 8Qa∈Oa i.e., CGLR
r
is invariant, then if a
Neyman-Pearson UMP test of H0: Q ¼ Q0 versus Ha: Q∈Oa having size a exists,
it is given by the GLR test. The UMP test would also be unbiased (Theorem 9.4).
Example 10.6
GLR Test of Simple Null
and Composite
Alternative for
Exponential
Distribution
Recall Examples 9.11, 9.13, and 10.1 regarding the operating lives of computer
screens. Examine the problem of deﬁning a size .05 GLR test of H0: y ¼ 1 versus
Ha: y ∈Oa, where Oa ¼ (1,1). The GLR for this problem is given by
l x
ð Þ ¼
L 1; x
ð
Þ
supy2 1;1
½
Þ L y; x
ð
Þ
f
g ¼
exp  P10
i¼1 xi


supy2 1;1
½
Þ y10 exp  P10
i¼1 xi=y


n
o :
The maximum of L(y;x) for y∈[1,1) can be deﬁned by ﬁrst solving the ﬁrst-order
condition
d ln Lðy; xÞ
ð
Þ
dy
¼ 10
y
þ
P10
i¼1 Xi
y2
¼ 0;
which yields
y ¼ P10
i¼1 xi=10
as the choice of y that maximizes ln(L(y;x)),
and hence maximizes L(y;x), when there are no constraints on y. Then, if the
constraint y  1, is recognized, l(x) can be deﬁned ultimately as2
l x
ð Þ ¼
P
10
i¼1
xi=10
	

10
exp 10  P
10
i¼1
xi
	

;
for P
10
i¼1
xi>10;
1
otherwise
8
<
:
It follows that for c < 1, the probability that l(x)  c is given by
PðlðxÞ  cÞ ¼ P
X
10
i¼1
xi=10
 
!10
exp 10 
X
10
i¼1
xi
 
!
2
4
3
5  c
0
@
1
A
¼ P
X
10
i¼1
xi  10 ln
X
10
i¼1
xi
 
!
"
#
 13:0259  lnðcÞ
½
:
 
!
ðÞ
Recall that P10
i¼1 Xi ~ Gamma(10,1) when y ¼ 1. Also, from Example 9.11 it is
known that P P10
i¼1 xi  15:70522


¼ :05. Note that P10
i¼1 xi  10 ln P10
i¼1 xi


is
strictly monotonically increasing in the value of P10
i¼1 xi for values of P10
i¼1 xi>10,
so that there will exist a value of c in (*) that deﬁnes the eventP10
i¼1 xi  15:70522.
2A more elegant solution procedure for this inequality constrained maximization problem could be formulated in terms of Kuhn-
Tucker conditions.
10.3
Generalized Likelihood Ratio Tests
619

In particular, the appropriate value is c ¼ .30387 (to ﬁve decimals places). Thus,
the rejection region for the GLR is [0, .30387], and the associated rejection
region for x isCGLR
r
¼
x : P10
i¼1 xi  15:70522
n
o
, which we know (from Example
9.13) deﬁnes the UMP and unbiased test of H0: y ¼ 1 versus Ha: y > 1.
To see that the GLR test satisﬁes Theorem 10.2, so that we can declare the
test to be UMP and unbiased independently of knowing the result of Example
9.13, the reader can retrace the steps followed in Example 10.1, replacing Ha: y ¼
5 with Ha: y ¼ ya, where ya is an arbitrary choice of y∈(1,1). The rejection
region will invariably have the form CGLR
r
¼
x : P10
i¼1 xi  15:70522
n
o
and will
agree with the Neyman-Pearson most powerful rejection region.
□
The GLR test can also lead to UMP and unbiased tests of the simple or
composite null hypothesis H0: Y∈O0 versus the composite alternative hypothe-
sis Ha: Y∈Oa when Y is a scalar, the alternative hypothesis is one-sided, and the
problem is characterized by a monotone likelihood ratio.
Theorem 10.3
UMP and Unbiased GLR
Level a Test of
H0: Y∈O0 versus
One-Sided Ha: Y∈Oa in
Case of Monotone
Likelihood Ratio
Let the sampling density of X, given by f(x;Y) for scalar Y∈O, be a family of
density functions having a monotone likelihood ratio in the statistic T¼ t(X).
Then the GLR test of H0: Y∈O0 versus one-sided Ha: Y∈Oa is a UMP and
unbiased level a test if P(l(x)  c) ¼ a and either
1. H0: Y ¼ Y0 or Y  Y0, Ha: Y > Y0, and l(x)  c iff t(x)  c*,
or
2. H0: Y ¼ Y0 or Y  Y0, Ha: Y < Y0, and l(x)  c iff t(x)  c*.
Proof
The proof follows immediately from Corollaries 9.1 and 9.2 upon recognition
that l(x)  c is simply an alternative representation of the UMP and unbiased
level a rejection region for the respective hypothesis tests based on the properties
of monotone likelihood ratios.
n
Example 10.7
UMP Test of H0: p  p0
versus Ha: p > p0 in
Bernoulli Population
Using GLR
A personal computer manufacturer claims that  80 percent of its new
computers are shipped to its customers without any defects whatsoever.
A random sample, with replacement, of 20 purchases of the company’s products
resulted in 6 reports of initial defects upon delivery of computers. The responses
of the purchasers are viewed as outcomes of iid Bernoulli random variables
Xi  pxi 1  p
ð
Þ1xiI 0;1
f
g xi
ð
Þ , where xi ¼ 1 ) defect reported and xi ¼ 0 ) no
defect reported. We seek to construct a size .01 GLR test of the hypothesis
that H0: p  .20 versus Ha: p > .20, i.e., the null hypothesis is that the propor-
tion of computers shipped that are defective is less than or equal to .20 versus the
alternative that the proportion is greater than .20.
The GLR for this problem is
620
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

lðxÞ ¼
supp  :20
p
P20
i¼1 xi 1  p
ð
Þ20P20
i¼1 xi


supp2 0;1
½

p
P20
i¼1 xi 1  p
ð
Þ20P20
i¼1 xi

 :
The value of p that solves the denominator supremum problem is simply the
MLE outcome for p; ^p ¼ P20
i¼1 xi=20. The value of p that solves the numerator
supremum problem is deﬁned as
^p0 ¼
P
20
i¼1
xi=20
if P
20
i¼1
xi=20  :20
:20
otherwise
8
<
:
9
=
;:
Therefore, l(x) can be represented as
lðxÞ ¼
:20
ð
Þz :80
ð
Þ20z
z=20
ð
Þz 1  z=20
ð
Þ20z
if P
20
i¼1
xi>4
1
otherwise
8
<
:
where Z ¼ P20
i¼1 Xi  Binomial (20, p).
Note that l(x) is a strictly decreasing function of z for z > 4. Because
P(z  9) ¼ .01 it follows that P(l(x)  .04172) ¼ P(z  9) ¼ .01 so that the
rejection region for the GLR test is
CGLR
r
¼ fx : lðxÞ  :04172g ¼
x :
X
20
i¼1
xi  9
(
)
:
Note the statistic t(X) of the monotone likelihood ratio can be speciﬁed as t(X) ¼
P20
i¼1 Xi. It follows from Theorem 10.3 that the GLR test is UMP and unbiased
with level .01. Given that P20
i¼1 xi ¼ 6, the hypothesis H0: p  .20 cannot be
rejected at the .01 level.
□
At this point we focus the reader’s attention on a common procedure that
was used in the preceding examples when assigning probabilities to events of the
form l(x)  c for the GLR. Namely, in each case we were able to ﬁnd a strictly
monotonically increasing or decreasing function of l(x), say h(l(x)), whose PDF
had a known tractable form. Then probability was assigned using the PDF of
h(l(X)) as P(l(x)  c) ¼ P(h  k) or P(h  k) for h(·) monotonically increasing or
decreasing respectively. It is often the case that the probability density of the
GLR, l(X), is difﬁcult to deﬁne or intractable to work with for deﬁning a size a
critical region. In applications of GLR tests one must often seek a test statistic,
h(l(X)), having a tractable probability density in order to be able to both deﬁne a
size a critical region and to investigate the ﬁnite sample properties of the GLR
test. Unfortunately, in practice, it is not always possible to deﬁne such a test
statistic.
10.3
Generalized Likelihood Ratio Tests
621

We will refrain from attempting to provide additional results concerning
ﬁnite sample properties of GLR tests mainly because there are few additional
generalizations that can be made. Typically, a GLR test is constructed in a given
problem context, and then an attempt is made to assess its properties. It is
sometimes the case that little can be deﬁnitively established regarding the ﬁnite
sample properties of a GLR test. Fortunately, it is typically the case that the large
sample properties of GLR tests are very good, and the results apply quite gener-
ally to simple or composite hypotheses involving scalar or multidimensional
parameters for problems with or without nuisance parameters. We examine this
topic next.
10.3.2
GLR Test Properties: Asymptotics
The GLR test is generally a consistent test, and in cases where H0 is deﬁned by
functional restrictions on the parameter space, the asymptotic distribution of
2 ln l X
ð Þ
ð
Þ is generally a w2 distribution. In many cases, the veriﬁcation of
consistency and the identiﬁcation of the asymptotic distribution of the GLR
test can best be accomplished by analyzing either the properties of the random
variable l(X) directly or else the properties of a test statistic h(l(X)) that is a
function of the GLR statistic. In other cases, there exist regularity conditions
that ensure the consistency and asymptotic distribution of the GLR test. We
present some results for the asymptotic properties of the GLR test below.
Additional results can be found in S. Wilks, (1962), Mathematical Statistics,
New York: John Wiley, p. 419; and R.J. Serﬂing, (1980), Approximation
Theorems of Mathematical Statistics, New York: John Wiley, pp. 151–160.
10.3.2.1
Consistency of GLR Tests
We begin with the property of consistency and introduce a sufﬁcient condition
that applies quite generally and is often not difﬁcult to establish.
Theorem 10.4
Consistency of the GLR
Test
Assume the conditions for consistency of the maximum likelihood estimator
(MLE) given by Theorems 8.16–8.18. Let [0, cn] for n¼1,2,3,. . ., represent level a
rejection regions of the GLR statistic for testing H0: Q∈O0 versus Ha: Q∈Oa
based on increasing sample size n. Then limn!1(P(l(x)  cn;Q)) ¼ 1 8Q∈Ha, so
that the sequence of GLR tests is consistent if either of the following conditions
hold:
(a) The GLR statistic is bounded below 1 with probability ! 1 as n ! 1
8Q∈Ha, i.e., limn!1 (P(l(x)  t; Q)) ¼ 1 8Q∈Ha, where t < 1.
(b) plim(l(X)) ¼ d(Q)  t < 1 8Q∈Ha.
Proof
(a) Assuming that the true Q0∈H0, it follows that plim(l(X)) ¼ 1. To see this, let
^Y0 and ^Y represent the MLEs for Q∈H0 and Q∈H0 [ Ha, respectively, and
expand the logarithm of the likelihood function L( ^Q0;x) in a Taylor series
around the point
^Q
to obtain ln(l(x)) ¼ ln L( ^Q0; x )  ln L( ^Q; x ) ¼
622
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

@lnLðQ; XÞ=@Q
ð
Þ0
^Q0  ^Q


, where Q* ¼  ^Q0 + (1) ^Q and ∈[0,1].3 Since
both ^Q0 !
p Q0 and ^Q !
p Q0 (recall Q0∈H0), then Q* !
p
Q0, and it follows
that both ∂lnL(Q*;x)/∂Q and
^Q0  ^Q


!
p 0. Therefore, ln(l(X)) !
p 0, so that
l(X) !
p 1 and if the GLR test is to be of size  a 8n, so that 8n and 8Q∈H0,
P(l(x)  cn;Q)  a, it follows that cn ! 1 as n ! 1. Finally, if limn!1 (P(l(x) 
t;Q))¼ 1 with t < 1 8Q∈Ha, then limn!1 (P(l(x)  cn;Q)) ¼ 1 8Q∈Ha.
(b) 8Q∈Ha, plim (l(X)) ¼ d(Q)  t < 1 ) limn!1( P(l(x)  t;Q)) ¼ 1.
n
Example 10.8
Consistency of GLR Test
for Exponential
Distribution
Revisit Example 10.6 regarding the operating lives of computer screens. Assume
that the true y0∈Ha ¼ (1,1) and examine the behavior of the GLR l(X).
Letting Zn ¼ Xn exp(n(1 Xn )), the GLR can be represented as l(X) ¼ I[0,1]( XnÞ
+ Zn I(1,1) ðXnÞ , and if y0∈Ha, then I[0,1]( Xn )
!
p
0 and I(1,1)( Xn )
!
p
1.
Then plim(l(X)) ¼ plim(Zn) if the latter probability limit exists. Note that
n1 ln(Zn) ¼ n1 ln(Xn) + (1  Xn) !
p 1  y0 ¼ x < 0 since Xn!
p y0 > 1. It follows
from the deﬁnition of convergence in probability that 8e > 0,
Pðn1 ln zn
ð
Þ<x þ eÞ  Pðx  e<n1 ln zn
ð
Þ<x þ eÞ ! 1
and then choosing e > 0 small enough so that x + e < 0,
P n1 lnðznÞ<x þ e


¼ P lnðznÞ<nðx þ eÞ
ð
Þ ¼ P zn< exp nðx þ eÞ
ð
Þ
ð
Þ ! 1:
Since exp(n(x + e)) ! 0 and zn  0, limn!1 (P(zn∈[0,t))) ¼ 1 8t > 0, so that
plim(Zn) ¼ 0, and thus plim(l(X)) ¼ 0. Then the sequence of GLR tests of
H0: y ¼ 1 versus Ha: y > 1 is consistent by Theorem 10.4.b.
□
Example 10.9
Consistency of GLR Test
for Binomial
Distribution
Recall Example 10.7 regarding the claim that  80 percent of new computers are
shipped defect free. Assume that the true p0∈Ha ¼ (.20, 1] and examine the
behavior of the GLR l(X). Letting wn ¼ :20
ð
Þnx :80
ð
Þn 1x
ð
Þ= xnx 1  x
ð
Þn 1x
ð
Þ
h
i
; the
GLR can be represented as l(X) ¼ I[0,.20](Xn) + Wn I(.20,1)(Xn), and if p0∈Ha, then
I[0,.20]( Xn) !
p
0 and I(.20,1)( Xn) !
p
1. Then plim(l(X)) ¼ plim(Wn) if the latter
probability limit exists. Note that
Wn
ð
Þ1=n ¼
:20
ð
Þ
X :80
ð
Þ 1X
ð
Þ
X
X 1 X

 1X
ð
Þ
h
i !
p
:20
ð
Þp :80
ð
Þ1p
pp 1p
ð
Þ1p ¼
:20
p
	

p
:80
1p
ð
Þ
	

1p
¼ x<1 8p 2 :2;1
ð
:
Following reasoning similar to the previous example one can establish that
P(wn <(x + e)n) ! 1 for e > 0 small enough such that x + e is positive and less
than 1. It follows from (x+e)n ! 0 and wn 0 that lim
n!1 Pðwn 2 ½0; tÞÞ ¼ 1 8t>0,
3R.G. Bartle, Real Analysis, p. 371.
10.3
Generalized Likelihood Ratio Tests
623

so that Wn !
p 0, and thus l(X) !
p 0. Then the sequence of GLR tests of H0: p 
.20 versus Ha: p > .20 is consistent by Theorem 10.4.b.
□
We have shown in the preceding examples that sequences of statistical tests
were consistent, so that for large enough sample sizes, one is essentially certain
to reject the null hypothesis if it is false. It is useful to note that consistent tests
are effectively asymptotically unbiased tests in the sense that for large enough n,
the rejection probability will eventually exceed whatever level a < 1 is
associated with the consistent test sequence since the probability of rejecting a
false H0 ! 1. Thus, the tests in the sequence eventually become unbiased
(if they weren’t unbiased to begin with) as n ! 1.
10.3.2.2
Asymptotic Distribution of GLR Statistics Under the Null Hypothesis
In
the examples of statistical tests that have been presented heretofore, the proba-
bility density of either the GLR statistic or a function of the GLR statistic has
been readily identiﬁable and tractable to work with. In cases where the identiﬁ-
cation and/or tractability of the probability distributions of GLR test statistics is
problematic, there are asymptotic results relating to the GLR test that can be
helpful so long as sample sizes are not too small. We will examine one such
result relating to the asymptotic distribution of the natural logarithm of the GLR
statistic for a special but important and prevalent form of null hypothesis.
A discussion of additional results, and further readings, can be found in D.R.
Cox and D.V. Hinkley, (1979), Theoretical Statistics, Chapman and Hall,
London, pp. 311–342.
Regarding the asymptotic distribution of the GLR, it will prove useful to
focus attention on the function 2ln(l(x)) rather than on l(x) itself. A general
result regarding the asymptotic distribution of 2ln(l(X)) can be obtained when
H0 is deﬁned via functional restrictions on the parameter space. In particular, we
will be examining null hypotheses of the form H0 ¼ {Q: R(Q) ¼ r, Q∈O} where
R(Q) is a (q1) differentiable vector function and R(Q) ¼ r places linear and/or
nonlinear constraints on the elements of the parameter vector Q. It will be
assumed that none of the q coordinate functions in R(Q) are redundant. In this
case, we can show that when the null hypothesis is true, 2ln(l(X)) 
a w2
q so that
an asymptotically valid size a GLR test of H0: R(Q) ¼ r versus Ha: R(Q) 6¼ r can be
conducted as
2lnðlðXÞÞ  w2
q;a ) reject H0;
or in terms of the GLR statistic itself,
lðxÞ  exp  1
2 w2
q;a
	

) reject H0;
where w2
q;a is the value of a w2-random variable with q degrees of freedom such
that the event ðw2
q;a; 1Þ is assigned probability a. Furthermore, 2ln(l(X)) will
have a noncentral w2 asymptotic distribution (see Section 10.9) when H0 is false
(and when we examine so-called local alternative hypotheses, to be discussed
624
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

shortly), allowing asymptotically valid power functions to be constructed. The
formal result on the asymptotic distribution of the GLR test when H0 is true is
given below.
Theorem 10.5
Asymptotic Distribution
of GLR Test of
H0: R(Q) ¼ r versus
Ha: R(Q) 6¼ r When
H0 is True
Assume the conditions for the consistency, asymptotic normality, and asymp-
totic efﬁciency of the MLE of the (k1) vector Q as given in Theorem 8.19.
Let l(x) ¼ supQ2H0 LðQ; xÞ
f
g=supQ2H0[Ha LðQ; xÞ
f
g be the GLR statistic for testing
H0: R(Q) ¼ r versus Ha: R(Q) 6¼ r, where R(Q) is a (q1) continuously differentia-
ble vector function having nonredundant coordinate functions and (qk).
Then 2ln(l(X) )!
d w2
q when H0 is true.
Proof
See Appendix.
n
In Examples 10.5 and 10.6, which were based on random sampling from an
exponential population distribution, we know from our study of the MLE in
Chapter 8 that the MLE adheres to the conditions of Theorem 8.19 and is
consistent, asymptotically normal, and asymptotically efﬁcient. It follows
by Theorem 10.5 that the GLR statistic for testing the simple null hypothesis
H0: y ¼ 1 is such that 2ln(l(X)) 
a w2
1. The asymptotic result would also apply to
any simple hypotheses tested in the context of Example 10.7 where sampling
was from a Bernoulli population distribution. While neither of these previous
cases presented signiﬁcant difﬁculties in determining rejection regions of the
test in terms of the distribution of the GLR test statistic, it is useful to note that
the asymptotic result for the GLR test provides an approximate method for
circumventing the inherently limited choices of test sizes in the discrete case
(recall Example 9.10 and Problem 9.7). In particular, since the w2 distribution is
continuous, any size test can be deﬁned in terms of the asymptotic distribution
of 2ln(l(X)), whether or not f(x;Q) is continuous, albeit the size will be an
approximation.
10.3.2.3
Asymptotic Distribution of GLR Statistics Under Local Alternatives
In order to establish an asymptotically valid method of investigating the power
of GLR tests, we now consider the asymptotic distribution of the GLR when the
alternative hypothesis is true. We will analyze so-called local alternatives to
the null hypothesis H0: R(Q) ¼ r. In particular, we will focus on alternatives of the
form R(Q) ¼ r + n1/2 f. In this context, the vector f speciﬁes in what direction
alternative hypotheses will be examined, and since n1/2 f ! 0 as n ! 1, we are
ultimately analyzing alternatives that are close or local to R(Q) ¼ r for large
enough n. These types of local alternatives are also referred to by the term
Pittman drift.
A primary reason for examining local alternatives as opposed to ﬁxed alter-
native hypotheses is that in the latter case, power will always be 1 for consistent
tests,
and
thus
no
further
information
is
gained
about
the
operating
characteristics of the test from asymptotic considerations other than what is
already known about consistent tests, namely, that one is sure to reject a false
null hypothesis when n ! 1. There is an analog to degenerate limiting
distributions, which are equally uninformative about the characteristics of
10.3
Generalized Likelihood Ratio Tests
625

random variables other than that they converge in probability to a constant. In
the latter case, the random variable sequence was centered and scaled to obtain a
nondegenerate limiting distribution that was more informative about the ran-
dom variable characteristics of interest, e.g., variance. In the current context, the
alternative hypotheses are scaled to establish “non-degenerate” power function
behavior.
Theorem 10.6
Asymptotic Distribution
of GLR Test of
H0: R(Q) ¼ r versus
Ha: R(Q) 6¼ r for Local
Alternatives When
H0 Is False
Consider the GLR test of H0: R(Q) ¼ r versus Ha: R(Q) 6¼ r under the conditions
and notation of Theorem 10.5, and let a sequence of local alternatives be
deﬁned by Han: R(Q) ¼ r + n1/2 f. Assume further that @R Q0
ð
Þ=@Q has full
row rank. Then the limiting distribution of  2 ln l X
ð Þ
ð
Þ under the sequence of
local alternatives is noncentral w2 as
2ln lðXÞ
ð
Þ !
d w2
qðlÞ where l ¼ 1
2 f0 @RðQ0Þ0
@Q
M Q0
ð
Þ1 @RðQ0Þ
@Q

1
f:
Proof
See Appendix.
n
The GLR statistic, and the LM and Wald statistics discussed in subsequent
sections, all share the same limiting distribution under sequences of local
alternatives of the type identiﬁed here and are thus referred to as asymptotically
equivalent tests, which will be established when we examine the alternative
testing procedures. For related readings on the relationships between the triad of
tests, see S.D. Silvey, (1959), “The Lagrangian Multiplier Test,” Annals of Math-
ematical Statistics, (30), and R. Davidson and J.G. MacKinnon, (1993) Estima-
tion and Inference in Econometrics, Oxford Univ. Press, NY, pp. 445–449.
Example 10.10
Asymptotic
Power of GLR Test of
H0: y¼y0 versus
Ha: y > y0 in
Exponential Population
Revisit Example 10.6 and consider the asymptotic power of the GLR test of H0:
y ¼ y0 versus Ha: y > y0 for the sequence of local alternatives yn ¼ y0 þ n1=2d,
where in this application y0 ¼ 1. For this case we know that  2 ln l X
ð Þ
ð
Þ 
a w2
1(l)
with l ¼ d0d/2 ¼ d2/2, and n ¼ 10 according to Example 10.6 (which is a bit
small for the asymptotic calculations to be accurate). The asymptotic power
function can be plotted in terms of the noncentrality parameter, or in this single
parameter case, in terms of d. It is more conventional to graph power in terms of
noncentrality, and we do this for a size .05 test in Figure 10.2, where p(l) ¼
R 1
w2
1;:05 f w; 1; l
ð
Þdw ¼
R 1
3:841 f w; 1; l
ð
Þdw and f(w;1,l) is a noncentral w2 density with
1 degree of freedom and noncentrality parameter l (the GAUSS procedure
CDFCHINC was used to calculate the integrals).
As is typical of other tests that we have examined, the closer d is to zero, and
thus the closer Ha: y ¼ y0 + n1/2 d is to H0: y ¼ y0, the less power there is for
detecting a false H0.
□
626
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

10.4
Lagrangian Multiplier Tests
The Lagrangian multiplier (LM) test of a statistical hypothesis utilizes the size
of Lagrangian multipliers as a measure of the discrepancy between restricted
(by H0) and unrestricted estimates of the parameters of a probability model.
The approach can be applied to various types of estimation objectives, such as
maximum likelihood, least squares, or to the minimization of the quadratic
forms that deﬁne generalized method of moments estimators. The Lagrange
multipliers measure the marginal changes in the optimized estimation objective
that are caused by imposing constraints on the estimation problem deﬁned by
H0. The intuition for this approach is that large values of Lagrangian multipliers
indicate that large increases in likelihood function values, large decreases in
sum of squared errors, or substantial reductions in moment discrepancies (in the
GMM approach) are possible from constraint relaxation. If the LM values are
substantially different from zero, the indication is that the estimation function
can be substantially improved by examining parameter values contained in Ha,
suggesting that H0 is false and that it should be rejected.
Like the GLR test, the LM test is a natural testing procedure to use for
testing hypotheses about Q or functions of Q when restricted maximum likeli-
hood estimation is being used in statistical analysis. It has a computational
advantage relative to the GLR approach in that only the restricted maximum
likelihood estimates are needed to perform the test, whereas the GLR approach
requires the unrestricted ML estimates as well. We will focus on the asymptotic
properties of the test,4 and on its application in maximum likelihood settings,
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
p(l)
l
Figure 10.2
Asymptotic power of GLR
test for H0: y ¼ y0 versus
Ha: y > y0, local
alternatives yn ¼
y0 + n1/2 d, l ¼ d2/2.
4Excellent references for additional details include L.G. Godfrey, (1988), Misspeciﬁcation Tests in Econometrics, Cambridge Univ.
Press, New York, pp. 5–20; and R.F. Engle, (1984), “Wald, Likelihood Ratio and Lagrange Multiplier Tests,” in Handbook of
Econometrics vol. 2, Z. Giliches and M. Intriligata, Amsterdam: North Holland, pp. 775–826.
10.4
Lagrangian Multiplier Tests
627

which is indicative of how it is applied in other estimation settings as well.
However, we will also provide a brief overview of its extensions to other estima-
tion objectives such as least squares and GMM.
10.4.1
LM Tests in the Context of Maximum Likelihood Estimation
In order to establish the form of the test rule, examine the problem of
maximizing ln(L(Q;x)) subject to the constraint H0: R(Q) ¼ r. We express the
problem in Lagrangian multiplier form as
ln LðQ; xÞ
ð
Þ  l0½RðQÞ  r
where l is (q1) vector of LMs. The ﬁrst order conditions for this problem (i.e.,
ﬁrst order derivatives with respect to Q and l) are
@ ln LðQ; xÞ
ð
Þ
@Q
 @RðQÞ
@Q
l ¼ 0 and R Q
ð
Þ  r ¼ 0:
Letting ^Qr represent the restricted MLE that solves the ﬁrst order conditions and
Lr represent the corresponding value of the LM, it follows that
@ ln L ^Qr; X




@Q

@R ^Qr


@Q
Lr ¼ 0 and R ^Qr


 r ¼ 0:
We now establish the LM test and its asymptotic distribution under H0.
Theorem 10.7
The LM Test of
H0: R(Q0)¼r versus
Ha: R(Q0)6¼r
Assume the conditions and notation of Theorem 8.19 ensuring the consistency,
asymptotic normality, and asymptotic efﬁciency of the MLE, ^Q; of Q. Let ^Qr and
Lr be the restricted MLE and the value of the Lagrangian multiplier that satisfy
the ﬁrst order conditions of the restricted ML problem speciﬁed in Lagrange
form, respectively, where R(Q) is a continuously differentiable (q1) vector
function containing no redundant coordinate functions. If G ¼ @R Q0
ð
Þ=@Q0
has full row rank, then under H0 it follows that5:
1. W ¼ L0
r
@R ^Qr

0
@Q

@2 ln L ^Qr; X


@Q@Q0
2
4
3
5
1
@R ^Qr


@Q
Lr !
d w2
q;
2. An asymptotic size a and consistent test of H0: R(Q0) ¼ r versus Ha: R(Q0) 6¼
r is given by
w

<
"
#
w2
q;a )
reject H0
do not reject H0
"
#
; and
3. An alternative and equivalent representation of W is given by
W ¼
@ ln L ^Qr; X



0
@Q

@2 ln L ^Qr; X




@Q@Q
2
4
3
5
1
@ ln L ^Qr; X




@Q
:
5Lr is the random vector whose outcome is lr.
628
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

(The test based on this alternative form of W was called the score test by C.
R. Rao, (1948), “Large Sample Tests of Statistical Hypotheses,” Proceedings
of the Cambridge Philosophical Society, (44), pp. 50–57.)
Proof
See Appendix.
n
In certain applications the LM test can have a computational advantage over
the GLR test since the latter involves both the restricted and unrestricted
estimates of Q whereas the former requires only the restricted estimates of Q.
In the following example, the relative convenience of the LM test is illustrated
in the case of testing a hypothesis relating to the gamma distribution. Note the
GLR approach in this case would be complicated by the fact that the unrestricted
MLE cannot be obtained in closed form. Obtaining the restricted MLE in the
case below is relatively straightforward.
Example 10.11
LM Test of H0: a ¼ 1
(Exponential Family)
versus H0: a6¼1 in
Gamma Population
Distribution
The operating life of a new MP3 player produced by a major manufacturer is
considered to be gamma distributed as
z 
1
ba GðaÞ za1ez=bIð0;1ÞðzÞ:
The marketing department is contemplating a warranty policy on the MP3
player, and wants to test the hypothesis that the player is “as good as new
while operating,” i.e., does the player’s operating life adhere to an exponential
population distribution? To test the null hypothesis H0: a ¼ 1 versus Ha: a 6¼ 1,
consider performing a size .05 LM test using a sample of 50 observations on the
operating lives of the new MP3 player.
The likelihood function is (suppressing the indicator function)
Lða; b; xÞ ¼
1
bnaGnðaÞ
Y
n
i¼1
xa1
i
ePn
i¼1 xi=b;
with n ¼ 50. We know from Example 8.20 that the conditions of Theorem 8.19
apply and so Theorem 10.7 is applicable. Under the constraint a ¼ 1,
the restricted ML estimate is the value of ^b such that
^b ¼ arg max
b
1
b50 eP50
i¼1 xi=b


;
which we know to be the sample mean, x.
To implement the LM test, consider calculating the LM statistic as indicated
in Theorem 10.7.3. The second order derivatives of ln(L(a,b;x)) are given in
Example 8.15. In order to evaluate these derivatives at a ¼ 1 and b ¼ x, note
that (to 5 decimal places)
10.4
Lagrangian Multiplier Tests
629

dGðaÞ
da
ja¼1 ¼ :57722 and d2 GðaÞ
da2
ja¼1 ¼ 1:64493
(see M. Abramowitz and I. Stegun, (1970) Handbook of Mathematical
Functions, Dover Publications, New York, pp. 258–260, or else calculate numer-
ically on a computer). Then letting Q ¼ (a, b)0,
where in this application, n ¼ 50.
The ﬁrst derivatives of ln(L(a,b;x)) are given in Example 8.10, and when
evaluated at a ¼ 1 and b ¼ x yield
@ ln Lða; b; xÞ
ð
Þ
@a
 a¼1
b¼x
¼ n lnðxÞ þ :57722n þ
X
n
i¼1
ln xi
ð
Þ;
@ ln Lða; b; xÞ
ð
Þ
db
 a¼1
b¼x
¼ nx1 þ nx1 ¼ 0:
The LM test statistic can then be written in the score test form (note that ^ur ¼
½1; x0)
w ¼
@ ln L ^ur; x



0
@Q

@2 ln L ^ur; x




@Q@Q0
2
4
3
5
1
@ ln L ^ur; x




@Q
¼ n ln xG
ð
Þ  ln x
ð Þ þ : 57722
ð
Þ
:31175
ð
Þ
;
where xG ¼ Qn
i¼1 xi

1=n is the geometric mean of the xi’s. The test rule is then
w ¼ 160:38492 ln xG
x
	

þ :57722
	


<
"
#
3:84146 )
reject H0
do not reject H0
"
#
where w2
1;:05 ¼ 3.84146.
Suppose the 50 observations on MP3 player lifetimes yielded x ¼ 3.00395
and xG ¼ 2.68992. The value of the LM test statistic is w ¼ 74.86822, and
since w  3.84146, H0 is rejected. We conclude the population distribution is
not exponential on the basis of a level .05 test. (In actuality, x and xG were
simulated from a random sample of 50 observations from a Gamma(2, 10)
population distribution, and so the test made the correct decision in this case).□
Analogous to the limiting distribution of 2ln(l(X)) under local alternatives
in the case of the GLR test, the limiting distribution of the LM statistic is
noncentral w2 under general conditions.
630
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Theorem 10.8
Asymptotic Distribution
of LM Test of
H0: R(Q) ¼ r versus
Ha: R(Q) 6¼ r for Local
Alternatives When
H0 is False
Consider the LM test of H0: R(Q) ¼ r versus Ha: R(Q) 6¼ r under the conditions
and notation of Theorem 10.7, and let a sequence of local alternatives be
deﬁned by Han: R(Q) ¼ r + n1/2 f. Then the limiting distribution of the LM
statistic is noncentral w2 as
W ¼ L0
r
@R ^Qr

0
@Q

@2 ln L ^Qr; X




@Q@Q0
2
4
3
5
1
@R ^Qr


@Q
Lr !
d w2
q ðlÞ;
where the noncentrality parameter equals
l ¼ 1
2 f0 @RðQ0Þ0
@Q
M ^Qr

1 @RðQ0Þ
@Q

1
f:
Proof
See Appendix.
n
Given Theorem 10.8, the asymptotic power of the LM test as a function of
l can be calculated and graphed. The asymptotic power function is monotoni-
cally increasing in the noncentrality parameter l. Thus the LM test is also
asymptotically unbiased for testing R(Q) ¼ r versus Ha: R(Q) 6¼ r. Being that
the LM statistic has the same limiting distribution under local alternatives as
the statistic 2[ln(L(^Qr; X)) ln(L(^Q;x))] used in the GLR test, the two procedures
cannot be distinguished on the basis of these types of asymptotic power
considerations. In applications, a choice of whether to use a GLR or LM test is
often made on the basis of convenience, or when both are tractable to compute,
one can consider both tests in assess H0. Comparisons of the ﬁnite sample
properties of the tests continue to be researched.
Historically, the LM test procedure is the most recent of the GLR, Wald, and
LM triad of tests, listed in order of discovery. Recently, substantial progress has
been made in applying the LM approach to a wide range of testing contexts in the
econometrics literature. Interested readers can consult the article by R.F. Engle,
op. cit., and the book by L.G. Godfrey to begin further reading. We provide a brief
overview of LM extensions beyond ML in the next section.
10.4.2
LM Tests in Other Estimation Contexts
In general, Lagrange multiplier (LM) tests are based on the ﬁnite or asymptotic
probability distribution of the Lagrange multipliers associated with functional
constraints deﬁned by a null hypothesis H0, within the context of a constrained
estimation problem that has been expressed in Lagrange multiplier form. The
form of the estimation problem depends on the objective function that is being
optimized to deﬁne the estimator. The objective function could involve
maximizing a likelihood function to deﬁne a ML estimator as discussed above,
minimizing a sum of squares of model residuals to deﬁne a linear or nonlinear
10.4
Lagrangian Multiplier Tests
631

least squares estimator, minimizing the weighted Euclidean distance of a vector
of moment constraints from the zero vector to deﬁne a generalized method of
moments estimator, or in general optimizing any measure of ﬁt with the data to
deﬁne an estimator.
When functional constraints deﬁned by H0 are added to an optimization
problem that deﬁnes an estimator, a constrained estimator is deﬁned together
with Lagrange multipliers associated with the functional constraints. Under
general regularity conditions, the Lagrange multipliers have asymptotic normal
distributions, and appropriately weighted quadratic forms in these multipliers
have asymptotic chisquare distributions that can be used to deﬁne asymptoti-
cally valid tests of the functional restrictions, and thus a test of H0. In some
fortunate cases, the Lagrange multipliers will be functions of the data that have a
tractable ﬁnite sample probability distribution, in which case exact size, level,
and power considerations can be determined for the LM tests. Establishing ﬁnite
sampling behavior will occur on a case by case basis, and we concentrate here on
the more generally applicable results that are possible using asymptotic
considerations.
Suppose that an estimator under consideration is deﬁned as the function ^Q
¼ arg maxQ hðY; QÞ
f
g of the probability sample Y. For example, in maximum
likelihood estimation h(Y,Q) ¼ ln(L(Q;Y)), or in the case of applying the least
squares criteria in a linear model framework, h(Y,Q) ¼  (YxQ)0(YxQ).
Let R(Q) ¼ r deﬁne q functionally independent restrictions on the parameter
vector Q. Then the constrained estimator is deﬁned by ^Q ¼ arg maxQ hðY; QÞ
f
l0ðRðQÞ  rÞgwhere l is a q  1 vector of Lagrange multipliers.
Assuming appropriate differentiability and second order conditions, the ﬁrst
order conditions to the problem deﬁne the constrained estimator as the solution to
@hðY; QÞ
@Q
 @RðQÞ
@Q
l ¼ 0 and RðQÞ  r ¼ 0:
The estimators forQand l then take the general form ^Qr ¼ ^QrðYÞand ^Lr ¼ ^LrðYÞ,
respectively.
In seeking an asymptotic normal distribution for
^Lr and subsequently
deriving an asymptotic chisquare-distributed test statistic in terms of ^Lr , one
generally appeals to some central limit theorem argument, and attempts to
ﬁnd some transformation of
@hðY; Q0Þ
ð
Þ=@Q that has an asymptotic normal
distribution, where Q0 denotes the true value of the parameter vector. For
example, it is often argued in statistical and econometric applications, that
n1=2 @hðY; Q0Þ
ð
Þ=@Q !
d N 0; S
ð
Þ:
In the case of an MLE with
hðY; Q0Þ ¼
ln L Q0; Y
ð
Þ
ð
Þ, this is the typical result that n1=2 @ ln LðY; Q0Þ
ð
Þ
ð
Þ=@Q !
d N 0; S
ð
Þ.
In the case of the classical linear model, wherehðY; Q0Þ ¼  Y  xQ0
ð
Þ0 Y  xQ0
ð
Þ,
this is the standard result that  2n1=2x0 Y  xQ0
ð
Þ ¼  2n1=2x0« !
d Nð0; SÞ.
632
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Assuming that the limiting distribution results can be argued, it follows
that n1=2S1=2 @hðY; Q0Þ
ð
Þ=@Q !
d Nð0; IÞ . Then following an approach that is
analogous to that used in the proof of Theorem 10.7, one can show that
n1=2Lr ¼
d
@R Q0
ð
Þ0
@Q
S1 @R Q0
ð
Þ
@Q

1 @R Q0
ð
Þ0
@Q
S1=2 n1=2S1=2 @h Y; Q0
ð
Þ
@Q


:
It follows that under H0 that
n1=2L0
r !
d N
0; @R Q0
ð
Þ0
@Q
S1 @R Q0
ð
Þ
@Q

1
 
!
:
Then a test statistic based on the Lagrange multiplier vector can be deﬁned as
W ¼ n1L0
r
@R Q0
ð
Þ0
@Q
S1 @R Q0
ð
Þ
@Q


Lr ¼
d L0
r
@R ^Qr

0
@Q
^S
1 @R ^Qr


@Q
Lr;
!
d w2
q under H0:
where ^S is a consistent estimator of the limiting covariance matrix under the
null hypothesis, and ¼
d denotes equivalence in limiting distribution.
Regarding the rejection region for a test based on the LM statistic above,
recall that large values of Lagrange multipliers indicate that their associated
restriction on parameters are substantially reducing the optimized value of the
objective function, which indicates a situation where the hypothesized
restrictions H0: R(Q) ¼ r are in considerable conﬂict with the data in terms of
a maximum likelihood, least squares, or other estimation criterion for deﬁning
an estimator. Since the LM statistic is a positive deﬁnite quadratic form in the
value of the Lagrange multiplier vector, it then makes sense that H0 should be
rejected for large values of LM, and not rejected for small values. Thus, a size a
test is based on a rejection region of the form CW
r ¼ w2
q;a; 1
h

.
Power considerations for LM tests in other estimation contexts are analo-
gous to the approach discussed in Section 10.4 for the ML context, where the
power was analyzed in terms of local alternatives and a noncentral chisquare
distribution was used to calculate the appropriate rejection probabilities.
Demonstrations of the consistency of such tests also follows along the lines
demonstrated in the previous section.
10.5
Wald Tests
The Wald test for testing statistical hypotheses, named after mathematician and
statistician Abraham Wald, utilizes a third alternative measure of the discrep-
ancy between restricted (by H0) and unrestricted estimates of the parameters of
the joint density of a probability sample in order to deﬁne a test procedure. In
particular, the Wald test assesses the signiﬁcance of the difference between the
10.5
Wald Tests
633

unrestricted estimate of R(Q) and the value of R(Q) that is speciﬁed in H0, r.
Signiﬁcantly large values of |R(^Q)  r| indicate signiﬁcant discrepancies between
the hypothesized value of R(Q) and the unrestricted estimate of R(Q) provided by
the data, suggesting that H0 is false.
Note that it is not necessary that the estimator ^Qn be an MLE for the Wald
test to be applicable. In fact, one need not specify a likelihood function at all to
perform the test, and in this respect, the Wald test is more general than the GLR
test presented heretofore. Moreover, unlike the GLR testing context, but like the
LM context, one does not need both the unrestricted and restricted estimates of
the parameters in order to conduct the test. The Wald test requires only the
outcome of the unrestricted estimator, whereas the LM test requires only the
outcome of the restricted estimator, as we have seen in Section 10.4.
Similar to the GLR and LM contexts, we will concentrate on providing some
asymptotic results on the behavior of the Wald test in this section because the
results are relatively general, and apply to a wide scope of applications. Finite
sample properties can sometimes be established as well, but these proceed case
by case, and require additional speciﬁc assumptions regarding the probability
model in order to be applicable. We examine some of these results in Sec-
tion 10.6, where we discuss statistical tests in the context of the General Linear
Model. The asymptotic results presented ahead can also be made to apply even
more generally if more advanced limiting distribution arguments are invoked,
and an excellent beginning reference for such extensions is provided by H.
White, Asymptotic Theory for Econometricians, Academic Press, Orlando.
Theorem 10.9
Wald Test of
Asymptotic Size a
When ^Q 
a N Q0; n1S
ð
Þ
Let the probability sample
X1; . . . ; Xn
ð
Þ have the joint probability density
function f(x;Q0), let ^Q be a consistent estimator for Q0 such that n1/2( ^Q  Q0)
!
d
N(0, S) and let ^Sn be a consistent estimator of the positive deﬁnite S.
Furthermore, let the null and alternative hypotheses be of the form H0: R(Q) ¼
r and Ha: R(Q) 6¼ r, where R(Q) is a (q1) continuously differentiable vector
function of Q for which (qk) and R(Q) contains no redundant coordinate
functions. Finally, let @R Q0
ð
Þ=@Q0 have full row rank. Then a consistent and
asymptotic size a test of H0 versus Ha is given by
w

<


w2
q;a )
reject H0
do not reject H0


where
W ¼ Rð ^QÞ  r
h
i0 @Rð ^QÞ0
@Q
n1 ^Sn

 @Rð ^QÞ
@Q
"
#1
Rð ^QÞ  r
h
i
Proof
Under the stated assumptions, Theorem 5.40 is applicable so that
n1=2 Rð ^QÞ  r
h
i
!
d N 0; @RðQ0Þ0
@Q
S @RðQ0Þ
@Q
	

634
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

under H0. Letting Sr represent the covariance matrix of this limiting distribution,
it follows that
n1=2 S1=2
r
Rð ^QÞ  r
h
i
!
d Nð0; IqÞ
so that by Theorem 5.3
n Rð ^QÞ  r
h
i0
S1
r
Rð ^QÞ  r
h
i
!
d w2
q :
Now note that
@Rð ^QÞ0
@Y
^Sn
@Rð ^QÞ
@Q
"
#1
!
p S1
r
by the continuity of ∂R(Q)/∂Q and the continuity of the inverse matrix function,
in which case it follows from Theorem 5.9 that W !
d
w2
q, so that the test as
deﬁned above has size a asymptotically.
Regarding consistency, assume a > 0 and thus c ¼ w2
q;a < 1. Since
^Q !
d
Q0 and R(Q) is continuous, R( ^Q)  r !
p
f6¼ 0 assuming Ha: R(Q) 6¼ r is true.
Then n1 W !
p f0 S1
r
f > 0, so that limn!1(P(w  c)) ¼ 1 for any c < 1, and the
test is consistent.
n
Example 10.12
Wald Test of Variance
in Exponential
Population
Revisit Example 10.5, and suppose in addition to a hypothesis regarding the
mean life of the computer screen a hypothesis regarding the variance of the
operating life was to be investigated. In particular, suppose H0: y2 ¼ 1 versus
Ha: y2 6¼ 1 was under consideration. We know in this case that for ^Y ¼ X ,
n1=2 X  m


!
d Nð0; s2Þ; and ^Sn ¼ S2 !
p s2 so that Theorem 10.9 is applicable
with R(y)¼ y 2. Because dR(y)/dy¼2y, and n¼10, the Wald statistic becomes
w ¼
^y
2  1

0 4^y
2s2
10
"
#1
^y
2  1


¼ 10 x2  1

2
4x2s2


:
and a Wald test of asymptotic size .05 can be deﬁned as
w

<
(
)
3:84 )
reject H0
do not reject H0


where P(w  3.84) ¼ .05 when W  w2
1.
Supposing that x ¼ 1.4 and s2 ¼ 2.1, then w ¼ .78 < 3.84, and thus H0 would
not be rejected using the Wald test.
□
The asymptotic distribution of the Wald statistic when H0: R(Q) ¼ r is false
can be established based on local alternatives, and leads to a noncentral w2 dis-
tribution, as in the case of the LM and GLR procedures.
10.5
Wald Tests
635

Theorem 10.10
Asymptotic Distribution
of Wald Statistic Under
Ha: R(Q) 6¼ r
Let the conditions and notation of Theorem 10.9 hold for testing H0: R(Q) ¼ r
versus Ha: R(Q) 6¼ r. Then for a sequence of local alternatives Han: R(Q) ¼
r + n1/2 f, it follows that W !
d w2
q(l) with noncentrality parameter
l ¼ 1
2 f0 @RðQ0Þ0
@Q
S @RðQ0Þ
@Q

1
f:
Proof
It follows from Theorem 5.40 that n1/2(R( ^Q)  R(Q0)) !
d N(0, GSG0) where G¼
@RðQ0Þ=@Q0. Under the sequence of local alternatives, R(Q) ¼ r + n1/2 f ! r and
n1=2 Rð ^QÞ  r þ n1=2f




!
d N 0; GSG0
ð
Þ;
in
which
case
n1=2 Rð ^QÞ  r


!
d Nðf; GSG0Þ and GSG0
½
1=2 n1=2 R ^Q


 r


!
d N
GSG0
½
1=2f; I


. Because
@R ^Q


=@Q0 !
p G and ^Sn !
p S, it follows from Theorem 5.9 and properties of the
noncentral w2 distribution that
W ¼ n1=2 R ^Q


 r

0 @Rð ^QÞ0
@Q
^Sn
@Rð ^QÞ
@Q
"
#1
ðRð ^QÞ  rÞn1=2
¼
d n1=2ðRð ^QÞ  rÞ0 GSG0
½
1ðRð ^QÞ  rÞn1=2 !
d w2
q ðlÞ;
with l ¼ 1
2 f0 GSG0
ð
Þ1f.
n
Example 10.13
Wald Test Based on a
GMM Estimator
Revisit Example 8.28 concerning GMM estimation of b in the demand function
Y¼Xb+V when X is random and E((X0X)1 X0V) 6¼ 0. The GMM estimator ^bG
¼(Z0X)1Z0Yas deﬁned in the example is a consistent and asymptotically normal
estimator of b, where n1/2 (^bGb0) !
d
N(0, s2 A0
ZXA1
ZZAZX

1. A consistent
estimator of the covariance matrix of the limiting distribution is given by
n^s2 X0Z Z0Z
ð
Þ1Z0X

1
where ^s2 ¼
Y  X^bG

0
Y  X^bG


=n. Let H0: Rb ¼ r
be any hypothesis concerning linear combinations of the parameters of the
demand function where R has full row rank. Then a Wald test of H0 having
asymptotic size a can be based on outcomes of
W ¼
R^bG  r

0
^s2R X0Z Z0Z
ð
Þ1Z0X

1
R0

1
R^bG  r


;
where the rejection region is deﬁned by Cw
r ¼ w2
q;a; 1
h

. How would the test
change if the null hypothesis were nonlinear in b, i.e., H0: R(b) ¼ r?
□
The asymptotic power function of the Wald test is monotonically increasing
in the noncentrality parameter l since P(w  c; l) for c < 1 is monotonically
increasing in l if W ~ w2
q(l). It follows that the Wald test is also approximately
unbiased for testing H0: R(Q0) ¼ r versus H0: R(Q0) 6¼ r based on asymptotic
power considerations.
636
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

As we had noted at the end of our LM discussion, each of the tests in the
GLR, LM, Wald triad share the same limiting distribution under sequences of
local alternatives when all of the tests are based on the unrestricted and/or
restricted MLEs of Q. As such, the tests cannot be distinguished on the basis
of these asymptotic power comparisons. If convenience dictates the choice of
test procedure, then the Wald procedure will be most useful when the unre-
stricted estimate (maximum likelihood, least square, generalized method of
moments) is relatively easy to obtain. Consideration of small sample properties
requires a case-by-case analysis. The Wald test has ﬂexibility in being quite
generally applicable to a wide array of estimation contexts other than ML
estimation. We emphasize, however, that this ﬂexibility is not exclusive to the
Wald test. In particular, as we note earlier, the LM approach has been gradually
expanding into contexts more general than maximum likelihood estimation
(see the Engle and Godfrey references).
10.6
Tests in the Context of the Classical GLM
In this section, we examine some statistical tests that are often applied in the
context of the Classical General Linear Model (GLM). The collection of tests
that are presented here will allow testing of statistical hypotheses relating to
values of the b and s2 parameters. We begin in the context of multivariate
normality of the residual term of Y ¼ xb + « and later examine asymptotically
valid procedures that do not depend on the normality of «, and that are appropri-
ate to semiparametric model applications.
10.6.1
Tests When « Is Multivariate Normal
In this subsection we begin by assuming that the classical assumptions of the
GLM hold and in addition that « ~ N(0, s2 I) in Y ¼ xb+ «. We concentrate here on
testing hypotheses relating to the value of linear combinations of b, i.e. Rb,
and the value of s2. We will discuss hypotheses concerning nonlinear functions
of b when we examine the more general non-normal semiparametric cases.
10.6.1.1
Testing H0: Rb ¼ r Versus Ha: Rb 6¼ r When R Is (qk): F-Tests
Under
the prevailing assumptions, Y ~ N(xb, s2I) and ^b ~ N(b, s2(x0x)1). Examine a
Wald-type test of the null hypothesis H0: Rb ¼ r versus Ha: Rb 6¼ r where R is
(qk) with full row rank. We know that n1/2(^b  b ) ~ N(0, s2(n1x0x)1), and thus
n1/2(^b  b) !
d N(0, s2Q1) assuming n1x0x!Q, a positive deﬁnite matrix. Also,
^S
2 n1x0x

1is a consistent estimator of the covariance matrix of the limiting
distribution for n1/2(^b  b). Then, Theorem 10.10 suggests that outcomes of the
Wald statistic
W ¼ ðR^b  rÞ0 ^S
2R x0x
ð
Þ1R0
h
i1
R^b  r


10.6
Tests in the Context of the Classical GLM
637

can be used to test H0, where the rejection region CW
r ¼ w2
q;a1
h

deﬁnes an
asymptotic size a, asymptotically unbiased and consistent test.
In this case, we have sufﬁcient information to establish the small sample
distribution of W, and we will also be able to deﬁne an exact size a test of H0.
In particular, rewrite W in the following form:
W ¼
R^b  r

0
s2R x0x
ð
Þ1R0
h
i1
R^b  r


ðn  kÞ^S
2=ðs2ðn  kÞÞ
h
i
¼
Yq
Znk = n  k
ð
Þ
ð
Þ :
Note that the numerator random variable is the sum of the squares of q inde-
pendent standard normal variates since V ¼ [s2 R(x0x)1 R0]1/2 (R^br) ~ N(0, Iq)
under H0, and the numerator can be represented as V0V. We know from
Section 8.2 that (n-k) ^S
2/s2 ~ w2
nk, and thus the denominator random variable
is a w2
nk random variable divided by its degrees of freedom. Finally, recall that ^b
and ^S
2 are independent random variables (Section 8.2) so that Yq and Zn-k are
independent. It follows from Section 6.7 that W/q ~ F(q, (nk)) under H0,6 so that
W itself is distributed as an F random variable multiplied by q, where the
F-distribution has q numerator and (nk) denominator degrees of freedom.
An exact size a test of H0: Rb ¼ r versus Ha: Rb 6¼ r is deﬁned by
w

<
(
)
qFa q; n  k
ð
Þ )
reject H0
do not reject H0
(
)
where the value Fa(q, (nk)) is such that P(f  Fa(q, (nk))) ¼ a when F ~
F(q, (nk)). This test is unbiased and is also consistent if (x0x)1 ! 0 as n !
1. To motivate unbiasedness, note that if Rb  r ¼f6¼ 0, so that Ha is true, then
[s2 R(x0x)1R0]1/2 (R ^b r  f) ~ N(0, I) so that [s2 R(x0x)1 R0]1/2(R ^b r) ~
N([s2 R(x0x)1 R0]1/2 f, I) and thus the numerator of the previous representation
of W is a random variable having a noncentral w2 distribution, i.e., Yq ~ w2
q(l),
where l ¼ (1/2) f0[s2 R(x0x)1 R0]1 f. It follows that W/q is the ratio of a
noncentral w2
q random variate divided by an independent central w2
nk variate,
each divided by their respective degrees of freedom, and thus W/q has a noncen-
tral F-distribution, F(q, (nk), l) (see Section 10.8). Then because P(w/q  c; l) is
monotonically increasing in l 8 c < 1, it follows that the power function of the
F test is strictly monotonically increasing in the noncentrality parameter, l, so
that the test is an unbiased test.
To motivate consistency, ﬁrst note that if (x0x)1 ! 0, so that R(x0x)1 R0 ! 0,
then all of the characteristic roots, l1,. . .,lq, of R(x0x)1R0 limit to 0 as n ! 1
since Pq
i¼1 li ¼ tr(R(x0x)1 R0) ! 0 and li  0 8i by the positive semi-deﬁniteness
of R(x0x)1 R0. Since the characteristic roots of [R(x0x)1R0]1 are the reciprocals
6Note that under H0 W/q is the ratio of two independent central w2 random variables each divided by their respective degrees of
freedom.
638
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

of the characteristic roots of R(x0x)1R0, all of the characteristic roots of
[R(x0x)1R0]1, say x1,. . .,xq, will then increase without bound as n ! 1. If Rb
r ¼ f 6¼ 0, it follows from results on the extrema of quadratic forms (see C.R.
Rao, Statistical Inference p. 62) that
l ¼ 1
2 f0 s2R x0x
ð
Þ1R0
h
i1
f 
1
2s2 xsf0f;
where xs is the smallest characteristic root of [R(x0x)1R0]1, and thus if xs ! 1
and f0f > 0, then l!1. Finally, note that because^S
2 !
p s2, it follows that W !
d w2
q
under H0, and q Fa(q,(nk)) ! w2
q;a ¼ c < 1. Therefore, since liml!1 Pðw  c;lÞ
ð
Þ
¼ 1 when W ~ w2
qðlÞ, the test sequence is consistent.
Example 10.14
Hypothesis Test of Joint
Explanatory Variable
Signiﬁcance in
Representing E(Y)
In this example we examine a size a test of the null hypothesis that none of the
explanatory variables, other than the intercept, are signiﬁcant in explaining the
expected value of the dependent variable in a GLM, Y ¼ xb + « with « ~ N(0, s2 I).
We suppose that the ﬁrst column of x is a vector of 1’s, so that b1 represents the
intercept. Then from the foregoing discussion, a test of the hypothesis that
none of the nonintercept explanatory variables are signiﬁcant, i.e., H0: bi ¼ 0,
i¼2,. . .,k, versus Ha: not H0 (i.e., at least one explanatory variable is signiﬁcant)
can be performed in terms of the F-statisticW ¼ R^br

0 ^S
2R x0x
ð
Þ1R0

1
R^br


;
where r k1
ð
Þ1 ¼ 0 and R k1
ð
Þk ¼ 0jIk1
½
. In the current context, the test statistic
can be represented alternatively as W ¼ ^b0

d
Cov ^b



1^b; where ^b ¼ ^b2;...;^bk

0
and d
Cov ^b


is the sub-matrix of ^S
2 x0x
ð
Þ1 referring to the estimated covariance
matrix of ^b. The test is deﬁned by
w

<
(
)
ðk  1ÞFaðk  1; n  kÞ )
reject H0
do not reject H0
(
)
:
As a numerical illustration, suppose Y ¼ xb+ « represented a linear relationship
that was hypothesized to explain quarterly average stock price levels for a
certain composite of stocks sensitive to international market conditions. Let
b ¼ ½1:2
2:75
3:130 and ^s2 x0x
ð
Þ1 ¼
:25
:15
:31
:9
:55
symmetric
ð
Þ
1:2
2
4
3
5
represent least squares-based estimates of b and s2(x0x)1, where n ¼ 30, b1¼ 1.2
represents the estimated intercept of the relationship, and b2 ¼ 2.75 and b3
¼ 3.13 represents estimated coefﬁcients on quarterly average composite
exchange rates and western country aggregate real income. The value of the
F-statistic for testing the signiﬁcance of the explanatory variables other than the
intercept is given by
10.6
Tests in the Context of the Classical GLM
639

w ¼ b0
 s2R x0x
ð
Þ1R0

1
b ¼ ½2:75
3:13
:9
:55
:55
1:2

1 2:75
3:13
"
#
¼ 10:8347:
Then H0: b2 ¼ b3 ¼ 0 is rejected using a size .05 unbiased and consistent test,
since w ¼ 10.8347  6.7082 ¼ 2F.05(2, 27). The interpretation of the outcome of
the statistical test is that at least one of the two explanatory variables is signiﬁ-
cant in explaining the expected value of the stock price composite.
□
It can be shown that there does not exist a UMP or UMPU test of H0: Rb¼ r
versus Ha: Rb 6¼ r when R has two or more rows. However, we will see next that
for the case where R is a single row vector, a UMPU test does exist.
10.6.1.2
Testing H0: Rb ¼ r, or H0: Rb  r, or H0: Rb  r versus Ha ¼ H0 When R Is
(1k): T-Tests
In order to test H0: Rb¼ r versus Ha: Rb6¼ r when R is a row vector, the F-statistic
in the preceding subsection can be used without modiﬁcation. However, since
Rb is a scalar, an alternative representation of W is possible as
W ¼
R^b  r

2
= ^S
2R x0x
ð
Þ1R0
h
i
 F 1; n  k
ð
Þ under H0:
An alternative test statistic, whose square is equal to W, can be based on the t-
distribution as
T ¼ ðR^b  rÞ=½^S
2R x0x
ð
Þ1R0
1=2
 TðnkÞunder H0
and is referred to as the T-statistic. Justiﬁcation for the t-distribution of T can be
provided from ﬁrst principles (recall Problem 3(a) of Chapter 8), or else from the
fact that the square of a T(nk) random variable is a random variable having a
F(1, nk) distribution. In terms of the t-statistic, the test is deﬁned by
t
2
=2
(
)
ð1; ta=2ðn  kÞ [ ½ta=2ðn  kÞ; 1Þ )
reject H0
do not reject H0
(
)
;
where ta/2(nk) denotes the value of a t-distribution with nk degrees of free-
dom for which P t  ta=2 n  k
ð
Þ


¼ a=2.
It can be shown using the principles in Section 9.5 that the t-test (and thus
the F-test) is a UMPU level a test of H0: Rb ¼ r versus Ha: Rb 6¼ r. We will defer
the demonstration of the UMPU property, as it is quite involved. Unbiasedness
can be demonstrated from the equivalence of the t-test and F-test or directly
from the power function of the t-test. Regarding the power function, when Rb r
¼ f 6¼ 0, the aforementioned T-statistic has a noncentral t-distribution with
noncentrality parameter l ¼ f /[s2 R(x0x)1R0]1/2. The power function, pCr(l) ¼
1 
Ð ta=2
 ta=2 f(t; nk, l)dt achieves its minimum value when l ¼ 0 (i.e., H0 is true)
and is strictly monotonically increasing as l ! 1 or l !  1 (see Section 9.6).
Thus the test is unbiased.
640
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Consistency of the t-test sequence follows from consistency of the F-test
sequence. As an alternative motivation for consistency, ﬁrst note that because
Tnk !
d N(0,1) under H0, then ta/2 (nk) ! za/2, where
Ð 1
za=2N(z;0,1) dz ¼ a/2. Based
on the t-test, the null hypothesis will be rejected iff
ðRbrÞ
^s2R x0x
ð
Þ1R0
½

1=2

  ta=2 n  k
ð
Þ:
Since the ei’s are iid, ^S
2 !
p
s2, and assuming (x0x)1 ! 0, then ^b !
p b. All of
the preceding results imply collectively that the condition for rejecting H0
becomes | f /s| > 0 in the limit. Thus when f 6¼0, H0 is rejected with probability
! 1 as n!1, and the test sequence is consistent.
Example 10.15
Hypothesis Tests of
Individual Explanatory
Variable Signiﬁcance in
Representing E(Y)
Revisit Example 10.14, and consider testing the individual null hypotheses that
each of the non-intercept explanatory variables are signiﬁcant in explaining the
daily composite stock value. That is, we wish to test H0: bi ¼ 0 versus Ha: bi 6¼ 0,
for i¼2,3.
Using a size .05 test for each test, in order to test the ﬁrst null hypothesis, set
R ¼ [0 1 0] and r ¼ 0, and calculate the t-statistic as
t ¼
Rb  r
^s2R x0x
ð
Þ1R0

1=2 ¼ 2:75
:9
ð
Þ1=2 ¼ 2:8988:
Then since
t ¼ 2:8988 2 CT
r ¼ ð1; t:025 27
ð
Þ [ t:025 27
ð
Þ
½
Þ ¼ ð1; 2:052 [ 2:052; 1
½
Þ
we reject H0: b2 ¼ 0. To test the second null hypothesis, set R ¼ [0 0 1] and r ¼ 0,
and calculate the t-statistic as
t ¼
Rb  r
^s2R x0x
ð
Þ1R0

1=2 ¼
3:13
1:2
ð
Þ1=2 ¼ 2:8573:
The rejection region is the same as above, so that t∈CT
r and H0: b3 ¼ 0 is rejected.
It appears that both explanatory variables are signiﬁcant in explaining the
composite stock value, but see the next subsection for the appropriate interpre-
tation of this joint conclusion in terms of the probability of Type I Error.
□
From the previous example it is clear that a test of the signiﬁcance of any
individual explanatory variable in the GLM will be based on a statistic whose
value is equal to the explanatory variable’s associated parameter estimate, bi,
divided by the estimated standard error of the estimate, ^s ^bi
ð Þ , as t ¼ bi=^s ^bi
ð Þ .
The statistic will be t-distributed with (nk) degrees of freedom, and the rejec-
tion region will be of the form (1, ta/2(nk)][[ta/2(nk),1).
10.6
Tests in the Context of the Classical GLM
641

The t-statistic can also be used to deﬁne level a UMPU tests of H0: Rb  r
versus Ha: Rb > r or of H0: Rb r versus Ha: Rb< r. The rejection regions deﬁned
in terms of the t-statistic are as follows:
Case
H0
Ha
CT
r
1.
Rbr
Rb>r
t ∈[ta(n-k), 1)
2.
Rbr
Rb<r
t ∈(1, ta (n-k)]
The proof of the UMPU property can be based on the results of Section 9.5,
but it is quite involved and is deferred. Unbiasedness can be veriﬁed from an
investigation of the power functions of the tests. Examine Case 1, and note that
in terms of the noncentrality parameter l ¼ (R b -r)/[s2 R(x0x)1 R0]1/2 of the
noncentral t-distribution, an equivalent representation of H0 and Ha is given
by H0: l  0 versus Ha: l > 0. Then since the power function of the test
pCrðlÞ ¼
ð1
ta=2
fðt; n  k; lÞdt
is strictly monotonically increasing in l, the test is an unbiased test. Unbiased-
ness of the test in Case 2 follows from an analogous argument with inequality
reversals and is left to the reader.
Regarding consistency of the respective test sequences, again examine case 1
and recall that ta(nk) ! za where
Ð 1
za N(z; 0,1)dz ¼ a. Based on the t-test,
the null hypothesis will be rejected iff
ðRbrÞ
^s2R x0x
ð
Þ1R0
½

1=2

  ta=2 n  k
ð
Þ: Given that
^S
2 !
p
s2, (x0x)1 ! 0, and thus ^b !
p
b, the condition for rejecting H0 becomes
f/s > 0 in the limit. Then if Ha is true, so that f > 0, the null hypothesis will
be rejected with probability ! 1 as n ! 1 and the test sequence is consistent.
Example 10.16
Testing Various
Hypotheses About
Elasticities in a Constant
Elasticity GLM
A constant elasticity quarterly demand relationship is speciﬁed as 1n(Y) ¼ b11n +
b2 ln(p) + b3 ln(ps) + b4 ln(m) + « where Y is quantity demanded of a commodity,
p is its price, ps is an index of substitute prices, m is disposable income, 1n is a
vector (n1) of 1’s, and « ~ N(0, s2I). Forty quarterly observations were used to
calculate the least squares-based estimates
b ¼ ½10:1  :77 :41 :560
and
^s2 x0x
ð
Þ1 ¼
3:79
:05
:02
:01
:04
:9  103
:1  103
:03
:1  103
symmetric
ð
Þ
:02
2
664
3
775:
We seek answers to the following statistical testing questions:
(a) Test the signiﬁcance of the matrix of explanatory variable values other than
the intercept using a size .05 test.
642
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

(b) Test the individual signiﬁcance of the effect of price, of the substitute price
index, and of disposable income each at the .05 level of signiﬁcance.
(c) Test whether the demand equation is homogenous degree zero in prices and
income.
(d) Test whether the own price elasticity is  1.
(e) Test whether the income elasticity is unitary elastic or larger.
For each of the hypotheses being tested, we list the speciﬁcation of R, the value
of r, the relationship hypothesized between Rb and r, whether the statistic used
is a F-statistic (w) or a t-statistic (t), the value of the calculated statistic, the
rejection region of the test statistic with degrees of freedom and distribution
indicated, and the outcome of the test (see Table 10.1).
Test (a) indicates that at least one of the explanatory variables is signiﬁcant
at the .05 level; test (b) indicates that each explanatory variable is individually
signiﬁcant at the .05 level (but see the next subsection on the interpretation of
the collective outcome of the three tests); test (c) indicates that homogeneity
of degree zero cannot be rejected; test (d) indicates that the price elasticity being
 1 cannot be rejected, and test (e) indicates that the conjecture that the
income elasticity is unitary elastic or greater is rejected suggesting that
the income response is inelastic. Regarding the two hypotheses that were not
rejected, the reader should contemplate the power function of the tests used in
order to temper ones enthusiasm for literal acceptance of the respective
hypotheses.
□
Table 10.1
Statistical tests of the value of Rb relative to r
Test
R

¼

0
@
1
A
r
w or t
Cr
Outcome
a.
0100
0010
0001
2
4
3
5
¼
0
0
0
2
4
3
5
w ¼ 36.6237
w > 8.5988
Reject H0
> 3F.05(3,36)
b.
[0 1 0 0]
¼
0
t ¼ 3.85
t =2 (2.028, 2.028)
Reject H0
=2 (t.025(36),t.025(36))
[0 0 1 0]
¼
0
t ¼ 2.3671
Reject H0
[0 0 0 1]
¼
0
t ¼ 3.9598
Reject H0
c.
[0 1 1 1]
¼
0
t ¼ .6587
t =2 (2.028, 2.028)
Do not reject H0
=2 (t.025(36),t.025(36))
d.
[0 1 0 0]

1
t ¼ 1.15
t ∈[1.688, 1)
Do not reject H0
∈[t.05(36),1)
e.
[0 0 0 1]

1
t ¼  3.1113
t ∈(1, 1.688]
Reject H0
∈(1, t.05(36)]
10.6
Tests in the Context of the Classical GLM
643

10.6.1.3
Bonferroni Joint Tests of Ri; b ¼ ri , Ri b  ri , or Ri b  ri , i¼1,. . .,m
Based on the Bonferroni probability inequality, it is possible to individually test
a number of equality and/or inequality hypotheses concerning various linear
combinations ofband still provide an upper bound to the probability of making a
Type I Error with respect to the null hypotheses taken collectively or simulta-
neously. To see what is involved, suppose a collection of m tests based on the
aforementioned t-statistics are performed that relate to any combination of
hypotheses of the form H0i: Rib ¼ ri or H0i: Rib  ri or H0i: Rib  ri versus
respectively Hai: Rib 6¼ ri or Hai: Rib > ri or Hai: Rib < ri, where Ri’s and ri’s are
(1k) vectors and scalars, respectively. Let Ti and Ci
r, i¼1,. . .,m, represent the
associated t-statistics and rejection regions, and suppose the sizes of the tests
were ai, i ¼ 1,. . .,m. It follows by Bonferroni’s inequality that if all of the null
hypotheses are true,
P ti=2Ci
r; i ¼ 1; . . . ; m


 1 
X
m
i¼1
P ti 2 Ci
r


¼ 1 
X
m
i¼1
ai:
It follows that the overall level of protection against Type I Error provided by the
collection of tests is  Pm
i¼1 ai.
For example, if four different tests are performed, each of size .025, then
treating the four different null hypotheses collectively as a joint hypothesis
regarding the parameter vector b, the size of the joint test is  .10. In performing
the four tests, one knows that the probability one or more individual hypotheses
will be rejected by mistake is  .10. In practice, one generally sets the overall
bound on Type I Error, say a, that is desired for the joint test comprised of the m
individual tests, and then sets the size of each individual test to a/m. Alterna-
tively, any distribution of test sizes across the m individual tests that sum to a
will afford the same overall level of Type I Error protection. The Bonferroni
approach can of course be applied in contexts other than the GLM.
While the individual tests in the collection of tests may have optimal
properties such as being UMPU, the properties do not generally transfer to the
Bonferroni-type joint test. However, asymptotically the situation is favorable in
the sense that if the individual test sequences are consistent, the joint test
sequence will also be consistent. This follows from the fact that the joint test
will reject the collection of H0’s as being simultaneously true with probability
P [m
i¼1fy : tiðyÞ 2 Ci
r


 P fy : tiðyÞ 2 Ci
r


8i;
and if P({y: ti(y)∈Ci
r) ! 1 as n ! 1 when Hai is true, then P [m
i¼1 y : tiðyÞ 2 Ci
r




! 1 as n ! 1. Thus the collection of null hypothesis [m
i¼1 H0i is rejected with
probability ! 1, and the joint test sequence is consistent.
A useful feature of the Bonferroni approach to joint testing of hypotheses is
the ability to “look inside” a joint vector hypothesis Rb ¼ r, or any other
collection of individual equality and/or inequality hypotheses, to analyze
which of the hypotheses in the collection are causing the rejection of the overall
joint hypothesis. For example, if the joint equality test Rb ¼ r is rejected, say by a
joint F-test, one can consider performing a series of tests of the individual null
644
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

hypotheses H0i: Rib¼ ri, i ¼ 1,. . .,m, to attempt to see which of these linear
combinations hypotheses are to be rejected and which are not. By choosing an
overall level of Type I Error of a, and then distributing a/m of this Type I Error
probability to each of the individual tests, the collection of conclusions remains
protected at the level of Type I Error a.
Example 10.17
Bonferroni Approach
for Testing Components
of a Joint Hypothesis
Revisit Examples 10.14–10.16. In each case, an F-test rejected a joint null
hypothesis of the form H0: bi ¼ 0, i ∈I and then individual t-tests were
performed on each of the null hypotheses H0i: bi ¼ 0 separately. In the case of
10.14 and 10.15, since each of the two tests had size .05, the overall conclusion
that both explanatory variables are signiﬁcant is protected against a Type I Error
at level .10 by the Bonferroni approach. Likewise, the conclusion that each of the
three explanatory variables is signiﬁcant in Example 10.16 is protected against of
Type I Error at level .15.
□
10.6.1.4
Tests for s2 Under Normality: w2-Tests
In order to test hypotheses concerning the magnitude of s2, consider the GLR
approach. The likelihood function for (b,s2) is given by
L b; s2jy


¼
1
2ps2
ð
Þn=2 exp  1
2 ðy  xbÞ0ðy  xbÞ=s2
	

:
Case I: H0: s2 ¼ d versus Ha: s2 6¼ d
Let H0: s2 ¼ d and Ha: s2 6¼ d . We have already seen in Section 9.3 that the
MLE for b and s2 is given by ^b ¼ (x0x)1x0Y and ^s2 Y
ð Þ ¼
Y  x^b

0
Y  x^b


=n,
so that
max
b;s2 L b; s2jy


¼ 2p^s2 y
ð Þ

n=2 exp  1
2 ðy  xbÞ0ðy  xbÞ=^s2 y
ð Þ
	

¼ 2p^s2 y
ð Þ

n=2 exp n=2
ð
Þ:
Now consider the MLE subject to the restriction s2 ¼ d. It is evident that L(b,d|y)
is maximized when (yxb)0(yxb) is minimized, so that ^b remains the optimum
choice of b under the restriction. The GLR statistic is then given by
l Y
ð Þ ¼
^s2 Y
ð Þ
d
	

n=2
exp n
2
1  ^s2 Y
ð Þ
d
	

	

:
In order to deﬁne a size a > 0 test, we need to ﬁnd a positive number c < 1 such
that P(l  c) ¼ a when H0 is true. The GLR statistic has a complicated probabil-
ity distribution and is therefore not convenient for solving this problem. We seek
an alternative test statistic for determining the rejection region of the GLR test.
Letting z ¼ ^s2 y
ð Þ=d, note that ln(l) ¼ (n/2) ln(z)+ (n/2) (1z) is such that dln(l)/dz
¼ (n/2)[(1/z)1] and d2 ln(l)/dz2 ¼ n/2z2 < 0 for z > 0, which indicates that ln(l),
and thus l itself, attains its maximum when z ¼ 1, and is strictly monotonically
decreasing as z increases in value above 1 or decreases in value below 1 and
moves toward zero. It follows that
10.6
Tests in the Context of the Classical GLM
645

l  c iff ^s2 y
ð Þ=d  t1 or  t2 for appropriate choices of t1<t2:
The problem can be simpliﬁed further via multiplying ^s2(Y)/d by the con-
stant n, resulting in n^s2 Y
ð Þ=d ¼ ðn  kÞ^S
2=d  w2
nk
ð
Þ under H0. We thus seek
appropriate values of t1 < t2 that satisfy both the size a requirement
1 
R n t2
n t1 fðw; n  k
ð
ÞÞ dz ¼ a, where f(w; (nk)) is a w2
nk density, and the require-
ment that the GLR l have the same value, c, at t1 and t2, implying that
ln t1
ð
Þ þ ð1  t1Þ ¼ ln t2
ð
Þþ ð1  t2Þ, or ln(t1/t2) ¼ t1  t2. The two simultaneous
equations must be solved numerically to obtain the appropriate values of t1 and
t2 that deﬁne the rejection region (1, nt1][[nt2, 1) for the w2 test statistic
outcome n  k
ð
Þ^s2=d. While obtaining the solution is highly feasible on personal
computers, it is probably safe to say that most practitioners nevertheless approx-
imate the generalized likelihood ratio test by using an “equal tails” test that can
be deﬁned by consulting a table of the w2 CDF. The equal tails test would be
ðn  kÞ^s2
d
2
=2
(
)
0; w2
nk;1a=2

i
[ w2
nk;a=21
h

)
reject H0
do not reject H0
(
)
:
Note that the GLR test is a UMPU level a test of H0, but the equal tails level a
test is neither UMPU nor unbiased.
Both the GLR test and “equal tails” test are both consistent. To motivate
consistency in the case of the equal tails test, note that H0 is rejected iff ^S
2/d
=2
n  k
ð
Þ1w2
nk;1a=2; n  k
ð
Þ1w2
nk;a=2


. Because (nk)1 w2
nk !
p 1, in the limit
the test amounts to deciding whether or not plim ^S
2/d ¼ 1, and since ^S
2!
p s2
0 the
test then amounts to whether s2
0=d ¼ 1. Thus if d 6¼ s2
0, so that Ha is true, H0 is
rejected with probability ! 1 as n ! 1, and the test sequence is consistent.
The consistency of the GLR test sequence can be established by showing
that Theorem 10.4 applies. To see this, recall that ln(l)¼
n=2
ð
Þln ^s2 y
ð Þ=d


þ
n=2
ð
Þ 1  ^s2 y
ð Þ=d


achieves its maximum value, equal to 0, when ^s2 y
ð Þ=d ¼ 1.
Now note that plim(n-1(2 ln l(Y))) ¼ ln s2
0=d


þ 1  s2
0=d<0 8s2
0 6¼ d , which
implies that l(Y) !
p
0 (recall Example 10.8). Then Theorem 10.4.b applies, and
the GLR test sequence is consistent.
Case II: H0: s2  d versus Ha: s2 > d or H0: s2  d versus Ha: s2 < d
In order to test the hypothesis H0: s2  d versus Ha: s2 > d the GLR approach
of the previous subsection can be applied except now the constrained MLE is
found by maximizing L(b, s2|y) subject to the inequality constraint s2  d. The
solution for b remains b ¼ (x0x)1x0y, while the solution for s2 is ^s2
 y
ð Þ ¼ ^s2 y
ð Þ ¼
n1 y  xb
ð
Þ0 y  xb
ð
Þ if ^s2 y
ð Þ  d and ^s2
 y
ð Þ ¼ d otherwise. The resulting GLR
statistic is therefore deﬁned to be
l ¼
^s2 Y
ð Þ
^s2
 Y
ð Þ
	

n=2
exp n
2
1  ^s2 Y
ð Þ
^s2
 Y
ð Þ
	



:
646
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Note that the deﬁnition of ^s2
 is such that ^s2 / ^s2
  1. Given our previous
discussion of the behavior of l ¼ (z)n/2 exp ((n/2)(1z)) indicating that l achieves
its maximum value of 1 when z ¼ 1 and l is strictly monotonically decreasing as
z increases in value above 1, it follows that
l  c iff ^s2 y
ð Þ=^s2
 y
ð Þ  t
for some appropriate choice of t > 1 for a size a > 0 test. Furthermore, since
^s2 y
ð Þ=^s2
 y
ð Þ>1 only if ^s2
 y
ð Þ ¼ d, it follows that
l  c iff ^s2 y
ð Þ=d  t:
Finally, by multiplying ^s2 y
ð Þ=d by n as before, we have
l  c iff ðn  kÞ^s2=d  nt:
To deﬁne the rejection region of the size a GLR test, we need to ﬁnd the value of
nt that satisﬁes
ð1
nt
fðz; n  kÞdz ¼ a;
where f(w;nk) is a w2 density function with nk degrees of freedom. The
appropriate value of nt can be found in tables of the w2 CDF for the typical
choices of a ¼ .01, .05, or .10, or else they can be found through use of the
computer. The GLR test is then
n  k
ð
Þ^s2
d
2
=2
(
)
w2
nk;a; 1
h

)
reject H0
do not reject H0
(
)
:
It can be shown that the GLR test is a UMPU level a test of H0: s2  d versus
Ha: s2 > d. Consistency of the GLR test sequence can be demonstrated by
showing that Theorem 10.4 applies following a similar argument to the one
used in the previous subsection. In order to test the hypothesis H0: s2  d
versus Ha: s2 < d, the GLR approach can be followed with appropriate inequal-
ity reversals, and can be shown to be
n  k
ð
Þ^s2
d
2
=2
(
)
0; w2
nk;1a
h

)
reject H0
do not reject H0
(
)
:
The test is also UMPU level a and consistent.
Example 10.18
Testing Hypotheses
About Variances in the
GLM Under Normality
Suppose that in Example 10.15, ^s2 ¼ .13. Consider testing the null hypothesis
that (a) s2 ¼ .10, (b) s2  .10, and (c) s2  .25. Use a size .05 test in each case.
In each case, nk ¼ 40–4 ¼ 36. We use the equal tails test for (a), and the
GLR tests for (b) and (c). Noting that w2
36;:975 ¼ 21.335, w2
36;:025 ¼ 54.437, w2
36;:05 ¼
50.998, and w2
36;:95 ¼ 23.268, then
10.6
Tests in the Context of the Classical GLM
647

(a)
nk
ð
Þ^s2=d ¼ 36 :13
ð
Þ=:10 ¼ 46:8 =2 0;21:335
½
[ 54:437;1
½
Þ ) do not reject H0;
(b)
n  k
ð
Þ^s2=d ¼ 36 :13
ð
Þ=:10 ¼ 46:8 =2 50:998; 1
½
Þ ) do not reject H0;
(c)
n  k
ð
Þ^s2=d ¼ 36 :13
ð
Þ=:25 ¼ 18:72 2 0; 23:268
½
 ) reject H0:
If one were to investigate the power function of the test used in (a) and (b),
one would gain perspective on the danger of accepting the null hypothesis in the
literal sense in these cases.
□
10.6.2
Tests in Semiparametric Models
In order to test hypotheses about the parameters of the GLM in a semiparametric
context where the family of probability distributions underlying « is not known,
we resort to asymptotic distributions of test statistics to establish asymptoti-
cally valid size a consistent tests. We assume that the classical GLM
assumptions apply and that the « is distributed according to some unknown
PDF in such a way that n1=2 ^b  b


!
d N 0; s2Q1


and n1=2 ^S
2  s2


!
d N 0; t
ð
Þ
(e.g., see Table 8.1 for some such conditions). Asymptotic tests can be devised for
more general cases where n1 x0x 6! Q and for cases where the limiting distribu-
tion result for the variance estimator does not hold if one resorts to more
advanced central limit theorems. An excellent source for beginning one’s
reading about these extensions is H. White, (1984), Asymptotic Theory for
Econometricians, Academic Press, Orlando.
10.6.2.1
Testing Rb ¼ r or R(b) ¼ r When R Has q Rows: Asymptotic w2 Tests
We ﬁrst reexamine the use of the Wald statistic for testing the linear restrictions
H0: Rb ¼ r versus Ha: Rb6¼ r where R is (qk) with full row rank. Assume that
n1=2 ^b  b


!
d N 0; s2Q1


, and that a consistent estimator of s2Q1 is given by
^S
2 n1x0x

1. It follows directly from Theorem 10.9 that the Wald test of H0: Rb
¼ r versus Ha: Rb 6¼ r deﬁned by
w ¼ Rb  r
ð
Þ0 ^s2R x0x
ð
Þ1R
h
i1
Rb  r
ð
Þ

<


w2
q;a )
reject H0
do not reject H0
(
)
is an asymptotically valid size a, asymptotically unbiased, and consistent test.
It is thus seen that under assumptions that ensure the asymptotic normality
of the estimator ^b and the consistency of ^S
2, the Wald test of H0: Rb ¼ r versus
Ha: Rb 6¼ r can be applied when the family of distributions for « is unknown.
A minor extension of the preceding argument allows the conclusion that the
preceding test rule is appropriate for testing nonlinear hypotheses H0: R(b) ¼ r
versus Ha: R(b) 6¼ r. In particular, assuming that R(b) is continuously differentia-
ble, contains no redundant constraints, and ∂R(b0 )/∂b0 has full row rank, it
follows directly from Theorem 10.9 that an asymptotic size a, asymptotically
unbiased and consistent test of H0: R(b) ¼ r versus Ha: R(b) 6¼ r is deﬁned by
648
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

w ¼ R b
ð Þ  r
ð
Þ0
^s2 @R b
ð Þ0
@b
x0x
ð
Þ1 @R b
ð Þ
@b
	

1
R b
ð Þ  r
ð
Þ

<


w2
q;a
)
reject H0
do not reject H0
(
)
:
The asymptotically valid power function of the Wald test under sequences of
local alternatives can be deﬁned in terms of the noncentral w2 distribution
based on Theorem 10.10. We provide an example of this Wald test later in
Example 10.19.
10.6.2.2
Testing R(b) ¼ r, R( b )  r, or R( b )  r When R Is a Scalar Function:
Asymptotic Normal Tests
In the case where R( b) is a scalar function of the vector b, the Wald statistic of
the preceding subsection specializes to
W ¼
R ^b
 
 r

2
= ^S
2 @R ^b
 0
@b
x0x
ð
Þ1 @R ^b
 
@b
2
64
3
75 !
d w2
1 under H0:
An alternative test statistic whose square is W is given by
Z ¼
R ^b
 
 r


= ^S
2 @R ^b
 0
@b
x0x
ð
Þ1 @R ^b
 
@b
2
64
3
75
1=2
!
d N 0; 1
ð
Þ under H0:
The latter limiting distribution can be established via an application of Theorem
5.39 applied to the scalar function, R( ^b ), of the asymptotically normally
distributed random vector ^b and through Slutsky’s theorem. An asymptotic
size a, unbiased and consistent test of H0: R(b) ¼ r versus Ha: R(b) 6¼ r in terms
of a rejection region for the Z-statistic is given by
z
2
=2
(
)
1; za=2


[ za=2; 1


)
reject H0
do not reject H0


:
This and other asymptotic size a , unbiased, and consistent tests based on the
Z-statistic are summarized in the table ahead (Table 10.2).
Table 10.2
Asymptotic level a tests on R b
ð Þ using a Z-statistic
Case
H0
Ha
Cr
1.
R(b)  r
R(b) > r
[za, 1)
2.
R(b)  r
R(b) < r
(1, za]
3.
R(b) ¼ r
R(b) 6¼ r
1; za=2


[ za=2; 1


10.6
Tests in the Context of the Classical GLM
649

In order to test the linear hypothesis H0: Rb ¼ r versus Ha: Rb 6¼ r, the test is
performed with R replacing @R ^b
 
=@b0 and R^b replacing R(^b) in the deﬁnition of
the z-statistic, and thus the linear case is subsumed.
Note that in practice, the t-distribution is sometimes used in place of the
standard normal distribution to deﬁne the rejection regions of the aforemen-
tioned tests. In the limit, there is no difference between the two procedures,
since tv ! N(0,1) as v ! 1. However, in small samples, the t-distribution has
fatter tails, so that ta > za and thus the actual size of the aforementioned tests is
smaller when based on ta and ta/2 in place of za and za/2, respectively. The use of
the t-distribution in this context can be viewed as a conservative policy towards
rejection of H0 in the sense that the probability of Type I Error is less, and
stronger data evidence will be required for rejection of a null hypothesis. On
the other hand, the actual power of the test is also reduced when the t-distribu-
tion is utilized.
Asymptotically valid power calculations for local alternatives when one of
the z-tests is used can be based on the standard normal distribution. In particu-
lar, for local alternatives of the form R(b)  r ¼ n1/2f, it follows that
R ^b
 
 r  n1=2f


= ^S
2 @R ^b
 0
@b
x0x
ð
Þ1 @R ^b
 
@b
2
64
3
75
1=2
!
d N 0; 1
ð
Þ:
Then assuming that n1x0x ! Q, Slutsky’s theorems can be used to show that
Z !
d N
f= s2 @R b
ð Þ0
@b
Q1 @R b
ð Þ
@b

1=2
; 1
 
!
¼ N f
x ; 1
	

;
where
x ¼
s2 @R b
ð Þ0
@b
Q1 @R b
ð Þ
@b

1=2
;
so that the asymptotic power function of the z-test is represented as
pCrðfÞ ¼
ð
x2Cr
N z; f
x ; 1
	

dz:
Note that the Bonferroni approach to hypothesis testing can be applied as before
using either Wald statistics or Z-statistics except that now the bound on the
level of Type I Error is only asymptotically valid.
Example 10.19
Tests of b Parameters in
a Semiparametric GLM
A quadratic production function representing output in terms of two variable
inputs is speciﬁed as
yt ¼ b1x1t þ b2x2t þ x1t x2t
½

b3 b5
b5 b4
"
#
x1t
x2t
"
#
þ et
650
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

for t ¼1,2,. . .,n, where yt is output in period t, xit is quantity of input i used in
period t, and the et’s are iid with E(et ) ¼ 0 and E(e2
t ) ¼ s2, 8t. Representing the
production relationship in standard GLM form, we have
yt ¼ x1t x2t x2
1t x2
2t 2x1tx2t


b1
b2
b3
b4
b5
2
6666664
3
7777775
þ et, for t ¼ 1; 2; . . . ; n; or
Y ¼ xb þ « in compact matrix form.
Assuming that observations on xproduces a matrix with full column rank,
the classical GLM assumptions hold. Assume further that n1x*0x* ! Q, so
that n1/2( ^b  b ) !
d
N(0, s2Q1), and note that ^S
2 n1x0x

1 is a consistent
estimator of s2Q1.
Suppose 30 observations are used to generate the following least squares-based
estimates:
b ¼ 7:1 5:2  :11  :07  :02
½
0;
^s2 ¼ :6  103;
x0
x
ð
Þ1 ¼
9:571
6:463
7:187
4:400
8:247
9:686
4:124
7:364
8:800
5:766
2:814
6:087
5:945
6:914
symmetric
ð
Þ
11:254
2
6666664
3
7777775
We will test the following hypotheses relative to expected output:
(a) H0 : the production function is concave. Use a level .10 test.
(b) H0 : the marginal products of inputs x1 and x2 are identical at equal input
levels. Use a size .05 test.
Regarding hypothesis (a), note that a necessary and sufﬁcient condition for
the production function to be concave is that b3  0 and b3 b4  b2
5  0. We will
test each hypothesis using a Z test of size .05, so that based on the Bonferroni
approach, the joint test will have size  .10 and thus level .10. For H0: b3  0, the
Z-statistic outcome is
z ¼
Rb  r
^s2R x0x
ð
Þ1R0
h
i1=2 ¼ :11  0
:0588
¼ 1:8707;
where R ¼ [0 0 1 0 0] and r ¼ 0. The rejection region for the test is Cr ¼ [z.05, 1)
¼ [1.645, 1), and since w =2 Cr, H0 is not rejected. For H0: b3b4  b2
5  0,
ﬁrst note that ∂R(b)/∂b ¼ ∂(b3 b4  b2
5 )/∂b ¼ [0
0
b4
b3
2b5]0. Then
10.6
Tests in the Context of the Classical GLM
651

^s2 @R ^b
 
=@b0


x0x
ð
Þ1 @R ^b
 
=@b


¼ :3996  104, so that the Z-statistic out-
come is
z ¼
:0073  0
:3996  104

1=2 ¼ 1:1548:
Because the rejection region is Cr ¼ [1,  z.05] ¼ (1, 1.645) and w =2 Cr, H0 is
not rejected. Overall, the joint null hypothesis of concavity of the production
function is not rejected at the .10 level.
Regarding hypothesis (b), note that:
@E Yt
ð
Þ
@xt1
¼ b1 þ 2b3xt1 þ 2b5xt2;
@E Yt
ð
Þ
@xt2
¼ b2 þ 2b5xt1 þ 2b4xt2:
The marginal products will be identical at equal input levels iff
@E Yt
ð
Þ
@xt1
 @E Yt
ð
Þ
@xt2
¼ b1  b2 þ 2 b3  b4
ð
Þx ¼ 0 8x ¼ xt1 ¼ xt2:
Thus we must test H0: b1  b2 ¼ 0 and b3  b4 ¼ 0 versus Ha: not H0. The Wald
statistic appropriate for this problem has the form
w ¼ Rb  r
ð
Þ0 ^s2R x0
x
ð
Þ1R0
h
i1
Rb  r
ð
Þ:
where
Rb ¼
b1  b2
b3  b4


;
r ¼
0
0
"
#
;
and R ¼
1
1
0
0
0
0
0
1
1
0


:
The Wald statistic outcome is 17,390.828, and because the rejection region of the
test is Cr ¼ w2
2;:05; 1
h

¼ [ 5.991, 1) and 17,390.828 ∈Cr, H0 is rejected at the .05
level.
□
10.6.2.3
Tests for s2: Asymptotic w2 and Normal Tests
In order to test hypotheses concerning the magnitude of s2, consider using a
Wald statistic. Assuming n1=2 ^S
2  s2


!
d N 0; m0
4  s4




(e.g., see Table 8.1), it
follows directly from Theorem 10.9 that a consistent, asymptotically unbiased,
and asymptotic size a Wald test of H0: s2 ¼ d versus Ha: s2 6¼ d is given by
w ¼
n ^s2  d

2
^x
2
=2
(
)
w2
1;a=2; 1
h

)
reject H0
do not reject H0
(
)
652
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

where^xis a consistent estimate ofvar e2
i


¼ m0
4  s4, such as^x ¼ n1 Pn
i¼1 ^e4
i  ^s4;
with ^e ¼ y  xb.
This and other asymptotic size a, unbiased, and consistent tests for
hypotheses about s2based on the use of the asymptotically valid Z-statistic, z
¼ n1=2 ^s2  d


=^x
1=2 , are summarized in Table 10.3.
The reader will be asked to justify the asymptotic size and consistency of
these tests in the problem section.
Example 10.20
Tests of s2 Parameter in
a Semiparametric GLM
Recall Example 10.19, and test the null hypothesis H0: s2  .25  103 versus
Ha: s2 > .25  103 at the .05 level.
Assuming the appropriate conditions for the asymptotic normality of ^S
2 and
consistency of n1 Pn
i¼1 ^e4
i  ^S
4 for m0
4  s4; suppose n1 Pn
i¼1 ^e4
i ¼ :87  106, so
that the z-statistic equals
z ¼
n1=2 ^s2  d


^x
1=2
¼ 40
ð
Þ1=2 :6  103  :25  103


:51  106

1=2
¼ 3:0997:
The rejection region of the test is Cr ¼ [z.05,1) ¼ [1.645, 1), and because
3.0997∈Cr, H0 is rejected.
□
Note that in the preceding three subsections we have made no explicit
assumptions about the functional forms of the PDFs associated with the residual
terms of the GLM and proceeded in a semiparametric context, relying on asymp-
totic properties of estimators to design statistical tests. If an explicit parametric
functional form for the PDF of the residual term is assumed (other than the
normal distribution previously assumed), it might be possible to use the GLR
and/or LM hypothesis testing procedures, in addition to the Wald methodology,
to provide alternative test statistics and statistical tests for the preceding
hypotheses. Such applications are generally a case by case affair, but the general
approaches presented in this and the previous chapter for deﬁning tests and tests
statistics can serve as a guide for how to pursue such deﬁnitions.
Table 10.3
Asymptotic level a tests on s2 using a Z-statistic
Case
H0
Ha
Cr
1.
s2  d
s2 > d
za; 1
½
Þ
2.
s2  d
s2 < d
1; za
ð

3.
s2 ¼ d
s2 6¼ d
1; za=2


[ za=2; 1


10.6
Tests in the Context of the Classical GLM
653

10.7
Conﬁdence Intervals and Regions
In this section we provide an introduction to the concept of conﬁdence intervals
and conﬁdence regions. We provide motivation both from the perspective of the
duality between hypothesis testing and conﬁdence region or interval estimation,
and from the concept of pivotal quantities. The study of conﬁdence region/
interval estimation encompasses its own theory and practice that would require
considerably more space if treated at a general level. In fact there is an entire
body of theory relating to the properties of conﬁdence regions/intervals and
methods of estimating optimal conﬁdence regions/intervals that parallels the
theories developed for hypothesis testing. Moreover, there is a full duality
between certain optimality properties of hypothesis tests and their counterpart
for conﬁdence region or interval estimation. We will note some of these
parallels in the presentation ahead, but we will not examine them in detail.
Readers interested in more depth can begin their readings by examining the
work of M. Kendall and A. Stuart, (1979), Advanced Statistics, Chapter 20, and
E. Lehmann, (1986), Testing Statistical Hypotheses, Chapter 5.
In the case of a single scalar parameter ,Y, or a scalar function of parameters,
R(Q), a conﬁdence interval (CI) is a random interval whose outcomes have a
known probability of containing the true value of the parameter or function of
parameters. In practice, a conﬁdence interval will be deﬁned by random variables
that represent the upper and lower bounds of the interval, and thus outcomes of the
random variables deﬁning the bounds also deﬁne outcomes of the random interval.
It is possible for one of the random variables deﬁning the bounds of the conﬁdence
interval to be replaced by a constant, in which case the conﬁdence interval is a
one-sided conﬁdence interval or a conﬁdence bound. We provide a formal deﬁni-
tion of what is meant by a conﬁdence interval and conﬁdence bound below.
Deﬁnition 10.2
Conﬁdence Intervals
and Conﬁdence Bounds
with Conﬁdence Level g
and Conﬁdence
Coefﬁcient 1  a
Let f(x;Q) be the joint density of the probability sample X for some Q∈O, and
let R(Q) be some scalar function of the parameter vector Q. The following are
conﬁdence intervals or conﬁdence bounds for R(Q) that have conﬁdence level
g and conﬁdence coefﬁcient 1  a.7
(a) Two-Sided CI: (‘(X), u(X)) such that P(‘ (x) < R(Q) < u(x);Q)  g, 8Q∈O and
inf
Q2O Pð‘ x
ð Þ<R Q
ð
Þ<u x
ð Þ; QÞ
f
g ¼ 1  a.
(b) One-Sided
CI
or
Lower
Conﬁdence
Bound:
(‘(X),
1)
such
that
P(‘ (x) < R(Q);Q )  g, 8Q∈O and inf
Q2O Pð‘ x
ð Þ<R Q
ð
Þ; QÞ
f
g ¼ 1  a.
(c) One-Sided CI or Upper Conﬁdence Bound: (1, u(X)) such that
P(R(Q) < m(x);Q)  g, 8Q∈O and inf
Q2O P R Q
ð
Þ<m x
ð Þ; Q
ð
Þ
f
g ¼ 1  a.
The random variables ‘(X) and u(X) are called the lower and upper conﬁ-
dence limits, respectively.
7Recall that inf denotes inﬁmum, which is the largest lower bound to the set of values under consideration. The inﬁmum is the
minimum if the minimum is contained in the set of values.
654
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Note that the conﬁdence level, g, is simply some stated lower bound on the
probability of the event that the outcome of the CI will contain the true value of
R(Q). The conﬁdence coefﬁcient, 1  a, is effectively the minimum probability
of the event that the CI will contain the true value of R(Q). Note the reason
we express the conﬁdence coefﬁcient in terms of the expression 1  a as
opposed to referring to it as a standalone letter or symbol has to do with the
duality between conﬁdence intervals and hypothesis tests, where we will see
that there is a direct connection between the size of a hypothesis test, a, and
the conﬁdence coefﬁcient, 1  a. Note that because g  1  a, a CI with conﬁ-
dence coefﬁcient 1  a is also a CI with level g, although in using the terminology,
the conﬁdence level and the conﬁdence coefﬁcient do not need to be equal.
From the deﬁnition, it is seen that the random variables representing the
lower and/or upper conﬁdence limits are chosen so that they have outcomes
deﬁning an interval containing the value of R(Q) with probability bounded as  g
(a stated level) whatever the value of Q, i.e. 8Q∈O. Note that for any given
outcome of a conﬁdence interval, either R(Q) is or is not in the interval, and so in
practice one does not know whether R(Q) is really contained in a given conﬁ-
dence interval outcome or not. However, since the probability of the event that
the conﬁdence interval outcome “covers” or contains R(Q) is known to be  g,
we do know that in repeated sampling 100g percent of the intervals generated
will contain the value of R(Q), on average. It is in this sense that we have
“conﬁdence” that a conﬁdence interval contains the value of R(Q).
The concept of a conﬁdence region generalizes the concept of conﬁdence
intervals to the case where (1) the random set designed to contain a scalar R(Q)
with known probability is not necessarily in the form of an interval, or else (2)
R(Q) is a vector function so that the random set used to contain R(Q) with a given
probability is inherently not in interval form. The random set deﬁning a conﬁ-
dence region will be designed so the probability that outcomes of the conﬁdence
region contain the value of R(Q) is known or is at least lower bounded. Note that
conﬁdence regions can be interpreted as subsuming conﬁdence intervals as a
special case. We provide the formal deﬁnition of the concept of a conﬁdence
region below.
Deﬁnition 10.3
Conﬁdence Region
with Conﬁdence
Level g and Conﬁdence
Coefﬁcient 1  a
Let f(x;Q) be the joint density of the probability sample X, and let R(Q) be a
vector (or scalar) function of the parameter vector Q. Then a conﬁdence region
for R(Q) with conﬁdence level g and conﬁdence coefﬁcient 1  a is deﬁned
by a random set A(X) for which P(R(Q) ∈A(X);Q)  g, 8Q∈O, and
inf
Q2O PðR Q
ð
Þ 2 A X
ð Þ; QÞ
f
g ¼ 1  a.
In practice the random set A(X) will be deﬁned using random variables that
appear in the set-deﬁning conditions of the set, and different outcomes of the
random set are deﬁned as these random variables assume different outcomes in
repeated sampling. For example, one possibility is that A(X) is a random
open rectangle deﬁned as A(X) ¼ {R(Q): ‘i(X) < Ri(Q) < ui(X), i¼1,. . .,m} so that
10.7
Conﬁdence Intervals and Regions
655

a given outcome of the conﬁdence region for R(Q) would be A(X) ¼ m
i¼1 (‘i (x),
ui(x)). Other examples of conﬁdence sets will be presented subsequently.
Note that conﬁdence intervals or regions can be thought of as an alternative
or supplement to point estimation, where instead of, or in addition to only
generating a best (in some sense) point estimate of the value of some unknown
function of the parameters of a probability model, a set of values is generated
that, a priori, has a given probability of containing the unknown function of the
parameters. When one is interested in knowing a probable range of values for the
unknown R(Q), the conﬁdence interval or region concept has obvious appeal. As
further motivation for the use of conﬁdence regions, recall that point estimators
generally have a high probability of generating point estimates that are literally
wrong, and in fact the probability that a continuous point estimator will gener-
ate the true value of R(Q) is zero. Alternatively, a random conﬁdence interval or
region has a high probability of generating an outcome that contain the true R(Q)
by construction.
10.7.1
Deﬁning Conﬁdence Intervals and Regions via Duality with Rejection
Regions
How does one go about deﬁning conﬁdence intervals or conﬁdence regions?
There is a duality between conﬁdence intervals and regions for R(Q) and rejec-
tion regions for hypothesis tests about R(Q) in the sense that if a rejection region
has been deﬁned for the hypothesis testing problem, one has in effect already
deﬁned a conﬁdence region for R(Q). The speciﬁc nature of this duality is
presented in the following theorem. Note, when we henceforth refer to conﬁ-
dence regions, the concept of conﬁdence intervals is tacitly assumed to be
subsumed within the conﬁdence region concept.
Theorem 10.11
Duality Between
Conﬁdence and
Rejection Regions
Let the probability sample X have joint density function f(x;Q) for some Q∈O,
and let Cr(t) be a level (or size) a rejection region for testing H0: R(Q) ¼ t versus
one of the alternatives (1) Ha: R(Q) 6¼ t , (2) Ha: R(Q) < t , or (3) Ha: R(Q) > t,
for t∈OR Y
ð
Þ ¼ {t: t ¼ R(Q), Q∈O}. Then the random set represented by
A(X) ¼ {t: X∈Cr(t), t∈OR Q
ð
Þis a conﬁdence region for R(Q) having conﬁdence
level (or coefﬁcient) 1a.
Proof
Given the deﬁnitions of Cr(t) and A(X) , x∈Cr(t) , t∈A(x). To motivate this
relationship, ﬁrst note that if x∈Cr(t), then hypothesis R(Q) ¼ t would not be
rejected by the test deﬁned by Cr(t). It follows that t is in the set of null
hypotheses, A(X), that would not be rejected on the basis of the sample outcome
x, so that x∈Cr (t)
) t∈A(X). Alternatively, if t∈A(X), then x is a sample
outcome that results in H0: R(Q) ¼ t not being rejected on the basis of Cr(t),
so that t∈A(X) ) x∈Cr(t), which completes the motivation for the relationship
x∈Cr(t) , t∈A(x).
From the equivalence of the two sets {x: x∈Cr(t)} and {x: t∈A(x)}, it follows
that
Pðt 2 A x
ð ÞÞ ¼ Pðx 2 Cr t
ð ÞÞ  1  a;
656
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

where the inequality holds because Cr(t) deﬁnes a level a test, so that a true Q
would be rejected with  a probability. Thus, A(X) is a conﬁdence region
for t ¼ R(Q) with conﬁdence level 1a. If the size of the test deﬁned by Cr(t)
is a so that the maximum probability of rejecting a true Q equals a, then
it follows that the smallest value of P(t∈A(X)) is 1a, which equals the conﬁ-
dence coefﬁcient.
n
Theorem 10.11 states that to generate an outcome of a conﬁdence region for
R(Q), one can begin with a statistical test for the null hypothesis H0: R(Q) ¼ t
against a one or two-sided alternative alternative. Then a conﬁdence region
outcome is the collection of all possible values of t (i.e., values in OR(Q)) that
represent null hypotheses that are not rejected on the basis of the testing
procedure applied to the sample outcome, x. Some examples presented ahead
will clarify the mechanics of the procedure. Note, as will be seen, that one-sided
and two-sided tests can be used to deﬁne one-sided and two-sided conﬁdence
intervals, respectively.
Example 10.21
Conﬁdence Interval for
m in a Normal
Distribution via Duality
Let X be a random sample of size 50 from a N(m,s2) population distribution
representing observations on ﬁll levels of 16 oz. bottles of a certain brand
of liquid detergent. A sample outcome resulted in x ¼ 16.02 and s2 ¼ .0001.
We will calculate the outcome of a two-sided conﬁdence interval having conﬁ-
dence coefﬁcient .95 for the mean ﬁll level of the bottles.
From Deﬁnition 9.15, we know that a UMPU level and size .05 test of H0:
m¼m0 versus Ha: m6¼m0 is given by
x  m0
s2=ðn  1Þ
ð
Þ1=2
2
=2
(
)
ð1; t:025 [ ½t:025; 1Þ )
reject H0
do not reject H0
(
)
where t.025 refers to the upper tail of the t-distribution with n-1 degrees of
freedom. Then on the basis of a given outcome x for the random sample, the
collection of null hypotheses, m0, that would not be rejected on the basis of this
test procedure is given by
A X
ð Þ ¼ fm0 : x  t:025ðs=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n  1
p
<m0<x þ t:025ðs=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n  1
p
Þg:
Because t.025 ¼ 2.262 for a t-distribution with 9 degrees of freedom, it follows
that the conﬁdence interval outcome is given by (16.013, 16.027). The actual
mean ﬁll level is then a number in the interval with conﬁdence level (and
coefﬁcient) .95.
□
Example 10.22
Conﬁdence Interval for
Rb in the GLM via
Duality
Let Yt ¼ b0
Q3
i¼1xbi
ti exp et
ð Þ , t ¼ 1; . . . ; 25 , represent a probability sample of
observations on a production function based on three inputs (xt1, xt2, xt3), and
assume that the classical GLM assumptions apply to the natural logarthmic
transformation of the production function,
ln Yt
ð
Þ ¼ b
0 þ
X
3
i¼1
bi ln xti
ð
Þ þ et:
10.7
Conﬁdence Intervals and Regions
657

Furthermore, let et ~ iid N(0,s2). An outcome of the sample resulted in the
following estimator outcomes corresponding to the transformed production
function:
b ¼
2:33
:17
:60
:18
2
6664
3
7775and ^s2 x0
x
ð
Þ1 ¼
:64
:011
:0001
:0008
:0025
:23  106
:3  105
:0003
:47  106
ðsymmetricÞ
:0036
2
664
3
775:
We will calculate a two-sided conﬁdence interval having conﬁdence level .95 for
the degree of homogeneity of the production function, Rb ¼ [0 1 1 1] b ¼ P3
i¼1 bi.
We will also calculate a one-sided upper conﬁdence bound with conﬁdence level
.95 for the output elasticity with respect to input 1, Rb ¼ [0 1 0 0] b ¼ b1.
Examine the problem of the two-sided conﬁdence interval. We know from
Section 10.6 that a UMPU level and size a test of H0: Rb ¼ t versus the two-
sided alternative Ha: Rb 6¼ t is deﬁned by
Rb  t
½^s2Rðx0x Þ1 R0 1=2
2
=2
(
)
ð1;  ta=2 [ ½ta=2; 1Þ )
reject H0
do not reject H0
(
)
:
For a given outcome b and ^s2, the set of null hypothesis values, t, that would not
be rejected on the basis of this test procedure is given by
A y
ð Þ ¼
t : Rb  ta=2 ^s2R x0
x
ð
Þ1R0

1=2
<t<Rb þ ta=2 ^s2R x0
x
ð
Þ1R0

1=2


:
Since t.025 ¼ 2.08 for a t-distribution with 21 degrees of freedom, Rb ¼ .95, and
^s2 R(x*0x*)1R0 ¼ .00641, it follows that the two-sided conﬁdence interval for
the homogeneity of the production function is (.78, 1.12). We have level of
conﬁdence (and conﬁdence coefﬁcient) .95 that the degree of homogeneity is
in the interval.
Regarding the one-sided upper conﬁdence bound for Rb ¼ b1, we know from
Section 10.6 that a UMPU size a test of H0: Rb ¼ t versus the (lower) one-sided
alternative hypothesis Ha: Rb < t is given by
Rb  t
½^s2Rðx0x Þ1 R0 1=2
2
=2
(
)
ð1; ta )
reject H0
do not reject H0
(
)
:
The set of null hypothesis values, t, that would not be rejected on the basis of a
given sample outcome and this test procedure is given by
A y
ð Þ ¼ ft : t<Rb þ tað^s2R x0
x
ð
Þ1R0Þ
1=2g
Since t.05 ¼ 1.721 for a t-distribution with 21 degrees of freedom, Rb ¼ .17, and
^s2R x0x
ð
Þ1R0Þ ¼ :0025, it follows that the one-sided upper conﬁdence bound
for the output elasticity of input 1 is (1, .26). We have level of conﬁdence
(and conﬁdence coefﬁcient) .95 that the elasticity is in the interval.
□
658
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Example 10.23
Conﬁdence Interval for
u in an Exponential
Distribution via Duality
Recall Example 10.6 in which operating lives of computer screens were being
analyzed, and a UMPU level and size .05 test of H0: y ¼ 1 versus Ha: y > 1 on the
basis of a random sample of size 10 was found. Retracing the development of
the statistical test, it is seen that the UMPU level .05 GLR test of H0: y ¼
y0 versus Ha: y > y0, for arbitrary choice of y0 > 0, can be represented in the
form ln(l(x)) ¼ 10 ln(x/y0) + 10(1  x/y0)  ln(c). Under H0: y ¼ y0, and assuming
Xi’s ~ iid y1
0 exp(xi/y0), we have that X ~ Gamma(10,y0/10) and then X/ y0 ~
Gamma(10, .1). Note that the value of ln(l(x)) attains its maximum value of
zero when x/y0 ¼ 1, and ln(l(x)) strictly decreases for movements of x/y0 away
from 1 in either direction. Since the one-sided nature of the GLR test is such that
c < 1 ⟺x/ y0 > 1 (recall Example 10.6), the test can be performed in terms of
the test statistic x /y0, and we seek a value of d such that P( x /y0  d) ¼ .05.
Using the Gamma(10, .1) distribution of
X /y0, the value of d is found to
be 1.57052 (compare to Example 10.6), and the UMPU level .05 test of
H0: y ¼ y0 versus Ha: y > y0 is
x=y0

<
(
)
1:57052 )
reject H0
do not reject H0
(
)
:
In order to calculate a one-sided lower conﬁdence bound for y having conﬁdence
level .95, note that the set of null hypothesis values, y0, not rejected by the
preceding UMPU level .05 testing procedure for a given sample outcome is
A x
ð Þ ¼
y : y>
x
1:57052


:
Suppose that x ¼ 1.37. Then an outcome for the one-sided lower conﬁdence
bound is (.87232, 1). We have level of conﬁdence (and conﬁdence coefﬁcient) .95
that the true mean operating life of the computer screens is in the interval.
□
Conﬁdence intervals or regions can be based on hypothesis testing
procedures that are only asymptotically valid, in which case the conﬁdence
regions inherit asymptotic validity from the duality result of Theorem 10.11.8
The use of asymptotic procedures can be especially convenient for simplifying
cases where the joint density of the random sample is discrete. Of course, the
simpliﬁcation comes at the price of the conﬁdence region’s conﬁdence level
being only an approximation to the true conﬁdence level.
Example 10.24
Asymptotic Conﬁdence
Interval for p of
Bernoulli Distribution
via Duality
A food processor has developed a new fat-free butter substitute and intends to
use a random sample of consumers to determine the proportion of U.S. food
consumers that prefer the taste of the new product to that of butter. Of 250
consumers who sampled the product, 97 preferred the taste of the new butter
substitute. We will calculate a conﬁdence interval outcome with conﬁdence
8We are suppressing a technical condition for this inheritance in that convergence of the test statistic’s probability distribution to a
limiting distribution should be uniform in Q∈O. This will occur for the typical PDFs used in practice. For further details, see C.R. Rao,
op. cit., pp. 350–351.
10.7
Conﬁdence Intervals and Regions
659

level .90 for the proportion of consumers who prefer the taste of the butter
substitute. Given the small sample size relative to the size of the food consum-
ing public, we will treat the sample as having occurred with replacement.
Recall that n1/2( X p)/(p(1p))1/2 !
d
N(0,1) by the LLCLT when random
sampling from the Bernoulli population distribution. Since the sample size
n ¼ 250 is large, we use a Wald test based on a w2
1 limiting distribution, or
equivalently, a Z-statistic based on the N(0,1) distribution to construct an
asymptotic level .10 consistent test of H0: p ¼ p0 versus Ha: p 6¼ p0. Adopting
the latter approach, and realizing that X (1X) !
p p(1p), an asymptotically valid
level .10 consistent test is given by
250
ð
Þ1=2 x  p0
ð
Þ
xð1  xÞ
½
1=2
2
=2
(
)
ð1; 1:645 [ ½1:645; 1Þ )
reject H0
do not reject H0
(
)
where
Ð 1
1:645 N(z;0, 1) dz ¼ .05.
Now consider using duality to deﬁne a conﬁdence interval for p that has an
asymptotic conﬁdence level of .90. Based on a given outcome of the random
sample, the set of null hypothesis values, p0, that would not be rejected on the
basis of the preceding test procedure is given by
A x
ð Þ ¼
p0 : x  1:645
xð1  xÞ
250
	

1=2
< p0 <x þ 1:645
xð1  xÞ
250
	

1=2
(
)
:
Since
x ¼ .388, the conﬁdence interval outcome is (.337, .439). We have
conﬁdence at approximately (asymptotic) level (and approximate conﬁdence
coefﬁcient) .90 that the true proportion of individuals preferring the butter
substitute is in the interval.
□
Although in principle a conﬁdence region for two or more functions of
parameters may have a myriad of shapes, the typical shape of a conﬁdence region
that is derived from duality with a hypothesis test based on the Wald statistic is
an ellipse (2 dimensions) or ellipsoid ( 3 dimensions). A typical application is in
the GLM.
Example 10.25
Ellipsoid Conﬁdence
Region in the GLM via
Duality
Revisit the production function problem in Example 10.22. We construct a
conﬁdence region having conﬁdence level .95 for the three output elasticities
b1, b2, b3.
From Section 10.6, we know that a level .05 and consistent test of
H0 : Rb ¼ t versus Ha: Rb 6¼ t can be deﬁned in terms of the Wald statistic
W ¼ (R^bt)0ð^s2R x0x
ð
Þ1R0Þ1(R^bt) as
w

<
(
)
qFa ðq; n  kÞ )
reject H0
do not reject H0


where in the current application q ¼ 3, nk ¼ 21, a ¼ .05, F.05(3, 21) ¼ 3.07, and
660
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

R ¼
0
1
0
0
0
0
1
0
0
0
0
1
2
4
3
5:
Based on the sample outcome reported in Example 10.22, the set of null
hypotheses, t, not rejected by the preceding test procedure is given by
AðyÞ ¼
t :
:17t1
:60t2
:18t3
2
64
3
75
0
:0025
:23106
:3105
:0003
:47106
ðsymmetricÞ
:0036
2
4
3
5
1
:17t1
:60t2
:18t3
2
64
3
75<9:21
8
>
<
>
:
9
>
=
>
;
:
The conﬁdence region is a three-dimensional ellipsoid with center at (.17, .60, .18),
and its shape resembles a football. We have conﬁdence at level .95 that the true
values of the output elasticities b1, b2, b3 are in the ellipsoid.
n
10.7.2
Properties of Conﬁdence Regions
Given the duality between conﬁdence regions and hypothesis tests, one might
expect that there is also a duality between properties of hypothesis tests and
properties of conﬁdence regions. Such is indeed the case, and we will brieﬂy
discuss properties of conﬁdence regions and their relationship with properties of
hypothesis tests.
Recall that in our discussion of hypothesis tests we examined the properties
of signiﬁcance level, size, unbiasedness, uniformly most powerful, uniformly
most powerful unbiased, and consistency. Each of these has a counterpart with
respect to properties of conﬁdence regions (see Table 10.4)
In deriving conﬁdence regions via duality with hypothesis tests, each of the
properties possessed by the hypothesis testing procedure is transferred to the
corresponding property of the conﬁdence region. We now examine deﬁnitions
for the latter four conﬁdence region properties.
A conﬁdence region is said to be unbiased if the probability that the
conﬁdence region contains the true R(Q) is greater than or equal to the
Table 10.4
Relationships between hypothesis tests and conﬁdence regions
Hypothesis test property
,
Conﬁdence region property
Signiﬁcance level, a
Conﬁdence level, 1a
Size, a
Conﬁdence coefﬁcient, 1a
Unbiased
Unbiased
UMP (uniformly most powerful)
UMA (uniformly most accurate)
UMPU (uniformly most powerful unbiased)
UMAU (uniformly most accurate unbiased)
Consistent
Consistent
10.7
Conﬁdence Intervals and Regions
661

probability that it contains
a false R(Q). The formal deﬁnition is as
follows, where P(R(Q) ∈A(X);Q*) denotes the probability that R(Q) ∈A(X)
when X  f(x; Q*).
Deﬁnition 10.4
Unbiased
Conﬁdence Region
A conﬁdence region A(X) for R(Q) is unbiased iff P(R(Q) ∈A(X);Q)  P(R(Q) ∈
A(X);Q*) 8R(Q) 6¼ R(Q*).
Unbiasedness is a reasonable property for a conﬁdence region to possess,
since one would certainly desire a conﬁdence region to contain the true value of
R(Q) more often, or with higher probability, than false values.
A conﬁdence region for R(Q) is uniformly most accurate (UMA) at conﬁ-
dence level g if it has the lowest probability of containing false values of R(Q)
relative to any other conﬁdence region for R(Q) with conﬁdence level g.
Formally,
Deﬁnition 10.5
Uniformly Most
Accurate (UMA) Level g
Conﬁdence Region
A conﬁdence region A(X) for R(Q) having conﬁdence level g is uniformly most
accurate iff P(R(Q) ∈A(X); Q*)  P(R(Q) ∈A*(X)); Q*) 8 R(Q) 6¼ R(Q*) and
8 A*(X) having conﬁdence level g.
Essentially, a UMA conﬁdence region “ﬁlters out” false values of R(Q) with
higher probability than any other conﬁdence region of like conﬁdence level,
which is clearly a desirable property.
We noted in our discussion of hypothesis testing in Section 9.5 that UMP
tests, and thus now UMA conﬁdence regions, do not exist with any degree of
generality. However, UMPU tests exist much more frequently, and thus so do
UMAU conﬁdence regions. A UMAU conﬁdence region is simply a conﬁdence
region that exhibits the UMA property when compared only to other unbiased
conﬁdence regions.
Deﬁnition 10.6
Uniformly Most
Accurate Unbiased
(UMAU) Conﬁdence
Regions
A conﬁdence region A(X) for R(Q) having conﬁdence level g is uniformly most
accurate unbiased iff A(X) is UMA within the class of unbiased conﬁdence
regions having conﬁdence level g.
In comparing conﬁdence intervals for R(Q), it can be shown that a UMAU
conﬁdence interval with conﬁdence level g has the smallest expected length of
any other conﬁdence interval for R(Q) with conﬁdence level g. In terms of
narrowing ignorance of the value of R(Q), this is clearly a desirable property.
Finally, consistency of a conﬁdence region sequence means that as the
sample size n ! 1, the length or volume of the conﬁdence regions shrinks to
zero at any conﬁdence level, and the true R(Q) becomes the only remaining point
in the conﬁdence region with probability ! 1. Formally,
662
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Deﬁnition 10.7
Consistent Level g
Conﬁdence Region
Sequence
A sequence of level g conﬁdence regions {An(X)} for R(Q) is a consistent
conﬁdence region sequence iff limn!1 (P(R(Q) ∈An(x); Q*)) ¼ 0, 8R(Q) 6¼
R(Q*).
A consistent conﬁdence region sequence is such that as n ! 1, all false
values of R(Q) are ultimately “ﬁltered out” with probability ! 1.
Based on the duality between hypothesis tests and conﬁdence regions, we
can state that the conﬁdence interval for m in Example 10.21, for P3
i¼1 bi and b1
in Example 10.22, and the conﬁdence interval for y in Example 10.23 are all
unbiased, UMAU, and consistent. The conﬁdence interval for p in Example
10.24 is approximately (asymptotically) unbiased and UMAU, and is also con-
sistent. Finally, the ellipsoid conﬁdence region for (b1, b2, b3) in Example 10.25 is
unbiased and consistent.
10.7.3
Conﬁdence Regions from Pivotal Quantities
A method of deﬁning a conﬁdence region for R(Q) without invoking duality with
a hypothesis test involves so-called pivotal quantities, deﬁned as follows.
Deﬁnition 10.8
Pivotal Quantities
Let the probability sample X have PDF f(x;Q). A function of X and R(Q),
Q ¼ q(X,R(Q)), is a pivotal quantity for R(Q) if the probability distribution
of Q does not depend on the value of Q∈O .
The fact that the pivotal quantity has a ﬁxed probability distribution that
does not depend on the parameter vector Q allows a conﬁdence region for R(Q) to
be deﬁned via the following method.
Theorem 10.12
Pivotal Quantity
Method of Conﬁdence
Region Construction
Let Q ¼ q(X,R(Q)) be a pivotal quantity for R(Q). Deﬁne the values ‘ and m so
that P(‘ < q(x,R(Q)) < m; Q) ¼ g. Then A(X) ¼ {R(Q): ‘ < q(X,R(Q)) < m} deﬁnes a
conﬁdence region for R(Q) having conﬁdence level (and coefﬁcient) g.
Proof
Since Q is a pivotal quantity, P(‘ < q(x,R(Q)) < m; Q) ¼ g holds for every Q∈O,
and in particular, for the true Q0∈O. Also note that for a given outcome of x, ‘ <
q(X,R(Q)) < m , R(Q)∈A(X). It follows that P(R(Q) ∈A(X); Q) ¼ g 8Q∈O, and
thus by Deﬁnition 10.3, A(X) deﬁnes a conﬁdence region for R(Q) with conﬁ-
dence level (and coefﬁcient) g.
n
Historically, the reason why the random variable Q ¼ q(X;R(Q)) of Deﬁni-
tion 10.8 and Theorem 10.12 was called a “pivotal” quantity is because when
R(Q) is a scalar many such random variables were such that the x argument in
‘ < q(x,R(Q)) < m could be “pivoted” (or better, inverted) out of the center term to
yield the alternative inequality representation t‘(x) < R(Q) < tm(x). The latter
inequality deﬁnes the conﬁdence interval (t‘(X),tm(X)) for R(Q) which is the
10.7
Conﬁdence Intervals and Regions
663

conﬁdence region A(X) of Theorem 10.9. In practice, while it is often the case
that such “pivoting” can be accomplished so that a conﬁdence interval is
deﬁned, the characteristics of a pivotal quantity in Deﬁnition 10.8 do not
guarantee that q(x,R(Q)) can be pivoted, in which case A(X) may not be an
interval. Of course, if R(Q) is a (j1) vector and j2, then obtaining a conﬁdence
interval will be neither possible nor relevant. The reader may ﬁnd it interesting
to know that every example of conﬁdence regions examined heretofore can be
motivated within the context of the pivotal quantity method, as will be seen in
the next example.
Example 10.26
Conﬁdence Regions
from Pivotal Quantities
Revisit Examples 10.21–10.25. For each of the examples, Table 10.5 identiﬁes
the pivotal quantity that can be used to derive the conﬁdence region for the
respective R(Q) as deﬁned in the example. In addition, the table identiﬁes (1) the
ﬁxed PDF for each pivotal quantity that applies regardless of the value of Q∈O,
(2) the event for each pivotal quantity from which the conﬁdence region is
deﬁned, and (3) the resultant explicit form of the conﬁdence region for R(Q).
Note that in the case of Example 10.24, the table identiﬁes an asymptotic pivotal
quantity, meaning a random variable of the form q(X,R(Q)) whose limiting
distribution does not depend on Q∈O. In practice, such asymptotic pivotal
quantities are motivated via central limit theory.
□
A notable practical difﬁculty in using pivotal quantities to deﬁne conﬁdence
regions is ﬁnding a pivotal quantity for a given R(Q). There is no general method
Table 10.5
Pivotal quantities and some associated conﬁdence regions
Ex
R(Q)
Pivotal quantity, Q
PDF for Q, 8Q∈V
Event for Q
Level 1a conﬁdence region, A
10.21
m
X  m
S=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n  1
p
t-distribution,
n1 df
ta/2 < Q < ta/2
(x  ta/2 S=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n  1
p
, x þ ta=2S=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n  1
p
)
10.22
Rb
R^b  Rb
½^S
2R x0x
ð
ÞR0 1=2
t-distribution, n-k df
 ta=2< Q < ta=2
R^b  ta=2D; R^b þ ta=2D


ta < Q < 1
(1, R^b + ta d)
Where D ¼ ^S
2R x0x
ð
Þ1R0
h
i1=2
10.23
y
X/y
Gamma(n,n1)
Q < ga
(x/ga, 1)a
10.24
p
n1=2ðX  pÞ
X 1  X
ð
Þ
½
1=2
N(0,1) (asymptotically)  za=2< Q < za=2
x  za=2D; x þ za=2D


Where D ¼ x 1  x
ð
Þ
n

1=2
10.25
Rb
q1(R^bRb)0
[^S
2R(x0*x*)1R0]1/2(R^bRbÞ
F-distribution, q
and n-k df
Q < Fa(q, n-k)
Ellipsoid with center R^b
aga is such that
Ð 1
ga Gamma(x; n, n1) dx ¼ a, df: degrees of freedom
664
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

for deﬁning pivotal quantities, and a pivotal quantity need not exist for a given
probability model. However, for certain special but important classes of
problems, pivotal quantities are readily available.
Theorem 10.13
Pivotal Quantities for
Location-Scale
Parameter Families of
PDFs
Let X be a random sample from a population distribution f(z;Q). Let ^Q denote
the MLE of Q. The following relationships exist between the functional form of
f(z;Q) and pivotal quantities for the elements of Q, where f0(y) denotes a PDF
whose values do not depend on unknown parameters:
1. Location Parameter Family of PDFs: f(z;Y) ¼ f0(zY) ) Q ¼ ^Y Y is a
pivotal quantity if Y is a scalar.
2. Scale Parameter Family of PDFs: f(z;Y) ¼ Y1 f0(z/Y) ) Q ¼ ^Y/Y is a pivotal
quantity if Y is a scalar.
3. Location-Scale Family of PDFs: f(z;Q) ¼ Y1
2 f0
z  Y1
ð
Þ=Y2
ð
Þ ) Q1 ¼ ( ^Y1 
Y1)/ ^Y2 and Q2 ¼ ^Y2/Y2 are pivotal quantities for Y1 and Y2 if Q is a (21)
parameter vector.
Proof
Antle, C.E. and L.J. Bain, (1969), “A Property of Maximum Likelihood
Estimators of Location and Scale Parameters,” SIAM Review, 11, p. 251.
n
The theorem indicates that so long as one is random sampling from a
population distribution, and that population distribution is a location, scale, or
location-scale family of PDFs, there are then speciﬁc known functions of the
MLEs that deﬁne pivotal quantities for the parameters of the population
distribution.
A more general result which applies to random sampling from any continu-
ous PDF having a scalar parameter Y is as follows.
Theorem 10.14
Pivotal Quantities for
Continuous Population
PDFs
Let X be a random sample from a population distribution having the continu-
ous PDF f(z;Y), where Y is a scalar. Then
 2
X
n
i¼1
ln F Xi; Y
ð
Þ
ð
Þ  w2
2n
is a pivotal quantity for Y, where F(z;Y) is the common CDF for the Xi
0s in the
random sample.
Proof
If Z ~ f(z;Y), then the probability integral transform of Z, F(Z;Y), is distributed
uniform (0,1) (recall Theorem 6.22). It follows that W ¼ ln(F(Z;Y)) ~ Exponen-
tial(1). Then
Q ¼ q X; Y
ð
Þ ¼ 2 Pn
i¼1 ln F Xi; Y
ð
Þ
ð
Þ
is 2 times the sum of
n independent Exponential(1) random variables, which has a w2 distribution
with 2n degrees of freedom. Because Q ¼ q(X,Y)  w2
2n 8Y∈O, it follows that
Q is a pivotal quantity for Y.
n
10.7
Conﬁdence Intervals and Regions
665

Theorem 10.14 implies that pivotal quantities always exist for a scalar Y
when random sampling is from a continuous PDF f(x;Y). Note that one could
also demonstrate that  2 Pn
i¼1 ln 1  F Xi; Y
ð
Þ
½
  w2
2n is an alternative pivotal
quantity for Y (see Problem 10.21).
Example 10.27
Conﬁdence Interval for
the Mean of a Power
Distribution via Pivotal
Quantities
The proportion of the work day that a particular assembly line is stopped
because of malfunctions on the line is the outcome of a random variable with
power distribution PDF f(z;Y) ¼ YzY1 I(0,1)(z), for Y>0. Based on a random
sample of n daily observations from f(z;Y), we deﬁne a conﬁdence region for Y
having conﬁdence coefﬁcient .95.
To use Theorem 10.14, ﬁrst note that the CDF of Z is F(b;Y) ¼ bYI[0,1](b) +
I(1,1)(b). Then 2ln(F(Xi; Y)) ¼ 2Y ln(Xi), and thus Q ¼ 2Y Pn
i¼1 ln Xi
ð
Þ~ w2
2n is
a pivotal quantity for Y. The probability of the event
q : w2
2n;1a=2<q<w2
2n;a=2
n
o
is
1a, and given a random sample outcome x, the event can be pivoted to deﬁne a
1a level conﬁdence interval for Y as
w2
2n;1a=2
2 Pn
i¼1 ln xi
ð
Þ <Y<
w2
2n;a=2
2 Pn
i¼1 ln xi
ð
Þ :
Note when “pivoting” that 2 Pn
i¼1 ln xi
ð
Þ > 0 in this case, and so the sense of
the inequalities do not reverse in the pivot operation.
□
Regarding the choice of the pivotal quantity event from which the conﬁ-
dence region is deﬁned, if the pivotal quantity is a monotonic function of R(Q)
and a conﬁdence interval or bound is desired, the relationships are useful, where
ti x
ð Þdenotes functions of the sample outcomes ultimately used to deﬁne lower
and/or upper bounds for the conﬁdence intervals (Table 10.6).
Also note that there are often many choices of ‘ and m available that are such
that P(‘ < q < m) ¼ g, e.g., an inﬁnite number in the case when q is a continuous
random variable and g ∈(0,1). If q(x, R(Q)) is a monotonic function of R(Q), then it
is desirable to choose ‘ and m so that the length, or expected length, of the
resultant conﬁdence interval is minimized. However, in practice, a rule of
Table 10.6
Relationships between events for pivotal quantity and conﬁdence
intervals
Pivotal quantity event
Montonicity of q(x,R(Q)) in R(Q)
Conﬁdence interval
‘ < q < m
Increasing
t1(x) < R (Q) < t2(x)
Decreasing
t1(x) < R (Q) < t2(x)
1 < q < m
Increasing
1 < R (Q) < t (x)
Decreasing
t(x) < R (Q) < 1
‘ < q < 1
Increasing
t(x) < R (Q) < 1
Decreasing
1 < R (Q) < t(x)
666
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

thumb for two-sided conﬁdence intervals is often followed whereby ‘ and m are
chosen so as to deﬁne “equal tail probabilities” in the deﬁnition of a g-level
conﬁdence interval. That is, ‘ and m are chosen such that P(q < ‘) ¼ P(q > m) ¼
a/2, leading to a g ¼ 1a level conﬁdence interval. Note that this principle was
followed when deﬁning the conﬁdence interval in Example 10.27. The expected
length of the conﬁdence interval could have been reduced slightly by choosing
the lower and upper w2 values, say w2
a‘ and w2
am , so that w2
am  w2
a‘ is minimized
subject to the conﬁdence level condition P w2
a‘ <q< w2
am


¼ g , which can be
accomplished on a computer. Note that if the distribution for the pivotal quan-
tity is symmetric, then choosing ‘ and m so as to have “equal tail probabilities” is
equivalent to choosing the conﬁdence bounds so that the length, or expected
length, of the conﬁdence interval is minimized.
The pivotal quantity method does not guarantee that any optimal properties
will apply to the conﬁdence regions derived from it. However, the method is
relatively straightforward, it applies in many cases of practical importance, and
the conﬁdence regions deﬁned by the method are often quite adequate for their
intended purpose.
10.7.4
Using Conﬁdence Regions to Conduct Statistical Tests
We brieﬂy note that because there is a duality between conﬁdence regions
and statistical tests of hypotheses, one can utilize the duality in the reverse of
what we have done heretofore and deﬁne statistical testing procedures from
conﬁdence regions. Speciﬁcally, a null hypothesis is either not rejected or
rejected depending on whether the hypothesized value of R(Q) is contained or
is not contained in the conﬁdence region, respectively. It follows that the
preceding pivotal quantity method can be used to deﬁne statistical tests.
A hypothesis test is unbiased, UMP, UMPU, and/or consistent according to
whether the conﬁdence region is unbiased, UMA, UMAU, and/or consistent,
respectively.
We will not pursue this “reverse duality” approach to deﬁning statistical
tests from conﬁdence regions any further because for the approach to be of
substantive practical importance, we would need to establish additional
methods of deriving UMA, UMAU, and/or consistent critical regions indepen-
dent of the statistical tests to which they are dual. This requires further study of
the theory of conﬁdence region estimation, which the reader can begin by
referring to the readings suggested at the beginning of this section.
10.8
Nonparametric Tests of Distributional Assumptions
We provide an introduction to nonparametric testing of the distributional
assumptions underlying a point estimation, hypothesis testing, or conﬁdence
region estimation problem in this section. By nonparametric we mean here that
the hypotheses under consideration are not deﬁned in terms of the values of
10.8
Nonparametric Tests of Distributional Assumptions
667

parameters, per se, as has been the case heretofore. Rather, the hypotheses under
consideration will be more general and refer to functional forms of probability
density functions, and whether random variables contained in a sample are iid.
The tests we will introduce address questions such as “could the probability
sample have come from a normal or exponential, or beta, or. . .distribution?” and
“can it be assumed that the outcomes observed in the sample are outcomes of an
iid random sample from some population distribution?”
Note that to a large degree the methods of statistical inference that we have
examined heretofore required certain basic assumptions to hold, collectively
representing the maintained hypothesis, before any analysis could proceed.
It is important to be able to rigorously assess the validity of assumptions that
are held only tentatively and/or that one does not have substantial conﬁdence
are true. Two of the more frequent assumptions made to this point have been the
assumption of a speciﬁc functional form for the probability density of a random
sample and the assumption that random variables in the sample are iid. In this
section we focus on some nonparametric tests of these assumptions that have
been useed in practice, and concentrate on scalar random variables. The ﬁeld of
nonparametric analysis is vast and growing. A useful place to begin additional
reading is J.D. Gibbons, (1985), Nonparametric Methods for Quantitative
Analysis (2nd Ed), American Science Press, Columbus, Ohio, and A. Pagan
and A. Ullah, (1999), Nonparametric Econometrics, Cambridge University
Press, Cambridge.
10.8.1
Nonparametric Tests of Functional Forms of Distributions
There are a number of testing procedures available for testing hypotheses regard-
ing the functional form of the joint density of the probability sample (see
M. Kendall and A. Stuart, Vol. 2, op. cit., Chapter 30; and C. Huang and
B. Bolch, (1974), “On Testing of Regression Disturbances for Normality,”
JASA, 1974, pp. 330–335 for alternatives and references). We will examine the
w2 goodness of ﬁt test, because of its versatility and wide applicability, the
Kolmogonov-Smirnov test and its reﬁnement to the Lilliefors test, because of
their reﬁnement in analyzing continuous distributions, and the Shapiro-Wilks
and Jarque-Bera tests because of their speciﬁc use in testing the ubiquitous
normality assumption.
10.8.1.1
w2 Goodness of Fit Test The w2 goodness of ﬁt test is used to test the
null hypothesis that a random sample is from a population distribution of the
form f(z;Q) where Q is a (k1) vector. In particular, the hypotheses under
consideration are H0: X ~ Qn
i¼1 f (xi;Q) versus Ha: X  Qn
i¼1 f (xi;Q). The test
procedure differs depending on whether the null hypothesis is simple, meaning
that the values of any parameters in f(z;Q) are fully speciﬁed, or the null hypoth-
esis is composite in the sense that Q in f(z;Q) is left unspeciﬁed so that an entire
family of density functions is implied by the null hypothesis.
We ﬁrst examine the case where the null hypothesis is simple. Given the
null hypothesis H0: Z ~ f(z;Q0) for the population distribution, where Q0 is
668
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

its ﬁxed and known value, and assuming the range of Z has been parti-
tioned into m subintervals Di, i ¼ 1,. . .,m, it follows that under H0, P(Di; Q0) ¼
R
z2Di f (z;Q0) dz ¼ pi(Q0) for i ¼ 1,. . .,m. For a random sample of size n, let ni
represent the number of times an outcome occurs in interval Di, i ¼ 1,. . .,m.
Then
the
probability
distribution
of
(N1,. . .,Nm)
is
multinomial
with
parameters (p1,. . .,pm), Pm
i¼1 pi ¼ 1 (recall Section 4.1.4).
Using
the
multivariate
version
of
the
LLCLT,
it
follows
that
n1=2 X  pÞ


!
d Nð0; SÞ; where
X ¼ n1(N1,. . .,Nm1)0, p ¼ (p1,. . .,pm1)0,
and Sis a (m1)(m1) covariance matrix with pi(1  pi), i¼1,. . .,(m1) along
the diagonal and pipj in the off-diagonal entries. (Note that nm is determined
by nm ¼ n  Pm1
i¼1 ni and thus (nm/n) ¼ 1  Pm1
i¼1 xi). Then from Theorem 5.9,
W ¼ n X  p

0S1

X  p


¼ N  np
ð
Þ0 nS
ð
Þ1 N  np
ð
Þ !
d w2
m1;
where N* ¼ (N1,. . .,Nm1)0. It follows that a size a test of H0: Z ~ f(z;Q0) versus
H0: Z f(z;Q0) is given by
w

<
(
)
w2
m1;a )
reject H0
do not reject H0
(
)
where (p1,. . .,pm1) in p and in S are deﬁned by their values under H0, i.e., pi ¼
pi(Q0), i ¼ 1,. . .,m1. The test is seen to reject H0 when the probabilities of the
subinterval events Di, i ¼ 1,. . .,m that are hypothesized under H0 are in conﬂict
with their estimated values, given by the observed relative frequencies, ni/n.
In practice, a statistic that is algebraically and numerically equivalent to W
is generally used to calculate the test outcome. In particular, some tedious but
conceptually straightforward matrix algebra leads to the simpliﬁed expression
(see Kendall and Stuart, vol. 2, op. cit., p. 381):
w ¼
X
m
i¼1
ni  npi
ð
Þ2
npi
:
Now suppose that the null hypothesis is composite as H0: Z ~ f(z;Q), Q∈O
versus Ha: Z f(z;Q), Q∈O. How do we proceed in this case when the speciﬁc
numerical value of Q is left unspeciﬁed? The unknown parameters are
estimated from the data, but how this is done has a substantial impact on the
form of the test procedure. Consider two different forms of MLEs, the difference
being the choice of likelihood function.
For one approach, let the likelihood function be the multinomial distribu-
tion parameterized via Q under H0, i.e.,
LðQ; xÞ ¼
n!
n1!n2! . . . nm!

 Y
m
i¼1
pi Q
ð
Þni for ni 2 f0; 1; . . . ; ng; 8i;
X
m
i¼1
ni ¼ n
where pi(Q) ¼ R
z2Di f (z;Q) dx 8i. After one obtains the maximum likelihood
estimate of Q, ^u , one proceeds by replacing the pi’s by ^pi ¼ pi ^u
 
in the
calculation of the w-statistic, obtaining
10.8
Nonparametric Tests of Distributional Assumptions
669

w ¼
X
m
i¼1
ni  n^pi
ð
Þ2
n^pi
Finally, the test rule is deﬁned as (note the reduced degrees of freedom compared
to the previous test)
w

<
(
)
w2
m1k;a )
reject H0
do not reject H0
(
)
:
The reason that one degree of freedom is lost for each parameter estimated
is somewhat involved. A detailed proof can be found in H. Cramer, (1946),
Mathematical Methods of Statistics, Princeton Univ. Press.
An alternative ML procedure is to estimate Q using the ML method applied
to the likelihood function LðQ; xÞ ¼ Qk
i¼1 f xi; Q
ð
Þ , i.e., use the hypothesized
population distribution directly to specify the likelihood function, and estimate
Q accordingly. One then computes ^pi ¼
R
z2Di f z; ^u


dz to calculate the w-statis-
tic above. In this case, it turns out that W does not have a limiting w2 distribution
at all. However, it can be shown that the distribution of W in this case is
bounded between a w2
m1 and a w2
m1k distribution (H. Chernoff and E.L.
Lehmann, (1954), “The Use of Maximum Likelihood Estimates in w2 Tests for
Goodness of Fit,” Ann. Math. Statist., p. 579). Unfortunately, the limiting
distribution is difﬁcult to use, and in practice one can use w2
m1;a to deﬁne a
size  a test, or else use w2
m1k;a to deﬁne a test that is more likely to reject a true
H0 than the size a would indicate.
It can be shown that all of the preceding tests are consistent for any
alternatives that imply multinomial probabilities different from those implied
by H0. On the other hand, the tests are generally biased to some degree.
One operational problem remains. How does one choose the sets Di,
i ¼ 1,. . .,m? Rules of thumb developed from both empirical and theoretical
considerations suggest that intervals be deﬁned that have equal probability,
i.e., 1/m, based on f(x;Q0) for simple H0’s or on f(x; ^u ) for composite H0’s.
Furthermore, in order that the asymptotics be a reasonable approximation, the
intervals chosen should be such that npi(Q0) or npi(^u)  5 8i.
Example 10.28
Testing Whether a
Population Distribution
Is an Exponential PDF
Forty observations on the waiting times, in minutes, between customer arrivals
at a service station/convenience store in a mid-size city were as follows:
1.37
1.96
0.74
0.42
0.12
0.61
1.98
1.76
1.73
3.32
1.44
2.46
0.34
2.31
2.14
2.11
2.84
2.47
1.25
0.66
(continued)
2.23
1.11
1.73
0.26
1.77
1.35
2.91
0.93
1.50
2.72
1.73
0.59
0.36
0.24
2.68
0.30
0.10
2.75
1.68
0.88
670
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

We will deﬁne a (approximate) size .05 w2 goodness of ﬁt test of the null
hypothesis that the waiting times are exponentially distributed versus some
other family of distributions, i.e., H0: z ~ y1 exp(z/y)I(0,1)(x), y > 0 versus Ha:
not H0.
The ML estimate of y based on the exponential population distribution
assumption is ^y ¼ x ¼ 1.494. We let m ¼ 8, and choose Di, i ¼ 1,. . .,8 so that
P(z∈Di; ^y) ¼ 1/8 8i. The boundaries of the intervals are found as follows:
:125
¼
R c1
0
1:494
ð
Þ1 exp z=1:494
ð
Þdz )
c1 ¼ :1995;
:125
¼
R c2
:1995 1:494
ð
Þ1 exp z=1:494
ð
Þdz )
c2 ¼ :4298;
:
c3 ¼ :7022;
:
c4 ¼ 1:0356;
:
c5 ¼ 1:4654;
:
c6 ¼ 2:0711;
:125
¼
R c7
2:0711 1:494
ð
Þ1 exp z=1:494
ð
Þdz )
c7 ¼ 3:1067:
The intervals and the number of observations that actually occurred in each are
given by:
i
Di
ni
1
(0, .1995]
2
2
(.1995, .4298]
6
3
(.4298, .7022]
3
4
(.7022, 1.0356]
3
5
(1.0356, 1.4654]
5
6
(1.4654, 2.0711]
9
7
(2.0711, 3.1067]
11
8
(3.1067, 1]
1
Since n^pi ¼ 40(.125) ¼ 58i, the value of the w2 statistic is given by
w ¼
X
8
i¼1
ni  5
ð
Þ2
5
¼ 17:2:
Adopting a conservative (toward H0) stance and using the critical value
w2
m1k;:05 ¼ w2
6;:05 ¼ 12.6, w > 12.6 and the exponential family of distributions
is rejected. Note if the more liberal critical value of w2
m1;:05 ¼ w2
7;:05 ¼ 14.1 is
used, w > 14.1 and H0 is still rejected, and we know that the signiﬁcance level of
this test is  .05.
□
10.8.1.2
Kolmogorov-Smirnov and Lilliefors Tests
The Kolmogorov-Smirnov (K-S) test is an alternative procedure for testing
whether a random sample outcome was drawn from a speciﬁed population
distribution. The test procedure is based on the empirical distribution function
(EDF) and the Glivenko-Cantelli theorem (recall Chapter 6) which implies that
the EDF will converge functionally across all points of comparison to the true
population CDF associated with a random sample. The basic idea of the K-S test
10.8
Nonparametric Tests of Distributional Assumptions
671

is to reject H0 when there is signiﬁcant discrepancies between an EDF and a
hypothesized CDF as revealed through the probability integral transform of the
data (recall Section 6.8).
Before examining the test procedure in more detail, we note some
advantages and limitations of the K-S test relative to the aforementioned
w2 test. On the positive side, the K-S test is fully applicable in the case of small
samples whereas the w2-test is only an asymptotically valid test. The K-S test
deals with sample observations directly whereas the data must be summarized
into categories for analysis via the w2 approach, so that information is potentially
lost in the categorization process. The K-S test assumes the population distribu-
tion is continuous and thus provides a more reﬁned analysis speciﬁc to this case.
Limitations include the fact that the K-S approach cannot be easily adjusted to
allow for estimation of unknown parameters as in the w2 case, so that in practice
the K-S test is generally restricted to testing simple null hypotheses. Also, the
applicability of K-S is limited to cases involving continuous distributions. We
note that the K-S test has been modiﬁed by H. Lilliefors, (1967), (“On the
Kolmogorov-Smirnov Test for Normality with Mean and Variance Unknown,”
JASA, pp. 399–402) to accommodate the estimation of m and s2 when the null
hypothesis is that of normality. We will examine this case below. Work has also
been done on extending the K-S approach to exponential and Weibull cases
where parameters are unknown.9
In order to identify the test procedure, we begin with the null hypothesis H0:
Z ~ F0(z) versus Ha: ZF0(z), where F0(z) is a completely speciﬁed CDF. Let X1,. . .,
Xn be a random sample, and let ^FnðzÞ be the EDF based on the random sample.
The Kolmogorov-Smirnov test statistic is given by10
dn ¼ sup
z
j^FnðzÞ  F0ðzÞj:
The value of dn is seen to be the largest distance between the hypothesized CDF,
F0ðzÞ, and the EDF estimate of the true CDF, ^FnðzÞ. The larger is dn, the greater is
the largest numerical discrepancy between the estimated and hypothesized
CDFs. Note also that y0 ¼ F0(z) is effectively the probability integral transform
of the random sample as stated in H0, while the EDF can be viewed as estimating
outcomes from the true probability integral transform y ¼ F(z). Thus, an alter-
native interpretation of dn is the largest discrepancy between hypothesized and
estimated probability integral transforms of the data.
It is useful to note for computational purposes that dn ¼ max dþ
n ; d
n


, where
dþ
n ¼ max
i2 1;...;n
ð
Þ
i
n  F0 xðiÞ


	

; d
n ¼ max
i2 1;...;n
ð
Þ
F0 xðiÞ


 i  1
ð
Þ
n
	

;
9See M.A. Stephens, (1974), JASA, p. 730; and Chandra, et. al, (1981), JASA, p. 729.
10As always, sup can be replaced by max when the maximum exists.
672
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

and x(1), x(2),. . .,x(n) are the random sample outcomes ordered from lowest to
highest (i.e., they are the order statistics). This computational approach makes
the calculation of Dn a simple matter on a computer through sorting and
differencing operations.
In order to decide whether a discrepancy is signiﬁcant, the sampling distri-
bution of dn is required. The exact distribution of dn for n  40 has been tabled
by J.D. Gibbons, op. cit., p. 400. It was shown by A.N. Kolmogornov, (1933),
(Giorn. Inst. Ital. Attuari, 4, pp. 83–91) that the limiting CDF of n1/2 dn can be
represented as
lim
n!1 P n1=2dn  t


¼ 1  2
X
1
i¼1
1
ð
Þi1 exp 2i2t2


;
which results in the approximate critical points
a
.10
.05
.01
dn;a
1.224n1/2
1.358n1/2
1.628n1/2
Stephens (see footnote 9) has analyzed adjustments to these approximate
critical values that make them more accurate for small n, the adjustments being
to divide each of them by the factor (1 + .12n1/2 + .11 n1).
A size a K-S test of H0: Z ~ F0(z) versus Ha: Z  F0(z) is given as follows:
dn

<
(
)
dn;a )
reject H0
do not reject H0
(
)
:
From the Glivenko-Cantelli theorem and the fact that dn;a ! 0 8 a ∈(0,1), it
follows that the test is consistent.
Example 10.29
Testing Whether a
Population Distribution
Is a Uniform PDF
Revisit Example 10.28, and examine the null hypothesis that the distribution of
waiting times is uniform over the interval (0, 3.5), i.e., H0: Z ~ F0(z) versus Ha: Z 
F0(z) with F0(z) ¼ z/3.5 I(0, 3.5) (z). The value of dn ¼ supz2 0;3:5
ð
Þj^FnðzÞ z=3:5
ð
Þj ¼ :1443;
the maximum difference occurring for z ¼ 2.47 which has an EDF value of .85
and a (hypothesized) CDF value of 2.47/3.5 ¼ .7057. Stephens’ adjusted critical
value for n ¼ 40 and a size a ¼ .05 test is given by d40;.05 ¼ [1.358/(40)1/2]/
(1 + .12/(40)1/2 + .11/40) ¼ .2102. Because dn ¼ .1443 < d40;.05, ¼ .2102, we do not
reject the hypothesis that waiting times are uniformly distributed over the interval
(0, 3.5).
□
Lilliefors provided an adjustment to the K-S test that justiﬁes its use for
testing the null hypothesis H0: Z ~ N(m, s2) versus Ha: Z  N(m, s2) when m and
s2 are unknown and must be estimated from the data. In particular, he tabled the
distribution of Dn under the null hypothesis for n ¼ 1,. . .,30 via Monte Carlo
methods (see the previous Lilliefors reference), and presented approximate criti-
cal values that are accurate for n > 30 as follows:
10.8
Nonparametric Tests of Distributional Assumptions
673

a
.10
.05
.01
d
n;a
.805n1/2
.886n1/2
1.031n1/2
The test proceeds by ﬁrst unbiasedly estimating m and s2 using the
estimators x and ^s2 ¼ ns2= n  1
ð
Þ (using the sample variance itself would lead
to the same test size asymptotically). Then N(x, ^s2) is used for the distribution
under the null hypothesis, and dn ¼ supzj^FnðzÞ 
R z
1 N z; x; ^s2


dzj is calculated.
Finally, the K-S test is performed as before but using Lilliefors’ critical values,
d
n;a, in place of dn;a.
Example 10.30
Testing Whether a
Population Distribution
Is a Normal PDF via
Lilliefor’s Test
We revisit Example 10.21, and use the K-S test, as adjusted by Lilliefors, to test
the hypothesis H0: Z ~ N(m, s2) versus Ha: Z  N(m, s2) for some m and s2. We use
a ¼ .05.
The estimates of m and s2 are respectively ^m ¼ x ¼ 1:494 and ^s2 ¼ :8338.
Then using N(1.494, .8338) for the distribution of Z under the null hypothesis,
the value of the K-S statistic can be calculated as
dn ¼ sup
z
j^FnðzÞ 
ð z1:494
ð
Þ=:9131
1
N z; 0; 1
ð
Þdzj ¼ :0955;
which
occurs
for
z ¼ .74,
in
which
case
the
EDF
value
is
.30
and
R :74
1N z;1:494;:8338
ð
Þdz ¼ :2045: Using Lilliefors’ critical value of .886/(40)1/2 ¼
.1401, the hypothesis of normality cannot be rejected with this sample
outcome.
□
While the Lilliefors K-S test is usually very effective in detecting a false H0,
in this case it has failed. It turns out that the waiting times data was generated by
a uniform distribution (based on uniform random numbers generated by a
computer), so that the Lilliefors test failed to reject a false H0. Note that for
large enough n, we would have rejected the hypothesis, since this K-S test is
consistent. We note that Lilliefors’ own Monte Carlo calculations indicated that
among all of the alternative distributions he examined his test had the lowest
power against the uniform distribution, which may be why the test had difﬁ-
culty in the preceding example.
10.8.1.3
Shapiro-Wilks Test
The Shapiro-Wilks (SW) test procedure was designed speciﬁcally to test the
hypothesis that a random sample is from a N(m, s2) population distribution, the
null and alternative hypotheses being H0: Z ~ N(m, s2) and Ha: Z N(m, s2),
respectively, for some (unspeciﬁed) m and s2. The test procedure has been shown
to perform especially well in comparison to other tests of normality when
applied to the calculated residuals ^e ¼ y  xb of a least squares estimate of the
GLM Y ¼ xb + «. The null hypothesis in this case is that ei ~ N(0, s2) 8i (see
C. Huang and B. Bolch, (1974) op. cit.) The procedure fairs well in more general
674
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

settings as well. The SW test was originally devised by S. Shapiro and M. Wilks,
(1965), in “An Analysis of Variance Test for Normality,” Biometrika, pp.
591–611.
The SW test statistic is calculated as
w ¼
X
m
i¼1
aniþ1 x niþ1
ð
Þ  xðiÞ


"
#2
=ns2;
m
n=2
if n is even,
n  1
ð
Þ=2
if n is odd,

where (x(1), x(2),. . .,x(n)) are the sample observations ordered from smallest to
largest (i.e., the order statistics), s2 is the sample variance, and the aj’s are
coefﬁcients that are tabulated in the article by SW, pp. 603–604. The distribution
of the test statistic is quite involved, but it has been tabulated by SW for sample
sizes  50 (p. 605 of SW article) and for various levels of tail probabilities. The
test rule is given by
w

>
(
)
wn;1a )
reject H0
do not reject H0
(
)
:
The test procedure can be applied for small samples and is also a consistent test
of H0 versus Ha.
The heuristic motivation for the test lies in the fact that under the null
hypothesis of normality, both the numerator and denominator of the SW statis-
tic can be shown to be estimating the same quantity, s2, apart from constants.
Under the alternative hypothesis, the denominator will still (apart from n1) be
estimating s2, but such will not be the case for the numerator (in general).
The difference in behavior of the numerator and denominator under Ha is
exploited by SW in the design of the test procedure, and further motivation
can be found in their article.
Example 10.31
Testing Residuals of
a GLM for Normality
Using Shapiro
Wilks Test
The relationship between varying levels of advertising expenditures and the
quantity demanded of a product being advertised was studied over a 20-week
period, and 20 weekly observations were used to estimate the GLM Y ¼ xb + «,
where Y represents the vector of 20 observations on quantities sold, and x
represents a matrix of variables explaining the demand for the product, including
the level of advertising expenditures. The calculated residuals, ^e, from the least
squares ﬁt of the model yielded the following values, ^e, ordered from lowest to
highest row-wise:
7.0890
5.6489
5.1510
4.9672
4.8390
3.9812
3.9161
3.6296
3.5914
3.2087
2.9827
2.7810
1.2857
1.1390
0.8321
5.3253
6.2581
11.3091
14.4254
16.0611
We use the SW test to assess the hypothesis that H0: « ~ N(0, s2 I) versus Ha:
not H0 at the level a ¼ .05.
In this case, ns2 ¼ Pn
i¼1 ^ei  ^e

2 ¼ Pn
i¼1 ^e2
i ¼ 906:0822 (since ^e ¼ 0). Because
n is even, m ¼ n/2 ¼ 10. To calculate the numerator of the SW statistic we need
10.8
Nonparametric Tests of Distributional Assumptions
675

the appropriate values of ani+1 ¼ a21i, i ¼ 1,. . .,10, which are given in SW’s
table as
a20 ¼ .4734
a15 ¼ .1334
a19 ¼ .3211
a14 ¼ .1013
a18 ¼ .2565
a13 ¼ .0711
a17 ¼ .2085
a12 ¼ .0422
a16 ¼ .1686
a11 ¼ .0140
Then
P
10
i¼1
a21i x 21i
ð
Þ  xðiÞ


¼ :4734 16:0611 þ 7:0890
ð
Þ
þ :3211 14:4254 þ 5:6489
ð
Þ
þ :2565 11:3091 þ 5:1510
ð
Þ
þ :2085 6:2581 þ 4:9672
ð
Þ
..
.
..
.
þ :0422 2:7810 þ 3:5914
ð
Þ þ :0140 2:9827 þ 3:2087
ð
Þ
¼ 26:8090:
The critical value of w is found from SW’s table to be w20;.05 ¼ .905. Then, since
w ¼ (26.8090)2/906.0822 ¼ .7932  .905, H0: « ~ N(0, s2 I) is rejected at the
.05 level.
□
10.8.1.4
Jarque-Bera Test
The Jarque-Bera (JB) test11 assesses the null hypothesis that the observations are
iid from a normal population distribution N m; s2


, versus the alternative
hypothesis that the observations follow some other non-normal distribution.
The test can also be used to test for normality of the residuals of a GLM that has
an intercept term as part of the speciﬁcation. The test can be altered to accom-
modate GLM model speciﬁcations that do not have intercept terms.
The rationale for the test is to assess whether the skewness and the kurtosis
of the sample of observations are consistent with data that would be generated
by a normal population distribution. The test statistic is deﬁned by
JB ¼ n
6
^m3
^s3
	

2
þ 1
4
^m4
^s4  3
	

2
 
!
where
^m3
^s3 ¼
n1 P
n
i¼1
ðxi  xÞ3
n1 P
n
i¼1
ðxi  xÞ2
	

3=2 and ^m4
^s4 ¼
n1 P
n
i¼1
ðxi  xÞ4
n1 P
n
i¼1
ðxi  xÞ2
	

2 :
11C. Jarque and A. Bera, “A Test for Normality of Observations and Regression Residuals”, International Statistical Review,
pp.163–172, 1987.
676
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Jarque and Bera demonstrated that the JB statistics has an asymptotic Chisquare
distribution with 2 degrees of freedom. Based on its asymptotic distribution, the
test would be conducted as
JB
>

(
)
w2
2;a )
reject H0
do not reject H0
(
)
:
However, there is some Monte Carlo evidence suggesting that the convergence
of the statistic to its asymptotic distribution is rather slow, so that relatively
large sample sizes are needed for critical values set by the Chisquare distribution
to be accurate. For smaller sample sizes, some analysts resort to the Lilliefors
test mentioned above, while others (e.g., see the MATLAB implementation,12
when n < 2,000) have used Monte Carlo simulations to approximate the appro-
priate critical values to use with the test. Jarque and Bera, in their article (see
footnote 3) describe how computer simulation can be used to estimate the small
sample distribution of the JB statistic, from which critical values for testing can
be derived when n is relatively small.
10.8.2
Testing the iid Assumption
In this section we examine a test of the hypothesis that (X1,. . .,Xn) is an iid
random sample from some population distribution, (i.e., a simple random sam-
ple), the alternative hypothesis being that (X1,. . .,Xn) is a sample from some
general (non-iid) experiment. That is, H0: Xi’s are iid versus Ha: Xi’s are not iid.
The test we will present, the Wald-Wolfowitz runs test, depends on the concept
of a run in the outcome of a random sample.
10.8.2.1
Runs
To be able to deﬁne a run, we need to categorize the data according to some
dichotomous criteria, which results in the sample outcomes being transformed
into a collection of iid Bernoulli outcomes. A run is deﬁned to be a succession of
one or more identical values (1’s or 0’s), preceded and followed by a different
value, or else no value at all if the run occurs at the beginning or end of the
sample sequence. For example, in the sequence of 0’s and 1’s given by
0 0 1 1 1 0 1 0 1 1 0 0 1 1
there are eight runs which we differentiate by vertical lines as
00j111j0j1j0j11j00j11:
The basic idea of using runs to test the iid assumption is that if the random
variables are truly iid, then there should be neither too few nor too many runs
12Analysis of the JB-Test in MATLAB. MathWorks.
http://www.mathworks.com/access/helpdesk/help/toolbox/stats/jbtest.html. Retrieved May 2, 2012.
10.8
Nonparametric Tests of Distributional Assumptions
677

observed in any given outcome of the sample. Too few runs could be indicative
of grouping, clustering, or trending. Too many runs could indicate a systematic
alternating pattern. The Wald-Wolfowitz Runs test exploits this idea rigorously.
10.8.2.2
Wald-Wolfowitz Runs Test of the iid Assumption
The Wald-Wolfowitz13 (WW) rums test is concerned with testing whether or not
(X1,. . .,Xn) can be considered an iid random sample from some population distri-
bution. According to some dichotomous characteristic, we transform the out-
come of Xi into a 1 or 0, and we apply the same dichotomous characterization to
all of the xj’s. Note that the characteristic can be something inherent to the
experiment, such as the male/female characteristic in a case where Xi’s refer to
some measurement of consumer’s response in a (hypothesized) random sample
of consumers. Alternatively, a dichotomy could be imposed on random variables
whose outcomes are in the form of some numerical response by assigning a 1 to
values exceeding a speciﬁed value (e.g., the median value) and a 0 to values
below the speciﬁed value.
However the dichotomy is deﬁned, note that if there are n1 responses
equaling 1 and n0 responses equaling 0, then there are
n1 þ n0
n1
	

different
ways to rearrange the n1 1’s and n0 0’s. If the xi’s are truly iid (i.e., H0 is true),
then each of these rearrangements is equally likely, and the probability of each
rearrangement is
n1 þ n0
n1
	

1
by classical probability.
Now let w1 and w0 represent the number of runs involving 1’s and involving
0’s, respectively. Note, by the deﬁnition of a run, it must be the case that
|w1  w0|  1. Examine the case where the total number of runs, w ¼ w1 + w0,
is even so that w1 ¼ w0 ¼ w/2, and consider the deﬁnition of sequences
consisting of n1 1’s and n0 0’s that represent w1 runs of 1’s and w0 runs of 0’s.
How many different ways can we deﬁne the sequence of w1 sets of 1’s in the
collection of w runs? This number is equivalent to the number of different ways
w1  1 ¼ w/2  1 vertical lines can be inserted in the n1  1 spaces between the
n1 1’s (recall our previous use of vertical lines to delineate runs), and equals
n1  1
w=2
ð
Þ  1
	

. Similarly, the number of different ways we can deﬁne the
sequence of w0 sets of 0’s in the collection of w runs is
n0  1
w=2
ð
Þ  1
	

. Finally,
since the sequence of w runs can begin with either a group of 0’s or a group of 1’s,
there are 2
n1  1
w=2
ð
Þ  1
	

n0  1
w=2
ð
Þ  1
	

different ways of obtaining w runs when
w is even, and thus
13Wald, A. and Wolfowitz, J., (1940), On a test whether two samples are from the same population. Ann. Math. Statist. 11, pp.
147–162.
678
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

P w; n1; n0
ð
Þ ¼
2
n1  1
w=2
ð
Þ  1
	

n0  1
w=2
ð
Þ  1
	

n1 þ n0
n1
	

; for even w:
Now examine the case were w ¼ w1 + w0 is odd, so that either w1 ¼ (w + 1)/2 and
w0 ¼ (w1)/2 if the sequence begins and ends with a group of 1’s, or else
w1 ¼ (w1)/2 and w0 ¼ (w+1)/2 if the sequence begins and ends with a group
of 0’s. Using logic analogous to the case where w was even, we have that the
number of different ways of obtaining w runs when w is odd is given by the
numerator of
P w;n1;n0
ð
Þ ¼
n1  1
w  1
ð
Þ=2
	

n0  1
w  3
ð
Þ=2
	

þ
n1  1
w  3
ð
Þ=2
	

n0  1
w  1
ð
Þ=2
	

n1 þ n0
n1
	

; for odd w:
We deﬁne P(w; n1, n0) ¼ 0 in all other cases.
The discrete density function P(w; n1, n0) can be used to deﬁne upper and
lower critical values, and thus upper and lower rejection regions for the test
statistic outcome w. As usual, the size of the test will be determined by the
choice of critical values and the associated upper and lower tail probabilities as
indicated by P(w; n1, n0).
It can be shown that the mean and variance of P(w; n1, n0) are given by
EðWÞ ¼ 2n1n0
n1 þ n0
þ 1 and varðWÞ ¼ 2n1n0 2n1n0  n1  n0
ð
Þ
n1 þ n0
ð
Þ2 n1 þ n0  1
ð
Þ
:
Furthermore, as n1 ! 1 and n0 ! 1, it follows that [W  E(W)]/[var(W)]1/2 !
d
N(0,1), allowing asymptotically valid size a test of the iid assumption to be
deﬁned in terms of standard normal critical values. The rate of convergence to
the limiting distribution is very rapid, so that for n1 > 10 and n0 > 10, the
normal distribution approximation is very good. A table of critical values
based on P(w;n1,n0) for n  20 has been published by Swed and Eisenhart,
(1943), Annals of Mathematical Statistics, (14), pp. 66–87.
A size a ¼ a‘ + ah runs test can be deﬁned as follows:
w
2
=2
(
)
0; wn1;n0;1a‘


[ wn1;n0;ah; n


)
reject H0
do not reject H0
(
)
:
Example 10.32
Testing the iid
Assumption via
the Wald-Wolfowitz
Runs Test
We will test whether the observations on the waiting times between customers
given in Example 10.28 can be viewed as iid observations from some population
distributionusing a test of size .05.
The observations occurred sequentially row-wise in the data matrix of
Example 10.28, and a median of the observation is (1.50 + 1.68)/2 ¼ 1.59.
10.8
Nonparametric Tests of Distributional Assumptions
679

Classifying the sample outcomes as 1’s or 0’s according to whether the outcome
is > or < 1.59 yields the following results, with runs delineated by vertical lines
0j1j0000j1111j0j1j0j11111j00j1j0j1j0j1j0j1j00j11j000j1j00j11j0:
In this case, there are w ¼ 23 runs. Since n1 and n0 are each > 10, we use the
normal approximation to the distribution of W to conduct the test. Note n1 ¼
n0 ¼ 20 in this case, so that
EðWÞ ¼ 2ð20Þð20Þ
40
þ 1 ¼ 21 and
varðWÞ ¼ 2ð20Þð20Þð2ð20Þð20Þ  20  20Þ
ð20 þ 20 Þ2 ð20 þ 20  1Þ
¼ 9:7436:
The size a runs test based on the asymptotic normal distribution for W is
z ¼ w  EðWÞ
ðvarðWÞ Þ1=2
2
=2
(
)
1; za=2


[ za=2;1


)
reject H0
do not reject H0
(
)
:
Since in the case at handz ¼ 23  21
ð
Þ= 9:7436
ð
Þ1=2 ¼ :6407and z.025 ¼ 1.96, then
z =2 (1, 1.96] [ [1.96, 1), and the iid hypothesis is not rejected.
□
One can consider using the runs test in the GLM context to check the
validity of the iid assumption for the disturbances. Unfortunately, even if the «
vector of Y ¼ xb + « consists of iid random variables, e ¼ Y  x^b does not, since
Cov e
ð Þ ¼ s2 I  x x0x
ð
Þ1x0


. However, as n increases,e ¼
I  x x0x
ð
Þ1x0


« 	 «if
(x0x)1 ! 0, which occurs very generally (see Section 10.8.2) and so for large
samples, the runs test may serve as an approximate test.
There are a number of alternative nonparametric tests of randomness that
have been proposed in the literature, including the signs test, the Mann-Kendall
test, and Bartel’s rank test, among others. The interested reader can continue her
reading in this area by consulting books devoted to nonparametric statistical
inference, such as the book by J. D. Gibbons and S. Chakraborti, (1992) Nonpara-
metric Statistical Inference. New York: Marcel Dekker.
10.9
Noncentral x2- and F-Distributions
10.9.1
Family Name: Noncentral x2-Distribution
Parameterization: v (degrees of freedom) and l (noncentrality parameter).
Deﬁnition: f(x; v, l) ¼ P
j¼0 lj=j!


elh x; v þ 2j
ð
ÞI 0;1
ð
ÞðxÞ where h(x; v+2j) is the
(central) w2-density with v+2j degrees of freedom.
Moments: m ¼ v + 2l, s2 ¼ 2(v + 4l), m3 ¼ 8(v + 6l)
MGF: MX(t) ¼ (12t)v/2 exp
2lt
1  2t
	

for t < 1/2
The noncentral w2 distribution is the distribution of Y ¼ Pv
i¼1 Z2
i , where the
Zi’s are independent normally distributed random variables with unit variances
680
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

and means fi, i ¼ 1,. . .,v, i.e., Zi~N(fi, 1), i ¼ 1,. . .,v with Z1,. . .,Zn being inde-
pendent random variables. The density deﬁnition can be found via the change of
variables approach of Section 6.6. The noncentrality parameter l is related to the
means of the Zi’s as l ¼f0f=2.
Note that the density deﬁnition can be recognized as a Poisson-weighted
sum of (central) w2
vþ2j density functions, j ¼ 0,1,2,. . ., and in effect can be thought
of as the expected value of a w2-density with v + 2J degrees of freedom, the
“random variable” J having a Poisson density. This type of density deﬁnition,
where the parameters of one density function family are effectively being treated
as random variables and an expectation is taken with respect to another family
of density functions, produces what is known as a mixture distribution. That is,
members of one family are effectively being "mixed" using weights, applied to
parameters, that are provided by another density family.
When l ¼ 0, the central w2-distribution is deﬁned, which we discussed in
Section 4.2. When l > 0, the mean of the distribution moves to the right, and the
variance and skewness increase so that the density has less height and has a
fatter, more pronounced right tail compared to the central w2 density. In particu-
lar, for any c < 1,
lim
l!1 Pðx  c; lÞ
ð
Þ ¼ lim
l!1
ð1
c
fðx; v; lÞdx
	

¼ 1
and P(x  c; l) is monotonically increasing as a function of l.
A principal use of the noncentral w2-distribution is in analyzing the power
function of statistical tests based on w2-statistics whose distribution under Ha is
noncentral w2. Power in these cases is generally represented as a function of the
noncentrality parameter as
pCrðlÞ ¼
ð
x2Cr
fðx; v; lÞdx:
Examples are given by the GLR, LM, and Wald statistics, all of which have
asymptotically
valid
noncentral
w2-distributions
under
local
alternative
hypotheses in Ha.
Integration of the noncentral w2-distribution is straightforwardly accom-
plished on personal computers using software such as GAUSS, SAS, or
MATLAB.
The
reader
must
be
warned
that
the
parameterization
of
noncentrality is not standard in the literature. Other parameterizations include
noncentrality equal to 2l and
ﬃﬃﬃﬃﬃﬃ
2l
p
. The parameterization we use is more
prevalent in the econometrics literature.
For further information, see N.L. Johnson and S. Kotz (1970), Continuous
Univariate Distributions, II, New York: Wiley, pp. 130–148.
10.9
Noncentral w2- and F-Distributions
681

10.9.2
Family Name: Noncentral F-Distribution
Parameterization: v1 (numerator degrees of freedom), v2 (denominator degrees of
freedom), l (noncentrality parameter)
Deﬁnition:
f x; v1; v2; l
ð
Þ ¼
X
1
j¼0
lj
j!
 !
el
v1=v2
ð
Þ v1
v2
x
	

 v1=2
ð
Þþj1
B v1=2
ð
Þ þ j; v2=2
ð
Þ 1 þ v1
v2
x
	

:5 v1þv2
ð
Þþj
2
6664
3
7775I 0;1
ð
ÞðxÞ
where B(a,b) is the Beta function (recall the Beta distribution discussion in
Section 4.2.3).
Moments:
m ¼ v2 v1 þ 2l
ð
Þ
v1 v2  2
ð
Þ for v2 > 2;
s2 ¼ 2 v2
v1
	

2 v1 þ 2l
ð
Þ2 þ v1 þ 4l
ð
Þ v2  2
ð
Þ
v2  2
ð
Þ2 v2  4
ð
Þ
for v2 > 4
m3 ¼ g v1; v2; l
ð
Þ increasing in l and v1, decreasing in v2 for v2 > 6:
MGF: Does not exist.
The noncentral F-distribution is the distribution of Y ¼ Zv1=v1
ð
Þ= Zv2=v2
ð
Þ,
where Zv1  w2
v1 l
ð Þ and Zv2  w2
v2 are independent noncentral and central w2 ran-
dom variables with v1 and v2 degrees of freedom, respectively. The density of Y
can be established via transformation from the joint density of Zv1; Zv2
ð
Þ, which
is the product of a noncentral and a central w2 density.
When l ¼ 0, Y is the ratio of two independent central w2-random variables,
each divided by their degrees of freedom, so that Y has the (central) F-distribution
as we derived in Section 6.7. When l > 0, the mean of the distribution moves to
the right, and the variance and skewness also increase so that the density has
less height and has a fatter, more pronounced right tail in comparison to the
central F-distribution. In particular, for c < 1,
liml!1 Pðx  c; lÞ
ð
Þ ¼ liml!1
ð1
c
f x; v1;v2; l


dx
	

¼ 1;
and P(x  c;l) is monotonically increasing as a function of l.
As in the case of the noncentral w2-distribution, a principal use of the
noncentral F-distribution is in analyzing the power function of statistical tests
based on test statistics that have a noncentral F-distribution when Ha is true.
Power in these cases is generally represented as a function of the noncentrality
parameter as
pCr l
ð Þ ¼
ð
x2Cr
f x; v1; v2; l
ð
Þdx:
682
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

An example of the use of the noncentral F-distribution was given in the discus-
sion of testing H0: Rb¼ r in the GLM context under normality, in which case the
power function was representable in terms of a noncentral F-distribution.
Integration of the noncentral F-distribution is straightforwardly accom-
plished on personal computers using software such as GAUSS, SAS, and
MATLAB. The warning regarding the parameterization of noncentrality given
in our discussion of the noncentral w2-distribution applies equally well here.
Further information can be found in N.L. Johnson and S. Kotz, Continuous
Distributions, pp. 189–200.
10.10
Appendix: Proofs and Proof References for Theorems
Theorem 10.5: Proof
(This proof is somewhat involved. For a related proof, see T. Amemiya, (1985),
Advanced Econometrics, Harvard University Press, Cambridge, pp. 142–144. The
original work of A. Wald, (1943), “Tests of Statistical Hypotheses Concerning
Several Parameters when the Number of Observations is Large,” Transactions of
the American Mathematical Society, pp. 426–482 provides an alternative, albeit
more difﬁcult and restrictive proof of the asymptotic distribution result).
Expand ln(L(Q0;X)) in a second order Taylor series around the ML estimator
^Q as14
ln L ^Q; X




 ln L Q0; X
ð
Þ
ð
Þ ¼  1
2
^QQ0

0 @2 ln L Q; X
ð
Þ
ð
Þ
@Q@Q0
^QQ0


;
ð10:1Þ
where Q* ¼ t ^Q + (1  t)Q0 for t∈[0,1] and the ﬁrst-order term is dropped since
@L ^Q; X


=@Q ¼ 0 by the ﬁrst order conditions deﬁning the MLE. Let R(Q) ¼ r
be used to deﬁne q of the entries in Q as functions of the remaining entries, and
without loss of generality, assume the entries in Q have been ordered so that the
ﬁrst q Yi’s are functions of the remaining (k  q) Yi’s, say as Qa ¼ g(Qb), where
Qa ¼ (Y1,. . .,Yq) and Qb ¼ (Yq+1,. . .,Yk). Then under the constraint R(Q) ¼ r, the
feasible Q-vectors can be characterized as
Q ¼
g Qb


Qb
"
#
¼ h Qb


:
Making this substitution in the likelihood function deﬁnes the restricted likeli-
hood function Lr(Qb;X) 
 L(h(Qb);X). To simplify notation, deﬁne h 
 Qb.
Assuming H0: R(Q) ¼ r to be true, then Q0 ¼ h(h0), and we can expand ln
(Lr(h0;x)) in a second order Taylor series around the ML estimate ^h, analogous
to the preceding expansion, as
ln Lrð^h; XÞ
ð
Þ  ln Lrðh0; XÞ
ð
Þ ¼  1
2 ð^h  h0Þ0 @2 ln Lrðh; XÞ
ð
Þ
@h@h0
ð^h  h0Þ:
ð10:2Þ
14R.G. Bartle, op. cit., p. 371.
10.10
Appendix: Proofs and Proof References for Theorems
683

Now note that since ^Q !
p Q0 and ^h ! h0, then Q* !
p
Q0 and h* ! h0. Then
rewriting the right hand side of (1) as
 1
2
^Q  Q0

0
n1=2 n1 @2 ln L Q; X
ð
Þ
ð
Þ
@Q@Q0
"
#
n1=2
^Q  Q0


;
note that the square bracketed expression converges in probability to M(Q0),
a positive deﬁnite symmetric matrix. Then ln(L( ^Q ;X)) 
ln(L(Q0;X)) and
 (1/2) ( ^Q  Q0)0 n1/2 M(Q0) n1/2( ^Q  Q0) will share the same limiting
distribution by Theorem 5.7. An analogous argument applied to (2) indicates
that ln(Lr(^h;X))  ln(Lr(h0;x)) and
 1
2 ð^h  h0Þ0n1=2Mrðh0Þn1=2ð^h  h0Þ
share the same limiting distribution. Letting ¼
d indicate equivalence in terms of
limiting distributions, it follows from L(Q0;x) ¼ Lr(h0;x) that
2 ln l X
ð Þ
ð
Þ ¼
d
^QQ0

0
n1=2M Q0
ð
Þn1=2
^Q  Q0


 ^h  h0
ð
Þ0n1=2Mr h0
ð
Þn1=2 ^h  h0
ð
Þ
ð10:3Þ
We now relate the limiting distributions of n1=2
^Q  Q0


and n1=2 ^h  h0
ð
Þ .
It follows from the proof of Theorem 8.17 that
n1=2
^Q  Q0


¼
d M Q0
ð
Þ1n1=2 @ ln L Q0; X
ð
Þ
ð
Þ
@Q
!
d N 0; M Q0
ð
Þ1


and
n1=2ð^h  h0Þ ¼
d Mr h0
ð
Þ1n1=2 @ ln Lr h0; X
ð
Þ
ð
Þ
@h
!
d N 0; Mr h0
ð
Þ1


:
Because
@ ln Lrðh0; XÞ
ð
Þ
@h
¼ @hðh0Þ
@
@ ln LðQ0; XÞ
ð
Þ
@Y
,
it follows that
n1=2 ð^h h0Þ ¼
d Mrðh0 Þ1 n1=2 @hðh0Þ
@h
n1=2 n1=2 @ ln LðQ0; XÞ
ð
Þ
@Q


:
Substituting for n1=2
^QQ0


and n1=2ð^h  h0Þ on the right hand side of (10.3),
and letting
Zn ¼ n1=2 @ ln L Q0; X
ð
Þ=@Q
ð
Þ
ð
Þ to simplify notation,
2 ln lðXÞ
ð
Þ ¼
d Z0
n M Q0
ð
Þ1  G0Mr h0
ð
Þ1G
h
i
Zn;
ð10:4Þ
where G ¼ @h h0
ð
Þ=@h.
Deﬁne Vn ¼ M(Q0)1/2 Zn, so that Vn !
d V ~ N(0,I), and rewrite (10.4) as
2 ln lðXÞ
ð
Þ ¼
d V0
n I  M Q0
ð
Þ1=2G0Mr h0
ð
Þ1GM Q0
ð
Þ1=2
h
i
Vn:
ð10:5Þ
684
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

The bracketed matrix is idempotent, which can be demonstrated by multiplying
the bracketed matrix by itself and using the condition G M(Q0)G0 ¼ Mr(h0).
Furthermore, the trace of the idempotent matrix is seen to be k  (k  q) ¼ q.
Then representing the quadratic form (10.5) in terms of the characteristic roots
and vectors of the idempotent matrix as V0n P0 L P Vn, analogous to the proof of
Theorem 6.12b, we ﬁnally obtain
2 ln lðXÞ
ð
Þ !
d V0
nPLP0V ¼
X
q
i¼1
W2
i  w2
q0
where W¼P0V ~ N(0, I) and L is a diagonal matrix having q-1’s and (k  q)-0’s
along the diagonal.
n
Theorem 10.6: Proof
Let ^Qr and ^Q represent the MLEs for Q∈H0 and Q∈H0[Ha, respectively, and
expand ln(L( ^Qr;X)) in a second order Taylor series around the point ^Q to obtain
2 ln L ^Qr; X




 ln L ^Q; X




h
i
¼ 2 ln l X
ð Þ
ð
Þ
¼
^Qr  ^Q

0 @2 ln LðQþ; XÞ
ð
Þ
@Q@Q0
"
#
^Qr  ^Q


;
where we have used the fact that @ ln Lð ^Q; XÞ


=@Q ¼ 0 in the ﬁrst order term of
the Taylor series and Qþ ¼ t ^Q þ ð1  tÞ ^Qr for some t∈[0,1]. We know from the
proofs of Theorems 8.18 and 8.19 that
n1=2
^Q  Q0


¼
d M Q0
ð
Þ1 n1=2 @ ln LðQ0; XÞ
ð
Þ
@Q


(recall ¼
d means equivalence in terms of limiting distributions).
Under the sequence of local alternatives Han: R(Q) ¼ r + n1/2 f, ^Qr !
p Q0,
and it can be shown that (see the subsequent proofs of Theorems 10.7 and 10.8)
n1=2
^Qr Q0


¼
d M Q0
ð
Þ1 n1=2 @ln LðQ0;XÞ
ð
Þ
@Q


þM Q0
ð
Þ1G0 GM Q0
ð
Þ1G0
h
i1

GM Q0
ð
Þ1 n1=2 @ln L Q0;X
ð
Þ
ð
Þ
@Q
	

f


where G ¼ ∂R(Q0)/∂Q0. Then
n1=2
^Qr  ^Q


¼ n1=2
^Qr Q0


n1=2
^QQ0


¼
d M Q0
ð
Þ1G0 GM Q0
ð
Þ1G0
h
i1
GM Q0
ð
Þ1 n1=2 @ln L Q0;X
ð
Þ
ð
Þ
@Q
	

f


;
¼
d M Q0
ð
Þ1G0Zn;
where Zn !
d N([GM(Q0)1 G0]1 f, [GM(Q0)1 G0]1).
10.10
Appendix: Proofs and Proof References for Theorems
685

Noting that 2ln(l(X)) ¼
d n1/2( ^Qr  ^Q)0 M(Q0) ( ^Qr  Q0)n1/2 because
 n1 @2L Qþ; X
ð
Þ
@Q@Q0
!
p M Q0
ð
Þ;
it follows that
2ln lðXÞ
ð
Þ ¼
d Z0
n GM Q0
ð
Þ1G0
h
i
Zn !
d w2
q ðlÞ
with l ¼ 1
2 f0 GM Q0
ð
Þ1G0
h
i1
f because
GM Q0
ð
Þ1G0
h
i1=2
Zn !
d N
GM Q0
ð
Þ1G0
h
i1=2
f; I
	

(see Section 10.9 on properties of the noncentral w2 distribution).
n
Theorem 10.7: Proof
Expanding both @ ln L ^Qr; X




=@Q and R ^Qr


 r in a ﬁrst order Taylor series
around the true Q0 allows the ﬁrst order conditions of the ML problem to be
written as
@ ln L Q0; X
ð
Þ
@Q
þ @2 ln L Q; X
ð
Þ
@Q@Q0
^Qr  Q0



@R ^Qr


@Q
Lr ¼ 0
@R ^Qþ

0
@Q
^Qr  Q0


¼ 0
;
where Q* and Q+ each lie between Q0 and ^Qr .15 Note the second equation
incorporates the fact that the ﬁrst term in the Taylor series, R(Q0)  r, is zero
under H0. Premultiplying the ﬁrst of the preceding equations by n1/2 and the
second by n1/2, leads to the partitioned matrix equation
n1 @2 ln L Q; X
ð
Þ
@Q@Q0
@R ^Qr


@Q
@R Qþ
ð
Þ0
@Q
0
2
66664
3
77775
n1=2
^Qr  Q0


n1=2Lr
"
#
¼
n1=2 @ ln L Q0; X
ð
Þ
@Q
0
"
#

ð Þ
15We must alert the reader to a technical point that we suppress notationally regarding the use of Taylor series representations of
vector functions. Speciﬁcally, such a representation is actually a collection of Taylor series representations, one for each entry in the
vector function, say f(z). As such, the point of evaluation of the ﬁnal derivative terms in each Taylor series can differ for each
coordinate function. For example, if f(z) is (j1), then in the Taylor series representation
fðzÞ ¼ f z0
ð
Þ þ @fðzÞ0
@z
z  z0
ð
Þ
it can be that each row of @fðzÞ0
@z
must be evaluated at a different z* ¼ tz + (1  t)z0, t∈[0,1]. Having alerted the reader to this situation,
we will tacitly assume henceforth that this is understood. What is most important for our purposes is that z* ! z0 as z ! z0, so in this
case, in the limit, all rows of @fðzÞ0
@z
will be evaluated at the same point.
686
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Observe that the right hand side of the matrix equation (*) converges in
distribution to the vector z
0
h i
, where Z ~ N(0, M(Q0)).
In examining the asymptotic behavior of n1/2 ( ^Qr  Q0) and n1/2 Lr it will be
permissible by Theorem 5.9 to write the ﬁrst matrix of (*) as
because ^Qr, and thus Q* and Q+, !
p Q0, ∂R(Q)/∂Q is continuous, and
n1 @2 ln L Q; X
ð
Þ
@Q@Q0
!
p M Q0
ð
Þ:
Using partitioned inversion,16 n1/2 Lr can be solved for, yielding
n1=2Lr ¼
@R Q0
ð
Þ0
@Q
M Q0
ð
Þ1 @R Q0
ð
Þ
@Q

1 @R Q0
ð
Þ0
@Q
M Q0
ð
Þ1 n1=2 @ ln L Q0; X
ð
Þ
ð
Þ
@Q


If G ¼ @R Q0
ð
Þ=@Q0 has full row rank, it follows by Slutsky’s theorems that
n1=2Lr !
d N 0; GM Q0
ð
Þ1G0
h
i1
	

;
and
GM Q0
ð
Þ1G0
h
i1=2
n1=2 Lr !
d
N 0; I
ð
Þ:
Because the limiting distribution is unaffected by replacing G with @R ^Qr


=@Q0
andM Q0
ð
Þ by
n1 @2 ln L ^Qr; X




=@Q@Q0


h
i
we ﬁnally have (recall ¼
d means
equivalent in limiting distribution)
W ¼ n1L0
r GM Q0
ð
Þ1G0
h
i
Lr ¼
d L0
r
@R ^Qr

0
@Q

@2 lnL ^Qr;X


@Q@Q0
0
@
1
A
2
4
3
5
1
@R ^Qr


@Q
Lr;
!
d w2
q under H0:
The size a test indicated in part (2) of the theorem follows immediately from the
limiting distribution of W. Consistency can be demonstrated by retracing the
previous argument beginning with the initial Taylor series expansion of the ﬁrst
order conditions, but with R(Q0)  r ¼ f 6¼ 0 so that 0 is replaced by n1/2 f
16One can use the following result for symmetric matrices (Theil, H., (1971), Principles of Econometrics, John Wiley, NY, p. 18:
A
C
C0
B

1
¼
A1 þ A1C B  C0A1C

1C0A1
A1C B  C0A1C

1
 B  C0A1C

1C0A1
B  C0A1C

1
2
4
3
5
10.10
Appendix: Proofs and Proof References for Theorems
687

in the vector on the right hand side of the equality in (*). The net result is that
P(w > t) ! 1 as n ! 1 8t > 0, and so H0 is rejected for any rejection region of size
∈(0,1) of the form
w2
q;a; 1
h

. See Silvey, S.D. op. cit. pp. 387–407, for further
discussion.
The alternative representation of W in part (3) of the theorem follows immedi-
ately from the ﬁrst order conditions @ ln L ^Qr; X




=@Q ¼
@R ^Qr


=@Q


Lr. n
Theorem 10.8: Proof
Assuming R(Q)  r ¼ n1/2 f ! 0, as implied by the sequence of local
alternatives, it follows that the sequence of restricted MLEs of Q0 is such that
^Qr !
p Q0. Matrix equation (*) in the proof of Theorem 10.7 applies except that
0 in the vector on the right hand side of the equality changes to – f (since the
Taylor series expansion of R ^Qr


 r ¼ 0 is now
n1=2f þ @R Qþ
ð
Þ
@Q0
^Qr  Q0


¼ 0:
Following the proof of Theorem 10.7 in applying partitioned inversion and the
convergence result of Theorem 5.9, the revised matrix equation (*) yields
n1=2L0
r¼
d
@R Q0
ð
Þ0
@Q
M Q0
ð
Þ1 @R Q0
ð
Þ
@Q

1 @R Q0
ð
Þ0
@Q
M Q0
ð
Þ1 n1=2 @ ln L Q0;X
ð
Þ
ð
Þ
@Q
	

f


:
By Slutsky’s theorem, the right-most bracketed term converges in distribution to
N f; @RðQ0Þ0
@Q
MðQ0 Þ1 @RðQ0Þ
@Q
	

;
and letting G ¼ @RðQ0Þ=@Q0, it follows that
GM Q0
ð
Þ1G0
h
i1=2
n1=2Lr !
d N  GM Q0
ð
Þ1G0
h
i1=2
f; I
	

Finally, since
@R ^Qr


=@Q !
p G and n1 @2 ln L ^Qr; X




=@Q@Q0


!
p M Q0
ð
Þ
then
@R ^Qr

0
@Q
n1 @2 ln L ^Qr; X


@Q@Q0
2
4
3
5
1
@R ^Qr


@Q
2
64
3
75
1=2
!
p
GM Q0
ð
Þ1G0
h
i1=2
so that by Slutsky’s theorem,
W ¼ L0
r
@R ^Qr

0
@Q

@2 ln L ^Qr; X


@Q@Q0
2
4
3
5
1
@R ^Qr


@Q
Lr !
d w2
q ðlÞ;
with l ¼ ð1=2Þf0 GM Q0
ð
Þ1G0
h
i1
f (see Section 10.9 for properties of the non-
central w2 distribution.
n
688
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

Keywords, Phrases, and Symbols
Asymptotic conﬁdence intervals and
regions
Asymptotically equivalent tests
Asymptotic pivotal quantity
Asymptotic power
Asymptotically unbiased tests
Conﬁdence coefﬁcient
Conﬁdence interval, conﬁdence
region
Conﬁdence level
Consistent conﬁdence
region
Duality between conﬁdence and
critical regions
Generalized likelihood ratio (GLR)
test
Heuristic principle of test
construction
Jarque-Bera Test
Kolmogorov-Smirnov test statistic
Lagrange multiplier (LM) test
Lilliefors Test
Local alternatives
Mixture distribution
Noncentral F distribution
Noncentral t-distribution
Noncentral w2 distribution
Pittman drift
Pivotal quantity
Pivotal quantity method
Restricted likelihood function
Runs
Scoring test
Shapiro-Wilks Test
Unbiased conﬁdence region
Uniformly most accurate (UMA)
Uniformly most accurate unbiased
(UMAU)
Upper and lower conﬁdence intervals
Wald Test
Wald-Wolfowitz (WW) Runs Test
w2 Goodness of ﬁt test
Problems
1. A manufacturer of breakfast cereals has been accused
of systematically underﬁlling their cereal packages. The
manufacturer claims that the complaint must stem from
settling of the cereal that occurs in shipment, and that the
air space at the top of the package is normal settling of the
contents. They claim that the ﬁlling process is normally
distributed, and that the system is under control at a
mean ﬁll rate of m ¼ E(Z) ¼ 16.03 ounces and a standard
deviation of s ¼ .01, so that it is highly improbable that a
package is ﬁlled below its stated contents of 16 ounces.
They want you to test the hypothesis that the ﬁlling
system is under control at a level of signiﬁcance equal to
.10. They provide you with the following random sample
outcome of 40 observations on ﬁll weights:
15.97
15.75
15.90
15.87
15.96
15.90
16.05
16.04
16.13
15.92
15.70
15.89
15.79
15.74
15.88
15.86
16.01
15.89
15.83
15.97
15.92
15.88
15.84
15.95
15.82
16.07
16.01
16.04
15.92
15.81
15.71
15.95
15.88
15.81
15.85
15.84
15.79
16.03
15.80
15.80
(a) Deﬁne a size .10 GLR test of the hypothesis H0:
m ¼ m0 , s ¼ s0 versus Ha: not H0. Test the hypothesis
that the ﬁlling process is under control. You may use
the asymptotic distribution of the GLR if you wish.
(b) Deﬁne a size .05 LM test of the same hypothesis as in
(a) above, and test the hypothesis at signiﬁcance level
.10. Are the two tests different? Are the two test
decisions in agreement?
(c) Test the two hypotheses H0: m ¼ 16.03 and H0:
s ¼ .01 individually using size .05 tests. Use what-
ever test procedures you feel are appropriate. Interpret
the outcomes of the tests individually. Interpret the
tests jointly using a Bonferroni approach.
(d) Can you deﬁne a Wald test for the hypothesis in (a)? If
so, perform a Wald test of the joint null hypothesis at
signiﬁcance level .10.
2. Testing for Differences in Two Populations: Variances.
After the ﬁrm in question (1) was inspected by the Dept.
of Weights and Measures, a new random sample of forty
observations on ﬁll weights was taken the next day,
resulting in the following summary statistics:
m0
1 ¼ 16:02753 and s2 ¼ :9968  102:
You may continue to assume normality for the population
distribution.
(a) Deﬁne a GLR size .05 test of the H0:s2
1 s2
2 versus Ha:
not H0, where s2
1 and s2
2 refer to the variances of the
populations from which the ﬁrst and second sample
were taken. Test the hypothesis.
(b) Repeat (a) for the hypothesis H0:s2
1 s2
2 versus Ha: not
H0.
Problems
689

(c) Deﬁne a GLR size .05 test of the H0: s2
1 ¼ s2
2 versus
Ha: not H0. Either go to the computer and perform
this test, or else deﬁne an approximation to this test
based on “equal tails” and perform the test.
(d) Deﬁne the power function for each of the hypothesis
testing procedures you deﬁned above. With the aid of
a computer, plot the power functions and interpret
their meanings.
3. Testing for Differences in Two Populations: Means
(Equal Variances).
Referring to the data obtained in both preceding questions,
consider testing whether the two populations means
are the same. Assume that the population variances are
identical in the two cases–did you ﬁnd any evidence to
contradict this in your answer to (2) above?
(a) Deﬁne a GLR size .05 test of the H0: m1  m2 versus
Ha: not H0, where m1 and m2 refer to the means of the
populations from which the ﬁrst and second sample
were taken. Test the hypothesis.
(b) Repeat (a) for the hypothesis H0: m1  m2 versus Ha:
not H0.
(c) Deﬁne a GLR size .05 test of the H0: m1 ¼ m2 versus
Ha: not H0 . Test the hypothesis at the .10 level of
signiﬁcance.
(d) Deﬁne the power function for each of the hypothesis
testing procedures you deﬁned above. With the aid of
a computer, plot the power functions and interpret
their meanings.
4. Testing for Differences in Two Populations: Means
(Unequal Variances) (The Behrens-Fisher Problem).
Consider the hypotheses in Problem (3) in the case where
it
is
not
assumed
that
the
variances
of
the
two
populations are equal.
(a) Attempt to ﬁnd a GLR test of the hypothesis H0:
m1 ¼ m2 versus Ha: not H0 based on a t-statistic simi-
lar to what you found in (3) above. Is this approach
valid? The problem of testing hypotheses concerning
the means of two normal population distributions
when it is not assumed that the variances are equal
is known as the Behrens-Fisher problem, which as of
yet has no universally accepted solution.
One reasonable solution to this problem is to use
the statistic
t ¼
x1  x2
s2
1=n1 þ s2
2=n2

1=2
where n1 and n2 refer to the respective sample sizes of
samples from the two populations. It can be shown that a
test of size  a of any of the hypotheses concerning the
means of the two population distributions given in
(3a)–(3c) can be performed by deﬁning rejection regions
as follows:
H0
Ha
Cr
m1 ¼ m2
m1 6¼ m2
(1,ta/2(m)][[ta/2(m), 1)
m1  m2
m1 < m2
(1, ta(m)]
m1  m2
m1 > m2
[ta(m), 1)
The value of ta(m) is a typical critical value of the
student t distribution found in t-tables, except the
degrees of freedom parameter m ¼ min(n1, n2).
The procedure is sometimes referred to as the
Hsu procedure. Further discussion of the Behrens
Fisher problem can be found in H. Scheffe, (1970),
“Practical
Solutions
of
the
Behrens-Fisher
Problems,” JASA, 65, pp. 1501–1508.
(b) Using the Hsu procedure discussed above, test the
hypothesis of the equality of the means in the popu-
lation distributions referred to in Problems (1) and (2)
above.
5. Regarding the asymptotically valid Wald and Z-tests
for the value of s2 in the case of nonnormally distributed
populations and the GLM as discussed at the end of
Section
10.6,
justify
their
asymptotic
size
and
consistency.
6. Recall the analysis of the demand for textiles in the
Netherlands performed by renown econometrician Henri
Theil, as discussed in Problem 8.9. With reference to the
least squares-based estimate of the constant elasticity
demand function generated from the data in the problem,
respond to the following, using size .10 tests when testing
is called for:
(a) Test the hypothesis that the matrix of explanatory
variables is signiﬁcant in explaining changes in the
logarithm of textile demand.
(b) Test whether income is signiﬁcant in explaining tex-
tile consumption. Plot the power function of the test,
and interpret its meaning.
(c) Test whether price is signiﬁcant in explaining textile
consumption.
(d) Test whether the price elasticity is inelastic. Test
whether the income elasticity is inelastic.
690
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

(e) Is the demand equation homogeneous degree zero in
price and income? Plot the power function of the test,
and interpret its meaning.
(f) Calculate a conﬁdence interval that has .95 a conﬁ-
dence coefﬁcient for the income elasticity. Interpret
the meaning of this conﬁdence interval.
(g) Calculate a conﬁdence interval that has .95 a conﬁ-
dence coefﬁcient for the price elasticity. Interpret the
meaning of this conﬁdence interval.
(h) Calculate a conﬁdence region having conﬁdence coef-
ﬁcient .90 for the income and price elasticities. With
the aid of a computer, graph the conﬁdence region.
Superimpose the intervals you calculated in (f) and (g)
on this graph. The regions are different–interpret the
difference.
(i) Test
the
least
squares
residuals
for
normality,
preferably using the Shapiro-Wilks test (you’ll need
tables for this), or else use the chisquare goodness of
ﬁt test.
7. Testing The Equality of Two Exponential Population
Distributions. The Reliable Computer Co. is considering
purchasing CPU chips from one of two different suppliers
to use in the production of personal computers. It has two
bids from the suppliers, with supplier number offering the
lower bid. Before making a purchase decision, you want to
test the durability of the chips, and you obtain a random
sample of 50 chips from each supplier. It is known that
both CPUs have operating lives that are exponentially
distributed, and you want to test the equality of the
expected operating lives of the chips.
(a) Deﬁne a size .10 test of the equality of the means of
the two population distributions, i.e., a test, of H0: y1
¼ y2 versus Ha: not H0.
(b) The respective sample means of operating lives for
the two sample were x1 ¼ 24:23 and x2 ¼ 18:23. Con-
duct the test of the null hypothesis. Does the test
outcome help you decide which supplier to purchase
chips from?
(c) How would your test rule change if you wanted to
test a one-sided hypothesis concerning the means of
the population distributions?
(d) Consider using the LM test procedure for this prob-
lem. What is the test rule? Can you perform the test
with the information provided?
(e) Consider using the WALD test procedure for this
problem. What is the test rule? Can you perform the
test with the information provided?
8. The Huntington Chemical Co. has been accused of
dumping improperly treated waste in a local trout lake,
and litigation may be imminent. The central allegation is
that the trout population has been severely reduced by the
company’s dumping practices. The company counters
with the claim that there still remain at least 100,000
trout in the lake, and this being a substantial number for
a lake of its size, they are innocent of all charges. In order to
investigate the size of the trout population, 500 trout are
captured, tagged, and then returned to the lake. After the
ﬁsh have redistributed themselves in the lake, 500 ﬁsh are
captured, and the number of tagged ﬁsh are observed.
Letting x represent the proportion of tagged ﬁsh, can a
size  .10 test of the company’s claim be deﬁned? Would
an outcome of x ¼ .14 support or contradict the company’s
claim?
9. Use a Wald-Wolfowitz runs test to assess whether the
data provided by the cereal manufacturer in Problem 1 is
the outcome of a random sample from some population
distribution.
The
observations
occurred
sequentially
row-wise. Use a size .05 test.
10. Test whether the data of the cereal manufacturer
provided in Problem 1 can be interpreted as an outcome of
a random sample from the speciﬁc normal population dis-
tribution claimed, i.e. testH0: Z ~ N(16.03, .0001) versus Ha:
not H0. Perform another test that assesses the more general
hypothesis that the data is the outcome of a random
sample from some normal population distribution, i.e.
test H0: Z ~ N(m, s2) versus Ha: not H0. Use size .05 tests.
11. Paired Comparisons of Means. A family counseling
service offers a 1-day program of training in meal planning
which they claim is effective in reducing cholesterol
levels of individuals completing the course. In order to
assess the effectiveness of the program, a consumer advo-
cacy group randomly samples the cholesterol levels of 50
program participants both one day before and one month
after the course is taken. They then summarize the pairs
of observations on the individuals by reporting the sample
mean and sample standard deviation of the differences
between the before and after observations of the 50 pro-
gram participants. Their ﬁnding were d ¼ 11.73 and
s ¼ 3.89.
(a) Assuming that the pairs of observations are iid
outcomes from some bivariate normal population
distribution with before and after means mb and ma ,
Problems
691

deﬁne the appropriate likelihood function for the
mean m ¼ ma  mb and variance s2 of the population
distribution of differences in pairs of cholesterol
measurements. Use this likelihood function to deﬁne
a GLR size .05 test of the hypothesis that the meal
planning program has no effect, i.e. a test for H0:
m ¼ 0 versus Ha: not H0. Test the hypothesis.
(b) Can you deﬁne an LM test of the null hypothesis in
(a)? Can you test the hypothesis with the information
available? If so, perform the test–if not, what other
information would you need?
(c) Describe how you might test the hypothesis that the
observations on paired differences are from a normal
population distribution. What information would
you need to test the hypothesis?
(d) Suppose that normality of the observations is not
assumed. Can you deﬁne another test of the effective-
ness of the meal planning program? If so, use the test
to assess the effectiveness of the meal planning pro-
gram. Discuss any differences that are required in the
interpretation of the outcome of this test compared to
the test in (a) that you could perform if the
observations were normally distributed.
12. The production function for a commodity can be
approximated over a restricted range of relevant input
levels by a linear relationship of the form Y ¼ xb + «.
The columns of x include, in order, a column of 1’s, and
three column vectors representing observations on labor,
energy, and capital input levels. Thirty observations on
weekly production levels yielded the following summary
statistics:
b ¼ x0x
ð
Þ1x0y ¼
13:792
3:005
1:327
6:385
2
666664
3
777775
;^s2 ¼ 3:775;
x0x
ð
Þ1 ¼
4:002
:136
:191
:452
:020
:006
:007
:041
:001
ðsymmetricÞ
:099
2
6664
3
7775
In answering the following questions, be sure to clearly
deﬁne any assumptions that you are making.
(a) Test the joint signiﬁcance of the input variables for
explaining the expected level of production using a
size .05 test.
(b) Test the signiﬁcance of the input variables individu-
ally. Which input variables contribute signiﬁcantly
to the explanation of the expected level of output? Be
sure to explain the basis for your conclusion.
(c) Deﬁne conﬁdence interval outcomes for each of the
marginal products of the inputs. Use .95 conﬁdence
coefﬁcients. What do these conﬁdence intervals mean?
(d) Test the hypothesis that expected output is  35
when labor, energy and capital are applied at levels
8, 4, and 3, respectively. Use a size .05 test.
(e) Calculate a conﬁdence interval outcome for the
expected level of production at the input levels
indicated in (d). Use a .95 conﬁdence coefﬁcient.
What is the meaning of this conﬁdence interval?
(f) Test the hypothesis that the variance of the production
process is  6 at a .10 level of signiﬁcance. Assume
normality in conducting this test. What information
would you need to conduct an asymptotically valid
test of the hypothesis in the absence of normality?
13. The production department of a paper products man-
ufacturer is analyzing the frequency of breakdowns in a
certain type of envelope machine as it contemplates
future machinery repair and replacement policy. It has
been suggested that the number of breakdowns per day
is a Poisson distributed random variables, and the depart-
ment intends to test this conjecture. Forty days worth of
observations on the number daily machine breakdowns
yielded the following observations which occurred in
sequence row-wise:
4
8
5
4
3
5
8
2
7
4
6
8
7
9
8
8
6
2
5
0
8
1
7
3
6
2
0
2
0
4
3
5
1
3
9
5
3
4
3
0
(a) Test the hypothesis that the observations are a ran-
dom sample outcome from some population distribu-
tion. Use a .10 level test.
692
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

(b) Test the hypothesis that the observations are from a
Poisson population distribution. Use a size .10 test.
(c) Test the hypothesis that the observations are from a
Uniform population distribution. Use a size .10 test.
(d) Test the hypothesis that the expected number of daily
breakdowns for this equipment is  8 using a signiﬁ-
cance level of .10. Use whatever test procedure you
feel is appropriate. Based on this test procedure, cal-
culate a .90 level conﬁdence interval for the expected
number of daily breakdowns.
14. The relationship between sales and level of advertis-
ing expenditure is hypothesized to be quadratic over the
relevant range of expenditure levels being examined, i.e.,
Yt ¼ b1 + b2 at + b3 at
2 +et , where yt is the level of sales in
period t, at is the level of advertising expenditure in period
t, and the et’s are disturbance terms assumed to be iid
normally distributed. The least squares-based estimate
of the relationship using 30 periods worth of observations
resulted in the following:
b ¼ (103.27 2.71 .13), ^s2 ¼ 1.27
x0x
ð
Þ1 ¼
111:17
1:71
:013
:64
:0017
symmetric
:004
2
4
3
5
(a) Test the hypothesis that advertising expenditure has
a signiﬁcant impact on the expected sales level. Use
any signiﬁcance level you feel is appropriate.
(b) Test whether the relationship between advertising
expenditures and sales is actually a linear as opposed
to a quadratic relationship.
(c) Deﬁne a conﬁdence interval with conﬁdence coefﬁ-
cient .95 for the expected level of sales expressed as a
function of advertising level at. Plot the conﬁdence
interval as function of advertising expenditures
(use expected sales on the vertical axis and advertis-
ing expenditures on the horizontal axis). Interpret the
plot.
(d) Test the hypothesis that the level of advertising
expenditure that maximizes expected sales is  15.
(e) Calculate a conﬁdence interval with conﬁdence coef-
ﬁcient .95 for the level of advertising expenditure
that maximizes expected sales.
15. An analyst is investigating the effect of certain policy
events on common stock prices in a given industry. In an
attempt to isolate abnormal from normal returns of ﬁrms
in the industry, the following returns-generating equation
was estimated via least squares: Rt ¼ b1 + b2 Rmt + et
where Rt is actual return by a ﬁrm on day t and Rmt is
the return on a large portfolio of stocks designed to be
representative of market return on day t. Abnormal
returns is deﬁned to be ARt ¼ et, and is estimated from
an estimate of the returns-generating equation as
d
ARt ¼ Rt  ^Rt;
where ^Rt ¼ b1 þ b2Rmt:
Thus, the estimated abnormal returns are effectively the
estimated residuals from the least squares estimate of the
returns generating equation. A summary of the estima-
tion results from an analysis of 43 observations on a given
ﬁrm is given below:
b ¼ :03 1:07
½
0;
x0x
ð
Þ1 ¼
:0001
:2  104
:2  104
:81
"
#
;
^s2 ¼ 1:0752;
^e ¼
0.44
1.49
0.46
0.90
0.73
0.45
1.04
0.24
0.68
0.40
0.070
2.44
1.21
1.53
0.21
0.30
0.23
0.85
0.55
0.58
0.030
0.16
0.74
0.30
1.52
2.39
0.28
0.66
0.10
0.75
1.59
0.97
2.5
0.62
1.24
0.24
0.14
0.76
0.84
0.16
0.18
0.99
1.74
(a) Use a runs test to provide an indication of whether
the residuals of the returns generating equation, and
hence the abnormal returns, can be viewed as iid
from some population distribution. Use a size .05
test. Comments on the appropriateness of the runs
test in this application.
(b) Test to see whether the abnormal returns can be
interpreted as a random sample from some normal
population distribution of returns. Use a size .05 level
of test.
(c) Assuming normality, test whether the expected daily
return of the ﬁrm is proportional to market return.
Use a size .05 test.
Problems
693

(d) Deﬁne and test a hypothesis that will assess whether
the expected return of the ﬁrm is greater than market
return for any Rmt  0. Use a size .05 test.
16. Testing for Independence in a Bivariate Normal Pop-
ulation Distribution. Let (Xi, Yi), i ¼ 1,. . .,n be a random
sample from some bivariate normal population distribu-
tion N(m, S) where s2
1 ¼ s2
2. In this case, we know that the
bivariate random variables are independent iff the corre-
lation between them is zero. Consider testing the hypoth-
esis of independence H0: r ¼ 0 versus the alternative
hypothesis of dependence Ha: r 6¼ 0.
(a) Deﬁne a size .05 GLR test of the independence of the
two random variables. You may use the limiting dis-
tribution of the GLR statistic to deﬁne the test.
(b) In a sample of 50 observations, it was found that s2
x
¼ 5.37, s2
y ¼ 3.62, and sxy ¼ .98. Is this sufﬁcient to
reject the hypothesis of independence based on the
asymptotically valid test above?
(c) Show that you can transform the GLR test into a
test involving a critical region for the test statistic
w ¼ sxy=
s2
x þ s2
y


=2
h
i
.
(d) Derive the sampling distribution of the test statistic
W deﬁned in c) under H0. Can you deﬁne a size .05
critical region for the test statistic? If so , test the
hypothesis using the exact (as opposed to asymptotic)
size .05 GLR test.
17. Test of the Equality of Proportions. Let X1 and X2 be
independent
random
variables
with
Binomial
distributions Binomial(ni, pi), i ¼ 1,2, where the ni’s are
assumed known. Consider testing the null hypothesis
that the proportions (or probabilities) p1 and p2 are equal,
i.e., the hypothesis is H0: p1 ¼ p2 versus Ha: p1 6¼ p2.
(a) Deﬁne an asymptotically valid size a Wald-type test
of H0 versus Ha.
(b) If n1 ¼ 45, n2 ¼ 34, x1 ¼ 19, and x2 ¼ 24, is the
hypothesis of equality of proportions rejected if
a ¼ .10?
(c) Deﬁne an asymptotically valid size a GLR-type test
of H0 versus Ha. Repeat part (b).
18. Testing Whether Random Sample is From a Discrete
Uniform Population Distribution. Let Xi, i ¼ 1,. . .,n be
random sample from some discrete integer-valued popu-
lation distribution. Let the range of the random variables
be 1,2,. . .,m.
(a) Describe how you would use the w2 goodness of ﬁt
size a test to assess the hypothesis that the popula-
tion distribution was a discrete uniform distribution
with support 1,2,. . .,m.
(b) The Nevada Gaming Commission has been called in
to
investigate
whether
the
infamous
Dewey,
Cheatum, & Howe Casino is using fair dice in its
crap games. One of the alleged crooked dice is tossed
240 times, and the following outcomes were recorded:
X
Frequency
1
33
2
44
3
47
4
39
5
31
6
46
Does this die appear to be fair? What size test will you
use? Why?
19. Describe how you would test one-sided and two-sided
hypotheses, using both ﬁnite sample and asymptotically
valid testing procedures, in each of the following cases.
Use whatever procedures you feel are appropriate.
(a) Testing hypotheses about the mean of a binomial
distribution based on a random sample of size 1
from the binomial distribution.
(b) Testing hypotheses about the mean of a Poisson dis-
tribution based on a random sample of size n from a
Poisson population distribution.
20. The weekly proportion of storage tank capacity that
is utilized at a regional hazardous liquid waste receiving
site is an outcome of a Beta(y,1) random variable. At the
end of each week the storage tank is emptied. A random
sample of a year’s worth of observations of capacity utili-
zation at the site produced the results displayed in the
table below, reported in sequence row-wise.
(a) Deﬁne a GLR size a test of H0: y ¼ y0 versus H0: y 6¼ y0.
(b) Test the hypothesis that the expected weekly storage
tank capacity utilized is equal to .5 at a signiﬁcance
level of .95. If the exact distribution of the GLR test
appears to be intractable, then use an asymptotic test.
(c) Test that the observations can be interpreted as a
random sample from some population distribution.
Use a size .05 test.
694
Chapter 10
Hypothesis Testing Methods and Conﬁdence Regions

(d) Test that the observations can be interpreted as a
random sample from a Beta(y,1) population distribu-
tion. Use a size .05 test.
(e) Would the use of an LM test be tractable here? If so,
perform an LM test of the hypothesis.
0.148
0.501
0.394
0.257
0.759
0.763
0.092
0.155
0.409
0.257
0.586
0.919
0.278
0.076
0.019
0.513
0.123
0.390
0.030
0.082
0.935
0.607
0.075
0.729
0.664
0.802
0.338
0.539
0.055
0.984
0.269
0.069
0.679
0.382
0.549
0.028
0.770
0.296
0.105
0.465
0.194
0.675
0.696
0.068
0.091
0.132
0.156
0.050
0.477
0.754
0.164
0.527
21. Follow the proof of Theorem 10.14 to demonstrate
that when random sampling from the continuous PDF
f(z; Y) with scalar parameter Y,
 2
X
n
i¼1
ln 1  F Xi; Y
ð
Þ
½
  w2
2n
is a pivotal quantity for Y.
22. Regional daily demand for gasoline in the summer
driving months is assumed to be the outcome of a N(m, s2)
random
variable.
Assume
you
have
40
iid
daily
observations on daily demand for gasoline with quantity
demanded measured in millions of gallons, and x ¼ 43
and s2 ¼ 2.
(a) Show that N(m,s2) is a location-scale parameter
family of PDFs (recall Theorem 10.13).
(b) Deﬁne a pivotal quantity for m, and use it to deﬁne a
.95 level conﬁdence interval for m. Also, deﬁne a .95
lower conﬁdence bound for m.
(c) Deﬁne a pivotal quantity for s2, and use it to deﬁne a
.95 level conﬁdence interval for s2.
(d) A colleague claims that mean daily gasoline demand
is only 37 million gallons. Is your answer to (b) con-
sistent with this claim? Explain.
23. The population distribution of income in a populous
developing country is assumed to be given (approximately)
by the continuous PDF f(x; Y) ¼ Y (1+x)(Y+1) I(0,1)(x)
where Y > 0. A summary measure of a random sample
outcome of size 100 from the population distribution is
given by P100
i¼1 ln 1 þ xi
ð
Þ ¼ 40.54, where xi is measured in
1,000’s of units of the developing countries currency.
(a) Deﬁne a pivotal quantity for the parameter Y (Hint:
Theorem 10.13 and Problem 10.21 might be useful
here).
(b) Deﬁne a general expression for a level g conﬁdence
interval for Y based on n iid observations from f(x; Y).
(c) Calculate an outcome of a .90 level conﬁdence inter-
val for Y based on the expression derived in (b). Note:
For large degrees of freedom > 30,
w2
v;a 	 v 1  2
9v þ za
2
9v
	

1=2
"
#3
;
where
Ð 1
za N(x; 0,1) dx ¼ a, is a very good approximation to
critical values of the w2 distribution (M. Abramowitz and
I.A. Stegun, Handbook of Mathematical Functions,
Dover, New York, 1972, p. 941)).
Problems
695

Math Review Appendix: Sets, Functions,
Permutations, Combinations, Notation,
and Real Analysis
A1.
Introduction
In this appendix we review basic results concerning set theory, relations and
functions, combinations and permutations, summation and integration nota-
tion, and some fundamental concepts in real analysis. We also review the mean-
ing of the terms deﬁnition, axiom, theorem, corollary, and lemma, which are
labels that are afﬁxed to a myriad of statements and results that constitute the
theory of probability and mathematical statistics. The topics reviewed in this
appendix constitute basic foundational material on which the study of mathe-
matical statistics is based. Additional mathematical results, often of a more
advanced nature, will be introduced throughout the text as the need arises.
A2.
Deﬁnitions, Axioms, Theorems, Corollaries, and Lemmas
The development of the theory of probability and mathematical statistics
involves a considerable number of statements consisting of deﬁnitions, axioms,
theorems, corollaries, and lemmas. These terms will be used for organizing the
various statements and results we will examine into these categories:
1. Descriptions of meaning;
2. Statements that are acceptable as true without proof;
3. Formulas or statements that require proof of validity;
4. Formulas or statements whose validity follows immediately from other true
formulas or statements; and
5. Results, generally from other branches of mathematics, whose primary
purpose is to facilitate the proof of validity of formulas or statements in
mathematical statistics.
More formally, we present the following meaning of the terms.
Deﬁnition: A statement of the meaning of a word, word group, sign, or symbol.
Axiom (or postulate): A statement that has found general acceptance, or is
thought to be worthy thereof, on the basis of an appeal to intrinsic merit or self-
evidence, and thus requires no proof of validity.
Theorem (or proposition): A formula or statement that is deduced from other
proved or accepted formulas or statements, and whose validity is thereby proved.
Corollary: A formula or statement that is immediately deducible from a
proven theorem, and that requires little or no additional proof of validity.

Lemma: A proven auxiliary proposition stated for the expressed purpose of
facilitating the proof of another proposition of more fundamental interest.
Thus, in the development of the theory of probability and mathematical
statistics, axioms are the fundamental truths that are to be accepted at face value
and not proven. Theorems and their corollaries are statements deducible from
the fundamental truths and other proven statements and thus are derived
truths. Lemmas represent proven results, often from ﬁelds outside of statistics
per se, that are used in the proofs of other results of more primary interest.
We elaborate on the concept of a lemma, since our discussions will implicitly
rely on lemmas more than any other type of statement, but we will generally
choose not to exhaustively catalogue lemmas in the discussions. What constitutes
a lemma and what does not depends on the problem context or one’s point of view.
A fundamental integration result from calculus could technically be referred to as
a lemma when used in a proof of a statement in mathematical statistics, while in
the study of calculus, it might be referred to as a theorem to be proved in and of
itself. Since our study will require numerous auxiliary results from algebra, calcu-
lus, and matrix theory, exhaustively cataloging these results as lemmas would be
cumbersome, and more importantly, not necessary given the prerequisites
assumed for this course of study, namely, a familiarity with the basic concepts of
algebra, univariate and multivariate calculus, and an introduction to matrix the-
ory. We will have occasion to state a number of lemmas, but we will generally
reserve this label for more exotic mathematical results that fall outside the realm
of mathematics encompassed by the prerequisites.
A3.
Elements of Set Theory
In the study of probability and mathematical statistics, sets are the fundamental
objects to which probability will be assigned, and it is important that the
concept of a set, and operations on sets, be well understood. In this section we
review some basic properties of and operations on sets. This begs the following
question: What is meant by the term set? In modern axiomatic developments of
set theory, the concept of a set is taken to be primitive and incapable of being
deﬁned in terms of more basic ideas. For our purposes, a more intuitive notion of
a set will sufﬁce, and we avoid the complexity of an axiomatic development of
the theory (see Marsden,1 Appendix A, for a brief introduction to the axiomatic
development). We base our deﬁnition of a set on the intuitive deﬁnition origi-
nally proposed by the founder of set theory, Georg Cantor (1845–1918).2
1J.E. Marsden, (1974), Elementary Classical Analysis, San Francisco: Freeman and Co.
2Cantor’s original text reads: “Unter einer ‘Menge’ verstehen wir jede Zusammenfassung M von bestimmten wohlunterschiedenen
Objekten m unserer Anschauung oder unseres Denkens (welche die ‘Elemente’ von M genannt werden) zu einem ganzen,” (Collected
Papers, p. 282). Our translation of Cantor’s deﬁnition is, “By set we mean any collection, M, of clearly deﬁned, distinguishable objects,
m, (which will be called elements of M) which from our perspective or through our reasoning we understand to be a whole.”
698
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Deﬁnition A.1
Set
A set is a collection of objects with the following characteristics:
1. All objects in the collection are clearly deﬁned, so that it is evident which
objects are members of the collection and which are not;
2. All objects are distinguishable, so that objects in the collection do not
appear more than once;
3. Order is irrelevant regarding the listing of objects in the collection, so two
collections that contain the same objects but are listed in different order
are nonetheless the same set; and
4. Objects in the collection can be sets themselves, so that a set of sets can
be deﬁned.
The objects in the collection of objects comprising a set are its elements. The
term members is also used to refer to the objects in the collection. In order to
signify that an object belongs to a given set, the symbol ∈, will be used in an
expression such as x∈A, which is to be read “x is an element (or member) of the
set A.” If an object is not a member of a given set, then a slash will be used asx=2A
to denote that “x is not an element (or member) of the set A.” Note that the slash
symbol, /, is used to indicate negation of a relationship. The characteristics of
sets presented in Deﬁnition A.1 will be clariﬁed and elaborated upon in
examples and discussions provided in subsequent subsections.
Set Deﬁning Methods
Three basic methods are used in deﬁning the objects in the collection
constituting a given set: (1) exhaustive listing; (2) verbal rule; and (3) mathemat-
ical rule. An exhaustive listing requires that each and every object in a collection
be individually identiﬁed either numerically, if the set is a collection of num-
bers, or by an explicit verbal description, if the collection is not of numbers. The
object descriptions are conventionally separated by commas, and the entire
group of descriptions is enclosed in brackets. The following are examples of
sets deﬁned by an exhaustive listing of the objects that are elements of the set:
Example A.1
S1 ¼ {HEAD, TAIL} Here S1 is the set of possible occurrences when tossing a
coin into the air and observing its resting position. Note the set can be equiva-
lently represented as S1 ¼ {TAIL, HEAD}.
□
Example A.2
S2 ¼ {1,2,3,4,5,6} Here S2 is the set of positive integers from 1 to 6. Note that the
set S2 can be equivalently represented by the listing of the positive integers 1 to 6
in any order.
□
Averbal rule is a verbal statement of characteristics that only the objects that
are elements of a given set possess and that can be used as a test to determine set
membership. The general form of the verbal rule is {x: verbal statement}, which is
A3.
Elements of Set Theory
699

to be read “the collection of all x for which verbal statement is true.” The
following are examples of sets described by verbal rules:
Example A.3
S3 ¼ {x: x is a college student} Here S3 is the set of college students. An individual
is an element of the set S3 iff (if and only if) he or she is a college student.
□
Example A.4
S4 ¼ {x: x is a positive integer} Here S4 is the set of positive integers 1,2,3, . . . . A
number is an element of the set S4 iff it is a positive integer.
□
A mathematical rule is of the same general form as a verbal rule, except the
verbal statement is replaced by a mathematical expression of some type. The
general form of the mathematical rule is {x: mathematical expression}, which is
to be read “the collection of all x for which mathematical expression is true.”
The following are examples of sets described by mathematical rules:
Example A.5
S5 ¼ {x: x ¼ 2k + 1, k ¼ 0,1,2,3,. . .} Here S5 is the set of odd positive integers. A
number is an element of the set S5 iff the number is equal to 2k + 1 for some
choice of k ¼ 0,1,2,3,. . . .
□
Example A.6
S6 ¼ {x: 0  x  1} Here S6 is the set of numbers greater than or equal to 0 but
less than or equal to 1. A number is an element of the set S6 iff it is neither less
than 0 nor greater than 1.
□
The choice of method for describing the objects that constitute elements of a
set depends on what is convenient and/or feasible for the case at hand. For
example, exhaustive listing of the elements in set S6 is impossible. On the
other hand, there is some discretion that can be exercised, since, for example,
a verbal rule could have adequately described the set S5, say as S5 ¼ {x: x is an
odd positive integer}. A mixing of the basic methods might also be used, such as
S5 ¼ {x: x ¼ 2k + 1, k is zero or a positive integer}. One can choose whatever
method appears most useful in a given problem context.
Note that although our preceding examples of verbal and mathematical
rules treat x as inherently one-dimensional, a vector interpretation of x is clearly
permissible. For example, we can represent the set of points on the boundary or
interior of a circle centered at (0,0) and having radius 1 as
S7 ¼
x1; x2
ð
Þ : x2
1 þ x2
2  1


or we can represent the set of input–output combinations associated with a two-
input Cobb-Douglas production function as
S8 ¼
y; x1; x2
ð
Þ : y ¼ b0 xb1
1 xb2
2 ; x1  0; x2  0
n
o
for given numerical values of b0, b1, and b2. Of course, the entries in the x-vector
need not be numbers, as in the set
S9 ¼
x1; x2
ð
Þ : x1 is an economist; x2 is an accountantg:
f
700
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Set Classiﬁcations
Sets are classiﬁed according to the number of elements they contain, and
whether the elements are countable. We differentiate between sets that have a
ﬁnite number of elements and sets whose elements are inﬁnite in number,
referring to a set of the former type as a ﬁnite set and a set of the latter type as
an inﬁnite set. In terms of countability, sets are classiﬁed as being either count-
able or uncountable. Note that when we count objects, we intuitively place the
objects in a one-to-one correspondence with the positive integers, i.e., we iden-
tify objects one by one, and count “1,2,3,4,. . . .” Thus, a countable set is one
whose elements can be placed in a one-to-one correspondence with some or all
of the positive integers-any other set is referred to as an uncountable set.
A ﬁnite set is, of course, always countable, and thus it would be redundant to
use the term “countable ﬁnite set.” Sets are thus either ﬁnite, countably inﬁ-
nite, or uncountably inﬁnite. Of the sets, S1 through S9 described earlier, S1, S2,
S3, and S9 are ﬁnite, S4 and S5 are countably inﬁnite, and S6, S7, and S8 are
uncountably inﬁnite (why?).
Special Sets, Set Operations, and Set Relationships
We now proceed to a number of deﬁnitions and illustrations establishing
relationships between sets, mathematical operations on sets, and the notions
of the universal and empty sets.
Deﬁnition A.2
Subset
A is a subset of B, denoted as A  B and read A is contained in B, iff every
element of A is also an element of B.
Deﬁnition A.3
Equality of Sets
Set A is equal to set B, denoted as A ¼ B, iff every element of A is also an
element of B, and every element of B is also an element of A, i.e., iff A  B and
B  A.
Deﬁnition A.4
Universal Set
The set containing all objects under consideration in a given problem setting,
and from which all subsets are extracted, is the universal set.
Deﬁnition A.5
Empty or Null Set
The set containing no elements, denoted by ;, is called the empty, or null set.
Deﬁnition A.6
Set Difference
Given any two subsets, A and B, of a universal set, the set of all elements in A
that are not in B is called the set difference between A and B, and is denoted by
AB. If A  B, then AB ¼ ;.
A3.
Elements of Set Theory
701

Deﬁnition A.7
Complement
Let A be a subset of a universal set, O. The complement of the set A is
the set of all elements in O that are not in A, and is denoted by A. Equivalently,
A ¼ O  A.
Deﬁnition A.8
Union
Let A and B be any two subsets of a universal set, O. Then, the union of the
sets A and B is the set of all elements in O that are in at least one of the sets A
or B, it is denoted by A[B.
Deﬁnition A.9
Intersection
Let A and B be any two subsets of a speciﬁed universal set, O. Then the
intersection of the sets A and B is the set of all elements in O that are in
both sets A and B, and is denoted by A\B.
Deﬁnition A.10
Mutually Exclusive
(or Disjoint) Sets
Subsets A and B of a universal set, O, are said to be mutually exclusive or
disjoint sets iff they have no elements in common, i.e., iff A\B ¼ ;.
We continue to use the slash, /, to indicate negation of a relationship (recall
that / was previously used to indicate the negation of 2 ). Thus, A 6 B denotes
that A is not a subset of B, and A 6¼ B denotes that A is not equal to B. We note
here (and we shall state later as a theorem) that it is a logical requirement that;is
a subset of any set A, since if ; does not contain any elements, it cannot be the
case that ; 6 A, since the negation of  would require the existence of an
element in ; that was not in A.
Example A.7
Let the universal set be deﬁned as O ¼ {x: 0  x  1} and deﬁne three additional
sets as
A ¼ fx : 0  x  :5g; B ¼ fx : :25  x  :75g and C ¼ fx : :75 <x  1g:
Then we can establish the following set relationships:
B ¼ x : 0  x<:25 or :75<x  1
f
g;
A [ C ¼ x : 0  x  :5 or :75<x  1
f
g;
B  A [ C;
C \ A ¼ C \ B ¼ ;;
C ¼ A [ B ¼ x : 0  x  :75
f
g;
A  B ¼ x : 0  x<:25
f
g;
A \ B ¼ x : :25  x  :5
f
g:
□
702
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Note that although our deﬁnitions of subset, equality of sets, set difference,
complement, union, and intersection explicitly involve only two sets A and B, it
is implicit that the concepts can be applied to more complicated expressions
involving an arbitrary number of sets. For example, since A\B is itself a set, we
can form its intersection with a set C as (A\B)\C, or form the set difference,
(A\B)C, or establish that (A\B) ¼ or 6¼ C, and so on. The point is that the
concepts apply to sets, which themselves may have been constructed from other
sets via various set operations.
Example A.8
Let O, A, B, and C be deﬁned as in Example A.7. Then the following set
relationships can be established:
A [ B [ C ¼ O ¼ x : 0  x  1
f
g;
A [ C
ð
Þ \ B ¼ x : :25  x  :5
f
g;
A \ B
ð
Þ \ C ¼ A [ B


\ C ¼ ;;
B [ C
ð
Þ  A ¼ x : :5<x  1
f
g;
A [ B
ð
Þ  A \ B
ð
Þ
ð
Þ  C \
A [ B


:
Can  be replaced with ¼ in the last relationship?
□
It
is
sometimes
useful
to
conceptualize
set
relationships
through
illustrations called Venn diagrams (named after the 19th-century English logi-
cian, John Venn). In a Venn diagram, the universal set is generally denoted by a
rectangle, with subsets of the universal set represented by various geometric
shapes located within the bounds of the rectangle. Figure A.1 uses Venn
diagrams to illustrate the set relationships deﬁned previously.
Rules Governing Set Operations
Operations on sets must satisfy a number of basic rules. We state these basic
rules as theorems, although we will not take the time to prove them here. The
reader may wish to verify the plausibility of some of the theorems through the
use of Venn diagrams. One of DeMorgan’s laws will be proved to illustrate the
formal proof method for the interested reader.
Theorem A.1
Idempotency Laws A[A ¼ A and A\A ¼ A
Theorem A.2
Commutative Laws A[B ¼ B[A and A\B ¼ B\A
Theorem A.3
Associative Laws (A[B)[C ¼ A[(B[C) and (A\B)\C ¼ A\(B\C)
Theorem A.4
Distributive Laws A\(B[C) ¼ (A\B)[(A\C) and A[(B\C) ¼ (A[B)\(A[C)
Theorem A.5
Identity Elements for \ and [ A\O ¼ A (O is the identity element for \) A[;
¼ A (; is the identity element for [)
A3.
Elements of Set Theory
703

Theorem A.6
Intersection and Union of Complements A [ A ¼ O and A \ A ¼ ;
Theorem A.7
Complements of Complements
A
 
¼ A
Theorem A.8
Intersection with the Null Set A\; ¼ ;
Theorem A.9
Null Set as a Subset If A is any set, then ;  A
Theorem A.10
DeMorgan’s Laws ðA [ BÞ ¼ A \ B and ðA \ BÞ ¼ A [ B
Example A.9
Formal Proof of ðA \ BÞ ¼ A [ B. By deﬁnition of the equality of sets, two sets are
equal iff each is contained in the other. We ﬁrst demonstrate that A \ B  A [ B.
By deﬁnition, x ∈A \ B implies that x =2 A\B. Suppose x=2A [ B. This implies
B
A
Shaded Area = A
A
B
A⊂B
A
and
B
A=B
A
B
Shaded Area =A - B
A
B
Shaded Area =A∪B
A
A
B
Shaded Area =A∩B
A∩B = Ø
A and B  mutually exclusive
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Figure A.1
Venn diagrams illustrating
set relationships.
704
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

x=2A and x=2B, which implies x ∈A and x ∈B, i.e., x 2 A \ B, a contradiction.
Therefore, if x ∈A \ B, then x ∈A[B, which implies A \ B  A [ B. We next
demonstrate that A [ B  A \ B. Let x ∈A[B. Then x =2 A\B, for if it were, then
x ∈A and x ∈B, contradicting that x belongs to at least one of
A and B .
However, x =2 A\B implies x ∈A \ B, and thus A [ B  A \ B.
□
We remind the reader that since the sets used in Theorems A.1–A.10 could
themselves be the result of set operations applied to other sets, the theorems are
extendable in a myriad of ways to involve an arbitrary number of sets. For
example, in the ﬁrst of DeMorgan’s laws listed as Theorem A.10, if A ¼ C[D
and B ¼ E[F, then by substitution,
C [ D [ E [ F


¼ C [ D


\ E [ F


:
Then by applying Theorem A.10 to both (C[D) and (E[F), we obtain a generali-
zation of DeMorgan’s law as
C [ D [ E [ F


¼ C \ D \ E \ F:
Given the wide range of extensions that are possible, Theorems A.1–A.10 pro-
vide a surprisingly broad conceptual foundation for applying the rules governing
set operations.
Some Useful Set Notation
Situations sometimes arise in which one is required to denote the union or
intersection of a large number of sets. A convenient notation that represents
such unions or intersections quite efﬁciently is available. Two types of notations
are generally used, and they are differentiated on the basis of whether the union
or intersection is of sets identiﬁed by a natural sequence of integer subscripts or
whether the sets are identiﬁed by subscripts, say i’s, that are elements of some
set of subscripts, I, called an index set.
Deﬁnition A.11
Multiple Union
Notation
a.
[n
i¼1 Ai ¼ A1 [ A2 [ A3 . . . [ An:
b. [i2IAi ¼ union of all sets Ai for which i ∈I.
Deﬁnition A.12
Multiple Intersection
Notation
a.
\n
i¼1 Ai ¼ A1 \ A2 \ A3 . . . An:
b. \i2IAi ¼ intersection of all sets Ai for which i ∈I.
Example A.10
Let the universal set be deﬁned as O ¼ {x: 0  x  1}, and examine the following
subsets of O:
A1 ¼ fx : 0  x  :25g; A2 ¼ fx : 0  x  :5g;
A3.
Elements of Set Theory
705

A3 ¼ fx : 0  x  :75g; A4 ¼ fx : :75  x  1g:
Deﬁne the index sets I1 and I2 as
I1 ¼ 1; 3
f
g; and I2 ¼ 1; 3; 4
f
g:
Then,
[4
i¼1Ai ¼ [4
i¼2Ai ¼ [4
i¼3Ai ¼ x : 0  x  1
f
g ¼ O;
[i2I1Ai ¼ A1 [ A3 ¼ x : 0  x  :75
f
g;
[i2I2Ai ¼ A1 [ A3 [ A4 ¼ x : 0  x  1
f
g ¼ O;
\4
i¼1Ai ¼ \4
i¼2Ai ¼ ;;
\4
i¼3Ai ¼ :75
f
g;
\i2I1Ai ¼ A1 \ A3 ¼ x : 0  x  :25
f
g;
\i2I2Ai ¼ A1 \ A3 \ A4 ¼ ;:
□
Whenever a set A is an interval subset of the real line (where the real line
refers to all of the numbers between 1 and 1), the set can be indicated in
abbreviated form by the standard notation for intervals, stated in the following
deﬁnition.
Deﬁnition A.13
Interval Set Notation
Let a and b be two numbers on the real line for which a < b. Then the
following four sets, called intervals with endpoints a and b, can be deﬁned as:
(a) Closed interval:
a; b
½
 ¼ fx : a  x  bg;
(b) Half-open (or half-closed) intervals:
a; b
ð
 ¼ fx : a<x  bg; and
a; b
½
Þ ¼ fx : a  x<bg;
(c) Open interval:
a; b
ð
Þ ¼ x : a<x<b
f
g:
Note that weak inequalities, x  or  x, are signiﬁed by brackets ] or
[, respectively. Strong inequalities, x < or < x, are signiﬁed by parentheses, )
or (, respectively. Note further that whether the interval set contains its
endpoints determines whether the set is closed.
As we have already done, (x,y) will also be used to denote coordinates in the
two-dimensional plane. The context of the discussion will make clear whether
we are referring to an open interval (a,b) or pair of coordinates (x,y).
706
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

A4.
Relations, Point Functions, Set Functions
The concepts of point function and set function are central to a discussion of
probability and statistics. We will see that probabilities can be represented by set
functions and that in a large number of cases of practical interest, set functions
can in turn be represented by a summation or integration operation applied to
point functions. While readers may be somewhat familiar with point functions
from introductory courses in algebra, the concept of a set function may not be
familiar. We will review both function concepts within the broader context of
the theory of relations. The relations context facilitates the presentation of a
very general deﬁnition of “function” in which inputs into and outputs from the
function may be objects of any kind including, but not limited to, numbers. The
relations context also facilitates a demonstration of the signiﬁcant similarities
between the concept of a set function and the more familiar point function
concept.
Cartesian Product
The concept of a relation can be made clear once we deﬁne what is meant by the
Cartesian product of two sets A and B, named after the French mathematician
Rene Descartes, (1596–1650).
Deﬁnition A.14
Cartesian Product of A
and B
Let A and B be two sets. Then the Cartesian product of A and B, denoted as
A  B, is the set of ordered pairs A  B ¼ {(x,y): x ∈A, y ∈B}.
In words, A  B is the set of all possible pairs (x,y) such that x is an element
of the set A and y is an element of the set B. Note carefully that the pairs are
ordered in the sense that the ﬁrst object in the pair must come from set A and
the second object from set B.
Example A.11
Let A ¼ {x: 1  x  2} and B ¼ {y: 2  y  4}. Then A  B ¼ {(x,y): 1  x  2
and 2  y  4}. (see Figure A.2).
□
Example A.12
Let A ¼ {x: x is a man} and B ¼ {y: y is a woman}. Then A  B ¼ {(x,y): x is a man
and y is a woman}, which is the set of all possible man-woman pairings.
□
In later chapters of the book we will have use for a more general notion of
Cartesian product involving more than just two sets. The extension is given in
the deﬁnition below.
Deﬁnition A.15
Cartesian Product
(General)
Let A1,. . .,An be n sets. Then the Cartesian product of A1,. . .,An is the set of
ordered n-tuples
n
i¼1 Ai ¼ A1  A2  . . .  An ¼
x1; . . . ; xn
ð
Þ : xi 2 Ai; i ¼ 1; . . . ; n
f
g:
A4.
Relations, Point Functions, Set Functions
707

In words, n
i¼1 Ai is the set of all possible n-tuples (x1,. . .,xn) such that x1 is
an element of set A1, x2 is an element of set A2, and so on. Note that should the
need arise, a general Cartesian product of sets could also be represented by
the notation i2I Ai, where here the product is taken over all sets having
subscript i in the index set I (recall Deﬁnitions A.11 and A.12, and the use of
index set notation).
In certain cases, we may be interested in forming a Cartesian product of a set
A with itself. While we might represent such a Cartesian product by the notation
n
i¼1 A ¼ {(x1,. . .,xn): xi ∈A, i ¼ 1,. . .,n}, such a Cartesian product is generally
denoted by the notation An, and so for example, A2 ¼ A  A.
Relation (Binary)
We now deﬁne what we mean by the term “binary relation.”
Deﬁnition A.16
Binary Relation
Any subset of the Cartesian product A  B is a binary relation from A to B.
Note that the adjective binary signiﬁes that only two sets are involved in the
relation. Relations that involve more than two sets can be deﬁned, but for our
purposes the concept of a binary relation will sufﬁce.3 Henceforth, we will use
the word relation to mean binary relation. We should also mention that in the
y
4
2
0
1
2
x
A×B = Shaded Area
A
B
Figure A.2
A  B ¼ shaded area.
3A higher order relation could be deﬁned by taking a subset of the Cartesian product n
i¼1 Ai, for example.
708
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

case where B ¼ A, we will simply remain consistent with Deﬁnition A.16 and
refer to a subset of A  A as a relation from A to A, although in this special case
some authors prefer to call the subset of A  A a relation on A.
Now let S  A  B. Thus by deﬁnition, S is a relation from A to B. We
emphasize at this point that the choice of the letter S is quite arbitrary, and we
could just as well have chosen any other letter to represent a subset of A  B
deﬁning a relation from A to B. If (x,y) ∈S, we say that x is in the relation S to y
or that x is S-related to y. An alternative notation for (x,y) ∈S is xSy. Also, we
use S: A ! B as an abbreviation for “the relation S from A to B.”
As it now stands, the concept of a relation no doubt appears quite abstract.
However, in practice, it is the context provided by the deﬁnition of the subset S
and the deﬁnitions of the sets A and B that provide intuitive meaning to xSy.
That is, x will be S-related to y because of some property satisﬁed by the (x,y)
pair, the property being indicated in the set deﬁnition of S. The real-world
objects being related will be clearly identiﬁed in the set deﬁnitions of A and B.
A few examples will clarify the intuitive side of the relation concept.
Example A.13
Let A ¼ [0,1), and form the Cartesian product A2 ¼ {(x,y): x ∈A and y ∈A}.
The set A2 can thus be interpreted as the nonnegative (or ﬁrst) quadrant of the
Euclidean plane. Then S ¼ {(x,y): x  y, (x,y) ∈A2} is a relation from A to A
representing the set of points in the nonnegative quadrant for which the ﬁrst
coordinate has a value greater than or equal to the value of the second coordi-
nate. The deﬁning property of the relation S is “.” This is displayed in
Figure A.3.
□
Example A.14
Let A ¼ {x: x is an employed U.S. citizen} and B ¼ {y: y is a U.S. corporation}.
Then A  B ¼ {(x,y):x is an employed U.S. citizen and y is a U.S. corporation} is
the set of all possible pairings of employed U.S. citizens with U.S. corporations.
The relation S ¼ {(x,y): x is employed by y, (x,y) ∈A  B} from A to B is the
collection of U.S. citizens who are employed by U.S. corporations paired with
their respective corporate afﬁliation. The deﬁning property of the relation is the
phrase “is employed by,” and xSy iff x is a U.S. citizen employed by a U.S.
corporation, and y is his or her corporate afﬁliation.
□
Function
We are now in a position to deﬁne what is meant by the concept of a function. As
indicated in the following deﬁnition, a function is simply a special type of
relation. In the deﬁnition we introduce the symbol 8, which stands for every
or for all, the symbol ∃which means there exists.
Deﬁnition A.17
Function
A function from A to B is a relation S: A ! B such that 8a ∈A ∃one unique
b ∈B such that (a,b) ∈S.
A4.
Relations, Point Functions, Set Functions
709

A relation satisfying the above condition will often be given a special symbol
to distinguish the relation as a function. A popular symbol used for this purpose
is “f,” where f: A ! B is a common notation for designating the function f from
A to B. As we had remarked when choosing a symbol to represent a relation, the
choice of the letter f is arbitrary, and when it is convenient or useful, any other
letter or symbol could be used to depict a subset of A  B that represents a
function from A to B. In the text we will often have occasion to use a variety of
letters to designate various functions of interest.
The unique element b ∈B that the function f: A ! B associates with a
given element x ∈A is called the image of x under f and is represented symboli-
cally by the notation f(x). If f(x) is a real number, the image of x under f is
alternatively referred to as the value of the function f at x. In the following
example, we use R to denote the set of real numbers (1, 1), i.e., R stands for
the real line. Furthermore, the nonnegative subset of the real line is represented
by R0, ¼ [0,1).
Example A.15
Let f: R ! R0 be deﬁned by f ¼ {(x,y): y ¼ x2, x ∈R}. The image of 2 under f is
f(2) ¼ 4. The value of the function f at 3 is f(3) ¼ 9.
□
Associated with a given function, f, are two important sets called the domain
and range of the function.
Deﬁnition A.18
Domain and Range
of a Function
The domain of a function f: A ! B is deﬁned as D(f) ¼ A. The range of f is
deﬁned by R(f) ¼ {y: y ¼ f(x), x ∈A}.
y
x
xSy in shaded area
Figure A.3
xSy in shaded area.
710
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Thus, the domain of a function f: A ! B is simply the set A of the Cartesian
product A  B associated with the function. The range of f is the collection of all
elements in B that are images of the elements in A under the function f. It
follows that R(f)  B. Figure A.4 provides a pictorial example of a function,
including its domain and range.
Note that the concept of a function is completely general regarding the
nature of the elements of the sets D(f) ¼ A, R(f), and B. The elements can be
numbers, or other objects, or the elements can be sets themselves. For our work,
it will sufﬁce to deal only with real-valued functions meaning that R(f) is a set of
real numbers.
Deﬁnition A.19
Real-Valued Function
A function f: A ! B such that R( f)  R is called a real-valued function.
The function deﬁned in Example A.15 is a real-valued function. The follow-
ing is another example.
Example A.16
Examine the Cobb-Douglas production function f:R2
 0 ! R  0 deﬁned as f ¼
x1; x2
ð
Þ; y
ð
Þ : y ¼ 10x2
1x2; x1; x2
ð
Þ 2 R2
 0


. Interpreting (x1,x2) as inputs into a
production process, and y as the output of the process, we see that the domain
of the production function is d(f) ¼ R2
 0, i.e., any nonnegative level of the input
pair (x1,x2) is an admissible input level. The associated range of the production
function is R(f) ¼ R0, i.e., any nonnegative level of output, y, is possible. Since
R(f) ¼ [0,1)  R, the production function is a real-valued function.
□
In some cases the relation from A to B that deﬁnes the function f: A ! B also
deﬁnes a function from B to A. This relates to the concept of an inverse function.
In particular, if for each element y ∈B there exists precisely one element x ∈A
whose image under f is y, then such an inverse function exists.
B
R(f)
A×B
D(f) = A
Figure A.4
Function with domain D(f )
and range R(f ).
A4.
Relations, Point Functions, Set Functions
711

Deﬁnition A.20
Inverse Function of f
Let f: A ! B be a function from A to B. If R(f) ¼ B and 8y ∈B ∃a unique
x ∈A such that y ¼ f(x), then the relation {(y,x): y ¼ f(x), y ∈B} is a function
from B to A called the inverse function of f and denoted by f1: B ! A.
Note that neither of the functions in Example A.15 or Example A.16 are such
that an inverse function exists. In Example A.15, the uniqueness condition of
Deﬁnition A.20 is violated since 8y 6¼ 0 there exist two values of x for which
y ¼ x2, namely x ¼ 
ﬃﬃﬃy
p . For example, when y ¼ 4, x ¼ 2 and 2 are each such
that y ¼ x2. The reader can verify that Example A.16 also violates the uniqueness
condition where an inﬁnite number of (x1, x2) values satisfyy ¼ 10x2
1x2 for a ﬁxed
value of y (deﬁning level sets or isoquants of the production function). Also, note
that an inverse function does not exist for the function illustrated in Figure A.4.
As an example of a function for which an inverse function does exist,
consider the following.
Example A.17
Let f: R ! R+ be deﬁned as
f ¼ f x; y
ð
Þ : y ¼ ex; x 2 Rg; where Rþ ¼ ð0; 1Þ:
Note that the inverse function can be represented as
f1 ¼ f y; x
ð
Þ : x ¼ lnðyÞ; y 2 Rþg
so that f1:R+ ! R. It is clear that 8y ∈R+, ∃one and only one x ∈R such that
x ¼ ln(y).
□
The ﬁnal concept concerning functions that we will review here is the
inverse image of y ∈R(f) or of H  R(f). The inverse image of y is the set of
domain elements x ∈D(f) such that y ¼ f(x), i.e., the collection of all x values in
the domain of f whose image under the function f is y. The inverse image of y can
be represented as the set {x: f(x) ¼ y}, and when the inverse function exists, the
inverse image of y can be represented as the value f1 (y). In Example A.17 above,
the inverse image of 5 is f1 (5) ¼ ln(5) ¼ 1.6094, in Example A.15, the inverse
image of 4 is {2,2}, and in Example A.16, the inverse image of 3 is the isoquant
x1; x2
ð
Þ : 3 ¼ 10x2
1x2; x1; x2
ð
Þ 2 R2
 0


. Similarly, the inverse image of H  R(f) is
the set of x values in the domain of f whose images under f equal some y ∈H,
i.e., the inverse image of H is {x: f(x) ¼ y, y ∈H}, and if the inverse function
exists, {x: x ¼ f1(y), y ∈H}.
Real-Valued Point Versus Set Functions
Two types of functions – point functions and set functions – are utilized exten-
sively in modern discussions of probability and mathematical statistics. The
reader should already have considerable experience with the application of real-
valued point functions, since this type of function is the one that appears in
elementary algebra and calculus courses and is central to discussions of utility,
demand, production, and supply that the reader has encountered in his or
her study of economic theory. Speciﬁcally, a real-valued point function is a
712
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

real-valued function whose domain consists of a collection of points, where
points are represented by coordinate vectors in ℝn. We have encountered
examples of this type of function previously in Example A.15 through Example
A.17. A typical ordered pair associated with a real-valued point function is of the
form (x,y), where x is a vector in ℝn and y is a real number in R.
A set function is more general than a point function in that its domain
consists of a collection of sets rather than a collection of points.4 A typical
ordered pair belonging to a real-valued set function would have the form (A,y),
where A is a set of some type of objects and y is a real number in R. If the sets in
the domain of the set function are contained in ℝn, i.e., they are sets of real
numbers, then a real-valued set function is assigns a real number to each set of
points in its domain in contrast to a real-valued point function which would
assign a real number to each point in its domain. A pictorial illustration of a set
function contrasted with a point function is given in Figure A.5.
Examples of set functions are presented below.
Example A.18
Let O ¼ {1,2,3}, and let A be the collection of all of the subsets of O, i.e.,A ¼
A1; A2; . . . ; A8
f
g, where
A1 ¼ 1
f g; A2 ¼ 2
f g; A3 ¼ 3
f g; A4 ¼ 1; 2
f
g; A5 ¼ 1; 3
f
g; A6 ¼ 2; 3
f
g;
A7 ¼ 1; 2; 3
f
g; and A8 ¼ ;
The following is a real-valued set function f: A ! R:
f ¼
Ai; y
ð
Þ : y ¼
X
x2Ai
x; Ai  A
(
)
;
where P
x2Ai x signiﬁes the sum of the numerical values of all of the elements in
the set Ai, and P
x2; x is deﬁned to be zero. The range of the set function is
(point function)
(set function)
D(f)
R(f)
•
•
•
•
•
•
•
•
•
•
Figure A.5
Point versus set function.
4Note that, in a sense, a point function can be viewed as a special case of a set function, since points can be interpreted as singleton
(single element) sets. The set function concept is introduced to accommodate the case where one or more sets in its domain are not
singleton.
A4.
Relations, Point Functions, Set Functions
713

R(f) ¼ {0,1,2,3,4,5,6} and the domain is the set of sets D(f) ¼ A. The function can
be represented in tabular form as
Ai
f(Ai)
A1
1
A2
2
A3
3
A4
3
A5
4
A6
5
A7
6
A8
0
□
Example A.19
Let A ¼ {Ar:Ar ¼ {(x,y): x2 + y2  r2}, r ∈[0,1]}, so that A is a set of sets, with
typical element Ar represents the set of points in R2 that are on the boundary and
in the interior of a circle centered at (0,0) with radius r. The following is a real
valued set function f: A ! R:
f ¼ f Ar; y
ð
Þ : y ¼ pr2; Ar  Ag:
Note the set function assigns a real number representing the area to each set, Ar.
The assignment is made for circles having a radius anywhere from 0 to 1.
The range of the set function is given by R(f) ¼ [0,p], and the domain is the set
of sets D(f) ¼ A.
□
A special type of set function called the size-of-set function will prove to be
quite useful.
Deﬁnition A.21
Size of Set Function
Let A be any set of objects. The size-of-set function, N, is the set function that
assignstothesetAthenumberofelementsthatareinsetA,i.e.,NðAÞ ¼ P
x2A 1.5
Applying the size-of-set function in Example A.18, note that N(A) ¼ 8.
In Example A.19 note that N(A) ¼ 1.
Another special (point) function that will be useful in our study is the
indicator function, deﬁned as follows:
Deﬁnition A.22
Indicator Function
Let A be any subset of some universal set O. The indicator function, denoted
by IA, is a real-valued function with domain O and range {0, 1} such that
IAðxÞ ¼
1
if
x 2 A
0
if
x =2 A
	

:
5Note that P
x2A 1signiﬁed that a collection of 1’s are being summed together, the number in the collection being equal to the number
of elements in the set A. If A ¼ ; , effectively no 1’s are being added together, and thus N ;
ð Þ ¼ 0.
714
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Note that the indicator function indicates the set A by assigning the number
1 to any x that is an element of A, while assigning zero to any x that is not an
element of A. The main use of the indicator function is notational efﬁciency in
deﬁning functions, as the following example illustrates:
Example A.20
Let the function f:R ! R be deﬁned by
fðxÞ ¼
0
for
x 2 ð1; 0
x
for
x 2 ð0; 2
3  x
for
x 2 ð2; 3
0
for
x 2 ð3; 1Þ
2
6664
3
7775
Utilizing the indicator function, we can alternatively represent f(x) as
fðxÞ ¼ xIð0;2ðxÞ þ 3  x
ð
ÞIð2;3ðxÞ:
□
As a ﬁnal note on the use of functions, we (as do the vast majority of other
authors) will generally use a shorthand method for deﬁning functions by simply
specifying the relationship between elements in the domain of a function and
their respective images in the range of the function. For example, we would
deﬁne the function in Example A.19 by f(Ar) ¼ pr2 for Ar  A, or deﬁne the
function in Example A.15 by f(x) ¼ x2 for x ∈R. In all cases, the reader should
remember that a function is a set of ordered pairs (x,f(x)), or (A,f(A)). The reader
will sometimes ﬁnd in the literature phrases like the function f(x) or the set
function f(A). Literally speaking, such phrases are inconsistent, because f(x) or
f(A) are not functions, but rather images of elements in the domain of the
respective functions. In fact, the reader should not take such phrases literally,
but rather interpret these phrases as shorthand for phrases such as the function
whose values are given by f(x) or the set function whose values are given by f(A).
A5.
Combinations and Permutations
In a number of situations involving probability assignments, it will be useful to
have an efﬁcient method for counting the number of different ways a group of r
objects can be selected from a group of n distinct objects, n  r. Obviously, if two
groups of r objects do not contain the same r objects, they must be considered
different. But what if two groups of r objects do contain the same objects, except
the objects in the group are arranged in different orders? Are the two groups to be
considered different? If difference in order constitutes difference in groups, then
we are dealing with the notion of permutations. On the other hand, if the order
of listing the objects in a group is not used as a basis for distinguishing between
groups, then we are dealing with the notion of combinations.
In order to establish a formula for determining the number of permutations
of n distinct objects taken r at a time, the following example is suggestive.
A5.
Combinations and Permutations
715

Example A.21
Examine the number of different ways a group of three letters can be selected
from the letters, a,b,c,d, where difference in order of listing is taken to mean
difference in groups. Note that the ﬁrst letter can be chosen in four different
ways. After we have chosen one of the letters for the ﬁrst selection, the second
selection can be any of the remaining three letters. Finally, after we have chosen
two letters in the ﬁrst two selections, there are then two letters left to be
potentially chosen for the third selection. Thus, there are 4∙3∙2 ¼ 24 different
ways of selecting a group of three letters from the letters a,b,c,d if difference in
the order of listing constitutes difference in groups (The reader should attempt to
list the 24 different groups).
The logic of the preceding example can be applied to establish a general
formula for determining the number of permutations of n distinct objects taken
r at a time:
ðnÞr ¼
n!
n  r
ð
Þ! ¼ n n  1
ð
Þ n  2
ð
Þ 	 	 	 n  r þ 1
ð
Þ;
where ! denotes the factorial operation, i.e.,6
n! ¼ n n  1
ð
Þ n  2
ð
Þ n  3
ð
Þ . . . 1:
So, for example, 4! ¼ 4∙3∙2∙1 ¼ 24. In Example A.21, n ¼ 4 and r ¼ 3, so that
(4)3 ¼ 4!/1! ¼ 24.
In order to establish a formula for determining the number of combinations
of n distinct objects taken r at a time, examine the number of different ways
a group of three letters can be selected from the letters a,b,c,d, where difference
in order of listing does not imply the groups are different. Recall that we
discovered that there were 24 permutations of the four letters a,b,c,d selected
three at a time. Now note that any three letters, say a,b,c, can be arranged
in
(3)3 ¼ 6
different
orders,
which
represents
“overcounting”
from
the
combinations point of view. Reducing the number of permutations by the degree
of “overcounting” results in the number of combinations, i.e., there are 24/6 ¼
4 combinations of the four letters taken three at a time, namely (a,b,c), (a,b,d),
(a,c,d), and (b,c,d).
□
In the preceding example, the number of permutations of n(¼4) objects taken
r(¼3) at a time was reduced by a factor of r!(¼3!), where the latter value
represents the number of possible permutations of r objects. This suggests the
general formula for the number of combinations of n objects taken r at a time:
n
r


¼ ðn Þr
r!
¼
n!
ðn  rÞ ! r!
6By deﬁnition, we take 0! ¼ 1.
716
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

In Example A.21, we have
4
3


¼
4!
1! 3! ¼ 4
as the appropriate number of combinations.
The concept of combinations is useful in determining the number of subsets
that can be constructed from a ﬁnite set A. Note that in counting the number of
subsets, changes in the order of listing set elements does not produce a different
set, e.g., the sets {a,b,c} and {c,a,b} are the same set of letters (recall the deﬁnition
of a set). Then the total number of subsets of a set A containing n elements is
given by the number of different subsets deﬁned by taking no elements (i.e., the
null set) plus the number of different subsets deﬁned by taking one element, plus
the number of different subsets deﬁned by taking two elements, . . ., and ﬁnally,
the number of different subsets deﬁned by taking all n elements (i.e., the set
A itself). Thus, the total number of different subsets of A can be written as
S
n
r¼0
n
r


¼ S
n
r¼0
n!
ðn  rÞ! r!
This sum can be greatly simpliﬁed by recalling that
x þ y
ð
Þn ¼
X
n
r¼0
n
r


xrynr; n ¼ 1; 2; . . . ;
which is the binomial theorem. Then letting x ¼ y ¼ 1, we have that
2n ¼ S
n
r¼0
n
r


so that 2n is the number of different subsets contained in a set A that has n
elements. The set containing all 2n subsets of a set, A, of n elements is called the
power set of A.
Example A.22
Power Set
In Example A.18, recall that we identiﬁed a total of eight subsets of the set
O ¼ {1,2,3}. This is the number of subsets we would expect from our discussion
above, i.e., since n ¼ 3, there are 23 ¼ 8 subsets of O.
□
It should be noted that
n
r


is deﬁned to be 0 whenever n < r, or whenever n
and/or r are < 0 or are not integer valued. The rationale for
n
r


¼ 0 in each of
these cases is that there is no way to deﬁne subsets of size r from a collection of n
objects for the designated values of n and r.
When n is large, the calculation of n! needed in the previous formulas
pertaining to numbers of permutations or combinations can be quite formidable.
A result known as Stirling’s formula can provide a useful approximation to n! for
large n.
A5.
Combinations and Permutations
717

Deﬁnition A.23
Stirling’s Formula7
n! 
 (2p)1/2 nn+.5 en for large n.8
A logical question to ask regarding the use of Stirling’s formula is how large
is “large n”? Stirling’s formula invariably underestimates n! but the percentage
error is  1 percent for n  10, and monotonically decreases as n ! 1.
A6.
Summation and Integration Notation
We will use a number of variations on summation and integration notation in
this text. The meaning of the various types of notation are presented in
Table A.1.
We illustrate the use of some of the notation in the following examples.
Example A.23
Summation Notation
Let A1 ¼ {1,2,3}, A2 ¼ {2,4,6}, A ¼ A1  A2, B ¼ {(x1,x2): x1 ∈A1; x2 ∈{x1,
x1 + 1,. . .,3x1}}, y ¼ (y1,y2,. . .,yn), and f(x1,x2) ¼ x1 + 2x2
2. Then
Table A.1
Summation and integration notation
Notation
Deﬁnition
Pn
i¼‘ xi
Sum the values of x‘, x‘þ1,. . .,xn, i.e., x‘ þ x‘þ1 þ . . . þ xn
P
i2I xi
Sum the values of the xi0s, for i ∈I
P
x2A x
Sum the values of x ∈A
Pb
x¼a x
Sum the values of x in the sequence of integers from a to b, i.e., a þ (a þ 1) þ (a þ 2)
þ . . . þ b
Pn
i¼‘
Pm
j¼k xij
Sum the values of the xij0s for i ¼ ‘, ‘ þ 1,. . .,n and j ¼ k, k þ 1,. . .,m
P
i2I
P
j2J xij or
S
i;j
ð Þ2A xij
Sum the values of the xij0s for i ∈I and j ∈J, or for (i,j) ∈A
P
x12A1 	 	 	 P
xn2An f x1; . . . ; xn
ð
Þ
Sum the values of f(x1,. . .,xn) for xi ∈Ai, i ¼ 1,. . .,n
S
x1;...;xn
ð
Þ2A f x1; . . . ; xn
ð
Þ
Sum the values of f(x1,. . .,xn) for (x1,. . .,xn) ∈A
Pb1
x1¼a1 	 	 	 Pbn
xn¼an f x1; . . . ; xn
ð
Þ
Sum the values of f(x1,. . .,xn) for xi in the sequence of integers ai to bi, i ¼ 1,. . .,n
Ð b
a fðxÞdx
Integral of the function f(x) from a to b (a and/or b can be 1 and 1)
Ð
x2A fðxÞdx
Integral of the function f(x) over the set of points A
Ð
x12A1 	 	 	
Ð
xn2An f x1; . . . ; xn
ð
Þdxn . . . dx1 Iterated integral of the function f(x1,. . .,xn) over the points xi ∈Ai, i ¼ 1,. . .,n
Ð
x1;...xn
ð
Þ2A
f x1; . . . ; xn
ð
Þdx1 . . . dxn
Multiple integral of the function f(x1,. . .,xn) over the points (x1,. . .,xn) ∈A
Ð b1
a1 	 	 	
Ð bn
an f x1; . . . ; xn
ð
Þdxn . . . dx1
Iterated integral of the function f(x1,. . .,xn) for xi in the (open, half open-half closed,
or closed) interval ai to bi, for i ¼ 1,. . .,n
7See Feller (1968), An Introduction to the Theory of Probability and Its Applications, 3rd ed., pp. 52–54.
8Note that 
 means “approximately equal to.”
718
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

X
x2A1
x ¼ 1 þ 2 þ 3 ¼ 6;
X
x2A2
x2 ¼ 22 þ 42 þ 62 ¼ 56;
X
i2A2
yi ¼ y2 þ y4 þ y6;
X
i2A1
yi ¼
X
3
i¼1
yi ¼ y1 þ y2 þ y3
X
x12A1
X
x22A2
f x1; x2
ð
Þ ¼
X
x12A1
X
x22A2
x1 þ 2x2
2


¼ 354;
X
x1;x2
ð
Þ2A
f x1; x2
ð
Þ ¼
X
x12A1
X
x22A2
f x1; x2
ð
Þ ¼ 354;
X
x1;x2
ð
Þ2B
f x1; x2
ð
Þ ¼
X
3
x1¼1
X
3x1
x2¼x1
x1 þ 2x2
2


¼ 802:
□
Example A.24
Integration Notation
Let A1 ¼ [0,3], A2 ¼ [2,4], A ¼ A1  A2, B ¼ {(x1,x2): x1 ∈A1, 0 < x2 < x2
1}, and
f(x1, x2) ¼ x1x2
2. Then
ð
x2A1
2xdx ¼
ð3
0
2xdx ¼ 2x2
2
j3
0 ¼ 9;
ð
x2A1\A2
x2dx ¼
ð3
2
x2dx ¼ x3
3 j3
2 ¼ 19
3 ;
ð
x12A1
ð
x22A2
f x1; x2
ð
Þdx2dx1 ¼
ð3
0
ð4
2
f x1; x2
ð
Þdx2dx1
¼
ð3
0
x1x3
2
3
j4
2 dx1 ¼
ð3
0
56
3 x1dx1
¼ 56x2
1
6
j3
0 ¼ 84;
ð
x1;x2
ð
Þ2A
f x1; x2
ð
Þdx1dx2 ¼
ð
x12A1
ð
x22A2
f x1; x2
ð
Þdx2dx1 ¼ 84;
ð
x1;x2
ð
Þ2B
f x1; x2
ð
Þdx1dx2 ¼
ð3
0
ðx2
1
0
f x1; x2
ð
Þdx2dx1
¼
ð3
0
x1x3
2
3
j
x2
1
0 dx1 ¼
ð3
0
x7
1
3 dx1
¼ x8
1
24 j3
0 ¼ 273:375:
□
Regarding
matrix
differentiation
notation,
we
utilize
the
following
conventions. Let g(x) and y(x) be a scalar and (n  1) vector function of the
(k  1) vector x, respectively. Then
A6.
Summation and Integration Notation
719

Derivative
Matrix dimension
(i,j)th entry
@gðxÞ
@x
(k  1)
@g
@xi
@gðxÞ
@x0
¼ @gðxÞ0
@x
(1  k)
@g
@xj
@gðxÞ
@x@x0
(k  k)
@2g
@xi@xj
@yðxÞ
@x
(k  n)
@yj
@xi
@yðxÞ
@x0
¼ @yðxÞ0
@x
(n  k)
@yi
@xj
A7. Elements of Real Analysis
In this section, we present a number of prerequisite results from real analysis
that facilitate the development and understanding of various types of asymptotic
probability behavior. In particular, the concepts of sequences, limits, continuity
of a function, and orders of magnitude of a sequence will be examined.
Sequences of Numbers and Random Variables
We begin with the notion of a sequence. In the deﬁnition, we refer to the set of
natural numbers, which is simply the set of positive integers in their natural
order, 1, 2, 3, . . . .
Deﬁnition A.24
Sequence
Let A be any set. A sequence in A is a function having the natural numbers, N,
for its domain, and its range contained in A, i.e., f: N ! A, is a sequence in A.
When utilizing the concept of a sequence, we (and others) will often suppress
the function aspect of its deﬁnition and concentrate on the ordered collection of
image elements of the function. Thus, given a sequence deﬁned by {(n,y): y ¼
f(n), n ∈N}, we will equivalently refer to the collection of image elements {y1,
y2, y3,. . .} as the sequence, where yn ¼ f(n). The subscripts on the elements of the
set {y1, y2, y3,. . .} serve to deﬁne the order of the elements in the sequence.
Furthermore, we will utilize the notation {yn} as an abbreviation for the
sequence {y1, y2, y3,. . .}9 In the following examples of sequences, we continue
to use N to denote the set of natural numbers.
9Another common abbreviated notation that is sometimes used to denote a sequence is given by (yn). The notation we have adopted is
more prevalent in the statistics literature. While there will be no confusion in this text, in general, the reader will have to rely on the
context of a discussion to determine whether {yn} refers to a sequence or to a set containing the single element yn.
720
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Example A.25
Sequences in ℝ
(a) {2,4,8,. . .}, which is deﬁned by the function y ¼ 2n, n ∈N.
(b) {1,1/3,1/9,1/27,. . .}, which is deﬁned by the function y ¼ (1/3)n1, n ∈N.
(c) {3,1,1,3,. . .}, which is deﬁned by the function y ¼ 2n  5, n ∈N.
□
Example A.26
A Sequence of Matrices
Let xn be an (n  2) matrix whose ith row is deﬁned by the (1  2) vector [1 i],
so that
xn¼
1
1
1
2
...
1
n
2
664
3
775:
Then
1
1
1
1
"
#
;
1
3
2
3
2
5
2
2
664
3
775;
1
2
2
14
3
2
4
3
5; . . .
8
>
>
<
>
>
:
9
>
>
=
>
>
;
is a sequence of matrices {y1,y2,y3,. . .} deﬁned by the function yn ¼ 1
n x0nxn; n 2 N,
where the nth element of the sequence is deﬁned as
yn¼
1
Pn
i¼1 i


n
Pn
i¼1 i


n
Pn
i¼1 i2


n
2
6664
3
7775¼
1
n þ 1
ð
Þ
2
n þ 1
ð
Þ
2
n þ 1
ð
Þ 2n þ 1
ð
Þ
6
2
664
3
775:
□
In statistical practice one frequently encounters sequences of random
variables. In this case, the set A in Deﬁnition A.24 is a collection of random
variables, and the function f: N ! A deﬁning the sequence places the random
variables in A in a speciﬁc order. That is, the sequence of random variables {Y1,
Y2,Y3,. . .} is simply an ordered collection of random variables. In our study of
asymptotics, the elements in the sequence of random variables will often be
deﬁned as functions of other random variables, such as Yn ¼ gn(X1,. . .,Xn), and
we will be interested in studying the characteristics of the sequence of probabil-
ity distributions associated with the sequence of Yn’s as n ! 1.
The following are examples of sequences of random variables. We introduce
the notation Y ~ f(y) to indicate that Y has probability density f(y), or that Y is
distributed as f(y). This notation can also be used as Y ~ F(y) to denote that Y has
the CDF F(y). The acronym iid used ahead stands for independent and identically
distributed, meaning that the random variables in a collection are independent
and each of the random variables has the same PDF or probability distribution.
Example A.27
Let X1,. . ., Xn be iid random variables, each with PDF N(m, s2), where Xi
represents the miles per gallon obtained from the ith automobile of a certain
type tested for fuel efﬁciency. Examine the sequence of random variables
A7. Elements of Real Analysis
721

fY1; Y2; Y3; . . .g; where Yn ¼ n1 S
n
i¼1Xi; n 2 N:
Note that the nth element of the sequence represents the average miles per
gallon obtained from n of the automobiles tested, and
Yn  N m; s2
n


;
so that we can deﬁne a sequence of probability density functions associated with
the sequence of random variables as
N m; s2


; N m; s2
2


; N m; s2
3


; . . .
n
o
:
□
Example A.28
Let X1,. . ., Xn be iid Bernoulli-type random variables each with density function
pz(1  p)1-z I{0,1}(z), where Xi indicates whether the ith customer entering a store
makes a purchase (xi ¼ 1) or not (xi ¼ 0). Examine the sequence of random
variables {Y1,Y2,Y3,. . .}, where Yn ¼ Pn
i¼1 Xi, n ∈N. Note that the nth element
of the sequence represents how many of the ﬁrst n customers make a purchase,
and Yn has a binomial distribution with parameters n and p, as BIN(n,p) or
Yn 
n
yn


pyn ð1  p ÞnynI 0;1;2...n
f
g yn
ð
Þ:
The sequence of probability density functions associated with the sequence of
random variables is given by {BIN(1,p), BIN(2,p), BIN(3,p),. . .}.
□
Limit of a Real Number Sequence
We now examine the concept of the limit of a real number sequence. We begin
with a sequence whose elements are scalars and then extend the result to a
sequence whose elements are vectors of real numbers.
Deﬁnition A.25
Limit of a Real Number
Sequence
Let {yn} be a sequence whose elements are scalar real numbers. Suppose there
exists a real number, y, such that for every real e > 0 there exists an integer
N(e) for which n  N(e) ) |yn  y| < e. Then y is the limit of the sequence
{yn}, and the sequence {yn} is said to converge to y as n ! 1. The existence
of the limit is denoted by yn ! y or limn!1 yn ¼ y. If the limit does not
exist, the sequence is said to be divergent.
The deﬁnition of the limit implies that for a sufﬁciently large choice of n, yn
(and yn+1, yn+2, yn+3,. . .) becomes arbitrarily close to the number y. This is so
since, by deﬁnition, we can choose e > 0 to be arbitrarily small and yet there
exists an n large enough (namely n  N(e)) such that y  e < yn < y + e.
Figure A.5 provides a graphical illustration of the limit concept.
722
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

In the ﬁgure, it is seen that for all elements of yi ∈{yn} for which i is large
enough, i.e., for i > N(e), the value of yi is contained in the interval (y  e, y + e).
In other words, yi is within e  distance of y for i > N(e). Furthermore, for every
choice of e > 0, no matter how small, there exists an N(e) for which a ﬁgure such
as Figure A.6 applies.
It can be shown that for the limit of a sequence of real numbers to exist, it is
necessary (but not sufﬁcient) that the sequence is bounded (Bartle, The Elements
of Real Analysis, 2nd Edition, John Wiley and Sons, New York, p. 93).
Deﬁnition A.26
Bounded Sequence of
Real Numbers
The sequence of real numbers {yn} is bounded iff there exists a ﬁnite number
m > 0 such that |yn|  m 8 n ∈N; otherwise the sequence is said to be
unbounded.
Thus, for a sequence of real numbers to be bounded, there must exist a
positive number that is larger than the absolute value of each and every number
in the sequence. For a sequence that has no limit and is also unbounded, we
write yn ! 1 (or yn ! 1), denoting that the sequence diverges to inﬁnity (or
diverges to negative inﬁnity), if 8m > 0 there exists a positive integer N(m) such
that yn > m (or yn < m) 8n > N(m).
Example A.29
Boundedness and
existence of a Limit for
Real Number Sequences
(a) yn ¼ 3 + n2, n ∈N. This sequence is bounded, since |yn|  4 8 n ∈N. Also,
the sequence has a limit, where yn ! 3. This follows since, 8 e > 0, |yn  3|
< e 8 n > e1/2, and there always exists an integer N(e)  e1/2 (e.g., trunc
(e1/2) + 1).
(b) yn ¼ sin(n), n ∈N (let n be measured in degrees). The sequence is bounded,
since |sin(x)|  1 8 x. The sequence does not have a limit, since sin(x) cycles
between the values of 1 and 1.
(c) yn ¼ n2  3n + 1, n ∈N. The sequence is not bounded, since there does
not exist a ﬁnite number m > 0 for which n2  3n + 1  m 8 n ∈N.
Since the sequence is unbounded, the sequence does not have a limit.
y − e
y
y + e
y1
y2 y3
y4
y6
y5
yi
2
3
4
5
6
1
N(ε)
i>N(ε)
n
Figure A.6
Illustration of the
sequence {yn} for which
limn!1 yn ¼ y.
A7. Elements of Real Analysis
723

Also note that yn ! 1, that is, the sequence diverges to inﬁnity, because
8 m > 0, n2  3n + 1 > m when n > N(m) ¼ trunc ([3 +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
5 þ 4m
p
]/2) + 1
(use the quadratic formula).□
The preceding examples illustrate that boundedness of a sequence is not
sufﬁcient for the existence of a limit for the sequence. We add that it is proper to
speak of the limit of a sequence since the limit will be unique if it exists at all
(Bartle, p. 93).
The limit concept can be extended to a sequence whose elements are real-
valued vectors or matrices. We introduce the notation yn[i,j] to indicate the
(i,j)th element of the (q  k) matrix yn in a sequence of matrices, and similarly
yn[i] denotes the ith element of the (q  1) vector yn in a sequence of vectors. The
extension of the limit concept amounts to viewing the matrix sequence as
encompassing mk sequences, {yn[i,j]}, one for each matrix element, with each
element examined for convergence (by Deﬁnition A.25). Limits of vector
sequences follow by letting k ¼ 1.
Deﬁnition A.27
Limit of a Real-Valued
Matrix Sequence
Let {yn} be a sequence whose elements are (q  k) real-valued matrices.
Suppose there exists a (q  k) matrix of real numbers y such that yn[i,j] !
y[i,j] for i ¼ 1,. . .,q and j ¼ 1,. . .,k. Then the matrix y is the limit of the matrix
sequence {yn}, and the sequence of matrices {yn} is said to converge to the
matrix y as n ! 1. The existence of the limit is denoted by yn ! y, or by
limn!1yn ¼ y. If the limit does not exist, the sequence is said to be divergent.
The deﬁnition of the limit implies that for a sufﬁciently large choice of n,
the matrix yn (and yn+1, yn+2, yn+3,. . .) becomes arbitrarily close to the matrix y,
element by element. The deﬁnition also implies that for the real-valued matrix
to have a limit, the sequence of matrices must be bounded elementwise, i.e.,
|yn[i,j]|  m 8 n ∈N and 8i , j or else yn[i,j] 6! y[i,j] for some i and j, and then yn
6! y. Regarding divergence to inﬁnity (or negative inﬁnity), since there are
essentially qk convergence conditions involved when examining sequences of
(q  k) matrices, patterns of divergence and convergence of the various elements
of the matrices can be quite diverse.
Example A.30
Boundedness and Limits
of Matrices
(a) Recall the sequence of matrices in Example A.26. In this case, only the
sequence {yn[1,1]} is bounded. All other sequences of matrix elements are
unbounded and, in fact, diverge to inﬁnity, i.e., yn[i,j] ! 1 for (i,j) 6¼ (1,1).
Since all of the sequences of matrix elements must be bounded for the
matrix sequence to converge, the matrix sequence does not have a limit.
(b) Let {yn} be a sequence of matrices such that yn ¼
3n1
n1
3
1 þ n1


, n ∈N.
All four sequences of matrix elements are bounded, since
3n1

  3,
| n1|  1, |3|  3, and |1 + n1|  2 8 n ∈N. Furthermore, limits exist for
all four sequences of matrix elements, since 3n1 ! 0, n1 ! 0, 3 ! 3,
and 1 þ n1 ! 1. Thus, yn ! y ¼
0
0
3
1


:
□
724
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

One might be interested in a sequence {yn} that is deﬁned via a function of
the elements of other sequences. For example, we may be interested in the
sequence {yn} that is deﬁned by adding corresponding elements in the sequences
{xn} and {zn}, as yn ¼ xn + zn. Of course, we can analyze the properties of the
sequence {yn} directly to establish whether the sequence converges, but if
the convergence properties of {xn} and {zn} are known, the following lemma can
expedite the analysis if the functions are deﬁned via addition, subtraction, or
multiplication. We precede the statement of the lemma with deﬁnitions for
adding, subtracting, and multiplying sequences. Note that x[.,i] refers to the
ith column of the matrix x.
Deﬁnition A.28
Adding, Subtracting,
Multiplying Sequences
Let {xn} and {zn} be sequences of conformable real-valued matrices.
(a) Summation: The summation of {xn} and {zn}, {xn} + {zn}, is a sequence {yn}
deﬁned by yn ¼ xn + zn , 8n.
(b) Difference: The difference between {xn} and {zn}, {xn}  {zn}, is a sequence
{yn} deﬁned by yn ¼ xn  zn , 8n.
(c) Product: The product of {xn} and {zn}, {xn} {zn}, is a sequence {yn} deﬁned
by yn ¼ xnzn , 8n.
Lemma A.1
Combinations of
Sequences
Let {xn} and {zn} be convergent sequences of conformable real-valued matrices
such that xn ! x and zn ! z. Then
(a) xn + zn ! x + z,
(b) xn  zn ! x  z,
(c) xnzn ! xz,
(d) if {an} ! a, then anxn ! ax,
(e) if {bn} ! b 6¼ 0, then b1
n xn ! b1x,
(f) P
k
i¼1
xn :; i
½
 ! P
k
i¼1
x :; i
½

(g) if {zn} is a sequence of nonsingular matrices that converges to the
nonsingular matrix z, then z1
n
! z1 and z1
n xn ! z1x.
Proof
Bartle, pp. 100–101, and Deﬁnitions 5.4 and 5.5.
Note that since the sequences {xn} and {zn} can themselves be deﬁned in
terms of combinations of other sequences, the lemma actually implies conver-
gence results involving more than just two sequences. For example, letting
zn ¼ an + bn ! a + b, then an + bn + zn ! a + b + z and (an + bn)zn !(a + b)z.
A7. Elements of Real Analysis
725

Example A.31
Convergence Properties
of Sequences
(a) Let {xn} and {zn} be deﬁned as xn ¼ 3 + n1/2 and zn ¼ 2 exp(2/n) for n ∈N,
respectively. Note that xn ! 3 and zn
! 2. Then using Lemma A.1,
xn + zn ! 5, xn  zn ! 1, and xnzn ! 6. Let {an} be deﬁned by an ¼
5(n + 1)/n for n ∈N, and note that an ! 5. Also deﬁne the vector sequence
{yn} by yn
21
ð
Þ
¼
xn
zn


so that yn !
3
2


. Then from Lemma A.1, anyn !
15
10


and a1
n yn !
3=5
2=5


.
(b) Let {wn} be a matrix sequence deﬁned by
wn ¼
2 þ n1
2
n
3
n þ 1
ð
Þ
n
2
664
3
775 for n 2 N;
and let {xn} be a vector sequence deﬁned by
xn ¼
1 þ n1
2 exp n1




for n 2 N:
Note that wn !
2
0
3
1


and xn !
1
2


: Using Lemma A.1, it follows that
w1
n !
2
0
3
1

1
¼
:5
0
1:5
1


;wn xn!
2
5


; and w1
n xn !
:5
:5


:
Note further that wn [.,1] + wn [.,2] + xn !
3
6


:
□
Continuous Functions
Continuous functions play a prominent role in a number of important asymp-
totic results. We deﬁne the concept of a continuous function below using two
alternative but completely equivalent characterizations. We remind the reader
that d x; w
ð
Þ ¼ ½ x  w
ð
Þ0 x  w
ð
Þ
1=2 is the distance between points x and w.
Deﬁnition A.29
Continuous Functions
The function10 g: A ! R, for A  Rm, is continuous at the point x ∈A iff
either:
(a) 8e > 0, ∃d(e) > 0 such that w ∈A and d(x,w) < d(e) implies gðwÞ  gðxÞ
j
j<e
(b) 8 sequence {xn} in A for which xn ! x, it is also true that g(xn) ! g(x).
10This deﬁnition can be altered to provide deﬁnitions for continuity from the right and continuity from the left. For continuity from
the right, the condition w  x is added in part (a). The condition xn  x 8n is added to part (b). For continuity from the left, the
conditions become w  x and xn  x 8n.
726
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

The k  1 vector function g: A ! Rk is continuous at the point x∈A iff each
coordinate function gj(x) is continuous at the point x, j ¼ 1,. . .,k. The function
g is said to be continuous on the set B  A if the function is continuous at every
point in B.
Intuitively, a function is continuous at a point x if, when the function is
evaluated at domain elements that are closer and closer to x, the value of the
function is closer and closer to the value of the function at x (in an elementwise
vector comparison sense if k  2). In the simple case where m ¼ k ¼ 1, the
graph of a function that is continuous on an interval set B ¼ (a,b) can be
drawn 8 x ∈B “without lifting the pencil from the paper,” i.e., the graph is an
unbroken curve.
Example A.32
Continuity Properties
of Functions
(a) Let f:(0,1) ! R be deﬁned by y ¼ x1. Intuitively we expect f to be continu-
ous on (0,1), since its graph is an unbroken curve. To demonstrate formally
that f is continuous on (0,1), let x0 ∈(0,1), and note that
fðxÞ  fðx0Þ
j
j ¼ x1  x1
0

 ¼ x0  x
xx0

 ¼ x0  x
j
j
xx0
where we have eliminated the denominator from the absolute value operator
because in the domain of f, x > 0. It follows that if |x  x0| < d, then
fðxÞ  fðx0Þ
j
j<d=xx0:
Now choose any e > 0. For f to be continuous at x0, it must be the case
that d can be chosen such that |x  x0| < d ) |f(x)  f(x0)| < e. Such choices
of d exist, one being equal to d ¼ x2
0 e /(1 + x0 e). Since the argument can be
applied 8x0 ∈(0,1) and 8 e > 0, f is continuous on (0,1).
(b) Let f: R2 ! R2be deﬁned by the coordinate functions
y½1
y½2


¼
f1 ðxÞ
f2 ðxÞ


¼
x½1 2 þ2x½2
x½1


:
Let x 2 R2 , and let {xn} be any sequence in R2 for which xn !
x .To
demonstrate continuity of f at x, we will show that xn ! x ) f(xn) !
f( x ). Examine f1(x) ﬁrst. Note that f1(xn) ¼ xn[1]2 + 2xn[2] ¼ (xn[1]xn[1]) +
2xn[2] can be interpreted as the summation of the sequence {xn[1]}{xn[1]}
and the sequence 2{xn[2]}. Since xn[1]
!
x [1] and xn[2]
!
x [2] by
assumption, it follows from Lemma A.1 that {xn[1]} {xn[1]}
!
x [1]2,
2{xn[2]} ! 2 x [2], and thus f1(xn) !
x [1]2 + 2x [2] ¼ f1(x ). Verifying
convergence of the second component function is straightforward, because
f2(xn) ¼ xn[1] ! x[1] ¼ f2(x). Thus, f is continuous at x, and since the
above argument holds for any x ∈R2, f is continuous on R2.
(c) Let f: R ! R be deﬁned by f(x) ¼ I(0,1)(x). Intuitively, since the graph of the
function has a break at x ¼ 0, we expect that the function is not continuous
on R. To formally demonstrate that f is discontinuous at x ¼ 0, it will be
shown that there does not exist a d(e) > 0 for every e > 0 such that |x  0|
< d(e) ) |f(x)  f(0)| < e. Choose any e ∈(0,1), and note that f(0) ¼ 0.
Given
any
choice
of
d(e) > 0,
let
x ¼ d(e)/2,
so
that
|x  0| < d(e).
A7. Elements of Real Analysis
727

Then |f(x)  f(0)| ¼ 1 ≮e. Therefore, f is not continuous on R. (Note: f is
continuous on (0,1) and on (1,0), as the reader might wish to verify).
□
Convergence of a Function Sequence
Convergence of a sequence of functions refers to a case where the function
deﬁnitions themselves can change as n changes, so that we can conceptualize
a sequence of image values
fnðxÞ
f
g ¼
f1ðxÞ; f2ðxÞ; f3ðxÞ; . . .
f
g for each value of
x (which could be a vector) in the common domain of the sequence of functions.
Interest centers on whether there exists a “limit” function deﬁnition, fðxÞ, such
that fnðxÞ ! fðxÞ for all x in some subset of the common domain of the sequence
of functions. The subset of x-values on which fnðxÞ
f
g converges to fðxÞ could be
the entire domain, or some smaller subset of points, or the null set (i.e., fnðxÞ
f
g
does not converge for any x).
We formalize the concept of convergence of a sequence of functions in the
next deﬁnition. In the deﬁnition, the notation
fn
f
g refers to the sequence of
function deﬁnitions, as opposed to fnðxÞ
f
g which denotes the sequence of image
values generated by the sequence of function deﬁnitions when evaluated at the
point x.
Deﬁnition A.30
Convergence of a
Function Sequence
Let {fn} be a sequence of functionsfn : D ! R‘ forn ¼ 1; 2; . . . ; having common
domain D  Rm . Let f : D0 ! R‘ be a function with domain D0  D. The
function sequence {fn} is said to converge on D0 to f if fnðxÞ ! fðxÞ; 8x 2 D0.
If {fn} converges to f on D0, f is called the limiting function of {fn} on D0, and {fn}
is said to be convergent on D0.
Intuitively, if f is the limit function of {fn} on D0, then for large enough n,
fn(x) 
 f(x) for x ∈D0 since fn(x) converges to f(x) on D0. Then f(x) can be viewed
as an approximation to fn(x) on D0 when n is large.
Example A.33
Convergence of
Function Sequences
(a) Let the function sequence {fn} be deﬁned by fn(x) ¼ n1 + 2x2 for x ∈R .
Deﬁne the function f: R ! R by f(x) ¼ 2x2. Then f is the limiting function
of {fn} on R. To see this, note that fn(x) ! 2x2 ¼ f(x) 8 x ∈R. For large n,
fn(x) 
 f(x) 8 x ∈R.
(b) Let the vector function sequence {fn} be deﬁned by
fn ðxÞ ¼
f1n ðxÞ
f2n ðxÞ


¼
ð2 x2
1 þ3 nx1x2Þ
n
x2
1 þ x2
2
2
4
3
5 for ðx1; x2Þ 2 R2:
Deﬁne the function f:R2 ! R2 by
fðxÞ ¼
f1 ðxÞ
f2 ðxÞ


¼
3 x1 x2
x2
1 þ x2
2


for ðx1; x2Þ 2 R2:
Then f is the limiting function of {fn} on
R2 . To see this, note that
f1n ðxÞ ¼ 2 x2
1 =n þ 3 x1 x2 ! 0 þ 3 x1 x2 ¼ f1 ðxÞ, and f2n ðxÞ ¼ x2
1 þ x2
2 ¼ f2 ðxÞ
8x 2 R2, so that fn(x) ! f(x) 8 x ∈R2.
728
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

(c) Let the function sequence {fn} be deﬁned by fn(x) ¼ x  2 exp(nx) for x ∈R.
Let f: R ! R be deﬁned by f(x) ¼ x  2I{0}(x) for x ∈D0 ¼ [0,1]. Then f is
the limiting function of {fn} on the set [0,1). The sequence {fn} does not
converge for x < 0. To justify these conclusions, note that, by Lemma A.1,
limn!1fnðxÞ ¼ limn!1x  limn!12 exp nx
ð
Þ
¼ x  2Ið0ÞðxÞ8x  0 . When
x < 0, 2 exp(nx) ! 1, and thus fn(x) does not converge.
□
Order of Magnitude of a Sequence
In analyses involving sequences, it is sometimes useful to be able to characterize
or compare sequences and/or terms in a sequence relative to their order of
magnitude. In particular, when the deﬁnition of a sequence contains a number
of terms, the orders of magnitude of the terms will distinguish which terms
make dominant contributions to the magnitude of the sequence as n increases.
The concept is deﬁned below.
Deﬁnition A.31
Order of Magnitude of a
Sequence
Let {xn} be a real number sequence, and let {wn} be a real-valued matrix
sequence.
(a) The sequence {xn} is said to be at most of order nk, denoted by O(nk), if
there exists a ﬁnite real number c such that |nk xn|  c 8 n ∈N.
(b) The sequence {xn} is said to be of order smaller than nk, denoted by o(nk), if
nkxn ! 0.
(c) If {wn[i,j]} is O(nk) (or o(nk)) 8 i and j, then the matrix sequence {wn} is said
to be O(nk) (or o(nk)).
Intuitively, a sequence {xn} is O(nk) if the corresponding sequence {nkxn} is
such that all elements nkxn are bounded in absolute value by some positive
number c. A sequence {xn} is o(nk) if the product sequence {nkxn} converges to
zero. Note that if {xn} is O(nk), then {xn} is o(nk+∈) 8 ∈> 0, and if {xn} is o(nk),
then it is also O(nk). Notationally, the case O(no) or o(no) is most often
represented as O(1) or o(1).
Example A.34
Order of Magnitude
of a Sequence
(a) Let {xn} be deﬁned by xn ¼ 3n3  n2 + 2, for n ∈N. Then {xn} is O(n3), since
n3xn ¼ 3  n1 + 2n3 is bounded. Also, {xn} is o(n3+e) for any e > 0 since
n3exn ¼ 3ne  n1e + 2n3e ! 0.
(b) Let {xn} be deﬁned by xn ¼ 3 + n1, for n ∈N. Then {xn} is O(1), since xn ¼ 3
+ n1 is bounded, and {xn} is o(ne) 8e > 0, since nexn ¼ 3ne + n1e ! 0.
(c) Let the vector sequence {xn} be deﬁned by
xn ½1
xn ½2


¼
3n1
n1


. Then the
vector sequence {xn} is o(1) and O(1), since xn !
0
0


.
□
Some useful results regarding the order of magnitude of sums and products
of sequences are given in the following lemma.
A7. Elements of Real Analysis
729

Lemma A.2
Let {xn} and {zn} be real number sequences. The following relationships
between orders of magnitude hold:
IF
THEN
{xn}
{zn}
{xn + zn}
{xn zn}
O(nk)
O(nm)
O(nmax(k,m))
O(nk+m)
o(nk)
o(nm)
o(nmax(k,m))
o(nk+m)
O(nk)
o(nm)
O(nmax(k,m))
o(nk+m)
Proof
H. White (1984), Asymptotic Theory for Econometricians. Orlando,
Academic Press, p. 15.
Given that the sequences referred to in Lemma A.2 can themselves be
functions of other sequences, the results can be extended in a myriad of ways
to an arbitrary ﬁnite number of sequences. In the following lemma we state
some useful extensions.
Lemma A.3
Let {xn} and {wn} be (m  ‘) and (r  m) matrix sequences, respectively.
(a) If {xn} is such that {xn[.,i]} is O nki


(or o nki


) for i ¼ 1,. . ., ‘ , then
{ P‘
i¼1 xn :; i
½
} is O(nkmax) (or o(nkmax)) where kmax ¼ max(k1,. . .,k‘).
(b) (Special case of (a)): If {xn} is O(nk) (or o(nk)), then { Pℓ
i¼1 xn :; i
½
} is O(nk)
(or o(nk)).
(c) If {xn} is O(nk) and {wn} is
OðndÞ
oðndÞ
	

, then {wnxn} is
OðnkþdÞ
oðnkþdÞ
	

and
{nvwnxn} is
OðnkþdvÞ
oðnkþdvÞ
	

.
(d) (Special case of (c)): If {xn} is O(nk) (or o(nk)), then {x0nxn} is O(n2k} (or o(n2k))
and {nvx0nxn} is O(n2kv) (or o(n2kv)).
Proof
This follows from Lemma A.2 and mathematical induction.
We illustrate the application of some of the preceding results in the follow-
ing example.
Example A.35
Orders of Magnitude for
Combinations of
Sequences
(a) Let {xn} be deﬁned by xn ¼ 2n1 + 5n and let {zn} be deﬁned by zn ¼ n2 + 2n.
Note that {xn} is O(n1) and {zn} is O(n2). It follows immediately, from Lemma
A.2 that {xn + zn} ¼ {n2 + 7n + 2n1} is O(n2) (and o(n2+e) for e > 0) and
{xn zn} ¼ {5n3 + 10n2 + 2n + 4} is O(n3) (and o(n3+e) for e > 0).
730
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

(b) Let {xn} and {wn} be deﬁned by
xn ¼
7 þ n1
n1


and wn ¼
n2 þ2n þ 1
3 n2 þ7
n2
n2 þn


so that xn is O(1) and wn is O(n2). It follows immediately from Lemma A.3 that
fwn xng ¼
7 n2 þ18n þ 9 þ 8 n1
7 n2 þ2n þ 1


	

is O(n2) and o(n2+e) for e > 0, and
x0nxn
f
g ¼ f49 þ 14n1 þ 2n2g 1
2 is O(1) and
o(ne) for e > 0. It also follows, for example, that {nx0nxn} and {n1wnxn} are both
O(n1) and o(n1+e) for e > 0.
□
Keywords, Phrases, and Symbols
∈
R0, R+
A2 ¼ A  A
Associative laws
Axiom (or postulate)
Binary relation from A to B, S: A ! B
Binomial theorem
Cartesian product
Closed, open, half-open intervals
Combinations,
n
r


Commutative laws
Complement, A
Complements of complements
Contained in, 
Corollary
Countable set
Deﬁnition
DeMorgan’s laws
Distributive laws
Element
Empty or null set,;
Equality of sets, ¼
Exhaustive listing
Finite set
For every, 8
Function from A to B, f:A ! B
Idempotency laws
Identity elements
iff (if and only if)
Image of x under f
Index set
Indicator function, IA(x)
Inﬁnite set
Integration notation
Intersection and union of
complements
Intersection with null set
Intersection, \
Interval set notation
Inverse function, f1:B ! A
Inverse image
Lemma
Mathematical rule
Multiple intersection notation
Multiple union notation
Mutually exclusive (disjoint)
Negation, /
Null set as a subset
Permutations, nr
Point function
QED
Real line
Real-valued function
Set
Set difference, 
Set function
Size of set function
Stirling’s formula
Subset
Such that (such that)
Summation notation
The function f(x)
The set function f(A)
Theorem (or proposition)
There exists, ∃
Uncountable set
Union, [
Universal set
Venn diagram
Verbal rule
xSy
Problems
1. Using either an exhaustive listing, verbal rule, or
mathematical rule, deﬁne the following sets:
(a) The set of all senior citizens receiving social security
payments in the United States.
(b) The set of all positive numbers that are positive inte-
ger powers of the number 10 (i.e., 101, 102, etc.).
(c) The set of all possible outcomes resulting from
rolling a red and a green die and calculating the values
Problems
731

of y  x, where y ¼ number of dots on the red die,
x ¼ number of dots on the green die.
(d) the set of all two-tuples (x1, x2) where x1 is any real
number and x2 is related to x1 by raising the number
e to the power x1.
2. Label the sets you have identiﬁed in Problem (1) as
being either ﬁnite, countably inﬁnite, or uncountably
inﬁnite, and explain your choice.
3. For each set below, state whether the set is ﬁnite,
countably inﬁnite, or uncountably inﬁnite.
(a) S ¼ {x: x is a U.S. citizen who has purchased a Japa-
nese car during the past year}.
(b) S ¼ {(x,y): y  x2, x is a positive integer, y ∈R0}.
(c) S ¼ {p: p is the price of a quart of milk sold at a retail
store in the U.S. on Friday, September 13, 1991}.
(d) S ¼ {x: x ¼ 2y, y is a positive integer}.
4. Let the universal set be O ¼ [0,10], and deﬁne the
following subsets of O.
A ¼ 0; 2
½
Þ; B ¼ 2; 7
½
; C ¼ 5; 6
½
; D ¼ 2
f g;
E ¼ fx : x ¼ y1; y is an even positive integer  4g:
(a) Deﬁne the following sets:
A [ B; A \ B; A [ C; ðA [ DÞ \ B; B  C; A \ E; D \ B
(b) For each of the sets in (a), indicate whether the set is
ﬁnite, countably inﬁnite, or uncountably inﬁnite.
5. Let the universal set be deﬁned by O ¼ [5,5], and
deﬁne the following subsets of O:
A1 ¼ ½2; 1Þ
A2 ¼ 1; 2
ð
Þ
A3 ¼ 2; 5
½

A4 ¼ ½5; 2
Also, deﬁne an index set I ¼ {1,3,4}.
(a) Deﬁne [i2IAi.
(b) Deﬁne [4
i¼1 Ai
(c) Deﬁne A1 \ A2
(d) Deﬁne A4  A1
(e) Deﬁne A4
6. Deﬁne the universal set, O, as O ¼ {x : 0  x  5 or
10  x  20}, and deﬁne the following subsets of O as
A1 ¼ fx : 0  x< 2:5g;
A2 ¼ fx : 15 <x  20g;
A3 ¼ fx : 2:5  x  5 or 10  x  20g;
A4 ¼ fx : 0  x  5 or 10  x  15g:
In addition, deﬁne the following two index sets as:
I1 ¼ 1; 3
f
g; I2 ¼ 1; 4
f
g:
Deﬁne the following sets:
(a) [i2I1Ai
(b) \4
i¼1 Ai
(c) \i2I2Ai
(d) \2
i¼1 Ai
(e) A1  A2
(f) A4  A3
(g) A3
(h) A2  [i2I1Ai
7. In each situation below indicate where the relation is
a function. If so, determine the domain and range of the
function.
(a) A ¼ [0,10), B ¼ [0, 1n(11)], S ¼ {(x,y): y ¼ 1n(1 + x),
(x,y) ∈A  B}
(b) Consider S1, the inverse of S in (a).
(c) A ¼ R2
 0, B ¼ [0,1), S ¼ {((x1, x2), y): y ¼ 5 x1 x2
2, ((x1,
x2), y) ∈A  B}
(d) Consider S1, the inverse of S in (c).
8. For each relation below, state whether the relation is
a function, and state whether an inverse function exists.
Explicitly deﬁne the inverse function if it exists.
(a) Let P ¼ {$.01, $.02,. . .,$1.00, $1.01,. . .} represent a set
of possible prices for a given commodity, and let
Q ¼ [0,1)
represent
possible
levels
of
quantity
demanded. Deﬁne S: P ! Q as S ¼ {(p,q): q ¼ 20p1.5,
(p,q) ∈P  Q}
(b) Let A ¼ {D: D ¼ {(x1, x2): x1 ∈[a1, b1], x2 ∈[a2, b2]},
a1 < b1, a2 < b2} be a set of rectangular sets. Deﬁne
S: A ! R as
S ¼
D; y
ð
Þ : y ¼ area of D;
D; y
ð
Þ 2 A  Rþ
f
g
732
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

9. Let A ¼ (0,1), and examine the following relation
on A:
S ¼ f x; y
ð
Þ : y ¼ 2 þ 3x;
x; y
ð
Þ 2 A2Þg:
(a) Is 2S8?
(b) Is S a function?
(c) Does an inverse function exist?
(d) If you can, deﬁne f(2) and f1(5).
(e) What is D(S)? What is R(S)?
10. Deﬁne a universal set as O ¼ {x:0  x  5}, and con-
sider the set function
PðAÞ ¼ :5
Z
x2A
xdx þ 12:5
where the domain of the set function is all subsets A  O
of the form A ¼ [a, b], for 0  a  b  5.
(a) What is the image of the set A ¼ [0, 2] under P?
(b) What is the image of O under P?
(c) What is the image of set A ¼ [3,3] under P?
(d) What is the range of the set function?
(e) What is the inverse image of 2?
11. Deﬁne a set function that will assign the appropriate
area to all rectangles of the form [x1, x2]  [y1, y2], x2  x1
and y2  y1, contained in R 2. Be sure to identify the
domain and range of the set function.
12. A statistics class has 20 students in attendance, and
in the room where the class meets, there are 25 desks
available for the students. How many different ways can
the students leave ﬁve desks unoccupied?
13. There are 15 students in an econometrics class that
you are attending.
(a) How many different ways can a three-person com-
mittee be formed to give a class report?
(b) Of the number of possible three-person committees
indicated in (a), how many involve you?
14. Competing for the title of Miss America are 50
contestants from each of the 50 states plus one contestant
from the District of Columbia. How many different ways
can the contestants be assigned the titles of Miss Amer-
ica, ﬁrst runner up,. . ., fourth runner up?
15. Let A1 ¼ {x: x is a positive integer}, A2 ¼ {1, 2, 3, 4, 5},
B ¼ {(x1, x2): (x1, x2) ∈A1  A2, x1  x2}, and yi ¼ i2. Cal-
culate the values of the following sums.
(a) P
x2A2 x
(b) P
i2A2 yi
(c) P
x12A1
P
x22A2 1=2
ð
Þx1x2
2
(d) P
x2 A1A2
ð
Þ 1=3
ð
Þx
(e) P
x1;x2
ð
Þ2B
x1 þ x2
ð
Þ
16. Let A1 ¼ [0, 1), A2 ¼ [1, 10], and B ¼ {(x1, x2): (x1,
x2) ∈A1  A2, x2 > x1}. Calculate the values of the fol-
lowing integrals.
(a)
Ð
x2A1 1=2
ð
Þex=2dx
(b)
Ð
x12A1
Ð
x22A2 x2ex1dx2dx1
(c)
Ð
x1;x2
ð
Þ2B
x1 þ x2
ð
Þdx1dx2
(d)
R 2
0
Ð
x22A1\A2 x1x2
2dx2dx1
Problems
733

Useful Tables
Table B.1
Cumulative normal distribution FðxÞ ¼
R x
1
1ﬃﬃﬃﬃ
2p
p
et2=2dt
x
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.0
0.5000
0.5040
0.5080
0.5120
0.5160
0.5199
0.5239
0.5279
0.5319
0.5359
0.1
0.5398
0.5438
0.5478
0.5517
0.5557
0.5596
0.5636
0.5675
0.5714
0.5753
0.2
0.5793
0.5832
0.5871
0.5910
0.5948
0.5987
0.6026
0.6064
0.6103
0.6141
0.3
0.6179
0.6217
0.6255
0.6293
0.6331
0.6368
0.6406
0.6443
0.6480
0.6517
0.4
0.6554
0.6591
0.6628
0.6664
0.6700
0.6736
0.6772
0.6808
0.6844
0.6879
0.5
0.6915
0.6950
0.6985
0.7019
0.7054
0.7088
0.7123
0.7157
0.7190
0.7224
0.6
0.7257
0.7291
0.7324
0.7357
0.7389
0.7422
0.7454
0.7486
0.7517
0.7549
0.7
0.7580
0.7611
0.7642
0.7673
0.7704
0.7734
0.7764
0.7794
0.7823
0.7852
0.8
0.7881
0.7910
0.7939
0.7967
0.7995
0.8023
0.8051
0.8078
0.8106
0.8133
0.9
0.8159
0.8186
0.8212
0.8238
0.8264
0.8289
0.8315
0.8340
0.8365
0.8389
1.0
0.8413
0.8438
0.8461
0.8485
0.8508
0.8531
0.8554
0.8577
0.8599
0.8621
1.1
0.8643
0.8665
0.8686
0.8708
0.8729
0.8749
0.8770
0.8790
0.8810
0.8830
1.2
0.8849
0.8869
0.8888
0.8907
0.8925
0.8944
0.8962
0.8980
0.8997
0.9015
1.3
0.9032
0.9049
0.9066
0.9082
0.9099
0.9115
0.9131
0.9147
0.9162
0.9177
1.4
0.9192
0.9207
0.9222
0.9236
0.9251
0.9265
0.9279
0.9292
0.9306
0.9319
1.5
0.9332
0.9345
0.9357
0.9370
0.9382
0.9394
0.9406
0.9418
0.9429
0.9441
1.6
0.9452
0.9463
0.9474
0.9484
0.9495
0.9505
0.9515
0.9525
0.9535
0.9545
1.7
0.9554
0.9564
0.9573
0.9582
0.9591
0.9599
0.9608
0.9616
0.9625
0.9633
1.8
0.9641
0.9649
0.9656
0.9664
0.9671
0.9678
0.9686
0.9693
0.9699
0.9706
1.9
0.9713
0.9719
0.9726
0.9732
0.9738
0.9744
0.9750
0.9756
0.9761
0.9767
2.0
0.9772
0.9778
0.9783
0.9788
0.9793
0.9798
0.9803
0.9808
0.9812
0.9817
2.1
0.9821
0.9826
0.9830
0.9830
0.9834
0.9838
0.9846
0.9850
0.9854
0.9857
2.2
0.9861
0.9864
0.9868
0.9871
0.9875
0.9878
0.9881
0.9884
0.9887
0.9890
2.3
0.9893
0.9896
0.9898
0.9901
0.9904
0.9906
0.9909
0.9911
0.9913
0.9916
2.4
0.9918
0.9920
0.9922
0.9925
0.9927
0.9929
0.9931
0.9932
0.9934
0.9936
2.5
0.9938
0.9940
0.9941
0.9943
0.9945
0.9946
0.9948
0.9949
0.9951
0.9952
2.6
0.9953
0.9955
0.9956
0.9957
0.9959
0.9960
0.9961
0.9962
0.9963
0.9964
2.7
0.9965
0.9966
0.9967
0.9968
0.9969
0.9970
0.9971
0.9972
0.9973
0.9974
2.8
0.9974
0.9975
0.9976
0.9977
0.9978
0.9979
0.9979
0.9979
0.9980
0.9981
2.9
0.9981
0.9982
0.9982
0.9983
0.9984
0.9984
0.9985
0.9985
0.9986
0.9986
3.0
0.9987
0.9987
0.9987
0.9988
0.9988
0.9989
0.9989
0.9989
0.9990
0.9990
3.1
0.9990
0.9991
0.9991
0.9991
0.9992
0.9992
0.9992
0.9992
0.9993
0.9993
3.2
0.9993
0.9993
0.9994
0.9994
0.9994
0.9994
0.9994
0.9995
0.9995
0.9995
3.3
0.9995
0.9995
0.9995
0.9996
0.9996
0.9996
0.9996
0.9996
0.9996
0.9997
3.4
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9997
0.9998
Source: Reprinted, by permission of the publisher, from A. M. Mood, F. A. Graybill, and D. C. Boes, Introduction to the Theory of
Statistics, 3d ed., New York: McGraw-Hill, 1974, p. 552
Problems
735

Table B.2
Student’s t distribution. The ﬁrst column lists
the number of degrees of freedom (v). The headings
of the other columns give probabilities (P) for t
to exceed the entry value. Use symmetry
for negative t values
P
v
0.10
0.05
0.025
0.01
0.005
1
3.078
6.314
12.706
31.821
63.657
2
1.886
2.920
4.303
6.965
9.925
3
1.638
2.353
3.182
4.541
5.841
4
1.533
2.132
2.776
3.747
4.604
5
1.476
2.015
2.571
3.365
4.032
6
1.440
1.943
2.447
3.143
3.707
7
1.415
1.895
2.365
2.998
3.499
8
1.397
1.860
2.306
2.896
3.355
9
1.383
1.833
2.262
2.821
3.250
10
1.372
1.812
2.228
2.764
3.169
11
1.363
1.796
2.201
2.718
3.106
12
1.356
1.782
2.179
2.681
3.055
13
1.350
1.771
2.160
2.650
3.012
14
1.345
1.761
2.145
2.624
2.977
15
1.341
1.753
2.131
2.602
2.947
16
1.337
1.746
2.120
2.583
2.921
17
1.333
1.740
2.110
2.567
2.898
18
1.330
1.734
2.101
2.552
2.878
19
1.328
1.729
2.093
2.539
2.861
20
1.325
1.725
2.086
2.528
2.845
21
1.323
1.721
2.080
2.518
2.831
22
1.321
1.717
2.074
2.508
2.819
23
1.319
1.714
2.069
2.500
2.807
24
1.318
1.711
2.064
2.492
2.797
25
1.316
1.708
2.060
2.485
2.787
26
1.315
1.706
2.056
2.479
2.779
27
1.314
1.703
2.052
2.473
2.771
28
1.313
1.701
2.048
2.467
2.763
29
1.311
1.699
2.045
2.462
2.756
30
1.310
1.697
2.042
2.457
2.750
40
1.303
1.684
2.021
2.423
2.704
60
1.296
1.671
2.000
2.390
2.660
120
1.289
1.658
1.980
2.358
2.617
1
1.282
1.645
1.960
2.326
2.576
Source: Reprinted, by permission of the publisher, from P. G. Hoel, Introduction to Mathematical Statistics,
4th ed., New York: John Wiley and Sons, Inc., 1971, p. 393
736
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Table B.3
Chi-square distribution. The ﬁrst column lists the number of degrees of freedom (v). The headings of the
other columns give probabilities (P) for the w2
v random variable to exceed the entry value
P
0.995
0.990
0.975
0.950
0.900
0.750
v
1
392704  1010
157088  109
982069  109
393214  108
0.0157908
0.1015308
2
0.0100251
0.0201007
0.0506356
0.102587
0.210720
0.575364
3
0.0717212
0.114832
0.215795
0.351846
0.584375
1.212534
4
0.206990
0.297110
0.484419
0.710721
1.063623
1.92255
5
0.411740
0.554300
0.831211
1.145476
1.61031
2.67460
6
0.675727
0.872085
1.237347
1.63539
2.20413
3.45460
7
0.989265
1.239043
1.68987
2.16735
2.83311
4.25485
8
1.344419
1.646482
2.17973
2.73264
3.48954
5.07064
9
1.734926
2.087912
2.70039
3.32511
1.16816
5.89883
10
2.15585
2.55821
3.24697
3.94030
4.86518
6.73720
11
2.60321
3.15347
3.81575
4.57481
5.57779
7.58412
12
3.07382
3.57056
4.40379
5.22603
6.30380
8.43842
13
3.56503
4.10691
5.00874
5.89186
7.04150
9.29906
14
4.07468
4.66043
5.62872
6.57063
7.78953
10.1653
15
4.60094
5.22935
6.26214
7.26094
8.54675
11.0365
16
5.14224
5.81221
6.90766
7.96164
9.31223
11.9122
17
5.69724
6.40776
7.56418
8.67176
10.0852
12.7919
18
6.26481
7.01491
8.23075
9.39046
10.8649
13.6753
19
6.84398
7.63273
8.90655
10.1170
11.6509
14.5620
20
7.43386
8.26040
9.59083
10.8508
12.4426
15.4518
21
8.03366
8.89720
10.28293
11.5613
13.2396
16.3444
22
8.64272
9.54279
10.3923
12.3380
14.0415
17.2396
23
9.26042
10.19567
11.6885
13.0905
14.8479
18.1373
24
9.88623
10.8564
12.4011
13.8484
15.6587
19.0372
25
10.5197
11.5240
13.1197
14.6114
16.4734
19.9393
26
11.1603
12.1981
13.8439
15.3791
17.2919
20.8434
27
11.8076
12.8786
14.5733
16.1513
18.1138
21.7494
28
12.4613
13.5648
15.3079
16.9279
18.9392
22.6572
29
13.1211
14.2565
16.0471
17.7083
19.7677
23.5666
30
13.7867
14.9535
16.7908
18.4926
20.5992
24.4776
40
20.7065
22.1643
24.4331
26.5093
29.0505
33.6603
50
27.9907
29.7067
32.3574
34.7642
37.6886
42.9421
60
35.5346
37.4848
40.4817
43.1879
46.4589
52.2938
70
43.2752
45.4418
48.7576
51.7393
55.3290
61.6983
80
51.1720
53.5400
57.1532
60.3915
64.2778
71.1445
90
59.1963
61.7541
65.6466
69.1260
73.2912
80.6247
100
67.3276
70.0648
74.2219
77.9295
82.3581
90.1332
Problems
737

Table B.3
(continued)
P
0.500
0.250
0.100
0.050
0.025
0.010
0.005
v
1
0.454937
1.32330
2.70554
3.84146
5.02389
6.63490
7.87944
2
1.38629
2.77259
4.60517
5.99147
7.37776
9.21034
10.5966
3
2.36597
1.10835
6.25139
7.81473
9.34840
11.3449
12.8381
4
3.35670
5.38527
7.77944
9.48773
11.1433
13.2767
14.8602
5
4.35146
6.62568
9.23635
11.0705
12.8325
15.0863
16.7496
6
5.34812
7.84080
10.6446
12.5916
14.4494
16.8119
18.5476
7
6.34581
9.03715
12.0170
14.0671
16.0128
18.4753
20.2777
8
7.34412
10.2188
13.3616
15.5073
17.5346
20.0902
21.9550
9
8.34283
11.3887
14.6837
16.9190
19.0228
21.6660
23.5893
10
9.34182
12.5489
15.9871
18.3070
20.4831
23.2093
25.1882
11
10.3410
13.7007
17.2750
19.6751
21.9200
24.7250
26.7569
12
11.3403
14.8454
18.5494
21.0261
23.3367
26.2170
28.2995
13
12.3398
15.9839
19.8119
22.3621
24.7356
27.6883
29.8194
14
13.3393
17.1170
21.0642
23.6848
26.1190
29.1413
31.3193
15
14.3389
18.2451
22.3072
24.9958
27.4884
30.5779
32.8013
16
15.3385
19.3688
23.5418
26.2962
28.8454
31.9999
34.2672
17
16.3381
20.4887
24.4690
27.5871
30.1910
33.4087
35.7185
18
17.3379
21.6049
25.9894
28.8693
31.5264
34.8053
37.1564
19
18.3376
22.7178
27.2036
30.1435
32.8523
36.1908
38.5822
20
19.3374
23.8277
28.4120
31.4104
34.1696
37.5662
39.9968
21
20.3372
24.9348
29.6151
32.6705
35.4789
38.9321
41.4010
22
21.3370
26.0393
30.8133
33.9244
36.7807
40.2894
42.7956
23
22.3369
27.1413
32.0069
35.1725
38.0757
41.6384
44.1813
24
23.3367
28.2412
33.1963
36.4151
39.3641
42.9798
45.5585
25
24.3366
29.3389
34.3816
37.6525
40.6465
44.3141
46.9278
26
25.3364
30.4345
35.5631
38.8852
41.9232
45.6417
48.2899
27
26.3363
31.5284
36.7412
40.1133
43.1944
46.9630
49.6449
28
27.3363
32.6205
37.9159
41.3372
44.4607
48.2782
50.9933
29
28.3362
33.7109
39.0875
42.5569
45.7222
49.5879
52.3356
30
29.3360
34.7998
40.2560
43.7729
46.9792
50.8922
53.6720
40
39.3354
45.6160
51.8050
55.7585
59.3417
63.6907
66.7659
50
49.3349
56.3336
63.1671
67.5048
71.4202
76.1539
79.4900
60
59.3347
66.9814
74.3970
79.0819
83.2976
88.3794
91.9517
70
69.3344
77.5766
85.5271
90.5312
95.0231
100.425
104.215
80
79.3343
88.1303
96.5782
101.879
106.629
112.329
116.321
90
89.3342
98.6499
107.565
113.145
118.136
124.116
128.299
100
99.3341
109.141
118.498
124.342
129.561
135.807
140.169
Source: Reprinted, by permission of the Biometrika Trustees from C. M. Thompson, “Tables of Percentage Points of the
w2 Distribution,” Biometrika 32 (1941): 188–189
738
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Table B.4
F-distribution: 5 % points. The ﬁrst column lists the number of denominator degrees of freedom (v2).
The headings of the other columns list the numerator degrees of freedom (v1). The table entry is the value of c for
which PðFv1;v2  cÞ ¼ :05
v1
1
2
3
4
5
6
7
8
9
v2
1
161.45
199.50
215.71
224.58
230.16
233.99
236.77
238.88
240.54
2
18.513
19.000
19.164
19.247
19.296
19.330
19.353
19.371
19.385
3
10.128
9.5521
9.2766
9.1172
9.0135
8.9406
8.8868
8.8452
8.8123
4
7.7086
6.9443
6.5914
6.3883
6.3560
6.1631
6.0942
6.0410
5.9988
5
6.6079
5.7861
5.4095
5.1922
5.0503
4.9503
4.8759
4.8183
4.7725
6
5.9874
5.1433
4.7571
4.5337
1.3874
4.2839
4.2066
4.1468
4.0990
7
5.5914
4.7374
4.3468
4.1203
3.9715
3.8660
3.7870
3.7257
3.6767
8
5.3177
4.4590
4.0662
3.8378
3.6875
3.5806
3.5005
3.4381
3.3881
9
5.1174
4.2565
3.8626
3.6331
3.4817
3.3738
3.2927
3.2296
3.1789
10
4.9646
4.1028
3.7083
3.4780
3.3258
3.2172
3.1355
3.0717
3.0204
11
4.8443
3.9823
3.5874
3.3567
3.2039
3.0946
3.0123
2.9480
2.8962
12
4.7472
3.8856
3.4903
3.2592
3.1059
2.9961
3.9134
2.8486
2.7964
13
4.6672
3.8056
3.4105
3.1791
3.0254
2.9153
2.8321
2.7669
2.7144
14
4.6001
3.7389
3.3439
3.1122
2.9582
2.8477
2.7642
2.6987
2.6458
15
4.5431
3.6823
3.2874
3.0556
2.9013
2.7905
2.7066
2.6408
2.5876
16
4.4940
3.6337
3.2389
3.0069
2.8524
2.7413
2.6572
2.5911
2.5377
17
4.4513
3.5915
3.1968
2.9647
2.8100
2.6987
2.6143
2.5480
2.4943
18
4.4139
3.5546
3.1599
2.9277
2.7729
2.6613
2.5767
2.5102
2.4563
19
4.3808
3.5219
3.1274
2.8951
2.7401
2.6283
2.5435
2.4768
2.4227
20
4.3513
3.4928
3.0984
2.8661
2.7109
2.5990
2.5140
2.4471
2.3928
21
4.3248
3.4668
3.0725
2.8401
2.6848
2.5727
2.4876
2.4205
2.3661
22
4.3009
3.4434
3.0491
2.8167
2.6613
2.5491
2.4638
2.3965
2.3419
23
4.2793
3.4221
3.0280
2.7955
2.6400
2.5277
2.4422
2.3748
2.3201
24
4.2597
3.4028
3.0088
3.7763
2.6207
2.5082
2.4226
2.3551
2.3002
25
4.2417
3.3852
2.9912
2.7587
2.6030
2.4904
2.4047
2.3371
2.2821
26
4.2252
3.3690
2.9751
2.7426
2.5868
2.4741
2.3883
2.3205
2.2655
27
4.2100
3.3541
2.9604
2.7278
2.5719
2.4591
2.3732
2.3053
2.2501
28
4.1960
3.3404
2.9467
2.7141
2.5581
2.4453
2.3593
2.2913
2.2360
29
4.1830
3.3277
2.9340
2.7014
2.5454
2.4324
2.3463
2.2782
2.2229
30
4.1709
3.3158
2.9223
2.6896
2.5336
2.4205
2.3343
2.2662
2.2107
40
4.0848
3.2317
2.8387
2.6060
2.4495
2.3359
2.2490
2.1802
2.1240
60
4.0012
3.1504
2.7581
2.5252
2.3683
2.2540
2.1665
2.0970
2.0401
120
3.9201
3.0718
2.6802
2.4472
2.2900
2.1750
2.0867
2.0164
1.9588
1
3.8415
2.9957
2.6049
2.3719
2.2141
2.0986
2.0096
1.9354
1.8799
Problems
739

Table B.4
(continued)
v1
10
12
15
20
24
30
40
60
120
1
v2
1
241.88
243.91
245.95
248.01
249.05
250.09
251.14
252.20
253.25
254.32
2
19.396
19.413
19.429
19.446
19.454
19.462
19.471
19.479
19.487
19.496
3
8.7855
8.7446
8.7029
8.6602
8.6385
8.6166
8.5944
8.5720
8.5494
8.5265
4
5.9644
5.9117
5.8578
5.8025
5.7744
5.7459
5.7170
5.6878
5.6581
5.6281
5
4.7351
4.6777
4.6188
4.5581
4.5272
4.4957
4.4638
4.4314
4.3984
4.3650
6
4.0600
3.9999
3.9381
3.8742
3.8415
3.8082
3.7743
3.7398
3.7047
3.6688
7
3.6365
3.5747
3.5108
3.4445
3.4105
3.3758
3.3404
3.3043
3.2674
3.2298
8
3.3472
3.2840
3.2184
3.1503
3.1152
3.0794
3.0428
3.0053
2.9669
2.9276
9
3.1373
3.0729
3.0061
2.9365
2.9005
2.8637
2.8259
2.7872
2.7475
2.7067
10
2.9782
2.9130
2.8450
2.7740
2.7372
2.6996
2.6609
2.6211
2.5801
2.5379
11
2.8536
2.7876
2.7186
2.6464
2.6090
2.5705
2.5309
2.4901
2.4480
2.4045
12
2.7534
2.6866
2.6169
2.5436
2.5055
2.4663
2.4259
2.3842
2.3410
2.2962
13
2.6710
2.6037
2.5331
2.4589
2.4202
2.3803
2.3392
2.2966
2.2524
2.2064
14
2.6021
2.5342
2.4630
2.3879
2.3487
2.3082
2.2664
2.2230
2.1778
2.1307
15
2.5437
2.4753
2.4035
2.3275
2.2878
2.2468
2.2043
2.1601
2.1141
2.0658
16
2.4935
2.4247
2.3522
2.2756
2.2354
2.1938
2.1507
2.1058
2.0589
2.0096
17
2.4499
2.3807
2.3077
2.2304
2.1898
2.1477
2.1040
2.0584
2.0107
1.9604
18
2.4117
2.3421
2.2686
2.1906
2.1497
2.1071
2.0629
2.0166
1.9681
1.9168
19
2.3779
2.3080
2.2341
2.1555
2.1141
2.0712
2.0264
1.9796
1.9302
1.8780
20
2.3479
2.2776
2.2033
2.1242
2.0825
2.0391
1.9938
1.9464
1.8963
1.8432
21
2.3210
2.2504
2.1757
2.0960
2.0540
2.0102
1.9645
1.9165
1.8657
1.8117
22
2.2967
2.2258
2.1508
2.0707
2.0283
1.9842
1.9380
1.8895
1.8380
1.7831
23
2.2747
2.2036
2.1282
1.0476
2.0050
1.9605
1.9139
1.8649
1.8128
1.7570
24
2.2547
2.1834
2.1077
2.0267
1.9838
1.9390
1.8920
1.8424
1.7897
1.7331
25
2.2365
2.1649
2.0889
2.0075
1.9643
1.9192
1.8718
1.8217
1.7684
1.7110
26
2.2197
2.1479
2.0716
1.9898
1.9464
1.9010
1.8533
1.8027
1.7488
1.6906
27
2.2043
2.1323
2.0558
1.9736
1.9299
1.8842
1.8361
1.7851
1.7307
1.6717
28
2.1900
2.1179
2.0411
1.9586
1.9147
1.8687
1.8203
1.7689
1.7138
1.6541
29
2.1768
2.1045
2.0245
1.9446
1.9005
1.8543
1.8055
1.7537
1.6981
1.6377
30
2.1646
2.0921
2.0148
1.9317
1.8874
1.8409
1.7918
1.7396
1.6835
1.6223
40
2.0772
2.0035
1.9245
1.8389
1.7929
1.7444
1.6928
1.6373
1.5766
1.5089
60
1.9926
1.9174
1.8364
1.7480
1.7001
1.6491
1.5943
1.5343
1.4673
1.3893
120
1.9105
1.8337
1.7505
1.6587
1.6084
1.5543
1.4952
1.4290
1.3519
1.2539
1
1.8307
1.7522
1.6664
1.5705
1.5173
1.4591
1.3940
1.3180
1.2214
1.0000
Source: Reprinted, by permission of the Biometrika Trustees from M. Merrington and C. M. Thompson, “Tables of Percentage
Points of the Inverted Beta(F) Distribution,” Biometrika 33 (1943): 80–81
740
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

Table B.5
F-distribution: 1 % points. The ﬁrst column lists the number of denominator degrees of freedom (v2).
The headings of the other columns list the numerator degrees of freedom (v1). The table entry is the value of c for
which PðFv1;v2  cÞ ¼ :01
v1
1
2
3
4
5
6
7
8
9
v2
1
4,052.2
4,999.5
5,403.3
5,624.6
5,763.7
5,859.0
5,928.3
5,981.6
6,022.5
2
98.503
99.000
99.166
99.249
99.299
99.332
99.356
99.374
99.388
3
34.116
30.817
29.457
28.710
28.237
27.911
27.672
27.489
27.345
4
21.198
18.000
16.694
15.977
15.522
15.207
14.976
14.799
14.659
5
16.258
13.274
12.060
11.392
10.967
10.672
10.456
10.289
10.158
6
13.745
10.925
9.7795
9.1483
8.7459
8.4661
8.2600
8.1016
7.9761
7
12.246
9.5466
8.4513
7.8467
7.4604
7.1914
6.9928
6.8401
6.7188
8
11.259
8.6491
7.5910
7.0060
6.6318
6.3707
6.1776
6.0289
5.9106
9
10.561
8.0215
6.9919
6.4221
6.0569
5.8018
5.6129
5.4671
5.3511
10
10.044
7.5594
6.5523
5.9943
5.6363
5.3858
5.2001
5.0567
4.9424
11
9.6460
7.2057
6.2167
5.6683
5.3160
5.0692
4.8861
4.7445
4.6315
12
9.3302
6.9266
5.9526
5.4119
5.0643
4.8206
4.6395
4.4994
4.3875
13
9.0738
6.7010
5.7394
5.2053
4.8616
4.6204
4.4410
4.3021
4.1911
14
8.8616
6.5149
5.5639
5.0354
4.6950
4.4558
4.2779
4.1399
4.0297
15
8.6831
6.3589
5.4170
4.8932
4.5556
4.3183
4.1415
4.0045
3.8948
16
8.5310
6.2262
5.2922
4.7726
4.4374
4.2016
4.0259
3.8896
3.7804
17
8.3997
6.1121
5.1850
4.6690
4.3359
4.1015
3.9267
3.7910
3.6822
18
8.2854
6.0129
5.0919
4.5790
4.2479
4.0146
3.8406
3.7054
3.5971
19
8.1850
5.9259
5.0103
4.5003
4.1708
3.9386
3.7653
3.6305
3.5225
20
8.0906
5.8489
4.9382
4.4307
4.1027
3.8714
3.6987
3.5644
3.4567
21
8.0166
5.7804
4.8740
4.3688
4.0421
3.8117
3.6396
3.5056
3.3981
22
7.9454
5.7190
4.8166
4.3134
3.9880
3.7583
3.5867
3.4530
3.3458
23
7.8811
5.6637
4.7649
4.2635
3.9392
3.7102
3.5390
3.4057
3.2986
24
7.8229
5.6131
4.7181
4.2184
3.8951
3.6667
3.4959
3.3629
3.2560
25
7.7689
5.5680
4.6755
4.1774
3.8550
3.6272
3.4568
3.3239
3.2172
26
7.7213
5.5263
4.6366
4.1400
3.8183
3.5911
3.4210
3.2884
3.1818
27
7.6767
5.4881
4.6009
4.1056
3.7848
3.5580
3.3882
3.2558
3.1494
28
7.6356
5.4529
4.5681
4.0740
3.7539
3.5276
3.3581
3.2259
3.1195
29
7.5976
5.4205
4.5378
4.0449
3.7254
3.4995
3.3302
3.1982
3.0920
30
7.5625
5.3904
4.5097
4.0179
3.6990
3.4735
3.3045
3.1726
3.0665
40
7.3141
5.1785
4.3126
3.8283
3.5138
3.2910
3.1238
2.9930
2.8876
60
7.0771
4.9774
4.1259
3.6491
3.3389
3.1187
2.9530
2.8233
2.7185
120
6.8510
4.7865
3.9493
3.4796
3.1735
2.9559
2.7918
2.6629
2.5586
1
6.6349
4.6052
3.7816
3.3192
3.0173
2.8020
2.6393
2.5113
2.4073
Problems
741

Table B.5
(continued)
v1
10
12
15
20
24
30
40
60
120
1
v2
1 6,055.8
6,106.3
6,157.3
6,208.7
6,234.6
6,260.7
6,286.8
6,313.0
6,339.4
6,366.0
2
99.399
99.416
99.449
99.458
99.458
99.466
99.474
99.483
99.491
99.501
3
27.229
27.052
26.872
26.690
26.598
26.505
26.411
26.316
26.221
26.125
4
14.546
14.374
14.198
14.020
13.929
13.838
13.745
13.652
13.558
13.463
5
10.051
9.8883
9.7222
9.5527
9.4665
9.3793
9.2912
9.2020
9.1118
9.0204
6
7.8741
7.7183
7.5590
7.3958
7.3127
7.2285
7.1432
7.0568
6.9690
6.8801
7
6.6201
6.4691
6.3143
6.1554
6.0743
5.9921
5.9084
5.8236
5.7372
5.6495
8
5.8143
5.6668
5.5151
5.3591
5.2793
5.1981
5.1156
5.0316
4.9460
4.8588
9
5.2565
5.1114
4.9621
4.8080
4.7290
4.6486
4.5667
4.4831
4.3978
4.3105
10
4.8492
4.7059
4.5582
4.4054
4.3269
4.2469
4.1653
4.0819
3.9965
3.9090
11
4.5393
4.3974
4.2509
4.0990
4.0209
3.9411
3.8596
3.7761
3.6904
3.6025
12
4.2961
4.1553
4.0096
3.8584
3.7805
3.7008
3.6192
3.5355
3.4494
3.3608
13
4.1003
3.9603
3.8154
3.6646
3.5868
3.5070
3.4253
3.3413
3.2548
3.1654
14
3.9394
3.8001
3.6557
3.5052
3.4274
3.3476
3.2656
3.1813
3.0942
3.0040
15
3.8049
3.6662
3.5255
3.3719
3.2940
3.2141
3.1319
3.0471
2.9595
2.8684
16
3.6909
3.5527
3.4089
3.2588
3.1808
3.1007
3.0182
2.9330
2.8447
2.7528
17
3.5931
3.4552
3.3117
3.1615
3.0835
3.0032
2.9205
2.8348
2.7459
2.6530
18
3.5082
3.3706
3.2273
3.0771
2.9990
2.9185
3.8354
2.7493
2.6597
2.5660
19
3.4338
3.2965
3.1533
3.0031
2.9249
2.8442
2.7608
2.6742
2.5839
2.4893
20
3.3682
3.2311
3.0880
2.9377
2.8594
2.7785
2.6947
2.6077
2.5168
2.4212
21
3.3098
3.1729
3.0299
2.8796
2.8011
2.7200
2.6359
2.5484
2.4568
2.3603
22
3.2576
3.1209
2.9780
2.8274
2.7488
2.6675
2.5831
2.4951
2.4029
2.3055
23
3.2106
3.0740
2.9311
2.7805
2.7017
2.6202
2.5355
2.4471
2.3542
2.2559
24
3.1681
3.0316
2.8887
2.7380
2.6591
2.5773
2.4923
2.4035
2.3099
2.2107
25
3.1294
2.9331
2.8502
2.6993
2.6203
2.5383
2.4530
2.3667
2.2695
2.1694
26
3.0941
2.9576
2.8150
2.6640
2.5848
2.5026
2.4170
2.3273
2.2325
2.1315
27
3.0618
2.2956
2.7827
2.6316
2.5522
2.4699
2.3840
2.2938
2.1984
2.0965
28
3.0320
2.8959
2.7530
2.6017
2.5223
2.4397
2.3535
2.2629
2.1670
2.0642
29
3.0045
2.8685
2.7256
2.5742
2.4946
2.4118
2.3253
2.2344
2.1378
2.0342
30
2.9791
2.8431
2.7002
2.5487
2.4689
2.3680
2.2992
2.2079
2.1107
2.0062
40
2.8005
2.6648
2.5216
2.3689
2.2880
2.2034
2.1142
2.0194
1.9172
1.8047
60
2.6318
2.4961
2.3523
2.1978
2.1154
2.0285
1.9360
1.8363
1.7263
1.6006
120
2.4721
2.3363
2.1915
2.0346
1.9500
1.8600
1.7628
1.6557
1.5330
1.3805
1
2.3209
2.1848
2.0385
1.8783
1.7908
1.6964
1.5923
1.4730
1.3246
1.0000
Source: Reprinted, by permission of the Biometrika Trustees from M. Merrington and C. M. Thompson, “Tables of Percentage
Points of the Inverted Beta(F) Distribution,” Biometrika 33 (1943): 84–85
742
Appendix A
Math Review Appendix: Sets, Functions, Permutations, Combinations. . .

