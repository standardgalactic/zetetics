Optimal Sensor Placement for Bayesian Parametric
Identiﬁcation of Structures
Thesis by
Pinaky Bhattacharyya
In Partial Fulﬁllment of the Requirements for the
degree of
Doctor of Philosophy in Civil Engineering
CALIFORNIA INSTITUTE OF TECHNOLOGY
Pasadena, California
2017
Defended December 5, 2016

ii
© 2017
Pinaky Bhattacharyya
ORCID: [0000-0003-3773-0392]
All rights reserved

iii
ACKNOWLEDGEMENTS
This thesis would not have been possible without the mentorship and guidance of
my advisor, Prof. James (Jim) Beck, who has helped me learn and investigate the
subject matter, not in small part through long afternoon discussions at Steele House.
I am also very grateful for his help through tough spots in my career. Working with
him has been a great privilege and also very enjoyable.
I would also like to thank Professors Joel Burdick, Steven Low and Thomas Heaton
for their inputs as my thesis committee members.
The administrative staﬀat Gates-Thomas and Annenberg, the International Students
Program, Caltech Security and the rest of the Caltech support system, including
Ernie, have worked hard to keep my stay at Caltech smooth and for that I am very
thankful.
I would like to thank Professors Ravichandran and Bhattacharya for the teaching as-
sistantship opportunities and guidance; Dr. Michael Mello for several fun and infor-
mative hours in the mechanical engineering lab. Thanks also to Prof.Swaminathan
Krishnan for helping me get started at Caltech.
Among many others, I’d like to thank my friends Keng-Wit, Jonathan, Subrah-
manyam, Priya, Hemanth, Ramses, Ramya, Scott, Eric, Lucy and Utkarsh as well
as Sujeet, Vikas and Nisha for late night discussions over donuts at Winchell’s.
Finally, I would like to thank my family for their unwavering support and for putting
up with my odd hours and erratic contact through these six years.

iv
ABSTRACT
There exists a choice in where to place sensors to collect data for Bayesian model
updating and system identiﬁcation of structures. It is desirable to use an avail-
able deterministic predictive model, such as a ﬁnite-element model, along with
prior information on the uncertain model parameters and the uncertain accuracy
of the predictive model, to determine which optimal sensor locations should be
instrumented in the structure. In this thesis, an information-theoretic framework for
optimality is considered.
The mutual information between the uncertain model predictions for the data and
the uncertain model parameters is presented as a natural measure of reduction in
uncertainty to maximize over sensor conﬁgurations. A combinatorial search over
all sensor conﬁgurations is usually prohibitively expensive. A convex optimization
method is developed to provide a fast sub-optimal, but possibly optimal, sensor
conﬁguration when certain simplifying assumptions can be made about the chosen
stochastic model class for the structure. The optimization method is demonstrated
to work for a 50-story uniform shear building, with 20 sensors to be installed.
The stability of optimal sensor conﬁgurations under reﬁnement of the mesh of
the underlying ﬁnite-element model is investigated and related to the choice of
prediction-error correlations in the model. An example problem of placement of a
single sensor on the continuum of an elastic axial bar is solved analytically.
In order to solve the optimal sensor placement problem in the more general case,
numerical estimation of mutual information between the model predictions for the
data and the model parameters becomes necessary. To this end, a thermodynamic
integration scheme based on path sampling is developed with the aim of estimating
the entropy of the data prediction distribution. The scheme is demonstrated to work
for an example that uses synthetic data for model class comparison between linear
and Duﬃng oscillator model classes. The thermodynamic integration method is
then used to determine the optimal location of a single sensor for a two degree-of-
freedom oscillator model.

v
TABLE OF CONTENTS
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iii
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iv
Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
List of Illustrations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ix
Chapter I: Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1 General Overview . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3 Thesis outline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4 A note on terminology . . . . . . . . . . . . . . . . . . . . . . . . .
5
Chapter II: Mutual Information-based Optimal Sensor Placement . . . . . . .
7
2.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2 Bayesian model updating
. . . . . . . . . . . . . . . . . . . . . . .
8
2.3 Mutual information
. . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.4 Statement of the general problem . . . . . . . . . . . . . . . . . . .
13
2.5 Diﬀerences in mutual information . . . . . . . . . . . . . . . . . . .
15
2.6 Computing the mutual information
. . . . . . . . . . . . . . . . . .
15
2.7 Sensor rearrangement problem
. . . . . . . . . . . . . . . . . . . .
23
Chapter III: Eﬃcient Solution Using Convex Relaxation . . . . . . . . . . . .
26
3.1 Log-determinant formulation
. . . . . . . . . . . . . . . . . . . . .
26
3.2 Entropy-based optimal sensor location
. . . . . . . . . . . . . . . .
29
3.3 Convex relaxation of the combinatorial optimization problem
. . . .
31
3.4 Application to multistory structure
. . . . . . . . . . . . . . . . . .
36
Chapter IV: Modeling allowable sensor locations
. . . . . . . . . . . . . . .
46
4.1 Sensor placement on a continuum . . . . . . . . . . . . . . . . . . .
46
4.2 Finite-element mesh reﬁnement . . . . . . . . . . . . . . . . . . . .
48
4.3 Comparison with the ﬁnite selection-space case . . . . . . . . . . . .
52
4.4 Example: free-vibration of a cantilever beam with prediction-error
correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
Chapter V: Bayesian model class selection using thermodynamic integration .
63
5.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
5.2 Numerical Evaluation of Model Evidence . . . . . . . . . . . . . . .
64
5.3 Duﬃng oscillator . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
5.4 Numerical example . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5.5 Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
Chapter VI: Conclusions and Future Work . . . . . . . . . . . . . . . . . . .
85
6.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
Appendix A: Matrix Calculus Identities
. . . . . . . . . . . . . . . . . . . .
87

vi
A.1 Useful matrix calculus identities and properties . . . . . . . . . . . .
87
Appendix B: Derivatives for Convex Problem . . . . . . . . . . . . . . . . .
89
B.1 Calculation of parametric-gradient for modal response . . . . . . . .
89

vii
LIST OF ILLUSTRATIONS
Number
Page
1.1
Organization of the thesis
. . . . . . . . . . . . . . . . . . . . . . .
4
3.1
The resulting sensor locations after solving the convex optimization
problem for a 50-story structure . . . . . . . . . . . . . . . . . . . .
41
3.2
Another solution for the 50-story building but with diﬀerent prior
samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
4.1
Optimal location of the second sensor as a function of the prediction-
error correlation length scale (note that the ﬁrst sensor is always at
the free end of the cantilever beam) . . . . . . . . . . . . . . . . . .
61
4.2
Additional bits of information from the optimally placed second sen-
sor plotted as a function of the correlation length scale . . . . . . . .
62
5.1
The closed state-space trajectory of a double-well Duﬃng oscillator .
68
5.2
Period doubling bifurcation for forced, damped Duﬃng oscillator
. .
69
5.3
Chaotic trajectory for a forced, damped Duﬃng oscillator
. . . . . .
69
5.4
Displacement trajectory plots of the data data from a linear oscillator
(blue) and a Duﬃng oscillator (red) . . . . . . . . . . . . . . . . . .
73
5.5
Posterior component-wise marginal histograms for p(ζ|DL, ML)
. .
76
5.6
Posterior component-wise marginal histograms for p(ζ|DL, MD)
. .
77
5.7
Posterior component-wise marginal histograms for p(ζ|DD, ML)
. .
77
5.8
Posterior component-wise marginal histograms for p(ζ|DD, MD) . .
78
5.9
Displacement trajectory plots for data from a linear oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a
linear oscillator model class . . . . . . . . . . . . . . . . . . . . . .
79
5.10
Displacement trajectory plots for data from a linear oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a
Duﬃng oscillator model class . . . . . . . . . . . . . . . . . . . . .
79
5.11
Probability density function for the log-normal prior marginal for the
non-linear coeﬃcient, α
. . . . . . . . . . . . . . . . . . . . . . . .
80
5.12
Displacement trajectory plots for data from a Duﬃng oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a
linear oscillator model class . . . . . . . . . . . . . . . . . . . . . .
80

viii
5.13
Displacement trajectory plots for data from a Duﬃng oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a
Duﬃng oscillator model class . . . . . . . . . . . . . . . . . . . . .
81
5.14
Thermodynamic integration curve for the (DL, ML) case . . . . . . .
83

ix
LIST OF TABLES
Number
Page
2.1
Simulation results for entropy-of-evidence . . . . . . . . . . . . . . .
20
2.2
Conditional entropy
. . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3
Mutual information estimate for a single DOF system . . . . . . . . .
21
2.4
Single degree-of-freedom example: Simulation parameters . . . . . .
21
2.5
Two degree-of-freedom example: Simulation parameters . . . . . . .
22
2.6
Mutual information estimates for 2-DOF system
. . . . . . . . . . .
22
3.1
Simulation results
. . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.2
Description of various sensor conﬁgurations for comparative example
43
3.3
Comparison of information gain relative to optimal conﬁguration
. .
43
5.1
Speciﬁcation of the data-generating parameters for ML and MD . . .
71
5.2
Speciﬁcation of priors for ML and MD . . . . . . . . . . . . . . . .
72
5.3
Data generating samples and MAP estimates for each scenario . . . .
81
5.4
AIMS parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.5
Log-evidence values from thermodynamic integration
. . . . . . . .
82

1
C h a p t e r 1
INTRODUCTION
An important goal of the ﬁeld of structural monitoring is to use dynamic response
data from an instrumented structure to perform system identiﬁcation to quantify
engineering quantities of interest such as substructure stiﬀness, as well as to make
predictions about the future behavior of the structure. Response data is typically
available in the form of acceleration records from ambient vibrations, vibration tests
or seismic events. The parameters of a mechanical model that explains the behavior
of a structure are typically sought. To this end, parametric system identiﬁcation is
used to quantify the values of parameters of the model that is assumed to govern the
structure.
It is often not possible to postulate a true and exact model that explains the observa-
tions and makes good predictions. The primary reason for this is that the mechanical
model used to describe the structure, however complex, will usually fail to account
for every mechanical phenomenon, geometric and material detail correctly. In ad-
dition, it is not clear how complex and detailed a model is appropriate, since a
suﬃciently adjustable model would always be able to explain the given observations
at the cost of being able to make accurate predictions about future observations.
Given these limitations, in order to formulate a more meaningful system identi-
ﬁcation problem, it is important to account for modeling uncertainties related to
the prediction accuracy and the parameters associated with the mechanical model
being considered. This is done by specifying a probability model for predicting the
response based on the deterministic mechanical model of the structure and a proba-
bility distribution over the uncertain parameters of the mechanical model, together
known as a stochastic embedding of the parameterized deterministic model class,
to form a stochastic model class [1].
The axioms of Bayesian probability form a foundation for performing plausible
inference in a principled manner. Therefore, the data, together with the stochastic
embedding, allow for the update of the distribution over the uncertain parameters of
the model class.
The structural response data obtained is from locations in the structure that have
been chosen to be instrumented. There is an element of choice as to the nature of

2
instrumentation to be performed. Given that Bayesian inference will be done with
the data, there is a problem to be addressed: the choice of optimal data-set to be
collected for this inference.
This thesis will address the problem of optimal sensor location for Bayesian pa-
rameteric system identiﬁcation. For the purpose of illustration, consider a simple
version of the sensor-selection problem of selecting a certain number of locations to
be instrumented for structural monitoring, from a ﬁnite number of possibilities. For
instance, one could imagine being budgeted with 20 sensors to be used to instru-
ment a 50-story structure. In this situation, the number of possible conﬁgurations is
roughly 47 trillion! The evaluation of any objective function for each such conﬁg-
uration would be infeasible for even state-of-the-art computers available as of this
writing.
There are several important factors to be considered when formulating an optimal
sensor location problem:
• The measure of optimality used
• The type of data to be collected
• The choice of locations available for instrumentation
• Prior information about the structure to be instrumented
Each of these factors can have a signiﬁcant impact on the solution to the problem
and its solvability and will be addressed in this thesis.
The primary objective of this research is to introduce new methods for more eﬃ-
ciently solving existing formulations of the optimal sensor placement problem in the
context of structural engineering. In addition, the assumptions made by contempo-
rary formulations are relaxed to determine if the resulting more general framework
is solvable.
Optimality in the current research is based on objective functions that quantify an
information-theoretic measure of the anticipated uncertainty of the parameters to
be inferred, or quantities derived from them. This is in contrast to other objectives
such as those based on ﬁnancial cost, which have been considered in the literature
[2].

3
1.1
General Overview
In the domain of earthquake engineering and structural dynamics, the optimal
sensor location problem can be cast into a determinant maximization problem for
an entropy-based objective function, if certain assumptions can be made about the
stochastic model class. The ﬁrst is that it should be globally identiﬁable under the
selected data. In addition, it is assumed that a suﬃciently large number of data
points would be available so as to justify the use of the Laplace approximation to
the posterior that would be obtained. Finally, a small parametric uncertainty is
assumed. The optimization problem here is still one over all the possible sensor
conﬁgurations available The use of genetic algorithms was proposed in [3] to obtain
a sub-optimal and possibly near-optimal solution to the problem.
When such assumptions cannot be made for the problem under consideration, then
the computational complexity of and theoretical insight into the problem deteriorates
signiﬁcantly. More advanced theoretical and computational formulations are needed
to tackle such a problem.
So far, nothing has been said about what the possible sensor locations are or can be.
In some problems, the possible sensor locations can be a pre-ordained constraint.
For instance, consider the problem of selecting which stories of a building are to be
instrumented based on a shear-building model. If the possible location in the ﬂoor
for sensor placement in each story is ﬁxed, then the possible sensor locations are
already determined by the constraint of being only on ﬂoors.
In some problems, however, the possible sensor locations is a modeling choice.
There are certain issues that can arise in this scenario. For instance, if there exists a
highly informative “hotspot”, then the algorithm may have a tendency to cluster all
available sensors near that location. This in turn is a result of a ﬂaw in the modeling
of the mutual informativeness of the sensors in such a conﬁguration. This issue will
be addressed here.
1.2
Contributions
A contribution of this work has been to improve the existing approximate sensor
location problem by providing a sub-optimal solution much more eﬃciently than
combinatorial algorithms by using existing techniques from the theory of convex
optimization.
This method of solution may produce an optimal solution to the
problem, although there are no guarantees as to when this happens. Otherwise,
the method is able to provide a bound on how sub-optimal the resulting sensor

4
Bayesian model updating
Mutual information-based 
optimal sensor placement
2-DOF 
system
Model 
class 
selection
Log-determinant formulation
Combinatorial methods
Convex relaxation scheme
Uniform shear 
building
Allowable sensor 
locations
Cantilever 
beam
MCMC and 
thermodynamic 
integration
Figure 1.1: Organization of the thesis
conﬁguration is.
In the process of attempting to computationally solve the fully general information-
theoretic formulation, the evaluation of the Bayesian probabilistic quantity known
as the evidence for the stochastic model class is necessary. A framework known
as path sampling is shown to naturally arise out of a speciﬁc sampling algorithm.
The model evidence thus calculated is used to perform model class selection using
synthetic data from a linear oscillator versus one from a cubic non-linear, or Duﬃng,
oscillator.
Finally, the current research draws on existing information-theoretic concepts to
theoretically analyze and model certain speciﬁc situations that are encountered
in the sensor selection problem, such as the eﬀect of grid reﬁnement on sensor
selection, as well as sensor relocation.
1.3
Thesis outline
Figure 1.1 presents a ﬂowchart depicting the thesis outline.
Chapter 2 brieﬂy recaps the problem of Bayesian inference in a structural dynamics
context and lays out the sensor placement problem in its full generality as a mutual
information maximization problem while discussing theoretical and computational
aspects of the problem. Since the estimation of mutual information is very expensive,
the sensor selection problem is tackled for only a two degree-of-freedom problem.

5
In Chapter 3 the problem is tackled for the situation in which simplifying assumptions
about model class identiﬁablity and the suitability of applying Laplace’s approxi-
mation to the posterior that is to be obtained, can be made. Methods developed
in the ﬁeld of convex optimization are incorporated into the solution method for
the problem, in order to avoid a combinatorially large search for the optimal sensor
conﬁguration.
Chapter 4 discusses the eﬀect of choices regarding the allowable sensor locations.
Optimal sensor location on a continuous elastic bar is used as an example to illustrate
the sensor placement on a continuum and to illustrate the relationship between
the objective function and the mesh size. Finally, the eﬀect of prediction-error
correlation on sensor locations is discussed in the context of a cantilever beam.
Chapter 5 is a bit of a detour from the optimal sensor selection problem. The focus
here is on demonstrating how a path sampling scheme arises naturally out of the
AIMS Markov sampling algorithm for drawing posterior samples and can be used
to estimate model evidence. The problem of model class selection between a linear
and non-linear Duﬃng oscillator is tackled for data from either type of oscillator, to
demonstrate consistency with existing interpretations of model evidence.
1.4
A note on terminology
The nomenclature for probabilistic quantities in Bayesian inference is based on
the presumption that the data is already available. In the optimal sensor location
problem, the data is not available yet. Thus, expressions for already named quantities
but with the term data considered variable are called by a diﬀerent name. The term,
“stochastic forward model” is used for the probability density of the stochastic
prediction of the data given the parameters. Note that the prior distribution is still
referred to as the prior.

6
References
[1]
J. L. Beck, L. S. Katafygiotis, “Updating Models and Their Uncertainties. I:
Bayesian Statistical Framework,” Journal of Engineering Mechanics, vol. 124,
no. 4, pp. 455–461, Apr. 1998.
[2]
J. Mitrani-Resier, S. Wu, J. L. Beck, “Virtual Inspector and its application to
immediate pre-event and post-event earthquake loss and safety assessment of
buildings,” Natural Hazards, vol. 81, no. 3, pp. 1861–1878, Apr. 2016.
[3]
C. Papadimitriou, J. L. Beck, S.-K. Au, “Entropy-Based Optimal Sensor Lo-
cation for Structural Model Updating,” Journal of Vibration and Control, vol.
6, no. 5, pp. 781–800, Jan. 2000.

7
C h a p t e r 2
MUTUAL INFORMATION-BASED OPTIMAL SENSOR
PLACEMENT
The Bayesian optimal sensor placement problem, in its full generality, seeks to
maximize the mutual information between the uncertain parameters and the data to
be collected for the purpose of performing Bayesian inference. This maximization
is done over all possible sensor conﬁgurations for a given sensor budget.
Thus, in this chapter, a recap of the Bayesian model updating framework is ﬁrst
presented. The general expression for mutual information between prior and data for
typical stochastic dynamical systems encountered in structural dynamics is presented
next. Diﬀerent interpretations of the mutual information objective are considered.
It is established that the computation of mutual information in the context of dynam-
ical systems is computationally expensive. In order to make the problem tractable,
certain simplifying assumptions are made to yield a more tractable objective func-
tion. Later, Chapter 3 discusses an eﬃcient convex relaxation scheme to determine
possibly-optimal solution to the problem.
2.1
Introduction
The mutual information between two stochastic variables is an important quantity
in experimental design for Bayesian system identiﬁcation of stochastic dynamical
systems. In this context, several possible interpretations of mutual information exist.
Prior information on the parameter values is expressed in terms of a prior distri-
bution, p(θ|M). The stochastic forward model, typically denoted by p(y|θ, M),
prescribes the probability of observing a particular data set given the parameters.
The probability model class, M, is used to indicate a speciﬁc pairing of stochastic
forward model and prior. The stochastic forward model with the actual data D
substituted for y, when viewed simply as a function of the uncertain parameters, θ,
is called the likelihood function.
Bayes’ rule implies that the posterior probability density, p(θ|y, M), is proportional
to the product of the likelihood and prior probability densities. The reciprocal of
the normalizing factor is called the evidence, which when viewed as a function of

8
the stochastic predictions can be denoted by p(y|M). This can be determined using
the Total Probability Theorem, as p(y|M) =
∫
p(y|θ, M)p(θ|M)dθ
For a given data set, the parameter vector that maximizes the likelihood function
is known as the maximum likelihood estimate (MLE), usually denoted by ˆθMLE.
Similarly, the parameter vector that maximizes the posterior probability is known
as the maximum a posteriori (MAP) estimate of the parameters, denoted usually by
ˆθMAP.
2.2
Bayesian model updating
This section lays out the framework for Bayesian model updating for structural
dynamics which forms the basis for the optimal sensor placement problem. The
measured data on which inference is to be performed is assumed to be available
in the form of dynamic test data in the time domain. It is assumed, here, that the
complete system input can be determined given the parameters.
In a typical structural dynamics problem, one would have observed the acceleration,
velocity or displacement time history at a certain number of locations. We shall
assume, for the time being, that these locations are selected from among the degrees-
of-freedom of a ﬁnite-element model that governs the structure.
Denote the uncertain system parameters by θs. These could correspond, for instance,
to sub-structure stiﬀness, Rayleigh damping parameters or input ground acceleration
amplitude. In general, they would depend on the mechanical model of the structure
and the ﬁnite element approximation used to predict its response.
The prediction-error parameters are denoted by θe. When the prediction errors
are modeled as Gaussian, with a stationary and isotropic co-variance matrix with
diagnonal entries σ2, θe could just be the scalar, σ. For an anisotropic model for the
co-variance matrix without prediction error correlation between diﬀerent locations,
this could be a vector of variances corresponding to the diagonal entries of the
co-variance matrix.
Denote by yn the stochastic prediction of the quantity to be observed at the time-point
tn. Note that yn ∈RNo, where No is the number of observed degrees-of-freedom
out of a total of Nd degrees-of-freedom. The set of stochastic predictions for the
observations over the whole duration of the measurement is denoted by y1:N.
The vector of deterministic predictions at each time-step of interest is denoted by
xn(θs), where xn(θs) ∈RNd. The prediction-error is additive and is denoted by

9
ϵn. δ ∈{0, 1}Nd is a Boolean vector that indicates the presence or absence of a
measurement sensor, so that the sum of entries of δ equals the number of observed
DOFs, No.
A sensor selection matrix, So(δ) ∈RNo×Nd selects the components that are to be
observed from the full stochastic prediction given by the sum of the deterministic
prediction and noise terms.
The equation relating stochastic predictions to be observed, to the deterministic
predictions, is, then:
yn = So(δ) (xn(θs) + ϵn(θe))
(2.1)
In Equation (2.1), δ refers to a Boolean vector whose entries equal 1 when the re-
sponse from a particular degree-of-freedom is observed, and 0 when not. The matrix
So(δ) is a rectangular selection matrix and picks out those entries corresponding to
observed degrees of freedom.
Let M denote the probability model class. This includes the deterministic forward
model with a stochastic embedding. The stochastic embedding, in turn, is speciﬁed
by the stochastic forward model together with a prior distribution over the uncertain
parameters. Our statistical modeling assumptions lead us to the following stochastic
forward model:
p(y1:N|θ, M) =
N
Ö
n=1
p (yn|θ, M)
(2.2)
Bayes’ theorem can be used to perform the update as in Equation (2.3):
p(θ|y1:N, M) = p(y1:N|θ, M)p(θ|M)
p(y1:N|M)
(2.3)
The term in the denominator is called the evidence for the model class, M, under
the data, DN and is given by the Total Probability Theorem in Equation (2.4):
p(y1:N|M) =
∫
p(y1:N|θ, M)p(θ|M)dθ
(2.4)
This quantity will be dealt with in greater detail in Chapter 5 for the purposes of
model selection.
The Prediction-Error Model
The examples here will deal with the case where we model the prediction errors as
Gaussian with equal variance at every degree of freedom, and uncorrelated across

10
time and location, so that:
E

ϵi,nϵj,m

= σ2δijδmn where ϵi,n ∼N(·|0, σ2) i.i.d.
(2.5)
We then have an expression for the stochastic forward model:
p(y1:N|θ, M) =
N
Ö
n=1
N

yn|So(δ)xn(θ), σ2
(2.6)
Using Bayes’ theorem, the distribution for the posterior, given the data is:
p(θ|y1:N, M) = p(y1:N|θ, M)p(θ|M)
p(y1:N|M)
= c−1p(y1:N|θ, M)p(θ|M)
(2.7)
The evidence, c, for the model class in Equation (2.7) is treated simply as a normal-
izing constant for the purpose of this section. In later sections such as in Chapters
5, it will be viewed as a function of the stochastic predictions as well as the model
class.
The logarithm of the posterior is sometimes easier to work with. In the case of this
example, we have:
log p(θ|y1:N, M) = −
NNo
log 2πσ2 −
1
2σ2
N
Õ
n=1
∥yn −So(δ)xn(θs)∥2 + log p(θ|M) (2.8)
Note that when observations are actually made, then the collection of data at hand
is denoted by DN = { ˆyn}N
n=1 or ˆy1:N.
2.3
Mutual information
The mutual information between two stochastic variables is a degree of measure
about how much they imply about each others’ possible values. Formally, the mutual
information between two continuous stochastic variables is deﬁned as follows:
I(X; Z) = EX,Z log
 pX,Z(x, z)
pX(x)pZ(z)

(2.9)
The mutual information between two sets of stochastic variables is zero if and
only if they are independent, since the joint density factors into the marginals.
Additionally, if two stochastic variables are linearly correlated as in the case of a bi-
variate Gaussian with a non-zero correlation coeﬃcient, then the mutual information
controls the correlation coeﬃcient, ρ:
I(X; Z) = −1
2 log(1 −ρ2)
(2.10)

11
The mutual information can be expressed as a diﬀerence of two entropies:
I(X; Z) = H(X) −H(X|Z)
(2.11)
Equation (2.11) states that the mutual information is the diﬀerence in entropy of the
marginal distribution for one variable and the conditional entropy. The expression
is symmetric in either variable because of Bayes’ rule.
The conditional entropy is the entropy of one variable, conditioned on a partic-
ular value of the other and averaged over that value according to its probability
distribution. That is,
H(X|Z) =
∫
PZ(z)[H(X|Z = z)] dz
(2.12)
The mutual information is always non-negative, since conditioning always reduces
entropy or leaves it unchanged, on average.
In the Bayesian system identiﬁcation setting, the mutual information quantity has
several interpretations, that are discussed in the following sections.
Mutual information and Bayesian system identiﬁcation
With the usual notation, the mutual information between the stochastic predictions
for the observed data, y1:N, and the parameter vector, θ, is:
I(y1:N(δ); θ) = Ey1:N(δ),θ|M

ln
p(y1:N(δ), θ|M)
p(y1:N(δ)|M)p(θ|M)

(2.13)
Equation (2.13) emphasizes the dependence of the stochastic prediction for the data,
y1:N(δ), on the sensor conﬁguration, δ. This explicit dependence will be omitted
for convenience until a discussion of the optimal sensor location problem in Section
2.4.
Divergence from posterior to prior
The equation (2.13) can be rewritten after applying Bayes’ theorem to express mutual
information as the expected Kullback-Leibler (KL) divergence, or expected relative
entropy, from the posterior to the prior. That is:

12
I(y1:N(δ), θ) = Ey1:N(δ),θ|M

ln
p(y1:N(δ), θ|M)
p(y1:N(δ)|M)p(θ|M)

(2.14)
=
∫∫
p(y1:N(δ), θ|M)

ln p(θ|y1:N(δ), M)
p(θ|M)

dYdθ
(2.15)
=
∫∫
p(θ|y1:N(δ), M)p(y1:N(δ)|M)

ln p(θ|y1:N(δ), M)
p(θ|M)

dYdθ
(2.16)
= Ey1:N(δ)|MDKL[p(θ|y1:N(δ), M)||p(θ|M)]
(2.17)
Here, the Kullback-Leibler divergence from one continuous probability distribution,
P, to another, Q, speciﬁed by their probability density functions p(x) and q(x), is:
DKL(p(x)||q(x)) =
∫∞
−∞
p(x) log p(x)
q(x) dx
(2.18)
The KL-divergence in this case can also be referred to as the relative entropy of P
with respect to Q.
If one thinks of information maximization in the sensor placement framework,
then maximizing this expected KL divergence would be equivalent to choosing
the sensor conﬁguration that has the largest expected reduction in entropy of the
posterior distribution from the prior. Of all sensor conﬁgurations, the optimal one
would result in the maximum reduction in entropy, on average, upon measuring the
data.
Expression in terms of known quantities
The mutual information in a Bayesian system identiﬁcation framework, usually
involves only two speciﬁed distributions: the prior and the stochastic forward model.
We can use Bayes’ theorem to express Equation (2.13) in terms of these distributions
as:
I(y1:N(δ); θ) =
∫∫
p(y1:N(δ)|θ, M)p(θ|M)

log p(y1:N(δ)|θ, M)
p(y1:N(δ)|M)

dy1:N(δ)dθ
(2.19)
The denominator in the logarithm term is what one would associate with the evidence
term, based on given data. Here, the data is not available and this term is simply a
function obtained upon marginalizing out the parameters from the joint:

13
p(y1:N(δ)|M) =
∫
p(y1:N(δ)|θ, M)p(θ|M)dθ
(2.20)
In principle, allthesequantitiesshouldbecalculable, given p(θ) and p(y1:N(δ)|θ, M).
In practice, calculating these quantities turns out to be quite challenging. Section
2.6 addresses this issue.
2.4
Statement of the general problem
We propose to use the mutual information between the uncertain parameters and the
data variable as the objective function to be maximized over sensor conﬁguration.
Consider the objective function:
maximize
δ
I(y1:N(δ); θ)
subject to
δi ∈{0, 1},
i = 1, . . ., Nd
and
1Tδ = No.
(2.21)
Equation (2.19) can be used for the objective function for the problem in (2.21).
Recall that δ is a binary Nd-dimensional vector specifying the sensor conﬁguration
over the degrees of freedom in the system.
Log-determinant approximation
The mutual information criterion can be simpliﬁed to a more manageable log-
determinant criterion when the following assumptions can be made about the prob-
ability model class:
• The posterior distribution, upon collection of the data, is globally identiﬁable
• The Laplace approximation holds; that is, a suﬃcient of data points are
measured so that the posterior is approximately Gaussian
Under these assumptions, the expression for mutual information may be simpliﬁed as
follows. First, recognize the expression for mutual information as the KL-divergence
from the posterior to the prior:
I(y1:N(δ); θ) = Ey1:N(δ),θ

log p(θ|y1:N(δ))
p(θ)

(2.22)
Begin by noting that the prior term is irrelevant in the sensor placement problem
and may be discarded to yield the utility function, U(δ), as:
U(δ) = Ey1:N(δ) [p(θ|y1:N(δ)) log p(θ|y1:N(δ))]
(2.23)

14
Next, apply the Gaussian approximation to the posterior. That is,
p(θ|y1:N(δ)) ≈N(θ| ˆθ(y1:N), A−1( ˆθ(y1:N), δ))
(2.24)
Note that the precision matrix here is given by the Hessian of the logarithm of the
stochastic forward model:
[A( ˆθ(y1:N), δ)]pq = −∂2 log p(y1:N(δ)|θ)
∂θpθq
(2.25)
The utility function becomes is now simply related to the entropy of this approxi-
mately Gaussian posterior which may be expressed in terms of the log-determinant
of its precision matrix as:
⇒U(δ) = −Ey1:N(δ)

constant −1
2 log det A( ˆθ(y1:N), δ)

(2.26)
Finally, we assume that the expectation under the marginal distribution for the data
equals that under the prior or nominal parameters, so that:
U(δ) ≈constant + 1
2Eθ [log det A(θ, δ)]
(2.27)
Hence, the mutual information utility function can be approximated by a Gaussian-
like posterior entropy. Yet, there is still the matter of searching over possible sensor
conﬁgurations. The convex optimization technique developed in Chapter 3 reduces
the computational complexity of this search and provides expressions for the Hessian
mentioned in Equation (2.26).
Comparison of two sensor conﬁgurations
Consider the diﬀerence in mutual information between two sensor conﬁgurations:
∆I12
∆= I(y1:N(δ2); θ) −I(y1:N(δ1)|θ) = H(y1:N(δ1)|θ) −H(y1:N(δ2)|θ)
(2.28)
If the same approximations hold in either case, then we have:
∆I12 = 1
2Eθ [log det A(θ, δ2)] −1
2Eθ [log det A(θ, δ1)]
(2.29)
Now, note that the Hessian A(θ, δ) consists of two block diagonal parts corresponding
to the system parameters, θs and the single prediction error parameter, σ2. We may
write:
Eθ [log det A(θ, δ)] = Eθs [log det B(θs, δ)] + Eσ2

log C(σ2)

(2.30)

15
For the same number of sensors, No, the scalar factor, C(σ2) = NNo
2σ2 , cancels out in
the diﬀerence.
We are left with the result:
∆I12 = H(y1:N(δ1)|θ) −H(y1:N(δ2)|θ)
(2.31)
= 1
2Eθs [log det B(θs, δ2)] −1
2Eθs [log det B(θs, δ1)]
(2.32)
We can therefore compare the eﬀect of sensor conﬁguration on the diﬀerence in
diﬀerential entropies of the posteriors that would result in the mean in each case.
2.5
Diﬀerences in mutual information
The mutual information between two stochastic variables is the mean increase in
information of one variable upon learning the value of the other. This can be thought
of as the mean gain in the number of bits of Shannon information, if the logarithm
to the base 2 is used in the deﬁnition of entropy. (ref. CoverThomas)
In the expression above, the diﬀerence in values of the mutual information for
two sensor conﬁgurations, therefore corresponds to the mean increase in bits of
information gained relative to the prior for one conﬁguration, δ1 versus the other,
δ2.
2.6
Computing the mutual information
Recall that Equation (2.19) deﬁnes the mutual information that can in principle be
solved. A piece of that equation is the evidence term which is given in equation
(2.20).
To evaluate the mutual information, we split the expression from Eqn. (2.19) into
two terms using the diﬀerence of the logarithms:
I(y1:N(δ), θ) =
∫∫
p(y1:N(δ)|θ, M)p(θ|M)[log p(y1:N(δ)|θ, M)
−log p(y1:N(δ)|M)]dy1:N(δ)dθ
(2.33)
=
∫
p(θ|M)
∫
p(y1:N(δ)|θ, M) log p(y1:N(δ)|θ, M)dy1:N(δ)dθ
−
∫
p(y1:N(δ)|M) log p(y1:N(δ)|M) dy1:N(δ)
(2.34)
⇒I(y1:N(δ), θ) = H(y1:N(δ)) −H(y1:N(δ)|θ)
(2.35)
The conditional entropy is usually easy to compute: it is the expected conditional
entropy of the stochastic forward model over the parameters, which, in the case

16
of a conditionally independent Gaussian stochastic forward model reduces to the
expectation of a simple expression over the prediction-error uncertainty parameters
only as will be seen in the examples that follow.
The entropy of the marginal distribution for the data prediction, on the other hand,
is much harder to calculate in general, since an analytic expression for this marginal
is usually unavailable. In addition, a single evaluation of the integrand involves one
evaluation of evidence for the model class under that particular sample from the
prediction for the data. As will be explored in Chapter 5, this is not a trivial task.
Methods to compute mutual information between low-dimensional variables exist,
using samples from the joint distribution.
Monte Carlo approximation
Recall that mutual information can be decomposed into the diﬀerence of two en-
tropies:
I(y1:N(δ); θ) = H(y1:N(δ)) −H(y1:N(δ)|θ)
(2.36)
Evaluation of the conditional entropy (the second term) is usually trivial in many
situations such as when the prediction-errors are modeled as additive and inde-
pendently Gaussian. This would typically involve calculating the expectation of
the log-determinant of the co-variance matrix over the prediction-error uncertainty
parameters.
The challenging computation is that of the entropy of evidence. A naive Monte
Carlo approximation to this quantity is presented:
H(y1:N(δ)) = −
∫
p(y1:N(δ)) log p(y1:N(δ))y1:N(δ)
(2.37)
≈1
Ns
Ns
Õ
k=1
log p(y1:N(δ)(k))
(2.38)
Here, y1:N(δ)(k) is a sample from the joint distribution, p(y1:N(δ), θ). The variance
of a naive estimate of this might be quite high, since the data variable can be of a
high dimension. However, this example is illustrative for the purpose of discussing
the complexity of any sample-based estimate that uses roughly the same number of
samples.
For each sample of the data variable drawn from the joint distribution, one instance
of model evidence needs to be evaluated. This is an expensive computation that is

17
not straightforward, since again naive estimators tend to have high variance. To this
end, a method of evaluating log-evidence is discussed in Chapter 5.
Example: Gaussian data with uncertain mean
The purpose of this example is to give insight into what the expression for mutual
information may looks like. Consider a model class MG that speciﬁes a No-variate
Gaussian stochastic forward model:
p(y1:N(δ)|µ(θ), Σ(θ), MG) =
N
Ö
n=1
N(yn|µ(θ), Σ(θ))
(2.39)
=
1
(2π)NNo/2|Σ(θ)|N/2×
exp
 
−1
2
N
Õ
n=1
(yn −µ(θ))TΣ−1(θ)(yn −µ(θ))
!
(2.40)
It is also assumed that a prior model, p(θ|MG) is speciﬁed. In this case, the ﬁrst
term of Equation (2.35) is given by the expected entropy of a multivariate Gaussian
distribution:
H(y1:N|θ) = −Eθ
 N
2 log(2πe) + 1
2 log det Σ(θ)

(2.41)
= −NNo
2
log(2πe) −N
2 Eθ[log det Σ(θ)]
(2.42)
On the other hand, the entropy of the distribution over the prediction of the data, or
the entropy of the evidence density, is hard to calculate. This is because the marginal
likelihood, p(y1:N|M), and its logarithm needs to be evaluated for every integration
point over y1:N. There is usually no closed-form expression to p(y1:N|M), except
perhaps when the predictions are linear in the parameters which in turn have a
Gaussian prior, or when conjugate priors are employed. Again, these introduce
restrictions on the class of models that may be employed and are usually not appro-
priate modeling choices for dynamical systems. However, this example will look
at the case where the prior for the mean parameter is chosen to be conjugate to the
stochastic forward model.
To simplify this example further, assume that No = 1 so that the observations are
modeled as univariate Gaussian data. Let only the mean be uncertain and have a
Gaussian distribution, that is θ = µ ∼N(·|µ0, σ0). For simplicity, the prediction-
error standard deviation, σ, is taken as known.

18
The resulting marginal likelihood is Gaussian:
p(y1:N|MG) =
∫
p(y1:N|µ)p(µ|µ0, σ0) dµ
(2.43)
=
∫
1
(2πσ2)N/2
q
2πσ2
0
exp
"
−1
2σ2
N
Õ
n=1
(yn −µ)2 −(µ −µ0)2
2σ2
0
#
dµ
(2.44)
= N  y1:N|µ01, ˜Σ(σ0, σ)
(2.45)
where
| ˜Σ(σ0, σ)| = σ2N
 
1 +
Nσ2
0
σ2
!
(2.46)
The mutual information between the prediction of the data and the uncertain mean
parameter is therefore:
I(y1:N; µ) = −N
2 log(2πeσ2) + N
2 log(2πe) + 1
2 log | ˜Σ(σ0, σ)|
(2.47)
= N
2 log
 
1 +
Nσ2
0
σ2
!
(2.48)
By inspection, Equation (2.48) tells us the following:
1. More observations, N, result in a higher mutual information
2. The data to be observed, y1:N, is more informative about the uncertain mean
parameter, µ, for a larger prior variance, σ2
0
3. For this contrived example, when the known prediction-error variance, σ2, is
small, then this is an assertion that the observations are close to the predictions
and therefore more informative
To conclude this example, it is interesting to note that for N = 1, the expression
for mutual information in Equation (2.48) is the same as that for a Gaussian com-
munication channel with signal power σ2
0 and noise variance σ2, as described in
[1].

19
Example: Spring-mass oscillator
The simple case of a single degree-of-freedom dynamical system is considered next,
to provide insight into mutual information calculations in more complicated cases.
There is no analytical expression for the mutual information in this case.
Consider an undamped mechanical oscillator with uncertain natural frequency ω,
with unit initial displacement and starting from rest. The governing equation is:
Üx + ω2x = 0, with x(0) = 1, Ûx(0) = 0
(2.49)
The solution to the displacement time history of this oscillator is:
x(t) = cos(ωt)
(2.50)
We assume that data, y1:N = {yn}N
n=1, is obtained from points in time equally spaced
at ∆t. The equation for the stochastic prediction is:
yn = cos(ωn∆t) + ϵn
(2.51)
As in the previous example, we assume the prediction-error to be stationary and
Gaussian with uncertain variance σ2, so that the stochastic forward model may be
speciﬁed as:
p(y1:N|ω, σ2, MO) =
1
(2πσ2)N/2 exp
"
−1
2σ2
N
Õ
n=1
(yn −cos(ωn∆t))2
#
(2.52)
Let the uncertain parameters be distributed according to a prior, as ω ∼p(ω),
σ2 ∼p(σ2) and p(ω, σ2) = p(ω)p(σ2). Then, the model evidence is given by:
p(y1:N|MO) =
∫∫p(ω)p(σ2)
(2πσ2)N/2 exp
"
−1
2σ2
N
Õ
n=1
(yn −cos(ωn∆t))2
#
dωdσ2
(2.53)
Chapter 5 discusses the application of Asymptotically Independent Markov Sam-
pling (AIMS) and path sampling for the the purpose of evaluating model evidence,
or integrals of the form:
p(y1:N|M) =
∫
p(y1:N|θ, M)p(θ|M)dθ
(2.54)
As mentioned in Section 2.6, the computation of mutual information requires the
evaluation of several model evidence-like terms, each of which is typically quite
expensive to evaluate and requires a full run of AIMS.

20
In this example, the entropy-of-evidence term is determined as follows:
H(D) ≈1
Ns
Ns
Õ
k=1
log p(y(k)
1:N|M)
(2.55)
where
(y(k)
1:N, θ(k)) ∼p(y1:N, θ|M) = p(y1:N|θ, M)p(θ|M)
(2.56)
Equations (2.55) and (2.56) specify that the entropy of evidence according to its
Monte-Carlo approximations using samples from the predicted data variable. These
samples come from the joint density. One sample from the joint is obtained by
drawing a prior sample and running the stochastic forward model on it.
Each logarithmic term in the sum of Equation (2.55) is calculated using AIMS and
path sampling. The sample average is an estimator for the mean. The standard devi-
ation of the estimator of the mean is itself estimated using the standard relationship
between the sample and population variances.
Table 2.1: Simulation results for entropy-of-evidence
Quantity
Trapezoid
Gauss 7-point
Kronrod 15-point
Mean estimate for
-22.5746
-23.9345
-23.2962
entropy of evidence
The conditional entropy is evaluated much more easily, since it requires the expec-
tation of the log variance under the prior:
H(y1:N|θ) = N
2 log 2π + N
2 Eσ2[log σ2]
(2.57)
In this case, we don’t even need use a Monte-Carlo estimate, since we have already
speciﬁed that our prior standard deviation follows a log-uniform.
Table 2.2: Conditional entropy
Quantity
Analytic Value
Conditional entropy
-30.81061
Using the values for the trapezoidal estimate, the estimated value for mutual infor-
mation for this example is given in Table 2.3.

21
Table 2.3: Mutual information estimate for a single DOF system
Quantity
Mean
Mutual Information
8.2360
It should be noted that this evaluation is quite expensive. The simulation parameters
for this example are listed in Table 2.4.
Table 2.4: Single degree-of-freedom example: Simulation parameters
Quantity
Value
Number of log-evidence samples
5000
Number of AIMS samples
300 × 30 or 9000
Total forward evaluations
45 million
Number of time-steps
10
Time-step
0.2
Prior frequency
ω ∼U[1
2, 3
2]
Prior standard deviation
log σ ∼U[−5, −3]
Example: Two-DOF unidentiﬁable system
Consider a 2-DOF system, consisting of two masses connected using two springs,
with the end of one spring ﬁxed. The equations of motion for this system are given
by:
m Üx(t) +
"
k1 + k2
−k2
−k2
k2
#
x(t) = 0
(2.58)
with
x(0) =
(
1
1
)
(2.59)
We consider a unit mass, m = 1, for simplicity. This system can be diagonalized
using a transformation matrix Φ that relates the modal co-ordinates qj to the original
ones xi. The resulting decoupled equations of motion have solutions:
qj(t) = (Φ1j + Φ2j) cos(ωjt)
(2.60)
The solution to the original system, is therefore:
xi(t) =
Õ
j
Φij(Φ1j + Φ2j) cos(ωjt)
(2.61)

22
This system has the peculiar property that the forward model output at the second
degree-of-freedom can be matched for two very diﬀerent stiﬀness conﬁgurations.
The conﬁgurations [k1 = k∗
1, k2 = k∗
2] and [k1 = 2k∗
2, k2 = k∗
1/2] result in identical
predictions for x2(t). On the other hand, the output x1(t) is deﬁned uniquely.
For a speciﬁc prior, therefore, we expect to see a higher mutual information between
the displacement at the ﬁrst sensor location as opposed to the second. As in the
previous example, the prediction-error parameter, σ, is uncertain. For this example,
however, there are two uncertain system stiﬀness parameters, k1 and k2.
The
stochastic forward model in either case is speciﬁed by:
p(y1:N(δ = ei)|k1, k2, σ) =
1
(2πσ2)N/2 exp
"
−1
2σ2
N
Õ
n=1
(yn −xi,n(k1, k2))2
#
(2.62)
The parameters used for this simulation are listed in Table 2.5.
Table 2.5: Two degree-of-freedom example: Simulation parameters
Quantity
Value
Number of log-evidence samples
5000
Number of AIMS samples
300 × 30 or 9000
Total forward evaluations
45 million
Number of time-steps
10
Time-step
0.2
Prior k1
k1 ∼U[2, 6]
Prior k2
k2 ∼U[2, 6]
Prior standard deviation
log σ ∼U[−5, −3]
The mutual information is evaluated at both locations for each case and is presented
in Table 2.6.
Table 2.6: Mutual information estimates for 2-DOF system
Location
Mean
DOF1
4.7524
DOF2
5.0922
The lack of identiﬁability of the system with data from the second degree-of-freedom
surprisingly did not result in a lower information gain compared to the ﬁrst location.

23
This warrants further analysis that is relegated to future work at this time, to verify
that this is not an artefact of numerical computations.
Note that the number of time-steps was kept small and the measurement time-step
fairly quite in order to keep in check the dimensionality of the data variable, while
simultaneously being able to capture one whole period of oscillation for every
frequency in the range of the prior.
2.7
Sensor rearrangement problem
The optimal sensor placement problem, in the absence of previous instrumentation
of a structure, relies on a prior distribution over the parameters that has the maxi-
mum entropic uncertainty for its model class, possibly with location and variance
constraints.
When a structure has already been instrumented, however, a posterior distribution
over the parameters is usually available after performing a Bayesian update. Here,
we investigate whether or not a posterior could simply be used as the prior for a new
optimal sensor placement problem.
Assumptions
To begin with, we assume that the same model class is chosen for the structure to
be re-instrumented. Denote this model class by M.
Denote the data to be obtained from the new sensor conﬁguration by Y′
N′(δ′). The
data from the old conﬁguration is, before performing the update, denoted by YN(δ).
Then, we make the assumption that:
p(Y′
N′(δ′),YN(δ)|θ, M) = p(Y′
N′(δ′)|θ, M)p(YN(δ)|θ, M)
(2.63)
This states that the new data is conditionally independent of the old data, given the
parameter vector.
For convenience of notation, denoteYN(δ) by D1 andY′
N′(δ′) by D2. For this problem,
we are given a realization d1 ∼D1. Also, conditioning on M is omitted.
In this notation, Equation (2.63) becomes:
p(D2, D1 = d1|θ) = p(D2|θ)p(D1 = d1|θ)
(2.64)
Optimal placement for sensor reconﬁguration
For the new sensor placement problem, we would like to use an information-theoretic
measure as the utility function to be maximized over then new sensor conﬁguration,

24
δ′. We use the expected value of the KL-divergence from the updated prior to the
pre-posterior given the as-yet-unmeasured data. That is:
U(δ′) = ED2|D1=d1 [KL {p(θ|D2, D1 = d1)||p(θ|D1 = d1)}]
(2.65)
This may be further expressed as follows:
U(δ′) = ED2|D1=d1
∫
p(θ|D2, D1 = d1) log
 p(θ|D2, D1 = d1)
p(θ|D1 = d1)

dθ

(2.66)
Conditional probability and Bayes’ theorem allows us to express this in terms of
quantities that can be estimated numerically. Denote the logarithm term in Eqn.
(2.66) by L(θ, D2, d1). Then, we have:
U(δ′) =
∫
p(D2|D1 = d1)p(θ|D2, D1 = d1)L(θ, D2, d1) dθ dD2
(2.67)
Using the conditional independence assumption of Eqn. (2.63), the logarithm term
can be simpliﬁed:
L(θ, D2, d1) = log

p(D2|θ)p(D1 = d1|θ)p(θ)p(D1 = d1)
p(D2|D1 = d1)p(D1 = d1)p(D1 = d1|θ)p(θ)

⇒L(θ, D2, d1) = log

p(D2|θ)
p(D2|D1 = d1)

(2.68)
Thus, upon simplifying the expression for the remaining terms, we get:
U(δ′) =
∫
p(D2|θ)p(θ|D1 = d1) log

p(D2|θ)
p(D2|D1 = d1)

dθ dD2
(2.69)
The new problem, therefore requires evaluating the following entropy-like expres-
sion:
H21 = −
∫
p(D2|D1 = d1) log p(D2|D1 = d1) dD2
(2.70)
Further,
p(D2|D1 = d1) =
∫
p(D2|θ)p(θ|D1 = d1) dθ
(2.71)
This implies that:
H21 = log p(D1 = d1) −
∫p(D2, D1 = d1)
p(D1 = d1)
log {p(D2, D1 = d1)} dD2
(2.72)
Then, the optimal sensor rearrangement reconﬁguration problem can proceed as
in the original optimal sensor location problem.
Note, however, that integrals

25
against θ in Equations (2.69) and (2.72) are against the posterior distribution over
the parameters p(θ|D1 = d1), that have been updated using data from the ﬁrst
deployment. Thus, the complexity of this problem is compounded by the fact that
unlike the sensor location problem for which the prior distribution is easy to sample
from, the sensor reconﬁguration problem required drawing several sets of samples
from the posterior distribution under the ﬁrst set of data. An example problem
is not attempted therefore, since reconﬁguration problem, in the absence of any
simplifying assumptions, is considerably more computationally expensive than the
already costly sensor location problem.
References
[1]
T. M. Cover, J. A. Thomas, Elements of Information Theory. Hoboken, NJ,
USA: John Wiley & Sons, Inc., Sep. 2005, isbn: 9780471748823.

26
C h a p t e r 3
EFFICIENT SOLUTION USING CONVEX RELAXATION
In the previous chapter, the Bayesian information-theoretic foundations of the op-
timal sensor placement problem were laid out. Here, The entropy-based optimal
sensor placement problem is formulated as in [1] and an eﬃcient solution using con-
vex optimization techniques is presented. Model identiﬁability is discussed and the
Laplace approximation to the posterior in a Bayesian system identiﬁcation problem
is described.
3.1
Log-determinant formulation
This section expands on the derivation of the log-determinant formulation set up
in Section 2.4. The Laplace approximation to the posterior together with addi-
tional assumptions about the prediction-errors, allows for the development of a
log-determinant entropy-based objective.
System Identiﬁability
The notion of identiﬁability is important to characterize the topography of the
posterior distribution. The deﬁnition used here is in line with [2]. Essentially,
the posterior distribution can either be globally identiﬁable, locally identiﬁable or
unidentiﬁable as follows.
Denote by S(θ), the set of all admissible values of the parameter vector, θ. Also,
let ˆθ correspond to the optimal parameters that maximize the likelihood function in
Equation (2.6).
If Sopt(θ; DN), a subset of S(θ), is the set of all optimal models in the model class
M given data DN, then a parameter θ j is said to be globally system identiﬁable if
Sopt(θ; DN) contains only one optimal parameter, or else if for two diﬀerent optimal
parameter vectors ˆα(1) and ˆθ(2), the corresponding components of the j’th parameter
are equal as ˆθ(1)
j
= ˆθ(2)
j .
In general, a parameter is system identiﬁable if no two optimal parameters are
arbitrarily close together. In other words, for ˆθ(1)
j , ˆθ(2)
j
∈Sopt(θ; DN), there exists a
positive number, ϵj such that exactly one of | ˆθ(1)
j
−ˆθ(2)
j | > ϵj and ˆθ(1)
j
= ˆθ(2)
j
holds
true.

27
Finally, not that if a model is system identiﬁable but not globally so, then it is said
to be locally system identiﬁable.
Laplace’s asymptotic approximation
When the system is globally identiﬁable, then we may expand the log-posterior
about the most probable value to second order using a Taylor series to give us a
Gaussian approximation to the posterior density:
p(θ|y1:N, M) ≈p( ˆθ|y1:N, M) exp

−1
2(θ −ˆθ)T AN( ˆθ)(θ −ˆθ)

(3.1)
The normalizing constant in front of the exponential can be determined by inspec-
tion:
p(θ|y1:N, M) ≈
p
det AN( ˆθ)
(2π)NNo/2 exp

−1
2(θ −ˆθ)T AN( ˆθ)(θ −ˆθ)

(3.2)
The precision matrix of the multivariate Gaussian in Equation (3.1) is given by the
Hessian of the logarithm of the posterior density, evaluated at the unique most-
probable parameter vector. That is,
AN,ij(θ) = −∂2 log p(θ|y1:N, M)
∂θi∂θ j
(3.3)
It is to be borne in mind that the parameter vector and its associated derivatives
can be partitioned into two parts - the system parameters and the prediction-error
parameters:
AN(θs) =
"
BN(θs)
0
0
CN(ˆσ)
#
(3.4)
where
[BN(θs)]ij =
∂2 log p(y1:N|[θs, ˆσ], M)
∂θs,i∂θs,j

(3.5)
and
CN(ˆσ) = ∂2 log p(y1:N|[θs, σ], M)
∂σ∂σ

σ=ˆσ
(3.6)
The oﬀ-diagonal terms in the Hessian of Equation (3.4) are zero because of our
choice of stochastic forward model and prior.

28
The posterior distribution of the system parameters only, with the prediction-error
parameter marginalized out, can therefore be expressed independently as:
p(θs|y1:N, M) =
p
det BN( ˆθs)
(2π)NNo/2 exp

−1
2(θs −ˆθs)T BN( ˆθs)(θs −ˆθs)

(3.7)
We now seek to provide an approximation to the diagonal block of the Hessian that
corresponds to the system parameters, BN( ˆθs), in terms of the sensitivity coeﬃcients
of the predictions with respect to the system parameters.
For ﬁxed system parameters θs, the optimal (in the maximum-likelihood sense)
prediction-error variance in terms of the data as:
ˆσ2(θs) =
1
NNo
N
Õ
n=1
∥yn −So(δ)xn(θs)∥2 ∆= J(θs)
(3.8)
Equation (3.8) can be used to re-express the stochastic forward model at the optimal
parameter:
p(y1:N|θ = [θs, ˆσ], M) = [2πeJ(θs)]−NNo/2
(3.9)
We can evaluate the block diagonal terms of the Hessian corresponding to the system
parameters by:
BN( ˆθs) = NNo
2
∂2J(θs)
∂θs∂θs

θs= ˆθs
(3.10)
We expand the second-order derivative in Equation (3.10) using the expression for
J(θs) from Equation (3.8):
[BN( ˆθs)]pq = 1
ˆσ2
N
Õ
n=1
"
∂xn
∂θs,p
SoST
o
∂xT
n
∂θs,q
+ ϵn

So
∂2xn
∂θs,p∂θs,q
T#
(3.11)
This is where an additional assumption is needed in order to determine this second-
derivative approximately: it is assumed that the term involving the second derivative
in Equation 3.11 can be neglected. This can be justiﬁed if at least one of the fol-
lowing is true: the prediction errors, ϵn are small in magnitude; or the deterministic
predictions, xn, vary slowly with respect to the system parameters and therefore have
a small second derivative. The curvature assumption can be assessed before instru-
menting the structure, while the prediction-error assumption can only be validated
after checking the agreement between the deterministic predictions and the data.

29
With this assumption, the relevant portion of the Hessian matrix can be expressed in
terms of the prediction sensitivity coeﬃcients at the observed degrees of freedom.
The resulting approximation for the second derivative is:
[BN( ˆθs)]pq ≈1
ˆσ2
N
Õ
n=1
 ∂xn
∂θs,p
SoST
o
∂xT
n
∂θs,q

(3.12)
In addition, notice the dependence on the sensor conﬁguration as ST
o So = diag(δ).
The expression in Equation (3.12) can therefore be transformed to a double-sum:
[BN( ˆθs)]pq = 1
ˆσ2
Nd
Õ
i=1
δi
N
Õ
n=1
∂xn,i
∂θs,p
∂xT
n,i
∂θs,q
(3.13)
Equation (3.13) is a linear sum of contributions of terms from each sensor location.
For convenience of notation in the formulation of the optimal sensor placement
problem, we use the following notation while discussing the matrix of the equation:
[BN( ˆθs)]pq = 1
ˆσ2Qpq(δ) = 1
ˆσ2
Nd
Õ
i=1
δiQpq(ei)
(3.14)
In Equation (3.14),
Qpq(ei) =
N
Õ
n=1
∂xn,i
∂θs,p
∂xT
n,i
∂θs,q
(3.15)
Having developed an approximate expression for the posterior distribution over the
system parameters, we are now in a position to discuss the optimal sensor location
problem.
3.2
Entropy-based optimal sensor location
So far, we have discussed how data can be used to update uncertainties about our
model. We now address the optimal sensor location problem. We begin by deﬁning
an objective function to be optimized with respect to sensor locations, that quantiﬁes
the posterior uncertainty that we expect in the parameters after measuring the data.
Note that the optimal sensor location problem is to be solved typically before any
response data is available from the real structure to be instrumented.
What is
available is information about the structure either from its design or from preliminary
tests, in the form of a structural model and a prior distribution over the system
parameters.

30
What is very convenient in the formulation of Section 2.2, is that the dependence
on the data of the approximate posterior distribution over the uncertain parameters
is only through the optimal parameters, ˆθ = ˆθ(y1:N).
That is,
p(θs|y1:N, M) = p(θs| ˆθ(y1:N), M)
(3.16)
The nominal values for the system parameters are used in place of the optimal
parameters:
p(θs|θ0, M) =
p
det BN(θs,0)
(2π)NNo/2
exp

−1
2(θs −θs,0)T BN(θ0)(θs −θs,0)

(3.17)
Since the designer is uncertain about the nominal values, a prior distribution, p(θ0),
is speciﬁed on the nominal parameters.
Equation (3.17) replaces the dependence of the posterior on the unavailable data
with prior information that is already available in terms of the hyper-parameters,
θ0 = [θs,0, σ0]. These hyper-parameters are either available in the form of structural
design information, or in the form of information obtained from preliminary tests.
Entropy-based objective
We are now in a position to specify an objective function to be optimized with
respect to sensor location. Over all the possible sensor conﬁgurations, we would
like to select the conﬁguration that results in the lowest possible expected entropy
of the pre-posterior, relative to the prior, upon arrival of the data.
∆H(δ) = H(θs|θ0) −H(θ0)
(3.18)
= E(θs,θ0)[−log p(θs|θ0)] −Eθ0[−log p(θ0)]
(3.19)
Only the conditional entropy term in Equation (3.18) depends on the value of the
sensor conﬁguration, δ. That is, we want to minimize w.r.t δ:
∆H(δ) = −1
2
∫
log det Q(δ)p(θs,0) dθs,0 + constant
(3.20)
We now have an objective function to be optimized.
The expression for Q(δ)
comes from Equation (3.14). Denote the optimal sensor conﬁguration by δ∗. Then,
ignoring the irrelevant terms in Equation (3.20) we have:
δ∗= arg max
δ
∫
log det Q(δ, θs,0)p(θs,0) dθs,0
(3.21)

31
The entropy-based optimal sensor location problem is now fully speciﬁed in Equa-
tion (3.21).
Complexity of the problem
Equation(3.21)speciﬁes acombinatorialoptimizationproblem. Thatis, the problem
is of choosing No sensor locations among Nd ones. Depending upon the problem,
this could become prohibitively expensive. Each sensor conﬁguration requires one
evaluation of the expectation of the log-determinant of an expensive prediction.
An example of a prohibitively expensive sensor placement problem is say one of
selecting 20 locations from among 50 locations, to instrument: the number of
possible combinations is about 47 trillion.
At this point, it is clear that a brute-force search for the optimal solution over all
possible sensor conﬁgurations is not a wise approach. Instead, information obtained
by evaluating diﬀerent conﬁgurations should be used to guide the optimization
process towards the optimal conﬁguration.
It turns out that it is, in general, not possible to guarantee an optimal solution
in a manner that does not require an exhaustive search. Heuristic methods such
as genetic algorithms can produce an acceptable sub-optimal value if run for long
enough. Incremental greedy algorithms are also guaranteed to produce a suboptimal
value within (1 −1/e) of the optimal [3], relative to the range from the least to the
most optimal conﬁguration.
The method developed in Section 3.3 along the lines of [4], applies a convex
relaxation technique to provide a sub-optimal solution to the problem. An upper
bound on the optimal value is automatically provided by the solution.
3.3
Convex relaxation of the combinatorial optimization problem
Here, a relaxed version of the optimization problem is set up and argued to be convex.
The relevant partial derivatives of the objective function, that is, its gradient and
its Hessian matrix are derived for the purpose of computing the solution. The ﬁnal
step involves replacing the expectation integral over the prior distribution on the
uncertain parameters by its Monte Carlo approximation so that the problem may be
solved by a generic convex solver. This is discussed in Section 3.3.
In Section 3.4, the relevant quantities are calculated for a multistory structure subject
to sinusoidal ground motion. Assumptions about the parameterization, made for
tractability of the problem, are stated.
Results for optimal sensor locations are

32
presented for speciﬁc scenarios.
Appendix A lists some of the useful matrix calculus identities used in evaluating
the gradient of the objective. Appendix B contains detailed calculations relevant to
the parametric gradient of the displacement predictions for the multistory structure
example considered.
Combinatorial optimization problem
The original problem reﬂects the reality that a sensor must be present or absent at a
degree of freedom. Thus, the optimization problem is over a boolean vector:
maximize
δ
h(δ)
subject to
δi ∈{0, 1},
i = 1, . . ., Nd
and
1Tδ = No.
(3.22)
The objective function h(δ) corresponds to the gain in Shannon information associ-
ated with a given sensor conﬁguration, δ. It is given ([5]) by:
h(δ) = Eθs [log det Q(δ)]
(3.23)
The quantities in Equation 3.23 are summarized in Chapter 3. The negative sign
simply converts the maximization problem to a minimization one in order to be
consistent with the standard description of an optimization problem.
The practical diﬃculty in solving this problem is the fact that a naive exploration
of all possibilities quickly becomes intractable over even moderate values of No
and Nd. For instance, the placement of 20 sensors in a 50 DOF structure would
involve  50
20
 ≈4.71 × 1013 evaluations of the objective function, h(δ). In order to
overcome this diﬃculty, the original problem in Problem 3.22 is relaxed to allow the
boolean vector δ to take values continuously between zero and one. The problem
thus speciﬁed can be solved using convex optimization techniques that guarantee a
relatively cheap and unique solution, as shown next.
It is to be noted that the solution to this new optimization problem need not be a
Boolean vector, but it could contain entries between 0 and 1. The solution is still
very meaningful, however. If the solution is indeed a Boolean vector, then it is the
optimal solution to the original combinatorial problem. If not, then it is an upper
bound to the value of the objective function

33
Relaxed Problem statement
The original Boolean sensor placement vector, δ, is relaxed into a vector, z, in the
hypercube [0, 1]Nd, resulting in the relaxed optimization problem:
minimize
z
f (z)
subject to
−zi ≤0
zi ≤1,
i = 1, . . ., Nd
and
1Tz = No.
(3.24)
Here, the objective is the continuous extension of the previous, converted to a
minimization problem using a minus sign:
f (z) = −h(z) = −Eθs [log det Q(z)]
(3.25)
Note that this allows one to use continuous, rather than discrete, optimization
packages to solve the problem.
Convex nature of the relaxed problem
Problem 3.24 describes a convex optimization problem. This is because the objec-
tive function is a convex in z, and the equality and inequality constraints are aﬃne,
the log-determinant function in the objective is convex in z [4] and the expectation
operator preserves the convexity of its argument. Thus, the problem has a unique
global optimum. This can be determined computationally, as long as the objective
can be computed at every z.
For larger problems, it becomes necessary to supply the gradient and hessian of
the objective with respect to z. This avoids expensive computations of their ﬁnite-
diﬀerence counterparts. Fortunately, their expressions are tractable and, along with
their Monte Carlo approximations, are described next.
Derivatives of the objective
The gradient and hessian of the objective will be evaluated here.
Identities in
Appendix A are used. Consider the gradient of the objective,

34
∂f
∂zi
(z) = −∂
∂zi
∫
log det Q(z, θs)p(θs)dθs
(3.26)
= −
∫
∂log det Q
∂zi
p(θs)dθs
(interchange)
(3.27)
= −
∫
tr

Q−1 ∂Q
∂zi

p(θs)dθs
(derivative of log-det)
(3.28)
= −
∫
tr

Q−1(z, θs)Q(ei, θs)

p(θs)dθs
(evaluation of partial)
(3.29)
= −
∫©­­
«
Na
Õ
j=1
Na
Õ
k=1
Q−1
jk (z, θs)Qk j(ei, θs)
ª®®
¬
p(θs)dθs
(trace of product) (3.30)
= −
∫©­­
«
Na
Õ
j=1
Na
Õ
k=1
Q−1
jk (z, θs)Qjk(ei, θs)
ª®®
¬
p(θs)dθs
(symmetry) (3.31)
= −Eθs

Q−1(z, θs):Q(ei, θs)

(notation) (3.32)
The Hessian of the objective,
∂2 f
∂zp∂zq
(z) = −∂
∂zqEθs

Na
Õ
j=1
Na
Õ
k=1
Q−1
jk (z, θs)Qk j(ep, θs)

(3.33)
= −Eθs

Na
Õ
j=1
Na
Õ
k=1
∂Q−1
jk (z, θs)
∂zq
Qk j(ep, θs)

(interchanges)
(3.34)
= Eθs

Na
Õ
j=1
Na
Õ
k=1

Q−1 ∂Q
∂zq
Q−1

jk
(z, θs)Qjk(ep, θs)

(derivative of inverse)
(3.35)
= Eθs
h
tr

Q−1(z)Q(eq)Q−1(z)Q(ep)

(θs)
i
(derivative, trace)
(3.36)
= Eθs

((Q−1(z)Q(eq)):((Q−1(z)Q(ep))T)(θs)

(associativity, trace)
(3.37)

35
The expectation integral in the objective and its derivatives can be numerically
approximated using stochastic simulations. These numerical approximations would
be required for each constrained Newton step of the optimization scheme.
Numerical approximations
Given Nk samples θ(k)
s
distributed according to the prior p(θs) speciﬁed by the
designer, the integrals in question may be approximated by their corresponding
Monte Carlo estimates.
For the objective function,
f (z) ≈1
Nk
Nk
Õ
k=1
log det Q(z, θ(k)
s )
(3.38)
For the gradient of the objective,
∂f
∂zi
(z) ≈−1
Nk
Nk
Õ
k=1
Na
Õ
j=1
Na
Õ
k=1
Q−1
jk (z, θ(k)
s )Qjk(ei, θ(k)
s )
(3.39)
Finally, for the hessian of the objective,
∂2 f
∂zp∂zq
(z) ≈1
Nk
Nk
Õ
k=1
Na
Õ
j=1
Na
Õ
k=1

Q−1Q(eq)Q−1
jk (z, θ(k)
s )Q(ep, θ(k)
s )k j
(3.40)
Some computational eﬀort may be spared by noting that,
Q(z, θs) =
Nd
Õ
l=1
zl
N
Õ
n=1
∂xl
∂θs
∂xl
∂θs
T
(tn, θs)
(3.41)
=
Nd
Õ
l=1
zlQ(el, θs)
(3.42)
Hence, stored values of Q(ei, θs) may be used to determine Q(z, θs).
Solver for optimization
Since Problem 3.24 is convex, it may be solved using Newton’s method. In order to
apply this in practice, numerical approximations to the objective and its derivatives,
Equations 3.38 - 3.40, would need to be used.

36
The optimization problem has equality and inequality constraints. Hence, a con-
strained convex optimization solver needs to be employed. The expression for the
Newton step is not as straightforward as in the unconstrained case [6].
3.4
Application to multistory structure
The problem of optimally placing a ﬁxed number of sensors over a structure with
several degrees of freedom is considered.
The structure is assumed to behave
linearly, with classical damping. The structure is subject to a sinusoidal ground
acceleration at the base speciﬁed by an amplitude and frequency.
In this problem, the nominal natural frequency, input frequency, input amplitude
and the Rayleigh damping parameters are taken to be uncertain, along with the
prediction-error variance for a stationary, isotropic prediction error co-variance
matrix. The designer speciﬁes a prior distribution on these uncertain parameters.
Problem description
Consider a linear structure subject to sinusoidal input ground motion. The governing
ordinary matrix diﬀerential equation for its displacements x(t) as a function of time
is given by,
M Üx(t) + C Ûx(t) + Kx(t) = −M1a0 sin(ωt), with x(0) = 0, Ûx(0) = 0
(3.43)
Before converting this system into its modal co-ordinates, qj(t) with corresponding
natural frequency and damping ratio ωj and ζj respectively, the problem is simpliﬁed
to take into account uncertainties in the system. This is done largely through the
substructuring scheme.
All the assumptions have consequences that result in a
simple expression, relative to the uncertain parameters, for each mode.
The following simpliﬁcations are assumed:
1. M = mM∗and K = kK∗, implying that the mass and stiﬀness matrices can
be factored into an uncertain scalar and a deterministic matrix part. k and m
will be called the nominal stiﬀness and mass of the system respectively.
2. ω0 =
r
k
m is the uncertain nominal natural frequency.
3. C = αM + βK = αmM∗+ βkK∗, is the Rayleigh damping assumption with
uncertain parameters α and β.

37
Here, although k and m are uncertain, their uncertainty is irrelevant in the sensor
placement problem. Only their square-root ratio ω0 is relevant and may be treated
explicitly.
As a consequence of these simplifying assumptions, the following conditions hold
while one attempts to determine the modal solution of the problem:
1. The eigenvalues of the (K∗, M∗) system are constants c2
j =
ω2
j
ω2
0
.
2. The eigenvector matrix Φ that diagonalizes the (K∗, M∗) system, also diago-
nalizes the (K, M) system and can be used to solve the original system.
3. ΦT M∗Φ = diag{µi} and ΦTK∗Φ = diag{κi}, such that κi
µi
= c2
i .
Given these simpliﬁcations, a modal solution to the problem may be calculated
analytically.
Modal solution
We let x(t) = Φq(t), where q(t) are the modal displacement co-ordinates. Then, the
original equation may be simpliﬁed to,
ΦT MΦ Üq(t) + ΦTCΦ Ûq(t) + ΦTKΦq(t) = ΦT M1a0 sin(ωt)
(3.44)
Using the simplifying assumptions in Section 3.4, this simpliﬁes to a decoupled set
of modal ordinary diﬀerential equations as,
Üqj(t) + 2ζjωj Ûqj(t) + ω2
j qj(t) = aj sin(ωt)
(3.45)
Here,
ω2
j = c2
j ω2
0
(3.46)
2ζjωj = α + βω2
j and
(3.47)
aj = −a0
µj
ΦT1
(3.48)
Each equation is the governing equation to a driven, damped oscillator. The solution
to each equation is given by the expression,

38
qj(t) = aj
" ω3 + ω2
jω(2ζ2
j −1)
ωdj
!
exp(−ζjωjt) sin(ωdjt)
(3.49)
+ (2ζjωjω) exp(−ζjωjt) cos(ωdjt) + (ω2
j −ω2) sin(ωt)
(3.50)
−(2ζjωjω) cos(ωt)

/
h
(ω2
j −ω2)2 + (2ζjωjω)2i
(3.51)
The prediction equation for the displacements then becomes,
xi(t) =
Nd
Õ
j=1
Φijqj(t)
(3.52)
Sensor placement algorithm
In order to solve the relaxed optimal sensor placement problem in this case, the
following operations need to be performed:
1. Determine the eigenvalues c2
j and eigenvector matrix Φ of the (K∗, M∗) system
2. Begin with an initial guess z0 for the sensor positions that satisﬁes the con-
straints
3. Generate Nk samples of θs = [ω0, α, β, a0, ω] from the designer-speciﬁed
distribution p(θs)
4. For each θ(k)
s , calculate the gradient of qj(t) with respect to θs for several
values tn = n∆t for n = 1, . . ., N
5. Calculate the gradient ∇θs xi(tn, θ(k)
s )
6. Calculate the elementary matrices Q(ei, θ(k)
s ) for every i and k. Store them.
7. Compute the objective, its gradient and hessian, at the current vector zm
8. Update zm to z(m+1) and repeat steps 7 and 8 until convergence
Example: Uniform building
Consider the problem of placing a single displacement sensor on a uniform Nd-DOF
structure.

39
Substructuring scheme
In this case, the mass and stiﬀness matrices are given by:
M = mM∗= mINd
(3.53)
K = kK∗= k

2
−1
0
. . .
0
0
−1
2
−1
0
. . .
0
0
−1
2
−1
. . .
0
...
...
...
...
0
. . .
0
−1
2
−1
0
0
. . .
0
−1
1

(3.54)
Prior parameters
The prior parameters are taken to be independent. They are distributed as:
ω0 ∼ln N(·|µ = 2π, σ = 0.25)
(3.55)
α ∼ln N(·|µ = 0.1, σ = 0.01)
(3.56)
β ∼ln N(·|µ = 10−4, σ = 10−5)
(3.57)
a0 ∼N(·|µ = 0, σ = 40%g)
(3.58)
ω ∼ln N(·|µ = 2π, σ = 0.25)
(3.59)
These numbers are all physically reasonable and encountered in typical situations.
However, they have been not chosen to model any speciﬁc physical system.
Resulting sensor locations
The resulting locations for several values of Nd are determined. While the stochastic
forward model is diﬀerent for structures with a diﬀerent number of stories, the same
prior is used for each structure considered in this example. The results are tabulated
in Table 3.1.
For larger values of Nd, there are sometimes diﬃculties encountered in interpreting
the results, since a relaxed representation of the presence or absence of the sensor
is being used.
Once the Q(ei, θ(k)
s ) matrices have been computed and stored, the optimization
algorithm always converges to the same solution irrespective of the initial starting

40
Table 3.1: Simulation results
Case
Sensor DOF #
(1 ≡Base, Nd ≡Roof)
Nd
No
N
Nk
2
1
1000
1000
2
2
1
1000
2000
2
2
1
2000
1000
2
2
1
2000
2000
2
4
1
1000
1000
4
4
1
1000
2000
4
4
1
2000
1000
4
4
1
2000
2000
4
4
2
1000
1000
2, 4
4
2
1000
2000
2, 4
4
2
2000
1000
2, 4
4
2
2000
2000
2, 4
8
2
1000
1000
6, 8
8
2
2000
2000
6, 8
point, z0. This is expected, since the problem is convex and is guaranteed to have a
global minimum.
Table 3.1 contains the results of solving the relaxed, convex optimization problem for
structures with up to 8 degrees-of-freedom on which 2 sensors are to be installed. For
each structure or value of Nd, the sensor conﬁguration remains stable to changes in
the number of time-steps or prior samples used. There does seem to be a preference
for a sensor on the roof in every case. However, one should not read into this result
too much since these positions depend a lot on the parameters that are to be learned
as well as the prior uncertainties associated with them.
Next, the results for the sensor location scheme for a 50-story shear building, to be
instrumented with 20 sensors, are discussed. Figures 3.1 and 3.2 present typical
solutions to the convex problem. Since this problem involves a larger number of
degrees-of-freedom, there is a higher variance associated with the computation of
expectation of the log-determinant of the sensitivity matrix using prior samples. For
this example, this shows up as a diﬀerence in the value of the non-Boolean sensor
conﬁguration at stories 26 through 28. It is to be noted, however, that there does
not appear to be much of an ambiguity upon rounding the numbers to their Boolean
values. For example, no sensors should be placed at stories 26 and 27 in Figure 3.1,
or at story 27 in Figure 3.2.
The results suggest that all ﬂoors above the 27th, except ﬂoors 40, 43, 46 and 49,

41
and no ﬂoor below the 28th except ﬂoor 2 should be instrumented.
0
5
10
15
20
25
30
35
40
45
50
55
Story !
Example 50-story non-Boolean convex solution
Story 02: 0.92
Story 26: 0.07
Story 27: 0.02
Story 28: 1.00
Story 29: 1.00
Story 30: 1.00
Story 31: 1.00
Story 32: 1.00
Story 33: 1.00
Story 34: 1.00
Story 35: 1.00
Story 36: 1.00
Story 37: 0.99
Story 38: 1.00
Story 39: 1.00
Story 41: 1.00
Story 42: 1.00
Story 44: 1.00
Story 45: 1.00
Story 47: 1.00
Story 48: 1.00
Story 50: 1.00
Figure 3.1: The resulting sensor locations after solving the convex optimization
problem for a 50-story structure

42
0
5
10
15
20
25
30
35
40
45
50
55
Story !
Example 50-story non-Boolean convex solution
Story 02: 0.82
Story 27: 0.41
Story 28: 0.77
Story 29: 1.00
Story 30: 1.00
Story 31: 1.00
Story 32: 1.00
Story 33: 1.00
Story 34: 1.00
Story 35: 1.00
Story 36: 1.00
Story 37: 1.00
Story 38: 1.00
Story 39: 1.00
Story 41: 1.00
Story 42: 1.00
Story 44: 1.00
Story 45: 1.00
Story 47: 1.00
Story 48: 1.00
Story 50: 1.00
Figure 3.2: Another solution for the 50-story building but with diﬀerent prior
samples

43
It is to be remarked, as in the introduction, that an exhaustive search for the optimal
conﬁguration would have required over 47 trillion evaluations. The gradient-based
method requires fewer than 100 evaluations! Of course, heuristic methods could
have provided a good sub-optimal solution. In our case, however, we appear to have
converged to the optimal solution itself.
Comparison of diﬀerent sensor conﬁgurations
The number of bits of information gain of the optimal conﬁguration, z∗, obtained
using the convex technique over some other conﬁgurations is considered here.
The following conﬁgurations are considered, as described in Table 3.2:
Table 3.2: Description of various sensor conﬁgurations for comparative example
Case
Description
z∗
Optimal conﬁguration using convex relaxation
zlow
Stories 1 through 20 instrumented
zhigh
Stories 31 through 50 instrumented
zcommon
Sensors evenly spaced (Stories 1 through 50 in steps of 2.5, rounded up)
zgreedy
Solution using greedy sequential placement
The conﬁguration zgreedy is the sensor conﬁguration obtained using a greedy se-
quential placement algorithm that picks the next best sensor at each stage.
Here, zcommon is a commonly-used scheme of distributing the sensors evenly in the
structure.
Table 3.3: Comparison of information gain relative to optimal conﬁguration
Case
Objective Value
# bits gain using z*
z∗
6.14E+01
zlow
5.75E+01
5.7
zhigh
6.12E+01
0.3
zcommon
6.05E+01
1.3
zgreedy
6.14E+01
0.0
In this case, it appears to be very ineﬃcient to place sensors at at the lower stories
relative to the higher ones.
It turns out in this case that zgreedy = z∗.
With the convex relaxation scheme,
however, it is immediately understood that the solution is optimal, if Boolean.
The commonly-used sensor conﬁguration appears to be signiﬁcantly sub-optimal
for the given model class. This conﬁguration is sometimes popular, however, for

44
considerations beyond the model class being considered; for instance, when it is
expected that the behavior of the structure is not well understood.
Variation of optimal conﬁguration with sample set
For a Monte Carlo sample size, Nk, of 1000 or 2000, the optimal conﬁguration is
consistent for smaller structures (Nd < 10).
For larger structures (Nd ≈50), however, the optimal conﬁguration may diﬀer from
sample to sample. The severity of the problem increases with the size of the structure
and the number of parameters used. It turns out that for this example, the majority
of the sensor locations agree. Those in agreement are typically found in the upper
half of the structure. This may change with the speciﬁc model choices made in
formulating the problem, however.
Possible solutions to this problem:
• Select a suﬃciently large sample size to avoid variation between sample sets
• Avoid naive Monte Carlo integration and use an integral that provides an
estimate with smaller variance. The method would depend on the nature of
the integrand. An algorithm similar to simulated annealing could be used to
compute the expectation value [7]
• Obtain better design information about the uncertain parameters to obtain a
more peaked prior with less variation between samples within a sample set
Variation of optimal conﬁguration with number of time-steps
For a ﬁxed sample set, there is a variation in the optimal conﬁguration when the
number of observation time steps is changed from 1000 to 2000 for structures with
Nd > 10. Sensor locations near the roof typically remain the same. However, below
some story, the sensor locations can be quite diﬀerent.
Conclusion
The original combinatorial determinant maximization problem for optimal sensor
placement was cast into a relaxed convex optimization problem and solved eﬃciently
to yield sub-optimal solutions. Locations for which a sensor was unambiguously
present or absent were typically obtained, across diﬀering Monte Carlo samples
and diﬀering number of time steps, as well as ambiguous locations where sample
variability led to varying results.

45
Note that in the absence of design uncertainties, no variation would be present and
the algorithm would always converge to a stable, sub-optimal, and possibly optimal,
solution. Ambiguous locations could be resolved using a brute-force combinatorial
approach since the problem size would be reduced and manageable.
References
[1]
C. Papadimitriou, J. L. Beck, S.-K. Au, “Entropy-Based Optimal Sensor Lo-
cation for Structural Model Updating,” Journal of Vibration and Control, vol.
6, no. 5, pp. 781–800, Jan. 2000.
[2]
J. L. Beck, L. S. Katafygiotis, “Updating Models and Their Uncertainties. I:
Bayesian Statistical Framework,” Journal of Engineering Mechanics, vol. 124,
no. 4, pp. 455–461, Apr. 1998.
[3]
C. Guestrin, A. Krause, A. P. Singh, “Near-optimal sensor placements in
Gaussian processes,” Proceedings of the 22nd international conference on
Machine learning - ICML ’05, New York, New York, USA: ACM Press, 2005,
pp. 265–272, isbn: 1595931805.
[4]
S. Joshi, S. Boyd, “Sensor Selection via Convex Optimization,” IEEE Trans-
actions on Signal Processing, vol. 57, no. 2, pp. 451–462, Feb. 2009.
[5]
J. L. Beck, C. Papadimitriou, S.-K. Au, M. W. Vanik, “Entropy-based optimal
sensor location for structural damage detection,” S.-C. Liu, Ed., Jun. 1998,
pp. 161–172.
[6]
S. Boyd, L. Vandenberghe, Convex Optimization. New York, NY, USA: Cam-
bridge University Press, 2004, isbn: 0521833787.
[7]
J. L. Beck, K. M. Zuev, “ASYMPTOTICALLY INDEPENDENT MARKOV
SAMPLING: A NEW MARKOV CHAIN MONTE CARLO SCHEME FOR
BAYESIAN INFERENCE,” International Journal for Uncertainty Quantiﬁ-
cation, vol. 3, no. 5, pp. 445–474, 2013.

46
C h a p t e r 4
MODELING ALLOWABLE SENSOR LOCATIONS
So far, we have considered problems where the possible sensor locations correspond
to ﬁnite element nodes of the model at hand. This choice was made partially due to
the fact that the nodes may actually correspond to sensor locations in a real problem,
for example in a coarse model with nodes corresponding to stories of a building;
and also partially due to the face that it is convenient to do so. It is to be recognized
that the set of possible sensor locations is a design choice in some problems.
4.1
Sensor placement on a continuum
Here, we consider the case where the entire continuum of an elastic bar is available
for the placement of a single sensor, and apply the formulation developed in Chapter
3.
Standing wave on an elastic bar
Consider a bar of unit length deﬁned on the domain x ∈[0, 1]. The bar is elastic
and is governed by the wave equation with wave speed α, as:
u,tt = α2u,xx
(4.1)
with ﬁxed, zero displacement boundary conditions:
u(0, t) = 0 and u(1, t) = 0
(4.2)
and a sinusoidal initial displacement:
u(x, 0) = sin(πx) and u,t(x, 0) = 0
(4.3)
The solution to this problem is given by:
u(x, t) = sin(πx) sin(παt)
(4.4)
Given the wave speed, α, the predictions at equally spaced intervals of time, tn = n∆t,
is given by:
u(x, n∆t) = u(x, t) = sin(πx) sin(παn∆t)
(4.5)

47
We are now in a position to evaluate the matrix of sensitivity coeﬃcients, Q(x, α).
Note that it is a function of every possible sensor location, x ∈[0, 1].
Q(x, α) =
N
Õ
n=1
∂u(x, n∆t)
∂α
2
(4.6)
= (πn∆t)2 sin2(πx)
 N
Õ
n=1
cos(nπα∆t)
!2
(4.7)
The uncertain parameter, α, is distributed according to a prior, p(α). We wish
ﬁnd the optimal sensor location, x∗, as the solution to the following optimization
problem:
x∗= arg max
x∈[0,1]
h(x)
(4.8)
Here, according to the theory developed in Chapter 3, the objective function, h(x)
is given by:
h(x) = Eα[log det Q(x, α)]
(4.9)
The previous matrix of sensitivity coeﬃcients, Q(x, α), is simply a scalar here and
is a diﬀerentiable function of the position, x. We can set the ﬁrst derivative of the
objective function with respect to x in Equation (4.8) to determine an extremum as:
0 = dh(x)
dx

x=x∗
(4.10)
⇒0 = d
dxEα log

(πn∆t)2 sin2(πx)
 N
Õ
n=1
cos(nπα∆t)
!2

x=x∗
(4.11)
⇒0 = 2 d
dxEα log(πn∆t) + 2 d
dxEα log sin(πx)

x=x∗
+
2 Eα log
 N
Õ
n=1
cos(nπα∆t)
!
(4.12)
⇒0 = cot(πx∗)
(4.13)
⇒x∗= 1
2
(4.14)
As expected by an argument from symmetry, the location of the optimal sensor
is at the mid-point of the bar. Notice in Equations (4.10) through (4.13) that the
characterization of the prior uncertainty of α does not aﬀect the sensor location.
This is a property of this example.

48
4.2
Finite-element mesh reﬁnement
Typically, unlike in the previous example of Section 4.1, it is not possible to solve the
governing diﬀerential equation analytically. In practice, the ﬁnite element method
is used to determine an approximate solution to the ﬁeld of interest. The solution to
a ﬁnite element problem begins with obtaining the value of the approximating ﬁeld
at a ﬁnite number of nodes. For locations between nodes, shape functions are used
to determine the solution there. The number of nodes in the model depends on the
mesh size, which could be uniform or non-uniform.
In the original log-determinant sensor-placement formulation, the ﬁnite-element
nodes were considered to be the only possible sensor locations.
Under mesh-
reﬁnement, these possible locations change and increase in number. While the eﬀect
of reﬁning the mesh of the underlying ﬁnite-element is to increase the accuracy of
the approximate ﬁeld, it also increases the number of possible sensor locations due
to the increased number of nodes.
Example: Finite-element formulation of an axial bar
Consider an elastic axial rod with wave speed v of length L. One end at x = 0 is
ﬁxed and the other end is subject to a sinusoidal strain, ε f = ε0 sin ωt. This can be
thought of as a forcing stress if the elastic modulus of the bar is known.
The governing PDE is:
∂2u
∂t2 = v2 ∂2u
∂x2
with BCs: u(0, t) = 0,
∂u/∂x(L, t) = ε0 sin ωt,
and ICs: u(x, 0) = 0,
∂u/∂t(x, 0) = 0.
(4.15)
A ﬁnite element solution is introduced with the help of shape-functions:
˜u(x, t) =
Nd
Õ
i=1
ui(t)φi(x)
(4.16)

49
The weak form of the wave equation is now set up:
∫
V
Ü˜uwdx
−v2
∫
V
∂2 ˜u
∂x2 wdx
= 0
⇒
∫
V
Nd
Õ
i=1
(Üui(t)φi(x)) uj(t)φj(x)dx
+v2
∫
V
∂u
∂x
∂w
∂x dx
= 0
(int by parts)
⇒
Nd
Õ
i=1
Üui(t)
∫
V
φi(x)φj(x)dx
+v2
Nd
Õ
i=1
ui(t)
∫
V
φ′
i(x)φ′
j(x)dx
= 0 ∀j
(4.17)
In matrix form, this may be written as:
M ÜU(t) + KU(t) = f (t)
(4.18)
The forcing term on the right is from enforcing the Neumann BC at the last node.
A Rayleigh damping term may also be introduced in the ﬁnal matrix equation. The
damping matrix would be a linear combination of the mass and stiﬀness matrices.
M ÜU(t) + C ÛU(t) + KU(t) = f (t)
(4.19)
The matrix equation 4.19 can now be solved using Newmark integration, given the
parameters L, v, ε0 and ω. For a full simulation, additional parameters such as the
Rayleigh parameters α and β, as well as the time step ∆t, would need to be speciﬁed.
The time-integrated solution yields the nodal displacements:
U(tn) = [u1(tn), . . ., uNd(tn)]T
(4.20)
These can be used to construct a spatially continuous displacement ﬁeld, at discrete
times, according to equation 4.16:
˜u(x, tn) =
Nd
Õ
i=1
ui(tn)φi(x)
(4.21)

50
Shape functions
A linear, 2-node element is chosen, so that the shape functions are deﬁned as follows:
φi(x)
=


x −xi−1
h
,
x
∈
[xi−1, xi]
1 −x −xi
h
,
x
∈
[xi, xi+1]
0,
x
<
[xi−1, xi+1]
, i ∈{2, . . ., Nd −1}
φ1(x)
=
(
1 −x −x1
h
,
x
∈
[x1, x2]
0,
x
<
[x1, x2]
φNd(x)
=


x −xNd−1
h
,
x
∈
[xNd−1, xNd]
0,
x
<
[xNd−1, xNd]
(4.22)
The shape function derivatives are simply piece-wise constant corresponding to the
slopes of the lines in the non-zero regions.
Uncertainty quantiﬁcation
Consider a speciﬁc case, where three uncertain system parameters are considered:
• Bar wave speed, v
• Forcing amplitude, ε0
• Forcing frequency, ω
In addition, a prediction error uncertainty parameter, σ2, is considered.
The parameter vector is therefore represented by θ = [θs, σ2], where the system
parameters are given by θs = [v, ε0, ω].
A bar of unit length is considered and the damping parameters can be chosen so
that, at the prior modal values of the parameters in the list above, a damping ratio
of 5% is achieved in the ﬁrst two modes.
The ﬁnite-element solution notation is now augmented to include the uncertain
parameters. That is, the FEM solution is now denoted by:
˜u(x, t, θs) =
Nd
Õ
i=1
ui(x, θs)φi(x)
(4.23)

51
In order to have the continuous treatment, we assume that observed displacement
Yx(t) is governed by a Gaussian process (GP) over spatial coordinate index, x, with
mean function Mt(x|θ) and co-variance kernel Kt(x, y|θ). The mean prediction is
given by Mt(x|θs) = ˜u(x, t, θs). The co-variance kernel is assumed to be stationary
and isotropic, so that Kt(x, y|σ2) = σ2δxy.
Formulation of the optimization problem
Points in the domain, [0, L] are indexed by the set V and the sensor location set
is denoted by A. Thus, A ⊂V. Note that A is ﬁnite with |A| = No. The
observed displacements at the locations indexed by A are distributed according to
a multivariate Gaussian as per the GP as:
yAn|θ ∼N

·| ˜u(A, tn, θs), σ2I

(4.24)
(Notation: ˜u(A, tn, θs) is a vector of size No, corresponding to displacement predic-
tions at each location in A at time tn)
Modeling the prediction of the observations at diﬀerent times as probabilistically
independent, the likelihood of a full observation record over N observations is:
p(YAN|θ) =
N
Ö
n=1
p(yAn|θ)
(4.25)
Now, it is assumed that the model is globally identiﬁable, so that there exists a
unique global maximum likelihood estimate:
ˆθ = [ ˆθs, ˆσ2] = arg max
θ
p(YAN|θ)
(4.26)
The partial MLE estimate of σ2, viz. ˆσ2(θs), follows through as per the original
formulation:
ˆσ2(θs) = J(θs) =
1
NNo
N
Õ
n=1
∥yAn −˜u(A, tn, θs)∥2
(4.27)
The Laplace approximation can now be applied to the posterior on the system
parameters as in the original formulation, to obtain:
p(θs|YAN) ≈[det Q(A, ˆθs)]1/2
(2π ˆσ2)Na/2
exp

−1
2ˆσ2(θs −ˆθs)TQ(A, ˆθs)(θs −ˆθs)

(4.28)

52
The elements of the matrix, Q, have their origins in the Hessian matrix of the Laplace
approximation:
Q(A, θs) =
N
Õ
n=1
(∇θs ˜u)(∇θs ˜u)T(A, tn, θs)
(4.29)
The optimization problem is therefore:
A∗= arg max
A⊂V
|A|=No
Eθs[log det Q(A, θs)]
(4.30)
Here, the expectation is taken over a nominal distribution on the system parameters
speciﬁed by the designer.
For convenience, denote the objective by f (A) = E[log det Q(A, θs)]. Note that the
expectation variable, θs, is implicit in the notation from now on.
4.3
Comparison with the ﬁnite selection-space case
When the list of possible sensor locations was ﬁnite, then a selection matrix, So(δ),
based on the boolean sensor-selection vector, δ, could be employed to split the
matrix Q(δ, θs) into Nd elementary matrices Qi(θs). The optimization was then
over a ﬁnite set of points of cardinality Nd. That is, the optimization problem was
a combinatorial one of selecting No of the Nd elementary sensing matrices Qi(θs)
whose sum maximized the expectation-log-determinant objective function.
In the continuous case, we have an continuum of points to select from. This hints
towards a gradient-based optimization as opposed to a combinatorial one.
Simple case: single sensor placement
Consider the simple problem of selecting just one sensor location optimally. In this
case, |A| = 1. We would like to determine:
x∗= arg max
x∈V
f (x)
= arg max
x∈V
E[log det Q(x, θs)]
(4.31)
With a slight abuse of notation, we consider the derivative of the objective with
respect to location, x, assuming that it exists.
If the optimal location is in the
interior, then this derivative should be zero. Else, the optimal location is at the
boundary according to the intermediate value theorem.

53
Speciﬁcally,
f ′(x∗) = 0 or x∗= 0 or x∗= L
(4.32)
This calls for an investigation of the derivative:
f ′(x) = d
dxE log det Q(x, θs)
= E d
dx log det Q(x, θs)
(assuming allowed)
= E

tr

Q−1(x, θs) d
dxQ(x, θs)

(derivative of log-det)
(4.33)
Consider the second term inside the trace involving the derivative of the sensing
matrix:
d
dxQ(x, θs) = d
dx
N
Õ
n=1
∇˜u ∇˜uT(x, tn, θs)
(4.34)
(Note: The implicit vector gradient, ∇, is with respect to the system parameters, θs)
The derivative needs to be performed entry-wise and we therefore need to look at:
d
dxQpq(x, θs) = d
dx
N
Õ
n=1
∂˜u
∂θsp
∂˜u
∂θsq
(x, tn, θs)
=
N
Õ
n=1
 ∂
∂θsp
∂˜u
∂x
 ∂˜u
∂θsq
+
 ∂
∂θsq
∂˜u
∂x
 ∂˜u
∂θsp
(assuming all interchanges allowed)
(4.35)
At this point, it is convenient to denote the x derivative of the displacement by strain,
˜ε(x, tn, θs). We then have,
d
dxQpq(x, θs) =
N
Õ
n=1
 ∂˜ε
∂θsp
∂˜u
∂θsq
+ ∂˜u
∂θsp
∂˜ε
∂θsq

(x, tn, θs)
(4.36)
Finally, we consider the derivative of the ﬁnite-element displacement ﬁeld with re-
spect to location. One can see that this derivative involves, ultimately, the derivative
of the ﬁnite element shape functions.
The localized nature of the shape functions comes into play here, assuming that we
have used linear, 2-node elements to discretize the bar. For a given location, x,

54
we consider the contribution from only its immediately adjacent shape functions
associated with nodes j and j + 1, so that x ∈[xj, xj+1]. Accordingly,
˜ε(x, tn, θs) = ∂˜u
∂x(x, tn, θs)
=
Nd
Õ
i=1
ui(tn, θs)φ′
i(x)
= uj(tn, θs)φ′
j(x) + uj+1(tn, θs)φ′
j+1(x)
(4.37)
If x happens to be one of the nodes, then only the shape function associated with
that node is relevant:
˜ε(xj, tn, θs) = ∂˜u
∂x(xj, tn, θs)
= uj(tn, θs)φ′
j(xj)
(4.38)
The 2-node, linear elements are constant-strain elements. That is, the derivatives
in question have a constant value in the interior of elements.
The strain ﬁeld
determined by the ﬁnite element solution with this linear interpolation is ill-deﬁned
at the ﬁnite-element nodes, since the displacement function has a kink there.
This means that the φ′
j(xj) term in the resulting expression of Equation (4.38) is
undeﬁned. So for now, the possibility of a ﬁnite-element nodal point being the
optimal solution is considered as a case to be tackled separately.
For a point x not at a node, we have the resulting forward diﬀerence equation:
˜ε(x, tn, θs) = ∂˜u
∂x(x, tn, θs)
= uj(tn, θs)φ′
j(x) + uj+1(tn, θs)φ′
j+1(x)
= (uj+1 −uj)
h
(tn, θs)
(4.39)
Here, h = L/(Nd −1), is the mesh width.
We are now equipped to solve the optimization problem. Note that the objective
is not convex, since the matrix, Q(A, θs), is not an aﬃne function of the set of
locations in A.
General case: multiple sensors
In this case, we attempt to set the whole gradient, with respect to No possible
locations in the set A, to zero. Begin by evaluating the gradient:

55
∂f (A)
∂xa
= ∂
∂xa
E [log det Q(A, θs)]
=E

tr

Q−1(A, θs) ∂
∂xa
Q(A, θs)

(4.40)
Now, for the matrix derivative term, we have:
∂
∂xa
Q(A, θs) =
∂
∂xa
N
Õ
n=1
Õ
b∈A

∇˜u ∇˜uT(xb, tn, θs)

=
N
Õ
n=1
∂
∂xa
Õ
b∈A

∇˜u ∇˜uT(xb, tn, θs)

=
N
Õ
n=1
∂
∂xa

∇˜u ∇˜uT(xa, tn, θs)

(4.41)
Note that, in general,
˜u(xa, tn, θs)
= uj(tn, θs)φj(xa) + uj+1(tn, θs)φj+1(xa)
⇒∇˜u(xa, tn, θs)
= ∇uj(tn, θs)φj(xa) + ∇uj+1(tn, θs)φj+1(xa)
⇒∇˜u∇˜uT(xa, tn, θs)
= ∇uj∇uT
j (tn, θs)φj(xa)φj(xa)+
∇uj+1∇uT
j+1(tn, θs)φj+1(xa)φj+1(xa)+
2∇˜uj∇˜uT
j+1(tn, θs)φj(xa)φj+1(xa)
(4.42)
The spatial partial derivative may now be evaluated:
∂
∂xa
∇˜u∇˜uT(xa, tn, θs)
= [2∇uj∇uT
j φj(xa)φ′
j(xa)+
2∇uj+1∇uT
j+1φj+1(xa)φ′
j+1(xa)+
2∇˜uj∇˜uT
j+1(φ′
j(xa)φj+1(xa) + φj(xa)φ′
j+1(xa))]
(4.43)
Solution and sensor clumping
Refer back to the equation 4.40 for the location gradient of the objective. At the
optimal sensor locations, this gradient should be zero as long as none of the sensors
are at the boundaries of the domain, which in our case is the ends of the bar.

56
∂f (A)
∂xa
= 0 ∀a ∈A ⊂V
= ∂
∂xa
E [log det Q(A, θs)]
=E

tr

Q−1(A, θs) ∂
∂xa
Q(A, θs)

(4.44)
We therefore have No equations in No unknowns which can presumably solved for
the sensor locations.
There is a problem with this approach and it is this: the gradient vector is symmetric
with respect to every sensor location co-ordinate, xa. What this means is that if one
component of the gradient is determined to be zero at a particular location, then the
exclusion of that point results in an ill-posed problem, since the point immediately
‘adjacent’ to the excluded point also result in a zero gradient. We therefore have the
problem of all sensor locations concentrating at a single location.
Resolution
The root of the problem is the fact that the formulation developed in Chapter 3 does
not model an important aspect of the sensor selection problem: prediction-error
correlations at small length scales.
The isotropic co-variance kernel fails to capture that fact that the prediction errors
at points arbitrarily close to each other are correlated in practice. In order to capture
this correlation, one could select a co-variance kernel that is not diagonal as:
Kt(x, y|θ) = σ2 exp(−c|x −y|)
(4.45)
The exponential factor in Equation (4.45) takes on a non-zero value for points that
are not coincident. This correlation decays exponentially as the distance between
the points considered. A multiplicative factor controls the desired length scale.
The problem with this approach is that the kernel violates the underlying assumptions
of the framework, and the new resulting uncertainty objective cannot be expressed
neatly in terms of a sum of positive deﬁnite matrices associated with each possible
sensor location. A preliminary sensor deployment might provide some insight into
determining the structure of the co-variance kernel [1], although this is ultimately
a modeling choice.
It should be noted, however, that in the absence of prior
information about this correlation structure, the maximum entropy principle tells us

57
to choose an isotropic co-variance matrix for the discretized version of the problem.
This, as we know, results in sensor clumping. In this situation, the best resolution is
to specify a pre-deﬁned set of possible sensor locations that are suﬃciently far apart
or appropriately positioned such that no correlation between the prediction errors at
those locations is suspected a priori. This is the approach that was used in Chapter
3.
An example examining the eﬀect of prediction error correlation on sensor placement
on a cantilever beam is discussed next.
4.4
Example: free-vibration of a cantilever beam with prediction-error cor-
relations
Here, we look at the optimal sensor placement problem with prediction-error cor-
relations included as considered in [2] and [3]. Here, we consider the case of a
free-vibrating, uniform Euler-Bernoulli cantilever beam . A cantilever beam is ﬁxed
at one end and free at the other for displacements and rotations. The governing
equation for the beam is:
EI ∂4v
∂x4 + µ∂2v
∂t2 = 0
(4.46)
with:
Initial conditions: v(x, 0) = f (x) and Ûv(x, 0) = 0
(4.47)
Boundary conditions: v(0, t) = 0, ∂v
∂x(0, t) = 0
(4.48)
∂v2
∂x2(L, t) = 0 and ∂v2
∂x3(L, t) = 0
(4.49)
This problem can be solved using separation of variables as:
v(x, t) =
∞
Õ
m=1
Xm(x)Tm(t)
(4.50)
where
Xm(x) = Fm

(cosh βmx −cos βmx) + cosh βmL + cos βmL
sinh βmL + sin βmL (sin βmx −sinh βmx)

(4.51)
and
Tm(t) = cos(ωmt)
(4.52)
Equation (4.51) is for the mode shapes of vibration. Here, m is used to index the
mode corresponding to the natural frequency of vibration, ωm.

58
The coeﬃcients Fm are the Fourier coeﬃcients under each mode shape, of the
function f (x) that describes the initial displacement condition.
The coeﬃcients, βm are obtained as a result of looking for valid solutions to the
separation-of-variables method of solution. They are solutions to the equation:
cosh βmL cos βmL + 1 = 0
(4.53)
The natural frequencies are given by:
ωm = β2
m
s
EI
µ
(4.54)
Finally, Tm(t) is the time-dependent part of the contribution from the m’th mode.
Correlated sensor placement for ﬁrst modal frequency parameter
Here, we wish to compare the diﬀerence in mutual information, in bits, of the
optimal single sensor conﬁguration versus the optimal two-sensor conﬁguration for
diﬀerent values of the prediction-error correlation parameter for the cantilever beam,
for the purpose of updating uncertainties associated with the ﬁrst modal frequency
parameter, ω1.
Entropy of the posterior
For two sensors located at co-ordinates p and q with correlated prediction-errors as:
Σ = σ2
"
1
c(p, q)
c(p, q)
1
#
(4.55)
Here, the chosen model for the correlation coeﬃcient decays exponentially as the
square of the distance between the two locations:
c(p, q) = exp

−
 p −q
λ
2
(4.56)
The quantity, λ, can be interpreted as a correlation length. In this analysis, varia-
tion of the entropic quantities of interest as a function of λ are investigated. The
determinant of the covariance matrix is:
|Σ| = σ4(1 −c2(p, q))
(4.57)
The inverse of the covariance matrix is:
Σ−1 =
1
σ2(1 −c2(p, q))
"
1
−c(p, q)
−c(p, q)
1
#
(4.58)

59
Laplace approximation
We begin by writing out the likelihood function and then expressing the posterior
over the parameters as a Gaussian distribution at the MLE parameters.
p(y1:N|θs, σ2) =
1
(2π)NNo/2|Σ|N/2 exp
"
−1
2
N
Õ
n=1
(yn −xn(θs))TΣ−1(yn −xn(θs))
#
(4.59)
Here, No = 2 and we may substitute the expressions for the covariance matrix
determinant and inverse:
f (y1:N, θs, σ2) = p(y1:N|θs, σ2)
(4.60)
=
1
(2πσ2p
1 −c2(p, q))NNo/2 exp
"
−
ÍN
n=1(e2
p + e2
q −2c(p, q)epeq)
2σ2(1 −c2(p, q))
#
(4.61)
For the convenience of notation, the following quantities have been introduced:
ep ≡ep(θs, p, q) = (ynp −xnp(θs)) and eq ≡eq(θs, p, q) = (ynq −xnq(θs))
(4.62)
The optimal prediction error parameter, ˆσ2, as a function of the system parameters
is:
ˆσ2(θs) =
ÍN
n=1(e2
p + e2
q −2c(p, q)epeq)(θs)
NNo(1 −c2(p, q))
(4.63)
We have, therefore:
f (y1:N, θs, ˆσ2(θs)) = (2πe ˆσ2(θs)
q
1 −c2(p, q))NNo/2
(4.64)
The relevant part of the inverse covariance matrix, in the Laplace approximation to
the posterior, is therefore:
Bij =
ÍN
n=1
∂xn(p)
∂θsi
∂xn(p)
∂θsj
+ ∂xn(q)
∂θsi
∂xn(q)
∂θsj
−2c(p, q)∂xn(p)
∂θsi
∂xn(q)
∂θsj
  ˆθs
ˆσ2
(4.65)
We would like to compare the the information gain in bits by placing two sensors
versus placing a single sensor. In the case of the cantilever beam, in order to learn
the ﬁrst mode of a free-vibration-from-initial-displacement response, we have the
deterministic prediction:
xn(p) =
∞
Õ
m=1
Xm(p) cos(ωmtn)
(4.66)

60
The sensitivity coeﬃcient for the ﬁrst modal frequency is therefore:
∂xn(p)
∂ω1
= −tnX1(p) sin(ω1tn)
(4.67)
The objective of interest is the entropy of the posterior density, expressed as:
H(p, q) = 1
2 log(X2
1(p)+X2
1(q)−2c(p, q)X1(p)X1(q))−1
2 log(1−c2(p, q))+K (4.68)
Here, K includes constant and irrelevant time-dependent terms:
K = −1
2 log 2πe −1
2 log ˆσ2 + 1
2E
"
log
 N
Õ
n=1
t2
n sin2(ω1tn)
!#
(4.69)
Let the reference be the placement of a single sensor on the cantilever beam for the
same problem. We would like to know the diﬀerence in the mutual information in
terms of bits as before. This can be calculated by simply taking the diﬀerence in
the log-determinants of the corresponding block diagonals parts of the covariance
matrix in the Laplace approximation to the posterior. The expected information gain
is therefore:
∆I = H0(r∗) −H(p∗, q∗)
(4.70)
In Equation (4.70), the quantity H0(r∗) refers to the objective evaluated at the optimal
sensor location for a single sensor only:
H0(r) = log X1(r) + K
(4.71)
The logarithm is expressed in base 2 to express the diﬀerence in entropies in bits.
Let B0 denote the covariance matrix of the posterior for a single sensor’s placement:
B0 = 1
ˆσ2
N
Õ
n=1
X2
1(r)t2
n sin2(ω1tn)
(4.72)
The determinant is not needed since we are working with only the single scalar
fundamental frequency parameter. The term, K, is common to both entropies and
cancels out in the subtraction. The resulting expression in:
∆I = 1
2 log2(X2
1(p∗) + X2
1(q∗) −2c(p∗, q∗)X1(p∗)X1(q∗)) −log2 X1(r∗)
(4.73)
For this problem, it turns out the the optimal location for a single sensor is at the free
end of the cantilever beam. For the 2-sensor problem, it turns out that one sensor
is always placed at the free end while the other is determined by the correlation

61
length parameter, λ. The information gain from the second sensor is a function of
the correlation length parameter.
Figure 4.1 plots the optimal location, q∗, of the second sensor with respect to the
prediction-error correlation length scale, λ. As λ increases, the second sensor is
driven further away from the ﬁrst, up to a limit beyond which it prefers place the
sensor at the non-informative ﬁxed end of the beam.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
6 !
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
q$ !
Optimal sensor location
vs
Prediction-error correlation length
Figure 4.1: Optimal location of the second sensor as a function of the prediction-
error correlation length scale (note that the ﬁrst sensor is always at the free end of
the cantilever beam)
Figure 4.2 is a plot of the additional information gain from the second sensor when
it is placed optimally.

62
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
6 !
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
" I (bits) !
Additional information provided by second sensor
vs
Prediction-error correlation length
Figure 4.2: Additional bits of information from the optimally placed second sensor
plotted as a function of the correlation length scale
At the critical correlation length scale of about λ = 0.42 as a fraction of the length
of the beam, the second sensor becomes non-informative.
Through this example, we conclude that modeling prediction-error correlations is
essential to prevent sensor clustering or clumping under mesh reﬁnement. In the
absence of a correlation provision, a predetermined set of possible sensor locations,
spaced suﬃciently apart, should be selected to prevent sensor clumping.
References
[1]
C. Guestrin, A. Krause, A. P. Singh, “Near-optimal sensor placements in
Gaussian processes,” Proceedings of the 22nd international conference on
Machine learning - ICML ’05, New York, New York, USA: ACM Press, 2005,
pp. 265–272, isbn: 1595931805.
[2]
D. Bayard, F. Hadaegh, D. Meldrum, “Optimal experiment design for identiﬁ-
cation of large space structures,” Automatica, vol. 24, no. 3, pp. 357–364, May
1988.
[3]
C. Papadimitriou, G. Lombaert, “The eﬀect of prediction error correlation on
optimal sensor placement in structural dynamics,” Mechanical Systems and
Signal Processing, vol. 28, pp. 105–127, 2012.

63
C h a p t e r 5
BAYESIAN MODEL CLASS SELECTION USING
THERMODYNAMIC INTEGRATION
5.1
Introduction
The goal of system identiﬁcation is to ﬁnd an optimal model that can make good
predictions about the behavior of the system in the future. Given data from the sys-
tem, making good predictions involves two aspects: data ﬁt and Ockham’s principle.
A suﬃciently complex model can always ﬁt the data at hand to any desired accuracy
and therefore, as far as data-ﬁt is concerned, there are many plausible models to
choose from. Ockham’s principle states that of these models, the simpler ones are
more plausible for making reasonable predictions since they do not have a tendency
to overﬁt the data.
Using Bayes’ Theorem, one can obtain a posterior probability distribution over
the uncertain parameters that quantiﬁes the plausibility of each model in the class,
by updating prior uncertainties in a manner consistent with the stochastic forward
model. If several model classes are being considered, however, the challenging
problem of model class selection arises. That is, one would like to have a way of
comparing the predictive power of every chosen candidate model class.
It is known that the model evidence term, together with a prior over the model
classes, allows one to evaluate the posterior probability of each model class, follow-
ing which, model averaging or model class selection may be performed. The model
evidence term respects Ockham’s principle and penalizes more complex models that
oﬀer the same goodness-of-ﬁt to the data ([1]).
In order to compare diﬀerent model classes based on available data from a system
using the Bayesian approach, computation of the evidence for each model class is
required. Each evidence term can then be used to either choose the most plausible
model or perform model averaging over quantities of interest [1].

64
5.2
Numerical Evaluation of Model Evidence
Consider a probability model class, M, and available system output data, D. De-
note the uncertain parameters in M by θ. M speciﬁes a stochastic forward-model,
p(Y|θ, M), as well as a prior distribution, p(θ|M), over the uncertain parameters.
Of interest is the evidence for the model class under the data, p(Y|M). The basic
analytic expression for this quantity is given by Total Probability:
p(Y|M) =
∫
p(Y|θ, M)p(θ|M) dθ
(5.1)
Diﬃculties in computing model evidence
The likelihood function, which is the stochastic forward model with the actual data
substituted, viewed as a function of the uncertain parameters, θ, can, in general, be
a complex function of θ. Thus, except in the restricted case of priors algebraically
conjugate to the likelihoods, it is very unlikely that an analytic expression for the
evidence can be obtained.
Eqn. (5.1) can be viewed as the expected value of the likelihood function under the
prior:
p(Y|M) = Eθ∼p(θ|M)[p(Y|θ, M)]
(5.2)
The question of how to go about evaluating this quantity needs to be addressed.
One can imagine estimating the model evidence with a Monte-Carlo estimate using
samples drawn from the prior density. The estimate would be given by:
p(Y|M) ≈1
Ns
Ns
Õ
k=1
p(Y|θ(k), M)
(5.3)
For problems involving the stochastic embedding of dynamical systems with several
uncertain parameters, estimates for the evidence term using prior samples alone
can have unacceptably high variance. This is because the topography of likelihood
function is typically diﬀerent from that of the prior.
In the case of locally identiﬁable systems with a ﬁnite number of local a posteriori
maxima, the Laplace approximation can be used to obtain a reasonable estimate for
the log-evidence ([1]). The current endeavor is towards developing MCMC methods
to handle more general probability model classes.

65
Thermodynamic Integration
In order to overcome the diﬃculties that arise when trying to compute the evidence
directly, the concept of path sampling and thermodynamic integration for evaluation
of model evidence are introduced along the lines of [2]. The path sampling estimate
of evidence is obtained by integrating the log-likelihood function over distributions
intermediate to the prior and the posterior. Such intermediate distributions arise
naturally in annealing-based sampling schemes, such as AIMS. If these intermediate
distributions are chosen carefully, then the aforementioned integration of the log-
likelihood function can be performed numerically. Path sampling is also numerically
stable since it deals with the logarithm of the likelihood function at every stage of
the calculation, for which regular ﬂoating point arithmetic is suﬃcient [3].
Path sampling employs samples from distributions intermediate to the prior and
posterior in order to reduce the variance of the evidence estimate. An exponent,
β ∈[0, 1], controls the contribution of the likelihood function to the intermediate
distribution. The following function is the ﬁrst step in constructing the desired
intermediate distribution:
qβ(Y, θ) ∆= [p(Y|θ, M]βp(θ|M)
(5.4)
Also of importance is the logarithm of this intermediate, un-normalized distribution:
log qβ(Y, θ) = β log p(Y|θ, M) + log p(θ|M)
(5.5)
The normalizing constant needed to turn this function into a distribution on θ is:
Zβ(Y) =
∫
qβ(Y, θ)dθ
(5.6)
We may thus construct the intermediate probability distribution as:
pβ(θ|Y) = qβ(Y, θ)
Zβ(θ)
(5.7)
As is evident now, the role of β is to control the transition of the prior distribution
into the posterior distribution as it ranges over [0, 1].
For β = 0, we can see
that the intermediate distribution is equal to the prior: pβ=0(Y, θ) = p(θ).
At
the other extreme, for β = 1, the intermediate distribution equals the posterior:
pβ=1(θ|Y) = p(θ|Y, M).

66
The quantity of interest to us is the normalizing constant Zβ=1, which corresponds
to the evidence term for our model class:
Zβ=1(Y) =
∫
p(Y|θ, M)p(θ)dθ
(5.8)
It turns out that the logarithm of this evidence constant can be determined by
thermodynamic integration as follows [2]:
∂log Zβ(Y)
∂β
=
1
Zβ(Y)
∂Zβ(Y)
∂β
(5.9)
=
1
Zβ(Y)
∂
∂β
∫
qβ(Y, θ) dθ
(5.10)
=
1
Zβ(Y)
∫∂qβ(Y, θ)
∂β
dθ
(5.11)
=
∫
1
Zβ(Y)
∂qβ(Y, θ)
∂β
dθ
(5.12)
⇒log Zβ(Y)

1
β=0
=
∫1
β=0
∫
1
Zβ(Y)
∂qβ(Y, θ)
∂β
dθdβ
(5.13)
⇒log Zβ=1(Y) −log Zβ=0 =
∫1
0
∫
pβ(θ|Y)
qβ(D, θ)
∂qβ(Y, θ)
∂β
dθdβ
(5.14)
⇒log Zβ=1(Y) −log 1 =
∫1
0
Eθ∼pβ(θ|Y)

1
qβ(Y, θ)
∂qβ(Y, θ)
∂β

dβ
(5.15)
⇒log p(Y|M) =
∫1
β=0
Eθ∼pβ(θ|Y)
∂log qβ(Y, θ)
∂β

dβ
(5.16)
Using Equation (5.16) now have an expression for the log-evidence in terms of a
path integral over the expected value of the log-likelihood function.
log p(Y|M) =
∫1
β=0
Eθ∼pβ(θ|D) [log p(Y|θ, M)] dβ
(5.17)
Numerical estimates
Eqn. (5.17) can be estimated numerically. First, the integral over the β unit interval
can be approximated by a numerical integration scheme such as the trapezoidal rule
or the Gauss-Kronrod numerical integration scheme.
A(Y) ∆= log p(Y|M) ≈
Nw
Õ
i=1
wiEθ∼pβi(θ|Y) [log p(D|θ, M)]
(5.18)

67
The question remains as to how to compute the integrand each discrete value of β.
For each discrete value of β, a simple Monte Carlo estimate for the expected value
using samples from an MCMC algorithm will suﬃce.
For a sampling algorithm such as AIMS that uses simulated annealing [4], the
tempering schedule can be chosen to match that of the integration points for path
sampling.
The sampling algorithm would then automatically generate a set of
samples from the intermediate distributions, which, in turn, could be used to deter-
mine the appropriate expected values of the log-likelihood in order to construct a
numerical estimate of the path sampling integral. That is,
A(Y) ≈
Nw
Õ
i=1
wi
Ns,i
Ns,i
Õ
k=1
log p

Y|θ(i,k), M

(5.19)
5.3
Duﬃng oscillator
For the system identiﬁcation of structures using Bayesian updating, competing model
classes can be compared in order to explain the observed response of the structure
to excitation.
For instance, when the amplitude of excitation is low, a linear dynamical system
with an appropriate stochastic embedding is typically used to explain the behavior
of engineering structures, for example in ([5]).
However, at larger amplitudes,
non-linear geometric and material eﬀects, such as plasticity, may come into play.
Were it possible to eﬃciently evaluate the evidence term, then Bayesian model class
selection would be applicable.
The Duﬃng oscillator is perhaps the most straightforward extension of the equation
of motion for the linear oscillator to the non-linear case. This example will be
used to demonstrate model class selection by evaluating the evidence term using
AIMS-based path sampling.
The governing diﬀerential equation for a sinusoidally-driven Duﬃng oscillator may
be taken as:
Üx + δ Ûx + βx + αx3 = γ cos ωt
(5.20)
Therestoringforcethereforenotonlyincludesaspring-likedisplacement-proportional
term but also a cubic non-linear term.

68
Non-linear behavior
The coeﬃcient α in Eqn. (5.20) controls the type and extent of non-linear behavior
in the Duﬃng oscillator. To gain insight into the speciﬁc nature of the non-linearity
of this governing equation as in [6], we consider a simple case of the undamped
oscillator with a potential given by:
V(x) = x4
4 −x2
2
(5.21)
When subject to zero damping and input excitation, this is a conservative system
with a double-well potential that can be described by the state-space diﬀerential
equation:
(
Ûx
Ûv
)
=
(
v
x −x3
)
(5.22)
There are two stable equilibrium points at unit distance from the origin along the
axis of displacement. The equilibrium point at the origin is unstable [7].
In state space, the trajectory of such a particle is closed due to the conservation of
energy, as in Figure 5.1. Since the diﬀerential equation is non-linear, however, the
solution is not sinusoidal. This system corresponds to the undamped free-vibration
case with α = 1 and β = −1 in Eqn. (5.20) with the damping and forcing coeﬃcients
zero. In this case, while the non-linear term is the restoring force, the linear term is
a repelling force.
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
Displacement x →
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Velocity v →
Conservative Duffing oscillator in double-well potential
Figure 5.1: The closed state-space trajectory of a double-well Duﬃng oscillator
When this case is extended to include damping and forcing, the Duﬃng oscillator
can behave in the rather complex fashion that non-linear systems commonly exhibit.

69
As the amplitude of the forcing input increases, the system can exhibit a period
doubling bifurcation, as shown in Figure 5.2.
2.0
1.5
1.0
0.5
0.0
0.5
Displacement x →
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Velocity v →
Forced Duffing oscillator: Period doubling
Figure 5.2: Period doubling bifurcation for forced, damped Duﬃng oscillator
With even further increases in the forcing amplitude, the system exhibits chaos and
unpredictably spends its time near one or the other equilibrium point. An example
of such a trajectory is shown in Figure 5.3. Note, however, that the ﬁgure only shows
that a stable limit cycle is not achieved and does not reveal the intricacies of the
underlying chaotic behavior, such as sensitivity to initial conditions.
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
Displacement x →
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Velocity v →
Forced Duffing oscillator: Chaos
Figure 5.3: Chaotic trajectory for a forced, damped Duﬃng oscillator
In the case where α, β > 0, there is no double-well potential. The only equilibrium
point is at the origin and it is stable. This may be thought of as the case of a linear
oscillator with an added cubic restoring force. When the forcing amplitude is high

70
relative to the damping, this system also exhibits period doubling bifurcations and
eventually chaos.
In this section, a speciﬁc stochastic embedding of the Duﬃng oscillator is considered,
according to how Bayesian inference might typically be done in practice. The focus
of this work is on using Bayesian model class selection between the linear and
Duﬃng stochastically embedded model classes. The bifurcation and chaotic regime
of the Duﬃng oscillator will not be considered here since the likelihood function
would vary quite sensitively with respect to the initial condition parameters. This
warrants a separate investigation to verify that the posterior is indeed stable under
the minute errors associated with ﬁnite-precision arithmetic.
5.4
Numerical example
Here, we consider a speciﬁc example. Let there exist two model classes, ML and
MD, that correspond to a linear and Duﬃng oscillator model classes respectively.
We parameterize the model class with uncertain parameters ζ ∈R5 and θ ∈R6,
respectively.
The uncertain parameters considered are: θ = [δ, β, γ, ω, log σ, α]. ζ consists of the
same uncertain parameters, except for the non-linear coeﬃcient, α, which is omitted
for the linear oscillator, which is equivalent to ﬁxing it to zero.
Synthetic data generation
Denote the synthetic, noisy data generated from the linear and Duﬃng oscillators
by DL and DD respectively.
A total of 500 data points, spaced equally apart in time at 40 ms intervals, are
generated for either case. The values for the data-generating parameters are listed
in Table 5.1.
The data at these time-points consists of the displacement prediction xn(θ∗) or xn(ζ∗)
along with zero-mean independent Gaussian noise of standard deviation σ∗added
to it, to mirror the imperfections that one might ﬁnd in real-world data.
Stochastic embedding
Given data from the oscillator, on which Bayesian inference for the Duﬃng forward
model is to be performed, the speciﬁcation of a stochastic forward model along with
an appropriate prior becomes necessary.

71
Table 5.1: Speciﬁcation of the data-generating parameters for ML and MD
Component
Linear
Model (ζ∗)
Duﬃng
Model (θ∗)
δ∗
0.1
0.1
β∗
1.0
1.0
α∗
N/A
1.0
γ∗
0.3
0.3
ω∗
1.4
1.4
log σ∗
-4.0
-4.0
For the stochastic forward model, we assume that displacement data are available
at discrete points in time and that these measurements are equal to the predictions
corrupted by stationary, Gaussian white noise. That is,
yn = xn(θ) + ϵn(σ), where ϵn ∼N(·|0, σ2) i. i. d.
(5.23)
The conditional probability of observing a certain data set of measurements, DN =
[yn]N
n=1, is then:
p(DN|θ, σM) =
N
Ö
n=1
N(yn|xn(θ), σ2)
(5.24)
The prediction is obtained by numerically solving the trajectory of the Duﬃng
oscillator:
Üx = −θ1 Ûx −θ2x −θ3x3 + θ4 cos(θ5t), with x(0) = 0 and Ûx(0) = 0
(5.25)
It is often desirable to look at the logarithm of the quantity in Eqn. (5.24) for several
reasons: ﬁrst, this is often the form in which computer programs can work with since
the stochastic forward model, being a product of probability densities in this case,
can be a very small number; and second, one can see how this stochastic forward
model relates to non-linear least-squares estimation That is,
L = log p(DN|θ, M) = N
2 log(2πσ2) +
1
2σ2
N
Õ
n=1
(ˆyn −xn(θ))2
(5.26)
The additive noise model has been chosen here for its simplicity. More complex
ones such as the additive-multiplicative noise model involve more free parameters
and are harder to deal with both computationally and analytically. However, they

72
may be able to model the problem more realistically, since it works with relative
values of prediction errors.
The other important component of the stochastic embedding that needs speciﬁcation
is the prior on the uncertain parameters. In engineering problems, the prior mean
and variance of the system parameters are typically constrained to be in line with the
engineer’s judgment, which in turn would be consistent with the design speciﬁcations
of the system. With these constraints, a prior can be chosen according to the principle
of maximum entropy, which simply means that no unwarranted additional entropic
certainty should be introduced in the prior other than the desired moments such
as the mean and variance. This is to reﬂect the true state of knowledge about the
parameters that does not incorporate unintended additional information that could
possibly mislead one into obtaining an overly narrow posterior distribution over the
parameters.
In the example considered in this section, synthetic data is generated from a known
parameter vector. To reﬂect the uncertainty over the parameters in a real application
of the problem, a prior distribution, p(θ|M) over the parameters, θ, is speciﬁed.
The prior distribution over the parameters is chosen to be independent in each
component. That is:
p(θ|M) =
Nθ
Ö
i=1
p(θi|M)
(5.27)
The following describes the choice of bounds over the uncertain parameters for the
Duﬃng oscillator. The distribution is the same for the linear oscillator, only the
non-linear coeﬃcient, α, is removed from the parameter vector. Table 5.2 details
the prior for each component.
Table 5.2: Speciﬁcation of priors for ML and MD
Component
Linear Model
Duﬃng Model
δ
U[0.0, 0.3]
U[0.0, 0.3]
β
U[0.5, 1.5]
U[0.5, 1.5]
γ
U[0.0, 0.5]
U[0.0, 0.5]
ω
U[1.0, 1.5]
U[1.0, 1.5]
log σ
U[-5.0, -3.0]
U[-5.0, -3.0]
α
N/A
ln N[µ=0.0, σ2=0.1]
All parameters except for the non-linear coeﬃcient, α, were chosen to take the same
value range in both models.

73
0
5
10
15
20
t →
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
x →
Displacement vs Time Plots data for model selection
Linear
Duffing
Figure 5.4: Displacement trajectory plots of the data data from a linear oscillator
(blue) and a Duﬃng oscillator (red)
Figure 5.4 shows the plot of the displacement trajectories for the data-generating
samples from either model class.
Notice that the peaks and troughs for either
trajectory begin to mis-align after one oscillation cycle about 7 seconds in.
In
addition, the amplitude of oscillation of the Duﬃng oscillator is higher beyond the
second cycle.
Prior sensitivity analysis
We would like our estimates of evidence to not be sensitive to perturbations in the
prior. In the example of the linear oscillator, the prior is a uniform distribution on a
hyperrectangle in a d-dimensional parameter space:
Ω=
d
Ö
i=1
[ai, bi]
(5.28)
The expression for the independent uniform prior is therefore:
p(θ|M) = 1
|Ω|1Ω
(5.29)
Here, |Ω| denotes the volume of the region, Ω. If we denote the likelihood function
by f (θ) ∆= p(D|θ, M), then we have an expression for the evidence term under the

74
original, unperturbed prior:
p(D|M) =
∫
f (θ)p(θ)dθ
(5.30)
= 1
|Ω|
∫
Ω
f (θ)dθ
(5.31)
Now, consider a perturbation of the prior, wherein the new prior is again uniform, but
with the interval along each dimension extended in both directions by a perturbation
factor. That is,
Ω′ =
d
Ö
i=1
[ai −ϵi, bi + ϵi]
(5.32)
Observe that we can approximate the new region volume as:
|Ω′| ≈|Ω| + 2d
d
Õ
i=1
ϵi
Ö
j,i
(bj −aj) + O(∥ϵ∥2)
(5.33)
≈|Ω|
 
1 + 2d
d
Õ
i=1
ϵi
(bi −ai)
!
(5.34)
= |Ω|(1 + c · ϵ)
(5.35)
Again, the deﬁnition of the density follows from the deﬁnition of the region:
pϵ(θ|M) =
1
|Ω′|1Ω′ ≈
1 −c · ϵ
|Ω|

1Ω′
(5.36)
We therefore have an expression for the perturbed model evidence:
pϵ(D|M) =
∫
f (θ)pϵ(θ)dθ
(5.37)
=
1
|Ω′|
∫
Ω′ f (θ)dθ
(5.38)
≈1 −c · ϵ
|Ω|
∫
Ω′ f (θ)dθ
(5.39)
We now analyze the integral. We can break it up as:
∫
Ω′ f (θ)dθ =
∫
Ω
f (θ)dθ +
∫
Ω′\Ω
f (θ)dθ
(5.40)
Now, let the likelihood function be bounded by M in the region Rd \ Ω. This would
be a function of the data and so we may write M(D) instead. Then it will also be

75
bounded by M(D) in the region Ω′\Ω. Also, the likelihood function is non-negative
since it is a density when viewed as a function of the data. Hence, we have:
0 < f (θ) < M(D), ∀θ ∈Ω′ \ Ω
(5.41)
Thus, we may bound the left hand side of Eqn. (5.40) as:
∫
Ω′ f (θ)dθ <
∫
Ω
f (θ)dθ + M(D)|Ω′ \ Ω|
(5.42)
[The assumption that the likelihood function is bounded in a region outside that of
the uniform prior is not an unreasonable one, since the stochastic forward model is
usually bounded by choice. For instance, a common choice is:
p(D|θ, M) =
Nt
Ö
n=1
N(yn|xn(θ), σ2(θ))
(5.43)
So long as no choice of θ results in zero variance in the Gaussian distribution, we
can always bound this quantity.]
Note that:
|Ω′ \ Ω| ≈2d
d
Õ
i=1
ϵi
Ö
j,i
(bj −aj) + O(∥ϵ∥2)
(5.44)
= 2d|Ω|(c · ϵ) + O(∥ϵ∥2)
(5.45)
Substituting this result in Eqn. (5.42), we get:
∫
Ω′ f (θ)dθ <
∫
Ω
f (θ)dθ + 2dM(D)|Ω|(c · ϵ) + O(∥ϵ∥2)
(5.46)
This may be substituted in the original expression for the perturbed evidence in Eqn.
(5.39) to give:
pϵ(D|M) < (1 −c · ϵ)

p(D|M) + 2dM(D)(c · ϵ) + O(∥ϵ∥2)

(5.47)
= p(D|M) + O(∥ϵ∥)
(5.48)
Thus, at least in this problem, perturbations to the prior within the same family of
priors results in changes of the same order as the perturbation. This is a desirable
property to have for a problem, since it implies that the value of the evidence is not
brittle to minute changes in the prior ([8]).

76
5.5
Simulation Results
The posterior samples that AIMS produces are samples of dimension 7 or 8 de-
pending on the model class. There is no good way of visualizing such sample sets.
However, we can work with summary statistics as well as histograms of the marginal
posterior distributions of the samples for each parameter. It is to be kept in mind,
however, that such visualizations are limited and may not be able to convey the
intricate correlations between the parameters that are introduced by the data upon
performing the Bayesian update.
Figures 5.5 through 5.8 are histograms of the marginal distribution for each parame-
ter dimension of the posterior samples generated by AIMS for the linear and Duﬃng
oscillators. Superposed over each histogram is the data-generating parameter value
in dashed blue as well as the maximum a posteriori (MAP) sample from AIMS in
dotted red.
0.0
0.3
0.1
0.1
0
2900
δ
0.5
1.5
1.0
1.0
0
2903
β
0.0
0.5
0.3
0.3
0
2907
γ
1.0
1.5
1.4
1.4
0
2905
ω
-5.0
-3.0
-4.0
-3.5
0
2899
logσ
Posterior Marginal Histograms for case (DL,ML)
Figure 5.5: Posterior component-wise marginal histograms for p(ζ|DL, ML)
Figure 5.5 plots the marginal histograms for the linear oscillator data under the linear
model class, that is (DL, ML). We can see that the distribution is quite peaked and
appears to be unimodal. The mode is not centered at the data-generating estimate.
This is because in a problem with a moderate number of dimensions such as this,
a sampling algorithm may get stuck in a local minimum if it does not generate a
candidate near the data-generating sample.

77
0.0
0.3
0.1
0.1
0
2920
δ
0.5
1.5
1.0
0.8
0
2921
β
0.0
0.5
0.3
0.3
0
2921
γ
1.0
1.5
1.4
1.4
0
2922
ω
-5.0
-3.0
-4.0
-3.5
0
2921
logσ
0.5
1.5
0.0
0.8
0
2926
α
Posterior Marginal Histograms for case (DL,MD)
Figure 5.6: Posterior component-wise marginal histograms for p(ζ|DL, MD)
In contrast to Figure 5.5, we observe in Figure 5.6 that the marginal histograms
for explaining the data from the linear oscillator (DL) using the Duﬃng model are
not as peaked for some components. Note that in this case, since we presupposed
non-linear behavior, the parameter α is not allowed to take the zero value. Because
of this, no good data-ﬁt is obtained and there is a correspondingly larger uncertainty
associated with some of the parameters.
0.0
0.3
0.1
0.2
0
2911
δ
0.5
1.5
1.0
1.3
0
2919
β
0.0
0.5
0.30.3
0
2911
γ
1.0
1.5
1.4
1.4
0
2911
ω
-5.0
-3.0
-4.0
-3.3
0
2913
logσ
Posterior Marginal Histograms for case (DD,ML)
Figure 5.7: Posterior component-wise marginal histograms for p(ζ|DD, ML)
Figure 5.7 attempts to ﬁt the data from a Duﬃng oscillator with a linear model. In
this case, we do not anticipate a sample that might result in a very good ﬁt at all
data points, since we already know that the non-linear behavior of DD cannot be
captured by ML. For this speciﬁc 20 second window, however, the linear model is
able to identify parameters that provide a good data ﬁt. These do not correspond

78
to the data-generating parameters, however. The damping, δ and linear stiﬀness, β,
have high marginal posterior probability since they provide a good data-ﬁt.
0.0
0.3
0.1
0.1
0
2983
δ
0.5
1.5
1.01.1
0
2981
β
0.0
0.5
0.3
0.3
0
2981
γ
1.0
1.5
1.4
1.4
0
2983
ω
-5.0
-3.0
-4.0
-3.5
0
2981
logσ
0.5
1.5
1.0
0.8
0
2981
α
Posterior Marginal Histograms for case (DD,MD)
Figure 5.8: Posterior component-wise marginal histograms for p(ζ|DD, MD)
Finally, Figure 5.8 is the result of trying to perform inference on DD with the
Duﬃng model class, MD.
AIMS is able to ﬁnd the posterior parameters that
provide the best data-ﬁt. Note that for this 20-second window, the most probable
marginal damping and forcing parameters, δ, γ and ω, are nearly equal to their
data-generating values.
The stiﬀness and non-linear coeﬃcients are noticeably
diﬀerent from the data-generating values and appear to have high marginal posterior
probability.
The most probable samples from these histograms are now used to simulate the
oscillator and the results are compared with the original data that was generated.
Figures 5.9 through 5.13 compares the prediction of the maximum a posteriori
sample with the observed data.

79
0
5
10
15
20
t →
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
x →
Displacement vs Time Plots for case (DL,ML)
Data
MAP sample
Figure 5.9: Displacement trajectory plots for data from a linear oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a linear oscillator
model class
Figure 5.9 is a plot of the linear oscillator data against the displacement prediction
for the most probable posterior sample. The prediction is a good ﬁt to the data and
values for the system parameters closely match the data-generating values.
0
5
10
15
20
t →
0.6
0.4
0.2
0.0
0.2
0.4
0.6
x →
Displacement vs Time Plots for case (DL,MD)
Data
MAP sample
Figure 5.10: Displacement trajectory plots for data from a linear oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a Duﬃng oscillator
model class
Figure 5.10 shows how the Duﬃng model class has trouble providing a good-data ﬁt.
Figure 5.11 is the plot of the prior marginal density for the non-linear parameter, α
for the Duﬃng model class. The log-normal prior on the non-linear term penalizes
values of α close to 0, and so the posterior contains low probability mass near the

80
data-generating parameter. Thus, the maximum a posterior sample contains a value
of α that is consistent with the prior, that can provide a good data-ﬁt.
0
1
2
3
4
5
α →
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density →
Log-normal density for non-linear parameter, α
Figure 5.11: Probability density function for the log-normal prior marginal for the
non-linear coeﬃcient, α
Figure 5.12 is interesting because for this 20 second window, the posterior is able
to ﬁnd a sample that provides a good data-ﬁt. However, the most probable posterior
sample does not match the data-generating sample for the parameters very well.
0
5
10
15
20
t →
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
x →
Displacement vs Time Plots for case (DD,ML)
Data
MAP sample
Figure 5.12: Displacement trajectory plots for data from a Duﬃng oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a linear oscillator
model class
Finally, in Figure 5.13, we can see that the most probable sample is able to provide
a good data-ﬁt and match the oscillation phase. There appears to be some trade-
oﬀbetween the linear and nonlinear stiﬀness coeﬃcients, since the most probably
sample has noticeably diﬀerent values for those parameters. This is probably because

81
within the tolerance of the noise added, these alternative posterior values also have
high probability.
0
5
10
15
20
t →
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
x →
Displacement vs Time Plots for case (DD,MD)
Data
MAP sample
Figure 5.13: Displacement trajectory plots for data from a Duﬃng oscillator (blue)
against the maximum a posteriori sample trajectory (red) from a Duﬃng oscillator
model class
Table 5.3: Data generating samples and MAP estimates for each scenario
Quantity
Data-
generating
values
Linear
model
(Linear
data)
Duﬃng
model
(Linear
data)
Linear
model
(Duﬃng
data)
Duﬃng
model
(Duﬃng
data)
δ
0.1
0.1080
0.0924
0.2024
0.1000
β
1.0
1.0017
0.8375
1.2702
1.0951
γ
0.3
0.3043
0.2952
0.3362
0.2764
ω
1.4
1.3988
1.4043
1.3808
1.3950
log σ
−4.0
-3.5239
-3.5106
-3.3213
-3.5298
α
1.0
N/A
0.8184
N/A
0.8061
Table 5.3 has the data-generating sample used for generating the synthetic data for
the model selection example, along with the MAP estimates for the samples.
Now, in order to evaluate the log-evidence using path sampling, AIMS was used
to draw samples from distributions intermediate to the prior and posterior along a
thermodynamic integration curve parameterized by b (b is used here since β has
been used for the stiﬀness of the oscillator in this section). The following AIMS
parameters and schedules were selected:

82
Table 5.4: AIMS parameters
Parameter
Description
Value
Comment
γa
Relative eﬀective
sample size
0.9
Highest value greater
than the recommended
0.5 and requiring rea-
sonable computer time
cj
Proposal
ker-
nel
standard
deviation
Varies
Adaptively
chosen
equal
to
mean
sam-
ple
nearest-neighbor
distance
Nj
Number of sam-
ples per annealing
stage
10,000
Highest constant per-
mitted by computation
time
The following values for the logarithm of the evidence were obtained for each case
with diﬀerent numerical integration schemes:
Table 5.5: Log-evidence values from thermodynamic integration
Measurements
Model
Trapezoid Gauss
Kronrod
Linear (DL)
Linear (ML)
1204.87
1214.37
1206.41
Linear (DL)
Duﬃng (MD)
1001.2
996.356
1002.53
Duﬃng (DD)
Linear (ML)
983.415
994.515
988.207
Duﬃng (DD)
Duﬃng (MD)
1087.86
1103.93
991.423
Table 5.5 lists the values for the logarithm of the evidence obtained for each case.
It should be noted that the values obtained using the Gauss/Kronrod quadrature are
not as reliable as those obtained by Trapezoidal integration. This is because the
quadrature methods do not give very accurate results for curves with sharp corners
near the boundary: here, at β = 0. This, in turn, is because there is a steep rise in the
value of expected log-likelihood in going from the prior stage to the ﬁrst annealed
stage. The thermodynamic integration curve for the linear case is considered in
Figure 5.14.

83
0
20
40
60
80
100
j →
350000
300000
250000
200000
150000
100000
50000
0
50000
logL →
Thermodynamic curve for case (DL,ML)
Figure 5.14: Thermodynamic integration curve for the (DL, ML) case
We also observe that when the the “correct" model class is employed, the model
evidence is clearly much higher compared to the alternative.
5.6
Conclusion
These results show that path sampling with thermodynamic integration can be used
to give reasonable estimates for model evidence in the case of a stochastically
embedded dynamical model with a moderate number of dimensions. The AIMS
algorithm is naturally suited to generating the samples required by the path sampling
scheme.
In addition, it was found that when the model class containing the data-generating
model is used along with a suitable prior, then the evidence for that model class
turns out to be higher than that for competing classes.
References
[1]
J. L. Beck, K. V. Yuen, “Model selection using response measurements:
Bayesian probabilistic approach,” Journal of Engineering Mechanics, vol. 130,
no. 2, pp. 192–203, 2004.
[2]
B. Calderhead, M. Girolami, “Estimating Bayes factors via thermodynamic in-
tegration and population MCMC,” Computational Statistics & Data Analysis,
vol. 53, no. 12, pp. 4028–4045, Oct. 2009.
[3]
A. Gelman, X.-L. Meng, “Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling,” Statistical Science, vol. 13, no.
2, pp. 163–185, May 1998.

84
[4]
J. L. Beck, K. M. Zuev, “ASYMPTOTICALLY INDEPENDENT MARKOV
SAMPLING: A NEW MARKOV CHAIN MONTE CARLO SCHEME FOR
BAYESIAN INFERENCE,” International Journal for Uncertainty Quantiﬁ-
cation, vol. 3, no. 5, pp. 445–474, 2013.
[5]
K.-V. Yuen, J. L. Beck, L. S. Katafygiotis, “Uniﬁed Probabilistic Approach for
Model Updating and Damage Detection,” Journal of Applied Mechanics, vol.
73, no. 4, p. 555, 2006.
[6]
P. Holmes, D. Rand, “The bifurcations of duﬃng’s equation: An application of
catastrophe theory,” Journal of Sound and Vibration, vol. 44, no. 2, pp. 237–
253, Jan. 1976.
[7]
C. Holmes, P. Holmes, “Second order averaging and bifurcations to subhar-
monics in duﬃng’s equation,” Journal of Sound and Vibration, vol. 78, no. 2,
pp. 161–174, Sep. 1981.
[8]
H. Owhadi, C. Scovel, T. Sullivan, “On the Brittleness of Bayesian Inference,”
SIAM Review, vol. 57, no. 4, pp. 566–582, Jan. 2015.

85
C h a p t e r 6
CONCLUSIONS AND FUTURE WORK
The work in this thesis addresses the problem of optimal sensor placement for
Bayesian parametric identiﬁcation of structures. Several technical aspects of the
problem were addressed, along with the development of a method of solution for a
speciﬁc version of the problem.
An eﬃcient method of solution to the log-determinant formulation of the sensor se-
lection problem was presented by relaxing the original Boolean-vector independent
optimization variable to one that can take any values on the unit interval. Since
the resulting conﬁguration is not Boolean, resolutions to interpretations of possible
ambiguous results are presented. It should be noted that the solution is sometimes
unambiguous despite the constraints and in this case this is the optimal solution to
the original combinatorial problem.
When the list of possible sensor locations is over a continuum or a large number of
points, then certain technical challenges in modeling the problem arise. A descrip-
tion of them is presented and possible resolutions to the problem are discussed, in
the context set by the example of an elastic axial bar. In the absence of using a spec-
iﬁed correlation model, it is best to choose points on a pre-deﬁned grid suﬃciently
spaced apart for which prediction-error correlations are negligible. This prevents
sensor clumping that occurs as the characteristic length scale of the sensors goes to
zero.
When the simplifying assumptions that allow the use of the log-determinant problem
formulation are violated, then the general entropy-based objective used as a criterion
for sensor conﬁguration optimality is the mutual information between the data
variable and the uncertain parameters, given the model class. For practitioners, this
quantity may be perceived as the expected gain in information entropy in going from
the prior to the posterior distribution.
The mutual information in this formulation could not be expressed simply in terms
of the sensitivity coeﬃcients of the predictions. In the absence of analytical ap-
proaches to compute the objective, it became necessary to compute Monte Carlo
estimates of the mutual information. Simple examples were considered and the
mutual information estimate was obtained in these cases.

86
6.1
Future Work
The optimal sensor location problem in the context of Bayesian parametric identiﬁ-
cation continues to pose several interesting challenges.
The computational intractability of the general mutual information formulation is
the most challenging aspect of the problem, in the absence of simplifying model
assumptions about identiﬁability and shape of the posterior. This is because the
computation of the entropy-of-evidence is very challenging if samples from the
marginal distribution for the data predictions are used to compute a Monte Carlo
estimate. Improvements in simulation techniques for approximating log-evidence
would greatly ameliorate this problem.
In addition, a case was made to investigate the link between system identiﬁability
and the mutual information of a particular sensor conﬁguration.
When it comes to prediction-error correlations, there are two open problems. First,
is properly characterizing prediction-error correlations and determining the length
scales at which they occur, without making use of a preliminary sensor deploy-
ment. The second problem is to determine a method to compute relevant quantities
eﬃciently, for a speciﬁed prediction-error correlation model. Even in the case of
the formulation involving small prediction-errors or predictions slowly varying with
respect to the parameters, if prediction-error correlations are included, then the
objective does not neatly decompose into the log-determinant of the matrix sum of
contributions from each point on the sensor location grid. The convex optimization
method used, then does not apply directly to the optimal sensor location problem.
It is necessary, therefore, to develop a new solution that does not involve a search
over all possible conﬁgurations.

87
A p p e n d i x A
MATRIX CALCULUS IDENTITIES
A.1
Useful matrix calculus identities and properties
This appendix contains useful matrix and linear algebra identities.
Sum of rank-one matrices
For sample vectors ai ∈Rn, i = 1, . . ., N, the sum A =
N
Õ
i=1
aiaT
i :
• Is symmetric
• Is positive semi-deﬁnite
• Is positive deﬁnite if the samples ai span Rn
Gradient of log-determinant
The expression for the derivative of the log-determinant of a matrix parameterized
by a vector is given by,
∂log det Q(z)
∂zi
= tr

Q−1 ∂Q
∂zi

(A.1)
Note that in the current problem, we have,
∂Q(z)
∂zi
= ∂
∂zi
©­­
«
Nd
Õ
k=1
zkQ(ek)
ª®®
¬
(A.2)
= Q(ei)
(A.3)
Trace of matrix product
The trace of the product of two matrices, when they exist as expressed, is given by,
tr(XYT) = tr(XTY) = tr(YT X) = tr(Y XT) =
N
Õ
i=1
N
Õ
j=1
XijYij
(A.4)

88
In the current problem, this is applied in the following evaluation:
tr

Q−1(z)Q(ei)

=
N
Õ
j=1
N
Õ
k=1

Q−1
jk

Q(ei)

k j
(A.5)
Gradient of matrix inverse
The derivative of the inverse of a matrix parameterized by a vector is given by,
∂Q−1(z)
∂zi
= −Q−1 ∂Q
∂zi
Q−1
(A.6)

89
A p p e n d i x B
DERIVATIVES FOR CONVEX PROBLEM
B.1
Calculation of parametric-gradient for modal response
In order to evaluate the covariance shape matrix, Q, the gradient of the prediction
xi(t) with respect to the parameters θ are required.
Since the modal matrix Φ is independent of the uncertain parameters, the gradient
of the prediction may be written solely in terms of the gradient of each mode:
∇θs xi(tn) =
Nd
Õ
i=1
Φij∇θsqj(tn)
(B.1)
Now, qj(tn) is given by Equation 3.49. Its partial derivatives with respect to each
component of θs = [ω0, α, β, a0, ω] are calculated analytically.
Breakdown of modal solution
Here, the modal solution is broken down into more elementary terms for ease of
diﬀerentiation. The terms here are in shorthand and the explicit dependence on the

90
mode number, j, and time, t, is sometimes dropped. Deﬁne:
N1a =
ω3 + (2ζ2
j −1)ω2
jω
ωdj
(B.2)
N1b = exp(−ζjωjt)
(B.3)
N1c = sin(ωdjt)
(B.4)
N2a = 2ζjωjω
(B.5)
N2b = exp(−ζjωjt)
(B.6)
N2c = cos(ωdjt)
(B.7)
N3a = (ω2
j −ω2)
(B.8)
N3b = sin(ωt)
(B.9)
N4a = −2ζjωjω
(B.10)
N4b = cos(ωt)
(B.11)
N1 = N1a · N1b · N1c
(B.12)
N2 = N2a · N2b · N2c
(B.13)
N3 = N3a · N3b
(B.14)
N4 = N4a · N4b
(B.15)
D1 = (ω2
j −ω2)2
(B.16)
D2 = (2ζjωjω)2
(B.17)
Nj = N1 + N2 + N3 + N4
(B.18)
Dj = D1 + D2
(B.19)
qj(t) = aj
Nj
Dj
(B.20)

91
The gradient with respect to the uncertain parameters θs may now be expressed as,
∇qj(t) = qj(t)
∇aj
aj
+ ∇Nj
Nj
−∇Dj
Dj

(B.21)
∇Nj = ∇N1 + ∇N2 + ∇N3 + ∇N4
(B.22)
∇Dj = ∇D1 + ∇D2
(B.23)
∇N1 = N1
∇N1a
N1a
+ ∇N1b
N1b
+ ∇N1c
N1c

(B.24)
∇N2 = N2
∇N2a
N2a
+ ∇N2b
N2b
+ ∇N2c
N2c

(B.25)
∇N3 = N3
∇N3a
N3a
+ ∇N3b
N3b

(B.26)
∇N4 = N4
∇N4a
N4a
+ ∇N4b
N4b

(B.27)
Low-level derivatives
∂ωj
∂ω0
= cj
(B.28)
∂ζj
∂ω0
= cj
2
 
β −α
ω2
j
!
(B.29)
∂ζj
∂α =
1
2ωj
(B.30)
∂ζj
∂β = ωj
2
(B.31)
∂ωdj
∂ω0
= cj
q
1 −ζ2
j −
ζjωj
q
1 −ζ2
j
∂ζj
∂ω0
(B.32)
∂ωdj
∂α or β = −
ωjζj
q
1 −ζ2
j
∂ζj
∂α or β
(B.33)
Derivatives with respect to forcing amplitude
The only relevant term in this case is,
∂aj
∂a0
= −ΦT1
µj
(B.34)
Derivatives with respect to base natural frequency
The relevant terms in this case are those that depend on ωj and ζj:

92
∂N1a
∂ω0
=
 2ωjcjω(2ζ2
j −1)
ωdj
+
4ζjω2
jω
ωdj
∂ζj
∂ω0
!
−
 ω3 + (2ζ2
j −1)ω2
jω
ω2
dj
∂ωdj
ω0
!
(B.35)
∂N1b
∂ω0
= −tζjωjN1b
 1
ζj
∂ζj
∂ω0
+ cj
ωj

(B.36)
∂N1c
∂ω0
= t
∂ωdj
∂ω0
cos(ωdjt)
(B.37)
∂N2a
∂ω0
= 2ζjωjω
 1
ζj
∂ζj
∂ω0
+ cj
ωj

(B.38)
∂N2b
∂ω0
= ∂N1b
∂ω0
(B.39)
∂N2c
∂ω0
= −t
∂ωdj
∂ω0
sin(ωdjt)
(B.40)
∂N3a
∂ω0
= 2cjωj
(B.41)
∂N4a
∂ω0
= −∂N2a
∂ω0
(B.42)
∂D1
∂ω0
= 4ωjcj(ω2
j −ω2)
(B.43)
∂D2
∂ω0
= 2(2ζjωjω)2
 1
ζj
∂ζj
∂ω0
+ cj
ωj

(B.44)

93
Derivatives with respect to Rayleigh damping coeﬃcients
The relevant terms here are those that depend on ζj.
∂N1a
∂α or β =
 4ζjω2
jω
ωdj
∂ζj
∂α or β
!
−N1a
ωdj
∂ωdj
∂α or β
(B.45)
∂N1b
∂α or β = −tωjN1b
∂ζj
∂α or β
(B.46)
∂N1c
∂α or β = t cos(ωdjt)
∂ωdj
∂α or β
(B.47)
∂N2a
∂α or β = 2ωjω
∂ζj
∂α or β
(B.48)
∂N2b
∂α or β = −ωjt exp(−ζjωjt)
∂ζj
∂α or β
(B.49)
∂N2c
∂α or β = −t sin(ωdjt)
∂ωdj
∂α or β
(B.50)
∂N4a
∂α or β = −∂N2a
∂α or β
(B.51)
∂D2
∂α or β = 8ζjω2
jω2
∂ζj
∂α or β
(B.52)
Derivatives with respect to forcing frequency
The following terms are relevant:
∂N1a
∂ω =
3ω2 + (2ζ2
j −1)ω2
j
ωdj
(B.53)
∂N2a
∂ω = 2ζjωj
(B.54)
∂N3a
∂ω = −2ω
(B.55)
∂N3b
∂ω = t cos(ωt)
(B.56)
∂N4a
∂ω = −∂N2a
∂ω
(B.57)
∂N4b
∂ω = −t sin(ωt)
(B.58)
∂D1
∂ω = −4ω(ω2
j −ω2)
(B.59)
∂D2
∂ω = 8ζ2
j ω2
jω
(B.60)
Equations B.2 - B.60 can be used to compute the gradient of the displacements in
Equation B.1.

