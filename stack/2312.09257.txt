BRAIN-INSPIRED MACHINE INTELLIGENCE: A SURVEY OF
NEUROBIOLOGICALLY-PLAUSIBLE CREDIT ASSIGNMENT
Alexander Ororbia
Rochester Institute of Technology
Rochester, NY 14623
ago@cs.rit.edu
ABSTRACT
In this survey, we examine algorithms for conducting credit assignment in artificial neural net-
works that are inspired or motivated by neurobiology. These processes are unified under one
possible taxonomy, which is constructed based on how a learning algorithm answers a central
question underpinning the mechanisms of synaptic plasticity in complex adaptive neuronal sys-
tems: where do the signals that drive the learning in individual elements of a network come from
and how are they produced? In this unified treatment, we organize the ever-growing set of brain-
inspired learning schemes into six general families and consider these in the context of backprop-
agation of errors and its known criticisms. The results of this review are meant to encourage
future developments in neuro-mimetic systems and their constituent learning processes, wherein
lies an important opportunity to build a strong bridge between machine learning, computational
neuroscience, and cognitive science.
Keywords Credit assignment · Brain-inspired computing · Neuro-mimetic learning · Synaptic plasticity
1
Introduction
One of the central goals that underpins research in biologically-inspired, or neuro-mimetic, machine intelligence
is to create a complete theory of inference and learning that emulates how the brain can learn complex functions
from the environment. Such a theory would be one that is not only biologically-plausible but also one that makes
sense from a machine learning point-of-view [38]. Consequentially, this theory would be credible from both neuro-
science and statistical perspectives on learning, and as a result, could be empirically tested and validated from both
perspectives. Furthermore, progress in this burgeoning line of scientific inquiry could serve as a central basis for
the grander goals of neuro-mimetic cognitive architectures [90, 330, 362, 315, 380, 42] and the type of embodied
brain-inspired machine intelligence behind the more recently cast “NeuroAI” initiative [359, 337, 470, 288].
A challenging aspect in constructing the aforementioned theory centers around the development of a plausible
mechanism for conducting credit assignment – in support of optimizing a behavioral scoring function(s) – within
computational neural systems. Credit assignment itself refers to the act of assigning “credit” and/or “blame” to
individual processing elements, e.g., neuronal units, within a complex adaptive system based on their contribution
to final behavioral output(s). With respect to networks of neurons, credit assignment is particularly difficult given
that the impact or effect of early-stage neurons depends on downstream synaptic connections and neural activities.
This challenge has also been referred to as the credit assignment problem [34].
Although strong criticism of its biological plausibility has existed for several decades [141, 479, 73, 311, 154, 438],
the algorithm known as backpropagation of errors [248, 372], or backprop, has been almost exclusively used to
train modern-day, state-of-the-art artificial neural networks (ANNs) in supervised, unsupervised, and reinforcement
learning tasks/problems. Though elegant and powerful in its own right, as well as one of the driving forces behind
the “deep learning revolution” [229], it is one of the aspects of artificial neural computation that is the most difficult
to reconcile with the current insights and findings that we have from cognitive neuroscience. Furthermore, many
mechanisms and elements used to construct the computational architectures of current ANNs, such as normalization
operations including batch and layer normalization [186, 17], have been largely designed to address issues in credit
assignment rather than acting as architectural features in service of solving the task at hand [323, 326]. As we will
arXiv:2312.09257v2  [cs.NE]  23 Dec 2023

Preprint
see throughout in our treatment of various alternative algorithms, although initially inspired by properties/behaviors
of real neurons in the brain [273], even the processing elements that constitute deep neural networks (DNNs) omit
many of the details that characterize actual neurobiological mechanisms and dynamics. Ultimately, considering the
formalization and integration of other mechanisms and elements of computation that underlie biological neurons
[210, 211] might prove important in creating more powerful forms of credit assignment and facilitating more human-
like generalization abilities in DNNs, addressing issues such as reliability/model calibration, robustness, and sample
inefficiency. It is the view of this work that biological plausibility in the learning, inference, and design of artificial
neural systems is not solely a (niche) property of interest to neuroscientists and cognitive scientists; it will play
critical a role in the future of machine intelligence, such in efforts that seek implementation on low-energy analog
and neuromorphic chips [75, 138, 217, 466]. Furthermore, these biologically-inspired computational frameworks
will need to complemented with examination and evaluation from a behavioral point-of-view [24, 326, 247], either
considering how approaches to credit assignment scale to higher-dimensional, complex tasks [24] or investigating
how particular algorithms generalize in the context of modular cognitive architectures [315, 322]. As a result, new
forms of analysis and benchmarking, enriched by ideas and concepts from computer science, cognitive science,
and computational neuroscience [249, 263, 359, 60], will be needed to make consistent progress as well as new
breakthroughs that advance the current state of research towards flexible, robust brain-inspired intelligent systems.
Concretely, this review will focus on algorithms that have been proposed over the past several decades to train
ANNs without backprop; these methodological frameworks have sometimes referred to as “backprop-free” or
“biologically-inspired” learning algorithms. In order to better compare and organize the ever-growing plethora
of approaches, we construct a taxonomy based on answering one of the key questions that centrally motivates the
development of biologically-plausible forms of credit assignment: Where do the driving forces, or signals, behind
the synaptic plasticity needed for adapting a network’s processing elements come from and how are they produced?
The way that backprop answers this question, as we will shall soon examine, is what stands in contrast to many the-
ories of the brain [140, 354, 181, 67]. In this survey, algorithm clusters/families (“themes”) will be formulated with
respect to how various schemes attempt to answer this question; in the scope of this work, this yields six different
families. Along the way, we will consider how, and the extent to which, different biological algorithms address or
resolve the neurophysiological and engineering criticisms of backprop. It is hoped that the taxonomy and unified
treatment that we present of extant algorithmic proposals for biologically-motivated credit assignment will inform,
inspire, and aid the field in generating new ideas that extend, combine, or even supplant current methods.
Structure of the Survey. This article is organized in the following manner. First, to contextualize our review, we
provide, in Section 1.1, our survey’s notation/symbology, its running-example neural architecture, and a character-
ization of backpropagation of errors (backprop); we furthermore elucidate backprop’s core problems and sources
of biological implausibility. Following this, in Section 1.4, we present our framing question and the corresponding
taxonomy it induces to organize the approaches we will examine. In Section 2, we then review the various families
of neuro-mimetic credit assignment from the perspective of our taxonomy. To conclude, in Section 3, we then move
to synthesize the results in terms of addressing the issues inherent to backprop as well as highlight several important
future directions for research in neurobiological credit assignment and brain-inspired machine intelligence.
1.1
Algorithmic Framing
Notation. We start by defining key notation and symbols that will be commonly used throughout this survey (see the
supplement for tables containing symbol/operator and acronym definitions). A capital bold symbol M represents
a matrix while a lowercase bold one v is a vector; note that Mij retrieves a scalar at position (i, j). A matrix-
matrix/vector multiplication is denoted by ·, a Hadamard product is ⊙, and (v)T represents the transpose of v.
We will represent elementwise functions (applied to matrices or vectors) in terms of notation g(v) with the first
derivative (with respect to its input argument) denoted as ∂g(v).
Algorithmic Framing: The Neural System Context. To contextualize and unify the various learning processes
studied in this review, we introduce what we call the ‘neural system context’: this is the neuronal model framing and
information-processing pipeline that a credit assignment scheme would be situated and operate within. We consider
a neural system context to be one that specifies and implements (at least) the following elements:
• a computational architectural consisting of processing elements or set of interacting neuronal constructs,
e.g., such as an MLP graphical model (including its weight initialization scheme);
• an inference or information communication procedure across its elements, e.g., forward propagation of
activities in an ANN model;
• a credit assignment process which calculates/produces updates/adjustments to (synaptic) parameters (the
learning algorithm), e.g., backprop in the case of a standard DNN;
2

Preprint
Credit 
Assignment
Optimization 
Scheme
Inference/Sampling 
Process
Sensory 
Stream
Queries: 
y|x, p(y|x), p(y,x), p(x), p(x(t)|x(<t)) ...
TP / DTP / DTP-𝝈
LRA / rec-LRA
PC / NGC / BC-DIM
CHL / Gene-Rec
CD / SAP
Wake-Sleep
EProp 
Signal Type
Implicit
Hebbian (2F) 
Competitive
Explicit
Global
Local
Non-synergistic
Synergistic
RFA / DFA / IFA
KP/ WM
DRTP
HSIC
Neuromod / Hebbian (3F)
Discrepancy-Based
Energy-Based
Local Signalers 
Recirculation
SLU
SigProp
FF / PFF
PEPITA
EFP
Forward-Only
Credit Assignment 
Taxonomy
Computational
Architecture
Objective 
Function
Figure 1:
The neural system context and the proposed algorithm taxonomy shown. Our proposed credit assignment
taxonomy (see red zoom-in) centers around the question: What are the driving forces or signals that underlie synaptic plasticity?
Components in dashed box outlines indicate possible (direct or indirect) involvement in the design of the credit assignment.
Purple italicized text in the taxonomy indicate a leaf or algorithm family. Note: See supplement for acronym definitions.
• a parameter optimization function, or update rule, that takes in updates provided by the credit assignment
process and directly changes the values of the parameters Θ, e.g., stochastic gradient descent (SGD),
RMSprop [433], Adam [198];
• a problem-specific (global) cost function, usually determined by the desired task;
• a sensory stream, which could be a collection of patterns, as in a dataset D, or those drawn online from a
streaming data generating process.
The above points are graphically depicted in Figure 1, and as indicated by the red-tinted box “credit assignment”,
this survey will focus on the learning process of a neural system context. As indicated by the dashed boxes, the
“computational architecture”, “task objective”, and “inference/sampling process” will also be considered to the
extent in which they are involved in or influenced by the learning scheme.1 However, we remark that other compo-
nents not only provide the context for the learning and inference processes but, depending on setup, can significantly
affect their performance/behavior, e.g., choice of parameter optimization scheme, e.g., RMSprop versus Adam.
The Computational Architecture Running Example. Here, we will present the base form of our running ex-
ample of the computational neural architecture that backprop, as well as brain-inspired alternatives, perform credit
assignment with respect to.2 Specifically, we examine the popularly-used ANN structure known as the feedforward
network; see Figure 2. In essence, a feedforward ANN can be represented as a stack of nonlinear transformations,
or fΘ(x) = {fℓ(zℓ−1; θℓ)}L
ℓ=1, where the input is z0 = x. The dimension of any vector within this construct is
zℓ∈RJℓ×1 while x ∈RJ0×1 (assuming a mini-batch size of one) and y ∈RC×1. If we further simplify the ANN
to be a multilayer perceptron (MLP), each transformation zℓ= fℓ(zℓ−1) produces an output zℓfrom the value zℓ−1
of the previous layer using a matrix of synaptic weight values θℓ= {Wℓ} where Wℓ∈RJℓ×Jℓ−1. Each layer-wise
function fℓof the MLP fΘ(x) is made up of two operations (biases omitted for clarity):
hℓ= Wℓ· zℓ−1,
zℓ= ϕℓ(hℓ) ,
(1)
where ϕℓis an elementwise activation function, zℓ∈RJℓis the post-transformation of layer ℓ, and hℓ∈RJℓis the
pre-transformation vector of layer ℓ.3 Note that this computational framing is treating neurons as rate-based units;
the activity of any neuron i in a layer ℓof a network is described by its real-valued firing rate zℓ
i. The final output
prediction made by the MLP fΘ(x) is zL. The central goal of any credit assignment process will be adjust the values
within Θ so as minimize a task-specified loss/cost functional, i.e., a function that evaluates the quality of fΘ(x)’s
data-fitting ability and overall performance on a given task. Note that the parameters of the entire network would
1This relationship is what we will refer to later as the degree of entanglement.
2The caveat here is that some algorithm families examined in this survey essentially alter the nature/form of this architecture.
3In this survey, we will utilize “pre-activation” to refer to an incoming vector/set of neural activity values, i.e., zℓ−1, and
“post-activation” to refer to an outgoing vector/set of neural activity values, i.e., zℓ
3

Preprint
be written out as Θ = {W1, ..., Wℓ, ..., WL} and that some learning algorithms might introduce extra parameters
beyond those of the original architecture, e.g., inverse mapping or error feedback synapses.
z L
z L-1
y
ㅅ
W L
y
z 1
z 0 = x
W 1
z
W
Figure 2: An example MLP, processing sample (y, x).
The activation function for the output layer L, as
well as the cost function, of a feedforward network
are chosen based on the problem at hand.
For in-
stance, in the case of regression over unbounded con-
tinuous values, an identity activation function is typ-
ically employed, i.e., zL = ϕL(hL) = hL, paired
with a squared error cost function, i.e., L(y, zL) =
1
2
P
i(zL
i −yi)2. For task of classification, the activa-
tion function of the output is generally set to be the soft-
max: p(y|zL) = ϕL(zL) = exp(zL)/(P
j exp(zL
j ).
Any element in the output vector,
i.e.,
yj
≡
ϕL(v)j = p(j|v), is the specific scalar probability for
class j. In this case, the cost function typically chosen
is the categorical (multinoulli) negative log-likelihood:
L(y, zL) = −P
i (y ⊙log p(y|zL))i. Note that y is
the task-specific target vector (sometimes this will be
referred to as the ‘context’ vector), which could be a la-
bel/regression target, as is common in supervised learn-
ing setups, or even the data input itself, as is the case
for some unsupervised learning contexts.
1.2
Backpropagation of Errors
According to backprop-based adaptation [248, 372], and its many variations [94, 361], to conduct credit assignment
in a model such as the MLP, we take gradients of L(y, zL) with respect to each matrix in Θ, i.e., for each synaptic
matrix, our goal is to obtain another matrix ∂L(y,zL)
∂Wℓ
which contains synaptic adjustments; this adjustment matrix
can then be used in an optimization rule such as SGD. To obtain these necessary adjustments, we employ the chain
rule of calculus, i.e., reverse-mode differentiation [26], starting by computing gradients of the cost function (at the
output) and moving recursively backward through the computational graph of operations that defines the network.
Working back through the consequently long chain of operations4 step-by-step, layer-by-layer, the required gradient
∇ΘL(y, zL), or ∆Θ, is computed. Formally, calculating the required delta matrices is done as follows:
∆WL ∝∂L(y, zL)
∂WL
= ∂L(y, zL)
∂hL
∂hL
∂WL = ∂L(y, zL)
∂hL
· (zL−1)T
(2)
=
∂L(y, zL)
∂zL
∂zL
∂hL

· (zL−1)T =

δL ⊙∂ϕL(hL)

· (zL−1)T
(3)
∆Wℓ∝∂L(y, zL)
∂Wℓ
= ∂L(y, zL)
∂hℓ
∂hℓ
∂Wℓ=
 ∂L
∂hL
∂hL
∂zL−1
∂zL−1
∂hL−1 · · · ∂hℓ+1
∂zℓ
∂zℓ
∂hℓ
 ∂hℓ
∂Wℓ
(4)
=

(Wℓ+1)T ·
∂L
∂hℓ+2

⊙ϕ′(hℓ)

· (zℓ−1)T =

δℓ⊙∂ϕℓ(zL)

· (zℓ−1)T ,
(5)
where, above, we show first how to compute the adjustment for the output matrix WL and then how to compute the
model’s internal intermediate synaptic parameter matrices for any layer ℓ̸= L, i.e., Wℓ. Note that ∂ϕℓ(hℓ) is the
first derivative of activation ϕℓ() with respect to its pre-transformed vector hℓargument. In the above equations, we
group sets of differentiation operations (the gradient of the cost with respect to a layer ℓ’s pre-transformed activities
hℓ) into the delta construct δℓto further represent what is sometimes referred to as a “teaching signal” from the
backprop perspective. It is common to speed up the simulation of the above backprop credit assignment equations
by using more than one data point at a time, i.e., a mini-batch, for the requisite calculations. That is, one could either
use one single sensory input x for the equations above, i.e., z0 = x as in online learning, or one could substitute x
with matrix x ∈RJ0×B (and possibly y ∈RC×B) where B > 1 is the batch size, i.e., this is sometimes referred to
as batch-based learning. Figure 3 illustrates the resulting credit assignment induced by backprop on an MLP.
Once the gradient matrices have been calculated using Equations 3 and 5, resulting in ∆Θ = {∆Wℓ}L
ℓ=1, we
may then appeal to an optimization rule that changes the actual values of the MLP’s synaptic values (or efficacies)
4This is also what is referred to as moving back along the “global feedback pathway” [36, 323]; also see Figure 3.
4

Preprint
Figure 3: The global feedback pathway of backpropagation of errors. Depicted is the global feedback pathway, or backward
flow of recursively computed teaching signals, that characterizes backprop-based credit assignment.
in a subsequent step, e.g., through SGD with step size η which entails Wℓ←Wℓ−η∆Wℓ. We remark that,
although we have shown weight updates computed via the chain rule for an MLP network, the scheme of backprop-
based credit assignment can be applied to any type graph so long as it is restricted to be acyclic and its constituent
mathematical operators, e.g., linear transforms, elementwise activations, are differentiable. This includes long
operation chains, including the kind that compose unfolded recurrent neural networks or deep autoencoders; in these
cases, however, the task of credit assignment becomes much more difficult as the processing depth, or length, of the
chain increases. Notably, many variations of backprop have been proposed over the years, including [94, 360, 160].
1.3
The Problems with Backpropagation of Errors
As presented before, credit assignment entails identifying the degree to which neuronal processing units, within a
system, have an impact on a particular objective/cost function and, after doing so, adjusting their synaptic values
(efficacies) in order to improve performance in the future. In terms of error, this would mean that credit assignment
is engaged with assigning (partial) error values to every neural unit in service of minimizing a task-specific objective
while, in terms of reward, this would mean allocating (partial) reward values to each unit in service of maximizing
a task-central reward function. The updates made to synaptic parameters are made in terms of these computed
per-unit error/reward allocations; in the abstract, a similar process, at least with respect to error/reward-centric
optimization, has been theorized/observed to occur in the brain [354, 110, 111, 416, 417]. However, in the context
of deep learning, the manner in which backprop carries out this distributed allocation of credit is largely considered
to be implausible, with little to no neurophysiological evidence to support this form of learning. Over the decades
since its earliest critiques [142, 73], it has become more apparent that the backprop-based adaptation is unlikely to
occur in systems of real neuronal cells. In what follows, we explain several of backprop’s core problems and long-
standing issues, many of which are a mixture of both practical engineering issues and neurobiological criticisms.
The Global Feedback Pathway Problem. Deep neural models trained with backprop have long been known to
struggle with what has been labeled as vanishing and exploding gradients [34, 335] (we will also refer to these
collectively as “unstable gradients”), which lead to instability in the training/fitting process of a DNN. Specifically,
the issue of unstable gradients refers to the fact that the (Frobenius) norm of the gradients produced by backprop to
update DNN parameters grows (exponentially explodes) or shrinks (exponentially vanishes) throughout the course
of training; mathematically, a product of (Jacobian) matrices can grow towards infinity or shrink to zero (along a
particular vector direction) much in the same way that a similar length series of numbers would [335]. The issue
of unstable gradient values stems from the fact that a backprop-based scheme is attempting to conduct allocate
per-unit credit across a deep hierarchy of computational elements, percolating information recursively by coming
down/back from the entire system’s output layer to the lower hidden layers. The resulting long chain or pathway of
recursive operations has been referred to as the “global feedback pathway” [328, 323] (see Figure 3) and have been
argued to be an important aspect of error-driven learning inherent to backprop that needs to be addressed in order to
emulate the more robust, stable learning that characterizes natural neuronal networks. Note that the issues related
to credit assignment over deep hierarchies of computational units are exacerbated when training temporal neural
models on sequence data, such as recurrent neural networks (RNNs) [40], which require unfolding the neural model
backwards across time (an act in of itself that has been criticized to be quite biologically implausible [320]). The
resulting instability created by the global feedback pathway created by backprop (through time) makes it extremely
difficult for an RNN to learn correlations between temporally distant events.
The Weight Transport Problem. Weight transport, or the problem of symmetric synaptic connections, refers
to the fact that, in backprop, the same synaptic parameter matrices that are used to conduct inference are reused
in communicating information for the learning phase (see Figure 4a). This means that in a backprop-adapted
5

Preprint
W3
W
(
)
T
3
W2
W
(
)
T
2
W1
𝛅3
𝛅2
𝛅1
(a) The weight transport problem.
W3
W2
W1
Depends on
Depends on
Depends on
(b) The forward-locking problem.
W3
W2
W1
W3
Δ
W2
Δ
W1
Δ
𝛅3
𝛅2
𝛅1
Depends on
Depends on
(c) The update-locking problem.
Figure 4: Problems with backpropagation of errors. Depicted are three of the core issues underlying credit assignment that
is based on backprop (from Left to Right): the weight transport, the forward-locking, and the update-locking problem.
DNN, pre-synaptic neurons receive error information from post-synaptic ones through the very same synapses
that transmitted (input) information forward [141]. In neurobiology, this operation is not feasible in bio-chemical
synapses given that neurotransmitters and receptors enforce a unidirectional flow of information. As a consequence,
synaptic feedback loops in the brain are the result of leveraging two different sets of synapses [246, 247, 328, 453,
36]. Furthermore, it is entirely possible that feedback loops are not event present in some cases [407], which is an
aspect that certain brain-inspired algorithms attempt to address (e.g., forward-only learning; see end of Section 2.4).
The Inference-Learning Dependency Problem. This refers to the central dependency of the pathways between
those of learning and those of inference . This is a consequence of the fact that backprop-based models require
the presence of two heterogeneuos computational/calculation phases – a forward and a backward pass – each
characterized by their own constituent operations [372, 247]. This means that the adjustments made to synaptic
parameters, which driven by the backward transmission of error gradients (or “teaching signals”) of a global cost
function, inherently depend on the statistics produced by the forward propagation of information (the inference
phase), a conditional dependency that is not observed in real neuronal structures. In terms of the brain, this rela-
tionship between synaptic adjustments and neural activity values produced by forward propagation-based inference
in backprop would implausibility mean that biological neurons would require storage capabilities for memoriz-
ing the forward signals in direct support of learning. Furthermore, the computation underwriting inference and
learning in backprop needs to be precisely clocked to alternate between forward and backward propagation phases
[36] whereas, in the brain, there is no need for such external control [452], i.e., neurons perform computation
autonomously with little external routing information applied at externally-decided times.
The Problem of Locality and Locking. Another critical issue inherent to backprop-based learning is that the
rules/mechanisms which dictate synaptic updates are non-local (this is also a consequence of the inference-learning
dependency problem above) and are dependent upon the minimization of a globally defined cost function that itself
depends on the value of neural activities across the network, including those that are near the bottom of the hierarchy.
This strongly contrasts with how plasticity is believed to occur in biological neurons and the synapses that connect
them, i.e., neuronal adaptation and plasticity is believed to rely on information that is local in both space and time
[351, 157]. The non-locality inherent to backprop-based credit assignment further gives rise to three related sub-
problems: the forward-locking, backward-locking, and update-locking problems [188, 74]. The forward-locking
problem is the result of the very nature of (feed-forward) inference in modern-day DNNs; activity values of one
layer of processing elements depend on all of the existence/activities of the layers below/that come before them (see
Figure 4b); in other words, no single layer can process incoming information before the previous layers in a directed
graph have been executed. Update-locking itself stems from the problem of forward-locking – the adjustments to
one portion of the computational graph depend on computations that come after them (see Figure 4c); no layer’s
synapses can be updated before all of the dependent modules have executed in forward inference mode and the
error gradient information has been back-transmitted in the layers above/ahead. The backward-locking problem
is effectively the forward-locking problem but applied to the chain of operations that characterize backprop-based
learning in a DNN; no neuronal layer can be updated before all dependent neuronal layers have been executed
in both forward inference and backwards propagation mode. Resolving these three locking problems inherent to
backprop-centric learning would open the door to a plethora of biological and practical computational possibilities;
this includes the ability to make adjustments to portions of a neural system in parallel and asynchronously.
6

Preprint
The Problem of Constraint and Sensitivity. Through the use of automatic differentiation [264], it is possible
to train different types of computational neural structures (beyond linear chains) consisting of multiple types of
operations. Many modern-day DNNs consist of multiple, various kinds of layers, including those that leverage
convolution, as in convolutional networks [121, 66, 212], or multiple heads of self-attention, as in neural trans-
formers [439]. However, despite its flexibility, a backprop-centric form of learning does impose several constraints
and functional requirements on the architectures that can be built: 1) model must be fully differentiable, and 2)
backprop is limited to training networks that take the form of directed acyclic graphs. With respect to the first
constraint, a model being fully differentiable means that all of its constituent operations must also be differentiable
(including its elementwise activation functions); otherwise, it is not possible to carry out backprop transmission due
to the fact that the matrix chain rule of calculus entails a chained product of local first derivatives/Jacobians. This
makes it difficult to utilize discrete-valued functions and stochastic elements (such as Bernoulli sampling), making
it challenging to design systems that communicate using discrete/spike values (as in spiking neuronal systems), thus
further hindering our ability to construct more energy efficient neural systems [314, 466]. Biologically, this means
that the feedback pathways that characterize the underlying brain structures would require precise knowledge of
the derivatives of the nonlinear dynamics (at particular operating points) of the neurons that transmit information
in the corresponding feedforward computations of the DNN’s inference pathway. In terms of the second constraint,
if a cycle is present inside a DNN’s neural structure, an infinite loop is created in the forward pass which makes
learning impossible. To address this limitation, such as for temporal/sequential data, researchers have developed
a time-dependent variation of backprop which stores vectors on neural activity values across time [177] – this is
known as backpropagation-through-time (BPTT). However, this further increases the implausibility of this type of
error-driven learning since the neural activity values that were earlier stored for error computations now must be
further stored over time [162]. This limits biological plausibility and prevents emulating much of the message-
passing structure that characterizes processing in brain given that the structure of biological networks is extremely
complex, full of cycles, and heterarchically organized with small-world connections [16, 380]. These topologies
are likely highly optimized by evolution, where different topological attributes promote different types of commu-
nication mechanisms [415, 95, 48]. In addition, the requirement of an acyclic form makes it difficult to incorporate
mechanisms such as lateral competition and cross-layer interactions (which further hinder the use of mechanisms
useful for fighting off catastrophic forgetting).
Finally, another aspect of this problem is that DNN’s are highly sensitive to their initial conditions as well as the
choice of normalization. In particular, it is well-known in DNN optimization that a key ingredient in guaranteeing
convergence (and subsequent generalization) is the initialization scheme used to randomly instantiate the synaptic
weights. However, DNNs trained with backprop are particularly sensitive to the random initial values chosen for
its synapses [209], which ultimately hinders its overall final performance. A great deal of research has gone into
crafting effective initialization setups [134, 386, 156, 180] (including data-level/dependent ones [230, 386]); never-
theless, this issue still persists to this day. Furthermore, widely-used initialization schemes in modern-day DNNs,
e.g., Glorot initialization [134], have been shown to have their limitations, e.g., [218] and [190] demonstrated that
blindly using Glorot initialization can result in (long-term) suboptimal generalization performance. Furthermore, in
practice, modern DNNs depend heavily depend not only on initialization but also on the normalization schemes ap-
plied at the data and activation levels [230, 17, 186]; the choice of a good normalization scheme can aid in ensuring
faster convergence and possibly better generalization. Nevertheless, batch or activity-level normalization introduces
further issues – some schemes only work well with small batch sizes [17], others only work when operating across
certain input patterns or particular activation functions, and some do not work well when used jointly with other
normalization strategies and/or noise-injection processes. In addition, the dependence on normalization results in
leads to a training process that struggles when there are dependencies between samples within a mini-batch (since
necessary statistics are calculated at the batch-level) and further can result in model operational instability when the
distribution during test-time inference is different from or strongly drifts away from the training distribution.
The Problem of Short-Term Plasticity. This problem pertains to the nature of how inference is typically carried out
in DNNs. Concretely, DNNs do not model nor offer an account for short-term plasticity; in effect, neural activities
do not “exist” until data is clamped at the input layer(s) of the system and then forward propagation calculations
are performed (this also is partly the cause of the forward-locking problem described earlier). Furthermore, in
backprop, the error information that is propagated backwards only effect changes in synaptic values and do not
result in any modification of the neural activities themselves [43, 247]. In contrast, in the brain, inherent feedback
connectivity is observed to operate quite differently, e.g., feedback synapses in the cortex influence feedforward
neural dynamics in a top-down modulating fashion [311]. Although the problem of short-term plasticity centers
around the way that inference is carried out in DNNs, since inference is a critical aspect of functionality that
(directly or indirectly) affects learning, it is an important aspect of neural computation that many credit assignment
approaches try to address, as we will see throughout this survey. This problem, which we have made explicit, also
relates to another important aspect that brain-inspired machine intelligence has begun to consider – biologically-
7

Preprint
plausible architectural design, which involves considering that connectivity patterns within a neural model should
be consistent/in accordance with basic constraints of brain connectivity (in the neocortex) [452].
1.4
On the Target Signals that Drive Synaptic Plasticity
As foregrounded at the start of this article, any computational process designed for conducting credit assign-
ment within a neural system context must answer the question: where do the (target) signals for inducing learn-
ing/adaptation, for each processing unit within a neuronal network, come from?5 There are many possible answers
to this question, and in this survey, we will focus on six of the more prominent answers to it, each of which motivates
a different family of biologically-inspired algorithms and frameworks.
As shown in Figure 1, we start by breaking down the signal type into two general categories, implicit and explicit.
In the case of learning and adaptation that makes use of implicit signals, which is also one of the six computational
families, the information used to adjust synaptic parameters is completely local in both time and space; this is
what centrally characterizes pure Hebbian-type rules [157] or what is also known as “correlation learning” [310].
Specifically, synaptic change depends only on information that is immediately usable at pre- and post-synaptic
sites/locations, i.e., the incoming and outgoing neurons that a synaptic cable connects.6 Consequentially, there is
no explicit signal or external information, such as that based on error (such as a signal created by comparing a
prediction to a reference value), that is transmitted to a particular neuron.
Alternatives to credit assignment based on implicit signals are naturally those based explicit values; this encom-
passes many sets of procedures that rely on information beyond what is locally available to any single synapse.
Any biologically-inspired process under this general partition typically creates signal values based on some sort of
process (which itself could be local), such as those based on the message passing of error/mismatch measurements.
Within the cluster of explicit signal algorithms, we partition credit assignment schemes based on whether or not
they make use of a local or global signal to induce synaptic weight change. Under the global category (which is a
leaf in the taxonomic tree and thus a distinct family), we have many possible frameworks, ranging from feedback
alignment to neuromodulation. Within the local category, procedures can be further sorted based on “how local”
they really are and, in this survey, we propose two sub-partitions – ‘non-synergistic’ and ‘synergistic’. A non-
synergistic local scheme operates with information that is local in the sense that it is spatially and temporally “near”
the neurons it will affect but, unlike an implicit signal scheme, this is information or target values beyond what a
synapse and the two neurons it connects could possibly provide. Mechanisms capable of creating this information
generally involve additional neurons and synaptic parameters, typically forming a local predictor of some kind, e.g.,
a classifier that has immediate access to a label context, or specialized local feedback synapses that enable a pair
of layers to form an encoder/decoder sub-system. On the other hand, a synergistic local learning process is one
where the signaling information or target values are produced using some indirect knowledge of the neural system
state; this kind of information is usually acquired through a message passing scheme or iterative settling process,
typically (though not always) involving additional neural circuitry to construct feedback loops. Within this classi-
fication, there are three primary sub-paradigms (or taxonomic leaves): discrepancy-based [113, 328], energy-based
[231], and forward-only frameworks [205].
2
Families of Neuro-mimetic Credit Assignment
2.1
Implicit Signal Algorithms
The first family of procedures that we review offers a simple answer to this survey’s central question: there is no
target or external signal. Rather, the target is implicit and not produced by external processes, unlike the other
algorithms that we will investigate. These schemes utilize information that is readily available to each individual
synaptic connection of the architecture. This could mean, in the case of Hebbian learning, that only the activities of
the pre-synaptic and post-synaptic neuronal elements are needed, and adaptation is effectively a form of correlation-
based learning.
The schemes, sometimes referred to as ‘update rules’ in the literature, that fall under this family only make use of
the information that is produced by the inference process of the neural model, e.g., a network’s feedforward pass.
This is particularly attractive not only from a practical perspective, given that computation only involves neural
5Note that target signals could come in many forms, ranging from separate pools of neuronal activities produce mismatch
values all the way to control signals that trigger or modulate changes in synaptic strengths.
6In neurobiology, a synapse is a specialized junction that carries/mediates the information between neurons; it is many of
these “cables” that facilitate communication between an individual neuron and its small subset of pre-synaptic transmitters and
post-synaptic receivers.
8

Preprint
activity vectors immediately available from the inference process of the system (with no further signals provided
by mechanisms such as feedback), but also from a neurophysiological standpoint, since synaptic changes can be
calculated with exclusively local information. Furthermore, activation derivatives are typically not required in any
of the methods within this family and, furthermore, training and test-time computations are identical.
Hebbian Learning. Hebbian-based adjustment is a classical, biologically-plausible synaptic modification rule. It is
based on the idea that synaptic plasticity is the result of a pre-synaptic neuronal cell’s persistent and repeated stim-
ulation of a post-synaptic cell [157, 220]7, popularly summarized by the phrase: “neurons wire together if they fire
together” [254]. Each and every occurrence of input activity patterns (from pre-synaptic specializations) strengthen
the ability of the related synaptic parameters to recall or reproduce the pattern later on. In effect, Hebbian learning
allows neural structures to encode memories in their synaptic connectivity. What is quite attractive about Hebbian-
like rules is that they operate completely locally – this resolves one of the central problems inherent to backprop,
the problem of locality and locking. In essence, synaptic adjustments only require information readily available and
in close proximity to the (location of the) synaptic weight of interest. In other words, Hebbian rules are generally
cell-by-cell rules, where information regarding some aspect of pre-synaptic activation and post-synaptic activation
– such as magnitudes of neuronal activation patterns – as well as possibly a dependency on current synaptic effi-
cacies, are used to calculate and update the matrix of weights connecting layer ℓ−1 to ℓ. Formally, this type of
adjustment can be expressed in scalar (single synapse) form, i.e., τ ∂Wij
∂t
= Wij(zℓ
izℓ−1
j
), or in matrix-vector form,
i.e., τ ∂Wℓ
∂t
= ∆Wℓ= Wℓ⊙

zℓ·(zℓ−1)T
(τ is a time constant that would be refactored into what is known as the
…
zi
zj
zj-1
zk
Factor 2
Factor 1
Figure 5: Two-factor Hebbian plasticity: here, only pre- (neuron
j) and post-synaptic (neuron i) measurements of a specific synaptic
connection (wij) are used to produce its adjustment ∆wij.
“learning rate”). This update equation has also be
referred to as a two-factor Hebbian rule; one factor is
the pre-synaptic activity while the other is the post-
synaptic activity (the use of the synapse with the rule
is generally not counted as a separate factor).
Although this scheme is simple and efficient, requir-
ing only the activity values produces by the neu-
rons in the pairing of layers ℓ−1 and ℓ, as well as
the current value of the synapses connecting them,
one critical drawback of naïvely-interpreted Heb-
bian update rules is possibility of exploding weight
magnitudes; an implicit positive feedback loop is
created by repeated application of the rule, result-
ing in ever-increasing weight magnitudes. This can
be taken care of with the introduction of plasticity
constraints [280], e.g., some form of weight decay
[469], normalization [442, 370], or both [307, 183]
(as in Oja’s rule8). For example, one simple way to introduce a depressing force on the synaptic magnitudes is via:
∆Wℓ= Wℓ⊙

zℓ· (zℓ−1)T
−γWℓ, where γ is a non-negative decay factor. Beyond decay, synaptic strengths
can further be bounded to a magnitude range, yielding the full Hebbian plasticity update below:
τ ∂Wℓ
∂t
=

wmax −Wℓ
⊙

zℓ· (zℓ−1)T
−γWℓ
(6)
where wmax is a scalar bound on the maximal value any synaptic strength in Wℓcan take on and Wℓis assumed to
only take on non-negative values. The above differential equation results in softly-constrained, multi-term Hebbian
plasticity update rule.
Another way to correct the explosive nature of Hebbian-like rules is to incorporate a local mechanism for weight
depression, thus leading to the incorporation of anti-Hebbian counter-pressures [102] or the use of gated Hebbian
rules [129]. For example, one could use the post-synaptic gated update rule, or ∆Wℓ= (zℓ−gℓ) · (zℓ−1)T, where
gℓis a set of thresholds, one-per-neuron in layer ℓ, that allow the post-synaptic activity to change the sign/direction
of the weight change but still emphasizes the importance of pre-synaptic activity for any change. Synaptic gating
serves as a building block for more sophisticated Hebbian-like adjustment rules, such as the Bienenstock-Cooper-
Monroe (BCM) update rule [45, 70]; ∆Wℓ= αΦ(zℓ−gℓ) · (zℓ−1)T where Φ is a nonlinearity applied to the
7Specifically, it was observed in [157] that: “When an axon of cell A is near enough to excite a cell B and repeatedly or
persistently takes part in firing it...A’s efficiency, as one of the cells firing B, is increased”.
8Oja’s rule [307] is a generalized Hebbian plasticity rule which introduces a mechanism for ensuring the norms of each vector
synaptic weights are approximately constant after adjustment.
9

Preprint
post-synaptic gating. Alternatively, there is the pre-synaptically gated rule: ∆Wℓ= zℓ· (zℓ−1 −gℓ−1)T [129].
Other formulated schemes introduce stabilizing mechanisms, e.g., calculated values based on statistics of tracked
neural dynamics, into the original Hebbian framework. An example of such a rule is the Hebbian covariance rule
[398]:
∆Wℓ= (zℓ−⟨zℓ⟩) · (zℓ−1 −⟨zℓ−1⟩)T
(7)
where ⟨zℓ⟩indicates a short-term running average of each neural firing rate within zℓ. However, these types of rules
generally mean incorporating statistics that partially violate the property of locality – Equation 7 breaches locality
in time – in order to adjust the synaptic weight values. Further note that generalizations of Hebbian learning have
been explored in the context of complex vision architectures, such as those that employ convolution [223].
Although Hebbian learning has historically been presented as an unsupervised process for synaptic plasticity, vari-
ants introduce means of conducting supervised learning in the presence of a desired goal y, e.g., a label. Instances
of supervised Hebbian learning, some of which are reminiscent of the principle of “teacher forcing” [435], include
the perceptron learning rule [367], the delta rule [153, 10, 441], and the Widrow-Hoff rule [455, 456]. The last one
takes the form ∆Wℓ= (zℓ−y)·(zℓ−1)T; notice that this rule happens to correspond to the first derivative of a mean
squared error cost – it is sometimes referred to as the least mean squared errors rule. In addition, notions of Heb-
bian plasticity have appear in other credit assignment paradigms; frameworks such as contrastive Hebbian learning
[19] or predictive coding [354, 380], of which we will review later, also entail final synaptic adjustments that are
made using information such as pre- and post-activity signals (although such schemes require external mechanisms
such as message passing). It is interesting to note that the delta rule [10], which extends the Widrow-Hoff rule
by incorporating the derivative of the activation function, can be viewed as a building block that gave rise to the
full generalization that is known today as backprop [372]. Note that modern research efforts have crafted schemes
based on supervised Hebbian plasticity to train deeper, multi-layer neural models [146, 6].
Although raw Hebbian plasticity has its drawbacks and, as of today, is rarely used in isolation to directly to train
complex neural systems (except in some more recent cases [192]), it still plays an invaluable role in emulating other
aspects of neurobiological organization and functionality. For instance, a powerful idea behind Hebbian-based and
neuro-correlational rules is that they are engage a form of “model-learning” [311]. Rather, such rules facilitate the
general extraction of co-occurrence statistical structure from a neural system’s environment, making it particularly
attractive to utilize for unsupervised dimensionality reduction. For instance, prior work has established a strong
connection between Hebbian rule variants and principle components analysis (PCA) [307]. Beyond this, Hebbian
learning has crucially been shown to be a powerful means of constructing models of memory, such as those based
on Minerva-2 [174] and sparse distributed memory [194].
Another important aspect of Hebbian plasticity is its generalization to the spiking temporal domain, i.e., spike-
timing-dependent plasticity (STDP) [242, 2, 44], which facilitates the adaptation of networks based on communi-
cation of discrete (action potential) values, such as those composed of spiking neuronal cells such as spiking neural
networks (SNNs) [259]. Hebbian adjustment through STDP specifically entails using the relative timing of the
action potentials (or spike emissions) of pre- and post-synaptically located neurons; a sliding temporal window is
used to determine if a pre-synaptic spike arrives before a post-synaptic one which results in a positive increase in
a synapse’s efficacy (long-term potentiation) while, if this timing is reversed, i.e., post- occurs before pre-synaptic
spike, then a decrease is applied (long-term depression). STDP-centric Hebbian plasticity notably facilitates the
capture of temporal correlations inherent to sensory data streams9 and serves to this day as a useful biophysical
mathematical model of neuronal organization and synaptic adaptation [473, 294, 203, 428, 13]. However, STDP
and general Hebbian-based plasticity largely work as unsupervised forms of adaptation and are not directly useful
for modeling behavioral learning (though there are supervised variants, as mentioned before). In general, to make
Hebbian adjustments work for action-oriented functionality and behavioral conditioning experimental settings, it
is typically extended to include a third additional factor; a three-factor Hebbian adjustment [336] combines the
original two factors, i.e., pre- and post-synaptic activity values, with a third one typically labeled as the modulator,
e.g., a dompanie neuromodulatory signal (such as an encoding of a reward function that conveys the degree of suc-
cess of actions taken).10 However, adding an additional factor to Hebbian plasticity technically results in a credit
assignment mechanism that falls under a different category in our taxonomy, i.e., global explicit target algorithms,
of which we will review/cover later (see Section 2.2).
Competitive Hebbian Learning. One of the simplest rules entails adding intra-layer connections between neurons,
particularly inhibitory connections. The idea is that neurons in a given layer will compete with each other for the
9The integral over an STDP window, assuming slowly changing neuronal firing rates, can be shown to recover a Hebbian
correlational update similar to the ones described earlier in this section.
10Three-factor Hebbian schemes furthermore integrate what is called an eligibility trace in order to handle the temporal
correlations inherent to sequences of actions taken over time in the presence of delayed reward signals.
10

Preprint
Algorithm 1 Procedure for selecting the (maximal) K best matching units.
Input: Activation vector zℓ, K BMU desired, winner_type string flag
function FINDBMU(zℓ, K)
▷Run K-winners selection process
w = ∅, zmax = max(zℓ)
▷Initialize statistics
for k = 1 to K do
q = arg maxi zℓ, zℓ[q, 1] = zmax
▷winner_type is unit w/ maximum value
w ←w ∪{q}
▷Record index q of kth winning neuron
Return w
▷Output set of K winning neuronal units
chance to represent particular input patterns; this is the essence of what is historically known as ‘competitive
learning’ [374, 83, 451, 267, 80, 443, 133]. In winner-take-all style competition [206], the unit with the highest
level of activation will be declared the winner, warranting an update to its incoming synaptic weights while the
rest of the losing neurons receive no adjustment to their incoming weights. Note that weights and inputs must be
normalized (unless they contain bipolar values, i.e., values in the set {−1, 0, 1}). For a winner-take-all (WTA) block
of neurons – which involves an entire layer or be limited to a specific sub-group of neurons, i.e., local WTA units
[412]) – any processing element zℓ
j in layer ℓis updated according to a Hebbian update that operates in tandem with
a hard interaction function as follows:
∆W ℓ
ij =
(
zℓ−1
j
−W ℓ
ij
if
i = arg max{1,2,..,Jℓ}

zℓ
0
otherwise,
(8)
where we note that a vector update is produced for the ith row of Wℓas a result of the local update. Notably, it
was the general use of WTA-driven synaptic change in a multi-layer neural model that served as a key part of the
classical Neocognitron [121, 120], the historical predecessor to the modern-day convolutional network. The above
is a variation of the ‘instar update rule’ [139, 206], which, simply leads to modifying the synapses connecting to
the neuron that took on the largest post-activation value for a given input zℓ−1; the other synaptic connections are
left unchanged. Note that this scheme may be extended to leverage the top K highest activity values instead (a K-
winners-take-all scheme). To extend the instar algorithm to supervised learning settings, the ‘outstar update rule’
was proposed [139], where, instead, outgoing weights are updated so that neuronal output matches a desired target
pattern, such a label vector y. The instar and outstar forms of competitive plasticity ultimately gave rise to what is
known as adaptive resonance theory [141, 143] – and its plethora of variants, such as fuzzy ART [55], ARTMAP
[54], and ART-C 2A [155] – as well as the counter-propagation generalization for credit assignment in multi-
layer models [158]. Other ways to induce competitive neural dynamics include anti-Hebbian learning [102, 312]
and explicit lateral/cross-layer synaptic connections (with influences on more modern deep learning architectures
[213]) or by enforcing a sparse kurtotic prior distribution (or penalty), e.g., a Laplacian or Cauchy prior, within an
iterative inference process [308, 354] (the latter of which is used in sparse coding, which is technically part of a
different class of credit assignment schemes that will be covered in Section 2.4.1).
Competition Scheme
(WTA, K-WTA, etc.)
zq
zj
z j -
(
)
=
Winner q triggers 
plasticity update
Figure 6: Competitive learning focuses on layer dynamics where
neurons compete for the right to compute, i.e., they laterally inhibit
or excite one another to form sparse distributed representations.
In general, the premise of competitive learning could
be stated to center around the idea of ‘neuronal tem-
plate matching’ or clustering [103, 250] – a given
pool of neuronal units fight for the right to activate
leading to different (sets of) units activating for dif-
ferent clusters/partitions of patterns. As more and
more sensory input patterns are presented to this
pool of competing units, each neuron within the pool
will converge to the center of the emergent cluster
is has come to model. In other words, each neural
unit will activate more strongly for sensory patterns
strongly correlated with its cluster “template” and
more weakly for those related to other cluster tem-
plates. As discussed in [312], systems that operate
under competitive neural dynamics are variations of
the above story; this includes classical supervised systems such as vector quantization [136] to explicit topology
clustering systems such as the venerable self-organizing map (SOM) [207, 256, 208, 133] and self-organizing (in-
cremental) neural networks [255, 122]. Some schemes generalize the competitive dynamics to localized forms of
competition or facilitating the emergence of multiple winning neurons (i.e., K > 1) as is the case for competitive
Hebbian learning [451, 267] and variations of compression systems based on neural gas [268, 117].
11

Preprint
To fully characterize competitive neural systems, we borrow several perspectives [373, 312] to organize them under
a few core foundational principles. In essence, a competitive model’s computation can be broken down into:
1. A Response Specificity Measurement: the neuronal units start as highly similar (except for a randomized
initial condition, which makes each unit respond slightly differently to a set of inputs). There is also a limit
to the “strength” of each unit and activation in this context is produced by a set of measurements either as
an array of distance values:
zℓ[i, 1] =

zℓ−1 −(Wℓ)T[:, i]


pfor i = 1, 2, ..., Jℓ,
(9)
or as a set of (parallel) dot products:
zℓ[i, 1] = Wℓ[i, :] · zℓ−1 for i = 1, 2, ..., Jℓ,
(10)
where the ith element of the activation vector zℓis either a dot product (or, as in Equation 9, a subtraction
fed into a p-norm function, e.g., p = 2 yields the Euclidean distance) of the current pattern vector and the
ith column of the transposed synaptic (memory) matrix;
2. A Competition Mechanism: units compete for the right to respond to a particular subset of inputs – this
requires mechanisms for selecting what is known as the ‘best matching unit’ (BMU), or the ‘prototype’
(template) that satisfies a particular constraint.11 A typical function for choosing a winning neuron(s)
entails using the maximum, i.e., the max() and arg max(), out of a set of Jℓactivation values and is
typically dependent on how the neural post-activities are computed in the first place. Once the values
for zℓhave been computed for neuronal layer ℓ, the selection function is applied, as formally depicted
in Algorithm 1 (note that this algorithm is depicted as picking K maximal neurons BMUs, stored in the
list/set w).
3. A Synaptic Adjustment Rule: Given the results of the competition function or dynamics, synaptic efficacies
are adjusted, typically in the form of a Hebbian or anti-Hebbian rule [102], e.g., for a subtractive distance
form of specificity, a synaptic update would be ∆Wℓ= zℓ−1 −(Wℓ)T[:, q] where q is a BMU index.
The resultant update matrix ∆Wℓis then used to alter the values within Wℓas in: Wℓ←Wℓ+ α∆Wℓ
where 0 < α < 1 controls the magnitude of the update applied to parameters Wℓ.
The above three components are important in the design of a minimal model of neural competitive learning. Com-
petition dynamics across neuronal units notably results in sparse activity patterns, which has been demonstrated to
be an invaluable biological property of neurons useful in preventing forgetting [272, 107] in systems such as sparse
distributed memory [194]. Many neural models driven by competitive learning embody the core principles above
to varying implementational degrees, including incremental WTA models (e.g., vector quantization-based systems)
[136], self-organizing maps [206, 207], competitive neural Gaussian mixture models [344, 274].
2.2
Global Explicit Signal Algorithms
The next credit assignment family we examine embodies a different answer to our organizing question – there is
an explicit target that drives the learning process. These schemes effectively take a completely global approach to
playing the credit assignment game, which is what backprop does; take a signal, such as an error measurement,
originating at the output units and transmit (a transformation of) this signal back to each neuron inside the network.
However, although these schemes do rely on a single (global) feedback pathway to ferry along teaching/adjustment
signals, the design and nature of this pathway generally varies in form and nature in contrast to backprop. Within this
category, more biologically-plausible variations of backprop have been proposed, e.g., random feedback alignment
[246], which notably offer means of resolving the problem weight transport, as well as methods based on either
implicitly/explicitly modeling the act of (chemical) neuromodulation, e.g., three-factor Hebbian plasticity [221].
Feedback Alignment. Feedback alignment [246, 227, 108] – also referred to as random feedback alignment (RFA)
– and its variants [301, 21, 243] have shown that random feedback weights can also, surprisingly, still deliver useful
teaching signals. In other words, feedback alignment algorithms resolve the weight-transport problem described
earlier, showing that coherent learning is possible with asymmetric forward and backward pathways. Rather, the
back-projection pathways for carrying backwards derivative information need not be transpositions of the connec-
tion weights used to carry out forward propagation; the process of credit assignment can instead be viewed as
focused on the (partial) alignment of feedforward weights with (complementary) feedback weights. When the feed-
back and forward weights undergo a form of synaptic normalization and forced to approximate sign concordance
[20, 21], this form of learning can result, across various tasks, in networks with performance nearly as strong as
those learned via backprop [243].
11Note that multiple BMUs may be selected/win the competition, as in growing neural gas [117].
12

Preprint
𝝓 (h  )
3
3
𝝓  (h  )
3
3
=
z 3
h3
W  3 ΔW  3
/∂h
∂L
(W  ) 
3 T
=
e 3
_ y)
(z 3
L = ||e  ||
3
2
2
𝝓 (h  )
2
2
𝝓  (h  )
2
2
=
z 2
h2
W  2 ΔW  2
/∂h
∂L
𝝓 (h  )
1
1
𝝓  (h  )
1
1
=
z 1
h1
W  1 ΔW  1
/∂h
∂L
x
=
z 0
3
2
1
e 3
y
∂
∂
∂
(W  ) 
2 T
𝝓 (h  )
3
3
𝝓  (h  )
3
3
=
z 3
h3
W  3 ΔW  3
/∂h
∂L
B  3
=
e 3
_ y)
(z 3
L = ||e  ||
3
2
2
𝝓 (h  )
2
2
𝝓  (h  )
2
2
=
z 2
h2
W  2 ΔW  2
𝝓 (h  )
1
1
𝝓  (h  )
1
1
=
z 1
h1
B  2
W  1 ΔW  1
x
=
z 0
3
Δh2
e 3
Δh1
y
∂
∂
∂
𝝓 (h  )
3
3
𝝓  (h  )
3
3
=
z 3
h3
W  3 ΔW  3
B  3
=
e 3
_ y)
(z 3
L = ||e  ||
3
2
2
𝝓 (h  )
2
2
𝝓  (h  )
2
2
=
z 2
h2
W  2 ΔW  2
𝝓 (h  )
1
1
𝝓  (h  )
1
1
=
z 1
h1
B  2
W  1 ΔW  1
x
=
z 0
/∂h
∂L
3
Δh2
e 3
Δh1
y
∂
∂
∂
Figure 7: Credit Assignment through Principles of Feedback Alignment. Credit assignment schemes under backprop (Left),
random feedback alignment (RFA; Middle), and direct feedback alignment (DFA; Right), applied to a three-layer MLP. Blue
lines indicate the feedback pathway taken to produce teaching signals, or ∆hℓfor each layer (which drives synaptic adjustment);
solid lines indicate synaptic pathways while dash-dotted ones indicate “virtual” ones that depend on feedforward connections.
In backprop, the (global) feedback pathway used to backwards propagate adjustment (derivative) information is made up of
the transposes of the feedforward parameters. In contrast, RFA and DFA (on the same model) use separate, explicit parameter
matrices to build either a feedback pathway similar to that of backprop (as in RFA) or a shorter, more direct one (as in DFA).
In essence, RFA is effectively backprop but with the key removal of its symmetric weight constraint for transmitting
error signals back along the network; the updates produced by RFA look similar to those in Equation 3 (backprop)
with the crucial difference that a teaching signal δℓis produced by a synaptic pathway. Concretely, this means that
the synaptic adjustment under RFA for any hidden layer is carried out as follows:
∆Wℓ=

Bℓ·
∂L
∂hℓ+1

⊙∂ϕℓ(hℓ)

· (zℓ−1)T =

bδℓ⊙∂ϕℓ(hℓ)

· (zℓ−1)T ,
(11)
where the RFA teaching signal bδℓis produced via a product of a fixed (i.e., it is never adjusted), randomly initialized
feedback matrix Bℓ∈RJℓ−1×Jℓ(the same shape as (Wℓ)T. In the case of the direct feedback alignment (DFA)
algorithm [301], the update for any hidden layer ℓsynaptic matrix Wℓis instead:
∆Wℓ=

Bℓ· ∂L
∂hL

⊙∂ϕℓ(hℓ)

· (zℓ−1)T.
(12)
Notice that, in DFA, while fixed random feedback synapses are also employed, they take on a very different form
than those in RFA: feedback synapses are directly wired to skip from the output layer to each layer directly, rather
than traversing backwards across each layer of activities as in RFA. In either RFA or DFA, during the learning step,
the forward weights are adapted to bring the network to a regime where the random backward weights are able to
carry information that roughly approximates the gradient produced by backprop. Notably, this form of “random
backpropagation” [21, 356] has also been used to develop an event-driven variation of the learning rule suitable for
spiking neural networks [298, 383]. In addition, empirical results have been obtained to show that schemes based
direct feedback alignment (DFA) can scale to large-scale datasets [290, 226]; however, it has also been observed
that such schemes fail to learn efficiently [24, 290, 356], particularly in the case of convolutional networks, which
stems from the inherent challenge of aligning the (very sparse) Toeplitz matrix formulation of convolution towards
the randomized feedback parameters of alignment frameworks.
An interesting variation of RFA/DFA was furthermore proposed, labeled as indirect feedback alignment (IFA),
which also employed skip-layer fixed feedback pathways but in tandem with the feed-forward inference machinery:
∆Wℓ=

bδℓ⊙∂ϕℓ(hℓ)

· (zℓ−1)T, where bδℓ=
(
B ·
∂L
∂hL
if ℓ= 1
Wℓ·

bδℓ−1 ⊙∂ϕℓ−1(hℓ−1)

otherwise. .
(13)
We highlight IFA given that it: 1) introduces a short feedback pathway using only a single matrix of synapses, and
2) it reuses the feedforward pathway (unmodified) to produce its teaching signals. The latter of these two useful
properties serves as a useful basis for some algorithms that fall under a more recent family of credit assignment that
we will later review – forward-only learning (see end of Section 2.4).
13

Preprint
R
zj
-1
zi
y
M = 1 / 0
zk
+1
M = 1 / 0
+1
R
y
zj
-1
zi
zk
+1
M
M
Figure 8: Neuromodulatory Approaches to Credit Assignment. Depicted are forms of neuromodulation for adjusting
synapses in a neural circuit (in a globally-synchronized, parallel fashion). R is a dopamine (reward) functional that encodes
behavioral goals and is used to produce a modulatory signal M that is either: (Left) a (binary) gating variable for accept-
ing/rejecting a synaptic adjustment to be made, or (Right) a value to scale the update yielded by local Hebbian plasticity.
The underlying premise of random backpropagation has spurned a chain of developments over the years [21, 71,
151, 65]. Some variants of feedback alignment change what is propagated along the fixed random pathways [109]
(such as using the label directly instead of the first derivative of the cost function) while others address the problem
of using fixed random feedback parameters; alignment approaches based on underlying ideas of the Kolen-Pollack
(KP) method [209] adjust the feedback connections using the (transpose of the) same update produced for the
feedforward ones in tandem with a synaptic decay (regularization) term. Schemes that introduce a form of this
complementary feedback adjustment, e.g., ‘weight mirrors’ (WM) [5], rest on the theoretical premise that, given
enough adjustments over time, the feedback synapses will approximately converge to the transpose of the forward
ones (which, consequentially, means a weight-mirrored RFA recovers backprop in the limit). Alignment schemes
such as weight mirrors have yielded promising results on large extensive benchmarks, all without requiring weight
transport and even when incorporated into convolutional frameworks. Very recent credit assignment schemes have
combined the notion of weight mirroring in the context of complementary (global) forward and backward pathways
[62, 436] (this scheme combines RFA with concepts of target propagation [30], reviewed later). Other variant
schemes take the sign of the forward activities instead, such as the ‘sign symmetry’ method [462], where the
feedback synapses that generate the requisite teaching signals share only the sign but not the magnitude of the
feedforward ones [243]. It is worth pointing out that sign symmetry-based approaches lack theoretical justification
and such schemes do not accurately propagate either the magnitude or sign of the error gradient. Nevertheless,
their performance has been empirically shown to be only slightly worse than backprop (even when using tensor
connections as in convolutional architectures [462]).
Neuromodulatory Approaches. Another set of approaches within this family are those that, instead of implement-
ing an explicit transmission pathway in terms of (separate) synapses to place credit/blame on individual neuronal
units, produce adjustments by broadcasting a ‘modulatory’ signal M that will drive (typically locally-generated)
synaptic adjustments. In the brain, neuromodulators, e.g., dopamine, norepinephrine, or acetylcholine, repre-
sent viable biological candidates for broadcasting success/failure (or uncertainty/surprisal) signals across neurons
[396, 15, 395]. As such, this sub-family as labeled ‘neuromodulatory’ (or dopamine-modulated) credit assignment,
largely from the fact that many approaches within it draw inspiration from the concept of ‘neurotransmittion’.12
How this globally-modulated form of adjustment occurs varies greatly across schemes. For example, a rather
simple way to view neuromodulation is through an implementation of a form of stochastic hill-climbing, where
an ensemble of noisy perturbations is directed by the (possibly task-specific) cost function(s) L; see Figure 8
(Left). In essence, a random process is leveraged to “confabulate” a set of possible directions that synaptic weights
could be nudged towards and, so long as these directions are related back to an objective (with which the current
state of the network can be directly correlated to), e.g., a dopamine/reward functional R [23] that could encode
a typically cost L and target values y (as shown in Figure 8), the neural system could be gradually moved to
more promising configurations. Credit assignment schemes that operate like this are, effectively, evolving synapses
by largely disregarding the nature by which information flows through a neural system (to produce its internal
distributed representations) and instead move along the cost function (e.g., error) space via random perturbations
of synaptic connections, checking the global cost function along the way [423, 58]. A simplified variation of such
an update, per synapse, would be: ∆wℓ
ij = Mϵℓ
ij, where M = {0, 1} is a binary variable produced by reward
12In neurobiology, a neurotransmitter is a chemical messenger that strives to carry/move a bio-chemical signal from one
neuron (such as a nerve cell) to another.
14

Preprint
functional (for instance, M = 1 if Rt −Rt−1 > 0 in the case of reward maximization). Note that variations of
stochastic hill-climbing that fall under this family do not require gradient information nor any functional aspect
of the system to be differentiable (including the cost function itself) – these approaches generally correlate the
current state/configuration of a network, under random fluctuation(s), to a desired objective, deciding if a particular
nudge/move that could be made results in performance improvement. Variations of this theme incorporate synaptic
feedback pathways, much akin to those studied in the previous section (e.g., RFA/DFA pathways), and have even
shown that noise-based (feedback) modulation cycles can approximate the gradients produced by backprop [225].
An improved format for conducting the above cost/reward-driven perturbation style of credit assignment is simu-
lated annealing (SA) [341, 201], which is a generalization of stochastic hill-climbing that decides on which random
moves to take based on a probability of acceptance, guided by a temperature/settling schedule and a comparison of
the current state of a cost functional with its previous value. Notably, SA has been used to train ANNs before [61],
including the well-known, classical Boltzmann machine [172, 1], despite its origins as a metaheuristic inspired by
the heating/cooling of materials to change their fundamental properties in metallurgy [341, 201]. Although it is
unlikely that longer-term plasticity in neuronal networks could be likened to purely random walk-like processes
(which renders random hill-climbing less plausible than other schemes, e.g., Hebbian ones), a cost-modulated form
of perturbation offers a massively parallel scheme for synaptic change – given that only random noise values,
the cost function, and a synapse are required at any given time, parameter adjustment is local while behaviorally
driven/guided by a task-centric signal, i.e., the cost functional. Neuro-evolutionary frameworks and strategies repre-
sent an ever-growing set of metaheuristic efforts (which greatly improve upon the premise of modulated stochastic
adjustments) [413, 184, 379] for searching for optimal synaptic values.13
Other approaches build on the premise of global modulation but do not resort to exclusively random perturbation(s);
these schemes utilize forms of reinforcement to correlate the activities of individual neurons with a global objective
in the form of reward function [270, 23, 459]; note that some variations combine noise with reward modulation [98].
The assumption is that neurons, which behave stochastically, receive a signal that describes/encapsulates the error at
the output, i.e., the “neuromodulator” in dopamine/reward-drive adaptation. When an improvement is observed in
an objective or reward functional, the connection weights are changed so as to make the produced activity associated
with the positive change more likely. The AuGMEnT framework, which produces adjustments labeled as ‘synaptic
tags’, represents one such neuromodulatory-centered credit assignment scheme [363, 365, 364, 349] and one that is
generally combined with other neurocognitively-plausible mechanisms such as models of working memory [214,
467]. Other relevant modulation approaches center around the error (or first derivative) associated with the output
cost function and craft mechanisms for broadcasting a value correlated to this error – as in error-vector broadcasting
[69] – while other schemes broadcast a context value (label/regression target) – as in the dendritic gating framework
[52, 401], e.g., Gaussian linear-gated neural networks. Notably, it is in the context of neuromodulation that three-
factor Hebbian plasticity emerges (Hebbian 3F) [348, 339, 221, 130], as visually depicted in Figure 8 (Right). For
instance, if we focus on three-factor credit assignment using dopamine/reward modulation [358, 251, 339], then we
can replace the perturbation factors described earlier, e.g., the ϵij as in Figure 8 (Left), with synaptic adjustments
generated by a two-factor Hebbian rule, such as one present in Section 2.1, to obtain: ∆wij = M(t)

Wij(zℓ
izℓ−1
j
)

or, more generally, ∆Wℓ= M

Wℓ⊙(zℓ· (zℓ−1)T)

. Note that the modulator M can be a function of time M(t)
as well as the dopamine/reward functional, having taken on different forms in the literature; an oft-chosen form is
the temporal difference error: M(t) = r(t) −E[r(t)], where E[r(t)] is an estimate of the expected reward, i.e.,
typically a dynamically calculated running average. This reward-modulated multi-factor setup is further modified
to incorporate what is known as an eligibility trace14 Eℓ(t); this yields a set of ordinary differential equations that
characterizes a generalized Hebbian plasticity scheme via the following related differential equations:
τe
∂Eℓ(t)
∂t
= −Eℓ(t) +

Wℓ⊙(zℓ· (zℓ−1)T
(14)
∆Wℓ∝τw
∂Wℓ(t)
∂t
= M(t)

Eℓ(t) ⊙

Wℓ⊙(zℓ· (zℓ−1)T
(15)
where τe and τw are the eligibility and synaptic plasticity time constants, respectively (both generally set to be on
the order of milliseconds). An eligibility trace, which has support in neuroscience [118], is required particularly in
13We remark that, given this paper’s focus is on single-agent systems, rather than multi-agent or population-based ones,
reviewing this branch of metaheuristic optimization further is beyond the particular scope of this survey. For relevant reviews on
this topic, we refer the reader to [100, 125].
14Mechanistically, such a trace offers a temporary record or memory of an event’s occurrence, e.g., taking a specific action,
and provides a marker or “tag” of this event as eligible for inducing a form of adaptation (or learning). When a modulator
triggers synaptic adjustment, only events/states (and associated actions) that are deemed eligible are assigned credit/blame in
association with the (global) error, i.e., eligibility tracing facilitates a basic form of temporal credit assignment.
15

Preprint
light of action-oriented/behavioral tasks; such as those that characterize reinforcement learning [421]. Given that
success/failure identification can come after a delayed period time once an action has been taken, neuronal networks
will, in some way, need to store a short-term memory of past actions taken and, neurobiologically, one suitable lo-
cation for such a memory is at the synapses themselves.15 Notably, three-factor Hebbian plasticity (sometimes
referred to as “neoHebbian plasticity”) has proven to be invaluable for adapting the parameters of spiking neu-
ronal systems [105], resulting in what is known as reward-modulated spike-timing-dependent plasticity (R-STDP)
[187, 240, 106]. Note that the process of R-STDP enjoys considerable experimental validation and support in
neurobiological studies [59, 336, 51, 130].
Another important approach that falls under this family takes its motivation from information theory – the Hilbert-
Schmidt independence criterion (HSIC) bottleneck algorithm [258]. HSIC credit assignment, which is based on
the Information Bottleneck (IB) principle [434], operates with the aim of seeking (internal/hidden) representations
that exhibit high mutuality with target value(s) and less mutuality with input patterns (or neural activities) presented
to each layer. In other words, this approach is not driven by a dependency on the information flow (propagated
layer-by-layer) through the neural model. Notably, some work has demonstrated that a kernelized version of HISC
can recover three-factor Hebbian rules [346].
While approaches within this family exhibit some very appealing properties from a biological standpoint, it remains
a challeng to scale them to complex, high-dimensional tasks [449, 176]. For instance, some of these approaches
(such as SA) exhibit slow convergence, requiring many iterations in order to find optimal solutions.
2.3
Non-Synergistic Local Explicit Signal Algorithms
As we have seen in the previous few sections, signals for driving changes in plasticity can either be implicit – there
is no particular target signal and statistics/correlational properties purely local to any given synapse are used – or
single explicit yet global – a signal, such as an output error or a dopamine functional, is directly broadcasted to any
given synapse. In contrast to these types of types of driving signals are what we refer to within our credit assignment
taxonomy as explicit local target signals. These type of signals are further broken down in this work to be either
non-synergistic or synergistic. For non-synergistic local signals, a credit assignment scheme would utilize (neural)
machinery (such as a local classifier or a inversion model/decoder), which is itself situated within the immediate
vicinity of synapses to be adapted. This local mechanism produces driving signals that operate with information
only available to a particular block/region of the network (or its “computational subgraph”), typically focusing on
a pair of inter-connected neuronal layers. In contrast, as discussed later in Section 2.4, synergistic local signals are
those that are iteratively produced by neural machinery that often takes the form of a settling process or message
passing schemes. It is important to note that, in a non-synergistic local credit assignment algorithm, the global link
between the updates across regions/layers of neurons is through the flow of information across the whole network
via inference (e.g., a feedforward pass in our MLP example).
Synthetic Local Updates (SLU). An interesting alternative to backprop is a hybrid algorithmic and architectural
approach referred to as error critic(s) [447], local gradient estimators [392, 393] and meta-learning (local) backprop
[202], or decoupled neural interfaces [188, 74, 29, 478], which, at least originally, was not methodology directly
motivated by biological considerations though later connections to neurobiology have been made [338]. Other
complementary schemes related to the goal of decoupling include the method of alternating direction method of
multipliers (ADMM) [426] and block-coordinate descent-centric training [475]. The primary motivation behind
this work, which we will broadly refer to as ‘synthetic local updates’, came from considering the three practical
computational flow problems that plague neural systems described in Section 1.3; forward locking, update locking,
and backwards locking. These three locking problems constrain the computation in a neural system context to be
purely linear, hindering how much benefit one can take from massively parallel computing systems.
In terms of our survey’s MLP architectural running example, the basic idea is to augment each layer with an
additional parametric model, one that learns to predict weight updates, or gradients in the context of differentiable
systems. Assuming the simplest possible ‘update synthesizer’, a linear predictor, we would only need to introduce
one additional synaptic weight matrix Kℓ(similar in spirit to the random local classifier approach described above).
15Biologically, it is postulated that co-activation of pre- and post-synaptic neuronal cells provides/sets a “flag” (at the connect-
ing synapse) that is used in a later weight adjustment in the presence of a trigger; such a trigger could be reward/punishment,
novelty, or surprise, which could be implemented by either particular special neuronal signaling events or the phasic activity of
neuromodulators. If dopamine receptors are blocked, then synaptic change (long-term potentiation induced via STDP) occurs.
16

Preprint
z
W
ΔW
z
- 1
 
K
Update 
Synthesizer
z
W
y
y
—
K
B
ΔW
z
- 1
Figure 9: Non-synergistic Forms of Credit Assignment. Orange diamonds represent error/mismatch. (Left) In a synthetic
local update scheme, an extra set of neurons synthesizes an update ∆zℓto be used as part of a plasticity update ∆Wℓ. The
synthesizing module is itself updated with a value of the real local (layer) gradient δℓ. (Right) In a local signaling process,
the neural sub-module attached to layer ℓ, aimed at predicting regression target y using fixed random parameters Kℓ, provides
a local error message that is transmitted back along fixed random synapses Bℓto provide a post-synaptic term that triggers a
plasticity adjustment ∆Wℓto synapses Wℓthat connect neuronal units in layers ℓ−1 (i.e., zℓ−1) to those in ℓlayers (i.e., zℓ).
This would mean that the computation of a synaptic adjustment within the MLP would be:
∆Wℓ=

∆zℓ⊙∂ϕℓ(hℓ)

· (zℓ−1)T
(16)
where ∆zℓ= ϕp(Kℓ· zℓ).
ϕp(◦) is the post-activation of the gradient/update predictor, which can be set to the identity function to recover the
setting of [188]. In the equation above, we see that during feedforward computation, layer by layer, changes in plas-
ticity may be readily computed along the way. Note that each ∆zℓis an approximation of the true update/gradient,
or rather, ∂L(y,zL)
∂zℓ
= δℓ≈∆zℓ. To train the weight update prediction models for any layer ℓ, we simply use the
proxy update from the layer above ℓ+1, and compare the layerwise predicted update with the one determined above
via backward transmission. Assuming a local cost, e.g., mean squared error, measuring the difference between the
top-down generated gradient and the predicted gradient, Ld(δℓ, ∆zℓ), the updates to the update synthesizer are:
δL = ∂L(y, zL)
∂zL
, ∆KL ∝∂Ld(δL, ∆zL)
∂KL
=

(∆zL −δL) ⊙∂ϕp(KL · zL)

· (zL−1)T
(17)
δℓ= (Wℓ+1)T · ∆zℓ+1, ∆Kℓ∝∂Ld(δℓ, ∆zℓ)
∂Kℓ
=

(∆zℓ−δℓ) ⊙∂ϕp(Kℓ· zℓ)

· (zℓ−1)T.
(18)
The above local generator approach, which resolves the update (and backward) locking problem, can be taken
even further and used to generate the activity values themselves, which completely decouples the entire MLP
network and resolves the forward locking problem (see [188] for details). The interpretation that we will take of
a local gradient/update synthesizer scheme is that each layerwise predictor serves as a form of (hetero-associative)
memory; in other words, we may instead view each Kℓas a memory matrix that aims to recall error gradient
patterns generated for a given target layer. These locally-integrated memory modules effectively decouple the
computations that occurs across the layers in a deep neural system – each gradient (and/or activation) predictor
iteratively and progressively learns to become estimators of the kinds of synaptic/ativation updates that result in
better latent representations of the input/output data pairing.
Local Signalers (Predictors). One scheme that has been shown to be effective for conducting credit assignment
in neural systems, based on non-synergistic signals, is through the integration of a set of layer-wise, often non-
learnable “signalers”, i.e., predictors or estimators for classification or regression, in conjunction with the idea of
random fixed feedback from feedback alignment. This characterizes the central idea proposed in [291]. Specifically,
for each (hidden) layer ℓof the model, e.g., our MLP, a neural classification module (or possibly a regression
module, depending on the type of supervised task being tackled) parameterized by fixed, random synaptic weights
Kℓthat project the activity values of layer zℓand are followed by a normalized nonlinearity so as to produce a set of
17

Preprint
class scores ¯yℓ. As a result, calculating the update to any layer’s set of connection weights Wℓ, except for the top-
level ones, amounts to calculating the partial derivative ∂L(y,¯yℓ)
∂Wℓ
. A further modification to computing this gradient
is that the transpose of the local classifier’s weight matrix (Kℓ)T is replaced with a fixed random matrix Bℓof the
same shape. Interestingly enough, it is not necessary to learn the parameters of this random classifier, which means
that, in reality, no extra parameters have been introduced to the model from the perspective of credit assignment;
it is further relatively simple to align each pair of classification Kℓand feedback Bℓweights at initialization. This
yields the following synaptic update for any layer ℓin our MLP (except for the topmost layer ℓ= L):
¯yℓ= ϕy(¯hℓ), and ¯hℓ= Kℓ· zℓ
(19)
∆Wℓ=

Bℓ· ∂Lℓ(y, ¯yℓ)
∂¯hℓ

⊙∂ϕℓ(hℓ)

·

zℓ−1T
(20)
where ϕy() is a normalized layer-wise local prediction activation function and Bℓ∈RJℓ×C and Kℓ∈RC×Jℓ.
For classification, a variation of Equation 20 entails setting ϕy(¯hℓ) = ¯hℓ(the identity function) and eschewing
∂ϕℓ(hℓ), resulting in an update rule that is derivative-free. ¯hℓcontains the pre-transformation score values for the
local classifier at ℓwhile ¯yℓis its local prediction of y (the class label or regression target). The key is that each
layer of neurons is coupled to a local prediction model (with typically fixed random parameters) with a local cost
function related to the overall network’s central task or possibly a related auxiliary task – the error associated with
each local predictor signals (triggers) and modulates the synaptic adjustment for layer ℓ.
It was observed in [291] that the classification performance of each layer of the neural model would improve across
parameter updates/learning iterations, even though the local classifiers themselves remained static. Furthermore, the
predictions of each layer’s random classifier can also be utilized at test-time inference. More importantly, the value
of this local learning framework is that the activities and error signals for each layer zℓare immediately available
upon the feedforward pass of the MLP, meaning that for any layer weights Wℓ, the update may be readily calculated
as soon as the local classifier at ℓhas made its prediction – only the activities {z0, z1, ...zℓ−1} are required/need to
exist for layer ℓ’s update. This has been argued to be more hardware-friendly [291] and more biologically realistic,
notably resolving the update-locking and backwards-locking problem presented in Section 1.3. Furthermore, this
approach to local learning is similar in spirit to the older plasticity process proposed in [327] (specifically the
Bottom-Up algorithm), which was a scheme that for pseudo-jointly learned a stack of dual-winged harmoniums and
usefully did not require any supervisory signals; however, this historical scheme relied on a local form of contrastive
divergence (discussed later) to train its per-layer parameters, which faces difficulties related to the mixing of the
Markov chain used for sampling. Beyond its use as a biologically-plausible learning scheme, it is worth pointing
out that even backprop-based approaches have been benefited from some use of (greedy) layer-wise local learning
[233, 371, 27] (though these schemes sometimes “mix” the signals produced by the local classifiers with those
produced by backprop’s global feedback pathway) – this is sometimes referred to as “early exiting” [429, 269] or
as introducing local classification “branches” as in the famous GoogleNet [422].
In general, the ideas underpinning local predictors have gradually become a more explored in the context of con-
ducting credit assignment in deep neural systems [394, 193, 28, 18]. Other complementary local predictor schemes
introduce different local loss functionals [248, 448, 392, 22, 144, 302, 137, 304, 430], each with different beneficial
properties, while other schemes craft layerwise learning approaches based on local information propagation [444].
Desirably, the local signalers scheme has been demonstrated to work in training spiking neural systems [257].
Recirculation. In contrast to the local signaler scheme presented above, recirculation takes on an unsupervised
format. Historically, recirculation [169, 309] was originally proposed as a simpler alternative to training auto-
associators (or autoencoders) with backprop. The core procedure works by essentially using two forward passes
through the encoder to create signals for learning. The intuition is that the second forward pass (through the
encoder) is used to inform the higher of two layers about the effect that it had on the lower-level layer. Training
an autoencoder block via recirculation then amounts to: 1) using the input zℓ−1 to the encoder as the target for the
decoder’s reconstruction ¯zℓ−1, and 2) using the encoder’s output (the initial encoded value from the first forward
pass) zℓas the target for the second forward pass through the encoder, or ¯zℓ. While the original algorithm of [169]
was crafted for a single hidden-layer autoencoder and contained a brief description at the end of the original paper
as to how this might apply to models with multiple hidden layers, we formulate and treat the core principles of
recirculation in the context of an arbitrarily deep neural system, much as a far later effort did [425]. Concretely,
the local statistics needed for computing synaptic updates, as produced via recirculant autoencoding machinery,
formulated for any layer of our running example MLP would be as follows:
¯zℓ−1 = βdzℓ−1 + (1 −βd)ϕℓ−1
d
(Vℓ· zℓ) and, ¯zℓ= ϕℓ(Wℓ· ¯zℓ−1)
(21)
where Vℓ∈RJℓ−1×Jℓcontains the synaptic weights associated with the local decoder for layer ℓ. In the above, fur-
ther observe that another important idea is introduced, through the use of the coefficient βd, from [169] – the notion
18

Preprint
z
W
z
- 1
V
W
1 - ꞵ
ꞵ
z
- 1
z
ΔW
ΔV
Interpolated 
Decoding
Re-Encoding
Figure 10:
The recirculation process used to locally modify the
synapses connecting neurons in ℓ−1 to ℓ: 1) an interpolated de-
coding step yields a reconstruction of zℓ−1 which is then fed into the
next step, 2) a re-encoding of the interpolated reconstruction of layer
ℓ−1’s activities is performed. The reconstruction and re-encoded
values are used, in tandem with the original activities of ℓ−1 and ℓ,
to produce updates for both Wℓand paired decoding synapses Vℓ.
Orange diamonds represent error/mismatch.
of “regression” or, in other words, interpolation
(so as to avoid confusion with the act of regres-
sion in statistical learning). Specifically, in [169],
it was argued that the coefficient βd ensured that
the original input values were not simply replaced
by the reconstructed values; this convex combina-
tion of the original input to the encoder and the
decoder’s reconstruction, with a value of βd set to
a value closer to one, would ensure that the new
input to the encoder (for the second pass) would
not be too different from the initial input value.
This interpolation further ensures that the upper
layer “measures” the effect that a small change
to the input would have. Once the statistics from
Equation 21 are produced, a synaptic adjustment
is triggered as follows:
∆Wℓ= −

zℓ−¯zℓ
·

zℓ−1T
(22)
∆Vℓ= −

zℓ−1 −¯zℓ−1
·

zℓT
(23)
where we notice that the above particular updates,
akin to delta rules, do not require the derivative of the activation functions of neither the encoder nor decoder output
units. Furthermore, the adjustment equations only use the information produced by recirculated flow of information
through the locally coupled encoding and decoding neural modules – the plasticity that occurs at any layer ℓwithin
the MLP is only concerned with perturbations related to the encoder and decoder pair associated with it. Although
recirculation requires a decoder to be paired with the encoder in order to work properly, the learning process is
self-aligning, and as a result, unmatched encoder/decoder weights would ultimately tend towards symmetry.
The notions underpinning recirculation have been generalized to construct larger-scale networks [425], even those
of residual form, as in [135]. Other schemes operate much like the local signalers scheme described earlier but in
an unsupervised fashion, incorporating principles similar in spirit to those underpinning recirculation, e.g., local
feature alignment [472]. The generalized recirculation procedure [309], or Gene-Rec, which was inspired by the
work of [169] (and originally utilized the moniker ‘recirculation’), operates a bit differently than the recirculation
scheme characterized above by Equations 21, 22, and 23 and utilizes a top-down feedback synapses in a relaxation
process more akin to contrastive Hebbian learning. This makes Gene-Rec more of a synergistic local explicit signal
algorithm and we will treat it as a scheme under the family of energy-based / phased-based schemes in Section 2.4.
2.4
Synergistic Local Explicit Algorithms
In contrast to non-synergistic local explicit signal schemes, where layers rely on exclusively local machinery to
produce signals that drive plasticity, synergistic local credit assignment introduces a degree of (indirect) coordina-
tion across the layers in a neural system context, representing a different branch within our taxonomy. In contrast
to backprop, which employs a global feedback pathway to transmit error gradients to internal neurons and adjust
their relevant synaptic connections, algorithms within this organizational partition leverage either pathways based
on functional inversion, error message transmission pathways, or cross-layer feedback synapses that induce a form
of message passing based on the combination of both top-down and bottom-up effects/influences of neural activi-
ties. As such, our taxonomic categorization decomposes synergistic local learning into three emergent sub-families
of synaptic adjustment: discrepancy reduction (through message passing), energy-based (through settling phases),
and forward-only (through inference alone) mechanics. The first comprises a set of procedures that, at minimum,
operate with under two general computational processes – a target generation process, e.g., using a hierarchical
transmission pathway of functional inversion targets, and a subsequent (semi-)local synaptic update process. Al-
ternatively, as we shall see, updates to neural activities and relevant synapses could be produced according to a
predictive generative process instead, similar synthetic local updating schemes. The second comprises an even
more vast set of algorithms, often starting from the formation of an energy functional that globally relates every
neuron to each other and then deriving a set of procedures (which takes into account complex interactions) for cre-
ating the target signals that will drive synaptic plasticity through two computation phases of relaxation that travel
down this energy function. The final and third family focuses on strictly re-using the information flow offered by
the forward inference scheme alone, generally posed in a self-supervised contrastive context.
19

Preprint
2.4.1
Discrepancy Reduction Approaches
This class of credit assignment, which share some similarities with the previously studied families, is primarily
characterized by a form of coordination across neuronal units and layers but not through a single global (synchro-
nizing) signal as in the schemes of Section 2.2. We name this particular family “discrepancy reduction” given that
the procedures studied here typically work to minimize local errors or difference signals based on target activity
vectors produced by a complementary neural process(es) that might span multiple layers, e.g., a message passing
synaptic structure or inversion pathway, in service of learning how to better internally represent the neural system’s
environment. Broadly, these credit assignment schemes typically instantiate computational mechanisms for at least
two key steps:
1. A “search” for latent representations that better explain the input/output, also known as target representa-
tions (this act we refer to as “local target creation”). This creates the need for local, multi-level objectives
that will guide current neural activity representations towards better ones and might take the form of synap-
tic pathways that propagate information across the entire system in some way.
2. Once (suitable) representations are found, updates to parameters are triggered so as to, as much as possi-
ble, reduce the mismatch between a model’s currently “formed” representations and the produced target
representations. The sum of the internal, local losses have been likened to the ‘total discrepancy’ in a
system, which can be thought of as a sort of approximate free energy functional [284, 113].
Many of these algorithms offer computations and synaptic adjustments that are effectively local at particular points
in time, and, in some cases, such as predictive coding, offer completely unlocked neural systems (no forward,
backwards, or updating locking) but at the price of entangling the inference machinery with the learning process.
However, many of these approaches, though far more biologically plausible than many of their predecessors, still
require training and test time computations to be a bit different; the notable exception is forward-only learning,
which uses the exact same information propagation computations in both its inference and plasticity processes.
Target Propagation. Target propagation [30, 234, 276, 92] (sometimes abbreviated to ‘targetprop’ or ‘TP’) builds
on a premise similar to that of recirculation – a learning signal could be created by computing targets through an
encoder/decoder pairing. With the general framework of target propagation, this notion is specifically extended to
leveraging a backwards pathway through the network that transmits target vector values rather than error gradients
(as in backprop); the global feedback pathway is replaced with a global (approximate) ‘inversion’ pathway. In
essence, targetprop revolves around the concept of the function inverse; if we had access to the inverse of the
network of forward propagations (e.g., an MLP), we could compute which input values at the lower levels of the
network might result in better values at the top that would “please” the global/output cost. This is much akin
to the self-alignment inherent to recirculation: the feedback weights of a decoding module should be trained to
(approximately) learn the inverse of the feedforward mapping represented by the encoder.
In contrast to recirculation, targetprop produces the target values for each layer within a neural network by propa-
gating reconstruction values through the approximate ‘inversion pathway’ of the network. Concretely, under target
prop [228], this means that for our MLP example, starting with bzL = y (or a random encoding of this task-level
target), a set of layer-wise target values is produced in the following manner:
bzℓ−1 = ϕℓ−1
d

Vℓ· bzℓ
for ℓ= L, L −1, ..., 2.
(24)
In the above equation, we see that inversion of the forward flow of information through the network is approximated
via a backwards propagation of target signal values through each layer’s complementary decoders. The triggered
synaptic update for any layer then becomes:
∆Wℓ= ∂||zℓ−bzℓ||2
2
∂zℓ
·

zℓ−1T
(25)
∆Vℓ= ∂||(zℓ−1 −ϵℓ) −¯zℓ−1||2
2
∂¯zℓ−1
·

¯zℓT
,
(26)
where ¯zℓ−1 = ϕℓ−1
d
(Vℓ· ϕℓ(Wℓ· (zℓ−1 + ϵℓ))).
Note that ϕℓ
d() is the decoder output activation function (much as in recirculation) while ϵℓ∼N(0, σℓ); note that
the zero-mean Gaussian noise can have different values and even be a function of local functionals, as in [323] (in
its noisy version of difference targetprop, i.e., DTP-σ).16 In difference target propagation (DTP; [234, 24]), the
16Note that noise injection, which can be omitted as in some instantiations of target prop [24], is integrated into inver-
sion/decoder loss (and update) so as to ensure that the encoder, i.e., RJℓ−1 7→RJℓ, and decoder, i.e., RJℓ7→RJℓ−1, become
approximate inverse of one another at the point of not only their respective input vectors (zℓ−1 for the encoder and zℓfor the
decoder) but also the neighborhoods around those points, i.e., a Gaussian ball with radius σℓ).
20

Preprint
z
W
z
- 1
z L
y
ΔW
 
V
ΔV
+ 1
+ 1
z
+ 1
W
+ 1
z
- 1
z
z
V
z
W
z
- 1
z L
y
ΔW
z
+ 1
W
+ 1
 
 
 
 
E
+1→ 
E L→L-1
E L→
z L -1
z
z
+ 1
z L-1
Figure 11: Credit Assignment via Discrepancy Reduction. Orange diamonds represent error/mismatch. (Left) A target
propagation scheme (with no noise injected) producing a target bzℓthat triggers a synaptic update for Wℓ(also shown is adjusting
for the corresponding inversion synapses Vℓ+1). (Right) A recursive local representation alignment scheme producing target bzℓ
so as to trigger a synaptic update for Wℓ.
inversion target signals are produced using an alternative to Equation 24:
bzℓ−1 = ϕℓ−1
d
(Vℓ· bzℓ) +

zℓ−ϕℓ−1
d
(Vℓ· zℓ)

for ℓ= L, L −1, ..., 2,
(27)
where we see that DTP includes a second term that serves as a correction factor that compensates for the imperfect-
ness of the decoding process (which can obstruct learning) of the original targetprop [234]; this term measures the
reconstruction error (between a local decoding from zℓto zℓ−1 and the original activity values of zℓ) which provides
a linear correction to account for the use of learned, imprecise inversion functions. In any variant of targetprop, the
process of credit assignment simply entails using the global inversion pathway – the chain of approximate inversions
to produce target values – and then updating each layer to move closer to its target value. Like recirculation, tar-
getprop also works with non-differentiable nonlinear activation functions, including those that are discrete-valued
(so long as it is possible to approximate its inverse). Under simple conditions, when all the layer objectives are
combined, targetprop can yield updates with the same sign as the updates obtained by backprop [228, 4] and, under
other theoretical conditions – assuming that underlying function to be approximated is Lipschitz continuous and
that the difference between backward and forward activities is less than some small value ϵ – the resultant learned
ANN can be shown to converge to an optimal point (and loosely approximate a predictive coding scheme [380]).
Various modern incarnations of targetprop exist, such as those that choose to keep the inversion/decoding synapses
random and fixed [403], as well as algorithms that embody its central principles [345] such as function inversion.
Targetprop has also been applied to training recurrent networks that process sequential data points [461, 262, 261,
368], e.g., natural language modeling, but still requires unrolling the computation graph along the length of the
sequence. Furthermore, some connections have been made between targetprop and spike-time-dependent plasticity
[14], although targetprop has not, at the time of this article’s writing, been formulated for spiking neural networks.
Empirically, however, it has been demonstrated that, at times, targetprop can result in an unstable learning process
[24, 323] (unless an intelligent noise scheme is used when the underlying network is deep) and, furthermore, its
performance is not quite comparable to that of backprop’s. A theoretical study attempted to provide an explanation
of this gap by establishing an equivalency between targetprop and Gaussian-Newton optimization [277], and further
proposed an algorithmic reformulation that can be viewed as a hybrid between Gauss-Newton optimization and
gradient descent. The biological plausibility and generalization performance of this reformulation was improved
through the introduction of a prior that nudged the Jacobian of the feedback/inversion module towards that of the
corresponding feedforward parameters [92].
Local Representation Alignment. Credit assignment within the framework of local representation alignment
(LRA) [323] can viewed as a blending of principles underlying targetprop, recirculation, and predictive processing
theory [354, 68, 380]. However, as opposed to formulating each layer of neurons as a pairing of an encoder and
decoder in the context of functional inversion, LRA crafts a form of synergistic learning that seeks to minimize
a (pseudo-)free energy functional [328, 323] – effectively a form of what is known in predictive coding as the
21

Preprint
variational free energy [113] – which facilitates the matching or alignment of a neural model’s current means of
representing input-output pairings, e.g., a sensory input x and its associated context vector y, to a corresponding
set of desired ‘target representations’. Much as targetprop schemes [56, 30, 234] do, LRA posits that these target
values must be created by a complementary process (in the form of a neural structure), but instead of constructing
a backwards chain of approximate decoders/inversions, LRA entails constructing a synaptic message passing struc-
ture, where mismatch values (or local ‘discrepancies’) are communicated along error connections. Furthermore,
LRA-based schemes [325, 323, 326, 101, 195] (including variants such as feedback control [279, 278]) focus on
decomposing the larger credit assignment problem into smaller, easier-to-solve sub-problems; viewing a full neu-
ral architecture’s underlying computational graph as a composition of connected “computational subgraphs” (the
boundaries of each subgraph are defined as the set of all pre-synaptic neural units/variables and all post-synaptic
neural units/outputs). Ultimately, learning under LRA will first require that, for any given input/output pattern pair-
ing, target representations are computed and then, given the acquired targets, synaptic parameters are adjusted to
better recall these target representations for similar sensory input/output pairings.
Concretely, each neuronal layer in LRA will have a target (activity) vector associated with it such that adjusting the
associated synaptic weights will help move that layer’s activity values towards the better, matched target values in
the future. LRA ensures that useful changes in plasticity are produced within LRA by effectively choosing targets
that are “within” the possible representation of a network’s associated layers, i.e. layers are not forced to try to
match a target that is impossible to achieve [325] (for exampling, the layer’s activation function imposes bounds on
what values it outputs). Formally, under LRA, we define the mismatch signals at any layer ℓto be computed as the
first derivative of a local distance function eℓ=
∂||zℓ−bzℓ||q
p
∂zℓ
, e.g., if p = 2 then eℓ= −2(zℓ−bzℓ), and the target
representation to be produced for any layer bzℓis done according to the following:
bzℓ= ϕℓ
hℓ−β
 X
b∈Bℓ
dℓ
b

, where dℓ
b = Eb→ℓ· eb
(28)
where β is a the pre-transformation perturbation coefficient and Bℓis the set of integer indices where each index
corresponds to the bth layer (b ̸= ℓ) in the neural system that will pass/transmit its own mismatch message/signal
to layer ℓ. Note that Eb→ℓindicates a synaptic error message passing pathway between layers b and ℓ. In effect,
according to Equation 28, an adjustment, i.e., the accumulation of all transmitted error messages of layers indexed
in set Bℓ, is applied to the (pre-transformed) neural activity itself to produce a target representation vector bzℓ(in
effect, this could be viewed as a single step of short-term plasticity). Once a target is produced for layer ℓ, synaptic
plasticity proceeds according to the following local rules:
∆Wℓ= eℓ· (zℓ−1)T, and, ∆Eb→ℓ= γ

eℓ· (zb)T
(29)
where the above prescribes an adjustment to both the connections between layers ℓ−1 and ℓbut also for any
message passing pathway Eb→ℓ; 0 ≤γ < 1 is a temporal scaling constant that slows down the evolution of
message passing synapses and has been shown to ensure stability in a neural system’s learning [323, 326]. Note
that the error synapses could also be set to fixed random values as was done in LRA’s earlier incarnations [325]).
Note that the notion that every single layer must be associated with a target can be relaxed, as was shown in the
scheme known as recursive-LRA (rec-LRA; [326]), which circumvents the problems of backwards and update
locking. This stems from the very fact that LRA schemes do not require the message passing structure to be
designed to mirror the information flow through a neural model’s various layers, i.e., its inference scheme; this
permits “skip error feedback connections” or (error) synaptic transmission pathways that skip across layers (similar
in spirit to, but more general than, the wiring patterns of direct feedback alignment [301]). Underwriting the setup
of rec-LRA is the idea that subgraphs – portions of a neural architecture such as the stack of operators (represented
by hierarchically arranged neurons) that characterizes a ‘residual block’ in a deep residual network – can be paired
with one target, which has also been referred to as an intermediate or “supporting representation” [326]. Once the
message passing structure produces a target representation vector for a given subgraph, the synaptic updates made
to the internal layers of that subgraph can be made with a small skip-layer transmission pathway – in other words,
a recursive application of rec-LRA to the block itself – or. alternatively. with direct feedback alignment or Hebbian
plasticity rules, independently of and in parallel to the other subgraphs that compose the entire neural system.
Notably, under some mild assumptions, it can also be shown that LRA approximates backprop [325] as well as
predictive coding [380]. LRA-based approaches have been mainly tested on classification tasks focused on the
domain of computer vision and have been shown to generalize well on natural images [471], e.g., CIFAR-10 or
SVHN, as well as large-scale (image) benchmarks such as ImageNet [326]. Furthermore, evidence has been found
showing that LRA is robust to poor choices of initialization and can train deep networks (including very thin and
deep ones) from initializations that would cause backprop, as well as some other biologically-inspired algorithms,
to fail [325, 323]. Furthermore, LRA, like targetprop, does not require differentiable neural activities and has been
shown to robustly handle stochastic and discrete-valued functions [325, 323, 326, 471]. In addition, elements of
22

Preprint
LRA have been been integrated into temporal learning architectures that process sequential data online [320] and
interesting generalizations of it entail treating the synergy afforded by feedback as a control problem [279, 278].
Predictive Coding.
While some neurobiological algorithms and schemes borrow ideas from predictive cod-
ing/processing [67, 265], such as the aforementioned LRA or targetprop, a particularly noteworthy member of
the discrepancy reduction family in our credit assignment taxonomy is predictive coding itself, or rather, compu-
tational (sparse) predictive coding [354, 308, 369, 111, 25, 333, 57, 410, 316, 382]. Crucially, predictive coding
centers around the fundamental premise that the brain is a probabilistic generative model; it is a stochastic engine
that continuously makes predictions about its environment and updates its internal hypotheses (that create those
predictions) based on the sensory evidence it gathers. Theoretically, predictive coding entails a commitment to the
Bayesian brain or predictive processing hypothesis [239, 87, 67]. Mechanistically, a core distinguishing element of
predictive coding is that of the error neuron – a specific neuronal cell (modeling aspects of superficial pyramidal
cells) whose dynamics embody local mismatch signals – and the state neuron – a stateful neuronal cell whose dy-
namics (that model aspects of deep excitatory pyramidal cells) capture statistical regularities and, depending on their
design, temporal dependencies inherent to sensory streams. Error neurons effectively encode the difference between
the activity at a given layer of the network and that predicted/generated by a higher-level; these mismatch signals
are then propagated throughout the network [354]. In neurophysiological studies, evidence of prediction errors in
neural activity during perceptual decision tasks has been found [416, 417]. Furthermore, neurons in the primary
visual cortex have been reported to respond to mismatches between predicted and actual visual input [480, 99].
Formally, predictive coding (PC) starts from the perspective that any one of the brain’s underlying neural circuits
(a network of neuronal units) can be cast a generative model and expressed in terms of a quantity that it optimizes
at any single step in time – a functional known as the variational free energy (VFE; [113]). VFE effectively states
that two key terms must be balanced: one that encourages improvement in the accuracy the generative component
and another that penalizes model complexity (which aligning with the notion of Occam’s razor) by encouraging the
underlying model to ensure that the Kullback-Leibler (KL) divergence [216] between its recognition model and its
prior is as small as possible. Formally, VFE can be stated in the following manner:
E(p, q, x) =
X
z
q(z)log
q(z)
p(z)

|
{z
}
Complexity Term
+
X
z
q(z)log

1
p(x | z)

|
{z
}
Accuracy Term
(30)
where p(x|z) is the underlying directed generative model (likelihood) that a predictive coding circuit embodies
while p(z) is the prior over its (latent) variables, cast in terms of neural activity values. q(s) is an approximate
posterior distribution over the latent activities z (given an observation); the choice of this posterior often depends on
the structure of the generative model and the parameters to be optimized. The marginal probability of a predictive
coding neural circuit exhibits the following dependencies:17
p(z0, . . . , zL) = p(z0)
L
Y
ℓ=1
p(zℓ| zℓ−1)
(31)
and, if each intermediate distribution is chosen to be multivariate Gaussian, i.e., p(zℓ| zℓ−1) with mean produced
by ¯zℓ= Wℓ(t) · ϕℓ−1(zℓ−1(t)), a transformation of the latent neural states in the layer above, we obtain:
p(z0) = N(z0;¯z0, Σ0), and, p(zℓ| zℓ−1) = N(zℓ;¯zℓ, Σℓ),
(32)
meaning that we treat a neural circuit as a hierarchical latent variable Gaussian generative model. To optimize this
generative neural circuit, we can write down a concrete form of the VFE in Equation 30, after assuming a mean-
field approximation that enforces independencies among the neurons within the circuit (see [113, 321, 316, 380] for
details), in the following manner:
F(Θ) =
L
X
ℓ=0
1
2Σℓ
Jℓ
X
i=1
(zℓ
i(t) −¯zℓ
i)2
(33)
where Θ = {Wℓ(t), Eℓ(t)}L
ℓ=1, the construct that houses all of the synapses that make up the predictive coding
circuit; Wℓ(t) are the generative/predictive synapses while Eℓ(t) are the message passing synapses. The functional
of Equation 33 highlights two key neurobiological commitments that predictive coding models make: 1) there
exists a kind of neuron which specializes in calculating mismatch values – this means that each layer of the circuit
17Note that, in what follows, we have, for consistency within this survey, presented predictive coding in a “bottom-up”
depiction whereas, in the literature, this kind of model is presented in a “top-down” fashion, i.e., the generative model is typically
formulated as p(z0, ...zL) = p(zL QL−1
ℓ=0 p(zℓ|zℓ+1); see [380] for further details.
23

Preprint
contains a set of precision-weighted error neurons as in: eℓ(t) =
1
Σℓ(zℓ(t) −¯zℓ)18, and 2) any state neuron i in one
layer of the circuit (specifically inside of the vector zℓ+1(t)) is actively attempting to guess the activity value of
another neuron j in layer ℓ. Furthermore, in PC, the activities of any layer of (state) neurons are modeled in terms
of time-evolving dynamics, formally through variations of the following ordinary differential equation:
τm
∂zℓ(t)
∂t
= −γzℓ(t) + dℓ⊙∂ϕℓ(zℓ(t)) −eℓ(t)
(34)
where τm is the cellular membrane time constant and dℓ= Eℓ+1(t) · eℓ+1(t), i.e., the perturbations produced by
error messages that are passed back along feedback synapses Eℓ(t). Typically, predictive coding circuitry is simu-
lated over a window of time of length T (the stimulus presentation time), in a dynamic expectation-maximization
fashion [82, 116, 114], where Equation 34 is repeatedly run, via Euler integration, for T discrete steps to obtain
activity values for zℓ(t) (this equation is applied to all layers in parallel). After values for each layer are obtained, a
synaptic update is triggered (for both the generative/predictive synapses and the message passing ones), such as in
the following two-factor Hebbian rules:
τw
∂Wℓ(t)
∂t
= −γwWℓ(t) + eℓ(t) ·

zℓ−1(t)
T
(35)
τe
∂Eℓ(t)
∂t
= −γeEℓ(t) + zℓ−1(t) ·

eℓ(t)
T
(36)
where τw and τe are generative and error feedback synaptic plasticity time constants, respectively, while γw and γe
are the decay constants for the generative and error feedback synaptic updates, respectively. In essence, the above
equations indicate that plasticity for synapses is based on error neuron activity and pre-synaptic state unit values.
Note that, while we present a generalized form of predictive coding using separate forward and backward connec-
tions, many implementations of predictive coding simplify the process by setting Eℓ(t) = (Wℓ(t))T , removing the
need for Equation 36 entirely.
z
z
- 1
z
+ 1
W
+ 1
W
E
E
+ 1
ΔW
e
+ 1
e
 
 
Figure 12: A predictive coding (sub-)circuit pro-
ducing a synaptic update for Wℓ; this change in
plasticity is a function of local information, i.e.,
(post-synaptic) error neuron activities at ℓand (pre-
synaptic) state neuron activities at ℓ−1.
PC circuitry can be used to construct a wide plethora of models, in-
cluding unsupervised generative models [316] and graph networks
[382]. To construct a supervised prediction model, like our ex-
ample MLP, all that one needs to do is clamp the bottom layer of
the network to an input observation z0(t) = x and the top layer
to a target context value zL(t) = y, initialize the values for the
hidden layers by next running a feedforward pass through the net-
work19, and then run Equation 34 for T steps20 before finally calcu-
lating synaptic updates using Equations 35 and 36. Notice that this
scheme can be viewed as treating the updates to state neurons as a
form of short-term plasticity and the synaptic updates as a form of
long-term plasticity (potentiation and depression).
Recent efforts in machine intelligence have provided encouraging
results for a wide range of architectures based on predictive cod-
ing credit assignment [57, 384, 328, 452, 320], including those
that have demonstrated its value on time-varying, sequential data
streams [328, 320, 191], continual learning [321, 468], reinforce-
ment learning control [318, 132, 355], and large language model-
ing [340] (see [380] for a more thorough survey of machine learn-
ing applications of predictive coding).
Furthermore, predictive
coding has been generalized, in various ways, to operate at the level
of spiking neuronal dynamics [474, 353, 314, 296]. Other comple-
mentary work has proposed augmenting the typical state-of-the-art
feedforward DNNs [445] with predictive coding elements/features
(a “side predictive network” that can be treated as a process that
must settle to good representations that feed to a final linear classi-
fier). Others have drawn inspiration from the concept of the error
unit in predictive coding and incorporated it into backprop-based frameworks for video processing [252].
18This treatment of predictive coding assumes a fixed scalar precision
1
Σℓ. However, this need not be the case and Σℓcould
instead take the form of a learnable synaptic (covariance) matrix, which is done in some variants of predictive coding [316, 380].
19This greatly speeds up model optimization convergence and reduces the number of steps T taken for the short-term plasticity
equations to be run [315].
20An alternative is to run Equation 34 until the neural circuit has converged to a fixed point/steady state [452].
24

Preprint
The synergistic local credit assignment conducted under the framework of predictive coding satisfies many criteria
for biological plausibility and is further grounded in a principled Bayesian framing of neuronal inference and learn-
ing [112, 115], further offering reasonably efficient computational processing mechanisms. The update rules are
simple and local – as presented above, they reduce to two-factor Hebbian-like adjustments [354, 452, 320, 316] –
and are triggered by the neural machinery of the system’s inference, particularly its message passing scheme as con-
structed via error transmission pathways. Crucially, a predictive coding circuit is completely unlocked – it resolves
the forward, backwards, and update locking problems inherent to backprop. Furthermore, in the formulation shown
above, the weight transport problem is additionally resolved (if using separate error transmission synapses [316] as
opposed to tied forward/backward weights as in [354]). Furthermore, it has even been theoretically shown that, un-
der certain conditions, predictive coding can approximate, or rather converge to, backprop [452, 382], specifically in
the limit where the activity of the error neurons approaches zero; earlier evidence of PC-like neural model approxi-
mately emulating backprop was provided in [211]. PC structures, including models such as [144, 375], although far
more biologically-plausible than backprop-trained ANNs, are still not without issue – it is argued that their typical
formulation of one-to-one pairing between states neurons and error neurons is not biophysically realistic [46, 452]
and that error neurons cannot take on both positive and negative values as they do in the dynamics presented above,
i.e., biological neurons cannot have negative activity values though it has been remarked that this could be corrected
by using the more biologically plausible linear rectifier [47]. In general, for more focused, targeted reviews of PC
that also cover its wide plethora of variants as well as task applications, we refer the reader to [411, 282, 380].
2.4.2
Energy-Based / Phase-Based Approaches
This credit assignment family shares many similarities with discrepancy reduction; notably that it is possible to
write down a global energy functional that is being optimized by the neural system. However, a defining feature of
the procedures reviewed here is that the signals required to trigger changes in synaptic values come from multiple
phases of settling or relaxation; these require processes that require multiple steps of iterative computation to the
dynamics of each neuronal layer. In short, the underlying neural model must “settle” to an equilibrium point (if
deterministic) or stationary point (if stochastic) to obtain both its predictions and the targets needed for parameter
adaptation/adjustment – a key advantage, as we will observe, is that effectively the same neural machinery used in
inference directly plays a role in changes in synaptic plasticity.
The main motivation behind this family is that neurobiological learning/memory can be cast as manipulating points
in an energy functional’s landscape [231]. This means that the hidden units of the network will gradually, as a result
of adjustments made to synapses, move towards configurations that are more probable given the sensory input and
the agent’s internal model of the world (as represented by its parameters). Note that phase-based computation typ-
ically applies to a type of neural model called an ‘interactive activation network’ [271], a well-known instantiation
being the continuous Hopfield model [178] with modern incarnations [352]. In contrast to networks such as our
MLP example, interactive networks offer several interesting benefits: 1) they naturally exhibit a pattern completion
capability (i.e., imputation of missing features), 2) their design offers a flexible treatment of units as either inputs or
outputs and have a useful connection to an associated energy function that is useful for pattern recognition in gen-
eral [457, 231], 3) they can be used to solve soft-constraint problems (making them suitable for modeling various
cognitive processes [271]), and 4) they often do not require post-activation derivatives (unless the iterative dynamics
of the neurons have been particularly specified to require them). Central drawbacks for phase-based models are that,
as we will see, they generally require strict symmetry of forward and feedback weights and, more significantly, are
slow to train given the lengthy relaxation phases needed to find a system’s fixed-points.
Why is an energy functional useful? The main idea is to create a set of attractor states in a nonlinear network and, as
pointed out by [179, 178], if the feedforward and feedback connections are symmetric, the network will settle down
into states that are the local minima of a simple (energy) functional. New minima would be produce by Hebbian
plasticity, yielding dynamics that would allow a network (with fixed weights) to conduct memory retrieval from
incomplete or corrupted patterns/memories. Anti-Hebbian learning could then be used to store new memories. The
energy functional, in a Hopfield network, could be used to determine the “fast” dynamics of neuronal activities
while a different objective, one that measured the proximity of energy minima to pattern vectors that needed to be
stored, could be used to determine the “slow” dynamics of the synaptic strengths [161]. In addition to characterizing
such a system’s generalization ability [172], the (stochastic) dynamics of neurons could be justified as performing
Bayesian inference, providing mathematical justification of simple synaptic adjustment rules; we will observe this in
credit assignment schemes such as contrastive Hebbian learning and contrastive divergence. A consequence of this
perspective is that the algorithms of this section must use one phase to sample interpretations of a sensory vector, in
proportion to their probabilities, to lower the energies for the sampled interpretations (by increasing synaptic weight
values) while another phase is used to raise the energies of the alternative interpretations.
25

Preprint
Contrastive Hebbian Learning.
Contrastive Hebbian learning [179, 292, 124, 19, 400, 197, 85] (CHL), with
modern variations [350, 458], is a generalization of Hebbian plasticity that ultimately results in a subtractive mea-
surement between the cross-products of activation values collected from a clamped phase and those collected from
a free running phase. CHL notably underpins the foundations of Boltzmann learning [164] as well as the general
adaptation of mean-field networks [12]. Though CHL was originally proposed for a specific class of interactive net-
works, later work demonstrated CHL could be applied to any case of what is now known as the continuous Hopfield
model [164, 292, 405]; recent incarnations are sometimes referred to as ‘modern’ or ‘universal’ Hopfield networks
[352, 397, 281]. These early results are important in that they allow us to treat even our running example MLP as
an interactive system, provided that we treat its forward connections also as symmetric feedback connections (any
arbitrary network can be created from the fully-connected Hopfield by simply zeroing out certain connections, an
important point that was revived in equilibrium propagation [388]; reviewed later in this section).
Positive phase
Negative phase
Figure 13:
The two phases of contrastive Hebbian learning: a
clamped positive phase followed by an unclamped negative phase.
As mentioned earlier, given the presence of recur-
rent connections, if several simple conditions are met
[178], it is guaranteed that a system’s neural activi-
ties will settle to a fixed point. More importantly, this
fixed point will lie at a minimum of an energy func-
tion. In the decomposition provided in [292, 309],
the full energy function can be broken into two terms,
one that reflects the constraints imposed by the con-
nection weights of the model which will drive neu-
ral activities to extreme values and another one that
serves as a penalty function that drives neural activ-
ities towards a resting value. By evolving the activi-
ties of neurons through a differential equation (since
the model is considered from the perspective of con-
tinuous time), we will find a state that is maximally
harmonious with the information already encoded in
the weights and one that minimizes the energy. In
essence, CHL modifies synapses such that the sta-
ble states of the neuronal units match desired pat-
terns of activations. This requires the use of a con-
trastive function, which has been shown to come from
an even broader class of functions [19], which fo-
cuses not on the trajectories taken by the units to
equilibrium but the final state found at equilibrium
itself. In effect, this functional is simply L(Θ) =
E({zℓ,+}L
ℓ=0) −E({zℓ,−}L
ℓ=0) (or L = E+ −E−in simplified notation), where E({zℓ,+}L
ℓ=0) is the value of
the energy function at equilibrium when output units are clamped and E({zℓ,−}L
ℓ=0) is the equilibrium value when
outputs are free. In other words, the contrastive function measures the difference of the network’s Lyapunov func-
tions [88] (or measures of stability) between clamped and free modes. After each step of learning, the difference in
energy at equilibrium between clamped and free states becomes smaller.
Formally, in an interactive network that is trained via CHL, layer-wise activities are updated in accordance with the
following neuronal dynamics:
zℓ(t + ∆t) = zℓ(t) + ∆t
τ

−zℓ(t) + ϕℓ(Wℓ· zℓ−1(t) + γ(Wℓ+1)T · zℓ+1(t))

(37)
where we notice that neural values change due to (leaky) Euler integration and the top-down and bottom-up trans-
mitted signals are combined to produce a perturbation to the layer. Notice that the top-down recurrent feedback
connections are set to be equal to the transpose of the corresponding forward ones. To produce synaptic adjust-
ments, Equation 37 must be run iteratively for two sets of T steps. The first set of steps (the positive clamped phase)
entails clamping both the bottom and top layers to input and output patterns, i.e., z0 = x and zL = y; this results
in layer-wise activities {z0,+ = x} ∪{zℓ,+}L−1
ℓ=1 ∪{zL,+ = y}. The second set of steps (the negative free phase)
entails, starting from the current values of activities produced from the first phase, clamping only the bottom layer,
i.e., z0 = x, to produce the layer-wise activities {z0,−= x} ∪{zℓ,−}L
ℓ=1. Once these two sets of neural activity
values have been obtained, a change in plasticity is computed as follows:
∆Wℓ=
1
γL−ℓ
∂E+
∂Θ −∂E−
∂Θ

=
1
γL−ℓ

zℓ,+ · (zℓ−1,+)T
−

zℓ,−· (zℓ−1,−)T
(38)
26

Preprint
where we notice that the recurrent dampening factor γ (to introduce stability and prevent feedback synapses from
being perfectly symmetric to the forward ones) appears in the final synaptic update, as a function of the layer index
it is applied to [464]. The above plasticity equation demonstrates that CHL is effectively a difference between two
dual-term Hebbian adjustments.
In [292], varieties of CHL were presented, with some instances that flipped the order of the positive/clamped and
negative/free phases; each were argued to have different issues and strengths. The version presented above, as
discussed in [292], (empirically) leads to quicker training and comes with some theoretical guarantees with respect
to how the energy function is optimized in different phases. Interestingly enough, different generalizations of CHL
either follow this same ordering of phases (clamped then unclamped, e.g., contrastive divergence [165], or the
opposite (unclamped then clamped), e.g., equilibrium propagation [388]. The GeneRec process [309] (part of the
Leabra framework [311]), as mentioned earlier in this survey, combines principles from both recirculation and CHL
to construct a phase-based form of credit assignment that further approximates recurrent backprop [8]. It can, as
argued in [309], recover CHL when treating it as an approximation of second-order Runge-Kutta integration in
tandem with a symmetry preserving constraint (connecting it to the symmetric delta rule [232]).
Contrastive Divergence and Wake-Sleep. An early biologically-plausible connectionist model that built on the
ideas of Hopfield memory was the harmonium [172, 406] – also later referred to as the restricted Boltzmann machine
(RBM) [165] – trained with an algorithm known as contrastive divergence (CD), which built on the credit assign-
ment scheme of contrastive Hebbian learning. Through a Markov Chain Monte Carlo (MCMC) sampling process,
or rather, alternating block Gibbs sampling, “fantasy” data vectors are generated from the model after initializing
a Markov chain from a clamped vector drawn from the empirical distribution / training data. By propagating the
data up to the latent variables and back down to the visible units, the model is run to “thermal equilibrium”, and
a sample is taken/drawn at this special state (serving as an approximation of the model’s internal distribution) and
subsequently used in what reduces to a CHL subtractive synaptic adjustment. One interpretation of this credit
assignment process is that it is attempting to minimize the Kullback-Leibler (KL) Divergence [165] between two
probability distributions – the model’s fantasy distribution and the data’s real underlying distribution.
Concretely, the synapses of a single-layer harmonium [398, 165] would be adjusted in accordance to optimizing its
defined energy functional. Specifically, an RBM can be fully specified in terms of this functional (biases omitted):
E(zℓ−1, zℓ; Θ) = −

(zℓ−1)T · Wℓ· zℓ
.
(39)
The above joint energy function can further be used to compute the marginal probability that the RBM assigns its
sensory input layer zℓ−1. Formally, this means that the underlying model computes:
p(zℓ−1) = 1
C
X
zℓ∈Zℓ
exp

−E(zℓ−1, zℓ; Θ)

(40)
where Zℓis the set of all possible (binary) configuration values that a vector zℓcould take and C is the normalizing
constant (or partition function); C = P
zℓ−1∈Zℓ−1,zℓ∈Zℓexp

−E(zℓ−1, zℓ; Θ)

.21 Taking the gradient of the
above marginal probability (with respect to synapses) results in a rather simple adjustment recipe:
∂log p(zℓ−1)
∂Wℓ
=
D
zℓ,+ · (zℓ−1,+)TE
data −
D
zℓ,−· (zℓ−1,−)TE
model
(41)
where < ◦>d denotes taking an expectation under the distribution d specified in the subscript (such as under the
d =‘data’ or d =‘reconstruction’ distribution, as in the above). The above plasticity rule only requires access to the
neural activities of the RBM’s input and output layers and further means that the neural system can learn to raise the
probability of the input patterns it encounters by adjusting its synaptic efficacies to lower the energy on those inputs
while raising the energy on others (those it does not encounter), particularly manipulating the energy for patterns
that make a large contribution to the normalizing constant.
Measuring the neural activities in an RBM is also straightforward and efficient since we may exploit a key simpli-
fication made to its synaptic structure – there are no lateral connections between its units in any layer. This lack of
cross-layer connections facilitates the use of block Gibbs sampling [128] (since the individual units in a layer can be
assumed to be conditionally independent of one another given the other layer) to first collect samples of the RBM’s
data-dependent (i.e., positive or clamped) phase and then samples of its data-independent phase (i.e., negative or
21An important drawback of harmoniums is the difficulty in tracking its log likelihood objective; this requires computing the
partition function C which is intractable to calculate for any reasonably-sized RBM. Expensive approximations of the partition
function, such as annealed importance sampling, or metrics like reconstruction cross entropy can be used as proxies for tracking
actual progress, but do not reflect how well or how poorly an RBM model is learning.
27

Preprint
Algorithm 2 K-step contrastive divergence (CD-K) for computing synaptic updates for a two-layer harmonium.
1: Input: Θ = {W1}, K
2: function COMPUTEUPDATES(x, Θ)
3:
// Compute positive-phase activity values
4:
z0,+ = x
▷Clamp input layer to sensory pattern x
5:
p1,+ = σ

W1 · z0,+
, z1,+
j
∼B(zℓ,+
j
= 1; p1,+
j
)
6:
z0,−= z0,+,
z1,−= z1,+
7:
// Compute negative-phase activity values
8:
for k = 1 to K do
▷Run K steps of persistent block Gibbs sampling
9:
p0,−= σ

(W1)T · z1,−
, z0,−
i
∼B(z0
j = 1; p0,−
i
)
10:
p1,−= σ

W1 · z0,−
, z1,−
j
∼B(zℓ,−
j
= 1; p1,−
j
)
11:
// Compute synaptic updates:
12:
∆W1 = z1,+ · (z0,+)T −z1,−· (z0,−)T
▷Update based on Equation 44
13:
return {∆W1}
unclamped). As a consequence, sampling the model’s activities is done as follows:
pℓ= ϕℓ(Wℓ· zℓ−1), where pℓ
j = p(zℓ
j|zℓ−1) and zℓ
j ∼B(zℓ
j = 1; pℓ
j)
(42)
pℓ−1 = ϕℓ((Wℓ)T · zℓ), where pℓ−1
j
= p(zℓ−1
j
|zℓ) and zℓ−1
i
∼B(zℓ−1
i
= 1; pℓ−1
i
)
(43)
noting that ∼B(z = 1|p) denotes sampling from the Bernoulli distribution (with probability that the value is one
parameterized by a value p). Values collected from the positive phase are tagged with ‘+’ as in zℓ,+ whereas those
obtained a negative phase are tagged with ‘-’, as in zℓ,−. To find the values needed for Equation 41, Equations 42 and
43 are run several times in succession; however, computing the second term of Equation 41 (taking the expectation
under the model distribution) is still very difficult, requiring the construction of a Markov chain Monte Carlo
process, running Equations 42 and 43 many times until until (statistical) equilibrium has been reached. Instead, as a
fast approximation to the log gradient update, [165, 166] argued that, instead, a far simpler contrastive (correlational)
scheme could be leveraged, replacing the second model distribution term with a reconstruction distribution term.
This rule uses only two subsequent applications of 42 and 43 (to obtain positive and negative phase statistics):
∆Wℓ≈
D
zℓ,+ · (zℓ−1,+)TE
data −
D
zℓ,−· (zℓ−1,−)TE
recon.
(44)
The above update rule is referred to as contrastive divergence (CD, or CD-1); a consequence of this fast rule
is that it is technically optimizing the difference between two KL divergences, sans a tricky term, rather than
the actual negative log likelihood of p(x) [420, 166]. Nevertheless, in practice, this plasticity rule works well
for training RBMs. A variation of CD is presented in Algorithm 2, where we depict a slightly more accurate
implementation that uses K sampling steps to estimate the reconstruction term, i.e., CD-K. Note that computing
the synaptic updates/gradients via CD does entail using a biased sample, however, the variance introduced through
this approximation is not significant enough to reduce the learning process’s effectiveness. Further improvements
to CD-centric credit assignment have been explored [432, 431, 32, 366]; notably, a useful improvement is known as
parallel tempering [84], which exploits the advantages offered by persistent Gibbs (block) sampling, alternatively
referred as stochastic maximum likelihood. In essence, one maintains one or more continuously running chains in
the background, using either random chain swapping or an expectation calculated across these “fantasy particles”,
to facilitate a broader and faster exploration of the RBM’s internal configuration state space [84].
Although this survey’s treatment of harmoniums and contrastive divergence-based learning has been presented
in the context of arbitrary, multi-layer systems, training ‘deep harmoniums’ has been shown to be a challenging
endeavor [35, 378, 171, 289, 327]; note that such systems are also referred to as deep belief networks (DBNs)
[170, 287, 376] and, when additional top-down recurrent connections are introduced, as deep Boltzmann machines
(DBMs) [377, 63]. For a DBM, the neuronal activities are a function of bottom-up and top-down pressures as in:
zℓ= ϕℓ
Wℓ· zℓ−1 + (Wℓ+1)T · zℓ+1
(45)
where the top-down recurrent weights are set to be the transpose of the weight matrix of the layer above. Given
their probabilistic, sampling-based nature, one specific scheme generalizes contrastive divergence to what is known
as the stochastic approximation procedure (SAP). In effect, SAP entails carefully crafting a block Gibbs sampler
that alternates the sampling of evenly-numbered and oddly-numbered layers in the harmonium system. The SAP
learning process preserves a synergistic form of locality in the credit assignment although this comes at the price of
greater external control (to coordinate the interaction of layers); SAP further requires custom initialization proce-
dures for the layer-wise activities, such as greedy layer-wise pre-training of a stack of RBMs.
28

Preprint
p
W
p
- 1
pL
ΔW
p
+ 1
W
+ 1
p L -1
z
z
- 1
z L
z
+ 1
V
+ 1
z L -1
z 0
 
V
Figure 14: The sleep phase of a Helmholtz machine: the top-
down directed generative model (right network) is sampled an-
cestrally to obtain a set of neural activities that drive the recog-
nition model (left network) to produce layer-wise probabilities.
Connections between ℓ−1 and ℓare updated via a local product
of pre-synaptic activity zℓand post-synaptic probabilities bpℓ.
The wake-sleep algorithm [168, 49] was originally pro-
posed as a simple way to learn a generative model com-
posed of two opposite flowing directed sub-models; the
Helmholtz machine [173, 79, 163, 78, 76] and the sig-
moid belief network [385, 418] are among some of the
more prominent models that adhere to this framework.
Helmholtz machines, which are neural systems trained
by optimizing a (Helmholtzian) free energy functional
(average energy minus entropy; which also corresponds
to the surprise of the generative model weighted by the
probability of the data) [163, 200], consist of a network
that is used to generate samples (known as the ‘gen-
erative distribution’) and another network that is used
for inference (known as the ‘recognition distribution’).
The “wake phase” of the learning algorithm entails pre-
senting a data point to the inference model, propagating
the activities forward across this network (co-model),
and then updating the generative network (co-model),
at each layer, to make it more likely to generate the val-
ues found in the inference network. Formally, this step
of the wake-sleep process requires running the recog-
nition and generative co-models (with z0 = x) in the
following manner:
pℓ= ϕℓ(Wℓ· zℓ−1), and, zℓ
j ∼B(zℓ
j = 1; pℓ
j) for ℓ= 1, 2, ..., L
(46)
bpL = ϕL(bL), and, bpℓ= ϕℓ(Vℓ· zℓ) for ℓ= L −1, L −2, ..., 0
(47)
which then triggers the following synaptic adjustments to the generative model:
∆bL = zL −bpL and ∆Vℓ= (zℓ−bpℓ) · (zℓ+1)T.
(48)
Notice that we have introduced a particular bias vector pL which serves as the parameters of the prior of the
generative co-model. The “sleep phase” involves sampling the generative model to obtain a set of “fantasy” samples
and the inference model is adapted to better infer the fantasy samples’ causes. Formally, this entails:
pL = ϕℓ(bL), and zL
j ∼B(zL
j = 1; pL
j )
(49)
pℓ= ϕℓ(Vℓ· zℓ+1) and zℓ
j ∼B(zℓ
j = 1; pℓ
j) for ℓ= L −1, L −2, ..., 0
(50)
bpℓ= ϕℓ(Wℓ· zℓ−1), for ℓ= 1, 2, ..., L
(51)
which then triggers the following synaptic adjustments to be made to the recognition model:
∆Wℓ= (zℓ−bpℓ) · (zℓ−1)T.
(52)
The above process for a Helmholtz machine – Equations 46-47, 51-50, and synaptic updates via Equations 48 and 52
– can be tracked by measuring the KL divergence between probability of the data and the probabilities produced by
the top-down directed generative model [200]. Beyond the specialized wake-sleep credit assignment scheme applied
to Helmholtz machines, the up-down back-fitting algorithm [170] was developed as a generalization of wake-sleep
for fine-tuning a deep belief network (after it was constructed via a greedy, layer-wise pre-training phase). Notably,
this scheme addressed/circumvented one of the wake-sleep algorithm’s issues: the problem of “mode averaging”.
In wake-sleep, it is possible for the inference weights to pick a particular mode of the layer above and remain
“stuck” to it, even if other modes would be useful to infer when learning to generate sensory data; this hinders the
effectiveness of the sleep phase (downward pass), constraining the training of the recognition weights to whatever
mode it uncovers first without exploring other useful modes of the data’s underlying distribution. In contrast, in the
up-down algorithm, instead of starting the generative pass using a completely random sample that is input to the
topmost layer of the generative co-model, instead, the sample synthesized at the top is biased by running a Gibbs
sampler for a few steps using the associative memory formed by the top-most RBM of the DBN.
Beyond DBNs, DBMs, and Helmholtz machines, many other models and algorithms for learning generative systems
take inspiration from contrastive divergence and wake-sleep, including the variational walkback algorithm [41, 123]
and equilibrium propagation [388] (reviewed next). Furthermore, research efforts related to Helmholtz machines
and harmoniums not only helped to spark the ‘deep learning revolution’, through the stacking of RBMs to pre-train
deep neural architectures [35, 287], but they also inspired later incarnations of neural generative models, e.g., those
learned through the framework of neural variational inference, e.g., variational autoencoders [199, 399].
29

Preprint
Equilibrium Propagation. The equilibrium propagation (EProp) procedure [33, 387, 388, 390], like the other
schemes within this family, centers around an energy functional, specifically one that corresponds to the continuous
Hopfield network model [178, 33]. Although EProp shares many similarities to contrastive Hebbian learning, there
are some subtle yet important differences. A setting where EProp looks markedly different from CHL is when
supervised learning is the goal; in this context, an external potential energy L(zL, y) is introduced to drive the
neural system’s output units towards target values, e.g., encoded labels.22 For instance, [388] employed a quadratic
cost for L(zL, y) = 1
2||zL −y. This external energy (or ‘force’) is then combined with the usual Hopfield function
E to create a total energy functional of the form: F = E({zℓ}L
ℓ=1) + γL(zL, y) (or, in a simplified format:
F = E + γL). The external energy value is itself further weighted by the influence factor γ ≥0 which controls
the degree to which the output units zL are pushed towards the target y; this yields the notion of ‘weakly-clamped’
(or softly clamped) outputs as to the opposed to fully/hard-clamped output values inherent to the earlier CHL/CD
schemes. The total energy function F accounts for both the internal interactions within the network (via the term
E) as well as how the target variables influence/nudge the output units towards desired states (via L).
As in CHL, neural activities are described with a differential equation (for motion) which takes the form:
∂hℓ
∂t = ∂E
∂hℓ−γ ∂L
∂hℓ
(53)
where the total energy of the system decreases as time progresses (notice that the above equation is specifically
expressed in terms of pre-transformed neural activities hℓ). The equation for neuronal evolution – the first term on
the right hand side of Equation 53 – can take on one of the following forms:
∂E
∂hℓ= ∂ϕℓ(hℓ) ⊙

Wℓ· ϕℓ−1(hℓ−1)

−hℓ, or,
(54)
∂E
∂hℓ=

Wℓ· ϕℓ−1(hℓ−1)

−hℓ
(55)
where the first form (Equation 54) follows from [387] whereas the second one is the simpler point neuron model
[77], used in [390] (the rate-based leaky integrator neuronal model); using this second form avoids requiring the
first derivative of the activation, i.e., ∂ϕℓ(◦). Crucially, the second term of Equation 53 depends on the choice of
cost function used for the external force; in the case of the quadratic cost of [388], this simply becomes −γ ∂L
∂hL =
γ(y −hL) for the output units and −γ ∂L
∂hℓ= 0 for all other units.
To compute synaptic weight updates under the EProp scheme, like CHL/CD, two phases of computation are used
(desirably using the same neural machinery in both instances) but, unlike CHL/CD, in a different order. In the first
phase, we clamp the inputs to a data vector, set γ = 0 – this is the negative or free (data / context-independent) phase
– and iteratively update each layer of (pre-transformed) neural hℓactivities using the forward Euler method until a
fixed point hℓ,−is reached. Prediction can also be carried out upon reaching this fixed point as well; at the end of
the negative, the output unit values are read out from hL. The second (positive) phase – the data/context-dependent
phase — is then initiated by setting γ > 0 to a small positive value, permitting the external energy term to play a
role in the measure of the total energy F, critically nudging the output units to the desired target or context y. Since
this force only directly impacts the output units (at their free fixed point), the effect of this nudging will reach the
internal hidden neurons as time progresses, by iterating one of the neuronal differential equations above (Equation
54 or 55). This (activation) spreading of the external force over the internal units of the neural system can also
be viewed as conducting a form of backprop itself [387, 390] but without the required global feedback pathway
that uses different neural calculations. In fact, the second phase of this algorithm has been shown to approximate
backprop, or rather, it “back-propagates” error derivatives across the network [33, 38]. Nevertheless, once the
network settles to a new fixed point during the weakly-clamped positive phase we can then use the activities of the
neurons acquired here, i.e., {x} ∪{hℓ,+}L−1
ℓ=1 ∪{y}, and compare with those found at the end of the free/negative
phase, i.e., {x} ∪{hℓ,−}L
ℓ=1. As a result, the ultimate update applied to connection weights can then be calculated
in one of two ways:
∆Wℓ= ϕℓ(hℓ,+) ·

ϕℓ−1(hℓ−1,+)
T
−ϕℓ(hℓ,−) ·

ϕℓ−1(hℓ−1,−)
T
, or,
(56)
∆Wℓ= (hℓ,+ −hℓ,−) ·

ϕℓ−1(hℓ−1,−)
T
.
(57)
The first rule is a contrastive rule, taking a form similar to the plasticity update induced by CHL; note that, however,
the output unit values in the positive phase are the result of nudging towards a target whereas, in CHL, the outputs
22Note that [387, 388] presented the neural system in terms of a full Hopfield model, from which a feedforward network struc-
ture could be extracted. This survey directly presents EProp in terms of the feedforward, hierarchical structure for simplicity;
however, the credit assignment scheme applies to arbitrarily connected systems.
30

Preprint
would be fully clamped to the output context/target y, i.e., γ = ∞.23 This difference in clamping (‘weak’ in
EProp versus ‘hard’ in CHL) clearly stems from the fact that each optimizes a different energy functional, i.e.,
L = E+ −E−in CHL and F = E + γL in EProp. CHL’s energy optimization suffers from the (theoretical) fact
that the objective could take on negative values if the clamped and free fixed points stabilize in different modes of
the energy function; EProp, in contrast, does not face this issue given that its energy functional’s form guarantees
that the weakly-clamped (positive phase) fixed point will be close to the free (negative phase) fixed point, i.e., both
fixed points lie within the same mode of the energy function – this is also partly the result of reordering the negative
phase to come before the positive phase [292]. The second rule (Equation 57) is a simplified Hebbian rule based
on the proposed spike-timing dependent plasticity-like rule of [38]. The central idea of this rule, which assumes
no synaptic modifications during the negative/free phase, is to multiply the neuronal pre-activities with the rate of
change of the post-activities, or simply ∆Wℓ= ∆hℓ· (ϕℓ−1(hℓ−1))T, where the first term would ideally be a
temporal derivative (which is what encodes the necessary information for driving weight plasticity in spiking neural
systems). In effect, the rule shown in Equation 57 is a simple secant approximation of this temporal change whereas
the first rule Equation 56 is derived by integrating over the full trajectory taken by the neural activities during the
each phase [390]. Desirably, it is important to note that equilibrium propagation has been generalized to operate
with finer-grained dynamics, such as those that characterize spiking neuronal units, as in [329, 266].
An important classical algorithm that is intimately related to EProp is recurrent backprop [8, 342, 343], where, for a
given set of (clamped) inputs and target states, the neural model is trained to settle into a stable activation state where
the output units correspond to/align with desired target activity. Recent work has notably revised recurrent backprop,
proposing useful modifications to improve its performance/effectiveness [245]. One key biological implausibility of
recurrent backprop, however, is that, although it considers the same objective that EProp optimizes, its constrained
(Lagrangian) formulation of the optimization problem leads to computing a fixed point for its second phase using a
linearized form the recurrent network itself. This crucially means that the second phase of the algorithm does not
follow the same dynamics as the first phase, thus implausibly (from a biological standpoint) requiring two different
sets of neural computation for its inference and learning.
2.4.3
Forward-Only Learning
One of the more recent families of credit assignment schemes to have been proposed – forward-only processes
[460, 159, 175, 306, 205, 204, 162, 319, 476, 237] – focus on producing synaptic adjustments exclusively with
a neural system’s inference machinery, particularly working to offer a computational explanation of how neurons
might adapt/receive learning signals without the presence of recurrent feedback parameters (as in predictive cod-
ing). Notably, forward-only approaches also, like those under the discrepancy reduction and energy-based families,
employ a set of local loss functionals but particularly enjoy their global synergy as a consequence of two or more
applications of forward propagation of information through the neural system itself. In effect, a second ‘forward
pass’ replaces the usual ‘backward pass’ (or reverse set of calculations) and, desirably, is done so in parallel; this
further circumvents the need for chained/conditioned relaxation phases as in energy-based schemes (CHL/CD).
Forward-only schemes have been argued to provide the fewest constraints on the neural system (including both its
inference and learning processes) [205, 204], given that its central imperative is to utilize one kind of computa-
tion for inference and learning and thus does not require feedback connectivity (further side-stepping the weight
transport problem). In addition, synaptic parameters for any layer of neurons are updated as information is propa-
gated to that layer; learning signals “travel” with the inputs across the ANN simultaneously. A positive side-effect
of forward-only credit assignment is that, in addition having a lower computational complexity (e.g., very cheap
or no iterative inference/relaxation is needed), it reduces the cost of additional memory, a price paid by many al-
gorithms reviewed in the previous sections (for storing activation patterns or holding extra feedback connectivity
structures/parameters), making it an ideal candidate for implementation on neuromorphic, edge-computing hard-
ware [204, 313].
While the development of forward-only credit assignment is rather new ([205] was one of the earlier scalable pro-
posals of forward-only learning and [249] was the earliest proposal of local self-supervised forward adaptation),
having become more prominent over the last few years, two particular classes of forward-only learning procedures
currently exist. Specifically, there are those that leverage a ‘supervised context’ to drive a parallel (target-oriented)
pathway and there are those that center around ‘self-supervised context’ (typically relying on contrastive objec-
tives) to drive theirs. In algorithms that are supervised context-driven [205, 204, 476], the hidden representation of
a sensory input and the representation of its corresponding context, e.g., label, are brought closer together (while
also pushed farther apart from other pairs of sensory inputs and their contexts) via a set of local loss functionals.
Specifically, the total loss for neural systems learned in this manner can be specified as L(Θ) = PL
ℓ=1 Lℓ(zℓ, cℓ)
23This means that, in CHL, the output units are completely insensitive to the internal force induced by the rest of the units of
the neural system. This is an issue that EProp directly resolves.
31

Preprint
W
ΔW
 
z 0= x
c 0= y
z
z
- 1
c
c
- 1
z 0= x
z
z
- 1
z 0= x
z
z
- 1
+
-
c = 1
c = 0
ΔW
 
W
 
ΔW
W
p(c = 1;    )
z
p(c = 1;    )
z
Figure 15: Forward-Only Credit Assignment. Depicted are types of forward-only learning: (Left) supervised context-
centered with signal propagation (sigprop) as an example. The dashed blue box indicates that the activation at ℓis composed
(via concatenation) producing a larger vector that feeds into the weight update. (Right) Self-supervised-context with the forward-
forward algorithm (FFA) as an example. Notice that, in FFA, the kind of the sensory input provided to the network affects what
the synaptic update will do: a positive (original) datapoint x+ raises (↑) the goodness probability p(c = 1; zℓ) of layer ℓwhile a
negative (confabulated) datapoint x−lowers (↓) the goodness probability p(c = 1; zℓ) of layer ℓ. In FFA, Layer normalization
is implied by the purplish trapezoids that appear right above each neuronal layer of activities; note that the weight update ∆Wℓ
uses the layer-normalized pre-synaptic activities but the raw unnormalized post-synaptic activities are used in the local goodness
functional. In either scheme (sigprop or FFA), a local functional produces a signal that triggers a synaptic update for Wℓ.
(notice that this expression is similar to the form that discrepancy reduction losses take – a sum of local measure-
ments). Concretely, an algorithm such as signal propagation [205, 204] runs both a sensory data pattern x and its
corresponding context label y as input through the neural model (our example MLP), producing a set of compound
activities Z = {[zℓ, cℓ]}L
ℓ=1, as follows:
[zℓ, cℓ] =



h
ϕℓ(Wℓ· zℓ−1), ϕℓ(Cℓ· cℓ−1)
i
ℓ= 1
ϕℓ
Wℓ· [zℓ−1, cℓ−1]

otherwise,
(58)
where we notice that a neuronal layer is represented as a composition of two separate vectors of values, i.e.,
[zℓ, cℓ] = mℓ∈R2Jℓ×1 (except for input layer ℓ= 0, where [x, y] = m0 ∈R(J0+C)×1), since, as mentioned
above, there are two forward pathways run in parallel. Immediately, after each layer ℓof neural activity values have
been computed as in Equation 58, the synapses may be adjusted in accordance with the following rule:
∆Wℓ= ∂Lℓ(zℓ, cℓ)
∂[zℓ, cℓ]
·

[zℓ−1, cℓ−1]
T
(59)
where the local loss could take on many forms such as a distance function Lℓ(zℓ, cℓ) = ||cℓ−zℓ||2
2 or a (cosine)
similarity measurement Lℓ(zℓ, cℓ) =
(cℓ)T·zℓ
||zℓ||2zℓ||2 . Notice that the local loss at ℓseparates the context-oriented
pathway cℓfrom the input-oriented pathway activity zℓso as to extract the layer-wise target that will drive/trigger
the synaptic update for the ℓth layer.24 Error forward-propagation (EFP) is notable variation of the supervised
context-driven theme presented above, where an error signal [205] or error-modulated input pattern is propagated
through the network instead of the context [159, 175, 306, 357, 81]. For example, a scheme may, such as the one
in [81], introduce a single set of fixed random feedback connections that wires the output error directly to the input,
producing an error-perturbed version of the sensory input, i.e., ˜x = x + B · ∂L(zL,y)
∂zL
, which is then subsequently
run through the neural model to produce context-driven target values for the hidden layers.25
24At test time inference, note that, to make a prediction in the above model, one would need to cycle through possible context
values y and select the one that yields the lowest total loss (a prediction scheme often used in energy-based models such as
harmoniums [166]). Other schemes for crafting the dual forward pathways are possible, beyond the one presented here, if one
did want to spare the expense for this type of test-time inference [205, 204].
25Note that this type of scheme yields spatially local but not temporally local synaptic updates; this stems from the fact that
the second forward pass is not completed in parallel but constrained to operate sequentially. Furthermore, this scheme has the
unfortunate drawback that the feedback synapses are not learned, as in feedback alignment [246]; though weight mirrors [5]
could be employed to rectify this issue.
32

Preprint
In schemes that take a self-supervised approach [162, 319, 237, 3] (including [131] which mixes [162] with local
backprop), such as the forward-forward (FF) and predictive forward-forward (PFF) procedures, the focus is on
distinguishing between a ‘positive sample’, or data pattern, and a ‘negative sample’, or an automatically generated
adversarial pattern, through the use of two parallel inference pathways (instead of depending on a context label).
The goal of these forms of credit assignment are to maximize the ‘goodness’ for data points taken from the data
distribution and to minimize the goodness for those that are drawn from outside of it adversarially. In an FF-
based scheme, the total loss of a neural system can, like in signal propagation described earlier, be expressed as
a sum of local functionals; however, these functionals focus on comparing and contrasting positive and negative
activity patterns: G(Θ) = PL
ℓ=1 Gℓ(zℓ, cℓ). We use the symbol G to imply ‘goodness’, or the determination of the
probability that the activities of a group of neurons is indicative that an incoming signal comes from the (training)
data distribution (is an instance of a positive class) as opposed to those come from outside of it, e.g., confabulations,
adversarial examples, etc. Formally, for any layer ℓin a neural model, its ‘goodness probability’ is dependent upon
the sum of its squared activities compared to a threshold value θℓ
z:
p(c = 1; zℓ) =
1
1 + exp ( −(PJℓ
j=1(zℓ
j)2 −θz))
,
(60)
where p(c = 1; zℓ) indicates the probability that the activity value at ℓcomes from the data distribution – the
positive class is indicated by c = 1 – while the probability that the data does not is p(c = 0; zℓ) = 1 −p(c = 1; zℓ).
The goodness functional can then be formulated akin to binary class logistic regression:
L(zℓ, c) = −

c log p(c = 1; zℓ) + (1 −c) log p(c = 0; zℓ)

(61)
where the binary label c (a goodness label for sensory datapoint x) is produced by generative process that synthe-
sizes negative data samples. Patterns sampled from the data distribution are labeled as c = 1 while patterns sampled
from the negative data generating process are labeled as c = 0; key to FF’s effectiveness is the design or access to
a useful negative data distribution, much as is done for related statistical procedures such as noise contrastive esti-
mation [148]. With goodness defined, the process undertaken by FF learning entails first running a sensory sample,
regardless of whether it is positive or negative, through the network according to the following slightly modified
forward pass: zℓ= ϕℓ
Wℓ· LN(zℓ−1)

where LN(zℓ−1) =
zℓ−1
(||zℓ−1||2+ϵ) denotes a (parameter-less) layer-wise
normalization [17] operation and ϵ is a small numerical stability factor for preventing division by zero.26 Then, as
soon as the data pattern has been propagated through layer ℓ, its goodness label may be immediately fed in so as to
trigger that layer’s local functional, and the consequent synaptic update, as follows:
∆Wℓ=

2
∂L(zℓ, c)
∂PJℓ
j=1(zℓ
j)2 ⊙zℓ
· (LN(zℓ−1))T .
(62)
Desirably, the above FF plasticity rule facilitates the adjustment of the synapses of any particular neuronal layer that
is independent of the others but, unlike Hebbian rules, requires knowledge across a group of neurons, given that a
layer’s goodness depends on the sum of squares of its set of activity values rather on that of any individual unit.
Notice that the updates computed for a positive or negative data sample may done at any time and independently,
facilitating a flexible asynchronous, variable scheduling of when negative datapoints are used versus positive ones
(though current prominent efforts [162, 319] have focused on simultaneous adjustments based on balanced batches
of positive and negative samples). Finally, it is important to point out that the flexibility of goodness-based (con-
trastive) learning lends itself to an important recurrent generalization that further resolves the locking problems
inherent to backprop-based neural models [162, 319, 313]; concretely, this means the neuronal dynamics can take
on an iterative form (similar to schemes of the previous two credit assignment families):
zℓ(t) = (1 −β)zℓ(t −1) + β

ϕℓ
Wℓ· LN(zℓ−1(t −1)) + Vℓ· LN(zℓ+1(t −1)) −Lℓ· LN(zℓ(t −1))

(63)
which is the generalized form [319] of the model in [162]. The dynamics of Equation 63 define not only bottom-
up and top-down synaptic (local) pressures but also lateral/cross-layer synaptic parameters27; all of which can be
locally adjusted with the same goodness functional Lℓ(zℓ, c). Other self-supervised forward-only schemes either
design local predictive contrastive losses [185] (which further mixes a local prediction of neural activity to produce
a modulatory signal per neuron) or mutual information preservation [249, 253, 465] functionals (with modern in-
spirations such as [404, 104], which mixes local backprop with decoupled, block-local self-supervised functionals).
26Layer normalization has been argued to be an important operation for forward-forward credit assignment as well as in
biology [53], as it, in effect, forces any given layer to use relative activities in judging goodness; the length of a layer of neural
activity vector is corresponds to its goodness while its orientation is information that is passed on to the next layer [162].
27Neurobiologically, the connectivity patterns of these lateral synapses can be designed to model inhibitory and self-excitatory
cross-layer effects and neural competition patterns [319, 313].
33

Preprint
We remark that the principles underlying forward-only learning (e.g., learning without feedback neural circuitry)
have been seen application in temporal credit assignment, such as learning time-varying sequence models of data
as in recurrent networks. Several of these are often based on approximations/formulations of computing gradients
in accordance with forward-mode differentiation [460, 189, 424, 293, 357]. Finally, it is important to point out
that forward-only algorithms, notably signal propagation and the recurrent form FF/PFF, have been generalized to
operate in the context of spiking neural networks [238, 204, 313]. This marks the beginning of an important move to
investigation of such biological credit assignment schemes in the context of energy-efficient hardware; for example,
an instantiation of FF learning has been demonstrated to operate well on an optical (fiber) platform [305].
3
Discussion
We motivated the general organization and direction of this survey with a single important question: where do
the signals that induce credit assignment with respect to the individual processing units of neuronal systems come
from and how are they created? Throughout this review, we have examined six general ways, centering around
different algorithmic “groupings” or “families”, to answer this question: implicit signals, explicit global signals,
non-synergistic explicit local signals, and synergistic explicit local signals in three flavors – discrepancy reduction,
energy-based, and forward-only schemes.
We furthermore began this work with a treatment of backpropagation of errors (backprop), the popular workhorse
algorithm behind the advances of deep learning today, and its core criticisms/central issues (with respect to neuro-
biology as well as practical considerations; see Section 1.3). In Table 1, we mark/annotate the six general credit
assignment families with respect to how well each resolves each of the six prominent problems of backprop, if
they do so at all; problem 5 (P5) is further subdivided into three sub-problems. A ✗denotes that not any of the
algorithms within the family offer anything to resolve//alleviate the given problem, ✔+ indicates the opposite (all
algorithms within the family resolve the problem), and ✓indicates that some, but not all, procedures offer a possible
solution. In addition, we added three columns corresponding to three general properties that would prove most use-
ful for any one family of biologically-plausible credit assignment schemes to have.28 These three properties were:
1) whether the algorithm grouping was architecture-agnostic (it did not require any particular structural forms or
design patterns in the neural architecture it would conduct credit assignment with respect to); 2) whether the credit
assignment family could effectively (in an online manner, without unfolding) handle time-varying data sequences;
and 3) whether the algorithm grouping worked in the context of goal-directed behavior, decision-making, or control.
The final column – ‘All’ – is marked whether or not an item met all eleven criteria. Finally, notice that under each
credit assignment family is a single row examining the one algorithm within it that satisfies the most criteria by
itself:
• Implicit Signals (‘Imp’): two-factor Hebbian adaptation (Hebbian (2F));
• Explicit Global Signals (‘EG’): three-factor Hebbian adaptation (Hebbian (3F));
• Non-Synergistic Explicit Local Signals (‘NSEL’): synthetic local updates (SLU);
• Synergistic Explicit Local Signals, Discrepancy-reduction (‘SEL:DR’): predictive coding/neural genera-
tive coding (PC/NGC);
• Synergistic Explicit Local Signals, Energy-based (‘SEL:EB’): equilibrium propagation (EProp);
• Synergistic Explicit Local Signals, Forward-only (‘SEL:FO’): recurrent forward-forward/predictive
forward-forward (RFF/PFF) learning.
In light of the construction of Table 1, notice that, among all six credit assignment groupings, in aggregate, it is
the forward-only (FO) credit assignment class that meets all eleven criteria, resolving all eight (sub)problems
and meeting three useful generalization properties. However, this promising, positive synthesized result should be
taken with a grain of salt – no one single algorithm within the family resolves all eight problems, with RFF/PFF
learning [162, 319] resolving only seven. When considering all six credit assignment families together, we further
see, in the final row of Table 1, that all eleven criteria (eight problems and three properties) are resolved, indicating
that positive progress has indeed been made despite the challenges facing research efforts in brain-inspired machine
intelligence. In addition, this aggregate result hints that perhaps an important key to future success is in a confluence
of algorithms/families – a grand synergy or hybrid of approaches, where schemes work to compensate for each
other’s weaknesses by leveraging their particular strengths. We remark that comparing and contrasting this diverse
set of credit assignment schemes usefully lends itself to highlighting important gaps that research in the domain
might want to consider in future developments. For instance, we can see that control, which relates to behavioral
28These three properties, by no means, do not make up an exhaustive list of all that might be desired, as this would largely
depend on the research or engineering problem context and its constituent goals.
34

Preprint
Problem to Resolve
Property
All
P1
P2
P3
P4
P5
P6
AA
Time
Ctrl
Approach
LP
FL
UL
Family: Imp
✔+
✔+
✗
✓
✓
✔+
✓
✔+
✓
✓
✗
✗
Hebbian (2F)
✔+
✔+
✗
✗
✔+
N/A
✔+
✔+
✔+
✔+
✗
✗
Family: EG
✓
✔+
✗
✗
✗
✗
✓
✓
✔+
✓
✗
✗
Hebbian (3F)
✔+
✔+
✗
✗
✔+
N/A
✔+
✔+
✔+
✗
✔+
✗
Family: NSEL
✔+
✓
✗
✓
✓
✗
✓
✔+
✔+
✓
✗
✗
SLU
✔+
✗
✗
✗
✔+
✔+
✔+
✔+
✔+
✔+
✗
✗
Family: SEL: DR
✓
✓
✓
✓
✓
✓
✓
✗
✓
✓
✓
✗
PC/NGC
✔+
✔+
✔+
✗
✔+
✔+
✔+
✗
✗
✔+
✔+
✗
Family: SEL: EB
✔+
✓
✓
✓
✔+
✓
✓
✔+
✗
✓
✗
✗
EProp
✔+
✗
✔+
✗
✔+
✔+
✔+
✗
✗
✗
✗
✗
Family: SEL: FO
✔+
✔+
✓
✓
✓
✓
✓
✔+
✔+
✓
✓
✓
RFF/PFF
✔+
✔+
✔+
✗
✔+
✔+
✔+
✔+
✗
✗
✗
✗
All Families
✔+
✔+
✔+
✔+
✔+
✔+
✔+
✔+
✔+
✔+
✔+
✔+
Table 1: What problems central to backprop-based learning (see Section 1.3 for details) do different biologically-inspired
credit assignment (bio-CA) algorithm families resolve? In each row, we consider one complete algorithm family as well as
one procedure within it that satisifies the most criterion. For a family, ✔+ means all schemes within it satisfy the criterion,
✓means that at least one scheme within it clears the criterion, and ✗means that no scheme within it resolves the criterion.
For a single algorithm, ✔+ means that it resolves the criterion directly whereas ✗means that it does not. Problem names are
abbreviated as follows: ‘P1’ = global feedback pathway, ‘P2’ = weight transport, ‘P3’ = short-term plasticity, ‘P4’ = constraint
and sensitivity, ‘P5’ = locality and locking, ‘P6’ = inference-learning dependency. Notice that ‘P5’ has been broken down into
three sub-problems: ‘LP’ = local plasticity, ‘FL’ = forward locking, ‘UL’ = update locking. Three extra criteria are introduced
to determine if a scheme embodies particular properties; ‘AA’ = architecture agnostic?, ‘Time’ = naturally extends to sequences,
‘Ctrl’ = facilitates behavioral/control-based learning.
Note: Some of the family naming labels have been abbreviated to: ‘Imp’ = Implicit (Signals), ‘S’ = Synergistic, ‘NS’ = Non-Synergistic, ‘EG’ = Explicit Global
(Signals), ‘EL’ = Explicit Local (Signals), while ‘DR’ = Discrepancy Reduction, ‘EB’ = Energy-based, and ‘FO’ = Forward-only.
and goal-oriented adaptation (reinforcement learning), poses the greatest difficulty; this is corroborated by the
analysis in Table 2, as only the SEL:DR family has a stronger body of work in the area.29
Another way that we examined biologically-inspired credit assignment (bio-CA) was with respect to the types of
(machine intelligence) contexts that algorithms have been adapted/applied to; the results of this treatment are in
Table 2. Specifically, we grouped related works under each family based on (a non-exhaustive collection of) four
challenging task types that test credit assignment scalability in different ways: ‘Vision’ for evaluating whether any of
a family’s schemes have been generalized to work with convolutional/pooling structures that typically characterize
computer vision models; ‘Time/Graph’ to capture whether adaptations have been made to operate with inherently
time-varying data sequences (e.g., video prediction tasks or language modeling) or relational knowledge model-
ing problems; ‘Control/RL’ to determine if any algorithms in a family handle control/decision-making tasks (e.g.,
reinforcement learning or bandit problems); and ‘Spike’ to specifically focus on whether generalizations of any
algorithms in a group exist for spiking neural networks and neuromorphic architectures. Several patterns emerge
from this investigation. First, many attempts have been to develop generalizations of various credit assignment rou-
tines in the category of ‘Time/Graph’, with the primary bulk of studies focused on temporal modeling tasks, often
attempting to deal with capturing distal correlations in sequences without requiring the unrolling inherent to back-
prop through time (in contrast, very few efforts have engaged with relational/graph problems). Not surprisingly,
many studies have engaged with the scaling of various algorithms to vision problems, likely because the general su-
pervised learning setup remains unchanged, e.g., classification/regression, requiring only changing the underlying
neural transformations, e.g., the use of convolution or pooling-type operations instead of fully-connected synpatic
structures. In terms of control, far less work exists to date in determining how various credit assignment schemes
handle the difficult challenges posed by reinforcement learning (RL) – this might partially relate to the fact that
even current state-of-the-art backprop-based approaches struggle with sample efficiency, requiring many episodes
(or replayed action/event sequences) to uncover useful structure about an agent environment as well as facing diffi-
culty in successfully navigating the exploration-exploitation [421] trade-off central to RL. Thus, adding a typically
more complex, noisier backprop-alternative, such as one of the schemes studied in this survey, would only serve to
complicate an already complicated problem. Very little work in RL/control exists for most credit assignment fam-
ilies except for the SEL:DR family, specifically due to predictive coding as mentioned before, and the EG family,
largely due to three-factor Hebbian plasticity. Finally, and perhaps surprisingly, there is a great deal more work
29We remark that this state-of-affairs is likely a consequence of the work done on predictive coding that exploits its intimate
relationship with the neurobiological process framework known as active inference [115, 334].
35

Preprint
CA Family
Vision
Time/Graph
Control/RL
Spike
Implicit Target
[146, 223, 9]
[121, 120]
[50, 446, 189]
[332]
None
[242, 2, 187]
[105, 240, 51]
Explicit Global Target
[151, 347, 290]
[430]
[244, 151, 226]
[365, 214, 93]
[295]
[187, 379, 221]
[240, 106, 7]
[438, 383, 106]
[59, 336, 130]
Explicit Local Target
Non-Synergistic
[371, 425, 28]
[188, 74, 295]
None
[257, 14, 298]
[299]
Explicit Local Target
Synergistic
Discrepancy-reduction
[24, 471, 326]
[317, 92]
[391, 328, 461]
[262, 320, 191]
[384, 340, 382]
[261, 368]
[318, 132, 324]
[355, 315, 322]
[477, 145]
[314, 329, 275]
[474, 353, 296]
Explicit Local Target
Synergistic
Energy-based
[127, 91, 222]
[303, 235, 236]
[427, 419, 286]
[126, 300, 163]
[196]
[72, 454, 215]
[167, 463, 275]
[481, 297, 266]
Explicit Local Target
Synergistic
Forward-only
[253, 162, 204]
[465, 404, 104]
[414, 162, 331]
[460, 424, 293]
[241]
[313, 204, 238]
[305]
Table 2: Biological Credit Assignment as Applied to Task Settings. Overview of neurobiologically-inspired credit assign-
ment (bio-CA) schemes organized by task setups where they have been utilized/evaluated, i.e., ‘Vision’ refers to scaling up to
problems applied to natural images, ‘Time / Graph’ refers to temporal or relational modeling tasks, ‘Control’ refers to reinforce-
ment learning control tasks, and ‘Spike’ refers to generalizing the bio-CA scheme to adapt spiking neural networks.
with respect to generalization towards the realm of spiking neural networks, arguably one of the most challenging
spaces for application of biological adaptation schemes (despite these schemes seemingly being more of a fit for
neurobiological contexts, at least with respect to neuronal architecture [90, 89]). The greater concentration of more
successful developments in spiking systems include studies in four out of the six families, i.e., Imp, EG, SEL:DR,
and EL:EB. We highlight the importance of this last observation, given that being able to successfully adapt spiking
neuronal systems would have major implications in terms of energy efficiency; neuromorphic chips [75] and related
hardware platforms stand to impact edge-based computing and (neuro)robotics greatly [149, 182].
Taken together, the synthesis of Tables 1 and 2 demonstrates both promise as well as directions for growth. While
biological credit assignment has come a long way since some of its earliest mathematical/algorithmic incarnations
[157, 367], giving rise to the design of computational agents processing various kinds of information sources – rang-
ing from natural images/videos to event-based streams – much work remains to bridge the performance gap between
current state-of-the-art biological approaches to credit assignment and backprop-based DNNs [24]. Furthermore,
more concerted effort will be needed to facilitate the construction of powerful, energy-efficient neuro-mimetic ma-
chines, as desired by the field. One of the domains that we argue might be most important will be the realm of
RL/control, as this often encourages/lends itself to the design of modular neural agent architectures, often ones that
share many similarities with the cognitive architectures typically crafted [11, 224, 402, 315] in Cognitive Science.
Equivalence to Backprop. A notable theoretical direction that has been pursued in service of neuro-mimetic credit
assignment has with respect to convergence properties, comparing the results of particular brain-inspired adaptation
schemes to the capabilities of backprop-trained ANNs [234, 323, 24, 64, 219, 316, 326]. This trend in theoretical
effort, particularly for the case of supervised learning setups, has lead to the establishment of valuable equivalencies
between some approaches in the SEL algorithms, such as PC, and backprop. In effect, this work can be viewed
as establishing that the credit assignment conducted by a non-backprop routine “biologically approximates” the
backprop algorithm, i.e., treating backprop as a sort of gold standard given its modern success in catalyzing the
progress made in DNN-driven systems [229]. Specifically, [452, 285] have shown, under particular conditions
(such as small output error), that PC approximates the weight updates yielded by backprop when training MLPs and
for computational graphs in general; furthermore, this approximate equivalency can be demonstrated if a temporal
scheduling of PC’s weight updates, with respect to the underlying neural activity dynamics, is carefully crafted [408,
382]. Similar equivalencies between biologically-inspired credit assignment and backprop have also established for
other algorithms in the families reviewed in this survey; this notably includes target propagation [228, 232, 30, 234,
31, 283, 368], contrastive Hebbian learning [292, 464], and equilibrium propagation [388, 284, 283]. A key step
underpinning many established theoretical equivalencies is to show that the updates to parameters calculated by a
biological learning process [464, 37, 38, 39, 388, 390] align within 90◦of the direction suggested by the gradients
36

Preprint
computed by backprop [246, 301]. Other results from this line of theoretical research include equivalences between
bio-CA algorithms themselves [309, 325, 389, 390] – for example, showing that predictive coding and contrastive
Hebbian learning (and equilibrium propagation) can be equivalent to one another [283] or target propagation is
(approximately) a form of Gauss-Newton optimization [31, 276]. In [162, 319], relationships were established
between forward-forward learning and contrastive Hebbian learning, contrastive divergence, and predictive coding.
Biological Behavioural versus Physiological Realism.
To lay additional foundation for future progress in
biologically-inspired credit assignment, and in the spirit of arguments put forth in [24, 147], we next highlight
an important consideration for research in the area. In particular, when developing biologically-motivated algo-
rithms and frameworks and studying how they scale, we argue that design and development will need to account
for: 1) the sufficiency of the learning algorithm in question (in terms of a task or class/set of tasks), 2) the impact that
incorporating biological constraints into a neural system has (with respect to task performance or the kinds of poli-
cies uncovered for solving the task), and 3) the necessity of other mechanisms for neural adaptivity (beyond those
that explicitly work in service of credit assignment/plasticity). These three complementary notions indicate a possi-
ble pathway for developing more unified, effective benchmarks for evaluating and contextualizing biological credit
assignment algorithms with respect to one another. Furthermore, these notions shift the focus from a more strict
characterization in terms of and comparison to backprop itself (though this type of comparison will still likely play
an important role in the near future) and towards an examination of the behavior that biological credit assignment
would facilitate when tackling certain problems and task contexts – a particular class of tasks/problems the would
complement this shift, as suggested by our earlier synthesis (Table 1), is that of reinforcement learning and control,
further motivated empirically by the gap we observed in the literature through our synthesis tables (Table 2). This
particular emphasis might prove useful as development in RL often entails crafting agent architectures that have
multiple interacting modules – this would test the flexibility and computational expense of various credit assign-
ment schemes and might further promote hybrid designs that enjoy a heterogeneity of biological credit assignment
processes as opposed to the use of one single scheme as the uniform explanation of learning across neuronal units
within an architecture. Furthermore, tackling RL will usefully leverage the more substantial accumulation of work
in computer vision and temporal sequence learning (as we saw, in Table 2, more efforts have provided contribu-
tions towards these) and notably lead to intersections with important complementary research threads in cognitive
science and neurorobotics. This might also motivate work to fill the gap in relational, graph-centered learning (or
‘knowledge-graph learning’), as we observed there is far less work in this area as compared to temporal modeling.
In complement to the above behavioral/task-family central perspective, a valuable shorter-term direction would
involve cross-family empirical analyses and (simulation) studies as well as further cross-algorithm theoretical es-
tablishments, ideally for more than two credit assignment schemes, similar in spirit to the work done in [283].
Empirical research into algorithmic scalability has so far revealed mixed results, either in the negative [24], or in
the positive [462]. However, very few of the current studies have done proper, comprehensive comparisons of al-
gorithms across families (though a few minor efforts exist [24, 323, 471]); most modern studies typically narrow
down to a more recent, small subset of schemes (with typical choices being those from the feedback alignment
family as benchmark candidates). In essence, studying larger aggregates of algorithms across a greater diversity of
credit assignment families will help in teasing out further strengths and weaknesses inherent to each scheme beyond
their comparison to backprop; this might possibly aid us, if more tightly integrated with results in computational
cognitive neuroscience, in further understanding the intricacies of how brains and natural neuronal networks con-
duct learning, with both physiological and behavioral elements accounted for and evaluated properly. Furthermore,
it will become ever more important to more deeply characterize the failure cases of each algorithm in turn, e.g..,
examining stability and robustness of the learning process [323, 261], as well as how the underlying neural architec-
tural complexity (e.g., number of layers/depth, types of connectivity patterns, design modularity and the resulting
information communications that will be needed) interacts with the efficacy of various credit assignment process.
In addition, investigating the effect of adversarial perturbations has on credit assignment scheme could prove useful
in uncovering further strengths/weaknesses [96].
Towards A Unified Story of Neurobiological Credit Assignment. As we have seen throughout this review,
algorithms within different credit assignment families address issues/criticisms of backprop to varying degrees
while addressing other aspects of neurobiology along the way. As a result, the next question that arises is: what
other criterion need to be addressed by current and future neurobiological-motivated algorithms? Criteria have been
offered [310, 36, 452, 316], but no one clear consensus seems to exist, largely due to the fact that the world of brain-
inspired credit assignment, despite the many efforts across several decades, is still rather “young” in comparison
to other lines/threads of inquiry in machine intelligence research. On the path to developing a complete theory
of learning, credit assignment, and inference, it will be important to develop a more complete set of standards
with which we can fully judge and compare algorithms and/or architectures, with complementary benchmarks
and problems/tasks that rigorously and thoroughly test these criteria. In particular, as discussed above, it will be
37

Preprint
crucial to move beyond supervised learning, specifically classification (on databases such as MNIST, CIFAR-10,
and ImageNet), when designing bio-CA schemes for neuro-mimetic agents as this will require researchers and
engineers to consider biologically-plausible architectural design elements, such as lateral and cross-layer recurrent
connectivity patterns30 and mechanisms for memory storage and knowledge consolidation.
Constructing coherent cognitive architectures that combine one or more of the algorithms from the credit assignment
families reviewed in this work might prove invaluable in achieving the above goals – an important continuance of
the historical aspirations of classical connectionism and parallel distributed processing [373] – where inspiration can
be drawn from early work in neural cognitive architectures such as Leabra [330], CogNGen [315], or Spaun/Nengo
[89]. In complement to this, further results in computational and cognitive neuroscience could aid to shape and
corroborate the principles developed in brain-inspired machine intelligence models, possibly aiding researchers and
theoreticians in crafting a story and spectrum between the higher-level, coarser-grained dynamics simulated by
rate-coded models, learned under different forms of bio-CA, and the finer-grained dynamics underwriting intricate
systems of spiking neuronal cells. Interesting questions arise from this cognitive systems-level premise: how would
a complex dynamical system that combines different credit assignment schemes function and how might it general-
ize differently than systems that use only a single scheme uniformly? Beyond this, where and how should different
credit assignment schemes be applied (in what “artificial brain region” would any specific approach work best)? For
example, SEL:EB schemes, e.g., equilibrium propagation, might be better suited to adaptation in memory modules,
which might operate a slower time-scale of adaptation, whereas SEL:DR, e.g., predictive coding, and EG schemes,
e.g., multi-factor Hebbian plasticity, might be useful for faster processing time-scales. It is known that Hebbian
correlational-learning can usefully constrain task-learning [152, 310], helping bias a neural circuit’s distributed
representations by incorporating additional co-occurrence information that might further improve task-specific per-
formance; for instance, it has been shown that even backprop benefits from being combined with Hebbian update
rules [409] (offering a form of implicit regularization, at the very least).
An emergent pattern or principle that we observed throughout this review of various algorithms is what we label
here as the degree of entanglement between inference and learning. Specifically, this refers to the distinction that is
typically applied between inference, learning, and structure (in this survey’s case, the computational structure or ar-
rangement of neuronal units within a communication network) in machine intelligence. Many algorithms observed
throughout this review, such as those within the discrepancy reduction (predictive coding, target propagation, etc.)
and energy-based (sub-)families (contrastive Hebbian learning and equilibrium propagation), introduce additional
architectural components such as feedback weights, error units, decoding mechanisms, additional cost functionals,
etc. This aspect of biological learning highlights that credit assignment does not need to be regarded as a separate
computational process that “floats” outside of the neuronal processing architecture – the form and arrangement of
the neuronal processing units, as well as the synaptic information message passing pathways connecting them, can
be intertwined with the learning/plasticity process itself. In many instances, by imposing particular architectural
constraints on a system’s neural computation, e.g., local top-down and bottom-up forms of feedback31, simple,
efficient, and local synaptic adjustment rules may be readily employed (as in contrastive Hebbian learning or the
recurrent form of forward-forward learning). This somewhat contradicts the properties that we evaluated credit
assignment schemas for in Table 1 (and possibly works against resolving inference-learning dependency problem);
we remark that, although architectural agnosticism might be a valuable property, providing maximal freedom to the
experimenter or engineer when constructing their model for a given problem, giving up some degree of this freedom
appears to bring with it easy ways of emulating short-term plasticity as well as brain-like forms of parallelism (e.g.,
layer-wise parallelism) in not only the learning process but also in the very inference itself. Take, for example,
predictive coding – such a scheme offers parallelism across layers in both the inference and synaptic adjustment
steps, resolving the forward-locking and upate-locking problems cleanly, due to the message passing induced by
its feedback synaptic structure. Characterizing this entanglement further could prove useful for biological credit
assignment and brain-inspired machine intelligence research. This opens the door to energy-efficient implementa-
tions of asynchronous computation [29, 381]. If our goal is to develop intelligent agents that generalize in the ways
30The interaction between distributed representations and lateral competition, via cross-neuronal excitation/inhibition, is
among the most important known biological mechanisms that could benefit neuro-mimetic computational models the most,
facilitating natural emergent sparsity [308] (fewer neuronal units are active at any time step), which is useful for energy ef-
ficiency, as well as possibly aiding in combating the grand challenge of catastrophic forgetting in ANNs [107]. Models that
compromise between a winner-take-all style approach and full lateral inhibition appear to work well in practice [260, 119],
though much work remains to integrate them properly and completely in probabilistic, neural frameworks.
31The cortex has been shown to feature quite a bit of (top-down-bottom-up) bidirectional connectivity [450, 97] and has been
argued to facilitate constraint-satisfaction processing, where low-level (perceptual) and high-level (conceptual) constraints can
be brought to be bear simultaneously in the context of neuronal processing. This top-down-bottom-up notion has been shown to
explain a variety of cognitive phenomena [271], e.g., higher-level word processing can influence lower-level letter perception,
and has been shown to aid in resolving ambiguity in visual inputs [440].
38

Preprint
that humans do, then computationally formalizing and integrating structural/architecture properties of the human
brain is nearly if not as important as computationally formalizing and integrating properties of the brain’s synaptic
plasticity and adaptation. We note, however, that some particular aspects of architectural agnosticism would likely
remain even in the neural circuitry that features some entanglement of its structure with learning/inference – many
of the credit assignment schemes we reviewed did not require the neuronal activities to be differentiable themselves,
such as in forward-only learning, which could, in heterogeneous hybrid systems, permit the adaptation of black-box
building blocks [162] or ‘sub-circuits’.
Going forward, the process of crafting neurobiological forms of neuronal computation and learning will
need to strike a balance between how closely the details of actual neurobiological circuitry and biochemi-
cal/electrophysiological processes are modeled and how much effort is placed in constructing fast, efficient method-
ology for practical applications. This leads us to ask the following questions: How much neurobiological realism
do we need to sufficiently generalize? How much is too much? If we lean too heavily towards exactly modeling
(neuro)biology, which carries with it tremendous value in computational cognitive neuroscience, we might risk
constructing architectures and learning procedures that are far too slow to simulate, restricting how well we may
be to exploit the unreasonable effectiveness [150] of large-scale datasets and streams. On the other hand, if we
lean too heavily towards building algorithms that are only meant to be fast and handle the demands of industrial
applications, we risk eschewing important neurophysiological mechansims that might be invaluable in allowing our
artificial neural systems to generalize in the right ways, e.g., generalizing from small samples as humans do, gradu-
ally forgetting old task information instead of catastrophically forgetting it, and transferring knowledge across task
domains to work with datasets with little to no human annotations/labels. This balance between biophysical real-
ism/faithfulness will be critical to consider going forward in brain-inspired machine intelligence research and might
possibly span concurrent, complementary lines of inquiry (and related communities of theoreticians, scientists, and
engineers), guided by each thread’s particular scientific directives and questions.
4
Conclusion
A core mission of brain-inspired machine intelligence, and modern initiatives such as “NeuroAI” [359, 337, 470,
288], is to construct a unified theoretical and computational framework for the neural processing and dynamics
that underwrite inference and credit assignment in the brain. Achieving this aspiration would lay the groundwork
for constructing “thinking machines” [437] and artificial general intelligence, based on artificial neural systems,
capable of emulating the complex, energy-efficient behavior of humans and animals while further providing scalable
models useful for computational neuroscience and cognitive science. With the hope of foregrounding research in
biologically-inspired, neuro-mimetic credit assignment, this survey examined and synthesized the scientific efforts,
ranging from the theoretical to the empirical, of countless researchers who have made important contributions to
this grand overarching objective. In essence, the pursuit of biologically-inspired credit assignment entails looking
to neurobiology and cognitive neuroscience for inspiration and a grounding in to how things might operate in the
neuronal circuitry that makes up brains. As presented in this review, this kind of credit assignment can be broken
down into several families or classes centering around answering one of many possible questions: where do the
signals that drive learning, or credit assignment, in neuronal networks come from and how are they produced?
Studying this question resulted in six general families of algorithms and procedures, each embodying different
answers to this central question. We furthermore examined the mechanics of and characterized several prominent
issues/criticisms [141, 479, 73, 311, 154, 438] underlying backpropagation of errors [248, 374] (backprop), the
credit assignment scheme that has served as the catalyst behind the ever-growing set of benchmark-breaking, state-
of-the-art results in modern-day machine intelligence [229]. In light of these, we examined how the current body
of biological credit assignment research resolves these issues/criticisms and furthermore examined what problems
in artificial intelligence these schemes have been applied to.
Based on this review of “backprop-free” biologically-inspired approaches to credit assignment and neural computa-
tion, one might ask which family or which procedure is a possible “master algorithm” [86] that underlies the brain’s
adaptive capabilities? To this question, the answer, as one might suspect based on the synthesis offered by this
work, is that no single scheme or family developed, to date, offers the perfect, viable candidate. Furthermore, no
one scheme completely and resoundingly resolves all of the issues that backprop has been criticized for, especially
when it comes to neuro-physiological plausibility. Nevertheless, the “galaxy of algorithms” that currently exist, of
which we suspect are only but a subset of a much larger space of possibilities, represent promising alternatives to
the very learning algorithm that the domain of deep learning has become so well-accustomed to using. As further
argued in this review, it might be the combination and synchrony of this heterogeneous set of schemes, i.e., in the
form of general neuro-mimetic cognitive architectures [90, 330, 362, 315, 380, 42] further engendering the design
of embodied and enactive agents [112, 113, 334], that might generate new pathways towards far greater energy effi-
39

Preprint
ciency, compatibility and viable integration into neuromoprhic edge-computing and low-energy analog platforms as
well as robotic systems [75, 138, 217, 466], and different, more neurobiologically-motivated forms of generaliza-
tion than what today’s intelligent technological artifacts are capable of. Besides offering mathematical formulations
of “tricks” used by the brain that might benefit current backprop-based machine learning [263], the development
of brain-inspired approaches to machine intelligence might open the door to powerful intelligent computational
tools that not only tackle machine learning tasks that modern-day deep neural networks struggle with but also serve
as useful, credible hypotheses for credit assignment [38] in the brain that could be validated and extended from a
neuroscience and cognitive sciences points-of-view.
Acknowledgments
We would like to thank Alexander Ororbia (Sr.) for reviewing and providing comments for an early draft of this article. Further-
more, we acknowledge the support of the Cisco Research Gift Award #26224.
References
[1] AARTS, E., AND KORST, J. Simulated annealing and boltzmann machines. Wiley, New York (1988).
[2] ABBOTT, L. F., AND NELSON, S. B. Synaptic plasticity: taming the beast. Nature neuroscience 3, 11 (2000), 1178–
1183.
[3] AHAMED, M. A., CHEN, J.,
AND IMRAN, A.-A.-Z.
Forward-forward contrastive learning.
arXiv preprint
arXiv:2305.02927 (2023).
[4] AHMAD, N., VAN GERVEN, M. A., AND AMBROGIONI, L. Gait-prop: A biologically plausible learning rule derived
from backpropagation of error. Advances in Neural Information Processing Systems 33 (2020), 10913–10923.
[5] AKROUT, M., WILSON, C., HUMPHREYS, P. C., LILLICRAP, T., AND TWEED, D. Deep learning without weight
transport. arXiv preprint arXiv:1904.05391 (2019).
[6] ALEMANNO, F., AQUARO, M., KANTER, I., BARRA, A., AND AGLIARI, E. Supervised hebbian learning. Europhysics
Letters 141, 1 (2023), 11001.
[7] ALJADEFF, J., D’AMOUR, J., FIELD, R. E., FROEMKE, R. C., AND CLOPATH, C. Cortical credit assignment by
Hebbian, neuromodulatory and inhibitory plasticity. arXiv:1911.00307 (2019).
[8] ALMEIDA, L. B.
A learning rule for asynchronous perceptrons with feedback in a combinatorial environment.
In
Proceedings, 1st First International Conference on Neural Networks (1987), vol. 2, IEEE, pp. 609–618.
[9] AMATO, G., CARRARA, F., FALCHI, F., GENNARO, C., AND LAGANI, G. Hebbian learning meets deep convolutional
neural networks. In Image Analysis and Processing–ICIAP 2019: 20th International Conference, Trento, Italy, September
9–13, 2019, Proceedings, Part I 20 (2019), Springer, pp. 324–334.
[10] ANDERSEN, T. J., AND WILAMOWSKI, B. M. A. modified regression algorithm for fast one layer neural network
training. In World Congress of Neural Networks (1995), vol. 1, Citeseer, pp. 687–690.
[11] ANDERSON, J. R., AND LEBIERE, C. The atomic components of thought. Lawrence Erlbaum Associates, Mahwah, NJ,
1998.
[12] ANDERSON, J. R., AND PETERSON, C. A mean field theory learning algorithm for neural networks. Complex Systems
1 (1987), 995–1019.
[13] ANDRADE-TALAVERA, Y., FISAHN, A., AND RODRÍGUEZ-MORENO, A. Timing to be precise? an overview of spike
timing-dependent plasticity, brain rhythmicity, and glial cells interplay within neuronal circuits. Molecular Psychiatry
(2023), 1–12.
[14] ANDREW, A. M. Spiking neuron models: Single neurons, populations, plasticity. Kybernetes 32, 7/8 (2003).
[15] ANGELA, J. Y., AND DAYAN, P. Uncertainty, neuromodulation, and attention. Neuron 46, 4 (2005), 681–692.
[16] AVENA-KOENIGSBERGER, A., MISIC, B., AND SPORNS, O. Communication dynamics in complex brain networks.
Nature Reviews Neuroscience 19, 1 (2018), 17–33.
[17] BA, J. L., KIROS, J. R., AND HINTON, G. E. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).
[18] BAHROUN, Y., CHKLOVSKII, D. B., AND SENGUPTA, A. M. Duality principle and biologically plausible learning:
Connecting the representer theorem and hebbian learning. arXiv preprint arXiv:2309.16687 (2023).
[19] BALDI, P., AND PINEDA, F. Contrastive learning and neural oscillations. Neural Computation 3, 4 (1991), 526–545.
[20] BALDI, P., AND SADOWSKI, P. A theory of local learning, the learning channel, and the optimality of backpropagation.
Neural Networks 83 (2016), 51–74.
[21] BALDI, P., SADOWSKI, P., AND LU, Z. Learning in the machine: Random backpropagation and the deep learning
channel. Artificial intelligence 260 (2018), 1–35.
40

Preprint
[22] BALDUZZI, D., VANCHINATHAN, H., AND BUHMANN, J. M. Kickback cuts backprop’s red-tape: Biologically plausible
credit assignment in neural networks. In AAAI (2015), pp. 485–491.
[23] BARTO, A., AND JORDAN, M. Gradient following without back-propagation in layered networks. et-al. Frontiers in
cognitive neuroscience (1992), 443–449.
[24] BARTUNOV, S., SANTORO, A., RICHARDS, B., MARRIS, L., HINTON, G. E., AND LILLICRAP, T. Assessing the
scalability of biologically-motivated deep learning algorithms and architectures. In Advances in Neural Information
Processing Systems (2018), pp. 9390–9400.
[25] BASTOS, A. M., USREY, W. M., ADAMS, R. A., MANGUN, G. R., FRIES, P., AND FRISTON, K. J. Canonical
microcircuits for predictive coding. Neuron 76, 4 (2012), 695–711.
[26] BAYDIN, A. G., PEARLMUTTER, B. A., RADUL, A. A., AND SISKIND, J. M. Automatic differentiation in machine
learning: a survey. Journal of Marchine Learning Research 18 (2018), 1–43.
[27] BELILOVSKY, E., EICKENBERG, M., AND OYALLON, E. Greedy layerwise learning can scale to imagenet. In Interna-
tional conference on machine learning (2019), PMLR, pp. 583–593.
[28] BELILOVSKY, E., EICKENBERG, M., AND OYALLON, E. Decoupled greedy learning of cnns. In International Confer-
ence on Machine Learning (2020), PMLR, pp. 736–745.
[29] BELILOVSKY, E., LECONTE, L., CACCIA, L., EICKENBERG, M., AND OYALLON, E. Decoupled greedy learning of
cnns for synchronous and asynchronous distributed learning. arXiv preprint arXiv:2106.06401 (2021).
[30] BENGIO, Y. How auto-encoders could provide credit assignment in deep networks via target propagation. arXiv preprint
arXiv:1407.7906 (2014).
[31] BENGIO,
Y.
Deriving differential target propagation from iterating approximate inverses.
arXiv preprint
arXiv:2007.15139 (2020).
[32] BENGIO, Y., AND DELALLEAU, O. Justifying and generalizing contrastive divergence. Neural computation 21, 6 (2009),
1601–1621.
[33] BENGIO, Y., AND FISCHER, A. Early inference in energy-based models approximates back-propagation. arXiv preprint
arXiv:1510.02777 (2015).
[34] BENGIO, Y., AND FRASCONI, P. Credit assignment through time: Alternatives to backpropagation. Advances in neural
information processing systems 6 (1993).
[35] BENGIO, Y., LAMBLIN, P., POPOVICI, D., AND LAROCHELLE, H.
Greedy layer-wise training of deep networks.
Advances in neural information processing systems 19 (2006).
[36] BENGIO, Y., LEE, D.-H., BORNSCHEIN, J., MESNARD, T., AND LIN, Z. Towards biologically plausible deep learning.
arXiv preprint arXiv:1502.04156 (2015).
[37] BENGIO, Y., MESNARD, T., FISCHER, A., ZHANG, S., AND WU, Y. Stdp as presynaptic activity times rate of change
of postsynaptic activity. arXiv preprint arXiv:1509.05936 (2015).
[38] BENGIO, Y., MESNARD, T., FISCHER, A., ZHANG, S., AND WU, Y. Stdp-compatible approximation of backpropaga-
tion in an energy-based model. Neural computation 29, 3 (2017), 555–577.
[39] BENGIO, Y., SCELLIER, B., BILANIUK, O., SACRAMENTO, J., AND SENN, W. Feedforward initialization for fast
inference of deep generative networks is biologically plausible. arXiv preprint arXiv:1606.01651 (2016).
[40] BENGIO, Y., SIMARD, P., AND FRASCONI, P. Learning long-term dependencies with gradient descent is difficult. IEEE
transactions on neural networks 5, 2 (1994), 157–166.
[41] BENGIO, Y., YAO, L., ALAIN, G., AND VINCENT, P.
Generalized denoising auto-encoders as generative models.
Advances in neural information processing systems 26 (2013).
[42] BERNÁEZ TIMÓN, L., EKELMANS, P., KRAYNYUKOVA, N., ROSE, T., BUSSE, L., AND TCHUMATCHENKO, T. How
to incorporate biological insights into network models and why it matters. The Journal of Physiology 601, 15 (2023),
3037–3053.
[43] BETTI, A., GORI, M., AND MARRA, G. Backpropagation and biological plausibility. arXiv preprint arXiv:1808.06934
(2018).
[44] BI, G.-Q., AND POO, M.-M. Synaptic modification by correlated activity: Hebb’s postulate revisited. Annual review of
neuroscience 24, 1 (2001), 139–166.
[45] BIENENSTOCK, E. L., COOPER, L. N., AND MUNRO, P. W. Theory for the development of neuron selectivity: orienta-
tion specificity and binocular interaction in visual cortex. Journal of Neuroscience 2, 1 (1982), 32–48.
[46] BOGACZ, R. A tutorial on the free-energy framework for modelling perception and learning. Journal of mathematical
psychology 76 (2017), 198–211.
[47] BOGACZ, R., MORAUD, E. M., ABDI, A., MAGILL, P. J., AND BAUFRETON, J. Properties of neurons in external
globus pallidus can support optimal action selection. PLoS computational biology 12, 7 (2016), e1005004.
41

Preprint
[48] BOGUNA, M., BONAMASSA, I., DE DOMENICO, M., HAVLIN, S., KRIOUKOV, D., AND SERRANO, M. Á. Network
geometry. Nature Reviews Physics 3, 2 (2021), 114–135.
[49] BORNSCHEIN, J., AND BENGIO, Y. Reweighted wake-sleep. arXiv preprint arXiv:1406.2751 (2014).
[50] BRUNEL, N. Hebbian learning of context in recurrent neural networks. Neural computation 8, 8 (1996), 1677–1710.
[51] BRZOSKO, Z., ZANNONE, S., SCHULTZ, W., CLOPATH, C., AND PAULSEN, O. Sequential neuromodulation of hebbian
plasticity offers mechanism for effective reward-based navigation. Elife 6 (2017), e27756.
[52] BUDDEN, D., MARBLESTONE, A., SEZENER, E., LATTIMORE, T., WAYNE, G., AND VENESS, J. Gaussian gated
linear networks. Advances in Neural Information Processing Systems 33 (2020), 16508–16519.
[53] CARANDINI, M., AND HEEGER, D. J. Normalization as a canonical neural computation. Nature Reviews Neuroscience
13, 1 (2012), 51–62.
[54] CARPENTER, G. A., GROSSBERG, S., AND REYNOLDS, J. H. Artmap: Supervised real-time learning and classification
of nonstationary data by a self-organizing neural network. Neural networks 4, 5 (1991), 565–588.
[55] CARPENTER, G. A., GROSSBERG, S., AND ROSEN, D. B. Fuzzy art: Fast stable learning and categorization of analog
patterns by an adaptive resonance system. Neural networks 4, 6 (1991), 759–771.
[56] CARREIRA-PERPIÑÁN, M. Á.,
AND WANG, W.
Distributed optimization of deeply nested systems.
CoRR
abs/1212.5921 (2012).
[57] CHALASANI, R., AND PRINCIPE, J. C. Deep predictive coding networks. arXiv preprint arXiv:1301.3541 (2013).
[58] CHALUP, S., AND MAIRE, F. A study on hill climbing algorithms for neural network training. In Evolutionary Compu-
tation, 1999. CEC 99. Proceedings of the 1999 Congress on (1999), vol. 3, IEEE, pp. 2014–2021.
[59] CHASE, S., SCHWARTZ, A., MAASS, W., AND LEGENSTEIN, R. Functional network reorganization in motor cortex
can be explained by reward-modulated hebbian learning. Advances in neural information processing systems 22 (2009).
[60] CHAVLIS, S., AND POIRAZI, P. Drawing inspiration from biological dendrites to empower artificial neural networks.
Current opinion in neurobiology 70 (2021), 1–10.
[61] CHEN, L., AND AIHARA, K. Chaotic simulated annealing by a neural network model with transient chaos. Neural
networks 8, 6 (1995), 915–930.
[62] CHEUNG, B., AND JIANG, D. The many directions of feedback alignment. In Conference on Cognitive Computational
Neuroscience (2018).
[63] CHO, K. H., RAIKO, T., AND ILIN, A. Gaussian-bernoulli deep boltzmann machine. In The 2013 International Joint
Conference on Neural Networks (IJCNN) (2013), IEEE, pp. 1–7.
[64] CHOROMANSKA, A., COWEN, B., KUMARAVEL, S., LUSS, R., RIGOTTI, M., RISH, I., DIACHILLE, P., GUREV, V.,
KINGSBURY, B., TEJWANI, R., ET AL. Beyond backprop: Online alternating minimization with auxiliary variables. In
International Conference on Machine Learning (2019), PMLR, pp. 1193–1202.
[65] CHU, T., MYKITIUK, K., SZEWCZYK, M., WIKTOR, A., AND WOJNA, Z. Training dnns in o (1) memory with mem-dfa
using random matrices. arXiv preprint arXiv:2012.11745 (2020).
[66] CIREGAN, D., MEIER, U., AND SCHMIDHUBER, J. Multi-column deep neural networks for image classification. In
2012 IEEE conference on computer vision and pattern recognition (2012), IEEE, pp. 3642–3649.
[67] CLARK, A. Whatever next? predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain
Sciences 36, 3 (2013), 181–204.
[68] CLARK, A. Surfing uncertainty: Prediction, action, and the embodied mind. Oxford University Press, 2015.
[69] CLARK, D., ABBOTT, L., AND CHUNG, S. Credit assignment through broadcasting a global error vector. Advances in
Neural Information Processing Systems 34 (2021), 10053–10066.
[70] COOPER, L. N. Theory of cortical plasticity. World Scientific, 2004.
[71] CRAFTON, B., PARIHAR, A., GEBHARDT, E., AND RAYCHOWDHURY, A. Direct feedback alignment with sparse
connections for local learning. Frontiers in neuroscience 13 (2019), 525.
[72] CRAWFORD, D., LEVIT, A., GHADERMARZY, N., OBEROI, J. S., AND RONAGH, P. Reinforcement learning using
quantum boltzmann machines. arXiv preprint arXiv:1612.05695 (2016).
[73] CRICK, F. The recent excitement about neural networks. Nature 337, 6203 (1989), 129–132.
[74] CZARNECKI, W. M., ´SWIRSZCZ, G., JADERBERG, M., OSINDERO, S., VINYALS, O., AND KAVUKCUOGLU, K.
Understanding synthetic gradients and decoupled neural interfaces. In International Conference on Machine Learning
(2017), PMLR, pp. 904–912.
[75] DAVIES, M., SRINIVASA, N., LIN, T.-H., CHINYA, G., CAO, Y., CHODAY, S. H., DIMOU, G., JOSHI, P., IMAM, N.,
JAIN, S., ET AL. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 38, 1 (2018), 82–99.
[76] DAYAN, P. Helmholtz machines and wake-sleep learning. Handbook of Brain Theory and Neural Network. MIT Press,
Cambridge, MA 44, 0 (2000), 1–12.
42

Preprint
[77] DAYAN, P., ABBOTT, L. F., AND ABBOTT, L. Theoretical neuroscience: computational and mathematical modeling of
neural systems. MIT Press (2001).
[78] DAYAN, P., AND HINTON, G. E. Varieties of helmholtz machine. Neural Networks 9, 8 (1996), 1385–1403.
[79] DAYAN, P., HINTON, G. E., NEAL, R. M., AND ZEMEL, R. S. The helmholtz machine. Neural computation 7, 5
(1995), 889–904.
[80] DAYAN, P., AND ZEMEL, R. S. Competition and multiple cause models. Neural Computation 7, 3 (1995), 565–579.
[81] DELLAFERRERA, G., AND KREIMAN, G. Error-driven input modulation: solving the credit assignment problem without
a backward pass. In International Conference on Machine Learning (2022), PMLR, pp. 4937–4955.
[82] DEMPSTER, A. P., LAIRD, N. M., AND RUBIN, D. B. Maximum likelihood from incomplete data via the em algorithm.
Journal of the royal statistical society: series B (methodological) 39, 1 (1977), 1–22.
[83] DESIENO, D. Adding a conscience to competitive learning. In ICNN (1988), vol. 1.
[84] DESJARDINS, G., COURVILLE, A. C., BENGIO, Y., VINCENT, P., AND DELALLEAU, O. Tempered markov chain
monte carlo for training of restricted boltzmann machines. In International Conference on Artificial Intelligence and
Statistics (2010), pp. 145–152.
[85] DETORAKIS, G., BARTLEY, T., AND NEFTCI, E. Contrastive hebbian learning with random feedback weights. Neural
Networks 114 (2019), 1–14.
[86] DOMINGOS, P. The master algorithm: How the quest for the ultimate learning machine will remake our world. Basic
Books, 2015.
[87] DOYA, K. Bayesian brain: Probabilistic approaches to neural coding. MIT press, 2007.
[88] DRAZIN, P. G. Nonlinear systems. Cambridge University Press, 1992.
[89] ELIASMITH, C. How to build a brain: A neural architecture for biological cognition. Oxford University Press, 2013.
[90] ELIASMITH, C., STEWART, T. C., CHOO, X., BEKOLAY, T., DEWOLF, T., TANG, Y., AND RASMUSSEN, D. A
large-scale model of the functioning brain. science 338, 6111 (2012), 1202–1205.
[91] ERNOULT, M., GROLLIER, J., QUERLIOZ, D., BENGIO, Y., AND SCELLIER, B. Continual weight updates and convo-
lutional architectures for equilibrium propagation. arXiv preprint arXiv:2005.04169 (2020).
[92] ERNOULT, M. M., NORMANDIN, F., MOUDGIL, A., SPINNEY, S., BELILOVSKY, E., RISH, I., RICHARDS, B., AND
BENGIO, Y. Towards scaling difference target propagation by learning backprop targets. In International Conference on
Machine Learning (2022), PMLR, pp. 5968–5987.
[93] EVANUSA, M., FERMÜLLER, C., AND ALOIMONOS, Y. Deep reservoir networks with learned hidden reservoir weights
using direct feedback alignment. arXiv preprint arXiv:2010.06209 (2020).
[94] FAHLMAN, S. E.
Faster learning variations of back propagation: An empirical study. In Proceedings of the 1988
connectionist models summer school (1988), Morgan Kaufmann, pp. 38–51.
[95] FARAHANI, F. V., KARWOWSKI, W., AND LIGHTHALL, N. R. Application of graph theory for identifying connectivity
patterns in human brain networks: a systematic review. frontiers in Neuroscience 13 (2019), 585.
[96] FARINHA, M. T., ORTNER, T., DELLAFERRERA, G., GREWE, B., AND PANTAZI, A. Efficient biologically plausible
adversarial training. arXiv preprint arXiv:2309.17348 (2023).
[97] FELLEMAN, D. J., AND VAN, D. E. Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex
(New York, NY: 1991) 1, 1 (1991), 1–47.
[98] FIETE, I. R., AND SEUNG, H. S. Gradient learning in spiking neural networks by dynamic perturbation of conductances.
Physical review letters 97, 4 (2006), 048104.
[99] FISER, A., MAHRINGER, D., OYIBO, H. K., PETERSEN, A. V., LEINWEBER, M., AND KELLER, G. B. Experience-
dependent spatial expectations in mouse visual cortex. Nature neuroscience 19, 12 (2016), 1658.
[100] FLOREANO, D., DÜRR, P., AND MATTIUSSI, C. Neuroevolution: from architectures to learning. Evolutionary intelli-
gence 1 (2008), 47–62.
[101] FLÜGEL, K., COQUELIN, D., WEIEL, M., DEBUS, C., STREIT, A., AND GÖTZ, M. Feed-forward optimization with
delayed feedback for neural networks. arXiv preprint arXiv:2304.13372 (2023).
[102] FÖLDIAK, P. Forming sparse representations by local anti-hebbian learning. Biological cybernetics 64, 2 (1990), 165–
170.
[103] FORGY, E. W. Cluster analysis of multivariate data: efficiency versus interpretability of classifications. biometrics 21
(1965), 768–769.
[104] FOURNIER, L., PATEL, A., EICKENBERG, M., OYALLON, E., AND BELILOVSKY, E. Preventing dimensional collapse
in contrastive local learning with subsampling. In ICML 2023 Workshop on Localized Learning (LLW) (2023).
[105] FRÉMAUX, N., AND GERSTNER, W. Neuromodulated spike-timing-dependent plasticity, and theory of three-factor
learning rules. Frontiers in neural circuits 9 (2016), 85.
43

Preprint
[106] FRÉMAUX, N., SPREKELER, H., AND GERSTNER, W. Functional requirements for reward-modulated spike-timing-
dependent plasticity. Journal of Neuroscience 30, 40 (2010), 13326–13337.
[107] FRENCH, R. M. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3, 4 (1999), 128–135.
[108] FRENKEL, C., LEFEBVRE, M., AND BOL, D. Learning without feedback: Fixed random learning signals allow for
feedforward training of deep neural networks. Frontiers in neuroscience 15 (2021), 629892.
[109] FRENKEL, C., LEFEBVRE, M., AND BOL, D. Learning without feedback: Fixed random learning signals allow for
feedforward training of deep neural networks. Frontiers in neuroscience 15 (2021), 629892.
[110] FRISTON, K. Learning and inference in the brain. Neural Networks 16, 9 (2003), 1325–1352.
[111] FRISTON, K. A theory of cortical responses. Philosophical Transactions of the Royal Society of London B: Biological
Sciences 360, 1456 (2005), 815–836.
[112] FRISTON, K. Hierarchical models in the brain. PLoS Computational Biology (2008).
[113] FRISTON, K. The free-energy principle: a unified brain theory? Nature reviews neuroscience 11, 2 (2010), 127.
[114] FRISTON, K., STEPHAN, K., LI, B., DAUNIZEAU, J., ET AL. Generalised filtering. Mathematical Problems in Engi-
neering 2010 (2010).
[115] FRISTON, K. J., PARR, T., AND DE VRIES, B. The graphical brain: belief propagation and active inference. Network
Neuroscience 1, 4 (2017), 381–414.
[116] FRISTON, K. J., TRUJILLO-BARRETO, N., AND DAUNIZEAU, J. DEM: A variational treatment of dynamic systems.
Neuroimage 41, 3 (2008), 849–885.
[117] FRITZKE, B., ET AL. A growing neural gas network learns topologies. Advances in neural information processing
systems 7 (1995), 625–632.
[118] FROEMKE, R. C., MERZENICH, M. M., AND SCHREINER, C. E. A synaptic memory trace for cortical receptive field
plasticity. Nature 450, 7168 (2007), 425–429.
[119] FUKAI, T., AND TANAKA, S. A simple neural network exhibiting selective activation of neuronal ensembles: from
winner-take-all to winners-share-all. Neural computation 9, 1 (1997), 77–97.
[120] FUKUSHIMA, K. Neocognitron: A hierarchical neural network capable of visual pattern recognition. Neural networks 1,
2 (1988), 119–130.
[121] FUKUSHIMA, K., AND MIYAKE, S. Neocognitron: A new algorithm for pattern recognition tolerant of deformations
and shifts in position. Pattern recognition 15, 6 (1982), 455–469.
[122] FURAO, S., OGURA, T., AND HASEGAWA, O. An enhanced self-organizing incremental neural network for online
unsupervised learning. Neural Networks 20, 8 (2007), 893–903.
[123] G., A., KE, N., GANGULI, S., AND BENGIO, Y. Variational walkback: Learning a transition operator as a stochastic
recurrent net. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4392–4402.
[124] GALLAND, C. C., AND HINTON, G. E. Deterministic boltzmann learning in networks with asymmetric connectivity. In
Connectionist Models. Elsevier, 1991, pp. 3–9.
[125] GALVÁN, E., AND MOONEY, P. Neuroevolution in deep neural networks: Current trends and future challenges. IEEE
Transactions on Artificial Intelligence 2, 6 (2021), 476–493.
[126] GAN, Z., LI, C., HENAO, R., CARLSON, D. E., AND CARIN, L. Deep temporal sigmoid belief networks for sequence
modeling. Advances in Neural Information Processing Systems 28 (2015).
[127] GAO, J., YANG, J., WANG, G., AND LI, M. A novel feature extraction method for scene recognition based on centered
convolutional restricted boltzmann machines. Neurocomputing 214 (2016), 708–717.
[128] GELFAND, A. E. Gibbs sampling. Journal of the American statistical Association 95, 452 (2000), 1300–1304.
[129] GERSTNER, W., AND KISTLER, W. M. Mathematical formulations of hebbian learning. Biological cybernetics 87, 5-6
(2002), 404–415.
[130] GERSTNER, W., LEHMANN, M., LIAKONI, V., CORNEIL, D., AND BREA, J.
Eligibility traces and plasticity on
behavioral time scales: experimental support of neohebbian three-factor learning rules. Frontiers in neural circuits 12
(2018), 53.
[131] GIAMPAOLO, F., IZZO, S., PREZIOSO, E., AND PICCIALLI, F. Investigating random variations of the forward-forward
algorithm for training neural networks. In 2023 International Joint Conference on Neural Networks (IJCNN) (2023),
IEEE, pp. 1–7.
[132] GKLEZAKOS, D. C., AND RAO, R. P. N. Active predictive coding networks: A neural solution to the problem of learning
reference frames and part-whole hierarchies. arXiv preprint arXiv:2201.08813 (2022).
[133] GLIOZZI, V., AND PLUNKETT, K. Self-organizing maps and generalization: an algorithmic description of numerosity
and variability effects. arXiv preprint arXiv:1802.09442 (2018).
44

Preprint
[134] GLOROT, X., AND BENGIO, Y. Understanding the difficulty of training deep feedforward neural networks. In Proceed-
ings of the thirteenth international conference on artificial intelligence and statistics (2010), pp. 249–256.
[135] GOMEZ, A. N., REN, M., URTASUN, R., AND GROSSE, R. B. The reversible residual network: Backpropagation
without storing activations. Advances in neural information processing systems 30 (2017).
[136] GRAY, R. Vector quantization. IEEE Assp Magazine 1, 2 (1984), 4–29.
[137] GRINBERG, L., HOPFIELD, J., AND KROTOV, D. Local unsupervised learning for image analysis. arXiv preprint
arXiv:1908.08993 (2019).
[138] GROLLIER, J., QUERLIOZ, D., CAMSARI, K., EVERSCHOR-SITTE, K., FUKAMI, S., AND STILES, M. D. Neuromor-
phic spintronics. Nature electronics 3, 7 (2020), 360–370.
[139] GROSSBERG, S. Embedding fields: A theory of learning with physiological implications. Journal of Mathematical
Psychology 6, 2 (1969), 209–239.
[140] GROSSBERG, S. How does a brain build a cognitive code? In Studies of mind and brain. Springer, 1982, pp. 1–52.
[141] GROSSBERG, S. Competitive learning: From interactive activation to adaptive resonance. Cognitive science 11, 1 (1987),
23–63.
[142] GROSSBERG, S. Competitive learning: From interactive activation to adaptive resonance. Cognitive Science 11, 1 (1987),
23 – 63.
[143] GROSSBERG, S. Adaptive resonance theory: How a brain learns to consciously attend, learn, and recognize a changing
world. Neural networks 37 (2013), 1–47.
[144] GUERGUIEV, J., LILLICRAP, T. P., AND RICHARDS, B. A. Towards deep learning with segregated dendrites. ELife 6
(2017), e22901.
[145] GUPTA, D., MIHUCZ, G., SCHLEGEL, M., KOSTAS, J., THOMAS, P. S., AND WHITE, M. Structural credit assignment
in neural networks using reinforcement learning. Advances in Neural Information Processing Systems 34 (2021), 30257–
30270.
[146] GUPTA, M., AMBIKAPATHI, A., AND RAMASAMY, S. Hebbnet: A simplified hebbian learning framework to do bi-
ologically plausible learning. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) (2021), IEEE, pp. 3115–3119.
[147] GUPTA, M., MODI, S. K., ZHANG, H., LEE, J. H., AND LIM, J. H. Is bio-inspired learning better than backprop?
benchmarking bio learning vs. backprop. arXiv preprint arXiv:2212.04614 (2022).
[148] GUTMANN, M., AND HYVÄRINEN, A. Noise-contrastive estimation: A new estimation principle for unnormalized
statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (2010),
JMLR Workshop and Conference Proceedings, pp. 297–304.
[149] HAGRAS, H., POUNDS-CORNISH, A., COLLEY, M., CALLAGHAN, V., AND CLARKE, G. Evolving spiking neu-
ral network controllers for autonomous robots. In IEEE International Conference on Robotics and Automation, 2004.
Proceedings. ICRA’04. 2004 (2004), vol. 5, IEEE, pp. 4620–4626.
[150] HALEVY, A., NORVIG, P., AND PEREIRA, F. The unreasonable effectiveness of data. IEEE intelligent systems 24, 2
(2009), 8–12.
[151] HAN, D., PARK, G., RYU, J., AND YOO, H.-J. Extension of direct feedback alignment to convolutional and recurrent
neural network for bio-plausible deep learning. arXiv preprint arXiv:2006.12830 (2020).
[152] HANCOCK, P. J., SMITH, L. S., AND PHILLIPS, W. A. A biologically supported error-correcting learning rule. Neural
Computation 3, 2 (1991), 201–212.
[153] HANSON, S. J. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena 42, 1-3 (1990), 265–272.
[154] HARRIS, K. D. Stability of the fittest: organizing learning through retroaxonal signals. Trends in neurosciences 31, 3
(2008), 130–136.
[155] HE, J., TAN, A.-H., AND TAN, C.-L. Modified art 2a growing network capable of generating a fixed number of nodes.
IEEE Transactions on Neural Networks 15, 3 (2004), 728–737.
[156] HE, K., ZHANG, X., REN, S., AND SUN, J. Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification. In Proceedings of the IEEE international conference on computer vision (2015), pp. 1026–1034.
[157] HEBB, D. O., ET AL. The organization of behavior, 1949.
[158] HECHT-NIELSEN, R. Counterpropagation networks. Applied optics 26 23 (1987), 4979–83.
[159] HEINZ, A. P. Pipelined neural tree learning by error forward-propagation. In Proceedings of ICNN’95-International
Conference on Neural Networks (1995), vol. 1, IEEE, pp. 394–397.
[160] HERTZ, J., KROGH, A., LAUTRUP, B., AND LEHMANN, T. Nonlinear backpropagation: doing backpropagation without
derivatives of the activation function. IEEE Transactions on neural networks 8, 6 (1997), 1321–1327.
[161] HINTON, G. The ups and downs of hebb synapses. Canadian Psychology/Psychologie canadienne 44, 1 (2003), 10.
45

Preprint
[162] HINTON, G. The forward-forward algorithm: Some preliminary investigations. arXiv preprint arXiv:2212.13345 (2022).
[163] HINTON, G., DAYAN, P., TO, A., AND NEAL, R. The helmholtz machine through time. In ICANN-95 (1995), pp. 483–
490.
[164] HINTON, G. E. Deterministic boltzmann learning performs steepest descent in weight-space. Neural computation 1, 1
(1989), 143–150.
[165] HINTON, G. E. Training products of experts by minimizing contrastive divergence. Neural computation 14, 8 (2002),
1771–1800.
[166] HINTON, G. E. A practical guide to training restricted boltzmann machines. In Neural Networks: Tricks of the Trade:
Second Edition. Springer, 2012, pp. 599–619.
[167] HINTON, G. E., AND BROWN, A. Spiking boltzmann machines. Advances in neural information processing systems 12
(1999).
[168] HINTON, G. E., DAYAN, P., FREY, B. J., AND NEAL, R. M. The" wake-sleep" algorithm for unsupervised neural
networks. Science 268, 5214 (1995), 1158.
[169] HINTON, G. E., AND MCCLELLAND, J. L. Learning representations by recirculation. In Neural information processing
systems (1988), pp. 358–366.
[170] HINTON, G. E., OSINDERO, S., AND TEH, Y. W. A fast learning algorithm for deep belief nets. Neural Computation
18 (2006), 1527–1554.
[171] HINTON, G. E., AND SALAKHUTDINOV, R. R. A better way to pretrain deep boltzmann machines. Advances in Neural
Information Processing Systems 25 (2012).
[172] HINTON, G. E., SEJNOWSKI, T. J., ET AL.
Learning and relearning in boltzmann machines. Parallel distributed
processing: Explorations in the microstructure of cognition 1 (1986), 282–317.
[173] HINTON, G. E., AND ZEMEL, R. Autoencoders, minimum description length and helmholtz free energy. Advances in
neural information processing systems 6 (1993).
[174] HINTZMAN, D. L. Minerva 2: A simulation model of human memory. Behavior Research Methods, Instruments, &
Computers 16, 2 (1984), 96–101.
[175] HIRASAWA, K., OHBAYASHI, M., KOGA, M., AND HARADA, M. Forward propagation universal learning network. In
Proceedings of international conference on neural networks (ICNN’96) (1996), vol. 1, IEEE, pp. 353–358.
[176] HIRATANI, N., MEHTA, Y., LILLICRAP, T., AND LATHAM, P. E. On the stability and scalability of node perturbation
learning. Advances in Neural Information Processing Systems 35 (2022), 31929–31941.
[177] HOCHREITER, S., AND SCHMIDHUBER, J. Long short-term memory. Neural computation 9, 8 (1997).
[178] HOPFIELD, J. J. Neurons with graded response have collective computational properties like those of two-state neurons.
Proceedings of the national academy of sciences 81, 10 (1984), 3088–3092.
[179] HOPFIELD, J. J., FEINSTEIN, D., AND PALMER, R. ‘unlearning’has a stabilizing effect in collective memories. Nature
304, 5922 (1983), 158.
[180] HU, W., XIAO, L., AND PENNINGTON, J. Provable benefit of orthogonal initialization in optimizing deep linear net-
works. arXiv preprint arXiv:2001.05992 (2020).
[181] HUANG, Y., AND RAO, R. P. Predictive coding. Wiley Interdisciplinary Reviews: Cognitive Science 2, 5 (2011), 580–
593.
[182] HWU, T., ISBELL, J., OROS, N., AND KRICHMAR, J. A self-driving robot using deep convolutional neural networks on
neuromorphic hardware. In 2017 International Joint Conference on Neural Networks (IJCNN) (2017), IEEE, pp. 635–
641.
[183] HYVÄRINEN, A., AND OJA, E. Independent component analysis by general nonlinear hebbian-like learning rules. signal
processing 64, 3 (1998), 301–313.
[184] IGEL, C. Neuroevolution for reinforcement learning using evolution strategies. In The 2003 Congress on Evolutionary
Computation, 2003. CEC’03. (2003), vol. 4, IEEE, pp. 2588–2595.
[185] ILLING, B., VENTURA, J., BELLEC, G., AND GERSTNER, W. Local plasticity rules can learn deep representations using
self-supervised contrastive predictions. Advances in Neural Information Processing Systems 34 (2021), 30365–30379.
[186] IOFFE, S., AND SZEGEDY, C. Batch normalization: Accelerating deep network training by reducing internal covariate
shift. In International conference on machine learning (2015), pp. 448–456.
[187] IZHIKEVICH, E. M. Solving the distal reward problem through linkage of stdp and dopamine signaling. Cerebral cortex
17, 10 (2007), 2443–2452.
[188] JADERBERG, M., CZARNECKI, W. M., OSINDERO, S., VINYALS, O., GRAVES, A., AND KAVUKCUOGLU, K. Decou-
pled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343 (2016).
[189] JAEGER, H. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the" echo state network"
approach, vol. 5. GMD-Forschungszentrum Informationstechnik Bonn, 2002.
46

Preprint
[190] JAISWAL, A., WANG, P., CHEN, T., ROUSSEAU, J. F., DING, Y., AND WANG, Z. Old can be gold: Better gradient flow
can make vanilla-gcns great again. arXiv preprint arXiv:2210.08122 (2022).
[191] JIANG, L. P., AND RAO, R. P. N. Dynamic predictive coding: A new model of hierarchical sequence learning and
prediction in the cortex. bioRxiv (2022), 2022–06.
[192] JOURNÉ, A., RODRIGUEZ, H. G., GUO, Q., AND MORAITIS, T. Hebbian deep learning without feedback. arXiv
preprint arXiv:2209.11883 (2022).
[193] KAISER, J., MOSTAFA, H., AND NEFTCI, E. Synaptic plasticity dynamics for deep continuous local learning (decolle).
Frontiers in Neuroscience 14 (2020), 424.
[194] KANERVA, P. Sparse distributed memory. MIT press, 1988.
[195] KAPPEL, D., NAZEER, K. K., FOKAM, C. T., MAYR, C., AND SUBRAMONEY, A. Block-local learning with proba-
bilistic latent representations. arXiv preprint arXiv:2305.14974 (2023).
[196] KENDALL, J., PANTONE, R., MANICKAVASAGAM, K., BENGIO, Y., AND SCELLIER, B. Training end-to-end analog
neural networks with equilibrium propagation. arXiv:2006.01981 (2020).
[197] KERMICHE, N. Contrastive hebbian feedforward learning for neural networks. IEEE transactions on neural networks
and learning systems 31, 6 (2019), 2118–2128.
[198] KINGMA, D., AND BA, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[199] KINGMA, D. P., AND WELLING, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).
[200] KIRBY, K. G. A tutorial on helmholtz machines. Department of Computer Science, Northern Kentucky University (2006).
[201] KIRKPATRICK, S., GELATT JR, C. D., AND VECCHI, M. P. Optimization by simulated annealing. science 220, 4598
(1983), 671–680.
[202] KIRSCH, L., AND SCHMIDHUBER, J. Meta learning backpropagation and improving it. Advances in Neural Information
Processing Systems 34 (2021), 14122–14134.
[203] KOCH, G., PONZO, V., DI LORENZO, F., CALTAGIRONE, C., AND VENIERO, D. Hebbian and anti-hebbian spike-
timing-dependent plasticity of human cortico-cortical connections. Journal of Neuroscience 33, 23 (2013), 9725–9733.
[204] KOHAN, A., RIETMAN, E. A., AND SIEGELMANN, H. T. Signal propagation: The framework for learning and inference
in a forward pass. IEEE Transactions on Neural Networks and Learning Systems (2023).
[205] KOHAN, A. A., RIETMAN, E. A., AND SIEGELMANN, H. T. Error forward-propagation: Reusing feedforward connec-
tions to propagate errors in deep learning. arXiv preprint arXiv:1808.03357 (2018).
[206] KOHONEN, T. The’neural’phonetic typewriter. Computer 21, 3 (1988), 11–22.
[207] KOHONEN, T. The self-organizing map. Proceedings of the IEEE 78, 9 (1990), 1464–1480.
[208] KOHONEN, T. Learning vector quantization. In Self-organizing maps. Springer, 1995, pp. 175–189.
[209] KOLEN, J., AND POLLACK, J. Back propagation is sensitive to initial conditions. Advances in Neural Information
Processing Systems 3 (1990).
[210] KÖRDING, K. P., AND KÖNIG, P. Learning with two sites of synaptic integration. Network: Computation in neural
systems 11, 1 (2000), 25–39.
[211] KÖRDING, K. P., AND KÖNIG, P. Supervised and unsupervised learning with two sites of synaptic integration. Journal
of computational neuroscience 11 (2001), 207–215.
[212] KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. ImageNet classification with deep convolutional neural net-
works. In 26th Annual Conference on Neural Information Processing Systems (NIPS) 2012 (2012).
[213] KROTOV, D., AND HOPFIELD, J. Unsupervised learning by competing hidden units. arXiv preprint arXiv:1806.10181
(2018).
[214] KRUIJNE, W., BOHTE, S. M., ROELFSEMA, P. R., AND OLIVERS, C. N. Flexible working memory through selective
gating and attentional tagging. Neural Computation 33, 1 (2021), 1–40.
[215] KUBO, Y., CHALMERS, E., AND LUCZAK, A. Combining backpropagation with equilibrium propagation to improve an
actor-critic reinforcement learning framework. Frontiers in Computational Neuroscience 16 (2022), 980613.
[216] KULLBACK, S. Information theory and statistics. Courier Corporation, 1997.
[217] KUMAR, S., WANG, X., STRACHAN, J. P., YANG, Y., AND LU, W. D. Dynamical memristors for higher-complexity
neuromorphic computing. Nature Reviews Materials 7, 7 (2022), 575–591.
[218] KUMAR, S. K. On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863 (2017).
[219] KUNIN, D., NAYEBI, A., SAGASTUY-BRENA, J., GANGULI, S., BLOOM, J., AND YAMINS, D. Two routes to scalable
credit assignment without weight symmetry. In International Conference on Machine Learning (2020), PMLR, pp. 5511–
5521.
47

Preprint
[220] KURISCAK, E., MARSALEK, P., STROFFEK, J., AND TOTH, P. G. Biological context of hebb learning in artificial neural
networks, a review. Neurocomputing 152 (2015), 27–35.
[221] KU ´SMIERZ, Ł., ISOMURA, T., AND TOYOIZUMI, T. Learning with three factors: modulating hebbian plasticity with
errors. Current opinion in neurobiology 46 (2017), 170–177.
[222] LABORIEUX, A., ERNOULT, M., SCELLIER, B., BENGIO, Y., GROLLIER, J., AND QUERLIOZ, D. Scaling equilibrium
propagation to deep convnets by drastically reducing its gradient estimator bias. Frontiers in neuroscience 15 (2021),
129.
[223] LAGANI, G., FALCHI, F., GENNARO, C., AND AMATO, G. Comparing the performance of hebbian against backpropa-
gation learning using convolutional neural networks. Neural Computing and Applications 34, 8 (2022), 6503–6519.
[224] LAIRD, J. E. The Soar Cognitive Architecture. MIT Press, Cambridge, MA, 2012.
[225] LANSDELL, B., PRAKASH, P., AND KORDING, K. Learning to solve the credit assignment problem. arXiv preprint
arXiv:1906.00889 (2019).
[226] LAUNAY, J., POLI, I., BONIFACE, F., AND KRZAKALA, F. Direct feedback alignment scales to modern deep learning
tasks and architectures. Advances in neural information processing systems 33 (2020), 9346–9360.
[227] LAUNAY, J., POLI, I., AND KRZAKALA, F. Principled training of neural networks with direct feedback alignment. arXiv
preprint arXiv:1906.04554 (2019).
[228] LE CUN, Y. Learning process in an asymmetric threshold network. In Disordered systems and biological organization.
Springer, 1986, pp. 233–240.
[229] LECUN, Y., BENGIO, Y., AND HINTON, G. Deep learning. nature 521, 7553 (2015), 436–444.
[230] LECUN, Y., BOTTOU, L., ORR, G. B., AND MÜLLER, K.-R. Efficient backprop. In Neural networks: Tricks of the
trade. Springer, 2002, pp. 9–50.
[231] LECUN, Y., CHOPRA, S., HADSELL, R., RANZATO, M., AND HUANG, F. A tutorial on energy-based learning. Pre-
dicting structured data 1, 0 (2006).
[232] LECUN, Y., AND DENKER, J. A new learning rule for recurrent networks. In Proceedings of of the Conference on Neural
Networks for Computing (Snowbird, UT) (1991).
[233] LEE, C.-Y., XIE, S., GALLAGHER, P., ZHANG, Z., AND TU, Z. Deeply-Supervised Nets. arXiv:1409.5185 [cs, stat]
(2014).
[234] LEE, D.-H., ZHANG, S., FISCHER, A., AND BENGIO, Y. Difference target propagation. In Joint European Conference
on Machine Learning and Knowledge Discovery in Databases (2015), Springer, pp. 498–515.
[235] LEE, H., GROSSE, R., RANGANATH, R., AND NG, A. Y. Convolutional deep belief networks for scalable unsupervised
learning of hierarchical representations. In Proceedings of the 26th annual international conference on machine learning
(2009), pp. 609–616.
[236] LEE, H., GROSSE, R., RANGANATH, R., AND NG, A. Y. Unsupervised learning of hierarchical representations with
convolutional deep belief networks. Communications of the ACM 54, 10 (2011), 95–103.
[237] LEE, H.-C., AND SONG, J. Symba: Symmetric backpropagation-free contrastive learning with forward-forward algo-
rithm for optimizing convergence. arXiv preprint arXiv:2303.08418 (2023).
[238] LEE, J. H., HAGHIGHATSHOAR, S., AND KARBASI, A. Exact gradient computation for spiking neural networks via
forward propagation. In International Conference on Artificial Intelligence and Statistics (2023), PMLR, pp. 1812–1831.
[239] LEE, T. S., AND MUMFORD, D. Hierarchical Bayesian inference in the visual cortex. JOSA A 20, 7 (2003), 1434–1448.
[240] LEGENSTEIN, R., PECEVSKI, D., AND MAASS, W. A learning theory for reward-modulated spike-timing-dependent
plasticity with application to biofeedback. PLoS computational biology 4, 10 (2008), e1000180.
[241] LEMMEL, J., AND GROSU, R. Real-time recurrent reinforcement learning. arXiv preprint arXiv:2311.04830 (2023).
[242] LEVY, W., AND STEWARD, O. Temporal contiguity requirements for long-term associative potentiation/depression in
the hippocampus. Neuroscience 8, 4 (1983), 791–797.
[243] LIAO, Q., LEIBO, J. Z., AND POGGIO, T. A. How important is weight symmetry in backpropagation? In AAAI (2016),
pp. 1837–1844.
[244] LIAO, Q., AND POGGIO, T. Bridging the gaps between residual learning, recurrent neural networks and visual cortex.
arXiv preprint arXiv:1604.03640 (2016).
[245] LIAO, R., XIONG, Y., FETAYA, E., ZHANG, L., YOON, K., PITKOW, X., URTASUN, R., AND ZEMEL, R. Reviving
and improving recurrent back-propagation. arXiv preprint arXiv:1803.06396 (2018).
[246] LILLICRAP, T. P., COWNDEN, D., TWEED, D. B., AND AKERMAN, C. J. Random synaptic feedback weights support
error backpropagation for deep learning. Nature communications 7 (2016), 13276.
[247] LILLICRAP, T. P., SANTORO, A., MARRIS, L., AKERMAN, C. J., AND HINTON, G. Backpropagation and the brain.
Nature Reviews Neuroscience (2020).
48

Preprint
[248] LINNAINMAA, S. The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local
rounding errors. Master’s Thesis (in Finnish), Univ. Helsinki (1970), 6–7.
[249] LINSKER, R. Self-organization in a perceptual network. Computer 21, 3 (1988), 105–117.
[250] LLOYD, S. Least squares quantization in pcm. IEEE transactions on information theory 28, 2 (1982), 129–137.
[251] LOEWENSTEIN, Y. Robustness of learning that is based on covariance-driven synaptic plasticity. PLoS Computational
Biology 4, 3 (2008), e1000007.
[252] LOTTER, W., KREIMAN, G., AND COX, D. Deep predictive coding networks for video prediction and unsupervised
learning. arXiv preprint arXiv:1605.08104 (2016).
[253] LÖWE, S., O’CONNOR, P., AND VEELING, B. Putting an end to end-to-end: Gradient-isolated learning of representa-
tions. Advances in neural information processing systems 32 (2019).
[254] LÖWEL, S., AND SINGER, W. Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal
activity. Science 255, 5041 (1992), 209–212.
[255] LUTTRELL, S. Self-supervised adaptive networks. IEE PROCEEDINGS PART F RADAR AND SIGNAL PROCESSING
139 (1992), 371–371.
[256] LUTTRELL, S. P. A bayesian analysis of self-organizing maps. Neural Computation 6, 5 (1994), 767–794.
[257] MA, C., YAN, R., YU, Z., AND YU, Q. Deep spike learning with local classifiers. IEEE Transactions on Cybernetics
(2022).
[258] MA, W.-D. K., LEWIS, J. P., AND KLEIJN, W. B. The hsic bottleneck: Deep learning without back-propagation, 2019.
[259] MAASS, W. Networks of spiking neurons: the third generation of neural network models. Neural networks 10, 9 (1997),
1659–1671.
[260] MAJANI, E., ERLARSON, R., AND ABU-MOSTAFA, Y. The induction of multiscale temporal structure. Advances in
neural information processing systems 1 (1989), 634–642.
[261] MALI, A., ORORBIA, A., KIFER, D., AND GILES, L. Investigating backpropagation alternatives when learning to
dynamically count with recurrent neural networks. In International Conference on Grammatical Inference (2021), PMLR,
pp. 154–175.
[262] MANCHEV, N., AND SPRATLING, M. Target propagation in recurrent neural networks. The Journal of Machine Learning
Research 21, 1 (2020), 250–282.
[263] MARBLESTONE, A. H., WAYNE, G., AND KORDING, K. P. Toward an integration of deep learning and neuroscience.
Frontiers in computational neuroscience 10 (2016), 94.
[264] MARGOSSIAN, C. C. A review of automatic differentiation and its efficient implementation. Wiley interdisciplinary
reviews: data mining and knowledge discovery 9, 4 (2019), e1305.
[265] MARINO, J. Predictive coding, variational autoencoders, and biological connections. Neural Computation 34, 1 (2022),
1–44.
[266] MARTIN, E., ERNOULT, M., LAYDEVANT, J., LI, S., QUERLIOZ, D., PETRISOR, T., AND GROLLIER, J. Eqspike:
spike-driven equilibrium propagation for neuromorphic implementations. Iscience 24, 3 (2021), 102222.
[267] MARTINETZ, T. Competitive hebbian learning rule forms perfectly topology preserving maps. In International confer-
ence on artificial neural networks (1993), Springer, pp. 427–434.
[268] MARTINETZ, T. M., BERKOVICH, S. G., AND SCHULTEN, K. J. ’neural-gas’ network for vector quantization and its
application to time-series prediction. IEEE transactions on neural networks 4, 4 (1993), 558–569.
[269] MATSUBARA, Y., LEVORATO, M., AND RESTUCCIA, F. Split computing and early exiting for deep learning applica-
tions: Survey and research challenges. ACM Computing Surveys 55, 5 (2022), 1–30.
[270] MAZZONI, P., ANDERSEN, R. A., AND JORDAN, M. I. A more biologically plausible learning rule for neural networks.
Proceedings of the National Academy of Sciences 88, 10 (1991), 4433–4437.
[271] MCCLELLAND, J. L., AND RUMELHART, D. E. An interactive activation model of context effects in letter perception:
I. an account of basic findings. Psychological review 88, 5 (1981), 375.
[272] MCCLOSKEY, M., AND COHEN, N. J. Catastrophic interference in connectionist networks: The sequential learning
problem. The psychology of learning and motivation 24, 109 (1989), 92.
[273] MCCULLOCH, W. S., AND PITTS, W. A logical calculus of the ideas immanent in nervous activity. The bulletin of
mathematical biophysics 5, 4 (1943), 115–133.
[274] MCLACHLAN, G. J., LEE, S. X., AND RATHNAYAKE, S. I. Finite mixture models. Annual review of statistics and its
application 6 (2019), 355–378.
[275] MESNARD, T., GERSTNER, W., AND BREA, J. Towards deep learning with spiking neurons in energy based models
with contrastive hebbian plasticity. arXiv preprint arXiv:1612.03214 (2016).
49

Preprint
[276] MEULEMANS, A., CARZANIGA, F., SUYKENS, J., SACRAMENTO, J., AND GREWE, B. F. A theoretical framework for
target propagation. Advances in Neural Information Processing Systems 33 (2020), 20024–20036.
[277] MEULEMANS, A., CARZANIGA, F. S., SUYKENS, J. A. K., SACRAMENTO, J., AND GREWE, B. F. A theoretical
framework for target propagation, 2020.
[278] MEULEMANS, A., FARINHA, M. T., CERVERA, M. R., SACRAMENTO, J., AND GREWE, B. F. Minimizing control for
credit assignment with strong feedback. In International Conference on Machine Learning (2022), PMLR, pp. 15458–
15483.
[279] MEULEMANS, A., TRISTANY FARINHA, M., GARCÍA ORDÓÑEZ, J., VILIMELIS ACEITUNO, P., SACRAMENTO, J.,
AND GREWE, B. F. Credit assignment in neural networks through deep feedback control. Advances in Neural Information
Processing Systems 34 (2021), 4674–4687.
[280] MILLER, K. D., AND MACKAY, D. J. The role of constraints in hebbian learning. Neural computation 6, 1 (1994),
100–126.
[281] MILLIDGE, B., SALVATORI, T., SONG, Y., LUKASIEWICZ, T., AND BOGACZ, R. Universal hopfield networks: A
general framework for single-shot associative memory models. In International Conference on Machine Learning (2022),
PMLR, pp. 15561–15583.
[282] MILLIDGE, B., SETH, A., AND BUCKLEY, C. L. Predictive coding: a theoretical and experimental review, 2021.
[283] MILLIDGE, B., SONG, Y., SALVATORI, T., LUKASIEWICZ, T., AND BOGACZ, R. Backpropagation at the infinitesimal
inference limit of energy-based models: Unifying predictive coding, equilibrium propagation, and contrastive Hebbian
learning. In Proceedings of the 11th International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
1–5 May 2023 (2023).
[284] MILLIDGE, B., SONG, Y., SALVATORI, T., LUKASIEWICZ, T., AND BOGACZ, R. A theoretical framework for in-
ference and learning in predictive coding networks. In Proceedings of the 11th International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, 1–5 May 2023 (2023), OpenReview.net.
[285] MILLIDGE, B., TSCHANTZ, A., AND BUCKLEY, C. L. Predictive coding approximates backprop along arbitrary com-
putation graphs. arXiv:2006.04182 (2020).
[286] MITTELMAN, R., KUIPERS, B., SAVARESE, S., AND LEE, H.
Structured recurrent temporal restricted boltzmann
machines. In International Conference on Machine Learning (2014), PMLR, pp. 1647–1655.
[287] MOHAMED, A.-R., DAHL, G., HINTON, G., ET AL. Deep belief networks for phone recognition. In Nips workshop on
deep learning for speech recognition and related applications (2009), vol. 1, Vancouver, Canada, p. 39.
[288] MOMENNEJAD, I. A rubric for human-like agents and neuroai. Philosophical Transactions of the Royal Society B 378,
1869 (2023), 20210446.
[289] MONTAVON, G., AND MÜLLER, K.-R. Deep boltzmann machines and the centering trick. Neural Networks: Tricks of
the Trade: Second Edition (2012), 621–637.
[290] MOSKOVITZ, T. H., LITWIN-KUMAR, A., AND ABBOTT, L. Feedback alignment in deep convolutional networks.
arXiv:1812.06488 (2018).
[291] MOSTAFA, H., RAMESH, V., AND CAUWENBERGHS, G. Deep supervised learning using local errors. Frontiers in
neuroscience 12 (2018), 608.
[292] MOVELLAN, J. R. Contrastive Hebbian learning in the continuous Hopfield model. In Connectionist Models. Elsevier,
1991, pp. 10–17.
[293] MUJIKA, A., MEIER, F., AND STEGER, A. Approximating real-time recurrent learning with random kronecker factors.
arXiv preprint arXiv:1805.10842 (2018).
[294] MÜLLER-DAHLHAUS, F., ZIEMANN, U., AND CLASSEN, J. Plasticity resembling spike-timing dependent synaptic
plasticity: the evidence in human cortex. Frontiers in Synaptic Neuroscience 2 (2010), 34.
[295] MURRAY, J. M. Local online learning in recurrent networks with random feedback. Elife 8 (2019), e43299.
[296] N’DRI, A. W., BARBIER, T., TEULIÈRE, C., AND TRIESCH, J. Predictive coding light: learning compact visual codes
by combining excitatory and inhibitory spike timing-dependent plasticity. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (2023), pp. 3996–4005.
[297] NEFTCI, E., DAS, S., PEDRONI, B., KREUTZ-DELGADO, K., AND CAUWENBERGHS, G. Event-driven contrastive
divergence for spiking neuromorphic systems. Frontiers in neuroscience 7 (2014), 272.
[298] NEFTCI, E. O., AUGUSTINE, C., PAUL, S., AND DETORAKIS, G. Event-driven random back-propagation: Enabling
neuromorphic deep learning machines. Frontiers in neuroscience 11 (2017), 324.
[299] NEFTCI, E. O., MOSTAFA, H., AND ZENKE, F. Surrogate gradient learning in spiking neural networks: Bringing the
power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine 36, 6 (2019), 51–63.
[300] NIEHUES, J., AND WAIBEL, A. Continuous space language models using restricted boltzmann machines. In Proceedings
of the 9th International Workshop on Spoken Language Translation: Papers (2012), pp. 164–170.
50

Preprint
[301] NØKLAND, A. Direct feedback alignment provides learning in deep neural networks. In Advances in Neural Information
Processing Systems (2016), pp. 1037–1045.
[302] NØKLAND, A., AND EIDNES, L. H. Training neural networks with local error signals. In International Conference on
Machine Learning (2019), PMLR, pp. 4839–4850.
[303] NOROUZI, M., RANJBAR, M., AND MORI, G. Stacks of convolutional restricted boltzmann machines for shift-invariant
feature learning. In 2009 IEEE Conference on Computer Vision and Pattern Recognition (2009), IEEE, pp. 2735–2742.
[304] OBEID, D., RAMAMBASON, H., AND PEHLEVAN, C. Structured and deep similarity matching via structured and deep
Hebbian networks. In Advances in Neural Information Processing Systems (2019).
[305] OGUZ, I., KE, J., WANG, Q., YANG, F., YILDIRIM, M., DINC, N. U., HSIEH, J.-L., MOSER, C., AND PSALTIS, D.
Forward-forward training of an optical neural network. arXiv preprint arXiv:2305.19170 (2023).
[306] OHAMA, Y., FUKUMURA, N., AND UNO, Y. A forward-propagation rule for acquiring neural inverse models using a rls
algorithm. In Neural Information Processing: 11th International Conference, ICONIP 2004, Calcutta, India, November
22-25, 2004. Proceedings 11 (2004), Springer, pp. 585–591.
[307] OJA, E. Simplified neuron model as a principal component analyzer. Journal of mathematical biology 15, 3 (1982),
267–273.
[308] OLSHAUSEN, B. A., AND FIELD, D. J. Sparse coding with an overcomplete basis set: A strategy employed by v1?
Vision research 37, 23 (1997), 3311–3325.
[309] O’REILLY, R. C. Biologically plausible error-driven learning using local activation differences: The generalized recir-
culation algorithm. Neural Computation 8, 5 (1996), 895–938.
[310] O’REILLY, R. C. Six principles for biologically based computational models of cortical cognition. Trends in cognitive
sciences 2, 11 (1998), 455–462.
[311] O’REILLY, R. C., AND MUNAKATA, Y. Computational explorations in cognitive neuroscience: Understanding the mind
by simulating the brain. MIT press, 2000.
[312] ORORBIA, A. Continual competitive memory: A neural system for online task-free lifelong learning. arXiv preprint
arXiv:2106.13300 (2021).
[313] ORORBIA, A.
Contrastive-signal-dependent plasticity: Forward-forward learning of spiking neural systems.
arXiv
preprint arXiv:2303.18187 (2023).
[314] ORORBIA, A. Spiking neural predictive coding for continually learning from data streams. Neurocomputing 544 (2023),
126292.
[315] ORORBIA, A., AND KELLY, M. A. Cogngen: Building the kernel for a hyperdimensional predictive processing cognitive
architecture. In Proceedings of the Annual Meeting of the Cognitive Science Society (2022), vol. 44.
[316] ORORBIA, A., AND KIFER, D. The neural coding framework for learning generative models. Nature communications
13, 1 (2022), 2064.
[317] ORORBIA, A., AND MALI, A. Convolutional neural generative coding: Scaling predictive coding to natural images.
arXiv preprint arXiv:2211.12047 (2022).
[318] ORORBIA, A., AND MALI, A. Active predictive coding: Brain-inspired reinforcement learning for sparse reward robotic
control problems. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (2023), IEEE, pp. 3015–
3021.
[319] ORORBIA, A., AND MALI, A. The predictive forward-forward algorithm. arXiv preprint arXiv:2301.01452 (2023).
[320] ORORBIA, A., MALI, A., GILES, C. L., AND KIFER, D. Continual learning of recurrent neural networks by locally
aligning distributed representations. IEEE transactions on neural networks and learning systems 31, 10 (2020), 4267–
4278.
[321] ORORBIA, A., MALI, A., GILES, C. L., AND KIFER, D. Lifelong neural predictive coding: Learning cumulatively
online without forgetting. Advances in Neural Information Processing Systems 35 (2022), 5867–5881.
[322] ORORBIA, A. G., AND KELLY, M. A. Maze learning using a hyperdimensional predictive processing cognitive archi-
tecture. In Artificial General Intelligence: 15th International Conference, AGI 2022, Seattle, WA, USA, August 19–22,
2022, Proceedings (2023), Springer, pp. 321–331.
[323] ORORBIA, A. G., AND MALI, A. Biologically motivated algorithms for propagating local target representations. In
Proceedings of the aaai conference on artificial intelligence (2019), vol. 33, pp. 4651–4658.
[324] ORORBIA, A. G., AND MALI, A.
Backprop-free reinforcement learning with active neural generative coding.
In
Proceedings of the AAAI Conference on Artificial Intelligence (2022), vol. 36, pp. 29–37.
[325] ORORBIA, A. G., MALI, A., KIFER, D., AND GILES, C. L. Deep credit assignment by aligning local representations.
arXiv preprint arXiv:1803.01834 (2018).
[326] ORORBIA, A. G., MALI, A., KIFER, D., AND GILES, C. L. Backpropagation-free deep learning with recursive local
representation alignment. In Proceedings of the AAAI Conference on Artificial Intelligence (2023), vol. 37, pp. 9327–
9335.
51

Preprint
[327] ORORBIA II, A. G., GILES, C. L., AND REITTER, D. Learning a deep hybrid model for semi-supervised text classifica-
tion. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Lisbon,
Portugal, 2015).
[328] ORORBIA II, A. G., HAFFNER, P., REITTER, D., AND GILES, C. L. Learning to adapt by minimizing discrepancy.
arXiv preprint arXiv:1711.11542 (2017).
[329] O’CONNOR, P., GAVVES, E., AND WELLING, M. Training a spiking neural network with equilibrium propagation. In
The 22nd international conference on artificial intelligence and statistics (2019), PMLR, pp. 1516–1523.
[330] O’REILLY, R. C., HAZY, T. E., AND HERD, S. A. The leabra cognitive architecture: How to play 20 principles with
nature. The Oxford handbook of cognitive science (2016), 91.
[331] PALIOTTA, D., ALAIN, M., MÁTÉ, B., AND FLEURET, F. Graph neural networks go forward-forward. arXiv preprint
arXiv:2302.05282 (2023).
[332] PANDA, P., AND ROY, K. Learning to generate sequences with combination of hebbian and non-hebbian plasticity in
recurrent spiking neural networks. Frontiers in neuroscience 11 (2017), 693.
[333] PANICHELLO, M., CHEUNG, O., AND BAR, M. Predictive feedback and conscious visual experience. Frontiers in
Psychology 3 (2013), 620.
[334] PARR, T., PEZZULO, G., AND FRISTON, K. J.
Active Inference: The Free Energy Principle in Mind, Brain, and
Behavior. MIT Press, 2022.
[335] PASCANU, R., MIKOLOV, T., AND BENGIO, Y. On the difficulty of training recurrent neural networks. In International
Conference on Machine Learning (2013), pp. 1310–1318.
[336] PAWLAK, V., WICKENS, J. R., KIRKWOOD, A., AND KERR, J. N. Timing is not everything: neuromodulation opens
the stdp gate. Frontiers in synaptic neuroscience 2 (2010), 146.
[337] PAYEUR, A., GUERGUIEV, J., ZENKE, F., RICHARDS, B. A., AND NAUD, R. Burst-dependent synaptic plasticity can
coordinate learning in hierarchical circuits. Nature neuroscience 24, 7 (2021), 1010–1019.
[338] PEMBERTON, J., BOVEN, E., APPS, R., AND PONTE COSTA, R. Cortico-cerebellar networks as decoupling neural
interfaces. Advances in neural information processing systems 34 (2021), 7745–7759.
[339] PFEIFFER, M., NESSLER, B., DOUGLAS, R. J., AND MAASS, W. Reward-modulated hebbian learning of decision
making. Neural Computation 22, 6 (2010), 1399–1444.
[340] PINCHETTI, L., SALVATORI, T., MILLIDGE, B., SONG, Y., YORDANOV, Y., AND LUKASIEWICZ, T. Predictive coding
beyond Gaussian assumptions. Advances in Neural Information Processing Systems (2022).
[341] PINCUS, M. A monte carlo method for the approximate solution of certain types of constrained optimization problems.
Operations research 18, 6 (1970), 1225–1228.
[342] PINEDA, F. J. Generalization of back-propagation to recurrent neural networks. Physical review letters 59, 19 (1987),
2229.
[343] PINEDA, F. J. Recurrent backpropagation and the dynamical approach to adaptive neural computation. Neural Compu-
tation 1, 2 (1989), 161–172.
[344] PINTO, R. C., AND ENGEL, P. M. A fast incremental gaussian mixture model. PloS one 10, 10 (2015), e0139931.
[345] PODLASKI, B., AND MACHENS, C. K. Biological credit assignment through dynamic inversion of feedforward networks.
Advances in Neural Information Processing Systems 33 (2020), 10065–10076.
[346] POGODIN, R., AND LATHAM, P. Kernelized information bottleneck leads to biologically plausible 3-factor hebbian
learning in deep networks. Advances in Neural Information Processing Systems 33 (2020), 7296–7307.
[347] POGODIN, R., MEHTA, Y., LILLICRAP, T., AND LATHAM, P. E. Towards biologically plausible convolutional networks.
Advances in Neural Information Processing Systems 34 (2021), 13924–13936.
[348] PORR, B., AND WÖRGÖTTER, F. Learning with “relevance”: using a third factor to stabilize hebbian learning. Neural
computation 19, 10 (2007), 2694–2719.
[349] POZZI, I., BOHTÉ, S., AND ROELFSEMA, P. A biologically plausible learning rule for deep learning in the brain. arXiv
preprint arXiv:1811.01768 (2018).
[350] QIN, S., MUDUR, N., AND PEHLEVAN, C. Contrastive similarity matching for supervised learning. Neural computation
33, 5 (2021), 1300–1328.
[351] RAMÓN Y CAJAL, S. The croonian lecture.—la fine structure des centres nerveux. Proceedings of the Royal Society of
London 55, 331-335 (1894), 444–468.
[352] RAMSAUER, H., SCHÄFL, B., LEHNER, J., SEIDL, P., WIDRICH, M., ADLER, T., GRUBER, L., HOLZLEITNER, M.,
PAVLOVI ´C, M., SANDVE, G. K., ET AL. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217 (2020).
[353] RAO, R. P. Hierarchical bayesian inference in networks of spiking neurons. Advances in neural information processing
systems 17 (2004).
52

Preprint
[354] RAO, R. P., AND BALLARD, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-
classical receptive-field effects. Nature neuroscience 2, 1 (1999).
[355] RAO, R. P. N., GKLEZAKOS, D. C., AND SATHISH, V. Active predictive coding: A unified neural framework for
learning hierarchical world models for perception and planning. arXiv preprint arXiv:2210.13461 (2022).
[356] REFINETTI, M., D’ASCOLI, S., OHANA, R., AND GOLDT, S. Align, then memorise: the dynamics of learning with
feedback alignment. In International Conference on Machine Learning (2021), PMLR, pp. 8925–8935.
[357] REN, M., KORNBLITH, S., LIAO, R., AND HINTON, G. Scaling forward gradient with local losses. arXiv preprint
arXiv:2210.03310 (2022).
[358] REYNOLDS, J. N., AND WICKENS, J. R. Dopamine-dependent plasticity of corticostriatal synapses. Neural networks
15, 4-6 (2002), 507–521.
[359] RICHARDS, B. A., LILLICRAP, T. P., BEAUDOIN, P., BENGIO, Y., BOGACZ, R., CHRISTENSEN, A., CLOPATH, C.,
COSTA, R. P., DE BERKER, A., GANGULI, S., ET AL. A deep learning framework for neuroscience. Nature neuroscience
22, 11 (2019), 1761–1770.
[360] RIEDMILLER, M., AND BRAUN, H. Rprop-a fast adaptive learning algorithm. In Proc. of ISCIS VII), Universitat (1992),
Citeseer.
[361] RIEDMILLER, M., AND BRAUN, H. A direct adaptive method for faster backpropagation learning: The rprop algorithm.
In IEEE international conference on neural networks (1993), IEEE, pp. 586–591.
[362] ROELFSEMA, P. R., AND HOLTMAAT, A. Control of synaptic plasticity in deep cortical networks. Nature Reviews
Neuroscience 19, 3 (2018), 166–180.
[363] ROELFSEMA, P. R., VAN OOYEN, A., AND WATANABE, T. Perceptual learning rules based on reinforcers and attention.
Trends in cognitive sciences 14, 2 (2010), 64–71.
[364] ROMBOUTS, J. O., BOHTE, S. M., MARTINEZ-TRUJILLO, J., AND ROELFSEMA, P. R. A learning rule that explains
how rewards teach attention. Visual Cognition 23, 1-2 (2015), 179–205.
[365] ROMBOUTS, J. O., BOHTE, S. M., AND ROELFSEMA, P. R. How attention can create synaptic tags for the learning of
working memories in sequential tasks. PLoS computational biology 11, 3 (2015), e1004060.
[366] ROMERO, E., MAZZANTI, F., DELGADO, J., AND BUCHACA, D. Weighted contrastive divergence. Neural Networks
114 (2019), 147–156.
[367] ROSENBLATT, F. The perceptron: a probabilistic model for information storage and organization in the brain. Psycho-
logical review 65, 6 (1958), 386.
[368] ROULET, V., AND HARCHAOUI, Z. Target propagation via regularized inversion. arXiv preprint arXiv:2112.01453
(2021).
[369] ROZELL, C. J., JOHNSON, D. H., BARANIUK, R. G., AND OLSHAUSEN, B. A. Sparse coding via thresholding and
local competition in neural circuits. Neural computation 20, 10 (2008), 2526–2563.
[370] RUBNER, J., AND TAVAN, P. A self-organizing network for principal-component analysis. EPL (Europhysics Letters)
10, 7 (1989), 693.
[371] RUEDA-PLATA, D., RAMOS-POLLÁN, R., AND GONZÁLEZ, F. A. Supervised greedy layer-wise training for deep
convolutional networks with small datasets. In Computational Collective Intelligence: 7th International Conference,
ICCCI 2015, Madrid, Spain, September 21-23, 2015, Proceedings, Part I (2015), Springer, pp. 275–284.
[372] RUMELHART, D. E., HINTON, G. E., AND WILLIAMS, R. J. Learning representations by back-propagating errors.
Nature 323, 6088 (1986), 533–536.
[373] RUMELHART, D. E., MCCLELLAND, J. L., AND PDP RESEARCH GROUP, C. Parallel distributed processing: Explo-
rations in the microstructure of cognition, Vol. 1: Foundations. MIT press, 1986.
[374] RUMELHART, D. E., AND ZIPSER, D. Feature discovery by competitive learning. Cognitive Science 9, 1 (1985), 75–112.
[375] SACRAMENTO, J., COSTA, R. P., BENGIO, Y., AND SENN, W. Dendritic cortical microcircuits approximate the back-
propagation algorithm. In Advances in Neural Information Processing Systems (2018), pp. 8735–8746.
[376] SALAKHUTDINOV, R., AND HINTON, G. Deep boltzmann machines. In Artificial intelligence and statistics (2009),
PMLR, pp. 448–455.
[377] SALAKHUTDINOV, R., AND LAROCHELLE, H.
Efficient learning of deep boltzmann machines. In Proceedings of
the thirteenth international conference on artificial intelligence and statistics (2010), JMLR Workshop and Conference
Proceedings, pp. 693–700.
[378] SALAKHUTDINOV, R., AND MURRAY, I. On the quantitative analysis of deep belief networks. In Proceedings of the
25th international conference on Machine learning (2008), pp. 872–879.
[379] SALIMANS, T., HO, J., CHEN, X., SIDOR, S., AND SUTSKEVER, I. Evolution strategies as a scalable alternative to
reinforcement learning. arXiv preprint arXiv:1703.03864 (2017).
53

Preprint
[380] SALVATORI, T., MALI, A., BUCKLEY, C. L., LUKASIEWICZ, T., RAO, R. P., FRISTON, K., AND ORORBIA, A.
Brain-inspired computational intelligence via predictive coding. arXiv preprint arXiv:2308.07870 (2023).
[381] SALVATORI, T., SONG, Y., MILLIDGE, B., XU, Z., SHA, L., EMDE, C., BOGACZ, R., AND LUKASIEWICZ, T.
Incremental predictive coding: A parallel and fully automatic learning algorithm.
arXiv preprint arXiv:2212.00720
(2022).
[382] SALVATORI, T., SONG, Y., XU, Z., LUKASIEWICZ, T., AND BOGACZ, R. Reverse differentiation via predictive coding.
In Proceedings of the 36th AAAI Conference on Artificial Intelligence (2022), AAAI Press.
[383] SAMADI, A., LILLICRAP, T. P., AND TWEED, D. B. Deep learning with dynamic spiking neurons and fixed feedback
weights. Neural computation 29, 3 (2017), 578–602.
[384] SANTANA, E., EMIGH, M. S., ZEGERS, P., AND PRINCIPE, J. C. Exploiting spatio-temporal structure with recurrent
winner-take-all networks. IEEE Transactions on Neural Networks and Learning Systems (2017).
[385] SAUL, L. K., JAAKKOLA, T., AND JORDAN, M. I. Mean field theory for sigmoid belief networks. Journal of artificial
intelligence research 4 (1996), 61–76.
[386] SAXE, A. M., MCCLELLAND, J. L., AND GANGULI, S. Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks. arXiv preprint arXiv:1312.6120 (2013).
[387] SCELLIER, B., AND BENGIO, Y. Towards a biologically plausible backprop. arXiv preprint arXiv:1602.05179 914
(2016).
[388] SCELLIER, B., AND BENGIO, Y. Equilibrium propagation: Bridging the gap between energy-based models and back-
propagation. Frontiers in computational neuroscience 11 (2017), 24.
[389] SCELLIER, B., AND BENGIO, Y. Equivalence of equilibrium propagation and recurrent backpropagation. arXiv preprint
arXiv:1711.08416 (2017).
[390] SCELLIER, B., GOYAL, A., BINAS, J., MESNARD, T., AND BENGIO, Y. Generalization of equilibrium propagation to
vector field dynamics. arXiv preprint arXiv:1808.04873 (2018).
[391] SCHMIDHUBER, J. A local learning algorithm for dynamic feedforward and recurrent networks. Connection Science 1,
4 (1989), 403–412.
[392] SCHMIDHUBER, J. Networks adjusting networks. In Proceedings of" Distributed Adaptive Neural Information Process-
ing" (1990), pp. 197–208.
[393] SCHMIDHUBER, J. Recurrent networks adjusted by adaptive critics. In Proc. IEEE/INNS International Joint Conference
on Neural Networks (1990), pp. 719–722.
[394] SCHÖLKOPF, B., HERBRICH, R., AND SMOLA, A. J. A generalized representer theorem. In International conference
on computational learning theory (2001), Springer, pp. 416–426.
[395] SCHULTZ, W. Dopamine signals for reward value and risk: basic and recent data. Behavioral and brain functions 6, 1
(2010), 1–9.
[396] SCHULTZ, W., DAYAN, P., AND MONTAGUE, P. R. A neural substrate of prediction and reward. Science 275, 5306
(1997), 1593–1599.
[397] SEIDL, P., RENZ, P., DYUBANKOVA, N., NEVES, P., VERHOEVEN, J., WEGNER, J. K., SEGLER, M., HOCHREITER,
S., AND KLAMBAUER, G. Improving few-and zero-shot reaction template prediction using modern hopfield networks.
Journal of chemical information and modeling 62, 9 (2022), 2111–2120.
[398] SEJNOWSKI, T. J., AND TESAURO, G. The hebb rule for synaptic plasticity: algorithms and implementations. In Neural
models of plasticity. Elsevier, 1989, pp. 94–103.
[399] SERBAN, I. V., ORORBIA II, A. G., PINEAU, J., AND COURVILLE, A. Piecewise latent variables for neural variational
text processing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (2017),
pp. 422–432.
[400] SEUNG, H. S., AND ZUNG, J. A correlation game for unsupervised learning yields computational interpretations of
hebbian excitation, anti-hebbian inhibition, and synapse elimination. arXiv preprint arXiv:1704.00646 (2017).
[401] SEZENER, E., GRABSKA-BARWI ´NSKA, A., KOSTADINOV, D., BEAU, M., KRISHNAGOPAL, S., BUDDEN, D., HUT-
TER, M., VENESS, J., BOTVINICK, M., CLOPATH, C., ET AL. A rapid and efficient learning rule for biological neural
circuits. BioRxiv (2021), 2021–03.
[402] SHARMA, S., AUBIN, S., AND ELIASMITH, C. Large-scale cognitive model design using the nengo neural simulator.
Biologically Inspired Cognitive Architectures 17 (2016), 86–100.
[403] SHIBUYA, T., INOUE, N., KAWAKAMI, R., AND SATO, I. Fixed-weight difference target propagation. In Proceedings
of the AAAI Conference on Artificial Intelligence (2023), vol. 37, pp. 9811–9819.
[404] SIDDIQUI, S. A., KRUEGER, D., LECUN, Y., AND DENY, S. Blockwise self-supervised learning at scale. arXiv preprint
arXiv:2302.01647 (2023).
54

Preprint
[405] ŠÍMA, J., AND ORPONEN, P. Continuous-time symmetric hopfield nets are computationally universal. Neural Compu-
tation 15, 3 (2003), 693–733.
[406] SMOLENSKY, P., ET AL. Information processing in dynamical systems: Foundations of harmony theory. Technical
Report (1986).
[407] SONG, S., SJÖSTRÖM, P. J., REIGL, M., NELSON, S., AND CHKLOVSKII, D. B. Highly nonrandom features of synaptic
connectivity in local cortical circuits. PLOS Biology 3, 3 (03 2005).
[408] SONG, Y., LUKASIEWICZ, T., XU, Z., AND BOGACZ, R. Can the brain do backpropagation? — Exact implementation
of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (2020), vol. 33.
[409] SOO-YOUNG, L., AND DONG-GYU, J. Merging back-propagation and hebbian learning rules for robust classifications.
Neural networks: the official journal of the International Neural Network Society 9, 7 (1996), 1213–1222.
[410] SPRATLING, M. W. A hierarchical predictive coding model of object recognition in natural images. Cognitive Compu-
tation 9, 2 (2017), 151–167.
[411] SPRATLING, M. W. A review of predictive coding algorithms. Brain and Cognition 112 (2017), 92–97.
[412] SRIVASTAVA, R. K., MASCI, J., KAZEROUNIAN, S., GOMEZ, F., AND SCHMIDHUBER, J. Compete to compute. In
Advances in Neural Information Processing Systems 26, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 2310–2318.
[413] STANLEY, K. O., AND MIIKKULAINEN, R. Evolving neural networks through augmenting topologies. Evolutionary
computation 10, 2 (2002), 99–127.
[414] STEIL, J. J. Backpropagation-decorrelation: online recurrent learning with o (n) complexity. In Neural Networks, 2004.
Proceedings. 2004 IEEE International Joint Conference on (2004), vol. 2, IEEE, pp. 843–848.
[415] SUÁREZ, L. E., MARKELLO, R. D., BETZEL, R. F., AND MISIC, B. Linking structure and function in macroscale brain
networks. Trends in cognitive sciences 24, 4 (2020), 302–315.
[416] SUMMERFIELD, C., EGNER, T., GREENE, M., KOECHLIN, E., MANGELS, J., AND HIRSCH, J. Predictive codes for
forthcoming perception in the frontal cortex. Science 314, 5803 (2006), 1311–1314.
[417] SUMMERFIELD, C., TRITTSCHUH, E. H., MONTI, J. M., MESULAM, M.-M., AND EGNER, T. Neural repetition
suppression reflects fulfilled perceptual expectations. Nature neuroscience 11, 9 (2008), 1004.
[418] SUTSKEVER, I., AND HINTON, G. E. Deep, narrow sigmoid belief networks are universal approximators. Neural
computation 20, 11 (2008), 2629–2636.
[419] SUTSKEVER, I., HINTON, G. E., AND TAYLOR, G. W. The recurrent temporal restricted boltzmann machine. Advances
in neural information processing systems 21 (2008).
[420] SUTSKEVER, I., AND TIELEMAN, T.
On the convergence properties of contrastive divergence.
In Proceedings of
the thirteenth international conference on artificial intelligence and statistics (2010), JMLR Workshop and Conference
Proceedings, pp. 789–795.
[421] SUTTON, R. S., AND BARTO, A. G. Reinforcement learning: An introduction. MIT press, 2018.
[422] SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S., ANGUELOV, D., ERHAN, D., VANHOUCKE, V., AND
RABINOVICH, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and
pattern recognition (2015), pp. 1–9.
[423] TAKEFUJI, Y., AND LEE, K. An artificial hysteresis binary neuron: A model suppressing the oscillatory behaviors of
neural dynamics. Biological Cybernetics 64, 5 (1991), 353–356.
[424] TALLEC, C., AND OLLIVIER, Y. Unbiased online recurrent optimization. arXiv preprint arXiv:1702.05043 (2017).
[425] TAVAKOLI, M., SADOWSKI, P., AND BALDI, P. Tourbillon: a physically plausible neural architecture. arXiv preprint
arXiv:2107.06424 (2021).
[426] TAYLOR, G., BURMEISTER, R., XU, Z., SINGH, B., PATEL, A., AND GOLDSTEIN, T. Training neural networks without
gradients: A scalable admm approach. In International conference on machine learning (2016), PMLR, pp. 2722–2731.
[427] TAYLOR, G. W., HINTON, G. E., AND ROWEIS, S. Modeling human motion using binary latent variables. Advances in
neural information processing systems 19 (2006).
[428] TAZERART, S., MITCHELL, D. E., MIRANDA-ROTTMANN, S., AND ARAYA, R. A spike-timing-dependent plasticity
rule for dendritic spines. Nature communications 11, 1 (2020), 4276.
[429] TEERAPITTAYANON, S., MCDANEL, B., AND KUNG, H.-T. Branchynet: Fast inference via early exiting from deep
neural networks. In 2016 23rd international conference on pattern recognition (ICPR) (2016), IEEE, pp. 2464–2469.
[430] TENG, Q., WANG, K., ZHANG, L., AND HE, J. The layer-wise training convolutional neural networks using local loss
for sensor-based human activity recognition. IEEE Sensors Journal 20, 13 (2020), 7265–7274.
[431] TIELEMAN, T. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings
of the 25th international conference on Machine learning (2008), pp. 1064–1071.
55

Preprint
[432] TIELEMAN, T., AND HINTON, G. Using fast weights to improve persistent contrastive divergence. In Proceedings of the
26th annual international conference on machine learning (2009), pp. 1033–1040.
[433] TIELEMAN, T., HINTON, G., ET AL.
Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural networks for machine learning 4, 2 (2012), 26–31.
[434] TISHBY, N., PEREIRA, F. C., AND BIALEK, W. The information bottleneck method. arXiv preprint physics/0004057
(2000).
[435] TOOMARIAN, N. B., AND BARHEN, J.
Learning a trajectory using adjoint functions and teacher forcing.
Neural
networks 5, 3 (1992), 473–484.
[436] TOOSI, T., AND ISSA, E. B. Brain-like flexible visual inference by harnessing feedback-feedforward alignment. arXiv
preprint arXiv:2310.20599 (2023).
[437] TURING, A. M. Computing machinery and intelligence. Mind (1948).
[438] URBANCZIK, R., AND SENN, W. Reinforcement learning in populations of spiking neurons. Nature neuroscience 12, 3
(2009), 250.
[439] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N., KAISER, L., AND POLO-
SUKHIN, I. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems (2017).
[440] VECERA, S. P., AND O’REILLY, R. C. Figure-ground organization and object recognition processes: an interactive
account. Journal of Experimental Psychology: Human Perception and Performance 24, 2 (1998), 441.
[441] VOELKER, A. R. A solution to the dynamics of the prescribed error sensitivity learning rule. Waterloo: Centre for
Theoretical Neuroscience (2015).
[442] VON DER MALSBURG, C. Self-organization of orientation sensitive cells in the striate cortex. Kybernetik 14, 2 (1973),
85–100.
[443] WANG, L. On competitive learning. IEEE Transactions on neural networks 8, 5 (1997), 1214–1217.
[444] WANG, Y., NI, Z., SONG, S., YANG, L., AND HUANG, G. Revisiting locally supervised learning: an alternative to
end-to-end training. arXiv preprint arXiv:2101.10832 (2021).
[445] WEN, H., HAN, K., SHI, J., ZHANG, Y., CULURCIELLO, E., AND LIU, Z. Deep predictive coding network for object
recognition. arXiv preprint arXiv:1802.04762 (2018).
[446] WENNEKERS, T., GARAGNANI, M., AND PULVERMÜLLER, F. Language models based on hebbian cell assemblies.
Journal of Physiology-Paris 100, 1-3 (2006), 16–30.
[447] WERBOS, P. Approximate dynamic programming for real-time control and neural modeling. Handbook of intelligent
control (1992).
[448] WERBOS, P. J. Applications of advances in nonlinear sensitivity analysis. In System modeling and optimization. Springer,
1982, pp. 762–770.
[449] WERFEL, J., XIE, X., AND SEUNG, H. S. Learning curves for stochastic gradient descent in linear feedforward networks.
In Advances in neural information processing systems (2004), pp. 1197–1204.
[450] WHITE, E. L., AND KELLER, A. Cortical circuits: synaptic organization of the cerebral cortex: structure, function, and
theory. Birkhäuser Boston, 1989.
[451] WHITE, R. H. Competitive hebbian learning. In IJCNN-91-Seattle International Joint Conference on Neural Networks
(1991), vol. 2, IEEE, pp. 949–vol.
[452] WHITTINGTON, J. C., AND BOGACZ, R. An approximation of the error backpropagation algorithm in a predictive
coding network with local hebbian synaptic plasticity. Neural computation 29, 5 (2017), 1229–1262.
[453] WHITTINGTON, J. C. R., AND BOGACZ, R. Theories of error back-propagation in the brain. Trends in Cognitive
Sciences (2019).
[454] WIDRICH, M., HOFMARCHER, M., PATIL, V. P., BITTO-NEMLING, A., AND HOCHREITER, S. Modern hopfield
networks for return decomposition for delayed rewards. In Deep RL Workshop NeurIPS 2021 (2021).
[455] WIDROW, B., AND HOFF, M. E. Adaptive switching circuits. Tech. rep., Stanford Univ Ca Stanford Electronics Labs,
1960.
[456] WIDROW, B., AND WALACH, E. Adaptive signal processing for adaptive control. IFAC Proceedings Volumes 16, 9
(1983), 7–12.
[457] WILLIAMS, C. K., AND HINTON, G. E. Mean field networks that learn to discriminate temporally distorted strings. In
Connectionist Models. Elsevier, 1991, pp. 18–22.
[458] WILLIAMS, E., BREDENBERG, C., AND LAJOIE, G. Flexible phase dynamics for bio-plausible contrastive learning.
arXiv preprint arXiv:2302.12431 (2023).
[459] WILLIAMS, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine
learning 8, 3-4 (1992), 229–256.
56

Preprint
[460] WILLIAMS, R. J., AND ZIPSER, D. Gradient-based learning algorithms for recurrent connectionist networks. Technical
Report NU-CCS-90-9). Boston: Northeastern University, 1990.
[461] WISEMAN, S., CHOPRA, S., RANZATO, M., SZLAM, A., SUN, R., CHINTALA, S., AND VASILACHE, N. Training
language models using target-propagation. arXiv preprint arXiv:1702.04770 (2017).
[462] XIAO, W., CHEN, H., LIAO, Q., AND POGGIO, T. Biologically-plausible learning algorithms can scale to large datasets.
arXiv preprint arXiv:1811.03567 (2018).
[463] XIE, X., AND SEUNG, H. S. Spike-based learning rules and stabilization of persistent neural activity. In Advances in
neural information processing systems (2000), pp. 199–208.
[464] XIE, X., AND SEUNG, H. S. Equivalence of backpropagation and contrastive hebbian learning in a layered network.
Neural computation 15, 2 (2003), 441–454.
[465] XIONG, Y., REN, M., AND URTASUN, R. Loco: Local contrastive representation learning. Advances in neural informa-
tion processing systems 33 (2020), 11142–11153.
[466] YI, S.-I., KENDALL, J. D., WILLIAMS, R. S., AND KUMAR, S. Activity-difference training of deep neural networks
using memristor crossbars. Nature Electronics 6, 1 (2023), 45–51.
[467] YOO, A. H., AND COLLINS, A. G. How working memory and reinforcement learning are intertwined: A cognitive,
neural, and computational perspective. Journal of cognitive neuroscience 34, 4 (2022), 551–568.
[468] YOO, J., AND WOOD, F. BayesPCN: A continually learnable predictive coding associative memory. Advances in Neural
Information Processing Systems 35 (2022), 29903–29914.
[469] YUILLE, A., KAMMEN, D., AND COHEN, D. Quadrature and the development of orientation selective cortical cells by
hebb rules. Biological Cybernetics 61, 3 (1989), 183–194.
[470] ZADOR, A., ESCOLA, S., RICHARDS, B., ÖLVECZKY, B., BENGIO, Y., BOAHEN, K., BOTVINICK, M., CHKLOVSKII,
D., CHURCHLAND, A., CLOPATH, C., ET AL. Catalyzing next-generation artificial intelligence through neuroai. Nature
communications 14, 1 (2023), 1597.
[471] ZEE, T., ORORBIA, A. G., MALI, A., AND NWOGU, I. A robust backpropagation-free framework for images. arXiv
preprint arXiv:2206.01820 (2022).
[472] ZHANG, J., YU, J., AND TAO, D. Local deep-feature alignment for unsupervised dimension reduction. IEEE transac-
tions on image processing 27, 5 (2018), 2420–2432.
[473] ZHANG, L. I., TAO, H. W., HOLT, C. E., HARRIS, W. A., AND POO, M.-M. A critical window for cooperation and
competition among developing retinotectal synapses. Nature 395, 6697 (1998), 37–44.
[474] ZHANG, Z., AND BALLARD, D. H. A single spike model of predictive coding. Neurocomputing 58 (2004), 165–171.
[475] ZHANG, Z., AND BRAND, M. Convergent block coordinate descent for training tikhonov regularized deep neural net-
works. Advances in Neural Information Processing Systems 30 (2017).
[476] ZHAO, G., WANG, T., LI, Y., JIN, Y., LANG, C., AND LING, H. The cascaded forward algorithm for neural network
training. arXiv preprint arXiv:2303.09728 (2023).
[477] ZHONG, J., CANGELOSI, A., ZHANG, X., AND OGATA, T. Afa-prednet: The action modulation within predictive
coding. In 2018 International Joint Conference on Neural Networks (IJCNN) (2018), IEEE.
[478] ZHUANG, H., WANG, Y., LIU, Q., AND LIN, Z. Fully decoupled neural network learning using delayed gradients. IEEE
transactions on neural networks and learning systems 33, 10 (2021), 6013–6020.
[479] ZIPSER, D., AND ANDERSEN, R. A. A back-propagation programmed network that simulates response properties of a
subset of posterior parietal neurons. Nature 331, 6158 (1988), 679.
[480] ZMARZ, P., AND KELLER, G. B. Mismatch receptive fields in mouse visual cortex. Neuron 92, 4 (2016), 766–772.
[481] ZOPPO, G., MARRONE, F., AND CORINTO, F. Equilibrium propagation for memristor-based recurrent neural networks.
Frontiers in neuroscience 14 (2020), 240.
57

Preprint
Supplemental Material
Potential Research Questions
Other potentially useful research questions related to research in biologically-inspired credit assignment, ranging from scientific
to engineering considerations, include the following:
• When does a credit assignment scheme only apply to a certain level of abstraction of the neural (assembly) dynamics?
What changes or modifications are needed to make an algorithm operate at different time-scales (e.g., moving from
rate-coded dynamics to spiking dynamics and vice versa)?
• Does a credit assignment scheme or process work uniformly well across different types of learning setups, e.g., super-
vised, semi/weakly-supervised, unsupervised, and reinforcement learning? Which cases/setups highlight weakness or
strengths of specific algorithm families/procedures? What algorithm types complement each other in hybrid/combined
setups?
• To what degree of entanglement between (computational) neural architecture and inference/credit assignment is needed
to result in reasonable generalization in certain task contexts? What flexibility must be ceded to obtain particular
properties, such as energy efficiency or massive parallelism, and how is generalization impacted by different degrees
of entanglement?
• What other criterion drawn from neuro-physiology/biology would help to improve the stability and generalization
ability of artificial neural systems and their processes of credit assignment?
• How do different update computations, produced by different biological credit assignment schemes, mix or interact
with outer optimization procedures, e.g., RMSprop [433], Adam [198], etc.?
• What modifications or generalizations are needed for a credit assignment scheme to efficiently and effectively handle
time-varying information, particularly without unrolling or unfolding in time?
58

Preprint
Terms and Symbology
In Tables 3 and 4, we provide a table that collects and briefly defines several acronyms, abbreviations, key symbols, and operators
used throughout the survey.
Table 3: Key symbols and operators.
Item
Explanation
·
Matrix/vector multiplication
⊙
Hadamard product (element-wise multiplication)
zj
jth scalar of vector z
||v||2
Euclidean norm of vector v
L
Number of layers
zℓ
A layer ℓof neural activity values
ϕℓ()
Nonlinear activation/transfer function applied to a vector/matrix
L
Cost/objective functional
M
Neuromodulatory signal
T
Transpose operation
Wℓ
A matrix of synaptic weight values
E()
Energy functional
F()
Free Energy functional
Table 4: Key abbreviations/acronyms and definitions.
Item
Explanation
Item
Explanation
2F
Two-Factor
3F
Three-Factor
Backprop
Backpropagation of errors
CD-K
Contrastive divergence (K steps)
CHL
Contrastive hebbian learning
SAP
Stochastic approximation procedure
EProp
Equilibrium propagation
SLU
Synthetic local updates
PC
Predictive coding
DBN
Deep belief network
RFF
Recurrent forward-forward
RBM
Restricted Boltzmann machine
PFF
Predictive forward-forward
DBM
Deep Boltzmann machine
SNN
Spiking neural network
ANN
Artificial neural network
STDP
Spike-timing-dependent plasticity
DNN
Deep neural network
R-STDP
Reward-modulated STDP
SA
Simulated annealing
LRA
Local representation alignment
GeneRec
Generalized recirculation
SigProp
Signal Propagation
rec-LRA
Recursive LRA
Targetprop
Target propagation
DTP
Difference target propagation
MLP
Multilayer perceptron
HSIC
Hilbert-Schmidt independence criterion
KP
Kolen-Pollack method
WTA
Winner-take-all
RFA
Random feedback alignment
DFA
Direct feedback alignment
SOM
Self-organizing map
ART
Adaptive resonance theory
RNN
Recurrent neural network
BPTT
Backprop through time
SGD
Stochastic gradient descent
CIFAR10, SVHN, ImageNet
Natural image datasets/benchmarks
RMSprop
Root mean squared propagation
Adam
Adaptive moment estimation
SLU
Synthetic Local Updates
HISC
Hilbert-Schmidt indepedence criterion
EFP
Error Forward Propagation
PEPITA
Present the error to perturb the input
to modulate activity
Neuromod
Neuro-modulation
Competitive
Competitive (Hebbian) learning
DRTP
Direct random target projection
WM
Weight Mirrors
59

