Transformers are 
Graph Neural Networks
Chaitanya K. Joshi
Graph Deep Learning Reading Group
Full Blogpost: 
https://graphdeeplearning.github.io/post/transformers-are-gnns/

Some success stories: GNNs for RecSys

Some success stories: GNNs for RecSys

Another success story? 
The Transformer architecture for NLP


Representation Learning for NLP

Breaking down the Transformer
We update the hidden feature h of the i'th
word in a sentence S from layer ℓto 
layer ℓ+1 as follows:
where j∈S denotes the set of words in the 
sentence and Q, K, V are learnable linear 
weights.

Breaking down the Transformer
We update the hidden feature h of the i'th
word in a sentence S from layer ℓto 
layer ℓ+1 as follows:
where j∈S denotes the set of words in the 
sentence and Q, K, V are learnable linear 
weights.

Breaking down the Transformer
We update the hidden feature h of the i'th
word in a sentence S from layer ℓto 
layer ℓ+1 as follows:
where j∈S denotes the set of words in the 
sentence and Q, K, V are learnable linear 
weights.

Multi-head Attention
Bad random initializations can de-stabilize the learning process of this 
dot-product attention mechanism. We can ‘hedge our bets’ through 
concatenating multiple attention ‘heads’:

Multi-head Attention

The Final Picture
Update each word’s features through
Multi-head Attention mechanism
as a weighted sum of features of other words 
in the sentence.
+ Scaling dot product attention
+ Normalization layers
+ Residual links

The Final Picture
Update each word’s features through
Multi-head Attention mechanism
as a weighted sum of features of other words 
in the sentence.
+ Scaling dot product attention
+ Normalization layers
+ Residual links

The Final Picture
Update each word’s features through
Multi-head Attention mechanism
as a weighted sum of features of other words 
in the sentence.
+ Scaling dot product attention
+ Normalization layers
+ Residual links

Representation Learning on Graphs

Graph Neural Networks
GNNs update the hidden features h of node i at 
layer ℓvia a non-linear transformation of the node’s own 
features added to the aggregation of features from each 
neighbouring node j∈N(i):
where U, V are learnable weight matrices of the GNN 
layer and σ is a non-linearity.

Connections b/w GNNs and Transformers
Consider a sentence as a fully 
connected graph of words…

Connections b/w GNNs and Transformers


Sum over local 
neighborhood
Sum over 
all words 
in S


Simple linear 
transform 
and sum
Weighted 
sum via 
attention

Standard 
Graph NN
Graph 
Attention 
Network

Standard GNN
GAT
Transformer
+ Multi-head 
mechanism
+ Normalization 
layers
+ Residual links
+ Weighted sum 
aggregation

