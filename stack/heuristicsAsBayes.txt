Heuristics as Bayesian inference
under extreme priors
Paula Parparta,1, Matt Jonesb, and Bradley C. Lovea,c
aDepartment of Experimental Psychology, University College London, London WC1H 0AP, United Kingdom; bDepartment of Psychology and Neuroscience, University of
Colorado Boulder, Boulder, C0, 80309; cThe Alan Turing Institute, London NW1 2DB, United Kingdom
Simple heuristics are often regarded as tractable decision strategies
because they ignore a great deal of information in the input data.
One puzzle is why heuristics can outperform full-information models,
such as linear regression, which make full use of the available infor-
mation. These “less-is-more” effects, in which a relatively simpler
model outperforms a more complex model, are prevalent throughout
cognitive science, and are frequently argued to demonstrate an in-
herent advantage of simplifying computation or ignoring information.
In contrast, we show at the computational level (where algorithmic
restrictions are set aside) that it is never optimal to discard infor-
mation. Through a formal Bayesian analysis, we prove that popular
heuristics, such as tallying and Take-the Best, are formally equiv-
alent to Bayesian inference under the limit of inﬁnitely strong pri-
ors. Varying the strength of the prior yields a continuum of Bayesian
models with the heuristics at one end and ordinary regression at the
other. Critically, intermediate models perform better across all our
simulations, suggesting that down-weighting information with the
appropriate prior is preferable to entirely ignoring it.
Rather than
because of their simplicity, our analyses suggest heuristics perform
well because they implement strong priors that approximate the ac-
tual structure of the environment. We end by considering how new
heuristics could be derived by inﬁnitely strengthening the priors of
other Bayesian models. These formal results have implications for
work in psychology, machine learning and economics.
heuristics | Bayesian inference | decision making | ridge regression
M
any real-world prediction problems involve binary clas-
siﬁcation based on available information, such as pre-
dicting whether Germany or England will win a soccer match
based on the teams’ statistics. A relatively simple decision
procedure would use a rule to combine available information
(i.e., cues), such as the teams’ league position, the result of
the last game between Germany and England, which team has
scored more goals recently, and which team is home versus
away.
One such decision procedure, the tallying heuristic,
simply checks which team is better on each cue and chooses
the team that has more cues in its favor, ignoring any possible
diﬀerences among cues in magnitude or predictive value (1, 2).
In the scenario depicted in Fig. 1A this heuristic would choose
England. Another algorithm, Take-The-Best (TTB), would
base the decision on the best single cue that diﬀerentiates the
two options. TTB works by ranking the cues according to
their cue validity (i.e., predictive value), then sequentially pro-
ceeding from the most valid to least valid until a cue is found
that favors one team over the other (3). Thus TTB terminates
at the ﬁrst discriminative cue, discarding all remaining cues.
In contrast to these heuristic algorithms, a full-information
model such as linear regression would make use of all the cues,
their magnitudes, their predictive values, and observed covari-
ation among them. For example, league position and number
of goals scored are highly correlated, and this correlation in-
ﬂuences the weights obtained from a regression model (Fig.
1B). Although such covariances naturally arise and can be
meaningful, the cue validities used by the tallying and TTB
heuristics completely ignore them (4). Instead, cue validities
assess only the probability with which a single cue can identify
the correct alternative, as the proportion of correct inferences
made by that cue alone across a set of binary comparisons
(formal deﬁnition in the SI Appendix). When two cues co-vary
highly, they essentially provide the same information, but
heuristics ignore this redundancy and treat the related cues
as independent information sources.
Heuristics have a long history of study in cognitive science,
where they are often viewed as more psychologically plausible
than full-information models, because ignoring data makes
the calculation easier and thus may be more compatible with
inherent cognitive limitations (5–7). This view suggests that
heuristics should underperform full-information models, with
the loss in performance compensated by reduced computa-
tional cost. This prediction is challenged by observations of
less-is-more eﬀects, wherein heuristics sometimes outperform
full-information models, such as linear regression, in real-world
prediction tasks (1–3, 8–11). These ﬁndings have been used
to argue that ignoring information can actually improve per-
formance, even in the absence of processing limitations. For
example, Gigerenzer et al. (12) write, “There is a point where
too much information and too much information processing
can hurt” (p. 21). Likewise, Gigerenzer and Brighton (13)
conclude, “A less-is-more eﬀect, however, means that minds
would not gain anything from relying on complex strategies,
even if direct costs and opportunity costs were zero.” (p. 111)
Signiﬁcance Statement
When people make decisions under uncertainty, such as choos-
ing which apartment to rent, one common view is that they rely
on heuristic algorithms, which can be viewed as shortcuts in
that they do not fully utilize all available information. Heuris-
tics are often contrasted with full-information decision models,
which make full use of the available information. We develop a
formal framework that puts both heuristics and standard regres-
sion models on equal footing by treating them as extreme cases
of the same rational Bayesian model (a full-information model).
This integration helps explain why heuristics can sometimes
perform better than full-information models.
Author contributions: P.P., M.J., and B.L. came up with the framework; P.P. performed simulations
and computational modeling of data; M.J. contributed mathematical derivations, and P.P., B.L, and
M.J. wrote the paper.
The authors declare no conﬂict of interest.
1To whom correspondence should be addressed. E-mail: paula.parpart@ucl.ac.uk
August 30, 2017
|
1–7

(1)
.90 ☺
"
+1
(2)
.81 #
#
0
(3)
.73 "
☺
-1
(4)
.54 "
☺
-1
   Criterion:
Who will win the 
game?
Home vs. 
away
 League
position
Last game 
result
No.
N  of goalsls
No
No
No
No of
of
of
of goa
goa
 League pos.
Home vs. away
No. of goals
Last game result
v
cue 
coding
B
A
Fig. 1. Illustrative example of a binary prediction task. (A) Predicting whether Team
Germany or England will win is based on four cues: league position, last game
result, home vs. away match, and recent goal scoring. Cue validities (v) reﬂect the
relative frequency with which each cue makes correct inferences across many team
comparisons (formula in SI Appendix). Smiley and frowning faces indicate which team
is superior on each cue, whereas a grey face indicates the two teams are equal on that
cue. For modeling, a cue is coded +1 when it favors the team on the left (Germany),
-1 when it favors the team on the right (England), and 0 when the teams are equal
along that cue. (B) Irrespective of cue validity, cues can co-vary (illustrated by overlap)
with the criterion variable but also with each other. The heuristics considered here
ignore this covariance among cues.
Less-is-more arguments also arise in other domains of cog-
nitive science, such as in claims that learning is more success-
ful when processing capacity is (at least initially) restricted
(14, 15). Contrary to existing claims, we argue there is no
inherent computational advantage to simplicity of information
processing. Less-is-more eﬀects can arise only when the space
of models under consideration is limited to a particular family
or architecture. At a computational level of analysis, where
restrictions on algorithms are set aside (16), more information
is always better.
We cast our argument in a Bayesian framework, wherein
additional information (input data) is always helpful but must
be correctly combined with appropriate prior knowledge. We
ﬁrst prove that the tallying and TTB heuristics are equivalent
to Bayesian inference under the limit of an inﬁnitely strong
prior. This connection suggests that heuristics perform well
because their relative inﬂexibility amounts to a strong induc-
tive bias, one that is suitable for many real-world learning and
decision problems.
We then use this connection to deﬁne a continuum of
Bayesian models, determined by parametric variation in the
strength of the prior. At one end of the continuum (inﬁnitely
diﬀuse prior), the Bayesian model is equivalent to a variant of
linear regression, and at the other end (inﬁnitely strong prior)
it is equivalent to a heuristic. Although the Bayesian models
mimic the heuristics perfectly in the limit, a crucial diﬀerence
is that the Bayesian account regulates cue weights but never
discards any information. The models are tested on classic
datasets that have been used to demonstrate superiority of
the heuristics over linear regression, and in all cases we ﬁnd
that best performance comes from intermediate models on the
continuum, which do not entirely ignore cue weights or cue
covariance but that nonetheless down-weight this information
via the inﬂuence of their priors. These results suggest that
the success of heuristics, and ﬁndings of less-is-more eﬀects
more broadly in cognitive science, are due not to a computa-
tional advantage of simplicity per se, but rather to the fact
that simpler models can approximate strong priors that are
well-suited to the true structure of the environment.
  65
70
75
80
85
90
Model
  Ordinary Linear Regression
Take−The−Best
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
 
 
   
Small 
 Large 
Goodness of Fit
Generalizability
zil
a
r
e
n
e
G
at
P n
o
i
e
m
r
o
fr
a
)
%
( e
c
n
B
A
Model Flexibility
Training Sample  Training Sample  
Fig. 2. The concept of overﬁtting. (A) More ﬂexible models can ﬁt the training sample
better (goodness of ﬁt), accounting for most of the variability. However, these models
can fare poorly in generalization tasks that test on novel samples (generalizability)
(19). (B) Our re-analysis of a dataset (2) used to evaluate heuristics (predicting house
prices) ﬁnds that TTB outperforms ordinary linear regression at generalization when
the training sample is small (20 training cases). However, the pattern reverses when
the training sample is enlarged (100 training cases). Error bars represent ± SEM.
Details are in the SI Appendix.
Bias, Variance, and Bayesian Inference
The current explanation for less-is-more eﬀects in the heuristics
literature is based on the bias-variance dilemma (12). The
present paper extends this Frequentist concept into a Bayesian
framework that formally links heuristics and full-information
models. From a statistical perspective, every model, including
heuristics, has an inductive bias, which makes it best-suited
to certain learning problems (17). A model’s bias and the
training data are responsible for what the model learns. In
addition to diﬀering in bias, models can also diﬀer in how
sensitive they are to sampling variability in the training data,
which is reﬂected in the variance of the model’s parameters
after training (i.e., across diﬀerent training samples).
A core tool in machine learning and psychology for eval-
uating the performance of learning models, cross-validation,
assesses how well a model can apply what it has learned from
past experiences (i.e., the training data) to novel test cases (18).
From a psychological standpoint, a model’s cross-validation
performance can be understood as its ability to generalize from
past experience to guide future behavior. How well a model
classiﬁes test cases in cross-validation is jointly determined
by its bias and variance. Higher ﬂexibility can in fact hurt
performance because it makes the model more sensitive to
the idiosyncrasies of the training sample. This phenomenon,
commonly referred to as overﬁtting, is characterized by high
performance on experienced cases from the training sample
but poor performance on novel test items. Overﬁtted models
have high goodness of ﬁt but low generalization performance
(Fig. 2A) (19).
Bias and variance tend to trade oﬀwith one another such
that models with low bias suﬀer from high variance and vice
versa (17). With small training samples, more ﬂexible (i.e.,
less biased) models will overﬁt and can be bested by simpler
(i.e., more biased) models such as heuristics. As the size of
the training sample increases, variance becomes less inﬂuential
and the advantage shifts to the complex models (9). Indeed,
in a reanalysis of a dataset used to evaluate heuristics (2), we
ﬁnd that the advantage for the heuristic over linear regression
disappears when training sample size is increased (Fig. 2B).
2
|
Parpart et al.

The Bayesian framework oﬀers a diﬀerent perspective on
the bias-variance dilemma.
Provided a Bayesian model is
correctly speciﬁed, it always integrates new data optimally,
striking the perfect balance between prior and data. Thus
using more information can only improve performance. From
the Bayesian standpoint, a less-is-more eﬀect can arise only if
a model uses the data incorrectly, for example by weighting
it too heavily relative to prior knowledge (e.g., with ordinary
linear regression, where there eﬀectively is no prior). In that
case, the data might indeed increase estimation variance to the
point that ignoring some of the information could improve per-
formance. However, that can never be the best solution. One
can always obtain superior predictive performance by using
all of the information but tempering it with the appropriate
prior. The results in the remainder of this paper demonstrate
this conclusion explicitly.
Tallying as a Limiting Case of Regularized Regression
The ﬁrst Bayesian model we develop is conceptually related
to ridge regression (20), a successful regularized regression
approach in machine learning. Ridge regression extends ordi-
nary linear regression by incorporating a penalty term that
adjusts model ﬂexibility to improve weight estimates and avoid
overﬁtting (Fig. 2A). The types of tasks we model in this
paper are binary comparisons, where each input represents
a comparison between two alternatives on a set of cues, and
the output represents which alternative has the greater value
on some outcome variable. Consider a training set of input-
output pairs (x1, y1) , ..., (xn, yn) with xi œ {≠1, 0, 1}m and
yi œ {≠1, 1}. An example is Fig. 1A, where the explanatory
variables (x) encode which soccer team is superior on each
cue, and the outcome variable (y) indicates which team won
each comparison (match). The aim in any linear regression
problem is to estimate the weights, i.e., a vector of regres-
sion coeﬃcients w = [w1, ..., wm]T , such that prediction error
between y and Xw is minimized. The weights estimated by
ridge regression are deﬁned by
ˆwridge = arg min
w
Y
]
[ Îy ≠XwÎ2
¸
˚˙
˝
Goodness-of-Fit
+
◊ÎwÎ2
¸ ˚˙ ˝
Penalty Term
Z
^
\ ,
[1]
where the penalty parameter ◊is nonnegative. Î.Î2 denotes the
square of the Euclidean norm, y = [y1, ..., yn]T is the outcome
variable deﬁned over all n binary comparisons in the training
sample, and X is an n ◊m matrix with one column for each
of the m predictor variables xj. When the penalty parameter
equals zero, ridge regression is concerned only with goodness of
ﬁt (i.e., minimizing squared error on the training set). For this
special case, ridge regression is equivalent to ordinary linear
regression, which is highly sensitive to sampling variability
in the training set. As the penalty parameter increases, the
pressure to shrink the weights increases, reducing them to zero
as ◊æ Œ. Thus larger values of ◊lead to stronger inductive
bias, which can reduce overﬁtting by reducing sensitivity to
noise in the training sample. However, the optimal setting
of ◊will always depend on the environment from which the
weights, cues, and outcomes were sampled.
The ridge penalty term is mathematically equivalent to a
Gaussian Bayesian prior on the weights, where ◊is inversely
proportional to the prior variance ÷2 of each wi (i.e., ◊= ‡2/÷2,
where ‡2 is the variance of the error in y, also assumed to
be Gaussian). In the Bayesian interpretation, the strength
of the prior is thus reﬂected by 1/÷2, growing stronger as
÷ æ 0. This prior distribution is combined with observations
from the training sample to form a posterior distribution (also
Gaussian) over the weights. Like ordinary linear regression,
ridge regression provides a point estimate for the weights,
equal to the mean (and also the mode) of the full Bayesian
posterior distribution (21, 22). The conceptual relationships
among ridge regression, ordinary linear regression, and the
Bayesian model are illustrated in the SI Appendix, Fig. S2.
Half-Ridge Model and Tallying. Our Bayesian derivation of the
tallying heuristic extends ridge regression by assuming the
directionalities of the cues (i.e., the signs of the true weights)
are known in advance. For example, being higher in the league
standings will, if anything, make a team more likely (not less)
to win a given match. This assumption is concordant with how
the tallying heuristic was originally proposed in the literature
(1). We refer to this deﬁnition of the tallying heuristic as
directed tallying in order to diﬀerentiate it from the version of
the tallying heuristic that learns cue directionalities from the
training data (2). Thus we deﬁne the prior for each weight
as half-Gaussian, truncated at zero (right-hand side in Fig.
S2, SI Appendix), and we refer to this Bayesian model as the
half-ridge model. Formally, the joint prior is deﬁned by
w ≥N (0, Σ)|wœO
Σ = ÷2I,
[2]
where Σ is the covariance matrix among the weights (prior to
truncation) and ÷2 determines the variance for each weight.
The restriction notation, |w œ O, indicates we truncate the
distribution to one orthant O µ Rm, deﬁned by the predeter-
mined directionalities of the cues. For example, if the cues were
assumed all to have positive (or null) eﬀects on the outcome,
then O would equal {w œ Rm|’i, wi Ø 0}. Under this assump-
tion, the posterior distribution inherits the same truncation
(see SI Appendix for derivations). The important question is
what happens to this posterior as the prior becomes arbitrarily
strong, that is, as ÷ æ 0. Just as with increasing the penalty
parameter in regular ridge regression, strengthening the prior
in the half-ridge model shrinks the posterior weights toward
zero (Equation 3). However, the ratios of the weights—that
is, the relative inferred strengths of the cues—all converge to
unity. This result can be seen through a simple rescaling of
the weights, which has no impact on a binary comparison task.
In particular, we show in the SI Appendix that the posterior
distribution for w/÷ obeys
w
÷
dæ N (0, I)|wœO as ÷ æ 0
[3]
conditional on X and y (where
dæ indicates convergence in
distribution). Consequently, the rescaled weights all have the
same posterior mean in the limit:
lim
÷æ0 E
5
wi
÷
----X, y
6
= ±
Ú
2
ﬁ,
[4]
with signs determined by each cue’s assumed directionality.
Therefore, the optimal decision-making strategy under the
Bayesian half-ridge model converges to a simple summation of
Parpart et al.
August 30, 2017
|
3

the predictors—that is, the directed tallying heuristic. Note
that, under this limit, the model becomes completely invariant
to the training data. In particular, it ignores how strongly
each cue is associated with the outcome in the training set
(i.e., magnitudes of cue validities). At the other extreme, as
the prior becomes extremely weak (÷ æ Œ), the Bayesian
half-ridge model converges to a full regression model akin to
ordinary linear regression in that it diﬀerentially weights the
cues (e.g., more predictive cues receive higher weights than less
predictive cues), the only diﬀerence being that the weights are
constrained to have their predetermined signs. In conclusion,
the half-ridge model demonstrates how the directed tallying
heuristic arises as an extreme case of a Bayesian prior on the
distributions of weights in the environment, and it shows that
tallying and linear regression can be related by a continuum
of models that diﬀer only in the strength of this prior.
Heuristics vs. Intermediate Models. From a Bayesian perspec-
tive, the model that fares best on a given decision task should
be the one with a prior most closely matching the data’s gen-
erating process. In many decision environments, cues diﬀer in
their predictiveness, but these diﬀerences are not arbitrarily
large (i.e., the cue weights are not drawn uniformly from all
real numbers). An advantage of the Bayesian half-ridge frame-
work is that it speciﬁes a continuum of models between the
extremes of linear regression and the directed tallying heuristic.
For many environments, the best-performing model should
lie somewhere between these two extremes.
Furthermore,
the best-performing model should not change with diﬀerent
training set sizes (cf. Fig.2), because—unlike the Frequentist
phenomenon of bias-variance tradeoﬀ—a correctly speciﬁed
Bayesian model is guaranteed to ﬁnd the optimal tradeoﬀ
between prior and likelihood, for any sample size.
The Bayesian half-ridge model was simulated on 20 datasets
that have been used to compare heuristic and regression ap-
proaches (2). The key ﬁnding is that intermediate models
perform best in all cases for all training sample sizes (see Fig.
3). Interestingly, the ordinary regression model (i.e., the limit
of ÷ æ Œ) outperforms the tallying heuristic (i.e., the limit of
÷ æ 0). This discrepancy from past less-is-more results arises
because cue directions are not learned in these simulations,
and therefore there is no opportunity for the more ﬂexible
regression model to misestimate the cue directions. We do
demonstrate less-is-more results in the 20 datasets (2, 10) when
comparing heuristics and regression models that estimate cue
directions from the training set (SI Appendix, Fig. S5 and Fig.
S6). The main ﬁnding, that intermediate half-ridge models
outperform tallying in all 20 datasets, suggests that ignoring
information is never the best solution. The best-performing
model uses all the information in the training data, combining
it with the appropriate prior.
A Covariance-Based Bayesian Model and Heuristics
In this section, we consider a second Bayesian model that,
unlike the half-ridge model, learns cue directions from the
training set and provides a uniﬁcation of TTB, tallying, and
linear regression. Given that ridge regression (L2 regulariza-
tion) yields tallying, one might wonder whether a strong prior
of a diﬀerent functional form might yield the TTB heuristic. In
particular, lasso regression (L1 regularization) (22) is known
to produce sparsity in cue selection (i.e., many weights are es-
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●●
●
●●
●
●●
●
●●
●●●
●●●●
●●
●●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
House Prices
Mortality Rates
City Size
Professors' Salaries
Body Fat
Car Accidents
Cloud Rainfall
High School Dropouts
Obesity
Fuel Consumption
Biodiversity
Homelessness
Land Rent
Mammals' Sleep
Oxidants in L.A.
Attractiveness Men
Attractiveness Women
Fish Fertility
Oxygen
Ozone in S.F.
80.0
82.5
85.0
66
69
72
75
78
64
66
68
70
66
68
70
72
74
76
52.5
55.0
57.5
67.5
70.0
72.5
75.0
60.0
62.5
65.0
67.5
60
61
62
63
64
65
64
66
68
70
72
69
72
75
78
70
75
80
50
55
60
60
65
70
75
72
74
76
78
60.0
62.5
65.0
57
58
59
60
61
62
63
52.5
55.0
57.5
60.0
62.5
56
58
60
62
63
65
67
69
71
64
65
66
67
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
TrainingSize
●
●
●
10
20
115
Generalization performance (%)
Strength of the prior 
Fig. 3. Generalization performance of the Bayesian half-ridge model by training
sample size and as a function of the strength of the prior for 20 datasets for which
heuristics have been previously evaluated (2). The abscissa represents the strength
of the prior, and the ordinate represents the predictive accuracy of the model on test
comparisons. Note that an approximately inﬁnitely strong prior on the far right of each
graph (small values of ÷) corresponds to the directed tallying heuristic. Intermediate
models (i.e., with a medium-strength prior) performed best in all datasets regardless
of training sample size. Error bars represent ± SEM. Because the Oxygen and Ozone
datasets contain less than 115 object pairs in total, training size 115 is not included
for them. See SI Appendix for details.
timated as zero), and thus might be expected to yield TTB in
the limit. Instead, derivations show that lasso regression also
converges to tallying in the limit when the cue directionalities
are known a priori. This result highlights the robustness of the
conclusions of the previous sections, with tallying arising as a
limiting case of Bayesian inference under a variety of diﬀerent
priors.
Given this formal result, we take a diﬀerent approach. One
key observation is that, unlike linear regression, both TTB and
tallying rely on isolated cue-outcome relationships (i.e., cue
validity) that disregard covariance information among cues.
We use this insight to construct our second Bayesian model,
with a prior that suppresses information about cue covariance
but leaves information about cue validity unaﬀected. We refer
to this model as Covariance Orthogonalizing Regularization
(COR), because our regularization method essentially makes
cues appear more orthogonal to each other. The strength of
the prior yields a continuum of models (Fig. 4) deﬁned by
sensitivity to covariation among cues, which smoothly vary in
their mean posterior weight estimates from those of ordinary
linear regression to weights that are linear transforms of the
heuristics’ cue validities (see SI Appendix derivations).
In contrast to ridge regression, we express the regression
problem in multivariate terms by multiplexing the outcome
m times (the number of predictors), which allows the model
to capture the sequential nature of TTB. As shown in Fig.
4, every copy of the output receives input from every cue,
and thus the weights can be represented as an m ◊m weight
matrix W. Unlike in ridge regression, where the Gaussian prior
shrinks all model weights toward zero, only the cross-weights
(i.e., the oﬀ-diagonal elements) are penalized. In the limiting
case, when the precision of the prior, 1/÷2, approaches Œ, the
4
|
Parpart et al.

  ∞
࢞ଵ
ݕ
ݕ
࢞ଶ
࢞ଷ
ݕ
࢞ଵ
ݕ
ݕ
࢞ଶ
࢞ଷ
ݕ
࢞ଵ
ݕ
ݕ
࢞ଶ
࢞ଷ
ݕ
  0
 
Heuristics
Linear Regression
Strength of the prior 
Fig. 4. The prior of the COR model inﬂuences the posterior solution (i.e., the mean
of the posterior on W) such that the model encompasses linear regression and the
heuristics as extreme cases. In this example, there are m = 3 cues, represented
as vectors x1, x2, x3, where one set of entries xi1, xi2, xi3 pertains to the ith
binary comparison. In order to establish a continuum of covariation sensitivity, the
criterion variable is multiplexed as many times as there are cues (i.e., m times). The
result is a multivariate regression problem with a dependent matrix Y of m columns
of identical criterion variables. We refer to the dashed arrows as cross-weights, and
the solid arrows as direct weights, corresponding respectively to the off-diagonal and
diagonal entries of the weight matrix W. In an ordinary linear regression model, the
estimated weights depend on the cue covariances. In contrast, a model structure
without any of the cross-weights would revert to three simple regressions with exactly
one predictor each (x1 , x2, or x3). Therefore, in the limit 1/÷2 æ 0, the prior does
not penalize the cross-weights, and the set of mean posterior weights to each copy
of the criterion variable is equal to the ordinary linear regression solution (leftmost
network). At the other extreme, when 1/÷2 æ Œ, the cross-weights are shrunk to
zero, and the knowledge captured in the direct weights becomes equivalent to that
embodied by cue validities in heuristics that ignore covariation information (rightmost
network). Between these two extreme values of 1/÷2 lie models that are sensitive to
covariation to varying degrees (middle network).
cross-weights reduce to zero and the posterior estimates for
the direct (diagonal) weights are equivalent to cue validities as
used by the heuristics (i.e., neglecting covariance information),
up to a linear transformation. At the other extreme, when
1/÷2 = 0, every copy of y has the same posterior for its set of
weights, and the mean (and mode) of this posterior is equal
to the ordinary linear regression solution. In particular, the
covariance information is reﬂected in the posterior weights as
it is in the ordinary regression solution.
The model weights are paired with a decision rule to classify
test items. First, the vector xi = [xi1, . . . , xim] is multiplied by
the mean posterior weight matrix Wú to generate an output
vector ˆyi = [ˆyi1, . . . , ˆyim]:
ˆyi = xiW ú.
[5]
Note that using the posterior mean is equivalent to integrating
over the full posterior distribution, due to the linearity of
Equation 5. The TTB decision rule is then applied to the
resulting ˆyi as
zi = ˆyi,jú
i where jú
i = arg max
j
--ˆyij
--
[6]
and
choicei =
Y
]
[
+1 (left),
if zi > 0
≠1 (right),
if zi < 0
0 (guess),
if zi = 0.
[7]
Thus, the TTB decision rule selects the maximum absolute
output (Equation 6) and takes the valence of that output as its
choice (Equation 7). When 1/÷2 ¥ Œ (and the cross-weights
are thus zero), the decision rule exhibits the exact sequential
nature of the TTB heuristic, because then each output ˆyij
in Equation 5 equals the value of the corresponding cue, xij,
times its cue validity. The largest output will correspond to
the most valid cue that is not equal to zero (i.e., indiﬀerent)
for the particular test comparison.
Thus when the TTB
decision rule is adopted, the COR model converges to the
TTB heuristic as 1/÷2 æ Œ. In the SI Appendix, Fig. S3
shows simulations of an artiﬁcial binary prediction task similar
to Fig. 1, demonstrating that the COR model (with TTB
decision rule) and the TTB heuristic reach perfect agreement
in their predictions as the prior becomes strong enough.
Notably, the tallying heuristic can also be derived from the
COR model, in its undirected version that uses cue validities
in the training data to infer cue directionalities. The tallying
decision rule is deﬁned by
zi =
ÿ
j
sgn (ˆyij) .
[8]
The tallying decision rule chooses the option with a majority of
outputs in its favor (conveyed by their valences indicated by the
sign function), irrespective of the magnitudes of the outputs.
The choice is determined by Equation 7, as in the TTB decision
rule. When the tallying decision rule is adopted by the COR
model, the model converges to the tallying heuristic in the
limit as 1/÷2 æ Œ (Fig. S4, SI Appendix). Lastly, in the limit
of 1/÷2 æ 0, either decision rule will yield decisions equivalent
to ordinary linear regression. Under this limit, the outputs ˆyi
produced according to Equation 5 are all equal to the ordinary
linear regression prediction (as outlined above), and both the
TTB and tallying decision rules will yield a choice equal to
the valence of that prediction.
The COR model demonstrates how ordinary linear regres-
sion and both TTB and the tallying heuristic can be derived
as extreme cases of a Bayesian prior deﬁned by covariance
expectation. Importantly, the only element varying across the
continuum is the prior’s strength, and the prior is responsible
for recovering the heuristics in the limit. The model converges
to ordinary regression as the strength of the prior goes to zero
regardless of the decision rule, and these model properties also
hold under other forms of regularization (e.g. lasso regular-
ization). As with the half-ridge model, we ﬁnd that COR’s
performance peaks for intermediate priors for all 20 datasets
(2) (SI Appendix, Fig. S5 and Fig. S6). Thus once again less
is not more, as the heuristics are outperformed by a prior of
ﬁnite strength that uses all information in the training data
but nonetheless down-weights that information.
Discussion
A central message of this work is that, in contrast to less-is-
more claims, ignoring information is rarely, if ever optimal
(12, 13, 23). Heuristics may work well in practice because
they correspond to inﬁnitely strong priors that make them
oblivious to aspects of the training data, but they will usually
be outperformed by a prior of ﬁnite strength that leaves room
for learning from experience (Fig. 3, and Figs. S5-6 in SI
Appendix). That is, the strong form of less-is-more, that one
can do better with heuristics by throwing out information
rather than using it, is false. The optimal solution always
uses all relevant information, but it combines that information
with the appropriate prior. In contrast, no amount of data can
overcome the heuristics’ inductive biases. The tallying heuris-
tic is deﬁned to entirely ignore diﬀerences in cue magnitude
Parpart et al.
August 30, 2017
|
5

and predictiveness, unlike the intermediate half-ridge models,
and cue validities are deﬁned to entirely ignore covariation
information, unlike the intermediate COR models.
Although the current contribution is formal in nature, it
nevertheless has implications for psychology. In the psycholog-
ical literature, heuristics have been repeatedly pitted against
full-information algorithms (2, 9, 10) that diﬀerentially weight
the available information or are sensitive to covariation among
cues. The current work indicates that the best-performing
model will usually lie between the extremes of ordinary linear
regression and fast-and-frugal heuristics, i.e., at a prior of
intermediate strength. Between these extremes lie a host of
models with diﬀerent sensitivity to cue-outcome correlations
in the environment.
One question for future research is whether heuristics give
an accurate characterization of psychological processing, or
whether actual psychological processing is more akin to these
more complex intermediate models. On the one hand, it could
be that implementing the intermediate models is computation-
ally intractable, and thus the brain uses heuristics because
they eﬃciently approximate these more optimal models. This
case would coincide with the view from the heuristics-and-
biases tradition of heuristics as a tradeoﬀof accuracy for
eﬃciency (6). On the other hand, it could be that the brain
has tractable means for implementing the intermediate models
(i.e., for using all available information but down-weighting
it appropriately).
This case would be congruent with the
view from ecological rationality where the brain’s inferential
mechanisms are adapted to the statistical structure of the envi-
ronment. However, this possibility suggests a reinterpretation
of the empirical evidence used to support heuristics: heuristics
might ﬁt behavioral data well only because they closely mimic
a more sophisticated strategy used by the mind.
Although we focused on explaining the success of two pop-
ular decision heuristics through a Bayesian analysis, our ap-
proach also suggests one could start with a Bayesian model
and attempt to derive a novel heuristic by strengthening the
prior. For example, in Gaussian Process regression with a
radial-basis kernel, the length-scale parameter determines how
similar a training example needs to be to a test item to signif-
icantly inﬂuence the model’s prediction. Taking the limit as
the length scale approaches zero might yield a heuristic akin
to the nearest neighbor algorithm, in which the prediction is
based solely on the most similar training item, ignoring all
other training data. Such a solution would be algorithmically
simple, but likely would be bested by models with interme-
diate prior strength. Whether this approach to deriving new
heuristics would prove fruitful is an open question for future
research.
There have been various recent approaches looking at the
compatibility between psychologically plausible processes and
probabilistic models of cognition (24–30). These investigations
are interlinked with our own, and while most of that work
has focused on ﬁnding algorithms that approximate Bayesian
models, we have taken the opposite approach. This contri-
bution reiterates the importance of applying fundamental
machine learning concepts to psychological ﬁndings (13). In
doing so, we provide a formal understanding of why heuris-
tics can outperform full-information models by placing all
models in a common probabilistic inference framework, where
heuristics correspond to an extreme prior that will usually be
outperformed by an intermediate model that uses all available
information.
ACKNOWLEDGMENTS.
We thank T. Noguchi, N. Bramley, E.
Schulz, and M. Speekenbrink for helpful comments. P.P. acknowl-
edges support from the UCL Centre for Doctoral Training in Fi-
nancial Computing & Analytics.
This work was supported by
the Leverhulme Trust (Grant RPG-2014-075), the NIH (Grant
1P01HD080679), and a Wellcome Trust Investigator Award (Grant
WT106931MA) to B.C.L., as well as The Alan Turing Institute under
the EPSRC grant EP/N510129/1, and a AFOSR grant FA9550-14-
1-0318 to M.J.. The code and datasets in this paper will be made
available at the Open Science Framework: https://osf.io/pb9yt/.
1.
Dawes RM (1979) The robust beauty of improper linear models in decision making. American
psychologist 34(7):571.
2.
Czerlinski J, Gigerenzer G, Goldstein DG (1999) How good are simple heuristics?, eds.
Gigerenzer G, Todd P, Group AR. (Oxford University Press, New York), p. 97–118.
3.
Gigerenzer G, Goldstein DG (1996) Reasoning the fast and frugal way: models of bounded
rationality. Psychol Rev 103(4):650–69.
4.
Martignon L, Hoffrage U (1999) Why does one-reason decision making work. A case study
in ecological rationality. (Oxford University Press, New York), pp. 119–140.
5.
Simon HA (1990) Invariants of human behavior. Annu Rev Psychol 41:1–19.
6.
Tversky A, Kahneman D (1974) Judgment under uncertainty: Heuristics and biases. Science
185(4157):1124–31.
7.
Kahneman D (2003) A perspective on judgment and choice: mapping bounded rationality.
Am Psychol 58(9):697–720.
8.
Goldstein DG, Gigerenzer G (2002) Models of ecological rationality: the recognition heuristic.
Psychol Rev 109(1):75–90.
9.
Chater N, Oaksford M, Nakisa R, Redington M (2003) Fast, frugal, and rational: How rational
norms explain behavior. Organizational behavior and human decision processes 90(1):63–
86.
10.
Katsikopoulos KV, Schooler LJ, Hertwig R (2010) The robust beauty of ordinary information.
Psychological Review 117(4):1259.
11.
Hogarth RM, Karelaia N (2007) Heuristic and linear models of judgment: matching rules and
environments. Psychol Rev 114(3):733–58.
12.
Gigerenzer G, Todd PM, Group AR (1999) Simple heuristics that make us smart. (Oxford
University Press).
13.
Gigerenzer G, Brighton H (2009) Homo heuristicus: why biased minds make better infer-
ences. Top Cogn Sci 1(1):107–43.
14.
Elman JL (1993) Learning and development in neural networks: the importance of starting
small. Cognition 48(1):71 – 99.
15.
Newport EL (1990) Maturational constraints on language learning.
Cognitive Science
14(1):11–28.
16.
Marr D (1982) Vision: A computational investigation into the human representation and pro-
cessing of visual information.
17.
Geman S, Bienenstock E, Doursat R (1992) Neural networks and the bias/variance dilemma.
Neural computation 4(1):1–58.
18.
Kohavi R (1995) A study of cross-validation and bootstrap for accuracy estimation and model
selection in Ijcai. Vol. 14, pp. 1137–1145.
19.
Pitt MA, Myung IJ (2002) When a good ﬁt can be bad. Trends Cogn Sci 6(10):421–425.
20.
Hoerl AE, Kennard RW (1970) Ridge regression: Biased estimation for nonorthogonal prob-
lems. Technometrics 12(1):55–67.
21.
Marquaridt DW (1970) Generalized inverses, ridge regression, biased linear estimation, and
nonlinear estimation. Technometrics 12(3):591–612.
22.
Ripley BD (2007) Pattern recognition and neural networks. (Cambridge University Press).
23.
Tsetsos K, et al. (2016) Economic irrationality is optimal during noisy decision making. Pro-
ceedings of the National Academy of Sciences 113(11):3102–3107.
24.
Bramley NR, Dayan P, Grifﬁths TL, Lagnado DA (2017) Formalizing neurath’s ship: Approxi-
mate algorithms for online causal learning. Psychological review 124(3):301.
25.
Daw N, Courville A (2008) The pigeon as particle ﬁlter.
26.
Jones M, Love BC (2011) Bayesian fundamentalism or enlightenment?
on the explana-
tory status and theoretical contributions of bayesian models of cognition. Behav Brain Sci
34(4):169–88.
27.
Lee MD, Cummins TD (2004) Evidence accumulation in decision making: unifying the "take
the best" and the "rational" models. Psychon Bull Rev 11(2):343–52.
28.
Sanborn AN, Grifﬁths TL, Navarro DJ (2010) Rational approximations to rational models:
alternative algorithms for category learning. Psychological review 117(4):1144.
29.
Scheibehenne B, Rieskamp J, Wagenmakers EJ (2013) Testing adaptive toolbox models: A
bayesian hierarchical approach. Psychological review 120(1):39.
30.
Grifﬁths TL, Lieder F, Goodman ND (2015) Rational use of cognitive resources: Levels of
analysis between the computational and the algorithmic. Topics in cognitive science 7(2):217–
229.
6
|
Parpart et al.

Figure Legends
1. Figure 1: Illustrative example of a binary prediction task
p2.
2. Figure 2: The concept of overﬁtting
p2.
3. Figure 3: Generalization performance of the half-ridge model
p4.
4. Figure 4: The COR model
p5.
Parpart et al.
August 30, 2017
|
7

Supporting Information Appendix
Parpart et al.
Supporting Information (SI)
Cue validities. Fast and frugal heuristics, such as TTB and
tallying, rely on cue validities for weights. Cue validities are
deﬁned for binary decision tasks, wherein two objects (e.g., two
soccer teams) are compared on several cues and the inference
is made about which object has the higher criterion value
(i.e., which team will win the match). The criterion variable
encodes the actual outcomes (e.g., which teams actually win
the soccer matches), and can be coded as -1 and +1 as in
Figure 1 (main text). Cue validities, v, reﬂect the probability
with which single cues can identify the correct alternative, and
can be derived as the proportion of correct inferences made
by each cue across a set binary comparisons (1):
v =
R
R + W
[S1]
where R = number of correct predictions, W = number of
incorrect predictions, and consequently, 0 Æ v Æ 1.
For example, Table S1 portrays a binary decision environ-
ment where ﬁve object comparisons are made on the basis of
three cues. Note that the computation of cue validities ignores
those cases where a cue predicts indiﬀerence between objects.
A fundamental diﬀerence between cue validities and the regres-
sion weights derived by linear regression is that cue validities
completely ignore covariance among cues. This is because
cue validities are computed in isolation of one another, only
considering how good each cue is at making correct inferences
about the criterion separately from all other cues. In contrast,
regression weights as estimated by a multiple linear regression
model always consider the covariation among cues, as seen in
the expression for the parameter estimate,
ˆw = (XT X)
≠1XT y,
[S2]
where XT X captures the covariances. In an ordinary linear
regression analysis with multiple cues, the covariance among
cues has a direct inﬂuence on the regression weights ˆw. If
the regression weights were instead derived by regressing the
criterion variable on each cue alone, i.e., eliminating all other
cues from the model (single-predictor regression analysis), the
weight magnitudes, valences as well rank order of weights
would change.
It can be shown that cue validities are a
linear transformation of single-predictor regression weights (1),
according to the following relationship:
ˆw = 2v ≠1.
[S3]
This relation holds because, when there is a single predictor
(x), the XT X term in Eq. S2 is equal to the number of cases
where the predictor makes a prediction (x = ±1), with cases
where the predictor is indiﬀerent (x = 0) excluded. This can
be seen from the computation in Table S1. That is,
xT x = R + W.
[S4]
At the same time, xT y counts up all cases where a cue predicts
the criterion (i.e., xi = xi) and subtracts those cases where
the cue makes the opposite prediction (i.e., xi = ≠xi), while
ignoring indiﬀerent cases of xi = 0 (see Table S1). Thus
xT y = R ≠W.
[S5]
Therefore, the single-predictor regression coeﬃcient estimate
ˆw can be reformulated as
ˆw = R ≠W
R + W
=
2R
R + W ≠R + W
R + W
= 2v ≠1.
[S6]
Note also that the expression R≠W
R+W in the ﬁrst line of Eq. S6
represents the Goodman-Kruskal rank correlation (1). The
linear relationship in Eq. S3 reveals that cue validities are a
positive linear rescaling of single-predictor regression weights.
Therefore they yield the same predictions in binary compar-
isons.
Simulation 1: Reanalysis of a heuristic dataset (Fig. 2B). Lin-
ear regression and the TTB heuristic were both ﬁt to one of the
original 20 datasets reported by the ABC Research Group (2).
In these original simulations ((2)), the continuous values were
transformed to binary values of 0 and 1 by median split. The
criterion variable of the dataset analyzed in Fig. 2B encodes
which of two houses has a higher actual sales price. There are
10 cues, which include things like the number of bedrooms,
number of ﬁreplaces, number of garage spaces, living space,
current taxes, and the age of the house. We created all 231
possible pairwise comparisons of the original 22 houses. Both
the linear regression model and TTB were cross-validated on
the dataset by splitting the total number of pairwise compar-
isons randomly into training and test sets. The size of the
training set was 20 comparisons (≥9% of all comparisons) or
100 comparisons (≥43% of all comparisons), and the test set
was always the complementary set of comparisons. For each
training set size, the cross-validation split into training and
test sets was repeated 1000 times and performance of each
model was averaged across these replications. Fig. 2B in
the main text demonstrates the generalization performance,
i.e., the out-of-sample performance, of both multiple linear
regression and TTB as a function of the training set size (small
or large). Error bars in Fig. 2B represent the variation in
performance across all cross-validation splits, expressed as
standard errors of the mean.
To summarize, these were the parameters of the
dataset as presented in Fig. 2B:
Parpart et al.
1 of 11

Number of objects
22
Number of pairwise comparisons
N = 231
Number of cues
m = 10
Class variable (which house had
the higher actual sales price)
Binary, ±1
Absolute
correlation
between
cues averaged over cue pairs
0.35
Sample cue validities
[1.00, 0.99, 0.94, 0.88, 0.83, 0.76,
0.73, 0.73, 0.72, 0.31]
Small training sample size
20 (≥9% of all pairwise compar-
isons)
Large training sample size
100 (≥43% of all pairwise compar-
isons)
Test sample size
N ≠20, N ≠100
Number of cross-validation repeti-
tions
1000
Simulation 2: Generalization performance of the Bayesian
half-ridge model in classic datasets (Fig. 3). The goal of this
simulation was to explore the predictive performance of the
Bayesian half-ridge model in real-world datasets that have
been previously used to evaluate heuristics on (2), and as a
function of factors such as training sample size. The main
text demonstrates the model’s performance in all original
20 heuristic datasets reported by the ABC Research Group
(2) (Fig.
3).
These datasets span various domains from
psychology to biology, health and environmental science, and
range from predicting house prices and predicting mammals’
sleep time to predicting the attractiveness of famous men and
women. The number of predictors varies from 3 (ﬁsh fertility
dataset) to 18 (high school dropout dataset). In these classic
datasets, the attributes are discretized at their medians into
values of 0 and 1 from originally continuous data. For each
dataset, we created all possible pairwise comparisons of the
objects, with attributes coded as 0, 1 or -1 for each pair. The
dependent variable was always binary and coded as -1 and
+1.
The Bayesian half-ridge model was cross-validated on each
dataset by splitting the total set of pairwise comparisons
randomly into training and test sets. The size of the training
set was varied between 10, 20 and 115 comparisons, and the
test set was always the complementary set of comparisons. As
two of the datasets, Oxygen and Ozone, only have 91 and 55
object pairs in total respectively, the large training sample size
of 115 was excluded for those datasets. For each training set
size, the cross-validation split into training and test set was
repeated 1000 times and performance was averaged across all
of these splits. Error bars in Fig. 3 represent the variation in
performance across all 1000 cross-validation splits, expressed as
standard errors of the mean. The half-ridge model predictions
were derived by calculating the posterior weights from the
training set using Eq. S13 below. The truncation used in Eq.
S13 (i.e., the choice of orthant O) depended on the actual
cue directions in the full dataset, following the assumption
that the cue directions are known in advance. We derived a
diﬀerent posterior distribution for the weights under each value
of the strength of the Bayesian prior (i.e., 1/÷2 in Eq. S13).
Next, we used the mean of the posterior to make predictions
for all comparisons in the test set. To assess the half-ridge
model’s predictive accuracy on each test set, the predictions
were compared to the actual binary criterion values in the
test set, e.g., in the house price dataset this refers to which
of two houses had the higher sales price. The model’s overall
generalization performance was then computed as the average
predictive accuracy across all 1000 test sets.
The performance results are depicted in Fig. 3 of the main
text which contains results for small, medium and large train-
ing sample sizes (10, 20 and 115 pairwise comparisons). The
ﬁgure demonstrates the generalization performance of the half-
ridge model for a range of 1/÷2 = [1000000, 100000, 1000, 700,
330.08, 156.81, 74.50, 35.39, 16.81, 7.99, 3.80, 1.80, 0.86, 0.41,
0.19, 0.09, 0.03, 0.01, 0.001, 0.0001, 0.00001]. Importantly, an
approximately inﬁnite value of 1/÷2 in the half-ridge model cor-
responds to the directed tallying heuristic (deﬁned in the main
text), as explained in the mathematical derivations below (Eq.
S16 and Eq. S17). Crucially, in the cross-validated evaluation
of the half-ridge model, all models including heuristics and
full regression are on the same level playing ﬁeld. That is, our
formulation places the heuristic within the same framework
as other Bayesian models with the same prior, varying only in
prior strength. The optimal prior strength (i.e., the best model
within the continuum) may vary from one domain to another,
but other than this choice of free parameter, the half-ridge
model can be straightforwardly applied in any settings where
the heuristic can.
We found that in all 20 datasets, the performance peaked
for strengths of the prior lying between the two extremes of full
regression (i.e., 1/÷2 = 0) and the directed tallying heuristic
(i.e., 1/÷2 = Œ).
For all training set sizes, the directed
tallying heuristic (as approximated by 1/÷2 = 1000000) is
outperformed by full regression, but intermediate models
performed best. The reason that there are no less-is-more
eﬀects here (i.e., tallying outperforming regression) is that
cue directions are not learned by the half-ridge model, as the
assumption in the half-ridge model is that cue directions are
known in advance (see mathematical derivations in section on
Bayesian half-ridge model below). Thus 1/÷2 = 0 does not
correspond to ordinary linear regression, but to a variant in
which the weight estimates are constrained to have the correct
signs (i.e., to lie in O). This means that there is less scope for
the more ﬂexible regression model to incorrectly estimate the
true weights. In comparison, when we run ordinary regression
(with unconstrained weights) on these datasets, we replicate
the less-is-more eﬀects previously found in these datasets
(2, 3) as can be seen in the COR model simulations of Fig. S5.
Parameters in the 20 datasets as presented in Fig. 3:
Number of objects
11 to 395
Number of pairwise comparisons
N = 55 to N = 77815
Number of cues
m = 3 to m = 18
Class variable (e.g., which house
had the higher actual sales price)
Binary, ±1
Absolute
correlation
between
cues averaged over cue pairs
range = 0.12 to 0.63, mean = 0.31,
median = 0.28, sd = 0.14
Training sample size
10, 20, 115
Test sample size
N ≠10, N ≠20, N ≠115
Number of cross-validation repeti-
tions
1000
Error variance
‡2
Á = 1
Strength of prior
1/÷2 = [1000000, 100000, 1000,
700, 330.08, 156.81, 74.50, 35.39,
16.81, 7.99, 3.80, 1.80,0.86, 0.41,
0.19,
0.09,
0.03,
0.01,
0.001,
0.0001, 0.00001]
2 of 11
Parpart et al.

In the current simulations, we deﬁned training sets by sam-
pling a subset of all possible comparisons (i.e., object pairs).
In some past work, training sets have been deﬁned by sampling
a subset of the objects and then training on all pairs within
the sampled subset. We have found both methods in the lit-
erature, e.g., sampling comparisons (4) and sampling objects
(2). To determine whether our results reported here would
be dependent on this sampling decision, we compared both
sampling methods. In short, the qualitative pattern of results
is not dependent on the sampling method. When sampling
objects rather than comparisons, we varied the training sample
size between sampling 5, 7 and 16 objects, which correspond
to 10, 21 and 120 possible comparisons for the training sets,
respectively. We chose these training sample sizes to roughly
match the training sample sizes used for the half-ridge simula-
tions when sampling comparisons (i.e., 10, 20 and 115 training
cases in Fig. 3). The pattern of results is almost the same
under both sampling methods. The performance of models
with extremely strong priors (i.e., directed tallying heuristic) is
approximately the same under both methods (with some small
error) and so is the performance of models with priors of zero
strength (i.e., ordinary regression). Also, the location of the
intermediate peak is approximately the same (with some small
error) for both sampling methods in each of the 20 datasets.
Simulation 3: Agreement between the Bayesian COR model
and heuristics (Fig. S3 and Fig. S4). This simulation demon-
strates how the COR model converges to the heuristics (i.e.,
tallying and TTB) as a function of the model’s prior strength
in an artiﬁcial dataset. In order to generate the COR model’s
predictions for artiﬁcial pairwise comparisons, the posterior
weights (i.e., the model’s knowledge representations) were
paired with either a TTB or a tallying decision rule.
We
simulated 1000 similar datasets overall and the model’s perfor-
mance was averaged across all of them. Each artiﬁcial dataset
was created as follows: The dataset had m = 3 cues (e.g., cues
in Fig. 1 would be rank, last game result, home vs. away
match, and number of goals scored). We generated cue values
on these three cues for 20 objects by uniformly sampling cue
values of 0 or 1. These cue values refer to the positive and
negative smileys in the illustrative example of Fig. 1. An
object refers to a single item (e.g., a soccer team in Fig. 1),
not a pair of items to be compared.
We then created all
possible pairwise comparisons of the 20 objects which results
in 190 possible comparisons. A single pairwise comparison is
like comparing two soccer teams, e.g., Team Germany versus
Team England. For each pair, we computed the cue diﬀerence
vector by subtracting the cue values of the second object from
the ﬁrst object. For example, in Fig. 1, the third column
contains these cue diﬀerence values, which can take values
of 1, -1 and 0. Next, we created a matrix of cue diﬀerence
vectors with one row for each object pair. For each of the
1000 simulated datasets, we sampled m = 3 weights from an
exponential distribution with rate parameter equal to 2 as
generating weights. Finally, we calculated a criterion vari-
able by relying on the cue diﬀerences matrix, the generating
weights, and additional Gaussian noise. The criterion variable
contains the outcome for each object comparison, indicating
which object won the comparison. If the cue-diﬀerences matrix
is X, the vector of generating weights —, and the Gaussian
noise Á, then the criterion variable y was generated through
matrix multiplication,
y = X— + Á.
[S7]
The continuous y variable was thresholded at zero into +1
and -1 to indicate which object won the competition. Thus,
y is coded in the same way as the cue diﬀerences above,
with -1 indicating the second object won the competition
and +1 indicating the ﬁrst object won. All models, i.e., the
COR model, the heuristics, and ordinary linear regression,
were trained on this artiﬁcial dataset of 190 comparisons,
and subsequently made predictions for a novel test set. The
predictions on the test set were used to measure agreement
among the diﬀerent models. The test set was constructed
according to a complete sampling approach where each possible
combination of cue diﬀerences occurs once. For three cues
with possible cue diﬀerence values of {≠1, +1, 0}, there are
27 possible cue combinations. However, we deleted the test
item that has zeros on all cue values, as it does not provide
any information for discriminating among models (all models
would guess for this comparison).
Hence, the test matrix
contains 26 test comparisons, one in each row. Each test pair
corresponds to a novel pairwise comparison, e.g., between two
soccer teams.
Linear regression was ﬁt to the training set to estimate the
ordinary least squares regression coeﬃcients, and then cross-
validated by predicting the unseen 26 test items with the ﬁtted
optimal regression coeﬃcients in matrix multiplication. The
initial predictions are continuous and therefore were binarized
by taking the signs of these predictions. Both the TTB and
the tallying heuristic were ﬁt to the artiﬁcial training set
by estimating the cue validities according to Eq. S1. After
learning the cue validities from the training sample, both
heuristics made predictions with respect to the novel test
pairs.
The TTB heuristic makes predictions for each test
pair by sequentially searching through cues in order of their
validity until a ﬁrst cue discriminates among the alternatives
(i.e., its diﬀerence value is nonzero). The discriminating cue’s
value (±1) is subsequently used for prediction. Tallying, in
contrast, simply learns the signs of the cue validities, i.e.,
their unit weights (+1 when validity is greater 0.5, or -1 when
validity is below 0.5), neglecting all validity magnitudes. At
test, tallying then applies the unit weights to the unseen test
pairs and counts up the positive and negative evidence for
each alternative. The alternative with more evidence in its
favor wins the comparison and is used for prediction.
To
derive the COR model predictions, we estimated the posterior
weight matrix from the training set using the exact Bayesian
posterior mean as detailed below in the section on COR model
predictions (i.e., Eq. S23). As we were interested in the change
of the posterior weight matrix as a function of the strength
of the prior in the model, we derived a diﬀerent posterior
estimate for each value of the strength of the prior. Next, we
used the mean posterior weight matrix to make predictions
with respect to the test set via matrix multiplication. If the
cue diﬀerences for the test set are represented by a matrix
M containing m = 3 columns and 26 rows, and the mean
posterior weight matrix Wú is a 3 ◊3 square matrix, then
by matrix multiplication, the output is also a matrix Y with
dimensions 26 ◊3,
Y = MWú.
[S8]
Parpart et al.
3 of 11

The output matrix Y contains the continuous predictions
of the Bayesian model with respect to the three copies of y
(see section on COR model below). In order to convert this
output matrix into the model’s choices, a TTB or a tallying
decision rule was applied to each row of the output matrix
as explained in the main text (Eq.
6 and 8).
Lastly, to
measure convergence between the COR model (with the TTB
or tallying decision rule) and the TTB heuristic or the tallying
heuristic, we computed the agreement between models by
dividing the number of equal predictions made on the test
set by the total number of test comparisons. The agreement
between the COR model and ordinary linear regression was
computed in the same way. The simulation results of the COR
model with the TTB decision rule are displayed in Fig. S3.
The simulation results of the COR model with the tallying
decision rule are displayed in Fig. S4.
The convergence ﬁndings were also veriﬁed by estimating
the posterior mean through Markov chain Monte Carlo
(MCMC) to sample from the true posterior probability
distribution over the weight matrix Wú.
To summarize, these were the parameters of the
artiﬁcial dataset as presented in Fig. S3 and Fig. S4:
Number of objects
20
Number of pairwise comparisons
N = 190
Number of cues
m = 3
Class variable
Binary, ±1
Absolute
correlation
between
cues averaged over cue pairs
0.26
Generating weights
randomly sampled from an exponen-
tial distribution with rate parameter
equal to 2
Training Sample Size
190
Test Sample Size
26
Number of cross-validation repeti-
tions
1000
Error variance
‡2
Á = 1
Strength of prior
1/÷2
= [1E+06, 8E+05, 5E+05,
1E+05, 10000, 1000, 700, 600, 500,
400, 330.08, 200, 156.81, 74.50,
35.39, 16.81, 7.99, 3.80, 1.80, 0.86,
0.41, 0.19, 0.09, 0 ]
Simulation 4: Generalization performance of the COR model
in heuristic datasets (Fig. S5 and Fig. S6). The goal of this
simulation was to explore the predictive performance of the
COR model in real-world datasets, and as a function of factors
such as training sample size. The supplementary ﬁgures, Fig.
S5 and Fig. S6, demonstrate simulations of the COR model
on all original 20 heuristic datasets reported by the ABC
Research Group (2) that were also used to test performance of
the half-ridge model in Fig. 3 of the main text, as described
in simulation 2 of the SI above.
In these classic datasets, the attributes are discretized at
their medians into 0 and 1 (from originally continuous data).
We created all possible pairwise comparisons of the objects,
which ends up in attribute data containing the possible values
0, 1 or -1. The dependent variable was always binary and
coded as -1 or +1. The COR model was cross-validated on each
dataset by splitting the total number of pairwise comparisons
randomly into training and test set. The size of the training
set was varied between 10, 20 and 115 comparisons, and the
test set represented the complementary set of comparisons
always. For each training set size, the cross-validation split into
training and test set was repeated 1000 times and performance
was averaged across all of them. Error bars in Fig. S5 and Fig.
S6 represent the variation in performance across all thousand
cross-validation splits, expressed as standard errors of the
mean.
The COR model predictions were derived by calculating
the posterior weights based on the training set using the exact
Bayesian posterior as in Eq. S23 below. That means, we
could compute the posterior mean according to Eq. S23, by
calculating the posterior weights for each copy of y one at
a time. Next, we used the mean posterior weight matrix to
make predictions with respect to the test set. We also vali-
dated these results with Markov chain Monte Carlo (MCMC)
which samples directly from the Bayesian posterior over weight
matrices. Since the Bayesian prior’s strength is represented
by 1/÷2, we derived a new posterior mean for each value of
1/÷2. At the prediction stage, for each value of 1/÷2, the mean
posterior weight matrix was used to make predictions with
respect to the test set via matrix multiplication (Eq. S8), and
was then combined with either of the two decision rules. To
assess the COR model’s predictive accuracy, the predictions
were compared to the actual criterion values in the test set,
e.g., which of two houses had the higher sales price. When
any of the models predicted a tie, i.e., a prediction of 0 which
means the model is indeterminate about the binary outcome
(label -1 or +1), the models were assumed to guess.
The performance results depicted in Fig. S5 represent the
COR model with the tallying decision rule, while Fig. S6
displays the results for the TTB decision rule. The graphs
in Fig.
S5 and Fig.
S6 demonstrate the generalization
performance of the COR model for diﬀerent prior’s strengths
1/÷2 = [0.00001,0.0001,0.001, 0.01, 0.03, 0.09, 0.19, 0.41, 0.86,
1.80, 3.80, 7.99, 16.81, 35.39, 74.50, 156.81, 330.08, 700, 1000,
1000000], under the condition of varying training set sizes,
i.e., 10, 20 and 115 of the pairwise comparisons.
We found that, in eleven out of the 20 datasets, a less-
is-more eﬀect could be observed where the heuristic model,
e.g., the tallying heuristic (Fig. S5) (or COR with inﬁnitely
strong prior), outperformed ordinary linear regression (prior
strength of zero), with 10 and 20 training cases. However,
the maximal performance was found for intermediate models,
i.e., intermediate prior strengths, across all 20 datasets, and
roughly in the same place across training sample sizes of 10,
20 and 115 training cases. This echoes a central ﬁnding from
the half-ridge model (Fig. 3) where the performance peak
could also be found in the middle, between the extremes
of the heuristic and the full regression model. Results for
the TTB decision rule were similar (Fig. S6), as the TTB
heuristic (or COR with inﬁnitely strong prior) outperformed
ordinary linear regression (prior strength of zero), in eighteen
out of the 20 heuristic datasets with 10 and 20 training cases.
Again, the performance peak could usually be found in the
middle, i.e., for medium-strength priors.
4 of 11
Parpart et al.

Parameters in the 20 datasets as presented in Fig.S5
and Fig. S6:
Number of objects
11 to 395
Number of pairwise comparisons
N = 55 to N = 77815
Number of cues
m = 3 to m = 18
Class variable (e.g., which house
had the higher actual sales price)
Binary, ±1
Absolute
correlation
between
cues averaged over cue pairs
range = 0.12 to 0.63, mean = 0.31,
median = 0.28, sd = 0.14
Training sample size
10, 20, 115
Test sample size
N ≠10, N ≠20, N ≠115
Number of cross-validation repeti-
tions
1000
Error variance
‡2
Á = 1
Strength of prior
1/÷2 = [1000000, 100000, 1000,
700, 330.08, 156.81, 74.50, 35.39,
16.81, 7.99, 3.80, 1.80,0.86, 0.41,
0.19,
0.09,
0.03,
0.01,
0.001,
0.0001, 0.00001]
As with the half-ridge simulations, the COR simulations
reported here deﬁned training sets by directly sampling pairs
of objects (i.e., comparisons). We compared this approach
to one of sampling objects (and training on all pairs in the
sampled subset), to determine whether our results would be
dependent on this sampling decision.
In short, the quali-
tative pattern of results is not dependent on the sampling
method. When sampling objects rather than comparisons, we
varied the training sample size between sampling 5, 7 and
16 objects, which correspond to 10, 21 and 120 possible com-
parisons, respectively. We chose these training sample sizes
to approximate the training sample sizes used for the COR
simulations when sampling comparisons (i.e., 10, 20 and 115
training cases in Figs. S5 and S6). For both the tallying and
the TTB decision rule, the pattern is almost the same under
both sampling methods. Performance of all models is lower
overall by a few percent in accuracy when sampling objects,
which makes sense as the models do not encounter test objects
in the training set ﬁrst. Additionally, models with weaker
priors (i.e., closer to ordinary regression) showed a larger drop
in performance under object sampling (especially for smaller
training sizes) than did models with stronger priors (i.e., closer
to the heuristics). Thus, sampling objects gives the heuristics
a small advantage over ordinary regression for the training
sample sizes considered here. However, the number of less-
is-more eﬀects (i.e., datasets in which heuristics outperform
ordinary regression) is the same and they occur in the same
environments for both sampling methods. Also, the location
of the performance peak is the same (with some small error)
under both sampling methods for both the TTB and tallying
decision rules.
I. Bayesian Half-ridge model. The half-ridge model’s prior is
a truncated Normal distribution, equivalent to assuming the
cue directions are known in advance (5):
w ≥N (0, Σ)|wœO ,
[S9]
The restriction notation |w œ O indicates we truncate the
distribution to one orthant O µ Rm, deﬁned by the prede-
termined directionalities of the cues, and renormalize. The
covariance matrix Σ assumes the weights are all independent
with variance ÷2 (prior to truncation):
Σ = ÷2I.
[S10]
Linear regression with an untruncated Gaussian prior (i.e.,
L2 regularization) yields a Gaussian posterior for the weights,
having mean
!
XT X + ‡2Σ≠1"≠1 XT y
[S11]
and variance
‡2 !
XT X + ‡2Σ≠1"≠1 .
[S12]
The posterior for the half-ridge model inherits the trunca-
tion from the prior and is otherwise unchanged except for
renormalization:
w|X, y ≥N
1 !
XT X + ‡2Σ≠1"≠1 XT y,
‡2 !
XT X + ‡2Σ≠1"≠1 2
|wœO
.
[S13]
To understand how the posterior behaves as the prior becomes
arbitrarily strong, we can rescale the weights by 1/÷ and
substitute Eq. S13 to rewrite the posterior as
w
÷
---- X, y ≥N
1
÷ !
÷2XT X + ‡2I"≠1 XT y,
3
÷2
‡2 XT X + I
4≠1 2
|wœO
.
[S14]
Rescaling the weights has no impact in a binary comparison
task, so we can work with the distribution of w/÷ in place
of that of w. The convenience of this rescaling is that the
resulting distribution obeys a simple convergence:
w
÷
dæ N (0, I)|wœO as ÷ æ 0
[S15]
Therefore all weights converge to the same value, namely
lim
÷æ0 E
5
wi
÷
----X, y
6
= ±
Ú
2
ﬁ,
[S16]
with signs determined by each cue’s assumed directionality. In
particular, for any two weights j and k, their ratio converges
to unity:
lim
÷æ0
E [wj|X, y]
E [wk|X, y] = 1.
[S17]
Therefore the optimal decision-making strategy converges to
a simple equal-weight strategy, or tallying strategy.
To understand this result intuitively, refer to Eq.
S12
and Eq. S11 and note that the posterior mean and posterior
variance both scale with the prior’s covariance matrix Σ as Σ
approaches 0 (i.e., as the precision of the prior approaches
Œ). Thus, the mean of each weight goes to 0 faster than
its standard deviation, or in other words the coeﬃcient of
variation goes to 0. That fact is not consequential when the
directions of the cues are unknown, but it is signiﬁcant when
the cue directions are known. In the latter case, the signs of w
become the most important information the learner has, and
in fact the training data become (in the limit ÷ æ 0) irrelevant.
Note that there is also an alternative approach to building
the model continua within a (Bayesian) logistic regression
Parpart et al.
5 of 11

framework rather than the linear one used here. We believe
this would be a useful avenue for further research, as part of a
general program to build on the theoretical ideas introduced
here to develop new, more powerful decision algorithms and
to further link heuristics to other modeling approaches. For
now, we note that the linear models we used here support the
main conclusions just as well although they are not ideally
tailored to the task being analyzed (i.e., where criterion values
are binarized). The linear models were chosen in order to be
consistent with past work (e.g., (2)) to replicate less-is-more
eﬀects and in order to build a continuum between these models
traditionally used in the heuristic literature.
As with the
comparison between half-ridge and COR, and the observation
that diﬀerent choices of regularization schemes (corresponding
to Gaussian vs. Laplacian priors) lead to the same heuristics
in the limit, we conjecture that heuristics can arise as limiting
cases of many diﬀerent Bayesian models that assume diﬀerent
generative processes.
II. Bayesian COR model. The COR model approach diﬀers
from standard regularized regression in that the prior modu-
lates sensitivity for covariation among cues. This is achieved
by expressing the regression problem in multivariate terms,
by replicating the criterion variable y as many times as there
are cues (i.e., m times). Due to this multiplexing, the model
architecture implements m regression problems at once, mean-
ing the criterion variable y is regressed onto all cues m times
(Fig. S1). The weights constitute an m ◊m matrix W, with
each column, W·j, representing the weights for the jth copy
of the outcome, yj:
W =
S
WU
w11
· · ·
w1m
...
...
...
wm1
· · ·
wmm
T
XV .
[S18]
As in standard regression, the likelihood for each yj is given
by a Gaussian with error variance ‡2:
p(yj|X, W) Ã exp
3
≠(XW·j ≠yj)T (XW·j ≠yj)
2‡2
4
[S19]
where X is the matrix that contains the cue data and is indexed
by trials and cues (i.e., n ◊m).
In contrast to ridge regression, where all weights are penal-
ized equally, in the COR model only the oﬀ-diagonal elements
of the weight matrix W are penalized, while the diagonal
weights are left unpenalized. This is implemented by assuming
an improper uniform prior on all Wii (1 Æ i Æ m) and a prior
of N !
0, ÷2"
for all Wij (i ”= j). The joint distribution on
W treats all weights as independent. The model architecture
is illustrated in Fig. S1, were the solid arrows represent the
diagonal weights (direct weights) and the dashed arrows repre-
sent the oﬀ-diagonal weights (cross-weights). Penalizing only
the cross-weights has the eﬀect that the strength of the prior
(1/÷2) modulates the model’s sensitivity to covariation among
cues. When 1/÷2 = 0 (uniform prior on all weights), the poste-
rior for the weights W·j is identical for all yj, with mean (and
mode) equal to the ordinary least squares linear regression
solution. As 1/÷2 æ Œ, the estimated cross-weights converge
to zero, while the direct weights stay un-penalized. Thus in the
limit the direct weight wjj is the only nonzero weight in each
column W·j. This means that each cue eﬀectively has its own
isolated regression (i.e., as if only direct weights were present
in Fig. S1, with no cross-weights). These single-predictor
regression weights are linear transforms of the cue validities as
used by the heuristics (see proof in Eq. S6). Therefore, in the
limit, when the COR model weights are paired with a decision
rule (Eq. 6 or 8), the model’s behavior converges to that of
the respective heuristic.
To derive the posterior distribution for COR’s weight ma-
trix, we observe ﬁrst that the weights for the diﬀerent copies
of y are decoupled. More precisely, the prior, likelihood, and
hence posterior all factor into separate functions, one for each
set of weights W·j. Therefore we can derive the posterior
separately for each set. The prior for each set of weights is
given by
p(W·j) Ã exp
1
≠1
2WT
·jΣW·j
2
[S20]
where Σ is the precision matrix, deﬁned by Σjj = 0, Σii =
1
÷2
for i ”= j, and Σik = 0 for i ”= k. Combining this with the
likelihood in Eq. S19 yields the posterior:
p(W·j|X, y) Ã exp
1
≠1
2WT
·j
1
Σ + 1
‡2 XT X
2
W·j + 1
‡2 WT
·jXT y
2
[S21]
Ã exp
A
≠1
2‡2
1
W·j ≠!
Λ + XT X"≠1XT y
2T
!
Λ + XT X" 1
W·j ≠!
Λ + XT X"≠1XT y
2B
.
[S22]
That is, the posterior for W·j is a multivariate Gaussian with
mean at
!
Λ + XT X"≠1XT y
[S23]
and covariance matrix equal to
‡2!
Λ + XT X"≠1.
[S24]
The matrix Λ is interpretable as a matrix of penalties on the
components of W·j, with Λjj = 0, Λik = 0 for i ”= k, and
Λii = ‡2
÷2 for i ”= j.
In the current work, this exact Bayesian solution of the
COR model was used for deriving the model’s predictions in
Simulation 3 (Fig. S3 and Fig. S4) and Simulation 4 (Fig. S5
and Fig. S6), by inserting the appropriate training data into
Eq. S23 for the posterior mean weights. To derive the COR
model’s predictions with respect to new test items, the poste-
rior mean was multiplied with the test cue data to generate
outputs (Eq. 5). Note that, by linearity, using the posterior
mean gives the same result as integrating the prediction over
the full posterior. These outputs were then combined with
the tallying or TTB decision rule (Eq. 6 or 8) to classify a
test item. As detailed in the main text, the posterior weight
matrix changes with the strength of the prior, 1/÷2 in Λ in
Eq. S23, and a diﬀerent posterior weight matrix is estimated
for each value of ÷.
Importantly, in contrast to the half-ridge model, the COR
model is misspeciﬁed, because of the multiplexing of the crite-
rion variable. The resulting model architecture is artiﬁcially
6 of 11
Parpart et al.

multivariate despite the original prediction problem being uni-
variate. Nevertheless, the COR model opens up new insights
into the role of cue covariance in establishing a continuum be-
tween heuristics that rely on cue validity and full-information
models. Penalizing only the cross-weights in the COR model
architecture results in a regularization of covariance sensitivity
in the model, with a continuum ranging from ordinary linear
regression (fully sensitive to the covariance structure among
cues) to heuristics that rely on cue validities (insensitive to
any cue covariance).
Note that a reviewer also suggested an alternative approach
to building the model continua within a (Bayesian) logistic
regression framework rather than the linear one used here. We
believe this would be a useful avenue for further research, as
part of a general program to build on the theoretical ideas intro-
duced here to develop new, more powerful decision algorithms
and to further link heuristics to other modeling approaches.
For now, we note that the linear models we used here support
the main conclusions just as well although they are not ideally
tailored to the task being analyzed (i.e., where criterion values
are binarized). The linear models were chosen in order to be
consistent with past work (e.g., (2)) to replicate less-is-more
eﬀects (e.g., such as the less-is-more ﬁndings in Figs. S5 and
S6) and in order to build a continuum between these mod-
els traditionally used in the heuristic literature. However, as
with the comparison between half-ridge and COR, and the
observation that diﬀerent choices of regularization schemes
(corresponding to Gaussian vs. Laplacian priors) lead to the
same heuristics in the limit, we conjecture that heuristics can
arise as limiting cases of many diﬀerent Bayesian models that
assume diﬀerent generative processes.
Similarly, we chose the two particular heuristics, i.e., TTB
and tallying, as they are among the most well-known fast-and-
frugal heuristics, and because they are intuitive and arise in
a number of contexts. Both heuristics have been repeatedly
contrasted with “rational” full-information linear regression
approaches (2, 3, 6), which makes them very suitable for
consideration as part of a Bayesian inference model for our
purpose.
However, including other heuristics will provide
a great extension of the current Bayesian program to better
understand heuristics and their relationship to full-information
models and Bayesian inference models. Fortunately, analyses
with the initial two heuristics proved tractable, providing an
opportunity to argue that less is not more when less involves
ignoring (rather than down-weighting) information.
࢞ଵ
ݕ
ݕ
࢞ଶ
࢞ଷ
ݕ
࢝૚૚
࢝૛૚
࢝૜૚
࢝૚૜
࢝૛૜
࢝૜૛
࢝૛૛
࢝૚૛
࢝૜૜
Fig. S1. COR model architecture with m = 3 cues as presented in Fig. 4 of the main
paper. All y variables are replicas of one another and contain the same outcome
information. Dashed arrows are called cross-weights, and solid arrows are called
direct weights. Weight indices refer to the weight matrix W (Eq. S18).
Full Bayesian with Gaussian prior
Ridge Regression
Ordinary Linear Regression
Half-ridge Model
Directed Tallying
i
Di
t d
Li
Mode/mean of full posterior
Predetermined cue directionalities 
Limit of infinitely
strong prior
Fixing prior 
strength to 0
Fig. S2. Formal relationships among full Bayesian regression, ridge regression,
ordinary least-squares linear regression, the Bayesian half-ridge model, and the
directed tallying heuristic. The lower-right arrow represents the main contribution of
this paper, that a heuristic is a limiting case of Bayesian inference (here, the half-ridge
model) with an inﬁnitely strong prior.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
92
94
96
98
100
1e+00
1e+02
1e+04
1e+06
Model
●
●
OLS Regression
TTB Heuristic
Agreement (%)
Strength of the prior 
Fig. S3. Agreement between the COR model (with TTB decision rule) and the TTB
heuristic, as well as ordinary linear regression, as a function of the strength of the
prior. As expected, agreement (i.e., proportion of equal predictions on test items)
between the Bayesian COR model and TTB heuristic increased with a stronger
prior, reaching an asymptote of perfect agreement as 1/÷2 approached inﬁnity. The
opposite pattern held for ordinary linear regression, with agreement being perfect at
1/÷2 = 0 and declining as the prior strength increases. Parallel results hold for the
tallying decision rule (Fig. S4). The ordinate indicates the percentage agreement on
test item choices in a simulated binary decision task with three cues. The simulated
task contained 20 objects (e.g., an object represents a soccer team in Fig. 1), and
the training set comprised all 190 possible pairwise comparisons of the objects. The
test set represented all possible combinations of three cues which can take values
of {≠1, +1, 0} (coding scheme followed pattern in Fig. 1), and contained 26 cue
combinations. The simulation process was repeated 1000 times, and error bars
represent ± SEM across simulation runs. More details are provided in the Simulation
3 text.
Parpart et al.
7 of 11

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
70
80
90
100
1e+00
1e+02
1e+04
1e+06
Model
●
●
OLS Regression
Tallying Heuristic
Agreement (%)
Strength of the prior 
Fig. S4. Agreement between the COR model (with tallying decision rule) and the
tallying heuristic, as well as ordinary linear regression, as a function of the prior
strength. The ordinate reﬂects the percentage agreement on test item choices in an
artiﬁcial dataset. The artiﬁcial dataset used for this simulation is equivalent to the one
displayed in Fig. S3 and described in the Simulation 2 text. Error bars represent ±
SEM across simulation runs.
8 of 11
Parpart et al.

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
House Prices
Mortality Rates
City Size
Professors' Salaries
Body Fat
Car Accidents
Cloud Rainfall
High School Dropouts
Obesity
Fuel Consumption
Biodiversity
Homelessness
Land Rent
Mammals' Sleep
Oxidants in L.A.
Attractiveness Men
Attractiveness Women
Fish Fertility
Oxygen
Ozone in S.F.
65
70
75
80
85
60
65
70
75
80
60
65
70
72.5
75.0
77.5
80.0
52
54
56
60
65
70
75
57.5
60.0
62.5
65.0
67.5
55
60
65
60
65
70
67.5
70.0
72.5
75.0
77.5
70.0
72.5
75.0
77.5
80.0
56
58
60
62
64
68
72
76
80
65
70
75
64
66
68
70
68
70
72
64
66
68
70
70
71
72
73
74
74
75
76
67
68
69
70
71
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
TrainingSize
●
●
●
10
20
115
Generalization performance (%)
Strength of the prior 
Fig. S5. Generalization performance of the Bayesian COR model with the Tallying
decision rule by training sample size in all 20 datasets that heuristics have been
extensively tested on (2). The abscissa represents an increasing prior strength from
left to right, and the ordinate represents the predictive accuracy of the model. Note
that an approx. inﬁnitely strong prior (e.g., 1/÷2 = 1e+06) corresponds to the tallying
heuristic, and a prior strength of zero (1/÷2 = 0) corresponds to ordinary linear
regression. In 11 out of the 20 datasets, a less-is-more effect can be observed, where
the tallying heuristic outperformed ordinary linear regression, with 10 and 20 training
cases. For example, in the City Size, Car Accidents, and Mammals datasets, the
tallying heuristic outperformed ordinary linear regression for training samples sizes of
10 or 20 training cases. But the optimal performance could be found in the middle,
i.e., for medium-strength priors. The optimal performance peak was robust across
training sample sizes of 10, 20 and 115 training cases. In other datasets, such as
Homelessness, Fish Fertility, and Women’s Attractiveness, ordinary linear regression
outperformed tallying. However, the optimal performance for all datasets was found
for intermediate COR models, i.e., for medium-strength priors. Error bars represent
± SEM.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
House Prices
Mortality Rates
City Size
Professors' Salaries
Body Fat
Car Accidents
Cloud Rainfall
High School Dropouts
Obesity
Fuel Consumption
Biodiversity
Homelessness
Land Rent
Mammals' Sleep
Oxidants in L.A.
Attractiveness Men
Attractiveness Women
Fish Fertility
Oxygen
Ozone in S.F.
65
70
75
80
85
60
65
70
75
80
60
65
70
72.5
75.0
77.5
80.0
82.5
52
54
56
60
65
70
75
57.5
60.0
62.5
65.0
67.5
55
60
65
60
65
70
67.5
70.0
72.5
75.0
77.5
70.0
72.5
75.0
77.5
80.0
56
58
60
62
64
74
76
78
80
65
70
75
80
65.0
67.5
70.0
72.5
68
70
72
66
68
70
71
72
73
74
75
76
77
67
68
69
70
71
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
1e−04 1e−01 1e+02 1e+05
TrainingSize
●
●
●
10
20
115
Generalization performance (%)
Strength of the prior 
Fig. S6. Generalization performance of the Bayesian COR model with the TTB
decision rule by training sample size in all 20 datasets that heuristics have been
extensively tested on (2). The abscissa represents an increasing prior strength from
left to right, and the ordinate represents the predictive accuracy of the model. Note
that an approx. inﬁnitely strong prior (e.g., 1/÷2 = 1e+06) corresponds to the TTB
heuristic, and a prior strength of zero (1/÷2 = 0) corresponds to ordinary linear
regression. In 18 out of the 20 datasets, a less-is-more effect can be observed,
where the TTB heuristic outperformed ordinary linear regression, with 10 and 20
training cases. For example, in the House Prices, Mortality, City Size, and Professor
Salaries datasets, the TTB heuristic outperformed ordinary linear regression for
training samples sizes of 10 or 20, but the optimal performance could be found in the
middle, i.e., for medium-strength priors. The optimal performance peak was roughly
in the same place across training sample sizes of 10, 20 and 115 training cases. In
other datasets, such as the Cloud Rainfall or the Ozone levels dataset, ordinary linear
regression outperformed the TTB heuristic, but the optimal performance can still be
found in the intermediate COR models, i.e., for medium-strength priors. Error bars
represent ± SEM.
Parpart et al.
9 of 11

Table S1. Computation of cue validities: A binary prediction task where ﬁve objects comparisons are made on the basis of three cues.
Comparison
Cue x1
Cue x2
Cue x3
y
r1
w1
x1T y
x1T x1
1
-1
-1
0
-1
1
0
1
1
2
1
-1
1
-1
0
1
-1
1
3
0
-1
0
1
0
0
0
0
4
1
1
1
1
1
0
1
1
5
1
1
-1
1
1
0
1
1
v1 = 3
4
v2 = 4
5
v3 = 1
3
R1 = 3
W1 = 1
R1 ≠W1 = 2
R1 + W1 = 4
The cue columns represent cue diﬀerence values, x1, x2, x3 respectively, and are coded in the same way as the coding column in Fig. 1 in the main
text. The criterion variable y contains the outcome of each comparison. r1 and w1 indicate whether cue x1 predicted the outcome correctly or
incorrectly (r = right, w = wrong) on each comparison, and R1 and W1 are the sums across all comparisons, R1 = q
r1 and W1 = q
w1. Then,
the cue validity for cue x1is computed as v1 =
R1
W1+R1 . The validities for x2 and x3 are deﬁned similarly.
10 of 11
Parpart et al.

Supporting Information References
1.
Martignon L, Hoffrage U (1999) Why does one-reason decision making work. A case study in
ecological rationality. (Oxford University Press, New York), pp. 119–140.
2.
Czerlinski J, Gigerenzer G, Goldstein DG (1999) How good are simple heuristics?, eds.
Gigerenzer G, Todd P, Group AR. (Oxford University Press, New York), p. 97–118.
3.
Katsikopoulos KV, Schooler LJ, Hertwig R (2010) The robust beauty of ordinary information.
Psychological Review 117(4):1259.
4.
Chater N, Oaksford M, Nakisa R, Redington M (2003) Fast, frugal, and rational: How rational
norms explain behavior. Organizational behavior and human decision processes 90(1):63–86.
5.
Dawes RM (1979) The robust beauty of improper linear models in decision making. American
psychologist 34(7):571.
6.
Gigerenzer G, Goldstein DG (1996) Reasoning the fast and frugal way: models of bounded
rationality. Psychol Rev 103(4):650–69.
Parpart et al.
11 of 11

