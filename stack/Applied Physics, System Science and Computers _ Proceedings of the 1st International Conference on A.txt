Klimis Ntalianis
Anca Croitoru   Editors
Applied Physics, 
System Science 
and Computers
Lecture Notes in Electrical Engineering   428
Proceedings of the 1st International 
Conference on Applied Physics, System
Science and Computers (APSAC2016), 
September 28–30, Dubrovnik, Croatia

Lecture Notes in Electrical Engineering
Volume 428
Board of Series editors
Leopoldo Angrisani, Napoli, Italy
Marco Arteaga, Coyoacán, México
Samarjit Chakraborty, München, Germany
Jiming Chen, Hangzhou, P.R. China
Tan Kay Chen, Singapore, Singapore
Rüdiger Dillmann, Karlsruhe, Germany
Haibin Duan, Beijing, China
Gianluigi Ferrari, Parma, Italy
Manuel Ferre, Madrid, Spain
Sandra Hirche, München, Germany
Faryar Jabbari, Irvine, USA
Janusz Kacprzyk, Warsaw, Poland
Alaa Khamis, New Cairo City, Egypt
Torsten Kroeger, Stanford, USA
Tan Cher Ming, Singapore, Singapore
Wolfgang Minker, Ulm, Germany
Pradeep Misra, Dayton, USA
Sebastian Möller, Berlin, Germany
Subhas Mukhopadyay, Palmerston, New Zealand
Cun-Zheng Ning, Tempe, USA
Toyoaki Nishida, Sakyo-ku, Japan
Bijaya Ketan Panigrahi, New Delhi, India
Federica Pascucci, Roma, Italy
Tariq Samad, Minneapolis, USA
Gan Woon Seng, Nanyang Avenue, Singapore
Germano Veiga, Porto, Portugal
Haitao Wu, Beijing, China
Junjie James Zhang, Charlotte, USA

About this Series
“Lecture Notes in Electrical Engineering (LNEE)” is a book series which reports
the latest research and developments in Electrical Engineering, namely:
• Communication, Networks, and Information Theory
• Computer Engineering
• Signal, Image, Speech and Information Processing
• Circuits and Systems
• Bioengineering
LNEE publishes authored monographs and contributed volumes which present
cutting edge research information as well as new perspectives on classical ﬁelds,
while maintaining Springer’s high standards of academic excellence. Also
considered for publication are lecture materials, proceedings, and other related
materials of exceptionally high quality and interest. The subject matter should be
original and timely, reporting the latest research and developments in all areas of
electrical engineering.
The audience for the books in LNEE consists of advanced level students,
researchers, and industry professionals working at the forefront of their ﬁelds. Much
like Springer’s other Lecture Notes series, LNEE will be distributed through
Springer’s print and electronic publishing channels.
More information about this series at http://www.springer.com/series/7818

Klimis Ntalianis ⋅Anca Croitoru
Editors
Applied Physics, System
Science and Computers
Proceedings of the 1st International
Conference on Applied Physics, System
Science and Computers (APSAC2016),
September 28–30, Dubrovnik, Croatia
123

Editors
Klimis Ntalianis
University of Applied Sciences
Athens
Greece
Anca Croitoru
Faculty of Mathematics
Alexandru Ioan Cuza University
Iaşi
Romania
ISSN 1876-1100
ISSN 1876-1119
(electronic)
Lecture Notes in Electrical Engineering
ISBN 978-3-319-53933-1
ISBN 978-3-319-53934-8
(eBook)
DOI 10.1007/978-3-319-53934-8
Library of Congress Control Number: 2017939605
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Contents
Part I
Applied Physics
Quantum Thermodynamics and Coherence in Ion Channels. . . . . . . . . .
3
Samyadeb Bhattacharya and Sisir Roy
Micro-pulse Stimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Marie Nedvedova, Milan Chmelar, Ivo Provaznik and Zdenek Reznicek
Exploring the Therapeutic Effects of Micro-pulse Stimulation . . . . . . . .
21
Marie Nedvedova, Milan Chmelar, Ivo Provaznik and Kristina Zuffova
Application of BaTiO3 Perovskite Material for Piezoelectric
Multilayer Actuators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
Magdalena Gromada, Mojtaba Biglar, Tomasz Trzepiecinski
and Feliks Stachowicz
Modeling of the Waterﬂooding Process in the Presence of
Discontinuities in the Oil Reservoirs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Vladimir Astafev, Elena Andriyanova and Andrey Kasatkin
Terahertz Spectroscopy Applications in Medicament Analysis . . . . . . . .
45
Kateřina Sulovská
Stability of Capillary Waves of Finite Amplitude. . . . . . . . . . . . . . . . . . .
51
Alexander Petrov, Mariana Lopushanski and Vladimir Vanovskiy
High Temperature Behavior of Two Titanium Aluminides
for Blade Engine Applications. Preliminary Study . . . . . . . . . . . . . . . . . .
59
Alexandra Banu, Alexandru Paraschiv, Luminita Georgescu
and Cristina Juganaru
The Numerical Scheme for the Basset Type Integro-Differential
Equation in Hydrodynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
Vladimir Vanovskiy and Alexander Petrov
v

On the Issue of Choosing the Measuring Zones in a Faraday
Balance When Studying Magnetic Susceptibility of Small Samples. . . . .
. . . .
77
Alexander Sandulyak, Anna Sandulyak, Maria Polismakova,
Vera Ershova, Darya Sandulyak and Dmitriy Kiselev
Part II
System Science and Computers
Energy Aware Autonomous Deployment for Mobile Wireless
Sensor Networks: Cellular Automata Approach. . . . . . . . . . . . . . . . . . . .
87
Shahinaz M. Al-Tabbakh and Eman Shaaban
An Optimal Process for Average Value-at-Risk Portfolios
in Financial Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
Yuji Yoshida
On Quantiﬁcation of the Hidden Distributed Generation
Capacity and Its Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
Vladislav Samoylenko, Stanislav Eroshenko and Andrew Pazderin
Modeling the Operating Costs for Production
of the Hydrolyzate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
Hana Vaskova and Karel Kolomaznik
The Problems of Data Security in Cloud Computing
and Its Solution Using Petri Nets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
Zoltán Balogh and Martin Magdin
Designing of the Pseudorandom Number Generators
on the Basis of Two-Dimensional Cellular Automata . . . . . . . . . . . . . . . .
137
Stepan Bilan, Mykola Bilan, Ruslan Motornyuk, Andrii Bilan
and Sergii Bilan
A Mixed Fixed Point and Floating Point Graphics Pipeline . . . . . . . . . .
145
Ovidiu Sicoe and Mircea Popa
Functional Veriﬁcation of AMS-SoC Models Using Hardware
Emulation Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Hanan Tawﬁk, Mohamed AbdElSalam, Mona Safar and Ashraf Salem
Inﬂuence of the Antenna’s Height to the Standing Waves
Ratio When Performing the Electromagnetic Susceptibility
Tests in Anechoic Chambers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
Martin Pospisilik, Milan Adamek and Petr Neumann
Carstairs-McCarthy’s Morphological Rules of English
Language in RDFCFL Graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
Alena Lukasová, Martin Žáček and Marek Vajgl
vi
Contents

Mathematical Modeling and Computer Simulation of Simple
Permutation Brainteaser in MS Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
Michal Musilek, Stepan Hubalovsky and Marie Hubalovska
Research of Methods of Learning of Programming Objects-First
and Object-Later. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
Ondrej Korinek and Stepan Hubalovsky
Multichannel Queueing Systems and Their Simulation . . . . . . . . . . . . . .
191
Miloš Šeda, Jindřiška Šedová and Miroslav Horký
On Computational Evaluation of Stress Concentration Using
Micropolar Elasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
Victor A. Eremeyev, Andrzej Skrzat and Feliks Stachowicz
An Algorithm for Edge Detection of the Image for Application
in WSN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Adrian Shehu, Astrit Hulaj and Xhevahir Bajrami
A Mathematical Model of the Behavior of SIP Signaling
and Media Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
Naser K.A. Alajmi, Hadeel Saleh Haj Aliwi, Kamal Alieyan
and Muhammad-Imran Sarwar
Blood Vessel Segmentation from Color Retinal Images
Using K-Means Clustering and 2D Gabor Wavelet . . . . . . . . . . . . . . . . .
221
Aziah Ali, Wan Mimi Diyana Wan Zaki and Aini Hussain
Training Samples Construction for Energy Utilities Operational
Assets Management. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
Alexandra Khalyasmaa and Stanislav Eroshenko
A Host Program Implementation for Linux File System Tracing
Method Using the Kprobes Linux Dynamic Instrumentation
System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Sang-Young Cho
Simulation VANET Networks on a Random and Realistic
Spatial Scenario. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
Suad Kasapovic and Lejla Banjanovic-Mehmedovic
Sensor Module for Monitoring Wine Fermentation Process . . . . . . . . . .
253
Dimitrija Angelkov and Cveta Martinovska Bande
Study the Transmittance Properties of Light Sources
Under Simulated Hazy Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
Haibo Yuan, Xiaoli Zhou, Zheqian Zhang and Fanghui Xu
Contents
vii

Numerical Study on the Thermal Fatigue of Cryogenic
Vacuum Insulated Pipe. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
Jae-Hoon Lee, Si-Pom Kim, Rock-Won Jeon and Geun-Ho Lee
Unconventional Usage of Entropy in the Field of Web
Usage Data Preprocessing and Machine Translation
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Michal Munk and Ľubomír Benko
Advantages of Intelligent Multimedia Application . . . . . . . . . . . . . . . . . .
287
Eva Milkova and Abdel-Badeeh M. Salem
FFLD-Based Modeling of Fractional-Order State Space
LTI MIMO Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
Krzysztof J. Latawiec, Rafał Stanisławski, Marian Łukaniszyn,
Marek Rydel and Bogusław R. Szkuta
A Dispatching Policy for the Dynamic and Stochastic
Pickup and Delivery Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303
Gianpaolo Ghiani, Emanuele Manni and Alessandro Romano
viii
Contents

Part I
Applied Physics

Quantum Thermodynamics and Coherence
in Ion Channels
Samyadeb Bhattacharya and Sisir Roy
Abstract We showed that quantum mechanical superposition can sustain in the
process of ion transfer in protein membrane for a substantial period in spite of the
presence of the interactions with environmental modes of molecular vibration. The
spectral temperature, as deﬁned in quantum thermodynamical framework plays a sig-
niﬁcant role in maintaining the coherence. The ratio of decoherence time and dwell
time has been calculated, which can be directly related to the degree of coherence.
The results shead new light to build quantum information system of entangled ionic
states in the voltage gated biological channels.
Keywords
Ion channels ⋅Decoherence time ⋅Dwell time ⋅Spectral temperature
1
Introduction
The approaches towards dynamics of ion transport in protein membranes (ion chan-
nels) are generally considered to be classical and based on molecular dynamics or
Brownian dynamics. Recently, one of the present authors showed that [1] the dynam-
ics of K-ion channel can be explained using nonlinear Schrodinger equation which
is compatible with the results of MacKinnon’s experimental observation [2]. Here,
the two K-ions may form an entangled state within selectivity ﬁlter during a ﬁnite
period of time. The temperature within the channel is generally considered to be
high enough to destroy the coherence within very short period. Moreover, molecular
modes of the protein environment induce dynamical decoherence, which destroys the
quantum mechanical superposition of states in a very short period of time. So quan-
S. Bhattacharya
Harish-Chandra Research Institute, Allahabad, India
e-mail: sbh.phys@gmail.com
S. Roy (✉)
National Institute of Advanced Studies, IISC Campus, Bangalore, India
e-mail: sisir.sisirroy@gmail.com
URL: http://www.springer.com/lncs
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_1
3

4
S. Bhattacharya and S. Roy
tum mechanical approach in these area of research was mostly speculative and far
from experimental realization. But in recent times, some experimental demonstra-
tion of the presence of quantum coherence in the process of photosynthetic energy
transfer [3, 4] lead us to reconsider the theoretical approach and understanding. Here
we are concerned with the question that whether and under what condition a sustain-
able quantum superposition is achievable in the process of transfer of ions through
biological channels. Now it is quite a practical argument that the quantum state of the
traversing ion is strongly coupled with the molecular vibrational modes of the protein
environment and hence fast decoherence is almost absolutely unavoidable. But it is
to be noted that the traversal time of the ion through the membrane is also quite small
and it is the ratio of the time scale of decoherence with this traversal time that plays
an important role in understanding the maintenance of coherence. If the decoherence
time is larger than the traversal time of the ion, then the quantum superposition of the
ionic states is sustainable enough for the traversing entity within the period of ionic
transfer. Here we also need to consider the eﬀect of temperature in estimating the
decoherence time. With the recent developments of quantum thermodynamics [5],
a new concept of temperature (known as spectral temperature) for micro-states at
non-equilibrium condition has been proposed, instead of the usual concept of ther-
modynamic temperature. Since ion transport through protein membrane is essen-
tially a non-equilibrium phenomena, we propose that the spectral temperature plays
important role in understanding the coherence in the channel dynamics.
2
Decoherence Time and Dwell Time
Considering the master equation of the density operator for a certain quantum sys-
tem, the decoherence time [6] can be written as
𝜏dec =
ℏ2
2m𝛾KBT(x −x′)2
(1)
where 𝛾is the relaxation (dissipation) parameter, T is the thermodynamic temper-
ature and 𝛥x = x −x′ is the spatial shift of the particle. Now there are numerous
deﬁnition of traversal time, among which phase time and dwell time are generally
accepted by the community [7]. Phase time is equated to dwell time with an additive
self-interference term and argued to be the time interval between the energy stor-
age and release in the barrier region [8–11]. So for non-dispersive barrier, where the
self-interference term vanishes, the dwell time can be readily interpreted as the life
time of energy storage and release in the barrier region. In a previous work [12], we
have calculated the weak value of dwell time as
𝜏D = 1
𝛾coth
(𝛾𝜏M
2
)
(2)

Quantum Thermodynamics and Coherence in Ion Channels
5
Fig. 1
V(x) versus x with
parameters A = 14 and
B = 45
x0
x1
x2
2
0
2
4
6
8
10
200
0
200
400
V x
where 𝜏M is the measurement time.
The protein membrane can be assumed as an array of molecules with certain modes
of vibration. We have mentioned earlier that the quantum states of the traversing
entity is interactively coupled to the molecular vibration modes of the protein mem-
brane. The traversing quantum entity loses energy to the vibrational modes, due to
the presence of this environmental coupling. So the protein membrane can be inter-
preted as sort of capacitive system, which can store and release energy with the dwell
time as it’s lifetime of energy storage. In this case the time interval between the open-
ing and closing of the channel gates can be taken as the measurement time (𝜏M). Now
we consider a double well potential of the form [13]
V(x) = 1
2m𝜔2x2
[(x
a
)2
−A
(x
a
)
+ B
]
(3)
where A and B are dimensionless constants. For the particular case of asymmetric
double well, A = 14 and B = 45. The bistable nature of the potential is useful in prac-
tical situation, since the ionic transfer in the channel can be interpreted as tunneling
between the two stability regions situated at x0 and x2 (see Fig. 1).
3
Spectral Temperature and the Ratio of Decoherence
Time and Dwell Time
It is also very important to reconsider the concept of temperature in this aspect. Tem-
perature, as we know from the context of thermodynamics, is a property of equilib-
rium. The ensemble mean of the kinetic energy equals to Boltzman constant times
the temperature. So assuming ergodicity, temperature is deﬁned as the time aver-
aged kinetic energy. But the process of ion transfer through channels is a dynamical
process interactive to the protein environment and subject to energy exchange with

6
S. Bhattacharya and S. Roy
the environment. This is certainly not a situation for thermodynamic equilibrium and
hence the usual concept of thermodynamic temperature may not be suitable. Here
we introduce the concept of spectral temperature, originally formulated by Gemmer
et al. [5]. It is deﬁned as a function of the microstates, to include the non-equilibrium
properties of the system. It is formulated as a function of energy occupation proba-
bility of the diﬀerent states of a certain quantum system. This temperature evolves
with the evolution of the probability of occurrence. The inverse of the spectral tem-
perature is deﬁned as [5]
1
KBTspec = −
(
1 −P0+PN
2
)−1 ∑N
i=1
(
Pi+Pi−1
2
)
×
[
ln
(
Pi
Pi−1
)
−ln
(
𝜙i
𝜙i−1
)
Ei−Ei−1
]
(4)
where Pi is the probability of ﬁnding the particle within an energy compartment with
mean energy Ei, having the degree of degeneracy 𝜙i. This deﬁnition depends on the
energy probability distribution and of course, the spectrum of the concerning system.
So it cannot change in time for an isolated system and deﬁned independent of the fact
that whether the system is in equilibrium or not. In this deﬁnition, the association
of quantum probability, which evolves in time, gives temperature a evolving feature
representing the dynamical situation between two successive equilibrium. Now here
we are approximating the bistable potential as a two-state system having only the
ground states of the asymmetric wells. So for our case of non-degenerate two-state
system, the expression of the spectral temperature reduces to
1
KBTspec
= −
(
1 −P0 + P1
2
)−1 (P0 + P1
2
) ⎡
⎢
⎢
⎢⎣
ln
(
P0
P1
)
E0 −E1
⎤
⎥
⎥
⎥⎦
(5)
where P0 and P1 are the probabilities corresponding to the ground states of the lower
and higher well respectively. Since P0 is the probability corresponding to the lowest
state, it should not decay. Now using the decay probability of the higher state (P1 ∼
e−𝛾t) within the time interval equal to the dwell time, we ﬁnd
1
KBTspec
=
1
E1 −E0
coth
(𝛾𝜏M
2
)
coth
[1
2 coth
(𝛾𝜏M
2
)]
(6)
Now if we consider the energy loss by the particle traversing from the higher barrier
to the lower one in terms of the usual kinetic temperature (Tk) as E1 −E0 = 1
2KBTk,
we get
Tspec
Tk
= 2 tanh
(𝛾𝜏M
2
)
tanh
[1
2 coth
(𝛾𝜏M
2
)]
(7)

Quantum Thermodynamics and Coherence in Ion Channels
7
Fig. 2
Tspec∕Tk versus 𝛾𝜏M.
Here we keep 𝛾as a constant
and basically study the
variation with increasing 𝜏M
0
2
4
6
8
10
0.80
0.85
0.90
Tspec
Tk
Fig. 3
𝜏dec∕𝜏D versus 𝛾𝜏M.
Here also we keep 𝛾as a
constant and basically study
the variation with increasing
𝜏M. Here F = 2ℏ
w
√
2
m𝜖0 . As
with increment of 𝜏M, the
process becomes
quasi-static, we see that the
ratio of the two timescales
also reach a stable value
0
2
4
6
8
1.0
1.2
1.4
1.6
1.8
2.0
From Eq. (7), under the condition 𝜏M ≫1
𝛾, we ﬁnd that Tspec ≃Tk. i.e. the spectral
temperature is almost equal to the usual kinetic temperature, if the time interval of
gate opening and closing is very greater than the dissipation time scale. If the gate
opening and closing mechanism is slow enough to be considered as a quasi-static
process, the spectral temperature remains very close to it’s kinetic counterpart. Now
it should also be noted that though the process is sort of quasi-static one, it should
not be considered as a reversible process, because the coupling to the molecular
vibrational modes of the protein environment ensures some generation of dissipative
entropy (Figs. 2, 3, and 4).
In case of a double well potential given by (3), using (1), (2) and (7) we get
𝜏dec
𝜏D
= 2ℏ
w
√
2
m𝜖0
coth
[1
2 coth
(𝛾𝜏M
2
)]
(8)

8
S. Bhattacharya and S. Roy
Fig. 4
𝜏dec∕𝜏D versus g(1)
0.0
0.2
0.4
0.6
0.8
1.0
0
10
20
30
40
50
g
where 𝜖0 = E1 −E0 is the asymmetry energy of the potential and w = 15a
2
is the
separation length between the wells. For the quasi-static condition 𝜏M ≫1
𝛾
𝜏dec
𝜏D
≃4.5ℏ
w
√
2
m𝜖0
(9)
So we ﬁnd that in the quasi-static region, whether the decoherence time scale is
larger than the dwell time depends on the mass of the traversing particle, the length
scale of the channel and the asymmetry energy which is basically the energy lost by
the particle during the process of traversal. The ratio of the time scales is inversely
proportional to all the mentioned parameters. i.e. greater the inertia of the traversing
particle, stronger the interaction (hence greater energy loss) and larger the traversal
length, faster shall be the decoherence and hence the situation will be more and more
classical. But if the parameter can be chosen in such a way in which the process
of decoherence is slower than the process of ionic transfer (i.e. 𝜏dec∕𝜏D > 1), then
quantum superposition will be sustainable enough within the time period of ionic
transfer.
4
Relation Between Degree of Coherence and Decoherence
Time-Dwell Time Ratio
Now we turn our attention to the “degree of coherence” for such cases of ionic trans-
fer through protein membranes and try to establish a relation of the afore mentioned
ratio of decoherence-dwell time scales with it. Convenient way to discuss coherence
in quantitative terms can be done through the introduction of normalized form of
correlation functions such as “degree of coherence” [14]

Quantum Thermodynamics and Coherence in Ion Channels
9
g(n)(𝜉1...𝜉2n, t1...t2n) =
G(n)(𝜉1...𝜉2n, t1...t2n)
∏2n
j=1[G(1)(𝜉j, 𝜉j, t1)]1∕2
(10)
where 𝜉= x∕a. For our case of double well system approximated as two-state sys-
tem, (10) is reduced to
g(1) =
G(1)
12 (𝜏)
√
G(1)
11 (0)G(1)
22 (0)
(11)
where
G(1)
12 (𝜏) = ⟨𝜓1(𝜉, t)𝜓2(𝜉, t + 𝜏)⟩
= e−𝛾𝜏∫∞
−∞𝜓(𝜉−𝜉1)𝜓∗(𝜉)d𝜉
(12)
and
0 ≤g(1) ≤1
(13)
For completely coherent situation, the degree of coherence is 1 and for completely
decohered case it is 0. Generally the value should lie in between. If we approximate
the potential around the left well at 𝜉= 0 as a harmonic potential, we ﬁnd that the
wavefunction can be estimated as [13]
𝜓(𝜉) =
( 𝜈
𝜋
)1∕2
exp
[
−1
2𝜈𝜉2]
(14)
where 𝜈=
√
B m𝜔a2
ℏ. Expressing the asymmetry energy as
𝜖0 = V(x0) −V(x2)
(15)
we ﬁnd that for a double well potential (A = 14 and B = 45)
𝜔=
4
15a
√
2𝜖0
15m = 2
w
√
2𝜖0
15m
(16)
So for the interval of dwell time given by Eq. (2), we ﬁnd that the degree of coherence
g(1) = exp
[
−
√m𝜖0
2
w
ℏ
]
exp
[
−coth
(𝛾𝜏M
2
)]
(17)
For the quasi-static condition 𝜏M ≫1∕𝛾
g(1) ≈exp
[
−
√m𝜖0
2
w
ℏ
]
(18)

10
S. Bhattacharya and S. Roy
From Eqs. (9) and (18), we can also establish a relation
𝜏dec
𝜏D
=
4.5
ln
[
1
g(1)
]
(19)
From this relation we see that for completely coherent and decoherent situation, the
decoherence-dwell time ratio is inﬁnity and zero respectively and in general situation
it lies in between. So this time-scale ratio gives us a certain measure of coherence.
As the value of this ratio gets bigger, the “quantumness” of the traversing entity
increases.
5
Conclusion
Depending on the above analysis, we suggest that the ion selectivity ﬁlter may exhibit
quantum coherence which can play crucial role in the process of selectivity and con-
duction of speciﬁc ions in biological membranes. For the time scales shorter than
that of decoherence time, quantum coherence can be expected to sustain and have
vital importance in the dynamics, despite the presence of interactive protein envi-
ronment. Our analysis shows that for a sort of quasi-static situation, where the gate
opening and closing mechanism is slower than the relaxation (dissipation) time scale,
the decoherence-dwell time ratio reaches a static value and can also be greater than
unity depending on the mass, energy and length parameters. In such situations, coher-
ent phenomena like entanglement can be of vital importance in understanding the
mechanism of selectivity and transport. In case of K+ ﬁlter, there exists two ener-
getically almost degenerate binding states, commonly referred as (1, 3) and (2, 4)
states [15–17]. Presence of quantum superposition may lead us to explain the trans-
port phenomena in terms of quantum mechanical tunneling between these two states.
The interplay between quantum coherence and environmental noise induced dephas-
ing may also be of fundamental importance. Progress in the atomic spectroscopy of
membrane proteins indicate that protein membrane organization may carry a cer-
tain coding potency, implying quantum entanglement within ion channels [15–18].
Increasing number of studies are indicating towards the probabilistic nature of ion
channel gating mechanism [2, 19]. In the light of these researches, we conclude that
it may be relevant to build certain model of quantum information system driven by
the entangled ionic states in the voltage gated selectivity ﬁlters, which can provide
necessary inferences into the biological ion channel dynamics.
References
1. Roy, S., Llinás, R.: C.R. Biologies 332, 517 (2009)
2. Doyle, D., MacKinnon, R., et al.: Science 280, 69 (1998)

Quantum Thermodynamics and Coherence in Ion Channels
11
3. Engel, G.S., et al.: Nature 446, 782 (2007)
4. Mercer, I.P., et al.: Phys. Rev. Lett. 102, 057402 (2009)
5. Gemmer, J., Michel, M., Mahler, G.: Quantum Thermodynamics, vol. LNP657. Springer,
Heidelberg, Berlin (2004)
6. Zurek, W.H.: Prog. Math. Phys. 48(1) (2007)
7. Hauge, E.H., Stoveng, J.A.: Rev. Mod. Phys. 61(917) (1989)
8. Winful, H.G.: New J. Phys. 8, 101 (2006)
9. Winful, H.G.: Opt. Express 10, 1491 (2002)
10. Winful, H.G.: Phys. Rev. E 68, 016615 (2003)
11. Winful, H.G.: Phys. Rep. 436, 1 (2006)
12. Bhattacharya, S., Roy, S.: Phys. Rev. A 85, 062119 (2012)
13. Mohsen, R.: Quantum Theory of Tunneling, p. 255. World Scientiﬁc Publishing (2003)
14. Glauber, R.J.: Quantum Theory of Optical Coherence. WILEY-VCH Verlag GmbH and Co,
KGaA (2007)
15. Bernroider, G., Roy, S.: SPIE 5841, 205 (2005)
16. Summhammer, J., Salari, V., Bernroider, G.: J. Integr. Nanosci. 11, 123 (2012)
17. Salari, V., Tuszynski, J., Rahnama, M., Bernroider, G.: JPCS 306, 012075 (2011)
18. Hans, J.B., Popescu, S.: arXiv:0806.4552v2 (2009)
19. Zhou, Y., Morais-Cabral, A., Kaufman, A., MacKinnon, R.: Nature 414, 43 (2001)

Micro-pulse Stimulation
Marie Nedvedova, Milan Chmelar, Ivo Provaznik
and Zdenek Reznicek
Abstract This paper deals with a new electrotherapeutic method for use in
physiatric treatment. The micro-pulse stimulation is based on the combination of
three electrotherapeutic methods: burst therapy, high-voltage pulsed current ther-
apy, and microelectrostimulation. Micro-pulse stimulation is special for its unusual
conﬁguration of pulse parameters; high-voltage electric pulses are safely used as a
result of a cumulative effect of the subthreshold monophasic pulses with a very
short duration. This combination of parameters should connect the advantages of
used methods. This includes an analgesic effect without any obvious adaptation of
the stimulated tissue and making the tissue penetration easier. As the micro-pulse
stimulation was designed especially for the treatment of swelling and pain in animal
therapy, the device is small, portable, battery-operated and easy to use.
Keywords Burst therapy ⋅High-voltage pulsed current therapy ⋅Micro-pulse
stimulation ⋅Subthreshold stimulation
1
Introduction
The role of electrotherapy and electrostimulation is quite extraordinary in com-
parison to other methods of physical therapy. Their usage is limited not only for
rehabilitation as a standard application but it has a great signiﬁcance for the
M. Nedvedova (✉) ⋅M. Chmelar ⋅I. Provaznik
Faculty of Electrical Engineering and Communication, Department of Biomedical
Engineering, Brno University of Technology, Technicka 3082/12, 61600 Brno,
Czech Republic
e-mail: tobolova@fai.utb.cz
M. Nedvedova
Faculty of Applied Informatics, Department of Electronics and Measurements,
Tomas Bata University in Zlin, Nad Stranemi 4511, 76005 Zlin, Czech Republic
Z. Reznicek
Hybrid Integrated Technologies, Ltd., Nedachlebice 233, 68715 Nedachlebice,
Czech Republic
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_2
13

diagnostics, treatment and for research. Instead of the classical electrostimulation
methods with single pulses of the speciﬁed frequency, shape, duration and period,
there were found new alternatives that seem to be more effective in some appli-
cations. We can mention various types of modulation (amplitude, frequency) or
burst therapy [1–3]. Micro-pulse stimulation was designed as a type of elec-
trotherapy which could combine the advantages of the three chosen methods,
especially microelectro stimulation, high-voltage pulsed current and burst therapy.
MicroElectroStimulation (MES) is also known as MicrocurrentElectroTherapy
(MET) or MicroElectroNeuroStimulation (MENS). The preﬁx “micro” is common
for all modalities because of using extremely low values of electric current about
the millionth of ampere. Similar microampere values can be measured both at
humans and some animals because their bodies form the natural electromagnetic
ﬁeld [4]. That is why it can be also called Biostimulation. The current values for the
MES are subthreshold and thousand times lower than for the Transcutaneous
Electrical NeuroStimulation (TENS). There are described positive therapeutic
effects of MES on all sorts of pain, it moderates inﬂammation and edema, evolves
the moving potential, muscular strength and relaxation [5]. Furthermore, MES
supports signiﬁcantly the wound healing [6–8], treatment of patients with the
diabetes mellitus, hypertension and chronic wounds [9], obesity reduction [10],
reduction of inﬂammatory induced by ultraviolet irradiation [11]. But not all the
studies are so optimistic [12]. Study [13] revealed that MES could enhance apop-
tosis, the adverse effect to the tissue healing. Despite it, MES is highly promising
for very problematic tendon regeneration at horses.
High-Voltage Pulsed Current (HVPC) uses the pulsed current with relatively low
intensity, short duration (usually 10–30 µs), but voltage much higher than it is usual
in electrotherapy (up to 500 V) [1]. The device for HVPC must work in the regime
of the constant voltage. The level of the stimulation intensity is usually threshold up
to motor suprathreshold. The pulse frequency can be constant or modulated and it
directly inﬂuences the myorelaxation and analgesic effect. HVPC causes vasodi-
latation supporting the edema reduction [14], tissue regeneration and wound
healing, especially decubitis ulcers [15], venous crural ulceration [16] and diabetic
foot ulcers [17].
The Burst Therapy is based on the theory of the subthreshold pulses summation
[18]. The pulses of the burst come separately and gradually summate so that each of
them brings the membrane closer to its threshold. The membrane threshold is
reached when the pulse summation culminates in the sufﬁcient depolarization
leading to the formation of an action potential (AP). The total number of APs agrees
with the number of bursts [19]. The summation effect requires that each pulse in the
burst follows so fast that there is not enough time for the membrane relaxation after
the subthreshold polarization. The depolarization caused by the summation of the
subthreshold pulses is expected at the higher kilohertz frequencies [20]. According
to available sources, the burst therapy has good results in the prevention of venous
thrombosis at the orthopaedic patients and postoperative cases [21] and in treatment
of the various types of arrhythmia, such as ventricular and atrial tachycardia [22].
Further, it has a noticeable analgesic effect and the great advantage lies in no
14
M. Nedvedova et al.

obvious adaptation of the tissues that is a common problem in all classic elec-
trotherapeutic methods [1].
2
Device Working Principles
The goal was to construct the device combining the principles of high-voltage
pulsed current, microelectrostimulation and burst therapy. Using voltage up to
1000 V makes the tissue penetration easier, but the high voltage must be com-
pensated by the setting of other parameters to maximally keep the safety [23]:
(a) An appropriate choice of the pulse duration and working frequency prevents
inadequate load.
(b) The electric circuit is closed between the concentric electrodes that precisely
deﬁne the application area and prevent the current ﬂow through other area.
(c) The battery power itself disables the secondary circuit formation (e.g. through
the heart); this is a prevention of the electric current injury.
Microstimulator generates high-voltage pulses (about hundreds volts) with a
very short duration (microseconds). Considering the easier construction and ther-
apeutic advantages, an accumulation of subthreshold pulses was used for formation
of the ﬁnal pulse with required characteristics. Several pulses are arranged in the
burst coming with the working frequency many times lower than the repeating
frequency of each pulse. The advantage of high carrier pulse frequency is an easier
penetration into the body, again. In this case, the envelope of the whole burst can be
understood as one therapeutic pulse in classic electrotherapy. This agrees also with
the fact that the threshold voltage drops with the cumulative number of the pulses in
one series (with the burst width) [19]. The burst can be formed by both biphasic and
monophasic pulses. The experiments using the bursts in Functional ElectroStimu-
lation advert to the better efﬁcacy of the monophasic rectangular pulses for the
lower threshold of the AP initiation [24]. In our case, we have used just the
monophasic pulses.
3
Design of the Microstimulator
The design of the Microstimulator is protected by the patent no. 304960 published
in the Bulletin No. 5/2015 by the Industrial Property Ofﬁce of Czech Republic. An
exact electrotechnical scheme of the device exceeds the range of this article;
however, Fig. 1 shows a principal scheme of the Microstimulator.
The circuit consists of three 555 precision timers (marked A, B, C), two NAND
gates (D, E), switch (F) and pulse transformer (G). Generator C generates the pulses
with high frequency (about 100 kHz) that are arranged into the bursts with the
Micro-pulse Stimulation
15

stimulation parameters (frequency and burst width) adjusted by the astable and
monostable 555 oscillators. NAND gate D separates the burst of pulses according to
the required parameters of stimulation. NAND gate E inverts the output of gate D.
Switch F manages transfer of the pulses to the primary winding of pulse transformer
G.
The time relation of the important output signals is demonstrated in Fig. 2.
Signal no. 3 represents function of the generator C. Signal no. 1 demonstrates the
output of the astable 555 oscillator A that activates the monostable 555 oscillator B
to generation of the pulse with the required width, see signal no. 2. This way, the
equivalent number of pulses is deﬁned and transferred from the generator, see
signal no. 4.
A ﬁnal realization of the Microstimulator and measured signal output are shown
in Fig. 3. Microstimulator was designed in such a way the device is suitable for
human and also for animal electrotherapy, particularly for horses that were chosen
as the target objects for the following micro-pulse stimulation effect testing [25].
The device is small, portable, battery-operated and easy to use. The horse is a
typical skittish and restless animal with an unpredictable behaviour which could be
the reason of some dangerous situations not only for themselves but also for their
close surroundings. Therefore, a compliance with some requirements and condi-
tions was in an effort to keep considering the safety and preventive precaution when
an unpredictable situation occurs. Microstimulator is classiﬁed as a medical elec-
trical device and strict requirements for the design and function must be kept
Fig. 1 Principal scheme of microstimulator
Fig. 2 Time diagram of the output signals from the astable 555 oscillator (1), monostable 555
oscillator (2), generator (3), and switch (4)
16
M. Nedvedova et al.

according to the relevant standards. The device was tested by the accredited testing
laboratory and got CE mark certiﬁcation.
Two preliminary studies of the micro-pulse stimulation effects were carried out.
First experiment explored the physiological effect of the micro-pulse stimulation in
an objective way [25]. Two miniature thermodynamic sensors were applied on the
skin of two limbs when one of them was stimulated and the other one was just
considered as a reference. This type of sensor is able to detect even minor thermal
changes. Based on the results, an increased thermal activity was detected in the
stimulated limb compared to the reference. This should be caused by an increased
blood perfusion and warming-up of the stimulated area. Parallel to this experiment,
a pilot experiment investigating the therapeutic effect on the edema reduction of the
horses’ limbs was carried out with the promising result [26]. The edema treatment
relates to the blood circulation support closely, therefore the results of both studies
complement each other.
4
Conclusion
This paper presents a design of the Microstimulator; a new electrotherapeutic
device for a micro-pulse stimulation. Its therapeutic effect is based on the con-
necting of the selected electrotherapeutic methods to take the therapeutic advan-
tages of all of them; using high-voltage pulses coming with high frequency makes
tissue penetration easier and causes no adaptation of stimulated tissue. The device is
designed especially for animal electrotherapy, therefore there must be kept both the
strict safety instructions and preventive precautions to manage also unpredictable
reaction of animal. The therapeutic effects of the micro-pulse stimulation were
successfully tested on horses.
Fig. 3 The Microstimulator (b) and the shape of pulses on Microstimulator’s output (a) [23]. The
recording was made during the real measurement using oscilloscope. The therapeutic pulse is
marked clearly as an envelope of the burst
Micro-pulse Stimulation
17

Acknowledgements This work was supported by the Hybrid Integrated Technologies, Ltd., Brno
University of Technology and European Regional Development Fund under the project
CEBIA-Tech Instrumentation No. CZ.1.05/2.1.00/19.0376.
References
1. Podebradsky, J., Vareka, I.: Fyzikální terapie I. Grada Publishing, Praha (1998)
2. Podebradsky, J., Vareka, I.: Fyzikální terapie II. Grada Publishing, Praha (1998)
3. Capko, J.: Základy fyziatrické léčby. Grada Publishing, Praha (1998)
4. Plonsey, R., Barr, R.C.: Bioelectricity: a Quantitative Approach. Springer, New York (2007)
5. Mercola, J.M., Kirsch, D.L.: The basis for microcurrent electrical therapy in conventional
medical practice. J. Adv. Med. 8, 107–120 (1995)
6. Poltawski, L., Watson, T.: Bioelectricity and microcurrent therapy for tissue healing—a
narrative review. Phys. Ther. Rev. 14, 104–114 (2009)
7. Butcher, M.: How to use POSiFECT® bio-electric stimulation therapy in chronic wounds.
Wound Essentials 2, 186–193 (2007)
8. De Gaspi, F.O. et al.: Effects of the topical application of hydroalcoholic leaf extract of
Oncidium ﬂexuosum Sims. (Orchidaceae) and Microcurrent on the healing of wounds
surgically induced in Wistar rats. Evid. Based Complement. Altern. Med. 1–9 (2011)
9. Lee, B.Y., et al.: Ultra-low microcurrent in the management of diabetes mellitus, hypertension
and chronic wounds: Report of twelve cases and discussion of mechanism of action. Int.
J. Med. Sci. 7, 29–35 (2010)
10. Park, R., et al.: The effect of wearing shoes generating micro-currents on body composition
and blood lipid concentrations of overweight females. J. Phys. Ther. Sci. 23, 177–180 (2011)
11. Lee, J., et al.: The effects of microcurrents on inﬂammatory reaction induced by ultraviolet
irradiation. J. Phys. Ther. Sci. 23, 693–696 (2011)
12. Gossrau, G., et al.: Microcurrent transcutaneous electric nerve stimulation in painful diabetic
neuropathy: A randomized placebo-controlled study. Pain Med. 12, 953–960 (2011)
13. Lin, Y., Van Weeren, P.R., et al.: Effect of microcurrent electrical tissue stimulation on equine
tenocytes in culture. Am. J. Vet. Res. 67, 271–276 (2006)
14. Karnes, J., et al.: High-voltage pulsed current: Its inﬂuence on diameters of histamine-dilated
arterioles in hamster cheek pouches. Arch. Phys. Med. Rehabil. 76, 381–386 (1995)
15. Kloth, L.C., Feedar, J.A.: Acceleration of Wound Healing with High Voltage, Monophasic,
Pulsed Current. Phys. Ther. 68, 503–508 (1988)
16. Franek, A., Polak, A., Kucharzewski, M.: Modern application of high voltage stimulation for
enhanced healing of venous crural ulceration. Med. Eng. Phys. 22, 647–655 (2000)
17. Burdge, J.J., Hartman, J.F., Wright, M.L.: A retrospective study of highvoltage, pulsed current
as an adjunctive therapy in limb salvage for chronic diabetic wounds of the lower extremity.
Ostomy Wound Manag. 55, 30–38 (2009)
18. Gildemeister, M.: Zur theorie des elektrischen Reizes. V. Polarisation durch Wechselströme.
Berichte über die Verhandlungen der Sachsischen Akademie der Wissenschaften zu Leipzig.
Math. Phys. Kl. 81, 303–313 (1930)
19. Laufer, Y., Elboim, M.: Effect of burst frequency and duration of kilohertz-frequency
alternating currents and of low-frequency pulsed currents on strength of contraction, muscle
fatigue, and perceived discomfort. Phys. Ther. 88, 1167–1176 (2008)
20. Ward, A.R., Robertson, V.J., Ioannou, H.: The effect of duty cycle and frequency on muscle
torque production using kilohertz frequency range alternating current. Med. Eng. Phys. 26,
569–579 (2004)
21. Powell, J.H. et al.: Parameter selection and electrode placement of neuromuscular electrical
stimulation apparatus. United States Patent 5358513 (1994)
18
M. Nedvedova et al.

22. Salama, G., Kanai, A., Eﬁmov, I.R.: Subthreshold stimulation of Purkinje ﬁbers interrupts
ventricular tachycardia in intact hearts. experimental study with voltage-sensitive dyes and
imaging techniques. Circ. Res. 74, 604–619 (1994)
23. Tobolova, M.: Microstimulator: Master’s Thesis. Brno University of Technology, Brno
(2012)
24. Kaczmarek, P., et al.: Investigation of the relationship between stimulus parameters and a
human muscle contraction force during stimulation of the gastrocnemius muscle. Artif.
Organs 34, 126–135 (2009)
25. Tobolova, M., et al.: Testing the effects of micro-pulse stimulation on blood circulation using
the thermodynamic sensors. J. Biosens. Bioelectron. 5, 1–7 (2014)
26. Nedvedova, M., Chmelar, M., Provaznik, I., Zuffova, K.: Exploring the therapeutic effects of
micro-pulse stimulation. In: 1st International Conference on: Applied Physics, System
Science and Computers, Dubrovnik (2016)
Micro-pulse Stimulation
19

Exploring the Therapeutic Effects
of Micro-pulse Stimulation
Marie Nedvedova, Milan Chmelar, Ivo Provaznik
and Kristina Zuffova
Abstract Electrotherapy is not so commonly used in veterinary practice as in
human. Microstimulator is a new electrotherapeutic device designed especially for
the horses’ patients. Its unusual conﬁguration of pulse parameters makes this
method appropriate for the swelling treatment and tissue regeneration after the
minor injuries. Nine horses with swelling of the limb caused by minor injury such
as tendinitis, sprains etc. were included in the study. Micro-pulse stimulation was
applied once a day until the total regress of problems. The course of the experiment
was documented in photographs and video recordings that were signiﬁcant for
evaluation of the effectiveness of micro-pulse stimulation in this application. Based
on the results, it seems the micro-pulse stimulation really has positive effect on
swelling reduction, and so the overall tissue regeneration, consisting probably in
analgesic and anti-inﬂammatory effects.
Keywords Burst therapy ⋅Swelling treatment ⋅High-voltage pulsed current
therapy ⋅Subthreshold stimulation
M. Nedvedova (✉) ⋅M. Chmelar ⋅I. Provaznik
Faculty of Electrical Engineering and Communication, Department of Biomedical
Engineering, Brno University of Technology, Technicka 3082/12, 61600 Brno,
Czech Republic
e-mail: tobolova@fai.utb.cz
M. Nedvedova
Faculty of Applied Informatics, Department of Electronics and Measurements,
Tomas Bata University in Zlin, Nad Stranemi 4511, 76005 Zlin, Czech Republic
K. Zuffova
Faculty of Veterinary Medicine, Equine Clinic, University of Veterinary and
Pharmaceutical Sciences Brno, Palackeho 1/3, 61242 Brno, Czech Republic
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_3
21

1
Introduction
The positive effect of electrotherapy was found some tens of years ago and many
companies started a successful business on manufacturing electrostimulators and
one type of device was recommended as a universal therapeutic device for a number
of problems. But this is a mistake, because even the change of one parameter of
stimulation could have totally different effect. Therefore, the possible negative
impact of the stimulation should be considered carefully as well and the task is a
choice of the relevant pulse parameters according to the therapeutic application as
well as the possible cytotoxicity [1–6]. Maybe this is one of the reasons of focusing
on the subthreshold stimulation.
Micro-pulse stimulation was designed as a type of electrotherapy which could
combine the advantages of the three chosen methods, especially microelectros-
timulation, high voltage pulsed current and burst therapy [7]. Microelectrostimu-
lation is known for its positive effect on pain [8], inﬂammation [9], swelling
reduction [10] and tissue healing [11–14]. High voltage pulsed current is beneﬁcial
for the tissue regeneration, wound healing and also swelling reduction [15–18]. The
burst therapy based on subthreshold pulses summation [19] is not so common in
electrotherapy, yet. But some studies point out on its demonstrable analgesic effect
without any obvious adaptation of the stimulated tissue [20]. The combination of
these parameters seems to be efﬁcient for the painful swelling treatment. From this
point of view, a horse seems to be appropriate for testing the therapeutic effect of
micro-pulse stimulation. The breeders face the problem of swollen horses’ limbs
quite frequently because just the limbs are the most vulnerable part at the horses
considering their usage for sport and work in harness. Usually, the treatment is quite
costly and also lengthy process because the appropriate quiescent regime is not
possible to observe due to the constant loading of the limbs. The aim of the
following experiment is to observe if the micro-pulse stimulation has any positive
effect on the reduction of swelling of the horses’ legs and if it could help to make
the whole treatment faster and less costly.
2
Experimental Design
The Microstimulator, a device for micro-pulse stimulation, was used in this study. It
generates very short (microseconds) high-voltage pulses (hundreds volts). The main
advantage of using the high voltage (peak up to 1000 V) lies in making the tissue
penetration easier. However, the high voltage must be compensated by the setting
of other parameters to keep safety maximally. An accumulation of subthreshold
pulses is used to form the ﬁnal pulse of deﬁned intensity and width [7]. The design
of the Microstimulator is described more detailed in [21].
It seems that the micro-pulse stimulation would be better and easier to test on
humans, but an important advantage of the testing on animals is at least partial
22
M. Nedvedova et al.

elimination of the placebo effect. We expect that testing on animals could bring
more objective results. Especially in this application, the improvement is visible and
the condition of the horse as a target animal can be well evaluated. The procedure of
the whole experiment was approved by the ethics committee and all steps were
carried out under the veterinary supervision. The informed consent form was pre-
pared for each owner of participating horse. It documented his familiarization with
the methods and aims of the experiment, possible problems and also conditions of
the Privacy Policy. Using the handwritten signature, each owner agreed with the
participation of his horse in the study, with the publication and usage of the results.
All measured data were further provided anonymously, it means that the
alpha-numeric code Txx was assigned to each horse. There were no special limi-
tations of the breed, sex or age. The adult horses between the ages of 5 and 22
participated. The common mark and also the assumption of the subject submission
into the experiment was obvious presence of the acute swelling of some limb. Only
the swelling caused by minor injuries (such as distortion, kicking by another horse
and tendinitis) was considered. In many cases, the aching swelling is attended by
the walking with the limp or total rejecting of the walking. The studied parameters
were the swelling reduction, mobility and also soreness of the injured limb, or
wound healing. The horses with swelling of an infectious origin did not participate
in the study. In this case, the antibiotics are preferred type of medication; however,
the micro-pulse stimulation could be used as an additional supporting therapy for
the acceleration of the healing.
Altogether, nine subjects with swelling of the above-mentioned origin were
documented. The documentation included probable cause of the injury, description
of the affected area, localization and extent of the swelling, description of used
parameters and time of stimulation, reaction of horse to the stimulation and progress
of the treatment in the following days. The whole experiment was supported by
photography and video recordings that ensure demonstrative material for effec-
tiveness evaluation of the micro-pulse stimulation.
Before the stimulation, the affected limb was cleaned and gently washed with the
pure water to reduce the electric resistance of the skin. Then, the electrodes of
Microstimulator were applied on the swollen area. Microstimulator stayed in one
position for several minutes. Then, the position was changed in the proximal
direction from the affected part of the limb. This procedure was repeated once a day
for maximally 30 min at each horse, until the total regress of the problems.
3
Results
For a better interpretation of the results, a case of the 7-year Moravian Warmblood
gelding is described in detail. This horse was affected by a milder form of the mud
fever which is the disease causing irritation and dermatitis in the lower limbs. First,
only a small swelling formed in the pastern area of the left hind-limb and warmed.
The limb was very painful to touch and the horse limped. Occasionally, the owner
Exploring the Therapeutic Effects of Micro-pulse Stimulation
23

cared for the affected limb by washing with the iodine solution and packing into the
wet wrap. After a week, the painful sores and scabs did not regress and the painful
swelling extended from the pastern area up to the hock of the limb, see Fig. 1a. The
ﬁrst electrotherapy was carried out for 30 min and regarding the parameters of
stimulation, the intensity was steadily set on the maximal value, while the fre-
quency of the pulses was changed gradually from minimal to maximal level. After 2
days, before the second therapy, a signiﬁcant swelling reduction was noticed, but
the swollen pastern area remained, see Fig. 1b. The limb was less sensitive to touch
and the horse was less nervous during the therapy. The procedure was repeated
again. Next day, the swelling and pain disappeared completely and the scabs’
healing was evident, see Fig. 1c.
The experimental results are summarized in Table 1. A progress of the treatment
in time is presented at each horse using the description of four observed parameters
—Limp (L), Swelling (S), Sensitivity and Pain (SP), and Warm (W). The values
from the range 0 to 2 classify the actual state of the parameter in particular phase of
the treatment (before therapy, after 1st and 2nd therapy). The evaluation scale has
three levels expressing the strength of appropriate symptom: 2—intensive, 1—mild,
0—practically without symptom.
4
Discussion
The swelling is the typical epiphenomenon of the acute inﬂammation. The
semipermeability of the blood vessels is damaged and normal penetration of water,
salts and other low-molecule-weight substances into the tissues is affected and
water retains inside bigger molecules, such as the proteins. When the tissue is
affected by inﬂammation, the blood vessel wall becomes permeable also for pro-
teins that get into the surrounding tissue and disrupt the osmotic pressure balance
between the interstitium and plasma; therefore the ﬂuid is collected in the tissue. In
Fig. 1 Front view on the limbs before therapy (a), after the ﬁrst (b) and second therapy (c) [7]
24
M. Nedvedova et al.

addition, the function of the lymph system is also affected and the lymph ﬂowing
becomes sluggish [22].
Based on the results of this study, a signiﬁcant improvement is obvious at all
participated horses after two therapies with the micro-pulse stimulation. However,
the course of treatment was very individual depending on the extent of injury. The
longest therapy lasts 4 days; considerable improvement was registered most often
in 2 days. The results of the study and complete documentation (photography and
video recordings) of the experiment were reviewed by the independent veterinarian.
The analgesic and anti-inﬂammatory effects were conﬁrmed [7]. It seems that the
principal reason of the shortened time of swelling treatment using electrostimula-
tion is based just on the inﬂuencing the blood vessel permeability. The mobility of
proteins, blood cells and lymph increases and this supports the ﬂuid absorption in
the swollen area.
The approximate mechanism of the micro-pulse stimulation can be deduced
based on the results of the experiment [23] which was running parallel to this study.
It studied the physical principle of the stimulation effect on the living tissue. The
blood circulation in the stimulated area was monitored using the thermodynamic
sensors. Just the swelling treatment is closely associated with the blood circulation
support. Based on the results, an increased thermal activity connected with an
increased blood perfusion was detected in the stimulated limb compared to the
reference.
5
Conclusion
The presented pilot experiment explores the therapeutic effects of the micro-pulse
stimulation. There were found positive effects on the swelling and pain reduction of
the horses’ limbs. The problem of swollen leg caused by minor injury is widespread
among the horses because of their usage for sport and work in harness. Based on the
Table 1 Evaluation of the
observed parameters during
the phases of the treatment at
each horse marked with Txx
code. Each parameter (L—
Limp, S—Swelling, SP—
Sensitivity and Pain, W—
Warm) was evaluated
according to its intensity (2—
intensive, 1—mild, 0—
practically without it)
Code
Before therapy
After 1st
therapy
After 2nd
therapy
L
S
SP
W
L
S
SP
W
L
S
SP
W
T01
2
1
1
1
1
1
0
0
0
0
0
0
T02
0
2
2
2
0
1
0
0
0
0
0
0
T03
0
2
2
2
0
1
1
1
0
0
0
0
T04
2
1
0
2
1
1
0
1
0
0
0
0
T05
2
2
2
2
1
1
2
1
0
0
0
0
T06
1
2
2
2
0
2
1
1
0
0
0
0
T07
2
2
2
2
1
0
0
0
0
0
0
0
T08
1
2
2
2
0
1
1
1
0
0
0
0
T09
2
2
2
2
1
1
1
1
0
1
0
0
Exploring the Therapeutic Effects of Micro-pulse Stimulation
25

results, we can observe a signiﬁcant shortening of the treatment time when the
stimulation is applied. This preliminary study could be the ﬁrst step leading to the
realization of major research project.
Acknowledgements This work was supported by Brno University of Technology and the
European
Regional
Development
Fund
under
the
project
CEBIA-Tech
Instrumentation
No. CZ.1.05/2.1.00/19.0376.
References
1. Lin, Y., van Weeren, P.R., et al.: Effect of microcurrent electrical tissue stimulation on equine
tenocytes in culture. Am. J. Vet. Res. 67, 271–276 (2006)
2. Kaczmarek, P., et al.: Investigation of the relationship between stimulus parameters and a
human muscle contraction force during stimulation of the gastrocnemius muscle. Artif.
Organs 34, 126–135 (2009)
3. Sooksood, K., Stieglitz, T., Ortmanns, M.: An active approach for charge balancing in
functional electrical stimulation. IEEE T. Biomed. Circ. S. 4, 162–170 (2010)
4. Pakhomov, A.G., et al.: Characterization of the cytotoxic effect of high-intensity, 10-ns
duration electrical pulses. IEEE T. Plasma Sci. 32, 1579–1586 (2004)
5. Nosaka, K., et al.: Muscle damage induced by electrical stimulation. Eur. J. Appl. Physiol.
111, 2427–2437 (2011)
6. Mackey, A.L., et al.: Sequenced response of extracellular matrix deadhesion and ﬁbrotic
regulators after muscle damage in involved in protection against future injury in human
skeletal muscle. FASEB J. 25, 1943–1959 (2011)
7. Tobolova, M.: Microstimulator: Master’s Thesis. Brno University of Technology, Brno
(2012)
8. Gossrau, G., et al.: Microcurrent transcutaneous electric nerve stimulation in painful diabetic
neuropathy: a randomized placebo-controlled study. Pain Med. 12, 953–960 (2011)
9. Lee, J., et al.: The effects of microcurrents on inﬂammatory reaction induced by ultraviolet
irradiation. J. Phys. Ther. Sci. 23, 693–696 (2011)
10. Mercola, J.M., Kirsch, D.L.: The basis for microcurrent electrical therapy in conventional
medical practice. J. Adv. Med. 8, 107–120 (1995)
11. Poltawski, L., Watson, T.: Bioelectricity and microcurrent therapy for tissue healing—a
narrative review. Phys. Ther. Rev. 14, 104–114 (2009)
12. Butcher, M.: How to use POSiFECT® bio-electric stimulation therapy in chronic wounds.
Wound Essentials 2, 186–193 (2007)
13. de Gaspi, F.O., et al.: Effects of the topical application of hydroalcoholic leaf extract of
Oncidium ﬂexuosum Sims. (Orchidaceae) and Microcurrent on the healing of wounds
surgically induced in Wistar rats. Evid. Based Complement. Alternat. Med. 9, art. ID 950347
(2011)
14. Lee, B.Y., et al.: Ultra-low microcurrent in the management of diabetes mellitus, hypertension
and chronic wounds: report of twelve cases and discussion of mechanism of action. Int.
J. Med. Sci. 7, 29–35 (2010)
15. Karnes, J., et al.: High-voltage pulsed current: Its inﬂuence on diameters of histamine-dilated
arterioles in hamster cheek pouches. Arch. Phys. Med. Rehabil. 76, 381–386 (1995)
16. Kloth, L.C., Feedar, J.A.: Acceleration of wound healing with high voltage, monophasic.
Pulsed Current. Phys. Ther. 68, 503–508 (1988)
17. Franek, A., Polak, A., Kucharzewski, M.: Modern application of high voltage stimulation for
enhanced healing of venous crural ulceration. Med. Eng. Phys. 22, 647–655 (2000)
26
M. Nedvedova et al.

18. Burdge, J.J., Hartman, J.F., Wright, M.L.: A retrospective study of highvoltage, pulsed current
as an adjunctive therapy in limb salvage for chronic diabetic wounds of the lower extremity.
Ostomy Wound Manage. 55, 30–38 (2009)
19. Gildemeister, M.: Zur theorie des elektrischen Reizes. V. Polarisation durch Wechselströme.
Berichte über die Verhandlungen der Sachsischen Akademie der Wissenschaften zu Leipzig.
Mathematisch–Physische Klasse 81, 303–313 (1930)
20. Podebradsky, J., Vareka, I.: Fyzikální terapie I. Grada Publishing, Praha (1998)
21. Nedvedova, M., Chmelar, M., Provaznik, I., and Reznicek, Z.: Micro-pulse stimulation. In:
1st International Conference on: Applied Physics, System Science and Computers, Dubrovnik
(2016)
22. Kankova, K., et al.: Patologická fyziologie pro bakalářské studijní programy. Masaryk
University Publishing, Brno (2007)
23. Tobolova, M., et al.: Testing the effects of micro-pulse stimulation on blood circulation using
the thermodynamic sensors. J. Biosens. Bioelectron. 5, 1–7 (2014)
Exploring the Therapeutic Effects of Micro-pulse Stimulation
27

Application of BaTiO3 Perovskite Material
for Piezoelectric Multilayer Actuators
Magdalena Gromada, Mojtaba Biglar, Tomasz Trzepiecinski
and Feliks Stachowicz
Abstract In this paper, the results of the manufacturing of BaTiO3 material des-
tined for use in stacked-disk multilayer actuator production are presented. SEM
microstructures and electric properties of the fabricated pellets are presented and
discussed. The dilatometric curve was executed using the high temperature
dilatometer in order to determine at which temperature barium titanate pellets and
beams should be sintered to receive full dense sinters. Finally, the problem of metal
layer deposition on barium titanate ceramics during actuator fabrication is
considered.
Keywords Barium titanate ⋅Piezoelectric properties ⋅Multilayer actuator
1
Introduction
The dielectric properties of BaTiO3 are controlled by purity and microstructure
which are dependent on the methods of preparation [1]. Controlling the phase,
composition homogeneity, particle size and monodispersity are other concerns in
developing techniques for synthesizing barium titanate. The electrical properties of
ceramic materials are a direct result of their microstructure [2], while it is well
M. Gromada
Ceramic Department CEREL, Institute of Power Engineering,
Research Institute, ul. Techniczna 1, 36-040 Boguchwała, Poland
e-mail: gromada@cerel.pl
M. Biglar ⋅T. Trzepiecinski ⋅F. Stachowicz (✉)
Department of Materials Forming and Processing, Rzeszow University
of Technology, al. Powstańców Warszawy 8, 35-959 Rzeszów, Poland
e-mail: stafel@prz.edu.pl
M. Biglar
e-mail: m_biglar@prz.edu.pl
T. Trzepiecinski
e-mail: tomtrz@prz.edu.pl
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_4
29

known that there is a connection between microstructure and type of porosity, grain
size distribution and second-phase content [3]. Generally, the properties of BaTiO3
depend on the synthesis method and sintering regime.
The traditional solid-state method used in our research for BaTiO3 synthesis has
strict criteria with regard to particle size and purity of the raw material, while it is
also easy to produce non-homogeneous powders [4]. The advantages of the
solid-state reaction used in mass production are its simplicity, precise stoichiometric
control and low cost, with the main disadvantage being that the high calcining
temperature results in very large and non-uniform grain sizes [5]. As a conse-
quence, this method does not allow the production of materials with a high
dielectric constant [5]. In order to eliminate the disadvantages of the solid-state
reaction, a variety of novel wet chemical synthesis methods have been developed
[6].
In this study, we analyse the manufacture of BaTiO3 material destined for use in
stacked-disk multilayer actuator production. SEM microstructures and electric
properties of the fabricated pellets are also presented and discussed. The problem of
metal layer deposition on barium titanate ceramics during actuator fabrication is
considered.
2
Results and Discussion
The BaTiO3 pellets were obtained in a mould of external diameter 11.5 mm, with
home-made BaTiO3 granulate of weight 0.6 g and uniaxial pressing under a
pressure of 1 MPa. The beams were uniaxially pressed under a pressure of 5 MPa
in a full mould ﬁlled for beam execution with barium titanate granulate. The green
discs and beams obtained in this manner were then isostatically pressed under a
pressure of 150 MPa.
The dilatometric curve was executed using the high temperature dilatometer
from BÄHR-Gerätebau GmbH company in order to determine at which temperature
barium titanate pellets and beams should be sintered to receive full dense sinters.
On the basis of the obtained dilatometric curve (Fig. 1a), the maximal sintering
temperatures were estimated as 1250 °C. Pellets and beams were sintered in the
electric furnace with the rate of both heating and cooling equal to 100 °C/h and the
dwell time—4 h. Sintered pellets and beams are hereafter denoted as BT_0.
The electrical properties of BT_0 pellets were determined, with pellet AC
conductivity measured as a function of frequency at 150 °C as shown in Fig. 1b. It
is well known that at room temperature, conductivity is almost independent of
temperature and instead depends strongly on frequency. Analysis of AC conduc-
tivity versus frequency plot (Fig. 1b) reveals that BT_0 conductivity increases with
increasing frequency. However, there is a little signature of a plateau in the curve at
higher frequencies, beyond the measuring range. It is therefore evident that the
linear (dispersion) region represents some sort of relaxation process. The depen-
dence of AC conductivity on frequency may be inﬂuenced by carrier transport
30
M. Gromada et al.

excited to localised states near the band edge, as well as hopping of the charge
carriers through trap sites separated by energy barriers of various heights. In other
words, AC conductivity contributes in the form of the hopping of localised and free
charge carriers [7].
Particularly, at low temperature, AC conductivity can occur through the carrier
motion over a shorter distance between sites in the potential well. In the present
study, the AC conductivity for BT_0 was found to be of the order of 10−9, which is
close related with literature. It is considered that the AC conductivity is contributed
by hopping of localized and free charges.
At room temperature, the dielectric constant value for BT_0 pellets was found to
decrease with frequency (Fig. 2a). BT_0 thus exhibits normal dielectric dispersion
because the dielectric constant (ε′ and ε′′) decreases with increasing frequency. The
phenomenon of dielectric dispersion is attributed to Maxwell–Wagner type inter-
facial polarisation in accordance with Koop’s phenomenological theory [8]. This
theory states that the conductivity of grain boundaries contributes more to dielectric
permittivity at lower frequencies and temperatures. The sintering of polycrystalline
perovskite involves the formation of grains with high conductivity. These grains are
separated by poorly conducting grain boundaries, as can been seen in SEM images,
(a)
(b)
Fig. 1 The dilatometric curve of beam before sintering (a) and AC conductivity versus frequency (b)
(a)
(b)
Fig. 2 Dielectric constant versus frequency (a) and dielectric loss versus frequency (b)
Application of BaTiO3 Perovskite Material for Piezoelectric …
31

and exhibit low conduction; BT_0 perovskite thus behaves as a heterogeneous
dielectric material.
In addition Ti4+ is reduced to Ti3+ and formed a conduction electron (Ti4+•e) in
order to keep charge neutrality. It can be found that dielectric permittivity measured
at lower frequency is always greater than higher frequency. With the increasing
frequency, dielectric constant decreases very fast up to 105 Hz and in frequency
range from 106 to 107 Hz it is almost constant. The higher value of dielectric
constant of 1700 may possibly also strongly depend on the density of the sample
due to the sintering process of BaTiO3.
The dielectric loss represents a combined result of electrical conduction and
orientational polarization of the matter. Figure 2b shows the variation of dielectric
loss tangent δ (tanδ) with frequency for BT_0. The investigated material shows a
normal dielectric behaviour. Dielectric loss tangent decreases with increasing fre-
quency of the alternating ﬁeld, and the maximum value of 0.5 was obtained at low
frequencies. The permittivity loss curve at 30 °C shows two relaxation peaks which
occurs when the frequency jumping of electrons Ti4+ ↔Ti3+ is equal to the
frequency of the applied AC ﬁeld [9]. Moreover, the values of tanδ decrease at
higher frequency in the range 0.007–0.001. Therefore, the energy losses were due
to conductivity and dipole relaxation. The loss factor tan δ of a dielectric material is
a useful indicator of the energy loss as heat.
The ﬁrst stage of actuator fabrication is frequently characterised by the problem
of metal layer deposition on barium titanate ceramics. In order to ensure the most
suitable adhesion of layers, pellets were polished using abrasive paper 320, which
also enabled the production of the expected pellet thickness of 0.71 mm and
diameter of 9.66 mm.
Two commercial products (Technicqll, Chester Molecular Metal Super) devel-
oped for metallic layer deposition were applied. Technicqll is a silver-based pro-
duct, which allows the creation of adhesive layers that conduct an electric current,
and is characterised by a low resistance equal to 0.01 Ω. The second product—
Chester Metal Super—is a two-element tixotropic epoxy-metallic composite. Both
commercial products were utilised during trials of metallic layer deposition on
BT_0 pellets. The treated samples are hereafter denoted as BT_0_S for Technicqll
and BT_0_M for Chester Molecular Metal Super treatments.
Analysis of the top-most metallic layers deposited on the barium titanate pellets
revealed their high quality at the macroscopic scale. However, in order to verify
whether the bonding layers adhered well to the ceramic pellets, SEM images of
fracture surfaces for BT_0_S (Fig. 3a, b) and BT_0_M (Fig. 3c, d) were obtained.
Considered fracture surfaces of BT_0_S and BT_0_M samples with indicated
three areas under observation are presented in Fig. 4. In both samples, areas number
1 and 3 are placed on barium titanate ceramic and area number 2 is situated on
bonding layers.
Results of weight % of chemical elements presented in these three indicated
areas for BT_0_S and BT_0_M samples, were determined. As one can expected, on
areas number 1 and 3, Ba and Ti elements are presented and the weight percent of
these elements is very similar in all area, 56.9% ± 0.4 and 23.5% ± 0.1
32
M. Gromada et al.

respectively for Ba and Ti. In the case of BT_0_S sample, the metallic layer is
mainly composed from silver (66.4%), which is also according to expectation.
A small amount of Si (3.3%) in bonded layer comes probably from the applied
product composition. The chemical composition of Chester Molecular Metal Super
Fig. 3 SEM images of (a, b) BT_0_S and (c, d) BT_0_M fracture surfaces at two different
magniﬁcations: (a, c) × 50 and (b, d) × 200
Fig. 4 SEM microstructure of fracture surface of BT_0_S (a) and BT_0_M (b) samples with
identiﬁed three considered areas
Application of BaTiO3 Perovskite Material for Piezoelectric …
33

was not presented on the product datasheet but from obtained results follow that this
product is composed from Fe (10.7%), Cr (25.4%), Ti (12.8%) and Si (22.3%). It
should be underlined that at both metallic layers Ti element is present but it is not
sure whether its appearance is connected with local migration from ceramic layers
to bonding one.
The ﬁrst short actuator presented in Fig. 5, was made from a piece of BT_0
beams with cutting three gaps by saw. Finally, these gaps are ﬁlled in using
Technicqll product to obtain metal layers.
3
Conclusions
The investigated material shows a normal dielectric behaviour. Dielectric loss
tangent decreases with increasing frequency of the alternating ﬁeld, and the max-
imum value of 0.5 was obtained at low frequencies. What is interesting, the sin-
tering temperature did not increase considerably the size of grains. The obtained in
this approach the sinter microstructure, is very similar to this presented in literature
by other authors. In order to receive better result of the barium titanate dielectric
constant, the microstructure of sinter must be improved in the direction of the
smaller grains getting.
The SEM images of both BT_0_S and BT_0_M samples reveal high quality
ceramic-metallic-ceramic bonding, with very good adhesion properties and without
any fractures. In the case of BT_0_S, additional intermediate layers between the
barium titanate and metallic material were created. These thin layers should ensure
better adhesion of appropriate materials. Therefore, Technicqll was selected for use
in further investigations.
Acknowledgements. The research leading to these results has received funding
from the People Programme (Marie Curie Actions) of the European Union’s
Seventh Framework Programme FP7/2007-2013/under REA grant agreement
No. PITN-GA-2013-606878.
Fig. 5 Photo of the actuator
manufactured from barium
titanate ceramic and three
metallic layers
34
M. Gromada et al.

References
1. Zhao, Z., Buscaglia, V., Viviani, M., Buscaglia, M.T., Mitoseriu, L., Testino, A., Nygren, M.,
Johnsson, M., Nanni, P.: Grain-size effects on the ferroelectric behavior of dense nanocrys-
talline BaTiO3 ceramics. Phys. Rev. B. 70, 024107 (2004)
2. Hu, J., Shen, Z.: Intragranular heterojunctions formed by ordered coalescence of strontium and
barium titanate nanocrystals. Scr. Mater. 107, 14–17 (2015)
3. Cai, W., Fu, C., Lin, Z., Deng, X.: Vanadium doping effects on microstructure and dielectric
properties of barium titanate ceramics. Ceram. Int. 37, 3643–3650 (2011)
4. Yu, P., Wang, X., Cui, B.: Preparation and characterization of BaTiO3 powders and ceramics
by the Sol-Gel process using organic monoacid as surfactant. Scr. Mater. 57, 623–626 (2007)
5. Kao, C.F., Yang, W.D.: Preparation of barium strontium titanate powder from citrate precursor.
Appl. Organomet. Chem. 13, 383–397 (1999)
6. Chen, J.F., Shen, Z.G., Liu, F.T., Liu, X.L., Yun, J.: Preparation and properties of barium
titanate nanopowder by conventional and high-gravity reactive precipitation methods. Scr.
Mater. 49, 509–514 (2003)
7. Park, J.H., Yoo, D.H., Kim, C.S., Yang, H.S., Moon, B.K., Jung, G.J., Jeong, E.D., Hong,
K.S.: Synthesis, structure and dielectric properties of BaTiO3 nanoparticles. J. Korean Phys.
Soc. 49, S680–S683 (2006)
8. Koops, C.G.: On the dispersion of resistivity and dielectric constant of some semiconductors at
audiofrequencies. Phys. Rev. 83, 121–124 (1951)
9. Batoo, K.M., Kumar, S., Lee, C.G., Alimuddin.: Inﬂuence of Al doping on electrical properties
of Ni–Cd nano ferrites. Curr. Appl. Phys. 9, 826–832 (2009)
Application of BaTiO3 Perovskite Material for Piezoelectric …
35

Modeling of the Waterﬂooding Process
in the Presence of Discontinuities in the Oil
Reservoirs
Vladimir Astafev, Elena Andriyanova and Andrey Kasatkin
Abstract The knowledge of the nature of ﬂooding allows us to optimize the system
of oilﬁeld development. The study of the ﬁltration process in reservoirs with dis-
continuities, such as highly permeable cracks or impermeable barriers, has a great
importance for the oilﬁeld development. The steady-state ﬂow process of incom-
pressible ﬂuid to the production well in a reservoir of constant height and perme-
ability is considered. There is a thin area in the reservoir, which might be a highly
permeable crack or impermeable barrier. The production and injection wells are
placed inside the reservoir’s external boundary. The characteristics of ﬂooding
process are studied for different locations of the discontinuity and a pair of wells.
The ﬂow lines of the ﬂuid ﬂow will be analyzed for every considered case.
Keywords Hydrodynamic model of waterﬂooding ⋅Highly permeable cracks ⋅
Impermeable barriers
1
Introduction
As we know, the main part of ﬂuid ﬂow occurs through the more permeable zones,
therefore any deviation in the reservoir homogeneity acts on the production [1].
A characteristic feature of the development process of such reservoirs has the
deviation in the dependence well productivity and rock permeability, signiﬁcant
dependence of IPR curves (Inﬂow Performance Relationship) on the pressure, etc.
Oil ﬁltration modelling in reservoirs with cracks is also interesting from the point
of view of application for hydraulic fracturing. Hydraulic fracturing is currently one
of the most effective methods to increase oil production [2], especially for tight
sands and shale gas. Therefore these days it is especially important to study the
V. Astafev (✉) ⋅E. Andriyanova ⋅A. Kasatkin
Oil Field Development Department, Samara State Technical University,
Molodogvardeiskaya Str., 244, 443100 Samara, Russian Federation
e-mail: vladimir.astafev@mail.ru
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_5
37

ﬁltration process in the reservoirs with tectonic faults, hydraulic fractures (HF) and
impermeable boundaries.
The previous our papers [3–6] have discussed the ﬂuid ﬂow to the single pro-
duction well in the reservoir with cracks of different permeability, and the skin
effect equations were deﬁned for every case. These articles show the impact of such
inclusions in the reservoir on the waterﬂooding process. The problem is about the
fact, that in some cases injected water appears in producing well too fast, so we
need understand the mechanism of ﬂuid ﬂow and nature of water breakthrough
time.
So, in this work modelling of the ﬂuid ﬂow process from an injection well to a
production well in the presence of crack (discontinuity) with different permeability
will be discussed, the impact of such discontinuities on the nature of the ﬂuid ﬂow
process will be studied. The task is modiﬁed by the representation of crack in the
section view of zero thickness but ﬁnite conductivity and by the difference of
pressure above and below the section.
2
Problem Formulation
Let us consider a plane stationary ﬂow of incompressible ﬂuid to the vertical
production well in an isotropic porous medium. This process in the plane (x, y) is
described by the equation of incompressibility and the Darcy’s law of ﬁltration [1]:
divV!ðx, yÞ = 0, V!= −ðk ̸μÞgrad p,
ð1Þ
where V(x, y) is the velocity vector of ﬂuid ﬁltration, p(x, y) is the pressure in the
liquid, μ is the ﬂuid viscosity and k is the reservoir permeability by the thickness h.
In early works of other authors the highly permeable area is usually represented
by ellipse [7, 8]. In our works [3–6, 9] the discontinuity in the section view of zero
thickness but ﬁnite conductivity.
Let us consider, that in the reservoir at the point M1(x1, y1) is placed the pro-
duction well with a ﬂow rate Q1 and at the point M2(x2, y2) is placed the injection
well with a ﬂow rate Q2. Inside the external boundary there is a crack with length
2 l and thickness 2δ (δ ≪l) and permeability kf. Let us consider that the crack is
oriented along the axis x, and its center coincides with the origin plane (x, y).
Then, as described in work [9], the ﬂow potential, can be represented in the
form:
ΦðzÞ = q −1φðzÞ = lnðz −z0Þ +
∑
∞
n = 0
cnz −n.
ð2Þ
where q = µQ/(2πkh) is the modiﬁed ﬂow rate, and cn are the unknown coefﬁcients.
38
V. Astafev et al.

In the symmetrical case [9], when the well is located on the crack line, the
fracture conductivity parameter Fcd = δkf/lk has been introduced and the ﬂow
potential has been written in the following form:
ΦðvÞ = lnðv, v0Þ +
∑
∞
n = 0
1
n
n ⋅Fcd −1
n ⋅Fcd + 1 ðvv0Þ −n


+ C0.
ð3Þ
But if we consider nonsymmetrical case, when the well is located on the some
distance from the discontinuity, or we have a couple of injection and production
wells, we need to evaluate the inﬂow and the outﬂow from the crack. Thus we need
to take into account the pressure difference above and below the section, and we use
more complicated boundary conditions [5, 6]:
α0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 −ξ2
p
d
dξ ReðΦ + + Φ −Þ = ImðΦ + −Φ −Þ,
β0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 −ξ2
p
d
dξ ImðΦ + + Φ −Þ = −ReðΦ + −Φ −Þ;
(
ð4Þ
where −1 < ξ = x/l < 1, Φ+(ξ) and Φ−(ξ) are the ﬂow potentials above and below
the section, coefﬁcient a0 = δkf /lk is similar to Fcd for the hydraulic fractures and
β0 = δk/lkf is very important for the impermeable case.
3
Problem Solution
Now let us consider the case when we have a pair of wells. Flow potential can be
expressed by this equation:
φðzÞ = q1 lnðz −z1Þ + q2 lnðz −z2Þ + ∑
∞
n = 0
cnz −n.
ð5Þ
Further, we consider the case when q1 = 1 and q2 = −1 or the case of injection
and production wells with equal injectivity and productivity. The solution the
boundary-value problem (4) allows us to express the ﬂow potential in the following
form:
φðvÞ = ln ðv, v1Þ
ðv, v2Þ + ∑
∞
n = 1
n ⋅α0 −1
n ⋅α0 + 1
cos nθ1
nρn
1
−cos nθ2
nρn
2


−i n ⋅β0 −1
n ⋅β0 + 1
sin nθ1
nρn
1
−sin nθ2
nρn
2




v −n + C0.
ð6Þ
where z = l(ν + ν−1)/2, v1 = ρ1 ⋅eiθ1, v2 = ρ2 ⋅eiθ2.
For the case, when α0 = ∞and β0 = 0, the ﬂow potential can be expressed as
follows:
Modeling of the Waterﬂooding Process in the Presence …
39

φðvÞ = ln ðv, v1Þ
ðv, v2Þ −
∑
∞
n = 1
1
n
vv1
ð
Þ −n −
vv2
ð
Þ −n
ð
Þ = ln ðv, v1Þ
ðv, v2Þ
1 −
1
vv2


1 −
1
vv1

 .
ð7Þ
For the case, when α0 = 0 and β0 = ∞, the ﬂow potential can be expressed as
follows:
φðvÞ = ln ðv, v1Þ
ðv, v2Þ −∑
∞
n = 1
1
n
vv1
ð
Þ −n −
vv2
ð
Þ −n
ð
Þ = ln ðv, v1Þ
ðv, v2Þ
1 −
1
vv1


1 −
1
vv2

 .
ð8Þ
Flow lines for this kind of potentials are shown on the Figs. 1 and 3 (left—for
the case α0 = ∞and β0 = 0; right – for the case α0 = 0 and β0 = ∞).
On the Figs. 2 and 4 we can see the water tracer lines along the ﬂow lines. These
results have been obtained by the use of method of calculations, published in [10].
Different colors in this pictures, shows the boundaries between the ﬂooding stages,
so we can predict the water front at any moment, and water breakthrough time.
As we can see, the obtained ﬂow potential equation allows us to solve the
problem for any wells and discontinuity location and for different fracture
conductivity.
Fig. 1 Streamlines of the ﬂuid, the injection well located at the point (0, −1.5), the production
well—at the point (0, 0.5), for the values of α0 = ∞; β0 = 0 (left) and α0 = 0; β0 = ∞(right)
40
V. Astafev et al.

Fig. 2 Streamlines waterﬂooding process with tracer lines, the injection well located at the point
(0, −1.5), the production well—at the point (0, 0.5), for the values of α0 = ∞; β0 = 0 (left) and
α0 = 0; β0 = ∞(right)
Fig. 3 Streamlines of the ﬂuid, the injection well located at the point (−1, −1), the production
well—at the point (1, 1), for the values of α0 = ∞; β0 = 0 (left) and α0 = 0; β0 = ∞(right)
Modeling of the Waterﬂooding Process in the Presence …
41

4
Conclusions
In this work the formulation and solution of the problem of waterﬂooding process at
the presence of a crack of different permeability has been done. The solution has
been obtained under consideration the discontinuity like the crack zero thickness
but ﬁnite conductivity. More general boundary conditions were considered taking
into account the pressure difference above and below the section. This solution is
suitable for any cases of various wells and discontinuity places and for different
values of fracture permeability.
In the ﬁnal part of the paper the nature of ﬂuid ﬂow was analyzed. The tracer
lines for waterﬂooding process were calculated. As the result, we can predict the
water front at any time of the process and deﬁne the water breakthrough time.
As we can see, the problem has enough interest from the petroleum engineers.
Further development of the solutions is to present the ﬂow potential through sin-
gular integral equations, which will greatly expand the applications.
Acknowledgements This work is performed on the grant of the Russian Science Foundation
(Project №15-17-00019).
Fig. 4 Streamlines waterﬂooding process with tracer lines, the injection well located at the point
(−1, −1), the production well—at the point (1, 1), for the values of α0 = ∞; β0 = 0 (left) and
α0 = 0; β0 = ∞(right)
42
V. Astafev et al.

References
1. Muskat, M.: The Flow of Homogeneous Fluids Through Porous Media. J. W. Edwards, Inc.,
Ann Arbor, Michigan (1946)
2. Economides, M., Oligney, R., Valko, P.: Uniﬁed Fracture Design. Bridging the Gap Between
Theory and Practice. Alvin, TX, Orsa Press (2002)
3. Astaﬁev, V.I., Andriyanova, E.V.: Modeling of the Fluid Flow to the Production Well in the
Presence of Discontinuities in the Reservoir. In: 11th International Conference on Fluid
Mechanics, Budapest, Hungary, pp. 65–68 (2015)
4. Astaﬁev, V.I., Andriyanova, E.V.: Inﬂuence of reservoir’s discontinuities on the process of oil
ﬁltration to the production Well. In: Fourth International Geoscience Conference, Deep
Subsoil and Science Horizons, Tyumen, Russia, pp. 52–56 (2015). DOI:10.3997/2214-4609.
20141203420152015
5. Astaﬁev, V.I., Andriyanova, E.V.: The inﬂuence of the reservoir discontinuities on ﬂuid
ﬁltration to the production well. WSEAS Trans. Fluid Mech. 11(2), 10–17 (2016)
6. Andriyanova, E.V.: The inﬂuence of discontinuities in the reservoir on well productivity. In:
78th EAGE Conference and Exhibition 2016—Student Programme, Vienna, Austria, (2016).
DOI:10.3997/2214-4609.201601621
7. Prats, M.: Effect of vertical fractures on reservoir behaviour—incompressible ﬂuid case.
SPE J. 103–118 (1961)
8. Kanevskaya, R.D.: Mathematical Modeling of the Development of Oil and Gas using
Hydraulic Fracturing. Nedra Publishing, Moscow (1999) (in Russian)
9. Astaﬁev, V.I., Fedorchenko, G.D.: Simulation of ﬂuid ﬂow in the presence of a crack fracture.
Vestnik of SamSTU, Ser. Phys. Math. Sci. 2, 128–132 (2007) (in Russian)
10. Astaﬁev, V.I., Kasatkin, A.E.: Modeling and Numerical Calculation of Piston-like Oil
Displacement for Doubly-periodic Systems of Oil Fields Development. In: VI International
Conference on Coupled Problems in Science and Engineering, Venice, Italy, 734–743 (2015)
Modeling of the Waterﬂooding Process in the Presence …
43

Terahertz Spectroscopy Applications
in Medicament Analysis
Kateřina Sulovská
Abstract Terahertz spectroscopy came to the attention of the broader scientiﬁc
community in the 90s of the 20th century with the development of science and
technology in particular, enabling to work in terahertz spectral region. Terahertz
spectroscopy has a wide possibilities of applications beginning in classical spec-
troscopy of gun powders, plastics, explosives, liquids, through terahertz imaging of
medicaments, tissues, materials or layers, to remote observation/identiﬁcation of
hidden objects mainly for security purposes. The aim of this paper is to introduce
possible applications in analysis of pharmaceuticals with different amount of active
ingredient content. According to the results, the classical terahertz spectroscopy is
not as suitable as other spectroscopic methods targeted on analyzing content of
pharmaceuticals showing persistent inconveniences of this spectroscopy. All
measurements were done using the TPS Spectra 3000 instrument.
Keywords Attenuated
total
reﬂection
(ATR)
terahertz
time-domain
spec-
troscopy ⋅Terahertz imaging ⋅Transmittance ⋅Absorbance ⋅Pharmaceutical
analysis ⋅Euthyrox
1
Introduction
Terahertz (THz) spectroscopy came to the attention of the broader scientiﬁc com-
munity in the 90s of the 20th century with the development of science and tech-
nology in particular, enabling to work in terahertz spectral region. Terahertz
spectroscopy has a wide possibilities of applications beginning in classical spec-
troscopy of gun powders, plastics, explosives, liquids, through terahertz imaging of
medicaments, tissues, materials or layers (including searching for cracks, air
K. Sulovská (✉)
Faculty of Applied Informatics, Department of Electronics and Measurement,
Tomas Bata University in Zlín, Nám. T. G. Masaryka 5555, 760 01 Zlín,
Czech Republic
e-mail: sulovska@fai.utb.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_6
45

pockets, etc.), to remote observation/identiﬁcation of hidden objects mainly for
security purposes (see Fig. 1). Therefore, the area for potential employment of the
THz spectroscopy is very wide [1–9].
The drug control is a pharmaceutical discipline theoretically and practically
aimed at ensuring the quality, safety and drug efﬁcacy, together with identiﬁcation
and quantity analysis of the medicament, with emphasis on detection of active
ingredients, accompanying impurities and the stability of drugs. The chemical
structure of the drug is the starting point for drug control and determination of
appropriate procedure for analyses. The identiﬁcation of pharmaceuticals can be
made on ground of physical constants (melting point, distillation range, refractive
index, optical rotation or density) with utilization of physico-chemical methods
(spectral, separation and electrochemical), chemical reaction, etc.
This paper is dealing with a spectral method, which can in future serve as a
useful tool to obtain sufﬁcient information for handling and manufacturing drugs—
the THz spectroscopy. One of the commonly used methods for examination of
pharmaceuticals in terahertz spectral region is attenuated total reﬂection (ATR) THz
time-domain spectroscopy [10, 11]. This method is very suitable for samples that
can be measured in a whole, e.g. tablets, plastics. Together with THz imaging, one
can get an information about the content behavior in the sample, i.e. wetting,
stability, and decomposition of content [12, 13]. Such information may be crucial
for further processes in not only developing the medicament but also for mecha-
nisms, kinetics and potential effects on pre-clinical and clinical procedures. THz
spectroscopy may be a good tool for these trials, as it stand between microwave and
infrared spectral region gap and therefore can support other spectroscopic
techniques.
The aim of this paper is therefore to demonstrate the possibility of detecting
speciﬁc vibrations and rotations of medicament content by the THz spectroscopy
with respect to the amount of active ingredient, and to evaluate the possibility of
these technique as a stand-alone method.
Fig. 1 Examples of practical applications of THz spectroscopy—THz imaging of pharmaceu-
ticals [14]
46
K. Sulovská

2
Materials and Methods
As a testing medicament used in this paper was selected the Euthyrox tablets by
Merck with 75, 88 or 100 µg of levothyroxinum natricum. These pills are used for
patients with thyroid issues or as a prophylaxis as they supplement the thyroid
hormone (Fig. 2).
Tablets were measured whole as manufactured using the ATR module of the
TPS Spectra 3000 spectrometer, which measures spectra by direct contact of sample
with the module inserted on the workplace of the device. Each measurement were
done ten times, for three tablets of same dosage to obtain as accurate data as
possible.
All experiments were done on the TPSTM Spectra 3000 by TeraView Ltd., using
the ATR module with contact angle 35°. The measured data of prepared samples
are processed via device software. The spectrometer was operating with resolution
of 1.2 cm−1 (32 GHz), scanning frequency of 1800 scans per second with 30 scans
in each point. The instrument was operating in vacuum to lower the amount of
atmospheric water vapours spectra. As a reference, a module without sample was
chosen. Thickness of each sample was measured at ten different spots and averaged
for proper spectra calculations in TeraView spectrometer’s software and to get
representative values of material for a total overview of selected behaviour in THz
frequency. The measurements were done in ambient laboratory conditions. The
frequency range of the experiment was set from 0 THz to 3 THz.
3
Results and Discussion
The whole experiment was focused on changes in behavior in THz spectral region
of Euthyrox tablets with different content of active ingredients. As can be seen from
following ﬁgure, some characteristic peaks are found throughout whole spectra
between 0.5 and 2.1 THz for absorbance measurements. Aside this region, small
changes in spectra occurs. All behaviors are in agreement with basic absorbance
predicaments, supported by data obtained for the transmittance.
The main issue during assigning speciﬁc spectral peaks to rotation/vibration
positions of known molecules in pharmaceuticals is the fact, that these medicaments
always contain not only vibrations/rotations of smaller molecules, but also states of
Fig. 2 Levothyroxinum
natricum (also known as
levothyroxine sodium) [15]
Terahertz Spectroscopy Applications in Medicament Analysis
47

more complex molecules, their structure, and their amount in the whole medica-
ment resulting in intermolecular bonds overlaying monitored molecules (Fig. 3).
An example of this statement and presence of very similar compounds is the
vibration of aromatic rings, which are present in majority of drugs, and can be
found at 1.75 THz. The methyl torsion group vibration can be found in the spectral
region of 0–0.2 THz and is connected with the compounds of the Euthyrox tablets.
The peak at 1.15 THz belong to bond with hydrogen, while peaks between 1.45 and
1.55 THz belong to O–H vibrations, which are also contained in the Euthyrox
tablets, and are probably caused by the lattice vibrations. A very small peak around
1.75 THz belonging to N–H vibration occurs, together with the C–C vibration
small peak around 1.9 THz. Therefore, the behaviors in region from 1 to 2 THz are
the most important ones for characterization of tablets, as the region beneath
2.5 THz may contain noise and interference, even when the vacuum was used for
the measurements. Nevertheless, the region behind 2 THz contain peaks of CH3
transition between torsional levels, nitrogen bonds, O–H vibrations and vibrations
like NH2 belonging not only to active ingredient of the medicament, but there can
be also traits of compounds forming the ﬁlling.
Figure 4 depicts the information about average refractive indices of examined
medicaments. As is visible from the plot, the behavior for all three samples is
identical, with small changes in spectra at the beginning of observed spectral range
at 0–0.3 THz, which can be caused by differences in amount of ﬁlling and active
ingredient and also with noise produced by small vibrations in lattice. Such
information can be a satisfactory one in case of drug identiﬁcation.
Fig. 3 Plot for averaged absorbance of particular medicaments
48
K. Sulovská

4
Conclusion
This paper presents experiment focused on the solid pills with different amount of
active ingredient (and also slightly different amount of ﬁllings). Terahertz spec-
troscopy is said to be a potential method for distinguishing between various
pharmaceuticals or compounds in different ratios, or for their identiﬁcation during
pharmaceutics study. The challenging issue of studying basic steps for identiﬁcation
and analysis of drugs is presented. Results of our experiments showed that this
method has a potential for such analyses, but a more effective and accurate
techniques/procedures must be found; currently the method remains only a sup-
portive and highly experimental one. The main issue is to discover proper peak
positions belonging to chosen active ingredient without any doubt as the terahertz
spectroscopy may provide misleading data due to intermolecular vibrations of other
compounds in medicament, and to study spectral changes over time. Therefore,
further and discerning research in this sphere is necessary.
Acknowledgements This work was supported by the Ministry of Education, Youth and Sports of
the Czech Republic within the National Sustainability Programme project No. LO1303
(MSMT-7778/2014)
and the
European
Regional
Development
Fund
under
the
project
CEBIA-Tech (CZ.1.05/2.1.00/03.0089). The authors declare no competing ﬁnancial interests.
Fig. 4 Averaged refractive indices for Euthyrox tablets
Terahertz Spectroscopy Applications in Medicament Analysis
49

References
1. Shen, Y.C., Taday, P.F., Newnham, D.A., Pepper, M.: Chemical mapping using reﬂection
terahertz pulsed imaging. Semicond. Sci. Technol. 20, S254–S257 (2005)
2. Soltani, A., Jahn, D., Duschek, L., Castro-Camus, E., Koch, M., Withayachumnankul, W.:
Attenuated total reﬂection terahertz time-domain spectroscopy: uncertainty analysis and
reduction scheme. IEEE Trans. Terahertz Sci. Technol. 6, 1–8 (2015)
3. Banerjee, D., von Spiegel, W., Thomson, M.D., Schabel, S., Roskos, H.G.: Diagnosing water
content in paper by terahertz radiation. Opt. Express 16, 9060–9066 (2008)
4. Watanabe, Y., Kawase, K., Ikari, T., Ito, H., Ishikawa, Y., Minamide, H.: Component spatial
pattern analysis of chemicals using terahertz spectroscopic imaging. Appl. Phys. Lett. 83,
800–802 (2003)
5. Ajito, K., Ueno, Y., Song, H.-J.: Visualization of pharmaceutical drug molecules by Terahertz
chemical imaging. NTT Tech. Rev. 10 (2012)
6. Giles, J.P., Raitt, B.J., Joseph, C.S., Hines, M.E., Giles, R.H.: Investigating the effects of
terahertz radiation on Bacillus subtilis. In: Tuchin V.V., Duncan D.D., Larin K.V., Leahy M.
J., Wang R.K. (eds.) Proceedings SPIE 8222; Dynamics and Fluctuations in Biomedical
Photonics IX. p. 9. SPIE (2012)
7. Sun, Y., Fischer, M.B., MacPherson, E.P.: Effects of formaling ﬁxing on the THz properties
of biological tissues. J. Biol. Opt. 14(6), 6401–6417 (2009)
8. Globus, T., Bykhovskaia, M., Woolard, D., Gelmont, B.: Sub-millimetre wave absorption
spectra of artiﬁcial RNA molecules. J. Phys. D Appl. Phys. 36, 1314–1322 (2003)
9. Jin, Y., Kim, G., Jeon, S.: Terahertz dielectric properties of polymers. J. Korean Phys. Soc.
49, 513–517 (2006)
10. Suhandy, D., Suzuki, T., Ogawa, Y., Kondo, N., Ishihara, T., Takemoto, Y.: A quantitative
study for determination of sugar concentration using attenuated total reﬂectance terahertz
(ATR-THz) spectroscopy. Proceedings SPIE. 8027, 802705–802705-6 (2011)
11. Takebe, G., Kawada, Y., Akiyama, K., Takahashi, H., Takamoto, H., Hiramatsu, M.:
Evaluation of drug crystallinity in aqueous suspension using terahertz time-domain attenuated
total reﬂection spectroscopy. J. Pharm. Sci. 102, 4065–4071 (2013)
12. Liu, G.F., Ma, S.H., Ji, T., Zhao, H.W., Wang, W.F.: Differentiation of illicit drugs with THz
time-domain spectroscopy. Nucl. Sci. Tech. 21, 209–213 (2010)
13. Williams, M.R.C., Aschaffenburg, D.J., Ofori-Okai, B.K., Schmuttenmaer, C.A.: Intermolec-
ular vibrations in hydrophobic amino acid crystals: experiments and calculations. J. Phys.
Chem. B. 117, 10444–10461 (2013)
14. Zhang, Q., Gladden, L.F., Avalle, P., Axel Zeitler, J., Mantle, M.D.: Terahertz pulsed imaging
and magnetic resonance imaging as tools to probe formulation stability. Pharmaceutics 5,
591–608 (2013)
15. Pharmacopeia,
U.S.:
Levothyroxine
sodium.
http://www.pharmacopeia.cn/v29240/
usp29nf24s0_m45000.html. Accessed 09 July 2016
50
K. Sulovská

Stability of Capillary Waves of Finite
Amplitude
Alexander Petrov, Mariana Lopushanski and Vladimir Vanovskiy
Abstract The direct Lyapunov method is used to prove the stability of the exact
Crapper solution for capillary waves. The dynamic equations of the capillary wave
are presented in the form of an inﬁnite Euler-Lagrange chain of equations for the
Stokes coeﬃcients. The stationary solution found for these equations is the Crapper
solution for capillary waves. With the help of energy and momentum conservation
laws the Lyapunov function is constructed. It is shown that the Lyapunov function is
positive deﬁnite with respect to any perturbations of waves surfaces, for waves with
the period multiple of wavelength.
Keywords
Direct Lyapunov method ⋅Capillary waves ⋅Lyapunov function
1
Introduction
In [1] the exact solution of the problem of the potential plane-parallel ﬂow of an
ideal ﬂuid in the domain −∞< x < ∞, −∞< y < 𝜂(kx) was constructed, where
the function 𝜂(kx) is periodic 𝜂(kx) = 𝜂(kx + 2𝜋), the wave number k is related to
the wave length as follows 𝜆= 2𝜋∕k. The Laplace condition p −p0 + 𝜎∕r = 0 is
satisﬁed on the wave surface 𝜂(kx), where r is the curvature radius of the cylinder, p
A. Petrov ⋅M. Lopushanski (✉) ⋅V. Vanovskiy
Moscow Institute of Physics and Technology, Institutskiy per. 9,
141700 Dolgoprudny, Russia
e-mail: masha.alexandra@gmail.com
A. Petrov
e-mail: petrovipmech@gmail.com
V. Vanovskiy
e-mail: vovici@gmail.com
A. Petrov ⋅M. Lopushanski ⋅V. Vanovskiy
Institute for Problems in Mechanics RAS, Pr. Vernadskogo 101-1,
119526 Moscow, Russia
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_7
51

52
A. Petrov et al.
and p0 are the ﬂuid pressure inside and outside the cylinder, 𝜎is the surface tension
coeﬃcient.
The Crapper wave stability was studied in [2]. The inﬁnite chain of linear diﬀeren-
tial equations for perturbations was obtained and the eigenvalues of these equations
were studied. Such a solution requires an excessively large number of computations.
The Lagrange method of generalized coordinates may be proposed as an alternative.
An analogic method was used in [3] to study the stability of the McLeod plane-
parallel ﬂow [4], which models the motion of a drop with surface tension.
The direct Lyapunov method reduces the stability problem to the eﬃcient poten-
tial energy minimum condition. The potential energy minimum condition is used to
prove the stability of the stationary motion of capillary waves in the frames of the
weakened Lyapunov stability deﬁnition [5].
In this work we present the analytic solution for the capillary waves stability prob-
lem (earlier numerical methods were used). We use the second variation of the Lya-
punov function to prove the stability of capillary waves with respect to symmetric
and non-symmetric disturbances.
2
Lyapunov Function
To describe the dynamics of capillary waves, we use the wave parametrization, intro-
duced by Stokes [6, 7]. We seek the conformal mapping of the disc |𝜁| < 1 of the
complex plane 𝜁with a cut on the positive part of abscissa on the domain of one
wave period on the complex plane z = x + iy in the following form
z(𝜁) = 𝜆
2𝜋
[
i ln 𝜁+
∞
∑
n=1
zn𝜁n
]
.
(1)
The circle 𝜁= ei𝛾corresponds to the surface of the wave z = xs + i𝜂. We consider
the real and imaginary parts of the Laurent series coeﬃcients zn = xn + iyn, n =
1, 2, … to be the generalizaed coordinates of the wave qi, i = 1, 2, ….
The kinetic energy of the wave is the quadratic function of generalized velocities
̇x0, ̇qi, i = 1, 2, …, where x0 is the cyclic coordinate that determines the horizontal
movement of the wave, ̇x0—the wave propagation velocity.
The summands in the kinetic energy may be separated into three groups: quadratic
in ̇x0, linear in ̇x0 and independent of ̇x0
Ekin = 1
2M ̇x2
0 + M1 ̇x0 + M2 = (M ̇x0 + M1)2
2M
+ M∗,
M∗= M2 −
M2
1
2M .
(2)
Here M is independent of velocities, M1 and M2 are the linear and quadratic func-
tion of velocities ̇qi. As Ekin is positively deﬁnite, then M∗is also a positively deﬁnite
quadratic form of ̇qi.

Stability of Capillary Waves of Finite Amplitude
53
Suppose that the system of Lagrange equations has a stationary solution, for which
̇x0 = u,
̇qi = 0,
i = 1, 2, … .
In this solution the surface of the wave moves with velocity u, without changing its
form.
For stationary motion M∗= 0 and, thus, the energy value is
(M0u2)
2
+ E0
pot = E0 ,
where E0
pot is the value of potential energy at a stationary point. The function E is a
Lyapunov function if it is positively deﬁnite. As M∗is positively deﬁnite, we consider
only the functional t
U = (M0u)2
2M
+ Epot ,
If the stationary point is the minimum of U, the Lyapunov Theorem implies that the
stationary motion is stable.
Consider the system of coordinates in which the ﬂuid is at rest at inﬁnity. The
kinetic energy of one period of the stationary wave in this system is expressed
through the Stokes coeﬃcients yn as follows [8]
Ekin = 1
2M ̇x2
0,
M = 𝜌𝜆2
2𝜋
S
2,
S =
∞
∑
n=1
n(x2
n + y2
n) .
(3)
The capillary potential energy is proportional to the arc length l of one wave period
Epot = 𝜎l,
where 𝜎is the surface tension coeﬃcient. On the complex plane 𝜁the arc length is
calculated as follows
l = ∮ds = ∮
||||
dz
d𝜁
||||
d𝜁
i𝜁
,
(4)
where the integral of the diﬀerential of the arc length ds is taken along the circle
|𝜁| = 1.
The Stokes coeﬃcients are not very suitable for arc length calculation l. So, an
analytical function, expressed through parameters qi, is introduced and, using the
Residue Theorem we obtain that the Lyapunov function can be expressed in the
dimensionless form as follows
U = 𝜎𝜆
2𝜋
̄U,
̄U =
S2
0
4Sc2 + ̄l,
̄l = l
𝜆= 1 +
∞
∑
k=1
|qn|2 ,
(5)

54
A. Petrov et al.
where ̄U and ̄l is a dimensionless Lyapunov function and the arc length of one wave
period and c is the dimensionless wave velocity.
The assertion that the ﬁrst variation of ̄U equals zero allows us to ﬁnd the para-
meters qn of the wave and its propagation velocity c.
3
Stationary Capillary Waves
The solution of the variational equation 𝛿̄U = 0 may be presented as follows
qi = 2bi ,
(6)
where b is a parameter of a family of solutions. To prove this we consider small
disturbances of coordinates with respect to stationary values
qn = 2bn + 𝜀(𝜉n + i𝜂n),
n = 1, 2, … .
(7)
We substitute them into function ̄l (5) and expand by parameter 𝜀. Then we will
ﬁnd the expansion for the Stokes coeﬃcients xn and yn, for functional S and for
function U:
̄l = 1 +
∞
∑
k=1
((2bn + 𝜀𝜉n)2 + 𝜀2𝜂2
n
) = 1 + 𝜀𝛿̄l + 𝜀2𝛿2̄l ,
(8)
S = S0 + 𝜀𝛿S + 𝜀2𝛿2S ,
̄U = U0 + 𝜀𝛿U + 𝜀2𝛿2U ,
where 𝛿and 𝛿2 denote the ﬁrst and second variation accordingly. Then we calculate
the ﬁrst variation of U, considering (6). Then we obtain the same expressions for x
and y as did Crapper. Therefore, a new deduction method for the known exact solu-
tion for the capillary wave [1] is presented. The values b = b0 = 0.454, a = 2.280
corresponds to the maximum wave development. In Fig. 1 the graphs of waves with
values b ∶= 0.1; 0.3 and maximum wave development b = 0.454 are presented.
Fig. 1
Capillary waves at
diﬀerent values of
parameter b

Stability of Capillary Waves of Finite Amplitude
55
4
Second Variation
The second variation
𝛿2 ̄U = 1
2
d2 ̄U
d𝜀2
|||𝜀=0
is the quadratic form of variations 𝜉i, 𝜂i, i = 1, 2, …. It is expressed through the ﬁrst
and second variations of functionals S and ̄l. The variables 𝜉n and 𝜂n of the second
variation 𝛿2 ̄U may be separated and the second variation 𝛿2 ̄U may be presented as
the sum of two quadratic forms 𝛿2 ̄U = 𝛿2 ̄U1(𝜉) + 𝛿2 ̄U2(𝜂). The ﬁrst 𝛿2 ̄U1(𝜉) depends
only on 𝜉and is expressed through 𝛿2S1 and 𝛿2̄l1, which depend only on 𝜉. The second
is expressed through 𝛿2S2 and 𝛿2̄l2, which depend only on 𝜂.
The ﬁrst quadratic form deﬁnes the stability of the wave with respect to the sym-
metric disturbances 𝜉n, the second one deﬁnes the stability with respect to the asym-
metric disturbances 𝜂n.
Let us ﬁrst consider the quadratic forms of second variations for symmetric dis-
turbances
𝛿2 ̄U1 = c2
4
((𝛿S)2
S0
−𝛿2S1
)
+ 𝛿2̄l1 .
For 𝛿2U1(𝜉) the following inequality holds
𝛿2 ̄U1 > 𝜆min
∞
∑
n=1
(𝜉n)2 ,
(9)
where 𝜆min is the smallest eigenvalue of the quadratic form.
The matrix amn of the quadratic form 𝛿2 ̄U1 as b = 0 is diagonal and its diagonal
elements are a11 = 4, ann = (n −1)∕n, n = 2, 3, …. The eigenvalues that correspond
to the adjoint linear operator are 𝜆n = ann. The smallest eigenvalue is equal to the
second diagonal element 𝜆min = a22 = 1∕2. For eigenvalues the expansion in powers
of b may be obtained. In Fig. 2 the dependences of the ﬁrst ﬁve eigenvalues on b are
presented. The solid lines stand for the results of numeric calculations, the dashed
lines stand for the expansions in powers of b. From the graphs we see that the greater
the index of the eigenvalue is, the better it is approximated by its expansion. In the
second variation N = 20 independent variations 𝛿qi, i = 1, 2, … 20 are taken into
consideration. The smallest eigenvalue 𝜆1(b) decreases monotonously until it reaches
the value 𝜆(b0) = 0.03069 and for N > 15 almost does not depend on N.
Thus, inequality (9) implies that the second variation 𝛿2U > 0 is strictly positive
for all variations 𝛿qi. By the Lyapunov Theorem the stationary motion of capillary
wave is stable for all possible amplitude values.
The eigenvalues determine the main oscillation frequencies near stationary
motion.

56
A. Petrov et al.
Fig. 2
Eigenvalues
(symmetric disturbances)
Consider now the quadratic form of the second variation for non-symmetric dis-
turbances
dU2 𝛿2 ̄U2 = −c2
4 𝛿2S2 + 𝛿2̄l2 .
(10)
The matrix
bmn = 1
2
𝜕2(𝛿2 ̄U2)
𝜕𝜂m𝜕𝜂n
for b = 0 is diagonal and bnn = (n −1)∕n, n = 2, 3, …. The eigenvalues are 𝜆n =
bnn.
The matrix bmn is singular, its determinant equals zero. This is due to the linear
dependence of the generalized 𝜂1, 𝜂2, 𝜂3, …, which is expressed as follows
r = 1
2
∞
∑
k=1
kbk 𝜕(𝛿2U2)
𝜕𝜂k
= 0 .
(11)
Equality (11) is equivalent to the fact that the linear combination of the matrix
columns b⋅n satisﬁes the equality
∞
∑
k=1
kbkbkm = 0
and, thus, the matrix (bmn) is singular.
This can also be explained by the fact that the mapping (1) is multivalued. The
mapping 𝜁′ = ei𝛾0 maps the circle |𝜁| = 1 into itself, therefore the form of the wave
does not change. So, let us put the ﬁrst coordinate 𝜂1 equal to zero. Then all the other
coordinates 𝜂n, n = 2, 3, … are independent.
Such choice of coordinates implies that the matrix (bmn) as b = 0 is diagonal
bmn = (n −1)∕n𝛿mn, and the smallest eigenvalue equals the second diagonal element
𝜆min = b22 = 1∕2. For eigenvalues we may obtain expansion in powers of b. In Fig. 3

Stability of Capillary Waves of Finite Amplitude
57
Fig. 3
Eigenvalues
(non-symmetric
disturbances)
the dependences of the ﬁrst four eigenvalues on parameter b are presented. The solid
lines stand for the numeric calculations, the dashed ones for the expansion in powers
of b. The smallest eigenvalue 𝜆1(b) monotonously decreases until 𝜆(b0) = 0.181408
and for the number of generalized coordinates N > 15 almost does not depend on N.
Acknowledgements The study has been supported by the Russian Science Foundation (project N
14-19-01633) in IPMech RAS.
References
1. Crapper, G.D.: An exact solution for progressive capillary waves of arbitrary amplitudes. J. Fluid
Mech. 2, 532–540 (1957)
2. Tiron, R., Choi, W.: Linear stability of ﬁnite-amplitude capillary waves on water of inﬁnite
depth. J. Fluid Mech. 696, 402–422 (2012)
3. Petrov, A.G.: On the stability of a liquid cylinder in a plane-parallel ﬂow of ideal ﬂuid. J. Appl.
Math. Mech. 3, 366–374 (2016). [in Russian]
4. McLeod, E.B.: The explicit solution of a free boundary problem involving surface tension. J.
Ration. Mech. Anal. 4(4), 557–567 (1955)
5. Lyapunov, A.M.: On the stability of ellipsoidal equilibrium forms of a rotating ﬂuid. Collected
works, vol. 3, pp. 5–113. Izd. Akad. Nauk SSSR, Moscow (1959) [in Russian]
6. Sretenskii, L.N.: Theory of Wave Motion of Fluid. Nauka, Moscow (1977) [in Russian]
7. Petrov, A.G.: Analytical Hydrodynamics. Fizmatlit, Moscow (2009) [in Russian]
8. Maklakov, D.V., Petrov, A.G.: On stokes coeﬃcients and wave resistance. Dokl. RAN. 463(2),
155–159 (2015)

High Temperature Behavior of Two
Titanium Aluminides for Blade Engine
Applications. Preliminary Study
Alexandra Banu, Alexandru Paraschiv, Luminita Georgescu
and Cristina Juganaru
Abstract The developmental tendencies in obtaining high performance of gas
turbines are chieﬂy connected with an increase in the engine’s capacity, its efﬁ-
ciency, lifetime, and reduce the deleterious emissions. One way to achieve these
targets is the usage of lighter and stronger materials and enhancing the method of
parts manufacture. The present work is a part of a national project whose main goal
is to develop a new class of light alloys-coatings structures for aerospace, energy
and the automotive industry, fated to reduce pollution and to contribute to a
friendlier environment. One way to enhancing the high temperature response
against the severe environmental conditions, is by development of titanium alu-
minides with increased niobium content and two different structures: α2 + γ
(Ti2Al + TiAl), and Orto AlNbTi2. The paper present the preliminary results on
mechanical properties modiﬁcation because of niobium content increased from
10 at.% to 25% and on beneﬁcial inﬂuence of Thermal barrier coating.
Keywords Titanium aluminide ⋅Mechanical properties ⋅Tensile test ⋅Creep
test
1
Introduction
Today there are increasing demands for the development of energy conversion
systems with improved efﬁciency and ecological compatibility. In aerospace,
energy and automotive industry, and advanced design concepts are based on
higher service temperatures, lighter weight and higher operation speeds. The
A. Banu (✉) ⋅L. Georgescu ⋅C. Juganaru
Department of Manufacturing Engineering, Politehnica University from Bucharest,
Splaiul Independentei 313, Bucharest, Romania
e-mail: alexandrabanu14@yahoo.com
A. Paraschiv
Comoti, Romanian Research & Developmet Institute for Gas Turbines,
220 D Iuliu Maniu Bd, Bucharest, Romania
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_8
59

developmental tendencies in obtaining high performance of gas turbines are chieﬂy
connected with an increase in the engine’s capacity, its efﬁciency, lifetime, relia-
bility, decrease in the fuel consumption and reduce the deleterious emissions. One
way to achieve these targets is the usage of lighter and stronger materials and
enhancing the method of parts manufacture. The main goal of our work is to
develop a new class of light alloys-coatings structures and new technologies for
aerospace, energy and the automotive industry, fated to reduce pollution and to
contribute to a friendlier environment. In this context, 30 years ago, a new class of
materials begins to develop: the Ti–Al intermetallics alloys. There has been a major
effort aimed at developing TiAl-based alloys because they have a density about one
half that of Ni-based alloys and they retain their strength up to about 750 °C. It has
been argued that if these alloys could be manufactured so that an acceptable balance
could be obtained between their high and low temperature properties they would be
used widely in aerospace engineering (gas-turbine parts), heat power engineering
(gas-turbine units), and car engine (internal combustion engine) parts [1]. The main
aim of this work was to highlight the inﬂuence of niobium content on mechanical
behavior of titanium aluminide at 850 °C. To minimize the temperature effect on
materials properties and improve their behavior during heating service, the samples
were covered with thermal barrier coating formed from two layers. The inner
metallic layer and the outer ceramic layer were obtained by Air Plasma deposition
technique. The main concern is the poor high temperature oxidability followed by
embrittlement of titanium aluminide alloys and as consequence, premature fracture
can occur in service. The present work is going to focus on studying the inﬂuence
of alloying level on the structure and mechanical properties of titanium aluminides.
In this context were studied two materials with two niobium alloying levels with
two different microstructures, α2 + γ, and Orto AlNbTi2. In order to improve the
oxidation behavior, at work temperature during mechanical tests, the samples were
covered with modiﬁed thermal barrier coatings formed from inner bonding layer
(NiCrMoY + Al2O3) and zirconia yttria outer layer.
2
Experimental Detail
The titanium aluminides whose chemical compositions are presented in the Table 1
are used in casting state. The experimental samples were machined by EDM at
speciﬁc shapes and dimension samples for mechanical properties veriﬁcation. As
protective coating was studied a new system composed from a bonding layer type
(NiCr MoAlY) and an outer layer with diffusion and thermal barrier function,
Al2O3. The coatings were obtained from micro sized powders by Air Plasma
Spraying procedure. The structure of base materials and coatings were performed
by SEM + EDX techniques.
High temperatures mechanical properties were evaluated, according the ASTM
E8, by tensile testing at temperature of 850 °C (processed specimens Fig. 2) on
universal static and dynamic testing machine Instron 8802, equipped with high
60
A. Banu et al.

temperature system, for maximum 1000 °C, and extensometer with a measuring
basis of 40 mm. The creep tests were performed on creep testing machine SF50
Wolpert, with a maximum 50 KN force, a force measurement accuracy of ±1% and
endowed with a system of automatic compensation of force. The tests were con-
ducted in accordance with ASTM E 139 at a temperature of 850 °C.
3
Results and Discussions
3.1
Structural Evaluation
The structures of cast studied alloys presented in the Fig. 1 have been analyzed by
SEM technique. The microstructure of alloy 1 is monophasic, with well-formed
grains of α2 phase. No secondary phases were observed in the grain boundaries and
intergranular area. The grains are very ﬁne for a cast material, the grains with sizes
ranging between 10 and 30 μm. The increasing in niobium content (alloy 2) has, as
a structural consequence, a high order level of lamellas arrangement, with trans-
formation of AlTi3 phase in AlNbTi2, a hard phase and relative uniform distribution
of alloying elements in the matrix. The XRD analysis revealed that the alloy has a
monophasic orthorhombic AlNbTi2 structure Fig. 2, and the SEM analysis revealed
a well-ordered slides aspect characterizing the Orto structure.
Table 1 The chemical
compositions of titanium
aluminides (at.%)
Alloy
The structure of alloy
Ti
Al
Nb
Mo
V
1
α2 + γ
52.3
40.2
4.7
–
2.8
2
Ortho AlNbTi2
50
22
25
3
–
Fig. 1 SEM secondary electrons image of alloys a alloy 1(α2 + γ) and b alloy 2 (Orto)
High Temperature Behavior of Two Titanium Aluminides …
61

Knowing that the increasing in the hard alloying elements such molybdenum
and niobium has as a consequence a deterioration of mechanical properties because
of hardening effect, for improving the mechanical properties of alloy 2 were done
the aging thermal treatments at 850 °C for 12 h followed by 990 °C for 2 h in order
to stimulate the formation of B2 phase, see Fig. 3. The samples, in ﬁnal thermal
treatment state, were covered with metallo ceramic double layers using atmospheric
plasma spray technique. The SEM evaluation of coating quality before mechanical
tests showed a continuous and relative uniform layers.
3.2
High Temperature. Mechanical Properties Evaluation
3.2.1
The 850 °C Tensile Test Results
The samples prepared for tensile test are presented in the Fig. 2 (general view for
shape and surface aspect).
In order to highlight the importance of protective coatings on mechanical
properties of titanium aluminides, were performed tensile tests on alloy 1 in two
surface states: with protected layers and no covered.
Comparing the tensile behaviour of low Nb content titanium aluminide (alloy 1)
in these two surface states, can be summarized the following aspects (Fig. 3):
• the shape of deformation curve is similar for both states;
• was observed decreasing of conventional yield strength Rp02 at approx. 70 MPa
and at approx. 30 MPa of tensile strength after applying protective coatings;
Fig. 2 The general view of tensile samples before mechanical test
Fig. 3 The tensile curves at 850 °C for alloy 1 in two states: (a) noncovered sample and
(b) covered with TBC
62
A. Banu et al.

• was observed too increasing the value of elongation from 1.7 to 6.8% (four
times higher) and increasing the ﬂow range at approx. 100 MPa.
On the other hand, can be shown that on the protected sample the slight hard-
ening trend of the material, after material is ﬂowing, remains at comparable levels
to that exhibited by the uncovered sample. So, the protection does not improve the
hardening resistance of alloy 1. The broken specimen aspect is shown in Fig. 4a
and the surface appearance of rupture observed microscopically (SEM) is shown in
Fig. 4b. Because of beneﬁc inﬂuence of thermal barrier coatings the alloy 2 samples
have been tested only in protected state.
The 850 °C tensile test results—Table 2 show that although the alloy 2 show the
more coarse appearance of surface breaking, indicating a fragile intergranular
cracking and the breaking elongation at a lower temperature, is superior of alloy 1.
That can be explained by the important volume fraction of B2 phase in
microstructure.
3.2.2
The 850 °C Creep Resistance Results
The creep resistance test of covered samples from both titanium aluminide was
performed at 850 °C with a load of 180 MPa. Both materials have a similar
behavior with speciﬁc three stages of deformation. Both materials revealed a fragile
behavior. The Extensometer used was one dedicated testing at high temperatures,
with the base measuring 40 mm and an accuracy of ±1% (class B1 acc. To ASTM
E83). Fixturing of specimens were machined from plastic deformed bar of nickel
based superalloy. The value of efforts maintained during the tests was chosen from
Table 2 The tensile test parameters values, for studied titanium alloys with TBC at 850°C
Alloy
Rm, MPa
Rp02, MPa
A5, %
1
367.1
302.5
1.7
2
393.7
–
2.5
Fig. 4 The aspects after tensile test of alloy 1: (a) The tensile broken sample and (b) the SEM
aspect of surface of rupture (fragile, intergranular fracture)
High Temperature Behavior of Two Titanium Aluminides …
63

the results of tensile tests at elevated temperatures presented above. The creep
behaviour of α2 + γ TiAl alloys (1) at 850 °C and 180 MPa (severe conditions)
kept in good condition coverage, presented as a ﬁrst creep stage in Fig. 5, even in
the outer areas intensely deformed. In these areas, examined microscopically,
micro-cracks were observed in the outer layer of alumina but the adhesion of the
coating, in its entirety, was good. This will be followed by long term test of the
experimental model.
The scanning electron microscopy of breaking surface revealed the presence of
voids speciﬁc for high temperatures break, but also the character of the intergran-
ular fracture (Fig. 6). Microscopic examination of broken specimen in the cross and
longitudinal section showed the following: We’re not observed phenomenon of
recrystallization of grain, not even close to the breaking surface, where deformation
Fig. 5 The ﬁrst creep stage of alloy 1. Effort: 180 MPa; 850 °C temperature
Fig. 6 The SEM aspect of
creep surface After creep test
(alloy 1)
64
A. Banu et al.

is more pronounced; on the longitudinal sample, was observed, however, a rear-
rangement of the grains, with the formation of preferential orientation strip (see
Fig. 7). The breaking was initiated by the appearance of intergranular cracks seen
on the longitudinal sample. The coatings behaved well, proven adherent and
compact, except in areas where deformation of the specimen is relatively high. It
was observed on a cross section, a certain amount of diffusion of nickel and cobalt
from the bonding alloy layer in the support, to form compounds rich in nickel and
niobium Fig. 8 selected area 1. The TBC layer is compact and adherent. The high
Fig. 7 The SEM longitudinal
aspect of creep samples alloy
1 (near creep surface) with
rearrangement of the grains,
the formation of preferential
orientation strip and micro
intergranular cracks
Fig. 8 The SEM image in
transversal section on coated
creep sample. (X 2500)
High Temperature Behavior of Two Titanium Aluminides …
65

temperatures behavior of alloy (2) is determined, outside by the chemical compo-
sition or material state (molded, plastic deformation, etc.), by the mechanisms by
which heat treatments favors structural changes that inﬂuence mechanical proper-
ties. Boehlert [2] showed, citing several authors, that the alloys with predominant
orthorhombic structure are generally fragile and is therefore required such heat
treatment to maintain an appropriate level of phase volume fraction of the bcc for
improving ductility.
Smith and Porter [3] have investigated the inﬂuence of heat treatment on a
titanium orthorhombic alloy Ti-22Al-23Nb and reveal that all heat treatments to
temperatures below-transus led improve tensile strength, yield strength, elongation
and creep resistance. It has also mentioned, as was observed after application of the
coating diffusion barrier, an increase of plasticity of alloy but at this stage we cannot
say that this is due to stopping uptake of oxygen and brittle superﬁcial or structural
changes induced by diffusion of elements Fig. 9.
4
Conclusion
The increasing of niobium content of titanium aluminides for high temperature
oxidation increasing reasons has a consequence the deterioration of mechanical
properties in terms of elongation and creep resistance. The mechanical properties of
alloys with more than 10 at.% of niobium cannot be improved signiﬁcantly by heat
treatments. However, the thermal barrier coatings improve mechanical behavior of
these alloys. The next studies will be going to focus on development of a new
thermal barrier coating in order to improve the both oxidation resistance and
mechanical properties of niobium titanium aluminides.
Acknowledgements This work was funded by National Project PCCA no. 65/2012.
Fig. 9 The ﬁrst creep stage
of alloy 2 curves in two state
of samples: with and without
TBC
66
A. Banu et al.

References
1. Belyanchikov, L.N.: Special electrometallurgy of promising structural materials based on
titanium aluminides. Russ. Metall. (Metally) 2008(7), 567–576. © Pleiades Publishing, Ltd.
(2008)
2. Boehlert, C.J.: Microstructure, creep, and tensile behavior of a Ti–12Al–38Nb (at.%)
beta + orthorhombic alloy. Mat. Sci. Eng. A267, 82–98 (1999)
3. Smith, P.R., Porter, W.J.: The effect of heat treatment on the tensile and creep behaviour of
‘‘neat’’ matrix Ti-22Al-23Nb. J. Mater. Sci. 32, 6215–6220 (1997)
High Temperature Behavior of Two Titanium Aluminides …
67

The Numerical Scheme for the Basset
Type Integro-Diﬀerential Equation
in Hydrodynamics
Vladimir Vanovskiy and Alexander Petrov
Abstract A high-order precision numerical scheme was proposed for the Basset
kernel integrals and the Basset type integro-diﬀerential equations that often arise in
hydrodynamics. The scheme was tested on a model integral and on a real problem of
particle focusing in a standing acoustic wave in liquid. The scheme showed around 2
orders higher speed of integral estimation compared with the existing analogs. The
obtained results show that the integral can be approximated with high order of preci-
sion and the real problem is simulated well by the proposed scheme. A variable step
technique was used to increase the precision of integro-diﬀerential equation simula-
tion even more by eliminating the discrepancy at the very start of the simulation. The
proposed numerical scheme may found its applications in many biological and med-
ical problems related to acoustophoresis or in particle sedimentation simulations.
Keywords
Basset force⋅Basset kernel⋅Fractional derivative⋅Integro-diﬀerential
equation ⋅Acoustophoresis ⋅BBO equation
1
Introduction
The numerical simulation of the integro-diﬀerential equations describing “history”
forces or physical systems with memory is complicated because at any step of the
numerical integration all the previous steps should be considered and the time of
execution may become quadratic in the number of simulation steps. In case of the
integrals with the singular kernels the situation may become even worse as the usual
numerical integration schemes of the Newton-Cotes family are useless.
V. Vanovskiy (✉) ⋅A. Petrov
Moscow Institute of Physics and Technology, Institutskiy per. 9,
141700 Dolgoprudny, Russia
e-mail: vladimir.vanovsky@gmail.com
V. Vanovskiy ⋅A. Petrov
Institute for Problems in Mechanics of the Russian Academy of Sciences,
Pr. Vernadskogo 101-1, 119526 Moscow, Russia
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_9
69

70
V. Vanovskiy and A. Petrov
The example of such an integral is the Basset kernel integral described by the
integral operator
̂I(f) = ∫
t
𝜏=0
f(𝜏)d𝜏
√
𝜋(t −𝜏)
.
(1)
The kernel is singular at 𝜏→t and its fast computation is a big challenge in all the
problems where such a kernel appears such as Basset force consideration in hydro-
dynamics or fractional derivative calculation. Many approaches of such an integral
approximation were developed in the last years. A comprehensive review that con-
tains most of them was recently published [1]. We try to focus on the exact integral
estimation without changing the kernel as it was done in [2]. However the proposed
here approach is much simpler and the integral is calculated more than 100 times
quicker. Also the proposed approach gives the possibility for the exact estimation of
the obtained error of integration for the functions with limited corresponding order
derivative value.
2
The Numerical Scheme
The integrand function in (1) is assumed to be k −1 times continuously diﬀerentiable
over the closed interval [0, t] and to have the kth derivative in the opened interval
(0, t) limited by some constant. The integral is assumed to be taken over the uniform
mesh containing n points with step h, t = nh. It will be shown that under such an
assumptions there exists a numerical scheme providing the integration error limited
by Mhn√
t where M stays for some constant. The numerical scheme coeﬃcients will
be calculated for k = 3.
The integral may be rewritten as follows
I = ̂I(f) = −2
√
h
𝜋
∑N
i=1 ∫i
i−1 f(hx)d
√
n −x = 2
√
h
𝜋
∑n
i=1 Ii,
Ii = −∫i
i−1 f(hx)d
√
n −x .
(2)
The function f on the interval ((i −1)h, ih) may be represented by the Taylor poly-
nomial with the remainder in the Lagrange form
f(𝜏) = fi−1 + f ′
i−1(𝜏−ti−1) + f ′′
i−1(𝜏−ti−1)2∕2 + ⋯
⋯+ f (k−1)
i−1 (𝜏−ti−1)k−1∕(k −1)! + f (k)(𝜉)(𝜏−ti−1)k∕k!,
(i −1)h ≤𝜉≤𝜏≤ih,
ti−1 = (i −1)h,
fi = f(ti) = f(ih) .
(3)
One may substitute (3) in (2) up to the needed order of precision and after inte-
grating and approximating the result by the function values in the grid points the
scheme coeﬃcients will be obtained. The following integral is obtained for k = 3

The Numerical Scheme for the Basset Type Integro-Diﬀerential ...
71
Ii = −∫i
i−1
{fi−1 + hf ′
i−1(x −i + 1) + (h2∕2)f ′′
i−1(x −i + 1)2+
+(h3∕6)f ′′′(𝜉)(x −i + 1)3} d
√
n −x,
𝜉∈((i −1)h, ih)
(4)
and approximated by the weighted sum of function values in the points (i −2)h,
(i −1)h, ih
̃Ii ≈
(√
n −i + 1 −
√
n −i
) (
𝛼ifi−2 + 𝛽ifi−1 + γ
i fi
)
.
(5)
Three points are used for approximation because it is the minimal amount that can
provide the diﬀerence between the exact value of the integral and the approximation
value to be smaller or proportional to h3. After calculating the integral and making
an important substitution
𝜀i =
(√
n −i + 1 −
√
n −i
)2
,
the following expansion in powers of h is obtained
Ii∕√𝜀i = fi−1 + hf ′
i−1
(
1
2 + 𝜀i
6
)
+ h2
2 f ′′
i−1
(
1
3 + 𝜀i
6 +
𝜀2
i
30
)
+
+ h3
6 f ′′′(𝜉)
(
1
4 + 3𝜀i
20 +
𝜀2
i
20 +
𝜀3
i
140
)
,
𝜉∈((i −1)h, ih) .
(6)
After the expansion of (5) in a Taylor series and equating it with (6) up to the
second power of h the following linear system of equations together with the error
estimation is obtained
𝛼i + 𝛽i + γi = 1
γi −𝛼i = 1∕2 + 𝜀i∕6
𝛼i + γi = 1∕3 + 𝜀i∕6 + 𝜀2
i ∕30
Ii
√𝜀i + h3
6 f ′′′(𝜉)
(
1
4 + O( 1
n−i)
)
=
=
̃Ii
√𝜀i + h3
6
(γi f ′′′(𝜉′) −𝛼if ′′′(𝜉′′))
𝜉, 𝜉′ ∈((i −1)h, ih)
𝜉′′ ∈((i −2)h, (i −1)h)
⇔
𝛼i = −1∕12 + 𝜀2
i ∕60
𝛽i = 2∕3 −𝜀i∕6 −𝜀2
i ∕30
γi = 5∕12 + 𝜀i∕6 + 𝜀2
i ∕60
|Ii−̃Ii|
√𝜀i < h3
6 f ′′′
i max
(
11
12 + O( 1
n−i)
)
f ′′′
i max = max(f ′′′(𝜉)),
𝜉∈((i −2)h, ih) .
(7)
The proposed scheme estimates the integral Ii using function values in three grid
points (i −2)h, (i −1)h and ih. For i = 1 it means that the integrand should also
possess all the good properties on the interval (−h, 0). Speaking about higher degrees
of approximation this interval will be extended even more. A selection of asymmetric
interval ((i −k + 1)h, ih) for every Ii estimation is recommended because the use of
the grid points bigger than ih for Ii estimation leads to the implicit numerical schemes
for the problems where such integrals enter some integro-diﬀerential equation and
increases vastly the overall simulation time.

72
V. Vanovskiy and A. Petrov
Using (7) the whole integral (2) value and the approximation error can be
estimated
I ≈̃I = 2
√
h
𝜋
∑n
i=1 ̃Ii = 2
√
h
𝜋
∑n
i=1
√𝜀i
(𝛼ifi−2 + 𝛽ifi−1 + γi fi
) ,
err = |I −̃I| <
11
36
√
𝜋h3√
tf ′′′
max
(
1 + O( 1
√
n)
)
, n →∞.
(8)
The same way the numerical scheme may be obtained for every k value but for
high k values it becomes less stable. Following [2] we will say that the scheme has
kth order of precision as the error is majorized by O(hk√
t) (strictly speaking the
order of precision is k −1—the highest order of polynomial which will be integrated
precisely). The scheme may be summarized as
I ≈2
√
h
𝜋
∑n
i=1
√𝜀i
((−1∕12 + 𝜀2
i ∕60)fi−2+
+ (2∕3 −𝜀i∕6 −𝜀2
i ∕30)fi−1 + (5∕12 + 𝜀i∕6 + 𝜀2
i ∕60)fi
) .
(9)
The numerical schemes of the orders 1–3 are evaluated on a model function
f(𝜏) = cos 𝜏integrated over interval [0, 50𝜋]. The answer can be expressed analyti-
cally in terms of Fresnel integral as
√
2C(10) ≈0.70696351315804088. The func-
tion and interval were chosen the same as in [1] in order to compare the obtained
results. All the calculations were made in Matlab environment with a modern i7
CPU. Matlab is considered to be far from the fastest computing environment and
some speed increase from 3 to 5 times may appear if the simulations were done
using Fortran as in [1].
However in (Fig. 1) one may observe that the precision of the schemes corre-
sponds to declared and calculation speed is more than two orders higher than of
the third order Daitche scheme and much better than any other scheme investigated
(a)
(b)
Fig. 1
The time and error values versus step size plotted for the diﬀerent order of accuracy meth-
ods. Dotted, dashed and solid lines stay for the 1st–3rd order numerical schemes correspondingly

The Numerical Scheme for the Basset Type Integro-Diﬀerential ...
73
in [1]. Furthermore, the coeﬃcients in the scheme depend only on n −i and may
be calculated in advance once for all the n in order to boost the simulation of the
integro-diﬀerential equation when the integral is calculated on every step.
3
The Basset Type Integro-Diﬀerential Equation Solution
The Basset-Boussinesq-Oseen equation (BBO equation) is used to describe the
motion of a small spherical particle in unsteady ﬂow with velocity ﬁeld v(x, t) at
low Reynolds numbers
(mparticle + madded)̈x = Finertial + FStokes + FBasset + FFaxen + Fg,
mparticle = 4
3𝜋𝜌pa3,
madded = 2
3𝜋𝜌a3,
Finertial = 4
3𝜋𝜌a3 (
w + W
2
)
,
w = 𝜕v
𝜕t + v 𝜕v
𝜕x,
W = 𝜕v
𝜕t + ̇x 𝜕v
𝜕x,
FStokes = −6𝜋𝜇a( ̇x −v),
FBasset = −6𝜋𝜇at1∕2
𝜇
(
d
dt
)1∕2
( ̇x −v),
t𝜇= 𝜌a2
𝜇,
(
d
dt
)1∕2
( ̇x −v) = ∫t
t′=0
(
d2x
dt′2 −W(t′)
)
dt′
√
𝜋(t−t′) + ̇x(0)−v(0)
√
𝜋t
,
FFaxen ∼𝜇a3∇2v ∼𝜇av(a∕𝜆)2,
Fg = 4
3𝜋a3(𝜌p −𝜌)g .
(10)
Here 𝜌p and 𝜌stand for particle and ﬂuid densities, a stands for particle radius, 𝜇
stands for ﬂuid viscosity which we consider not to depend on density. By w and
W are denoted ﬂuid acceleration along ﬂuid and along the particle trajectory. The
Basset force is used in the form proposed by [3] which contains the corrections for
nonzero initial particle relative velocity with the ﬂuid. The inertial force is used in
the Maxey-Riley form [4].
The last two forces are neglected because of their smallness and (10) is divided
by (2∕3)𝜋a3
(𝜌+ 2𝜌p)̈x = 2𝜌w + 𝜌W −9𝜇
a2
(
dx
dt −v
)
+ FB,
FB = −
9√𝜌𝜇
a
(
∫t
t′=0
(
d2x
dt′2 −W(t′)
)
dt′
√
𝜋(t−t′) + ̇x(0)−v(0)
√
𝜋t
)
.
(11)
The dimensionless acoustic wave amplitude is denoted by b ≪1. It is deﬁned as
the ratio of maximal ﬂuid velocity in the wave to the sound speed in the ﬂuid. The
dimensionless variables q = (𝜔∕c)x, 𝜏= 𝜔t and the dimensionless viscosity para-
meter K =
3𝜇
𝜔𝜌a2b ∼1 are introduced, 𝜔stands for the wave angular frequency. The
asymptotic averaged equation for b →0 is obtained using the Krylov-Bogoliubov
averaging technique as described in [5].
The initial and the averaged equations were modeled using a proposed numerical
scheme (9) for the Basset integral calculation. As the integrand contains second order

74
V. Vanovskiy and A. Petrov
Fig. 2
The asymptotic and the exact numerical solutions of the integro-diﬀerential BBO equation
(light solid line and dashed line) and the numerical solution of the same equation without the Basset
force (dotted line). The inset shows the magniﬁed part of the dependence. 𝜏1 = 3b𝜏∕4 stands for
the “slow” dimensionless time
derivatives the 5-point grid was used in order to estimate the integrand with the
needed precision. As the initial conditions are not deﬁned very well a step variation
technique was used [2]. The ﬁrst 400 steps were calculated with a small step h∕100
and than the numerical scheme step size was reset to h.
In the (Fig. 2) the trajectories of heavy particle 𝜌p∕𝜌= 1.5 are modeled for
b = 16∕3 ⋅10−4. The trajectory for small b practically coincides with the asymp-
totic solution at b →0. The initial equation simulation took about 8 h of one core
CPU time and the averaged integro-diﬀerential equation simulation took only 170 ms
using the same numerical scheme. The ability to compare the results for these two
approaches up to such a big time (𝜏max = 60000, h = 0.05, 1.2M steps) shows the
advantage of our scheme over the one used in [5] where only the small part of the
particle trajectory was calculated.
4
Conclusion
The proposed numerical scheme for the Basset integral approximation proves to be
much simpler and faster than the analogs. It also allows some optimizations such
as the precomputed coeﬃcients for the integro-diﬀerential equation solution and the
smaller step size in the beginning of the grid. The work of the scheme with the 3rd
order of the integral approximation was demonstrated on the example of the model
integral and in a model problem of particle focusing in the standing acoustic wave.
The proposed scheme allows the solution to be extended to much bigger interval than
in the previous works on this problem up to the moment of the particle focusing in
a standing wave pressure node.
Acknowledgements The work was supported by the Russian Science Foundation (project 14-19-
01633) in IPMech RAS.

The Numerical Scheme for the Basset Type Integro-Diﬀerential ...
75
References
1. Moreno-Casas, P.A., Bombardelli, F.A.: Computation of the Basset force: recent advances and
environmental ﬂow applications. Environ. Fluid Mech. 16(1), 193–208 (2016)
2. Daitche, A.: Advection of inertial particles in the presence of the history force: Higher order
numerical schemes. J. Comput. Phys. 254, 93–106 (2013)
3. Michaelides, E.E.: A novel way of computing the Basset term in unsteady multiphase ﬂow com-
putations. Phys. Fluids A Fluid Dyn. 4(7), 1579–1582 (1992)
4. Maxey, M.R., Riley, J.J.: Equation of motion for a small rigid sphere in a nonuniform ﬂow. Phys.
Fluids 26(4), 883–889 (1983)
5. Aksenov, A.V., Petrov, A.G., Shunderyuk, M.M.: Motion of solid particles in ﬂuid in a nonlinear
standing ultrasonic wave. Dokl. Phys. 439(1), 37–41 (2011)

On the Issue of Choosing the Measuring
Zones in a Faraday Balance When
Studying Magnetic Susceptibility of Small
Samples
Alexander Sandulyak, Anna Sandulyak, Maria Polismakova,
Vera Ershova, Darya Sandulyak and Dmitriy Kiselev
Abstract By the example of polar pieces of spherical, conical and truncated
conical shape, we substantiate and implement the approach to identifying a local,
spatially limited, working zone (to position and examine magnetic susceptibility of
‘microsamples’) for the Faraday balance, viz. the zone with stable inhomogeneity of
the ﬁeld. It involves obligatory obtaining and analyzing coordinate characteristics
of induction (intensity) in the interpolar area, at this, sinuous characteristics, i.e. the
ones with an inﬂexion, should be give preference to out of the traditional nonlinear
characteristics. Then, the well-linearized zone near the inﬂexion point (the corre-
sponding derivative here demonstrates an extremum) most fully satisﬁes the
requirement (to a working zone) for preserving the ﬁeld inhomogeneity constancy.
It is established that in comparison with the spherical polar pieces, the conical ones
provide a working zone that is closer to the axial line, whereas the truncated conical
polar pieces ensure the zone farther from their axial line.
Keywords Faraday balance ⋅Measurement working zone ⋅Coordinate char-
acteristic of intensity (induction) ⋅Sinuosity characteristic ⋅Gradient extremum
1
Introduction the Framework of the Approach
A Faraday balance is usually used to obtain information on magnetic susceptibility
χ for samples of low volume V, both solid and dispersed ones (e.g. samples of ferro-
or ferrimagnetic powders which represent disperse phase of many technological
media subjected to magnetophoresis and magnetic control [1–6]). When we create a
ﬁeld with intensity H (induction B = μ0H) and inhomogeneity gradH (gradB)
between the polar pieces, the magnetic (ponderomotive) force F affecting the
studied ‘microsample’ becomes the main measured quantity. Then if we employ the
A. Sandulyak ⋅A. Sandulyak (✉) ⋅M. Polismakova ⋅V. Ershova ⋅D. Sandulyak ⋅
D. Kiselev
Moscow Technological University, Stromynka 20, 107076 Moscow, Russian Federation
e-mail: anna.sandulyak@mail.ru
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_10
77

known functional expression F = μ0 ⋅χ ⋅V ⋅HgradH = χ ⋅V ⋅BgradB/μ0 (where
μ0 = 4π ⋅10–7 H/m is a magnetic constant) with some values of the constituent
variables, it becomes possible to deﬁne values χ by back calculating (for ferro- and
ferrimagnetics—the values χ of its nonlinear ﬁeld dependence).
The key requirements to implementing the Faraday ponderomotive method
usually include the demand for rigorous positioning of the sample studied, i.e. in the
zone of interpolar area (working zone) where the constant ﬁeld inhomogeneity
condition is observed. However, the issue of ensuring the zone with stable ﬁeld
inhomogeneity (the choice of polar pieces shape) and the diagnostics of this zone is
sadly not given due consideration. The features necessary to deﬁne these conditions
are virtually absent; the available recommendations are quite vague and hardly
suitable for speciﬁc practical application.
To give an unbiased estimation on the acceptability of the polar pieces choice
and to have the data on the inhomogeneity (gradient) characteristic of some
direction x, permeating the interpolar area, e.g. in the line of ponderomotive force,
we must initially have the coordinate characteristic, i.e. the dependence of
parameter H or B on x. Such a characteristic, speciﬁcally, parameter B, obtained e.g.
by small stepwise movement of the Hall sensor represented in graphic and/or
analytical form, allows us to ﬁnd the coordinate characteristic of the gradient, i.e.
parameter gradB = dB/dx. Judging by it, we can deﬁne, ﬁrstly, whether there is (or
is not) the zone in question in some given interpolar area, i.e. the zone with stable
values gradB, and secondly, we can estimate the coordinates of its location, which
means, the coordinates of positioning the studied sample.
Thus, it is sufﬁce to pose a question on identifying a linear (or near-linear)
segment, even a short one (commensurable with the sample dimensions), of
dependence B on x. The presence of such a segment will mean that this is the place
where gradB = dB/dx = Const, and its coordinates will deﬁnitely point at the
location of the working zone.
Here we must specially bespeak that for the polar pieces of traditional and
speciﬁc shapes dependences B (or H) on x in general are usually objectively
non-linear, and this somewhat contradicts to the whole concept of detecting their
linear segment. However, this inconsistency can be eliminated by an approach
which essentially stipulates the choice of polar pieces of the shape (not obligatory
something special) that would ensure the key characteristic B (or H) is sinuous, i.e.
it has a bend (inﬂexion). Then its segment (short in relation to all expanded non-
linear characteristic) near the inﬂexion point can be considered close to the desired
linear one, and value gradB (gradH) almost stable. This argument can be addi-
tionally (and quite illustratively) supported by the corresponding derivative of the
obtained dependence B (or H) on x, i.e. a coordinate characteristic gradB (gradH)
which will have an extremum (conforming to the deﬁned inﬂexion) in the vicinity
of which values gradB (gradH) are practically stable.
The research into magnetization (by the ﬁeld with intensity H0) of the ball chain
[7, 8] also points to the acceptable (for a Faraday balance) shape of the polar pieces.
It is these nonlinear characteristics with a noticeable inﬂexion for the ﬁeld with
78
A. Sandulyak et al.

intensity H (H ≫H0), that are inherent to the gap areas between opposing
spherical surfaces in the normal direction from centre-to-centre line of balls. In the
inﬂexion point of characteristic H, there is a pronounced extremum of characteristic
dH/dx [7, 8]. It means, that from the perspective of creating the required working
zone, the use of spherical polar pieces (Fig. 1a) is one of the satisfactory solutions
for choosing the polar pieces for a Faraday balance (clearly, as non-contacting
semi-spherical bodies).
2
Research Outcomes and Their Discussion
Figure 2 (points) exhibits the results of measuring induction B in the symmetry
plane of the area between polar spherical pieces (diameter is 100 mm, mutual
distancing is 10 mm) located at different x distances from the axial line (coil feeding
current is 16A). The inﬂexion of the obtained coordinate characteristic B is clearly
observed, which tells about a possible linear approximation of the data in the
vicinity of the inﬂexion point (shown by dashed lines), thus proving the stability of
value gradB here.
Fig. 1 Options of creating local (located near the abscissa of the inﬂexion point of the coordinate
characteristic of intensity or induction and, respectively, the abscissa of the gradient extremum)
working zones between spherical a, conical b and truncated conical c poles; the working zones are
conditionally highlighted according to results in Figs. 2, 3, 4, and 5
Fig. 2 Coordinate
characteristic of the ﬁeld
induction between polar
pieces of spherical shape;
dots—experiment, lines—
calculation
On the Issue of Choosing the Measuring Zones in a Faraday …
79

The extreme view of characteristic gradB in Fig. 3 illustratively proves the same:
near the extremum value gradB is actually virtually stable. To obtain characteristic
gradB, we should resort to analytical (phenomenological) approximation of char-
acteristic B (Fig. 2, dots) with its further differentiation. It is reasonable in this
context to use, for example, an approximating function in the form of a polynomial
of the 4th, or 5th degree at the most (in Fig. 2 this is practically one uniﬁed line
which describes the original feed experimental data well).
Characteristics gradB = dB/dx obtained for 5th and 4th degree polynoms are
pronounced extreme ones and correlate between each other (Fig. 3 the dashed and
solid lines respectively). Their mutual separation is noticeable only at relatively
high and low values x, here even with the sign reversal (Fig. 3). However, taking
into the account that these data are of no interest for solving the given task, the
preference in this case should be given to the 4th degree polynom, even more so
that it provides an opportunity to get a formula for calculating the extremum
abscissa x = xextr of characteristic gradB in the analytical form. We should only
make double differentiation of this polynom, zero out and solve the deduced
quadratic equation.
The results of calculating abscises xextr conform exactly to the results which are
provided by software (to name a few: Advanced Grapher, Excel). At that, the
obtained value xextr = 14.8 mm is close to values xextr = 14.1 mm when using the
5th degree polynom.
As to the size of the working zone itself (the area between the given spherical
pieces), i.e. the zone of practically stable value gradB it ranges (by x) from 12 to
17 mm, the corresponding interval of x in Figs. 2 and 3 is specially highlighted. At
that, the proper size of this zone, i.e. the size of the sample studied, should not
exceed here 5 mm.
The same approach to identifying the working zone and based on deriving and
analyzing a coordinate characteristic of induction B (intensity H) of the ﬁeld in
interpolar area is applicable to other shapes of polar pieces, including traditional
Fig. 3 Coordinate
characteristic of induction
gradient (by data in Fig. 2);
dashed and solid lines—
calculation by expressions
obtained by polynomial
differentiation according to
the 5th and 4th degree for
coordinate characteristic of
induction
80
A. Sandulyak et al.

ones. Figures 4 and 5 feature coordinate nonlinear characteristics B in the area
between same-size and mutually distanced polar pieces but of conical shape
(Fig. 1b, the vertex angle of 900) and truncated conical shape (Fig. 1c, diameters
100/40 mm).
We can clearly see in Figs. 4 and 5 that each of these characteristics is sinuous
which allows us to speak about the applicability (as a matter of principle) of these
pieces for a Faraday balance. Near the inﬂexion point the corresponding segment of
characteristic B yields to artiﬁcial linearization (it is illustrated by straight dashed
lines in Figs. 4 and 5), which indicates practically stable values of parameter
gradB here (in other words—near the extremum of the corresponding characteristic
gradB).
Fig. 4 Coordinate characteristic of ﬁeld induction between polar pieces of conical shape (Fig. 1b)
Fig. 5 Coordinate characteristic of ﬁeld induction between polar pieces of truncated conical shape
(Fig. 1c)
On the Issue of Choosing the Measuring Zones in a Faraday …
81

It is worth mentioning that in comparison with the spherical pieces (Figs. 1a, 2
and 3), the location of the working zone between the conical (Fig. 1b) and truncated
conical ones (Fig. 1c) differs greatly. Thus, in these cases (see Figs. 4 and 5,
individual intervals of x are shadowed) the working zones are limited by x values,
i.e. by the distances to the poles axial lines from 2 to 6 mm respectively (i.e. closer
to the axial line) and from 23 to 28 mm (i.e. farther from the axial line).
3
Conclusion
The paper considers the issue covered rather poorly in modern literature, it concerns
identifying a working zone with spatial constraints, zones in the inter-polar area of
Faraday balance—the zones for positioning the small-sized samples studied when
examining their magnetic susceptibility. We observed that such identiﬁcation can
be productive when the specially obtained coordinate characteristic of induction
(intensity) of the ﬁeld between the polar pieces (it is usually nonlinear and defying
the desired, even partial, linear approximation—in order to ascertain the induction
gradient values that are stable here, which corresponds to the accepted condition of
the sample positioning) is sinuous, i.e. it has an inﬂexion. In this case, there is an
opportunity to objectively perform linear approximation of a relatively short seg-
ment of this characteristic, i.e. in the vicinity of the inﬂexion point, with additional
illustration of the corresponding extreme form of the induction gradient charac-
teristic, near the extremum of which the values of the gradient are relatively stable.
The research is conducted on deﬁning the location of the working zones between
the polar pieces of spherical, conical and truncated conical shape (diameter of
100 mm, mutual distancing of 10 mm). It has been established that the working
zones in them are localized within the following ranges: from 12 to 17 mm, from 2
to 6 mm, and from 23 to 28 mm respectively.
Acknowledgements The reported study was funded by RFBR according to the research projects
№16-38-60034 mol_a_dk, №16-58-10049 КO_a.
References
1. Nandy, K., Chaudhuri, S., Ganguly, R., Puri, I.K.: Analytical model for the magnetophoretic
capture of magnetic microspheres in microﬂuidic devices. J. Magn. Magn. Mater. 320, 1398–
1405 (2008)
2. Kawano, M., Watarai H.: Two-dimensional ﬂow magnetophoresis of microparticles. Anal.
Bioanal. Chem. 403(9), 2645–2653 (2012)
3. Murariu, V., Svoboda, J.: The applicability of Davis tube tests to ore separation by drum
magnetic separators. Phys. Sep. Sci. Eng. 12(1), 1–11 (2003)
4. Das, S., Chakraborty, S., Mitra, S.K.: Magnetohydrodynamics in narrow ﬂuidic channels in
presence of spatially non-uniform magnetic ﬁelds: framework for combined magnetohydro-
dynamic and magnetophoretic particle transport. Microﬂuid. Nanoﬂuid. 13(5), 799–807 (2012)
82
A. Sandulyak et al.

5. Sandulyak, A.A., Sandulyak, A.V., Belgacem, F.B.M., Kiselev, D.O.: Special solutions for
magnetic separation problems using force and energy conditions for ferro-particles capture.
J. Magn. Magn. Mater. 401, 902–905 (2016)
6. Sandulyak, A.V., Garaschenko, V.I., Korkhov, O.J.: Method of determining the quantity of
solid fraction of ferromagnetic matter in a ﬂuid. Patent USA No 4492921
7. Sandulyak, A.V., Sandulyak, A.A., Ershova, V.A.: Magnetization curve of a granulated
medium in terms of the channel-by-channel magnetization model (New approach). Dokl. Phys.
52(4), 179 (2007)
8. Sandulyak, A.V., Sandulyak, A.A., Ershova, V.A.: On the model of channel-by-channel
magnetization of a granular medium (with a radial permeability proﬁle of a quasi-continuous
channel). Techn. Phys. 54(5), 743 (2009)
On the Issue of Choosing the Measuring Zones in a Faraday …
83

Part II
System Science and Computers

Energy Aware Autonomous Deployment
for Mobile Wireless Sensor Networks:
Cellular Automata Approach
Shahinaz M. Al-Tabbakh and Eman Shaaban
Abstract Wireless sensor networks are formed of thousands of nodes that need to
be deployed to maintain good sensing coverage area. Deployment of a wireless
sensor network in a hostile or hazardous environment is a challenging problem. To
address this problem, a bundle of sensors to be deployed is initially placed in a
deﬁnite region within the area of the network. Then the sensor nodes are diffused
cooperatively and gradually to increase coverage area. This paper proposes a
self-deployment heuristic approach for WSN based on the Four Quadrants
Deployment Model (FQDM). It studies the effect of applying a realistic energy
model to FQDM, and proposes an extended version of FQDM to enhance the
coverage at border areas. The proposed approach relocates the sensors in a way that
maximizes the coverage while minimizing energy consumption. Results show that
it gives a good coverage for deﬁnite region while conserving energy of sensors.
Keywords Sensor networks ⋅IOT ⋅Sensor deployment ⋅Sensor coverage ⋅
Cellular automata
1
Introduction
Wireless Sensor Network (WSN) is a promising technology as it is the basis of
future “Internet Of Things”. In WSNs, the sensors are capable of communicating
with each other and in a multi-hop way propagating sensing information to data
sinks and operations centers. In the last few years, wireless sensor networking has
S.M. Al-Tabbakh
Faculty of Girls for Arts, Sciences and Education, Ain Shams University,
Cairo, Egypt
e-mail: Shahinaz.altabbakh@women.asu.edu.eg
E. Shaaban (✉)
Faculty of Computer and Information Science, Ain Shams University,
Cairo, Egypt
e-mail: eman.shaaban@cis.asu.edu.eg
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_11
87

been a very active research area in both academia and industry with a wide variety
of applications such as area monitoring, environmental sensing, industry automa-
tion, structural monitoring, water and wastewater monitoring, surveillance, health
monitoring, tracking of materials, and many others. Recently, numerous researches
are conducted regarding diverse challenges in wireless sensor networks. These
challenges include coverage and connectivity issues, MAC protocol design, routing
protocol design, power saving, and data gathering and fusion. However, the per-
formance of the proposed solutions strongly depends on the way sensors are
deployed in the sensed area. Sensors deployment is still an active research area in
WSN, where the main objective of these researches is maximizing the area cov-
erage while preserving connectivity and minimizing energy consumption.
Cellular automata CA is a discrete mathematical model for systems consisting of
large number of simple identical components with local interactions in which space
and time are discrete. A cellular automaton works on a grid of cells, and each cell
can assume a state from a ﬁnite set of states. The cells update their states syn-
chronously on discrete steps according to a local rule. The new state of each cell
depends on the previous states of a set of cells, including the cell itself, and
constitutes its neighborhood. The state of all cells in the lattice is described by a
conﬁguration. The rule and the initial conﬁguration of the CA specify the evolution
of CA that tells how each conﬁguration is changed in one step. Cellular automata
perform complex computations with a high degree of efﬁciency and robustness [1].
The neighbourhood of a cell might be deﬁned as the set of cells located at equal
distance from the cell. Two commonly used neighborhoods include Moore and Von
Neumann neighbourhood. Figure 1a shows the eight Moore neighborhoods cells of
cell(0, 0): cell(1, 0), cell(−1, 0), cell(0, 1), cell(0, −1), cell(1, 1), cell(1, −1), cell
(−1, 1), cell(−1, −1). Figure 1b shows the four Von Neumann neighborhood cells
of cell(0, 0): cell(1, 0), cell(−1, 0), cell(0, 1), cell(0, −1).
Given that sensors of WSNs have limited bandwidth and processing capability
that communicates only with its neighbour’s processors, proposed solutions for
WSN have distributed processing nature to ensure efﬁciency and scalability. This is
similar to CA where the communication between constituent cells is limited to local
interaction. Each individual cell in CA is in a speciﬁc state which changes over time
depending on the states of its local neighbours.
(a)                    
(b)
(-1,-1)
(-1,0)
(-1,1)
(-1,0)
(0,-1)
(0,0)
(0,1)
(0,-1)
(0,0)
(0,1)
(1,-1)
(1,0)
(1,1)
(1,0)
Fig. 1 CA neighborhood:
a Moore. b Von Neumann
88
S.M. Al-Tabbakh and E. Shaaban

This paper proposes a feasible self-deployment heuristic approach for WSN
based on the Four Quadrants Deployment Model (FQDM) proposed by Choudhury
in [2]. FQDM is a cellular automata model that divides the neighborhood of a cell
into four quadrants (North West, North East, South West and South East), and
sensors try to ﬁnd out the directions where they can move to increase the coverage.
However FQDM is an ideal deployment approach as it assumes unlimited energy
source for sensor nodes, and does not take into account the real energy consumption
rates for movement and communication of the sensors nor their residual energy
during deployment. Also FQDM reaches a saturated state where no coverage
improvement is obtained. Hence FQDM does not give a good coverage especially
at border areas. The objective of this paper is to study the effect of applying a
realistic energy model to FQDM, and propose an extended version of FQDM to
enhance the coverage at border area. The goal of the proposed approach is to
relocate the sensors in a way that maximizes the coverage while minimizing energy
consumption.
The rest of paper is organized as follows: Sect. 2 surveys researches related to
coverage problem. Section 3 presents network model and assumptions. Section 4
describes the energy-aware realistic version of the Four Quadrant Deployment
Model (FQDM). Section 5 presents the proposed Drift Towards Corners Algorithm
(DTCA). The performance of FQDM and DTCA is evaluated and compared
through conducting a simulation study presented in Sect. 6. Section 7 concludes the
paper.
2
Related Work
The deployment of sensors to achieve the maximum coverage of grid area with
minimum number of mobile sensors is proven to be NP-hard problem [3]. Many
researches have targeted the area coverage problem, and proposed several heuristics
to ﬁnd sub-optimal solutions. In this section we brieﬂy discuss some of the related
works.
Authors in [4] proposed an autonomous sensor deployment algorithm to opti-
mize energy consumption and solve energy holes problem for unattended mobile
sensor networks. It divides the target area into multiple equal width rings‚ and
solves a mathematical problem that models sensor nodes layout and energy-aware
transmission mechanism to maximize network coverage and reduce communication
cost. The simulation results demonstrate its effectiveness in terms of coverage and
network lifetime.
Another optimal placement algorithm based on a “mosaicked technology” such
that different types of mobile sensors organize themselves forming a mosaicked
pattern to achieve efﬁcient coverage with minimum cost is proposed in [5]. To
converge to the optimal state, a swarm intelligence (SI)-based sensor movement
algorithm is developed to drive the randomly deployed sensors self-organizing
themselves and reach the optimal placement state. Simulation results are presented
Energy Aware Autonomous Deployment for Mobile Wireless Sensor …
89

to demonstrate the effectiveness of the mosaic placement and the SI-based move-
ment. Other solutions of this problem in some class of work [6–8] are inspired from
physics where the sensor node determines its movement direction based on the
force received from its neighbours. Other strategies are the grid-based [9, 10] which
provide a deterministic deployment where the position of the sensor nodes is ﬁxed
according to a special grid pattern such as a triangular lattice, a square grid or a
hexagonal grid.
Quite number of researches in coverage for wireless sensor networks is presented
in the last decade using cellular automata by deterministic algorithm and proba-
bilistic algorithm [2, 11–13]. In [11] proposes a cellular learning automata-based
deployment strategy which guides the movements of unaware location sensor nodes
within the area of the network. The learning automaton in each node in cooperation
with the learning automata in the neighbouring nodes controls the movements of the
node in order to attain high coverage. The work in [12] focuses on a subset of
topology control algorithms (duty cycling and scheduling while maintaining con-
nectivity and coverage) and use the cellular automata simulation approach to
experimentally investigate which type of neighborhood should be preferred. The
algorithm in [13] determines the movement direction of a sensor based on the
weighted number of its neighbours in the positive and in the negative direction.
Choudhury in [2] proposes a cellular automata based Four Quadrants Deploy-
ment Model (FQDM) that divides the neighbourhood of a cell into four quadrants
(North West, North East, South West and South East), and sensors try to ﬁnd out
the directions where they can move to increase the coverage. However FQDM is an
ideal deployment approach as it assumes unlimited energy source for sensor nodes,
and does not take into account the real energy consumption rates for movement and
communication of the sensors nor their residual energy during deployment.
Moreover FQDM does not give a good coverage at border areas. Hence the
objective of this paper is to study the effect of applying a realistic energy model to
FQDM, and propose an extended version of FQDM to enhance the coverage at
border area. The goal of the proposed heuristic approach is to relocate the sensors in
a way that maximizes the coverage while minimizing energy consumption.
3
Network Model and Assumptions
The monitored area is assumed to be a grid of cells where the state of the cell
indicates the existence or the absence of a mobile sensor. We consider a mobile
WSN with a set of sensors initially accumulated on a center of two dimensional
area. Sensor nodes are homogenous in terms of the communication range, sensing
range, power consumption rate, and initial energy values. The sensing radius Rs and
communication range Rc of mobile sensors are Moore’s neighbourhood, and are
taken up to two and four layers respectively. When Rc <= 2Rs sensor deployments
that maximize coverage of WSNs maintaining the connectivity of its nodes [3]. In
each time epoch, a sensor can move a maximum of one cell in any direction. Each
90
S.M. Al-Tabbakh and E. Shaaban

sensor tracks its energy reserve. Obstacle free terrain is assumed in the deployment.
The sensors are allowed to spread gradually according to CA rules as we discuss in
the following section.
4
Energy Aware Four Quadrant Deployment Model
Energy aware FQDM is a feasible self-deployment heuristic approach for WSN
based on FQDM. FQDM is a cellular automata model that divides the neighbor-
hood of a cell into four quadrants (North West NW, North East NE, South West SA
and South East SE), and sensors follow CA rules to ﬁnd out the quadrant where
they can move to increase the coverage. However FQDM is an ideal deployment
approach as it assumes unlimited energy source for sensor nodes, and does not take
into account the real energy consumption rates for movement and communication
of the sensors nor their residual energy during deployment. Each time epoch of
FQDM is divided into two phases: In the ﬁrst phase, each sensor determines
whether it should move or not, and direction of movement, and the weight value
according to following equation:
Wx = 4 * N1 + 3 * N2 + 2 * N3 + 1 * N4
ð1Þ
where Ni is the number of sensors in quadrant x at distance exactly i from the
current cell [2]. A higher weight is given for sensors that are closer to the current
sensor. Each sensor determines the maximum and the minimum weight values for
its four quadrants. If the minimum value is less than k1 and the maximum value is
more than k2, then the sensor chooses the quadrant with the minimum weight value
to direct to. Where k1, k2 are two chosen adjusted positive integers. For each
quadrant, there are two nearest position where the sensor node can move to. As
shown in Fig. 2. sensor i, j can move to cell 1 or 2 in NE, for SE it can move to cell
Fig. 2 FQDM deployment
model
Energy Aware Autonomous Deployment for Mobile Wireless Sensor …
91

3 or 4, for SW it can move to cell 5 or 6, and cell 7 or 8 for NW. In the second
phase, the sensor moves to previously chosen cell if that cell is empty.
For each time epoch of energy aware FQDM, sensor node in the ﬁeld exchanges
a status packet with its neighbors up to radius four (communication range) that
inform each other about its grid position.
The energy status of each node is updated by calculating the energy cost for
communication (transmit and receiving), and motion taking into consideration the
residual energy of each node. Each sensor iterates the procedures of the two phases
for each time epoch until it reaches a steady state where its residual energy reaches a
previously set threshold value Eth, or a ﬁnal conﬁguration is obtained. At this state,
the sensor node remains still to save energy, however it shares its status with its
neighbors. For each quadrant a negative weight −1 is assigned to a steady state
node.
5
Proposed Drifting Towards Corners Algorithm (DTCA)
To enhance the coverage of the monitored area specially at corners, we proposed a
Drifting Towards Corners Algorithm (DTCA). Energy aware FQDM is initially
applied until we reach an epoch where no signiﬁcant coverage improvement is
observed. Then DTCA changes the transition rules of a subset of sensor nodes
whose residual energy reaches a previously set threshold value in order to improve
the obtained coverage. DTCA assumed that the whole monitored area is divided
into four equivalent adjacent partitions: North East (NE), North West (NW), South
East (SE) and South West (SW). It also assumes that the sensors are aware of which
partition they belong to in each epoch. Sensors that reach their steady state (residual
energy reached a threshold value) start a timer of T epochs to save energy. After
elapsing the period T, the sensor opts randomly to migrate to one corner of its
partition with different probabilities, and updates its energy status as illustrated in
Fig. 3. The impact of variation of these probability settings was heuristically
Fig. 3 DTCA deployment
model
92
S.M. Al-Tabbakh and E. Shaaban

investigated to distribute the sensors as evenly as possible while conserving their
energy as much as this is possible. Different probability settings are assigned as
follows.
The sensor node in the North West partition of the area is assigned a highest
probability 60% to move to north west neighborhood positions cell(i −1, j −1) or
cell(i −1, j) to maximize the coverage of border area, and 10% to move back to
opposite corner SW cell(i + 1, j + 1) or cell(i + 1, j) to prevent hole creation. It is
also assigned a probability 15% to move to cell(i, j −1) or cell(i + 1, j −1) in SW
and 15% move to cell (i −1, j + 1) or cell(i, j + 1) in NE to distribute the sensors
as evenly as possible.
The sensor node in the North East partition of the area is assigned a highest
probability 60% to move to north east neighborhood positions cell(i −1, j + 1) or
cell(i, j + 1) to maximize the coverage of border area, and 10% to move back to
opposite corner cell(i, j −1) or cell(i + 1, j −1) to prevent hole creation. It is also
assigned probability 15% to move to cell(i −1, j) or cell(i −1, j −1) in NW and
15% move to cell (i + 1, j + 1) or cell(i + 1, j) in SW to distribute the sensors as
evenly as possible.
The sensor node in the South West partition of the area is assigned a highest
probability 60% to move to south west neighborhood positions cell(i, j −1) or cell
(i + 1, j −1) to maximize the coverage of border area, and 10% to move back to
opposite corner cell(i −1, j + 1) or cell(i, j + 1) to prevent hole creation. It is also
assigned probability 15% to move to cell(i + 1, j) or cell(i + 1, j + 1) in SE and
15% to move to cell(i −1, j −1) or cell(i −1, j) in NW to distribute the sensors as
evenly as possible.
The sensor node in the South East partition of the area is assigned a highest
probability 60% to move to south east neighborhood positions cell(i + 1, j) or cell
(i + 1, j + 1) to maximize the coverage of border area, and 10% to move back to
opposite corner cell(i −1, j −1) or cell(i −1, j) to prevent hole creation. It is also
assigned a probability 15% to move to cell(i, j −1) or cell(i + 1, j −1) in SW, and
15% to move to cell(i −1, j + 1) or cell(i, j + 1) in NE to distribute the sensors as
evenly as possible. DTCA algorithm is listed in Fig. 4.
6
Performance Evaluation
We evaluated the coverage of FQDM deployment model in a bounded area with
realistic energy model and compare its performance with DTCA deployment model
through conducting an extensive simulation study. To save WSN energy during
deployment in FQDM, sensors opt to stop moving when their energy reserves reach
a given threshold value Eth speciﬁed by the designer. Setting a proper value for
energy threshold highly affects the network lifetime. Our goal is to study and
compare the effect of different energy thresholds on the deployment performance
for both FQDM and DTCA. This section describes the simulation environment,
performance metrics, and analyses of the obtained results.
Energy Aware Autonomous Deployment for Mobile Wireless Sensor …
93

6.1
Simulation Environment and Performance Metrics
To evaluate the performance of the energy aware FQDM, we simulated an envi-
ronment of two dimension 100 × 100 grid in Matlab. We applied the Energy
model in [14] that simulate the well-known IEEE 802.15.4 Mica2 wireless sensor
Set energy threshold to Eth;
Do while coverage is improving
Each sensor in WSN applies Energy-aware FQDM;
End Do
Repeat
Select steady state nodes {Sth } with residual energy <=Eth
For each sensor S in {Sth }
{
Wait for T epochs
If S is located in NW Then
Assign 60% probability to move to cell(i-1, j-1) or cell(i-1, j) // to maximize coverage
Assign 10% probability to move to cell(i+1, j+1) or cell(i+1, j) // to prevent hole creation
Assign 15% probability to move to cell(i, j-1) or cell(i+1, j-1) // for even distribution 
Assign 15% probability to move to cell(i-1, j+1) or cell(i, j+1) // for even distribution
ElseIf  S is located in NE Then
Assign 60% probability to move to cell(i-1, j+1) or cell(i, j+1)  // to maximize coverage
Assign 10% probability to move to cell(i, j-1) or cell(i+1, j-1) // to prevent hole creation
Assign 15% probability to move to cell(i-1, j-1) or cell(i-1, j-1) // for even distribution 
Assign 15% probability to move to cell(i+1, j+1) or cell(i+1, j) // for even distribution
ElseIf  S is located in SW Then
Assign 60% probability to move to cell(i, j-1) or cell(i+1, j-1)  // to maximize coverage
Assign 10% probability to move to cell(i-1, j+1) or cell(i, j+1) // to prevent hole creation
Assign 15% probability to move to cell(i+1, j) or cell(i+1, j+1) // for even distribution 
Assign 15% probability to move to cell(i-1, j-1) or cell(i-1, j) // for even distribution
ElseIf  S is located in SE Then
Assign 60% probability to move to cell(i+1, j) or cell(i+1, j+1)   // to maximize coverage
Assign 10% probability to move to cell(i-1, j-1) or cell(i-1, j) // to prevent hole creation
Assign 15% probability to move to cell(i, j-1) or cell(i+1, j-1) // for even distribution 
Assign 15% probability to move to cell(i-1, j+1) or cell(i, j+1) // for even distribution
EndIf
Relocate sensor S and update its energy status; 
}
For each sensor S not in {Sth}
Apply Energy-aware FQDM;
Until maximum coverage is achieved
Fig. 4 DTCA algorithm
94
S.M. Al-Tabbakh and E. Shaaban

mote. Energy Tx = 3.12 μJ/bit. Energy Rx = 2.34 μJ/bit. Energy motion = 90
mJ/m. Each sensor is assigned an initial energy of 500 J. Different experiments are
carried out to deploy a bundle of sensors N in bounded square ﬁeld of dimension
100 * 100. We set N equal to 1225 after conducting many simulation experiments
to get the best value. The sensors N are initially placed in the center of a two
dimensions 100 × 100 grid. Each cell has two states: state 1 (white) to indicate that
the cell has a sensor node, while empty cell is set to state 0 (black).
The performance is evaluated using the following metrics:
• Area coverage percentage: the percentage of total grids covered by at least one
sensor node.
• Deployment time: the required time epochs to get the ﬁnal conﬁguration.
• Residual energy percentage: the ratio of total energy reserved on sensor nodes to
the total initial energy when the ﬁnal conﬁguration is reached.
• Number of relocations: total number of movement done by sensor nodes of the
network while they are deployed.
• Steady state nodes: the ratio of nodes that reached Eth to the total deployed
nodes while they are deployed.
• k-coverage: a sensor deployment pattern where each cell in the area is covered
by at least k deployed sensor nodes.
6.2
Results Analysis
Figure 5a shows the initial position of sensor nodes on the ﬁeld while Fig. 5b, c
indicate the deployment pattern of 1225 sensors at 400 time epoch for FQDM and
DTCA. Figure 6 shows the area coverage percentage for FQDM and DTCA with
residual energy threshold Eth adjusted to 200J and 300J respectively. Figure 7
shows how the total residual energy is affected.
a. Initial Position
b. FQDM
c. DTCA
Fig. 5 Deployment pattern of 1225 sensors at 400 time epoch for FQDM and DTCA
Energy Aware Autonomous Deployment for Mobile Wireless Sensor …
95

Results show that with increasing time epochs, DTCA achieves the highest area
coverage during the deployment for Eth = 200j. The deployment pattern obtained
by DTCA achieves a maximum coverage of 97% with saving 45% residual energy
percentage at time epoch 375, while FQDM achieves 91% with almost the same
residual energy saving 46% at the same time epoch and energy threshold.
With FQDM, varying Eth has no signiﬁcant effect on the coverage percentage
though it consumes a considerable higher amount of energy. It achieves a maximum
coverage of 91% with 46% residual energy percentage for Eth = 200j‚ and 89%
coverage with 52% residual energy percentage for Eth = 300j. However varying
Eth highly affects the coverage percentage and residual energy for DTCA, it
achieve a maximum coverage of 97% with 45% residual energy percentage for
Eth = 200j, and 89% coverage with 52% residual energy percentage for 300j. From
the simulation results we can deduce that with a given node density and adjusted
energy threshold our proposed algorithm DTCA to enhance FQDM can converge to
a global optimal distribution.
Figure 8 shows the total number of relocations during deployment for FQDM
and DTCA with Eth equals to 200j and 300j respectively.
Fig. 6 Area coverage
percentage for FQDM and
DTCA with different Eth
Fig. 7 Residual energy for
FQDM and DTCA with
different Eth
96
S.M. Al-Tabbakh and E. Shaaban

Results show that with increasing epochs, the number of relocations is increased
constantly. Intuitively, steady state sensors growth with increasing number of
relocations results in hindering further relocation and coverage improving. How-
ever it extends the lifetime of the network. Figure 9 shows the percentage of steady
state nodes during deployment for both FQDM and DTCA with Eth equals to 200j
and 300j respectively.
Interestingly we also observe that in DTCA deployment, the minimum residual
energy gets close to the average residual energy (the total residual energy divided
by the number of sensors) when reaching the ﬁnal conﬁguration. This demonstrates
Fig. 9 Steady state nodes for FQDM and DTCA with different Eth
Fig. 8 Total relocations for FQDM and DTCA with different Eth
Energy Aware Autonomous Deployment for Mobile Wireless Sensor …
97

that DTCA has the ability to balance the energy consumption of WSN sensors and
avoid overburdening some of the nodes.
It should be mentioned that although energy aware FQDM and DTCA are not
designed for k-coverage requirement where each cell is covered by at least k
sensors, they provide a good results for applications with k-coverage requirements.
Table 1 shows k-coverage results at 375 epoch for DTCA and FQDM with different
Eth. We noticed that 67% of the monitored area is covered by at least 3 sensors for
DTCA with Eth = 200j, and 66% of the area for DTCA with Eth = 300j. For
energy aware FQDM, 64% of the monitored area is covered by at least 3 sensors
with Eth = 200j, and 66% of the area with Eth = 300j.
7
Conclusion
This paper proposes an energy aware autonomous deployment heuristic approach
for mobile WSNs based on Four Quadrants Deployment Model (FQDM). The
objective of the paper is to study the effect of applying a realistic energy model to
FQDM, and propose an extended version DTCA to enhance the coverage at border
areas by relocating the sensors in a way that maximizes the coverage as much as
possible while minimizing energy consumption. We evaluate the coverage of
FQDM in a bounded square area and compare its performance with DTCA. Sim-
ulation results demonstrate that DTCA can converge to a global optimal distribu-
tion. It gives a signiﬁcant higher coverage while conserving energy of sensors.
References
1. Chopard, B., Droz, M.: Cellular Automata Modelling of Physical Systems. Cambridge
University Press (2005). ISBN 0521461685
2. Choudury, S.: Cellular Automaton Based Algorithms for Wireless Sensor Networks. PhD
thesis, School of Computing, Queen’s University, Canada (2011)
3. Wang, Y., Zhang, Y., Liu, J., Bhandari.: Coverage, connectivity, and deployment in wireless
sensor networks. In: Recent Development in Wireless Sensor and Ad-hoc Networks, pp. 25–
44. Springer, India (2015)
4. Chen, J., Jie, J., Wen, Y., Zhao, D.: Towards an optimal energy consumption for unattended
mobile sensor networks through autonomous sensor redeployment. Sci. World J. (2014)
Table 1 k-coverage for
DTCA and FQDM
k/coverage
DTCA
FQDM
200j (%)
300j (%)
200j (%)
300j (%)
k >= 1
97
91
89
88
k >= 3
67
66
64
66
k >= 5
10
19
14
18
k >= 7
1
0.53
2
2
98
S.M. Al-Tabbakh and E. Shaaban

5. Miao, L., Qi, H., Wang, F.: Biologically-inspired self-deployable heterogeneous mobile
sensor networks. In: IEEE/RSJ International Conference on Intelligent Robots and Systems,
2005, (IROS 2005). IEEE (2005)
6. Zou, Y., Chakrabarty, K.: Sensor Deployment and Target Localization Based on Virtual
Forces. IEEE INFOCOM (2003)
7. Li, J., Zhang, B., Cui, L., Chai, S.: An extended virtual force-based approach to distributed
self-deployment in mobile sensor networks. Int. J. Distrib. Sens. Netw. (2012)
8. Chen, J., Li, S., Yuxian, S.: Novel deployment schemes for mobile sensor networks. Sensors 7
(11), 2907–2919 (2007)
9. Park, P., Min, S.G., Han, Y.H.: A grid-based self-deployment schemes in mobile sensor
networks. In: Ubiquitous Information Technologies and Applications (CUTE), IEEE Explore,
pp. 1–5 (2010)
10. Fletcher, G., Li, X., Nayak, A., Stojmenovic, I.: Back-tracking based sensor deployment by a
robot team. In: 2010 7th Annual IEEE Communications Society Conference on Sensor, Mesh
and Ad Hoc Communications and Networks (SECON), pp. 1–9. IEEE (2010)
11. Esnaashari, M., Meybodi, M.: A cellular learning automata-based deployment strategy for
mobile wireless sensor networks. J. Parallel Distrib. Comput. 988–1001 (2011)
12. Athanssopoulos, S., Athanassopoulos, S., Kaklamanis, C., Kalfountzos, G., Papaioannou, E.:
Cellular automata for topology control in wireless sensor networks using Matlab. In: Future
Information Technology, Applications and Services. Springer, pp. 13–20 (2012)
13. Choudhury, S., Salomaa, K., Akl, S.G.: Cellular automaton based motion planning algorithms
for mobile sensor networks. In: Theory and Practice of Natural Computing, pp. 108–120.
Springer, Berlin (2012)
14. Calle, M.: Energy consumption in wireless sensor networks using gsp. Master’s Thesis,
School of Information Sciences, University of Pittsburgh (2006)
Energy Aware Autonomous Deployment for Mobile Wireless Sensor …
99

An Optimal Process for Average
Value-at-Risk Portfolios in Financial
Management
Yuji Yoshida
Abstract A dynamic average value-at-risk portfolio model under uncertainty is
discussed. At each period analytical solutions for the problem are obtained. By
dynamic programming, an optimality equation for optimal average value-at-risks is
derived. It is shown that the optimal average value-at-risk portfolios are solutions of
the optimality equation.
Keywords
Average value-at-risk ⋅Optimal portfolio ⋅Dynamic risk allocation ⋅
Risk probability ⋅Bankruptcy ⋅Dynamic programming
1
Introduction
In ﬁnancial asset management, portfolio technique is important for hedging the risk
and it is used to make asset management stable. As a classical portfolio theory,
Markowitz’s mean-variance model is studied by many researchers and fruitful results
have been achieved, and the variance-minimizing is also important to minimize the
risk in portfolio [5, 7–9]. Recently, value-at-risk (VaR) is used widely in ﬁnance to
estimate the risk of worst-scenarios. VaR is a risk-sensitive criterion based on per-
centiles, and it is one of the standard criteria in asset management [3, 6, 12–14].
VaR is a kind of risk values of the asset prices at a speciﬁed risk-level probability
and it is for selecting portfolios to get rid of bad scenarios in investment. An exten-
sion of VaR is average value-at-risk (AVaR), and it is known that AVaR is a coher-
ent risk measure but VaR is not coherent [1]. Markowitz’s mean-variance criterion
and variance-minimizing criterion are represented by quadratic programming, and
AVaR criterion is analyzed based on the results regarding Markowitz’s criterion. In
this paper dynamic AVaR portfolio selection problem is proposed in order to max-
imize both of AVaR and the expected rates of return, and owing to AVaR we can
Y. Yoshida (✉)
Faculty of Economics and Business Administration, University of Kitakyushu,
4-2-1 Kitagata, Kokuraminami, Kitakyushu 802-8577, Japan
e-mail: yoshida@kitakyu-u.ac.jp
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_12
101

102
Y. Yoshida
maximize the expected rate of return after due consideration of the worst-scenarios.
This paper derives analytical solutions for the AVaR portfolio problem at each step,
which is strongly related to the bankruptcy and the falling in the asset prices [4].
Introducing AVaR based on conditional probability, we discuss a dynamic optimiza-
tion problem and we derive the optimality equation and the optimal solution for the
model by dynamic programming.
2
A Dynamic Portfolio Model
First we explain a portfolio model with n stocks, where n is a positive integer. Let
{0, 1, 2, … , T} be the time space with an expiration date T, and ℝdenotes the set of
all real numbers. Let (𝛺, , P) be a probability space, where is a 𝜎-ﬁeld and P
is a non-atomic probability on a sample space 𝛺. For an asset i = 1, 2, … , n, a stock
price process {Si
t}T
t=0 is given by rates of return Ri
t [10, 11]:
Si
t = Si
t−1(1 + Ri
t)
(1)
for t = 1, 2, … , T, where {Ri
t}T
t=1 is an integrable sequence of independent real-
valued random variables. Hence (w1
t , w2
t , … , wn
t ) is called a portfolio weight vector
if it satisﬁes ∑n
i=1 wi = 1, and further the portfolio is said to allow for short selling if
wi
t ≥0 for all i = 1, 2, … , n. Then the rate of return with a portfolio (w1
t , w2
t , … , wn
t )
is given by
Rt = w1
t R1
t + w2
t R2
t + ⋯+ wn
t Rn
t .
(2)
Therefore, the reward at time t(= 1, 2, … , T) follows
St = St−1
n
∑
i=1
wi
t(1 + Ri
t) = St−1(1 + Rt).
(3)
This paper deals with a dynamic portfolio model for stock price processes {Si
t}T
t=0.
The falling of asset prices is one of the most important risks in stock markets. The
theoretical bankruptcy at time t occurs on scenarios 𝜔satisfying St(𝜔) ≤0, i.e. it
follows 1 + Rt(𝜔) ≤0 from (3). Similarly, for a constant 𝛿∈[0, 1], a set of sample
paths {𝜔∈𝛺∣1 + Rt(𝜔) ≤1 −𝛿} = {𝜔∈𝛺∣Rt(𝜔) ≤−𝛿} is the event of scenar-
ios where the asset price St will fall from the current price St−1 to a lower level than
100(1 −𝛿) % of the current price St−1, i.e. the rate of falling is 100 𝛿%. The parame-
ter 𝛿is called the rate of falling. Then the probability of falling is also given by
p𝛿= P(Rt ≤−𝛿).
(4)

An Optimal Process for Average Value-at-Risk Portfolios ...
103
For example, p𝛿denotes the probability of the falling below par value if ‘𝛿= 0’ and
it indicates the probability of the bankruptcy if ‘𝛿= 1’. In this paper, we discuss
dynamic portfolio optimization regarding the rate of falling 𝛿.
For a positive probability p, VaR regarding the rate of return Rt is given by a real
number v satisfying
P(Rt ≤v) = p
(5)
since P is non-atomic. VaR v is the upper bound of the rate of return Rt at the worst
scenarios under a given risk probability p, and then VaR v in (5) is denoted by
VaRp(Rt). From (4) and (5), for a risk probability p = p𝛿, the rate of falling is
𝛿= −VaRp(Rt).
(6)
In next section we discuss the minimization of the rate of falling (6). In this paper,
we deal with a case where VaR v in (5) has the following representation.
(VaR v) = (the mean) −(a positive constant 𝜅p) × (the standard deviation),
(7)
where the positive constant 𝜅p is given corresponding to the probability p. Equation
(7) holds if the distribution of the rate of return Rt is Gaussian [2, 6].
3
A Dynamic AVaR Portfolio Model
First we introduce mathematical notations of average value-at-risk [12, 13]. Let 
be the set of all integrable -adapted real-valued random variables X on 𝛺with
a continuous distribution function x ↦FX(x) = P(X < x) for which there exists a
non-empty open interval I such that FX(⋅) ∶I →(0, 1) is strictly increasing and
onto. Then there exists a strictly increasing and continuous inverse function F−1
X ∶
(0, 1) →I. We note that FX(⋅) ∶I →(0, 1) and F−1
X ∶(0, 1) →I are one-to-one and
onto. The value-at-risk (VaR) at a probability p(∈(0, 1)) is given by the percentile
of the distribution function FX, i.e. VaRp(X) = sup{x ∈I ∣FX(x) ≤p}. Then we
have FX(VaRp(X)) = p and VaRp(X) = F−1
X (p). The average value-at-risk (AVaR) at
a probability p is given by
AVaRp(X) = 1
p ∫
p
0
VaRq(X) dq
(8)
for p ∈(0, 1). It is known that AVaR is a coherent risk measure [1] but VaR is
not coherent. From (3), AVaR for the reward St at time t is given by AVaRp(St) =
AVaRp
(St−1(1 + Rt)) . To discuss the dynamics, we introduce VaR based on con-
ditional expectations. Let be a sub-𝜎-ﬁeld of . Deﬁne a map x ↦FX(x ∣) =
P(X < x ∣) = E(1{X<x} ∣). We introduce VaR of X(∈) under a condition at a

104
Y. Yoshida
probability p by VaRp(X ∣) = sup{x ∈I ∣FX(x ∣) ≤p}. Then AVaR under a con-
dition is given by AVaRp(X ∣) = 1
p ∫p
0 VaRq(X ∣) dq for p ∈(0, 1), and we note
AVaRp(X ∣) is a -measurable random variable and FX(AVaRp(X ∣) ∣) ≤p for
p ∈(0, 1). From (3), portfolio weights (w1
t , w2
t , … , wn
t ) are decided sequentially and
predictably. We note that St is estimated under information t−1 up to time t −1.
Then VaR of St under information t−1 at a probability p is AVaRp(St ∣t−1) =
AVaRp
(St−1(1 + Rt) ∣t−1
) . This term means the risk of worst scenarios which
occur on the transition from time t −1 to time t. Therefore, taking the sum of these
risks which occur at each time, we demonstrate the following dynamic portfolio
problem regarding the total AVaR under information {t−1}T
t=1. Let a discount rate
𝛽be a positive constant and take S0 = 1 for simplicity. Deﬁne the set of portfolios
by = {(w1, w2, … , wn) ∈ℝn ∣∑n
i=1 wi = 1 and wi ≥0 (i = 1, 2, … , n)}.
Dynamic problem for AVaR:
Maximize the total AVaR
E
( T
∑
t=1
𝛽t−1AVaRp(St ∣t−1)
)
=
T
∑
t=1
𝛽t−1
t−1
∏
s=1
(1 + E(Rs)) ⋅(1 + AVaRp(Rt))
(9)
with portfolio weights (w1
t , w2
t , … , wn
t ) ∈(t = 1, 2, … , T).
By dynamic programming, we obtain the following equations.
Theorem 1 The optimal AVaR in Dynamic problem for AVaR is given by v1 which is
deﬁned inductively by the sequence {vt} of sub-total AVaR after time t −1 satisfying
the following backward optimality equations:
vt−1 =
max
(w1,…,wn)∈
{
1 + AVaRp
( n
∑
i=1
wiRi
t−1
)
+ 𝛽
(
1 +
n
∑
i=1
wiE(Ri
t−1)
)
vt
}
(10)
for t = 2, 3, … , T, and
vT =
max
(w1,…,wn)∈
{
1 + AVaRp
( n
∑
i=1
wiRi
T
)}
.
(11)
4
An Optimal Portfolio for AVaR
First we estimate the rate of return with a portfolio [12, 13]. Let the mean, the vari-
ance and the covariance of the rate of return Ri
t, which is given in (1), respectively by
𝜇i
t = E(Ri
t), (𝜎i
t)2 = E((Ri
t −𝜇i
t)2) and 𝜎ij
t = E((Ri
t −𝜇i
t)(Rj
t −𝜇j
t)) for i, j = 1, 2, … , n.
Hence we assume that the determinant of the variance-covariance matrix 𝛴t = [𝜎ij
t ]
is not zero and there exists its inverse matrix 𝛴−1
t . This assumption is natural and

An Optimal Process for Average Value-at-Risk Portfolios ...
105
it can be realized easily by taking care of the combinations of assets. For a port-
folio (w1, w2, … , wn) ∈, we calculate the expectation and the variance regard-
ing Rt = ∑n
i=1 wiRi
t. The expectation 𝜇t of the rate of return Rt with the portfolio is
𝜇t = E(Rt) = ∑n
i=1 wiE(Ri
t) = ∑n
i=1 wi𝜇i
t. On the other hand, the variance (𝜎t)2 of the
rate of return Rt with the portfolio is (𝜎t)2 = E((Rt −𝜇t)2) = ∑n
i=1
∑n
j=1 wiwj𝜎ij
t for
i = 1, 2, … , n. Therefore, for a positive probability p, AVaR of the rate of return Rt
is evaluated as AVaRp(Rt) = ∑n
i=1 wi𝜇i
t −𝜅
√∑n
i=1
∑n
j=1 wiwj𝜎ij
t with a positive con-
stant 𝜅= 1
p ∫p
0 𝜅q dq with 𝜅p in (7). Let 𝟏be a unit vector and let 𝜇t = [𝜇i
t], 𝛴t = [𝜎ij
t ],
At = 𝟏T𝛴−1
t 𝟏, Bt = 𝟏T𝛴−1
t 𝜇t, Ct = 𝜇T
t𝛴−1
t 𝜇t and 𝛥t = AtCt −B2
t , where T denotes the
transpose of a vector. Now step by step we discuss a portfolio problem to minimize
the rate of falling. First, we deal with a variance-minimizing problem. For a given
constant 𝛾, which is the total expected rate of return to be guaranteed for portfolios,
we consider the following quadratic programming with respect to portfolios.
Variance-minimizing (P1):
Minimize
the
variance
∑n
i=1
∑n
j=1 wiwj𝜎ij
t
with
respect to portfolios (w1, w2, … , wn) satisfying ∑n
i=1 wi = 1 under a condition
∑n
i=1 wi𝜇i
t = 𝛾.
Lemma 1 [12, 13]. The solution of the variance-minimizing (P1) is given by w =
𝜉𝛴−1𝟏+ 𝜂𝛴−1𝜇and then the corresponding variance is 𝜌= A𝛾2−2B𝛾+C
𝛥
.
Solution w in Lemma 1 is called a minimal risk portfolio, and a set = {(𝜌, 𝜇) ∣
𝜌= A(𝜇)2−2B𝜇+C
𝛥
and 𝜇≥B
A} is also called the eﬃcient frontier [7, 8], where 𝜌is a
variance and 𝜇is an expected rate of return for acceptable minimal risk portfolios.
Next we consider a risk-sensitive model in order to deal with a portfolio problem
to minimize the rate of falling in the third step. For a constant 𝛾, we discuss the
following risk-sensitive portfolio problem.
Risk-sensitive problem (P2):
Maximize risk-sensitive expected rate of return
n
∑
i=1
wi𝜇i
t −𝜅
√
√
√
√
n
∑
i=1
n
∑
j=1
wiwj𝜎ij
t
(12)
with respect to portfolios (w1, w2, … , wn) (∑n
i=1 wi = 1) under the condition
∑n
i=1 wi𝜇i
t = 𝛾.
Lemma 2 Let A and 𝛥be positive. If 𝜅satisﬁes 𝜅2 > 𝛥∕A, then a function v ∶ℝ→
ℝgiven by v(𝛾) = 𝛾−𝜅
√
A𝛾2−2B𝛾+C
𝛥
(𝛾∈ℝ) is concave and it has the maximum
v(𝛾∗) = B−
√
A𝜅2−𝛥
A
at 𝛾∗= B
A +
𝛥
A
√
A𝜅2−𝛥.

106
Y. Yoshida
Now we discuss the following portfolio problem to minimize the rate of falling (6)
without allowance for short selling. The rate of falling (6) is given by the following
(13).
Portfolio minimizing the risk of falling (P3):
Minimize the risk of falling
𝛿= −AVaRp(Rt) = −
n
∑
i=1
wi𝜇i
t + 𝜅
√
√
√
√
n
∑
i=1
n
∑
j=1
wiwj𝜎ij
t
(13)
with portfolio weights (w1, w2, … , wn) ∈.
Hence since we have infw(13) = inf𝛾(infw∶∑n
i=1 wi𝜇i
t=𝛾(13)) = −sup𝛾(12), by
Lemma 2 we arrive at the following analytical solutions for (P3).
Lemma 3 Let A and 𝛥be positive, and let 𝜅satisfy 𝜅2 > C. Then an optimal solu-
tion for (P3) is given by w∗= 𝜉𝛴−1𝟏+ 𝜂𝛴−1𝜇, and then the corresponding rate of
falling is 𝛿(𝛾∗) = −B−
√
A𝜅2−𝛥
A
at the expected rate of return 𝛾∗= B
A +
𝛥
A
√
A𝜅2−𝛥, where
𝜉= C−B𝛾∗
𝛥
and 𝜂= A𝛾∗−B
𝛥
.
Condition (V). vt > 0 for all t = 1, 2, … , T.
Condition (V) is a natural one because if Condition (V) is not satisﬁed at some
time t, it has already gone bankrupt. Under Condition (V), from Theorem 1 and
Lemma 3, we obtain the following results. We can calculate numerical optimal solu-
tions by the optimality equation in Theorem 2 and numerical optimal solutions at
each time given in Lemma 3.
Theorem 2 Suppose Condition (V) is satisﬁed. The optimal AVaR v1 in Theorem 1 is
given by the sequence {vt} of sub-total AVaR after time t −1 satisfying the following
backward optimality equations:
vt−1 =
max
(w1,…,wn)∈(1 + 𝛽vt)
⎛
⎜
⎜⎝
1 +
n
∑
i=1
wi𝜇i
t−1 −
𝜅
1 + 𝛽vt
√
√
√
√
n
∑
i=1
n
∑
j=1
wiwj𝜎ij
t−1
⎞
⎟
⎟⎠
(14)
for t = 2, 3, … , T, and
vT =
max
(w1,…,wn)∈
⎛
⎜
⎜⎝
1 +
n
∑
i=1
wi𝜇i
T −𝜅
√
√
√
√
n
∑
i=1
n
∑
j=1
wiwj𝜎ij
T
⎞
⎟
⎟⎠
.
(15)
Acknowledgements This
research
is
supported
from
JSPS
KAKENHI
Grant
Number
JP 16K05282.

An Optimal Process for Average Value-at-Risk Portfolios ...
107
References
1. Artzner, P., Delbaen, F., Eber, J.-M., Heath, D.: Coherent measures of risk. Math. Finance 9,
203–228 (1999)
2. Chaoui, L.El., Oks, M., Oustry, F.: Worst-case value at risk and robust portfolio optimization:
a conic programming approach. Oper. Res. 51, 543–556 (2003)
3. Jorion, P.: Value at Risk: The New Benchmark for Managing Financial Risk. McGraw-Hill,
New York (2006)
4. Kumar, P.R., Ravi, V.: Bankruptcy prediction in banks and ﬁrms via statistical and intelligent
techniques—a review. Eur. J. Oper. Res. 180, 1–28 (2007)
5. Markowitz, H.: Mean-Variance Analysis in Portfolio Choice and Capital Markets. Blackwell,
Oxford (1990)
6. Meucci, A.: Risk and Asset Allocation. Springer, Heidelberg (2005)
7. Pliska, S.R.: Introduction to Mathematical Finance: Discrete-Time Models. Blackwell Publish-
ers, New York (1997)
8. Ross, S.M.: An Introduction to Mathematical Finance. Cambridge University Press, Cam-
bridge (1999)
9. Steinbach, M.C.: Markowitz revisited: mean-variance model in ﬁnancial portfolio analysis.
SIAM Rev. 43, 31–85 (2001)
10. Yoshida, Y.: The valuation of European options in uncertain environment. Eur. J. Oper. Res.
145, 221–229 (2003)
11. Yoshida, Y.: A discrete-time model of American put option in an uncertain environment. Eur.
J. Oper. Res. 151, 153–166 (2003)
12. Yoshida,Y.: A dynamic value-at-risk portfolio model In: Torra, V., et al. (eds.) Modeling Deci-
sions for Artiﬁcial Intelligence 2011. LNAI, vol. 6820, pp. 43–54. Springer (2011)
13. Yoshida, Y.: A dynamic risk allocation of value-at-risks with portfolios. J. Adv. Comput. Intell.
Intell. Inform. 16, 800–806 (2012)
14. Yoshida,Y.: A decision process with portfolios for value-at-risks. Int. Conf. Numer. Anal. Appl.
Math. 1648(070008), 1–5 (2015). American Institute of Physics Publishing

On Quantiﬁcation of the Hidden
Distributed Generation Capacity
and Its Effects
Vladislav Samoylenko, Stanislav Eroshenko and Andrew Pazderin
Abstract The energy efﬁciency starts from energy accounting. At the level of
strategic planning, that means the formation energy balances. In terms of distributed
generation (DG), the problem at hand is how to ensure a correct estimation of the
installed capacity of DG in cases when it operates standalone. This “hidden” DG
may sufﬁciently inﬂuence the power system operation. The paper presents the
results of investigations devoted to DG penetration assessment. A novel approach
to existing and prospective DG assessment is proposed, which combines statistical,
economic and geographical methods. The approach provides DG power ranging
according to the sphere of its application. It is suitable to predict the centers of DG
interconnection.
Keywords DG ⋅CHP ⋅Energy efﬁciency ⋅Installed capacity ⋅Develop-
ment plans
1
Introduction
Currently, in many documents that describe perspectives of the energy sector, the
reference is made to the necessity for distributed generation (DG) development [1,
2]. DG is usually deﬁned as a set of modular installations of about several MW
(Gcal/h) installed capacity, producing electricity and heat at the point of con-
sumption and not using an electrical grid to transmit it [3, 4]. At the areas of severe
climatic conditions and the areas where the cogeneration is essential (both include
the Russian Federation), preference is usually given to hydrocarbon-fueled DG.
V. Samoylenko ⋅S. Eroshenko (✉) ⋅A. Pazderin
Ural Federal University, Mira Str. 19, 620002 Yekaterinburg, Russia
e-mail: s.a.eroshonko@urfu.ru
V. Samoylenko
e-mail: v.o.samoylenko@urfu.ru
A. Pazderin
e-mail: trenina_katerina@mail.ru
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_13
109

However, the documents or their sections devoted to DG are often declarative:
they promote the necessity for DG development, but do not take into account and
do not clarify the mechanism of development. In other words, they do not respond
fully to the questions as to where and why DG emerges, who are the main stake-
holders, what beneﬁts the region gain from DG implementation.
Furthermore, at the level of strategic planning, every power system needs the
formation of generation and consumption balances. In terms of DG, the problem at
hand is how to ensure a correct estimation of the installed capacity and the power
available from DG, which is not scheduled, in cases when it operates standalone or
just doesn’t participate in the market. This “hidden” DG may sufﬁciently inﬂuence
the power system operation as well as the region energy infrastructure development.
For this reason, it should be taken into account in energy development plans.
The work presented includes compiled statistics of the DG success in Russia for
different types of business entities; the active types of economic entities being
potential DG stakeholders are considered; existing and prospective DG assessment
is proposed, which combines statistical, economic and geographical methods [5, 6].
2
DG Penetration Level
The initial data sources for the quantiﬁcation of DG are provided by:
• DG Interconnection Acts (for existing and prospective objects);
• Energy Development Plans (for existing objects);
• Opensource Data (for existing objects).
Interconnection acts make available the quantiﬁcation of the interconnected DG
according to the whole complex of the ofﬁcial requirements. Interconnection acts
database is shared by authorities, such as the System Operator and Interregional
Grid Company. Energy development plans enable to account existing and
prospective objects in different areas of the region. The open source data is mostly
lists of projects implemented by different design companies and corresponding
analytical reviews.
The data collected show the level of DG at about 500 MW that is 4.9% of the
region generation installed capacity (Fig. 1). It is much more than 1.2–2.0%
Fig. 1 The initial data sources
110
V. Samoylenko et al.

declared by the authorities. Also, the dynamics of the installed capacity is available,
which makes it possible to make forecasts up to 2020. Currently, the share of DG
does not exceed 4.9% of the installed capacity. The slowdown is due to general
energy consumption reduction and the development of all types of civil engineering
infrastructures. It is unlikely that the percentage of the DG installed capacity in the
region energy system will exceed 6% up to 2020.
Then, it is necessary to consider the DG features possessed by the particular
districts of the region.
3
DG Development Forecasting
First of all, the economic aspect is considered. It provides for the economic
development of the region due to the proﬁt gained by certain companies by
reducing energy costs in case of DG utilization. Nevertheless, the reduction of
region energy prices as a result of the market competition is unlikely to occur in the
short-term perspective for a number of technological and organizational reasons.
The main technological background of the DG development (Fig. 2) is related
to:
Novolyalinskiy UD
Garinskiy UD
Karpinsk
Taborinskiy MD
0
20
40
60
80
100 km
Tavda
Tugulim
Reg
Artemovskiy UD
Tumen
region
Asbest
Bogdanovich
Revda
Sredneuralsk
Verhnyaya
Pishma
Krasnoufimsk
Staroutkinsk
Verhnyaya
Salda
Gornouralskiy
Kachkanar
Serov
Krasnoturinsk
Volchansk
Arti
Shalya
Perm
region
Pelim UD
N.Tura
Lesnoy
Nighniy
Tagil
Pervouralskaya
Kamensk-
Uralskiy
Sisert
Aramil
Zarechniy
Degtyarsk
Suhoy Log
Novouralsk
Verhniy
Tagil
Irbit
Alapaevsk
Krasnouralsk
Yugorsk
Gremyachinsk
Chelyabinsk
Turinskiy UD
Slobodo-
Turinskiy MD
Baikalovskiy MD
Alapaevskiy MD
Sosvinskiy UD
Talickiy UD
Pishminskiy UD
Kamishlovskiy MD
Achitskiy UD
Nigneserginskiy UD
economically active industrial centers
Severouralsk
Ivdel'
Chelyabinsk
region
Verhoturskiy UD
natural gas distribution system availability
Nighnyaya Salda
Kushvinskiy UD
Polevskoy
Novaya Lyalya
timber industrial centers
peat deposits
economically developing industrial centers
economically passive industrial centers
(a) Power grid infrastructure
(b) Fuel insfrastructure
Fig. 2 The main technological background of the DG development a the availability of power
grid infrastructure in the region; distant rural areas and villages without the grid interconnection;
b cogeneration centers and main gas consumption centers
On Quantiﬁcation of the Hidden Distributed Generation …
111

• the availability of the power grid infrastructure, including load of 35–110 kV
power substations, the need for the electriﬁcation of distant rural areas and
villages (Fig. 2a); DG capacity is also speciﬁed;
• the availability of external heat supply systems and the account of the possible
consumption of low- and medium-grade heat for industrial purposes (green
areas); the extent of development of gas distribution networks as the main fuel
infrastructure; the use of local fuels as an alternative fuel infrastructure (these are
given in Fig. 2b).
According to the research methods of economic and geographical zoning, a DG
forecast map for the Sverdlovsk region of Russia has been obtained. It points out
the speciﬁc groups of consumers that use energy produced by DG and marks it with
corresponding numbers. Two-digit numbering allows dividing the groups into
subgroups by the DG power which is directly proportional to the second digit.
Hereinafter the numbers are given in italics.
The groups of typical DG-owners are listed in Table 1. The most high-powered
DG projects are in mining {1} and construction industries {6.3} (16–25 MW),
ferrous and non-ferrous metallurgy {2.2} (14–22 MW), as well as heavy engi-
neering {3.3} (10–20 MW). Medium-power DG is used for the production of
construction and decoration materials {6.2} (4–12 MW), metalworking companies
{2.1} and integrated engineering {3.2} (4.5–6 MW).
Also, it is used to supply auxiliary needs of gas industry as well as for pipeline
transportation of oil gas {5.1, 5.2} (1.7–10 MW). DG of capacity ranging from 6 to
18 MW is used for modern residential and ofﬁce buildings energy supply {4}.
Low-power DG supplies the instrument-making industry {3.1}, chemical and
production of innovative materials {6.1} (1–2 MW). DG is popular for power
supply of transport and logistics centers and bases {7}, light industry {11} (0.5–
1.5 MW). There is a potential for DG in the region to generate energy by waste
processing {10}.
DG in the Sverdlovsk region is concentrated mostly in densely populated areas,
especially in the industrial urban areas with a lot of companies, like Yekaterinburg
city (the region center) and satellite towns. The positive dynamics can be expected
from districts close to Yekaterinburg due to the suburbanization of economic
activity.
In terms of technology, agriculture is convenient for DG application because of
the smooth load curve and the inertia of the main production processes. DG is
required by livestock farms, poultry farms, dairy, meat and bread factories {9.1}
(1–4 MW), greenhouse farms {9.2} (4.5–9 MW). In this regard, a positive DG
trend can be expected in agro-industrial areas.
112
V. Samoylenko et al.

The development of DG, using local fuels is possible due to the availability of an
integrated production cycle, such as waste or by-products of primary production
suitable as a fuel (sawdust, peat) and the use of a gasiﬁer for syngas {8.1, 8.2} (0.5–
1 MW). The payback period is acceptable for such social projects for the devel-
opment of rural areas. In addition, unlike the grid to be serviced and the plants to be
supplied with diesel power, such DG is almost autonomous.
The map is presented in Fig. 3. The boundaries of the DG and consumer groups
are painted in accordance with Table 1.
Fig. 3 A forecast DG map for Sverdlovsk region
On Quantiﬁcation of the Hidden Distributed Generation …
113

4
Conclusions
Geographical mapping is known to be an effective tool for decision making in
power industry, especially regarding power generation siting and sizing tasks.
Provided methodology provides more precise DG installed capacity assessment and
Table 1 The groups of consumers using DG
Mark
DG power (MW)
DG purpose
№
Color
Electricity
Heat
1. Mining industry
1
16–25
16–50
Minerals mining
2. Metallurgy
2.1
4.5–6
4.5–10
Metal working
2.2
14–22
14–45
Ferrous and non-ferrous metallurgy
3. Machinery construction
3.1
1–2
1–2
Machinery, instrument-making industry
3.2
4.5–6
4.5–12
Complex engineering industry
3.3
10–20
10–30
Heavy engineering industry
4. Power industry
4
6–18
6–36
Residential and ofﬁce energy supply
5. Gas industry
5.1
1.7–4
1.7–5
Gas industry auxiliary
5.2
4–10
4–15
Gas compressors
6. Materials production and construction industry
6.1
0.5–1
0.5–1
Chemical materials
6.2
4–12
4–24
Decoration materials
6.3
16–25
16–50
Building materials
7. Transport and logistics
7
0.5–2.5
1–15
Transport and logistics hubs and bases
8. Dendrochemistry
8.1
0.5–1*
0.5–1*
Woodworking
8.2
1–2
1–4
Resin industry
9. Agrarian industry
9.1
1–4
1–8
Poultry, meat, dairy, bread-baking plants
9.2
4.5–9
4.5–18
Greenhouses
10. Waste utilization
10
0.5–1
0.5–1
Waste processing and utilization
11. Light industry
11
0.5–2
1–2
Light industry
114
V. Samoylenko et al.

gives the possibility to perform approximate perspective DG forecasting. As far as
DG development is inﬂuenced by different factors simultaneously, technical and
economic criteria are to be taken into account. DG business-success statistical
analysis gave us the possibility to obtain typical ranges of DG installed capacities
according to the sphere of its application.
For a given case study, the share of distributed generation in 2020 perspective
planning is expected to be about 6% of total power generation capacities. DG covers
4.9% of power balance against 1.2–2.0% estimated for earlier periods. About 80%
of DG is used by small and medium-sized business companies, 20%—large com-
panies. The most expected spheres of DG development and the corresponding
expected DG installed capacities are directly correlated to economic performance of
the district under consideration. In terms of Sverdlovsk region, which was taken as
a case study, the leading spheres of industry are metallurgy and transport with DG
capacity ranges 4.0–45.0 and 1.0–2.0, correspondingly. Almost all the DGs use gas
as a fuel. This is due to a wide range of power generation based on gas, easy-to use
gas fuel infrastructure. However, provided methodology can be expanded for
renewable energy source application in case energy source potential is adequately
assessed.
Acknowledgements The work was supported by Act 211 Government of the Russian Federation,
contract №02.A03.21.0006 and the Ministry of Education and Science of the Russian Federation
(in the framework of state assignment, №13.1928.2014/K (project №1928)).
References
1. Heating schemes of Yekaterinburg City municipality up to 2030. Management of housing and
public utilities. Yekaterinburg City informational portal (2015) [in Russian]
2. The scheme of Yekaterinburg City municipality power supply up to 2025. Yekaterinburg City
informational portal (2015) [in Russian]
3. Chicco, G., Mancarella, P.: Environmental sustainability of distributed cogeneration systems.
In: Proceedings of the 14th IEEE Mediterranean Electrotechnical Conference, 2008.
MELECON 2008, pp. 514–519. IEEE, Ajaccio (2008)
4. Schellong, W., Schmidla, T.: Optimization of distributed cogeneration systems. In: Proceed-
ings of the 2013 IEEE International Conference on Industrial Technology (ICIT), pp. 879–884.
IEEE, Cape Town (2013)
5. Samoylenko, V.O., Eroshenko, S.A.: The forecast of DG development features by methods of
economic and geographical zoning on the example of Sverdlovsk region. The materials of
public periodic seminar. The issues of distributed generation interconnection and operation.
Russian National Committee of CIGRE (2014) [in Russian]. http://cigre.ru/activity/conference/
seminar_c6/materials/
6. Khalyasmaa, A.I., Dmitriev, S.A. Expert system for engineering assets’ management of utility
companies. In: Proceedings—SDEMPED 2015: IEEE 10th International Symposium on
Diagnostics for Electrical Machines, Power Electronics and Drives, pp. 421–427 (2015)
On Quantiﬁcation of the Hidden Distributed Generation …
115

Modeling the Operating Costs
for Production of the Hydrolyzate
Hana Vaskova and Karel Kolomaznik
Abstract Nowadays, the waste management is an important issue. In this paper a
mathematic modeling of the main operating costs for production of collagen
hydrolyzate from leather shavings is presented. The model is based on physical and
chemical ongoing processes in the reactor and other parts of the production system,
such as kinetics of the hydrolysis, mass and dry matter balances and protein balance
in equilibrium. The aim is to determine optimal time of hydrolysis reaction, optimal
mass fraction of decomposed protein and hence the main operating costs. The
modeling is performed for a batch reactor and pilot plant conditions.
Keywords Collagen hydrolyzate ⋅Modeling ⋅Operating costs ⋅Protein ⋅
Waste
1
Introduction
Leather industry contributes signiﬁcantly to the production of several types of waste
and pollutes the environment in the long-term. In the production of leather the
utilization of a feedstock, the raw hides, is only about 20%. The rest consists of
solid wastes and waste water containing in particular chromium, since the chrome
tanning is still the most common method of tanning globally. Only in tanneries
million tons of waste is generated annually [1].
Processes based on hydrolysis reaction offer a reasonable solution for the pro-
cessing of certain part of these wastes. The ways for reduction of the amount of
waste are needful, otherwise, waste end up in landﬁlls or incinerators, what brings a
risk of leakage into the soil and groundwater or pollution of the air [2]. However,
necessary condition for implementation of new technologies for waste treatment is
their economic aspect. In terms of processing a waste generated from the leather
H. Vaskova (✉) ⋅K. Kolomaznik
Faculty of Applied Informatics, Tomas Bata University in Zlin,
Nad Stranemi 4511, 76005 Zlín, Czech Republic
e-mail: vaskova@fai.utb.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_14
117

industry, one option consists in separating collagen protein (or its processed form)
and its further utilization.
Mathematical modeling has gained a great importance recently for the analysis
and prediction of behavior of processes and phenomena in various areas. Modeling
enables to penetrate through the analysis into a nature of a problem and to obtain
useful information, deeper, more thorough understanding, to inﬂuence real pro-
cesses and to prepare the way for better, optimized design or control of the modeled
system.
2
Treatment of Leather Waste
Hydrolysis, as a decomposition reaction is signiﬁcantly used in technological
processes for the purpose of fermentation of matrices (e.g. leather wastes) for
subsequent analysis of the studied material. Hydrolysis of collagen material in
alkaline or acidic medium is used also in a complex dechromation process of leather
industry wastes, such as shavings. One component—chromium or protein is sub-
jected to liquefaction. In this study, the protein is liquiﬁed and after further treat-
ment can be effectively utilized in forms of gelatin or protein hydrolyzates.
2.1
The Dechromation Process
The complex processing of leather shavings can be, in simpliﬁed way, divided into
three stages as follows.
Collagen protein is liqueﬁed and separated from the chromium sludge in the ﬁrst
stage. Chromium sludge is subjected to revitalization for obtaining chromium in a
form of salts for further leather tanning [3]. One part of resulting gelatinous protein,
a high-quality gelatin, ﬁnds cost-effective usage and applications in pharmacy, food
industry or cosmetics. The rest of gelatinous protein is further processed. Enzyme
activity leads in alkaline conditions to molar mass decrease of gelatinous hydro-
lyzate. Further splitting of protein chain can continue by the act of acid. The
hydrolyzate with lower molar mass is acquired subsequently and can ﬁnd appli-
cation as biostimulator in agriculture [4].
2.2
The Production System
The production system for producing of the hydrolysate is composed of three
fundamental elements: the reactor, the ﬁlter and the vacuum evaporator. The sim-
pliﬁed diagram is shown in Fig. 1 [5]. Feedstocks in the reactor are leather waste,
water and an alkali. Together they create a reaction mixture. When the desired
118
H. Vaskova and K. Kolomaznik

degree of hydrolysis is reached, hot heterogeneous mixture is ﬁltered and the
resulting ﬁltrate is transferred to an evaporator, where the concentration of
hydrolysed substance is increased.
3
Mathematical Model
The main operating costs for production of the hydrolyzate are expressed as a
function of N, see Eq. (1). The function of N is considered for a batch reactor.
N = PtKE + mWHKP + Q̇ZtKP,
ð1Þ
where ﬁrst term represents the cost of power consumption to drive the stirrer, the
second term describes the cost of the desired product concentration and the third
term is heat loss. Physical quantities and constants occurring in the mathematical
model are listed in Table 1.
A function of n is considered as a target function. The function corresponds to
the operating costs for production of hydrolyzate relating to 1 kg of product con-
taining 30% of dry matter. The function of n is
n = N
mD
= PtKE + mWHKP + Q̇ZtKP
mD
,
ð2Þ
where mD is a mass of the product—the hydrolyzate of collagen protein. The value
of mD is calculated from mass balances and dry matter balances of the reactor,
Fig. 1 Diagram of the production system for a production of gelatinous hydrolyzate
Modeling the Operating Costs for Production of the Hydrolyzate
119

the ﬁlter and the evaporator. The balances describe the physicochemical processes
occurring in reactor and other parts of the production system.
The speciﬁc expression of function of n is based on the kinetics of hydrolysis in
the reactor to determine the optimal time of reaction, on the mass balance of protein
in a steady state and on the balance of the evaporator. Assuming, the hydrolysis in
the reactor proceeds according to ﬁrst-order kinetics, i.e. the reaction rate depends
on the concentration of the unreacted protein, the reaction is described by differ-
ential equation
daE
dt = k aR −aE
ð
Þ.
ð3Þ
A solution of the Eq. (3) gives the information about time t of the reaction
t = −1
k lnðaR −aE
aR
Þ.
ð4Þ
Considering the balance of the evaporator to determine the mass of evaporated
water and assuming that the mass fraction of dry matter in the ﬁltrate corresponds to
the proportion of decomposed protein, the function of n is
n = −PKE + Q̇ZKP
kmD
lnð1 −aE
aR
Þ + HKPðaD
aE
−1Þ.
ð5Þ
Table 1 Physical quantities and assignments for the mathematical model
Symbol
Physical quantity
Value [Unit]
P
Electric power of stirrer of hydrolysis reactor
104 [W]
t
Time
– [s]
KE
Price of electricity
1.25. 10−6 [CZK/Ws] (∼4.62 ×
10−8 EUR/Ws)
mW
Mass of evaporated water
– [kg]
H
Speciﬁc latent heat of vaporization
2.3 × 106 [J/kg]
KP
Prize of steam
0.6. 10−6 [CZK/Ws] (∼2.2 × 10−8
EUR/Ws)
Q̇Z
Loss of heat ﬂow through a heat transfer area
of the reactor
2.4 × 105 [W]
mD
Mass of hydrolyzate
2 × 103 [kg]
aE
Mass fraction of decomposed proteins in
hydrolyzate
– [l]
k
Velocity constant of hydrolysis reaction
1.7 × 10−4 [s−1]
aR
Mass fraction of protein in ﬁltrate at
equilibrium
– [l]
aD
Final mass fraction of hydrolyzate
0.3 [l]
120
H. Vaskova and K. Kolomaznik

For minimizing operating costs, it is necessary to determine the extreme of the
function of n. The concentration aE is then given by the expression (6)
aE =
ﬃﬃﬃﬃﬃﬃﬃﬃ
βaD
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
βaD + 4αaR
p
−βaD
2α
,
ð6Þ
where the coefﬁcients α and β are given by (7) and (8)
α = PKE + Q̇ZKP
kmD
,
ð7Þ
β = HKP.
ð8Þ
Substituting (6) into the Eq. (4) we obtain the expression for the optimal time
t = −1
k lnð1 −
ﬃﬃﬃﬃﬃﬃﬃﬃ
βaD
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
βaD + 4αaR
p
−βaD
2αaR
Þ.
ð9Þ
3.1
Optimum Values of Variables
Modeling is performed for a batch reactor usable for pilot plant experiments, i.e.
when processing materials on the order of tons. The optimal values of the search
parameters t, aE and aR are obtained after substituting the known parameters from
Table 1. Optimal time for the reaction based on the calculation is 5 h and 10 min,
the optimal mass fraction of decomposed protein is 0.0386 with the mass fraction of
protein in ﬁltrate in equilibrium 0.0403. The target function of n was modeled in
Wolfram Mathematica software. The dependence of the operating costs on the
concentration aE is shown in Fig. 2.
Fig. 2 The dependence of
the operating costs on the
concentration of collagen
hydrolyzate aE
Modeling the Operating Costs for Production of the Hydrolyzate
121

The current price related to 1 kg of collagen hydrolyzate with optimal mass
fraction aE is calculated on 10.60 CZK (∼0.39 EUR). Currently, a number of
products for nutrition and joints support (a collagen hydrolyzate) available on
markets, are sold from about 250–300 CZK/kg (∼9.2–11 EUR/kg).
The steep decline to an optimum value of aE and even sharper rise of the
dependence clearly demonstrate sense of mathematical-physical modeling and
contribution to the economic evaluation of the collagen hydrolyzate production
process.
4
Conclusion
Mathematical model of main operating costs for the collagen hydrolyzate acquired
from leather waste was presented in this paper. Hydrolysis reaction seems to be
reasonable solution for safe and environmentally friendly method for reduction of
waste gained from leather manufacturing sector. This method offers also econom-
ically effective way of waste reduction as the waste (e.g. shavings) still contains
material usable in a form of gelatin or collagen hydrolyzate. Both of these forms are
economically interesting. Reaching the optimal time of the reaction evaluated from
the presented model brings another savings especially in energy required to operate
the production system.
Acknowledgements This work was supported by the Ministry of Education, Youth and Sports of
the
Czech
Republic
within
the
National
Sustainability
Program
project
No.
LO1303
(MSMT-7778/2014).
References
1. World Statistical Compendium for raw hides and skins, leather and leather footwear
1993-2012. http://www.fao.org/ﬁleadmin/templates/est/MARKETS_MONITORING/HidesSkins/
documents/COMPENDIUM2013.pdf
2. Janacova, D., Vasek, V., Kolomaznik, K.: Optimalizace recyklačních technologií. In:
Automatizace, regulace a procesy, pp. 55–66 (2010)
3. Kolomaznik, K., Mladek, M., Langmaier, F., Janacova, D., Taylor, M.M.: Experience in
industrial practice of enzymatic de-chromation of chrome shavings. J. Am. Leather Chem.
Assoc. 94, 55–63 (2000)
4. Pecha, J., Fürst, T., Kolomazník, K., Friebrová, V., Svoboda, P.: Protein biostimulant foliar
uptake modeling: the impact of climatic conditions. AIChE J. 58, 2010–2019 (2012)
5. Vaskova, H.: Modelovani chemickeho reaktoru pro dechromaci kozedelnych odpadu.
Dissertation, Tomas Bata Univerzity in Zlin (2015)
122
H. Vaskova and K. Kolomaznik

The Problems of Data Security
in Cloud Computing and Its Solution
Using Petri Nets
Zoltán Balogh and Martin Magdin
Abstract Cloud is understood as a storage which provides space for data storage.
The concept of Cloud and Cloud computing has a wider range. It can be charac-
terized as providing services and various applications which are saved on servers
while there is access to them through Internet. It is possible to work with data in
cloud not only online mode but also in ofﬂine—it means to use the state of
short-term connection and thus save data in mobile connection. Provided that the
service is paid, the advantage of it is that the users do not pay for their software but
for its use. Scientiﬁc publications state that the most often ﬂaw of the service is its
vulnerability in the form of data security. It is the reason why nowadays, all the
attention is paid to this problematic ﬁeld. In the report a data security design in
Cloud computing will be described and also all the results will be stated that have
been acquired through experiments to verify the overall security of a chosen cloud
storage. According to measurements of chosen parameters of a basic and an
extended cloud storage model, it has been found out that the security increase of
data stored in cloud storage brings also worsening of other parameters of the
storage. This is particularly the case of the access speed to data that (if they are
automatically encrypted) can be inﬂuenced by the computing power of a particular
PC hardware.
Keywords Petri nets ⋅Cloud computing ⋅Security ⋅Cloud storage security
Z. Balogh (✉) ⋅M. Magdin
Faculty of Natural Sciences, Department of Computer Science, Constantine
the Philosopher University in Nitra, Tr. A. Hlinku 1, 949 74 Nitra, Slovakia
e-mail: zbalogh@ukf.sk
M. Magdin
e-mail: mmagdin@ukf.sk
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_15
123

1
Introduction
The concept of cloud is now a highly inﬂected term. Some experts explain this concept
as a dominant computing model in IT infrastructures, enabling ﬂexible, ubiquitous,
on-demand and cost-effective access to a wide pool of shared resources [4].
Cloud Computing has emerged from the most promising business concept to
new fast growing business trend. Cloud Computing provides the ability to utilize
resources from distributed computing environments via Internet. Cloud Computing
is the promising hosting platform that allows the resources and collection of
applications usage in the shared infrastructure with pools of computers and storage
resources [17]. The proliferation of the cloud paradigm has created a strong trend to
transfer traditional services and applications to the cloud [18]. With the ubiquitous
nature of social networks and cloud computing, we are starting to explore a new
way to interact with and exploit these developing paradigms [11]. The gradual
development of the Internet infrastructure and improving the various providers of
services allow to accept the cloud paradigm and enjoying the following two key
offerings [1, 6]:
(a) low costs from the point of view of invest on hardware or software,
(b) reduction of costs from the point of management of complex IT systems.
These two economic factors they are economy of scale enables additional
decrease in total costs, as numerous small-scale and typically underutilized data
centers are replaced by larger infrastructures that target higher resource efﬁciency [5].
With the rapid shift to the cloud computing paradigm, one of the most critical
issues faced by system administrators is the construction and management of
systems in a manner that eliminates or hides their complexity from the end users,
and, at the same time, maintain their control, ﬂexibility, dependability, and security
[9]. Next generation cloud systems will require a paradigm shift in how they are
constructed and managed. Conventional control and management platforms are
facing considerable challenges regarding ﬂexibility, dependability and security that
next generation systems must handle [10].
Security, trust, and privacy always remain challenges for organizations that
adopt cloud computing and big data. Although there are demands for businesses to
move their data to the cloud and centralize management for data centers, services
and applications are designed to reduce cost and increase operational efﬁciency.
System design and deployment based on current security practices should be
simultaneously enforced to ensure compliance of all data and services with
up-to-date patches and policies. A risk-based approach to the development of a
security program that recognizes (and funds) appropriate controls will ensure
protection of all users and conﬁdentiality, integrity, and availability of data [7].
Security is one of the main attributes of information technology. By access to
cloud services and data through the Internet, the importance of security is increased
because data is at bigger amount of security risk. Within various cloud service
models, data responsibility is shared by more groups. Data is the most important for
124
Z. Balogh and M. Magdin

the organization which owns them and uses the cloud services for storage. That is
the reason why the organization should be aware of all risks which exist for data
stored in cloud.
From the year 2013 was created many architectures that directly support data
security in cloud repositories. Most often we meet with the protection on the basis
of software solution.
By Mukundha [15] we can observed the fast change in the cloud network by the
Software Deﬁned Networking (SDN) paradigm that differentiate the control plane
from the data plane to give the ﬂexibility for programmability and centralized
control of the cloud networks, SDN networks not only provide simpliﬁcation of
cloud network management it also provides more security with SDN by imple-
menting ﬁrewalls with in the SDNs (Fig. 1).
The security of cloud repositories deserves our special attention for the chal-
lenges it brings and also the opportunities to enhance the network security. In the
paper Modeling of Data Security in Cloud Computing (Balogh and Turčáni [3];
paper was published in 10th Annual IEEE International Systems Conference; http://
2016.ieeesyscon.org/) we have designed the way of modeling of data security in
cloud repositories. In the next section is short explanation of our proposal.
2
Data Security Modeling in Cloud Computing
Data in cloud computing is stored, transmitted and there is access to it through
various layers of cloud computing in order to all data security attributes to be
guaranteed such as conﬁdentiality, integrity and availability. It is necessary to deal
with data security in all layers of cloud computing. Therefore we designed
expanded fourth-level model of data security in cloud computing (standard cloud
storage uses three-level data security model) (Fig. 2).
Fig. 1 Security SDN architecture with ﬁrewall in cloud networks [15]
The Problems of Data Security in Cloud Computing …
125

Fig. 2 Expanded data security model in cloud computing (our creation)
Hardware1
OS 1
ObjectData Storage
Sync
Mobileclient
Browser
WebAPI
Hardware2
OS 2
Hardwaren
OS n
CloudFiles Systems
Authentication
Authentication
PIN
GENERATOR
SMS/EMAIL
Authentication
Server
Secure Data
Flow
ClientEncryption Application
Security
Message
Flow
Security
Message
Flow
Third
Party
Auditor
Server
Fig. 3 Cloud storage model design with expanded security (our creation)
126
Z. Balogh and M. Magdin

Data security model in cloud computing was designed as an expansion of the
current standard use of data storage model in cloud computing. The standard model
is used for example in Dropbox, Seaﬁle or in other services to store or to work with.
The base is an object data storage which was built on a server cluster creating n
servers. Above the object storage there was the cloud ﬁle system which was pro-
vided by the application server (Dropbox, Seaﬁle). In the following ﬁgure is an
architecture model design of a secure data storage in cloud computing (Fig. 3).
To demonstrate architecture model operation of cloud storage with expanded
security, Petri nets have been created, shown in Fig. 4. Petri nets are a mathematical
means appropriate to model system, composing of parallel components in mutual
interaction. By the use of Petri nets, it would be easier to discreetly simulate and
also to understand how the security model operates. Then each step that has been
made while securing data in cloud storage would be easily described. The deﬁnition
of Petri nets according to [2]:
Fig. 4 Petri nets model for data security in cloud computing (our creation)
The Problems of Data Security in Cloud Computing …
127

Petri nets is a trio N = (P, T, F) where P ≠0 is a ﬁnite set of places, T ≠0 is a
ﬁnite set of transitions and F is transition ﬂoat.
F: P × T
ð
ÞU T × P
ð
Þ →N0
With this designed model of a complex data security in cloud computing has
adequately increased data security in all three attributes of data security which are
conﬁdentiality, integrity and availability. In the next section is analysis of the
likelihood of overcoming the security of cloud repositories and experiment
describing the creation of cloud repositories and increasing its security.
3
An Experiment of Creating a Cloud Storage
and Increasing Its Security
In the previous part, there was a designed and described model of extended data
security in cloud storage. In this section, with an experiment the standard model
would be compared with a model of extended security, a two-level authentication
and data encryption in peace. The parameters for comparing the two models were
gained based on experience of 7 years from the company LAN Systems Ltd. Those
have been chosen which are considered to be the most important for the majority of
users:
• the speed of setting up the data cloud storage,
• the speed of data synchronization between the client and cloud storage,
• costs of data storage.
For the experiment we have chosen a hardware and a software of the cloud
storage provider which belong to the most spread among the cloud storage users
according to the experiences of the company LAN Systems Ltd.
The cloud storage has been chosen from the company Dropbox (https://www.
dropbox.com) because it is the most spread. For data deciphering in peace, the
application Boxcryptor (https://boxcryptor.com) has been chosen because it is
possible to implement it relatively quickly.
3.1
The Basic Parameters of the Hardware and Software
for the Experiment
Hardware of the computer on which the Dropbox client has been installed:
CPU: Intel Core i3-2100 3,1 Ghz, RAM: 8 GB, HDD: SSD HDD, OS:
Microsoft Windows 7 Professional Service Pack 1 64 bit, The speed of Internet
connection: 100 Mbps, Cloud storage: Dropbox with a size of 50 GB.
128
Z. Balogh and M. Magdin

3.2
Standard Security Model Description of Dropbox Cloud
Storage
Authentication: one-level with name and password [8].
Data protection while transfer: SSL a AES-256 bit [8].
Data protection in peace: ciphering on Dropbox server with encryption method
AES-256 bit with the access permission of the chosen workers of the company
Dropbox. The data owner does not have access to the list of the third persons who
have access to his data. The cipher key was saved on Dropbox servers and the data
owner does not have access to them [8].
Quick data restore: Dropbox cloud storage saves all edits and deletions up to
30 days. Thus, it is possible to restore all the ﬁles up to 30 days that have been
deleted. It is also possible to restore the previous versions of ﬁles which are not
older than 30 days [8]
Data integrity check: is not implemented.
3.3
Extended Security Model Description of Dropbox Cloud
Storage
Authentication: a two-step authentication is turned on:
1. Step: name and password
2. Step: A safe code from the phone, sent as a message or generated by a phone
application [8]
Data protection while transfer: SSL a AES-256 bit [8]
Data protection in peace:
1. Level: ciphering on Dropbox server with encryption method AES-256 bit with
the access permission of the chosen workers of the company Dropbox. The data
owner does not have access to the list of the third persons who have access to his
data. The cipher key was saved on Dropbox servers and the data owner does not
have access to them [8].
2. Level: encryption on a client’s device with the application Boxcryptor that uses
encryption AES and RSA. The cipher key is saved on the client’s device and the
provider of Dropbox cloud service has no access [16].
Quick data restore: Dropbox cloud storage saves all edits and deletions up to
30 days. Thus, it is possible to restore all the ﬁles up to 30 days that have been
deleted. It is also possible to restore the previous versions of ﬁles which are not
older than 30 days [8].
The Problems of Data Security in Cloud Computing …
129

3.4
Measurement Description of the Chosen Cloud Storage
Parameters
The ﬁrst step was to create a cloud storage with a standard security that had four
operations: the creation of Dropbox account, download and installation the clients
Dropbox into the client’s device, authentication of the clients Dropbox on the
client’s device and ﬁnally, its conﬁguration on the device.
The next step was a creation of a cloud storage with an extended security but
which contained 10 following operations:
1. Creation of Dropbox account.
2. Turning on the second step of authentication.
3. Downloading and installation of client’s Dropbox into the client’s device.
4. The ﬁrst step of authentication of client’s Dropbox on a cloud storage.
5. The second step of authentication.
6. Conﬁguration of client’s Dropbox on the client’s device.
7. Registration of Boxcryptor account.
8. Downloading and installation of client’s Boxcryptor on a client’s device.
9. Authentication of client’s Boxcryptor into the Boxcryptor server.
10. Conﬁguration of the client’s Boxcryptor on a client’s device.
The third step was to measure the synchronization time of testing data for both
variants of cloud storage security. Within this measurement testing data synchro-
nization speed were measured in sizes of 100, 200, 300, 400, 500, 1000, 1500,
2000, 3000 and 4000 MB. The data were created from the shared data of the LAN
Systems Ltd. in which there were various types of ﬁles pdf, doc, png, txt, rar. The
production data were used in order to get closer to the environment within the
experiment in which the cloud storage is used in production [14].
The last step was calculation of monthly expenses on one user for both variants
of cloud storage security.
4
The Results of the Experiment
4.1
Time Measurement of the Account in Cloud Storage
Creating an account in the Dropbox cloud storage with account registration,
installation and setting the client in the standard security model lasted 6 min and
30 s. After the extended security model (that has been designed by us) with
two-level authentication and data encryption in peace, the time of installation has
increased to 13 min and 15 s because an account was needed to be created, and the
encryption application Boxcryptor was needed to be installed t set. In the following
130
Z. Balogh and M. Magdin

chart, the measured values were stated and there is the graphical difference of times
in Fig. 5 (Table 1).
4.2
Time Measurement of Testing Data Synchronization
When measuring time of synchronization of various testing data sizes between the
cloud storage and its client it has been found out that there are signiﬁcant differ-
ences between the synchronization time of the standard data security model and the
extended one. In data encryption in peace, the data synchronization time increases
several times between the cloud storage and its client.
After the measurements in the experiment, it has been found out that synchro-
nized data were saved more slowly into the cloud ﬁle of the client and conse-
quently, they synchronize more slowly with the cloud storage. The slow storage of
testing data into the ﬁle was caused by the fact that data were encrypted before
storing them into this ﬁle. Since the encryption is power consuming for the CPU,
additional measurements were completed within the experiment in order to ﬁnd out
in what extent the CPU inﬂuences the synchronization time of testing data with
cloud storage (Table 2).
Fig. 5 The installation time
of one account for one user
Table 1 Time measurement
of account for one user in
cloud storage
Time of installation
(minutes: seconds)
Standard model
6:30
Extended model
13:15
Table 2 Time measurements of testing data synchronization (Mb/s)
Model
100
MB
200
MB
300
MB
400
MB
500
MB
1000
MB
1500
MB
2000
MB
3000
MB
4000
MB
Standard
model (s)
31
48
72
85
105
120
218
260
356
413
Extended
model (s)
252
622
840
1170
1436
1848
3348
4154
5850
7080
The Problems of Data Security in Cloud Computing …
131

While measuring the speed of data synchronization encrypted in peace it has
been observed that the synchronization of testing data (of already encrypted data)
itself took signiﬁcantly longer. The synchronization increase of encrypted data in
peace with cloud storage has been caused by low effectiveness of encrypted data
compression for data transfer by computer network [13] (Fig. 6).
4.3
Additional Measurements of Sequential Read and Write
on PC with Various Computing Power
To ﬁnd out how a PC hardware power inﬂuences the speed of synchronization and
the work with testing data in cloud storage, speed measurements of sequential read
and write into cloud ﬁle on a PC with various computing power have been
completed.
The measurements were completed into cloud ﬁle in the standard data security
model where there were no data encryptions in peace completed, and in the
extended data security model where the data encryption in peace took place by the
application Boxcryptor (Table 3).
The power of CPU of each PC is according to source PASSMARK 2015.
Fig. 6 Synchronization time of client’s cloud with cloud storage
Table 3 Hardware of each testing computers
PC1
PC2
PC3
PC4
CPU
Intel Pentium Dual
Core T4300 2,1 Ghz
Intel Pentium
3558 1,7 Ghz
Intel Core
i3-2100 3,1 Ghz
AMD Phenom 2
X4 965 3,4 Ghz
RAM
3 GB
4 GB
8 GB
8 GB
HDD
2,5” HDD
5400 rpm
SSD
OCZ ARC
100
RAID10 HDD
7200 rpm SATA
SSD Intel 720
CPU power
PassMark
1246
1765
3612
4267
132
Z. Balogh and M. Magdin

In Table 4, measurement results of sequential read on a PC with various com-
puting power are stated. Then in Table 5, measurement results of sequential data
write on a PC with various computing power.
4.4
Calculation of Monthly Cost for One User of Cloud
Storage
In Table 6 there are monthly costs of one user on standard and extended security
model. The differences on monthly costs on one user have been presented in a
diagram on Fig. 7.
Table 4 Measurements of sequential read (Mb/s) on various PC
PC1 (Mb/s)
PC2 (Mb/s)
PC3 (Mb/s)
PC4 (Mb/s)
Standard model
53,35
418
216,51
249,25
Extended model
19,66
26,57
46,45
47,15
Table 5 Measurements of sequential write (Mb/s) on various PC
PC1 (Mb/s)
PC2 (Mb/s)
PC3 (Mb/s)
PC4 (Mb/s)
Standard model
24,59
364,2
132,7
132,04
Extended model
19,5
27,33
38
47,4
Table 6 Monthly costs on
one user of cloud storage
(EUR)
EUR
Standard model
12
Extended model
18
Fig. 7 Monthly costs on one
user
The Problems of Data Security in Cloud Computing …
133

5
Conclusion
According to measurements of chosen parameters of a standard and an extended
cloud storage model, it has been found out that the security increase of data stored
in cloud storage brings also worsening of other parameters of the storage. The time
of account creation for one user has increased twice on average and the monthly
costs have increased by one third on average. In absolute numbers and small
number of users, it is not big increase of costs and time, but in a bigger number of
users, there is a bigger increase and it needs to be taken into account when
designing the security of a cloud storage. The most signiﬁcant difference between
the standard and the extended model of data storage in cloud computing is the data
synchronization time with cloud storage. With an additional measurement access
speed to data, it can be observed that there was a considerable difference in access
speed to data that have not been encrypted in peace and to data that have been
encrypted in peace. The speed of access to data is inﬂuenced by the computing
power of the PC’s hardware in automatically encrypted data in peace. The access
deceleration to data when encrypting in peace was caused by the fact that the data
are decrypted and consequently encrypted while accessing them. Then deceleration
of data synchronization with cloud storage when encrypting data in peace was
caused by a low effectiveness for encrypted data when transferring them through a
computer network. Data storage with the possibility of encryption in data storage is
currently a trend that is justiﬁed. A typical example of the use of encrypted data
storage is storing large amounts of data (example: Comparing individual facial
features—see Magdin, Turcani, Hudec), which is achieved by datamining [12].
According to this experiment it can be stated that when designing the cloud
storage model, it is recommended for cloud storage users to determine before
designing what requirements they do have for securing their own data. Alterna-
tively, they can divide data into groups with different level of required security.
Acknowledgements This publication is supported thanks to the project with a name “Pilot
project with UKF Nitra university: Modelling the behaviour of users based on data mining with
support of IBM Bluemix”.
References
1. Armbrust, M., Fox, A., Grifﬁth, R., Joseph, A.D., Katz, R., Konwinski, A., Zaharia, M.: A
view of cloud computing. Commun. ACM 53(4), 50–58 (2010). doi:10.1145/1721654.
1721672
2. Balogh, Z., Turčáni, M., Magdin, M.: Design and creation of a universal model of educational
process with the support of petri nets. Lecture Notes in Electrical Engineering, vol. 269,
pp. 1049–1060 (2014). doi:10.1007/978-94-007-7618-0_103
3. Balogh, Z., Turčáni, M.: Modeling of data security in cloud computing. In: 10th Annual
International Systems Conference, SysCon 2016. Institute of Electrical and Electronics
Engineers Inc., pp. 940–946 (2016)
134
Z. Balogh and M. Magdin

4. Barroso, L.A., Clidaras, J., Hölzle, U.: The datacenter as a computer: an introduction to the
design of warehouse-scale machines. Synth. Lect. Comput. Archit. 2013(24), 1–156 (2013)
5. Beloglazov, A., Abawajy, J., Buyya, R.: Energy-aware resource allocation heuristics for
efﬁcient management of data centers for cloud computing. Future Gener. Comput. Syst. 28(5),
755–768 (2012). doi:10.1016/j.future.2011.04.017
6. Buyya, R., Yeo, C.S., Venugopal, S., Broberg, J., Brandic, I.: Cloud computing and emerging
IT platforms: vision, hype, and reality for delivering computing as the 5th utility. Future
Gener. Comput. Syst. 25(6), 599–616 (2009). doi:10.1016/j.future.2008.12.001
7. Chang, V., Kuo, Y., Ramachandran, M.: Cloud computing adoption framework: a security
framework for business clouds. Future Gener. Comput. Syst. 57, 24–41 (2016). doi:10.1016/j.
future.2015.09.031
8. Dropbox Inc.: How secure is Dropbox? (2015). https://www.dropbox.com/help/27
9. Hariri, S., Khargharia, B., Chen, H., Yang, J., Zhang, Y., Parashar, M., Liu, H.: The
autonomic computing paradigm. Cluster Comput. 9(1), 5–17 (2006). doi:10.1007/s10586-
006-4893-0
10. Jararweh, Y., Al-Ayyoub, M., Darabseh, A., Benkhelifa, E., Vouk, M., Rindos, A.: Software
deﬁned cloud: survey, system and evaluation. Future Gener. Comput. Syst. 58, 56–74 (2016).
doi:10.1016/j.future.2015.10.015
11. Kim, S.: Dynamic social cloud management scheme based on transformable stackelberg
game. Eurasip J. Wirel. Commun. Networking 2016(1), 1–9 (2016)
12. Magdin, M., Turčáni, M., Hudec, M.: Evaluating the emotional state of a user using a
webcam. Int. J. Interact. Multimedia Artif. Intell. 4(1), 61–68 (2016)
13. Mcgregor, J.P., Lee, R.B.: Performance Impact od Data Compression on Virtual Private
Network Transactions (2000). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.
7667&rep=rep1&type=pdf
14. Munk, M., Kapusta, J., Švec, P.: Data preprocessing evaluation for web log mining:
reconstruction of activities of a web visitor. In: 10th International Conference on
Computational Science 2010, ICCS 2010, pp. 2273–2280, Amsterdam (2010)
15. Mukundha, C., Prabha, I., Sreenu, K.: Providing security in cloud based networks through
software deﬁned networks. J. Theor. Appl. Inf. Technol. 84(2), 287–293 (2016)
16. Secomba GmbH: AES and RSA Encryption (2015). https://www.boxcryptor.com/en/
encryption/
17. Sreedhar Acharya, B., Siddappa, M.: Novel method of designing and implementation of
security challenges in data transmission and storage in cloud computing. Int. J. Appl. Eng.
Res. 11(4), 2283–2286 (2016)
18. Wu, J., Liang, Q., Bertino, E.: Improving scalability of software cloud for composite web
services. Paper presented at the CLOUD 2009, 2009 IEEE International Conference on Cloud
Computing, pp. 143–146 (2009). doi:10.1109/CLOUD.2009.75
The Problems of Data Security in Cloud Computing …
135

Designing of the Pseudorandom
Number Generators on the Basis
of Two-Dimensional Cellular Automata
Stepan Bilan, Mykola Bilan, Ruslan Motornyuk, Andrii Bilan
and Sergii Bilan
Abstract In this paper three pseudorandom number generators are considered
which are built on cellular automata. Structures and generator models, and the
organization of a cellular automaton for each generator are studied. Two pseudo-
random number generators are built on the asynchronous cellular automata, and the
third generator is implemented on the synchronous cellular automata. The structure
of cell that realizes the ﬁrst two pseudorandom number generators is described.
There is the analysis of the proposed pseudorandom number generators uses NIST
test and also describes their main characteristics.
Keywords Cellular automata ⋅Pseudorandom number generator ⋅Tests
S. Bilan (✉)
State Economy and Technology University of Transport,
Lukeshevicha, str., 19, Kiev 03049, Ukraine
e-mail: bstepan@ukr.net
M. Bilan
The Municipal Educational Institution Mayakskaya Secondary School,
Mayak, Moldova
e-mail: nickni@mail.ru
R. Motornyuk
Data Processing Center of Southwestern Railways, Kiev, Ukraine
e-mail: hehap0@gmail.com
A. Bilan
Business Soft Ltd., Tiraspol, Moldova
e-mail: hl181582@gmail.com
S. Bilan
Win-Interactive LLC, Vinnytsia, Ukraine
e-mail: belan@svitonline.com
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_16
137

1
Introduction
Formation and obtaining pseudorandom numbers is a necessary operation that is
widely used in different ﬁelds. Today, the need for the pseudorandom number
generators (PRNG) has signiﬁcantly increased. At the same time there is a need in
the PRNG with the certain properties for each problem. These properties satisfy its
positive solution [1, 2]. PRNG are implemented on the basis of the different
mathematical, hardware and software approaches. The PRNG got the widespread
popularity and development, which are implemented on the basis of cellular auto-
mata (CA). The ﬁrst generator that was implemented on the one-dimensional CA has
been proposed by S. Wolfram [3]. PRNGs, which are implemented on the hybrid
CAs were considered in later works [4, 5]. The combination of rules for different CA
cells are used in such generators. There were proposed some developed PRNGs,
which are based on CA, that are implemented with usage of few CAs and additional
generator. Additional generator is built on the linear feedback shift register (LFSR)
[6, 7]. Different approaches are used to analyze the properties of the PRNG. They are
implemented as software products that are available in the Internet in appropriate
sites [8–10]. They include such software as: ENT, DIEHARD, NIST and etc.
The paper analyzes three PRNGs based on the CA and there are the results of
using NIST tests for analysis presented here.
2
PRNG Based on the CA that the Additional Bits Using
The ﬁrst PRNG is implemented on CA in which only one cell (the active cell)
changes its own state at each timestep. In such a CA the neighborhood of cells are
constitute in such a way so that the one active cell could be selected out of the
whole neighborhood of active cell. This cell will be set in active state. Each active
cell has one informational output and N informational inputs, N active inputs and N
active outputs. Behavior of CA can be described by the following model.
bi t + 1
ð
Þ =
f bi
Nj tð Þ
h
i
,
if
∃bi, act
Nj
tð Þ = 1
bi tð Þ,
in other case
(
,
ð1Þ
where bi
Nj tð Þ—signals on the information outputs of the cells that constitute the cell
neighborhood of i-th cell at time t; bi, act
Nj
tð Þ—signal at the j-th activation input of i-th
cell and this signal comes from activation output of the cell that belongs to the cell
neighborhood of i-th cell at time t j = 1, N


; N—the amount of neighbor cells, that
makes the neighborhood of the i-th cell.
The model shows the process of selecting of the active cell in the next
timestep. The activating signal can be transmitted only to one cell of this
138
S. Bilan et al.

neighborhood. At the same time the state of the active cell changes according to the
function f[]. Each cell can hold the main (informational) and additional (active)
states. The pictorial representation of the active signal transmission and the infor-
mation state changes are represented on the Fig. 1.
Bits of the pseudorandom binary sequences are composed from the bits that
come out from activated cells outputs in each timestep. For elimination of the cycles
an additional bit is used which is also taken out of one of the CA cells. The
information state of a cell is formed around the Moore’s neighborhood by the
following model.
bj t + 1
ð
Þ = bj, 1 tð Þ ⊕bj, 2 tð Þ ⊕⋯⊕bj, 8 tð Þ ⊕badd tð Þ ⊕bj tð Þ,
ð2Þ
where bj, 1 tð Þ, . . . , bj, 8 tð Þ—the main informational state of cells, that make the
Moore neighborhood for the cells with active state at time t; badd tð Þ—informational
state of the cell, the value of which is added according to XOR function to the state
of the active cell at time t (additional bit value).
Each cell is connected with neighborhood cells by circuits of activation. The
activating signal is transmitted by these circuits. The active cell analyzes the
information state of neighborhood cells, forms the “1” signal on one of the active
outputs, which is deﬁned by a function of being activated and by the signals of
neighborhood cells.
The cell of neighborhood is selected on the odd timesteps of the generator that is
at the state of logic “1” and has bigger code of numbering among the neighborhood
cells, which have the state of the log. “1”. In the even-numbered timestep the
neighborhood cell of the active cell is selected, it has a state of logic “0” and has
bigger value of numbering among the rest neighborhood cells in the zero state.
PRNG consists of two CA: the basic and additional (КAadd) one. There is the
initial state of CA stored in CAadd. This state is stored during N × M timesteps in
the CAadd (where N × M—the size of two-dimensional CA). Additional bits in
each timestep are selected from CAadd cells. A signal of “1” on the active output of
cell allows to connect its data-output to the output of the generator with the help of
switching system (SC). The example of generator operation is represented on
Fig. 2.
Fig. 1 The example of active signal transmission and the information state changes
Designing of the Pseudorandom Number Generators …
139

The second PRNG constructed on the basis of a single CA. The scanning
process of the current state of the cells CA is implemented in it. It allows reducing
the time wasted for generating pseudorandom sequence. In such a PRNG the cells
functioning in the same way. An example of the PRNG functioning is represented
on Fig. 3.
3
PRNG Based on the CA with Heterogeneous Cells
The generators that are suggested above have a set of structural disadvantages.
These generators uses a complicated switching circuitry for the constant switching
the generator output to the data-output of active cells CA. Both generators have a
large amount of connections, which deteriorate the reliability of functioning.
PRNG that contains the CA with homogeneous and heterogeneous cells is
proposed in this work. Homogeneous cells are called all CA cells, which perform
the same functions, and heterogeneous cells—are the cells that perform functions
different from homogeneous cell function.
At the initial time all the CA cells are set in the state of the logical “1” and “0”,
and all of them perform the same function. They are homogeneous cells. There is a
Fig. 2 An example of the ﬁrst generator functioning
Fig. 3 Example of the PRNG with the inner CA scanning
140
S. Bilan et al.

chosen cell, which date-output is connected to the output of the PRNG. Then, the
few cells are selected among all the cells of CA. These cells will perform another
function. These cells are heterogeneous ones. The XOR function is chosen for
homogeneous cells, which perform this function above the signals of the neigh-
borhood cells and the signal of its own state. Heterogeneous cells perform the
majoritarian function of the signals from neighborhood cells and its own state.
At each timestep the CA state is recorded to the memory unit (MU), and it is also
being compared with all the previous states, which are recorded in the MU. If the
MU meaning does not equal the current state of CA, the comparison unit
(CU) produces a signal to the control input MU and allows recording the current
state of the CA inside of it. If the state of CA equals one of the MU code, CU resets
the MU down to zero and CA changes the coordinates of the heterogeneous cells. In
the next timesteps PRNG works in a similar way. The function of calculating new
coordinates of heterogeneous cells is selected in advance. Figure 4 depicts the
example of PRNG work.
The Fig. 4 shows the work of third PRNG. Formed bits are read from the
data-output of the main cell at each timestep. This PRNG allows to increase sig-
niﬁcantly the repeating period, however, it has a low speed.
4
Analysis of the Quality of the PRNG Work Based on CA
Currently the quality of PRNG is evaluated by means of specially designed tests.
These tests make it possible to determine the generator behavior and to determine
the optimum range of the initial settings. Among all the tests the DIEHARD and
NIST tests have the greatest authority. To use them, you have to create the ﬁles of
appropriate format that store the generated bit sequence. ENT and NIST tests were
used in this paper for the analysis of the proposed PRNG. These tests were used for
the series of bit sequences from each PRNG. They gave positive results for
sequences of different length [11].
There were NIST tests used for the analyses of these generators. And each of the
tests was implemented in a separate program. Each program allows selecting the
necessary parameters to perform the comprehensive analysis of the PRNG
Fig. 4 An example of PRNG operation is based on the CA with heterogeneous cells
Designing of the Pseudorandom Number Generators …
141

behavior. The results of the analyzing the behavior of all three PRNGs are shown in
Table 1. The symbol “+” indicates the positive result, and “−”—the negative one.
If “±” is present in the table cell, this means that there were negative results among
positive ones but these negative results do not exceed the permissible limits.
The second PRNG has the signiﬁcantly greater operating speed because it
spends the least time on the formation of a single bit of sequence. The third PRNG
has poor operating speed performance. However, the third PRNG has the greatest
length of the repeating period. The experiment showed that if the number of
heterogeneous cells is more than 3, then there are not any coincidences of the CA
states. In this case the third PRNG will have the highest speed operation.
Table 1 Results of the analysis of the ﬁrst and second PRNGs (Only the Moore neighborhood)
and third PRNG (Only the Neumann neighborhood)
Name of test
The length of the bit
sequence
Test result
Test results for
given number
of
heterogeneous
cells
PRNG1
PRNG2
PRNG3
1
2
3
4
The Frequency (Monobit) Test
104, 106
+
+
±
+
+
+
Frequency Test within a Block
104, 106
+
+
±
+
+
+
The Runs Test
106
–
+
±
+
+
+
Tests for the
Longest-Run-of-Ones in a Block
750000
+
+
±
+
+
+
128, 6272
+
+
±
+
+
+
The Binary Matrix Rank Test
104–106
–
–
–
–
–
–
The Discrete Fourier Transform
Test
103, 104–106
+
+
±
+
+
+
The Non-overlapping Template
Matching Test
103–106
+
+
±
±
+
+
The Overlapping Template
Matching Test
106
–
–
–
–
–
–
Maurer’s “Universal Statistical”
Test
904960
+
+
±
±
+
+
The Lempel–Ziv Compression
Test
106
+
+
±
±
+
+
The Linear Complexity Test
106
+
+
±
+
+
+
The Serial Test
128, 103
+
+
±
±
+
+
The Approximate Entropy Test
104, 105
+
+
±
±
+
+
The Cumulative Sums (Cusums)
Test
104, 103–106
+
+
+
+
+
+
The Random Excursions Test
106
+
+
±
±
+
+
The Random Excursions Variant
Test
106
+
+
–
±
+
+
142
S. Bilan et al.

5
Conclusion
The made PRNG analysis showed that the application of CA allows us to construct
the PRNG that can compete with existing generators according to the main char-
acteristics. The test application has shown that the PRNG based on CA has good
characteristics, as they have positive results for the majority of tests. Research shows
that the initial number of single cells and the size of the CA has the main inﬂuence on
the quality of the PRNG. PRNGs showed the most effective work when the cells
were in the logic state “1” and their amount was variable from 35 to 70%. The best
CA size is determined from 17 × 17 to 30 × 30 of cells. The additional bit was
formed by cells of CA itself, which excluded the applying of the external source of
random numbers. Experimental research of PRNG based on the CA with hetero-
geneous cells allowed determining the amount of the initial CA cells with “1” state
and size of CA, which give a positive result. Also results allowed to determine the
number of heterogeneous cells (more than 3), in which there are almost no repeats. It
was found that there is no need in the intermediate comparing. PRNG on the CA has
high speed of random sequence generating without intermediate comparison.
References
1. Schneier, B.: Applied Cryptography: Protocols, Algorithms, and Source Code in C, 2nd edn.,
p. 784. Wiley Computer Publishing, Wiley (1996)
2. Marsaglia, G.: Random number generators. J. Modern Appl. Stat. Methods 2, 2–13 (2003)
3. Wolfram, S.: Cryptography with cellular automata. Lect. Notes Comput. Sci. 218, 429–432
(1986)
4. Rubio, C.F., Encinas, L.H., White, S.H., del Rey, A.M., Sánchez, G.R.: The use of linear
hybrid cellular automata as pseudorandom bit generators in cryptography. Neural Parallel Sci.
Comput. 12(2), 175–192 (2004)
5. Cattell, K., Muzio, J.C.: Synthesis of one-dimensional linear hybrid cellular automata. IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 15(3), 325–335 (1996)
6. Suhinin, B.M.: Development of generators of pseudorandom binary sequences based on
cellular automata. Sci. Educ. 9, 1–21 (2010)
7. Hoe, D.H., Comer,
J.M., Cerda,
J.C., Martinez,
C.D., Shirvaikar,
M.V.:
Cellular
automata-based parallel random number generators using FPGAs. Int. J. Reconﬁg. Comput.
Volume 2012 (2012), 1–13, Article ID 219028
8. Walker, J.: ENT. A Pseudorandom Number Sequence Test Program, 28 Jan 2008. http://
www.fourmilab.ch/random
9. NIST—National Institute of Standards and Technology. Computer Security Division.
Computer Security Resource Center. Download Documentation and Software: http://csrc.
nist.gov/groups/ST/toolkit/rng/documentation_software.html
10. Marsaglia, G.: The Marsaglia Random Number CDROM Including the Diehard Battery of
Tests of Randomness. Department of Statistics and Supercomputer Computations and
Research Institute. http://www.stat.fsu.edu/pub/diehard
11. Bilan, S., Bilan, M., Bilan, S.: Novel pseudorandom sequence of numbers generator based
cellular automata. Inf. Technol. Secur. 3(1), 38–50 (2015)
Designing of the Pseudorandom Number Generators …
143

A Mixed Fixed Point and Floating Point
Graphics Pipeline
Ovidiu Sicoe and Mircea Popa
Abstract The current paper presents the results obtained by modifying a graphics
pipeline implementation so that it partially operates with ﬁxed-point numbers and
partially with ﬂoating-point numbers. This was achieved by instrumenting a soft-
ware implementation of the OpenGL ES 1.1 speciﬁcation so that it uses the desired
format for representing real numbers. The targeted outcome was to ﬁnd a balance
between the areas of the pipeline where the two different number formats were used
so that the obtained output was similar. A variation in the format and precision of
the ﬁxed point number was also implemented and its impact on the output was
studied.
Keywords Fixed-point ⋅Floating-point ⋅Graphics pipeline ⋅Instrumentation
1
Introduction
Modern graphics processing units (GPUs) used in personal computers provide huge
data throughput [1]. They are general purpose GPUs, designed to perform in all
cases. Improvements can be made by specialized GPUs, designed to perform for
particular applications. Such GPUs could be implemented using FPGAs [2]. We
propose a study for optimizing the graphics pipeline of such an FPGA implemented
GPU by analyzing the impact of using different formats for representing real
numbers along the stages of the pipeline.
O. Sicoe (✉) ⋅M. Popa
Computer Engineering Department, Politehnica University of Timisoara,
Timisoara, Romania
e-mail: ovidiu.sicoe@gmail.com
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_17
145

2
Graphics Pipeline
Most of the graphics hardware works based on a multistage pipeline in which each
step is specialized in doing a certain operation. Initial graphics pipelines were ﬁxed
[3], meaning that the user could use a limited set of standard transformations that
could be applied to the processed data. Nowadays, the trend is to make the pipeline
more and more ﬂexible [3], allowing more complex effects to be applied during the
data ﬂow.
In order to use and control the graphics pipeline, certain application programing
interfaces (APIs) were conceived. Two of the most known and used are OpenGL
[4] and DirectX [5]. Those APIs are the interface to the graphic hardware and come
as an abstraction of its capabilities. They are implemented in the driver, thus having
little differences between each vendor.
Being an open speciﬁcation, OpenGL was quickly adopted by different plat-
forms and operating systems, allowing for easy, cross-platform graphics content
deployment. We have chosen to instrument our own software implementation of the
OpenGL ES 1.1 speciﬁcation due to the simplicity of the concept and because it
targets embedded systems.
Figure 1 presents the main stages of the abstraction of a graphics pipeline
described by the OpenGL ES 1.1 speciﬁcation.
In a ﬁxed pipeline, like the one described by OpenGL ES 1.1 speciﬁcation, most
of the heavy real number computations happen in the Per-Vertex Operations stage,
mainly implying matrix to matrix or matrix to vector computations [6], followed by
the Rasterization stage.
3
Real Numbers Representations
Accurate real numbers cannot be represented in binary computers, so approxima-
tions are used. For this, there are several formats that suite different needs. Two of
the most known and used are ﬂoating-point format and ﬁxed-point format.
Fig. 1 OpenGL ES 1.1 pipeline [4]
146
O. Sicoe and M. Popa

3.1
Fixed-Point Numbers
The ﬁxed-point number format always has a given number of bits for both the
fractional part and the integer part. For example, the number 2.5 can be represented
on a 16:16 format (16 bits for integer part and 16 bits for the fractional part) as
0000000000000010:1000000000000000.
The general rule is that the integer bits have ratios starting from 20 to 2i−1, where i is
the number of integer bits. On the other hand, the fractional bits have ratios from 2−1 to
2−f, where f is the number of fractional bits. This can be better observed in Fig. 2.
It can be easily observed that the precision of a ﬁxed-point number is 2f. The
main advantage of ﬁxed-point representation is that the operations can be executed
using the integer arithmetic and logic unit (ALU), so they are faster than the
ﬂoating-point operations.
3.2
Floating-Point Numbers
For ﬂoating point, the point doesn’t have a stationary position, thus the name
ﬂoating. The general form of such a number is depicted in Fig. 3.
The mathematical interpretation of those bits according to the IEEE 754 spec-
iﬁcation [7] is x = −1sign ⋅1 ⋅mantissa ⋅bexponent−bias, where b is the numera-
tion base of the representation and bias is 2size(exponent) −1. Following this, 2.5 is
represented as 0.011111111.01000000000000000000000 as a single precision
ﬂoating-point number.
4
Graphics Pipeline Instrumentation
We have conceived a software implementation of the OpenGL ES 1.1 speciﬁcation
so that we could easily change the number representation across the logical stages
of the pipeline. For this, we have provided hooks that are executed at the beginning
and at the end of each stage, respectively.
Fig. 2 Fixed-point format
Fig. 3 Floating-point format
A Mixed Fixed Point and Floating Point Graphics Pipeline
147

We have also implemented an abstraction for real numbers and provided two
concrete implementations, one for ﬁxed point numbers and one for ﬂoating point
numbers. This way, we could easily exchange the two different representations at
any point of the execution, even at runtime.
For the ﬁxed point representation, we have used 5:5, 10:10, 15:15, 16:16, 20:20
and 30:30 formats, while for the ﬂoating point representation, we have only used
the IEEE 754 single precision format, with the exponent represented on 8 bits and
the mantissa on 23 bits.
We have chosen the following instrumentation points, according to Fig. 1,
taking into consideration the operations that were involved in between them:
• the start of the pipeline, before the Per-Vertex Operations stage
• after the Per-Vertex Operations stage and before the Rasterization
The Per-Vertex Operations stage is the most computation intensive, so we tried
to see how a loss in precision during this stage would in ﬂuence the ﬁnal output. For
this, we have considered that the reference images would be the ones that result
from using the ﬂoating point representation across the whole pipeline. The next
steps have been to replace the ﬂoating point number format during the Per-Vertex
Operations stage with the ﬁxed point format, taking into consideration different
precisions. Here are the resulting combinations that we have used:
For each such combination we have generated 100 images, with the drawn
object rotated by 2° around the vector (−1; −1; −1) each frame. For example,
Fig. 4a shows the ﬁrst reference image by using ﬂoating point representation along
the complete pipeline. Additionally, we have created difference images for each
ﬁxed point image, relative to the ﬂoating point counterpart, as brieﬂy depicted
in Fig. 4.
(a) FP
(b) 5:5
(c) difference
Fig. 4 Frame 0 images
148
O. Sicoe and M. Popa

5
Results
The results we have obtained had shown that a full use of ﬁxed point numbers
across the complete pipeline would need a rather big increase in the size of the
format, the 30:30 format being the only one that provided results almost identical to
the original full FP rendering, as depicted in Fig. 5. Although the Fig. 5c seems
black not highlighting any difference, there are actually 8 pixels different.
As depicted in Fig. 4, the differences are quite signiﬁcant, so we tried to ﬁnd a
tradeoff between the output accuracy and the used formats, as depicted in Table 1.
The output difference images pointed out that the formats presented at line 9 in the
same Table 1 would yield outputs pretty close to the original ﬂoating point images.
Figure 6 presents the biggest difference between the outputs of the original full
FP pipeline and the hybrid one using 16:16 ﬁxed point combined with ﬂoating point
numbers. A total of 161 pixels are different, out of the whole 640,000 pixels
contained by the image.
(a) FP
(b) 30:30 
(c) difference
Fig. 5 Frame 30 images
Table 1 Precision
combinations
Per-Vertex operations
Rasterization
1
FPa
FP
2
5:5b
5:5
3
10:10
10:10
4
15:15
15:15
5
20:20
20:20
6
30:30
30:30
7
10:10
FP
8
15:15
FP
9
16:16
FP
10
20:20
FP
11
30:30
FP
aFloating point
bFixed point format precision as integer size: fraction size
A Mixed Fixed Point and Floating Point Graphics Pipeline
149

6
Conclusions
We succeeded in comparing the outputs of a graphics pipeline that used different
real number representations, either in a single format across the whole pipeline,
either using mixed ﬁxed and ﬂoating point representations along the stages.
We also drew the conclusion that the best mix of real number representations
across the pipeline, without a signiﬁcant increase in resources, but still preserving
the original output, would be represented by a 16:16 ﬁxed point representation
being used in the Per-Vertex Operations stage and a ﬂoating point format being
involved in the Rasterization and onward stages.
References
1. Lee, V.W., Kim, C., Chhugani, J., Deisher, M., Kim, D., Nguyen, A.D., Satish, N.,
Smelyanskiy, M., Chennupaty, S., Hammarlund, P., Singhal, R., Dubey, P.: Debunking the
100x GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU. In:
Seznec, A., Weiser, U.C., Ronen, R. (eds.) 37th International Symposium on Computer
Architecture (ISCA 2010), 19–23 June 2010, Saint-Malo, France, pp. 451–460. ACM (2010).
doi:10.1145/1815961.1816021
2. Franchini, S., Gentile, A., Sorbello, F., Vassallo, G., Vitabile, S.: An embedded, FPGA-based
computer graphics coprocessor with native geometric algebra support. Integration 42(3), 346–
355 (2009)
3. Luebke, D.P., Humphreys, G.: How GPUs work. IEEE Comput. 40(2), 96–100 (2007). doi:10.
1109/MC.2007.59
4. Inc, T.K.G.: Open GL ES Common/Common-Lite Proﬁle Speciﬁcation. https://www.khronos.
org/registry/gles/#specs11
5. Microsoft: DirectX Graphics and Gaming. https://msdn.microsoft.com/en-us/library/windows/
desktop/ee663274(v=vs.85).aspx
6. Hughes, J.F., van Dam, A., McGuire, M., Sklar, D.F., Foley, J.D., Feiner, S.K., Akeley, K.:
Computer Graphics: Principles and Practice, 3rd edn. Addison-Wesley (2013)
(a) FP
(b) 16:16 & FP
(c) difference
Fig. 6 Frame 28 images
150
O. Sicoe and M. Popa

7. Markstein, P.W.: The new IEEE-754 standard for ﬂoating point arithmetic. In: Cuyt, A.A.M.,
Kramer, W., Luther, W., Markstein, P.W. (eds.) Numerical Validation in Current Hardware
Architectures, 6.1, 11 Jan 2008. Dagstuhl Seminar Proceedings, vol. 08021. Internationales
Begegnungs- und Forschungszentrum fur Informatik (IBFI), Schloss Dagstuhl, Germany.
http://drops.dagstuhl.de/opus/volltexte/2008/1448 (2008)
A Mixed Fixed Point and Floating Point Graphics Pipeline
151

Functional Veriﬁcation of AMS-SoC
Models Using Hardware Emulation
Platforms
Hanan Tawﬁk, Mohamed AbdElSalam, Mona Safar
and Ashraf Salem
Abstract SystemC-AMS extensions to SystemC have been used in several
applications to model the analog part of a heterogeneous SoC. The SoC is usually a
pure simulation model where the digital part is modeled using SystemC. If an
emulation veriﬁcation environment is used, the digital part of the SoC would be
running on the emulator while the analog part, modeled with SystemC-AMS, would
be running on the co-model machine connected to emulator. In this paper, we
propose an approach to interface SystemC-AMS models running on the emulator
co-model machine with digital models running on the emulator. The veriﬁcation
challenge, addressed by this approach, lies in the fact that execution semantics of
models running on the co-model machine connected to the emulator are inherently
untimed, and SystemC-AMS is a timed environment with a time wheel completely
independent from the emulation time wheel. Our approach presents execution
dynamics to address this problem. We also describe a case study that demonstrates
the validity of the proposed approach.
Keywords Functional veriﬁcation ⋅Hardware emulation ⋅Dynamic TDF ⋅
SystemC-AMS
H. Tawﬁk ⋅M. AbdElSalam ⋅M. Safar ⋅A. Salem (✉)
Mentor Graphics Egypt, Cairo 11361, Egypt
e-mail: ashraf_salem@mentor.com
H. Tawﬁk
e-mail: hanan_tawﬁk@mentor.com
M. AbdElSalam
e-mail: mohamed_abdelsalam@mentor.com
M. Safar
e-mail: mona_safar@mentor.com
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_18
153

1
Introduction
The main advantage of HW emulation in industry is full System-on-Chip
(SoC) functional veriﬁcation. SoC functional veriﬁcation deals with protocols
validation and correctness of functional behavior of each component in SoC design
[1–3]. System-on-Chips (SoCs) nowadays, are heterogeneous in nature. They
contain components from different domains such as digital, analog and RF com-
ponents. This represents a big challenge when trying to functionally verify all
components working together before ﬁnal prototyping. In a system level pure
simulation environment, digital components can already be modeled using Sys-
temC [4]. To model analog/mixed signals behavior, deﬁning a new extension to
SystemC named SystemC-AMS, was necessary [5]. This extension allows SystemC
and SystemC-AMS to co-exist in the same simulation environment and to com-
municate as well using the converter ports provided [6, 7].
There could be some cases where the digital part of a SoC is provided in RTL
format running on a hardware emulator. In this case, the SystemC-AMS model
running on an emulator host machine will have to exchange data with the digital
part running on the hardware emulator. However according to [8], if the emulator
environment implements the SCE-MI standard, then only untimed SystemC can be
used for implementing models or testbenches running on the host machine.
SystemC-AMS models are timed and although allowed to run with emulation, their
simulation time is non-correlated to the emulation time.
In this paper, we present an approach to address this interface/co-veriﬁcation
challenge between SystemC-AMS models, running on a host machine, and the
digital RTL logic running on the emulator. We have also implemented a proof of
concept to demonstrate the validity of the proposed approach (Fig. 1).
The rest of this paper is organized as follows. Section 2 provides some back-
ground about SystemC-AMS extension and Direct Programming Interface (DPI).
Section 3 goes through the previous related work. Section 4 proposes execution
CPU
Master I/F
CPU
Master I/F
AMBA AHB/AXI
Software
Memory
Slave I/F
USB
Slave I/F
Slave I/F
Slave I/F
Ethernet
SATA
Display
Processor
Slave I/F
PHY (Digital+AMS)
PHY (Digital+AMS)
PHY (Digital+AMS)
PHY (Digital+AMS)
PHY (Digital+AMS)
PCI Express
Bridge
Bridge
AMBA APB
UART
GPIO
Slave I/F
Slave I/F
Arbiter
HW Emulation
Platform
Co-model
Host Machine
DPI
AMS SoC
Fig. 1 Full SoC functional veriﬁcation on HW emulation platforms
154
H. Tawﬁk et al.

dynamics to solve the interface challenge between SystemC-AMS and any HDL
design running on a hardware emulator using DPI. Section 5 describes the case
study used to demonstrate our proof of concept. Section 6 concludes the paper.
2
Background
SystemC-AMS
is
an
extension
to
SystemC.
It
allows
the
modeling
of
analog/mixed-signals systems at different level of abstractions. It can be used for
different use cases: executable speciﬁcation, virtual prototyping, architecture
exploration and integration validation. SystemC-AMS provides three different
models of computations [7, 9]:
Timed Data Flow. The system is modeled as a set of modules connected together
where data samples ﬂow from one module to another. The activation of a module
depends upon the time step speciﬁed by the user. The port rate of the module is also
speciﬁed by the user to determine the number of data samples, per activation, which
is to be read or written by this speciﬁc module. A set of connected TDF modules is
called a TDF cluster. To the SystemC scheduler, the TDF cluster is considered as a
normal SystemC module, that will be scheduled similarly to the other pure SystemC
modules. When the TDF cluster is selected to run, the SystemC-AMS scheduler
blocks the SystemC scheduler and activates the modules inside the cluster. The
activation occurs according to a static schedule that has been constructed based on
the port rates and time steps previously deﬁned by the user for each module in the
cluster [7]. This static scheduling approach speeds up the simulation signiﬁcantly
[11].
Signal Flow (LSF). It allows the modeling of continuous-time non-conservative
systems. The models are described using LSF pre-deﬁned primitives like adders,
integrators and differentiators. During elaboration phase, based on the description
provided, SystemC-AMS composes a corresponding system of linear equations that
can be solved by a dedicated solver that uses numerical methods to be applied at
appropriate timesteps [7].
Electrical Linear Networks (ELN). It allows the modeling of continuous-time
conservative systems. The models are described using ELN electrical network
primitives like resistors, capacitors and inductor. During elaboration phase, the
SystemC-AMS composes, based on the description provided, a corresponding
system of equations that can be solved by a dedicated solver using numerical
methods that are applied at appropriate timesteps [7].
In this paper, our focus is directed toward the TDF MoC and its interaction with
a digital model running on an emulator. SystemC-AMS library is released in the
form of proof of concept (PoC) releases. Latest stable release available is
SystemC-AMS PoC 1.0.1. The work presented in this paper is based on
SystemC-AMS PoC 2.0 Beta which is the latest release for this library. Although
still in beta state, it has the advantage of supporting the set of dynamic TDF
features, which mainly includes the TDF cluster activation based on events, and
Functional Veriﬁcation of AMS-SoC Models …
155

dynamic time step assignment during simulation time [10]. The event-based TDF
cluster activation is used in the approach proposed in this paper for proper inter-
facing with the hardware emulator.
The Direct Programming Interface (DPI) [12], deﬁnes the concept of import
functions that will be used by the approach proposed in this paper. The import
function is called from the transactor (usually written using a mix of System Verilog
and Verilog) and is deﬁned in the SystemC/C++ part in a layer usually called DPI
proxy. An import function is used to transfer data from HDL to SystemC/C++ or
vice versa. In [8], the concept of controlled and uncontrolled clock was also
introduced. The uncontrolled clock is a free running clock that never gets inter-
rupted. The controlled clock has the same time period as the uncontrolled clock but
it can be freezed by the emulator in order for a transactor to complete an ongoing
transaction and this to ensure correct operation of the DUT. When an import
function is called, the controlled clock is usually stopped until the call returns.
3
Related Work
A large number of publications covers the use of SystemC-AMS (for analog
mixed/signal components) coupled with SystemC (for digital components) to model
heterogeneous SoC. In [13], wireless sensor network nodes have been modeled. In
[14] the modeling of near-ﬁeld communication transceiver has been explored. In
[15], the model for a CMOS video sensor is implemented where SystemC-AMS
interfaces with SystemC-TLM using the wrapper described in [11]. To our
knowledge, the work in this paper is the ﬁrst to establish a bridge between
SystemC-AMS and the emulation technology.
4
Execution Dynamics
The execution semantics of models running on the co-model machine connected to
the emulator are inherently untimed, and SystemC-AMS is a timed environment
with a time wheel completely independent from the emulation time wheel.
The execution dynamics are composed of 2 phases: Phase 1: Initialization.
Initialization phase: should happen at the beginning of the emulation while no
other import calls are running. During this phase, a clock calculator C++ object is
used to calculate, approximately, the clock period of the uncontrolled clock of the
emulator. This value is used later in any TDF modules in SystemC-AMS that
calculates quantities that are a function of time. Figure 2a demonstrates the oper-
ation. An import call is called and the clock calculator entity records the current
156
H. Tawﬁk et al.

time t1. A second import function is called, one clock cycle later, and the clock
calculator entity record the current time t2. It calculates the clock period by sub-
tracting t1 from t2 and store the value.
Phase 2: Data transfer.
Time keeper update: During the data transfer operation, elapsed time is tracked
using the time keeper entity which is basically a counter that gets updated each DPI
call with the number of controlled clocks elapsed so far on the emulator side. Given
the number of controlled clocks elapsed and the uncontrolled clock period obtained
in the initialization phase, the approximate elapsed amount of time can be calcu-
lated (As mentioned the uncontrolled clock period is equal to the controlled clock
period but the number of clock cycles elapsed is not the same given the controlled
clock freezing time). This time being tracked by the time keeper can be used in TDF
clusters that generate quantities that are function of time.
Data exchange: We assume that the testbench has two TDF clusters, one for
each direction (directions are relative to the model running on the emulator host
machine). The Tx direction cluster transfers data to the emulator and the Rx
direction cluster receives data from the emulator. Data is transferred and received
through two DPI import functions, one for each direction. In each of the import
functions, corresponding TDF clusters are activated through events, using the new
capabilities offered in SystemC-AMS 2.0 Beta [10]. Figure 2b demonstrates the
operation described below:
Tx direction: ①Import function is called, from the transactor running on the emulator, with
an empty output parameter to be ﬁlled with data generated from the testbench side. ②
Import function activates the TDF cluster which was blocked on an event event_tx_tdf. The
import function blocks on another event event_tx_import waiting for the response from the
cluster. ③Once the cluster is ready with the data, it unblocks the import function blocked
on event_tx_import and wait for next cluster activation on the event event_tx_tdf during
next DPI import call. ④Import function returns with the data generated from the TDF
cluster back to the transactor.
Rx direction: ①′ Import function is called from the transactor running on the emulator with
an input parameter ﬁlled with data generated from the testbench side. ②′ Import function
Clock Calculator
DPI Proxy
Transactor
Other TDF modules
Emulator host
Emulator
Import function
at t1
Import function
at t2
v1
v2
Logic Design
(a)
(b)
Fig. 2 a Initialization phase. b Data transfer phase
Functional Veriﬁcation of AMS-SoC Models …
157

activates the TDF cluster which was blocked on an event event_rx_tdf. The import function
blocks on another event event_rx_import waiting for the cluster to consume the data.
5
Case Study
In order to verify the validity of our proposed interfacing approach, we have
implemented a case study with a Tx TDF cluster composed of two TDF modules
and an Rx cluster composed of a single TDF module. On the emulator side, a
simple transactor module is implemented that receives the data from Tx direction
and loop it back to the Rx direction (directions are relative to the model running on
the host machine). Import functions for Tx and Rx direction are called every clock
cycle Fig. 3 shows the case study’s architecture.
The different TDF modules in the clusters are listed below under their respective
directions:
Tx Direction:
Sine wave generator. The implementation has been reused from the example
provided in [7]. This TDF module depends on time, so it will be using the time
tracked by the time keeper to generate the values.
Flash ADC. An implementation of a 3 bit ADC with its circuit described [16].
Rx Direction:
Kelvin divider DAC (String DAC). An implementation of a 3 bit DAC with its
circuit described in [17].
The operation is similar to the steps provided in the previous section. During the
initialization phase, the clock period is determined. During data transfer phase, the
time keeper is updated with each import function call and used for generating an
analog value which is passed as an input to the ADC module, scheduled to run next.
The ADC module converts the analog input value into a 3 bit value then unblocks
the import function which returns the generated value to the transactor. In the Rx
Transactor
Emulator
Emulator host
Clock
Calculator
Time
Keeper
TDF cluster – Tx Direction
DPI Proxy
Loopback logic
ADC
Sine wave
generator
DAC
TDF cluster – Rx Direction
Fig. 3 Cast study architecture. Generated wave (top) versus Reconstructed wave (bottom)
158
H. Tawﬁk et al.

direction, another import function is called to activate the cluster and then blocks
until it executes. The DAC reads the value received from the transactor and con-
verts it back into an analog value. Upon completion the cluster unblocks the import
function in order to give it a chance to fetch another value from the transactor. Each
cluster will be activated again whenever the import function, corresponding to the
direction the cluster is used for, is called. The result expected is that the sine wave
generated in Tx direction should be approximately reconstructed in the Rx direc-
tion. Figure 3 shows the generated waveform versus the reconstructed one. The
amount of distortion shown in the reconstructed waveform is a factor of the
accuracy of the ADC and DAC process.
Table 1 lists runtime statistics about the main import functions used in the case
study. Those statistics include the total time consumed by each import function
throughout the whole run, the average execution time for a single call and the
number of times each import function has been called. SendValue is the import
function used for the Rx direction and is labeled as one-way because it transfers
data in one direction only, from RTL to software. GetValue is the import function
used for the Tx direction and is labeled as two-way because it is called from RTL to
software and returns back to RTL after fetching the required data from software.
CalculateClockPeriod is a one-way import function that is used during the initial-
ization phase to calculate the clock period. Those numbers has been collected while
running the case study on a hardware emulator [3] with an operating frequency of
694 kHz.
6
Conclusion
In this paper we have demonstrated the execution dynamics to interface a
SystemC-AMS model running on an emulator host machine with a digital design
running on an emulator. Future work will be exploring the possibility of sending
multiple analog values in a single import function in order to minimize context
switching between emulation and System-C AMS simulation. In this case, con-
trolled clock would need to freeze less frequently, leading to a better runtime
performance.
Table 1 Import DPI statistics
Import DPI call
Total time
consumed
Average time/call
(μs)
Number of time
called
SendValue (One-way)
9 s, 999503 μs
2
3,786,459
CalculateClockPeriod
(One-way)
4 μs
2
2
GetValue (Two-way)
88 s, 371095 μs
23
3,786,459
Functional Veriﬁcation of AMS-SoC Models …
159

References
1. Lin, Y.-.S.: Essential Issues in SOC Design: Designing Complex Systems-on-Chip. Springer
Science & Business Media, 31 May 2007
2. Nurmi, J., Tenhunen, H., Isoaho, J., Jantsch, A.: Interconnect-Centric Design for Advanced
SoC and NoC. Kluwer Academic Publishers (2004)
3. Mentor Graphics Corporation, Veloce Emulation Platform. http://www.mentor.com/products.
fv.emulation-systems/
4. SystemC. http://www.systemc.org
5. SystemC-AMS. http://www.systemc-ams.org
6. Vachoux, A., Grimm, C., Einwich, K.: Towards analog and mixed-signal SOC design with
SystemC-AMS. In: Proceedings of Second IEEE International Workshop on Electronic
Design, Test and Applications (DELTA), Perth, WA, Australia, pp. 97–102, Jan 2004
7. Accelera Systems Initiative: SystemC AMS Extensions User’s Guide, pp. 1–153 (2010)
8. Accelera Systems Initiative: Standard Co-Emulation Modeling Interface (SCE-MI) Reference
Manual, v2.2, pp. 1–203, Jan 2014
9. Li, F., Dekneuvel, E., Jacquemond, G., Quaglia, D., Lora, M., Pecheux, F., Butaud, R.:
Multi-level modeling of wireless embedded systems. In: Proceedings of the Forum on
Speciﬁcation and Design Languages (FDL), Munich, Germany, pp. 1–8, Oct 2014
10. Accelera Systems Initiative: Standard SystemC AMS Extensions 2.0 Language Reference
Manual, pp. 1–200 (2013)
11. Damm, M., Grimm, C., Haas, J., Herrholz, A., Nebel, W.: Connecting SystemC-AMS models
with OSCI TLM 2.0 models using temporal decoupling. In: Proceedings of the Forum on
Speciﬁcation, Veriﬁcation and Design Languages (FDL), Stuttgart, Germany, pp. 25–30, Sept
2008
12. Accelera: IEEE 1800-2012: SystemVerilog (SV), IEEE std 1800-2012, pp. 1–1275 (2012)
13. Vasilevski, M., Pecheux, F., Aboushady, H., De Lamarre, L.: Modeling heterogeneous
systems using SystemC-AMS case study: a wireless sensor network node. In: Proceedings of
the IEEE International Behavioral Modeling and Simulation Workshop (BMAS), San Jose,
CA, USA, pp. 11–16, Sept 2007
14. Li, W., Zhou, D., Li, M., Nguyen, B.P., Zeng, X.: Near-ﬁeld communication transceiver
system modeling and analysis using SystemC/SystemC-AMS with the consideration of noise
issues. IEEE Trans. Very Large Scale Integr. Syst. 21, 2250–2261 (2013)
15. Cenni, F., Scotti, S., Simeu, E.: Behavioral modeling of a CMOS video sensor platform using
SystemC-AMS/TLM. In: Forum on Speciﬁcation and Design Languages (FDL), Oldenburg,
Germany, pp. 1–6, Sept 2011
16. Kester, W.: MT-020 tutorial, ADC Architecture 1: The Flash Converter, Analog Devices.
http://www.analog.com
17. Kester, W.: MT-014 tutorial, Basic DAC Architecture 1: String DAC and Thermometer (Fully
Decoded) DACs, Analog Devices. http://www.analog.com
160
H. Tawﬁk et al.

Inﬂuence of the Antenna’s Height
to the Standing Waves Ratio When
Performing the Electromagnetic
Susceptibility Tests in Anechoic Chambers
Martin Pospisilik, Milan Adamek and Petr Neumann
Abstract Immunity tests to radiated electromagnetic ﬁeld are one of the most
common tests performed within the framework of the electromagnetic compati-
bility. These tests are performed inside a shielded chamber provided the require-
ments given by the appropriate standards are kept. As the levels of the generated
ﬁelds can be high and the output power of the transmitting ampliﬁers cannot be
inﬁnite, it is necessary to achieve a good performance of the transmitting antenna.
By means of the real experiment the author of this paper shows how the height of
the antenna placed above the conductive ﬂoor can affect the antenna’s voltage
standing waves ratio and how this phenomenon affects the parameters of the whole
measurement system.
Keywords EMC ⋅Susceptibility test ⋅Semi anechoic chamber ⋅Transmitting
antenna ⋅Voltage standing wave ratio ⋅Transmitting power
1
Introduction
Testing of electromagnetic susceptibility became one of the important disciplines as
the complexity of electrical systems that must operate together has increased. In
1968, H.M. Schilke, one of the founders of the ﬁeld of science related to the
electromagnetic compatibility, claimed: “The system itself may be perfectly reli-
able, but practically worthless in operation unless it is not electromagnetically
compatible at the same time.” [1]. Since that time constructers faced many problems
M. Pospisilik (✉) ⋅M. Adamek ⋅P. Neumann
Faculty of Applied Informatics, Tomas Bata University in Zlin,
Nad Stranemi 4511, 760 05 Zlin, Czech Republic
e-mail: pospisilik@fai.utb.cz
M. Adamek
e-mail: adamek@fai.utb.cz
P. Neumann
e-mail: neumann@fai.utb.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_19
161

raising at the ﬁeld of mutual electromagnetic compatibility of devices being in a
concurrent operation. For example, as described in [2], in 1984 the NATO airplane
“Tornado” crashed in Germany after its circuits interfered with a powerful trans-
mitter in Holkirchen. In 1982, the British cruiser Shefﬁeld was sunk by Argentine
aircraft in the Falklands War, partly because its defence system abetting the enemy
rockets was switched. Due to its electromagnetic incompatibility, it interfered with
radio communication, crucial for the cruiser’s crew. According to [3], there were
several accidents reported in the Czech Republic.
Therefore, the electronic devices should be tested for their immunity to the
radiated electromagnetic ﬁeld.
1.1
Standardization
In Europe the current basic deﬁnition of the test of the electromagnetic suscepti-
bility of common devices against the radiated electromagnetic ﬁeld is provided by
the standard EN 61000-4-3 [4]. It deﬁnes the frequency ranges, modulations and
intensities that are to be developed in the area in which the tested device is placed.
The ﬁeld intensity levels speciﬁed by the standard [4] are enlisted usually between 1
and 10 V/m.
Fig. 1 Basic conﬁguration of the immunity test [5]
162
M. Pospisilik et al.

1.2
Basic Conﬁguration of the Test
The basic conﬁguration of the test is depicted in Fig. 1. The tested device is placed
in an anechoic chamber on a non-conductive table. It is irradiated by the modulated
electromagnetic ﬁeld generated by an antenna placed in the distance speciﬁed by
the standards. The ﬁeld intensity is checked inside the space called Uniform ﬁeld
area. Its dimensions as well as the tolerance for the intensity levels are speciﬁed
in [4].
2
Problem Description
When the conﬁguration as depicted in Fig. 1 is used, there exists a risk of inter-
actions between the metal ﬂoor of the chamber and the transmitting antenna,
resulting in changes in the antenna’s impedance. This causes the impedance
matching of the antenna to the ampliﬁer is corrupted, resulting in the occurrence of
standing waves on the cable between the antenna and the ampliﬁer. The quantity of
standing waves can generally be described as a voltage standing waves ratio
(VSWR) by Eq. (1). It deﬁnes the ration between the maximum (Vmax) and min-
imum (Vmin) amplitude of the standing wave. Also the amplitudes of the incident
(Vi) and the reﬂected (Vr) waves can be applied.
VSWR = Vmax
Vmin
= Vi + Vr
Vi −Vr
ð1Þ
The amount of energy reﬂected back from the point of impedance mismatch can
be described by means of the reﬂection coefﬁcient ρ.
ρ = Vr
Vi
ð2Þ
Therefore the Eq. (3) can be applied.
VSWR = 1 + ρ
1 −ρ
ð3Þ
The efﬁciency of transmitting the energy to the space is then affected by mis-
match losses that are caused by the reﬂections from the antenna. According to [6]
the mismatch loss ratio (ML) is the ratio of incident power to the difference between
incident and reﬂected power:
Inﬂuence of the Antenna’s Height to the Standing Waves …
163

ML = 10 log10
Pi
Pi −Pr
= −10 log10 1 −ρ2


½dB
ð4Þ
In (4), Pi stands for the incident power (generated by the ampliﬁer) while Pr
stands for the reﬂected power, that is not transmitted but loads the cable and the
ampliﬁer in the form of the standing waves.
In terms of the voltage standing wave ratio (VSWR) the following equation can
be applied [6]:
ML = −10 log10
1 −
VSWR −1
VSWR + 1

2
 
!
½dB
ð5Þ
3
Experiment Description
The experiment was held in the semi-anechoic chamber Frankonia SAC-3 Plus that
is placed at the Faculty of Applied Informatics of Tomas Bata University in Zlin
[7]. The signal was generated by the generator Rohde & Schwarz SMF 100 A and
ampliﬁed by the ampliﬁer Ampliﬁer Research 150W1000. The signal was trans-
mitted with the antenna Rohde&Schwarz HL046E. The frequencies and the mod-
ulation were set in accordance with the standard EN 61000-4-3, but the frequency
range was limited to 250 MHz as the hereby described effects were observed in the
frequency range from 80 to 150 MHz. The power of the ampliﬁer was set by means
of a feedback ﬁeld probe ETS Lindgren HI-6005 that was located in the middle of
the Uniform ﬁeld area. The required level of the electromagnetic ﬁeld was set to
10 V/m and the polarization of the antenna was angled to vertical. The instruments
were driven by EMC 32 software. The conﬁguration of the semi-anechoic chamber
was as depicted in Fig. 1, using the absorbers placed on the ﬂoor.
During the experiment, the antenna’s height above the ﬂoor was consequently
changed from 90 to 135 cm with the step of 5 cm. For each of the heights the
transmitting frequencies from 80 to 250 MHz were applied increasingly with the
step of 1% and the response of the chamber (ﬁeld level at the position of the probe)
as well as the response of the transmitting system (output power of the ampliﬁer and
voltage standing waves ratio).
4
Results and Discussion
The results obtained by the experiment has shown that for vertical antenna’s
polarization the setting of its operating height can be critical in terms of VSWR and
power losses, especially at the frequencies below approximately 200 MHz.
164
M. Pospisilik et al.

In Fig. 2 the VSWR dependence on frequency and the antenna’s height above
the conductive ﬂoor is depicted. In Fig. 3 there is depicted the dependence of the
output power of the ampliﬁers needed to generate the required ﬁeld intensity
strength of 10 V/m at the point where the feedback sensor was placed on the
antenna height and the frequency. Although the level of the power generated by the
ampliﬁer does not directly reﬂect the antenna’s VSWR as there may be reﬂections
inside the chamber that affect the level measured by the sensor, a certain correlation
between the measured values of the ampliﬁer’s output power and the measured
antenna’s VSWR has been observed by means of the following method. For each of
the antenna’s height, the maximum of VSWR and of the output power has been
registered regardless of the carrier frequency. From these values the graph depicted
in Fig. 4 has been compiled.
In this graph, the dependencies of three different variables on the antenna’s
height are depicted in order to show the abovementioned correlation:
• Maximum VSWR observed in the range from 80 to 250 MHz,
• Maximum ampliﬁer’s power generated between 80 and 250 MHz,
• Hypothetical output power that would be needed to generate the required
intensity provided VSWR was as low as 1 in the whole frequency range cal-
culated according to Eq. (5).
According to Fig. 4, it can be stated that for antenna’s heights above approxi-
mately 110 cm the need for transmitting power mostly depends on the antenna’s
VSWR that changes with the antenna’s height. If the reﬂections on the antenna
were cancelled, the transmitting power needed to achieve the required level 10 V/m
in the distance of 3 m from the antenna would be from 40 to 55 W.
Fig. 2 VSWR versus antenna’s height and carrier frequency
Inﬂuence of the Antenna’s Height to the Standing Waves …
165

With the currently achieved VSWR the required output power varied from
approximately 52 W at the antenna’s height of 115 cm to approximately 142 W at
110 cm. For the antenna’s heights below 110 cm the real output power needed to
achieve the required ﬁeld intensity has been even higher than expected.
Fig. 3 VSWR versus antenna’s height and carrier frequency
Fig. 4 VSWR, real output power and hypothetical output power at VSWR = 1 versus antenna’s
height
166
M. Pospisilik et al.

A capacitive coupling between the ﬂoor and the antenna is suspected to cause this
phenomenon.
Finally, this experiment shows how complex the issues on radiation of the EM
ﬁeld inside the semianechoic chambers are. In the past, the authors performed other
types of experiments that could also be interesting for the audience, for example [8].
5
Conclusions
This paper describes the experiment in which the dependence of the transmitting
antenna’s VSWR was observed according to its height above the conductive ﬂoor.
A typical situation of EMC susceptibility test in a semi anechoic chamber has been
modeled. According to this experiment it can be generally stated that when verti-
cally polarized, the coupling between the antenna and the chamber’s ﬂoor affects
the antenna’s VSWR which results in increased requirements on the power of the
transmitting ampliﬁers. Because the standard EN 61000-4-3 does not accurately
specify the antenna’s height, it is advisable to search for the optimal antenna’s
height prior to performing of the uniform ﬁeld area calibration.
Acknowledgements This work was supported by the Ministry of Education, Youth and Sports of
the
Czech
Republic
within
the
National
Sustainability
Program
project
No.
LO1303
(MSMT-7778/2014).
References
1. Svacina, J.: Electromagnetic compatibility [Elektromagneticka kompatibilita]. Brno University
of Technology, Brno (2002)
2. Paul, C.R.: Introduction to Electromagnetic Compatibility. Wiley, New York (1992)
3. Vaculik, P.: Introduction to electromagnetic compatibility [Uvod do elektromagneticke
kompatibility]. In: Proceedings of Radio-Komunikace ’94, Pardubice (1994)
4. International Electrotechnical Commission: EN 61000-4-3
5. http://www.iec.ch/emc/basic_emc/basic_emc_immunity.htm
6. Eadie, A.: EMC Immunity Testing. https://www.emcfastpass.com/emc-testing-beginners-
guide/emc-immunity-testing/
7. Lo, Y.T., Lee, S.W.: Antenna Handbook: Theory, Applications, and Design. Springer Science
+ Business Media, New York (1988)
8. Pospisilik, M., Soldan, P.: Electromagnetic ﬁeld distribution within a semi anechoic chamber.
In: Proceedings of the 18th International Conference on Systems, Santorini Island (2014)
Inﬂuence of the Antenna’s Height to the Standing Waves …
167

Carstairs-McCarthy’s Morphological
Rules of English Language
in RDFCFL Graphs
Alena Lukasová, Martin Žáček and Marek Vajgl
Abstract The article presents an approach to express morphology and syntax rules
or speciﬁcations of basic terminology of the English language in RDFCFL graphic
metalanguage following the methodology of Andrew Carstairs-McCarthy’s book.
The formal modelling of English linguistics follows here two levels: syntactical
(investigating lexical and functional categories of speech, in particular rules of their
merging into larger units) and semantic-based morphology of the English language
which covers the importance of language morphemes.
Keywords RDF model ⋅Clausal form logic (CFL) ⋅RDFCFL formal system
1
Motivation
An RDF modelling principle of real world and human activities has become more
or less one of the leading approaches within artiﬁcial intelligence in the last decade.
Databases of RDF-linked basic statements describe a lot of most important areas of
real life. RDF modelling of a domain of real life has brought into knowledge
representation a strong demand on precise speciﬁcations of concepts within their
broader connections with other concepts of a modelled domain. From the point of
view of precise concept formal representation, Linguistics generally belongs to the
same category as exact sciences like Mathematics or Computer Science. No wonder
A. Lukasová ⋅M. Žáček (✉) ⋅M. Vajgl
Department of Computers and Informatics, University of Ostrava,
Ostrava, Czech Republic
e-mail: martin.zacek@osu.cz
A. Lukasová
e-mail: alena.lukasova@osu.cz
M. Vajgl
e-mail: marek.vajgl@osu.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_20
169

that some specialists in formal modelling of human thinking did not miss a pos-
sibility to make an experiment with meta-language expressing basic linguistic
concepts and to formulate linguistic research methodology by the RDF model
formal tools.
2
Introduction
The authors of the paper completed the original RDF model by introducing general
or existential statements (see [1]) as necessary conditions of formal deduction
corresponding to the natural human mental activity. Moreover, they have extended
the RDF model with “if – then” form of sentences following the Richard’s method
known as “clausal form logics” (CFL [2]).
We present here an experiment to express ground terminology of the English
language morphology and syntax on the base of ﬁrst order logic’s graphic tools
RDFCFL [3]. It seems that our meta-language representation of English gram-
matical rules taking into account predicate logic’s semantic principle with its high
expressivity and easy-to-read graph-based form could generally bring new aspects
into language theory.
The aim of this paper is modelling of linguistics at two levels
• syntactical: investigating lexical and functional categories of speech, in partic-
ular rules of their merging into higher units; here a graph-based tool of formal
expressing became the RDFCFL metalanguage,
• semantical: semantic-based morphology of a particular language which covers
the importance of language morphemes; here it is better to use simple semantic
(associative) networks together with some explaining by example.
All the life people learn to form sentences according to certain rules. Any natural
language has its own rules, e.g. the English language form sentences using a simple
formula SVOMPT (Subject, Verb, Objects, Manner, Place, Time), while the
Czech language has a structure that is much more complicated.
In order to model a natural language, it is necessary to ﬁnd suitable means in the
form of a formal system. As a default, the English language with its fairly simple
morphology rules has been chosen for our experiment.
3
Drawing Sentences About English Linguistics by Means
of RDFCFL Graph Language or Semantic Network
Richards [4] proposed the Clausal Form Logic (CFL) built on the base of the FOPL
and well corresponding with common using of the conditional “if—then” state-
ment. Generally, a conditional statement (clause) says that the consequent
170
A. Lukasová et al.

composed as a disjunction of some predicate atoms follows from the antecedent
composed as a conjunction of some predicate atoms.
The approach allows us to formulate clauses in the form
⟨antecedent⟩⟨implies⟩⟨consequent⟩
ð1Þ
Selecting a formal language for a knowledge representation is crucial. The
formal basis should become here the ﬁrst order predicate logic (FOPL) base for its
high expressivity and a wide range of already developed formal deduction tools.
Knowledge Representation (originally those contained in Web resources), which
are based on a domain ontology usually has been created in the framework of RDF
(Resource Description Framework) model. An RDF model manipulates the
semantic aspect of terms speciﬁed through URI references to resources in which
their meanings are always elucidated by means of a certain position in a relevant
ontology. The graphic RDF model in its form is easy and simple to understand even
for the users who do not have experience with formal modelling. The idea is based
on a simple statement concerning relations between items (resources) in the form of
basic vector (Fig. 1).
The vector pattern corresponds with the SVO part of the general SVOMPT
pattern of the English grammar.
The graph version of the vector representation uses notation of the Clausal Form
Logic (CFL [5]).
Developments in the ﬁeld of formal knowledge representation clearly show that
the language of the FOPL and speciﬁcally its clausal form (in text or graph version)
is an appropriate formal language that can virtually represent any assertion for-
mulated in a natural language.
The graphic form of the CFL language [4] became the main idea of the RDFCFL
graph language used here. Clauses use dashed lines in the cases of antecedent
vectors and full lines for vectors of clause consequent.
By means of adding elements of Description logic [5] into the RDF model it has
been possible to communicate with the web language OWL that also increases its
expressiveness. Both languages in their text format are based on the XML syntax,
making it easy for their machine processing.
Besides the RDFCFL representation of a clause we also use a simpler tool
semantic network expressing interrelations of concepts in English linguistic known
a long time before informatics ordered them among formal modelling tools. We use
Fig. 1 Basic vector
Carstairs-McCarthy’s Morphological Rules …
171

here the semantic network principle in the cases where the semantics of clauses is
more important than syntax of their corresponding rules.
Example
The RDFCFL graphic clausal statement of the form (1) in the following ﬁgure
expresses the fact (Fig. 2):
Words as building blocks of a language collected in its vocabulary expressed.
• by a collection of simple atomic statements of the form (1) like “word1 is item
of a vocabulary”,
• by means of an advanced RDF constructor rdf: bag (thick full line).
To each name of the graph items RDF CFL orders a URI identiﬁer within a
chosen ontology (top-level or domain speciﬁed) conventional to the topic of
linguistics.
In the book [6] Andrew Carstairs-McCarthy speciﬁes the concept of word as
follows:
“Words are units of language which are basic in two senses:
1. in that they have meanings that are unpredictable and so must be listed in
dictionaries and
2. in that they are the building-blocks out of which phrases and sentences are
formed.” (Figs. 3 and 4).
Fig. 2 Words are building blocks of a language
172
A. Lukasová et al.

4
Conclusion
We see a usability of our RDFCFL approach in two directions:
1. As an easy-to-understand tool of representation of language grammatical rules.
It is possible to use only a simple semantic network.
2. The approach based on logical principles could push authors of deﬁnitions or
speciﬁcations of rules to hold within special grammar categories to specify
carefully what are items of clauses, antecedents—prerequisites and consequents
—conclusions of clauses represented truth-full general sentences.
3. As a part of the knowledge base of general rules in domain ontology for con-
crete natural language because it gives a possibility of rewriting also into OWL
language.
4. As a general part of the knowledge base together with further clauses describing
the represented world gives a possibility to obtain new conclusions by means of
the RDFCFL formal system.
Fig. 3 2 Ad 1 semantic network expressing the role of dictionary to explain an unpredictable
word
Fig. 4 Ad 2 pattern of the role of words as building blocks of phrases or sentences
Carstairs-McCarthy’s Morphological Rules …
173

Further work will be in this area: Creating words would inﬂexion and derivation,
work with morphemes.
Acknowledgements The research described here has been ﬁnancially supported by University of
Ostrava grant SGS13/PŘF/16. Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the authors and do not reﬂect the views of the sponsors.
References
1. Lukasová, A., Vajgl, M., Žáček, M.: Reasoning in RDF graphic formal system with quantiﬁers.
In: Proceedings of the International Multiconference on Computer Science and Information
Technology, 2010, pp. 67–72
2. Richards, T.: Clausal Form Logic. An Introduction to the Logic of Computer Reasoning.
Addison Wesley (1989)
3. Lukasová, A., Žáček, M., Vajgl, M., Kotyrba, M.: Resolution reasoning by RDF clausal form
logic. IJCSI Int. J. Comput. Sci. Issues 9(3, No 1) (2012). ISSN (Online): 1694-0814. http://
www.IJCSI.org
4. Lukasová, A., Žáček, M., Vajgl, M.: Reasoning in graph-based clausal form logic. IJCSI Int.
J. Comput. Sci. Issues 9(1, No 3), 37–43 (2012). ISSN (Online): 1694-0814
5. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., Patel-Schneider, P.: The Description
Logic Handbook. Cambridge University Press, United Kingdom (2004)
6. Carstairs-McCarthy, A.: An Introduction to English Morphology: Words and Their Structure.
Edinburgh University Press 2002. ISBN 0748613269
174
A. Lukasová et al.

Mathematical Modeling and Computer
Simulation of Simple Permutation
Brainteaser in MS Excel
Michal Musilek, Stepan Hubalovsky and Marie Hubalovska
Abstract Originally, the concept of the system and the system approach was used
only in the natural sciences and technical disciplines. Currently, systems approach
begins to be applied in other scientiﬁc disciplines such sociology, psychology and
other social sciences, which deal with the behavior of the society. The system
theory is closely related to modeling and computer simulation that are understand as
one of the way of scientiﬁc research. Education has to reﬂect the application of
scientiﬁc principles and therefore the principles of system approach, modeling and
simulation should be applied even to pedagogical science and education. This new
educational method should be implemented to learning of prospective teachers. The
paper describes the implementation of principles of system approach, modeling and
computer simulation to learning of prospective teacher of informatics in form of
computer simulation of a simple permutation brainteaser.
Keywords Modeling ⋅Computer simulation ⋅Informatics ⋅Education ⋅
Case study ⋅Permutation brainteaser
1
Introduction
The education should reﬂect current trends of scientiﬁc knowledge. One of the
approaches that are widely implement to scientiﬁc work are principles of system
approach together with scientiﬁc modeling and computer simulation. Moreover
computer simulation is now understand as “the third way” of the science [1].
System approach, modeling and simulation are implemented not only in the natural
M. Musilek ⋅S. Hubalovsky (✉) ⋅M. Hubalovska
University of Hradec Králové, Hradec Králové, Czech Republic
e-mail: stepan.hubalovsky@uhk.cz
M. Musilek
e-mail: michal.musilek@uhk.cz
M. Hubalovska
e-mail: marie.hubalovska@uhk.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_21
175

sciences and technical disciplines, but also in economy, humanities, sociology and
other social science. Many of universities has principles of system approach,
modeling and simulation already implemented in their study programs.
The education has to reﬂect the application of scientiﬁc principles and therefore
the principles of system approach, modeling and simulation should be applied to
pedagogical science and education see e.g. [2, 3]. These principles should be
successively applied in lower stages of the school education, in secondary schools.
Implementation of system approach, modeling and simulation can support devel-
opment of key competencies, particularly development of key competency for
solving the problems, discovering and creativity [4, 5]. Development of the key
competency by means of implementation of system approach, modeling and sim-
ulation requires from the teacher well knowledge of these principles, the teacher
should be able to apply modeling and simulation in educational process.
The implementation of the method of modeling and computer simulation in
education supports pupils and students discover and understand science’s phe-
nomenon and processes, it helps them understand technical equipment, it develops
algorithmic as well as programming thinking and it develops computational, logical
and mathematical thinking. Application of principles modeling and simulation
develops student’s intuition. Last but not least the method modeling and simulation
helps to increase of ICT competency of pupils and students—see e.g. [4, 6].
Computer simulation together with visualization enable better understanding of the
basic features of the real systems and develop their discovering. It is also essential
that the teaching by means of simulation is much cheaper and faster than the
teaching carried by real experiment. In some cases providing the real experiment
cannot be feasible. Modeling and simulation can be and should be applied in all
subjects of secondary school curricula thus it helps to complex development of
children’s personality.
The authors present in this paper the case study of implementation of principles
of system theory, modeling and computer simulation to learning of prospective
teacher of informatics within the study specialization Teaching of Informatics
accredited by Department of Informatics, Faculty of Science, University of Hradec
Kralove in their compulsory subject Modeling and Simulation. The paper illustrates
problem analysis (system identiﬁcation), creation of conceptual model and simu-
lation program of solution and visualization of simple permutation brainteaser.
2
Case Study—Solution of Permutation Brainteaser
in Programming Language Visual Basic for Application
in MS Excel Spreadsheet
The students within the lectures of subject Modeling and simulation in study
program Teaching of informatics are acquainted with the system theory, scientiﬁc
modeling and simulation as well as with application of these principles in learning
176
M. Musilek et al.

at secondary schools. The seminar are focused to development of skills and
knowledge in identiﬁcation of real system and creation conceptual model and
computer simulation program.
Within the subject the student has to create and pass the ﬁnal project. The main
requirement for students is to create computer simulation program of selected
problem, phenomenon or process.
In the following text there is in the form of case study demonstrated example of
ﬁnal project created by students in the last academic year. The presented project
represents the simulation of the permutation brainteaser programmed in Visual
Basic for Application (VBA) implemented to MS Excel Spreadsheet.
2.1
Problem Analysis
Although the puzzle can be solved intuitively (the permutation puzzle is very
simple), the ﬁnding of automatic solution is not entirely elementary. Conversely, for
a number of undergraduate students can be intuitive solution problem. Finally,
those who are able to solve the puzzle intuitively are not sure that their solution is
optimal (e.g. in terms of the number of steps used).
Before the algorithm as well as program of the solution of permutation puzzle
will be described, it will be proved that mentioned puzzle is indeed permutation
puzzle whose arbitrary mixing is permutations of basic positions and various
possible moves also represent certain permutations and their chaining can be
modeled by combining of the permutations.
Denote puzzle segments according to Fig. 1 by numbers 1, 2, 3, 4, 5, 6, 7 and 8.
Then we the following four elementary strokes can be performed by counter-
clockwise rotation of the circle segments:
1. Rotation of the left circle by angle 90° counterclockwise: the basic permutation
(1, 2, 3, 4, 5, 6, 7, 8) is permuted to (4, 1, 2, 3, 5, 6, 7 8). Let name of the move
is “A”.
2. Rotation of the right circle by angle 90° counterclockwise: the basic permutation
(1, 2, 3, 4, 5, 6, 7, 8) is permuted to (1, 2, 3, 4, 8, 5, 6, 7). Let name of the move
is “B”.
3. Movement the upper half of both circles so that the upper right semicircle is
exactly above the lower left semicircle followed by rotation thus created circle
Fig. 1 Numbering of the
segments of permutation
brainteaser
Mathematical Modeling and Computer Simulation …
177

by angle 90° counterclockwise: the original permutation (1, 2, 3, 4, 5, 6, 7, 8) we
is permuted to (1, 2, 6, 3, 4, 5, 7, 8). Finally the upper half of both circles are
returned to the starting position. Let name of the move is “C”.
4. Movement the upper half of both circles so that the upper left semicircle is
exactly above the lower right semicircle followed by rotation thus created circle
by angle 90° counterclockwise: the original permutation (1, 2, 6, 3, 4, 5, 7, 8) we
is permuted to (8, 1, 3, 4, 5, 6, 2, 7). Finally the upper half of both circles are
returned to the starting position. Let name of the move is “D”.
Note that in each of these elementary strokes A, B, C, or D the positions of four
segments of the puzzle are changed at once. This begs the question: How these
elementary strokes can be combined (how the permutation can be composes), so the
positions only two segments will be changed? If only the position of two segments
is changed, the two segments are mutually replace, without changing the position of
any of the remaining six segments.
The task can be extended as follows: Does exist the sequence of the elementary
moves {A, B, C, D} so that just two elements are in different positions than their
original positions are and other six elements return to their original positions.
Intuitive ﬁnding of such sequences of movements is difﬁcult, so the solution can be
created in any simulation software or programming language.
2.2
Design of Computer Simulation Program
Visual Basic for Application of MS Excel Spreadsheet can be understood as “ap-
propriate tool” for solution of the permutation puzzle—see e.g. [7].
First, the function Set() will be created. The function deﬁnes the initial state of
the puzzle (the variable H of type array contains the actual permutation of the
segments of the puzzle):
Function Set() As Variant 
  Dim H As Variant 
  H = Array(0, 1, 2, 3, 4, 5, 6, 7, 8) 
  Set = H 
End Function 
Another function Move (H, i) realizes the various elementary moves of the set
{A, B, C, D}. The speciﬁc motion is deﬁned by input of the second parameter i.
The value of i = 1 represents move A, i = 2 represents the move B, i = 3 rep-
resents move of C and ﬁnally i = 4 represents move of D. Inside the functions are
permuted only the elements that change their order within given permutation. The
ﬁrst argument represents the state of the puzzle before making the turn. The state of
the puzzle after the turn is represented by the return value of the function
(Move = H).
178
M. Musilek et al.

Function Move(H As Variant, i) As Variant 
  If i = 1 Then 
    p = H(4) 
    H(4) = H(3) 
    H(3) = H(2) 
    H(2) = H(1)  
  End If 
… similar conditions are used for i = 2, i = 3 and i = 4…
  Move = H 
End Function 
The last procedure Calculation () searches and writes (to Excel Active-
sheet) the sequence of elementary strokes that have changed just two values of the
elements, while the six element remains in original place. The procedure consist of
nine nested cycles, because it was experimentally discovered that the shortest
sequence satisfying the requirements consists nine elementary movements of the set
{A, B, C, D}. The procedure consist of nine nested cycles of type For (see http://
musilek.eu/michal/).
The procedure Calculation () can be run as macro in activesheet of Excel
workbook. The macro will ﬁll the ﬁrst two columns of the excel table and lists a
total of 96 different sequences and their corresponding resulting permutations. By
inspection the listed sequence it is evident, that a quarter of the sequences represents
three consecutive repetitive sequence of turns X, Y and Z, which can be labeled as
XYZXYZXYZ, or short (XYZ)3. For such periodically repeated sequence (three
repetitions of the same ordered strokes) there are three other sequences without any
speciﬁed regularity. Thus the number of strokes is reduced to quarter, therefore
from 96 to 24 sequences. The 24 sequences are shown in Table 1.
The strokes A and B or C and D are immediately carried in some sequences. The
order A and B (C and D) doesn’t affect each other, i.e. it does not depend on
execution of order of strokes, AB = BA and CD = DC. This allows exclude
always one of the sequences leading to the same result—such redundant sequences
are indicated in Table 1 by gray ﬁll of the cells. The number of the sequence is
reduced to 16. Generally, the commutative law cannot apply to the connection of
the permutations.
Sequences shown in Table 1, solve 16 of the possible 28 two-element combi-
nations of eight elements. There are total 28 combinations.
The remaining of given 12 combinations can be easily obtained by rotation of
the given segment to suitable place, followed by performing some of the exchanges
from Table 1 and ﬁnally returning the replaced segment back to the correct
position.
The ﬁnding the sequence of commands solving the change of any two segments
of a permutation puzzle show both, that any possible state of mixing of the seg-
ments can be solved as well as the algorithm of the solution was designed.
For practical use it is not necessary to know all the sequences listed in Table 1, but
Mathematical Modeling and Computer Simulation …
179

only four sequences can be selected, one from each group in Table 1, i.e. sequences
for:
1. replacement of elements from one circle adjacent by sides;
2. replacement of elements from one circle adjacent by vertex;
3. replacement of elements from different circles, that after appropriate shift adjoin
by side;
4. replacement of elements from different circles, that after appropriate shift adjoin
by vertex.
Computer simulation written in form of program in Visual Basic for Applica-
tions helps us to ﬁnd a set of sequences for easy solution of the permutation puzzle.
The solution is not optimal in terms of the number of steps used. Finding the
shortest solution excludes the scope of this paper.
Table 1 Sequences of brainteaser solution
Element’s exchange
Final
permutation
Sequences
Adjacent by sides
1,4
42315678
DACDACDAC
(DAC)3
2,3
13245678
CADCADCAD
(CAD)3
5,8
12348675
CBDCBDCBD
(CBD)3
6,7
12345768
DBCDBCDBC
(DBC)3
Adjacent by vertex
1,3
32145678
CDACDACDA
(CDA)3
DCADCADCA
(DCA)3
2,4
14325678
ACDACDACD
(ACD)3
ADCADCADC
(ADC)3
5,7
12347658
CDBCDBCDB
(CDB)3
DCBDCBDCB
(DCB)3
6,8
12345876
BCDBCDBCD
(BCD)3
BDCBDCBDC
(BDC)3
In different circles, after move,
adjacent by sides
1,8
82345671
ADBADBADB
(ADB)3
2,7
17345628
BDABDABDA
(BDA)3
3,6
12645378
ACBACBACB
(ACB)3
4,5
12354678
BCABCABCA
(BCA)3
In different circles, after move,
adjacent by vertex
1,7
72345618
ABDABDABD
(ABD)3
BADBADBAD
(BAD)3
2,8
18345672
DABDABDAB
(DAB)3
DBADBADBA
(DBA)3
3,5
12543678
ABCABCABC
(ABC)3
BACBACBAC
(BAC)3
4,6
12365478
CABCABCAB
(CAB)3
CBACBACBA
(CBA)3
180
M. Musilek et al.

3
Conclusion
The paper describes one of the possible way, how to provide the students of study
program Teaching of informatics by the strategy of implementation the system
approach, modeling and computer simulation to secondary school education. The
case study present student’s ﬁnal projects in subject of Modeling and simulation.
The paper presents the mathematical analysis of the permutation teaser. The model
in form of computational algorithm is designed by process chart and process table.
The simulation of the solution was programmed in Visual Basic for Application in
MS Excel that easily enables transformation of the computational algorithm to
simulation. The mathematic analysis of the permutation as well as simulation
program are suitable for development of logical thinking of students. The simula-
tion can be used as supporting tool in secondary stage of education.
Acknowledgements The paper has been supported by Speciﬁc Research Project of Faculty of
Science, University of Hradec Kralove, 2016 and by Speciﬁc Research Project of Faculty of
Education University of Hradec Kralove, 2016.
References
1. Sokolowski, J.A., Banks, C.M.: Principles of Modeling and Simulation: A Multidisciplinary
Approach. Wiley, New Jersey (2009). ISBN 978-0-470-28943-3
2. Kollmansberger, S.: Helping students build a mental model of computation. In: Proceedings of
the 15th Annual Conference on Innovation and Technology in Computer Science Education
(2010)
3. Humphreys, P.: Extending Ourselves: Computational Science, Empiricism, and Scientiﬁc
Method. Oxford University Press, Oxford (2004). ISBN 0-19-531329-1
4. Hubalovska, M., Hubalovsky, S.: Learning method for development of discovering and
creativity of pupils and students in basic education. Int. J. Educ. Inf. Technol. 10, 36–40
(2016). ISSN 2074-1316
5. National Educational Framework [online]. Praha, Institute for Educational Research in Prague
(2007). http://www.nuv.cz/ﬁle/159_1_1/
6. Hubalovsky, S.: Modeling and computer simulation of real process—solution of mastermind
board game. Int. J. Math. Comput. Simul. 6(1), 107–118 (2012). ISSN 1998-0159
7. Musilek, M., Hubalovsky, S.: Principle and computer simulation model of variation of
Delastell’s Cipher BIFID. 9, 86–194 (2015). ISSN 2074-1316
Mathematical Modeling and Computer Simulation …
181

Research of Methods of Learning
of Programming Objects-First
and Object-Later
Ondrej Korinek and Stepan Hubalovsky
Abstract There are several ways to achieve the goals. Some ways are more direct
and faster, and others are slower. Some ways lead to a goal, while others do not. It
depends on the chosen way—procedure to achieve the chosen goals. A similar
problem is experienced by teachers of programming. They often solves the fun-
damental problem of what programming style is best for introducing learning of
programming. The goal of the learning of programming is to design and creation of
object-oriented programming applications. Ways to reach the goals is several. Some
teachers begin to teach the students by object-oriented design, which is followed by
structured designs. Other teachers prefer completely opposite approach. They teach
a structured design ﬁrst followed by object-oriented constructs. These two methods
of teaching
of object-oriented programming
are
known Objects-First
and
Objects-Later techniques of learning of programming. These techniques are the
most common practices of teaching programming. The paper presents results
of research of application of above mentioned techniques to learning of
object-oriented structures. The research was conducted among the students of study
program Teaching of Education, Faculty of Science, University of Hradec Kralove
in the academic years 2013/2014 and 2014/2015.
Keywords Programming methods ⋅Objects-First ⋅Objects-Later
1
Introduction
Object-oriented programming paradigm is currently the most common method of
programs design. The main concepts are class, object instance, constructor, method,
encapsulation, polymorphism, inheritance, interfaces and design patterns [1].
O. Korinek ⋅S. Hubalovsky (✉)
University of Hradec Králové, Hradec Králové, Czech Republic
e-mail: stepan.hubalovsky@uhk.cz
O. Korinek
e-mail: ondrej.korinek@uhk.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_22
183

Object-oriented programming can be taught in different ways. There are known
large number of different learning methods for implementation concepts of
object-oriented programming.
The well-known methods are Objects-First, Objects-Later, Model-First or
Design Patterns First.
The most common methods of learning of programming are Objects-First,
Objects and-Later, respectively. Although these methods are methodologically
different they are complementary in programming practice and using some basic
programming terms, e.g. the variable data type, basic control design, analysis,
implementation, etc.
2
Theoretical Background
2.1
Objects-First Method of Learning of Programming
The basic object-oriented constructions are ﬁrst explained in the Object-First
method of learning of programming. The structured constructions are explained
later. The structured constructions are sequences, conditions and cycles. Structured
construction are taught based on more complex, object-oriented projects.
The disadvantages of this method of teaching of programming are as follows:
• Abstraction of object structures;
• A large number of new concepts (object-oriented concepts, description of the
development environment);
• Low attractiveness of discussed examples;
• Complexity of object-oriented design.
Despite the mentioned disadvantages the method is common method of learning
of programming. Structured design can be better understood on more complex and
more practical examples.
2.2
Objects-Later Method of Learning of Programming
The basic structured constructions (sequence, condition and cycle) are explained in
the object-later method of learning of programming ﬁrst followed by learning of
object structures. Structured constructions are explained based on simple examples.
The structured examples do not have the practical applications. It may be for some
students unattractive and discouraging. The disadvantages of the method of
teaching of programming are as follows:
• Small attractiveness of training examples
• Simple artiﬁcial training examples;
184
O. Korinek and S. Hubalovsky

• Little time for training of object-oriented constructions;
• Misunderstanding of object structures;
• Gaining bad habit of thinking in the design of programs.
Despite the mentioned disadvantages the method is common method of learning
of programming.
3
Research of Objects-First and Object-Later Methods
in Learning of Programming at University of Hradec
Kralove
Many authors deal with the comparison of Objects-First and Objects-Later learning
methods of programming [2, 3]. A potential problem of the previous researches, can
be e.g. the separation of students into groups as well as the methods of veriﬁcation
of student knowledge. Despite the diversity of above mentioned researches, the
authors
came
to
similar
conclusions:
The
learning
methods
Objects-First
resp. Objects-Later show no noticeable differences.
The research of the Objects-First and Objects-Later methods in Learning of
Programming was conducted at Faculty of Science University of Hradec Kralove in
the academic years 2013/2014 and 2014/2015. The partial results were published
in the papers [4–6]. The research was conducted among students of the ﬁrst grade in
the subject Programming 1.
3.1
Comparison of the Learning Methods of Programming
Students were divided into two homogeneous groups in the subject Programming 1
according to the results of the course Algorithms and Data Structures (hereinafter
ALGDS). The detailed information can be found in [7, 8]. Students pass Post-test,
Re-test 1 and Re-test 2 Post-test is past in the middle of the course. Re-test 1 is past
at the end of the course. Re-test 2 can be past only by the student who have
succeeded in the Post-test and Re test 1. Post-test and Re-test 1 consist of similar
tasks based on the curriculum listed in Table 1. The curriculum is the same in both
groups of students after Post-test - see Table 2. The tests consist of a theoretical part
as well as practical part. Both parts are divided into structured constructions as well
as object-oriented constructions. The object-oriented part of test consists of the
following tasks:
1. Deﬁnition constructor data items and properties
2. Deﬁnition of methods
3. Create an instance method calls and use the program
The groups were taught by different teachers. The results of student’s tests can
be inﬂuenced by the personality of the teacher as well as different teaching methods.
Research of Methods of Learning of Programming …
185

The regular meeting between the teachers were organized to minimized mentioned
factors.
The result of the research presented in this paper compare the results of the exam
test of students taught by Objects-First method with the results of the exam test of
students taught by Objects-Later method of learning of programming. The
Objects-First group of student pass the subject Programming 1 in academic year
2013/14 while the Objects-Later group of student pass the subject Programming 1
in academic year 2014/15. Comparing the results of the exam tests passed in
different academic years, may involve certain disadvantages:
• Comparability of the groups;
• Other skills of students;
• Various experience of students with subject of Programming 1.
The above mentioned disadvantages are minimized by entrance test students
(Pre-test) in the subject ALGDS, based on the students are divided into the groups.
Testing of the students is based on a variety of test tasks.
3.2
The Results of Research—Evaluation of Post-test
of Object First Gourp and Object Later Groups
of Student
The Post test results of students were compared between two groups of students:
1. The ﬁrst group of students learned by Objects-First method in the academic year
2013/2014 (consists of 14 students);
2. The second group of students learned by Objects-Later method in the academic
year 2014/2015 (consists of 18 students).
Table 1 Curriculum of the
course Programming 1 before
Post-test
Objects-Later
Objects-First
Basic commands
Fundamentals of OOP in C #
One-dimensional array
Basic commands
Matrix
One-dimensional array
Square matrix
Matrix
Fundamentals of OOP in C #
Square matrix
Table 2 Curriculum of the
course Programming 1 after
Post-test
Objects-Later
Objects-First
Sorting algorithms
Sorting algorithms
Working with ﬁles
Working with ﬁles
Binary ﬁles, ﬁle system
Binary ﬁles, ﬁle system
Program parameters
Program parameters
186
O. Korinek and S. Hubalovsky

The Table 3 shows the distribution of Pre-test results of students of both groups.
The Table 3 shows that the better results are achieved by students of
Objects-Later group. It can be caused by less number of students in the
Objects-First group in the academic year 2013/2014.
To determine whether the median of the Post-test results of student is the same
for both group of students the nonparametric Mann-Whitney test was used. Table 4
Table 3 Distribution of Pre-test results of students of Object-First group and Object-Later group
Grade of pre-test
Objects-First (2013/2014)
Objects-Later (2014/2015)
1
5
7
2
1
3
3
5
5
4
3
3
Total
14
18
Table 4 Statistical evaluation of results of Post-test
Objects-First group
Objects-Later group
Total results
Value p—Mann—Whitney
1
Rejection of the null hypothesis
No
Statistically signiﬁcant difference
No
Average test result of the students
46.3%
47.7%
Object part—total results
Value p—Mann—Whitney
0.425
Rejection of the null hypothesis
No
Statistically signiﬁcant difference
No
Average test result of the students
44.2%
29.1%
Object part—constructor
Value p—Mann—Whitney
0.143
Rejection of the null hypothesis
No
Statistically signiﬁcant difference
No
Average test result of the students
55.4%
30.4%
Object part—methods
Value p—Mann—Whitney
0.805
Rejection of the null hypothesis
No
Statistically signiﬁcant difference
No
Average test result of the students
35.9%
31.3%
Object part—instances
Value p—Mann—Whitney
0.115
Rejection of the null hypothesis
No
Statistically signiﬁcant difference
No
Average test result of the students
50.9%
25.0%
Research of Methods of Learning of Programming …
187

shows the results of non-parametric test of the total test section as well as object test
section. Object test section is divided into three subsections. The Table 4 shows:
• The calculated p-value non-parametric test;
• The evaluation whether the null hypothesis is rejected (based on value of p);
• The decision, whether between both groups of students is statistically signiﬁcant
differences;
• The average test result of the students;
The signiﬁcance level was set α = 0.05.
The results of the Table 4 shows that the ﬁrst group of student (Object First
group) has (as expected) better results in object test section of the Post-test (despite
their worse results in the Pre-test). The statistical difference between the groups is
not signiﬁcant. The students of Objects-First group have better results in all three
parts of object sub-sections. Both groups have roughly similar results in the deﬁ-
nition of methods. On the other hand the students of Object-Later group have better
overall results of the Post-test. Unfortunately the statistical results are partly
inﬂuenced in the Objects-Later group by two students who were unsuccessful in the
subject Programming 1 in the previous academic year (they pass the subject Pro-
gramming 1 for the second time). Research eliminating the effect of the repeated
students exceeds the scope of this paper and it will be published later.
4
Conclusion
The paper characterized by two methods of learning of programming Objects-First
and Objects-Later. The result of the research conducted at the Faculty of Science,
University of Hradec Kralove in the academic years 2013/2014 and 2014/2015
conﬁrm the results of other researches [1–3]—Object First and Object Later
learning methods in programming show no noticeable differences. The research
presented in the paper minimize the impact of the teacher (both groups were taught
by the same teacher) on the other hand the research were inﬂuenced by different
groups of students as well as by repeated students.
Acknowledgements The paper has been supported by Speciﬁc Research Project of Faculty of
Science, University of Hradec Kralove, 2016 and by Speciﬁc Research Project of Faculty of
Education University of Hradec Kralove, 2016.
References
1. Pecinovský, R.: Současné trendy v metodice výuky programování. http://vyuka.pecinovsky.cz/
prispevky/ (2006)
2. Ehlert, A., Schutle, C.: Empirical comparison of Objects-First and Objects-Later. In:
Proceedings of the Fifth International Workshop on Computing Education Research
188
O. Korinek and S. Hubalovsky

Workshop, ICER ’09, s. 15–26. New York: ACM. https://home.cc.gatech.edu/csed/uploads/2/
ehlert2009.pdf (2009). ISBN 978-1-60558-615-1
3. Jonson, R.A., Moses, D.R.: Object-ﬁrst vs. structures-ﬁrst approaches to OO programing
education: an empirical study. In: Allied Academies International Conference, Reno, s. 244–
248. http://www.researchgate.net/publication/ (2008)
4. Hubálovský, Š., Kořínek, O.: Algorithmic thinking in paradigms of programming. In:
Proceedings of the Education and Modern Educational Technologies (EMET 2015). IEEE,
Piscataway (2015). ISBN: 978-1-61804-322-1. ISSN: 2227-4618
5. Hubálovský, Š., Kořínek, O.: Object thinking in paradigms of programming. In: Proceedings of
the Education and Modern Educational Technologies (EMET 2015). Piscataway: IEEE (2015).
ISBN: 978-1-61804-322-1. ISSN: 2227-4618
6. Hubálovský, Š., Kořínek, O.: Evaluation of algorithmic thinking of students using control
testing environment. Int. J. Educ. Inf. Technol. North Atlantic University Union (2015). ISSN:
2074-1316
7. Milková, E., Kořínek, O.: Students’ programming capabilities evaluation. In: Proceedings of
the Efﬁciency and Responsibility in Education (ERIE 2013), Prague, pp. 434–440 (2013)
8. Milková, E., Kořínek, O.: Future ICT teachers—programming aptitude. In: Proceedings of the
11th International Conference Efﬁciency and Responsibility in Education (ERIE 2014), Prague,
pp. 456–462 (2014)
Research of Methods of Learning of Programming …
189

Multichannel Queueing Systems and Their
Simulation
Miloš Šeda, Jindřiška Šedová and Miroslav Horký
Abstract This paper is concerned with multichannel queueing systems showing
how to derive their characteristics if the requirement arrivals correspond to a
Poisson process and the service times have the exponential distribution. However,
the requirements of stationarity, regularity, and independence of increases needed to
model these processes by Markov chains and to deﬁne the transition probabilities
may not be satisﬁed, or no information may be available on such parameters. Using
randomly generated data, we propose a strategy of processing the requirements in
multichannel systems and a way of evaluating the probabilities necessary to express
the characteristics of the systems comparing these results with the theoretical ones.
It has been discovered that with, as the number of outputs increases, the simulation
results converge to the theoretical ones.
Keywords Queueing system ⋅Poisson process ⋅System transition
1
Introduction
The Danish mathematician A.K. Erlang formulated the fundamentals of the
queueing theory about hundred years ago, but its current classiﬁcation was pro-
posed by the English mathematician D.G. Kendall. All details may be found, e.g.,
in [1–4, 6, 7].
Generally, at random moments, customers (requirements) enter the system and
require servicing. Service options may be limited, e.g., the number of service
M. Šeda (✉) ⋅M. Horký
Faculty of Mechanical Engineering, Brno University of Technology,
Technická 2, 616 69 Brno, Czech Republic
e-mail: seda@fme.vutbr.cz
J. Šedová
Faculty of Economics and Administration, Masaryk University, Lipová 41a,
602 00 Brno, Czech Republic
e-mail: jsedova@econ.muni.cz
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_23
191

channels (or service lines). If at least one serving line is empty, the demand entering
the system is immediately processed. However, the service time is also random in
nature because the performance requirements may vary. If all service lines are busy,
then the requirements (customers) must wait for their turn in a queue for the
processing of previous requirements, or be rejected.
Here, we restrict our considerations on systems of the form A/B/C/D/E/F, where
A. characterises the probability distribution of random variable period (interval)
between the requirement arrivals to the system,
B. the probability distribution of random variable service time of a requirement,
C. is the number of parallel service channels, in the case of “unrestricted” (i.e. very
large) number of channels is usual to express the parameter C by ∞,
D. integer indicating the maximum number of requirements in the system (i.e. the
capacity of the system), unless explicitly restricted, expressed by ∞,
E. integer expressing the maximum number of requirements in the input stream (or
in a resource requirements), if it is unlimited, ∞is used,
F. queue type (FIFO/LIFO/SIRO/PRI).
Let us ﬁrst assume that parameters A and B equal to M, i.e. intervals between the
arrivals of requirements and requirement-service-time are mutually stochastically
independent and have exponential distribution, this means that the input stream
represents a Poisson (Markov) process, that satisﬁes the following properties:
(1) Stationarity (homogeneity over time)—the number of events in equally long
time intervals is constant; (2) Regularity—the probability of more than one event at
a sufﬁciently small interval of length Δt is negligibly small. This means that in, the
interval (t, t + Δt), there is either exactly one event with probability λ Δt or no
event with probability 1 – λ Δt. In other words, in a Poisson process, the only
system transition to the next “higher” state is possible or the system remains in the
same condition; (3) Independence of increases—the number of events that occur in
one time interval does not depend on the number of events in other intervals.
2
The M/M/n/n/∞/FIFO System
To derive the characteristics of the system, it is convenient to describe the system
activity by a graph of system transitions. The nodes of the graph represent the states
and the directed edges transitions from one state to another. The evaluation of these
edges is described by the probability of transition from one state to another. State Sn
or more speciﬁcally, Sn(t) for ﬁxed t ∈〈0, ∞), is a random variable and indicates
that, at time t, n requirements are in the system. If m requirements, m > n > 1, are
in the system M/M/n/n/∞/FIFO, then each requirement is operating in a service
channel and the remaining m-n are waiting in the queue. Transitions between states
that differ in the number of requirements in a system can be understood as a process
of birth and death where the requirement birth represents the requirement entry into
192
M. Šeda et al.

the system and death corresponds to a requirement leaving from the system after
being processed.
We assume a Poisson stream of requirements with a parameter λ and an expo-
nential distribution of service time with parameter μ, and the queueing system
behaviour described by the Markov processes. Due to the regularity, only transition
probabilities P(Si →Sj), where either i = j or i and j differ by 1 have sense.
Using the regularity property and the method of calculating the total probability
and neglecting the powers of the interval length Δt, from the partial probabilities of
conjunction and disjunction of independent events, we get the transition
probabilities.
For example, transition probability P(S0 →S0) corresponds to the probability of
the event that, during the time interval of length Δt, no requirement enters the
system, transition probability P(Sk →Sk−1), n > k ≥1, is the probability of the
event that, during the time interval of length Δt, no requirement enters the system
and at the same time one requirement will be served and leaves the system, tran-
sition probability P(Sk →Sk), k ≥1, is equal to the probability of the event that,
during the time interval of length Δt, no requirement enters the system and no
requirement leaves the system, or, during this interval, one requirement enters and
one requirement will be served, etc. Let us summarise the previous considerations:
PðSk −1 →SkÞ = λ Δt,
k = 1, . . . , n
ð1Þ
PðSk →SkÞ = ð1 −λ ΔtÞð1 −kμ ΔtÞ≈1 −ðλ + kμÞΔt,
k = 0, . . .
ð2Þ
PðSk + 1 →SkÞ = ðk + 1Þμ Δt,
k = 0, . . . , n −1
ð3Þ
Let pk(t) denote the probability that, at time t, just k requirements are in the
system. Using the previous equations, we can calculate p0(t), p1(t), …, pk(t), …,
pn(t).
p0ðt + ΔtÞ = PðS0 →S0Þ + PðS1 →S0Þ = p0ðtÞ.ð1 −λ ΔtÞ + p1ðtÞ ⋅μ Δt
ð4Þ
p1ðt + ΔtÞ = PðS0 →S1Þ + PðS1 →S1Þ + PðS2 →S1Þ
= p0ðtÞ ⋅λ Δt + p1ðtÞ ⋅½1 −ðλ + μÞ Δt + p2ðtÞ ⋅2μ Δt
. . .
ð5Þ
pkðt + ΔtÞ = PðSk −1 →SkÞ + PðSk →SkÞ + PðSk + 1 →SkÞ
= pk −1ðtÞ ⋅λ Δt + pkðtÞ ⋅½1 −ðλ + kμÞΔt + pk + 1ðtÞ ⋅ðk + 1Þμ Δt,
k = 2, . . . , n −1
ð6Þ
However, if all channels are occupied and the queue is nonempty, the last
equation changes to (7).
Multichannel Queueing Systems and Their Simulation
193

pkðt + ΔtÞ = pk −1ðtÞ ⋅λ Δt + pkðtÞ ⋅½1 −ðλ + nμÞ Δt + pk + 1ðtÞ ⋅nμ Δt,
k ≥n ð7Þ
After easy simpliﬁcation of Eqs. (4), (6) and (7), a limit transition for Δt →0
we get a set of ﬁrst-order ordinary differential equations. Since the initial conditions
may also be simply expressed, we can derive that
p0 =
∑
n −1
k = 0
ψk
k! + ψn
n!
1
1 −ψ
n

 −1
ð8Þ
pk = ψk
k! p0, k = 1, . . . , n −1
ð9Þ
pk = ψk
n!
nn
nk p0, k ≥n
ð10Þ
These equations may now be used to derive other important characteristics of the
M/M/n/n/∞/FIFO system, which include:
1. Mean number of requirements in the system:
EðNsÞ = ns = ∑
∞
k = 0
k pk
ð11Þ
2. Mean number of requirements in the queue (mean queue length):
EðNf Þ = nf = ∑
∞
k = n
ðk −nÞpk
ð12Þ
3. Mean number of free service channels:
EðNcÞ = nc = ∑
n −1
k = 0
ðn −kÞpk
ð13Þ
4. Mean time spent by a requirement in the system:
EðTsÞ = ts = ns
λ
ð14Þ
194
M. Šeda et al.

5. Mean waiting time of a requirement in the queue:
EðTf Þ = tf = nf
λ
ð15Þ
3
Simulation of Queueing Processes
As, in practice, some assumptions may not be satisﬁed, particularly the Poisson
(Markov) process properties of stationarity and the independence of increases, such
as the number of clients in shops and railway stations substantially changing during
the daytime, the formulas that we have derived, may not be entirely accurate.
However, queueing systems can also be studied by Monte Carlo simulations, which
generate random numbers representing the moment of the requirements entering
into the system and the service time.
In Fig. 1, a queuing system with two service channels, 15 requirements, and the
FIFO queue type is considered. We can see that for 2 + 4 = 6 min from total
70 min there is no requirement in the system, that means
p0 = ⟨903, 905⟩+ ⟨930, 934⟩
⟨900, 1010⟩
= 2 + 4
70
= 6
70 = 0.086
Fig. 1 System with two channels
Multichannel Queueing Systems and Their Simulation
195

Similarly, we estimate p1, …, p5 and compute characteristics (11)–(15).
In [5], the M/M/n/n/∞/FIFO system was implemented in MATLAB using
simulation data from a supermarket.
It makes it possible to enter λ (mean intensity of the input), μ (mean service
intensity), the number of service lines n (then it is checked if λ/nμ < 1), and the
number
of
requirements.
For
these
data,
the
probabilities
pk(t)
and
the
above-mentioned characteristics are computed.
To understand the behaviour of the system, the program also offers graphical
output of simulations. In Fig. 2, four graphs are shown for a system with three lines,
which show requirement arrival times, requirement service times, requirement
waiting times for service and ﬁnishing times of services for these requirements.
Now we compare the analytical solution with the values obtained by simulation
for different numbers of requirements. In the analytical part, we use formulas
(8)–(10), and the corresponding characteristics (11), etc. Simulations were run for
50, 200, and 500 requirements (clients). Table 1 sums up the results of the ana-
lytical formulas and simulations.
We can see that, if the number of requirements increases, then the difference
between the analytical and the simulation results decreases. For 50 requirements,
the difference is about 12%, but for 500 requirements, only 5%. Based on these
achievements, we can conclude that the computer implementation of the simulation
model reasonably approximates the M/M/n/n/∞/FIFO system.
Fig. 2 Simulation of the M/M/n/n/∞system for λ = 45, μ = 18, n = 3, and 45 requirements
196
M. Šeda et al.

4
Conclusions
This paper describes an approach to modelling a queuing system with the use of
Markov process properties and, for an M/M/n/n/∞/FIFO system, it derives its
characteristics in detail. These derivations are based on the assumptions of sta-
tionarity, regularity, and independence of Markov processes.
In real situations, some of the assumptions may not be satisﬁed, particularly the
stationarity and the independence of increases, or even the distribution of stochastic
variables may not be known at all. For these reasons, the calculations of transition
probabilities that do not take this fact into account may give imprecise results.
Therefore, we propose a simulation approach, a strategy of requirement processing
implemented in MATLAB based on the number of service lines, and a way of
computing the characteristics from time intervals with the same number of
requirements. However, the approximation of a theoretical model by the simulation
model using real or randomly generated data depends on the number of require-
ments (and, therefore, on the time horizon length of the whole processing). The
higher the number of requirements, the more the simulation results match the
theoretical ones.
References
1. Bolch, G., Greiner, S., Meer, H., Trivedi, K.S.: Queueing Networks and Markov Chains.
Wiley, New York (2006)
2. Bose, S.K.: An Introduction to Queueing Systems. Springer, Berlin (2001)
3. Cooper, R.B.: Introduction to Queueing Theory. North Holland, New York (1981)
4. Gross, D., Shortle, J.F., Thompson, J.M., Harris, C.M.: Fundamentals of Queueing Theory.
Wiley, New York (2008)
5. Horký, M.: Models of Queueing Systems. Diploma Thesis, Brno University of Technology,
Brno (2015)
6. Šeda, M.: Models of Queueing Systems. Acta Logistica Moravica 1, 16–33 (2011)
7. Virtamo, J.: Queueing Theory. Lecture Notes. Helsinki University of Technology (2005)
Table 1 A comparison of analytical and simulation results
Analytical evaluation mean values
Simulation results number of
requirements
50
200
500
E(Ts)-E(Tf)
3.33333
3.64353
3.5118
3.45478
E(Tf)
4.68165
3.06941
4.17567
5.21177
E(Ts)
8.01498
6.71293
7.68747
8.66655
E(Ns)
6.01124
5.09125
5.62803
6.23332
E(Nf)
3.51124
2.30994
3.05703
3.74851
E(Ns)-E(Tf)
2.5
2.78131
2.57101
2.48481
Multichannel Queueing Systems and Their Simulation
197

On Computational Evaluation of Stress
Concentration Using Micropolar Elasticity
Victor A. Eremeyev, Andrzej Skrzat and Feliks Stachowicz
Abstract We discuss the implementation the ﬁnite element approach to the linear
micropolar elasticity in order to perform the analysis of the stress concentration near
holes and notches. Within the micropolar elasticity we analyze the behaviour of
such microstructured solids as foams and bones. With developed new ﬁnite element
few problems are analyzed where the inﬂuence of the microstructure may be
important. The provided comparison of solutions obtained within the micropolar
and classical elasticity show the inﬂuence of micropolar properties on stress con-
centration near notches and contact areas.
Keywords Cosserat continuum ⋅Micropolar elasticity ⋅FEM ⋅Stress con-
centration ⋅Porous media
1
Introduction
Nowadays the interest grows to extended models of continuum mechanics in order
to model micro- and nanostructured materials which demonstrate complex inner
structure. Among these generalized models there are the surface elasticity,
micropolar or Cosserat continua, microstretched and micromomorphic media,
surface elasticity, media with internal variables, gradient elasticity, etc. In partic-
ular, the micropolar model [6, 7] proposed by Cosserat brothers more than hundred
years ago found applications for modeling such materials as porous solids,
bones, masonries, composites, see [3, 8, 10, 12, 13] and reference therein.
V.A. Eremeyev (✉) ⋅A. Skrzat ⋅F. Stachowicz
Faculty of Mechanical Engineering and Aeronautics, Rzeszow University of Technology,
al. Powstańców Warszawy 8, 35959 Rzeszów, Poland
e-mail: veremeyev@prz.edu.pl
A. Skrzat
e-mail: askrzat@prz.edu.pl
F. Stachowicz
e-mail: stafel@prz.edu.pl
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_24
199

Within the Cosserat continuum model the translations and rotations determine the
kinematics of the medium and the stress and couple stress tensors are introduced.
The micropolar properties of material may be important near singularities or to
describe observed experimentally size-effect [1, 9, 11]. Let us also note that for such
complex media even more general models of continuum mechanics may be useful,
see for example [2]. Effective solution of boundary value problems requires
advanced numerical code such as the ﬁnite element method. For the moment
commercial FEM software gives the possibility to use user deﬁned elements and
user deﬁned procedure for implementation of nonstandard material models. Here
we developed new ﬁnite element and implement it in ABAQUS.
The paper is organized as follows. In Sect. 2 we present the basic equations of
the linear micropolar elasticity. The equilibrium equations, static and kinematic
boundary conditions and constitutive equations are given. In Sect. 3 we discuss the
ﬁnite element modelling for the micropolar solids.
2
Basic Governing Equations of the Micropolar Elasticity
Following [6, 7] we recall here the basic equations of the linear micropolar elas-
ticity of isotropic solids. The kinematic of a micropolar solid is described by two
ﬁelds that the ﬁeld of translations ui and the ﬁeld of rotations θi. The latter is
responsible for the description of moment interactions of the material particles.
Hereinafter the Latin indices take on values 1, 2, or 3 and we use the Einstein
summation rule over repeating indices. The equilibrium equations take the form
tji, j + fi = 0, mji, j + eimntmn + ci = 0,
ð1Þ
where tij and mij are the stress and couple stress tensors, respectively, eijk is the
Levi-Civita third-order tensor, and fi and ci are external forces and couples. Unlike
the linear elasticity tensors tij and mij are not symmetric. Equations (1) constitute the
local balance of momentum and moment of momentum, respectively.
The static and kinematic boundary conditions have the following form
nitij

At = ϕj, nimij

At = ηj, uijAu = u0
i , θijAu = θ0
i ,
ð2Þ
where ni is the unit vector of external normal to the boundary A = At ∪Au, ϕj and ηj
are external forces and couples given on At, and ui
0 and θi
0 are given on Au surface
ﬁelds of translations and rotations, respectively.
In what follows we are restricting ourselves by isotropic case. Using the Voigt
notation modiﬁed for the micropolar elasticity and introducing the stress and
moment stress vectors with stretch and wryness vectors by the formulae
200
V.A. Eremeyev et al.

T
f g = t11, t22, t33, t12, t21, t23, t32, t13, t31
f
gT,
ð3Þ
M
f
g = m11, m22, m33, m12, m21, m23, m32, m13, m31
f
gT,
ð4Þ
E
f g = ε11, ε22, ε33, ε12, ε21, ε23, ε32, ε13, ε31
f
gT,
ð5Þ
K
f g = κ11, κ22, κ33, κ12, κ21, κ23, κ32, κ13, κ31
f
gT,
ð6Þ
we represent the constitutive equations in the following uniﬁed form
σM
f
g = C
½  εM
f
g, C
½  =
A
0
0
B


,
σM
f
g =
T
M


,
εM
f
g =
E
K


,
ð7Þ
with 18 × 18 stiffness matrix [C], where A and B are three-diagonal 9 × 9
matrices which is not shown here. The values of used micropoalar elastic moduli
can be found from direct experiments provided in [10, 13] or using homogenization
[3, 8, 12]. Analysis of general constitutive equations of anisotropic micropolar
solids is performed in [4, 5, 7].
3
Implementation of Micropolar Elasticity
into Commercial Code
In order to solve various boundary value problems a special 8-node micropolar
isoparametric ﬁnite element has been developed in the form of UEL (user element)
procedure for commercial ABAQUS program. Requirements considering the UEL
procedure in ABAQUS program are very restricted. UEL procedure is called twice
for each ﬁnite element and for each Gaussian point. In the ﬁrst call the element
stiffness is welcomed. Very often UEL procedure calls UELMAT (user material
procedure) necessary to obtain the relation between stress and strain tensors (or
between stress and strain increments). Another call of UEL procedure is necessary
to compute residual forces—element nodal forces resulting from element stresses—
which essential in the monitoring of convergence in nonlinear problems. This
second call may also require another call of UELMAT procedure. The typical call
of UEL procedure is presented below
On Computational Evaluation of Stress Concentration …
201

The most important parameters are amatrx (element stiffness matrix) and rhs
(right-hand side vector of residuals necessary to check the convergence of com-
putation). In the most simple case of static linear analysis the rhs parameter can be
omitted because the convergence (equilibrium equations at nodes) is achieved in the
ﬁrst iteration. The meaning of other parameters is described in ABAQUS docu-
mentation. UEL procedure can be slightly simpliﬁed if contains constitutive
properties—the call of UELMAT procedure is not necessary, therefore. Short
description of micropolar elasticity 8-node isoparametric element implementation
consists of several steps
• In the loop over the Gaussian points (2 × 2 × 2 Gaussian procedure is used)
1. Find the shape functions at each Gaussian point and their derivatives with
respect to natural coordinates
2. Find the Jacobian matrix, numerically compute its inverse and determinant
3. Find the shape functions derivatives with respect to Cartesian coordinates x,
y, z
4. Find the matrix of shape functions derivatives D (the relation between strain
components and nodal displacements and microrotations)
5. Find the constitutive matric C (the relation between stress and strain tensors)
6. Compute DTCD (for the 2 point Gaussian quadrature all weights are equal to
ones) and add it as the contribution to the stiffness matrix which uses the
determinant of Jacobian matrix as the multipier
• Find the element nodal forces resulting from element stresses and subtract them
from the external nodal forces in order to compute residuals (necessary to check
the rate of the solution convergence)
• Make other computations e.g. update the strain energy
The whole procedure is typical for the isoparametric ﬁnite element formulation
in 3D problems with the exception of considered degrees of freedom (in micropolar
elasticity there are three displacement components and additionally three micro-
rotations) and strain and stress measures (strain and stress tensors contain more
components and are not symmetric). In UEL implementation of micropolar elas-
ticity the same shape functions are used for displacement and microrotations
Niðξ, η, μÞ = 1
8 1 + ξξi
ð
Þ 1 + ηηi
ð
Þ 1 + μμi
ð
Þ,
where ξi, ηi and µi are the nodal coordinates in the local coordinate system. For the
Eringen strain measure the matrix of the shape functions derivatives is:
202
V.A. Eremeyev et al.

εxx
εyy
εzz
εxy
εyx
εyz
εzy
εxz
εzx
κxx
κyy
κzz
κxy
κyx
κyz
κzy
κxz
κzx
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
;
=
f1, x
0
0
0
0
0
. . .
f8, x
0
0
0
0
0
0
f1, y
0
0
0
0
. . .
0
f8, y
0
0
0
0
0
0
f1, z
0
0
0
. . .
0
0
f8, z
0
0
0
0
f1, x
0
0
0
f1
. . .
0
f8, x
0
0
0
f8
f1, y
0
0
0
0
−f1
. . .
f8, y
0
0
0
0
−f8
0
0
f1, y
f1
0
0
. . .
0
0
f8, y
f8
0
0
0
f1, z
0
−f1
0
0
. . .
0
f8, z
0
−f8
0
0
0
0
f1, x
0
−f1
0
. . .
0
0
f8, x
0
−f8
0
f1, z
0
0
0
f1
0
. . .
f8, z
0
0
0
f8
0
0
0
0
f1, x
0
0
. . .
0
0
0
f8, x
0
0
0
0
0
0
f1, y
0
. . .
0
0
0
0
f8, y
0
0
0
0
0
0
f1, z
. . .
0
0
0
0
0
f8, z
0
0
0
f1, y
0
0
. . .
0
0
0
f8, y
0
0
0
0
0
0
f1, x
0
. . .
0
0
0
0
f8, x
0
0
0
0
0
f1, z
0
. . .
0
0
0
0
f8, z
0
0
0
0
0
0
f1, y
. . .
0
0
0
0
0
f8, y
0
0
0
f1, z
0
0
. . .
0
0
0
f8, z
0
0
0
0
0
0
0
f1, x
. . .
0
0
0
0
0
f8, x
2
6666666666666666666666666666666666666664
3
7777777777777777777777777777777777777775
u1
x
u1
y
u1
z
θ1
x
θ1
y
θ1
z
. . .
u8
x
u8
y
u8
z
θ8
x
θ8
y
θ8
z
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
;
,
where ui
j and θi
j are the i-th displacement or microrotation components in the j-th
node, respectively.
4
Examples
Using the presented above ﬁnite element we analyzed few 3D static problems for
solids with certain singularities such as notch or small contact area. For comparison
with the linear elasticity let us note that the micropolar elasticity provides the same
solutions if all microrotations are ﬁxed, or if only two material data are used (shear
or Young’s modulus and Poisson ratio) while the other material data are assumed to
be zeros.
In Fig. 1 the stressed state in the rectangular plate with two notches under
tension is presented with comparison with commercial software for classic elas-
ticity. One can see that excellent convergence of the distribution of stress and stress
magnitudes (not shown for micropolar software) is achieved. It proves that the
results obtained be the UEL procedure are reliable.
Another benchmark test considers the Hertzian contact between the cylinder and
the ﬂat plate which is an example of the nonlinear problem. The main problem
arising in this type of analysis is that ABAQUS program does not recognize the
volumes and faces of user elements—user element is represented by the set of
nodes. The only possible contact formulation is the contact between the surface
(ordinary ﬁnite elements implemented in ABAQUS) and the set of nodes
On Computational Evaluation of Stress Concentration …
203

(UEL elements), therefore. In this benchmark test the cylinder is modelled with the
micropolar user elements, while the plate is made of ordinary 3D ﬁnite elements.
Surface-nodes contact formulation is less precise than the contact between two
surfaces, but reasonable results can be achieved for sufﬁciently dense meshes.
Obtained results conﬁrmed well known Hertz formulas. In Fig. 2 the distribution of
couple stress myy (y is the vertical axis) in the vicinity of the contact zone is shown.
5
Conclusions
The new 8-node hybrid micropolar isoparametric element and its implementation in
ABAQUS is discussed. We analyzed few 3D benchmark problems of the
micropolar elasticity. Comparison of classical and micropolar solutions is carefully
discussed. Numerical tests have shown that couple stress appears almost in the
vicinity of singularities that is near notches and contact area. In other words the
inﬂuence of the micropolar properties is almost related to singularities of size of
the same order as length-scale parameters of the micropolar elasticity.
Acknowledgements Authors acknowledge the support by the People Program (Marie Curie ITN
transfer) of the European Union’s Seventh Framework Programme for research, technological
development and demonstration under grant agreement No PITN-GA-2013-606878.
Fig. 1 Comparison of von Mises stress (Pa) obtained by use of commercial software (left) and
micropolar user procedure (right)
Fig. 2 The distribution of
couple stress myy near the
contact zone
204
V.A. Eremeyev et al.

References
1. Ariman, T.: On the stresses around a circular hole in micropolar elasticity. Acta Mech. 4(3),
216–229 (1967)
2. dell’Isola, F., Giorgio, I., Pawlikowski, M., Rizzi, N.: Large deformations of planar extensible
beams and pantographic lattices: heuristic homogenization, experimental and numerical
examples of equilibrium. Proc. R. Soc. A 472(2185), 20150790 (2016)
3. Dos Reis, F., Ganghoffer, J.: Construction of micropolar continua from the asymptotic
homogenization of beam lattices. Comput. Struct. 112, 354–363 (2012)
4. Eremeyev, V.A., Pietraszkiewicz, W.: Material symmetry group of the non-linear polar-elastic
continuum. Int. J. Solids Struct. 49(14), 1993–2005 (2012)
5. Eremeyev, V.A., Pietraszkiewicz, W.: Material symmetry group and constitutive equations of
micropolar anisotropic elastic solids. Math. Mech. Solids 21(2), 210–221 (2016)
6. Eremeyev, V.A., Lebedev, L.P., Altenbach, H.: Foundations of Micropolar Mechanics.
Springer, Heidelberg (2013)
7. Eringen, A.C.: Microcontinuum Field Theory. I. Foundations and Solids. Springer, New York
(1999)
8. Goda, I., Ganghoffer, J.F.: Identiﬁcation of couple-stress moduli of vertebral trabecular bone
based on the 3d internal architectures. J. Mech. Behav. Biomed. Mater. 51, 99–118 (2015)
9. Kaloni, P.N., Ariman, T.: Stress concentration effects in micropolar elasticity. ZAMP 18(1),
136–141 (1967)
10. Lakes, R.: Experimental microelasticity of two porous solids. Int. J. Solids Struct. 22(1),
55–63 (1986)
11. Salehi, S.H., Salehi, M.: Numerical investigation of nanoindentation size effect using
micropolar theory. Acta Mech. 225(12), 3365–3376 (2014)
12. Trovalusci, P., Ostoja-Starzewski, M., De Bellis, M.L., Murrali, A.: Scale-dependent
homogenization of random composites as micropolar continua. Eur. J. Mech. A/Solids 49,
396–407 (2015)
13. Yang, J., Lakes, R.S.: Experimental study of micropolar and couple stress elasticity in
compact bone in bending. J. Biomechanics 15(2), 91–98 (1982)
On Computational Evaluation of Stress Concentration …
205

An Algorithm for Edge Detection
of the Image for Application in WSN
Adrian Shehu, Astrit Hulaj and Xhevahir Bajrami
Abstract The conversion of the image in black and white image is a very
important factor in the case of WSNs. This conversion affects the image size
reduction. In other words, it affects energy saving and bandwidth transmission of
sensor nodes. However, in this case, we can have a loss of details of image char-
acteristics. With purpose of preserving the image characteristics, the edge detection
as accurately as possible is a key factor. So, in this paper, we will present a new
algorithm which enables the efﬁcient realization of pixels detection that corresponds
to the edges of the image captured by the sensor nodes. The results obtained with
the application of this algorithm will be compared with results obtained from the
application of traditional Filters.
Keywords Edge detection ⋅Sensor ⋅Image ⋅Filter ⋅Algorithm
1
Introduction
Edge Detection is the process of locating the pixels that correspond to the edge of
the image. Edge detection affects the image size reduction and ﬁlters out infor-
mation that can be considered as less important, but maintaining important struc-
A. Shehu
Telecommunication Department Faculty of Information Technology,
Polytechnic University of Tirana, Square “Mother Teresa”, 4, Tirana, Albania
e-mail: adshehu@tcn.al
A. Hulaj (✉)
Telecommunication Department Faculty of Information Technology,
Polytechnic University of Tirana, Mati 1, Fitorja 2/13, Prishtina, Kosova
e-mail: astrithulaj@hotmail.com
X. Bajrami
Vienna University of Technology, IHRT, Favoritenstras 9/E325A6,
Vienna, Austria
e-mail: xhevahirbajrami@gmail.com
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_25
207

tural properties of the image. Reducing the size of the image is directly related to
the reduction of the transmission power of a sensor node. This reduction affects the
saving of energy in WSN. This is directly related to the lifetime of WSN.
Since edges in most cases represent the object boundaries, edge detection is
widely used in image segmentation, where in this case the images are divided into
zones, which correspond to different objects.
2
The Problem Description
The application of WSN along the state borderline is characterized by many
challenges [1]. One of these challenges is the power supply. Power supply of sensor
nodes is vital to the operation of WSNs. In other words, energy is WSN lifetime
itself.
The task of a sensor node is the capture, processing and routing of images
thought the network to the nearest Border Police Station. However, it is required to
be realized with minimum energy consumption. One of the methods to realize an
energy saving is conversion images captured by WSN in black and white images.
Conversion to black and white image affects the image size reduces. Reducing the
image size impact the reduction of the transmission power of sensor node. By
reducing transmission power, energy consumption is reduced. This directly affects
the lifetime of the sensor node. A black and white image with 128 × 128 pixels
has a size of 2 kB. The same image with greyscale has a size of 16 kB. The formula
for the ﬁle size in kBytes is:
Size of an image = ðrowsÞ * ðcolumnsÞ * bit depth
8 ðconvert to bytesÞ * 1024
ð1Þ
However, it should be noted that through the black and white images it can’t be
seen all speciﬁc details of the objects. Today, with purpose of the detection these
details of images some algorithms are presented. With details, we will understand
detection pixels that represent edges of the image. In this paper, a new algorithm for
edges detection in the framework of an image is presented.
3
Existing Filters for Edge Detection
With edge detection, we understand each mathematical method which is used for
identiﬁcation of points in a digital image in which the brightness of the image
rapidly changes or more formally, where are discontinuities. To realize the detec-
tion of edges in an image, the main purpose is to capture important events that occur
within the image. Usually, the edges are presented at the boundary between the two
different regions in an image. Points at which the intensity varies rapidly are
208
A. Shehu et al.

organized by lines that represent the image edges. Today there are some types of
ﬁlters that enable the edges detection in a digital image, as: Sobel, Prewitt, Roberts,
Laplacian, Canny, etc.
3.1
Sobel Filter
Sobel ﬁlter is a ﬁlter that is used for edges detection. Edges Detection is realized
through calculating the intensity gradient of the image at each pixel inside the
image. This ﬁlter ﬁnds the direction of increasing the intensity from brighter to
darker part, and the ratio of change in this direction. The Sobel Filter consists of a
pair of 3 × 3 convolution kernels. One kernel matrix is simply the other rotated by
90°. These kernel matrices are shown in [2, 3].
3.2
Prewitt Filter
The working principle of the Prewitt ﬁlter is similar to Sobel ﬁlter. Mathematically,
Prewitt ﬁlter also consists of a pair of 3 × 3 convolution kernels. One kernel
matrix is simply the other rotated by 90°. However, Prewitt Filter, unlike the Sobel
Filter, uses other kernel matrices. These kernel matrices are shown in [2]. This ﬁlter
uses ﬁrst order derivative for calculate the difference of intensity of pixels in an
edge region.
3.3
Robert Filter
Robert Filter, as Sobel and Prewitt ﬁlter uses ﬁrst order derivative for the edges
detection. Also, the application logic is similar to that of the Sobel and Prewitt ﬁlter.
Robert ﬁlter uses a pair 2 × 2 convolution masks. These masks are designed to
respond maximally to edges running at 45° to the pixel grid, one mask for each of
the two perpendicular orientations [3]. These matrices are rotated with each other
for 90°. These kernel matrices are shown in [3].
3.4
Laplacian Filter
The Laplacian Filter of an image highlights regions of rapid intensity change and is
therefore often used for edge detection [4]. The Laplacian is often applied to an image
that has ﬁrst been smoothed with something approximating a Gaussian Smoothing
An Algorithm for Edge Detection of the Image …
209

ﬁlter in order to reduce its sensitivity to noise [5]. The Laplacian ﬁlter is an operator of
the second order derivative. More about Laplacian Filter you can read in [4].
3.5
Canny Filter
Canny Edge Detector technique is very important for detecting edges in an image.
This operator isolates noise from an image before ﬁnding, edges of an image
without affecting the features of the image, and then applying the tendency to ﬁnd
the edges and the critical value for threshold [3].
4
The Proposed Algorithm
So far, many methods have been applied for image edges detection. However, we
will present a new algorithm. This algorithm can be summarized as follows.
Let’s take an image A(i,j) as input image with dimensions m × n. Where i take
values i ϵ (1….m) and j take values j ϵ (1…n).
The algorithm goes through these steps: 
Step1. Read input colors image; 
Step 2. Give value for Thresh;
Step 3. Convert read image to grayscale image,  A(i,j); 
Step 4.  Return the size of matrix A(i,j) in separate variables m and n; 
Step 5. Create the output using zeros with the data type for image A(i,j); 
Step 6. Implementation using loops for rows and columns; 
Step 7. Application 3x3 filter within loops;
Step 8. Gradient magnitude  calculation; 
Step 9. Apply condition for finding edges in the image;
Step 10. Give value for Factor;
Step 11. Find edges including Thresh and Factor values;
Step 12. Display image after edges detection.   
5
Results and Discussions
In this section we will present and discussions the results obtained with the
application of existing ﬁlters for edges detection within an image, and these results
will be compared with results obtained with the application of the new algorithm.
210
A. Shehu et al.

Results were derived by using MATLAB 2014 and an image with 153 × 329
pixels. In Fig. 1 are presented the results obtained with the application of Roberts
Filter. In Fig. 2, are presented the results obtained with the application of Prewitt
Filter. In Fig. 3 are presented the results obtained with the application of Sobel
Filter. In Fig. 4 are presented the results obtained with the application of Laplace
Filter. In Fig. 5 are presented the results obtained with the application of Canny
Filter. While in Fig. 6 are presented the results obtained as a result of applying the
proposed algorithm in this paper.
Original Image
Gray Image
Edge Detected Image
Fig. 1 Roberts Filter
Original Image
Gray Image
Edge Detected Image
Fig. 2 Prewitt Filter
Original Image
Gray Image
Edge Detected Image
Fig. 3 Sobel Filter
Original Image
Gray Image
Edge Detected Image
Fig. 4 Laplace Filter
An Algorithm for Edge Detection of the Image …
211

From the results shown in Fig. 1, we can see that Roberts Filter does not provide
the desired results. Comparing the results shown in Fig. 1 with the results presented
in Fig. 6, we can see that in the case of Roberts Filter, we have lost many pixels
along the edges.
From the results shown in Fig. 2, we can see that Prwitt Filter, same as the
Roberts Filter doesn’t provide the desired results. Comparing the results shown in
Fig. 1 with the presented results in Fig. 6, we can see that also in the case of
Roberts Filter, we have lost many pixels along the edges.
From the results shown in Fig. 3, we can see that Sobel Filter, same as the
Roberts and Prwitt Filter doesn’t provide good results for edges detection. If the
results obtained with the application of Sobel Filter compared with the presented
results in Fig. 6 we can see that in the case of Sobel Filter we have a lot of pixels no
detected along the edges.
From the results shown in Fig. 4, we can see that Laplace Filter, provide better
results than Robert, Prwitt and Sobel Filter. However, if the results obtained with
the application of Laplace Filter compared with the presented results in Fig. 6 we
can see that in the case of Laplace Filter we have least pixels the detected along the
edges. Especially, we can see changes in the tail, ﬁngers and wings of the eagle.
From the results shown in Fig. 5, we can see that Canny Filter, provide best
results than Laplace, Robert, Prwitt and Sobel Filter. However, if the results
obtained with the application of Canny Filter compared with the presented results in
Fig. 6 we can see that in the case of application of Canny Filter isn’t reached to
edges detection in the tail of the eagle.
Another difference we can see at the bottom of the left wing of the eagle (Right
in the Fig. 5). From Figs. 5 and 6 we can see that in the case of application of
Canny Filter detected edges are less highlighted than in the case of the results
Original Image
Gray Image
Edge Detected Image
Fig. 5 Canny Filter
Original Image
Gray Image
Edge Detected Image
Fig. 6 New algorithm
212
A. Shehu et al.

obtained by our proposed algorithm. The same changes we can see also to the toes
of the Eagle. However, despite this, the application of Canny Filter, as shown in
Fig. 5 offers a better correlation of pixels, especially those pixels that are detected
within the Eagle.
From the results shown in Fig. 2, it can be seen that with the application of our
proposed algorithm, we can obtain very good results in the detection of edges
within the image captured by the sensor nodes or any other electronic device. If the
results presented in Fig. 6 are compared with the results presented in Figs. 1, 2, 3, 4
and 5 it can be seen that the proposed algorithm enables a very good detection of
pixels that represent the edges of the image.
6
Conclusions
In this paper we presented an algorithm which is very efﬁcient in the detection of
pixels that correspond to the edges of the image. From comparing the results
obtained with the application of traditional ﬁlters with those obtained with the
application of the proposed algorithm, it can be seen that the proposed algorithm
provides very good results and concrete. Therefore, based on the results presented
and analyzed we can conclude that the proposed algorithm is very suitable to be
applied for edges detection of the image.
In the future, this algorithm should be applied to the image in which noise has
been present, and practically in the sensor, as and other electronic devices, as to
look at the results of practical application of this algorithm.
References
1. Hulaj, A., Shehu, A., Bajrami, X.: Application of wireless multimedia sensor networks for
green borderline surveillance, Annals of DAAAM & Proceedings, 27, (2016)
2. Sharma, P., Singh, G., Kaur, A.: Different techniques of edge detection in digital image
processing. Int. J. Eng. Res. Appl. 3(3), 458–461 (2013)
3. Igbinosa, I.E.: Comparison of edge detection technique in image processing techniques. Int.
J. Inf. Technol. Electr. Eng. 2(1), 25–29 (2013)
4. Maini, R., Aggarwal, H.: Study and comparison of various image edge detection techniques.
Int. J. Image Process. (IJIP) 3(1), 1–11 (2009)
5. Huertas, A., Medioni, G.: Detection of intensity changes with subpixel accuracy using
Laplacian-Gaussian masks. IEEE Trans. Pattern Anal. Mach. Intell. 5, 651–664 (1986)
An Algorithm for Edge Detection of the Image …
213

A Mathematical Model of the Behavior
of SIP Signaling and Media Messages
Naser K.A. Alajmi, Hadeel Saleh Haj Aliwi, Kamal Alieyan
and Muhammad-Imran Sarwar
Abstract Over the last few years, many multimedia conferencing and Voice over
Internet Protocol (VoIP) applications have been developed due to the use of sig-
naling protocols in providing video, audio and text chatting services between at
least two participants. This paper studies the behavior of the widely common
signaling protocol; Session Initiation Protocol (SIP) in terms of the behavior of the
signaling and media messages, as well as the delay time during call setup, call
teardown, and media sessions.
Keywords Signaling protocol ⋅Media conferencing ⋅VoIP ⋅SIP
1
Introduction
With the appearance of numerous multimedia conferencing and Voice over Internet
protocols [1, 2], the decision to choose the appropriate protocol to be utilized in
such a service has become very difﬁcult since each protocol has its own privileges
which differ from the corresponding privileges of the other protocols.
Choosing SIP protocol [3] to be discussed is due to many reasons; SIP is an
interesting alternative compared to the conventional VoIP protocols. Nowadays,
SIP is being deployed by service providers for their VoIP service offerings. SIP
protocol offers signiﬁcant features that are not provided by other existent VoIP
N.K.A. Alajmi
Saad Al-Abdullah Academy for Security Sciences, Al Asimah, Kuwait
H.S.H. Aliwi (✉)
School of Computer Sciences, Universiti Sains Malaysia, 11800 Gelugor,
Pulau Penang, Malaysia
e-mail: Hha11_com030@student.usm.my; hadeelsaleh12@yahoo.com
K. Alieyan ⋅M.-I. Sarwar
National Advanced IPv6 Centre, Universiti Sains Malaysia, 11800 Gelugor,
Pulau Penang, Malaysia
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_26
215

signaling protocols. Furthermore, many researchers have shown that SIP is slightly
better than H.323 [4], MGCP [5] and RSW [6, 7] in terms of quality of services.
2
SIP Protocol
SIP is an application-layer control protocol [8, 9] that can establish, modify, and
terminate multimedia sessions (conferences) such as Internet telephony calls [10].
SIP can also invite participants to already existing sessions, such as multicast
conferences. Media can be added to (and removed from) an existing session. SIP
transparently supports name mapping and redirection services, which supports
personal mobility-users can maintain a single externally visible identiﬁer regardless
of their network location [11, 12].
SIP protocol enables Internet endpoints (called user agents) to discover one
another and to agree on a characterization of a session they would like to share. For
locating prospective session participants, and for other functions, SIP enables the
creation of an infrastructure of network hosts (called proxy servers) to which user
agents can send registrations, invitations to sessions, and other requests. SIP is an
agile, general-purpose tool for creating, modifying, and terminating sessions that
works independently of underlying transport protocols and without dependency on
the type of session that is being established [13]. SIP does not carry any voice or
video data itself. It merely allows two endpoints to set up connection to transfer that
trafﬁc between each other via Real-time Transport Protocol (RTP) [14]
User Datagram Protocol (UDP) and Transport Control Protocol (TCP) [14] are
transport protocols used to transfer audio and video data. SIP protocol has many
features such as the service of text-based which allows easy implementation in
object oriented programming languages, ﬂexibility, extensibility, less signaling,
transport layer-protocol neutral and parallel search. SIP uses many signaling mes-
sages in order to handle the communication between two nodes or more. SIP makes
use of the request methods: INVITE, ACK, OPTIONS, BYE, CANCEL, etc. in
order to control the call setup and call teardown [15].
3
SIP Messages Analysis
Each control protocol has three types of sessions; registration session, call setup and
teardown sessions, and media data session. In this section each protocol session
type is presented by number I varies from 1 to 3 as number as the session types, so
when i = 1 means the current message is related to the registration session, and
same when i = 2 and 3. In this paper, only the behavior of the signaling and media
data messages will be discussed.
216
N.K.A. Alajmi et al.

I =
1 ⟶Registration Message
2 ⟶Signaling Message
3 ⟶Media data Message
8
<
:
Each one of the aforementioned messages has lists of messages type related to a
certain session. For example, the signaling messages can be either initiate message
or ringing message or accept message or terminate message. SIP signaling message
type is presented by number j varies from 1 to 7 as number as the signaling
messages.
J =
1 ⟶INVITE
2 ⟶TRYING
3 ⟶RINGING
4 ⟶OK
5 ⟶BYE
6 ⟶CANCEL
7 ⟶ACK
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
As the protocol message type is presented by I, so as an example when i = 2 and
j = 5 means the message is related to the SIP signaling session which is BYE. As a
result, to identify exactly each protocol message belongs to which session and what
is the message type during that session, each SIP message can be presented by I
which is the number of the session and J which is the number of the message inside
the session i.
SIP Message Type = Mij
ð1Þ
During the SIP signaling session, if client 1 sends message to client 2 (except
OK message), the latest should notify client 1 of receiving the message successfully
before receiving the next message. Otherwise, no message has to be sent back to
client 1. If the SIP message sent from client 1 to client 2 is presented by the function
Yn, so the notiﬁcation message sent back to client 1 is presented by the function
Yn+1. With more details:
A Mathematical Model of the Behavior of SIP Signaling …
217

As each SIP message is deﬁned by the number of session and the number of the
message inside the session, therefore:
In media session, j represents the order of a certain media message. J varies from
1 to r where 1 represents the order of the ﬁrst SIP message, r is an integer positive
number represents the order of the last SIP message respectively. Assuming that
N is the number of messages, so number of SIP media messages = (the order of the
last media message −the order of the ﬁrst media message) + 1. Thus,
NM3j = ðr −1Þ + 1 = r
ð2Þ
Since the payload of SIP client carried by RTP header in the media session, by
assuming that payload is presented by PL, so
RTPPL = M3j
ð3Þ
218
N.K.A. Alajmi et al.

4
SIP Session Time Analyses
In SIP Environment, two main sessions has to be considered which are signaling
session and media session. Signaling session is divided into two sessions; setup
session and teardown session. The delay time of each message is the difference
between the message sending time from client 1 and the message’s receiving time
by client 2. In order to calculate the time spent to complete the call setup session,
the time difference between the ﬁrst SIP message sent by client 1 and the last SIP
message received by client 2 during the setup session should be measured.
Assuming that T presents the message time, TS presents the sent time of the
message, and TR presents the received time of the message.
TM2j = TR M2j −TS M2j
ð4Þ
TSetupSIP = TR ACK −TS INVITE
ð5Þ
Hence,
TSetupSIP = TR M27 −TS M21
ð6Þ
In order to calculate the time spent to complete the call teardown session, the
time difference between BYE message sending time and OK message arrival time
should be calculated.
TTeardownSIP = TR OK −TS BYE
ð7Þ
Hence,
TTeardownSIP = TR M24 −TS M25
ð8Þ
Similar to the signaling message sending/receiving status, the delay time of each
media message is the difference between its sent and arrival times.
TM3j = TR M3j −TS M3j
ð9Þ
In order to calculate the time spent to complete the media session, the time
difference between BYE message sending time and ACK message arrival time
(once the call answered) has to be calculated.
TMediaSIP = TS BYE −TR ACK
ð10Þ
TMediaSIP = TS M25 −TR M27
ð11Þ
A Mathematical Model of the Behavior of SIP Signaling …
219

5
Conclusion
This paper provided a mathematical analysis of the behavior of the SIP protocol in
terms of the signaling and media messages, and the time of each call setup, call
teardown, and media sessions.
References
1. Goode, B.: Voice over internet protocol (VoIP). Proc. IEEE 1495–1517 (2002)
2. Haj Aliwi, H.S., Sumari, P.: A comparative study of VoIP protocols. (IJCSIS) Int. J. Comput.
Sci. Inf. Secur. 11, 97–101 (2013)
3. Haj Aliwi, H.S., Sumari, P.: A comparative study of VoIP, MCS, instant messaging protocol
and multimedia applications. (IJCNS) Int. J. Adv. Comput. Netw. Secur. 4, 67–70 (2014)
4. Basicevic, I., Popovic, M., Kukolj, D.: Comparison of SIP and h.323 protocols. In: The Third
International Conference on Digital Telecommunications ICDT ’08, pp. 162–167 (2008)
5. Dajiuklas, T., Ioannou, K., Garmpis, A.: A lightweight and scalable VoIP platform based on
MGCP/H.323 interworking and QOS management capabilities. In: Proceedings of the 4th
WSEAS International Conference on Information Security, Communications and Computers,
Tenerife, Spain, pp. 548–553 (2005)
6. Haj Aliwi, H.S., Alomari, S.A., Sumari, P.: An efﬁcient audio translation approach between
SIP and RSW protocols. In: Proceedings of 3rd World Conference on Information
Technology (WCIT-2012), University of Barcelon, Barcelona, Spain, vol. 3, pp. 31–37
(2013)
7. Ramadass, S., Subramanian, R.K, Guyennet, H., Trehel, M.: Using RSW control criteria to
create a distributed environment for multimedia conferencing. In: Proceedings on Research
and Development in Computer Science and its Applications, Penang, Malaysia, pp. 27–29
(1997)
8. Montoro, P., Casilari, E.: A comparative study of VoIP standards with asterisk. In: Fourth
International Conference on Digital Telecommunications (ICDT), France (2009) 1–6
9. Abbasi, T., Prasad, S., Seddigh, N., Lambadaris, I.: A comparative study of the SIP and IAX.
In: Canadian Conference on Electrical and Computer Engineering (CCECE), Saskatoon,
Canada, pp. 179–183 (2005)
10. Papageorgiou P.: A comparison of H.323 vs SIP. Master thesis, University of Maryland at
College Park, USA (2001)
11. Schulzrinne, H., Rosenberg, J.: A comparison of SIP and H.323 for internet telephony. In:
Proceedings of the 8th International Workshop on Network and Operating Systems Support
for Digital Audio and Video (NOSSDAV’98), Cambridge, UK, pp. 83–86 (1998)
12. Geneiatakis, D., Dagiuklas, T., Kambourakis, G., Lambrinoudakis, C., Gritzalis, S.: Survey of
security vulnerabilities in session initial protocol. IEEE Commun. Surv. Tutor. 8, 68–81
(2006)
13. Glasmann, J., Kellerer, W., Muller, H.: Service architectures in H.323 and SIP: a comparison.
IEEE Commun. Surv. Tutor. 5, 32–47 (2003)
14. Forouzan, B.: Data Communications and Networking, 4th edn. McGrawHill, New York, USA
(2007)
15. Abouabdalla, O., Ramadass, S.: Enable communication between the RSW control criteria and
SIP using R2SP. In: The 2nd International Conference on Distributed Frameworks for
Multimedia Applications (DFmA), Penang, Malaysia, pp. 1–7 (2006)
220
N.K.A. Alajmi et al.

Blood Vessel Segmentation from Color
Retinal Images Using K-Means Clustering
and 2D Gabor Wavelet
Aziah Ali, Wan Mimi Diyana Wan Zaki and Aini Hussain
Abstract This paper presents a new unsupervised method for segmenting blood
vessels in digital retinal images. The proposed method uses K-means clustering to
binarize grayscale vessel-enhanced images derived from green channel image and
Gabor wavelet feature image. The binary images are then combined using logical
OR to produce segmented vessels. The method was evaluated on the publicly
available DRIVE database and the results compared to published literature. The
method proved to have comparable performance to other published unsupervised
methods while being simple and fast to implement. In the future, the proposed
method can be further improved to be applied in real clinical setting to assist the
physicians in diagnosing ocular diseases through an automated screening system.
Keywords Retinal image ⋅Blood vessel segmentation ⋅K-means clustering ⋅
Gabor wavelet
1
Introduction
Accurate detection and measurement of retinal blood vessels from digital retinal
image is very important for early screening of disease onset and also in quantifying
the severity of ocular diseases. A reliable method of segmenting the retinal blood
vessels is necessary towards the development of automated diagnosis system for
retinal images. While manual segmentation is possible, the amount of effort and
time needed for even a small set of images could eventually become prohibitive.
A. Ali (✉) ⋅W.M.D. Wan Zaki ⋅A. Hussain
Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia,
Bangi, Selangor, Malaysia
e-mail: aziah.ali@mmu.edu.my; aziah@siswa.ukm.edu.my
A. Ali
Faculty of Computing and Informatics, Multimedia University,
Cyberjaya, Selangor, Malaysia
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_27
221

This is one of the reasons a large number of researchers have been investigating the
topic of automatic segmentation of retinal blood vessels in the recent years.
An extensive summary of previous works on blood vessel segmentation
methodologies was presented in [1], where the methods of segmenting retinal blood
vessels were divided into ﬁve main approaches namely pattern classiﬁcation,
matched ﬁltering, morphological processing, vessel tracking and multi-scale
approaches.
Pattern recognition approach with supervised classiﬁcation method have been
employed in [2–4], and in [5] Soares et al. proposed the use of Gaussian Mixture
Models classiﬁer to classify Gabor wavelet features extracted from retinal images.
Matched ﬁltering method was initially proposed by Chauduri in [6] which inspired
more extended work by Hoover [7] and Zhang [8]. Zana and Klein [9] adopted
morphological processing method to segment retinal blood vessels while Mendonca
et al. [10] combined it with centerline detection. In [11], Strisciuglio employed a set
of trainable B-COSFIRE ﬁlters to better capture the elongated structure of retinal
blood vessels.
This paper proposes an unsupervised method to perform automated segmenta-
tion of blood vessels on retinal images that is inspired by the use of Gabor wavelet
ﬁlter in a number of previous studies, for example in [5, 12]. As opposed to the use
of supervised classiﬁcation methods to binarize the Gabor feature in previous
studies, we propose the use of K-means clustering to binarize the Gabor wavelet
feature, which is simpler to implement and more efﬁcient for processing a huge
number of retinal images.
2
Proposed Method
The proposed method of automatic retinal vessel segmentation is divided into four
main parts. In Part 1, pre-processing is performed on the green channel image
obtained from the original color retinal and then enhanced to highlight blood
vessels. In Part 2, the green channel image is inverted and Gabor wavelet feature is
extracted. The Gabor feature then undergoes vessel-enhancement process. In Part 3
of the proposed method, K-means clustering are performed on the two
vessel-enhanced images from the previous part separately, resulting in binary
images that later go through post-processing. The ﬁnal segmentation output is then
produced in Part 4 by combining the two binary images using logical OR operation
after which a ﬁnal post-processing step is performed.
Pre-processing. Since green channel of the color retinal image provides highest
contrast between retinal background and blood vessels, it will be used as the initial
input in the proposed method. The Field-of-View (FOV) region in the green
channel image is expanded according to Soares’s recommendation in order to
minimize effects from the high contrast between FOV and black background [5].
222
A. Ali et al.

Blood vessels on the padded green channel image are then ﬁrst enhanced using
contrast-limited adaptive histogram equalization (CLAHE) method. After the
contrast is adjusted, the mean image is obtained by convolving the contrast-adjusted
image with a mean arithmetic kernel. The mean image is later subtracted from the
contrast-adjusted image and the resulting image is contrast-adjusted again to further
enhance the vessels resulting in a vessel-enhanced image.
Gabor Wavelet. We used the same parameters as proposed by Soares in [5] in
our method to derive Gabor wavelet response of inverted green channel image
obtained from the previous step. The Gabor wavelet response is further processed to
produce a vessel-enhanced Gabor wavelet feature image. Figure 1 shows a sample
of
green
channel
image
with
its
corresponding
contrast-adjusted
and
vessel-enhanced images for both green channel and Gabor feature.
Binary Thresholding. After obtaining both vessel-enhanced images from green
channel and Gabor wavelet feature, K-means clustering is performed on both
grayscale images to produce binary vessel images. The number of cluster for
K-means clustering is set to two clusters, one for a vessel pixel (value 1) and
another for background pixel (value 0). The output of this step normally contains
noisy pixels which are mostly background pixels detected as disconnected vessel
pixels. The objective of the next step is to remove as much of these noisy pixels as
possible, without removing actual vessel pixels.
Image Post-processing. In this step, noisy pixels and small regions that mostly
do not represent blood vessels are removed. This is achieved by adopting a mor-
phological operation called opening to remove any connected vessel pixels con-
taining less than a threshold value, which has been empirically determined to be 50
pixels for green channel binary image and 25 pixels for Gabor feature binary image.
Binary Image Combination. After the images have been individually
post-processed, they are then combined using the logical OR operation to produce a
single binary image. A ﬁnal post-processing step using morphological opening is
performed on the combination output. The result is a binary image representing the
vessels from both green channel and Gabor feature binary images, as illustrated in
Fig. 2.
Fig. 1 Sample green channel image and its corresponding pre-processing output: a Green channel
image with padded FOV, b contrast-adjusted green channel image, c vessel-enhanced green
channel image, d contrast-adjusted Gabor feature image, and e vessel-enhanced Gabor feature
image
Blood Vessel Segmentation from Color Retinal Images …
223

3
Results and Discussion
3.1
Image Database
Our proposed method was evaluated using retinal images from the publicly
available DRIVE database [7]. The database contains 40 color retinal images which
are divided into two sets, namely training set and test set. The image size is 565 by
584 pixels with 8 bits per color channel and they were captured using a Canon CR5
non-mydriatic camera with a ﬁeld of view of 45°. For each image in the database,
the authors also provide its corresponding mask image and a manual segmentation
image. For the test set, a second set of manually segmented images are also
included. In our evaluation, we used the test set for testing with the ﬁrst set of
manual segmentation (1st human observer) as ground truth. We use the second
manual segmentation (2nd human observer) set as the target benchmark for our
method.
3.2
Performance Metric
A number of different performance measure have been described in the literature to
evaluate vessel segmentation algorithm. For performance measure assessment in
this study, only pixels within the FOV are considered. A true positive (TP) is
deﬁned as a pixel identiﬁed as vessel by the algorithm and is also speciﬁed as vessel
in ground truth image. A false positive (FP) is a vessel identiﬁed as vessel by the
algorithm but speciﬁed as non-vessel in ground truth image. Any vessel identiﬁed
as non-vessels by the algorithm but marked as vessel in ground truth is a false
negative (FN) and true negatives (TN) are pixels identiﬁed as non-vessels by both
algorithm and ground truth image. The counted values for all TP, FP, TN and FN
can be used to calculate the performance measure of a vessel segmentation algo-
rithm. We have selected a number of usual measures used in previous literature
which includes Accuracy rate (ACC), Sensitivity (SN), Speciﬁcity (SP) and Mat-
thews Correlation Coefﬁcient (MCC) in order to quantify the performance of our
Fig. 2 Clustering, post-processing and binary image combination outputs: a Binary image from
green channel, b post-processed green channel image, c binary image from Gabor feature,
d post-processed Gabor feature image, and e result of OR operation on images 2(b) and 2(d)
224
A. Ali et al.

proposed method. We used the same formulas speciﬁed in [11] to calculate the
performance metric for our results.
3.3
Experimental Results
Before the binary images were combined, performance measures were calculated
for binary images from green channel and binary images from Gabor feature image
individually. 2nd human observer’s performance was obtained by comparing the
2nd observer’s segmentation to the 1st observer’s segmentation outputs which were
used as the ground truth in this study. From the results in Table 1, segmentation
outputs using only green channel image performs better than the outputs using only
Gabor feature image on all metrics considered except sensitivity. However sample
binary images from both feature in Fig. 2 indicates that Gabor feature (Fig. 2d) is
better in capturing ﬁner vessels compared to green channel image. This could be
due to the chosen value of 2 pixels for Gaussian envelope scale parameter, σ that
highlights ﬁne vessels well. While this is true, some of the larger vessels appear
hollow due to the central vessel reﬂex on Gabor binary image. This can be com-
plemented by the binary image from green channel that preserves most of the large
vessels well, contributing to a much improved results when the two binary images
combined as shown in the last row of Table 1.
Results for Zana, Chaudhuri and Jiang are calculated using the output images
provided on DRIVE database website, while other results are obtained from the
published literature. The proposed method has an average accuracy of 0.9425
Table 1 Performance of
proposed method compared to
other published methods
Methods
SN
SP
ACC
MCC
2nd human
observer
0.7763
0.9723
0.9470
0.7660
Green channel
image
0.5987
0.9884
0.9378
0.7021
Gabor feature
image
0.6062
0.9787
0.9306
0.6716
Zanaa [9]
0.6697
0.9830
0.9429
0.7315
Chaudhuria [6]
0.2706
0.9887
0.8969
0.4236
Jianga [13]
0.6480
0.9623
0.9218
0.6509
Strisciuglio [11]
0.7655
0.9704
0.9442
0.7475
Mendonca [10]
0.7344
0.9764
0.9452
–
Proposed
method
0.7206
0.9757
0.9425
0.7400
aResults calculated from provided binary images on DRIVE
website
Blood Vessel Segmentation from Color Retinal Images …
225

which are comparable to other methods’ results with highest accuracy of 0.9452 by
Mendonca. The proposed method’s sensitivity value of 0.7206 is higher than Zana,
Chauduri, and Jiang but slightly lower than Strisciuglio and Mendonca. In terms of
speciﬁcity, the proposed method has the value of 0.9757 which is comparable to
other published methods. Strisciuglio’s method recorded highest MCC value of
0.7475 while the proposed method has the second highest MCC value of 0.7400.
Speciﬁcity value of our method also exceeds those of second observer’s manual
segmentation results. Figure 3 compares 2 sample outputs with their corresponding
manual segmentation outputs, i.e. the ground truth.
4
Conclusion
We proposed a novel retinal blood vessel segmentation method that combines
binary images obtained from the vessel-enhanced green channel image and
vessel-enhanced Gabor feature image. The binary images are produced using
unsupervised K-means clustering and combined using logical OR operation. The
combination of binary images increased the performance of individual features by
complementing each other well. The results achieved (SN = 0.7206, SP = 0.9757,
ACC = 0.9425 and MCC = 0.7400) are comparable to published unsupervised
methods. The proposed method is efﬁcient since it does not involve sophisticated
calculations nor prior training of labeled images. In future work, other methods of
image combination will be investigated to improve the overall performance of the
segmentation method.
Acknowledgements This research is supported by MOHE Malaysia (FRGS/1/2015/TK04/
UKM/01/3) and UKM (DIP-2015-012). We thank Assoc. Prof. Dr. Jemaima Che Hamzah from
Dept. of Ophthalmology PPUKM who provided insight and expertise that greatly assisted the
research and also MMU for the funding.
Fig. 3 Sample vessel segmentation outputs of Image 12 and 2 from DRIVE database. a Ground
truth Image 12 from DRIVE database, b ﬁnal segmented vessels of Image 12, c Ground truth
Image 2 from DRIVE database, and d ﬁnal segmented vessels of Image 2
226
A. Ali et al.

References
1. Fraz, M.M., Remagnino, P., Hoppe, A., Uyyanonvara, B., Rudnicka, A.R., Owen, C.G.,
Barman, S.A.: Blood vessel segmentation methodologies in retinal images—a survey.
Comput. Methods Programs Biomed. 108, 407–433 (2012)
2. Manoj, S., Sandeep, P.M.: Neural network based classiﬁer for retinal blood vessel
segmentation. Int. J. Recent Trends Electr. Electron. Eng. 3, 44–53 (2013)
3. Osareh, A., Shadgar, B.: An automated tracking approach for extraction of retinal vasculature
in fundus images. J. Ophthalmic Vis. Res. 5, 20–26 (2010)
4. Staal, J.J., Abramoff, M.D., Niemeijer, M., Viergever, M.A., Van Ginneken, B.: Ridge-based
vessel segmentation in images of the retina. IEEE Trans. Med. Imaging 23, 501–509 (2005)
5. Soares, J.V.B., Leandro, J.J.G., Cesar Jr, R.M., Jelinek, H.F., Cree, M.J.: Retinal vessel
segmentation using the 2-D Gabor wavelet and supervised classiﬁcation. IEEE Trans. Med.
Imaging 25, 1214–1222 (2006)
6. Chaudhuri, S., Chatterjee, S., Katz, N., Nelson, M., Goldbaum, M.: Detection of blood vessels
in retinal images using 2D matched ﬁlters. IEEE Trans. Med. Imaging 8, 263–269 (1989)
7. Hoover, A.: Locating blood vessels in retinal images by piecewise threshold probing of a
matched ﬁlter response. IEEE Trans. Med. Imaging 19, 203–210 (2000)
8. Zhang, B., Zhang, L., Zhang, L., Karray, F.: Retinal vessel extraction by matched ﬁlter with
ﬁrst-order derivative of Gaussian. Comput. Biol. Med. 40, 438–445 (2010)
9. Zana, F., Klein, J.C.: A multimodal registration algorithm of eye fundus images using vessels
detection and Hough transform. IEEE Trans. Med. Imaging 18, 419–428 (1999)
10. Mendonça, A.M., Campilho, A.: Segmentation of retinal blood vessels by combining the
detection of centerlines and morphological reconstruction. IEEE Trans. Med. Imaging 25,
1200–1213 (2006)
11. Strisciuglio, N., Azzopardi, G., Vento, M., Petkov, N.: Supervised vessel delineation in retinal
fundus images with the auto selection of B-COSFIRE ﬁlters. Mach. Vis. Appl. 1–13 (2016)
12. Li, Q., You, J., Zhang, L., Bhattacharya, P.: A multiscale approach to retinal vessel
segmentation using Gabor ﬁlters and scale multiplication. In: IEEE International Conference
on Systems, Man and Cybernetics 2006. SMC ’06, vol. 4, pp. 3521–3527 (2006)
13. Jiang, Y.: Blood vessel tracking in retinal images. In: Proceedings of Image and Vision
Computing New Zealand 2007, pp. 126–131, Hamilton, New Zealand (2007)
Blood Vessel Segmentation from Color Retinal Images …
227

Training Samples Construction for Energy
Utilities Operational Assets Management
Alexandra Khalyasmaa and Stanislav Eroshenko
Abstract The paper is concerned with the training sets formation problems in the
intelligent information analytical systems for electrical grid equipment technical
state assessment and repair programs formation with the purpose of the power grid
companies’ plant assets management. The basic principles of the developed intel-
ligent information analytical system are presented in the paper. The problems
associated with the training sets formation for electrical grid equipment are also
described and the basic requirements to them are deﬁned. Within the framework of
the presented paper the analysis of the training set size impact on the equipment
state identiﬁcation accuracy in the context of 110 kV power transformers state
identiﬁcation was performed.
Keywords Plant assets ⋅
Training set ⋅
Analytical system ⋅
Electrical
equipment
1
Introduction
Nowadays different information analytical systems intended to electrical grid
equipment technical state assessment at substations, equipment residual life analysis
and power grid companies (utilities) plant assets management have become pop-
ular. It comes from the fact that under the modern conditions the hardware and
software are sufﬁciently developed which allows for the large data volumes
A. Khalyasmaa (✉) ⋅S. Eroshenko
Ural Federal University named after the ﬁrst President of Russia B. N. Yeltsin,
Mira st. 19, 620002 Ekaterinburg, Russia
e-mail: lkhalyasmaa@mail.ru
S. Eroshenko
e-mail: stas_ersh@mail.ru
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_28
229

accumulation [1]. The large data volume in its turn gives the opportunity to receive
the useful analytical information on the basis of which the new knowledge can be
obtained and possible correlations and interrelations can be found. Different arti-
ﬁcial intelligence methods such as the neural networks, the fuzzy inference, the
genetic algorithms, the machine learning [2] etc. are used for operational efﬁciency
improvement in the modern information analytical systems. Each method has its
advantages and disadvantages, but the system training principle is used in practi-
cally all the methods as far as the new system knowledge is obtained just by means
of training. Nowadays there exist a great number of the networks learning methods,
for instance: the supervised learning, the unsupervised learning, the statistical
learning, the self-organization etc. [3].
It is unlikely that the unsupervised learning can be used during the equipment
technical state and residual life assessment as far as the problem is ﬁnally reduced to
the analysis of equipment state and defects speciﬁed by the governing documents.
In other words the present system has always both input and output vector. The
electrical grid equipment diagnostics and tests results are used as the training set. It
is the parameters set (inputs) which characterizes the equipment state as far as
diagnoses and defects (outputs) corresponding to the present parameters sets. It is
obvious that the efﬁciency of the information analytical system with the supervised
learning depends heavily on the training set quality.
Not so much the high percentage of erroneous diagnoses (defects) as the pos-
sibility of the information analytical system further training is the major problem of
the defective training set. In this case the latter is of particular importance as far as it
is too difﬁcult to ﬁnd the equipment failures and defects statistics sufﬁcient for the
high-quality training set formation. Firstly, the problem is in that the total damage
rate of equipment is not so high, for instance according to [4] it is 0.5–2.5% per a
year for power transformers (the average value is ∼1%). It means that for the
middle-size grid company with seventy 110 kV substations on the balance sheet
with consideration of reserve facilities etc., the total number of 110 kV transformers
will be approximately 150, and, consequently, the transformers damage rate will be
approximately 2 transformers per a year.
In this case it will take approximately 50 years for the required representative
sample formation at least by the sufﬁcient minimum quantity condition. It is not
necessarily that over this period we will obtain any and all equipment defects types,
speciﬁed by the governing documents, moreover, the equipment ﬂeet even the ﬂeet
of the same type equipment comprises different modiﬁcations and in this case the
defects parameters of one modiﬁcation can differ markedly from another one. In
other words, the training set formation is one of the key problems during the
intelligent information analytical systems implementation, that is why the paper is
concerned with the presented problem solving.
230
A. Khalyasmaa and S. Eroshenko

2
The Intelligent Information Analytical System
Description
The intelligent information analytical system (IIA) developed by the authors is
designed for the electrical equipment state identiﬁcation, maintenance and repair
cycles optimization. IIA is based on the Takagi-Sugeno fuzzy knowledge base
(neuro fuzzy inference), which is described in detail in [5]. Actually the neuro fuzzy
inference structure and adjustment optimization are reduced to the iterative deter-
mination problem of the membership functions optimal number and type as far as
the production rules and training sets etc. The Takagi-Sugeno algorithm was taken
as the fuzzy inference algorithm because of the fact that the present algorithm is the
universal approximator of functions, which in case of the large number of the input
parameters has the advantage over the other neuro fuzzy inference algorithms.
According to [6] when modeling the multifactorial problems, for instance the
equipment technical state assessment problem, it is appropriate to use the
Takagi-Sugeno algorithm. The network in the Takagi-Sugeno algorithm comprises
5 layers and the linear function of the input variables is used as the conclusive rule
y1ðxÞ = pi0 + ∑
N
j = 1
pijxj
ð1Þ
where N is the number of the X set parameters; pi0, pi1, . . . , piN are the
Takagi-Sugeno polynomial parameters.
2.1
The Fuzzy Parameters Identiﬁcation Problems
The fuzzy knowledge base was used due to the fact that the equipment parameters
analyzed during the system operation can indicate the different equipment states
(defects types) at the same time with different degrees of membership to these
states. The state analysis of two TRDN-25000/110/10 type transformers (T1 and
T2) was performed for illustrative purposes. The results of chromatographic anal-
ysis of dissolved in oil gases (DGA) over the period of 2005–2015 were taken as
initial data. The concentrations of the following gases were analyzed: methane
(CH4), acetylene (C2H2), ethylene (C2H4), ethane (C2H6), hydrogen (H2), carbon
oxide (CO), and carbon dioxide CO2 by six different methods: Rodgers method,
Dornenburg method, IEC standard, nomograms method, ETRA method, Duval’s
triangle, and also experts estimates method. The detailed analysis is presented in
[7].
Among all the measurements the gases boundary concentrations exceedance was
observed only in two cases: in case of oil sampling from T1 transformer (T11) and
in case of oil sampling from T2 transformer (T21). In other cases all concentrations
values of the dissolved in oil gases are within the boundary values range. In spite of
Training Samples Construction for Energy …
231

this fact many DGA methods allow the defect identiﬁcation without gases con-
centrations exceedance criterion. The results of DGA data analysis by different
methods are presented in Table 1. As it is seen from Table 1 each method identiﬁes
the transformer state in its own way and the methods results are not always the
same. In this case the unique identiﬁcation of the transformer state is difﬁcult,
moreover the degree of membership to a particular state is also various due to the
differences in the boundary concentrations values for each method. It should be
mentioned that for each method the particular state possibility will also differ as
well as in the previous case due to the differences in the boundary concentrations
values and also due to the differences in the states gradation in each particular
method.
All the above mentioned shows that the use of the fuzzy knowledge base is
well-founded, moreover the necessity of the training sets formation by state (de-
fects) on the basis of the precedent information is obvious. In other words, in
different cases for the same transformer type, but, for instance, being operated in
different climatic conditions, the same gases concentrations values in one case show
that there is an incipient defect, in other case—that there is the outlier. Therefore
special attention should be given to the training sets creation for quality improve-
ment of the technical state assessment results and decisions concerning its further
operation.
2.2
Equipment Types Structure in the Training Set
On the ﬁrst step the initial training set is formed for each equipment type (trans-
former, circuit breaker, disconnector etc.) with consideration of each type division
into the main subtypes. For instance, the “power transformer” type comprises the
following subtypes: autotransformers; 110–220 kV transformers; 35 kV trans-
formers; 6–20 kV oil transformers; 6–35 kV dry-type transformers. The present
grading is based on the equipment design features, equipment operation mode
parameters as far as the techniques of diagnostics and tests during which the present
parameters are deﬁned. For each subtype the training set is initial, as far as the
uplearning is a cyclic process in the developed system. Only new precedents (inputs
and the corresponding outputs) are included in the holdout training set. In case of
the precedents repetition the network will not be uplearned, the training set will
remain constant from the last uplearning stage. As it was mentioned above, the
present decision is caused by the problems of equipment defects statistics accu-
mulation, and training without the training set completion leads to the increase in
the errors probability during the equipment state identiﬁcation.
232
A. Khalyasmaa and S. Eroshenko

Table 1 The transformers state identiﬁcation results
Method
T11
T12
T21
T22
T23
Rodgers
method
High-temperature defect
Insigniﬁcant overheating
(200–300 °C)
Undiagnosed state
High-temperature
defect
High-temperature
defect
Dornenburg
method
Thermal impact
–
–
–
–
60599 IEC
method
Overheating at a temperature
more than 700 °C
–
–
–
–
Nomograms
method
Thermal defect at high
temperatures
–
Defects caused by arc
–
–
ETRA
–
–
High power electrical
discharges
–
–
Duval’s triangle
Thermal defect at temperature
more than 700 °C
–
High power electrical
discharges
–
–
Expert
estimates
method
–
–
–
–
–
Training Samples Construction for Energy …
233

2.3
The Training Set Size
The training set size has a signiﬁcant impact and it should be sufﬁcient for correct
and certain equipment state identiﬁcation. It is generally accepted that if the
dependency between the analyzed variables is heavy it can be identiﬁed even is case
of small set. If the training set size is insufﬁcient it means that there are few possible
combinations of the present variables values and consequently the possibility of the
values combination identiﬁcation which show the heavy dependence is relatively
high. In the technical assessment problem the dependence between the analyzed
variables is week therefore the present dependence can be identiﬁed only for suf-
ﬁciently large set.
At the same time the smaller size of the required training set as compared to the
traditional multilayer neural network [8] is the neuro fuzzy inference advantage
which makes possible the sufﬁcient training set formation even under the condition
of the limited data. Analysis of the training set size impact on the equipment state
identiﬁcation accuracy in the context of 110 kV power transformers state identiﬁ-
cation was performed in the paper. For each element of 110 kV power transformer
its own neuro fuzzy inference structure was developed. The developed system
adjustment accuracy was evaluated on the basis of the training and test sets of
different size for 110 kV power oil transformers of the similar power and the same
type structure. The sets mentioned above are presented in Table 2. Due to the high
dimension of the current problem its veriﬁcation was performed on the basis of the
diagnostic data of the following elements: transformer oil (in the transformer tank)
—on the basis of the chromatographic analysis data of the dissolved in oil gases;
Table 2 Basic network parameters for the transformers state assessment
The pairs number
in the training set
30
40
50
60
70
80
90
100
The pairs number
in the test set
74
74
74
74
74
74
74
74
Number of nodes
524
524
524
524
524
524
524
524
Number of linear
parameters
243
243
243
243
243
243
243
243
Number of
nonlinear
parameters
45
45
45
45
45
45
45
45
Total number of
parameters
288
288
288
288
288
288
288
288
The fuzzy rules
number
243
243
243
243
243
243
243
243
Average training
error (%)
10.2
9.4
8.5
7.8
6.8
6.7
6.5
6.3
Average testing
error (%)
0.1081
0.0682
0.0594
0.0524
0.0495
0.0489
0.0488
0.0486
234
A. Khalyasmaa and S. Eroshenko

magnetic core—on the basis of no load measurements data; solid insulation—on the
basis of the measurements data of the winding insulation resistance; general state of
windings—on the basis of the windings’ ohmic resistances and also on the basis of
the transformer production and overhaul year (the operational lifetime data).
The training set size impact on the equipment state identiﬁcation accuracy was
veriﬁed during the obtained data analysis. It is seen from Table 2 that the more the
training set size the less the average training and testing errors, that is the more the
equipment state identiﬁcation accuracy. Consequently, the sufﬁcient amount of
statistical data should be accumulated during the training set formation, in this case
more than 70 pairs are required for the presented problem solving. As it is seen
from Table 2 when using 70 and more pairs there is a slight reduction in the
prediction error. In this case the set structure, information value and completeness
in addition to the training set size have an impact on the identiﬁcation results
accuracy. It is described in detail by the authors in [9, 10].
3
Conclusions
Determination of the minimum required training set size is one of the basic prob-
lems during the transformer equipment technical state assessment and reliable
identiﬁcation of the equipment state on the basis of neuro fuzzy inference. In order
to exclude the system model overﬁtting it is also required to use the independent
data for training and test sets, in this case the test and training sets should meet the
same requirements concerning the set structure, completeness etc. The requirements
mentioned above are explained by the fact that the system model further operation
and the equipment state identiﬁcation accuracy depend on the system model
adjustment. And on the real objects the defective training and test sets can lead to
the incorrectly identiﬁed equipment state and consequently to the false staff
operation.
Acknowledgements The reported study was supported by Russian Foundation for Basic
Research, research project No. 16-38-00310 (mol_a).
References
1. Shevelev L.V. Methods of analytical data to support decision making. DBMS 4–5 (1998) (in
Russian). http://infovisor.ivanovo.ru/press/paper04.html
2. Hastie T., Friedman, J.: The Elements of Statistical Learning: Data Mining, Inference, and
Prediction R. Tibshirani, 745 pp.. Springer (2009)
3. Zhou, Z., Wang, H., Lou, P.: Manufacturing Intelligence for Industrial Engineering: Methods
for System Self-Organization, Learning, and Adaptation, p. 407 (2010)
4. Lohanin, A.K.: Questions transformer production of 43 the session CIGRE. Electricity 1,
58–64 (2011) (in Russian)
Training Samples Construction for Energy …
235

5. Jang, J.-S.R.: ANFIS: adaptive-network-based fuzzy inference system. IEEE Trans. Syst.
Cybern. 23, 665–685 (1993)
6. Nguyen, Hung T., Prasad, Nadipuram R.: Fuzzy Modeling and Control: Selected Works of
Sugeno, p. 429. CRC Press, Boca Raton (1999)
7. Khalyasmaa, A.I., Bliznyuk, D.I., Verkhozin, A.A., Ovchinnikov, K.Y.: The problems of
dissolved in oil gases analysis results’ interpretation in information analytical systems. In:
Conference and Exposition on Electrical and Power Engineering (EPE) (in press)
8. Galushka, V.V., Fathi, V.A.: Formation of the training sample by using artiﬁcial neural
networks in search problems database errors. Inženernyj vestnik Dona (Rus) 2 (2013) (in
Russian). http://ivdon.ru/magazine/archive/n2y2013/1597
9. Khalyasmaa, A.I., Dmitriev, S.A., Verxozin, A.A., Sarapulov, S.F.: Hybrid neural network
and fuzzy logic methods for implementation of equipment actual state assessment of power
stations and substations. In: Proceedings of Modelling, Identiﬁcation and Control (MIC
2015), Innsbruck, Austria, pp. 167–170 (2015)
10. Khalyasmaa, A.I., Dmitriev, S.A.: Expert system for engineering assets’ management of
utility companies. In: Proceedings 10th edition of the IEEE International Symposium on
Diagnostics for Electric Machines, Power Electronics and Drives (SDEMPED 2015),
pp. 421–427 (2015)
236
A. Khalyasmaa and S. Eroshenko

A Host Program Implementation
for Linux File System Tracing Method
Using the Kprobes Linux Dynamic
Instrumentation System
Sang-Young Cho
Abstract A storage performance analysis tool is crucial to ﬁnding performance
bottlenecks in I/O storage systems and developing efﬁcient storage system archi-
tectures or algorithms. This work is based on an integrated performance analysis
tool for Linux ﬁle systems. The tool provides actual time information for Linux ﬁle
system functions. In contrast to other existing tools, the tool provides a ﬁltering
mechanism, a graphical interface, and system-level analysis information without a
heavy load of measurement. This paper describes a host program implementation
for the performance analysis tool. It may be used by Linux developers or end-users
for analyzing ﬁle system layers or measuring software performance to ﬁnd bot-
tlenecks in Linux ﬁle systems.
Keywords Linux ﬁlesystem ⋅Performance analysis ⋅EXT4 ⋅Kernel trace
1
Introduction
Though the application performance of an Android-based device can be affected by
many factors, the most inﬂuential factor is the performance of the underlying I/O
storage system [1]. The storage system is a collection of software layers such as
SQLite, EXT4, a block device driver, and a storage media driver. Each software
module of the I/O storage system has been utilized for hard-disk storage media.
Several performance issues began to occur when the integrated I/O storage system
is applied to battery-powered mobile devices that are using NAND-based media.
Therefore, many studies have been devoted to improving the performance of I/O
storage systems.
S.-Y. Cho (✉)
Division of Computer & Electronic Systems Engineering,
Hankuk University of Foreign Studies, 81 Oedae-ro, Mohyeon-myen,
Cheoin-gu, Yongin-si, Gyeonggi-do 17035, Korea
e-mail: sycho@hufs.ac.kr
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_29
237

Existing benchmarking tools measure storage system performance in various
ways. The most well-known tool is IOzone [2], which provides numerous testing
environments and data views with read/write speed and I/O turnaround time.
IOzone shows performance results in statistical values, but it is difﬁcult to use in
Android-based systems. AndroBench [3] was developed to benchmark the storage
performance of Android-based mobile devices at the application layer. These tools
can give the overall performance behavior at the system level, but they lack the
ability to report detailed inter- or intra-function behaviors that are useful for ana-
lyzing exact timing behaviors.
There are many function-level performance analysis tools, also known as
function tracers [4–8], such as ftrace, perf, strace, dtrace, and blktrace. These tools
allow users to trace almost all function-level events within the Linux kernel
including ﬁle system domain functions. However, the tools do not provide any
convenient interfaces to interpret or to ﬁlter the trace results, so users ﬁnd it difﬁcult
to translate the trace results into useful information for analysis. To get system-level
performance information, several tools should be used at the same time and each
tool’s trace information should be integrated to generate a wholistic view of
system-level performance data.
In this paper, we describe a host program implementation for the performance
analysis tool. It may be used by Linux developers or end-users for analyzing ﬁle
system layers or measuring software performance to ﬁnd bottlenecks in Linux ﬁle
systems.
This paper is organized as follows. In Sect. 2, we introduce the Linux ﬁle system
layers as well as the Kprobes Linux dynamic instrumentation system. Section 3
elaborates on the design of the Linux tracing tool and its host program imple-
mentation issues are discussed. The paper is concluded in Sect. 4.
2
Related Studies
2.1
Linux Filesystem Layer and Write System
Call Operation
The Linux ﬁle system can be analyzed by investigating the function call sequence
of the kernel when a read or write system call has been invoked by a user process.
All of the performance analysis information is acquired by collecting the timing
data of every invoked function’s call and return, which is then processed in order to
draw a complete picture of the read/write process. Though read and write system
calls are all frequently invoked ﬁle-related functions in the Linux ﬁle system, write
is the most inﬂuential system call on the entire storage system’s performance if the
ﬁle system is constructed on NAND-based storage media. This is because the write
operation of NAND-based media is more time-consuming than the read operation
238
S.-Y. Cho

and the journaling operation is performed during the write operations. Therefore,
we focus on analyzing the Linux ﬁle system’s write operation.
Figure 1 shows an example of the Linux ﬁle system’s layers. The Linux ﬁle
system has a layered structure that is divided into layers of VFS (Virtual File
System), EXT4 (for example), a generic block layer, the I/O scheduler layer and
block device driver, and physical storage media. Various ﬁle systems can co-exist
in a Linux system through the VFS interface. The generic block layer supports the
block-based device I/O. Each block device has its own queue and I/O requests are
inserted at the I/O scheduler layer. When storage media are eMMC or UFS devices,
the MMC block device driver, MMC core, and MMC host driver are in charge of
media accesses for storage I/O.
2.2
The Kprobles Linux Dynamic Instrumentation System
A static or dynamic Linux kernel tracepoint is a kind of breakpoint that provides a
hook to call a function (probe) to extract kernel behavior information [9]. Static
tracepoints cannot be inserted at run-time but they can be located at any points
throughout the program. Dynamic tracepoints can be easily inserted or enabled
during run-time, but they incur some overhead and are usually placed at function
entry and exit points. Kprobes enables us to dynamically break into any kernel
routine and collect debugging and performance information non-disruptively. The
Kprobes instrumentation is built as a kernel module. Thus, rather than having to
recompile and reboot the system with an instrumented kernel, a kprobe instru-
mentation module can be written, compiled, and loaded on the system. There is no
need to reboot the system. Once the instrumentation module has served its purpose,
it can be unloaded, and the kernel returned to its normal operation.
Fig. 1 An example of the
Linux ﬁle system’s layers
A Host Program Implementation for Linux File …
239

There are currently three types of probes: kprobes, jprobes, and kretprobes (also
called return probes). A kprobe can be inserted on virtually any instruction in the
kernel. A jprobe is inserted at the entry point of a kernel function, and provides
convenient access to the function’s arguments. A return probe ﬁres when a speciﬁed
function returns. Kretprobes also provides an optional user-speciﬁed handler which
runs on function entry. The performance analysis tool of this paper was aimed at
dynamic focusing. Probing overhead may not be signiﬁcant because our main
concern lies not on debugging information such as local variable investigation, but
on the functions’ timing information. The main focus of this work is the execution
time of each function in relation to ﬁle-related operations. Therefore, we modify
“kretprobe” to invoke entry and exit handlers for each kernel function.
Kretprobe can be applied within a kernel module using the following function
calls: int register_kretprobes(struct kretprobe *p) and void unregister_kretprobes
(struct kretprobe *p).
3
Software Structure and Operation Scenario
Figure 2 shows the software structure and the operating scenario of the storage
performance analysis tool.
A kernel module has been implemented under Fedora 3.13 using the kretprobe
interface to store the tracing data of the Linux ﬁlesystem on main memory. The
collected data is written on a text ﬁle after executing a test application that has ﬁle
access operations. To do this, user sets up the testing environment including writing
a test program and choosing functions to trace. The testing environment setting is
written as a text ﬁle and transmitted through Xshell via serial communication. The
target Linux system receives the text ﬁle and conﬁgures a corresponding test
environment based on the information of the ﬁle. The kernel module that controls
the whole trace operation starts gathering tracing data with kretprobe interface. The
result ﬁle is transferred to a Windows host PC through Xshell and analyzed by a
viewer program that is developed under .NET framework 4 in C# language.
Fig. 2 The structure and operating scenario of the storage performance analysis tool
240
S.-Y. Cho

The viewer program provides the storage performance analysis information using
visualized interface such as graph, grid view and trees.
The host control program initiates ﬁlesystem tracing and shows tracing results in
graphical views. Figure 3 shows the main window and a pop-up function selection
window of the host control program. The Kprobes mechanism requires function
names to call an entry and the return handler. These names must be registered using
the Kprobes API. This means the user must know the exact names of the functions
he/she wishes to trace. It is not easy to remember all of the function names.
Furthermore, function names may change from version to version in Linux kernels.
The host control program provides function names in four versions of Linux ker-
nels. The names are classiﬁed according to the ﬁle system layers. Therefore the user
can select a Linux version and a speciﬁc layer. The program lists the function
names for the chosen layer so that the user can select the functions that the user
wants to trace. The selected function names and trace environment setting infor-
mation are written to an input ﬁle and transferred to the trace control program of a
target system.
It is very important for a tracing tool to provide an intuitive and informative
interface. The tool presented here supports three types of viewing interfaces. The
graph view interface as shown in Fig. 4a displays the cumulative time of each
function of a result ﬁle in single mode view. The view enables the user to ﬁnd
time-consuming functions at a glance. It also supports a multiple mode view where
several test results can be compared to verify algorithmic performance enhance-
ments or the differences in performance of various ﬁle access modes. The grid view
interface of Fig. 4b shows the map data of a result ﬁle analysis. A ﬁle access system
call in the Linux Ext4 ﬁle system is performed by three threads: the main thread that
makes system calls, the jdb2 thread for journalling, and the mmcqd thread for
Fig. 3 The main window and a pop-up function selection window of the host program
A Host Program Implementation for Linux File …
241

low-level I/O. The tool in this paper can trace all the functions called by the three
threads and the traced data is displayed in three different grids. By providing the
layer information of each function, the user can easily recognize the overhead of
layers during ﬁle system access. The tree view interface in Fig. 4c displays the
whole call graph of all the individual function calls using the tree data structure of a
result ﬁle. The call graph display can be merged or expanded at a function so as to
hide all functions called by a speciﬁc function. This helps the user see only the part
of the call graph that is of interest. A pop-up window is used to show node time
information when the mouse is located at a speciﬁc function name.
4
Conclusion
In this paper, we describe the design and implementation of a powerful Linux ﬁle
system analysis tool that provides ﬁltering services, a graphical interface, and
system-level analysis information without incurring heavy penalties. The tool
mainly consists of an in-kernel tracing module and a host control program that runs
Fig. 4 User interface of the host viewer program
242
S.-Y. Cho

on a host PC. The host control program is developed on .NET framework 4 in C#.
The kernel module has low probing overhead compared to other existing tools. The
host control program sets up a performance tracing environment and provides three
types of performance viewers after analyzing a result ﬁle generated by the kernel
trace module.
Acknowledgements This research was supported by Basic Science Research Program through
the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT &
Future Planning (2015R1A5A7036384). This work was supported by Hankuk University of
Foreign Studies Research Fund of 2017.
References
1. Jeong, S., Lee, K., Lee, S., Son, S., Won, Y.: I/O stack optimization for smartphones.
In: Proceedings of the 2013 USENIX Annual Technical Conference, San Jose, USA,
pp. 309–320 (2013)
2. IOzone. http://www.iozone.org/
3. Kim, J.-M., Kim, J.-S.: AndroBench: benchmarking the storage performance of android-based
mobile devices. In: Proceedings of the Frontiers in Computer Education, vol. 133,
pp. 667–674. Springer, Heidelberg (2012)
4. Spear, A., Levy, M., Desnoyers, M.: Using tracing to solve the multicore system debug
problem. IEEE Comput. 60–64 (2012)
5. de Oliveria, D.B., de Oliveria, R.S.: Comparative analysis of trace tools for real-time Linux.
IEEE Latin America Trans. 12(6), 1134–1140 (2014)
6. Kernel Trace System. http://elinux.org/Kernel_Trace_Systems/
7. Bird, T.: Measuring function duration with Ftrace. In: Proceedings of the Linux Symposium,
Ottawa, Canada, pp. 47–54 (2009)
8. LTTng Documentation. http://lttng.org/docs/
9. Mavinakayanahalli, A., Panchamukhi, P., Keniston, J.: Probing the guts of Kprobes.
In: Proceedings of the Linux Symposium, Ottawa, Canada, vol. 2, pp. 109–123 (2006)
A Host Program Implementation for Linux File …
243

Simulation VANET Networks
on a Random and Realistic Spatial
Scenario
Suad Kasapovic and Lejla Banjanovic-Mehmedovic
Abstract The paper describes the use of VANET simulator to demonstrate the
potential and importance of their application in a network of vehicles, as well as
some of the network performance that can be predicted in this way. Using VANET
simulator is shown scenario analysis of selected trafﬁc parameters and network
performance, based on the coupling trafﬁc simulator VanetMobiSim and network
simulator ns2. Vanet simulator using OpenStreetMap for the insertion of real map
of the city.
Keywords VANET ⋅Networks ⋅Simulation ⋅ITS ⋅VanetMobiSim ⋅ns2
1
Introduction
VANET (Vehicular Ad hoc NETwork) network play a key role in ITS (Intelligent
Transport Systems). VANET comprises a number of technologies and active areas
of research, standardization and development. Therefore, issues of stability, scal-
ability, reliability, and network security vehicles is a challenge [1]. In VANET
network, mobile nodes are vehicles. Network topology changes often and very
quickly because of their high mobility and speed. For test purpose of VANET
networks, software simulations can play a major role in imitating real world sce-
narios because it simple, easy and cheap [2]. Comparative survey of several pub-
licly available mobility generators, network simulators, and VANET simulators are
presented in [3, 4]. This paper presents a one of available VANET simulation
software and their components and characteristics with simulation environment.
S. Kasapovic (✉) ⋅L. Banjanovic-Mehmedovic
Faculty of Electrical Engineering, University of Tuzla, Franjevacka 2,
75000 Tuzla, Bosnia and Herzegovina
e-mail: suad.kasapovic@untz.ba
L. Banjanovic-Mehmedovic
e-mail: lejla.mehmedovic@untz.ba
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_30
245

2
Vehicular Mobility Simulation for VANET
This article presents the simulation of network trafﬁc vehicles using two modules of
VANET simulator. In one case, VanetMobiSim is used as a trafﬁc simulator, and in
the second case, ns2 is used as a network simulator [5].
Trafﬁc simulator VanetMobiSim includes a number of models of mobility, and
parsers for geographic data sources in different formats and modules for data
visualization (Fig. 1).
It is based on the concept of switching modules, so that this software tool easily
expandable. Its main drawback is that it offers a poor documentation. A set of
extensions that contain VanetMobiSim, divided into spatial model and a
set-oriented model of mobility vehicles [6, 7]. The spatial model of the vehicle is
made up of spatial elements such as trafﬁc lights or multi-lane road, their attributes
and ways of linking these spatial elements. The spatial model is made up of
topological data obtained (user-deﬁned, random, geographic data ﬁles). Within it,
there is a set of models of mobility vehicles such as: Intelligent Driving Model with
Intersection Management (IDS_IM)—a model that perfectly describes the rela-
tionship vehicle-to-vehicle and managed intersections and Intelligent Driving
Model with Lane Changing (IDM_LC)—model that includes overtaking other
vehicles, communicate with IDM_IM management changes lane and accelerate or
slow the vehicle. VanetMobiSim provides many opportunities to create a realistic
scenario. The simulation scenario for VanetMobiSim deﬁned in XML format.
VanetMobiSim simulator requires a Java Development Kit (JDK) and Apache Ant,
tasked with developing Java applications. For the ﬁrst scenario analyzed in this
VANET scenario 
definition 
(VanetMobiSim)
Communications 
definition (ns-2)
VanetMobiSim
Vehicular traffic 
traces generator
Ns-2
MANET networks 
simulator
Vehicular 
traffic traces 
file (ns-2)
VANET simulation 
traces file
Fig. 1 VANET simulator based on coupling VanetMobiSim and ns-2
246
S. Kasapovic and L. Banjanovic-Mehmedovic

article is used Random Spatial Model. With <seed> tag speciﬁes the random
numbers of vehicles VanetMobiSim. With an instance of the class x.mobisim.
extensions.NSOutput deﬁned ﬁle format and with ns2 simulation selected time of
400 s, which is deﬁned by instance: x.mobisim.simulations.TimeSimulation. Spatial
environment is added with the help of instance x.spatialmodel.core.SpatialModel.
Spatial environment provides support for different types of roads and leaves the
possibility of the existence of the trafﬁc lights at intersections. Support for trafﬁc
lights added by instance x.spatialmodel.extensions.TrafﬁcLight. It is possible to
deﬁne the time interval between changes color trafﬁc lights, expressed in ms.
The XML ﬁle is implemented with random option, known as SpaceGraph using
instances x.spacegraph.SpaceGraph. It creates random graph constructed using the
Voronoi tessellation method to set random points. Characteristics of clusters (areas)
are given using the <cluster> tag. In the analyzed a random scenario density cluster
has a value of 0.000001 clusters/m2, which means that the surface area of
4,000,000 m2 divided into 400 clusters. The <ratio> determines the percentage of
certain types of cluster on the simulation area, <speed> tag speciﬁes the maximum
permissible speed in m/s on the segment path created with this kind of cluster.
Group nodes are added to the simulation using <nodegroup> tag. The presented
scenario includes only some of the possible speciﬁcations, such as maximum and
minimum speed, although VanetMobiSim includes many more features that can be
conﬁgured. During simulation, GUI environment shows the spatial model and its
constituent elements such as roads and facilities. Figure 2 shows random scenario
spatial model.
When the simulation is complete, a ﬁle named ns_trace is created. Ns_trace ﬁle
contains traces of mobility necessary for ns2 simulation. At the start of the simu-
lation are given x, y and z coordinates of each of the 10 monitored nodes. The ﬁrst
two coordinates describe the position of the nodes in the simulation area. The third
coordinate describes time observation of the vehicle in the area of simulation, in
seconds. Any change time of 0.1 s, gets the current position of the nodes. After
some time, some of the nodes disappear, because the duration of each node sub-
stantially less than 400 s. After losing a node, occurs another node with the same
label and the new duration. On the other hand, the ns2 network simulation and
provides simulation-level package through many different protocols in wired or
wireless network (the ability to use mobile nodes) [8]. The mobility of the nodes
can be speciﬁed either directly in the simulation ﬁle, or by using the mobility trace
ﬁles. To run the simulation, it is necessary to deﬁne the script by using the TCL
programming language. General simulation speciﬁcations, such as the duration of
the simulation, the simulation space, as well as some characteristics of networks and
channels are deﬁned. After completing the simulation, ns2 stops communication
and closes out.tr ﬁle containing information of the routing during simulation, as
well as scenario.nam ﬁle. From out.tr ﬁles can know more about the simulation. On
the output ﬁle out.tr can apply a ﬁlter to the selected part of the simulation. For
these purposes can be used awk scripts. Example start awk ﬁlter is awk -f ﬁlter.awk
out.tr. After completion of the awk scripts output.txt ﬁle contains traces such as the
number of sent and received packets or bits.
Simulation VANET Networks on a Random …
247

3
Simulation Using a Map Part of the City of Sarajevo
For the simulation can be used and a real map part of the city. In the real case, in
this paper is used map of Sarajevo along with tools Openstreetmaps [9]. After using
Openstreetmap ﬁnd speciﬁc area to perform simulations (in this example the core of
the city of Sarajevo) takes on the desired map. The already initiated the simulator,
choose the folder sarajevo.osm and imported into the simulator. When taking a
map, we will choose XML as the desired ﬁle format. The simulator is tested by
opening the folder sarajevo.xml and loading the script ﬁle sarajevo_scenarij.xml.
The urban part of the script, conﬁgured the way with two different directions. As for
the attributes of the vehicle, there are different speeds, as well as the level of trafﬁc
congestion. For vehicles has set the maximum speed and acceleration of the vehicle
that can be achieved. During the simulation you can display and some additional
information about the scenario, such as the communication distance, vehicle ID etc.
The movement of nodes and data transmission starts at t = 0 s. Simulation is used
with quite a large number of vehicles and wireless communication includes all
active vehicles. Figure 3 shows a transport map of the selected area of Sarajevo by
VANET simulator.
The blue arrow represents the area of observation related to transport and
communication. It analyzes the trafﬁc in the street Hamdije Čemerlića in Sarajevo,
Fig. 2 Illustration of the selected scenarios
248
S. Kasapovic and L. Banjanovic-Mehmedovic

at a distance of 192.8 m. For all vehicles that are observed, showing the commu-
nication distance as well as their ID code. Traces of simulations show that the total
number of active vehicles during the simulation is 20, and all the vehicles are
covered by wireless communication. The value of the mean velocity of the vehicle
is 49.43 km/h. The mean distance that vehicles crossing for a period of 60 s is
700.22 m.
4
Algorithmic Flow and Simulation Results
In terms of implementation of the simulator, it is ﬁrst necessary to install the
simulator ad hoc network VanetMobiSim then deﬁne spatial scenario parameters.
After starting the.xml script creates the output ﬁle with the extension.txt. Then, after
the installation of a network simulator ns2 and start.tcl script, it create at the output
ﬁle with extension.tr and.nam. Tcl script takes as input a vehicular trafﬁc traces ﬁle.
Thereafter, using ﬁlters such as awk script performs simulation analysis. As a result
of the examples presented here are given the ratio of the received packet (PDR—
Packet Delivery Ratio) and the ratio of lost packets (PLR—Packet Loss Ratio).
PDR is the ratio between the number of packets received at the destination node and
the number of packets transmitted from the source node and indicates the success of
the delivery of packages from source to destination. In order to calculate these data,
should be deﬁned by two counters, one that should increase for each successfully
received packet to the destination node, and another that would increase each time
you send a package. Figure 4 graphically shows the dependence of the number of
packages of the duration of the simulation.
Fig. 3 Simulation VANET communication on a map of Sarajevo
Simulation VANET Networks on a Random …
249

After completing simulations and the obtained values, as an example shown by
the ratio of received and lost packets, i.e., PDR and PLR value: 73.8% and 26.2%
respectively. Figure 4 shows the change in the value of the PDR and PLR during
the simulation time and where you see that at the end of the simulation the biggest
success of transmitted packets.
5
Conclusion
The vehicular safety application should be thoroughly tested before it is deployed in
a real world to use. Due to the high mobility of nodes and high speed of the vehicle,
routing data packets through VANET network is a very complex process. Disen-
tangling herself density trafﬁc, which affects the reliability of delivery of the
package. Before you embark on the implementation of projects with VANET
networks, is of great importance to make high-quality simulation as similar real
scenario, in order to reduce costs in the implementation of the project. The simu-
lation should be “closer to the” real scenario, trafﬁc and network performance.
Network simulators are used to evaluate network performances and the trafﬁc
simulators are used for trafﬁc engineering in a variety of conditions. Required
solution is to use trafﬁc and network simulator together, with proper coordination.
Such attempts are given in [10, 11].
References
1. Olariu, S., Weigle, M.C.: Vehicular Networks: From Theory to Practice. Chapman and
Hall/CRC Press (2012)
2. Guo, H.: Automotive informatics and communicative systems: principles in vehicular
networks and data exchange. Information Science Reference, Chapter XIV, pp. 264–282
(2009)
3. Mittal, N.M., Choudhary, S.: Comparative study of simulators for vehicular ad-hoc networks.
Int. J. Emerg. Technol. Adv. Eng. 4, 528–537 (2014)
Fig. 4 Number of sent, received and lost packets, depending on the time and their changes during
the simulation
250
S. Kasapovic and L. Banjanovic-Mehmedovic

4. Martinez, F.J., Toh, C.K., Cano, J.-C., Calafate, C.T., Manzon, P.: A survey and comparative
study of simulators for vehicular ad hoc networks. Wirel. Commun. Mobile Comput. 11(7),
813–828 (2011)
5. Härri, J., Fiore, M., Fethi, F., Bonnet, C.: VanetMobiSim: generating realistic mobility
patterns for VANETs. In: Proceedings of the 3rd ACM International Workshop on Vehicular
Ad Hoc Networks (VANET’06), Los Angeles, pp. 96–97 (2006)
6. University of Malaga, E.T.S.I. Informática, Dept. Lenguajes y Ciencias de la Computación.
http://neo.lcc.uma.esv
7. Fiore, M., Härri, J., Fethi, F., Bonnet, C.: Vehicular mobility simulation for VANETs. In:
Proceedings of the 40th IEEE Annual Simulation Symposium (ANSS’07), Norfolk (2007)
8. Boston University, Department of Computer Science. http://www.cs.bu.edu
9. Ramm, F., Topf, J., Chilton, S.: OpenStreetMap: Using and Enhancing the Free Map of the
World. UIT Cambridge (2011)
10. Su, Y., Cai, H., Shi, J.: An improved realistic mobility model and mechanism for VANET
based on SUMO and NS3 collaborative simulations. In: 20th IEEE International Conference
on Parallel and Distributed Systems (ICPADS), pp. 900–905 (2014)
11. Sanjay Kumar, A.K.A.: Trafﬁc simulation of vehicular cloud network using sumo. Int. J. Adv.
Res. Comput. Sci. Softw. Eng. 6(1), 379–383 (2016)
Simulation VANET Networks on a Random …
251

Sensor Module for Monitoring Wine
Fermentation Process
Dimitrija Angelkov and Cveta Martinovska Bande
Abstract This paper presents a module for monitoring the grape fermentation
process constructed of low cost sensors for temperature, wine acidity (pH), alcohol
and carbon dioxide released gases. Sensor values are recorded over the wine fer-
mentation process and are sent through wireless modules in real time to a server.
Constituent part of the module is a microcontroller PIC16F877A. It processes data
received from sensors and sends them to the server through Ethernet controller
ENC28J60. The main advantage of this low cost prototype is the possibility to be
used by small winemakers for control and monitoring of a grape fermentation
process. The proposed system has been tested in a winery in the Tikves region and
it fulﬁlled the initial expectations.
Keywords Sensor network ⋅Wine fermentation ⋅Temperature sensor
1
Introduction
Over the past decade sensor networks have gained increased attention due to the
applications in different areas, such as agriculture [1], environment monitoring [2],
air pollution [3], health care [4–7], positioning and tracking [8], localizations, trafﬁc
detection and avoiding road congestion [9], etc. Recent applications of wireless
sensor networks are systems for recognition of human activity and behavior which
enables ambient assisted living and home automated systems [10]. There are many
systems that use sensor networks based on different technologies, applications and
standards [11, 12].
This paper describes implementation of a system for remote monitoring and
control of sensor module installed in a winery. The aim is to monitor the conditions
D. Angelkov ⋅C. Martinovska Bande (✉)
Computer Science Faculty, University Goce Delcev, Shtip, Macedonia
e-mail: cveta.martinovska@ugd.edu.mk
D. Angelkov
e-mail: dimitrija.21098@student.ugd.edu.mk
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_31
253

in the winery as well as weather conditions for growing grapes. Sensors located in
the vineyard are used to gain knowledge about relations between soil and air
conditions and interventions like fertilization, treatment with chemicals and irri-
gation. Parameters relevant for these systems are: solar irradiation and temperature
which affect the sugar level and the ripeness of grapes, and air humidity which
stimulates the development of some fungi and could lead to spread of infections.
The temperature in the fermentation process has impact on wine characteristics
and quality. In the process of wine fermentation microorganisms transform must
into wine. Microorganisms use grape’s sugar to produce alcohol and CO2. As a
result of the fermentation process the heat of the must increases. In order to preserve
the microorganisms alive and for proper fermentation the temperature has to be
maintained at about 30 °C. The designed sensor prototype enables efﬁcient moni-
toring of the parameters important for wine fermentation and production using low
cost equipment.
There are many studies that report on developed sensor systems for monitoring
and control of wine production process [13] including temperature [14] sensing
phenolic components [15] or sulﬁtes in wines [16]. Several systems are proposed
for classiﬁcation of wines according to grape types and geographic origin [17–21].
For wine quality during winemaking process is important to measure some grape
quality parameters, including pH-value, tartaric acid and malic acid. To assess these
quality parameters of wine grapes during ripening different methods are developed
[22, 23].
2
Structure of the Portable Sensor Module
The microcontroller PIC16F877A is the main component of the system. This
component processes sensors data, transforms them from analog to digital and
sends the data to the server through Ethernet module ENC28J60. Our aim was to
develop small, portable device that can be inserted in the wine barrels, therefore the
size of the board was important for preparing the sensor module. Other reasons for
selection of this device were its high performance and low power consumption
which have inﬂuence over the system autonomy. The proposed sensor module is
applicable for small wineries. The owners of these wineries are not ready to invest
in technology so we searched for the cheap components that we can easily integrate
and program.
Among the many producers that offer commercial solutions we have chosen PIC
family of microcontrollers developed by Microchip Technology Inc. Important
properties of the PIC microcontrollers are wide availability, low cost, ease of
reprogramming with built-in EEPROM and many development tools. Microchip
Technology offers simple and economic development board PICDEM 2 Plus
(Fig. 1), software MPLAB ICD 3, low price microcontroller, with low consumption
and many applications and examples available on Internet. These reasons made this
company the best option for development of our prototype.
254
D. Angelkov and C. Martinovska Bande

PICDEM 2 Plus includes port for communication with the computer, in-circuit
programmer/debugger module for testing of the programs, access to the
input/output ports of the microcontroller and the necessary software and drivers for
the computer.
2.1
Description of the Microcontroller PIC16F877A
The microcontroller PIC16F877A is selected from the set of microcontrollers
developed by Microchip Technology due to the compatibility with PICDEM 2 Plus
development board. This microcontroller includes internal temperature sensor and
integrated analog-to-digital convertor, programmable in C, and has small price and
low power consumption.
The module for ADC conversion supports fast, 10 bits analog-to-digital con-
versions. It is conﬁgured with two control registers: control register ADCON0 and
control register ADCON1. The module is powered up with ADCON0 bit 0.
ADCON0 register controls the operation of the A/D module, while the ADCON1
register conﬁgures the functions of the port pins. The port pins can be conﬁgured as
analog inputs (RA3 can also be the voltage reference) or as digital I/O. Every time
the ADC module prepares and stores the conversion result in the register pair
ADRESH:ADRESL the data transfer controller is triggered without any software
intervention. The central processing unit is stopped to avoid any connection over
the data bus during the data transfer. When data transfer operation is ﬁnished then
CPU continues its work. In order to activate the integrated temperature sensor in the
microcontroller analog input has to be selected. With this sensor the microcontroller
uses the internal voltage reference. The way of accessing the ADC registers is the
same when external temperature sensor is used. As an external temperature sensor
is used LM35 precision integrated-circuit.
Fig. 1 PICDEM 2 Plus
development board
Sensor Module for Monitoring Wine Fermentation Process
255

2.2
Connecting the Microcontroller with the Network
Controller
The communication between the microcontroller PIC16F877A and the network
controller ENC28J60is established through 5 pins: RC0, RC1, RC3, RC4 and RC5.
Except RC4 the other pins are directly connected with the network module
ENC28J60 (Fig. 2).
Fig. 2 Electric scheme for data transmission from temperature sensor to the server through the
network module ENC28J60
256
D. Angelkov and C. Martinovska Bande

The microcontroller PIC16F877A and the Ethernet controller ENC28J60 operate
on different voltages. While the microcontroller operating voltage is 5 V the net-
work module has operating voltage range of 3.14–3.45 V. For that reason the
integrated circuit 74HCT08N is used as a logic multiplier which in this combination
acts as ampliﬁer for the logic signal that comes from the network controller.
3
Design and Development of the Prototype
To create the prototype we used advanced integrated development environment
FLOWCODE, developed by Matrix Technology Solutions, which includes tem-
plates for popular development boards such as Microchip devices. It enables
debugging during the development of the program after installing the drivers on the
computer (Fig. 3). The program developed in C language was embedded into the
microcontroller. After that the microcontroller without the computer can be used.
Using the design suite Proteus (Fig. 4) which is a product of Labcenter Elec-
tronics we designed a scheme to test the connections. Figure 4 shows schematic
design of the components. Using this software tool we designed the prototype, ﬁrst
with internal temperature sensor and then with external temperature sensor
LM35DZ.
Figure 5 shows the box with sensors ﬁxed on the wine reservoir. The sensor
module for monitoring and recording sensor data which is shown on Fig. 6 sends
values over Internet in real time.
4
Sensor Network
The next step of our project was to create a sensor network where one computer will
store the data from sensors installed on each of the reservoirs in the winery. To
connect the server which stores the data and each of the sensor modules installed in
the reservoirs we used the integrated circuit FT232BM (Fig. 7). The FT232BM
circuit was connected with the computer using USB port and served as a USB to
serial UART interface with data transfer rates from 300 baud to 3 Mbaud.
In the process of communication each of the reservoirs has a dedicated port, such
as COM3, COM4, etc. USB Hub device is used to solve the problem with the small
number of USB ports. The values from the sensors installed in the reservoirs are
read with time sharing. Each of the reservoirs has sets of sensors with microcon-
troller. The microcontroller through the UART protocol communicates with the
Sensor Module for Monitoring Wine Fermentation Process
257

Fig. 3 Block diagram and panel of the simulation
258
D. Angelkov and C. Martinovska Bande

integrated circuit FT232BM. Communication is performed using pin 25 for TXD
and 24 for RXD of the FT232BM with RS-232 transceiver MAX232ECWE which
transforms the voltage of the logic signals using speciﬁed UART protocol.
Fig. 4 Design and simulation of the electric circuit in PROTEUS
Fig. 5 Module for
monitoring and recording
sensor data
Sensor Module for Monitoring Wine Fermentation Process
259

5
Conclusion
This paper describes application of Internet based control of sensor network for
monitoring the fermentation process in a winery. The design of such a control
system includes requirements speciﬁcations, architecture design, algorithm design
and control interface for data access.
Fig. 6 Box with sensors ﬁxed on a reservoir
Fig. 7 Electric circuit USB2COM for the connection between the computer and reservoirs with
installed communication and sensor modules
260
D. Angelkov and C. Martinovska Bande

Speciﬁcally the paper presents design of sensor network for monitoring the
conditions like temperature and humidity in the winery and monitoring the fer-
mentation of wine by controlling the level of CO2, alcohol and temperature in the
barrels. Data from sensors are processed with PIC16F877A microcontroller and
sent to the server via Ethernet controller ENC28J60.
We expect that the data collected while using the system will be useful for wine
producers and will contribute to better quality of wines. The stored data can be
analyzed in order to reveal correlations between environmental parameters and the
resulting quality of the wine.
References
1. Ruiz-Garcia, L., Lunadei, L., Barreiro, P., Robla, J. I.: A review of wireless sensor
technologies and applications in agriculture and food industry: state of the art and current
trends. Sensors 9, 4728–4750 (2009)
2. Ong, J., You, Y.Z., Mills-Beale, J., Tan, E.L., Pereles, B., Ghee, K.: A wireless, passive
embedded sensor for real-time monitoring of water content in civil engineering materials.
IEEE Sensors J. 8, 2053–2058 (2008)
3. Yi, W.Y., Lo, K.M., Mak, T., Leung, K.S., Leung, Y., Meng, M.L.: A survey of wireless
sensor network based air pollution monitoring systems. Sensors 15(12), 31392–31427 (2015)
4. Lorincz, K., et al.: Sensor networks for emergency response: challenges and opportunities.
IEEE Pervasive Comput. 16–23 (2004)
5. Alemdar, H., Ersoy, C.: Wireless sensor networks for healthcare: a survey. Comput. Netw. 54
(15), 2688–2710 (2010)
6. Shnayder, V., Chen, B., Lorincz, K., Fulford, T., Jones, F., Welsh, M.: Sensor networks for
medical care. Technical Report TR-08-05, Division of Engineering and Applied Sciences,
Harvard University (2005)
7. Ghamari, M., Janko, B., Sherratt, R. S., Harwin, W., Piechockic, R., Soltanpur, C.A.: Survey
on wireless body area networks for eHealthcare systems in residential environments. Sensors
16(6), 831–864 (2016)
8. Hao, J., Brady, J., Guenther, B., Burchett, J., Shankar, M., Feller, S.: Human tracking with
wireless distributed pyroelectric sensors. IEEE Sensors J. 6, 1683–1696 (2006)
9. Nellore, K., Hancke, G.P.: A survey on urban trafﬁc management system using wireless
sensor networks. Sensors 16(2), 157–182 (2016)
10. Tunca, C., Alemdar, H., Ertan, H., Incel, O.D., Ersoy, C.: Multimodal wireless sensor
network-based ambient assisted living in real homes with multiple residents. Sensors 14(6),
9692–9719 (2014)
11. Akyildiz, I.F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: A survey on sensor networks.
IEEE Commun. Mag. 40(8), 102–114 (2002)
12. Buratti, C., Conti, A., Dardari, D., Verdone, R.: An overview on wireless sensor networks
technology and evolution. Sensors 9(9), 6869–6896 (2009)
13. Anastasi, G., Farruggia, O., Lo Re, G., Ortolani, M.: Monitoring high-quality wine production
using wireless sensor networks. In: Proceedings of 42st Hawaii International Conference on
Systems Science (HICSS-42 2009): Waikoloa, Big Island, HI, USA (2009)
14. Sainz, B., Antolín, J., López-Coronado, M., Castro, C.D.: A novel low-cost sensor prototype
for monitoring temperature during wine fermentation in tanks. Sensors 13(3), 2848–2861
(2013)
Sensor Module for Monitoring Wine Fermentation Process
261

15. Kim, S.K., Kwen, H.D., Choi, S.H.: Fabrication of a microbial biosensor based on
QD-MWNT supports by a one-step radiation reaction and detection of phenolic compounds in
red wines. Sensors 11, 2001–2012 (2011)
16. Chinvongamorn, C., Pinwattana, K., Praphairaksit, N., Imato, T., Chailapakul, O.:
Amperometric
determination
of
sulﬁte
by
gas
diffusion-sequential
injection
with
boron-doped diamond electrode. Sensors 8, 1846–1857 (2008)
17. Aguilera, T., Lozano, J., Paredes, J.A., Álvarez, F.J., Suárez, J.I.: Electronic nose based on
independent component analysis combined with partial least squares and artiﬁcial neural
networks for wine prediction. Sensors 12, 8055–8072 (2012)
18. Fu, J., Huang, C., Xing, J., Zheng, J.: Pattern classiﬁcation using an olfactory model with
PCA feature selection in electronic noses: study and application. Sensors 12, 2818–2830
(2012)
19. Gutiérrez, M., Llobera, A., Ipatov, A., Vila-Planas, J., Mínguez, S., Demming, S.,
Büttgenbach, S., Capdevila, F., Domingo, C., Jiménez-Jorquera, C.: Application of an
E-tongue to the analysis of monovarietal and blends of white wines. Sensors 11, 4840–4857
(2011)
20. Verrelli, G., Lvova, L., Paolesse, R., Di Natale, C., Damico, A.: Metalloporphyrin-based
electronic tongue: an application for the analysis of Italian white wines. Sensors 7, 2750–2762
(2007)
21. Twomey, K., Truemper, A., Murphy, K.: A portable sensing system for electronic tongue
operations. Sensors 6, 1679–1696 (2006)
22. González-Caballero, V., Pérez-Marín, D., López, M.I., Sánchez, M.T.: Optimization of NIR
spectral data management for quality control of grape bunches during on-vine ripening.
Sensors 11, 6109–6124 (2011)
23. Ghozlen, N.B., Cerovic, Z.G., Germain, C., Toutain, S., Latouche, G.: Non-destructive optical
monitoring of grape maturation by proximal sensing. Sensors 10, 10040–10068 (2010)
262
D. Angelkov and C. Martinovska Bande

Study the Transmittance Properties
of Light Sources Under Simulated Hazy
Condition
Haibo Yuan, Xiaoli Zhou, Zheqian Zhang and Fanghui Xu
Abstract With the rapid development of industrialization, pollutant from industry
and automobile has a big inﬂuence on the air quality of cities. With the high
humidity caused by the urban microclimate, the visibility is greatly reduced due to
the hazy weather formed by the accumulation of pollutant and vapor. In this paper,
a hazy simulation environment is established to measure the transmittance of LED
lamps, metal halide lamps and sodium lamps. First, we studied their transmittance
under the fog condition (liquid) as well as the haze condition (solid). Then, the
comparison among transmittance of light sources under different hazy environment
is shown. The results are useful to the road lighting design.
Keywords Hazy weather ⋅LED lamp ⋅Halide lamp ⋅Sodium lamp ⋅
Atmospheric aerosol
1
Introduction
All the studies of visibility of the target is under ideal conditions for road lighting or
other visual environment, that is the gaseous medium (air) is dry, clean, and
attenuation of light energy can be neglected. But in actually, all kinds of outdoor
lighting in different weather conditions, especially for bad weather such as hazy
condition, actual illumination value may be far lower than the theoretical calcula-
tion value. It is necessary to carry out the related research on the performance of
light sources in bad weather conditions to ensure that the actual lighting effect can
meet the requirements of relevant standards.
H. Yuan
School of Automotive Engineering, Wuhan University of Technology,
Wuhan 430070, China
X. Zhou (✉) ⋅Z. Zhang ⋅F. Xu
Institute for Electric Light Sources, Fudan University, Shanghai 200433, China
e-mail: zhouxl@fudan.edu.cn
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_32
263

Hazy weather, regarded as atmospheric aerosol, appear frequently in recent years
[1–3]. The light from light sources reduced rapidly in severe weathers due to
atmospheric aerosols’ extinction for light. The present studies are concentrated on
optical engineer and meteorology, the former aims to interpret the adsorption of
monochromatic light (e.g. laser) because of the atmospheric aerosols, the latter
tends to uncover the adsorption of natural light (e.g. sunlight) and the inﬂuence of
scattering or determination of visibility owing to atmospheric aerosols [4, 5]. Lots
of literatures mentioned how to measure the optical adsorption behaviors of aero-
sols quantitatively, however experimental data were rarely referred to and the
previous researches were not systematically. In the lighting ﬁeld, it’s more realistic
to understand the transmission properties of lamps under the condition of atmo-
spheric aerosol. Most of the studies were focused on the special light sources such
as laser, which can’t be used in road lighting. The studies are meaningless for
electronic light sources, especially for HID or LED lamps which have a compound
spectrum. Reference [6] accomplished the experiment with only several different
low-power LEDs to study the transmittance in fog conditions, Ref. [7] investigated
the optimal transmittance wavelength in fog weather, they compared the trans-
mission of light at a certain distance under different thickness of the fog. The results
showed the best wavelength is 578 nm. Whereas this study is useless for outdoor
lighting since the light sources for outdoor lighting are mostly compound light. In
this paper, LEDs with various wavelengths and color temperatures, metal halide
lamps and sodium lamps are used to study the transmittance in fog and haze
conditions.
2
Set-Up of the Experimental Device
2.1
Simulation of the Uniform Hazy Condition
The formation of fog requires plenty of water vapor in air and thermal cools sharply
or ﬂows past a cold surface, i.e. rather high humidity, or proper temperature and
suitable wind. The fog must be uniform, stable and controllable in the experiments.
In this paper an ultrasonic humidiﬁer is used to produce fog, minute droplets of
water is formed by making the atomization piece remain in high frequent resonance
with ultrasonic wave (1.7 MHz), the diameters of the water droplets is 5 μm which
are in agreement with the diameter of fog in nature, which is from 4 to 10 μm. The
uniformity of fog is guaranteed by a fan attached to the humidiﬁer, and different
thickness of fog is obtained through changing the gears of humidiﬁer. Haze is
aerosol system composed of non-aqueous materials, such as dirt, sulfuric acid, nitric
acid and hydrocarbon. The diameters of particles of haze mainly range from 0.001
to 10 μm. The relative humidity is small when haze happens, while the relative
humidity in fog is saturated. In this paper, we simulate haze by using air blower to
264
H. Yuan et al.

blow plant ashes into a box, through changing the quantity of plant ashes into the
box, we can achieve different densities of simulative haze.
The schematic diagram of hazy simulation device is shown in Fig. 1. This
experiment is to be completed under the dark environment to exclude the external
interference. This paper uses a large dark box of 4.2 m × 1.2 m × 1.2 m
(length × width × height). The ﬂoor of the box is spliced by two pieces of wood,
bracket is made of wood, and its surface is covered by a black plastic. The light
sources is installed at the end of the box, blow head of the humidiﬁer and power
supply is ﬁxed on both sides of the box by opening holes. There are slots at 1, 2,
and 3 m away from the light sources. Each slot is equipped with a long board and a
short one. Inserting the short board can only close slots, however, insert the long
one can separate the box into two independent space. We can test illumination value
of the different distance by changing the position of the long board. Make a hole at
the center of the long board to insert the detector of photometer. There is a guide in
the corresponding location in the box to ensure that the detector is ﬁxed.
2.2
Determination the Thickness of Haze
The air conditioner is used to keep room temperature stable to achieve the same
initial humidity. After setting the thickness of fog or haze, illumination values are
detected when the light source is stable. The data is recorded when haze inside the
box reaches uniform state, in which the illumination value is stable. The humidiﬁer
or blower is turned off before recording data in order to avoid the inﬂuence of air
The black box
Light souces
The holder
detector
Baffle plate
Humidifier spray head
/Blower export
Humidifier/Blower
Fig. 1 Schematic diagram of hazy simulation device
Study the Transmittance Properties of Light Sources …
265

ﬂow on the results. For example, in the fog condition, when adjusting the
humidiﬁer knob in the same position, blowing fog into the dark box and conducting
a number of tests for 40 W incandescent lamp, the illumination values detected at
1 m away is shown in Table 1.
Form Table 1, the average value of illumination is 280.2 lx, the standard
deviation S is 1.54 lx, S/Eavg ≈0.5% through calculation. That is a relatively
uniform fog ﬁeld can be obtained after ﬁxing the humidiﬁer gear for a period of
time. On this basis, fog thickness is divided into 1–3 three levels, and draw a tick at
the corresponding position of the knob to mark. The state that humidiﬁer is not
open is deﬁned as 0 gear. The fog thickness increases from 1 to 3 gear. The amount
of blown dust is changed to get different thickness of haze. Two different haze
thickness is tested in this paper, 0 gear presents no haze and haze thickness
increases from 1 to 2 gear.
2.3
Light Sources for the Experiment
LED lamps of different colors and different color temperature, incandescent lamps,
metal halide lamps and high pressure sodium lamps are chosen to do the experi-
ment; relevant information of lamps is shown in Table 2.
Table 1 Results for
incandescent lamp
The number of
experiments
1
2
3
4
5
6
E (lx)
282
278
279
282
281
279
Table 2 Parameters of the experiment lamps
No
Type of the lamps
Power (W)
Remarks
1
Incandescent lamp
40
40 W
2
Metal halide lamp
400
PHILIPS HPI-T400
3
High pressure sodium lamp
150
150 W
4
Red LED
48
Main wavelength of 625 nm
5
Green LED
48
Main wavelength of 660 nm
6
Blue LED
48
Main wavelength of 470 nm
7
Yellow LED
48
Main wavelength of 575 nm
8
White LED
48
Color temperature of 3200 K
9
White LED
48
Color temperature of 4500 K
10
White LED
48
Color temperature of 6500 K
266
H. Yuan et al.

3
Transmittance of Light Sources Through Different
Thickness of Fog
The dark box is set in a ventilated state, and air conditioner is turned on (20 °C, 1
gear wind velocity). Twenty minutes later, put the light source into the dark box
and connect the power. In this paper, the illumination is detected at a distance of
2 m from the light source. Long board is inserted in the slot at 2 m; other slots are
closed with short board. Insert the detector of photometer into the hole at the center
of the long board, and the blow head of the humidiﬁer nozzle aim at the vent hole.
Turn on the power; record the illumination value 30 min later when the system is
stable. Open the humidiﬁer and raise to the ﬁrst gear, record the illumination value
until the data does not change. Change humidiﬁer gears and repeat the above steps.
Remove the lamps and dry water within the box when complete the test, measure
another lamps after 20 min of ventilation. All the results for tested light sources are
shown in Table 3. Gear 0 is the illumination value without fog, gear 1, 2, 3 are the
measured illumination value for the fog thickness gradually increasing, transmit-
tance of different fog thickness is the illumination values under that thickness divide
with the value without fog, that is
T = E ̸E0
ð1Þ
T is transmittance, E is illumination value in the fog, and E0 is illumination value
without fog.
Figure 2 is the illumination of a variety of light sources at different thickness of
fog. It shows that the transmittance of all the light sources signiﬁcantly decreased
with the thickness of fog increasing.
Table 3 Transmittance of light sources under different thickness of fog
Illumination
Transmittance
Gear 0
Gear 1
Gear 2
Gear 3
Gear 1
Gear 2
Gear 3
Red LED
169
151.8
30.3
16.9
0.898
0.179
0.10
Green LED
297
270
50.5
31.9
0.909
0.170
0.107
Blue LED
63.9
54
14.3
6.8
0.845
0.224
0.106
Yellow LED
167.4
171
43
21.1
1.022
0.257
0.126
White LED (3200 K)
320
321
95
42.5
1.003
0.297
0.132
White LED (4500 K)
356
315
60.6
32
0.884
0.170
0.090
White LED (6500 K)
400
340
65.9
36
0.850
0.165
0.09
High pressure sodium lamp
1140
1130
220
160
0.991
0.193
0.1409
Metal halide lamp
3340
3300
1180
530
0.988
0.353
0.159
Incandescent lamp
24.1
23.6
7
4
0.979
0.290
0.166
Study the Transmittance Properties of Light Sources …
267

Figure 3 shows the transmittance of different light sources in the maximum
thickness of the fog (humidity is more than 90%).
The incandescent lamp has the best performance through the fog in all the types
of lamps. For different color LEDs, the order of performance through fog is yellow,
green, blue and red. The 3200 K White LED has best performance in white LEDs
with different color temperature. Further analysis of LEDs’ performance through
fog is shown in Fig. 4. The performance through fog ﬁrst increase and then
decrease with wavelength increasing, that is to say there is an optimal wavelength,
yellow due to the study, for using in the fog condition. This is consistent with prior
research. Transmittance of monochromatic radiation in atmosphere depends on two
factors:
the
absorption
and
scattering
of
the
atmosphere.
Therefore,
the
0.988
0.353
0.159
0
0.2
0.4
0.6
0.8
1
1.2
Gear 1
Gear 2
Gear 3
Transimittance
Red LED
Green LED
Blue LED
Yellow LED
White LED
3200K
White LED (4500K)
White LED11 (6500K)
High pressure sodium
lamp
Metal halide lamp
Incandescent lamp
Fig. 2 Transmittance of different light sources
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Transimittance
Fig. 3 Transmittance of different lamps in the same thickness of fog
268
H. Yuan et al.

transmittance of monochromatic light in the fog also depends on absorption and
scattering of fog.
The absorption of different medium to different wavelengths of light are not the
same, when use a humidiﬁer to produce fog, the main Component is water vapor.
There is such a law of the moisture absorption of light of different colors: the longer
the wavelength, the more absorbent. And Light is scattered mainly by Mie scat-
tering and Rayleigh scattering. Rayleigh scattering study the scattering phe-
nomenon of particles that the particle diameter is much smaller than the
wavelength. Mie scattering mainly study the scattering phenomenon that particle
diameter is greater than 10. The scattered light intensity is inversely proportional to
the fourth power of the incident wavelength in Rayleigh scattering. Namely, the
shorter the wavelength, the greater the scattering coefﬁcient. That is to say, the
greater the attenuation coefﬁcient, The lower transmittance. Scattered light intensity
is almost independent of the wavelength in Mie scattering. Therefore, considered
the absorption and scattering two factors, there is such a wavelength that the sum of
absorption and scattering is the minimum, so that transmittance is the maximum.
And the results obtained in our experiments is that the transmittance of yellow band
is the best, which agrees with the literature [7].
4
Transmittance of Light Sources Through Different
Thickness of Haze
The Measurement methods in the haze is similar to in the fog, the blower hole is
aligned with vent, measure dust with dosage cup in the box outlet, change the
amount of blown dust to adjust different haze thickness. Two different thickness are
measured in this paper, the experimental data is shown in Table 4. Gear 0 is data
without haze, Gear 1, 2 are data with haze. Transmittance of different haze thick-
ness is deﬁned the illumination values under that thickness divide with the value
without haze.
The transmittance of various lamps in haze at different thickness is shown in
Fig. 5, the comparison of transmittance of lamps in haze at the same thickness is
shown in Fig. 6.
RED LED
Yellow 
LED
Green LED
Blue LED
0
0.05
0.1
0.15
0
1
2
3
4
5
Transmittance
Fig. 4 Transmittance of
LEDs
Study the Transmittance Properties of Light Sources …
269

It can be conclude from Figs. 5 and 6 that:
(1) The transmittance of all light sources decrease with the thickness of haze
increasing.
(2) Different lamps has different performance through haze, the order of the
transmittance is LED, Metal Halide lamp, Incandescent lamp and sodium
lamp. LED has best performance among all the light sources.
(3) For white LED, the higher the color temperature, the better performance
through haze.
(4) For LEDs with different color, the order of the performance through haze is
yellow, red, green and blue in higher thickness of haze.
Table 4 Illumination values in different thickness of haze
Type of lamp
Illumination values
Transmittance
No haze
Gear 1
Gear 2
Gear 1
Gear 2
Incandescent lamp
17.9
17.1
11.5
0.956
0.642
High pressure sodium lamp
60.6
46.2
28.5
0.762
0.470
White LED (3500 K)
243
228
174.3
0.938
0.717
White LED (4500 K)
265
247
202
0.932
0.762
White LED (6500 K)
281
273
218
0.972
0.776
Red LED
122.5
119.7
80.9
0.977
0.660
Yellow LED
92.2
90.1
70.4
0.977
0.764
Blue LED
79.4
78.1
49.2
0.984
0.620
Green LED
229
215
149.9
0.939
0.655
Metal halide lamp
544
485
328
0.892
0.603
0
100
200
300
400
500
600
No haze
Gear 1
Gear 2
Illumination value
Incandescent lamp
High pressure sodium lamp
White LED
3200K
White LED (4500K)
White LED11 (6500K)
Red LED
Yellow LED
Blue LED
Green LED
Metal halide lamp
Fig. 5 Illumination value of light sources in different thickness of haze
270
H. Yuan et al.

5
Conclusion
A uniform hazy simulated environment is produced in this paper, and experiment is
carried out to study the performance of different light sources, such as LEDs with
different color and color temperature, incandescent lamp, metal halide lamp and
sodium lamp, through fog and haze respectively. The results show that light sources
of different types and different colors through fog and haze have different
performance.
References
1. Jue, D., Liying, L.: An analysis on characteristics of particulate pollutants and the process of
absorbing gaseous pollutants in hazy weather. Shanghai Environ. Sci. 28(1), 11–14 (2009)
2. Huang, S., Li, Z., Yang, J.: Light absorbing properties of aerosols in the atmospheric boundary
layer over China. Plateau Meteorol. 19(4), 487–494 (2000)
3. Yang, X., Luo, J., Wang, J., et al.: Measurement of the absorption coefﬁcient of light by aerosol
particles in atmosphere. High Power Laser Part. Beams 15(6), 543–546 (2003)
4. Li, X., Hu, S., Xu, Q., et al.: Characteristics measurement of extinction and refractive index of
aerosol particles. High Power Laser Part. Beams 19(2), 207–210 (2007)
5. Li, X., Li, W., Zhou, X.: Analysis of the solar radiation variation of China in recent 30 years.
Q. J. Appl. Meteorol. 9(1), 25–31 (1998)
6. Kurniawan, B.A., Nakashima, Y., Takamatsu, M., et al.: Visual perception of color LED light
in dense fog. J. Light Vis. Environ. 31(3), 152–154 (2007)
7. Lv, Z., Xu, T.: Determination of optimum penetration wavelength for fog lights. J. Appl. Opt.
29(4), 530–532, 547 (2008)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Transmittance
Fig. 6 Transmittance of various lamps in the same thickness of haze
Study the Transmittance Properties of Light Sources …
271

Numerical Study on the Thermal Fatigue
of Cryogenic Vacuum Insulated Pipe
Jae-Hoon Lee, Si-Pom Kim, Rock-Won Jeon and Geun-Ho Lee
Abstract A vacuum insulated pipe used to transport LNG was exposed to repeated
thermal loads at an extremely low temperature in order to study its thermal-fatigue
lifetime. To prevent fatigue failure, the lifetime of vulnerable parts needs to be
estimated beforehand; therefore, thermal stress and thermal deformation were ﬁrst
observed using thermal-structural coupling analysis. Based on the results, the
structural safety of the vacuum insulated pipe was evaluated. Furthermore, fatigue
life was evaluated according to the design parameters by applying the boundary
condition
of
thermal
fatigue
analysis
based
on
the
results
from
the
thermal-structural coupling analysis.
Keywords Vacuum insulated pipe ⋅Bellows type ⋅Joint type ⋅Thermal-
structural analysis ⋅Thermal fatigue analysis
1
Introduction
Natural gas is one of the world’s three major energy resources including petroleum
and coal, and its production and demand rate are increasing [1]. As a result, the
development of natural gas transport and storage technology is accelerating. The
use of natural gas as fuel is also increasing; however, the pipe used for LNG
transport is exposed to various environments, and approximately 0.15% of the LNG
J.-H. Lee (✉) ⋅S.-P. Kim (✉) ⋅R.-W. Jeon ⋅G.-H. Lee
Department of Mechanical Engineering, Dong-a University, Busan City,
Republic of Korea
e-mail: dhxowkd31019@naver.com
S.-P. Kim
e-mail: spkim@dau.ac.kr
R.-W. Jeon
e-mail: rockwoni@naver.com
G.-H. Lee
e-mail: lgh6072@gmail.com
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_33
273

evaporate sowing to temperature differences. This evaporation, known as boil-off
gas (BOG), has an adverse effect on the transport and storage of LNG [2, 3]. To
prevent BOG, a vacuum insulated pipe was introduced to cut off the ﬂow of
external heat. A vacuum insulated pipe is a double-structure pipe with a vacuum
layer that helps to minimize heat conduction/convection [4]. The disadvantage of
vacuum insulated pipe is that it is affected by the adverse thermal load generated by
the low-temperature LNG owing to structural problems. The continuous and
repeated thermal load of the LNG ﬂowing in the pipe causes thermal fatigue in the
pipe [5]. Research on the durability of vacuum insulated pipe and fatigue problems
is sparse, and the fatigue fracture mechanism needs to be studied. When the vacuum
of a vacuum insulated pipe is lost, the insulation performance drops abruptly as the
convective heat transfer increases through the insulating material; the pipe breaks as
a result of the sudden pressure change, and subsequently, other equipment may be
damaged [6]. Therefore, fatigue fracture needs to be prevented by predicting the
vulnerable areas in the vacuum insulated pipe using thermal fatigue analysis [7, 8].
In this study, a thermal fatigue analysis was performed for the cryogenic vacuum
insulated pipe according to pipe type and diameter. Thermal stress and thermal
deformation were observed by thermal structural coupling analysis, and the struc-
tural stability of the vacuum insulated pipe was evaluated. Moreover, based on the
temperature distribution, stress distribution, and strain changes obtained from the
thermal structural coupling analysis and applying them as the boundary conditions
in the thermal fatigue analysis, the fatigue life was evaluated according to the
design factors of the vacuum insulated pipe. The results are provided as basic data
for the thermal fatigue study.
2
Numerical Analysis
2.1
Finite Element Method Model
The vacuum insulated pipe used as the model in the numerical analysis was a
circular pipe, and its model was reduced in size by applying the symmetry condition
for modeling. The model used in the actual numerical analysis was 1/16 the size of
the vacuum insulated pipe. The types of pipe and parameters used in the numerical
calculation are shown in Fig. 1 and Table 1, respectively.
(a) bellows type  
 
 
(b) joint type 
Fig. 1 Types of vacuum insulated pipe
274
J.-H. Lee et al.

The pipe length in Table 1 is the total length of the numerical model applied in
the calculation, and the materials commonly used for the insulation and pipe were
selected for the numerical analysis also.
2.2
Thermal Structural Coupling Analysis
A transient thermal analysis suitable for the time-dependent transient analysis was
completed before the thermal structural coupling analysis. The total calculation time
was set to 30 min, and the transient thermal analysis was carried out at 1-s intervals
to increase precision.
The temperature distribution acquired by the transient thermal analysis was
applied in the thermal structural coupling analysis, while forced deformation was
applied as the boundary condition at both ends of the pipe. The conditions for
thermal and thermal structural coupling analysis are summarized in Table 2. The
thermal stress and thermal deformation data were obtained for the thermal fatigue
analysis.
2.3
Thermal Fatigue Analysis
The thermal fatigue analysis was performed based on the stress distribution
obtained from the thermal structural coupling analysis. The parameters used in the
numerical calculation are shown in Table 3.
Table 1 Design parameters of vacuum insulated pipe
Vacuum insulated pipe type
Bellows, joint
Pipe length (mm)
2000
Pipe radius (mm)
15, 50, 100, 150
Insulation material radius (mm)
15
Insulation material
Polyurethane
Pipe material
SUS304
Table 2 Boundary condition for thermal structural coupling analysis
Inner tube temperature
−179 °C
Outer tube temperature
22 °C
Convective heat transfer coefﬁcient
6.5 W/m2 K
Radiation rate of pipe
0.1
Radiation rate of insulation material
0.2
Internal pressure condition
320 bar
Vacuum pressure
10−3 Torr (−6.6661e−7 MPa)
Numerical Study on the Thermal Fatigue …
275

The surface roughness weakens the fatigue behavior. In this study, the surface
was treated to enhance the fatigue strength. In addition, the structure analysis result
in terms of multi-axis load was converted to a single-axis load for depiction on the
S-N diagram.
3
Results and Discussions
3.1
Thermal Structural Coupling Analysis
The thermal stress and thermal deformation according to the diameter and type of
vacuum insulated pipe results of the thermal structural coupling analysis are shown
in Fig. 2.
Based on the result of the thermal structural coupling analysis, the maximum
thermal stress and thermal deformation occurred at both ends and the middle of the
pipe for both the joint type and the bellows type. Changes in thermal stress and
thermal deformation increased as the pipe diameter increased, and the maximum
joint-pipe thermal stress and thermal deformation at the largest diameter of 150 A
were 228.34 MPa and 0.1 mm, respectively. However, the maximum thermal stress
was much less than the allowable stress of SUS304, and the thermal deformation
was negligible, making it structurally stable.
Table 3 Parameter for
thermal fatigue analysis
Surface roughness factor
Polished
Short-axis stress condition
Von-Mises stress
Stress life estimation
Goodman corrected equation
Load type
Ratio
(a) Thermal stress
(b) Thermal deformation 
Fig. 2 Thermal structural coupling analysis result for vacuum insulated pipe
276
J.-H. Lee et al.

3.2
Thermal Fatigue
The results of thermal fatigue analysis using the thermal stress data of thermal
structural coupling analysis for different sizes and types of pipe are shown in Figs. 3
and 4.
Based on the thermal fatigue analysis results, the 150A case showed the lowest
fatigue lifetime. The fatigue lifetimes of the bellows-type and join-type pipes were
3.572E6 and 1.744E6, respectively, proving that the bellow-type pipe structure is
more stable. The fatigue lifetime decreases at the curved surface for both pipe types
due to the stress distribution obtained from the thermal-structural coupling analysis.
This result demonstrates that pipe fatigue lifetime is inversely proportional to its
stress level. The thermal fatigue results according to fatigue lifetime, type, and pipe
size are shown in Fig. 5.
(a) 15A
(b) 50A
(c) 100A
(d) 150A
Fig. 3 Thermal fatigue analysis of bellows-type pipe
(a) 15A
(b) 50A
(c) 100A
(d) 150A
Fig. 4 Thermal fatigue analysis of joint-type pipe
Numerical Study on the Thermal Fatigue …
277

Of the four pipe sizes, the bellows type was found to be the safest considering
the fatigue lifetime, but since the fatigue lifetime varied with the size of the pipe,
additional studies using a variation of pipe diameters are needed.
4
Conclusions
In this study, the structural safety of vacuum insulated pipe was evaluated by
thermal and thermal structural coupling analysis. Based on the results of these
analyses, a thermal fatigue analysis was carried out, and the conclusions are as
follows.
1. We found a lack of examples of fatigue lifetime calculated by thermal fatigue
analysis for vacuum insulated pipe. The results of this study can be used as basic
data for estimating the lifetime of a vacuum insulated pipe.
2. The thermal structural coupling analysis showed that the thermal stress and
thermal deformation were higher for the joint-type pipe (228.34 MPa and
0.1 mm, respectively) than the bellows-type pipe, but the thermal stress was still
lower than the SUS304 allowable stress, and thus, the joint-type pipe is con-
sidered structurally stable.
3. The thermal fatigue analysis showed that the minimum fatigue lifetime of the
bellows-type pipe (3.572E6) was higher than that of the joint-type (1.744E6) for
a pipe size of 150A; therefore, the bellows-type is recommended for a 150A
pipe.
4. The effect of pipe size on fatigue lifetime needs to be further investigated.
Acknowledgements This work was supported by the Materials and Components Technology
Development Program of MOTIE/KEIT. [10046500/Development of Cryogenic (−196 °C), high
pressure (320 bar) welded insulation piping and valve jacket.]
(a) bellows type 
(b) joint type  
Fig. 5 Thermal fatigue according to type and size of pipe
278
J.-H. Lee et al.

References
1. Lee, D.-S.: A developing tendency of liqueﬁed natural gas carriers. J. Korean Soc. Marin
Environ. Saf. 15(3), 269–274 (2009)
2. Lee, D.H., Jang, C.B., Jung, S.Y., Kim, J.H., Lee, H.S., Kim, B.S., Ko, J.W.: Study on the
comparison of new and used reliquefaction system of boil-off-gas by LNG cold energy.
J. Korean Inst. Gas 14(1), 42–46 (2010)
3. Lee, Y.P., Shin, Y.H., Lee, S.H., Kim, K.H.: Boil-off gas reliquefaction system for LNG
carriers with BOG-BOG heat exchange. J. Soc. Nav. Arch. Korea 46(4), 444–451 (2009)
4. Kim, H.-Y., Kang, B.H.: Thermal Insulation technology for cryogenic cooling devices.
J. Korean Soc. Precis. Eng. 19(11), 24–30 (2002)
5. Hong, S.W., Seok, C.S., Koo, J.M.: Study on the fatigue life of spiral welded waterworks pipe
material. Korea Soc. Mech. Eng. 2395–2398 (2015)
6. Lee, H.-Y., Kim, Y.: A Study on thermal performance for damaged vacuum insulation. Arch.
Inst. Korea 34(2), 319–320 (2014)
7. Lee, J.-H., Lee, J.-S.: On the Fatigue Analysis of Large Crane Pedestal in Drillship. J. Soc.
Nav. Arch. Korea 48(4), 342–345 (2011)
8. Choi, B.L., Chang, H.: Prediction of thermal fatigue life of engine exhaust manifold under
thermo-mechanical cyclic loading. Trans. Korean Soc. Mech. Eng. A 34(7), 911–917 (2010)
Numerical Study on the Thermal Fatigue …
279

Unconventional Usage of Entropy
in the Field of Web Usage Data
Preprocessing and Machine
Translation Evaluation
Michal Munk and Ľubomír Benko
Abstract This paper focuses on an unconventional usage of entropy. On one side it
deals with preprocessing phase, especially the session identiﬁcation using the
Reference Length method. Entropy, in this case, offers an alternative to determining
the ratio of auxiliary pages that is important for this method. With the approach
introduced in this paper, the need of a sitemap becomes void. On the other hand, the
paper looks at entropy in the case of reliability analysis of Machine Translation
metrics. In this case, entropy offers also an alternative mean to validate the metrics.
Keywords Entropy ⋅
Data preprocessing ⋅
Reference length ⋅
Machine
translation
1
Introduction
Entropy offers a lot of possibilities to many ﬁelds of study. This paper focuses on
Information Entropy introduced by Shannon and its unorthodox usage in two ﬁelds—
Web Usage Mining and Machine Translation evaluation. Entropy can be used as a
measure of disorder, where lower entropy means order and higher entropy on the
contrary disorder. Following the deﬁnitions by Shannon [1], entropy can be used as a
measure of uncertainty in a data set.
The rest of the paper is structured as follows: in Sect. 2 is summarized the
related work of other authors about entropy and maximum entropy. In the ﬁeld of
M. Munk (✉) ⋅Ľ. Benko
Faculty of Natural Sciences, Department of Informatics,
Constantine the Philosopher University, Nitra, Slovakia
e-mail: mmunk@ukf.sk
Ľ. Benko
e-mail: lubomir.benko@gmail.com
Ľ. Benko
Institute of System Engineering and Informatics,
University of Pardubice, Pardubice, Czech Republic
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_34
281

Web Usage Mining was the aim of the paper to create an alternative way to
calculate the ratio of auxiliary pages for the session identiﬁcation method Reference
Length. This is dealt with in Sect. 3. To validate Machine Translation metrics were
usually used Cronbach’s alpha or Standardized alpha. Section 4 deals with another
aim of the paper, to analyze entropy as an alternative mean to evaluate the metrics.
Subsequently, the discussion and future work are offered in the last section.
2
Related Work
The ﬁrst concept of entropy originates from thermodynamics [2], where it was used
to provide a statement of the second law of thermodynamics on the irreversibility of
the evolution, i.e. an isolated system cannot pass from a state of higher entropy to a
state of lower entropy [3]. Shannon (1948) was the ﬁrst to re-deﬁne entropy and
mutual information, for this purpose he used a thought experiment to propose a
measure of uncertainty in a discrete distribution based on the Boltzmann entropy of
classical statistical mechanics [3]. Entropy can be described as a measure of the
expected content of the information or uncertainty probability distribution. It is also
described as the degree of disorder or randomness in a system. Based on Shannon’s
deﬁnition [1, 4], given a class random variable C with a discrete probability dis-
tribution
pi = Pr C = ci
½

f
gk
i = 1, ∑k
i = 1 pi = 1 where ci is the ith class. Then the
entropy H C
ð Þ is deﬁned as H C
ð Þ = −∑k
i = 1 pi log pi, while the function decreases
from inﬁnity to zero and pi takes values from interval 0–1 [1, 4].
E.T. Jaynes formulated [5] the principle of Maximum Entropy and transformed
that way entropy to a modeling tool. Maximum Entropy is used to estimate
unknown parameters of a multinomial discrete choice problem, whereas the Gen-
eralized Maximum Entropy includes noise terms in the multinomial information
constraints [3].
3
Entropy in the Field of Web Usage Data Preprocessing
Data preprocessing is a crucial part of Web Usage Mining and based on another
experiment [6] especially session identiﬁcation can prove important. Authors in [7]
combine the maximum entropy model for the recommendation system. Their results
showed that the recommendation system can achieve better accuracy than standard
Markov model for page recommendation. It also provided a better interpretation of
web users’ navigational behavior. Authors in [8] focused on comparing the per-
formance of maximum entropy with Naïve Bayes and Support Vector Machine,
where the entropy outperformed both of them.
In the experiment [6] was the assumption about the ratio of auxiliary pages
estimated for the session identiﬁcation method Reference Length based on the
sitemap. Based on the ratio can be determined the cutoff time C = −ln 1 −p
ð
Þ
λ
[6]. The
282
M. Munk and Ľ. Benko

sitemap can offer an accurate estimate of the ratio but can be problematic if the
examined web portal was changed in the meantime. In that case is the log ﬁle the
only possible option to get the necessary data. One option would be to extract the
sitemap from the log ﬁle using a complicated algorithm.
The experiment was conducted on a log ﬁle of a course of virtual learning
environment (VLE) portal. The log ﬁle was prepared using standard data mining
techniques same as in [9]. The ﬁnal log ﬁle was imported to a database where were
conducted several experiments involving the entropy, on the basis of which could
be distinguished navigation pages from the content pages from one another. The
aim was to create an algorithm that would be able to calculate entropy for a speciﬁc
page on the basis of a random variable Length that represents the length of the time
spent on each web page of the portal. With the use of the algorithm was created the
variable Relative Time, which represented the relative time spent on the page by the
user. From this variable was derived MaxEnt for each page and created a new data
matrix (Table 1) that contained MaxEnt of each page. Subsequently was calculated
the average length of all accesses on the web portal and served as the cutoff time
that divides the pages to auxiliary and content pages. Pages with smaller MaxEnt
than the MaxEnt of the average length of time spent across the whole portal will be
classiﬁed as auxiliary pages. On the opposite pages with higher MaxEnt will be
content pages.
In the log ﬁle of the course of VLE portal were identiﬁed 58 pages. Using the
algorithm were 10 pages classiﬁed as auxiliary and the rest (48 pages) was clas-
siﬁed as content pages (Table 2). Therefore the ratio of auxiliary pages of the web
portal is 17.24%. Another option for specifying the time threshold of time spent on
the web portal were quartiles. The time was calculated by QSTT = QIII + 1.5Q, where
QIII represents the upper quartile and Q represents the quartile range. Using the
quartiles were 9 pages classiﬁed as auxiliary and 49 pages were identiﬁed as content
pages. Compared to the calculation of the ratio of auxiliary pages based on the
sitemap (15.34%) that was based on previous research [6], is the ratio calculated
based on MaxEnt average time spent on the whole portal higher by almost 2%. But
the ratio calculated based on MaxEnt quartiles of the time spent on the whole portal
was almost similar (15.51%) to the sitemap calculation. In comparison to the
subjective estimate of the ratio (25%), that was determined by the administrator of
the web portal, the estimates calculated from the sitemap or MaxEnt offer more
accurate results. The inﬂuence of more accurately calculated ratio of auxiliary pages
in the session identiﬁcation phase was described in more detail by authors in [6].
Table 1 Data matrix
extended by maximum
entropy
URL ID
Length
Relative Time
MaxEnt
58450
24
0.0039636
7.9789487
661
138
0.0001486
12.7156915
69022
48
0.0008319
10.5307314
69022
6
0.0001039
13.2311711
⋮
⋮
⋮
⋮
Unconventional Usage of Entropy in the Field …
283

4
Entropy in the Field of Machine Translation Evaluation
Entropy can be used not only in the phase of preprocessing but also for analysis of
reliability. Authors experiment with entropy also in the ﬁeld of Machine Transla-
tion. Authors in [10] offer a complex survey of data selection methods in Machine
Translation. They also describe articles that focused on cross-entropy which has
become the most commonly used approach in data selection. Authors in [11]
introduce a novel framework based on maximum entropy for word alignment.
Based on the experiment the authors improved the alignment quality and translation
quality as measured by standard reliability metrics.
PER, WER, and CDER metrics are called metrics of error rate, i.e. the higher
values of these metrics, the lower the translation quality. On the other hand, metrics
Precision, Recall, F-measure, and BLEUs are called metrics of accuracy, i.e. the
higher values of these metrics, the better the translation quality. The automatic
metrics deﬁning Machine Translation error rate and representing the automatic
evaluation of Machine Translation are considered highly reliable based on the direct
estimation of reliability. The aim of this experiment was to assess the reliability of
the automatic evaluation of machine translation for inﬂectional languages using
traditional methods and entropy; in this case on-line statistical machine translation
system was used.
All items (Table 3) correlate (Avg inter-metrics correlation: 0.885) with the total
score of evaluation and after their eliminating the coefﬁcient of reliability has not
increased except the WER metric (Cronbach’s alpha: 0.947; Standardized alpha:
0.950). After elimination of the metric WER, the coefﬁcient of reliability—Cron-
bach’s alpha increased from 0.947 to 0.964, but that it is negligible.
For the entropy calculation (Table 3), in the case of analysis of automatic metrics
characterizing the error rate of Machine Translation evaluation, individual metrics
in comparison over accuracy metrics were used. Entropy was calculated for each
sentence analyzed using the speciﬁc metrics and for the comparison was used the
Table 2 Ratio of auxiliary pages based on different calculations
Auxiliary pages
Content pages
Ratio (%)
Subjective estimate
–
–
25
Sitemap calculation
29
189
15.34
MaxEnt of average
10
48
17.24
MaxEnt of quartiles
9
49
15.51
Table 3 Statistics of automatic metrics of error rate
Metrics-total correlation
Alpha if deleted
Metrics-total accuracy entropy
PER
0.878
0.934
0.934
WER
0.845
0.964
0.852
CDER
0.958
0.869
0.895
284
M. Munk and Ľ. Benko

average entropy of all sentences. From the Shannon deﬁnition of entropy [1], if the
entropy is closer to 1, then the system is more irregular. The results of the entropy
for each of the error rate metric relate with the coefﬁcient of reliability—Cron-
bach’s alpha.
Even though the reliability analysis of automatic metrics characterizing the error
rate of Machine Translation evaluation showed that the WER metric is the most
deviated from the other automatic metrics of automatic Machine Translation
evaluation (PER or CDER) in translation quality assessment. It is understandable
seeing that, the WER metric is very strict to syntax errors (word order).
The estimations of the entropy of automatic metrics of accuracy were similarly
calculated as in the case of metrics of error rate. Also, in this case, was used the
average entropy of all sentences for each metric and the results relate with the
coefﬁcient of reliability—Cronbach’s alpha with negligible variations. In the case
of entropy, it also showed that the metric BLEU-4 deviates the most from the other
metrics.
We explain this by the fact that BLEU-4 metric measures a score of a sequence
of four words (including articles and prepositions) and sometimes it is very com-
plicated to achieve an output with such a sequence by systems of machine
translation.
5
Discussion and Future Work
As has been shown in this paper it is possible with the use of Maximum Entropy to
divide the pages of the web portal to auxiliary and content pages. This can be used
in the method Reference Length of session identiﬁcation of data preprocessing of
log ﬁles. The ratio of auxiliary pages for the Reference Length method can be then
calculated with similar accuracy as if calculated from the sitemap. Since it is
possible to work also with historical data, there could not be available an appro-
priate sitemap of the web portal of that time. Therefore the possibility to estimate
the ratio of the auxiliary pages without the sitemap of the web portal is beneﬁcial.
An important role plays also a thoroughly made log ﬁle cleaning from unnecessary
data because poorly cleaned log may generate an inaccurate ratio of auxiliary pages.
The experiment was realized only on log ﬁle of a course of VLE portal, the future
work could be focused also on web portals with anonymous accesses because of a
bigger variance of the structure of such portals. Also these kind of portals (for
example e-shops) contain bigger log ﬁles that could prove more difﬁcult to analyze
for the algorithm. Future work could be also focused on determining the size of the
page content. It would be assumed that the high entropy would suggest the high
proportion of the content of a particular page. That would inform the web portal
administrator of content-rich pages. On basis of such information, the page could be
divided into more pages with less content and thus improve browsing of the web
portal for users. The principle could be based on research [12] which calculated
page rank for each page of the web portal.
Unconventional Usage of Entropy in the Field …
285

The next aim of the paper was focused on verifying the reliability of the auto-
matic metrics for Machine Translation evaluation. Three different measures of
reliability were used—Cronbach’s alpha, Standardized alpha, and entropy—to
estimate reliability. Cronbach’s alpha and Standardized alpha were very similar,
i.e. individual automatic metrics for Machine Translation evaluation have the same
variability. The use of entropy provided alternative means to validate the reliability
of metrics and the results relate to results of Cronbach’s alpha and Standardized
alpha.
Acknowledgements This work was supported by the Slovak Research and Development Agency
under the contract No. APVV-14-0336 and Scientiﬁc Grant Agency of the Ministry of Education
of the Slovak Republic (ME SR) and of Slovak Academy of Sciences (SAS) under the contracts
No. VEGA-1/0559/14 and by the project No. UGA VII/3/2015 Modelling the behavior of web
users depending on time.
References
1. Shannon, C.E.: A mathematical theory of communication. ACM SIGMOBILE Mob. Comput.
Commun. Rev. 5, 3 (2001)
2. Clausius, R.: On the Motive Power of Heat, and on the Laws which Can Be Deduced from It
for the Theory of Heat. Dover (1960)
3. Holzinger, A., Hörtenhuber, M., Mayer, C., Bachler, M., Wassertheurer, S., Pinho, A.J.,
Koslicki, D.: On entropy-based data mining. In: Holzinger, A., Jurisica, I. (eds.) Interactive
Knowledge Discovery and Data Mining in Biomedical Informatics: State-of-the-Art and
Future Challenges, pp. 209–226. Springer, Berlin (2014)
4. Lima, C.F.L., de Assis, F.M., de Souza, C.P.: A Comparative Study of Use of Shannon, Rényi
and Tsallis Entropy for Attribute Selecting in Network Intrusion Detection (2012)
5. Jaynes, E.T.: Information theory and statistical mechanics. Phys. Rev. 106, 620 (1957)
6. Munk, M., Benko, Ľ., Gangur, M., Turčáni, M.: Inﬂuence of ratio of auxiliary pages on the
pre-processing phase of Web Usage Mining. E+M Ekon. a Manag. 3, 144–159 (2015)
7. Jin, X., Zhou, Y., Mobasher, B.: A maximum entropy web recommendation system. In:
Proceeding of the eleventh ACM SIGKDD International Conference on Knowledge
Discovery in Data Mining—KDD’05, p. 612. ACM Press, New York (2005)
8. Wang, H., Wang, L., Yi, L.: Maximum entropy framework used in text classiﬁcation. In: 2010
IEEE International Conference on Intelligent Computing and Intelligent Systems, pp. 828–
833. IEEE (2010)
9. Benko, Ľ., Reichel, J., Munk, M.: Analysis of student behavior in virtual learning
environment depending on student assessments. In: ICETA 2015: 13th International
Conference on Emerging eLearning Technologies and Applications, Stary Smokovec,
November 26–27, 2015. pp. 33–38. IEEE, Stary Smokovec, Danvers (2015)
10. Eetemadi, S., Lewis, W., Toutanova, K., Radha, H.: Survey of data-selection methods in
statistical machine translation. Mach. Transl. 29, 189–223 (2015)
11. Tomeh, N., Allauzen, A., Yvon, F.: Maximum-entropy word alignment and posterior-based
phrase extraction for machine translation. Mach. Trans. 28, 19–56 (2014)
12. Kapusta, J., Munk, M., Drlík, M.: Analysis of differences between expected and observed
probability of accesses to web pages. In: Hwang, D., Jung, J., and Nguyen, N.-T. (eds.)
Computational Collective Intelligence. Technologies and Applications SE-68, pp. 673–683.
Springer International Publishing (2014)
286
M. Munk and Ľ. Benko

Advantages of Intelligent Multimedia
Application
Eva Milkova and Abdel-Badeeh M. Salem
Abstract The aim of subjects dealing with graph theory and combinatorial opti-
mization is above all to develop and deepen students’ capacity for logical and
algorithmic thinking. It should also support the ability to form images in mind.
Students should be able to describe various situations with the aid of graphs, solve
the given problem expressed by the graph, and translate the solution back into the
initial situation. As a lot of our students are visual learners it is useful to complete
teaching and learning using various multimedia applications. One of the programs
enabling visual representation of basic graph-concepts and graph-algorithms is
introduced in the paper, as well as relevant educational principles, which have been
applied in the course of many years. The paper can serve to the teachers and
instructors as an inspirational material when dealing with graph theory and com-
binatorial optimization.
1
Introduction
Information and communication technology has changed many things in the world
and has substantially inﬂuenced also education. Demonstration and visualization
using suitable multimedia applications make the subject much clearer and com-
prehensible. Students need images and visualization in addition to words. Science
learning is about creating images in mind, and teaching should support such image
formation [1].
E. Milkova (✉)
University of Hradec Králové, Rokitanskeho 62,
50002 Hradec Kralove, Czech Republic
e-mail: eva.milkova@uhk.cz
A.-B.M. Salem
Faculty of Computer and Information Sciences,
Ain Shams University, Abbassia, Cairo, Egypt
e-mail: abmsalem@yahoo.com
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_35
287

Intelligent learning system has been focusing on acquiring and building the
student’s knowledge, experience, to minimize their weaknesses and boost their
strengths. It allows ﬂexibility in teaching methods, achieving many of the same
beneﬁts as one-on-one instruction (cf. [2]).
Graph theory and combinatorial optimization, the areas located at the intersec-
tion of applied mathematics and operations research, belong to fundamental parts of
computer science. For the reasons that students could get a deeper insight into a
problem and were able to solve it using a suitable algorithm, it is necessary to
ensure especially in the instruction of subjects dealing with these areas that the
students are ﬁrst informed about the necessary graph-concepts and they thoroughly
understand their properties (corresponding theorems and their proofs). There are
various pedagogical ways to achieve this.
Our approach is based on the following basic teaching principles that we have
been applying in our teaching for many years.
When starting an explanation of new subject matter, we introduce a particular
problem with a real life example, logical task or puzzle as a prototype of the
explained concept or algorithm and properly discuss suitable graph-representation
of a problem. Student engagement is crucial for successful education. Practical
tasks attract students to know more about the explained subject matter and to apply
gained knowledge (cf. [3]). If an interesting enjoyable task is assessed to each
topic, students recall the explained subject matter much better and their engage-
ment progresses when looking for similar examples.
We examine each problem from more than one point of view and discuss various
approaches to the given problem solution with respect to the already explained
subject matter. Using the constructed knowledge and suitable modiﬁcation of the
problem solution we proceed to new subject matter. In this way students get deeper
insight into each problem and entirely understand it.
In addition to words we visualize the particular issue as well as it is possible. For the
subjects dealing with the graph theory and combinatorial optimization was created
GrAlg program that is closer presented in Sect. 2. Visual attributes of a problem or
situation are the primary, and most inﬂuential, connection to meaning and under-
standing. Visualization used before symbolic development increases the likelihood
that students will remember the mathematical concept being taught. Mathematics
reasoning both takes from and gives the other parts of the mind. Thanks to graphs, we
primates grasp mathematics with our eyes and our mind’s eye [4].
We thoroughly practice the explained topic on various examples, discuss stu-
dents’ own examples describing the topic and encourage them to solve similar
examples. Practical and enjoyable examples serve very well as good tool for
ﬁnding out if students are able to describe a given task with the aid of graphs, i.e.
ﬁnd a graph-representation of the task, solve it and translate the solution back into
the initial situation. When solving tasks using graph theory knowledge, it isn’t
always easy to ﬁnd immediately the needed graph-representation.
The aim of the paper is to introduce the multimedia program GrAlg enabling
visual representation of basic graph-concepts and graph-algorithms with respect to
above mentioned principles.
288
E. Milkova and A.-B.M. Salem

2
Multimedia Support—GrAlg Program
In the introductory course dealing with graph theory and graph algorithms we
explain and discuss only the algorithms on simple undirected graphs (cf. [5–8]). In
Fig. 1 the graph whose edges illustrate the interactions between these algorithms is
presented. Numbers of vertices indicate the order in which the algorithms are
gradually discussed in the lectures.
Technology and social media have not only become a part of school life, but
have also begun to have a signiﬁcant effect on the forms and methods of teaching
and learning [10]. Along with large software products dealing with a wide spec-
trum of objects developed by a team of professionals there are also various smaller
programs dealing with objects appropriate to course subject matter created by a
student on a script given by the teacher with regard to student’s needs. In this way
the students receive study material closely corresponding with their reasoning.
Student-author forms the content of the application, according to teacher’s docu-
ments and guidelines, but on the other hand he/she formally captures the visual
sensibilities of their fellow-students (cf. [11]).
One of the most important applications among programs created within students’
theses for the subjects dealing with the graph theory and combinatorial optimization
is the GrAlg program [12].
Fig. 1 Mutual relation among given graph algorithms. 1. MST = Minimum spanning tree
problem, 2. Tarry = Tarry’s approach to maze problem, 3. Trémaux = Trémaux’s approach to
maze problem, 4. E-J = Edmonds-Johnson’s approach to maze problem, 5. Eulerian Trail = al-
gorithm determining an Eulerian trail, 6. Trails = algorithm determining a minimum trail cover, 7.
Train = algorithm solving the Chinese Postman Problem, 8. BFS = Breadth-First-Search, 9.
DFS = Depth-First-Search,
10.
Components = algorithm
determining
components,
Cut
Edge = algorithm determining if an edge is/isn’t cut edge, 11. Shortest Path = algorithm
determining the length (according number of edges) of the shortest path between two vertices,
Circles = algorithms determining circles with given properties (in more detail—see [9]), 12. Cut
Vertex = algorithm determining if a vertex is/isn’t cut vertex, Blocks = algorithm determining
blocks, 13. Dijkstra = Dijkstra’s algorithm
Advantages of Intelligent Multimedia Application
289

2.1
GrAlg Program—Brief Description
The program is created in the Delphi environment and its main purpose is the easy
creation and modiﬁcation of graphs and the possibility to emphasize with colours
basic graph-concepts and graph algorithms on graphs created within the program.
There are the following main possibilities of the program:
The program enables the creation of a new graph represented by ﬁgure, editing
it, saving graph in the program, in its matrix representation and also saving graph in
bmp format.
It also makes it possible to display some graph properties of the given graph.
The program enables to add colour to vertices and edges, to add text next
vertices, and to change positions of vertices and edges by “drop and draw a vertex
(an edge respectively)”.
The program allows the user to open more than one window so that two (or
more) objects or algorithms can be compared at once. Window size can be adjusted
as needed (see Fig. 2).
In the GrAlg program there is the option to run programs visualizing all of the
subjects explained algorithms in a way from which the whole process and used data
structures can clearly be seen (see Fig. 2).
The program is available on http://lide.uhk.cz/prf/ucitel/milkoev1/en_index.htm,
the page GRAFALG/Lectures.
Fig. 2 Two open windows of the GrAlg program with BFS algorithm visualization (left window)
and found appropriate BFS tree (in the window)
290
E. Milkova and A.-B.M. Salem

2.2
GrAlg Program—Advantages from Teacher Point
of View
The GrAlg program enables the teacher to complete his/her explanation within
lectures in such a way that the topic is more comprehensible; the possibility to use
colours allows the teacher to emphasize needed objects and relations, the option to
open more than one window enables him/her to explain the problem from more
points of view and show mutual relations among used concepts and algorithms. The
possibility to save each created graph in bmt format allows him/her easy insertion
of needed graphs into the study material (all graphs in this paper were prepared with
the program) and thus saves his/her time when preparing text material and
presentations.
2.3
GrAlg Program—Advantages from Student Point
of View
Using the GrAlg program students can revise subject-matter within the area of
graph theory and more deeply understand it. They can use not only graphs prepared
by the teacher but also graphs created by themselves, explore the properties of these
graphs and monitor the behaviour of algorithms. The possibility to open more than
one window enables them to follow mutual relations among used concepts and
algorithms. The option “Save Graph in bmp format” enables them easy creation
needed graphs for their tasks (texts and/or presentations) where they describe
various practical situations with the aid of graphs and solve the given problem.
3
Conclusion and Future Work
Mathematics is one of the oldest sciences however the area known as Combinatorial
Optimization close connected with Graph Theory and Computer Science is quite
young. Modern technology provides access to education and learning in the areas
(and not only in them) in a modern way attractive for students.
We agree with the authors of [13], who underpin that the core of enhancing
mathematical thinking is conscious questioning (discussion) where the numbers do
not have to be used. Students need to apprehend vocabulary, deﬁnitions and
implement/shift aspects of a problem situation into understanding, solving and then
connecting new knowledge into their mental concepts. Deeper questioning is the
means for students to make sense of mathematics and digital tools should be
effectively used to promote student questioning.
Advantages of Intelligent Multimedia Application
291

Intended learning may fail to be achieved for many reasons and if the purpose of the
mathematical tasks is conﬁned or limited to ‘solving’, rather than ‘learning from solving’, it
is likely that learners may neither learn nor enjoy engaging with the problem [13].
In the paper we have presented our teaching principles together with intelligent
multimedia application GrAlg, a useful complement of the subjects dealing with the
graph theory and combinatorial optimization. This program which visualises the
discussed concepts and graph algorithms, and which enables their comparison, gives
students the opportunity to get deeper insight into the subject matter and to enhance
their facility to solve everyday life practical situations. Students gain many useful
ideas and inspiration for their own solutions to tasks within various research areas.
At present we are dealing with a research concerning visualization of theorems
and their proofs explained within the above mentioned courses.
Acknowledgements This research has been supported by speciﬁc research project of the Faculty
of Science, University of Hradec Kralove in 2016.
References
1. Williams, R.: e-Learning strategy: what’s in the blend? In: Proceedings of the 4th European
Conference on e-Learning, Amsterdam, Nederland, ACL, UK, pp. 245–251 (2005)
2. Salem, A.-B.M.: Intelligent methodologies and technologies for e-Learning. In: IEEE 10th
International Conference on Emerging eLearning Technologies and Applications, pp. 331–
337 (2012)
3. Prazak, P.: Teaching simulation for management students in LMS. In: Advances in
Web-Based Learning—ICWL 2015, vol. 9412 of the series Lecture Notes in Computer
Science, pp. 121–130 (2015)
4. Pinker, S.: How the Mind Works, Norton (1997)
5. Biggs, N.L., Lloyd, K.E., Wilson, R.J.: Graph Theory 1736–1936. Clarendon Press, Oxford
(1976)
6. Graham, R.L., Hell, P.: On the history of the minimum spanning tree problem. Ann. Hist.
Comput. 7(1), 43–57 (1985)
7. Tarjan, R.E.: Data structures and network algorithms, Chap. 6. In: CBMS Regional
Conference, SIAM, Philadelphia (1983)
8. Dijkstra, E.W.: A note on two problems in connection with graphs. Numer. Math. 1, 269–271
(1959)
9. Milková, E.: Breadth-ﬁrst-search tree—levels and subtrees. Int. J. Math. Comput. Simul. 9,
206–212 (2015)
10. Kurvits, M., Laanpere, M., Väljataga, T.: Analysis of tools and methods for describing and
sharing reusable pedagogical scenarios. In: Advances in Web-Based Learning—ICWL 2015,
vol. 9412 of the series Lecture Notes in Computer Science, pp. 251–257 (2015)
11. Valjataga, T., Fiedler, S., Laanpere, M.: Re-thinking digital textbooks: students as co-authors.
In: Advances in Web-Based Learning—ICWL 2015, vol. 9412 of the series Lecture Notes in
Computer Science, pp. 143–151 (2015)
12. Šitina, J.: Grafové algoritmy a jejich vizualizace. Thesis, University of Hradec Králové, Czech
Republic (June 2010)
13. Khan, K., Mason, J.: Non numerical aspects of school mathematics. In: Proceedings of the
23rd International Conference on Computers in Education (ICCE 2015), Hangzhou, China,
pp. 477–482 (2015)
292
E. Milkova and A.-B.M. Salem

FFLD-Based Modeling of Fractional-Order
State Space LTI MIMO Systems
Krzysztof J. Latawiec, Rafał Stanisławski, Marian Łukaniszyn,
Marek Rydel and Bogusław R. Szkuta
Abstract This paper introduces a multivariable version of the Grünwald-Letnikov
fractional-order diﬀerence (FD) and approximates it with a powerful combination
of ﬁnite fractional diﬀerence (FFD) and ﬁnite Laguerre-based diﬀerence (FLD) to
yield ﬁnite fractional/Laguerre-based diﬀerence (FFLD). The multivariable FFLD
is eﬀectively used to model fractional-order state-space LTI MIMO systems.
Keywords
Grünwald-Letnikov fractional diﬀerence ⋅Laguerre-based diﬀerence ⋅
Multivariable fractional diﬀerence ⋅Fractional order systems ⋅State space systems
Nomenclature
CFLD
Combined fractional/Laguerre-based diﬀerence
FD
Grünwald-Letnikov fractional-order diﬀerence
FFD
Finite fractional diﬀerence
FFLD
Finite fractional/Laguerre-based diﬀerence
FLD
Finite Laguerre-based diﬀerence
GL
Grünwald-Letnikov
LD
Laguerre-based diﬀerence
OBF
Orthonormal basis functions
K.J. Latawiec (✉) ⋅R. Stanisławski ⋅M. Łukaniszyn ⋅M. Rydel ⋅B.R. Szkuta
Department of Electrical, Control and Computer Engineering, Opole University
of Technology, ul. Prószkowska 76, 45-758 Opole, Poland
e-mail: k.latawiec@po.opole.pl
R. Stanisławski
e-mail: r.stanislawski@po.opole.pl
M. Łukaniszyn
e-mail: m.lukaniszyn@po.opole.pl
M. Rydel
e-mail: m.rydel@po.opole.pl
B.R. Szkuta
e-mail: b.szkuta@po.opole.pl
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_36
293

294
K.J. Latawiec et al.
1
Introduction
Various approximations to the Grünwald-Letnikov fractional diﬀerence (FD) have
been pursued in order to prevent its computational explosion problem [1–11]. Our
main reservation against a plethora of approximation approaches results form the
fact that the proposed approximating IIR/FIR/OBF ﬁlters are quite arbitrary in that
they do not use any a priori knowledge about the mathematical (not to say physical)
structure of the Grünwald-Letnikov FD. Therefore, an alternative approach relying
on the approximation of the FD ﬁlter with its truncated, ﬁnite-length versions is
advocated here [1, 2]. In analogy to ﬁnite impulse response (FIR) the term ﬁnite
FD, or FFD, has been coined [12, 13]. New, eﬀective combinations of FFD with
Laguerre ﬁlters have led to the introduction of Finite Laguerre-based diﬀerence (or
FLD) [14, 15] and Finite Fractional/Laguerre-based diﬀerence (or FFLD) [14, 15].
In the FFLD, the FFD contribution covers the high-frequency properties of FD, while
its medium/low-frequency range is modeled by the FLD share. The FFLD provides
excellent performance, both in terms of high approximation accuracy and low com-
putational load [15]. Up to date, FFLD has been employed in the single-input/single-
output (SISO) environment [16, 17]. Here we extend the applicability of the FFLD
to modeling of multivariable state-space systems.
This paper is organized as follows. Having introduced the approximation problem
for the Grünwald-Letnikov diﬀerence in Sect. 1, the FD modeling task by means of
FFD is recalled and extended to the multivariable case in Sect. 2. In a similar way,
Laguerre-based fractional diﬀerences LD and CFLD together with their approxima-
tions FLD and FFLD, are resumed in Sect. 3. A uniﬁed multivariable framework
for FD/LD/CFLD and their approximators FFD/FLD/FFLD is presented in Sect. 4.
An FFLD-based application to fractional-order state space modeling is given in two
implementation schemes in Sect. 5. Conclusions of Sect. 6 complete the paper.
2
Fractional Diﬀerence and Finite Fractional Diﬀerence
It is well known that the Grünwald-Letnikov fractional order diﬀerence (FD)
𝛥𝛼
FDx(t) = x(t) +
t∑
j=1
Pj(𝛼)x(t −j)
t = 0, 1, …
(1)
can be described as [12]
𝛥𝛼
FDx(t) = x(t) +
∞
∑
j=1
Pj(𝛼)x(t −j)
= x(t) + XFD(t)
t = 0, 1, …
(2)

FFLD-Based Modeling of Fractional-Order State ...
295
with x(l) = 0∀l < 0, Pj(𝛼) = (−1)j(𝛼
j
) and 𝛼∈(0, 2) is the fractional order. Here we
admit the signal x(t) to be an nx-vector, thus introducing the multivariable FD.
The simplest time-domain approximation to FD, avoiding its possible computa-
tional explosion, is the ﬁnite fractional diﬀerence (FFD) deﬁned as [12]
𝛥𝛼
FFDx(t) = x(t) +
J∑
j=1
Pj(𝛼)x(t)q−j
(3)
where J = min(t, J) and J is the upper bound for j when t > J. It is well known that
the FFD suﬀers from the steady-state error problem.
2.1
Multivariable FFD
In the multivariable FD case, the signal x(t) is assumed to be an nx-vector. For
the multivariable FFD case, we discriminate between various bounds J = Jr, r =
1, … , nx, with Jr = min(t, Jr), thus accounting for various dynamics in the xi-to-
(𝛥𝛼x)i channels, i = 1, … , nx. This way we introduce the multivariable version of
FFD.
3
Laguerre-Based Fractional Diﬀerences and Their
Approximators
Firstly, the Laguerre-based (fractional) diﬀerence (LD) is recalled [14], which, like
the FD, is valid for both scalar and multivariable cases
𝛥𝛼
LDx(t) = x(t) +
∞
∑
j=1
cjLj(q−1)x(t)
= x(t) + XLD(t)
t = 0, 1, …
(4)
with x(l) = 0∀l < 0, Lj(q−1) and cj, j = 1, 2, …, are the Laguerre ﬁlters and weight-
ing parameters, respectively. Interestingly, in the scalar case, values of cj have been
derived that provided the equivalence of LD and FD in the sense that XLD ≡XFD
[14].
Again, to cope with the computational explosion problem, a ﬁnite approximation
to LD, called FLD, has been deﬁned [14] for the scalar case

296
K.J. Latawiec et al.
𝛥𝛼
FLDx(t) = x(t) +
M
∑
j=1
cjLj(q−1)x(t)
= x(t) + xFLD(t)
t = 0, 1, …
(5)
where M is the number of the Laguerre ﬁlters used do calculate the diﬀerence FLD
and cj, j = 1, 2, … , M, are computed as for LD in Eq. (4). For the multivariable case,
the signal x(t) is an nx-vector and Ms, s = 1, … , nx, is substituted for M.
Also, another Laguerre-based (fractional) diﬀerence, being a combination of the
FFD and LD, called CFLD, has been deﬁned for the scalar case as [14]
𝛥𝛼
CFLDx(t) = x(t) + XCFLD(t)
t = 0, 1, …
(6)
where
XCFLD(t) =
J∑
i=1
Pi(𝛼)x(t)q−i +
∞
∑
j=1
cjLj(q−1)q−Jx(t)
(7)
and the ﬁrst component at the right-hand side of Eq. (7) constituting the FFD con-
tribution into the CFLD, while the second one being the (J-delayed) LD share, with
Pj(𝛼), j = 1, … , J, as in Eqs. (2) and (3), and Lj(q−1) and cj, j = 1, 2, …, as in Eqs.
(4) and (5), respectively.
Again, conditions for equivalence of CFLD and FD have been established in the
scalar case [14].
Finally, a ﬁnite approximation to CFLD/FD, called FFLD, has been presented for
the scalar case [14, 15]
𝛥𝛼
FFLDx(t) = x(t) +
J∑
i=1
Pi(𝛼)x(t)q−i +
M
∑
j=1
cjLj(q−1)q−Jx(t)
= x(t) + XFFLD(t)
t = 0, 1, …
(8)
where J is as before and M is a number of Laguerre ﬁlters used in the model.
In the scalar case, the Laguerre-based approximators to FD, in particular FFLD,
have been shown to provide excellent modeling performance [14], outperforming
the recognized FD approximators [15].
In the multivariable FFLD case, the signal x(t) is an nx-vector and Jr, r = 1, … , nx
and Ms, s = 1, … , nx, are substituted for J and M, respectively, just like in case of
the multivariable FFD and FLD.

FFLD-Based Modeling of Fractional-Order State ...
297
4
FFLD-Based Approximators to Multivariable FD—A
Uniﬁed Framework
Now, a uniﬁed framework is oﬀered for both multivariable FD/LD/CFLD and their
FFLD-based approximators FFD/FLD/FFLD
𝛥𝛼
FFLDx(t) = x(t) + XFFLD(t)
t = 0, 1, …
(9)
where
XFFLD(t) =
Jr
∑
j=1
Pj(𝛼)x(t)q−j +
Ms
∑
j=1
cjLj(q−1)x(t −Jr)
(10)
with Jr and Ms deﬁned as before, which is speciﬁed to
(i) FD for Jr →∞, r = 1, … , nx and Ms = 0, s = 1, … , nx,
(ii) FFD for Jr < ∞and Ms = 0,
(iii) LD for Jr = 0, Ms →∞and cj calculated as in Theorem 1 of [14],
(iv) FLD for Jr = 0, Ms < ∞and cj calculated as in Theorem 1 of [14],
(v) CFLD for 0 < Jr < ∞, Ms →∞and cj calculated as in Theorem 2 of [14],
(vi) FFLD for 0 < JrMs < ∞, r = 1, … , nx, s = 1, … , nx, and cj calculated as in
Theorem 2 of [14].
It should be emphasized that Eqs. (9) and (10) constitute a generalization of the
FD/FFD/LD/FLD/CFLD/FFLD models, thus presenting a uniﬁed framework for the
three fractional diﬀerences and their three approximators considered. Now, depend-
ing on a speciﬁc application, or a speciﬁc context, we can pick up one (or more) of
the speciﬁc items (i) to (vi) in order to analyze/synthesize a speciﬁc model.
5
Application to Fractional State Space System
Consider a discrete-time state space LTI MIMO system governed by the
(commensurate-order) fractional equations
𝛥𝛼x(t + 1) = Af x(t) + Bu(t),
x0
(11)
y(t) = Cx(t) + Du(t)
(12)
where x(t) ∈ℜnx, u(t) ∈ℜnu and y(t) ∈ℜny are the state, input and output vectors,
respectively, Af ∈ℜnx×nx, B ∈ℜnx×nu, C ∈ℜny×nx and D ∈ℜny×nu. Without loss of
generality we will assume in the sequel that the initial vector x0 is zero, especially that
we will operate on ﬁnite-length FD approximators that do not trace back to x0. Note
that Af = A −I, with A ∈ℜnx×nx representing a discrete-time state space system in
a ‘regular’ form (with 𝛼= 1) and I ∈ℜnx×nx is the identity matrix.

298
K.J. Latawiec et al.
Accounting that the FD is described by formula (2), Eq. (11) can be modeled in
the following form [2, 18, 19]
x(t + 1) = (Af + 𝛼I)x(t) −
t+1
∑
j=2
Pj(𝛼)x(t −j + 1) + Bu(t)
(13)
Using the approximations to FD presented in Sect. 4, the corresponding state space
models can be speciﬁed.
The stability analysis for discrete-time state space systems has been given in Refs.
[18, 19].
5.1
Uniﬁed Framework for State Space Models
Using the uniﬁed framework for all the approximations to multivariable FD pre-
sented in Sect. 4, the FD/FFD/LD/FLD/CFLD/FFLD-based discrete-time state equa-
tion can be uniﬁed as
x(t + 1) = Af x(t) + Bu(t) −XFFLD(t + 1)
t = 0, 1, …
= Af x(t) + Bu(t) −
Jr
∑
j=1
Pj(𝛼)x(t −j + 1) −
Ms
∑
j=1
cjLj(q−1)x(t −Jr + 1)
(14)
with the speciﬁcations (i) to (vi) of Sect. 4 still valid here.
Note that, in fact, the FFLD description (14) is the most general one for all the
FD/FFD/LD/FLD/CFLD/FFLD-based LTI state space systems, as can be seen from
the uniﬁed framework of Sect. 4.
Remark 1 Possible accounting for the sampling period T (when transferring from a
continuous-time derivative to the discrete-time diﬀerence) results in the substitutions
Af →Af T𝛼and B →BT𝛼in Eqs. (13) and (14) [2, 18, 19].
5.2
Implementation of State Space Models
Implementation of the fractional order state space system reduces to determining the
function XFFLD(t) in the state Eq. (14).
Implementation scheme 1 (general). Since the summation bounds Jr and Ms, r, s =
1, … , nx, are varying with entries of the vector x(t), the particular entries xi(t), i =
1, … , nx, have to be calculated separately, which is rather burdensome.

FFLD-Based Modeling of Fractional-Order State ...
299
Fig. 1
Block diagram of
regular/fractional-order state
space system
D
B
C
+
+
+
+
y(t)
u(t)
x(t)
Af
q−1
−
F(q−1)
Implementation scheme 2 (simpliﬁed). We can simplify the calculations assuming
that J1 = J2 = ⋯= Jnx = J and M1 = M2 = ⋯= Mnx = M. This enables to calcu-
late the whole vector x(t) as in the quite familiar block diagram of Fig. 1. Accounting
for the response of the “fractionalizing” ﬁlter F(q−1) to the signal x(t), the block dia-
gram of the fractional-order state space models of Fig. 1 is similar to the regular one.
For fractional-order systems the ﬁlter F(q−1) represents the particular approximation
to FD/LD/CFLD and can be speciﬁed as follows:
(a) F(q−1) = ∑J
j=1 Pj(𝛼)q−j+1 for FFD-based state space system,
(b) F(q−1) = ∑M
j=1 cjLj(q−1)q with cj calculated as in Theorem 1 of [14] for FLD-
based system,
(c) F(q−1)= ∑J
j=1 Pj(𝛼)q−j+1+ ∑M
j=1 cjLj(q-1)q−J+1 with cj calculated as in Theorem 2
of [14] for FFLD-based system.
Remark 2 Note that for the ideal case of FD/LD/CFLD we have F(q−1) = q[(1 −
q−1)𝛼−1] (compare [18, 19]). Also note that for the regular state space system (𝛼=
1), we have F(q−1) = −1.
6
Simulation Example
Example 1 Out of a plethora of simulation runs we select a simple two-input/two-
output discrete-time fractional-order (commensurate) state space system Af , B, C as
in Eqs. (11) and (12) with D = 0, nx = 2, 𝛼= 0.5 and
Af =
[
−0.1 0.1
0.15 −0.6
]
, B =
[
0 0.85
0.3 0.1
]
, C = I2
thus providing diﬀerent dynamics in the dynamical channels. The multivariable FD
is approximated with the (multivariable) FFLD as in Eq. (10), speciﬁcation (vi), and
implemented using the general scheme 1. In order to limit the computational load
of the modeling algorithm we introduce the constraint on the maximum number of

300
K.J. Latawiec et al.
Table 1
Modeling accuracy for fractional state space system; Ex. 1
J1, M1
15, 15
15, 20
15, 20
20, 20
25, 25
25, 20
25, 25
J2, M2
15, 15
15, 15
15, 20
15, 15
15, 15
20, 25
15, 25
N × MSPE (𝜖1) 8.137
0.64
0.79
0.1025
0.02
0.039
0.0045
N × MSPE (𝜖2) 0.64
0.031
0.061
0.027
0.042
0.0017
0.0012
parameters Ji + Mi ≤90, i = 1, … , nx = 2. (Note that the constraint is only loosely
related with the computational complexity of the algorithm due to the presence of the
Laguerre ﬁlters in the FLD part of the FFLD.) Table 1 presents selected results of
computation of modeling accuracy in terms of MSPEs for 𝜖i(t) = xi(t) −̂xi(t), i =
1, 2, where xi(t) and ̂xi(t) are the system state and its FFLD-based estimates at
time t = 1, … , N = 3000, respectively. We do not provide plots of exemplary xis
and ̂xis, i = 1, 2 as these are hardly distinguishable, especially for higher Ji and Mi,
i = 1, 2. For the same ‘distinguishability’ reason, in Table 1 we show the values of
N × MSPE(𝜖i) instead of MSPE(𝜖i), i = 1, 2.
The heuristically driven results in Table 1 are self-explanatory. Clearly, higher Ji
and Mi, i = 1, 2 produce lower modeling errors, with diversiﬁed values of Ji and Mi,
i = 1, 2 in various dynamical channels also justiﬁed. We have additionally conﬁrmed
those results arranging for the optimization task
[ ̂J1, ̂M1, ̂J2, ̂M2] = arg
min
J1,M1,J2,M2
N
∑
t=1
[𝜖2
1(t) + 𝜖2
2(t)]
subject to the aforementioned constraint Ji + Mi ≤90, i = 1, 2. The solution yields
the optimal least-squares estimates
̂J1 = 24, ̂M1 = 27, ̂J2 = 14, ̂M2 = 25
with the sum of error squares equal to 0.00179. The outcome supports the results of
Table 1.
Remark 3 It is interesting to compare the above modeling results for the FFLD-
based state space system of Example 1 with those for ‘classical’ FFD-based ones
(M1 = M2 = 0). Not surprisingly, the comparable modeling accuracy in the latter
case can be obtained for as high J1 = J2 = J as 2865 that is at the (very) high com-
putational cost. The FLD-based modeling is more eﬀective than that for FFD but
still remarkably inferior to FFLD (compare [15]).
Remark 4 It is worth mentioning that the introduced methodology for approxima-
tion of multivariable GL fractional-order diﬀerences would most likely be more
eﬀective in modelling of noncommensurate-order fractional state space LTI MIMO

FFLD-Based Modeling of Fractional-Order State ...
301
systems. In fact, various fractional orders in the channels xi-to-(𝛥𝛼x)i, i = 1, … , nx,
can incur (much) more diversiﬁed dynamics in the particular channels than for
commensurate-order systems. This will be a subject of our future research.
7
Conclusion
This paper has presented a uniﬁed framework for various time-domain approxima-
tions to a multivariable version of the Grünwald-Letnikov fractional-order diﬀer-
ence (FD) and its Laguerre-based equivalents (LD, CFLD), namely FFD, FLD and
FFLD. The approximators have been employed to model fractional-order state space
LTI MIMO systems, with the multivariable FFLD model clearly outperforming the
two other ones. A simulation example conﬁrms the quality of the new FFLD-based
methodology for modeling of fractional-order commensurate state space LTI MIMO
systems. Extension to noncommensurate systems is a subject of our future research
work.
References
1. Podlubny, I.: Fractional Diﬀerential Equations. Academic Press, Orlando, FL (1999)
2. Monje, C., Chen, Y., Vinagre, B., Xue, D., Feliu, V.: Fractional-Order Systems and Controls:
Fundamentals and Applications. Series on Advances in Industrial Control. Springer, London,
UK (2010)
3. Chen, Y., Vinagre, B., Podlubny, I.: A new discretization method for fractional order diﬀer-
entiators via continued fraction expansion. In: Proceedings of DETC’2003, ASME Design
Engineering Technical Conferences. vol. 340, pp. 349–362, Chicago, IL (2003)
4. Maione, G.: On the Laguerre rational approximation to fractional discrete derivative and inte-
gral operators. IEEE Trans. Autom. Control 58(6), 1579–1585 (2013)
5. Baeumer, B., Kovacs, M., Sankaranarayanan, H.: Higher order Grünwald approximations of
fractional derivatives and fractional powers of operators. Trans. Am. Math. Soc. 367(2), 813–
834 (2015)
6. Gao, Z.: Improved Oustaloup approximation of fractional-order operators using adaptive
chaotic particle swarm optimization. J. Sys. Eng. Electron. 23(1), 145–153 (2012)
7. Gao, Z., Liao, X.: Rational approximation for fractional-order system by particle swarm opti-
mization. Nonlinear Dyn. 67(2), 1387–1395 (2012)
8. Khanra, M.: Rational approximation of fractional operator—a comparative study. In: Interna-
tional Conference on Power, Control and Embedded Systems (ICPCES), Allahabad, India, pp.
1–5 (2010)
9. Lin, Y., Xu, C.: Finite diﬀerence/spectral approximations for the time-fractional diﬀusion equa-
tion. J. Comput. Phys. 225, 1533–1552 (2007)
10. Ditzian, Z.: Fractional derivatives and best approximation. Acta Math. Hung. 81(4), 323–348
(1998)
11. Tseng, C.C.: Design of variable and adaptive fractional order FIR diﬀerentiators. Signal
Process. 86(10), 2554–2566 (2006)
12. Stanisławski, R., Latawiec, K.J.: Normalized ﬁnite fractional diﬀerences—the computational
and accuracy breakthroughs. Int. J. Appl. Math. Comput. Sci. 22(4), 907–919 (2012)

302
K.J. Latawiec et al.
13. Stanisławski, R., Latawiec, K.J.: Modeling of open-loop stable linear systems using a combi-
nation of a ﬁnite fractional derivative and orthonormal basis functions. In: Proceedings of the
15th International Conference on Methods and Models in Automation and Robotics, Miedzyz-
droje, Poland, pp. 411–414 (2010)
14. Stanisławski, R.: New Laguerre ﬁlter approximators to the Grünwald-Letnikov fractional dif-
ference. Math. Probl. Eng. 2012, 1–21 (2012) Article ID: 732917
15. Stanisławski, R., Latawiec, K.J., Łukaniszyn, M.: A comparative analysis of Laguerre-based
approximators to the Grünwald-Letnikov fractional-order diﬀerence. Math. Probl. Eng. 2015,
1–10, Article ID: 512104 (2015)
16. Stanisławski, R., Latawiec, K.J., Łukaniszyn, M., Gałek, M.: Time-domain approximations to
the Grünwald-Letnikov diﬀerence with application to modeling of fractional-order state space
systems. In: 20th International Conference on Methods and Models in Automation and Robot-
ics (MMAR), Miedzyzdroje, Poland, pp. 579–584, Aug 2015
17. Stanisławski, R., Latawiec, K.J.: Fractional-order discrete-time Laguerre ﬁlters—a new tool
for modeling and stability analysis of fractional-order LTI SISO systems. Discret. Dyn. Nat.
Soc. 2016, 1–9, Article ID: 9590687 (2016)
18. Stanisławski, R., Latawiec, K.J.: Stability analysis for discrete-time fractional-order LTI state-
space systems. Part I: New necessary and suﬃcient conditions for asymptotic stability. Bull.
Pol. Acad. Sci. Tech. Sci. 61(2), 353–361 (2013)
19. Stanisławski, R., Latawiec, K.J.: Stability analysis for discrete-time fractional-order LTI state-
space systems. Part II: New stability criterion for FD-based systems. Bull. Pol. Acad. Sci. Tech.
Sci. 61(2), 362–370 (2013)

A Dispatching Policy for the Dynamic
and Stochastic Pickup and Delivery Problem
Gianpaolo Ghiani, Emanuele Manni and Alessandro Romano
Abstract Real-time vehicle routing problems arise in a number of applications
spanning from couriers to emergency services. In this article, we present a new
dispatching policy for the dynamic and stochastic pickup and delivery problem,
in which a ﬂeet of vehicles must service a set of dynamically occurring customers
requests that are partitioned in several classes (according to their priority). The basic
idea of our policy is to reserve a fraction of the ﬂeet capacity to the top prior classes
that deserve to be serviced as soon as possible. Moreover, our dispatching policy
is parameterized and the optimal parameter setting is determined by solving an oﬀ-
line training problem on a sample of the instance population. To asses the quality
of our approach, we compare it with two policies already proposed in the literature,
namely a reactive and an anticipatory procedure. Computational results on randomly-
generated instances indicate that our procedure can often match the quality of an
anticipatory algorithm with a computational eﬀort comparable to that of a reactive
approach.
Keywords
Real-time vehicle routing⋅Dispatching policies⋅Pickup and delivery⋅
Optimization
G. Ghiani ⋅E. Manni (✉) ⋅A. Romano
Dipartimento di Ingegneria dell’Innovazione, Università del Salento,
via per Monteroni, 73100 Lecce, Italy
e-mail: emanuele.manni@unisalento.it
G. Ghiani
e-mail: gianpaolo.ghiani@unisalento.it
A. Romano
e-mail: alessandro.romano@unisalento.it
© Springer International Publishing AG 2018
K. Ntalianis and A. Croitoru (eds.), Applied Physics, System Science
and Computers, Lecture Notes in Electrical Engineering 428,
DOI 10.1007/978-3-319-53934-8_37
303

304
G. Ghiani et al.
1
Introduction
In this paper, we deal with the dynamic and stochastic Pickup and Delivery Problem
(PDP) in which a ﬂeet of vehicles must service a set of customers requests charac-
terized by a pickup and a delivery location, as well as by a class according to their
priority. The goal is to maximize the overall customer service level (which is equiva-
lent to minimize customer inconvenience). The problem is dynamic in that customers
requests are disclosed during the planning horizon, whereas it is stochastic because
we assume that the requests arrive according to known stochastic processes. This
type of problem occurs in several sectors (e.g., the couriers industry and emergency
systems). Nowadays, the recent advances in communication and information tech-
nologies allow to obtain in real-time—among others—data on vehicles locations and
customers requests. This continuous ﬂow of data forces the dispatcher to modify the
vehicle routes in real time. Moreover, at each stage it is important to make accurate
decisions, since a bad choice in the present might aﬀect the ability to make good deci-
sions in the future. A detailed survey can be found in [5]. For this class of problems
two kinds of policies are common in the literature: (i) reactive policies which man-
age new requests only once they have occurred, neglecting any available stochastic
information; (ii) anticipatory algorithms which exploit the stochastic characteriza-
tion of future demand, in an attempt of anticipating it, to provide the highest possible
quality of service. Reactive policies [2] are characterized by extremely fast running
times, which are obtained at the expenses of solution quality, mainly because of
the myopic choices made at each stage. On the other hand, anticipatory procedures
[1, 3, 4] typically achieve better results than reactive algorithms, but with longer
running times mainly due to the computational eﬀort of simulating future demand.
In this paper we aim to devise a dispatching policy that, trading oﬀbetween these
two extremes, is able to match the quality of anticipatory algorithms with a compu-
tational eﬀort comparable to that of reactive approaches. The work by Ghiani et al.
[3] is particularly relevant to our paper, because we consider a problem with similar
characteristics. Moreover, we use their anticipatory algorithm as a benchmark for
our approach, as will be described in Sect. 3.
2
A Dispatching Policy
In our formulation of the dynamic and stochastic PDP, we assume that the entire
territory is represented by a rectangular area with given dimensions h and w, so that
a generic point a (pickup, delivery, depots and vehicles’ positions) is identiﬁed by
means of its Cartesian coordinates (xa, ya). The time tab needed to go from an origin
a to a destination b is obtained by using the Euclidean distance between the two
points and considering a given ﬁxed speed. We assume there is a ﬂeet of m vehicles
at disposal, each of which is located at a depot at the beginning of the planning
horizon. In addition, each vehicle route must meet the following constraints: (i) each

A Dispatching Policy for the Dynamic and Stochastic ...
305
route starts and ends at a depot; (ii) the pickup and the delivery points of a request
must be visited by the same vehicle; (iii) a pickup point must precede its associated
delivery point; (iv) a vehicle cannot be diverted from its next destination to service a
new request. The customers (and therefore their service requests) are classiﬁed into a
number C of classes, according to their “importance”. Without any loss of generality,
we assume that class c = 1 represents the most important requests, whereas class
c = C represents the requests of customers whose service can be delayed without
incurring in high penalties. In general, the k-th request (k = 1, 2, …) is characterized
by (i+
k , i−
k , ck, Tk), where i+
k = (x+
k , y+
k ) are the coordinates of the pickup point, i−
k =
(x−
k , y−
k ) are the coordinates of the delivery point, ck ∈{1, … , C} is the class of the
request, Tk ≥0 is the occurrence time of the request. We assume that the requests are
independent and uniformly distributed over the service territory and arrive according
to a known stochastic process.
Our goal is to maximize the customer satisfaction by minimizing the sum of the
inconveniences of the requests, that are calculated diﬀerently according to the class
the request belongs to. More speciﬁcally, we denote by f(𝜏k, wk) the penalty function
measuring the inconvenience associated with the k-th request, deﬁned as:
f(𝜏k, wk) =
{
0
Tk ≤𝜏k < Dk
pck(𝜏k −Dk)
𝜏k ≥Dk.
Here, 𝜏k is the time instant when the delivery is performed and wk = (lck, pck). In
particular, lck ≥0 is the amount of time (after the instant Tk) after which the penalty
starts to be counted, determining a soft deadline Dk = Tk + lck. Finally, pck ≥0 is the
slope of the penalty function. Thus, the objective function is: min z = ∑
k f(𝜏k, wk).
The policy we propose in this paper aims to achieve performance that are com-
parable to those of a reactive approach in terms of reduced computing times and to
those of an anticipatory algorithm in terms of solution quality. For the former pur-
pose, we avoid using complex rules for assigning the requests to the vehicles and for
managing the multiple classes of requests. Rather, we choose to reserve a fraction
𝛼c of the ﬂeet of m vehicles to service the requests belonging to class c = 1, … , C.
Thus, 𝛼c denotes the fraction of the ﬂeet that can be used to service the requests
belonging to class c and to the more prioretized classes. In this way, when taking the
dispatching decision we employ a cheapest-insertion approach, but the number of
alternatives is limited by the fact that not all the vehicles can service all the requests.
As an additional dispatching rule, when evaluating the diﬀerent alternatives we allow
an insertion if the delivery instants of the requests of each class c (c = 1, … , C) that
are already allocated on a route are not delayed more than a given value 𝜖c ≥0. As
pointed out before, we also want to get advantage of the available stochastic knowl-
edge of the problem. As a consequence, the values of the vector 𝜶= (𝛼1, … , 𝛼C) are
determined by solving oﬀ-line a training problem on a sample that is representative
of the instances to be solved. Of course, the solution must be such that ∑C
i=1 𝛼i = 1.
The procedure starts with a given value for 𝜶. Then, we try to repeatedly modify
each component 𝛼c by adding or subtracting a value 𝛿and evaluating the (possible)

306
G. Ghiani et al.
objective function improvement. In case of an improvement, the value of 𝛼c is
updated and the procedure is iterated. The procedure terminates when no further
improvements are possible.
3
Experimental Results
To asses the eﬀectiveness of our dispatching policy, we compare it with two diﬀer-
ent approaches: (a) a purely reactive procedure that for each new request chooses the
vehicle with the cheapest insertion cost, neglecting all the available stochastic infor-
mation; (b) the anticipatory procedure proposed in [3], suitably modiﬁed to manage
diﬀerent classes of requests. All the heuristics are implemented in C++ and run
on a Linux machine clocked at 2.67 GHz and equipped with 27 GB of RAM. We
perform a number of computational experiments on randomly generated instances.
All the datasets are characterized by several experimental factors, some of which
have the same value across all the datasets, whereas the others have diﬀerent values.
In particular, in the former set of experimental factors we consider: C = 2, a plan-
ning horizon of 480 min, an average speed of the vehicles of 40 km/h, h = 20 km,
w = 20 km, p1 = 10 and p2 = 1. Then, the experimental factors having diﬀerent val-
ues depending on the speciﬁc dataset are:
∙expected number of requests per class Nc (c = 1, … , C): we have tested various
values for N1 and N2, such that the overall expected number of requests is 200 or
1500;
∙number m of vehicles at disposal: m = 20, 40, 60, 80, 100, 120, 140;
∙parameters lc and 𝜖c (c = 1, … , C): we have tested various combinations, gener-
ating four diﬀerent datasets.
We evaluate the performance of the diﬀerent dispatching policies by using two indi-
cators. First, we consider the average time Ts (in seconds) needed by the procedure to
allocate a single request. We observe that, to allow a particular heuristic to be useful
in the real world, the value of Ts must be lower then the inter-arrival time between two
consecutive requests. Second, we compute the average percentage objective func-
tion deviation of the anticipatory algorithm (ANT, in the following) and of our pol-
icy reserving a fraction of the vehicles for the most prioretized classes (RES, in
the following) with respect to the reactive procedure. These deviations are obtained
as 100 ∗(objH −objR)∕objR, where objR represents the objective function value of
the reactive procedure, whereas objH is the objective function value of the heuristic
we want to evaluate. Thus, a negative value indicates that the considered heuristic
achieves an improvement over the reactive procedure. Tables 1 and 2 contain the
results of our experiments, diﬀerentiated according to the average number of overall
daily requests per class. For all the datasets, we have ﬁrst determined the best values
for 𝜶by employing the procedure illustrated in the previous section with 𝛿= 0.1.
Such values are reported in the tables under the column 𝜶∗. The results contained
in Table 1 show that the RES policy obtains a maximum improvement of about 3%,

A Dispatching Policy for the Dynamic and Stochastic ...
307
Table 1
Results for N1 = 40 and N2 = 160
Dataset
(l1, l2, 𝜖1, 𝜖2)
m
𝜶∗
RES
ANT
DEV (%)
Ts
DEV (%)
Ts
(0, 0, 0, 0)
20
(0.1, 0.9)
−0.39
0.01
4.92
29.36
40
(0.2, 0.8)
−1.09
0.01
−4.43
76.97
60
(0.2, 0.8)
−1.12
0.01
−4.84
144.205
80
(0.2, 0.8)
−1.12
0.01
−5.21
243.92
100
(0.2, 0.8)
−1.12
0.01
−4.87
383.23
(15, 15, 0, 0)
20
(0.1, 0.9)
−0.46
0.01
10.24
28.145
40
(0.2, 0.8)
−2.78
0.01
−8.03
71.005
60
(0.2, 0.8)
−2.9
0.01
−10.57
134.68
80
(0.2, 0.8)
−2.9
0.01
−10.19
229.54
100
(0.1, 0.9)
−2.9
0.01
−9.76
361.67
(15, 15, 15, 15)
20
(0.1, 0.9)
−0.35
0.01
13.32
24.445
40
(0.2, 0.8)
−2.1
0.01
−7.36
70.405
60
(0.1, 0.9)
−2.16
0.01
−7.91
133.395
80
(0.1, 0.9)
−2.11
0.01
−8.64
229.63
100
(0.1, 0.9)
−1.97
0.01
−8.81
359.105
(15, 15, 0, 15)
20
(0.1, 0.9)
−0.46
0.01
13.74
30.74
40
(0.2, 0.8)
−2.06
0.01
−6.87
72.66
60
(0.1, 0.9)
−2.11
0.01
−7.56
131.995
80
(0.1, 0.9)
−2.07
0.01
−8.1
226.97
100
(0.1, 0.9)
−1.92
0.01
−9.01
363.075
whereas the maximum improvement of the anticipatory procedure is about 10%. On
the other hand, we observe that the RES policy is extremely fast, with an average time
to allocate a single request that is consistently equal to 0.01 s. As expected, the ANT
procedure is much slower, taking up to 383 s to take a dispatching decision. This is
obviously in contrast with the requirement that the value of Ts must be lower than
the inter-arrival time between two consecutive requests (about 144 s in this case).
Table 2 contains the results for the case with an overall number of requests equal
to 1500. For this table, we report only the results for RES because the ANT proce-
dure has always obtained a value for Ts consistently higher than the inter-arrival time
between two consecutive requests (about 20 s in this case). The results show that the
RES policy obtains a maximum improvement of about 33%. As far as the values of
Ts are concerned, the procedure is a little slower than the previous case, even if still
feasible for a real-world application. Indeed, the values are always 0.02 s, with the
only exceptions of 0.03 s in one case and 0.30 s in two cases.

308
G. Ghiani et al.
Table 2
Results for N1 = 750 and N2 = 750
Dataset
(l1, l2, 𝜖1, 𝜖2)
m
𝜶∗
RES
DEV (%)
Ts
(0, 0, 0, 0)
60
(0.6, 0.4)
−23.86
0.30
80
(0.2, 0.8)
−0.27
0.02
100
(0.3, 0.7)
−1.46
0.02
120
(0.4, 0.6)
−2.11
0.02
140
(0.4, 0.6)
−2.19
0.02
(15, 15, 0, 0)
60
(0.6, 0.4)
−32.72
0.30
80
(0.2, 0.8)
−1.13
0.02
100
(0.4, 0.6)
−3.86
0.02
120
(0.4, 0.6)
−5.36
0.02
140
(0.4, 0.6)
−5.48
0.02
(15, 15, 15, 15)
60
(0.1, 0.9)
0.08
0.02
80
(0.2, 0.8)
−1.59
0.02
100
(0.3, 0.7)
−4.34
0.02
120
(0.3, 0.7)
−5.12
0.02
140
(0.3, 0.7)
−5.05
0.02
(15, 15, 0, 15)
60
(0.6, 0.4)
−2.23
0.03
80
(0.3, 0.7)
−1.8
0.02
100
(0.3, 0.7)
−4.18
0.02
120
(0.4, 0.6)
−4.98
0.02
140
(0.3, 0.7)
−5.04
0.02
4
Conclusions
In this paper, we have studied the dynamic and stochastic pickup and delivery prob-
lem, in which a ﬂeet of vehicles must service a set of customers requests that arise
dynamically and are partitioned in several classes. We have proposed a simple but
eﬀective dispatching policy that, every time a new request occurs, allocates it with
the goal of minimizing the overall customers inconvenience. The basic idea is to
reserve a fraction of the vehicles to service the requests qualiﬁed for being satisﬁed
as soon as possible. In addition, our policy is parameterized and the optimal para-
meter setting is determined by solving oﬀ-line a training problem. The goal of our
computational results was to show that our policy can achieve results comparable to
those of an anticipatory procedure in terms of solution quality and to those of a sim-
ple reactive approach in terms of running times. The results of the experimentation
on randomly-generated instances have conﬁrmed this intuition, especially when the
number of overall daily requests grew up to 1500.

A Dispatching Policy for the Dynamic and Stochastic ...
309
Acknowledgements This work was partly supported by the Ministero dell’Istruzione,
dell’Università e della Ricerca (MIUR) of Italy (PRIN project “Transportation and Logistics Opti-
mization in the Era of Big and Open Data”). This support is gratefully acknowledged.
References
1. Ferrucci, F., Bock, S., Gendreau, M.: A pro-active real-time control approach for dynamic vehi-
cle routing problems dealing with the delivery of urgent goods. Eur. J. Oper. Res. 225(1), 130–
141 (2013)
2. Gendreau, M., Guertin, F., Potvin, J.Y., Sguin, R.: Neighborhood search heuristics for a dynamic
vehicle dispatching problem with pick-ups and deliveries. Transp. Res. Part C: Emerg. Technol.
14(3), 157–174 (2006)
3. Ghiani, G., Manni, E., Quaranta, A., Triki, C.: Anticipatory algorithms for same-day courier
dispatching. Transp. Res. Part E: Logist. Transp. Rev. 45(1), 96–106 (2009)
4. Ghiani, G., Manni, E., Thomas, B.W.: A comparison of anticipatory algorithms for the dynamic
and stochastic traveling salesman problem. Transp. Sci. 46(3), 374–387 (2012)
5. Ritzinger, U., Puchinger, J., Hartl, R.F.: A survey on dynamic and stochastic vehicle routing
problems. Int. J. Prod. Res. 54(1), 215–231 (2016)

